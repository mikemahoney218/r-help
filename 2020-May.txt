From @purd|e@@ @end|ng |rom gm@||@com  Fri May  1 07:21:32 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Fri, 1 May 2020 17:21:32 +1200
Subject: [R] stats:: spline's method could not be monoH.FC
In-Reply-To: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
Message-ID: <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>

I agree, the documentation gives the impression that stats::spline
would allow "monoH.FC".

My guess is that stats::spline doesn't allow it because the function
is designed to return a list with x and y components. This doesn't
suit monotonic cubic Hermite splines because the function would also
need to return the slopes. Furthermore, additional functions may (or
may not) be required depending on what you want to do with x, y and
slope vectors.

Note that my package kubik, provides a range of functions for working
with cubic Hermite splines.






On Fri, May 1, 2020 at 4:39 AM Samuel Granjeaud IR/Inserm
<samuel.granjeaud at inserm.fr> wrote:
>
> Hi,
>
> I have just noticed that the argument method of the spline function of
> the stats package does not allow to specify monoH.FC although the
> documentation tells it should be possible.
>
> I know how to program a workaround. This post intends to alert the
> maintainers.
>
> Stay safe,
> Samuel
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Fri May  1 16:14:47 2020
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 1 May 2020 16:14:47 +0200
Subject: [R] [FORGED] Re:  pair correlation function of 3D points
In-Reply-To: <e2fdd5e8-2d1e-a07e-e5ed-4163380c0e17@auckland.ac.nz>
References: <07f9c7f7f729a3a9bf641fd67fd8a165f7d799e5.camel@math.aau.dk>
 <F5F41F99-A6C7-42D8-A197-C4A532C1123B@icmpe.cnrs.fr>
 <e2fdd5e8-2d1e-a07e-e5ed-4163380c0e17@auckland.ac.nz>
Message-ID: <24236.11991.134621.644096@stat.math.ethz.ch>

>>>>> Rolf Turner 
>>>>>     on Thu, 30 Apr 2020 10:42:46 +1200 writes:

    > On 30/04/20 12:28 am, Eric Leroy wrote:

    >> Dear all, I am sorry to see all the reactions I provoked from a
    >> newbie user. Anyway, thank you for the answer I think that the
    >> pcf3est function responds to my question.
    >> Indeed the spatstat is a very impressive library and I am very grateful to the the developers.

    > (1) Not to worry.  Certainly not your fault!

    > (2) I'm glad that the pcf3est() function was useful to you.

    > (3) Thank you for your kind words about spatstat.

    > (4) But *please* --- spatstat is a *package* not a "library"!!!  A 
    > library is a *collection* of packages; the library() function "checks 
    > out" a package from a library, like checking a book out of a "real" 
    > library (biblioth?que en fran?ais, just in case there is any confusion, 
    > "library" and "libraire" being false cognates).  But I'm sure you knew that.

    > I know that insisting on this distinction is being pedantic --- but it 
    > never hurts to get things right!  And saying "library" when you mean 
    > "package" upsets Martin Maechler!!! :-)

Thanks a lot, Rolf!

Well, indeed, I've become a teeny bit wiser with age, so
currently, it's just a little inaudible "ouch", and no longer a
real upsetting ...

And yes

> fortunes::fortune("rrgh")

(3 times...rrrrgh...) and why do you think the mailing list is called R-*packages* ???????????
Please do
  for(i in 1:20) cat("It's a package!\n")
   -- Martin Maechler (after a newly released *package* has been called *library* three times in its announcement
      on R-packages)
      R-help (March 2006)

>

so maybe we can close the thread on a bit of a  cheerful tone.

Best, Martin


From g||ted|||e2014 @end|ng |rom gm@||@com  Fri May  1 18:46:25 2020
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Fri, 1 May 2020 17:46:25 +0100
Subject: [R] Datetime misrepresented on x-axis of a multiple plot
Message-ID: <CAC8ss33yaDtFY-6+8dFD8jUxks7jtu9HN6n13_CTRKgjHP8vVw@mail.gmail.com>

Dear Contributors,
I am trying to do a plot with multiple y-axis on a common x-axis. I have
made some progress but I am having difficulties with datetime appearing on
x-axis. Instead of as date, it appears as large numbers.

Sample of my data:
78 09 28  0 6.7 -40.4 -3.5 2.3 -3.6 278036 5.8 612
78 09 28  1 5.7 7.3 -4.4 1.4 0.6 261169 4.9 623
78 09 28  2 5.6 -20.9 -3.6 3.1 -1.8 269323 4.8 625
78 09 28  3 5.8 -46.9 -3.4 0.2 -3.6 254221 4.7 626
78 09 28  4 6.2 -41.4 -3.9 1.2 -3.6 248567 4.2 618
78 09 28  5 6.1 -54.6 -3 1 -4.4 252669 4.4 629
78 09 28  6 NA NA NA NA NA 220480 4.3 606
78 09 28  7 7.1 26 -4.8 3.6 2.9 216887 4.4 613
78 09 28  8 7.1 -21.4 -4.7 3.5 -2.3 249928 4 643
78 09 28  9 7 -13.6 -2.7 5.2 -1.4 268806 4 656
78 09 28  10 6.6 -18.5 -3.7 3.6 -1.7 285608 4.2 651
78 09 28  11 6.7 -6.6 -2.1 4.7 -0.6 280446 4.2 661
78 09 28  12 6.7 -29.7 -1.9 4 -2.5 281928 4.2 659
78 09 28  13 6.2 -8.7 -2.1 4.7 -0.8 273465 4.4 651
78 09 28  14 6.1 31.8 -4.7 0.9 3 212374 4.2 631
78 09 28  15 5.4 9.3 -4.5 -0.9 0.8 219662 3.9 636
78 09 28  16 4.9 6.3 -4.5 -0.5 0.5 220610 3.8 628
78 09 28  17 4.7 4.5 -4.3 -0.6 0.3 214432 3.7 628
78 09 28  18 4.9 3.1 -4.5 0.3 0.2 202199 3.5 623
78 09 28  19 5 -13 -4.6 -0.3 -1.1 192859 3.2 619
78 09 28  20 5.1 -26 -3.5 2 -2 193868 3.1 627
78 09 28  21 10.1 -37.5 -5.1 4.9 -5.4 284122 5.9 683
78 09 28  22 8.4 -7.3 -3.6 5.5 -0.8 367499 6.2 694
78 09 28  23 8.2 -17.7 -4.7 4.4 -2.1 346644 4.9 689
78 09 29  0 8 6.3 -4.3 5.8 0.8 269569 4.7 708
78 09 29  1 8.6 35 -3 6.2 4.8 187132 3.2 709
78 09 29  2 8.1 24.8 -4.1 6 3.3 166644 3.5 689
78 09 29  3 15.9 29.6 -2.4 9.6 5.6 720902 6.6 866
78 09 29  4 14.9 18.9 -2.8 13.8 4.8 1587324 8.1 912
78 09 29  5 14.1 8.2 -10.2 8.3 1.9 1509336 8.1 871
78 09 29  6 15.6 -2.9 -6.1 11.5 -0.7 968890 7.8 878
78 09 29  7 19.6 -49.7 1.2 12.1 -14.4 574978 5.4 906
78 09 29  8 23.5 -44.2 -1.4 11.7 -11.5 287721 4.1 865
78 09 29  9 25.3 -71.8 -2.4 7.5 -23.9 58521 5.1 859
78 09 29  10 24.8 -63.1 -2.3 10.9 -22 74956 1.5 821
78 09 29  11 23.8 -50 -2 15.1 -18.1 64510 3 807
78 09 29  12 21.4 -34.3 0.1 17.6 -12 50247 2.3 795
78 09 29  13 18.4 -20 -2 17 -6.3 59939 2.9 802
78 09 29  14 16.4 -13.9 -2.8 15.6 -3.9 56499 1.4 799
78 09 29  15 15.3 -7.9 -2 14.9 -2.1 49346 1.3 759
78 09 29  16 14.1 -0.5 -2.9 13.8 -0.1 57732 0.5 730
78 09 29  17 12.6 14.7 -3.1 11.5 3.1 44351 1.7 722
78 09 29  18 11.2 74.3 0.6 2.9 10.4 68224 0.4 712
78 09 29  19 10.5 60.5 3.2 4 9.1 48082 0.2 706
78 09 29  20 10 54.7 3.9 4.2 8.1 56404 0.2 710
78 09 29  21 9.3 61.1 3.4 2.9 8.1 68824 0.2 694
78 09 29  22 8.6 64.4 2.6 2.7 7.8 44579 0.2 683
78 09 29  23 7.9 63.3 2 3 7.1 39760 0.2 661
78 09 30  0 7.3 59.8 2.3 2.8 6.3 NA NA NA
78 09 30  1 6.6 65.6 1.7 2.1 6 57382 0.5 664
78 09 30  2 6 70.8 1.3 1.4 5.5 63540 0.5 654
78 09 30  3 4.4 45.1 1 2.7 2.9 60856 2.5 635
78 09 30  4 3.6 -28 -1 -0.3 -0.6 80967 1.9 633
78 09 30  5 4 -47.6 -2.2 -0.1 -2.4 48391 1.2 642
78 09 30  6 4.1 -28.9 -3.1 -1.4 -1.9 45012 1.1 643
78 09 30  7 4.2 -42.7 -2.7 1.2 -2.7 45562 1.1 633
 and part of my code is:
Sys.setenv( TZ="GMT" )
dta<-read.table("IFEDIFIG1D",col.names=c("year", "month", "day",
"hour","B","LAT","BX","BZ","BY","SWT","SWD","SW"))
dta$year <- with( dta, ifelse(year < 50, year + 2000, year + 1900))
dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year,
month,day,hour,0,0)))
x =  dta$datetime
B=dta$B
LAT=dta$LAT
BX=dta$BX
BZ=dta$BZ
 par(mfcol = c(3, 1), mar = numeric(4), oma = c(4, 4, 1, 1),
    mgp = c(2, 0.5, 0))#0.5 here controls x-axis margins
plot(x, B,type="l", axes = FALSE)
abline(v=-3)
axis(2L)
plot(x, LAT,type="l", axes = FALSE)
abline(v=-3)
axis(2L)
 plot(x, BX, type="l",axes = FALSE)
abline(v=-3)
axis(1L)
axis(2L)
mtext("A1", side = 1, outer = TRUE, line = 2.2)
mtext("B1", side = 2, outer = TRUE, line = 2.2,at =0.2)
mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.5)
mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.8)

If I do a simple plot of x and B, the x-axis will be fine, appearing as
Thu, Fri, Sat, Sun.
Please let me know what I am doing wrong with the multiple plot code above.
I want the same Thu, Fri, Sat, Sun to appear on the common x-axis.

I am most grateful for your kind response.

Warmest regards
Ogbos

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri May  1 19:29:56 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 1 May 2020 18:29:56 +0100
Subject: [R] Datetime misrepresented on x-axis of a multiple plot
In-Reply-To: <CAC8ss33yaDtFY-6+8dFD8jUxks7jtu9HN6n13_CTRKgjHP8vVw@mail.gmail.com>
References: <CAC8ss33yaDtFY-6+8dFD8jUxks7jtu9HN6n13_CTRKgjHP8vVw@mail.gmail.com>
Message-ID: <80280e72-a629-ad6a-5f54-f7691589e59d@sapo.pt>

Hello,

If you don't mind a ggplot2 solution, here it is.
As usual with ggplot, it's better to have the data in long format so I 
also load packages dplyr and tidyr. The reshaping accounts for half the 
code, the code  for the graph itself is not very complicated.



library(ggplot2)
library(scales)
library(dplyr)
library(tidyr)

dta %>%
   select(datetime, B, BX, BZ) %>%
   rename(x = datetime, B1 = B, B2 = BX, B3 = BZ) %>%
   pivot_longer(
     cols = starts_with("B"),
     names_to = "variable",
     values_to = "value"
   ) %>%
   ggplot(aes(x, value)) +
   geom_line() +
   xlab("") + ylab("") +
   scale_x_datetime(
     breaks = seq(min(x), max(x), by = "days"),
     labels = time_format("%a")) +
   facet_grid(rows = vars(variable)) +
   theme_bw()



Hope this helps,

Rui Barradas

?s 17:46 de 01/05/20, Ogbos Okike escreveu:
> Dear Contributors,
> I am trying to do a plot with multiple y-axis on a common x-axis. I have
> made some progress but I am having difficulties with datetime appearing on
> x-axis. Instead of as date, it appears as large numbers.
> 
> Sample of my data:
> 78 09 28  0 6.7 -40.4 -3.5 2.3 -3.6 278036 5.8 612
> 78 09 28  1 5.7 7.3 -4.4 1.4 0.6 261169 4.9 623
> 78 09 28  2 5.6 -20.9 -3.6 3.1 -1.8 269323 4.8 625
> 78 09 28  3 5.8 -46.9 -3.4 0.2 -3.6 254221 4.7 626
> 78 09 28  4 6.2 -41.4 -3.9 1.2 -3.6 248567 4.2 618
> 78 09 28  5 6.1 -54.6 -3 1 -4.4 252669 4.4 629
> 78 09 28  6 NA NA NA NA NA 220480 4.3 606
> 78 09 28  7 7.1 26 -4.8 3.6 2.9 216887 4.4 613
> 78 09 28  8 7.1 -21.4 -4.7 3.5 -2.3 249928 4 643
> 78 09 28  9 7 -13.6 -2.7 5.2 -1.4 268806 4 656
> 78 09 28  10 6.6 -18.5 -3.7 3.6 -1.7 285608 4.2 651
> 78 09 28  11 6.7 -6.6 -2.1 4.7 -0.6 280446 4.2 661
> 78 09 28  12 6.7 -29.7 -1.9 4 -2.5 281928 4.2 659
> 78 09 28  13 6.2 -8.7 -2.1 4.7 -0.8 273465 4.4 651
> 78 09 28  14 6.1 31.8 -4.7 0.9 3 212374 4.2 631
> 78 09 28  15 5.4 9.3 -4.5 -0.9 0.8 219662 3.9 636
> 78 09 28  16 4.9 6.3 -4.5 -0.5 0.5 220610 3.8 628
> 78 09 28  17 4.7 4.5 -4.3 -0.6 0.3 214432 3.7 628
> 78 09 28  18 4.9 3.1 -4.5 0.3 0.2 202199 3.5 623
> 78 09 28  19 5 -13 -4.6 -0.3 -1.1 192859 3.2 619
> 78 09 28  20 5.1 -26 -3.5 2 -2 193868 3.1 627
> 78 09 28  21 10.1 -37.5 -5.1 4.9 -5.4 284122 5.9 683
> 78 09 28  22 8.4 -7.3 -3.6 5.5 -0.8 367499 6.2 694
> 78 09 28  23 8.2 -17.7 -4.7 4.4 -2.1 346644 4.9 689
> 78 09 29  0 8 6.3 -4.3 5.8 0.8 269569 4.7 708
> 78 09 29  1 8.6 35 -3 6.2 4.8 187132 3.2 709
> 78 09 29  2 8.1 24.8 -4.1 6 3.3 166644 3.5 689
> 78 09 29  3 15.9 29.6 -2.4 9.6 5.6 720902 6.6 866
> 78 09 29  4 14.9 18.9 -2.8 13.8 4.8 1587324 8.1 912
> 78 09 29  5 14.1 8.2 -10.2 8.3 1.9 1509336 8.1 871
> 78 09 29  6 15.6 -2.9 -6.1 11.5 -0.7 968890 7.8 878
> 78 09 29  7 19.6 -49.7 1.2 12.1 -14.4 574978 5.4 906
> 78 09 29  8 23.5 -44.2 -1.4 11.7 -11.5 287721 4.1 865
> 78 09 29  9 25.3 -71.8 -2.4 7.5 -23.9 58521 5.1 859
> 78 09 29  10 24.8 -63.1 -2.3 10.9 -22 74956 1.5 821
> 78 09 29  11 23.8 -50 -2 15.1 -18.1 64510 3 807
> 78 09 29  12 21.4 -34.3 0.1 17.6 -12 50247 2.3 795
> 78 09 29  13 18.4 -20 -2 17 -6.3 59939 2.9 802
> 78 09 29  14 16.4 -13.9 -2.8 15.6 -3.9 56499 1.4 799
> 78 09 29  15 15.3 -7.9 -2 14.9 -2.1 49346 1.3 759
> 78 09 29  16 14.1 -0.5 -2.9 13.8 -0.1 57732 0.5 730
> 78 09 29  17 12.6 14.7 -3.1 11.5 3.1 44351 1.7 722
> 78 09 29  18 11.2 74.3 0.6 2.9 10.4 68224 0.4 712
> 78 09 29  19 10.5 60.5 3.2 4 9.1 48082 0.2 706
> 78 09 29  20 10 54.7 3.9 4.2 8.1 56404 0.2 710
> 78 09 29  21 9.3 61.1 3.4 2.9 8.1 68824 0.2 694
> 78 09 29  22 8.6 64.4 2.6 2.7 7.8 44579 0.2 683
> 78 09 29  23 7.9 63.3 2 3 7.1 39760 0.2 661
> 78 09 30  0 7.3 59.8 2.3 2.8 6.3 NA NA NA
> 78 09 30  1 6.6 65.6 1.7 2.1 6 57382 0.5 664
> 78 09 30  2 6 70.8 1.3 1.4 5.5 63540 0.5 654
> 78 09 30  3 4.4 45.1 1 2.7 2.9 60856 2.5 635
> 78 09 30  4 3.6 -28 -1 -0.3 -0.6 80967 1.9 633
> 78 09 30  5 4 -47.6 -2.2 -0.1 -2.4 48391 1.2 642
> 78 09 30  6 4.1 -28.9 -3.1 -1.4 -1.9 45012 1.1 643
> 78 09 30  7 4.2 -42.7 -2.7 1.2 -2.7 45562 1.1 633
>   and part of my code is:
> Sys.setenv( TZ="GMT" )
> dta<-read.table("IFEDIFIG1D",col.names=c("year", "month", "day",
> "hour","B","LAT","BX","BZ","BY","SWT","SWD","SW"))
> dta$year <- with( dta, ifelse(year < 50, year + 2000, year + 1900))
> dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year,
> month,day,hour,0,0)))
> x =  dta$datetime
> B=dta$B
> LAT=dta$LAT
> BX=dta$BX
> BZ=dta$BZ
>   par(mfcol = c(3, 1), mar = numeric(4), oma = c(4, 4, 1, 1),
>      mgp = c(2, 0.5, 0))#0.5 here controls x-axis margins
> plot(x, B,type="l", axes = FALSE)
> abline(v=-3)
> axis(2L)
> plot(x, LAT,type="l", axes = FALSE)
> abline(v=-3)
> axis(2L)
>   plot(x, BX, type="l",axes = FALSE)
> abline(v=-3)
> axis(1L)
> axis(2L)
> mtext("A1", side = 1, outer = TRUE, line = 2.2)
> mtext("B1", side = 2, outer = TRUE, line = 2.2,at =0.2)
> mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.5)
> mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.8)
> 
> If I do a simple plot of x and B, the x-axis will be fine, appearing as
> Thu, Fri, Sat, Sun.
> Please let me know what I am doing wrong with the multiple plot code above.
> I want the same Thu, Fri, Sat, Sun to appear on the common x-axis.
> 
> I am most grateful for your kind response.
> 
> Warmest regards
> Ogbos
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri May  1 19:35:22 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 1 May 2020 18:35:22 +0100
Subject: [R] Datetime misrepresented on x-axis of a multiple plot
In-Reply-To: <80280e72-a629-ad6a-5f54-f7691589e59d@sapo.pt>
References: <CAC8ss33yaDtFY-6+8dFD8jUxks7jtu9HN6n13_CTRKgjHP8vVw@mail.gmail.com>
 <80280e72-a629-ad6a-5f54-f7691589e59d@sapo.pt>
Message-ID: <28f66e3f-f988-bf07-e07f-f3842e2086b1@sapo.pt>

Hello,

I have just noticed that the facets' labels are on the right, in your 
code example they are on the left. Change the following:


facet_grid(rows = vars(variable), switch = "y")


Hope this helps,

Rui Barradas

?s 18:29 de 01/05/20, Rui Barradas escreveu:
> Hello,
> 
> If you don't mind a ggplot2 solution, here it is.
> As usual with ggplot, it's better to have the data in long format so I 
> also load packages dplyr and tidyr. The reshaping accounts for half the 
> code, the code? for the graph itself is not very complicated.
> 
> 
> 
> library(ggplot2)
> library(scales)
> library(dplyr)
> library(tidyr)
> 
> dta %>%
>  ? select(datetime, B, BX, BZ) %>%
>  ? rename(x = datetime, B1 = B, B2 = BX, B3 = BZ) %>%
>  ? pivot_longer(
>  ??? cols = starts_with("B"),
>  ??? names_to = "variable",
>  ??? values_to = "value"
>  ? ) %>%
>  ? ggplot(aes(x, value)) +
>  ? geom_line() +
>  ? xlab("") + ylab("") +
>  ? scale_x_datetime(
>  ??? breaks = seq(min(x), max(x), by = "days"),
>  ??? labels = time_format("%a")) +
>  ? facet_grid(rows = vars(variable)) +
>  ? theme_bw()
> 
> 
> 
> Hope this helps,
> 
> Rui Barradas
> 
> ?s 17:46 de 01/05/20, Ogbos Okike escreveu:
>> Dear Contributors,
>> I am trying to do a plot with multiple y-axis on a common x-axis. I have
>> made some progress but I am having difficulties with datetime 
>> appearing on
>> x-axis. Instead of as date, it appears as large numbers.
>>
>> Sample of my data:
>> 78 09 28? 0 6.7 -40.4 -3.5 2.3 -3.6 278036 5.8 612
>> 78 09 28? 1 5.7 7.3 -4.4 1.4 0.6 261169 4.9 623
>> 78 09 28? 2 5.6 -20.9 -3.6 3.1 -1.8 269323 4.8 625
>> 78 09 28? 3 5.8 -46.9 -3.4 0.2 -3.6 254221 4.7 626
>> 78 09 28? 4 6.2 -41.4 -3.9 1.2 -3.6 248567 4.2 618
>> 78 09 28? 5 6.1 -54.6 -3 1 -4.4 252669 4.4 629
>> 78 09 28? 6 NA NA NA NA NA 220480 4.3 606
>> 78 09 28? 7 7.1 26 -4.8 3.6 2.9 216887 4.4 613
>> 78 09 28? 8 7.1 -21.4 -4.7 3.5 -2.3 249928 4 643
>> 78 09 28? 9 7 -13.6 -2.7 5.2 -1.4 268806 4 656
>> 78 09 28? 10 6.6 -18.5 -3.7 3.6 -1.7 285608 4.2 651
>> 78 09 28? 11 6.7 -6.6 -2.1 4.7 -0.6 280446 4.2 661
>> 78 09 28? 12 6.7 -29.7 -1.9 4 -2.5 281928 4.2 659
>> 78 09 28? 13 6.2 -8.7 -2.1 4.7 -0.8 273465 4.4 651
>> 78 09 28? 14 6.1 31.8 -4.7 0.9 3 212374 4.2 631
>> 78 09 28? 15 5.4 9.3 -4.5 -0.9 0.8 219662 3.9 636
>> 78 09 28? 16 4.9 6.3 -4.5 -0.5 0.5 220610 3.8 628
>> 78 09 28? 17 4.7 4.5 -4.3 -0.6 0.3 214432 3.7 628
>> 78 09 28? 18 4.9 3.1 -4.5 0.3 0.2 202199 3.5 623
>> 78 09 28? 19 5 -13 -4.6 -0.3 -1.1 192859 3.2 619
>> 78 09 28? 20 5.1 -26 -3.5 2 -2 193868 3.1 627
>> 78 09 28? 21 10.1 -37.5 -5.1 4.9 -5.4 284122 5.9 683
>> 78 09 28? 22 8.4 -7.3 -3.6 5.5 -0.8 367499 6.2 694
>> 78 09 28? 23 8.2 -17.7 -4.7 4.4 -2.1 346644 4.9 689
>> 78 09 29? 0 8 6.3 -4.3 5.8 0.8 269569 4.7 708
>> 78 09 29? 1 8.6 35 -3 6.2 4.8 187132 3.2 709
>> 78 09 29? 2 8.1 24.8 -4.1 6 3.3 166644 3.5 689
>> 78 09 29? 3 15.9 29.6 -2.4 9.6 5.6 720902 6.6 866
>> 78 09 29? 4 14.9 18.9 -2.8 13.8 4.8 1587324 8.1 912
>> 78 09 29? 5 14.1 8.2 -10.2 8.3 1.9 1509336 8.1 871
>> 78 09 29? 6 15.6 -2.9 -6.1 11.5 -0.7 968890 7.8 878
>> 78 09 29? 7 19.6 -49.7 1.2 12.1 -14.4 574978 5.4 906
>> 78 09 29? 8 23.5 -44.2 -1.4 11.7 -11.5 287721 4.1 865
>> 78 09 29? 9 25.3 -71.8 -2.4 7.5 -23.9 58521 5.1 859
>> 78 09 29? 10 24.8 -63.1 -2.3 10.9 -22 74956 1.5 821
>> 78 09 29? 11 23.8 -50 -2 15.1 -18.1 64510 3 807
>> 78 09 29? 12 21.4 -34.3 0.1 17.6 -12 50247 2.3 795
>> 78 09 29? 13 18.4 -20 -2 17 -6.3 59939 2.9 802
>> 78 09 29? 14 16.4 -13.9 -2.8 15.6 -3.9 56499 1.4 799
>> 78 09 29? 15 15.3 -7.9 -2 14.9 -2.1 49346 1.3 759
>> 78 09 29? 16 14.1 -0.5 -2.9 13.8 -0.1 57732 0.5 730
>> 78 09 29? 17 12.6 14.7 -3.1 11.5 3.1 44351 1.7 722
>> 78 09 29? 18 11.2 74.3 0.6 2.9 10.4 68224 0.4 712
>> 78 09 29? 19 10.5 60.5 3.2 4 9.1 48082 0.2 706
>> 78 09 29? 20 10 54.7 3.9 4.2 8.1 56404 0.2 710
>> 78 09 29? 21 9.3 61.1 3.4 2.9 8.1 68824 0.2 694
>> 78 09 29? 22 8.6 64.4 2.6 2.7 7.8 44579 0.2 683
>> 78 09 29? 23 7.9 63.3 2 3 7.1 39760 0.2 661
>> 78 09 30? 0 7.3 59.8 2.3 2.8 6.3 NA NA NA
>> 78 09 30? 1 6.6 65.6 1.7 2.1 6 57382 0.5 664
>> 78 09 30? 2 6 70.8 1.3 1.4 5.5 63540 0.5 654
>> 78 09 30? 3 4.4 45.1 1 2.7 2.9 60856 2.5 635
>> 78 09 30? 4 3.6 -28 -1 -0.3 -0.6 80967 1.9 633
>> 78 09 30? 5 4 -47.6 -2.2 -0.1 -2.4 48391 1.2 642
>> 78 09 30? 6 4.1 -28.9 -3.1 -1.4 -1.9 45012 1.1 643
>> 78 09 30? 7 4.2 -42.7 -2.7 1.2 -2.7 45562 1.1 633
>> ? and part of my code is:
>> Sys.setenv( TZ="GMT" )
>> dta<-read.table("IFEDIFIG1D",col.names=c("year", "month", "day",
>> "hour","B","LAT","BX","BZ","BY","SWT","SWD","SW"))
>> dta$year <- with( dta, ifelse(year < 50, year + 2000, year + 1900))
>> dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year,
>> month,day,hour,0,0)))
>> x =? dta$datetime
>> B=dta$B
>> LAT=dta$LAT
>> BX=dta$BX
>> BZ=dta$BZ
>> ? par(mfcol = c(3, 1), mar = numeric(4), oma = c(4, 4, 1, 1),
>> ???? mgp = c(2, 0.5, 0))#0.5 here controls x-axis margins
>> plot(x, B,type="l", axes = FALSE)
>> abline(v=-3)
>> axis(2L)
>> plot(x, LAT,type="l", axes = FALSE)
>> abline(v=-3)
>> axis(2L)
>> ? plot(x, BX, type="l",axes = FALSE)
>> abline(v=-3)
>> axis(1L)
>> axis(2L)
>> mtext("A1", side = 1, outer = TRUE, line = 2.2)
>> mtext("B1", side = 2, outer = TRUE, line = 2.2,at =0.2)
>> mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.5)
>> mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.8)
>>
>> If I do a simple plot of x and B, the x-axis will be fine, appearing as
>> Thu, Fri, Sat, Sun.
>> Please let me know what I am doing wrong with the multiple plot code 
>> above.
>> I want the same Thu, Fri, Sat, Sun to appear on the common x-axis.
>>
>> I am most grateful for your kind response.
>>
>> Warmest regards
>> Ogbos
>>
>> ????[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rcoppock @end|ng |rom cox@net  Fri May  1 18:24:03 2020
From: rcoppock @end|ng |rom cox@net (Roger Coppock)
Date: Fri, 1 May 2020 09:24:03 -0700
Subject: [R] CRAN library down?
Message-ID: <E17F07D7-3E85-4737-9DB6-EB5C511E619B@cox.net>

After I changed to Version R version 4.0.0 (2020-04-24) -- "Arbor Day" on my MacBook running Mac OS 10.13.6, I lost all my loaded libraries.  Also, the package installer can not contact CRAN libraries either for binaries or sources, to replace the missing loaded libraries.  The package installer can contact "BioConductor", however.  I am now specifically looking for "HURDAT" and "lmtest", which were on CRAN but not "BioConductor".

- -  Roger Coppock (rcoppock at cox.net)

From murdoch@dunc@n @end|ng |rom gm@||@com  Fri May  1 20:19:32 2020
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Fri, 1 May 2020 14:19:32 -0400
Subject: [R] CRAN library down?
In-Reply-To: <E17F07D7-3E85-4737-9DB6-EB5C511E619B@cox.net>
References: <E17F07D7-3E85-4737-9DB6-EB5C511E619B@cox.net>
Message-ID: <3a6465ff-519b-7cf4-6a3a-0de5bb86647e@gmail.com>

On 01/05/2020 12:24 p.m., Roger Coppock wrote:
> After I changed to Version R version 4.0.0 (2020-04-24) -- "Arbor Day" on my MacBook running Mac OS 10.13.6, I lost all my loaded libraries.  Also, the package installer can not contact CRAN libraries either for binaries or sources, to replace the missing loaded libraries.  The package installer can contact "BioConductor", however.  I am now specifically looking for "HURDAT" and "lmtest", which were on CRAN but not "BioConductor".

I'd guess you're using a mirror that is slow to update.  I recommend 
cloud.r-project.org.  It is very well maintained by the RStudio folks.

Duncan Murdoch


From r@turner @end|ng |rom @uck|@nd@@c@nz  Fri May  1 23:13:25 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sat, 2 May 2020 09:13:25 +1200
Subject: [R] 
 [FORGED]  Datetime misrepresented on x-axis of a multiple plot
In-Reply-To: <CAC8ss33yaDtFY-6+8dFD8jUxks7jtu9HN6n13_CTRKgjHP8vVw@mail.gmail.com>
References: <CAC8ss33yaDtFY-6+8dFD8jUxks7jtu9HN6n13_CTRKgjHP8vVw@mail.gmail.com>
Message-ID: <7ee9828c-90d3-cb30-c074-35fd952998ef@auckland.ac.nz>


On 2/05/20 4:46 am, Ogbos Okike wrote:

> Dear Contributors,
> I am trying to do a plot with multiple y-axis on a common x-axis.

<SNIP>

I would strongly advise you *not* to.  This, although often done, is a 
bad practice, and can sometimes (often) give misleading impressions.  See

     https://blog.datawrapper.de/dualaxis/

cheers,

Rolf


-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From drj|m|emon @end|ng |rom gm@||@com  Sat May  2 00:46:50 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sat, 2 May 2020 08:46:50 +1000
Subject: [R] Datetime misrepresented on x-axis of a multiple plot
In-Reply-To: <CAC8ss33yaDtFY-6+8dFD8jUxks7jtu9HN6n13_CTRKgjHP8vVw@mail.gmail.com>
References: <CAC8ss33yaDtFY-6+8dFD8jUxks7jtu9HN6n13_CTRKgjHP8vVw@mail.gmail.com>
Message-ID: <CA+8X3fWGCtgq7wHC0AvDmgfzbJ4MNgDv62t2ZMT9r6iJf9C9Yg@mail.gmail.com>

Hi Ogbos,
The following code may get you close to what you want. I have used the
names of the columns in "dta" as it is less confusing for me. I think
you meant to request horizontal ablines as none appeared in the
example. In order to get the axis tick label "Sun" you would have to
increase the x axis limits beyond your data.

plot(dta$datetime,dta$B,type="l",axes=FALSE)
# this is out of range and doesn't appear
abline(h=-3,col="red")
axis(2)
plot(dta$datetime,dta$LAT,type="l",axes=FALSE)
abline(h=-3,col="red")
axis(2)
plot(dta$datetime,dta$BX,type="l",axes=FALSE)
abline(h=-3,col="red")
axis(2)
axis.POSIXct(1,dta$datetime,format="%a")
mtext("A1", side = 1, outer = TRUE, line = 2.2)
mtext("B1", side = 2, outer = TRUE, line = 2.2,at =0.2)
mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.5)
mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.8)

Jim

On Sat, May 2, 2020 at 2:47 AM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>
> Dear Contributors,
> I am trying to do a plot with multiple y-axis on a common x-axis. I have
> made some progress but I am having difficulties with datetime appearing on
> x-axis. Instead of as date, it appears as large numbers.
> ...
> If I do a simple plot of x and B, the x-axis will be fine, appearing as
> Thu, Fri, Sat, Sun.
> Please let me know what I am doing wrong with the multiple plot code above.
> I want the same Thu, Fri, Sat, Sun to appear on the common x-axis.
>


From @purd|e@@ @end|ng |rom gm@||@com  Sat May  2 01:05:57 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Sat, 2 May 2020 11:05:57 +1200
Subject: [R] [FORGED] Re: pair correlation function of 3D points
In-Reply-To: <24236.11991.134621.644096@stat.math.ethz.ch>
References: <07f9c7f7f729a3a9bf641fd67fd8a165f7d799e5.camel@math.aau.dk>
 <F5F41F99-A6C7-42D8-A197-C4A532C1123B@icmpe.cnrs.fr>
 <e2fdd5e8-2d1e-a07e-e5ed-4163380c0e17@auckland.ac.nz>
 <24236.11991.134621.644096@stat.math.ethz.ch>
Message-ID: <CAB8pepzEhwd4vqPnGnBH8uyfKnrUjLJoqBmtgFPKKz2pBrLV0Q@mail.gmail.com>

It may be of some comfort to some readers, that I received emails from
two moderators, telling me that I was....
I acknowledge that my initial email was a mistake.

However, I need to note recent public comments from the author of the
spatstat package:

    Your internet skills are pathetic.

    There is far too much bland "Shhhh.
    We *mustn't* offend anybody" content in current discourse.
    Tell it like it is!
    Ripley into people!
    If the recipient can't take the heat,
    he or she should get out of the kitchen!

    and not whinge about being offended.

https://stat.ethz.ch/pipermail/r-help/2019-July/463555.html

In short, everything I said was in accordance with the wishes of the
package authors...


From g||ted|||e2014 @end|ng |rom gm@||@com  Sat May  2 04:17:07 2020
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Sat, 2 May 2020 03:17:07 +0100
Subject: [R] Datetime misrepresented on x-axis of a multiple plot
In-Reply-To: <28f66e3f-f988-bf07-e07f-f3842e2086b1@sapo.pt>
References: <CAC8ss33yaDtFY-6+8dFD8jUxks7jtu9HN6n13_CTRKgjHP8vVw@mail.gmail.com>
 <80280e72-a629-ad6a-5f54-f7691589e59d@sapo.pt>
 <28f66e3f-f988-bf07-e07f-f3842e2086b1@sapo.pt>
Message-ID: <CAC8ss317CgN19BZ9kZ6WJVZ3j6wJOvTeWeZdm+M-1yNwthRVnQ@mail.gmail.com>

Dear Rui Barradas,
Many thanks for your time.

Though I have not used ggplot before, I quickly installed all the required
packages and attempted your code. It produced the plot and solved the
initial problem of the wrong labels on x-axis.
But I noticed several other problems with the plot (some of which I have
circumvented by adjusting the script I displayed earlier). As I am not
quite familiar with ggplot, it was very difficult for me to interact with
your code. I decided to go back to my own code. I tried to use axis(1 ...,
labels=c()...) to assign Thu, Fri .... It worked for me before I got a
third response (very helpful) from the list.
Best regards
Ogbos

On Fri, May 1, 2020 at 6:35 PM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> I have just noticed that the facets' labels are on the right, in your
> code example they are on the left. Change the following:
>
>
> facet_grid(rows = vars(variable), switch = "y")
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 18:29 de 01/05/20, Rui Barradas escreveu:
> > Hello,
> >
> > If you don't mind a ggplot2 solution, here it is.
> > As usual with ggplot, it's better to have the data in long format so I
> > also load packages dplyr and tidyr. The reshaping accounts for half the
> > code, the code  for the graph itself is not very complicated.
> >
> >
> >
> > library(ggplot2)
> > library(scales)
> > library(dplyr)
> > library(tidyr)
> >
> > dta %>%
> >    select(datetime, B, BX, BZ) %>%
> >    rename(x = datetime, B1 = B, B2 = BX, B3 = BZ) %>%
> >    pivot_longer(
> >      cols = starts_with("B"),
> >      names_to = "variable",
> >      values_to = "value"
> >    ) %>%
> >    ggplot(aes(x, value)) +
> >    geom_line() +
> >    xlab("") + ylab("") +
> >    scale_x_datetime(
> >      breaks = seq(min(x), max(x), by = "days"),
> >      labels = time_format("%a")) +
> >    facet_grid(rows = vars(variable)) +
> >    theme_bw()
> >
> >
> >
> > Hope this helps,
> >
> > Rui Barradas
> >
> > ?s 17:46 de 01/05/20, Ogbos Okike escreveu:
> >> Dear Contributors,
> >> I am trying to do a plot with multiple y-axis on a common x-axis. I have
> >> made some progress but I am having difficulties with datetime
> >> appearing on
> >> x-axis. Instead of as date, it appears as large numbers.
> >>
> >> Sample of my data:
> >> 78 09 28  0 6.7 -40.4 -3.5 2.3 -3.6 278036 5.8 612
> >> 78 09 28  1 5.7 7.3 -4.4 1.4 0.6 261169 4.9 623
> >> 78 09 28  2 5.6 -20.9 -3.6 3.1 -1.8 269323 4.8 625
> >> 78 09 28  3 5.8 -46.9 -3.4 0.2 -3.6 254221 4.7 626
> >> 78 09 28  4 6.2 -41.4 -3.9 1.2 -3.6 248567 4.2 618
> >> 78 09 28  5 6.1 -54.6 -3 1 -4.4 252669 4.4 629
> >> 78 09 28  6 NA NA NA NA NA 220480 4.3 606
> >> 78 09 28  7 7.1 26 -4.8 3.6 2.9 216887 4.4 613
> >> 78 09 28  8 7.1 -21.4 -4.7 3.5 -2.3 249928 4 643
> >> 78 09 28  9 7 -13.6 -2.7 5.2 -1.4 268806 4 656
> >> 78 09 28  10 6.6 -18.5 -3.7 3.6 -1.7 285608 4.2 651
> >> 78 09 28  11 6.7 -6.6 -2.1 4.7 -0.6 280446 4.2 661
> >> 78 09 28  12 6.7 -29.7 -1.9 4 -2.5 281928 4.2 659
> >> 78 09 28  13 6.2 -8.7 -2.1 4.7 -0.8 273465 4.4 651
> >> 78 09 28  14 6.1 31.8 -4.7 0.9 3 212374 4.2 631
> >> 78 09 28  15 5.4 9.3 -4.5 -0.9 0.8 219662 3.9 636
> >> 78 09 28  16 4.9 6.3 -4.5 -0.5 0.5 220610 3.8 628
> >> 78 09 28  17 4.7 4.5 -4.3 -0.6 0.3 214432 3.7 628
> >> 78 09 28  18 4.9 3.1 -4.5 0.3 0.2 202199 3.5 623
> >> 78 09 28  19 5 -13 -4.6 -0.3 -1.1 192859 3.2 619
> >> 78 09 28  20 5.1 -26 -3.5 2 -2 193868 3.1 627
> >> 78 09 28  21 10.1 -37.5 -5.1 4.9 -5.4 284122 5.9 683
> >> 78 09 28  22 8.4 -7.3 -3.6 5.5 -0.8 367499 6.2 694
> >> 78 09 28  23 8.2 -17.7 -4.7 4.4 -2.1 346644 4.9 689
> >> 78 09 29  0 8 6.3 -4.3 5.8 0.8 269569 4.7 708
> >> 78 09 29  1 8.6 35 -3 6.2 4.8 187132 3.2 709
> >> 78 09 29  2 8.1 24.8 -4.1 6 3.3 166644 3.5 689
> >> 78 09 29  3 15.9 29.6 -2.4 9.6 5.6 720902 6.6 866
> >> 78 09 29  4 14.9 18.9 -2.8 13.8 4.8 1587324 8.1 912
> >> 78 09 29  5 14.1 8.2 -10.2 8.3 1.9 1509336 8.1 871
> >> 78 09 29  6 15.6 -2.9 -6.1 11.5 -0.7 968890 7.8 878
> >> 78 09 29  7 19.6 -49.7 1.2 12.1 -14.4 574978 5.4 906
> >> 78 09 29  8 23.5 -44.2 -1.4 11.7 -11.5 287721 4.1 865
> >> 78 09 29  9 25.3 -71.8 -2.4 7.5 -23.9 58521 5.1 859
> >> 78 09 29  10 24.8 -63.1 -2.3 10.9 -22 74956 1.5 821
> >> 78 09 29  11 23.8 -50 -2 15.1 -18.1 64510 3 807
> >> 78 09 29  12 21.4 -34.3 0.1 17.6 -12 50247 2.3 795
> >> 78 09 29  13 18.4 -20 -2 17 -6.3 59939 2.9 802
> >> 78 09 29  14 16.4 -13.9 -2.8 15.6 -3.9 56499 1.4 799
> >> 78 09 29  15 15.3 -7.9 -2 14.9 -2.1 49346 1.3 759
> >> 78 09 29  16 14.1 -0.5 -2.9 13.8 -0.1 57732 0.5 730
> >> 78 09 29  17 12.6 14.7 -3.1 11.5 3.1 44351 1.7 722
> >> 78 09 29  18 11.2 74.3 0.6 2.9 10.4 68224 0.4 712
> >> 78 09 29  19 10.5 60.5 3.2 4 9.1 48082 0.2 706
> >> 78 09 29  20 10 54.7 3.9 4.2 8.1 56404 0.2 710
> >> 78 09 29  21 9.3 61.1 3.4 2.9 8.1 68824 0.2 694
> >> 78 09 29  22 8.6 64.4 2.6 2.7 7.8 44579 0.2 683
> >> 78 09 29  23 7.9 63.3 2 3 7.1 39760 0.2 661
> >> 78 09 30  0 7.3 59.8 2.3 2.8 6.3 NA NA NA
> >> 78 09 30  1 6.6 65.6 1.7 2.1 6 57382 0.5 664
> >> 78 09 30  2 6 70.8 1.3 1.4 5.5 63540 0.5 654
> >> 78 09 30  3 4.4 45.1 1 2.7 2.9 60856 2.5 635
> >> 78 09 30  4 3.6 -28 -1 -0.3 -0.6 80967 1.9 633
> >> 78 09 30  5 4 -47.6 -2.2 -0.1 -2.4 48391 1.2 642
> >> 78 09 30  6 4.1 -28.9 -3.1 -1.4 -1.9 45012 1.1 643
> >> 78 09 30  7 4.2 -42.7 -2.7 1.2 -2.7 45562 1.1 633
> >>   and part of my code is:
> >> Sys.setenv( TZ="GMT" )
> >> dta<-read.table("IFEDIFIG1D",col.names=c("year", "month", "day",
> >> "hour","B","LAT","BX","BZ","BY","SWT","SWD","SW"))
> >> dta$year <- with( dta, ifelse(year < 50, year + 2000, year + 1900))
> >> dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year,
> >> month,day,hour,0,0)))
> >> x =  dta$datetime
> >> B=dta$B
> >> LAT=dta$LAT
> >> BX=dta$BX
> >> BZ=dta$BZ
> >>   par(mfcol = c(3, 1), mar = numeric(4), oma = c(4, 4, 1, 1),
> >>      mgp = c(2, 0.5, 0))#0.5 here controls x-axis margins
> >> plot(x, B,type="l", axes = FALSE)
> >> abline(v=-3)
> >> axis(2L)
> >> plot(x, LAT,type="l", axes = FALSE)
> >> abline(v=-3)
> >> axis(2L)
> >>   plot(x, BX, type="l",axes = FALSE)
> >> abline(v=-3)
> >> axis(1L)
> >> axis(2L)
> >> mtext("A1", side = 1, outer = TRUE, line = 2.2)
> >> mtext("B1", side = 2, outer = TRUE, line = 2.2,at =0.2)
> >> mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.5)
> >> mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.8)
> >>
> >> If I do a simple plot of x and B, the x-axis will be fine, appearing as
> >> Thu, Fri, Sat, Sun.
> >> Please let me know what I am doing wrong with the multiple plot code
> >> above.
> >> I want the same Thu, Fri, Sat, Sun to appear on the common x-axis.
> >>
> >> I am most grateful for your kind response.
> >>
> >> Warmest regards
> >> Ogbos
> >>
> >>     [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From g||ted|||e2014 @end|ng |rom gm@||@com  Sat May  2 04:36:32 2020
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Sat, 2 May 2020 03:36:32 +0100
Subject: [R] Datetime misrepresented on x-axis of a multiple plot: Problem
 Fixd
In-Reply-To: <CA+8X3fWGCtgq7wHC0AvDmgfzbJ4MNgDv62t2ZMT9r6iJf9C9Yg@mail.gmail.com>
References: <CAC8ss33yaDtFY-6+8dFD8jUxks7jtu9HN6n13_CTRKgjHP8vVw@mail.gmail.com>
 <CA+8X3fWGCtgq7wHC0AvDmgfzbJ4MNgDv62t2ZMT9r6iJf9C9Yg@mail.gmail.com>
Message-ID: <CAC8ss31sf+-3pD2F_jBpu5j2G+hq97gc70SMeDtjmmU8Nz_6GQ@mail.gmail.com>

Dear Jim and Rolf Turner,
This is great!!!!!!!!!!!!!!!!
The labels on x-axis are now correctly displayed. Jim, your singular code:
axis.POSIXct(1,dta$datetime,format="%a")
made all the required adjustment, making life quite easy for me.

The abline commands are indeed meant to draw a common vertical line across
the different panels. I have quickly achieved that with:
abline(v=as.POSIXct("1978-09-29 00:00:00"))

I have successfully added the rest of the data, displaying seven/more
variables on a single plot and using some common vertical lines to indicate
the variations in several solar wind plasma data/cosmic rays at Earth at
the time of coronal mass ejections from the sun. It makes an interesting
plot to those in our field.

Confusion or "misleading impressions" do not arise at all as the data used
as well as date plotted are usually online for easy verification by
experts/anyone.

Many thanks for all your contributions.

Warmest regards
Ogbos



On Fri, May 1, 2020 at 11:47 PM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Ogbos,
> The following code may get you close to what you want. I have used the
> names of the columns in "dta" as it is less confusing for me. I think
> you meant to request horizontal ablines as none appeared in the
> example. In order to get the axis tick label "Sun" you would have to
> increase the x axis limits beyond your data.
>
> plot(dta$datetime,dta$B,type="l",axes=FALSE)
> # this is out of range and doesn't appear
> abline(h=-3,col="red")
> axis(2)
> plot(dta$datetime,dta$LAT,type="l",axes=FALSE)
> abline(h=-3,col="red")
> axis(2)
> plot(dta$datetime,dta$BX,type="l",axes=FALSE)
> abline(h=-3,col="red")
> axis(2)
> axis.POSIXct(1,dta$datetime,format="%a")
> mtext("A1", side = 1, outer = TRUE, line = 2.2)
> mtext("B1", side = 2, outer = TRUE, line = 2.2,at =0.2)
> mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.5)
> mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.8)
>
> Jim
>
> On Sat, May 2, 2020 at 2:47 AM Ogbos Okike <giftedlife2014 at gmail.com>
> wrote:
> >
> > Dear Contributors,
> > I am trying to do a plot with multiple y-axis on a common x-axis. I have
> > made some progress but I am having difficulties with datetime appearing
> on
> > x-axis. Instead of as date, it appears as large numbers.
> > ...
> > If I do a simple plot of x and B, the x-axis will be fine, appearing as
> > Thu, Fri, Sat, Sun.
> > Please let me know what I am doing wrong with the multiple plot code
> above.
> > I want the same Thu, Fri, Sat, Sun to appear on the common x-axis.
> >
>

	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Sat May  2 18:43:11 2020
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Sat, 2 May 2020 18:43:11 +0200
Subject: [R] stats:: spline's method could not be monoH.FC
In-Reply-To: <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
Message-ID: <24237.41759.777920.646961@stat.math.ethz.ch>

>>>>> Abby Spurdle 
>>>>>     on Fri, 1 May 2020 17:21:32 +1200 writes:

    > I agree, the documentation gives the impression that stats::spline
    > would allow "monoH.FC".

You are right, this is a lapsus,  thank you for reporting it!

    > My guess is that stats::spline doesn't allow it because the function
    > is designed to return a list with x and y components. This doesn't
    > suit monotonic cubic Hermite splines because the function would also
    > need to return the slopes. Furthermore, additional functions may (or
    > may not) be required depending on what you want to do with x, y and
    > slope vectors.

Well, not quite.  AFAIR, the reasons I did not add the option for
spline()  were mostly

- using  splinefun() is uniformly more flexible than using spline()
  {you can always get an (x,y) coordinate list from
   splinefun()'s result later}

- splinefun() is using's R feature of "(non-trivial) closure", i.e, 
  it returns a *function* containing its own state

- convenience (less work, notably less maintenance burden)

Still, in spite of the above:  If somebody provides minimal patches
(keeping *elegant* code) to allow spline() to also allow method="monoH.FC",
I'd consider applying that.
For now, I'd rather amend the documentation (as Samuel proposes)

    > Note that my package kubik, provides a range of functions for working
    > with cubic Hermite splines.

That's interesting.  For years, I had wanted to add a bit more
functionality to R's builtin (but not widely known !!) package 'splines',
and just today a colleague asked me about spline interpolation
with general 2nd derivative boundary conditions

   s''(x_1) = s2_1,  s''(x_n) = s2_2

generalizing the usual ( "natural" ) boundary conditions

   s''(x_1) = 0,     s''(x_n) = 0

and I'd "hope" it should really only need a few lines of R code
to generalize from natural interpolating splines to such very
slightly more general ones.
(I don't see very quickly how to do this with 'kubik' either,
but I've only looked for 5 minutes).

---

Note that our CRAN package 'cobs'  is really for
(robust, namely quantile) regression splines where the user can also
ask for monotonicity, convexity, concavity of s() and for bound
constraints in different regions
(and the implementation is fast for larger 'n', thanks to using
 sparse matrices)... which seems partly a generalization of  'kubik'
as it's not only (just) for interpolation and still allows many
shape and bound constraints.
 
> citation("cobs")

To cite the cobs package in publications use:

  Pin T. Ng and Martin Maechler (2020). COBS -- Constrained B-splines (Sparse matrix
  based). R package version 1.3-4. URL https://CRAN.R-project.org/package=cobs

Ng P, Maechler M (2007). ?A Fast and Efficient Implementation of Qualitatively
Constrained Quantile Smoothing Splines.? _Statistical Modelling_, *7*(4),
 315-328. <URL:http://smj.sagepub.com/content/7/4/315.abstract>.


From you@r|@|@nou@ @end|ng |rom gm@||@com  Sun May  3 01:30:49 2020
From: you@r|@|@nou@ @end|ng |rom gm@||@com (Yousri Fanous)
Date: Sat, 2 May 2020 19:30:49 -0400
Subject: [R] possible issue with scatterplot function in car package
Message-ID: <CADsEwSdLUbnNjnUYALdycEODBknqd5L1PDEv9WNHbXZtHQ8N-Q@mail.gmail.com>

library (car)

 aa <- data.frame(x=c(2,  5, 6, 7, 8),
+  y=c(5,  10, 9, 12, 11),
+ ch=c("N",  "Q", "R", "S", "T"),
+ stringsAsFactors=FALSE)

scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame = FALSE,regLine=F)

Both x and y boxplots are correct
and in particular the median of the x box is at 6 which is confirmed

> median(aa$x)
[1] 6

Now I do only one addition to the scatterplot: I add xlim
> scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame =
FALSE,regLine=F,xlim=c(0,8))

This causes the boxplot on x-axis to be in error:
1) the lower whisker starts now from zero
2) the median is between 4 and 6 and no longer at 6 as before

> sessionInfo()
R version 3.6.3 (2020-02-29)
 [1] car_3.0-7

	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Sun May  3 01:51:51 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sun, 3 May 2020 11:51:51 +1200
Subject: [R] [FORGED] possible issue with scatterplot function in car
 package
In-Reply-To: <CADsEwSdLUbnNjnUYALdycEODBknqd5L1PDEv9WNHbXZtHQ8N-Q@mail.gmail.com>
References: <CADsEwSdLUbnNjnUYALdycEODBknqd5L1PDEv9WNHbXZtHQ8N-Q@mail.gmail.com>
Message-ID: <a87259ca-a79d-cd4e-4ec2-6f4ad59cc207@auckland.ac.nz>


On 3/05/20 11:30 am, Yousri Fanous wrote:

> library (car)
> 
>   aa <- data.frame(x=c(2,  5, 6, 7, 8),
> +  y=c(5,  10, 9, 12, 11),
> + ch=c("N",  "Q", "R", "S", "T"),
> + stringsAsFactors=FALSE)
> 
> scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame = FALSE,regLine=F)
> 
> Both x and y boxplots are correct
> and in particular the median of the x box is at 6 which is confirmed
> 
>> median(aa$x)
> [1] 6
> 
> Now I do only one addition to the scatterplot: I add xlim
>> scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame =
> FALSE,regLine=F,xlim=c(0,8))
> 
> This causes the boxplot on x-axis to be in error:
> 1) the lower whisker starts now from zero
> 2) the median is between 4 and 6 and no longer at 6 as before
> 
>> sessionInfo()
> R version 3.6.3 (2020-02-29)
>   [1] car_3.0-7

(1) Please present data using dput(); this makes life a lot easier for 
your respondents.  (See posting guide.)

(2) Please do not post in html.  (See posting guide.)

(3) I agree that this looks like a bug.

(4) Email about issues like this should be sent to the maintainer of the 
package in question (see maintainer("car")) and not to r-help.

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From rcoppock @end|ng |rom cox@net  Sun May  3 04:18:53 2020
From: rcoppock @end|ng |rom cox@net (Roger Coppock)
Date: Sat, 2 May 2020 19:18:53 -0700
Subject: [R] CRAN library down? - UPDATE
In-Reply-To: <E17F07D7-3E85-4737-9DB6-EB5C511E619B@cox.net>
References: <E17F07D7-3E85-4737-9DB6-EB5C511E619B@cox.net>
Message-ID: <5AC5E4D9-2D7A-49FD-AC74-475C04BF70A1@cox.net>

Recently a message appears when I try to view the index of the CRAN library.

Warning: unable to access index for repository http://cran.cnr.Berkeley.edu/bin/macosx/contrib/4.0:
  cannot open URL 'http://cran.cnr.Berkeley.edu/bin/macosx/contrib/4.0/PACKAGES'

Does the CRAN library have the Coronavirus?

- -  Roger Coppock (rcoppock at cox.net)

> On May 1, 2020, at 9:24 AM, Roger Coppock <rcoppock at cox.net> wrote:
> 
> After I changed to Version R version 4.0.0 (2020-04-24) -- "Arbor Day" on my MacBook running Mac OS 10.13.6, I lost all my loaded libraries.  Also, the package installer can not contact CRAN libraries either for binaries or sources, to replace the missing loaded libraries.  The package installer can contact "BioConductor", however.  I am now specifically looking for "HURDAT" and "lmtest", which were on CRAN but not "BioConductor".
> 
> - -  Roger Coppock (rcoppock at cox.net)


From cry@n @end|ng |rom b|ngh@mton@edu  Sun May  3 04:49:05 2020
From: cry@n @end|ng |rom b|ngh@mton@edu (Christopher W. Ryan)
Date: Sat, 2 May 2020 22:49:05 -0400
Subject: [R] CRAN library down? - UPDATE
In-Reply-To: <5AC5E4D9-2D7A-49FD-AC74-475C04BF70A1@cox.net>
References: <E17F07D7-3E85-4737-9DB6-EB5C511E619B@cox.net>
 <5AC5E4D9-2D7A-49FD-AC74-475C04BF70A1@cox.net>
Message-ID: <4bb01384-4b0e-ba1a-f7a4-00ef6126e0b0@binghamton.edu>

The message at that URL reads:

CRAN mirror restricted to UC Berkeley

The CRAN mirror at UC Berkeley's College of Natural Resources is no
longer available to off campus users and has been removed from the CRAN
mirror list. The load on our server was too much.

Berkeley folks can continue to access the mirror from on-campus networks
or from within the VPN.

The rest of the R community: Sorry, but you'll have to please choose a
different CRAN mirror.

--Chris Ryan


Roger Coppock wrote:
> Recently a message appears when I try to view the index of the CRAN library.
> 
> Warning: unable to access index for repository http://cran.cnr.Berkeley.edu/bin/macosx/contrib/4.0:
>   cannot open URL 'http://cran.cnr.Berkeley.edu/bin/macosx/contrib/4.0/PACKAGES'
> 
> Does the CRAN library have the Coronavirus?
> 
> - -  Roger Coppock (rcoppock at cox.net)
> 
>> On May 1, 2020, at 9:24 AM, Roger Coppock <rcoppock at cox.net> wrote:
>>
>> After I changed to Version R version 4.0.0 (2020-04-24) -- "Arbor Day" on my MacBook running Mac OS 10.13.6, I lost all my loaded libraries.  Also, the package installer can not contact CRAN libraries either for binaries or sources, to replace the missing loaded libraries.  The package installer can contact "BioConductor", however.  I am now specifically looking for "HURDAT" and "lmtest", which were on CRAN but not "BioConductor".
>>
>> - -  Roger Coppock (rcoppock at cox.net)
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sun May  3 05:10:09 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 02 May 2020 20:10:09 -0700
Subject: [R] CRAN library down? - UPDATE
In-Reply-To: <5AC5E4D9-2D7A-49FD-AC74-475C04BF70A1@cox.net>
References: <E17F07D7-3E85-4737-9DB6-EB5C511E619B@cox.net>
 <5AC5E4D9-2D7A-49FD-AC74-475C04BF70A1@cox.net>
Message-ID: <F5F88764-1703-4670-8AD0-7E6A0A7C31CD@dcn.davis.ca.us>

a) You will help yourself and those who you attempt to communicate with if you learn the terminology correctly as described in the RcInstallation and Administration manual (e.g. RShowDoc("R-admin") or https://cran.r-project.org/doc/manuals/r-release/R-admin.html):

  1) Packages are collections of functions and documentation.
  2) Libraries are the directories _on our computers_ in which we keep packages that we want convenient access to.
  3) Repositories are directories of packages on internet servers from which we typically retrieve packages that we want to keep in our libraries.

b) There are many web servers that mirror the contents of the CRAN repository. They are operated as volunteer contributions to the R community. You have referenced one of them in your error message.

c) If you use your web browser to visit the URL you reported in your post then there is a message returned that explains what happened and offers a suggestion as to next steps you should take.

On May 2, 2020 7:18:53 PM PDT, Roger Coppock <rcoppock at cox.net> wrote:
>Recently a message appears when I try to view the index of the CRAN
>library.
>
>Warning: unable to access index for repository
>http://cran.cnr.Berkeley.edu/bin/macosx/contrib/4.0:
>cannot open URL
>'http://cran.cnr.Berkeley.edu/bin/macosx/contrib/4.0/PACKAGES'
>
>Does the CRAN library have the Coronavirus?
>
>- -  Roger Coppock (rcoppock at cox.net)
>
>> On May 1, 2020, at 9:24 AM, Roger Coppock <rcoppock at cox.net> wrote:
>> 
>> After I changed to Version R version 4.0.0 (2020-04-24) -- "Arbor
>Day" on my MacBook running Mac OS 10.13.6, I lost all my loaded
>libraries.  Also, the package installer can not contact CRAN libraries
>either for binaries or sources, to replace the missing loaded
>libraries.  The package installer can contact "BioConductor", however. 
>I am now specifically looking for "HURDAT" and "lmtest", which were on
>CRAN but not "BioConductor".
>> 
>> - -  Roger Coppock (rcoppock at cox.net)
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From j|ox @end|ng |rom mcm@@ter@c@  Sun May  3 05:47:21 2020
From: j|ox @end|ng |rom mcm@@ter@c@ (Fox, John)
Date: Sun, 3 May 2020 03:47:21 +0000
Subject: [R] possible issue with scatterplot function in car package
In-Reply-To: <31777_1588462276_042NV9Hx024074_CADsEwSdLUbnNjnUYALdycEODBknqd5L1PDEv9WNHbXZtHQ8N-Q@mail.gmail.com>
References: <31777_1588462276_042NV9Hx024074_CADsEwSdLUbnNjnUYALdycEODBknqd5L1PDEv9WNHbXZtHQ8N-Q@mail.gmail.com>
Message-ID: <A26E6ABA-72CB-4DE0-BD6F-AAF0886D4445@mcmaster.ca>


Dear Yousri,

Yes, this is clearly a bug, and almost surely a long-standing one. We'll fix it in the next release of the car package.

BTW, stringsAsFactors defaults to FALSE in R 4.0.0 (and you don't use the ch variable in the example). Also, although it has no bearing on the bug, I'd generally prefer

	scatterplot(y ~ x, data=aa, smooth=FALSE, grid=FALSE, 
            	frame=FALSE, regLine=FALSE, xlim=c(0, 8))

Thank you for the bug report,
 John

  -----------------------------
  John Fox, Professor Emeritus
  McMaster University
  Hamilton, Ontario, Canada
  Web: http::/socserv.mcmaster.ca/jfox

> On May 2, 2020, at 7:30 PM, Yousri Fanous <yousri.fanous at gmail.com> wrote:
> 
> library (car)
> 
> aa <- data.frame(x=c(2,  5, 6, 7, 8),
> +  y=c(5,  10, 9, 12, 11),
> + ch=c("N",  "Q", "R", "S", "T"),
> + stringsAsFactors=FALSE)
> 
> scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame = FALSE,regLine=F)
> 
> Both x and y boxplots are correct
> and in particular the median of the x box is at 6 which is confirmed
> 
>> median(aa$x)
> [1] 6
> 
> Now I do only one addition to the scatterplot: I add xlim
>> scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame =
> FALSE,regLine=F,xlim=c(0,8))
> 
> This causes the boxplot on x-axis to be in error:
> 1) the lower whisker starts now from zero
> 2) the median is between 4 and 6 and no longer at 6 as before
> 
>> sessionInfo()
> R version 3.6.3 (2020-02-29)
> [1] car_3.0-7
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @purd|e@@ @end|ng |rom gm@||@com  Sun May  3 06:15:17 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Sun, 3 May 2020 16:15:17 +1200
Subject: [R] stats:: spline's method could not be monoH.FC
In-Reply-To: <24237.41759.777920.646961@stat.math.ethz.ch>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
Message-ID: <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>

> and just today a colleague asked me about spline interpolation
> with general 2nd derivative boundary conditions
>    s''(x_1) = s2_1,  s''(x_n) = s2_2

It should possible via cubic Hermite splines.
A nontrivial design decision in my package was the computation of
slopes at the endpoints.
(Something which I got wrong, twice...)

My guess is that I could write a function in about 60 to 90 minutes,
including all the testing and calculus.

However, I need to note two things:
(1) Cubic Hermite splines do not have a continuous second derivative.
(2) Specifying the second derivatives (at the endpoints) would prevent
the user from specifying the first derivatives (aka the slopes).

Could you please confirm if the function would still be of interest...?

And completely diverging...
> it returns a *function* containing its own state

I use function objects extensively.
However, I haven't been able to find a definitive guide to terminology.
The word "closure" appears to have a lisp origin, but it usage in R is
a bit grey.

Many of my functions have attributes.
However, I recognize that the use the function environments is more
popular, and has the advantage that the user can take advantage of
lexical scoping, but has the disadvantage that copying function
objects can have unexpected results.

Recently, I've been using the terms "Self-Referencing Function
Objects" and "Functions Bundled with Data", but was wondering if these
terms are sub-optimal...?


From @dr|@n @end|ng |rom tr@p|ett|@org  Sun May  3 10:06:13 2020
From: @dr|@n @end|ng |rom tr@p|ett|@org (Adrian Trapletti)
Date: Sun, 3 May 2020 10:06:13 +0200
Subject: [R] R 4.0.0 with Intel MKL for Windows
Message-ID: <CAFmikf17XhpXNCCy9Xv+gcmSN98=4X85OwXY-9K5c9Xzechshw@mail.gmail.com>

For Windows users, some instructions how to use R 4.0.0 with Intel MKL:

https://linkedin.com/pulse/r-400-intel-mkl-windows-adrian-trapletti

Best Regards
Adrian

Adrian Trapletti

Steinstrasse 9b, 8610 Uster, Switzerland
P +41 44 994 56 30  |  M +41 79 103 71 31
adrian at trapletti.org  |  www.trapletti.org


From you@r|@|@nou@ @end|ng |rom gm@||@com  Sun May  3 10:37:52 2020
From: you@r|@|@nou@ @end|ng |rom gm@||@com (Yousri Fanous)
Date: Sun, 3 May 2020 04:37:52 -0400
Subject: [R] possible issue with scatterplot function in car package
In-Reply-To: <A26E6ABA-72CB-4DE0-BD6F-AAF0886D4445@mcmaster.ca>
References: <31777_1588462276_042NV9Hx024074_CADsEwSdLUbnNjnUYALdycEODBknqd5L1PDEv9WNHbXZtHQ8N-Q@mail.gmail.com>
 <A26E6ABA-72CB-4DE0-BD6F-AAF0886D4445@mcmaster.ca>
Message-ID: <CADsEwSdxFMTpyoHaYm3LiW4aMyxihAvnuse6VNCYVYCt8hg4mQ@mail.gmail.com>

Thank you Professor John for your answer.

As you rightly said I am not using the ch in my example report as it has no
bearing to the issue.
However it is the ch that led me to find the issue.
I was trying to label each point with its corresponding aa$ch value.
I used this code:

scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame = FALSE,regLine=F)
text(aa$x,aa$y, labels=aa$ch,font=1 ,cex=.9,pos=3)

The annotation was correct for 4 points but not for the (2,5) point.
I figured it is because it is close to the margin of the plot hence as a
quick solution I modified xlim to shift the point away from the margin.
This worked for the annotation but eventually led to the issue I described.

Thank you so much for your time

Yousri Fanous

Software Developer
IBM CANADA

On Sat, May 2, 2020 at 11:47 PM Fox, John <jfox at mcmaster.ca> wrote:

>
> Dear Yousri,
>
> Yes, this is clearly a bug, and almost surely a long-standing one. We'll
> fix it in the next release of the car package.
>
> BTW, stringsAsFactors defaults to FALSE in R 4.0.0 (and you don't use the
> ch variable in the example). Also, although it has no bearing on the bug,
> I'd generally prefer
>
>         scatterplot(y ~ x, data=aa, smooth=FALSE, grid=FALSE,
>                 frame=FALSE, regLine=FALSE, xlim=c(0, 8))
>
> Thank you for the bug report,
>  John
>
>   -----------------------------
>   John Fox, Professor Emeritus
>   McMaster University
>   Hamilton, Ontario, Canada
>   Web: http::/socserv.mcmaster.ca/jfox
>
> > On May 2, 2020, at 7:30 PM, Yousri Fanous <yousri.fanous at gmail.com>
> wrote:
> >
> > library (car)
> >
> > aa <- data.frame(x=c(2,  5, 6, 7, 8),
> > +  y=c(5,  10, 9, 12, 11),
> > + ch=c("N",  "Q", "R", "S", "T"),
> > + stringsAsFactors=FALSE)
> >
> > scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame =
> FALSE,regLine=F)
> >
> > Both x and y boxplots are correct
> > and in particular the median of the x box is at 6 which is confirmed
> >
> >> median(aa$x)
> > [1] 6
> >
> > Now I do only one addition to the scatterplot: I add xlim
> >> scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame =
> > FALSE,regLine=F,xlim=c(0,8))
> >
> > This causes the boxplot on x-axis to be in error:
> > 1) the lower whisker starts now from zero
> > 2) the median is between 4 and 6 and no longer at 6 as before
> >
> >> sessionInfo()
> > R version 3.6.3 (2020-02-29)
> > [1] car_3.0-7
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>

	[[alternative HTML version deleted]]


From @purd|e@@ @end|ng |rom gm@||@com  Sun May  3 11:11:00 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Sun, 3 May 2020 21:11:00 +1200
Subject: [R] stats:: spline's method could not be monoH.FC
In-Reply-To: <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
Message-ID: <CAB8pepwv+xPbnHFpv=dxdd5hihe7w-QpWTf_0FmXqRxUD01x1Q@mail.gmail.com>

I just realized that note (2) is not completely correct.
It would be possible to specify both the first and second derivatives
at the endpoints.
However, that would require subdivision of the outermost spline
segments, which increases the complexity of the algorithm.


From @purd|e@@ @end|ng |rom gm@||@com  Sun May  3 12:19:24 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Sun, 3 May 2020 22:19:24 +1200
Subject: [R] stats:: spline's method could not be monoH.FC
In-Reply-To: <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
Message-ID: <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>

Hi Mark,

The article is good.
However, there's still some grey areas.

The documentation for base::typeof equates a closure with a function.
However, the article defines a closure as a function together with an
environment.
A very minor difference I know, but it creates the problem that the
word closure is used inconsistently.
And that's without even getting into "frames".

Equating a closure with a function (only), is perhaps misleading...???

Also, re-iterating I use function objects with attributes.
(I prefer this approach, because I can make copies of function
objects, and if necessary modify them).
My guess is that doesn't meet the definition of an R closure (if you
ignore the environment)...???, and it's unclear whether it meets the
definition of a closure, more generally...???

So, I've still got the same problem, of how to refer to functions that
have *either* attributes *or* environments, containing data.
Maybe I should just stick to "Self-Referencing Function Objects" and
"Functions Bundled with Data"...???

One last thing, the last time I read S4 documentation, I couldn't tell
if it was possible to have S4-based function objects, and if so, could
the body of the S4-based function object (while being called) access
it's own slots...???


On Sun, May 3, 2020 at 4:27 PM Mark Leeds <markleeds2 at gmail.com> wrote:
>
> Abby: Here is an article on environments/closures which might be useful to you. I was reviewing environments recently and it
> was a clear explanation of how environments/closures work in R. Even though it's from 2000, I'm pretty certain that everything
> said in it still holds.


From m@rk|eed@2 @end|ng |rom gm@||@com  Sun May  3 16:22:08 2020
From: m@rk|eed@2 @end|ng |rom gm@||@com (Mark Leeds)
Date: Sun, 3 May 2020 10:22:08 -0400
Subject: [R] stats:: spline's method could not be monoH.FC
In-Reply-To: <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
Message-ID: <CAHz+bWZi28R6YTJc0DgmJGxFBHZ2DBv2Oa4zqhFvB6iFun_zBw@mail.gmail.com>

Hi Abby: your questions are good but I can't help because I don't need to
actually use them
to that level of detail. I only have to understand it in its generalities.
Hopefully, Martin
or one of the other R-core people or guRus can answer your questions in a
definitive and clear
way.

But, as far as the term closure, I think that they referring to the
environment that
a function looks in after its looks in its own environment. So, if you
defined
a function say, foo, in the global environment, then the foo's closure is
the global
environment because, when finding the values of variables,  foo will first
look in its
environment  and then its closure. So, closure is just an environment
that's the parent
of a function's environment. It answers the question of where a function
looks next
when it can't find the values of some of the variables that it needs to
evaluate.

I'd also love to hear an explanation of "frame' because that term has
always confused me
I think ( emphasis on think ), when a function, foo,  gets called, it
creates a new
environment to evaluate its arguments and this new  environment of foo is
called a "frame".
But where is it and what is its parent ? Is "frame"  just a fancy term for
the
function's own environment ?  or is  the function's own environment the
parent of its evaluation
frame I'm not sure.

I have other documents that may help ( I recently did a document gathering
but I only read
the one I sent you )  but I think it's better to wait to see if anyone can
help here first. Then,
if not, I can send you some  other things. Oh, the slot question was also
interesting but I've never
used S4. Also, this list has gotten thinner over the years so another place
to ask is of course
stackoverflow.

























On Sun, May 3, 2020 at 6:19 AM Abby Spurdle <spurdle.a at gmail.com> wrote:

> Hi Mark,
>
> The article is good.
> However, there's still some grey areas.
>
> The documentation for base::typeof equates a closure with a function.
> However, the article defines a closure as a function together with an
> environment.
> A very minor difference I know, but it creates the problem that the
> word closure is used inconsistently.
> And that's without even getting into "frames".
>
> Equating a closure with a function (only), is perhaps misleading...???
>
> Also, re-iterating I use function objects with attributes.
> (I prefer this approach, because I can make copies of function
> objects, and if necessary modify them).
> My guess is that doesn't meet the definition of an R closure (if you
> ignore the environment)...???, and it's unclear whether it meets the
> definition of a closure, more generally...???
>
> So, I've still got the same problem, of how to refer to functions that
> have *either* attributes *or* environments, containing data.
> Maybe I should just stick to "Self-Referencing Function Objects" and
> "Functions Bundled with Data"...???
>
> One last thing, the last time I read S4 documentation, I couldn't tell
> if it was possible to have S4-based function objects, and if so, could
> the body of the S4-based function object (while being called) access
> it's own slots...???
>
>
> On Sun, May 3, 2020 at 4:27 PM Mark Leeds <markleeds2 at gmail.com> wrote:
> >
> > Abby: Here is an article on environments/closures which might be useful
> to you. I was reviewing environments recently and it
> > was a clear explanation of how environments/closures work in R. Even
> though it's from 2000, I'm pretty certain that everything
> > said in it still holds.
>

	[[alternative HTML version deleted]]


From t|omby @end|ng |rom m@||@@mu@edu  Sun May  3 07:39:58 2020
From: t|omby @end|ng |rom m@||@@mu@edu (Fomby, Tom)
Date: Sun, 3 May 2020 05:39:58 +0000
Subject: [R] Question about "sample" function and inconsistent results I am
 getting across machines.
Message-ID: <c0c8bafef4fe4122aae2614f15cb4574@mail.smu.edu>

Please consider the following code:

set.seed(1)

train.index = sample(181,150)
head(train.index)
# [1]  49  67 103 162  36 159  Result from my ASUS computer
#
# [1]  68 167 129 162 43 14  Result from my wife's HP Pavilion computer

In both cases, version 3.6.3 of R are being used.

In addition, of the 20 students in my Predictive Analytics class, 14 got the first result while 6 got the latter result.  These results do not seem to be specific to MAC (OS) versus PC (Windows).  In several cases, students using 3.6.3 got differing results.  This makes grading of homework challenging not knowing which partitions of the data are being used by the student.

Thank you for considering my question.

Sincerely,

Tom Fomby

Professor of Economics

SMU

Dallas, TX 75275

tfomby at smu.edu


	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Sun May  3 21:32:54 2020
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sun, 3 May 2020 15:32:54 -0400
Subject: [R] 
 Question about "sample" function and inconsistent results I am
 getting across machines.
In-Reply-To: <c0c8bafef4fe4122aae2614f15cb4574@mail.smu.edu>
References: <c0c8bafef4fe4122aae2614f15cb4574@mail.smu.edu>
Message-ID: <d100571a-c498-6574-2232-17b685336ad4@gmail.com>

On 03/05/2020 1:39 a.m., Fomby, Tom wrote:
> Please consider the following code:
> 
> set.seed(1)
> 
> train.index = sample(181,150)
> head(train.index)
> # [1]  49  67 103 162  36 159  Result from my ASUS computer
> #
> # [1]  68 167 129 162 43 14  Result from my wife's HP Pavilion computer
> 
> In both cases, version 3.6.3 of R are being used.
> 
> In addition, of the 20 students in my Predictive Analytics class, 14 got the first result while 6 got the latter result.  These results do not seem to be specific to MAC (OS) versus PC (Windows).  In several cases, students using 3.6.3 got differing results.  This makes grading of homework challenging not knowing which partitions of the data are being used by the student.
> 
> Thank you for considering my question.

Likely some of you are storing and restoring workspaces, and have been 
doing so for a long time.  If you type

RNGkind()

what you should see is

[1] "Mersenne-Twister" "Inversion"        "Rejection"

but if the .Random.seed is restored from an old session, you might see

[1] "Mersenne-Twister" "Inversion"        "Rounding"

The latter uses the buggy version of sample().  Those users should run

RNGkind(sample.kind = "Rejection")

to start using the corrected sampling algorithm.  (The default was 
changed in R 3.6.0, but if you saved your seed from a previous version, 
you'd get the old sampler).

They should also stop reloading old workspaces, but that's another 
discussion.

Duncan Murdoch


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sun May  3 21:33:32 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sun, 03 May 2020 12:33:32 -0700
Subject: [R] 
 Question about "sample" function and inconsistent results I am
 getting across machines.
In-Reply-To: <c0c8bafef4fe4122aae2614f15cb4574@mail.smu.edu>
References: <c0c8bafef4fe4122aae2614f15cb4574@mail.smu.edu>
Message-ID: <4A5DEF2D-099F-4846-85E8-DE905517021B@dcn.davis.ca.us>

It is a lot easier from this side of the conversation to view skeptically the claim that all of these installations of R are using the same version than that the software seed has started behaving randomly within the same version of R.

On May 2, 2020 10:39:58 PM PDT, "Fomby, Tom" <tfomby at mail.smu.edu> wrote:
>Please consider the following code:
>
>set.seed(1)
>
>train.index = sample(181,150)
>head(train.index)
># [1]  49  67 103 162  36 159  Result from my ASUS computer
>#
># [1]  68 167 129 162 43 14  Result from my wife's HP Pavilion computer
>
>In both cases, version 3.6.3 of R are being used.
>
>In addition, of the 20 students in my Predictive Analytics class, 14
>got the first result while 6 got the latter result.  These results do
>not seem to be specific to MAC (OS) versus PC (Windows).  In several
>cases, students using 3.6.3 got differing results.  This makes grading
>of homework challenging not knowing which partitions of the data are
>being used by the student.
>
>Thank you for considering my question.
>
>Sincerely,
>
>Tom Fomby
>
>Professor of Economics
>
>SMU
>
>Dallas, TX 75275
>
>tfomby at smu.edu
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From murdoch@dunc@n @end|ng |rom gm@||@com  Sun May  3 21:36:50 2020
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sun, 3 May 2020 15:36:50 -0400
Subject: [R] 
 Question about "sample" function and inconsistent results I am
 getting across machines.
In-Reply-To: <d100571a-c498-6574-2232-17b685336ad4@gmail.com>
References: <c0c8bafef4fe4122aae2614f15cb4574@mail.smu.edu>
 <d100571a-c498-6574-2232-17b685336ad4@gmail.com>
Message-ID: <266325cd-a7ee-1e69-3693-f81e6802c464@gmail.com>

I just tried both versions, and it's the ASUS that's using the buggy old 
algorithm.

Duncan Murdoch

On 03/05/2020 3:32 p.m., Duncan Murdoch wrote:
> On 03/05/2020 1:39 a.m., Fomby, Tom wrote:
>> Please consider the following code:
>>
>> set.seed(1)
>>
>> train.index = sample(181,150)
>> head(train.index)
>> # [1]  49  67 103 162  36 159  Result from my ASUS computer
>> #
>> # [1]  68 167 129 162 43 14  Result from my wife's HP Pavilion computer
>>
>> In both cases, version 3.6.3 of R are being used.
>>
>> In addition, of the 20 students in my Predictive Analytics class, 14 got the first result while 6 got the latter result.  These results do not seem to be specific to MAC (OS) versus PC (Windows).  In several cases, students using 3.6.3 got differing results.  This makes grading of homework challenging not knowing which partitions of the data are being used by the student.
>>
>> Thank you for considering my question.
> 
> Likely some of you are storing and restoring workspaces, and have been
> doing so for a long time.  If you type
> 
> RNGkind()
> 
> what you should see is
> 
> [1] "Mersenne-Twister" "Inversion"        "Rejection"
> 
> but if the .Random.seed is restored from an old session, you might see
> 
> [1] "Mersenne-Twister" "Inversion"        "Rounding"
> 
> The latter uses the buggy version of sample().  Those users should run
> 
> RNGkind(sample.kind = "Rejection")
> 
> to start using the corrected sampling algorithm.  (The default was
> changed in R 3.6.0, but if you saved your seed from a previous version,
> you'd get the old sampler).
> 
> They should also stop reloading old workspaces, but that's another
> discussion.
> 
> Duncan Murdoch
>


From |br3524 @end|ng |rom gm@||@com  Sat May  2 18:44:45 2020
From: |br3524 @end|ng |rom gm@||@com (Abraham Rammaha)
Date: Sat, 2 May 2020 11:44:45 -0500
Subject: [R] Is it possible to reopen event in Summer of St. Louis. I am
 interested!
Message-ID: <CAOSJU8aOkRaCD0u0q46ZCQi4rd+D_1uyoPzQ5YvTkOR1OyqoNg@mail.gmail.com>

This is Ibrahim Rammaha. I am not a SLU student but interested in useR!
2021 in St. Louis. How can I register and volunteer/help with event? Can it
be a physical and free virtual conference? I think coronavirus shouldn't
affect the conference if it's online conference

	[[alternative HTML version deleted]]


From m@rk|eed@2 @end|ng |rom gm@||@com  Sun May  3 06:27:35 2020
From: m@rk|eed@2 @end|ng |rom gm@||@com (Mark Leeds)
Date: Sun, 3 May 2020 00:27:35 -0400
Subject: [R] stats:: spline's method could not be monoH.FC
In-Reply-To: <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
Message-ID: <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>

Abby: Here is an article on environments/closures which might be useful to
you. I was reviewing environments recently and it
was a clear explanation of how environments/closures work in R. Even though
it's from 2000, I'm pretty certain that everything
said in it still holds.











On Sun, May 3, 2020 at 12:16 AM Abby Spurdle <spurdle.a at gmail.com> wrote:

> > and just today a colleague asked me about spline interpolation
> > with general 2nd derivative boundary conditions
> >    s''(x_1) = s2_1,  s''(x_n) = s2_2
>
> It should possible via cubic Hermite splines.
> A nontrivial design decision in my package was the computation of
> slopes at the endpoints.
> (Something which I got wrong, twice...)
>
> My guess is that I could write a function in about 60 to 90 minutes,
> including all the testing and calculus.
>
> However, I need to note two things:
> (1) Cubic Hermite splines do not have a continuous second derivative.
> (2) Specifying the second derivatives (at the endpoints) would prevent
> the user from specifying the first derivatives (aka the slopes).
>
> Could you please confirm if the function would still be of interest...?
>
> And completely diverging...
> > it returns a *function* containing its own state
>
> I use function objects extensively.
> However, I haven't been able to find a definitive guide to terminology.
> The word "closure" appears to have a lisp origin, but it usage in R is
> a bit grey.
>
> Many of my functions have attributes.
> However, I recognize that the use the function environments is more
> popular, and has the advantage that the user can take advantage of
> lexical scoping, but has the disadvantage that copying function
> objects can have unexpected results.
>
> Recently, I've been using the terms "Self-Referencing Function
> Objects" and "Functions Bundled with Data", but was wondering if these
> terms are sub-optimal...?
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: ih_gent_lexical.pdf
Type: application/pdf
Size: 220482 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200503/fe569c8d/attachment.pdf>

From t|omby @end|ng |rom m@||@@mu@edu  Sun May  3 21:43:05 2020
From: t|omby @end|ng |rom m@||@@mu@edu (Fomby, Tom)
Date: Sun, 3 May 2020 19:43:05 +0000
Subject: [R] 
 Question about "sample" function and inconsistent results I am
 getting across machines.
In-Reply-To: <d100571a-c498-6574-2232-17b685336ad4@gmail.com>
References: <c0c8bafef4fe4122aae2614f15cb4574@mail.smu.edu>,
 <d100571a-c498-6574-2232-17b685336ad4@gmail.com>
Message-ID: <6355fc0de4614a4493ac5c64d4803c29@mail.smu.edu>


Dear Duncan,

OK, I will certainly ask my students to download the most recent version of Basic R at the first of each semester and, just to be safe, include the RNGkind(sample.kind="Rejection") command before the students get started on the data partitioning part of their exercise using the sample function.

By the way, how is it that one can take a membership in the R community so as to provide support for volunteers like yourself.

Thank you,

Tom Fomby

Department of Economics

SMU

Dallas, TX 75275


________________________________
From: Duncan Murdoch <murdoch.duncan at gmail.com>
Sent: Sunday, May 3, 2020 2:32 PM
To: Fomby, Tom; r-help at R-project.org
Subject: Re: [R] Question about "sample" function and inconsistent results I am getting across machines.

On 03/05/2020 1:39 a.m., Fomby, Tom wrote:
> Please consider the following code:
>
> set.seed(1)
>
> train.index = sample(181,150)
> head(train.index)
> # [1]  49  67 103 162  36 159  Result from my ASUS computer
> #
> # [1]  68 167 129 162 43 14  Result from my wife's HP Pavilion computer
>
> In both cases, version 3.6.3 of R are being used.
>
> In addition, of the 20 students in my Predictive Analytics class, 14 got the first result while 6 got the latter result.  These results do not seem to be specific to MAC (OS) versus PC (Windows).  In several cases, students using 3.6.3 got differing results.  This makes grading of homework challenging not knowing which partitions of the data are being used by the student.
>
> Thank you for considering my question.

Likely some of you are storing and restoring workspaces, and have been
doing so for a long time.  If you type

RNGkind()

what you should see is

[1] "Mersenne-Twister" "Inversion"        "Rejection"

but if the .Random.seed is restored from an old session, you might see

[1] "Mersenne-Twister" "Inversion"        "Rounding"

The latter uses the buggy version of sample().  Those users should run

RNGkind(sample.kind = "Rejection")

to start using the corrected sampling algorithm.  (The default was
changed in R 3.6.0, but if you saved your seed from a previous version,
you'd get the old sampler).

They should also stop reloading old workspaces, but that's another
discussion.

Duncan Murdoch

	[[alternative HTML version deleted]]


From t|omby @end|ng |rom m@||@@mu@edu  Sun May  3 21:49:55 2020
From: t|omby @end|ng |rom m@||@@mu@edu (Fomby, Tom)
Date: Sun, 3 May 2020 19:49:55 +0000
Subject: [R] 
 Question about "sample" function and inconsistent results I am
 getting across machines.
In-Reply-To: <266325cd-a7ee-1e69-3693-f81e6802c464@gmail.com>
References: <c0c8bafef4fe4122aae2614f15cb4574@mail.smu.edu>
 <d100571a-c498-6574-2232-17b685336ad4@gmail.com>,
 <266325cd-a7ee-1e69-3693-f81e6802c464@gmail.com>
Message-ID: <b872ca4d6ef04bbd92e446644d80a895@mail.smu.edu>

Thank you so much, Duncan.

Out of the 20 students in my class, evidently 6 out of the 20 have been using the buggy version of sample().  I am so appreciative that you have helped me get a grip on things.  I was tired of having two keys to my homework exercises.  Amazing that your were able to trace the version of sample() in my ASUS computer.  Me running on 3.6.3 did not fix things because of its determined adherence to the buggy version.

Much appreciation,

Tom Fomby

Department of Economics

SMU

________________________________
From: Duncan Murdoch <murdoch.duncan at gmail.com>
Sent: Sunday, May 3, 2020 2:36:50 PM
To: Fomby, Tom; r-help at R-project.org
Subject: Re: [R] Question about "sample" function and inconsistent results I am getting across machines.

I just tried both versions, and it's the ASUS that's using the buggy old
algorithm.

Duncan Murdoch

On 03/05/2020 3:32 p.m., Duncan Murdoch wrote:
> On 03/05/2020 1:39 a.m., Fomby, Tom wrote:
>> Please consider the following code:
>>
>> set.seed(1)
>>
>> train.index = sample(181,150)
>> head(train.index)
>> # [1]  49  67 103 162  36 159  Result from my ASUS computer
>> #
>> # [1]  68 167 129 162 43 14  Result from my wife's HP Pavilion computer
>>
>> In both cases, version 3.6.3 of R are being used.
>>
>> In addition, of the 20 students in my Predictive Analytics class, 14 got the first result while 6 got the latter result.  These results do not seem to be specific to MAC (OS) versus PC (Windows).  In several cases, students using 3.6.3 got differing results.  This makes grading of homework challenging not knowing which partitions of the data are being used by the student.
>>
>> Thank you for considering my question.
>
> Likely some of you are storing and restoring workspaces, and have been
> doing so for a long time.  If you type
>
> RNGkind()
>
> what you should see is
>
> [1] "Mersenne-Twister" "Inversion"        "Rejection"
>
> but if the .Random.seed is restored from an old session, you might see
>
> [1] "Mersenne-Twister" "Inversion"        "Rounding"
>
> The latter uses the buggy version of sample().  Those users should run
>
> RNGkind(sample.kind = "Rejection")
>
> to start using the corrected sampling algorithm.  (The default was
> changed in R 3.6.0, but if you saved your seed from a previous version,
> you'd get the old sampler).
>
> They should also stop reloading old workspaces, but that's another
> discussion.
>
> Duncan Murdoch
>


	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Sun May  3 21:56:08 2020
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sun, 3 May 2020 15:56:08 -0400
Subject: [R] 
 Question about "sample" function and inconsistent results I am
 getting across machines.
In-Reply-To: <6355fc0de4614a4493ac5c64d4803c29@mail.smu.edu>
References: <c0c8bafef4fe4122aae2614f15cb4574@mail.smu.edu>
 <d100571a-c498-6574-2232-17b685336ad4@gmail.com>
 <6355fc0de4614a4493ac5c64d4803c29@mail.smu.edu>
Message-ID: <afe29b94-2f2a-290b-7aa4-11e60d89e9e9@gmail.com>

On 03/05/2020 3:43 p.m., Fomby, Tom wrote:
> 
> Dear Duncan,
> 
> OK, I will certainly ask my students to download the most recent version 
> of Basic R at the first of each semester?and, just to be safe, include 
> the RNGkind(sample.kind="Rejection") command before the students get 
> started on the data partitioning part of their exercise using the sample 
> function.

Actually, it would probably be a better idea to say

RNGkind(kind = "default", normal.kind = "default", sample.kind = "default")

in case bugs are found in any of the current algorithms and they change 
again.

> 
> By the way, how is it that one can take a membership in the R community 
> so as to provide support for volunteers like yourself.

The R Foundation accepts donations to become a "Supporting Member"; see 
here:  https://www.r-project.org/foundation/donors.html.  They sponsor 
various events, so that is one way.  There is probably also a local user 
group somewhere near you that would appreciate contributions of some 
sort.  There's a list of those here: 
https://blog.revolutionanalytics.com/local-r-groups.html, and another 
one here:  https://www.meetup.com/pro/r-user-groups/.  (I haven't 
checked how similar those two lists are.)

Duncan Murdoch


> 
> Thank you,
> 
> Tom Fomby
> 
> Department of Economics
> 
> SMU
> 
> Dallas, TX 75275
> 
> 
> 
> ------------------------------------------------------------------------
> *From:* Duncan Murdoch <murdoch.duncan at gmail.com>
> *Sent:* Sunday, May 3, 2020 2:32 PM
> *To:* Fomby, Tom; r-help at R-project.org
> *Subject:* Re: [R] Question about "sample" function and inconsistent 
> results I am getting across machines.
> On 03/05/2020 1:39 a.m., Fomby, Tom wrote:
>> Please consider the following code:
>> 
>> set.seed(1)
>> 
>> train.index = sample(181,150)
>> head(train.index)
>> # [1]? 49? 67 103 162? 36 159? Result from my ASUS computer
>> #
>> # [1]? 68 167 129 162 43 14? Result from my wife's HP Pavilion computer
>> 
>> In both cases, version 3.6.3 of R are being used.
>> 
>> In addition, of the 20 students in my Predictive Analytics class, 14 got the first result while 6 got the latter result.? These results do not seem to be specific to MAC (OS) versus PC (Windows).? In several cases, students using 3.6.3 got differing results. This makes grading of homework challenging not knowing which partitions 
> of the data are being used by the student.
>> 
>> Thank you for considering my question.
> 
> Likely some of you are storing and restoring workspaces, and have been
> doing so for a long time.? If you type
> 
> RNGkind()
> 
> what you should see is
> 
> [1] "Mersenne-Twister" "Inversion"??????? "Rejection"
> 
> but if the .Random.seed is restored from an old session, you might see
> 
> [1] "Mersenne-Twister" "Inversion"??????? "Rounding"
> 
> The latter uses the buggy version of sample().? Those users should run
> 
> RNGkind(sample.kind = "Rejection")
> 
> to start using the corrected sampling algorithm.? (The default was
> changed in R 3.6.0, but if you saved your seed from a previous version,
> you'd get the old sampler).
> 
> They should also stop reloading old workspaces, but that's another
> discussion.
> 
> Duncan Murdoch


From m@rk|eed@2 @end|ng |rom gm@||@com  Sun May  3 21:56:51 2020
From: m@rk|eed@2 @end|ng |rom gm@||@com (Mark Leeds)
Date: Sun, 3 May 2020 15:56:51 -0400
Subject: [R] Fwd:  stats:: spline's method could not be monoH.FC
In-Reply-To: <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
Message-ID: <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>

Abby: Just one other thing. A friend of mine recommended reading the
R-language manual which I haven't read in many years.
I don't  know if it's because I'm more expeRienced or just plain oldeR but
I started it today and it's not nearly as daunting as I remember it
being in the past. It's going to take me some time but  I highly recommend
checking it out if you haven't already. Maybe it's like
a fine wine and just gets better with age !!!!!!! Good luck.


Mark





---------- Forwarded message ---------
From: Abby Spurdle <spurdle.a at gmail.com>
Date: Sun, May 3, 2020 at 6:19 AM
Subject: Re: [R] stats:: spline's method could not be monoH.FC
To: Mark Leeds <markleeds2 at gmail.com>
Cc: Martin Maechler <maechler at stat.math.ethz.ch>, Samuel Granjeaud
IR/Inserm <samuel.granjeaud at inserm.fr>, r-help <r-help at r-project.org>


Hi Mark,

The article is good.
However, there's still some grey areas.

The documentation for base::typeof equates a closure with a function.
However, the article defines a closure as a function together with an
environment.
A very minor difference I know, but it creates the problem that the
word closure is used inconsistently.
And that's without even getting into "frames".

Equating a closure with a function (only), is perhaps misleading...???

Also, re-iterating I use function objects with attributes.
(I prefer this approach, because I can make copies of function
objects, and if necessary modify them).
My guess is that doesn't meet the definition of an R closure (if you
ignore the environment)...???, and it's unclear whether it meets the
definition of a closure, more generally...???

So, I've still got the same problem, of how to refer to functions that
have *either* attributes *or* environments, containing data.
Maybe I should just stick to "Self-Referencing Function Objects" and
"Functions Bundled with Data"...???

One last thing, the last time I read S4 documentation, I couldn't tell
if it was possible to have S4-based function objects, and if so, could
the body of the S4-based function object (while being called) access
it's own slots...???


On Sun, May 3, 2020 at 4:27 PM Mark Leeds <markleeds2 at gmail.com> wrote:
>
> Abby: Here is an article on environments/closures which might be useful
to you. I was reviewing environments recently and it
> was a clear explanation of how environments/closures work in R. Even
though it's from 2000, I'm pretty certain that everything
> said in it still holds.

	[[alternative HTML version deleted]]


From t|omby @end|ng |rom m@||@@mu@edu  Sun May  3 21:58:50 2020
From: t|omby @end|ng |rom m@||@@mu@edu (Fomby, Tom)
Date: Sun, 3 May 2020 19:58:50 +0000
Subject: [R] 
 Question about "sample" function and inconsistent results I am
 getting across machines.
In-Reply-To: <afe29b94-2f2a-290b-7aa4-11e60d89e9e9@gmail.com>
References: <c0c8bafef4fe4122aae2614f15cb4574@mail.smu.edu>
 <d100571a-c498-6574-2232-17b685336ad4@gmail.com>
 <6355fc0de4614a4493ac5c64d4803c29@mail.smu.edu>,
 <afe29b94-2f2a-290b-7aa4-11e60d89e9e9@gmail.com>
Message-ID: <526d93f307204a358daa4b1a2c0b6d9c@mail.smu.edu>

Thank you so much Duncan.  I will pitch in.  Tom


________________________________
From: Duncan Murdoch <murdoch.duncan at gmail.com>
Sent: Sunday, May 3, 2020 2:56 PM
To: Fomby, Tom; r-help at R-project.org
Subject: Re: [R] Question about "sample" function and inconsistent results I am getting across machines.

On 03/05/2020 3:43 p.m., Fomby, Tom wrote:
>
> Dear Duncan,
>
> OK, I will certainly ask my students to download the most recent version
> of Basic R at the first of each semester and, just to be safe, include
> the RNGkind(sample.kind="Rejection") command before the students get
> started on the data partitioning part of their exercise using the sample
> function.

Actually, it would probably be a better idea to say

RNGkind(kind = "default", normal.kind = "default", sample.kind = "default")

in case bugs are found in any of the current algorithms and they change
again.

>
> By the way, how is it that one can take a membership in the R community
> so as to provide support for volunteers like yourself.

The R Foundation accepts donations to become a "Supporting Member"; see
here:  https://www.r-project.org/foundation/donors.html.  They sponsor
various events, so that is one way.  There is probably also a local user
group somewhere near you that would appreciate contributions of some
sort.  There's a list of those here:
https://blog.revolutionanalytics.com/local-r-groups.html, and another
one here:  https://www.meetup.com/pro/r-user-groups/.  (I haven't
checked how similar those two lists are.)

Duncan Murdoch


>
> Thank you,
>
> Tom Fomby
>
> Department of Economics
>
> SMU
>
> Dallas, TX 75275
>
>
>
> ------------------------------------------------------------------------
> *From:* Duncan Murdoch <murdoch.duncan at gmail.com>
> *Sent:* Sunday, May 3, 2020 2:32 PM
> *To:* Fomby, Tom; r-help at R-project.org
> *Subject:* Re: [R] Question about "sample" function and inconsistent
> results I am getting across machines.
> On 03/05/2020 1:39 a.m., Fomby, Tom wrote:
>> Please consider the following code:
>>
>> set.seed(1)
>>
>> train.index = sample(181,150)
>> head(train.index)
>> # [1]  49  67 103 162  36 159  Result from my ASUS computer
>> #
>> # [1]  68 167 129 162 43 14  Result from my wife's HP Pavilion computer
>>
>> In both cases, version 3.6.3 of R are being used.
>>
>> In addition, of the 20 students in my Predictive Analytics class, 14 got the first result while 6 got the latter result.  These results do not seem to be specific to MAC (OS) versus PC (Windows).  In several cases, students using 3.6.3 got differing results. This makes grading of homework challenging not knowing which partitions
> of the data are being used by the student.
>>
>> Thank you for considering my question.
>
> Likely some of you are storing and restoring workspaces, and have been
> doing so for a long time.  If you type
>
> RNGkind()
>
> what you should see is
>
> [1] "Mersenne-Twister" "Inversion"        "Rejection"
>
> but if the .Random.seed is restored from an old session, you might see
>
> [1] "Mersenne-Twister" "Inversion"        "Rounding"
>
> The latter uses the buggy version of sample().  Those users should run
>
> RNGkind(sample.kind = "Rejection")
>
> to start using the corrected sampling algorithm.  (The default was
> changed in R 3.6.0, but if you saved your seed from a previous version,
> you'd get the old sampler).
>
> They should also stop reloading old workspaces, but that's another
> discussion.
>
> Duncan Murdoch


	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Mon May  4 04:54:12 2020
From: j|ox @end|ng |rom mcm@@ter@c@ (Fox, John)
Date: Mon, 4 May 2020 02:54:12 +0000
Subject: [R] possible issue with scatterplot function in car package
In-Reply-To: <CADsEwSdxFMTpyoHaYm3LiW4aMyxihAvnuse6VNCYVYCt8hg4mQ@mail.gmail.com>
References: <31777_1588462276_042NV9Hx024074_CADsEwSdLUbnNjnUYALdycEODBknqd5L1PDEv9WNHbXZtHQ8N-Q@mail.gmail.com>
 <A26E6ABA-72CB-4DE0-BD6F-AAF0886D4445@mcmaster.ca>
 <CADsEwSdxFMTpyoHaYm3LiW4aMyxihAvnuse6VNCYVYCt8hg4mQ@mail.gmail.com>
Message-ID: <55CFE3E3-39A1-4217-B6B1-B8FA9C5D50CF@mcmaster.ca>

Dear Yousri,

The problem with scatterplot() is now fixed in the development version 3.0-8 of the car package on R-Forge, which eventually will be submitted to CRAN. Until then, you can install the package via install.packages("car", repos="http://R-Forge.R-project.org")

Thanks again for the bug report,
 John

> On May 3, 2020, at 4:37 AM, Yousri Fanous <yousri.fanous at gmail.com> wrote:
> 
> Thank you Professor John for your answer.
> 
> As you rightly said I am not using the ch in my example report as it has no bearing to the issue.
> However it is the ch that led me to find the issue.
> I was trying to label each point with its corresponding aa$ch value.
> I used this code:
> 
> scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame = FALSE,regLine=F)
> text(aa$x,aa$y, labels=aa$ch,font=1 ,cex=.9,pos=3)
> 
> The annotation was correct for 4 points but not for the (2,5) point.
> I figured it is because it is close to the margin of the plot hence as a quick solution I modified xlim to shift the point away from the margin.
> This worked for the annotation but eventually led to the issue I described.
> 
> Thank you so much for your time
> 
> Yousri Fanous
> 
> Software Developer
> IBM CANADA
> 
> On Sat, May 2, 2020 at 11:47 PM Fox, John <jfox at mcmaster.ca> wrote:
> 
> Dear Yousri,
> 
> Yes, this is clearly a bug, and almost surely a long-standing one. We'll fix it in the next release of the car package.
> 
> BTW, stringsAsFactors defaults to FALSE in R 4.0.0 (and you don't use the ch variable in the example). Also, although it has no bearing on the bug, I'd generally prefer
> 
>         scatterplot(y ~ x, data=aa, smooth=FALSE, grid=FALSE, 
>                 frame=FALSE, regLine=FALSE, xlim=c(0, 8))
> 
> Thank you for the bug report,
>  John
> 
>   -----------------------------
>   John Fox, Professor Emeritus
>   McMaster University
>   Hamilton, Ontario, Canada
>   Web: http::/socserv.mcmaster.ca/jfox
> 
> > On May 2, 2020, at 7:30 PM, Yousri Fanous <yousri.fanous at gmail.com> wrote:
> > 
> > library (car)
> > 
> > aa <- data.frame(x=c(2,  5, 6, 7, 8),
> > +  y=c(5,  10, 9, 12, 11),
> > + ch=c("N",  "Q", "R", "S", "T"),
> > + stringsAsFactors=FALSE)
> > 
> > scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame = FALSE,regLine=F)
> > 
> > Both x and y boxplots are correct
> > and in particular the median of the x box is at 6 which is confirmed
> > 
> >> median(aa$x)
> > [1] 6
> > 
> > Now I do only one addition to the scatterplot: I add xlim
> >> scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame =
> > FALSE,regLine=F,xlim=c(0,8))
> > 
> > This causes the boxplot on x-axis to be in error:
> > 1) the lower whisker starts now from zero
> > 2) the median is between 4 and 6 and no longer at 6 as before
> > 
> >> sessionInfo()
> > R version 3.6.3 (2020-02-29)
> > [1] car_3.0-7
> > 
> >       [[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 


From you@r|@|@nou@ @end|ng |rom gm@||@com  Mon May  4 05:31:35 2020
From: you@r|@|@nou@ @end|ng |rom gm@||@com (Yousri Fanous)
Date: Sun, 3 May 2020 23:31:35 -0400
Subject: [R] possible issue with scatterplot function in car package
In-Reply-To: <55CFE3E3-39A1-4217-B6B1-B8FA9C5D50CF@mcmaster.ca>
References: <31777_1588462276_042NV9Hx024074_CADsEwSdLUbnNjnUYALdycEODBknqd5L1PDEv9WNHbXZtHQ8N-Q@mail.gmail.com>
 <A26E6ABA-72CB-4DE0-BD6F-AAF0886D4445@mcmaster.ca>
 <CADsEwSdxFMTpyoHaYm3LiW4aMyxihAvnuse6VNCYVYCt8hg4mQ@mail.gmail.com>
 <55CFE3E3-39A1-4217-B6B1-B8FA9C5D50CF@mcmaster.ca>
Message-ID: <CADsEwSck6+F1K_8Ap=vuqdisoEHuDa71tefi+8zDtZ4we8d7QA@mail.gmail.com>

Great, thanks for the update!

Yousri

On Sun, May 3, 2020 at 10:54 PM Fox, John <jfox at mcmaster.ca> wrote:

> Dear Yousri,
>
> The problem with scatterplot() is now fixed in the development version
> 3.0-8 of the car package on R-Forge, which eventually will be submitted to
> CRAN. Until then, you can install the package via install.packages("car",
> repos="http://R-Forge.R-project.org")
>
> Thanks again for the bug report,
>  John
>
> > On May 3, 2020, at 4:37 AM, Yousri Fanous <yousri.fanous at gmail.com>
> wrote:
> >
> > Thank you Professor John for your answer.
> >
> > As you rightly said I am not using the ch in my example report as it has
> no bearing to the issue.
> > However it is the ch that led me to find the issue.
> > I was trying to label each point with its corresponding aa$ch value.
> > I used this code:
> >
> > scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame =
> FALSE,regLine=F)
> > text(aa$x,aa$y, labels=aa$ch,font=1 ,cex=.9,pos=3)
> >
> > The annotation was correct for 4 points but not for the (2,5) point.
> > I figured it is because it is close to the margin of the plot hence as a
> quick solution I modified xlim to shift the point away from the margin.
> > This worked for the annotation but eventually led to the issue I
> described.
> >
> > Thank you so much for your time
> >
> > Yousri Fanous
> >
> > Software Developer
> > IBM CANADA
> >
> > On Sat, May 2, 2020 at 11:47 PM Fox, John <jfox at mcmaster.ca> wrote:
> >
> > Dear Yousri,
> >
> > Yes, this is clearly a bug, and almost surely a long-standing one. We'll
> fix it in the next release of the car package.
> >
> > BTW, stringsAsFactors defaults to FALSE in R 4.0.0 (and you don't use
> the ch variable in the example). Also, although it has no bearing on the
> bug, I'd generally prefer
> >
> >         scatterplot(y ~ x, data=aa, smooth=FALSE, grid=FALSE,
> >                 frame=FALSE, regLine=FALSE, xlim=c(0, 8))
> >
> > Thank you for the bug report,
> >  John
> >
> >   -----------------------------
> >   John Fox, Professor Emeritus
> >   McMaster University
> >   Hamilton, Ontario, Canada
> >   Web: http::/socserv.mcmaster.ca/jfox
> >
> > > On May 2, 2020, at 7:30 PM, Yousri Fanous <yousri.fanous at gmail.com>
> wrote:
> > >
> > > library (car)
> > >
> > > aa <- data.frame(x=c(2,  5, 6, 7, 8),
> > > +  y=c(5,  10, 9, 12, 11),
> > > + ch=c("N",  "Q", "R", "S", "T"),
> > > + stringsAsFactors=FALSE)
> > >
> > > scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame =
> FALSE,regLine=F)
> > >
> > > Both x and y boxplots are correct
> > > and in particular the median of the x box is at 6 which is confirmed
> > >
> > >> median(aa$x)
> > > [1] 6
> > >
> > > Now I do only one addition to the scatterplot: I add xlim
> > >> scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame =
> > > FALSE,regLine=F,xlim=c(0,8))
> > >
> > > This causes the boxplot on x-axis to be in error:
> > > 1) the lower whisker starts now from zero
> > > 2) the median is between 4 and 6 and no longer at 6 as before
> > >
> > >> sessionInfo()
> > > R version 3.6.3 (2020-02-29)
> > > [1] car_3.0-7
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
>
>

	[[alternative HTML version deleted]]


From p_conno||y @end|ng |rom @||ng@hot@co@nz  Mon May  4 06:15:42 2020
From: p_conno||y @end|ng |rom @||ng@hot@co@nz (Patrick Connolly)
Date: Mon, 4 May 2020 16:15:42 +1200
Subject: [R] PCRE configure problem with R-4.0.0
Message-ID: <20200504041542.GA7518@slingshot.co.nz>

When I try ./configure --enable-R-shlib

I get this error:

configure: error: PCRE2 library and headers are required, or use --with-pcre1 and PCRE >= 8.32 with UTF-8 support

I have to admit I'm completely in the dark as to what functionality
PCRE provides.

Next, I tried using --with-pcre1 but it made no difference.

There are quite a lot of packages in the repository for Linux Mint
17.2 with 'pcre' in the name and these are installed:

> aptitude search pcre | grep  ^i
i   libpcre3                        - Perl 5 Compatible Regular Expression Libra
i   libpcre3:i386                   - Perl 5 Compatible Regular Expression Libra
i   libpcre3-dev                    - Perl 5 Compatible Regular Expression Libra
i   libpcrecpp0                     - Perl 5 Compatible Regular Expression Libra

Apparantly the '3' doesn't indicate an updated '2' version.  The only
packages with prce2 in the name are these:

> aptitude search pcre2
v   apertium-pcre2                                           -
v   apertium-pcre2:i386                                      -

Suggestions welcome.


-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}                   Great minds discuss ideas    
 _( Y )_  	         Average minds discuss events 
(:_~*~_:)                  Small minds discuss people  
 (_)-(_)  	                      ..... Eleanor Roosevelt
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From bgunter@4567 @end|ng |rom gm@||@com  Mon May  4 07:33:30 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 3 May 2020 22:33:30 -0700
Subject: [R] PCRE configure problem with R-4.0.0
In-Reply-To: <20200504041542.GA7518@slingshot.co.nz>
References: <20200504041542.GA7518@slingshot.co.nz>
Message-ID: <CAGxFJbT19sUrX1XxvmuZMxHBV0zr+fYzEYanZQg92Q4ATGhXTQ@mail.gmail.com>

"I have to admit I'm completely in the dark as to what functionality
PCRE provides"

https://www.pcre.org/

(Sorry, beyond that I know nothing).

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Sun, May 3, 2020 at 9:16 PM Patrick Connolly
<p_connolly at slingshot.co.nz> wrote:
>
> When I try ./configure --enable-R-shlib
>
> I get this error:
>
> configure: error: PCRE2 library and headers are required, or use --with-pcre1 and PCRE >= 8.32 with UTF-8 support
>
> I have to admit I'm completely in the dark as to what functionality
> PCRE provides.
>
> Next, I tried using --with-pcre1 but it made no difference.
>
> There are quite a lot of packages in the repository for Linux Mint
> 17.2 with 'pcre' in the name and these are installed:
>
> > aptitude search pcre | grep  ^i
> i   libpcre3                        - Perl 5 Compatible Regular Expression Libra
> i   libpcre3:i386                   - Perl 5 Compatible Regular Expression Libra
> i   libpcre3-dev                    - Perl 5 Compatible Regular Expression Libra
> i   libpcrecpp0                     - Perl 5 Compatible Regular Expression Libra
>
> Apparantly the '3' doesn't indicate an updated '2' version.  The only
> packages with prce2 in the name are these:
>
> > aptitude search pcre2
> v   apertium-pcre2                                           -
> v   apertium-pcre2:i386                                      -
>
> Suggestions welcome.
>
>
> --
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>    ___    Patrick Connolly
>  {~._.~}                   Great minds discuss ideas
>  _( Y )_                 Average minds discuss events
> (:_~*~_:)                  Small minds discuss people
>  (_)-(_)                              ..... Eleanor Roosevelt
>
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From kry|ov@r00t @end|ng |rom gm@||@com  Mon May  4 09:51:40 2020
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Mon, 4 May 2020 10:51:40 +0300
Subject: [R] PCRE configure problem with R-4.0.0
In-Reply-To: <20200504041542.GA7518@slingshot.co.nz>
References: <20200504041542.GA7518@slingshot.co.nz>
Message-ID: <20200504100010.5d771a78@trisector>

First of all, you mentioned Linux Mint, so you might get better advice
on R-SIG-Debian mailing list.

On Mon, 4 May 2020 16:15:42 +1200
Patrick Connolly <p_connolly at slingshot.co.nz> wrote:

>There are quite a lot of packages in the repository for Linux Mint
>17.2 with 'pcre' in the name and these are installed:

>Apparantly the '3' doesn't indicate an updated '2' version

The funny thing about libpcre3 is that it is the old PCRE1 version,
third ABI-incompatible upgrade of it [*], and libpcre2 (available in
current releases of Linux Mint, Ubuntu and Debian) is supposed to be
the newer PCRE2.

Linux Mint 17.2 is based on Ubuntu 14.04, which has been released in
April 2014, while PCRE2 has been released in 2015. This might be the
reason why libpcre2 doesn't seem to be available to you (I have tried
searching both repositories, including backports, with no success).
Moreover, Ubuntu 14.04 only has PCRE 8.31, which is too old to work
with R 4.0. The official builds of R 4.0 [**] are not available for
Ubuntu 14.04, either.

> Suggestions welcome.

You can try to backport PCRE 8.32 for Linux Mint 17.2 by taking the
source package for 8.31 (apt-get source pcre3), extracting the new
version of PCRE into it and fiddling with it until it builds (see
[***] for more information on that). This is a complicated procedure,
and if an important system component depends on PCRE, you can end up
breaking the system.

You can also try to install latest PCRE2 from source (./configure; make;
sudo make install) into /usr/local where it shouldn't interfere too much
with the rest of the system.

Another option could be upgrading to a supported release of Linux Mint
and installing the official binary build from [**].

Good luck!

-- 
Best regards,
Ivan

[*]
https://www.debian.org/doc/debian-policy/ch-sharedlibs.html#run-time-shared-libraries

[**] https://cran.r-project.org/bin/linux/ubuntu/

[***] https://www.debian.org/doc/manuals/maint-guide/


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Mon May  4 16:04:18 2020
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Mon, 4 May 2020 16:04:18 +0200
Subject: [R] 
 'closure' (was "stats:: spline's method could not be monoH.FC")
In-Reply-To: <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
 <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
Message-ID: <24240.8418.914054.939133@stat.math.ethz.ch>

Just about this one some important term  'closure', hence I'm
modifying the subject. 

Note we came here from my 2nd reason why I had added 'monoH.FC'
feature only for splinefun() and not for spline() :

- splinefun() is using's R feature of "(non-trivial) closure", i.e, 
  it returns a *function* containing its own state

-------------

Yes, there is some vagueness / ambiguity about how the term
"closure" is used within R documentation and teaching :

Yes, indeed, the term  stems from lisp ("Scheme" more
specifically according to
 https://en.wikipedia.org/wiki/Closure_(computer_programming) ,
but you should remember that R originally had been implemented
as a "lisp with S-syntax" (that's my paraphrasing), see
 https://en.wikipedia.org/wiki/R_(programming_language)
   and (in the box on the right hand side) its list of 

    "Influenced by" : Common Lisp, S, Scheme[2], XLispStat

and [2] is Ross Ihaka's famous Interface paper :

    Ihaka, Ross (1998). R : Past and Future History (PDF) (Technical report).
    Statistics Department, The University of Auckland, Auckland, New Zealand.
    https://www.stat.auckland.ac.nz/~ihaka/downloads/Interface98.pdf

And that's the reason why  typeof(f)   for all functions 'f'
which are not primitive is "closure".
All such functions (i.e. *not* the primitives) have an
environment, as Mark Leeds explains,  and what this means and
why this is very important is beyond 'R-help'.
Inside R's own C code, all such R functions are "closures", and
programmers (incl R corers) who think more about the low level
view of R objects would use the term like that ex

OTOH, Hadley Wickham has written a book "Advanced R"  which has been
the best book about advanced R by far (in my view, notably
before it morphed (towards the 2nd edition) to use more and more
non-base R packages).  There, he used "Closure" in a different,
stricter sense, starting the section  'Closures' with

    ?An object is data with functions.
     A closure is a function with data.? ? John D. Cook

Now, most functions have only a "trivial environment" (my own
  terminology,  when I'm teaching "advanced R" courses/classes,
  see https://github.com/mmaechler/ProgRRR/ for some teaching material)

"trivial environment" meaning that their environment is
- either the namespace belonging to the package the function is part of
- or .GlobalEnv  { which is the same as globalenv() }

and most functions with non-trivial environment are just
"helper" functions defined inside other functions which are very
short lived (during the evaluation of the outer function's call).

Now the remaining few functions with non-trivial environments
that you see in "base R"  are those returned by

  splinefun(), approxfun(),  ecdf(), or stepfun()

where the last two actually are implemented via approxfun().

-- -- --

I hope this has been useful  "writeup" about
'closure' ..

Best,
Martin

Martin Maechler
ETH Zurich  and  R Core team


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Mon May  4 16:25:02 2020
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Mon, 4 May 2020 16:25:02 +0200
Subject: [R] stats:: spline's method could not be monoH.FC
In-Reply-To: <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
Message-ID: <24240.9662.85376.166974@stat.math.ethz.ch>

>>>>> Abby Spurdle 
>>>>>     on Sun, 3 May 2020 16:15:17 +1200 writes:

    >> and just today a colleague asked me about spline interpolation
    >> with general 2nd derivative boundary conditions
    >> s''(x_1) = s2_1,  s''(x_n) = s2_2

actually I was wrong... I *read* it as the above, but what he
really wanted was what the wikipedia page  class "clamped"
boundary conditions, i.e., for the *first* derivative

   s'(x_1) = s1_1,  s'(x_n) = s1_2
   

    > It should possible via cubic Hermite splines.

and indeed that is available via cubic Hermite splines available
in R via stats package's  splinefunH()

{which I wrote when implementing  "monoH.FC"}

    > A nontrivial design decision in my package was the computation of
    > slopes at the endpoints.
    > (Something which I got wrong, twice...)

The "wikiversity" has a very nice small math lecture on this
 https://en.wikiversity.org/wiki/Cubic_Spline_Interpolation

which derives *both* cases (first and 2nd derivative boundary conditions),
the only draw back to quickly do it with ('base R') is that I
need to "translate" that (2nd derivative values $M_i$) parametrization
into either the one used into the (a,b,c,y)-parametrizat of the
default spline() / splinefun() methods or the Hilbert spline
form for  splinefunH(x[],y[],m[]).

Martin

    > My guess is that I could write a function in about 60 to 90 minutes,
    > including all the testing and calculus.

    > However, I need to note two things:
    > (1) Cubic Hermite splines do not have a continuous second derivative.

well, many don't if you allow any slopes at node. However, if
you only set 2 boundary conditions *instead* of the natural
spline ones  f''(x_1) = f''(x_1) = 0, 
you can still remain in C_2 (i.e. continuous 2nd derivative).

(And that's also what the above wikiversity lecture provides).

    > (2) Specifying the second derivatives (at the endpoints) would prevent
    > the user from specifying the first derivatives (aka the slopes).

    > Could you please confirm if the function would still be of interest...?

Well, as I know spent enough time reading and thinking, I'd
really like to add   method = "clamped" to splinefun() and also
the other one where fix the 2nd derivatives (to arbitrary values
instead of zero).

So if you already have the R code leading up to one of the 2
"parametrizations" we use in spline() / splinefun(),  I'd be
grateful, if you have the time.

Martin


From |@t@z@hn @end|ng |rom gm@||@com  Mon May  4 17:03:04 2020
From: |@t@z@hn @end|ng |rom gm@||@com (Ista Zahn)
Date: Mon, 4 May 2020 11:03:04 -0400
Subject: [R] PCRE configure problem with R-4.0.0
In-Reply-To: <20200504100010.5d771a78@trisector>
References: <20200504041542.GA7518@slingshot.co.nz>
 <20200504100010.5d771a78@trisector>
Message-ID: <CA+vqiLEcbaU=Wm7dF=eEdygavtBRx1oxSaR8FyjX16GyBrYj0Q@mail.gmail.com>

On Mon, May 4, 2020 at 3:51 AM Ivan Krylov <krylov.r00t at gmail.com> wrote:
>
> First of all, you mentioned Linux Mint, so you might get better advice
> on R-SIG-Debian mailing list.
>
> On Mon, 4 May 2020 16:15:42 +1200
> Patrick Connolly <p_connolly at slingshot.co.nz> wrote:
>
> >There are quite a lot of packages in the repository for Linux Mint
> >17.2 with 'pcre' in the name and these are installed:
>
> >Apparantly the '3' doesn't indicate an updated '2' version
>
> The funny thing about libpcre3 is that it is the old PCRE1 version,
> third ABI-incompatible upgrade of it [*], and libpcre2 (available in
> current releases of Linux Mint, Ubuntu and Debian) is supposed to be
> the newer PCRE2.
>
> Linux Mint 17.2 is based on Ubuntu 14.04, which has been released in
> April 2014, while PCRE2 has been released in 2015.

Moreover, support for 17.2 ended over a year ago (according to
https://en.wikipedia.org/wiki/Linux_Mint_version_history). I suggest
upgrading to a supported version.

Best,
Ista

This might be the
> reason why libpcre2 doesn't seem to be available to you (I have tried
> searching both repositories, including backports, with no success).
> Moreover, Ubuntu 14.04 only has PCRE 8.31, which is too old to work
> with R 4.0. The official builds of R 4.0 [**] are not available for
> Ubuntu 14.04, either.
>
> > Suggestions welcome.
>
> You can try to backport PCRE 8.32 for Linux Mint 17.2 by taking the
> source package for 8.31 (apt-get source pcre3), extracting the new
> version of PCRE into it and fiddling with it until it builds (see
> [***] for more information on that). This is a complicated procedure,
> and if an important system component depends on PCRE, you can end up
> breaking the system.
>
> You can also try to install latest PCRE2 from source (./configure; make;
> sudo make install) into /usr/local where it shouldn't interfere too much
> with the rest of the system.
>
> Another option could be upgrading to a supported release of Linux Mint
> and installing the official binary build from [**].
>
> Good luck!
>
> --
> Best regards,
> Ivan
>
> [*]
> https://www.debian.org/doc/debian-policy/ch-sharedlibs.html#run-time-shared-libraries
>
> [**] https://cran.r-project.org/bin/linux/ubuntu/
>
> [***] https://www.debian.org/doc/manuals/maint-guide/
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Mon May  4 17:08:44 2020
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Mon, 4 May 2020 17:08:44 +0200
Subject: [R] stats:: spline's method could not be monoH.FC
In-Reply-To: <24240.9662.85376.166974@stat.math.ethz.ch>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <24240.9662.85376.166974@stat.math.ethz.ch>
Message-ID: <24240.12284.675977.989318@stat.math.ethz.ch>

>>>>> Martin Maechler 
>>>>>     on Mon, 4 May 2020 16:25:02 +0200 writes:

>>>>> Abby Spurdle 
>>>>>     on Sun, 3 May 2020 16:15:17 +1200 writes:

    >>> and just today a colleague asked me about spline interpolation
    >>> with general 2nd derivative boundary conditions
    >>> s''(x_1) = s2_1,  s''(x_n) = s2_2

    > actually I was wrong... I *read* it as the above, but what he
    > really wanted was what the wikipedia page  class "clamped"
    > boundary conditions, i.e., for the *first* derivative

    > s'(x_1) = s1_1,  s'(x_n) = s1_2
   

    >> It should possible via cubic Hermite splines.

    > and indeed that is available via cubic Hermite splines available
    > in R via stats package's  splinefunH()

    > {which I wrote when implementing  "monoH.FC"}

    >> A nontrivial design decision in my package was the computation of
    >> slopes at the endpoints.
    >> (Something which I got wrong, twice...)

    > The "wikiversity" has a very nice small math lecture on this
    > https://en.wikiversity.org/wiki/Cubic_Spline_Interpolation

    > which derives *both* cases (first and 2nd derivative boundary conditions),
    > the only draw back to quickly do it with ('base R') is that I
    > need to "translate" that (2nd derivative values $M_i$) parametrization
    > into either the one used into the (a,b,c,y)-parametrizat of the
    > default spline() / splinefun() methods or the Hilbert spline
    > form for  splinefunH(x[],y[],m[]).

    > Martin

Well, I should have looked a bit further, first : at least the case for

    s''(x_1) = s2_1,    s''(x_n) = s2_2

seems trivially "hidden" in the C code in

R's  src/library/stats/src/splines.c     i.e.
https://svn.r-project.org/R/trunk/src/library/stats/src/splines.c

at the end of natural_spline() we should set c[1] and c[n] to non-zero.
...
and actually I think in spline_eval() for extrapolation (to the
left? and right), there's currently also an assumption that c[1]
 and c[n] are zero.

So as a matter of fact I would ask for patches there (both in C
and in R calling C ... maybe too much for 30 minutes ;-)

Best,
Martin


    >> My guess is that I could write a function in about 60 to 90 minutes,
    >> including all the testing and calculus.

    >> However, I need to note two things:
    >> (1) Cubic Hermite splines do not have a continuous second derivative.

    > well, many don't if you allow any slopes at node. However, if
    > you only set 2 boundary conditions *instead* of the natural
    > spline ones  f''(x_1) = f''(x_1) = 0, 
    > you can still remain in C_2 (i.e. continuous 2nd derivative).

    > (And that's also what the above wikiversity lecture provides).

    >> (2) Specifying the second derivatives (at the endpoints) would prevent
    >> the user from specifying the first derivatives (aka the slopes).

    >> Could you please confirm if the function would still be of interest...?

    > Well, as I know spent enough time reading and thinking, I'd
    > really like to add   method = "clamped" to splinefun() and also
    > the other one where fix the 2nd derivatives (to arbitrary values
    > instead of zero).

    > So if you already have the R code leading up to one of the 2
    > "parametrizations" we use in spline() / splinefun(),  I'd be
    > grateful, if you have the time.

    > Martin


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Mon May  4 19:15:44 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Mon, 4 May 2020 12:15:44 -0500
Subject: [R] if else statement
Message-ID: <CAF9-5jODa79FKHbkmRqD4T2Lzak5GrQL4TWji4pPkTpC3ndd4Q@mail.gmail.com>

Hello,

I have a data frame like this:

> head(b)
       FID   IID FLASER PLASER
1: fam1000 G1000      1      1
2: fam1001 G1001      1      1
3: fam1003 G1003      1      2
4: fam1005 G1005      1      1
5: fam1009 G1009      NA      2
6: fam1052 G1052      1      1
...
> unique(b$PLASER)
[1]  1  2 NA
> unique(b$FLASER)
[1]  1  2 NA

how can I do if else statement so that I am creating a
PHENO =2 if b$FLASER=2 or b$PLASER=2
PHENO=1 if b$FLASER=1 and b$PLASER=1
otherwise PHENO=NA

I tried this but I am not sure if this is correct:
b$pheno=ifelse(b$PLASER==1 & b$FLASER==1,1,ifelse(b$PLASER==2 |
b$FLASER==2,2,NA))

Thanks
Ana

	[[alternative HTML version deleted]]


From m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com  Mon May  4 19:33:12 2020
From: m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com (Patrick (Malone Quantitative))
Date: Mon, 4 May 2020 13:33:12 -0400
Subject: [R] if else statement
In-Reply-To: <CAF9-5jODa79FKHbkmRqD4T2Lzak5GrQL4TWji4pPkTpC3ndd4Q@mail.gmail.com>
References: <CAF9-5jODa79FKHbkmRqD4T2Lzak5GrQL4TWji4pPkTpC3ndd4Q@mail.gmail.com>
Message-ID: <CAJc=yOG82RM0bhf9Npk6eKbKva56=94e4swT4Vh6kOjTgq8nsg@mail.gmail.com>

"I tried this but I am not sure if this is correct:"

Does it provide the expected result for all possible combinations of 1/2/NA
for both variables?

On Mon, May 4, 2020 at 1:16 PM Ana Marija <sokovic.anamarija at gmail.com>
wrote:

> Hello,
>
> I have a data frame like this:
>
> > head(b)
>        FID   IID FLASER PLASER
> 1: fam1000 G1000      1      1
> 2: fam1001 G1001      1      1
> 3: fam1003 G1003      1      2
> 4: fam1005 G1005      1      1
> 5: fam1009 G1009      NA      2
> 6: fam1052 G1052      1      1
> ...
> > unique(b$PLASER)
> [1]  1  2 NA
> > unique(b$FLASER)
> [1]  1  2 NA
>
> how can I do if else statement so that I am creating a
> PHENO =2 if b$FLASER=2 or b$PLASER=2
> PHENO=1 if b$FLASER=1 and b$PLASER=1
> otherwise PHENO=NA
>
> I tried this but I am not sure if this is correct:
> b$pheno=ifelse(b$PLASER==1 & b$FLASER==1,1,ifelse(b$PLASER==2 |
> b$FLASER==2,2,NA))
>
> Thanks
> Ana
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Patrick S. Malone, Ph.D., Malone Quantitative
NEW Service Models: http://malonequantitative.com

He/Him/His

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon May  4 19:45:05 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 04 May 2020 10:45:05 -0700
Subject: [R] if else statement
In-Reply-To: <CAJc=yOG82RM0bhf9Npk6eKbKva56=94e4swT4Vh6kOjTgq8nsg@mail.gmail.com>
References: <CAF9-5jODa79FKHbkmRqD4T2Lzak5GrQL4TWji4pPkTpC3ndd4Q@mail.gmail.com>
 <CAJc=yOG82RM0bhf9Npk6eKbKva56=94e4swT4Vh6kOjTgq8nsg@mail.gmail.com>
Message-ID: <D03511ED-4587-489F-B24D-D6DD4571AF1F@dcn.davis.ca.us>

To expand on Patrick's response...

You can use the expand.grid function to generate a test table containing all combinations. However, we would not be in a position to verify that the results you get when you apply your logic to the test table are what you want... you know the requirements much better than we do. Nor is that kind of service what this mailing list is for, so please focus on showing what you cannot figure out how to accomplish rather than asking us to do or check your work for you.

On May 4, 2020 10:33:12 AM PDT, "Patrick (Malone Quantitative)" <malone at malonequantitative.com> wrote:
>"I tried this but I am not sure if this is correct:"
>
>Does it provide the expected result for all possible combinations of
>1/2/NA
>for both variables?
>
>On Mon, May 4, 2020 at 1:16 PM Ana Marija <sokovic.anamarija at gmail.com>
>wrote:
>
>> Hello,
>>
>> I have a data frame like this:
>>
>> > head(b)
>>        FID   IID FLASER PLASER
>> 1: fam1000 G1000      1      1
>> 2: fam1001 G1001      1      1
>> 3: fam1003 G1003      1      2
>> 4: fam1005 G1005      1      1
>> 5: fam1009 G1009      NA      2
>> 6: fam1052 G1052      1      1
>> ...
>> > unique(b$PLASER)
>> [1]  1  2 NA
>> > unique(b$FLASER)
>> [1]  1  2 NA
>>
>> how can I do if else statement so that I am creating a
>> PHENO =2 if b$FLASER=2 or b$PLASER=2
>> PHENO=1 if b$FLASER=1 and b$PLASER=1
>> otherwise PHENO=NA
>>
>> I tried this but I am not sure if this is correct:
>> b$pheno=ifelse(b$PLASER==1 & b$FLASER==1,1,ifelse(b$PLASER==2 |
>> b$FLASER==2,2,NA))
>>
>> Thanks
>> Ana
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>

-- 
Sent from my phone. Please excuse my brevity.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Mon May  4 20:05:25 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Mon, 4 May 2020 13:05:25 -0500
Subject: [R] if else statement
In-Reply-To: <D03511ED-4587-489F-B24D-D6DD4571AF1F@dcn.davis.ca.us>
References: <CAF9-5jODa79FKHbkmRqD4T2Lzak5GrQL4TWji4pPkTpC3ndd4Q@mail.gmail.com>
 <CAJc=yOG82RM0bhf9Npk6eKbKva56=94e4swT4Vh6kOjTgq8nsg@mail.gmail.com>
 <D03511ED-4587-489F-B24D-D6DD4571AF1F@dcn.davis.ca.us>
Message-ID: <CAF9-5jMyxGgk=sXFQKHZh9vvL0-RK5NWm=HkRCh54r7c3_ABFg@mail.gmail.com>

Thank you for the tip about table function, it seems correct:

> table(b$FLASER, b$PLASER, exclude = NULL)

         1   2 <NA>
  1    836 691    6
  2     14  70    8
  <NA>   0  45   28
> table(b$pheno,exclude = NULL)

   1    2 <NA>
 836  828   34

On Mon, May 4, 2020 at 12:45 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> To expand on Patrick's response...
>
> You can use the expand.grid function to generate a test table containing
> all combinations. However, we would not be in a position to verify that the
> results you get when you apply your logic to the test table are what you
> want... you know the requirements much better than we do. Nor is that kind
> of service what this mailing list is for, so please focus on showing what
> you cannot figure out how to accomplish rather than asking us to do or
> check your work for you.
>
> On May 4, 2020 10:33:12 AM PDT, "Patrick (Malone Quantitative)" <
> malone at malonequantitative.com> wrote:
> >"I tried this but I am not sure if this is correct:"
> >
> >Does it provide the expected result for all possible combinations of
> >1/2/NA
> >for both variables?
> >
> >On Mon, May 4, 2020 at 1:16 PM Ana Marija <sokovic.anamarija at gmail.com>
> >wrote:
> >
> >> Hello,
> >>
> >> I have a data frame like this:
> >>
> >> > head(b)
> >>        FID   IID FLASER PLASER
> >> 1: fam1000 G1000      1      1
> >> 2: fam1001 G1001      1      1
> >> 3: fam1003 G1003      1      2
> >> 4: fam1005 G1005      1      1
> >> 5: fam1009 G1009      NA      2
> >> 6: fam1052 G1052      1      1
> >> ...
> >> > unique(b$PLASER)
> >> [1]  1  2 NA
> >> > unique(b$FLASER)
> >> [1]  1  2 NA
> >>
> >> how can I do if else statement so that I am creating a
> >> PHENO =2 if b$FLASER=2 or b$PLASER=2
> >> PHENO=1 if b$FLASER=1 and b$PLASER=1
> >> otherwise PHENO=NA
> >>
> >> I tried this but I am not sure if this is correct:
> >> b$pheno=ifelse(b$PLASER==1 & b$FLASER==1,1,ifelse(b$PLASER==2 |
> >> b$FLASER==2,2,NA))
> >>
> >> Thanks
> >> Ana
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Mon May  4 20:31:12 2020
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard McGarvey)
Date: Mon, 4 May 2020 14:31:12 -0400 (EDT)
Subject: [R] COVID-19 datasets...
Message-ID: <961343375.1407915.1588617072873@connect.xfinity.com>

Just curious does anyone know of a website that has data available in a format that R can download and analyze?
?
Thanks


Bernard McGarvey


Director, Fort Myers Beach Lions Foundation, Inc.


Retired (Lilly Engineering Fellow).


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Mon May  4 20:32:06 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Mon, 4 May 2020 19:32:06 +0100
Subject: [R] if else statement
In-Reply-To: <CAF9-5jODa79FKHbkmRqD4T2Lzak5GrQL4TWji4pPkTpC3ndd4Q@mail.gmail.com>
References: <CAF9-5jODa79FKHbkmRqD4T2Lzak5GrQL4TWji4pPkTpC3ndd4Q@mail.gmail.com>
Message-ID: <5862eb7d-db98-98af-280f-1a5ffbcc2587@sapo.pt>

Hello,

Here is a way, using logical indices.

b$pheno <- NA
b$pheno[b$FLASER == 1 & b$PLASER == 1] <- 1
b$pheno[b$FLASER == 2 | b$PLASER == 2] <- 2


Hope this helps,

Rui Barradas

?s 18:15 de 04/05/20, Ana Marija escreveu:
> Hello,
> 
> I have a data frame like this:
> 
>> head(b)
>         FID   IID FLASER PLASER
> 1: fam1000 G1000      1      1
> 2: fam1001 G1001      1      1
> 3: fam1003 G1003      1      2
> 4: fam1005 G1005      1      1
> 5: fam1009 G1009      NA      2
> 6: fam1052 G1052      1      1
> ...
>> unique(b$PLASER)
> [1]  1  2 NA
>> unique(b$FLASER)
> [1]  1  2 NA
> 
> how can I do if else statement so that I am creating a
> PHENO =2 if b$FLASER=2 or b$PLASER=2
> PHENO=1 if b$FLASER=1 and b$PLASER=1
> otherwise PHENO=NA
> 
> I tried this but I am not sure if this is correct:
> b$pheno=ifelse(b$PLASER==1 & b$FLASER==1,1,ifelse(b$PLASER==2 |
> b$FLASER==2,2,NA))
> 
> Thanks
> Ana
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From btupper @end|ng |rom b|ge|ow@org  Mon May  4 20:34:42 2020
From: btupper @end|ng |rom b|ge|ow@org (Ben Tupper)
Date: Mon, 4 May 2020 14:34:42 -0400
Subject: [R] COVID-19 datasets...
In-Reply-To: <961343375.1407915.1588617072873@connect.xfinity.com>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
Message-ID: <CALrbzg1OHAxZ+4yh7LwdADrynhOrj+yaSzWZwCMX7qKZsbMpBg@mail.gmail.com>

This works very well https://github.com/covid19datahub/COVID19

Cheers,
Ben

On Mon, May 4, 2020 at 2:32 PM Bernard McGarvey <
mcgarvey.bernard at comcast.net> wrote:

> Just curious does anyone know of a website that has data available in a
> format that R can download and analyze?
>
> Thanks
>
>
> Bernard McGarvey
>
>
> Director, Fort Myers Beach Lions Foundation, Inc.
>
>
> Retired (Lilly Engineering Fellow).
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ben Tupper
Bigelow Laboratory for Ocean Science
East Boothbay, Maine
http://www.bigelow.org/
https://eco.bigelow.org

	[[alternative HTML version deleted]]


From h@@@n@d|w@n @end|ng |rom gm@||@com  Mon May  4 20:43:42 2020
From: h@@@n@d|w@n @end|ng |rom gm@||@com (Hasan Diwan)
Date: Mon, 4 May 2020 11:43:42 -0700
Subject: [R] COVID-19 datasets...
In-Reply-To: <961343375.1407915.1588617072873@connect.xfinity.com>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
Message-ID: <CAP+bYWDq_bjtNT=aTUCVNH=b30W6GGeq3F3HGbLEW2j47aLPpQ@mail.gmail.com>

On Mon, 4 May 2020 at 11:32, Bernard McGarvey <mcgarvey.bernard at comcast.net>
wrote:

> Just curious does anyone know of a website that has data available in a
> format that R can download and analyze?
>

https://hd1-units.herokuapp.com/covid has a days parameter one can adjust
to go back in time and a suffix parameter to obtain json, yml, or csv. -- H

-- 
OpenPGP:
https://sks-keyservers.net/pks/lookup?op=get&search=0xFEBAD7FFD041BBA1
If you wish to request my time, please do so using
*bit.ly/hd1AppointmentRequest
<http://bit.ly/hd1AppointmentRequest>*.
Si vous voudrais faire connnaisance, allez a *bit.ly/hd1AppointmentRequest
<http://bit.ly/hd1AppointmentRequest>*.

<https://sks-keyservers.net/pks/lookup?op=get&search=0xFEBAD7FFD041BBA1>Sent
from my mobile device
Envoye de mon portable

	[[alternative HTML version deleted]]


From you@r|@|@nou@ @end|ng |rom gm@||@com  Mon May  4 20:51:03 2020
From: you@r|@|@nou@ @end|ng |rom gm@||@com (Yousri Fanous)
Date: Mon, 4 May 2020 14:51:03 -0400
Subject: [R] why outer function is failing?
Message-ID: <CADsEwSfuFpV=oJni7E_CnLzUKa-PSOtc8vnRQ5pkj4mLV-y8GA@mail.gmail.com>

Hello

>From outer help page:
outer takes two vectors
<https://renenyffenegger.ch/notes/development/languages/R/data-structures/vector/index>
and a function (that itself takes two arguments) and builds a matrix
<https://renenyffenegger.ch/notes/development/languages/R/data-structures/matrix/index>
by calling the given function for each combination of the elements in the
two vectors.

x<-1:6
y<-3:10

 m<-outer (x,y,function (x,y) rnorm(x,y))
works as expected.

But now when I replace rnorm with rolldie from package (prob) outer
complains
 library (prob)
m<-outer (x,y,function (x,y) nrow(rolldie(x,y)))
Error in rep("X", times) : invalid 'times' argument
In addition: Warning messages:
1: In 1:times : numerical expression has 48 elements: only the first used
2: In 1:nsides : numerical expression has 48 elements: only the first used

nrow(rolldie(5,4))
[1] 1024

1) why outer is failing with rolldie?
2) What does the error mean?

As a workaround I can do this thru a double loop, but I was hoping to get a
more efficient way.

Thanks for the help

Yousri Fanous
Software developer
IBM Canada

	[[alternative HTML version deleted]]


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Mon May  4 20:56:25 2020
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard Comcast)
Date: Mon, 4 May 2020 14:56:25 -0400
Subject: [R] COVID-19 datasets...
In-Reply-To: <E3562BF5-42BC-43D6-96AB-E043B6EA1C57@jsasoc.com>
References: <E3562BF5-42BC-43D6-96AB-E043B6EA1C57@jsasoc.com>
Message-ID: <01B72E8D-CDF6-445F-8E95-EB8D3158CC77@comcast.net>

Thanks, i will take a look

Bernard
Sent from my iPhone so please excuse the spelling!"

> On May 4, 2020, at 2:49 PM, James Spottiswoode <james at jsasoc.com> wrote:
> 
> ?Sure. COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University is available here:
> 
> https://github.com/CSSEGISandData/COVID-19
> 
> All in csv fiormat.
> 
> 
>> On May 4, 2020, at 11:31 AM, Bernard McGarvey <mcgarvey.bernard at comcast.net> wrote:
>> 
>> Just curious does anyone know of a website that has data available in a format that R can download and analyze?
>>  
>> Thanks
>> 
>> 
>> Bernard McGarvey
>> 
>> 
>> Director, Fort Myers Beach Lions Foundation, Inc.
>> 
>> 
>> Retired (Lilly Engineering Fellow).
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> James Spottiswoode
> Applied Mathematics & Statistics
> (310) 270 6220
> jamesspottiswoode Skype
> james at jsasoc.com
> 
> 
> 

	[[alternative HTML version deleted]]


From dc@r|@on @end|ng |rom t@mu@edu  Mon May  4 21:18:40 2020
From: dc@r|@on @end|ng |rom t@mu@edu (David Carlson)
Date: Mon, 4 May 2020 14:18:40 -0500
Subject: [R] why outer function is failing?
In-Reply-To: <CADsEwSfuFpV=oJni7E_CnLzUKa-PSOtc8vnRQ5pkj4mLV-y8GA@mail.gmail.com>
References: <CADsEwSfuFpV=oJni7E_CnLzUKa-PSOtc8vnRQ5pkj4mLV-y8GA@mail.gmail.com>
Message-ID: <CAE-dL2pS1meP02WsahMgAyBm3gYRL6u5--24mScX-B-O+xKWLA@mail.gmail.com>

The FUN= argument must be a vectorized function (see the documentation,
?outer), but the function rolldie takes only scalar values as arguments:

rolldie(x, y)
Error in rep("X", times) : invalid 'times' argument
In addition: Warning messages:
1: In 1:times : numerical expression has 6 elements: only the first used
2: In 1:nsides : numerical expression has 8 elements: only the first used

David Carlson
Anthropology Department
Texas A&M University

On Mon, May 4, 2020 at 1:54 PM Yousri Fanous <yousri.fanous at gmail.com>
wrote:

> Hello
>
> From outer help page:
> outer takes two vectors
> <
> https://urldefense.proofpoint.com/v2/url?u=https-3A__renenyffenegger.ch_notes_development_languages_R_data-2Dstructures_vector_index&d=DwICAg&c=u6LDEWzohnDQ01ySGnxMzg&r=VAaHUElasUXjP9TzIcfIrXdkDpHnJBBZ9Q1u5LcXz9s&m=IdpldO8NxT8R9A1xPy5xD8VynSPRwZchLY3bUEufCYE&s=aLWLgMoHHIk7DaXgX4L0emzN-KDe2WFMYPpVwZuk35U&e=
> >
> and a function (that itself takes two arguments) and builds a matrix
> <
> https://urldefense.proofpoint.com/v2/url?u=https-3A__renenyffenegger.ch_notes_development_languages_R_data-2Dstructures_matrix_index&d=DwICAg&c=u6LDEWzohnDQ01ySGnxMzg&r=VAaHUElasUXjP9TzIcfIrXdkDpHnJBBZ9Q1u5LcXz9s&m=IdpldO8NxT8R9A1xPy5xD8VynSPRwZchLY3bUEufCYE&s=j5k3zAaxPR8LMHbTe72GTbSGOV_RE5K1Uc2jytaB8SE&e=
> >
> by calling the given function for each combination of the elements in the
> two vectors.
>
> x<-1:6
> y<-3:10
>
>  m<-outer (x,y,function (x,y) rnorm(x,y))
> works as expected.
>
> But now when I replace rnorm with rolldie from package (prob) outer
> complains
>  library (prob)
> m<-outer (x,y,function (x,y) nrow(rolldie(x,y)))
> Error in rep("X", times) : invalid 'times' argument
> In addition: Warning messages:
> 1: In 1:times : numerical expression has 48 elements: only the first used
> 2: In 1:nsides : numerical expression has 48 elements: only the first used
>
> nrow(rolldie(5,4))
> [1] 1024
>
> 1) why outer is failing with rolldie?
> 2) What does the error mean?
>
> As a workaround I can do this thru a double loop, but I was hoping to get a
> more efficient way.
>
> Thanks for the help
>
> Yousri Fanous
> Software developer
> IBM Canada
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=u6LDEWzohnDQ01ySGnxMzg&r=VAaHUElasUXjP9TzIcfIrXdkDpHnJBBZ9Q1u5LcXz9s&m=IdpldO8NxT8R9A1xPy5xD8VynSPRwZchLY3bUEufCYE&s=6jxhmxrSNqyZS4-oU2g8r2R0LEZ0yhtSm-4GdfP0Cbk&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=u6LDEWzohnDQ01ySGnxMzg&r=VAaHUElasUXjP9TzIcfIrXdkDpHnJBBZ9Q1u5LcXz9s&m=IdpldO8NxT8R9A1xPy5xD8VynSPRwZchLY3bUEufCYE&s=rFq4TmerLd5tegHnWSbx5ISdfDNks-TIa9Whne6bBaM&e=
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon May  4 21:32:16 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 04 May 2020 12:32:16 -0700
Subject: [R] why outer function is failing?
In-Reply-To: <CADsEwSfuFpV=oJni7E_CnLzUKa-PSOtc8vnRQ5pkj4mLV-y8GA@mail.gmail.com>
References: <CADsEwSfuFpV=oJni7E_CnLzUKa-PSOtc8vnRQ5pkj4mLV-y8GA@mail.gmail.com>
Message-ID: <27846454-E813-4FB1-940F-BC190E208039@dcn.davis.ca.us>

The outer function only calls FUN once with two vectors representing all combinations of the inputs. If rolldie is not vectorized then it will have trouble with this input.

Why aren't you using sample?

On May 4, 2020 11:51:03 AM PDT, Yousri Fanous <yousri.fanous at gmail.com> wrote:
>Hello
>
>From outer help page:
>outer takes two vectors
><https://renenyffenegger.ch/notes/development/languages/R/data-structures/vector/index>
>and a function (that itself takes two arguments) and builds a matrix
><https://renenyffenegger.ch/notes/development/languages/R/data-structures/matrix/index>
>by calling the given function for each combination of the elements in
>the
>two vectors.
>
>x<-1:6
>y<-3:10
>
> m<-outer (x,y,function (x,y) rnorm(x,y))
>works as expected.
>
>But now when I replace rnorm with rolldie from package (prob) outer
>complains
> library (prob)
>m<-outer (x,y,function (x,y) nrow(rolldie(x,y)))
>Error in rep("X", times) : invalid 'times' argument
>In addition: Warning messages:
>1: In 1:times : numerical expression has 48 elements: only the first
>used
>2: In 1:nsides : numerical expression has 48 elements: only the first
>used
>
>nrow(rolldie(5,4))
>[1] 1024
>
>1) why outer is failing with rolldie?
>2) What does the error mean?
>
>As a workaround I can do this thru a double loop, but I was hoping to
>get a
>more efficient way.
>
>Thanks for the help
>
>Yousri Fanous
>Software developer
>IBM Canada
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From dc@r|@on @end|ng |rom t@mu@edu  Mon May  4 21:34:18 2020
From: dc@r|@on @end|ng |rom t@mu@edu (David Carlson)
Date: Mon, 4 May 2020 14:34:18 -0500
Subject: [R] why outer function is failing?
In-Reply-To: <CAE-dL2pS1meP02WsahMgAyBm3gYRL6u5--24mScX-B-O+xKWLA@mail.gmail.com>
References: <CADsEwSfuFpV=oJni7E_CnLzUKa-PSOtc8vnRQ5pkj4mLV-y8GA@mail.gmail.com>
 <CAE-dL2pS1meP02WsahMgAyBm3gYRL6u5--24mScX-B-O+xKWLA@mail.gmail.com>
Message-ID: <CAE-dL2rRLtVjubQM7zJii1iTHe=6Lm+TEeVoSQky_ZMNqnztRg@mail.gmail.com>

Re-reading your question it appears you want the number of permutations for
each pair, e.g. the number of permutations of the numbers 1 - 3 taken 1
time, etc. If I am correct, this should get you want you want:

ct <- outer(y, x, "^")
ct
     [,1] [,2] [,3]  [,4]   [,5]    [,6]
[1,]    3    9   27    81    243     729
[2,]    4   16   64   256   1024    4096
[3,]    5   25  125   625   3125   15625
[4,]    6   36  216  1296   7776   46656
[5,]    7   49  343  2401  16807  117649
[6,]    8   64  512  4096  32768  262144
[7,]    9   81  729  6561  59049  531441
[8,]   10  100 1000 10000 100000 1000000

David L Carlson
Anthropology Department
Texas A&M University

On Mon, May 4, 2020 at 2:18 PM David Carlson <dcarlson at tamu.edu> wrote:

> The FUN= argument must be a vectorized function (see the documentation,
> ?outer), but the function rolldie takes only scalar values as arguments:
>
> rolldie(x, y)
> Error in rep("X", times) : invalid 'times' argument
> In addition: Warning messages:
> 1: In 1:times : numerical expression has 6 elements: only the first used
> 2: In 1:nsides : numerical expression has 8 elements: only the first used
>
> David Carlson
> Anthropology Department
> Texas A&M University
>
> On Mon, May 4, 2020 at 1:54 PM Yousri Fanous <yousri.fanous at gmail.com>
> wrote:
>
>> Hello
>>
>> From outer help page:
>> outer takes two vectors
>> <
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__renenyffenegger.ch_notes_development_languages_R_data-2Dstructures_vector_index&d=DwICAg&c=u6LDEWzohnDQ01ySGnxMzg&r=VAaHUElasUXjP9TzIcfIrXdkDpHnJBBZ9Q1u5LcXz9s&m=IdpldO8NxT8R9A1xPy5xD8VynSPRwZchLY3bUEufCYE&s=aLWLgMoHHIk7DaXgX4L0emzN-KDe2WFMYPpVwZuk35U&e=
>> >
>> and a function (that itself takes two arguments) and builds a matrix
>> <
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__renenyffenegger.ch_notes_development_languages_R_data-2Dstructures_matrix_index&d=DwICAg&c=u6LDEWzohnDQ01ySGnxMzg&r=VAaHUElasUXjP9TzIcfIrXdkDpHnJBBZ9Q1u5LcXz9s&m=IdpldO8NxT8R9A1xPy5xD8VynSPRwZchLY3bUEufCYE&s=j5k3zAaxPR8LMHbTe72GTbSGOV_RE5K1Uc2jytaB8SE&e=
>> >
>> by calling the given function for each combination of the elements in the
>> two vectors.
>>
>> x<-1:6
>> y<-3:10
>>
>>  m<-outer (x,y,function (x,y) rnorm(x,y))
>> works as expected.
>>
>> But now when I replace rnorm with rolldie from package (prob) outer
>> complains
>>  library (prob)
>> m<-outer (x,y,function (x,y) nrow(rolldie(x,y)))
>> Error in rep("X", times) : invalid 'times' argument
>> In addition: Warning messages:
>> 1: In 1:times : numerical expression has 48 elements: only the first used
>> 2: In 1:nsides : numerical expression has 48 elements: only the first used
>>
>> nrow(rolldie(5,4))
>> [1] 1024
>>
>> 1) why outer is failing with rolldie?
>> 2) What does the error mean?
>>
>> As a workaround I can do this thru a double loop, but I was hoping to get
>> a
>> more efficient way.
>>
>> Thanks for the help
>>
>> Yousri Fanous
>> Software developer
>> IBM Canada
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=u6LDEWzohnDQ01ySGnxMzg&r=VAaHUElasUXjP9TzIcfIrXdkDpHnJBBZ9Q1u5LcXz9s&m=IdpldO8NxT8R9A1xPy5xD8VynSPRwZchLY3bUEufCYE&s=6jxhmxrSNqyZS4-oU2g8r2R0LEZ0yhtSm-4GdfP0Cbk&e=
>> PLEASE do read the posting guide
>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=u6LDEWzohnDQ01ySGnxMzg&r=VAaHUElasUXjP9TzIcfIrXdkDpHnJBBZ9Q1u5LcXz9s&m=IdpldO8NxT8R9A1xPy5xD8VynSPRwZchLY3bUEufCYE&s=rFq4TmerLd5tegHnWSbx5ISdfDNks-TIa9Whne6bBaM&e=
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From @purd|e@@ @end|ng |rom gm@||@com  Mon May  4 21:36:33 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Tue, 5 May 2020 07:36:33 +1200
Subject: [R] COVID-19 datasets...
In-Reply-To: <CALrbzg1OHAxZ+4yh7LwdADrynhOrj+yaSzWZwCMX7qKZsbMpBg@mail.gmail.com>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
 <CALrbzg1OHAxZ+4yh7LwdADrynhOrj+yaSzWZwCMX7qKZsbMpBg@mail.gmail.com>
Message-ID: <CAB8pepypKNhY22No4QPAZV48-oO11DodcPgHqa9P8RwmeUFetg@mail.gmail.com>

While we're on this topic...

I was interested in testing hypotheses (plural) that acid base
chemistry (blood H+/HCO3/PCO2) or closely-related metabolic processes
could influence the rate of viral replication, possibly by having some
sort of effect on the acidity of phagocytes, vesicles or other
cellular/intracellular-level "host" environments.

There appears to be a relationship between age and blood pH, with a
possibility of a change point around retirement age. This could (being
somewhat speculative) be related to the differences we see in
mortality rates by age.

However, I've got no idea how to test the second part of the
hypotheses, that blood acidity (or any other metabolic process) could
influence the acidity of phagocytes or vesicles.

Suggestions welcome...


From @purd|e@@ @end|ng |rom gm@||@com  Mon May  4 21:40:00 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Tue, 5 May 2020 07:40:00 +1200
Subject: [R] COVID-19 datasets...
In-Reply-To: <CAB8pepypKNhY22No4QPAZV48-oO11DodcPgHqa9P8RwmeUFetg@mail.gmail.com>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
 <CALrbzg1OHAxZ+4yh7LwdADrynhOrj+yaSzWZwCMX7qKZsbMpBg@mail.gmail.com>
 <CAB8pepypKNhY22No4QPAZV48-oO11DodcPgHqa9P8RwmeUFetg@mail.gmail.com>
Message-ID: <CAB8pepy3BsWyrp8GUOaWEVcHW9oX6pRoRiz3ixsTYF9a+u4S5g@mail.gmail.com>

sorry, blood acidity probably wasn't the best choice of words


From bgunter@4567 @end|ng |rom gm@||@com  Mon May  4 22:42:44 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 4 May 2020 13:42:44 -0700
Subject: [R] COVID-19 datasets...
In-Reply-To: <CAB8pepy3BsWyrp8GUOaWEVcHW9oX6pRoRiz3ixsTYF9a+u4S5g@mail.gmail.com>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
 <CALrbzg1OHAxZ+4yh7LwdADrynhOrj+yaSzWZwCMX7qKZsbMpBg@mail.gmail.com>
 <CAB8pepypKNhY22No4QPAZV48-oO11DodcPgHqa9P8RwmeUFetg@mail.gmail.com>
 <CAB8pepy3BsWyrp8GUOaWEVcHW9oX6pRoRiz3ixsTYF9a+u4S5g@mail.gmail.com>
Message-ID: <CAGxFJbQ8uyzsJCSnYG+oeB3rw4BDt7RNNJEUWXZR-JM6-spBGg@mail.gmail.com>

Suggestion: Please stay on topic.

This list is R-Help, not about scientific discussions or even what
statistical procedures might be used for specific research questions.
Perhaps https://stats.stackexchange.com/  for the latter.


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Mon, May 4, 2020 at 12:55 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
>
> sorry, blood acidity probably wasn't the best choice of words
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @purd|e@@ @end|ng |rom gm@||@com  Mon May  4 22:53:01 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Tue, 5 May 2020 08:53:01 +1200
Subject: [R] COVID-19 datasets...
In-Reply-To: <CAGxFJbQ8uyzsJCSnYG+oeB3rw4BDt7RNNJEUWXZR-JM6-spBGg@mail.gmail.com>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
 <CALrbzg1OHAxZ+4yh7LwdADrynhOrj+yaSzWZwCMX7qKZsbMpBg@mail.gmail.com>
 <CAB8pepypKNhY22No4QPAZV48-oO11DodcPgHqa9P8RwmeUFetg@mail.gmail.com>
 <CAB8pepy3BsWyrp8GUOaWEVcHW9oX6pRoRiz3ixsTYF9a+u4S5g@mail.gmail.com>
 <CAGxFJbQ8uyzsJCSnYG+oeB3rw4BDt7RNNJEUWXZR-JM6-spBGg@mail.gmail.com>
Message-ID: <CAB8pepyFb-SbWaufy52kE7OLPvex9-dWboHntWOqdynBx8EesA@mail.gmail.com>

Yes, you're right.

Note that stack exchange is 10x more likely to flag my post as
off-topic, and I was looking for *interesting* applications of R.
There's only so many times one can use Fisher's iris data...


On Tue, May 5, 2020 at 8:42 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
> Suggestion: Please stay on topic.
>
> This list is R-Help, not about scientific discussions or even what
> statistical procedures might be used for specific research questions.
> Perhaps https://stats.stackexchange.com/  for the latter.
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
> On Mon, May 4, 2020 at 12:55 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
> >
> > sorry, blood acidity probably wasn't the best choice of words
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From @purd|e@@ @end|ng |rom gm@||@com  Tue May  5 00:11:08 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Tue, 5 May 2020 10:11:08 +1200
Subject: [R] COVID-19 datasets...
In-Reply-To: <961343375.1407915.1588617072873@connect.xfinity.com>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
Message-ID: <CAB8pepyuJ-vLVG2NmigaKF23dmeP0Y_GFpPSVQAp-A_eNLGX_w@mail.gmail.com>

I don't know if this is useful of not.
But here's some dis-aggregated US data.
It's about 10 days old, and I need to improve the dis-aggregation algorithm.
------------------------------------------------------
age <- 0:80
rel.mort.rate <- c (0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
5.97119198173175e-14, 1.49279799543293e-12,
1.80329997848298e-11, 1.40323011570696e-10, 7.91302361419091e-10,
3.45021443896442e-09, 0.0000000121231319447505, 0.0000000353442018829079,
0.0000000874198628343878, 0.000000186810950489511, 0.000000350496608960251,
0.000000586292710567257, 0.00000088809586113832, 0.00000123828572996647,
0.00000161636339411418, 0.00000200770373248443, 0.00000240656741448015,
0.00000281336507897391, 0.00000323131179486445, 0.00000366643530132155,
0.00000412942874923419, 0.00000463520698193433, 0.00000519914074326175,
0.00000583299029582447, 0.00000654330938109396, 0.00000733211405261902,
0.00000819881232642656, 0.00000914397689953956, 0.0000101752369848173,
0.0000113121562971092, 0.0000125849353938131, 0.0000140250299768815,
0.0000156519020717643, 0.0000174632527695486, 0.000019434044086,
0.0000215255604245307, 0.0000237027683240887, 0.0000259554459399679,
0.0000283147468915187, 0.0000308551986409676, 0.0000336771894650306,
0.0000368753737436115, 0.0000405063605263262, 0.0000445688466005247,
0.0000490032796563514, 0.0000537118848883967, 0.0000585949512128272,
0.0000635935967055111, 0.0000687239859205501, 0.0000740884186923496,
0.0000798570919656766, 0.0000862259973389807, 0.0000933650267485426,
0.000101373734773254, 0.000110260898647345, 0.0001199575254273,
0.000130361177821875, 0.000141396704797265, 0.000153071840957868,
0.00016550903061107, 0.000178943792317475, 0.000193689554660746,
0.000210077052171998, 0.00022838282771009, 0.000248763951569319,
0.000271212123374848, 0.000295531695409173, 0.000321339382531619,
0.000348084036019435, 0.000375091815336575, 0.000401647747211891)

names (rel.mort.rate) <- age

options (scipen=4)
mar <- par ("mar")
mar [2] <- mar [2] + 1
p0 <- par (mar=mar)
barplot (rel.mort.rate [41:81], ylim = c (0, 0.0004), xlab="age",
ylab="relative mortality rate\n(ndeaths / npeople | age)")
par (p0)


From byron_dom @end|ng |rom y@hoo@com  Tue May  5 01:46:12 2020
From: byron_dom @end|ng |rom y@hoo@com (Byron Dom)
Date: Mon, 4 May 2020 23:46:12 +0000 (UTC)
Subject: [R] Problems installing rgl (3D graphics package). Error messages
 "... not available, " "non-zero exit status" and so on
References: <1265948541.865911.1588635972728.ref@mail.yahoo.com>
Message-ID: <1265948541.865911.1588635972728@mail.yahoo.com>

I want to install the 3D graphics rgl.?
I'm running R version 3.2.3 on Windows version "10.0.18362 N/A Build 18362".
I've been using R for about 15 years but only as a casual user. I have installed two packages in the past, but the last was several years ago.
When I issued the command "install.packages("rgl",dependencies=TRUE)," The output I got included quite a few error messages. I've pasted below some of the output that came at the end. There was so much output that I've only included the parts that contained the error and warning messages, which was at the end. I am happy to provide all of the output, if that would help in diagnosing the problem(s).
My questions about this are the obvious ones:(1) What is causing it??(2) How can I fix it?
I'm also wondering if installing R 4.0 before installing rgl, would fix the problems. I've been planning to do that in the near future anyway.
Thanks in advance for any help.
? - - Byron Dom

ERROR: dependency 'processx' is not available for package 'callr'* removing 'C:/Users/byron/Documents/R/win-library/3.2/callr'* installing *source* package 'magic' ...** package 'magic' successfully unpacked and MD5 sums checked** R** data** inst** preparing package for lazy loadingWarning: package 'abind' was built under R version 3.2.5** help*** installing help indices** building package indices** installing vignettes** testing if installed package can be loadedWarning: package 'abind' was built under R version 3.2.5* DONE (magic)* installing *source* package 'stringr' ...** package 'stringr' successfully unpacked and MD5 sums checked** R** data*** moving datasets to lazyload DB** inst** preparing package for lazy loadingError in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) :?? namespace 'stringi' 1.1.5 is being loaded, but >= 1.1.7 is requiredERROR: lazy loading failed for package 'stringr'* removing 'C:/Users/byron/Documents/R/win-library/3.2/stringr'* installing *source* package 'tinytex' ...** package 'tinytex' successfully unpacked and MD5 sums checked** R** inst** preparing package for lazy loading** help*** installing help indices** building package indices** testing if installed package can be loaded* DONE (tinytex)ERROR: dependencies 'htmltools', 'later', 'promises', 'rlang', 'fastmap' are not available for package 'shiny'* removing 'C:/Users/byron/Documents/R/win-library/3.2/shiny'ERROR: dependency 'htmltools' is not available for package 'crosstalk'* removing 'C:/Users/byron/Documents/R/win-library/3.2/crosstalk'* installing *source* package 'rstudioapi' ...** package 'rstudioapi' successfully unpacked and MD5 sums checked** R** inst** preparing package for lazy loading** help*** installing help indices*** copying figures** building package indices** installing vignettes** testing if installed package can be loaded* DONE (rstudioapi)ERROR: dependencies 'shiny', 'htmltools' are not available for package 'miniUI'* removing 'C:/Users/byron/Documents/R/win-library/3.2/miniUI'ERROR: dependency 'callr' is not available for package 'webshot'* removing 'C:/Users/byron/Documents/R/win-library/3.2/webshot'ERROR: dependency 'stringr' is not available for package 'knitr'* removing 'C:/Users/byron/Documents/R/win-library/3.2/knitr'ERROR: dependencies 'shiny', 'miniUI', 'htmltools', 'knitr', 'webshot' are not available for package 'manipulateWidget'* removing 'C:/Users/byron/Documents/R/win-library/3.2/manipulateWidget'ERROR: dependencies 'knitr', 'htmltools', 'stringr' are not available for package 'rmarkdown'* removing 'C:/Users/byron/Documents/R/win-library/3.2/rmarkdown'
The downloaded source packages are in? ? ? ? ?C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb\downloaded_packages?

There were 18 warnings (use warnings() to see them)> warnings()Warning messages:1: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/callr_3.4.3.tar.gz' had status 12: In install.packages("rgl", dependencies = TRUE) :? installation of package ?callr? had non-zero exit status3: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/stringr_1.4.0.tar.gz' had status 14: In install.packages("rgl", dependencies = TRUE) :? installation of package ?stringr? had non-zero exit status5: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/shiny_1.4.0.2.tar.gz' had status 16: In install.packages("rgl", dependencies = TRUE) :? installation of package ?shiny? had non-zero exit status7: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/crosstalk_1.1.0.1.tar.gz' had status 18: In install.packages("rgl", dependencies = TRUE) :? installation of package ?crosstalk? had non-zero exit status9: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/miniUI_0.1.1.1.tar.gz' had status 110: In install.packages("rgl", dependencies = TRUE) :? installation of package ?miniUI? had non-zero exit status11: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/webshot_0.5.2.tar.gz' had status 112: In install.packages("rgl", dependencies = TRUE) :? installation of package ?webshot? had non-zero exit status13: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/knitr_1.28.tar.gz' had status 114: In install.packages("rgl", dependencies = TRUE) :? installation of package ?knitr? had non-zero exit status15: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/manipulateWidget_0.10.1.tar.gz' had status 116: In install.packages("rgl", dependencies = TRUE) :? installation of package ?manipulateWidget? had non-zero exit status17: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/rmarkdown_2.1.tar.gz' had status 118: In install.packages("rgl", dependencies = TRUE) :? installation of package ?rmarkdown? had non-zero exit status>?
> date()[1] "Mon May 04 15:55:35 2020"


	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Tue May  5 01:55:23 2020
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Mon, 4 May 2020 19:55:23 -0400
Subject: [R] 
 Problems installing rgl (3D graphics package). Error messages
 "... not available, " "non-zero exit status" and so on
In-Reply-To: <1265948541.865911.1588635972728@mail.yahoo.com>
References: <1265948541.865911.1588635972728.ref@mail.yahoo.com>
 <1265948541.865911.1588635972728@mail.yahoo.com>
Message-ID: <bf9e480b-db97-bb67-01a2-0280f9a3eb67@gmail.com>

On 04/05/2020 7:46 p.m., Byron Dom via R-help wrote:
> I want to install the 3D graphics rgl.
> I'm running R version 3.2.3 on Windows version "10.0.18362 N/A Build 18362".

That's a four year old version of R.  You'll likely have to find 
packages of a similar age to be able to get it to work, or upgrade to 
3.6.3 (the previous version, very stable) or 4.0.0 (the current version).

Duncan Murdoch

> I've been using R for about 15 years but only as a casual user. I have installed two packages in the past, but the last was several years ago.
> When I issued the command "install.packages("rgl",dependencies=TRUE)," The output I got included quite a few error messages. I've pasted below some of the output that came at the end. There was so much output that I've only included the parts that contained the error and warning messages, which was at the end. I am happy to provide all of the output, if that would help in diagnosing the problem(s).
> My questions about this are the obvious ones:(1) What is causing it??(2) How can I fix it?
> I'm also wondering if installing R 4.0 before installing rgl, would fix the problems. I've been planning to do that in the near future anyway.
> Thanks in advance for any help.
>  ? - - Byron Dom
> 
> ERROR: dependency 'processx' is not available for package 'callr'* removing 'C:/Users/byron/Documents/R/win-library/3.2/callr'* installing *source* package 'magic' ...** package 'magic' successfully unpacked and MD5 sums checked** R** data** inst** preparing package for lazy loadingWarning: package 'abind' was built under R version 3.2.5** help*** installing help indices** building package indices** installing vignettes** testing if installed package can be loadedWarning: package 'abind' was built under R version 3.2.5* DONE (magic)* installing *source* package 'stringr' ...** package 'stringr' successfully unpacked and MD5 sums checked** R** data*** moving datasets to lazyload DB** inst** preparing package for lazy loadingError in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) :?? namespace 'stringi' 1.1.5 is being loaded, but >= 1.1.7 is requiredERROR: lazy loading failed for package 'stringr'* removing 'C:/Users/byron/Documents/R/win-library/3.2/stringr'* installing *source* package 'tinytex' ...** package 'tinytex' successfully unpacked and MD5 sums checked** R** inst** preparing package for lazy loading** help*** installing help indices** building package indices** testing if installed package can be loaded* DONE (tinytex)ERROR: dependencies 'htmltools', 'later', 'promises', 'rlang', 'fastmap' are not available for package 'shiny'* removing 'C:/Users/byron/Documents/R/win-library/3.2/shiny'ERROR: dependency 'htmltools' is not available for package 'crosstalk'* removing 'C:/Users/byron/Documents/R/win-library/3.2/crosstalk'* installing *source* package 'rstudioapi' ...** package 'rstudioapi' successfully unpacked and MD5 sums checked** R** inst** preparing package for lazy loading** help*** installing help indices*** copying figures** building package indices** installing vignettes** testing if installed package can be loaded* DONE (rstudioapi)ERROR: dependencies 'shiny', 'htmltools' are not available for package 'miniUI'* removing 'C:/Users/byron/Documents/R/win-library/3.2/miniUI'ERROR: dependency 'callr' is not available for package 'webshot'* removing 'C:/Users/byron/Documents/R/win-library/3.2/webshot'ERROR: dependency 'stringr' is not available for package 'knitr'* removing 'C:/Users/byron/Documents/R/win-library/3.2/knitr'ERROR: dependencies 'shiny', 'miniUI', 'htmltools', 'knitr', 'webshot' are not available for package 'manipulateWidget'* removing 'C:/Users/byron/Documents/R/win-library/3.2/manipulateWidget'ERROR: dependencies 'knitr', 'htmltools', 'stringr' are not available for package 'rmarkdown'* removing 'C:/Users/byron/Documents/R/win-library/3.2/rmarkdown'
> The downloaded source packages are in? ? ? ? ?C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb\downloaded_packages?
> 
> There were 18 warnings (use warnings() to see them)> warnings()Warning messages:1: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/callr_3.4.3.tar.gz' had status 12: In install.packages("rgl", dependencies = TRUE) :? installation of package ?callr? had non-zero exit status3: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/stringr_1.4.0.tar.gz' had status 14: In install.packages("rgl", dependencies = TRUE) :? installation of package ?stringr? had non-zero exit status5: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/shiny_1.4.0.2.tar.gz' had status 16: In install.packages("rgl", dependencies = TRUE) :? installation of package ?shiny? had non-zero exit status7: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/crosstalk_1.1.0.1.tar.gz' had status 18: In install.packages("rgl", dependencies = TRUE) :? installation of package ?crosstalk? had non-zero exit status9: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/miniUI_0.1.1.1.tar.gz' had status 110: In install.packages("rgl", dependencies = TRUE) :? installation of package ?miniUI? had non-zero exit status11: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/webshot_0.5.2.tar.gz' had status 112: In install.packages("rgl", dependencies = TRUE) :? installation of package ?webshot? had non-zero exit status13: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/knitr_1.28.tar.gz' had status 114: In install.packages("rgl", dependencies = TRUE) :? installation of package ?knitr? had non-zero exit status15: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/manipulateWidget_0.10.1.tar.gz' had status 116: In install.packages("rgl", dependencies = TRUE) :? installation of package ?manipulateWidget? had non-zero exit status17: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/rmarkdown_2.1.tar.gz' had status 118: In install.packages("rgl", dependencies = TRUE) :? installation of package ?rmarkdown? had non-zero exit status>
>> date()[1] "Mon May 04 15:55:35 2020"
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From r@oknz @end|ng |rom gm@||@com  Tue May  5 02:41:53 2020
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Tue, 5 May 2020 12:41:53 +1200
Subject: [R] if else statement
In-Reply-To: <CAF9-5jODa79FKHbkmRqD4T2Lzak5GrQL4TWji4pPkTpC3ndd4Q@mail.gmail.com>
References: <CAF9-5jODa79FKHbkmRqD4T2Lzak5GrQL4TWji4pPkTpC3ndd4Q@mail.gmail.com>
Message-ID: <CABcYAdKnu-x6hFBNfJyALNEptw9fJZcsFhdx=sfuYi2z-RQcCw@mail.gmail.com>

Your ifelse expression looks fine.  What goes wrong with it?

On Tue, 5 May 2020 at 05:16, Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> Hello,
>
> I have a data frame like this:
>
> > head(b)
>        FID   IID FLASER PLASER
> 1: fam1000 G1000      1      1
> 2: fam1001 G1001      1      1
> 3: fam1003 G1003      1      2
> 4: fam1005 G1005      1      1
> 5: fam1009 G1009      NA      2
> 6: fam1052 G1052      1      1
> ...
> > unique(b$PLASER)
> [1]  1  2 NA
> > unique(b$FLASER)
> [1]  1  2 NA
>
> how can I do if else statement so that I am creating a
> PHENO =2 if b$FLASER=2 or b$PLASER=2
> PHENO=1 if b$FLASER=1 and b$PLASER=1
> otherwise PHENO=NA
>
> I tried this but I am not sure if this is correct:
> b$pheno=ifelse(b$PLASER==1 & b$FLASER==1,1,ifelse(b$PLASER==2 |
> b$FLASER==2,2,NA))
>
> Thanks
> Ana
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From petr@p|k@| @end|ng |rom prechez@@cz  Tue May  5 08:46:55 2020
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Tue, 5 May 2020 06:46:55 +0000
Subject: [R] COVID-19 datasets...
In-Reply-To: <01B72E8D-CDF6-445F-8E95-EB8D3158CC77@comcast.net>
References: <E3562BF5-42BC-43D6-96AB-E043B6EA1C57@jsasoc.com>
 <01B72E8D-CDF6-445F-8E95-EB8D3158CC77@comcast.net>
Message-ID: <942c647ec2ac4309a6c3513670af1052@SRVEXCHCM1302.precheza.cz>

Another option is

https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide

Together with instruments from

https://www.repidemicsconsortium.org/

Cheers
Petr

Here is some simple code

library(EpiEstim)
library(ggplot2)
library(lubridate)
library(incidence)
library(distcrete)
library(epitrix)
library(readxl)

mena <- c("Austria", "Czechia", "Germany", "Italy", "Japan", "Russia", "South_Korea", 
"Spain", "Sweden", "Taiwan", "United_States_of_America", "United_Kingdom")

plot_Ri <- function(estimate_R_obj) {
    p_I <- plot(estimate_R_obj, "incid") + ggtitle(staty[vyber][i])  # plots the incidence
    p_SI <- plot(estimate_R_obj, "SI")  # plots the serial interval distribution
    p_Ri <- plot(estimate_R_obj, "R") + ylim(c(0,5))
    return(gridExtra::grid.arrange(p_I, p_Ri, ncol = 1))
}

data <- read_excel("covid.xlsx")
data <- as.data.frame(data)
staty <- levels(factor(data[,7]))
vyber <- which(staty %in% mena)
staty[vyber]

# covid.xlsx is downloaded data

vyber <- which(staty %in% mena)
vyber <- vyber[-6]
staty[vyber]
ddd <- vector("list", length=length(vyber))
pdf("grafy2.pdf")
for (i in 1:length(vyber)) {
temp <- data[data[,7]==staty[vyber][i],]
temp$cas <- ymd(temp$dateRep)
ooo <- order(temp$cas)
temp <- temp[ooo,]
temp<- temp[-1,]
temp <- temp[-(1:min(which(temp$cases>0))-1),]
head(temp)
test <- temp[, c(12,5)]
names(test) <- c("date", "I")
test$I <- abs(test$I)
inc <- rep(test$date, test$I)
inci <- incidence(inc)
peak <- find_peak(inci)
fit <- incidence::fit(inci, split=peak)
print(plot(inci, fit = fit)+ggtitle(staty[vyber][i]))
ddd[[i]] <- rbind(fit$before$info$doubling.conf, fit$after$info$halving.conf)
vysled <- estimate_R(test, method = "uncertain_si",
    config = make_config(list(mean_si = 4, std_mean_si = 2,
        min_mean_si = 1, max_mean_si = 8.4, std_si = 2.4, std_std_si = 1,
        min_std_si = 0.5, max_std_si = 4, n1 = 1000, n2 = 1000)))
print(plot_Ri(vysled))
}
dev.off()
names(ddd) <- staty[vyber]
ddd


> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Bernard Comcast
> Sent: Monday, May 4, 2020 8:56 PM
> To: James Spottiswoode <james at jsasoc.com>
> Cc: r-help at r-project.org
> Subject: Re: [R] COVID-19 datasets...
> 
> Thanks, i will take a look
> 
> Bernard
> Sent from my iPhone so please excuse the spelling!"
> 
> > On May 4, 2020, at 2:49 PM, James Spottiswoode <james at jsasoc.com>
> wrote:
> >
> > ?Sure. COVID-19 Data Repository by the Center for Systems Science and
> Engineering (CSSE) at Johns Hopkins University is available here:
> >
> > https://github.com/CSSEGISandData/COVID-19
> >
> > All in csv fiormat.
> >
> >
> >> On May 4, 2020, at 11:31 AM, Bernard McGarvey
> <mcgarvey.bernard at comcast.net> wrote:
> >>
> >> Just curious does anyone know of a website that has data available in a
> format that R can download and analyze?
> >>
> >> Thanks
> >>
> >>
> >> Bernard McGarvey
> >>
> >>
> >> Director, Fort Myers Beach Lions Foundation, Inc.
> >>
> >>
> >> Retired (Lilly Engineering Fellow).
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> > James Spottiswoode
> > Applied Mathematics & Statistics
> > (310) 270 6220
> > jamesspottiswoode Skype
> > james at jsasoc.com
> >
> >
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From petr@p|k@| @end|ng |rom prechez@@cz  Tue May  5 09:12:53 2020
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Tue, 5 May 2020 07:12:53 +0000
Subject: [R] if else statement
In-Reply-To: <5862eb7d-db98-98af-280f-1a5ffbcc2587@sapo.pt>
References: <CAF9-5jODa79FKHbkmRqD4T2Lzak5GrQL4TWji4pPkTpC3ndd4Q@mail.gmail.com>
 <5862eb7d-db98-98af-280f-1a5ffbcc2587@sapo.pt>
Message-ID: <24c9f303c2bb4194805097ff6a28d95d@SRVEXCHCM1302.precheza.cz>

Hi

another possible version 

b$pheno <- ((b$FLASER==2) | (b$PLASER==2))+1

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Rui Barradas
> Sent: Monday, May 4, 2020 8:32 PM
> To: sokovic.anamarija at gmail.com; r-help <r-help at r-project.org>
> Subject: Re: [R] if else statement
> 
> Hello,
> 
> Here is a way, using logical indices.
> 
> b$pheno <- NA
> b$pheno[b$FLASER == 1 & b$PLASER == 1] <- 1 b$pheno[b$FLASER == 2 |
> b$PLASER == 2] <- 2
> 
> 
> Hope this helps,
> 
> Rui Barradas
> 
> ?s 18:15 de 04/05/20, Ana Marija escreveu:
> > Hello,
> >
> > I have a data frame like this:
> >
> >> head(b)
> >         FID   IID FLASER PLASER
> > 1: fam1000 G1000      1      1
> > 2: fam1001 G1001      1      1
> > 3: fam1003 G1003      1      2
> > 4: fam1005 G1005      1      1
> > 5: fam1009 G1009      NA      2
> > 6: fam1052 G1052      1      1
> > ...
> >> unique(b$PLASER)
> > [1]  1  2 NA
> >> unique(b$FLASER)
> > [1]  1  2 NA
> >
> > how can I do if else statement so that I am creating a PHENO =2 if
> > b$FLASER=2 or b$PLASER=2
> > PHENO=1 if b$FLASER=1 and b$PLASER=1
> > otherwise PHENO=NA
> >
> > I tried this but I am not sure if this is correct:
> > b$pheno=ifelse(b$PLASER==1 & b$FLASER==1,1,ifelse(b$PLASER==2 |
> > b$FLASER==2,2,NA))
> >
> > Thanks
> > Ana
> >
> > 	[[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From p_conno||y @end|ng |rom @||ng@hot@co@nz  Tue May  5 10:37:17 2020
From: p_conno||y @end|ng |rom @||ng@hot@co@nz (Patrick Connolly)
Date: Tue, 5 May 2020 20:37:17 +1200
Subject: [R] PCRE configure problem with R-4.0.0
In-Reply-To: <CA+vqiLEcbaU=Wm7dF=eEdygavtBRx1oxSaR8FyjX16GyBrYj0Q@mail.gmail.com>
References: <20200504041542.GA7518@slingshot.co.nz>
 <20200504100010.5d771a78@trisector>
 <CA+vqiLEcbaU=Wm7dF=eEdygavtBRx1oxSaR8FyjX16GyBrYj0Q@mail.gmail.com>
Message-ID: <20200505083717.GB7518@slingshot.co.nz>

On Mon, 04-May-2020 at 11:03AM -0400, Ista Zahn wrote:

|> On Mon, May 4, 2020 at 3:51 AM Ivan Krylov <krylov.r00t at gmail.com> wrote:
|> >
|> > First of all, you mentioned Linux Mint, so you might get better advice
|> > on R-SIG-Debian mailing list.
|> >
|> > On Mon, 4 May 2020 16:15:42 +1200
|> > Patrick Connolly <p_connolly at slingshot.co.nz> wrote:
|> >
|> > >There are quite a lot of packages in the repository for Linux Mint
|> > >17.2 with 'pcre' in the name and these are installed:
|> >
|> > >Apparantly the '3' doesn't indicate an updated '2' version
|> >
|> > The funny thing about libpcre3 is that it is the old PCRE1 version,
|> > third ABI-incompatible upgrade of it [*], and libpcre2 (available in
|> > current releases of Linux Mint, Ubuntu and Debian) is supposed to be
|> > the newer PCRE2.
|> >
|> > Linux Mint 17.2 is based on Ubuntu 14.04, which has been released in
|> > April 2014, while PCRE2 has been released in 2015.
|> 
|> Moreover, support for 17.2 ended over a year ago (according to
|> https://en.wikipedia.org/wiki/Linux_Mint_version_history). I suggest
|> upgrading to a supported version.

Thanks for making that clear -- though I don't relish the hassle of
upgrading an OS.  I was quite happy with the features of Mint 17.x.

[...]


-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}                   Great minds discuss ideas    
 _( Y )_  	         Average minds discuss events 
(:_~*~_:)                  Small minds discuss people  
 (_)-(_)  	                      ..... Eleanor Roosevelt
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From |@t@z@hn @end|ng |rom gm@||@com  Tue May  5 12:28:52 2020
From: |@t@z@hn @end|ng |rom gm@||@com (Ista Zahn)
Date: Tue, 5 May 2020 06:28:52 -0400
Subject: [R] PCRE configure problem with R-4.0.0
In-Reply-To: <20200505083717.GB7518@slingshot.co.nz>
References: <20200504041542.GA7518@slingshot.co.nz>
 <20200504100010.5d771a78@trisector>
 <CA+vqiLEcbaU=Wm7dF=eEdygavtBRx1oxSaR8FyjX16GyBrYj0Q@mail.gmail.com>
 <20200505083717.GB7518@slingshot.co.nz>
Message-ID: <CA+vqiLE1rjP6AtgUNkxVZdWX4D4-GTRB_104fP7qh8SzCeFn6Q@mail.gmail.com>

<snip>

> |> > Linux Mint 17.2 is based on Ubuntu 14.04, which has been released in
> |> > April 2014, while PCRE2 has been released in 2015.
> |>
> |> Moreover, support for 17.2 ended over a year ago (according to
> |> https://en.wikipedia.org/wiki/Linux_Mint_version_history). I suggest
> |> upgrading to a supported version.
>
> Thanks for making that clear -- though I don't relish the hassle of
> upgrading an OS.  I was quite happy with the features of Mint 17.x.
>

Sure, but there is no free lunch and as your system gets older and
older you'll find that more and more modern software doesn't work with
it. Additionally there are increasing security risks because the
vendor is no longer releasing security patches.

That said, if you really really wanna, you can use
https://docs.conda.io/en/latest/, https://spack.io/, or similar to
install recent software releases on old systems.

Best,
Ista

> [...]
>
>
> --
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>    ___    Patrick Connolly
>  {~._.~}                   Great minds discuss ideas
>  _( Y )_                 Average minds discuss events
> (:_~*~_:)                  Small minds discuss people
>  (_)-(_)                              ..... Eleanor Roosevelt
>
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From mehd|d@dkh@h91 @end|ng |rom gm@||@com  Tue May  5 12:42:06 2020
From: mehd|d@dkh@h91 @end|ng |rom gm@||@com (Mehdi Dadkhah)
Date: Tue, 5 May 2020 15:12:06 +0430
Subject: [R] Converting CSV file to UTF-8 encoding
In-Reply-To: <CAGN=ytO6qi4M63X7DucyfCKz8HNNbT_SzrwALQU4cRgrPgeVeg@mail.gmail.com>
References: <CAGN=ytO6qi4M63X7DucyfCKz8HNNbT_SzrwALQU4cRgrPgeVeg@mail.gmail.com>
Message-ID: <CAGN=ytPxB8xW62BuGxute_62cYE_1mW9oEK3WjOSnHQuXwcAkA@mail.gmail.com>

Hi,
I hope you are doing well!
I have a CSV file which its encoding is ANSI. How can i change its encoding
to UTF-8 in R?
Many thanks!
With best regards,

-- 
*Mehdi Dadkhah*

	[[alternative HTML version deleted]]


From jr@| @end|ng |rom po@teo@no  Tue May  5 12:56:57 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Tue, 5 May 2020 12:56:57 +0200
Subject: [R] Converting CSV file to UTF-8 encoding
In-Reply-To: <CAGN=ytPxB8xW62BuGxute_62cYE_1mW9oEK3WjOSnHQuXwcAkA@mail.gmail.com>
References: <CAGN=ytO6qi4M63X7DucyfCKz8HNNbT_SzrwALQU4cRgrPgeVeg@mail.gmail.com>
 <CAGN=ytPxB8xW62BuGxute_62cYE_1mW9oEK3WjOSnHQuXwcAkA@mail.gmail.com>
Message-ID: <20200505105657.GA1073515@posteo.no>

On 2020-05-05 15:12 +0430, Mehdi Dadkhah wrote:
> I have a CSV file which its encoding is 
> ANSI. How can i change its encoding to 
> UTF-8 in R?

Hi!

I do not know about ANSI, but to read latin1 
encoded csv files into readr, do this:

Determine that your file is latin1-encoded:

	rasmus at twentyfive ~ % file -i SAA.csv
	SAA.csv: application/csv; charset=iso-8859-1

read it in using readr::read_csv

	locale <- readr::locale(encoding = "latin1")
	SAA <- suppressMessages(
	  readr::read_csv(file="SAA.csv",
	                  locale=locale))

Best,
Rasmus


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Tue May  5 13:07:10 2020
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Tue, 5 May 2020 13:07:10 +0200
Subject: [R] How to use pakcage R0
Message-ID: <CAMk+s2T-X8wXp+owCj3E1Cmez-imgE-fByNLNY_XDibHXOgPqQ@mail.gmail.com>

Dear all,
I have been trying to use the package R0
https://www.rdocumentation.org/packages/R0/versions/1.2-6/topics/estimate.R but
the manual is not so rich of words.
The example given is based on the named vector Germany.1918
```
> library("R0")
> data(Germany.1918)
> Germany.1918
1918-09-29 1918-09-30 1918-10-01 1918-10-02 1918-10-03 1918-10-04
1918-10-05
        10          4          4         19          6         13
28
1918-10-06 1918-10-07 1918-10-08 1918-10-09 1918-10-10 1918-10-11
1918-10-12
        23         35         27         42         51         43
78
1918-10-13 1918-10-14 1918-10-15 1918-10-16 1918-10-17 1918-10-18
1918-10-19
        86         80        109        126        126        159
 190
[...]
```

Then it creates a gamma function and applied the estimate.R function:
```
mGT<-generation.time("gamma", c(3, 1.5))
estR0<-estimate.R(Germany.1918, mGT, begin=1, end=27, methods=c("EG", "ML",
"TD", "AR", "SB"),
                  pop.size=100000, nsim=100)
```

I tried with a similar approach for the current epidemics in China:
```
> china_vect
23/01/20 24/01/20 25/01/20 26/01/20 27/01/20 28/01/20 29/01/20 30/01/20
31/01/20
     259      457      688      769     1771     1459     1737     1981
2099
> mGT = generation.time("gamma", c(3, 1.5))   # create distribution
> estR0 = estimate.R(china_vect, mGT, begin=1, end=length(china_vect),
                  methods="EG",
                  pop.size=pop_ch, nsim=100)
Error in integrity.checks(epid, t, GT, begin, end, date.first.obs,
time.step,  :
  If both 'begin'= 1  and 'end'= 103  are provided, they must be of the
same class (dates, character strings or integers).
```
So I gave the value 103 directly (why it did not accept length, is the
first question?) and it worked:
> estR0 = estimate.R(china_vect, mGT, begin=1, end=103,
+                   methods="EG",
+                   pop.size=pop_ch, nsim=100)
Waiting for profiling to be done...
> estR0
Reproduction number estimate using  Exponential Growth  method.
R :  0.3359444[ 0.3209695 , 0.3510899 ]
```
I tried another endpoint, 27 as in the example:
```
> estR0 = estimate.R(china_vect, mGT, begin=1, end=27,
+                   methods="EG",
+                   pop.size=pop_ch, nsim=100)
Waiting for profiling to be done...
Error: no valid set of coefficients has been found: please supply starting
values
In addition: There were 11 warnings (use warnings() to see them)
> warnings()
Warning messages:
1: glm.fit: algorithm did not converge
2: glm.fit: fitted rates numerically 0 occurred
3: glm.fit: fitted rates numerically 0 occurred
4: glm.fit: fitted rates numerically 0 occurred
5: glm.fit: fitted rates numerically 0 occurred
6: glm.fit: fitted rates numerically 0 occurred
7: glm.fit: fitted rates numerically 0 occurred
8: glm.fit: fitted rates numerically 0 occurred
9: glm.fit: fitted rates numerically 0 occurred
10: glm.fit: fitted rates numerically 0 occurred
11: glm.fit: fitted rates numerically 0 occurred
```
Why these errors?
Is there a better tutorial on how to apply this function?
Thank you

	[[alternative HTML version deleted]]


From r@oknz @end|ng |rom gm@||@com  Tue May  5 13:17:31 2020
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Tue, 5 May 2020 23:17:31 +1200
Subject: [R] Converting CSV file to UTF-8 encoding
In-Reply-To: <CAGN=ytPxB8xW62BuGxute_62cYE_1mW9oEK3WjOSnHQuXwcAkA@mail.gmail.com>
References: <CAGN=ytO6qi4M63X7DucyfCKz8HNNbT_SzrwALQU4cRgrPgeVeg@mail.gmail.com>
 <CAGN=ytPxB8xW62BuGxute_62cYE_1mW9oEK3WjOSnHQuXwcAkA@mail.gmail.com>
Message-ID: <CABcYAdLUqbfJJJyYnVh1xNODR4E-BS1+C0CcOj_-1YXGszsLqg@mail.gmail.com>

What do you mean "ANSI"?
Do you mean ASCII?  In that case there is nothing to be done.
Do you mean some member of the ISO 8859 family of 8-bit character sets?
Do you mean some Microsoft-specific code page, such as CP-1252?
(Microsoft CP-437 and CP-1252 "ANSI" but if they have any connection
whatever with ANSI I would appreciate being informed of it.)
If you really do mean the ANSI Extended Latin (ANSEL) character
set, you are out of luck.

If it is supported in your environment, the easiest way is that use the
iconv() function.  That's what it is for.  See ?iconv.

But there is something easier, and that is not to.
Just let R know what the external encoding is, and just read the file.
If you check the documentation of read.csv, ?read.csv
you will find the fileEncoding="..." argument.

fileEncoding: character string: if non-empty declares the encoding used
          on a file (not a connection) so the character data can be
          re-encoded.  See the 'Encoding' section of the help for
          'file', the 'R Data Import/Export Manual' and 'Note'.

At a guess, you  want fileEncoding="WINDOWS-1252".

On Tue, 5 May 2020 at 22:42, Mehdi Dadkhah <mehdidadkhah91 at gmail.com> wrote:
>
> Hi,
> I hope you are doing well!
> I have a CSV file which its encoding is ANSI. How can i change its encoding
> to UTF-8 in R?
> Many thanks!
> With best regards,
>
> --
> *Mehdi Dadkhah*
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From j@me@ @end|ng |rom j@@@oc@com  Mon May  4 20:48:58 2020
From: j@me@ @end|ng |rom j@@@oc@com (James Spottiswoode)
Date: Mon, 4 May 2020 11:48:58 -0700
Subject: [R] COVID-19 datasets...
In-Reply-To: <961343375.1407915.1588617072873@connect.xfinity.com>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
Message-ID: <E3562BF5-42BC-43D6-96AB-E043B6EA1C57@jsasoc.com>

Sure. COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University is available here:

https://github.com/CSSEGISandData/COVID-19

All in csv fiormat.


> On May 4, 2020, at 11:31 AM, Bernard McGarvey <mcgarvey.bernard at comcast.net> wrote:
> 
> Just curious does anyone know of a website that has data available in a format that R can download and analyze?
>  
> Thanks
> 
> 
> Bernard McGarvey
> 
> 
> Director, Fort Myers Beach Lions Foundation, Inc.
> 
> 
> Retired (Lilly Engineering Fellow).
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

James Spottiswoode
Applied Mathematics & Statistics
(310) 270 6220
jamesspottiswoode Skype
james at jsasoc.com




	[[alternative HTML version deleted]]


From h@yden@m@cdon@|d@8778 @end|ng |rom gm@||@com  Mon May  4 14:03:22 2020
From: h@yden@m@cdon@|d@8778 @end|ng |rom gm@||@com (Hayden MacDonald)
Date: Mon, 4 May 2020 08:03:22 -0400
Subject: [R] [R-pkgs] squashinformr: Politely web scrape data from
 SquashInfo in R
Message-ID: <CANTbTwDL04K6fsgVVzFgPmsMJdkZMSfuKHVW5WFrWVbU_aJ3Xw@mail.gmail.com>

Hi all,

I hope this message finds you well. I have developed a new R package,
squashinformr, that allows users to web scrape data on the Professional
Squash Association World Tour and other squash tournaments. Currently,
squashinformr provides functions for accessing data on players, rankings,
and tournaments. Additionally, squashinformr ethically scrapes data from
SquashInfo <http://www.squashinfo.com/> by adhering to `polite` principles
<https://github.com/dmi3kno/polite>. Version 0.1.2 is now available on CRAN!

Here is a blog post introducing the package and some of its uses:
https://needleinthehay.ca/introducing-squashinformr/

and here is the package's GitHub repository:
https://github.com/HaydenMacDonald/squashinformr

Best wishes,

Hayden MacDonald

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From bjoern@boettcher @end|ng |rom tu-dre@den@de  Mon May  4 11:04:06 2020
From: bjoern@boettcher @end|ng |rom tu-dre@den@de (=?UTF-8?Q?Bj=c3=b6rn_B=c3=b6ttcher?=)
Date: Mon, 4 May 2020 11:04:06 +0200
Subject: [R] [R-pkgs] multivariance: Measuring Multivariate Dependence Using
 Distance Multivariance
Message-ID: <47d83b4d-e077-5956-cb49-b252e72818d3@tu-dresden.de>

Dear R-users and developers,

based on a recent series of of papers [1-6] the package 'multivariance' 
(available on CRAN; latest version 2.3.0, 2020-04-23) was developed.
It provides in particular:

+ *fast global tests of independence* for an arbitrary number of 
variables of arbitrary dimensions

+ a detection and visualization algorithm for *higher order dependence 
structures*

+ estimators for multivariate dependence measures which *characterize 
independence*, i.e. the population version is 0 if and only if the 
variables are independent (in contrast to the standard correlation 'cor')

As a side remark, some food for thought: Note that in [3] it is referred 
to over 350 datasets from more than 150 R-packages, which all feature 
some statistical significant higher order dependencies. Some are 
probably artefacts, but in any case it is likely that these have been 
unnoticed and undiscussed so far. Moreover, since it was purely a brute 
force study, this might provide starting points for plenty of research 
by the corresponding field specialists.

Comments and questions on 'multivariance' and the underlying theory are 
welcome.

Best wishes

Bj?rn B?ttcher


References:

[1] B. B?ttcher, M. Keller-Ressel, R.L. Schilling, Detecting 
independence of random vectors: generalized distance covariance and 
Gaussian covariance.
Modern Stochastics: Theory and Applications, Vol. 5, No. 3 (2018) 353-383.
https://www.vmsta.org/journal/VMSTA/article/127/info

[2] B. B?ttcher, M. Keller-Ressel, R.L. Schilling, Distance 
multivariance: New dependence measures for random vectors.
The Annals of Statistics, Vol. 47, No. 5 (2019) 2757-2789.
https://projecteuclid.org/euclid.aos/1564797863

[3] B. B?ttcher, Dependence and Dependence Structures: Estimation and 
Visualization using the Unifying Concept of Distance Multivariance.
Open Statistics, Vol. 1, No. 1 (2020) 1-46.
https://doi.org/10.1515/stat-2020-0001

[4] G. Berschneider, B. B?ttcher, On complex Gaussian random fields, 
Gaussian quadratic forms and sample distance multivariance. Preprint.
https://arxiv.org/abs/1808.07280

[5] B. B?ttcher, Copula versions of distance multivariance and dHSIC via 
the distributional transform -- a general approach to construct 
invariant dependence measures.
Statistics, (2020) 1-18.
https://doi.org/10.1080/02331888.2020.1748029

[6] B. B?ttcher, Notes on the interpretation of dependence measures -- 
Pearson's correlation, distance correlation, distance multicorrelations 
and their copula versions. Preprint.
https://arxiv.org/abs/2004.07649


-- 
Dr. Bj?rn B?ttcher
TU Dresden
Institut f?r Math. Stochastik
D-01062 Dresden, Germany
Phone: +49 (0) 351 463 32423
Fax:   +49 (0) 351 463 37251
Web:   http://www.math.tu-dresden.de/~boettch/

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From cpoiw@rt m@iii@g oii chemo@org@uk  Tue May  5 22:11:03 2020
From: cpoiw@rt m@iii@g oii chemo@org@uk (cpoiw@rt m@iii@g oii chemo@org@uk)
Date: Tue, 05 May 2020 21:11:03 +0100
Subject: [R] How to use pakcage R0
In-Reply-To: <CAMk+s2QuwOdyykoivk0uyzFdynP-0M5pdvJG5iqCSsxOeQ6-oQ@mail.gmail.com>
References: <CAMk+s2T-X8wXp+owCj3E1Cmez-imgE-fByNLNY_XDibHXOgPqQ@mail.gmail.com>
 <cb4dd3eb-6944-4676-b1e9-524f90568eed@email.android.com>
 <CAMk+s2QWUTcm52=vd75qK+C-QeonjUcsERgZXb=eUAKX_GeMdw@mail.gmail.com>
 <CAMk+s2QuwOdyykoivk0uyzFdynP-0M5pdvJG5iqCSsxOeQ6-oQ@mail.gmail.com>
Message-ID: <85590efbf776b8ff52dba0bdf245ba3a@chemo.org.uk>


>> R0 = estimate.R(germany_vect, mGT, begin=germany_vect[1],
> end=germany_vect[length(germany_vect)], methods="EG", pop.size=pop_de,
> nsim=100)
> 
> Error in begin.nb:end.nb : argument of length 0
> 
>> germany_vect[1]
>   1
> 184
>> germany_vect[length(germany_vect)]
>  57
> 488
> 
> ```
> What might be the problem here?

begin = germany_vect[1]
So begin = 184

but do you not want begin = 1

and same for end?


From ch@r|e@|ehnen @end|ng |rom gm@||@com  Tue May  5 17:25:42 2020
From: ch@r|e@|ehnen @end|ng |rom gm@||@com (Charles Lehnen)
Date: Tue, 5 May 2020 10:25:42 -0500
Subject: [R] Question about combining foodwebs and phylogenetic trees
Message-ID: <CAA560r57hoQHe9x8psci8NaYhoLNGzsd9WQ0PY1MUFf4yNnYAA@mail.gmail.com>

I have trying to combine foodweb outputs like the bipartite package's
plotweb() function of bipartiteD3?s bipartite_D3 function with phylogenetic
trees, similar to a tanglegram. Because of the very large size and a high
amount of variability in my dataset, standard tanglegrams turn out very
convoluted, but the plotweb() outputs are still lovely.

I was able to export tips to manually order the tips of the plotweb()
output to match the order of the phylogenetic tree tips which allowed me to
align tips manually in Inkscape, but this proved extremely time consuming
whenever I made an addition to my dataset.

require('ape')

tree1<-read.tree(text="((A,(B,(C,D))),E);")
tree1<-ladderize(tree1, right = FALSE)

tree2<-read.tree(text="(F,(G,((H,I),(J,K))));")

is_tip <- tree1$edge[,2] <= length(tree1$tip.label)
ordered_tips <- tree1$edge[is_tip, 2]
tree1tips<-tree1$tip.label[ordered_tips]

is_tip <- tree2$edge[,2] <= length(tree2$tip.label)
ordered_tips <- tree2$edge[is_tip, 2]
tree2tips<-tree2$tip.label[ordered_tips]


I tried to edit the plotweb() script to accept phylo class variables as an
additional argument, but that was evidently beyond my abilities at this
time.

I also tried combing the outputs using the grid package, we were able to
visually combine outputs next to one another and match the order of the
tips. However, I have not been able to figure out how to actually line up
the tips of the trees to the outputs of plotweb(). This becomes very
evident with my actual, very large dataset

require('ape')
require('bipartite')
require('ggplotify')
require('cowplot')
require('grid')

tree1<-read.tree(text="((A,(B,(C,D))),E);")
tree1<-ladderize(tree1, right = FALSE)

tree2<-read.tree(text="(F,(G,((H,I),(J,K))));")

bipartite<-cbind(c(0,2,3,2,0,0),c(2,0,2,4,8,0),c(4,3,0,0,5,0),c(0,2,0,0,0,1),c(0,7,2,2,0,0))
colnames(bipartite)<-c("D","C","B","A","E")
rownames(bipartite)<-c("K","J","I","H","G","F")
bipartite<-as.data.frame(bipartite)

p12 = as.grob(~cophyloplot(tree1, tree2))
bipartite = as.data.frame(t(bipartite))
p3 = as.grob(~plotweb(bipartite,
        method = "normal",
        empty = "false",
        text.rot = "90"
        ))
grid.newpage()
grid.draw(p12)
vp = viewport(x = 0.53, y = 0.6, width = 0.6, height = 0.8, angle = -90)
pushViewport(vp)
grid.draw(p3)


If anyone could direct me on how to proceed, I would greatly appreciate it!
I have been coming back to this problem for many months now and have not
been to solve it

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Wed May  6 00:01:34 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 5 May 2020 15:01:34 -0700
Subject: [R] Question about combining foodwebs and phylogenetic trees
In-Reply-To: <CAA560r57hoQHe9x8psci8NaYhoLNGzsd9WQ0PY1MUFf4yNnYAA@mail.gmail.com>
References: <CAA560r57hoQHe9x8psci8NaYhoLNGzsd9WQ0PY1MUFf4yNnYAA@mail.gmail.com>
Message-ID: <CAGxFJbR7pzTvL-SXjG1T2eJrtaB4mzPPX1rPwunPe+Cj76zR1Q@mail.gmail.com>

I think it unlikely that you'll get such specific help here.
Try posting on:
R-SIG-phylo: R SIG on phylogenetic and comparative methods and analyses
instead.

(I also assume you are aware of:
https://CRAN.R-project.org/view=Phylogenetics  ,
but I have no idea whether it is helpful).


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Tue, May 5, 2020 at 2:12 PM Charles Lehnen <charleslehnen at gmail.com> wrote:
>
> I have trying to combine foodweb outputs like the bipartite package's
> plotweb() function of bipartiteD3?s bipartite_D3 function with phylogenetic
> trees, similar to a tanglegram. Because of the very large size and a high
> amount of variability in my dataset, standard tanglegrams turn out very
> convoluted, but the plotweb() outputs are still lovely.
>
> I was able to export tips to manually order the tips of the plotweb()
> output to match the order of the phylogenetic tree tips which allowed me to
> align tips manually in Inkscape, but this proved extremely time consuming
> whenever I made an addition to my dataset.
>
> require('ape')
>
> tree1<-read.tree(text="((A,(B,(C,D))),E);")
> tree1<-ladderize(tree1, right = FALSE)
>
> tree2<-read.tree(text="(F,(G,((H,I),(J,K))));")
>
> is_tip <- tree1$edge[,2] <= length(tree1$tip.label)
> ordered_tips <- tree1$edge[is_tip, 2]
> tree1tips<-tree1$tip.label[ordered_tips]
>
> is_tip <- tree2$edge[,2] <= length(tree2$tip.label)
> ordered_tips <- tree2$edge[is_tip, 2]
> tree2tips<-tree2$tip.label[ordered_tips]
>
>
> I tried to edit the plotweb() script to accept phylo class variables as an
> additional argument, but that was evidently beyond my abilities at this
> time.
>
> I also tried combing the outputs using the grid package, we were able to
> visually combine outputs next to one another and match the order of the
> tips. However, I have not been able to figure out how to actually line up
> the tips of the trees to the outputs of plotweb(). This becomes very
> evident with my actual, very large dataset
>
> require('ape')
> require('bipartite')
> require('ggplotify')
> require('cowplot')
> require('grid')
>
> tree1<-read.tree(text="((A,(B,(C,D))),E);")
> tree1<-ladderize(tree1, right = FALSE)
>
> tree2<-read.tree(text="(F,(G,((H,I),(J,K))));")
>
> bipartite<-cbind(c(0,2,3,2,0,0),c(2,0,2,4,8,0),c(4,3,0,0,5,0),c(0,2,0,0,0,1),c(0,7,2,2,0,0))
> colnames(bipartite)<-c("D","C","B","A","E")
> rownames(bipartite)<-c("K","J","I","H","G","F")
> bipartite<-as.data.frame(bipartite)
>
> p12 = as.grob(~cophyloplot(tree1, tree2))
> bipartite = as.data.frame(t(bipartite))
> p3 = as.grob(~plotweb(bipartite,
>         method = "normal",
>         empty = "false",
>         text.rot = "90"
>         ))
> grid.newpage()
> grid.draw(p12)
> vp = viewport(x = 0.53, y = 0.6, width = 0.6, height = 0.8, angle = -90)
> pushViewport(vp)
> grid.draw(p3)
>
>
> If anyone could direct me on how to proceed, I would greatly appreciate it!
> I have been coming back to this problem for many months now and have not
> been to solve it
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mehd|d@dkh@h91 @end|ng |rom gm@||@com  Wed May  6 06:29:15 2020
From: mehd|d@dkh@h91 @end|ng |rom gm@||@com (Mehdi Dadkhah)
Date: Wed, 6 May 2020 08:59:15 +0430
Subject: [R] Converting CSV file to UTF-8 encoding
In-Reply-To: <20200505105657.GA1073515@posteo.no>
References: <CAGN=ytO6qi4M63X7DucyfCKz8HNNbT_SzrwALQU4cRgrPgeVeg@mail.gmail.com>
 <CAGN=ytPxB8xW62BuGxute_62cYE_1mW9oEK3WjOSnHQuXwcAkA@mail.gmail.com>
 <20200505105657.GA1073515@posteo.no>
Message-ID: <CAGN=ytOrCUr_=1J8yNUCH8ee+gLQhK-SXjyTN5J3srapxaeszQ@mail.gmail.com>

Thank you!
it works for me.
With best regards,

On Tue, May 5, 2020 at 3:27 PM Rasmus Liland <jral at posteo.no> wrote:

> On 2020-05-05 15:12 +0430, Mehdi Dadkhah wrote:
> > I have a CSV file which its encoding is
> > ANSI. How can i change its encoding to
> > UTF-8 in R?
>
> Hi!
>
> I do not know about ANSI, but to read latin1
> encoded csv files into readr, do this:
>
> Determine that your file is latin1-encoded:
>
>         rasmus at twentyfive ~ % file -i SAA.csv
>         SAA.csv: application/csv; charset=iso-8859-1
>
> read it in using readr::read_csv
>
>         locale <- readr::locale(encoding = "latin1")
>         SAA <- suppressMessages(
>           readr::read_csv(file="SAA.csv",
>                           locale=locale))
>
> Best,
> Rasmus
>


-- 
*Mehdi Dadkhah*

	[[alternative HTML version deleted]]


From mehd|d@dkh@h91 @end|ng |rom gm@||@com  Wed May  6 06:29:39 2020
From: mehd|d@dkh@h91 @end|ng |rom gm@||@com (Mehdi Dadkhah)
Date: Wed, 6 May 2020 08:59:39 +0430
Subject: [R] Converting CSV file to UTF-8 encoding
In-Reply-To: <CABcYAdLUqbfJJJyYnVh1xNODR4E-BS1+C0CcOj_-1YXGszsLqg@mail.gmail.com>
References: <CAGN=ytO6qi4M63X7DucyfCKz8HNNbT_SzrwALQU4cRgrPgeVeg@mail.gmail.com>
 <CAGN=ytPxB8xW62BuGxute_62cYE_1mW9oEK3WjOSnHQuXwcAkA@mail.gmail.com>
 <CABcYAdLUqbfJJJyYnVh1xNODR4E-BS1+C0CcOj_-1YXGszsLqg@mail.gmail.com>
Message-ID: <CAGN=ytMg33kOKjYYd6B-WdKtgco0Mf+qOg1zW_69D_xigQ5sAw@mail.gmail.com>

Thank you!
With best regards,

On Tue, May 5, 2020 at 3:47 PM Richard O'Keefe <raoknz at gmail.com> wrote:

> What do you mean "ANSI"?
> Do you mean ASCII?  In that case there is nothing to be done.
> Do you mean some member of the ISO 8859 family of 8-bit character sets?
> Do you mean some Microsoft-specific code page, such as CP-1252?
> (Microsoft CP-437 and CP-1252 "ANSI" but if they have any connection
> whatever with ANSI I would appreciate being informed of it.)
> If you really do mean the ANSI Extended Latin (ANSEL) character
> set, you are out of luck.
>
> If it is supported in your environment, the easiest way is that use the
> iconv() function.  That's what it is for.  See ?iconv.
>
> But there is something easier, and that is not to.
> Just let R know what the external encoding is, and just read the file.
> If you check the documentation of read.csv, ?read.csv
> you will find the fileEncoding="..." argument.
>
> fileEncoding: character string: if non-empty declares the encoding used
>           on a file (not a connection) so the character data can be
>           re-encoded.  See the 'Encoding' section of the help for
>           'file', the 'R Data Import/Export Manual' and 'Note'.
>
> At a guess, you  want fileEncoding="WINDOWS-1252".
>
> On Tue, 5 May 2020 at 22:42, Mehdi Dadkhah <mehdidadkhah91 at gmail.com>
> wrote:
> >
> > Hi,
> > I hope you are doing well!
> > I have a CSV file which its encoding is ANSI. How can i change its
> encoding
> > to UTF-8 in R?
> > Many thanks!
> > With best regards,
> >
> > --
> > *Mehdi Dadkhah*
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>


-- 
*Mehdi Dadkhah*
PhD candidate & Research assistant
Department of Management, Faculty of Economics and Administrative Sciences,
Ferdowsi University of Mashhad, Mashhad, Iran
*Email Addresses:*
mehdidadkhah91 at gmail.com
Mehdidadkhah at mail.um.ac.ir

	[[alternative HTML version deleted]]


From g@@@powe|| @end|ng |rom protonm@||@com  Wed May  6 06:50:17 2020
From: g@@@powe|| @end|ng |rom protonm@||@com (Gregg)
Date: Wed, 06 May 2020 04:50:17 +0000
Subject: [R] How to combine two rows in a data table into a third new row,
 such that the values in the row are added together in the new row?
Message-ID: <b9UMGduMidxZZVV4YyGRRf4-zyj0dJkZjBd3W-XEybdWx0Kj6Rr58s2Pjqe769m0xP4ARLI4LOUEFYQrLxuCtRM3qNS0CBYepMx_mSFoTSg=@protonmail.com>




I have a data table titled: "dt_count" - it contains:

> "","STATUS","N"
> "1","Resolved",650
> "2","Assigned",135
> "3","Closed",530
> "4","In Progress",56
> "5","Pending",75
> "6","Cancelled",20
> 

> I need to change the "dt_count" data table to a new data table that looks like this:
> 

> "","STATUS","N"
> "1","Resolved/Closed",1180
> "2","Assigned",135
> "3","In Progress",56
> "4","Pending",75
> "5","Cancelled",20
> 

> Or, to state the question:
> 

> I need to combine the "Resolved" Row with the "Closed" Row, into a Third new row titled "Resolved/Closed", whereby the "N" ticket count in each of the "Resolved" row and the "Closed" row are added together in the third new?"Resolved/Closed" - also, would need the old?"Resolved" Row with the "Closed" Rows to go away.
> 

> To complicate the issue, the rows in the "dt_count" data table when they are output, are not always in the same order.
> 

> I have the data.table library is installed.
> 

> I'm thinking there is a very easy way to do this... but I am not finding it. I've spent several hours searching thru among other things? - several data table cheatsheets, and I've also read thru this:
> https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html
> 

> Just can't sort it out. Just started using R a few weeks ago.
> 

> Any help would be so very much appreciated!
> 

> Thanks.
> 

> Gregg
> AZ, USA
> 



-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 477 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200506/83cb4840/attachment.sig>

From g@@@powe|| @end|ng |rom protonm@||@com  Wed May  6 06:41:53 2020
From: g@@@powe|| @end|ng |rom protonm@||@com (Gregg)
Date: Wed, 06 May 2020 04:41:53 +0000
Subject: [R] How to combine two rows in a data table into a third new row,
 such that the values in the row are added together in the new row?
Message-ID: <jo12v-il8yoIf0BT6nz1vR8DCnVoTRnU93eBBuSsa_6p3VtEoL0G_Ez-2jVryDXdU6E8gFO4ozJiGAOIJxV-Ili4n145VLMLQD9zFLSAOHw=@protonmail.com>

If I have a data table that is essentially output titled: "dt_count" - it contains:

"","STATUS","N"
"1","Resolved",650
"2","Assigned",135
"3","Closed",530
"4","In Progress",56
"5","Pending",75
"6","Cancelled",20

Need to change the "dt_count" data table to a new data table that looks like this:

"","STATUS","N"
"1","Resolved/Closed",1180
"2","Assigned",135
"3","In Progress",56
"4","Pending",75
"5","Cancelled",20

Or, to state the question:

I need to combine the "Resolved" Row with the "Closed" Row, into a Third new row titled "Resolved/Closed", whereby the "N" ticket count in each of the "Resolved" row and the "Closed" row are added together in the third new?"Resolved/Closed" - also, would need the old?"Resolved" Row with the "Closed" Rows to go away.

To complicate the issue, the rows in the "dt_count" data table when they are output, are not always in the same order.

I have the data.table library is installed.

I'm thinking there is a very easy way to do this... but I am not finding it. I've search thru several data table cheatsheets, and I've also read thru this:
https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html

Just can't sort it out. Just started using R a few weeks ago.

Any help would be so very much appreciated!

Thanks.

Gregg
AZ, USA
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 477 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200506/8fddece8/attachment.sig>

From m@||||@t@ @end|ng |rom pp@|net@||  Wed May  6 11:09:42 2020
From: m@||||@t@ @end|ng |rom pp@|net@|| (K. Elo)
Date: Wed, 06 May 2020 12:09:42 +0300
Subject: [R] 
 How to combine two rows in a data table into a third new row,
 such that the values in the row are added together in the new row?
In-Reply-To: <jo12v-il8yoIf0BT6nz1vR8DCnVoTRnU93eBBuSsa_6p3VtEoL0G_Ez-2jVryDXdU6E8gFO4ozJiGAOIJxV-Ili4n145VLMLQD9zFLSAOHw=@protonmail.com>
References: <jo12v-il8yoIf0BT6nz1vR8DCnVoTRnU93eBBuSsa_6p3VtEoL0G_Ez-2jVryDXdU6E8gFO4ozJiGAOIJxV-Ili4n145VLMLQD9zFLSAOHw=@protonmail.com>
Message-ID: <d33c96e67e0cf3f437fe9838dbbbadd3a04e059e.camel@pp.inet.fi>

Hi!

With 'dplyr':

dt_count %>% mutate(STATUS=ifelse(STATUS %in%
c("Resolved","Closed"),"Resolved/Closed",STATUS)) %>% group_by(STATUS)
%>% summarise(n=sum(N))

Output:

1 Assigned          135
2 Cancelled          20
3 In Progress        56
4 Pending            75
5 Resolved/Closed  1180

HTH,
Kimmo

2020-05-06, 04:41 +0000, Gregg via R-help wrote:
> If I have a data table that is essentially output titled: "dt_count"
> - it contains:
> 
> "","STATUS","N"
> "1","Resolved",650
> "2","Assigned",135
> "3","Closed",530
> "4","In Progress",56
> "5","Pending",75
> "6","Cancelled",20
> 
> Need to change the "dt_count" data table to a new data table that
> looks like this:
> 
> "","STATUS","N"
> "1","Resolved/Closed",1180
> "2","Assigned",135
> "3","In Progress",56
> "4","Pending",75
> "5","Cancelled",20
> 
> Or, to state the question:
> 
> I need to combine the "Resolved" Row with the "Closed" Row, into a
> Third new row titled "Resolved/Closed", whereby the "N" ticket count
> in each of the "Resolved" row and the "Closed" row are added together
> in the third new "Resolved/Closed" - also, would need the
> old "Resolved" Row with the "Closed" Rows to go away.
> 
> To complicate the issue, the rows in the "dt_count" data table when
> they are output, are not always in the same order.
> 
> I have the data.table library is installed.
> 
> I'm thinking there is a very easy way to do this... but I am not
> finding it. I've search thru several data table cheatsheets, and I've
> also read thru this:
> 
https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html
> 
> Just can't sort it out. Just started using R a few weeks ago.
> 
> Any help would be so very much appreciated!
> 
> Thanks.
> 
> Gregg
> AZ, USA
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From petr@p|k@| @end|ng |rom prechez@@cz  Wed May  6 11:13:22 2020
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Wed, 6 May 2020 09:13:22 +0000
Subject: [R] 
 How to combine two rows in a data table into a third new row,
 such that the values in the row are added together in the new row?
In-Reply-To: <jo12v-il8yoIf0BT6nz1vR8DCnVoTRnU93eBBuSsa_6p3VtEoL0G_Ez-2jVryDXdU6E8gFO4ozJiGAOIJxV-Ili4n145VLMLQD9zFLSAOHw=@protonmail.com>
References: <jo12v-il8yoIf0BT6nz1vR8DCnVoTRnU93eBBuSsa_6p3VtEoL0G_Ez-2jVryDXdU6E8gFO4ozJiGAOIJxV-Ili4n145VLMLQD9zFLSAOHw=@protonmail.com>
Message-ID: <220eb24467d947438526d4961cff8e15@SRVEXCHCM1302.precheza.cz>

Hi

Maybe aggregate?

1. Make your column STATUS a factor
2. Combine levels Resolved and Closed to one common factor named 
Reslolved/Closed
3. aggregate according to new STATUS

temp <- read.table("clipboard", sep=",", header=T)
temp$STATUS
[1] "Resolved"    "Assigned"    "Closed"      "In Progress" "Pending"
[6] "Cancelled"
temp$STATUS <- factor(temp$STATUS)
levels(temp$STATUS)
[1] "Assigned"    "Cancelled"   "Closed"      "In Progress" "Pending"
[6] "Resolved"
levels(temp$STATUS)[c(3,6)] <- "Resolved/Closed"
aggregate(temp$N, list(temp$STATUS), sum)
          Group.1    x
1        Assigned  135
2       Cancelled   20
3 Resolved/Closed 1180
4     In Progress   56
5         Pending   75

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Gregg via R-help
> Sent: Wednesday, May 6, 2020 6:42 AM
> To: r-help at r-project.org
> Subject: [R] How to combine two rows in a data table into a third new row,
> such that the values in the row are added together in the new row?
>
> If I have a data table that is essentially output titled: "dt_count" - it 
> contains:
>
> "","STATUS","N"
> "1","Resolved",650
> "2","Assigned",135
> "3","Closed",530
> "4","In Progress",56
> "5","Pending",75
> "6","Cancelled",20
>
> Need to change the "dt_count" data table to a new data table that looks like
> this:
>
> "","STATUS","N"
> "1","Resolved/Closed",1180
> "2","Assigned",135
> "3","In Progress",56
> "4","Pending",75
> "5","Cancelled",20
>
> Or, to state the question:
>
> I need to combine the "Resolved" Row with the "Closed" Row, into a Third
> new row titled "Resolved/Closed", whereby the "N" ticket count in each of 
> the
> "Resolved" row and the "Closed" row are added together in the third
> new "Resolved/Closed" - also, would need the old "Resolved" Row with the
> "Closed" Rows to go away.
>
> To complicate the issue, the rows in the "dt_count" data table when they are
> output, are not always in the same order.
>
> I have the data.table library is installed.
>
> I'm thinking there is a very easy way to do this... but I am not finding it. 
> I've
> search thru several data table cheatsheets, and I've also read thru this:
> https://cran.r-project.org/web/packages/data.table/vignettes/datatable-
> intro.html
>
> Just can't sort it out. Just started using R a few weeks ago.
>
> Any help would be so very much appreciated!
>
> Thanks.
>
> Gregg
> AZ, USA

From p_conno||y @end|ng |rom @||ng@hot@co@nz  Wed May  6 12:15:07 2020
From: p_conno||y @end|ng |rom @||ng@hot@co@nz (Patrick Connolly)
Date: Wed, 6 May 2020 22:15:07 +1200
Subject: [R] PCRE configure problem with R-4.0.0
In-Reply-To: <CA+vqiLE1rjP6AtgUNkxVZdWX4D4-GTRB_104fP7qh8SzCeFn6Q@mail.gmail.com>
References: <20200504041542.GA7518@slingshot.co.nz>
 <20200504100010.5d771a78@trisector>
 <CA+vqiLEcbaU=Wm7dF=eEdygavtBRx1oxSaR8FyjX16GyBrYj0Q@mail.gmail.com>
 <20200505083717.GB7518@slingshot.co.nz>
 <CA+vqiLE1rjP6AtgUNkxVZdWX4D4-GTRB_104fP7qh8SzCeFn6Q@mail.gmail.com>
Message-ID: <20200506101507.GC7518@slingshot.co.nz>

On Tue, 05-May-2020 at 06:28AM -0400, Ista Zahn wrote:

|> <snip>
|> 
|> > |> > Linux Mint 17.2 is based on Ubuntu 14.04, which has been released in
|> > |> > April 2014, while PCRE2 has been released in 2015.
|> > |>
|> > |> Moreover, support for 17.2 ended over a year ago (according to
|> > |> https://en.wikipedia.org/wiki/Linux_Mint_version_history). I suggest
|> > |> upgrading to a supported version.
|> >
|> > Thanks for making that clear -- though I don't relish the hassle of
|> > upgrading an OS.  I was quite happy with the features of Mint 17.x.
|> >
|> 
|> Sure, but there is no free lunch and as your system gets older and
|> older you'll find that more and more modern software doesn't work with
|> it. Additionally there are increasing security risks because the
|> vendor is no longer releasing security patches.
|> 
|> That said, if you really really wanna, you can use
|> https://docs.conda.io/en/latest/, https://spack.io/, or similar to
|> install recent software releases on old systems.

Thanks.  Condo is a new one to me.  



|> 
|> Best,
|> Ista
|> 
|> > [...]
|> >
|> >
|> > --
|> > ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
|> >    ___    Patrick Connolly
|> >  {~._.~}                   Great minds discuss ideas
|> >  _( Y )_                 Average minds discuss events
|> > (:_~*~_:)                  Small minds discuss people
|> >  (_)-(_)                              ..... Eleanor Roosevelt
|> >
|> > ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}                   Great minds discuss ideas    
 _( Y )_  	         Average minds discuss events 
(:_~*~_:)                  Small minds discuss people  
 (_)-(_)  	                      ..... Eleanor Roosevelt
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Wed May  6 15:28:11 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Wed, 6 May 2020 08:28:11 -0500
Subject: [R] calculating t-score/t-stats as my zscores
Message-ID: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>

Hello,

Can I apply the quantile function qt() this way?
qt(pvals/2, 406-34, lower.tail = F)
to get the T-scores?

Thanks
Ama


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed May  6 16:27:20 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 6 May 2020 15:27:20 +0100
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
Message-ID: <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>

Hello,

That gives the *absolute* t-scores. If it's all you need/want, then the 
answer is yes, you can.

Hope this helps,

Rui Barradas

?s 14:28 de 06/05/20, Ana Marija escreveu:
> Hello,
> 
> Can I apply the quantile function qt() this way?
> qt(pvals/2, 406-34, lower.tail = F)
> to get the T-scores?
> 
> Thanks
> Ama
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Wed May  6 16:31:38 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Wed, 6 May 2020 09:31:38 -0500
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
 <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
Message-ID: <CAF9-5jOXc3CWfYtRwxo2f+VuSX3np6o+ZTq8ykyNqH-t4AsbqQ@mail.gmail.com>

Hi Rui,

Thank you for getting back to me. Is there is a better way to
calculate Z scores if I have p values, SE and Beta?

Thanks
Ana

On Wed, May 6, 2020 at 9:27 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
> Hello,
>
> That gives the *absolute* t-scores. If it's all you need/want, then the
> answer is yes, you can.
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 14:28 de 06/05/20, Ana Marija escreveu:
> > Hello,
> >
> > Can I apply the quantile function qt() this way?
> > qt(pvals/2, 406-34, lower.tail = F)
> > to get the T-scores?
> >
> > Thanks
> > Ama
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed May  6 16:33:39 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 6 May 2020 15:33:39 +0100
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
 <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
Message-ID: <4fd55d5a-8028-70e3-5130-b9502204ba4c@sapo.pt>

Hello,

Sorry but after reading my answer I believe it's not completely clear.

I meant absolute values, the actual t-scores as computed from the data 
might be negative. Your code will always produce positive numbers.

Hope this helps,

Rui Barradas

?s 15:27 de 06/05/20, Rui Barradas escreveu:
> Hello,
> 
> That gives the *absolute* t-scores. If it's all you need/want, then the 
> answer is yes, you can.
> 
> Hope this helps,
> 
> Rui Barradas
> 
> ?s 14:28 de 06/05/20, Ana Marija escreveu:
>> Hello,
>>
>> Can I apply the quantile function qt() this way?
>> qt(pvals/2, 406-34, lower.tail = F)
>> to get the T-scores?
>>
>> Thanks
>> Ama
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Wed May  6 16:37:23 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Wed, 6 May 2020 09:37:23 -0500
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <4fd55d5a-8028-70e3-5130-b9502204ba4c@sapo.pt>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
 <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
 <4fd55d5a-8028-70e3-5130-b9502204ba4c@sapo.pt>
Message-ID: <CAF9-5jNtfNGE-DPjHfXTX_yOAPepkwhXbJBSAYi21XGpGotrhw@mail.gmail.com>

thanks, can you please tell em what would be the way not to get the
absolute (always positive values)

On Wed, May 6, 2020 at 9:33 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
> Hello,
>
> Sorry but after reading my answer I believe it's not completely clear.
>
> I meant absolute values, the actual t-scores as computed from the data
> might be negative. Your code will always produce positive numbers.
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 15:27 de 06/05/20, Rui Barradas escreveu:
> > Hello,
> >
> > That gives the *absolute* t-scores. If it's all you need/want, then the
> > answer is yes, you can.
> >
> > Hope this helps,
> >
> > Rui Barradas
> >
> > ?s 14:28 de 06/05/20, Ana Marija escreveu:
> >> Hello,
> >>
> >> Can I apply the quantile function qt() this way?
> >> qt(pvals/2, 406-34, lower.tail = F)
> >> to get the T-scores?
> >>
> >> Thanks
> >> Ama
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Wed May  6 16:40:51 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Wed, 6 May 2020 09:40:51 -0500
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <CAF9-5jNtfNGE-DPjHfXTX_yOAPepkwhXbJBSAYi21XGpGotrhw@mail.gmail.com>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
 <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
 <4fd55d5a-8028-70e3-5130-b9502204ba4c@sapo.pt>
 <CAF9-5jNtfNGE-DPjHfXTX_yOAPepkwhXbJBSAYi21XGpGotrhw@mail.gmail.com>
Message-ID: <CAF9-5jMsxZPpBzwt1Nt5RVhfcuDdWpEEaW+iBP7cCATRs+v22g@mail.gmail.com>

I guess I can have

z-score=Beta/StdErr

On Wed, May 6, 2020 at 9:37 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> thanks, can you please tell em what would be the way not to get the
> absolute (always positive values)
>
> On Wed, May 6, 2020 at 9:33 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
> >
> > Hello,
> >
> > Sorry but after reading my answer I believe it's not completely clear.
> >
> > I meant absolute values, the actual t-scores as computed from the data
> > might be negative. Your code will always produce positive numbers.
> >
> > Hope this helps,
> >
> > Rui Barradas
> >
> > ?s 15:27 de 06/05/20, Rui Barradas escreveu:
> > > Hello,
> > >
> > > That gives the *absolute* t-scores. If it's all you need/want, then the
> > > answer is yes, you can.
> > >
> > > Hope this helps,
> > >
> > > Rui Barradas
> > >
> > > ?s 14:28 de 06/05/20, Ana Marija escreveu:
> > >> Hello,
> > >>
> > >> Can I apply the quantile function qt() this way?
> > >> qt(pvals/2, 406-34, lower.tail = F)
> > >> to get the T-scores?
> > >>
> > >> Thanks
> > >> Ama
> > >>
> > >> ______________________________________________
> > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> PLEASE do read the posting guide
> > >> http://www.R-project.org/posting-guide.html
> > >> and provide commented, minimal, self-contained, reproducible code.
> > >>
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed May  6 16:41:09 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 6 May 2020 15:41:09 +0100
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <CAF9-5jOXc3CWfYtRwxo2f+VuSX3np6o+ZTq8ykyNqH-t4AsbqQ@mail.gmail.com>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
 <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
 <CAF9-5jOXc3CWfYtRwxo2f+VuSX3np6o+ZTq8ykyNqH-t4AsbqQ@mail.gmail.com>
Message-ID: <bd67ebdd-186c-d7b8-8171-a1df8ebc0cf6@sapo.pt>

Hello,

By z-scores do you mean function help('scale')?

Hope this helps,

Rui Barradas

?s 15:31 de 06/05/20, Ana Marija escreveu:
> Hi Rui,
> 
> Thank you for getting back to me. Is there is a better way to
> calculate Z scores if I have p values, SE and Beta?
> 
> Thanks
> Ana
> 
> On Wed, May 6, 2020 at 9:27 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>>
>> Hello,
>>
>> That gives the *absolute* t-scores. If it's all you need/want, then the
>> answer is yes, you can.
>>
>> Hope this helps,
>>
>> Rui Barradas
>>
>> ?s 14:28 de 06/05/20, Ana Marija escreveu:
>>> Hello,
>>>
>>> Can I apply the quantile function qt() this way?
>>> qt(pvals/2, 406-34, lower.tail = F)
>>> to get the T-scores?
>>>
>>> Thanks
>>> Ama
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Wed May  6 16:49:14 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Wed, 6 May 2020 09:49:14 -0500
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <bd67ebdd-186c-d7b8-8171-a1df8ebc0cf6@sapo.pt>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
 <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
 <CAF9-5jOXc3CWfYtRwxo2f+VuSX3np6o+ZTq8ykyNqH-t4AsbqQ@mail.gmail.com>
 <bd67ebdd-186c-d7b8-8171-a1df8ebc0cf6@sapo.pt>
Message-ID: <CAF9-5jP2zYyT45tH0ZnCDBWQbQND-ZBQLKL-6+Uq8U81=Qnc2A@mail.gmail.com>

as defined here:
https://huwenboshi.github.io/data%20management/2017/11/23/tips-for-formatting-gwas-summary-stats.html

where Effect size is Beta

On Wed, May 6, 2020 at 9:41 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
> Hello,
>
> By z-scores do you mean function help('scale')?
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 15:31 de 06/05/20, Ana Marija escreveu:
> > Hi Rui,
> >
> > Thank you for getting back to me. Is there is a better way to
> > calculate Z scores if I have p values, SE and Beta?
> >
> > Thanks
> > Ana
> >
> > On Wed, May 6, 2020 at 9:27 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
> >>
> >> Hello,
> >>
> >> That gives the *absolute* t-scores. If it's all you need/want, then the
> >> answer is yes, you can.
> >>
> >> Hope this helps,
> >>
> >> Rui Barradas
> >>
> >> ?s 14:28 de 06/05/20, Ana Marija escreveu:
> >>> Hello,
> >>>
> >>> Can I apply the quantile function qt() this way?
> >>> qt(pvals/2, 406-34, lower.tail = F)
> >>> to get the T-scores?
> >>>
> >>> Thanks
> >>> Ama
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>


From m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com  Wed May  6 16:51:50 2020
From: m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com (Patrick (Malone Quantitative))
Date: Wed, 6 May 2020 10:51:50 -0400
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <bd67ebdd-186c-d7b8-8171-a1df8ebc0cf6@sapo.pt>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
 <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
 <CAF9-5jOXc3CWfYtRwxo2f+VuSX3np6o+ZTq8ykyNqH-t4AsbqQ@mail.gmail.com>
 <bd67ebdd-186c-d7b8-8171-a1df8ebc0cf6@sapo.pt>
Message-ID: <CAJc=yOFehbugQb8ASJ9tuG3s8qdPP66C6=QUtQQvqW7HVC4=-A@mail.gmail.com>

Guessing for Ana, but no, that's a different meaning. Beta/StdErr is a
z statistic--a test statistic against (usually) the tails of the unit
normal distribution. So like a t-test with infinite df.


On Wed, May 6, 2020 at 10:41 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
> Hello,
>
> By z-scores do you mean function help('scale')?
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 15:31 de 06/05/20, Ana Marija escreveu:
> > Hi Rui,
> >
> > Thank you for getting back to me. Is there is a better way to
> > calculate Z scores if I have p values, SE and Beta?
> >
> > Thanks
> > Ana
> >
> > On Wed, May 6, 2020 at 9:27 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
> >>
> >> Hello,
> >>
> >> That gives the *absolute* t-scores. If it's all you need/want, then the
> >> answer is yes, you can.
> >>
> >> Hope this helps,
> >>
> >> Rui Barradas
> >>
> >> ?s 14:28 de 06/05/20, Ana Marija escreveu:
> >>> Hello,
> >>>
> >>> Can I apply the quantile function qt() this way?
> >>> qt(pvals/2, 406-34, lower.tail = F)
> >>> to get the T-scores?
> >>>
> >>> Thanks
> >>> Ama
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Patrick S. Malone, Ph.D., Malone Quantitative
NEW Service Models: http://malonequantitative.com

He/Him/His


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Wed May  6 16:54:03 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Wed, 6 May 2020 09:54:03 -0500
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <CAJc=yOFehbugQb8ASJ9tuG3s8qdPP66C6=QUtQQvqW7HVC4=-A@mail.gmail.com>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
 <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
 <CAF9-5jOXc3CWfYtRwxo2f+VuSX3np6o+ZTq8ykyNqH-t4AsbqQ@mail.gmail.com>
 <bd67ebdd-186c-d7b8-8171-a1df8ebc0cf6@sapo.pt>
 <CAJc=yOFehbugQb8ASJ9tuG3s8qdPP66C6=QUtQQvqW7HVC4=-A@mail.gmail.com>
Message-ID: <CAF9-5jMr7SWF6BeW7NvNuaMdPGUPDd8DUiGsH4bMAGoo=0DmEw@mail.gmail.com>

Thanks Patrick, so in conclusion this is fine?
z-score=Beta/StdErr

On Wed, May 6, 2020 at 9:52 AM Patrick (Malone Quantitative)
<malone at malonequantitative.com> wrote:
>
> Guessing for Ana, but no, that's a different meaning. Beta/StdErr is a
> z statistic--a test statistic against (usually) the tails of the unit
> normal distribution. So like a t-test with infinite df.
>
>
> On Wed, May 6, 2020 at 10:41 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
> >
> > Hello,
> >
> > By z-scores do you mean function help('scale')?
> >
> > Hope this helps,
> >
> > Rui Barradas
> >
> > ?s 15:31 de 06/05/20, Ana Marija escreveu:
> > > Hi Rui,
> > >
> > > Thank you for getting back to me. Is there is a better way to
> > > calculate Z scores if I have p values, SE and Beta?
> > >
> > > Thanks
> > > Ana
> > >
> > > On Wed, May 6, 2020 at 9:27 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
> > >>
> > >> Hello,
> > >>
> > >> That gives the *absolute* t-scores. If it's all you need/want, then the
> > >> answer is yes, you can.
> > >>
> > >> Hope this helps,
> > >>
> > >> Rui Barradas
> > >>
> > >> ?s 14:28 de 06/05/20, Ana Marija escreveu:
> > >>> Hello,
> > >>>
> > >>> Can I apply the quantile function qt() this way?
> > >>> qt(pvals/2, 406-34, lower.tail = F)
> > >>> to get the T-scores?
> > >>>
> > >>> Thanks
> > >>> Ama
> > >>>
> > >>> ______________________________________________
> > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > >>> and provide commented, minimal, self-contained, reproducible code.
> > >>>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Patrick S. Malone, Ph.D., Malone Quantitative
> NEW Service Models: http://malonequantitative.com
>
> He/Him/His


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed May  6 16:54:43 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 6 May 2020 15:54:43 +0100
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <CAF9-5jMsxZPpBzwt1Nt5RVhfcuDdWpEEaW+iBP7cCATRs+v22g@mail.gmail.com>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
 <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
 <4fd55d5a-8028-70e3-5130-b9502204ba4c@sapo.pt>
 <CAF9-5jNtfNGE-DPjHfXTX_yOAPepkwhXbJBSAYi21XGpGotrhw@mail.gmail.com>
 <CAF9-5jMsxZPpBzwt1Nt5RVhfcuDdWpEEaW+iBP7cCATRs+v22g@mail.gmail.com>
Message-ID: <bd18b8bf-fcc7-844d-8544-898390df77b9@sapo.pt>

Hello,

You can write a function to compute the scores:


z_score <- function(x, beta = mean, beta0 = 0){
   beta <- match.fun(beta)
   n <- length(x)
   score <- sqrt(n)*(beta(x) - beta0)/sd(x)
   names(score) <- if(n < 30) "t.score" else "z.score"
   score
}

# data example
x <- rt(20, df = 1)
tt <- t.test(x)

pvals <- tt$p.value

# Now compare these 3 results and see if it answers the question

qt(pvals/2, 19, lower.tail = F)
tt$statistic
z_score(x)


Hope this helps,

Rui Barradas

?s 15:40 de 06/05/20, Ana Marija escreveu:
> I guess I can have
> 
> z-score=Beta/StdErr
> 
> On Wed, May 6, 2020 at 9:37 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>>
>> thanks, can you please tell em what would be the way not to get the
>> absolute (always positive values)
>>
>> On Wed, May 6, 2020 at 9:33 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>>>
>>> Hello,
>>>
>>> Sorry but after reading my answer I believe it's not completely clear.
>>>
>>> I meant absolute values, the actual t-scores as computed from the data
>>> might be negative. Your code will always produce positive numbers.
>>>
>>> Hope this helps,
>>>
>>> Rui Barradas
>>>
>>> ?s 15:27 de 06/05/20, Rui Barradas escreveu:
>>>> Hello,
>>>>
>>>> That gives the *absolute* t-scores. If it's all you need/want, then the
>>>> answer is yes, you can.
>>>>
>>>> Hope this helps,
>>>>
>>>> Rui Barradas
>>>>
>>>> ?s 14:28 de 06/05/20, Ana Marija escreveu:
>>>>> Hello,
>>>>>
>>>>> Can I apply the quantile function qt() this way?
>>>>> qt(pvals/2, 406-34, lower.tail = F)
>>>>> to get the T-scores?
>>>>>
>>>>> Thanks
>>>>> Ama
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed May  6 17:05:20 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 6 May 2020 16:05:20 +0100
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <CAF9-5jMr7SWF6BeW7NvNuaMdPGUPDd8DUiGsH4bMAGoo=0DmEw@mail.gmail.com>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
 <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
 <CAF9-5jOXc3CWfYtRwxo2f+VuSX3np6o+ZTq8ykyNqH-t4AsbqQ@mail.gmail.com>
 <bd67ebdd-186c-d7b8-8171-a1df8ebc0cf6@sapo.pt>
 <CAJc=yOFehbugQb8ASJ9tuG3s8qdPP66C6=QUtQQvqW7HVC4=-A@mail.gmail.com>
 <CAF9-5jMr7SWF6BeW7NvNuaMdPGUPDd8DUiGsH4bMAGoo=0DmEw@mail.gmail.com>
Message-ID: <c24c800e-a16b-a28d-fe23-43c3dcc07980@sapo.pt>

Hello,

Another option is to use stats::t.test

t.test(x)$statistic

Or, if you want to test against a beta0 != 0,

t.test(x, mu = beta0)$statistic


But in this case the estimator is the estimator for the mean value.

Hope this helps,

Rui Barradas

?s 15:54 de 06/05/20, Ana Marija escreveu:
> Thanks Patrick, so in conclusion this is fine?
> z-score=Beta/StdErr
> 
> On Wed, May 6, 2020 at 9:52 AM Patrick (Malone Quantitative)
> <malone at malonequantitative.com> wrote:
>>
>> Guessing for Ana, but no, that's a different meaning. Beta/StdErr is a
>> z statistic--a test statistic against (usually) the tails of the unit
>> normal distribution. So like a t-test with infinite df.
>>
>>
>> On Wed, May 6, 2020 at 10:41 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>>>
>>> Hello,
>>>
>>> By z-scores do you mean function help('scale')?
>>>
>>> Hope this helps,
>>>
>>> Rui Barradas
>>>
>>> ?s 15:31 de 06/05/20, Ana Marija escreveu:
>>>> Hi Rui,
>>>>
>>>> Thank you for getting back to me. Is there is a better way to
>>>> calculate Z scores if I have p values, SE and Beta?
>>>>
>>>> Thanks
>>>> Ana
>>>>
>>>> On Wed, May 6, 2020 at 9:27 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>>>>>
>>>>> Hello,
>>>>>
>>>>> That gives the *absolute* t-scores. If it's all you need/want, then the
>>>>> answer is yes, you can.
>>>>>
>>>>> Hope this helps,
>>>>>
>>>>> Rui Barradas
>>>>>
>>>>> ?s 14:28 de 06/05/20, Ana Marija escreveu:
>>>>>> Hello,
>>>>>>
>>>>>> Can I apply the quantile function qt() this way?
>>>>>> qt(pvals/2, 406-34, lower.tail = F)
>>>>>> to get the T-scores?
>>>>>>
>>>>>> Thanks
>>>>>> Ama
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>> --
>> Patrick S. Malone, Ph.D., Malone Quantitative
>> NEW Service Models: http://malonequantitative.com
>>
>> He/Him/His


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Wed May  6 18:02:31 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Wed, 6 May 2020 11:02:31 -0500
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <c24c800e-a16b-a28d-fe23-43c3dcc07980@sapo.pt>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
 <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
 <CAF9-5jOXc3CWfYtRwxo2f+VuSX3np6o+ZTq8ykyNqH-t4AsbqQ@mail.gmail.com>
 <bd67ebdd-186c-d7b8-8171-a1df8ebc0cf6@sapo.pt>
 <CAJc=yOFehbugQb8ASJ9tuG3s8qdPP66C6=QUtQQvqW7HVC4=-A@mail.gmail.com>
 <CAF9-5jMr7SWF6BeW7NvNuaMdPGUPDd8DUiGsH4bMAGoo=0DmEw@mail.gmail.com>
 <c24c800e-a16b-a28d-fe23-43c3dcc07980@sapo.pt>
Message-ID: <CAF9-5jMMXjPeXt7DyuM0kCWnnGW1Uwi-DjKpFfdWYS1zBWyGbw@mail.gmail.com>

Thank you so much! I mostly worry which of those procedures is the
closest to the z-score

On Wed, May 6, 2020 at 10:05 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
> Hello,
>
> Another option is to use stats::t.test
>
> t.test(x)$statistic
>
> Or, if you want to test against a beta0 != 0,
>
> t.test(x, mu = beta0)$statistic
>
>
> But in this case the estimator is the estimator for the mean value.
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 15:54 de 06/05/20, Ana Marija escreveu:
> > Thanks Patrick, so in conclusion this is fine?
> > z-score=Beta/StdErr
> >
> > On Wed, May 6, 2020 at 9:52 AM Patrick (Malone Quantitative)
> > <malone at malonequantitative.com> wrote:
> >>
> >> Guessing for Ana, but no, that's a different meaning. Beta/StdErr is a
> >> z statistic--a test statistic against (usually) the tails of the unit
> >> normal distribution. So like a t-test with infinite df.
> >>
> >>
> >> On Wed, May 6, 2020 at 10:41 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
> >>>
> >>> Hello,
> >>>
> >>> By z-scores do you mean function help('scale')?
> >>>
> >>> Hope this helps,
> >>>
> >>> Rui Barradas
> >>>
> >>> ?s 15:31 de 06/05/20, Ana Marija escreveu:
> >>>> Hi Rui,
> >>>>
> >>>> Thank you for getting back to me. Is there is a better way to
> >>>> calculate Z scores if I have p values, SE and Beta?
> >>>>
> >>>> Thanks
> >>>> Ana
> >>>>
> >>>> On Wed, May 6, 2020 at 9:27 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
> >>>>>
> >>>>> Hello,
> >>>>>
> >>>>> That gives the *absolute* t-scores. If it's all you need/want, then the
> >>>>> answer is yes, you can.
> >>>>>
> >>>>> Hope this helps,
> >>>>>
> >>>>> Rui Barradas
> >>>>>
> >>>>> ?s 14:28 de 06/05/20, Ana Marija escreveu:
> >>>>>> Hello,
> >>>>>>
> >>>>>> Can I apply the quantile function qt() this way?
> >>>>>> qt(pvals/2, 406-34, lower.tail = F)
> >>>>>> to get the T-scores?
> >>>>>>
> >>>>>> Thanks
> >>>>>> Ama
> >>>>>>
> >>>>>> ______________________________________________
> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
> >>
> >> --
> >> Patrick S. Malone, Ph.D., Malone Quantitative
> >> NEW Service Models: http://malonequantitative.com
> >>
> >> He/Him/His


From thpe @end|ng |rom @|meco|@de  Wed May  6 21:28:06 2020
From: thpe @end|ng |rom @|meco|@de (Thomas Petzoldt)
Date: Wed, 6 May 2020 21:28:06 +0200
Subject: [R] COVID-19 datasets...
In-Reply-To: <E3562BF5-42BC-43D6-96AB-E043B6EA1C57@jsasoc.com>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
 <E3562BF5-42BC-43D6-96AB-E043B6EA1C57@jsasoc.com>
Message-ID: <d6e08a07-71c0-473f-7959-b9e1dcac81d1@simecol.de>

Sorry if I'm joining a little bit late.

I've put some related links and scripts together a few weeks ago. Then I 
stopped with this, because there is so much.

The data format employed by John Hopkins CSSE was sort of a big surprise 
to me. An opposite approach was taken in Germany, that organized it as a 
big JSON trees.

Fortunately, both can be "tidied" with R, and represent good didactic 
examples for our students.

Here yet another repo linking to the data:

https://github.com/tpetzoldt/covid


Thomas


On 04.05.2020 at 20:48 James Spottiswoode wrote:
> Sure. COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University is available here:
> 
> https://github.com/CSSEGISandData/COVID-19
> 
> All in csv fiormat.
> 
> 
>> On May 4, 2020, at 11:31 AM, Bernard McGarvey <mcgarvey.bernard at comcast.net> wrote:
>>
>> Just curious does anyone know of a website that has data available in a format that R can download and analyze?
>>   
>> Thanks
>>
>>
>> Bernard McGarvey
>>
>>
>> Director, Fort Myers Beach Lions Foundation, Inc.
>>
>>
>> Retired (Lilly Engineering Fellow).
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> James Spottiswoode
> Applied Mathematics & Statistics
> (310) 270 6220
> jamesspottiswoode Skype
> james at jsasoc.com
> 
>


From p@u|bern@|07 @end|ng |rom gm@||@com  Wed May  6 23:20:47 2020
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Wed, 6 May 2020 16:20:47 -0500
Subject: [R] Working with very large datasets and generating an executable
 file
Message-ID: <CAMOcQfMHwmsS-4Ewjt_OSEy3vUbHUutN5Dqgjr2a4K1DFzbqXA@mail.gmail.com>

Dear R friends,

Hope you are doing well. I have two questions, the first one is, can I work
with very large datasets in R? That is, say I need to test several machine
learning algorithms, like (random forest, multiple linear regression, etc.)
on datasets having between 50 to 100 columns and 20 million observations,
is there any way that R can handle data that large?

The second question is, is there a way I can develop an R model and turn it
into an executable program that can work on any OS?

Any help and/or guidance will be greatly appreciated,

Best regards,

Paul

	[[alternative HTML version deleted]]


From @purd|e@@ @end|ng |rom gm@||@com  Thu May  7 00:05:15 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Thu, 7 May 2020 10:05:15 +1200
Subject: [R] 
 'closure' (was "stats:: spline's method could not be monoH.FC")
In-Reply-To: <24240.8418.914054.939133@stat.math.ethz.ch>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
 <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
 <24240.8418.914054.939133@stat.math.ethz.ch>
Message-ID: <CAB8pepxxZS+xEEU85bbeMHKDRN3o=i9+rsj5nYAfLLf7YsnYzw@mail.gmail.com>

> OTOH, Hadley Wickham has written a book "Advanced R"  which has been
> the best book about advanced R by far (in my view, notably
> before it morphed (towards the 2nd edition) to use more and more
> non-base R packages).  There, he used "Closure" in a different,
> stricter sense, starting the section  'Closures' with
>     ?An object is data with functions.
>      A closure is a function with data.? ? John D. Cook

Martin, Thank you,

I've reviewed everything, and I've come to the following conclusion:
In general, a self-referencing function is not a closure, as such; and
In general, a closure is not a self-referencing function, as such.

So, to describe the superset, maybe I should say something like:

    *Self-Referencing Functions and Closures*

Also, I support multi-paradigm programming, including some global
state data mainly for default formatting-related options.
But if I understand things correctly, you support (or at least have
some preference for) purely-functional programming.
Which is one area, where we diverge.

It seems to me, the most people who advocate closures prefer
purely-functional programming.

If we use the principle, that in purely-functional programming, the
output of a function is not dependent on mutable state data, then
wouldn't it be better to say something like:

A closure is a function with (preferably non-mutable) data.


P.S.
If anyone's interested in creating R functions that reference
themselves, look at:
base::sys.function

Also, my package intoo contains convenience functions (THIS, THAT and
THEN), that wrap sys.function, giving it a slightly more
object-oriented flavor, and explore this idea further.
However, re-reading my documentation, I note that my examples need
some improvement...


From jr@| @end|ng |rom po@teo@no  Thu May  7 00:35:15 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Thu, 7 May 2020 00:35:15 +0200
Subject: [R] 
 How to combine two rows in a data table into a third new row,
 such that the values in the row are added together in the new row?
In-Reply-To: <220eb24467d947438526d4961cff8e15@SRVEXCHCM1302.precheza.cz>
References: <jo12v-il8yoIf0BT6nz1vR8DCnVoTRnU93eBBuSsa_6p3VtEoL0G_Ez-2jVryDXdU6E8gFO4ozJiGAOIJxV-Ili4n145VLMLQD9zFLSAOHw=@protonmail.com>
 <220eb24467d947438526d4961cff8e15@SRVEXCHCM1302.precheza.cz>
Message-ID: <20200506223515.GB1249402@posteo.no>

On 2020-05-06 09:13 +0000, PIKAL Petr wrote:
> Maybe aggregate?

Hi!  I agree aggregate is an elegant solution 
for this, so I continued your example a bit:

dt_count <- '"","STATUS","N"
"1","Resolved",650
"2","Assigned",135
"3","Closed",530
"4","In Progress",56
"5","Pending",75
"6","Cancelled",20'
dt_count <- read.csv(text=dt_count)

dt_count[
  dt_count$STATUS %in%
  c("Resolved", "Closed"),
  "STATUS"] <-
  "Resolved/Closed"
aggregate(
  x=list("N"=dt_count$N),
  by=list("STATUS"=dt_count$STATUS),
  FUN=sum)

Best,
Rasmus

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200507/5178fe64/attachment.sig>

From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu May  7 01:22:41 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 06 May 2020 16:22:41 -0700
Subject: [R] 
 Working with very large datasets and generating an executable file
In-Reply-To: <CAMOcQfMHwmsS-4Ewjt_OSEy3vUbHUutN5Dqgjr2a4K1DFzbqXA@mail.gmail.com>
References: <CAMOcQfMHwmsS-4Ewjt_OSEy3vUbHUutN5Dqgjr2a4K1DFzbqXA@mail.gmail.com>
Message-ID: <72C06F2F-A6EB-40AA-83C8-7E01DBA9D61E@dcn.davis.ca.us>

Large data... yes, though how this can be done may vary. I have used machines with 128G of RAM before with no special big data packages.

Making an executable... theoretically, yes, though there are some significant technical (and possibly legal) challenges that will most likely make you question whether it was worth it if you try, particularly if your intent is to obscure your code from the recipient. I (as a random user and programmer on the Internet) would strongly discourage such efforts... it will almost certainly be more practical to deliver code in script/package form.

On May 6, 2020 2:20:47 PM PDT, Paul Bernal <paulbernal07 at gmail.com> wrote:
>Dear R friends,
>
>Hope you are doing well. I have two questions, the first one is, can I
>work
>with very large datasets in R? That is, say I need to test several
>machine
>learning algorithms, like (random forest, multiple linear regression,
>etc.)
>on datasets having between 50 to 100 columns and 20 million
>observations,
>is there any way that R can handle data that large?
>
>The second question is, is there a way I can develop an R model and
>turn it
>into an executable program that can work on any OS?
>
>Any help and/or guidance will be greatly appreciated,
>
>Best regards,
>
>Paul
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From bgunter@4567 @end|ng |rom gm@||@com  Thu May  7 01:30:03 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 6 May 2020 16:30:03 -0700
Subject: [R] 
 Working with very large datasets and generating an executable file
In-Reply-To: <72C06F2F-A6EB-40AA-83C8-7E01DBA9D61E@dcn.davis.ca.us>
References: <CAMOcQfMHwmsS-4Ewjt_OSEy3vUbHUutN5Dqgjr2a4K1DFzbqXA@mail.gmail.com>
 <72C06F2F-A6EB-40AA-83C8-7E01DBA9D61E@dcn.davis.ca.us>
Message-ID: <CAGxFJbSAYqbNyKrn5LcVy+=nr7gkccN_K2kq9qQyCUrnA503rg@mail.gmail.com>

To supplement Jeff's comments:

Big Data:
https://CRAN.R-project.org/view=HighPerformanceComputing

To deploy models:
https://cran.r-project.org/web/views/ModelDeployment.html

Opinion: Executables are a security risk. I wouldn't touch one unless
from a trusted source. I think I understand what you want to do, but I
would second Jeff's comment about using R packages. Don't bother to
disagree with me -- just dismiss if you do-- as this is wandering O/T
anyway.


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Wed, May 6, 2020 at 4:23 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>
> Large data... yes, though how this can be done may vary. I have used machines with 128G of RAM before with no special big data packages.
>
> Making an executable... theoretically, yes, though there are some significant technical (and possibly legal) challenges that will most likely make you question whether it was worth it if you try, particularly if your intent is to obscure your code from the recipient. I (as a random user and programmer on the Internet) would strongly discourage such efforts... it will almost certainly be more practical to deliver code in script/package form.
>
> On May 6, 2020 2:20:47 PM PDT, Paul Bernal <paulbernal07 at gmail.com> wrote:
> >Dear R friends,
> >
> >Hope you are doing well. I have two questions, the first one is, can I
> >work
> >with very large datasets in R? That is, say I need to test several
> >machine
> >learning algorithms, like (random forest, multiple linear regression,
> >etc.)
> >on datasets having between 50 to 100 columns and 20 million
> >observations,
> >is there any way that R can handle data that large?
> >
> >The second question is, is there a way I can develop an R model and
> >turn it
> >into an executable program that can work on any OS?
> >
> >Any help and/or guidance will be greatly appreciated,
> >
> >Best regards,
> >
> >Paul
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r@oknz @end|ng |rom gm@||@com  Thu May  7 04:47:10 2020
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Thu, 7 May 2020 14:47:10 +1200
Subject: [R] 
 'closure' (was "stats:: spline's method could not be monoH.FC")
In-Reply-To: <CAB8pepxxZS+xEEU85bbeMHKDRN3o=i9+rsj5nYAfLLf7YsnYzw@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
 <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
 <24240.8418.914054.939133@stat.math.ethz.ch>
 <CAB8pepxxZS+xEEU85bbeMHKDRN3o=i9+rsj5nYAfLLf7YsnYzw@mail.gmail.com>
Message-ID: <CABcYAd+iYTSWPbh=XqWvTB+utkyc1_3Sy96apJ-vBPVZ6tWLzA@mail.gmail.com>

A closure is a function plus an environment.

That's it.  This is a sixty-year-old thing in programming languages.

A closure is a dynamic value representing an instance of a function in
a particular context which it can refer to.  When a program in Algol
60 passed a procedure P to a procedure Q, what it passed was logically
a pair consisting of a pointer to the executable code of P and a
pointer to the context P was declared in, often called the static
link.  This was nothing other than a closure.  The limit in Algol 60
was that contexts were not retained, they formed a pure stack.  So all
you could do with a procedure parameter was call it or pass it on.
Same thing in Pascal and other Algol-like languages.

Modern functional languages retain just as much of the context as the
function can actually refer to.  My Smalltalk compiler, for example,
classifies variables as
 - only the value needs to be kept
 - the variable needs to be retained *as* a variable
 - not used in a nested function
R doesn't have that luxury, because R is rather more dynamic and
allows environments to be inspected.

For most practical purposes, there is simply no point in bothering to
distinguish "function" and "closure" in R.  The only functions that
are not closures are primitives.  But you can call args(f), body(f),
and environment(f) on any function, whether it is a closure or a
primitive.  If you want to mess around with the environment of a
function, then you need to understand this stuff, but you probably
shouldn't do that.

On Thu, 7 May 2020 at 10:06, Abby Spurdle <spurdle.a at gmail.com> wrote:
>
> > OTOH, Hadley Wickham has written a book "Advanced R"  which has been
> > the best book about advanced R by far (in my view, notably
> > before it morphed (towards the 2nd edition) to use more and more
> > non-base R packages).  There, he used "Closure" in a different,
> > stricter sense, starting the section  'Closures' with
> >     ?An object is data with functions.
> >      A closure is a function with data.? ? John D. Cook
>
> Martin, Thank you,
>
> I've reviewed everything, and I've come to the following conclusion:
> In general, a self-referencing function is not a closure, as such; and
> In general, a closure is not a self-referencing function, as such.
>
> So, to describe the superset, maybe I should say something like:
>
>     *Self-Referencing Functions and Closures*
>
> Also, I support multi-paradigm programming, including some global
> state data mainly for default formatting-related options.
> But if I understand things correctly, you support (or at least have
> some preference for) purely-functional programming.
> Which is one area, where we diverge.
>
> It seems to me, the most people who advocate closures prefer
> purely-functional programming.
>
> If we use the principle, that in purely-functional programming, the
> output of a function is not dependent on mutable state data, then
> wouldn't it be better to say something like:
>
> A closure is a function with (preferably non-mutable) data.
>
>
> P.S.
> If anyone's interested in creating R functions that reference
> themselves, look at:
> base::sys.function
>
> Also, my package intoo contains convenience functions (THIS, THAT and
> THEN), that wrap sys.function, giving it a slightly more
> object-oriented flavor, and explore this idea further.
> However, re-reading my documentation, I note that my examples need
> some improvement...
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @purd|e@@ @end|ng |rom gm@||@com  Thu May  7 05:27:29 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Thu, 7 May 2020 15:27:29 +1200
Subject: [R] 
 'closure' (was "stats:: spline's method could not be monoH.FC")
In-Reply-To: <CABcYAd+iYTSWPbh=XqWvTB+utkyc1_3Sy96apJ-vBPVZ6tWLzA@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
 <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
 <24240.8418.914054.939133@stat.math.ethz.ch>
 <CAB8pepxxZS+xEEU85bbeMHKDRN3o=i9+rsj5nYAfLLf7YsnYzw@mail.gmail.com>
 <CABcYAd+iYTSWPbh=XqWvTB+utkyc1_3Sy96apJ-vBPVZ6tWLzA@mail.gmail.com>
Message-ID: <CAB8pepytqkTjgrDKFOVUM=hPgG4KHtRtQu70nu=BeKKiiYxP3Q@mail.gmail.com>

> If you want to mess around with the environment of a
> function, then you need to understand this stuff, but you probably
> shouldn't do that.

What exactly do you mean?

Are you implying that a user should not use environments?
In which case, I would disagree.


From @purd|e@@ @end|ng |rom gm@||@com  Thu May  7 05:34:55 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Thu, 7 May 2020 15:34:55 +1200
Subject: [R] 
 Working with very large datasets and generating an executable file
In-Reply-To: <CAMOcQfMHwmsS-4Ewjt_OSEy3vUbHUutN5Dqgjr2a4K1DFzbqXA@mail.gmail.com>
References: <CAMOcQfMHwmsS-4Ewjt_OSEy3vUbHUutN5Dqgjr2a4K1DFzbqXA@mail.gmail.com>
Message-ID: <CAB8pepy3XPii53Jz46bSJ07JJqeV_OSjixEo2ZBLrkS1f86Nmw@mail.gmail.com>

> The second question is, is there a way I can develop an R model and turn it
> into an executable program that can work on any OS?

------myrscript.c--------
int main (int argc, char* argv [])
{   system ("Rscript myrscript.r");
    return 0;
}
-------------------------

command line > gcc -o myrscript.exe myrscript.c
command line > myrscript.exe


From m@rk|eed@2 @end|ng |rom gm@||@com  Thu May  7 05:35:51 2020
From: m@rk|eed@2 @end|ng |rom gm@||@com (Mark Leeds)
Date: Wed, 6 May 2020 23:35:51 -0400
Subject: [R] 
 'closure' (was "stats:: spline's method could not be monoH.FC")
In-Reply-To: <CAB8pepytqkTjgrDKFOVUM=hPgG4KHtRtQu70nu=BeKKiiYxP3Q@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
 <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
 <24240.8418.914054.939133@stat.math.ethz.ch>
 <CAB8pepxxZS+xEEU85bbeMHKDRN3o=i9+rsj5nYAfLLf7YsnYzw@mail.gmail.com>
 <CABcYAd+iYTSWPbh=XqWvTB+utkyc1_3Sy96apJ-vBPVZ6tWLzA@mail.gmail.com>
 <CAB8pepytqkTjgrDKFOVUM=hPgG4KHtRtQu70nu=BeKKiiYxP3Q@mail.gmail.com>
Message-ID: <CAHz+bWZYZOof=gpVzNV3Rvp5Jtmp7+xk5N3BfyG57w=mtPVBfw@mail.gmail.com>

Hi Abby: I agree with you because below is a perfect example of where not
understanding environments causes a somewhat
mysterious problem. Chuck Berry explains it in a follow up  email.

https://r.789695.n4.nabble.com/issues-with-environment-handling-in-model-frame-td4762855.html


On Wed, May 6, 2020 at 11:28 PM Abby Spurdle <spurdle.a at gmail.com> wrote:

> > If you want to mess around with the environment of a
> > function, then you need to understand this stuff, but you probably
> > shouldn't do that.
>
> What exactly do you mean?
>
> Are you implying that a user should not use environments?
> In which case, I would disagree.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r@oknz @end|ng |rom gm@||@com  Thu May  7 06:40:10 2020
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Thu, 7 May 2020 16:40:10 +1200
Subject: [R] 
 'closure' (was "stats:: spline's method could not be monoH.FC")
In-Reply-To: <CAB8pepytqkTjgrDKFOVUM=hPgG4KHtRtQu70nu=BeKKiiYxP3Q@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
 <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
 <24240.8418.914054.939133@stat.math.ethz.ch>
 <CAB8pepxxZS+xEEU85bbeMHKDRN3o=i9+rsj5nYAfLLf7YsnYzw@mail.gmail.com>
 <CABcYAd+iYTSWPbh=XqWvTB+utkyc1_3Sy96apJ-vBPVZ6tWLzA@mail.gmail.com>
 <CAB8pepytqkTjgrDKFOVUM=hPgG4KHtRtQu70nu=BeKKiiYxP3Q@mail.gmail.com>
Message-ID: <CABcYAdJq5yBAy6K0cX_Gozy=u6+-SZDD2U0BTr9JZ534o2XwCQ@mail.gmail.com>

By "mess around with" I mean environment(f) <- ...

That is for _very_ advanced players.

Never assume that someone meant something stupid, make them prove it.

On Thu, 7 May 2020 at 15:28, Abby Spurdle <spurdle.a at gmail.com> wrote:
>
> > If you want to mess around with the environment of a
> > function, then you need to understand this stuff, but you probably
> > shouldn't do that.
>
> What exactly do you mean?
>
> Are you implying that a user should not use environments?
> In which case, I would disagree.


From r@oknz @end|ng |rom gm@||@com  Thu May  7 06:41:53 2020
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Thu, 7 May 2020 16:41:53 +1200
Subject: [R] 
 'closure' (was "stats:: spline's method could not be monoH.FC")
In-Reply-To: <CAHz+bWZYZOof=gpVzNV3Rvp5Jtmp7+xk5N3BfyG57w=mtPVBfw@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
 <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
 <24240.8418.914054.939133@stat.math.ethz.ch>
 <CAB8pepxxZS+xEEU85bbeMHKDRN3o=i9+rsj5nYAfLLf7YsnYzw@mail.gmail.com>
 <CABcYAd+iYTSWPbh=XqWvTB+utkyc1_3Sy96apJ-vBPVZ6tWLzA@mail.gmail.com>
 <CAB8pepytqkTjgrDKFOVUM=hPgG4KHtRtQu70nu=BeKKiiYxP3Q@mail.gmail.com>
 <CAHz+bWZYZOof=gpVzNV3Rvp5Jtmp7+xk5N3BfyG57w=mtPVBfw@mail.gmail.com>
Message-ID: <CABcYAdKcVEQOMXdXnmR6+SBWu8S+DJD10Mx+FbJGE3FBNwqFyQ@mail.gmail.com>

That example is NOT an example of "messing around with environments."

On Thu, 7 May 2020 at 15:36, Mark Leeds <markleeds2 at gmail.com> wrote:
>
> Hi Abby: I agree with you because below is a perfect example of where not understanding environments causes a somewhat
> mysterious problem. Chuck Berry explains it in a follow up  email.
>
> https://r.789695.n4.nabble.com/issues-with-environment-handling-in-model-frame-td4762855.html
>
>
> On Wed, May 6, 2020 at 11:28 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
>>
>> > If you want to mess around with the environment of a
>> > function, then you need to understand this stuff, but you probably
>> > shouldn't do that.
>>
>> What exactly do you mean?
>>
>> Are you implying that a user should not use environments?
>> In which case, I would disagree.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From m@rk|eed@2 @end|ng |rom gm@||@com  Thu May  7 06:47:25 2020
From: m@rk|eed@2 @end|ng |rom gm@||@com (Mark Leeds)
Date: Thu, 7 May 2020 00:47:25 -0400
Subject: [R] 
 'closure' (was "stats:: spline's method could not be monoH.FC")
In-Reply-To: <CABcYAdKcVEQOMXdXnmR6+SBWu8S+DJD10Mx+FbJGE3FBNwqFyQ@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
 <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
 <24240.8418.914054.939133@stat.math.ethz.ch>
 <CAB8pepxxZS+xEEU85bbeMHKDRN3o=i9+rsj5nYAfLLf7YsnYzw@mail.gmail.com>
 <CABcYAd+iYTSWPbh=XqWvTB+utkyc1_3Sy96apJ-vBPVZ6tWLzA@mail.gmail.com>
 <CAB8pepytqkTjgrDKFOVUM=hPgG4KHtRtQu70nu=BeKKiiYxP3Q@mail.gmail.com>
 <CAHz+bWZYZOof=gpVzNV3Rvp5Jtmp7+xk5N3BfyG57w=mtPVBfw@mail.gmail.com>
 <CABcYAdKcVEQOMXdXnmR6+SBWu8S+DJD10Mx+FbJGE3FBNwqFyQ@mail.gmail.com>
Message-ID: <CAHz+bWa0iS6ToNroo2KdoP+qSD_DYtty6T534WLxs60xf2GKBg@mail.gmail.com>

Hi Richard: I didn't say it was and didn't mean to imply it.  All I said is
that it was an example of where "not understanding environments"  can lead
to errors that won't be understood by the person experiencing them. It just
so happens that the use of  "environment(f) <- " can lead to a solution but
there
are other solutions and it was not my intention to talk about the various
solutions. Only that it can be helpful if one understands the notion of
environments in R.








On Thu, May 7, 2020 at 12:42 AM Richard O'Keefe <raoknz at gmail.com> wrote:

> That example is NOT an example of "messing around with environments."
>
> On Thu, 7 May 2020 at 15:36, Mark Leeds <markleeds2 at gmail.com> wrote:
> >
> > Hi Abby: I agree with you because below is a perfect example of where
> not understanding environments causes a somewhat
> > mysterious problem. Chuck Berry explains it in a follow up  email.
> >
> >
> https://r.789695.n4.nabble.com/issues-with-environment-handling-in-model-frame-td4762855.html
> >
> >
> > On Wed, May 6, 2020 at 11:28 PM Abby Spurdle <spurdle.a at gmail.com>
> wrote:
> >>
> >> > If you want to mess around with the environment of a
> >> > function, then you need to understand this stuff, but you probably
> >> > shouldn't do that.
> >>
> >> What exactly do you mean?
> >>
> >> Are you implying that a user should not use environments?
> >> In which case, I would disagree.
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@rk|eed@2 @end|ng |rom gm@||@com  Thu May  7 06:50:41 2020
From: m@rk|eed@2 @end|ng |rom gm@||@com (Mark Leeds)
Date: Thu, 7 May 2020 00:50:41 -0400
Subject: [R] 
 'closure' (was "stats:: spline's method could not be monoH.FC")
In-Reply-To: <CAHz+bWa0iS6ToNroo2KdoP+qSD_DYtty6T534WLxs60xf2GKBg@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
 <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
 <24240.8418.914054.939133@stat.math.ethz.ch>
 <CAB8pepxxZS+xEEU85bbeMHKDRN3o=i9+rsj5nYAfLLf7YsnYzw@mail.gmail.com>
 <CABcYAd+iYTSWPbh=XqWvTB+utkyc1_3Sy96apJ-vBPVZ6tWLzA@mail.gmail.com>
 <CAB8pepytqkTjgrDKFOVUM=hPgG4KHtRtQu70nu=BeKKiiYxP3Q@mail.gmail.com>
 <CAHz+bWZYZOof=gpVzNV3Rvp5Jtmp7+xk5N3BfyG57w=mtPVBfw@mail.gmail.com>
 <CABcYAdKcVEQOMXdXnmR6+SBWu8S+DJD10Mx+FbJGE3FBNwqFyQ@mail.gmail.com>
 <CAHz+bWa0iS6ToNroo2KdoP+qSD_DYtty6T534WLxs60xf2GKBg@mail.gmail.com>
Message-ID: <CAHz+bWbH4n2_4TtzNHs4T2J6j2JCJ6P-uQjOGtrucfRA2-m+YA@mail.gmail.com>

Richard: I may have implied that one should "mess with environments" by
saying that I agree with Abby. If so, my apologies because that's not what
I meant.
I only meant understanding.



On Thu, May 7, 2020 at 12:47 AM Mark Leeds <markleeds2 at gmail.com> wrote:

> Hi Richard: I didn't say it was and didn't mean to imply it.  All I said
> is that it was an example of where "not understanding environments"  can
> lead
> to errors that won't be understood by the person experiencing them. It
> just so happens that the use of  "environment(f) <- " can lead to a
> solution but there
> are other solutions and it was not my intention to talk about the various
> solutions. Only that it can be helpful if one understands the notion of
> environments in R.
>
>
>
>
>
>
>
>
> On Thu, May 7, 2020 at 12:42 AM Richard O'Keefe <raoknz at gmail.com> wrote:
>
>> That example is NOT an example of "messing around with environments."
>>
>> On Thu, 7 May 2020 at 15:36, Mark Leeds <markleeds2 at gmail.com> wrote:
>> >
>> > Hi Abby: I agree with you because below is a perfect example of where
>> not understanding environments causes a somewhat
>> > mysterious problem. Chuck Berry explains it in a follow up  email.
>> >
>> >
>> https://r.789695.n4.nabble.com/issues-with-environment-handling-in-model-frame-td4762855.html
>> >
>> >
>> > On Wed, May 6, 2020 at 11:28 PM Abby Spurdle <spurdle.a at gmail.com>
>> wrote:
>> >>
>> >> > If you want to mess around with the environment of a
>> >> > function, then you need to understand this stuff, but you probably
>> >> > shouldn't do that.
>> >>
>> >> What exactly do you mean?
>> >>
>> >> Are you implying that a user should not use environments?
>> >> In which case, I would disagree.
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From r@oknz @end|ng |rom gm@||@com  Thu May  7 06:59:31 2020
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Thu, 7 May 2020 16:59:31 +1200
Subject: [R] 
 'closure' (was "stats:: spline's method could not be monoH.FC")
In-Reply-To: <CAHz+bWbH4n2_4TtzNHs4T2J6j2JCJ6P-uQjOGtrucfRA2-m+YA@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
 <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
 <24240.8418.914054.939133@stat.math.ethz.ch>
 <CAB8pepxxZS+xEEU85bbeMHKDRN3o=i9+rsj5nYAfLLf7YsnYzw@mail.gmail.com>
 <CABcYAd+iYTSWPbh=XqWvTB+utkyc1_3Sy96apJ-vBPVZ6tWLzA@mail.gmail.com>
 <CAB8pepytqkTjgrDKFOVUM=hPgG4KHtRtQu70nu=BeKKiiYxP3Q@mail.gmail.com>
 <CAHz+bWZYZOof=gpVzNV3Rvp5Jtmp7+xk5N3BfyG57w=mtPVBfw@mail.gmail.com>
 <CABcYAdKcVEQOMXdXnmR6+SBWu8S+DJD10Mx+FbJGE3FBNwqFyQ@mail.gmail.com>
 <CAHz+bWa0iS6ToNroo2KdoP+qSD_DYtty6T534WLxs60xf2GKBg@mail.gmail.com>
 <CAHz+bWbH4n2_4TtzNHs4T2J6j2JCJ6P-uQjOGtrucfRA2-m+YA@mail.gmail.com>
Message-ID: <CABcYAd+m-0hxCdEik=gMHNyHKNpN=qk3jp3O_FyubqKF+nQ-Kg@mail.gmail.com>

I should clarify that the original *problem* wasn't a case of environment()<-
but a proposed solution was.  As the proposer wrote,
"However, this can lead to headaches downstream".

We agree that it is very important and useful to understand environments in R.
I once set out to construct a formal model of environments in S and R and how
lookup works.  I lost track of all the functions I had to consider and gave up.

On Thu, 7 May 2020 at 16:50, Mark Leeds <markleeds2 at gmail.com> wrote:
>
> Richard: I may have implied that one should "mess with environments" by saying that I agree with Abby. If so, my apologies because that's not what I meant.
> I only meant understanding.
>
>
>
> On Thu, May 7, 2020 at 12:47 AM Mark Leeds <markleeds2 at gmail.com> wrote:
>>
>> Hi Richard: I didn't say it was and didn't mean to imply it.  All I said is that it was an example of where "not understanding environments"  can lead
>> to errors that won't be understood by the person experiencing them. It just so happens that the use of  "environment(f) <- " can lead to a solution but there
>> are other solutions and it was not my intention to talk about the various solutions. Only that it can be helpful if one understands the notion of environments in R.
>>
>>
>>
>>
>>
>>
>>
>>
>> On Thu, May 7, 2020 at 12:42 AM Richard O'Keefe <raoknz at gmail.com> wrote:
>>>
>>> That example is NOT an example of "messing around with environments."
>>>
>>> On Thu, 7 May 2020 at 15:36, Mark Leeds <markleeds2 at gmail.com> wrote:
>>> >
>>> > Hi Abby: I agree with you because below is a perfect example of where not understanding environments causes a somewhat
>>> > mysterious problem. Chuck Berry explains it in a follow up  email.
>>> >
>>> > https://r.789695.n4.nabble.com/issues-with-environment-handling-in-model-frame-td4762855.html
>>> >
>>> >
>>> > On Wed, May 6, 2020 at 11:28 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
>>> >>
>>> >> > If you want to mess around with the environment of a
>>> >> > function, then you need to understand this stuff, but you probably
>>> >> > shouldn't do that.
>>> >>
>>> >> What exactly do you mean?
>>> >>
>>> >> Are you implying that a user should not use environments?
>>> >> In which case, I would disagree.
>>> >>
>>> >> ______________________________________________
>>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>>> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> >> and provide commented, minimal, self-contained, reproducible code.


From @purd|e@@ @end|ng |rom gm@||@com  Thu May  7 07:32:22 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Thu, 7 May 2020 17:32:22 +1200
Subject: [R] 
 'closure' (was "stats:: spline's method could not be monoH.FC")
In-Reply-To: <CAHz+bWbH4n2_4TtzNHs4T2J6j2JCJ6P-uQjOGtrucfRA2-m+YA@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
 <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
 <24240.8418.914054.939133@stat.math.ethz.ch>
 <CAB8pepxxZS+xEEU85bbeMHKDRN3o=i9+rsj5nYAfLLf7YsnYzw@mail.gmail.com>
 <CABcYAd+iYTSWPbh=XqWvTB+utkyc1_3Sy96apJ-vBPVZ6tWLzA@mail.gmail.com>
 <CAB8pepytqkTjgrDKFOVUM=hPgG4KHtRtQu70nu=BeKKiiYxP3Q@mail.gmail.com>
 <CAHz+bWZYZOof=gpVzNV3Rvp5Jtmp7+xk5N3BfyG57w=mtPVBfw@mail.gmail.com>
 <CABcYAdKcVEQOMXdXnmR6+SBWu8S+DJD10Mx+FbJGE3FBNwqFyQ@mail.gmail.com>
 <CAHz+bWa0iS6ToNroo2KdoP+qSD_DYtty6T534WLxs60xf2GKBg@mail.gmail.com>
 <CAHz+bWbH4n2_4TtzNHs4T2J6j2JCJ6P-uQjOGtrucfRA2-m+YA@mail.gmail.com>
Message-ID: <CAB8pepxFXt7V15EKXngc57HiSwBSLoSdjLqs8kMCZgGzzHKNHg@mail.gmail.com>

On Thu, May 7, 2020 at 4:50 PM Mark Leeds <markleeds2 at gmail.com> wrote:
> Richard: I may have implied that one should "mess with environments" by saying that I agree with Abby. If so, my apologies because that's not what I meant.
> I only meant understanding.

Hi Mark,
I don't think you need to apologize.

According the the Cambridge English Dictionary, "Mess Around" means:

    to spend time doing various things
    that are not important,
    without any particular purpose or plan.

Substituting that into the original statement we get:

    If you want to spend time doing various things
    with the environment of a function,
    that are not important,
    without any particular purpose or plan,
    then you need to understand this stuff, but you probably
    shouldn't do that.

It's not my fault or yours, if the previous poster choose ambiguous
language with condescending tone.

I note that the purpose of this thread was to create clarity
surrounding closures and related topics.


From p@u|bern@|07 @end|ng |rom gm@||@com  Thu May  7 07:46:45 2020
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Thu, 7 May 2020 00:46:45 -0500
Subject: [R] 
 Working with very large datasets and generating an executable file
In-Reply-To: <CAB8pepy3XPii53Jz46bSJ07JJqeV_OSjixEo2ZBLrkS1f86Nmw@mail.gmail.com>
References: <CAMOcQfMHwmsS-4Ewjt_OSEy3vUbHUutN5Dqgjr2a4K1DFzbqXA@mail.gmail.com>
 <CAB8pepy3XPii53Jz46bSJ07JJqeV_OSjixEo2ZBLrkS1f86Nmw@mail.gmail.com>
Message-ID: <CAMOcQfOGZ+hRXF4uZkmRxy0P6g4oLQVYfnxW3o=DkfvN0t36xw@mail.gmail.com>

Thank you Abby!

 Cheers!

El mi?., 6 de mayo de 2020 10:35 p. m., Abby Spurdle <spurdle.a at gmail.com>
escribi?:

> > The second question is, is there a way I can develop an R model and turn
> it
> > into an executable program that can work on any OS?
>
> ------myrscript.c--------
> int main (int argc, char* argv [])
> {   system ("Rscript myrscript.r");
>     return 0;
> }
> -------------------------
>
> command line > gcc -o myrscript.exe myrscript.c
> command line > myrscript.exe
>

	[[alternative HTML version deleted]]


From p@u|bern@|07 @end|ng |rom gm@||@com  Thu May  7 07:53:00 2020
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Thu, 7 May 2020 00:53:00 -0500
Subject: [R] 
 Working with very large datasets and generating an executable file
In-Reply-To: <72C06F2F-A6EB-40AA-83C8-7E01DBA9D61E@dcn.davis.ca.us>
References: <CAMOcQfMHwmsS-4Ewjt_OSEy3vUbHUutN5Dqgjr2a4K1DFzbqXA@mail.gmail.com>
 <72C06F2F-A6EB-40AA-83C8-7E01DBA9D61E@dcn.davis.ca.us>
Message-ID: <CAMOcQfMdBne1PqEQoLND6xt_WGgQZ=Kji0N7AdVoU3QH9TUAgA@mail.gmail.com>

Dear Jeff,

Thank you for the feedback. So, after reading your comments, it seems that,
in order to develop an executable model that could be run in any OS, python
might be the way to go then?

I appreciate all of your valuable responses.

Best regards,

Paul

El mi?., 6 de mayo de 2020 6:22 p. m., Jeff Newmiller <
jdnewmil at dcn.davis.ca.us> escribi?:

> Large data... yes, though how this can be done may vary. I have used
> machines with 128G of RAM before with no special big data packages.
>
> Making an executable... theoretically, yes, though there are some
> significant technical (and possibly legal) challenges that will most likely
> make you question whether it was worth it if you try, particularly if your
> intent is to obscure your code from the recipient. I (as a random user and
> programmer on the Internet) would strongly discourage such efforts... it
> will almost certainly be more practical to deliver code in script/package
> form.
>
> On May 6, 2020 2:20:47 PM PDT, Paul Bernal <paulbernal07 at gmail.com> wrote:
> >Dear R friends,
> >
> >Hope you are doing well. I have two questions, the first one is, can I
> >work
> >with very large datasets in R? That is, say I need to test several
> >machine
> >learning algorithms, like (random forest, multiple linear regression,
> >etc.)
> >on datasets having between 50 to 100 columns and 20 million
> >observations,
> >is there any way that R can handle data that large?
> >
> >The second question is, is there a way I can develop an R model and
> >turn it
> >into an executable program that can work on any OS?
> >
> >Any help and/or guidance will be greatly appreciated,
> >
> >Best regards,
> >
> >Paul
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu May  7 08:22:46 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 06 May 2020 23:22:46 -0700
Subject: [R] 
 Working with very large datasets and generating an executable file
In-Reply-To: <CAMOcQfMdBne1PqEQoLND6xt_WGgQZ=Kji0N7AdVoU3QH9TUAgA@mail.gmail.com>
References: <CAMOcQfMHwmsS-4Ewjt_OSEy3vUbHUutN5Dqgjr2a4K1DFzbqXA@mail.gmail.com>
 <72C06F2F-A6EB-40AA-83C8-7E01DBA9D61E@dcn.davis.ca.us>
 <CAMOcQfMdBne1PqEQoLND6xt_WGgQZ=Kji0N7AdVoU3QH9TUAgA@mail.gmail.com>
Message-ID: <6A387E70-B031-44E3-8B5D-BE4AA244B6C7@dcn.davis.ca.us>

There is no executable that can run on any OS. As for python... it is hardly the only game in town for building executables, but it and those other options are off topic here.

On May 6, 2020 10:53:00 PM PDT, Paul Bernal <paulbernal07 at gmail.com> wrote:
>Dear Jeff,
>
>Thank you for the feedback. So, after reading your comments, it seems
>that,
>in order to develop an executable model that could be run in any OS,
>python
>might be the way to go then?
>
>I appreciate all of your valuable responses.
>
>Best regards,
>
>Paul
>
>El mi?., 6 de mayo de 2020 6:22 p. m., Jeff Newmiller <
>jdnewmil at dcn.davis.ca.us> escribi?:
>
>> Large data... yes, though how this can be done may vary. I have used
>> machines with 128G of RAM before with no special big data packages.
>>
>> Making an executable... theoretically, yes, though there are some
>> significant technical (and possibly legal) challenges that will most
>likely
>> make you question whether it was worth it if you try, particularly if
>your
>> intent is to obscure your code from the recipient. I (as a random
>user and
>> programmer on the Internet) would strongly discourage such efforts...
>it
>> will almost certainly be more practical to deliver code in
>script/package
>> form.
>>
>> On May 6, 2020 2:20:47 PM PDT, Paul Bernal <paulbernal07 at gmail.com>
>wrote:
>> >Dear R friends,
>> >
>> >Hope you are doing well. I have two questions, the first one is, can
>I
>> >work
>> >with very large datasets in R? That is, say I need to test several
>> >machine
>> >learning algorithms, like (random forest, multiple linear
>regression,
>> >etc.)
>> >on datasets having between 50 to 100 columns and 20 million
>> >observations,
>> >is there any way that R can handle data that large?
>> >
>> >The second question is, is there a way I can develop an R model and
>> >turn it
>> >into an executable program that can work on any OS?
>> >
>> >Any help and/or guidance will be greatly appreciated,
>> >
>> >Best regards,
>> >
>> >Paul
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> >______________________________________________
>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >PLEASE do read the posting guide
>> >http://www.R-project.org/posting-guide.html
>> >and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Sent from my phone. Please excuse my brevity.
>>

-- 
Sent from my phone. Please excuse my brevity.


From p@u|bern@|07 @end|ng |rom gm@||@com  Thu May  7 08:39:09 2020
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Thu, 7 May 2020 01:39:09 -0500
Subject: [R] 
 Working with very large datasets and generating an executable file
In-Reply-To: <6A387E70-B031-44E3-8B5D-BE4AA244B6C7@dcn.davis.ca.us>
References: <CAMOcQfMHwmsS-4Ewjt_OSEy3vUbHUutN5Dqgjr2a4K1DFzbqXA@mail.gmail.com>
 <72C06F2F-A6EB-40AA-83C8-7E01DBA9D61E@dcn.davis.ca.us>
 <CAMOcQfMdBne1PqEQoLND6xt_WGgQZ=Kji0N7AdVoU3QH9TUAgA@mail.gmail.com>
 <6A387E70-B031-44E3-8B5D-BE4AA244B6C7@dcn.davis.ca.us>
Message-ID: <CAMOcQfP+f2uKcH+TY+5QT1CAF6mBss0ZOwc5ZP11QJV5jrzJ5A@mail.gmail.com>

Dear Jeff, an executable in terms of deploying a machine learning model,
whether it a classifocation, regression, time series or deep learning model.

Best regards,

Paul

El jue., 7 de mayo de 2020 1:22 a. m., Jeff Newmiller <
jdnewmil at dcn.davis.ca.us> escribi?:

> There is no executable that can run on any OS. As for python... it is
> hardly the only game in town for building executables, but it and those
> other options are off topic here.
>
> On May 6, 2020 10:53:00 PM PDT, Paul Bernal <paulbernal07 at gmail.com>
> wrote:
> >Dear Jeff,
> >
> >Thank you for the feedback. So, after reading your comments, it seems
> >that,
> >in order to develop an executable model that could be run in any OS,
> >python
> >might be the way to go then?
> >
> >I appreciate all of your valuable responses.
> >
> >Best regards,
> >
> >Paul
> >
> >El mi?., 6 de mayo de 2020 6:22 p. m., Jeff Newmiller <
> >jdnewmil at dcn.davis.ca.us> escribi?:
> >
> >> Large data... yes, though how this can be done may vary. I have used
> >> machines with 128G of RAM before with no special big data packages.
> >>
> >> Making an executable... theoretically, yes, though there are some
> >> significant technical (and possibly legal) challenges that will most
> >likely
> >> make you question whether it was worth it if you try, particularly if
> >your
> >> intent is to obscure your code from the recipient. I (as a random
> >user and
> >> programmer on the Internet) would strongly discourage such efforts...
> >it
> >> will almost certainly be more practical to deliver code in
> >script/package
> >> form.
> >>
> >> On May 6, 2020 2:20:47 PM PDT, Paul Bernal <paulbernal07 at gmail.com>
> >wrote:
> >> >Dear R friends,
> >> >
> >> >Hope you are doing well. I have two questions, the first one is, can
> >I
> >> >work
> >> >with very large datasets in R? That is, say I need to test several
> >> >machine
> >> >learning algorithms, like (random forest, multiple linear
> >regression,
> >> >etc.)
> >> >on datasets having between 50 to 100 columns and 20 million
> >> >observations,
> >> >is there any way that R can handle data that large?
> >> >
> >> >The second question is, is there a way I can develop an R model and
> >> >turn it
> >> >into an executable program that can work on any OS?
> >> >
> >> >Any help and/or guidance will be greatly appreciated,
> >> >
> >> >Best regards,
> >> >
> >> >Paul
> >> >
> >> >       [[alternative HTML version deleted]]
> >> >
> >> >______________________________________________
> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >https://stat.ethz.ch/mailman/listinfo/r-help
> >> >PLEASE do read the posting guide
> >> >http://www.R-project.org/posting-guide.html
> >> >and provide commented, minimal, self-contained, reproducible code.
> >>
> >> --
> >> Sent from my phone. Please excuse my brevity.
> >>
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From p@u|bern@|07 @end|ng |rom gm@||@com  Thu May  7 08:40:00 2020
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Thu, 7 May 2020 01:40:00 -0500
Subject: [R] 
 Working with very large datasets and generating an executable file
In-Reply-To: <0f0645d7-2c4a-4d5a-9d52-b29c5a3cf9fe@email.android.com>
References: <CAMOcQfMdBne1PqEQoLND6xt_WGgQZ=Kji0N7AdVoU3QH9TUAgA@mail.gmail.com>
 <0f0645d7-2c4a-4d5a-9d52-b29c5a3cf9fe@email.android.com>
Message-ID: <CAMOcQfMNOXF4sMFPMFSp4-PZ4jotvSMwEAE9yZpiOh6fDKKOgA@mail.gmail.com>

That could be the answer, yes.

El jue., 7 de mayo de 2020 1:22 a. m., <cpolwart at chemo.org.uk> escribi?:

> Or maybe a Shiny Application?
>
> On 7 May 2020 06:53, Paul Bernal <paulbernal07 at gmail.com> wrote:
>
> Dear Jeff,
>
> Thank you for the feedback. So, after reading your comments, it seems
> that,
> in order to develop an executable model that could be run in any OS,
> python
> might be the way to go then?
>
> I appreciate all of your valuable responses.
>
> Best regards,
>
> Paul
>
> El mi?., 6 de mayo de 2020 6:22 p. m., Jeff Newmiller <
> jdnewmil at dcn.davis.ca.us> escribi?:
>
> > Large data... yes, though how this can be done may vary. I have used
> > machines with 128G of RAM before with no special big data packages.
> >
> > Making an executable... theoretically, yes, though there are some
> > significant technical (and possibly legal) challenges that will most
> likely
> > make you question whether it was worth it if you try, particularly if
> your
> > intent is to obscure your code from the recipient. I (as a random user
> and
> > programmer on the Internet) would strongly discourage such efforts... it
> > will almost certainly be more practical to deliver code in
> script/package
> > form.
> >
> > On May 6, 2020 2:20:47 PM PDT, Paul Bernal <paulbernal07 at gmail.com>
> wrote:
> > >Dear R friends,
> > >
> > >Hope you are doing well. I have two questions, the first one is, can I
> > >work
> > >with very large datasets in R? That is, say I need to test several
> > >machine
> > >learning algorithms, like (random forest, multiple linear regression,
> > >etc.)
> > >on datasets having between 50 to 100 columns and 20 million
> > >observations,
> > >is there any way that R can handle data that large?
> > >
> > >The second question is, is there a way I can develop an R model and
> > >turn it
> > >into an executable program that can work on any OS?
> > >
> > >Any help and/or guidance will be greatly appreciated,
> > >
> > >Best regards,
> > >
> > >Paul
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > >______________________________________________
> > >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >https://stat.ethz.ch/mailman/listinfo/r-help
> > >PLEASE do read the posting guide
> > >http://www.R-project.org/posting-guide.html
> > >and provide commented, minimal, self-contained, reproducible code.
> >
> > --
> > Sent from my phone. Please excuse my brevity.
> >
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu May  7 09:12:24 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 07 May 2020 00:12:24 -0700
Subject: [R] 
 Working with very large datasets and generating an executable file
In-Reply-To: <CAMOcQfP+f2uKcH+TY+5QT1CAF6mBss0ZOwc5ZP11QJV5jrzJ5A@mail.gmail.com>
References: <CAMOcQfMHwmsS-4Ewjt_OSEy3vUbHUutN5Dqgjr2a4K1DFzbqXA@mail.gmail.com>
 <72C06F2F-A6EB-40AA-83C8-7E01DBA9D61E@dcn.davis.ca.us>
 <CAMOcQfMdBne1PqEQoLND6xt_WGgQZ=Kji0N7AdVoU3QH9TUAgA@mail.gmail.com>
 <6A387E70-B031-44E3-8B5D-BE4AA244B6C7@dcn.davis.ca.us>
 <CAMOcQfP+f2uKcH+TY+5QT1CAF6mBss0ZOwc5ZP11QJV5jrzJ5A@mail.gmail.com>
Message-ID: <3BD66A20-BB5F-438D-9DFE-76172CD8DB18@dcn.davis.ca.us>

You could deploy a rocker image, possibly with an API (built with plumber). But I think it is misleading to refer to that as an executable.

On May 6, 2020 11:39:09 PM PDT, Paul Bernal <paulbernal07 at gmail.com> wrote:
>Dear Jeff, an executable in terms of deploying a machine learning
>model,
>whether it a classifocation, regression, time series or deep learning
>model.
>
>Best regards,
>
>Paul
>
>El jue., 7 de mayo de 2020 1:22 a. m., Jeff Newmiller <
>jdnewmil at dcn.davis.ca.us> escribi?:
>
>> There is no executable that can run on any OS. As for python... it is
>> hardly the only game in town for building executables, but it and
>those
>> other options are off topic here.
>>
>> On May 6, 2020 10:53:00 PM PDT, Paul Bernal <paulbernal07 at gmail.com>
>> wrote:
>> >Dear Jeff,
>> >
>> >Thank you for the feedback. So, after reading your comments, it
>seems
>> >that,
>> >in order to develop an executable model that could be run in any OS,
>> >python
>> >might be the way to go then?
>> >
>> >I appreciate all of your valuable responses.
>> >
>> >Best regards,
>> >
>> >Paul
>> >
>> >El mi?., 6 de mayo de 2020 6:22 p. m., Jeff Newmiller <
>> >jdnewmil at dcn.davis.ca.us> escribi?:
>> >
>> >> Large data... yes, though how this can be done may vary. I have
>used
>> >> machines with 128G of RAM before with no special big data
>packages.
>> >>
>> >> Making an executable... theoretically, yes, though there are some
>> >> significant technical (and possibly legal) challenges that will
>most
>> >likely
>> >> make you question whether it was worth it if you try, particularly
>if
>> >your
>> >> intent is to obscure your code from the recipient. I (as a random
>> >user and
>> >> programmer on the Internet) would strongly discourage such
>efforts...
>> >it
>> >> will almost certainly be more practical to deliver code in
>> >script/package
>> >> form.
>> >>
>> >> On May 6, 2020 2:20:47 PM PDT, Paul Bernal
><paulbernal07 at gmail.com>
>> >wrote:
>> >> >Dear R friends,
>> >> >
>> >> >Hope you are doing well. I have two questions, the first one is,
>can
>> >I
>> >> >work
>> >> >with very large datasets in R? That is, say I need to test
>several
>> >> >machine
>> >> >learning algorithms, like (random forest, multiple linear
>> >regression,
>> >> >etc.)
>> >> >on datasets having between 50 to 100 columns and 20 million
>> >> >observations,
>> >> >is there any way that R can handle data that large?
>> >> >
>> >> >The second question is, is there a way I can develop an R model
>and
>> >> >turn it
>> >> >into an executable program that can work on any OS?
>> >> >
>> >> >Any help and/or guidance will be greatly appreciated,
>> >> >
>> >> >Best regards,
>> >> >
>> >> >Paul
>> >> >
>> >> >       [[alternative HTML version deleted]]
>> >> >
>> >> >______________________________________________
>> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >> >PLEASE do read the posting guide
>> >> >http://www.R-project.org/posting-guide.html
>> >> >and provide commented, minimal, self-contained, reproducible
>code.
>> >>
>> >> --
>> >> Sent from my phone. Please excuse my brevity.
>> >>
>>
>> --
>> Sent from my phone. Please excuse my brevity.
>>

-- 
Sent from my phone. Please excuse my brevity.


From deep@y@n@@@rk@r @end|ng |rom gm@||@com  Thu May  7 11:19:16 2020
From: deep@y@n@@@rk@r @end|ng |rom gm@||@com (Deepayan Sarkar)
Date: Thu, 7 May 2020 14:49:16 +0530
Subject: [R] COVID-19 datasets...
In-Reply-To: <d6e08a07-71c0-473f-7959-b9e1dcac81d1@simecol.de>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
 <E3562BF5-42BC-43D6-96AB-E043B6EA1C57@jsasoc.com>
 <d6e08a07-71c0-473f-7959-b9e1dcac81d1@simecol.de>
Message-ID: <CADfFDC6uvYytwfrC1xZHBN9=h5KcbYW8B1buwEJFd80ChiwcOQ@mail.gmail.com>

On Thu, May 7, 2020 at 12:58 AM Thomas Petzoldt <thpe at simecol.de> wrote:
>
> Sorry if I'm joining a little bit late.
>
> I've put some related links and scripts together a few weeks ago. Then I
> stopped with this, because there is so much.
>
> The data format employed by John Hopkins CSSE was sort of a big surprise
> to me.

Why? I find it quite convenient to drop the first few columns and
extract the data as a matrix (using data.matrix()).

-Deepayan

> An opposite approach was taken in Germany, that organized it as a
> big JSON trees.
>
> Fortunately, both can be "tidied" with R, and represent good didactic
> examples for our students.
>
> Here yet another repo linking to the data:
>
> https://github.com/tpetzoldt/covid
>
>
> Thomas
>
>
> On 04.05.2020 at 20:48 James Spottiswoode wrote:
> > Sure. COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University is available here:
> >
> > https://github.com/CSSEGISandData/COVID-19
> >
> > All in csv fiormat.
> >
> >
> >> On May 4, 2020, at 11:31 AM, Bernard McGarvey <mcgarvey.bernard at comcast.net> wrote:
> >>
> >> Just curious does anyone know of a website that has data available in a format that R can download and analyze?
> >>
> >> Thanks
> >>
> >>
> >> Bernard McGarvey
> >>
> >>
> >> Director, Fort Myers Beach Lions Foundation, Inc.
> >>
> >>
> >> Retired (Lilly Engineering Fellow).
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> > James Spottiswoode
> > Applied Mathematics & Statistics
> > (310) 270 6220
> > jamesspottiswoode Skype
> > james at jsasoc.com
> >
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From thpe @end|ng |rom @|meco|@de  Thu May  7 12:46:34 2020
From: thpe @end|ng |rom @|meco|@de (Thomas Petzoldt)
Date: Thu, 7 May 2020 12:46:34 +0200
Subject: [R] COVID-19 datasets...
In-Reply-To: <CADfFDC6uvYytwfrC1xZHBN9=h5KcbYW8B1buwEJFd80ChiwcOQ@mail.gmail.com>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
 <E3562BF5-42BC-43D6-96AB-E043B6EA1C57@jsasoc.com>
 <d6e08a07-71c0-473f-7959-b9e1dcac81d1@simecol.de>
 <CADfFDC6uvYytwfrC1xZHBN9=h5KcbYW8B1buwEJFd80ChiwcOQ@mail.gmail.com>
Message-ID: <1e2aa343-ebfe-811e-f3e9-0c9bc275396a@simecol.de>

On 07.05.2020 at 11:19 Deepayan Sarkar wrote:
> On Thu, May 7, 2020 at 12:58 AM Thomas Petzoldt <thpe at simecol.de> wrote:
>>
>> Sorry if I'm joining a little bit late.
>>
>> I've put some related links and scripts together a few weeks ago. Then I
>> stopped with this, because there is so much.
>>
>> The data format employed by John Hopkins CSSE was sort of a big surprise
>> to me.
> 
> Why? I find it quite convenient to drop the first few columns and
> extract the data as a matrix (using data.matrix()).
> 
> -Deepayan

Many thanks for the hint to use data.matrix

My aim was not to say that it is difficult, especially as R has all the 
tools for data mangling.

My surprise was that "wide tables" and non-ISO dates as column names are 
not the "data base way" that we in general teach to our students

With reshape2::melt or tidyr::gather resp. pivot_longer, conversion is 
quite easy, regardless if one wants to use tidyverse or not, see example 
below.

Again, thanks, Thomas


library("dplyr")
library("readr")
library("tidyr")

file <- 
"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv"

dat <- read_delim(file, delim=",")
names(dat)[1:2] <- c("Province_State", "Country_Region")
dat2 <-
   dat %>%
   ## summarize Country/Region duplicates
   group_by(Country_Region) %>% summarise_at(vars(-(1:4)), sum) %>%
   ## make it a long table
   pivot_longer(cols = -Country_Region, names_to = "time") %>%
   ## convert to ISO 8601 date
   mutate(time = as.POSIXct(time, format="%m/%e/%y"))



> 
>> An opposite approach was taken in Germany, that organized it as a
>> big JSON trees.
>>
>> Fortunately, both can be "tidied" with R, and represent good didactic
>> examples for our students.
>>
>> Here yet another repo linking to the data:
>>
>> https://github.com/tpetzoldt/covid
>>
>>
>> Thomas
>>
>>
>> On 04.05.2020 at 20:48 James Spottiswoode wrote:
>>> Sure. COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University is available here:
>>>
>>> https://github.com/CSSEGISandData/COVID-19
>>>
>>> All in csv fiormat.
>>>
>>>
>>>> On May 4, 2020, at 11:31 AM, Bernard McGarvey <mcgarvey.bernard at comcast.net> wrote:
>>>>
>>>> Just curious does anyone know of a website that has data available in a format that R can download and analyze?
>>>>
>>>> Thanks
>>>>
>>>>
>>>> Bernard McGarvey
>>>>
>>>>
>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>
>>>>
>>>> Retired (Lilly Engineering Fellow).
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>> James Spottiswoode
>>> Applied Mathematics & Statistics
>>> (310) 270 6220
>>> jamesspottiswoode Skype
>>> james at jsasoc.com
>>>

-- 
Dr. Thomas Petzoldt
senior scientist

Technische Universitaet Dresden
Faculty of Environmental Sciences
Institute of Hydrobiology
01062 Dresden, Germany

https://tu-dresden.de/Members/thomas.petzoldt


From deep@y@n@@@rk@r @end|ng |rom gm@||@com  Thu May  7 13:12:50 2020
From: deep@y@n@@@rk@r @end|ng |rom gm@||@com (Deepayan Sarkar)
Date: Thu, 7 May 2020 16:42:50 +0530
Subject: [R] COVID-19 datasets...
In-Reply-To: <1e2aa343-ebfe-811e-f3e9-0c9bc275396a@simecol.de>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
 <E3562BF5-42BC-43D6-96AB-E043B6EA1C57@jsasoc.com>
 <d6e08a07-71c0-473f-7959-b9e1dcac81d1@simecol.de>
 <CADfFDC6uvYytwfrC1xZHBN9=h5KcbYW8B1buwEJFd80ChiwcOQ@mail.gmail.com>
 <1e2aa343-ebfe-811e-f3e9-0c9bc275396a@simecol.de>
Message-ID: <CADfFDC6Duk3A2YC2a0=K2AW-nDbfezq==uEpBgoUruAYbw_2hQ@mail.gmail.com>

On Thu, May 7, 2020 at 4:16 PM Thomas Petzoldt <thpe at simecol.de> wrote:
>
> On 07.05.2020 at 11:19 Deepayan Sarkar wrote:
> > On Thu, May 7, 2020 at 12:58 AM Thomas Petzoldt <thpe at simecol.de> wrote:
> >>
> >> Sorry if I'm joining a little bit late.
> >>
> >> I've put some related links and scripts together a few weeks ago. Then I
> >> stopped with this, because there is so much.
> >>
> >> The data format employed by John Hopkins CSSE was sort of a big surprise
> >> to me.
> >
> > Why? I find it quite convenient to drop the first few columns and
> > extract the data as a matrix (using data.matrix()).
> >
> > -Deepayan
>
> Many thanks for the hint to use data.matrix
>
> My aim was not to say that it is difficult, especially as R has all the
> tools for data mangling.
>
> My surprise was that "wide tables" and non-ISO dates as column names are
> not the "data base way" that we in general teach to our students

Well, I am all for long format data when it makes sense, but I would
disagree that that is always the "right approach". In the case of
regular multiple time series, as in this context, a matrix-like
structure seems much more natural (and nicely handled by ts() in R),
and I wouldn't even bother reshaping the data in the first place.

See, for example,

https://github.com/deepayan/deepayan.github.io/blob/master/covid-19/deaths.rmd

and

https://deepayan.github.io/covid-19/deaths.html

-Deepayan

> With reshape2::melt or tidyr::gather resp. pivot_longer, conversion is
> quite easy, regardless if one wants to use tidyverse or not, see example
> below.
>
> Again, thanks, Thomas
>
>
> library("dplyr")
> library("readr")
> library("tidyr")
>
> file <-
> "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv"
>
> dat <- read_delim(file, delim=",")
> names(dat)[1:2] <- c("Province_State", "Country_Region")
> dat2 <-
>    dat %>%
>    ## summarize Country/Region duplicates
>    group_by(Country_Region) %>% summarise_at(vars(-(1:4)), sum) %>%
>    ## make it a long table
>    pivot_longer(cols = -Country_Region, names_to = "time") %>%
>    ## convert to ISO 8601 date
>    mutate(time = as.POSIXct(time, format="%m/%e/%y"))
>
>
>
> >
> >> An opposite approach was taken in Germany, that organized it as a
> >> big JSON trees.
> >>
> >> Fortunately, both can be "tidied" with R, and represent good didactic
> >> examples for our students.
> >>
> >> Here yet another repo linking to the data:
> >>
> >> https://github.com/tpetzoldt/covid
> >>
> >>
> >> Thomas
> >>
> >>
> >> On 04.05.2020 at 20:48 James Spottiswoode wrote:
> >>> Sure. COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University is available here:
> >>>
> >>> https://github.com/CSSEGISandData/COVID-19
> >>>
> >>> All in csv fiormat.
> >>>
> >>>
> >>>> On May 4, 2020, at 11:31 AM, Bernard McGarvey <mcgarvey.bernard at comcast.net> wrote:
> >>>>
> >>>> Just curious does anyone know of a website that has data available in a format that R can download and analyze?
> >>>>
> >>>> Thanks
> >>>>
> >>>>
> >>>> Bernard McGarvey
> >>>>
> >>>>
> >>>> Director, Fort Myers Beach Lions Foundation, Inc.
> >>>>
> >>>>
> >>>> Retired (Lilly Engineering Fellow).
> >>>>
> >>>> ______________________________________________
> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>>
> >>> James Spottiswoode
> >>> Applied Mathematics & Statistics
> >>> (310) 270 6220
> >>> jamesspottiswoode Skype
> >>> james at jsasoc.com
> >>>
>
> --
> Dr. Thomas Petzoldt
> senior scientist
>
> Technische Universitaet Dresden
> Faculty of Environmental Sciences
> Institute of Hydrobiology
> 01062 Dresden, Germany
>
> https://tu-dresden.de/Members/thomas.petzoldt


From thpe @end|ng |rom @|meco|@de  Thu May  7 13:53:17 2020
From: thpe @end|ng |rom @|meco|@de (Thomas Petzoldt)
Date: Thu, 7 May 2020 13:53:17 +0200
Subject: [R] COVID-19 datasets...
In-Reply-To: <CADfFDC6Duk3A2YC2a0=K2AW-nDbfezq==uEpBgoUruAYbw_2hQ@mail.gmail.com>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
 <E3562BF5-42BC-43D6-96AB-E043B6EA1C57@jsasoc.com>
 <d6e08a07-71c0-473f-7959-b9e1dcac81d1@simecol.de>
 <CADfFDC6uvYytwfrC1xZHBN9=h5KcbYW8B1buwEJFd80ChiwcOQ@mail.gmail.com>
 <1e2aa343-ebfe-811e-f3e9-0c9bc275396a@simecol.de>
 <CADfFDC6Duk3A2YC2a0=K2AW-nDbfezq==uEpBgoUruAYbw_2hQ@mail.gmail.com>
Message-ID: <e7ddde8e-128b-339e-1cb4-1af4e14ad760@simecol.de>

On 07.05.2020 at 13:12 Deepayan Sarkar wrote:
> On Thu, May 7, 2020 at 4:16 PM Thomas Petzoldt <thpe at simecol.de> wrote:
>> On 07.05.2020 at 11:19 Deepayan Sarkar wrote:
>>> On Thu, May 7, 2020 at 12:58 AM Thomas Petzoldt <thpe at simecol.de> wrote:
>>>> Sorry if I'm joining a little bit late.
>>>>
>>>> I've put some related links and scripts together a few weeks ago. Then I
>>>> stopped with this, because there is so much.
>>>>
>>>> The data format employed by John Hopkins CSSE was sort of a big surprise
>>>> to me.
>>> Why? I find it quite convenient to drop the first few columns and
>>> extract the data as a matrix (using data.matrix()).
>>>
>>> -Deepayan
>> Many thanks for the hint to use data.matrix
>>
>> My aim was not to say that it is difficult, especially as R has all the
>> tools for data mangling.
>>
>> My surprise was that "wide tables" and non-ISO dates as column names are
>> not the "data base way" that we in general teach to our students
> Well, I am all for long format data when it makes sense, but I would
> disagree that that is always the "right approach". In the case of
> regular multiple time series, as in this context, a matrix-like
> structure seems much more natural (and nicely handled by ts() in R),
> and I wouldn't even bother reshaping the data in the first place.
>
> See, for example,
>
> https://github.com/deepayan/deepayan.github.io/blob/master/covid-19/deaths.rmd
>
> and
>
> https://deepayan.github.io/covid-19/deaths.html
>
> -Deepayan

Great, thank you for the link with the comprehensive lattice graphs and 
the explanations. I like your package very much and use it often, since 
it appeared on CRAN (3 of my CRAN packages depend on it). As "dynamic 
modeller", I consider time always as the first column, but I agree on 
the other hand, that long tables are often, but not always the right 
approach, let's think about gridded multi-dimensional netcdf data.

Many thanks for sharing your analysis publicly, I'll add your repo to my 
link list.

Thomas

>> With reshape2::melt or tidyr::gather resp. pivot_longer, conversion is
>> quite easy, regardless if one wants to use tidyverse or not, see example
>> below.
>>
>> Again, thanks, Thomas
>>
>>
>> library("dplyr")
>> library("readr")
>> library("tidyr")
>>
>> file <-
>> "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv"
>>
>> dat <- read_delim(file, delim=",")
>> names(dat)[1:2] <- c("Province_State", "Country_Region")
>> dat2 <-
>>     dat %>%
>>     ## summarize Country/Region duplicates
>>     group_by(Country_Region) %>% summarise_at(vars(-(1:4)), sum) %>%
>>     ## make it a long table
>>     pivot_longer(cols = -Country_Region, names_to = "time") %>%
>>     ## convert to ISO 8601 date
>>     mutate(time = as.POSIXct(time, format="%m/%e/%y"))
>>
>>
>>
>>>> An opposite approach was taken in Germany, that organized it as a
>>>> big JSON trees.
>>>>
>>>> Fortunately, both can be "tidied" with R, and represent good didactic
>>>> examples for our students.
>>>>
>>>> Here yet another repo linking to the data:
>>>>
>>>> https://github.com/tpetzoldt/covid
>>>>
>>>>
>>>> Thomas
>>>>
>>>>
>>>> On 04.05.2020 at 20:48 James Spottiswoode wrote:
>>>>> Sure. COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University is available here:
>>>>>
>>>>> https://github.com/CSSEGISandData/COVID-19
>>>>>
>>>>> All in csv fiormat.
>>>>>
>>>>>
>>>>>> On May 4, 2020, at 11:31 AM, Bernard McGarvey <mcgarvey.bernard at comcast.net> wrote:
>>>>>>
>>>>>> Just curious does anyone know of a website that has data available in a format that R can download and analyze?
>>>>>>
>>>>>> Thanks
>>>>>>
>>>>>>
>>>>>> Bernard McGarvey
>>>>>>
>>>>>>
>>>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>>>
>>>>>>
>>>>>> Retired (Lilly Engineering Fellow).
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>
>>>>> James Spottiswoode
>>>>> Applied Mathematics & Statistics
>>>>> (310) 270 6220
>>>>> jamesspottiswoode Skype
>>>>> james at jsasoc.com
>>>>>
>> --
>> Dr. Thomas Petzoldt
>> senior scientist
>>
>> Technische Universitaet Dresden
>> Faculty of Environmental Sciences
>> Institute of Hydrobiology
>> 01062 Dresden, Germany
>>
>> https://tu-dresden.de/Members/thomas.petzoldt


From @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com  Thu May  7 14:26:58 2020
From: @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com (Subhamitra Patra)
Date: Thu, 7 May 2020 17:56:58 +0530
Subject: [R] [R ] Writing loop to estimate ARCH test for a multiple columns
 of a data frame?
Message-ID: <CAOFE=kMaSgqy7_dMsB3WtTgN+BDa4b5m28GO0uxm0rUsDmqc9g@mail.gmail.com>

Dear R-users,

I want to estimate ARCH test for multiple columns (i.e.,  from 2:21 COLUMNS
) in my data. For this purpose, I want to run a loop to calculate ARCH test
results for each column in the data frame. I tried by using for loop and
lapply function, but unable to write a loop for computing the ARCH test
simultaneously for each column (i.e., from 2:21 columns) of my data frame.

Below is my ARCH test code which I want to estimate for multiple columns of
the data frame in a loop.

library(tseries)

library(FinTS)

ArchTest (A, lags=1, demean = FALSE)

Hence, A is a vector for which the ARCH test result is calculated. Here, I
want to write a loop so that the ArchTest can be calculated simultaneously
for each column of my data frame. From ARCH test result, I require only the
calculated Chi-square value and its p-value for each column that stored in
another matrix or object for each column as an output file.

For your convenience, I attached my sample data below. Please find it.

Please help me for which I shall be always grateful to you.

Thank you.

-- 
*Best Regards,*
*Subhamitra Patra*
*Phd. Research Scholar*
*Department of Humanities and Social Sciences*
*Indian Institute of Technology, Kharagpur*
*INDIA*

[image: Mailtrack]
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
Sender
notified by
Mailtrack
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
05/07/20,
05:51:03 PM

From petr@p|k@| @end|ng |rom prechez@@cz  Thu May  7 15:12:53 2020
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Thu, 7 May 2020 13:12:53 +0000
Subject: [R] unstable results of nlxb fit
Message-ID: <745db2c879ba43149ff7e1f009219cca@SRVEXCHCM1302.precheza.cz>

Dear all

I started to use nlxb instead of nls to get rid of singular gradient error.
I try to fit double exponential function to my data, but results I obtain
are strongly dependent on starting values. 

tsmes ~ A*exp(a*plast) + B* exp(b*plast)

Changing b from 0.1 to 0.01 gives me completely different results. I usually
check result by a plot but could the result be inspected if it achieved good
result without plotting?

Or is there any way how to perform such task?

Cheers
Petr

Below is working example.

> dput(temp)
temp <- structure(list(tsmes = c(31, 32, 32, 32, 32, 32, 32, 32, 33, 
34, 35, 35, 36, 36, 36, 37, 38, 39, 40, 40, 40, 40, 40, 41, 43, 
44, 44, 44, 46, 47, 47, 47, 47, 48, 49, 51, 51, 51, 52, 53, 54, 
54, 55, 57, 57, 57, 59, 59, 60, 62, 63, 64, 65, 66, 66, 67, 67, 
68, 70, 72, 74, 76, 78, 81, 84, 85, 86, 88, 90, 91, 92, 94, 96, 
97, 99, 100, 102, 104, 106, 109, 112, 115, 119, 123, 127, 133, 
141, 153, 163, 171), plast = c(50, 51, 52, 52, 53, 53, 53, 54, 
55, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 64, 64, 65, 65, 66, 
66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 75, 76, 76, 77, 77, 78, 
78, 79, 80, 81, 82, 83, 84, 85, 85, 86, 86, 87, 88, 88, 89, 90, 
91, 91, 93, 93, 94, 95, 96, 96, 97, 98, 98, 99, 100, 100, 101, 
102, 103, 103, 104, 105, 106, 107, 107, 108, 109, 110, 111, 112, 
112, 113, 113, 114, 115, 116)), row.names = 2411:2500, class = "data.frame")

library(nlsr)

fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
start=list(A=1, B=15, a=0.025, b=0.01))
coef(fit)
           A            B            a            b 
3.945167e-13 9.772749e+00 2.802274e-01 2.179781e-02 

plot(temp$plast, temp$tsmes, ylim=c(0,200))
lines(temp$plast, predict(fit, newdata=temp), col="pink", lwd=3)
ccc <- coef(fit)
lines(0:120,ccc[1]*exp(ccc[3]*(0:120)))
lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2)

# wrong fit with slightly different b
fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
start=list(A=1, B=15, a=0.025, b=0.1))
coef(fit)
           A            B            a            b 
2911.6448377    6.8320597  -49.1373979    0.0261391 
lines(temp$plast, predict(fit, newdata=temp), col="red", lwd=3)
ccc <- coef(fit)
lines(0:120,ccc[1]*exp(ccc[3]*(0:120)), col="red")
lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2, col="red")


From pro|jcn@@h @end|ng |rom gm@||@com  Thu May  7 15:33:32 2020
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Thu, 7 May 2020 09:33:32 -0400
Subject: [R] unstable results of nlxb fit
In-Reply-To: <745db2c879ba43149ff7e1f009219cca@SRVEXCHCM1302.precheza.cz>
References: <745db2c879ba43149ff7e1f009219cca@SRVEXCHCM1302.precheza.cz>
Message-ID: <16e4ad6a-12dd-ccfe-0b54-403be54d8b6e@gmail.com>

The double exponential is well-known as a disaster to fit. Lanczos in his
1956 book Applied Analysis, p. 276 gives a good example which is worked through.
I've included it with scripts using nlxb in my 2014 book on Nonlinear Parameter
Optimization Using R Tools (Wiley). The scripts were on Wiley's site for the book,
but I've had difficulty getting Wiley to fix things and not checked lately if it
is still accessible. Ask off-list if you want the script and I'll dig into my
archives.

nlxb (preferably from nlsr which you used rather than nlmrt which is now not
maintained), will likely do as well as any general purpose code. There may be
special approaches that do a bit better, but I suspect the reality is that
the underlying problem is such that there are many sets of parameters with
widely different values that will get quite similar sums of squares.

Best, JN


On 2020-05-07 9:12 a.m., PIKAL Petr wrote:
> Dear all
> 
> I started to use nlxb instead of nls to get rid of singular gradient error.
> I try to fit double exponential function to my data, but results I obtain
> are strongly dependent on starting values. 
> 
> tsmes ~ A*exp(a*plast) + B* exp(b*plast)
> 
> Changing b from 0.1 to 0.01 gives me completely different results. I usually
> check result by a plot but could the result be inspected if it achieved good
> result without plotting?
> 
> Or is there any way how to perform such task?
> 
> Cheers
> Petr
> 
> Below is working example.
> 
>> dput(temp)
> temp <- structure(list(tsmes = c(31, 32, 32, 32, 32, 32, 32, 32, 33, 
> 34, 35, 35, 36, 36, 36, 37, 38, 39, 40, 40, 40, 40, 40, 41, 43, 
> 44, 44, 44, 46, 47, 47, 47, 47, 48, 49, 51, 51, 51, 52, 53, 54, 
> 54, 55, 57, 57, 57, 59, 59, 60, 62, 63, 64, 65, 66, 66, 67, 67, 
> 68, 70, 72, 74, 76, 78, 81, 84, 85, 86, 88, 90, 91, 92, 94, 96, 
> 97, 99, 100, 102, 104, 106, 109, 112, 115, 119, 123, 127, 133, 
> 141, 153, 163, 171), plast = c(50, 51, 52, 52, 53, 53, 53, 54, 
> 55, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 64, 64, 65, 65, 66, 
> 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 75, 76, 76, 77, 77, 78, 
> 78, 79, 80, 81, 82, 83, 84, 85, 85, 86, 86, 87, 88, 88, 89, 90, 
> 91, 91, 93, 93, 94, 95, 96, 96, 97, 98, 98, 99, 100, 100, 101, 
> 102, 103, 103, 104, 105, 106, 107, 107, 108, 109, 110, 111, 112, 
> 112, 113, 113, 114, 115, 116)), row.names = 2411:2500, class = "data.frame")
> 
> library(nlsr)
> 
> fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
> start=list(A=1, B=15, a=0.025, b=0.01))
> coef(fit)
>            A            B            a            b 
> 3.945167e-13 9.772749e+00 2.802274e-01 2.179781e-02 
> 
> plot(temp$plast, temp$tsmes, ylim=c(0,200))
> lines(temp$plast, predict(fit, newdata=temp), col="pink", lwd=3)
> ccc <- coef(fit)
> lines(0:120,ccc[1]*exp(ccc[3]*(0:120)))
> lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2)
> 
> # wrong fit with slightly different b
> fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
> start=list(A=1, B=15, a=0.025, b=0.1))
> coef(fit)
>            A            B            a            b 
> 2911.6448377    6.8320597  -49.1373979    0.0261391 
> lines(temp$plast, predict(fit, newdata=temp), col="red", lwd=3)
> ccc <- coef(fit)
> lines(0:120,ccc[1]*exp(ccc[3]*(0:120)), col="red")
> lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2, col="red")
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @orenh @end|ng |rom m@th@@@u@dk  Thu May  7 15:42:59 2020
From: @orenh @end|ng |rom m@th@@@u@dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Thu, 7 May 2020 13:42:59 +0000
Subject: [R] Possible bug in optimize (related to naming the arguments)
Message-ID: <77eb990781cc46618e9cf78b8f1246ba@math.aau.dk>

Dear all,

I am wondering if there is a minor bug in the optimimize function; please see below:


---


> ## example taken from optimize documentation
> f <- function (x, a) (x - a)^2
> xmin <- optimize(f, c(0, 1), tol = 0.0001, a = 1/3)
> xmin
$minimum
[1] 0.3333333

$objective
[1] 0

> ## if we change argument a to j things still work fine
> f2 <- function (x, j) (x - j)^2
> xmin2 <- optimize(f2, c(0, 1), tol = 0.0001, j = 1/3)
> xmin2
$minimum
[1] 0.3333333

$objective
[1] 0

> ## if we change argument a to i things fail
> f3 <- function (x, i) (x - i)^2
> xmin3 <- optimize(f3, c(0, 1), tol = 0.0001, i = 1/3)
Error in optimize(f3, c(0, 1), tol = 1e-04, i = 1/3) :
  'xmin' not less than 'xmax'
> xmin3
$minimum
[1] 0.3333333

$objective
[1] 0

> ##Same here
> xmin3 <- optimize(f3, lower=0, upper=1, tol = 0.0001, i = 1/3)
Error in f(arg, ...) (from #1) : argument "i" is missing, with no default
> xmin3
$minimum
[1] 0.3333333

$objective
[1] 0

> ## a workaround is
> xmin3 <- optimize(f3, interval=c(0, 1), tol = 0.0001, i = 1/3)
> xmin3
$minimum
[1] 0.3333333

$objective
[1] 0

---

the problem is, I guess, due to the keyword 'interval' gets mixed
up with 'i'.

Has anyone experienced that?

Best regards
S?ren



	[[alternative HTML version deleted]]


From rub@k @end|ng |rom m@th@@@u@dk  Thu May  7 16:00:00 2020
From: rub@k @end|ng |rom m@th@@@u@dk (Ege Rubak)
Date: Thu, 7 May 2020 14:00:00 +0000
Subject: [R] Possible bug in optimize (related to naming the arguments)
In-Reply-To: <77eb990781cc46618e9cf78b8f1246ba@math.aau.dk>
References: <77eb990781cc46618e9cf78b8f1246ba@math.aau.dk>
Message-ID: <7a440435cb2517fcc40c94fdf5879d98c039dbb3.camel@math.aau.dk>

Dear S?ren,

I suspect that the good R souls wouldn't consider this a bug, but a
logical consequence of the R language design. It is of course a valid
question whether this should be explicitly mentioned in documentation.

If I recall correctly about function evaluation: First all named
arguments are found by partial matching, so in your example you are
really providing the value 1/3 for the argument `interval` which is a
user mistake. The exact same things happens for the apply family, e.g.

> myfun <- function(x, F) x-F
> sapply(1:3, myfun, F = 1)
Error in match.fun(FUN) : '1' is not a function, character or symbol

It works fine if we provide a valid value for the argument `FUN` rather
than the constant value `FUN = 1`:

> sapply(1:3, FUN = myfun, F = 1)
[1] 0 1 2

(I realize no sane person would use the argument name `F` in this case,
but you get the point.)

Best,
Ege

On Thu, 2020-05-07 at 13:42 +0000, S?ren H?jsgaard wrote:
> Dear all,
> 
> I am wondering if there is a minor bug in the optimimize function;
> please see below:
> 
> 
> ---
> 
> 
> > ## example taken from optimize documentation
> > f <- function (x, a) (x - a)^2
> > xmin <- optimize(f, c(0, 1), tol = 0.0001, a = 1/3)
> > xmin
> 
> $minimum
> [1] 0.3333333
> 
> $objective
> [1] 0
> 
> > ## if we change argument a to j things still work fine
> > f2 <- function (x, j) (x - j)^2
> > xmin2 <- optimize(f2, c(0, 1), tol = 0.0001, j = 1/3)
> > xmin2
> 
> $minimum
> [1] 0.3333333
> 
> $objective
> [1] 0
> 
> > ## if we change argument a to i things fail
> > f3 <- function (x, i) (x - i)^2
> > xmin3 <- optimize(f3, c(0, 1), tol = 0.0001, i = 1/3)
> 
> Error in optimize(f3, c(0, 1), tol = 1e-04, i = 1/3) :
>   'xmin' not less than 'xmax'
> > xmin3
> 
> $minimum
> [1] 0.3333333
> 
> $objective
> [1] 0
> 
> > ##Same here
> > xmin3 <- optimize(f3, lower=0, upper=1, tol = 0.0001, i = 1/3)
> 
> Error in f(arg, ...) (from #1) : argument "i" is missing, with no
> default
> > xmin3
> 
> $minimum
> [1] 0.3333333
> 
> $objective
> [1] 0
> 
> > ## a workaround is
> > xmin3 <- optimize(f3, interval=c(0, 1), tol = 0.0001, i = 1/3)
> > xmin3
> 
> $minimum
> [1] 0.3333333
> 
> $objective
> [1] 0
> 
> ---
> 
> the problem is, I guess, due to the keyword 'interval' gets mixed
> up with 'i'.
> 
> Has anyone experienced that?
> 
> Best regards
> S?ren
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
Ege Rubak, Associate Professor,
Department of Mathematical Sciences, Aalborg University
Skjernvej 4A, 9220 Aalborg East, Denmark
Phone: (+45)99408861
Mobile: (+45)30230252
Email: rubak at math.aau.dk

From murdoch@dunc@n @end|ng |rom gm@||@com  Thu May  7 16:07:41 2020
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Thu, 7 May 2020 10:07:41 -0400
Subject: [R] unstable results of nlxb fit
In-Reply-To: <745db2c879ba43149ff7e1f009219cca@SRVEXCHCM1302.precheza.cz>
References: <745db2c879ba43149ff7e1f009219cca@SRVEXCHCM1302.precheza.cz>
Message-ID: <3d515cd3-ac48-fe4c-52c7-6b957dbe4634@gmail.com>

As John said, sums of exponentials are hard.  One thing that often helps 
a lot is to use the partially linear structure:  given a and b, you've 
got a linear model to compute A and B.  Now that you're down to two 
nonlinear parameters, you can draw a contour plot of nearby values to 
see how much of a mess you're dealing with.

Duncan Murdoch

On 07/05/2020 9:12 a.m., PIKAL Petr wrote:
> Dear all
> 
> I started to use nlxb instead of nls to get rid of singular gradient error.
> I try to fit double exponential function to my data, but results I obtain
> are strongly dependent on starting values.
> 
> tsmes ~ A*exp(a*plast) + B* exp(b*plast)
> 
> Changing b from 0.1 to 0.01 gives me completely different results. I usually
> check result by a plot but could the result be inspected if it achieved good
> result without plotting?
> 
> Or is there any way how to perform such task?
> 
> Cheers
> Petr
> 
> Below is working example.
> 
>> dput(temp)
> temp <- structure(list(tsmes = c(31, 32, 32, 32, 32, 32, 32, 32, 33,
> 34, 35, 35, 36, 36, 36, 37, 38, 39, 40, 40, 40, 40, 40, 41, 43,
> 44, 44, 44, 46, 47, 47, 47, 47, 48, 49, 51, 51, 51, 52, 53, 54,
> 54, 55, 57, 57, 57, 59, 59, 60, 62, 63, 64, 65, 66, 66, 67, 67,
> 68, 70, 72, 74, 76, 78, 81, 84, 85, 86, 88, 90, 91, 92, 94, 96,
> 97, 99, 100, 102, 104, 106, 109, 112, 115, 119, 123, 127, 133,
> 141, 153, 163, 171), plast = c(50, 51, 52, 52, 53, 53, 53, 54,
> 55, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 64, 64, 65, 65, 66,
> 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 75, 76, 76, 77, 77, 78,
> 78, 79, 80, 81, 82, 83, 84, 85, 85, 86, 86, 87, 88, 88, 89, 90,
> 91, 91, 93, 93, 94, 95, 96, 96, 97, 98, 98, 99, 100, 100, 101,
> 102, 103, 103, 104, 105, 106, 107, 107, 108, 109, 110, 111, 112,
> 112, 113, 113, 114, 115, 116)), row.names = 2411:2500, class = "data.frame")
> 
> library(nlsr)
> 
> fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
> start=list(A=1, B=15, a=0.025, b=0.01))
> coef(fit)
>             A            B            a            b
> 3.945167e-13 9.772749e+00 2.802274e-01 2.179781e-02
> 
> plot(temp$plast, temp$tsmes, ylim=c(0,200))
> lines(temp$plast, predict(fit, newdata=temp), col="pink", lwd=3)
> ccc <- coef(fit)
> lines(0:120,ccc[1]*exp(ccc[3]*(0:120)))
> lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2)
> 
> # wrong fit with slightly different b
> fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
> start=list(A=1, B=15, a=0.025, b=0.1))
> coef(fit)
>             A            B            a            b
> 2911.6448377    6.8320597  -49.1373979    0.0261391
> lines(temp$plast, predict(fit, newdata=temp), col="red", lwd=3)
> ccc <- coef(fit)
> lines(0:120,ccc[1]*exp(ccc[3]*(0:120)), col="red")
> lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2, col="red")
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t  Thu May  7 16:09:04 2020
From: @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t (Stefano Sofia)
Date: Thu, 7 May 2020 14:09:04 +0000
Subject: [R] Change the colours of some of the labels of the x-axis
Message-ID: <8B435C9568170B469AE31E8891E8CC4F809B8064@ESINO.regionemarche.intra>

Dear R users,
in a plot I need to change the colours of some of the labels of the x-axis.
In the example below reported, I would like to have the labels of the last and second-last date in red.
I tried to find the solution searching the web, but I could not understand the hints and I was not sure I could adapt them to my example.
There might be an easy way to do that?

Thank you for your attention and your help
Stefano

first_day <- "2005-01-23-09-00"
last_day <- "2005-01-27-09-00"
first_day_POSIX <- as.POSIXct(first_day, format="%Y-%m-%d-%H-%M")
last_day_POSIX <- as.POSIXct(last_day, format="%Y-%m-%d-%H-%M")
df_plot <- data.frame(data_POSIX=seq(first_day_POSIX, last_day_POSIX, by="30 mins"))
df_plot$hs1 <- 5
df_plot$hs2 <- 7

plot(df_plot$data_POSIX, df_plot$hs1, type="b", ylim=c(0, 10), col="blue", xaxt="n", axes=F, main="", xlab="", ylab="Snow height")
lines(df_plot$data_POSIX, df_plot$hs2, ylim=c(0, 10), col="red", type="b")
axis.POSIXct(1, at=seq(trunc(as.POSIXct(first_day, format="%Y-%m-%d-%H-%M"), "days"), trunc(as.POSIXct(last_day, format="%Y-%m-%d-%H-%M"), "days"), by="12 hours"), format="h%H-%d-%m-%y", pos=0)
axis(side=2, at=seq(0, 15, 5))


         (oo)
--oOO--( )--OOo----------------
Stefano Sofia PhD
Civil Protection - Marche Region
Meteo Section
Snow Section
Via del Colle Ameno 5
60126 Torrette di Ancona, Ancona
Uff: 071 806 7743
E-mail: stefano.sofia at regione.marche.it
---Oo---------oO----------------

________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.

--
Questo messaggio  stato analizzato da Libra ESVA ed  risultato non infetto.
This message was scanned by Libra ESVA and is believed to be clean.


	[[alternative HTML version deleted]]


From pd@|gd @end|ng |rom gm@||@com  Thu May  7 16:14:52 2020
From: pd@|gd @end|ng |rom gm@||@com (Peter Dalgaard)
Date: Thu, 7 May 2020 16:14:52 +0200
Subject: [R] Possible bug in optimize (related to naming the arguments)
In-Reply-To: <7a440435cb2517fcc40c94fdf5879d98c039dbb3.camel@math.aau.dk>
References: <77eb990781cc46618e9cf78b8f1246ba@math.aau.dk>
 <7a440435cb2517fcc40c94fdf5879d98c039dbb3.camel@math.aau.dk>
Message-ID: <9C25BBA6-5C8E-45FD-8112-EAFA40E70A34@gmail.com>

Partial matching is a feature regretted by its designer, but is not really possible to remove at this point. One early change in R relative to S was to require full match of anything after "...", but as you have noticed, that does not apply for interval= 

So, "don't do that, then"

-pd

> On 7 May 2020, at 16:00 , Ege Rubak <rubak at math.aau.dk> wrote:
> 
> Dear S?ren,
> 
> I suspect that the good R souls wouldn't consider this a bug, but a
> logical consequence of the R language design. It is of course a valid
> question whether this should be explicitly mentioned in documentation.
> 
> If I recall correctly about function evaluation: First all named
> arguments are found by partial matching, so in your example you are
> really providing the value 1/3 for the argument `interval` which is a
> user mistake. The exact same things happens for the apply family, e.g.
> 
>> myfun <- function(x, F) x-F
>> sapply(1:3, myfun, F = 1)
> Error in match.fun(FUN) : '1' is not a function, character or symbol
> 
> It works fine if we provide a valid value for the argument `FUN` rather
> than the constant value `FUN = 1`:
> 
>> sapply(1:3, FUN = myfun, F = 1)
> [1] 0 1 2
> 
> (I realize no sane person would use the argument name `F` in this case,
> but you get the point.)
> 
> Best,
> Ege
> 
> On Thu, 2020-05-07 at 13:42 +0000, S?ren H?jsgaard wrote:
>> Dear all,
>> 
>> I am wondering if there is a minor bug in the optimimize function;
>> please see below:
>> 
>> 
>> ---
>> 
>> 
>>> ## example taken from optimize documentation
>>> f <- function (x, a) (x - a)^2
>>> xmin <- optimize(f, c(0, 1), tol = 0.0001, a = 1/3)
>>> xmin
>> 
>> $minimum
>> [1] 0.3333333
>> 
>> $objective
>> [1] 0
>> 
>>> ## if we change argument a to j things still work fine
>>> f2 <- function (x, j) (x - j)^2
>>> xmin2 <- optimize(f2, c(0, 1), tol = 0.0001, j = 1/3)
>>> xmin2
>> 
>> $minimum
>> [1] 0.3333333
>> 
>> $objective
>> [1] 0
>> 
>>> ## if we change argument a to i things fail
>>> f3 <- function (x, i) (x - i)^2
>>> xmin3 <- optimize(f3, c(0, 1), tol = 0.0001, i = 1/3)
>> 
>> Error in optimize(f3, c(0, 1), tol = 1e-04, i = 1/3) :
>>  'xmin' not less than 'xmax'
>>> xmin3
>> 
>> $minimum
>> [1] 0.3333333
>> 
>> $objective
>> [1] 0
>> 
>>> ##Same here
>>> xmin3 <- optimize(f3, lower=0, upper=1, tol = 0.0001, i = 1/3)
>> 
>> Error in f(arg, ...) (from #1) : argument "i" is missing, with no
>> default
>>> xmin3
>> 
>> $minimum
>> [1] 0.3333333
>> 
>> $objective
>> [1] 0
>> 
>>> ## a workaround is
>>> xmin3 <- optimize(f3, interval=c(0, 1), tol = 0.0001, i = 1/3)
>>> xmin3
>> 
>> $minimum
>> [1] 0.3333333
>> 
>> $objective
>> [1] 0
>> 
>> ---
>> 
>> the problem is, I guess, due to the keyword 'interval' gets mixed
>> up with 'i'.
>> 
>> Has anyone experienced that?
>> 
>> Best regards
>> S?ren
>> 
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> -- 
> Ege Rubak, Associate Professor,
> Department of Mathematical Sciences, Aalborg University
> Skjernvej 4A, 9220 Aalborg East, Denmark
> Phone: (+45)99408861
> Mobile: (+45)30230252
> Email: rubak at math.aau.dk
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu May  7 17:41:41 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 7 May 2020 16:41:41 +0100
Subject: [R] Change the colours of some of the labels of the x-axis
In-Reply-To: <8B435C9568170B469AE31E8891E8CC4F809B8064@ESINO.regionemarche.intra>
References: <8B435C9568170B469AE31E8891E8CC4F809B8064@ESINO.regionemarche.intra>
Message-ID: <dff646d6-dc28-5258-b300-275c82a9dfca@sapo.pt>

Hello,

You cannot pass a vector of colors to col.axis, you need to plot the 
axis twice, once the normal axis, like in your code, then overplot just 
those last 2 labels.


xlabels <- seq(trunc(as.POSIXct(first_day, format="%Y-%m-%d-%H-%M"), 
"days"), trunc(as.POSIXct(last_day, format="%Y-%m-%d-%H-%M"), "days"), 
by="12 hours")

plot(df_plot$data_POSIX, df_plot$hs1, type="b",
      xlim = c(min(xlabels), max(xlabels)), ylim=c(0, 10), col="blue", 
xaxt="n", axes=F, main="", xlab="", ylab="Snow height")
lines(df_plot$data_POSIX, df_plot$hs2, ylim=c(0, 10), col="red", type="b")
axis.POSIXct(1, at = xlabels, format="h%H-%d-%m-%y", pos=0, las = 2)
axis.POSIXct(1, at = tail(xlabels, 2),
              format="h%H-%d-%m-%y", pos=0, las = 2, col.axis = "red")

axis(side=2, at=seq(0, 15, 5))



Hope this helps,

Rui Barradas

?s 15:09 de 07/05/20, Stefano Sofia escreveu:
> Dear R users,
> in a plot I need to change the colours of some of the labels of the x-axis.
> In the example below reported, I would like to have the labels of the last and second-last date in red.
> I tried to find the solution searching the web, but I could not understand the hints and I was not sure I could adapt them to my example.
> There might be an easy way to do that?
> 
> Thank you for your attention and your help
> Stefano
> 
> first_day <- "2005-01-23-09-00"
> last_day <- "2005-01-27-09-00"
> first_day_POSIX <- as.POSIXct(first_day, format="%Y-%m-%d-%H-%M")
> last_day_POSIX <- as.POSIXct(last_day, format="%Y-%m-%d-%H-%M")
> df_plot <- data.frame(data_POSIX=seq(first_day_POSIX, last_day_POSIX, by="30 mins"))
> df_plot$hs1 <- 5
> df_plot$hs2 <- 7
> 
> plot(df_plot$data_POSIX, df_plot$hs1, type="b", ylim=c(0, 10), col="blue", xaxt="n", axes=F, main="", xlab="", ylab="Snow height")
> lines(df_plot$data_POSIX, df_plot$hs2, ylim=c(0, 10), col="red", type="b")
> axis.POSIXct(1, at=seq(trunc(as.POSIXct(first_day, format="%Y-%m-%d-%H-%M"), "days"), trunc(as.POSIXct(last_day, format="%Y-%m-%d-%H-%M"), "days"), by="12 hours"), format="h%H-%d-%m-%y", pos=0)
> axis(side=2, at=seq(0, 15, 5))
> 
> 
>           (oo)
> --oOO--( )--OOo----------------
> Stefano Sofia PhD
> Civil Protection - Marche Region
> Meteo Section
> Snow Section
> Via del Colle Ameno 5
> 60126 Torrette di Ancona, Ancona
> Uff: 071 806 7743
> E-mail: stefano.sofia at regione.marche.it
> ---Oo---------oO----------------
> 
> ________________________________
> 
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
> 
> --
> Questo messaggio  stato analizzato da Libra ESVA ed  risultato non infetto.
> This message was scanned by Libra ESVA and is believed to be clean.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Thu May  7 22:17:15 2020
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Thu, 7 May 2020 20:17:15 +0000 (UTC)
Subject: [R] Replication : How to get only 1 value
References: <526588415.1508527.1588882635098.ref@mail.yahoo.com>
Message-ID: <526588415.1508527.1588882635098@mail.yahoo.com>

Dear R-experts,

My goal is to get only 1 value : the average/ the mean of the 100 MSE values. How can I finish my R code ?

###################################################################
my.experiment <- function()? {

n<-500
x<-runif(n, 0, 5)
z <- rnorm(n, 2, 3)
a <- runif(n, 0, 5) 

y_model<- 0.1*x^3 - 0.5 * z^2 - a + 10
y_obs <- y_model +c( rnorm(n*0.97, 0, 0.1), rnorm(n*0.03, 0, 0.5) )
fit1<- lm(y_obs~x^3+z^2+a)

MSE<-mean((fit1$fitted.values - y_model)^2)
return( c(MSE) )
}

my.data = t(replicate( 100, my.experiment() ))
summary(my.data)
################################################################


From g||ted|||e2014 @end|ng |rom gm@||@com  Thu May  7 23:00:29 2020
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Thu, 7 May 2020 22:00:29 +0100
Subject: [R] Adding overlap legend to a histogram
Message-ID: <CAC8ss30Av_LoM4eYh9TsPhEnpUWG4nDxGduoDiREiuQDmpL+sQ@mail.gmail.com>

Dear Experts,
Greetings.

I am trying to display two datasets in a histogram. I have been able to
plot the graph and added the legend for the two colors. I am, however,
having difficulties adding a legend to represent the regions of overlap
(the third legend).  Below are my data and code.

Thank you very much for your kind response.
Best wishes
Ogbos


Part of the two data are:
96 11 28 -5.0439243156971
96 11 30 -6.47673663925309
96 12 03 -6.82839470197342
96 12 05 -6.21642465505491
96 12 07 -6.25580747537018
96 12 10 -5.77540853474434
96 12 14 -3.98857218877879
96 12 16 -4.25191042337454
96 12 20 -3.20551034549018
96 12 24 -4.2702754047348
96 12 28 -6.03085851418479
96 12 31 -6.51403505358144
97 01 02 -6.32673280540791
97 01 08 -5.8537576420137
97 01 13 -5.54092007919419
97 01 23 -4.03617528404112
97 01 28 -6.21414660666954
97 01 30 -5.82248535055029
97 02 02 -4.52703090771556
97 02 04 -4.24731404905759
97 02 06 -5.22553031464346
97 02 10 -3.31737825603324
97 02 13 -1.48147648915881
97 02 16 -1.80195032791485
97 02 19 -2.16701154625054
97 02 21 -2.06571846213066
97 02 24 -3.39623775344558
97 03 02 -4.70829707054833
97 03 07 -3.73377684116639
97 03 11 -2.76476446486583
97 03 19 -2.30766786606313
97 03 21 -2.36150976836853
97 03 24 -1.84664525535518
97 04 01 -4.30618772775492
97 04 12 -4.51646004530582
97 04 19 -4.12507873323636
97 04 23 -3.46413598467606
97 04 25 -2.95371560706153
97 05 02 -4.04170720343953
97 05 13 -5.60975985858423
97 05 15 -5.52471869367602
97 05 23 -3.85761006637094
97 05 26 -3.47054347969788
97 05 29 -3.46506967197854
97 06 04 -0.307928317047413
97 06 08 -2.87534017937205
97 06 11 -3.00325467983916
97 06 14 -3.15331224991201
97 06 26 -3.14375164434693
97 07 02 -1.28362211606

and the second is:
98 05 02 -4.09334391050803
98 06 07 -0.161822520097844
98 06 19 -0.274151702680992
98 06 21 -0.285384620939307
98 06 25 -0.554974659138863
98 08 23 -2.22867947962777
98 08 27 -5.44129410150581
98 09 25 -4.46403021303242
98 11 09 -0.723468433013585
98 12 14 -0.824564697338419
99 01 16 -0.880729288629993
99 01 19 -0.566207577397178
99 01 21 -0.566207577397178
99 01 24 -4.99197737117322
99 02 13 -1.15031932682955
99 02 18 -5.733349976222
99 02 23 -0.869496370371678
99 03 06 -0.319083375714252
99 03 19 -1.35251185547922
99 03 24 -1.33004601896259
99 04 11 -0.229220029647733
99 04 16 -0.521275904363918
99 04 22 -1.2626485094127
99 05 07 -0.453878394814029
99 05 10 -1.35251185547922
99 05 24 -1.97032235968653
99 05 30 -0.330316293972566
99 06 06 -1.24018267289607
99 06 23 -0.0157945827397513
99 06 27 -2.86895582035172
99 07 03 -1.04922306250472
99 07 08 -0.375247967005826
99 07 24 -0.139356683581214
99 07 28 -1.50977271109562
99 07 31 -0.465111313072344
99 08 20 -4.35170103044927
99 08 22 -4.86841527033175
99 08 25 -4.03717931921646
99 09 01 -2.2511453161444
99 09 05 -3.22840920461779
99 09 07 -2.98128500293486
99 09 09 -3.28457379590937
99 09 16 -4.08211099224972
99 09 18 -4.00348056444151
99 09 21 -4.17197433831624
99 09 25 -3.11608002203464
99 09 29 -3.85745262708342
99 10 03 -3.77882219927522

and my code is:
data <- read.table("AUTOFD2a", col.names = c("year", "month", "day","fd"))

new.century <- data$year < 50

data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)

data$date <- as.Date(ISOdate(data$year, data$month, data$day))
x1 = data$date

 library(date)

c1a<-rgb(1,0,0,8/9)
hist(x1,breaks="years",freq=T,axes=F,col=c1a)


data <- read.table("MANFD2a", col.names = c("year", "month", "day","fd"))

new.century <- data$year < 50

data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)

data$date <- as.Date(ISOdate(data$year, data$month, data$day))
x2 = data$date

c2a=rgb(0,0,1,8/9)
hist(x2,breaks="years",freq=T,axes=F,add=T,col=c2a)
 axis.Date(1, at=seq(as.Date(min(x1)), as.Date(max(x1)), by="years"))
 axis(2)
legend("topright", c("AUTO", "MANUAL"), col=c("red", "blue"), lwd=10)

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Thu May  7 23:24:29 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 8 May 2020 07:24:29 +1000
Subject: [R] 
 [R ] Writing loop to estimate ARCH test for a multiple columns
 of a data frame?
In-Reply-To: <CAOFE=kMaSgqy7_dMsB3WtTgN+BDa4b5m28GO0uxm0rUsDmqc9g@mail.gmail.com>
References: <CAOFE=kMaSgqy7_dMsB3WtTgN+BDa4b5m28GO0uxm0rUsDmqc9g@mail.gmail.com>
Message-ID: <CA+8X3fWOMFQ8Ptz75j8DjhJuaEKi8nR1p+82LeNqTcB2NfSjKA@mail.gmail.com>

Hi Subhamitra,
For some reason, your data didn't make it through. Maybe you tried to
send an .xls or .xlsx file. If so, export it as CSV or if it's not too
big, just paste the text into your email.

Jim

On Thu, May 7, 2020 at 10:30 PM Subhamitra Patra
<subhamitra.patra at gmail.com> wrote:
>
> Dear R-users,
>
> I want to estimate ARCH test for multiple columns (i.e.,  from 2:21 COLUMNS
> ) in my data. For this purpose, I want to run a loop to calculate ARCH test
> results for each column in the data frame. I tried by using for loop and
> lapply function, but unable to write a loop for computing the ARCH test
> simultaneously for each column (i.e., from 2:21 columns) of my data frame.
>
> Below is my ARCH test code which I want to estimate for multiple columns of
> the data frame in a loop.
>
> library(tseries)
>
> library(FinTS)
>
> ArchTest (A, lags=1, demean = FALSE)
>
> Hence, A is a vector for which the ARCH test result is calculated. Here, I
> want to write a loop so that the ArchTest can be calculated simultaneously
> for each column of my data frame. From ARCH test result, I require only the
> calculated Chi-square value and its p-value for each column that stored in
> another matrix or object for each column as an output file.
>
> For your convenience, I attached my sample data below. Please find it.
>
> Please help me for which I shall be always grateful to you.
>
> Thank you.
>
> --
> *Best Regards,*
> *Subhamitra Patra*
> *Phd. Research Scholar*
> *Department of Humanities and Social Sciences*
> *Indian Institute of Technology, Kharagpur*
> *INDIA*
>
> [image: Mailtrack]
> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
> Sender
> notified by
> Mailtrack
> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
> 05/07/20,
> 05:51:03 PM
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu May  7 23:27:10 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 7 May 2020 22:27:10 +0100
Subject: [R] Replication : How to get only 1 value
In-Reply-To: <526588415.1508527.1588882635098@mail.yahoo.com>
References: <526588415.1508527.1588882635098.ref@mail.yahoo.com>
 <526588415.1508527.1588882635098@mail.yahoo.com>
Message-ID: <b3f025c0-ba7d-9b61-045d-8e461ccc7c56@sapo.pt>

Hello,

That's just

mean(my.data)

Note that

summary(t(my.data))

also gives the mean. (You need to transpose because the way you ran 
replicate outputs a 1x100 matrix.)


Hope this helps,

Rui Barradas


?s 21:17 de 07/05/20, varin sacha via R-help escreveu:
> Dear R-experts,
> 
> My goal is to get only 1 value : the average/ the mean of the 100 MSE values. How can I finish my R code ?
> 
> ###################################################################
> my.experiment <- function()? {
> 
> n<-500
> x<-runif(n, 0, 5)
> z <- rnorm(n, 2, 3)
> a <- runif(n, 0, 5)
> 
> y_model<- 0.1*x^3 - 0.5 * z^2 - a + 10
> y_obs <- y_model +c( rnorm(n*0.97, 0, 0.1), rnorm(n*0.03, 0, 0.5) )
> fit1<- lm(y_obs~x^3+z^2+a)
> 
> MSE<-mean((fit1$fitted.values - y_model)^2)
> return( c(MSE) )
> }
> 
> my.data = t(replicate( 100, my.experiment() ))
> summary(my.data)
> ################################################################
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Thu May  7 23:41:38 2020
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard McGarvey)
Date: Thu, 7 May 2020 17:41:38 -0400 (EDT)
Subject: [R] unstable results of nlxb fit
In-Reply-To: <16e4ad6a-12dd-ccfe-0b54-403be54d8b6e@gmail.com>
References: <745db2c879ba43149ff7e1f009219cca@SRVEXCHCM1302.precheza.cz>
 <16e4ad6a-12dd-ccfe-0b54-403be54d8b6e@gmail.com>
Message-ID: <755635422.1450418.1588887698767@connect.xfinity.com>

John/Petr, I think there is an issue between a global optimum and local optima. I added a multistart loop around the code to see if I could find different solutions. Here is the code I added (I am not a great coder so please excuse any inefficiencies in this code segment):

# Multistart approach
NT <- 100
Results <- matrix(data=NA, nrow = NT, ncol=5, dimnames=list(NULL,c("SS", "A", "B", "a", "b")))
A1 <- runif(NT,0,100)
B1 <- runif(NT,0,100)
a1 <- runif(NT,0.0,0.1)
b1 <- runif(NT,0.0,0.1)
for (I in 1:NT) {
  if (A1[I] > B1[I]) { # Ensure that the A'a are always the lower so that nlxb() always converge to the same values
    A0 <- A1[I]
    a0 <- a1[I]
    A1[I] <- B1[I]
    a1[I] <- b1[I]
    B1[I] <- A0
    b1[I] <- a0
  }
  fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
              start=list(A=A1[I], B=B1[I], a=a1[I], b=b1[I]))
  ccc <- coef(fit)
  Results[I,1] <- fit$ssquares
  Results[I,2] <- ccc[1]
  Results[I,3] <- ccc[2]
  Results[I,4] <- ccc[3]
  Results[I,5] <- ccc[4]
}
Results

What I found is that the minimum SS generated at each trial had two distinct values, 417.8 and 3359.2. The A,B,a, and b values when the SS was 417.8 were all the same but I got different values for the case where the minimal SS was 3359.2. This indicates that the SS=417.8 may be the global minimum solution whereas the others are local optima. Here are the iteration results for a 100 trial multistart:

Results
           SS           A           B           a           b
  [1,] 3359.2  8.3546e+03  6.8321e+00   -1.988226  2.6139e-02
  [2,] 3359.2  8.2865e+03  6.8321e+00   -5.201735  2.6139e-02
  [3,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
  [4,] 3359.2  6.8321e+00  7.7888e+02    0.026139 -7.2812e-01
  [5,] 3359.2 -3.9020e+01  4.5852e+01    0.026139  2.6139e-02
  [6,] 3359.2  6.8321e+00  2.6310e+02    0.026139 -1.8116e+00
  [7,] 3359.2 -2.1509e+01  2.8341e+01    0.026139  2.6139e-02
  [8,] 3359.2 -3.8075e+01  4.4908e+01    0.026139  2.6139e-02
  [9,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [10,] 3359.2  1.2466e+04  6.8321e+00   -4.196000  2.6139e-02
 [11,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
 [12,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [13,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [14,] 3359.2  3.8018e+02  6.8321e+00   -0.806414  2.6139e-02
 [15,] 3359.2 -3.1921e+00  1.0024e+01    0.026139  2.6139e-02
 [16,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [17,] 3359.2 -1.5938e+01  2.2770e+01    0.026139  2.6139e-02
 [18,] 3359.2 -3.1205e+01  3.8037e+01    0.026139  2.6139e-02
 [19,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [20,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [21,] 3359.2  8.6627e+03  6.8321e+00   -3.319778  2.6139e-02
 [22,] 3359.2  6.8321e+00  1.9318e+01    0.026139 -6.5773e-01
 [23,] 3359.2  6.2991e+01 -5.6159e+01    0.026139  2.6139e-02
 [24,] 3359.2  2.8865e-03  6.8321e+00   -1.576307  2.6139e-02
 [25,] 3359.2 -1.2496e+01  1.9328e+01    0.026139  2.6139e-02
 [26,] 3359.2 -5.9432e+00  1.2775e+01    0.026139  2.6139e-02
 [27,] 3359.2  1.6884e+02  6.8321e+00 -211.866423  2.6139e-02
 [28,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [29,] 3359.2  5.4972e+03  6.8321e+00   -3.432094  2.6139e-02
 [30,] 3359.2  6.8321e+00  1.4427e+03    0.026139 -4.2771e+02
 [31,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
 [32,] 3359.2  3.5760e+01 -2.8928e+01    0.026139  2.6139e-02
 [33,] 3359.2  6.8321e+00 -4.0737e+02    0.026139 -6.7152e-01
 [34,] 3359.2  6.8321e+00  1.2638e+04    0.026139 -2.8070e+00
 [35,] 3359.2  1.1813e+01 -4.9807e+00    0.026139  2.6139e-02
 [36,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [37,] 3359.2  6.8321e+00  1.2281e+03    0.026139 -3.0702e+02
 [38,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [39,] 3359.2 -2.6850e+01  3.3682e+01    0.026139  2.6139e-02
 [40,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [41,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
 [42,] 3359.2 -2.3279e+01  3.0111e+01    0.026139  2.6139e-02
 [43,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [44,] 3359.2  6.8321e+00  1.4550e+03    0.026139 -4.0303e+00
 [45,] 3359.2 -1.1386e+01  1.8218e+01    0.026139  2.6139e-02
 [46,] 3359.2  8.8026e+02  6.8321e+00  -65.430608  2.6139e-02
 [47,] 3359.2 -8.1985e+00  1.5031e+01    0.026139  2.6139e-02
 [48,] 3359.2 -6.7767e+00  1.3609e+01    0.026139  2.6139e-02
 [49,] 3359.2 -1.1436e+01  1.8268e+01    0.026139  2.6139e-02
 [50,] 3359.2  1.0710e+04  6.8321e+00   -2.349659  2.6139e-02
 [51,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
 [52,] 3359.2  6.8321e+00  7.1837e+02    0.026139 -7.4681e-01
 [53,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [54,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
 [55,] 3359.2 -4.8774e+00  6.8321e+00  -16.405584  2.6139e-02
 [56,] 3359.2  1.2687e+03  6.8321e+00   -3.775998  2.6139e-02
 [57,] 3359.2  1.5529e+01 -8.6967e+00    0.026139  2.6139e-02
 [58,] 3359.2 -1.0003e+01  1.6835e+01    0.026139  2.6139e-02
 [59,] 3359.2  6.8321e+00  3.9291e+02    0.026139 -4.1974e+02
 [60,] 3359.2 -2.1880e+01  2.8712e+01    0.026139  2.6139e-02
 [61,] 3359.2  4.1736e+03  6.8321e+00  -10.711457  2.6139e-02
 [62,] 3359.2 -3.3185e+01  4.0017e+01    0.026139  2.6139e-02
 [63,] 3359.2  7.6732e+02  6.8321e+00   -0.723977  2.6139e-02
 [64,] 3359.2  1.5334e+04  6.8321e+00  -52.573620  2.6139e-02
 [65,] 3359.2 -2.9556e+01  3.6388e+01    0.026139  2.6139e-02
 [66,] 3359.2 -1.0447e+00  7.8767e+00    0.026139  2.6139e-02
 [67,] 3359.2  6.8321e+00  2.1471e+02    0.026139 -7.0582e+01
 [68,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
 [69,] 3359.2 -2.2293e+01  2.9126e+01    0.026139  2.6139e-02
 [70,] 3359.2  6.2259e+02  6.8321e+00   -2.782527  2.6139e-02
 [71,] 3359.2 -1.4639e+01  2.1471e+01    0.026139  2.6139e-02
 [72,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [73,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [74,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [75,] 3359.2 -2.3449e+01  3.0281e+01    0.026139  2.6139e-02
 [76,] 3359.2 -2.5926e+01  6.8321e+00   -0.663656  2.6139e-02
 [77,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
 [78,] 3359.2  6.8321e+00  6.9426e+02    0.026139 -1.9442e+00
 [79,] 3359.2  2.8684e+02  6.8321e+00   -0.854394  2.6139e-02
 [80,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [81,] 3359.2 -4.5066e+01  5.1899e+01    0.026139  2.6139e-02
 [82,] 3359.2  4.4678e+03  6.8321e+00   -2.109446  2.6139e-02
 [83,] 3359.2  3.1376e+03  6.8321e+00   -1.104803  2.6139e-02
 [84,] 3359.2  6.8321e+00  1.1167e+02    0.026139 -1.0280e+00
 [85,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [86,] 3359.2  5.3864e+02  6.8321e+00   -0.657971  2.6139e-02
 [87,] 3359.2  4.8227e+01  6.8321e+00   -2.304024  2.6139e-02
 [88,] 3359.2 -2.2048e+01  2.8880e+01    0.026139  2.6139e-02
 [89,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [90,] 3359.2  6.8321e+00 -4.1689e+01    0.026139 -3.6049e+00
 [91,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
 [92,] 3359.2 -4.1265e+01  4.8097e+01    0.026139  2.6139e-02
 [93,] 3359.2 -1.1565e+01  1.8397e+01    0.026139  2.6139e-02
 [94,] 3359.2  2.3698e+01 -1.6866e+01    0.026139  2.6139e-02
 [95,] 3359.2  4.4700e+03  6.8321e+00  -12.836180  2.6139e-02
 [96,] 3359.2  4.6052e+04  6.8321e+00   -7.158584  2.6139e-02
 [97,] 3359.2  2.5464e+03  6.8321e+00   -1.811626  2.6139e-02
 [98,] 3359.2  6.8321e+00  1.0338e+03    0.026139 -1.5365e+01
 [99,] 3359.2  1.3783e+01 -6.9507e+00    0.026139  2.6139e-02
[100,] 3359.2  6.8321e+00  6.7153e+02    0.026139 -1.5975e+03


Hope this helps,

Bernard McGarvey


Director, Fort Myers Beach Lions Foundation, Inc.


Retired (Lilly Engineering Fellow).

> On May 7, 2020 at 9:33 AM J C Nash <profjcnash at gmail.com> wrote:
> 
> 
> The double exponential is well-known as a disaster to fit. Lanczos in his
> 1956 book Applied Analysis, p. 276 gives a good example which is worked through.
> I've included it with scripts using nlxb in my 2014 book on Nonlinear Parameter
> Optimization Using R Tools (Wiley). The scripts were on Wiley's site for the book,
> but I've had difficulty getting Wiley to fix things and not checked lately if it
> is still accessible. Ask off-list if you want the script and I'll dig into my
> archives.
> 
> nlxb (preferably from nlsr which you used rather than nlmrt which is now not
> maintained), will likely do as well as any general purpose code. There may be
> special approaches that do a bit better, but I suspect the reality is that
> the underlying problem is such that there are many sets of parameters with
> widely different values that will get quite similar sums of squares.
> 
> Best, JN
> 
> 
> On 2020-05-07 9:12 a.m., PIKAL Petr wrote:
> > Dear all
> > 
> > I started to use nlxb instead of nls to get rid of singular gradient error.
> > I try to fit double exponential function to my data, but results I obtain
> > are strongly dependent on starting values. 
> > 
> > tsmes ~ A*exp(a*plast) + B* exp(b*plast)
> > 
> > Changing b from 0.1 to 0.01 gives me completely different results. I usually
> > check result by a plot but could the result be inspected if it achieved good
> > result without plotting?
> > 
> > Or is there any way how to perform such task?
> > 
> > Cheers
> > Petr
> > 
> > Below is working example.
> > 
> >> dput(temp)
> > temp <- structure(list(tsmes = c(31, 32, 32, 32, 32, 32, 32, 32, 33, 
> > 34, 35, 35, 36, 36, 36, 37, 38, 39, 40, 40, 40, 40, 40, 41, 43, 
> > 44, 44, 44, 46, 47, 47, 47, 47, 48, 49, 51, 51, 51, 52, 53, 54, 
> > 54, 55, 57, 57, 57, 59, 59, 60, 62, 63, 64, 65, 66, 66, 67, 67, 
> > 68, 70, 72, 74, 76, 78, 81, 84, 85, 86, 88, 90, 91, 92, 94, 96, 
> > 97, 99, 100, 102, 104, 106, 109, 112, 115, 119, 123, 127, 133, 
> > 141, 153, 163, 171), plast = c(50, 51, 52, 52, 53, 53, 53, 54, 
> > 55, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 64, 64, 65, 65, 66, 
> > 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 75, 76, 76, 77, 77, 78, 
> > 78, 79, 80, 81, 82, 83, 84, 85, 85, 86, 86, 87, 88, 88, 89, 90, 
> > 91, 91, 93, 93, 94, 95, 96, 96, 97, 98, 98, 99, 100, 100, 101, 
> > 102, 103, 103, 104, 105, 106, 107, 107, 108, 109, 110, 111, 112, 
> > 112, 113, 113, 114, 115, 116)), row.names = 2411:2500, class = "data.frame")
> > 
> > library(nlsr)
> > 
> > fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
> > start=list(A=1, B=15, a=0.025, b=0.01))
> > coef(fit)
> >            A            B            a            b 
> > 3.945167e-13 9.772749e+00 2.802274e-01 2.179781e-02 
> > 
> > plot(temp$plast, temp$tsmes, ylim=c(0,200))
> > lines(temp$plast, predict(fit, newdata=temp), col="pink", lwd=3)
> > ccc <- coef(fit)
> > lines(0:120,ccc[1]*exp(ccc[3]*(0:120)))
> > lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2)
> > 
> > # wrong fit with slightly different b
> > fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
> > start=list(A=1, B=15, a=0.025, b=0.1))
> > coef(fit)
> >            A            B            a            b 
> > 2911.6448377    6.8320597  -49.1373979    0.0261391 
> > lines(temp$plast, predict(fit, newdata=temp), col="red", lwd=3)
> > ccc <- coef(fit)
> > lines(0:120,ccc[1]*exp(ccc[3]*(0:120)), col="red")
> > lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2, col="red")
> > 
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pro|jcn@@h @end|ng |rom gm@||@com  Fri May  8 00:00:11 2020
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Thu, 7 May 2020 18:00:11 -0400
Subject: [R] unstable results of nlxb fit
In-Reply-To: <755635422.1450418.1588887698767@connect.xfinity.com>
References: <745db2c879ba43149ff7e1f009219cca@SRVEXCHCM1302.precheza.cz>
 <16e4ad6a-12dd-ccfe-0b54-403be54d8b6e@gmail.com>
 <755635422.1450418.1588887698767@connect.xfinity.com>
Message-ID: <8cfa9396-7a22-ba80-b8be-48dd935fe408@gmail.com>

These results reflect my experience with this sort of problem.

A couple of comments:

1) optimx package has a multistart wrapper. I probably should have written one for
nlsr. Maybe Bernard and I should work on that. The issues are largely to make things
resistant to silly inputs, which even the smart users (you know, the ones looking
back from the mirror) introduce.

2) Sometimes using the bounds constraint capability in nlsr can be helpful, e.g.,
to ensure the exponent parameters are kept apart, can be useful.

3) Combining with Duncan's suggestion of solving for the linear parameters also
helps.

All of the above can be sensitive to particular data.

Best, JN

On 2020-05-07 5:41 p.m., Bernard McGarvey wrote:
> John/Petr, I think there is an issue between a global optimum and local optima. I added a multistart loop around the code to see if I could find different solutions. Here is the code I added (I am not a great coder so please excuse any inefficiencies in this code segment):
> 
> # Multistart approach
> NT <- 100
> Results <- matrix(data=NA, nrow = NT, ncol=5, dimnames=list(NULL,c("SS", "A", "B", "a", "b")))
> A1 <- runif(NT,0,100)
> B1 <- runif(NT,0,100)
> a1 <- runif(NT,0.0,0.1)
> b1 <- runif(NT,0.0,0.1)
> for (I in 1:NT) {
>   if (A1[I] > B1[I]) { # Ensure that the A'a are always the lower so that nlxb() always converge to the same values
>     A0 <- A1[I]
>     a0 <- a1[I]
>     A1[I] <- B1[I]
>     a1[I] <- b1[I]
>     B1[I] <- A0
>     b1[I] <- a0
>   }
>   fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
>               start=list(A=A1[I], B=B1[I], a=a1[I], b=b1[I]))
>   ccc <- coef(fit)
>   Results[I,1] <- fit$ssquares
>   Results[I,2] <- ccc[1]
>   Results[I,3] <- ccc[2]
>   Results[I,4] <- ccc[3]
>   Results[I,5] <- ccc[4]
> }
> Results
> 
> What I found is that the minimum SS generated at each trial had two distinct values, 417.8 and 3359.2. The A,B,a, and b values when the SS was 417.8 were all the same but I got different values for the case where the minimal SS was 3359.2. This indicates that the SS=417.8 may be the global minimum solution whereas the others are local optima. Here are the iteration results for a 100 trial multistart:
> 
> Results
>            SS           A           B           a           b
>   [1,] 3359.2  8.3546e+03  6.8321e+00   -1.988226  2.6139e-02
>   [2,] 3359.2  8.2865e+03  6.8321e+00   -5.201735  2.6139e-02
>   [3,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>   [4,] 3359.2  6.8321e+00  7.7888e+02    0.026139 -7.2812e-01
>   [5,] 3359.2 -3.9020e+01  4.5852e+01    0.026139  2.6139e-02
>   [6,] 3359.2  6.8321e+00  2.6310e+02    0.026139 -1.8116e+00
>   [7,] 3359.2 -2.1509e+01  2.8341e+01    0.026139  2.6139e-02
>   [8,] 3359.2 -3.8075e+01  4.4908e+01    0.026139  2.6139e-02
>   [9,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [10,] 3359.2  1.2466e+04  6.8321e+00   -4.196000  2.6139e-02
>  [11,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
>  [12,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [13,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [14,] 3359.2  3.8018e+02  6.8321e+00   -0.806414  2.6139e-02
>  [15,] 3359.2 -3.1921e+00  1.0024e+01    0.026139  2.6139e-02
>  [16,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [17,] 3359.2 -1.5938e+01  2.2770e+01    0.026139  2.6139e-02
>  [18,] 3359.2 -3.1205e+01  3.8037e+01    0.026139  2.6139e-02
>  [19,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [20,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [21,] 3359.2  8.6627e+03  6.8321e+00   -3.319778  2.6139e-02
>  [22,] 3359.2  6.8321e+00  1.9318e+01    0.026139 -6.5773e-01
>  [23,] 3359.2  6.2991e+01 -5.6159e+01    0.026139  2.6139e-02
>  [24,] 3359.2  2.8865e-03  6.8321e+00   -1.576307  2.6139e-02
>  [25,] 3359.2 -1.2496e+01  1.9328e+01    0.026139  2.6139e-02
>  [26,] 3359.2 -5.9432e+00  1.2775e+01    0.026139  2.6139e-02
>  [27,] 3359.2  1.6884e+02  6.8321e+00 -211.866423  2.6139e-02
>  [28,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [29,] 3359.2  5.4972e+03  6.8321e+00   -3.432094  2.6139e-02
>  [30,] 3359.2  6.8321e+00  1.4427e+03    0.026139 -4.2771e+02
>  [31,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
>  [32,] 3359.2  3.5760e+01 -2.8928e+01    0.026139  2.6139e-02
>  [33,] 3359.2  6.8321e+00 -4.0737e+02    0.026139 -6.7152e-01
>  [34,] 3359.2  6.8321e+00  1.2638e+04    0.026139 -2.8070e+00
>  [35,] 3359.2  1.1813e+01 -4.9807e+00    0.026139  2.6139e-02
>  [36,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [37,] 3359.2  6.8321e+00  1.2281e+03    0.026139 -3.0702e+02
>  [38,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [39,] 3359.2 -2.6850e+01  3.3682e+01    0.026139  2.6139e-02
>  [40,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [41,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
>  [42,] 3359.2 -2.3279e+01  3.0111e+01    0.026139  2.6139e-02
>  [43,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [44,] 3359.2  6.8321e+00  1.4550e+03    0.026139 -4.0303e+00
>  [45,] 3359.2 -1.1386e+01  1.8218e+01    0.026139  2.6139e-02
>  [46,] 3359.2  8.8026e+02  6.8321e+00  -65.430608  2.6139e-02
>  [47,] 3359.2 -8.1985e+00  1.5031e+01    0.026139  2.6139e-02
>  [48,] 3359.2 -6.7767e+00  1.3609e+01    0.026139  2.6139e-02
>  [49,] 3359.2 -1.1436e+01  1.8268e+01    0.026139  2.6139e-02
>  [50,] 3359.2  1.0710e+04  6.8321e+00   -2.349659  2.6139e-02
>  [51,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
>  [52,] 3359.2  6.8321e+00  7.1837e+02    0.026139 -7.4681e-01
>  [53,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [54,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
>  [55,] 3359.2 -4.8774e+00  6.8321e+00  -16.405584  2.6139e-02
>  [56,] 3359.2  1.2687e+03  6.8321e+00   -3.775998  2.6139e-02
>  [57,] 3359.2  1.5529e+01 -8.6967e+00    0.026139  2.6139e-02
>  [58,] 3359.2 -1.0003e+01  1.6835e+01    0.026139  2.6139e-02
>  [59,] 3359.2  6.8321e+00  3.9291e+02    0.026139 -4.1974e+02
>  [60,] 3359.2 -2.1880e+01  2.8712e+01    0.026139  2.6139e-02
>  [61,] 3359.2  4.1736e+03  6.8321e+00  -10.711457  2.6139e-02
>  [62,] 3359.2 -3.3185e+01  4.0017e+01    0.026139  2.6139e-02
>  [63,] 3359.2  7.6732e+02  6.8321e+00   -0.723977  2.6139e-02
>  [64,] 3359.2  1.5334e+04  6.8321e+00  -52.573620  2.6139e-02
>  [65,] 3359.2 -2.9556e+01  3.6388e+01    0.026139  2.6139e-02
>  [66,] 3359.2 -1.0447e+00  7.8767e+00    0.026139  2.6139e-02
>  [67,] 3359.2  6.8321e+00  2.1471e+02    0.026139 -7.0582e+01
>  [68,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
>  [69,] 3359.2 -2.2293e+01  2.9126e+01    0.026139  2.6139e-02
>  [70,] 3359.2  6.2259e+02  6.8321e+00   -2.782527  2.6139e-02
>  [71,] 3359.2 -1.4639e+01  2.1471e+01    0.026139  2.6139e-02
>  [72,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [73,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [74,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [75,] 3359.2 -2.3449e+01  3.0281e+01    0.026139  2.6139e-02
>  [76,] 3359.2 -2.5926e+01  6.8321e+00   -0.663656  2.6139e-02
>  [77,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
>  [78,] 3359.2  6.8321e+00  6.9426e+02    0.026139 -1.9442e+00
>  [79,] 3359.2  2.8684e+02  6.8321e+00   -0.854394  2.6139e-02
>  [80,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [81,] 3359.2 -4.5066e+01  5.1899e+01    0.026139  2.6139e-02
>  [82,] 3359.2  4.4678e+03  6.8321e+00   -2.109446  2.6139e-02
>  [83,] 3359.2  3.1376e+03  6.8321e+00   -1.104803  2.6139e-02
>  [84,] 3359.2  6.8321e+00  1.1167e+02    0.026139 -1.0280e+00
>  [85,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [86,] 3359.2  5.3864e+02  6.8321e+00   -0.657971  2.6139e-02
>  [87,] 3359.2  4.8227e+01  6.8321e+00   -2.304024  2.6139e-02
>  [88,] 3359.2 -2.2048e+01  2.8880e+01    0.026139  2.6139e-02
>  [89,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [90,] 3359.2  6.8321e+00 -4.1689e+01    0.026139 -3.6049e+00
>  [91,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
>  [92,] 3359.2 -4.1265e+01  4.8097e+01    0.026139  2.6139e-02
>  [93,] 3359.2 -1.1565e+01  1.8397e+01    0.026139  2.6139e-02
>  [94,] 3359.2  2.3698e+01 -1.6866e+01    0.026139  2.6139e-02
>  [95,] 3359.2  4.4700e+03  6.8321e+00  -12.836180  2.6139e-02
>  [96,] 3359.2  4.6052e+04  6.8321e+00   -7.158584  2.6139e-02
>  [97,] 3359.2  2.5464e+03  6.8321e+00   -1.811626  2.6139e-02
>  [98,] 3359.2  6.8321e+00  1.0338e+03    0.026139 -1.5365e+01
>  [99,] 3359.2  1.3783e+01 -6.9507e+00    0.026139  2.6139e-02
> [100,] 3359.2  6.8321e+00  6.7153e+02    0.026139 -1.5975e+03
> 
> 
> Hope this helps,
> 
> Bernard McGarvey
> 
> 
> Director, Fort Myers Beach Lions Foundation, Inc.
> 
> 
> Retired (Lilly Engineering Fellow).
> 
>> On May 7, 2020 at 9:33 AM J C Nash <profjcnash at gmail.com> wrote:
>>
>>
>> The double exponential is well-known as a disaster to fit. Lanczos in his
>> 1956 book Applied Analysis, p. 276 gives a good example which is worked through.
>> I've included it with scripts using nlxb in my 2014 book on Nonlinear Parameter
>> Optimization Using R Tools (Wiley). The scripts were on Wiley's site for the book,
>> but I've had difficulty getting Wiley to fix things and not checked lately if it
>> is still accessible. Ask off-list if you want the script and I'll dig into my
>> archives.
>>
>> nlxb (preferably from nlsr which you used rather than nlmrt which is now not
>> maintained), will likely do as well as any general purpose code. There may be
>> special approaches that do a bit better, but I suspect the reality is that
>> the underlying problem is such that there are many sets of parameters with
>> widely different values that will get quite similar sums of squares.
>>
>> Best, JN
>>
>>
>> On 2020-05-07 9:12 a.m., PIKAL Petr wrote:
>>> Dear all
>>>
>>> I started to use nlxb instead of nls to get rid of singular gradient error.
>>> I try to fit double exponential function to my data, but results I obtain
>>> are strongly dependent on starting values. 
>>>
>>> tsmes ~ A*exp(a*plast) + B* exp(b*plast)
>>>
>>> Changing b from 0.1 to 0.01 gives me completely different results. I usually
>>> check result by a plot but could the result be inspected if it achieved good
>>> result without plotting?
>>>
>>> Or is there any way how to perform such task?
>>>
>>> Cheers
>>> Petr
>>>
>>> Below is working example.
>>>
>>>> dput(temp)
>>> temp <- structure(list(tsmes = c(31, 32, 32, 32, 32, 32, 32, 32, 33, 
>>> 34, 35, 35, 36, 36, 36, 37, 38, 39, 40, 40, 40, 40, 40, 41, 43, 
>>> 44, 44, 44, 46, 47, 47, 47, 47, 48, 49, 51, 51, 51, 52, 53, 54, 
>>> 54, 55, 57, 57, 57, 59, 59, 60, 62, 63, 64, 65, 66, 66, 67, 67, 
>>> 68, 70, 72, 74, 76, 78, 81, 84, 85, 86, 88, 90, 91, 92, 94, 96, 
>>> 97, 99, 100, 102, 104, 106, 109, 112, 115, 119, 123, 127, 133, 
>>> 141, 153, 163, 171), plast = c(50, 51, 52, 52, 53, 53, 53, 54, 
>>> 55, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 64, 64, 65, 65, 66, 
>>> 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 75, 76, 76, 77, 77, 78, 
>>> 78, 79, 80, 81, 82, 83, 84, 85, 85, 86, 86, 87, 88, 88, 89, 90, 
>>> 91, 91, 93, 93, 94, 95, 96, 96, 97, 98, 98, 99, 100, 100, 101, 
>>> 102, 103, 103, 104, 105, 106, 107, 107, 108, 109, 110, 111, 112, 
>>> 112, 113, 113, 114, 115, 116)), row.names = 2411:2500, class = "data.frame")
>>>
>>> library(nlsr)
>>>
>>> fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
>>> start=list(A=1, B=15, a=0.025, b=0.01))
>>> coef(fit)
>>>            A            B            a            b 
>>> 3.945167e-13 9.772749e+00 2.802274e-01 2.179781e-02 
>>>
>>> plot(temp$plast, temp$tsmes, ylim=c(0,200))
>>> lines(temp$plast, predict(fit, newdata=temp), col="pink", lwd=3)
>>> ccc <- coef(fit)
>>> lines(0:120,ccc[1]*exp(ccc[3]*(0:120)))
>>> lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2)
>>>
>>> # wrong fit with slightly different b
>>> fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
>>> start=list(A=1, B=15, a=0.025, b=0.1))
>>> coef(fit)
>>>            A            B            a            b 
>>> 2911.6448377    6.8320597  -49.1373979    0.0261391 
>>> lines(temp$plast, predict(fit, newdata=temp), col="red", lwd=3)
>>> ccc <- coef(fit)
>>> lines(0:120,ccc[1]*exp(ccc[3]*(0:120)), col="red")
>>> lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2, col="red")
>>>
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From drj|m|emon @end|ng |rom gm@||@com  Fri May  8 02:12:56 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 8 May 2020 10:12:56 +1000
Subject: [R] Adding overlap legend to a histogram
In-Reply-To: <CAC8ss30Av_LoM4eYh9TsPhEnpUWG4nDxGduoDiREiuQDmpL+sQ@mail.gmail.com>
References: <CAC8ss30Av_LoM4eYh9TsPhEnpUWG4nDxGduoDiREiuQDmpL+sQ@mail.gmail.com>
Message-ID: <CA+8X3fUZOcmHjJ=E5yLG5j+266US+vFgaYJS1CPR0ofJDZ8fpw@mail.gmail.com>

Hi Ogbos,
I don't think that your example allows us to work out what you are
trying to do. For one thing, "x1" and "x2" do not overlap. Also, you
are plotting the frequencies of dates of observations, which may not
be what you want.
The following code will correctly display your example:

hist(x1,breaks="years",freq=T,axes=F,xlim=c(9400,11100),col=c1a)
hist(x2,breaks="years",freq=T,axes=F,add=T,col=c2a)
 axis.Date(1, at=seq(as.Date(min(x1)), as.Date(max(x2)), by="years"))
 axis(2)
legend("topleft", c("AUTO", "MANUAL"), fill=c("red", "blue"))

What it is displaying is the frequency of observations in your two
vectors by calendar year. If this is what you want, and you can
explain how you would like "overlap" to be displayed, we can probably
provide better help.

Jim


On Fri, May 8, 2020 at 7:01 AM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>
> Dear Experts,
> Greetings.
>
> I am trying to display two datasets in a histogram. I have been able to
> plot the graph and added the legend for the two colors. I am, however,
> having difficulties adding a legend to represent the regions of overlap
> (the third legend).  Below are my data and code.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Fri May  8 04:17:56 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Thu, 7 May 2020 21:17:56 -0500
Subject: [R] Error: Cannot use `+.gg()` with a single argument.
Message-ID: <CAF9-5jPRu09gA=oNHZvhUeyoY2pY=pdr55=Md_tB7FFatkiHiw@mail.gmail.com>

Hello,

I got this error:
Error: Cannot use `+.gg()` with a single argument. Did you
accidentally put + on a new line?

After running this:
data(murders)
library(ggplot2)
library(dplyr)
library(ggplot2)
ggplot(data=murders)

#define the slope of the line
r<-murders %>% summarize(rate=sum(total)/sum(population)*10^6) %>%.$rate
#mamke the plot
murders %>% ggplot(aes(population/10^6,total,label=abb))+
  +geom_abline(intercept = log10(r),lty=2,color="darkgrey")+
  +geom_point(aes(col=region), size=3)+
  +geom_text_repel()+
  +scale_x_log10()+
  +scale_y_log10()+
  +xlab("Populations in millions (log scale)")+
  +ylab("Total number of murders (log scale)")+
  +ggtitle("US Gun Murders in US 2010")+
  +scale_color_discrete(name="Region")+
  +theme_economist()

Is this an issue with my dplyr? Or how I can fix this code in order to work?

Thanks
Ana


From rmh @end|ng |rom temp|e@edu  Fri May  8 05:27:07 2020
From: rmh @end|ng |rom temp|e@edu (Richard M. Heiberger)
Date: Thu, 7 May 2020 23:27:07 -0400
Subject: [R] Error: Cannot use `+.gg()` with a single argument.
In-Reply-To: <CAF9-5jPRu09gA=oNHZvhUeyoY2pY=pdr55=Md_tB7FFatkiHiw@mail.gmail.com>
References: <CAF9-5jPRu09gA=oNHZvhUeyoY2pY=pdr55=Md_tB7FFatkiHiw@mail.gmail.com>
Message-ID: <CAGx1TMCJZxi2Veswfozu3PC5PmwgNCeM07k2HqCk+kiLQ9-T+A@mail.gmail.com>

It is just like the message suggested. You have a + at the end of each line
and
the beginning of the next.  The one at the end is required.  The ones at
the beginning are
causing the error message.

Please put spaces around your assignment arrows.
Difficult to read:  r<-murders
Easy to read:    r <- murders

On Thu, May 7, 2020 at 10:46 PM Ana Marija <sokovic.anamarija at gmail.com>
wrote:

> Hello,
>
> I got this error:
> Error: Cannot use `+.gg()` with a single argument. Did you
> accidentally put + on a new line?
>
> After running this:
> data(murders)
> library(ggplot2)
> library(dplyr)
> library(ggplot2)
> ggplot(data=murders)
>
> #define the slope of the line
> r<-murders %>% summarize(rate=sum(total)/sum(population)*10^6) %>%.$rate
> #mamke the plot
> murders %>% ggplot(aes(population/10^6,total,label=abb))+
>   +geom_abline(intercept = log10(r),lty=2,color="darkgrey")+
>   +geom_point(aes(col=region), size=3)+
>   +geom_text_repel()+
>   +scale_x_log10()+
>   +scale_y_log10()+
>   +xlab("Populations in millions (log scale)")+
>   +ylab("Total number of murders (log scale)")+
>   +ggtitle("US Gun Murders in US 2010")+
>   +scale_color_discrete(name="Region")+
>   +theme_economist()
>
> Is this an issue with my dplyr? Or how I can fix this code in order to
> work?
>
> Thanks
> Ana
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From g||ted|||e2014 @end|ng |rom gm@||@com  Fri May  8 08:06:20 2020
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Fri, 8 May 2020 07:06:20 +0100
Subject: [R] Adding overlap legend to a histogram
In-Reply-To: <CA+8X3fUZOcmHjJ=E5yLG5j+266US+vFgaYJS1CPR0ofJDZ8fpw@mail.gmail.com>
References: <CAC8ss30Av_LoM4eYh9TsPhEnpUWG4nDxGduoDiREiuQDmpL+sQ@mail.gmail.com>
 <CA+8X3fUZOcmHjJ=E5yLG5j+266US+vFgaYJS1CPR0ofJDZ8fpw@mail.gmail.com>
Message-ID: <CAC8ss30gDSqkYtSTO-v_H+Z4XJq4Zn2oZ3uCzhnnYREC0fJPtg@mail.gmail.com>

Dear Jim,
Thank you for looking into this.
Sorry, there was actually no overlap in the small part of the data I
reported. My error of omission.

So when I run my full data with the adjustment you made, I got some thing
that was far from what I was expecting. That tell me that I need to send
the complete data to enable you correctly adjust the code, especially in
the light of the missing/present overlap.
I have used deput function to attach the two files. Please use any symbol
to depict the color/mark/legend of the overlap dates (just to enable the
reader visualize what is going on). I am actually trying to display event
frequency/occurrence per year.

Thank you and warmest regards
Ogbos

On Fri, May 8, 2020 at 1:13 AM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Ogbos,
> I don't think that your example allows us to work out what you are
> trying to do. For one thing, "x1" and "x2" do not overlap. Also, you
> are plotting the frequencies of dates of observations, which may not
> be what you want.
> The following code will correctly display your example:
>
> hist(x1,breaks="years",freq=T,axes=F,xlim=c(9400,11100),col=c1a)
> hist(x2,breaks="years",freq=T,axes=F,add=T,col=c2a)
>  axis.Date(1, at=seq(as.Date(min(x1)), as.Date(max(x2)), by="years"))
>  axis(2)
> legend("topleft", c("AUTO", "MANUAL"), fill=c("red", "blue"))
>
> What it is displaying is the frequency of observations in your two
> vectors by calendar year. If this is what you want, and you can
> explain how you would like "overlap" to be displayed, we can probably
> provide better help.
>
> Jim
>
>
> On Fri, May 8, 2020 at 7:01 AM Ogbos Okike <giftedlife2014 at gmail.com>
> wrote:
> >
> > Dear Experts,
> > Greetings.
> >
> > I am trying to display two datasets in a histogram. I have been able to
> > plot the graph and added the legend for the two colors. I am, however,
> > having difficulties adding a legend to represent the regions of overlap
> > (the third legend).  Below are my data and code.
>

From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri May  8 08:59:08 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 8 May 2020 07:59:08 +0100
Subject: [R] Error: Cannot use `+.gg()` with a single argument.
In-Reply-To: <CAGx1TMCJZxi2Veswfozu3PC5PmwgNCeM07k2HqCk+kiLQ9-T+A@mail.gmail.com>
References: <CAF9-5jPRu09gA=oNHZvhUeyoY2pY=pdr55=Md_tB7FFatkiHiw@mail.gmail.com>
 <CAGx1TMCJZxi2Veswfozu3PC5PmwgNCeM07k2HqCk+kiLQ9-T+A@mail.gmail.com>
Message-ID: <75bec72b-6de9-7d92-7733-bb54939b00b8@sapo.pt>

Hello,

Richard's answer solves the problem, I'm writing about details in the 
OP's post.

Ana, your code example is missing some library() calls:

library(ggplot2)
library(ggrepel)
library(ggthemes)
library(dplyr)

# and where to find the data set
data(murders, package = "dslabs")


As for the dplyr pipe, you end it with .$rate to pull only that column's 
value, the following avoids that construct and is, I believe, more 
idiomatic:

#define the slope of the line
r <- murders %>%
   summarize(rate = sum(total)/sum(population)*10^6) %>%
   pull(rate)


Hope this helps,

Rui Barradas


?s 04:27 de 08/05/20, Richard M. Heiberger escreveu:
> It is just like the message suggested. You have a + at the end of each line
> and
> the beginning of the next.  The one at the end is required.  The ones at
> the beginning are
> causing the error message.
> 
> Please put spaces around your assignment arrows.
> Difficult to read:  r<-murders
> Easy to read:    r <- murders
> 
> On Thu, May 7, 2020 at 10:46 PM Ana Marija <sokovic.anamarija at gmail.com>
> wrote:
> 
>> Hello,
>>
>> I got this error:
>> Error: Cannot use `+.gg()` with a single argument. Did you
>> accidentally put + on a new line?
>>
>> After running this:
>> data(murders)
>> library(ggplot2)
>> library(dplyr)
>> library(ggplot2)
>> ggplot(data=murders)
>>
>> #define the slope of the line
>> r<-murders %>% summarize(rate=sum(total)/sum(population)*10^6) %>%.$rate
>> #mamke the plot
>> murders %>% ggplot(aes(population/10^6,total,label=abb))+
>>    +geom_abline(intercept = log10(r),lty=2,color="darkgrey")+
>>    +geom_point(aes(col=region), size=3)+
>>    +geom_text_repel()+
>>    +scale_x_log10()+
>>    +scale_y_log10()+
>>    +xlab("Populations in millions (log scale)")+
>>    +ylab("Total number of murders (log scale)")+
>>    +ggtitle("US Gun Murders in US 2010")+
>>    +scale_color_discrete(name="Region")+
>>    +theme_economist()
>>
>> Is this an issue with my dplyr? Or how I can fix this code in order to
>> work?
>>
>> Thanks
>> Ana
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com  Fri May  8 09:24:40 2020
From: @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com (Subhamitra Patra)
Date: Fri, 8 May 2020 12:54:40 +0530
Subject: [R] 
 [R ] Writing loop to estimate ARCH test for a multiple columns
 of a data frame?
In-Reply-To: <CA+8X3fWOMFQ8Ptz75j8DjhJuaEKi8nR1p+82LeNqTcB2NfSjKA@mail.gmail.com>
References: <CAOFE=kMaSgqy7_dMsB3WtTgN+BDa4b5m28GO0uxm0rUsDmqc9g@mail.gmail.com>
 <CA+8X3fWOMFQ8Ptz75j8DjhJuaEKi8nR1p+82LeNqTcB2NfSjKA@mail.gmail.com>
Message-ID: <CAOFE=kOtwzrr3cFXhvvS+db+iw=p4eg1Fm6JCGtCzwessQ90FA@mail.gmail.com>

Dear Sir,

Herewith I am pasting a part of my sample data having 12 columns below, and
want to calculate ARCH test for the 12 columns by using a loop.

Please help me in this regard. Thank you very much for your help.

Year_Month A1 A2 A3 A4 A5 A6 A7 A8 A9 A10 A11 A12
94-Jan 0.051197 7.05E-05 0.058806 -0.00818 0.538001 0.009766 0.025787
0.035478 0.056663 0.014665 0.23132 0.008644
94-Feb 0.06424 -0.01086 0.049823 -0.04989 0.557945 0.00974 0.027757 0.021494
0.016947 0.014584 0.229776 -0.02317
94-Mar 0.056168 -0.00626 0.061555 -0.03427 0.524705 0.009694 0.027632
-0.00656 0.008358 0.014499 0.190421 0.003026
94-Apr 0.129051 0.043813 0.060453 0.017469 0.545895 0.009615 0.01932 0.01171
0.016003 0.014412 0.140396 0.017556
94-May 0.142182 -0.03848 0.059938 0.015054 0.525178 0.009479 0.027741
0.000605 0.0185 0.014327 0.093228 -0.03989
94-Jun 0.152981 -0.03227 0.071485 -0.01025 0.363882 0.009323 0.030762
0.013005 0.03634 0.014239 0.035625 -0.01355
94-Jul 0.16216 0.046374 0.073669 0.020508 0.3405 0.00926 0.044822 -0.00954
0.042422 0.014154 0.037954 0.00097
94-Aug 0.124355 -0.06952 0.091429 0.015932 0.38519 0.009269 0.071701
0.000623 0.051954 0.01407 0.055852 0.007522
94-Sep 0.059405 0.057487 0.086265 -0.01169 0.401963 0.009171 0.086685
-0.01058 0.054404 0.013986 0.07285 0.002022
94-Oct 0.0594 0.021166 0.080765 0.006442 0.438915 0.009041 0.070351 0.006776
0.033622 0.013906 0.068344 -0.01532
94-Nov 0.072064 -0.03104 0.079567 -0.03295 0.521214 0.008986 0.066044
-0.01853 0.035202 0.013826 0.067093 -0.02278
94-Dec 0.068208 0.01024 0.069919 -0.01507 0.461059 0.008856 0.050985
0.009514 0.008638 0.013744 0.040348 0.00423
95-Jan 0.079074 -0.00153 0.070458 -0.04205 0.506227 0.00883 0.046561
-0.03907 0.015322 0.013662 0.034103 -0.00888
95-Feb 0.074231 -0.05728 0.062612 0.035992 0.487126 0.008815 0.052816
-0.01344 0.06728 0.013583 0.063281 -0.0054
95-Mar 0.065212 0.056084 0.095783 0.006825 0.476386 0.008774 0.047498
0.015178 0.040273 0.013499 0.060805 0.006099
95-Apr 0.081238 0.024283 0.098827 0.005791 0.432363 0.008748 0.06047
0.011613 0.013068 0.013417 0.058321 -0.01281
95-May 0.093726 0.008623 0.076698 0.027274 0.321103 0.008679 0.037962
0.00115 0.013647 0.013339 0.066724 -0.00271
95-Jun 0.113998 0.005484 0.073392 -0.00252 0.38195 0.008684 0.042794
-0.01133 0.054244 0.013261 0.055655 0.015941
95-Jul 0.097076 0.008842 0.090776 0.006378 0.622055 0.008728 0.036476
0.016159 0.055301 0.013188 0.057034 -0.0036
95-Aug 0.075751 0.002437 0.094687 -0.00398 0.637972 0.008839 0.052791
-0.00819 0.327487 0.013114 0.067734 0.00565
95-Sep 0.074714 0.001279 0.091216 0.013169 0.656225 0.008956 0.086582
-0.0013 0.690172 0.01304 0.059523 0.028675
95-Oct 0.048771 -0.01775 0.098525 0.003447 0.68386 0.009071 0.091073
-0.01597 0.640065 0.012967 0.030469 0.005139
95-Nov 0.069776 -0.00164 0.077763 0.00158 0.559675 0.008808 0.094129 0.01832
0.726821 0.012893 0.030908 -0.00955
95-Dec 0.135469 0.001886 0.074658 0.01263 0.563716 0.008888 0.113828
0.011372 0.737532 0.012822 0.224459 -0.00186
96-Jan 0.175166 0.00068 0.071721 0.030701 0.534648 0.009114 0.086481
0.016228 0.687297 0.013112 0.349764 0.000727
96-Feb 0.167327 0.013771 0.055352 -0.03142 0.556339 0.009119 0.080475
-0.00691 0.696365 0.013077 0.342758 -4.90E-05
96-Mar 0.158759 -0.02094 0.042232 -0.00331 0.532126 0.009041 0.077231
0.009009 0.579396 0.012271 0.342196 -0.002
96-Apr 0.116956 0.02624 0.051037 -0.01496 0.575416 0.009123 0.079496
0.017197 0.557262 0.012094 0.299566 0.022657
96-May 0.109049 -0.02648 0.059972 0.00658 0.616302 0.009086 0.095365
-0.01682 0.521757 0.011933 0.074309 0.021621
96-Jun 0.102001 2.71E-05 0.060901 -0.00372 0.593491 0.009213 0.095232
0.001363 0.523983 0.011757 0.070504 -0.00507
96-Jul 0.079941 -0.02107 0.046018 -0.00708 0.562537 0.009136 0.094451
-0.01132 0.534417 0.011413 0.073706 -0.00615
96-Aug 0.109775 0.005178 0.051713 0.007174 0.54939 0.009008 0.088945
-0.01136 0.445843 0.010925 0.066559 0.009937
96-Sep 0.089581 -0.0005 0.049835 0.016873 0.54664 0.008887 0.082659 0.011384
0.435423 0.010697 0.091269 0.00687
96-Oct 0.07429 -0.01499 0.063584 0.008829 0.485504 0.008965 0.072986
-0.01695 0.54066 0.010649 0.325364 0.012261
96-Nov 0.060441 0.021057 0.100844 0.018152 0.415023 0.009033 0.072366
-0.00222 0.646444 0.010653 0.323194 0.01409
96-Dec 0.061482 0.0218 0.142038 -6.42E-06 0.492536 0.008947 0.081333
-0.02433 0.661019 0.010555 0.367988 -0.00023
97-Jan 0.053437 0.025314 0.137257 -0.00659 0.578904 0.008841 0.074613
-0.0068 0.628154 0.010609 0.355763 0.00581
97-Feb 0.080489 -0.01411 0.123644 0.009692 0.571364 0.008794 0.07673
0.005832 0.549697 0.010781 0.21588 0.070824
97-Mar 0.097621 0.00073 0.115192 -0.04503 0.639719 0.008686 0.065906
-0.01063 0.543819 0.010442 0.129773 0.004692
97-Apr 0.112502 -0.00052 0.064499 0.007382 0.648139 0.008674 0.038621
0.006408 0.591661 0.010283 0.079461 0.009395
97-May 0.109789 0.028968 0.079382 0.032543 0.530901 0.008884 0.029301
0.039566 0.492504 0.01004 0.042617 -0.00151
97-Jun 0.087521 -0.03031 0.037389 0.001738 0.500643 0.00886 0.028772 0.0146
0.47233 0.009869 0.033241 0.001504
97-Jul 0.085527 0.01424 0.029224 0.018015 0.496859 0.008725 0.036717
-0.00956 0.443577 0.009995 0.026995 0.014942
97-Aug 0.077215 -0.00935 0.025766 -0.05705 0.424685 0.008555 0.034725
-0.02395 0.481891 0.010128 0.029444 0.021642
97-Sep 0.08778 0.012634 0.064433 0.012569 0.495125 0.008406 0.06119 0.002597
0.515191 0.010241 0.056915 0.002897
97-Oct 0.067236 -0.07837 0.100233 -0.08455 0.603348 0.008296 0.065799
-0.02425 0.545503 0.010191 0.071548 0.002394
97-Nov 0.074344 0.008919 0.169394 -0.00212 0.615421 0.008215 0.076836
-0.04056 0.552007 0.01035 0.131526 0.001346
97-Dec 0.066797 0.001889 0.180801 -0.0114 0.607294 0.008081 0.11593 -0.01879
0.573696 0.01082 0.283244 0.002348
98-Jan 0.050659 -0.01722 0.163617 0.074366 0.52707 0.007969 0.091612
0.027019 0.583443 0.011549 0.267086 -0.02785
98-Feb 0.049803 0.015915 0.04491 0.054682 0.38982 0.008092 0.092166 0.002436
0.61572 0.011637 0.275094 -0.0145
98-Mar 0.047308 0.017094 0.047185 0.000772 0.455737 0.008007 0.087419
-0.01693 0.594474 0.012426 0.286536 -0.01181
98-Apr 0.047385 0.003939 0.048202 -0.03154 0.567806 0.007983 0.060277
-0.00191 0.597842 0.013039 0.127336 0.107101
98-May 0.064783 0.025706 0.060724 -0.03807 0.684579 0.007976 0.048987
-0.01964 0.518052 0.013439 0.069782 0.006585
98-Jun 0.159381 -0.01328 0.052821 -0.01119 0.684776 0.007913 0.038652
0.002138 0.543559 0.012967 0.110726 -0.00548
98-Jul 0.151487 0.004139 0.051838 -0.00775 0.70082 0.007946 0.048077
-0.00496 0.585499 0.012684 0.075913 0.000926
98-Aug 0.148411 -0.04313 0.047802 -0.0209 0.66453 0.007953 0.046124 -0.02537
0.69816 0.012255 0.072707 -0.00505
98-Sep 0.16881 -0.03613 0.052287 0.018093 0.595134 0.008081 0.044761
-0.00243 0.677852 0.012169 0.075073 -0.0052
98-Oct 0.070201 -0.07938 0.055037 0.062009 0.549705 0.007892 0.051584
0.034295 0.691742 0.011869 0.052704 -0.0219
98-Nov 0.049853 0.028386 0.068794 0.005291 0.442349 0.007833 0.045537
0.030026 0.669312 0.0117 0.219442 -0.0081
98-Dec 0.069926 0.000449 0.101457 -0.00323 0.356563 0.007826 0.05571
0.000408 0.529556 0.0108 0.390611 -0.004
99-Jan 0.07435 -0.01122 0.096968 -0.01734 0.343742 0.007695 0.078749
0.012134 0.541906 0.010943 0.41004 0.008279
99-Feb 0.082127 0.021732 0.114963 -0.00191 0.345825 0.007578 0.083706
-0.00624 0.546338 0.011204 0.425059 -0.01535
99-Mar 0.081221 0.040004 0.111538 0.023871 0.358622 0.007456 0.079843
0.032283 0.566893 0.011074 0.401445 0.102554
99-Apr 0.086991 0.02926 0.065761 0.097698 0.301239 0.007323 0.097955 0.01104
0.529548 0.010871 0.076728 0.027323
99-May 0.077475 0.023956 0.074776 -0.02335 0.321715 0.007218 0.063023
0.011379 0.397527 0.01031 0.073261 -0.00508
99-Jun 0.076112 0.020359 0.068651 0.033434 0.4018 0.007095 0.06711 0.021294
0.408022 0.010055 0.06967 0.006254
99-Jul 0.080411 -0.01148 0.060564 -0.00474 0.39331 0.007002 0.066918 0.02464
0.478281 0.010005 0.066309 0.015189
99-Aug 0.068091 -0.01721 0.060539 0.004914 0.37717 0.006855 0.062521
0.007325 0.482744 0.010036 0.088398 0.007067
99-Sep 0.086042 0.008278 0.077601 -0.01802 0.403218 0.006718 0.067205
0.000943 0.628366 0.009918 0.147324 0.005286
99-Oct 0.075385 0.010131 0.061047 -0.00448 0.372075 0.00665 0.060542 0.0217
0.780534 0.009656 0.131252 0.005054
99-Nov 0.055049 0.020592 0.068442 0.035127 0.399578 0.006527 0.065934
0.025071 0.772524 0.009266 0.157148 0.027663
99-Dec 0.062593 0.046883 0.067727 0.04049 0.502933 0.006406 0.053723
0.014904 0.728022 0.008887 0.167636 0.018708
00-Jan 0.057252 0.004454 0.065601 -0.02153 0.558368 0.006257 0.049291
0.009493 0.605325 0.008994 0.09732 0.037374
00-Feb 0.065328 0.045896 0.075552 0.026155 0.555294 0.006163 0.057978
-0.00837 0.504593 0.009741 0.130217 0.00261
00-March 0.065141 -0.02182 0.068206 0.003206 0.596057 0.006076 0.054416
0.011653 0.529128 0.009573 0.090386 -0.01596
00-Apr 0.080721 0.023865 0.059405 -0.03907 0.5262 0.005846 0.057356 -0.02115
0.468138 0.009582 0.034706 0.004283
00-May 0.074753 0.016991 0.063155 -0.00071 0.575856 0.005665 0.066024
-0.01439 0.402228 0.009809 0.030408 0.002116
00-Jun 0.14341 0.004936 0.054784 0.026521 0.616701 0.005597 0.049632
0.013278 0.471112 0.009514 0.034816 0.006285
00-Jul 0.141626 -0.0135 0.053571 0.010033 0.600591 0.005475 0.055653
-0.02939 0.483099 0.009243 0.034094 0.00531
00-Aug 0.127419 0.0245 0.061189 0.003298 0.568755 0.005432 0.067319 0.014447
0.513747 0.009217 0.22144 0.015615
00-Sep 0.122004 -0.00474 0.050212 -0.02615 0.503157 0.005269 0.087926
-0.01138 0.581831 0.009141 0.367495 0.001285
00-Oct 0.078788 -0.10172 0.044231 -0.0075 0.432512 0.0051 0.089254 -0.01574
0.656075 0.009038 0.359533 0.000368
00-Nov 0.053822 0.010106 0.034679 -0.01469 0.513539 0.00498 0.100637
-0.01064 0.622279 0.008928 0.377782 -0.00477
00-Dec 0.069006 -0.0005 0.051647 0.025002 0.546061 0.004794 0.103877
-0.02231 0.610257 0.00881 0.333657 0.09513
1-Jan 0.071511 -0.00837 0.057658 0.017076 0.553587 0.004638 0.095702
-0.02487 0.56779 0.00868 0.250782 0.008235
1-Feb 0.076866 0.00833 0.060892 -0.02218 0.59127 0.0045 0.093535 -0.0101
0.533402 0.008675 0.240717 0.003711
1-Mar 0.07557 -0.0462 0.074634 -0.03465 0.595882 0.004429 0.068847 0.002673
0.479236 0.008611 0.091359 0.034367
1-Apr 0.070519 0.033694 0.060347 0.003217 0.480071 0.004227 0.04972 0.03196
0.561474 0.008402 0.052372 0.002317
1-May 0.064918 0.011544 0.031374 0.009111 0.551824 0.004201 0.023617
0.001112 0.560844 0.008168 0.043051 0.009939
1-Jun 0.104019 -0.00345 0.024989 -0.01172 0.568941 0.004169 0.020633 -0.0132
0.560457 0.008024 0.049529 0.007287
1-Jul 0.090489 0.017047 0.021945 -0.01905 0.557895 0.004073 0.022649
-0.02304 0.56767 0.007859 0.048501 0.004123
1-Aug 0.099604 -0.02219 0.028166 -0.02333 0.625414 0.004008 0.033092
-0.00533 0.604529 0.007885 0.054362 0.007967
1-Sep 0.107159 -0.07639 0.029653 -0.02712 0.622213 0.004059 0.060953
-0.02039 0.597122 0.007776 0.059832 -0.03705
1-Oct 0.092227 0.008594 0.063975 -0.01046 0.60189 0.004079 0.092008 -0.00481
0.559119 0.007765 0.073072 0.00552
1-Nov 0.133317 0.017435 0.092959 0.026051 0.502762 0.004133 0.103452
-0.00171 0.54558 0.007544 0.051221 0.001584
1-Dec 0.148023 0.019271 0.104346 0.012594 0.490295 0.003975 0.107077
-0.01876 0.533209 0.007311 0.056159 0.012704
2-Jan 0.150961 -0.01348 0.10693 -0.01523 0.426876 0.003902 0.100983 -0.00865
0.512898 0.007222 0.045519 0.006621
2-Feb 0.129901 -0.00175 0.078699 0.007048 0.352362 0.003811 0.086475
0.022773 0.559536 0.007018 0.051134 0.001665
2-Mar 0.122265 -0.06762 0.042505 0.008784 0.410423 0.003871 0.067548
0.005427 0.568946 0.006885 0.044913 0.009651
2-Apr 0.075202 -0.01114 0.04052 0.003142 0.46432 0.003887 0.06172 0.005093
0.525967 0.006694 0.049629 0.032768
2-May 0.072844 -0.00547 0.022389 -0.00371 0.449056 0.003853 0.040507
0.004051 0.557733 0.00658 0.036551 0.005127
2-Jun 0.066132 0.005908 0.058852 -0.0166 0.461077 0.00388 0.049413 -0.01205
0.552187 0.00646 0.022301 -0.00198
2-Jul 0.059293 0.007211 0.059812 -0.01172 0.501559 0.003894 0.060857
-0.01298 0.55335 0.006188 0.022222 -0.00236
2-Aug 0.088299 -0.01081 0.068442 -0.00538 0.526355 0.003923 0.082171
-0.00367 0.632755 0.006052 0.022559 -0.00173
2-Sep 0.085233 -0.00312 0.07763 -0.02517 0.519692 0.00395 0.115502 -0.01224
0.77597 0.006027 0.047021 0.00225
2-Oct 0.079523 0.010872 0.057461 0.020658 0.506263 0.003923 0.125037
-0.00671 0.776439 0.005841 0.241231 -0.00458
2-Nov 0.092114 0.019084 0.056722 0.015734 0.482294 0.003937 0.132646 0.01848
0.776393 0.005798 0.241103 -0.01662
2-Dec 0.100498 -0.01973 0.058859 -0.01878 0.452742 0.003915 0.12689 0.003786
0.676371 0.005501 0.264177 0.019179
3-Jan 0.129572 -0.00262 0.048539 0.00309 0.544448 0.003876 0.094442 0.027311
0.556401 0.004892 0.254185 0.004473
3-Feb 0.132895 -0.00044 0.058255 -0.00171 0.600125 0.003869 0.078469
0.005541 0.679645 0.004928 0.115786 0.110436
3-Mar 0.136859 0.027562 0.058904 -0.01381 0.556289 0.003903 0.070073
0.004383 0.700573 0.004891 0.032592 0.03212
3-Apr 0.136273 0.068605 0.064695 -0.00261 0.613031 0.003871 0.071458
0.005898 0.699115 0.0048 0.029105 0.009723
3-May 0.099559 0.071864 0.0722 0.019948 0.626308 0.003719 0.078507 0.016676
0.648559 0.00479 0.023664 0.026138
3-Jun 0.065194 -0.02172 0.063345 0.001001 0.57964 0.003541 0.074453 0.015788
0.495185 0.004489 0.020389 0.019138
3-Jul 0.064383 -0.0382 0.05803 0.011462 0.582394 0.00346 0.055675 0.002646
0.500264 0.004442 0.015041 0.016287
3-Aug 0.070518 -0.01549 0.063356 0.017911 0.465972 0.00349 0.066048 0.022058
0.581051 0.004493 0.012775 0.025527
3-Sep 0.059538 0.000566 0.064277 0.014528 0.441227 0.003367 0.076711
0.024424 0.605593 0.004563 0.034063 0.012428
3-Oct 0.071278 0.016929 0.07425 0.023864 0.438028 0.003189 0.066948 0.014291
0.546467 0.004617 0.081215 -0.00357
3-Nov 0.078345 0.014875 0.082316 0.002779 0.545115 0.003026 0.090011
-0.00223 0.519579 0.004587 0.137425 0.006393
3-Dec 0.086351 0.016485 0.083428 0.009423 0.550845 0.002851 0.088829 0.02042
0.492655 0.004664 0.093447 0.010691
4-Jan 0.081329 0.006615 0.070349 0.011773 0.503661 0.002664 0.0562 0.023223
0.513912 0.004819 0.04145 0.009952
4-Feb 0.06281 0.006856 0.06135 0.011275 0.295814 0.002551 0.048915 0.001594
0.523612 0.00499 0.023872 -0.00975
4-Mar 0.049264 -0.00044 0.05989 -0.02068 0.056343 0.002607 0.04823 0.027175
0.609287 0.005025 0.023252 0.01872
4-Apr 0.049299 0.009245 0.052224 -0.00065 0.158436 0.00264 0.044182 -0.0123
0.607031 0.004961 0.024989 0.023746
4-May 0.07551 0.005179 0.046009 0.009392 0.206344 0.002588 0.036726 -0.01127
0.576678 0.005017 0.030117 0.000588
4-Jun 0.106293 0.012995 0.051612 -0.00029 0.200992 0.002555 0.053382
0.009459 0.469148 0.005189 0.031416 0.009538
4-Jul 0.108154 -0.00959 0.047847 0.001517 0.227312 -0.00647 0.05747 -0.01319
0.476149 0.005079 0.314022 0.013775
4-Aug 0.081915 0.004152 0.053783 0.011214 0.198439 0.045954 0.062876
0.003317 0.44414 0.005068 0.303724 0.00769
4-Sep 0.081102 0.009467 0.062976 0.00507 0.093854 -0.00086 0.070439 -0.01041
0.439919 0.004963 0.286094 0.011138
4-Oct 0.060353 -0.0099 0.044424 0.006191 0.10147 0.012374 0.050592 0.002093
0.461951 0.004918 0.286624 0.014818
4-Nov 0.061972 0.016949 0.054118 0.017346 0.108899 -0.00125 0.048133
0.009199 0.556459 0.005029 0.174995 0.089482
4-Dec 0.067244 0.023686 0.070182 0.00137 0.089756 0.004985 0.055462 0.021193
0.672079 0.005 0.075406 -0.0024
5-Jan 0.07898 -0.00534 0.08631 -0.0093 0.091402 0.009871 0.073969 0.006254
0.66872 0.004962 0.048242 0.040205
5-Feb 0.076154 0.014852 0.079967 0.012873 0.079875 0.01805 0.080537 0.006993
0.677289 0.004799 0.031595 0.026168
5-Mar 0.077074 0.00191 0.090161 -0.01357 0.052445 -0.00301 0.071842 -0.00201
0.603639 0.00486 0.038924 0.027139
5-Apr 0.079171 0.016211 0.081519 0.008182 0.067475 0.017158 0.084696
-0.00711 0.482537 0.004869 0.032052 -0.00649
5-May 0.08762 0.001995 0.061132 -0.00617 0.108894 -0.00095 0.036985 0.011574
0.499607 0.004559 0.036882 0.02224
5-Jun 0.112654 -0.02902 0.062165 0.005573 0.150261 -0.00456 0.051033
0.000359 0.454388 0.004385 0.047875 0.031248
5-Jul 0.114144 0.014178 0.05508 0.009906 0.133826 -0.00185 0.049361 0.001181
0.535222 0.004236 0.051302 -0.0007
5-Aug 0.118755 0.009505 0.045739 0.000422 0.134694 0.002732 0.048689
0.013723 0.65319 0.004264 0.050513 0.025555
5-Sep 0.126867 0.01075 0.060408 0.013722 0.120036 0.006571 0.063265 0.020154
0.656104 0.004173 0.070757 0.002251
5-Oct 0.089795 -0.01696 0.055212 -0.02112 0.107882 0.011945 0.059214
0.006303 0.690117 0.004071 0.045845 0.020133
5-Nov 0.090791 0.008801 0.05865 0.008551 0.117452 0.003818 0.061104 0.005212
0.641982 0.004077 0.049325 0.010303
5-Dec 0.118981 0.016034 0.081244 -0.00309 0.124691 -0.00724 0.072101
0.020097 0.578578 0.004283 0.04876 0.013458
6-Jan 0.070068 -0.01575 0.054248 0.013942 0.102255 0.056449 0.080795
0.019344 0.586796 0.00406 0.047243 0.043854
6-Feb 0.073022 -0.00889 0.052663 0.002691 0.109722 -0.01689 0.087112
-0.00481 0.601005 0.003781 0.048973 0.007074
6-Mar 0.104843 0.00397 0.052227 -0.00155 0.081988 -0.0331 0.113525 0.005888
0.515759 0.003533 0.06839 0.013283
6-Apr 0.104731 0.032016 0.04815 0.026012 0.08035 0.003807 0.115429 0.005909
0.338814 0.003188 0.066514 -0.01532
6-May 0.09766 0.001961 0.035253 -0.01271 0.103763 -0.01503 0.081776 0.002416
0.330045 0.003131 0.069321 -0.03033
6-Jun 0.119052 -0.02364 0.039689 0.005487 0.114745 0.010777 0.081682
-0.00304 0.236855 0.003015 0.095066 0.097317
6-Jul 0.088549 0.008985 0.036248 0.010201 0.093526 0.009576 0.054863
-0.01742 0.102126 0.002985 0.134684 -0.1041
6-Aug 0.067978 0.019824 0.044921 0.005189 0.07627 0.030992 0.047808 0.003101
0.155821 0.002881 0.167384 0.033159
6-Sep 0.066656 0.007713 0.060278 0.001498 0.102424 0.004677 0.057565
-0.00311 0.205756 0.002718 0.161339 0.002583
6-Oct 0.068295 0.026603 0.053452 0.010615 0.099656 0.000584 0.058789
-0.00066 0.199022 -0.02302 0.155059 -0.06468
6-Nov 0.089642 0.001217 0.067761 0.007713 0.086734 -0.00508 0.054733
0.000595 0.196888 -0.01408 0.125395 -0.05451
6-Dec 0.099192 -0.00135 0.059676 0.022421 0.099908 0.027104 0.050893 0.0043
0.245125 0.0489 0.13109 -0.00058
7-Jan 0.105593 0.00331 0.039473 0.003456 0.047097 -0.00912 0.055139 0.019326
0.117142 0.028954 0.109013 -0.05588
7-Feb 0.118889 0.004966 0.040191 -0.00952 0.041858 0.009777 0.05421 0.012598
0.113921 4.07E-05 0.114541 0.075385
7-Mar 0.08916 0.015319 0.025478 0.001806 0.038149 0.007381 0.053036 -0.00099
0.123744 0.011148 0.118334 0.013907
7-Apr 0.080799 0.038111 0.0491 0.011014 0.049405 -0.00921 0.022374 -0.00828
0.128256 0.024402 0.109033 -0.00633
7-May 0.088313 0.00604 0.044642 0.000793 0.031703 0.020668 0.019772 0.011723
0.113827 0.041275 0.122305 -0.01308
7-Jun 0.07316 -0.01507 0.042463 0.014393 0.043263 0.009882 0.021969 -0.00014
0.117902 0.011965 0.214639 -0.00689
7-Jul 0.065668 -0.0048 0.046214 0.01654 0.037575 0.017381 0.024748 -0.00012
0.101637 0.019989 0.205776 0.05221
7-Aug 0.070372 -0.02158 0.043537 0.007309 0.042105 -0.00364 0.044089
-0.00605 0.095848 -0.00647 0.180665 0.001296
7-Sep 0.047904 0.03766 0.033164 0.031409 0.048308 0.001623 0.033734 -0.00284
0.127852 0.008018 0.160366 0.012009
7-Oct 0.05138 0.041531 0.059524 0.034967 0.04954 0.014953 0.029942 0.001174
0.123522 0.004083 0.141225 0.077784
7-Nov 0.037378 0.000217 0.054199 -0.01968 0.041879 0.009651 0.028309
-0.00059 0.107465 -0.01076 0.132328 0.045576
7-Dec 0.042732 -0.00199 0.064209 -0.00477 0.049397 0.014798 0.023225
-0.01046 0.094376 0.02261 0.149627 0.049116
8-Jan 0.035948 -0.01236 0.060851 -0.03168 0.040511 0.002253 0.026289
-0.00212 0.085185 0.017828 0.130155 0.01301
8-Feb 0.037398 -0.00124 0.054232 -0.03426 0.050906 0.014738 0.0282 0.002013
0.069707 0.021845 0.15015 -0.01344
8-Mar 0.042094 -0.0305 0.032707 -0.00876 0.05264 -0.01032 0.024265 -0.00324
0.05907 0.013726 0.152467 -0.05545
8-Apr 0.04852 0.01582 0.031455 0.026955 0.063245 0.011327 0.025647 0.015876
0.067814 -0.00104 0.124019 0.05047
8-May 0.059497 -0.00208 0.038082 -0.00581 0.037738 0.006043 0.028794
0.017551 0.071995 -0.00976 0.104521 -0.0247
8-Jun 0.096974 -0.05471 0.041072 -0.03588 0.085357 -0.00082 0.035479
-0.01578 0.119341 0.00171 0.099164 -0.01334
8-Jul 0.092663 -0.03785 0.045204 0.004457 0.118828 -0.00513 0.037903
-0.00115 0.145136 0.007081 0.093327 0.001238
8-Aug 0.110892 -0.00404 0.042687 -0.02514 0.192407 -0.00432 0.028538
-0.01015 0.13363 -0.00083 0.164095 -0.0194
8-Sep 0.101492 -0.04277 0.064079 -0.043 0.190126 -0.02691 0.03497 -0.03088
0.137926 -0.03217 0.176231 -0.02584
8-Oct 0.084709 -0.06177 0.050796 -0.06637 0.202988 -0.00546 0.017852
-0.01142 0.128956 -0.04807 0.194271 -0.17044
8-Nov 0.057129 -0.08266 0.068863 -0.00142 0.204409 -0.03338 0.031681
0.007806 0.117484 -0.02477 0.234215 -0.07913
8-Dec 0.079164 0.020009 0.106671 0.018427 0.146101 0.041635 0.051961
0.011684 0.104922 -0.05998 0.180431 0.174908
9-Jan 0.063432 -0.00669 0.093999 0.018161 0.105719 -0.03781 0.082073
-0.03367 0.106395 -0.05258 0.12232 -0.01428
9-Feb 0.05705 -0.01499 0.083275 -0.00843 0.101315 -0.01958 0.095826 -0.03223
0.087682 -0.09934 0.12882 0.012371
9-Mar 0.061702 0.030318 0.088468 0.012107 0.108591 0.001889 0.095932
0.011516 0.057128 0.036127 0.146176 0.037564
9-Apr 0.059506 0.082299 0.08067 0.061903 0.086351 -0.02017 0.095266 0.0144
0.050497 0.017112 0.124557 0.062549
9-May 0.049584 0.050675 0.04681 0.055353 0.059368 0.008474 0.038863 0.062884
0.162174 0.010617 0.249354 0.019946
9-Jun 0.050694 0.005618 0.044184 0.002126 0.078106 -0.01453 0.043025
0.005721 0.182842 0.01024 0.307549 -0.00516
9-Jul 0.066973 0.030587 0.038676 0.020141 0.084477 -0.02209 0.034024
0.015118 0.171025 0.000822 0.333595 -0.00769
9-Aug 0.062135 0.010437 0.053099 -0.00917 0.096394 0.010992 0.05032 0.007868
0.186191 0.012318 0.353648 -0.02295
9-Sep 0.050647 0.033668 0.059205 0.012482 0.111134 0.009082 0.067715
0.010795 0.138849 0.05987 0.292975 0.137462
9-Oct 0.055956 0.013449 0.055734 0.000706 0.086458 -0.00339 0.053182
-0.00297 0.079412 -0.02269 0.163511 -0.01146
9-Nov 0.058756 -0.01045 0.059039 0.001159 0.073077 -0.02239 0.057373
-0.01219 0.077946 -0.03549 0.165941 0.02084
9-Dec 0.054019 0.016576 0.10987 0.001472 0.045507 -0.00868 0.05764 0.001159
0.056954 0.003272 0.093262 -0.00356
10-Jan 0.065299 -0.0012 0.07959 -0.014 0.046484 0.007152 0.030854 0.01432
0.038635 -0.00133 0.124903 0.01488
10-Feb 0.080965 -0.00718 0.075793 0.0106 0.050117 -0.00315 0.025844 0.003503
0.028887 0.029483 0.138384 0.01319
10-Mar 0.087112 0.019504 0.08891 0.005904 0.056132 0.008442 0.036296 0.01051
0.038374 0.020292 0.138234 0.027963
10-Apr 0.088547 -0.02642 0.039839 0.026567 0.069717 0.006489 0.041708
-0.00507 0.041266 0.00206 0.147216 0.012707
10-May 0.101039 -0.05872 0.037815 -0.00875 0.083237 -0.04479 0.030418 -0.005
0.051597 -0.01986 0.128739 -0.02404
10-Jun 0.095199 -0.00342 0.057961 0.004086 0.123835 -0.0142 0.035956
-0.00352 0.091948 0.005044 0.135558 0.021432
10-Jul 0.075234 0.025279 0.063847 0.009611 0.116717 -0.0035 0.031698
0.002359 0.252143 0.011422 0.287141 0.000121
10-Aug 0.079156 -0.00047 0.051437 -0.00546 0.112914 0.004803 0.02562
-0.00573 0.261809 0.013716 0.309455 -0.01186
10-Sep 0.059055 0.042768 0.057365 0.019033 0.075016 0.0137 0.023117 0.008312
0.249929 0.029749 0.313657 0.01661
10-Oct 0.054609 0.009678 0.063531 0.005707 0.081634 0.000622 0.02408
-0.00129 0.189048 0.000912 0.25145 -0.00483
10-Nov 0.068635 -0.00222 0.060139 -0.0012 0.094356 0.011296 0.051482
0.006082 0.107408 -0.0728 0.172865 -0.05986
10-Dec 0.081359 0.024868 0.069255 -0.0048 0.088267 -0.00047 0.05896 0.011501
0.06167 0.003235 0.151708 0.013402
11-Jan 0.073495 -0.01786 0.067798 0.003442 0.099323 0.009847 0.067582
0.007326 0.06768 -0.00637 0.149 -0.01795
11-Feb 0.063406 -0.01603 0.069918 0.006861 0.088746 -0.01965 0.076099
0.01099 0.052349 -0.03146 0.12507 -0.02541
11-Mar 0.048697 0.019537 0.078758 0.001983 0.033649 -0.00549 0.0742 -0.00639
0.044473 0.007343 0.078477 0.055442
11-Apr 0.038096 -0.00979 0.072229 0.002315 0.031185 -0.00677 0.093811
0.007807 0.047309 0.018028 0.231757 0.017197
11-May 0.047878 -0.0032 0.066941 0.001751 0.089032 -0.01206 0.057944
0.002471 0.064242 -0.0069 0.325707 0.006877
11-Jun 0.039098 -0.00809 0.073197 -0.01205 0.11624 -0.00688 0.060207
0.004771 0.070361 -0.00677 0.333381 -0.01486
11-Jul 0.037719 0.001729 0.062112 0.006542 0.114612 -0.00613 0.054476
0.005025 0.094604 0.000269 0.373547 -0.00403
11-Aug 0.034513 -0.05368 0.05377 -0.01739 0.121313 -0.0161 0.068306 -0.01597
0.122455 -0.01018 0.363369 0.079906
11-Sep 0.02704 -0.01352 0.053927 -0.04383 0.103685 -0.02907 0.088478
-0.01848 0.131231 0.003976 0.330517 0.054048
11-Oct 0.038786 0.027062 0.029992 0.031106 0.079689 -0.00876 0.083739
0.006048 0.10505 0.006914 0.348418 0.006048
11-Nov 0.041044 -0.01657 0.03751 -0.02094 0.054133 0.019163 0.088931
-0.01241 0.072494 -0.01463 0.133484 0.074594
11-Dec 0.103128 -0.01263 0.046591 0.008993 0.048258 -0.0251 0.074848
-0.00018 0.07466 -0.00086 0.087007 0.028796
12-Jan 0.118227 0.026444 0.052006 0.041587 0.046611 0.001658 0.01574
0.035412 0.063587 0.002389 0.074204 0.016416
12-Feb 0.122598 -0.00084 0.069188 0.01369 0.050482 0.001776 0.023857
0.009134 0.06617 0.005087 0.093116 0.030493
12-Mar 0.104265 0.017232 0.076994 -0.01145 0.052082 0.003762 0.027545
-0.00288 0.076327 0.004981 0.099453 0.037708
12-Apr 0.098936 0.022583 0.06595 0.000605 0.057745 0.002054 0.037188
-0.01349 0.064687 0.000578 0.138139 -0.01452
12-May 0.089141 -0.01862 0.053403 -0.02149 0.040155 6.19E-05 0.043449
-0.01936 0.056818 -0.0068 0.163505 -0.04733
12-Jun 0.099204 0.0029 0.055061 0.009495 0.033332 0.005187 0.070085 0.011337
0.070332 -0.00277 0.225889 -0.01912
12-Jul 0.097595 0.01046 0.039244 0.011626 0.078823 -0.00086 0.059504 -0.0049
0.082291 -0.0082 0.250106 0.002272
12-Aug 0.102617 -0.00756 0.033855 -0.00345 0.084789 0.005599 0.063005
-0.00181 0.081742 0.005522 0.229006 0.040341
12-Sep 0.102183 0.039397 0.04452 0.015495 0.081538 0.00815 0.041798 9.62E-05
0.077269 0.01068 0.190494 -0.02643
12-Oct 0.063156 0.009069 0.075626 0.008326 0.094997 -0.00238 0.028839
-0.00493 0.091971 -0.00171 0.183817 0.026617
12-Nov 0.059478 0.014209 0.069961 0.00384 0.071881 0.000951 0.022743
0.007659 0.093547 0.006532 0.118919 0.007045
12-Dec 0.04359 -0.0137 0.076152 0.007972 0.061429 0.003239 0.028688 0.013573
0.055046 -0.00359 0.08988 0.012686
13-Jan 0.042402 0.003296 0.071592 0.013726 0.055347 0.000777 0.017266
0.017428 0.025965 0.010131 0.077671 0.022031
13-Feb 0.053014 -0.00078 0.068647 0.005744 0.066683 0.005203 0.028252
0.010783 0.030288 -0.00102 0.062858 0.003411
13-Mar 0.062571 0.010994 0.067563 -0.01074 0.049167 -0.00132 0.027015
0.015522 0.03963 -0.00144 0.057604 0.016195
13-Apr 0.087643 0.001583 0.0732 0.000713 0.048623 0.008947 0.030543 0.01957
0.039617 0.01346 0.107268 0.001796
13-May 0.155529 -3.78E-05 0.070439 -0.0007 0.047366 0.022614 0.024684
-0.00973 0.088442 0.004269 0.114926 0.011779
13-Jun 0.144978 -0.00465 0.057574 -0.02281 0.108893 -0.00518 0.035611
0.00317 0.279286 -0.00842 0.1939 -0.00194
13-Jul 0.130431 0.011634 0.072031 0.014305 0.130615 0.003684 0.045301
0.003532 0.304674 0.010773 0.227933 0.012467
13-Aug 0.120132 -0.00195 0.068647 -0.00052 0.122781 0.001305 0.057854
-0.00476 0.311714 -0.00642 0.203608 0.017791
13-Sep 0.06575 -0.00545 0.080394 0.016555 0.119251 0.000583 0.084581
0.014775 0.297759 0.010611 0.250571 0.00938
13-Oct 0.047298 0.005285 0.120425 0.004647 0.101134 0.022547 0.067475
0.001918 0.150446 0.043009 0.145828 0.041185
13-Nov 0.058826 0.017242 0.109948 0.006176 0.071817 0.008307 0.062375
0.00121 0.053042 -0.00604 0.05076 0.011435
13-Dec 0.047956 -0.00802 0.103437 -0.00066 0.084003 0.009607 0.066285
0.004847 0.050402 0.000148 0.054412 0.006267
14-Jan 0.06039 -0.0104 0.107847 -0.01456 0.088995 0.008308 0.049891 0.004923
0.060003 0.00252 0.04058 0.008332
14-Feb 0.048697 0.016089 0.064049 -0.00105 0.095994 0.015907 0.063598
0.002466 0.062014 -0.00279 0.040226 0.009207
14-Mar 0.048803 0.012027 0.066596 -0.00688 0.092639 0.000271 0.071203
-0.00798 0.063202 0.016329 0.063374 0.010131
14-Apr 0.054393 -2.77E-05 0.068392 0.001996 0.100696 0.019122 0.07642
-0.0071 0.094228 0.004257 0.061417 0.005074
14-May 0.082217 -0.00517 0.06419 0.002143 0.074215 0.008081 0.029942
-0.01012 0.111502 -0.00381 0.165315 0.008571
14-Jun 0.159199 0.001468 0.064119 0.005526 0.096729 -0.0062 0.030144 0.01206
0.140136 -0.01303 0.339514 -0.00723
14-Jul 0.161122 -0.00064 0.055495 0.016062 0.086379 0.003641 0.032367
0.001952 0.136925 -0.00857 0.329419 0.014535
14-Aug 0.149222 -0.01024 0.065349 -6.26E-05 0.091117 0.010869 0.024955
-0.00442 0.128269 0.003591 0.296628 0.017301
14-Sep 0.150027 0.005866 0.059474 -0.02036 0.103996 0.002301 0.033988
-0.00461 0.136176 -0.00337 0.291533 -0.00818
14-Oct 0.073368 -0.04392 0.067435 0.008965 0.080492 -0.01163 0.020897
-0.01229 0.113464 -0.01768 0.133705 -0.23033
14-Nov 0.089288 -0.00557 0.084815 -0.00035 0.082332 -0.01346 0.032396
0.008304 0.124252 -0.01689 0.092065 -0.03321
14-Dec 0.094367 0.000647 0.142574 0.006715 0.054609 0.000543 0.047399
-0.00105 0.113318 -0.01409 0.067618 0.011339
15-Jan 0.084719 -0.00414 0.138934 0.01119 0.035142 0.005869 0.050001
0.010183 0.105044 -0.00425 0.047547 0.020317
15-Feb 0.081396 0.020125 0.11201 0.004226 0.038827 0.018429 0.064911
0.014608 0.093223 -0.00763 0.044216 0.005959
15-Mar 0.070694 0.005148 0.090721 0.000676 0.04484 -0.00662 0.068016
0.002282 0.071088 -0.0235 0.050961 -0.00289
15-Apr 0.046477 0.009808 0.039194 0.042848 0.051099 -0.00654 0.058671
0.004364 0.049306 0.005851 0.036681 0.021859
15-May 0.114129 -0.00053 0.04534 -0.00301 0.067418 -0.00534 0.080592
0.000444 0.04979 -0.01078 0.03599 0.000393
15-Jun 0.155868 0.000911 0.055488 -0.00909 0.110349 0.003204 0.09499
-0.00218 0.045165 0.000869 0.042706 -0.01044
15-Jul 0.143998 0.01204 0.055155 -0.00927 0.105175 -0.01308 0.082481
0.005026 0.048215 -0.00283 0.043197 0.0185
15-Aug 0.139796 -0.011 0.053131 -0.02828 0.112001 -0.00753 0.066901 -0.01171
0.040166 -0.01927 0.044748 -0.01313
15-Sep 0.13442 -0.04179 0.049929 -0.01784 0.125934 -0.00807 0.047968
-0.04907 0.082883 -0.00433 0.088255 -0.00258
15-Oct 0.058975 0.019236 0.096952 0.021717 0.116633 -0.00389 0.044177
0.023988 0.110388 -0.01154 0.087908 -0.02361
15-Nov 0.059629 -0.00198 0.097191 -0.00652 0.111808 -0.00195 0.035322
-0.00198 0.103403 0.006467 0.097863 -7.92E-05
15-Dec 0.065516 0.000394 0.107856 -0.00221 0.109277 -0.00446 0.045038
-0.00139 0.105924 -0.00173 0.094134 -0.00311
16-Jan 0.069967 -0.01648 0.124405 -0.02534 0.080609 -0.00782 0.045002
-0.03628 0.067507 -0.02547 0.061998 -0.03317
16-Feb 0.059642 -0.01092 0.093536 -0.04365 0.065717 -0.00534 0.033701
-0.01313 0.052069 0.012975 0.07676 0.00127
16-Mar 0.085488 0.009066 0.097588 0.00784 0.066972 -0.01077 0.035102
0.009299 0.041685 0.00368 0.097081 0.003638
16-Apr 0.077174 -0.02227 0.076671 -0.00295 0.067276 -0.00647 0.021694
0.005411 0.049655 0.005316 0.088254 0.022902
16-May 0.069195 -0.00497 0.067416 -0.00932 0.082775 0.004632 0.033202
-0.00077 0.196307 -0.01142 0.119456 -0.00425
16-Jun 0.097455 -0.0136 0.061482 -0.00162 0.129515 0.012956 0.029188
-0.00652 0.176014 -0.00397 0.125573 0.006204
16-Jul 0.104777 0.014497 0.045435 0.016457 0.100695 0.014244 0.036942
0.018711 0.177081 0.000672 0.09933 -0.00171
16-Aug 0.099126 0.008854 0.050532 0.008233 0.101403 -0.00082 0.040107
-0.00017 0.180629 -0.00393 0.0904 -0.00772
16-Sep 0.095867 0.001927 0.058721 0.006162 0.092854 -0.00179 0.032254
0.007411 0.082157 -0.02736 0.083736 0.006736
16-Oct 0.071165 -0.02676 0.055384 -0.00487 0.07185 0.014646 0.020497 0.00806
0.058031 -0.00201 0.06654 0.033218
16-Nov 0.046141 0.002952 0.063315 -0.00122 0.073631 0.011232 0.027677
-0.01164 0.057347 0.010014 0.056544 0.033138
16-Dec 0.066547 0.000778 0.091166 -0.00091 0.086377 0.02276 0.036674
0.002547 0.06248 0.01014 0.073954 0.008925
17-Jan 0.062046 -0.00505 0.070734 0.01391 0.073518 0.014533 0.049839
0.007438 0.06157 0.022513 0.069436 0.002046
17-Feb 0.048757 0.017769 0.063187 0.003425 0.079388 0.012774 0.056143
0.00427 0.058395 -0.00668 0.164802 -0.00184
17-Mar 0.042455 -0.00482 0.062797 0.002898 0.071547 -0.00202 0.062613
0.001129 0.072438 0.0018 0.237231 0.010924

[image: Mailtrack]
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
Sender
notified by
Mailtrack
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
05/08/20,
12:53:17 PM

On Fri, May 8, 2020 at 2:54 AM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Subhamitra,
> For some reason, your data didn't make it through. Maybe you tried to
> send an .xls or .xlsx file. If so, export it as CSV or if it's not too
> big, just paste the text into your email.
>
> Jim
>
> On Thu, May 7, 2020 at 10:30 PM Subhamitra Patra
> <subhamitra.patra at gmail.com> wrote:
> >
> > Dear R-users,
> >
> > I want to estimate ARCH test for multiple columns (i.e.,  from 2:21
> COLUMNS
> > ) in my data. For this purpose, I want to run a loop to calculate ARCH
> test
> > results for each column in the data frame. I tried by using for loop and
> > lapply function, but unable to write a loop for computing the ARCH test
> > simultaneously for each column (i.e., from 2:21 columns) of my data
> frame.
> >
> > Below is my ARCH test code which I want to estimate for multiple columns
> of
> > the data frame in a loop.
> >
> > library(tseries)
> >
> > library(FinTS)
> >
> > ArchTest (A, lags=1, demean = FALSE)
> >
> > Hence, A is a vector for which the ARCH test result is calculated. Here,
> I
> > want to write a loop so that the ArchTest can be calculated
> simultaneously
> > for each column of my data frame. From ARCH test result, I require only
> the
> > calculated Chi-square value and its p-value for each column that stored
> in
> > another matrix or object for each column as an output file.
> >
> > For your convenience, I attached my sample data below. Please find it.
> >
> > Please help me for which I shall be always grateful to you.
> >
> > Thank you.
> >
> > --
> > *Best Regards,*
> > *Subhamitra Patra*
> > *Phd. Research Scholar*
> > *Department of Humanities and Social Sciences*
> > *Indian Institute of Technology, Kharagpur*
> > *INDIA*
> >
> > [image: Mailtrack]
> > <
> https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&
> >
> > Sender
> > notified by
> > Mailtrack
> > <
> https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&
> >
> > 05/07/20,
> > 05:51:03 PM
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>


-- 
*Best Regards,*
*Subhamitra Patra*
*Phd. Research Scholar*
*Department of Humanities and Social Sciences*
*Indian Institute of Technology, Kharagpur*
*INDIA*

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Fri May  8 11:07:37 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 8 May 2020 19:07:37 +1000
Subject: [R] 
 [R ] Writing loop to estimate ARCH test for a multiple columns
 of a data frame?
In-Reply-To: <CAOFE=kOtwzrr3cFXhvvS+db+iw=p4eg1Fm6JCGtCzwessQ90FA@mail.gmail.com>
References: <CAOFE=kMaSgqy7_dMsB3WtTgN+BDa4b5m28GO0uxm0rUsDmqc9g@mail.gmail.com>
 <CA+8X3fWOMFQ8Ptz75j8DjhJuaEKi8nR1p+82LeNqTcB2NfSjKA@mail.gmail.com>
 <CAOFE=kOtwzrr3cFXhvvS+db+iw=p4eg1Fm6JCGtCzwessQ90FA@mail.gmail.com>
Message-ID: <CA+8X3fUB8Mp9Pn8RO7PB3knoEgLHEdT6LuwxyxPbNMvQNoVG5A@mail.gmail.com>

Hi Subhamitra,
This isn't too hard:

# read in the sample data that was
# saved in the file "sp_8_5.tab"
sp_8_5<-read.table("sp_8_5.tab",sep="\t",
 header=TRUE,stringsAsFactors=FALSE)
library(tseries)
library(FinTS)
# using "sapply", run the test on each column
spout<-sapply(sp_8_5[,2:12],ArchTest)

The list "spout" contains the test results. If you really want to use a
loop:

spout<-list()
for(i in 2:12) spout[[i-1]]<-ArchTest(sp_8_5[,i])

Jim


On Fri, May 8, 2020 at 5:27 PM Subhamitra Patra <subhamitra.patra at gmail.com>
wrote:

> Dear Sir,
>
> Herewith I am pasting a part of my sample data having 12 columns below,
> and want to calculate ARCH test for the 12 columns by using a loop.
>
>

	[[alternative HTML version deleted]]


From @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com  Fri May  8 12:23:46 2020
From: @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com (Subhamitra Patra)
Date: Fri, 8 May 2020 15:53:46 +0530
Subject: [R] 
 [R ] Writing loop to estimate ARCH test for a multiple columns
 of a data frame?
In-Reply-To: <CA+8X3fUB8Mp9Pn8RO7PB3knoEgLHEdT6LuwxyxPbNMvQNoVG5A@mail.gmail.com>
References: <CAOFE=kMaSgqy7_dMsB3WtTgN+BDa4b5m28GO0uxm0rUsDmqc9g@mail.gmail.com>
 <CA+8X3fWOMFQ8Ptz75j8DjhJuaEKi8nR1p+82LeNqTcB2NfSjKA@mail.gmail.com>
 <CAOFE=kOtwzrr3cFXhvvS+db+iw=p4eg1Fm6JCGtCzwessQ90FA@mail.gmail.com>
 <CA+8X3fUB8Mp9Pn8RO7PB3knoEgLHEdT6LuwxyxPbNMvQNoVG5A@mail.gmail.com>
Message-ID: <CAOFE=kNNFOy=GCSWidG+Fds4qi5r7w9M9SafZ+oqcQw-xeKmkg@mail.gmail.com>

Dear Sir,

Thank you very much for such an excellent solution to my problem. I was
trying sapply function since last days, but was really unable to write
properly. Now, I understood my mistake in using sapply function in the
code. Therefore, I have two queries regarding this which I want to discuss
here just for my learning purpose.

1. While using sapply function for estimating one method across the columns
of a data frame, one needs to define the list of the output table after
using sapply so that the test results for each column will be consistently
stored in an output object, right?

2. In the spout<- list() command, what spout[[i-1]]  indicates?

Sir, one more possibility which I would like to ask related to my above
problem just to learn for further R programming language.

After running your suggested code, all the results for each column are
being stored in the spout object. From this, I need only the statistics and
P-value for each column. So, my queries are:

1. Is there any way to extract only two values (i.e., statistics and
p-value) for each column that stored in spout object and save these two
values in another R data frame for each column?
 or
2. Is there any possibility that the statistics and p-value calculated for
each column can directly export to a word file in a table format (having 4
columns and 3 rows). In particular, is it possible to extract both
statistic and p-value results for each column to an MS word file with the
format of A1, A2, A3, A4 column results in 1st row, A5, A6, A7, A8 column
results in 2nd row, and A9, A10, A11, A12 column results in the 3rd row of
the table?


Like before, your suggestion will definitely help me to learn the advanced
R language.

Thank you very much for your help.

[image: Mailtrack]
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
Sender
notified by
Mailtrack
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
05/08/20,
03:47:26 PM

On Fri, May 8, 2020 at 2:37 PM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Subhamitra,
> This isn't too hard:
>
> # read in the sample data that was
> # saved in the file "sp_8_5.tab"
> sp_8_5<-read.table("sp_8_5.tab",sep="\t",
>  header=TRUE,stringsAsFactors=FALSE)
> library(tseries)
> library(FinTS)
> # using "sapply", run the test on each column
> spout<-sapply(sp_8_5[,2:12],ArchTest)
>
> The list "spout" contains the test results. If you really want to use a
> loop:
>
> spout<-list()
> for(i in 2:12) spout[[i-1]]<-ArchTest(sp_8_5[,i])
>
> Jim
>
>
> On Fri, May 8, 2020 at 5:27 PM Subhamitra Patra <
> subhamitra.patra at gmail.com> wrote:
>
>> Dear Sir,
>>
>> Herewith I am pasting a part of my sample data having 12 columns below,
>> and want to calculate ARCH test for the 12 columns by using a loop.
>>
>>

-- 
*Best Regards,*
*Subhamitra Patra*
*Phd. Research Scholar*
*Department of Humanities and Social Sciences*
*Indian Institute of Technology, Kharagpur*
*INDIA*

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Fri May  8 12:32:11 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 8 May 2020 20:32:11 +1000
Subject: [R] Adding overlap legend to a histogram
In-Reply-To: <CAC8ss30gDSqkYtSTO-v_H+Z4XJq4Zn2oZ3uCzhnnYREC0fJPtg@mail.gmail.com>
References: <CAC8ss30Av_LoM4eYh9TsPhEnpUWG4nDxGduoDiREiuQDmpL+sQ@mail.gmail.com>
 <CA+8X3fUZOcmHjJ=E5yLG5j+266US+vFgaYJS1CPR0ofJDZ8fpw@mail.gmail.com>
 <CAC8ss30gDSqkYtSTO-v_H+Z4XJq4Zn2oZ3uCzhnnYREC0fJPtg@mail.gmail.com>
Message-ID: <CA+8X3fUL9UP=r-pzQwHRVrQMgtyZrSUVXrPhVpRT=O=S2Wqn8A@mail.gmail.com>

Hi Ogbos,
While this solution is not entirely correct, I think it is a start.
First I took your data files and made them "sourceable" by adding
"FD[1|2]<-" at the top and renaming them "FD1.R" and "FD2.R". Running
the following code produces something that is at least close to what
you want. The code is fairly well commented so it should be easy to
see what I have done:

# read in the FD1 data
source("FD1.R")
# read in the FD2 data
source("FD2.R")
# convert year-month-day columns to dates
FD1$data.year<-FD1$data.year+ifelse(FD1$data.year < 50,2000,1900)
FD1$date<-as.Date(paste(FD1$data.year,FD1$data.month,FD1$data.day,sep="-"),
 format="%Y-%m-%d")
FD2$data.year<-FD2$data.year+ifelse(FD2$data.year < 50,2000,1900)
FD2$date<-as.Date(paste(FD2$data.year,FD2$data.month,FD2$data.day,sep="-"),
 format="%Y-%m-%d")
# check the ranges for overlap
range(FD1$date)
range(FD2$date)
# get the overall range of the plot
xlim<-range(c(FD1$date,FD2$date))
# FD1 spans the date range so xlim is not really needed here
# now get the counts for each data set
FD1counts<-as.vector(table(cut(FD1$date,breaks="years")))
# FD2 is missing 1996, 1997 and 2016 so add zeros at the beginning and end
FD2counts<-c(0,0,as.vector(table(cut(FD2$date,breaks="years"))),0)
# set up the bar colors
barcol<-matrix(c(rep("red",2),rep("blue",18),"red",
 rep("red",2),rep("green",18),"red"),nrow=2,byrow=TRUE)
# use barp as barplot can't do the colors
library(plotrix)
barp(rbind(FD1counts,FD2counts),names.arg=1996:2016,
 main="Observation counts for FD1 and FD2",
 xlab="Year",ylab="Observations",col=barcol)
legend(12,80,c("FD1 only","FD1 & FD2","FD2 & FD1"),
 fill=c("red","blue","green"))

This shows the overlap in blue and green. You can make the overlap
colors whatever you like. It doesn't account for the fact that FD2
only overlaps for part of a year on both ends. You may not be worried
about this.

Jim

On Fri, May 8, 2020 at 4:07 PM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>
> Dear Jim,
> Thank you for looking into this.
> Sorry, there was actually no overlap in the small part of the data I reported. My error of omission.
>
> So when I run my full data with the adjustment you made, I got some thing that was far from what I was expecting. That tell me that I need to send the complete data to enable you correctly adjust the code, especially in the light of the missing/present overlap.
> I have used deput function to attach the two files. Please use any symbol to depict the color/mark/legend of the overlap dates (just to enable the reader visualize what is going on). I am actually trying to display event frequency/occurrence per year.
>
> Thank you and warmest regards
> Ogbos
>
> On Fri, May 8, 2020 at 1:13 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>>
>> Hi Ogbos,
>> I don't think that your example allows us to work out what you are
>> trying to do. For one thing, "x1" and "x2" do not overlap. Also, you
>> are plotting the frequencies of dates of observations, which may not
>> be what you want.
>> The following code will correctly display your example:
>>
>> hist(x1,breaks="years",freq=T,axes=F,xlim=c(9400,11100),col=c1a)
>> hist(x2,breaks="years",freq=T,axes=F,add=T,col=c2a)
>>  axis.Date(1, at=seq(as.Date(min(x1)), as.Date(max(x2)), by="years"))
>>  axis(2)
>> legend("topleft", c("AUTO", "MANUAL"), fill=c("red", "blue"))
>>
>> What it is displaying is the frequency of observations in your two
>> vectors by calendar year. If this is what you want, and you can
>> explain how you would like "overlap" to be displayed, we can probably
>> provide better help.
>>
>> Jim
>>
>>
>> On Fri, May 8, 2020 at 7:01 AM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>> >
>> > Dear Experts,
>> > Greetings.
>> >
>> > I am trying to display two datasets in a histogram. I have been able to
>> > plot the graph and added the legend for the two colors. I am, however,
>> > having difficulties adding a legend to represent the regions of overlap
>> > (the third legend).  Below are my data and code.


From drj|m|emon @end|ng |rom gm@||@com  Fri May  8 13:17:06 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 8 May 2020 21:17:06 +1000
Subject: [R] 
 [R ] Writing loop to estimate ARCH test for a multiple columns
 of a data frame?
In-Reply-To: <CAOFE=kNNFOy=GCSWidG+Fds4qi5r7w9M9SafZ+oqcQw-xeKmkg@mail.gmail.com>
References: <CAOFE=kMaSgqy7_dMsB3WtTgN+BDa4b5m28GO0uxm0rUsDmqc9g@mail.gmail.com>
 <CA+8X3fWOMFQ8Ptz75j8DjhJuaEKi8nR1p+82LeNqTcB2NfSjKA@mail.gmail.com>
 <CAOFE=kOtwzrr3cFXhvvS+db+iw=p4eg1Fm6JCGtCzwessQ90FA@mail.gmail.com>
 <CA+8X3fUB8Mp9Pn8RO7PB3knoEgLHEdT6LuwxyxPbNMvQNoVG5A@mail.gmail.com>
 <CAOFE=kNNFOy=GCSWidG+Fds4qi5r7w9M9SafZ+oqcQw-xeKmkg@mail.gmail.com>
Message-ID: <CA+8X3fXcztWSqhUiKAS+uc+fH57RCV7jN5ihLeRbz5rUgkp28g@mail.gmail.com>

1) In general, *apply functions return a list with the number of elements
equal to the number of columns or other elements of the input data. You can
assign that list as I have to "spout" in the first example.

2) spout<-list() assigns the name "spout" to an empty list. As we are
processing columns 2 to 12 of the input data, spout[[i-1]] assigns the
results to elements 1 to 11 of the list "spout". Just a low trick.

1a) Yes, you can create a "wrapper" function that will return only the
statistic and p.value.

# create a function that returns only the
# statistic and p.value as a string
archStatP<-function(x) {
 archout<-ArchTest(x)
 return(sprintf("ChiSq = %f, p = %f",archout$statistic,archout$p.value))
}
# using "lapply", run the test on each column
spout<-lapply(sp_8_5[,2:12],archStatP)

Note that I should have used "lapply". I didn't check the output carefully
enough.

2a) Now you only have to separate the strings in "spout" with TAB
characters and import the result into Excel. I have to wash the dishes, so
you're on your own.

Jim

On Fri, May 8, 2020 at 8:26 PM Subhamitra Patra <subhamitra.patra at gmail.com>
wrote:

> Dear Sir,
>
> Thank you very much for such an excellent solution to my problem. I was
> trying sapply function since last days, but was really unable to write
> properly. Now, I understood my mistake in using sapply function in the
> code. Therefore, I have two queries regarding this which I want to discuss
> here just for my learning purpose.
>
> 1. While using sapply function for estimating one method across the
> columns of a data frame, one needs to define the list of the output table
> after using sapply so that the test results for each column will be
> consistently stored in an output object, right?
>
> 2. In the spout<- list() command, what spout[[i-1]]  indicates?
>
> Sir, one more possibility which I would like to ask related to my above
> problem just to learn for further R programming language.
>
> After running your suggested code, all the results for each column are
> being stored in the spout object. From this, I need only the statistics and
> P-value for each column. So, my queries are:
>
> 1. Is there any way to extract only two values (i.e., statistics and
> p-value) for each column that stored in spout object and save these two
> values in another R data frame for each column?
>  or
> 2. Is there any possibility that the statistics and p-value calculated for
> each column can directly export to a word file in a table format (having 4
> columns and 3 rows). In particular, is it possible to extract both
> statistic and p-value results for each column to an MS word file with the
> format of A1, A2, A3, A4 column results in 1st row, A5, A6, A7, A8 column
> results in 2nd row, and A9, A10, A11, A12 column results in the 3rd row of
> the table?
>
>
> Like before, your suggestion will definitely help me to learn the advanced
> R language.
>
> Thank you very much for your help.
>
> [image: Mailtrack]
> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
> notified by
> Mailtrack
> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 05/08/20,
> 03:47:26 PM
>
> On Fri, May 8, 2020 at 2:37 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>
>> Hi Subhamitra,
>> This isn't too hard:
>>
>> # read in the sample data that was
>> # saved in the file "sp_8_5.tab"
>> sp_8_5<-read.table("sp_8_5.tab",sep="\t",
>>  header=TRUE,stringsAsFactors=FALSE)
>> library(tseries)
>> library(FinTS)
>> # using "sapply", run the test on each column
>> spout<-sapply(sp_8_5[,2:12],ArchTest)
>>
>> The list "spout" contains the test results. If you really want to use a
>> loop:
>>
>> spout<-list()
>> for(i in 2:12) spout[[i-1]]<-ArchTest(sp_8_5[,i])
>>
>> Jim
>>
>>
>> On Fri, May 8, 2020 at 5:27 PM Subhamitra Patra <
>> subhamitra.patra at gmail.com> wrote:
>>
>>> Dear Sir,
>>>
>>> Herewith I am pasting a part of my sample data having 12 columns below,
>>> and want to calculate ARCH test for the 12 columns by using a loop.
>>>
>>>
>
> --
> *Best Regards,*
> *Subhamitra Patra*
> *Phd. Research Scholar*
> *Department of Humanities and Social Sciences*
> *Indian Institute of Technology, Kharagpur*
> *INDIA*
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri May  8 14:07:08 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 8 May 2020 13:07:08 +0100
Subject: [R] Warning in install.packages : converting NULL pointer to R NULL
Message-ID: <bbd798c8-eaa7-1b8d-cb39-7e8926600520@sapo.pt>

Hello,

R 4.0.0 on Ubuntu 20.04, sessionInfo() below.

Since I updated to R 4.0 that every time I try to install a package with 
install.packages() the warning in the title shows up at the end, be the 
installation successful or not. If it is successful, the package loads 
with no problems, so I'm not very worried but it isn't normal (expected) 
behavior, is it?

Here is a run of install.packages().


install.packages('cowplot')
Installing package into ?/usr/local/lib/R/site-library?
(as ?lib? is unspecified)
trying URL 'https://cloud.r-project.org/src/contrib/cowplot_1.0.0.tar.gz'
Content type 'application/x-gzip' length 1275585 bytes (1.2 MB)
==================================================
downloaded 1.2 MB

* installing *source* package ?cowplot? ...
** package ?cowplot? successfully unpacked and MD5 sums checked
** using staged installation
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
*** copying figures
** building package indices
** installing vignettes
** testing if installed package can be loaded from temporary location
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation 
path
* DONE (cowplot)

The downloaded source packages are in
	?/tmp/Rtmp9NXQkt/downloaded_packages?
Warning in install.packages :
   converting NULL pointer to R NULL


Also, I'm running this on RStudio and haven't changed the R library 
directory.


sessionInfo()
R version 4.0.0 (2020-04-24)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 20.04 LTS

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0
LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0

locale:
  [1] LC_CTYPE=pt_PT.UTF-8       LC_NUMERIC=C 
LC_TIME=pt_PT.UTF-8
  [4] LC_COLLATE=pt_PT.UTF-8     LC_MONETARY=pt_PT.UTF-8 
LC_MESSAGES=pt_PT.UTF-8
  [7] LC_PAPER=pt_PT.UTF-8       LC_NAME=C                  LC_ADDRESS=C 

[10] LC_TELEPHONE=C             LC_MEASUREMENT=pt_PT.UTF-8 
LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] MASS_7.3-51.6  ggthemes_4.2.0 ggrepel_0.8.2  dplyr_0.8.5 
ggplot2_3.3.0

loaded via a namespace (and not attached):
  [1] zoo_1.8-8          tidyselect_1.0.0   purrr_0.3.4 
reshape2_1.4.4     haven_2.2.0
  [6] lattice_0.20-41    sodium_1.1         carData_3.0-3 
colorspace_1.4-1   vctrs_0.2.4
[11] yaml_2.2.1         rlang_0.4.6        pillar_1.4.3 
withr_2.2.0        foreign_0.8-79
[16] glue_1.4.0         readxl_1.3.1       lifecycle_0.2.0    plyr_1.8.6 
         stringr_1.4.0
[21] MatrixModels_0.4-1 munsell_0.5.0      gtable_0.3.0 
cellranger_1.1.0   zip_2.0.4
[26] rio_0.5.16         forcats_0.5.0      SparseM_1.78 
quantreg_5.55      curl_4.3
[31] tis_1.38           Rcpp_1.0.4.6       readr_1.3.1 
scales_1.1.0       abind_1.4-5
[36] farver_2.0.3       sos_2.0-0          brew_1.0-6 
digest_0.6.25      hms_0.5.3
[41] png_0.1-7          stringi_1.4.6      openxlsx_4.1.4     grid_4.0.0 
         tools_4.0.0
[46] magrittr_1.5       tibble_3.0.1       pacman_0.5.1 
crayon_1.3.4       car_3.0-7
[51] pkgconfig_2.0.3    ellipsis_0.3.0     Matrix_1.2-18 
data.table_1.12.8  assertthat_0.2.1
[56] httr_1.4.1         rstudioapi_0.11    R6_2.4.1 
compiler_4.0.0


Thanks in advance,

Rui Barradas


From mehd|d@dkh@h91 @end|ng |rom gm@||@com  Fri May  8 14:26:14 2020
From: mehd|d@dkh@h91 @end|ng |rom gm@||@com (Mehdi Dadkhah)
Date: Fri, 8 May 2020 16:56:14 +0430
Subject: [R] Question about topic modelling
Message-ID: <CAGN=ytPU_WAtzC6XDO_MMePRrtEn1TL+N-kmA+E_76KkNza1kA@mail.gmail.com>

Hi,
I hope you are doing well!
I have a question about topic modeling. Please consider summarized steps
for making a LDA (Latent Direchlet Allocation) model:
1-importing data
2-making a corpus.
3-pre-processing and cleaning data
4-making term document matrix
5-Apply LDA in topicmodel package.
During mentioned steps, should i convert my data to Tidy format using
unnest_tokens or not?

Many thanks!
With best regards,

-- 
*Mehdi Dadkhah*

	[[alternative HTML version deleted]]


From mehd|d@dkh@h91 @end|ng |rom gm@||@com  Fri May  8 14:31:54 2020
From: mehd|d@dkh@h91 @end|ng |rom gm@||@com (Mehdi Dadkhah)
Date: Fri, 8 May 2020 17:01:54 +0430
Subject: [R] Question about package "SentimentAnalysis"
In-Reply-To: <CAGN=ytPU_WAtzC6XDO_MMePRrtEn1TL+N-kmA+E_76KkNza1kA@mail.gmail.com>
References: <CAGN=ytPU_WAtzC6XDO_MMePRrtEn1TL+N-kmA+E_76KkNza1kA@mail.gmail.com>
Message-ID: <CAGN=ytNHHmuP3Jn_6UAaaN9c3HQCDuEMG6JYRc4OeKjs+4_8Uw@mail.gmail.com>

Hi,
I hope you are doing well!
I read a vignette (
https://cran.r-project.org/web/packages/SentimentAnalysis/vignettes/SentimentAnalysis.html)
about interested package, "SentimentAnalysis". But i faced with a question.
In mentioned  vignette, the sentiment has been applied on a sentence or
multiple sentences separately. Can this package calculate sentiment
direction/score for a long texts?
for example:

# Create a vector of strings
documents <- "Wow, I really like the new light sabers!That book was
excellent.R is a fantastic language.The service in this restaurant was
miserable.This is neither positive or negative."

# Analyze sentiment
sentiment <- analyzeSentiment(documents)

Many thanks!
With best regards,

-- 
*Mehdi Dadkhah*

	[[alternative HTML version deleted]]


From g||ted|||e2014 @end|ng |rom gm@||@com  Fri May  8 14:33:12 2020
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Fri, 8 May 2020 13:33:12 +0100
Subject: [R] Adding overlap legend to a histogram: FIXED!!!!!!!!!!!!
In-Reply-To: <CA+8X3fUL9UP=r-pzQwHRVrQMgtyZrSUVXrPhVpRT=O=S2Wqn8A@mail.gmail.com>
References: <CAC8ss30Av_LoM4eYh9TsPhEnpUWG4nDxGduoDiREiuQDmpL+sQ@mail.gmail.com>
 <CA+8X3fUZOcmHjJ=E5yLG5j+266US+vFgaYJS1CPR0ofJDZ8fpw@mail.gmail.com>
 <CAC8ss30gDSqkYtSTO-v_H+Z4XJq4Zn2oZ3uCzhnnYREC0fJPtg@mail.gmail.com>
 <CA+8X3fUL9UP=r-pzQwHRVrQMgtyZrSUVXrPhVpRT=O=S2Wqn8A@mail.gmail.com>
Message-ID: <CAC8ss31GFH4BU4PNpvTedE1ow=vLy4A6hHyVb=P_LdohSTGWuA@mail.gmail.com>

Dear Jim,
This is too great!!! I nearly got lost as I struggle to compare my data
with the graph.  I have to use a coincident algorithm to compare the two
datasets with the histogram before I begin to understand what is going on.

Thank you for giving me more than I requested/expected!!! This is the kind
of confusing/complicating diagrams that best suit some reviewers.

I am yet at the verge of understanding the entire results and will surely
return with some more queries as I progress.

Warmest regards
Ogbos

On Fri, May 8, 2020 at 11:32 AM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Ogbos,
> While this solution is not entirely correct, I think it is a start.
> First I took your data files and made them "sourceable" by adding
> "FD[1|2]<-" at the top and renaming them "FD1.R" and "FD2.R". Running
> the following code produces something that is at least close to what
> you want. The code is fairly well commented so it should be easy to
> see what I have done:
>
> # read in the FD1 data
> source("FD1.R")
> # read in the FD2 data
> source("FD2.R")
> # convert year-month-day columns to dates
> FD1$data.year<-FD1$data.year+ifelse(FD1$data.year < 50,2000,1900)
> FD1$date<-as.Date(paste(FD1$data.year,FD1$data.month,FD1$data.day,sep="-"),
>  format="%Y-%m-%d")
> FD2$data.year<-FD2$data.year+ifelse(FD2$data.year < 50,2000,1900)
> FD2$date<-as.Date(paste(FD2$data.year,FD2$data.month,FD2$data.day,sep="-"),
>  format="%Y-%m-%d")
> # check the ranges for overlap
> range(FD1$date)
> range(FD2$date)
> # get the overall range of the plot
> xlim<-range(c(FD1$date,FD2$date))
> # FD1 spans the date range so xlim is not really needed here
> # now get the counts for each data set
> FD1counts<-as.vector(table(cut(FD1$date,breaks="years")))
> # FD2 is missing 1996, 1997 and 2016 so add zeros at the beginning and end
> FD2counts<-c(0,0,as.vector(table(cut(FD2$date,breaks="years"))),0)
> # set up the bar colors
> barcol<-matrix(c(rep("red",2),rep("blue",18),"red",
>  rep("red",2),rep("green",18),"red"),nrow=2,byrow=TRUE)
> # use barp as barplot can't do the colors
> library(plotrix)
> barp(rbind(FD1counts,FD2counts),names.arg=1996:2016,
>  main="Observation counts for FD1 and FD2",
>  xlab="Year",ylab="Observations",col=barcol)
> legend(12,80,c("FD1 only","FD1 & FD2","FD2 & FD1"),
>  fill=c("red","blue","green"))
>
> This shows the overlap in blue and green. You can make the overlap
> colors whatever you like. It doesn't account for the fact that FD2
> only overlaps for part of a year on both ends. You may not be worried
> about this.
>
> Jim
>
> On Fri, May 8, 2020 at 4:07 PM Ogbos Okike <giftedlife2014 at gmail.com>
> wrote:
> >
> > Dear Jim,
> > Thank you for looking into this.
> > Sorry, there was actually no overlap in the small part of the data I
> reported. My error of omission.
> >
> > So when I run my full data with the adjustment you made, I got some
> thing that was far from what I was expecting. That tell me that I need to
> send the complete data to enable you correctly adjust the code, especially
> in the light of the missing/present overlap.
> > I have used deput function to attach the two files. Please use any
> symbol to depict the color/mark/legend of the overlap dates (just to enable
> the reader visualize what is going on). I am actually trying to display
> event frequency/occurrence per year.
> >
> > Thank you and warmest regards
> > Ogbos
> >
> > On Fri, May 8, 2020 at 1:13 AM Jim Lemon <drjimlemon at gmail.com> wrote:
> >>
> >> Hi Ogbos,
> >> I don't think that your example allows us to work out what you are
> >> trying to do. For one thing, "x1" and "x2" do not overlap. Also, you
> >> are plotting the frequencies of dates of observations, which may not
> >> be what you want.
> >> The following code will correctly display your example:
> >>
> >> hist(x1,breaks="years",freq=T,axes=F,xlim=c(9400,11100),col=c1a)
> >> hist(x2,breaks="years",freq=T,axes=F,add=T,col=c2a)
> >>  axis.Date(1, at=seq(as.Date(min(x1)), as.Date(max(x2)), by="years"))
> >>  axis(2)
> >> legend("topleft", c("AUTO", "MANUAL"), fill=c("red", "blue"))
> >>
> >> What it is displaying is the frequency of observations in your two
> >> vectors by calendar year. If this is what you want, and you can
> >> explain how you would like "overlap" to be displayed, we can probably
> >> provide better help.
> >>
> >> Jim
> >>
> >>
> >> On Fri, May 8, 2020 at 7:01 AM Ogbos Okike <giftedlife2014 at gmail.com>
> wrote:
> >> >
> >> > Dear Experts,
> >> > Greetings.
> >> >
> >> > I am trying to display two datasets in a histogram. I have been able
> to
> >> > plot the graph and added the legend for the two colors. I am, however,
> >> > having difficulties adding a legend to represent the regions of
> overlap
> >> > (the third legend).  Below are my data and code.
>

	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Fri May  8 14:56:46 2020
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Fri, 8 May 2020 08:56:46 -0400
Subject: [R] 
 Warning in install.packages : converting NULL pointer to R NULL
In-Reply-To: <bbd798c8-eaa7-1b8d-cb39-7e8926600520@sapo.pt>
References: <bbd798c8-eaa7-1b8d-cb39-7e8926600520@sapo.pt>
Message-ID: <a835fb6e-b229-1460-0ce5-107594ba3f5b@gmail.com>

That looks like an RStudio message.  Do you get it if you run 
install.packages() in command line R?

Duncan Murdoch

On 08/05/2020 8:07 a.m., Rui Barradas wrote:
> Hello,
> 
> R 4.0.0 on Ubuntu 20.04, sessionInfo() below.
> 
> Since I updated to R 4.0 that every time I try to install a package with
> install.packages() the warning in the title shows up at the end, be the
> installation successful or not. If it is successful, the package loads
> with no problems, so I'm not very worried but it isn't normal (expected)
> behavior, is it?
> 
> Here is a run of install.packages().
> 
> 
> install.packages('cowplot')
> Installing package into ?/usr/local/lib/R/site-library?
> (as ?lib? is unspecified)
> trying URL 'https://cloud.r-project.org/src/contrib/cowplot_1.0.0.tar.gz'
> Content type 'application/x-gzip' length 1275585 bytes (1.2 MB)
> ==================================================
> downloaded 1.2 MB
> 
> * installing *source* package ?cowplot? ...
> ** package ?cowplot? successfully unpacked and MD5 sums checked
> ** using staged installation
> ** R
> ** inst
> ** byte-compile and prepare package for lazy loading
> ** help
> *** installing help indices
> *** copying figures
> ** building package indices
> ** installing vignettes
> ** testing if installed package can be loaded from temporary location
> ** testing if installed package can be loaded from final location
> ** testing if installed package keeps a record of temporary installation
> path
> * DONE (cowplot)
> 
> The downloaded source packages are in
> 	?/tmp/Rtmp9NXQkt/downloaded_packages?
> Warning in install.packages :
>     converting NULL pointer to R NULL
> 
> 
> Also, I'm running this on RStudio and haven't changed the R library
> directory.
> 
> 
> sessionInfo()
> R version 4.0.0 (2020-04-24)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Ubuntu 20.04 LTS
> 
> Matrix products: default
> BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0
> LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0
> 
> locale:
>    [1] LC_CTYPE=pt_PT.UTF-8       LC_NUMERIC=C
> LC_TIME=pt_PT.UTF-8
>    [4] LC_COLLATE=pt_PT.UTF-8     LC_MONETARY=pt_PT.UTF-8
> LC_MESSAGES=pt_PT.UTF-8
>    [7] LC_PAPER=pt_PT.UTF-8       LC_NAME=C                  LC_ADDRESS=C
> 
> [10] LC_TELEPHONE=C             LC_MEASUREMENT=pt_PT.UTF-8
> LC_IDENTIFICATION=C
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> other attached packages:
> [1] MASS_7.3-51.6  ggthemes_4.2.0 ggrepel_0.8.2  dplyr_0.8.5
> ggplot2_3.3.0
> 
> loaded via a namespace (and not attached):
>    [1] zoo_1.8-8          tidyselect_1.0.0   purrr_0.3.4
> reshape2_1.4.4     haven_2.2.0
>    [6] lattice_0.20-41    sodium_1.1         carData_3.0-3
> colorspace_1.4-1   vctrs_0.2.4
> [11] yaml_2.2.1         rlang_0.4.6        pillar_1.4.3
> withr_2.2.0        foreign_0.8-79
> [16] glue_1.4.0         readxl_1.3.1       lifecycle_0.2.0    plyr_1.8.6
>           stringr_1.4.0
> [21] MatrixModels_0.4-1 munsell_0.5.0      gtable_0.3.0
> cellranger_1.1.0   zip_2.0.4
> [26] rio_0.5.16         forcats_0.5.0      SparseM_1.78
> quantreg_5.55      curl_4.3
> [31] tis_1.38           Rcpp_1.0.4.6       readr_1.3.1
> scales_1.1.0       abind_1.4-5
> [36] farver_2.0.3       sos_2.0-0          brew_1.0-6
> digest_0.6.25      hms_0.5.3
> [41] png_0.1-7          stringi_1.4.6      openxlsx_4.1.4     grid_4.0.0
>           tools_4.0.0
> [46] magrittr_1.5       tibble_3.0.1       pacman_0.5.1
> crayon_1.3.4       car_3.0-7
> [51] pkgconfig_2.0.3    ellipsis_0.3.0     Matrix_1.2-18
> data.table_1.12.8  assertthat_0.2.1
> [56] httr_1.4.1         rstudioapi_0.11    R6_2.4.1
> compiler_4.0.0
> 
> 
> Thanks in advance,
> 
> Rui Barradas
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com  Fri May  8 15:25:20 2020
From: @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com (Subhamitra Patra)
Date: Fri, 8 May 2020 18:55:20 +0530
Subject: [R] 
 [R ] Writing loop to estimate ARCH test for a multiple columns
 of a data frame?
In-Reply-To: <CA+8X3fXcztWSqhUiKAS+uc+fH57RCV7jN5ihLeRbz5rUgkp28g@mail.gmail.com>
References: <CAOFE=kMaSgqy7_dMsB3WtTgN+BDa4b5m28GO0uxm0rUsDmqc9g@mail.gmail.com>
 <CA+8X3fWOMFQ8Ptz75j8DjhJuaEKi8nR1p+82LeNqTcB2NfSjKA@mail.gmail.com>
 <CAOFE=kOtwzrr3cFXhvvS+db+iw=p4eg1Fm6JCGtCzwessQ90FA@mail.gmail.com>
 <CA+8X3fUB8Mp9Pn8RO7PB3knoEgLHEdT6LuwxyxPbNMvQNoVG5A@mail.gmail.com>
 <CAOFE=kNNFOy=GCSWidG+Fds4qi5r7w9M9SafZ+oqcQw-xeKmkg@mail.gmail.com>
 <CA+8X3fXcztWSqhUiKAS+uc+fH57RCV7jN5ihLeRbz5rUgkp28g@mail.gmail.com>
Message-ID: <CAOFE=kO6Pz=vqOR6+mBvjkMMBMhv9bX9s9nUCRnBoF9dGkdiVA@mail.gmail.com>

Dear Sir,

Thank you very much for your wonderful suggestion for my problem. Your
suggested code has excellently worked and successfully extracted the
statistics and p-value in another R object.

Concerning your last suggestion, I attempted to separate the strings with
TAB character in the "spout" object by using different alternative packages
like dplyr, tidyr, qdap, ans also by using split,strsplit function so that
can export the statistics and p-values for each column to excel, and later
to the MSword file, but got the below error.

By using the  split function, I wrote the code as,
*string[] split = s.Split(spout, '\t')*
where I got the following errors.
Error: unexpected symbol in "string[] split"
Error: unexpected symbol in "string[[]]split"
Error in strsplit(row, "\t") : non-character argument

Then I tried with  strsplit function by the below code
*strsplit(spout, split)*
But, got the below error as
Error in as.character(split) :
  cannot coerce type 'closure' to vector of type 'character'.

Then used dplyr and tidyr package and the wrote the below code
library(dplyr)
library(tidyr)
*separate(spout,value,into=c(?ChiSq?,?p?),sep=?,?)*
*separate(spout,List of length 12,into=c(?ChiSq?,?p?),sep="\t")*
But, got the errors as,
Error: unexpected input in "separate(spout,value,into=c(?"
Error: unexpected symbol in "separate(spout,List of"

Then used qdap package with the code below

*colsplit2df(spout,, c("ChiSq", "p"), ",")*
*colsplit2df(spout,, c("ChiSq", "p"), sep = "\t")*
But got the following errors
Error in dataframe[, splitcol] : incorrect number of dimensions
In addition: Warning message:
In colsplit2df_helper(dataframe = dataframe, splitcol = splitcols[i],  :
  dataframe object is not of the class data.frame
Error in dataframe[, splitcol] : incorrect number of dimensions
In addition: Warning message:
In colsplit2df_helper(dataframe = dataframe, splitcol = splitcols[i],  :
  dataframe object is not of the class data.frame

Sir, please suggest me where I am going wrong in the above to separate
string in the "spout" object.

Thank you very much for your help.

[image: Mailtrack]
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
Sender
notified by
Mailtrack
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
05/08/20,
06:51:46 PM

On Fri, May 8, 2020 at 4:47 PM Jim Lemon <drjimlemon at gmail.com> wrote:

> 1) In general, *apply functions return a list with the number of elements
> equal to the number of columns or other elements of the input data. You can
> assign that list as I have to "spout" in the first example.
>
> 2) spout<-list() assigns the name "spout" to an empty list. As we are
> processing columns 2 to 12 of the input data, spout[[i-1]] assigns the
> results to elements 1 to 11 of the list "spout". Just a low trick.
>
> 1a) Yes, you can create a "wrapper" function that will return only the
> statistic and p.value.
>
> # create a function that returns only the
> # statistic and p.value as a string
> archStatP<-function(x) {
>  archout<-ArchTest(x)
>  return(sprintf("ChiSq = %f, p = %f",archout$statistic,archout$p.value))
> }
> # using "lapply", run the test on each column
> spout<-lapply(sp_8_5[,2:12],archStatP)
>
> Note that I should have used "lapply". I didn't check the output carefully
> enough.
>
> 2a) Now you only have to separate the strings in "spout" with TAB
> characters and import the result into Excel. I have to wash the dishes, so
> you're on your own.
>
> Jim
>
> On Fri, May 8, 2020 at 8:26 PM Subhamitra Patra <
> subhamitra.patra at gmail.com> wrote:
>
>> Dear Sir,
>>
>> Thank you very much for such an excellent solution to my problem. I was
>> trying sapply function since last days, but was really unable to write
>> properly. Now, I understood my mistake in using sapply function in the
>> code. Therefore, I have two queries regarding this which I want to discuss
>> here just for my learning purpose.
>>
>> 1. While using sapply function for estimating one method across the
>> columns of a data frame, one needs to define the list of the output table
>> after using sapply so that the test results for each column will be
>> consistently stored in an output object, right?
>>
>> 2. In the spout<- list() command, what spout[[i-1]]  indicates?
>>
>> Sir, one more possibility which I would like to ask related to my above
>> problem just to learn for further R programming language.
>>
>> After running your suggested code, all the results for each column are
>> being stored in the spout object. From this, I need only the statistics and
>> P-value for each column. So, my queries are:
>>
>> 1. Is there any way to extract only two values (i.e., statistics and
>> p-value) for each column that stored in spout object and save these two
>> values in another R data frame for each column?
>>  or
>> 2. Is there any possibility that the statistics and p-value
>> calculated for each column can directly export to a word file in a table
>> format (having 4 columns and 3 rows). In particular, is it possible to
>> extract both statistic and p-value results for each column to an MS word
>> file with the format of A1, A2, A3, A4 column results in 1st row, A5, A6,
>> A7, A8 column results in 2nd row, and A9, A10, A11, A12 column results in
>> the 3rd row of the table?
>>
>>
>> Like before, your suggestion will definitely help me to learn the
>> advanced R language.
>>
>> Thank you very much for your help.
>>
>> [image: Mailtrack]
>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
>> notified by
>> Mailtrack
>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 05/08/20,
>> 03:47:26 PM
>>
>> On Fri, May 8, 2020 at 2:37 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>>
>>> Hi Subhamitra,
>>> This isn't too hard:
>>>
>>> # read in the sample data that was
>>> # saved in the file "sp_8_5.tab"
>>> sp_8_5<-read.table("sp_8_5.tab",sep="\t",
>>>  header=TRUE,stringsAsFactors=FALSE)
>>> library(tseries)
>>> library(FinTS)
>>> # using "sapply", run the test on each column
>>> spout<-sapply(sp_8_5[,2:12],ArchTest)
>>>
>>> The list "spout" contains the test results. If you really want to use a
>>> loop:
>>>
>>> spout<-list()
>>> for(i in 2:12) spout[[i-1]]<-ArchTest(sp_8_5[,i])
>>>
>>> Jim
>>>
>>>
>>> On Fri, May 8, 2020 at 5:27 PM Subhamitra Patra <
>>> subhamitra.patra at gmail.com> wrote:
>>>
>>>> Dear Sir,
>>>>
>>>> Herewith I am pasting a part of my sample data having 12 columns below,
>>>> and want to calculate ARCH test for the 12 columns by using a loop.
>>>>
>>>>
>>
>> --
>> *Best Regards,*
>> *Subhamitra Patra*
>> *Phd. Research Scholar*
>> *Department of Humanities and Social Sciences*
>> *Indian Institute of Technology, Kharagpur*
>> *INDIA*
>>
>

-- 
*Best Regards,*
*Subhamitra Patra*
*Phd. Research Scholar*
*Department of Humanities and Social Sciences*
*Indian Institute of Technology, Kharagpur*
*INDIA*

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri May  8 15:46:45 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 8 May 2020 14:46:45 +0100
Subject: [R] 
 Warning in install.packages : converting NULL pointer to R NULL
In-Reply-To: <a835fb6e-b229-1460-0ce5-107594ba3f5b@gmail.com>
References: <bbd798c8-eaa7-1b8d-cb39-7e8926600520@sapo.pt>
 <a835fb6e-b229-1460-0ce5-107594ba3f5b@gmail.com>
Message-ID: <d05f1e8b-c0d3-9106-22ec-c0bce0187b06@sapo.pt>

Hello,

You are right,

Rscript -e 'install.packages("car")'

doesn't give that message, I will ask RStudio support.
And sorry to spam the list with something I should have checked, I'm so 
used to working in GUI 's that I forgot about the command line.

Thanks,

Rui Barradas

?s 13:56 de 08/05/20, Duncan Murdoch escreveu:
> That looks like an RStudio message.? Do you get it if you run 
> install.packages() in command line R?
> 
> Duncan Murdoch
> 
> On 08/05/2020 8:07 a.m., Rui Barradas wrote:
>> Hello,
>>
>> R 4.0.0 on Ubuntu 20.04, sessionInfo() below.
>>
>> Since I updated to R 4.0 that every time I try to install a package with
>> install.packages() the warning in the title shows up at the end, be the
>> installation successful or not. If it is successful, the package loads
>> with no problems, so I'm not very worried but it isn't normal (expected)
>> behavior, is it?
>>
>> Here is a run of install.packages().
>>
>>
>> install.packages('cowplot')
>> Installing package into ?/usr/local/lib/R/site-library?
>> (as ?lib? is unspecified)
>> trying URL 'https://cloud.r-project.org/src/contrib/cowplot_1.0.0.tar.gz'
>> Content type 'application/x-gzip' length 1275585 bytes (1.2 MB)
>> ==================================================
>> downloaded 1.2 MB
>>
>> * installing *source* package ?cowplot? ...
>> ** package ?cowplot? successfully unpacked and MD5 sums checked
>> ** using staged installation
>> ** R
>> ** inst
>> ** byte-compile and prepare package for lazy loading
>> ** help
>> *** installing help indices
>> *** copying figures
>> ** building package indices
>> ** installing vignettes
>> ** testing if installed package can be loaded from temporary location
>> ** testing if installed package can be loaded from final location
>> ** testing if installed package keeps a record of temporary installation
>> path
>> * DONE (cowplot)
>>
>> The downloaded source packages are in
>> ?????/tmp/Rtmp9NXQkt/downloaded_packages?
>> Warning in install.packages :
>> ??? converting NULL pointer to R NULL
>>
>>
>> Also, I'm running this on RStudio and haven't changed the R library
>> directory.
>>
>>
>> sessionInfo()
>> R version 4.0.0 (2020-04-24)
>> Platform: x86_64-pc-linux-gnu (64-bit)
>> Running under: Ubuntu 20.04 LTS
>>
>> Matrix products: default
>> BLAS:?? /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0
>> LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0
>>
>> locale:
>> ?? [1] LC_CTYPE=pt_PT.UTF-8?????? LC_NUMERIC=C
>> LC_TIME=pt_PT.UTF-8
>> ?? [4] LC_COLLATE=pt_PT.UTF-8???? LC_MONETARY=pt_PT.UTF-8
>> LC_MESSAGES=pt_PT.UTF-8
>> ?? [7] LC_PAPER=pt_PT.UTF-8?????? LC_NAME=C????????????????? LC_ADDRESS=C
>>
>> [10] LC_TELEPHONE=C???????????? LC_MEASUREMENT=pt_PT.UTF-8
>> LC_IDENTIFICATION=C
>>
>> attached base packages:
>> [1] stats???? graphics? grDevices utils???? datasets? methods?? base
>>
>> other attached packages:
>> [1] MASS_7.3-51.6? ggthemes_4.2.0 ggrepel_0.8.2? dplyr_0.8.5
>> ggplot2_3.3.0
>>
>> loaded via a namespace (and not attached):
>> ?? [1] zoo_1.8-8????????? tidyselect_1.0.0?? purrr_0.3.4
>> reshape2_1.4.4???? haven_2.2.0
>> ?? [6] lattice_0.20-41??? sodium_1.1???????? carData_3.0-3
>> colorspace_1.4-1?? vctrs_0.2.4
>> [11] yaml_2.2.1???????? rlang_0.4.6??????? pillar_1.4.3
>> withr_2.2.0??????? foreign_0.8-79
>> [16] glue_1.4.0???????? readxl_1.3.1?????? lifecycle_0.2.0??? plyr_1.8.6
>> ????????? stringr_1.4.0
>> [21] MatrixModels_0.4-1 munsell_0.5.0????? gtable_0.3.0
>> cellranger_1.1.0?? zip_2.0.4
>> [26] rio_0.5.16???????? forcats_0.5.0????? SparseM_1.78
>> quantreg_5.55????? curl_4.3
>> [31] tis_1.38?????????? Rcpp_1.0.4.6?????? readr_1.3.1
>> scales_1.1.0?????? abind_1.4-5
>> [36] farver_2.0.3?????? sos_2.0-0????????? brew_1.0-6
>> digest_0.6.25????? hms_0.5.3
>> [41] png_0.1-7????????? stringi_1.4.6????? openxlsx_4.1.4???? grid_4.0.0
>> ????????? tools_4.0.0
>> [46] magrittr_1.5?????? tibble_3.0.1?????? pacman_0.5.1
>> crayon_1.3.4?????? car_3.0-7
>> [51] pkgconfig_2.0.3??? ellipsis_0.3.0???? Matrix_1.2-18
>> data.table_1.12.8? assertthat_0.2.1
>> [56] httr_1.4.1???????? rstudioapi_0.11??? R6_2.4.1
>> compiler_4.0.0
>>
>>
>> Thanks in advance,
>>
>> Rui Barradas
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Fri May  8 16:30:46 2020
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 8 May 2020 16:30:46 +0200
Subject: [R] 
 Warning in install.packages : converting NULL pointer to R NULL
In-Reply-To: <d05f1e8b-c0d3-9106-22ec-c0bce0187b06@sapo.pt>
References: <bbd798c8-eaa7-1b8d-cb39-7e8926600520@sapo.pt>
 <a835fb6e-b229-1460-0ce5-107594ba3f5b@gmail.com>
 <d05f1e8b-c0d3-9106-22ec-c0bce0187b06@sapo.pt>
Message-ID: <24245.27926.352000.88429@stat.math.ethz.ch>

>>>>> Rui Barradas 
>>>>>     on Fri, 8 May 2020 14:46:45 +0100 writes:

    > Hello, You are right,

    > Rscript -e 'install.packages("car")'

    > doesn't give that message, I will ask RStudio support.
    > And sorry to spam the list with something I should have
    > checked, I'm so used to working in GUI 's that I forgot
    > about the command line.

Well, you forgot that Rstudio wraps quite a bit around R.
If you use install.packages() inside Rstudio you get their own
version instead of R's ... :

> install.packages
function (...) 
.rs.callAs(name, hook, original, ...)
<environment: 0x55a548da49f0>
>

And they have really tweaked R in a way that it behaves
illogically, and even I don't see how they kept their version of
install.packages hidden from the conflicts() and find()
functions :

> find("install.packages")
[1] "package:utils"

But of course

> identical(install.packages, utils::install.packages)
[1] FALSE

(Now closing Rstudio again .. and revert to use ESS)

Martin

    > Thanks,

    > Rui Barradas

    > ?s 13:56 de 08/05/20, Duncan Murdoch escreveu:
    >> That looks like an RStudio message.? Do you get it if you
    >> run install.packages() in command line R?
    >> 
    >> Duncan Murdoch
    >> 
    >> On 08/05/2020 8:07 a.m., Rui Barradas wrote:
    >>> Hello,
    >>> 
    >>> R 4.0.0 on Ubuntu 20.04, sessionInfo() below.
    >>> 
    >>> Since I updated to R 4.0 that every time I try to
    >>> install a package with install.packages() the warning in
    >>> the title shows up at the end, be the installation
    >>> successful or not. If it is successful, the package
    >>> loads with no problems, so I'm not very worried but it
    >>> isn't normal (expected) behavior, is it?
    >>> 
    >>> Here is a run of install.packages().
    >>> 
    >>> 
    >>> install.packages('cowplot') Installing package into
    >>> ?/usr/local/lib/R/site-library? (as ?lib? is
    >>> unspecified) trying URL
    >>> 'https://cloud.r-project.org/src/contrib/cowplot_1.0.0.tar.gz'
    >>> Content type 'application/x-gzip' length 1275585 bytes
    >>> (1.2 MB)
    >>> ==================================================
    >>> downloaded 1.2 MB
    >>> 
    >>> * installing *source* package ?cowplot? ...  ** package
    >>> ?cowplot? successfully unpacked and MD5 sums checked **
    >>> using staged installation ** R ** inst ** byte-compile
    >>> and prepare package for lazy loading ** help ***
    >>> installing help indices *** copying figures ** building
    >>> package indices ** installing vignettes ** testing if
    >>> installed package can be loaded from temporary location
    >>> ** testing if installed package can be loaded from final
    >>> location ** testing if installed package keeps a record
    >>> of temporary installation path * DONE (cowplot)
    >>> 
    >>> The downloaded source packages are in
    >>> ?????/tmp/Rtmp9NXQkt/downloaded_packages? Warning in
    >>> install.packages : ??? converting NULL pointer to R NULL
    >>> 
    >>> 
    >>> Also, I'm running this on RStudio and haven't changed
    >>> the R library directory.
    >>> 
    >>> 
    >>> sessionInfo() R version 4.0.0 (2020-04-24) Platform:
    >>> x86_64-pc-linux-gnu (64-bit) Running under: Ubuntu 20.04
    >>> LTS
    >>> 
    >>> Matrix products: default BLAS:??
    >>> /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0 LAPACK:
    >>> /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0
    >>> 
    >>> locale: ?? [1] LC_CTYPE=pt_PT.UTF-8?????? LC_NUMERIC=C
    >>> LC_TIME=pt_PT.UTF-8 ?? [4] LC_COLLATE=pt_PT.UTF-8????
    >>> LC_MONETARY=pt_PT.UTF-8 LC_MESSAGES=pt_PT.UTF-8 ?? [7]
    >>> LC_PAPER=pt_PT.UTF-8?????? LC_NAME=C?????????????????
    >>> LC_ADDRESS=C
    >>> 
    >>> [10] LC_TELEPHONE=C????????????
    >>> LC_MEASUREMENT=pt_PT.UTF-8 LC_IDENTIFICATION=C
    >>> 
    >>> attached base packages: [1] stats???? graphics?
    >>> grDevices utils???? datasets? methods?? base
    >>> 
    >>> other attached packages: [1] MASS_7.3-51.6?
    >>> ggthemes_4.2.0 ggrepel_0.8.2? dplyr_0.8.5 ggplot2_3.3.0
    >>> 
    >>> loaded via a namespace (and not attached): ?? [1]
    >>> zoo_1.8-8????????? tidyselect_1.0.0?? purrr_0.3.4
    >>> reshape2_1.4.4???? haven_2.2.0 ?? [6] lattice_0.20-41???
    >>> sodium_1.1???????? carData_3.0-3 colorspace_1.4-1??
    >>> vctrs_0.2.4 [11] yaml_2.2.1???????? rlang_0.4.6???????
    >>> pillar_1.4.3 withr_2.2.0??????? foreign_0.8-79 [16]
    >>> glue_1.4.0???????? readxl_1.3.1?????? lifecycle_0.2.0???
    >>> plyr_1.8.6 ????????? stringr_1.4.0 [21]
    >>> MatrixModels_0.4-1 munsell_0.5.0????? gtable_0.3.0
    >>> cellranger_1.1.0?? zip_2.0.4 [26] rio_0.5.16????????
    >>> forcats_0.5.0????? SparseM_1.78 quantreg_5.55?????
    >>> curl_4.3 [31] tis_1.38?????????? Rcpp_1.0.4.6??????
    >>> readr_1.3.1 scales_1.1.0?????? abind_1.4-5 [36]
    >>> farver_2.0.3?????? sos_2.0-0????????? brew_1.0-6
    >>> digest_0.6.25????? hms_0.5.3 [41] png_0.1-7?????????
    >>> stringi_1.4.6????? openxlsx_4.1.4???? grid_4.0.0
    >>> ????????? tools_4.0.0 [46] magrittr_1.5??????
    >>> tibble_3.0.1?????? pacman_0.5.1 crayon_1.3.4??????
    >>> car_3.0-7 [51] pkgconfig_2.0.3??? ellipsis_0.3.0????
    >>> Matrix_1.2-18 data.table_1.12.8? assertthat_0.2.1 [56]
    >>> httr_1.4.1???????? rstudioapi_0.11??? R6_2.4.1
    >>> compiler_4.0.0
    >>> 
    >>> 
    >>> Thanks in advance,
    >>> 
    >>> Rui Barradas
    >>> 
    >>> ______________________________________________
    >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    >>> more, see https://stat.ethz.ch/mailman/listinfo/r-help
    >>> PLEASE do read the posting guide
    >>> http://www.R-project.org/posting-guide.html and provide
    >>> commented, minimal, self-contained, reproducible code.
    >>> 
    >> 

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    > more, see https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide
    > http://www.R-project.org/posting-guide.html and provide
    > commented, minimal, self-contained, reproducible code.


From g@v@n@mcgr@th @end|ng |rom dbc@@w@@gov@@u  Wed May  6 05:50:12 2020
From: g@v@n@mcgr@th @end|ng |rom dbc@@w@@gov@@u (Gavan McGrath)
Date: Wed, 6 May 2020 03:50:12 +0000
Subject: [R] Rtools virus
Message-ID: <ME2PR01MB3714417631C9A5EC7B3C434C9BA40@ME2PR01MB3714.ausprd01.prod.outlook.com>

Hi,
My IT department instructed me to uninstall Windows 64-bit: rtools40-x86_64.exe as it contained a virus which they identified at

https://www.virustotal.com/gui/file/5c10d60e73dd0186e8f886ef0b9388bb7dbdfdc17366c14c16183edb08fdb58a/detection

Kind Regards,

Dr Gavan McGrath, PhD, B.E.

Research Scientist
Biodiversity and Conservation Science
Department of Biodiversity, Conservation and Attractions
Street Address: 17 Dick Perry Avenue, Kensington, WA 6151, Australia
Postal Address Locked Bag 104, Bentley Delivery Centre, WA 6983, Australia
Phone: +618 9219 9447 Mobile: +61 458 559 765
Email: gavan.mcgrath at dbca.wa.gov.au

Adjunct Research Fellow
School of Agriculture and Environment
The University of Western Australia
Perth, Western Australia
Email: gavan.mcgrath at uwa.edu.au

________________________________
 This message is confidential and is intended for the recipient named above. If you are not the intended recipient, you must not disclose, use or copy the message or any part of it. If you received this message in error, please notify the sender immediately by replying to this message, then delete it from your system.


From @@w||t @end|ng |rom unom@h@@edu  Wed May  6 21:54:43 2020
From: @@w||t @end|ng |rom unom@h@@edu (Andrew Swift)
Date: Wed, 6 May 2020 19:54:43 +0000
Subject: [R] Function Hints in Mac Dark Mode
Message-ID: <1E9E4AA6-D87A-46C2-A585-459645F6EF6D@unomaha.edu>

Sorry, wasn?t sure exactly where to post this but I noticed that with R 4.0.0 when running a Mac in Dark Mode that the Function Hints at the bottom of the Console and Editor windows become invisible.  
Thanks. 

From rom@no|@on @end|ng |rom yon@e|@@c@kr  Thu May  7 09:46:05 2020
From: rom@no|@on @end|ng |rom yon@e|@@c@kr (Roman Olson)
Date: Thu, 7 May 2020 16:46:05 +0900
Subject: [R] Bug in function arguments autocomplete and ellipsis?
Message-ID: <3E8A851C-9211-4CDE-B9B6-35DAF25828C9@yonsei.ac.kr>

Dear All,

I am wondering whether function arguments autocomplete causes a bug when additional ellipsis arguments are used.

Example:
a = function(robot) {
    cat(robot, "\n")
 }
a(r=5) prints 5, meaning that r is autocompleted to ?robot?. Not sure if this is normal behavior, but this does not cause problems.

This would be fine, but somehow causes problems when there is an additional ellipsis argument that can be used to autocomplete an already existing argument. In the example below, when we are calling sens.analysis.all, everything starting with q is an additional argument (?). Now, k is missed completely. And that is because there is another actual argument that starts with k ? key.legend.axes (it is assigned to 4 instead). If ?k=4? is changed to ?kk=4? the problem disappears.  

sens.analysis.all <- function(func, outgrid, parvec, parmin, parmax,
                              length.pgrid, outvname, zlim, plabs,
                              gridlab, mylog, outlog=FALSE, ytick=NULL,
                              xline=NULL, yline=NULL, mypal=topo.colors,
                              key.legend.axes=NULL, plot.guidance=FALSE, ... ) {

   cat(..1, "\n")
   cat(..2, "\n")
   cat(..3, "\n")
 
   out=1
   out
} 

out = sens.analysis.all(numer.wait.times.wrps, NA,
     c(5.4, 0.008, 1.5), c(4.9, 0.0079, 2.0), c(5.5, 0.0090, 3.5),
       length.pgrid=10, outvname="cvs", zlim=c(0.3, 2),
       plabs=c("mu", "lambda", "b"), gridlab="Soil Moisture [mm]",
       mylog=FALSE, outlog=FALSE, yline=2, plot.guidance=FALSE, q=3.1, k=4, y.c=670,
       realgrid=seq(630, 670, by=4), myseed=0,
       nt=1000000, burnin=300000, bin.cutoff=500,
       bdW.prelim=prec.struct.Mal)

From ||9212001 @end|ng |rom y@hoo@com  Thu May  7 15:23:09 2020
From: ||9212001 @end|ng |rom y@hoo@com (aiguo li)
Date: Thu, 7 May 2020 13:23:09 +0000 (UTC)
Subject: [R] Need a suggestion on a package to make a figure
References: <216384015.2752852.1588857789484.ref@mail.yahoo.com>
Message-ID: <216384015.2752852.1588857789484@mail.yahoo.com>

Hello all,
I need to make a table with a value imaged by greater than certain value as attached.? Could you give me a suggestions on which R package will be good for this type of table?
Thanks and stay safe!
Aiguo
-------------- next part --------------
A non-text attachment was scrubbed...
Name: sample_table.pdf
Type: application/pdf
Size: 26796 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200507/3e183657/attachment.pdf>

From k||6891 @end|ng |rom gm@||@com  Thu May  7 21:07:50 2020
From: k||6891 @end|ng |rom gm@||@com (Kevin Li)
Date: Thu, 7 May 2020 15:07:50 -0400
Subject: [R] Why doesn't rpart split further
Message-ID: <CAMTq2VorZsAztWQd6OHBg4fP9X06RGsV9WMUA_HuFHOOkmY_bw@mail.gmail.com>

Hi,

I am using the rpart package to construct regression trees and for the
purposes of simulation, would like to the tree completely split: each
leaf should contain exactly one observation.

However, I have observed that even by setting minsplit = 2, i.e.,

```
control <- rpart.control(
    minsplit = 2,
    cp = -1,
    xval = 0,
    maxcompete = 0,
    usesurrogate=0,
    maxdepth=30
)

model <- rpart(...., control = control)
```

the model will still have leaf nodes with more than one observation.
In fact, when I choose a subset of my dataset which fall into the same
terminal leaf, and run rpart on that subset, further split will occur.
Any advice on why this is occuring? Thanks!


Best regards,
Kevin


P.S. A snippet to showcase the behavior above:

--

library(rpart)
library(data.table)

mu <- function(x, y, z) sin(10 * pi * x + 2 * y) - cos(10 * pi * y) + exp(z)

control <- rpart.control(
    minsplit = 2,
    cp = -1,
    xval = 0,
    maxcompete = 0,
    usesurrogate=0,
    maxdepth=30
)

gen.data <- function(n, sd = 0.5) {
    X <- matrix(runif(3 * n), ncol=3)
    colnames(X) <- c('x', 'y', 'z')
    e <- rnorm(n, sd = sd)

    X <- data.table(X)
    X[, mu := mu(x, y, z)]
    X[, A := mu + e]
    return(X[])
}

# Run rpart on the simulated dataset ...
set.seed(12321)
X <- gen.data(30000, sd = 0.1)
X[, i := .I]
mod <- rpart(A ~ x + y + z, X, control = control)

frame <- as.data.table(mod$frame, keep.rownames=TRUE)
frame[, rn := as.integer(rn)][, i := .I]
setnames(frame, "rn", "id")

splits <- as.data.table(mod$splits, keep.rownames=TRUE)
setnames(splits, "rn", "var")
splits[, var := factor(var)]

where <- data.table(i = seq(1, X[,.N]), where=mod$where)


# m = 7191 is the row of the leaf that contains the most observations,
# in this case 11.
m <- frame[var == "<leaf>"][order(-n)][1, i]
obs <- where[where == m, i]

# Collect those 11 observations another dataframe
X2 <- X[i %in% obs]

# observe that rpart will split on that subset again, why?
mod2 <- rpart(A ~ x + y + z, X2, control=control)


From @me|@ne1 @end|ng |rom jhm|@edu  Thu May  7 21:32:36 2020
From: @me|@ne1 @end|ng |rom jhm|@edu (Allison Meisner)
Date: Thu, 7 May 2020 19:32:36 +0000
Subject: [R] Error in summary.warnings?
Message-ID: <BL0PR01MB43069FB2C1BC342FFDC19142EFA50@BL0PR01MB4306.prod.exchangelabs.com>

I believe there is an error in the summary.warnings function (typically called via 'summary(warnings())'). Below is a minimal working example:

#########

testfunction <- function(x){
if(x > 30){
warning("A big problem (should be 20 of these)")
}else{
warning("Bigger problem (should be 30 of these)")
}
}

for(i in 1:50){
testfunction(i)
}

summary(warnings())

#########

I checked the code for summary.warnings:

function (object, ...)
{
    msgs <- names(object)
    calls <- as.character(object)
    ss <- ": "
    c.m. <- paste(calls, msgs, sep = ss)
    if (length(i.no.call <- which(calls == "NULL")))
        c.m.[i.no.call] <- substr(c.m.[i.no.call], nchar(paste0("NULL",
            ss)) + 1L, 100000L)
    tm <- table(c.m., deparse.level = 0L)
    structure(unique(object), counts = as.vector(tm), class = "summary.warnings")
}

The problem appears to be in the last line: unique preserves the order of the input, but counts reflects the counts in the table tm, which is a problem because table names are in alphabetical order.

Am I missing something?

Allison



----------

Allison Meisner, PhD

Postdoctoral Fellow

Department of Biostatistics

Johns Hopkins Bloomberg School of Public Health

615 N. Wolfe Street

Baltimore, MD 21205



	[[alternative HTML version deleted]]


From gerh|g4 @end|ng |rom gm@||@com  Fri May  8 05:23:14 2020
From: gerh|g4 @end|ng |rom gm@||@com (Sam Rizzuto)
Date: Thu, 7 May 2020 23:23:14 -0400
Subject: [R] Obtain One Row From ggcorr() Matrix
Message-ID: <D309ECD9-D91D-43C6-89F9-22D13C3A895F@gmail.com>

Hi all,

I am looking to obtain one column/row of a correlation matrix from my data using the function ggcorr() from library(GGally). 

As an example, using the mtcars dataset, I have the following code that can reproduce one row/column:

df <- cor(x = mtcars$mpg, y = mtcars[2:11], use = ?everything?)
library(corrplot)
corrplot(df, tl.srt = 45, method = ?color?, addCoef.col = ?black?, cl.cex = 0.56)

This shows all the correlations between mpg. However using ggcorr() and plotting it:

ggcorr(mtcars, method = c(?everything?), label = TRUE, label_size = 2, label_round = 4)

It shows a much nice and prettier looking correlation plot but does it for all variables. 

I have tried putting mtcars$mpg and mtcars[,1] to only return the one row, but neither seem to work. 

Any ideas on how to do it using ggcorr()?

Thanks,
Sam


From @@mue|| @end|ng |rom ||||no|@@edu  Thu May  7 17:32:14 2020
From: @@mue|| @end|ng |rom ||||no|@@edu (Bonfim Fernandes, Samuel)
Date: Thu, 7 May 2020 15:32:14 +0000
Subject: [R] [R-pkgs] simplePHENOTYPES: SIMulation of Pleiotropic,
 Linked and Epistatic PHENOTYPES
Message-ID: <C478E421-C30E-4D57-B0EB-6A45B83E03D3@illinois.edu>

Hi All,

I hope this message finds you well.
I have developed the simplePHENOTYPES package to simulate single and multiple (correlated) traits in a wide range of scenarios, including additive, dominance, and epistatic (AxA) models.
The newest version, simplePHENOTYPES v1.2.4, which has just been released on CRAN, has substantial improvements compared to the previous one.

Some of the new features include:
- Options for using VCF, plink bed/ped files, GDS, HapMap, and Numeric as input files.
- Implementation of two types of spurious pleiotropy simulation (?direct" and "indirect").
- Simulation of residual correlation among traits.

Please check it our here:
https://cran.r-project.org/package=simplePHENOTYPES
Vignettes for the most common scenarios one would want to simulate may be found here:
https://cran.r-project.org/web/packages/simplePHENOTYPES/vignettes/simplePHENOTYPES.html

Best regards,
Samuel Fernandes

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages

From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri May  8 16:58:34 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 08 May 2020 07:58:34 -0700
Subject: [R] Rtools virus
In-Reply-To: <ME2PR01MB3714417631C9A5EC7B3C434C9BA40@ME2PR01MB3714.ausprd01.prod.outlook.com>
References: <ME2PR01MB3714417631C9A5EC7B3C434C9BA40@ME2PR01MB3714.ausprd01.prod.outlook.com>
Message-ID: <CF2FB038-0258-4698-B10B-0435C592BF5A@dcn.davis.ca.us>

Sorry to hear that. It is most likely a false positive (antivirus software has little incentive to minimise false positives), but no one here can follow up on your report because you did not say precisely which website you downloaded it from.

On May 5, 2020 8:50:12 PM PDT, Gavan McGrath <gavan.mcgrath at dbca.wa.gov.au> wrote:
>Hi,
>My IT department instructed me to uninstall Windows 64-bit:
>rtools40-x86_64.exe as it contained a virus which they identified at
>
>https://www.virustotal.com/gui/file/5c10d60e73dd0186e8f886ef0b9388bb7dbdfdc17366c14c16183edb08fdb58a/detection
>
>Kind Regards,
>
>Dr Gavan McGrath, PhD, B.E.
>
>Research Scientist
>Biodiversity and Conservation Science
>Department of Biodiversity, Conservation and Attractions
>Street Address: 17 Dick Perry Avenue, Kensington, WA 6151, Australia
>Postal Address Locked Bag 104, Bentley Delivery Centre, WA 6983,
>Australia
>Phone: +618 9219 9447 Mobile: +61 458 559 765
>Email: gavan.mcgrath at dbca.wa.gov.au
>
>Adjunct Research Fellow
>School of Agriculture and Environment
>The University of Western Australia
>Perth, Western Australia
>Email: gavan.mcgrath at uwa.edu.au
>
>________________________________
>This message is confidential and is intended for the recipient named
>above. If you are not the intended recipient, you must not disclose,
>use or copy the message or any part of it. If you received this message
>in error, please notify the sender immediately by replying to this
>message, then delete it from your system.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri May  8 17:02:42 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 08 May 2020 08:02:42 -0700
Subject: [R] Function Hints in Mac Dark Mode
In-Reply-To: <1E9E4AA6-D87A-46C2-A585-459645F6EF6D@unomaha.edu>
References: <1E9E4AA6-D87A-46C2-A585-459645F6EF6D@unomaha.edu>
Message-ID: <32244604-925F-4F40-A200-3F1AA6D95D36@dcn.davis.ca.us>

You seem to be confusing R and RStudio... so yeah, wrong mailing list. I don't know exactly where you should post either. Perhaps the GitHub issues page for RStudio?

On May 6, 2020 12:54:43 PM PDT, Andrew Swift via R-help <r-help at r-project.org> wrote:
>Sorry, wasn?t sure exactly where to post this but I noticed that with R
>4.0.0 when running a Mac in Dark Mode that the Function Hints at the
>bottom of the Console and Editor windows become invisible.  
>Thanks. 
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @@w||t @end|ng |rom unom@h@@edu  Fri May  8 17:12:37 2020
From: @@w||t @end|ng |rom unom@h@@edu (Andrew Swift)
Date: Fri, 8 May 2020 15:12:37 +0000
Subject: [R] Function Hints in Mac Dark Mode
In-Reply-To: <769FC136-30D0-4151-98AB-E29C22A4D6F7@me.com>
References: <1E9E4AA6-D87A-46C2-A585-459645F6EF6D@unomaha.edu>
 <32244604-925F-4F40-A200-3F1AA6D95D36@dcn.davis.ca.us>
 <769FC136-30D0-4151-98AB-E29C22A4D6F7@me.com>
Message-ID: <AB763CC0-4825-464D-B06D-9A2875A8E54A@unomaha.edu>

Marc,

Yes, that is exactly the issue.  I?ll post to r-sig-mac.

On May 8, 2020, at 10:11 AM, Marc Schwartz <marc_schwartz at me.com<mailto:marc_schwartz at me.com>> wrote:



On May 8, 2020, at 11:02 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us>> wrote:

You seem to be confusing R and RStudio... so yeah, wrong mailing list. I don't know exactly where you should post either. Perhaps the GitHub issues page for RStudio?

On May 6, 2020 12:54:43 PM PDT, Andrew Swift via R-help <r-help at r-project.org<mailto:r-help at r-project.org>> wrote:
Sorry, wasn?t sure exactly where to post this but I noticed that with R
4.0.0 when running a Mac in Dark Mode that the Function Hints at the
bottom of the Console and Editor windows become invisible.
Thanks.


Hi,

Actually, I am not sure that is the case.

See the attached screen capture of the default R.app (which I do not use) when the desktop is set to dark mode, which I also do not use.

When in light mode, those hints in the lower left hand corner are black, and they switch to white in dark mode, making them almost impossible to see.

For Andrew, if this is correct, and you are not referring to RStudio per Jeff, you should post this to r-sig-mac:

  https://stat.ethz.ch/mailman/listinfo/r-sig-mac<https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dsig-2Dmac&d=DwMFaQ&c=Cu5g146wZdoqVuKpTNsYHeFX_rg6kWhlkLF8Eft-wwo&r=W-hEH5pfgw6X6us1FQf4wr3R0QD-0iVKleJgEEY57pQ&m=eFzkYDSccZeaT6c3QZuFD8BxeoZmisZ1KA4oac3myN4&s=B_Lk9KI3pylJo27TNuCNwTyCM-i5Va2K-gKrWvIbjNg&e=>

Otherwise, if it is RStudio, they have their own support here:

   https://support.rstudio.com/hc/en-us<https://urldefense.proofpoint.com/v2/url?u=https-3A__support.rstudio.com_hc_en-2Dus&d=DwMFaQ&c=Cu5g146wZdoqVuKpTNsYHeFX_rg6kWhlkLF8Eft-wwo&r=W-hEH5pfgw6X6us1FQf4wr3R0QD-0iVKleJgEEY57pQ&m=eFzkYDSccZeaT6c3QZuFD8BxeoZmisZ1KA4oac3myN4&s=Dj-fBrHh61yxxSaZL_kUbp0dHadpIXholuPiNVejemA&e=>

Regards,

Marc Schwartz


<Screen Shot 2020-05-08 at 11.05.02 AM.png>



	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Fri May  8 17:37:29 2020
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 8 May 2020 17:37:29 +0200
Subject: [R] Error in summary.warnings?
In-Reply-To: <BL0PR01MB43069FB2C1BC342FFDC19142EFA50@BL0PR01MB4306.prod.exchangelabs.com>
References: <BL0PR01MB43069FB2C1BC342FFDC19142EFA50@BL0PR01MB4306.prod.exchangelabs.com>
Message-ID: <24245.31929.569398.561576@stat.math.ethz.ch>

>>>>> Allison Meisner 
>>>>>     on Thu, 7 May 2020 19:32:36 +0000 writes:

    > I believe there is an error in the summary.warnings function (typically called via 'summary(warnings())'). Below is a minimal working example:
    > #########

    > testfunction <- function(x){
    >  if(x > 30){
    >      warning("A big problem (should be 20 of these)")
    >  }else{
    >      warning("Bigger problem (should be 30 of these)")
    >  }
    > }

    > for(i in 1:50){
    >     testfunction(i)
    > }

    > summary(warnings())

    > #########

    > I checked the code for summary.warnings:

    > function (object, ...)
    > {
    > msgs <- names(object)
    > calls <- as.character(object)
    > ss <- ": "
    > c.m. <- paste(calls, msgs, sep = ss)
    > if (length(i.no.call <- which(calls == "NULL")))
    > c.m.[i.no.call] <- substr(c.m.[i.no.call], nchar(paste0("NULL",
    > ss)) + 1L, 100000L)
    > tm <- table(c.m., deparse.level = 0L)
    > structure(unique(object), counts = as.vector(tm), class = "summary.warnings")
    > }

    > The problem appears to be in the last line: unique preserves the order of the input, but counts reflects the counts in the table tm, which is a problem because table names are in alphabetical order.

    > Am I missing something?

No -- I think you are perfect and I was very imperfect ;-)  when
I created and tested the function ..

This will be fixed in the next versions of R.

Thank you very much for the report  and the nice concise
reproducible example!

Best regards,
Martin

    > Allison
    > ----------
    > Allison Meisner, PhD
    > Postdoctoral Fellow
    > Department of Biostatistics
    > Johns Hopkins Bloomberg School of Public Health
    > 615 N. Wolfe Street
    > Baltimore, MD 21205

Martin Maechler
ETH Zurich  and   R Core team


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri May  8 17:51:32 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 8 May 2020 16:51:32 +0100
Subject: [R] 
 Warning in install.packages : converting NULL pointer to R NULL
In-Reply-To: <24245.27926.352000.88429@stat.math.ethz.ch>
References: <bbd798c8-eaa7-1b8d-cb39-7e8926600520@sapo.pt>
 <a835fb6e-b229-1460-0ce5-107594ba3f5b@gmail.com>
 <d05f1e8b-c0d3-9106-22ec-c0bce0187b06@sapo.pt>
 <24245.27926.352000.88429@stat.math.ethz.ch>
Message-ID: <f42464df-cc07-4fd7-3f8c-b18010eb11cb@sapo.pt>

Hello,

My main error was that this is the first time it happens and I didn't do 
any real thinking, I just assumed that it was an upgrade both R and 
Ubuntu issue.

Thanks, the reminder of the differences between R and RStudio was very 
helpful.

Rui Barradas

?s 15:30 de 08/05/20, Martin Maechler escreveu:
>>>>>> Rui Barradas
>>>>>>      on Fri, 8 May 2020 14:46:45 +0100 writes:
> 
>      > Hello, You are right,
> 
>      > Rscript -e 'install.packages("car")'
> 
>      > doesn't give that message, I will ask RStudio support.
>      > And sorry to spam the list with something I should have
>      > checked, I'm so used to working in GUI 's that I forgot
>      > about the command line.
> 
> Well, you forgot that Rstudio wraps quite a bit around R.
> If you use install.packages() inside Rstudio you get their own
> version instead of R's ... :
> 
>> install.packages
> function (...)
> .rs.callAs(name, hook, original, ...)
> <environment: 0x55a548da49f0>
>>
> 
> And they have really tweaked R in a way that it behaves
> illogically, and even I don't see how they kept their version of
> install.packages hidden from the conflicts() and find()
> functions :
> 
>> find("install.packages")
> [1] "package:utils"
> 
> But of course
> 
>> identical(install.packages, utils::install.packages)
> [1] FALSE
> 
> (Now closing Rstudio again .. and revert to use ESS)
> 
> Martin
> 
>      > Thanks,
> 
>      > Rui Barradas
> 
>      > ?s 13:56 de 08/05/20, Duncan Murdoch escreveu:
>      >> That looks like an RStudio message.? Do you get it if you
>      >> run install.packages() in command line R?
>      >>
>      >> Duncan Murdoch
>      >>
>      >> On 08/05/2020 8:07 a.m., Rui Barradas wrote:
>      >>> Hello,
>      >>>
>      >>> R 4.0.0 on Ubuntu 20.04, sessionInfo() below.
>      >>>
>      >>> Since I updated to R 4.0 that every time I try to
>      >>> install a package with install.packages() the warning in
>      >>> the title shows up at the end, be the installation
>      >>> successful or not. If it is successful, the package
>      >>> loads with no problems, so I'm not very worried but it
>      >>> isn't normal (expected) behavior, is it?
>      >>>
>      >>> Here is a run of install.packages().
>      >>>
>      >>>
>      >>> install.packages('cowplot') Installing package into
>      >>> ?/usr/local/lib/R/site-library? (as ?lib? is
>      >>> unspecified) trying URL
>      >>> 'https://cloud.r-project.org/src/contrib/cowplot_1.0.0.tar.gz'
>      >>> Content type 'application/x-gzip' length 1275585 bytes
>      >>> (1.2 MB)
>      >>> ==================================================
>      >>> downloaded 1.2 MB
>      >>>
>      >>> * installing *source* package ?cowplot? ...  ** package
>      >>> ?cowplot? successfully unpacked and MD5 sums checked **
>      >>> using staged installation ** R ** inst ** byte-compile
>      >>> and prepare package for lazy loading ** help ***
>      >>> installing help indices *** copying figures ** building
>      >>> package indices ** installing vignettes ** testing if
>      >>> installed package can be loaded from temporary location
>      >>> ** testing if installed package can be loaded from final
>      >>> location ** testing if installed package keeps a record
>      >>> of temporary installation path * DONE (cowplot)
>      >>>
>      >>> The downloaded source packages are in
>      >>> ?????/tmp/Rtmp9NXQkt/downloaded_packages? Warning in
>      >>> install.packages : ??? converting NULL pointer to R NULL
>      >>>
>      >>>
>      >>> Also, I'm running this on RStudio and haven't changed
>      >>> the R library directory.
>      >>>
>      >>>
>      >>> sessionInfo() R version 4.0.0 (2020-04-24) Platform:
>      >>> x86_64-pc-linux-gnu (64-bit) Running under: Ubuntu 20.04
>      >>> LTS
>      >>>
>      >>> Matrix products: default BLAS:
>      >>> /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0 LAPACK:
>      >>> /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0
>      >>>
>      >>> locale: ?? [1] LC_CTYPE=pt_PT.UTF-8?????? LC_NUMERIC=C
>      >>> LC_TIME=pt_PT.UTF-8 ?? [4] LC_COLLATE=pt_PT.UTF-8
>      >>> LC_MONETARY=pt_PT.UTF-8 LC_MESSAGES=pt_PT.UTF-8 ?? [7]
>      >>> LC_PAPER=pt_PT.UTF-8?????? LC_NAME=C
>      >>> LC_ADDRESS=C
>      >>>
>      >>> [10] LC_TELEPHONE=C
>      >>> LC_MEASUREMENT=pt_PT.UTF-8 LC_IDENTIFICATION=C
>      >>>
>      >>> attached base packages: [1] stats???? graphics
>      >>> grDevices utils???? datasets? methods?? base
>      >>>
>      >>> other attached packages: [1] MASS_7.3-51.6
>      >>> ggthemes_4.2.0 ggrepel_0.8.2? dplyr_0.8.5 ggplot2_3.3.0
>      >>>
>      >>> loaded via a namespace (and not attached): ?? [1]
>      >>> zoo_1.8-8????????? tidyselect_1.0.0?? purrr_0.3.4
>      >>> reshape2_1.4.4???? haven_2.2.0 ?? [6] lattice_0.20-41
>      >>> sodium_1.1???????? carData_3.0-3 colorspace_1.4-1
>      >>> vctrs_0.2.4 [11] yaml_2.2.1???????? rlang_0.4.6
>      >>> pillar_1.4.3 withr_2.2.0??????? foreign_0.8-79 [16]
>      >>> glue_1.4.0???????? readxl_1.3.1?????? lifecycle_0.2.0
>      >>> plyr_1.8.6 ????????? stringr_1.4.0 [21]
>      >>> MatrixModels_0.4-1 munsell_0.5.0????? gtable_0.3.0
>      >>> cellranger_1.1.0?? zip_2.0.4 [26] rio_0.5.16
>      >>> forcats_0.5.0????? SparseM_1.78 quantreg_5.55
>      >>> curl_4.3 [31] tis_1.38?????????? Rcpp_1.0.4.6
>      >>> readr_1.3.1 scales_1.1.0?????? abind_1.4-5 [36]
>      >>> farver_2.0.3?????? sos_2.0-0????????? brew_1.0-6
>      >>> digest_0.6.25????? hms_0.5.3 [41] png_0.1-7
>      >>> stringi_1.4.6????? openxlsx_4.1.4???? grid_4.0.0
>      >>> ????????? tools_4.0.0 [46] magrittr_1.5
>      >>> tibble_3.0.1?????? pacman_0.5.1 crayon_1.3.4
>      >>> car_3.0-7 [51] pkgconfig_2.0.3??? ellipsis_0.3.0
>      >>> Matrix_1.2-18 data.table_1.12.8? assertthat_0.2.1 [56]
>      >>> httr_1.4.1???????? rstudioapi_0.11??? R6_2.4.1
>      >>> compiler_4.0.0
>      >>>
>      >>>
>      >>> Thanks in advance,
>      >>>
>      >>> Rui Barradas
>      >>>
>      >>> ______________________________________________
>      >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
>      >>> more, see https://stat.ethz.ch/mailman/listinfo/r-help
>      >>> PLEASE do read the posting guide
>      >>> http://www.R-project.org/posting-guide.html and provide
>      >>> commented, minimal, self-contained, reproducible code.
>      >>>
>      >>
> 
>      > ______________________________________________
>      > R-help at r-project.org mailing list -- To UNSUBSCRIBE and
>      > more, see https://stat.ethz.ch/mailman/listinfo/r-help
>      > PLEASE do read the posting guide
>      > http://www.R-project.org/posting-guide.html and provide
>      > commented, minimal, self-contained, reproducible code.
>


From kry|ov@r00t @end|ng |rom gm@||@com  Fri May  8 18:08:09 2020
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Fri, 8 May 2020 19:08:09 +0300
Subject: [R] Rtools virus
In-Reply-To: <CF2FB038-0258-4698-B10B-0435C592BF5A@dcn.davis.ca.us>
References: <ME2PR01MB3714417631C9A5EC7B3C434C9BA40@ME2PR01MB3714.ausprd01.prod.outlook.com>
 <CF2FB038-0258-4698-B10B-0435C592BF5A@dcn.davis.ca.us>
Message-ID: <20200508190809.463228d0@trisector>

On Fri, 08 May 2020 07:58:34 -0700
Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:

>no one here can follow up on your report because you did not say
>precisely which website you downloaded it from

To be fair, the SHA-256 sum in the VirusTotal report matches the one of
rtools40-x86_64.exe:

wget -qO- \
 https://cran.r-project.org/bin/windows/Rtools/rtools40-x86_64.exe | \
 sha256sum -
# 5c10d60e73dd0186e8f886ef0b9388bb7dbdfdc17366c14c16183edb08fdb58a  -

But I do agree that this is most likely a false positive: the AV engine
that detected it seems to be one of the less widely used and the virus
description [*] is as generic as it gets (if one is to believe Google
Translate).

-- 
Best regards,
Ivan

[*]
http://virusinfo.jiangmin.com/queryInfo.asp?virword=Trojan.Pincav.aml


From murdoch@dunc@n @end|ng |rom gm@||@com  Fri May  8 18:22:24 2020
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Fri, 8 May 2020 12:22:24 -0400
Subject: [R] Bug in function arguments autocomplete and ellipsis?
In-Reply-To: <3E8A851C-9211-4CDE-B9B6-35DAF25828C9@yonsei.ac.kr>
References: <3E8A851C-9211-4CDE-B9B6-35DAF25828C9@yonsei.ac.kr>
Message-ID: <60784bd5-81e7-8a24-da9d-d43f3864d987@gmail.com>

On 07/05/2020 3:46 a.m., Roman Olson wrote:
> Dear All,
> 
> I am wondering whether function arguments autocomplete causes a bug when additional ellipsis arguments are used.
> 
> Example:
> a = function(robot) {
>      cat(robot, "\n")
>   }
> a(r=5) prints 5, meaning that r is autocompleted to ?robot?. Not sure if this is normal behavior, but this does not cause problems.
> 
> This would be fine, but somehow causes problems when there is an additional ellipsis argument that can be used to autocomplete an already existing argument. In the example below, when we are calling sens.analysis.all, everything starting with q is an additional argument (?). Now, k is missed completely. And that is because there is another actual argument that starts with k ? key.legend.axes (it is assigned to 4 instead). If ?k=4? is changed to ?kk=4? the problem disappears.
> 
> sens.analysis.all <- function(func, outgrid, parvec, parmin, parmax,
>                                length.pgrid, outvname, zlim, plabs,
>                                gridlab, mylog, outlog=FALSE, ytick=NULL,
>                                xline=NULL, yline=NULL, mypal=topo.colors,
>                                key.legend.axes=NULL, plot.guidance=FALSE, ... ) {
> 
>     cat(..1, "\n")
>     cat(..2, "\n")
>     cat(..3, "\n")
>   
>     out=1
>     out
> }
> 
> out = sens.analysis.all(numer.wait.times.wrps, NA,
>       c(5.4, 0.008, 1.5), c(4.9, 0.0079, 2.0), c(5.5, 0.0090, 3.5),
>         length.pgrid=10, outvname="cvs", zlim=c(0.3, 2),
>         plabs=c("mu", "lambda", "b"), gridlab="Soil Moisture [mm]",
>         mylog=FALSE, outlog=FALSE, yline=2, plot.guidance=FALSE, q=3.1, k=4, y.c=670,
>         realgrid=seq(630, 670, by=4), myseed=0,
>         nt=1000000, burnin=300000, bin.cutoff=500,
>         bdW.prelim=prec.struct.Mal)

I'm afraid I don't understand what you find surprising here.  In your 
first example, the "r" gets attached to "robot" because of partial 
matching.  In the second example, the "k" gets attached to 
"key.legend.axes" for the same reason.  This is expected behaviour.  If 
you don't want to allow  partial matching to arguments, they need to be 
placed *after* the ellipsis.  Those arguments will need to be spelled 
out in full.

Duncan Murdoch


From bgunter@4567 @end|ng |rom gm@||@com  Fri May  8 18:24:46 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 8 May 2020 09:24:46 -0700
Subject: [R] Bug in function arguments autocomplete and ellipsis?
In-Reply-To: <3E8A851C-9211-4CDE-B9B6-35DAF25828C9@yonsei.ac.kr>
References: <3E8A851C-9211-4CDE-B9B6-35DAF25828C9@yonsei.ac.kr>
Message-ID: <CAGxFJbT3vbOoe-OHWwDGAC9fcuyE8C0tQVqKp9kg6uv+Nd0vOg@mail.gmail.com>

It would help if you consulted the docs, in this case, **The R
Language Definition** and, in particular, 4.3.2 on argument matching.
I won't repeat what it is there, but I believe it will suffice to
dispel your confusion.


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Fri, May 8, 2020 at 7:49 AM Roman Olson <romanolson at yonsei.ac.kr> wrote:
>
> Dear All,
>
> I am wondering whether function arguments autocomplete causes a bug when additional ellipsis arguments are used.
>
> Example:
> a = function(robot) {
>     cat(robot, "\n")
>  }
> a(r=5) prints 5, meaning that r is autocompleted to ?robot?. Not sure if this is normal behavior, but this does not cause problems.
>
> This would be fine, but somehow causes problems when there is an additional ellipsis argument that can be used to autocomplete an already existing argument. In the example below, when we are calling sens.analysis.all, everything starting with q is an additional argument (?). Now, k is missed completely. And that is because there is another actual argument that starts with k ? key.legend.axes (it is assigned to 4 instead). If ?k=4? is changed to ?kk=4? the problem disappears.
>
> sens.analysis.all <- function(func, outgrid, parvec, parmin, parmax,
>                               length.pgrid, outvname, zlim, plabs,
>                               gridlab, mylog, outlog=FALSE, ytick=NULL,
>                               xline=NULL, yline=NULL, mypal=topo.colors,
>                               key.legend.axes=NULL, plot.guidance=FALSE, ... ) {
>
>    cat(..1, "\n")
>    cat(..2, "\n")
>    cat(..3, "\n")
>
>    out=1
>    out
> }
>
> out = sens.analysis.all(numer.wait.times.wrps, NA,
>      c(5.4, 0.008, 1.5), c(4.9, 0.0079, 2.0), c(5.5, 0.0090, 3.5),
>        length.pgrid=10, outvname="cvs", zlim=c(0.3, 2),
>        plabs=c("mu", "lambda", "b"), gridlab="Soil Moisture [mm]",
>        mylog=FALSE, outlog=FALSE, yline=2, plot.guidance=FALSE, q=3.1, k=4, y.c=670,
>        realgrid=seq(630, 670, by=4), myseed=0,
>        nt=1000000, burnin=300000, bin.cutoff=500,
>        bdW.prelim=prec.struct.Mal)
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jrkr|de@u @end|ng |rom gm@||@com  Fri May  8 19:34:13 2020
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Fri, 8 May 2020 13:34:13 -0400
Subject: [R] Question about package "SentimentAnalysis"
In-Reply-To: <CAGN=ytNHHmuP3Jn_6UAaaN9c3HQCDuEMG6JYRc4OeKjs+4_8Uw@mail.gmail.com>
References: <CAGN=ytPU_WAtzC6XDO_MMePRrtEn1TL+N-kmA+E_76KkNza1kA@mail.gmail.com>
 <CAGN=ytNHHmuP3Jn_6UAaaN9c3HQCDuEMG6JYRc4OeKjs+4_8Uw@mail.gmail.com>
Message-ID: <CAKZQJMCo8vZM8E514_zmXdZ++sR7dXWK7oC+Nd2OWb1Z2iuQCw@mail.gmail.com>

I think your best bet is to ask the author/maintainer, Stefan Feuerriegel
,about this. The reference manual
https://cran.r-project.org/web/packages/SentimentAnalysis/SentimentAnalysis.pdf
gives his email address as <sentiment at sfeuerriegel.com>

On Fri, 8 May 2020 at 08:32, Mehdi Dadkhah <mehdidadkhah91 at gmail.com> wrote:

> Hi,
> I hope you are doing well!
> I read a vignette (
>
> https://cran.r-project.org/web/packages/SentimentAnalysis/vignettes/SentimentAnalysis.html
> )
> about interested package, "SentimentAnalysis". But i faced with a question.
> In mentioned  vignette, the sentiment has been applied on a sentence or
> multiple sentences separately. Can this package calculate sentiment
> direction/score for a long texts?
> for example:
>
> # Create a vector of strings
> documents <- "Wow, I really like the new light sabers!That book was
> excellent.R is a fantastic language.The service in this restaurant was
> miserable.This is neither positive or negative."
>
> # Analyze sentiment
> sentiment <- analyzeSentiment(documents)
>
> Many thanks!
> With best regards,
>
> --
> *Mehdi Dadkhah*
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
John Kane
Kingston ON Canada

	[[alternative HTML version deleted]]


From g@v@n@mcgr@th @end|ng |rom dbc@@w@@gov@@u  Fri May  8 17:08:05 2020
From: g@v@n@mcgr@th @end|ng |rom dbc@@w@@gov@@u (Gavan McGrath)
Date: Fri, 8 May 2020 15:08:05 +0000
Subject: [R] Rtools virus
In-Reply-To: <CF2FB038-0258-4698-B10B-0435C592BF5A@dcn.davis.ca.us>
References: <ME2PR01MB3714417631C9A5EC7B3C434C9BA40@ME2PR01MB3714.ausprd01.prod.outlook.com>,
 <CF2FB038-0258-4698-B10B-0435C592BF5A@dcn.davis.ca.us>
Message-ID: <ME2PR01MB371426B0F3130404AE66052D9BA20@ME2PR01MB3714.ausprd01.prod.outlook.com>

Thanks Jeff,
It was downloaded from https://cran.r-project.org/bin/windows/Rtools/

Kind Regards,
Gavan

Dr Gavan McGrath, PhD, B.E.

Research Scientist
Biodiversity and Conservation Science
Department of Biodiversity, Conservation and Attractions
Street Address: 17 Dick Perry Avenue, Kensington,
Postal Address Locked Bag 104, Bentley Delivery Centre, WA 6983
Phone: +618 9219 9447 Mobile: +61 458 559 765
Email: gavan.mcgrath at dbca.wa.gov.au

________________________________
From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
Sent: Friday, May 8, 2020 10:58:34 PM
To: r-help at r-project.org <r-help at r-project.org>; Gavan McGrath <gavan.mcgrath at dbca.wa.gov.au>; r-help at R-project.org <r-help at R-project.org>
Subject: Re: [R] Rtools virus

[External Email] This email was sent from outside the department ? be cautious, particularly with links and attachments.

Sorry to hear that. It is most likely a false positive (antivirus software has little incentive to minimise false positives), but no one here can follow up on your report because you did not say precisely which website you downloaded it from.

On May 5, 2020 8:50:12 PM PDT, Gavan McGrath <gavan.mcgrath at dbca.wa.gov.au> wrote:
>Hi,
>My IT department instructed me to uninstall Windows 64-bit:
>rtools40-x86_64.exe as it contained a virus which they identified at
>
>https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.virustotal.com%2Fgui%2Ffile%2F5c10d60e73dd0186e8f886ef0b9388bb7dbdfdc17366c14c16183edb08fdb58a%2Fdetection&amp;data=02%7C01%7Cgavan.mcgrath%40dbca.wa.gov.au%7Cd11b1d11755f4072370108d7f3604a58%7C7b934664cdcf4e28a3ee1a5bcca0a1b6%7C1%7C0%7C637245467257277714&amp;sdata=KQOX0z8gqcwhZaR9CUNpDMN1WCMTOFeqzafAoiQEYIw%3D&amp;reserved=0
>
>Kind Regards,
>
>Dr Gavan McGrath, PhD, B.E.
>
>Research Scientist
>Biodiversity and Conservation Science
>Department of Biodiversity, Conservation and Attractions
>Street Address: 17 Dick Perry Avenue, Kensington, WA 6151, Australia
>Postal Address Locked Bag 104, Bentley Delivery Centre, WA 6983,
>Australia
>Phone: +618 9219 9447 Mobile: +61 458 559 765
>Email: gavan.mcgrath at dbca.wa.gov.au
>
>Adjunct Research Fellow
>School of Agriculture and Environment
>The University of Western Australia
>Perth, Western Australia
>Email: gavan.mcgrath at uwa.edu.au
>
>________________________________
>This message is confidential and is intended for the recipient named
>above. If you are not the intended recipient, you must not disclose,
>use or copy the message or any part of it. If you received this message
>in error, please notify the sender immediately by replying to this
>message, then delete it from your system.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=02%7C01%7Cgavan.mcgrath%40dbca.wa.gov.au%7Cd11b1d11755f4072370108d7f3604a58%7C7b934664cdcf4e28a3ee1a5bcca0a1b6%7C1%7C0%7C637245467257287711&amp;sdata=GDLcw1UMHpiVcfAWoqdNaloY1phY%2FI0mMPaiX8TaJr8%3D&amp;reserved=0
>PLEASE do read the posting guide
>https://aus01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=02%7C01%7Cgavan.mcgrath%40dbca.wa.gov.au%7Cd11b1d11755f4072370108d7f3604a58%7C7b934664cdcf4e28a3ee1a5bcca0a1b6%7C1%7C0%7C637245467257287711&amp;sdata=isvQYq8ImiPnxS0OoMae5eGCzx6M9EXs85iVpEQ52uk%3D&amp;reserved=0
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.
________________________________
This message is confidential and is intended for the recipient named above. If you are not the intended recipient, you must not disclose, use or copy the message or any part of it. If you received this message in error, please notify the sender immediately by replying to this message, then delete it from your system.

	[[alternative HTML version deleted]]


From m@rc_@chw@rtz @end|ng |rom me@com  Fri May  8 17:11:15 2020
From: m@rc_@chw@rtz @end|ng |rom me@com (Marc Schwartz)
Date: Fri, 8 May 2020 11:11:15 -0400
Subject: [R] Function Hints in Mac Dark Mode
In-Reply-To: <32244604-925F-4F40-A200-3F1AA6D95D36@dcn.davis.ca.us>
References: <1E9E4AA6-D87A-46C2-A585-459645F6EF6D@unomaha.edu>
 <32244604-925F-4F40-A200-3F1AA6D95D36@dcn.davis.ca.us>
Message-ID: <769FC136-30D0-4151-98AB-E29C22A4D6F7@me.com>



> On May 8, 2020, at 11:02 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
> You seem to be confusing R and RStudio... so yeah, wrong mailing list. I don't know exactly where you should post either. Perhaps the GitHub issues page for RStudio?
> 
> On May 6, 2020 12:54:43 PM PDT, Andrew Swift via R-help <r-help at r-project.org> wrote:
>> Sorry, wasn?t sure exactly where to post this but I noticed that with R
>> 4.0.0 when running a Mac in Dark Mode that the Function Hints at the
>> bottom of the Console and Editor windows become invisible.  
>> Thanks. 


Hi,

Actually, I am not sure that is the case. 

See the attached screen capture of the default R.app (which I do not use) when the desktop is set to dark mode, which I also do not use.

When in light mode, those hints in the lower left hand corner are black, and they switch to white in dark mode, making them almost impossible to see.

For Andrew, if this is correct, and you are not referring to RStudio per Jeff, you should post this to r-sig-mac:
  
  https://stat.ethz.ch/mailman/listinfo/r-sig-mac

Otherwise, if it is RStudio, they have their own support here:

   https://support.rstudio.com/hc/en-us

Regards,

Marc Schwartz





From r@turner @end|ng |rom @uck|@nd@@c@nz  Sat May  9 00:11:42 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sat, 9 May 2020 10:11:42 +1200
Subject: [R] 
 Warning in install.packages : converting NULL pointer to R NULL
In-Reply-To: <bbd798c8-eaa7-1b8d-cb39-7e8926600520@sapo.pt>
References: <bbd798c8-eaa7-1b8d-cb39-7e8926600520@sapo.pt>
Message-ID: <3e3568fa-3b32-0d33-0418-12026bce7628@auckland.ac.nz>


Hi Rui.  Doesn't happen to me under Ubuntu 18.04:

> install.packages('cowplot',lib=.Rlib) 
> trying URL 'https://cloud.r-project.org/src/contrib
>/cowplot_1.0.0.tar.gz'
> Content type 'application/x-gzip' length 1275585 bytes (1.2 MB)
> ==================================================
> downloaded 1.2 MB
> 
> * installing *source* package ?cowplot? ...
> ** package ?cowplot? successfully unpacked and MD5 sums checked
> ** using staged installation
> ** R
> ** inst
> ** byte-compile and prepare package for lazy loading
> ** help
> *** installing help indices
> *** copying figures
> ** building package indices
> ** installing vignettes
> ** testing if installed package can be loaded from temporary location
> ** testing if installed package can be loaded from final location
> ** testing if installed package keeps a record of temporary installation path
> * DONE (cowplot)
> 
> The downloaded source packages are in
> 	?/tmp/RtmpG8wb97/downloaded_packages?

My session info is as follows:

> sessionInfo()
> R version 4.0.0 (2020-04-24)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Ubuntu 18.04.4 LTS
> 
> Matrix products: default
> BLAS:   /usr/lib/x86_64-linux-gnu/atlas/libblas.so.3.10.3
> LAPACK: /usr/lib/x86_64-linux-gnu/atlas/liblapack.so.3.10.3
> 
> Random number generation:
>  RNG:     Mersenne-Twister 
>  Normal:  Inversion 
>  Sample:  Rounding 
>  
> locale:
>  [1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C              
>  [3] LC_TIME=en_NZ.UTF-8        LC_COLLATE=en_GB.UTF-8    
>  [5] LC_MONETARY=en_NZ.UTF-8    LC_MESSAGES=en_GB.UTF-8   
>  [7] LC_PAPER=en_NZ.UTF-8       LC_NAME=C                 
>  [9] LC_ADDRESS=C               LC_TELEPHONE=C            
> [11] LC_MEASUREMENT=en_NZ.UTF-8 LC_IDENTIFICATION=C       
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base     
> 
> other attached packages:
> [1] brev_0.0-4
> 
> loaded via a namespace (and not attached):
> [1] compiler_4.0.0 tools_4.0.0  

No idea what to suggest.  Sorry.

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From jrkr|de@u @end|ng |rom gm@||@com  Fri May  8 19:35:57 2020
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Fri, 8 May 2020 13:35:57 -0400
Subject: [R] Need a suggestion on a package to make a figure
In-Reply-To: <216384015.2752852.1588857789484@mail.yahoo.com>
References: <216384015.2752852.1588857789484.ref@mail.yahoo.com>
 <216384015.2752852.1588857789484@mail.yahoo.com>
Message-ID: <CAKZQJMA0AKKWx4NEDcik66EmM9LzSZVKWdzoeVugdPFsVD7YWw-4623@mail.gmail.com>

What are you doing?

 http://adv-r.had.co.nz/Reproducibility.html

http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

On Fri, 8 May 2020 at 10:50, aiguo li via R-help <r-help at r-project.org>
wrote:

> Hello all,
> I need to make a table with a value imaged by greater than certain value
> as attached.  Could you give me a suggestions on which R package will be
> good for this type of table?
> Thanks and stay safe!
> Aiguo______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
John Kane
Kingston ON Canada

	[[alternative HTML version deleted]]


From rom@no|@on @end|ng |rom yon@e|@@c@kr  Sat May  9 04:38:17 2020
From: rom@no|@on @end|ng |rom yon@e|@@c@kr (Roman Olson)
Date: Sat, 9 May 2020 11:38:17 +0900
Subject: [R] Bug in function arguments autocomplete and ellipsis?
In-Reply-To: <CAGxFJbT3vbOoe-OHWwDGAC9fcuyE8C0tQVqKp9kg6uv+Nd0vOg@mail.gmail.com>
References: <3E8A851C-9211-4CDE-B9B6-35DAF25828C9@yonsei.ac.kr>
 <CAGxFJbT3vbOoe-OHWwDGAC9fcuyE8C0tQVqKp9kg6uv+Nd0vOg@mail.gmail.com>
Message-ID: <9095577E-093D-489C-9C8D-56F5D5588260@yonsei.ac.kr>

Dear All,

Thanks for clarifying this. Now I know that this is expected behavior, and will try to place ? before the rest of the arguments.

Stay home and stay safe!
-Roman

> 2020. 5. 9. ?? 1:24, Bert Gunter <bgunter.4567 at gmail.com> ??:
> 
> The R
> Language Definition


	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Sat May  9 05:28:13 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sat, 9 May 2020 13:28:13 +1000
Subject: [R] 
 [R ] Writing loop to estimate ARCH test for a multiple columns
 of a data frame?
In-Reply-To: <CAOFE=kO6Pz=vqOR6+mBvjkMMBMhv9bX9s9nUCRnBoF9dGkdiVA@mail.gmail.com>
References: <CAOFE=kMaSgqy7_dMsB3WtTgN+BDa4b5m28GO0uxm0rUsDmqc9g@mail.gmail.com>
 <CA+8X3fWOMFQ8Ptz75j8DjhJuaEKi8nR1p+82LeNqTcB2NfSjKA@mail.gmail.com>
 <CAOFE=kOtwzrr3cFXhvvS+db+iw=p4eg1Fm6JCGtCzwessQ90FA@mail.gmail.com>
 <CA+8X3fUB8Mp9Pn8RO7PB3knoEgLHEdT6LuwxyxPbNMvQNoVG5A@mail.gmail.com>
 <CAOFE=kNNFOy=GCSWidG+Fds4qi5r7w9M9SafZ+oqcQw-xeKmkg@mail.gmail.com>
 <CA+8X3fXcztWSqhUiKAS+uc+fH57RCV7jN5ihLeRbz5rUgkp28g@mail.gmail.com>
 <CAOFE=kO6Pz=vqOR6+mBvjkMMBMhv9bX9s9nUCRnBoF9dGkdiVA@mail.gmail.com>
Message-ID: <CA+8X3fVgYkZ00gqqVhdaE7Nn=VvQ_x=eoBdjqBKsmALsTGfB5A@mail.gmail.com>

Hi Subhamitra,
I have washed the dishes and had a night's sleep, so I can now deal with
your text munging problem. First, I'll reiterate the solution I sent:

sp_8_5<-read.table("sp_8_5.tab",sep="\t",
 header=TRUE,stringsAsFactors=FALSE)
library(tseries)
library(FinTS)
# create a function that returns only the
# statistic and p.value as a string
archStatP<-function(x) {
 archout<-ArchTest(x)
 # I have truncated the values here
 return(sprintf("ChiSq = %.1f, p = %.3f",archout$statistic,archout$p.value))
}
# using "lapply", run the test on each column
spout<-lapply(sp_8_5[,2:13],archStatP)

If you look at "spout" you will see that it is a list of 12 character
strings. I arranged this as you seem to want the contents of a 3x4 table in
a document. This is one way to do it, there are others.

First, create a text table of the desired dimensions. I'll do it with loops
as you seem to be familiar with them:

# open a text file
sink("sp_8_5.txt")
for(row in 0:2) {
 for(column in 1:4)
  cat(spout[[column+row*4]],ifelse(column < 4,"\t","\n"))
}
sink()

If you open this file in a text editor (e.g. Notepad) you will see that it
contains 3 lines (rows), each with four TAB separated strings. Now to
import this into a word processing document. I don't have MS Word, so I'll
do it with Libre Office Writer and hope that the procedure is similar.

Move to where you want the table in your document
Select Insert|Text from file from the top menu
Select (highlight) the text you have imported
Select Convert|Text to table from the top menu

The highlighted area should become a table. I had to reduce the font size
from 12 to 10 to get the strings to fit into the cells.

There are probably a few more changes that you will want, so let me know if
you strike trouble.

Jim


On Fri, May 8, 2020 at 11:28 PM Subhamitra Patra <subhamitra.patra at gmail.com>
wrote:

> Dear Sir,
>
> Thank you very much for your wonderful suggestion for my problem. Your
> suggested code has excellently worked and successfully extracted the
> statistics and p-value in another R object.
>
> Concerning your last suggestion, I attempted to separate the strings with
> TAB character in the "spout" object by using different alternative packages
> like dplyr, tidyr, qdap, ans also by using split,strsplit function so that
> can export the statistics and p-values for each column to excel, and later
> to the MSword file, but got the below error.
>
> By using the  split function, I wrote the code as,
> *string[] split = s.Split(spout, '\t')*
> where I got the following errors.
> Error: unexpected symbol in "string[] split"
> Error: unexpected symbol in "string[[]]split"
> Error in strsplit(row, "\t") : non-character argument
>
> Then I tried with  strsplit function by the below code
> *strsplit(spout, split)*
> But, got the below error as
> Error in as.character(split) :
>   cannot coerce type 'closure' to vector of type 'character'.
>
> Then used dplyr and tidyr package and the wrote the below code
> library(dplyr)
> library(tidyr)
> *separate(spout,value,into=c(?ChiSq?,?p?),sep=?,?)*
> *separate(spout,List of length 12,into=c(?ChiSq?,?p?),sep="\t")*
> But, got the errors as,
> Error: unexpected input in "separate(spout,value,into=c(?"
> Error: unexpected symbol in "separate(spout,List of"
>
> Then used qdap package with the code below
>
> *colsplit2df(spout,, c("ChiSq", "p"), ",")*
> *colsplit2df(spout,, c("ChiSq", "p"), sep = "\t")*
> But got the following errors
> Error in dataframe[, splitcol] : incorrect number of dimensions
> In addition: Warning message:
> In colsplit2df_helper(dataframe = dataframe, splitcol = splitcols[i],  :
>   dataframe object is not of the class data.frame
> Error in dataframe[, splitcol] : incorrect number of dimensions
> In addition: Warning message:
> In colsplit2df_helper(dataframe = dataframe, splitcol = splitcols[i],  :
>   dataframe object is not of the class data.frame
>
> Sir, please suggest me where I am going wrong in the above to separate
> string in the "spout" object.
>
> Thank you very much for your help.
>
> [image: Mailtrack]
> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
> notified by
> Mailtrack
> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 05/08/20,
> 06:51:46 PM
>
> On Fri, May 8, 2020 at 4:47 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>
>> 1) In general, *apply functions return a list with the number of elements
>> equal to the number of columns or other elements of the input data. You can
>> assign that list as I have to "spout" in the first example.
>>
>> 2) spout<-list() assigns the name "spout" to an empty list. As we are
>> processing columns 2 to 12 of the input data, spout[[i-1]] assigns the
>> results to elements 1 to 11 of the list "spout". Just a low trick.
>>
>> 1a) Yes, you can create a "wrapper" function that will return only the
>> statistic and p.value.
>>
>> # create a function that returns only the
>> # statistic and p.value as a string
>> archStatP<-function(x) {
>>  archout<-ArchTest(x)
>>  return(sprintf("ChiSq = %f, p = %f",archout$statistic,archout$p.value))
>> }
>> # using "lapply", run the test on each column
>> spout<-lapply(sp_8_5[,2:12],archStatP)
>>
>> Note that I should have used "lapply". I didn't check the output
>> carefully enough.
>>
>> 2a) Now you only have to separate the strings in "spout" with TAB
>> characters and import the result into Excel. I have to wash the dishes, so
>> you're on your own.
>>
>> Jim
>>
>> On Fri, May 8, 2020 at 8:26 PM Subhamitra Patra <
>> subhamitra.patra at gmail.com> wrote:
>>
>>> Dear Sir,
>>>
>>> Thank you very much for such an excellent solution to my problem. I was
>>> trying sapply function since last days, but was really unable to write
>>> properly. Now, I understood my mistake in using sapply function in the
>>> code. Therefore, I have two queries regarding this which I want to discuss
>>> here just for my learning purpose.
>>>
>>> 1. While using sapply function for estimating one method across the
>>> columns of a data frame, one needs to define the list of the output table
>>> after using sapply so that the test results for each column will be
>>> consistently stored in an output object, right?
>>>
>>> 2. In the spout<- list() command, what spout[[i-1]]  indicates?
>>>
>>> Sir, one more possibility which I would like to ask related to my above
>>> problem just to learn for further R programming language.
>>>
>>> After running your suggested code, all the results for each column are
>>> being stored in the spout object. From this, I need only the statistics and
>>> P-value for each column. So, my queries are:
>>>
>>> 1. Is there any way to extract only two values (i.e., statistics and
>>> p-value) for each column that stored in spout object and save these two
>>> values in another R data frame for each column?
>>>  or
>>> 2. Is there any possibility that the statistics and p-value
>>> calculated for each column can directly export to a word file in a table
>>> format (having 4 columns and 3 rows). In particular, is it possible to
>>> extract both statistic and p-value results for each column to an MS word
>>> file with the format of A1, A2, A3, A4 column results in 1st row, A5, A6,
>>> A7, A8 column results in 2nd row, and A9, A10, A11, A12 column results in
>>> the 3rd row of the table?
>>>
>>>
>>> Like before, your suggestion will definitely help me to learn the
>>> advanced R language.
>>>
>>> Thank you very much for your help.
>>>
>>> [image: Mailtrack]
>>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
>>> notified by
>>> Mailtrack
>>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 05/08/20,
>>> 03:47:26 PM
>>>
>>> On Fri, May 8, 2020 at 2:37 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>>>
>>>> Hi Subhamitra,
>>>> This isn't too hard:
>>>>
>>>> # read in the sample data that was
>>>> # saved in the file "sp_8_5.tab"
>>>> sp_8_5<-read.table("sp_8_5.tab",sep="\t",
>>>>  header=TRUE,stringsAsFactors=FALSE)
>>>> library(tseries)
>>>> library(FinTS)
>>>> # using "sapply", run the test on each column
>>>> spout<-sapply(sp_8_5[,2:12],ArchTest)
>>>>
>>>> The list "spout" contains the test results. If you really want to use a
>>>> loop:
>>>>
>>>> spout<-list()
>>>> for(i in 2:12) spout[[i-1]]<-ArchTest(sp_8_5[,i])
>>>>
>>>> Jim
>>>>
>>>>
>>>> On Fri, May 8, 2020 at 5:27 PM Subhamitra Patra <
>>>> subhamitra.patra at gmail.com> wrote:
>>>>
>>>>> Dear Sir,
>>>>>
>>>>> Herewith I am pasting a part of my sample data having 12 columns
>>>>> below, and want to calculate ARCH test for the 12 columns by using a loop.
>>>>>
>>>>>
>>>
>>> --
>>> *Best Regards,*
>>> *Subhamitra Patra*
>>> *Phd. Research Scholar*
>>> *Department of Humanities and Social Sciences*
>>> *Indian Institute of Technology, Kharagpur*
>>> *INDIA*
>>>
>>
>
> --
> *Best Regards,*
> *Subhamitra Patra*
> *Phd. Research Scholar*
> *Department of Humanities and Social Sciences*
> *Indian Institute of Technology, Kharagpur*
> *INDIA*
>

	[[alternative HTML version deleted]]


From mehd|d@dkh@h91 @end|ng |rom gm@||@com  Sat May  9 05:33:09 2020
From: mehd|d@dkh@h91 @end|ng |rom gm@||@com (Mehdi Dadkhah)
Date: Sat, 9 May 2020 08:03:09 +0430
Subject: [R] Question about package "SentimentAnalysis"
In-Reply-To: <CAKZQJMCo8vZM8E514_zmXdZ++sR7dXWK7oC+Nd2OWb1Z2iuQCw@mail.gmail.com>
References: <CAGN=ytPU_WAtzC6XDO_MMePRrtEn1TL+N-kmA+E_76KkNza1kA@mail.gmail.com>
 <CAGN=ytNHHmuP3Jn_6UAaaN9c3HQCDuEMG6JYRc4OeKjs+4_8Uw@mail.gmail.com>
 <CAKZQJMCo8vZM8E514_zmXdZ++sR7dXWK7oC+Nd2OWb1Z2iuQCw@mail.gmail.com>
Message-ID: <CAGN=ytOWE5S1joNu1yOpjep=EU7C1rmhpL1uQSwPKRW4gugo4g@mail.gmail.com>

Thank you!

On Fri, May 8, 2020 at 10:04 PM John Kane <jrkrideau at gmail.com> wrote:

> I think your best bet is to ask the author/maintainer, Stefan Feuerriegel
> ,about this. The reference manual
> https://cran.r-project.org/web/packages/SentimentAnalysis/SentimentAnalysis.pdf
> gives his email address as <sentiment at sfeuerriegel.com>
>
> On Fri, 8 May 2020 at 08:32, Mehdi Dadkhah <mehdidadkhah91 at gmail.com>
> wrote:
>
>> Hi,
>> I hope you are doing well!
>> I read a vignette (
>>
>> https://cran.r-project.org/web/packages/SentimentAnalysis/vignettes/SentimentAnalysis.html
>> )
>> about interested package, "SentimentAnalysis". But i faced with a
>> question.
>> In mentioned  vignette, the sentiment has been applied on a sentence or
>> multiple sentences separately. Can this package calculate sentiment
>> direction/score for a long texts?
>> for example:
>>
>> # Create a vector of strings
>> documents <- "Wow, I really like the new light sabers!That book was
>> excellent.R is a fantastic language.The service in this restaurant was
>> miserable.This is neither positive or negative."
>>
>> # Analyze sentiment
>> sentiment <- analyzeSentiment(documents)
>>
>> Many thanks!
>> With best regards,
>>
>> --
>> *Mehdi Dadkhah*
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>
> --
> John Kane
> Kingston ON Canada
>


-- 
*Mehdi Dadkhah*

	[[alternative HTML version deleted]]


From mehd|d@dkh@h91 @end|ng |rom gm@||@com  Sat May  9 09:20:32 2020
From: mehd|d@dkh@h91 @end|ng |rom gm@||@com (Mehdi Dadkhah)
Date: Sat, 9 May 2020 11:50:32 +0430
Subject: [R] Question about package "SentimentAnalysis"
In-Reply-To: <5d06afeb-9a0f-4536-81c0-8dc1ee6b8c11@email.android.com>
References: <CAGN=ytOWE5S1joNu1yOpjep=EU7C1rmhpL1uQSwPKRW4gugo4g@mail.gmail.com>
 <5d06afeb-9a0f-4536-81c0-8dc1ee6b8c11@email.android.com>
Message-ID: <CAGN=ytOw_BoVX9PH=3niT7n+BmkX7MUFY01kJz8C+r-2MUDo9w@mail.gmail.com>

Thank you!

On Sat, May 9, 2020 at 11:44 AM <cpolwart at chemo.org.uk> wrote:

> Or, to split the paragraph into sentences, analyse each sentence and
> decide how to agregate the result...
>
>
>
> On 9 May 2020 04:33, Mehdi Dadkhah <mehdidadkhah91 at gmail.com> wrote:
>
> Thank you!
>
> On Fri, May 8, 2020 at 10:04 PM John Kane <jrkrideau at gmail.com> wrote:
>
> > I think your best bet is to ask the author/maintainer, Stefan
> Feuerriegel
> > ,about this. The reference manual
> >
> https://cran.r-project.org/web/packages/SentimentAnalysis/SentimentAnalysis.pdf
> > gives his email address as <sentiment at sfeuerriegel.com>
> >
> > On Fri, 8 May 2020 at 08:32, Mehdi Dadkhah <mehdidadkhah91 at gmail.com>
> > wrote:
> >
> >> Hi,
> >> I hope you are doing well!
> >> I read a vignette (
> >>
> >>
> https://cran.r-project.org/web/packages/SentimentAnalysis/vignettes/SentimentAnalysis.html
> >> )
> >> about interested package, "SentimentAnalysis". But i faced with a
> >> question.
> >> In mentioned  vignette, the sentiment has been applied on a sentence or
> >> multiple sentences separately. Can this package calculate sentiment
> >> direction/score for a long texts?
> >> for example:
> >>
> >> # Create a vector of strings
> >> documents <- "Wow, I really like the new light sabers!That book was
> >> excellent.R is a fantastic language.The service in this restaurant was
> >> miserable.This is neither positive or negative."
> >>
> >> # Analyze sentiment
> >> sentiment <- analyzeSentiment(documents)
> >>
> >> Many thanks!
> >> With best regards,
> >>
> >> --
> >> *Mehdi Dadkhah*
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> >
> > --
> > John Kane
> > Kingston ON Canada
> >
>
>
> --
> *Mehdi Dadkhah*
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>

-- 
*Mehdi Dadkhah*

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sat May  9 05:08:10 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 8 May 2020 20:08:10 -0700
Subject: [R] [R-pkgs] addScales package available on CRAN
Message-ID: <CAGxFJbSMF=+Xk6L0jBFRxEELJ1cLqUhmJJJJxM_sy5XnwxbcBg@mail.gmail.com>

addScales is a small package that modifies trellis objects created
using lattice graphics by adding horizontal and/or vertical reference
lines that provide visual scaling information. This is mostly useful
for multi-panel plots that use the relation = 'free' option in their
'scales' argument list. Examples show when and how this might aid data
visualization.

Please feel free to contact me with suggestions for improvement.

Bert Gunter

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n|  Fri May  8 20:30:12 2020
From: wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n| (Viechtbauer, Wolfgang (SP))
Date: Fri, 8 May 2020 18:30:12 +0000
Subject: [R] [R-pkgs] mathjaxr: Using 'Mathjax' in Rd Files
Message-ID: <986895facff141379e5d379a666e5bcb@UM-MAIL3214.unimaas.nl>

Dear All,

I would like to announce the release of the 'mathjaxr' package: 

https://cran.r-project.org/package=mathjaxr

The package provides MathJax and a handful of macros to enable its use within Rd files for rendering equations in the HTML help files.

Package authors wanting to make use of the package and its functionality need to:

1. install the mathjaxr package,
2. add mathjaxr to 'Suggests' or 'Imports' in the DESCRIPTION file of their package,
3. add mathjaxr to 'RdMacros' in the DESCRIPTION file of their package (or add 'RdMacros: mathjaxr' if the DESCRIPTION file does not yet contain a 'RdMacros' entry),

One can then enable the use of MathJax by calling the \loadmathjax macro within the \description{} section of an .Rd file.

An inline equation can then be added with the \mjeqn{latex}{ascii} macro, with the LaTeX commands for the equation given between the first set of curly brackets (which will be rendered in the HTML and PDF help pages) and the plain-text version of the equation given between the second set of curly brackets (which will be shown in the plain text help). With the \mjdeqn{latex}{ascii} macro, one can add 'displayed equations' (as in LaTeX's displaymath environment). Single argument versions of these macros, namely \mjseqn{latexascii} and \mjsdeqn{latexascii}, are also available.

I hope package authors will find this useful for providing fully rendered equations not only in the PDF manual, but also in the HTML help files (which the vast majority of users are probably looking at).

Feedback, comments, suggestions more than welcome. For bug reports, please go to:

https://github.com/wviechtb/mathjaxr/issues

Best,
Wolfgang

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t  Sat May  9 11:32:25 2020
From: @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t (Stefano Sofia)
Date: Sat, 9 May 2020 09:32:25 +0000
Subject: [R] Change the colours of some of the labels of the x-axis
In-Reply-To: <dff646d6-dc28-5258-b300-275c82a9dfca@sapo.pt>
References: <8B435C9568170B469AE31E8891E8CC4F809B8064@ESINO.regionemarche.intra>,
 <dff646d6-dc28-5258-b300-275c82a9dfca@sapo.pt>
Message-ID: <8B435C9568170B469AE31E8891E8CC4F809B83CF@ESINO.regionemarche.intra>

Thank you very much.

Stefano

         (oo)
--oOO--( )--OOo----------------
Stefano Sofia PhD
Civil Protection - Marche Region
Meteo Section
Snow Section
Via del Colle Ameno 5
60126 Torrette di Ancona, Ancona
Uff: 071 806 7743
E-mail: stefano.sofia at regione.marche.it
---Oo---------oO----------------

________________________________________
Da: Rui Barradas [ruipbarradas at sapo.pt]
Inviato: gioved? 7 maggio 2020 17.41
A: Stefano Sofia; r-help at r-project.org
Oggetto: Re: [R] Change the colours of some of the labels of the x-axis

Hello,

You cannot pass a vector of colors to col.axis, you need to plot the
axis twice, once the normal axis, like in your code, then overplot just
those last 2 labels.


xlabels <- seq(trunc(as.POSIXct(first_day, format="%Y-%m-%d-%H-%M"),
"days"), trunc(as.POSIXct(last_day, format="%Y-%m-%d-%H-%M"), "days"),
by="12 hours")

plot(df_plot$data_POSIX, df_plot$hs1, type="b",
      xlim = c(min(xlabels), max(xlabels)), ylim=c(0, 10), col="blue",
xaxt="n", axes=F, main="", xlab="", ylab="Snow height")
lines(df_plot$data_POSIX, df_plot$hs2, ylim=c(0, 10), col="red", type="b")
axis.POSIXct(1, at = xlabels, format="h%H-%d-%m-%y", pos=0, las = 2)
axis.POSIXct(1, at = tail(xlabels, 2),
              format="h%H-%d-%m-%y", pos=0, las = 2, col.axis = "red")

axis(side=2, at=seq(0, 15, 5))



Hope this helps,

Rui Barradas

?s 15:09 de 07/05/20, Stefano Sofia escreveu:
> Dear R users,
> in a plot I need to change the colours of some of the labels of the x-axis.
> In the example below reported, I would like to have the labels of the last and second-last date in red.
> I tried to find the solution searching the web, but I could not understand the hints and I was not sure I could adapt them to my example.
> There might be an easy way to do that?
>
> Thank you for your attention and your help
> Stefano
>
> first_day <- "2005-01-23-09-00"
> last_day <- "2005-01-27-09-00"
> first_day_POSIX <- as.POSIXct(first_day, format="%Y-%m-%d-%H-%M")
> last_day_POSIX <- as.POSIXct(last_day, format="%Y-%m-%d-%H-%M")
> df_plot <- data.frame(data_POSIX=seq(first_day_POSIX, last_day_POSIX, by="30 mins"))
> df_plot$hs1 <- 5
> df_plot$hs2 <- 7
>
> plot(df_plot$data_POSIX, df_plot$hs1, type="b", ylim=c(0, 10), col="blue", xaxt="n", axes=F, main="", xlab="", ylab="Snow height")
> lines(df_plot$data_POSIX, df_plot$hs2, ylim=c(0, 10), col="red", type="b")
> axis.POSIXct(1, at=seq(trunc(as.POSIXct(first_day, format="%Y-%m-%d-%H-%M"), "days"), trunc(as.POSIXct(last_day, format="%Y-%m-%d-%H-%M"), "days"), by="12 hours"), format="h%H-%d-%m-%y", pos=0)
> axis(side=2, at=seq(0, 15, 5))
>
>
>           (oo)
> --oOO--( )--OOo----------------
> Stefano Sofia PhD
> Civil Protection - Marche Region
> Meteo Section
> Snow Section
> Via del Colle Ameno 5
> 60126 Torrette di Ancona, Ancona
> Uff: 071 806 7743
> E-mail: stefano.sofia at regione.marche.it
> ---Oo---------oO----------------
>
> ________________________________
>
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
>
> --
> Questo messaggio  stato analizzato da Libra ESVA ed  risultato non infetto.
> This message was scanned by Libra ESVA and is believed to be clean.
>
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

--

Questo messaggio  stato analizzato con Libra ESVA ed  risultato non infetto.


________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.

--
Questo messaggio  stato analizzato da Libra ESVA ed  risultato non infetto.
This message was scanned by Libra ESVA and is believed to be clean.


From mehd|d@dkh@h91 @end|ng |rom gm@||@com  Sat May  9 12:06:30 2020
From: mehd|d@dkh@h91 @end|ng |rom gm@||@com (Mehdi Dadkhah)
Date: Sat, 9 May 2020 14:36:30 +0430
Subject: [R] genericSummary in LSAfun
In-Reply-To: <CAGN=ytM4+Cn_egGauCt+hMZ4jX0OVQNacaLtgvHzmpfPy5WFbw@mail.gmail.com>
References: <CAGN=ytM4+Cn_egGauCt+hMZ4jX0OVQNacaLtgvHzmpfPy5WFbw@mail.gmail.com>
Message-ID: <CAGN=ytM6=ntjuRX_q7pbaYFzb5HbiV8VwR5i6JoZB_FRiX9_7w@mail.gmail.com>

Hi,
I hope you are doing well!
I have a data frame with a column. it contains about 140 posts. In each
row, there is a blog post. I named this data frame as "posttext". When i
use  genericSummary() function, it returns paragraph instead sentences.
What is problem?


Command which i use:

summaryp<-genericSummary(posttext,k=1,split=c(".","!","?"),min=5,breakdown=FALSE)



Many thanks!
With best regards,

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Sat May  9 13:24:40 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sat, 9 May 2020 21:24:40 +1000
Subject: [R] Need a suggestion on a package to make a figure
In-Reply-To: <216384015.2752852.1588857789484@mail.yahoo.com>
References: <216384015.2752852.1588857789484.ref@mail.yahoo.com>
 <216384015.2752852.1588857789484@mail.yahoo.com>
Message-ID: <CA+8X3fUBHezzeg4Rict5EA-WD6Vr+=64rH6W2DAvNzzKjdjL6Q@mail.gmail.com>

Hi Aiguo
Just for fun, I tried to work out what you wanted. I think you are
looking for a function that creates a horizontal stacked bar image of
a logical vector:

TFbar<-function(x,file="TFbar",grDev="png",width=100,height=25,
 col=c("green","red")) {

 do.call(grDev,
  list(file=paste(file,grDev,sep="."),width=width,height=height))
 par(xaxs="i",mar=c(0,0,0,0))
 plot(0,xlim=c(1,width),ylim=c(1,height),type="n",axes=FALSE)
 Twidth<-width*sum(x)/length(x)
 Fwidth<-width-Twidth
 rect(1,1,Twidth,height,col=col[1])
 rect(Twidth,1,width,height,col=col[2])
 dev.off()
 return(Twidth/width)
}

To instantiate this with your example:

TFnames<-paste0("Pathway",1:4)
for(i in 1:4) {
 assign("x",sample(c(TRUE,FALSE),100,TRUE))
 TFbar(x,TFnames[i])
}
TFtags<-paste0("<img src=",TFnames,".png>")
TFdf<-data.frame(GO_Terms=TFnames,S1=TFtags,stringsAsFactors=FALSE)
library(prettyR)
delim.table(TFdf,filename="TFtable.htm",leading.delim=FALSE,
 label="",html=TRUE,show.rownames=FALSE)

Sadly, I found a bug in delim.table that will be corrected in the next
version. The attached HTML file shows the table you get with the
corrected version.

Jim

On Sat, May 9, 2020 at 12:50 AM aiguo li via R-help
<r-help at r-project.org> wrote:
>
> Hello all,
> I need to make a table with a value imaged by greater than certain value as attached.  Could you give me a suggestions on which R package will be good for this type of table?
> Thanks and stay safe!
> Aiguo______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

From drj|m|emon @end|ng |rom gm@||@com  Sat May  9 13:26:28 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sat, 9 May 2020 21:26:28 +1000
Subject: [R] Need a suggestion on a package to make a figure
In-Reply-To: <CA+8X3fUBHezzeg4Rict5EA-WD6Vr+=64rH6W2DAvNzzKjdjL6Q@mail.gmail.com>
References: <216384015.2752852.1588857789484.ref@mail.yahoo.com>
 <216384015.2752852.1588857789484@mail.yahoo.com>
 <CA+8X3fUBHezzeg4Rict5EA-WD6Vr+=64rH6W2DAvNzzKjdjL6Q@mail.gmail.com>
Message-ID: <CA+8X3fXTJFwUMMvC_sxbwAmv_n+_=3zKUag-LcHeCoZipNgswg@mail.gmail.com>

Hi again,
Oops, you'll need the images as well. Save the HTML file and images to
the same directory.

Jim

On Sat, May 9, 2020 at 9:24 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi Aiguo
> Just for fun, I tried to work out what you wanted. I think you are
> looking for a function that creates a horizontal stacked bar image of
> a logical vector:
>
> TFbar<-function(x,file="TFbar",grDev="png",width=100,height=25,
>  col=c("green","red")) {
>
>  do.call(grDev,
>   list(file=paste(file,grDev,sep="."),width=width,height=height))
>  par(xaxs="i",mar=c(0,0,0,0))
>  plot(0,xlim=c(1,width),ylim=c(1,height),type="n",axes=FALSE)
>  Twidth<-width*sum(x)/length(x)
>  Fwidth<-width-Twidth
>  rect(1,1,Twidth,height,col=col[1])
>  rect(Twidth,1,width,height,col=col[2])
>  dev.off()
>  return(Twidth/width)
> }
>
> To instantiate this with your example:
>
> TFnames<-paste0("Pathway",1:4)
> for(i in 1:4) {
>  assign("x",sample(c(TRUE,FALSE),100,TRUE))
>  TFbar(x,TFnames[i])
> }
> TFtags<-paste0("<img src=",TFnames,".png>")
> TFdf<-data.frame(GO_Terms=TFnames,S1=TFtags,stringsAsFactors=FALSE)
> library(prettyR)
> delim.table(TFdf,filename="TFtable.htm",leading.delim=FALSE,
>  label="",html=TRUE,show.rownames=FALSE)
>
> Sadly, I found a bug in delim.table that will be corrected in the next
> version. The attached HTML file shows the table you get with the
> corrected version.
>
> Jim
>
> On Sat, May 9, 2020 at 12:50 AM aiguo li via R-help
> <r-help at r-project.org> wrote:
> >
> > Hello all,
> > I need to make a table with a value imaged by greater than certain value as attached.  Could you give me a suggestions on which R package will be good for this type of table?
> > Thanks and stay safe!
> > Aiguo______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Pathway1.png
Type: image/png
Size: 169 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200509/3837b0d1/attachment.png>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Pathway2.png
Type: image/png
Size: 167 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200509/3837b0d1/attachment-0001.png>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Pathway3.png
Type: image/png
Size: 168 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200509/3837b0d1/attachment-0002.png>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Pathway4.png
Type: image/png
Size: 170 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200509/3837b0d1/attachment-0003.png>

From |8|5h9 @end|ng |rom gm@||@com  Sat May  9 12:06:30 2020
From: |8|5h9 @end|ng |rom gm@||@com (=?utf-8?Q?Fernando_A_L=C3=B3pez_Hern=C3=A1ndez?=)
Date: Sat, 9 May 2020 12:06:30 +0200
Subject: [R] [R-pkgs] New version 'spsur' for the estimation of Spatial
 Seemingly Unrelated Regression
Message-ID: <10919E94-320E-43E7-9C78-1032E95915EA@gmail.com>

Dear all
I am pleased to announce that the R package ?spsur? for the estimation of Spatial Seemingly Unrelated Regression upload a new version in the CRAN. 

https://cran.r-project.org/web/packages/spsur/index.html <https://cran.r-project.org/web/packages/spsur/index.html>
This version of ?spsur? has been adapted to the standard arguments used in similar packages (spatialreg, spdep, splm, sphet) and include new functionalities.

A new user guide is available on: https://cran.r-project.org/web/packages/spsur/vignettes/vig1.html <https://cran.r-project.org/web/packages/spsur/vignettes/vig1.html>
I hope you find interest this package for you research or for your PhD students. 
Fernando A. L?pez Hern?ndez
fernando.lopez at upct.es <mailto:fernando.lopez at upct.es>
http://metodos.upct.es/falopez/ <http://metodos.upct.es/falopez/>
	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From kobyeongm|n @end|ng |rom kore@@@c@kr  Sat May  9 11:45:34 2020
From: kobyeongm|n @end|ng |rom kore@@@c@kr (=?UTF-8?B?4oCN6rOg67OR66+8WyDtlZnrtoDsnqztlZkgLyDqsr3soJztlZnqs7wgXQ==?=)
Date: Sat, 9 May 2020 11:45:34 +0200
Subject: [R] solve() function freezes CLI in GNU R 3.6.3
Message-ID: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>

Dear list,

there is a bug with the *solve() *function that I cannot find a solution
for a month. So I ask for your help.

*Whenever I try to invert a matrix using the said function, the console
hangs*.
Below I explain more about this situation.

Consider the code

D = matrix(
data = c(1, 2, 3, 4),
nrow = 2,
ncol = 2,
byrow = TRUE)
solve(D)

1. *If I launch the code in R called from a terminal, say, Konsole, the
session will freeze.*
  * I know that the exact timing of the system freeze is when I execute the
solve( ) function.
  * According to htop, one of my CPU core is used by 100% when this happens.

2. *If I launch the same code within RStudio, the code works as expected.*
However, if I call it using the terminal inside RStudio, the session hangs.
  * If the solve() function is used within RMarkdown document, the session
will freeze and the document will not be generated.

3. Launching R with --vanilla does not resolve the issue.

4. Rebooting the PC, using my external graphic card, reinstalling the
r-base-core package in apt, and trying with different terminal emulators do
not help.

5. From the documentation of the solve( ) function in R, it can be seen
that solve(A, B) actually takes two arguments: A is a matrix, and B a
vector or a matrix. If B is a vector, it solves the linear system Ax = B.
If B is a matrix, it solves AX = B and returns X. If nothing is given in
the second argument, it automatically assumes identity matrix of
appropriate size as B. **The first function of solving linear system
works.** If I specify matrices as the second argument, however, the same
problem happens.

6. Using QR decomposition with qr.solve(A) still works well.

Here are my questions:

*1. Has anyone had the same problem as me?*
*2. I also seek recommendations on how to fix this issue.*

For your information, I am using R version 3.6.3 installed from the default
apt repository. Here is the output of which R and R -- version:

    kobyeongmin at odie:~$ which R
    /usr/bin/R

    kobyeongmin at odie:~$ R --version
    R version 3.6.3 (2020-02-29) -- "Holding the Windsock"
    Copyright (C) 2020 The R Foundation for Statistical Computing
    Platform: x86_64-pc-linux-gnu (64-bit)

    R is free software and comes with ABSOLUTELY NO WARRANTY.
    You are welcome to redistribute it under the terms of the
    GNU General Public License versions 2 or 3.
    For more information about these matters see
    https://www.gnu.org/licenses/.

Thank you for reading this, and stay safe!

Best regards

Ko Byeongmin

	[[alternative HTML version deleted]]


From jo@o||gue|r@@ @end|ng |rom gm@||@com  Sat May  9 13:29:32 2020
From: jo@o||gue|r@@ @end|ng |rom gm@||@com (=?UTF-8?Q?Jo=c3=a3o_Marreiros?=)
Date: Sat, 9 May 2020 13:29:32 +0200
Subject: [R] ggplot2 axis
Message-ID: <61f15ea8-0794-0a01-90e6-937ceae15f45@gmail.com>

Dear users,
Does anyone had a problem with ggplot concerning the axis not being 
shown? (see attachment)

ggplot(db, aes (x = lenght, y = width, color = support)) +
 ? geom_point(size=2) +
 ? stat_ellipse() +
 ? labs(x="Lenght (mm)", y="width (mm)", title="Boxplot", color = "Support")

I'm using the following version: ggplot2 3.3.0.

Thank you for your help.

Joao M.



-------------- next part --------------
A non-text attachment was scrubbed...
Name: Rplot.png
Type: image/png
Size: 58980 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200509/37ac2fc2/attachment.png>

From edd @end|ng |rom deb|@n@org  Sat May  9 17:30:17 2020
From: edd @end|ng |rom deb|@n@org (Dirk Eddelbuettel)
Date: Sat, 9 May 2020 10:30:17 -0500
Subject: [R] solve() function freezes CLI in GNU R 3.6.3
In-Reply-To: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
References: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
Message-ID: <24246.52361.934223.41992@rob.eddelbuettel.com>


We can see that you use Linux.

Are you by chance

 - on a Debian or Ubuntu system, and 
 - have the libopenblas package installed ?

If so then it is a known bug with the libopenblas0-pthread package.

Installing libopen0-openmp (and also removing libopenblas0-pthread) should
fix it.

This is likely CPU dependent. But we need more info. Can you maybe come to the
r-sig-debian list (subscription needed to reduce spam) and we continue there?

Dirk


-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Sat May  9 17:33:01 2020
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Sat, 9 May 2020 17:33:01 +0200
Subject: [R] A stopifnot() nastiness, even if not a bug
In-Reply-To: <24212.52203.211594.608671@stat.math.ethz.ch>
References: <CAGxFJbSAA1t7GF6=2Sv+43syUQtjq1pY-DBppsqE+OE5Rnvp5Q@mail.gmail.com>
 <24212.11722.632757.269523@stat.math.ethz.ch>
 <AA9D2F47-252B-4D37-9F75-027818AE810E@gmail.com>
 <24212.23386.542411.93591@stat.math.ethz.ch>
 <93676b28-7b79-b70e-f2e4-1f1527fb80e6@fredhutch.org>
 <CAF8bMcZD12+L606rwH7+KRUCqzpEFTrnkbm09u+MWGxhSrh5FQ@mail.gmail.com>
 <24212.52203.211594.608671@stat.math.ethz.ch>
Message-ID: <24246.52525.875294.14177@stat.math.ethz.ch>

>>>>> Martin Maechler 
>>>>>     on Mon, 13 Apr 2020 22:30:35 +0200 writes:

>>>>> William Dunlap 
>>>>>     on Mon, 13 Apr 2020 09:57:11 -0700 writes:

    >> You can avoid the problem in Martin's example by only giving scalars to
    >> stopifnot().  E.g., using stopifnot(all(x>0)) or stopifnot(length(x)==1,
    x> 0) instead of stopifnot(x>0).  I think having stopifnot call
    >> all(predicate) if length(predicate)!=1 was probably a mistake.

> well, maybe.

> As I brought up the 0-length example:  One could think of making
> an exception for  logical(0)  and treat that as non-TRUE.

> (for R-devel only, [......])

> Martin

I have been a bit sad that nobody (not even Herv?) reacted to my
proposal, 4 weeks ago.

As I agree that it is safer for stopifnot() to be less lenient
here, and not allow the usual behavior of logical(0) to be
treated as TRUE, namely  as in   all(logical(0))  |-->  TRUE ,
I had actually implemented the above proposal in my own version of R-devel,
(but not committed!), nicely introducing a new optional argument
'allow.logical0'  where

- allow.logical0 = FALSE  is the new default

- allow.logical0 = TRUE   is back compatible

What I found is that this (not back compatible) change lead to a
few test breakages also in R & recommended packages, and IIRC in
a few of my own packages.  Still probably only in about 1 in 1000 
of the stopifnot cases, but in practically all cases, the breakage was
"wrong" in the sense that {conceptual example}

      stopifnot( f1(x) == f2(x) )

should test (almost, say apart from names(.)) identical behavior
of f1() and f2()  and that would naturally also extend to the
case of 0-extent 'x'.

So I had to change the above (half a dozen, say) cases to

    stopifnot( f1(x) == f2(x) , allow.logical0 = TRUE)

to keep the test working as it was intended to.
The nice thing about the change is that it is also working in
current versions of R  where  allow.logical is not a special
argument and just treated as part of '...' and is it TRUE, does
not change the semantic of stopifnot() in current (possibly,
then, "previous") versions of R.

Overall I think it may be a good idea to consider this
not-back-compatible change to  stopifnot()
  (if only to get Herv? into continue using it ! ;-) ;-))

BUT  I assume quite a few other people may have to get used to
see the following error in their stopifnot() code and will have
to add  occasional   'allow.logical0 = TRUE'  to those cases the
old behavior was really the intended one.

(I will have to finally get Matrix 1.3-0 released to CRAN
 before committing the change to R-devel,  and I may also ask
 help of someone to check all CRAN/Bioc against that change so I
 can alert package authors who need to adapt).

Please let us know your thoughts on this.

Martin



    >> On Mon, Apr 13, 2020 at 9:28 AM Herv? Pag?s <hpages at fredhutch.org> wrote:

    >>> 
    >>> 
    >>> On 4/13/20 05:30, Martin Maechler wrote:
    >>> >>>>>> peter dalgaard
    >>> >>>>>>      on Mon, 13 Apr 2020 12:00:38 +0200 writes:
    >>> >
    >>> >      > Inline...
    >>> >      >> On 13 Apr 2020, at 11:15 , Martin Maechler <
    >>> maechler at stat.math.ethz.ch> wrote:
    >>> >      >>
    >>> >      >>>>>>> Bert Gunter
    >>> >      >>>>>>> on Sun, 12 Apr 2020 16:30:09 -0700 writes:
    >>> >      >>
    >>> >      >>> Don't know if this has come up before, but ...
    >>> >      >>>> x <- c(0,0)
    >>> >      >>>> length(x)
    >>> >      >>> [1] 2
    >>> >      >>> ## but
    >>> >      >>>> stopifnot(length(x))
    >>> >      >>> Error: length(x) is not TRUE
    >>> >      >>> Called from: top level
    >>> >      >>> ## but
    >>> >      >>>> stopifnot(length(x) > 0)  ## not an error;  nor is
    >>> >      >>>> stopifnot(as.logical(length(x)))
    >>> >      >>> ## Ouch!
    >>> >      >>
    >>> >      >>> Maybe the man page should say something about not assuming
    >>> automatic
    >>> >      >>> coercion to logical, which is the usual expectation. Or fix
    >>> this.
    >>> >      >>
    >>> >      >>> Bert Gunter
    >>> >      >>
    >>> >      >> Well, what about the top most paragraph of the help page is not
    >>> clear here ?
    >>> >      >>
    >>> >      >>> Description:
    >>> >      >>
    >>> >      >>> If any of the expressions (in '...' or 'exprs') are not 'all'
    >>> >      >>> 'TRUE', 'stop' is called, producing an error message indicating
    >>> >      >>> the _first_ expression which was not ('all') true.
    >>> >      >>
    >>> >
    >>> >      > This, however, is somewhat less clear:
    >>> >
    >>> >      > ..., exprs: any number of (typically but not necessarily
    >>> ?logical?) R
    >>> >      > expressions, which should each evaluate to (a logical vector
    >>> >      > of all) ?TRUE?.  Use _either_ ?...? _or_ ?exprs?, the latter
    >>> >
    >>> >      > What does it mean, "typically but not necessarily ?logical?"?
    >>> >
    >>> > That's a good question: The '(....)' must have been put there a while
    >>> ago.
    >>> > I agree that it's not at all helpful. Strictly, we are really
    >>> > dealing with unevaluated expressions anyway ("promises"), but
    >>> > definitely all of them must evaluate to logical (vector or
    >>> > array..) of all TRUE values.  In the very beginning of
    >>> > stopifnot(), I had thought that it should also work in other
    >>> > cases, e.g.,  for    Matrix(TRUE, 4,5)  {from the Matrix package} etc,
    >>> > but several use cases had convinced us / me that stopifnot
    >>> > should be stricter...
    >>> >
    >>> >      > The code actually tests explicitly with is.logical, as far as I
    >>> can tell.
    >>> >
    >>> >      > This creates a discrepancy between if(!...)stop(...) and
    >>> stopifnot(),
    >>> >
    >>> > yes indeed, on purpose now, for a very long time ...
    >>> >
    >>> > There's another discrepancy, more dangerous I think,
    >>> > as shown in the following
    >>> > {Note this discrepancy has been noted for a long time .. also on
    >>> >   this R-devel list} :
    >>> >
    >>> >    m <- matrix(1:12, 3,4)
    >>> >    i <- (1:4) %% 2 == 1  & (0:3) %% 5 == 0
    >>> >
    >>> >    stopifnot(dim(m[,i]) == c(3,1))       # seems fine
    >>> >
    >>> >    if(dim(m[,i]) != c(3,1)) stop("wrong dim") # gives an error (but not
    >>> ..)
    >>> 
    >>> mmh... that is not good. I was under the impression that we could at
    >>> least expect 'stopifnot(x)' to be equivalent to 'if (!isTRUE(x))
    >>> stop(...)'. I'll have to revisit my use of stopifnot() in many many
    >>> places... again :-/ Or may be just stop using it and use 'if
    >>> (!isTRUE(...))' instead.
    >>> 
    >>> H.
    >>> 
    >>> >
    >>> >
    >>> > Martin
    >>> >
    >>> >      >> as in
    >>> >      >> f <- function (x) if (!x) stop(paste(deparse(substitute(x)), "is
    >>> not TRUE"))
    >>> >      >> f(0)
    >>> >      > Error in f(0) : 0 is not TRUE
    >>> >      >> f(1)
    >>> >      >> stopifnot(0)
    >>> >      > Error: 0 is not TRUE
    >>> >      >> stopifnot(1)
    >>> >      > Error: 1 is not TRUE
    >>> >
    >>> >      > -pd
    >>> >
    >>> >
    >>> >      >> If useR's expectations alone would guide the behavior of a
    >>> >      >> computer language, the language would have to behave
    >>> >      >> "personalized" and give different results depending on the user,
    >>> >      >> which may be desirable in medicine or psychotherapy but not with
    >>> R.

    >>> >      >> Martin


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Sat May  9 17:37:36 2020
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Sat, 9 May 2020 17:37:36 +0200
Subject: [R] Error in summary.warnings?
In-Reply-To: <24245.31929.569398.561576@stat.math.ethz.ch>
References: <BL0PR01MB43069FB2C1BC342FFDC19142EFA50@BL0PR01MB4306.prod.exchangelabs.com>
 <24245.31929.569398.561576@stat.math.ethz.ch>
Message-ID: <24246.52800.589144.210912@stat.math.ethz.ch>

>>>>> Martin Maechler 
>>>>>     on Fri, 8 May 2020 17:37:29 +0200 writes:

>>>>> Allison Meisner 
>>>>>     on Thu, 7 May 2020 19:32:36 +0000 writes:

    > I believe there is an error in the summary.warnings function (typically called via 'summary(warnings())'). Below is a minimal working example:

    > #########

    > testfunction <- function(x){
    >  if(x > 30){
    >      warning("A big problem (should be 20 of these)")
    >  }else{
    >      warning("Bigger problem (should be 30 of these)")
    >  }
    > }

    > for(i in 1:50){
    >     testfunction(i)
    > }

    > summary(warnings())

    > #########

    > I checked the code for summary.warnings:

    > function (object, ...)
    > {
    >  msgs <- names(object)
    >  calls <- as.character(object)
    >  ss <- ": "
    >  c.m. <- paste(calls, msgs, sep = ss)
    >  if(length(i.no.call <- which(calls == "NULL")))
    >     c.m.[i.no.call] <- substr(c.m.[i.no.call],
    > 				  nchar(paste0("NULL", ss))+1L, 100000L)
    >  tm <- table(c.m., deparse.level = 0L)
    >  structure(unique(object), counts = as.vector(tm), class = "summary.warnings")
    > }


    >> The problem appears to be in the last line: unique preserves the order of the input, but counts reflects the counts in the table tm, which is a problem because table names are in alphabetical order.

I've committed the fix.  If you are interested,
I've replaced the last 2 lines with

    i.uniq <- which(!duplicated(object, incomparables=FALSE))
    tm <- table(factor(c.m., levels=c.m.[i.uniq]), deparse.level=0L)
    structure(object[i.uniq], counts = as.vector(tm), class = "summary.warnings")


which (at least conceptually) should even be faster the previous code.

Thank you again,
Martin

    >> Am I missing something?

    > No -- I think you are perfect and I was very imperfect ;-)  when
    > I created and tested the function ..

    > This will be fixed in the next versions of R.

    > Thank you very much for the report  and the nice concise
    > reproducible example!

    > Best regards,
    > Martin

    >> Allison
    >> ----------
    >> Allison Meisner, PhD
    >> Postdoctoral Fellow
    >> Department of Biostatistics
    >> Johns Hopkins Bloomberg School of Public Health
    >> 615 N. Wolfe Street
    >> Baltimore, MD 21205

    > Martin Maechler
    > ETH Zurich  and   R Core team

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.


From pro|jcn@@h @end|ng |rom gm@||@com  Sat May  9 17:40:32 2020
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Sat, 9 May 2020 11:40:32 -0400
Subject: [R] solve() function freezes CLI in GNU R 3.6.3
In-Reply-To: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
References: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
Message-ID: <7754f2dc-33a5-ff7b-4867-ff0a3fe0e613@gmail.com>

I get the output at the bottom, which seems OK.

Can you include sessionInfo() output?
Possibly this is a quirk of the particular distro or machine, BLAS or LAPACK,
or something in your workspace. However, if we have full information, someone may be
able to run the same setup in a VM (if I have the .iso and can find
a way to set up R 3.6.3 easily, I'll be willing). It may be that you should
reinstall libblas or liblapack as one thing to try.

JN

Here's my output:

> D = matrix(
+ data = c(1, 2, 3, 4),
+ nrow = 2,
+ ncol = 2,
+ byrow = TRUE)
> solve(D)
     [,1] [,2]
[1,] -2.0  1.0
[2,]  1.5 -0.5
> D %*% solve(D)
     [,1]         [,2]
[1,]    1 1.110223e-16
[2,]    0 1.000000e+00
> sessionInfo()
R version 4.0.0 (2020-04-24)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Linux Mint 19.3

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1
LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.7.1

locale:
 [1] LC_CTYPE=en_CA.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_CA.UTF-8        LC_COLLATE=en_CA.UTF-8
 [5] LC_MONETARY=en_CA.UTF-8    LC_MESSAGES=en_CA.UTF-8
 [7] LC_PAPER=en_CA.UTF-8       LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_CA.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] compiler_4.0.0
>

On 2020-05-09 5:45 a.m., ????[ ???? / ???? ] wrote:
> Dear list,
> 
> there is a bug with the *solve() *function that I cannot find a solution
> for a month. So I ask for your help.
> 
> *Whenever I try to invert a matrix using the said function, the console
> hangs*.
> Below I explain more about this situation.
> 
> Consider the code
> 
> D = matrix(
> data = c(1, 2, 3, 4),
> nrow = 2,
> ncol = 2,
> byrow = TRUE)
> solve(D)
> 
> 1. *If I launch the code in R called from a terminal, say, Konsole, the
> session will freeze.*
>   * I know that the exact timing of the system freeze is when I execute the
> solve( ) function.
>   * According to htop, one of my CPU core is used by 100% when this happens.
> 
> 2. *If I launch the same code within RStudio, the code works as expected.*
> However, if I call it using the terminal inside RStudio, the session hangs.
>   * If the solve() function is used within RMarkdown document, the session
> will freeze and the document will not be generated.
> 
> 3. Launching R with --vanilla does not resolve the issue.
> 
> 4. Rebooting the PC, using my external graphic card, reinstalling the
> r-base-core package in apt, and trying with different terminal emulators do
> not help.
> 
> 5. From the documentation of the solve( ) function in R, it can be seen
> that solve(A, B) actually takes two arguments: A is a matrix, and B a
> vector or a matrix. If B is a vector, it solves the linear system Ax = B.
> If B is a matrix, it solves AX = B and returns X. If nothing is given in
> the second argument, it automatically assumes identity matrix of
> appropriate size as B. **The first function of solving linear system
> works.** If I specify matrices as the second argument, however, the same
> problem happens.
> 
> 6. Using QR decomposition with qr.solve(A) still works well.
> 
> Here are my questions:
> 
> *1. Has anyone had the same problem as me?*
> *2. I also seek recommendations on how to fix this issue.*
> 
> For your information, I am using R version 3.6.3 installed from the default
> apt repository. Here is the output of which R and R -- version:
> 
>     kobyeongmin at odie:~$ which R
>     /usr/bin/R
> 
>     kobyeongmin at odie:~$ R --version
>     R version 3.6.3 (2020-02-29) -- "Holding the Windsock"
>     Copyright (C) 2020 The R Foundation for Statistical Computing
>     Platform: x86_64-pc-linux-gnu (64-bit)
> 
>     R is free software and comes with ABSOLUTELY NO WARRANTY.
>     You are welcome to redistribute it under the terms of the
>     GNU General Public License versions 2 or 3.
>     For more information about these matters see
>     https://www.gnu.org/licenses/.
> 
> Thank you for reading this, and stay safe!
> 
> Best regards
> 
> Ko Byeongmin
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From edd @end|ng |rom deb|@n@org  Sat May  9 18:35:12 2020
From: edd @end|ng |rom deb|@n@org (Dirk Eddelbuettel)
Date: Sat, 9 May 2020 11:35:12 -0500
Subject: [R] solve() function freezes CLI in GNU R 3.6.3
In-Reply-To: <24246.52361.934223.41992@rob.eddelbuettel.com>
References: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
 <24246.52361.934223.41992@rob.eddelbuettel.com>
Message-ID: <24246.56256.474662.24178@rob.eddelbuettel.com>


On 9 May 2020 at 10:30, Dirk Eddelbuettel wrote:
| 
| We can see that you use Linux.
| 
| Are you by chance
| 
|  - on a Debian or Ubuntu system, and 
|  - have the libopenblas package installed ?
| 
| If so then it is a known bug with the libopenblas0-pthread package.
| 
| Installing libopen0-openmp (and also removing libopenblas0-pthread) should
| fix it.

That was imprecise. It should read "installing libopenblas-openmp-dev" and
removing both "libopenblas-pthread-dev libopenblas0-pthread".

I cannot reproduce the bug on the hardware I have access to (i5, i7, xeon)
though I was able to a few weeks ago. Maybe something changed already...

There was also a brief thread on the debian-science list (within Debian)
starting with https://lists.debian.org/debian-science/2020/04/msg00081.html
and leading to https://lists.debian.org/debian-science/2020/05/msg00003.html
(and this cross from April to May)

We would need some more information on hardware to chase this.

Dirk
 
| This is likely CPU dependent. But we need more info. Can you maybe come to the
| r-sig-debian list (subscription needed to reduce spam) and we continue there?
| 
| Dirk
| 
| 
| -- 
| http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org
| 
| ______________________________________________
| R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
| https://stat.ethz.ch/mailman/listinfo/r-help
| PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
| and provide commented, minimal, self-contained, reproducible code.

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat May  9 19:00:43 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sat, 9 May 2020 18:00:43 +0100
Subject: [R] ggplot2 axis
In-Reply-To: <61f15ea8-0794-0a01-90e6-937ceae15f45@gmail.com>
References: <61f15ea8-0794-0a01-90e6-937ceae15f45@gmail.com>
Message-ID: <81617bee-5179-3de1-e4b1-aa6827ffc9f8@sapo.pt>

Hello,

I cannot reproduce this. If I do

db <- iris[3:5]
names(db) <- c("length", "width", "support")

and change 'lenght' to 'length' in the ggplot call both axis are plotted.
But the code is not reproducible, db is missing. Can you post the output of


dput(head(db, 30))


in a next mail?

Hope this helps,

Rui Barradas

?s 12:29 de 09/05/20, Jo?o Marreiros escreveu:
> Dear users,
> Does anyone had a problem with ggplot concerning the axis not being 
> shown? (see attachment)
> 
> ggplot(db, aes (x = lenght, y = width, color = support)) +
>  ? geom_point(size=2) +
>  ? stat_ellipse() +
>  ? labs(x="Lenght (mm)", y="width (mm)", title="Boxplot", color = 
> "Support")
> 
> I'm using the following version: ggplot2 3.3.0.
> 
> Thank you for your help.
> 
> Joao M.
> 
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From iuke-tier@ey m@iii@g oii uiow@@edu  Sat May  9 20:13:17 2020
From: iuke-tier@ey m@iii@g oii uiow@@edu (iuke-tier@ey m@iii@g oii uiow@@edu)
Date: Sat, 9 May 2020 13:13:17 -0500 (CDT)
Subject: [R] [External] Re:  A stopifnot() nastiness, even if not a bug
In-Reply-To: <24246.52525.875294.14177@stat.math.ethz.ch>
References: <CAGxFJbSAA1t7GF6=2Sv+43syUQtjq1pY-DBppsqE+OE5Rnvp5Q@mail.gmail.com>
 <24212.11722.632757.269523@stat.math.ethz.ch>
 <AA9D2F47-252B-4D37-9F75-027818AE810E@gmail.com>
 <24212.23386.542411.93591@stat.math.ethz.ch>
 <93676b28-7b79-b70e-f2e4-1f1527fb80e6@fredhutch.org>
 <CAF8bMcZD12+L606rwH7+KRUCqzpEFTrnkbm09u+MWGxhSrh5FQ@mail.gmail.com>
 <24212.52203.211594.608671@stat.math.ethz.ch>
 <24246.52525.875294.14177@stat.math.ethz.ch>
Message-ID: <alpine.DEB.2.21.2005091311510.8504@luke-Latitude-7480>

Since you asked ...

There are two different use cases for stopifnot: in tests and
in implementation code.

stopifnot seems to be optimized for the test code case where speed and
clarity may not be that important.

For expresing assertions in implemantation code clarity and simplicity
are important. What I would want is something equivalent to

stpifnt0 <- function(e)
     if (! isTRUE(e))
         stop(deparse(exprs[i]), "is not true")

(maybe with some additional gettetxt/deparse/call fiddling magic, but
only _after_ the fail, not on the hot path for a passed test.)

I would not want to have to worry about fine print about whether there
is an implicit 'all' for an expression returning a vector, or whether
that is really an 'all' or a variant that treate the empty set
differently.

I might consider a multiargument verison along the lines of

stpfnt <- function(...) {
     exprs <- as.list(substitute(list(...)))[-1]
     for (i in seq_along(exprs))
         if (! isTRUE(...elt(i)))
             stop(deparse(exprs[i]), "is not true")
}

But I think in most cases I rather write two separate assertions.

The advantage of stpfnt0 is code is that it is more concise than
writing out the test and the code to produce an error message,
especially a nice one. But it this comes at a big performance cost
then using it would be harder to jsutify.

With simple semantics like the stpfnt0 definition here the compiler
could be taught to optimize it's use to cost little more than the test
in the passing case. That gets harder the more complexity is added to
stopifnot.

We could add a simpler stopifnot0, but I'm not sure I want to go
there.

Best,

luke

On Sat, 9 May 2020, Martin Maechler wrote:

>>>>>> Martin Maechler
>>>>>>     on Mon, 13 Apr 2020 22:30:35 +0200 writes:
>
>>>>>> William Dunlap
>>>>>>     on Mon, 13 Apr 2020 09:57:11 -0700 writes:
>
>    >> You can avoid the problem in Martin's example by only giving scalars to
>    >> stopifnot().  E.g., using stopifnot(all(x>0)) or stopifnot(length(x)==1,
>    x> 0) instead of stopifnot(x>0).  I think having stopifnot call
>    >> all(predicate) if length(predicate)!=1 was probably a mistake.
>
>> well, maybe.
>
>> As I brought up the 0-length example:  One could think of making
>> an exception for  logical(0)  and treat that as non-TRUE.
>
>> (for R-devel only, [......])
>
>> Martin
>
> I have been a bit sad that nobody (not even Herv?) reacted to my
> proposal, 4 weeks ago.
>
> As I agree that it is safer for stopifnot() to be less lenient
> here, and not allow the usual behavior of logical(0) to be
> treated as TRUE, namely  as in   all(logical(0))  |-->  TRUE ,
> I had actually implemented the above proposal in my own version of R-devel,
> (but not committed!), nicely introducing a new optional argument
> 'allow.logical0'  where
>
> - allow.logical0 = FALSE  is the new default
>
> - allow.logical0 = TRUE   is back compatible
>
> What I found is that this (not back compatible) change lead to a
> few test breakages also in R & recommended packages, and IIRC in
> a few of my own packages.  Still probably only in about 1 in 1000
> of the stopifnot cases, but in practically all cases, the breakage was
> "wrong" in the sense that {conceptual example}
>
>      stopifnot( f1(x) == f2(x) )
>
> should test (almost, say apart from names(.)) identical behavior
> of f1() and f2()  and that would naturally also extend to the
> case of 0-extent 'x'.
>
> So I had to change the above (half a dozen, say) cases to
>
>    stopifnot( f1(x) == f2(x) , allow.logical0 = TRUE)
>
> to keep the test working as it was intended to.
> The nice thing about the change is that it is also working in
> current versions of R  where  allow.logical is not a special
> argument and just treated as part of '...' and is it TRUE, does
> not change the semantic of stopifnot() in current (possibly,
> then, "previous") versions of R.
>
> Overall I think it may be a good idea to consider this
> not-back-compatible change to  stopifnot()
>  (if only to get Herv? into continue using it ! ;-) ;-))
>
> BUT  I assume quite a few other people may have to get used to
> see the following error in their stopifnot() code and will have
> to add  occasional   'allow.logical0 = TRUE'  to those cases the
> old behavior was really the intended one.
>
> (I will have to finally get Matrix 1.3-0 released to CRAN
> before committing the change to R-devel,  and I may also ask
> help of someone to check all CRAN/Bioc against that change so I
> can alert package authors who need to adapt).
>
> Please let us know your thoughts on this.
>
> Martin
>
>
>
>    >> On Mon, Apr 13, 2020 at 9:28 AM Herv? Pag?s <hpages at fredhutch.org> wrote:
>
>    >>>
>    >>>
>    >>> On 4/13/20 05:30, Martin Maechler wrote:
>    >>> >>>>>> peter dalgaard
>    >>> >>>>>>      on Mon, 13 Apr 2020 12:00:38 +0200 writes:
>    >>> >
>    >>> >      > Inline...
>    >>> >      >> On 13 Apr 2020, at 11:15 , Martin Maechler <
>    >>> maechler at stat.math.ethz.ch> wrote:
>    >>> >      >>
>    >>> >      >>>>>>> Bert Gunter
>    >>> >      >>>>>>> on Sun, 12 Apr 2020 16:30:09 -0700 writes:
>    >>> >      >>
>    >>> >      >>> Don't know if this has come up before, but ...
>    >>> >      >>>> x <- c(0,0)
>    >>> >      >>>> length(x)
>    >>> >      >>> [1] 2
>    >>> >      >>> ## but
>    >>> >      >>>> stopifnot(length(x))
>    >>> >      >>> Error: length(x) is not TRUE
>    >>> >      >>> Called from: top level
>    >>> >      >>> ## but
>    >>> >      >>>> stopifnot(length(x) > 0)  ## not an error;  nor is
>    >>> >      >>>> stopifnot(as.logical(length(x)))
>    >>> >      >>> ## Ouch!
>    >>> >      >>
>    >>> >      >>> Maybe the man page should say something about not assuming
>    >>> automatic
>    >>> >      >>> coercion to logical, which is the usual expectation. Or fix
>    >>> this.
>    >>> >      >>
>    >>> >      >>> Bert Gunter
>    >>> >      >>
>    >>> >      >> Well, what about the top most paragraph of the help page is not
>    >>> clear here ?
>    >>> >      >>
>    >>> >      >>> Description:
>    >>> >      >>
>    >>> >      >>> If any of the expressions (in '...' or 'exprs') are not 'all'
>    >>> >      >>> 'TRUE', 'stop' is called, producing an error message indicating
>    >>> >      >>> the _first_ expression which was not ('all') true.
>    >>> >      >>
>    >>> >
>    >>> >      > This, however, is somewhat less clear:
>    >>> >
>    >>> >      > ..., exprs: any number of (typically but not necessarily
>    >>> ?logical?) R
>    >>> >      > expressions, which should each evaluate to (a logical vector
>    >>> >      > of all) ?TRUE?.  Use _either_ ?...? _or_ ?exprs?, the latter
>    >>> >
>    >>> >      > What does it mean, "typically but not necessarily ?logical?"?
>    >>> >
>    >>> > That's a good question: The '(....)' must have been put there a while
>    >>> ago.
>    >>> > I agree that it's not at all helpful. Strictly, we are really
>    >>> > dealing with unevaluated expressions anyway ("promises"), but
>    >>> > definitely all of them must evaluate to logical (vector or
>    >>> > array..) of all TRUE values.  In the very beginning of
>    >>> > stopifnot(), I had thought that it should also work in other
>    >>> > cases, e.g.,  for    Matrix(TRUE, 4,5)  {from the Matrix package} etc,
>    >>> > but several use cases had convinced us / me that stopifnot
>    >>> > should be stricter...
>    >>> >
>    >>> >      > The code actually tests explicitly with is.logical, as far as I
>    >>> can tell.
>    >>> >
>    >>> >      > This creates a discrepancy between if(!...)stop(...) and
>    >>> stopifnot(),
>    >>> >
>    >>> > yes indeed, on purpose now, for a very long time ...
>    >>> >
>    >>> > There's another discrepancy, more dangerous I think,
>    >>> > as shown in the following
>    >>> > {Note this discrepancy has been noted for a long time .. also on
>    >>> >   this R-devel list} :
>    >>> >
>    >>> >    m <- matrix(1:12, 3,4)
>    >>> >    i <- (1:4) %% 2 == 1  & (0:3) %% 5 == 0
>    >>> >
>    >>> >    stopifnot(dim(m[,i]) == c(3,1))       # seems fine
>    >>> >
>    >>> >    if(dim(m[,i]) != c(3,1)) stop("wrong dim") # gives an error (but not
>    >>> ..)
>    >>>
>    >>> mmh... that is not good. I was under the impression that we could at
>    >>> least expect 'stopifnot(x)' to be equivalent to 'if (!isTRUE(x))
>    >>> stop(...)'. I'll have to revisit my use of stopifnot() in many many
>    >>> places... again :-/ Or may be just stop using it and use 'if
>    >>> (!isTRUE(...))' instead.
>    >>>
>    >>> H.
>    >>>
>    >>> >
>    >>> >
>    >>> > Martin
>    >>> >
>    >>> >      >> as in
>    >>> >      >> f <- function (x) if (!x) stop(paste(deparse(substitute(x)), "is
>    >>> not TRUE"))
>    >>> >      >> f(0)
>    >>> >      > Error in f(0) : 0 is not TRUE
>    >>> >      >> f(1)
>    >>> >      >> stopifnot(0)
>    >>> >      > Error: 0 is not TRUE
>    >>> >      >> stopifnot(1)
>    >>> >      > Error: 1 is not TRUE
>    >>> >
>    >>> >      > -pd
>    >>> >
>    >>> >
>    >>> >      >> If useR's expectations alone would guide the behavior of a
>    >>> >      >> computer language, the language would have to behave
>    >>> >      >> "personalized" and give different results depending on the user,
>    >>> >      >> which may be desirable in medicine or psychotherapy but not with
>    >>> R.
>
>    >>> >      >> Martin
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From @me|@ne1 @end|ng |rom jhm|@edu  Sat May  9 18:39:07 2020
From: @me|@ne1 @end|ng |rom jhm|@edu (Allison Meisner)
Date: Sat, 9 May 2020 16:39:07 +0000
Subject: [R] Error in summary.warnings?
In-Reply-To: <24246.52800.589144.210912@stat.math.ethz.ch>
References: <BL0PR01MB43069FB2C1BC342FFDC19142EFA50@BL0PR01MB4306.prod.exchangelabs.com>
 <24245.31929.569398.561576@stat.math.ethz.ch>,
 <24246.52800.589144.210912@stat.math.ethz.ch>
Message-ID: <BL0PR01MB4306CE1175CECA82E17CC48CEFA30@BL0PR01MB4306.prod.exchangelabs.com>

Great! Thanks for sharing your fix.

Allison
________________________________
From: Martin Maechler <maechler at stat.math.ethz.ch>
Sent: Saturday, May 9, 2020 11:37 AM
To: Allison Meisner <ameisne1 at jhmi.edu>; r-help at r-project.org <r-help at r-project.org>
Cc: Martin Maechler <maechler at stat.math.ethz.ch>
Subject: Re: [R] Error in summary.warnings?


>>>>> Martin Maechler
>>>>>     on Fri, 8 May 2020 17:37:29 +0200 writes:

>>>>> Allison Meisner
>>>>>     on Thu, 7 May 2020 19:32:36 +0000 writes:

    > I believe there is an error in the summary.warnings function (typically called via 'summary(warnings())'). Below is a minimal working example:

    > #########

    > testfunction <- function(x){
    >  if(x > 30){
    >      warning("A big problem (should be 20 of these)")
    >  }else{
    >      warning("Bigger problem (should be 30 of these)")
    >  }
    > }

    > for(i in 1:50){
    >     testfunction(i)
    > }

    > summary(warnings())

    > #########

    > I checked the code for summary.warnings:

    > function (object, ...)
    > {
    >  msgs <- names(object)
    >  calls <- as.character(object)
    >  ss <- ": "
    >  c.m. <- paste(calls, msgs, sep = ss)
    >  if(length(i.no.call <- which(calls == "NULL")))
    >     c.m.[i.no.call] <- substr(c.m.[i.no.call],
    >                              nchar(paste0("NULL", ss))+1L, 100000L)
    >  tm <- table(c.m., deparse.level = 0L)
    >  structure(unique(object), counts = as.vector(tm), class = "summary.warnings")
    > }


    >> The problem appears to be in the last line: unique preserves the order of the input, but counts reflects the counts in the table tm, which is a problem because table names are in alphabetical order.

I've committed the fix.  If you are interested,
I've replaced the last 2 lines with

    i.uniq <- which(!duplicated(object, incomparables=FALSE))
    tm <- table(factor(c.m., levels=c.m.[i.uniq]), deparse.level=0L)
    structure(object[i.uniq], counts = as.vector(tm), class = "summary.warnings")


which (at least conceptually) should even be faster the previous code.

Thank you again,
Martin

    >> Am I missing something?

    > No -- I think you are perfect and I was very imperfect ;-)  when
    > I created and tested the function ..

    > This will be fixed in the next versions of R.

    > Thank you very much for the report  and the nice concise
    > reproducible example!

    > Best regards,
    > Martin

    >> Allison
    >> ----------
    >> Allison Meisner, PhD
    >> Postdoctoral Fellow
    >> Department of Biostatistics
    >> Johns Hopkins Bloomberg School of Public Health
    >> 615 N. Wolfe Street
    >> Baltimore, MD 21205

    > Martin Maechler
    > ETH Zurich  and   R Core team

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From @xe|@urb|z @end|ng |rom gm@||@com  Sun May 10 01:45:40 2020
From: @xe|@urb|z @end|ng |rom gm@||@com (Axel Urbiz)
Date: Sat, 9 May 2020 19:45:40 -0400
Subject: [R] Loop inside dplyr::mutate
Message-ID: <9B339F38-5E93-41A3-A57D-B10C32C8F7CD@gmail.com>

Hello, 

Is there a less verbose approach to obtaining the PC_i variables inside the mutate?

library(tidyverse)
sim_data <- data.frame(borrower_id = sort(rep(1:10, 20)),
                       quarter = rep(1:20, 10),
                       pd = runif(length(rep(1:20, 10)))) # conditional probs
 
sim_data_wide <- tidyr::spread(sim_data, quarter, pd)  
colnames(sim_data_wide)[-1] <- paste0("P_", colnames(sim_data_wide)[-1])
      
# Compute cumulative probs
sim_data_wide <- sim_data_wide %>%
                  mutate(PC_1 = P_1,
                         PC_2 = 1-(1-P_1)*(1-P_2),
                         PC_3 = 1-(1-P_1)*(1-P_2)*(1-P_3),
                         PC_4 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4),
                         PC_5 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5),
                         PC_6 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6),
                         PC_7 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7),
                         PC_8 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8),
                         PC_9 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8)*(1-P_9),
                         PC_10 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8)*(1-P_9)*(1-P_10)
                        )


Thanks,
Axel.
	[[alternative HTML version deleted]]


From m@|||P@dpo@t @end|ng |rom gm@||@com  Sun May 10 02:30:46 2020
From: m@|||P@dpo@t @end|ng |rom gm@||@com (Medic)
Date: Sun, 10 May 2020 03:30:46 +0300
Subject: [R] Date format
Message-ID: <CAH6117+6Yw3kcUcdtNKJA2xE0=LC11QYXYB1Yax6ThH9gqTaDA@mail.gmail.com>

I took a SAMPLE CODE (for Connected scatterplot) from the R gallery
and applied to MY DATA, but got:
"Error in as.Date.numeric(mydata$date) : 'origin' must be supplied".
P.S. I can not understand ?as.Date()

SAMPLE CODE
library(ggplot2)
library(dplyr)
library(hrbrthemes)
data <- read.table("https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/3_TwoNumOrdered.csv",
header=T)

str(data)
'data.frame': 1822 obs. of  2 variables:
 $ date : chr  "2013-04-28" "2013-04-29" "2013-04-30" "2013-05-01" ...
 $ value: num  136 147 147 140 126 ...

data$date <- as.Date(data$date)

# Plot
data %>%
  tail(10) %>%
  ggplot( aes(x=date, y=value)) +
    geom_line( color="grey") +
    geom_point(shape=21, color="black", fill="#69b3a2", size=6) +
    theme_ipsum() +
    ggtitle("Evolution of bitcoin price")


MY DATA
mydata <- read.table("E:/mydata.csv", header=TRUE, sep=";", dec=",")

str(mydata)
'data.frame': 7 obs. of  2 variables:
 $ date : chr  "01.01.2000" "02.01.2000" "03.01.2000" "04.01.2000" ...
 $ value: int  11 12 13 14 15 16 17

mydata$date <- as.Date(mydata$date)
Error in as.Date.numeric(mydata$date) : 'origin' must be supplied


From md@umner @end|ng |rom gm@||@com  Sun May 10 02:42:43 2020
From: md@umner @end|ng |rom gm@||@com (Michael Sumner)
Date: Sun, 10 May 2020 10:42:43 +1000
Subject: [R] the volcano orientation
Message-ID: <CAAcGz9-d1Hq7zBO0vaaTWnrETAMX64=zESvM=QKon4au2tgvAw@mail.gmail.com>

Does anyone know why 'volcano' is oriented as it is?

image(volcano)  ## filled.contour is the same

I know it's all arbitrary, but north-up is a pretty solid convention. Is
there any reason why the classic 'image()' example data set would not
default to this orientation?

A Google map of the site (in Web Mercator):

https://www.google.com/maps/place/Maungawhau+%2F+Mount+Eden/@-36.8763271,174.7619561,856m/data=!3m1!1e3!4m8!1m2!2m1!1smaungawhau!3m4!1s0x6d0d47db8d7bd1ff:0x8bcffe2a5c7360d2!8m2!3d-36.8666667!4d174.7666667


For image(), the north-up orientation is 't(volcano[,ncol(volcano):1])'.

If you are interested in a roughly georeferenced version I have code here:

https://gist.github.com/mdsumner/20fe3ffa04421bf8e0517c19085e5fd8

(Also see fortunes::fortune("conventions") )

Best, Mike


-- 
Michael Sumner
Software and Database Engineer
Australian Antarctic Division
Hobart, Australia
e-mail: mdsumner at gmail.com

	[[alternative HTML version deleted]]


From b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com  Sun May 10 02:40:42 2020
From: b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com (Alessandra Bielli)
Date: Sat, 9 May 2020 18:40:42 -0600
Subject: [R] predicting waste per capita - is a gaussian model correct?
Message-ID: <CA+6N3yXDvFdjUeCKgoKi=3ZAnKLTv1jn0hodN1QN8=Bs79840g@mail.gmail.com>

Dear list,

I am new to this list and I hope it is ok to post here even though I
already posted this question on Cross Validated.

I am trying to predict the daily amount of waste per person produced in the
fishery sector. We surveyed fishing boats at the end of their fishing trip
and the variables I have are duration of trip (days), number of fishers,
waste category and waste weight (g), boat ID.

For each fishing trip I calculated grams of waste per person per day, i.e.
daily waste per capita. To predict daily waste per capita, I am using a
gaussian mixed effect model with log(waste per capita) as response variable
(I transformed it cause it was not normally distributed - and I'm not sure
it's correct to do so). Explanatory variable is waste category and boat ID
is a random effect. I use the predict function to estimate daily waste per
capita for each category and then back transformed it with exp(...).

My question is: is it correct to transform daily weight per capita to fit a
gaussian model?

Thanks so much for your advice!

Alessandra

	[[alternative HTML version deleted]]


From |e@gr@|n@ @end|ng |rom gm@||@com  Sun May 10 02:13:34 2020
From: |e@gr@|n@ @end|ng |rom gm@||@com (Adrien FABRE)
Date: Sun, 10 May 2020 02:13:34 +0200
Subject: [R] Sometimes commands do not terminate after upgrading to R 4.0
 and Ubuntu 20.04
Message-ID: <CAKPvFjCEmUKBYj6=m0_kt7D8ziw8KhSgdX3_9ncb1SC0+xo1_A@mail.gmail.com>

I have upgraded R (from 3.6 to 4.0) and RStudio (from 1.1 to 1.2.5) a few
days ago, and Ubuntu from 18.04 to 20.04 yesterday.

Since then, R sometimes never terminates when executing certain commands:
ivreg (from package AER), summary (of a logit regression) and logitmfx
(from package mfx). Sometimes these commands run fine, but most of the time
I have to kill the process because R won't terminate the execution, even
when pressing the red Stop button in RStudio.

When I tried example('AER'), it worked fine. Then I re-installed the
package AER. It threw 10 warnings of type In readLines(file, skipNul =
TRUE) :  cannot open compressed file
'/usr/lib/R/site-library/[package]/DESCRIPTION', probable reason 'No such
file or directory' where [package] is abind, colorspace, dichromat... (but
not AER).

Since then example('AER') throws a warning: no help found for ?AER?.

I've removed and reinstalled R 4.0: it didn't help. Besides, the apt purge
r-base* r-recommended r-cran-* threw a warning: dpkg: warning: while
removing r-base-core, directory '/usr/lib/R/site-library' not empty so not
removed. Also, there was a bunch of Package [package] is not installed, so
not removed, including for [package] equal to r-cran-abind and the other
listed above (this purge also returned a bunch of Note, selecting [package]
for glob 'r-cran-*').

I have the same bug when using R from the terminal. For the record, I was
probably working on RStudio during the upgrade to Ubuntu 20.04. Also, I
can't recall if this issue started after I upgraded R and RStudio (which
would be my best guess) or after I upgraded Ubuntu (a day or two later).

I hope someone can help.

	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Sun May 10 02:46:08 2020
From: j|ox @end|ng |rom mcm@@ter@c@ (Fox, John)
Date: Sun, 10 May 2020 00:46:08 +0000
Subject: [R] Loop inside dplyr::mutate
In-Reply-To: <13331_1589067958_049NjvZv006105_9B339F38-5E93-41A3-A57D-B10C32C8F7CD@gmail.com>
References: <13331_1589067958_049NjvZv006105_9B339F38-5E93-41A3-A57D-B10C32C8F7CD@gmail.com>
Message-ID: <F72DF2D9-A41E-441E-9F1C-F3EEC890BCE0@mcmaster.ca>

Dear Axel,

Assuming that you're not wedded to using mutate():

> D1 <- 1 - as.matrix(sim_data_wide[, 2:11])
> D2 <- matrix(0, 10, 10)
> colnames(D2) <- paste0("PC_", 1:10)
> for (i in 1:10) D2[, i] <- 1 - apply(D1[, 1:i, drop=FALSE], 1, prod)
> all.equal(D2, as.matrix(sim_data_wide[, 22:31]))
[1] TRUE 

I hope this helps,
 John

> On May 9, 2020, at 7:45 PM, Axel Urbiz <axel.urbiz at gmail.com> wrote:
> 
> Hello, 
> 
> Is there a less verbose approach to obtaining the PC_i variables inside the mutate?
> 
> library(tidyverse)
> sim_data <- data.frame(borrower_id = sort(rep(1:10, 20)),
>                       quarter = rep(1:20, 10),
>                       pd = runif(length(rep(1:20, 10)))) # conditional probs
> 
> sim_data_wide <- tidyr::spread(sim_data, quarter, pd)  
> colnames(sim_data_wide)[-1] <- paste0("P_", colnames(sim_data_wide)[-1])
> 
> # Compute cumulative probs
> sim_data_wide <- sim_data_wide %>%
>                  mutate(PC_1 = P_1,
>                         PC_2 = 1-(1-P_1)*(1-P_2),
>                         PC_3 = 1-(1-P_1)*(1-P_2)*(1-P_3),
>                         PC_4 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4),
>                         PC_5 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5),
>                         PC_6 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6),
>                         PC_7 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7),
>                         PC_8 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8),
>                         PC_9 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8)*(1-P_9),
>                         PC_10 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8)*(1-P_9)*(1-P_10)
>                        )
> 
> 
> Thanks,
> Axel.
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sun May 10 03:17:24 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 09 May 2020 18:17:24 -0700
Subject: [R] Loop inside dplyr::mutate
In-Reply-To: <9B339F38-5E93-41A3-A57D-B10C32C8F7CD@gmail.com>
References: <9B339F38-5E93-41A3-A57D-B10C32C8F7CD@gmail.com>
Message-ID: <D673A040-56B1-47C0-8AB6-4677019D9C22@dcn.davis.ca.us>

Does this help?

sim_wide2 <- (
    sim_data
%>% arrange( borrower_id, quarter )
%>% group_by( borrower_id )
%>% mutate( cumpd = 1 - cumprod( 1 - pd ) )
%>% ungroup()
%>% mutate( qlbl = paste0( "PC_", quarter ) )
%>% select( borrower_id, qlbl, cumpd )
%>% spread( qlbl, cumpd )
)

On May 9, 2020 4:45:40 PM PDT, Axel Urbiz <axel.urbiz at gmail.com> wrote:
>Hello, 
>
>Is there a less verbose approach to obtaining the PC_i variables inside
>the mutate?
>
>library(tidyverse)
>sim_data <- data.frame(borrower_id = sort(rep(1:10, 20)),
>                       quarter = rep(1:20, 10),
>                 pd = runif(length(rep(1:20, 10)))) # conditional probs
> 
>sim_data_wide <- tidyr::spread(sim_data, quarter, pd)  
>colnames(sim_data_wide)[-1] <- paste0("P_",
>colnames(sim_data_wide)[-1])
>      
># Compute cumulative probs
>sim_data_wide <- sim_data_wide %>%
>                  mutate(PC_1 = P_1,
>                         PC_2 = 1-(1-P_1)*(1-P_2),
>                         PC_3 = 1-(1-P_1)*(1-P_2)*(1-P_3),
>                         PC_4 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4),
>                      PC_5 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5),
>              PC_6 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6),
>      PC_7 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7),
>PC_8 =
>1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8),
>PC_9 =
>1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8)*(1-P_9),
>PC_10 =
>1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8)*(1-P_9)*(1-P_10)
>                        )
>
>
>Thanks,
>Axel.
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sun May 10 03:23:19 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 09 May 2020 18:23:19 -0700
Subject: [R] predicting waste per capita - is a gaussian model correct?
In-Reply-To: <CA+6N3yXDvFdjUeCKgoKi=3ZAnKLTv1jn0hodN1QN8=Bs79840g@mail.gmail.com>
References: <CA+6N3yXDvFdjUeCKgoKi=3ZAnKLTv1jn0hodN1QN8=Bs79840g@mail.gmail.com>
Message-ID: <5E75B7DA-819D-41AC-902B-C01F76A03A3C@dcn.davis.ca.us>

It could possibly be alright, except that:

a) you included no reference to your other post
b) you posted here using HTML format, which can severely corrupt what we see on this plain text only mailing list
c) your question is off topic, as your question is about statistics (theory) rather than R (a syntax and semantics for implementing theory).

So, no, not ok this time.

On May 9, 2020 5:40:42 PM PDT, Alessandra Bielli <bielli.alessandra at gmail.com> wrote:
>Dear list,
>
>I am new to this list and I hope it is ok to post here even though I
>already posted this question on Cross Validated.
>
>I am trying to predict the daily amount of waste per person produced in
>the
>fishery sector. We surveyed fishing boats at the end of their fishing
>trip
>and the variables I have are duration of trip (days), number of
>fishers,
>waste category and waste weight (g), boat ID.
>
>For each fishing trip I calculated grams of waste per person per day,
>i.e.
>daily waste per capita. To predict daily waste per capita, I am using a
>gaussian mixed effect model with log(waste per capita) as response
>variable
>(I transformed it cause it was not normally distributed - and I'm not
>sure
>it's correct to do so). Explanatory variable is waste category and boat
>ID
>is a random effect. I use the predict function to estimate daily waste
>per
>capita for each category and then back transformed it with exp(...).
>
>My question is: is it correct to transform daily weight per capita to
>fit a
>gaussian model?
>
>Thanks so much for your advice!
>
>Alessandra
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From rmh @end|ng |rom temp|e@edu  Sun May 10 03:40:28 2020
From: rmh @end|ng |rom temp|e@edu (Richard M. Heiberger)
Date: Sat, 9 May 2020 21:40:28 -0400
Subject: [R] Loop inside dplyr::mutate
In-Reply-To: <D673A040-56B1-47C0-8AB6-4677019D9C22@dcn.davis.ca.us>
References: <9B339F38-5E93-41A3-A57D-B10C32C8F7CD@gmail.com>
 <D673A040-56B1-47C0-8AB6-4677019D9C22@dcn.davis.ca.us>
Message-ID: <CAGx1TMABq+tGEoJZq8ksG5-TqB=LWqZooD3OgTrWMA2t2BhSwA@mail.gmail.com>

## I start with sim_data_wide

sim_data_wide <- tidyr::spread(sim_data, quarter, pd)

## and calculate wide
wide1 <- with(sim_data_wide, cbind(PC_1 = P_1,
                       PC_2 = 1-(1-P_1)*(1-P_2),
                       PC_3 = 1-(1-P_1)*(1-P_2)*(1-P_3),
                       PC_4 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4),
                       PC_5 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5),
                       PC_6 =
1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6),
                       PC_7 =
1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7),
                       PC_8 =
1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8),
                       PC_9 =
1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8)*(1-P_9),
                       PC_10 =
1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8)*(1-P_9)*(1-P_10)
                      )
)

## this simpler sequence gets the same value

A <- 1-sim_data_wide[,2:11]
B <- t(apply(A, 1, cumprod)[-1,])
wide2 <- cbind(sim_data_wide[,2], 1-B)
dimnames(wide2)[[2]] <- paste0("PC_", 1:10)

all.equal(wide1, wide2)


On Sat, May 9, 2020 at 9:28 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Does this help?
>
> sim_wide2 <- (
>     sim_data
> %>% arrange( borrower_id, quarter )
> %>% group_by( borrower_id )
> %>% mutate( cumpd = 1 - cumprod( 1 - pd ) )
> %>% ungroup()
> %>% mutate( qlbl = paste0( "PC_", quarter ) )
> %>% select( borrower_id, qlbl, cumpd )
> %>% spread( qlbl, cumpd )
> )
>
> On May 9, 2020 4:45:40 PM PDT, Axel Urbiz <axel.urbiz at gmail.com> wrote:
> >Hello,
> >
> >Is there a less verbose approach to obtaining the PC_i variables inside
> >the mutate?
> >
> >library(tidyverse)
> >sim_data <- data.frame(borrower_id = sort(rep(1:10, 20)),
> >                       quarter = rep(1:20, 10),
> >                 pd = runif(length(rep(1:20, 10)))) # conditional probs
> >
> >sim_data_wide <- tidyr::spread(sim_data, quarter, pd)
> >colnames(sim_data_wide)[-1] <- paste0("P_",
> >colnames(sim_data_wide)[-1])
> >
> ># Compute cumulative probs
> >sim_data_wide <- sim_data_wide %>%
> >                  mutate(PC_1 = P_1,
> >                         PC_2 = 1-(1-P_1)*(1-P_2),
> >                         PC_3 = 1-(1-P_1)*(1-P_2)*(1-P_3),
> >                         PC_4 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4),
> >                      PC_5 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5),
> >              PC_6 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6),
> >      PC_7 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7),
> >PC_8 =
> >1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8),
> >PC_9 =
> >1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8)*(1-P_9),
> >PC_10 =
>
> >1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8)*(1-P_9)*(1-P_10)
> >                        )
> >
> >
> >Thanks,
> >Axel.
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sun May 10 04:17:16 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sat, 9 May 2020 19:17:16 -0700
Subject: [R] Date format
In-Reply-To: <CAH6117+6Yw3kcUcdtNKJA2xE0=LC11QYXYB1Yax6ThH9gqTaDA@mail.gmail.com>
References: <CAH6117+6Yw3kcUcdtNKJA2xE0=LC11QYXYB1Yax6ThH9gqTaDA@mail.gmail.com>
Message-ID: <CAGxFJbSaLaF4uak1rA+qFpkCuuusidhWHLrtJVYoK300nSiHUA@mail.gmail.com>

$date is a factor, which is coded as numeric values internally, which
as.date sees as numeric, and therefore:
"as.Date will accept numeric data (the number of days since an epoch),
but only if origin is supplied." (from ?as.Date)

You need to supply a format argument to as.Date to get it to handle
the factor properly; e.g.
"%d.%m.%Y"  should work. See ?strptime for formatting details.

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Sat, May 9, 2020 at 5:31 PM Medic <mailiPadpost at gmail.com> wrote:
>
> I took a SAMPLE CODE (for Connected scatterplot) from the R gallery
> and applied to MY DATA, but got:
> "Error in as.Date.numeric(mydata$date) : 'origin' must be supplied".
> P.S. I can not understand ?as.Date()
>
> SAMPLE CODE
> library(ggplot2)
> library(dplyr)
> library(hrbrthemes)
> data <- read.table("https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/3_TwoNumOrdered.csv",
> header=T)
>
> str(data)
> 'data.frame': 1822 obs. of  2 variables:
>  $ date : chr  "2013-04-28" "2013-04-29" "2013-04-30" "2013-05-01" ...
>  $ value: num  136 147 147 140 126 ...
>
> data$date <- as.Date(data$date)
>
> # Plot
> data %>%
>   tail(10) %>%
>   ggplot( aes(x=date, y=value)) +
>     geom_line( color="grey") +
>     geom_point(shape=21, color="black", fill="#69b3a2", size=6) +
>     theme_ipsum() +
>     ggtitle("Evolution of bitcoin price")
>
>
> MY DATA
> mydata <- read.table("E:/mydata.csv", header=TRUE, sep=";", dec=",")
>
> str(mydata)
> 'data.frame': 7 obs. of  2 variables:
>  $ date : chr  "01.01.2000" "02.01.2000" "03.01.2000" "04.01.2000" ...
>  $ value: int  11 12 13 14 15 16 17
>
> mydata$date <- as.Date(mydata$date)
> Error in as.Date.numeric(mydata$date) : 'origin' must be supplied
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sun May 10 05:03:33 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 09 May 2020 20:03:33 -0700
Subject: [R] Date format
In-Reply-To: <CAGxFJbSaLaF4uak1rA+qFpkCuuusidhWHLrtJVYoK300nSiHUA@mail.gmail.com>
References: <CAH6117+6Yw3kcUcdtNKJA2xE0=LC11QYXYB1Yax6ThH9gqTaDA@mail.gmail.com>
 <CAGxFJbSaLaF4uak1rA+qFpkCuuusidhWHLrtJVYoK300nSiHUA@mail.gmail.com>
Message-ID: <F83AF5EA-41B0-45D1-9D75-6596C0C4FD4C@dcn.davis.ca.us>

... but str says it is character. This must be 4.0...

On May 9, 2020 7:17:16 PM PDT, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>$date is a factor, which is coded as numeric values internally, which
>as.date sees as numeric, and therefore:
>"as.Date will accept numeric data (the number of days since an epoch),
>but only if origin is supplied." (from ?as.Date)
>
>You need to supply a format argument to as.Date to get it to handle
>the factor properly; e.g.
>"%d.%m.%Y"  should work. See ?strptime for formatting details.
>
>Bert Gunter
>
>"The trouble with having an open mind is that people keep coming along
>and sticking things into it."
>-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>On Sat, May 9, 2020 at 5:31 PM Medic <mailiPadpost at gmail.com> wrote:
>>
>> I took a SAMPLE CODE (for Connected scatterplot) from the R gallery
>> and applied to MY DATA, but got:
>> "Error in as.Date.numeric(mydata$date) : 'origin' must be supplied".
>> P.S. I can not understand ?as.Date()
>>
>> SAMPLE CODE
>> library(ggplot2)
>> library(dplyr)
>> library(hrbrthemes)
>> data <-
>read.table("https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/3_TwoNumOrdered.csv",
>> header=T)
>>
>> str(data)
>> 'data.frame': 1822 obs. of  2 variables:
>>  $ date : chr  "2013-04-28" "2013-04-29" "2013-04-30" "2013-05-01"
>...
>>  $ value: num  136 147 147 140 126 ...
>>
>> data$date <- as.Date(data$date)
>>
>> # Plot
>> data %>%
>>   tail(10) %>%
>>   ggplot( aes(x=date, y=value)) +
>>     geom_line( color="grey") +
>>     geom_point(shape=21, color="black", fill="#69b3a2", size=6) +
>>     theme_ipsum() +
>>     ggtitle("Evolution of bitcoin price")
>>
>>
>> MY DATA
>> mydata <- read.table("E:/mydata.csv", header=TRUE, sep=";", dec=",")
>>
>> str(mydata)
>> 'data.frame': 7 obs. of  2 variables:
>>  $ date : chr  "01.01.2000" "02.01.2000" "03.01.2000" "04.01.2000"
>...
>>  $ value: int  11 12 13 14 15 16 17
>>
>> mydata$date <- as.Date(mydata$date)
>> Error in as.Date.numeric(mydata$date) : 'origin' must be supplied
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From bgunter@4567 @end|ng |rom gm@||@com  Sun May 10 06:02:46 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sat, 9 May 2020 21:02:46 -0700
Subject: [R] Date format
In-Reply-To: <F83AF5EA-41B0-45D1-9D75-6596C0C4FD4C@dcn.davis.ca.us>
References: <CAH6117+6Yw3kcUcdtNKJA2xE0=LC11QYXYB1Yax6ThH9gqTaDA@mail.gmail.com>
 <CAGxFJbSaLaF4uak1rA+qFpkCuuusidhWHLrtJVYoK300nSiHUA@mail.gmail.com>
 <F83AF5EA-41B0-45D1-9D75-6596C0C4FD4C@dcn.davis.ca.us>
Message-ID: <CAGxFJbQsOJQEogrGudbpEJ18Cb+srbM6zfxyFqaLkZP8vGVSgA@mail.gmail.com>

True. Whence the error message then?

Still, in my attempt to reproduce, the format statement worked.

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Sat, May 9, 2020 at 8:03 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>
> ... but str says it is character. This must be 4.0...
>
> On May 9, 2020 7:17:16 PM PDT, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> >$date is a factor, which is coded as numeric values internally, which
> >as.date sees as numeric, and therefore:
> >"as.Date will accept numeric data (the number of days since an epoch),
> >but only if origin is supplied." (from ?as.Date)
> >
> >You need to supply a format argument to as.Date to get it to handle
> >the factor properly; e.g.
> >"%d.%m.%Y"  should work. See ?strptime for formatting details.
> >
> >Bert Gunter
> >
> >"The trouble with having an open mind is that people keep coming along
> >and sticking things into it."
> >-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> >
> >On Sat, May 9, 2020 at 5:31 PM Medic <mailiPadpost at gmail.com> wrote:
> >>
> >> I took a SAMPLE CODE (for Connected scatterplot) from the R gallery
> >> and applied to MY DATA, but got:
> >> "Error in as.Date.numeric(mydata$date) : 'origin' must be supplied".
> >> P.S. I can not understand ?as.Date()
> >>
> >> SAMPLE CODE
> >> library(ggplot2)
> >> library(dplyr)
> >> library(hrbrthemes)
> >> data <-
> >read.table("https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/3_TwoNumOrdered.csv",
> >> header=T)
> >>
> >> str(data)
> >> 'data.frame': 1822 obs. of  2 variables:
> >>  $ date : chr  "2013-04-28" "2013-04-29" "2013-04-30" "2013-05-01"
> >...
> >>  $ value: num  136 147 147 140 126 ...
> >>
> >> data$date <- as.Date(data$date)
> >>
> >> # Plot
> >> data %>%
> >>   tail(10) %>%
> >>   ggplot( aes(x=date, y=value)) +
> >>     geom_line( color="grey") +
> >>     geom_point(shape=21, color="black", fill="#69b3a2", size=6) +
> >>     theme_ipsum() +
> >>     ggtitle("Evolution of bitcoin price")
> >>
> >>
> >> MY DATA
> >> mydata <- read.table("E:/mydata.csv", header=TRUE, sep=";", dec=",")
> >>
> >> str(mydata)
> >> 'data.frame': 7 obs. of  2 variables:
> >>  $ date : chr  "01.01.2000" "02.01.2000" "03.01.2000" "04.01.2000"
> >...
> >>  $ value: int  11 12 13 14 15 16 17
> >>
> >> mydata$date <- as.Date(mydata$date)
> >> Error in as.Date.numeric(mydata$date) : 'origin' must be supplied
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.


From rhur||n @end|ng |rom gwdg@de  Sun May 10 08:22:06 2020
From: rhur||n @end|ng |rom gwdg@de (Rainer Hurling)
Date: Sun, 10 May 2020 08:22:06 +0200
Subject: [R] Date format
In-Reply-To: <CAGxFJbSaLaF4uak1rA+qFpkCuuusidhWHLrtJVYoK300nSiHUA@mail.gmail.com>
References: <CAH6117+6Yw3kcUcdtNKJA2xE0=LC11QYXYB1Yax6ThH9gqTaDA@mail.gmail.com>
 <CAGxFJbSaLaF4uak1rA+qFpkCuuusidhWHLrtJVYoK300nSiHUA@mail.gmail.com>
Message-ID: <c06795d3-e636-0963-32e3-69418cbc7a2b@gwdg.de>

Am 10.05.20 um 04:17 schrieb Bert Gunter:
> $date is a factor, which is coded as numeric values internally, which
> as.date sees as numeric, and therefore:
> "as.Date will accept numeric data (the number of days since an epoch),
> but only if origin is supplied." (from ?as.Date)

as.Date is also able to read ISO date formatted strings by default (w/o
format).

as.Date("2013-04-28") works, as.Date("28.04.2013") not. The second
example needs as.Date("28.04.2013", format = "%d.%m.%Y").

> 
> You need to supply a format argument to as.Date to get it to handle
> the factor properly; e.g.
> "%d.%m.%Y"  should work. See ?strptime for formatting details.
> 
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> On Sat, May 9, 2020 at 5:31 PM Medic <mailiPadpost at gmail.com> wrote:
>>
>> I took a SAMPLE CODE (for Connected scatterplot) from the R gallery
>> and applied to MY DATA, but got:
>> "Error in as.Date.numeric(mydata$date) : 'origin' must be supplied".
>> P.S. I can not understand ?as.Date()
>>
>> SAMPLE CODE
>> library(ggplot2)
>> library(dplyr)
>> library(hrbrthemes)
>> data <- read.table("https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/3_TwoNumOrdered.csv",
>> header=T)
>>
>> str(data)
>> 'data.frame': 1822 obs. of  2 variables:
>>  $ date : chr  "2013-04-28" "2013-04-29" "2013-04-30" "2013-05-01" ...
>>  $ value: num  136 147 147 140 126 ...
>>
>> data$date <- as.Date(data$date)
>>
>> # Plot
>> data %>%
>>   tail(10) %>%
>>   ggplot( aes(x=date, y=value)) +
>>     geom_line( color="grey") +
>>     geom_point(shape=21, color="black", fill="#69b3a2", size=6) +
>>     theme_ipsum() +
>>     ggtitle("Evolution of bitcoin price")
>>
>>
>> MY DATA
>> mydata <- read.table("E:/mydata.csv", header=TRUE, sep=";", dec=",")
>>
>> str(mydata)
>> 'data.frame': 7 obs. of  2 variables:
>>  $ date : chr  "01.01.2000" "02.01.2000" "03.01.2000" "04.01.2000" ...
>>  $ value: int  11 12 13 14 15 16 17
>>
>> mydata$date <- as.Date(mydata$date)
>> Error in as.Date.numeric(mydata$date) : 'origin' must be supplied
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From e@ @end|ng |rom enr|co@chum@nn@net  Sun May 10 08:47:04 2020
From: e@ @end|ng |rom enr|co@chum@nn@net (Enrico Schumann)
Date: Sun, 10 May 2020 08:47:04 +0200
Subject: [R] 
 Sometimes commands do not terminate after upgrading to R 4.0
 and Ubuntu 20.04
In-Reply-To: <CAKPvFjCEmUKBYj6=m0_kt7D8ziw8KhSgdX3_9ncb1SC0+xo1_A@mail.gmail.com>
 (Adrien FABRE's message of "Sun, 10 May 2020 02:13:34 +0200")
References: <CAKPvFjCEmUKBYj6=m0_kt7D8ziw8KhSgdX3_9ncb1SC0+xo1_A@mail.gmail.com>
Message-ID: <87k11kl21j.fsf@enricoschumann.net>

>>>>> "Adrien" == Adrien FABRE <lesgrains at gmail.com> writes:

  Adrien> I have upgraded R (from 3.6 to 4.0) and RStudio (from 1.1 to 1.2.5) a few
  Adrien> days ago, and Ubuntu from 18.04 to 20.04 yesterday.

  Adrien> Since then, R sometimes never terminates when executing certain commands:
  Adrien> ivreg (from package AER), summary (of a logit regression) and logitmfx
  Adrien> (from package mfx). Sometimes these commands run fine, but most of the time
  Adrien> I have to kill the process because R won't terminate the execution, even
  Adrien> when pressing the red Stop button in RStudio.

  Adrien> When I tried example('AER'), it worked fine. Then I re-installed the
  Adrien> package AER. It threw 10 warnings of type In readLines(file, skipNul =
  Adrien> TRUE) :  cannot open compressed file
  Adrien> '/usr/lib/R/site-library/[package]/DESCRIPTION', probable reason 'No such
  Adrien> file or directory' where [package] is abind, colorspace, dichromat... (but
  Adrien> not AER).

  Adrien> Since then example('AER') throws a warning: no help found for ?AER?.

  Adrien> I've removed and reinstalled R 4.0: it didn't help. Besides, the apt purge
  Adrien> r-base* r-recommended r-cran-* threw a warning: dpkg: warning: while
  Adrien> removing r-base-core, directory '/usr/lib/R/site-library' not empty so not
  Adrien> removed. Also, there was a bunch of Package [package] is not installed, so
  Adrien> not removed, including for [package] equal to r-cran-abind and the other
  Adrien> listed above (this purge also returned a bunch of Note, selecting [package]
  Adrien> for glob 'r-cran-*').

  Adrien> I have the same bug when using R from the terminal. For the record, I was
  Adrien> probably working on RStudio during the upgrade to Ubuntu 20.04. Also, I
  Adrien> can't recall if this issue started after I upgraded R and RStudio (which
  Adrien> would be my best guess) or after I upgraded Ubuntu (a day or two later).

  Adrien> I hope someone can help.

There has been a discussion on R-SIG-Debian recently,
and /perhaps/ it is related to your troubles.

See https://stat.ethz.ch/pipermail/r-sig-debian/2020-April/003159.html
and in particular
https://stat.ethz.ch/pipermail/r-sig-debian/2020-April/003166.html
.


-- 
Enrico Schumann
Lucerne, Switzerland
http://enricoschumann.net


From m@|||P@dpo@t @end|ng |rom gm@||@com  Sun May 10 09:15:51 2020
From: m@|||P@dpo@t @end|ng |rom gm@||@com (Medic)
Date: Sun, 10 May 2020 10:15:51 +0300
Subject: [R] Date format
Message-ID: <CAH6117JToaaoFQssgiuMbiR7d_SEBWK2HWuRqxWFKugRP-Q5KQ@mail.gmail.com>

I took a SAMPLE CODE (for Connected scatterplot) from the R gallery
and applied to MY DATA, but got:
"Don't know how to automatically pick scale for object ..."
P.S. 1) R ver. 4.0 (Yes, Jeff);  2) Attached: mydata_dput (1 ??)

SAMPLE CODE
library(ggplot2)
library(dplyr)
library(hrbrthemes)
data <- read.table("https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/3_TwoNumOrdered.csv
", header=T)

data$date <- as.Date(data$date)

# Plot
data %>%
  tail(10) %>%
  ggplot( aes(x=date, y=value)) +
    geom_line( color="grey") +
    geom_point(shape=21, color="black", fill="#69b3a2", size=6) +
    theme_ipsum() +
    ggtitle("Evolution of bitcoin price")

======
MY DATA
mydata <- read.table("E:/mydata.csv", header=TRUE, sep=";", dec=",")

str(mydata)
'data.frame': 7 obs. of  2 variables:
 $ date : chr  "01.01.2000" "02.01.2000" ...
 $ value: int  11 12 ...

mydata$date <- as.Date(mydata$date, "%d.%m.%Y")

str(mydata$date)
Date[1:7], format: "2000-01-01"

# Bert, thanks for the explanation!
# Rainer, thanks for the specific code!

# And then the problem:
mydata %>%
    tail(10) %>%
    ggplot( aes(x=mydata, y=value)) +
    geom_line( color="grey") +
    geom_point(shape=21, color="black", fill="#69b3a2", size=6) +
    theme_ipsum() +
    ggtitle("Evolution")

"Don't know how to automatically pick scale for object of type
data.frame. Defaulting to continuous.
Error: Aesthetics must be either length 1 or the same as the data (7): x"

From m@rong|u@|u|g| @end|ng |rom gm@||@com  Sun May 10 10:17:47 2020
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Sun, 10 May 2020 10:17:47 +0200
Subject: [R] How to determine whether a value belong to a cumulative
 distribution?
Message-ID: <CAMk+s2RNWxxmMeOX6ZQo_Cko-=R0YpPUOnAar7d=isQp+JatYg@mail.gmail.com>

Hello,
I am trying to translate a mathematical formula into R. The formula (or
rather a  set of formulas) is meant to determine the first outlier in a
sequence of measurements. To do this, a parameter r is calculated; this is
essentially the ratio between the variance of the value x and the sum of
the variances of the x-1 elements of the series. x follows a certain
distribution (namely, sigmoid), whereas r follows a cumulative empirical
one.
The text says:
"Each r is distributed as t under the model. Therefore, we can test the
hypothesis whether a single observation deviates from the model by
comparing r with the t distribution, where F(?) is the cumulative
distribution function of the t distribution:
                                P-value = 2 * [1 ? F(1 ? |r|)]
"
I generated a cumulative function with
```
cum_fun = ecdf(abs(x[1:n])
```
which gives me:
```
> n=3
> Empirical CDF
Call: ecdf(abs(x{1:n])
 x[1:3] = 5.5568, 6.5737, 7.2471
```
But now how can I determine if x belongs to the distribution?
If I do, as in the formula:
```
> p = 2 * (1-cum_fun)
Error in 1 - cum_fun : non-numeric argument to binary operator
```
Can I get a p-value associated with this association?
Thank you

-- 
Best regards,
Luigi

	[[alternative HTML version deleted]]


From kobyeongm|n @end|ng |rom kore@@@c@kr  Sun May 10 10:24:51 2020
From: kobyeongm|n @end|ng |rom kore@@@c@kr (Ko Byeongmin)
Date: Sun, 10 May 2020 10:24:51 +0200
Subject: [R] solve() function freezes CLI in GNU R 3.6.3 - SOLVED
In-Reply-To: <24246.56256.474662.24178@rob.eddelbuettel.com>
References: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
 <24246.52361.934223.41992@rob.eddelbuettel.com>
 <24246.56256.474662.24178@rob.eddelbuettel.com>
Message-ID: <1346d8eb-57d4-d7e9-f4a0-4a1cb4e33290@korea.ac.kr>

Dear list,

Dirk and Professor J.C. Nash gave me an invaluable help! Prof. Nash
mentioned:

> Possibly this is a quirk of the particular distro or machine, BLAS or LAPACK[.]
This was indeed the case, and Dirk's suggestion to

> [install] libopenblas-openmp-dev" and
> [remove] both "libopenblas-pthread-dev libopenblas0-pthread"
immediately solved the problem!

Before applying the fix, I took note of my /sessionInfo() /output:

> > sessionInfo()
> R version 3.6.3 (2020-02-29)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Ubuntu 20.04 LTS
>
> Matrix products: default
> BLAS:?? /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3
> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/liblapack.so.3
>
> locale:
> ?[1] LC_CTYPE=en_US.UTF-8?????? LC_NUMERIC=C
> ?[3] LC_TIME=de_DE.UTF-8??????? LC_COLLATE=en_US.UTF-8
> ?[5] LC_MONETARY=de_DE.UTF-8??? LC_MESSAGES=en_US.UTF-8
> ?[7] LC_PAPER=de_DE.UTF-8?????? LC_NAME=C
> ?[9] LC_ADDRESS=C?????????????? LC_TELEPHONE=C
> [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats???? graphics? grDevices utils???? datasets? methods base
>
> loaded via a namespace (and not attached):
> [1] compiler_3.6.3
*Is there anything that I can do, so that others**
**do not experience the same issue?* Writing
a bug report comes into my mind...

In this last paragraph I append some information on
my hardware, as Dirk suggested in his previous mail,
as well as a summary of the software I'm using.

> Operating System: Kubuntu 20.04
> KDE Plasma Version: 5.18.4
> KDE Frameworks Version: 5.68.0
> Qt Version: 5.12.8
> Kernel Version: 5.4.0-28-generic
> OS Type: 64-bit
> Processors: 8 ? Intel? Core? i5-8300H CPU @ 2.30GHz
> Memory: 7,4 GiB of RAM
Again, thanks for all the help! I really appreciate it. :)

Byeongmin

On 09.05.20 18:35, Dirk Eddelbuettel wrote:
> On 9 May 2020 at 10:30, Dirk Eddelbuettel wrote:
> |
> | We can see that you use Linux.
> |
> | Are you by chance
> |
> |  - on a Debian or Ubuntu system, and
> |  - have the libopenblas package installed ?
> |
> | If so then it is a known bug with the libopenblas0-pthread package.
> |
> | Installing libopen0-openmp (and also removing libopenblas0-pthread) should
> | fix it.
>
> That was imprecise. It should read "installing libopenblas-openmp-dev" and
> removing both "libopenblas-pthread-dev libopenblas0-pthread".
>
> I cannot reproduce the bug on the hardware I have access to (i5, i7, xeon)
> though I was able to a few weeks ago. Maybe something changed already...
>
> There was also a brief thread on the debian-science list (within Debian)
> starting with https://lists.debian.org/debian-science/2020/04/msg00081.html
> and leading to https://lists.debian.org/debian-science/2020/05/msg00003.html
> (and this cross from April to May)
>
> We would need some more information on hardware to chase this.
>
> Dirk
>   
> | This is likely CPU dependent. But we need more info. Can you maybe come to the
> | r-sig-debian list (subscription needed to reduce spam) and we continue there?
> |
> | Dirk
> |
> |
> | --
> | http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org
> |
> | ______________________________________________
> | R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> | https://stat.ethz.ch/mailman/listinfo/r-help
> | PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> | and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@|||P@dpo@t @end|ng |rom gm@||@com  Sun May 10 10:27:51 2020
From: m@|||P@dpo@t @end|ng |rom gm@||@com (Medic)
Date: Sun, 10 May 2020 11:27:51 +0300
Subject: [R] Date format
Message-ID: <CAH6117JohtWqCOs-EAXXyU0rWrkFNUdsxcCj-UyoAFo=doH_Sw@mail.gmail.com>

Many Thanks!!!
> cpolwart at chemo.org.uk:
> Your X axis is plotting mydata not date?
> Use aes(x=date


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sun May 10 11:54:01 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sun, 10 May 2020 10:54:01 +0100
Subject: [R] 
 Warning in install.packages : converting NULL pointer to R NULL
In-Reply-To: <3e3568fa-3b32-0d33-0418-12026bce7628@auckland.ac.nz>
References: <bbd798c8-eaa7-1b8d-cb39-7e8926600520@sapo.pt>
 <3e3568fa-3b32-0d33-0418-12026bce7628@auckland.ac.nz>
Message-ID: <53adb45b-5c8d-6d37-434a-7625493bf2ad@sapo.pt>

Hello,

Thanks for the info.
It was an update issue, I updated to Ubuntu 20.04 and R to 4.0.0 but not 
RStudio. Now that I have all three most recent versions it works as 
before, with no strange warning message.
So I guess this thread is closed.

Thanks to all,

Rui Barradas

?s 23:11 de 08/05/20, Rolf Turner escreveu:
> 
> Hi Rui.? Doesn't happen to me under Ubuntu 18.04:
> 
>> install.packages('cowplot',lib=.Rlib) trying URL 
>> 'https://cloud.r-project.org/src/contrib
>> /cowplot_1.0.0.tar.gz'
>> Content type 'application/x-gzip' length 1275585 bytes (1.2 MB)
>> ==================================================
>> downloaded 1.2 MB
>>
>> * installing *source* package ?cowplot? ...
>> ** package ?cowplot? successfully unpacked and MD5 sums checked
>> ** using staged installation
>> ** R
>> ** inst
>> ** byte-compile and prepare package for lazy loading
>> ** help
>> *** installing help indices
>> *** copying figures
>> ** building package indices
>> ** installing vignettes
>> ** testing if installed package can be loaded from temporary location
>> ** testing if installed package can be loaded from final location
>> ** testing if installed package keeps a record of temporary 
>> installation path
>> * DONE (cowplot)
>>
>> The downloaded source packages are in
>> ?????/tmp/RtmpG8wb97/downloaded_packages?
> 
> My session info is as follows:
> 
>> sessionInfo()
>> R version 4.0.0 (2020-04-24)
>> Platform: x86_64-pc-linux-gnu (64-bit)
>> Running under: Ubuntu 18.04.4 LTS
>>
>> Matrix products: default
>> BLAS:?? /usr/lib/x86_64-linux-gnu/atlas/libblas.so.3.10.3
>> LAPACK: /usr/lib/x86_64-linux-gnu/atlas/liblapack.so.3.10.3
>>
>> Random number generation:
>> ?RNG:???? Mersenne-Twister ?Normal:? Inversion ?Sample:? Rounding
>> locale:
>> ?[1] LC_CTYPE=en_GB.UTF-8?????? LC_NUMERIC=C ?[3] 
>> LC_TIME=en_NZ.UTF-8??????? LC_COLLATE=en_GB.UTF-8 ?[5] 
>> LC_MONETARY=en_NZ.UTF-8??? LC_MESSAGES=en_GB.UTF-8 ?[7] 
>> LC_PAPER=en_NZ.UTF-8?????? LC_NAME=C ?[9] LC_ADDRESS=C               
>> LC_TELEPHONE=C [11] LC_MEASUREMENT=en_NZ.UTF-8 LC_IDENTIFICATION=C
>> attached base packages:
>> [1] stats???? graphics? grDevices utils???? datasets? methods?? base
>> other attached packages:
>> [1] brev_0.0-4
>>
>> loaded via a namespace (and not attached):
>> [1] compiler_4.0.0 tools_4.0.0 
> 
> No idea what to suggest.? Sorry.
> 
> cheers,
> 
> Rolf
>


From kry|ov@r00t @end|ng |rom gm@||@com  Sun May 10 14:02:43 2020
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Sun, 10 May 2020 15:02:43 +0300
Subject: [R] How to determine whether a value belong to a cumulative
 distribution?
In-Reply-To: <CAMk+s2RNWxxmMeOX6ZQo_Cko-=R0YpPUOnAar7d=isQp+JatYg@mail.gmail.com>
References: <CAMk+s2RNWxxmMeOX6ZQo_Cko-=R0YpPUOnAar7d=isQp+JatYg@mail.gmail.com>
Message-ID: <20200510150243.0ce1b940@trisector>

On Sun, 10 May 2020 10:17:47 +0200
Luigi Marongiu <marongiu.luigi at gmail.com> wrote:

>If I do, as in the formula:
>```
>> p = 2 * (1-cum_fun)  
>Error in 1 - cum_fun : non-numeric argument to binary operator
>```

The ecdf function returns another function that calculates the ECDF
value for an arbitrary input. For example,

e <- ecdf(1:10)
e
# Empirical CDF
# Call: ecdf(1:10)
#  x[1:10] =      1,      2,      3,  ...,      9,     10
e(c(-1, 5, 100)) # call the returned value as a function
# [1] 0.0 0.5 1.0

If you want to see the empirical distribution function values for the
points of the dataset itself, call the function returned by ecdf with
the same data again:

x <- 1:10
ecdf(x)(x)
# [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

If you want to calculate the CDF for a given value of 1 ? |r|, pass
this value as an argument to the function returned by ecdf:

cum_fun <- ecdf(abs(x[1:n])
p <- 2 * (1 - cum_fun(1 - abs(r)))

On the other hand, given the quotes from the text, I think than you
might need to use the theoretical t distribution function (available as
`dt` in R) in the formula instead of ECDF:

df <- ... # degrees of freedom for Student t distribution
p <- 2 * (1 - dt(1 - abs(r), df))

I am not sure about that, though.

-- 
Best regards,
Ivan


From btupper @end|ng |rom b|ge|ow@org  Sun May 10 16:05:00 2020
From: btupper @end|ng |rom b|ge|ow@org (Ben Tupper)
Date: Sun, 10 May 2020 10:05:00 -0400
Subject: [R] the volcano orientation
In-Reply-To: <CAAcGz9-d1Hq7zBO0vaaTWnrETAMX64=zESvM=QKon4au2tgvAw@mail.gmail.com>
References: <CAAcGz9-d1Hq7zBO0vaaTWnrETAMX64=zESvM=QKon4au2tgvAw@mail.gmail.com>
Message-ID: <CALrbzg3AGw7fY10MDb4RY39=htcijhQVppJzf1815Cs_4ubtjQ@mail.gmail.com>

Hi,

Hmmm.  The only place I have ever seen a georeferenced version of 'volcano'
is here...

https://waterdata.usgs.gov/blog/inlmiscmaps/

It was on the internet so I assumed it was true. Now, I suspect that, since
the original survey by Ross Ihaka, continental drift is happening waaaay
faster than anyone guessed. Could be a decent grant proposal somewhere in
all this.

Seriously, though, I haven't any idea why 'volcano' is the way it is shown,
nor was I awake enough to actually look at a map as you have done. I would
love-love-love to see a georeferenced version be part of the stars package
as example data. It's small enough to be lightweight but has enough
information in it to be handy for meaningful demonstrations. Maybe along
the lines of ...

https://gist.github.com/btupper/8e8eb8c0ebf4402a3f87b5638eca954a

... but with the correct spatial info.

Cheers,
Ben

On Sat, May 9, 2020 at 8:44 PM Michael Sumner <mdsumner at gmail.com> wrote:

> Does anyone know why 'volcano' is oriented as it is?
>
> image(volcano)  ## filled.contour is the same
>
> I know it's all arbitrary, but north-up is a pretty solid convention. Is
> there any reason why the classic 'image()' example data set would not
> default to this orientation?
>
> A Google map of the site (in Web Mercator):
>
>
> https://www.google.com/maps/place/Maungawhau+%2F+Mount+Eden/@-36.8763271,174.7619561,856m/data=!3m1!1e3!4m8!1m2!2m1!1smaungawhau!3m4!1s0x6d0d47db8d7bd1ff:0x8bcffe2a5c7360d2!8m2!3d-36.8666667!4d174.7666667
>
>
> For image(), the north-up orientation is 't(volcano[,ncol(volcano):1])'.
>
> If you are interested in a roughly georeferenced version I have code here:
>
> https://gist.github.com/mdsumner/20fe3ffa04421bf8e0517c19085e5fd8
>
> (Also see fortunes::fortune("conventions") )
>
> Best, Mike
>
>
> --
> Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> Hobart, Australia
> e-mail: mdsumner at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ben Tupper
Bigelow Laboratory for Ocean Science
East Boothbay, Maine
http://www.bigelow.org/
https://eco.bigelow.org

	[[alternative HTML version deleted]]


From @purd|e@@ @end|ng |rom gm@||@com  Sun May 10 23:03:23 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Mon, 11 May 2020 09:03:23 +1200
Subject: [R] the volcano orientation
In-Reply-To: <CAAcGz9-d1Hq7zBO0vaaTWnrETAMX64=zESvM=QKon4au2tgvAw@mail.gmail.com>
References: <CAAcGz9-d1Hq7zBO0vaaTWnrETAMX64=zESvM=QKon4au2tgvAw@mail.gmail.com>
Message-ID: <CAB8pepxEtn9bsMkrRjMN=Rm=LvgzfP3_7Sz=1RjeGcCTyDRPwQ@mail.gmail.com>

> Does anyone know why 'volcano' is oriented as it is?
> image(volcano)  ## filled.contour is the same

Great question!

graphics::image produces a "plot".
It follows the same x y conventions as other plots in the graphics package.
It's *defaults* are not designed to display photos, etc.

However, the format of the volcano data is not consistent with either
the defaults of graphics::image or what I (personally) would expect in
a photographic data, with top-left value at top-left of matrix and
bottom-right point at bottom-right of matrix, but that's debatable...

According, the documentation of the volcano data:

    A matrix with 87 rows and 61 columns,
    rows corresponding to grid lines running east to west
    and columns to grid lines running south to north.

Perhaps that could be improved slightly...?

And one more thing that caught me out.
My initial expectation (using a simple interpretation) was the data
would need to be transposed and then either the ylim reversed or the
rows reversed.
But when I tried to plot the volcano data using my own function (which
does just that), I got the wrong result.

But in the documentation for graphics::image we have:

    "Need to transpose and flip"
    image(t(volcano)[ncol(volcano):1,])

Which produces the right result.

I had to think about this for a while...

The example for graphics::image above is actually transposing the
matrix *twice*.
First in the input to the function, and then again (implicitly), where
rows (going down the data) are interpreted as x (going right across
the plot).


From @purd|e@@ @end|ng |rom gm@||@com  Sun May 10 23:21:10 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Mon, 11 May 2020 09:21:10 +1200
Subject: [R] the volcano orientation
In-Reply-To: <CAAcGz9-d1Hq7zBO0vaaTWnrETAMX64=zESvM=QKon4au2tgvAw@mail.gmail.com>
References: <CAAcGz9-d1Hq7zBO0vaaTWnrETAMX64=zESvM=QKon4au2tgvAw@mail.gmail.com>
Message-ID: <CAB8pepzqxtCyDhhcMdmmJjOXRXhxxvrc2vcAYbfUrxhUU8Xv3w@mail.gmail.com>

Sorry, one more thing.
My response didn't really answer your question.
But I would say that the formats of most datasets used in statistics
are reflective of the preferences of the people that collected or
published them, at the time...

Also, I've found the older publications quite often have considerable merit...

On Sun, May 10, 2020 at 12:44 PM Michael Sumner <mdsumner at gmail.com> wrote:
>
> Does anyone know why 'volcano' is oriented as it is?
>
> image(volcano)  ## filled.contour is the same
>
> I know it's all arbitrary, but north-up is a pretty solid convention. Is
> there any reason why the classic 'image()' example data set would not
> default to this orientation?
>
> A Google map of the site (in Web Mercator):
>
> https://www.google.com/maps/place/Maungawhau+%2F+Mount+Eden/@-36.8763271,174.7619561,856m/data=!3m1!1e3!4m8!1m2!2m1!1smaungawhau!3m4!1s0x6d0d47db8d7bd1ff:0x8bcffe2a5c7360d2!8m2!3d-36.8666667!4d174.7666667
>
>
> For image(), the north-up orientation is 't(volcano[,ncol(volcano):1])'.
>
> If you are interested in a roughly georeferenced version I have code here:
>
> https://gist.github.com/mdsumner/20fe3ffa04421bf8e0517c19085e5fd8
>
> (Also see fortunes::fortune("conventions") )
>
> Best, Mike
>
>
> --
> Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> Hobart, Australia
> e-mail: mdsumner at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From |r@|nj @end|ng |rom gm@||@com  Sun May 10 23:44:17 2020
From: |r@|nj @end|ng |rom gm@||@com (John C Frain)
Date: Sun, 10 May 2020 22:44:17 +0100
Subject: [R] predicting waste per capita - is a gaussian model correct?
In-Reply-To: <CA+6N3yXDvFdjUeCKgoKi=3ZAnKLTv1jn0hodN1QN8=Bs79840g@mail.gmail.com>
References: <CA+6N3yXDvFdjUeCKgoKi=3ZAnKLTv1jn0hodN1QN8=Bs79840g@mail.gmail.com>
Message-ID: <CAHrK516nu-TmMJq=Rzp1ddYr=RAnX41k8=aPfhgE4ZJNFbf88A@mail.gmail.com>

On Sun, 10 May 2020 at 02:00, Alessandra Bielli <bielli.alessandra at gmail.com>
wrote:

> Dear list,
>
> I am new to this list and I hope it is ok to post here even though I
> already posted this question on Cross Validated.
>
> I am trying to predict the daily amount of waste per person produced in the
> fishery sector. We surveyed fishing boats at the end of their fishing trip
> and the variables I have are duration of trip (days), number of fishers,
> waste category and waste weight (g), boat ID.
>
> For each fishing trip I calculated grams of waste per person per day, i.e.
> daily waste per capita. To predict daily waste per capita, I am using a
> gaussian mixed effect model with log(waste per capita) as response variable
> (I transformed it cause it was not normally distributed - and I'm not sure
> it's correct to do so). Explanatory variable is waste category and boat ID
> is a random effect. I use the predict function to estimate daily waste per
> capita for each category and then back transformed it with exp(...).
>
> My question is: is it correct to transform daily weight per capita to fit a
> gaussian model?
>
> Thanks so much for your advice!
>
> Alessandra
>
There is no requirement that the dependent variable in a "regression" type
estimation follows a gaussian distribution.  You need a model of the
process and then use an estimation technique to estimate your model.  If
effects in your model are additive do not use a log transformation. If
effects are multiplicative then use a log transformation.  The choice
should be determined by the mechanics of the problem and not by the
statistics.  If you do use a log transformation the trying to reverse the
process using an exponential transformation will be biased.  The extent of
that bias depends on your problem and it would not be possible to estimate
the significance of the bias without a much greater knowledge of the
process and data.  I would suggest that you consult a competent
statistician.

John C Frain
3 Aranleigh Park
Rathfarnham
Dublin 14
Ireland
www.tcd.ie/Economics/staff/frainj/home.html
mailto:frainj at tcd.ie
mailto:frainj at gmail.com

	[[alternative HTML version deleted]]


From h@r@h|t@khedk@r @end|ng |rom gm@||@com  Sun May 10 08:27:53 2020
From: h@r@h|t@khedk@r @end|ng |rom gm@||@com (Harshita Khedkar)
Date: Sun, 10 May 2020 14:27:53 +0800
Subject: [R] Unable to install Sequin R
Message-ID: <CAFV24eGLc6bkPCKYzF=HbsT9653MTcW+EqP60X2WhJyVp7Q-sg@mail.gmail.com>

Hello,

This is Harshita here,

I am trying to install Sequin R and could not able to get it to work.
Please find the attached error message, and let me know how o go ahead
about it.

Thank you and best regards,
Harshita Khedkar.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 2020-05-10 (1).png
Type: image/png
Size: 190215 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200510/6653f569/attachment.png>

From drj|m|emon @end|ng |rom gm@||@com  Mon May 11 00:40:32 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Mon, 11 May 2020 08:40:32 +1000
Subject: [R] Unable to install Sequin R
In-Reply-To: <CAFV24eGLc6bkPCKYzF=HbsT9653MTcW+EqP60X2WhJyVp7Q-sg@mail.gmail.com>
References: <CAFV24eGLc6bkPCKYzF=HbsT9653MTcW+EqP60X2WhJyVp7Q-sg@mail.gmail.com>
Message-ID: <CA+8X3fU9gVBOvsvB7CVQ=DMLG1Tf-drOtOxrCotqR8U8Qptbng@mail.gmail.com>

Hi Harshita,
I think you are trying to install the package "seqinr" (Biological
Sequences Retrieval and Analysis).

1) When you see something like <packagename> in the help pages, it
means "Insert the name of the package here". So you really want:

install.packages("seqinr")

2) Both spelling and capitalization matter in R. From your screenshot
you seem to be using Windows, where capitalization often doesn't
matter. So you have to spell words correctly and use the correct case.

Jim

On Mon, May 11, 2020 at 8:31 AM Harshita Khedkar
<harshitakhedkar at gmail.com> wrote:
>
> Hello,
>
> This is Harshita here,
>
> I am trying to install Sequin R and could not able to get it to work.
> Please find the attached error message, and let me know how o go ahead
> about it.
>
> Thank you and best regards,
> Harshita Khedkar.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @purd|e@@ @end|ng |rom gm@||@com  Mon May 11 00:56:29 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Mon, 11 May 2020 10:56:29 +1200
Subject: [R] predicting waste per capita - is a gaussian model correct?
In-Reply-To: <CAHrK516nu-TmMJq=Rzp1ddYr=RAnX41k8=aPfhgE4ZJNFbf88A@mail.gmail.com>
References: <CA+6N3yXDvFdjUeCKgoKi=3ZAnKLTv1jn0hodN1QN8=Bs79840g@mail.gmail.com>
 <CAHrK516nu-TmMJq=Rzp1ddYr=RAnX41k8=aPfhgE4ZJNFbf88A@mail.gmail.com>
Message-ID: <CAB8pepwn-cP8abZ=1Jw7yKnd8z+5Er1MNSUpR7ZD+SSNkaPCOA@mail.gmail.com>

Well, this is 100% off-topic...
And I wasn't planning to answer the OP's question.

However, I disagree with your answer.

> There is no requirement that the dependent variable in a "regression" type
> estimation follows a gaussian distribution.

False.
It's depends on what type of '"regression" type estimation' one uses,
among other things.

> You need a model of the
> process and then use an estimation technique to estimate your model.  If
> effects in your model are additive do not use a log transformation. If
> effects are multiplicative then use a log transformation.

The main question is, does the model satisfy the *assumptions*.

> The choice
> should be determined by the mechanics of the problem and not by the
> statistics.

While a mechanistic understanding is definitely valuable...
If the criteria for a good model vs a bad model, was whether the model
was consistent with mechanistic theory/understanding, then nearly
every statistical model I've seen would be a bad model.
I would say, a good model is one that is useful...

> If you do use a log transformation the trying to reverse the
> process using an exponential transformation will be biased.
> The extent of
> that bias depends on your problem and it would not be possible to estimate
> the significance of the bias without a much greater knowledge of the
> process and data.

Never heard of this before...
But I do note back-transformation is not trivial, and I'm not an
expert on back-transformations.

> I would suggest that you consult a competent
> statistician.

I agree on that part...


From twoo|m@n @end|ng |rom ont@rgettek@com  Mon May 11 01:48:04 2020
From: twoo|m@n @end|ng |rom ont@rgettek@com (Tom Woolman)
Date: Sun, 10 May 2020 19:48:04 -0400
Subject: [R] random forest significance testing tools
In-Reply-To: <c06795d3-e636-0963-32e3-69418cbc7a2b@gwdg.de>
References: <CAH6117+6Yw3kcUcdtNKJA2xE0=LC11QYXYB1Yax6ThH9gqTaDA@mail.gmail.com>
 <CAGxFJbSaLaF4uak1rA+qFpkCuuusidhWHLrtJVYoK300nSiHUA@mail.gmail.com>
 <c06795d3-e636-0963-32e3-69418cbc7a2b@gwdg.de>
Message-ID: <20200510194804.Horde.jcEhxIUy-BDAU7hGYgio9KH@www.ontargettek.com>

Hi everyone. I'm using a random forest in R to successfully perform a  
classification on a dichotomous DV in a dataset that has 29 IVs of  
type double and approximately 285,000 records. I ran my model on a  
70/30 train/test split of the original dataset.

I'm trying to use the rfUtilities package for rf model selection and  
performance evaluation, in order to generate a p-value and other  
quantitative performance statistics for use in hypothesis testing,  
similar to what I would do with a logistic regression glm model.

The initial random forest model results and OOB error estimates were  
as follows:

randomForest(formula = Class ~ ., data = train)
                Type of random forest: classification
                      Number of trees: 500
No. of variables tried at each split: 5

         OOB estimate of  error rate: 0.04%
Confusion matrix:
        0   1  class.error
0 199004  16 8.039393e-05
1     73 271 2.122093e-01


I'm running this model on my laptop (Win10, 8 GB RAM) as I don't have  
access to my server during the pandemic. The rfUtilities function call  
works (or at least it doesn't give me an error message or crash), but  
it's been running for over a day in RStudio on the original rf model  
and the training dataset without providing any results.

For anyone who has used the rfUtilities package before, is this just  
too large of a dataframe for a Win10 laptop to process effectively or  
should I be doing something different? This is my first time using the  
rfUtilities package and I understand that it is relatively new.

The function call for the rfUtilities function rf.significance is as  
follows (rf is my original random forest data model from the  
randomForest function):

rf.perm <- rf.significance(rf, train[,1:29], nperm=99, ntree=500)


Thanks in advance.

Tom Woolman
PhD student, Indiana State University


From r@oknz @end|ng |rom gm@||@com  Mon May 11 03:56:49 2020
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Mon, 11 May 2020 13:56:49 +1200
Subject: [R] the volcano orientation
In-Reply-To: <CAAcGz9-d1Hq7zBO0vaaTWnrETAMX64=zESvM=QKon4au2tgvAw@mail.gmail.com>
References: <CAAcGz9-d1Hq7zBO0vaaTWnrETAMX64=zESvM=QKon4au2tgvAw@mail.gmail.com>
Message-ID: <CABcYAdK4U7K_FwM=w04k5dK8rbbT5uKo-WyHBMmcBUQuaNwBYA@mail.gmail.com>

Hey, I know that volcano!  It's walking distance from the Intermediate
school I attended.
To you it's a plot; to me it's a place.
So I offer you four scenarios.

1. You think of it as a place you know and have been.
    In that case the "right" orientation is the one that best matches
what you are used to seeing.
    For me, that would put the peak on the right of the plot.

2. You think of it as a patch in a map.
    In that case the "right:" orientation is the one that matches the map.
    That would put the peak at the bottom of the plot.

3. You think of it as a product of geological processes, and are
perhaps interested in
    whether there is any connection between the orientation of the
volcano and the
    direction the Auckland hot-spot (currently at White Island) was moving.
    In that case you'd choose south-west -> north-east as the primary axis.
    (I think.  Not really sure.)

4. You think of it as a picture, an illustration in a textbook.  It
might need to be cropped
    vertically so you can fit another illustration on the same page.
For that and
    perceptual reasons you want the major linear axis of the image to
be  horizontal.
    In that case, what we have now is a perfectly reasonable choice.

"Quality is fitness for use."

On Sun, 10 May 2020 at 12:44, Michael Sumner <mdsumner at gmail.com> wrote:
>
> Does anyone know why 'volcano' is oriented as it is?
>
> image(volcano)  ## filled.contour is the same
>
> I know it's all arbitrary, but north-up is a pretty solid convention. Is
> there any reason why the classic 'image()' example data set would not
> default to this orientation?
>
> A Google map of the site (in Web Mercator):
>
> https://www.google.com/maps/place/Maungawhau+%2F+Mount+Eden/@-36.8763271,174.7619561,856m/data=!3m1!1e3!4m8!1m2!2m1!1smaungawhau!3m4!1s0x6d0d47db8d7bd1ff:0x8bcffe2a5c7360d2!8m2!3d-36.8666667!4d174.7666667
>
>
> For image(), the north-up orientation is 't(volcano[,ncol(volcano):1])'.
>
> If you are interested in a roughly georeferenced version I have code here:
>
> https://gist.github.com/mdsumner/20fe3ffa04421bf8e0517c19085e5fd8
>
> (Also see fortunes::fortune("conventions") )
>
> Best, Mike
>
>
> --
> Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> Hobart, Australia
> e-mail: mdsumner at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drj|m|emon @end|ng |rom gm@||@com  Mon May 11 08:09:43 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Mon, 11 May 2020 16:09:43 +1000
Subject: [R] Unable to install Sequin R
In-Reply-To: <CAFV24eHNdc_cm97D8iaGVW-R2TMRBwoDfkQmmaWKt+6x6_AMpg@mail.gmail.com>
References: <CAFV24eGLc6bkPCKYzF=HbsT9653MTcW+EqP60X2WhJyVp7Q-sg@mail.gmail.com>
 <CA+8X3fU9gVBOvsvB7CVQ=DMLG1Tf-drOtOxrCotqR8U8Qptbng@mail.gmail.com>
 <CAFV24eHNdc_cm97D8iaGVW-R2TMRBwoDfkQmmaWKt+6x6_AMpg@mail.gmail.com>
Message-ID: <CA+8X3fWFcS8it5iQjcwXj_MquKE5OHF6JEkaeH1j6wfVfyVk-A@mail.gmail.com>

Hi Harshita,
This usually means that R-4.0.0 is sufficiently different from R-3.6.3
that seqinr no longer works in R-4.0.0. You may want to go back to
R-3.6.3 if you really need it. You may also want to contact Simon
Penel, the maintainer (see the seqinr package page on CRAN) for
information on when an updated version will appear.

Jim

On Mon, May 11, 2020 at 3:31 PM Harshita Khedkar
<harshitakhedkar at gmail.com> wrote:
>
> Dear Jim,
>
> Thank you for your email, I tried the steps given by you and received the message as sequinr is not available for R version 4.0.
>
> Please find the attached screenshot.
> Please let me know how to proceed further.
>
>
> Thanks & Regards
>
>
> On Mon, May 11, 2020 at 6:40 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>>
>> Hi Harshita,
>> I think you are trying to install the package "seqinr" (Biological
>> Sequences Retrieval and Analysis).
>>
>> 1) When you see something like <packagename> in the help pages, it
>> means "Insert the name of the package here". So you really want:
>>
>> install.packages("seqinr")
>>
>> 2) Both spelling and capitalization matter in R. From your screenshot
>> you seem to be using Windows, where capitalization often doesn't
>> matter. So you have to spell words correctly and use the correct case.
>>
>> Jim
>>
>> On Mon, May 11, 2020 at 8:31 AM Harshita Khedkar
>> <harshitakhedkar at gmail.com> wrote:
>> >
>> > Hello,
>> >
>> > This is Harshita here,
>> >
>> > I am trying to install Sequin R and could not able to get it to work.
>> > Please find the attached error message, and let me know how o go ahead
>> > about it.
>> >
>> > Thank you and best regards,
>> > Harshita Khedkar.
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.


From petr@p|k@| @end|ng |rom prechez@@cz  Mon May 11 09:13:01 2020
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Mon, 11 May 2020 07:13:01 +0000
Subject: [R] unstable results of nlxb fit
In-Reply-To: <8cfa9396-7a22-ba80-b8be-48dd935fe408@gmail.com>
References: <745db2c879ba43149ff7e1f009219cca@SRVEXCHCM1302.precheza.cz>
 <16e4ad6a-12dd-ccfe-0b54-403be54d8b6e@gmail.com>
 <755635422.1450418.1588887698767@connect.xfinity.com>
 <8cfa9396-7a22-ba80-b8be-48dd935fe408@gmail.com>
Message-ID: <86f8ef8eb8ee48ec987cadea8ece3e0a@SRVEXCHCM1302.precheza.cz>

Dear all.

Thank you for your answers. I will try Duncan's approach (if I could manage 
it).

The issue is that first part of my data (actually temperature) up to certain 
time approximately follow one exponential. After that, another process 
prevails and the temperature increase starts to be "explosive". That is why I 
used these two exponentials. As I have many experiments I wanted to perform 
the fit programmatically.

Which leads me to the approach that in each cycle I perform a plot which I 
visually inspect. If I consider the fit satisfactory I keep results. If not, I 
perform the fit with different starting values until it is OK. I am aware that 
it is not optimal but should be easiest.

Thank you again.

Best regards
Petr

> -----Original Message-----
> From: J C Nash <profjcnash at gmail.com>
> Sent: Friday, May 8, 2020 12:00 AM
> To: Bernard McGarvey <mcgarvey.bernard at comcast.net>; PIKAL Petr
> <petr.pikal at precheza.cz>; r-help <r-help at r-project.org>
> Subject: Re: [R] unstable results of nlxb fit
>
> These results reflect my experience with this sort of problem.
>
> A couple of comments:
>
> 1) optimx package has a multistart wrapper. I probably should have written
> one for nlsr. Maybe Bernard and I should work on that. The issues are 
> largely
> to make things resistant to silly inputs, which even the smart users (you 
> know,
> the ones looking back from the mirror) introduce.
>
> 2) Sometimes using the bounds constraint capability in nlsr can be helpful,
> e.g., to ensure the exponent parameters are kept apart, can be useful.
>
> 3) Combining with Duncan's suggestion of solving for the linear parameters
> also helps.
>
> All of the above can be sensitive to particular data.
>
> Best, JN
>
> On 2020-05-07 5:41 p.m., Bernard McGarvey wrote:
> > John/Petr, I think there is an issue between a global optimum and local
> optima. I added a multistart loop around the code to see if I could find
> different solutions. Here is the code I added (I am not a great coder so 
> please
> excuse any inefficiencies in this code segment):
> >
> > # Multistart approach
> > NT <- 100
> > Results <- matrix(data=NA, nrow = NT, ncol=5,
> > dimnames=list(NULL,c("SS", "A", "B", "a", "b")))
> > A1 <- runif(NT,0,100)
> > B1 <- runif(NT,0,100)
> > a1 <- runif(NT,0.0,0.1)
> > b1 <- runif(NT,0.0,0.1)
> > for (I in 1:NT) {
> >   if (A1[I] > B1[I]) { # Ensure that the A'a are always the lower so that 
> > nlxb()
> always converge to the same values
> >     A0 <- A1[I]
> >     a0 <- a1[I]
> >     A1[I] <- B1[I]
> >     a1[I] <- b1[I]
> >     B1[I] <- A0
> >     b1[I] <- a0
> >   }
> >   fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
> >               start=list(A=A1[I], B=B1[I], a=a1[I], b=b1[I]))
> >   ccc <- coef(fit)
> >   Results[I,1] <- fit$ssquares
> >   Results[I,2] <- ccc[1]
> >   Results[I,3] <- ccc[2]
> >   Results[I,4] <- ccc[3]
> >   Results[I,5] <- ccc[4]
> > }
> > Results
> >
> > What I found is that the minimum SS generated at each trial had two
> distinct values, 417.8 and 3359.2. The A,B,a, and b values when the SS was
> 417.8 were all the same but I got different values for the case where the
> minimal SS was 3359.2. This indicates that the SS=417.8 may be the global
> minimum solution whereas the others are local optima. Here are the iteration
> results for a 100 trial multistart:
> >
> > Results
> >            SS           A           B           a           b
> >   [1,] 3359.2  8.3546e+03  6.8321e+00   -1.988226  2.6139e-02
> >   [2,] 3359.2  8.2865e+03  6.8321e+00   -5.201735  2.6139e-02
> >   [3,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >   [4,] 3359.2  6.8321e+00  7.7888e+02    0.026139 -7.2812e-01
> >   [5,] 3359.2 -3.9020e+01  4.5852e+01    0.026139  2.6139e-02
> >   [6,] 3359.2  6.8321e+00  2.6310e+02    0.026139 -1.8116e+00
> >   [7,] 3359.2 -2.1509e+01  2.8341e+01    0.026139  2.6139e-02
> >   [8,] 3359.2 -3.8075e+01  4.4908e+01    0.026139  2.6139e-02
> >   [9,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [10,] 3359.2  1.2466e+04  6.8321e+00   -4.196000  2.6139e-02
> >  [11,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
> >  [12,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [13,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [14,] 3359.2  3.8018e+02  6.8321e+00   -0.806414  2.6139e-02
> >  [15,] 3359.2 -3.1921e+00  1.0024e+01    0.026139  2.6139e-02
> >  [16,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [17,] 3359.2 -1.5938e+01  2.2770e+01    0.026139  2.6139e-02
> >  [18,] 3359.2 -3.1205e+01  3.8037e+01    0.026139  2.6139e-02
> >  [19,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [20,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [21,] 3359.2  8.6627e+03  6.8321e+00   -3.319778  2.6139e-02
> >  [22,] 3359.2  6.8321e+00  1.9318e+01    0.026139 -6.5773e-01
> >  [23,] 3359.2  6.2991e+01 -5.6159e+01    0.026139  2.6139e-02
> >  [24,] 3359.2  2.8865e-03  6.8321e+00   -1.576307  2.6139e-02
> >  [25,] 3359.2 -1.2496e+01  1.9328e+01    0.026139  2.6139e-02
> >  [26,] 3359.2 -5.9432e+00  1.2775e+01    0.026139  2.6139e-02
> >  [27,] 3359.2  1.6884e+02  6.8321e+00 -211.866423  2.6139e-02
> >  [28,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [29,] 3359.2  5.4972e+03  6.8321e+00   -3.432094  2.6139e-02
> >  [30,] 3359.2  6.8321e+00  1.4427e+03    0.026139 -4.2771e+02
> >  [31,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
> >  [32,] 3359.2  3.5760e+01 -2.8928e+01    0.026139  2.6139e-02
> >  [33,] 3359.2  6.8321e+00 -4.0737e+02    0.026139 -6.7152e-01
> >  [34,] 3359.2  6.8321e+00  1.2638e+04    0.026139 -2.8070e+00
> >  [35,] 3359.2  1.1813e+01 -4.9807e+00    0.026139  2.6139e-02
> >  [36,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [37,] 3359.2  6.8321e+00  1.2281e+03    0.026139 -3.0702e+02
> >  [38,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [39,] 3359.2 -2.6850e+01  3.3682e+01    0.026139  2.6139e-02
> >  [40,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [41,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
> >  [42,] 3359.2 -2.3279e+01  3.0111e+01    0.026139  2.6139e-02
> >  [43,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [44,] 3359.2  6.8321e+00  1.4550e+03    0.026139 -4.0303e+00
> >  [45,] 3359.2 -1.1386e+01  1.8218e+01    0.026139  2.6139e-02
> >  [46,] 3359.2  8.8026e+02  6.8321e+00  -65.430608  2.6139e-02
> >  [47,] 3359.2 -8.1985e+00  1.5031e+01    0.026139  2.6139e-02
> >  [48,] 3359.2 -6.7767e+00  1.3609e+01    0.026139  2.6139e-02
> >  [49,] 3359.2 -1.1436e+01  1.8268e+01    0.026139  2.6139e-02
> >  [50,] 3359.2  1.0710e+04  6.8321e+00   -2.349659  2.6139e-02
> >  [51,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
> >  [52,] 3359.2  6.8321e+00  7.1837e+02    0.026139 -7.4681e-01
> >  [53,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [54,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
> >  [55,] 3359.2 -4.8774e+00  6.8321e+00  -16.405584  2.6139e-02
> >  [56,] 3359.2  1.2687e+03  6.8321e+00   -3.775998  2.6139e-02
> >  [57,] 3359.2  1.5529e+01 -8.6967e+00    0.026139  2.6139e-02
> >  [58,] 3359.2 -1.0003e+01  1.6835e+01    0.026139  2.6139e-02
> >  [59,] 3359.2  6.8321e+00  3.9291e+02    0.026139 -4.1974e+02
> >  [60,] 3359.2 -2.1880e+01  2.8712e+01    0.026139  2.6139e-02
> >  [61,] 3359.2  4.1736e+03  6.8321e+00  -10.711457  2.6139e-02
> >  [62,] 3359.2 -3.3185e+01  4.0017e+01    0.026139  2.6139e-02
> >  [63,] 3359.2  7.6732e+02  6.8321e+00   -0.723977  2.6139e-02
> >  [64,] 3359.2  1.5334e+04  6.8321e+00  -52.573620  2.6139e-02
> >  [65,] 3359.2 -2.9556e+01  3.6388e+01    0.026139  2.6139e-02
> >  [66,] 3359.2 -1.0447e+00  7.8767e+00    0.026139  2.6139e-02
> >  [67,] 3359.2  6.8321e+00  2.1471e+02    0.026139 -7.0582e+01
> >  [68,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
> >  [69,] 3359.2 -2.2293e+01  2.9126e+01    0.026139  2.6139e-02
> >  [70,] 3359.2  6.2259e+02  6.8321e+00   -2.782527  2.6139e-02
> >  [71,] 3359.2 -1.4639e+01  2.1471e+01    0.026139  2.6139e-02
> >  [72,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [73,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [74,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [75,] 3359.2 -2.3449e+01  3.0281e+01    0.026139  2.6139e-02
> >  [76,] 3359.2 -2.5926e+01  6.8321e+00   -0.663656  2.6139e-02
> >  [77,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
> >  [78,] 3359.2  6.8321e+00  6.9426e+02    0.026139 -1.9442e+00
> >  [79,] 3359.2  2.8684e+02  6.8321e+00   -0.854394  2.6139e-02
> >  [80,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [81,] 3359.2 -4.5066e+01  5.1899e+01    0.026139  2.6139e-02
> >  [82,] 3359.2  4.4678e+03  6.8321e+00   -2.109446  2.6139e-02
> >  [83,] 3359.2  3.1376e+03  6.8321e+00   -1.104803  2.6139e-02
> >  [84,] 3359.2  6.8321e+00  1.1167e+02    0.026139 -1.0280e+00
> >  [85,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [86,] 3359.2  5.3864e+02  6.8321e+00   -0.657971  2.6139e-02
> >  [87,] 3359.2  4.8227e+01  6.8321e+00   -2.304024  2.6139e-02
> >  [88,] 3359.2 -2.2048e+01  2.8880e+01    0.026139  2.6139e-02
> >  [89,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [90,] 3359.2  6.8321e+00 -4.1689e+01    0.026139 -3.6049e+00
> >  [91,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
> >  [92,] 3359.2 -4.1265e+01  4.8097e+01    0.026139  2.6139e-02
> >  [93,] 3359.2 -1.1565e+01  1.8397e+01    0.026139  2.6139e-02
> >  [94,] 3359.2  2.3698e+01 -1.6866e+01    0.026139  2.6139e-02
> >  [95,] 3359.2  4.4700e+03  6.8321e+00  -12.836180  2.6139e-02
> >  [96,] 3359.2  4.6052e+04  6.8321e+00   -7.158584  2.6139e-02
> >  [97,] 3359.2  2.5464e+03  6.8321e+00   -1.811626  2.6139e-02
> >  [98,] 3359.2  6.8321e+00  1.0338e+03    0.026139 -1.5365e+01
> >  [99,] 3359.2  1.3783e+01 -6.9507e+00    0.026139  2.6139e-02
> > [100,] 3359.2  6.8321e+00  6.7153e+02    0.026139 -1.5975e+03
> >
> >
> > Hope this helps,
> >
> > Bernard McGarvey
> >
> >
> > Director, Fort Myers Beach Lions Foundation, Inc.
> >
> >
> > Retired (Lilly Engineering Fellow).
> >
> >> On May 7, 2020 at 9:33 AM J C Nash <profjcnash at gmail.com> wrote:
> >>
> >>
> >> The double exponential is well-known as a disaster to fit. Lanczos in
> >> his
> >> 1956 book Applied Analysis, p. 276 gives a good example which is worked
> through.
> >> I've included it with scripts using nlxb in my 2014 book on Nonlinear
> >> Parameter Optimization Using R Tools (Wiley). The scripts were on
> >> Wiley's site for the book, but I've had difficulty getting Wiley to
> >> fix things and not checked lately if it is still accessible. Ask
> >> off-list if you want the script and I'll dig into my archives.
> >>
> >> nlxb (preferably from nlsr which you used rather than nlmrt which is
> >> now not maintained), will likely do as well as any general purpose
> >> code. There may be special approaches that do a bit better, but I
> >> suspect the reality is that the underlying problem is such that there
> >> are many sets of parameters with widely different values that will get 
> >> quite
> similar sums of squares.
> >>
> >> Best, JN
> >>
> >>
> >> On 2020-05-07 9:12 a.m., PIKAL Petr wrote:
> >>> Dear all
> >>>
> >>> I started to use nlxb instead of nls to get rid of singular gradient 
> >>> error.
> >>> I try to fit double exponential function to my data, but results I
> >>> obtain are strongly dependent on starting values.
> >>>
> >>> tsmes ~ A*exp(a*plast) + B* exp(b*plast)
> >>>
> >>> Changing b from 0.1 to 0.01 gives me completely different results. I
> >>> usually check result by a plot but could the result be inspected if
> >>> it achieved good result without plotting?
> >>>
> >>> Or is there any way how to perform such task?
> >>>
> >>> Cheers
> >>> Petr
> >>>
> >>> Below is working example.
> >>>
> >>>> dput(temp)
> >>> temp <- structure(list(tsmes = c(31, 32, 32, 32, 32, 32, 32, 32, 33,
> >>> 34, 35, 35, 36, 36, 36, 37, 38, 39, 40, 40, 40, 40, 40, 41, 43, 44,
> >>> 44, 44, 46, 47, 47, 47, 47, 48, 49, 51, 51, 51, 52, 53, 54, 54, 55,
> >>> 57, 57, 57, 59, 59, 60, 62, 63, 64, 65, 66, 66, 67, 67, 68, 70, 72,
> >>> 74, 76, 78, 81, 84, 85, 86, 88, 90, 91, 92, 94, 96, 97, 99, 100,
> >>> 102, 104, 106, 109, 112, 115, 119, 123, 127, 133, 141, 153, 163,
> >>> 171), plast = c(50, 51, 52, 52, 53, 53, 53, 54, 55, 55, 56, 57, 58,
> >>> 59, 60, 61, 62, 63, 64, 64, 64, 65, 65, 66, 66, 67, 68, 69, 70, 71,
> >>> 72, 73, 74, 75, 75, 76, 76, 77, 77, 78, 78, 79, 80, 81, 82, 83, 84,
> >>> 85, 85, 86, 86, 87, 88, 88, 89, 90, 91, 91, 93, 93, 94, 95, 96, 96,
> >>> 97, 98, 98, 99, 100, 100, 101, 102, 103, 103, 104, 105, 106, 107,
> >>> 107, 108, 109, 110, 111, 112, 112, 113, 113, 114, 115, 116)),
> >>> row.names = 2411:2500, class = "data.frame")
> >>>
> >>> library(nlsr)
> >>>
> >>> fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
> >>> start=list(A=1, B=15, a=0.025, b=0.01))
> >>> coef(fit)
> >>>            A            B            a            b
> >>> 3.945167e-13 9.772749e+00 2.802274e-01 2.179781e-02
> >>>
> >>> plot(temp$plast, temp$tsmes, ylim=c(0,200)) lines(temp$plast,
> >>> predict(fit, newdata=temp), col="pink", lwd=3) ccc <- coef(fit)
> >>> lines(0:120,ccc[1]*exp(ccc[3]*(0:120)))
> >>> lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2)
> >>>
> >>> # wrong fit with slightly different b fit <- nlxb(tsmes ~
> >>> A*exp(a*plast) + B* exp(b*plast), data=temp, start=list(A=1, B=15,
> >>> a=0.025, b=0.1))
> >>> coef(fit)
> >>>            A            B            a            b
> >>> 2911.6448377    6.8320597  -49.1373979    0.0261391
> >>> lines(temp$plast, predict(fit, newdata=temp), col="red", lwd=3) ccc
> >>> <- coef(fit) lines(0:120,ccc[1]*exp(ccc[3]*(0:120)), col="red")
> >>> lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2, col="red")
> >>>
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.

From k@r|@@ch||||ng @end|ng |rom un|-bonn@de  Mon May 11 09:48:13 2020
From: k@r|@@ch||||ng @end|ng |rom un|-bonn@de (Karl Schilling)
Date: Mon, 11 May 2020 09:48:13 +0200
Subject: [R] Unable to install Sequin R
In-Reply-To: <CA+8X3fWFcS8it5iQjcwXj_MquKE5OHF6JEkaeH1j6wfVfyVk-A@mail.gmail.com>
References: <CAFV24eGLc6bkPCKYzF=HbsT9653MTcW+EqP60X2WhJyVp7Q-sg@mail.gmail.com>
 <CA+8X3fU9gVBOvsvB7CVQ=DMLG1Tf-drOtOxrCotqR8U8Qptbng@mail.gmail.com>
 <CAFV24eHNdc_cm97D8iaGVW-R2TMRBwoDfkQmmaWKt+6x6_AMpg@mail.gmail.com>
 <CA+8X3fWFcS8it5iQjcwXj_MquKE5OHF6JEkaeH1j6wfVfyVk-A@mail.gmail.com>
Message-ID: <41cf8570-ea43-4229-207f-174e2ed5eb11@uni-bonn.de>

Dear Harshita,

I just tried to install seqinr on my win10 machine (R4.0) and had no 
problem.

In your mail, you still had a typo: you wrote seqinr with an "u" - maybe 
that is the problem

Best
-- 
Karl Schilling


From b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com  Mon May 11 03:59:14 2020
From: b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com (Alessandra Bielli)
Date: Sun, 10 May 2020 19:59:14 -0600
Subject: [R] predicting waste per capita - is a gaussian model correct?
In-Reply-To: <CAB8pepwn-cP8abZ=1Jw7yKnd8z+5Er1MNSUpR7ZD+SSNkaPCOA@mail.gmail.com>
References: <CA+6N3yXDvFdjUeCKgoKi=3ZAnKLTv1jn0hodN1QN8=Bs79840g@mail.gmail.com>
 <CAHrK516nu-TmMJq=Rzp1ddYr=RAnX41k8=aPfhgE4ZJNFbf88A@mail.gmail.com>
 <CAB8pepwn-cP8abZ=1Jw7yKnd8z+5Er1MNSUpR7ZD+SSNkaPCOA@mail.gmail.com>
Message-ID: <CA+6N3yV7-_ZJ5TG8BQxWYwDQoenSuBNqvGt6EaLzcz63qWfe4A@mail.gmail.com>

Dear all

First of all apologies for the off-topic question and for not respecting
the other points.
Second, thanks for your advice and opinion I will definitely consult a
statistician.

Regards,

Alessandra

On Sun, May 10, 2020 at 4:57 PM Abby Spurdle <spurdle.a at gmail.com> wrote:

> Well, this is 100% off-topic...
> And I wasn't planning to answer the OP's question.
>
> However, I disagree with your answer.
>
> > There is no requirement that the dependent variable in a "regression"
> type
> > estimation follows a gaussian distribution.
>
> False.
> It's depends on what type of '"regression" type estimation' one uses,
> among other things.
>
> > You need a model of the
> > process and then use an estimation technique to estimate your model.  If
> > effects in your model are additive do not use a log transformation. If
> > effects are multiplicative then use a log transformation.
>
> The main question is, does the model satisfy the *assumptions*.
>
> > The choice
> > should be determined by the mechanics of the problem and not by the
> > statistics.
>
> While a mechanistic understanding is definitely valuable...
> If the criteria for a good model vs a bad model, was whether the model
> was consistent with mechanistic theory/understanding, then nearly
> every statistical model I've seen would be a bad model.
> I would say, a good model is one that is useful...
>
> > If you do use a log transformation the trying to reverse the
> > process using an exponential transformation will be biased.
> > The extent of
> > that bias depends on your problem and it would not be possible to
> estimate
> > the significance of the bias without a much greater knowledge of the
> > process and data.
>
> Never heard of this before...
> But I do note back-transformation is not trivial, and I'm not an
> expert on back-transformations.
>
> > I would suggest that you consult a competent
> > statistician.
>
> I agree on that part...
>

	[[alternative HTML version deleted]]


From @denener @end|ng |rom hotm@||@com  Mon May 11 16:28:59 2020
From: @denener @end|ng |rom hotm@||@com (karl adenener)
Date: Mon, 11 May 2020 14:28:59 +0000
Subject: [R] My dream ...
Message-ID: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>

It would be a dream, there would be a R-based software, which I configure according to my study (type of data, limits for meaningful measurements, handling of outliers and missing measurements, test method etc.), which then reads my original measurement data and after some computing time the software provides me with the statistical analysis. All steps of the evaluation have to be defined before the start of the study and cannot be changed after the start of the study.

Where could problems arise?
Does anyone know of a suitable R-Package or software?
Does anyone have the time and inclination to create a flexibly customizable package?

greetings
Adenener

	[[alternative HTML version deleted]]


From m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com  Mon May 11 17:19:51 2020
From: m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com (Patrick (Malone Quantitative))
Date: Mon, 11 May 2020 11:19:51 -0400
Subject: [R] My dream ...
In-Reply-To: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
Message-ID: <CAJc=yOFdYqEn58bcgcQ0hmQTUn8ixp_=BKoUebXr7rKQYHiQXw@mail.gmail.com>

This isn't an R code question and you posted in HTML, but briefly:

Simulate data that could arise from your study, including missing and
outliers, then write code that runs th analyses. Put the code in an
open-science archive.

Then run it as is when you actually have the data.

There will probably be some hiccups depending on how good your
simulation is, but that's the in-principle solution.

On Mon, May 11, 2020 at 11:10 AM karl adenener <adenener at hotmail.com> wrote:
>
> It would be a dream, there would be a R-based software, which I configure according to my study (type of data, limits for meaningful measurements, handling of outliers and missing measurements, test method etc.), which then reads my original measurement data and after some computing time the software provides me with the statistical analysis. All steps of the evaluation have to be defined before the start of the study and cannot be changed after the start of the study.
>
> Where could problems arise?
> Does anyone know of a suitable R-Package or software?
> Does anyone have the time and inclination to create a flexibly customizable package?
>
> greetings
> Adenener
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Patrick S. Malone, Ph.D., Malone Quantitative
NEW Service Models: http://malonequantitative.com

He/Him/His


From jwd @end|ng |rom @urewe@t@net  Mon May 11 18:58:09 2020
From: jwd @end|ng |rom @urewe@t@net (John)
Date: Mon, 11 May 2020 09:58:09 -0700
Subject: [R] the volcano orientation
In-Reply-To: <CABcYAdK4U7K_FwM=w04k5dK8rbbT5uKo-WyHBMmcBUQuaNwBYA@mail.gmail.com>
References: <CAAcGz9-d1Hq7zBO0vaaTWnrETAMX64=zESvM=QKon4au2tgvAw@mail.gmail.com>
 <CABcYAdK4U7K_FwM=w04k5dK8rbbT5uKo-WyHBMmcBUQuaNwBYA@mail.gmail.com>
Message-ID: <20200511095809.51439268@Draco>

Out of curiosity, and considering the bewildering array of projections
and grids in use for various mapping purposes, you seem to be saying in your
example 2 that the grid coordinates number south to north and east to
west.  Given scale of the coordinate numbers, would that be a national
grid system employed in New Zealnd?

J. W. Dougherty

On Mon, 11 May 2020 13:56:49 +1200
"Richard O'Keefe" <raoknz at gmail.com> wrote:

> Hey, I know that volcano!  It's walking distance from the Intermediate
> school I attended.
> To you it's a plot; to me it's a place.
> So I offer you four scenarios.
> 
> 1. You think of it as a place you know and have been.
>     In that case the "right" orientation is the one that best matches
> what you are used to seeing.
>     For me, that would put the peak on the right of the plot.
> 
> 2. You think of it as a patch in a map.
>     In that case the "right:" orientation is the one that matches the
> map. That would put the peak at the bottom of the plot.
> 
> 3. You think of it as a product of geological processes, and are
> perhaps interested in
>     whether there is any connection between the orientation of the
> volcano and the
>     direction the Auckland hot-spot (currently at White Island) was
> moving. In that case you'd choose south-west -> north-east as the
> primary axis. (I think.  Not really sure.)
> 
> 4. You think of it as a picture, an illustration in a textbook.  It
> might need to be cropped
>     vertically so you can fit another illustration on the same page.
> For that and
>     perceptual reasons you want the major linear axis of the image to
> be  horizontal.
>     In that case, what we have now is a perfectly reasonable choice.
> 
> "Quality is fitness for use."
> 
> On Sun, 10 May 2020 at 12:44, Michael Sumner <mdsumner at gmail.com>
> wrote:
> >
> > Does anyone know why 'volcano' is oriented as it is?
> >
> > image(volcano)  ## filled.contour is the same
> >
> > I know it's all arbitrary, but north-up is a pretty solid
> > convention. Is there any reason why the classic 'image()' example
> > data set would not default to this orientation?
> >
> > A Google map of the site (in Web Mercator):
> >
> > https://www.google.com/maps/place/Maungawhau+%2F+Mount+Eden/@-36.8763271,174.7619561,856m/data=!3m1!1e3!4m8!1m2!2m1!1smaungawhau!3m4!1s0x6d0d47db8d7bd1ff:0x8bcffe2a5c7360d2!8m2!3d-36.8666667!4d174.7666667
> >
> >
> > For image(), the north-up orientation is
> > 't(volcano[,ncol(volcano):1])'.
> >
> > If you are interested in a roughly georeferenced version I have
> > code here:
> >
> > https://gist.github.com/mdsumner/20fe3ffa04421bf8e0517c19085e5fd8
> >
> > (Also see fortunes::fortune("conventions") )
> >
> > Best, Mike
> >
> >
> > --
> > Michael Sumner
> > Software and Database Engineer
> > Australian Antarctic Division
> > Hobart, Australia
> > e-mail: mdsumner at gmail.com
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.  
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.


From 538280 @end|ng |rom gm@||@com  Mon May 11 20:11:03 2020
From: 538280 @end|ng |rom gm@||@com (Greg Snow)
Date: Mon, 11 May 2020 12:11:03 -0600
Subject: [R] My dream ...
In-Reply-To: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
Message-ID: <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>

It is a nice dream, but it is really abdicating ethical responsibility
to the computer instead of the researcher.  And I personally don't
trust computers over people for this.

What could go wrong?

First, how do you guarantee that the statistical plan was locked in
place before the data was collected?  With your proposed system some
people will still run a plan on their data, make changes, run again,
etc. until they get what they want, then claim that the statistical
plan came before the data that was used to tune it (I consider this
unethical, but see no way for R or computers in general to prevent
this without human oversight).

Second, what if the data shows something that you did not anticipate?
This methodology would prevent you doing Exploratory Data Analysis and
adapting accordingly.  If people start using this as a black box that
is "blessed" by the package as purely "objective", then many will not
even do any EDA and take the results as "True" when they are not even
appropriate.

Better would be a regular system for submitting your code to a
clinical trial registry or some other pre-study registry so that other
can compare what you did to what you claimed you were going to do.  If
you don't want to submit the entire code, you could submit an md5 hash
to the registry, then others could check to see if the code ran (put
into an online supplement) differs from the registered version.  If
the data show something unanticipated then you can show the original
code and the modified code along with your reasoning and let the
consumer decide if the changes were justified.

This could still be worked around if someone was really motivated, but
it is probably the best we will see.

In my opinion the advantage of computers is not Artificial
Intelligence, but rather Artificial Patience (most AI that I have seen
is really doing a bunch of what I would consider to be boring, really
fast so people don't have to).  Leave the Intelligence to the people.

On Mon, May 11, 2020 at 9:10 AM karl adenener <adenener at hotmail.com> wrote:
>
> It would be a dream, there would be a R-based software, which I configure according to my study (type of data, limits for meaningful measurements, handling of outliers and missing measurements, test method etc.), which then reads my original measurement data and after some computing time the software provides me with the statistical analysis. All steps of the evaluation have to be defined before the start of the study and cannot be changed after the start of the study.
>
> Where could problems arise?
> Does anyone know of a suitable R-Package or software?
> Does anyone have the time and inclination to create a flexibly customizable package?
>
> greetings
> Adenener
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From rkoenker @end|ng |rom ||||no|@@edu  Mon May 11 21:22:27 2020
From: rkoenker @end|ng |rom ||||no|@@edu (Koenker, Roger W)
Date: Mon, 11 May 2020 19:22:27 +0000
Subject: [R] My dream ...
In-Reply-To: <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
Message-ID: <B6991FF7-A8F0-4B8C-A8F6-B419F397FCED@illinois.edu>

Definitely a fortune:

"the advantage of computers is not Artificial
Intelligence, but rather Artificial Patience"

Greg Snow in response to a question about automated R-analysis.

Roger Koenker
r.koenker at ucl.ac.uk<mailto:r.koenker at ucl.ac.uk>
Honorary Professor of Economics
Department of Economics, UCL
Emeritus Professor of Economics
and Statistics, UIUC


On May 11, 2020, at 7:11 PM, Greg Snow <538280 at gmail.com<mailto:538280 at gmail.com>> wrote:

It is a nice dream, but it is really abdicating ethical responsibility
to the computer instead of the researcher.  And I personally don't
trust computers over people for this.

What could go wrong?

First, how do you guarantee that the statistical plan was locked in
place before the data was collected?  With your proposed system some
people will still run a plan on their data, make changes, run again,
etc. until they get what they want, then claim that the statistical
plan came before the data that was used to tune it (I consider this
unethical, but see no way for R or computers in general to prevent
this without human oversight).

Second, what if the data shows something that you did not anticipate?
This methodology would prevent you doing Exploratory Data Analysis and
adapting accordingly.  If people start using this as a black box that
is "blessed" by the package as purely "objective", then many will not
even do any EDA and take the results as "True" when they are not even
appropriate.

Better would be a regular system for submitting your code to a
clinical trial registry or some other pre-study registry so that other
can compare what you did to what you claimed you were going to do.  If
you don't want to submit the entire code, you could submit an md5 hash
to the registry, then others could check to see if the code ran (put
into an online supplement) differs from the registered version.  If
the data show something unanticipated then you can show the original
code and the modified code along with your reasoning and let the
consumer decide if the changes were justified.

This could still be worked around if someone was really motivated, but
it is probably the best we will see.

In my opinion the advantage of computers is not Artificial
Intelligence, but rather Artificial Patience (most AI that I have seen
is really doing a bunch of what I would consider to be boring, really
fast so people don't have to).  Leave the Intelligence to the people.

On Mon, May 11, 2020 at 9:10 AM karl adenener <adenener at hotmail.com<mailto:adenener at hotmail.com>> wrote:

It would be a dream, there would be a R-based software, which I configure according to my study (type of data, limits for meaningful measurements, handling of outliers and missing measurements, test method etc.), which then reads my original measurement data and after some computing time the software provides me with the statistical analysis. All steps of the evaluation have to be defined before the start of the study and cannot be changed after the start of the study.

Where could problems arise?
Does anyone know of a suitable R-Package or software?
Does anyone have the time and inclination to create a flexibly customizable package?

greetings
Adenener

       [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.



--
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com<mailto:538280 at gmail.com>

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon May 11 21:26:44 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 11 May 2020 12:26:44 -0700
Subject: [R] My dream ...
In-Reply-To: <B6991FF7-A8F0-4B8C-A8F6-B419F397FCED@illinois.edu>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
 <B6991FF7-A8F0-4B8C-A8F6-B419F397FCED@illinois.edu>
Message-ID: <27988C9E-6D16-4D34-9613-761B864210D4@dcn.davis.ca.us>

Seconded!

On May 11, 2020 12:22:27 PM PDT, "Koenker, Roger W" <rkoenker at illinois.edu> wrote:
>Definitely a fortune:
>
>"the advantage of computers is not Artificial
>Intelligence, but rather Artificial Patience"
>
>Greg Snow in response to a question about automated R-analysis.
>
>Roger Koenker
>r.koenker at ucl.ac.uk<mailto:r.koenker at ucl.ac.uk>
>Honorary Professor of Economics
>Department of Economics, UCL
>Emeritus Professor of Economics
>and Statistics, UIUC
>
>
>On May 11, 2020, at 7:11 PM, Greg Snow
><538280 at gmail.com<mailto:538280 at gmail.com>> wrote:
>
>It is a nice dream, but it is really abdicating ethical responsibility
>to the computer instead of the researcher.  And I personally don't
>trust computers over people for this.
>
>What could go wrong?
>
>First, how do you guarantee that the statistical plan was locked in
>place before the data was collected?  With your proposed system some
>people will still run a plan on their data, make changes, run again,
>etc. until they get what they want, then claim that the statistical
>plan came before the data that was used to tune it (I consider this
>unethical, but see no way for R or computers in general to prevent
>this without human oversight).
>
>Second, what if the data shows something that you did not anticipate?
>This methodology would prevent you doing Exploratory Data Analysis and
>adapting accordingly.  If people start using this as a black box that
>is "blessed" by the package as purely "objective", then many will not
>even do any EDA and take the results as "True" when they are not even
>appropriate.
>
>Better would be a regular system for submitting your code to a
>clinical trial registry or some other pre-study registry so that other
>can compare what you did to what you claimed you were going to do.  If
>you don't want to submit the entire code, you could submit an md5 hash
>to the registry, then others could check to see if the code ran (put
>into an online supplement) differs from the registered version.  If
>the data show something unanticipated then you can show the original
>code and the modified code along with your reasoning and let the
>consumer decide if the changes were justified.
>
>This could still be worked around if someone was really motivated, but
>it is probably the best we will see.
>
>In my opinion the advantage of computers is not Artificial
>Intelligence, but rather Artificial Patience (most AI that I have seen
>is really doing a bunch of what I would consider to be boring, really
>fast so people don't have to).  Leave the Intelligence to the people.
>
>On Mon, May 11, 2020 at 9:10 AM karl adenener
><adenener at hotmail.com<mailto:adenener at hotmail.com>> wrote:
>
>It would be a dream, there would be a R-based software, which I
>configure according to my study (type of data, limits for meaningful
>measurements, handling of outliers and missing measurements, test
>method etc.), which then reads my original measurement data and after
>some computing time the software provides me with the statistical
>analysis. All steps of the evaluation have to be defined before the
>start of the study and cannot be changed after the start of the study.
>
>Where could problems arise?
>Does anyone know of a suitable R-Package or software?
>Does anyone have the time and inclination to create a flexibly
>customizable package?
>
>greetings
>Adenener
>
>       [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
>UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>
>--
>Gregory (Greg) L. Snow Ph.D.
>538280 at gmail.com<mailto:538280 at gmail.com>
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Mon May 11 21:50:03 2020
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Mon, 11 May 2020 19:50:03 +0000
Subject: [R] R package for discrete-time competing-risk anlayses..
In-Reply-To: <B6991FF7-A8F0-4B8C-A8F6-B419F397FCED@illinois.edu>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>,
 <B6991FF7-A8F0-4B8C-A8F6-B419F397FCED@illinois.edu>
Message-ID: <MN2PR03MB5167C91EF77FF078D430AA31E2A10@MN2PR03MB5167.namprd03.prod.outlook.com>

Can someone direct me to an R package that can run discrete-time competing risk analyses?

Thank you,

John



John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)


________________________________
From: R-help <r-help-bounces at r-project.org> on behalf of Koenker, Roger W <rkoenker at illinois.edu>
Sent: Monday, May 11, 2020 3:22 PM
To: Greg Snow <538280 at gmail.com>
Cc: r-help at r-project.org <r-help at r-project.org>; karl adenener <adenener at hotmail.com>
Subject: Re: [R] My dream ...

Definitely a fortune:

"the advantage of computers is not Artificial
Intelligence, but rather Artificial Patience"

Greg Snow in response to a question about automated R-analysis.

Roger Koenker
r.koenker at ucl.ac.uk<mailto:r.koenker at ucl.ac.uk>
Honorary Professor of Economics
Department of Economics, UCL
Emeritus Professor of Economics
and Statistics, UIUC


On May 11, 2020, at 7:11 PM, Greg Snow <538280 at gmail.com<mailto:538280 at gmail.com>> wrote:

It is a nice dream, but it is really abdicating ethical responsibility
to the computer instead of the researcher.  And I personally don't
trust computers over people for this.

What could go wrong?

First, how do you guarantee that the statistical plan was locked in
place before the data was collected?  With your proposed system some
people will still run a plan on their data, make changes, run again,
etc. until they get what they want, then claim that the statistical
plan came before the data that was used to tune it (I consider this
unethical, but see no way for R or computers in general to prevent
this without human oversight).

Second, what if the data shows something that you did not anticipate?
This methodology would prevent you doing Exploratory Data Analysis and
adapting accordingly.  If people start using this as a black box that
is "blessed" by the package as purely "objective", then many will not
even do any EDA and take the results as "True" when they are not even
appropriate.

Better would be a regular system for submitting your code to a
clinical trial registry or some other pre-study registry so that other
can compare what you did to what you claimed you were going to do.  If
you don't want to submit the entire code, you could submit an md5 hash
to the registry, then others could check to see if the code ran (put
into an online supplement) differs from the registered version.  If
the data show something unanticipated then you can show the original
code and the modified code along with your reasoning and let the
consumer decide if the changes were justified.

This could still be worked around if someone was really motivated, but
it is probably the best we will see.

In my opinion the advantage of computers is not Artificial
Intelligence, but rather Artificial Patience (most AI that I have seen
is really doing a bunch of what I would consider to be boring, really
fast so people don't have to).  Leave the Intelligence to the people.

On Mon, May 11, 2020 at 9:10 AM karl adenener <adenener at hotmail.com<mailto:adenener at hotmail.com>> wrote:

It would be a dream, there would be a R-based software, which I configure according to my study (type of data, limits for meaningful measurements, handling of outliers and missing measurements, test method etc.), which then reads my original measurement data and after some computing time the software provides me with the statistical analysis. All steps of the evaluation have to be defined before the start of the study and cannot be changed after the start of the study.

Where could problems arise?
Does anyone know of a suitable R-Package or software?
Does anyone have the time and inclination to create a flexibly customizable package?

greetings
Adenener

       [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761516112&amp;sdata=0uX2IMimnT3HFKDY%2Fubv0JjEMYcSEbOtgzqEuzDPFow%3D&amp;reserved=0
PLEASE do read the posting guide https://nam03.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761516112&amp;sdata=UwzCHC4JGmC0YbjliQ8XhUdMSGju7EeyWfYVRFFVcjE%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.



--
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com<mailto:538280 at gmail.com>

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761516112&amp;sdata=0uX2IMimnT3HFKDY%2Fubv0JjEMYcSEbOtgzqEuzDPFow%3D&amp;reserved=0
PLEASE do read the posting guide https://nam03.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761526108&amp;sdata=PXbXmhRfsosD1leVo19l7o95QrQ2OBdGlSAiUNWIgNs%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.


        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761526108&amp;sdata=JE5ixJ4K6LIvA5DyhterWla8jryKhDLCnblbQFNd93o%3D&amp;reserved=0
PLEASE do read the posting guide https://nam03.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761526108&amp;sdata=PXbXmhRfsosD1leVo19l7o95QrQ2OBdGlSAiUNWIgNs%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Mon May 11 22:50:28 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 11 May 2020 13:50:28 -0700
Subject: [R] R package for discrete-time competing-risk anlayses..
In-Reply-To: <MN2PR03MB5167C91EF77FF078D430AA31E2A10@MN2PR03MB5167.namprd03.prod.outlook.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
 <B6991FF7-A8F0-4B8C-A8F6-B419F397FCED@illinois.edu>
 <MN2PR03MB5167C91EF77FF078D430AA31E2A10@MN2PR03MB5167.namprd03.prod.outlook.com>
Message-ID: <CAGxFJbSCmGXMPyYmU0ROZP+r=hAwiO6HagdzF5VAA7aTx2HHbw@mail.gmail.com>

Search!
"discrete time competing risk analysis" on rseek.org

The survival task view on CRAN:
https://CRAN.R-project.org/view=Survival

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Mon, May 11, 2020 at 12:50 PM Sorkin, John <jsorkin at som.umaryland.edu> wrote:
>
> Can someone direct me to an R package that can run discrete-time competing risk analyses?
>
> Thank you,
>
> John
>
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
>
> ________________________________
> From: R-help <r-help-bounces at r-project.org> on behalf of Koenker, Roger W <rkoenker at illinois.edu>
> Sent: Monday, May 11, 2020 3:22 PM
> To: Greg Snow <538280 at gmail.com>
> Cc: r-help at r-project.org <r-help at r-project.org>; karl adenener <adenener at hotmail.com>
> Subject: Re: [R] My dream ...
>
> Definitely a fortune:
>
> "the advantage of computers is not Artificial
> Intelligence, but rather Artificial Patience"
>
> Greg Snow in response to a question about automated R-analysis.
>
> Roger Koenker
> r.koenker at ucl.ac.uk<mailto:r.koenker at ucl.ac.uk>
> Honorary Professor of Economics
> Department of Economics, UCL
> Emeritus Professor of Economics
> and Statistics, UIUC
>
>
> On May 11, 2020, at 7:11 PM, Greg Snow <538280 at gmail.com<mailto:538280 at gmail.com>> wrote:
>
> It is a nice dream, but it is really abdicating ethical responsibility
> to the computer instead of the researcher.  And I personally don't
> trust computers over people for this.
>
> What could go wrong?
>
> First, how do you guarantee that the statistical plan was locked in
> place before the data was collected?  With your proposed system some
> people will still run a plan on their data, make changes, run again,
> etc. until they get what they want, then claim that the statistical
> plan came before the data that was used to tune it (I consider this
> unethical, but see no way for R or computers in general to prevent
> this without human oversight).
>
> Second, what if the data shows something that you did not anticipate?
> This methodology would prevent you doing Exploratory Data Analysis and
> adapting accordingly.  If people start using this as a black box that
> is "blessed" by the package as purely "objective", then many will not
> even do any EDA and take the results as "True" when they are not even
> appropriate.
>
> Better would be a regular system for submitting your code to a
> clinical trial registry or some other pre-study registry so that other
> can compare what you did to what you claimed you were going to do.  If
> you don't want to submit the entire code, you could submit an md5 hash
> to the registry, then others could check to see if the code ran (put
> into an online supplement) differs from the registered version.  If
> the data show something unanticipated then you can show the original
> code and the modified code along with your reasoning and let the
> consumer decide if the changes were justified.
>
> This could still be worked around if someone was really motivated, but
> it is probably the best we will see.
>
> In my opinion the advantage of computers is not Artificial
> Intelligence, but rather Artificial Patience (most AI that I have seen
> is really doing a bunch of what I would consider to be boring, really
> fast so people don't have to).  Leave the Intelligence to the people.
>
> On Mon, May 11, 2020 at 9:10 AM karl adenener <adenener at hotmail.com<mailto:adenener at hotmail.com>> wrote:
>
> It would be a dream, there would be a R-based software, which I configure according to my study (type of data, limits for meaningful measurements, handling of outliers and missing measurements, test method etc.), which then reads my original measurement data and after some computing time the software provides me with the statistical analysis. All steps of the evaluation have to be defined before the start of the study and cannot be changed after the start of the study.
>
> Where could problems arise?
> Does anyone know of a suitable R-Package or software?
> Does anyone have the time and inclination to create a flexibly customizable package?
>
> greetings
> Adenener
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761516112&amp;sdata=0uX2IMimnT3HFKDY%2Fubv0JjEMYcSEbOtgzqEuzDPFow%3D&amp;reserved=0
> PLEASE do read the posting guide https://nam03.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761516112&amp;sdata=UwzCHC4JGmC0YbjliQ8XhUdMSGju7EeyWfYVRFFVcjE%3D&amp;reserved=0
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Gregory (Greg) L. Snow Ph.D.
> 538280 at gmail.com<mailto:538280 at gmail.com>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761516112&amp;sdata=0uX2IMimnT3HFKDY%2Fubv0JjEMYcSEbOtgzqEuzDPFow%3D&amp;reserved=0
> PLEASE do read the posting guide https://nam03.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761526108&amp;sdata=PXbXmhRfsosD1leVo19l7o95QrQ2OBdGlSAiUNWIgNs%3D&amp;reserved=0
> and provide commented, minimal, self-contained, reproducible code.
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761526108&amp;sdata=JE5ixJ4K6LIvA5DyhterWla8jryKhDLCnblbQFNd93o%3D&amp;reserved=0
> PLEASE do read the posting guide https://nam03.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761526108&amp;sdata=PXbXmhRfsosD1leVo19l7o95QrQ2OBdGlSAiUNWIgNs%3D&amp;reserved=0
> and provide commented, minimal, self-contained, reproducible code.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r@oknz @end|ng |rom gm@||@com  Mon May 11 23:10:33 2020
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Tue, 12 May 2020 09:10:33 +1200
Subject: [R] the volcano orientation
In-Reply-To: <20200511095809.51439268@Draco>
References: <CAAcGz9-d1Hq7zBO0vaaTWnrETAMX64=zESvM=QKon4au2tgvAw@mail.gmail.com>
 <CABcYAdK4U7K_FwM=w04k5dK8rbbT5uKo-WyHBMmcBUQuaNwBYA@mail.gmail.com>
 <20200511095809.51439268@Draco>
Message-ID: <CABcYAdJrQgp21Rd2WkG3gfymRp0abXAZcOMhVCyafOvr0jnqpg@mail.gmail.com>

Like other countries, New Zealand revises its maps and its coordinate
system from time to time.  The one in use at the time that image was
digitised is probably the one described here:
https://www.linz.govt.nz/data/geodetic-system/datums-projections-heights/projections/new-zealand-map-grid-nzmg

On Tue, 12 May 2020 at 04:58, John via R-help <r-help at r-project.org> wrote:
>
> Out of curiosity, and considering the bewildering array of projections
> and grids in use for various mapping purposes, you seem to be saying in your
> example 2 that the grid coordinates number south to north and east to
> west.  Given scale of the coordinate numbers, would that be a national
> grid system employed in New Zealnd?
>
> J. W. Dougherty
>
> On Mon, 11 May 2020 13:56:49 +1200
> "Richard O'Keefe" <raoknz at gmail.com> wrote:
>
> > Hey, I know that volcano!  It's walking distance from the Intermediate
> > school I attended.
> > To you it's a plot; to me it's a place.
> > So I offer you four scenarios.
> >
> > 1. You think of it as a place you know and have been.
> >     In that case the "right" orientation is the one that best matches
> > what you are used to seeing.
> >     For me, that would put the peak on the right of the plot.
> >
> > 2. You think of it as a patch in a map.
> >     In that case the "right:" orientation is the one that matches the
> > map. That would put the peak at the bottom of the plot.
> >
> > 3. You think of it as a product of geological processes, and are
> > perhaps interested in
> >     whether there is any connection between the orientation of the
> > volcano and the
> >     direction the Auckland hot-spot (currently at White Island) was
> > moving. In that case you'd choose south-west -> north-east as the
> > primary axis. (I think.  Not really sure.)
> >
> > 4. You think of it as a picture, an illustration in a textbook.  It
> > might need to be cropped
> >     vertically so you can fit another illustration on the same page.
> > For that and
> >     perceptual reasons you want the major linear axis of the image to
> > be  horizontal.
> >     In that case, what we have now is a perfectly reasonable choice.
> >
> > "Quality is fitness for use."
> >
> > On Sun, 10 May 2020 at 12:44, Michael Sumner <mdsumner at gmail.com>
> > wrote:
> > >
> > > Does anyone know why 'volcano' is oriented as it is?
> > >
> > > image(volcano)  ## filled.contour is the same
> > >
> > > I know it's all arbitrary, but north-up is a pretty solid
> > > convention. Is there any reason why the classic 'image()' example
> > > data set would not default to this orientation?
> > >
> > > A Google map of the site (in Web Mercator):
> > >
> > > https://www.google.com/maps/place/Maungawhau+%2F+Mount+Eden/@-36.8763271,174.7619561,856m/data=!3m1!1e3!4m8!1m2!2m1!1smaungawhau!3m4!1s0x6d0d47db8d7bd1ff:0x8bcffe2a5c7360d2!8m2!3d-36.8666667!4d174.7666667
> > >
> > >
> > > For image(), the north-up orientation is
> > > 't(volcano[,ncol(volcano):1])'.
> > >
> > > If you are interested in a roughly georeferenced version I have
> > > code here:
> > >
> > > https://gist.github.com/mdsumner/20fe3ffa04421bf8e0517c19085e5fd8
> > >
> > > (Also see fortunes::fortune("conventions") )
> > >
> > > Best, Mike
> > >
> > >
> > > --
> > > Michael Sumner
> > > Software and Database Engineer
> > > Australian Antarctic Division
> > > Hobart, Australia
> > > e-mail: mdsumner at gmail.com
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html and provide commented,
> > > minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @purd|e@@ @end|ng |rom gm@||@com  Tue May 12 04:34:47 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Tue, 12 May 2020 14:34:47 +1200
Subject: [R] genericSummary in LSAfun
In-Reply-To: <CAGN=ytM6=ntjuRX_q7pbaYFzb5HbiV8VwR5i6JoZB_FRiX9_7w@mail.gmail.com>
References: <CAGN=ytM4+Cn_egGauCt+hMZ4jX0OVQNacaLtgvHzmpfPy5WFbw@mail.gmail.com>
 <CAGN=ytM6=ntjuRX_q7pbaYFzb5HbiV8VwR5i6JoZB_FRiX9_7w@mail.gmail.com>
Message-ID: <CAB8pepzTZ3HuE5tj0XK1JXRkpLDz_MKrn9byP3u2co2eViWLZw@mail.gmail.com>

Does increasing the value of k (the second argument) help?

Also, if I understand the documentation correctly, the first argument
should a single string, not a data.frame.
I note that the paste function (with collapse="") can be used to turn
a character vector into a single string.


On Sat, May 9, 2020 at 10:07 PM Mehdi Dadkhah <mehdidadkhah91 at gmail.com> wrote:
>
> Hi,
> I hope you are doing well!
> I have a data frame with a column. it contains about 140 posts. In each
> row, there is a blog post. I named this data frame as "posttext". When i
> use  genericSummary() function, it returns paragraph instead sentences.
> What is problem?
>
>
> Command which i use:
>
> summaryp<-genericSummary(posttext,k=1,split=c(".","!","?"),min=5,breakdown=FALSE)
>
>
>
> Many thanks!
> With best regards,
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @purd|e@@ @end|ng |rom gm@||@com  Tue May 12 04:53:27 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Tue, 12 May 2020 14:53:27 +1200
Subject: [R] stats:: spline's method could not be monoH.FC
In-Reply-To: <24240.9662.85376.166974@stat.math.ethz.ch>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <24240.9662.85376.166974@stat.math.ethz.ch>
Message-ID: <CAB8pepyYOa18nEb6No-HTNQCp+42Bv6KYPp4W3k=+eNwWiZYkw@mail.gmail.com>

Hi Martin,
(In regards to your last ***two*** posts).

(Excerpt, post one)
> Well, as I know spent enough time reading and thinking, I'd
> really like to add   method = "clamped" to splinefun() and also
> the other one where fix the 2nd derivatives (to arbitrary values
> instead of zero).

(Excerpt, post two)
> So as a matter of fact I would ask for patches there (both in C
> and in R calling C ... maybe too much for 30 minutes ;-)

I'm happy to have a look a the spline code.
However, there's likely to be a substantial delay.

First, I need to:
(1) Change operating systems. Hopefully soon...
(2) Learn to build and debug R packages with non-trivial C code.
(3) Update my kubik package.
Will be looking at second derivatives, but more importantly rewriting
the root finding algorithm.
(4) Familiarize myself with spline theory (other than bezier/hermite).

My guess is I will get there in about eight to ten months, from now.
If you need it done sooner, and want someone else to do it, that's fine...

Also, it may be a good idea for you to work out what you would like
the top-level function signatures to look like...


From |@rchuby @end|ng |rom gm@||@com  Mon May 11 23:09:40 2020
From: |@rchuby @end|ng |rom gm@||@com (Fernando Archuby)
Date: Mon, 11 May 2020 18:09:40 -0300
Subject: [R] Convex hulls after nMDS or PCoA
Message-ID: <CAE09ejdPAk0BgadOdiU01Tp9LXoW84e-H1Go8U2tAKS2OdhDKg@mail.gmail.com>

Dear all. I need to build convex hulls around groups of points of an
ordination (nMDS or PCoA). The groups were independently established with
cluster analysis. I have learnt to plot convex hulls but would need to
create every of the 21 groups independently. Do you know how could I just
add the hulls to the ordination plot in an easier way?
Thank you very much,
Fernando.

-- 

*Dr. Fernando M. Archuby*
CONICET-UNLP
Tel?fono Personal: +54-221-15-6129667.
farchuby at gmail.com
paleobiologia at gmail.com

	[[alternative HTML version deleted]]


From rhur||n @end|ng |rom gwdg@de  Tue May 12 09:50:44 2020
From: rhur||n @end|ng |rom gwdg@de (Rainer Hurling)
Date: Tue, 12 May 2020 09:50:44 +0200
Subject: [R] Date format
In-Reply-To: <CAH6117JToaaoFQssgiuMbiR7d_SEBWK2HWuRqxWFKugRP-Q5KQ@mail.gmail.com>
References: <CAH6117JToaaoFQssgiuMbiR7d_SEBWK2HWuRqxWFKugRP-Q5KQ@mail.gmail.com>
Message-ID: <86880d13-252f-a936-4151-3047c3d4149f@gwdg.de>

Hi Medic,

Am 10.05.20 um 09:15 schrieb Medic:
> I took a SAMPLE CODE (for Connected scatterplot) from the R gallery
> and applied to MY DATA, but got:
> "Don't know how to automatically pick scale for object ..."
> P.S. 1) R ver. 4.0 (Yes, Jeff);  2) Attached: mydata_dput (1 ??)
> 
> SAMPLE CODE
> library(ggplot2)
> library(dplyr)
> library(hrbrthemes)
> data <- read.table("https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/3_TwoNumOrdered.csv
> ", header=T)
> 
> data$date <- as.Date(data$date)
> 
> # Plot
> data %>%
>    tail(10) %>%
>    ggplot( aes(x=date, y=value)) +
>      geom_line( color="grey") +
>      geom_point(shape=21, color="black", fill="#69b3a2", size=6) +
>      theme_ipsum() +
>      ggtitle("Evolution of bitcoin price")
> 
> ======
> MY DATA
> mydata <- read.table("E:/mydata.csv", header=TRUE, sep=";", dec=",")
> 
> str(mydata)
> 'data.frame': 7 obs. of  2 variables:
>   $ date : chr  "01.01.2000" "02.01.2000" ...
>   $ value: int  11 12 ...
> 
> mydata$date <- as.Date(mydata$date, "%d.%m.%Y")
> 
> str(mydata$date)
> Date[1:7], format: "2000-01-01"
> 
> # Bert, thanks for the explanation!
> # Rainer, thanks for the specific code!
> 
> # And then the problem:
> mydata %>%
>      tail(10) %>%
>      ggplot( aes(x=mydata, y=value)) +
>      geom_line( color="grey") +
>      geom_point(shape=21, color="black", fill="#69b3a2", size=6) +
>      theme_ipsum() +
>      ggtitle("Evolution")
> 
> "Don't know how to automatically pick scale for object of type
> data.frame. Defaulting to continuous.
> Error: Aesthetics must be either length 1 or the same as the data (7): x"

Perhaps only a little typo? Pls try

ggplot( aes(x=date, y=value))
               ^^^^


From @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t  Tue May 12 10:11:41 2020
From: @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t (Stefano Sofia)
Date: Tue, 12 May 2020 08:11:41 +0000
Subject: [R] Classification of wind events
Message-ID: <8B435C9568170B469AE31E8891E8CC4F809B8DEA@ESINO.regionemarche.intra>

Dear R list users,
I am aware that this question is not strictly related, at the present moment, to R code and it is more general. Please forgive me, but I need to share my thoughts with you.

Foehn conditions on the southern slope of Alps happen with strong northerly flows that impact perpendicularly over the Apls. This situation triggers strong northerly leeward winds.
Given a single automatic weather station, I would like to identify these periods starting from wind direction and wind intensity data. Frequency of data is quarter of hour.
I would really find difficult to detect the moving windows of these events:
- I can't analyse data day by day;
- at the beginning and at the end of each event, when the process is not at full speed yet, the rotation is not always perfectly identifiable;
- I cannot claim in principle that the direction of each consecutive observation is costantly and strictly from the chosen direction.

Does anybody have a clue on how to start to build this process in the right way?

Thank you for your attention and your help
Stefano

         (oo)
--oOO--( )--OOo----------------
Stefano Sofia PhD
Civil Protection - Marche Region
Meteo Section
Snow Section
Via del Colle Ameno 5
60126 Torrette di Ancona, Ancona
Uff: 071 806 7743
E-mail: stefano.sofia at regione.marche.it
---Oo---------oO----------------

________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.

--
Questo messaggio  stato analizzato da Libra ESVA ed  risultato non infetto.
This message was scanned by Libra ESVA and is believed to be clean.


	[[alternative HTML version deleted]]


From @purd|e@@ @end|ng |rom gm@||@com  Tue May 12 10:38:31 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Tue, 12 May 2020 20:38:31 +1200
Subject: [R] My dream ...
In-Reply-To: <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
Message-ID: <CAB8pepx-x1yoYWjmJdH727z66smn-eh43P_E4o8R5gqj2aH9Ug@mail.gmail.com>

> In my opinion the advantage of computers is not Artificial
> Intelligence, but rather Artificial Patience (most AI that I have seen
> is really doing a bunch of what I would consider to be boring, really
> fast so people don't have to).  Leave the Intelligence to the people.

Hmmm...
https://en.wikipedia.org/wiki/Artificial_intelligence_in_video_games

Also, I found the following while searching for battle chess:
https://youtu.be/hBNG7444lOw

(Warning: Contains aggressive chess tactics).

Also, correct me if I'm wrong, but doesn't Emacs have historical
connections to AI research...?


From tuech|er @end|ng |rom gmx@@t  Tue May 12 11:10:17 2020
From: tuech|er @end|ng |rom gmx@@t (Heinz Tuechler)
Date: Tue, 12 May 2020 11:10:17 +0200
Subject: [R] My dream ...
In-Reply-To: <CAB8pepx-x1yoYWjmJdH727z66smn-eh43P_E4o8R5gqj2aH9Ug@mail.gmail.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
 <CAB8pepx-x1yoYWjmJdH727z66smn-eh43P_E4o8R5gqj2aH9Ug@mail.gmail.com>
Message-ID: <b46d3d76-e772-3054-bb73-16a84c64a96a@gmx.at>

Abby Spurdle wrote/hat geschrieben on/am 12.05.2020 10:38:
>> In my opinion the advantage of computers is not Artificial
>> Intelligence, but rather Artificial Patience (most AI that I have seen
>> is really doing a bunch of what I would consider to be boring, really
>> fast so people don't have to).  Leave the Intelligence to the people.
>
> Hmmm...
> https://en.wikipedia.org/wiki/Artificial_intelligence_in_video_games
>
> Also, I found the following while searching for battle chess:
> https://youtu.be/hBNG7444lOw
>
> (Warning: Contains aggressive chess tactics).
>
> Also, correct me if I'm wrong, but doesn't Emacs have historical
> connections to AI research...?
>
Maybe a matter of definition, but admittedly I have to use a lot of my
intelligence for doing boring work.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Po||ngW @end|ng |rom @etn@@com  Mon May 11 17:58:38 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Mon, 11 May 2020 15:58:38 +0000
Subject: [R] Help with Kmeans output and using broom to tidy etc..
Message-ID: <BYAPR06MB5383EA994C10F43076877EE4AEA10@BYAPR06MB5383.namprd06.prod.outlook.com>

#RStudio Version Version 1.2.1335 need this one--> 1.2.5019
sessionInfo() 
# R version 4.0.0 Patched (2020-05-03 r78349)
#Platform: x86_64-w64-mingw32/x64 (64-bit)
#Running under: Windows 10 x64 (build 17763)

Hello:

I have data that I am trying to manipulate for Kmeans clustering.

Original data looks like this

str(geo1) 
# 'data.frame':	2352 obs. of  5 variables:
# $ ID: Factor w/ 2352 levels "101040199600",..: 590 908 976 509 1674 690 1336 86 726 1702 ...
# $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 32 10 25 11 9 32 13 31 12 12 ...
# $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 932 156 230 698 965 1330 515 727 1127 1304 ...
# $ latitude        : num  40.4 31.2 40.8 42.1 26.8 ...
# $ longitude       : num  -79.9 -81.5 -74 -91.6 -82.1 ...

I created a subset adding column prop_of_total 
str(trnd1_tbl)
tibble [1,457 x 5] (S3: tbl_df/tbl/data.frame)
 $ city         : Factor w/ 1337 levels "ABBOTTSTOWN",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ state        : Factor w/ 41 levels "AL","AR","AZ",..: 32 36 10 28 12 36 10 11 26 38 ...
 $ Basecountsum : num [1:1457] 2352 2352 2352 2352 2352 ...
 $ Basecount2   : num [1:1457] 1 1 1 1 1 2 1 1 2 1 ...
 $ prop_of_total: num [1:1457] 0.000425 0.000425 0.000425 0.000425 0.000425 ...


Then I spread it

trnd2_tbl <- trnd1_tbl %>% 
    dplyr::select(city, state, prop_of_total) %>% 
    spread(key = city, value = prop_of_total, fill = 0) #remove the NA's with fill

str(trnd2_tbl)#tibble [41 x 1,338] (S3: tbl_df/tbl/data.frame)

Then I run a Kmeans

kmeans_obj1 <- trnd2_tbl  %>% 
  dplyr::select(- state) %>% 
  kmeans(centers = 20, nstart = 100)

str(kmeans_obj1)
List of 9
 $ cluster     : int [1:41] 11 11 9 11 11 4 11 11 16 2 ...
 $ centers     : num [1:20, 1:1337] 0 0 0 0 0 0 0 0 0 0 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:20] "1" "2" "3" "4" ...
  .. ..$ : chr [1:1337] "ABBOTTSTOWN" "ABILENE" "ACWORTH" "ADAMS" ...
 $ totss       : num 0.00158
 $ withinss    : num [1:20] 0 0 0 0 0 0 0 0 0 0 ...
 $ tot.withinss: num 0.0000848
 $ betweenss   : num 0.0015
 $ size        : int [1:20] 1 1 1 1 1 1 1 1 1 1 ...
 $ iter        : int 3
 $ ifault      : int 0
 - attr(*, "class")= chr "kmeans"

Then I go and try to tidy:

#Tidy, glance, augment
#Just makes it easier to use or view the obj's in the obj list
  
  broom::tidy(kmeans_obj1) %>% glimpse()

	broom::glance(kmeans_obj1)
##A tibble: 1 x 4
# totss tot.withinss betweenss  iter
# <dbl>        <dbl>     <dbl> <int>
#   1 0.00158    0.0000848   0.00150     3

However, when I run this piece I get an error:

broom::augment(kmeans_obj1, trnd2_tbl) %>% 
  dplyr::select(city, .cluster)             

#Error: Must subset columns with a valid subscript vector.
# The subscript has the wrong type `data.frame<
 # u: double
#  x: double
>`.
i It must be numeric or character.

Here is the back trace:

rlang::last_error()

# Backtrace:
#   1. broom::augment(kmeans_obj1, trnd2_tbl)
# 9. dplyr::select(., city, .cluster)
# 11. tidyselect::vars_select(tbl_vars(.data), !!!enquos(...))
# 12. tidyselect:::eval_select_impl(...)
# 20. tidyselect:::vars_select_eval(...)
# 21. tidyselect:::walk_data_tree(expr, data_mask, context_mask)
# 22. tidyselect:::eval_c(expr, data_mask, context_mask)
# 23. tidyselect:::reduce_sels(node, data_mask, context_mask, init = init)
# 24. tidyselect:::walk_data_tree(new, data_mask, context_mask)
# 25. tidyselect:::as_indices_sel_impl(...)
# 26. tidyselect:::as_indices_impl(x, vars, strict = strict)
# 27. vctrs::vec_as_subscript(x, logical = "error")

I am not sure what I am supposed to fix?

Maybe someone has had similar error and can advise me please?

Thank you.

WHP







Proprietary

NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}


From Po||ngW @end|ng |rom @etn@@com  Tue May 12 12:15:14 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Tue, 12 May 2020 10:15:14 +0000
Subject: [R] Help with R-Markdown please
Message-ID: <BYAPR06MB53837B82850FAC0378779843AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>

#UPDATED 05/05/2020
#RStudio Version Version 1.2.1335 need this one--> 1.2.5019
sessionInfo() 
# R version 4.0.0 Patched (2020-05-03 r78349)
#Platform: x86_64-w64-mingw32/x64 (64-bit)
#Running under: Windows 10 x64 (build 17763)

Good morning.

This is the first time I have tried RMarkdown on new laptop with new employer.
I am not sure how to go about fixing this problem below?

Would someone please advise course of action.

Thank you.

WHP 



processing file: Member-Geo-Location-Test-V1.Rmd
  |..........                                                            |  14%
  ordinary text without R code

  |....................                                                  |  29%
label: unnamed-chunk-1 (with options) 
List of 1
 $ echo: logi FALSE

  |..............................                                        |  43%
  ordinary text without R code

  |........................................                              |  57%
label: setup (with options) 
List of 2
 $ include: logi FALSE
 $ warning: logi FALSE

  |..................................................                    |  71%
  ordinary text without R code

  |............................................................          |  86%
label: Table1 (with options) 
List of 2
 $ echo   : logi FALSE
 $ results: chr "asis"

  |......................................................................| 100%
   inline R code fragments


output file: Member-Geo-Location-Test-V1.knit.md

"C:/Program Files/RStudio/bin/pandoc/pandoc" +RTS -K512m -RTS Member-Geo-Location-Test-V1.utf8.md --to html4 --from markdown+autolink_bare_uris+tex_math_single_backslash+smart --output Member-Geo-Location-Test-V1.html --email-obfuscation none --self-contained --standalone --section-divs --template "\\winp-oaf-113\FldrRedir_1$\A436798\Data\R\R-4.0.0patched\library\rmarkdown\rmd\h\default.html" --no-highlight --variable highlightjs=1 --variable "theme:bootstrap" --include-in-header "C:\Users\A436798\AppData\Local\Temp\RtmpSajZla\rmarkdown-str2ae846253313.html" --mathjax --variable "mathjax-url:https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" --lua-filter "\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library/rmarkdown/rmd/lua/pagebreak.lua" --lua-filter "\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library/rmarkdown/rmd/lua/latex-div.lua" 
Could not fetch http://?/UNC/winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library/rmarkdown/rmd/h/default.html
HttpExceptionRequest Request {
  host                 = ""
  port                 = 80
  secure               = False
  requestHeaders       = []
  path                 = "/"
  queryString          = "?/UNC/winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library/rmarkdown/rmd/h/default.html"
  method               = "GET"
  proxy                = Nothing
  rawBody              = False
  redirectCount        = 10
  responseTimeout      = ResponseTimeoutDefault
  requestVersion       = HTTP/1.1
}
 (InvalidDestinationHost "")
Error: pandoc document conversion failed with error 61
Execution halted

WHP


Proprietary

NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}


From rub@k @end|ng |rom m@th@@@u@dk  Tue May 12 14:42:48 2020
From: rub@k @end|ng |rom m@th@@@u@dk (Ege Rubak)
Date: Tue, 12 May 2020 12:42:48 +0000
Subject: [R] Help with R-Markdown please
In-Reply-To: <BYAPR06MB53837B82850FAC0378779843AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB53837B82850FAC0378779843AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <e1212a34f6b1dbabeddb85c4ad3cf015eb4c8c16.camel@math.aau.dk>

Looks like your files are on a Windows network drive (UNC path). I have
experienced many problems with this on a Windows laptop from work. If
at all possible you should avoid this. As an absolute minimum make sure
your R library (collection of installed packages) is on the local file
system and not a UNC path. If you are lucky this could be enough to
make things work.

You can check your library location(s) with the command .libPaths() in
an R session.

Good luck.

Ege

On Tue, 2020-05-12 at 10:15 +0000, Poling, William via R-help wrote:
> #UPDATED 05/05/2020
> #RStudio Version Version 1.2.1335 need this one--> 1.2.5019
> sessionInfo() 
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit)
> #Running under: Windows 10 x64 (build 17763)
> 
> Good morning.
> 
> This is the first time I have tried RMarkdown on new laptop with new
> employer.
> I am not sure how to go about fixing this problem below?
> 
> Would someone please advise course of action.
> 
> Thank you.
> 
> WHP 
> 
> 
> 
> processing file: Member-Geo-Location-Test-V1.Rmd
>  
> |..........                                                          
>   |  14%
>   ordinary text without R code
> 
>  
> |....................                                                
>   |  29%
> label: unnamed-chunk-1 (with options) 
> List of 1
>  $ echo: logi FALSE
> 
>  
> |..............................                                      
>   |  43%
>   ordinary text without R code
> 
>  
> |........................................                            
>   |  57%
> label: setup (with options) 
> List of 2
>  $ include: logi FALSE
>  $ warning: logi FALSE
> 
>  
> |..................................................                  
>   |  71%
>   ordinary text without R code
> 
>  
> |............................................................        
>   |  86%
> label: Table1 (with options) 
> List of 2
>  $ echo   : logi FALSE
>  $ results: chr "asis"
> 
>  
> |....................................................................
> ..| 100%
>    inline R code fragments
> 
> 
> output file: Member-Geo-Location-Test-V1.knit.md
> 
> "C:/Program Files/RStudio/bin/pandoc/pandoc" +RTS -K512m -RTS Member-
> Geo-Location-Test-V1.utf8.md --to html4 --from
> markdown+autolink_bare_uris+tex_math_single_backslash+smart --output
> Member-Geo-Location-Test-V1.html --email-obfuscation none --self-
> contained --standalone --section-divs --template "\\winp-oaf-
> 113\FldrRedir_1$\A436798\Data\R\R-
> 4.0.0patched\library\rmarkdown\rmd\h\default.html" --no-highlight --
> variable highlightjs=1 --variable "theme:bootstrap" --include-in-
> header "C:\Users\A436798\AppData\Local\Temp\RtmpSajZla\rmarkdown-
> str2ae846253313.html" --mathjax --variable "mathjax-url:
> https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML
> " --lua-filter "\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/lua/pagebreak.lua" --lua-filter
> "\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/lua/latex-div.lua" 
> Could not fetch http://?/UNC/winp-oaf-
> 113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/h/default.html
> HttpExceptionRequest Request {
>   host                 = ""
>   port                 = 80
>   secure               = False
>   requestHeaders       = []
>   path                 = "/"
>   queryString          = "?/UNC/winp-oaf-
> 113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/h/default.html"
>   method               = "GET"
>   proxy                = Nothing
>   rawBody              = False
>   redirectCount        = 10
>   responseTimeout      = ResponseTimeoutDefault
>   requestVersion       = HTTP/1.1
> }
>  (InvalidDestinationHost "")
> Error: pandoc document conversion failed with error 61
> Execution halted
> 
> WHP
> 
> 
> Proprietary
> 
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may
> con...{{dropped:16}}
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
Ege Rubak, Associate Professor,
Department of Mathematical Sciences, Aalborg University
Skjernvej 4A, 9220 Aalborg East, Denmark
Phone: (+45)99408861
Mobile: (+45)30230252
Email: rubak at math.aau.dk

From Po||ngW @end|ng |rom @etn@@com  Tue May 12 14:47:11 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Tue, 12 May 2020 12:47:11 +0000
Subject: [R] Help with R-Markdown please
In-Reply-To: <e1212a34f6b1dbabeddb85c4ad3cf015eb4c8c16.camel@math.aau.dk>
References: <BYAPR06MB53837B82850FAC0378779843AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <e1212a34f6b1dbabeddb85c4ad3cf015eb4c8c16.camel@math.aau.dk>
Message-ID: <BYAPR06MB5383BD2BE3FDFE4728FDFCB1AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>

Wow, ok, yep "\\\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library"

Thank you Ege. I did not have this issue with previous employer, I will see what our IT group says.

Thank you 

WHP


William H. Poling Ph.D., MPH? |?Senior Data Scientist, Medicare Stars, CVS Health 
p?813-777-5030


Proprietary

-----Original Message-----
From: Ege Rubak <rubak at math.aau.dk> 
Sent: Tuesday, May 12, 2020 7:43 AM
To: Poling, William <PolingW at aetna.com>; r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with R-Markdown please

**** External Email - Use Caution ****

Looks like your files are on a Windows network drive (UNC path). I have experienced many problems with this on a Windows laptop from work. If at all possible you should avoid this. As an absolute minimum make sure your R library (collection of installed packages) is on the local file system and not a UNC path. If you are lucky this could be enough to make things work.

You can check your library location(s) with the command .libPaths() in an R session.

Good luck.

Ege

On Tue, 2020-05-12 at 10:15 +0000, Poling, William via R-help wrote:
> #UPDATED 05/05/2020
> #RStudio Version Version 1.2.1335 need this one--> 1.2.5019
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit) #Running under: Windows 10 
> x64 (build 17763)
> 
> Good morning.
> 
> This is the first time I have tried RMarkdown on new laptop with new 
> employer.
> I am not sure how to go about fixing this problem below?
> 
> Would someone please advise course of action.
> 
> Thank you.
> 
> WHP
> 
> 
> 
> processing file: Member-Geo-Location-Test-V1.Rmd
>  
> |..........                                                          
>   |  14%
>   ordinary text without R code
> 
>  
> |....................                                                
>   |  29%
> label: unnamed-chunk-1 (with options) List of 1  $ echo: logi FALSE
> 
>  
> |..............................                                      
>   |  43%
>   ordinary text without R code
> 
>  
> |........................................                            
>   |  57%
> label: setup (with options)
> List of 2
>  $ include: logi FALSE
>  $ warning: logi FALSE
> 
>  
> |..................................................                  
>   |  71%
>   ordinary text without R code
> 
>  
> |............................................................        
>   |  86%
> label: Table1 (with options)
> List of 2
>  $ echo   : logi FALSE
>  $ results: chr "asis"
> 
>  
> |....................................................................
> ..| 100%
>    inline R code fragments
> 
> 
> output file: Member-Geo-Location-Test-V1.knit.md
> 
> "C:/Program Files/RStudio/bin/pandoc/pandoc" +RTS -K512m -RTS Member- 
> Geo-Location-Test-V1.utf8.md --to html4 --from
> markdown+autolink_bare_uris+tex_math_single_backslash+smart --output
> Member-Geo-Location-Test-V1.html --email-obfuscation none --self- 
> contained --standalone --section-divs --template "\\winp-oaf-
> 113\FldrRedir_1$\A436798\Data\R\R-
> 4.0.0patched\library\rmarkdown\rmd\h\default.html" --no-highlight -- 
> variable highlightjs=1 --variable "theme:bootstrap" --include-in- 
> header "C:\Users\A436798\AppData\Local\Temp\RtmpSajZla\rmarkdown-
> str2ae846253313.html" --mathjax --variable "mathjax-url:
> https://urldefense.proofpoint.com/v2/url?u=https-3A__mathjax.rstudio.c
> om_latest_MathJax.js-3Fconfig-3DTeX-2DAMS-2DMML-5FHTMLorMML&d=DwIGaQ&c
> =wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTC
> tKvneM2ExlYvnUWbsByY&m=uBce0mQRO8wdLXO8BDTtjk4R92KVzpjtkQ-NqYiAHCo&s=S
> MNU_B--j9FvBKh-0u1NG5RHd7AtLkvyVsi9ybyUwYU&e=
> " --lua-filter "\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/lua/pagebreak.lua" --lua-filter
> "\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/lua/latex-div.lua" 
> Could not fetch http://?/UNC/winp-oaf-
> 113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/h/default.html
> HttpExceptionRequest Request {
>   host                 = ""
>   port                 = 80
>   secure               = False
>   requestHeaders       = []
>   path                 = "/"
>   queryString          = "?/UNC/winp-oaf-
> 113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/h/default.html"
>   method               = "GET"
>   proxy                = Nothing
>   rawBody              = False
>   redirectCount        = 10
>   responseTimeout      = ResponseTimeoutDefault
>   requestVersion       = HTTP/1.1
> }
>  (InvalidDestinationHost "")
> Error: pandoc document conversion failed with error 61 Execution 
> halted
> 
> WHP
> 
> 
> Proprietary
> 
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may 
> con...{{dropped:16}}
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIGaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmv
> OXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=uBce0mQRO8wdLXO
> 8BDTtjk4R92KVzpjtkQ-NqYiAHCo&s=W5iLmdkyVAbq4AQuAsTYsadWBk1jduptKOZLR25
> jPWo&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIGaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmm
> vOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=uBce0mQRO8wdLX
> O8BDTtjk4R92KVzpjtkQ-NqYiAHCo&s=5V2OtH149c6MxDhlirX-Yd-v8uuEuVYih9DRq-
> ZmBF8&e= and provide commented, minimal, self-contained, reproducible 
> code.
--
Ege Rubak, Associate Professor,
Department of Mathematical Sciences, Aalborg University Skjernvej 4A, 9220 Aalborg East, Denmark
Phone: (+45)99408861
Mobile: (+45)30230252
Email: rubak at math.aau.dk

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From er|cjberger @end|ng |rom gm@||@com  Tue May 12 15:39:24 2020
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Tue, 12 May 2020 16:39:24 +0300
Subject: [R] Help with Kmeans output and using broom to tidy etc..
In-Reply-To: <BYAPR06MB5383EA994C10F43076877EE4AEA10@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB5383EA994C10F43076877EE4AEA10@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <CAGgJW74OPs+mB8OKLc0BCpaCiNMO08=-Ex+8mW5qtk38+Lnh8A@mail.gmail.com>

Can you create a reproducible example?
Your question involves objects that are unknown to us. (geo1, trnd1_tbl)

On Tue, May 12, 2020 at 2:41 PM Poling, William via R-help <
r-help at r-project.org> wrote:

> #RStudio Version Version 1.2.1335 need this one--> 1.2.5019
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit)
> #Running under: Windows 10 x64 (build 17763)
>
> Hello:
>
> I have data that I am trying to manipulate for Kmeans clustering.
>
> Original data looks like this
>
> str(geo1)
> # 'data.frame': 2352 obs. of  5 variables:
> # $ ID: Factor w/ 2352 levels "101040199600",..: 590 908 976 509 1674 690
> 1336 86 726 1702 ...
> # $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 32 10 25 11 9
> 32 13 31 12 12 ...
> # $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 932 156 230
> 698 965 1330 515 727 1127 1304 ...
> # $ latitude        : num  40.4 31.2 40.8 42.1 26.8 ...
> # $ longitude       : num  -79.9 -81.5 -74 -91.6 -82.1 ...
>
> I created a subset adding column prop_of_total
> str(trnd1_tbl)
> tibble [1,457 x 5] (S3: tbl_df/tbl/data.frame)
>  $ city         : Factor w/ 1337 levels "ABBOTTSTOWN",..: 1 2 3 4 5 6 7 8
> 9 10 ...
>  $ state        : Factor w/ 41 levels "AL","AR","AZ",..: 32 36 10 28 12 36
> 10 11 26 38 ...
>  $ Basecountsum : num [1:1457] 2352 2352 2352 2352 2352 ...
>  $ Basecount2   : num [1:1457] 1 1 1 1 1 2 1 1 2 1 ...
>  $ prop_of_total: num [1:1457] 0.000425 0.000425 0.000425 0.000425
> 0.000425 ...
>
>
> Then I spread it
>
> trnd2_tbl <- trnd1_tbl %>%
>     dplyr::select(city, state, prop_of_total) %>%
>     spread(key = city, value = prop_of_total, fill = 0) #remove the NA's
> with fill
>
> str(trnd2_tbl)#tibble [41 x 1,338] (S3: tbl_df/tbl/data.frame)
>
> Then I run a Kmeans
>
> kmeans_obj1 <- trnd2_tbl  %>%
>   dplyr::select(- state) %>%
>   kmeans(centers = 20, nstart = 100)
>
> str(kmeans_obj1)
> List of 9
>  $ cluster     : int [1:41] 11 11 9 11 11 4 11 11 16 2 ...
>  $ centers     : num [1:20, 1:1337] 0 0 0 0 0 0 0 0 0 0 ...
>   ..- attr(*, "dimnames")=List of 2
>   .. ..$ : chr [1:20] "1" "2" "3" "4" ...
>   .. ..$ : chr [1:1337] "ABBOTTSTOWN" "ABILENE" "ACWORTH" "ADAMS" ...
>  $ totss       : num 0.00158
>  $ withinss    : num [1:20] 0 0 0 0 0 0 0 0 0 0 ...
>  $ tot.withinss: num 0.0000848
>  $ betweenss   : num 0.0015
>  $ size        : int [1:20] 1 1 1 1 1 1 1 1 1 1 ...
>  $ iter        : int 3
>  $ ifault      : int 0
>  - attr(*, "class")= chr "kmeans"
>
> Then I go and try to tidy:
>
> #Tidy, glance, augment
> #Just makes it easier to use or view the obj's in the obj list
>
>   broom::tidy(kmeans_obj1) %>% glimpse()
>
>         broom::glance(kmeans_obj1)
> ##A tibble: 1 x 4
> # totss tot.withinss betweenss  iter
> # <dbl>        <dbl>     <dbl> <int>
> #   1 0.00158    0.0000848   0.00150     3
>
> However, when I run this piece I get an error:
>
> broom::augment(kmeans_obj1, trnd2_tbl) %>%
>   dplyr::select(city, .cluster)
>
> #Error: Must subset columns with a valid subscript vector.
> # The subscript has the wrong type `data.frame<
>  # u: double
> #  x: double
> >`.
> i It must be numeric or character.
>
> Here is the back trace:
>
> rlang::last_error()
>
> # Backtrace:
> #   1. broom::augment(kmeans_obj1, trnd2_tbl)
> # 9. dplyr::select(., city, .cluster)
> # 11. tidyselect::vars_select(tbl_vars(.data), !!!enquos(...))
> # 12. tidyselect:::eval_select_impl(...)
> # 20. tidyselect:::vars_select_eval(...)
> # 21. tidyselect:::walk_data_tree(expr, data_mask, context_mask)
> # 22. tidyselect:::eval_c(expr, data_mask, context_mask)
> # 23. tidyselect:::reduce_sels(node, data_mask, context_mask, init = init)
> # 24. tidyselect:::walk_data_tree(new, data_mask, context_mask)
> # 25. tidyselect:::as_indices_sel_impl(...)
> # 26. tidyselect:::as_indices_impl(x, vars, strict = strict)
> # 27. vctrs::vec_as_subscript(x, logical = "error")
>
> I am not sure what I am supposed to fix?
>
> Maybe someone has had similar error and can advise me please?
>
> Thank you.
>
> WHP
>
>
>
>
>
>
>
> Proprietary
>
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From 538280 @end|ng |rom gm@||@com  Tue May 12 16:01:05 2020
From: 538280 @end|ng |rom gm@||@com (Greg Snow)
Date: Tue, 12 May 2020 08:01:05 -0600
Subject: [R] My dream ...
In-Reply-To: <CAB8pepx-x1yoYWjmJdH727z66smn-eh43P_E4o8R5gqj2aH9Ug@mail.gmail.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
 <CAB8pepx-x1yoYWjmJdH727z66smn-eh43P_E4o8R5gqj2aH9Ug@mail.gmail.com>
Message-ID: <CAFEqCdwbXpD9DpgSEsHdsv0jBqqx0hLpu10ZN4SyejV06do4Jg@mail.gmail.com>

Here is one of my favorites:
https://medium.com/@ODSC/how-300-matchboxes-learned-to-play-tic-tac-toe-using-menace-35e0e4c29fc


On Tue, May 12, 2020 at 2:39 AM Abby Spurdle <spurdle.a at gmail.com> wrote:
>
> > In my opinion the advantage of computers is not Artificial
> > Intelligence, but rather Artificial Patience (most AI that I have seen
> > is really doing a bunch of what I would consider to be boring, really
> > fast so people don't have to).  Leave the Intelligence to the people.
>
> Hmmm...
> https://en.wikipedia.org/wiki/Artificial_intelligence_in_video_games
>
> Also, I found the following while searching for battle chess:
> https://youtu.be/hBNG7444lOw
>
> (Warning: Contains aggressive chess tactics).
>
> Also, correct me if I'm wrong, but doesn't Emacs have historical
> connections to AI research...?



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue May 12 16:16:50 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 12 May 2020 07:16:50 -0700
Subject: [R] Classification of wind events
In-Reply-To: <8B435C9568170B469AE31E8891E8CC4F809B8DEA@ESINO.regionemarche.intra>
References: <8B435C9568170B469AE31E8891E8CC4F809B8DEA@ESINO.regionemarche.intra>
Message-ID: <1E86B627-DA3B-4336-8589-1FC76D12B86E@dcn.davis.ca.us>

Please make a reproducible R example of input and output.

On May 12, 2020 1:11:41 AM PDT, Stefano Sofia <stefano.sofia at regione.marche.it> wrote:
>Dear R list users,
>I am aware that this question is not strictly related, at the present
>moment, to R code and it is more general. Please forgive me, but I need
>to share my thoughts with you.
>
>Foehn conditions on the southern slope of Alps happen with strong
>northerly flows that impact perpendicularly over the Apls. This
>situation triggers strong northerly leeward winds.
>Given a single automatic weather station, I would like to identify
>these periods starting from wind direction and wind intensity data.
>Frequency of data is quarter of hour.
>I would really find difficult to detect the moving windows of these
>events:
>- I can't analyse data day by day;
>- at the beginning and at the end of each event, when the process is
>not at full speed yet, the rotation is not always perfectly
>identifiable;
>- I cannot claim in principle that the direction of each consecutive
>observation is costantly and strictly from the chosen direction.
>
>Does anybody have a clue on how to start to build this process in the
>right way?
>
>Thank you for your attention and your help
>Stefano
>
>         (oo)
>--oOO--( )--OOo----------------
>Stefano Sofia PhD
>Civil Protection - Marche Region
>Meteo Section
>Snow Section
>Via del Colle Ameno 5
>60126 Torrette di Ancona, Ancona
>Uff: 071 806 7743
>E-mail: stefano.sofia at regione.marche.it
>---Oo---------oO----------------
>
>________________________________
>
>AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
>informazioni confidenziali, pertanto ? destinato solo a persone
>autorizzate alla ricezione. I messaggi di posta elettronica per i
>client di Regione Marche possono contenere informazioni confidenziali e
>con privilegi legali. Se non si ? il destinatario specificato, non
>leggere, copiare, inoltrare o archiviare questo messaggio. Se si ?
>ricevuto questo messaggio per errore, inoltrarlo al mittente ed
>eliminarlo completamente dal sistema del proprio computer. Ai sensi
>dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit?
>ed urgenza, la risposta al presente messaggio di posta elettronica pu?
>essere visionata da persone estranee al destinatario.
>IMPORTANT NOTICE: This e-mail message is intended to be received only
>by persons entitled to receive the confidential information it may
>contain. E-mail messages to clients of Regione Marche may contain
>information that is confidential and legally privileged. Please do not
>read, copy, forward, or store this message unless you are an intended
>recipient of it. If you have received this message in error, please
>forward it to the sender and delete it completely from your computer
>system.
>
>--
>Questo messaggio  stato analizzato da Libra ESVA ed  risultato non
>infetto.
>This message was scanned by Libra ESVA and is believed to be clean.
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @@r@h@go@|ee @end|ng |rom gm@||@com  Tue May 12 16:53:58 2020
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Tue, 12 May 2020 10:53:58 -0400
Subject: [R] Convex hulls after nMDS or PCoA
In-Reply-To: <CAE09ejdPAk0BgadOdiU01Tp9LXoW84e-H1Go8U2tAKS2OdhDKg@mail.gmail.com>
References: <CAE09ejdPAk0BgadOdiU01Tp9LXoW84e-H1Go8U2tAKS2OdhDKg@mail.gmail.com>
Message-ID: <CAM_vju=1FXhkGTYY98==Mda=qpMnqU9WN4=ocr8a2K+79QKDhw@mail.gmail.com>

Hi,

I use a modified version of dbscan::hullplot() but https://rseek.org
turns up a bunch of possibilities.

Sarah

On Tue, May 12, 2020 at 2:11 AM Fernando Archuby <farchuby at gmail.com> wrote:
>
> Dear all. I need to build convex hulls around groups of points of an
> ordination (nMDS or PCoA). The groups were independently established with
> cluster analysis. I have learnt to plot convex hulls but would need to
> create every of the 21 groups independently. Do you know how could I just
> add the hulls to the ordination plot in an easier way?
> Thank you very much,
> Fernando.
>
> --


-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From Po||ngW @end|ng |rom @etn@@com  Tue May 12 18:10:55 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Tue, 12 May 2020 16:10:55 +0000
Subject: [R] Help with Kmeans output and using broom to tidy etc..
In-Reply-To: <CAGgJW74OPs+mB8OKLc0BCpaCiNMO08=-Ex+8mW5qtk38+Lnh8A@mail.gmail.com>
References: <BYAPR06MB5383EA994C10F43076877EE4AEA10@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CAGgJW74OPs+mB8OKLc0BCpaCiNMO08=-Ex+8mW5qtk38+Lnh8A@mail.gmail.com>
Message-ID: <BYAPR06MB53832C0F432D88624E84AC25AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>

Hello Eric, thank you so much for your consideration.

Here are snippets of data that I hope will be helpful

WHP 

geo1a <- geo1[, c(2:5)] <-- eliminating ID which is not useful for my purposes anyway

#This is for R-Help use
geo1a <- geo1a %>% top_n(25)

state           city latitude longitude
1     ME      FAIRFIELD 44.64485 -69.65948
2     ME      JONESPORT 44.57935 -67.56743
3     ME        CASWELL 46.97529 -67.83023
4     ME      ELLSWORTH 44.52916 -68.38717
5     ME     VASSALBORO 44.45095 -69.60629
6     ME          UNION 44.20059 -69.26123
7     ME        PALERMO 44.45142 -69.41115
8     ME          ORONO 44.87426 -68.68327
9     ME    SANGERVILLE 45.10138 -69.33580
10    ME      ISLESBORO 44.29015 -68.90812
11    ME        TOPSHAM 43.93600 -69.96565
12    ME       FREEPORT 43.84089 -70.11160
13    ME      SKOWHEGAN 44.76687 -69.71644
14    ME    MILLINOCKET 45.65501 -68.70261
15    ME      ORRINGTON 44.72417 -68.74026
16    ME     ST. GEORGE 43.96726 -69.20827
17    ME FORT FAIRFIELD 46.80911 -67.88079
18    ME      MARS HILL 46.56580 -67.89006
19    ME       FREEPORT 43.85302 -70.03726
20    ME         EASTON 46.64143 -67.91203
21    ME     WATERVILLE 44.53621 -69.65913
22    ME      BRUNSWICK 43.87771 -69.96297
23    ME      BRUNSWICK 43.91719 -69.89905
24    ME      BUCKSPORT 44.60665 -68.81892
25    ME        FAYETTE 44.46380 -70.12047


trnd1_tbla <- trnd1_tbl %>% top_n(25)
print(trnd1_tbla)
head(trnd1_tbla,n=25)

A tibble: 25 x 5
   city      state Basecountsum Basecount2 prop_of_total
   <fct>     <fct>        <dbl>      <dbl>         <dbl>
 1 ATLANTA   GA            2352         12       0.00510
 2 BRADENTON FL            2352          8       0.00340
 3 BROOKLYN  NY            2352         30       0.0128 
 4 CHARLOTTE NC            2352          8       0.00340
 5 CHICAGO   IL            2352         17       0.00723
 6 COLUMBUS  OH            2352         11       0.00468
 7 CUMMING   GA            2352          8       0.00340
 8 DALLAS    TX            2352          8       0.00340
 9 ERIE      PA            2352         12       0.00510
10 HOUSTON   TX            2352         12       0.00510
# ... with 15 more rows

WHP

From: Eric Berger <ericjberger at gmail.com> 
Sent: Tuesday, May 12, 2020 8:39 AM
To: Poling, William <PolingW at aetna.com>
Cc: r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with Kmeans output and using broom to tidy etc..

**** External Email - Use Caution ****
Can you create a reproducible example??
Your question involves objects that are unknown to us. (geo1, trnd1_tbl)

On Tue, May 12, 2020 at 2:41 PM Poling, William via R-help <mailto:r-help at r-project.org> wrote:
#RStudio Version Version 1.2.1335 need this one--> 1.2.5019
sessionInfo() 
# R version 4.0.0 Patched (2020-05-03 r78349)
#Platform: x86_64-w64-mingw32/x64 (64-bit)
#Running under: Windows 10 x64 (build 17763)

Hello:

I have data that I am trying to manipulate for Kmeans clustering.

Original data looks like this

str(geo1) 
# 'data.frame': 2352 obs. of? 5 variables:
# $ ID: Factor w/ 2352 levels "101040199600",..: 590 908 976 509 1674 690 1336 86 726 1702 ...
# $ state? ? ? ? ? ?: Factor w/ 41 levels "AL","AR","AZ",..: 32 10 25 11 9 32 13 31 12 12 ...
# $ city? ? ? ? ? ? : Factor w/ 1337 levels "ABBOTTSTOWN",..: 932 156 230 698 965 1330 515 727 1127 1304 ...
# $ latitude? ? ? ? : num? 40.4 31.2 40.8 42.1 26.8 ...
# $ longitude? ? ? ?: num? -79.9 -81.5 -74 -91.6 -82.1 ...

I created a subset adding column prop_of_total 
str(trnd1_tbl)
tibble [1,457 x 5] (S3: tbl_df/tbl/data.frame)
?$ city? ? ? ? ?: Factor w/ 1337 levels "ABBOTTSTOWN",..: 1 2 3 4 5 6 7 8 9 10 ...
?$ state? ? ? ? : Factor w/ 41 levels "AL","AR","AZ",..: 32 36 10 28 12 36 10 11 26 38 ...
?$ Basecountsum : num [1:1457] 2352 2352 2352 2352 2352 ...
?$ Basecount2? ?: num [1:1457] 1 1 1 1 1 2 1 1 2 1 ...
?$ prop_of_total: num [1:1457] 0.000425 0.000425 0.000425 0.000425 0.000425 ...


Then I spread it

trnd2_tbl <- trnd1_tbl %>% 
? ? dplyr::select(city, state, prop_of_total) %>% 
? ? spread(key = city, value = prop_of_total, fill = 0) #remove the NA's with fill

str(trnd2_tbl)#tibble [41 x 1,338] (S3: tbl_df/tbl/data.frame)

Then I run a Kmeans

kmeans_obj1 <- trnd2_tbl? %>% 
? dplyr::select(- state) %>% 
? kmeans(centers = 20, nstart = 100)

str(kmeans_obj1)
List of 9
?$ cluster? ? ?: int [1:41] 11 11 9 11 11 4 11 11 16 2 ...
?$ centers? ? ?: num [1:20, 1:1337] 0 0 0 0 0 0 0 0 0 0 ...
? ..- attr(*, "dimnames")=List of 2
? .. ..$ : chr [1:20] "1" "2" "3" "4" ...
? .. ..$ : chr [1:1337] "ABBOTTSTOWN" "ABILENE" "ACWORTH" "ADAMS" ...
?$ totss? ? ? ?: num 0.00158
?$ withinss? ? : num [1:20] 0 0 0 0 0 0 0 0 0 0 ...
?$ tot.withinss: num 0.0000848
?$ betweenss? ?: num 0.0015
?$ size? ? ? ? : int [1:20] 1 1 1 1 1 1 1 1 1 1 ...
?$ iter? ? ? ? : int 3
?$ ifault? ? ? : int 0
?- attr(*, "class")= chr "kmeans"

Then I go and try to tidy:

#Tidy, glance, augment
#Just makes it easier to use or view the obj's in the obj list

? broom::tidy(kmeans_obj1) %>% glimpse()

? ? ? ? broom::glance(kmeans_obj1)
##A tibble: 1 x 4
# totss tot.withinss betweenss? iter
# <dbl>? ? ? ? <dbl>? ? ?<dbl> <int>
#? ?1 0.00158? ? 0.0000848? ?0.00150? ? ?3

However, when I run this piece I get an error:

broom::augment(kmeans_obj1, trnd2_tbl) %>% 
? dplyr::select(city, .cluster)? ? ? ? ? ? ?

#Error: Must subset columns with a valid subscript vector.
# The subscript has the wrong type `data.frame<
?# u: double
#? x: double
>`.
i It must be numeric or character.

Here is the back trace:

rlang::last_error()

# Backtrace:
#? ?1. broom::augment(kmeans_obj1, trnd2_tbl)
# 9. dplyr::select(., city, .cluster)
# 11. tidyselect::vars_select(tbl_vars(.data), !!!enquos(...))
# 12. tidyselect:::eval_select_impl(...)
# 20. tidyselect:::vars_select_eval(...)
# 21. tidyselect:::walk_data_tree(expr, data_mask, context_mask)
# 22. tidyselect:::eval_c(expr, data_mask, context_mask)
# 23. tidyselect:::reduce_sels(node, data_mask, context_mask, init = init)
# 24. tidyselect:::walk_data_tree(new, data_mask, context_mask)
# 25. tidyselect:::as_indices_sel_impl(...)
# 26. tidyselect:::as_indices_impl(x, vars, strict = strict)
# 27. vctrs::vec_as_subscript(x, logical = "error")

I am not sure what I am supposed to fix?

Maybe someone has had similar error and can advise me please?

Thank you.

WHP







Proprietary

NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}

______________________________________________
mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sMhCVDVDKajwJ9te2qVsWXQ2aq4kAe7150EICM51Pw4&s=eSV6ISkAsnmonaRvNdtmx4Lr9vumgXwMYF87DoRP86s&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sMhCVDVDKajwJ9te2qVsWXQ2aq4kAe7150EICM51Pw4&s=8wmXM73ofNcrn1i9gF-qxOzj7zRJZSPcaA5qg0vggG4&e=
and provide commented, minimal, self-contained, reproducible code.

Proprietary

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From er|cjberger @end|ng |rom gm@||@com  Tue May 12 21:07:10 2020
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Tue, 12 May 2020 22:07:10 +0300
Subject: [R] Help with Kmeans output and using broom to tidy etc..
In-Reply-To: <BYAPR06MB53832C0F432D88624E84AC25AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB5383EA994C10F43076877EE4AEA10@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CAGgJW74OPs+mB8OKLc0BCpaCiNMO08=-Ex+8mW5qtk38+Lnh8A@mail.gmail.com>
 <BYAPR06MB53832C0F432D88624E84AC25AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <CAGgJW74Zt+8dK=9kbssX_vK2yb_pxkyNiA+Shr-JAAsoi1PJeg@mail.gmail.com>

Please use dput()



On Tue, May 12, 2020 at 7:11 PM Poling, William <PolingW at aetna.com> wrote:

> Hello Eric, thank you so much for your consideration.
>
> Here are snippets of data that I hope will be helpful
>
> WHP
>
> geo1a <- geo1[, c(2:5)] <-- eliminating ID which is not useful for my
> purposes anyway
>
> #This is for R-Help use
> geo1a <- geo1a %>% top_n(25)
>
> state           city latitude longitude
> 1     ME      FAIRFIELD 44.64485 -69.65948
> 2     ME      JONESPORT 44.57935 -67.56743
> 3     ME        CASWELL 46.97529 -67.83023
> 4     ME      ELLSWORTH 44.52916 -68.38717
> 5     ME     VASSALBORO 44.45095 -69.60629
> 6     ME          UNION 44.20059 -69.26123
> 7     ME        PALERMO 44.45142 -69.41115
> 8     ME          ORONO 44.87426 -68.68327
> 9     ME    SANGERVILLE 45.10138 -69.33580
> 10    ME      ISLESBORO 44.29015 -68.90812
> 11    ME        TOPSHAM 43.93600 -69.96565
> 12    ME       FREEPORT 43.84089 -70.11160
> 13    ME      SKOWHEGAN 44.76687 -69.71644
> 14    ME    MILLINOCKET 45.65501 -68.70261
> 15    ME      ORRINGTON 44.72417 -68.74026
> 16    ME     ST. GEORGE 43.96726 -69.20827
> 17    ME FORT FAIRFIELD 46.80911 -67.88079
> 18    ME      MARS HILL 46.56580 -67.89006
> 19    ME       FREEPORT 43.85302 -70.03726
> 20    ME         EASTON 46.64143 -67.91203
> 21    ME     WATERVILLE 44.53621 -69.65913
> 22    ME      BRUNSWICK 43.87771 -69.96297
> 23    ME      BRUNSWICK 43.91719 -69.89905
> 24    ME      BUCKSPORT 44.60665 -68.81892
> 25    ME        FAYETTE 44.46380 -70.12047
>
>
> trnd1_tbla <- trnd1_tbl %>% top_n(25)
> print(trnd1_tbla)
> head(trnd1_tbla,n=25)
>
> A tibble: 25 x 5
>    city      state Basecountsum Basecount2 prop_of_total
>    <fct>     <fct>        <dbl>      <dbl>         <dbl>
>  1 ATLANTA   GA            2352         12       0.00510
>  2 BRADENTON FL            2352          8       0.00340
>  3 BROOKLYN  NY            2352         30       0.0128
>  4 CHARLOTTE NC            2352          8       0.00340
>  5 CHICAGO   IL            2352         17       0.00723
>  6 COLUMBUS  OH            2352         11       0.00468
>  7 CUMMING   GA            2352          8       0.00340
>  8 DALLAS    TX            2352          8       0.00340
>  9 ERIE      PA            2352         12       0.00510
> 10 HOUSTON   TX            2352         12       0.00510
> # ... with 15 more rows
>
> WHP
>
> From: Eric Berger <ericjberger at gmail.com>
> Sent: Tuesday, May 12, 2020 8:39 AM
> To: Poling, William <PolingW at aetna.com>
> Cc: r-help at r-project.org
> Subject: [EXTERNAL] Re: [R] Help with Kmeans output and using broom to
> tidy etc..
>
> **** External Email - Use Caution ****
> Can you create a reproducible example?
> Your question involves objects that are unknown to us. (geo1, trnd1_tbl)
>
> On Tue, May 12, 2020 at 2:41 PM Poling, William via R-help <mailto:
> r-help at r-project.org> wrote:
> #RStudio Version Version 1.2.1335 need this one--> 1.2.5019
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit)
> #Running under: Windows 10 x64 (build 17763)
>
> Hello:
>
> I have data that I am trying to manipulate for Kmeans clustering.
>
> Original data looks like this
>
> str(geo1)
> # 'data.frame': 2352 obs. of  5 variables:
> # $ ID: Factor w/ 2352 levels "101040199600",..: 590 908 976 509 1674 690
> 1336 86 726 1702 ...
> # $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 32 10 25 11 9
> 32 13 31 12 12 ...
> # $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 932 156 230
> 698 965 1330 515 727 1127 1304 ...
> # $ latitude        : num  40.4 31.2 40.8 42.1 26.8 ...
> # $ longitude       : num  -79.9 -81.5 -74 -91.6 -82.1 ...
>
> I created a subset adding column prop_of_total
> str(trnd1_tbl)
> tibble [1,457 x 5] (S3: tbl_df/tbl/data.frame)
>  $ city         : Factor w/ 1337 levels "ABBOTTSTOWN",..: 1 2 3 4 5 6 7 8
> 9 10 ...
>  $ state        : Factor w/ 41 levels "AL","AR","AZ",..: 32 36 10 28 12 36
> 10 11 26 38 ...
>  $ Basecountsum : num [1:1457] 2352 2352 2352 2352 2352 ...
>  $ Basecount2   : num [1:1457] 1 1 1 1 1 2 1 1 2 1 ...
>  $ prop_of_total: num [1:1457] 0.000425 0.000425 0.000425 0.000425
> 0.000425 ...
>
>
> Then I spread it
>
> trnd2_tbl <- trnd1_tbl %>%
>     dplyr::select(city, state, prop_of_total) %>%
>     spread(key = city, value = prop_of_total, fill = 0) #remove the NA's
> with fill
>
> str(trnd2_tbl)#tibble [41 x 1,338] (S3: tbl_df/tbl/data.frame)
>
> Then I run a Kmeans
>
> kmeans_obj1 <- trnd2_tbl  %>%
>   dplyr::select(- state) %>%
>   kmeans(centers = 20, nstart = 100)
>
> str(kmeans_obj1)
> List of 9
>  $ cluster     : int [1:41] 11 11 9 11 11 4 11 11 16 2 ...
>  $ centers     : num [1:20, 1:1337] 0 0 0 0 0 0 0 0 0 0 ...
>   ..- attr(*, "dimnames")=List of 2
>   .. ..$ : chr [1:20] "1" "2" "3" "4" ...
>   .. ..$ : chr [1:1337] "ABBOTTSTOWN" "ABILENE" "ACWORTH" "ADAMS" ...
>  $ totss       : num 0.00158
>  $ withinss    : num [1:20] 0 0 0 0 0 0 0 0 0 0 ...
>  $ tot.withinss: num 0.0000848
>  $ betweenss   : num 0.0015
>  $ size        : int [1:20] 1 1 1 1 1 1 1 1 1 1 ...
>  $ iter        : int 3
>  $ ifault      : int 0
>  - attr(*, "class")= chr "kmeans"
>
> Then I go and try to tidy:
>
> #Tidy, glance, augment
> #Just makes it easier to use or view the obj's in the obj list
>
>   broom::tidy(kmeans_obj1) %>% glimpse()
>
>         broom::glance(kmeans_obj1)
> ##A tibble: 1 x 4
> # totss tot.withinss betweenss  iter
> # <dbl>        <dbl>     <dbl> <int>
> #   1 0.00158    0.0000848   0.00150     3
>
> However, when I run this piece I get an error:
>
> broom::augment(kmeans_obj1, trnd2_tbl) %>%
>   dplyr::select(city, .cluster)
>
> #Error: Must subset columns with a valid subscript vector.
> # The subscript has the wrong type `data.frame<
>  # u: double
> #  x: double
> >`.
> i It must be numeric or character.
>
> Here is the back trace:
>
> rlang::last_error()
>
> # Backtrace:
> #   1. broom::augment(kmeans_obj1, trnd2_tbl)
> # 9. dplyr::select(., city, .cluster)
> # 11. tidyselect::vars_select(tbl_vars(.data), !!!enquos(...))
> # 12. tidyselect:::eval_select_impl(...)
> # 20. tidyselect:::vars_select_eval(...)
> # 21. tidyselect:::walk_data_tree(expr, data_mask, context_mask)
> # 22. tidyselect:::eval_c(expr, data_mask, context_mask)
> # 23. tidyselect:::reduce_sels(node, data_mask, context_mask, init = init)
> # 24. tidyselect:::walk_data_tree(new, data_mask, context_mask)
> # 25. tidyselect:::as_indices_sel_impl(...)
> # 26. tidyselect:::as_indices_impl(x, vars, strict = strict)
> # 27. vctrs::vec_as_subscript(x, logical = "error")
>
> I am not sure what I am supposed to fix?
>
> Maybe someone has had similar error and can advise me please?
>
> Thank you.
>
> WHP
>
>
>
>
>
>
>
> Proprietary
>
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}
>
> ______________________________________________
> mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sMhCVDVDKajwJ9te2qVsWXQ2aq4kAe7150EICM51Pw4&s=eSV6ISkAsnmonaRvNdtmx4Lr9vumgXwMYF87DoRP86s&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sMhCVDVDKajwJ9te2qVsWXQ2aq4kAe7150EICM51Pw4&s=8wmXM73ofNcrn1i9gF-qxOzj7zRJZSPcaA5qg0vggG4&e=
> and provide commented, minimal, self-contained, reproducible code.
>
> Proprietary
>
> NOTICE TO RECIPIENT OF INFORMATION:
> This e-mail may contain confidential or privileged information. If you
> think you have received this e-mail in error, please advise the sender by
> reply e-mail and then delete this e-mail immediately.
> This e-mail may also contain protected health information (PHI) with
> information about sensitive medical conditions, including, but not limited
> to, treatment for substance use disorders, behavioral health, HIV/AIDS, or
> pregnancy. This type of information may be protected by various federal
> and/or state laws which prohibit any further disclosure without the express
> written consent of the person to whom it pertains or as otherwise permitted
> by law. Any unauthorized further disclosure may be considered a violation
> of federal and/or state law. A general authorization for the release of
> medical or other information may NOT be sufficient consent for release of
> this type of information.
> Thank you. Aetna
>

	[[alternative HTML version deleted]]


From Po||ngW @end|ng |rom @etn@@com  Tue May 12 21:19:58 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Tue, 12 May 2020 19:19:58 +0000
Subject: [R] Help with Kmeans output and using broom to tidy etc..
In-Reply-To: <CAGgJW74Zt+8dK=9kbssX_vK2yb_pxkyNiA+Shr-JAAsoi1PJeg@mail.gmail.com>
References: <BYAPR06MB5383EA994C10F43076877EE4AEA10@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CAGgJW74OPs+mB8OKLc0BCpaCiNMO08=-Ex+8mW5qtk38+Lnh8A@mail.gmail.com>
 <BYAPR06MB53832C0F432D88624E84AC25AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CAGgJW74Zt+8dK=9kbssX_vK2yb_pxkyNiA+Shr-JAAsoi1PJeg@mail.gmail.com>
Message-ID: <BYAPR06MB5383571CED3926DF39CA0D96AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>

Yes, of course. Thank you for your patience.

str(geo1a)
'data.frame':     25 obs. of  4 variables:
$ state    : Factor w/ 41 levels "AL","AR","AZ",..: 19 19 19 19 19 19 19 19 19 19 ...
$ city     : Factor w/ 1337 levels "ABBOTTSTOWN",..: 366 574 193 343 1223 1207 885 871 1054 556 ...
$ latitude : num  44.6 44.6 47 44.5 44.5 ...
$ longitude: num  -69.7 -67.6 -67.8 -68.4 -69.6 ...

dput(geo1a)
structure(list(state = structure(c(19L, 19L, 19L, 19L, 19L, 19L,
19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L,
19L, 19L, 19L, 19L, 19L, 19L), .Label = c("AL", "AR", "AZ", "CA",
"CO", "CT", "DC", "DE", "FL", "GA", "IA", "IL", "IN", "KS", "KY",
"LA", "MA", "MD", "ME", "MI", "MN", "MO", "NC", "NE", "NJ", "NM",
"NV", "NY", "OH", "OK", "OR", "PA", "SC", "SD", "TN", "TX", "UT",
"VA", "WA", "WI", "WV"), class = "factor"), city = structure(c(366L,
574L, 193L, 343L, 1223L, 1207L, 885L, 871L, 1054L, 556L, 1192L,
414L, 1097L, 749L, 872L, 1134L, 397L, 700L, 414L, 332L, 1256L,
156L, 156L, 158L, 375L), .Label = c("ABBOTTSTOWN", "ABILENE",
"ACWORTH", "ADAMS", "ADDISON", "ADKINS", "ALBANY", "ALBIA", "ALBUQUERQUE",
"ALEXANDRIA", "ALFRED", "ALIQUIPPA", "ALISO VIEJO", "ALLEN PARK",
"ALLENTOWN", "ALPHA", "ALPHARETTA", "ALPINE", "ALTOONA", "AMARILLO",
"AMBLER", "AMBRIDGE", "AMHERST", "AMITYVILLE", "ANAHIEM", "ANDERSON",
"ANDOVER", "ANGIER", "ANGLETON", "ANKENY", "ANN ARBOR", "ANZA",
"APEX", "APOLLO", "ARCADIA", "ARCHDALE", "ARDMORE", "ARGYLE",
"ARKANSAS CITY", "ARLINGTON", "ARNOLD", "ARTHUR", "ASHEBORO",
"ASHEVILLE", "ASHLAND", "ASHLAND CITY", "ASHTON", "ASTON", "ATHENS",
"ATLANTA", "ATLANTIC BCH", "AUBORN", "AUGUSTA", "AURORA", "AUSTELL",
"AUSTIN", "AVENTURA", "AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN CITY",
"BALDWIN PLACE", "BALLWIN", "BANGOR", "BARBOURSVILLE", "BARNEGAT",
"BARTLETT", "BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", "BATESBURG",
"BATON ROUGE", "BAXTER SPRINGS", "BAY CITY", "BAYONNE", "BAYSIDE",
"BAYTOWN", "BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER CITY",
"BELFAIR", "BELLE VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR",
"BELLMORE", "BELLWOOD", "BELMAR", "BELMONT", "BELVIDERE", "BENNET",
"BENTON", "BENTONVILLE", "BERLIN", "BESSEMER CITY", "BETHANY",
"BETHEL", "BETHEL PARK", "BETHESDA", "BETHLEHEM", "BETTENDORF",
"BIGLERVILLE", "BINGHAMTON", "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN",
"BLACKSBURG", "BLAINE", "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD",
"BLUE BELL", "BLUE GRASS", "BLUE POINT", "BLUE SPRINGS", "BOCA RATON",
"BODFISH", "BOILING SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS",
"BONNEY LAKE", "BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING GREEN",
"BOYERTOWN", "BOYNTON BEACH", "BRADENTON", "BRADENVILLE", "BRANDAMORE",
"BRANDON", "BRANFORD", "BRANSON", "BRASELTON", "BREEZEWOOD",
"BRENTON", "BRENTWOOD", "BREWSTER", "BRICK", "BRIDGEPORT", "BRIDGETON",
"BRIGHAM CITY", "BROADDUS", "BROCKPORT", "BROGUE", "BROKEN ARROW",
"BRONX", "BROOKFIELD", "BROOKHAVEN", "BROOKLYN", "BROWNS MILLS",
"BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", "BRYAN", "BUCKSPORT",
"BUDD LAKE", "BUFFALO", "BULVERDE", "BUNKERVILLE", "BURBANK",
"BURFORD", "BURIEN", "BURLINGTON", "BURR RIDGE", "BURTON", "BUSKIRK",
"BUTLER", "BUXTON", "BYRON", "CALERA", "CAMBRIDGE", "CAMP HILL",
"CAMPBELL", "CANAAN", "CANAL WINCHESTER", "CANASTOTA", "CANTON",
"CARDIFF BY THE SEA", "CARL JUNCTION", "CARLISLE", "CARLTON",
"CAROL STREAM", "CARROLLTON", "CARSON CITY", "CARTERSVILLE",
"CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", "CASWELL", "CATO",
"CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER POINT", "CENTERTON",
"CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN FALLS", "CHALFONT",
"CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", "CHARLESTON",
"CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", "CHESHIRE",
"CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", "CHILLICOTHE",
"CHOCTAW", "CICERO", "CINCINNATI", "CLAY", "CLEARWATER", "CLEARWATER BEACH",
"CLEMMONS", "CLENDENIN", "CLEVELAND", "CLEVELAND HTS", "CLEVES",
"CLIFFSIDE PARK", "CLIFTON", "CLIFTON PARK", "CLIFTON SPGS",
"CLINTON", "COACHELLA", "COAL TOWNSHIP", "COATESVILLE", "COLLEGE STATION",
"COLLINSVILLE", "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE",
"CONCORD", "CONCORD TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY",
"CONYERS", "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA",
"CORNELIUS", "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY",
"COTTAGE HILLS", "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN",
"CRANBERRY TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER",
"CUMBERLAND FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS",
"DALLAS", "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT",
"DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR",
"DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH",
"DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY",
"DES MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER",
"DIXON", "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER",
"DOYLESTOWN", "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH",
"DUNCAN", "DUNCANNON", "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE",
"DURHAM", "DUTTON", "EAGLE GROVE", "EAST AURORA", "EAST CHICAGO",
"EAST EARL", "EAST HANOVER", "EAST HARTFORD", "EAST HARTLAND",
"EAST LEROY", "EAST LIVERPOOL", "EAST MEADOW", "EAST NORTHPORT",
"EAST ORANGE", "EAST PEORIA", "EAST SAINT LOUIS", "EASTON", "EDGERTON",
"EDISON", "EDMOND", "EGG HBR TWP", "EL DORADO", "EL PASO", "ELDRIDGE",
"ELGIN", "ELIZABETHVILLE", "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY",
"ELMHURST", "ELMWOOD PARK", "ELOY", "ELY", "EMERSON", "EMORY",
"ENDICOTT", "ENGLEWOOD", "ENID", "ENNICE", "ENOLA", "ENUMCLAW",
"EPHRATA", "ERIE", "ESTERO", "EUREKA SPRINGS", "EVANSVILLE",
"EVERETT", "EXCELSIOR SPRINGS", "EXTON", "FAIRFAX", "FAIRFIELD",
"FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", "FALMOUTH", "FAR HILLS",
"FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", "FAYETTE", "FAYETTEVILLE",
"FELTON", "FENTON", "FERGUSON", "FIELDALE", "FINDLAY", "FINGERVILLE",
"FLEMINGTON", "FLINT", "FLORENCE", "FLORESVILLE", "FLORISSANT",
"FLOVILLA", "FLOWER MOUND", "FLUSHING", "FOGELSVILLE", "FOLSOM",
"FONTANELLE", "FOREST HILLS", "FOREST PARK", "FORT DODGE", "FORT FAIRFIELD",
"FORT GAY", "FORT LAUDERDALE", "FORT LEE", "FORT MILL", "FORT MOHAVE",
"FORT PAYNE", "FORT PIERCE", "FORT SMITH", "FORT WAYNE", "FORT WORTH",
"FOSTORIA", "FOUNTAIN INN", "FRANKFORT", "FRANKLIN", "FREEDOM",
"FREEHOLD", "FREEPORT", "FREMONT", "FULLERTON", "FULSHEAR", "FULTON",
"GADSDEN", "GAFFNEY", "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE",
"GARDNERS", "GARNETT VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA",
"GETTYSBURG", "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE",
"GLEN CARBON", "GLEN COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW",
"GLENWOOD", "GOLDEN CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM",
"GOSHEN", "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS",
"GRANDVIEW", "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS",
"GRASS VALLEY", "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER",
"GREENLAWN", "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE",
"GREER", "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD",
"HADLEY", "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON",
"HAMBURG", "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE",
"HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON",
"HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL",
"HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE",
"HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH",
"HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE",
"HIGH HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS",
"HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE",
"HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD",
"HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS",
"HOT SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON",
"HUFFMAN", "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON",
"HUNTINGTON BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE",
"HURST", "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL",
"INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY",
"IRMO", "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP",
"ISSAQUAH", "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS",
"JACKSONVILLE", "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE",
"JEFFERSON", "JEFFERSON CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN",
"JONESBORO", "JONESPORT", "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE",
"KANNAPOLIS", "KANSAS CITY", "KATY", "KEARNEY", "KELLERTON",
"KEMAH", "KENMORE", "KENNESAW", "KENT", "KINGMAN", "KINGS MOUNTAIN",
"KINGSLAND", "KINGSTON", "KINGWOOD", "KIRKLAND", "KUTZTOWN",
"LA QUINTA", "LA RUE", "LA VERNE", "LAFAYETTE", "LAKE BLUFF",
"LAKE IN THE HILL", "LAKE ISABELLA", "LAKE PLACID", "LAKE SAINT LOUIS",
"LAKE TAPPS", "LAKE VILLAGE", "LAKE WAUKOMIS", "LAKE WORTH",
"LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD RCH", "LAMBERTVILLE",
"LANCASTER", "LANDISVILLE", "LANSDALE", "LAREDO", "LARGO", "LAS VEGAS",
"LATROBE", "LAUDERHILL", "LAWNDALE", "LAWRENCE", "LAWRENCEVILLE",
"LE MARS", "LE ROY", "LEAGUE CITY", "LEAVENWORTH", "LEAWOOD",
"LEBANON", "LECOMPTON", "LEECHBURG", "LEES SUMMIT", "LEESBURG",
"LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", "LEVITTOWN", "LEWES",
"LEWISTOWN", "LEXINGTON", "LIBERTY TWP", "LILLINGTON", "LINCOLN",
"LINDENHURST", "LINTON", "LISBON", "LITCHFIELD PK", "LITHIA",
"LITITZ", "LITTLE EGG HARBOR TO", "LK FOREST PK", "LK PEEKSKILL",
"LOCKHART", "LOCKWOOD", "LOGAN", "LOMA LINDA", "LONE JACK", "LONG BEACH",
"LONG LANE", "LONGVIEW", "LORAIN", "LORTON", "LOS ANGELES", "LOUISBURG",
"LOVES PARK", "LOWER BURRELL", "LOWRY CITY", "LUBBOCK", "LUGOFF",
"LUMBERTON", "LYNBROOK", "LYNNWOOD", "MACHESNEY PARK", "MACON",
"MAGNOLIA", "MAHOPAC", "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE",
"MANASSAS", "MANCHACA", "MANCHESTER", "MANCHESTER TOWNSHIP",
"MANCHESTER TW", "MANHEIM", "MANITO", "MANLIUS", "MANNINGTON",
"MANSFIELD", "MANSURA", "MAPLE PARK", "MARBLE FALLS", "MARCELLUS",
"MARICOPA", "MARIETTA", "MARION", "MARKLEYSBURG", "MARS HILL",
"MARSHALLTOWN", "MARSHFIELD", "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE",
"MASON CITY", "MASSEY", "MASSILLON", "MASURY", "MATAWAN", "MATEWAN",
"MATTAPOISETT", "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND",
"MC DONALD", "MC KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN",
"MCALESTER", "MCDONOUGH", "MCKEESPORT", "MCKINNEY", "MECHANICSBURG",
"MECHANICSVILLE", "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS",
"MERIDEN", "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA",
"MESQUITE", "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN",
"MIDLAND", "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK",
"MILLEDGEVILLE", "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE",
"MINEOLA", "MINERAL SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION",
"MISSION HILLS", "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY",
"MOLINE", "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE",
"MONTCLAIR", "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON",
"MORGANTOWN", "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE",
"MOUNT JOY", "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO",
"MOUNT UNION", "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT",
"MULVANE", "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT",
"MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS",
"NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES",
"NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG",
"NEW BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK",
"NEW CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON",
"NEW LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE",
"NEW ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK",
"NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT",
"NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA",
"NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK",
"NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON",
"NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND HILLS",
"NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE",
"NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON",
"OAK RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE",
"OMAHA", "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION",
"ORONO", "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO",
"OTTAWA", "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD",
"OYSTER BAY", "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR",
"PALMDALE", "PALMYRA", "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE",
"PARLIN", "PARMA", "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY",
"PAXTON", "PEA RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN",
"PELION", "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE",
"PENNSBURG", "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS",
"PERRYSBURG", "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI",
"PHOENIX", "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN",
"PINE BLUFF", "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY",
"PITMAN", "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY",
"PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO LAKE",
"POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", "PORT CHARLOTTE",
"PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", "PORT WASHINGTON",
"PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", "POTTSVILLE",
"POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE VILLAGE",
"PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT PLEASANT",
"PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", "QUINCY",
"RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", "READLYN",
"RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO PARK",
"REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", "RICH HILL",
"RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", "RINCON",
"RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", "RIVERSIDE",
"ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", "ROCKAWAY BCH",
"ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", "ROGERS",
"ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE",
"ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT",
"ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN",
"RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO",
"SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR",
"SAINT CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS",
"SAINT PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY",
"SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO",
"SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN",
"SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", "SARASOTA",
"SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", "SAYVILLE",
"SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", "SCOTT", "SCOTTSDALE",
"SEAFORD", "SEALE", "SEATTLE", "SEDONA", "SEGUIN", "SELDEN",
"SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", "SHARON", "SHARPSBURG",
"SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", "SHELLSBURG", "SHELTON",
"SHENANDOAH", "SHORELINE", "SHOREWOOD", "SHREVEPORT", "SICKLERVILLE",
"SILER CITY", "SILOAM SPRINGS", "SIMON", "SIMPSONVILLE", "SIMSBURY",
"SIOUX CITY", "SIOUX FALLS", "SKOKIE", "SKOWHEGAN", "SLATINGTON",
"SLIDELL", "SMITHFIELD", "SN BERNRDNO", "SNELLVILLE", "SOCORRO",
"SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH BELOIT", "SOUTH BEND",
"SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH PARK", "SOUTH PLAINFIELD",
"SOUTH SIOUX CITY", "SOUTHAMPTON", "SOUTHBURY", "SOUTHGATE",
"SPARKS", "SPARROW BUSH", "SPENCER", "SPRING", "SPRING BRANCH",
"SPRING GROVE", "SPRING HILL", "SPRING HOPE", "SPRING VALLEY",
"SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD GARDENS", "SPRINGHILL",
"SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", "ST PETERSBURG",
"ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", "STANLEY",
"STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", "STILLMAN VALLEY",
"STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", "STRAFFORD",
"STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", "SUFFIELD",
"SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", "SUMMIT",
"SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", "SWEENY",
"SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", "TAFTVILLE",
"TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", "TAYLORVILLE",
"TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", "THOMASVILLE",
"TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY PARK",
"TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", "TOPEKA",
"TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS RST",
"TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER",
"TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY",
"UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA",
"URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO",
"VAN BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO",
"VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA",
"VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH",
"W TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN",
"WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG",
"WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA",
"WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE",
"WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER",
"WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO",
"WEST CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN",
"WEST HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH",
"WEST POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD",
"WEST VALLEY CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER",
"WESTMORELAND CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD",
"WHARTON", "WHEELING", "WHITE PLAINS", "WHITEHALL", "WHITESTONE",
"WHITESTOWN", "WHITING", "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD",
"WILLARD", "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK",
"WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS",
"WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON",
"WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE",
"WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS",
"WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH",
"YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS",
"YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor"),
    latitude = c(44.644849, 44.579346, 46.975293, 44.529164,
    44.450953, 44.200589, 44.45142, 44.874264, 45.101378, 44.29015,
    43.935997, 43.840888, 44.766867, 45.655006, 44.724172, 43.967257,
    46.80911, 46.565795, 43.853017, 46.64143, 44.53621, 43.877711,
    43.91719, 44.606652, 44.4638), longitude = c(-69.659477,
    -67.567428, -67.83023, -68.387175, -69.606293, -69.261228,
    -69.411145, -68.683265, -69.335795, -68.90812, -69.965652,
    -70.111596, -69.716441, -68.702608, -68.740265, -69.208272,
    -67.880793, -67.890059, -70.037259, -67.91203, -69.65913,
    -69.962967, -69.899049, -68.818915, -70.12047)), row.names = c(NA,
-25L), class = "data.frame")

str(trnd1_tbla)
tibble [27 x 5] (S3: tbl_df/tbl/data.frame)
$ city         : Factor w/ 1337 levels "ABBOTTSTOWN",..: 50 130 152 209 217 242 269 272 358 530 ...
$ state        : Factor w/ 41 levels "AL","AR","AZ",..: 10 9 28 23 12 29 10 36 32 36 ...
$ Basecountsum : num [1:27] 2352 2352 2352 2352 2352 ...
$ Basecount2   : num [1:27] 12 8 30 8 17 11 8 8 12 12 ...
$ prop_of_total: num [1:27] 0.0051 0.0034 0.01276 0.0034 0.00723 ...

dput(trnd1_tbla)
structure(list(city = structure(c(50L, 130L, 152L, 209L, 217L,
242L, 269L, 272L, 358L, 530L, 544L, 563L, 580L, 613L, 618L, 879L,
919L, 932L, 970L, 1002L, 1033L, 1045L, 1056L, 1127L, 1140L, 1296L,
1330L), .Label = c("ABBOTTSTOWN", "ABILENE", "ACWORTH", "ADAMS",
"ADDISON", "ADKINS", "ALBANY", "ALBIA", "ALBUQUERQUE", "ALEXANDRIA",
"ALFRED", "ALIQUIPPA", "ALISO VIEJO", "ALLEN PARK", "ALLENTOWN",
"ALPHA", "ALPHARETTA", "ALPINE", "ALTOONA", "AMARILLO", "AMBLER",
"AMBRIDGE", "AMHERST", "AMITYVILLE", "ANAHIEM", "ANDERSON", "ANDOVER",
"ANGIER", "ANGLETON", "ANKENY", "ANN ARBOR", "ANZA", "APEX",
"APOLLO", "ARCADIA", "ARCHDALE", "ARDMORE", "ARGYLE", "ARKANSAS CITY",
"ARLINGTON", "ARNOLD", "ARTHUR", "ASHEBORO", "ASHEVILLE", "ASHLAND",
"ASHLAND CITY", "ASHTON", "ASTON", "ATHENS", "ATLANTA", "ATLANTIC BCH",
"AUBORN", "AUGUSTA", "AURORA", "AUSTELL", "AUSTIN", "AVENTURA",
"AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN CITY", "BALDWIN PLACE",
"BALLWIN", "BANGOR", "BARBOURSVILLE", "BARNEGAT", "BARTLETT",
"BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", "BATESBURG", "BATON ROUGE",
"BAXTER SPRINGS", "BAY CITY", "BAYONNE", "BAYSIDE", "BAYTOWN",
"BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER CITY", "BELFAIR",
"BELLE VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR", "BELLMORE",
"BELLWOOD", "BELMAR", "BELMONT", "BELVIDERE", "BENNET", "BENTON",
"BENTONVILLE", "BERLIN", "BESSEMER CITY", "BETHANY", "BETHEL",
"BETHEL PARK", "BETHESDA", "BETHLEHEM", "BETTENDORF", "BIGLERVILLE",
"BINGHAMTON", "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN", "BLACKSBURG",
"BLAINE", "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD", "BLUE BELL",
"BLUE GRASS", "BLUE POINT", "BLUE SPRINGS", "BOCA RATON", "BODFISH",
"BOILING SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS", "BONNEY LAKE",
"BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING GREEN", "BOYERTOWN",
"BOYNTON BEACH", "BRADENTON", "BRADENVILLE", "BRANDAMORE", "BRANDON",
"BRANFORD", "BRANSON", "BRASELTON", "BREEZEWOOD", "BRENTON",
"BRENTWOOD", "BREWSTER", "BRICK", "BRIDGEPORT", "BRIDGETON",
"BRIGHAM CITY", "BROADDUS", "BROCKPORT", "BROGUE", "BROKEN ARROW",
"BRONX", "BROOKFIELD", "BROOKHAVEN", "BROOKLYN", "BROWNS MILLS",
"BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", "BRYAN", "BUCKSPORT",
"BUDD LAKE", "BUFFALO", "BULVERDE", "BUNKERVILLE", "BURBANK",
"BURFORD", "BURIEN", "BURLINGTON", "BURR RIDGE", "BURTON", "BUSKIRK",
"BUTLER", "BUXTON", "BYRON", "CALERA", "CAMBRIDGE", "CAMP HILL",
"CAMPBELL", "CANAAN", "CANAL WINCHESTER", "CANASTOTA", "CANTON",
"CARDIFF BY THE SEA", "CARL JUNCTION", "CARLISLE", "CARLTON",
"CAROL STREAM", "CARROLLTON", "CARSON CITY", "CARTERSVILLE",
"CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", "CASWELL", "CATO",
"CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER POINT", "CENTERTON",
"CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN FALLS", "CHALFONT",
"CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", "CHARLESTON",
"CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", "CHESHIRE",
"CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", "CHILLICOTHE",
"CHOCTAW", "CICERO", "CINCINNATI", "CLAY", "CLEARWATER", "CLEARWATER BEACH",
"CLEMMONS", "CLENDENIN", "CLEVELAND", "CLEVELAND HTS", "CLEVES",
"CLIFFSIDE PARK", "CLIFTON", "CLIFTON PARK", "CLIFTON SPGS",
"CLINTON", "COACHELLA", "COAL TOWNSHIP", "COATESVILLE", "COLLEGE STATION",
"COLLINSVILLE", "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE",
"CONCORD", "CONCORD TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY",
"CONYERS", "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA",
"CORNELIUS", "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY",
"COTTAGE HILLS", "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN",
"CRANBERRY TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER",
"CUMBERLAND FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS",
"DALLAS", "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT",
"DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR",
"DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH",
"DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY",
"DES MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER",
"DIXON", "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER",
"DOYLESTOWN", "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH",
"DUNCAN", "DUNCANNON", "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE",
"DURHAM", "DUTTON", "EAGLE GROVE", "EAST AURORA", "EAST CHICAGO",
"EAST EARL", "EAST HANOVER", "EAST HARTFORD", "EAST HARTLAND",
"EAST LEROY", "EAST LIVERPOOL", "EAST MEADOW", "EAST NORTHPORT",
"EAST ORANGE", "EAST PEORIA", "EAST SAINT LOUIS", "EASTON", "EDGERTON",
"EDISON", "EDMOND", "EGG HBR TWP", "EL DORADO", "EL PASO", "ELDRIDGE",
"ELGIN", "ELIZABETHVILLE", "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY",
"ELMHURST", "ELMWOOD PARK", "ELOY", "ELY", "EMERSON", "EMORY",
"ENDICOTT", "ENGLEWOOD", "ENID", "ENNICE", "ENOLA", "ENUMCLAW",
"EPHRATA", "ERIE", "ESTERO", "EUREKA SPRINGS", "EVANSVILLE",
"EVERETT", "EXCELSIOR SPRINGS", "EXTON", "FAIRFAX", "FAIRFIELD",
"FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", "FALMOUTH", "FAR HILLS",
"FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", "FAYETTE", "FAYETTEVILLE",
"FELTON", "FENTON", "FERGUSON", "FIELDALE", "FINDLAY", "FINGERVILLE",
"FLEMINGTON", "FLINT", "FLORENCE", "FLORESVILLE", "FLORISSANT",
"FLOVILLA", "FLOWER MOUND", "FLUSHING", "FOGELSVILLE", "FOLSOM",
"FONTANELLE", "FOREST HILLS", "FOREST PARK", "FORT DODGE", "FORT FAIRFIELD",
"FORT GAY", "FORT LAUDERDALE", "FORT LEE", "FORT MILL", "FORT MOHAVE",
"FORT PAYNE", "FORT PIERCE", "FORT SMITH", "FORT WAYNE", "FORT WORTH",
"FOSTORIA", "FOUNTAIN INN", "FRANKFORT", "FRANKLIN", "FREEDOM",
"FREEHOLD", "FREEPORT", "FREMONT", "FULLERTON", "FULSHEAR", "FULTON",
"GADSDEN", "GAFFNEY", "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE",
"GARDNERS", "GARNETT VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA",
"GETTYSBURG", "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE",
"GLEN CARBON", "GLEN COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW",
"GLENWOOD", "GOLDEN CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM",
"GOSHEN", "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS",
"GRANDVIEW", "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS",
"GRASS VALLEY", "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER",
"GREENLAWN", "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE",
"GREER", "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD",
"HADLEY", "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON",
"HAMBURG", "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE",
"HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON",
"HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL",
"HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE",
"HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH",
"HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE",
"HIGH HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS",
"HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE",
"HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD",
"HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS",
"HOT SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON",
"HUFFMAN", "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON",
"HUNTINGTON BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE",
"HURST", "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL",
"INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY",
"IRMO", "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP",
"ISSAQUAH", "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS",
"JACKSONVILLE", "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE",
"JEFFERSON", "JEFFERSON CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN",
"JONESBORO", "JONESPORT", "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE",
"KANNAPOLIS", "KANSAS CITY", "KATY", "KEARNEY", "KELLERTON",
"KEMAH", "KENMORE", "KENNESAW", "KENT", "KINGMAN", "KINGS MOUNTAIN",
"KINGSLAND", "KINGSTON", "KINGWOOD", "KIRKLAND", "KUTZTOWN",
"LA QUINTA", "LA RUE", "LA VERNE", "LAFAYETTE", "LAKE BLUFF",
"LAKE IN THE HILL", "LAKE ISABELLA", "LAKE PLACID", "LAKE SAINT LOUIS",
"LAKE TAPPS", "LAKE VILLAGE", "LAKE WAUKOMIS", "LAKE WORTH",
"LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD RCH", "LAMBERTVILLE",
"LANCASTER", "LANDISVILLE", "LANSDALE", "LAREDO", "LARGO", "LAS VEGAS",
"LATROBE", "LAUDERHILL", "LAWNDALE", "LAWRENCE", "LAWRENCEVILLE",
"LE MARS", "LE ROY", "LEAGUE CITY", "LEAVENWORTH", "LEAWOOD",
"LEBANON", "LECOMPTON", "LEECHBURG", "LEES SUMMIT", "LEESBURG",
"LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", "LEVITTOWN", "LEWES",
"LEWISTOWN", "LEXINGTON", "LIBERTY TWP", "LILLINGTON", "LINCOLN",
"LINDENHURST", "LINTON", "LISBON", "LITCHFIELD PK", "LITHIA",
"LITITZ", "LITTLE EGG HARBOR TO", "LK FOREST PK", "LK PEEKSKILL",
"LOCKHART", "LOCKWOOD", "LOGAN", "LOMA LINDA", "LONE JACK", "LONG BEACH",
"LONG LANE", "LONGVIEW", "LORAIN", "LORTON", "LOS ANGELES", "LOUISBURG",
"LOVES PARK", "LOWER BURRELL", "LOWRY CITY", "LUBBOCK", "LUGOFF",
"LUMBERTON", "LYNBROOK", "LYNNWOOD", "MACHESNEY PARK", "MACON",
"MAGNOLIA", "MAHOPAC", "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE",
"MANASSAS", "MANCHACA", "MANCHESTER", "MANCHESTER TOWNSHIP",
"MANCHESTER TW", "MANHEIM", "MANITO", "MANLIUS", "MANNINGTON",
"MANSFIELD", "MANSURA", "MAPLE PARK", "MARBLE FALLS", "MARCELLUS",
"MARICOPA", "MARIETTA", "MARION", "MARKLEYSBURG", "MARS HILL",
"MARSHALLTOWN", "MARSHFIELD", "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE",
"MASON CITY", "MASSEY", "MASSILLON", "MASURY", "MATAWAN", "MATEWAN",
"MATTAPOISETT", "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND",
"MC DONALD", "MC KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN",
"MCALESTER", "MCDONOUGH", "MCKEESPORT", "MCKINNEY", "MECHANICSBURG",
"MECHANICSVILLE", "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS",
"MERIDEN", "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA",
"MESQUITE", "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN",
"MIDLAND", "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK",
"MILLEDGEVILLE", "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE",
"MINEOLA", "MINERAL SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION",
"MISSION HILLS", "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY",
"MOLINE", "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE",
"MONTCLAIR", "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON",
"MORGANTOWN", "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE",
"MOUNT JOY", "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO",
"MOUNT UNION", "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT",
"MULVANE", "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT",
"MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS",
"NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES",
"NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG",
"NEW BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK",
"NEW CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON",
"NEW LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE",
"NEW ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK",
"NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT",
"NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA",
"NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK",
"NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON",
"NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND HILLS",
"NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE",
"NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON",
"OAK RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE",
"OMAHA", "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION",
"ORONO", "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO",
"OTTAWA", "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD",
"OYSTER BAY", "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR",
"PALMDALE", "PALMYRA", "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE",
"PARLIN", "PARMA", "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY",
"PAXTON", "PEA RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN",
"PELION", "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE",
"PENNSBURG", "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS",
"PERRYSBURG", "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI",
"PHOENIX", "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN",
"PINE BLUFF", "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY",
"PITMAN", "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY",
"PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO LAKE",
"POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", "PORT CHARLOTTE",
"PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", "PORT WASHINGTON",
"PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", "POTTSVILLE",
"POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE VILLAGE",
"PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT PLEASANT",
"PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", "QUINCY",
"RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", "READLYN",
"RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO PARK",
"REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", "RICH HILL",
"RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", "RINCON",
"RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", "RIVERSIDE",
"ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", "ROCKAWAY BCH",
"ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", "ROGERS",
"ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE",
"ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT",
"ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN",
"RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO",
"SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR",
"SAINT CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS",
"SAINT PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY",
"SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO",
"SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN",
"SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", "SARASOTA",
"SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", "SAYVILLE",
"SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", "SCOTT", "SCOTTSDALE",
"SEAFORD", "SEALE", "SEATTLE", "SEDONA", "SEGUIN", "SELDEN",
"SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", "SHARON", "SHARPSBURG",
"SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", "SHELLSBURG", "SHELTON",
"SHENANDOAH", "SHORELINE", "SHOREWOOD", "SHREVEPORT", "SICKLERVILLE",
"SILER CITY", "SILOAM SPRINGS", "SIMON", "SIMPSONVILLE", "SIMSBURY",
"SIOUX CITY", "SIOUX FALLS", "SKOKIE", "SKOWHEGAN", "SLATINGTON",
"SLIDELL", "SMITHFIELD", "SN BERNRDNO", "SNELLVILLE", "SOCORRO",
"SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH BELOIT", "SOUTH BEND",
"SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH PARK", "SOUTH PLAINFIELD",
"SOUTH SIOUX CITY", "SOUTHAMPTON", "SOUTHBURY", "SOUTHGATE",
"SPARKS", "SPARROW BUSH", "SPENCER", "SPRING", "SPRING BRANCH",
"SPRING GROVE", "SPRING HILL", "SPRING HOPE", "SPRING VALLEY",
"SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD GARDENS", "SPRINGHILL",
"SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", "ST PETERSBURG",
"ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", "STANLEY",
"STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", "STILLMAN VALLEY",
"STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", "STRAFFORD",
"STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", "SUFFIELD",
"SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", "SUMMIT",
"SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", "SWEENY",
"SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", "TAFTVILLE",
"TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", "TAYLORVILLE",
"TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", "THOMASVILLE",
"TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY PARK",
"TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", "TOPEKA",
"TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS RST",
"TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER",
"TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY",
"UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA",
"URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO",
"VAN BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO",
"VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA",
"VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH",
"W TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN",
"WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG",
"WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA",
"WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE",
"WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER",
"WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO",
"WEST CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN",
"WEST HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH",
"WEST POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD",
"WEST VALLEY CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER",
"WESTMORELAND CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD",
"WHARTON", "WHEELING", "WHITE PLAINS", "WHITEHALL", "WHITESTONE",
"WHITESTOWN", "WHITING", "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD",
"WILLARD", "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK",
"WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS",
"WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON",
"WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE",
"WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS",
"WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH",
"YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS",
"YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor"),
    state = structure(c(10L, 9L, 28L, 23L, 12L, 29L, 10L, 36L,
    32L, 36L, 22L, 9L, 22L, 32L, 27L, 14L, 32L, 32L, 23L, 12L,
    22L, 36L, 9L, 22L, 28L, 14L, 32L), .Label = c("AL", "AR",
    "AZ", "CA", "CO", "CT", "DC", "DE", "FL", "GA", "IA", "IL",
    "IN", "KS", "KY", "LA", "MA", "MD", "ME", "MI", "MN", "MO",
    "NC", "NE", "NJ", "NM", "NV", "NY", "OH", "OK", "OR", "PA",
    "SC", "SD", "TN", "TX", "UT", "VA", "WA", "WI", "WV"), class = "factor"),
    Basecountsum = c(2352, 2352, 2352, 2352, 2352, 2352, 2352,
    2352, 2352, 2352, 2352, 2352, 2352, 2352, 2352, 2352, 2352,
    2352, 2352, 2352, 2352, 2352, 2352, 2352, 2352, 2352, 2352
    ), Basecount2 = c(12, 8, 30, 8, 17, 11, 8, 8, 12, 12, 8,
    9, 11, 9, 14, 8, 8, 23, 10, 12, 14, 20, 8, 8, 21, 9, 11),
    prop_of_total = c(0.00510204081632653, 0.00340136054421769,
    0.0127551020408163, 0.00340136054421769, 0.00722789115646259,
    0.00467687074829932, 0.00340136054421769, 0.00340136054421769,
    0.00510204081632653, 0.00510204081632653, 0.00340136054421769,
    0.0038265306122449, 0.00467687074829932, 0.0038265306122449,
    0.00595238095238095, 0.00340136054421769, 0.00340136054421769,
    0.00977891156462585, 0.00425170068027211, 0.00510204081632653,
    0.00595238095238095, 0.00850340136054422, 0.00340136054421769,
    0.00340136054421769, 0.00892857142857143, 0.0038265306122449,
    0.00467687074829932)), class = c("tbl_df", "tbl", "data.frame"
), row.names = c(NA, -27L))



William H. Poling Ph.D., MPH  | Senior Data Scientist, Medicare Stars, CVS Health
p 813-777-5030

From: Eric Berger <ericjberger at gmail.com>
Sent: Tuesday, May 12, 2020 2:07 PM
To: Poling, William <PolingW at aetna.com>
Cc: r-help at r-project.org
Subject: [EXTERNAL] Re: Re: [R] Help with Kmeans output and using broom to tidy etc..

**** External Email - Use Caution ****
Please use dput()



On Tue, May 12, 2020 at 7:11 PM Poling, William <PolingW at aetna.com<mailto:PolingW at aetna.com>> wrote:
Hello Eric, thank you so much for your consideration.

Here are snippets of data that I hope will be helpful

WHP

geo1a <- geo1[, c(2:5)] <-- eliminating ID which is not useful for my purposes anyway

#This is for R-Help use
geo1a <- geo1a %>% top_n(25)

state           city latitude longitude
1     ME      FAIRFIELD 44.64485 -69.65948
2     ME      JONESPORT 44.57935 -67.56743
3     ME        CASWELL 46.97529 -67.83023
4     ME      ELLSWORTH 44.52916 -68.38717
5     ME     VASSALBORO 44.45095 -69.60629
6     ME          UNION 44.20059 -69.26123
7     ME        PALERMO 44.45142 -69.41115
8     ME          ORONO 44.87426 -68.68327
9     ME    SANGERVILLE 45.10138 -69.33580
10    ME      ISLESBORO 44.29015 -68.90812
11    ME        TOPSHAM 43.93600 -69.96565
12    ME       FREEPORT 43.84089 -70.11160
13    ME      SKOWHEGAN 44.76687 -69.71644
14    ME    MILLINOCKET 45.65501 -68.70261
15    ME      ORRINGTON 44.72417 -68.74026
16    ME     ST. GEORGE 43.96726 -69.20827
17    ME FORT FAIRFIELD 46.80911 -67.88079
18    ME      MARS HILL 46.56580 -67.89006
19    ME       FREEPORT 43.85302 -70.03726
20    ME         EASTON 46.64143 -67.91203
21    ME     WATERVILLE 44.53621 -69.65913
22    ME      BRUNSWICK 43.87771 -69.96297
23    ME      BRUNSWICK 43.91719 -69.89905
24    ME      BUCKSPORT 44.60665 -68.81892
25    ME        FAYETTE 44.46380 -70.12047


trnd1_tbla <- trnd1_tbl %>% top_n(25)
print(trnd1_tbla)
head(trnd1_tbla,n=25)

A tibble: 25 x 5
   city      state Basecountsum Basecount2 prop_of_total
   <fct>     <fct>        <dbl>      <dbl>         <dbl>
 1 ATLANTA   GA            2352         12       0.00510
 2 BRADENTON FL            2352          8       0.00340
 3 BROOKLYN  NY            2352         30       0.0128
 4 CHARLOTTE NC            2352          8       0.00340
 5 CHICAGO   IL            2352         17       0.00723
 6 COLUMBUS  OH            2352         11       0.00468
 7 CUMMING   GA            2352          8       0.00340
 8 DALLAS    TX            2352          8       0.00340
 9 ERIE      PA            2352         12       0.00510
10 HOUSTON   TX            2352         12       0.00510
# ... with 15 more rows

WHP

From: Eric Berger <ericjberger at gmail.com<mailto:ericjberger at gmail.com>>
Sent: Tuesday, May 12, 2020 8:39 AM
To: Poling, William <PolingW at aetna.com<mailto:PolingW at aetna.com>>
Cc: r-help at r-project.org<mailto:r-help at r-project.org>
Subject: [EXTERNAL] Re: [R] Help with Kmeans output and using broom to tidy etc..

**** External Email - Use Caution ****
Can you create a reproducible example?
Your question involves objects that are unknown to us. (geo1, trnd1_tbl)

On Tue, May 12, 2020 at 2:41 PM Poling, William via R-help <mailto:r-help at r-project.org<mailto:r-help at r-project.org>> wrote:
#RStudio Version Version 1.2.1335 need this one--> 1.2.5019
sessionInfo()
# R version 4.0.0 Patched (2020-05-03 r78349)
#Platform: x86_64-w64-mingw32/x64 (64-bit)
#Running under: Windows 10 x64 (build 17763)

Hello:

I have data that I am trying to manipulate for Kmeans clustering.

Original data looks like this

str(geo1)
# 'data.frame': 2352 obs. of  5 variables:
# $ ID: Factor w/ 2352 levels "101040199600",..: 590 908 976 509 1674 690 1336 86 726 1702 ...
# $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 32 10 25 11 9 32 13 31 12 12 ...
# $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 932 156 230 698 965 1330 515 727 1127 1304 ...
# $ latitude        : num  40.4 31.2 40.8 42.1 26.8 ...
# $ longitude       : num  -79.9 -81.5 -74 -91.6 -82.1 ...

I created a subset adding column prop_of_total
str(trnd1_tbl)
tibble [1,457 x 5] (S3: tbl_df/tbl/data.frame)
 $ city         : Factor w/ 1337 levels "ABBOTTSTOWN",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ state        : Factor w/ 41 levels "AL","AR","AZ",..: 32 36 10 28 12 36 10 11 26 38 ...
 $ Basecountsum : num [1:1457] 2352 2352 2352 2352 2352 ...
 $ Basecount2   : num [1:1457] 1 1 1 1 1 2 1 1 2 1 ...
 $ prop_of_total: num [1:1457] 0.000425 0.000425 0.000425 0.000425 0.000425 ...


Then I spread it

trnd2_tbl <- trnd1_tbl %>%
    dplyr::select(city, state, prop_of_total) %>%
    spread(key = city, value = prop_of_total, fill = 0) #remove the NA's with fill

str(trnd2_tbl)#tibble [41 x 1,338] (S3: tbl_df/tbl/data.frame)

Then I run a Kmeans

kmeans_obj1 <- trnd2_tbl  %>%
  dplyr::select(- state) %>%
  kmeans(centers = 20, nstart = 100)

str(kmeans_obj1)
List of 9
 $ cluster     : int [1:41] 11 11 9 11 11 4 11 11 16 2 ...
 $ centers     : num [1:20, 1:1337] 0 0 0 0 0 0 0 0 0 0 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:20] "1" "2" "3" "4" ...
  .. ..$ : chr [1:1337] "ABBOTTSTOWN" "ABILENE" "ACWORTH" "ADAMS" ...
 $ totss       : num 0.00158
 $ withinss    : num [1:20] 0 0 0 0 0 0 0 0 0 0 ...
 $ tot.withinss: num 0.0000848
 $ betweenss   : num 0.0015
 $ size        : int [1:20] 1 1 1 1 1 1 1 1 1 1 ...
 $ iter        : int 3
 $ ifault      : int 0
 - attr(*, "class")= chr "kmeans"

Then I go and try to tidy:

#Tidy, glance, augment
#Just makes it easier to use or view the obj's in the obj list

  broom::tidy(kmeans_obj1) %>% glimpse()

        broom::glance(kmeans_obj1)
##A tibble: 1 x 4
# totss tot.withinss betweenss  iter
# <dbl>        <dbl>     <dbl> <int>
#   1 0.00158    0.0000848   0.00150     3

However, when I run this piece I get an error:

broom::augment(kmeans_obj1, trnd2_tbl) %>%
  dplyr::select(city, .cluster)

#Error: Must subset columns with a valid subscript vector.
# The subscript has the wrong type `data.frame<
 # u: double
#  x: double
>`.
i It must be numeric or character.

Here is the back trace:

rlang::last_error()

# Backtrace:
#   1. broom::augment(kmeans_obj1, trnd2_tbl)
# 9. dplyr::select(., city, .cluster)
# 11. tidyselect::vars_select(tbl_vars(.data), !!!enquos(...))
# 12. tidyselect:::eval_select_impl(...)
# 20. tidyselect:::vars_select_eval(...)
# 21. tidyselect:::walk_data_tree(expr, data_mask, context_mask)
# 22. tidyselect:::eval_c(expr, data_mask, context_mask)
# 23. tidyselect:::reduce_sels(node, data_mask, context_mask, init = init)
# 24. tidyselect:::walk_data_tree(new, data_mask, context_mask)
# 25. tidyselect:::as_indices_sel_impl(...)
# 26. tidyselect:::as_indices_impl(x, vars, strict = strict)
# 27. vctrs::vec_as_subscript(x, logical = "error")

I am not sure what I am supposed to fix?

Maybe someone has had similar error and can advise me please?

Thank you.

WHP







Proprietary

NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}

______________________________________________
mailto:R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sMhCVDVDKajwJ9te2qVsWXQ2aq4kAe7150EICM51Pw4&s=eSV6ISkAsnmonaRvNdtmx4Lr9vumgXwMYF87DoRP86s&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sMhCVDVDKajwJ9te2qVsWXQ2aq4kAe7150EICM51Pw4&s=8wmXM73ofNcrn1i9gF-qxOzj7zRJZSPcaA5qg0vggG4&e=
and provide commented, minimal, self-contained, reproducible code.

Proprietary

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna


Proprietary

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Wed May 13 00:09:56 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 13 May 2020 08:09:56 +1000
Subject: [R] My dream ...
In-Reply-To: <CAFEqCdwbXpD9DpgSEsHdsv0jBqqx0hLpu10ZN4SyejV06do4Jg@mail.gmail.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
 <CAB8pepx-x1yoYWjmJdH727z66smn-eh43P_E4o8R5gqj2aH9Ug@mail.gmail.com>
 <CAFEqCdwbXpD9DpgSEsHdsv0jBqqx0hLpu10ZN4SyejV06do4Jg@mail.gmail.com>
Message-ID: <CA+8X3fVgo0TursqcgWCqmVD1vDF3mAWm0_Ku_CVrMBqiqMp7Xw@mail.gmail.com>

Abby Spurdle:
In my opinion the advantage of computers is not Artificial
Intelligence, but rather Artificial Patience (most AI that I have seen
is really doing a bunch of what I would consider to be boring, really
fast so people don't have to).  Leave the Intelligence to the people.

Abby's response contains a complaint that is often directed at
technical advances. So what if we can devise a way to perform some
boring task rapidly? I answer that it allows us to delegate the boring
task to the machine and proceed with the integration of the results.
We run the risk of Douglas Adams' delightful result that we cannot
understand, but nearly all of the "big" scientific endeavors stand
upon the shoulders of machines doing boring tasks whose duration at
human speed would see us all out. My idea of AI is a sort of teamwork
between the error-prone synthesis of man and the precise analysis of
machine, not a struggle for dominance  of one or the other.

Jim


From you@r|@|@nou@ @end|ng |rom gm@||@com  Wed May 13 01:37:32 2020
From: you@r|@|@nou@ @end|ng |rom gm@||@com (Yousri Fanous)
Date: Tue, 12 May 2020 19:37:32 -0400
Subject: [R] How to write the segmented equation from MARS coefficients
Message-ID: <CADsEwScSk0vqd90rMsk4MaUq2TrN6jRwo9z=xSL_dzThAHx=GQ@mail.gmail.com>

Hello

I created a MARS model from earth package.for one predictor EngDispl vs FE.
These are the resulting coefficients:

 summary(marsFit)$coefficients
                        y
(Intercept)     18.049001
h(EngDispl-4.3) -4.378082
h(4.3-EngDispl) 10.925240
h(EngDispl-2.3)  5.942387
h(EngDispl-3.5) -2.552332

I am at a loss at writing the equation linking the dependent and
independent variables for such an output.
What is h? Is it a function / operator?

Thank you for your help

Yousri fanous
IBM Canada
Software developer


From @purd|e@@ @end|ng |rom gm@||@com  Wed May 13 02:13:09 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Wed, 13 May 2020 12:13:09 +1200
Subject: [R] My dream ...
In-Reply-To: <CA+8X3fVgo0TursqcgWCqmVD1vDF3mAWm0_Ku_CVrMBqiqMp7Xw@mail.gmail.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
 <CAB8pepx-x1yoYWjmJdH727z66smn-eh43P_E4o8R5gqj2aH9Ug@mail.gmail.com>
 <CAFEqCdwbXpD9DpgSEsHdsv0jBqqx0hLpu10ZN4SyejV06do4Jg@mail.gmail.com>
 <CA+8X3fVgo0TursqcgWCqmVD1vDF3mAWm0_Ku_CVrMBqiqMp7Xw@mail.gmail.com>
Message-ID: <CAB8pepytv-Atm4YVokWS_NQzERqDyvJTxE7fHgvJRisVD9rdog@mail.gmail.com>

Hi Jim,

I think you've mis-quoted me.
I didn't say that.


On Wed, May 13, 2020 at 10:10 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Abby Spurdle:
> In my opinion the advantage of computers is not Artificial
> Intelligence, but rather Artificial Patience (most AI that I have seen
> is really doing a bunch of what I would consider to be boring, really
> fast so people don't have to).  Leave the Intelligence to the people.
>
> Abby's response contains a complaint that is often directed at
> technical advances. So what if we can devise a way to perform some
> boring task rapidly? I answer that it allows us to delegate the boring
> task to the machine and proceed with the integration of the results.
> We run the risk of Douglas Adams' delightful result that we cannot
> understand, but nearly all of the "big" scientific endeavors stand
> upon the shoulders of machines doing boring tasks whose duration at
> human speed would see us all out. My idea of AI is a sort of teamwork
> between the error-prone synthesis of man and the precise analysis of
> machine, not a struggle for dominance  of one or the other.
>
> Jim
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drj|m|emon @end|ng |rom gm@||@com  Wed May 13 02:23:49 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 13 May 2020 10:23:49 +1000
Subject: [R] My dream ...
In-Reply-To: <CAB8pepytv-Atm4YVokWS_NQzERqDyvJTxE7fHgvJRisVD9rdog@mail.gmail.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
 <CAB8pepx-x1yoYWjmJdH727z66smn-eh43P_E4o8R5gqj2aH9Ug@mail.gmail.com>
 <CAFEqCdwbXpD9DpgSEsHdsv0jBqqx0hLpu10ZN4SyejV06do4Jg@mail.gmail.com>
 <CA+8X3fVgo0TursqcgWCqmVD1vDF3mAWm0_Ku_CVrMBqiqMp7Xw@mail.gmail.com>
 <CAB8pepytv-Atm4YVokWS_NQzERqDyvJTxE7fHgvJRisVD9rdog@mail.gmail.com>
Message-ID: <CA+8X3fUGA+7YbJsUij2eUb_hPLOQXAezJ5apMwwmgdNUzR8AHg@mail.gmail.com>

Sorry, it was listed in Hans' email as a reply from you. Far be it
from me to speak for someone else.

Jim

On Wed, May 13, 2020 at 10:13 AM Abby Spurdle <spurdle.a at gmail.com> wrote:
>
> Hi Jim,
>
> I think you've mis-quoted me.
> I didn't say that.
>
>
> On Wed, May 13, 2020 at 10:10 AM Jim Lemon <drjimlemon at gmail.com> wrote:
> >
> > Abby Spurdle:
> > In my opinion the advantage of computers is not Artificial
> > Intelligence, but rather Artificial Patience (most AI that I have seen
> > is really doing a bunch of what I would consider to be boring, really
> > fast so people don't have to).  Leave the Intelligence to the people.
> >
> > Abby's response contains a complaint that is often directed at
> > technical advances. So what if we can devise a way to perform some
> > boring task rapidly? I answer that it allows us to delegate the boring
> > task to the machine and proceed with the integration of the results.
> > We run the risk of Douglas Adams' delightful result that we cannot
> > understand, but nearly all of the "big" scientific endeavors stand
> > upon the shoulders of machines doing boring tasks whose duration at
> > human speed would see us all out. My idea of AI is a sort of teamwork
> > between the error-prone synthesis of man and the precise analysis of
> > machine, not a struggle for dominance  of one or the other.
> >
> > Jim
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed May 13 02:25:49 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 12 May 2020 17:25:49 -0700
Subject: [R] How to write the segmented equation from MARS coefficients
In-Reply-To: <CADsEwScSk0vqd90rMsk4MaUq2TrN6jRwo9z=xSL_dzThAHx=GQ@mail.gmail.com>
References: <CADsEwScSk0vqd90rMsk4MaUq2TrN6jRwo9z=xSL_dzThAHx=GQ@mail.gmail.com>
Message-ID: <8892166D-3257-4577-8412-C8866887BB74@dcn.davis.ca.us>

Carefully read the description of the package... it uses a particular variety of splines. You should go read the paper they reference.

On May 12, 2020 4:37:32 PM PDT, Yousri Fanous <yousri.fanous at gmail.com> wrote:
>Hello
>
>I created a MARS model from earth package.for one predictor EngDispl vs
>FE.
>These are the resulting coefficients:
>
> summary(marsFit)$coefficients
>                        y
>(Intercept)     18.049001
>h(EngDispl-4.3) -4.378082
>h(4.3-EngDispl) 10.925240
>h(EngDispl-2.3)  5.942387
>h(EngDispl-3.5) -2.552332
>
>I am at a loss at writing the equation linking the dependent and
>independent variables for such an output.
>What is h? Is it a function / operator?
>
>Thank you for your help
>
>Yousri fanous
>IBM Canada
>Software developer
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From Po||ngW @end|ng |rom @etn@@com  Wed May 13 02:56:45 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Wed, 13 May 2020 00:56:45 +0000
Subject: [R] Help with Radius problem
Message-ID: <BYAPR06MB5383B486628866FD2F36917CAEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>

#RStudio Version Version 1.2.1335 
sessionInfo() 
# R version 4.0.0 Patched (2020-05-03 r78349)
#Platform: x86_64-w64-mingw32/x64 (64-bit)
#Running under: Windows 10 x64 (build 17763)

Hello I am trying to figure out how to create concentration of ID's in the following data, based on determining the highest concentration in the smallest radius area based on Longitude and Latitude (geo location)

I have reviewed many websites looking for a function or tutorial, (if you are aware of either please let me know)

I have worked through the following to see if they are close to what I need, but they are not.

#Possible use https://stackoverflow.com/questions/21977720/r-finding-closest-neighboring-point-and-number-of-neighbors-within-a-given-rad

#Possible math ref http://janmatuschek.de/LatitudeLongitudeBoundingCoordinates

#Possible use https://stackoverflow.com/questions/39008850/find-number-of-points-within-a-radius-in-r-using-lon-and-lat-coordinates

# https://gis.stackexchange.com/questions/229453/create-a-circle-of-defined-radius-around-a-point-and-then-find-the-overlapping-a

# https://stackoverflow.com/questions/23071026/drawing-a-circle-with-a-radius-of-a-defined-distance-in-a-map/23072079#23072079

My actual data is 2353 records with 41 states and 1337 cities represented. Here is a sample below.

Some might call this a hotspot question I suppose: "What is the maximum number of ID's I can gather in the smallest radius size given the data at hand?

For example it would be useful to have a function that I could use to test different combinations of radius size and concentration of ID #'s by geo location.

Any insight would be very much appreciated.

Thank you.
WHP

str(sample)
'data.frame':	35 obs. of  5 variables:
 $ state    : Factor w/ 41 levels "AL","AR","AZ",..: 19 29 9 9 10 30 33 35 41 12 ...
 $ city     : Factor w/ 1337 levels "ABBOTTSTOWN",..: 281 1280 129 223 989 721 550 731 1325 688 ...
 $ latitude : num  43.5 40.1 26.5 27.9 31.9 ...
 $ longitude: num  -70.6 -82.9 -80.1 -82.7 -81.4 ...
 $ ID       : int  2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 ...

dput(sample)
structure(list(state = structure(c(19L, 29L, 9L, 9L, 10L, 30L, 
33L, 35L, 41L, 12L, 12L, 36L, 4L, 9L, 8L, 39L, 36L, 36L, 41L, 
28L, 12L, 28L, 28L, 12L, 11L, 2L, 12L, 22L, 32L, 22L, 12L, 10L, 
10L, 22L, 10L), .Label = c("AL", "AR", "AZ", "CA", "CO", "CT", 
"DC", "DE", "FL", "GA", "IA", "IL", "IN", "KS", "KY", "LA", "MA", 
"MD", "ME", "MI", "MN", "MO", "NC", "NE", "NJ", "NM", "NV", "NY", 
"OH", "OK", "OR", "PA", "SC", "SD", "TN", "TX", "UT", "VA", "WA", 
"WI", "WV"), class = "factor"), city = structure(c(281L, 1280L, 
129L, 223L, 989L, 721L, 550L, 731L, 1325L, 688L, 1184L, 281L, 
759L, 1171L, 1305L, 587L, 272L, 581L, 263L, 152L, 217L, 152L, 
390L, 5L, 571L, 1006L, 1162L, 939L, 170L, 1033L, 1002L, 586L, 
586L, 192L, 586L), .Label = c("ABBOTTSTOWN", "ABILENE", "ACWORTH", 
"ADAMS", "ADDISON", "ADKINS", "ALBANY", "ALBIA", "ALBUQUERQUE", 
"ALEXANDRIA", "ALFRED", "ALIQUIPPA", "ALISO VIEJO", "ALLEN PARK", 
"ALLENTOWN", "ALPHA", "ALPHARETTA", "ALPINE", "ALTOONA", "AMARILLO", 
"AMBLER", "AMBRIDGE", "AMHERST", "AMITYVILLE", "ANAHIEM", "ANDERSON", 
"ANDOVER", "ANGIER", "ANGLETON", "ANKENY", "ANN ARBOR", "ANZA", 
"APEX", "APOLLO", "ARCADIA", "ARCHDALE", "ARDMORE", "ARGYLE", 
"ARKANSAS CITY", "ARLINGTON", "ARNOLD", "ARTHUR", "ASHEBORO", 
"ASHEVILLE", "ASHLAND", "ASHLAND CITY", "ASHTON", "ASTON", "ATHENS", 
"ATLANTA", "ATLANTIC BCH", "AUBORN", "AUGUSTA", "AURORA", "AUSTELL", 
"AUSTIN", "AVENTURA", "AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN CITY", 
"BALDWIN PLACE", "BALLWIN", "BANGOR", "BARBOURSVILLE", "BARNEGAT", 
"BARTLETT", "BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", "BATESBURG", 
"BATON ROUGE", "BAXTER SPRINGS", "BAY CITY", "BAYONNE", "BAYSIDE", 
"BAYTOWN", "BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER CITY", 
"BELFAIR", "BELLE VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR", 
"BELLMORE", "BELLWOOD", "BELMAR", "BELMONT", "BELVIDERE", "BENNET", 
"BENTON", "BENTONVILLE", "BERLIN", "BESSEMER CITY", "BETHANY", 
"BETHEL", "BETHEL PARK", "BETHESDA", "BETHLEHEM", "BETTENDORF", 
"BIGLERVILLE", "BINGHAMTON", "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN", 
"BLACKSBURG", "BLAINE", "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD", 
"BLUE BELL", "BLUE GRASS", "BLUE POINT", "BLUE SPRINGS", "BOCA RATON", 
"BODFISH", "BOILING SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS", 
"BONNEY LAKE", "BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING GREEN", 
"BOYERTOWN", "BOYNTON BEACH", "BRADENTON", "BRADENVILLE", "BRANDAMORE", 
"BRANDON", "BRANFORD", "BRANSON", "BRASELTON", "BREEZEWOOD", 
"BRENTON", "BRENTWOOD", "BREWSTER", "BRICK", "BRIDGEPORT", "BRIDGETON", 
"BRIGHAM CITY", "BROADDUS", "BROCKPORT", "BROGUE", "BROKEN ARROW", 
"BRONX", "BROOKFIELD", "BROOKHAVEN", "BROOKLYN", "BROWNS MILLS", 
"BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", "BRYAN", "BUCKSPORT", 
"BUDD LAKE", "BUFFALO", "BULVERDE", "BUNKERVILLE", "BURBANK", 
"BURFORD", "BURIEN", "BURLINGTON", "BURR RIDGE", "BURTON", "BUSKIRK", 
"BUTLER", "BUXTON", "BYRON", "CALERA", "CAMBRIDGE", "CAMP HILL", 
"CAMPBELL", "CANAAN", "CANAL WINCHESTER", "CANASTOTA", "CANTON", 
"CARDIFF BY THE SEA", "CARL JUNCTION", "CARLISLE", "CARLTON", 
"CAROL STREAM", "CARROLLTON", "CARSON CITY", "CARTERSVILLE", 
"CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", "CASWELL", "CATO", 
"CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER POINT", "CENTERTON", 
"CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN FALLS", "CHALFONT", 
"CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", "CHARLESTON", 
"CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", "CHESHIRE", 
"CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", "CHILLICOTHE", 
"CHOCTAW", "CICERO", "CINCINNATI", "CLAY", "CLEARWATER", "CLEARWATER BEACH", 
"CLEMMONS", "CLENDENIN", "CLEVELAND", "CLEVELAND HTS", "CLEVES", 
"CLIFFSIDE PARK", "CLIFTON", "CLIFTON PARK", "CLIFTON SPGS", 
"CLINTON", "COACHELLA", "COAL TOWNSHIP", "COATESVILLE", "COLLEGE STATION", 
"COLLINSVILLE", "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE", 
"CONCORD", "CONCORD TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY", 
"CONYERS", "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA", 
"CORNELIUS", "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY", 
"COTTAGE HILLS", "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN", 
"CRANBERRY TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER", 
"CUMBERLAND FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS", 
"DALLAS", "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT", 
"DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR", 
"DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH", 
"DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY", 
"DES MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER", 
"DIXON", "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER", 
"DOYLESTOWN", "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH", 
"DUNCAN", "DUNCANNON", "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE", 
"DURHAM", "DUTTON", "EAGLE GROVE", "EAST AURORA", "EAST CHICAGO", 
"EAST EARL", "EAST HANOVER", "EAST HARTFORD", "EAST HARTLAND", 
"EAST LEROY", "EAST LIVERPOOL", "EAST MEADOW", "EAST NORTHPORT", 
"EAST ORANGE", "EAST PEORIA", "EAST SAINT LOUIS", "EASTON", "EDGERTON", 
"EDISON", "EDMOND", "EGG HBR TWP", "EL DORADO", "EL PASO", "ELDRIDGE", 
"ELGIN", "ELIZABETHVILLE", "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY", 
"ELMHURST", "ELMWOOD PARK", "ELOY", "ELY", "EMERSON", "EMORY", 
"ENDICOTT", "ENGLEWOOD", "ENID", "ENNICE", "ENOLA", "ENUMCLAW", 
"EPHRATA", "ERIE", "ESTERO", "EUREKA SPRINGS", "EVANSVILLE", 
"EVERETT", "EXCELSIOR SPRINGS", "EXTON", "FAIRFAX", "FAIRFIELD", 
"FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", "FALMOUTH", "FAR HILLS", 
"FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", "FAYETTE", "FAYETTEVILLE", 
"FELTON", "FENTON", "FERGUSON", "FIELDALE", "FINDLAY", "FINGERVILLE", 
"FLEMINGTON", "FLINT", "FLORENCE", "FLORESVILLE", "FLORISSANT", 
"FLOVILLA", "FLOWER MOUND", "FLUSHING", "FOGELSVILLE", "FOLSOM", 
"FONTANELLE", "FOREST HILLS", "FOREST PARK", "FORT DODGE", "FORT FAIRFIELD", 
"FORT GAY", "FORT LAUDERDALE", "FORT LEE", "FORT MILL", "FORT MOHAVE", 
"FORT PAYNE", "FORT PIERCE", "FORT SMITH", "FORT WAYNE", "FORT WORTH", 
"FOSTORIA", "FOUNTAIN INN", "FRANKFORT", "FRANKLIN", "FREEDOM", 
"FREEHOLD", "FREEPORT", "FREMONT", "FULLERTON", "FULSHEAR", "FULTON", 
"GADSDEN", "GAFFNEY", "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE", 
"GARDNERS", "GARNETT VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA", 
"GETTYSBURG", "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE", 
"GLEN CARBON", "GLEN COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW", 
"GLENWOOD", "GOLDEN CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM", 
"GOSHEN", "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS", 
"GRANDVIEW", "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS", 
"GRASS VALLEY", "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER", 
"GREENLAWN", "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE", 
"GREER", "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD", 
"HADLEY", "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON", 
"HAMBURG", "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE", 
"HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON", 
"HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL", 
"HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE", 
"HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH", 
"HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE", 
"HIGH HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS", 
"HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE", 
"HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD", 
"HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS", 
"HOT SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON", 
"HUFFMAN", "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON", 
"HUNTINGTON BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE", 
"HURST", "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL", 
"INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY", 
"IRMO", "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP", 
"ISSAQUAH", "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS", 
"JACKSONVILLE", "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE", 
"JEFFERSON", "JEFFERSON CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN", 
"JONESBORO", "JONESPORT", "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE", 
"KANNAPOLIS", "KANSAS CITY", "KATY", "KEARNEY", "KELLERTON", 
"KEMAH", "KENMORE", "KENNESAW", "KENT", "KINGMAN", "KINGS MOUNTAIN", 
"KINGSLAND", "KINGSTON", "KINGWOOD", "KIRKLAND", "KUTZTOWN", 
"LA QUINTA", "LA RUE", "LA VERNE", "LAFAYETTE", "LAKE BLUFF", 
"LAKE IN THE HILL", "LAKE ISABELLA", "LAKE PLACID", "LAKE SAINT LOUIS", 
"LAKE TAPPS", "LAKE VILLAGE", "LAKE WAUKOMIS", "LAKE WORTH", 
"LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD RCH", "LAMBERTVILLE", 
"LANCASTER", "LANDISVILLE", "LANSDALE", "LAREDO", "LARGO", "LAS VEGAS", 
"LATROBE", "LAUDERHILL", "LAWNDALE", "LAWRENCE", "LAWRENCEVILLE", 
"LE MARS", "LE ROY", "LEAGUE CITY", "LEAVENWORTH", "LEAWOOD", 
"LEBANON", "LECOMPTON", "LEECHBURG", "LEES SUMMIT", "LEESBURG", 
"LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", "LEVITTOWN", "LEWES", 
"LEWISTOWN", "LEXINGTON", "LIBERTY TWP", "LILLINGTON", "LINCOLN", 
"LINDENHURST", "LINTON", "LISBON", "LITCHFIELD PK", "LITHIA", 
"LITITZ", "LITTLE EGG HARBOR TO", "LK FOREST PK", "LK PEEKSKILL", 
"LOCKHART", "LOCKWOOD", "LOGAN", "LOMA LINDA", "LONE JACK", "LONG BEACH", 
"LONG LANE", "LONGVIEW", "LORAIN", "LORTON", "LOS ANGELES", "LOUISBURG", 
"LOVES PARK", "LOWER BURRELL", "LOWRY CITY", "LUBBOCK", "LUGOFF", 
"LUMBERTON", "LYNBROOK", "LYNNWOOD", "MACHESNEY PARK", "MACON", 
"MAGNOLIA", "MAHOPAC", "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE", 
"MANASSAS", "MANCHACA", "MANCHESTER", "MANCHESTER TOWNSHIP", 
"MANCHESTER TW", "MANHEIM", "MANITO", "MANLIUS", "MANNINGTON", 
"MANSFIELD", "MANSURA", "MAPLE PARK", "MARBLE FALLS", "MARCELLUS", 
"MARICOPA", "MARIETTA", "MARION", "MARKLEYSBURG", "MARS HILL", 
"MARSHALLTOWN", "MARSHFIELD", "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE", 
"MASON CITY", "MASSEY", "MASSILLON", "MASURY", "MATAWAN", "MATEWAN", 
"MATTAPOISETT", "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND", 
"MC DONALD", "MC KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN", 
"MCALESTER", "MCDONOUGH", "MCKEESPORT", "MCKINNEY", "MECHANICSBURG", 
"MECHANICSVILLE", "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS", 
"MERIDEN", "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA", 
"MESQUITE", "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN", 
"MIDLAND", "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK", 
"MILLEDGEVILLE", "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE", 
"MINEOLA", "MINERAL SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION", 
"MISSION HILLS", "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY", 
"MOLINE", "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE", 
"MONTCLAIR", "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON", 
"MORGANTOWN", "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE", 
"MOUNT JOY", "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO", 
"MOUNT UNION", "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT", 
"MULVANE", "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT", 
"MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS", 
"NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES", 
"NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG", 
"NEW BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK", 
"NEW CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON", 
"NEW LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE", 
"NEW ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK", 
"NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT", 
"NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA", 
"NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK", 
"NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON", 
"NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND HILLS", 
"NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE", 
"NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON", 
"OAK RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE", 
"OMAHA", "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION", 
"ORONO", "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO", 
"OTTAWA", "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD", 
"OYSTER BAY", "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR", 
"PALMDALE", "PALMYRA", "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE", 
"PARLIN", "PARMA", "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY", 
"PAXTON", "PEA RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN", 
"PELION", "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE", 
"PENNSBURG", "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS", 
"PERRYSBURG", "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI", 
"PHOENIX", "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN", 
"PINE BLUFF", "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY", 
"PITMAN", "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY", 
"PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO LAKE", 
"POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", "PORT CHARLOTTE", 
"PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", "PORT WASHINGTON", 
"PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", "POTTSVILLE", 
"POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE VILLAGE", 
"PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT PLEASANT", 
"PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", "QUINCY", 
"RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", "READLYN", 
"RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO PARK", 
"REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", "RICH HILL", 
"RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", "RINCON", 
"RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", "RIVERSIDE", 
"ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", "ROCKAWAY BCH", 
"ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", "ROGERS", 
"ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE", 
"ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT", 
"ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN", 
"RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO", 
"SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR", 
"SAINT CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS", 
"SAINT PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY", 
"SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO", 
"SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN", 
"SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", "SARASOTA", 
"SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", "SAYVILLE", 
"SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", "SCOTT", "SCOTTSDALE", 
"SEAFORD", "SEALE", "SEATTLE", "SEDONA", "SEGUIN", "SELDEN", 
"SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", "SHARON", "SHARPSBURG", 
"SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", "SHELLSBURG", "SHELTON", 
"SHENANDOAH", "SHORELINE", "SHOREWOOD", "SHREVEPORT", "SICKLERVILLE", 
"SILER CITY", "SILOAM SPRINGS", "SIMON", "SIMPSONVILLE", "SIMSBURY", 
"SIOUX CITY", "SIOUX FALLS", "SKOKIE", "SKOWHEGAN", "SLATINGTON", 
"SLIDELL", "SMITHFIELD", "SN BERNRDNO", "SNELLVILLE", "SOCORRO", 
"SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH BELOIT", "SOUTH BEND", 
"SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH PARK", "SOUTH PLAINFIELD", 
"SOUTH SIOUX CITY", "SOUTHAMPTON", "SOUTHBURY", "SOUTHGATE", 
"SPARKS", "SPARROW BUSH", "SPENCER", "SPRING", "SPRING BRANCH", 
"SPRING GROVE", "SPRING HILL", "SPRING HOPE", "SPRING VALLEY", 
"SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD GARDENS", "SPRINGHILL", 
"SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", "ST PETERSBURG", 
"ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", "STANLEY", 
"STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", "STILLMAN VALLEY", 
"STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", "STRAFFORD", 
"STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", "SUFFIELD", 
"SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", "SUMMIT", 
"SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", "SWEENY", 
"SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", "TAFTVILLE", 
"TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", "TAYLORVILLE", 
"TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", "THOMASVILLE", 
"TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY PARK", 
"TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", "TOPEKA", 
"TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS RST", 
"TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER", 
"TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY", 
"UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA", 
"URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO", 
"VAN BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO", 
"VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA", 
"VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH", 
"W TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN", 
"WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG", 
"WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA", 
"WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE", 
"WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER", 
"WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO", 
"WEST CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN", 
"WEST HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH", 
"WEST POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD", 
"WEST VALLEY CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER", 
"WESTMORELAND CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD", 
"WHARTON", "WHEELING", "WHITE PLAINS", "WHITEHALL", "WHITESTONE", 
"WHITESTOWN", "WHITING", "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD", 
"WILLARD", "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK", 
"WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS", 
"WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON", 
"WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE", 
"WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS", 
"WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH", 
"YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS", 
"YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor"), 
    latitude = c(43.53462, 40.142492, 26.538149, 27.9062, 31.90104, 
    34.833083, 35.118836, 35.136047, 39.425712, 40.519207, 40.101126, 
    30.044457, 33.62287, 27.910276, 39.752254, 47.403778, 32.961929, 
    29.820199, 38.408649, 40.668828, 41.983948, 40.58116, 40.7253, 
    41.921667, 41.669921, 36.442552, 38.564663, 37.455315, 40.867774, 
    38.783554, 42.244381, 34.024332, 34.001644, 36.622704, 34.017832
    ), longitude = c(-70.58809, -82.888789, -80.113071, -82.6661, 
    -81.372992, -95.840964, -82.016013, -89.92625, -80.276676, 
    -89.778385, -87.641063, -94.888036, -117.663119, -82.530568, 
    -75.574437, -122.209047, -96.799309, -95.746255, -80.553953, 
    -73.923314, -87.713482, -73.9731, -73.8187, -87.987023, -93.759478, 
    -94.110903, -90.00597, -93.257109, -79.932483, -90.201858, 
    -89.100233, -84.625923, -84.567614, -93.912921, -84.612564
    ), ID = 2318:2352), row.names = c(NA, -35L), class = "data.frame")


Proprietary

NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}


From |e@gr@|n@ @end|ng |rom gm@||@com  Wed May 13 03:01:59 2020
From: |e@gr@|n@ @end|ng |rom gm@||@com (Adrien FABRE)
Date: Wed, 13 May 2020 03:01:59 +0200
Subject: [R] Sometimes commands do not terminate after upgrading to R 4.0
 and Ubuntu 20.04
Message-ID: <CAKPvFjA6Nfv=H5cFmamCOgXKmrY8ZHniECwvKUqr78xGmSUY_w@mail.gmail.com>

I have upgraded R (from 3.6 to 4.0) and RStudio (from 1.1 to 1.2.5) a few
days ago, and Ubuntu from 18.04 to 20.04 yesterday.

Since then, R sometimes never terminates when executing certain commands:
ivreg (from package AER), summary (of a logit regression) and logitmfx
(from package mfx). Sometimes these commands run fine, but most of the time
I have to kill the process because R won't terminate the execution, even
when pressing the red Stop button in RStudio.

When I tried example('AER'), it worked fine. Then I re-installed the
package AER. It threw 10 warnings of type In readLines(file, skipNul =
TRUE) :  cannot open compressed file
'/usr/lib/R/site-library/[package]/DESCRIPTION', probable reason 'No such
file or directory' where [package] is abind, colorspace, dichromat... (but
not AER).

Since then example('AER') throws a warning: no help found for ?AER?.

I've removed and reinstalled R 4.0: it didn't help. Besides, the apt purge
r-base* r-recommended r-cran-* threw a warning: dpkg: warning: while
removing r-base-core, directory '/usr/lib/R/site-library' not empty so not
removed. Also, there was a bunch of Package [package] is not installed, so
not removed, including for [package] equal to r-cran-abind and the other
listed above (this purge also returned a bunch of Note, selecting [package]
for glob 'r-cran-*').

I have the same bug when using R from the terminal. For the record, I was
probably working on RStudio during the upgrade to Ubuntu 20.04. Also, I
can't recall if this issue started after I upgraded R and RStudio (which
would be my best guess) or after I upgraded Ubuntu (a day or two later).

I hope someone can help.

-- 
Adrien Fabre

?cole d'?conomie de Paris/Paris School of Economics ? Universit? Paris 1
(R4-47)

Page personnelle/Home page <http://sites.google.com/view/adrien-fabre>
(+33/0)6.10.37.90.51

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Wed May 13 03:37:26 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 12 May 2020 18:37:26 -0700
Subject: [R] 
 Sometimes commands do not terminate after upgrading to R 4.0
 and Ubuntu 20.04
In-Reply-To: <CAKPvFjA6Nfv=H5cFmamCOgXKmrY8ZHniECwvKUqr78xGmSUY_w@mail.gmail.com>
References: <CAKPvFjA6Nfv=H5cFmamCOgXKmrY8ZHniECwvKUqr78xGmSUY_w@mail.gmail.com>
Message-ID: <CAGxFJbQpGHTPXkt1O1ti9vmO4mamJBoHdWFW1oxZvs0NKRuVJA@mail.gmail.com>

Please, please, please ... RStudio is a wholly separate product from
R. Post on their website if you think Rstudio has problems. And ubuntu
concerns should be posted on r-sig-debian, not here! This list is
about R programming issues (mostly).

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Tue, May 12, 2020 at 6:07 PM Adrien FABRE <lesgrains at gmail.com> wrote:
>
> I have upgraded R (from 3.6 to 4.0) and RStudio (from 1.1 to 1.2.5) a few
> days ago, and Ubuntu from 18.04 to 20.04 yesterday.
>
> Since then, R sometimes never terminates when executing certain commands:
> ivreg (from package AER), summary (of a logit regression) and logitmfx
> (from package mfx). Sometimes these commands run fine, but most of the time
> I have to kill the process because R won't terminate the execution, even
> when pressing the red Stop button in RStudio.
>
> When I tried example('AER'), it worked fine. Then I re-installed the
> package AER. It threw 10 warnings of type In readLines(file, skipNul =
> TRUE) :  cannot open compressed file
> '/usr/lib/R/site-library/[package]/DESCRIPTION', probable reason 'No such
> file or directory' where [package] is abind, colorspace, dichromat... (but
> not AER).
>
> Since then example('AER') throws a warning: no help found for ?AER?.
>
> I've removed and reinstalled R 4.0: it didn't help. Besides, the apt purge
> r-base* r-recommended r-cran-* threw a warning: dpkg: warning: while
> removing r-base-core, directory '/usr/lib/R/site-library' not empty so not
> removed. Also, there was a bunch of Package [package] is not installed, so
> not removed, including for [package] equal to r-cran-abind and the other
> listed above (this purge also returned a bunch of Note, selecting [package]
> for glob 'r-cran-*').
>
> I have the same bug when using R from the terminal. For the record, I was
> probably working on RStudio during the upgrade to Ubuntu 20.04. Also, I
> can't recall if this issue started after I upgraded R and RStudio (which
> would be my best guess) or after I upgraded Ubuntu (a day or two later).
>
> I hope someone can help.
>
> --
> Adrien Fabre
>
> ?cole d'?conomie de Paris/Paris School of Economics ? Universit? Paris 1
> (R4-47)
>
> Page personnelle/Home page <http://sites.google.com/view/adrien-fabre>
> (+33/0)6.10.37.90.51
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From er|cjberger @end|ng |rom gm@||@com  Wed May 13 07:17:21 2020
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Wed, 13 May 2020 08:17:21 +0300
Subject: [R] 
 Sometimes commands do not terminate after upgrading to R 4.0
 and Ubuntu 20.04
In-Reply-To: <CAGxFJbQpGHTPXkt1O1ti9vmO4mamJBoHdWFW1oxZvs0NKRuVJA@mail.gmail.com>
References: <CAKPvFjA6Nfv=H5cFmamCOgXKmrY8ZHniECwvKUqr78xGmSUY_w@mail.gmail.com>
 <CAGxFJbQpGHTPXkt1O1ti9vmO4mamJBoHdWFW1oxZvs0NKRuVJA@mail.gmail.com>
Message-ID: <CAGgJW75yA+Kpb29a303bV4ZiOHPGXT8RynNBzcfJspqCr_7rcQ@mail.gmail.com>

Adrien,
you posted this same item 3 days ago to this list. And someone responded to
it.
Why are you posting the identical thing again to this list?
As Bert writes, you should post to the r-sig-debian list.

Eric


On Wed, May 13, 2020 at 4:38 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Please, please, please ... RStudio is a wholly separate product from
> R. Post on their website if you think Rstudio has problems. And ubuntu
> concerns should be posted on r-sig-debian, not here! This list is
> about R programming issues (mostly).
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
> On Tue, May 12, 2020 at 6:07 PM Adrien FABRE <lesgrains at gmail.com> wrote:
> >
> > I have upgraded R (from 3.6 to 4.0) and RStudio (from 1.1 to 1.2.5) a few
> > days ago, and Ubuntu from 18.04 to 20.04 yesterday.
> >
> > Since then, R sometimes never terminates when executing certain commands:
> > ivreg (from package AER), summary (of a logit regression) and logitmfx
> > (from package mfx). Sometimes these commands run fine, but most of the
> time
> > I have to kill the process because R won't terminate the execution, even
> > when pressing the red Stop button in RStudio.
> >
> > When I tried example('AER'), it worked fine. Then I re-installed the
> > package AER. It threw 10 warnings of type In readLines(file, skipNul =
> > TRUE) :  cannot open compressed file
> > '/usr/lib/R/site-library/[package]/DESCRIPTION', probable reason 'No such
> > file or directory' where [package] is abind, colorspace, dichromat...
> (but
> > not AER).
> >
> > Since then example('AER') throws a warning: no help found for ?AER?.
> >
> > I've removed and reinstalled R 4.0: it didn't help. Besides, the apt
> purge
> > r-base* r-recommended r-cran-* threw a warning: dpkg: warning: while
> > removing r-base-core, directory '/usr/lib/R/site-library' not empty so
> not
> > removed. Also, there was a bunch of Package [package] is not installed,
> so
> > not removed, including for [package] equal to r-cran-abind and the other
> > listed above (this purge also returned a bunch of Note, selecting
> [package]
> > for glob 'r-cran-*').
> >
> > I have the same bug when using R from the terminal. For the record, I was
> > probably working on RStudio during the upgrade to Ubuntu 20.04. Also, I
> > can't recall if this issue started after I upgraded R and RStudio (which
> > would be my best guess) or after I upgraded Ubuntu (a day or two later).
> >
> > I hope someone can help.
> >
> > --
> > Adrien Fabre
> >
> > ?cole d'?conomie de Paris/Paris School of Economics ? Universit? Paris 1
> > (R4-47)
> >
> > Page personnelle/Home page <http://sites.google.com/view/adrien-fabre>
> > (+33/0)6.10.37.90.51
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Wed May 13 08:38:47 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 13 May 2020 16:38:47 +1000
Subject: [R] Help with Radius problem
In-Reply-To: <BYAPR06MB5383B486628866FD2F36917CAEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB5383B486628866FD2F36917CAEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <CA+8X3fW6kLo-f3E32uUZMO5=93GMtjNixSpuiX4qo50_+E2_pw@mail.gmail.com>

Hi Bill,
A while ago I devised a couple of functions to accumulate millions of
geographic locations of events and then display the resulting matrix
of values on an existing plot. This may be of use to you, at least in
the visualization of the density of the locations. As your example
data only included a few points, the resulting plot looks pretty
chunky as I had to use a large symbol to make the cells with non-zero
counts obvious.

# read in your sample data
source("wp_data.R")
library(plotrix)
geomat<-makeDensityMatrix(geodat[,c("latitude","longitude")],
xlim=range(geodat$longitude),ylim=range(geodat$latitude))
# Range of density (>0) - 1.119816 3.922018
latlim<-range(geodat$latitude)
lonlim<-range(geodat$longitude)
library(maps)
map("world",xlim=lonlim,ylim=latlim)
axis(1)
axis(2)
densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
 red=c(0.5,1),green=0,blue=0,pch=15)

Jim

On Wed, May 13, 2020 at 10:57 AM Poling, William via R-help
<r-help at r-project.org> wrote:
>
> #RStudio Version Version 1.2.1335
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit)
> #Running under: Windows 10 x64 (build 17763)
>
> Hello I am trying to figure out how to create concentration of ID's in the following data, based on determining the highest concentration in the smallest radius area based on Longitude and Latitude (geo location)
>
> I have reviewed many websites looking for a function or tutorial, (if you are aware of either please let me know)
>


From pd@|gd @end|ng |rom gm@||@com  Wed May 13 08:39:09 2020
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Wed, 13 May 2020 08:39:09 +0200
Subject: [R] My dream ...
In-Reply-To: <CA+8X3fUGA+7YbJsUij2eUb_hPLOQXAezJ5apMwwmgdNUzR8AHg@mail.gmail.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
 <CAB8pepx-x1yoYWjmJdH727z66smn-eh43P_E4o8R5gqj2aH9Ug@mail.gmail.com>
 <CAFEqCdwbXpD9DpgSEsHdsv0jBqqx0hLpu10ZN4SyejV06do4Jg@mail.gmail.com>
 <CA+8X3fVgo0TursqcgWCqmVD1vDF3mAWm0_Ku_CVrMBqiqMp7Xw@mail.gmail.com>
 <CAB8pepytv-Atm4YVokWS_NQzERqDyvJTxE7fHgvJRisVD9rdog@mail.gmail.com>
 <CA+8X3fUGA+7YbJsUij2eUb_hPLOQXAezJ5apMwwmgdNUzR8AHg@mail.gmail.com>
Message-ID: <7B8F6D09-4866-4CD8-B9D8-DC3DCB7A8031@gmail.com>

Hans? Try Heinz ;-)

Actually listed as a quote _in_ Abby's, originally by Greg Snow, but w/o attribution...

-pd  



> On 13 May 2020, at 02:23 , Jim Lemon <drjimlemon at gmail.com> wrote:
> 
> Sorry, it was listed in Hans' email as a reply from you. Far be it
> from me to speak for someone else.
> 
> Jim
> 
> On Wed, May 13, 2020 at 10:13 AM Abby Spurdle <spurdle.a at gmail.com> wrote:
>> 
>> Hi Jim,
>> 
>> I think you've mis-quoted me.
>> I didn't say that.
>> 
>> 
>> On Wed, May 13, 2020 at 10:10 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>>> 
>>> Abby Spurdle:
>>> In my opinion the advantage of computers is not Artificial
>>> Intelligence, but rather Artificial Patience (most AI that I have seen
>>> is really doing a bunch of what I would consider to be boring, really
>>> fast so people don't have to).  Leave the Intelligence to the people.
>>> 
>>> Abby's response contains a complaint that is often directed at
>>> technical advances. So what if we can devise a way to perform some
>>> boring task rapidly? I answer that it allows us to delegate the boring
>>> task to the machine and proceed with the integration of the results.
>>> We run the risk of Douglas Adams' delightful result that we cannot
>>> understand, but nearly all of the "big" scientific endeavors stand
>>> upon the shoulders of machines doing boring tasks whose duration at
>>> human speed would see us all out. My idea of AI is a sort of teamwork
>>> between the error-prone synthesis of man and the precise analysis of
>>> machine, not a struggle for dominance  of one or the other.
>>> 
>>> Jim
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From drj|m|emon @end|ng |rom gm@||@com  Wed May 13 08:40:49 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 13 May 2020 16:40:49 +1000
Subject: [R] My dream ...
In-Reply-To: <7B8F6D09-4866-4CD8-B9D8-DC3DCB7A8031@gmail.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
 <CAB8pepx-x1yoYWjmJdH727z66smn-eh43P_E4o8R5gqj2aH9Ug@mail.gmail.com>
 <CAFEqCdwbXpD9DpgSEsHdsv0jBqqx0hLpu10ZN4SyejV06do4Jg@mail.gmail.com>
 <CA+8X3fVgo0TursqcgWCqmVD1vDF3mAWm0_Ku_CVrMBqiqMp7Xw@mail.gmail.com>
 <CAB8pepytv-Atm4YVokWS_NQzERqDyvJTxE7fHgvJRisVD9rdog@mail.gmail.com>
 <CA+8X3fUGA+7YbJsUij2eUb_hPLOQXAezJ5apMwwmgdNUzR8AHg@mail.gmail.com>
 <7B8F6D09-4866-4CD8-B9D8-DC3DCB7A8031@gmail.com>
Message-ID: <CA+8X3fW99FDpYgwhs=AobDyXjbnhaPWeY5S69TZMa2qHfEwjTw@mail.gmail.com>

Well, let's hope that was my big screw up for today...

On Wed, May 13, 2020 at 4:39 PM peter dalgaard <pdalgd at gmail.com> wrote:
>
> Hans? Try Heinz ;-)
>
> Actually listed as a quote _in_ Abby's, originally by Greg Snow, but w/o attribution...
>
> -pd
>
>
>
> > On 13 May 2020, at 02:23 , Jim Lemon <drjimlemon at gmail.com> wrote:
> >
> > Sorry, it was listed in Hans' email as a reply from you. Far be it
> > from me to speak for someone else.
> >
> > Jim
> >
> > On Wed, May 13, 2020 at 10:13 AM Abby Spurdle <spurdle.a at gmail.com> wrote:
> >>
> >> Hi Jim,
> >>
> >> I think you've mis-quoted me.
> >> I didn't say that.
> >>
> >>
> >> On Wed, May 13, 2020 at 10:10 AM Jim Lemon <drjimlemon at gmail.com> wrote:
> >>>
> >>> Abby Spurdle:
> >>> In my opinion the advantage of computers is not Artificial
> >>> Intelligence, but rather Artificial Patience (most AI that I have seen
> >>> is really doing a bunch of what I would consider to be boring, really
> >>> fast so people don't have to).  Leave the Intelligence to the people.
> >>>
> >>> Abby's response contains a complaint that is often directed at
> >>> technical advances. So what if we can devise a way to perform some
> >>> boring task rapidly? I answer that it allows us to delegate the boring
> >>> task to the machine and proceed with the integration of the results.
> >>> We run the risk of Douglas Adams' delightful result that we cannot
> >>> understand, but nearly all of the "big" scientific endeavors stand
> >>> upon the shoulders of machines doing boring tasks whose duration at
> >>> human speed would see us all out. My idea of AI is a sort of teamwork
> >>> between the error-prone synthesis of man and the precise analysis of
> >>> machine, not a struggle for dominance  of one or the other.
> >>>
> >>> Jim
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>


From drj|m|emon @end|ng |rom gm@||@com  Wed May 13 11:01:18 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 13 May 2020 19:01:18 +1000
Subject: [R] Classification of wind events
In-Reply-To: <8B435C9568170B469AE31E8891E8CC4F809B8DEA@ESINO.regionemarche.intra>
References: <8B435C9568170B469AE31E8891E8CC4F809B8DEA@ESINO.regionemarche.intra>
Message-ID: <CA+8X3fUsXA7reESuWpGC2jBGRRryF6RcYa5jZxMWV7oktF1kFg@mail.gmail.com>

Hi Stefano,
Given only one observation point you will find it difficult. If your
automatic weather station is in the low area where the foehn wind is
felt, it can only be distinguished from a dry katabatic wind if the
upwind conditions are known. There is a similar but milder version of
this in eastern Australia, but it is usually of the latter sort. There
may be a way to measure turbulence above the peak of the high ground
with radar or something, but I'm not familiar with that.

Jim

On Tue, May 12, 2020 at 6:13 PM Stefano Sofia
<stefano.sofia at regione.marche.it> wrote:
>
> Dear R list users,
> I am aware that this question is not strictly related, at the present moment, to R code and it is more general. Please forgive me, but I need to share my thoughts with you.
>
> Foehn conditions on the southern slope of Alps happen with strong northerly flows that impact perpendicularly over the Apls. This situation triggers strong northerly leeward winds.
> Given a single automatic weather station, I would like to identify these periods starting from wind direction and wind intensity data. Frequency of data is quarter of hour.
> I would really find difficult to detect the moving windows of these events:
> - I can't analyse data day by day;
> - at the beginning and at the end of each event, when the process is not at full speed yet, the rotation is not always perfectly identifiable;
> - I cannot claim in principle that the direction of each consecutive observation is costantly and strictly from the chosen direction.
>
> Does anybody have a clue on how to start to build this process in the right way?
>
> Thank you for your attention and your help
> Stefano
>
>          (oo)
> --oOO--( )--OOo----------------
> Stefano Sofia PhD
> Civil Protection - Marche Region
> Meteo Section
> Snow Section
> Via del Colle Ameno 5
> 60126 Torrette di Ancona, Ancona
> Uff: 071 806 7743
> E-mail: stefano.sofia at regione.marche.it
> ---Oo---------oO----------------
>
> ________________________________
>
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
>
> --
> Questo messaggio  stato analizzato da Libra ESVA ed  risultato non infetto.
> This message was scanned by Libra ESVA and is believed to be clean.
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bog@@o@chr|@to|er @end|ng |rom gm@||@com  Wed May 13 11:26:24 2020
From: bog@@o@chr|@to|er @end|ng |rom gm@||@com (Christofer Bogaso)
Date: Wed, 13 May 2020 14:56:24 +0530
Subject: [R] Fitting Richards' curve
Message-ID: <CA+dpOJmuzENh8zusk3hhb3Kpa6kpkcpQTRZ1H1S3Z0rp-ZSztw@mail.gmail.com>

Hi,

Is there any R package to fit Richards' curve in the form of
https://en.wikipedia.org/wiki/Generalised_logistic_function

I found there is one package grofit, but currently defunct.

Any pointer appreciated.


From Po||ngW @end|ng |rom @etn@@com  Wed May 13 11:30:57 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Wed, 13 May 2020 09:30:57 +0000
Subject: [R] Help with Radius problem
In-Reply-To: <CAGxFJbSv3jV5eszHqMcf5JYZSvy2OBHHpxq2W5FPPcju8jp+qA@mail.gmail.com>
References: <BYAPR06MB5383B486628866FD2F36917CAEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CAGxFJbSv3jV5eszHqMcf5JYZSvy2OBHHpxq2W5FPPcju8jp+qA@mail.gmail.com>
Message-ID: <BYAPR06MB53839852EC2BA4299B8C4465AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>

Good morning Bert. I will sign up for r-sig-geo and review your suggested link as well, thank you very much for your response.

WHP




Proprietary

-----Original Message-----
From: Bert Gunter <bgunter.4567 at gmail.com> 
Sent: Tuesday, May 12, 2020 8:30 PM
To: Poling, William <PolingW at aetna.com>
Cc: r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with Radius problem

**** External Email - Use Caution ****

No insight But have you consulted:
https://urldefense.proofpoint.com/v2/url?u=https-3A__CRAN.R-2Dproject.org_view-3DSpatial&d=DwIBaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=gbt_psl_UbXJa0-nqZs2cDd5cTAtzTGE6OOluljG2U0&s=aT7qePkClGUoKBh0QatR5mvySgsYeUp9gU0WLReStTU&e= 

Also, I believe this would be better posted on r-sig-geo, not here.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Tue, May 12, 2020 at 5:57 PM Poling, William via R-help <r-help at r-project.org> wrote:
>
> #RStudio Version Version 1.2.1335
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit) #Running under: Windows 10 
> x64 (build 17763)
>
> Hello I am trying to figure out how to create concentration of ID's in 
> the following data, based on determining the highest concentration in 
> the smallest radius area based on Longitude and Latitude (geo 
> location)
>
> I have reviewed many websites looking for a function or tutorial, (if 
> you are aware of either please let me know)
>
> I have worked through the following to see if they are close to what I need, but they are not.
>
> #Possible use 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stackoverflow.com
> _questions_21977720_r-2Dfinding-2Dclosest-2Dneighboring-2Dpoint-2Dand-
> 2Dnumber-2Dof-2Dneighbors-2Dwithin-2Da-2Dgiven-2Drad&d=DwIBaQ&c=wluqKI
> iwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2
> ExlYvnUWbsByY&m=gbt_psl_UbXJa0-nqZs2cDd5cTAtzTGE6OOluljG2U0&s=t6qPeVA_
> C9VbEMtn5earDo2F53CkazDyPAQrL81PifM&e=
>
> #Possible math ref 
> https://urldefense.proofpoint.com/v2/url?u=http-3A__janmatuschek.de_La
> titudeLongitudeBoundingCoordinates&d=DwIBaQ&c=wluqKIiwffOpZ6k5sqMWMBOn
> 0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=gb
> t_psl_UbXJa0-nqZs2cDd5cTAtzTGE6OOluljG2U0&s=o5R4BF22ow2gJd8RgJOS0pVRn6
> 6a4uCBcXXgK59Z1SM&e=
>
> #Possible use 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stackoverflow.com
> _questions_39008850_find-2Dnumber-2Dof-2Dpoints-2Dwithin-2Da-2Dradius-
> 2Din-2Dr-2Dusing-2Dlon-2Dand-2Dlat-2Dcoordinates&d=DwIBaQ&c=wluqKIiwff
> OpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlY
> vnUWbsByY&m=gbt_psl_UbXJa0-nqZs2cDd5cTAtzTGE6OOluljG2U0&s=cr5fY67h9cmL
> ZlNVPGcJ5gysN_qVQC-xZetElqwYVhk&e=
>
> # 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__gis.stackexchange
> .com_questions_229453_create-2Da-2Dcircle-2Dof-2Ddefined-2Dradius-2Dar
> ound-2Da-2Dpoint-2Dand-2Dthen-2Dfind-2Dthe-2Doverlapping-2Da&d=DwIBaQ&
> c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmT
> CtKvneM2ExlYvnUWbsByY&m=gbt_psl_UbXJa0-nqZs2cDd5cTAtzTGE6OOluljG2U0&s=
> xK2iPfFjKfUevj39Uk8ux2OZYYxE3N4kOQU3ecP0W4M&e=
>
> # 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stackoverflow.com
> _questions_23071026_drawing-2Da-2Dcircle-2Dwith-2Da-2Dradius-2Dof-2Da-
> 2Ddefined-2Ddistance-2Din-2Da-2Dmap_23072079-2323072079&d=DwIBaQ&c=wlu
> qKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvn
> eM2ExlYvnUWbsByY&m=gbt_psl_UbXJa0-nqZs2cDd5cTAtzTGE6OOluljG2U0&s=susH1
> Qr4Cz3oohO3OF6Bs6PWuAzo5hc7WMQccLvuX08&e=
>
> My actual data is 2353 records with 41 states and 1337 cities represented. Here is a sample below.
>
> Some might call this a hotspot question I suppose: "What is the maximum number of ID's I can gather in the smallest radius size given the data at hand?
>
> For example it would be useful to have a function that I could use to test different combinations of radius size and concentration of ID #'s by geo location.
>
> Any insight would be very much appreciated.
>
> Thank you.
> WHP
>
> str(sample)
> 'data.frame':   35 obs. of  5 variables:
>  $ state    : Factor w/ 41 levels "AL","AR","AZ",..: 19 29 9 9 10 30 33 35 41 12 ...
>  $ city     : Factor w/ 1337 levels "ABBOTTSTOWN",..: 281 1280 129 223 989 721 550 731 1325 688 ...
>  $ latitude : num  43.5 40.1 26.5 27.9 31.9 ...
>  $ longitude: num  -70.6 -82.9 -80.1 -82.7 -81.4 ...
>  $ ID       : int  2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 ...
>
> dput(sample)
> structure(list(state = structure(c(19L, 29L, 9L, 9L, 10L, 30L, 33L, 
> 35L, 41L, 12L, 12L, 36L, 4L, 9L, 8L, 39L, 36L, 36L, 41L, 28L, 12L, 
> 28L, 28L, 12L, 11L, 2L, 12L, 22L, 32L, 22L, 12L, 10L, 10L, 22L, 10L), 
> .Label = c("AL", "AR", "AZ", "CA", "CO", "CT", "DC", "DE", "FL", "GA", 
> "IA", "IL", "IN", "KS", "KY", "LA", "MA", "MD", "ME", "MI", "MN", 
> "MO", "NC", "NE", "NJ", "NM", "NV", "NY", "OH", "OK", "OR", "PA", 
> "SC", "SD", "TN", "TX", "UT", "VA", "WA", "WI", "WV"), class = 
> "factor"), city = structure(c(281L, 1280L, 129L, 223L, 989L, 721L, 
> 550L, 731L, 1325L, 688L, 1184L, 281L, 759L, 1171L, 1305L, 587L, 272L, 
> 581L, 263L, 152L, 217L, 152L, 390L, 5L, 571L, 1006L, 1162L, 939L, 
> 170L, 1033L, 1002L, 586L, 586L, 192L, 586L), .Label = c("ABBOTTSTOWN", 
> "ABILENE", "ACWORTH", "ADAMS", "ADDISON", "ADKINS", "ALBANY", "ALBIA", 
> "ALBUQUERQUE", "ALEXANDRIA", "ALFRED", "ALIQUIPPA", "ALISO VIEJO", 
> "ALLEN PARK", "ALLENTOWN", "ALPHA", "ALPHARETTA", "ALPINE", "ALTOONA", 
> "AMARILLO", "AMBLER", "AMBRIDGE", "AMHERST", "AMITYVILLE", "ANAHIEM", 
> "ANDERSON", "ANDOVER", "ANGIER", "ANGLETON", "ANKENY", "ANN ARBOR", 
> "ANZA", "APEX", "APOLLO", "ARCADIA", "ARCHDALE", "ARDMORE", "ARGYLE", 
> "ARKANSAS CITY", "ARLINGTON", "ARNOLD", "ARTHUR", "ASHEBORO", 
> "ASHEVILLE", "ASHLAND", "ASHLAND CITY", "ASHTON", "ASTON", "ATHENS", 
> "ATLANTA", "ATLANTIC BCH", "AUBORN", "AUGUSTA", "AURORA", "AUSTELL", 
> "AUSTIN", "AVENTURA", "AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN 
> CITY", "BALDWIN PLACE", "BALLWIN", "BANGOR", "BARBOURSVILLE", 
> "BARNEGAT", "BARTLETT", "BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", 
> "BATESBURG", "BATON ROUGE", "BAXTER SPRINGS", "BAY CITY", "BAYONNE", 
> "BAYSIDE", "BAYTOWN", "BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER 
> CITY", "BELFAIR", "BELLE VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR", 
> "BELLMORE", "BELLWOOD", "BELMAR", "BELMONT", "BELVIDERE", "BENNET", 
> "BENTON", "BENTONVILLE", "BERLIN", "BESSEMER CITY", "BETHANY", 
> "BETHEL", "BETHEL PARK", "BETHESDA", "BETHLEHEM", "BETTENDORF", 
> "BIGLERVILLE", "BINGHAMTON", "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN", 
> "BLACKSBURG", "BLAINE", "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD", 
> "BLUE BELL", "BLUE GRASS", "BLUE POINT", "BLUE SPRINGS", "BOCA RATON", 
> "BODFISH", "BOILING SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS", 
> "BONNEY LAKE", "BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING 
> GREEN", "BOYERTOWN", "BOYNTON BEACH", "BRADENTON", "BRADENVILLE", 
> "BRANDAMORE", "BRANDON", "BRANFORD", "BRANSON", "BRASELTON", 
> "BREEZEWOOD", "BRENTON", "BRENTWOOD", "BREWSTER", "BRICK", 
> "BRIDGEPORT", "BRIDGETON", "BRIGHAM CITY", "BROADDUS", "BROCKPORT", 
> "BROGUE", "BROKEN ARROW", "BRONX", "BROOKFIELD", "BROOKHAVEN", 
> "BROOKLYN", "BROWNS MILLS", "BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", 
> "BRYAN", "BUCKSPORT", "BUDD LAKE", "BUFFALO", "BULVERDE", 
> "BUNKERVILLE", "BURBANK", "BURFORD", "BURIEN", "BURLINGTON", "BURR 
> RIDGE", "BURTON", "BUSKIRK", "BUTLER", "BUXTON", "BYRON", "CALERA", 
> "CAMBRIDGE", "CAMP HILL", "CAMPBELL", "CANAAN", "CANAL WINCHESTER", 
> "CANASTOTA", "CANTON", "CARDIFF BY THE SEA", "CARL JUNCTION", 
> "CARLISLE", "CARLTON", "CAROL STREAM", "CARROLLTON", "CARSON CITY", 
> "CARTERSVILLE", "CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", 
> "CASWELL", "CATO", "CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER 
> POINT", "CENTERTON", "CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN 
> FALLS", "CHALFONT", "CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", 
> "CHARLESTON", "CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", 
> "CHESHIRE", "CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", 
> "CHILLICOTHE", "CHOCTAW", "CICERO", "CINCINNATI", "CLAY", 
> "CLEARWATER", "CLEARWATER BEACH", "CLEMMONS", "CLENDENIN", 
> "CLEVELAND", "CLEVELAND HTS", "CLEVES", "CLIFFSIDE PARK", "CLIFTON", 
> "CLIFTON PARK", "CLIFTON SPGS", "CLINTON", "COACHELLA", "COAL 
> TOWNSHIP", "COATESVILLE", "COLLEGE STATION", "COLLINSVILLE", 
> "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE", "CONCORD", "CONCORD 
> TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY", "CONYERS", 
> "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA", "CORNELIUS", 
> "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY", "COTTAGE HILLS", 
> "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN", "CRANBERRY 
> TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER", "CUMBERLAND 
> FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS", "DALLAS", 
> "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT", 
> "DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR", 
> "DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH", 
> "DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY", "DES 
> MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER", "DIXON", 
> "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER", "DOYLESTOWN", 
> "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH", "DUNCAN", "DUNCANNON", 
> "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE", "DURHAM", "DUTTON", "EAGLE 
> GROVE", "EAST AURORA", "EAST CHICAGO", "EAST EARL", "EAST HANOVER", 
> "EAST HARTFORD", "EAST HARTLAND", "EAST LEROY", "EAST LIVERPOOL", 
> "EAST MEADOW", "EAST NORTHPORT", "EAST ORANGE", "EAST PEORIA", "EAST 
> SAINT LOUIS", "EASTON", "EDGERTON", "EDISON", "EDMOND", "EGG HBR TWP", 
> "EL DORADO", "EL PASO", "ELDRIDGE", "ELGIN", "ELIZABETHVILLE", 
> "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY", "ELMHURST", "ELMWOOD PARK", 
> "ELOY", "ELY", "EMERSON", "EMORY", "ENDICOTT", "ENGLEWOOD", "ENID", 
> "ENNICE", "ENOLA", "ENUMCLAW", "EPHRATA", "ERIE", "ESTERO", "EUREKA 
> SPRINGS", "EVANSVILLE", "EVERETT", "EXCELSIOR SPRINGS", "EXTON", 
> "FAIRFAX", "FAIRFIELD", "FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", 
> "FALMOUTH", "FAR HILLS", "FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", 
> "FAYETTE", "FAYETTEVILLE", "FELTON", "FENTON", "FERGUSON", "FIELDALE", 
> "FINDLAY", "FINGERVILLE", "FLEMINGTON", "FLINT", "FLORENCE", 
> "FLORESVILLE", "FLORISSANT", "FLOVILLA", "FLOWER MOUND", "FLUSHING", 
> "FOGELSVILLE", "FOLSOM", "FONTANELLE", "FOREST HILLS", "FOREST PARK", 
> "FORT DODGE", "FORT FAIRFIELD", "FORT GAY", "FORT LAUDERDALE", "FORT 
> LEE", "FORT MILL", "FORT MOHAVE", "FORT PAYNE", "FORT PIERCE", "FORT 
> SMITH", "FORT WAYNE", "FORT WORTH", "FOSTORIA", "FOUNTAIN INN", 
> "FRANKFORT", "FRANKLIN", "FREEDOM", "FREEHOLD", "FREEPORT", "FREMONT", 
> "FULLERTON", "FULSHEAR", "FULTON", "GADSDEN", "GAFFNEY", 
> "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE", "GARDNERS", "GARNETT 
> VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA", "GETTYSBURG", 
> "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE", "GLEN CARBON", "GLEN 
> COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW", "GLENWOOD", "GOLDEN 
> CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM", "GOSHEN", 
> "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS", "GRANDVIEW", 
> "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS", "GRASS VALLEY", 
> "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER", "GREENLAWN", 
> "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE", "GREER", 
> "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD", "HADLEY", 
> "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON", "HAMBURG", 
> "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE", 
> "HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON", 
> "HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL", 
> "HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE", 
> "HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH", 
> "HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE", "HIGH 
> HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS", 
> "HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE", 
> "HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD", 
> "HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS", "HOT 
> SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON", "HUFFMAN", 
> "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON", "HUNTINGTON 
> BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE", "HURST", 
> "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL", 
> "INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY", "IRMO", 
> "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP", "ISSAQUAH", 
> "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS", "JACKSONVILLE", 
> "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE", "JEFFERSON", "JEFFERSON 
> CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN", "JONESBORO", "JONESPORT", 
> "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE", "KANNAPOLIS", "KANSAS 
> CITY", "KATY", "KEARNEY", "KELLERTON", "KEMAH", "KENMORE", "KENNESAW", 
> "KENT", "KINGMAN", "KINGS MOUNTAIN", "KINGSLAND", "KINGSTON", 
> "KINGWOOD", "KIRKLAND", "KUTZTOWN", "LA QUINTA", "LA RUE", "LA VERNE", 
> "LAFAYETTE", "LAKE BLUFF", "LAKE IN THE HILL", "LAKE ISABELLA", "LAKE 
> PLACID", "LAKE SAINT LOUIS", "LAKE TAPPS", "LAKE VILLAGE", "LAKE 
> WAUKOMIS", "LAKE WORTH", "LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD 
> RCH", "LAMBERTVILLE", "LANCASTER", "LANDISVILLE", "LANSDALE", 
> "LAREDO", "LARGO", "LAS VEGAS", "LATROBE", "LAUDERHILL", "LAWNDALE", 
> "LAWRENCE", "LAWRENCEVILLE", "LE MARS", "LE ROY", "LEAGUE CITY", 
> "LEAVENWORTH", "LEAWOOD", "LEBANON", "LECOMPTON", "LEECHBURG", "LEES 
> SUMMIT", "LEESBURG", "LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", 
> "LEVITTOWN", "LEWES", "LEWISTOWN", "LEXINGTON", "LIBERTY TWP", 
> "LILLINGTON", "LINCOLN", "LINDENHURST", "LINTON", "LISBON", 
> "LITCHFIELD PK", "LITHIA", "LITITZ", "LITTLE EGG HARBOR TO", "LK 
> FOREST PK", "LK PEEKSKILL", "LOCKHART", "LOCKWOOD", "LOGAN", "LOMA 
> LINDA", "LONE JACK", "LONG BEACH", "LONG LANE", "LONGVIEW", "LORAIN", 
> "LORTON", "LOS ANGELES", "LOUISBURG", "LOVES PARK", "LOWER BURRELL", 
> "LOWRY CITY", "LUBBOCK", "LUGOFF", "LUMBERTON", "LYNBROOK", 
> "LYNNWOOD", "MACHESNEY PARK", "MACON", "MAGNOLIA", "MAHOPAC", 
> "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE", "MANASSAS", "MANCHACA", 
> "MANCHESTER", "MANCHESTER TOWNSHIP", "MANCHESTER TW", "MANHEIM", 
> "MANITO", "MANLIUS", "MANNINGTON", "MANSFIELD", "MANSURA", "MAPLE 
> PARK", "MARBLE FALLS", "MARCELLUS", "MARICOPA", "MARIETTA", "MARION", 
> "MARKLEYSBURG", "MARS HILL", "MARSHALLTOWN", "MARSHFIELD", 
> "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE", "MASON CITY", "MASSEY", 
> "MASSILLON", "MASURY", "MATAWAN", "MATEWAN", "MATTAPOISETT", 
> "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND", "MC DONALD", "MC 
> KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN", "MCALESTER", "MCDONOUGH", 
> "MCKEESPORT", "MCKINNEY", "MECHANICSBURG", "MECHANICSVILLE", 
> "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS", "MERIDEN", 
> "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA", "MESQUITE", 
> "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN", "MIDLAND", 
> "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK", "MILLEDGEVILLE", 
> "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE", "MINEOLA", "MINERAL 
> SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION", "MISSION HILLS", 
> "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY", "MOLINE", 
> "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE", "MONTCLAIR", 
> "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON", "MORGANTOWN", 
> "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE", "MOUNT JOY", 
> "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO", "MOUNT UNION", 
> "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT", "MULVANE", 
> "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT", 
> "MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS", 
> "NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES", 
> "NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG", "NEW 
> BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK", "NEW 
> CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON", "NEW 
> LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE", "NEW 
> ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK", 
> "NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT", 
> "NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA", 
> "NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK", 
> "NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON", 
> "NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND 
> HILLS", "NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE", 
> "NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON", "OAK 
> RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE", "OMAHA", 
> "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION", "ORONO", 
> "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO", "OTTAWA", 
> "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD", "OYSTER BAY", 
> "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR", "PALMDALE", "PALMYRA", 
> "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE", "PARLIN", "PARMA", 
> "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY", "PAXTON", "PEA 
> RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN", "PELION", 
> "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE", "PENNSBURG", 
> "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS", "PERRYSBURG", 
> "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI", "PHOENIX", 
> "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN", "PINE BLUFF", 
> "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY", "PITMAN", 
> "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY", 
> "PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO 
> LAKE", "POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", 
> "PORT CHARLOTTE", "PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", 
> "PORT WASHINGTON", "PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", 
> "POTTSVILLE", "POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE 
> VILLAGE", "PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT 
> PLEASANT", "PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", 
> "QUINCY", "RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", 
> "READLYN", "RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO 
> PARK", "REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", 
> "RICH HILL", "RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", 
> "RINCON", "RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", 
> "RIVERSIDE", "ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", 
> "ROCKAWAY BCH", "ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", 
> "ROGERS", "ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE", 
> "ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT", 
> "ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN", 
> "RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO", 
> "SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR", "SAINT 
> CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS", "SAINT 
> PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY", 
> "SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO", 
> "SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN", 
> "SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", 
> "SARASOTA", "SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", 
> "SAYVILLE", "SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", 
> "SCOTT", "SCOTTSDALE", "SEAFORD", "SEALE", "SEATTLE", "SEDONA", 
> "SEGUIN", "SELDEN", "SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", 
> "SHARON", "SHARPSBURG", "SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", 
> "SHELLSBURG", "SHELTON", "SHENANDOAH", "SHORELINE", "SHOREWOOD", 
> "SHREVEPORT", "SICKLERVILLE", "SILER CITY", "SILOAM SPRINGS", "SIMON", 
> "SIMPSONVILLE", "SIMSBURY", "SIOUX CITY", "SIOUX FALLS", "SKOKIE", 
> "SKOWHEGAN", "SLATINGTON", "SLIDELL", "SMITHFIELD", "SN BERNRDNO", 
> "SNELLVILLE", "SOCORRO", "SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH 
> BELOIT", "SOUTH BEND", "SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH 
> PARK", "SOUTH PLAINFIELD", "SOUTH SIOUX CITY", "SOUTHAMPTON", 
> "SOUTHBURY", "SOUTHGATE", "SPARKS", "SPARROW BUSH", "SPENCER", 
> "SPRING", "SPRING BRANCH", "SPRING GROVE", "SPRING HILL", "SPRING 
> HOPE", "SPRING VALLEY", "SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD 
> GARDENS", "SPRINGHILL", "SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", 
> "ST PETERSBURG", "ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", 
> "STANLEY", "STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", 
> "STILLMAN VALLEY", "STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", 
> "STRAFFORD", "STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", 
> "SUFFIELD", "SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", 
> "SUMMIT", "SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", 
> "SWEENY", "SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", 
> "TAFTVILLE", "TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", 
> "TAYLORVILLE", "TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", 
> "THOMASVILLE", "TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY 
> PARK", "TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", 
> "TOPEKA", "TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS 
> RST", "TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER", 
> "TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY", 
> "UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA", 
> "URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO", "VAN 
> BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO", 
> "VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA", 
> "VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH", "W 
> TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN", 
> "WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG", 
> "WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA", 
> "WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE", 
> "WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER", 
> "WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO", "WEST 
> CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN", "WEST 
> HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH", "WEST 
> POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD", "WEST VALLEY 
> CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER", "WESTMORELAND 
> CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD", "WHARTON", "WHEELING", 
> "WHITE PLAINS", "WHITEHALL", "WHITESTONE", "WHITESTOWN", "WHITING", 
> "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD", "WILLARD", 
> "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK", 
> "WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS", 
> "WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON", 
> "WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE", 
> "WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS", 
> "WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH", 
> "YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS", "YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor"),
>     latitude = c(43.53462, 40.142492, 26.538149, 27.9062, 31.90104,
>     34.833083, 35.118836, 35.136047, 39.425712, 40.519207, 40.101126,
>     30.044457, 33.62287, 27.910276, 39.752254, 47.403778, 32.961929,
>     29.820199, 38.408649, 40.668828, 41.983948, 40.58116, 40.7253,
>     41.921667, 41.669921, 36.442552, 38.564663, 37.455315, 40.867774,
>     38.783554, 42.244381, 34.024332, 34.001644, 36.622704, 34.017832
>     ), longitude = c(-70.58809, -82.888789, -80.113071, -82.6661,
>     -81.372992, -95.840964, -82.016013, -89.92625, -80.276676,
>     -89.778385, -87.641063, -94.888036, -117.663119, -82.530568,
>     -75.574437, -122.209047, -96.799309, -95.746255, -80.553953,
>     -73.923314, -87.713482, -73.9731, -73.8187, -87.987023, -93.759478,
>     -94.110903, -90.00597, -93.257109, -79.932483, -90.201858,
>     -89.100233, -84.625923, -84.567614, -93.912921, -84.612564
>     ), ID = 2318:2352), row.names = c(NA, -35L), class = "data.frame")
>
>
> Proprietary
>
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may 
> con...{{dropped:16}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIBaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmv
> OXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=gbt_psl_UbXJa0-
> nqZs2cDd5cTAtzTGE6OOluljG2U0&s=QL9JHIC7CR7kgMybKwS9d3Q3PSpRgaxHKx9QG1o
> Fv_M&e= PLEASE do read the posting guide 
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIBaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmm
> vOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=gbt_psl_UbXJa0
> -nqZs2cDd5cTAtzTGE6OOluljG2U0&s=L9Wz0HxE7TZHsDtBQPJw8BkJwzig5LFyl9_kpe
> pkPVc&e= and provide commented, minimal, self-contained, reproducible 
> code.

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From petr@p|k@| @end|ng |rom prechez@@cz  Wed May 13 11:38:47 2020
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Wed, 13 May 2020 09:38:47 +0000
Subject: [R] Fitting Richards' curve
In-Reply-To: <CA+dpOJmuzENh8zusk3hhb3Kpa6kpkcpQTRZ1H1S3Z0rp-ZSztw@mail.gmail.com>
References: <CA+dpOJmuzENh8zusk3hhb3Kpa6kpkcpQTRZ1H1S3Z0rp-ZSztw@mail.gmail.com>
Message-ID: <ab1c0c0f77b64626b42c64629d12c972@SRVEXCHCM1302.precheza.cz>

Hi Christofer

Try FlexParamCurve or maybe drc package.

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Christofer Bogaso
> Sent: Wednesday, May 13, 2020 11:26 AM
> To: r-help <r-help at r-project.org>
> Subject: [R] Fitting Richards' curve
> 
> Hi,
> 
> Is there any R package to fit Richards' curve in the form of
> https://en.wikipedia.org/wiki/Generalised_logistic_function
> 
> I found there is one package grofit, but currently defunct.
> 
> Any pointer appreciated.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From pd@|gd @end|ng |rom gm@||@com  Wed May 13 11:41:59 2020
From: pd@|gd @end|ng |rom gm@||@com (Peter Dalgaard)
Date: Wed, 13 May 2020 11:41:59 +0200
Subject: [R] Fitting Richards' curve
In-Reply-To: <CA+dpOJmuzENh8zusk3hhb3Kpa6kpkcpQTRZ1H1S3Z0rp-ZSztw@mail.gmail.com>
References: <CA+dpOJmuzENh8zusk3hhb3Kpa6kpkcpQTRZ1H1S3Z0rp-ZSztw@mail.gmail.com>
Message-ID: <A9518FD1-4E6D-4BF2-95D2-9F165CAD38FA@gmail.com>

Shouldn't be hard to set up with nls(). (I kind of suspect that the Richards curve has more flexibility than data can resolve, especially the subset (Q,B,nu) seems highly related, but hey, it's your data...)

-pd 

> On 13 May 2020, at 11:26 , Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
> 
> Hi,
> 
> Is there any R package to fit Richards' curve in the form of
> https://en.wikipedia.org/wiki/Generalised_logistic_function
> 
> I found there is one package grofit, but currently defunct.
> 
> Any pointer appreciated.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From Po||ngW @end|ng |rom @etn@@com  Wed May 13 11:53:12 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Wed, 13 May 2020 09:53:12 +0000
Subject: [R] Help with Radius problem
In-Reply-To: <CA+8X3fW6kLo-f3E32uUZMO5=93GMtjNixSpuiX4qo50_+E2_pw@mail.gmail.com>
References: <BYAPR06MB5383B486628866FD2F36917CAEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CA+8X3fW6kLo-f3E32uUZMO5=93GMtjNixSpuiX4qo50_+E2_pw@mail.gmail.com>
Message-ID: <BYAPR06MB5383FA2A6F643D6C96D630DEAEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>

Good morning Jim.

This is awesome start, visualization is splendid, thank you very much.

I have signed on to r-sig-geo and submitted my question there. I have no idea of the volume of traffic on that list
However, hopefully, I will gain additional insight into how to determine max number of member locations in the most minimum radius area.

Have a great day!

WHP



Proprietary

-----Original Message-----
From: Jim Lemon <drjimlemon at gmail.com> 
Sent: Wednesday, May 13, 2020 1:39 AM
To: Poling, William <PolingW at aetna.com>
Cc: r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with Radius problem

**** External Email - Use Caution ****

Hi Bill,
A while ago I devised a couple of functions to accumulate millions of geographic locations of events and then display the resulting matrix of values on an existing plot. This may be of use to you, at least in the visualization of the density of the locations. As your example data only included a few points, the resulting plot looks pretty chunky as I had to use a large symbol to make the cells with non-zero counts obvious.

# read in your sample data
source("wp_data.R")
library(plotrix)
geomat<-makeDensityMatrix(geodat[,c("latitude","longitude")],
xlim=range(geodat$longitude),ylim=range(geodat$latitude))
# Range of density (>0) - 1.119816 3.922018
latlim<-range(geodat$latitude)
lonlim<-range(geodat$longitude)
library(maps)
map("world",xlim=lonlim,ylim=latlim)
axis(1)
axis(2)
densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
 red=c(0.5,1),green=0,blue=0,pch=15)

Jim

On Wed, May 13, 2020 at 10:57 AM Poling, William via R-help <r-help at r-project.org> wrote:
>
> #RStudio Version Version 1.2.1335
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit) #Running under: Windows 10 
> x64 (build 17763)
>
> Hello I am trying to figure out how to create concentration of ID's in 
> the following data, based on determining the highest concentration in 
> the smallest radius area based on Longitude and Latitude (geo 
> location)
>
> I have reviewed many websites looking for a function or tutorial, (if 
> you are aware of either please let me know)
>

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com  Wed May 13 13:28:26 2020
From: @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com (Subhamitra Patra)
Date: Wed, 13 May 2020 16:58:26 +0530
Subject: [R] 
 [R ] Writing loop to estimate ARCH test for a multiple columns
 of a data frame?
In-Reply-To: <CA+8X3fVgYkZ00gqqVhdaE7Nn=VvQ_x=eoBdjqBKsmALsTGfB5A@mail.gmail.com>
References: <CAOFE=kMaSgqy7_dMsB3WtTgN+BDa4b5m28GO0uxm0rUsDmqc9g@mail.gmail.com>
 <CA+8X3fWOMFQ8Ptz75j8DjhJuaEKi8nR1p+82LeNqTcB2NfSjKA@mail.gmail.com>
 <CAOFE=kOtwzrr3cFXhvvS+db+iw=p4eg1Fm6JCGtCzwessQ90FA@mail.gmail.com>
 <CA+8X3fUB8Mp9Pn8RO7PB3knoEgLHEdT6LuwxyxPbNMvQNoVG5A@mail.gmail.com>
 <CAOFE=kNNFOy=GCSWidG+Fds4qi5r7w9M9SafZ+oqcQw-xeKmkg@mail.gmail.com>
 <CA+8X3fXcztWSqhUiKAS+uc+fH57RCV7jN5ihLeRbz5rUgkp28g@mail.gmail.com>
 <CAOFE=kO6Pz=vqOR6+mBvjkMMBMhv9bX9s9nUCRnBoF9dGkdiVA@mail.gmail.com>
 <CA+8X3fVgYkZ00gqqVhdaE7Nn=VvQ_x=eoBdjqBKsmALsTGfB5A@mail.gmail.com>
Message-ID: <CAOFE=kPqS0VyLeU-VDCd=2a9B4d98Gv3ov1oxUriW0adCEJexw@mail.gmail.com>

Dear Sir,

I am so sorry that due to certain inconveniences, I became late to try your
suggested code and to reply to your email.

Thank you very much for your wonderful solution and suggestion for my
problem. Like before,  Your suggested code has worked awesome. Even, I
successfully imported the required output to the word following your
suggested similar path for the Libre office editor.

But, I have certain queries on your suggested code mentioned below which I
would like to discuss with you for my further learning.

1. Is there any difference between reading the tab and text file in R
because when I used  sp_8_5<-read.table("sp_8_5.tab",sep="\t",


header=TRUE,stringsAsFactors=FALSE)
it had thrown some error. But, when I changed the sp_8_5.tab into
sp_8_5.text, it worked. So, here my query, "does R read tab and text file
differently, however, both the files are similar"?

2. In the code, "return(sprintf("ChiSq = %.1f, p =
%.3f",archout$statistic,archout$p.value))", sprintf stands for printing the
particular results (i.e., statistics and p-value), right? Further, "ChiSq =
%.1f, p = %.3f" indicate the calling the values up to 1 and 3 decimal
points respectively, right? kindly correct me if I am worng in my
interpretation.

3. While opening a text file, sink("sp_8_5.txt")
                                         for(row in 0:2) {
                                         for(column in 1:4)

cat(spout[[column+row*4]],ifelse(column
< 4,"\t","\n"))
                                         }
                                                   sink()
3.1. what sink indicates, I think here sink calls for the arranging of the
statistics and p-values in the required 3*4 dimension in the generated text
file, right? Please educate me.
3.2 Hence, the results are arranged in 3 rows and 4 columns in the text
file. I understand the code for arranging loop for columns [i.e.,
for(column in 1:4) ], but i didn't understand the loop for row [i.e., for(row
in 0:2)]. In particular, what is the logic behind the setting of 2 rather
than 3 for 3 rows in "for(row in 0:2)"?
3.3. In the code, "cat(spout[[column+row*4]],ifelse(column <
4,"\t","\n"))", what cat indicates? what is the logic behind [column+row*4]
 and ifelse(column < 4,"\t","\n") ? This is my major query in the entire
code. Please help me to understand this line.


Along with the above queries in your suggested code, I have one more query that
is it possible to rename each row and column? Actually, why I am asking
this because I have data from 80 countries, and each country has 5 columns
of data arranging in 5 columns. In other words, the total number of columns
in my study is 400. While doing the ARCH test for each column, there may be
a mistake to arrange the results in the text file. Thus, I want to arrange
the resulted statistics for 5 columns (for instance A1, A2, A3, A4, A5) for
each country in the following way which I think will definitely avoid any
kind of typo-mistake in arranging output in the text file. In other words,
Each row will have results for each country arranged in 5 columns for the
particular 5 variables which help to identify the particular result for the
particular columns of the particular countries in an easy manner.


Country           A1        A2       A3     A4     A5
India              0.65      0.33   0.32   0.12  0.34
Israel              0.35      0.05   0.10    0.15   0.23
Australia          0.43      0.25    0.45    0.55    0.56

and so on.


Thank you very much, Sir, for educating a R learner for which I shall be
always grateful to you.


[image: Mailtrack]
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
Sender
notified by
Mailtrack
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
05/13/20,
04:56:34 PM

On Sat, May 9, 2020 at 8:58 AM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Subhamitra,
> I have washed the dishes and had a night's sleep, so I can now deal with
> your text munging problem. First, I'll reiterate the solution I sent:
>
> sp_8_5<-read.table("sp_8_5.tab",sep="\t",
>  header=TRUE,stringsAsFactors=FALSE)
> library(tseries)
> library(FinTS)
> # create a function that returns only the
> # statistic and p.value as a string
> archStatP<-function(x) {
>  archout<-ArchTest(x)
>  # I have truncated the values here
>  return(sprintf("ChiSq = %.1f, p =
> %.3f",archout$statistic,archout$p.value))
> }
> # using "lapply", run the test on each column
> spout<-lapply(sp_8_5[,2:13],archStatP)
>
> If you look at "spout" you will see that it is a list of 12 character
> strings. I arranged this as you seem to want the contents of a 3x4 table in
> a document. This is one way to do it, there are others.
>
> First, create a text table of the desired dimensions. I'll do it with
> loops as you seem to be familiar with them:
>
> # open a text file
> sink("sp_8_5.txt")
> for(row in 0:2) {
>  for(column in 1:4)
>   cat(spout[[column+row*4]],ifelse(column < 4,"\t","\n"))
> }
> sink()
>
> If you open this file in a text editor (e.g. Notepad) you will see that it
> contains 3 lines (rows), each with four TAB separated strings. Now to
> import this into a word processing document. I don't have MS Word, so I'll
> do it with Libre Office Writer and hope that the procedure is similar.
>
> Move to where you want the table in your document
> Select Insert|Text from file from the top menu
> Select (highlight) the text you have imported
> Select Convert|Text to table from the top menu
>
> The highlighted area should become a table. I had to reduce the font size
> from 12 to 10 to get the strings to fit into the cells.
>
> There are probably a few more changes that you will want, so let me know
> if you strike trouble.
>
> Jim
>
>
> On Fri, May 8, 2020 at 11:28 PM Subhamitra Patra <
> subhamitra.patra at gmail.com> wrote:
>
>> Dear Sir,
>>
>> Thank you very much for your wonderful suggestion for my problem. Your
>> suggested code has excellently worked and successfully extracted the
>> statistics and p-value in another R object.
>>
>> Concerning your last suggestion, I attempted to separate the strings with
>> TAB character in the "spout" object by using different alternative packages
>> like dplyr, tidyr, qdap, ans also by using split,strsplit function so that
>> can export the statistics and p-values for each column to excel, and later
>> to the MSword file, but got the below error.
>>
>> By using the  split function, I wrote the code as,
>> *string[] split = s.Split(spout, '\t')*
>> where I got the following errors.
>> Error: unexpected symbol in "string[] split"
>> Error: unexpected symbol in "string[[]]split"
>> Error in strsplit(row, "\t") : non-character argument
>>
>> Then I tried with  strsplit function by the below code
>> *strsplit(spout, split)*
>> But, got the below error as
>> Error in as.character(split) :
>>   cannot coerce type 'closure' to vector of type 'character'.
>>
>> Then used dplyr and tidyr package and the wrote the below code
>> library(dplyr)
>> library(tidyr)
>> *separate(spout,value,into=c(?ChiSq?,?p?),sep=?,?)*
>> *separate(spout,List of length 12,into=c(?ChiSq?,?p?),sep="\t")*
>> But, got the errors as,
>> Error: unexpected input in "separate(spout,value,into=c(?"
>> Error: unexpected symbol in "separate(spout,List of"
>>
>> Then used qdap package with the code below
>>
>> *colsplit2df(spout,, c("ChiSq", "p"), ",")*
>> *colsplit2df(spout,, c("ChiSq", "p"), sep = "\t")*
>> But got the following errors
>> Error in dataframe[, splitcol] : incorrect number of dimensions
>> In addition: Warning message:
>> In colsplit2df_helper(dataframe = dataframe, splitcol = splitcols[i],  :
>>   dataframe object is not of the class data.frame
>> Error in dataframe[, splitcol] : incorrect number of dimensions
>> In addition: Warning message:
>> In colsplit2df_helper(dataframe = dataframe, splitcol = splitcols[i],  :
>>   dataframe object is not of the class data.frame
>>
>> Sir, please suggest me where I am going wrong in the above to separate
>> string in the "spout" object.
>>
>> Thank you very much for your help.
>>
>> [image: Mailtrack]
>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
>> notified by
>> Mailtrack
>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 05/08/20,
>> 06:51:46 PM
>>
>> On Fri, May 8, 2020 at 4:47 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>>
>>> 1) In general, *apply functions return a list with the number of
>>> elements equal to the number of columns or other elements of the input
>>> data. You can assign that list as I have to "spout" in the first example.
>>>
>>> 2) spout<-list() assigns the name "spout" to an empty list. As we are
>>> processing columns 2 to 12 of the input data, spout[[i-1]] assigns the
>>> results to elements 1 to 11 of the list "spout". Just a low trick.
>>>
>>> 1a) Yes, you can create a "wrapper" function that will return only the
>>> statistic and p.value.
>>>
>>> # create a function that returns only the
>>> # statistic and p.value as a string
>>> archStatP<-function(x) {
>>>  archout<-ArchTest(x)
>>>  return(sprintf("ChiSq = %f, p = %f",archout$statistic,archout$p.value))
>>> }
>>> # using "lapply", run the test on each column
>>> spout<-lapply(sp_8_5[,2:12],archStatP)
>>>
>>> Note that I should have used "lapply". I didn't check the output
>>> carefully enough.
>>>
>>> 2a) Now you only have to separate the strings in "spout" with TAB
>>> characters and import the result into Excel. I have to wash the dishes, so
>>> you're on your own.
>>>
>>> Jim
>>>
>>> On Fri, May 8, 2020 at 8:26 PM Subhamitra Patra <
>>> subhamitra.patra at gmail.com> wrote:
>>>
>>>> Dear Sir,
>>>>
>>>> Thank you very much for such an excellent solution to my problem. I was
>>>> trying sapply function since last days, but was really unable to write
>>>> properly. Now, I understood my mistake in using sapply function in the
>>>> code. Therefore, I have two queries regarding this which I want to discuss
>>>> here just for my learning purpose.
>>>>
>>>> 1. While using sapply function for estimating one method across the
>>>> columns of a data frame, one needs to define the list of the output table
>>>> after using sapply so that the test results for each column will be
>>>> consistently stored in an output object, right?
>>>>
>>>> 2. In the spout<- list() command, what spout[[i-1]]  indicates?
>>>>
>>>> Sir, one more possibility which I would like to ask related to my above
>>>> problem just to learn for further R programming language.
>>>>
>>>> After running your suggested code, all the results for each column are
>>>> being stored in the spout object. From this, I need only the statistics and
>>>> P-value for each column. So, my queries are:
>>>>
>>>> 1. Is there any way to extract only two values (i.e., statistics and
>>>> p-value) for each column that stored in spout object and save these two
>>>> values in another R data frame for each column?
>>>>  or
>>>> 2. Is there any possibility that the statistics and p-value
>>>> calculated for each column can directly export to a word file in a table
>>>> format (having 4 columns and 3 rows). In particular, is it possible to
>>>> extract both statistic and p-value results for each column to an MS word
>>>> file with the format of A1, A2, A3, A4 column results in 1st row, A5, A6,
>>>> A7, A8 column results in 2nd row, and A9, A10, A11, A12 column results in
>>>> the 3rd row of the table?
>>>>
>>>>
>>>> Like before, your suggestion will definitely help me to learn the
>>>> advanced R language.
>>>>
>>>> Thank you very much for your help.
>>>>
>>>> [image: Mailtrack]
>>>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
>>>> notified by
>>>> Mailtrack
>>>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 05/08/20,
>>>> 03:47:26 PM
>>>>
>>>> On Fri, May 8, 2020 at 2:37 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>>>>
>>>>> Hi Subhamitra,
>>>>> This isn't too hard:
>>>>>
>>>>> # read in the sample data that was
>>>>> # saved in the file "sp_8_5.tab"
>>>>> sp_8_5<-read.table("sp_8_5.tab",sep="\t",
>>>>>  header=TRUE,stringsAsFactors=FALSE)
>>>>> library(tseries)
>>>>> library(FinTS)
>>>>> # using "sapply", run the test on each column
>>>>> spout<-sapply(sp_8_5[,2:12],ArchTest)
>>>>>
>>>>> The list "spout" contains the test results. If you really want to use
>>>>> a loop:
>>>>>
>>>>> spout<-list()
>>>>> for(i in 2:12) spout[[i-1]]<-ArchTest(sp_8_5[,i])
>>>>>
>>>>> Jim
>>>>>
>>>>>
>>>>> On Fri, May 8, 2020 at 5:27 PM Subhamitra Patra <
>>>>> subhamitra.patra at gmail.com> wrote:
>>>>>
>>>>>> Dear Sir,
>>>>>>
>>>>>> Herewith I am pasting a part of my sample data having 12 columns
>>>>>> below, and want to calculate ARCH test for the 12 columns by using a loop.
>>>>>>
>>>>>>
>>>>
>>>> --
>>>> *Best Regards,*
>>>> *Subhamitra Patra*
>>>> *Phd. Research Scholar*
>>>> *Department of Humanities and Social Sciences*
>>>> *Indian Institute of Technology, Kharagpur*
>>>> *INDIA*
>>>>
>>>
>>
>> --
>> *Best Regards,*
>> *Subhamitra Patra*
>> *Phd. Research Scholar*
>> *Department of Humanities and Social Sciences*
>> *Indian Institute of Technology, Kharagpur*
>> *INDIA*
>>
>

-- 
*Best Regards,*
*Subhamitra Patra*
*Phd. Research Scholar*
*Department of Humanities and Social Sciences*
*Indian Institute of Technology, Kharagpur*
*INDIA*

	[[alternative HTML version deleted]]


From pro|jcn@@h @end|ng |rom gm@||@com  Wed May 13 14:42:23 2020
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Wed, 13 May 2020 08:42:23 -0400
Subject: [R] Fitting Richards' curve
In-Reply-To: <A9518FD1-4E6D-4BF2-95D2-9F165CAD38FA@gmail.com>
References: <CA+dpOJmuzENh8zusk3hhb3Kpa6kpkcpQTRZ1H1S3Z0rp-ZSztw@mail.gmail.com>
 <A9518FD1-4E6D-4BF2-95D2-9F165CAD38FA@gmail.com>
Message-ID: <05c6b23a-277d-e3ee-e9bc-384f0f886189@gmail.com>

The Richards' curve is analytic, so nlsr::nlxb() should work better than nls() for getting derivatives --
the dreaded "singular gradient" error will likely stop nls(). Also likely, since even a 3-parameter
logistic can suffer from it (my long-standing Hobbs weed infestation problem below), is
that the Jacobian will be near-singular. And badly scaled. Nonlinear fitting problems essentially
have different scale in different portions of the parameter space.

You may also want to "fix" or mask one or more parameters to reduce the dimensionality of the problem,
and nlsr::nlxb() can do that.

The Hobbs problem has the following 12 data values for time points 1:12

# Data for Hobbs problem
ydat  <-  c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443,
          38.558, 50.156, 62.948, 75.995, 91.972) # for testing
tdat  <-  seq_along(ydat) # for testing

An unscaled model is

eunsc  <-   y ~ b1/(1+b2*exp(-b3*tt))

This problem looks simple, but has given lots of software grief over nearly 5 decades. In 1974 an
extensive search had all commonly available software failing, which led to the code that evolved
into nlsr, though there are plenty of cases where really awful code will luckily find a good
solution. The issue is getting a solution and knowing it is reasonable. I suspect a Richards'
model will be more difficult unless the OP has a lot of data and maybe some external information
to fix or constrain some parameters.

JN


On 2020-05-13 5:41 a.m., Peter Dalgaard wrote:
> Shouldn't be hard to set up with nls(). (I kind of suspect that the Richards curve has more flexibility than data can resolve, especially the subset (Q,B,nu) seems highly related, but hey, it's your data...)
> 
> -pd 
> 
>> On 13 May 2020, at 11:26 , Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
>>
>> Hi,
>>
>> Is there any R package to fit Richards' curve in the form of
>> https://en.wikipedia.org/wiki/Generalised_logistic_function
>>
>> I found there is one package grofit, but currently defunct.
>>
>> Any pointer appreciated.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From m@n|@hmukherjee @end|ng |rom hotm@||@com  Wed May 13 15:33:03 2020
From: m@n|@hmukherjee @end|ng |rom hotm@||@com (Manish Mukherjee)
Date: Wed, 13 May 2020 13:33:03 +0000
Subject: [R] Extracting the first currency value from PDF files
Message-ID: <MA1PR01MB3513376769EBEFABDAB09D05B9BF0@MA1PR01MB3513.INDPRD01.PROD.OUTLOOK.COM>

Hi All,

Need some help with the following code , i have a number of pdf files , and the first page of those files gives a currency value $xxx,xxx,xxx . How to extract this value from a number of PDF files and put it in a data frame . I am able to do it for a single file
with the code where opinions is the text data and 1 is the first currency value
```
d=str_nth_currency(opinions, 1)
df = subset(d, select = c(amount) )
df

I want this to loop over multiple pdf files

I have tried somesthing like this but not working
for (i in 1:length(files)){
  print(i)
  pdf_text(paste("filepath ", files[i],sep = ""))
  str_nth_currency(files[i], 1)
}


Please help.

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed May 13 15:44:14 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 13 May 2020 06:44:14 -0700
Subject: [R] Extracting the first currency value from PDF files
In-Reply-To: <MA1PR01MB3513376769EBEFABDAB09D05B9BF0@MA1PR01MB3513.INDPRD01.PROD.OUTLOOK.COM>
References: <MA1PR01MB3513376769EBEFABDAB09D05B9BF0@MA1PR01MB3513.INDPRD01.PROD.OUTLOOK.COM>
Message-ID: <7555D8CD-0D2F-4DC5-8B39-17AA2EF21BA4@dcn.davis.ca.us>

PDF files are actually "programs" that place graphic symbols on pages, and the order in which those symbols are placed (the order in which most pdf-to-text conversions return characters) may have nothing to do with how they appear visually. There is not even a guarantee that those symbols are represented as characters in the file... they could be part of embedded bitmaps.

In summary, you need to review what your "pdf_text" function is able to extract from your files without filtering... it may or may not be consistent enough to allow you to do what you want... and we certainly have no idea what it is able to extract from your files.

On May 13, 2020 6:33:03 AM PDT, Manish Mukherjee <manishmukherjee at hotmail.com> wrote:
>Hi All,
>
>Need some help with the following code , i have a number of pdf files ,
>and the first page of those files gives a currency value $xxx,xxx,xxx .
>How to extract this value from a number of PDF files and put it in a
>data frame . I am able to do it for a single file
>with the code where opinions is the text data and 1 is the first
>currency value
>```
>d=str_nth_currency(opinions, 1)
>df = subset(d, select = c(amount) )
>df
>
>I want this to loop over multiple pdf files
>
>I have tried somesthing like this but not working
>for (i in 1:length(files)){
>  print(i)
>  pdf_text(paste("filepath ", files[i],sep = ""))
>  str_nth_currency(files[i], 1)
>}
>
>
>Please help.
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From jr@| @end|ng |rom po@teo@no  Wed May 13 16:17:06 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Wed, 13 May 2020 16:17:06 +0200
Subject: [R] Extracting the first currency value from PDF files
In-Reply-To: <7555D8CD-0D2F-4DC5-8B39-17AA2EF21BA4@dcn.davis.ca.us>
References: <MA1PR01MB3513376769EBEFABDAB09D05B9BF0@MA1PR01MB3513.INDPRD01.PROD.OUTLOOK.COM>
 <7555D8CD-0D2F-4DC5-8B39-17AA2EF21BA4@dcn.davis.ca.us>
Message-ID: <20200513141706.GA1705207@posteo.no>

On 2020-05-13 06:44 -0700, Jeff Newmiller wrote:
> On May 13, 2020 6:33:03 AM PDT, Manish Mukherjee wrote:
> > 
> > How to extract this value from a number 
> > of PDF files and put it in a data frame. 
> 
> they could be part of embedded bitmaps.

Dear Manish and Jeff,

I recently found the programs pdftoppm [1] 
and Google tesseract [2] to be really useful 
when reading text from pdfs formatted as "a 
single column of text of variable sizes", 
e.g. a receipt from a grocery store :)

folder <- "path/to/pdfs"
pdfs <- list.files(folder, ".pdf$")
pdf <- pdfs[1]
cmd <-
  paste0("pdftoppm -png -r 500 ",
         folder, pdf, " /tmp/out && ",
         "tesseract /tmp/out-1.png - ",
         "-l nor --psm 4")
lines <- system(cmd, intern=TRUE)
# x <- lapply(x, system, intern=TRUE)
# names(x) <- pdfs
# saveRDS(x, "texts.rds")

In any other case with a sensibly formatted 
pdf, I would have used pdftotext [3] ...

Best,
Rasmus

[1] https://manpages.debian.org/buster/poppler-utils/pdftoppm.1.en.html
[2] https://manpages.debian.org/buster/tesseract-ocr/tesseract.1.en.html
[3] https://manpages.debian.org/buster/poppler-utils/pdftotext.1.en.html


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Wed May 13 17:05:59 2020
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard Comcast)
Date: Wed, 13 May 2020 11:05:59 -0400
Subject: [R] Fitting Richards' curve
In-Reply-To: <05c6b23a-277d-e3ee-e9bc-384f0f886189@gmail.com>
References: <05c6b23a-277d-e3ee-e9bc-384f0f886189@gmail.com>
Message-ID: <C80AB811-BABB-4FD0-B47F-D3BAC2578317@comcast.net>

John, have you ever looked at interval optimization as an alternative since it can lead to provably global minima?

Bernard
Sent from my iPhone so please excuse the spelling!"

> On May 13, 2020, at 8:42 AM, J C Nash <profjcnash at gmail.com> wrote:
> 
> ?The Richards' curve is analytic, so nlsr::nlxb() should work better than nls() for getting derivatives --
> the dreaded "singular gradient" error will likely stop nls(). Also likely, since even a 3-parameter
> logistic can suffer from it (my long-standing Hobbs weed infestation problem below), is
> that the Jacobian will be near-singular. And badly scaled. Nonlinear fitting problems essentially
> have different scale in different portions of the parameter space.
> 
> You may also want to "fix" or mask one or more parameters to reduce the dimensionality of the problem,
> and nlsr::nlxb() can do that.
> 
> The Hobbs problem has the following 12 data values for time points 1:12
> 
> # Data for Hobbs problem
> ydat  <-  c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443,
>          38.558, 50.156, 62.948, 75.995, 91.972) # for testing
> tdat  <-  seq_along(ydat) # for testing
> 
> An unscaled model is
> 
> eunsc  <-   y ~ b1/(1+b2*exp(-b3*tt))
> 
> This problem looks simple, but has given lots of software grief over nearly 5 decades. In 1974 an
> extensive search had all commonly available software failing, which led to the code that evolved
> into nlsr, though there are plenty of cases where really awful code will luckily find a good
> solution. The issue is getting a solution and knowing it is reasonable. I suspect a Richards'
> model will be more difficult unless the OP has a lot of data and maybe some external information
> to fix or constrain some parameters.
> 
> JN
> 
> 
>> On 2020-05-13 5:41 a.m., Peter Dalgaard wrote:
>> Shouldn't be hard to set up with nls(). (I kind of suspect that the Richards curve has more flexibility than data can resolve, especially the subset (Q,B,nu) seems highly related, but hey, it's your data...)
>> 
>> -pd 
>> 
>>>> On 13 May 2020, at 11:26 , Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
>>> 
>>> Hi,
>>> 
>>> Is there any R package to fit Richards' curve in the form of
>>> https://en.wikipedia.org/wiki/Generalised_logistic_function
>>> 
>>> I found there is one package grofit, but currently defunct.
>>> 
>>> Any pointer appreciated.
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pro|jcn@@h @end|ng |rom gm@||@com  Wed May 13 17:11:48 2020
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Wed, 13 May 2020 11:11:48 -0400
Subject: [R] Fitting Richards' curve
In-Reply-To: <C80AB811-BABB-4FD0-B47F-D3BAC2578317@comcast.net>
References: <05c6b23a-277d-e3ee-e9bc-384f0f886189@gmail.com>
 <C80AB811-BABB-4FD0-B47F-D3BAC2578317@comcast.net>
Message-ID: <cb1776b5-5716-5379-ab84-7248881bf165@gmail.com>

Many moons ago (I think early 80s) I looked at some of the global optimizers,
including several based on intervals. For problems of this size, your suggestion
makes a lot of sense, though it has been so long since I looked at those techniques
that I will avoid detailed comment.

I've not looked to see if there are any such solvers for R, but would be happy
to learn (probably best off-list). Also I'm willing to work at a modest pace on
developing one. A starting point might be nls2 package.

Best, JN

On 2020-05-13 11:05 a.m., Bernard Comcast wrote:
> John, have you ever looked at interval optimization as an alternative since it can lead to provably global minima?
> 
> Bernard
> Sent from my iPhone so please excuse the spelling!"
> 
>> On May 13, 2020, at 8:42 AM, J C Nash <profjcnash at gmail.com> wrote:
>>
>> ?The Richards' curve is analytic, so nlsr::nlxb() should work better than nls() for getting derivatives --
>> the dreaded "singular gradient" error will likely stop nls(). Also likely, since even a 3-parameter
>> logistic can suffer from it (my long-standing Hobbs weed infestation problem below), is
>> that the Jacobian will be near-singular. And badly scaled. Nonlinear fitting problems essentially
>> have different scale in different portions of the parameter space.
>>
>> You may also want to "fix" or mask one or more parameters to reduce the dimensionality of the problem,
>> and nlsr::nlxb() can do that.
>>
>> The Hobbs problem has the following 12 data values for time points 1:12
>>
>> # Data for Hobbs problem
>> ydat  <-  c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443,
>>          38.558, 50.156, 62.948, 75.995, 91.972) # for testing
>> tdat  <-  seq_along(ydat) # for testing
>>
>> An unscaled model is
>>
>> eunsc  <-   y ~ b1/(1+b2*exp(-b3*tt))
>>
>> This problem looks simple, but has given lots of software grief over nearly 5 decades. In 1974 an
>> extensive search had all commonly available software failing, which led to the code that evolved
>> into nlsr, though there are plenty of cases where really awful code will luckily find a good
>> solution. The issue is getting a solution and knowing it is reasonable. I suspect a Richards'
>> model will be more difficult unless the OP has a lot of data and maybe some external information
>> to fix or constrain some parameters.
>>
>> JN
>>
>>
>>> On 2020-05-13 5:41 a.m., Peter Dalgaard wrote:
>>> Shouldn't be hard to set up with nls(). (I kind of suspect that the Richards curve has more flexibility than data can resolve, especially the subset (Q,B,nu) seems highly related, but hey, it's your data...)
>>>
>>> -pd 
>>>
>>>>> On 13 May 2020, at 11:26 , Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
>>>>
>>>> Hi,
>>>>
>>>> Is there any R package to fit Richards' curve in the form of
>>>> https://en.wikipedia.org/wiki/Generalised_logistic_function
>>>>
>>>> I found there is one package grofit, but currently defunct.
>>>>
>>>> Any pointer appreciated.
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Wed May 13 17:15:50 2020
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard Comcast)
Date: Wed, 13 May 2020 11:15:50 -0400
Subject: [R] Fitting Richards' curve
In-Reply-To: <05c6b23a-277d-e3ee-e9bc-384f0f886189@gmail.com>
References: <05c6b23a-277d-e3ee-e9bc-384f0f886189@gmail.com>
Message-ID: <4F3563DB-CBEB-4E0B-83D0-DB99EB3A5006@comcast.net>

Also, in the full curve referenced on Wikpedia, the parameters Q And M are confounded - you only need one or the other But not both. If you are using both and trying to estimate them both you will have problems.

I have fitted these curves quite easily using the Solver in Excel.

Bernard
Sent from my iPhone so please excuse the spelling!"

> On May 13, 2020, at 8:42 AM, J C Nash <profjcnash at gmail.com> wrote:
> 
> ?The Richards' curve is analytic, so nlsr::nlxb() should work better than nls() for getting derivatives --
> the dreaded "singular gradient" error will likely stop nls(). Also likely, since even a 3-parameter
> logistic can suffer from it (my long-standing Hobbs weed infestation problem below), is
> that the Jacobian will be near-singular. And badly scaled. Nonlinear fitting problems essentially
> have different scale in different portions of the parameter space.
> 
> You may also want to "fix" or mask one or more parameters to reduce the dimensionality of the problem,
> and nlsr::nlxb() can do that.
> 
> The Hobbs problem has the following 12 data values for time points 1:12
> 
> # Data for Hobbs problem
> ydat  <-  c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443,
>          38.558, 50.156, 62.948, 75.995, 91.972) # for testing
> tdat  <-  seq_along(ydat) # for testing
> 
> An unscaled model is
> 
> eunsc  <-   y ~ b1/(1+b2*exp(-b3*tt))
> 
> This problem looks simple, but has given lots of software grief over nearly 5 decades. In 1974 an
> extensive search had all commonly available software failing, which led to the code that evolved
> into nlsr, though there are plenty of cases where really awful code will luckily find a good
> solution. The issue is getting a solution and knowing it is reasonable. I suspect a Richards'
> model will be more difficult unless the OP has a lot of data and maybe some external information
> to fix or constrain some parameters.
> 
> JN
> 
> 
>> On 2020-05-13 5:41 a.m., Peter Dalgaard wrote:
>> Shouldn't be hard to set up with nls(). (I kind of suspect that the Richards curve has more flexibility than data can resolve, especially the subset (Q,B,nu) seems highly related, but hey, it's your data...)
>> 
>> -pd 
>> 
>>>> On 13 May 2020, at 11:26 , Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
>>> 
>>> Hi,
>>> 
>>> Is there any R package to fit Richards' curve in the form of
>>> https://en.wikipedia.org/wiki/Generalised_logistic_function
>>> 
>>> I found there is one package grofit, but currently defunct.
>>> 
>>> Any pointer appreciated.
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jr@| @end|ng |rom po@teo@no  Wed May 13 17:28:39 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Wed, 13 May 2020 17:28:39 +0200
Subject: [R] solve() function freezes CLI in GNU R 3.6.3
In-Reply-To: <7754f2dc-33a5-ff7b-4867-ff0a3fe0e613@gmail.com>
References: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
 <7754f2dc-33a5-ff7b-4867-ff0a3fe0e613@gmail.com>
Message-ID: <20200513152839.GA1776058@posteo.no>

On 2020-05-09 11:40 -0400, J C Nash wrote:
> 
> > solve(D)
>      [,1] [,2]
> [1,] -2.0  1.0
> [2,]  1.5 -0.5
> > D %*% solve(D)
>      [,1]         [,2]
> [1,]    1 1.110223e-16
> [2,]    0 1.000000e+00
> >

Dear list,

I get another solution on my Linux i7-7500U 
laptop, but the same solution on my FreeBSD 
E3-1240Lv5 machine with a really old R 
version (without BLAS) ...

How is this possible?

> D %*% solve(D)
             [,1] [,2]
[1,] 1.000000e+00    0
[2,] 8.881784e-16    1
> sessionInfo()
R version 3.6.3 (2020-02-29)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Arch Linux

Matrix products: default
BLAS:   /usr/lib/libopenblasp-r0.3.9.so
LAPACK: /usr/lib/liblapack.so.3.9.0

locale:
 [1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=en_DK.UTF-8        LC_COLLATE=en_GB.UTF-8    
 [5] LC_MONETARY=nb_NO.UTF-8    LC_MESSAGES=en_GB.UTF-8   
 [7] LC_PAPER=nb_NO.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=nb_NO.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

loaded via a namespace (and not attached):
[1] compiler_3.6.3
> 

>From the machine running FreeBSD:

> D %*% solve(D)
     [,1]         [,2]
[1,]    1 1.110223e-16
[2,]    0 1.000000e+00
> sessionInfo()
R version 3.5.2 (2018-12-20)
Platform: amd64-portbld-freebsd12.0 (64-bit)
Running under: FreeBSD hmm 12.0-RELEASE FreeBSD 12.0-RELEASE r341666 GENERIC  amd64

Matrix products: default
LAPACK: /usr/local/lib/R/lib/libRlapack.so

locale:
[1] C/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] compiler_3.5.2
>

Best,
Rasmus


From Po||ngW @end|ng |rom @etn@@com  Wed May 13 18:13:13 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Wed, 13 May 2020 16:13:13 +0000
Subject: [R] Help with Radius problem --update
Message-ID: <BYAPR06MB53831A2287FAA73544C21AEFAEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>

Hello all. Thank you in advance for any additional suggestions.

I have with, Jim's help, found some traction in my pursuit of this problem. "determine the largest concentration of members in the smallest radius"
However, I need guidance in efficiencies as I will explain below.

1. I have used Jim's routine to map ALL Member density -- Very helpful visualization

geomat<-makeDensityMatrix(geo1[,c("latitude","longitude")],
                          xlim=range(geo1$longitude),ylim=range(geo1$latitude))
# Range of density (>0) - 1.119816 3.922018
latlim<-range(geo1$latitude)
lonlim<-range(geo1$longitude)

#library(maps) Now in pkgs 


map("world",xlim=lonlim,ylim=latlim)
axis(1)
axis(2)
densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
            red=c(0.5,1),green=0,blue=0,pch=15)
            title("All Member Geo Density Plot")

2. Then using this reference -->  https://stackoverflow.com/questions/60141556/how-to-calculate-distance-between-points-from-a-reference-point-using-r and the sf pkg functions
3. And identifying and using a single point in my data located in the most dense city "Brooklyn" I have created a distance column in miles with Brooklyn as reference point

str(geo1)

individual_dets_sf <- st_as_sf(geo1, coords = c("longitude", "latitude"),
                               crs = 4326) %>% 
  ungroup()
brooklyn <- data.frame("longitude" = -73.973516, "latitude" = 40.57672)
brooklyn_sf <- st_as_sf(brooklyn, 
                       coords = c("longitude", "latitude"),
                       crs = 4326,
                       remove = FALSE) 

individual_dets_sf_2 <- individual_dets_sf %>% 
  mutate(distances = st_distance(., brooklyn_sf, by_element = TRUE))

individual_dets_sf_2$distances

View(individual_dets_sf_2)

individual_dets_sf_3 <- as.data.frame(individual_dets_sf_2)

4. Then I did the following;

I arbitrarily created 8 Radius routines 100 Mi through 2000k Mi
Example:
#Radius1----

individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,distances_Mi) %>% 
  group_by(distances_Mi, MBR_SUBSCRIBERID) %>% 
  filter(distances_Mi <= 2000) %>%
  tally()


Then I used the tibble info from the console to manually calculate the percent of members covered and missed
# A tibble: 2226 x 3 
#2226/2352 = 0.9464286-1 =-0.0535714 missed

 My optimal Radius by the way is 1200k Mi. which I mapped like I did for All members above

My new question is:

Is there a more elegant way to output something like this that I created manually and entered into excel

Radius Distance	                                 Members Covered	Percent Covered	Percent Missed		
R1              2000Mi	                            2226	                       0.9464286	               0.0535714		                                              Missed Diff From Radius2
R2              1500Mi  	                            2126	                       0.9039116	               0.0960884		                                              
R3              1200Mi	                            1990	                       0.8460884	               0.1539116        Optimal	                                                                           0.0578232
R4              1000Mi	                            1644	                       0.6989796	               0.3010204		                                                                           0.204932
R5                900Mi	                            1573	                       0.6687925	               0.3312075		
R6                700Mi	                            1198	                       0.5093537	               0.4906463	Break Even point 	
R7                500Mi	                              972	                       0.4132653	               0.4132653		
R8                100Mi	                              356	                       0.1513605	               0.1513605		
 

Isn't there a better way to do all this below rather than manually? See sample data below please?
Here is where I would like to create some efficiencies please.

#Radius1----

individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,distances_Mi) %>% 
  group_by(distances_Mi,MBR_SUBSCRIBERID) %>% 
  filter(distances_Mi <= 2000) %>%
  tally()
#summarize(mean_size = mean(Prov_ActiveMonths))
## A tibble: 2226 x 3 2226/2352 = 0.9464286-1 =-0.0535714 missed


#Radius2----

individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,distances_Mi) %>% 
  group_by(distances_Mi,MBR_SUBSCRIBERID) %>% 
  filter(distances_Mi <= 1500) %>%
  tally()
#summarize(mean_size = mean(Prov_ActiveMonths))
## A tibble: 2126 x 3 2126/2352 = 0.9039116-1 =-0.0960884 missed

#Radius3----

individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,distances_Mi) %>% 
  group_by(distances_Mi,MBR_SUBSCRIBERID) %>% 
  filter(distances_Mi <= 1200) %>%
  tally()
#summarize(mean_size = mean(Prov_ActiveMonths))
## A tibble: 1990 x 3 1990/2352 = 0.8460884-1 =-0.1539116 missed


#Radius4----

individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,distances_Mi) %>% 
  group_by(distances_Mi,MBR_SUBSCRIBERID) %>% 
  filter(distances_Mi <= 1000) %>%
  tally()
#summarize(mean_size = mean(Prov_ActiveMonths))
## A tibble: 1644 x 3 1644/2352 = 0.6989796-1 =--0.3010204 missed

#Radius5----

individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,distances_Mi) %>% 
  group_by(distances_Mi,MBR_SUBSCRIBERID) %>% 
  filter(distances_Mi <= 900) %>%
  tally()
#summarize(mean_size = mean(Prov_ActiveMonths))
## A tibble: 1573 x 3 1573/2352 = 0.6687925-1 =--0.3312075 missed

#Radius6----

individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,distances_Mi) %>% 
  group_by(distances_Mi,MBR_SUBSCRIBERID) %>% 
  filter(distances_Mi <= 700) %>%
  tally()
#summarize(mean_size = mean(Prov_ActiveMonths))
## A tibble: 1198 x 3 1198/2352 = 0.5093537-1 =-0.4906463 missed ~ 50/50 at 700 mile radius

#Radius7----

individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,distances_Mi) %>% 
  group_by(distances_Mi,MBR_SUBSCRIBERID) %>% 
  filter(distances_Mi <= 500) %>%
  tally()
#summarize(mean_size = mean(Prov_ActiveMonths))
## A tibble: 972 x 3 972/2352 = 0.4132653-1 =-0.5867347 missed

#Radius8----

individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,distances_Mi) %>% 
  group_by(distances_Mi,MBR_SUBSCRIBERID) %>% 
  filter(distances_Mi <= 100) %>%
  tally()
#summarize(mean_size = mean(Prov_ActiveMonths))
## A tibble: 356 x 2 356/2352 = 0.1513605-1 =-0.8486395 missed
	


dput(sample)
structure(list(state = structure(c(2L, 22L, 10L, 12L, 6L, 22L, 
11L, 22L, 22L, 32L, 19L, 29L, 9L, 9L, 10L, 30L, 33L, 35L, 41L, 
12L, 12L, 36L, 4L, 9L, 8L, 39L, 36L, 36L, 41L, 28L, 12L, 28L, 
28L, 12L, 11L, 2L, 12L, 22L, 32L, 22L, 12L, 10L, 10L, 22L, 10L
), .Label = c("AL", "AR", "AZ", "CA", "CO", "CT", "DC", "DE", 
"FL", "GA", "IA", "IL", "IN", "KS", "KY", "LA", "MA", "MD", "ME", 
"MI", "MN", "MO", "NC", "NE", "NJ", "NM", "NV", "NY", "OH", "OK", 
"OR", "PA", "SC", "SD", "TN", "TX", "UT", "VA", "WA", "WI", "WV"
), class = "factor"), city = structure(c(838L, 1030L, 262L, 218L, 
166L, 973L, 339L, 451L, 660L, 358L, 281L, 1280L, 129L, 223L, 
989L, 721L, 550L, 731L, 1325L, 688L, 1184L, 281L, 759L, 1171L, 
1305L, 587L, 272L, 581L, 263L, 152L, 217L, 152L, 390L, 5L, 571L, 
1006L, 1162L, 939L, 170L, 1033L, 1002L, 586L, 586L, 192L, 586L
), .Label = c("ABBOTTSTOWN", "ABILENE", "ACWORTH", "ADAMS", "ADDISON", 
"ADKINS", "ALBANY", "ALBIA", "ALBUQUERQUE", "ALEXANDRIA", "ALFRED", 
"ALIQUIPPA", "ALISO VIEJO", "ALLEN PARK", "ALLENTOWN", "ALPHA", 
"ALPHARETTA", "ALPINE", "ALTOONA", "AMARILLO", "AMBLER", "AMBRIDGE", 
"AMHERST", "AMITYVILLE", "ANAHIEM", "ANDERSON", "ANDOVER", "ANGIER", 
"ANGLETON", "ANKENY", "ANN ARBOR", "ANZA", "APEX", "APOLLO", 
"ARCADIA", "ARCHDALE", "ARDMORE", "ARGYLE", "ARKANSAS CITY", 
"ARLINGTON", "ARNOLD", "ARTHUR", "ASHEBORO", "ASHEVILLE", "ASHLAND", 
"ASHLAND CITY", "ASHTON", "ASTON", "ATHENS", "ATLANTA", "ATLANTIC BCH", 
"AUBORN", "AUGUSTA", "AURORA", "AUSTELL", "AUSTIN", "AVENTURA", 
"AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN CITY", "BALDWIN PLACE", 
"BALLWIN", "BANGOR", "BARBOURSVILLE", "BARNEGAT", "BARTLETT", 
"BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", "BATESBURG", "BATON ROUGE", 
"BAXTER SPRINGS", "BAY CITY", "BAYONNE", "BAYSIDE", "BAYTOWN", 
"BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER CITY", "BELFAIR", 
"BELLE VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR", "BELLMORE", 
"BELLWOOD", "BELMAR", "BELMONT", "BELVIDERE", "BENNET", "BENTON", 
"BENTONVILLE", "BERLIN", "BESSEMER CITY", "BETHANY", "BETHEL", 
"BETHEL PARK", "BETHESDA", "BETHLEHEM", "BETTENDORF", "BIGLERVILLE", 
"BINGHAMTON", "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN", "BLACKSBURG", 
"BLAINE", "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD", "BLUE BELL", 
"BLUE GRASS", "BLUE POINT", "BLUE SPRINGS", "BOCA RATON", "BODFISH", 
"BOILING SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS", "BONNEY LAKE", 
"BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING GREEN", "BOYERTOWN", 
"BOYNTON BEACH", "BRADENTON", "BRADENVILLE", "BRANDAMORE", "BRANDON", 
"BRANFORD", "BRANSON", "BRASELTON", "BREEZEWOOD", "BRENTON", 
"BRENTWOOD", "BREWSTER", "BRICK", "BRIDGEPORT", "BRIDGETON", 
"BRIGHAM CITY", "BROADDUS", "BROCKPORT", "BROGUE", "BROKEN ARROW", 
"BRONX", "BROOKFIELD", "BROOKHAVEN", "BROOKLYN", "BROWNS MILLS", 
"BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", "BRYAN", "BUCKSPORT", 
"BUDD LAKE", "BUFFALO", "BULVERDE", "BUNKERVILLE", "BURBANK", 
"BURFORD", "BURIEN", "BURLINGTON", "BURR RIDGE", "BURTON", "BUSKIRK", 
"BUTLER", "BUXTON", "BYRON", "CALERA", "CAMBRIDGE", "CAMP HILL", 
"CAMPBELL", "CANAAN", "CANAL WINCHESTER", "CANASTOTA", "CANTON", 
"CARDIFF BY THE SEA", "CARL JUNCTION", "CARLISLE", "CARLTON", 
"CAROL STREAM", "CARROLLTON", "CARSON CITY", "CARTERSVILLE", 
"CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", "CASWELL", "CATO", 
"CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER POINT", "CENTERTON", 
"CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN FALLS", "CHALFONT", 
"CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", "CHARLESTON", 
"CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", "CHESHIRE", 
"CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", "CHILLICOTHE", 
"CHOCTAW", "CICERO", "CINCINNATI", "CLAY", "CLEARWATER", "CLEARWATER BEACH", 
"CLEMMONS", "CLENDENIN", "CLEVELAND", "CLEVELAND HTS", "CLEVES", 
"CLIFFSIDE PARK", "CLIFTON", "CLIFTON PARK", "CLIFTON SPGS", 
"CLINTON", "COACHELLA", "COAL TOWNSHIP", "COATESVILLE", "COLLEGE STATION", 
"COLLINSVILLE", "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE", 
"CONCORD", "CONCORD TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY", 
"CONYERS", "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA", 
"CORNELIUS", "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY", 
"COTTAGE HILLS", "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN", 
"CRANBERRY TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER", 
"CUMBERLAND FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS", 
"DALLAS", "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT", 
"DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR", 
"DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH", 
"DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY", 
"DES MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER", 
"DIXON", "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER", 
"DOYLESTOWN", "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH", 
"DUNCAN", "DUNCANNON", "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE", 
"DURHAM", "DUTTON", "EAGLE GROVE", "EAST AURORA", "EAST CHICAGO", 
"EAST EARL", "EAST HANOVER", "EAST HARTFORD", "EAST HARTLAND", 
"EAST LEROY", "EAST LIVERPOOL", "EAST MEADOW", "EAST NORTHPORT", 
"EAST ORANGE", "EAST PEORIA", "EAST SAINT LOUIS", "EASTON", "EDGERTON", 
"EDISON", "EDMOND", "EGG HBR TWP", "EL DORADO", "EL PASO", "ELDRIDGE", 
"ELGIN", "ELIZABETHVILLE", "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY", 
"ELMHURST", "ELMWOOD PARK", "ELOY", "ELY", "EMERSON", "EMORY", 
"ENDICOTT", "ENGLEWOOD", "ENID", "ENNICE", "ENOLA", "ENUMCLAW", 
"EPHRATA", "ERIE", "ESTERO", "EUREKA SPRINGS", "EVANSVILLE", 
"EVERETT", "EXCELSIOR SPRINGS", "EXTON", "FAIRFAX", "FAIRFIELD", 
"FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", "FALMOUTH", "FAR HILLS", 
"FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", "FAYETTE", "FAYETTEVILLE", 
"FELTON", "FENTON", "FERGUSON", "FIELDALE", "FINDLAY", "FINGERVILLE", 
"FLEMINGTON", "FLINT", "FLORENCE", "FLORESVILLE", "FLORISSANT", 
"FLOVILLA", "FLOWER MOUND", "FLUSHING", "FOGELSVILLE", "FOLSOM", 
"FONTANELLE", "FOREST HILLS", "FOREST PARK", "FORT DODGE", "FORT FAIRFIELD", 
"FORT GAY", "FORT LAUDERDALE", "FORT LEE", "FORT MILL", "FORT MOHAVE", 
"FORT PAYNE", "FORT PIERCE", "FORT SMITH", "FORT WAYNE", "FORT WORTH", 
"FOSTORIA", "FOUNTAIN INN", "FRANKFORT", "FRANKLIN", "FREEDOM", 
"FREEHOLD", "FREEPORT", "FREMONT", "FULLERTON", "FULSHEAR", "FULTON", 
"GADSDEN", "GAFFNEY", "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE", 
"GARDNERS", "GARNETT VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA", 
"GETTYSBURG", "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE", 
"GLEN CARBON", "GLEN COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW", 
"GLENWOOD", "GOLDEN CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM", 
"GOSHEN", "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS", 
"GRANDVIEW", "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS", 
"GRASS VALLEY", "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER", 
"GREENLAWN", "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE", 
"GREER", "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD", 
"HADLEY", "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON", 
"HAMBURG", "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE", 
"HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON", 
"HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL", 
"HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE", 
"HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH", 
"HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE", 
"HIGH HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS", 
"HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE", 
"HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD", 
"HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS", 
"HOT SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON", 
"HUFFMAN", "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON", 
"HUNTINGTON BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE", 
"HURST", "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL", 
"INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY", 
"IRMO", "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP", 
"ISSAQUAH", "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS", 
"JACKSONVILLE", "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE", 
"JEFFERSON", "JEFFERSON CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN", 
"JONESBORO", "JONESPORT", "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE", 
"KANNAPOLIS", "KANSAS CITY", "KATY", "KEARNEY", "KELLERTON", 
"KEMAH", "KENMORE", "KENNESAW", "KENT", "KINGMAN", "KINGS MOUNTAIN", 
"KINGSLAND", "KINGSTON", "KINGWOOD", "KIRKLAND", "KUTZTOWN", 
"LA QUINTA", "LA RUE", "LA VERNE", "LAFAYETTE", "LAKE BLUFF", 
"LAKE IN THE HILL", "LAKE ISABELLA", "LAKE PLACID", "LAKE SAINT LOUIS", 
"LAKE TAPPS", "LAKE VILLAGE", "LAKE WAUKOMIS", "LAKE WORTH", 
"LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD RCH", "LAMBERTVILLE", 
"LANCASTER", "LANDISVILLE", "LANSDALE", "LAREDO", "LARGO", "LAS VEGAS", 
"LATROBE", "LAUDERHILL", "LAWNDALE", "LAWRENCE", "LAWRENCEVILLE", 
"LE MARS", "LE ROY", "LEAGUE CITY", "LEAVENWORTH", "LEAWOOD", 
"LEBANON", "LECOMPTON", "LEECHBURG", "LEES SUMMIT", "LEESBURG", 
"LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", "LEVITTOWN", "LEWES", 
"LEWISTOWN", "LEXINGTON", "LIBERTY TWP", "LILLINGTON", "LINCOLN", 
"LINDENHURST", "LINTON", "LISBON", "LITCHFIELD PK", "LITHIA", 
"LITITZ", "LITTLE EGG HARBOR TO", "LK FOREST PK", "LK PEEKSKILL", 
"LOCKHART", "LOCKWOOD", "LOGAN", "LOMA LINDA", "LONE JACK", "LONG BEACH", 
"LONG LANE", "LONGVIEW", "LORAIN", "LORTON", "LOS ANGELES", "LOUISBURG", 
"LOVES PARK", "LOWER BURRELL", "LOWRY CITY", "LUBBOCK", "LUGOFF", 
"LUMBERTON", "LYNBROOK", "LYNNWOOD", "MACHESNEY PARK", "MACON", 
"MAGNOLIA", "MAHOPAC", "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE", 
"MANASSAS", "MANCHACA", "MANCHESTER", "MANCHESTER TOWNSHIP", 
"MANCHESTER TW", "MANHEIM", "MANITO", "MANLIUS", "MANNINGTON", 
"MANSFIELD", "MANSURA", "MAPLE PARK", "MARBLE FALLS", "MARCELLUS", 
"MARICOPA", "MARIETTA", "MARION", "MARKLEYSBURG", "MARS HILL", 
"MARSHALLTOWN", "MARSHFIELD", "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE", 
"MASON CITY", "MASSEY", "MASSILLON", "MASURY", "MATAWAN", "MATEWAN", 
"MATTAPOISETT", "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND", 
"MC DONALD", "MC KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN", 
"MCALESTER", "MCDONOUGH", "MCKEESPORT", "MCKINNEY", "MECHANICSBURG", 
"MECHANICSVILLE", "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS", 
"MERIDEN", "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA", 
"MESQUITE", "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN", 
"MIDLAND", "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK", 
"MILLEDGEVILLE", "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE", 
"MINEOLA", "MINERAL SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION", 
"MISSION HILLS", "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY", 
"MOLINE", "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE", 
"MONTCLAIR", "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON", 
"MORGANTOWN", "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE", 
"MOUNT JOY", "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO", 
"MOUNT UNION", "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT", 
"MULVANE", "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT", 
"MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS", 
"NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES", 
"NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG", 
"NEW BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK", 
"NEW CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON", 
"NEW LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE", 
"NEW ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK", 
"NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT", 
"NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA", 
"NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK", 
"NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON", 
"NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND HILLS", 
"NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE", 
"NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON", 
"OAK RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE", 
"OMAHA", "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION", 
"ORONO", "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO", 
"OTTAWA", "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD", 
"OYSTER BAY", "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR", 
"PALMDALE", "PALMYRA", "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE", 
"PARLIN", "PARMA", "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY", 
"PAXTON", "PEA RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN", 
"PELION", "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE", 
"PENNSBURG", "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS", 
"PERRYSBURG", "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI", 
"PHOENIX", "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN", 
"PINE BLUFF", "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY", 
"PITMAN", "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY", 
"PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO LAKE", 
"POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", "PORT CHARLOTTE", 
"PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", "PORT WASHINGTON", 
"PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", "POTTSVILLE", 
"POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE VILLAGE", 
"PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT PLEASANT", 
"PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", "QUINCY", 
"RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", "READLYN", 
"RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO PARK", 
"REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", "RICH HILL", 
"RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", "RINCON", 
"RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", "RIVERSIDE", 
"ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", "ROCKAWAY BCH", 
"ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", "ROGERS", 
"ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE", 
"ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT", 
"ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN", 
"RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO", 
"SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR", 
"SAINT CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS", 
"SAINT PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY", 
"SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO", 
"SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN", 
"SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", "SARASOTA", 
"SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", "SAYVILLE", 
"SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", "SCOTT", "SCOTTSDALE", 
"SEAFORD", "SEALE", "SEATTLE", "SEDONA", "SEGUIN", "SELDEN", 
"SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", "SHARON", "SHARPSBURG", 
"SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", "SHELLSBURG", "SHELTON", 
"SHENANDOAH", "SHORELINE", "SHOREWOOD", "SHREVEPORT", "SICKLERVILLE", 
"SILER CITY", "SILOAM SPRINGS", "SIMON", "SIMPSONVILLE", "SIMSBURY", 
"SIOUX CITY", "SIOUX FALLS", "SKOKIE", "SKOWHEGAN", "SLATINGTON", 
"SLIDELL", "SMITHFIELD", "SN BERNRDNO", "SNELLVILLE", "SOCORRO", 
"SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH BELOIT", "SOUTH BEND", 
"SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH PARK", "SOUTH PLAINFIELD", 
"SOUTH SIOUX CITY", "SOUTHAMPTON", "SOUTHBURY", "SOUTHGATE", 
"SPARKS", "SPARROW BUSH", "SPENCER", "SPRING", "SPRING BRANCH", 
"SPRING GROVE", "SPRING HILL", "SPRING HOPE", "SPRING VALLEY", 
"SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD GARDENS", "SPRINGHILL", 
"SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", "ST PETERSBURG", 
"ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", "STANLEY", 
"STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", "STILLMAN VALLEY", 
"STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", "STRAFFORD", 
"STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", "SUFFIELD", 
"SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", "SUMMIT", 
"SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", "SWEENY", 
"SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", "TAFTVILLE", 
"TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", "TAYLORVILLE", 
"TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", "THOMASVILLE", 
"TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY PARK", 
"TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", "TOPEKA", 
"TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS RST", 
"TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER", 
"TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY", 
"UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA", 
"URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO", 
"VAN BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO", 
"VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA", 
"VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH", 
"W TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN", 
"WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG", 
"WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA", 
"WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE", 
"WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER", 
"WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO", 
"WEST CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN", 
"WEST HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH", 
"WEST POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD", 
"WEST VALLEY CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER", 
"WESTMORELAND CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD", 
"WHARTON", "WHEELING", "WHITE PLAINS", "WHITEHALL", "WHITESTONE", 
"WHITESTOWN", "WHITING", "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD", 
"WILLARD", "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK", 
"WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS", 
"WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON", 
"WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE", 
"WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS", 
"WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH", 
"YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS", 
"YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor"), 
    clusters = c(6L, 10L, 3L, 10L, 5L, 6L, 10L, 6L, 6L, 7L, 5L, 
    7L, 8L, 8L, 3L, 6L, 3L, 10L, 7L, 10L, 10L, 4L, 2L, 8L, 9L, 
    1L, 4L, 4L, 7L, 5L, 10L, 5L, 5L, 10L, 6L, 6L, 10L, 6L, 7L, 
    10L, 10L, 3L, 3L, 6L, 3L), Longitude = c(-93.883554, -90.582058, 
    -83.868923, -89.544083, -72.902467, -94.458904, -90.588533, 
    -94.527843, -92.93769, -80.029456, -70.58809, -82.888789, 
    -80.113071, -82.6661, -81.372992, -95.840964, -82.016013, 
    -89.92625, -80.276676, -89.778385, -87.641063, -94.888036, 
    -117.663119, -82.530568, -75.574437, -122.209047, -96.799309, 
    -95.746255, -80.553953, -73.923314, -87.713482, -73.9731, 
    -73.8187, -87.987023, -93.759478, -94.110903, -90.00597, 
    -93.257109, -79.932483, -90.201858, -89.100233, -84.625923, 
    -84.567614, -93.912921, -84.612564), Latitude = c(34.503045, 
    38.752515, 33.406527, 40.893588, 41.733088, 38.999032, 41.657674, 
    38.890929, 37.639766, 42.099693, 43.53462, 40.142492, 26.538149, 
    27.9062, 31.90104, 34.833083, 35.118836, 35.136047, 39.425712, 
    40.519207, 40.101126, 30.044457, 33.62287, 27.910276, 39.752254, 
    47.403778, 32.961929, 29.820199, 38.408649, 40.668828, 41.983948, 
    40.58116, 40.7253, 41.921667, 41.669921, 36.442552, 38.564663, 
    37.455315, 40.867774, 38.783554, 42.244381, 34.024332, 34.001644, 
    36.622704, 34.017832), distances_Me = c(1879089.079, 1437296.523, 
    1185638.742, 1313997.97, 156759.9125, 1759443.351, 1398398.997, 
    1767861.917, 1668917.322, 534234.2576, 431749.2965, 758508.4701, 
    1657031.938, 1615575.668, 1169085.702, 2024642.021, 930944.4681, 
    1525322.746, 553069.7532, 1337002.822, 1161367.37, 2223496.376, 
    3918520.212, 1609088.602, 164252.5146, 3878581.001, 2197910.688, 
    2304526.789, 614908.3151, 11075.24597, 1160375.454, 494.2997496, 
    21063.72622, 1182681.434, 1662351.873, 1810881.911, 1393581.436, 
    1701982.013, 504395.772, 1404348.991, 1276505.454, 1190457.718, 
    1188029.899, 1787261.336, 1189994.055), distances_Mi = c(1167.607468, 
    893.0913246, 736.719012, 816.477441, 97.40573055, 1093.263337, 
    868.9216123, 1098.494372, 1037.01328, 331.9565399, 268.2755749, 
    471.3135552, 1029.628072, 1003.868436, 726.4334682, 1258.049536, 
    578.4599174, 947.7880794, 343.6603307, 830.7719404, 721.6375366, 
    1381.611443, 2434.846498, 999.8375755, 102.0614003, 2410.029515, 
    1365.713293, 1431.96122, 382.0848884, 6.881813138, 721.0211909, 
    0.30714248, 13.08834388, 734.8814328, 1032.933715, 1125.225657, 
    865.9281298, 1057.55865, 313.4158337, 872.6187534, 793.1807589, 
    739.7133739, 738.2048023, 1110.548567, 739.4252678), ID = 2308:2352), row.names = c(NA, 
-45L), class = "data.frame")























Proprietary

-----Original Message-----
From: Jim Lemon <drjimlemon at gmail.com> 
Sent: Wednesday, May 13, 2020 1:39 AM
To: Poling, William <PolingW at aetna.com>
Cc: r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with Radius problem

**** External Email - Use Caution ****

Hi Bill,
A while ago I devised a couple of functions to accumulate millions of geographic locations of events and then display the resulting matrix of values on an existing plot. This may be of use to you, at least in the visualization of the density of the locations. As your example data only included a few points, the resulting plot looks pretty chunky as I had to use a large symbol to make the cells with non-zero counts obvious.

# read in your sample data
source("wp_data.R")
library(plotrix)
geomat<-makeDensityMatrix(geodat[,c("latitude","longitude")],
xlim=range(geodat$longitude),ylim=range(geodat$latitude))
# Range of density (>0) - 1.119816 3.922018
latlim<-range(geodat$latitude)
lonlim<-range(geodat$longitude)
library(maps)
map("world",xlim=lonlim,ylim=latlim)
axis(1)
axis(2)
densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
 red=c(0.5,1),green=0,blue=0,pch=15)

Jim

On Wed, May 13, 2020 at 10:57 AM Poling, William via R-help <r-help at r-project.org> wrote:
>
> #RStudio Version Version 1.2.1335
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit) #Running under: Windows 10 
> x64 (build 17763)
>
> Hello I am trying to figure out how to create concentration of ID's in 
> the following data, based on determining the highest concentration in 
> the smallest radius area based on Longitude and Latitude (geo 
> location)
>
> I have reviewed many websites looking for a function or tutorial, (if 
> you are aware of either please let me know)
>

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From pro|jcn@@h @end|ng |rom gm@||@com  Wed May 13 19:04:36 2020
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Wed, 13 May 2020 13:04:36 -0400
Subject: [R] solve() function freezes CLI in GNU R 3.6.3
In-Reply-To: <20200513152839.GA1776058@posteo.no>
References: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
 <7754f2dc-33a5-ff7b-4867-ff0a3fe0e613@gmail.com>
 <20200513152839.GA1776058@posteo.no>
Message-ID: <1c5e1a18-2e87-9507-5156-f5140692e4b8@gmail.com>

Note that my sessionInfo() gave

R version 4.0.0 (2020-04-24)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Linux Mint 19.3

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1
LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.7.1

So you have an older R but newer libraries, and the libblas is
a different one.

Given the output is very similar, and within the rounding
margins of the double arithmetic, this looks like the
libraries are very slightly different. I suppose I should
be more inquisitive and try to seek out the changelog or other
description of the differences, but ...

JN


On 2020-05-13 11:28 a.m., Rasmus Liland wrote:
> On 2020-05-09 11:40 -0400, J C Nash wrote:
>>
>>> solve(D)
>>      [,1] [,2]
>> [1,] -2.0  1.0
>> [2,]  1.5 -0.5
>>> D %*% solve(D)
>>      [,1]         [,2]
>> [1,]    1 1.110223e-16
>> [2,]    0 1.000000e+00
>>>
> 
> Dear list,
> 
> I get another solution on my Linux i7-7500U 
> laptop, but the same solution on my FreeBSD 
> E3-1240Lv5 machine with a really old R 
> version (without BLAS) ...
> 
> How is this possible?
> 
>> D %*% solve(D)
>              [,1] [,2]
> [1,] 1.000000e+00    0
> [2,] 8.881784e-16    1
>> sessionInfo()
> R version 3.6.3 (2020-02-29)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Arch Linux
> 
> Matrix products: default
> BLAS:   /usr/lib/libopenblasp-r0.3.9.so
> LAPACK: /usr/lib/liblapack.so.3.9.0
> 
> locale:
>  [1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C              
>  [3] LC_TIME=en_DK.UTF-8        LC_COLLATE=en_GB.UTF-8    
>  [5] LC_MONETARY=nb_NO.UTF-8    LC_MESSAGES=en_GB.UTF-8   
>  [7] LC_PAPER=nb_NO.UTF-8       LC_NAME=C                 
>  [9] LC_ADDRESS=C               LC_TELEPHONE=C            
> [11] LC_MEASUREMENT=nb_NO.UTF-8 LC_IDENTIFICATION=C       
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base     
> 
> loaded via a namespace (and not attached):
> [1] compiler_3.6.3
>>
> 
> From the machine running FreeBSD:
> 
>> D %*% solve(D)
>      [,1]         [,2]
> [1,]    1 1.110223e-16
> [2,]    0 1.000000e+00
>> sessionInfo()
> R version 3.5.2 (2018-12-20)
> Platform: amd64-portbld-freebsd12.0 (64-bit)
> Running under: FreeBSD hmm 12.0-RELEASE FreeBSD 12.0-RELEASE r341666 GENERIC  amd64
> 
> Matrix products: default
> LAPACK: /usr/local/lib/R/lib/libRlapack.so
> 
> locale:
> [1] C/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> loaded via a namespace (and not attached):
> [1] compiler_3.5.2
>>
> 
> Best,
> Rasmus
>


From jr@| @end|ng |rom po@teo@no  Wed May 13 20:29:33 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Wed, 13 May 2020 20:29:33 +0200
Subject: [R] solve() function freezes CLI in GNU R 3.6.3
In-Reply-To: <1c5e1a18-2e87-9507-5156-f5140692e4b8@gmail.com>
References: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
 <7754f2dc-33a5-ff7b-4867-ff0a3fe0e613@gmail.com>
 <20200513152839.GA1776058@posteo.no>
 <1c5e1a18-2e87-9507-5156-f5140692e4b8@gmail.com>
Message-ID: <20200513182933.GG1776058@posteo.no>

On 2020-05-13 13:04 -0400, J C Nash wrote:
> On 2020-05-13 11:28 a.m., Rasmus Liland wrote:
> > 
> > I get another solution on my Linux i7-7500U 
> > 
> > > D %*% solve(D)
> >              [,1] [,2]
> > [1,] 1.000000e+00    0
> > [2,] 8.881784e-16    1
> > > sessionInfo()
> > BLAS:   /usr/lib/libopenblasp-r0.3.9.so
> > LAPACK: /usr/lib/liblapack.so.3.9.0
> 
> Note that my sessionInfo() gave
> 
> R version 4.0.0 (2020-04-24)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Linux Mint 19.3
> 
> Matrix products: default
> BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1
> LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.7.1
> 
> So you have an older R but newer libraries, and the libblas is
> a different one.
> 
> Given the output is very similar, and within the rounding
> margins of the double arithmetic, this looks like the
> libraries are very slightly different. I suppose I should
> be more inquisitive and try to seek out the changelog or other
> description of the differences, but ...
> 
> JN

Dear JN,

I was thinking BLAS could be changed to 
OpenBLAS, apparently not:

If I switch from OpenBLAS back to regular 
BLAS, the output is as expected ... I thought 
OpenBLAS should be a real alternative to BLAS 
in many cases, but not in this example?

> D %*% solve(D)
     [,1]         [,2]
[1,]    1 1.110223e-16
[2,]    0 1.000000e+00
> sessionInfo()
R version 3.6.3 (2020-02-29)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Arch Linux

Matrix products: default
BLAS:   /usr/lib/libblas.so.3.9.0
LAPACK: /usr/lib/liblapack.so.3.9.0

Best,
Rasmus


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed May 13 20:44:54 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 13 May 2020 11:44:54 -0700
Subject: [R] solve() function freezes CLI in GNU R 3.6.3
In-Reply-To: <20200513182933.GG1776058@posteo.no>
References: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
 <7754f2dc-33a5-ff7b-4867-ff0a3fe0e613@gmail.com>
 <20200513152839.GA1776058@posteo.no>
 <1c5e1a18-2e87-9507-5156-f5140692e4b8@gmail.com>
 <20200513182933.GG1776058@posteo.no>
Message-ID: <CF387B40-317A-4A0D-8B3C-FB322A5E448B@dcn.davis.ca.us>

Depending on reproducibility in the least significant bits of floating point calculations is a bad practice. Just because you decide based on this one example that one implementation of BLAS is better than another does not mean that will be true for all specific examples. IMO you are drawing conclusions on data that is effectively random and should change your definition of "sufficient to the task".

On May 13, 2020 11:29:33 AM PDT, Rasmus Liland <jral at posteo.no> wrote:
>On 2020-05-13 13:04 -0400, J C Nash wrote:
>> On 2020-05-13 11:28 a.m., Rasmus Liland wrote:
>> > 
>> > I get another solution on my Linux i7-7500U 
>> > 
>> > > D %*% solve(D)
>> >              [,1] [,2]
>> > [1,] 1.000000e+00    0
>> > [2,] 8.881784e-16    1
>> > > sessionInfo()
>> > BLAS:   /usr/lib/libopenblasp-r0.3.9.so
>> > LAPACK: /usr/lib/liblapack.so.3.9.0
>> 
>> Note that my sessionInfo() gave
>> 
>> R version 4.0.0 (2020-04-24)
>> Platform: x86_64-pc-linux-gnu (64-bit)
>> Running under: Linux Mint 19.3
>> 
>> Matrix products: default
>> BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1
>> LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.7.1
>> 
>> So you have an older R but newer libraries, and the libblas is
>> a different one.
>> 
>> Given the output is very similar, and within the rounding
>> margins of the double arithmetic, this looks like the
>> libraries are very slightly different. I suppose I should
>> be more inquisitive and try to seek out the changelog or other
>> description of the differences, but ...
>> 
>> JN
>
>Dear JN,
>
>I was thinking BLAS could be changed to 
>OpenBLAS, apparently not:
>
>If I switch from OpenBLAS back to regular 
>BLAS, the output is as expected ... I thought 
>OpenBLAS should be a real alternative to BLAS 
>in many cases, but not in this example?
>
>> D %*% solve(D)
>     [,1]         [,2]
>[1,]    1 1.110223e-16
>[2,]    0 1.000000e+00
>> sessionInfo()
>R version 3.6.3 (2020-02-29)
>Platform: x86_64-pc-linux-gnu (64-bit)
>Running under: Arch Linux
>
>Matrix products: default
>BLAS:   /usr/lib/libblas.so.3.9.0
>LAPACK: /usr/lib/liblapack.so.3.9.0
>
>Best,
>Rasmus
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From Po||ngW @end|ng |rom @etn@@com  Wed May 13 20:46:30 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Wed, 13 May 2020 18:46:30 +0000
Subject: [R] Help with R-Markdown please
In-Reply-To: <e1212a34f6b1dbabeddb85c4ad3cf015eb4c8c16.camel@math.aau.dk>
References: <BYAPR06MB53837B82850FAC0378779843AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <e1212a34f6b1dbabeddb85c4ad3cf015eb4c8c16.camel@math.aau.dk>
Message-ID: <BYAPR06MB5383D57CB539FC1E76498221AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>

Hello all.

I am still struggling with this issue.

It appears that new installations are going to local drive.


# #Test 05/13/2020
# install.packages("abjutils")
# package ?abjutils? successfully unpacked and MD5 sums checked
# 
# The downloaded binary packages are in
# C:\Users\A436798\AppData\Local\Temp\RtmpCuXNJn\downloaded_packages

However, when I run .libPaths() it indicates our UNC path

[1] "\\\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library"


Is there a way for me to change this myself in my instance of R?

Thank you.

WHP 



Proprietary

-----Original Message-----
From: Ege Rubak <rubak at math.aau.dk> 
Sent: Tuesday, May 12, 2020 7:43 AM
To: Poling, William <PolingW at aetna.com>; r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with R-Markdown please

**** External Email - Use Caution ****

Looks like your files are on a Windows network drive (UNC path). I have experienced many problems with this on a Windows laptop from work. If at all possible you should avoid this. As an absolute minimum make sure your R library (collection of installed packages) is on the local file system and not a UNC path. If you are lucky this could be enough to make things work.

You can check your library location(s) with the command .libPaths() in an R session.

Good luck.

Ege

On Tue, 2020-05-12 at 10:15 +0000, Poling, William via R-help wrote:
> #UPDATED 05/05/2020
> #RStudio Version Version 1.2.1335 need this one--> 1.2.5019
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit) #Running under: Windows 10 
> x64 (build 17763)
> 
> Good morning.
> 
> This is the first time I have tried RMarkdown on new laptop with new 
> employer.
> I am not sure how to go about fixing this problem below?
> 
> Would someone please advise course of action.
> 
> Thank you.
> 
> WHP
> 
> 
> 
> processing file: Member-Geo-Location-Test-V1.Rmd
>  
> |..........                                                          
>   |  14%
>   ordinary text without R code
> 
>  
> |....................                                                
>   |  29%
> label: unnamed-chunk-1 (with options) List of 1  $ echo: logi FALSE
> 
>  
> |..............................                                      
>   |  43%
>   ordinary text without R code
> 
>  
> |........................................                            
>   |  57%
> label: setup (with options)
> List of 2
>  $ include: logi FALSE
>  $ warning: logi FALSE
> 
>  
> |..................................................                  
>   |  71%
>   ordinary text without R code
> 
>  
> |............................................................        
>   |  86%
> label: Table1 (with options)
> List of 2
>  $ echo   : logi FALSE
>  $ results: chr "asis"
> 
>  
> |....................................................................
> ..| 100%
>    inline R code fragments
> 
> 
> output file: Member-Geo-Location-Test-V1.knit.md
> 
> "C:/Program Files/RStudio/bin/pandoc/pandoc" +RTS -K512m -RTS Member- 
> Geo-Location-Test-V1.utf8.md --to html4 --from
> markdown+autolink_bare_uris+tex_math_single_backslash+smart --output
> Member-Geo-Location-Test-V1.html --email-obfuscation none --self- 
> contained --standalone --section-divs --template "\\winp-oaf-
> 113\FldrRedir_1$\A436798\Data\R\R-
> 4.0.0patched\library\rmarkdown\rmd\h\default.html" --no-highlight -- 
> variable highlightjs=1 --variable "theme:bootstrap" --include-in- 
> header "C:\Users\A436798\AppData\Local\Temp\RtmpSajZla\rmarkdown-
> str2ae846253313.html" --mathjax --variable "mathjax-url:
> https://urldefense.proofpoint.com/v2/url?u=https-3A__mathjax.rstudio.c
> om_latest_MathJax.js-3Fconfig-3DTeX-2DAMS-2DMML-5FHTMLorMML&d=DwIGaQ&c
> =wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTC
> tKvneM2ExlYvnUWbsByY&m=uBce0mQRO8wdLXO8BDTtjk4R92KVzpjtkQ-NqYiAHCo&s=S
> MNU_B--j9FvBKh-0u1NG5RHd7AtLkvyVsi9ybyUwYU&e=
> " --lua-filter "\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/lua/pagebreak.lua" --lua-filter
> "\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/lua/latex-div.lua" 
> Could not fetch http://?/UNC/winp-oaf-
> 113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/h/default.html
> HttpExceptionRequest Request {
>   host                 = ""
>   port                 = 80
>   secure               = False
>   requestHeaders       = []
>   path                 = "/"
>   queryString          = "?/UNC/winp-oaf-
> 113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/h/default.html"
>   method               = "GET"
>   proxy                = Nothing
>   rawBody              = False
>   redirectCount        = 10
>   responseTimeout      = ResponseTimeoutDefault
>   requestVersion       = HTTP/1.1
> }
>  (InvalidDestinationHost "")
> Error: pandoc document conversion failed with error 61 Execution 
> halted
> 
> WHP
> 
> 
> Proprietary
> 
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may 
> con...{{dropped:16}}
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIGaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmv
> OXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=uBce0mQRO8wdLXO
> 8BDTtjk4R92KVzpjtkQ-NqYiAHCo&s=W5iLmdkyVAbq4AQuAsTYsadWBk1jduptKOZLR25
> jPWo&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIGaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmm
> vOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=uBce0mQRO8wdLX
> O8BDTtjk4R92KVzpjtkQ-NqYiAHCo&s=5V2OtH149c6MxDhlirX-Yd-v8uuEuVYih9DRq-
> ZmBF8&e= and provide commented, minimal, self-contained, reproducible 
> code.
--
Ege Rubak, Associate Professor,
Department of Mathematical Sciences, Aalborg University Skjernvej 4A, 9220 Aalborg East, Denmark
Phone: (+45)99408861
Mobile: (+45)30230252
Email: rubak at math.aau.dk

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From jr@| @end|ng |rom po@teo@no  Wed May 13 20:57:09 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Wed, 13 May 2020 20:57:09 +0200
Subject: [R] solve() function freezes CLI in GNU R 3.6.3
In-Reply-To: <CF387B40-317A-4A0D-8B3C-FB322A5E448B@dcn.davis.ca.us>
References: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
 <7754f2dc-33a5-ff7b-4867-ff0a3fe0e613@gmail.com>
 <20200513152839.GA1776058@posteo.no>
 <1c5e1a18-2e87-9507-5156-f5140692e4b8@gmail.com>
 <20200513182933.GG1776058@posteo.no>
 <CF387B40-317A-4A0D-8B3C-FB322A5E448B@dcn.davis.ca.us>
Message-ID: <20200513185709.GH1776058@posteo.no>

On 2020-05-13 11:44 -0700, Jeff Newmiller wrote:
> Depending on reproducibility in the least 
> significant bits of floating point 
> calculations is a bad practice. Just 
> because you decide based on this one 
> example that one implementation of BLAS is 
> better than another does not mean that will 
> be true for all specific examples. IMO you 
> are drawing conclusions on data that is 
> effectively random and should change your 
> definition of "sufficient to the task".

Dear Jeff,

Right, so I really would have wanted OpenBLAS 
to be as reproducible as regular BLAS in this 
one random example, but my hands remains tied 
on this since I do not know anything about 
BLAS ... 

More interestingly, could you dream up any 
idea as to what might cause this difference?

Best,
Rasmus


From jr@| @end|ng |rom po@teo@no  Wed May 13 21:09:51 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Wed, 13 May 2020 21:09:51 +0200
Subject: [R] Help with R-Markdown please
In-Reply-To: <BYAPR06MB5383D57CB539FC1E76498221AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB53837B82850FAC0378779843AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <e1212a34f6b1dbabeddb85c4ad3cf015eb4c8c16.camel@math.aau.dk>
 <BYAPR06MB5383D57CB539FC1E76498221AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <20200513190951.GI1776058@posteo.no>

On 2020-05-13 18:46 +0000, Poling, William via R-help wrote:
> Hello all.
> 
> I am still struggling with this issue.
> 
> It appears that new installations are going 
> to local drive.
> 
> # #Test 05/13/2020
> # install.packages("abjutils")
> # package ?abjutils? successfully unpacked and MD5 sums checked
> # 
> # The downloaded binary packages are in
> # C:\Users\A436798\AppData\Local\Temp\RtmpCuXNJn\downloaded_packages
> 
> However, when I run .libPaths() it 
> indicates our UNC path
> 
> [1] "\\\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library"
> 
> Is there a way for me to change this myself 
> in my instance of R?

Dear William,

Perhaps you could try to set the variable 
R_LIBS_USER to some local folder.  I do not 
know how to do this on Windows, but there 
seems to be some guys in this rstudio 
discussion thread that came to some sort of a 
conclusion: 
https://community.rstudio.com/t/help-regarding-package-installation-renviron-rprofile-r-libs-r-libs-site-and-r-libs-user-oh-my/13888/8 

Since long ago, I have had this line in my 
~/.zshrc.local  :)

[ -e /usr/bin/Rscript ] && R_LIBS_USER=`Rscript -e 'cat(Sys.getenv("R_LIBS_USER"))'`

Best,
Rasmus


From Po||ngW @end|ng |rom @etn@@com  Wed May 13 21:27:02 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Wed, 13 May 2020 19:27:02 +0000
Subject: [R] Help with R-Markdown please
In-Reply-To: <20200513190951.GI1776058@posteo.no>
References: <BYAPR06MB53837B82850FAC0378779843AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <e1212a34f6b1dbabeddb85c4ad3cf015eb4c8c16.camel@math.aau.dk>
 <BYAPR06MB5383D57CB539FC1E76498221AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <20200513190951.GI1776058@posteo.no>
Message-ID: <BYAPR06MB5383E715E004A64C543041C4AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>

Hi Rasmus, thank you I will see if this is something I can do without IT admin access.

In the mean time I have reloaded rmarkdown. To local

package ?rmarkdown? successfully unpacked and MD5 sums checked

The downloaded binary packages are in
	C:\Users\A436798\AppData\Local\Temp\RtmpYjSy7G\downloaded_packages

library(rmarkdown)

But R is still looking for it in : " Could not fetch http://?/UNC/winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library/rmarkdown/rmd/h/default.html"


More recently I have installed other pkgs for the first time use and they have gone to local like above for rmarkdown and I have used functions from them so they are being searched for correctly I guess?

Very confusing.

Thank you for your response Sir!

WHP

"C:/Program Files/RStudio/bin/pandoc/pandoc" +RTS -K512m -RTS Member-Geo-Location-Test-V1.utf8.md --to html4 --from markdown+autolink_bare_uris+tex_math_single_backslash+smart --output Member-Geo-Location-Test-V1.html --email-obfuscation none --self-contained --standalone --section-divs --template "\\winp-oaf-113\FldrRedir_1$\A436798\Data\R\R-4.0.0patched\library\rmarkdown\rmd\h\default.html" --no-highlight --variable highlightjs=1 --variable "theme:bootstrap" --include-in-header "C:\Users\A436798\AppData\Local\Temp\RtmpglP9GJ\rmarkdown-str314c55414a00.html" --mathjax --variable "mathjax-url:https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" --lua-filter "\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library/rmarkdown/rmd/lua/pagebreak.lua" --lua-filter "\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library/rmarkdown/rmd/lua/latex-div.lua" 
output file: Member-Geo-Location-Test-V1.knit.md

Could not fetch http://?/UNC/winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library/rmarkdown/rmd/h/default.html
HttpExceptionRequest Request {
  host                 = ""
  port                 = 80
  secure               = False
  requestHeaders       = []
  path                 = "/"
  queryString          = "?/UNC/winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library/rmarkdown/rmd/h/default.html"
  method               = "GET"
  proxy                = Nothing
  rawBody              = False
  redirectCount        = 10
  responseTimeout      = ResponseTimeoutDefault
  requestVersion       = HTTP/1.1
}
 (InvalidDestinationHost "")
Error: pandoc document conversion failed with error 61
Execution halted


WHP


Proprietary

-----Original Message-----
From: Rasmus Liland <jral at posteo.no> 
Sent: Wednesday, May 13, 2020 2:10 PM
To: Poling, William <PolingW at aetna.com>
Cc: r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with R-Markdown please

**** External Email - Use Caution ****

On 2020-05-13 18:46 +0000, Poling, William via R-help wrote:
> Hello all.
> 
> I am still struggling with this issue.
> 
> It appears that new installations are going to local drive.
> 
> # #Test 05/13/2020
> # install.packages("abjutils")
> # package ?abjutils? successfully unpacked and MD5 sums checked # # 
> The downloaded binary packages are in # 
> C:\Users\A436798\AppData\Local\Temp\RtmpCuXNJn\downloaded_packages
> 
> However, when I run .libPaths() it
> indicates our UNC path
> 
> [1] "\\\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library"
> 
> Is there a way for me to change this myself in my instance of R?

Dear William,

Perhaps you could try to set the variable R_LIBS_USER to some local folder.  I do not know how to do this on Windows, but there seems to be some guys in this rstudio discussion thread that came to some sort of a
conclusion: 
https://urldefense.proofpoint.com/v2/url?u=https-3A__community.rstudio.com_t_help-2Dregarding-2Dpackage-2Dinstallation-2Drenviron-2Drprofile-2Dr-2Dlibs-2Dr-2Dlibs-2Dsite-2Dand-2Dr-2Dlibs-2Duser-2Doh-2Dmy_13888_8&d=DwIDaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=q_5fam9luSaeNgs-KOUZS-w-YAVaPzQdZgIlejBCDFA&s=NWDnltFH8cjC-pj6K08lWXC1FjfFcyY1mueSZnor9Ng&e=  

Since long ago, I have had this line in my ~/.zshrc.local  :)

[ -e /usr/bin/Rscript ] && R_LIBS_USER=`Rscript -e 'cat(Sys.getenv("R_LIBS_USER"))'`

Best,
Rasmus

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From jr@| @end|ng |rom po@teo@no  Wed May 13 21:49:30 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Wed, 13 May 2020 21:49:30 +0200
Subject: [R] Help with R-Markdown please
In-Reply-To: <BYAPR06MB5383E715E004A64C543041C4AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB53837B82850FAC0378779843AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <e1212a34f6b1dbabeddb85c4ad3cf015eb4c8c16.camel@math.aau.dk>
 <BYAPR06MB5383D57CB539FC1E76498221AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <20200513190951.GI1776058@posteo.no>
 <BYAPR06MB5383E715E004A64C543041C4AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <20200513194930.GJ1776058@posteo.no>

On 2020-05-13 19:27 +0000, Poling, William wrote:
> if this is something I can do without IT admin access.

Hi!  O.T. on laptops: Also, perhaps it is 
easier to find another laptop.  There seems 
to be some great ThinkPads readily available 
anywhere around the U.S., like an X61 or X220 
or something, at least that is my impression 
from various Facebook groups ... I mean if it 
is easier for you to complete your work on 
another machine where you have admin access 
... just for making a rmarkdown presentation 
or something, not for working with the 
sensitive insurance data ... 

Best,
Rasmus


From Po||ngW @end|ng |rom @etn@@com  Wed May 13 21:54:04 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Wed, 13 May 2020 19:54:04 +0000
Subject: [R] Help with R-Markdown please
In-Reply-To: <20200513194930.GJ1776058@posteo.no>
References: <BYAPR06MB53837B82850FAC0378779843AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <e1212a34f6b1dbabeddb85c4ad3cf015eb4c8c16.camel@math.aau.dk>
 <BYAPR06MB5383D57CB539FC1E76498221AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <20200513190951.GI1776058@posteo.no>
 <BYAPR06MB5383E715E004A64C543041C4AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <20200513194930.GJ1776058@posteo.no>
Message-ID: <BYAPR06MB538371A33E8A8E86F0AF5216AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>

I have R on personal laptop for consultative purposes from time to time, however, I cannot move data, confidentiality constraints, as you can imagine.

I have initiated another IT ticket with organization, I think I will get to the bottom of this at some point.

Thank you.

WHP


Proprietary

-----Original Message-----
From: Rasmus Liland <jral at posteo.no> 
Sent: Wednesday, May 13, 2020 2:50 PM
To: Poling, William <PolingW at aetna.com>
Cc: r-help at r-project.org
Subject: [EXTERNAL] Re: Re: [R] Help with R-Markdown please

**** External Email - Use Caution ****

On 2020-05-13 19:27 +0000, Poling, William wrote:
> if this is something I can do without IT admin access.

Hi!  O.T. on laptops: Also, perhaps it is easier to find another laptop.  There seems to be some great ThinkPads readily available anywhere around the U.S., like an X61 or X220 or something, at least that is my impression from various Facebook groups ... I mean if it is easier for you to complete your work on another machine where you have admin access ... just for making a rmarkdown presentation or something, not for working with the sensitive insurance data ... 

Best,
Rasmus

NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed May 13 22:13:00 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 13 May 2020 13:13:00 -0700
Subject: [R] solve() function freezes CLI in GNU R 3.6.3
In-Reply-To: <20200513185709.GH1776058@posteo.no>
References: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
 <7754f2dc-33a5-ff7b-4867-ff0a3fe0e613@gmail.com>
 <20200513152839.GA1776058@posteo.no>
 <1c5e1a18-2e87-9507-5156-f5140692e4b8@gmail.com>
 <20200513182933.GG1776058@posteo.no>
 <CF387B40-317A-4A0D-8B3C-FB322A5E448B@dcn.davis.ca.us>
 <20200513185709.GH1776058@posteo.no>
Message-ID: <2DAD00EF-5DB9-40DC-9BEB-B2A598492810@dcn.davis.ca.us>

In general, any time you deal with floating point numbers having different magnitudes, you risk pushing some low precision bits out of the result. Simply changing the sequence of calculations such as a literal polynomial evaluation versus Horner's method can obtain different results. Take a course in Numerical Analysis to learn more.

[1] https://en.m.wikipedia.org/wiki/Horner%27s_method
[2] https://en.m.wikipedia.org/wiki/Numerical_analysis

On May 13, 2020 11:57:09 AM PDT, Rasmus Liland <jral at posteo.no> wrote:
>On 2020-05-13 11:44 -0700, Jeff Newmiller wrote:
>> Depending on reproducibility in the least 
>> significant bits of floating point 
>> calculations is a bad practice. Just 
>> because you decide based on this one 
>> example that one implementation of BLAS is 
>> better than another does not mean that will 
>> be true for all specific examples. IMO you 
>> are drawing conclusions on data that is 
>> effectively random and should change your 
>> definition of "sufficient to the task".
>
>Dear Jeff,
>
>Right, so I really would have wanted OpenBLAS 
>to be as reproducible as regular BLAS in this 
>one random example, but my hands remains tied 
>on this since I do not know anything about 
>BLAS ... 
>
>More interestingly, could you dream up any 
>idea as to what might cause this difference?
>
>Best,
>Rasmus

-- 
Sent from my phone. Please excuse my brevity.


From @purd|e@@ @end|ng |rom gm@||@com  Wed May 13 22:17:16 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Thu, 14 May 2020 08:17:16 +1200
Subject: [R] Help with Radius problem --update
In-Reply-To: <BYAPR06MB53831A2287FAA73544C21AEFAEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB53831A2287FAA73544C21AEFAEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <CAB8pepxirFcetLgnK1pyHqdkr7Q1aNeERrfkxoLTrQkerQxEKg@mail.gmail.com>

> "determine the largest concentration of members in the smallest radius"

I haven't read the whole thread, and I'm not familiar with this topic.
However, looking at it from an intuitive perspective, isn't the
smallest radius zero.
If the concentration means the number of "members" divided by the area...
...then would the largest concentration be undefined, merely because
of division by zero.

Perhaps your question is a valid one.
But if you're wanting a large pool of people to consider helping, I'd
recommend a more (mathematically) precise definition of the problem
you're trying to solve.


From jr@| @end|ng |rom po@teo@no  Wed May 13 23:03:58 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Wed, 13 May 2020 23:03:58 +0200
Subject: [R] solve() function freezes CLI in GNU R 3.6.3
In-Reply-To: <2DAD00EF-5DB9-40DC-9BEB-B2A598492810@dcn.davis.ca.us>
References: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
 <7754f2dc-33a5-ff7b-4867-ff0a3fe0e613@gmail.com>
 <20200513152839.GA1776058@posteo.no>
 <1c5e1a18-2e87-9507-5156-f5140692e4b8@gmail.com>
 <20200513182933.GG1776058@posteo.no>
 <CF387B40-317A-4A0D-8B3C-FB322A5E448B@dcn.davis.ca.us>
 <20200513185709.GH1776058@posteo.no>
 <2DAD00EF-5DB9-40DC-9BEB-B2A598492810@dcn.davis.ca.us>
Message-ID: <20200513210358.GK1776058@posteo.no>

On 2020-05-13 13:13 -0700, Jeff Newmiller wrote:
> In general, any time you deal with floating 
> point numbers having different magnitudes, 
> you risk pushing some low precision bits 
> out of the result. Simply changing the 
> sequence of calculations such as a literal 
> polynomial evaluation versus Horner's 
> method can obtain different results. Take a 
> course in Numerical Analysis to learn 
> more.
> 
> [1] https://en.m.wikipedia.org/wiki/Horner%27s_method
> [2] https://en.m.wikipedia.org/wiki/Numerical_analysis

Right, it seems fairly interesting.  I'll 
look into it at some point.

/JR

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200513/8fa8a0f6/attachment.sig>

From Po||ngW @end|ng |rom @etn@@com  Wed May 13 23:05:09 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Wed, 13 May 2020 21:05:09 +0000
Subject: [R] Help with Radius problem --update
In-Reply-To: <CAB8pepxirFcetLgnK1pyHqdkr7Q1aNeERrfkxoLTrQkerQxEKg@mail.gmail.com>
References: <BYAPR06MB53831A2287FAA73544C21AEFAEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CAB8pepxirFcetLgnK1pyHqdkr7Q1aNeERrfkxoLTrQkerQxEKg@mail.gmail.com>
Message-ID: <BYAPR06MB538339AFB669DEDF219F24AFAEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>

Hello Abby and thank you for your response.

Your surly correct.

I have not worked a problem like this previously, however, I am learning fast.

I did not think I would need to apply mathematical formula-calculations for this task, math in general not my primary area of expertise, but always very curious.

However your point is provocative and now I am interested to learn more in that space as well.

By researching the literature on Radii, and disk partial coverage problem, and anything in stack overflow remotely relevant to my question I am slowly piecing a solution together.

What I described in the string is my best thinking in terms of delivering something, anything, at the moment that I can build on, refine, etc.

1. I have data set with 2352 geo locations in 41 states from Maine to California
2. I have determined, using density plot (thank to Jim L), that the most reasonable reference point in terms of realizing population density using distance among my 2353 is Brooklyn, NY.
3. Calculating distances from that point to the other 2351 based on a script referenced in stack over flow I simply plug in, manually-iteratively, distances (8 at the moment) and determine how many members are in that radius
4. Then determine the % decay from Max to Min of these 8 distances and settle on the one that has the highest concentration with the least decay
4. This piece (Item 3) I would like to make more efficient and was the point of my last submission to the R-Help list on this topic

For example: Radius2 = 1500 miles, Radius3 = 1200 miles, Radius4 = 1000 miles
The difference b/w Radius2 in terms of percent missing coverage and Radius3 is 0.057 while the percent missing coverage b/w Radius2 and Radius4 is 0.204

So my logic is that despite being a larger radius ,Radius3 vs Radius4, the trade off is more inclusive of the membership and therefore optimal among the 8 iterations. For the moment

I appreciate your response and any direction or advice would be most appreciated.

WHP

 



Proprietary

-----Original Message-----
From: Abby Spurdle <spurdle.a at gmail.com> 
Sent: Wednesday, May 13, 2020 3:17 PM
To: Poling, William <PolingW at aetna.com>
Cc: r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with Radius problem --update

**** External Email - Use Caution ****

> "determine the largest concentration of members in the smallest radius"

I haven't read the whole thread, and I'm not familiar with this topic.
However, looking at it from an intuitive perspective, isn't the smallest radius zero.
If the concentration means the number of "members" divided by the area...
...then would the largest concentration be undefined, merely because of division by zero.

Perhaps your question is a valid one.
But if you're wanting a large pool of people to consider helping, I'd recommend a more (mathematically) precise definition of the problem you're trying to solve.

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From jr@| @end|ng |rom po@teo@no  Wed May 13 23:13:40 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Wed, 13 May 2020 23:13:40 +0200
Subject: [R] Help with R-Markdown please
In-Reply-To: <BYAPR06MB538371A33E8A8E86F0AF5216AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB53837B82850FAC0378779843AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <e1212a34f6b1dbabeddb85c4ad3cf015eb4c8c16.camel@math.aau.dk>
 <BYAPR06MB5383D57CB539FC1E76498221AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <20200513190951.GI1776058@posteo.no>
 <BYAPR06MB5383E715E004A64C543041C4AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <20200513194930.GJ1776058@posteo.no>
 <BYAPR06MB538371A33E8A8E86F0AF5216AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <20200513211340.GL1776058@posteo.no>

On 2020-05-13 19:54 +0000, Poling, William wrote:
> I have R on personal laptop for 
> consultative purposes from time to time, 
> however, I cannot move data, 
> confidentiality constraints, as you can 
> imagine.
> 
> I have initiated another IT ticket with 
> organization, I think I will get to the 
> bottom of this at some point.

Great, I hope you get to the bottom of this! 
??????


From @purd|e@@ @end|ng |rom gm@||@com  Wed May 13 23:15:54 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Thu, 14 May 2020 09:15:54 +1200
Subject: [R] Fitting Richards' curve
In-Reply-To: <CA+dpOJmuzENh8zusk3hhb3Kpa6kpkcpQTRZ1H1S3Z0rp-ZSztw@mail.gmail.com>
References: <CA+dpOJmuzENh8zusk3hhb3Kpa6kpkcpQTRZ1H1S3Z0rp-ZSztw@mail.gmail.com>
Message-ID: <CAB8pepwt2hzL0kyfP7QPwf9tFA5QtAxR_1AfzGpP-_j8TJNNug@mail.gmail.com>

Hi Christofer,

This doesn't really answer your question.
But if the goal is to fit an S-shaped curve to data, with increased
flexibility...
(I'm assuming that's the goal).

...then I'd like to note the option of splines (or smoothing), subject
to shape constraints...

My guess, is it's probably easier to model the inverse of a growth
curve this way, than to model the growth curve directly.
In which case, a 4-piece to 10-piece spline should give considerably flexibly.

It's possible that Martin's package, cobs, can do this, but not sure,
I haven't tried it.
And there may be other R packages for fitting splines/smoothers to
data, subject to shape constraints.

If not, I'm guessing it wouldn't be too difficult to implement, via
extensions to the quadprog package, for quadratic programming.


On Wed, May 13, 2020 at 9:26 PM Christofer Bogaso
<bogaso.christofer at gmail.com> wrote:
>
> Hi,
>
> Is there any R package to fit Richards' curve in the form of
> https://en.wikipedia.org/wiki/Generalised_logistic_function
>
> I found there is one package grofit, but currently defunct.
>
> Any pointer appreciated.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jrkr|de@u @end|ng |rom gm@||@com  Wed May 13 16:04:11 2020
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Wed, 13 May 2020 10:04:11 -0400
Subject: [R] Extracting the first currency value from PDF files
In-Reply-To: <MA1PR01MB3513376769EBEFABDAB09D05B9BF0@MA1PR01MB3513.INDPRD01.PROD.OUTLOOK.COM>
References: <MA1PR01MB3513376769EBEFABDAB09D05B9BF0@MA1PR01MB3513.INDPRD01.PROD.OUTLOOK.COM>
Message-ID: <CAKZQJMBEaa93pygciMsa=aeS2Qj4X-EL-GybTZzD+Tc8WcDaDQ-17@mail.gmail.com>

It looks like you are using the str_nth_currency() function from the strex
package but we have no idea of what the pdf files are or how you are
importing them is to R.
We need a lot more information on what you are doing "before" you use the
function.

Have a look at
http://adv-r.had.co.nz/Reproducibility.html
or
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example



On Wed, 13 May 2020 at 09:33, Manish Mukherjee <manishmukherjee at hotmail.com>
wrote:

> Hi All,
>
> Need some help with the following code , i have a number of pdf files ,
> and the first page of those files gives a currency value $xxx,xxx,xxx . How
> to extract this value from a number of PDF files and put it in a data frame
> . I am able to do it for a single file
> with the code where opinions is the text data and 1 is the first currency
> value
> ```
> d=str_nth_currency(opinions, 1)
> df = subset(d, select = c(amount) )
> df
>
> I want this to loop over multiple pdf files
>
> I have tried somesthing like this but not working
> for (i in 1:length(files)){
>   print(i)
>   pdf_text(paste("filepath ", files[i],sep = ""))
>   str_nth_currency(files[i], 1)
> }
>
>
> Please help.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
John Kane
Kingston ON Canada

	[[alternative HTML version deleted]]


From @purd|e@@ @end|ng |rom gm@||@com  Thu May 14 03:41:57 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Thu, 14 May 2020 13:41:57 +1200
Subject: [R] Fitting Richards' curve
In-Reply-To: <CAB8pepwt2hzL0kyfP7QPwf9tFA5QtAxR_1AfzGpP-_j8TJNNug@mail.gmail.com>
References: <CA+dpOJmuzENh8zusk3hhb3Kpa6kpkcpQTRZ1H1S3Z0rp-ZSztw@mail.gmail.com>
 <CAB8pepwt2hzL0kyfP7QPwf9tFA5QtAxR_1AfzGpP-_j8TJNNug@mail.gmail.com>
Message-ID: <CAB8pepwJKxdy3=8z8Fh+tL8fNDedP9XdA_hJnTKX=F+P0hyiqA@mail.gmail.com>

> It's possible that Martin's package, cobs, can do this, but not sure,
> I haven't tried it.
> And there may be other R packages for fitting splines/smoothers to
> data, subject to shape constraints.

Further to my previous post.
I read through the documentation for the cobs package.
And (someone please correct me if I'm wrong) I don't think it can
produce a smoother with an S-shaped shape constraint.
I found two other packages for smoothing under shape constraints, but
doesn't look like they can either.

I have code that could be adapted for this purpose.
But one it's incomplete, two it (currently) requires the user to
specify a smoothness parameter, and three doesn't produce any kind of
inference...


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Thu May 14 03:50:32 2020
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard Comcast)
Date: Wed, 13 May 2020 21:50:32 -0400
Subject: [R] Fitting Richards' curve
In-Reply-To: <CAB8pepwt2hzL0kyfP7QPwf9tFA5QtAxR_1AfzGpP-_j8TJNNug@mail.gmail.com>
References: <CAB8pepwt2hzL0kyfP7QPwf9tFA5QtAxR_1AfzGpP-_j8TJNNug@mail.gmail.com>
Message-ID: <44C966E9-88B9-48FC-BF4D-2094680C73A0@comcast.net>

I have been using nlsr() to fit s curves to Covid-19 data over the past few weeks and I have not had any issues.

Bernard
Sent from my iPhone so please excuse the spelling!"

> On May 13, 2020, at 5:16 PM, Abby Spurdle <spurdle.a at gmail.com> wrote:
> 
> ?Hi Christofer,
> 
> This doesn't really answer your question.
> But if the goal is to fit an S-shaped curve to data, with increased
> flexibility...
> (I'm assuming that's the goal).
> 
> ...then I'd like to note the option of splines (or smoothing), subject
> to shape constraints...
> 
> My guess, is it's probably easier to model the inverse of a growth
> curve this way, than to model the growth curve directly.
> In which case, a 4-piece to 10-piece spline should give considerably flexibly.
> 
> It's possible that Martin's package, cobs, can do this, but not sure,
> I haven't tried it.
> And there may be other R packages for fitting splines/smoothers to
> data, subject to shape constraints.
> 
> If not, I'm guessing it wouldn't be too difficult to implement, via
> extensions to the quadprog package, for quadratic programming.
> 
> 
>> On Wed, May 13, 2020 at 9:26 PM Christofer Bogaso
>> <bogaso.christofer at gmail.com> wrote:
>> 
>> Hi,
>> 
>> Is there any R package to fit Richards' curve in the form of
>> https://en.wikipedia.org/wiki/Generalised_logistic_function
>> 
>> I found there is one package grofit, but currently defunct.
>> 
>> Any pointer appreciated.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From g@@@uu| @end|ng |rom gm@||@com  Thu May 14 07:40:57 2020
From: g@@@uu| @end|ng |rom gm@||@com (ani jaya)
Date: Thu, 14 May 2020 14:40:57 +0900
Subject: [R] Missing x label in barplot
Message-ID: <CAHXS41yYT=ZpvefderOxC3=voPOmNMwHXczESE9NfdHZZfm-Gw@mail.gmail.com>

Dear R community,

I found some missing x label when I saving this plot to tiff file:

justsample <- rnorm(n=1095*3,mean=100,sd=10)
justsample <- as.data.frame(matrix(justsample,ncol=3))
dd <- seq(from=as.Date("1985-01-01"), to =as.Date("1987-12-31"), by='day')
y <- data.frame(Year=substr(dd,1,4),
              Month=substr(dd,6,7),
              Day=substr(dd,9,10), stringsAsFactors = FALSE)


clim <- aggregate(justsample,list(y$Month),mean,na.rm=TRUE)
for (i in 1:3){
  file_name <- paste("clim_bug", i, ".tiff", sep="")
  tiff(file_name)
barplot(clim[,i+1], ylab="Daily rainfall (mm)", xlab="Time (month)",
        main="Climatological Mean",
        names.arg=c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul",
"Aug", "Sep", "Oct","Nov","Dec"),
        col="cadetblue3", cex.axis = 1, ylim=c(0,120))
dev.off()
}

I can see complete x label in plots window in RStudio and I thought it
was RStudio error
but I also have some error in Rgui.

I believe my code is terrible and as newbie R, I welcome to any
suggestions and comments.


> sessionInfo()
R version 3.6.3 (2020-02-29)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 18363)


Best,
Ani


From den|z343 @end|ng |rom gm@||@com  Thu May 14 01:18:31 2020
From: den|z343 @end|ng |rom gm@||@com (=?utf-8?Q?Deniz_U=C4=9Fur?=)
Date: Thu, 14 May 2020 02:18:31 +0300
Subject: [R] gdistance::accCost fails when nrows is bigger than 90
Message-ID: <3958C26E-8523-4F18-9219-CC8AF91979BC@gmail.com>

Hello all,

It has been only 2 days since I started using R and I was trying movecost library. Before explaining the unexpected behavior I should say that I?m calling R function from Python using rpy2 package. That shouldn?t be an issue because everything works as expected when a matrix of size 100x90 is given.

Okay now, basically I?m converting an 2D matrix to RasterLayer with appropriate CRS and extent properties then transfer it to R runtime. A modified version of Tobler?s hiking function is applied then a shortest-path is calculated. This procedure works for a matrix with column size anywhere between 1 and memory?s limit. However if row size of the matrix exceeds 90 then function is halted on gdistance::accCost saying that:

"At structural_properties.c:5313 : cannot run Bellman-Ford algorithm, Negative loop detected while calculating shortest paths?

It?s very interesting issue because I have explicitly tried ?88, 89, 90, 91 and it halted on +90

I?ve created a gist in case you want to try the application on your computer. Also I?ve included a clean version of the R script I was using bellow.

Thank you.

https://gist.github.com/DenizUgur/01e18edd03321f3d3928c3afb2bd7145 <https://gist.github.com/DenizUgur/01e18edd03321f3d3928c3afb2bd7145>

movecost <- function (dtm, origin, destin, time="s") {

  altDiff <- function(x){x[2] - x[1]}
  hd <- gdistance::transition(dtm, altDiff, 8, symm=FALSE)

  slope <- gdistance::geoCorrection(hd)

  cost_function <- function(x){ ifelse(x[adj] > 0, 1.3 * exp(-3.5 * abs(x[adj] + 0.05)), 5.3 * exp(-3.5 * abs(x[adj] + 0.05))) }

  adj <- raster::adjacent(dtm, cells=1:ncell(dtm), pairs=TRUE, directions=8)
  speed <- slope
  speed[adj] <- cost_function(slope)
  speed <- speed * 0.278
  Conductance <- gdistance::geoCorrection(speed)

  accum_final <- gdistance::accCost(Conductance, sp::coordinates(origin))

  accum_final <- raster::mask(accum_final, dtm)

  sPath <- gdistance::shortestPath(Conductance, sp::coordinates(origin), sp::coordinates(destin), output="SpatialLines")
  sPath$length <- rgeos::gLength(sPath, byid=TRUE)
  destin$cost <- raster::extract(accum_final, destin)
  
  results <- list("accumulated.cost.raster"=accum_final,
                  "LCPs"=sPath,
                  "dest.loc.w.cost"=destin)
}
	[[alternative HTML version deleted]]


From petr@p|k@| @end|ng |rom prechez@@cz  Thu May 14 08:16:30 2020
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Thu, 14 May 2020 06:16:30 +0000
Subject: [R] Missing x label in barplot
In-Reply-To: <CAHXS41yYT=ZpvefderOxC3=voPOmNMwHXczESE9NfdHZZfm-Gw@mail.gmail.com>
References: <CAHXS41yYT=ZpvefderOxC3=voPOmNMwHXczESE9NfdHZZfm-Gw@mail.gmail.com>
Message-ID: <a5590b70c0e141b48b5a7c910b40dc22@SRVEXCHCM1302.precheza.cz>

Hi

Not enough space for labels in x axes. Change width.

for (i in 1:3){
  file_name <- paste("clim_bug", i, ".tiff", sep="")
  tiff(file_name, width=520)
barplot(clim[,i+1], ylab="Daily rainfall (mm)", xlab="Time (month)",
        main="Climatological Mean",
        names.arg=c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul",
"Aug", "Sep", "Oct","Nov","Dec"),
        col="cadetblue3", cex.axis = 1, ylim=c(0,120))
dev.off()
}

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of ani jaya
> Sent: Thursday, May 14, 2020 7:41 AM
> To: r-help <r-help at r-project.org>
> Subject: [R] Missing x label in barplot
> 
> Dear R community,
> 
> I found some missing x label when I saving this plot to tiff file:
> 
> justsample <- rnorm(n=1095*3,mean=100,sd=10)
> justsample <- as.data.frame(matrix(justsample,ncol=3))
> dd <- seq(from=as.Date("1985-01-01"), to =as.Date("1987-12-31"), by='day')
> y <- data.frame(Year=substr(dd,1,4),
>               Month=substr(dd,6,7),
>               Day=substr(dd,9,10), stringsAsFactors = FALSE)
> 
> 
> clim <- aggregate(justsample,list(y$Month),mean,na.rm=TRUE)
> for (i in 1:3){
>   file_name <- paste("clim_bug", i, ".tiff", sep="")
>   tiff(file_name)
> barplot(clim[,i+1], ylab="Daily rainfall (mm)", xlab="Time (month)",
>         main="Climatological Mean",
>         names.arg=c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul",
> "Aug", "Sep", "Oct","Nov","Dec"),
>         col="cadetblue3", cex.axis = 1, ylim=c(0,120))
> dev.off()
> }
> 
> I can see complete x label in plots window in RStudio and I thought it
> was RStudio error
> but I also have some error in Rgui.
> 
> I believe my code is terrible and as newbie R, I welcome to any
> suggestions and comments.
> 
> 
> > sessionInfo()
> R version 3.6.3 (2020-02-29)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows 10 x64 (build 18363)
> 
> 
> Best,
> Ani
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From m@rong|u@|u|g| @end|ng |rom gm@||@com  Thu May 14 11:18:48 2020
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Thu, 14 May 2020 11:18:48 +0200
Subject: [R] Resolve unmet dependencies and install R 3.6 on chromebook
Message-ID: <CAMk+s2REb6CTceq+YNS-o_xOJENTP-CdrcwZBoHP+3uo+dP2Bw@mail.gmail.com>

I tried to upgrade R from 3,3 to 3.6 on a Chromebook but I am getting this
error
```
.org/debian stretch InRelease
Hit:4 https://deb.debian.org/debian-security stretch/updates InRelease
Hit:5 https://deb.debian.org/debian stretch-backports InRelease
Hit:6 http://cran.wustl.edu/bin/linux/ubuntu xenial/ InRelease
Hit:7 https://deb.debian.org/debian stretch Release
Hit:8 https://deb.nodesource.com/node_13.x stretch InRelease
Hit:9 https://desktop-download.mendeley.com/download/apt stable InRelease
Ign:10 https://storage.googleapis.com/cros-packages/81 stretch InRelease
Hit:11 https://storage.googleapis.com/cros-packages/81 stretch Release
Hit:12 https://apt.llvm.org/stretch llvm-toolchain-stretch-7 InRelease
Reading package lists... Done
E: The repository 'http://ppa.launchpad.net/kivy-team/kivy/ubuntu focal
Release' does not have a Release file.
N: Updating from such a repository can't be done securely, and is therefore
disabled by default.
N: See apt-secure(8) manpage for repository creation and user configuration
details.
W: Target Packages (Packages) is configured multiple times in
/etc/apt/sources.list:3 and /etc/apt/sources.list:4
W: Target Translations (en_US) is configured multiple times in
/etc/apt/sources.list:3 and /etc/apt/sources.list:4
W: Target Translations (en) is configured multiple times in
/etc/apt/sources.list:3 and /etc/apt/sources.list:4
W: Target Packages (Packages) is configured multiple times in
/etc/apt/sources.list:3 and /etc/apt/sources.list:4
W: Target Translations (en_US) is configured multiple times in
/etc/apt/sources.list:3 and /etc/apt/sources.list:4
W: Target Translations (en) is configured multiple times in
/etc/apt/sources.list:3 and /etc/apt/sources.list:4
marongiuluigi at penguin:~$ sudo apt upgrade r-base r-base-dev
Reading package lists... Done
Building dependency tree
Reading state information... Done
Calculating upgrade... Done
  404  Not Found [IP: 2001:67c:1560:8008::15 80]
Ign:14 http://ppa.launchpad.net/kivy-team/kivy/ubuntu focal/main
Translation-en
Ign:15 http://ppa.launchpad.net/kivy-team/kivy/ubuntu focal/main
Translation-en_US
Ign:17 http://ppa.launchpad.net/kivy-team/kivy/ubuntu focal/main amd64
DEP-11 Metadata
Ign:18 http://ppa.launchpad.net/kivy-team/kivy/ubuntu focal/main all DEP-11
Metadata
Reading package lists... Done
W: The repository 'http://ppa.launchpad.net/kivy-team/kivy/ubuntu focal
Release' does not have a Release file.
N: Data from such a repository can't be authenticated and is therefore
potentially dangerous to use.
N: See apt-secure(8) manpage for repository creation and user configuration
details.
W: Target Packages (Packages) is configured multiple times in
/etc/apt/sources.list:3 and /etc/apt/sources.list:4
W: Target Translations (en_US) is configured multiple times in
/etc/apt/sources.list:3 and /etc/apt/sources.list:4
W: Target Translations (en) is configured multiple times in
/etc/apt/sources.list:3 and /etc/apt/sources.list:4
E: Failed to fetch
http://ppa.launchpad.net/kivy-team/kivy/ubuntu/dists/focal/main/binary-amd64/Packages
404  Not Found [IP: 2001:67c:1560:8008::15 80]
E: Some index files failed to download. They have been ignored, or old ones
used instead.
W: Target Packages (Packages) is configured multiple times in
/etc/apt/sources.list:3 and /etc/apt/sources.list:4
W: Target Translations (en_US) is configured multiple times in
/etc/apt/sources.list:3 and /etc/apt/sources.list:4
W: Target Translations (en) is configured multiple times in
/etc/apt/sources.list:3 and /etc/apt/sources.list:4
marongiuluigi at penguin:~$ sudo apt-get install r-base
Reading package lists... Done
Building dependency tree
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:

The following packages have unmet dependencies:
 r-base : Depends: r-base-core (>= 3.4.4-1xenial0) but it is not going to
be installed
          Depends: r-recommended (= 3.4.4-1xenial0) but it is not going to
be installed
          Recommends: r-base-html but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
```
These packages are not even available because in theory I have uninstalled
R:
```
$ sudo apt remove purge r-base-core
Reading package lists... Done
Building dependency tree
Reading state information... Done
E: Unable to locate package purge
$ sudo apt remove purge r-base-recommended
Reading package lists... Done
Building dependency tree
Reading state information... Done
E: Unable to locate package purge
E: Unable to locate package r-base-recommended
$ sudo apt remove purge r-base
Reading package lists... Done
Building dependency tree
Reading state information... Done
E: Unable to locate package purge
$ sudo apt remove purge r-base-dev
Reading package lists... Done
Building dependency tree
Reading state information... Done
E: Unable to locate package purge
```
Is there a way to solve this problem and install R 3.6 on a Chromebook?

-- 
Best regards,
Luigi

	[[alternative HTML version deleted]]


From Po||ngW @end|ng |rom @etn@@com  Thu May 14 12:23:27 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Thu, 14 May 2020 10:23:27 +0000
Subject: [R] Help with map()
Message-ID: <BYAPR06MB5383688B7688345330422859AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>

#RStudio Version Version 1.2.1335 
sessionInfo() 
# R version 4.0.0 Patched (2020-05-03 r78349)
#Platform: x86_64-w64-mingw32/x64 (64-bit)
#Running under: Windows 10 x64 (build 17763)

Good morning. 

I ran routines like this one yesterday with no errors.

#Radius3----
str(radius3)

radius3 <- individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,state,city,Latitude,Longitude,distances_Mi) %>% 
  filter(distances_Mi <= 1200)
  
str(radius3)

  geomat<-makeDensityMatrix(radius3[,c("Latitude","Longitude")],
                            xlim=range(radius3$Longitude),ylim=range(radius3$Latitude))

latlim<-range(radius3$Latitude)
lonlim<-range(radius3$Longitude)

<- map("world",xlim=lonlim,ylim=latlim)
axis(1)
axis(2)
densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
            red=c(0.5,1),green=0,blue=0,pch=15)
title("Member Geo Density Plot For Radius3")


This morning I am trying slightly new routine and receiving this error

clus1 <- individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,state,city,clusters,Latitude,Longitude,distances_Mi) %>%   
  filter(clusters == 1)

geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
                          xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))

latlim<-range(clus1$Latitude)
lonlim<-range(clus1$Longitude)

map("world",xlim=lonlim,ylim=latlim)
axis(1)
axis(2)
densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
            red=c(0.5,1),green=0,blue=0,pch=15)
title("Member Geo Density Plot For Cluster1")


geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
+                           xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))

> latlim<-range(clus1$Latitude)
> lonlim<-range(clus1$Longitude)
> 
> map("world",xlim=lonlim,ylim=latlim)
Error in .C(C_map_type, as.character(mapbase), integer(1)) : 
  Incorrect number of arguments (2), expecting 0 for ''
> axis(1)
Error in axis(1) : plot.new has not been called yet
> axis(2)
Error in axis(2) : plot.new has not been called yet
> densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
+             red=c(0.5,1),green=0,blue=0,pch=15)
Error in plot.xy(xy.coords(x, y), type = type, ...) : 
  plot.new has not been called yet
> title("Member Geo Density Plot For Cluster1")
Error in title("Member Geo Density Plot For Cluster1") : 
  plot.new has not been called yet


It seems to error at the point of map("world",xlim=lonlim,ylim=latlim)

I find a ref in stack overflow https://stackoverflow.com/questions/45066628/cannot-run-map-datastate
and tried solutions but they do not seem to work?

Here is a sample (MBR_SUBSCRIBERID(factor) is replaced with ID(integer) but of no consequence in my routine since not really used.

I hope someone recognizes the problem.

Thank you for any advice 

WHP

> dput(sample)
structure(list(state = structure(c(2L, 22L, 10L, 12L, 6L, 22L, 
11L, 22L, 22L, 32L, 19L, 29L, 9L, 9L, 10L, 30L, 33L, 35L, 41L, 
12L, 12L, 36L, 4L, 9L, 8L, 39L, 36L, 36L, 41L, 28L, 12L, 28L, 
28L, 12L, 11L, 2L, 12L, 22L, 32L, 22L, 12L, 10L, 10L, 22L, 10L
), .Label = c("AL", "AR", "AZ", "CA", "CO", "CT", "DC", "DE", 
"FL", "GA", "IA", "IL", "IN", "KS", "KY", "LA", "MA", "MD", "ME", 
"MI", "MN", "MO", "NC", "NE", "NJ", "NM", "NV", "NY", "OH", "OK", 
"OR", "PA", "SC", "SD", "TN", "TX", "UT", "VA", "WA", "WI", "WV"
), class = "factor"), city = structure(c(838L, 1030L, 262L, 218L, 
166L, 973L, 339L, 451L, 660L, 358L, 281L, 1280L, 129L, 223L, 
989L, 721L, 550L, 731L, 1325L, 688L, 1184L, 281L, 759L, 1171L, 
1305L, 587L, 272L, 581L, 263L, 152L, 217L, 152L, 390L, 5L, 571L, 
1006L, 1162L, 939L, 170L, 1033L, 1002L, 586L, 586L, 192L, 586L
), .Label = c("ABBOTTSTOWN", "ABILENE", "ACWORTH", "ADAMS", "ADDISON", 
"ADKINS", "ALBANY", "ALBIA", "ALBUQUERQUE", "ALEXANDRIA", "ALFRED", 
"ALIQUIPPA", "ALISO VIEJO", "ALLEN PARK", "ALLENTOWN", "ALPHA", 
"ALPHARETTA", "ALPINE", "ALTOONA", "AMARILLO", "AMBLER", "AMBRIDGE", 
"AMHERST", "AMITYVILLE", "ANAHIEM", "ANDERSON", "ANDOVER", "ANGIER", 
"ANGLETON", "ANKENY", "ANN ARBOR", "ANZA", "APEX", "APOLLO", 
"ARCADIA", "ARCHDALE", "ARDMORE", "ARGYLE", "ARKANSAS CITY", 
"ARLINGTON", "ARNOLD", "ARTHUR", "ASHEBORO", "ASHEVILLE", "ASHLAND", 
"ASHLAND CITY", "ASHTON", "ASTON", "ATHENS", "ATLANTA", "ATLANTIC BCH", 
"AUBORN", "AUGUSTA", "AURORA", "AUSTELL", "AUSTIN", "AVENTURA", 
"AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN CITY", "BALDWIN PLACE", 
"BALLWIN", "BANGOR", "BARBOURSVILLE", "BARNEGAT", "BARTLETT", 
"BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", "BATESBURG", "BATON ROUGE", 
"BAXTER SPRINGS", "BAY CITY", "BAYONNE", "BAYSIDE", "BAYTOWN", 
"BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER CITY", "BELFAIR", 
"BELLE VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR", "BELLMORE", 
"BELLWOOD", "BELMAR", "BELMONT", "BELVIDERE", "BENNET", "BENTON", 
"BENTONVILLE", "BERLIN", "BESSEMER CITY", "BETHANY", "BETHEL", 
"BETHEL PARK", "BETHESDA", "BETHLEHEM", "BETTENDORF", "BIGLERVILLE", 
"BINGHAMTON", "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN", "BLACKSBURG", 
"BLAINE", "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD", "BLUE BELL", 
"BLUE GRASS", "BLUE POINT", "BLUE SPRINGS", "BOCA RATON", "BODFISH", 
"BOILING SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS", "BONNEY LAKE", 
"BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING GREEN", "BOYERTOWN", 
"BOYNTON BEACH", "BRADENTON", "BRADENVILLE", "BRANDAMORE", "BRANDON", 
"BRANFORD", "BRANSON", "BRASELTON", "BREEZEWOOD", "BRENTON", 
"BRENTWOOD", "BREWSTER", "BRICK", "BRIDGEPORT", "BRIDGETON", 
"BRIGHAM CITY", "BROADDUS", "BROCKPORT", "BROGUE", "BROKEN ARROW", 
"BRONX", "BROOKFIELD", "BROOKHAVEN", "BROOKLYN", "BROWNS MILLS", 
"BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", "BRYAN", "BUCKSPORT", 
"BUDD LAKE", "BUFFALO", "BULVERDE", "BUNKERVILLE", "BURBANK", 
"BURFORD", "BURIEN", "BURLINGTON", "BURR RIDGE", "BURTON", "BUSKIRK", 
"BUTLER", "BUXTON", "BYRON", "CALERA", "CAMBRIDGE", "CAMP HILL", 
"CAMPBELL", "CANAAN", "CANAL WINCHESTER", "CANASTOTA", "CANTON", 
"CARDIFF BY THE SEA", "CARL JUNCTION", "CARLISLE", "CARLTON", 
"CAROL STREAM", "CARROLLTON", "CARSON CITY", "CARTERSVILLE", 
"CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", "CASWELL", "CATO", 
"CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER POINT", "CENTERTON", 
"CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN FALLS", "CHALFONT", 
"CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", "CHARLESTON", 
"CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", "CHESHIRE", 
"CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", "CHILLICOTHE", 
"CHOCTAW", "CICERO", "CINCINNATI", "CLAY", "CLEARWATER", "CLEARWATER BEACH", 
"CLEMMONS", "CLENDENIN", "CLEVELAND", "CLEVELAND HTS", "CLEVES", 
"CLIFFSIDE PARK", "CLIFTON", "CLIFTON PARK", "CLIFTON SPGS", 
"CLINTON", "COACHELLA", "COAL TOWNSHIP", "COATESVILLE", "COLLEGE STATION", 
"COLLINSVILLE", "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE", 
"CONCORD", "CONCORD TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY", 
"CONYERS", "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA", 
"CORNELIUS", "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY", 
"COTTAGE HILLS", "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN", 
"CRANBERRY TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER", 
"CUMBERLAND FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS", 
"DALLAS", "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT", 
"DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR", 
"DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH", 
"DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY", 
"DES MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER", 
"DIXON", "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER", 
"DOYLESTOWN", "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH", 
"DUNCAN", "DUNCANNON", "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE", 
"DURHAM", "DUTTON", "EAGLE GROVE", "EAST AURORA", "EAST CHICAGO", 
"EAST EARL", "EAST HANOVER", "EAST HARTFORD", "EAST HARTLAND", 
"EAST LEROY", "EAST LIVERPOOL", "EAST MEADOW", "EAST NORTHPORT", 
"EAST ORANGE", "EAST PEORIA", "EAST SAINT LOUIS", "EASTON", "EDGERTON", 
"EDISON", "EDMOND", "EGG HBR TWP", "EL DORADO", "EL PASO", "ELDRIDGE", 
"ELGIN", "ELIZABETHVILLE", "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY", 
"ELMHURST", "ELMWOOD PARK", "ELOY", "ELY", "EMERSON", "EMORY", 
"ENDICOTT", "ENGLEWOOD", "ENID", "ENNICE", "ENOLA", "ENUMCLAW", 
"EPHRATA", "ERIE", "ESTERO", "EUREKA SPRINGS", "EVANSVILLE", 
"EVERETT", "EXCELSIOR SPRINGS", "EXTON", "FAIRFAX", "FAIRFIELD", 
"FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", "FALMOUTH", "FAR HILLS", 
"FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", "FAYETTE", "FAYETTEVILLE", 
"FELTON", "FENTON", "FERGUSON", "FIELDALE", "FINDLAY", "FINGERVILLE", 
"FLEMINGTON", "FLINT", "FLORENCE", "FLORESVILLE", "FLORISSANT", 
"FLOVILLA", "FLOWER MOUND", "FLUSHING", "FOGELSVILLE", "FOLSOM", 
"FONTANELLE", "FOREST HILLS", "FOREST PARK", "FORT DODGE", "FORT FAIRFIELD", 
"FORT GAY", "FORT LAUDERDALE", "FORT LEE", "FORT MILL", "FORT MOHAVE", 
"FORT PAYNE", "FORT PIERCE", "FORT SMITH", "FORT WAYNE", "FORT WORTH", 
"FOSTORIA", "FOUNTAIN INN", "FRANKFORT", "FRANKLIN", "FREEDOM", 
"FREEHOLD", "FREEPORT", "FREMONT", "FULLERTON", "FULSHEAR", "FULTON", 
"GADSDEN", "GAFFNEY", "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE", 
"GARDNERS", "GARNETT VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA", 
"GETTYSBURG", "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE", 
"GLEN CARBON", "GLEN COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW", 
"GLENWOOD", "GOLDEN CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM", 
"GOSHEN", "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS", 
"GRANDVIEW", "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS", 
"GRASS VALLEY", "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER", 
"GREENLAWN", "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE", 
"GREER", "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD", 
"HADLEY", "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON", 
"HAMBURG", "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE", 
"HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON", 
"HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL", 
"HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE", 
"HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH", 
"HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE", 
"HIGH HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS", 
"HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE", 
"HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD", 
"HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS", 
"HOT SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON", 
"HUFFMAN", "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON", 
"HUNTINGTON BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE", 
"HURST", "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL", 
"INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY", 
"IRMO", "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP", 
"ISSAQUAH", "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS", 
"JACKSONVILLE", "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE", 
"JEFFERSON", "JEFFERSON CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN", 
"JONESBORO", "JONESPORT", "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE", 
"KANNAPOLIS", "KANSAS CITY", "KATY", "KEARNEY", "KELLERTON", 
"KEMAH", "KENMORE", "KENNESAW", "KENT", "KINGMAN", "KINGS MOUNTAIN", 
"KINGSLAND", "KINGSTON", "KINGWOOD", "KIRKLAND", "KUTZTOWN", 
"LA QUINTA", "LA RUE", "LA VERNE", "LAFAYETTE", "LAKE BLUFF", 
"LAKE IN THE HILL", "LAKE ISABELLA", "LAKE PLACID", "LAKE SAINT LOUIS", 
"LAKE TAPPS", "LAKE VILLAGE", "LAKE WAUKOMIS", "LAKE WORTH", 
"LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD RCH", "LAMBERTVILLE", 
"LANCASTER", "LANDISVILLE", "LANSDALE", "LAREDO", "LARGO", "LAS VEGAS", 
"LATROBE", "LAUDERHILL", "LAWNDALE", "LAWRENCE", "LAWRENCEVILLE", 
"LE MARS", "LE ROY", "LEAGUE CITY", "LEAVENWORTH", "LEAWOOD", 
"LEBANON", "LECOMPTON", "LEECHBURG", "LEES SUMMIT", "LEESBURG", 
"LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", "LEVITTOWN", "LEWES", 
"LEWISTOWN", "LEXINGTON", "LIBERTY TWP", "LILLINGTON", "LINCOLN", 
"LINDENHURST", "LINTON", "LISBON", "LITCHFIELD PK", "LITHIA", 
"LITITZ", "LITTLE EGG HARBOR TO", "LK FOREST PK", "LK PEEKSKILL", 
"LOCKHART", "LOCKWOOD", "LOGAN", "LOMA LINDA", "LONE JACK", "LONG BEACH", 
"LONG LANE", "LONGVIEW", "LORAIN", "LORTON", "LOS ANGELES", "LOUISBURG", 
"LOVES PARK", "LOWER BURRELL", "LOWRY CITY", "LUBBOCK", "LUGOFF", 
"LUMBERTON", "LYNBROOK", "LYNNWOOD", "MACHESNEY PARK", "MACON", 
"MAGNOLIA", "MAHOPAC", "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE", 
"MANASSAS", "MANCHACA", "MANCHESTER", "MANCHESTER TOWNSHIP", 
"MANCHESTER TW", "MANHEIM", "MANITO", "MANLIUS", "MANNINGTON", 
"MANSFIELD", "MANSURA", "MAPLE PARK", "MARBLE FALLS", "MARCELLUS", 
"MARICOPA", "MARIETTA", "MARION", "MARKLEYSBURG", "MARS HILL", 
"MARSHALLTOWN", "MARSHFIELD", "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE", 
"MASON CITY", "MASSEY", "MASSILLON", "MASURY", "MATAWAN", "MATEWAN", 
"MATTAPOISETT", "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND", 
"MC DONALD", "MC KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN", 
"MCALESTER", "MCDONOUGH", "MCKEESPORT", "MCKINNEY", "MECHANICSBURG", 
"MECHANICSVILLE", "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS", 
"MERIDEN", "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA", 
"MESQUITE", "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN", 
"MIDLAND", "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK", 
"MILLEDGEVILLE", "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE", 
"MINEOLA", "MINERAL SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION", 
"MISSION HILLS", "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY", 
"MOLINE", "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE", 
"MONTCLAIR", "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON", 
"MORGANTOWN", "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE", 
"MOUNT JOY", "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO", 
"MOUNT UNION", "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT", 
"MULVANE", "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT", 
"MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS", 
"NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES", 
"NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG", 
"NEW BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK", 
"NEW CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON", 
"NEW LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE", 
"NEW ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK", 
"NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT", 
"NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA", 
"NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK", 
"NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON", 
"NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND HILLS", 
"NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE", 
"NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON", 
"OAK RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE", 
"OMAHA", "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION", 
"ORONO", "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO", 
"OTTAWA", "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD", 
"OYSTER BAY", "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR", 
"PALMDALE", "PALMYRA", "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE", 
"PARLIN", "PARMA", "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY", 
"PAXTON", "PEA RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN", 
"PELION", "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE", 
"PENNSBURG", "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS", 
"PERRYSBURG", "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI", 
"PHOENIX", "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN", 
"PINE BLUFF", "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY", 
"PITMAN", "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY", 
"PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO LAKE", 
"POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", "PORT CHARLOTTE", 
"PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", "PORT WASHINGTON", 
"PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", "POTTSVILLE", 
"POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE VILLAGE", 
"PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT PLEASANT", 
"PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", "QUINCY", 
"RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", "READLYN", 
"RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO PARK", 
"REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", "RICH HILL", 
"RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", "RINCON", 
"RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", "RIVERSIDE", 
"ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", "ROCKAWAY BCH", 
"ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", "ROGERS", 
"ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE", 
"ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT", 
"ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN", 
"RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO", 
"SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR", 
"SAINT CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS", 
"SAINT PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY", 
"SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO", 
"SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN", 
"SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", "SARASOTA", 
"SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", "SAYVILLE", 
"SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", "SCOTT", "SCOTTSDALE", 
"SEAFORD", "SEALE", "SEATTLE", "SEDONA", "SEGUIN", "SELDEN", 
"SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", "SHARON", "SHARPSBURG", 
"SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", "SHELLSBURG", "SHELTON", 
"SHENANDOAH", "SHORELINE", "SHOREWOOD", "SHREVEPORT", "SICKLERVILLE", 
"SILER CITY", "SILOAM SPRINGS", "SIMON", "SIMPSONVILLE", "SIMSBURY", 
"SIOUX CITY", "SIOUX FALLS", "SKOKIE", "SKOWHEGAN", "SLATINGTON", 
"SLIDELL", "SMITHFIELD", "SN BERNRDNO", "SNELLVILLE", "SOCORRO", 
"SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH BELOIT", "SOUTH BEND", 
"SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH PARK", "SOUTH PLAINFIELD", 
"SOUTH SIOUX CITY", "SOUTHAMPTON", "SOUTHBURY", "SOUTHGATE", 
"SPARKS", "SPARROW BUSH", "SPENCER", "SPRING", "SPRING BRANCH", 
"SPRING GROVE", "SPRING HILL", "SPRING HOPE", "SPRING VALLEY", 
"SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD GARDENS", "SPRINGHILL", 
"SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", "ST PETERSBURG", 
"ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", "STANLEY", 
"STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", "STILLMAN VALLEY", 
"STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", "STRAFFORD", 
"STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", "SUFFIELD", 
"SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", "SUMMIT", 
"SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", "SWEENY", 
"SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", "TAFTVILLE", 
"TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", "TAYLORVILLE", 
"TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", "THOMASVILLE", 
"TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY PARK", 
"TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", "TOPEKA", 
"TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS RST", 
"TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER", 
"TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY", 
"UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA", 
"URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO", 
"VAN BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO", 
"VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA", 
"VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH", 
"W TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN", 
"WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG", 
"WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA", 
"WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE", 
"WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER", 
"WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO", 
"WEST CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN", 
"WEST HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH", 
"WEST POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD", 
"WEST VALLEY CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER", 
"WESTMORELAND CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD", 
"WHARTON", "WHEELING", "WHITE PLAINS", "WHITEHALL", "WHITESTONE", 
"WHITESTOWN", "WHITING", "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD", 
"WILLARD", "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK", 
"WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS", 
"WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON", 
"WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE", 
"WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS", 
"WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH", 
"YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS", 
"YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor"), 
    clusters = c(6L, 10L, 3L, 10L, 5L, 6L, 10L, 6L, 6L, 7L, 5L, 
    7L, 8L, 8L, 3L, 6L, 3L, 10L, 7L, 10L, 10L, 4L, 2L, 8L, 9L, 
    1L, 4L, 4L, 7L, 5L, 10L, 5L, 5L, 10L, 6L, 6L, 10L, 6L, 7L, 
    10L, 10L, 3L, 3L, 6L, 3L), Longitude = c(-93.883554, -90.582058, 
    -83.868923, -89.544083, -72.902467, -94.458904, -90.588533, 
    -94.527843, -92.93769, -80.029456, -70.58809, -82.888789, 
    -80.113071, -82.6661, -81.372992, -95.840964, -82.016013, 
    -89.92625, -80.276676, -89.778385, -87.641063, -94.888036, 
    -117.663119, -82.530568, -75.574437, -122.209047, -96.799309, 
    -95.746255, -80.553953, -73.923314, -87.713482, -73.9731, 
    -73.8187, -87.987023, -93.759478, -94.110903, -90.00597, 
    -93.257109, -79.932483, -90.201858, -89.100233, -84.625923, 
    -84.567614, -93.912921, -84.612564), Latitude = c(34.503045, 
    38.752515, 33.406527, 40.893588, 41.733088, 38.999032, 41.657674, 
    38.890929, 37.639766, 42.099693, 43.53462, 40.142492, 26.538149, 
    27.9062, 31.90104, 34.833083, 35.118836, 35.136047, 39.425712, 
    40.519207, 40.101126, 30.044457, 33.62287, 27.910276, 39.752254, 
    47.403778, 32.961929, 29.820199, 38.408649, 40.668828, 41.983948, 
    40.58116, 40.7253, 41.921667, 41.669921, 36.442552, 38.564663, 
    37.455315, 40.867774, 38.783554, 42.244381, 34.024332, 34.001644, 
    36.622704, 34.017832), distances_Me = c(1879089.079, 1437296.523, 
    1185638.742, 1313997.97, 156759.9125, 1759443.351, 1398398.997, 
    1767861.917, 1668917.322, 534234.2576, 431749.2965, 758508.4701, 
    1657031.938, 1615575.668, 1169085.702, 2024642.021, 930944.4681, 
    1525322.746, 553069.7532, 1337002.822, 1161367.37, 2223496.376, 
    3918520.212, 1609088.602, 164252.5146, 3878581.001, 2197910.688, 
    2304526.789, 614908.3151, 11075.24597, 1160375.454, 494.2997496, 
    21063.72622, 1182681.434, 1662351.873, 1810881.911, 1393581.436, 
    1701982.013, 504395.772, 1404348.991, 1276505.454, 1190457.718, 
    1188029.899, 1787261.336, 1189994.055), distances_Mi = c(1167.607468, 
    893.0913246, 736.719012, 816.477441, 97.40573055, 1093.263337, 
    868.9216123, 1098.494372, 1037.01328, 331.9565399, 268.2755749, 
    471.3135552, 1029.628072, 1003.868436, 726.4334682, 1258.049536, 
    578.4599174, 947.7880794, 343.6603307, 830.7719404, 721.6375366, 
    1381.611443, 2434.846498, 999.8375755, 102.0614003, 2410.029515, 
    1365.713293, 1431.96122, 382.0848884, 6.881813138, 721.0211909, 
    0.30714248, 13.08834388, 734.8814328, 1032.933715, 1125.225657, 
    865.9281298, 1057.55865, 313.4158337, 872.6187534, 793.1807589, 
    739.7133739, 738.2048023, 1110.548567, 739.4252678), ID = 2308:2352), row.names = c(NA, 
-45L), class = "data.frame")

Proprietary

NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Thu May 14 13:11:41 2020
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Thu, 14 May 2020 13:11:41 +0200
Subject: [R] (no subject)
Message-ID: <CAMk+s2T+opiyMpZRKxE6HBbCJ+HKJ=gz0T0-fociZYn7=wSF-Q@mail.gmail.com>

Dear all,
I am trying to upgrade from ubuntu 19 to 20 but I got some R stuff that
stop the process:
```
$ apt list --upgradable
Listing... Done
r-cran-kernsmooth/xenial 2.23-15-3xenial0 amd64 [upgradable from:
2.23-15-3build2]
r-cran-nnet/xenial 7.3-12-2xenial0 amd64 [upgradable from: 7.3-12-2build2]
$ R --version
R version 3.6.1 (2019-07-05) -- "Action of the Toes"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)
```
Since I have R 3.6 how can I remove or update these r-cran things?



-- 
Best regards,
Luigi

	[[alternative HTML version deleted]]


From Po||ngW @end|ng |rom @etn@@com  Thu May 14 13:29:04 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Thu, 14 May 2020 11:29:04 +0000
Subject: [R] Help with map()
In-Reply-To: <CA+8X3fWwQ2HrBUxUW1Od5AeBegfmurkpz1Gu-wFhfAZ4dX9cmQ@mail.gmail.com>
References: <BYAPR06MB5383688B7688345330422859AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CA+8X3fWwQ2HrBUxUW1Od5AeBegfmurkpz1Gu-wFhfAZ4dX9cmQ@mail.gmail.com>
Message-ID: <BYAPR06MB538311FCA8A217B27743C85BAEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>


Hi Jim, and Mark, thank you for your response.

1. I have restarted R
2. I have only initiated library(magrittr)#for %>% function & library(plotrix), no other libraries thinking that another may be overwriting something.
3. I have checked the str()

str(radius3)
'data.frame':	1990 obs. of  6 variables:
 $ MBR_SUBSCRIBERID: Factor w/ 2352 levels "101040199600",..: 590 908 976 509 1674 690 1336 726 1702 2331 ...
 $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 32 10 25 11 9 32 13 12 12 17 ...
 $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 932 156 230 698 965 1330 515 1127 1304 1316 ...
 $ Latitude        : num  40.4 31.2 40.8 42.1 26.8 ...
 $ Longitude       : num  -79.9 -81.5 -74 -91.6 -82.1 ...
 $ distances_Mi    : num  310.3 769.9 16.1 920.4 1057.6 ...

str(clus1)
tibble [53 x 7] (S3: tbl_df/tbl/data.frame)
 $ MBR_SUBSCRIBERID: Factor w/ 2352 levels "101040199600",..: 86 2111 995 899 953 924 769 92 790 1748 ...
 $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 31 39 4 39 39 39 39 31 39 39 ...
 $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 727 85 455 1018 1203 604 1169 727 984 295 ...
 $ clusters        : num [1:53] 1 1 1 1 1 1 1 1 1 1 ...
 $ Latitude        : num [1:53] 42.4 47.6 39.2 46.9 48 ...
 $ Longitude       : num [1:53] -123 -122 -121 -123 -122 ...
 $ distances_Mi    : num [1:53] 2499 2406 2472 2430 2408 ...

4. I change clus1 to DF
'data.frame':	53 obs. of  7 variables:
 $ MBR_SUBSCRIBERID: Factor w/ 2352 levels "101040199600",..: 86 2111 995 899 953 924 769 92 790 1748 ...
 $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 31 39 4 39 39 39 39 31 39 39 ...
 $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 727 85 455 1018 1203 604 1169 727 984 295 ...
 $ clusters        : num  1 1 1 1 1 1 1 1 1 1 ...
 $ Latitude        : num  42.4 47.6 39.2 46.9 48 ...
 $ Longitude       : num  -123 -122 -121 -123 -122 ...
 $ distances_Mi    : num  2499 2406 2472 2430 2408 ...

Then try again, no luck?

Weird?

Thanks

WHP

Proprietary

-----Original Message-----
From: Jim Lemon <drjimlemon at gmail.com> 
Sent: Thursday, May 14, 2020 5:44 AM
To: Poling, William <PolingW at aetna.com>
Cc: r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with map()

**** External Email - Use Caution ****

Hi Bill,
Have you compared str(radius3) to str(clus1)? It may be quite different.

Jim

On Thu, May 14, 2020 at 8:24 PM Poling, William via R-help <r-help at r-project.org> wrote:
>
> #RStudio Version Version 1.2.1335
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit) #Running under: Windows 10 
> x64 (build 17763)
>
> Good morning.
>
> I ran routines like this one yesterday with no errors.
>
> #Radius3----
> str(radius3)
>
> radius3 <- individual_dets_sf_3X %>%
>
>   dplyr::select(MBR_SUBSCRIBERID,state,city,Latitude,Longitude,distances_Mi) %>%
>   filter(distances_Mi <= 1200)
>
> str(radius3)
>
>   geomat<-makeDensityMatrix(radius3[,c("Latitude","Longitude")],
>                             
> xlim=range(radius3$Longitude),ylim=range(radius3$Latitude))
>
> latlim<-range(radius3$Latitude)
> lonlim<-range(radius3$Longitude)
>
> <- map("world",xlim=lonlim,ylim=latlim)
> axis(1)
> axis(2)
> densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
>             red=c(0.5,1),green=0,blue=0,pch=15)
> title("Member Geo Density Plot For Radius3")
>
>
> This morning I am trying slightly new routine and receiving this error
>
> clus1 <- individual_dets_sf_3X %>%
>
>   dplyr::select(MBR_SUBSCRIBERID,state,city,clusters,Latitude,Longitude,distances_Mi) %>%
>   filter(clusters == 1)
>
> geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
>                           
> xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))
>
> latlim<-range(clus1$Latitude)
> lonlim<-range(clus1$Longitude)
>
> map("world",xlim=lonlim,ylim=latlim)
> axis(1)
> axis(2)
> densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
>             red=c(0.5,1),green=0,blue=0,pch=15)
> title("Member Geo Density Plot For Cluster1")
>
>
> geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
> +                           
> + xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))
>
> > latlim<-range(clus1$Latitude)
> > lonlim<-range(clus1$Longitude)
> >
> > map("world",xlim=lonlim,ylim=latlim)
> Error in .C(C_map_type, as.character(mapbase), integer(1)) :
>   Incorrect number of arguments (2), expecting 0 for ''
> > axis(1)
> Error in axis(1) : plot.new has not been called yet
> > axis(2)
> Error in axis(2) : plot.new has not been called yet
> > densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
> +             red=c(0.5,1),green=0,blue=0,pch=15)
> Error in plot.xy(xy.coords(x, y), type = type, ...) :
>   plot.new has not been called yet
> > title("Member Geo Density Plot For Cluster1")
> Error in title("Member Geo Density Plot For Cluster1") :
>   plot.new has not been called yet
>
>
> It seems to error at the point of map("world",xlim=lonlim,ylim=latlim)
>
> I find a ref in stack overflow 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stackoverflow.com
> _questions_45066628_cannot-2Drun-2Dmap-2Ddatastate&d=DwIBaQ&c=wluqKIiw
> ffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2Ex
> lYvnUWbsByY&m=sRhKIDOv4AuwrfDBU9b2Rpj-VPfs0lcpkQqNBfVLSe4&s=sEjkBzlaFR
> 7NhUDpUPGPp1nOkAxOlBbFNadLowcDbFw&e=
> and tried solutions but they do not seem to work?
>
> Here is a sample (MBR_SUBSCRIBERID(factor) is replaced with ID(integer) but of no consequence in my routine since not really used.
>
> I hope someone recognizes the problem.
>
> Thank you for any advice
>
> WHP
>
> > dput(sample)
> structure(list(state = structure(c(2L, 22L, 10L, 12L, 6L, 22L, 11L, 
> 22L, 22L, 32L, 19L, 29L, 9L, 9L, 10L, 30L, 33L, 35L, 41L, 12L, 12L, 
> 36L, 4L, 9L, 8L, 39L, 36L, 36L, 41L, 28L, 12L, 28L, 28L, 12L, 11L, 2L, 
> 12L, 22L, 32L, 22L, 12L, 10L, 10L, 22L, 10L ), .Label = c("AL", "AR", 
> "AZ", "CA", "CO", "CT", "DC", "DE", "FL", "GA", "IA", "IL", "IN", 
> "KS", "KY", "LA", "MA", "MD", "ME", "MI", "MN", "MO", "NC", "NE", 
> "NJ", "NM", "NV", "NY", "OH", "OK", "OR", "PA", "SC", "SD", "TN", 
> "TX", "UT", "VA", "WA", "WI", "WV"
> ), class = "factor"), city = structure(c(838L, 1030L, 262L, 218L, 
> 166L, 973L, 339L, 451L, 660L, 358L, 281L, 1280L, 129L, 223L, 989L, 
> 721L, 550L, 731L, 1325L, 688L, 1184L, 281L, 759L, 1171L, 1305L, 587L, 
> 272L, 581L, 263L, 152L, 217L, 152L, 390L, 5L, 571L, 1006L, 1162L, 
> 939L, 170L, 1033L, 1002L, 586L, 586L, 192L, 586L ), .Label = 
> c("ABBOTTSTOWN", "ABILENE", "ACWORTH", "ADAMS", "ADDISON", "ADKINS", 
> "ALBANY", "ALBIA", "ALBUQUERQUE", "ALEXANDRIA", "ALFRED", "ALIQUIPPA", 
> "ALISO VIEJO", "ALLEN PARK", "ALLENTOWN", "ALPHA", "ALPHARETTA", 
> "ALPINE", "ALTOONA", "AMARILLO", "AMBLER", "AMBRIDGE", "AMHERST", 
> "AMITYVILLE", "ANAHIEM", "ANDERSON", "ANDOVER", "ANGIER", "ANGLETON", 
> "ANKENY", "ANN ARBOR", "ANZA", "APEX", "APOLLO", "ARCADIA", 
> "ARCHDALE", "ARDMORE", "ARGYLE", "ARKANSAS CITY", "ARLINGTON", 
> "ARNOLD", "ARTHUR", "ASHEBORO", "ASHEVILLE", "ASHLAND", "ASHLAND 
> CITY", "ASHTON", "ASTON", "ATHENS", "ATLANTA", "ATLANTIC BCH", 
> "AUBORN", "AUGUSTA", "AURORA", "AUSTELL", "AUSTIN", "AVENTURA", 
> "AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN CITY", "BALDWIN PLACE", 
> "BALLWIN", "BANGOR", "BARBOURSVILLE", "BARNEGAT", "BARTLETT", 
> "BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", "BATESBURG", "BATON 
> ROUGE", "BAXTER SPRINGS", "BAY CITY", "BAYONNE", "BAYSIDE", "BAYTOWN", 
> "BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER CITY", "BELFAIR", "BELLE 
> VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR", "BELLMORE", "BELLWOOD", 
> "BELMAR", "BELMONT", "BELVIDERE", "BENNET", "BENTON", "BENTONVILLE", 
> "BERLIN", "BESSEMER CITY", "BETHANY", "BETHEL", "BETHEL PARK", 
> "BETHESDA", "BETHLEHEM", "BETTENDORF", "BIGLERVILLE", "BINGHAMTON", 
> "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN", "BLACKSBURG", "BLAINE", 
> "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD", "BLUE BELL", "BLUE GRASS", 
> "BLUE POINT", "BLUE SPRINGS", "BOCA RATON", "BODFISH", "BOILING 
> SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS", "BONNEY LAKE", 
> "BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING GREEN", 
> "BOYERTOWN", "BOYNTON BEACH", "BRADENTON", "BRADENVILLE", 
> "BRANDAMORE", "BRANDON", "BRANFORD", "BRANSON", "BRASELTON", 
> "BREEZEWOOD", "BRENTON", "BRENTWOOD", "BREWSTER", "BRICK", 
> "BRIDGEPORT", "BRIDGETON", "BRIGHAM CITY", "BROADDUS", "BROCKPORT", 
> "BROGUE", "BROKEN ARROW", "BRONX", "BROOKFIELD", "BROOKHAVEN", 
> "BROOKLYN", "BROWNS MILLS", "BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", 
> "BRYAN", "BUCKSPORT", "BUDD LAKE", "BUFFALO", "BULVERDE", 
> "BUNKERVILLE", "BURBANK", "BURFORD", "BURIEN", "BURLINGTON", "BURR 
> RIDGE", "BURTON", "BUSKIRK", "BUTLER", "BUXTON", "BYRON", "CALERA", 
> "CAMBRIDGE", "CAMP HILL", "CAMPBELL", "CANAAN", "CANAL WINCHESTER", 
> "CANASTOTA", "CANTON", "CARDIFF BY THE SEA", "CARL JUNCTION", 
> "CARLISLE", "CARLTON", "CAROL STREAM", "CARROLLTON", "CARSON CITY", 
> "CARTERSVILLE", "CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", 
> "CASWELL", "CATO", "CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER 
> POINT", "CENTERTON", "CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN 
> FALLS", "CHALFONT", "CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", 
> "CHARLESTON", "CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", 
> "CHESHIRE", "CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", 
> "CHILLICOTHE", "CHOCTAW", "CICERO", "CINCINNATI", "CLAY", 
> "CLEARWATER", "CLEARWATER BEACH", "CLEMMONS", "CLENDENIN", 
> "CLEVELAND", "CLEVELAND HTS", "CLEVES", "CLIFFSIDE PARK", "CLIFTON", 
> "CLIFTON PARK", "CLIFTON SPGS", "CLINTON", "COACHELLA", "COAL 
> TOWNSHIP", "COATESVILLE", "COLLEGE STATION", "COLLINSVILLE", 
> "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE", "CONCORD", "CONCORD 
> TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY", "CONYERS", 
> "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA", "CORNELIUS", 
> "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY", "COTTAGE HILLS", 
> "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN", "CRANBERRY 
> TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER", "CUMBERLAND 
> FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS", "DALLAS", 
> "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT", 
> "DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR", 
> "DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH", 
> "DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY", "DES 
> MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER", "DIXON", 
> "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER", "DOYLESTOWN", 
> "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH", "DUNCAN", "DUNCANNON", 
> "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE", "DURHAM", "DUTTON", "EAGLE 
> GROVE", "EAST AURORA", "EAST CHICAGO", "EAST EARL", "EAST HANOVER", 
> "EAST HARTFORD", "EAST HARTLAND", "EAST LEROY", "EAST LIVERPOOL", 
> "EAST MEADOW", "EAST NORTHPORT", "EAST ORANGE", "EAST PEORIA", "EAST 
> SAINT LOUIS", "EASTON", "EDGERTON", "EDISON", "EDMOND", "EGG HBR TWP", 
> "EL DORADO", "EL PASO", "ELDRIDGE", "ELGIN", "ELIZABETHVILLE", 
> "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY", "ELMHURST", "ELMWOOD PARK", 
> "ELOY", "ELY", "EMERSON", "EMORY", "ENDICOTT", "ENGLEWOOD", "ENID", 
> "ENNICE", "ENOLA", "ENUMCLAW", "EPHRATA", "ERIE", "ESTERO", "EUREKA 
> SPRINGS", "EVANSVILLE", "EVERETT", "EXCELSIOR SPRINGS", "EXTON", 
> "FAIRFAX", "FAIRFIELD", "FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", 
> "FALMOUTH", "FAR HILLS", "FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", 
> "FAYETTE", "FAYETTEVILLE", "FELTON", "FENTON", "FERGUSON", "FIELDALE", 
> "FINDLAY", "FINGERVILLE", "FLEMINGTON", "FLINT", "FLORENCE", 
> "FLORESVILLE", "FLORISSANT", "FLOVILLA", "FLOWER MOUND", "FLUSHING", 
> "FOGELSVILLE", "FOLSOM", "FONTANELLE", "FOREST HILLS", "FOREST PARK", 
> "FORT DODGE", "FORT FAIRFIELD", "FORT GAY", "FORT LAUDERDALE", "FORT 
> LEE", "FORT MILL", "FORT MOHAVE", "FORT PAYNE", "FORT PIERCE", "FORT 
> SMITH", "FORT WAYNE", "FORT WORTH", "FOSTORIA", "FOUNTAIN INN", 
> "FRANKFORT", "FRANKLIN", "FREEDOM", "FREEHOLD", "FREEPORT", "FREMONT", 
> "FULLERTON", "FULSHEAR", "FULTON", "GADSDEN", "GAFFNEY", 
> "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE", "GARDNERS", "GARNETT 
> VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA", "GETTYSBURG", 
> "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE", "GLEN CARBON", "GLEN 
> COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW", "GLENWOOD", "GOLDEN 
> CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM", "GOSHEN", 
> "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS", "GRANDVIEW", 
> "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS", "GRASS VALLEY", 
> "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER", "GREENLAWN", 
> "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE", "GREER", 
> "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD", "HADLEY", 
> "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON", "HAMBURG", 
> "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE", 
> "HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON", 
> "HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL", 
> "HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE", 
> "HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH", 
> "HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE", "HIGH 
> HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS", 
> "HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE", 
> "HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD", 
> "HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS", "HOT 
> SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON", "HUFFMAN", 
> "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON", "HUNTINGTON 
> BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE", "HURST", 
> "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL", 
> "INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY", "IRMO", 
> "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP", "ISSAQUAH", 
> "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS", "JACKSONVILLE", 
> "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE", "JEFFERSON", "JEFFERSON 
> CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN", "JONESBORO", "JONESPORT", 
> "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE", "KANNAPOLIS", "KANSAS 
> CITY", "KATY", "KEARNEY", "KELLERTON", "KEMAH", "KENMORE", "KENNESAW", 
> "KENT", "KINGMAN", "KINGS MOUNTAIN", "KINGSLAND", "KINGSTON", 
> "KINGWOOD", "KIRKLAND", "KUTZTOWN", "LA QUINTA", "LA RUE", "LA VERNE", 
> "LAFAYETTE", "LAKE BLUFF", "LAKE IN THE HILL", "LAKE ISABELLA", "LAKE 
> PLACID", "LAKE SAINT LOUIS", "LAKE TAPPS", "LAKE VILLAGE", "LAKE 
> WAUKOMIS", "LAKE WORTH", "LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD 
> RCH", "LAMBERTVILLE", "LANCASTER", "LANDISVILLE", "LANSDALE", 
> "LAREDO", "LARGO", "LAS VEGAS", "LATROBE", "LAUDERHILL", "LAWNDALE", 
> "LAWRENCE", "LAWRENCEVILLE", "LE MARS", "LE ROY", "LEAGUE CITY", 
> "LEAVENWORTH", "LEAWOOD", "LEBANON", "LECOMPTON", "LEECHBURG", "LEES 
> SUMMIT", "LEESBURG", "LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", 
> "LEVITTOWN", "LEWES", "LEWISTOWN", "LEXINGTON", "LIBERTY TWP", 
> "LILLINGTON", "LINCOLN", "LINDENHURST", "LINTON", "LISBON", 
> "LITCHFIELD PK", "LITHIA", "LITITZ", "LITTLE EGG HARBOR TO", "LK 
> FOREST PK", "LK PEEKSKILL", "LOCKHART", "LOCKWOOD", "LOGAN", "LOMA 
> LINDA", "LONE JACK", "LONG BEACH", "LONG LANE", "LONGVIEW", "LORAIN", 
> "LORTON", "LOS ANGELES", "LOUISBURG", "LOVES PARK", "LOWER BURRELL", 
> "LOWRY CITY", "LUBBOCK", "LUGOFF", "LUMBERTON", "LYNBROOK", 
> "LYNNWOOD", "MACHESNEY PARK", "MACON", "MAGNOLIA", "MAHOPAC", 
> "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE", "MANASSAS", "MANCHACA", 
> "MANCHESTER", "MANCHESTER TOWNSHIP", "MANCHESTER TW", "MANHEIM", 
> "MANITO", "MANLIUS", "MANNINGTON", "MANSFIELD", "MANSURA", "MAPLE 
> PARK", "MARBLE FALLS", "MARCELLUS", "MARICOPA", "MARIETTA", "MARION", 
> "MARKLEYSBURG", "MARS HILL", "MARSHALLTOWN", "MARSHFIELD", 
> "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE", "MASON CITY", "MASSEY", 
> "MASSILLON", "MASURY", "MATAWAN", "MATEWAN", "MATTAPOISETT", 
> "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND", "MC DONALD", "MC 
> KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN", "MCALESTER", "MCDONOUGH", 
> "MCKEESPORT", "MCKINNEY", "MECHANICSBURG", "MECHANICSVILLE", 
> "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS", "MERIDEN", 
> "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA", "MESQUITE", 
> "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN", "MIDLAND", 
> "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK", "MILLEDGEVILLE", 
> "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE", "MINEOLA", "MINERAL 
> SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION", "MISSION HILLS", 
> "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY", "MOLINE", 
> "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE", "MONTCLAIR", 
> "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON", "MORGANTOWN", 
> "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE", "MOUNT JOY", 
> "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO", "MOUNT UNION", 
> "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT", "MULVANE", 
> "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT", 
> "MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS", 
> "NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES", 
> "NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG", "NEW 
> BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK", "NEW 
> CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON", "NEW 
> LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE", "NEW 
> ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK", 
> "NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT", 
> "NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA", 
> "NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK", 
> "NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON", 
> "NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND 
> HILLS", "NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE", 
> "NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON", "OAK 
> RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE", "OMAHA", 
> "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION", "ORONO", 
> "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO", "OTTAWA", 
> "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD", "OYSTER BAY", 
> "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR", "PALMDALE", "PALMYRA", 
> "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE", "PARLIN", "PARMA", 
> "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY", "PAXTON", "PEA 
> RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN", "PELION", 
> "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE", "PENNSBURG", 
> "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS", "PERRYSBURG", 
> "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI", "PHOENIX", 
> "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN", "PINE BLUFF", 
> "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY", "PITMAN", 
> "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY", 
> "PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO 
> LAKE", "POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", 
> "PORT CHARLOTTE", "PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", 
> "PORT WASHINGTON", "PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", 
> "POTTSVILLE", "POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE 
> VILLAGE", "PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT 
> PLEASANT", "PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", 
> "QUINCY", "RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", 
> "READLYN", "RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO 
> PARK", "REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", 
> "RICH HILL", "RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", 
> "RINCON", "RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", 
> "RIVERSIDE", "ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", 
> "ROCKAWAY BCH", "ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", 
> "ROGERS", "ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE", 
> "ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT", 
> "ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN", 
> "RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO", 
> "SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR", "SAINT 
> CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS", "SAINT 
> PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY", 
> "SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO", 
> "SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN", 
> "SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", 
> "SARASOTA", "SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", 
> "SAYVILLE", "SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", 
> "SCOTT", "SCOTTSDALE", "SEAFORD", "SEALE", "SEATTLE", "SEDONA", 
> "SEGUIN", "SELDEN", "SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", 
> "SHARON", "SHARPSBURG", "SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", 
> "SHELLSBURG", "SHELTON", "SHENANDOAH", "SHORELINE", "SHOREWOOD", 
> "SHREVEPORT", "SICKLERVILLE", "SILER CITY", "SILOAM SPRINGS", "SIMON", 
> "SIMPSONVILLE", "SIMSBURY", "SIOUX CITY", "SIOUX FALLS", "SKOKIE", 
> "SKOWHEGAN", "SLATINGTON", "SLIDELL", "SMITHFIELD", "SN BERNRDNO", 
> "SNELLVILLE", "SOCORRO", "SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH 
> BELOIT", "SOUTH BEND", "SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH 
> PARK", "SOUTH PLAINFIELD", "SOUTH SIOUX CITY", "SOUTHAMPTON", 
> "SOUTHBURY", "SOUTHGATE", "SPARKS", "SPARROW BUSH", "SPENCER", 
> "SPRING", "SPRING BRANCH", "SPRING GROVE", "SPRING HILL", "SPRING 
> HOPE", "SPRING VALLEY", "SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD 
> GARDENS", "SPRINGHILL", "SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", 
> "ST PETERSBURG", "ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", 
> "STANLEY", "STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", 
> "STILLMAN VALLEY", "STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", 
> "STRAFFORD", "STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", 
> "SUFFIELD", "SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", 
> "SUMMIT", "SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", 
> "SWEENY", "SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", 
> "TAFTVILLE", "TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", 
> "TAYLORVILLE", "TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", 
> "THOMASVILLE", "TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY 
> PARK", "TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", 
> "TOPEKA", "TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS 
> RST", "TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER", 
> "TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY", 
> "UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA", 
> "URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO", "VAN 
> BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO", 
> "VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA", 
> "VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH", "W 
> TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN", 
> "WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG", 
> "WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA", 
> "WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE", 
> "WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER", 
> "WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO", "WEST 
> CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN", "WEST 
> HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH", "WEST 
> POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD", "WEST VALLEY 
> CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER", "WESTMORELAND 
> CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD", "WHARTON", "WHEELING", 
> "WHITE PLAINS", "WHITEHALL", "WHITESTONE", "WHITESTOWN", "WHITING", 
> "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD", "WILLARD", 
> "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK", 
> "WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS", 
> "WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON", 
> "WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE", 
> "WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS", 
> "WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH", 
> "YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS", "YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor"),
>     clusters = c(6L, 10L, 3L, 10L, 5L, 6L, 10L, 6L, 6L, 7L, 5L,
>     7L, 8L, 8L, 3L, 6L, 3L, 10L, 7L, 10L, 10L, 4L, 2L, 8L, 9L,
>     1L, 4L, 4L, 7L, 5L, 10L, 5L, 5L, 10L, 6L, 6L, 10L, 6L, 7L,
>     10L, 10L, 3L, 3L, 6L, 3L), Longitude = c(-93.883554, -90.582058,
>     -83.868923, -89.544083, -72.902467, -94.458904, -90.588533,
>     -94.527843, -92.93769, -80.029456, -70.58809, -82.888789,
>     -80.113071, -82.6661, -81.372992, -95.840964, -82.016013,
>     -89.92625, -80.276676, -89.778385, -87.641063, -94.888036,
>     -117.663119, -82.530568, -75.574437, -122.209047, -96.799309,
>     -95.746255, -80.553953, -73.923314, -87.713482, -73.9731,
>     -73.8187, -87.987023, -93.759478, -94.110903, -90.00597,
>     -93.257109, -79.932483, -90.201858, -89.100233, -84.625923,
>     -84.567614, -93.912921, -84.612564), Latitude = c(34.503045,
>     38.752515, 33.406527, 40.893588, 41.733088, 38.999032, 41.657674,
>     38.890929, 37.639766, 42.099693, 43.53462, 40.142492, 26.538149,
>     27.9062, 31.90104, 34.833083, 35.118836, 35.136047, 39.425712,
>     40.519207, 40.101126, 30.044457, 33.62287, 27.910276, 39.752254,
>     47.403778, 32.961929, 29.820199, 38.408649, 40.668828, 41.983948,
>     40.58116, 40.7253, 41.921667, 41.669921, 36.442552, 38.564663,
>     37.455315, 40.867774, 38.783554, 42.244381, 34.024332, 34.001644,
>     36.622704, 34.017832), distances_Me = c(1879089.079, 1437296.523,
>     1185638.742, 1313997.97, 156759.9125, 1759443.351, 1398398.997,
>     1767861.917, 1668917.322, 534234.2576, 431749.2965, 758508.4701,
>     1657031.938, 1615575.668, 1169085.702, 2024642.021, 930944.4681,
>     1525322.746, 553069.7532, 1337002.822, 1161367.37, 2223496.376,
>     3918520.212, 1609088.602, 164252.5146, 3878581.001, 2197910.688,
>     2304526.789, 614908.3151, 11075.24597, 1160375.454, 494.2997496,
>     21063.72622, 1182681.434, 1662351.873, 1810881.911, 1393581.436,
>     1701982.013, 504395.772, 1404348.991, 1276505.454, 1190457.718,
>     1188029.899, 1787261.336, 1189994.055), distances_Mi = c(1167.607468,
>     893.0913246, 736.719012, 816.477441, 97.40573055, 1093.263337,
>     868.9216123, 1098.494372, 1037.01328, 331.9565399, 268.2755749,
>     471.3135552, 1029.628072, 1003.868436, 726.4334682, 1258.049536,
>     578.4599174, 947.7880794, 343.6603307, 830.7719404, 721.6375366,
>     1381.611443, 2434.846498, 999.8375755, 102.0614003, 2410.029515,
>     1365.713293, 1431.96122, 382.0848884, 6.881813138, 721.0211909,
>     0.30714248, 13.08834388, 734.8814328, 1032.933715, 1125.225657,
>     865.9281298, 1057.55865, 313.4158337, 872.6187534, 793.1807589,
>     739.7133739, 738.2048023, 1110.548567, 739.4252678), ID = 
> 2308:2352), row.names = c(NA, -45L), class = "data.frame")
>
> Proprietary
>
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may 
> con...{{dropped:16}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIBaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmv
> OXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sRhKIDOv4AuwrfD
> BU9b2Rpj-VPfs0lcpkQqNBfVLSe4&s=CloI44kowc5J_mOah6vx_Kl4PFIeqxmwexvRiqw
> xuOA&e= PLEASE do read the posting guide 
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIBaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmm
> vOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sRhKIDOv4Auwrf
> DBU9b2Rpj-VPfs0lcpkQqNBfVLSe4&s=iOWreiJDomXv-uJ3NzQNX0IUsupILo_4DmkFK3
> GRUig&e= and provide commented, minimal, self-contained, reproducible 
> code.

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From kry|ov@r00t @end|ng |rom gm@||@com  Thu May 14 13:33:22 2020
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Thu, 14 May 2020 14:33:22 +0300
Subject: [R] (no subject)
In-Reply-To: <CAMk+s2T+opiyMpZRKxE6HBbCJ+HKJ=gz0T0-fociZYn7=wSF-Q@mail.gmail.com>
References: <CAMk+s2T+opiyMpZRKxE6HBbCJ+HKJ=gz0T0-fociZYn7=wSF-Q@mail.gmail.com>
Message-ID: <20200514143322.52c7c9d1@Tarkus>

Dear Luigi,

These questions (and your previous one about R on Chromebook) are best
directed to the R-SIG-Debian mailing list [*], or maybe Ubuntu forums
[**]. I think that they are off-topic here on R-help.

Here is a short advice: try to make sure that you have the
r-recommended package installed from CRAN, not Ubuntu repository. If
that doesn't help, try to raise the priority [***] of packages from the
CRAN repository in order to avoid them being superseded by Ubuntu
packages. Another workaround would be removing the *.deb kernsmooth and
nnet packages from the system and installing them manually from CRAN in
the personal library by means of install.packages().

> 	[[alternative HTML version deleted]]

Please don't post in HTML (see link to posting guide below).

-- 
Best regards,
Ivan

[*] https://stat.ethz.ch/mailman/listinfo/r-sig-debian
[**] https://ubuntuforums.org/
[***] https://jaqque.sbih.org/kplug/apt-pinning.html


From Po||ngW @end|ng |rom @etn@@com  Thu May 14 13:45:35 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Thu, 14 May 2020 11:45:35 +0000
Subject: [R] Help with map()
In-Reply-To: <BYAPR06MB538311FCA8A217B27743C85BAEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB5383688B7688345330422859AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CA+8X3fWwQ2HrBUxUW1Od5AeBegfmurkpz1Gu-wFhfAZ4dX9cmQ@mail.gmail.com>
 <BYAPR06MB538311FCA8A217B27743C85BAEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <BYAPR06MB53836AD813508C208C7D5B61AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>

Hi, I ran this which is partially successful. I got long & lat  (x&y) with red plot values however, no map behind it?

Not sure what I might be missing now in terms of pkgs I suppose?
#Just use the basic pkgs
library(magrittr)#for %>% function
library(plotrix)
library(maps)
library(dplyr)#For filter()
str(individual_dets_sf_3X)
str(radius3)
str(clus1)


#rm(clus1)

clus1 <- individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,state,city,clusters,Latitude,Longitude,distances_Mi) %>% #Notice now that these are capitalized <--"L"
  
  filter(clusters == 1)

str(clus1)

clus1 <- as.data.frame(clus1)

geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
                          xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))
# Range of density (>0) - 1.292155 3.89646 
latlim<-range(clus1$Latitude)
lonlim<-range(clus1$Longitude)

map("world",xlim=lonlim,ylim=latlim)
axis(1)
axis(2)
densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
            red=c(0.5,1),green=0,blue=0,pch=15)
title("Member Geo Density Plot For Cluster1")





Proprietary

-----Original Message-----
From: Poling, William 
Sent: Thursday, May 14, 2020 6:29 AM
To: Jim Lemon <drjimlemon at gmail.com>
Cc: r-help at r-project.org; Mark Fowler <gmark.fowler at outlook.com>
Subject: RE: [EXTERNAL] Re: [R] Help with map()


Hi Jim, and Mark, thank you for your response.

1. I have restarted R
2. I have only initiated library(magrittr)#for %>% function & library(plotrix), no other libraries thinking that another may be overwriting something.
3. I have checked the str()

str(radius3)
'data.frame':	1990 obs. of  6 variables:
 $ MBR_SUBSCRIBERID: Factor w/ 2352 levels "101040199600",..: 590 908 976 509 1674 690 1336 726 1702 2331 ...
 $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 32 10 25 11 9 32 13 12 12 17 ...
 $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 932 156 230 698 965 1330 515 1127 1304 1316 ...
 $ Latitude        : num  40.4 31.2 40.8 42.1 26.8 ...
 $ Longitude       : num  -79.9 -81.5 -74 -91.6 -82.1 ...
 $ distances_Mi    : num  310.3 769.9 16.1 920.4 1057.6 ...

str(clus1)
tibble [53 x 7] (S3: tbl_df/tbl/data.frame)  $ MBR_SUBSCRIBERID: Factor w/ 2352 levels "101040199600",..: 86 2111 995 899 953 924 769 92 790 1748 ...
 $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 31 39 4 39 39 39 39 31 39 39 ...
 $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 727 85 455 1018 1203 604 1169 727 984 295 ...
 $ clusters        : num [1:53] 1 1 1 1 1 1 1 1 1 1 ...
 $ Latitude        : num [1:53] 42.4 47.6 39.2 46.9 48 ...
 $ Longitude       : num [1:53] -123 -122 -121 -123 -122 ...
 $ distances_Mi    : num [1:53] 2499 2406 2472 2430 2408 ...

4. I change clus1 to DF
'data.frame':	53 obs. of  7 variables:
 $ MBR_SUBSCRIBERID: Factor w/ 2352 levels "101040199600",..: 86 2111 995 899 953 924 769 92 790 1748 ...
 $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 31 39 4 39 39 39 39 31 39 39 ...
 $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 727 85 455 1018 1203 604 1169 727 984 295 ...
 $ clusters        : num  1 1 1 1 1 1 1 1 1 1 ...
 $ Latitude        : num  42.4 47.6 39.2 46.9 48 ...
 $ Longitude       : num  -123 -122 -121 -123 -122 ...
 $ distances_Mi    : num  2499 2406 2472 2430 2408 ...

Then try again, no luck?

Weird?

Thanks

WHP

Proprietary

-----Original Message-----
From: Jim Lemon <drjimlemon at gmail.com>
Sent: Thursday, May 14, 2020 5:44 AM
To: Poling, William <PolingW at aetna.com>
Cc: r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with map()

**** External Email - Use Caution ****

Hi Bill,
Have you compared str(radius3) to str(clus1)? It may be quite different.

Jim

On Thu, May 14, 2020 at 8:24 PM Poling, William via R-help <r-help at r-project.org> wrote:
>
> #RStudio Version Version 1.2.1335
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit) #Running under: Windows 10
> x64 (build 17763)
>
> Good morning.
>
> I ran routines like this one yesterday with no errors.
>
> #Radius3----
> str(radius3)
>
> radius3 <- individual_dets_sf_3X %>%
>
>   dplyr::select(MBR_SUBSCRIBERID,state,city,Latitude,Longitude,distances_Mi) %>%
>   filter(distances_Mi <= 1200)
>
> str(radius3)
>
>   geomat<-makeDensityMatrix(radius3[,c("Latitude","Longitude")],
>                             
> xlim=range(radius3$Longitude),ylim=range(radius3$Latitude))
>
> latlim<-range(radius3$Latitude)
> lonlim<-range(radius3$Longitude)
>
> <- map("world",xlim=lonlim,ylim=latlim)
> axis(1)
> axis(2)
> densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
>             red=c(0.5,1),green=0,blue=0,pch=15)
> title("Member Geo Density Plot For Radius3")
>
>
> This morning I am trying slightly new routine and receiving this error
>
> clus1 <- individual_dets_sf_3X %>%
>
>   dplyr::select(MBR_SUBSCRIBERID,state,city,clusters,Latitude,Longitude,distances_Mi) %>%
>   filter(clusters == 1)
>
> geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
>                           
> xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))
>
> latlim<-range(clus1$Latitude)
> lonlim<-range(clus1$Longitude)
>
> map("world",xlim=lonlim,ylim=latlim)
> axis(1)
> axis(2)
> densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
>             red=c(0.5,1),green=0,blue=0,pch=15)
> title("Member Geo Density Plot For Cluster1")
>
>
> geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
> +                           
> + xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))
>
> > latlim<-range(clus1$Latitude)
> > lonlim<-range(clus1$Longitude)
> >
> > map("world",xlim=lonlim,ylim=latlim)
> Error in .C(C_map_type, as.character(mapbase), integer(1)) :
>   Incorrect number of arguments (2), expecting 0 for ''
> > axis(1)
> Error in axis(1) : plot.new has not been called yet
> > axis(2)
> Error in axis(2) : plot.new has not been called yet
> > densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
> +             red=c(0.5,1),green=0,blue=0,pch=15)
> Error in plot.xy(xy.coords(x, y), type = type, ...) :
>   plot.new has not been called yet
> > title("Member Geo Density Plot For Cluster1")
> Error in title("Member Geo Density Plot For Cluster1") :
>   plot.new has not been called yet
>
>
> It seems to error at the point of map("world",xlim=lonlim,ylim=latlim)
>
> I find a ref in stack overflow
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stackoverflow.com
> _questions_45066628_cannot-2Drun-2Dmap-2Ddatastate&d=DwIBaQ&c=wluqKIiw
> ffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2Ex
> lYvnUWbsByY&m=sRhKIDOv4AuwrfDBU9b2Rpj-VPfs0lcpkQqNBfVLSe4&s=sEjkBzlaFR
> 7NhUDpUPGPp1nOkAxOlBbFNadLowcDbFw&e=
> and tried solutions but they do not seem to work?
>
> Here is a sample (MBR_SUBSCRIBERID(factor) is replaced with ID(integer) but of no consequence in my routine since not really used.
>
> I hope someone recognizes the problem.
>
> Thank you for any advice
>
> WHP
>
> > dput(sample)
> structure(list(state = structure(c(2L, 22L, 10L, 12L, 6L, 22L, 11L, 
> 22L, 22L, 32L, 19L, 29L, 9L, 9L, 10L, 30L, 33L, 35L, 41L, 12L, 12L, 
> 36L, 4L, 9L, 8L, 39L, 36L, 36L, 41L, 28L, 12L, 28L, 28L, 12L, 11L, 2L, 
> 12L, 22L, 32L, 22L, 12L, 10L, 10L, 22L, 10L ), .Label = c("AL", "AR", 
> "AZ", "CA", "CO", "CT", "DC", "DE", "FL", "GA", "IA", "IL", "IN", 
> "KS", "KY", "LA", "MA", "MD", "ME", "MI", "MN", "MO", "NC", "NE", 
> "NJ", "NM", "NV", "NY", "OH", "OK", "OR", "PA", "SC", "SD", "TN", 
> "TX", "UT", "VA", "WA", "WI", "WV"
> ), class = "factor"), city = structure(c(838L, 1030L, 262L, 218L, 
> 166L, 973L, 339L, 451L, 660L, 358L, 281L, 1280L, 129L, 223L, 989L, 
> 721L, 550L, 731L, 1325L, 688L, 1184L, 281L, 759L, 1171L, 1305L, 587L, 
> 272L, 581L, 263L, 152L, 217L, 152L, 390L, 5L, 571L, 1006L, 1162L, 
> 939L, 170L, 1033L, 1002L, 586L, 586L, 192L, 586L ), .Label = 
> c("ABBOTTSTOWN", "ABILENE", "ACWORTH", "ADAMS", "ADDISON", "ADKINS", 
> "ALBANY", "ALBIA", "ALBUQUERQUE", "ALEXANDRIA", "ALFRED", "ALIQUIPPA", 
> "ALISO VIEJO", "ALLEN PARK", "ALLENTOWN", "ALPHA", "ALPHARETTA", 
> "ALPINE", "ALTOONA", "AMARILLO", "AMBLER", "AMBRIDGE", "AMHERST", 
> "AMITYVILLE", "ANAHIEM", "ANDERSON", "ANDOVER", "ANGIER", "ANGLETON", 
> "ANKENY", "ANN ARBOR", "ANZA", "APEX", "APOLLO", "ARCADIA", 
> "ARCHDALE", "ARDMORE", "ARGYLE", "ARKANSAS CITY", "ARLINGTON", 
> "ARNOLD", "ARTHUR", "ASHEBORO", "ASHEVILLE", "ASHLAND", "ASHLAND 
> CITY", "ASHTON", "ASTON", "ATHENS", "ATLANTA", "ATLANTIC BCH", 
> "AUBORN", "AUGUSTA", "AURORA", "AUSTELL", "AUSTIN", "AVENTURA", 
> "AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN CITY", "BALDWIN PLACE", 
> "BALLWIN", "BANGOR", "BARBOURSVILLE", "BARNEGAT", "BARTLETT", 
> "BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", "BATESBURG", "BATON 
> ROUGE", "BAXTER SPRINGS", "BAY CITY", "BAYONNE", "BAYSIDE", "BAYTOWN", 
> "BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER CITY", "BELFAIR", "BELLE 
> VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR", "BELLMORE", "BELLWOOD", 
> "BELMAR", "BELMONT", "BELVIDERE", "BENNET", "BENTON", "BENTONVILLE", 
> "BERLIN", "BESSEMER CITY", "BETHANY", "BETHEL", "BETHEL PARK", 
> "BETHESDA", "BETHLEHEM", "BETTENDORF", "BIGLERVILLE", "BINGHAMTON", 
> "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN", "BLACKSBURG", "BLAINE", 
> "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD", "BLUE BELL", "BLUE GRASS", 
> "BLUE POINT", "BLUE SPRINGS", "BOCA RATON", "BODFISH", "BOILING 
> SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS", "BONNEY LAKE", 
> "BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING GREEN", 
> "BOYERTOWN", "BOYNTON BEACH", "BRADENTON", "BRADENVILLE", 
> "BRANDAMORE", "BRANDON", "BRANFORD", "BRANSON", "BRASELTON", 
> "BREEZEWOOD", "BRENTON", "BRENTWOOD", "BREWSTER", "BRICK", 
> "BRIDGEPORT", "BRIDGETON", "BRIGHAM CITY", "BROADDUS", "BROCKPORT", 
> "BROGUE", "BROKEN ARROW", "BRONX", "BROOKFIELD", "BROOKHAVEN", 
> "BROOKLYN", "BROWNS MILLS", "BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", 
> "BRYAN", "BUCKSPORT", "BUDD LAKE", "BUFFALO", "BULVERDE", 
> "BUNKERVILLE", "BURBANK", "BURFORD", "BURIEN", "BURLINGTON", "BURR 
> RIDGE", "BURTON", "BUSKIRK", "BUTLER", "BUXTON", "BYRON", "CALERA", 
> "CAMBRIDGE", "CAMP HILL", "CAMPBELL", "CANAAN", "CANAL WINCHESTER", 
> "CANASTOTA", "CANTON", "CARDIFF BY THE SEA", "CARL JUNCTION", 
> "CARLISLE", "CARLTON", "CAROL STREAM", "CARROLLTON", "CARSON CITY", 
> "CARTERSVILLE", "CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", 
> "CASWELL", "CATO", "CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER 
> POINT", "CENTERTON", "CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN 
> FALLS", "CHALFONT", "CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", 
> "CHARLESTON", "CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", 
> "CHESHIRE", "CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", 
> "CHILLICOTHE", "CHOCTAW", "CICERO", "CINCINNATI", "CLAY", 
> "CLEARWATER", "CLEARWATER BEACH", "CLEMMONS", "CLENDENIN", 
> "CLEVELAND", "CLEVELAND HTS", "CLEVES", "CLIFFSIDE PARK", "CLIFTON", 
> "CLIFTON PARK", "CLIFTON SPGS", "CLINTON", "COACHELLA", "COAL 
> TOWNSHIP", "COATESVILLE", "COLLEGE STATION", "COLLINSVILLE", 
> "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE", "CONCORD", "CONCORD 
> TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY", "CONYERS", 
> "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA", "CORNELIUS", 
> "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY", "COTTAGE HILLS", 
> "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN", "CRANBERRY 
> TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER", "CUMBERLAND 
> FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS", "DALLAS", 
> "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT", 
> "DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR", 
> "DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH", 
> "DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY", "DES 
> MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER", "DIXON", 
> "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER", "DOYLESTOWN", 
> "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH", "DUNCAN", "DUNCANNON", 
> "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE", "DURHAM", "DUTTON", "EAGLE 
> GROVE", "EAST AURORA", "EAST CHICAGO", "EAST EARL", "EAST HANOVER", 
> "EAST HARTFORD", "EAST HARTLAND", "EAST LEROY", "EAST LIVERPOOL", 
> "EAST MEADOW", "EAST NORTHPORT", "EAST ORANGE", "EAST PEORIA", "EAST 
> SAINT LOUIS", "EASTON", "EDGERTON", "EDISON", "EDMOND", "EGG HBR TWP", 
> "EL DORADO", "EL PASO", "ELDRIDGE", "ELGIN", "ELIZABETHVILLE", 
> "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY", "ELMHURST", "ELMWOOD PARK", 
> "ELOY", "ELY", "EMERSON", "EMORY", "ENDICOTT", "ENGLEWOOD", "ENID", 
> "ENNICE", "ENOLA", "ENUMCLAW", "EPHRATA", "ERIE", "ESTERO", "EUREKA 
> SPRINGS", "EVANSVILLE", "EVERETT", "EXCELSIOR SPRINGS", "EXTON", 
> "FAIRFAX", "FAIRFIELD", "FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", 
> "FALMOUTH", "FAR HILLS", "FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", 
> "FAYETTE", "FAYETTEVILLE", "FELTON", "FENTON", "FERGUSON", "FIELDALE", 
> "FINDLAY", "FINGERVILLE", "FLEMINGTON", "FLINT", "FLORENCE", 
> "FLORESVILLE", "FLORISSANT", "FLOVILLA", "FLOWER MOUND", "FLUSHING", 
> "FOGELSVILLE", "FOLSOM", "FONTANELLE", "FOREST HILLS", "FOREST PARK", 
> "FORT DODGE", "FORT FAIRFIELD", "FORT GAY", "FORT LAUDERDALE", "FORT 
> LEE", "FORT MILL", "FORT MOHAVE", "FORT PAYNE", "FORT PIERCE", "FORT 
> SMITH", "FORT WAYNE", "FORT WORTH", "FOSTORIA", "FOUNTAIN INN", 
> "FRANKFORT", "FRANKLIN", "FREEDOM", "FREEHOLD", "FREEPORT", "FREMONT", 
> "FULLERTON", "FULSHEAR", "FULTON", "GADSDEN", "GAFFNEY", 
> "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE", "GARDNERS", "GARNETT 
> VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA", "GETTYSBURG", 
> "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE", "GLEN CARBON", "GLEN 
> COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW", "GLENWOOD", "GOLDEN 
> CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM", "GOSHEN", 
> "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS", "GRANDVIEW", 
> "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS", "GRASS VALLEY", 
> "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER", "GREENLAWN", 
> "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE", "GREER", 
> "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD", "HADLEY", 
> "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON", "HAMBURG", 
> "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE", 
> "HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON", 
> "HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL", 
> "HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE", 
> "HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH", 
> "HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE", "HIGH 
> HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS", 
> "HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE", 
> "HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD", 
> "HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS", "HOT 
> SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON", "HUFFMAN", 
> "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON", "HUNTINGTON 
> BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE", "HURST", 
> "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL", 
> "INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY", "IRMO", 
> "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP", "ISSAQUAH", 
> "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS", "JACKSONVILLE", 
> "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE", "JEFFERSON", "JEFFERSON 
> CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN", "JONESBORO", "JONESPORT", 
> "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE", "KANNAPOLIS", "KANSAS 
> CITY", "KATY", "KEARNEY", "KELLERTON", "KEMAH", "KENMORE", "KENNESAW", 
> "KENT", "KINGMAN", "KINGS MOUNTAIN", "KINGSLAND", "KINGSTON", 
> "KINGWOOD", "KIRKLAND", "KUTZTOWN", "LA QUINTA", "LA RUE", "LA VERNE", 
> "LAFAYETTE", "LAKE BLUFF", "LAKE IN THE HILL", "LAKE ISABELLA", "LAKE 
> PLACID", "LAKE SAINT LOUIS", "LAKE TAPPS", "LAKE VILLAGE", "LAKE 
> WAUKOMIS", "LAKE WORTH", "LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD 
> RCH", "LAMBERTVILLE", "LANCASTER", "LANDISVILLE", "LANSDALE", 
> "LAREDO", "LARGO", "LAS VEGAS", "LATROBE", "LAUDERHILL", "LAWNDALE", 
> "LAWRENCE", "LAWRENCEVILLE", "LE MARS", "LE ROY", "LEAGUE CITY", 
> "LEAVENWORTH", "LEAWOOD", "LEBANON", "LECOMPTON", "LEECHBURG", "LEES 
> SUMMIT", "LEESBURG", "LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", 
> "LEVITTOWN", "LEWES", "LEWISTOWN", "LEXINGTON", "LIBERTY TWP", 
> "LILLINGTON", "LINCOLN", "LINDENHURST", "LINTON", "LISBON", 
> "LITCHFIELD PK", "LITHIA", "LITITZ", "LITTLE EGG HARBOR TO", "LK 
> FOREST PK", "LK PEEKSKILL", "LOCKHART", "LOCKWOOD", "LOGAN", "LOMA 
> LINDA", "LONE JACK", "LONG BEACH", "LONG LANE", "LONGVIEW", "LORAIN", 
> "LORTON", "LOS ANGELES", "LOUISBURG", "LOVES PARK", "LOWER BURRELL", 
> "LOWRY CITY", "LUBBOCK", "LUGOFF", "LUMBERTON", "LYNBROOK", 
> "LYNNWOOD", "MACHESNEY PARK", "MACON", "MAGNOLIA", "MAHOPAC", 
> "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE", "MANASSAS", "MANCHACA", 
> "MANCHESTER", "MANCHESTER TOWNSHIP", "MANCHESTER TW", "MANHEIM", 
> "MANITO", "MANLIUS", "MANNINGTON", "MANSFIELD", "MANSURA", "MAPLE 
> PARK", "MARBLE FALLS", "MARCELLUS", "MARICOPA", "MARIETTA", "MARION", 
> "MARKLEYSBURG", "MARS HILL", "MARSHALLTOWN", "MARSHFIELD", 
> "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE", "MASON CITY", "MASSEY", 
> "MASSILLON", "MASURY", "MATAWAN", "MATEWAN", "MATTAPOISETT", 
> "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND", "MC DONALD", "MC 
> KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN", "MCALESTER", "MCDONOUGH", 
> "MCKEESPORT", "MCKINNEY", "MECHANICSBURG", "MECHANICSVILLE", 
> "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS", "MERIDEN", 
> "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA", "MESQUITE", 
> "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN", "MIDLAND", 
> "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK", "MILLEDGEVILLE", 
> "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE", "MINEOLA", "MINERAL 
> SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION", "MISSION HILLS", 
> "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY", "MOLINE", 
> "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE", "MONTCLAIR", 
> "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON", "MORGANTOWN", 
> "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE", "MOUNT JOY", 
> "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO", "MOUNT UNION", 
> "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT", "MULVANE", 
> "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT", 
> "MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS", 
> "NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES", 
> "NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG", "NEW 
> BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK", "NEW 
> CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON", "NEW 
> LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE", "NEW 
> ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK", 
> "NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT", 
> "NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA", 
> "NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK", 
> "NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON", 
> "NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND 
> HILLS", "NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE", 
> "NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON", "OAK 
> RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE", "OMAHA", 
> "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION", "ORONO", 
> "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO", "OTTAWA", 
> "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD", "OYSTER BAY", 
> "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR", "PALMDALE", "PALMYRA", 
> "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE", "PARLIN", "PARMA", 
> "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY", "PAXTON", "PEA 
> RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN", "PELION", 
> "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE", "PENNSBURG", 
> "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS", "PERRYSBURG", 
> "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI", "PHOENIX", 
> "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN", "PINE BLUFF", 
> "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY", "PITMAN", 
> "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY", 
> "PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO 
> LAKE", "POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", 
> "PORT CHARLOTTE", "PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", 
> "PORT WASHINGTON", "PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", 
> "POTTSVILLE", "POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE 
> VILLAGE", "PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT 
> PLEASANT", "PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", 
> "QUINCY", "RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", 
> "READLYN", "RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO 
> PARK", "REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", 
> "RICH HILL", "RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", 
> "RINCON", "RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", 
> "RIVERSIDE", "ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", 
> "ROCKAWAY BCH", "ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", 
> "ROGERS", "ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE", 
> "ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT", 
> "ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN", 
> "RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO", 
> "SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR", "SAINT 
> CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS", "SAINT 
> PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY", 
> "SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO", 
> "SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN", 
> "SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", 
> "SARASOTA", "SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", 
> "SAYVILLE", "SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", 
> "SCOTT", "SCOTTSDALE", "SEAFORD", "SEALE", "SEATTLE", "SEDONA", 
> "SEGUIN", "SELDEN", "SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", 
> "SHARON", "SHARPSBURG", "SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", 
> "SHELLSBURG", "SHELTON", "SHENANDOAH", "SHORELINE", "SHOREWOOD", 
> "SHREVEPORT", "SICKLERVILLE", "SILER CITY", "SILOAM SPRINGS", "SIMON", 
> "SIMPSONVILLE", "SIMSBURY", "SIOUX CITY", "SIOUX FALLS", "SKOKIE", 
> "SKOWHEGAN", "SLATINGTON", "SLIDELL", "SMITHFIELD", "SN BERNRDNO", 
> "SNELLVILLE", "SOCORRO", "SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH 
> BELOIT", "SOUTH BEND", "SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH 
> PARK", "SOUTH PLAINFIELD", "SOUTH SIOUX CITY", "SOUTHAMPTON", 
> "SOUTHBURY", "SOUTHGATE", "SPARKS", "SPARROW BUSH", "SPENCER", 
> "SPRING", "SPRING BRANCH", "SPRING GROVE", "SPRING HILL", "SPRING 
> HOPE", "SPRING VALLEY", "SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD 
> GARDENS", "SPRINGHILL", "SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", 
> "ST PETERSBURG", "ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", 
> "STANLEY", "STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", 
> "STILLMAN VALLEY", "STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", 
> "STRAFFORD", "STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", 
> "SUFFIELD", "SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", 
> "SUMMIT", "SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", 
> "SWEENY", "SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", 
> "TAFTVILLE", "TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", 
> "TAYLORVILLE", "TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", 
> "THOMASVILLE", "TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY 
> PARK", "TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", 
> "TOPEKA", "TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS 
> RST", "TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER", 
> "TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY", 
> "UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA", 
> "URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO", "VAN 
> BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO", 
> "VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA", 
> "VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH", "W 
> TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN", 
> "WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG", 
> "WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA", 
> "WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE", 
> "WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER", 
> "WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO", "WEST 
> CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN", "WEST 
> HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH", "WEST 
> POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD", "WEST VALLEY 
> CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER", "WESTMORELAND 
> CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD", "WHARTON", "WHEELING", 
> "WHITE PLAINS", "WHITEHALL", "WHITESTONE", "WHITESTOWN", "WHITING", 
> "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD", "WILLARD", 
> "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK", 
> "WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS", 
> "WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON", 
> "WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE", 
> "WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS", 
> "WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH", 
> "YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS", "YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor"),
>     clusters = c(6L, 10L, 3L, 10L, 5L, 6L, 10L, 6L, 6L, 7L, 5L,
>     7L, 8L, 8L, 3L, 6L, 3L, 10L, 7L, 10L, 10L, 4L, 2L, 8L, 9L,
>     1L, 4L, 4L, 7L, 5L, 10L, 5L, 5L, 10L, 6L, 6L, 10L, 6L, 7L,
>     10L, 10L, 3L, 3L, 6L, 3L), Longitude = c(-93.883554, -90.582058,
>     -83.868923, -89.544083, -72.902467, -94.458904, -90.588533,
>     -94.527843, -92.93769, -80.029456, -70.58809, -82.888789,
>     -80.113071, -82.6661, -81.372992, -95.840964, -82.016013,
>     -89.92625, -80.276676, -89.778385, -87.641063, -94.888036,
>     -117.663119, -82.530568, -75.574437, -122.209047, -96.799309,
>     -95.746255, -80.553953, -73.923314, -87.713482, -73.9731,
>     -73.8187, -87.987023, -93.759478, -94.110903, -90.00597,
>     -93.257109, -79.932483, -90.201858, -89.100233, -84.625923,
>     -84.567614, -93.912921, -84.612564), Latitude = c(34.503045,
>     38.752515, 33.406527, 40.893588, 41.733088, 38.999032, 41.657674,
>     38.890929, 37.639766, 42.099693, 43.53462, 40.142492, 26.538149,
>     27.9062, 31.90104, 34.833083, 35.118836, 35.136047, 39.425712,
>     40.519207, 40.101126, 30.044457, 33.62287, 27.910276, 39.752254,
>     47.403778, 32.961929, 29.820199, 38.408649, 40.668828, 41.983948,
>     40.58116, 40.7253, 41.921667, 41.669921, 36.442552, 38.564663,
>     37.455315, 40.867774, 38.783554, 42.244381, 34.024332, 34.001644,
>     36.622704, 34.017832), distances_Me = c(1879089.079, 1437296.523,
>     1185638.742, 1313997.97, 156759.9125, 1759443.351, 1398398.997,
>     1767861.917, 1668917.322, 534234.2576, 431749.2965, 758508.4701,
>     1657031.938, 1615575.668, 1169085.702, 2024642.021, 930944.4681,
>     1525322.746, 553069.7532, 1337002.822, 1161367.37, 2223496.376,
>     3918520.212, 1609088.602, 164252.5146, 3878581.001, 2197910.688,
>     2304526.789, 614908.3151, 11075.24597, 1160375.454, 494.2997496,
>     21063.72622, 1182681.434, 1662351.873, 1810881.911, 1393581.436,
>     1701982.013, 504395.772, 1404348.991, 1276505.454, 1190457.718,
>     1188029.899, 1787261.336, 1189994.055), distances_Mi = c(1167.607468,
>     893.0913246, 736.719012, 816.477441, 97.40573055, 1093.263337,
>     868.9216123, 1098.494372, 1037.01328, 331.9565399, 268.2755749,
>     471.3135552, 1029.628072, 1003.868436, 726.4334682, 1258.049536,
>     578.4599174, 947.7880794, 343.6603307, 830.7719404, 721.6375366,
>     1381.611443, 2434.846498, 999.8375755, 102.0614003, 2410.029515,
>     1365.713293, 1431.96122, 382.0848884, 6.881813138, 721.0211909,
>     0.30714248, 13.08834388, 734.8814328, 1032.933715, 1125.225657,
>     865.9281298, 1057.55865, 313.4158337, 872.6187534, 793.1807589,
>     739.7133739, 738.2048023, 1110.548567, 739.4252678), ID = 
> 2308:2352), row.names = c(NA, -45L), class = "data.frame")
>
> Proprietary
>
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may 
> con...{{dropped:16}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIBaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmv
> OXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sRhKIDOv4AuwrfD
> BU9b2Rpj-VPfs0lcpkQqNBfVLSe4&s=CloI44kowc5J_mOah6vx_Kl4PFIeqxmwexvRiqw
> xuOA&e= PLEASE do read the posting guide 
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIBaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmm
> vOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sRhKIDOv4Auwrf
> DBU9b2Rpj-VPfs0lcpkQqNBfVLSe4&s=iOWreiJDomXv-uJ3NzQNX0IUsupILo_4DmkFK3
> GRUig&e= and provide commented, minimal, self-contained, reproducible 
> code.

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From Po||ngW @end|ng |rom @etn@@com  Thu May 14 14:02:57 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Thu, 14 May 2020 12:02:57 +0000
Subject: [R] Help with map()
In-Reply-To: <BYAPR06MB53836AD813508C208C7D5B61AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB5383688B7688345330422859AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CA+8X3fWwQ2HrBUxUW1Od5AeBegfmurkpz1Gu-wFhfAZ4dX9cmQ@mail.gmail.com>
 <BYAPR06MB538311FCA8A217B27743C85BAEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <BYAPR06MB53836AD813508C208C7D5B61AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <BYAPR06MB538355DBADC052E34DA1D3D8AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>

Hi, it is working now using just the most necessary pkgs.

 Evidently when there are so few rows (clus1=53) the map does not cooperate, I get the plot but no map image?

As I increase records the map begins to appear as it did for Radius1-8 routines

Thank you for your time and trouble my friends.

WHP





Proprietary

-----Original Message-----
From: Poling, William 
Sent: Thursday, May 14, 2020 6:46 AM
To: Jim Lemon <drjimlemon at gmail.com>
Cc: r-help at r-project.org; Mark Fowler <gmark.fowler at outlook.com>
Subject: RE: [EXTERNAL] Re: [R] Help with map()

Hi, I ran this which is partially successful. I got long & lat  (x&y) with red plot values however, no map behind it?

Not sure what I might be missing now in terms of pkgs I suppose?
#Just use the basic pkgs
library(magrittr)#for %>% function
library(plotrix)
library(maps)
library(dplyr)#For filter()
str(individual_dets_sf_3X)
str(radius3)
str(clus1)


#rm(clus1)

clus1 <- individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,state,city,clusters,Latitude,Longitude,distances_Mi) %>% #Notice now that these are capitalized <--"L"
  
  filter(clusters == 1)

str(clus1)

clus1 <- as.data.frame(clus1)

geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
                          xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))
# Range of density (>0) - 1.292155 3.89646 
latlim<-range(clus1$Latitude)
lonlim<-range(clus1$Longitude)

map("world",xlim=lonlim,ylim=latlim)
axis(1)
axis(2)
densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
            red=c(0.5,1),green=0,blue=0,pch=15)
title("Member Geo Density Plot For Cluster1")





Proprietary

-----Original Message-----
From: Poling, William 
Sent: Thursday, May 14, 2020 6:29 AM
To: Jim Lemon <drjimlemon at gmail.com>
Cc: r-help at r-project.org; Mark Fowler <gmark.fowler at outlook.com>
Subject: RE: [EXTERNAL] Re: [R] Help with map()


Hi Jim, and Mark, thank you for your response.

1. I have restarted R
2. I have only initiated library(magrittr)#for %>% function & library(plotrix), no other libraries thinking that another may be overwriting something.
3. I have checked the str()

str(radius3)
'data.frame':	1990 obs. of  6 variables:
 $ MBR_SUBSCRIBERID: Factor w/ 2352 levels "101040199600",..: 590 908 976 509 1674 690 1336 726 1702 2331 ...
 $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 32 10 25 11 9 32 13 12 12 17 ...
 $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 932 156 230 698 965 1330 515 1127 1304 1316 ...
 $ Latitude        : num  40.4 31.2 40.8 42.1 26.8 ...
 $ Longitude       : num  -79.9 -81.5 -74 -91.6 -82.1 ...
 $ distances_Mi    : num  310.3 769.9 16.1 920.4 1057.6 ...

str(clus1)
tibble [53 x 7] (S3: tbl_df/tbl/data.frame)  $ MBR_SUBSCRIBERID: Factor w/ 2352 levels "101040199600",..: 86 2111 995 899 953 924 769 92 790 1748 ...
 $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 31 39 4 39 39 39 39 31 39 39 ...
 $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 727 85 455 1018 1203 604 1169 727 984 295 ...
 $ clusters        : num [1:53] 1 1 1 1 1 1 1 1 1 1 ...
 $ Latitude        : num [1:53] 42.4 47.6 39.2 46.9 48 ...
 $ Longitude       : num [1:53] -123 -122 -121 -123 -122 ...
 $ distances_Mi    : num [1:53] 2499 2406 2472 2430 2408 ...

4. I change clus1 to DF
'data.frame':	53 obs. of  7 variables:
 $ MBR_SUBSCRIBERID: Factor w/ 2352 levels "101040199600",..: 86 2111 995 899 953 924 769 92 790 1748 ...
 $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 31 39 4 39 39 39 39 31 39 39 ...
 $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 727 85 455 1018 1203 604 1169 727 984 295 ...
 $ clusters        : num  1 1 1 1 1 1 1 1 1 1 ...
 $ Latitude        : num  42.4 47.6 39.2 46.9 48 ...
 $ Longitude       : num  -123 -122 -121 -123 -122 ...
 $ distances_Mi    : num  2499 2406 2472 2430 2408 ...

Then try again, no luck?

Weird?

Thanks

WHP

Proprietary

-----Original Message-----
From: Jim Lemon <drjimlemon at gmail.com>
Sent: Thursday, May 14, 2020 5:44 AM
To: Poling, William <PolingW at aetna.com>
Cc: r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with map()

**** External Email - Use Caution ****

Hi Bill,
Have you compared str(radius3) to str(clus1)? It may be quite different.

Jim

On Thu, May 14, 2020 at 8:24 PM Poling, William via R-help <r-help at r-project.org> wrote:
>
> #RStudio Version Version 1.2.1335
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit) #Running under: Windows 10
> x64 (build 17763)
>
> Good morning.
>
> I ran routines like this one yesterday with no errors.
>
> #Radius3----
> str(radius3)
>
> radius3 <- individual_dets_sf_3X %>%
>
>   dplyr::select(MBR_SUBSCRIBERID,state,city,Latitude,Longitude,distances_Mi) %>%
>   filter(distances_Mi <= 1200)
>
> str(radius3)
>
>   geomat<-makeDensityMatrix(radius3[,c("Latitude","Longitude")],
>                             
> xlim=range(radius3$Longitude),ylim=range(radius3$Latitude))
>
> latlim<-range(radius3$Latitude)
> lonlim<-range(radius3$Longitude)
>
> <- map("world",xlim=lonlim,ylim=latlim)
> axis(1)
> axis(2)
> densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
>             red=c(0.5,1),green=0,blue=0,pch=15)
> title("Member Geo Density Plot For Radius3")
>
>
> This morning I am trying slightly new routine and receiving this error
>
> clus1 <- individual_dets_sf_3X %>%
>
>   dplyr::select(MBR_SUBSCRIBERID,state,city,clusters,Latitude,Longitude,distances_Mi) %>%
>   filter(clusters == 1)
>
> geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
>                           
> xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))
>
> latlim<-range(clus1$Latitude)
> lonlim<-range(clus1$Longitude)
>
> map("world",xlim=lonlim,ylim=latlim)
> axis(1)
> axis(2)
> densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
>             red=c(0.5,1),green=0,blue=0,pch=15)
> title("Member Geo Density Plot For Cluster1")
>
>
> geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
> +                           
> + xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))
>
> > latlim<-range(clus1$Latitude)
> > lonlim<-range(clus1$Longitude)
> >
> > map("world",xlim=lonlim,ylim=latlim)
> Error in .C(C_map_type, as.character(mapbase), integer(1)) :
>   Incorrect number of arguments (2), expecting 0 for ''
> > axis(1)
> Error in axis(1) : plot.new has not been called yet
> > axis(2)
> Error in axis(2) : plot.new has not been called yet
> > densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
> +             red=c(0.5,1),green=0,blue=0,pch=15)
> Error in plot.xy(xy.coords(x, y), type = type, ...) :
>   plot.new has not been called yet
> > title("Member Geo Density Plot For Cluster1")
> Error in title("Member Geo Density Plot For Cluster1") :
>   plot.new has not been called yet
>
>
> It seems to error at the point of map("world",xlim=lonlim,ylim=latlim)
>
> I find a ref in stack overflow
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stackoverflow.com
> _questions_45066628_cannot-2Drun-2Dmap-2Ddatastate&d=DwIBaQ&c=wluqKIiw
> ffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2Ex
> lYvnUWbsByY&m=sRhKIDOv4AuwrfDBU9b2Rpj-VPfs0lcpkQqNBfVLSe4&s=sEjkBzlaFR
> 7NhUDpUPGPp1nOkAxOlBbFNadLowcDbFw&e=
> and tried solutions but they do not seem to work?
>
> Here is a sample (MBR_SUBSCRIBERID(factor) is replaced with ID(integer) but of no consequence in my routine since not really used.
>
> I hope someone recognizes the problem.
>
> Thank you for any advice
>
> WHP
>
> > dput(sample)
> structure(list(state = structure(c(2L, 22L, 10L, 12L, 6L, 22L, 11L, 
> 22L, 22L, 32L, 19L, 29L, 9L, 9L, 10L, 30L, 33L, 35L, 41L, 12L, 12L, 
> 36L, 4L, 9L, 8L, 39L, 36L, 36L, 41L, 28L, 12L, 28L, 28L, 12L, 11L, 2L, 
> 12L, 22L, 32L, 22L, 12L, 10L, 10L, 22L, 10L ), .Label = c("AL", "AR", 
> "AZ", "CA", "CO", "CT", "DC", "DE", "FL", "GA", "IA", "IL", "IN", 
> "KS", "KY", "LA", "MA", "MD", "ME", "MI", "MN", "MO", "NC", "NE", 
> "NJ", "NM", "NV", "NY", "OH", "OK", "OR", "PA", "SC", "SD", "TN", 
> "TX", "UT", "VA", "WA", "WI", "WV"
> ), class = "factor"), city = structure(c(838L, 1030L, 262L, 218L, 
> 166L, 973L, 339L, 451L, 660L, 358L, 281L, 1280L, 129L, 223L, 989L, 
> 721L, 550L, 731L, 1325L, 688L, 1184L, 281L, 759L, 1171L, 1305L, 587L, 
> 272L, 581L, 263L, 152L, 217L, 152L, 390L, 5L, 571L, 1006L, 1162L, 
> 939L, 170L, 1033L, 1002L, 586L, 586L, 192L, 586L ), .Label = 
> c("ABBOTTSTOWN", "ABILENE", "ACWORTH", "ADAMS", "ADDISON", "ADKINS", 
> "ALBANY", "ALBIA", "ALBUQUERQUE", "ALEXANDRIA", "ALFRED", "ALIQUIPPA", 
> "ALISO VIEJO", "ALLEN PARK", "ALLENTOWN", "ALPHA", "ALPHARETTA", 
> "ALPINE", "ALTOONA", "AMARILLO", "AMBLER", "AMBRIDGE", "AMHERST", 
> "AMITYVILLE", "ANAHIEM", "ANDERSON", "ANDOVER", "ANGIER", "ANGLETON", 
> "ANKENY", "ANN ARBOR", "ANZA", "APEX", "APOLLO", "ARCADIA", 
> "ARCHDALE", "ARDMORE", "ARGYLE", "ARKANSAS CITY", "ARLINGTON", 
> "ARNOLD", "ARTHUR", "ASHEBORO", "ASHEVILLE", "ASHLAND", "ASHLAND 
> CITY", "ASHTON", "ASTON", "ATHENS", "ATLANTA", "ATLANTIC BCH", 
> "AUBORN", "AUGUSTA", "AURORA", "AUSTELL", "AUSTIN", "AVENTURA", 
> "AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN CITY", "BALDWIN PLACE", 
> "BALLWIN", "BANGOR", "BARBOURSVILLE", "BARNEGAT", "BARTLETT", 
> "BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", "BATESBURG", "BATON 
> ROUGE", "BAXTER SPRINGS", "BAY CITY", "BAYONNE", "BAYSIDE", "BAYTOWN", 
> "BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER CITY", "BELFAIR", "BELLE 
> VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR", "BELLMORE", "BELLWOOD", 
> "BELMAR", "BELMONT", "BELVIDERE", "BENNET", "BENTON", "BENTONVILLE", 
> "BERLIN", "BESSEMER CITY", "BETHANY", "BETHEL", "BETHEL PARK", 
> "BETHESDA", "BETHLEHEM", "BETTENDORF", "BIGLERVILLE", "BINGHAMTON", 
> "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN", "BLACKSBURG", "BLAINE", 
> "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD", "BLUE BELL", "BLUE GRASS", 
> "BLUE POINT", "BLUE SPRINGS", "BOCA RATON", "BODFISH", "BOILING 
> SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS", "BONNEY LAKE", 
> "BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING GREEN", 
> "BOYERTOWN", "BOYNTON BEACH", "BRADENTON", "BRADENVILLE", 
> "BRANDAMORE", "BRANDON", "BRANFORD", "BRANSON", "BRASELTON", 
> "BREEZEWOOD", "BRENTON", "BRENTWOOD", "BREWSTER", "BRICK", 
> "BRIDGEPORT", "BRIDGETON", "BRIGHAM CITY", "BROADDUS", "BROCKPORT", 
> "BROGUE", "BROKEN ARROW", "BRONX", "BROOKFIELD", "BROOKHAVEN", 
> "BROOKLYN", "BROWNS MILLS", "BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", 
> "BRYAN", "BUCKSPORT", "BUDD LAKE", "BUFFALO", "BULVERDE", 
> "BUNKERVILLE", "BURBANK", "BURFORD", "BURIEN", "BURLINGTON", "BURR 
> RIDGE", "BURTON", "BUSKIRK", "BUTLER", "BUXTON", "BYRON", "CALERA", 
> "CAMBRIDGE", "CAMP HILL", "CAMPBELL", "CANAAN", "CANAL WINCHESTER", 
> "CANASTOTA", "CANTON", "CARDIFF BY THE SEA", "CARL JUNCTION", 
> "CARLISLE", "CARLTON", "CAROL STREAM", "CARROLLTON", "CARSON CITY", 
> "CARTERSVILLE", "CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", 
> "CASWELL", "CATO", "CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER 
> POINT", "CENTERTON", "CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN 
> FALLS", "CHALFONT", "CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", 
> "CHARLESTON", "CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", 
> "CHESHIRE", "CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", 
> "CHILLICOTHE", "CHOCTAW", "CICERO", "CINCINNATI", "CLAY", 
> "CLEARWATER", "CLEARWATER BEACH", "CLEMMONS", "CLENDENIN", 
> "CLEVELAND", "CLEVELAND HTS", "CLEVES", "CLIFFSIDE PARK", "CLIFTON", 
> "CLIFTON PARK", "CLIFTON SPGS", "CLINTON", "COACHELLA", "COAL 
> TOWNSHIP", "COATESVILLE", "COLLEGE STATION", "COLLINSVILLE", 
> "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE", "CONCORD", "CONCORD 
> TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY", "CONYERS", 
> "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA", "CORNELIUS", 
> "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY", "COTTAGE HILLS", 
> "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN", "CRANBERRY 
> TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER", "CUMBERLAND 
> FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS", "DALLAS", 
> "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT", 
> "DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR", 
> "DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH", 
> "DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY", "DES 
> MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER", "DIXON", 
> "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER", "DOYLESTOWN", 
> "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH", "DUNCAN", "DUNCANNON", 
> "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE", "DURHAM", "DUTTON", "EAGLE 
> GROVE", "EAST AURORA", "EAST CHICAGO", "EAST EARL", "EAST HANOVER", 
> "EAST HARTFORD", "EAST HARTLAND", "EAST LEROY", "EAST LIVERPOOL", 
> "EAST MEADOW", "EAST NORTHPORT", "EAST ORANGE", "EAST PEORIA", "EAST 
> SAINT LOUIS", "EASTON", "EDGERTON", "EDISON", "EDMOND", "EGG HBR TWP", 
> "EL DORADO", "EL PASO", "ELDRIDGE", "ELGIN", "ELIZABETHVILLE", 
> "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY", "ELMHURST", "ELMWOOD PARK", 
> "ELOY", "ELY", "EMERSON", "EMORY", "ENDICOTT", "ENGLEWOOD", "ENID", 
> "ENNICE", "ENOLA", "ENUMCLAW", "EPHRATA", "ERIE", "ESTERO", "EUREKA 
> SPRINGS", "EVANSVILLE", "EVERETT", "EXCELSIOR SPRINGS", "EXTON", 
> "FAIRFAX", "FAIRFIELD", "FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", 
> "FALMOUTH", "FAR HILLS", "FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", 
> "FAYETTE", "FAYETTEVILLE", "FELTON", "FENTON", "FERGUSON", "FIELDALE", 
> "FINDLAY", "FINGERVILLE", "FLEMINGTON", "FLINT", "FLORENCE", 
> "FLORESVILLE", "FLORISSANT", "FLOVILLA", "FLOWER MOUND", "FLUSHING", 
> "FOGELSVILLE", "FOLSOM", "FONTANELLE", "FOREST HILLS", "FOREST PARK", 
> "FORT DODGE", "FORT FAIRFIELD", "FORT GAY", "FORT LAUDERDALE", "FORT 
> LEE", "FORT MILL", "FORT MOHAVE", "FORT PAYNE", "FORT PIERCE", "FORT 
> SMITH", "FORT WAYNE", "FORT WORTH", "FOSTORIA", "FOUNTAIN INN", 
> "FRANKFORT", "FRANKLIN", "FREEDOM", "FREEHOLD", "FREEPORT", "FREMONT", 
> "FULLERTON", "FULSHEAR", "FULTON", "GADSDEN", "GAFFNEY", 
> "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE", "GARDNERS", "GARNETT 
> VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA", "GETTYSBURG", 
> "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE", "GLEN CARBON", "GLEN 
> COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW", "GLENWOOD", "GOLDEN 
> CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM", "GOSHEN", 
> "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS", "GRANDVIEW", 
> "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS", "GRASS VALLEY", 
> "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER", "GREENLAWN", 
> "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE", "GREER", 
> "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD", "HADLEY", 
> "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON", "HAMBURG", 
> "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE", 
> "HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON", 
> "HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL", 
> "HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE", 
> "HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH", 
> "HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE", "HIGH 
> HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS", 
> "HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE", 
> "HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD", 
> "HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS", "HOT 
> SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON", "HUFFMAN", 
> "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON", "HUNTINGTON 
> BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE", "HURST", 
> "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL", 
> "INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY", "IRMO", 
> "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP", "ISSAQUAH", 
> "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS", "JACKSONVILLE", 
> "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE", "JEFFERSON", "JEFFERSON 
> CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN", "JONESBORO", "JONESPORT", 
> "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE", "KANNAPOLIS", "KANSAS 
> CITY", "KATY", "KEARNEY", "KELLERTON", "KEMAH", "KENMORE", "KENNESAW", 
> "KENT", "KINGMAN", "KINGS MOUNTAIN", "KINGSLAND", "KINGSTON", 
> "KINGWOOD", "KIRKLAND", "KUTZTOWN", "LA QUINTA", "LA RUE", "LA VERNE", 
> "LAFAYETTE", "LAKE BLUFF", "LAKE IN THE HILL", "LAKE ISABELLA", "LAKE 
> PLACID", "LAKE SAINT LOUIS", "LAKE TAPPS", "LAKE VILLAGE", "LAKE 
> WAUKOMIS", "LAKE WORTH", "LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD 
> RCH", "LAMBERTVILLE", "LANCASTER", "LANDISVILLE", "LANSDALE", 
> "LAREDO", "LARGO", "LAS VEGAS", "LATROBE", "LAUDERHILL", "LAWNDALE", 
> "LAWRENCE", "LAWRENCEVILLE", "LE MARS", "LE ROY", "LEAGUE CITY", 
> "LEAVENWORTH", "LEAWOOD", "LEBANON", "LECOMPTON", "LEECHBURG", "LEES 
> SUMMIT", "LEESBURG", "LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", 
> "LEVITTOWN", "LEWES", "LEWISTOWN", "LEXINGTON", "LIBERTY TWP", 
> "LILLINGTON", "LINCOLN", "LINDENHURST", "LINTON", "LISBON", 
> "LITCHFIELD PK", "LITHIA", "LITITZ", "LITTLE EGG HARBOR TO", "LK 
> FOREST PK", "LK PEEKSKILL", "LOCKHART", "LOCKWOOD", "LOGAN", "LOMA 
> LINDA", "LONE JACK", "LONG BEACH", "LONG LANE", "LONGVIEW", "LORAIN", 
> "LORTON", "LOS ANGELES", "LOUISBURG", "LOVES PARK", "LOWER BURRELL", 
> "LOWRY CITY", "LUBBOCK", "LUGOFF", "LUMBERTON", "LYNBROOK", 
> "LYNNWOOD", "MACHESNEY PARK", "MACON", "MAGNOLIA", "MAHOPAC", 
> "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE", "MANASSAS", "MANCHACA", 
> "MANCHESTER", "MANCHESTER TOWNSHIP", "MANCHESTER TW", "MANHEIM", 
> "MANITO", "MANLIUS", "MANNINGTON", "MANSFIELD", "MANSURA", "MAPLE 
> PARK", "MARBLE FALLS", "MARCELLUS", "MARICOPA", "MARIETTA", "MARION", 
> "MARKLEYSBURG", "MARS HILL", "MARSHALLTOWN", "MARSHFIELD", 
> "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE", "MASON CITY", "MASSEY", 
> "MASSILLON", "MASURY", "MATAWAN", "MATEWAN", "MATTAPOISETT", 
> "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND", "MC DONALD", "MC 
> KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN", "MCALESTER", "MCDONOUGH", 
> "MCKEESPORT", "MCKINNEY", "MECHANICSBURG", "MECHANICSVILLE", 
> "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS", "MERIDEN", 
> "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA", "MESQUITE", 
> "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN", "MIDLAND", 
> "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK", "MILLEDGEVILLE", 
> "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE", "MINEOLA", "MINERAL 
> SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION", "MISSION HILLS", 
> "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY", "MOLINE", 
> "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE", "MONTCLAIR", 
> "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON", "MORGANTOWN", 
> "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE", "MOUNT JOY", 
> "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO", "MOUNT UNION", 
> "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT", "MULVANE", 
> "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT", 
> "MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS", 
> "NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES", 
> "NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG", "NEW 
> BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK", "NEW 
> CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON", "NEW 
> LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE", "NEW 
> ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK", 
> "NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT", 
> "NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA", 
> "NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK", 
> "NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON", 
> "NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND 
> HILLS", "NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE", 
> "NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON", "OAK 
> RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE", "OMAHA", 
> "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION", "ORONO", 
> "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO", "OTTAWA", 
> "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD", "OYSTER BAY", 
> "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR", "PALMDALE", "PALMYRA", 
> "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE", "PARLIN", "PARMA", 
> "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY", "PAXTON", "PEA 
> RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN", "PELION", 
> "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE", "PENNSBURG", 
> "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS", "PERRYSBURG", 
> "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI", "PHOENIX", 
> "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN", "PINE BLUFF", 
> "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY", "PITMAN", 
> "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY", 
> "PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO 
> LAKE", "POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", 
> "PORT CHARLOTTE", "PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", 
> "PORT WASHINGTON", "PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", 
> "POTTSVILLE", "POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE 
> VILLAGE", "PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT 
> PLEASANT", "PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", 
> "QUINCY", "RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", 
> "READLYN", "RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO 
> PARK", "REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", 
> "RICH HILL", "RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", 
> "RINCON", "RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", 
> "RIVERSIDE", "ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", 
> "ROCKAWAY BCH", "ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", 
> "ROGERS", "ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE", 
> "ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT", 
> "ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN", 
> "RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO", 
> "SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR", "SAINT 
> CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS", "SAINT 
> PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY", 
> "SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO", 
> "SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN", 
> "SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", 
> "SARASOTA", "SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", 
> "SAYVILLE", "SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", 
> "SCOTT", "SCOTTSDALE", "SEAFORD", "SEALE", "SEATTLE", "SEDONA", 
> "SEGUIN", "SELDEN", "SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", 
> "SHARON", "SHARPSBURG", "SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", 
> "SHELLSBURG", "SHELTON", "SHENANDOAH", "SHORELINE", "SHOREWOOD", 
> "SHREVEPORT", "SICKLERVILLE", "SILER CITY", "SILOAM SPRINGS", "SIMON", 
> "SIMPSONVILLE", "SIMSBURY", "SIOUX CITY", "SIOUX FALLS", "SKOKIE", 
> "SKOWHEGAN", "SLATINGTON", "SLIDELL", "SMITHFIELD", "SN BERNRDNO", 
> "SNELLVILLE", "SOCORRO", "SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH 
> BELOIT", "SOUTH BEND", "SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH 
> PARK", "SOUTH PLAINFIELD", "SOUTH SIOUX CITY", "SOUTHAMPTON", 
> "SOUTHBURY", "SOUTHGATE", "SPARKS", "SPARROW BUSH", "SPENCER", 
> "SPRING", "SPRING BRANCH", "SPRING GROVE", "SPRING HILL", "SPRING 
> HOPE", "SPRING VALLEY", "SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD 
> GARDENS", "SPRINGHILL", "SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", 
> "ST PETERSBURG", "ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", 
> "STANLEY", "STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", 
> "STILLMAN VALLEY", "STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", 
> "STRAFFORD", "STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", 
> "SUFFIELD", "SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", 
> "SUMMIT", "SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", 
> "SWEENY", "SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", 
> "TAFTVILLE", "TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", 
> "TAYLORVILLE", "TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", 
> "THOMASVILLE", "TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY 
> PARK", "TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", 
> "TOPEKA", "TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS 
> RST", "TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER", 
> "TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY", 
> "UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA", 
> "URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO", "VAN 
> BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO", 
> "VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA", 
> "VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH", "W 
> TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN", 
> "WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG", 
> "WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA", 
> "WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE", 
> "WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER", 
> "WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO", "WEST 
> CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN", "WEST 
> HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH", "WEST 
> POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD", "WEST VALLEY 
> CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER", "WESTMORELAND 
> CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD", "WHARTON", "WHEELING", 
> "WHITE PLAINS", "WHITEHALL", "WHITESTONE", "WHITESTOWN", "WHITING", 
> "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD", "WILLARD", 
> "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK", 
> "WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS", 
> "WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON", 
> "WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE", 
> "WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS", 
> "WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH", 
> "YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS", "YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor"),
>     clusters = c(6L, 10L, 3L, 10L, 5L, 6L, 10L, 6L, 6L, 7L, 5L,
>     7L, 8L, 8L, 3L, 6L, 3L, 10L, 7L, 10L, 10L, 4L, 2L, 8L, 9L,
>     1L, 4L, 4L, 7L, 5L, 10L, 5L, 5L, 10L, 6L, 6L, 10L, 6L, 7L,
>     10L, 10L, 3L, 3L, 6L, 3L), Longitude = c(-93.883554, -90.582058,
>     -83.868923, -89.544083, -72.902467, -94.458904, -90.588533,
>     -94.527843, -92.93769, -80.029456, -70.58809, -82.888789,
>     -80.113071, -82.6661, -81.372992, -95.840964, -82.016013,
>     -89.92625, -80.276676, -89.778385, -87.641063, -94.888036,
>     -117.663119, -82.530568, -75.574437, -122.209047, -96.799309,
>     -95.746255, -80.553953, -73.923314, -87.713482, -73.9731,
>     -73.8187, -87.987023, -93.759478, -94.110903, -90.00597,
>     -93.257109, -79.932483, -90.201858, -89.100233, -84.625923,
>     -84.567614, -93.912921, -84.612564), Latitude = c(34.503045,
>     38.752515, 33.406527, 40.893588, 41.733088, 38.999032, 41.657674,
>     38.890929, 37.639766, 42.099693, 43.53462, 40.142492, 26.538149,
>     27.9062, 31.90104, 34.833083, 35.118836, 35.136047, 39.425712,
>     40.519207, 40.101126, 30.044457, 33.62287, 27.910276, 39.752254,
>     47.403778, 32.961929, 29.820199, 38.408649, 40.668828, 41.983948,
>     40.58116, 40.7253, 41.921667, 41.669921, 36.442552, 38.564663,
>     37.455315, 40.867774, 38.783554, 42.244381, 34.024332, 34.001644,
>     36.622704, 34.017832), distances_Me = c(1879089.079, 1437296.523,
>     1185638.742, 1313997.97, 156759.9125, 1759443.351, 1398398.997,
>     1767861.917, 1668917.322, 534234.2576, 431749.2965, 758508.4701,
>     1657031.938, 1615575.668, 1169085.702, 2024642.021, 930944.4681,
>     1525322.746, 553069.7532, 1337002.822, 1161367.37, 2223496.376,
>     3918520.212, 1609088.602, 164252.5146, 3878581.001, 2197910.688,
>     2304526.789, 614908.3151, 11075.24597, 1160375.454, 494.2997496,
>     21063.72622, 1182681.434, 1662351.873, 1810881.911, 1393581.436,
>     1701982.013, 504395.772, 1404348.991, 1276505.454, 1190457.718,
>     1188029.899, 1787261.336, 1189994.055), distances_Mi = c(1167.607468,
>     893.0913246, 736.719012, 816.477441, 97.40573055, 1093.263337,
>     868.9216123, 1098.494372, 1037.01328, 331.9565399, 268.2755749,
>     471.3135552, 1029.628072, 1003.868436, 726.4334682, 1258.049536,
>     578.4599174, 947.7880794, 343.6603307, 830.7719404, 721.6375366,
>     1381.611443, 2434.846498, 999.8375755, 102.0614003, 2410.029515,
>     1365.713293, 1431.96122, 382.0848884, 6.881813138, 721.0211909,
>     0.30714248, 13.08834388, 734.8814328, 1032.933715, 1125.225657,
>     865.9281298, 1057.55865, 313.4158337, 872.6187534, 793.1807589,
>     739.7133739, 738.2048023, 1110.548567, 739.4252678), ID = 
> 2308:2352), row.names = c(NA, -45L), class = "data.frame")
>
> Proprietary
>
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may 
> con...{{dropped:16}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIBaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmv
> OXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sRhKIDOv4AuwrfD
> BU9b2Rpj-VPfs0lcpkQqNBfVLSe4&s=CloI44kowc5J_mOah6vx_Kl4PFIeqxmwexvRiqw
> xuOA&e= PLEASE do read the posting guide 
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIBaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmm
> vOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sRhKIDOv4Auwrf
> DBU9b2Rpj-VPfs0lcpkQqNBfVLSe4&s=iOWreiJDomXv-uJ3NzQNX0IUsupILo_4DmkFK3
> GRUig&e= and provide commented, minimal, self-contained, reproducible 
> code.

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From Po||ngW @end|ng |rom @etn@@com  Thu May 14 14:19:59 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Thu, 14 May 2020 12:19:59 +0000
Subject: [R] Help with map()
In-Reply-To: <BYAPR06MB538355DBADC052E34DA1D3D8AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB5383688B7688345330422859AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CA+8X3fWwQ2HrBUxUW1Od5AeBegfmurkpz1Gu-wFhfAZ4dX9cmQ@mail.gmail.com>
 <BYAPR06MB538311FCA8A217B27743C85BAEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <BYAPR06MB53836AD813508C208C7D5B61AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <BYAPR06MB538355DBADC052E34DA1D3D8AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <BYAPR06MB5383ECACAEE69771084F4699AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>

Hello Jim, et.al, again.

One last question please.

Since this routine works best with larger volume of geo points, is there a way to modify this routine to accommodate more than one cluster and differentiated by different colors?

#Single cluster routine
clus3 <- individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,state,city,clusters,Latitude,Longitude,distances_Mi) %>% #Notice now that these are capitalized <--"L"
  
  filter(clusters == 3)

str(clus3)

clus3 <- as.data.frame(clus3)

geomat<-makeDensityMatrix(clus3[,c("Latitude","Longitude")],
                          xlim=range(clus3$Longitude),ylim=range(clus3$Latitude))
# Range of density (>0) - 1.292155 3.89646 
latlim<-range(clus3$Latitude)
lonlim<-range(clus3$Longitude)

map("world",xlim=lonlim,ylim=latlim)
axis(1)
axis(2)
densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
            red=c(0.5,1),green=0,blue=0,pch=15)
title("Member Geo Density Plot For Cluster3")

knitr::kable(individual_dets_sf_3X %>% group_by(clusters) %>% tally(sort = TRUE))

#   | clusters|   n|
#   |--------:|---:|
#   |        3| 384|
#   |        5| 335|
#   |        9| 305|
#   |        7| 298|
#   |       10| 286|
#   |        6| 279|
#   |        4| 168|
#   |        8| 131|
#   |        2| 113|
#   |        1|  53|

'data.frame':	384 obs. of  7 variables:
 $ MBR_SUBSCRIBERID: Factor w/ 2352 levels "101040199600",..: 908 2272 1613 1357 1952 1437 795 1634 1160 902 ...
 $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 10 23 23 10 33 10 10 23 10 10 ...
 $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 156 496 354 827 670 1161 897 970 249 494 ...
 $ clusters        : num  3 3 3 3 3 3 3 3 3 3 ...
 $ Latitude        : num  31.2 35.3 36.5 33.4 34.2 ...
 $ Longitude       : num  -81.5 -82.4 -81 -84.7 -80.7 ...
 $ distances_Mi    : num  770 586 471 769 573 ...





Proprietary

-----Original Message-----
From: Poling, William 
Sent: Thursday, May 14, 2020 7:03 AM
To: Jim Lemon <drjimlemon at gmail.com>
Cc: r-help at r-project.org; Mark Fowler <gmark.fowler at outlook.com>
Subject: RE: [EXTERNAL] Re: [R] Help with map()

Hi, it is working now using just the most necessary pkgs.

 Evidently when there are so few rows (clus1=53) the map does not cooperate, I get the plot but no map image?

As I increase records the map begins to appear as it did for Radius1-8 routines

Thank you for your time and trouble my friends.

WHP





Proprietary

-----Original Message-----
From: Poling, William 
Sent: Thursday, May 14, 2020 6:46 AM
To: Jim Lemon <drjimlemon at gmail.com>
Cc: r-help at r-project.org; Mark Fowler <gmark.fowler at outlook.com>
Subject: RE: [EXTERNAL] Re: [R] Help with map()

Hi, I ran this which is partially successful. I got long & lat  (x&y) with red plot values however, no map behind it?

Not sure what I might be missing now in terms of pkgs I suppose?
#Just use the basic pkgs
library(magrittr)#for %>% function
library(plotrix)
library(maps)
library(dplyr)#For filter()
str(individual_dets_sf_3X)
str(radius3)
str(clus1)


#rm(clus1)

clus1 <- individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,state,city,clusters,Latitude,Longitude,distances_Mi) %>% #Notice now that these are capitalized <--"L"
  
  filter(clusters == 1)

str(clus1)

clus1 <- as.data.frame(clus1)

geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
                          xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))
# Range of density (>0) - 1.292155 3.89646 
latlim<-range(clus1$Latitude)
lonlim<-range(clus1$Longitude)

map("world",xlim=lonlim,ylim=latlim)
axis(1)
axis(2)
densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
            red=c(0.5,1),green=0,blue=0,pch=15)
title("Member Geo Density Plot For Cluster1")





Proprietary

-----Original Message-----
From: Poling, William 
Sent: Thursday, May 14, 2020 6:29 AM
To: Jim Lemon <drjimlemon at gmail.com>
Cc: r-help at r-project.org; Mark Fowler <gmark.fowler at outlook.com>
Subject: RE: [EXTERNAL] Re: [R] Help with map()


Hi Jim, and Mark, thank you for your response.

1. I have restarted R
2. I have only initiated library(magrittr)#for %>% function & library(plotrix), no other libraries thinking that another may be overwriting something.
3. I have checked the str()

str(radius3)
'data.frame':	1990 obs. of  6 variables:
 $ MBR_SUBSCRIBERID: Factor w/ 2352 levels "101040199600",..: 590 908 976 509 1674 690 1336 726 1702 2331 ...
 $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 32 10 25 11 9 32 13 12 12 17 ...
 $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 932 156 230 698 965 1330 515 1127 1304 1316 ...
 $ Latitude        : num  40.4 31.2 40.8 42.1 26.8 ...
 $ Longitude       : num  -79.9 -81.5 -74 -91.6 -82.1 ...
 $ distances_Mi    : num  310.3 769.9 16.1 920.4 1057.6 ...

str(clus1)
tibble [53 x 7] (S3: tbl_df/tbl/data.frame)  $ MBR_SUBSCRIBERID: Factor w/ 2352 levels "101040199600",..: 86 2111 995 899 953 924 769 92 790 1748 ...
 $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 31 39 4 39 39 39 39 31 39 39 ...
 $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 727 85 455 1018 1203 604 1169 727 984 295 ...
 $ clusters        : num [1:53] 1 1 1 1 1 1 1 1 1 1 ...
 $ Latitude        : num [1:53] 42.4 47.6 39.2 46.9 48 ...
 $ Longitude       : num [1:53] -123 -122 -121 -123 -122 ...
 $ distances_Mi    : num [1:53] 2499 2406 2472 2430 2408 ...

4. I change clus1 to DF
'data.frame':	53 obs. of  7 variables:
 $ MBR_SUBSCRIBERID: Factor w/ 2352 levels "101040199600",..: 86 2111 995 899 953 924 769 92 790 1748 ...
 $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 31 39 4 39 39 39 39 31 39 39 ...
 $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 727 85 455 1018 1203 604 1169 727 984 295 ...
 $ clusters        : num  1 1 1 1 1 1 1 1 1 1 ...
 $ Latitude        : num  42.4 47.6 39.2 46.9 48 ...
 $ Longitude       : num  -123 -122 -121 -123 -122 ...
 $ distances_Mi    : num  2499 2406 2472 2430 2408 ...

Then try again, no luck?

Weird?

Thanks

WHP

Proprietary

-----Original Message-----
From: Jim Lemon <drjimlemon at gmail.com>
Sent: Thursday, May 14, 2020 5:44 AM
To: Poling, William <PolingW at aetna.com>
Cc: r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with map()

**** External Email - Use Caution ****

Hi Bill,
Have you compared str(radius3) to str(clus1)? It may be quite different.

Jim

On Thu, May 14, 2020 at 8:24 PM Poling, William via R-help <r-help at r-project.org> wrote:
>
> #RStudio Version Version 1.2.1335
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit) #Running under: Windows 10
> x64 (build 17763)
>
> Good morning.
>
> I ran routines like this one yesterday with no errors.
>
> #Radius3----
> str(radius3)
>
> radius3 <- individual_dets_sf_3X %>%
>
>   dplyr::select(MBR_SUBSCRIBERID,state,city,Latitude,Longitude,distances_Mi) %>%
>   filter(distances_Mi <= 1200)
>
> str(radius3)
>
>   geomat<-makeDensityMatrix(radius3[,c("Latitude","Longitude")],
>                             
> xlim=range(radius3$Longitude),ylim=range(radius3$Latitude))
>
> latlim<-range(radius3$Latitude)
> lonlim<-range(radius3$Longitude)
>
> <- map("world",xlim=lonlim,ylim=latlim)
> axis(1)
> axis(2)
> densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
>             red=c(0.5,1),green=0,blue=0,pch=15)
> title("Member Geo Density Plot For Radius3")
>
>
> This morning I am trying slightly new routine and receiving this error
>
> clus1 <- individual_dets_sf_3X %>%
>
>   dplyr::select(MBR_SUBSCRIBERID,state,city,clusters,Latitude,Longitude,distances_Mi) %>%
>   filter(clusters == 1)
>
> geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
>                           
> xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))
>
> latlim<-range(clus1$Latitude)
> lonlim<-range(clus1$Longitude)
>
> map("world",xlim=lonlim,ylim=latlim)
> axis(1)
> axis(2)
> densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
>             red=c(0.5,1),green=0,blue=0,pch=15)
> title("Member Geo Density Plot For Cluster1")
>
>
> geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
> +                           
> + xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))
>
> > latlim<-range(clus1$Latitude)
> > lonlim<-range(clus1$Longitude)
> >
> > map("world",xlim=lonlim,ylim=latlim)
> Error in .C(C_map_type, as.character(mapbase), integer(1)) :
>   Incorrect number of arguments (2), expecting 0 for ''
> > axis(1)
> Error in axis(1) : plot.new has not been called yet
> > axis(2)
> Error in axis(2) : plot.new has not been called yet
> > densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
> +             red=c(0.5,1),green=0,blue=0,pch=15)
> Error in plot.xy(xy.coords(x, y), type = type, ...) :
>   plot.new has not been called yet
> > title("Member Geo Density Plot For Cluster1")
> Error in title("Member Geo Density Plot For Cluster1") :
>   plot.new has not been called yet
>
>
> It seems to error at the point of map("world",xlim=lonlim,ylim=latlim)
>
> I find a ref in stack overflow
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stackoverflow.com
> _questions_45066628_cannot-2Drun-2Dmap-2Ddatastate&d=DwIBaQ&c=wluqKIiw
> ffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2Ex
> lYvnUWbsByY&m=sRhKIDOv4AuwrfDBU9b2Rpj-VPfs0lcpkQqNBfVLSe4&s=sEjkBzlaFR
> 7NhUDpUPGPp1nOkAxOlBbFNadLowcDbFw&e=
> and tried solutions but they do not seem to work?
>
> Here is a sample (MBR_SUBSCRIBERID(factor) is replaced with ID(integer) but of no consequence in my routine since not really used.
>
> I hope someone recognizes the problem.
>
> Thank you for any advice
>
> WHP
>
> > dput(sample)
> structure(list(state = structure(c(2L, 22L, 10L, 12L, 6L, 22L, 11L, 
> 22L, 22L, 32L, 19L, 29L, 9L, 9L, 10L, 30L, 33L, 35L, 41L, 12L, 12L, 
> 36L, 4L, 9L, 8L, 39L, 36L, 36L, 41L, 28L, 12L, 28L, 28L, 12L, 11L, 2L, 
> 12L, 22L, 32L, 22L, 12L, 10L, 10L, 22L, 10L ), .Label = c("AL", "AR", 
> "AZ", "CA", "CO", "CT", "DC", "DE", "FL", "GA", "IA", "IL", "IN", 
> "KS", "KY", "LA", "MA", "MD", "ME", "MI", "MN", "MO", "NC", "NE", 
> "NJ", "NM", "NV", "NY", "OH", "OK", "OR", "PA", "SC", "SD", "TN", 
> "TX", "UT", "VA", "WA", "WI", "WV"
> ), class = "factor"), city = structure(c(838L, 1030L, 262L, 218L, 
> 166L, 973L, 339L, 451L, 660L, 358L, 281L, 1280L, 129L, 223L, 989L, 
> 721L, 550L, 731L, 1325L, 688L, 1184L, 281L, 759L, 1171L, 1305L, 587L, 
> 272L, 581L, 263L, 152L, 217L, 152L, 390L, 5L, 571L, 1006L, 1162L, 
> 939L, 170L, 1033L, 1002L, 586L, 586L, 192L, 586L ), .Label = 
> c("ABBOTTSTOWN", "ABILENE", "ACWORTH", "ADAMS", "ADDISON", "ADKINS", 
> "ALBANY", "ALBIA", "ALBUQUERQUE", "ALEXANDRIA", "ALFRED", "ALIQUIPPA", 
> "ALISO VIEJO", "ALLEN PARK", "ALLENTOWN", "ALPHA", "ALPHARETTA", 
> "ALPINE", "ALTOONA", "AMARILLO", "AMBLER", "AMBRIDGE", "AMHERST", 
> "AMITYVILLE", "ANAHIEM", "ANDERSON", "ANDOVER", "ANGIER", "ANGLETON", 
> "ANKENY", "ANN ARBOR", "ANZA", "APEX", "APOLLO", "ARCADIA", 
> "ARCHDALE", "ARDMORE", "ARGYLE", "ARKANSAS CITY", "ARLINGTON", 
> "ARNOLD", "ARTHUR", "ASHEBORO", "ASHEVILLE", "ASHLAND", "ASHLAND 
> CITY", "ASHTON", "ASTON", "ATHENS", "ATLANTA", "ATLANTIC BCH", 
> "AUBORN", "AUGUSTA", "AURORA", "AUSTELL", "AUSTIN", "AVENTURA", 
> "AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN CITY", "BALDWIN PLACE", 
> "BALLWIN", "BANGOR", "BARBOURSVILLE", "BARNEGAT", "BARTLETT", 
> "BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", "BATESBURG", "BATON 
> ROUGE", "BAXTER SPRINGS", "BAY CITY", "BAYONNE", "BAYSIDE", "BAYTOWN", 
> "BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER CITY", "BELFAIR", "BELLE 
> VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR", "BELLMORE", "BELLWOOD", 
> "BELMAR", "BELMONT", "BELVIDERE", "BENNET", "BENTON", "BENTONVILLE", 
> "BERLIN", "BESSEMER CITY", "BETHANY", "BETHEL", "BETHEL PARK", 
> "BETHESDA", "BETHLEHEM", "BETTENDORF", "BIGLERVILLE", "BINGHAMTON", 
> "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN", "BLACKSBURG", "BLAINE", 
> "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD", "BLUE BELL", "BLUE GRASS", 
> "BLUE POINT", "BLUE SPRINGS", "BOCA RATON", "BODFISH", "BOILING 
> SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS", "BONNEY LAKE", 
> "BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING GREEN", 
> "BOYERTOWN", "BOYNTON BEACH", "BRADENTON", "BRADENVILLE", 
> "BRANDAMORE", "BRANDON", "BRANFORD", "BRANSON", "BRASELTON", 
> "BREEZEWOOD", "BRENTON", "BRENTWOOD", "BREWSTER", "BRICK", 
> "BRIDGEPORT", "BRIDGETON", "BRIGHAM CITY", "BROADDUS", "BROCKPORT", 
> "BROGUE", "BROKEN ARROW", "BRONX", "BROOKFIELD", "BROOKHAVEN", 
> "BROOKLYN", "BROWNS MILLS", "BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", 
> "BRYAN", "BUCKSPORT", "BUDD LAKE", "BUFFALO", "BULVERDE", 
> "BUNKERVILLE", "BURBANK", "BURFORD", "BURIEN", "BURLINGTON", "BURR 
> RIDGE", "BURTON", "BUSKIRK", "BUTLER", "BUXTON", "BYRON", "CALERA", 
> "CAMBRIDGE", "CAMP HILL", "CAMPBELL", "CANAAN", "CANAL WINCHESTER", 
> "CANASTOTA", "CANTON", "CARDIFF BY THE SEA", "CARL JUNCTION", 
> "CARLISLE", "CARLTON", "CAROL STREAM", "CARROLLTON", "CARSON CITY", 
> "CARTERSVILLE", "CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", 
> "CASWELL", "CATO", "CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER 
> POINT", "CENTERTON", "CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN 
> FALLS", "CHALFONT", "CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", 
> "CHARLESTON", "CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", 
> "CHESHIRE", "CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", 
> "CHILLICOTHE", "CHOCTAW", "CICERO", "CINCINNATI", "CLAY", 
> "CLEARWATER", "CLEARWATER BEACH", "CLEMMONS", "CLENDENIN", 
> "CLEVELAND", "CLEVELAND HTS", "CLEVES", "CLIFFSIDE PARK", "CLIFTON", 
> "CLIFTON PARK", "CLIFTON SPGS", "CLINTON", "COACHELLA", "COAL 
> TOWNSHIP", "COATESVILLE", "COLLEGE STATION", "COLLINSVILLE", 
> "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE", "CONCORD", "CONCORD 
> TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY", "CONYERS", 
> "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA", "CORNELIUS", 
> "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY", "COTTAGE HILLS", 
> "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN", "CRANBERRY 
> TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER", "CUMBERLAND 
> FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS", "DALLAS", 
> "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT", 
> "DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR", 
> "DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH", 
> "DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY", "DES 
> MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER", "DIXON", 
> "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER", "DOYLESTOWN", 
> "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH", "DUNCAN", "DUNCANNON", 
> "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE", "DURHAM", "DUTTON", "EAGLE 
> GROVE", "EAST AURORA", "EAST CHICAGO", "EAST EARL", "EAST HANOVER", 
> "EAST HARTFORD", "EAST HARTLAND", "EAST LEROY", "EAST LIVERPOOL", 
> "EAST MEADOW", "EAST NORTHPORT", "EAST ORANGE", "EAST PEORIA", "EAST 
> SAINT LOUIS", "EASTON", "EDGERTON", "EDISON", "EDMOND", "EGG HBR TWP", 
> "EL DORADO", "EL PASO", "ELDRIDGE", "ELGIN", "ELIZABETHVILLE", 
> "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY", "ELMHURST", "ELMWOOD PARK", 
> "ELOY", "ELY", "EMERSON", "EMORY", "ENDICOTT", "ENGLEWOOD", "ENID", 
> "ENNICE", "ENOLA", "ENUMCLAW", "EPHRATA", "ERIE", "ESTERO", "EUREKA 
> SPRINGS", "EVANSVILLE", "EVERETT", "EXCELSIOR SPRINGS", "EXTON", 
> "FAIRFAX", "FAIRFIELD", "FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", 
> "FALMOUTH", "FAR HILLS", "FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", 
> "FAYETTE", "FAYETTEVILLE", "FELTON", "FENTON", "FERGUSON", "FIELDALE", 
> "FINDLAY", "FINGERVILLE", "FLEMINGTON", "FLINT", "FLORENCE", 
> "FLORESVILLE", "FLORISSANT", "FLOVILLA", "FLOWER MOUND", "FLUSHING", 
> "FOGELSVILLE", "FOLSOM", "FONTANELLE", "FOREST HILLS", "FOREST PARK", 
> "FORT DODGE", "FORT FAIRFIELD", "FORT GAY", "FORT LAUDERDALE", "FORT 
> LEE", "FORT MILL", "FORT MOHAVE", "FORT PAYNE", "FORT PIERCE", "FORT 
> SMITH", "FORT WAYNE", "FORT WORTH", "FOSTORIA", "FOUNTAIN INN", 
> "FRANKFORT", "FRANKLIN", "FREEDOM", "FREEHOLD", "FREEPORT", "FREMONT", 
> "FULLERTON", "FULSHEAR", "FULTON", "GADSDEN", "GAFFNEY", 
> "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE", "GARDNERS", "GARNETT 
> VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA", "GETTYSBURG", 
> "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE", "GLEN CARBON", "GLEN 
> COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW", "GLENWOOD", "GOLDEN 
> CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM", "GOSHEN", 
> "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS", "GRANDVIEW", 
> "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS", "GRASS VALLEY", 
> "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER", "GREENLAWN", 
> "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE", "GREER", 
> "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD", "HADLEY", 
> "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON", "HAMBURG", 
> "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE", 
> "HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON", 
> "HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL", 
> "HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE", 
> "HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH", 
> "HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE", "HIGH 
> HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS", 
> "HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE", 
> "HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD", 
> "HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS", "HOT 
> SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON", "HUFFMAN", 
> "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON", "HUNTINGTON 
> BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE", "HURST", 
> "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL", 
> "INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY", "IRMO", 
> "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP", "ISSAQUAH", 
> "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS", "JACKSONVILLE", 
> "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE", "JEFFERSON", "JEFFERSON 
> CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN", "JONESBORO", "JONESPORT", 
> "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE", "KANNAPOLIS", "KANSAS 
> CITY", "KATY", "KEARNEY", "KELLERTON", "KEMAH", "KENMORE", "KENNESAW", 
> "KENT", "KINGMAN", "KINGS MOUNTAIN", "KINGSLAND", "KINGSTON", 
> "KINGWOOD", "KIRKLAND", "KUTZTOWN", "LA QUINTA", "LA RUE", "LA VERNE", 
> "LAFAYETTE", "LAKE BLUFF", "LAKE IN THE HILL", "LAKE ISABELLA", "LAKE 
> PLACID", "LAKE SAINT LOUIS", "LAKE TAPPS", "LAKE VILLAGE", "LAKE 
> WAUKOMIS", "LAKE WORTH", "LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD 
> RCH", "LAMBERTVILLE", "LANCASTER", "LANDISVILLE", "LANSDALE", 
> "LAREDO", "LARGO", "LAS VEGAS", "LATROBE", "LAUDERHILL", "LAWNDALE", 
> "LAWRENCE", "LAWRENCEVILLE", "LE MARS", "LE ROY", "LEAGUE CITY", 
> "LEAVENWORTH", "LEAWOOD", "LEBANON", "LECOMPTON", "LEECHBURG", "LEES 
> SUMMIT", "LEESBURG", "LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", 
> "LEVITTOWN", "LEWES", "LEWISTOWN", "LEXINGTON", "LIBERTY TWP", 
> "LILLINGTON", "LINCOLN", "LINDENHURST", "LINTON", "LISBON", 
> "LITCHFIELD PK", "LITHIA", "LITITZ", "LITTLE EGG HARBOR TO", "LK 
> FOREST PK", "LK PEEKSKILL", "LOCKHART", "LOCKWOOD", "LOGAN", "LOMA 
> LINDA", "LONE JACK", "LONG BEACH", "LONG LANE", "LONGVIEW", "LORAIN", 
> "LORTON", "LOS ANGELES", "LOUISBURG", "LOVES PARK", "LOWER BURRELL", 
> "LOWRY CITY", "LUBBOCK", "LUGOFF", "LUMBERTON", "LYNBROOK", 
> "LYNNWOOD", "MACHESNEY PARK", "MACON", "MAGNOLIA", "MAHOPAC", 
> "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE", "MANASSAS", "MANCHACA", 
> "MANCHESTER", "MANCHESTER TOWNSHIP", "MANCHESTER TW", "MANHEIM", 
> "MANITO", "MANLIUS", "MANNINGTON", "MANSFIELD", "MANSURA", "MAPLE 
> PARK", "MARBLE FALLS", "MARCELLUS", "MARICOPA", "MARIETTA", "MARION", 
> "MARKLEYSBURG", "MARS HILL", "MARSHALLTOWN", "MARSHFIELD", 
> "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE", "MASON CITY", "MASSEY", 
> "MASSILLON", "MASURY", "MATAWAN", "MATEWAN", "MATTAPOISETT", 
> "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND", "MC DONALD", "MC 
> KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN", "MCALESTER", "MCDONOUGH", 
> "MCKEESPORT", "MCKINNEY", "MECHANICSBURG", "MECHANICSVILLE", 
> "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS", "MERIDEN", 
> "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA", "MESQUITE", 
> "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN", "MIDLAND", 
> "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK", "MILLEDGEVILLE", 
> "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE", "MINEOLA", "MINERAL 
> SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION", "MISSION HILLS", 
> "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY", "MOLINE", 
> "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE", "MONTCLAIR", 
> "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON", "MORGANTOWN", 
> "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE", "MOUNT JOY", 
> "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO", "MOUNT UNION", 
> "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT", "MULVANE", 
> "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT", 
> "MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS", 
> "NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES", 
> "NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG", "NEW 
> BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK", "NEW 
> CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON", "NEW 
> LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE", "NEW 
> ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK", 
> "NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT", 
> "NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA", 
> "NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK", 
> "NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON", 
> "NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND 
> HILLS", "NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE", 
> "NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON", "OAK 
> RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE", "OMAHA", 
> "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION", "ORONO", 
> "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO", "OTTAWA", 
> "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD", "OYSTER BAY", 
> "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR", "PALMDALE", "PALMYRA", 
> "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE", "PARLIN", "PARMA", 
> "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY", "PAXTON", "PEA 
> RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN", "PELION", 
> "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE", "PENNSBURG", 
> "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS", "PERRYSBURG", 
> "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI", "PHOENIX", 
> "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN", "PINE BLUFF", 
> "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY", "PITMAN", 
> "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY", 
> "PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO 
> LAKE", "POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", 
> "PORT CHARLOTTE", "PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", 
> "PORT WASHINGTON", "PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", 
> "POTTSVILLE", "POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE 
> VILLAGE", "PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT 
> PLEASANT", "PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", 
> "QUINCY", "RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", 
> "READLYN", "RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO 
> PARK", "REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", 
> "RICH HILL", "RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", 
> "RINCON", "RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", 
> "RIVERSIDE", "ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", 
> "ROCKAWAY BCH", "ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", 
> "ROGERS", "ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE", 
> "ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT", 
> "ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN", 
> "RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO", 
> "SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR", "SAINT 
> CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS", "SAINT 
> PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY", 
> "SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO", 
> "SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN", 
> "SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", 
> "SARASOTA", "SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", 
> "SAYVILLE", "SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", 
> "SCOTT", "SCOTTSDALE", "SEAFORD", "SEALE", "SEATTLE", "SEDONA", 
> "SEGUIN", "SELDEN", "SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", 
> "SHARON", "SHARPSBURG", "SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", 
> "SHELLSBURG", "SHELTON", "SHENANDOAH", "SHORELINE", "SHOREWOOD", 
> "SHREVEPORT", "SICKLERVILLE", "SILER CITY", "SILOAM SPRINGS", "SIMON", 
> "SIMPSONVILLE", "SIMSBURY", "SIOUX CITY", "SIOUX FALLS", "SKOKIE", 
> "SKOWHEGAN", "SLATINGTON", "SLIDELL", "SMITHFIELD", "SN BERNRDNO", 
> "SNELLVILLE", "SOCORRO", "SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH 
> BELOIT", "SOUTH BEND", "SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH 
> PARK", "SOUTH PLAINFIELD", "SOUTH SIOUX CITY", "SOUTHAMPTON", 
> "SOUTHBURY", "SOUTHGATE", "SPARKS", "SPARROW BUSH", "SPENCER", 
> "SPRING", "SPRING BRANCH", "SPRING GROVE", "SPRING HILL", "SPRING 
> HOPE", "SPRING VALLEY", "SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD 
> GARDENS", "SPRINGHILL", "SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", 
> "ST PETERSBURG", "ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", 
> "STANLEY", "STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", 
> "STILLMAN VALLEY", "STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", 
> "STRAFFORD", "STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", 
> "SUFFIELD", "SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", 
> "SUMMIT", "SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", 
> "SWEENY", "SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", 
> "TAFTVILLE", "TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", 
> "TAYLORVILLE", "TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", 
> "THOMASVILLE", "TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY 
> PARK", "TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", 
> "TOPEKA", "TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS 
> RST", "TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER", 
> "TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY", 
> "UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA", 
> "URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO", "VAN 
> BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO", 
> "VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA", 
> "VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH", "W 
> TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN", 
> "WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG", 
> "WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA", 
> "WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE", 
> "WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER", 
> "WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO", "WEST 
> CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN", "WEST 
> HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH", "WEST 
> POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD", "WEST VALLEY 
> CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER", "WESTMORELAND 
> CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD", "WHARTON", "WHEELING", 
> "WHITE PLAINS", "WHITEHALL", "WHITESTONE", "WHITESTOWN", "WHITING", 
> "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD", "WILLARD", 
> "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK", 
> "WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS", 
> "WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON", 
> "WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE", 
> "WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS", 
> "WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH", 
> "YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS", "YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor"),
>     clusters = c(6L, 10L, 3L, 10L, 5L, 6L, 10L, 6L, 6L, 7L, 5L,
>     7L, 8L, 8L, 3L, 6L, 3L, 10L, 7L, 10L, 10L, 4L, 2L, 8L, 9L,
>     1L, 4L, 4L, 7L, 5L, 10L, 5L, 5L, 10L, 6L, 6L, 10L, 6L, 7L,
>     10L, 10L, 3L, 3L, 6L, 3L), Longitude = c(-93.883554, -90.582058,
>     -83.868923, -89.544083, -72.902467, -94.458904, -90.588533,
>     -94.527843, -92.93769, -80.029456, -70.58809, -82.888789,
>     -80.113071, -82.6661, -81.372992, -95.840964, -82.016013,
>     -89.92625, -80.276676, -89.778385, -87.641063, -94.888036,
>     -117.663119, -82.530568, -75.574437, -122.209047, -96.799309,
>     -95.746255, -80.553953, -73.923314, -87.713482, -73.9731,
>     -73.8187, -87.987023, -93.759478, -94.110903, -90.00597,
>     -93.257109, -79.932483, -90.201858, -89.100233, -84.625923,
>     -84.567614, -93.912921, -84.612564), Latitude = c(34.503045,
>     38.752515, 33.406527, 40.893588, 41.733088, 38.999032, 41.657674,
>     38.890929, 37.639766, 42.099693, 43.53462, 40.142492, 26.538149,
>     27.9062, 31.90104, 34.833083, 35.118836, 35.136047, 39.425712,
>     40.519207, 40.101126, 30.044457, 33.62287, 27.910276, 39.752254,
>     47.403778, 32.961929, 29.820199, 38.408649, 40.668828, 41.983948,
>     40.58116, 40.7253, 41.921667, 41.669921, 36.442552, 38.564663,
>     37.455315, 40.867774, 38.783554, 42.244381, 34.024332, 34.001644,
>     36.622704, 34.017832), distances_Me = c(1879089.079, 1437296.523,
>     1185638.742, 1313997.97, 156759.9125, 1759443.351, 1398398.997,
>     1767861.917, 1668917.322, 534234.2576, 431749.2965, 758508.4701,
>     1657031.938, 1615575.668, 1169085.702, 2024642.021, 930944.4681,
>     1525322.746, 553069.7532, 1337002.822, 1161367.37, 2223496.376,
>     3918520.212, 1609088.602, 164252.5146, 3878581.001, 2197910.688,
>     2304526.789, 614908.3151, 11075.24597, 1160375.454, 494.2997496,
>     21063.72622, 1182681.434, 1662351.873, 1810881.911, 1393581.436,
>     1701982.013, 504395.772, 1404348.991, 1276505.454, 1190457.718,
>     1188029.899, 1787261.336, 1189994.055), distances_Mi = c(1167.607468,
>     893.0913246, 736.719012, 816.477441, 97.40573055, 1093.263337,
>     868.9216123, 1098.494372, 1037.01328, 331.9565399, 268.2755749,
>     471.3135552, 1029.628072, 1003.868436, 726.4334682, 1258.049536,
>     578.4599174, 947.7880794, 343.6603307, 830.7719404, 721.6375366,
>     1381.611443, 2434.846498, 999.8375755, 102.0614003, 2410.029515,
>     1365.713293, 1431.96122, 382.0848884, 6.881813138, 721.0211909,
>     0.30714248, 13.08834388, 734.8814328, 1032.933715, 1125.225657,
>     865.9281298, 1057.55865, 313.4158337, 872.6187534, 793.1807589,
>     739.7133739, 738.2048023, 1110.548567, 739.4252678), ID = 
> 2308:2352), row.names = c(NA, -45L), class = "data.frame")
>
> Proprietary
>
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may 
> con...{{dropped:16}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIBaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmv
> OXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sRhKIDOv4AuwrfD
> BU9b2Rpj-VPfs0lcpkQqNBfVLSe4&s=CloI44kowc5J_mOah6vx_Kl4PFIeqxmwexvRiqw
> xuOA&e= PLEASE do read the posting guide 
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIBaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmm
> vOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sRhKIDOv4Auwrf
> DBU9b2Rpj-VPfs0lcpkQqNBfVLSe4&s=iOWreiJDomXv-uJ3NzQNX0IUsupILo_4DmkFK3
> GRUig&e= and provide commented, minimal, self-contained, reproducible 
> code.

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From drj|m|emon @end|ng |rom gm@||@com  Thu May 14 14:27:08 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Thu, 14 May 2020 22:27:08 +1000
Subject: [R] Help with map()
In-Reply-To: <BYAPR06MB5383ECACAEE69771084F4699AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB5383688B7688345330422859AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CA+8X3fWwQ2HrBUxUW1Od5AeBegfmurkpz1Gu-wFhfAZ4dX9cmQ@mail.gmail.com>
 <BYAPR06MB538311FCA8A217B27743C85BAEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <BYAPR06MB53836AD813508C208C7D5B61AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <BYAPR06MB538355DBADC052E34DA1D3D8AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <BYAPR06MB5383ECACAEE69771084F4699AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <CA+8X3fW9dm+frfX3M_TE4S9Pi+cFnV23fn99c3Z-GoUvPZDjRQ@mail.gmail.com>

Hi Bill,
I guess if you run makeDensityMatrix on subsets of the geographic
locations and then overlay the different results with densityGrid
using different colors, it should work as long as the regions don't
overlap.

Jim

On Thu, May 14, 2020 at 10:20 PM Poling, William <PolingW at aetna.com> wrote:
>
> Hello Jim, et.al, again.
>
> One last question please.
>
> Since this routine works best with larger volume of geo points, is there a way to modify this routine to accommodate more than one cluster and differentiated by different colors?
>
> #Single cluster routine
> clus3 <- individual_dets_sf_3X %>%
>
>   dplyr::select(MBR_SUBSCRIBERID,state,city,clusters,Latitude,Longitude,distances_Mi) %>% #Notice now that these are capitalized <--"L"
>
>   filter(clusters == 3)
>
> str(clus3)
>
> clus3 <- as.data.frame(clus3)
>
> geomat<-makeDensityMatrix(clus3[,c("Latitude","Longitude")],
>                           xlim=range(clus3$Longitude),ylim=range(clus3$Latitude))
> # Range of density (>0) - 1.292155 3.89646
> latlim<-range(clus3$Latitude)
> lonlim<-range(clus3$Longitude)
>
> map("world",xlim=lonlim,ylim=latlim)
> axis(1)
> axis(2)
> densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
>             red=c(0.5,1),green=0,blue=0,pch=15)
> title("Member Geo Density Plot For Cluster3")
>
> knitr::kable(individual_dets_sf_3X %>% group_by(clusters) %>% tally(sort = TRUE))
>
> #   | clusters|   n|
> #   |--------:|---:|
> #   |        3| 384|
> #   |        5| 335|
> #   |        9| 305|
> #   |        7| 298|
> #   |       10| 286|
> #   |        6| 279|
> #   |        4| 168|
> #   |        8| 131|
> #   |        2| 113|
> #   |        1|  53|
>
> 'data.frame':   384 obs. of  7 variables:
>  $ MBR_SUBSCRIBERID: Factor w/ 2352 levels "101040199600",..: 908 2272 1613 1357 1952 1437 795 1634 1160 902 ...
>  $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 10 23 23 10 33 10 10 23 10 10 ...
>  $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 156 496 354 827 670 1161 897 970 249 494 ...
>  $ clusters        : num  3 3 3 3 3 3 3 3 3 3 ...
>  $ Latitude        : num  31.2 35.3 36.5 33.4 34.2 ...
>  $ Longitude       : num  -81.5 -82.4 -81 -84.7 -80.7 ...
>  $ distances_Mi    : num  770 586 471 769 573 ...
>
>
>
>
>
> Proprietary
>
> -----Original Message-----
> From: Poling, William
> Sent: Thursday, May 14, 2020 7:03 AM
> To: Jim Lemon <drjimlemon at gmail.com>
> Cc: r-help at r-project.org; Mark Fowler <gmark.fowler at outlook.com>
> Subject: RE: [EXTERNAL] Re: [R] Help with map()
>
> Hi, it is working now using just the most necessary pkgs.
>
>  Evidently when there are so few rows (clus1=53) the map does not cooperate, I get the plot but no map image?
>
> As I increase records the map begins to appear as it did for Radius1-8 routines
>
> Thank you for your time and trouble my friends.
>
> WHP
>
>
>
>
>
> Proprietary
>
> -----Original Message-----
> From: Poling, William
> Sent: Thursday, May 14, 2020 6:46 AM
> To: Jim Lemon <drjimlemon at gmail.com>
> Cc: r-help at r-project.org; Mark Fowler <gmark.fowler at outlook.com>
> Subject: RE: [EXTERNAL] Re: [R] Help with map()
>
> Hi, I ran this which is partially successful. I got long & lat  (x&y) with red plot values however, no map behind it?
>
> Not sure what I might be missing now in terms of pkgs I suppose?
> #Just use the basic pkgs
> library(magrittr)#for %>% function
> library(plotrix)
> library(maps)
> library(dplyr)#For filter()
> str(individual_dets_sf_3X)
> str(radius3)
> str(clus1)
>
>
> #rm(clus1)
>
> clus1 <- individual_dets_sf_3X %>%
>
>   dplyr::select(MBR_SUBSCRIBERID,state,city,clusters,Latitude,Longitude,distances_Mi) %>% #Notice now that these are capitalized <--"L"
>
>   filter(clusters == 1)
>
> str(clus1)
>
> clus1 <- as.data.frame(clus1)
>
> geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
>                           xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))
> # Range of density (>0) - 1.292155 3.89646
> latlim<-range(clus1$Latitude)
> lonlim<-range(clus1$Longitude)
>
> map("world",xlim=lonlim,ylim=latlim)
> axis(1)
> axis(2)
> densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
>             red=c(0.5,1),green=0,blue=0,pch=15)
> title("Member Geo Density Plot For Cluster1")
>
>
>
>
>
> Proprietary
>
> -----Original Message-----
> From: Poling, William
> Sent: Thursday, May 14, 2020 6:29 AM
> To: Jim Lemon <drjimlemon at gmail.com>
> Cc: r-help at r-project.org; Mark Fowler <gmark.fowler at outlook.com>
> Subject: RE: [EXTERNAL] Re: [R] Help with map()
>
>
> Hi Jim, and Mark, thank you for your response.
>
> 1. I have restarted R
> 2. I have only initiated library(magrittr)#for %>% function & library(plotrix), no other libraries thinking that another may be overwriting something.
> 3. I have checked the str()
>
> str(radius3)
> 'data.frame':   1990 obs. of  6 variables:
>  $ MBR_SUBSCRIBERID: Factor w/ 2352 levels "101040199600",..: 590 908 976 509 1674 690 1336 726 1702 2331 ...
>  $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 32 10 25 11 9 32 13 12 12 17 ...
>  $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 932 156 230 698 965 1330 515 1127 1304 1316 ...
>  $ Latitude        : num  40.4 31.2 40.8 42.1 26.8 ...
>  $ Longitude       : num  -79.9 -81.5 -74 -91.6 -82.1 ...
>  $ distances_Mi    : num  310.3 769.9 16.1 920.4 1057.6 ...
>
> str(clus1)
> tibble [53 x 7] (S3: tbl_df/tbl/data.frame)  $ MBR_SUBSCRIBERID: Factor w/ 2352 levels "101040199600",..: 86 2111 995 899 953 924 769 92 790 1748 ...
>  $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 31 39 4 39 39 39 39 31 39 39 ...
>  $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 727 85 455 1018 1203 604 1169 727 984 295 ...
>  $ clusters        : num [1:53] 1 1 1 1 1 1 1 1 1 1 ...
>  $ Latitude        : num [1:53] 42.4 47.6 39.2 46.9 48 ...
>  $ Longitude       : num [1:53] -123 -122 -121 -123 -122 ...
>  $ distances_Mi    : num [1:53] 2499 2406 2472 2430 2408 ...
>
> 4. I change clus1 to DF
> 'data.frame':   53 obs. of  7 variables:
>  $ MBR_SUBSCRIBERID: Factor w/ 2352 levels "101040199600",..: 86 2111 995 899 953 924 769 92 790 1748 ...
>  $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 31 39 4 39 39 39 39 31 39 39 ...
>  $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 727 85 455 1018 1203 604 1169 727 984 295 ...
>  $ clusters        : num  1 1 1 1 1 1 1 1 1 1 ...
>  $ Latitude        : num  42.4 47.6 39.2 46.9 48 ...
>  $ Longitude       : num  -123 -122 -121 -123 -122 ...
>  $ distances_Mi    : num  2499 2406 2472 2430 2408 ...
>
> Then try again, no luck?
>
> Weird?
>
> Thanks
>
> WHP
>
> Proprietary
>
> -----Original Message-----
> From: Jim Lemon <drjimlemon at gmail.com>
> Sent: Thursday, May 14, 2020 5:44 AM
> To: Poling, William <PolingW at aetna.com>
> Cc: r-help at r-project.org
> Subject: [EXTERNAL] Re: [R] Help with map()
>
> **** External Email - Use Caution ****
>
> Hi Bill,
> Have you compared str(radius3) to str(clus1)? It may be quite different.
>
> Jim
>
> On Thu, May 14, 2020 at 8:24 PM Poling, William via R-help <r-help at r-project.org> wrote:
> >
> > #RStudio Version Version 1.2.1335
> > sessionInfo()
> > # R version 4.0.0 Patched (2020-05-03 r78349)
> > #Platform: x86_64-w64-mingw32/x64 (64-bit) #Running under: Windows 10
> > x64 (build 17763)
> >
> > Good morning.
> >
> > I ran routines like this one yesterday with no errors.
> >
> > #Radius3----
> > str(radius3)
> >
> > radius3 <- individual_dets_sf_3X %>%
> >
> >   dplyr::select(MBR_SUBSCRIBERID,state,city,Latitude,Longitude,distances_Mi) %>%
> >   filter(distances_Mi <= 1200)
> >
> > str(radius3)
> >
> >   geomat<-makeDensityMatrix(radius3[,c("Latitude","Longitude")],
> >
> > xlim=range(radius3$Longitude),ylim=range(radius3$Latitude))
> >
> > latlim<-range(radius3$Latitude)
> > lonlim<-range(radius3$Longitude)
> >
> > <- map("world",xlim=lonlim,ylim=latlim)
> > axis(1)
> > axis(2)
> > densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
> >             red=c(0.5,1),green=0,blue=0,pch=15)
> > title("Member Geo Density Plot For Radius3")
> >
> >
> > This morning I am trying slightly new routine and receiving this error
> >
> > clus1 <- individual_dets_sf_3X %>%
> >
> >   dplyr::select(MBR_SUBSCRIBERID,state,city,clusters,Latitude,Longitude,distances_Mi) %>%
> >   filter(clusters == 1)
> >
> > geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
> >
> > xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))
> >
> > latlim<-range(clus1$Latitude)
> > lonlim<-range(clus1$Longitude)
> >
> > map("world",xlim=lonlim,ylim=latlim)
> > axis(1)
> > axis(2)
> > densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
> >             red=c(0.5,1),green=0,blue=0,pch=15)
> > title("Member Geo Density Plot For Cluster1")
> >
> >
> > geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
> > +
> > + xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))
> >
> > > latlim<-range(clus1$Latitude)
> > > lonlim<-range(clus1$Longitude)
> > >
> > > map("world",xlim=lonlim,ylim=latlim)
> > Error in .C(C_map_type, as.character(mapbase), integer(1)) :
> >   Incorrect number of arguments (2), expecting 0 for ''
> > > axis(1)
> > Error in axis(1) : plot.new has not been called yet
> > > axis(2)
> > Error in axis(2) : plot.new has not been called yet
> > > densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
> > +             red=c(0.5,1),green=0,blue=0,pch=15)
> > Error in plot.xy(xy.coords(x, y), type = type, ...) :
> >   plot.new has not been called yet
> > > title("Member Geo Density Plot For Cluster1")
> > Error in title("Member Geo Density Plot For Cluster1") :
> >   plot.new has not been called yet
> >
> >
> > It seems to error at the point of map("world",xlim=lonlim,ylim=latlim)
> >
> > I find a ref in stack overflow
> > https://urldefense.proofpoint.com/v2/url?u=https-3A__stackoverflow.com
> > _questions_45066628_cannot-2Drun-2Dmap-2Ddatastate&d=DwIBaQ&c=wluqKIiw
> > ffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2Ex
> > lYvnUWbsByY&m=sRhKIDOv4AuwrfDBU9b2Rpj-VPfs0lcpkQqNBfVLSe4&s=sEjkBzlaFR
> > 7NhUDpUPGPp1nOkAxOlBbFNadLowcDbFw&e=
> > and tried solutions but they do not seem to work?
> >
> > Here is a sample (MBR_SUBSCRIBERID(factor) is replaced with ID(integer) but of no consequence in my routine since not really used.
> >
> > I hope someone recognizes the problem.
> >
> > Thank you for any advice
> >
> > WHP
> >
> > > dput(sample)
> > structure(list(state = structure(c(2L, 22L, 10L, 12L, 6L, 22L, 11L,
> > 22L, 22L, 32L, 19L, 29L, 9L, 9L, 10L, 30L, 33L, 35L, 41L, 12L, 12L,
> > 36L, 4L, 9L, 8L, 39L, 36L, 36L, 41L, 28L, 12L, 28L, 28L, 12L, 11L, 2L,
> > 12L, 22L, 32L, 22L, 12L, 10L, 10L, 22L, 10L ), .Label = c("AL", "AR",
> > "AZ", "CA", "CO", "CT", "DC", "DE", "FL", "GA", "IA", "IL", "IN",
> > "KS", "KY", "LA", "MA", "MD", "ME", "MI", "MN", "MO", "NC", "NE",
> > "NJ", "NM", "NV", "NY", "OH", "OK", "OR", "PA", "SC", "SD", "TN",
> > "TX", "UT", "VA", "WA", "WI", "WV"
> > ), class = "factor"), city = structure(c(838L, 1030L, 262L, 218L,
> > 166L, 973L, 339L, 451L, 660L, 358L, 281L, 1280L, 129L, 223L, 989L,
> > 721L, 550L, 731L, 1325L, 688L, 1184L, 281L, 759L, 1171L, 1305L, 587L,
> > 272L, 581L, 263L, 152L, 217L, 152L, 390L, 5L, 571L, 1006L, 1162L,
> > 939L, 170L, 1033L, 1002L, 586L, 586L, 192L, 586L ), .Label =
> > c("ABBOTTSTOWN", "ABILENE", "ACWORTH", "ADAMS", "ADDISON", "ADKINS",
> > "ALBANY", "ALBIA", "ALBUQUERQUE", "ALEXANDRIA", "ALFRED", "ALIQUIPPA",
> > "ALISO VIEJO", "ALLEN PARK", "ALLENTOWN", "ALPHA", "ALPHARETTA",
> > "ALPINE", "ALTOONA", "AMARILLO", "AMBLER", "AMBRIDGE", "AMHERST",
> > "AMITYVILLE", "ANAHIEM", "ANDERSON", "ANDOVER", "ANGIER", "ANGLETON",
> > "ANKENY", "ANN ARBOR", "ANZA", "APEX", "APOLLO", "ARCADIA",
> > "ARCHDALE", "ARDMORE", "ARGYLE", "ARKANSAS CITY", "ARLINGTON",
> > "ARNOLD", "ARTHUR", "ASHEBORO", "ASHEVILLE", "ASHLAND", "ASHLAND
> > CITY", "ASHTON", "ASTON", "ATHENS", "ATLANTA", "ATLANTIC BCH",
> > "AUBORN", "AUGUSTA", "AURORA", "AUSTELL", "AUSTIN", "AVENTURA",
> > "AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN CITY", "BALDWIN PLACE",
> > "BALLWIN", "BANGOR", "BARBOURSVILLE", "BARNEGAT", "BARTLETT",
> > "BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", "BATESBURG", "BATON
> > ROUGE", "BAXTER SPRINGS", "BAY CITY", "BAYONNE", "BAYSIDE", "BAYTOWN",
> > "BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER CITY", "BELFAIR", "BELLE
> > VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR", "BELLMORE", "BELLWOOD",
> > "BELMAR", "BELMONT", "BELVIDERE", "BENNET", "BENTON", "BENTONVILLE",
> > "BERLIN", "BESSEMER CITY", "BETHANY", "BETHEL", "BETHEL PARK",
> > "BETHESDA", "BETHLEHEM", "BETTENDORF", "BIGLERVILLE", "BINGHAMTON",
> > "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN", "BLACKSBURG", "BLAINE",
> > "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD", "BLUE BELL", "BLUE GRASS",
> > "BLUE POINT", "BLUE SPRINGS", "BOCA RATON", "BODFISH", "BOILING
> > SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS", "BONNEY LAKE",
> > "BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING GREEN",
> > "BOYERTOWN", "BOYNTON BEACH", "BRADENTON", "BRADENVILLE",
> > "BRANDAMORE", "BRANDON", "BRANFORD", "BRANSON", "BRASELTON",
> > "BREEZEWOOD", "BRENTON", "BRENTWOOD", "BREWSTER", "BRICK",
> > "BRIDGEPORT", "BRIDGETON", "BRIGHAM CITY", "BROADDUS", "BROCKPORT",
> > "BROGUE", "BROKEN ARROW", "BRONX", "BROOKFIELD", "BROOKHAVEN",
> > "BROOKLYN", "BROWNS MILLS", "BROWNSBURG", "BROWNSVILLE", "BRUNSWICK",
> > "BRYAN", "BUCKSPORT", "BUDD LAKE", "BUFFALO", "BULVERDE",
> > "BUNKERVILLE", "BURBANK", "BURFORD", "BURIEN", "BURLINGTON", "BURR
> > RIDGE", "BURTON", "BUSKIRK", "BUTLER", "BUXTON", "BYRON", "CALERA",
> > "CAMBRIDGE", "CAMP HILL", "CAMPBELL", "CANAAN", "CANAL WINCHESTER",
> > "CANASTOTA", "CANTON", "CARDIFF BY THE SEA", "CARL JUNCTION",
> > "CARLISLE", "CARLTON", "CAROL STREAM", "CARROLLTON", "CARSON CITY",
> > "CARTERSVILLE", "CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE",
> > "CASWELL", "CATO", "CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER
> > POINT", "CENTERTON", "CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN
> > FALLS", "CHALFONT", "CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON",
> > "CHARLESTON", "CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY",
> > "CHESHIRE", "CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO",
> > "CHILLICOTHE", "CHOCTAW", "CICERO", "CINCINNATI", "CLAY",
> > "CLEARWATER", "CLEARWATER BEACH", "CLEMMONS", "CLENDENIN",
> > "CLEVELAND", "CLEVELAND HTS", "CLEVES", "CLIFFSIDE PARK", "CLIFTON",
> > "CLIFTON PARK", "CLIFTON SPGS", "CLINTON", "COACHELLA", "COAL
> > TOWNSHIP", "COATESVILLE", "COLLEGE STATION", "COLLINSVILLE",
> > "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE", "CONCORD", "CONCORD
> > TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY", "CONYERS",
> > "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA", "CORNELIUS",
> > "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY", "COTTAGE HILLS",
> > "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN", "CRANBERRY
> > TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER", "CUMBERLAND
> > FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS", "DALLAS",
> > "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT",
> > "DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR",
> > "DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH",
> > "DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY", "DES
> > MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER", "DIXON",
> > "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER", "DOYLESTOWN",
> > "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH", "DUNCAN", "DUNCANNON",
> > "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE", "DURHAM", "DUTTON", "EAGLE
> > GROVE", "EAST AURORA", "EAST CHICAGO", "EAST EARL", "EAST HANOVER",
> > "EAST HARTFORD", "EAST HARTLAND", "EAST LEROY", "EAST LIVERPOOL",
> > "EAST MEADOW", "EAST NORTHPORT", "EAST ORANGE", "EAST PEORIA", "EAST
> > SAINT LOUIS", "EASTON", "EDGERTON", "EDISON", "EDMOND", "EGG HBR TWP",
> > "EL DORADO", "EL PASO", "ELDRIDGE", "ELGIN", "ELIZABETHVILLE",
> > "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY", "ELMHURST", "ELMWOOD PARK",
> > "ELOY", "ELY", "EMERSON", "EMORY", "ENDICOTT", "ENGLEWOOD", "ENID",
> > "ENNICE", "ENOLA", "ENUMCLAW", "EPHRATA", "ERIE", "ESTERO", "EUREKA
> > SPRINGS", "EVANSVILLE", "EVERETT", "EXCELSIOR SPRINGS", "EXTON",
> > "FAIRFAX", "FAIRFIELD", "FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK",
> > "FALMOUTH", "FAR HILLS", "FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON",
> > "FAYETTE", "FAYETTEVILLE", "FELTON", "FENTON", "FERGUSON", "FIELDALE",
> > "FINDLAY", "FINGERVILLE", "FLEMINGTON", "FLINT", "FLORENCE",
> > "FLORESVILLE", "FLORISSANT", "FLOVILLA", "FLOWER MOUND", "FLUSHING",
> > "FOGELSVILLE", "FOLSOM", "FONTANELLE", "FOREST HILLS", "FOREST PARK",
> > "FORT DODGE", "FORT FAIRFIELD", "FORT GAY", "FORT LAUDERDALE", "FORT
> > LEE", "FORT MILL", "FORT MOHAVE", "FORT PAYNE", "FORT PIERCE", "FORT
> > SMITH", "FORT WAYNE", "FORT WORTH", "FOSTORIA", "FOUNTAIN INN",
> > "FRANKFORT", "FRANKLIN", "FREEDOM", "FREEHOLD", "FREEPORT", "FREMONT",
> > "FULLERTON", "FULSHEAR", "FULTON", "GADSDEN", "GAFFNEY",
> > "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE", "GARDNERS", "GARNETT
> > VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA", "GETTYSBURG",
> > "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE", "GLEN CARBON", "GLEN
> > COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW", "GLENWOOD", "GOLDEN
> > CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM", "GOSHEN",
> > "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS", "GRANDVIEW",
> > "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS", "GRASS VALLEY",
> > "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER", "GREENLAWN",
> > "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE", "GREER",
> > "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD", "HADLEY",
> > "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON", "HAMBURG",
> > "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE",
> > "HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON",
> > "HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL",
> > "HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE",
> > "HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH",
> > "HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE", "HIGH
> > HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS",
> > "HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE",
> > "HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD",
> > "HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS", "HOT
> > SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON", "HUFFMAN",
> > "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON", "HUNTINGTON
> > BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE", "HURST",
> > "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL",
> > "INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY", "IRMO",
> > "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP", "ISSAQUAH",
> > "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS", "JACKSONVILLE",
> > "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE", "JEFFERSON", "JEFFERSON
> > CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN", "JONESBORO", "JONESPORT",
> > "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE", "KANNAPOLIS", "KANSAS
> > CITY", "KATY", "KEARNEY", "KELLERTON", "KEMAH", "KENMORE", "KENNESAW",
> > "KENT", "KINGMAN", "KINGS MOUNTAIN", "KINGSLAND", "KINGSTON",
> > "KINGWOOD", "KIRKLAND", "KUTZTOWN", "LA QUINTA", "LA RUE", "LA VERNE",
> > "LAFAYETTE", "LAKE BLUFF", "LAKE IN THE HILL", "LAKE ISABELLA", "LAKE
> > PLACID", "LAKE SAINT LOUIS", "LAKE TAPPS", "LAKE VILLAGE", "LAKE
> > WAUKOMIS", "LAKE WORTH", "LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD
> > RCH", "LAMBERTVILLE", "LANCASTER", "LANDISVILLE", "LANSDALE",
> > "LAREDO", "LARGO", "LAS VEGAS", "LATROBE", "LAUDERHILL", "LAWNDALE",
> > "LAWRENCE", "LAWRENCEVILLE", "LE MARS", "LE ROY", "LEAGUE CITY",
> > "LEAVENWORTH", "LEAWOOD", "LEBANON", "LECOMPTON", "LEECHBURG", "LEES
> > SUMMIT", "LEESBURG", "LEESVILLE", "LEETONIA", "LENEXA", "LENOIR",
> > "LEVITTOWN", "LEWES", "LEWISTOWN", "LEXINGTON", "LIBERTY TWP",
> > "LILLINGTON", "LINCOLN", "LINDENHURST", "LINTON", "LISBON",
> > "LITCHFIELD PK", "LITHIA", "LITITZ", "LITTLE EGG HARBOR TO", "LK
> > FOREST PK", "LK PEEKSKILL", "LOCKHART", "LOCKWOOD", "LOGAN", "LOMA
> > LINDA", "LONE JACK", "LONG BEACH", "LONG LANE", "LONGVIEW", "LORAIN",
> > "LORTON", "LOS ANGELES", "LOUISBURG", "LOVES PARK", "LOWER BURRELL",
> > "LOWRY CITY", "LUBBOCK", "LUGOFF", "LUMBERTON", "LYNBROOK",
> > "LYNNWOOD", "MACHESNEY PARK", "MACON", "MAGNOLIA", "MAHOPAC",
> > "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE", "MANASSAS", "MANCHACA",
> > "MANCHESTER", "MANCHESTER TOWNSHIP", "MANCHESTER TW", "MANHEIM",
> > "MANITO", "MANLIUS", "MANNINGTON", "MANSFIELD", "MANSURA", "MAPLE
> > PARK", "MARBLE FALLS", "MARCELLUS", "MARICOPA", "MARIETTA", "MARION",
> > "MARKLEYSBURG", "MARS HILL", "MARSHALLTOWN", "MARSHFIELD",
> > "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE", "MASON CITY", "MASSEY",
> > "MASSILLON", "MASURY", "MATAWAN", "MATEWAN", "MATTAPOISETT",
> > "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND", "MC DONALD", "MC
> > KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN", "MCALESTER", "MCDONOUGH",
> > "MCKEESPORT", "MCKINNEY", "MECHANICSBURG", "MECHANICSVILLE",
> > "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS", "MERIDEN",
> > "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA", "MESQUITE",
> > "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN", "MIDLAND",
> > "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK", "MILLEDGEVILLE",
> > "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE", "MINEOLA", "MINERAL
> > SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION", "MISSION HILLS",
> > "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY", "MOLINE",
> > "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE", "MONTCLAIR",
> > "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON", "MORGANTOWN",
> > "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE", "MOUNT JOY",
> > "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO", "MOUNT UNION",
> > "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT", "MULVANE",
> > "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT",
> > "MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS",
> > "NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES",
> > "NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG", "NEW
> > BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK", "NEW
> > CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON", "NEW
> > LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE", "NEW
> > ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK",
> > "NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT",
> > "NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA",
> > "NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK",
> > "NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON",
> > "NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND
> > HILLS", "NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE",
> > "NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON", "OAK
> > RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE", "OMAHA",
> > "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION", "ORONO",
> > "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO", "OTTAWA",
> > "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD", "OYSTER BAY",
> > "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR", "PALMDALE", "PALMYRA",
> > "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE", "PARLIN", "PARMA",
> > "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY", "PAXTON", "PEA
> > RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN", "PELION",
> > "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE", "PENNSBURG",
> > "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS", "PERRYSBURG",
> > "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI", "PHOENIX",
> > "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN", "PINE BLUFF",
> > "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY", "PITMAN",
> > "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY",
> > "PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO
> > LAKE", "POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON",
> > "PORT CHARLOTTE", "PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE",
> > "PORT WASHINGTON", "PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND",
> > "POTTSVILLE", "POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE
> > VILLAGE", "PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT
> > PLEASANT", "PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG",
> > "QUINCY", "RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING",
> > "READLYN", "RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO
> > PARK", "REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC",
> > "RICH HILL", "RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD",
> > "RINCON", "RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD",
> > "RIVERSIDE", "ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL",
> > "ROCKAWAY BCH", "ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT",
> > "ROGERS", "ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE",
> > "ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT",
> > "ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN",
> > "RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO",
> > "SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR", "SAINT
> > CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS", "SAINT
> > PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY",
> > "SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO",
> > "SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN",
> > "SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA",
> > "SARASOTA", "SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH",
> > "SAYVILLE", "SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG",
> > "SCOTT", "SCOTTSDALE", "SEAFORD", "SEALE", "SEATTLE", "SEDONA",
> > "SEGUIN", "SELDEN", "SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR",
> > "SHARON", "SHARPSBURG", "SHARPSVILLE", "SHAVANO PARK", "SHAWNEE",
> > "SHELLSBURG", "SHELTON", "SHENANDOAH", "SHORELINE", "SHOREWOOD",
> > "SHREVEPORT", "SICKLERVILLE", "SILER CITY", "SILOAM SPRINGS", "SIMON",
> > "SIMPSONVILLE", "SIMSBURY", "SIOUX CITY", "SIOUX FALLS", "SKOKIE",
> > "SKOWHEGAN", "SLATINGTON", "SLIDELL", "SMITHFIELD", "SN BERNRDNO",
> > "SNELLVILLE", "SOCORRO", "SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH
> > BELOIT", "SOUTH BEND", "SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH
> > PARK", "SOUTH PLAINFIELD", "SOUTH SIOUX CITY", "SOUTHAMPTON",
> > "SOUTHBURY", "SOUTHGATE", "SPARKS", "SPARROW BUSH", "SPENCER",
> > "SPRING", "SPRING BRANCH", "SPRING GROVE", "SPRING HILL", "SPRING
> > HOPE", "SPRING VALLEY", "SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD
> > GARDENS", "SPRINGHILL", "SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE",
> > "ST PETERSBURG", "ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH",
> > "STANLEY", "STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING",
> > "STILLMAN VALLEY", "STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN",
> > "STRAFFORD", "STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS",
> > "SUFFIELD", "SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE",
> > "SUMMIT", "SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON",
> > "SWEENY", "SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA",
> > "TAFTVILLE", "TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON",
> > "TAYLORVILLE", "TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON",
> > "THOMASVILLE", "TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY
> > PARK", "TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER",
> > "TOPEKA", "TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS
> > RST", "TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER",
> > "TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY",
> > "UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA",
> > "URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO", "VAN
> > BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO",
> > "VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA",
> > "VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH", "W
> > TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN",
> > "WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG",
> > "WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA",
> > "WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE",
> > "WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER",
> > "WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO", "WEST
> > CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN", "WEST
> > HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH", "WEST
> > POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD", "WEST VALLEY
> > CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER", "WESTMORELAND
> > CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD", "WHARTON", "WHEELING",
> > "WHITE PLAINS", "WHITEHALL", "WHITESTONE", "WHITESTOWN", "WHITING",
> > "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD", "WILLARD",
> > "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK",
> > "WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS",
> > "WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON",
> > "WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE",
> > "WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS",
> > "WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH",
> > "YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS", "YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor"),
> >     clusters = c(6L, 10L, 3L, 10L, 5L, 6L, 10L, 6L, 6L, 7L, 5L,
> >     7L, 8L, 8L, 3L, 6L, 3L, 10L, 7L, 10L, 10L, 4L, 2L, 8L, 9L,
> >     1L, 4L, 4L, 7L, 5L, 10L, 5L, 5L, 10L, 6L, 6L, 10L, 6L, 7L,
> >     10L, 10L, 3L, 3L, 6L, 3L), Longitude = c(-93.883554, -90.582058,
> >     -83.868923, -89.544083, -72.902467, -94.458904, -90.588533,
> >     -94.527843, -92.93769, -80.029456, -70.58809, -82.888789,
> >     -80.113071, -82.6661, -81.372992, -95.840964, -82.016013,
> >     -89.92625, -80.276676, -89.778385, -87.641063, -94.888036,
> >     -117.663119, -82.530568, -75.574437, -122.209047, -96.799309,
> >     -95.746255, -80.553953, -73.923314, -87.713482, -73.9731,
> >     -73.8187, -87.987023, -93.759478, -94.110903, -90.00597,
> >     -93.257109, -79.932483, -90.201858, -89.100233, -84.625923,
> >     -84.567614, -93.912921, -84.612564), Latitude = c(34.503045,
> >     38.752515, 33.406527, 40.893588, 41.733088, 38.999032, 41.657674,
> >     38.890929, 37.639766, 42.099693, 43.53462, 40.142492, 26.538149,
> >     27.9062, 31.90104, 34.833083, 35.118836, 35.136047, 39.425712,
> >     40.519207, 40.101126, 30.044457, 33.62287, 27.910276, 39.752254,
> >     47.403778, 32.961929, 29.820199, 38.408649, 40.668828, 41.983948,
> >     40.58116, 40.7253, 41.921667, 41.669921, 36.442552, 38.564663,
> >     37.455315, 40.867774, 38.783554, 42.244381, 34.024332, 34.001644,
> >     36.622704, 34.017832), distances_Me = c(1879089.079, 1437296.523,
> >     1185638.742, 1313997.97, 156759.9125, 1759443.351, 1398398.997,
> >     1767861.917, 1668917.322, 534234.2576, 431749.2965, 758508.4701,
> >     1657031.938, 1615575.668, 1169085.702, 2024642.021, 930944.4681,
> >     1525322.746, 553069.7532, 1337002.822, 1161367.37, 2223496.376,
> >     3918520.212, 1609088.602, 164252.5146, 3878581.001, 2197910.688,
> >     2304526.789, 614908.3151, 11075.24597, 1160375.454, 494.2997496,
> >     21063.72622, 1182681.434, 1662351.873, 1810881.911, 1393581.436,
> >     1701982.013, 504395.772, 1404348.991, 1276505.454, 1190457.718,
> >     1188029.899, 1787261.336, 1189994.055), distances_Mi = c(1167.607468,
> >     893.0913246, 736.719012, 816.477441, 97.40573055, 1093.263337,
> >     868.9216123, 1098.494372, 1037.01328, 331.9565399, 268.2755749,
> >     471.3135552, 1029.628072, 1003.868436, 726.4334682, 1258.049536,
> >     578.4599174, 947.7880794, 343.6603307, 830.7719404, 721.6375366,
> >     1381.611443, 2434.846498, 999.8375755, 102.0614003, 2410.029515,
> >     1365.713293, 1431.96122, 382.0848884, 6.881813138, 721.0211909,
> >     0.30714248, 13.08834388, 734.8814328, 1032.933715, 1125.225657,
> >     865.9281298, 1057.55865, 313.4158337, 872.6187534, 793.1807589,
> >     739.7133739, 738.2048023, 1110.548567, 739.4252678), ID =
> > 2308:2352), row.names = c(NA, -45L), class = "data.frame")
> >
> > Proprietary
> >
> > NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may
> > con...{{dropped:16}}
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> > man_listinfo_r-2Dhelp&d=DwIBaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmv
> > OXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sRhKIDOv4AuwrfD
> > BU9b2Rpj-VPfs0lcpkQqNBfVLSe4&s=CloI44kowc5J_mOah6vx_Kl4PFIeqxmwexvRiqw
> > xuOA&e= PLEASE do read the posting guide
> > https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> > g_posting-2Dguide.html&d=DwIBaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmm
> > vOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sRhKIDOv4Auwrf
> > DBU9b2Rpj-VPfs0lcpkQqNBfVLSe4&s=iOWreiJDomXv-uJ3NzQNX0IUsupILo_4DmkFK3
> > GRUig&e= and provide commented, minimal, self-contained, reproducible
> > code.
>
> NOTICE TO RECIPIENT OF INFORMATION:
> This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.
> This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
> Thank you. Aetna


From Po||ngW @end|ng |rom @etn@@com  Thu May 14 15:13:07 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Thu, 14 May 2020 13:13:07 +0000
Subject: [R] Help with map()
In-Reply-To: <CA+8X3fW9dm+frfX3M_TE4S9Pi+cFnV23fn99c3Z-GoUvPZDjRQ@mail.gmail.com>
References: <BYAPR06MB5383688B7688345330422859AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CA+8X3fWwQ2HrBUxUW1Od5AeBegfmurkpz1Gu-wFhfAZ4dX9cmQ@mail.gmail.com>
 <BYAPR06MB538311FCA8A217B27743C85BAEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <BYAPR06MB53836AD813508C208C7D5B61AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <BYAPR06MB538355DBADC052E34DA1D3D8AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <BYAPR06MB5383ECACAEE69771084F4699AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CA+8X3fW9dm+frfX3M_TE4S9Pi+cFnV23fn99c3Z-GoUvPZDjRQ@mail.gmail.com>
Message-ID: <BYAPR06MB53837D662CE7E52191B98CB7AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>

Hi Jim, thanks for the advice, those modifications are outside my capabilities at the moment but I will play around with it I guess.

Thank you.

WHP



Proprietary

-----Original Message-----
From: Jim Lemon <drjimlemon at gmail.com> 
Sent: Thursday, May 14, 2020 7:27 AM
To: Poling, William <PolingW at aetna.com>; r-help mailing list <r-help at r-project.org>
Subject: [EXTERNAL] Re: Re: [R] Help with map()

**** External Email - Use Caution ****

Hi Bill,
I guess if you run makeDensityMatrix on subsets of the geographic locations and then overlay the different results with densityGrid using different colors, it should work as long as the regions don't overlap.

Jim

On Thu, May 14, 2020 at 10:20 PM Poling, William <PolingW at aetna.com> wrote:
>
> Hello Jim, et.al, again.
>
> One last question please.
>
> Since this routine works best with larger volume of geo points, is there a way to modify this routine to accommodate more than one cluster and differentiated by different colors?
>
> #Single cluster routine
> clus3 <- individual_dets_sf_3X %>%
>
>   dplyr::select(MBR_SUBSCRIBERID,state,city,clusters,Latitude,Longitude,distances_Mi) %>% #Notice now that these are capitalized <--"L"
>
>   filter(clusters == 3)
>
> str(clus3)
>
> clus3 <- as.data.frame(clus3)
>
> geomat<-makeDensityMatrix(clus3[,c("Latitude","Longitude")],
>                           
> xlim=range(clus3$Longitude),ylim=range(clus3$Latitude))
> # Range of density (>0) - 1.292155 3.89646
> latlim<-range(clus3$Latitude)
> lonlim<-range(clus3$Longitude)
>
> map("world",xlim=lonlim,ylim=latlim)
> axis(1)
> axis(2)
> densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
>             red=c(0.5,1),green=0,blue=0,pch=15)
> title("Member Geo Density Plot For Cluster3")
>
> knitr::kable(individual_dets_sf_3X %>% group_by(clusters) %>% 
> tally(sort = TRUE))
>
> #   | clusters|   n|
> #   |--------:|---:|
> #   |        3| 384|
> #   |        5| 335|
> #   |        9| 305|
> #   |        7| 298|
> #   |       10| 286|
> #   |        6| 279|
> #   |        4| 168|
> #   |        8| 131|
> #   |        2| 113|
> #   |        1|  53|
>
> 'data.frame':   384 obs. of  7 variables:
>  $ MBR_SUBSCRIBERID: Factor w/ 2352 levels "101040199600",..: 908 2272 1613 1357 1952 1437 795 1634 1160 902 ...
>  $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 10 23 23 10 33 10 10 23 10 10 ...
>  $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 156 496 354 827 670 1161 897 970 249 494 ...
>  $ clusters        : num  3 3 3 3 3 3 3 3 3 3 ...
>  $ Latitude        : num  31.2 35.3 36.5 33.4 34.2 ...
>  $ Longitude       : num  -81.5 -82.4 -81 -84.7 -80.7 ...
>  $ distances_Mi    : num  770 586 471 769 573 ...
>
>
>
>
>
> Proprietary
>
> -----Original Message-----
> From: Poling, William
> Sent: Thursday, May 14, 2020 7:03 AM
> To: Jim Lemon <drjimlemon at gmail.com>
> Cc: r-help at r-project.org; Mark Fowler <gmark.fowler at outlook.com>
> Subject: RE: [EXTERNAL] Re: [R] Help with map()
>
> Hi, it is working now using just the most necessary pkgs.
>
>  Evidently when there are so few rows (clus1=53) the map does not cooperate, I get the plot but no map image?
>
> As I increase records the map begins to appear as it did for Radius1-8 
> routines
>
> Thank you for your time and trouble my friends.
>
> WHP
>
>
>
>
>
> Proprietary
>
> -----Original Message-----
> From: Poling, William
> Sent: Thursday, May 14, 2020 6:46 AM
> To: Jim Lemon <drjimlemon at gmail.com>
> Cc: r-help at r-project.org; Mark Fowler <gmark.fowler at outlook.com>
> Subject: RE: [EXTERNAL] Re: [R] Help with map()
>
> Hi, I ran this which is partially successful. I got long & lat  (x&y) with red plot values however, no map behind it?
>
> Not sure what I might be missing now in terms of pkgs I suppose?
> #Just use the basic pkgs
> library(magrittr)#for %>% function
> library(plotrix)
> library(maps)
> library(dplyr)#For filter()
> str(individual_dets_sf_3X)
> str(radius3)
> str(clus1)
>
>
> #rm(clus1)
>
> clus1 <- individual_dets_sf_3X %>%
>
>   dplyr::select(MBR_SUBSCRIBERID,state,city,clusters,Latitude,Longitude,distances_Mi) %>% #Notice now that these are capitalized <--"L"
>
>   filter(clusters == 1)
>
> str(clus1)
>
> clus1 <- as.data.frame(clus1)
>
> geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
>                           
> xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))
> # Range of density (>0) - 1.292155 3.89646
> latlim<-range(clus1$Latitude)
> lonlim<-range(clus1$Longitude)
>
> map("world",xlim=lonlim,ylim=latlim)
> axis(1)
> axis(2)
> densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
>             red=c(0.5,1),green=0,blue=0,pch=15)
> title("Member Geo Density Plot For Cluster1")
>
>
>
>
>
> Proprietary
>
> -----Original Message-----
> From: Poling, William
> Sent: Thursday, May 14, 2020 6:29 AM
> To: Jim Lemon <drjimlemon at gmail.com>
> Cc: r-help at r-project.org; Mark Fowler <gmark.fowler at outlook.com>
> Subject: RE: [EXTERNAL] Re: [R] Help with map()
>
>
> Hi Jim, and Mark, thank you for your response.
>
> 1. I have restarted R
> 2. I have only initiated library(magrittr)#for %>% function & library(plotrix), no other libraries thinking that another may be overwriting something.
> 3. I have checked the str()
>
> str(radius3)
> 'data.frame':   1990 obs. of  6 variables:
>  $ MBR_SUBSCRIBERID: Factor w/ 2352 levels "101040199600",..: 590 908 976 509 1674 690 1336 726 1702 2331 ...
>  $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 32 10 25 11 9 32 13 12 12 17 ...
>  $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 932 156 230 698 965 1330 515 1127 1304 1316 ...
>  $ Latitude        : num  40.4 31.2 40.8 42.1 26.8 ...
>  $ Longitude       : num  -79.9 -81.5 -74 -91.6 -82.1 ...
>  $ distances_Mi    : num  310.3 769.9 16.1 920.4 1057.6 ...
>
> str(clus1)
> tibble [53 x 7] (S3: tbl_df/tbl/data.frame)  $ MBR_SUBSCRIBERID: Factor w/ 2352 levels "101040199600",..: 86 2111 995 899 953 924 769 92 790 1748 ...
>  $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 31 39 4 39 39 39 39 31 39 39 ...
>  $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 727 85 455 1018 1203 604 1169 727 984 295 ...
>  $ clusters        : num [1:53] 1 1 1 1 1 1 1 1 1 1 ...
>  $ Latitude        : num [1:53] 42.4 47.6 39.2 46.9 48 ...
>  $ Longitude       : num [1:53] -123 -122 -121 -123 -122 ...
>  $ distances_Mi    : num [1:53] 2499 2406 2472 2430 2408 ...
>
> 4. I change clus1 to DF
> 'data.frame':   53 obs. of  7 variables:
>  $ MBR_SUBSCRIBERID: Factor w/ 2352 levels "101040199600",..: 86 2111 995 899 953 924 769 92 790 1748 ...
>  $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 31 39 4 39 39 39 39 31 39 39 ...
>  $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 727 85 455 1018 1203 604 1169 727 984 295 ...
>  $ clusters        : num  1 1 1 1 1 1 1 1 1 1 ...
>  $ Latitude        : num  42.4 47.6 39.2 46.9 48 ...
>  $ Longitude       : num  -123 -122 -121 -123 -122 ...
>  $ distances_Mi    : num  2499 2406 2472 2430 2408 ...
>
> Then try again, no luck?
>
> Weird?
>
> Thanks
>
> WHP
>
> Proprietary
>
> -----Original Message-----
> From: Jim Lemon <drjimlemon at gmail.com>
> Sent: Thursday, May 14, 2020 5:44 AM
> To: Poling, William <PolingW at aetna.com>
> Cc: r-help at r-project.org
> Subject: [EXTERNAL] Re: [R] Help with map()
>
> **** External Email - Use Caution ****
>
> Hi Bill,
> Have you compared str(radius3) to str(clus1)? It may be quite different.
>
> Jim
>
> On Thu, May 14, 2020 at 8:24 PM Poling, William via R-help <r-help at r-project.org> wrote:
> >
> > #RStudio Version Version 1.2.1335
> > sessionInfo()
> > # R version 4.0.0 Patched (2020-05-03 r78349)
> > #Platform: x86_64-w64-mingw32/x64 (64-bit) #Running under: Windows 
> > 10
> > x64 (build 17763)
> >
> > Good morning.
> >
> > I ran routines like this one yesterday with no errors.
> >
> > #Radius3----
> > str(radius3)
> >
> > radius3 <- individual_dets_sf_3X %>%
> >
> >   dplyr::select(MBR_SUBSCRIBERID,state,city,Latitude,Longitude,distances_Mi) %>%
> >   filter(distances_Mi <= 1200)
> >
> > str(radius3)
> >
> >   geomat<-makeDensityMatrix(radius3[,c("Latitude","Longitude")],
> >
> > xlim=range(radius3$Longitude),ylim=range(radius3$Latitude))
> >
> > latlim<-range(radius3$Latitude)
> > lonlim<-range(radius3$Longitude)
> >
> > <- map("world",xlim=lonlim,ylim=latlim)
> > axis(1)
> > axis(2)
> > densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
> >             red=c(0.5,1),green=0,blue=0,pch=15)
> > title("Member Geo Density Plot For Radius3")
> >
> >
> > This morning I am trying slightly new routine and receiving this 
> > error
> >
> > clus1 <- individual_dets_sf_3X %>%
> >
> >   dplyr::select(MBR_SUBSCRIBERID,state,city,clusters,Latitude,Longitude,distances_Mi) %>%
> >   filter(clusters == 1)
> >
> > geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
> >
> > xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))
> >
> > latlim<-range(clus1$Latitude)
> > lonlim<-range(clus1$Longitude)
> >
> > map("world",xlim=lonlim,ylim=latlim)
> > axis(1)
> > axis(2)
> > densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
> >             red=c(0.5,1),green=0,blue=0,pch=15)
> > title("Member Geo Density Plot For Cluster1")
> >
> >
> > geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
> > +
> > + xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))
> >
> > > latlim<-range(clus1$Latitude)
> > > lonlim<-range(clus1$Longitude)
> > >
> > > map("world",xlim=lonlim,ylim=latlim)
> > Error in .C(C_map_type, as.character(mapbase), integer(1)) :
> >   Incorrect number of arguments (2), expecting 0 for ''
> > > axis(1)
> > Error in axis(1) : plot.new has not been called yet
> > > axis(2)
> > Error in axis(2) : plot.new has not been called yet
> > > densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
> > +             red=c(0.5,1),green=0,blue=0,pch=15)
> > Error in plot.xy(xy.coords(x, y), type = type, ...) :
> >   plot.new has not been called yet
> > > title("Member Geo Density Plot For Cluster1")
> > Error in title("Member Geo Density Plot For Cluster1") :
> >   plot.new has not been called yet
> >
> >
> > It seems to error at the point of 
> > map("world",xlim=lonlim,ylim=latlim)
> >
> > I find a ref in stack overflow
> > https://urldefense.proofpoint.com/v2/url?u=https-3A__stackoverflow.c
> > om 
> > _questions_45066628_cannot-2Drun-2Dmap-2Ddatastate&d=DwIBaQ&c=wluqKI
> > iw 
> > ffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2
> > Ex 
> > lYvnUWbsByY&m=sRhKIDOv4AuwrfDBU9b2Rpj-VPfs0lcpkQqNBfVLSe4&s=sEjkBzla
> > FR 7NhUDpUPGPp1nOkAxOlBbFNadLowcDbFw&e=
> > and tried solutions but they do not seem to work?
> >
> > Here is a sample (MBR_SUBSCRIBERID(factor) is replaced with ID(integer) but of no consequence in my routine since not really used.
> >
> > I hope someone recognizes the problem.
> >
> > Thank you for any advice
> >
> > WHP
> >
> > > dput(sample)
> > structure(list(state = structure(c(2L, 22L, 10L, 12L, 6L, 22L, 11L, 
> > 22L, 22L, 32L, 19L, 29L, 9L, 9L, 10L, 30L, 33L, 35L, 41L, 12L, 12L, 
> > 36L, 4L, 9L, 8L, 39L, 36L, 36L, 41L, 28L, 12L, 28L, 28L, 12L, 11L, 
> > 2L, 12L, 22L, 32L, 22L, 12L, 10L, 10L, 22L, 10L ), .Label = c("AL", 
> > "AR", "AZ", "CA", "CO", "CT", "DC", "DE", "FL", "GA", "IA", "IL", 
> > "IN", "KS", "KY", "LA", "MA", "MD", "ME", "MI", "MN", "MO", "NC", 
> > "NE", "NJ", "NM", "NV", "NY", "OH", "OK", "OR", "PA", "SC", "SD", 
> > "TN", "TX", "UT", "VA", "WA", "WI", "WV"
> > ), class = "factor"), city = structure(c(838L, 1030L, 262L, 218L, 
> > 166L, 973L, 339L, 451L, 660L, 358L, 281L, 1280L, 129L, 223L, 989L, 
> > 721L, 550L, 731L, 1325L, 688L, 1184L, 281L, 759L, 1171L, 1305L, 
> > 587L, 272L, 581L, 263L, 152L, 217L, 152L, 390L, 5L, 571L, 1006L, 
> > 1162L, 939L, 170L, 1033L, 1002L, 586L, 586L, 192L, 586L ), .Label = 
> > c("ABBOTTSTOWN", "ABILENE", "ACWORTH", "ADAMS", "ADDISON", "ADKINS", 
> > "ALBANY", "ALBIA", "ALBUQUERQUE", "ALEXANDRIA", "ALFRED", 
> > "ALIQUIPPA", "ALISO VIEJO", "ALLEN PARK", "ALLENTOWN", "ALPHA", 
> > "ALPHARETTA", "ALPINE", "ALTOONA", "AMARILLO", "AMBLER", "AMBRIDGE", 
> > "AMHERST", "AMITYVILLE", "ANAHIEM", "ANDERSON", "ANDOVER", "ANGIER", 
> > "ANGLETON", "ANKENY", "ANN ARBOR", "ANZA", "APEX", "APOLLO", 
> > "ARCADIA", "ARCHDALE", "ARDMORE", "ARGYLE", "ARKANSAS CITY", 
> > "ARLINGTON", "ARNOLD", "ARTHUR", "ASHEBORO", "ASHEVILLE", "ASHLAND", 
> > "ASHLAND CITY", "ASHTON", "ASTON", "ATHENS", "ATLANTA", "ATLANTIC 
> > BCH", "AUBORN", "AUGUSTA", "AURORA", "AUSTELL", "AUSTIN", 
> > "AVENTURA", "AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN CITY", 
> > "BALDWIN PLACE", "BALLWIN", "BANGOR", "BARBOURSVILLE", "BARNEGAT", 
> > "BARTLETT", "BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", 
> > "BATESBURG", "BATON ROUGE", "BAXTER SPRINGS", "BAY CITY", "BAYONNE", 
> > "BAYSIDE", "BAYTOWN", "BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER 
> > CITY", "BELFAIR", "BELLE VERNON", "BELLEAIR", "BELLEVUE", 
> > "BELLMAWR", "BELLMORE", "BELLWOOD", "BELMAR", "BELMONT", 
> > "BELVIDERE", "BENNET", "BENTON", "BENTONVILLE", "BERLIN", "BESSEMER 
> > CITY", "BETHANY", "BETHEL", "BETHEL PARK", "BETHESDA", "BETHLEHEM", 
> > "BETTENDORF", "BIGLERVILLE", "BINGHAMTON", "BIRDSBORO", "BIWABIK", 
> > "BLACK MOUNTAIN", "BLACKSBURG", "BLAINE", "BLAIRSVILLE", 
> > "BLAKESLEE", "BLOOMFIELD", "BLUE BELL", "BLUE GRASS", "BLUE POINT", 
> > "BLUE SPRINGS", "BOCA RATON", "BODFISH", "BOILING SPRINGS", 
> > "BOLIVAR", "BON AQUA", "BONNER SPRINGS", "BONNEY LAKE", 
> > "BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING GREEN", 
> > "BOYERTOWN", "BOYNTON BEACH", "BRADENTON", "BRADENVILLE", 
> > "BRANDAMORE", "BRANDON", "BRANFORD", "BRANSON", "BRASELTON", 
> > "BREEZEWOOD", "BRENTON", "BRENTWOOD", "BREWSTER", "BRICK", 
> > "BRIDGEPORT", "BRIDGETON", "BRIGHAM CITY", "BROADDUS", "BROCKPORT", 
> > "BROGUE", "BROKEN ARROW", "BRONX", "BROOKFIELD", "BROOKHAVEN", 
> > "BROOKLYN", "BROWNS MILLS", "BROWNSBURG", "BROWNSVILLE", 
> > "BRUNSWICK", "BRYAN", "BUCKSPORT", "BUDD LAKE", "BUFFALO", 
> > "BULVERDE", "BUNKERVILLE", "BURBANK", "BURFORD", "BURIEN", 
> > "BURLINGTON", "BURR RIDGE", "BURTON", "BUSKIRK", "BUTLER", "BUXTON", 
> > "BYRON", "CALERA", "CAMBRIDGE", "CAMP HILL", "CAMPBELL", "CANAAN", 
> > "CANAL WINCHESTER", "CANASTOTA", "CANTON", "CARDIFF BY THE SEA", 
> > "CARL JUNCTION", "CARLISLE", "CARLTON", "CAROL STREAM", 
> > "CARROLLTON", "CARSON CITY", "CARTERSVILLE", "CARTHAGE", "CARY", 
> > "CASEYVILLE", "CASSVILLE", "CASWELL", "CATO", "CEDAR RAPIDS", 
> > "CEDARBURG", "CEDARVILLE", "CENTER POINT", "CENTERTON", "CENTRAL 
> > ISLIP", "CENTRAL POINT", "CHAGRIN FALLS", "CHALFONT", "CHAPEL HILL", 
> > "CHAPIN", "CHARDON", "CHARITON", "CHARLESTON", "CHARLOTTE", 
> > "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", "CHESHIRE", "CHEST 
> > SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", "CHILLICOTHE", 
> > "CHOCTAW", "CICERO", "CINCINNATI", "CLAY", "CLEARWATER", "CLEARWATER 
> > BEACH", "CLEMMONS", "CLENDENIN", "CLEVELAND", "CLEVELAND HTS", 
> > "CLEVES", "CLIFFSIDE PARK", "CLIFTON", "CLIFTON PARK", "CLIFTON 
> > SPGS", "CLINTON", "COACHELLA", "COAL TOWNSHIP", "COATESVILLE", 
> > "COLLEGE STATION", "COLLINSVILLE", "COLONIA", "COLUMBIA", 
> > "COLUMBUS", "COMMERCE", "CONCORD", "CONCORD TOWNSHIP", 
> > "CONNELLSVILLE", "CONROE", "CONWAY", "CONYERS", "COOKEVILLE", "COON 
> > RAPIDS", "CORAOPOLIS", "CORDOVA", "CORNELIUS", "CORNWALL", 
> > "CORONADO", "CORPUS CHRISTI", "CORRY", "COTTAGE HILLS", 
> > "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN", "CRANBERRY 
> > TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER", "CUMBERLAND 
> > FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS", "DALLAS", 
> > "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT", 
> > "DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR", 
> > "DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH", 
> > "DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY", "DES 
> > MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER", "DIXON", 
> > "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER", "DOYLESTOWN", 
> > "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH", "DUNCAN", 
> > "DUNCANNON", "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE", "DURHAM", 
> > "DUTTON", "EAGLE GROVE", "EAST AURORA", "EAST CHICAGO", "EAST EARL", 
> > "EAST HANOVER", "EAST HARTFORD", "EAST HARTLAND", "EAST LEROY", 
> > "EAST LIVERPOOL", "EAST MEADOW", "EAST NORTHPORT", "EAST ORANGE", 
> > "EAST PEORIA", "EAST SAINT LOUIS", "EASTON", "EDGERTON", "EDISON", 
> > "EDMOND", "EGG HBR TWP", "EL DORADO", "EL PASO", "ELDRIDGE", 
> > "ELGIN", "ELIZABETHVILLE", "ELLISVILLE", "ELLSWORTH", "ELLWOOD 
> > CITY", "ELMHURST", "ELMWOOD PARK", "ELOY", "ELY", "EMERSON", 
> > "EMORY", "ENDICOTT", "ENGLEWOOD", "ENID", "ENNICE", "ENOLA", 
> > "ENUMCLAW", "EPHRATA", "ERIE", "ESTERO", "EUREKA SPRINGS", 
> > "EVANSVILLE", "EVERETT", "EXCELSIOR SPRINGS", "EXTON", "FAIRFAX", 
> > "FAIRFIELD", "FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", "FALMOUTH", 
> > "FAR HILLS", "FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", "FAYETTE", 
> > "FAYETTEVILLE", "FELTON", "FENTON", "FERGUSON", "FIELDALE", 
> > "FINDLAY", "FINGERVILLE", "FLEMINGTON", "FLINT", "FLORENCE", 
> > "FLORESVILLE", "FLORISSANT", "FLOVILLA", "FLOWER MOUND", "FLUSHING", 
> > "FOGELSVILLE", "FOLSOM", "FONTANELLE", "FOREST HILLS", "FOREST 
> > PARK", "FORT DODGE", "FORT FAIRFIELD", "FORT GAY", "FORT 
> > LAUDERDALE", "FORT LEE", "FORT MILL", "FORT MOHAVE", "FORT PAYNE", 
> > "FORT PIERCE", "FORT SMITH", "FORT WAYNE", "FORT WORTH", "FOSTORIA", 
> > "FOUNTAIN INN", "FRANKFORT", "FRANKLIN", "FREEDOM", "FREEHOLD", 
> > "FREEPORT", "FREMONT", "FULLERTON", "FULSHEAR", "FULTON", "GADSDEN", 
> > "GAFFNEY", "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE", "GARDNERS", 
> > "GARNETT VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA", 
> > "GETTYSBURG", "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE", "GLEN 
> > CARBON", "GLEN COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW", 
> > "GLENWOOD", "GOLDEN CITY", "GOLIAD", "GOODSON", "GOODYEAR", 
> > "GORHAM", "GOSHEN", "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND 
> > RAPIDS", "GRANDVIEW", "GRANITE CITY", "GRANITE FALLS", "GRANTS 
> > PASS", "GRASS VALLEY", "GRAVETTE", "GRAYSVILLE", "GREEN LANE", 
> > "GREENBRIER", "GREENLAWN", "GREENSBORO", "GREENSBURG", "GREENUP", 
> > "GREENVILLE", "GREER", "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", 
> > "GUILFORD", "HADLEY", "HAGAN", "HALF WAY", "HALLANDALE BEACH", 
> > "HAMBLETON", "HAMBURG", "HAMDEN", "HAMILTON", "HAMMOND", 
> > "HAMMONTON", "HAMPSHIRE", "HAPEVILLE", "HARBORCREEK", "HARLAN", 
> > "HARRISBURG", "HARRISON", "HARRISONVILLE", "HARTWELL", "HASTINGS", 
> > "HATFIELD", "HAVERHILL", "HAWK POINT", "HAYESVILLE", "HELEN", 
> > "HENDERSON", "HENDERSONVILLE", "HENRICO", "HENRIETTA", "HERMITAGE", 
> > "HERNDON", "HERSHEY", "HIALEAH", "HIAWATHA", "HIBBING", "HICKORY", 
> > "HICKORY GROVE", "HIDDENITE", "HIGH HILL", "HIGH POINT", "HIGH 
> > RIDGE", "HIGHLAND", "HIGHLAND MILLS", "HINCKLEY", "HIRAM", "HOBART", 
> > "HOLBROOK", "HOLDEN", "HOLDENVILLE", "HOLLADAY", "HOLLIS", "HOLLY 
> > SPRINGS", "HOLLYWOOD", "HOLTWOOD", "HOMER", "HOMESTEAD", 
> > "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS", "HOT SPRINGS NATIONAL 
> > PARK", "HOUSTON", "HUBBARD", "HUDSON", "HUFFMAN", "HULL", 
> > "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON", "HUNTINGTON BEACH", 
> > "HUNTS POINT", "HURDLE MILLS", "HURRICANE", "HURST", "IMPERIAL", 
> > "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL", "INDIANA", 
> > "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY", "IRMO", 
> > "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP", "ISSAQUAH", 
> > "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS", "JACKSONVILLE", 
> > "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE", "JEFFERSON", 
> > "JEFFERSON CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN", "JONESBORO", 
> > "JONESPORT", "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE", 
> > "KANNAPOLIS", "KANSAS CITY", "KATY", "KEARNEY", "KELLERTON", 
> > "KEMAH", "KENMORE", "KENNESAW", "KENT", "KINGMAN", "KINGS MOUNTAIN", 
> > "KINGSLAND", "KINGSTON", "KINGWOOD", "KIRKLAND", "KUTZTOWN", "LA 
> > QUINTA", "LA RUE", "LA VERNE", "LAFAYETTE", "LAKE BLUFF", "LAKE IN 
> > THE HILL", "LAKE ISABELLA", "LAKE PLACID", "LAKE SAINT LOUIS", "LAKE 
> > TAPPS", "LAKE VILLAGE", "LAKE WAUKOMIS", "LAKE WORTH", "LAKEBAY", 
> > "LAKELAND", "LAKEWOOD", "LAKEWOOD RCH", "LAMBERTVILLE", "LANCASTER", 
> > "LANDISVILLE", "LANSDALE", "LAREDO", "LARGO", "LAS VEGAS", 
> > "LATROBE", "LAUDERHILL", "LAWNDALE", "LAWRENCE", "LAWRENCEVILLE", 
> > "LE MARS", "LE ROY", "LEAGUE CITY", "LEAVENWORTH", "LEAWOOD", 
> > "LEBANON", "LECOMPTON", "LEECHBURG", "LEES SUMMIT", "LEESBURG", 
> > "LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", "LEVITTOWN", "LEWES", 
> > "LEWISTOWN", "LEXINGTON", "LIBERTY TWP", "LILLINGTON", "LINCOLN", 
> > "LINDENHURST", "LINTON", "LISBON", "LITCHFIELD PK", "LITHIA", 
> > "LITITZ", "LITTLE EGG HARBOR TO", "LK FOREST PK", "LK PEEKSKILL", 
> > "LOCKHART", "LOCKWOOD", "LOGAN", "LOMA LINDA", "LONE JACK", "LONG 
> > BEACH", "LONG LANE", "LONGVIEW", "LORAIN", "LORTON", "LOS ANGELES", 
> > "LOUISBURG", "LOVES PARK", "LOWER BURRELL", "LOWRY CITY", "LUBBOCK", 
> > "LUGOFF", "LUMBERTON", "LYNBROOK", "LYNNWOOD", "MACHESNEY PARK", 
> > "MACON", "MAGNOLIA", "MAHOPAC", "MAHWAH", "MAINEVILLE", "MALVERN", 
> > "MALVERNE", "MANASSAS", "MANCHACA", "MANCHESTER", "MANCHESTER 
> > TOWNSHIP", "MANCHESTER TW", "MANHEIM", "MANITO", "MANLIUS", 
> > "MANNINGTON", "MANSFIELD", "MANSURA", "MAPLE PARK", "MARBLE FALLS", 
> > "MARCELLUS", "MARICOPA", "MARIETTA", "MARION", "MARKLEYSBURG", "MARS 
> > HILL", "MARSHALLTOWN", "MARSHFIELD", "MARSHVILLE", "MARTINSVILLE", 
> > "MARYSVILLE", "MASON CITY", "MASSEY", "MASSILLON", "MASURY", 
> > "MATAWAN", "MATEWAN", "MATTAPOISETT", "MATTHEWS", "MAYFIELD HTS", 
> > "MAYSEL", "MC CLELLAND", "MC DONALD", "MC KEES ROCKS", "MC 
> > SHERRYSTOWN", "MC VEYTOWN", "MCALESTER", "MCDONOUGH", "MCKEESPORT", 
> > "MCKINNEY", "MECHANICSBURG", "MECHANICSVILLE", "MEDFORD", "MEDIA", 
> > "MEDWAY", "MELVILLE", "MEMPHIS", "MERIDEN", "MERRIAM", "MERRICK", 
> > "MERRILL", "MERRILLVILLE", "MESA", "MESQUITE", "MIAMI", "MIAMI 
> > GARDENS", "MICHIGAN CITY", "MIDDLETOWN", "MIDLAND", "MIFFLINTOWN", 
> > "MILFORD", "MILL CREEK", "MILLBROOK", "MILLEDGEVILLE", 
> > "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE", "MINEOLA", "MINERAL 
> > SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION", "MISSION HILLS", 
> > "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY", "MOLINE", 
> > "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE", "MONTCLAIR", 
> > "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON", "MORGANTOWN", 
> > "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE", "MOUNT JOY", 
> > "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO", "MOUNT UNION", 
> > "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT", "MULVANE", 
> > "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT", 
> > "MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS", 
> > "NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES", 
> > "NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG", "NEW 
> > BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK", "NEW 
> > CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON", "NEW 
> > LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE", "NEW 
> > ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK", 
> > "NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT", 
> > "NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA", 
> > "NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY 
> > PARK", "NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH 
> > JACKSON", "NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH 
> > RICHLAND HILLS", "NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", 
> > "NORTHRIDGE", "NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", 
> > "O FALLON", "OAK RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", 
> > "OLATHE", "OMAHA", "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", 
> > "ORION", "ORONO", "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", 
> > "OSWEGO", "OTTAWA", "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", 
> > "OXFORD", "OYSTER BAY", "OZARK", "PALERMO", "PALM CITY", "PALM 
> > HARBOR", "PALMDALE", "PALMYRA", "PALOS HEIGHTS", "PANA", "PAOLA", 
> > "PARK RIDGE", "PARLIN", "PARMA", "PARRISH", "PARROTT", "PATASKALA", 
> > "PAULS VALLEY", "PAXTON", "PEA RIDGE", "PEARLAND", "PECULIAR", 
> > "PEEKSKILL", "PEKIN", "PELION", "PEMBROKE", "PEMBROKE PINES", "PENN 
> > YAN", "PENNELLVILLE", "PENNSBURG", "PENNSVILLE", "PEORIA", "PERRY", 
> > "PERRYOPOLIS", "PERRYSBURG", "PERRYVILLE", "PHARR", "PHILADELPHIA", 
> > "PHILIPPI", "PHOENIX", "PICKENS", "PICKERINGTON", "PIERSON", "PILOT 
> > MOUNTAIN", "PINE BLUFF", "PINE HILL", "PINEVILLE", "PINNELLAS PARK", 
> > "PISCATAWAY", "PITMAN", "PITTSBURGH", "PLACIDA", "PLAINFIELD", 
> > "PLANO", "PLANT CITY", "PLATTEKILL", "PLEASANT HILL", "PLEASANT 
> > HOPE", "PLYMOUTH", "POCONO LAKE", "POMFRET CENTER", "POMPANO BEACH", 
> > "POOLER", "PORT CARBON", "PORT CHARLOTTE", "PORT NORRIS", "PORT 
> > ORANGE", "PORT SAINT LUCIE", "PORT WASHINGTON", "PORTAGE", "PORTER", 
> > "PORTERSVILLE", "PORTLAND", "POTTSVILLE", "POUGHKEEPSIE", "POWDER 
> > SPRINGS", "POWELL", "PRAIRIE VILLAGE", "PRESCOTT", "PRESCOTT 
> > VALLEY", "PRINCETON", "PROSPECT", "PT PLEASANT", "PUNTA GORDA", 
> > "PURCHASE", "QUAKERTOWN", "QUEENS VLG", "QUINCY", "RALEIGH", 
> > "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", "READLYN", "RED 
> > BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO PARK", "REHOBOTH 
> > BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", "RICH HILL", 
> > "RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", "RINCON", "RIO 
> > GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", "RIVERSIDE", 
> > "ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", "ROCKAWAY BCH", 
> > "ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", "ROGERS", 
> > "ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE", 
> > "ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT", 
> > "ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN", 
> > "RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO", 
> > "SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR", "SAINT 
> > CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS", "SAINT 
> > PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY", 
> > "SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN 
> > ANTONIO", "SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", 
> > "SAN JUAN", "SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA 
> > ANA", "SARASOTA", "SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", 
> > "SAVANNAH", "SAYVILLE", "SCALY MOUNTAIN", "SCHAEFFERSTOWN", 
> > "SCHULENBURG", "SCOTT", "SCOTTSDALE", "SEAFORD", "SEALE", "SEATTLE", 
> > "SEDONA", "SEGUIN", "SELDEN", "SENECA", "SENOIA", "SEVEN VALLEYS", 
> > "SEYMOUR", "SHARON", "SHARPSBURG", "SHARPSVILLE", "SHAVANO PARK", 
> > "SHAWNEE", "SHELLSBURG", "SHELTON", "SHENANDOAH", "SHORELINE", 
> > "SHOREWOOD", "SHREVEPORT", "SICKLERVILLE", "SILER CITY", "SILOAM 
> > SPRINGS", "SIMON", "SIMPSONVILLE", "SIMSBURY", "SIOUX CITY", "SIOUX 
> > FALLS", "SKOKIE", "SKOWHEGAN", "SLATINGTON", "SLIDELL", 
> > "SMITHFIELD", "SN BERNRDNO", "SNELLVILLE", "SOCORRO", "SOMERS", 
> > "SOPHIA", "SOUTH AMBOY", "SOUTH BELOIT", "SOUTH BEND", "SOUTH 
> > DARTMOUTH", "SOUTH HOUSTON", "SOUTH PARK", "SOUTH PLAINFIELD", 
> > "SOUTH SIOUX CITY", "SOUTHAMPTON", "SOUTHBURY", "SOUTHGATE", 
> > "SPARKS", "SPARROW BUSH", "SPENCER", "SPRING", "SPRING BRANCH", 
> > "SPRING GROVE", "SPRING HILL", "SPRING HOPE", "SPRING VALLEY", 
> > "SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD GARDENS", "SPRINGHILL", 
> > "SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", "ST PETERSBURG", "ST. 
> > GEORGE", "STAFFORD", "STAMFORD", "STANDISH", "STANLEY", "STANTON", 
> > "STATEN ISLAND", "STATESVILLE", "STERLING", "STILLMAN VALLEY", 
> > "STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", "STRAFFORD", 
> > "STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", "SUFFIELD", 
> > "SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", "SUMMIT", "SUN 
> > CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", "SWEENY", 
> > "SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", 
> > "TAFTVILLE", "TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", 
> > "TAYLORVILLE", "TECUMSEH", "THE WOODLANDS", "THEODOSIA", 
> > "THOMASTON", "THOMASVILLE", "TIERRA VERDE", "TIFTON", "TILLSON", 
> > "TILTON", "TINLEY PARK", "TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", 
> > "TOMS RIVER", "TOPEKA", "TOPSHAM", "TORRINGTON", "TOVEY", 
> > "TOWNSEND", "TRAVELERS RST", "TRENT", "TRINITY", "TROUTMAN", "TROY", 
> > "TRUMBULL", "TUCKER", "TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", 
> > "UNION", "UNION CITY", "UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR 
> > CHICHSTR", "URBANA", "URBANDALE", "UTICA", "VAIL", "VALE", 
> > "VALENCIA", "VALPARAISO", "VAN BUREN", "VAN METER", "VAN WERT", 
> > "VANDERGRIFT", "VASSALBORO", "VAUGHN", "VENICE", "VERNAL", "VERNON", 
> > "VERONA", "VICTOR", "VIDALIA", "VIENNA", "VILLA GROVE", "VILLA 
> > RIDGE", "VOLANT", "W HAMPTON BCH", "W TERRE HAUTE", "WADSWORTH", 
> > "WAHOO", "WAKE FOREST", "WALDEN", "WALLINGFORD", "WALLINGTON", 
> > "WALNUT COVE", "WANAQUE", "WARFORDSBURG", "WARMINSTER", "WARREN", 
> > "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA", "WATAUGA", "WATERBURY", 
> > "WATERFORD", "WATERLOO", "WATERVILLE", "WATKINS GLEN", "WAXAHACHIE", 
> > "WAYCROSS", "WAYNESBORO", "WEBSTER", "WEIRTON", "WELLSBORO", 
> > "WELLSVILLE", "WENTZVILLE", "WESLACO", "WEST CHESTER", "WEST 
> > DEPTFORD", "WEST DES MOINES", "WEST HAVEN", "WEST HICKORY", "WEST 
> > LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH", "WEST POINT", "WEST 
> > READING", "WEST SENECA", "WEST SUFFIELD", "WEST VALLEY CITY", 
> > "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER", "WESTMORELAND CITY", 
> > "WESTPORT", "WESTVILLE", "WETHERSFIELD", "WHARTON", "WHEELING", 
> > "WHITE PLAINS", "WHITEHALL", "WHITESTONE", "WHITESTOWN", "WHITING", "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD", "WILLARD", "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK", "WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS", "WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON", "WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE", "WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS", "WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH", "YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS", "YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor"),
> >     clusters = c(6L, 10L, 3L, 10L, 5L, 6L, 10L, 6L, 6L, 7L, 5L,
> >     7L, 8L, 8L, 3L, 6L, 3L, 10L, 7L, 10L, 10L, 4L, 2L, 8L, 9L,
> >     1L, 4L, 4L, 7L, 5L, 10L, 5L, 5L, 10L, 6L, 6L, 10L, 6L, 7L,
> >     10L, 10L, 3L, 3L, 6L, 3L), Longitude = c(-93.883554, -90.582058,
> >     -83.868923, -89.544083, -72.902467, -94.458904, -90.588533,
> >     -94.527843, -92.93769, -80.029456, -70.58809, -82.888789,
> >     -80.113071, -82.6661, -81.372992, -95.840964, -82.016013,
> >     -89.92625, -80.276676, -89.778385, -87.641063, -94.888036,
> >     -117.663119, -82.530568, -75.574437, -122.209047, -96.799309,
> >     -95.746255, -80.553953, -73.923314, -87.713482, -73.9731,
> >     -73.8187, -87.987023, -93.759478, -94.110903, -90.00597,
> >     -93.257109, -79.932483, -90.201858, -89.100233, -84.625923,
> >     -84.567614, -93.912921, -84.612564), Latitude = c(34.503045,
> >     38.752515, 33.406527, 40.893588, 41.733088, 38.999032, 41.657674,
> >     38.890929, 37.639766, 42.099693, 43.53462, 40.142492, 26.538149,
> >     27.9062, 31.90104, 34.833083, 35.118836, 35.136047, 39.425712,
> >     40.519207, 40.101126, 30.044457, 33.62287, 27.910276, 39.752254,
> >     47.403778, 32.961929, 29.820199, 38.408649, 40.668828, 41.983948,
> >     40.58116, 40.7253, 41.921667, 41.669921, 36.442552, 38.564663,
> >     37.455315, 40.867774, 38.783554, 42.244381, 34.024332, 34.001644,
> >     36.622704, 34.017832), distances_Me = c(1879089.079, 1437296.523,
> >     1185638.742, 1313997.97, 156759.9125, 1759443.351, 1398398.997,
> >     1767861.917, 1668917.322, 534234.2576, 431749.2965, 758508.4701,
> >     1657031.938, 1615575.668, 1169085.702, 2024642.021, 930944.4681,
> >     1525322.746, 553069.7532, 1337002.822, 1161367.37, 2223496.376,
> >     3918520.212, 1609088.602, 164252.5146, 3878581.001, 2197910.688,
> >     2304526.789, 614908.3151, 11075.24597, 1160375.454, 494.2997496,
> >     21063.72622, 1182681.434, 1662351.873, 1810881.911, 1393581.436,
> >     1701982.013, 504395.772, 1404348.991, 1276505.454, 1190457.718,
> >     1188029.899, 1787261.336, 1189994.055), distances_Mi = c(1167.607468,
> >     893.0913246, 736.719012, 816.477441, 97.40573055, 1093.263337,
> >     868.9216123, 1098.494372, 1037.01328, 331.9565399, 268.2755749,
> >     471.3135552, 1029.628072, 1003.868436, 726.4334682, 1258.049536,
> >     578.4599174, 947.7880794, 343.6603307, 830.7719404, 721.6375366,
> >     1381.611443, 2434.846498, 999.8375755, 102.0614003, 2410.029515,
> >     1365.713293, 1431.96122, 382.0848884, 6.881813138, 721.0211909,
> >     0.30714248, 13.08834388, 734.8814328, 1032.933715, 1125.225657,
> >     865.9281298, 1057.55865, 313.4158337, 872.6187534, 793.1807589,
> >     739.7133739, 738.2048023, 1110.548567, 739.4252678), ID = 
> > 2308:2352), row.names = c(NA, -45L), class = "data.frame")
> >
> > Proprietary
> >
> > NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may 
> > con...{{dropped:16}}
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> > https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_ma
> > il 
> > man_listinfo_r-2Dhelp&d=DwIBaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJm
> > mv 
> > OXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sRhKIDOv4Auwr
> > fD 
> > BU9b2Rpj-VPfs0lcpkQqNBfVLSe4&s=CloI44kowc5J_mOah6vx_Kl4PFIeqxmwexvRi
> > qw xuOA&e= PLEASE do read the posting guide 
> > https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.
> > or 
> > g_posting-2Dguide.html&d=DwIBaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJ
> > mm 
> > vOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sRhKIDOv4Auw
> > rf
> > DBU9b2Rpj-VPfs0lcpkQqNBfVLSe4&s=iOWreiJDomXv-uJ3NzQNX0IUsupILo_4DmkF
> > K3 GRUig&e= and provide commented, minimal, self-contained, 
> > reproducible code.
>
> NOTICE TO RECIPIENT OF INFORMATION:
> This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.
> This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
> Thank you. Aetna

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From m@rong|u@|u|g| @end|ng |rom gm@||@com  Thu May 14 17:12:08 2020
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Thu, 14 May 2020 17:12:08 +0200
Subject: [R] How to find a split point in a curve?
Message-ID: <CAMk+s2TvXxGHY_LPQQRNSXkiqdDgQW2iuObOQbeVGOVzYArzZQ@mail.gmail.com>

Dear all,
I am trying to find a turning point in some data. In the initial phase, the
data increases more or less exponentially (thus it is linear in a nat log
transform), then reaches a plateau. I would like to find the point that
marks the end of the exponential phase.
I understand that the function spline can build a curve; is it possible
with it to find the turning points? I have no idea of how to use spline
though.
Here is a working example.
Thank you

```
Y = c(259, 716, 1404, 2173, 3944, 5403, 7140, 9121,
      11220, 13809, 16634, 19869, 23753, 27447,
      30590, 33975, 36627, 39600, 42067, 44082,
      58190, 63280, 65921, 67929, 69977, 71865,
      73614, 74005, 74894, 75717, 76365, 76579,
      77087, 77493, 77926, 78253, 78680, 79253,
      79455, 79580, 79699, 79838, 79981, 80080,
      80124, 80164, 80183, 80207, 80222, 80230,
      80241, 80261, 80261, 80277, 80290, 80303,
      80337, 80376, 80422, 80461, 80539, 80586,
      80653, 80708, 80762, 80807, 80807, 80886,
      80922, 80957, 80988, 81007, 81037, 81076,
      81108, 81108, 81171, 81213, 81259, 81358,
      81466, 81555, 81601, 81647, 81673, 81998,
      82025, 82041, 82053, 82064, 82094, 82104,
      82110, 82122, 82133, 82136, 82142, 82164,
      82168, 82180, 82181, 82184, 82187, 82188,
      82190, 82192, 82193, 82194)
Y = log(Y)
X = 1:length(Y)
plot(Y ~ X, ylab = "Ln(Y)", xlim=c(0,10, main="zoomed in"))
abline(lm(Y[1:3] ~ X[1:3]))
abline(lm(Y[1:5] ~ X[1:5]), lty=2)
text(7, 6, "After third or fifth point, there is deviance", pos=3)
text(2.5, 10, "Solid line: linear model points 1:3", pos =3)
text(2.5, 9, "Dashed line: linear model points 1:5", pos =3)
plot(Y ~ X, ylab = "Ln(Y)", xlim=c(0,10, main="overall"))
abline(lm(Y[1:3] ~ X[1:3]))
```

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu May 14 17:21:18 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 14 May 2020 16:21:18 +0100
Subject: [R] How to find a split point in a curve?
In-Reply-To: <CAMk+s2TvXxGHY_LPQQRNSXkiqdDgQW2iuObOQbeVGOVzYArzZQ@mail.gmail.com>
References: <CAMk+s2TvXxGHY_LPQQRNSXkiqdDgQW2iuObOQbeVGOVzYArzZQ@mail.gmail.com>
Message-ID: <800713b6-4363-7c2c-9097-d2fad3b57c54@sapo.pt>

Hello,

Are you looking for a segmented regression?

fit <- lm(Y ~ X)
seg <- segmented::segmented(fit, seg.Z = ~X)
seg$psi[, 'Est.']
#[1] 29.21595

plot(X, Y)
plot(seg, add = TRUE)


Hope this helps,

Rui Barradas


?s 16:12 de 14/05/20, Luigi Marongiu escreveu:
> Dear all,
> I am trying to find a turning point in some data. In the initial phase, the
> data increases more or less exponentially (thus it is linear in a nat log
> transform), then reaches a plateau. I would like to find the point that
> marks the end of the exponential phase.
> I understand that the function spline can build a curve; is it possible
> with it to find the turning points? I have no idea of how to use spline
> though.
> Here is a working example.
> Thank you
> 
> ```
> Y = c(259, 716, 1404, 2173, 3944, 5403, 7140, 9121,
>        11220, 13809, 16634, 19869, 23753, 27447,
>        30590, 33975, 36627, 39600, 42067, 44082,
>        58190, 63280, 65921, 67929, 69977, 71865,
>        73614, 74005, 74894, 75717, 76365, 76579,
>        77087, 77493, 77926, 78253, 78680, 79253,
>        79455, 79580, 79699, 79838, 79981, 80080,
>        80124, 80164, 80183, 80207, 80222, 80230,
>        80241, 80261, 80261, 80277, 80290, 80303,
>        80337, 80376, 80422, 80461, 80539, 80586,
>        80653, 80708, 80762, 80807, 80807, 80886,
>        80922, 80957, 80988, 81007, 81037, 81076,
>        81108, 81108, 81171, 81213, 81259, 81358,
>        81466, 81555, 81601, 81647, 81673, 81998,
>        82025, 82041, 82053, 82064, 82094, 82104,
>        82110, 82122, 82133, 82136, 82142, 82164,
>        82168, 82180, 82181, 82184, 82187, 82188,
>        82190, 82192, 82193, 82194)
> Y = log(Y)
> X = 1:length(Y)
> plot(Y ~ X, ylab = "Ln(Y)", xlim=c(0,10, main="zoomed in"))
> abline(lm(Y[1:3] ~ X[1:3]))
> abline(lm(Y[1:5] ~ X[1:5]), lty=2)
> text(7, 6, "After third or fifth point, there is deviance", pos=3)
> text(2.5, 10, "Solid line: linear model points 1:3", pos =3)
> text(2.5, 9, "Dashed line: linear model points 1:5", pos =3)
> plot(Y ~ X, ylab = "Ln(Y)", xlim=c(0,10, main="overall"))
> abline(lm(Y[1:3] ~ X[1:3]))
> ```
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bgunter@4567 @end|ng |rom gm@||@com  Thu May 14 18:23:48 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 14 May 2020 09:23:48 -0700
Subject: [R] How to find a split point in a curve?
In-Reply-To: <CAMk+s2TvXxGHY_LPQQRNSXkiqdDgQW2iuObOQbeVGOVzYArzZQ@mail.gmail.com>
References: <CAMk+s2TvXxGHY_LPQQRNSXkiqdDgQW2iuObOQbeVGOVzYArzZQ@mail.gmail.com>
Message-ID: <CAGxFJbSYdzxJkezcx=TqQpA0+aZis+DmsXdGSwmXF32Q6qAaJQ@mail.gmail.com>

You need to mathematically define 'turning point' first: "end of
exponential phase" is subjective and meaningless. Once you have a precise
mathematical formulation in hand, you can proceed.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, May 14, 2020 at 8:12 AM Luigi Marongiu <marongiu.luigi at gmail.com>
wrote:

> Dear all,
> I am trying to find a turning point in some data. In the initial phase, the
> data increases more or less exponentially (thus it is linear in a nat log
> transform), then reaches a plateau. I would like to find the point that
> marks the end of the exponential phase.
> I understand that the function spline can build a curve; is it possible
> with it to find the turning points? I have no idea of how to use spline
> though.
> Here is a working example.
> Thank you
>
> ```
> Y = c(259, 716, 1404, 2173, 3944, 5403, 7140, 9121,
>       11220, 13809, 16634, 19869, 23753, 27447,
>       30590, 33975, 36627, 39600, 42067, 44082,
>       58190, 63280, 65921, 67929, 69977, 71865,
>       73614, 74005, 74894, 75717, 76365, 76579,
>       77087, 77493, 77926, 78253, 78680, 79253,
>       79455, 79580, 79699, 79838, 79981, 80080,
>       80124, 80164, 80183, 80207, 80222, 80230,
>       80241, 80261, 80261, 80277, 80290, 80303,
>       80337, 80376, 80422, 80461, 80539, 80586,
>       80653, 80708, 80762, 80807, 80807, 80886,
>       80922, 80957, 80988, 81007, 81037, 81076,
>       81108, 81108, 81171, 81213, 81259, 81358,
>       81466, 81555, 81601, 81647, 81673, 81998,
>       82025, 82041, 82053, 82064, 82094, 82104,
>       82110, 82122, 82133, 82136, 82142, 82164,
>       82168, 82180, 82181, 82184, 82187, 82188,
>       82190, 82192, 82193, 82194)
> Y = log(Y)
> X = 1:length(Y)
> plot(Y ~ X, ylab = "Ln(Y)", xlim=c(0,10, main="zoomed in"))
> abline(lm(Y[1:3] ~ X[1:3]))
> abline(lm(Y[1:5] ~ X[1:5]), lty=2)
> text(7, 6, "After third or fifth point, there is deviance", pos=3)
> text(2.5, 10, "Solid line: linear model points 1:3", pos =3)
> text(2.5, 9, "Dashed line: linear model points 1:5", pos =3)
> plot(Y ~ X, ylab = "Ln(Y)", xlim=c(0,10, main="overall"))
> abline(lm(Y[1:3] ~ X[1:3]))
> ```
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Thu May 14 19:05:53 2020
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Thu, 14 May 2020 10:05:53 -0700 (PDT)
Subject: [R] Automating package updates after major version change
In-Reply-To: <CAM_vjum70bW89t5AyJF+GgjQHNqNb6Fk_5=-5YF--yLh43Ey_w@mail.gmail.com>
References: <alpine.LNX.2.20.2004260856590.29900@salmo.appl-ecosys.com>
 <CAM_vjum70bW89t5AyJF+GgjQHNqNb6Fk_5=-5YF--yLh43Ey_w@mail.gmail.com>
Message-ID: <alpine.LNX.2.20.2005141001190.30325@salmo.appl-ecosys.com>

On Sun, 26 Apr 2020, Sarah Goslee wrote:

> Not so coincidentally, I just worked thru this for myself.
> http://numberwright.com/2020/04/clean-and-new/

Sarah,

This isn't working for me on Slackware-14.2/x86_64. My R library is in
/usr/lib64/R/library/ and I copied the contents to a text file, one package
per line.

Using the filename, packagelist, as an argument to install.packages() fails:
> install.packages(packagelist)
Error in install.packages(packagelist) : object 'packagelist' not found

Adding quotes didn't help.

Creating a script, newversionupdate.R, containing the line,
install.packages(packagelist)

and sourcing it also failed.

What might I be doing incorrectly?

Regards,

Rich


From bog@@o@chr|@to|er @end|ng |rom gm@||@com  Thu May 14 19:42:09 2020
From: bog@@o@chr|@to|er @end|ng |rom gm@||@com (Christofer Bogaso)
Date: Thu, 14 May 2020 23:12:09 +0530
Subject: [R] Notational derivative in R
Message-ID: <CA+dpOJkUYNfHo3HTeDOCjD7NEV=e1CWf_T5UC9gMLic6QbpmqA@mail.gmail.com>

Hi,

I was wondering if R can perform notational derivative something like
Mathematica does as explained in
https://reference.wolfram.com/language/howto/TakeADerivative.html

Any pointer will be highly appreciated.

Thanks,


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu May 14 19:50:58 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 14 May 2020 10:50:58 -0700
Subject: [R] Automating package updates after major version change
In-Reply-To: <alpine.LNX.2.20.2005141001190.30325@salmo.appl-ecosys.com>
References: <alpine.LNX.2.20.2004260856590.29900@salmo.appl-ecosys.com>
 <CAM_vjum70bW89t5AyJF+GgjQHNqNb6Fk_5=-5YF--yLh43Ey_w@mail.gmail.com>
 <alpine.LNX.2.20.2005141001190.30325@salmo.appl-ecosys.com>
Message-ID: <51187116-75E6-4664-AF60-98C12FB79484@dcn.davis.ca.us>

Why are you mucking with the system-level library? It is quite unusual to have a use-case in which you should not be adjusting your personal library.

On May 14, 2020 10:05:53 AM PDT, Rich Shepard <rshepard at appl-ecosys.com> wrote:
>On Sun, 26 Apr 2020, Sarah Goslee wrote:
>
>> Not so coincidentally, I just worked thru this for myself.
>> http://numberwright.com/2020/04/clean-and-new/
>
>Sarah,
>
>This isn't working for me on Slackware-14.2/x86_64. My R library is in
>/usr/lib64/R/library/ and I copied the contents to a text file, one
>package
>per line.
>
>Using the filename, packagelist, as an argument to install.packages()
>fails:
>> install.packages(packagelist)
>Error in install.packages(packagelist) : object 'packagelist' not found
>
>Adding quotes didn't help.
>
>Creating a script, newversionupdate.R, containing the line,
>install.packages(packagelist)
>
>and sourcing it also failed.
>
>What might I be doing incorrectly?
>
>Regards,
>
>Rich
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From er|cjberger @end|ng |rom gm@||@com  Thu May 14 19:50:56 2020
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Thu, 14 May 2020 20:50:56 +0300
Subject: [R] Notational derivative in R
In-Reply-To: <CA+dpOJkUYNfHo3HTeDOCjD7NEV=e1CWf_T5UC9gMLic6QbpmqA@mail.gmail.com>
References: <CA+dpOJkUYNfHo3HTeDOCjD7NEV=e1CWf_T5UC9gMLic6QbpmqA@mail.gmail.com>
Message-ID: <CAGgJW74mtXXw2_gRj5UKO62TrxH6TriNs+WBP5kMksmZiNO3=w@mail.gmail.com>

Hi Christofer,
Look at https://cran.r-project.org/web/views/NumericalMathematics.html
and within that page search for Symbolic Mathematics. It shows two
packages: Ryacas and rSymPy.
I have no experience with them but they may be a good place to start.

HTH,
Eric


On Thu, May 14, 2020 at 8:43 PM Christofer Bogaso <
bogaso.christofer at gmail.com> wrote:

> Hi,
>
> I was wondering if R can perform notational derivative something like
> Mathematica does as explained in
> https://reference.wolfram.com/language/howto/TakeADerivative.html
>
> Any pointer will be highly appreciated.
>
> Thanks,
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu May 14 20:00:20 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 14 May 2020 11:00:20 -0700
Subject: [R] Notational derivative in R
In-Reply-To: <CAGgJW74mtXXw2_gRj5UKO62TrxH6TriNs+WBP5kMksmZiNO3=w@mail.gmail.com>
References: <CA+dpOJkUYNfHo3HTeDOCjD7NEV=e1CWf_T5UC9gMLic6QbpmqA@mail.gmail.com>
 <CAGgJW74mtXXw2_gRj5UKO62TrxH6TriNs+WBP5kMksmZiNO3=w@mail.gmail.com>
Message-ID: <7C22DB28-84C0-4933-A2FA-E0819947F9DF@dcn.davis.ca.us>

FWIW I have found all such tools to require babysitting... and for interactive use I prefer wxMaxima and some manual translation to R.

On May 14, 2020 10:50:56 AM PDT, Eric Berger <ericjberger at gmail.com> wrote:
>Hi Christofer,
>Look at https://cran.r-project.org/web/views/NumericalMathematics.html
>and within that page search for Symbolic Mathematics. It shows two
>packages: Ryacas and rSymPy.
>I have no experience with them but they may be a good place to start.
>
>HTH,
>Eric
>
>
>On Thu, May 14, 2020 at 8:43 PM Christofer Bogaso <
>bogaso.christofer at gmail.com> wrote:
>
>> Hi,
>>
>> I was wondering if R can perform notational derivative something like
>> Mathematica does as explained in
>> https://reference.wolfram.com/language/howto/TakeADerivative.html
>>
>> Any pointer will be highly appreciated.
>>
>> Thanks,
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Thu May 14 20:15:02 2020
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Thu, 14 May 2020 11:15:02 -0700 (PDT)
Subject: [R] Automating package updates after major version change
In-Reply-To: <51187116-75E6-4664-AF60-98C12FB79484@dcn.davis.ca.us>
References: <alpine.LNX.2.20.2004260856590.29900@salmo.appl-ecosys.com>
 <CAM_vjum70bW89t5AyJF+GgjQHNqNb6Fk_5=-5YF--yLh43Ey_w@mail.gmail.com>
 <alpine.LNX.2.20.2005141001190.30325@salmo.appl-ecosys.com>
 <51187116-75E6-4664-AF60-98C12FB79484@dcn.davis.ca.us>
Message-ID: <alpine.LNX.2.20.2005141113560.30325@salmo.appl-ecosys.com>

On Thu, 14 May 2020, Jeff Newmiller wrote:

> Why are you mucking with the system-level library? It is quite unusual to
> have a use-case in which you should not be adjusting your personal
> library.

Jeff,

I'm the only user so there's no difference.

Regards,

Rich


From @@r@h@go@|ee @end|ng |rom gm@||@com  Thu May 14 20:15:06 2020
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Thu, 14 May 2020 14:15:06 -0400
Subject: [R] Automating package updates after major version change
In-Reply-To: <alpine.LNX.2.20.2005141001190.30325@salmo.appl-ecosys.com>
References: <alpine.LNX.2.20.2004260856590.29900@salmo.appl-ecosys.com>
 <CAM_vjum70bW89t5AyJF+GgjQHNqNb6Fk_5=-5YF--yLh43Ey_w@mail.gmail.com>
 <alpine.LNX.2.20.2005141001190.30325@salmo.appl-ecosys.com>
Message-ID: <CAM_vju=jMHjC8P4xmaiM9FyyWo_xdQ6o4PnuN27=0fvb3OqAVw@mail.gmail.com>

Hi Rich,

Note that install.packages requires "character vector of the names of
packages whose current versions should be downloaded from the
repositories" as stated in the help file.

If you aren't going to do it the way I did, then you need to read your
text file into R and create that character vector there.

Sarah

On Thu, May 14, 2020 at 1:06 PM Rich Shepard <rshepard at appl-ecosys.com> wrote:
>
> On Sun, 26 Apr 2020, Sarah Goslee wrote:
>
> > Not so coincidentally, I just worked thru this for myself.
> > http://numberwright.com/2020/04/clean-and-new/
>
> Sarah,
>
> This isn't working for me on Slackware-14.2/x86_64. My R library is in
> /usr/lib64/R/library/ and I copied the contents to a text file, one package
> per line.
>
> Using the filename, packagelist, as an argument to install.packages() fails:
> > install.packages(packagelist)
> Error in install.packages(packagelist) : object 'packagelist' not found
>
> Adding quotes didn't help.
>
> Creating a script, newversionupdate.R, containing the line,
> install.packages(packagelist)
>
> and sourcing it also failed.
>
> What might I be doing incorrectly?
>
> Regards,
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Thu May 14 20:18:27 2020
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Thu, 14 May 2020 11:18:27 -0700 (PDT)
Subject: [R] Automating package updates after major version change
In-Reply-To: <CAM_vju=jMHjC8P4xmaiM9FyyWo_xdQ6o4PnuN27=0fvb3OqAVw@mail.gmail.com>
References: <alpine.LNX.2.20.2004260856590.29900@salmo.appl-ecosys.com>
 <CAM_vjum70bW89t5AyJF+GgjQHNqNb6Fk_5=-5YF--yLh43Ey_w@mail.gmail.com>
 <alpine.LNX.2.20.2005141001190.30325@salmo.appl-ecosys.com>
 <CAM_vju=jMHjC8P4xmaiM9FyyWo_xdQ6o4PnuN27=0fvb3OqAVw@mail.gmail.com>
Message-ID: <alpine.LNX.2.20.2005141116110.30325@salmo.appl-ecosys.com>

On Thu, 14 May 2020, Sarah Goslee wrote:

> Note that install.packages requires "character vector of the names of
> packages whose current versions should be downloaded from the
> repositories" as stated in the help file.

Sarah,

Ah, yes. I missed that.

> If you aren't going to do it the way I did, then you need to read your
> text file into R and create that character vector there.

I tried to translate your MacOS R commands to my linux system but failed in
my attempts.

I'll create a one-line list of packages, demarked as strings, and read that
into R as a vector.

Thanks,

Rich


From murdoch@dunc@n @end|ng |rom gm@||@com  Thu May 14 20:24:22 2020
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Thu, 14 May 2020 14:24:22 -0400
Subject: [R] Notational derivative in R
In-Reply-To: <CAGgJW74mtXXw2_gRj5UKO62TrxH6TriNs+WBP5kMksmZiNO3=w@mail.gmail.com>
References: <CA+dpOJkUYNfHo3HTeDOCjD7NEV=e1CWf_T5UC9gMLic6QbpmqA@mail.gmail.com>
 <CAGgJW74mtXXw2_gRj5UKO62TrxH6TriNs+WBP5kMksmZiNO3=w@mail.gmail.com>
Message-ID: <7b496b51-e5b0-8f22-673c-c14fc7565b6b@gmail.com>

On 14/05/2020 1:50 p.m., Eric Berger wrote:
> Hi Christofer,
> Look at https://cran.r-project.org/web/views/NumericalMathematics.html
> and within that page search for Symbolic Mathematics. It shows two
> packages: Ryacas and rSymPy.
> I have no experience with them but they may be a good place to start.

There's also `D` and related functions in base R.

Duncan Murdoch

> 
> HTH,
> Eric
> 
> 
> On Thu, May 14, 2020 at 8:43 PM Christofer Bogaso <
> bogaso.christofer at gmail.com> wrote:
> 
>> Hi,
>>
>> I was wondering if R can perform notational derivative something like
>> Mathematica does as explained in
>> https://reference.wolfram.com/language/howto/TakeADerivative.html
>>
>> Any pointer will be highly appreciated.
>>
>> Thanks,
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @@r@h@go@|ee @end|ng |rom gm@||@com  Thu May 14 20:24:59 2020
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Thu, 14 May 2020 14:24:59 -0400
Subject: [R] Automating package updates after major version change
In-Reply-To: <alpine.LNX.2.20.2005141116110.30325@salmo.appl-ecosys.com>
References: <alpine.LNX.2.20.2004260856590.29900@salmo.appl-ecosys.com>
 <CAM_vjum70bW89t5AyJF+GgjQHNqNb6Fk_5=-5YF--yLh43Ey_w@mail.gmail.com>
 <alpine.LNX.2.20.2005141001190.30325@salmo.appl-ecosys.com>
 <CAM_vju=jMHjC8P4xmaiM9FyyWo_xdQ6o4PnuN27=0fvb3OqAVw@mail.gmail.com>
 <alpine.LNX.2.20.2005141116110.30325@salmo.appl-ecosys.com>
Message-ID: <CAM_vjukZCmaMCi=h+HBH8YFbSVyewAYq88YdvHV0Q0_=CHFdXg@mail.gmail.com>

If you don't mind, I'm curious what didn't work - I would like to be
able to update the document accordingly.

When you used

.libPaths()

what did it tell you? Did you edit that result to be for the previous
version, and swap that into the list.files() command? I thought about
automating it, but I can't be sure what previous version you're
upgrading from.

I suspect you tried to run the code by copy and paste, and not by
putting in your own path, but if so then I'd like to improve the
directions.

Sarah

On Thu, May 14, 2020 at 2:19 PM Rich Shepard <rshepard at appl-ecosys.com> wrote:
>
> On Thu, 14 May 2020, Sarah Goslee wrote:
>
> > Note that install.packages requires "character vector of the names of
> > packages whose current versions should be downloaded from the
> > repositories" as stated in the help file.
>
> Sarah,
>
> Ah, yes. I missed that.
>
> > If you aren't going to do it the way I did, then you need to read your
> > text file into R and create that character vector there.
>
> I tried to translate your MacOS R commands to my linux system but failed in
> my attempts.
>
> I'll create a one-line list of packages, demarked as strings, and read that
> into R as a vector.
>
> Thanks,
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Sarah Goslee (she/her)
http://www.sarahgoslee.com


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Thu May 14 20:42:58 2020
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Thu, 14 May 2020 11:42:58 -0700 (PDT)
Subject: [R] Automating package updates after major version change
In-Reply-To: <CAM_vjukZCmaMCi=h+HBH8YFbSVyewAYq88YdvHV0Q0_=CHFdXg@mail.gmail.com>
References: <alpine.LNX.2.20.2004260856590.29900@salmo.appl-ecosys.com>
 <CAM_vjum70bW89t5AyJF+GgjQHNqNb6Fk_5=-5YF--yLh43Ey_w@mail.gmail.com>
 <alpine.LNX.2.20.2005141001190.30325@salmo.appl-ecosys.com>
 <CAM_vju=jMHjC8P4xmaiM9FyyWo_xdQ6o4PnuN27=0fvb3OqAVw@mail.gmail.com>
 <alpine.LNX.2.20.2005141116110.30325@salmo.appl-ecosys.com>
 <CAM_vjukZCmaMCi=h+HBH8YFbSVyewAYq88YdvHV0Q0_=CHFdXg@mail.gmail.com>
Message-ID: <alpine.LNX.2.20.2005141138040.30325@salmo.appl-ecosys.com>

On Thu, 14 May 2020, Sarah Goslee wrote:

> If you don't mind, I'm curious what didn't work - I would like to be
> able to update the document accordingly.

Sarah,

Here's part of the sequence:

> install.packages('packagelist')
Warning message:
package ?packagelist? is not available (for R version 4.0.0) 
> source('newversionupdate.R')
Error in install.packages(packagelist) : object 'packagelist' not found
> .libpath
Error: object '.libpath' not found
> .libpath()
Error in .libpath() : could not find function ".libpath"
> .libpaths()
Error in .libpaths() : could not find function ".libpaths"
> libpaths()
Error in libpaths() : could not find function "libpaths"
> ?libpaths
No documentation for ?libpaths? in specified packages and libraries:
you could try ???libpaths?
> ?libpaths()
Error in .helpForCall(topicExpr, parent.frame()) :
   no methods for ?libpaths? and no documentation for it as a function
> ?.libpaths()

> When you used
> .libPaths()

Oops! I missed the uppercase 'P'.

> .libPaths()
[1] "/usr/lib64/R/library"

> what did it tell you? Did you edit that result to be for the previous
> version, and swap that into the list.files() command? I thought about
> automating it, but I can't be sure what previous version you're
> upgrading from.

When I upgrade a package, such as R, the old version is removed. My library
has remained the same over many R package upgrades.

> I suspect you tried to run the code by copy and paste, and not by putting
> in your own path, but if so then I'd like to improve the directions.

No, you're running a different OS than I do so I didn't copy and paste.

I'll get back to this after finishing a document edit that has the highest
priority now.

Thanks,

Rich


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu May 14 21:30:31 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 14 May 2020 12:30:31 -0700
Subject: [R] Automating package updates after major version change
In-Reply-To: <alpine.LNX.2.20.2005141113560.30325@salmo.appl-ecosys.com>
References: <alpine.LNX.2.20.2004260856590.29900@salmo.appl-ecosys.com>
 <CAM_vjum70bW89t5AyJF+GgjQHNqNb6Fk_5=-5YF--yLh43Ey_w@mail.gmail.com>
 <alpine.LNX.2.20.2005141001190.30325@salmo.appl-ecosys.com>
 <51187116-75E6-4664-AF60-98C12FB79484@dcn.davis.ca.us>
 <alpine.LNX.2.20.2005141113560.30325@salmo.appl-ecosys.com>
Message-ID: <6B143159-AD3E-4703-A6EB-C36D59DAE637@dcn.davis.ca.us>

On the contrary... that is the use case in which it makes the least sense to muck with system files. System files are maintained using system package management tools which at best lag terribly, while R packages are much easier to manage using R's internal package tools with no elevated privileges.

On May 14, 2020 11:15:02 AM PDT, Rich Shepard <rshepard at appl-ecosys.com> wrote:
>On Thu, 14 May 2020, Jeff Newmiller wrote:
>
>> Why are you mucking with the system-level library? It is quite
>unusual to
>> have a use-case in which you should not be adjusting your personal
>> library.
>
>Jeff,
>
>I'm the only user so there's no difference.
>
>Regards,
>
>Rich
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From ggrothend|eck @end|ng |rom gm@||@com  Thu May 14 22:09:26 2020
From: ggrothend|eck @end|ng |rom gm@||@com (Gabor Grothendieck)
Date: Thu, 14 May 2020 16:09:26 -0400
Subject: [R] How to find a split point in a curve?
In-Reply-To: <CAMk+s2TvXxGHY_LPQQRNSXkiqdDgQW2iuObOQbeVGOVzYArzZQ@mail.gmail.com>
References: <CAMk+s2TvXxGHY_LPQQRNSXkiqdDgQW2iuObOQbeVGOVzYArzZQ@mail.gmail.com>
Message-ID: <CAP01uRnRUUF_o6xe_rhybV5LErq9buDX6+c6NhW1ScDcdtoR3Q@mail.gmail.com>

We can use nls2 to try each value in 10:100 as a possible split point
picking the one with lowest residual sum of squares:

library(nls2)
fm <- nls2(Y ~ cbind(1, pmin(X, X0)), start = data.frame(X0 = 10:100),
  algorithm = "plinear-brute")
plot(Y ~ X)
lines(fitted(fm) ~ X, col = "red")

> fm
Nonlinear regression model
  model: Y ~ cbind(1, pmin(X, X0))
   data: parent.frame()
     X0   .lin1   .lin2
18.0000  6.5570  0.2616
 residual sum-of-squares: 4.999

Number of iterations to convergence: 91
Achieved convergence tolerance: NA

On Thu, May 14, 2020 at 11:13 AM Luigi Marongiu
<marongiu.luigi at gmail.com> wrote:
>
> Dear all,
> I am trying to find a turning point in some data. In the initial phase, the
> data increases more or less exponentially (thus it is linear in a nat log
> transform), then reaches a plateau. I would like to find the point that
> marks the end of the exponential phase.
> I understand that the function spline can build a curve; is it possible
> with it to find the turning points? I have no idea of how to use spline
> though.
> Here is a working example.
> Thank you
>
> ```
> Y = c(259, 716, 1404, 2173, 3944, 5403, 7140, 9121,
>       11220, 13809, 16634, 19869, 23753, 27447,
>       30590, 33975, 36627, 39600, 42067, 44082,
>       58190, 63280, 65921, 67929, 69977, 71865,
>       73614, 74005, 74894, 75717, 76365, 76579,
>       77087, 77493, 77926, 78253, 78680, 79253,
>       79455, 79580, 79699, 79838, 79981, 80080,
>       80124, 80164, 80183, 80207, 80222, 80230,
>       80241, 80261, 80261, 80277, 80290, 80303,
>       80337, 80376, 80422, 80461, 80539, 80586,
>       80653, 80708, 80762, 80807, 80807, 80886,
>       80922, 80957, 80988, 81007, 81037, 81076,
>       81108, 81108, 81171, 81213, 81259, 81358,
>       81466, 81555, 81601, 81647, 81673, 81998,
>       82025, 82041, 82053, 82064, 82094, 82104,
>       82110, 82122, 82133, 82136, 82142, 82164,
>       82168, 82180, 82181, 82184, 82187, 82188,
>       82190, 82192, 82193, 82194)
> Y = log(Y)
> X = 1:length(Y)
> plot(Y ~ X, ylab = "Ln(Y)", xlim=c(0,10, main="zoomed in"))
> abline(lm(Y[1:3] ~ X[1:3]))
> abline(lm(Y[1:5] ~ X[1:5]), lty=2)
> text(7, 6, "After third or fifth point, there is deviance", pos=3)
> text(2.5, 10, "Solid line: linear model points 1:3", pos =3)
> text(2.5, 9, "Dashed line: linear model points 1:5", pos =3)
> plot(Y ~ X, ylab = "Ln(Y)", xlim=c(0,10, main="overall"))
> abline(lm(Y[1:3] ~ X[1:3]))
> ```
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Thu May 14 23:21:43 2020
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Thu, 14 May 2020 14:21:43 -0700 (PDT)
Subject: [R] Automating package updates after major version change
In-Reply-To: <6B143159-AD3E-4703-A6EB-C36D59DAE637@dcn.davis.ca.us>
References: <alpine.LNX.2.20.2004260856590.29900@salmo.appl-ecosys.com>
 <CAM_vjum70bW89t5AyJF+GgjQHNqNb6Fk_5=-5YF--yLh43Ey_w@mail.gmail.com>
 <alpine.LNX.2.20.2005141001190.30325@salmo.appl-ecosys.com>
 <51187116-75E6-4664-AF60-98C12FB79484@dcn.davis.ca.us>
 <alpine.LNX.2.20.2005141113560.30325@salmo.appl-ecosys.com>
 <6B143159-AD3E-4703-A6EB-C36D59DAE637@dcn.davis.ca.us>
Message-ID: <alpine.LNX.2.20.2005141416560.30325@salmo.appl-ecosys.com>

On Thu, 14 May 2020, Jeff Newmiller wrote:

> On the contrary... that is the use case in which it makes the least sense
> to muck with system files. System files are maintained using system
> package management tools which at best lag terribly, while R packages are
> much easier to manage using R's internal package tools with no elevated
> privileges.

Jeff,

I must not be following your thoughts. The slackbuilds.org group maintains a
package for the current R distribution (when the maintainer gets around to
checking that the build script works with the source.) Every library not
provided in the core R distribution I download and install from the local
CRAN mirror.

All Slackware distribution executables are stored in /usr/bin/. Most of the
SBo application packages are also installed there.

Application-specific functions are the province of the user and I let root
download, build, and install R packages.

Regards,

Rich


From gm@rk@|ow|er @end|ng |rom out|ook@com  Thu May 14 13:00:52 2020
From: gm@rk@|ow|er @end|ng |rom out|ook@com (Mark Fowler)
Date: Thu, 14 May 2020 11:00:52 +0000
Subject: [R] Help with map()
In-Reply-To: <BYAPR06MB5383688B7688345330422859AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB5383688B7688345330422859AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <BL0PR11MB29611E5018288E828FA7398585BC0@BL0PR11MB2961.namprd11.prod.outlook.com>

Any chance you forgot to load a library (e.g. maps) for the second round, or maybe changed a loading sequence and overwrote the intended map function? Looks like #2 might be trying to use a purrr-style map function.

Sent from Mail<https://go.microsoft.com/fwlink/?LinkId=550986> for Windows 10

From: Poling, William via R-help<mailto:r-help at r-project.org>
Sent: May 14, 2020 7:23 AM
To: r-help at r-project.org<mailto:r-help at r-project.org>
Subject: [R] Help with map()

#RStudio Version Version 1.2.1335
sessionInfo()
# R version 4.0.0 Patched (2020-05-03 r78349)
#Platform: x86_64-w64-mingw32/x64 (64-bit)
#Running under: Windows 10 x64 (build 17763)

Good morning.

I ran routines like this one yesterday with no errors.

#Radius3----
str(radius3)

radius3 <- individual_dets_sf_3X %>%

  dplyr::select(MBR_SUBSCRIBERID,state,city,Latitude,Longitude,distances_Mi) %>%
  filter(distances_Mi <= 1200)

str(radius3)

  geomat<-makeDensityMatrix(radius3[,c("Latitude","Longitude")],
                            xlim=range(radius3$Longitude),ylim=range(radius3$Latitude))

latlim<-range(radius3$Latitude)
lonlim<-range(radius3$Longitude)

<- map("world",xlim=lonlim,ylim=latlim)
axis(1)
axis(2)
densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
            red=c(0.5,1),green=0,blue=0,pch=15)
title("Member Geo Density Plot For Radius3")


This morning I am trying slightly new routine and receiving this error

clus1 <- individual_dets_sf_3X %>%

  dplyr::select(MBR_SUBSCRIBERID,state,city,clusters,Latitude,Longitude,distances_Mi) %>%
  filter(clusters == 1)

geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
                          xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))

latlim<-range(clus1$Latitude)
lonlim<-range(clus1$Longitude)

map("world",xlim=lonlim,ylim=latlim)
axis(1)
axis(2)
densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
            red=c(0.5,1),green=0,blue=0,pch=15)
title("Member Geo Density Plot For Cluster1")


geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
+                           xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))

> latlim<-range(clus1$Latitude)
> lonlim<-range(clus1$Longitude)
>
> map("world",xlim=lonlim,ylim=latlim)
Error in .C(C_map_type, as.character(mapbase), integer(1)) :
  Incorrect number of arguments (2), expecting 0 for ''
> axis(1)
Error in axis(1) : plot.new has not been called yet
> axis(2)
Error in axis(2) : plot.new has not been called yet
> densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
+             red=c(0.5,1),green=0,blue=0,pch=15)
Error in plot.xy(xy.coords(x, y), type = type, ...) :
  plot.new has not been called yet
> title("Member Geo Density Plot For Cluster1")
Error in title("Member Geo Density Plot For Cluster1") :
  plot.new has not been called yet


It seems to error at the point of map("world",xlim=lonlim,ylim=latlim)

I find a ref in stack overflow https://stackoverflow.com/questions/45066628/cannot-run-map-datastate
and tried solutions but they do not seem to work?

Here is a sample (MBR_SUBSCRIBERID(factor) is replaced with ID(integer) but of no consequence in my routine since not really used.

I hope someone recognizes the problem.

Thank you for any advice

WHP

> dput(sample)
structure(list(state = structure(c(2L, 22L, 10L, 12L, 6L, 22L,
11L, 22L, 22L, 32L, 19L, 29L, 9L, 9L, 10L, 30L, 33L, 35L, 41L,
12L, 12L, 36L, 4L, 9L, 8L, 39L, 36L, 36L, 41L, 28L, 12L, 28L,
28L, 12L, 11L, 2L, 12L, 22L, 32L, 22L, 12L, 10L, 10L, 22L, 10L
), .Label = c("AL", "AR", "AZ", "CA", "CO", "CT", "DC", "DE",
"FL", "GA", "IA", "IL", "IN", "KS", "KY", "LA", "MA", "MD", "ME",
"MI", "MN", "MO", "NC", "NE", "NJ", "NM", "NV", "NY", "OH", "OK",
"OR", "PA", "SC", "SD", "TN", "TX", "UT", "VA", "WA", "WI", "WV"
), class = "factor"), city = structure(c(838L, 1030L, 262L, 218L,
166L, 973L, 339L, 451L, 660L, 358L, 281L, 1280L, 129L, 223L,
989L, 721L, 550L, 731L, 1325L, 688L, 1184L, 281L, 759L, 1171L,
1305L, 587L, 272L, 581L, 263L, 152L, 217L, 152L, 390L, 5L, 571L,
1006L, 1162L, 939L, 170L, 1033L, 1002L, 586L, 586L, 192L, 586L
), .Label = c("ABBOTTSTOWN", "ABILENE", "ACWORTH", "ADAMS", "ADDISON",
"ADKINS", "ALBANY", "ALBIA", "ALBUQUERQUE", "ALEXANDRIA", "ALFRED",
"ALIQUIPPA", "ALISO VIEJO", "ALLEN PARK", "ALLENTOWN", "ALPHA",
"ALPHARETTA", "ALPINE", "ALTOONA", "AMARILLO", "AMBLER", "AMBRIDGE",
"AMHERST", "AMITYVILLE", "ANAHIEM", "ANDERSON", "ANDOVER", "ANGIER",
"ANGLETON", "ANKENY", "ANN ARBOR", "ANZA", "APEX", "APOLLO",
"ARCADIA", "ARCHDALE", "ARDMORE", "ARGYLE", "ARKANSAS CITY",
"ARLINGTON", "ARNOLD", "ARTHUR", "ASHEBORO", "ASHEVILLE", "ASHLAND",
"ASHLAND CITY", "ASHTON", "ASTON", "ATHENS", "ATLANTA", "ATLANTIC BCH",
"AUBORN", "AUGUSTA", "AURORA", "AUSTELL", "AUSTIN", "AVENTURA",
"AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN CITY", "BALDWIN PLACE",
"BALLWIN", "BANGOR", "BARBOURSVILLE", "BARNEGAT", "BARTLETT",
"BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", "BATESBURG", "BATON ROUGE",
"BAXTER SPRINGS", "BAY CITY", "BAYONNE", "BAYSIDE", "BAYTOWN",
"BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER CITY", "BELFAIR",
"BELLE VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR", "BELLMORE",
"BELLWOOD", "BELMAR", "BELMONT", "BELVIDERE", "BENNET", "BENTON",
"BENTONVILLE", "BERLIN", "BESSEMER CITY", "BETHANY", "BETHEL",
"BETHEL PARK", "BETHESDA", "BETHLEHEM", "BETTENDORF", "BIGLERVILLE",
"BINGHAMTON", "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN", "BLACKSBURG",
"BLAINE", "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD", "BLUE BELL",
"BLUE GRASS", "BLUE POINT", "BLUE SPRINGS", "BOCA RATON", "BODFISH",
"BOILING SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS", "BONNEY LAKE",
"BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING GREEN", "BOYERTOWN",
"BOYNTON BEACH", "BRADENTON", "BRADENVILLE", "BRANDAMORE", "BRANDON",
"BRANFORD", "BRANSON", "BRASELTON", "BREEZEWOOD", "BRENTON",
"BRENTWOOD", "BREWSTER", "BRICK", "BRIDGEPORT", "BRIDGETON",
"BRIGHAM CITY", "BROADDUS", "BROCKPORT", "BROGUE", "BROKEN ARROW",
"BRONX", "BROOKFIELD", "BROOKHAVEN", "BROOKLYN", "BROWNS MILLS",
"BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", "BRYAN", "BUCKSPORT",
"BUDD LAKE", "BUFFALO", "BULVERDE", "BUNKERVILLE", "BURBANK",
"BURFORD", "BURIEN", "BURLINGTON", "BURR RIDGE", "BURTON", "BUSKIRK",
"BUTLER", "BUXTON", "BYRON", "CALERA", "CAMBRIDGE", "CAMP HILL",
"CAMPBELL", "CANAAN", "CANAL WINCHESTER", "CANASTOTA", "CANTON",
"CARDIFF BY THE SEA", "CARL JUNCTION", "CARLISLE", "CARLTON",
"CAROL STREAM", "CARROLLTON", "CARSON CITY", "CARTERSVILLE",
"CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", "CASWELL", "CATO",
"CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER POINT", "CENTERTON",
"CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN FALLS", "CHALFONT",
"CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", "CHARLESTON",
"CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", "CHESHIRE",
"CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", "CHILLICOTHE",
"CHOCTAW", "CICERO", "CINCINNATI", "CLAY", "CLEARWATER", "CLEARWATER BEACH",
"CLEMMONS", "CLENDENIN", "CLEVELAND", "CLEVELAND HTS", "CLEVES",
"CLIFFSIDE PARK", "CLIFTON", "CLIFTON PARK", "CLIFTON SPGS",
"CLINTON", "COACHELLA", "COAL TOWNSHIP", "COATESVILLE", "COLLEGE STATION",
"COLLINSVILLE", "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE",
"CONCORD", "CONCORD TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY",
"CONYERS", "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA",
"CORNELIUS", "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY",
"COTTAGE HILLS", "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN",
"CRANBERRY TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER",
"CUMBERLAND FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS",
"DALLAS", "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT",
"DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR",
"DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH",
"DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY",
"DES MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER",
"DIXON", "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER",
"DOYLESTOWN", "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH",
"DUNCAN", "DUNCANNON", "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE",
"DURHAM", "DUTTON", "EAGLE GROVE", "EAST AURORA", "EAST CHICAGO",
"EAST EARL", "EAST HANOVER", "EAST HARTFORD", "EAST HARTLAND",
"EAST LEROY", "EAST LIVERPOOL", "EAST MEADOW", "EAST NORTHPORT",
"EAST ORANGE", "EAST PEORIA", "EAST SAINT LOUIS", "EASTON", "EDGERTON",
"EDISON", "EDMOND", "EGG HBR TWP", "EL DORADO", "EL PASO", "ELDRIDGE",
"ELGIN", "ELIZABETHVILLE", "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY",
"ELMHURST", "ELMWOOD PARK", "ELOY", "ELY", "EMERSON", "EMORY",
"ENDICOTT", "ENGLEWOOD", "ENID", "ENNICE", "ENOLA", "ENUMCLAW",
"EPHRATA", "ERIE", "ESTERO", "EUREKA SPRINGS", "EVANSVILLE",
"EVERETT", "EXCELSIOR SPRINGS", "EXTON", "FAIRFAX", "FAIRFIELD",
"FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", "FALMOUTH", "FAR HILLS",
"FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", "FAYETTE", "FAYETTEVILLE",
"FELTON", "FENTON", "FERGUSON", "FIELDALE", "FINDLAY", "FINGERVILLE",
"FLEMINGTON", "FLINT", "FLORENCE", "FLORESVILLE", "FLORISSANT",
"FLOVILLA", "FLOWER MOUND", "FLUSHING", "FOGELSVILLE", "FOLSOM",
"FONTANELLE", "FOREST HILLS", "FOREST PARK", "FORT DODGE", "FORT FAIRFIELD",
"FORT GAY", "FORT LAUDERDALE", "FORT LEE", "FORT MILL", "FORT MOHAVE",
"FORT PAYNE", "FORT PIERCE", "FORT SMITH", "FORT WAYNE", "FORT WORTH",
"FOSTORIA", "FOUNTAIN INN", "FRANKFORT", "FRANKLIN", "FREEDOM",
"FREEHOLD", "FREEPORT", "FREMONT", "FULLERTON", "FULSHEAR", "FULTON",
"GADSDEN", "GAFFNEY", "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE",
"GARDNERS", "GARNETT VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA",
"GETTYSBURG", "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE",
"GLEN CARBON", "GLEN COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW",
"GLENWOOD", "GOLDEN CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM",
"GOSHEN", "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS",
"GRANDVIEW", "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS",
"GRASS VALLEY", "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER",
"GREENLAWN", "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE",
"GREER", "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD",
"HADLEY", "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON",
"HAMBURG", "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE",
"HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON",
"HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL",
"HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE",
"HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH",
"HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE",
"HIGH HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS",
"HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE",
"HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD",
"HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS",
"HOT SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON",
"HUFFMAN", "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON",
"HUNTINGTON BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE",
"HURST", "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL",
"INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY",
"IRMO", "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP",
"ISSAQUAH", "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS",
"JACKSONVILLE", "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE",
"JEFFERSON", "JEFFERSON CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN",
"JONESBORO", "JONESPORT", "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE",
"KANNAPOLIS", "KANSAS CITY", "KATY", "KEARNEY", "KELLERTON",
"KEMAH", "KENMORE", "KENNESAW", "KENT", "KINGMAN", "KINGS MOUNTAIN",
"KINGSLAND", "KINGSTON", "KINGWOOD", "KIRKLAND", "KUTZTOWN",
"LA QUINTA", "LA RUE", "LA VERNE", "LAFAYETTE", "LAKE BLUFF",
"LAKE IN THE HILL", "LAKE ISABELLA", "LAKE PLACID", "LAKE SAINT LOUIS",
"LAKE TAPPS", "LAKE VILLAGE", "LAKE WAUKOMIS", "LAKE WORTH",
"LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD RCH", "LAMBERTVILLE",
"LANCASTER", "LANDISVILLE", "LANSDALE", "LAREDO", "LARGO", "LAS VEGAS",
"LATROBE", "LAUDERHILL", "LAWNDALE", "LAWRENCE", "LAWRENCEVILLE",
"LE MARS", "LE ROY", "LEAGUE CITY", "LEAVENWORTH", "LEAWOOD",
"LEBANON", "LECOMPTON", "LEECHBURG", "LEES SUMMIT", "LEESBURG",
"LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", "LEVITTOWN", "LEWES",
"LEWISTOWN", "LEXINGTON", "LIBERTY TWP", "LILLINGTON", "LINCOLN",
"LINDENHURST", "LINTON", "LISBON", "LITCHFIELD PK", "LITHIA",
"LITITZ", "LITTLE EGG HARBOR TO", "LK FOREST PK", "LK PEEKSKILL",
"LOCKHART", "LOCKWOOD", "LOGAN", "LOMA LINDA", "LONE JACK", "LONG BEACH",
"LONG LANE", "LONGVIEW", "LORAIN", "LORTON", "LOS ANGELES", "LOUISBURG",
"LOVES PARK", "LOWER BURRELL", "LOWRY CITY", "LUBBOCK", "LUGOFF",
"LUMBERTON", "LYNBROOK", "LYNNWOOD", "MACHESNEY PARK", "MACON",
"MAGNOLIA", "MAHOPAC", "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE",
"MANASSAS", "MANCHACA", "MANCHESTER", "MANCHESTER TOWNSHIP",
"MANCHESTER TW", "MANHEIM", "MANITO", "MANLIUS", "MANNINGTON",
"MANSFIELD", "MANSURA", "MAPLE PARK", "MARBLE FALLS", "MARCELLUS",
"MARICOPA", "MARIETTA", "MARION", "MARKLEYSBURG", "MARS HILL",
"MARSHALLTOWN", "MARSHFIELD", "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE",
"MASON CITY", "MASSEY", "MASSILLON", "MASURY", "MATAWAN", "MATEWAN",
"MATTAPOISETT", "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND",
"MC DONALD", "MC KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN",
"MCALESTER", "MCDONOUGH", "MCKEESPORT", "MCKINNEY", "MECHANICSBURG",
"MECHANICSVILLE", "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS",
"MERIDEN", "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA",
"MESQUITE", "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN",
"MIDLAND", "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK",
"MILLEDGEVILLE", "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE",
"MINEOLA", "MINERAL SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION",
"MISSION HILLS", "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY",
"MOLINE", "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE",
"MONTCLAIR", "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON",
"MORGANTOWN", "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE",
"MOUNT JOY", "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO",
"MOUNT UNION", "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT",
"MULVANE", "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT",
"MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS",
"NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES",
"NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG",
"NEW BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK",
"NEW CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON",
"NEW LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE",
"NEW ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK",
"NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT",
"NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA",
"NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK",
"NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON",
"NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND HILLS",
"NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE",
"NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON",
"OAK RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE",
"OMAHA", "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION",
"ORONO", "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO",
"OTTAWA", "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD",
"OYSTER BAY", "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR",
"PALMDALE", "PALMYRA", "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE",
"PARLIN", "PARMA", "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY",
"PAXTON", "PEA RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN",
"PELION", "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE",
"PENNSBURG", "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS",
"PERRYSBURG", "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI",
"PHOENIX", "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN",
"PINE BLUFF", "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY",
"PITMAN", "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY",
"PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO LAKE",
"POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", "PORT CHARLOTTE",
"PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", "PORT WASHINGTON",
"PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", "POTTSVILLE",
"POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE VILLAGE",
"PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT PLEASANT",
"PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", "QUINCY",
"RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", "READLYN",
"RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO PARK",
"REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", "RICH HILL",
"RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", "RINCON",
"RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", "RIVERSIDE",
"ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", "ROCKAWAY BCH",
"ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", "ROGERS",
"ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE",
"ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT",
"ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN",
"RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO",
"SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR",
"SAINT CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS",
"SAINT PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY",
"SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO",
"SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN",
"SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", "SARASOTA",
"SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", "SAYVILLE",
"SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", "SCOTT", "SCOTTSDALE",
"SEAFORD", "SEALE", "SEATTLE", "SEDONA", "SEGUIN", "SELDEN",
"SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", "SHARON", "SHARPSBURG",
"SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", "SHELLSBURG", "SHELTON",
"SHENANDOAH", "SHORELINE", "SHOREWOOD", "SHREVEPORT", "SICKLERVILLE",
"SILER CITY", "SILOAM SPRINGS", "SIMON", "SIMPSONVILLE", "SIMSBURY",
"SIOUX CITY", "SIOUX FALLS", "SKOKIE", "SKOWHEGAN", "SLATINGTON",
"SLIDELL", "SMITHFIELD", "SN BERNRDNO", "SNELLVILLE", "SOCORRO",
"SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH BELOIT", "SOUTH BEND",
"SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH PARK", "SOUTH PLAINFIELD",
"SOUTH SIOUX CITY", "SOUTHAMPTON", "SOUTHBURY", "SOUTHGATE",
"SPARKS", "SPARROW BUSH", "SPENCER", "SPRING", "SPRING BRANCH",
"SPRING GROVE", "SPRING HILL", "SPRING HOPE", "SPRING VALLEY",
"SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD GARDENS", "SPRINGHILL",
"SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", "ST PETERSBURG",
"ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", "STANLEY",
"STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", "STILLMAN VALLEY",
"STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", "STRAFFORD",
"STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", "SUFFIELD",
"SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", "SUMMIT",
"SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", "SWEENY",
"SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", "TAFTVILLE",
"TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", "TAYLORVILLE",
"TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", "THOMASVILLE",
"TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY PARK",
"TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", "TOPEKA",
"TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS RST",
"TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER",
"TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY",
"UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA",
"URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO",
"VAN BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO",
"VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA",
"VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH",
"W TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN",
"WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG",
"WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA",
"WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE",
"WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER",
"WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO",
"WEST CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN",
"WEST HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH",
"WEST POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD",
"WEST VALLEY CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER",
"WESTMORELAND CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD",
"WHARTON", "WHEELING", "WHITE PLAINS", "WHITEHALL", "WHITESTONE",
"WHITESTOWN", "WHITING", "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD",
"WILLARD", "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK",
"WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS",
"WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON",
"WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE",
"WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS",
"WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH",
"YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS",
"YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor"),
    clusters = c(6L, 10L, 3L, 10L, 5L, 6L, 10L, 6L, 6L, 7L, 5L,
    7L, 8L, 8L, 3L, 6L, 3L, 10L, 7L, 10L, 10L, 4L, 2L, 8L, 9L,
    1L, 4L, 4L, 7L, 5L, 10L, 5L, 5L, 10L, 6L, 6L, 10L, 6L, 7L,
    10L, 10L, 3L, 3L, 6L, 3L), Longitude = c(-93.883554, -90.582058,
    -83.868923, -89.544083, -72.902467, -94.458904, -90.588533,
    -94.527843, -92.93769, -80.029456, -70.58809, -82.888789,
    -80.113071, -82.6661, -81.372992, -95.840964, -82.016013,
    -89.92625, -80.276676, -89.778385, -87.641063, -94.888036,
    -117.663119, -82.530568, -75.574437, -122.209047, -96.799309,
    -95.746255, -80.553953, -73.923314, -87.713482, -73.9731,
    -73.8187, -87.987023, -93.759478, -94.110903, -90.00597,
    -93.257109, -79.932483, -90.201858, -89.100233, -84.625923,
    -84.567614, -93.912921, -84.612564), Latitude = c(34.503045,
    38.752515, 33.406527, 40.893588, 41.733088, 38.999032, 41.657674,
    38.890929, 37.639766, 42.099693, 43.53462, 40.142492, 26.538149,
    27.9062, 31.90104, 34.833083, 35.118836, 35.136047, 39.425712,
    40.519207, 40.101126, 30.044457, 33.62287, 27.910276, 39.752254,
    47.403778, 32.961929, 29.820199, 38.408649, 40.668828, 41.983948,
    40.58116, 40.7253, 41.921667, 41.669921, 36.442552, 38.564663,
    37.455315, 40.867774, 38.783554, 42.244381, 34.024332, 34.001644,
    36.622704, 34.017832), distances_Me = c(1879089.079, 1437296.523,
    1185638.742, 1313997.97, 156759.9125, 1759443.351, 1398398.997,
    1767861.917, 1668917.322, 534234.2576, 431749.2965, 758508.4701,
    1657031.938, 1615575.668, 1169085.702, 2024642.021, 930944.4681,
    1525322.746, 553069.7532, 1337002.822, 1161367.37, 2223496.376,
    3918520.212, 1609088.602, 164252.5146, 3878581.001, 2197910.688,
    2304526.789, 614908.3151, 11075.24597, 1160375.454, 494.2997496,
    21063.72622, 1182681.434, 1662351.873, 1810881.911, 1393581.436,
    1701982.013, 504395.772, 1404348.991, 1276505.454, 1190457.718,
    1188029.899, 1787261.336, 1189994.055), distances_Mi = c(1167.607468,
    893.0913246, 736.719012, 816.477441, 97.40573055, 1093.263337,
    868.9216123, 1098.494372, 1037.01328, 331.9565399, 268.2755749,
    471.3135552, 1029.628072, 1003.868436, 726.4334682, 1258.049536,
    578.4599174, 947.7880794, 343.6603307, 830.7719404, 721.6375366,
    1381.611443, 2434.846498, 999.8375755, 102.0614003, 2410.029515,
    1365.713293, 1431.96122, 382.0848884, 6.881813138, 721.0211909,
    0.30714248, 13.08834388, 734.8814328, 1032.933715, 1125.225657,
    865.9281298, 1057.55865, 313.4158337, 872.6187534, 793.1807589,
    739.7133739, 738.2048023, 1110.548567, 739.4252678), ID = 2308:2352), row.names = c(NA,
-45L), class = "data.frame")

Proprietary

NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:12}}


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri May 15 01:13:58 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 14 May 2020 16:13:58 -0700
Subject: [R] Automating package updates after major version change
In-Reply-To: <alpine.LNX.2.20.2005141416560.30325@salmo.appl-ecosys.com>
References: <alpine.LNX.2.20.2004260856590.29900@salmo.appl-ecosys.com>
 <CAM_vjum70bW89t5AyJF+GgjQHNqNb6Fk_5=-5YF--yLh43Ey_w@mail.gmail.com>
 <alpine.LNX.2.20.2005141001190.30325@salmo.appl-ecosys.com>
 <51187116-75E6-4664-AF60-98C12FB79484@dcn.davis.ca.us>
 <alpine.LNX.2.20.2005141113560.30325@salmo.appl-ecosys.com>
 <6B143159-AD3E-4703-A6EB-C36D59DAE637@dcn.davis.ca.us>
 <alpine.LNX.2.20.2005141416560.30325@salmo.appl-ecosys.com>
Message-ID: <CDA8883C-EC6F-41BD-B3C6-2F46B0BE4E4E@dcn.davis.ca.us>

Running R as root, ever, is a completely unnecessary elevation of privileges... but that discussion is getting OT as it pertains to OS security and stability rather than R language.

On May 14, 2020 2:21:43 PM PDT, Rich Shepard <rshepard at appl-ecosys.com> wrote:
>On Thu, 14 May 2020, Jeff Newmiller wrote:
>
>> On the contrary... that is the use case in which it makes the least
>sense
>> to muck with system files. System files are maintained using system
>> package management tools which at best lag terribly, while R packages
>are
>> much easier to manage using R's internal package tools with no
>elevated
>> privileges.
>
>Jeff,
>
>I must not be following your thoughts. The slackbuilds.org group
>maintains a
>package for the current R distribution (when the maintainer gets around
>to
>checking that the build script works with the source.) Every library
>not
>provided in the core R distribution I download and install from the
>local
>CRAN mirror.
>
>All Slackware distribution executables are stored in /usr/bin/. Most of
>the
>SBo application packages are also installed there.
>
>Application-specific functions are the province of the user and I let
>root
>download, build, and install R packages.
>
>Regards,
>
>Rich
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Fri May 15 01:25:25 2020
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Thu, 14 May 2020 16:25:25 -0700 (PDT)
Subject: [R] Automating package updates after major version change
In-Reply-To: <CDA8883C-EC6F-41BD-B3C6-2F46B0BE4E4E@dcn.davis.ca.us>
References: <alpine.LNX.2.20.2004260856590.29900@salmo.appl-ecosys.com>
 <CAM_vjum70bW89t5AyJF+GgjQHNqNb6Fk_5=-5YF--yLh43Ey_w@mail.gmail.com>
 <alpine.LNX.2.20.2005141001190.30325@salmo.appl-ecosys.com>
 <51187116-75E6-4664-AF60-98C12FB79484@dcn.davis.ca.us>
 <alpine.LNX.2.20.2005141113560.30325@salmo.appl-ecosys.com>
 <6B143159-AD3E-4703-A6EB-C36D59DAE637@dcn.davis.ca.us>
 <alpine.LNX.2.20.2005141416560.30325@salmo.appl-ecosys.com>
 <CDA8883C-EC6F-41BD-B3C6-2F46B0BE4E4E@dcn.davis.ca.us>
Message-ID: <alpine.LNX.2.20.2005141624200.30325@salmo.appl-ecosys.com>

On Thu, 14 May 2020, Jeff Newmiller wrote:

> Running R as root, ever, is a completely unnecessary elevation of
> privileges... but that discussion is getting OT as it pertains to OS
> security and stability rather than R language.

Only to upgrade the core package and install/update CRAN packages. All work
is done as a user.

Rich


From v@|kremk @end|ng |rom gm@||@com  Fri May 15 04:58:03 2020
From: v@|kremk @end|ng |rom gm@||@com (Val)
Date: Thu, 14 May 2020 21:58:03 -0500
Subject: [R] sort
Message-ID: <CAJOiR6YRm2YKgTKpMTkkCqoa4j0sNHuYrjgusUbA82ER2QrVbA@mail.gmail.com>

HI All,
I have a sample of data frame
DF1<-read.table(text="name ddate
  A  2019-10-28
  A  2018-01-25
  A  2020-01-12
  A  2017-10-20
  B  2020-11-20
  B  2019-10-20
  B  2017-05-20
  B  2020-01-20
  c  2009-10-01  ",header=TRUE)

1. I want sort by name and ddate on decreasing order and the output
should like as follow
   A  2020-01-12
   A  2019-01-12
   A  2018-01-25
   A  2017-10-20
   B  2020-11-21
  B  2020-11-01
  B  2019-10-20
  B  2017-05-20
  c  2009-10-01

2.  Take the top two rows by group( names) and the out put should like
   A  2020-01-12
   A  2019-01-12
   B  2020-11-21
   B  2020-11-01
    c  2009-10-01

3.  Within each group (name) get the date difference  between the
first and second rows dates. If a group has only one row then the
difference should be 0

The final out put is
Name diff
   A  365
    B  20
    C  0

Here is my attempt and have an issue at the sorting
DF1$DTime <- as.POSIXct(DF1$ddate , format = "%Y-%m-%d")
DF2 <- DF1[order(DF1$name, ((as.Date(DF1$DTime, decreasing = TRUE)))), ]

not working
Any help?

Thank you


From drj|m|emon @end|ng |rom gm@||@com  Thu May 14 12:43:58 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Thu, 14 May 2020 20:43:58 +1000
Subject: [R] Help with map()
In-Reply-To: <BYAPR06MB5383688B7688345330422859AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB5383688B7688345330422859AEBC0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <CA+8X3fWwQ2HrBUxUW1Od5AeBegfmurkpz1Gu-wFhfAZ4dX9cmQ-4518@mail.gmail.com>


Hi Bill,
Have you compared str(radius3) to str(clus1)? It may be quite different.

Jim

On Thu, May 14, 2020 at 8:24 PM Poling, William via R-help
<r-help at r-project.org> wrote:
>
> #RStudio Version Version 1.2.1335
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit)
> #Running under: Windows 10 x64 (build 17763)
>
> Good morning.
>
> I ran routines like this one yesterday with no errors.
>
> #Radius3----
> str(radius3)
>
> radius3 <- individual_dets_sf_3X %>%
>
>   dplyr::select(MBR_SUBSCRIBERID,state,city,Latitude,Longitude,distances_Mi) %>%
>   filter(distances_Mi <= 1200)
>
> str(radius3)
>
>   geomat<-makeDensityMatrix(radius3[,c("Latitude","Longitude")],
>                             xlim=range(radius3$Longitude),ylim=range(radius3$Latitude))
>
> latlim<-range(radius3$Latitude)
> lonlim<-range(radius3$Longitude)
>
> <- map("world",xlim=lonlim,ylim=latlim)
> axis(1)
> axis(2)
> densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
>             red=c(0.5,1),green=0,blue=0,pch=15)
> title("Member Geo Density Plot For Radius3")
>
>
> This morning I am trying slightly new routine and receiving this error
>
> clus1 <- individual_dets_sf_3X %>%
>
>   dplyr::select(MBR_SUBSCRIBERID,state,city,clusters,Latitude,Longitude,distances_Mi) %>%
>   filter(clusters == 1)
>
> geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
>                           xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))
>
> latlim<-range(clus1$Latitude)
> lonlim<-range(clus1$Longitude)
>
> map("world",xlim=lonlim,ylim=latlim)
> axis(1)
> axis(2)
> densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
>             red=c(0.5,1),green=0,blue=0,pch=15)
> title("Member Geo Density Plot For Cluster1")
>
>
> geomat<-makeDensityMatrix(clus1[,c("Latitude","Longitude")],
> +                           xlim=range(clus1$Longitude),ylim=range(clus1$Latitude))
>
> > latlim<-range(clus1$Latitude)
> > lonlim<-range(clus1$Longitude)
> >
> > map("world",xlim=lonlim,ylim=latlim)
> Error in .C(C_map_type, as.character(mapbase), integer(1)) :
>   Incorrect number of arguments (2), expecting 0 for ''
> > axis(1)
> Error in axis(1) : plot.new has not been called yet
> > axis(2)
> Error in axis(2) : plot.new has not been called yet
> > densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
> +             red=c(0.5,1),green=0,blue=0,pch=15)
> Error in plot.xy(xy.coords(x, y), type = type, ...) :
>   plot.new has not been called yet
> > title("Member Geo Density Plot For Cluster1")
> Error in title("Member Geo Density Plot For Cluster1") :
>   plot.new has not been called yet
>
>
> It seems to error at the point of map("world",xlim=lonlim,ylim=latlim)
>
> I find a ref in stack overflow https://stackoverflow.com/questions/45066628/cannot-run-map-datastate
> and tried solutions but they do not seem to work?
>
> Here is a sample (MBR_SUBSCRIBERID(factor) is replaced with ID(integer) but of no consequence in my routine since not really used.
>
> I hope someone recognizes the problem.
>
> Thank you for any advice
>
> WHP
>
> > dput(sample)
> structure(list(state = structure(c(2L, 22L, 10L, 12L, 6L, 22L,
> 11L, 22L, 22L, 32L, 19L, 29L, 9L, 9L, 10L, 30L, 33L, 35L, 41L,
> 12L, 12L, 36L, 4L, 9L, 8L, 39L, 36L, 36L, 41L, 28L, 12L, 28L,
> 28L, 12L, 11L, 2L, 12L, 22L, 32L, 22L, 12L, 10L, 10L, 22L, 10L
> ), .Label = c("AL", "AR", "AZ", "CA", "CO", "CT", "DC", "DE",
> "FL", "GA", "IA", "IL", "IN", "KS", "KY", "LA", "MA", "MD", "ME",
> "MI", "MN", "MO", "NC", "NE", "NJ", "NM", "NV", "NY", "OH", "OK",
> "OR", "PA", "SC", "SD", "TN", "TX", "UT", "VA", "WA", "WI", "WV"
> ), class = "factor"), city = structure(c(838L, 1030L, 262L, 218L,
> 166L, 973L, 339L, 451L, 660L, 358L, 281L, 1280L, 129L, 223L,
> 989L, 721L, 550L, 731L, 1325L, 688L, 1184L, 281L, 759L, 1171L,
> 1305L, 587L, 272L, 581L, 263L, 152L, 217L, 152L, 390L, 5L, 571L,
> 1006L, 1162L, 939L, 170L, 1033L, 1002L, 586L, 586L, 192L, 586L
> ), .Label = c("ABBOTTSTOWN", "ABILENE", "ACWORTH", "ADAMS", "ADDISON",
> "ADKINS", "ALBANY", "ALBIA", "ALBUQUERQUE", "ALEXANDRIA", "ALFRED",
> "ALIQUIPPA", "ALISO VIEJO", "ALLEN PARK", "ALLENTOWN", "ALPHA",
> "ALPHARETTA", "ALPINE", "ALTOONA", "AMARILLO", "AMBLER", "AMBRIDGE",
> "AMHERST", "AMITYVILLE", "ANAHIEM", "ANDERSON", "ANDOVER", "ANGIER",
> "ANGLETON", "ANKENY", "ANN ARBOR", "ANZA", "APEX", "APOLLO",
> "ARCADIA", "ARCHDALE", "ARDMORE", "ARGYLE", "ARKANSAS CITY",
> "ARLINGTON", "ARNOLD", "ARTHUR", "ASHEBORO", "ASHEVILLE", "ASHLAND",
> "ASHLAND CITY", "ASHTON", "ASTON", "ATHENS", "ATLANTA", "ATLANTIC BCH",
> "AUBORN", "AUGUSTA", "AURORA", "AUSTELL", "AUSTIN", "AVENTURA",
> "AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN CITY", "BALDWIN PLACE",
> "BALLWIN", "BANGOR", "BARBOURSVILLE", "BARNEGAT", "BARTLETT",
> "BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", "BATESBURG", "BATON ROUGE",
> "BAXTER SPRINGS", "BAY CITY", "BAYONNE", "BAYSIDE", "BAYTOWN",
> "BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER CITY", "BELFAIR",
> "BELLE VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR", "BELLMORE",
> "BELLWOOD", "BELMAR", "BELMONT", "BELVIDERE", "BENNET", "BENTON",
> "BENTONVILLE", "BERLIN", "BESSEMER CITY", "BETHANY", "BETHEL",
> "BETHEL PARK", "BETHESDA", "BETHLEHEM", "BETTENDORF", "BIGLERVILLE",
> "BINGHAMTON", "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN", "BLACKSBURG",
> "BLAINE", "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD", "BLUE BELL",
> "BLUE GRASS", "BLUE POINT", "BLUE SPRINGS", "BOCA RATON", "BODFISH",
> "BOILING SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS", "BONNEY LAKE",
> "BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING GREEN", "BOYERTOWN",
> "BOYNTON BEACH", "BRADENTON", "BRADENVILLE", "BRANDAMORE", "BRANDON",
> "BRANFORD", "BRANSON", "BRASELTON", "BREEZEWOOD", "BRENTON",
> "BRENTWOOD", "BREWSTER", "BRICK", "BRIDGEPORT", "BRIDGETON",
> "BRIGHAM CITY", "BROADDUS", "BROCKPORT", "BROGUE", "BROKEN ARROW",
> "BRONX", "BROOKFIELD", "BROOKHAVEN", "BROOKLYN", "BROWNS MILLS",
> "BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", "BRYAN", "BUCKSPORT",
> "BUDD LAKE", "BUFFALO", "BULVERDE", "BUNKERVILLE", "BURBANK",
> "BURFORD", "BURIEN", "BURLINGTON", "BURR RIDGE", "BURTON", "BUSKIRK",
> "BUTLER", "BUXTON", "BYRON", "CALERA", "CAMBRIDGE", "CAMP HILL",
> "CAMPBELL", "CANAAN", "CANAL WINCHESTER", "CANASTOTA", "CANTON",
> "CARDIFF BY THE SEA", "CARL JUNCTION", "CARLISLE", "CARLTON",
> "CAROL STREAM", "CARROLLTON", "CARSON CITY", "CARTERSVILLE",
> "CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", "CASWELL", "CATO",
> "CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER POINT", "CENTERTON",
> "CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN FALLS", "CHALFONT",
> "CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", "CHARLESTON",
> "CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", "CHESHIRE",
> "CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", "CHILLICOTHE",
> "CHOCTAW", "CICERO", "CINCINNATI", "CLAY", "CLEARWATER", "CLEARWATER BEACH",
> "CLEMMONS", "CLENDENIN", "CLEVELAND", "CLEVELAND HTS", "CLEVES",
> "CLIFFSIDE PARK", "CLIFTON", "CLIFTON PARK", "CLIFTON SPGS",
> "CLINTON", "COACHELLA", "COAL TOWNSHIP", "COATESVILLE", "COLLEGE STATION",
> "COLLINSVILLE", "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE",
> "CONCORD", "CONCORD TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY",
> "CONYERS", "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA",
> "CORNELIUS", "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY",
> "COTTAGE HILLS", "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN",
> "CRANBERRY TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER",
> "CUMBERLAND FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS",
> "DALLAS", "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT",
> "DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR",
> "DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH",
> "DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY",
> "DES MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER",
> "DIXON", "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER",
> "DOYLESTOWN", "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH",
> "DUNCAN", "DUNCANNON", "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE",
> "DURHAM", "DUTTON", "EAGLE GROVE", "EAST AURORA", "EAST CHICAGO",
> "EAST EARL", "EAST HANOVER", "EAST HARTFORD", "EAST HARTLAND",
> "EAST LEROY", "EAST LIVERPOOL", "EAST MEADOW", "EAST NORTHPORT",
> "EAST ORANGE", "EAST PEORIA", "EAST SAINT LOUIS", "EASTON", "EDGERTON",
> "EDISON", "EDMOND", "EGG HBR TWP", "EL DORADO", "EL PASO", "ELDRIDGE",
> "ELGIN", "ELIZABETHVILLE", "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY",
> "ELMHURST", "ELMWOOD PARK", "ELOY", "ELY", "EMERSON", "EMORY",
> "ENDICOTT", "ENGLEWOOD", "ENID", "ENNICE", "ENOLA", "ENUMCLAW",
> "EPHRATA", "ERIE", "ESTERO", "EUREKA SPRINGS", "EVANSVILLE",
> "EVERETT", "EXCELSIOR SPRINGS", "EXTON", "FAIRFAX", "FAIRFIELD",
> "FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", "FALMOUTH", "FAR HILLS",
> "FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", "FAYETTE", "FAYETTEVILLE",
> "FELTON", "FENTON", "FERGUSON", "FIELDALE", "FINDLAY", "FINGERVILLE",
> "FLEMINGTON", "FLINT", "FLORENCE", "FLORESVILLE", "FLORISSANT",
> "FLOVILLA", "FLOWER MOUND", "FLUSHING", "FOGELSVILLE", "FOLSOM",
> "FONTANELLE", "FOREST HILLS", "FOREST PARK", "FORT DODGE", "FORT FAIRFIELD",
> "FORT GAY", "FORT LAUDERDALE", "FORT LEE", "FORT MILL", "FORT MOHAVE",
> "FORT PAYNE", "FORT PIERCE", "FORT SMITH", "FORT WAYNE", "FORT WORTH",
> "FOSTORIA", "FOUNTAIN INN", "FRANKFORT", "FRANKLIN", "FREEDOM",
> "FREEHOLD", "FREEPORT", "FREMONT", "FULLERTON", "FULSHEAR", "FULTON",
> "GADSDEN", "GAFFNEY", "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE",
> "GARDNERS", "GARNETT VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA",
> "GETTYSBURG", "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE",
> "GLEN CARBON", "GLEN COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW",
> "GLENWOOD", "GOLDEN CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM",
> "GOSHEN", "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS",
> "GRANDVIEW", "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS",
> "GRASS VALLEY", "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER",
> "GREENLAWN", "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE",
> "GREER", "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD",
> "HADLEY", "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON",
> "HAMBURG", "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE",
> "HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON",
> "HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL",
> "HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE",
> "HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH",
> "HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE",
> "HIGH HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS",
> "HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE",
> "HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD",
> "HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS",
> "HOT SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON",
> "HUFFMAN", "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON",
> "HUNTINGTON BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE",
> "HURST", "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL",
> "INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY",
> "IRMO", "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP",
> "ISSAQUAH", "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS",
> "JACKSONVILLE", "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE",
> "JEFFERSON", "JEFFERSON CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN",
> "JONESBORO", "JONESPORT", "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE",
> "KANNAPOLIS", "KANSAS CITY", "KATY", "KEARNEY", "KELLERTON",
> "KEMAH", "KENMORE", "KENNESAW", "KENT", "KINGMAN", "KINGS MOUNTAIN",
> "KINGSLAND", "KINGSTON", "KINGWOOD", "KIRKLAND", "KUTZTOWN",
> "LA QUINTA", "LA RUE", "LA VERNE", "LAFAYETTE", "LAKE BLUFF",
> "LAKE IN THE HILL", "LAKE ISABELLA", "LAKE PLACID", "LAKE SAINT LOUIS",
> "LAKE TAPPS", "LAKE VILLAGE", "LAKE WAUKOMIS", "LAKE WORTH",
> "LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD RCH", "LAMBERTVILLE",
> "LANCASTER", "LANDISVILLE", "LANSDALE", "LAREDO", "LARGO", "LAS VEGAS",
> "LATROBE", "LAUDERHILL", "LAWNDALE", "LAWRENCE", "LAWRENCEVILLE",
> "LE MARS", "LE ROY", "LEAGUE CITY", "LEAVENWORTH", "LEAWOOD",
> "LEBANON", "LECOMPTON", "LEECHBURG", "LEES SUMMIT", "LEESBURG",
> "LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", "LEVITTOWN", "LEWES",
> "LEWISTOWN", "LEXINGTON", "LIBERTY TWP", "LILLINGTON", "LINCOLN",
> "LINDENHURST", "LINTON", "LISBON", "LITCHFIELD PK", "LITHIA",
> "LITITZ", "LITTLE EGG HARBOR TO", "LK FOREST PK", "LK PEEKSKILL",
> "LOCKHART", "LOCKWOOD", "LOGAN", "LOMA LINDA", "LONE JACK", "LONG BEACH",
> "LONG LANE", "LONGVIEW", "LORAIN", "LORTON", "LOS ANGELES", "LOUISBURG",
> "LOVES PARK", "LOWER BURRELL", "LOWRY CITY", "LUBBOCK", "LUGOFF",
> "LUMBERTON", "LYNBROOK", "LYNNWOOD", "MACHESNEY PARK", "MACON",
> "MAGNOLIA", "MAHOPAC", "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE",
> "MANASSAS", "MANCHACA", "MANCHESTER", "MANCHESTER TOWNSHIP",
> "MANCHESTER TW", "MANHEIM", "MANITO", "MANLIUS", "MANNINGTON",
> "MANSFIELD", "MANSURA", "MAPLE PARK", "MARBLE FALLS", "MARCELLUS",
> "MARICOPA", "MARIETTA", "MARION", "MARKLEYSBURG", "MARS HILL",
> "MARSHALLTOWN", "MARSHFIELD", "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE",
> "MASON CITY", "MASSEY", "MASSILLON", "MASURY", "MATAWAN", "MATEWAN",
> "MATTAPOISETT", "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND",
> "MC DONALD", "MC KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN",
> "MCALESTER", "MCDONOUGH", "MCKEESPORT", "MCKINNEY", "MECHANICSBURG",
> "MECHANICSVILLE", "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS",
> "MERIDEN", "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA",
> "MESQUITE", "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN",
> "MIDLAND", "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK",
> "MILLEDGEVILLE", "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE",
> "MINEOLA", "MINERAL SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION",
> "MISSION HILLS", "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY",
> "MOLINE", "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE",
> "MONTCLAIR", "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON",
> "MORGANTOWN", "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE",
> "MOUNT JOY", "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO",
> "MOUNT UNION", "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT",
> "MULVANE", "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT",
> "MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS",
> "NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES",
> "NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG",
> "NEW BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK",
> "NEW CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON",
> "NEW LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE",
> "NEW ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK",
> "NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT",
> "NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA",
> "NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK",
> "NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON",
> "NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND HILLS",
> "NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE",
> "NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON",
> "OAK RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE",
> "OMAHA", "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION",
> "ORONO", "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO",
> "OTTAWA", "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD",
> "OYSTER BAY", "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR",
> "PALMDALE", "PALMYRA", "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE",
> "PARLIN", "PARMA", "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY",
> "PAXTON", "PEA RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN",
> "PELION", "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE",
> "PENNSBURG", "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS",
> "PERRYSBURG", "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI",
> "PHOENIX", "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN",
> "PINE BLUFF", "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY",
> "PITMAN", "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY",
> "PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO LAKE",
> "POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", "PORT CHARLOTTE",
> "PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", "PORT WASHINGTON",
> "PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", "POTTSVILLE",
> "POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE VILLAGE",
> "PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT PLEASANT",
> "PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", "QUINCY",
> "RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", "READLYN",
> "RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO PARK",
> "REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", "RICH HILL",
> "RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", "RINCON",
> "RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", "RIVERSIDE",
> "ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", "ROCKAWAY BCH",
> "ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", "ROGERS",
> "ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE",
> "ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT",
> "ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN",
> "RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO",
> "SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR",
> "SAINT CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS",
> "SAINT PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY",
> "SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO",
> "SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN",
> "SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", "SARASOTA",
> "SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", "SAYVILLE",
> "SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", "SCOTT", "SCOTTSDALE",
> "SEAFORD", "SEALE", "SEATTLE", "SEDONA", "SEGUIN", "SELDEN",
> "SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", "SHARON", "SHARPSBURG",
> "SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", "SHELLSBURG", "SHELTON",
> "SHENANDOAH", "SHORELINE", "SHOREWOOD", "SHREVEPORT", "SICKLERVILLE",
> "SILER CITY", "SILOAM SPRINGS", "SIMON", "SIMPSONVILLE", "SIMSBURY",
> "SIOUX CITY", "SIOUX FALLS", "SKOKIE", "SKOWHEGAN", "SLATINGTON",
> "SLIDELL", "SMITHFIELD", "SN BERNRDNO", "SNELLVILLE", "SOCORRO",
> "SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH BELOIT", "SOUTH BEND",
> "SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH PARK", "SOUTH PLAINFIELD",
> "SOUTH SIOUX CITY", "SOUTHAMPTON", "SOUTHBURY", "SOUTHGATE",
> "SPARKS", "SPARROW BUSH", "SPENCER", "SPRING", "SPRING BRANCH",
> "SPRING GROVE", "SPRING HILL", "SPRING HOPE", "SPRING VALLEY",
> "SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD GARDENS", "SPRINGHILL",
> "SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", "ST PETERSBURG",
> "ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", "STANLEY",
> "STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", "STILLMAN VALLEY",
> "STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", "STRAFFORD",
> "STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", "SUFFIELD",
> "SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", "SUMMIT",
> "SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", "SWEENY",
> "SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", "TAFTVILLE",
> "TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", "TAYLORVILLE",
> "TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", "THOMASVILLE",
> "TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY PARK",
> "TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", "TOPEKA",
> "TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS RST",
> "TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER",
> "TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY",
> "UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA",
> "URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO",
> "VAN BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO",
> "VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA",
> "VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH",
> "W TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN",
> "WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG",
> "WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA",
> "WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE",
> "WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER",
> "WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO",
> "WEST CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN",
> "WEST HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH",
> "WEST POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD",
> "WEST VALLEY CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER",
> "WESTMORELAND CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD",
> "WHARTON", "WHEELING", "WHITE PLAINS", "WHITEHALL", "WHITESTONE",
> "WHITESTOWN", "WHITING", "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD",
> "WILLARD", "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK",
> "WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS",
> "WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON",
> "WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE",
> "WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS",
> "WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH",
> "YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS",
> "YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor"),
>     clusters = c(6L, 10L, 3L, 10L, 5L, 6L, 10L, 6L, 6L, 7L, 5L,
>     7L, 8L, 8L, 3L, 6L, 3L, 10L, 7L, 10L, 10L, 4L, 2L, 8L, 9L,
>     1L, 4L, 4L, 7L, 5L, 10L, 5L, 5L, 10L, 6L, 6L, 10L, 6L, 7L,
>     10L, 10L, 3L, 3L, 6L, 3L), Longitude = c(-93.883554, -90.582058,
>     -83.868923, -89.544083, -72.902467, -94.458904, -90.588533,
>     -94.527843, -92.93769, -80.029456, -70.58809, -82.888789,
>     -80.113071, -82.6661, -81.372992, -95.840964, -82.016013,
>     -89.92625, -80.276676, -89.778385, -87.641063, -94.888036,
>     -117.663119, -82.530568, -75.574437, -122.209047, -96.799309,
>     -95.746255, -80.553953, -73.923314, -87.713482, -73.9731,
>     -73.8187, -87.987023, -93.759478, -94.110903, -90.00597,
>     -93.257109, -79.932483, -90.201858, -89.100233, -84.625923,
>     -84.567614, -93.912921, -84.612564), Latitude = c(34.503045,
>     38.752515, 33.406527, 40.893588, 41.733088, 38.999032, 41.657674,
>     38.890929, 37.639766, 42.099693, 43.53462, 40.142492, 26.538149,
>     27.9062, 31.90104, 34.833083, 35.118836, 35.136047, 39.425712,
>     40.519207, 40.101126, 30.044457, 33.62287, 27.910276, 39.752254,
>     47.403778, 32.961929, 29.820199, 38.408649, 40.668828, 41.983948,
>     40.58116, 40.7253, 41.921667, 41.669921, 36.442552, 38.564663,
>     37.455315, 40.867774, 38.783554, 42.244381, 34.024332, 34.001644,
>     36.622704, 34.017832), distances_Me = c(1879089.079, 1437296.523,
>     1185638.742, 1313997.97, 156759.9125, 1759443.351, 1398398.997,
>     1767861.917, 1668917.322, 534234.2576, 431749.2965, 758508.4701,
>     1657031.938, 1615575.668, 1169085.702, 2024642.021, 930944.4681,
>     1525322.746, 553069.7532, 1337002.822, 1161367.37, 2223496.376,
>     3918520.212, 1609088.602, 164252.5146, 3878581.001, 2197910.688,
>     2304526.789, 614908.3151, 11075.24597, 1160375.454, 494.2997496,
>     21063.72622, 1182681.434, 1662351.873, 1810881.911, 1393581.436,
>     1701982.013, 504395.772, 1404348.991, 1276505.454, 1190457.718,
>     1188029.899, 1787261.336, 1189994.055), distances_Mi = c(1167.607468,
>     893.0913246, 736.719012, 816.477441, 97.40573055, 1093.263337,
>     868.9216123, 1098.494372, 1037.01328, 331.9565399, 268.2755749,
>     471.3135552, 1029.628072, 1003.868436, 726.4334682, 1258.049536,
>     578.4599174, 947.7880794, 343.6603307, 830.7719404, 721.6375366,
>     1381.611443, 2434.846498, 999.8375755, 102.0614003, 2410.029515,
>     1365.713293, 1431.96122, 382.0848884, 6.881813138, 721.0211909,
>     0.30714248, 13.08834388, 734.8814328, 1032.933715, 1125.225657,
>     865.9281298, 1057.55865, 313.4158337, 872.6187534, 793.1807589,
>     739.7133739, 738.2048023, 1110.548567, 739.4252678), ID = 2308:2352), row.names = c(NA,
> -45L), class = "data.frame")
>
> Proprietary
>
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rmh @end|ng |rom temp|e@edu  Fri May 15 05:38:14 2020
From: rmh @end|ng |rom temp|e@edu (Richard M. Heiberger)
Date: Thu, 14 May 2020 23:38:14 -0400
Subject: [R] [External]  sort
In-Reply-To: <CAJOiR6YRm2YKgTKpMTkkCqoa4j0sNHuYrjgusUbA82ER2QrVbA@mail.gmail.com>
References: <CAJOiR6YRm2YKgTKpMTkkCqoa4j0sNHuYrjgusUbA82ER2QrVbA@mail.gmail.com>
Message-ID: <CAGx1TMD43L7D9=R1K8t-eiw1QaYEaECVNjMiEkaBVba42=C2rw@mail.gmail.com>

## the data you gave us

DF1 <- read.table(text="name ddate
  A  2019-10-28
  A  2018-01-25
  A  2020-01-12
  A  2017-10-20
  B  2020-11-20
  B  2019-10-20
  B  2017-05-20
  B  2020-01-20
  c  2009-10-01  ",
  header=TRUE, colClasses=c("character", "POSIXct"))

DF1

D2 <- split(DF1, DF1$name)
D2

sapply(D2, function(x) {
  DD <- c(sort(x$ddate, decreasing=TRUE), min(x$ddate))
  DD[1]-DD[2]
})


## the data that your intended answer is based on

DF1 <- read.table(text="name ddate
  A  2019-01-12  ## intended value
  A  2018-01-25
  A  2020-01-12
  A  2017-10-20
  B  2020-11-20
  B  2019-10-20
  B  2017-05-20
  B  2020-01-20
  c  2009-10-01  ",
  header=TRUE, colClasses=c("character", "POSIXct"))

DF1

D2 <- split(DF1, DF1$name)
D2

sapply(D2, function(x) {
  DD <- c(sort(x$ddate, decreasing=TRUE), min(x$ddate))
  DD[1]-DD[2]
})

On Thu, May 14, 2020 at 11:00 PM Val <valkremk at gmail.com> wrote:

> HI All,
> I have a sample of data frame
> DF1<-read.table(text="name ddate
>   A  2019-10-28
>   A  2018-01-25
>   A  2020-01-12
>   A  2017-10-20
>   B  2020-11-20
>   B  2019-10-20
>   B  2017-05-20
>   B  2020-01-20
>   c  2009-10-01  ",header=TRUE)
>
> 1. I want sort by name and ddate on decreasing order and the output
> should like as follow
>    A  2020-01-12
>    A  2019-01-12
>    A  2018-01-25
>    A  2017-10-20
>    B  2020-11-21
>   B  2020-11-01
>   B  2019-10-20
>   B  2017-05-20
>   c  2009-10-01
>
> 2.  Take the top two rows by group( names) and the out put should like
>    A  2020-01-12
>    A  2019-01-12
>    B  2020-11-21
>    B  2020-11-01
>     c  2009-10-01
>
> 3.  Within each group (name) get the date difference  between the
> first and second rows dates. If a group has only one row then the
> difference should be 0
>
> The final out put is
> Name diff
>    A  365
>     B  20
>     C  0
>
> Here is my attempt and have an issue at the sorting
> DF1$DTime <- as.POSIXct(DF1$ddate , format = "%Y-%m-%d")
> DF2 <- DF1[order(DF1$name, ((as.Date(DF1$DTime, decreasing = TRUE)))), ]
>
> not working
> Any help?
>
> Thank you
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Fri May 15 05:58:14 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 15 May 2020 13:58:14 +1000
Subject: [R] sort
In-Reply-To: <CAJOiR6YRm2YKgTKpMTkkCqoa4j0sNHuYrjgusUbA82ER2QrVbA@mail.gmail.com>
References: <CAJOiR6YRm2YKgTKpMTkkCqoa4j0sNHuYrjgusUbA82ER2QrVbA@mail.gmail.com>
Message-ID: <CA+8X3fU3RxeyG6iy3QW-eQYJ8KkQQ15HZYyZYFRu597tdm6f=Q@mail.gmail.com>

Hi Val,
Your problem is keeping the orders straight. You want dates decreasing
and names increasing:

DF1<-read.table(text="name ddate
  A  2019-10-28
  A  2018-01-25
  A  2020-01-12
  A  2017-10-20
  B  2020-11-20
  B  2019-10-20
  B  2017-05-20
  B  2020-01-20
  C  2009-10-01  ",header=TRUE)
DF1$Time<-as.POSIXct(DF1$ddate , format = "%Y-%m-%d")
# get dates in decreasing order
DF2<-DF1[order(DF1$Time,decreasing=TRUE),]
# create the final output with the names in increasing order
DF3<-data.frame(name=sort(unique(DF2$name)))
# create a function that returns the first diff of a vector
getdate1diff<-function(x)
 return(ifelse(length(x)>1,abs(diff(x))[1],0))
# apply the function by the unique names in DF2
DF3$date1diff<-by(DF2$Time,DF2$name,getdate1diff)
DF3

Jim

On Fri, May 15, 2020 at 1:00 PM Val <valkremk at gmail.com> wrote:
>
> HI All,
> I have a sample of data frame
> DF1<-read.table(text="name ddate
>   A  2019-10-28
>   A  2018-01-25
>   A  2020-01-12
>   A  2017-10-20
>   B  2020-11-20
>   B  2019-10-20
>   B  2017-05-20
>   B  2020-01-20
>   c  2009-10-01  ",header=TRUE)
>
> 1. I want sort by name and ddate on decreasing order and the output
> should like as follow
>    A  2020-01-12
>    A  2019-01-12
>    A  2018-01-25
>    A  2017-10-20
>    B  2020-11-21
>   B  2020-11-01
>   B  2019-10-20
>   B  2017-05-20
>   c  2009-10-01
>
> 2.  Take the top two rows by group( names) and the out put should like
>    A  2020-01-12
>    A  2019-01-12
>    B  2020-11-21
>    B  2020-11-01
>     c  2009-10-01
>
> 3.  Within each group (name) get the date difference  between the
> first and second rows dates. If a group has only one row then the
> difference should be 0
>
> The final out put is
> Name diff
>    A  365
>     B  20
>     C  0
>
> Here is my attempt and have an issue at the sorting
> DF1$DTime <- as.POSIXct(DF1$ddate , format = "%Y-%m-%d")
> DF2 <- DF1[order(DF1$name, ((as.Date(DF1$DTime, decreasing = TRUE)))), ]
>
> not working
> Any help?
>
> Thank you
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From den|z343 @end|ng |rom gm@||@com  Thu May 14 17:55:54 2020
From: den|z343 @end|ng |rom gm@||@com (Deniz Ugur)
Date: Thu, 14 May 2020 18:55:54 +0300
Subject: [R] gdistance::accCost fails on some cases
In-Reply-To: <3958C26E-8523-4F18-9219-CC8AF91979BC@gmail.com>
References: <3958C26E-8523-4F18-9219-CC8AF91979BC@gmail.com>
Message-ID: <CAMCM02stB76J6avZDwtZ=FOca2VUj4Gx+WT=6moDYzYs0wvPbg@mail.gmail.com>

Referring to my mail bellow, this situation is not a expected behavior. Do
you think that this is because of a fault in my codes or some bug in
gdistance? What could be the problem?

Thank you for your consideration.

---------- Forwarded message ---------
From: Deniz U?ur <deniz343 at gmail.com>
Date: Thu, May 14, 2020, 02:18
Subject: gdistance::accCost fails when nrows is bigger than 90
To: <r-help at r-project.org>


Hello all,

It has been only 2 days since I started using R and I was trying movecost
library. Before explaining the unexpected behavior I should say that I?m
calling R function from Python using rpy2 package. That shouldn?t be an
issue because everything works as expected when a matrix of size 100x90 is
given.

Okay now, basically I?m converting an 2D matrix to RasterLayer with
appropriate CRS and extent properties then transfer it to R runtime. A
modified version of Tobler?s hiking function is applied then a
shortest-path is calculated. This procedure works for a matrix with column
size anywhere between 1 and memory?s limit. However if row size of the
matrix exceeds 90 then function is halted on gdistance::accCost saying that:

"At structural_properties.c:5313 : cannot run Bellman-Ford algorithm,
Negative loop detected while calculating shortest paths?

It?s very interesting issue because I have explicitly tried ?88, 89, 90, 91
and it halted on +90

I?ve created a gist in case you want to try the application on your
computer. Also I?ve included a clean version of the R script I was using
bellow.

Thank you.

https://gist.github.com/DenizUgur/01e18edd03321f3d3928c3afb2bd7145

movecost <- function (dtm, origin, destin, time="s") {

altDiff <- function(x){x[2] - x[1]}
hd <- gdistance::transition(dtm, altDiff, 8, symm=FALSE)

slope <- gdistance::geoCorrection(hd)

cost_function <- function(x){ ifelse(x[adj] > 0, 1.3 * exp(-3.5 * abs(x[adj]
+ 0.05)), 5.3 * exp(-3.5 * abs(x[adj] + 0.05))) }

adj <- raster::adjacent(dtm, cells=1:ncell(dtm), pairs=TRUE, directions=8)
speed <- slope
speed[adj] <- cost_function(slope)
speed <- speed * 0.278
Conductance <- gdistance::geoCorrection(speed)

accum_final <- gdistance::accCost(Conductance, sp::coordinates(origin))

accum_final <- raster::mask(accum_final, dtm)

sPath <- gdistance::shortestPath(Conductance, sp::coordinates(origin), sp::
coordinates(destin), output="SpatialLines")
sPath$length <- rgeos::gLength(sPath, byid=TRUE)
destin$cost <- raster::extract(accum_final, destin)
results <- list("accumulated.cost.raster"=accum_final,
"LCPs"=sPath,
"dest.loc.w.cost"=destin)
}

	[[alternative HTML version deleted]]


From petr@p|k@| @end|ng |rom prechez@@cz  Fri May 15 09:15:06 2020
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Fri, 15 May 2020 07:15:06 +0000
Subject: [R] How to find a split point in a curve?
In-Reply-To: <CAMk+s2TvXxGHY_LPQQRNSXkiqdDgQW2iuObOQbeVGOVzYArzZQ@mail.gmail.com>
References: <CAMk+s2TvXxGHY_LPQQRNSXkiqdDgQW2iuObOQbeVGOVzYArzZQ@mail.gmail.com>
Message-ID: <fae02458b9af4bea81dace31d1f1fe15@SRVEXCHCM1302.precheza.cz>

Hi

Maybe I am wrong but it seems to me that it is cumulative data from recent
epidemy in some state.

If yes, instead of inventing wheel I would go to canned and proved solution
using tools from
https://www.repidemicsconsortium.org/

I found useful and enlightening this blog especially first part from
February 18th.
https://timchurches.github.io/blog/

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Luigi Marongiu
> Sent: Thursday, May 14, 2020 5:12 PM
> To: r-help <r-help at r-project.org>
> Subject: [R] How to find a split point in a curve?
> 
> Dear all,
> I am trying to find a turning point in some data. In the initial phase,
the
> data increases more or less exponentially (thus it is linear in a nat log
> transform), then reaches a plateau. I would like to find the point that
> marks the end of the exponential phase.
> I understand that the function spline can build a curve; is it possible
> with it to find the turning points? I have no idea of how to use spline
> though.
> Here is a working example.
> Thank you
> 
> ```
> Y = c(259, 716, 1404, 2173, 3944, 5403, 7140, 9121,
>       11220, 13809, 16634, 19869, 23753, 27447,
>       30590, 33975, 36627, 39600, 42067, 44082,
>       58190, 63280, 65921, 67929, 69977, 71865,
>       73614, 74005, 74894, 75717, 76365, 76579,
>       77087, 77493, 77926, 78253, 78680, 79253,
>       79455, 79580, 79699, 79838, 79981, 80080,
>       80124, 80164, 80183, 80207, 80222, 80230,
>       80241, 80261, 80261, 80277, 80290, 80303,
>       80337, 80376, 80422, 80461, 80539, 80586,
>       80653, 80708, 80762, 80807, 80807, 80886,
>       80922, 80957, 80988, 81007, 81037, 81076,
>       81108, 81108, 81171, 81213, 81259, 81358,
>       81466, 81555, 81601, 81647, 81673, 81998,
>       82025, 82041, 82053, 82064, 82094, 82104,
>       82110, 82122, 82133, 82136, 82142, 82164,
>       82168, 82180, 82181, 82184, 82187, 82188,
>       82190, 82192, 82193, 82194)
> Y = log(Y)
> X = 1:length(Y)
> plot(Y ~ X, ylab = "Ln(Y)", xlim=c(0,10, main="zoomed in"))
> abline(lm(Y[1:3] ~ X[1:3]))
> abline(lm(Y[1:5] ~ X[1:5]), lty=2)
> text(7, 6, "After third or fifth point, there is deviance", pos=3)
> text(2.5, 10, "Solid line: linear model points 1:3", pos =3)
> text(2.5, 9, "Dashed line: linear model points 1:5", pos =3)
> plot(Y ~ X, ylab = "Ln(Y)", xlim=c(0,10, main="overall"))
> abline(lm(Y[1:3] ~ X[1:3]))
> ```
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From emm@nue|@po|zot @end|ng |rom |ecn@m@net  Fri May 15 11:38:31 2020
From: emm@nue|@po|zot @end|ng |rom |ecn@m@net (Poizot Emmanuel)
Date: Fri, 15 May 2020 11:38:31 +0200
Subject: [R] Date from text
Message-ID: <b2831f71-d154-dc16-5044-af4335123fbe@lecnam.net>

Dear all,

I've a data frame with a column "Date":

[1] 11-1993 11-1993 11-1993 11-1993 11-1993 11-1993 11-1996 11-1996 11-1996
[10] 11-1996 11-1996 11-1996 02-1998 02-1998 02-1998 02-1998 02-1998 02-1998
[19] 11-1998 11-1998 11-1998 11-1998 11-1998 11-1998 10-2001 10-2001 10-2001
[28] 10-2001 10-2001 10-2001 02-2003 02-2003 02-2003 02-2003 02-2003 02-2003
[37] 11-2004 11-2004 11-2004 11-2004 11-2004 11-2004 11-2005 11-2005 11-2005
[46] 11-2005 11-2005 11-2005 11-2007 11-2007 11-2007 11-2007 11-2007 11-2007
[55] 10-2008 10-2008 10-2008 10-2008 10-2008 10-2008 03-2009 03-2009 03-2009
[64] 03-2009 03-2009 03-2009 10-2012 10-2012 10-2012 10-2012 10-2012 10-2012
[73] 12-2017 12-2017 12-2017 12-2017 12-2017 12-2017 12-2018 12-2018 12-2018
[82] 12-2018 12-2018 12-2018

I want to convert that into real dates:
as.POSIXct(Date, format="%m-%Y") always return "NA" values.
Where am I wrong ?

regards

From m@r||n- @end|ng |rom gmx@cn  Fri May 15 12:34:34 2020
From: m@r||n- @end|ng |rom gmx@cn (Jialin Ma)
Date: Fri, 15 May 2020 06:34:34 -0400
Subject: [R] Notational derivative in R
In-Reply-To: <7C22DB28-84C0-4933-A2FA-E0819947F9DF@dcn.davis.ca.us>
References: <CA+dpOJkUYNfHo3HTeDOCjD7NEV=e1CWf_T5UC9gMLic6QbpmqA@mail.gmail.com>
 <CAGgJW74mtXXw2_gRj5UKO62TrxH6TriNs+WBP5kMksmZiNO3=w@mail.gmail.com>
 <7C22DB28-84C0-4933-A2FA-E0819947F9DF@dcn.davis.ca.us>
Message-ID: <2482707.Z6HOKt64ns@plastic>

Hi,

There is a new package published on CRAN called symengine (I am the
maintainer). That might be suitable for your need.

Best,
Jialin

On Thursday, May 14, 2020 2:00:20 PM EDT Jeff Newmiller wrote:
> FWIW I have found all such tools to require babysitting... and for
> interactive use I prefer wxMaxima and some manual translation to R.
> On May 14, 2020 10:50:56 AM PDT, Eric Berger <ericjberger at gmail.com> wrote:
> >Hi Christofer,
> >Look at https://cran.r-project.org/web/views/NumericalMathematics.html
> >and within that page search for Symbolic Mathematics. It shows two
> >packages: Ryacas and rSymPy.
> >I have no experience with them but they may be a good place to start.
> >
> >HTH,
> >Eric
> >
> >
> >On Thu, May 14, 2020 at 8:43 PM Christofer Bogaso <
> >
> >bogaso.christofer at gmail.com> wrote:
> >> Hi,
> >>
> >> I was wondering if R can perform notational derivative something like
> >> Mathematica does as explained in
> >> https://reference.wolfram.com/language/howto/TakeADerivative.html
> >>
> >> Any pointer will be highly appreciated.
> >>
> >> Thanks,
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> >	[[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.


From er|cjberger @end|ng |rom gm@||@com  Fri May 15 13:30:43 2020
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Fri, 15 May 2020 14:30:43 +0300
Subject: [R] Notational derivative in R
In-Reply-To: <2482707.Z6HOKt64ns@plastic>
References: <CA+dpOJkUYNfHo3HTeDOCjD7NEV=e1CWf_T5UC9gMLic6QbpmqA@mail.gmail.com>
 <CAGgJW74mtXXw2_gRj5UKO62TrxH6TriNs+WBP5kMksmZiNO3=w@mail.gmail.com>
 <7C22DB28-84C0-4933-A2FA-E0819947F9DF@dcn.davis.ca.us>
 <2482707.Z6HOKt64ns@plastic>
Message-ID: <CAGgJW74XcwnbMGQmVc-kvDVwe8qpgTMYro9U1CvsdPCB-rwrPg@mail.gmail.com>

Hi Jialin,
I was not aware of symengine. I just did a search and found that you were
recognized by the 'John Chambers Statistical Software Award' committee for
2020 for your work on the symengine package. Congratulations!




On Fri, May 15, 2020 at 1:34 PM Jialin Ma <marlin- at gmx.cn> wrote:

> Hi,
>
> There is a new package published on CRAN called symengine (I am the
> maintainer). That might be suitable for your need.
>
> Best,
> Jialin
>
> On Thursday, May 14, 2020 2:00:20 PM EDT Jeff Newmiller wrote:
> > FWIW I have found all such tools to require babysitting... and for
> > interactive use I prefer wxMaxima and some manual translation to R.
> > On May 14, 2020 10:50:56 AM PDT, Eric Berger <ericjberger at gmail.com>
> wrote:
> > >Hi Christofer,
> > >Look at https://cran.r-project.org/web/views/NumericalMathematics.html
> > >and within that page search for Symbolic Mathematics. It shows two
> > >packages: Ryacas and rSymPy.
> > >I have no experience with them but they may be a good place to start.
> > >
> > >HTH,
> > >Eric
> > >
> > >
> > >On Thu, May 14, 2020 at 8:43 PM Christofer Bogaso <
> > >
> > >bogaso.christofer at gmail.com> wrote:
> > >> Hi,
> > >>
> > >> I was wondering if R can perform notational derivative something like
> > >> Mathematica does as explained in
> > >> https://reference.wolfram.com/language/howto/TakeADerivative.html
> > >>
> > >> Any pointer will be highly appreciated.
> > >>
> > >> Thanks,
> > >>
> > >> ______________________________________________
> > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> PLEASE do read the posting guide
> > >> http://www.R-project.org/posting-guide.html
> > >> and provide commented, minimal, self-contained, reproducible code.
> > >
> > >     [[alternative HTML version deleted]]
> > >
> > >______________________________________________
> > >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >https://stat.ethz.ch/mailman/listinfo/r-help
> > >PLEASE do read the posting guide
> > >http://www.R-project.org/posting-guide.html
> > >and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
>

	[[alternative HTML version deleted]]


From m@rc_@chw@rtz @end|ng |rom me@com  Fri May 15 13:44:02 2020
From: m@rc_@chw@rtz @end|ng |rom me@com (Marc Schwartz)
Date: Fri, 15 May 2020 07:44:02 -0400
Subject: [R] Date from text
In-Reply-To: <b2831f71-d154-dc16-5044-af4335123fbe@lecnam.net>
References: <b2831f71-d154-dc16-5044-af4335123fbe@lecnam.net>
Message-ID: <BADD13C5-05C5-49EB-9DC0-B8BEA59AE145@me.com>

Hi, 

For the usual R text to date conversions, you need a complete date. Since you are missing the day of the month in your source text, you would need to impute that part before making the conversion.

Also, since you don't appear to need to worry about time of day, just use as.Date(), instead of as.POSIXct().

In this case, use ?sub to add the day of the month to the source text, for which I will impute the 15th, and then make the conversion:

## Single value for now
x <- "11-1993"

> sub("-", "-15-", x)
[1] "11-15-1993"

> as.Date(sub("-", "-15-", x), format = "%m-%d-%Y")
[1] "1993-11-15"


Regards,

Marc Schwartz



> On May 15, 2020, at 5:38 AM, Poizot Emmanuel <emmanuel.poizot at lecnam.net> wrote:
> 
> Dear all,
> 
> I've a data frame with a column "Date":
> 
> [1] 11-1993 11-1993 11-1993 11-1993 11-1993 11-1993 11-1996 11-1996 11-1996
> [10] 11-1996 11-1996 11-1996 02-1998 02-1998 02-1998 02-1998 02-1998 02-1998
> [19] 11-1998 11-1998 11-1998 11-1998 11-1998 11-1998 10-2001 10-2001 10-2001
> [28] 10-2001 10-2001 10-2001 02-2003 02-2003 02-2003 02-2003 02-2003 02-2003
> [37] 11-2004 11-2004 11-2004 11-2004 11-2004 11-2004 11-2005 11-2005 11-2005
> [46] 11-2005 11-2005 11-2005 11-2007 11-2007 11-2007 11-2007 11-2007 11-2007
> [55] 10-2008 10-2008 10-2008 10-2008 10-2008 10-2008 03-2009 03-2009 03-2009
> [64] 03-2009 03-2009 03-2009 10-2012 10-2012 10-2012 10-2012 10-2012 10-2012
> [73] 12-2017 12-2017 12-2017 12-2017 12-2017 12-2017 12-2018 12-2018 12-2018
> [82] 12-2018 12-2018 12-2018
> 
> I want to convert that into real dates:
> as.POSIXct(Date, format="%m-%Y") always return "NA" values.
> Where am I wrong ?
> 
> regards


From e@ @end|ng |rom enr|co@chum@nn@net  Fri May 15 13:57:22 2020
From: e@ @end|ng |rom enr|co@chum@nn@net (Enrico Schumann)
Date: Fri, 15 May 2020 13:57:22 +0200
Subject: [R] Date from text
In-Reply-To: <b2831f71-d154-dc16-5044-af4335123fbe@lecnam.net> (Poizot
 Emmanuel's message of "Fri, 15 May 2020 11:38:31 +0200")
References: <b2831f71-d154-dc16-5044-af4335123fbe@lecnam.net>
Message-ID: <87blmpv2al.fsf@enricoschumann.net>

On Fri, 15 May 2020, Poizot Emmanuel writes:

> Dear all,
>
> I've a data frame with a column "Date":
>
> [1] 11-1993 11-1993 11-1993 11-1993 11-1993 11-1993 11-1996 11-1996 11-1996
> [10] 11-1996 11-1996 11-1996 02-1998 02-1998 02-1998 02-1998 02-1998 02-1998
> [19] 11-1998 11-1998 11-1998 11-1998 11-1998 11-1998 10-2001 10-2001 10-2001
> [28] 10-2001 10-2001 10-2001 02-2003 02-2003 02-2003 02-2003 02-2003 02-2003
> [37] 11-2004 11-2004 11-2004 11-2004 11-2004 11-2004 11-2005 11-2005 11-2005
> [46] 11-2005 11-2005 11-2005 11-2007 11-2007 11-2007 11-2007 11-2007 11-2007
> [55] 10-2008 10-2008 10-2008 10-2008 10-2008 10-2008 03-2009 03-2009 03-2009
> [64] 03-2009 03-2009 03-2009 10-2012 10-2012 10-2012 10-2012 10-2012 10-2012
> [73] 12-2017 12-2017 12-2017 12-2017 12-2017 12-2017 12-2018 12-2018 12-2018
> [82] 12-2018 12-2018 12-2018
>
> I want to convert that into real dates:
> as.POSIXct(Date, format="%m-%Y") always return "NA" values.
> Where am I wrong ?
>
> regards

If you really want a date, I'd suggest 'as.Date'. The help
for ?as.Date says:

  "If the date string does not specify the date completely,
   the returned answer may be system-specific."

So perhaps try something like

    as.Date(paste0("01-", "11-1993"), format = "%d-%m-%Y")
    ## [1] "1993-11-01"

Or look at 'yearmon' in package 'zoo':

    library("zoo")
    as.yearmon("11-1993", format = "%m-%Y")
    ## [1] "Nov 1993"

-- 
Enrico Schumann
Lucerne, Switzerland
http://enricoschumann.net


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri May 15 14:29:18 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 15 May 2020 13:29:18 +0100
Subject: [R] sort
In-Reply-To: <CAJOiR6YRm2YKgTKpMTkkCqoa4j0sNHuYrjgusUbA82ER2QrVbA@mail.gmail.com>
References: <CAJOiR6YRm2YKgTKpMTkkCqoa4j0sNHuYrjgusUbA82ER2QrVbA@mail.gmail.com>
Message-ID: <4308a3b5-7ac9-90b9-6ab9-75ab0f54670b@sapo.pt>

Hello,

Here is a dplyr solution. arrange() sorts by name and desc(ddate) and 
top_n keeps the first 2 after grouping by ddate. Then it's a matter of 
being careful with diff() in the summarise instruction.


library(dplyr)

DF1 %>%
   mutate(ddate = as.Date(ddate)) %>%
   arrange(name, desc(ddate)) %>%
   group_by(name) %>%
   top_n(2) %>%
   summarise(diff = ifelse(n() > 1, diff(rev(ddate)), 0))
#Selecting by ddate
## A tibble: 3 x 2
#  name   diff
#  <chr> <dbl>
#1 A        76
#2 B       305
#3 c         0


Hope this helps,

Rui Barradas

?s 03:58 de 15/05/20, Val escreveu:
> HI All,
> I have a sample of data frame
> DF1<-read.table(text="name ddate
>    A  2019-10-28
>    A  2018-01-25
>    A  2020-01-12
>    A  2017-10-20
>    B  2020-11-20
>    B  2019-10-20
>    B  2017-05-20
>    B  2020-01-20
>    c  2009-10-01  ",header=TRUE)
> 
> 1. I want sort by name and ddate on decreasing order and the output
> should like as follow
>     A  2020-01-12
>     A  2019-01-12
>     A  2018-01-25
>     A  2017-10-20
>     B  2020-11-21
>    B  2020-11-01
>    B  2019-10-20
>    B  2017-05-20
>    c  2009-10-01
> 
> 2.  Take the top two rows by group( names) and the out put should like
>     A  2020-01-12
>     A  2019-01-12
>     B  2020-11-21
>     B  2020-11-01
>      c  2009-10-01
> 
> 3.  Within each group (name) get the date difference  between the
> first and second rows dates. If a group has only one row then the
> difference should be 0
> 
> The final out put is
> Name diff
>     A  365
>      B  20
>      C  0
> 
> Here is my attempt and have an issue at the sorting
> DF1$DTime <- as.POSIXct(DF1$ddate , format = "%Y-%m-%d")
> DF2 <- DF1[order(DF1$name, ((as.Date(DF1$DTime, decreasing = TRUE)))), ]
> 
> not working
> Any help?
> 
> Thank you
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Fri May 15 19:00:04 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Fri, 15 May 2020 12:00:04 -0500
Subject: [R] how to extract strings in any column and in any row that start
 with
Message-ID: <CAF9-5jMgB+dG-s1XCYPNn85LA3n5To3vBBTu4R4h0Eco8wwFeA@mail.gmail.com>

Hello,

I have a data frame:

> dim(tot)
[1] 502536   1093

How would I extract from it all strings that start with E10?

I know how to extract all rows that contain with E10
df0<-tot %>% filter_all(any_vars(. %in% c('E10')))
> dim(df0)
[1] 5105 1093

but I just need a vector of strings that start with E10...
it would look something like this:

[1] "E102" "E109" "E108" "E103" "E104" "E105" "E101" "E106" "E107"

Thanks
Ana


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri May 15 19:13:50 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 15 May 2020 10:13:50 -0700
Subject: [R] 
 how to extract strings in any column and in any row that start with
In-Reply-To: <CAF9-5jMgB+dG-s1XCYPNn85LA3n5To3vBBTu4R4h0Eco8wwFeA@mail.gmail.com>
References: <CAF9-5jMgB+dG-s1XCYPNn85LA3n5To3vBBTu4R4h0Eco8wwFeA@mail.gmail.com>
Message-ID: <6BFCD271-9DAA-49BF-8D01-4815CD26028F@dcn.davis.ca.us>

Read about regular expressions... they are extremely useful.

df1 <- tot %>% filter_all(any_vars(grepl( '^E10', .)))

It is bad form not to put spaces around the <- assignment.


On May 15, 2020 10:00:04 AM PDT, Ana Marija <sokovic.anamarija at gmail.com> wrote:
>Hello,
>
>I have a data frame:
>
>> dim(tot)
>[1] 502536   1093
>
>How would I extract from it all strings that start with E10?
>
>I know how to extract all rows that contain with E10
>df0<-tot %>% filter_all(any_vars(. %in% c('E10')))
>> dim(df0)
>[1] 5105 1093
>
>but I just need a vector of strings that start with E10...
>it would look something like this:
>
>[1] "E102" "E109" "E108" "E103" "E104" "E105" "E101" "E106" "E107"
>
>Thanks
>Ana
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @gr@w@|@nur@g1999 @end|ng |rom gm@||@com  Fri May 15 16:32:09 2020
From: @gr@w@|@nur@g1999 @end|ng |rom gm@||@com (Anurag Agrawal)
Date: Fri, 15 May 2020 20:02:09 +0530
Subject: [R] [R-pkgs] Package "roptions"
Message-ID: <CAPxR1s4AkpjaxW7QkjrDi_ot2jUcvDbROokBavNVKWWfY=tJDQ@mail.gmail.com>

Thanks all,
My package "roptions" is now available on CRAN. My package contains a
collection of tools to develop options strategies, value option contracts
using the Black-Scholes-Merten option pricing model and calculate the
option Greeks. For more information see Hull, John C. "Options, Futures,
and Other Derivatives" (1997, ISBN:0-13-601589-1). Fischer Black, Myron
Scholes (1973) "The Pricing of Options and Corporate Liabilities"


Download using: install.packages("roptions")


https://cran.r-project.org/package=roptions


Regards,
Anurag Agrawal
Bachelor of Science in Finance,
Narsee Monjee Institute of Management Studies.
___________________________________________
Disclaimer: The content of this message is confidential. If you have
received it by mistake, please inform us by an email reply and then delete
the message. It is forbidden to copy, forward, or in any way reveal the
contents of this message to anyone. The integrity and security of this
email cannot be guaranteed over the Internet. Therefore, the sender will
not be held liable for any damage caused by the message.

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Fri May 15 21:24:17 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Fri, 15 May 2020 14:24:17 -0500
Subject: [R] 
 how to extract strings in any column and in any row that start with
In-Reply-To: <6BFCD271-9DAA-49BF-8D01-4815CD26028F@dcn.davis.ca.us>
References: <CAF9-5jMgB+dG-s1XCYPNn85LA3n5To3vBBTu4R4h0Eco8wwFeA@mail.gmail.com>
 <6BFCD271-9DAA-49BF-8D01-4815CD26028F@dcn.davis.ca.us>
Message-ID: <CAF9-5jOcfoDohWfOpOV85ieSNM-KUH=FMB2WCg91EWKWrfwVRA@mail.gmail.com>

Hello,

this command was running for more than 2 hours
grep("E10",tot,value=T)
and no output

and this command
df1 <- tot %>% filter_all(any_vars(grepl( '^E10', .)))

gave me a subset (a data frame) of tot where ^E10

what I need is just a vector or all values in tot which start with E10.

Thanks
Ana

On Fri, May 15, 2020 at 12:13 PM Jeff Newmiller
<jdnewmil at dcn.davis.ca.us> wrote:
>
> Read about regular expressions... they are extremely useful.
>
> df1 <- tot %>% filter_all(any_vars(grepl( '^E10', .)))
>
> It is bad form not to put spaces around the <- assignment.
>
>
> On May 15, 2020 10:00:04 AM PDT, Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >Hello,
> >
> >I have a data frame:
> >
> >> dim(tot)
> >[1] 502536   1093
> >
> >How would I extract from it all strings that start with E10?
> >
> >I know how to extract all rows that contain with E10
> >df0<-tot %>% filter_all(any_vars(. %in% c('E10')))
> >> dim(df0)
> >[1] 5105 1093
> >
> >but I just need a vector of strings that start with E10...
> >it would look something like this:
> >
> >[1] "E102" "E109" "E108" "E103" "E104" "E105" "E101" "E106" "E107"
> >
> >Thanks
> >Ana
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri May 15 22:34:11 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 15 May 2020 13:34:11 -0700
Subject: [R] 
 how to extract strings in any column and in any row that start with
In-Reply-To: <CAF9-5jOcfoDohWfOpOV85ieSNM-KUH=FMB2WCg91EWKWrfwVRA@mail.gmail.com>
References: <CAF9-5jMgB+dG-s1XCYPNn85LA3n5To3vBBTu4R4h0Eco8wwFeA@mail.gmail.com>
 <6BFCD271-9DAA-49BF-8D01-4815CD26028F@dcn.davis.ca.us>
 <CAF9-5jOcfoDohWfOpOV85ieSNM-KUH=FMB2WCg91EWKWrfwVRA@mail.gmail.com>
Message-ID: <A5F6AF9F-5FC4-4782-A25C-10D3BED7F975@dcn.davis.ca.us>

If you want to treat your data frame as if it were a vector, then convert it to a vector before you give it to grep.

unlist(tot)

On May 15, 2020 12:24:17 PM PDT, Ana Marija <sokovic.anamarija at gmail.com> wrote:
>Hello,
>
>this command was running for more than 2 hours
>grep("E10",tot,value=T)
>and no output
>
>and this command
>df1 <- tot %>% filter_all(any_vars(grepl( '^E10', .)))
>
>gave me a subset (a data frame) of tot where ^E10
>
>what I need is just a vector or all values in tot which start with E10.
>
>Thanks
>Ana
>
>On Fri, May 15, 2020 at 12:13 PM Jeff Newmiller
><jdnewmil at dcn.davis.ca.us> wrote:
>>
>> Read about regular expressions... they are extremely useful.
>>
>> df1 <- tot %>% filter_all(any_vars(grepl( '^E10', .)))
>>
>> It is bad form not to put spaces around the <- assignment.
>>
>>
>> On May 15, 2020 10:00:04 AM PDT, Ana Marija
><sokovic.anamarija at gmail.com> wrote:
>> >Hello,
>> >
>> >I have a data frame:
>> >
>> >> dim(tot)
>> >[1] 502536   1093
>> >
>> >How would I extract from it all strings that start with E10?
>> >
>> >I know how to extract all rows that contain with E10
>> >df0<-tot %>% filter_all(any_vars(. %in% c('E10')))
>> >> dim(df0)
>> >[1] 5105 1093
>> >
>> >but I just need a vector of strings that start with E10...
>> >it would look something like this:
>> >
>> >[1] "E102" "E109" "E108" "E103" "E104" "E105" "E101" "E106" "E107"
>> >
>> >Thanks
>> >Ana
>> >
>> >______________________________________________
>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >PLEASE do read the posting guide
>> >http://www.R-project.org/posting-guide.html
>> >and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Sent from my phone. Please excuse my brevity.

-- 
Sent from my phone. Please excuse my brevity.


From cpoiw@rt m@iii@g oii chemo@org@uk  Fri May 15 22:43:49 2020
From: cpoiw@rt m@iii@g oii chemo@org@uk (cpoiw@rt m@iii@g oii chemo@org@uk)
Date: Fri, 15 May 2020 21:43:49 +0100
Subject: [R] 
 how to extract strings in any column and in any row that start with
In-Reply-To: <A5F6AF9F-5FC4-4782-A25C-10D3BED7F975@dcn.davis.ca.us>
References: <CAF9-5jMgB+dG-s1XCYPNn85LA3n5To3vBBTu4R4h0Eco8wwFeA@mail.gmail.com>
 <6BFCD271-9DAA-49BF-8D01-4815CD26028F@dcn.davis.ca.us>
 <CAF9-5jOcfoDohWfOpOV85ieSNM-KUH=FMB2WCg91EWKWrfwVRA@mail.gmail.com>
 <A5F6AF9F-5FC4-4782-A25C-10D3BED7F975@dcn.davis.ca.us>
Message-ID: <f5b2718065f98e66f3cf605cf877dc51@chemo.org.uk>

This is almost certainly not the most efficient way:

tot <- data.frame(v1 = paste0(LETTERS[seq(1:5)],seq(1:10)),
              v2 = paste0(LETTERS[seq(1:5)],seq(from = 101, to=110, by = 
1)),
              v3 = paste0(LETTERS[seq(1:5)],seq(from = 111, to=120, by = 
1)),
              v4 = paste0(LETTERS[seq(1:5)],seq(from = 121, to=130, by = 
1)),
              v5 = paste0(LETTERS[seq(1:5)],seq(from = 131, to=140, by = 
1)),
              v6 = paste0(LETTERS[seq(1:5)],seq(from = 101, to=110, by = 
1))
              )

# set a variable to hold the result
myResult <- NULL

# iterate through each variable
for (v in 1:length(tot[1,])) {
   thisResult <- as.character(tot[grepl ('^E10', tot[,v]),v])
   myResult <- c(myResult, thisResult)
}

myResult <- unique( myResult )


===

Indeed as I wrote this Jeff has popped along with unlist!

Using my example above:

unique ( as.character( unlist (tot) )[grepl ('^E10', as.character( 
unlist (tot) ) )] )

does what you wanted (you may not need the as.characters if you are on R 
4.o, or if your df has chars rather than factors.

On 2020-05-15 21:34, Jeff Newmiller wrote:
> If you want to treat your data frame as if it were a vector, then
> convert it to a vector before you give it to grep.
> 
> unlist(tot)
> 
> On May 15, 2020 12:24:17 PM PDT, Ana Marija 
> <sokovic.anamarija at gmail.com> wrote:
>> Hello,
>> 
>> this command was running for more than 2 hours
>> grep("E10",tot,value=T)
>> and no output
>> 
>> and this command
>> df1 <- tot %>% filter_all(any_vars(grepl( '^E10', .)))
>> 
>> gave me a subset (a data frame) of tot where ^E10
>> 
>> what I need is just a vector or all values in tot which start with 
>> E10.
>> 
>> Thanks
>> Ana
>> 
>> On Fri, May 15, 2020 at 12:13 PM Jeff Newmiller
>> <jdnewmil at dcn.davis.ca.us> wrote:
>>> 
>>> Read about regular expressions... they are extremely useful.
>>> 
>>> df1 <- tot %>% filter_all(any_vars(grepl( '^E10', .)))
>>> 
>>> It is bad form not to put spaces around the <- assignment.
>>> 
>>> 
>>> On May 15, 2020 10:00:04 AM PDT, Ana Marija
>> <sokovic.anamarija at gmail.com> wrote:
>>> >Hello,
>>> >
>>> >I have a data frame:
>>> >
>>> >> dim(tot)
>>> >[1] 502536   1093
>>> >
>>> >How would I extract from it all strings that start with E10?
>>> >
>>> >I know how to extract all rows that contain with E10
>>> >df0<-tot %>% filter_all(any_vars(. %in% c('E10')))
>>> >> dim(df0)
>>> >[1] 5105 1093
>>> >
>>> >but I just need a vector of strings that start with E10...
>>> >it would look something like this:
>>> >
>>> >[1] "E102" "E109" "E108" "E103" "E104" "E105" "E101" "E106" "E107"
>>> >
>>> >Thanks
>>> >Ana
>>> >
>>> >______________________________________________
>>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> >https://stat.ethz.ch/mailman/listinfo/r-help
>>> >PLEASE do read the posting guide
>>> >http://www.R-project.org/posting-guide.html
>>> >and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> --
>>> Sent from my phone. Please excuse my brevity.


From @xe|@urb|z @end|ng |rom gm@||@com  Fri May 15 23:32:51 2020
From: @xe|@urb|z @end|ng |rom gm@||@com (Axel Urbiz)
Date: Fri, 15 May 2020 17:32:51 -0400
Subject: [R] "effects" package with "lme4"
Message-ID: <FFCED16E-BF87-4BD1-B2D6-9F4B755D024C@gmail.com>

Hello John and others,

I?d appreciate your help as I?m trying to plot the effect of predictor ?Days? on Reaction by Subject. I?m only getting one plot in the example below. 

### Start example

library(lme4)
library(splines)
data("sleepstudy")

fm1 <- lmer(Reaction ~ ns(Days, 3) + (ns(Days, 3) | Subject), sleepstudy)
coef(fm1)
plot(allEffects(fm1))

### End example

Thanks,
Axel. 

From @purd|e@@ @end|ng |rom gm@||@com  Fri May 15 23:42:07 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Sat, 16 May 2020 09:42:07 +1200
Subject: [R] 
 how to extract strings in any column and in any row that start with
In-Reply-To: <CAF9-5jMgB+dG-s1XCYPNn85LA3n5To3vBBTu4R4h0Eco8wwFeA@mail.gmail.com>
References: <CAF9-5jMgB+dG-s1XCYPNn85LA3n5To3vBBTu4R4h0Eco8wwFeA@mail.gmail.com>
Message-ID: <CAB8pepyb3bP0rYc_ZRfc7YMRwQbkvr8uBf7kS_t_MSkGWVoXrw@mail.gmail.com>

> How would I extract from it all strings that start with E10?

Hi Ana,

Here's a simple solution:

    x <- c ("P24601", "E101", "E102", "3.141593",
        "E101", "xE101", "e103", " E104 ")

    x [substring (x, 1, 3) == "E10"]

You' will need to replace x with another *character vector*.
(As touched on earlier, a data.frame may cause some problems).

Here's some variations:

    unique (x [substring (x, 1, 3) == "E10"])

    y <- toupper (x)
    y [substring (y, 1, 3) == "E10"]

    y <- trimws (x)
    y [substring (y, 1, 3) == "E10"]


From j|ox @end|ng |rom mcm@@ter@c@  Fri May 15 23:51:31 2020
From: j|ox @end|ng |rom mcm@@ter@c@ (Fox, John)
Date: Fri, 15 May 2020 21:51:31 +0000
Subject: [R] "effects" package with "lme4"
In-Reply-To: <FFCED16E-BF87-4BD1-B2D6-9F4B755D024C@gmail.com>
References: <FFCED16E-BF87-4BD1-B2D6-9F4B755D024C@gmail.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC88CEC9E0E@FHSDB2D11-2.csu.mcmaster.ca>

Dear Axel,

There only one fixed effect in the model, ns(Days, 3), so I don't know what you expected.

Best,
 John
--------------------------------------
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
Web: socialsciences.mcmaster.ca/jfox/



> -----Original Message-----
> From: Axel Urbiz <axel.urbiz at gmail.com>
> Sent: Friday, May 15, 2020 5:33 PM
> To: Fox, John <jfox at mcmaster.ca>; R-help at r-project.org
> Subject: "effects" package with "lme4"
> 
> Hello John and others,
> 
> I?d appreciate your help as I?m trying to plot the effect of predictor
> ?Days? on Reaction by Subject. I?m only getting one plot in the example
> below.
> 
> ### Start example
> 
> library(lme4)
> library(splines)
> data("sleepstudy")
> 
> fm1 <- lmer(Reaction ~ ns(Days, 3) + (ns(Days, 3) | Subject), sleepstudy)
> coef(fm1)
> plot(allEffects(fm1))
> 
> ### End example
> 
> Thanks,
> Axel.

From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat May 16 00:12:22 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 15 May 2020 23:12:22 +0100
Subject: [R] 
 how to extract strings in any column and in any row that start with
In-Reply-To: <CAF9-5jOcfoDohWfOpOV85ieSNM-KUH=FMB2WCg91EWKWrfwVRA@mail.gmail.com>
References: <CAF9-5jMgB+dG-s1XCYPNn85LA3n5To3vBBTu4R4h0Eco8wwFeA@mail.gmail.com>
 <6BFCD271-9DAA-49BF-8D01-4815CD26028F@dcn.davis.ca.us>
 <CAF9-5jOcfoDohWfOpOV85ieSNM-KUH=FMB2WCg91EWKWrfwVRA@mail.gmail.com>
Message-ID: <9800b3f3-b2e1-87e2-cb80-966704fd910c@sapo.pt>

Hello,

I have tried several options and with large dataframes this one was the 
fastest (in my tests, of the ones I have tried).


s1 <- sapply(tot, function(x) grep('^E10', x, value = TRUE))


Then unlist(s1).
A close second (15% slower) was


s2 <- tot[sapply(tot, function(x) grepl('^E10', x))]


grep/unlist was 3.7 times slower:


grep("^E10", unlist(tot), value = TRUE)


Hope this helps,

Rui Barradas

?s 20:24 de 15/05/20, Ana Marija escreveu:
> Hello,
> 
> this command was running for more than 2 hours
> grep("E10",tot,value=T)
> and no output
> 
> and this command
> df1 <- tot %>% filter_all(any_vars(grepl( '^E10', .)))
> 
> gave me a subset (a data frame) of tot where ^E10
> 
> what I need is just a vector or all values in tot which start with E10.
> 
> Thanks
> Ana
> 
> On Fri, May 15, 2020 at 12:13 PM Jeff Newmiller
> <jdnewmil at dcn.davis.ca.us> wrote:
>>
>> Read about regular expressions... they are extremely useful.
>>
>> df1 <- tot %>% filter_all(any_vars(grepl( '^E10', .)))
>>
>> It is bad form not to put spaces around the <- assignment.
>>
>>
>> On May 15, 2020 10:00:04 AM PDT, Ana Marija <sokovic.anamarija at gmail.com> wrote:
>>> Hello,
>>>
>>> I have a data frame:
>>>
>>>> dim(tot)
>>> [1] 502536   1093
>>>
>>> How would I extract from it all strings that start with E10?
>>>
>>> I know how to extract all rows that contain with E10
>>> df0<-tot %>% filter_all(any_vars(. %in% c('E10')))
>>>> dim(df0)
>>> [1] 5105 1093
>>>
>>> but I just need a vector of strings that start with E10...
>>> it would look something like this:
>>>
>>> [1] "E102" "E109" "E108" "E103" "E104" "E105" "E101" "E106" "E107"
>>>
>>> Thanks
>>> Ana
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Sent from my phone. Please excuse my brevity.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Sat May 16 00:28:32 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Fri, 15 May 2020 17:28:32 -0500
Subject: [R] 
 how to extract strings in any column and in any row that start with
In-Reply-To: <9800b3f3-b2e1-87e2-cb80-966704fd910c@sapo.pt>
References: <CAF9-5jMgB+dG-s1XCYPNn85LA3n5To3vBBTu4R4h0Eco8wwFeA@mail.gmail.com>
 <6BFCD271-9DAA-49BF-8D01-4815CD26028F@dcn.davis.ca.us>
 <CAF9-5jOcfoDohWfOpOV85ieSNM-KUH=FMB2WCg91EWKWrfwVRA@mail.gmail.com>
 <9800b3f3-b2e1-87e2-cb80-966704fd910c@sapo.pt>
Message-ID: <CAF9-5jPR=1OpmL8FXDt43PsTfVUvDhgLAQcxBXpGWdysu0Xn3A@mail.gmail.com>

Hi Rui,

thank you so much that is exactly what I needed!

Cheers,
Ana

On Fri, May 15, 2020 at 5:12 PM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
> Hello,
>
> I have tried several options and with large dataframes this one was the
> fastest (in my tests, of the ones I have tried).
>
>
> s1 <- sapply(tot, function(x) grep('^E10', x, value = TRUE))
>
>
> Then unlist(s1).
> A close second (15% slower) was
>
>
> s2 <- tot[sapply(tot, function(x) grepl('^E10', x))]
>
>
> grep/unlist was 3.7 times slower:
>
>
> grep("^E10", unlist(tot), value = TRUE)
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 20:24 de 15/05/20, Ana Marija escreveu:
> > Hello,
> >
> > this command was running for more than 2 hours
> > grep("E10",tot,value=T)
> > and no output
> >
> > and this command
> > df1 <- tot %>% filter_all(any_vars(grepl( '^E10', .)))
> >
> > gave me a subset (a data frame) of tot where ^E10
> >
> > what I need is just a vector or all values in tot which start with E10.
> >
> > Thanks
> > Ana
> >
> > On Fri, May 15, 2020 at 12:13 PM Jeff Newmiller
> > <jdnewmil at dcn.davis.ca.us> wrote:
> >>
> >> Read about regular expressions... they are extremely useful.
> >>
> >> df1 <- tot %>% filter_all(any_vars(grepl( '^E10', .)))
> >>
> >> It is bad form not to put spaces around the <- assignment.
> >>
> >>
> >> On May 15, 2020 10:00:04 AM PDT, Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >>> Hello,
> >>>
> >>> I have a data frame:
> >>>
> >>>> dim(tot)
> >>> [1] 502536   1093
> >>>
> >>> How would I extract from it all strings that start with E10?
> >>>
> >>> I know how to extract all rows that contain with E10
> >>> df0<-tot %>% filter_all(any_vars(. %in% c('E10')))
> >>>> dim(df0)
> >>> [1] 5105 1093
> >>>
> >>> but I just need a vector of strings that start with E10...
> >>> it would look something like this:
> >>>
> >>> [1] "E102" "E109" "E108" "E103" "E104" "E105" "E101" "E106" "E107"
> >>>
> >>> Thanks
> >>> Ana
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >> --
> >> Sent from my phone. Please excuse my brevity.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >


From bgunter@4567 @end|ng |rom gm@||@com  Sat May 16 01:15:55 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 15 May 2020 16:15:55 -0700
Subject: [R] addScales package announcement
Message-ID: <CAGxFJbRSG9OdoQi1xshj-sd3UD=GM5Hk5soAhnE1vFLZ7jafvA@mail.gmail.com>

In case anyone here is interested (this appeared on the r-packages
list, but it seems that posts there are no longer forwarded here):

addScales is a small package now on CRAN that modifies trellis objects
created
using lattice graphics by adding horizontal and/or vertical reference
lines to provide visual scaling information. This is mostly useful
for multi-panel plots that use the relation = 'free' option in their
'scales' argument list. Examples show when and how this might aid data
visualization.

Please feel free to contact me with any suggestions for improvement.

Bert Gunter

	[[alternative HTML version deleted]]


From @xe|@urb|z @end|ng |rom gm@||@com  Sat May 16 01:39:47 2020
From: @xe|@urb|z @end|ng |rom gm@||@com (Axel Urbiz)
Date: Fri, 15 May 2020 19:39:47 -0400
Subject: [R] "effects" package with "lme4"
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC88CEC9E0E@FHSDB2D11-2.csu.mcmaster.ca>
References: <FFCED16E-BF87-4BD1-B2D6-9F4B755D024C@gmail.com>
 <ACD1644AA6C67E4FBD0C350625508EC88CEC9E0E@FHSDB2D11-2.csu.mcmaster.ca>
Message-ID: <D4E7BD65-5600-4F47-A9E7-0313D92712E3@gmail.com>

Dear John,

Thank you for your response. 

My apologies as I?m only recently getting exposed to mixed-model. My understanding, is that the model specified below also has random intercepts and slope, as they vary by Subject. `coef(fm1)` shows this. I was looking to plot the fitted splines by Subject.

Sorry if my interpretation is incorrect. 

Best,
Axel.



> On May 15, 2020, at 5:51 PM, Fox, John <jfox at mcmaster.ca> wrote:
> 
> Dear Axel,
> 
> There only one fixed effect in the model, ns(Days, 3), so I don't know what you expected.
> 
> Best,
> John
> --------------------------------------
> John Fox, Professor Emeritus
> McMaster University
> Hamilton, Ontario, Canada
> Web: socialsciences.mcmaster.ca/jfox/
> 
> 
> 
>> -----Original Message-----
>> From: Axel Urbiz <axel.urbiz at gmail.com>
>> Sent: Friday, May 15, 2020 5:33 PM
>> To: Fox, John <jfox at mcmaster.ca>; R-help at r-project.org
>> Subject: "effects" package with "lme4"
>> 
>> Hello John and others,
>> 
>> I?d appreciate your help as I?m trying to plot the effect of predictor
>> ?Days? on Reaction by Subject. I?m only getting one plot in the example
>> below.
>> 
>> ### Start example
>> 
>> library(lme4)
>> library(splines)
>> data("sleepstudy")
>> 
>> fm1 <- lmer(Reaction ~ ns(Days, 3) + (ns(Days, 3) | Subject), sleepstudy)
>> coef(fm1)
>> plot(allEffects(fm1))
>> 
>> ### End example
>> 
>> Thanks,
>> Axel.


From j|ox @end|ng |rom mcm@@ter@c@  Sat May 16 02:08:52 2020
From: j|ox @end|ng |rom mcm@@ter@c@ (Fox, John)
Date: Sat, 16 May 2020 00:08:52 +0000
Subject: [R] "effects" package with "lme4"
In-Reply-To: <D4E7BD65-5600-4F47-A9E7-0313D92712E3@gmail.com>
References: <FFCED16E-BF87-4BD1-B2D6-9F4B755D024C@gmail.com>
 <ACD1644AA6C67E4FBD0C350625508EC88CEC9E0E@FHSDB2D11-2.csu.mcmaster.ca>
 <D4E7BD65-5600-4F47-A9E7-0313D92712E3@gmail.com>
Message-ID: <FFC6DF58-FEAF-430C-941B-276AE3F819AC@mcmaster.ca>

Dear Axel,

> On May 15, 2020, at 7:39 PM, Axel Urbiz <axel.urbiz at gmail.com> wrote:
> 
> Dear John,
> 
> Thank you for your response. 
> 
> My apologies as I?m only recently getting exposed to mixed-model. My understanding, is that the model specified below also has random intercepts and slope, as they vary by Subject. `coef(fm1)` shows this. I was looking to plot the fitted splines by Subject.
> 
> Sorry if my interpretation is incorrect.

There's no need to apologize. 

The functions in the effects package compute and graph fixed effects for mixed models. You could compute and graph the BLUP for the fitted spline for each subject (see below) but it's not what the effects package does.

I think that the following does what you want:

-------- snip ----------

subjects <- levels(sleepstudy$Subject)
fits <- matrix(0, length(subjects), 10)
rownames(fits) <- subjects
for (subject in subjects){
  fits[subject, ] <- predict(fm1, newdata=data.frame(Subject=subject, Days=0:9))
}
plot(c(0, 10), range(fits), type="n", xlab="Days", ylab="Reaction")
for (subject in subjects) lines(0:9, fits[subject, ])

-------- snip ----------

Best,
 John
 
> 
> Best,
> Axel.
> 
> 
> 
>> On May 15, 2020, at 5:51 PM, Fox, John <jfox at mcmaster.ca> wrote:
>> 
>> Dear Axel,
>> 
>> There only one fixed effect in the model, ns(Days, 3), so I don't know what you expected.
>> 
>> Best,
>> John
>> --------------------------------------
>> John Fox, Professor Emeritus
>> McMaster University
>> Hamilton, Ontario, Canada
>> Web: socialsciences.mcmaster.ca/jfox/
>> 
>> 
>> 
>>> -----Original Message-----
>>> From: Axel Urbiz <axel.urbiz at gmail.com>
>>> Sent: Friday, May 15, 2020 5:33 PM
>>> To: Fox, John <jfox at mcmaster.ca>; R-help at r-project.org
>>> Subject: "effects" package with "lme4"
>>> 
>>> Hello John and others,
>>> 
>>> I?d appreciate your help as I?m trying to plot the effect of predictor
>>> ?Days? on Reaction by Subject. I?m only getting one plot in the example
>>> below.
>>> 
>>> ### Start example
>>> 
>>> library(lme4)
>>> library(splines)
>>> data("sleepstudy")
>>> 
>>> fm1 <- lmer(Reaction ~ ns(Days, 3) + (ns(Days, 3) | Subject), sleepstudy)
>>> coef(fm1)
>>> plot(allEffects(fm1))
>>> 
>>> ### End example
>>> 
>>> Thanks,
>>> Axel.
> 


From den|@ @end|ng |rom m@th@un|v-|yon1@|r  Fri May 15 17:00:37 2020
From: den|@ @end|ng |rom m@th@un|v-|yon1@|r (Roland Denis)
Date: Fri, 15 May 2020 17:00:37 +0200
Subject: [R] Compilation error with Intel compiler & Ubuntu 18.04 related to
 matherr feature detection
Message-ID: <9200a04a-1013-5e94-49a6-91cbb10b69fb-5591@math.univ-lyon1.fr>


Hello,

while compiling R (tested with version 4.0.0 and 3.6.2) with Intel 
compiler (version 19) under Ubuntu 18.04, I get the following error:

icc -I../../src/extra? -I. -I../../src/include -I../../src/include 
-I/usr/local/include -I../../src/nmath -DHAVE_CONFIG_H?? -fopenmp -g -O2 
-std=c99? -c arithmetic.c -o arithmetic.o
arithmetic.c(59): warning #274: declaration is not visible outside of 
function
 ? int matherr(struct exception *exc)
 ???????????????????? ^

arithmetic.c(61): error: pointer to incomplete class type is not allowed
 ????? switch (exc->type) {
 ????????????? ^

arithmetic.c(62): error: identifier "DOMAIN" is undefined
 ????? case DOMAIN:
 ?????????? ^

On the other side, everything works fine with Ubuntu 16.04.

Same error has already been reported on Intel forum 
(https://software.intel.com/en-us/forums/intel-c-compiler/topic/843647) 
and the remaining of this message is mainly a copy-paste from an answer 
that I just post to the Intel forum (waiting for moderation).

 From what I understand, it comes from the removal of SVID math library 
exception handling (that defines matherr function and exception 
structure) from math.h starting with glibc >= 2.27 (see the manpage of 
matherr, e.g. here http://man7.org/linux/man-pages/man3/matherr.3.html).

Ubuntu 16.04 is shipped with version 2.23 and Ubuntu 18.04 with version 
2.27: the test code from the manpage above compiles fine with gcc 5.5.0 
from Ubuntu 16.04 but triggers a compilation error (lack of definition 
of exception structure and associated typedefs) with gcc 7.5.0 from 
Ubuntu 18.04.

During configuration step of R, it checks for availability of this 
exception handling by checking if a matherr function is found during 
linking time, i.e. with a likewise minimal code (extracted from the 
generated conftext.c):

```
char matherr ();

int main () {
 ??? return matherr();
}
```
If it compiles fine (with -lm option during linking pass), then 
HAVE_MATHERR is set in src/include/config.h and corresponding code is 
visible during compilation of R (the faulty part that triggered an error 
in arithmetic.c).

With Ubuntu 16.04, this test code compiles fine with gcc and math.h have 
the corresponding definitions (`exception`).
With Ubuntu 18.04, this test code fails at linking step (undefined 
reference to matherr), thus hiding associated code in arithmetic.c

Now with Intel compiler, this minimal test code compiles fine both under 
Ubuntu 16.04 and 18.04, but math.h from Intel doesn't define the 
`exception` structure (only `exceptionf` and `exceptionl`). It is 
probably relying on the definitions in the glibc math.h (included in the 
beginning of Intel's math.h) that has been removed in glibc >= 2.27.


To resume, R checks for SVID math library exception handling by looking 
for the `matherr` symbol in libm and then suppose that the associated 
`exception` structure is defined in math.h. While this implication holds 
true for GCC (under Ubuntu 16.04 & 18.04), the Intel compiler seems to 
always feature the `matherr` symbol while relying on system wide 
`math.h` (e.g. glibc) for the associated definitions, definitions that 
has been removed in glibc >= 2.27 (Ubuntu 18.04).

The temporary fix that I use is simply to remove HAVE_MATHERR macro in 
src/include/config.h from R sources between the configuration and 
building steps.

But even if Intel compiler has probably a faulty behavior in this case, 
I was wondering if it should be safer to also check for `exception` 
structure definition during configuration of R instead of just relying 
on matherr symbol existence?

Regards,

-- 
Roland DENIS


From den|@ @end|ng |rom m@th@un|v-|yon1@|r  Fri May 15 17:00:37 2020
From: den|@ @end|ng |rom m@th@un|v-|yon1@|r (Roland Denis)
Date: Fri, 15 May 2020 17:00:37 +0200
Subject: [R] Compilation error with Intel compiler & Ubuntu 18.04 related to
 matherr feature detection
Message-ID: <9200a04a-1013-5e94-49a6-91cbb10b69fb-8506@math.univ-lyon1.fr>


Hello,

while compiling R (tested with version 4.0.0 and 3.6.2) with Intel 
compiler (version 19) under Ubuntu 18.04, I get the following error:

icc -I../../src/extra? -I. -I../../src/include -I../../src/include 
-I/usr/local/include -I../../src/nmath -DHAVE_CONFIG_H?? -fopenmp -g -O2 
-std=c99? -c arithmetic.c -o arithmetic.o
arithmetic.c(59): warning #274: declaration is not visible outside of 
function
 ? int matherr(struct exception *exc)
 ???????????????????? ^

arithmetic.c(61): error: pointer to incomplete class type is not allowed
 ????? switch (exc->type) {
 ????????????? ^

arithmetic.c(62): error: identifier "DOMAIN" is undefined
 ????? case DOMAIN:
 ?????????? ^

On the other side, everything works fine with Ubuntu 16.04.

Same error has already been reported on Intel forum 
(https://software.intel.com/en-us/forums/intel-c-compiler/topic/843647) 
and the remaining of this message is mainly a copy-paste from an answer 
that I just post to the Intel forum (waiting for moderation).

 From what I understand, it comes from the removal of SVID math library 
exception handling (that defines matherr function and exception 
structure) from math.h starting with glibc >= 2.27 (see the manpage of 
matherr, e.g. here http://man7.org/linux/man-pages/man3/matherr.3.html).

Ubuntu 16.04 is shipped with version 2.23 and Ubuntu 18.04 with version 
2.27: the test code from the manpage above compiles fine with gcc 5.5.0 
from Ubuntu 16.04 but triggers a compilation error (lack of definition 
of exception structure and associated typedefs) with gcc 7.5.0 from 
Ubuntu 18.04.

During configuration step of R, it checks for availability of this 
exception handling by checking if a matherr function is found during 
linking time, i.e. with a likewise minimal code (extracted from the 
generated conftext.c):

```
char matherr ();

int main () {
 ??? return matherr();
}
```
If it compiles fine (with -lm option during linking pass), then 
HAVE_MATHERR is set in src/include/config.h and corresponding code is 
visible during compilation of R (the faulty part that triggered an error 
in arithmetic.c).

With Ubuntu 16.04, this test code compiles fine with gcc and math.h have 
the corresponding definitions (`exception`).
With Ubuntu 18.04, this test code fails at linking step (undefined 
reference to matherr), thus hiding associated code in arithmetic.c

Now with Intel compiler, this minimal test code compiles fine both under 
Ubuntu 16.04 and 18.04, but math.h from Intel doesn't define the 
`exception` structure (only `exceptionf` and `exceptionl`). It is 
probably relying on the definitions in the glibc math.h (included in the 
beginning of Intel's math.h) that has been removed in glibc >= 2.27.


To resume, R checks for SVID math library exception handling by looking 
for the `matherr` symbol in libm and then suppose that the associated 
`exception` structure is defined in math.h. While this implication holds 
true for GCC (under Ubuntu 16.04 & 18.04), the Intel compiler seems to 
always feature the `matherr` symbol while relying on system wide 
`math.h` (e.g. glibc) for the associated definitions, definitions that 
has been removed in glibc >= 2.27 (Ubuntu 18.04).

The temporary fix that I use is simply to remove HAVE_MATHERR macro in 
src/include/config.h from R sources between the configuration and 
building steps.

But even if Intel compiler has probably a faulty behavior in this case, 
I was wondering if it should be safer to also check for `exception` 
structure definition during configuration of R instead of just relying 
on matherr symbol existence?

Regards,

-- 
Roland DENIS


From @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t  Sat May 16 12:21:24 2020
From: @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t (Stefano Sofia)
Date: Sat, 16 May 2020 10:21:24 +0000
Subject: [R] Classification of wind events
In-Reply-To: <CA+8X3fUsXA7reESuWpGC2jBGRRryF6RcYa5jZxMWV7oktF1kFg@mail.gmail.com>
References: <8B435C9568170B469AE31E8891E8CC4F809B8DEA@ESINO.regionemarche.intra>,
 <CA+8X3fUsXA7reESuWpGC2jBGRRryF6RcYa5jZxMWV7oktF1kFg@mail.gmail.com>
Message-ID: <8B435C9568170B469AE31E8891E8CC4F809B9B6F@ESINO.regionemarche.intra>

Dear Jim and Jeff,
thank you for your comments. You are right, it is quite difficult to detect this process through a single observation point, I am awre of it.
I need to set up an automatic algorithm to filter 20 years of data, and I have to find an easy way to do it.
I know quite well my automatic stations, the wind direction is very stable during these situations, and therefore I would like to start from it. (I should use also wind speed, relative humidity and temperature, but I will introduce them only once I will be able to manage the direction).
In the case of the example below reported, I know that the directions of this particular automatic station must be only SW or WSW.

My biggest problem, obviously, is to find the beginning and the end of each event, when there is a change in the main direction.
Thinking about categorical data in general, is there a way to detect periods when one particular category is more frequent?

Here I reproduce a real example 24 hours long, where these Foehn condition start between 09 and 10 and finish after 19:

first_day_POSIX <- as.POSIXct("2020-02-19-00-00", format="%Y-%m-%d-%H-%M")
last_day_POSIX <- as.POSIXct("2020-02-20-00-00", format="%Y-%m-%d-%H-%M")
mydf <- data.frame(data_POSIX=seq(first_day_POSIX, last_day_POSIX, by="10 min"))

mydf$main_dir <- c(WSW, WSW, SW, SW, W, WSW, WSW, WSW, W, W, SW, WSW, SSW, S, SW, SW, WSW, WNW, W, WSW, WSW, SE, SE, SE, NW, NNE, ENE, SE, NNW, NW, NW, NW, NW, NW, NW, NE, NW, NW, NW, NW, NW, N, WNW, NW, NNW, NNW, NW, NW, NW, WNW, ESE, W, WSW, SW, SW, SW, WSW, SW, S, S, SSW, SW, WSW, WSW, WSW, WSW, WSW, WSW, WSW, SW, WSW, WSW, WSW, WSW, SW, SW, WSW, WSW, WSW, WSW, WSW, SW, SW, SW, SW, SW, SW, SW, SW, SW, WSW, WSW, WSW, WSW, SW, SW, SW, SW, WSW, SW, SW, SW, SW, SW, WSW, SW, SW, W, WSW, WSW, SSW, S, WNW, SW, W, WSW, WSW, SE, SE, SE, NW, NNE, ENE, SE, NNW, NW, NW, NW, NW, NW, NW, NE, NW, NW, NW, NW, NW, N, WNW, NW, NNW, NNW, NW, NW, NW)

mydf$max_speed <- c(4.60, 4.60, 3.40, 3.10, 4.80, 4.20, 4.10, 4.50, 4.70, 4.30, 2.40, 2.30, 2.20, 2.10, 2.90, 2.80, 1.80, 2.70, 4.30, 3.30, 2.30, 2.30, 3.20, 3.20, 2.90, 2.30, 1.50, 1.80, 2.90, 2.40, 1.80, 2.40, 2.30, 2.60, 1.80, 2.30, 1.90, 2.20, 2.80, 2.40, 1.00, 1.10, 1.60, 2.30, 2.50, 3.30, 3.40, 3.20, 4.50, 3.90, 3.10, 2.40, 6.00, 7.80, 6.30, 7.80, 8.10, 6.10, 7.40, 9.50, 8.90, 9.10, 10.10, 10.50, 11.10, 10.10, 10.90, 11.30, 13.40, 13.50, 12.80, 11.50, 13.10, 13.50, 11.10, 10.50, 8.50, 10.10, 10.70, 13.60, 11.90, 14.90, 10.90, 10.90, 12.80, 12.10, 9.10, 8.30, 8.80, 7.40, 8.40, 10.30, 10.00, 7.00, 8.50, 8.40, 8.60, 6.70, 7.30, 6.20, 5.90, 5.90, 5.10, 5.80, 5.60, 6.50, 6.60, 11.70, 11.30, 8.70, 7.10, 6.90, 4.30, 3.80, 4.30, 3.30, 2.30, 2.30, 3.20, 3.20, 2.90, 2.30, 1.50, 1.80, 2.90, 2.40, 1.80, 2.40, 2.30, 2.60, 1.80, 2.30, 1.90, 2.20, 2.80, 2.40, 1.00, 1.10, 1.60, 2.30, 2.50, 3.30, 3.40, 3.20, 4.50)


Thank you for your attention
Stefano


         (oo)
--oOO--( )--OOo----------------
Stefano Sofia PhD
Civil Protection - Marche Region
Meteo Section
Snow Section
Via del Colle Ameno 5
60126 Torrette di Ancona, Ancona
Uff: 071 806 7743
E-mail: stefano.sofia at regione.marche.it
---Oo---------oO----------------

________________________________________
Da: Jim Lemon [drjimlemon at gmail.com]
Inviato: mercoled? 13 maggio 2020 11.01
A: Stefano Sofia; r-help mailing list
Oggetto: Re: [R] Classification of wind events

Hi Stefano,
Given only one observation point you will find it difficult. If your
automatic weather station is in the low area where the foehn wind is
felt, it can only be distinguished from a dry katabatic wind if the
upwind conditions are known. There is a similar but milder version of
this in eastern Australia, but it is usually of the latter sort. There
may be a way to measure turbulence above the peak of the high ground
with radar or something, but I'm not familiar with that.

Jim

On Tue, May 12, 2020 at 6:13 PM Stefano Sofia
<stefano.sofia at regione.marche.it> wrote:
>
> Dear R list users,
> I am aware that this question is not strictly related, at the present moment, to R code and it is more general. Please forgive me, but I need to share my thoughts with you.
>
> Foehn conditions on the southern slope of Alps happen with strong northerly flows that impact perpendicularly over the Apls. This situation triggers strong northerly leeward winds.
> Given a single automatic weather station, I would like to identify these periods starting from wind direction and wind intensity data. Frequency of data is quarter of hour.
> I would really find difficult to detect the moving windows of these events:
> - I can't analyse data day by day;
> - at the beginning and at the end of each event, when the process is not at full speed yet, the rotation is not always perfectly identifiable;
> - I cannot claim in principle that the direction of each consecutive observation is costantly and strictly from the chosen direction.
>
> Does anybody have a clue on how to start to build this process in the right way?
>
> Thank you for your attention and your help
> Stefano
>
>          (oo)
> --oOO--( )--OOo----------------
> Stefano Sofia PhD
> Civil Protection - Marche Region
> Meteo Section
> Snow Section
> Via del Colle Ameno 5
> 60126 Torrette di Ancona, Ancona
> Uff: 071 806 7743
> E-mail: stefano.sofia at regione.marche.it
> ---Oo---------oO----------------
>
> ________________________________
>
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
>
> --
> Questo messaggio  stato analizzato da Libra ESVA ed  risultato non infetto.
> This message was scanned by Libra ESVA and is believed to be clean.
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--

Questo messaggio  stato analizzato con Libra ESVA ed  risultato non infetto.


________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.

--
Questo messaggio  stato analizzato da Libra ESVA ed  risultato non infetto.
This message was scanned by Libra ESVA and is believed to be clean.


From percent||101 @end|ng |rom gm@||@com  Sat May 16 10:21:10 2020
From: percent||101 @end|ng |rom gm@||@com (=?UTF-8?Q?Pedro_p=C3=A1ramo?=)
Date: Sat, 16 May 2020 10:21:10 +0200
Subject: [R] Extracting files per types of data file
Message-ID: <CAB-TgNtbBKFfvtdLd=9KTTtmAFuQx9tn=6aVAeDK7-o-WAdr+Q@mail.gmail.com>

Hi all,

For many years I have left R in my daily task but I have returned.

I write to try to get your help on what function look for to program the
next:

My data file is a csv file with lots of data and at the end it ends
with \r\n

Imagine

Name ; Surname; Code; age; CP; \r\n
Oscar ; example ; 1; 42; 4857; \r\n
Maria; Ex3; 2; 33; 879;\r\n
Luz; pruve; 1; 42; 4785; \r\n


I want to run a function that detects all different items in Code in this
case "1" and "2" and output two files named "1.csv or txt" and "2.csv or
txt"  and in this file contain the rows withs all items where Code is 1 and
so on.

The thing is thar I dont know exactly how many different codes are there,
so the programm should "count them" and extract the differents codes and
output the equivalent files of the same kind of code.

It is possible with R? I use big files (and in excel is tedious).

Can you suggest some functions to analyse and to achieve my goal?

Many thanks in advance

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Sat May 16 12:46:37 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sat, 16 May 2020 20:46:37 +1000
Subject: [R] Extracting files per types of data file
In-Reply-To: <CAB-TgNtbBKFfvtdLd=9KTTtmAFuQx9tn=6aVAeDK7-o-WAdr+Q@mail.gmail.com>
References: <CAB-TgNtbBKFfvtdLd=9KTTtmAFuQx9tn=6aVAeDK7-o-WAdr+Q@mail.gmail.com>
Message-ID: <CA+8X3fXt+4vv9WaBKVuZjvsjzkmB1w2SCpAHh8=CBVgBGfgtxA@mail.gmail.com>

Hi Pedro,
Will this be sufficient?

ppdf<-read.table(text="Name ; Surname; Code; age; CP; \r\n
Oscar ; example ; 1; 42; 4857; \r\n
Maria; Ex3; 2; 33; 879;\r\n
Luz; pruve; 1; 42; 4785; \r\n",
header=TRUE,sep=";",stringsAsFactors=FALSE)
write.csv(ppdf[ppdf$age==1,1:5],file="ppdf1.csv")
write.csv(ppdf[ppdf$age==2,1:5],file="ppdf2.csv")

Jim

On Sat, May 16, 2020 at 8:36 PM Pedro p?ramo <percentil101 at gmail.com> wrote:
>
> Hi all,
>
> For many years I have left R in my daily task but I have returned.
>
> I write to try to get your help on what function look for to program the
> next:
>
> My data file is a csv file with lots of data and at the end it ends
> with \r\n
>
> Imagine
>
> Name ; Surname; Code; age; CP; \r\n
> Oscar ; example ; 1; 42; 4857; \r\n
> Maria; Ex3; 2; 33; 879;\r\n
> Luz; pruve; 1; 42; 4785; \r\n
>
>
> I want to run a function that detects all different items in Code in this
> case "1" and "2" and output two files named "1.csv or txt" and "2.csv or
> txt"  and in this file contain the rows withs all items where Code is 1 and
> so on.
>
> The thing is thar I dont know exactly how many different codes are there,
> so the programm should "count them" and extract the differents codes and
> output the equivalent files of the same kind of code.
>
> It is possible with R? I use big files (and in excel is tedious).
>
> Can you suggest some functions to analyse and to achieve my goal?
>
> Many thanks in advance
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From re|chm@nj @end|ng |rom @bcg|ob@|@net  Sat May 16 17:33:51 2020
From: re|chm@nj @end|ng |rom @bcg|ob@|@net (Jeff Reichman)
Date: Sat, 16 May 2020 10:33:51 -0500
Subject: [R] facets
References: <000301d62b97$674eec90$35ecc5b0$.ref@sbcglobal.net>
Message-ID: <000301d62b97$674eec90$35ecc5b0$@sbcglobal.net>

A non-text attachment was scrubbed...
Name: 000012.png
Type: image/png
Size: 36465 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200516/0b00e545/attachment.png>

From re|chm@nj @end|ng |rom @bcg|ob@|@net  Sat May 16 17:37:40 2020
From: re|chm@nj @end|ng |rom @bcg|ob@|@net (Jeff Reichman)
Date: Sat, 16 May 2020 10:37:40 -0500
Subject: [R] Recall: facets
References: <!&!GAAAAAAAAABWob9e2JwrSqMGgmGX/wghwoAAABgAAAAAAAAAVqG/XticK0qjBoJhl/8IISQMWgAAAAAAEAAAAMLmaJWUE1dIuC/iUiOl6pQHAAAAZmFjZXRzAA==.ref@sbcglobal.net>
Message-ID: <!&!GAAAAAAAAABWob9e2JwrSqMGgmGX/wghwoAAABgAAAAAAAAAVqG/XticK0qjBoJhl/8IISQMWgAAAAAAEAAAAMLmaJWUE1dIuC/iUiOl6pQHAAAAZmFjZXRzAA==@sbcglobal.net>

Jeff Reichman would like to recall the message, "facets".

From percent||101 @end|ng |rom gm@||@com  Sat May 16 16:42:37 2020
From: percent||101 @end|ng |rom gm@||@com (=?UTF-8?Q?Pedro_p=C3=A1ramo?=)
Date: Sat, 16 May 2020 16:42:37 +0200
Subject: [R] Extracting files per types of data file
In-Reply-To: <CA+8X3fXt+4vv9WaBKVuZjvsjzkmB1w2SCpAHh8=CBVgBGfgtxA@mail.gmail.com>
References: <CAB-TgNtbBKFfvtdLd=9KTTtmAFuQx9tn=6aVAeDK7-o-WAdr+Q@mail.gmail.com>
 <CA+8X3fXt+4vv9WaBKVuZjvsjzkmB1w2SCpAHh8=CBVgBGfgtxA@mail.gmail.com>
Message-ID: <CAB-TgNsQof=DPC3iA=YhCkUK7PP0hYLPD7M111OcmGVf48kMEA@mail.gmail.com>

Its perfect???

Many thanks to all.

El s?b., 16 may. 2020 a las 12:46, Jim Lemon (<drjimlemon at gmail.com>)
escribi?:

> Hi Pedro,
> Will this be sufficient?
>
> ppdf<-read.table(text="Name ; Surname; Code; age; CP; \r\n
> Oscar ; example ; 1; 42; 4857; \r\n
> Maria; Ex3; 2; 33; 879;\r\n
> Luz; pruve; 1; 42; 4785; \r\n",
> header=TRUE,sep=";",stringsAsFactors=FALSE)
> write.csv(ppdf[ppdf$age==1,1:5],file="ppdf1.csv")
> write.csv(ppdf[ppdf$age==2,1:5],file="ppdf2.csv")
>
> Jim
>
> On Sat, May 16, 2020 at 8:36 PM Pedro p?ramo <percentil101 at gmail.com>
> wrote:
> >
> > Hi all,
> >
> > For many years I have left R in my daily task but I have returned.
> >
> > I write to try to get your help on what function look for to program the
> > next:
> >
> > My data file is a csv file with lots of data and at the end it ends
> > with \r\n
> >
> > Imagine
> >
> > Name ; Surname; Code; age; CP; \r\n
> > Oscar ; example ; 1; 42; 4857; \r\n
> > Maria; Ex3; 2; 33; 879;\r\n
> > Luz; pruve; 1; 42; 4785; \r\n
> >
> >
> > I want to run a function that detects all different items in Code in this
> > case "1" and "2" and output two files named "1.csv or txt" and "2.csv or
> > txt"  and in this file contain the rows withs all items where Code is 1
> and
> > so on.
> >
> > The thing is thar I dont know exactly how many different codes are there,
> > so the programm should "count them" and extract the differents codes and
> > output the equivalent files of the same kind of code.
> >
> > It is possible with R? I use big files (and in excel is tedious).
> >
> > Can you suggest some functions to analyse and to achieve my goal?
> >
> > Many thanks in advance
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sat May 16 21:04:28 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 16 May 2020 12:04:28 -0700
Subject: [R] Classification of wind events
In-Reply-To: <8B435C9568170B469AE31E8891E8CC4F809B9B6F@ESINO.regionemarche.intra>
References: <8B435C9568170B469AE31E8891E8CC4F809B8DEA@ESINO.regionemarche.intra>,
 <CA+8X3fUsXA7reESuWpGC2jBGRRryF6RcYa5jZxMWV7oktF1kFg@mail.gmail.com>
 <8B435C9568170B469AE31E8891E8CC4F809B9B6F@ESINO.regionemarche.intra>
Message-ID: <98642722-2500-4C90-A297-8F8FFC097BAE@dcn.davis.ca.us>

Please run your code before posting it... you forgot the quotes in your main_dir column.

first_day_POSIX <- as.POSIXct("2020-02-19-00-00", format="%Y-%m-%d-%H-%M")
last_day_POSIX <- as.POSIXct("2020-02-20-00-00", format="%Y-%m-%d-%H-%M")
mydf <- data.frame(data_POSIX=seq(first_day_POSIX, last_day_POSIX, by="10 min"))

mydf$main_dir <- c("WSW", "WSW", "SW", "SW", "W", "WSW", "WSW", "WSW", "W", "W", "SW", "WSW", "SSW", "S", "SW", "SW", "WSW", "WNW", "W", "WSW", "WSW", "SE", "SE", "SE", "NW", "NNE", "ENE", "SE", "NNW", "NW", "NW", "NW", "NW", "NW", "NW", "NE", "NW", "NW", "NW", "NW", "NW", "N", "WNW", "NW", "NNW", "NNW", "NW", "NW", "NW", "WNW", "ESE", "W", "WSW", "SW", "SW", "SW", "WSW", "SW", "S", "S", "SSW", "SW", "WSW", "WSW", "WSW", "WSW", "WSW", "WSW", "WSW", "SW", "WSW", "WSW", "WSW", "WSW", "SW", "SW", "WSW", "WSW", "WSW", "WSW", "WSW", "SW", "SW", "SW", "SW", "SW", "SW", "SW", "SW", "SW", "WSW", "WSW", "WSW", "WSW", "SW", "SW", "SW", "SW", "WSW", "SW", "SW", "SW", "SW", "SW", "WSW", "SW", "SW", "W", "WSW", "WSW", "SSW", "S", "WNW", "SW", "W", "WSW", "WSW", "SE", "SE", "SE", "NW", "NNE", "ENE", "SE", "NNW", "NW", "NW", "NW", "NW", "NW", "NW", "NE", "NW", "NW", "NW", "NW", "NW", "N", "WNW", "NW", "NNW", "NNW", "NW", "NW", "NW")

mydf$max_speed <- c(4.60, 4.60, 3.40, 3.10, 4.80, 4.20, 4.10, 4.50, 4.70, 4.30, 2.40, 2.30, 2.20, 2.10, 2.90, 2.80, 1.80, 2.70, 4.30, 3.30, 2.30, 2.30, 3.20, 3.20, 2.90, 2.30, 1.50, 1.80, 2.90, 2.40, 1.80, 2.40, 2.30, 2.60, 1.80, 2.30, 1.90, 2.20, 2.80, 2.40, 1.00, 1.10, 1.60, 2.30, 2.50, 3.30, 3.40, 3.20, 4.50, 3.90, 3.10, 2.40, 6.00, 7.80, 6.30, 7.80, 8.10, 6.10, 7.40, 9.50, 8.90, 9.10, 10.10, 10.50, 11.10, 10.10, 10.90, 11.30, 13.40, 13.50, 12.80, 11.50, 13.10, 13.50, 11.10, 10.50, 8.50, 10.10, 10.70, 13.60, 11.90, 14.90, 10.90, 10.90, 12.80, 12.10, 9.10, 8.30, 8.80, 7.40, 8.40, 10.30, 10.00, 7.00, 8.50, 8.40, 8.60, 6.70, 7.30, 6.20, 5.90, 5.90, 5.10, 5.80, 5.60, 6.50, 6.60, 11.70, 11.30, 8.70, 7.10, 6.90, 4.30, 3.80, 4.30, 3.30, 2.30, 2.30, 3.20, 3.20, 2.90, 2.30, 1.50, 1.80, 2.90, 2.40, 1.80, 2.40, 2.30, 2.60, 1.80, 2.30, 1.90, 2.20, 2.80, 2.40, 1.00, 1.10, 1.60, 2.30, 2.50, 3.30, 3.40, 3.20, 4.50)
# mark candidate rows
mydf$foehn1a <- mydf$main_dir %in% c( "WSW", "SW" )
# mark unstable conditions
mydf$foehn1b <- with( mydf
                    , cumsum( !foehn1a )
                    )
# find minimum length of foehn conditions
mydf$foehn1c <- ave( rep( 1, nrow( mydf ) )
                   , mydf$foehn1b
                   , FUN=function(v) 10 < length( v )
                   )
# find starts of foehns
mydf$foehn1d <- with( mydf
                    , 0 < diff( c( 0, foehn1c ) )
                    )
# identify foehns distinctly (multiple days)
mydf$foehn1e <- with( mydf
                    , ifelse( foehn1c
                            , cumsum( foehn1d )
                            , 0
                            )
                    )
mydf[ , c( 1, 2, 8 ) ]

On May 16, 2020 3:21:24 AM PDT, Stefano Sofia <stefano.sofia at regione.marche.it> wrote:
>Dear Jim and Jeff,
>thank you for your comments. You are right, it is quite difficult to
>detect this process through a single observation point, I am awre of
>it.
>I need to set up an automatic algorithm to filter 20 years of data, and
>I have to find an easy way to do it.
>I know quite well my automatic stations, the wind direction is very
>stable during these situations, and therefore I would like to start
>from it. (I should use also wind speed, relative humidity and
>temperature, but I will introduce them only once I will be able to
>manage the direction).
>In the case of the example below reported, I know that the directions
>of this particular automatic station must be only SW or WSW.
>
>My biggest problem, obviously, is to find the beginning and the end of
>each event, when there is a change in the main direction.
>Thinking about categorical data in general, is there a way to detect
>periods when one particular category is more frequent?
>
>Here I reproduce a real example 24 hours long, where these Foehn
>condition start between 09 and 10 and finish after 19:
>
>first_day_POSIX <- as.POSIXct("2020-02-19-00-00",
>format="%Y-%m-%d-%H-%M")
>last_day_POSIX <- as.POSIXct("2020-02-20-00-00",
>format="%Y-%m-%d-%H-%M")
>mydf <- data.frame(data_POSIX=seq(first_day_POSIX, last_day_POSIX,
>by="10 min"))
>
>mydf$main_dir <- c(WSW, WSW, SW, SW, W, WSW, WSW, WSW, W, W, SW, WSW,
>SSW, S, SW, SW, WSW, WNW, W, WSW, WSW, SE, SE, SE, NW, NNE, ENE, SE,
>NNW, NW, NW, NW, NW, NW, NW, NE, NW, NW, NW, NW, NW, N, WNW, NW, NNW,
>NNW, NW, NW, NW, WNW, ESE, W, WSW, SW, SW, SW, WSW, SW, S, S, SSW, SW,
>WSW, WSW, WSW, WSW, WSW, WSW, WSW, SW, WSW, WSW, WSW, WSW, SW, SW, WSW,
>WSW, WSW, WSW, WSW, SW, SW, SW, SW, SW, SW, SW, SW, SW, WSW, WSW, WSW,
>WSW, SW, SW, SW, SW, WSW, SW, SW, SW, SW, SW, WSW, SW, SW, W, WSW, WSW,
>SSW, S, WNW, SW, W, WSW, WSW, SE, SE, SE, NW, NNE, ENE, SE, NNW, NW,
>NW, NW, NW, NW, NW, NE, NW, NW, NW, NW, NW, N, WNW, NW, NNW, NNW, NW,
>NW, NW)
>
>mydf$max_speed <- c(4.60, 4.60, 3.40, 3.10, 4.80, 4.20, 4.10, 4.50,
>4.70, 4.30, 2.40, 2.30, 2.20, 2.10, 2.90, 2.80, 1.80, 2.70, 4.30, 3.30,
>2.30, 2.30, 3.20, 3.20, 2.90, 2.30, 1.50, 1.80, 2.90, 2.40, 1.80, 2.40,
>2.30, 2.60, 1.80, 2.30, 1.90, 2.20, 2.80, 2.40, 1.00, 1.10, 1.60, 2.30,
>2.50, 3.30, 3.40, 3.20, 4.50, 3.90, 3.10, 2.40, 6.00, 7.80, 6.30, 7.80,
>8.10, 6.10, 7.40, 9.50, 8.90, 9.10, 10.10, 10.50, 11.10, 10.10, 10.90,
>11.30, 13.40, 13.50, 12.80, 11.50, 13.10, 13.50, 11.10, 10.50, 8.50,
>10.10, 10.70, 13.60, 11.90, 14.90, 10.90, 10.90, 12.80, 12.10, 9.10,
>8.30, 8.80, 7.40, 8.40, 10.30, 10.00, 7.00, 8.50, 8.40, 8.60, 6.70,
>7.30, 6.20, 5.90, 5.90, 5.10, 5.80, 5.60, 6.50, 6.60, 11.70, 11.30,
>8.70, 7.10, 6.90, 4.30, 3.80, 4.30, 3.30, 2.30, 2.30, 3.20, 3.20, 2.90,
>2.30, 1.50, 1.80, 2.90, 2.40, 1.80, 2.40, 2.30, 2.60, 1.80, 2.30, 1.90,
>2.20, 2.80, 2.40, 1.00, 1.10, 1.60, 2.30, 2.50, 3.30, 3.40, 3.20, 4.50)
>
>
>Thank you for your attention
>Stefano
>
>
>         (oo)
>--oOO--( )--OOo----------------
>Stefano Sofia PhD
>Civil Protection - Marche Region
>Meteo Section
>Snow Section
>Via del Colle Ameno 5
>60126 Torrette di Ancona, Ancona
>Uff: 071 806 7743
>E-mail: stefano.sofia at regione.marche.it
>---Oo---------oO----------------
>
>________________________________________
>Da: Jim Lemon [drjimlemon at gmail.com]
>Inviato: mercoled? 13 maggio 2020 11.01
>A: Stefano Sofia; r-help mailing list
>Oggetto: Re: [R] Classification of wind events
>
>Hi Stefano,
>Given only one observation point you will find it difficult. If your
>automatic weather station is in the low area where the foehn wind is
>felt, it can only be distinguished from a dry katabatic wind if the
>upwind conditions are known. There is a similar but milder version of
>this in eastern Australia, but it is usually of the latter sort. There
>may be a way to measure turbulence above the peak of the high ground
>with radar or something, but I'm not familiar with that.
>
>Jim
>
>On Tue, May 12, 2020 at 6:13 PM Stefano Sofia
><stefano.sofia at regione.marche.it> wrote:
>>
>> Dear R list users,
>> I am aware that this question is not strictly related, at the present
>moment, to R code and it is more general. Please forgive me, but I need
>to share my thoughts with you.
>>
>> Foehn conditions on the southern slope of Alps happen with strong
>northerly flows that impact perpendicularly over the Apls. This
>situation triggers strong northerly leeward winds.
>> Given a single automatic weather station, I would like to identify
>these periods starting from wind direction and wind intensity data.
>Frequency of data is quarter of hour.
>> I would really find difficult to detect the moving windows of these
>events:
>> - I can't analyse data day by day;
>> - at the beginning and at the end of each event, when the process is
>not at full speed yet, the rotation is not always perfectly
>identifiable;
>> - I cannot claim in principle that the direction of each consecutive
>observation is costantly and strictly from the chosen direction.
>>
>> Does anybody have a clue on how to start to build this process in the
>right way?
>>
>> Thank you for your attention and your help
>> Stefano
>>
>>          (oo)
>> --oOO--( )--OOo----------------
>> Stefano Sofia PhD
>> Civil Protection - Marche Region
>> Meteo Section
>> Snow Section
>> Via del Colle Ameno 5
>> 60126 Torrette di Ancona, Ancona
>> Uff: 071 806 7743
>> E-mail: stefano.sofia at regione.marche.it
>> ---Oo---------oO----------------
>>
>> ________________________________
>>
>> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu?
>contenere informazioni confidenziali, pertanto ? destinato solo a
>persone autorizzate alla ricezione. I messaggi di posta elettronica per
>i client di Regione Marche possono contenere informazioni confidenziali
>e con privilegi legali. Se non si ? il destinatario specificato, non
>leggere, copiare, inoltrare o archiviare questo messaggio. Se si ?
>ricevuto questo messaggio per errore, inoltrarlo al mittente ed
>eliminarlo completamente dal sistema del proprio computer. Ai sensi
>dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit?
>ed urgenza, la risposta al presente messaggio di posta elettronica pu?
>essere visionata da persone estranee al destinatario.
>> IMPORTANT NOTICE: This e-mail message is intended to be received only
>by persons entitled to receive the confidential information it may
>contain. E-mail messages to clients of Regione Marche may contain
>information that is confidential and legally privileged. Please do not
>read, copy, forward, or store this message unless you are an intended
>recipient of it. If you have received this message in error, please
>forward it to the sender and delete it completely from your computer
>system.
>>
>> --
>> Questo messaggio  stato analizzato da Libra ESVA ed  risultato non
>infetto.
>> This message was scanned by Libra ESVA and is believed to be clean.
>>
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>--
>
>Questo messaggio  stato analizzato con Libra ESVA ed  risultato non
>infetto.
>
>
>________________________________
>
>AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
>informazioni confidenziali, pertanto ? destinato solo a persone
>autorizzate alla ricezione. I messaggi di posta elettronica per i
>client di Regione Marche possono contenere informazioni confidenziali e
>con privilegi legali. Se non si ? il destinatario specificato, non
>leggere, copiare, inoltrare o archiviare questo messaggio. Se si ?
>ricevuto questo messaggio per errore, inoltrarlo al mittente ed
>eliminarlo completamente dal sistema del proprio computer. Ai sensi
>dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit?
>ed urgenza, la risposta al presente messaggio di posta elettronica pu?
>essere visionata da persone estranee al destinatario.
>IMPORTANT NOTICE: This e-mail message is intended to be received only
>by persons entitled to receive the confidential information it may
>contain. E-mail messages to clients of Regione Marche may contain
>information that is confidential and legally privileged. Please do not
>read, copy, forward, or store this message unless you are an intended
>recipient of it. If you have received this message in error, please
>forward it to the sender and delete it completely from your computer
>system.
>
>-->
>Questo messaggio  stato analizzato da Libra ESVA ed  risultato non
>infetto.>
>This message was scanned by Libra ESVA and is believed to be clean.

-- 
Sent from my phone. Please excuse my brevity.


From r@m|ro @end|ng |rom prec|@|onb|o@@@@y@com  Sun May 17 00:50:15 2020
From: r@m|ro @end|ng |rom prec|@|onb|o@@@@y@com (Ramiro Barrantes)
Date: Sat, 16 May 2020 22:50:15 +0000
Subject: [R] Using random effects on gamm4
Message-ID: <d39552f5a8e44ac5b679363189af0347@precisionbioassay.com>

My basic question is, how do I specify random effects in gamm4 properly?

Reproducible example:

library(gamm4)
library(ggplot2)

residualNoise <- 20
blockNoise<- 10
nBlocks <- 3
n<-100
df <- data.frame(x=rep(1:n,nBlocks),y=rep(0.01*(1:n)^2,nBlocks))
df$block <- rep(1:nBlocks,each=n)
blockNoise <- rnorm(nBlocks,mean=0,sd=blockNoise)
df$y <- df$y+blockNoise[df$block]
df$y <- df$y+rnorm(1:nrow(df),mean=0,sd=residualNoise)
df$block <- factor(df$block)

fit1<-gamm4(y~s(x),random=~(1|block),data=df)
df$predicted1 <- predict(fit1$gam)

ggplot(df,aes(x,y))+
  geom_point()+
  facet_wrap(~block)+
  geom_line(aes(x=x,y=predicted1),colour="red")

fit2<-gamm4(y~s(x)+s(block,bs="re"),data=df)
df$predicted2 <- predict(fit2$gam)

ggplot(df,aes(x,y))+
  geom_point()+
  facet_wrap(~block)+
  geom_line(aes(x=x,y=predicted2),colour="red")

With fit1 I am getting the same fit on every block, whereas on fit2 I am getting a different fit on every block, although in both I am specifying random effects on block.  I would expect different fits per block when specifying random effects on them.  Why doesn't it happen on fit1?

Thank you very much in advance for any insight.

	[[alternative HTML version deleted]]


From Po||ngW @end|ng |rom @etn@@com  Sun May 17 11:50:27 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Sun, 17 May 2020 09:50:27 +0000
Subject: [R] Help with spTransform() function and final plot colors
Message-ID: <BYAPR06MB5383C3848EE99DB9776092D7AEBB0@BYAPR06MB5383.namprd06.prod.outlook.com>

#RStudio Version Version 1.2.1335 
sessionInfo() 
# R version 4.0.0 Patched (2020-05-03 r78349)
#Platform: x86_64-w64-mingw32/x64 (64-bit)
#Running under: Windows 10 x64 (build 17763)

Hello. I am running my data through a routine I found that finds clusters of data points based on distance rule.
https://gis.stackexchange.com/questions/64392/finding-clusters-of-points-based-distance-rule-using-r

1. I get this error when I get to this point in the routine, see complete routine below?
xy <- spTransform(xy, CRS("+init=epsg:27700 +datum=WGS84"))
non finite transformation detected:

I tried converting columns from numeric to Integer but did not help.

However, I continue to run the rest of the routine and the final sequence, the plot itself, seems to work
Can this error be corrected somehow, despite the fact that it continues to work, just curious what it is I guess"

2. However in the plot at the end of the routine the color key appears with the colors but the clusters in the plot are black, see plot call at the end of the routine below?

Thank you for any advice.

WHP

ERROR:
non finite transformation detected:
      coords.x1 coords.x2        
 [1,] -119.7339  39.53939 Inf Inf
 [2,] -119.7665  39.39630 Inf Inf
 [3,] -119.7794  39.28768 Inf Inf
 [4,] -121.0234  39.20503 Inf Inf
 [5,] -122.0047  47.19262 Inf Inf
 [6,] -122.0135  47.18883 Inf Inf
 [7,] -122.0379  47.52190 Inf Inf
 [8,] -122.0578  47.60975 Inf Inf
 [9,] -122.1330  47.13669 Inf Inf
[10,] -122.1509  47.55962 Inf Inf
[11,] -122.1706  47.15546 Inf Inf
[12,] -122.1846  47.23485 Inf Inf
[13,] -122.1846  48.15307 Inf Inf
[14,] -122.1851  47.44870 Inf Inf
[15,] -122.1954  47.68485 Inf Inf
[16,] -122.1990  47.51610 Inf Inf
[17,] -122.2014  47.44772 Inf Inf
[18,] -122.2025  47.69815 Inf Inf
[19,] -122.2037  47.67190 Inf Inf
[20,] -122.2090  47.40378 Inf Inf
[21,] -122.2108  47.25336 Inf Inf
[22,] -122.2291  47.63880 Inf Inf
[23,] -122.2419  47.76870 Inf Inf
[24,] -122.2722  48.04803 Inf Inf
[25,] -122.2732  47.87700 Inf Inf
[26,] -122.2804  47.77620 Inf Inf
[27,] -122.2839  47.82103 Inf Inf
[28,] -122.2890  47.86899 Inf Inf
[29,] -122.2993  47.67306 Inf Inf
[30,] -122.3180  47.38217 Inf Inf
[31,] -122.3270  47.40378 Inf Inf
[32,] -122.3474  47.43884 Inf Inf
[33,] -122.3484  47.53083 Inf Inf
[34,] -122.3581  47.27678 Inf Inf
[35,] -122.3618  47.76735 Inf Inf
[36,] -122.3700  47.56567 Inf Inf
[37,] -122.3908  47.54938 Inf Inf
[38,] -122.4128  47.64622 Inf Inf
[39,] -122.4293  47.17660 Inf Inf
[40,] -122.4621  47.44505 Inf Inf
[41,] -122.4904  47.27460 Inf Inf
[42,] -122.5515  46.93979 Inf Inf
[43,] -122.7348  42.37320 Inf Inf
[44,] -122.7827  47.31059 Inf Inf
[45,] -122.7987  47.23475 Inf Inf
[46,] -122.8385  42.35119 Inf Inf
[47,] -122.8537  42.34495 Inf Inf
[48,] -122.8904  42.39555 Inf Inf
[49,] -122.8927  42.33022 Inf Inf
[50,] -122.9451  47.37574 Inf Inf
[51,] -122.9594  42.30376 Inf Inf
[52,] -123.0641  47.16428 Inf Inf
[53,] -123.3413  42.44117 Inf Inf
Error in spTransform(xSP, CRSobj, ...) : 
  failure in points 1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:23:24:25:26:27:28:29:30:31:32:33:34:35:36:37:38:39:40:41:42:43:44:45:46:47:48:49:50:51:52:53
In addition: Warning message:
In spTransform(xSP, CRSobj, ...) : 53 projected point(s) not finite

Here is the data:
dput(sample)
structure(list(ID = 1:53, Longitude = c(-119.733899, -119.766493, 
-119.779416, -121.0234, -122.004736, -122.013456, -122.0379, 
-122.0578, -122.132971, -122.150901, -122.170608, -122.18462, 
-122.184639, -122.185079, -122.195398, -122.198994, -122.201356, 
-122.202507, -122.20371, -122.209047, -122.210797, -122.229095, 
-122.2419, -122.27216, -122.273164, -122.280355, -122.28389, 
-122.289043, -122.299261, -122.318009, -122.326987, -122.347382, 
-122.34844, -122.358115, -122.361839, -122.37003, -122.390815, 
-122.41282, -122.429323, -122.462136, -122.490417, -122.551483, 
-122.734847, -122.782736, -122.798669, -122.838498, -122.853683, 
-122.8904, -122.89271, -122.94511, -122.959407, -123.064087, 
-123.341346), Latitude = c(39.53939, 39.396298, 39.287681, 39.205028, 
47.192616, 47.188833, 47.5219, 47.609748, 47.13669, 47.559616, 
47.155455, 47.234849, 48.15307, 47.448697, 47.684854, 47.516104, 
47.447723, 47.698146, 47.6719, 47.403778, 47.253364, 47.638795, 
47.768701, 48.048027, 47.876997, 47.776205, 47.821029, 47.868987, 
47.673056, 47.382165, 47.403785, 47.438836, 47.530831, 47.276776, 
47.76735, 47.565667, 47.549377, 47.646222, 47.176596, 47.445053, 
47.274599, 46.939789, 42.373195, 47.310595, 47.234748, 42.351189, 
42.344953, 42.395547, 42.33022, 47.375736, 42.303755, 47.164278, 
42.441172)), class = "data.frame", row.names = c(NA, -53L))

Here is the routine:
require(sp)
require(rgdal)
#25*1609.34 = 40233.5      
dis <- 40233.5 #Distance threshold 25miles converted to meters

x <- tmp1b[,c(6)]
head(x)
y <- tmp1b[,c(5)]
head(y)
str(y)

xy <- SpatialPointsDataFrame(matrix(c(x,y), ncol=2), data.frame(ID=seq(1:length(x))),
                             proj4string=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84"))

str(xy)

xy <- spTransform(xy, CRS("+init=epsg:27700 +datum=WGS84"))#Error occurring here?

chc <- hclust(dist(data.frame(rownames=rownames(xy at data), x=coordinates(xy)[,1], #What is xy at data???? see str(xy)
                              y=coordinates(xy)[,2])), method="complete")
str(chc)

view(xy at data)#Look like 3 clusters

# Distance with a 25 Mile threshold  
chc.dis <- cutree(chc, h=dis) 
str(chc.dis)
view(chc.dis)

# Join results to meuse sp points
xy at data <- data.frame(xy at data, Clust=chc.dis)
str(xy at data)
view(xy at data)

# Plot results
plot(xy, col=factor(xy at data$Clust), pch=19)
box(col="black")
title(main="Clustering")
legend("topleft", legend=paste("Cluster", 1:3,sep=""),#Change from 4 in demo to 3 in my data
       col=palette()[1:3], pch=rep(19,3), bg="white")




Proprietary

NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}


From @purd|e@@ @end|ng |rom gm@||@com  Sun May 17 11:52:30 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Sun, 17 May 2020 21:52:30 +1200
Subject: [R] Classification of wind events
In-Reply-To: <8B435C9568170B469AE31E8891E8CC4F809B9B6F@ESINO.regionemarche.intra>
References: <8B435C9568170B469AE31E8891E8CC4F809B8DEA@ESINO.regionemarche.intra>
 <CA+8X3fUsXA7reESuWpGC2jBGRRryF6RcYa5jZxMWV7oktF1kFg@mail.gmail.com>
 <8B435C9568170B469AE31E8891E8CC4F809B9B6F@ESINO.regionemarche.intra>
Message-ID: <CAB8pepzWXGRBKaQ=N_pFmwnONVKMf2E6Xg2Fm+U5AbhKLx83WQ@mail.gmail.com>

Hi Stefano,

I don't have any specific suggestions, but...

If you could convert your (character) direction vector into a
(numeric) time-ordered direction vector giving radians or degrees, and
provide the corresponding speed vector...
(i.e. North -> 90 degrees).

I'd be happy to try and plot the data in a variety of ways...
...and see if there's any obvious relationships, or anything else useful.


On Sat, May 16, 2020 at 10:23 PM Stefano Sofia
<stefano.sofia at regione.marche.it> wrote:
>
> Dear Jim and Jeff,
> thank you for your comments. You are right, it is quite difficult to detect this process through a single observation point, I am awre of it.
> I need to set up an automatic algorithm to filter 20 years of data, and I have to find an easy way to do it.
> I know quite well my automatic stations, the wind direction is very stable during these situations, and therefore I would like to start from it. (I should use also wind speed, relative humidity and temperature, but I will introduce them only once I will be able to manage the direction).
> In the case of the example below reported, I know that the directions of this particular automatic station must be only SW or WSW.
>
> My biggest problem, obviously, is to find the beginning and the end of each event, when there is a change in the main direction.
> Thinking about categorical data in general, is there a way to detect periods when one particular category is more frequent?
>
> Here I reproduce a real example 24 hours long, where these Foehn condition start between 09 and 10 and finish after 19:
>
> first_day_POSIX <- as.POSIXct("2020-02-19-00-00", format="%Y-%m-%d-%H-%M")
> last_day_POSIX <- as.POSIXct("2020-02-20-00-00", format="%Y-%m-%d-%H-%M")
> mydf <- data.frame(data_POSIX=seq(first_day_POSIX, last_day_POSIX, by="10 min"))
>
> mydf$main_dir <- c(WSW, WSW, SW, SW, W, WSW, WSW, WSW, W, W, SW, WSW, SSW, S, SW, SW, WSW, WNW, W, WSW, WSW, SE, SE, SE, NW, NNE, ENE, SE, NNW, NW, NW, NW, NW, NW, NW, NE, NW, NW, NW, NW, NW, N, WNW, NW, NNW, NNW, NW, NW, NW, WNW, ESE, W, WSW, SW, SW, SW, WSW, SW, S, S, SSW, SW, WSW, WSW, WSW, WSW, WSW, WSW, WSW, SW, WSW, WSW, WSW, WSW, SW, SW, WSW, WSW, WSW, WSW, WSW, SW, SW, SW, SW, SW, SW, SW, SW, SW, WSW, WSW, WSW, WSW, SW, SW, SW, SW, WSW, SW, SW, SW, SW, SW, WSW, SW, SW, W, WSW, WSW, SSW, S, WNW, SW, W, WSW, WSW, SE, SE, SE, NW, NNE, ENE, SE, NNW, NW, NW, NW, NW, NW, NW, NE, NW, NW, NW, NW, NW, N, WNW, NW, NNW, NNW, NW, NW, NW)
>
> mydf$max_speed <- c(4.60, 4.60, 3.40, 3.10, 4.80, 4.20, 4.10, 4.50, 4.70, 4.30, 2.40, 2.30, 2.20, 2.10, 2.90, 2.80, 1.80, 2.70, 4.30, 3.30, 2.30, 2.30, 3.20, 3.20, 2.90, 2.30, 1.50, 1.80, 2.90, 2.40, 1.80, 2.40, 2.30, 2.60, 1.80, 2.30, 1.90, 2.20, 2.80, 2.40, 1.00, 1.10, 1.60, 2.30, 2.50, 3.30, 3.40, 3.20, 4.50, 3.90, 3.10, 2.40, 6.00, 7.80, 6.30, 7.80, 8.10, 6.10, 7.40, 9.50, 8.90, 9.10, 10.10, 10.50, 11.10, 10.10, 10.90, 11.30, 13.40, 13.50, 12.80, 11.50, 13.10, 13.50, 11.10, 10.50, 8.50, 10.10, 10.70, 13.60, 11.90, 14.90, 10.90, 10.90, 12.80, 12.10, 9.10, 8.30, 8.80, 7.40, 8.40, 10.30, 10.00, 7.00, 8.50, 8.40, 8.60, 6.70, 7.30, 6.20, 5.90, 5.90, 5.10, 5.80, 5.60, 6.50, 6.60, 11.70, 11.30, 8.70, 7.10, 6.90, 4.30, 3.80, 4.30, 3.30, 2.30, 2.30, 3.20, 3.20, 2.90, 2.30, 1.50, 1.80, 2.90, 2.40, 1.80, 2.40, 2.30, 2.60, 1.80, 2.30, 1.90, 2.20, 2.80, 2.40, 1.00, 1.10, 1.60, 2.30, 2.50, 3.30, 3.40, 3.20, 4.50)
>
>
> Thank you for your attention
> Stefano
>
>
>          (oo)
> --oOO--( )--OOo----------------
> Stefano Sofia PhD
> Civil Protection - Marche Region
> Meteo Section
> Snow Section
> Via del Colle Ameno 5
> 60126 Torrette di Ancona, Ancona
> Uff: 071 806 7743
> E-mail: stefano.sofia at regione.marche.it
> ---Oo---------oO----------------
>
> ________________________________________
> Da: Jim Lemon [drjimlemon at gmail.com]
> Inviato: mercoled? 13 maggio 2020 11.01
> A: Stefano Sofia; r-help mailing list
> Oggetto: Re: [R] Classification of wind events
>
> Hi Stefano,
> Given only one observation point you will find it difficult. If your
> automatic weather station is in the low area where the foehn wind is
> felt, it can only be distinguished from a dry katabatic wind if the
> upwind conditions are known. There is a similar but milder version of
> this in eastern Australia, but it is usually of the latter sort. There
> may be a way to measure turbulence above the peak of the high ground
> with radar or something, but I'm not familiar with that.
>
> Jim
>
> On Tue, May 12, 2020 at 6:13 PM Stefano Sofia
> <stefano.sofia at regione.marche.it> wrote:
> >
> > Dear R list users,
> > I am aware that this question is not strictly related, at the present moment, to R code and it is more general. Please forgive me, but I need to share my thoughts with you.
> >
> > Foehn conditions on the southern slope of Alps happen with strong northerly flows that impact perpendicularly over the Apls. This situation triggers strong northerly leeward winds.
> > Given a single automatic weather station, I would like to identify these periods starting from wind direction and wind intensity data. Frequency of data is quarter of hour.
> > I would really find difficult to detect the moving windows of these events:
> > - I can't analyse data day by day;
> > - at the beginning and at the end of each event, when the process is not at full speed yet, the rotation is not always perfectly identifiable;
> > - I cannot claim in principle that the direction of each consecutive observation is costantly and strictly from the chosen direction.
> >
> > Does anybody have a clue on how to start to build this process in the right way?
> >
> > Thank you for your attention and your help
> > Stefano
> >
> >          (oo)
> > --oOO--( )--OOo----------------
> > Stefano Sofia PhD
> > Civil Protection - Marche Region
> > Meteo Section
> > Snow Section
> > Via del Colle Ameno 5
> > 60126 Torrette di Ancona, Ancona
> > Uff: 071 806 7743
> > E-mail: stefano.sofia at regione.marche.it
> > ---Oo---------oO----------------
> >
> > ________________________________
> >
> > AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> > IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
> >
> > --
> > Questo messaggio  stato analizzato da Libra ESVA ed  risultato non infetto.
> > This message was scanned by Libra ESVA and is believed to be clean.
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
>
> Questo messaggio  stato analizzato con Libra ESVA ed  risultato non infetto.
>
>
> ________________________________
>
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
>
> --
> Questo messaggio  stato analizzato da Libra ESVA ed  risultato non infetto.
> This message was scanned by Libra ESVA and is believed to be clean.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @purd|e@@ @end|ng |rom gm@||@com  Sun May 17 12:07:07 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Sun, 17 May 2020 22:07:07 +1200
Subject: [R] Classification of wind events
In-Reply-To: <CAB8pepzWXGRBKaQ=N_pFmwnONVKMf2E6Xg2Fm+U5AbhKLx83WQ@mail.gmail.com>
References: <8B435C9568170B469AE31E8891E8CC4F809B8DEA@ESINO.regionemarche.intra>
 <CA+8X3fUsXA7reESuWpGC2jBGRRryF6RcYa5jZxMWV7oktF1kFg@mail.gmail.com>
 <8B435C9568170B469AE31E8891E8CC4F809B9B6F@ESINO.regionemarche.intra>
 <CAB8pepzWXGRBKaQ=N_pFmwnONVKMf2E6Xg2Fm+U5AbhKLx83WQ@mail.gmail.com>
Message-ID: <CAB8pepzzYgTHL9vW2U2_UW7yE5=2om2AUuCsr9+rRdgBCSx=FA@mail.gmail.com>

Sorry, please put my last post aside.
I mis-read your question.

I agree with Jim's comments.









On Sun, May 17, 2020 at 9:52 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
>
> Hi Stefano,
>
> I don't have any specific suggestions, but...
>
> If you could convert your (character) direction vector into a
> (numeric) time-ordered direction vector giving radians or degrees, and
> provide the corresponding speed vector...
> (i.e. North -> 90 degrees).
>
> I'd be happy to try and plot the data in a variety of ways...
> ...and see if there's any obvious relationships, or anything else useful.
>
>
> On Sat, May 16, 2020 at 10:23 PM Stefano Sofia
> <stefano.sofia at regione.marche.it> wrote:
> >
> > Dear Jim and Jeff,
> > thank you for your comments. You are right, it is quite difficult to detect this process through a single observation point, I am awre of it.
> > I need to set up an automatic algorithm to filter 20 years of data, and I have to find an easy way to do it.
> > I know quite well my automatic stations, the wind direction is very stable during these situations, and therefore I would like to start from it. (I should use also wind speed, relative humidity and temperature, but I will introduce them only once I will be able to manage the direction).
> > In the case of the example below reported, I know that the directions of this particular automatic station must be only SW or WSW.
> >
> > My biggest problem, obviously, is to find the beginning and the end of each event, when there is a change in the main direction.
> > Thinking about categorical data in general, is there a way to detect periods when one particular category is more frequent?
> >
> > Here I reproduce a real example 24 hours long, where these Foehn condition start between 09 and 10 and finish after 19:
> >
> > first_day_POSIX <- as.POSIXct("2020-02-19-00-00", format="%Y-%m-%d-%H-%M")
> > last_day_POSIX <- as.POSIXct("2020-02-20-00-00", format="%Y-%m-%d-%H-%M")
> > mydf <- data.frame(data_POSIX=seq(first_day_POSIX, last_day_POSIX, by="10 min"))
> >
> > mydf$main_dir <- c(WSW, WSW, SW, SW, W, WSW, WSW, WSW, W, W, SW, WSW, SSW, S, SW, SW, WSW, WNW, W, WSW, WSW, SE, SE, SE, NW, NNE, ENE, SE, NNW, NW, NW, NW, NW, NW, NW, NE, NW, NW, NW, NW, NW, N, WNW, NW, NNW, NNW, NW, NW, NW, WNW, ESE, W, WSW, SW, SW, SW, WSW, SW, S, S, SSW, SW, WSW, WSW, WSW, WSW, WSW, WSW, WSW, SW, WSW, WSW, WSW, WSW, SW, SW, WSW, WSW, WSW, WSW, WSW, SW, SW, SW, SW, SW, SW, SW, SW, SW, WSW, WSW, WSW, WSW, SW, SW, SW, SW, WSW, SW, SW, SW, SW, SW, WSW, SW, SW, W, WSW, WSW, SSW, S, WNW, SW, W, WSW, WSW, SE, SE, SE, NW, NNE, ENE, SE, NNW, NW, NW, NW, NW, NW, NW, NE, NW, NW, NW, NW, NW, N, WNW, NW, NNW, NNW, NW, NW, NW)
> >
> > mydf$max_speed <- c(4.60, 4.60, 3.40, 3.10, 4.80, 4.20, 4.10, 4.50, 4.70, 4.30, 2.40, 2.30, 2.20, 2.10, 2.90, 2.80, 1.80, 2.70, 4.30, 3.30, 2.30, 2.30, 3.20, 3.20, 2.90, 2.30, 1.50, 1.80, 2.90, 2.40, 1.80, 2.40, 2.30, 2.60, 1.80, 2.30, 1.90, 2.20, 2.80, 2.40, 1.00, 1.10, 1.60, 2.30, 2.50, 3.30, 3.40, 3.20, 4.50, 3.90, 3.10, 2.40, 6.00, 7.80, 6.30, 7.80, 8.10, 6.10, 7.40, 9.50, 8.90, 9.10, 10.10, 10.50, 11.10, 10.10, 10.90, 11.30, 13.40, 13.50, 12.80, 11.50, 13.10, 13.50, 11.10, 10.50, 8.50, 10.10, 10.70, 13.60, 11.90, 14.90, 10.90, 10.90, 12.80, 12.10, 9.10, 8.30, 8.80, 7.40, 8.40, 10.30, 10.00, 7.00, 8.50, 8.40, 8.60, 6.70, 7.30, 6.20, 5.90, 5.90, 5.10, 5.80, 5.60, 6.50, 6.60, 11.70, 11.30, 8.70, 7.10, 6.90, 4.30, 3.80, 4.30, 3.30, 2.30, 2.30, 3.20, 3.20, 2.90, 2.30, 1.50, 1.80, 2.90, 2.40, 1.80, 2.40, 2.30, 2.60, 1.80, 2.30, 1.90, 2.20, 2.80, 2.40, 1.00, 1.10, 1.60, 2.30, 2.50, 3.30, 3.40, 3.20, 4.50)
> >
> >
> > Thank you for your attention
> > Stefano
> >
> >
> >          (oo)
> > --oOO--( )--OOo----------------
> > Stefano Sofia PhD
> > Civil Protection - Marche Region
> > Meteo Section
> > Snow Section
> > Via del Colle Ameno 5
> > 60126 Torrette di Ancona, Ancona
> > Uff: 071 806 7743
> > E-mail: stefano.sofia at regione.marche.it
> > ---Oo---------oO----------------
> >
> > ________________________________________
> > Da: Jim Lemon [drjimlemon at gmail.com]
> > Inviato: mercoled? 13 maggio 2020 11.01
> > A: Stefano Sofia; r-help mailing list
> > Oggetto: Re: [R] Classification of wind events
> >
> > Hi Stefano,
> > Given only one observation point you will find it difficult. If your
> > automatic weather station is in the low area where the foehn wind is
> > felt, it can only be distinguished from a dry katabatic wind if the
> > upwind conditions are known. There is a similar but milder version of
> > this in eastern Australia, but it is usually of the latter sort. There
> > may be a way to measure turbulence above the peak of the high ground
> > with radar or something, but I'm not familiar with that.
> >
> > Jim
> >
> > On Tue, May 12, 2020 at 6:13 PM Stefano Sofia
> > <stefano.sofia at regione.marche.it> wrote:
> > >
> > > Dear R list users,
> > > I am aware that this question is not strictly related, at the present moment, to R code and it is more general. Please forgive me, but I need to share my thoughts with you.
> > >
> > > Foehn conditions on the southern slope of Alps happen with strong northerly flows that impact perpendicularly over the Apls. This situation triggers strong northerly leeward winds.
> > > Given a single automatic weather station, I would like to identify these periods starting from wind direction and wind intensity data. Frequency of data is quarter of hour.
> > > I would really find difficult to detect the moving windows of these events:
> > > - I can't analyse data day by day;
> > > - at the beginning and at the end of each event, when the process is not at full speed yet, the rotation is not always perfectly identifiable;
> > > - I cannot claim in principle that the direction of each consecutive observation is costantly and strictly from the chosen direction.
> > >
> > > Does anybody have a clue on how to start to build this process in the right way?
> > >
> > > Thank you for your attention and your help
> > > Stefano
> > >
> > >          (oo)
> > > --oOO--( )--OOo----------------
> > > Stefano Sofia PhD
> > > Civil Protection - Marche Region
> > > Meteo Section
> > > Snow Section
> > > Via del Colle Ameno 5
> > > 60126 Torrette di Ancona, Ancona
> > > Uff: 071 806 7743
> > > E-mail: stefano.sofia at regione.marche.it
> > > ---Oo---------oO----------------
> > >
> > > ________________________________
> > >
> > > AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> > > IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
> > >
> > > --
> > > Questo messaggio  stato analizzato da Libra ESVA ed  risultato non infetto.
> > > This message was scanned by Libra ESVA and is believed to be clean.
> > >
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > --
> >
> > Questo messaggio  stato analizzato con Libra ESVA ed  risultato non infetto.
> >
> >
> > ________________________________
> >
> > AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> > IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
> >
> > --
> > Questo messaggio  stato analizzato da Libra ESVA ed  risultato non infetto.
> > This message was scanned by Libra ESVA and is believed to be clean.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From Po||ngW @end|ng |rom @etn@@com  Sun May 17 12:20:18 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Sun, 17 May 2020 10:20:18 +0000
Subject: [R] Help with spTransform() function and final plot colors
In-Reply-To: <BYAPR06MB5383C3848EE99DB9776092D7AEBB0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB5383C3848EE99DB9776092D7AEBB0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <BYAPR06MB5383C86A2FC0CA02394A9BDBAEBB0@BYAPR06MB5383.namprd06.prod.outlook.com>

Hello, I have found an additional problem.

I should be getting 3 columns back in the xy at data at some point.
$ ID                           : int  1 2 3 4 5 6 7 8 9 10 ...
# $ Clust                  : int  1 1 1 1 1 1 1 1 1 1 ... This is always - 1
# $ Clust.1               : int  1 1 1 1 1 1 1 1 1 1 ... This would be the cluster number, like 1-3, I am not getting this column back?


Yet the plot runs and indicates three different clusters?



WHP


Proprietary

-----Original Message-----
From: Poling, William <PolingW at aetna.com> 
Sent: Sunday, May 17, 2020 4:50 AM
To: r-help at r-project.org
Cc: Poling, William <PolingW at aetna.com>
Subject: Help with spTransform() function and final plot colors

#RStudio Version Version 1.2.1335
sessionInfo()
# R version 4.0.0 Patched (2020-05-03 r78349)
#Platform: x86_64-w64-mingw32/x64 (64-bit) #Running under: Windows 10 x64 (build 17763)

Hello. I am running my data through a routine I found that finds clusters of data points based on distance rule.
https://gis.stackexchange.com/questions/64392/finding-clusters-of-points-based-distance-rule-using-r

1. I get this error when I get to this point in the routine, see complete routine below?
xy <- spTransform(xy, CRS("+init=epsg:27700 +datum=WGS84")) non finite transformation detected:

I tried converting columns from numeric to Integer but did not help.

However, I continue to run the rest of the routine and the final sequence, the plot itself, seems to work Can this error be corrected somehow, despite the fact that it continues to work, just curious what it is I guess"

2. However in the plot at the end of the routine the color key appears with the colors but the clusters in the plot are black, see plot call at the end of the routine below?

Thank you for any advice.

WHP

ERROR:
non finite transformation detected:
      coords.x1 coords.x2        
 [1,] -119.7339  39.53939 Inf Inf
 [2,] -119.7665  39.39630 Inf Inf
 [3,] -119.7794  39.28768 Inf Inf
 [4,] -121.0234  39.20503 Inf Inf
 [5,] -122.0047  47.19262 Inf Inf
 [6,] -122.0135  47.18883 Inf Inf
 [7,] -122.0379  47.52190 Inf Inf
 [8,] -122.0578  47.60975 Inf Inf
 [9,] -122.1330  47.13669 Inf Inf
[10,] -122.1509  47.55962 Inf Inf
[11,] -122.1706  47.15546 Inf Inf
[12,] -122.1846  47.23485 Inf Inf
[13,] -122.1846  48.15307 Inf Inf
[14,] -122.1851  47.44870 Inf Inf
[15,] -122.1954  47.68485 Inf Inf
[16,] -122.1990  47.51610 Inf Inf
[17,] -122.2014  47.44772 Inf Inf
[18,] -122.2025  47.69815 Inf Inf
[19,] -122.2037  47.67190 Inf Inf
[20,] -122.2090  47.40378 Inf Inf
[21,] -122.2108  47.25336 Inf Inf
[22,] -122.2291  47.63880 Inf Inf
[23,] -122.2419  47.76870 Inf Inf
[24,] -122.2722  48.04803 Inf Inf
[25,] -122.2732  47.87700 Inf Inf
[26,] -122.2804  47.77620 Inf Inf
[27,] -122.2839  47.82103 Inf Inf
[28,] -122.2890  47.86899 Inf Inf
[29,] -122.2993  47.67306 Inf Inf
[30,] -122.3180  47.38217 Inf Inf
[31,] -122.3270  47.40378 Inf Inf
[32,] -122.3474  47.43884 Inf Inf
[33,] -122.3484  47.53083 Inf Inf
[34,] -122.3581  47.27678 Inf Inf
[35,] -122.3618  47.76735 Inf Inf
[36,] -122.3700  47.56567 Inf Inf
[37,] -122.3908  47.54938 Inf Inf
[38,] -122.4128  47.64622 Inf Inf
[39,] -122.4293  47.17660 Inf Inf
[40,] -122.4621  47.44505 Inf Inf
[41,] -122.4904  47.27460 Inf Inf
[42,] -122.5515  46.93979 Inf Inf
[43,] -122.7348  42.37320 Inf Inf
[44,] -122.7827  47.31059 Inf Inf
[45,] -122.7987  47.23475 Inf Inf
[46,] -122.8385  42.35119 Inf Inf
[47,] -122.8537  42.34495 Inf Inf
[48,] -122.8904  42.39555 Inf Inf
[49,] -122.8927  42.33022 Inf Inf
[50,] -122.9451  47.37574 Inf Inf
[51,] -122.9594  42.30376 Inf Inf
[52,] -123.0641  47.16428 Inf Inf
[53,] -123.3413  42.44117 Inf Inf
Error in spTransform(xSP, CRSobj, ...) : 
  failure in points 1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:23:24:25:26:27:28:29:30:31:32:33:34:35:36:37:38:39:40:41:42:43:44:45:46:47:48:49:50:51:52:53
In addition: Warning message:
In spTransform(xSP, CRSobj, ...) : 53 projected point(s) not finite

Here is the data:
dput(sample)
structure(list(ID = 1:53, Longitude = c(-119.733899, -119.766493, -119.779416, -121.0234, -122.004736, -122.013456, -122.0379, -122.0578, -122.132971, -122.150901, -122.170608, -122.18462, -122.184639, -122.185079, -122.195398, -122.198994, -122.201356, -122.202507, -122.20371, -122.209047, -122.210797, -122.229095, -122.2419, -122.27216, -122.273164, -122.280355, -122.28389, -122.289043, -122.299261, -122.318009, -122.326987, -122.347382, -122.34844, -122.358115, -122.361839, -122.37003, -122.390815, -122.41282, -122.429323, -122.462136, -122.490417, -122.551483, -122.734847, -122.782736, -122.798669, -122.838498, -122.853683, -122.8904, -122.89271, -122.94511, -122.959407, -123.064087, -123.341346), Latitude = c(39.53939, 39.396298, 39.287681, 39.205028, 47.192616, 47.188833, 47.5219, 47.609748, 47.13669, 47.559616, 47.155455, 47.234849, 48.15307, 47.448697, 47.684854, 47.516104, 47.447723, 47.698146, 47.6719, 47.403778, 47.253364, 47.638795, 47.768701, 48.048027, 47.876997, 47.776205, 47.821029, 47.868987, 47.673056, 47.382165, 47.403785, 47.438836, 47.530831, 47.276776, 47.76735, 47.565667, 47.549377, 47.646222, 47.176596, 47.445053, 47.274599, 46.939789, 42.373195, 47.310595, 47.234748, 42.351189, 42.344953, 42.395547, 42.33022, 47.375736, 42.303755, 47.164278, 42.441172)), class = "data.frame", row.names = c(NA, -53L))

Here is the routine:
require(sp)
require(rgdal)
#25*1609.34 = 40233.5      
dis <- 40233.5 #Distance threshold 25miles converted to meters

x <- tmp1b[,c(6)]
head(x)
y <- tmp1b[,c(5)]
head(y)
str(y)

xy <- SpatialPointsDataFrame(matrix(c(x,y), ncol=2), data.frame(ID=seq(1:length(x))),
                             proj4string=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84"))

str(xy)

xy <- spTransform(xy, CRS("+init=epsg:27700 +datum=WGS84"))#Error occurring here?

chc <- hclust(dist(data.frame(rownames=rownames(xy at data), x=coordinates(xy)[,1], #What is xy at data???? see str(xy)
                              y=coordinates(xy)[,2])), method="complete")
str(chc)

view(xy at data)#Look like 3 clusters

# Distance with a 25 Mile threshold
chc.dis <- cutree(chc, h=dis)
str(chc.dis)
view(chc.dis)

# Join results to meuse sp points
xy at data <- data.frame(xy at data, Clust=chc.dis)
str(xy at data)
view(xy at data)

# Plot results
plot(xy, col=factor(xy at data$Clust), pch=19)
box(col="black")
title(main="Clustering")
legend("topleft", legend=paste("Cluster", 1:3,sep=""),#Change from 4 in demo to 3 in my data
       col=palette()[1:3], pch=rep(19,3), bg="white")




Proprietary

NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}


From drj|m|emon @end|ng |rom gm@||@com  Sun May 17 12:41:14 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sun, 17 May 2020 20:41:14 +1000
Subject: [R] Classification of wind events
In-Reply-To: <CAB8pepzzYgTHL9vW2U2_UW7yE5=2om2AUuCsr9+rRdgBCSx=FA@mail.gmail.com>
References: <8B435C9568170B469AE31E8891E8CC4F809B8DEA@ESINO.regionemarche.intra>
 <CA+8X3fUsXA7reESuWpGC2jBGRRryF6RcYa5jZxMWV7oktF1kFg@mail.gmail.com>
 <8B435C9568170B469AE31E8891E8CC4F809B9B6F@ESINO.regionemarche.intra>
 <CAB8pepzWXGRBKaQ=N_pFmwnONVKMf2E6Xg2Fm+U5AbhKLx83WQ@mail.gmail.com>
 <CAB8pepzzYgTHL9vW2U2_UW7yE5=2om2AUuCsr9+rRdgBCSx=FA@mail.gmail.com>
Message-ID: <CA+8X3fXVbPywQ_ZxN3MpXWG++nvWLdB5Cx1yNaa9cTY+2pM-ig@mail.gmail.com>

Hi Stefano,
I don't know whether this will help you, but here is a way to
visualize wind speed and direction using clock24.plot:

first_day_POSIX <- as.POSIXct("2020-02-19-00-00", format="%Y-%m-%d-%H-%M")
last_day_POSIX <- as.POSIXct("2020-02-20-00-00", format="%Y-%m-%d-%H-%M")
mydf <- data.frame(data_POSIX=seq(first_day_POSIX, last_day_POSIX, by="10 min"))
mydf$main_dir<-c("WSW","WSW","SW","SW","W","WSW","WSW","WSW","W","W","SW",
 "WSW","SSW","S","SW","SW","WSW","WNW","W","WSW","WSW","SE","SE","SE",
 "NW","NNE","ENE","SE","NNW","NW","NW","NW","NW","NW","NW","NE","NW","NW",
 "NW","NW","NW","N","WNW","NW","NNW","NNW","NW","NW","NW","WNW","ESE","W",
 "WSW","SW","SW","SW","WSW","SW","S","S","SSW","SW","WSW","WSW","WSW","WSW",
 "WSW","WSW","WSW","SW","WSW","WSW","WSW","WSW","SW","SW","WSW","WSW","WSW",
 "WSW","WSW","SW","SW","SW","SW","SW","SW","SW","SW","SW","WSW","WSW","WSW",
 "WSW","SW","SW","SW","SW","WSW","SW","SW","SW","SW","SW","WSW","SW","SW",
 "W","WSW","WSW","SSW","S","WNW","SW","W","WSW","WSW","SE","SE","SE","NW",
 "NNE","ENE","SE","NNW","NW","NW","NW","NW","NW","NW","NE","NW","NW","NW",
 "NW","NW","N","WNW","NW","NNW","NNW","NW","NW","NW")
mydf$max_speed <- c(4.60, 4.60, 3.40, 3.10, 4.80, 4.20, 4.10, 4.50,
4.70, 4.30, 2.40, 2.30, 2.20, 2.10, 2.90, 2.80, 1.80, 2.70, 4.30,
3.30, 2.30, 2.30, 3.20, 3.20, 2.90, 2.30, 1.50, 1.80, 2.90, 2.40,
1.80, 2.40, 2.30, 2.60, 1.80, 2.30, 1.90, 2.20, 2.80, 2.40, 1.00,
1.10, 1.60, 2.30, 2.50, 3.30, 3.40, 3.20, 4.50, 3.90, 3.10, 2.40,
6.00, 7.80, 6.30, 7.80, 8.10, 6.10, 7.40, 9.50, 8.90, 9.10, 10.10,
10.50, 11.10, 10.10, 10.90, 11.30, 13.40, 13.50, 12.80, 11.50, 13.10,
13.50, 11.10, 10.50, 8.50, 10.10, 10.70, 13.60, 11.90, 14.90, 10.90,
10.90, 12.80, 12.10, 9.10, 8.30, 8.80, 7.40, 8.40, 10.30, 10.00, 7.00,
8.50, 8.40, 8.60, 6.70, 7.30, 6.20, 5.90, 5.90, 5.10, 5.80, 5.60,
6.50, 6.60, 11.70, 11.30, 8.70, 7.10, 6.90, 4.30, 3.80, 4.30, 3.30,
2.30, 2.30, 3.20, 3.20, 2.90, 2.30, 1.50, 1.80, 2.90, 2.40, 1.80,
2.40, 2.30, 2.60, 1.80, 2.30, 1.90, 2.20, 2.80, 2.40, 1.00, 1.10,
1.60, 2.30, 2.50, 3.30, 3.40, 3.20, 4.50)
mydf$main_dir<-factor(mydf$main_dir,
 levels=c("N","NNE","NE","ENE","E","ESE","SE","SSE",
 "S","SSW","SW","WSW","W","WNW","NW","NNW"))
dir2degrees<-seq(0,337.5,by=22.5)
mydf$wind_dir<-dir2degrees[as.numeric(mydf$main_dir)]
library(plotrix)
png("SS_spd_dir.png")
clock24.plot(mydf$max_speed,minutes=TRUE,mar=c(4,2,3,2),
 line.col=rainbow(16)[as.numeric(mydf$main_dir)],
 main="Wind speed and direction (color)")
legend(0,-17.5,levels(mydf$main_dir),fill=rainbow(16),
 xjust=0.5,ncol=8,cex=0.8)
dev.off()

Jim

On Sun, May 17, 2020 at 8:07 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
>
> Sorry, please put my last post aside.
> I mis-read your question.
>
> I agree with Jim's comments.
>
>
>
>
>
>
>
>
>
> On Sun, May 17, 2020 at 9:52 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
> >
> > Hi Stefano,
> >
> > I don't have any specific suggestions, but...
> >
> > If you could convert your (character) direction vector into a
> > (numeric) time-ordered direction vector giving radians or degrees, and
> > provide the corresponding speed vector...
> > (i.e. North -> 90 degrees).
> >
> > I'd be happy to try and plot the data in a variety of ways...
> > ...and see if there's any obvious relationships, or anything else useful.
> >
> >
> > On Sat, May 16, 2020 at 10:23 PM Stefano Sofia
> > <stefano.sofia at regione.marche.it> wrote:
> > >
> > > Dear Jim and Jeff,
> > > thank you for your comments. You are right, it is quite difficult to detect this process through a single observation point, I am awre of it.
> > > I need to set up an automatic algorithm to filter 20 years of data, and I have to find an easy way to do it.
> > > I know quite well my automatic stations, the wind direction is very stable during these situations, and therefore I would like to start from it. (I should use also wind speed, relative humidity and temperature, but I will introduce them only once I will be able to manage the direction).
> > > In the case of the example below reported, I know that the directions of this particular automatic station must be only SW or WSW.
> > >
> > > My biggest problem, obviously, is to find the beginning and the end of each event, when there is a change in the main direction.
> > > Thinking about categorical data in general, is there a way to detect periods when one particular category is more frequent?
> > >
> > > Here I reproduce a real example 24 hours long, where these Foehn condition start between 09 and 10 and finish after 19:
> > >
> > > first_day_POSIX <- as.POSIXct("2020-02-19-00-00", format="%Y-%m-%d-%H-%M")
> > > last_day_POSIX <- as.POSIXct("2020-02-20-00-00", format="%Y-%m-%d-%H-%M")
> > > mydf <- data.frame(data_POSIX=seq(first_day_POSIX, last_day_POSIX, by="10 min"))
> > >
> > > mydf$main_dir <- c(WSW, WSW, SW, SW, W, WSW, WSW, WSW, W, W, SW, WSW, SSW, S, SW, SW, WSW, WNW, W, WSW, WSW, SE, SE, SE, NW, NNE, ENE, SE, NNW, NW, NW, NW, NW, NW, NW, NE, NW, NW, NW, NW, NW, N, WNW, NW, NNW, NNW, NW, NW, NW, WNW, ESE, W, WSW, SW, SW, SW, WSW, SW, S, S, SSW, SW, WSW, WSW, WSW, WSW, WSW, WSW, WSW, SW, WSW, WSW, WSW, WSW, SW, SW, WSW, WSW, WSW, WSW, WSW, SW, SW, SW, SW, SW, SW, SW, SW, SW, WSW, WSW, WSW, WSW, SW, SW, SW, SW, WSW, SW, SW, SW, SW, SW, WSW, SW, SW, W, WSW, WSW, SSW, S, WNW, SW, W, WSW, WSW, SE, SE, SE, NW, NNE, ENE, SE, NNW, NW, NW, NW, NW, NW, NW, NE, NW, NW, NW, NW, NW, N, WNW, NW, NNW, NNW, NW, NW, NW)
> > >
> > > mydf$max_speed <- c(4.60, 4.60, 3.40, 3.10, 4.80, 4.20, 4.10, 4.50, 4.70, 4.30, 2.40, 2.30, 2.20, 2.10, 2.90, 2.80, 1.80, 2.70, 4.30, 3.30, 2.30, 2.30, 3.20, 3.20, 2.90, 2.30, 1.50, 1.80, 2.90, 2.40, 1.80, 2.40, 2.30, 2.60, 1.80, 2.30, 1.90, 2.20, 2.80, 2.40, 1.00, 1.10, 1.60, 2.30, 2.50, 3.30, 3.40, 3.20, 4.50, 3.90, 3.10, 2.40, 6.00, 7.80, 6.30, 7.80, 8.10, 6.10, 7.40, 9.50, 8.90, 9.10, 10.10, 10.50, 11.10, 10.10, 10.90, 11.30, 13.40, 13.50, 12.80, 11.50, 13.10, 13.50, 11.10, 10.50, 8.50, 10.10, 10.70, 13.60, 11.90, 14.90, 10.90, 10.90, 12.80, 12.10, 9.10, 8.30, 8.80, 7.40, 8.40, 10.30, 10.00, 7.00, 8.50, 8.40, 8.60, 6.70, 7.30, 6.20, 5.90, 5.90, 5.10, 5.80, 5.60, 6.50, 6.60, 11.70, 11.30, 8.70, 7.10, 6.90, 4.30, 3.80, 4.30, 3.30, 2.30, 2.30, 3.20, 3.20, 2.90, 2.30, 1.50, 1.80, 2.90, 2.40, 1.80, 2.40, 2.30, 2.60, 1.80, 2.30, 1.90, 2.20, 2.80, 2.40, 1.00, 1.10, 1.60, 2.30, 2.50, 3.30, 3.40, 3.20, 4.50)
> > >
> > >
> > > Thank you for your attention
> > > Stefano
> > >
> > >
> > >          (oo)
> > > --oOO--( )--OOo----------------
> > > Stefano Sofia PhD
> > > Civil Protection - Marche Region
> > > Meteo Section
> > > Snow Section
> > > Via del Colle Ameno 5
> > > 60126 Torrette di Ancona, Ancona
> > > Uff: 071 806 7743
> > > E-mail: stefano.sofia at regione.marche.it
> > > ---Oo---------oO----------------
> > >
> > > ________________________________________
> > > Da: Jim Lemon [drjimlemon at gmail.com]
> > > Inviato: mercoled? 13 maggio 2020 11.01
> > > A: Stefano Sofia; r-help mailing list
> > > Oggetto: Re: [R] Classification of wind events
> > >
> > > Hi Stefano,
> > > Given only one observation point you will find it difficult. If your
> > > automatic weather station is in the low area where the foehn wind is
> > > felt, it can only be distinguished from a dry katabatic wind if the
> > > upwind conditions are known. There is a similar but milder version of
> > > this in eastern Australia, but it is usually of the latter sort. There
> > > may be a way to measure turbulence above the peak of the high ground
> > > with radar or something, but I'm not familiar with that.
> > >
> > > Jim
> > >
> > > On Tue, May 12, 2020 at 6:13 PM Stefano Sofia
> > > <stefano.sofia at regione.marche.it> wrote:
> > > >
> > > > Dear R list users,
> > > > I am aware that this question is not strictly related, at the present moment, to R code and it is more general. Please forgive me, but I need to share my thoughts with you.
> > > >
> > > > Foehn conditions on the southern slope of Alps happen with strong northerly flows that impact perpendicularly over the Apls. This situation triggers strong northerly leeward winds.
> > > > Given a single automatic weather station, I would like to identify these periods starting from wind direction and wind intensity data. Frequency of data is quarter of hour.
> > > > I would really find difficult to detect the moving windows of these events:
> > > > - I can't analyse data day by day;
> > > > - at the beginning and at the end of each event, when the process is not at full speed yet, the rotation is not always perfectly identifiable;
> > > > - I cannot claim in principle that the direction of each consecutive observation is costantly and strictly from the chosen direction.
> > > >
> > > > Does anybody have a clue on how to start to build this process in the right way?
> > > >
> > > > Thank you for your attention and your help
> > > > Stefano
> > > >
> > > >          (oo)
> > > > --oOO--( )--OOo----------------
> > > > Stefano Sofia PhD
> > > > Civil Protection - Marche Region
> > > > Meteo Section
> > > > Snow Section
> > > > Via del Colle Ameno 5
> > > > 60126 Torrette di Ancona, Ancona
> > > > Uff: 071 806 7743
> > > > E-mail: stefano.sofia at regione.marche.it
> > > > ---Oo---------oO----------------
> > > >
> > > > ________________________________
> > > >
> > > > AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> > > > IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
> > > >
> > > > --
> > > > Questo messaggio  stato analizzato da Libra ESVA ed  risultato non infetto.
> > > > This message was scanned by Libra ESVA and is believed to be clean.
> > > >
> > > >
> > > >         [[alternative HTML version deleted]]
> > > >
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > > --
> > >
> > > Questo messaggio  stato analizzato con Libra ESVA ed  risultato non infetto.
> > >
> > >
> > > ________________________________
> > >
> > > AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> > > IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
> > >
> > > --
> > > Questo messaggio  stato analizzato da Libra ESVA ed  risultato non infetto.
> > > This message was scanned by Libra ESVA and is believed to be clean.
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: SS_spd_dir.png
Type: image/png
Size: 81812 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200517/39122647/attachment.png>

From Po||ngW @end|ng |rom @etn@@com  Sun May 17 12:43:21 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Sun, 17 May 2020 10:43:21 +0000
Subject: [R] Help with spTransform() function and final plot colors
In-Reply-To: <BYAPR06MB5383C86A2FC0CA02394A9BDBAEBB0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB5383C3848EE99DB9776092D7AEBB0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <BYAPR06MB5383C86A2FC0CA02394A9BDBAEBB0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <BYAPR06MB538316F40C8301D86E4BF717AEBB0@BYAPR06MB5383.namprd06.prod.outlook.com>

Hello again.

I also found this discussion on non-finite transformation error, however, I am not sure what to look for in the output after I apply my data?

https://stackoverflow.com/questions/14880294/non-finite-transformation-detected-in-sptransform-in-rgdal-r-package


str(sample)
GPS.Points <- sample[,c(2,3)]

n_chunks <- 3 #number of pieces you will break you data into
n.points <- dim(GPS.Points)[1]
breaks <- seq(1,n.points, by=round(n.points/n_chunks))
breaks <- c(breaks, n.points) #make sure to include last points as well

i=1
for (i in 1:(length(breaks)-1)){
  cat('\n','converting points', breaks[i], "to", breaks[i+1])  
  temp.GPS.Points=GPS.Points[breaks[i]:breaks[i+1],]
  temp.GPS.Points.Spatial.Data <- SpatialPoints(temp.GPS.Points, 
                                                proj4string=CRS("+proj=longlat +ellps=WGS84"))
  temp.GPS.Points.UTM.Spatial.Data <- spTransform(temp.GPS.Points.Spatial.Data,
                                                  CRS("+proj=utm +south +zone=34 
+ellps=WGS84"))
}

Thank you for any advice.

WHP

Proprietary

-----Original Message-----
From: Poling, William <PolingW at aetna.com> 
Sent: Sunday, May 17, 2020 5:20 AM
To: r-help at r-project.org
Cc: Poling, William <PolingW at aetna.com>
Subject: RE: Help with spTransform() function and final plot colors

Hello, I have found an additional problem.

I should be getting 3 columns back in the xy at data at some point.
$ ID                           : int  1 2 3 4 5 6 7 8 9 10 ...
# $ Clust                  : int  1 1 1 1 1 1 1 1 1 1 ... This is always - 1
# $ Clust.1               : int  1 1 1 1 1 1 1 1 1 1 ... This would be the cluster number, like 1-3, I am not getting this column back?


Yet the plot runs and indicates three different clusters?



WHP


Proprietary

-----Original Message-----
From: Poling, William <PolingW at aetna.com> 
Sent: Sunday, May 17, 2020 4:50 AM
To: r-help at r-project.org
Cc: Poling, William <PolingW at aetna.com>
Subject: Help with spTransform() function and final plot colors

#RStudio Version Version 1.2.1335
sessionInfo()
# R version 4.0.0 Patched (2020-05-03 r78349)
#Platform: x86_64-w64-mingw32/x64 (64-bit) #Running under: Windows 10 x64 (build 17763)

Hello. I am running my data through a routine I found that finds clusters of data points based on distance rule.
https://gis.stackexchange.com/questions/64392/finding-clusters-of-points-based-distance-rule-using-r

1. I get this error when I get to this point in the routine, see complete routine below?
xy <- spTransform(xy, CRS("+init=epsg:27700 +datum=WGS84")) non finite transformation detected:

I tried converting columns from numeric to Integer but did not help.

However, I continue to run the rest of the routine and the final sequence, the plot itself, seems to work Can this error be corrected somehow, despite the fact that it continues to work, just curious what it is I guess"

2. However in the plot at the end of the routine the color key appears with the colors but the clusters in the plot are black, see plot call at the end of the routine below?

Thank you for any advice.

WHP

ERROR:
non finite transformation detected:
      coords.x1 coords.x2        
 [1,] -119.7339  39.53939 Inf Inf
 [2,] -119.7665  39.39630 Inf Inf
 [3,] -119.7794  39.28768 Inf Inf
 [4,] -121.0234  39.20503 Inf Inf
 [5,] -122.0047  47.19262 Inf Inf
 [6,] -122.0135  47.18883 Inf Inf
 [7,] -122.0379  47.52190 Inf Inf
 [8,] -122.0578  47.60975 Inf Inf
 [9,] -122.1330  47.13669 Inf Inf
[10,] -122.1509  47.55962 Inf Inf
[11,] -122.1706  47.15546 Inf Inf
[12,] -122.1846  47.23485 Inf Inf
[13,] -122.1846  48.15307 Inf Inf
[14,] -122.1851  47.44870 Inf Inf
[15,] -122.1954  47.68485 Inf Inf
[16,] -122.1990  47.51610 Inf Inf
[17,] -122.2014  47.44772 Inf Inf
[18,] -122.2025  47.69815 Inf Inf
[19,] -122.2037  47.67190 Inf Inf
[20,] -122.2090  47.40378 Inf Inf
[21,] -122.2108  47.25336 Inf Inf
[22,] -122.2291  47.63880 Inf Inf
[23,] -122.2419  47.76870 Inf Inf
[24,] -122.2722  48.04803 Inf Inf
[25,] -122.2732  47.87700 Inf Inf
[26,] -122.2804  47.77620 Inf Inf
[27,] -122.2839  47.82103 Inf Inf
[28,] -122.2890  47.86899 Inf Inf
[29,] -122.2993  47.67306 Inf Inf
[30,] -122.3180  47.38217 Inf Inf
[31,] -122.3270  47.40378 Inf Inf
[32,] -122.3474  47.43884 Inf Inf
[33,] -122.3484  47.53083 Inf Inf
[34,] -122.3581  47.27678 Inf Inf
[35,] -122.3618  47.76735 Inf Inf
[36,] -122.3700  47.56567 Inf Inf
[37,] -122.3908  47.54938 Inf Inf
[38,] -122.4128  47.64622 Inf Inf
[39,] -122.4293  47.17660 Inf Inf
[40,] -122.4621  47.44505 Inf Inf
[41,] -122.4904  47.27460 Inf Inf
[42,] -122.5515  46.93979 Inf Inf
[43,] -122.7348  42.37320 Inf Inf
[44,] -122.7827  47.31059 Inf Inf
[45,] -122.7987  47.23475 Inf Inf
[46,] -122.8385  42.35119 Inf Inf
[47,] -122.8537  42.34495 Inf Inf
[48,] -122.8904  42.39555 Inf Inf
[49,] -122.8927  42.33022 Inf Inf
[50,] -122.9451  47.37574 Inf Inf
[51,] -122.9594  42.30376 Inf Inf
[52,] -123.0641  47.16428 Inf Inf
[53,] -123.3413  42.44117 Inf Inf
Error in spTransform(xSP, CRSobj, ...) : 
  failure in points 1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:23:24:25:26:27:28:29:30:31:32:33:34:35:36:37:38:39:40:41:42:43:44:45:46:47:48:49:50:51:52:53
In addition: Warning message:
In spTransform(xSP, CRSobj, ...) : 53 projected point(s) not finite

Here is the data:
dput(sample)
structure(list(ID = 1:53, Longitude = c(-119.733899, -119.766493, -119.779416, -121.0234, -122.004736, -122.013456, -122.0379, -122.0578, -122.132971, -122.150901, -122.170608, -122.18462, -122.184639, -122.185079, -122.195398, -122.198994, -122.201356, -122.202507, -122.20371, -122.209047, -122.210797, -122.229095, -122.2419, -122.27216, -122.273164, -122.280355, -122.28389, -122.289043, -122.299261, -122.318009, -122.326987, -122.347382, -122.34844, -122.358115, -122.361839, -122.37003, -122.390815, -122.41282, -122.429323, -122.462136, -122.490417, -122.551483, -122.734847, -122.782736, -122.798669, -122.838498, -122.853683, -122.8904, -122.89271, -122.94511, -122.959407, -123.064087, -123.341346), Latitude = c(39.53939, 39.396298, 39.287681, 39.205028, 47.192616, 47.188833, 47.5219, 47.609748, 47.13669, 47.559616, 47.155455, 47.234849, 48.15307, 47.448697, 47.684854, 47.516104, 47.447723, 47.698146, 47.6719, 47.403778, 47.253364, 47.638795, 47.768701, 48.048027, 47.876997, 47.776205, 47.821029, 47.868987, 47.673056, 47.382165, 47.403785, 47.438836, 47.530831, 47.276776, 47.76735, 47.565667, 47.549377, 47.646222, 47.176596, 47.445053, 47.274599, 46.939789, 42.373195, 47.310595, 47.234748, 42.351189, 42.344953, 42.395547, 42.33022, 47.375736, 42.303755, 47.164278, 42.441172)), class = "data.frame", row.names = c(NA, -53L))

Here is the routine:
require(sp)
require(rgdal)
#25*1609.34 = 40233.5      
dis <- 40233.5 #Distance threshold 25miles converted to meters

x <- tmp1b[,c(6)]
head(x)
y <- tmp1b[,c(5)]
head(y)
str(y)

xy <- SpatialPointsDataFrame(matrix(c(x,y), ncol=2), data.frame(ID=seq(1:length(x))),
                             proj4string=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84"))

str(xy)

xy <- spTransform(xy, CRS("+init=epsg:27700 +datum=WGS84"))#Error occurring here?

chc <- hclust(dist(data.frame(rownames=rownames(xy at data), x=coordinates(xy)[,1], #What is xy at data???? see str(xy)
                              y=coordinates(xy)[,2])), method="complete")
str(chc)

view(xy at data)#Look like 3 clusters

# Distance with a 25 Mile threshold
chc.dis <- cutree(chc, h=dis)
str(chc.dis)
view(chc.dis)

# Join results to meuse sp points
xy at data <- data.frame(xy at data, Clust=chc.dis)
str(xy at data)
view(xy at data)

# Plot results
plot(xy, col=factor(xy at data$Clust), pch=19)
box(col="black")
title(main="Clustering")
legend("topleft", legend=paste("Cluster", 1:3,sep=""),#Change from 4 in demo to 3 in my data
       col=palette()[1:3], pch=rep(19,3), bg="white")




Proprietary

NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}


From L@urentRHe|p @end|ng |rom |ree@|r  Sun May 17 15:52:30 2020
From: L@urentRHe|p @end|ng |rom |ree@|r (Laurent Rhelp)
Date: Sun, 17 May 2020 15:52:30 +0200
Subject: [R] iterators : checkFunc with ireadLines
Message-ID: <72174dd3-a819-b5fd-5a6f-2a6b46342774-2015@free.fr>

Dear R-Help List,

 ?? I would like to use an iterator to read a file filtering some 
selected lines according to the line name in order to use after a 
foreach loop. I wanted to use the checkFunc argument as the following 
example found on internet to select only prime numbers :

|??????????????????????????????? iprime <- ||iter||(1:100, checkFunc = 
||function||(n) ||isprime||(n))|

|(https://datawookie.netlify.app/blog/2013/11/iterators-in-r/) 
<https://datawookie.netlify.app/blog/2013/11/iterators-in-r/>|

but the checkFunc argument seems not to be available with the function 
ireadLines (package iterators). So, I did the code below to solve my 
problem but I am sure that I miss something to use iterators with files. 
Since I found nothing on the web about ireadLines and the checkFunc 
argument, could somebody help me to understand how we have to use 
iterator (and foreach loop) on files keeping only selected lines ?

Thank you very much
Laurent

Presently here is my code:

##??????? mock file to read: test.txt
##
# Time??? 0??? 0.000999??? 0.001999??? 0.002998??? 0.003998 0.004997??? 
0.005997??? 0.006996??? 0.007996
# N023??? -0.031323??? -0.035026??? -0.029759??? -0.024886 -0.024464??? 
-0.026816??? -0.03369??? -0.041067??? -0.038747
# N053??? -0.014083??? -0.004741??? 0.001443??? -0.010152 -0.012996??? 
-0.005337??? -0.008738??? -0.015094??? -0.012104
# N123??? -0.019008??? -0.013494??? -0.01318??? -0.029208 -0.032748??? 
-0.020243??? -0.015089??? -0.014439??? -0.011681
# N163??? -0.054023??? -0.049345??? -0.037158??? -0.04112 -0.044612??? 
-0.036953??? -0.036061??? -0.044516??? -0.046436
# N193??? -0.022171??? -0.022384??? -0.022338??? -0.023304 -0.022569??? 
-0.021827??? -0.021996??? -0.021755??? -0.021846


# sensors to keep

sensors <-? c("N053", "N163")


library(iterators)

library(rlist)


file_name <- "test.txt"

con_obj <- file( file_name , "r")
ifile <- ireadLines( con_obj , n = 1 )


## I do not do a loop for the example

res <- list()

r <- get_Lines_iter( ifile , sensors)
res <- list.append( res , r )
res
r <- get_Lines_iter( ifile , sensors)
res <- list.append( res , r )
res
r <- get_Lines_iter( ifile , sensors)
do.call("cbind",res)

## the function get_Lines_iter to select and process the line

get_Lines_iter? <-? function( iter , sensors, sep = '\t', quiet = FALSE){
 ? ## read the next record in the iterator
 ? r = try( nextElem(iter) )
 ?while(? TRUE ){
 ? ? if( class(r) == "try-error") {
 ?? ? ? ?? return( stop("The iterator is empty") )
 ?? } else {
 ?? ## split the read line according to the separator
 ??? r_txt <- textConnection(r)
 ??? fields <- scan(file = r_txt, what = "character", sep = sep, quiet = 
quiet)
 ???? ## test if we have to keep the line
 ???? if( fields[1] %in% sensors){
 ?????? ## data processing for the selected line (for the example 
transformation in dataframe)
 ???? ? n <- length(fields)
 ?????? x <- data.frame( as.numeric(fields[2:n]) )
 ?????? names(x) <- fields[1]
 ???? ? ## We return the values
 ???? ? print(paste0("sensor ",fields[1]," ok"))
 ??? ?? return( x )
 ?? ? }else{
 ????? print(paste0("Sensor ", fields[1] ," not selected"))
 ????? r = try(nextElem(iter) )}
 ?? }
}# end while loop
}







-- 
L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e par le logiciel antivirus Avast.
https://www.avast.com/antivirus

	[[alternative HTML version deleted]]


From @purd|e@@ @end|ng |rom gm@||@com  Mon May 18 03:39:59 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Mon, 18 May 2020 13:39:59 +1200
Subject: [R] addScales package announcement
In-Reply-To: <CAGxFJbRSG9OdoQi1xshj-sd3UD=GM5Hk5soAhnE1vFLZ7jafvA@mail.gmail.com>
References: <CAGxFJbRSG9OdoQi1xshj-sd3UD=GM5Hk5soAhnE1vFLZ7jafvA@mail.gmail.com>
Message-ID: <CAB8pepyKDH3UMZatuBMF3CMuc79-YWchUVrScQgZYvW-ZxrkgQ@mail.gmail.com>

That's a good idea.

I used something similar in one of packages:
https://cran.r-project.org/web/packages/probhat/vignettes/probhatx_ckernels.pdf
(Currently, being updated).

Maybe you should write a vignette (even a small one), or create a
simple webpage.
I doubt that your *description* alone is sufficient to convey possible
usefulness.


On Sat, May 16, 2020 at 11:16 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
> In case anyone here is interested (this appeared on the r-packages
> list, but it seems that posts there are no longer forwarded here):
>
> addScales is a small package now on CRAN that modifies trellis objects
> created
> using lattice graphics by adding horizontal and/or vertical reference
> lines to provide visual scaling information. This is mostly useful
> for multi-panel plots that use the relation = 'free' option in their
> 'scales' argument list. Examples show when and how this might aid data
> visualization.
>
> Please feel free to contact me with any suggestions for improvement.
>
> Bert Gunter
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t  Mon May 18 12:14:06 2020
From: @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t (Stefano Sofia)
Date: Mon, 18 May 2020 10:14:06 +0000
Subject: [R] Classification of wind events
In-Reply-To: <98642722-2500-4C90-A297-8F8FFC097BAE@dcn.davis.ca.us>
References: <8B435C9568170B469AE31E8891E8CC4F809B8DEA@ESINO.regionemarche.intra>,
 <CA+8X3fUsXA7reESuWpGC2jBGRRryF6RcYa5jZxMWV7oktF1kFg@mail.gmail.com>
 <8B435C9568170B469AE31E8891E8CC4F809B9B6F@ESINO.regionemarche.intra>,
 <98642722-2500-4C90-A297-8F8FFC097BAE@dcn.davis.ca.us>
Message-ID: <8B435C9568170B469AE31E8891E8CC4F809B9E53@ESINO.regionemarche.intra>

Sorry for my fault.
I am very grateful for such code, which is extremely efficient. I would have never been able to reach these results.

In order to preserve the quality of this code, I dare to ask you a final question: once identified each single period in the column foehn1c, this period can be taken into consideration only if within it the mean of max_speed is higher than 8.0 (which is speed in m/s).
Could you please help me in this final step?

Thank you again for all your help
Stefano


         (oo)
--oOO--( )--OOo----------------
Stefano Sofia PhD
Civil Protection - Marche Region
Meteo Section
Snow Section
Via del Colle Ameno 5
60126 Torrette di Ancona, Ancona
Uff: 071 806 7743
E-mail: stefano.sofia at regione.marche.it
---Oo---------oO----------------

________________________________________
Da: Jeff Newmiller [jdnewmil at dcn.davis.ca.us]
Inviato: sabato 16 maggio 2020 21.04
A: Stefano Sofia; Jim Lemon; r-help mailing list
Oggetto: RE: [R] Classification of wind events

Please run your code before posting it... you forgot the quotes in your main_dir column.

first_day_POSIX <- as.POSIXct("2020-02-19-00-00", format="%Y-%m-%d-%H-%M")
last_day_POSIX <- as.POSIXct("2020-02-20-00-00", format="%Y-%m-%d-%H-%M")
mydf <- data.frame(data_POSIX=seq(first_day_POSIX, last_day_POSIX, by="10 min"))

mydf$main_dir <- c("WSW", "WSW", "SW", "SW", "W", "WSW", "WSW", "WSW", "W", "W", "SW", "WSW", "SSW", "S", "SW", "SW", "WSW", "WNW", "W", "WSW", "WSW", "SE", "SE", "SE", "NW", "NNE", "ENE", "SE", "NNW", "NW", "NW", "NW", "NW", "NW", "NW", "NE", "NW", "NW", "NW", "NW", "NW", "N", "WNW", "NW", "NNW", "NNW", "NW", "NW", "NW", "WNW", "ESE", "W", "WSW", "SW", "SW", "SW", "WSW", "SW", "S", "S", "SSW", "SW", "WSW", "WSW", "WSW", "WSW", "WSW", "WSW", "WSW", "SW", "WSW", "WSW", "WSW", "WSW", "SW", "SW", "WSW", "WSW", "WSW", "WSW", "WSW", "SW", "SW", "SW", "SW", "SW", "SW", "SW", "SW", "SW", "WSW", "WSW", "WSW", "WSW", "SW", "SW", "SW", "SW", "WSW", "SW", "SW", "SW", "SW", "SW", "WSW", "SW", "SW", "W", "WSW", "WSW", "SSW", "S", "WNW", "SW", "W", "WSW", "WSW", "SE", "SE", "SE", "NW", "NNE", "ENE", "SE", "NNW", "NW", "NW", "NW", "NW", "NW", "NW", "NE", "NW", "NW", "NW", "NW", "NW", "N", "WNW", "NW", "NNW", "NNW", "NW", "NW", "NW")

mydf$max_speed <- c(4.60, 4.60, 3.40, 3.10, 4.80, 4.20, 4.10, 4.50, 4.70, 4.30, 2.40, 2.30, 2.20, 2.10, 2.90, 2.80, 1.80, 2.70, 4.30, 3.30, 2.30, 2.30, 3.20, 3.20, 2.90, 2.30, 1.50, 1.80, 2.90, 2.40, 1.80, 2.40, 2.30, 2.60, 1.80, 2.30, 1.90, 2.20, 2.80, 2.40, 1.00, 1.10, 1.60, 2.30, 2.50, 3.30, 3.40, 3.20, 4.50, 3.90, 3.10, 2.40, 6.00, 7.80, 6.30, 7.80, 8.10, 6.10, 7.40, 9.50, 8.90, 9.10, 10.10, 10.50, 11.10, 10.10, 10.90, 11.30, 13.40, 13.50, 12.80, 11.50, 13.10, 13.50, 11.10, 10.50, 8.50, 10.10, 10.70, 13.60, 11.90, 14.90, 10.90, 10.90, 12.80, 12.10, 9.10, 8.30, 8.80, 7.40, 8.40, 10.30, 10.00, 7.00, 8.50, 8.40, 8.60, 6.70, 7.30, 6.20, 5.90, 5.90, 5.10, 5.80, 5.60, 6.50, 6.60, 11.70, 11.30, 8.70, 7.10, 6.90, 4.30, 3.80, 4.30, 3.30, 2.30, 2.30, 3.20, 3.20, 2.90, 2.30, 1.50, 1.80, 2.90, 2.40, 1.80, 2.40, 2.30, 2.60, 1.80, 2.30, 1.90, 2.20, 2.80, 2.40, 1.00, 1.10, 1.60, 2.30, 2.50, 3.30, 3.40, 3.20, 4.50)
# mark candidate rows
mydf$foehn1a <- mydf$main_dir %in% c( "WSW", "SW" )
# mark unstable conditions
mydf$foehn1b <- with( mydf
                    , cumsum( !foehn1a )
                    )
# find minimum length of foehn conditions
mydf$foehn1c <- ave( rep( 1, nrow( mydf ) )
                   , mydf$foehn1b
                   , FUN=function(v) 10 < length( v )
                   )
# find starts of foehns
mydf$foehn1d <- with( mydf
                    , 0 < diff( c( 0, foehn1c ) )
                    )
# identify foehns distinctly (multiple days)
mydf$foehn1e <- with( mydf
                    , ifelse( foehn1c
                            , cumsum( foehn1d )
                            , 0
                            )
                    )
mydf[ , c( 1, 2, 8 ) ]

On May 16, 2020 3:21:24 AM PDT, Stefano Sofia <stefano.sofia at regione.marche.it> wrote:
>Dear Jim and Jeff,
>thank you for your comments. You are right, it is quite difficult to
>detect this process through a single observation point, I am awre of
>it.
>I need to set up an automatic algorithm to filter 20 years of data, and
>I have to find an easy way to do it.
>I know quite well my automatic stations, the wind direction is very
>stable during these situations, and therefore I would like to start
>from it. (I should use also wind speed, relative humidity and
>temperature, but I will introduce them only once I will be able to
>manage the direction).
>In the case of the example below reported, I know that the directions
>of this particular automatic station must be only SW or WSW.
>
>My biggest problem, obviously, is to find the beginning and the end of
>each event, when there is a change in the main direction.
>Thinking about categorical data in general, is there a way to detect
>periods when one particular category is more frequent?
>
>Here I reproduce a real example 24 hours long, where these Foehn
>condition start between 09 and 10 and finish after 19:
>
>first_day_POSIX <- as.POSIXct("2020-02-19-00-00",
>format="%Y-%m-%d-%H-%M")
>last_day_POSIX <- as.POSIXct("2020-02-20-00-00",
>format="%Y-%m-%d-%H-%M")
>mydf <- data.frame(data_POSIX=seq(first_day_POSIX, last_day_POSIX,
>by="10 min"))
>
>mydf$main_dir <- c(WSW, WSW, SW, SW, W, WSW, WSW, WSW, W, W, SW, WSW,
>SSW, S, SW, SW, WSW, WNW, W, WSW, WSW, SE, SE, SE, NW, NNE, ENE, SE,
>NNW, NW, NW, NW, NW, NW, NW, NE, NW, NW, NW, NW, NW, N, WNW, NW, NNW,
>NNW, NW, NW, NW, WNW, ESE, W, WSW, SW, SW, SW, WSW, SW, S, S, SSW, SW,
>WSW, WSW, WSW, WSW, WSW, WSW, WSW, SW, WSW, WSW, WSW, WSW, SW, SW, WSW,
>WSW, WSW, WSW, WSW, SW, SW, SW, SW, SW, SW, SW, SW, SW, WSW, WSW, WSW,
>WSW, SW, SW, SW, SW, WSW, SW, SW, SW, SW, SW, WSW, SW, SW, W, WSW, WSW,
>SSW, S, WNW, SW, W, WSW, WSW, SE, SE, SE, NW, NNE, ENE, SE, NNW, NW,
>NW, NW, NW, NW, NW, NE, NW, NW, NW, NW, NW, N, WNW, NW, NNW, NNW, NW,
>NW, NW)
>
>mydf$max_speed <- c(4.60, 4.60, 3.40, 3.10, 4.80, 4.20, 4.10, 4.50,
>4.70, 4.30, 2.40, 2.30, 2.20, 2.10, 2.90, 2.80, 1.80, 2.70, 4.30, 3.30,
>2.30, 2.30, 3.20, 3.20, 2.90, 2.30, 1.50, 1.80, 2.90, 2.40, 1.80, 2.40,
>2.30, 2.60, 1.80, 2.30, 1.90, 2.20, 2.80, 2.40, 1.00, 1.10, 1.60, 2.30,
>2.50, 3.30, 3.40, 3.20, 4.50, 3.90, 3.10, 2.40, 6.00, 7.80, 6.30, 7.80,
>8.10, 6.10, 7.40, 9.50, 8.90, 9.10, 10.10, 10.50, 11.10, 10.10, 10.90,
>11.30, 13.40, 13.50, 12.80, 11.50, 13.10, 13.50, 11.10, 10.50, 8.50,
>10.10, 10.70, 13.60, 11.90, 14.90, 10.90, 10.90, 12.80, 12.10, 9.10,
>8.30, 8.80, 7.40, 8.40, 10.30, 10.00, 7.00, 8.50, 8.40, 8.60, 6.70,
>7.30, 6.20, 5.90, 5.90, 5.10, 5.80, 5.60, 6.50, 6.60, 11.70, 11.30,
>8.70, 7.10, 6.90, 4.30, 3.80, 4.30, 3.30, 2.30, 2.30, 3.20, 3.20, 2.90,
>2.30, 1.50, 1.80, 2.90, 2.40, 1.80, 2.40, 2.30, 2.60, 1.80, 2.30, 1.90,
>2.20, 2.80, 2.40, 1.00, 1.10, 1.60, 2.30, 2.50, 3.30, 3.40, 3.20, 4.50)
>
>
>Thank you for your attention
>Stefano
>
>
>         (oo)
>--oOO--( )--OOo----------------
>Stefano Sofia PhD
>Civil Protection - Marche Region
>Meteo Section
>Snow Section
>Via del Colle Ameno 5
>60126 Torrette di Ancona, Ancona
>Uff: 071 806 7743
>E-mail: stefano.sofia at regione.marche.it
>---Oo---------oO----------------
>
>________________________________________
>Da: Jim Lemon [drjimlemon at gmail.com]
>Inviato: mercoled? 13 maggio 2020 11.01
>A: Stefano Sofia; r-help mailing list
>Oggetto: Re: [R] Classification of wind events
>
>Hi Stefano,
>Given only one observation point you will find it difficult. If your
>automatic weather station is in the low area where the foehn wind is
>felt, it can only be distinguished from a dry katabatic wind if the
>upwind conditions are known. There is a similar but milder version of
>this in eastern Australia, but it is usually of the latter sort. There
>may be a way to measure turbulence above the peak of the high ground
>with radar or something, but I'm not familiar with that.
>
>Jim
>
>On Tue, May 12, 2020 at 6:13 PM Stefano Sofia
><stefano.sofia at regione.marche.it> wrote:
>>
>> Dear R list users,
>> I am aware that this question is not strictly related, at the present
>moment, to R code and it is more general. Please forgive me, but I need
>to share my thoughts with you.
>>
>> Foehn conditions on the southern slope of Alps happen with strong
>northerly flows that impact perpendicularly over the Apls. This
>situation triggers strong northerly leeward winds.
>> Given a single automatic weather station, I would like to identify
>these periods starting from wind direction and wind intensity data.
>Frequency of data is quarter of hour.
>> I would really find difficult to detect the moving windows of these
>events:
>> - I can't analyse data day by day;
>> - at the beginning and at the end of each event, when the process is
>not at full speed yet, the rotation is not always perfectly
>identifiable;
>> - I cannot claim in principle that the direction of each consecutive
>observation is costantly and strictly from the chosen direction.
>>
>> Does anybody have a clue on how to start to build this process in the
>right way?
>>
>> Thank you for your attention and your help
>> Stefano
>>
>>          (oo)
>> --oOO--( )--OOo----------------
>> Stefano Sofia PhD
>> Civil Protection - Marche Region
>> Meteo Section
>> Snow Section
>> Via del Colle Ameno 5
>> 60126 Torrette di Ancona, Ancona
>> Uff: 071 806 7743
>> E-mail: stefano.sofia at regione.marche.it
>> ---Oo---------oO----------------
>>
>> ________________________________
>>
>> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu?
>contenere informazioni confidenziali, pertanto ? destinato solo a
>persone autorizzate alla ricezione. I messaggi di posta elettronica per
>i client di Regione Marche possono contenere informazioni confidenziali
>e con privilegi legali. Se non si ? il destinatario specificato, non
>leggere, copiare, inoltrare o archiviare questo messaggio. Se si ?
>ricevuto questo messaggio per errore, inoltrarlo al mittente ed
>eliminarlo completamente dal sistema del proprio computer. Ai sensi
>dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit?
>ed urgenza, la risposta al presente messaggio di posta elettronica pu?
>essere visionata da persone estranee al destinatario.
>> IMPORTANT NOTICE: This e-mail message is intended to be received only
>by persons entitled to receive the confidential information it may
>contain. E-mail messages to clients of Regione Marche may contain
>information that is confidential and legally privileged. Please do not
>read, copy, forward, or store this message unless you are an intended
>recipient of it. If you have received this message in error, please
>forward it to the sender and delete it completely from your computer
>system.
>>
>> --
>> Questo messaggio  stato analizzato da Libra ESVA ed  risultato non
>infetto.
>> This message was scanned by Libra ESVA and is believed to be clean.
>>
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>--
>
>Questo messaggio  stato analizzato con Libra ESVA ed  risultato non
>infetto.
>
>
>________________________________
>
>AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
>informazioni confidenziali, pertanto ? destinato solo a persone
>autorizzate alla ricezione. I messaggi di posta elettronica per i
>client di Regione Marche possono contenere informazioni confidenziali e
>con privilegi legali. Se non si ? il destinatario specificato, non
>leggere, copiare, inoltrare o archiviare questo messaggio. Se si ?
>ricevuto questo messaggio per errore, inoltrarlo al mittente ed
>eliminarlo completamente dal sistema del proprio computer. Ai sensi
>dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit?
>ed urgenza, la risposta al presente messaggio di posta elettronica pu?
>essere visionata da persone estranee al destinatario.
>IMPORTANT NOTICE: This e-mail message is intended to be received only
>by persons entitled to receive the confidential information it may
>contain. E-mail messages to clients of Regione Marche may contain
>information that is confidential and legally privileged. Please do not
>read, copy, forward, or store this message unless you are an intended
>recipient of it. If you have received this message in error, please
>forward it to the sender and delete it completely from your computer
>system.
>
>-->
>Questo messaggio  stato analizzato da Libra ESVA ed  risultato non
>infetto.>
>This message was scanned by Libra ESVA and is believed to be clean.

--
Sent from my phone. Please excuse my brevity.

--

Questo messaggio  stato analizzato con Libra ESVA ed  risultato non infetto.


________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.

--
Questo messaggio  stato analizzato da Libra ESVA ed  risultato non infetto.
This message was scanned by Libra ESVA and is believed to be clean.


From drj|m|emon @end|ng |rom gm@||@com  Mon May 18 13:11:52 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Mon, 18 May 2020 21:11:52 +1000
Subject: [R] Classification of wind events
In-Reply-To: <8B435C9568170B469AE31E8891E8CC4F809B9E53@ESINO.regionemarche.intra>
References: <8B435C9568170B469AE31E8891E8CC4F809B8DEA@ESINO.regionemarche.intra>
 <CA+8X3fUsXA7reESuWpGC2jBGRRryF6RcYa5jZxMWV7oktF1kFg@mail.gmail.com>
 <8B435C9568170B469AE31E8891E8CC4F809B9B6F@ESINO.regionemarche.intra>
 <98642722-2500-4C90-A297-8F8FFC097BAE@dcn.davis.ca.us>
 <8B435C9568170B469AE31E8891E8CC4F809B9E53@ESINO.regionemarche.intra>
Message-ID: <CA+8X3fWfZ1E5vEaJwuEMxkThRhy_iCzRqOL9Yo6SHidf8Cqq0A@mail.gmail.com>

Hi Stefano,
If I understand your request, this may also help, Uses the same data
transformations as my previous email.

png("SS_foehn.png")
plot(mydf$data_POSIX,
 ifelse(mydf$main_dir %in% c("WSW","SW"),mydf$max_speed,NA),
 type="b",main="Wind speed (WSW or SW) by time",
 xlab="Time of day",ylab="Wind speed km/h",
 col=rainbow(16)[as.numeric(mydf$main_dir)])
abline(h=8,col="orange",lwd=2)
source("../rollmean.R")
rmws<-rollmean(mydf$max_speed,4)
lines(mydf$data_POSIX,rmws,col="orange",lwd=2)
legend("topleft","Rolling mean of 4 for wind speed",
 lty=1,lwd=2,col="orange")
dev.off()

Jim

-------------- next part --------------
A non-text attachment was scrubbed...
Name: SS_foehn.png
Type: image/png
Size: 34365 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200518/091f0aea/attachment.png>

From rub@k @end|ng |rom m@th@@@u@dk  Mon May 18 13:41:29 2020
From: rub@k @end|ng |rom m@th@@@u@dk (Ege Rubak)
Date: Mon, 18 May 2020 11:41:29 +0000
Subject: [R] Help with spTransform() function and final plot colors
In-Reply-To: <BYAPR06MB5383C3848EE99DB9776092D7AEBB0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB5383C3848EE99DB9776092D7AEBB0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <f1739bbb754a71d02495bf9189010f86d633331e.camel@math.aau.dk>

You are more likely to get help with specific problems related to
spTransform() on the dedicated list r-sig-geo.

You should provide a minimal reproducible example. Your code refers to
e.g. the object `tmp1b`, which we don't have. I think the spTransform()
part will work with this correction:

xy <- SpatialPointsDataFrame(sample[,2:3], sample[,1,drop=FALSE],
                             proj4string=CRS("+proj=longlat
+ellps=WGS84 +datum=WGS84"))

xy <- spTransform(xy, CRS("+init=epsg:27700 +datum=WGS84"))

The spatial workflow in R has largely moved to the `sf` package and you
are probably better off using that in the future if you are new in the
game anyway.

Hope this helps,
Ege

On Sun, 2020-05-17 at 09:50 +0000, Poling, William via R-help wrote:
> #RStudio Version Version 1.2.1335 
> sessionInfo() 
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit)
> #Running under: Windows 10 x64 (build 17763)
> 
> Hello. I am running my data through a routine I found that finds
> clusters of data points based on distance rule.
> 
https://gis.stackexchange.com/questions/64392/finding-clusters-of-points-based-distance-rule-using-r
> 
> 1. I get this error when I get to this point in the routine, see
> complete routine below?
> xy <- spTransform(xy, CRS("+init=epsg:27700 +datum=WGS84"))
> non finite transformation detected:
> 
> I tried converting columns from numeric to Integer but did not help.
> 
> However, I continue to run the rest of the routine and the final
> sequence, the plot itself, seems to work
> Can this error be corrected somehow, despite the fact that it
> continues to work, just curious what it is I guess"
> 
> 2. However in the plot at the end of the routine the color key
> appears with the colors but the clusters in the plot are black, see
> plot call at the end of the routine below?
> 
> Thank you for any advice.
> 
> WHP
> 
> ERROR:
> non finite transformation detected:
>       coords.x1 coords.x2        
>  [1,] -119.7339  39.53939 Inf Inf
>  [2,] -119.7665  39.39630 Inf Inf
>  [3,] -119.7794  39.28768 Inf Inf
>  [4,] -121.0234  39.20503 Inf Inf
>  [5,] -122.0047  47.19262 Inf Inf
>  [6,] -122.0135  47.18883 Inf Inf
>  [7,] -122.0379  47.52190 Inf Inf
>  [8,] -122.0578  47.60975 Inf Inf
>  [9,] -122.1330  47.13669 Inf Inf
> [10,] -122.1509  47.55962 Inf Inf
> [11,] -122.1706  47.15546 Inf Inf
> [12,] -122.1846  47.23485 Inf Inf
> [13,] -122.1846  48.15307 Inf Inf
> [14,] -122.1851  47.44870 Inf Inf
> [15,] -122.1954  47.68485 Inf Inf
> [16,] -122.1990  47.51610 Inf Inf
> [17,] -122.2014  47.44772 Inf Inf
> [18,] -122.2025  47.69815 Inf Inf
> [19,] -122.2037  47.67190 Inf Inf
> [20,] -122.2090  47.40378 Inf Inf
> [21,] -122.2108  47.25336 Inf Inf
> [22,] -122.2291  47.63880 Inf Inf
> [23,] -122.2419  47.76870 Inf Inf
> [24,] -122.2722  48.04803 Inf Inf
> [25,] -122.2732  47.87700 Inf Inf
> [26,] -122.2804  47.77620 Inf Inf
> [27,] -122.2839  47.82103 Inf Inf
> [28,] -122.2890  47.86899 Inf Inf
> [29,] -122.2993  47.67306 Inf Inf
> [30,] -122.3180  47.38217 Inf Inf
> [31,] -122.3270  47.40378 Inf Inf
> [32,] -122.3474  47.43884 Inf Inf
> [33,] -122.3484  47.53083 Inf Inf
> [34,] -122.3581  47.27678 Inf Inf
> [35,] -122.3618  47.76735 Inf Inf
> [36,] -122.3700  47.56567 Inf Inf
> [37,] -122.3908  47.54938 Inf Inf
> [38,] -122.4128  47.64622 Inf Inf
> [39,] -122.4293  47.17660 Inf Inf
> [40,] -122.4621  47.44505 Inf Inf
> [41,] -122.4904  47.27460 Inf Inf
> [42,] -122.5515  46.93979 Inf Inf
> [43,] -122.7348  42.37320 Inf Inf
> [44,] -122.7827  47.31059 Inf Inf
> [45,] -122.7987  47.23475 Inf Inf
> [46,] -122.8385  42.35119 Inf Inf
> [47,] -122.8537  42.34495 Inf Inf
> [48,] -122.8904  42.39555 Inf Inf
> [49,] -122.8927  42.33022 Inf Inf
> [50,] -122.9451  47.37574 Inf Inf
> [51,] -122.9594  42.30376 Inf Inf
> [52,] -123.0641  47.16428 Inf Inf
> [53,] -123.3413  42.44117 Inf Inf
> Error in spTransform(xSP, CRSobj, ...) : 
>   failure in points
> 1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:23:24:25:26:
> 27:28:29:30:31:32:33:34:35:36:37:38:39:40:41:42:43:44:45:46:47:48:49:
> 50:51:52:53
> In addition: Warning message:
> In spTransform(xSP, CRSobj, ...) : 53 projected point(s) not finite
> 
> Here is the data:
> dput(sample)
> structure(list(ID = 1:53, Longitude = c(-119.733899, -119.766493, 
> -119.779416, -121.0234, -122.004736, -122.013456, -122.0379, 
> -122.0578, -122.132971, -122.150901, -122.170608, -122.18462, 
> -122.184639, -122.185079, -122.195398, -122.198994, -122.201356, 
> -122.202507, -122.20371, -122.209047, -122.210797, -122.229095, 
> -122.2419, -122.27216, -122.273164, -122.280355, -122.28389, 
> -122.289043, -122.299261, -122.318009, -122.326987, -122.347382, 
> -122.34844, -122.358115, -122.361839, -122.37003, -122.390815, 
> -122.41282, -122.429323, -122.462136, -122.490417, -122.551483, 
> -122.734847, -122.782736, -122.798669, -122.838498, -122.853683, 
> -122.8904, -122.89271, -122.94511, -122.959407, -123.064087, 
> -123.341346), Latitude = c(39.53939, 39.396298, 39.287681,
> 39.205028, 
> 47.192616, 47.188833, 47.5219, 47.609748, 47.13669, 47.559616, 
> 47.155455, 47.234849, 48.15307, 47.448697, 47.684854, 47.516104, 
> 47.447723, 47.698146, 47.6719, 47.403778, 47.253364, 47.638795, 
> 47.768701, 48.048027, 47.876997, 47.776205, 47.821029, 47.868987, 
> 47.673056, 47.382165, 47.403785, 47.438836, 47.530831, 47.276776, 
> 47.76735, 47.565667, 47.549377, 47.646222, 47.176596, 47.445053, 
> 47.274599, 46.939789, 42.373195, 47.310595, 47.234748, 42.351189, 
> 42.344953, 42.395547, 42.33022, 47.375736, 42.303755, 47.164278, 
> 42.441172)), class = "data.frame", row.names = c(NA, -53L))
> 
> Here is the routine:
> require(sp)
> require(rgdal)
> #25*1609.34 = 40233.5      
> dis <- 40233.5 #Distance threshold 25miles converted to meters
> 
> x <- tmp1b[,c(6)]
> head(x)
> y <- tmp1b[,c(5)]
> head(y)
> str(y)
> 
> xy <- SpatialPointsDataFrame(matrix(c(x,y), ncol=2),
> data.frame(ID=seq(1:length(x))),
>                              proj4string=CRS("+proj=longlat
> +ellps=WGS84 +datum=WGS84"))
> 
> str(xy)
> 
> xy <- spTransform(xy, CRS("+init=epsg:27700 +datum=WGS84"))#Error
> occurring here?
> 
> chc <- hclust(dist(data.frame(rownames=rownames(xy at data),
> x=coordinates(xy)[,1], #What is xy at data???? see str(xy)
>                               y=coordinates(xy)[,2])),
> method="complete")
> str(chc)
> 
> view(xy at data)#Look like 3 clusters
> 
> # Distance with a 25 Mile threshold  
> chc.dis <- cutree(chc, h=dis) 
> str(chc.dis)
> view(chc.dis)
> 
> # Join results to meuse sp points
> xy at data <- data.frame(xy at data, Clust=chc.dis)
> str(xy at data)
> view(xy at data)
> 
> # Plot results
> plot(xy, col=factor(xy at data$Clust), pch=19)
> box(col="black")
> title(main="Clustering")
> legend("topleft", legend=paste("Cluster", 1:3,sep=""),#Change from 4
> in demo to 3 in my data
>        col=palette()[1:3], pch=rep(19,3), bg="white")
> 
> 
> 
> 
> Proprietary
> 
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may
> con...{{dropped:16}}
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
Ege Rubak, Associate Professor,
Department of Mathematical Sciences, Aalborg University
Skjernvej 4A, 9220 Aalborg East, Denmark
Phone: (+45)99408861
Mobile: (+45)30230252
Email: rubak at math.aau.dk

From Po||ngW @end|ng |rom @etn@@com  Mon May 18 13:51:01 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Mon, 18 May 2020 11:51:01 +0000
Subject: [R] Help with spTransform() function and final plot colors
In-Reply-To: <f1739bbb754a71d02495bf9189010f86d633331e.camel@math.aau.dk>
References: <BYAPR06MB5383C3848EE99DB9776092D7AEBB0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <f1739bbb754a71d02495bf9189010f86d633331e.camel@math.aau.dk>
Message-ID: <BYAPR06MB5383199CF889DDBAE257B49FAEB80@BYAPR06MB5383.namprd06.prod.outlook.com>

Thank you Ege, I appreciate your response.

I have move this to r-sig-geo.

WHP

Proprietary

-----Original Message-----
From: Ege Rubak <rubak at math.aau.dk> 
Sent: Monday, May 18, 2020 6:41 AM
To: Poling, William <PolingW at aetna.com>; r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with spTransform() function and final plot colors

**** External Email - Use Caution ****

You are more likely to get help with specific problems related to
spTransform() on the dedicated list r-sig-geo.

You should provide a minimal reproducible example. Your code refers to e.g. the object `tmp1b`, which we don't have. I think the spTransform() part will work with this correction:

xy <- SpatialPointsDataFrame(sample[,2:3], sample[,1,drop=FALSE],
                             proj4string=CRS("+proj=longlat
+ellps=WGS84 +datum=WGS84"))

xy <- spTransform(xy, CRS("+init=epsg:27700 +datum=WGS84"))

The spatial workflow in R has largely moved to the `sf` package and you are probably better off using that in the future if you are new in the game anyway.

Hope this helps,
Ege

On Sun, 2020-05-17 at 09:50 +0000, Poling, William via R-help wrote:
> #RStudio Version Version 1.2.1335
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit) #Running under: Windows 10 
> x64 (build 17763)
> 
> Hello. I am running my data through a routine I found that finds 
> clusters of data points based on distance rule.
> 
https://urldefense.proofpoint.com/v2/url?u=https-3A__gis.stackexchange.com_questions_64392_finding-2Dclusters-2Dof-2Dpoints-2Dbased-2Ddistance-2Drule-2Dusing-2Dr&d=DwIGaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sFygsSovnYNWjYOpjSzuSLI1MUrJN8Lih39pDQN98xI&s=2FCT-JBdWSB66pNvGLTM7Ec5Q7IbXatXBLsQqYT0gng&e= 
> 
> 1. I get this error when I get to this point in the routine, see 
> complete routine below?
> xy <- spTransform(xy, CRS("+init=epsg:27700 +datum=WGS84")) non finite 
> transformation detected:
> 
> I tried converting columns from numeric to Integer but did not help.
> 
> However, I continue to run the rest of the routine and the final 
> sequence, the plot itself, seems to work Can this error be corrected 
> somehow, despite the fact that it continues to work, just curious what 
> it is I guess"
> 
> 2. However in the plot at the end of the routine the color key appears 
> with the colors but the clusters in the plot are black, see plot call 
> at the end of the routine below?
> 
> Thank you for any advice.
> 
> WHP
> 
> ERROR:
> non finite transformation detected:
>       coords.x1 coords.x2        
>  [1,] -119.7339  39.53939 Inf Inf
>  [2,] -119.7665  39.39630 Inf Inf
>  [3,] -119.7794  39.28768 Inf Inf
>  [4,] -121.0234  39.20503 Inf Inf
>  [5,] -122.0047  47.19262 Inf Inf
>  [6,] -122.0135  47.18883 Inf Inf
>  [7,] -122.0379  47.52190 Inf Inf
>  [8,] -122.0578  47.60975 Inf Inf
>  [9,] -122.1330  47.13669 Inf Inf
> [10,] -122.1509  47.55962 Inf Inf
> [11,] -122.1706  47.15546 Inf Inf
> [12,] -122.1846  47.23485 Inf Inf
> [13,] -122.1846  48.15307 Inf Inf
> [14,] -122.1851  47.44870 Inf Inf
> [15,] -122.1954  47.68485 Inf Inf
> [16,] -122.1990  47.51610 Inf Inf
> [17,] -122.2014  47.44772 Inf Inf
> [18,] -122.2025  47.69815 Inf Inf
> [19,] -122.2037  47.67190 Inf Inf
> [20,] -122.2090  47.40378 Inf Inf
> [21,] -122.2108  47.25336 Inf Inf
> [22,] -122.2291  47.63880 Inf Inf
> [23,] -122.2419  47.76870 Inf Inf
> [24,] -122.2722  48.04803 Inf Inf
> [25,] -122.2732  47.87700 Inf Inf
> [26,] -122.2804  47.77620 Inf Inf
> [27,] -122.2839  47.82103 Inf Inf
> [28,] -122.2890  47.86899 Inf Inf
> [29,] -122.2993  47.67306 Inf Inf
> [30,] -122.3180  47.38217 Inf Inf
> [31,] -122.3270  47.40378 Inf Inf
> [32,] -122.3474  47.43884 Inf Inf
> [33,] -122.3484  47.53083 Inf Inf
> [34,] -122.3581  47.27678 Inf Inf
> [35,] -122.3618  47.76735 Inf Inf
> [36,] -122.3700  47.56567 Inf Inf
> [37,] -122.3908  47.54938 Inf Inf
> [38,] -122.4128  47.64622 Inf Inf
> [39,] -122.4293  47.17660 Inf Inf
> [40,] -122.4621  47.44505 Inf Inf
> [41,] -122.4904  47.27460 Inf Inf
> [42,] -122.5515  46.93979 Inf Inf
> [43,] -122.7348  42.37320 Inf Inf
> [44,] -122.7827  47.31059 Inf Inf
> [45,] -122.7987  47.23475 Inf Inf
> [46,] -122.8385  42.35119 Inf Inf
> [47,] -122.8537  42.34495 Inf Inf
> [48,] -122.8904  42.39555 Inf Inf
> [49,] -122.8927  42.33022 Inf Inf
> [50,] -122.9451  47.37574 Inf Inf
> [51,] -122.9594  42.30376 Inf Inf
> [52,] -123.0641  47.16428 Inf Inf
> [53,] -123.3413  42.44117 Inf Inf
> Error in spTransform(xSP, CRSobj, ...) : 
>   failure in points
> 1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:23:24:25:26:
> 27:28:29:30:31:32:33:34:35:36:37:38:39:40:41:42:43:44:45:46:47:48:49:
> 50:51:52:53
> In addition: Warning message:
> In spTransform(xSP, CRSobj, ...) : 53 projected point(s) not finite
> 
> Here is the data:
> dput(sample)
> structure(list(ID = 1:53, Longitude = c(-119.733899, -119.766493, 
> -119.779416, -121.0234, -122.004736, -122.013456, -122.0379, 
> -122.0578, -122.132971, -122.150901, -122.170608, -122.18462, 
> -122.184639, -122.185079, -122.195398, -122.198994, -122.201356, 
> -122.202507, -122.20371, -122.209047, -122.210797, -122.229095, 
> -122.2419, -122.27216, -122.273164, -122.280355, -122.28389, 
> -122.289043, -122.299261, -122.318009, -122.326987, -122.347382, 
> -122.34844, -122.358115, -122.361839, -122.37003, -122.390815, 
> -122.41282, -122.429323, -122.462136, -122.490417, -122.551483, 
> -122.734847, -122.782736, -122.798669, -122.838498, -122.853683, 
> -122.8904, -122.89271, -122.94511, -122.959407, -123.064087, 
> -123.341346), Latitude = c(39.53939, 39.396298, 39.287681, 39.205028, 
> 47.192616, 47.188833, 47.5219, 47.609748, 47.13669, 47.559616, 
> 47.155455, 47.234849, 48.15307, 47.448697, 47.684854, 47.516104, 
> 47.447723, 47.698146, 47.6719, 47.403778, 47.253364, 47.638795, 
> 47.768701, 48.048027, 47.876997, 47.776205, 47.821029, 47.868987, 
> 47.673056, 47.382165, 47.403785, 47.438836, 47.530831, 47.276776, 
> 47.76735, 47.565667, 47.549377, 47.646222, 47.176596, 47.445053, 
> 47.274599, 46.939789, 42.373195, 47.310595, 47.234748, 42.351189, 
> 42.344953, 42.395547, 42.33022, 47.375736, 42.303755, 47.164278, 
> 42.441172)), class = "data.frame", row.names = c(NA, -53L))
> 
> Here is the routine:
> require(sp)
> require(rgdal)
> #25*1609.34 = 40233.5      
> dis <- 40233.5 #Distance threshold 25miles converted to meters
> 
> x <- tmp1b[,c(6)]
> head(x)
> y <- tmp1b[,c(5)]
> head(y)
> str(y)
> 
> xy <- SpatialPointsDataFrame(matrix(c(x,y), ncol=2), 
> data.frame(ID=seq(1:length(x))),
>                              proj4string=CRS("+proj=longlat
> +ellps=WGS84 +datum=WGS84"))
> 
> str(xy)
> 
> xy <- spTransform(xy, CRS("+init=epsg:27700 +datum=WGS84"))#Error 
> occurring here?
> 
> chc <- hclust(dist(data.frame(rownames=rownames(xy at data),
> x=coordinates(xy)[,1], #What is xy at data???? see str(xy)
>                               y=coordinates(xy)[,2])),
> method="complete")
> str(chc)
> 
> view(xy at data)#Look like 3 clusters
> 
> # Distance with a 25 Mile threshold
> chc.dis <- cutree(chc, h=dis)
> str(chc.dis)
> view(chc.dis)
> 
> # Join results to meuse sp points
> xy at data <- data.frame(xy at data, Clust=chc.dis)
> str(xy at data)
> view(xy at data)
> 
> # Plot results
> plot(xy, col=factor(xy at data$Clust), pch=19)
> box(col="black")
> title(main="Clustering")
> legend("topleft", legend=paste("Cluster", 1:3,sep=""),#Change from 4 
> in demo to 3 in my data
>        col=palette()[1:3], pch=rep(19,3), bg="white")
> 
> 
> 
> 
> Proprietary
> 
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may 
> con...{{dropped:16}}
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIGaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmv
> OXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sFygsSovnYNWjYO
> pjSzuSLI1MUrJN8Lih39pDQN98xI&s=LPc_mc7BpGwhXBO906nsqoT4CXOHHrXZCm40KIS
> PGwU&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIGaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmm
> vOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sFygsSovnYNWjY
> OpjSzuSLI1MUrJN8Lih39pDQN98xI&s=VDesA7ipmtCRBfuDdto1Ad4emF2L4VoK54tX9Y
> qsqDQ&e= and provide commented, minimal, self-contained, reproducible 
> code.
--
Ege Rubak, Associate Professor,
Department of Mathematical Sciences, Aalborg University Skjernvej 4A, 9220 Aalborg East, Denmark
Phone: (+45)99408861
Mobile: (+45)30230252
Email: rubak at math.aau.dk

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon May 18 17:21:42 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 18 May 2020 08:21:42 -0700
Subject: [R] Classification of wind events
In-Reply-To: <8B435C9568170B469AE31E8891E8CC4F809B9E53@ESINO.regionemarche.intra>
References: <8B435C9568170B469AE31E8891E8CC4F809B8DEA@ESINO.regionemarche.intra>,
 <CA+8X3fUsXA7reESuWpGC2jBGRRryF6RcYa5jZxMWV7oktF1kFg@mail.gmail.com>
 <8B435C9568170B469AE31E8891E8CC4F809B9B6F@ESINO.regionemarche.intra>,
 <98642722-2500-4C90-A297-8F8FFC097BAE@dcn.davis.ca.us>
 <8B435C9568170B469AE31E8891E8CC4F809B9E53@ESINO.regionemarche.intra>
Message-ID: <6D49E0CD-698C-4AE8-9DCA-CE493C2CACBB@dcn.davis.ca.us>

While I can understand that such techniques might not seem obvious at first, they are building blocks that you should be able to use to solve similar problems in the future. Don't give up because it surprised you this time, and do play with modifying it to better understand this time.

Replace code starting with calculation of foehn1d:

# calculate mean values by candidate group
mydf$foehn1c2 <- ave( mydf$max_speed
                   , mydf$foehn1b
                   , FUN=mean
                   )
# find starts of foehns
mydf$foehn1d <- with( mydf
                    , 0 < diff( c( 0, foehn1c & 8<foehn1c2 ) )
                    )
# identify foehns distinctly (multiple days)
mydf$foehn1e <- with( mydf
                    , ifelse( foehn1c
                            , cumsum( foehn1d )
                            , 0
                            )
                    )
mydf[ , c( "data_POSIX" , "main_dir", "max_speed" , "foehn1e" ) ]


On May 18, 2020 3:14:06 AM PDT, Stefano Sofia <stefano.sofia at regione.marche.it> wrote:
>Sorry for my fault.
>I am very grateful for such code, which is extremely efficient. I would
>have never been able to reach these results.
>
>In order to preserve the quality of this code, I dare to ask you a
>final question: once identified each single period in the column
>foehn1c, this period can be taken into consideration only if within it
>the mean of max_speed is higher than 8.0 (which is speed in m/s).
>Could you please help me in this final step?
>
>Thank you again for all your help
>Stefano
>
>
>         (oo)
>--oOO--( )--OOo----------------
>Stefano Sofia PhD
>Civil Protection - Marche Region
>Meteo Section
>Snow Section
>Via del Colle Ameno 5
>60126 Torrette di Ancona, Ancona
>Uff: 071 806 7743
>E-mail: stefano.sofia at regione.marche.it
>---Oo---------oO----------------
>
>________________________________________
>Da: Jeff Newmiller [jdnewmil at dcn.davis.ca.us]
>Inviato: sabato 16 maggio 2020 21.04
>A: Stefano Sofia; Jim Lemon; r-help mailing list
>Oggetto: RE: [R] Classification of wind events
>
>Please run your code before posting it... you forgot the quotes in your
>main_dir column.
>
>first_day_POSIX <- as.POSIXct("2020-02-19-00-00",
>format="%Y-%m-%d-%H-%M")
>last_day_POSIX <- as.POSIXct("2020-02-20-00-00",
>format="%Y-%m-%d-%H-%M")
>mydf <- data.frame(data_POSIX=seq(first_day_POSIX, last_day_POSIX,
>by="10 min"))
>
>mydf$main_dir <- c("WSW", "WSW", "SW", "SW", "W", "WSW", "WSW", "WSW",
>"W", "W", "SW", "WSW", "SSW", "S", "SW", "SW", "WSW", "WNW", "W",
>"WSW", "WSW", "SE", "SE", "SE", "NW", "NNE", "ENE", "SE", "NNW", "NW",
>"NW", "NW", "NW", "NW", "NW", "NE", "NW", "NW", "NW", "NW", "NW", "N",
>"WNW", "NW", "NNW", "NNW", "NW", "NW", "NW", "WNW", "ESE", "W", "WSW",
>"SW", "SW", "SW", "WSW", "SW", "S", "S", "SSW", "SW", "WSW", "WSW",
>"WSW", "WSW", "WSW", "WSW", "WSW", "SW", "WSW", "WSW", "WSW", "WSW",
>"SW", "SW", "WSW", "WSW", "WSW", "WSW", "WSW", "SW", "SW", "SW", "SW",
>"SW", "SW", "SW", "SW", "SW", "WSW", "WSW", "WSW", "WSW", "SW", "SW",
>"SW", "SW", "WSW", "SW", "SW", "SW", "SW", "SW", "WSW", "SW", "SW",
>"W", "WSW", "WSW", "SSW", "S", "WNW", "SW", "W", "WSW", "WSW", "SE",
>"SE", "SE", "NW", "NNE", "ENE", "SE", "NNW", "NW", "NW", "NW", "NW",
>"NW", "NW", "NE", "NW", "NW", "NW", "NW", "NW", "N", "WNW", "NW",
>"NNW", "NNW", "NW", "NW", "NW")
>
>mydf$max_speed <- c(4.60, 4.60, 3.40, 3.10, 4.80, 4.20, 4.10, 4.50,
>4.70, 4.30, 2.40, 2.30, 2.20, 2.10, 2.90, 2.80, 1.80, 2.70, 4.30, 3.30,
>2.30, 2.30, 3.20, 3.20, 2.90, 2.30, 1.50, 1.80, 2.90, 2.40, 1.80, 2.40,
>2.30, 2.60, 1.80, 2.30, 1.90, 2.20, 2.80, 2.40, 1.00, 1.10, 1.60, 2.30,
>2.50, 3.30, 3.40, 3.20, 4.50, 3.90, 3.10, 2.40, 6.00, 7.80, 6.30, 7.80,
>8.10, 6.10, 7.40, 9.50, 8.90, 9.10, 10.10, 10.50, 11.10, 10.10, 10.90,
>11.30, 13.40, 13.50, 12.80, 11.50, 13.10, 13.50, 11.10, 10.50, 8.50,
>10.10, 10.70, 13.60, 11.90, 14.90, 10.90, 10.90, 12.80, 12.10, 9.10,
>8.30, 8.80, 7.40, 8.40, 10.30, 10.00, 7.00, 8.50, 8.40, 8.60, 6.70,
>7.30, 6.20, 5.90, 5.90, 5.10, 5.80, 5.60, 6.50, 6.60, 11.70, 11.30,
>8.70, 7.10, 6.90, 4.30, 3.80, 4.30, 3.30, 2.30, 2.30, 3.20, 3.20, 2.90,
>2.30, 1.50, 1.80, 2.90, 2.40, 1.80, 2.40, 2.30, 2.60, 1.80, 2.30, 1.90,
>2.20, 2.80, 2.40, 1.00, 1.10, 1.60, 2.30, 2.50, 3.30, 3.40, 3.20, 4.50)
># mark candidate rows
>mydf$foehn1a <- mydf$main_dir %in% c( "WSW", "SW" )
># mark unstable conditions
>mydf$foehn1b <- with( mydf
>                    , cumsum( !foehn1a )
>                    )
># find minimum length of foehn conditions
>mydf$foehn1c <- ave( rep( 1, nrow( mydf ) )
>                   , mydf$foehn1b
>                   , FUN=function(v) 10 < length( v )
>                   )
># find starts of foehns
>mydf$foehn1d <- with( mydf
>                    , 0 < diff( c( 0, foehn1c ) )
>                    )
># identify foehns distinctly (multiple days)
>mydf$foehn1e <- with( mydf
>                    , ifelse( foehn1c
>                            , cumsum( foehn1d )
>                            , 0
>                            )
>                    )
>mydf[ , c( 1, 2, 8 ) ]
>
>On May 16, 2020 3:21:24 AM PDT, Stefano Sofia
><stefano.sofia at regione.marche.it> wrote:
>>Dear Jim and Jeff,
>>thank you for your comments. You are right, it is quite difficult to
>>detect this process through a single observation point, I am awre of
>>it.
>>I need to set up an automatic algorithm to filter 20 years of data,
>and
>>I have to find an easy way to do it.
>>I know quite well my automatic stations, the wind direction is very
>>stable during these situations, and therefore I would like to start
>>from it. (I should use also wind speed, relative humidity and
>>temperature, but I will introduce them only once I will be able to
>>manage the direction).
>>In the case of the example below reported, I know that the directions
>>of this particular automatic station must be only SW or WSW.
>>
>>My biggest problem, obviously, is to find the beginning and the end of
>>each event, when there is a change in the main direction.
>>Thinking about categorical data in general, is there a way to detect
>>periods when one particular category is more frequent?
>>
>>Here I reproduce a real example 24 hours long, where these Foehn
>>condition start between 09 and 10 and finish after 19:
>>
>>first_day_POSIX <- as.POSIXct("2020-02-19-00-00",
>>format="%Y-%m-%d-%H-%M")
>>last_day_POSIX <- as.POSIXct("2020-02-20-00-00",
>>format="%Y-%m-%d-%H-%M")
>>mydf <- data.frame(data_POSIX=seq(first_day_POSIX, last_day_POSIX,
>>by="10 min"))
>>
>>mydf$main_dir <- c(WSW, WSW, SW, SW, W, WSW, WSW, WSW, W, W, SW, WSW,
>>SSW, S, SW, SW, WSW, WNW, W, WSW, WSW, SE, SE, SE, NW, NNE, ENE, SE,
>>NNW, NW, NW, NW, NW, NW, NW, NE, NW, NW, NW, NW, NW, N, WNW, NW, NNW,
>>NNW, NW, NW, NW, WNW, ESE, W, WSW, SW, SW, SW, WSW, SW, S, S, SSW, SW,
>>WSW, WSW, WSW, WSW, WSW, WSW, WSW, SW, WSW, WSW, WSW, WSW, SW, SW,
>WSW,
>>WSW, WSW, WSW, WSW, SW, SW, SW, SW, SW, SW, SW, SW, SW, WSW, WSW, WSW,
>>WSW, SW, SW, SW, SW, WSW, SW, SW, SW, SW, SW, WSW, SW, SW, W, WSW,
>WSW,
>>SSW, S, WNW, SW, W, WSW, WSW, SE, SE, SE, NW, NNE, ENE, SE, NNW, NW,
>>NW, NW, NW, NW, NW, NE, NW, NW, NW, NW, NW, N, WNW, NW, NNW, NNW, NW,
>>NW, NW)
>>
>>mydf$max_speed <- c(4.60, 4.60, 3.40, 3.10, 4.80, 4.20, 4.10, 4.50,
>>4.70, 4.30, 2.40, 2.30, 2.20, 2.10, 2.90, 2.80, 1.80, 2.70, 4.30,
>3.30,
>>2.30, 2.30, 3.20, 3.20, 2.90, 2.30, 1.50, 1.80, 2.90, 2.40, 1.80,
>2.40,
>>2.30, 2.60, 1.80, 2.30, 1.90, 2.20, 2.80, 2.40, 1.00, 1.10, 1.60,
>2.30,
>>2.50, 3.30, 3.40, 3.20, 4.50, 3.90, 3.10, 2.40, 6.00, 7.80, 6.30,
>7.80,
>>8.10, 6.10, 7.40, 9.50, 8.90, 9.10, 10.10, 10.50, 11.10, 10.10, 10.90,
>>11.30, 13.40, 13.50, 12.80, 11.50, 13.10, 13.50, 11.10, 10.50, 8.50,
>>10.10, 10.70, 13.60, 11.90, 14.90, 10.90, 10.90, 12.80, 12.10, 9.10,
>>8.30, 8.80, 7.40, 8.40, 10.30, 10.00, 7.00, 8.50, 8.40, 8.60, 6.70,
>>7.30, 6.20, 5.90, 5.90, 5.10, 5.80, 5.60, 6.50, 6.60, 11.70, 11.30,
>>8.70, 7.10, 6.90, 4.30, 3.80, 4.30, 3.30, 2.30, 2.30, 3.20, 3.20,
>2.90,
>>2.30, 1.50, 1.80, 2.90, 2.40, 1.80, 2.40, 2.30, 2.60, 1.80, 2.30,
>1.90,
>>2.20, 2.80, 2.40, 1.00, 1.10, 1.60, 2.30, 2.50, 3.30, 3.40, 3.20,
>4.50)
>>
>>
>>Thank you for your attention
>>Stefano
>>
>>
>>         (oo)
>>--oOO--( )--OOo----------------
>>Stefano Sofia PhD
>>Civil Protection - Marche Region
>>Meteo Section
>>Snow Section
>>Via del Colle Ameno 5
>>60126 Torrette di Ancona, Ancona
>>Uff: 071 806 7743
>>E-mail: stefano.sofia at regione.marche.it
>>---Oo---------oO----------------
>>
>>________________________________________
>>Da: Jim Lemon [drjimlemon at gmail.com]
>>Inviato: mercoled? 13 maggio 2020 11.01
>>A: Stefano Sofia; r-help mailing list
>>Oggetto: Re: [R] Classification of wind events
>>
>>Hi Stefano,
>>Given only one observation point you will find it difficult. If your
>>automatic weather station is in the low area where the foehn wind is
>>felt, it can only be distinguished from a dry katabatic wind if the
>>upwind conditions are known. There is a similar but milder version of
>>this in eastern Australia, but it is usually of the latter sort. There
>>may be a way to measure turbulence above the peak of the high ground
>>with radar or something, but I'm not familiar with that.
>>
>>Jim
>>
>>On Tue, May 12, 2020 at 6:13 PM Stefano Sofia
>><stefano.sofia at regione.marche.it> wrote:
>>>
>>> Dear R list users,
>>> I am aware that this question is not strictly related, at the
>present
>>moment, to R code and it is more general. Please forgive me, but I
>need
>>to share my thoughts with you.
>>>
>>> Foehn conditions on the southern slope of Alps happen with strong
>>northerly flows that impact perpendicularly over the Apls. This
>>situation triggers strong northerly leeward winds.
>>> Given a single automatic weather station, I would like to identify
>>these periods starting from wind direction and wind intensity data.
>>Frequency of data is quarter of hour.
>>> I would really find difficult to detect the moving windows of these
>>events:
>>> - I can't analyse data day by day;
>>> - at the beginning and at the end of each event, when the process is
>>not at full speed yet, the rotation is not always perfectly
>>identifiable;
>>> - I cannot claim in principle that the direction of each consecutive
>>observation is costantly and strictly from the chosen direction.
>>>
>>> Does anybody have a clue on how to start to build this process in
>the
>>right way?
>>>
>>> Thank you for your attention and your help
>>> Stefano
>>>
>>>          (oo)
>>> --oOO--( )--OOo----------------
>>> Stefano Sofia PhD
>>> Civil Protection - Marche Region
>>> Meteo Section
>>> Snow Section
>>> Via del Colle Ameno 5
>>> 60126 Torrette di Ancona, Ancona
>>> Uff: 071 806 7743
>>> E-mail: stefano.sofia at regione.marche.it
>>> ---Oo---------oO----------------
>>>
>>> ________________________________
>>>
>>> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu?
>>contenere informazioni confidenziali, pertanto ? destinato solo a
>>persone autorizzate alla ricezione. I messaggi di posta elettronica
>per
>>i client di Regione Marche possono contenere informazioni
>confidenziali
>>e con privilegi legali. Se non si ? il destinatario specificato, non
>>leggere, copiare, inoltrare o archiviare questo messaggio. Se si ?
>>ricevuto questo messaggio per errore, inoltrarlo al mittente ed
>>eliminarlo completamente dal sistema del proprio computer. Ai sensi
>>dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di
>necessit?
>>ed urgenza, la risposta al presente messaggio di posta elettronica pu?
>>essere visionata da persone estranee al destinatario.
>>> IMPORTANT NOTICE: This e-mail message is intended to be received
>only
>>by persons entitled to receive the confidential information it may
>>contain. E-mail messages to clients of Regione Marche may contain
>>information that is confidential and legally privileged. Please do not
>>read, copy, forward, or store this message unless you are an intended
>>recipient of it. If you have received this message in error, please
>>forward it to the sender and delete it completely from your computer
>>system.
>>>
>>> --
>>> Questo messaggio  stato analizzato da Libra ESVA ed  risultato non
>>infetto.
>>> This message was scanned by Libra ESVA and is believed to be clean.
>>>
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>--
>>
>>Questo messaggio  stato analizzato con Libra ESVA ed  risultato non
>>infetto.
>>
>>
>>________________________________
>>
>>AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
>>informazioni confidenziali, pertanto ? destinato solo a persone
>>autorizzate alla ricezione. I messaggi di posta elettronica per i
>>client di Regione Marche possono contenere informazioni confidenziali
>e
>>con privilegi legali. Se non si ? il destinatario specificato, non
>>leggere, copiare, inoltrare o archiviare questo messaggio. Se si ?
>>ricevuto questo messaggio per errore, inoltrarlo al mittente ed
>>eliminarlo completamente dal sistema del proprio computer. Ai sensi
>>dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di
>necessit?
>>ed urgenza, la risposta al presente messaggio di posta elettronica pu?
>>essere visionata da persone estranee al destinatario.
>>IMPORTANT NOTICE: This e-mail message is intended to be received only
>>by persons entitled to receive the confidential information it may
>>contain. E-mail messages to clients of Regione Marche may contain
>>information that is confidential and legally privileged. Please do not
>>read, copy, forward, or store this message unless you are an intended
>>recipient of it. If you have received this message in error, please
>>forward it to the sender and delete it completely from your computer
>>system.
>>
>>-->
>>Questo messaggio  stato analizzato da Libra ESVA ed  risultato non
>>infetto.>
>>This message was scanned by Libra ESVA and is believed to be clean.
>
>--
>Sent from my phone. Please excuse my brevity.
>
>--
>
>Questo messaggio  stato analizzato con Libra ESVA ed  risultato non
>infetto.
>
>
>________________________________
>
>AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
>informazioni confidenziali, pertanto ? destinato solo a persone
>autorizzate alla ricezione. I messaggi di posta elettronica per i
>client di Regione Marche possono contenere informazioni confidenziali e
>con privilegi legali. Se non si ? il destinatario specificato, non
>leggere, copiare, inoltrare o archiviare questo messaggio. Se si ?
>ricevuto questo messaggio per errore, inoltrarlo al mittente ed
>eliminarlo completamente dal sistema del proprio computer. Ai sensi
>dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit?
>ed urgenza, la risposta al presente messaggio di posta elettronica pu?
>essere visionata da persone estranee al destinatario.
>IMPORTANT NOTICE: This e-mail message is intended to be received only
>by persons entitled to receive the confidential information it may
>contain. E-mail messages to clients of Regione Marche may contain
>information that is confidential and legally privileged. Please do not
>read, copy, forward, or store this message unless you are an intended
>recipient of it. If you have received this message in error, please
>forward it to the sender and delete it completely from your computer
>system.
>
>-->
>Questo messaggio  stato analizzato da Libra ESVA ed  risultato non
>infetto.>
>This message was scanned by Libra ESVA and is believed to be clean.

-- 
Sent from my phone. Please excuse my brevity.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon May 18 17:26:29 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 18 May 2020 08:26:29 -0700
Subject: [R] Classification of wind events
In-Reply-To: <CA+8X3fWfZ1E5vEaJwuEMxkThRhy_iCzRqOL9Yo6SHidf8Cqq0A@mail.gmail.com>
References: <8B435C9568170B469AE31E8891E8CC4F809B8DEA@ESINO.regionemarche.intra>
 <CA+8X3fUsXA7reESuWpGC2jBGRRryF6RcYa5jZxMWV7oktF1kFg@mail.gmail.com>
 <8B435C9568170B469AE31E8891E8CC4F809B9B6F@ESINO.regionemarche.intra>
 <98642722-2500-4C90-A297-8F8FFC097BAE@dcn.davis.ca.us>
 <8B435C9568170B469AE31E8891E8CC4F809B9E53@ESINO.regionemarche.intra>
 <CA+8X3fWfZ1E5vEaJwuEMxkThRhy_iCzRqOL9Yo6SHidf8Cqq0A@mail.gmail.com>
Message-ID: <9B65D74C-215C-4591-8F98-D1DDBA1C8213@dcn.davis.ca.us>

? source("../rollmean.R") ?

On May 18, 2020 4:11:52 AM PDT, Jim Lemon <drjimlemon at gmail.com> wrote:
>Hi Stefano,
>If I understand your request, this may also help, Uses the same data
>transformations as my previous email.
>
>png("SS_foehn.png")
>plot(mydf$data_POSIX,
> ifelse(mydf$main_dir %in% c("WSW","SW"),mydf$max_speed,NA),
> type="b",main="Wind speed (WSW or SW) by time",
> xlab="Time of day",ylab="Wind speed km/h",
> col=rainbow(16)[as.numeric(mydf$main_dir)])
>abline(h=8,col="orange",lwd=2)
>source("../rollmean.R")
>rmws<-rollmean(mydf$max_speed,4)
>lines(mydf$data_POSIX,rmws,col="orange",lwd=2)
>legend("topleft","Rolling mean of 4 for wind speed",
> lty=1,lwd=2,col="orange")
>dev.off()
>
>Jim

-- 
Sent from my phone. Please excuse my brevity.


From kenn@rdj @end|ng |rom m@ch@et@com  Mon May 18 15:30:00 2020
From: kenn@rdj @end|ng |rom m@ch@et@com (Kennard McDaniel)
Date: Mon, 18 May 2020 08:30:00 -0500
Subject: [R] rgl 0.100.54 using rgl.setMouseCallbacks in pan3d with
 rglwidget() and shiny server
Message-ID: <7ccce836-2bd4-3401-d77c-fa56b6f40c95@machaet.com>


I am currently attempting to implement rgl and the pan3d function on a 
shiny server but can't get the pan3d function to work. Using the right 
mouse button (2).? Here is a little sample code I was using to try to 
get it to work. Session info below the signature line.


 ?options(rgl.useNULL = TRUE)

 ?library(shiny)
 ?library(rgl)

app = shinyApp(
 ??? ui =
 ??? ? rglwidgetOutput("rglPlot")
 ???? ,
 ? server = function(input, output) {
 ???? ??? output$rglPlot <- renderRglwidget({
 ???? ??? ? options(rgl.useNULL = TRUE)
 ???? ??? ? #rgl::open3d()

 ???? ??? ? # pan3d(2)

 ???? ??? ? ## setup pan function from right mouse button
 ???? ??? ? button <- 2
 ???? ??? ? dev = rgl.cur()
 ???? ??? ? subscene = currentSubscene3d(dev)

 ???? ??? ? start <- list()
 ???? ??? ? begin <- function(x, y) {
 ???? ??? ??? activeSubscene <- rgl::par3d("activeSubscene", dev = dev)
 ???? ??? ??? start$listeners <<- rgl::par3d("listeners", dev = dev, 
subscene = activeSubscene)
 ???? ??? ??? for (sub in start$listeners) {
 ???? ??? ????? init <- rgl::par3d(c("userProjection","viewport"), dev = 
dev, subscene = sub)
 ???? ??? ????? init$pos <- c(x/init$viewport[3], 1 - 
y/init$viewport[4], 0.5)
 ???? ??? ????? start[[as.character(sub)]] <<- init
 ???? ??? ??? }
 ???? ??? ? }

 ???? ??? ? update <- function(x, y) {
 ???? ??? ??? for (sub in start$listeners) {
 ???? ??? ????? init <- start[[as.character(sub)]]
 ???? ??? ????? xlat <- 2*(c(x/init$viewport[3], 1 - y/init$viewport[4], 
0.5) - init$pos)
 ???? ??? ????? mouseMatrix <- rgl::translationMatrix(xlat[1], xlat[2], 
xlat[3])
 ???? ??? ????? rgl::par3d(userProjection = mouseMatrix %*% 
init$userProjection, dev = dev, subscene = sub )
 ???? ??? ??? }
 ???? ??? ? }
 ???? ??? ? rgl::rgl.setMouseCallbacks(button, begin, update, dev = dev, 
subscene = subscene)
 ???? ??? ? #cat("Callbacks set on button", button, "of rgl device", 
dev, "in subscene", subscene, "\n")


 ???? ??? ? spheres3d(rnorm(100), rnorm(100), rnorm(100,sd = 0.1), col = 
"red",
 ? ??? ? radius = 0.1)
 ???? ??? ??? axes3d()

 ??? ??? ??? rglwidget()
 ??? ??? })
 ??? })

runApp(app)
-- 

Thank you,

*Kennard J. McDaniel*

sessionInfo() R version 3.5.3 (2019-03-11) Platform: 
x86_64-w64-mingw32/x64 (64-bit) Running under: Windows 10 x64 (build 
18363) Matrix products: default locale: [1] LC_COLLATE=English_United 
States.1252 LC_CTYPE=English_United States.1252 [3] 
LC_MONETARY=English_United States.1252 LC_NUMERIC=C [5] 
LC_TIME=English_United States.1252 attached base packages: [1] stats 
graphics grDevices utils datasets methods base other attached packages: 
[1] NiQMatrixFoamAnalysis_0.2.4 shiny_1.4.0.2 shinyRGL_0.1.0 [4] 
rgl_0.100.54 loaded via a namespace (and not attached): [1] Rcpp_1.0.4.6 
lattice_0.20-38 tidyr_1.0.3 [4] assertthat_0.2.1 digest_0.6.25 
packrat_0.5.0 [7] mime_0.9 R6_2.4.1 plyr_1.8.6 [10] odbc_1.2.2 
evaluate_0.14 ggplot2_3.3.0 [13] pillar_1.4.4 rlang_0.4.6 qcc_2.7 [16] 
misc3d_0.8-4 rstudioapi_0.11 miniUI_0.1.1.1 [19] blob_1.2.1 
rmarkdown_2.1 webshot_0.5.2 [22] stringr_1.4.0 htmlwidgets_1.5.1 
igraph_1.2.5 [25] bit_1.1-15.2 munsell_0.5.0 compiler_3.5.3 [28] 
httpuv_1.5.2 xfun_0.13 pkgconfig_2.0.3 [31] htmltools_0.4.0 
tidyselect_1.0.0 tibble_3.0.1 [34] SixSigma_0.9-52 codetools_0.2-16 
crayon_1.3.4 [37] dplyr_0.8.5 later_1.0.0 MASS_7.3-51.6 [40] 
nat.utils_0.5.1 grid_3.5.3 jsonlite_1.6.1 [43] xtable_1.8-4 gtable_0.3.0 
lifecycle_0.2.0 [46] DBI_1.1.0 magrittr_1.5 scales_1.1.0 [49] nat_1.8.14 
stringi_1.4.6 nabor_0.5.0 [52] reshape2_1.4.4 promises_1.1.0 
testthat_2.3.2 [55] ellipsis_0.3.0 vctrs_0.2.4 plot3D_1.3 [58] 
tools_3.5.3 bit64_0.9-7 manipulateWidget_0.10.1 [61] glue_1.4.0 
purrr_0.3.4 hms_0.5.3 [64] crosstalk_1.1.0.1 fastmap_1.0.1 yaml_2.2.1 
[67] colorspace_1.4-1 filehash_2.4-2 knitr_1.28



	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Tue May 19 03:02:35 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Tue, 19 May 2020 11:02:35 +1000
Subject: [R] Classification of wind events
In-Reply-To: <9B65D74C-215C-4591-8F98-D1DDBA1C8213@dcn.davis.ca.us>
References: <8B435C9568170B469AE31E8891E8CC4F809B8DEA@ESINO.regionemarche.intra>
 <CA+8X3fUsXA7reESuWpGC2jBGRRryF6RcYa5jZxMWV7oktF1kFg@mail.gmail.com>
 <8B435C9568170B469AE31E8891E8CC4F809B9B6F@ESINO.regionemarche.intra>
 <98642722-2500-4C90-A297-8F8FFC097BAE@dcn.davis.ca.us>
 <8B435C9568170B469AE31E8891E8CC4F809B9E53@ESINO.regionemarche.intra>
 <CA+8X3fWfZ1E5vEaJwuEMxkThRhy_iCzRqOL9Yo6SHidf8Cqq0A@mail.gmail.com>
 <9B65D74C-215C-4591-8F98-D1DDBA1C8213@dcn.davis.ca.us>
Message-ID: <CA+8X3fX7b3kSi8vLHxpBTnqckBCnSuYLXn=skK+Xed1T5oDdsA@mail.gmail.com>

Sorry, I should know better:

rollmean<-function(x,width=2) {
 lenx<-length(x)
 result<-rep(NA,lenx)
 for(i in 1:lenx) {
  chunk<-i:(i+width-1)
  if(i<width) chunk<-c(rep(1,width-i),1:i)
  if(i>(lenx-width)) chunk<-c(i:lenx,rep(lenx,i-(width-1)))
  result[i]<-mean(x[chunk])
 }
 return(result)
}

I forgot to replace this with:

library(zoo)
rollmean...

Jim

On Tue, May 19, 2020 at 1:26 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>
> ? source("../rollmean.R") ?
>
> On May 18, 2020 4:11:52 AM PDT, Jim Lemon <drjimlemon at gmail.com> wrote:
> >Hi Stefano,
> >If I understand your request, this may also help, Uses the same data
> >transformations as my previous email.
> >
> >png("SS_foehn.png")
> >plot(mydf$data_POSIX,
> > ifelse(mydf$main_dir %in% c("WSW","SW"),mydf$max_speed,NA),
> > type="b",main="Wind speed (WSW or SW) by time",
> > xlab="Time of day",ylab="Wind speed km/h",
> > col=rainbow(16)[as.numeric(mydf$main_dir)])
> >abline(h=8,col="orange",lwd=2)
> >source("../rollmean.R")
> >rmws<-rollmean(mydf$max_speed,4)
> >lines(mydf$data_POSIX,rmws,col="orange",lwd=2)
> >legend("topleft","Rolling mean of 4 for wind speed",
> > lty=1,lwd=2,col="orange")
> >dev.off()
> >
> >Jim
>
> --
> Sent from my phone. Please excuse my brevity.


From L@urentRHe|p @end|ng |rom |ree@|r  Mon May 18 12:35:11 2020
From: L@urentRHe|p @end|ng |rom |ree@|r (Laurent Rhelp)
Date: Mon, 18 May 2020 12:35:11 +0200
Subject: [R] iterators : checkFunc with ireadLines
In-Reply-To: <CAA99HCwtOek_jaNsD6-=CtTRoL9OHYgDqfgdSAfbNBaEZ_w8JQ@mail.gmail.com>
References: <72174dd3-a819-b5fd-5a6f-2a6b46342774-2015@free.fr>
 <CAA99HCwtOek_jaNsD6-=CtTRoL9OHYgDqfgdSAfbNBaEZ_w8JQ@mail.gmail.com>
Message-ID: <a1ad0aec-7457-b516-1a3c-43e9e83d0ae1-9269@free.fr>


Dear William,
 ?Thank you for your answer
My file is very large so I cannot read it in my memory (I cannot use 
read.table). So I want to put in memory only the line I need to process. 
With readLines, as I did, it works but I would like to use an iterator 
and a foreach loop to understand this way to do because I thought that 
it was a better solution to write a nice code.


Le 18/05/2020 ? 04:54, William Michels a ?crit?:
> Apologies, Laurent, for this two-part answer. I misunderstood your
> post where you stated you wanted to "filter(ing) some
> selected lines according to the line name... ." I thought that meant
> you had a separate index (like a series of primes) that you wanted to
> use to only read-in selected line numbers from a file (test file below
> with numbers 1:1000 each on a separate line):
>
>> library(gmp)
>> library(iterators)
>> iprime <- iter(1:100, checkFunc = function(n) isprime(n))
>> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
> Read 1 item
> [1] 2
>> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
> Read 1 item
> [1] 3
>> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
> Read 1 item
> [1] 5
>> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
> Read 1 item
> [1] 7
> However, what it really seems that you want to do is read each line of
> a (possibly enormous) file, test each line "string-wise" to keep or
> discard, and if you're keeping it, append the line to a list. I can
> certainly see the advantage of this strategy for reading in very, very
> large files, but it's not clear to me how the "ireadLines" function (
> in the "iterators" package) will help you, since it doesn't seem to
> generate anything but a sequential index.
>
> Anyway, below is an absolutely standard read-in of your data using
> read.table(). Hopefully some of the code I've posted has been useful
> to you.
>
>> sensors <-  c("N053", "N163")
>> read.table("test2.txt")
>      V1        V2        V3        V4        V5        V6        V7
>     V8        V9       V10
> 1 Time  0.000000  0.000999  0.001999  0.002998  0.003998  0.004997
> 0.005997  0.006996  0.007996
> 2 N023 -0.031323 -0.035026 -0.029759 -0.024886 -0.024464 -0.026816
> -0.033690 -0.041067 -0.038747
> 3 N053 -0.014083 -0.004741  0.001443 -0.010152 -0.012996 -0.005337
> -0.008738 -0.015094 -0.012104
> 4 N123 -0.019008 -0.013494 -0.013180 -0.029208 -0.032748 -0.020243
> -0.015089 -0.014439 -0.011681
> 5 N163 -0.054023 -0.049345 -0.037158 -0.041120 -0.044612 -0.036953
> -0.036061 -0.044516 -0.046436
> 6 N193 -0.022171 -0.022384 -0.022338 -0.023304 -0.022569 -0.021827
> -0.021996 -0.021755 -0.021846
>> Laurent_data <- read.table("test2.txt")
>> Laurent_data[Laurent_data$V1 %in% sensors, ]
>      V1        V2        V3        V4        V5        V6        V7
>     V8        V9       V10
> 3 N053 -0.014083 -0.004741  0.001443 -0.010152 -0.012996 -0.005337
> -0.008738 -0.015094 -0.012104
> 5 N163 -0.054023 -0.049345 -0.037158 -0.041120 -0.044612 -0.036953
> -0.036061 -0.044516 -0.046436
>
> Best, Bill.
>
> W. Michels, Ph.D.
>
>
> On Sun, May 17, 2020 at 5:43 PM Laurent Rhelp <LaurentRHelp at free.fr> wrote:
>> Dear R-Help List,
>>
>>      I would like to use an iterator to read a file filtering some
>> selected lines according to the line name in order to use after a
>> foreach loop. I wanted to use the checkFunc argument as the following
>> example found on internet to select only prime numbers :
>>
>> |                                iprime <- ||iter||(1:100, checkFunc =
>> ||function||(n) ||isprime||(n))|
>>
>> |(https://datawookie.netlify.app/blog/2013/11/iterators-in-r/)
>> <https://datawookie.netlify.app/blog/2013/11/iterators-in-r/>|
>>
>> but the checkFunc argument seems not to be available with the function
>> ireadLines (package iterators). So, I did the code below to solve my
>> problem but I am sure that I miss something to use iterators with files.
>> Since I found nothing on the web about ireadLines and the checkFunc
>> argument, could somebody help me to understand how we have to use
>> iterator (and foreach loop) on files keeping only selected lines ?
>>
>> Thank you very much
>> Laurent
>>
>> Presently here is my code:
>>
>> ##        mock file to read: test.txt
>> ##
>> # Time    0    0.000999    0.001999    0.002998    0.003998 0.004997
>> 0.005997    0.006996    0.007996
>> # N023    -0.031323    -0.035026    -0.029759    -0.024886 -0.024464
>> -0.026816    -0.03369    -0.041067    -0.038747
>> # N053    -0.014083    -0.004741    0.001443    -0.010152 -0.012996
>> -0.005337    -0.008738    -0.015094    -0.012104
>> # N123    -0.019008    -0.013494    -0.01318    -0.029208 -0.032748
>> -0.020243    -0.015089    -0.014439    -0.011681
>> # N163    -0.054023    -0.049345    -0.037158    -0.04112 -0.044612
>> -0.036953    -0.036061    -0.044516    -0.046436
>> # N193    -0.022171    -0.022384    -0.022338    -0.023304 -0.022569
>> -0.021827    -0.021996    -0.021755    -0.021846
>>
>>
>> # sensors to keep
>>
>> sensors <-  c("N053", "N163")
>>
>>
>> library(iterators)
>>
>> library(rlist)
>>
>>
>> file_name <- "test.txt"
>>
>> con_obj <- file( file_name , "r")
>> ifile <- ireadLines( con_obj , n = 1 )
>>
>>
>> ## I do not do a loop for the example
>>
>> res <- list()
>>
>> r <- get_Lines_iter( ifile , sensors)
>> res <- list.append( res , r )
>> res
>> r <- get_Lines_iter( ifile , sensors)
>> res <- list.append( res , r )
>> res
>> r <- get_Lines_iter( ifile , sensors)
>> do.call("cbind",res)
>>
>> ## the function get_Lines_iter to select and process the line
>>
>> get_Lines_iter  <-  function( iter , sensors, sep = '\t', quiet = FALSE){
>>     ## read the next record in the iterator
>>     r = try( nextElem(iter) )
>>    while(  TRUE ){
>>       if( class(r) == "try-error") {
>>             return( stop("The iterator is empty") )
>>      } else {
>>      ## split the read line according to the separator
>>       r_txt <- textConnection(r)
>>       fields <- scan(file = r_txt, what = "character", sep = sep, quiet =
>> quiet)
>>        ## test if we have to keep the line
>>        if( fields[1] %in% sensors){
>>          ## data processing for the selected line (for the example
>> transformation in dataframe)
>>          n <- length(fields)
>>          x <- data.frame( as.numeric(fields[2:n]) )
>>          names(x) <- fields[1]
>>          ## We return the values
>>          print(paste0("sensor ",fields[1]," ok"))
>>          return( x )
>>        }else{
>>         print(paste0("Sensor ", fields[1] ," not selected"))
>>         r = try(nextElem(iter) )}
>>      }
>> }# end while loop
>> }
>>
>>
>>
>>
>>
>>
>>
>> --
>> L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e par le logiciel antivirus Avast.
>> https://www.avast.com/antivirus
>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.



-- 
L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e par le logiciel antivirus Avast.
https://www.avast.com/antivirus


From wjm1 @end|ng |rom c@@@co|umb|@@edu  Mon May 18 20:37:46 2020
From: wjm1 @end|ng |rom c@@@co|umb|@@edu (William Michels)
Date: Mon, 18 May 2020 11:37:46 -0700
Subject: [R] iterators : checkFunc with ireadLines
In-Reply-To: <a1ad0aec-7457-b516-1a3c-43e9e83d0ae1@free.fr>
References: <72174dd3-a819-b5fd-5a6f-2a6b46342774-2015@free.fr>
 <CAA99HCwtOek_jaNsD6-=CtTRoL9OHYgDqfgdSAfbNBaEZ_w8JQ@mail.gmail.com>
 <a1ad0aec-7457-b516-1a3c-43e9e83d0ae1@free.fr>
Message-ID: <CAA99HCx_VvNJzTQ=mQ7zoRFX5ZJOA3mCgbDkMiCSPAH_6Zv=QA-7372@mail.gmail.com>


Hi Laurent,

Thank you for explaining your size limitations. Below is an example
using the read.fwf() function to grab the first column of your input
file (in 2000 row chunks). This column is converted to an index, and
the index is used to create an iterator useful for skipping lines when
reading input with scan(). (You could try processing your large file
in successive 2000 line chunks, or whatever number of lines fits into
memory). Maybe not as elegant as the approach you were going for, but
read.fwf() should be pretty efficient:

> sensors <-  c("N053", "N163")
> read.fwf("test2.txt", widths=c(4), as.is=TRUE, flush=TRUE, n=2000, skip=0)
    V1
1 Time
2 N023
3 N053
4 N123
5 N163
6 N193
> first_col <- read.fwf("test2.txt", widths=c(4), as.is=TRUE, flush=TRUE, n=2000, skip=0)
> which(first_col$V1 %in% sensors)
[1] 3 5
> index1 <- which(first_col$V1 %in% sensors)
> iter_index1 <- iter(1:2000, checkFunc= function(n) {n %in% index1})
> unlist(scan(file="test2.txt", what=list("","","","","","","","","",""), flush=TRUE, multi.line=FALSE, skip=nextElem(iter_index1)-1, nlines=1, quiet=TRUE))
 [1] "N053"      "-0.014083" "-0.004741" "0.001443"  "-0.010152"
"-0.012996" "-0.005337" "-0.008738" "-0.015094" "-0.012104"
> unlist(scan(file="test2.txt", what=list("","","","","","","","","",""), flush=TRUE, multi.line=FALSE, skip=nextElem(iter_index1)-1, nlines=1, quiet=TRUE))
 [1] "N163"      "-0.054023" "-0.049345" "-0.037158" "-0.04112"
"-0.044612" "-0.036953" "-0.036061" "-0.044516" "-0.046436"
>

(Note for this email and the previous one, I've deleted the first
"hash" character from each line of your test file for clarity).

HTH, Bill.

W. Michels, Ph.D.





On Mon, May 18, 2020 at 3:35 AM Laurent Rhelp <LaurentRHelp at free.fr> wrote:
>
> Dear William,
>   Thank you for your answer
> My file is very large so I cannot read it in my memory (I cannot use
> read.table). So I want to put in memory only the line I need to process.
> With readLines, as I did, it works but I would like to use an iterator
> and a foreach loop to understand this way to do because I thought that
> it was a better solution to write a nice code.
>
>
> Le 18/05/2020 ? 04:54, William Michels a ?crit :
> > Apologies, Laurent, for this two-part answer. I misunderstood your
> > post where you stated you wanted to "filter(ing) some
> > selected lines according to the line name... ." I thought that meant
> > you had a separate index (like a series of primes) that you wanted to
> > use to only read-in selected line numbers from a file (test file below
> > with numbers 1:1000 each on a separate line):
> >
> >> library(gmp)
> >> library(iterators)
> >> iprime <- iter(1:100, checkFunc = function(n) isprime(n))
> >> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
> > Read 1 item
> > [1] 2
> >> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
> > Read 1 item
> > [1] 3
> >> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
> > Read 1 item
> > [1] 5
> >> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
> > Read 1 item
> > [1] 7
> > However, what it really seems that you want to do is read each line of
> > a (possibly enormous) file, test each line "string-wise" to keep or
> > discard, and if you're keeping it, append the line to a list. I can
> > certainly see the advantage of this strategy for reading in very, very
> > large files, but it's not clear to me how the "ireadLines" function (
> > in the "iterators" package) will help you, since it doesn't seem to
> > generate anything but a sequential index.
> >
> > Anyway, below is an absolutely standard read-in of your data using
> > read.table(). Hopefully some of the code I've posted has been useful
> > to you.
> >
> >> sensors <-  c("N053", "N163")
> >> read.table("test2.txt")
> >      V1        V2        V3        V4        V5        V6        V7
> >     V8        V9       V10
> > 1 Time  0.000000  0.000999  0.001999  0.002998  0.003998  0.004997
> > 0.005997  0.006996  0.007996
> > 2 N023 -0.031323 -0.035026 -0.029759 -0.024886 -0.024464 -0.026816
> > -0.033690 -0.041067 -0.038747
> > 3 N053 -0.014083 -0.004741  0.001443 -0.010152 -0.012996 -0.005337
> > -0.008738 -0.015094 -0.012104
> > 4 N123 -0.019008 -0.013494 -0.013180 -0.029208 -0.032748 -0.020243
> > -0.015089 -0.014439 -0.011681
> > 5 N163 -0.054023 -0.049345 -0.037158 -0.041120 -0.044612 -0.036953
> > -0.036061 -0.044516 -0.046436
> > 6 N193 -0.022171 -0.022384 -0.022338 -0.023304 -0.022569 -0.021827
> > -0.021996 -0.021755 -0.021846
> >> Laurent_data <- read.table("test2.txt")
> >> Laurent_data[Laurent_data$V1 %in% sensors, ]
> >      V1        V2        V3        V4        V5        V6        V7
> >     V8        V9       V10
> > 3 N053 -0.014083 -0.004741  0.001443 -0.010152 -0.012996 -0.005337
> > -0.008738 -0.015094 -0.012104
> > 5 N163 -0.054023 -0.049345 -0.037158 -0.041120 -0.044612 -0.036953
> > -0.036061 -0.044516 -0.046436
> >
> > Best, Bill.
> >
> > W. Michels, Ph.D.
> >
> >
> > On Sun, May 17, 2020 at 5:43 PM Laurent Rhelp <LaurentRHelp at free.fr> wrote:
> >> Dear R-Help List,
> >>
> >>      I would like to use an iterator to read a file filtering some
> >> selected lines according to the line name in order to use after a
> >> foreach loop. I wanted to use the checkFunc argument as the following
> >> example found on internet to select only prime numbers :
> >>
> >> |                                iprime <- ||iter||(1:100, checkFunc =
> >> ||function||(n) ||isprime||(n))|
> >>
> >> |(https://datawookie.netlify.app/blog/2013/11/iterators-in-r/)
> >> <https://datawookie.netlify.app/blog/2013/11/iterators-in-r/>|
> >>
> >> but the checkFunc argument seems not to be available with the function
> >> ireadLines (package iterators). So, I did the code below to solve my
> >> problem but I am sure that I miss something to use iterators with files.
> >> Since I found nothing on the web about ireadLines and the checkFunc
> >> argument, could somebody help me to understand how we have to use
> >> iterator (and foreach loop) on files keeping only selected lines ?
> >>
> >> Thank you very much
> >> Laurent
> >>
> >> Presently here is my code:
> >>
> >> ##        mock file to read: test.txt
> >> ##
> >> # Time    0    0.000999    0.001999    0.002998    0.003998 0.004997
> >> 0.005997    0.006996    0.007996
> >> # N023    -0.031323    -0.035026    -0.029759    -0.024886 -0.024464
> >> -0.026816    -0.03369    -0.041067    -0.038747
> >> # N053    -0.014083    -0.004741    0.001443    -0.010152 -0.012996
> >> -0.005337    -0.008738    -0.015094    -0.012104
> >> # N123    -0.019008    -0.013494    -0.01318    -0.029208 -0.032748
> >> -0.020243    -0.015089    -0.014439    -0.011681
> >> # N163    -0.054023    -0.049345    -0.037158    -0.04112 -0.044612
> >> -0.036953    -0.036061    -0.044516    -0.046436
> >> # N193    -0.022171    -0.022384    -0.022338    -0.023304 -0.022569
> >> -0.021827    -0.021996    -0.021755    -0.021846
> >>
> >>
> >> # sensors to keep
> >>
> >> sensors <-  c("N053", "N163")
> >>
> >>
> >> library(iterators)
> >>
> >> library(rlist)
> >>
> >>
> >> file_name <- "test.txt"
> >>
> >> con_obj <- file( file_name , "r")
> >> ifile <- ireadLines( con_obj , n = 1 )
> >>
> >>
> >> ## I do not do a loop for the example
> >>
> >> res <- list()
> >>
> >> r <- get_Lines_iter( ifile , sensors)
> >> res <- list.append( res , r )
> >> res
> >> r <- get_Lines_iter( ifile , sensors)
> >> res <- list.append( res , r )
> >> res
> >> r <- get_Lines_iter( ifile , sensors)
> >> do.call("cbind",res)
> >>
> >> ## the function get_Lines_iter to select and process the line
> >>
> >> get_Lines_iter  <-  function( iter , sensors, sep = '\t', quiet = FALSE){
> >>     ## read the next record in the iterator
> >>     r = try( nextElem(iter) )
> >>    while(  TRUE ){
> >>       if( class(r) == "try-error") {
> >>             return( stop("The iterator is empty") )
> >>      } else {
> >>      ## split the read line according to the separator
> >>       r_txt <- textConnection(r)
> >>       fields <- scan(file = r_txt, what = "character", sep = sep, quiet =
> >> quiet)
> >>        ## test if we have to keep the line
> >>        if( fields[1] %in% sensors){
> >>          ## data processing for the selected line (for the example
> >> transformation in dataframe)
> >>          n <- length(fields)
> >>          x <- data.frame( as.numeric(fields[2:n]) )
> >>          names(x) <- fields[1]
> >>          ## We return the values
> >>          print(paste0("sensor ",fields[1]," ok"))
> >>          return( x )
> >>        }else{
> >>         print(paste0("Sensor ", fields[1] ," not selected"))
> >>         r = try(nextElem(iter) )}
> >>      }
> >> }# end while loop
> >> }
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >> --
> >> L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e par le logiciel antivirus Avast.
> >> https://www.avast.com/antivirus
> >>
> >>          [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e par le logiciel antivirus Avast.
> https://www.avast.com/antivirus
>


From wjm1 @end|ng |rom c@@@co|umb|@@edu  Mon May 18 04:54:15 2020
From: wjm1 @end|ng |rom c@@@co|umb|@@edu (William Michels)
Date: Sun, 17 May 2020 19:54:15 -0700
Subject: [R] iterators : checkFunc with ireadLines
In-Reply-To: <72174dd3-a819-b5fd-5a6f-2a6b46342774-2015@free.fr>
References: <72174dd3-a819-b5fd-5a6f-2a6b46342774-2015@free.fr>
Message-ID: <CAA99HCwtOek_jaNsD6-=CtTRoL9OHYgDqfgdSAfbNBaEZ_w8JQ-2708@mail.gmail.com>


Apologies, Laurent, for this two-part answer. I misunderstood your
post where you stated you wanted to "filter(ing) some
selected lines according to the line name... ." I thought that meant
you had a separate index (like a series of primes) that you wanted to
use to only read-in selected line numbers from a file (test file below
with numbers 1:1000 each on a separate line):

> library(gmp)
> library(iterators)
> iprime <- iter(1:100, checkFunc = function(n) isprime(n))
> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
Read 1 item
[1] 2
> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
Read 1 item
[1] 3
> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
Read 1 item
[1] 5
> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
Read 1 item
[1] 7
>

However, what it really seems that you want to do is read each line of
a (possibly enormous) file, test each line "string-wise" to keep or
discard, and if you're keeping it, append the line to a list. I can
certainly see the advantage of this strategy for reading in very, very
large files, but it's not clear to me how the "ireadLines" function (
in the "iterators" package) will help you, since it doesn't seem to
generate anything but a sequential index.

Anyway, below is an absolutely standard read-in of your data using
read.table(). Hopefully some of the code I've posted has been useful
to you.

> sensors <-  c("N053", "N163")
> read.table("test2.txt")
    V1        V2        V3        V4        V5        V6        V7
   V8        V9       V10
1 Time  0.000000  0.000999  0.001999  0.002998  0.003998  0.004997
0.005997  0.006996  0.007996
2 N023 -0.031323 -0.035026 -0.029759 -0.024886 -0.024464 -0.026816
-0.033690 -0.041067 -0.038747
3 N053 -0.014083 -0.004741  0.001443 -0.010152 -0.012996 -0.005337
-0.008738 -0.015094 -0.012104
4 N123 -0.019008 -0.013494 -0.013180 -0.029208 -0.032748 -0.020243
-0.015089 -0.014439 -0.011681
5 N163 -0.054023 -0.049345 -0.037158 -0.041120 -0.044612 -0.036953
-0.036061 -0.044516 -0.046436
6 N193 -0.022171 -0.022384 -0.022338 -0.023304 -0.022569 -0.021827
-0.021996 -0.021755 -0.021846
> Laurent_data <- read.table("test2.txt")
> Laurent_data[Laurent_data$V1 %in% sensors, ]
    V1        V2        V3        V4        V5        V6        V7
   V8        V9       V10
3 N053 -0.014083 -0.004741  0.001443 -0.010152 -0.012996 -0.005337
-0.008738 -0.015094 -0.012104
5 N163 -0.054023 -0.049345 -0.037158 -0.041120 -0.044612 -0.036953
-0.036061 -0.044516 -0.046436

Best, Bill.

W. Michels, Ph.D.


On Sun, May 17, 2020 at 5:43 PM Laurent Rhelp <LaurentRHelp at free.fr> wrote:
>
> Dear R-Help List,
>
>     I would like to use an iterator to read a file filtering some
> selected lines according to the line name in order to use after a
> foreach loop. I wanted to use the checkFunc argument as the following
> example found on internet to select only prime numbers :
>
> |                                iprime <- ||iter||(1:100, checkFunc =
> ||function||(n) ||isprime||(n))|
>
> |(https://datawookie.netlify.app/blog/2013/11/iterators-in-r/)
> <https://datawookie.netlify.app/blog/2013/11/iterators-in-r/>|
>
> but the checkFunc argument seems not to be available with the function
> ireadLines (package iterators). So, I did the code below to solve my
> problem but I am sure that I miss something to use iterators with files.
> Since I found nothing on the web about ireadLines and the checkFunc
> argument, could somebody help me to understand how we have to use
> iterator (and foreach loop) on files keeping only selected lines ?
>
> Thank you very much
> Laurent
>
> Presently here is my code:
>
> ##        mock file to read: test.txt
> ##
> # Time    0    0.000999    0.001999    0.002998    0.003998 0.004997
> 0.005997    0.006996    0.007996
> # N023    -0.031323    -0.035026    -0.029759    -0.024886 -0.024464
> -0.026816    -0.03369    -0.041067    -0.038747
> # N053    -0.014083    -0.004741    0.001443    -0.010152 -0.012996
> -0.005337    -0.008738    -0.015094    -0.012104
> # N123    -0.019008    -0.013494    -0.01318    -0.029208 -0.032748
> -0.020243    -0.015089    -0.014439    -0.011681
> # N163    -0.054023    -0.049345    -0.037158    -0.04112 -0.044612
> -0.036953    -0.036061    -0.044516    -0.046436
> # N193    -0.022171    -0.022384    -0.022338    -0.023304 -0.022569
> -0.021827    -0.021996    -0.021755    -0.021846
>
>
> # sensors to keep
>
> sensors <-  c("N053", "N163")
>
>
> library(iterators)
>
> library(rlist)
>
>
> file_name <- "test.txt"
>
> con_obj <- file( file_name , "r")
> ifile <- ireadLines( con_obj , n = 1 )
>
>
> ## I do not do a loop for the example
>
> res <- list()
>
> r <- get_Lines_iter( ifile , sensors)
> res <- list.append( res , r )
> res
> r <- get_Lines_iter( ifile , sensors)
> res <- list.append( res , r )
> res
> r <- get_Lines_iter( ifile , sensors)
> do.call("cbind",res)
>
> ## the function get_Lines_iter to select and process the line
>
> get_Lines_iter  <-  function( iter , sensors, sep = '\t', quiet = FALSE){
>    ## read the next record in the iterator
>    r = try( nextElem(iter) )
>   while(  TRUE ){
>      if( class(r) == "try-error") {
>            return( stop("The iterator is empty") )
>     } else {
>     ## split the read line according to the separator
>      r_txt <- textConnection(r)
>      fields <- scan(file = r_txt, what = "character", sep = sep, quiet =
> quiet)
>       ## test if we have to keep the line
>       if( fields[1] %in% sensors){
>         ## data processing for the selected line (for the example
> transformation in dataframe)
>         n <- length(fields)
>         x <- data.frame( as.numeric(fields[2:n]) )
>         names(x) <- fields[1]
>         ## We return the values
>         print(paste0("sensor ",fields[1]," ok"))
>         return( x )
>       }else{
>        print(paste0("Sensor ", fields[1] ," not selected"))
>        r = try(nextElem(iter) )}
>     }
> }# end while loop
> }
>
>
>
>
>
>
>
> --
> L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e par le logiciel antivirus Avast.
> https://www.avast.com/antivirus
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wjm1 @end|ng |rom c@@@co|umb|@@edu  Mon May 18 03:09:10 2020
From: wjm1 @end|ng |rom c@@@co|umb|@@edu (William Michels)
Date: Sun, 17 May 2020 18:09:10 -0700
Subject: [R] iterators : checkFunc with ireadLines
In-Reply-To: <72174dd3-a819-b5fd-5a6f-2a6b46342774-2015@free.fr>
References: <72174dd3-a819-b5fd-5a6f-2a6b46342774-2015@free.fr>
Message-ID: <CAA99HCwJakeNb-+QzEjps-KB91VNSRRW8PooWVeXg-w4=mGMoQ-6452@mail.gmail.com>


Dear Laurent,

I'm going through your code quickly, and the first question I have is
whether you loaded the "gmp" library?

> library(gmp)

Attaching package: ?gmp?

The following objects are masked from ?package:base?:

    %*%, apply, crossprod, matrix, tcrossprod

> library(iterators)
> iter(1:100, checkFunc = function(n) isprime(n))
$state
<environment: 0x7fbead8837f0>

$length
[1] 100

$checkFunc
function (n)
isprime(n)

$recycle
[1] FALSE

attr(,"class")
[1] "containeriter" "iter"
>

HTH, Bill.

W. Michels, Ph.D.



On Sun, May 17, 2020 at 5:43 PM Laurent Rhelp <LaurentRHelp at free.fr> wrote:
>
> Dear R-Help List,
>
>     I would like to use an iterator to read a file filtering some
> selected lines according to the line name in order to use after a
> foreach loop. I wanted to use the checkFunc argument as the following
> example found on internet to select only prime numbers :
>
> |                                iprime <- ||iter||(1:100, checkFunc =
> ||function||(n) ||isprime||(n))|
>
> |(https://datawookie.netlify.app/blog/2013/11/iterators-in-r/)
> <https://datawookie.netlify.app/blog/2013/11/iterators-in-r/>|
>
> but the checkFunc argument seems not to be available with the function
> ireadLines (package iterators). So, I did the code below to solve my
> problem but I am sure that I miss something to use iterators with files.
> Since I found nothing on the web about ireadLines and the checkFunc
> argument, could somebody help me to understand how we have to use
> iterator (and foreach loop) on files keeping only selected lines ?
>
> Thank you very much
> Laurent
>
> Presently here is my code:
>
> ##        mock file to read: test.txt
> ##
> # Time    0    0.000999    0.001999    0.002998    0.003998 0.004997
> 0.005997    0.006996    0.007996
> # N023    -0.031323    -0.035026    -0.029759    -0.024886 -0.024464
> -0.026816    -0.03369    -0.041067    -0.038747
> # N053    -0.014083    -0.004741    0.001443    -0.010152 -0.012996
> -0.005337    -0.008738    -0.015094    -0.012104
> # N123    -0.019008    -0.013494    -0.01318    -0.029208 -0.032748
> -0.020243    -0.015089    -0.014439    -0.011681
> # N163    -0.054023    -0.049345    -0.037158    -0.04112 -0.044612
> -0.036953    -0.036061    -0.044516    -0.046436
> # N193    -0.022171    -0.022384    -0.022338    -0.023304 -0.022569
> -0.021827    -0.021996    -0.021755    -0.021846
>
>
> # sensors to keep
>
> sensors <-  c("N053", "N163")
>
>
> library(iterators)
>
> library(rlist)
>
>
> file_name <- "test.txt"
>
> con_obj <- file( file_name , "r")
> ifile <- ireadLines( con_obj , n = 1 )
>
>
> ## I do not do a loop for the example
>
> res <- list()
>
> r <- get_Lines_iter( ifile , sensors)
> res <- list.append( res , r )
> res
> r <- get_Lines_iter( ifile , sensors)
> res <- list.append( res , r )
> res
> r <- get_Lines_iter( ifile , sensors)
> do.call("cbind",res)
>
> ## the function get_Lines_iter to select and process the line
>
> get_Lines_iter  <-  function( iter , sensors, sep = '\t', quiet = FALSE){
>    ## read the next record in the iterator
>    r = try( nextElem(iter) )
>   while(  TRUE ){
>      if( class(r) == "try-error") {
>            return( stop("The iterator is empty") )
>     } else {
>     ## split the read line according to the separator
>      r_txt <- textConnection(r)
>      fields <- scan(file = r_txt, what = "character", sep = sep, quiet =
> quiet)
>       ## test if we have to keep the line
>       if( fields[1] %in% sensors){
>         ## data processing for the selected line (for the example
> transformation in dataframe)
>         n <- length(fields)
>         x <- data.frame( as.numeric(fields[2:n]) )
>         names(x) <- fields[1]
>         ## We return the values
>         print(paste0("sensor ",fields[1]," ok"))
>         return( x )
>       }else{
>        print(paste0("Sensor ", fields[1] ," not selected"))
>        r = try(nextElem(iter) )}
>     }
> }# end while loop
> }
>
>
>
>
>
>
>
> --
> L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e par le logiciel antivirus Avast.
> https://www.avast.com/antivirus
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From L@urentRHe|p @end|ng |rom |ree@|r  Mon May 18 09:05:58 2020
From: L@urentRHe|p @end|ng |rom |ree@|r (Laurent Rhelp)
Date: Mon, 18 May 2020 09:05:58 +0200
Subject: [R] iterators : checkFunc with ireadLines
In-Reply-To: <CAA99HCwtOek_jaNsD6-=CtTRoL9OHYgDqfgdSAfbNBaEZ_w8JQ@mail.gmail.com>
References: <72174dd3-a819-b5fd-5a6f-2a6b46342774-2015@free.fr>
 <CAA99HCwtOek_jaNsD6-=CtTRoL9OHYgDqfgdSAfbNBaEZ_w8JQ@mail.gmail.com>
Message-ID: <5fa47bc9-5ea8-611b-015e-73269ae23a67-885@free.fr>


Dear William,
 ?Thank you for your answer
My file is very large so I cannot read it in my memory (I cannot use 
read.table). So I want to put in memory only the line I need to process. 
With readLines, as I did, it works but I would like to use an iterator 
and a foreach loop to understand this way to do because I thought that 
it was a better solution to write a nice code.


Le 18/05/2020 ? 04:54, William Michels a ?crit?:
> Apologies, Laurent, for this two-part answer. I misunderstood your
> post where you stated you wanted to "filter(ing) some
> selected lines according to the line name... ." I thought that meant
> you had a separate index (like a series of primes) that you wanted to
> use to only read-in selected line numbers from a file (test file below
> with numbers 1:1000 each on a separate line):
>
>> library(gmp)
>> library(iterators)
>> iprime <- iter(1:100, checkFunc = function(n) isprime(n))
>> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
> Read 1 item
> [1] 2
>> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
> Read 1 item
> [1] 3
>> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
> Read 1 item
> [1] 5
>> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
> Read 1 item
> [1] 7
> However, what it really seems that you want to do is read each line of
> a (possibly enormous) file, test each line "string-wise" to keep or
> discard, and if you're keeping it, append the line to a list. I can
> certainly see the advantage of this strategy for reading in very, very
> large files, but it's not clear to me how the "ireadLines" function (
> in the "iterators" package) will help you, since it doesn't seem to
> generate anything but a sequential index.
>
> Anyway, below is an absolutely standard read-in of your data using
> read.table(). Hopefully some of the code I've posted has been useful
> to you.
>
>> sensors <-  c("N053", "N163")
>> read.table("test2.txt")
>      V1        V2        V3        V4        V5        V6        V7
>     V8        V9       V10
> 1 Time  0.000000  0.000999  0.001999  0.002998  0.003998  0.004997
> 0.005997  0.006996  0.007996
> 2 N023 -0.031323 -0.035026 -0.029759 -0.024886 -0.024464 -0.026816
> -0.033690 -0.041067 -0.038747
> 3 N053 -0.014083 -0.004741  0.001443 -0.010152 -0.012996 -0.005337
> -0.008738 -0.015094 -0.012104
> 4 N123 -0.019008 -0.013494 -0.013180 -0.029208 -0.032748 -0.020243
> -0.015089 -0.014439 -0.011681
> 5 N163 -0.054023 -0.049345 -0.037158 -0.041120 -0.044612 -0.036953
> -0.036061 -0.044516 -0.046436
> 6 N193 -0.022171 -0.022384 -0.022338 -0.023304 -0.022569 -0.021827
> -0.021996 -0.021755 -0.021846
>> Laurent_data <- read.table("test2.txt")
>> Laurent_data[Laurent_data$V1 %in% sensors, ]
>      V1        V2        V3        V4        V5        V6        V7
>     V8        V9       V10
> 3 N053 -0.014083 -0.004741  0.001443 -0.010152 -0.012996 -0.005337
> -0.008738 -0.015094 -0.012104
> 5 N163 -0.054023 -0.049345 -0.037158 -0.041120 -0.044612 -0.036953
> -0.036061 -0.044516 -0.046436
>
> Best, Bill.
>
> W. Michels, Ph.D.
>
>
> On Sun, May 17, 2020 at 5:43 PM Laurent Rhelp <LaurentRHelp at free.fr> wrote:
>> Dear R-Help List,
>>
>>      I would like to use an iterator to read a file filtering some
>> selected lines according to the line name in order to use after a
>> foreach loop. I wanted to use the checkFunc argument as the following
>> example found on internet to select only prime numbers :
>>
>> |                                iprime <- ||iter||(1:100, checkFunc =
>> ||function||(n) ||isprime||(n))|
>>
>> |(https://datawookie.netlify.app/blog/2013/11/iterators-in-r/)
>> <https://datawookie.netlify.app/blog/2013/11/iterators-in-r/>|
>>
>> but the checkFunc argument seems not to be available with the function
>> ireadLines (package iterators). So, I did the code below to solve my
>> problem but I am sure that I miss something to use iterators with files.
>> Since I found nothing on the web about ireadLines and the checkFunc
>> argument, could somebody help me to understand how we have to use
>> iterator (and foreach loop) on files keeping only selected lines ?
>>
>> Thank you very much
>> Laurent
>>
>> Presently here is my code:
>>
>> ##        mock file to read: test.txt
>> ##
>> # Time    0    0.000999    0.001999    0.002998    0.003998 0.004997
>> 0.005997    0.006996    0.007996
>> # N023    -0.031323    -0.035026    -0.029759    -0.024886 -0.024464
>> -0.026816    -0.03369    -0.041067    -0.038747
>> # N053    -0.014083    -0.004741    0.001443    -0.010152 -0.012996
>> -0.005337    -0.008738    -0.015094    -0.012104
>> # N123    -0.019008    -0.013494    -0.01318    -0.029208 -0.032748
>> -0.020243    -0.015089    -0.014439    -0.011681
>> # N163    -0.054023    -0.049345    -0.037158    -0.04112 -0.044612
>> -0.036953    -0.036061    -0.044516    -0.046436
>> # N193    -0.022171    -0.022384    -0.022338    -0.023304 -0.022569
>> -0.021827    -0.021996    -0.021755    -0.021846
>>
>>
>> # sensors to keep
>>
>> sensors <-  c("N053", "N163")
>>
>>
>> library(iterators)
>>
>> library(rlist)
>>
>>
>> file_name <- "test.txt"
>>
>> con_obj <- file( file_name , "r")
>> ifile <- ireadLines( con_obj , n = 1 )
>>
>>
>> ## I do not do a loop for the example
>>
>> res <- list()
>>
>> r <- get_Lines_iter( ifile , sensors)
>> res <- list.append( res , r )
>> res
>> r <- get_Lines_iter( ifile , sensors)
>> res <- list.append( res , r )
>> res
>> r <- get_Lines_iter( ifile , sensors)
>> do.call("cbind",res)
>>
>> ## the function get_Lines_iter to select and process the line
>>
>> get_Lines_iter  <-  function( iter , sensors, sep = '\t', quiet = FALSE){
>>     ## read the next record in the iterator
>>     r = try( nextElem(iter) )
>>    while(  TRUE ){
>>       if( class(r) == "try-error") {
>>             return( stop("The iterator is empty") )
>>      } else {
>>      ## split the read line according to the separator
>>       r_txt <- textConnection(r)
>>       fields <- scan(file = r_txt, what = "character", sep = sep, quiet =
>> quiet)
>>        ## test if we have to keep the line
>>        if( fields[1] %in% sensors){
>>          ## data processing for the selected line (for the example
>> transformation in dataframe)
>>          n <- length(fields)
>>          x <- data.frame( as.numeric(fields[2:n]) )
>>          names(x) <- fields[1]
>>          ## We return the values
>>          print(paste0("sensor ",fields[1]," ok"))
>>          return( x )
>>        }else{
>>         print(paste0("Sensor ", fields[1] ," not selected"))
>>         r = try(nextElem(iter) )}
>>      }
>> }# end while loop
>> }
>>
>>
>>
>>
>>
>>
>>
>> --
>> L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e par le logiciel antivirus Avast.
>> https://www.avast.com/antivirus
>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.



-- 
L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e par le logiciel antivirus Avast.
https://www.avast.com/antivirus


From L@urentRHe|p @end|ng |rom |ree@|r  Mon May 18 22:21:26 2020
From: L@urentRHe|p @end|ng |rom |ree@|r (Laurent Rhelp)
Date: Mon, 18 May 2020 22:21:26 +0200
Subject: [R] iterators : checkFunc with ireadLines
In-Reply-To: <CAA99HCx_VvNJzTQ=mQ7zoRFX5ZJOA3mCgbDkMiCSPAH_6Zv=QA@mail.gmail.com>
References: <72174dd3-a819-b5fd-5a6f-2a6b46342774-2015@free.fr>
 <CAA99HCwtOek_jaNsD6-=CtTRoL9OHYgDqfgdSAfbNBaEZ_w8JQ@mail.gmail.com>
 <a1ad0aec-7457-b516-1a3c-43e9e83d0ae1@free.fr>
 <CAA99HCx_VvNJzTQ=mQ7zoRFX5ZJOA3mCgbDkMiCSPAH_6Zv=QA@mail.gmail.com>
Message-ID: <aee88fc6-d08d-4577-2b0e-ae78cf5e37f5-3641@free.fr>



GREAT ! It is exactly in the idea of my request !
I like the nextElem call in the skip argument.
Thank you very much William
Best Regards
Laurent


Le 18/05/2020 ? 20:37, William Michels a ?crit?:
> Hi Laurent,
>
> Thank you for explaining your size limitations. Below is an example
> using the read.fwf() function to grab the first column of your input
> file (in 2000 row chunks). This column is converted to an index, and
> the index is used to create an iterator useful for skipping lines when
> reading input with scan(). (You could try processing your large file
> in successive 2000 line chunks, or whatever number of lines fits into
> memory). Maybe not as elegant as the approach you were going for, but
> read.fwf() should be pretty efficient:
>
>> sensors <-  c("N053", "N163")
>> read.fwf("test2.txt", widths=c(4), as.is=TRUE, flush=TRUE, n=2000, skip=0)
>      V1
> 1 Time
> 2 N023
> 3 N053
> 4 N123
> 5 N163
> 6 N193
>> first_col <- read.fwf("test2.txt", widths=c(4), as.is=TRUE, flush=TRUE, n=2000, skip=0)
>> which(first_col$V1 %in% sensors)
> [1] 3 5
>> index1 <- which(first_col$V1 %in% sensors)
>> iter_index1 <- iter(1:2000, checkFunc= function(n) {n %in% index1})
>> unlist(scan(file="test2.txt", what=list("","","","","","","","","",""), flush=TRUE, multi.line=FALSE, skip=nextElem(iter_index1)-1, nlines=1, quiet=TRUE))
>   [1] "N053"      "-0.014083" "-0.004741" "0.001443"  "-0.010152"
> "-0.012996" "-0.005337" "-0.008738" "-0.015094" "-0.012104"
>> unlist(scan(file="test2.txt", what=list("","","","","","","","","",""), flush=TRUE, multi.line=FALSE, skip=nextElem(iter_index1)-1, nlines=1, quiet=TRUE))
>   [1] "N163"      "-0.054023" "-0.049345" "-0.037158" "-0.04112"
> "-0.044612" "-0.036953" "-0.036061" "-0.044516" "-0.046436"
> (Note for this email and the previous one, I've deleted the first
> "hash" character from each line of your test file for clarity).
>
> HTH, Bill.
>
> W. Michels, Ph.D.
>
>
>
>
>
> On Mon, May 18, 2020 at 3:35 AM Laurent Rhelp <LaurentRHelp at free.fr> wrote:
>> Dear William,
>>    Thank you for your answer
>> My file is very large so I cannot read it in my memory (I cannot use
>> read.table). So I want to put in memory only the line I need to process.
>> With readLines, as I did, it works but I would like to use an iterator
>> and a foreach loop to understand this way to do because I thought that
>> it was a better solution to write a nice code.
>>
>>
>> Le 18/05/2020 ? 04:54, William Michels a ?crit :
>>> Apologies, Laurent, for this two-part answer. I misunderstood your
>>> post where you stated you wanted to "filter(ing) some
>>> selected lines according to the line name... ." I thought that meant
>>> you had a separate index (like a series of primes) that you wanted to
>>> use to only read-in selected line numbers from a file (test file below
>>> with numbers 1:1000 each on a separate line):
>>>
>>>> library(gmp)
>>>> library(iterators)
>>>> iprime <- iter(1:100, checkFunc = function(n) isprime(n))
>>>> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
>>> Read 1 item
>>> [1] 2
>>>> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
>>> Read 1 item
>>> [1] 3
>>>> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
>>> Read 1 item
>>> [1] 5
>>>> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
>>> Read 1 item
>>> [1] 7
>>> However, what it really seems that you want to do is read each line of
>>> a (possibly enormous) file, test each line "string-wise" to keep or
>>> discard, and if you're keeping it, append the line to a list. I can
>>> certainly see the advantage of this strategy for reading in very, very
>>> large files, but it's not clear to me how the "ireadLines" function (
>>> in the "iterators" package) will help you, since it doesn't seem to
>>> generate anything but a sequential index.
>>>
>>> Anyway, below is an absolutely standard read-in of your data using
>>> read.table(). Hopefully some of the code I've posted has been useful
>>> to you.
>>>
>>>> sensors <-  c("N053", "N163")
>>>> read.table("test2.txt")
>>>       V1        V2        V3        V4        V5        V6        V7
>>>      V8        V9       V10
>>> 1 Time  0.000000  0.000999  0.001999  0.002998  0.003998  0.004997
>>> 0.005997  0.006996  0.007996
>>> 2 N023 -0.031323 -0.035026 -0.029759 -0.024886 -0.024464 -0.026816
>>> -0.033690 -0.041067 -0.038747
>>> 3 N053 -0.014083 -0.004741  0.001443 -0.010152 -0.012996 -0.005337
>>> -0.008738 -0.015094 -0.012104
>>> 4 N123 -0.019008 -0.013494 -0.013180 -0.029208 -0.032748 -0.020243
>>> -0.015089 -0.014439 -0.011681
>>> 5 N163 -0.054023 -0.049345 -0.037158 -0.041120 -0.044612 -0.036953
>>> -0.036061 -0.044516 -0.046436
>>> 6 N193 -0.022171 -0.022384 -0.022338 -0.023304 -0.022569 -0.021827
>>> -0.021996 -0.021755 -0.021846
>>>> Laurent_data <- read.table("test2.txt")
>>>> Laurent_data[Laurent_data$V1 %in% sensors, ]
>>>       V1        V2        V3        V4        V5        V6        V7
>>>      V8        V9       V10
>>> 3 N053 -0.014083 -0.004741  0.001443 -0.010152 -0.012996 -0.005337
>>> -0.008738 -0.015094 -0.012104
>>> 5 N163 -0.054023 -0.049345 -0.037158 -0.041120 -0.044612 -0.036953
>>> -0.036061 -0.044516 -0.046436
>>>
>>> Best, Bill.
>>>
>>> W. Michels, Ph.D.
>>>
>>>
>>> On Sun, May 17, 2020 at 5:43 PM Laurent Rhelp <LaurentRHelp at free.fr> wrote:
>>>> Dear R-Help List,
>>>>
>>>>       I would like to use an iterator to read a file filtering some
>>>> selected lines according to the line name in order to use after a
>>>> foreach loop. I wanted to use the checkFunc argument as the following
>>>> example found on internet to select only prime numbers :
>>>>
>>>> |                                iprime <- ||iter||(1:100, checkFunc =
>>>> ||function||(n) ||isprime||(n))|
>>>>
>>>> |(https://datawookie.netlify.app/blog/2013/11/iterators-in-r/)
>>>> <https://datawookie.netlify.app/blog/2013/11/iterators-in-r/>|
>>>>
>>>> but the checkFunc argument seems not to be available with the function
>>>> ireadLines (package iterators). So, I did the code below to solve my
>>>> problem but I am sure that I miss something to use iterators with files.
>>>> Since I found nothing on the web about ireadLines and the checkFunc
>>>> argument, could somebody help me to understand how we have to use
>>>> iterator (and foreach loop) on files keeping only selected lines ?
>>>>
>>>> Thank you very much
>>>> Laurent
>>>>
>>>> Presently here is my code:
>>>>
>>>> ##        mock file to read: test.txt
>>>> ##
>>>> # Time    0    0.000999    0.001999    0.002998    0.003998 0.004997
>>>> 0.005997    0.006996    0.007996
>>>> # N023    -0.031323    -0.035026    -0.029759    -0.024886 -0.024464
>>>> -0.026816    -0.03369    -0.041067    -0.038747
>>>> # N053    -0.014083    -0.004741    0.001443    -0.010152 -0.012996
>>>> -0.005337    -0.008738    -0.015094    -0.012104
>>>> # N123    -0.019008    -0.013494    -0.01318    -0.029208 -0.032748
>>>> -0.020243    -0.015089    -0.014439    -0.011681
>>>> # N163    -0.054023    -0.049345    -0.037158    -0.04112 -0.044612
>>>> -0.036953    -0.036061    -0.044516    -0.046436
>>>> # N193    -0.022171    -0.022384    -0.022338    -0.023304 -0.022569
>>>> -0.021827    -0.021996    -0.021755    -0.021846
>>>>
>>>>
>>>> # sensors to keep
>>>>
>>>> sensors <-  c("N053", "N163")
>>>>
>>>>
>>>> library(iterators)
>>>>
>>>> library(rlist)
>>>>
>>>>
>>>> file_name <- "test.txt"
>>>>
>>>> con_obj <- file( file_name , "r")
>>>> ifile <- ireadLines( con_obj , n = 1 )
>>>>
>>>>
>>>> ## I do not do a loop for the example
>>>>
>>>> res <- list()
>>>>
>>>> r <- get_Lines_iter( ifile , sensors)
>>>> res <- list.append( res , r )
>>>> res
>>>> r <- get_Lines_iter( ifile , sensors)
>>>> res <- list.append( res , r )
>>>> res
>>>> r <- get_Lines_iter( ifile , sensors)
>>>> do.call("cbind",res)
>>>>
>>>> ## the function get_Lines_iter to select and process the line
>>>>
>>>> get_Lines_iter  <-  function( iter , sensors, sep = '\t', quiet = FALSE){
>>>>      ## read the next record in the iterator
>>>>      r = try( nextElem(iter) )
>>>>     while(  TRUE ){
>>>>        if( class(r) == "try-error") {
>>>>              return( stop("The iterator is empty") )
>>>>       } else {
>>>>       ## split the read line according to the separator
>>>>        r_txt <- textConnection(r)
>>>>        fields <- scan(file = r_txt, what = "character", sep = sep, quiet =
>>>> quiet)
>>>>         ## test if we have to keep the line
>>>>         if( fields[1] %in% sensors){
>>>>           ## data processing for the selected line (for the example
>>>> transformation in dataframe)
>>>>           n <- length(fields)
>>>>           x <- data.frame( as.numeric(fields[2:n]) )
>>>>           names(x) <- fields[1]
>>>>           ## We return the values
>>>>           print(paste0("sensor ",fields[1]," ok"))
>>>>           return( x )
>>>>         }else{
>>>>          print(paste0("Sensor ", fields[1] ," not selected"))
>>>>          r = try(nextElem(iter) )}
>>>>       }
>>>> }# end while loop
>>>> }
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e par le logiciel antivirus Avast.
>>>> https://www.avast.com/antivirus
>>>>
>>>>           [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>> --
>> L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e par le logiciel antivirus Avast.
>> https://www.avast.com/antivirus
>>


From @purd|e@@ @end|ng |rom gm@||@com  Tue May 19 12:58:18 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Tue, 19 May 2020 22:58:18 +1200
Subject: [R] Classification of wind events
In-Reply-To: <CA+8X3fX7b3kSi8vLHxpBTnqckBCnSuYLXn=skK+Xed1T5oDdsA@mail.gmail.com>
References: <8B435C9568170B469AE31E8891E8CC4F809B8DEA@ESINO.regionemarche.intra>
 <CA+8X3fUsXA7reESuWpGC2jBGRRryF6RcYa5jZxMWV7oktF1kFg@mail.gmail.com>
 <8B435C9568170B469AE31E8891E8CC4F809B9B6F@ESINO.regionemarche.intra>
 <98642722-2500-4C90-A297-8F8FFC097BAE@dcn.davis.ca.us>
 <8B435C9568170B469AE31E8891E8CC4F809B9E53@ESINO.regionemarche.intra>
 <CA+8X3fWfZ1E5vEaJwuEMxkThRhy_iCzRqOL9Yo6SHidf8Cqq0A@mail.gmail.com>
 <9B65D74C-215C-4591-8F98-D1DDBA1C8213@dcn.davis.ca.us>
 <CA+8X3fX7b3kSi8vLHxpBTnqckBCnSuYLXn=skK+Xed1T5oDdsA@mail.gmail.com>
Message-ID: <CAB8pepxRhPbo6gqR-gvWBGkWRG=71qyvpvU5cfBQr-kJNhEhbQ@mail.gmail.com>

I was impressed by Jim's effort.
So, I thought I'd try to produce an exploratory plot.

I've adapted some of his code.
The following script produces a heatmap for a cylindrical density estimate.
Bright areas are (mathematical) regions of high density.
However, the interpretation is complicated by the fact that the data
uses max wind speed rather than wind speed, per se.

I note your subject line, "Classification of wind events".
I'm not sure what sort of classification is possible.

Could there be some sort of lagged effect...?
(i.e. Could wind direction or speed at time t, be related to wind
direction or speed at time t-1, t-2, etc ...?).

------------------------------------------------------
library (barsurf)
library (probhat)
set.bs.theme ("blue")

g <- c ("WSW", "WSW", "SW", "SW", "W", "WSW", "WSW", "WSW", "W", "W",
    "SW", "WSW", "SSW", "S", "SW", "SW", "WSW", "WNW", "W", "WSW",
    "WSW", "SE", "SE", "SE", "NW", "NNE", "ENE", "SE", "NNW", "NW",
    "NW", "NW", "NW", "NW", "NW", "NE", "NW", "NW", "NW", "NW", "NW",
    "N", "WNW", "NW", "NNW", "NNW", "NW", "NW", "NW", "WNW", "ESE",
    "W", "WSW", "SW", "SW", "SW", "WSW", "SW", "S", "S", "SSW", "SW",
    "WSW", "WSW", "WSW", "WSW", "WSW", "WSW", "WSW", "SW", "WSW",
    "WSW", "WSW", "WSW", "SW", "SW", "WSW", "WSW", "WSW", "WSW",
    "WSW", "SW", "SW", "SW", "SW", "SW", "SW", "SW", "SW", "SW",
    "WSW", "WSW", "WSW", "WSW", "SW", "SW", "SW", "SW", "WSW", "SW",
    "SW", "SW", "SW", "SW", "WSW", "SW", "SW", "W", "WSW", "WSW",
    "SSW", "S", "WNW", "SW", "W", "WSW", "WSW", "SE", "SE", "SE",
    "NW", "NNE", "ENE", "SE", "NNW", "NW", "NW", "NW", "NW", "NW",
    "NW", "NE", "NW", "NW", "NW", "NW", "NW", "N", "WNW", "NW", "NNW",
    "NNW", "NW", "NW", "NW")
levels <- c("E", "ENE", "NE", "NNE", "N", "NNW", "NW", "WNW",
    "W", "WSW", "SW", "SSW", "S", "SSE", "SE", "ESE")
g <- factor (g, levels=levels)

r <- seq (0, 337.5, by=22.5)
x <- r [as.integer (g)]

y <- c (4.6, 4.6, 3.4, 3.1, 4.8, 4.2, 4.1, 4.5, 4.7, 4.3, 2.4, 2.3,
    2.2, 2.1, 2.9, 2.8, 1.8, 2.7, 4.3, 3.3, 2.3, 2.3, 3.2, 3.2, 2.9,
    2.3, 1.5, 1.8, 2.9, 2.4, 1.8, 2.4, 2.3, 2.6, 1.8, 2.3, 1.9, 2.2,
    2.8, 2.4, 1, 1.1, 1.6, 2.3, 2.5, 3.3, 3.4, 3.2, 4.5, 3.9, 3.1,
    2.4, 6, 7.8, 6.3, 7.8, 8.1, 6.1, 7.4, 9.5, 8.9, 9.1, 10.1, 10.5,
    11.1, 10.1, 10.9, 11.3, 13.4, 13.5, 12.8, 11.5, 13.1, 13.5, 11.1,
    10.5, 8.5, 10.1, 10.7, 13.6, 11.9, 14.9, 10.9, 10.9, 12.8, 12.1,
    9.1, 8.3, 8.8, 7.4, 8.4, 10.3, 10, 7, 8.5, 8.4, 8.6, 6.7, 7.3,
    6.2, 5.9, 5.9, 5.1, 5.8, 5.6, 6.5, 6.6, 11.7, 11.3, 8.7, 7.1,
    6.9, 4.3, 3.8, 4.3, 3.3, 2.3, 2.3, 3.2, 3.2, 2.9, 2.3, 1.5, 1.8,
    2.9, 2.4, 1.8, 2.4, 2.3, 2.6, 1.8, 2.3, 1.9, 2.2, 2.8, 2.4, 1,
    1.1, 1.6, 2.3, 2.5, 3.3, 3.4, 3.2, 4.5)

data.frame (g, x, y)

x2 <- c (x - 360, x, x + 360)
y2 <- rep (y, times=3)

fh <- pdfmv.cks (cbind (x2, y2), bw = c (240, 9.5) )

N <- 64
u <- seq (270, -90, length.out=N)
v <- seq (0, 15, length.out=N)
fv <- outer (u, v, function (x, y) fh (cbind (x, y) ) )

#not necessary for exploratory purposes
Fh <- cdfmv.cks (cbind (x2, y2), bw = c (240, 9.5) )
scaling.factor <- probmv (Fh, c (0, 0), c (360, max (y) ) )
fv <- fv / scaling.factor

#currently problems with descending x/y coords
#(to fix in near future)
#so set x-axis to [0, 1]
u2 <- seq (0, 1, length.out=N)

plot_cfield (u2, v, fv,
    axes = c (FALSE, TRUE),
    main="Wind Speed\n(Cylindrical Density Estimate)",
    xlab="direction", ylab="max wind speed",
    hcv=TRUE)
axis (1, c (0.05, 0.275, 0.5, 0.725, 0.95), c ("South", "West",
"North", "East", "South"), FALSE)
abline (v=0.5, lty=2, col="white")

-------------- next part --------------
A non-text attachment was scrubbed...
Name: wind_speed.png
Type: image/png
Size: 24321 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200519/dc6daff3/attachment.png>

From Po||ngW @end|ng |rom @etn@@com  Tue May 19 14:28:40 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Tue, 19 May 2020 12:28:40 +0000
Subject: [R] Help with ggplot error: #Error in FUN(X[[i]],
 ...) : object 'x' not found
Message-ID: <BYAPR06MB53835FCA0A4E629E6C4048EAAEB90@BYAPR06MB5383.namprd06.prod.outlook.com>

#RStudio Version Version 1.2.1335 
sessionInfo() 
# R version 4.0.0 Patched (2020-05-03 r78349)
#Platform: x86_64-w64-mingw32/x64 (64-bit)
#Running under: Windows 10 x64 (build 17763)

Good morning.

I am testing a small sample of my data using an example found here:
#https://www.r-orms.org/mixed-integer-linear-programming/practicals/problem-tsp/

If needed I can provide my complete script which is simply the same as the example only using a few rows from my data, let me know, as I think this is just a ggplot issue.

I am getting this error when I use ggplot
#Error in FUN(X[[i]], ...) : object 'x' not found

1. I have reviewed the following with no success.
#Sources referred to for help
#https://github.com/business-science/anomalize/issues/2
#https://stackoverflow.com/questions/38988028/error-in-funxi-object-x-not-found
#https://community.rstudio.com/t/error-in-fun-x-i-object-variable-not-found/62532/3
#https://www.neonscience.org/packages-in-r

2. I have rebooted and re-installed ggplot2 for updates

3. These are the only libraries running
library(knitr)
library(dplyr)
library(ggplot2)
library(ggmap)
library(ompr)
library(ompr.roi)
library(ROI.plugin.glpk)   

Here is the original plot attempt:
ggplot(c1members2, aes(x , y)) + 
  geom_point() + 
  geom_line(data = paths, aes(group = trip_id)) + #Error in FUN(X[[i]], ...) : object 'x' not found
  ggtitle(paste0("Optimal route with cost: ", round(objective_value(result), 2)))

#I have gone through each of the steps one by one and error occurs at --> geom_line(data = paths, aes(group = trip_id))

I have also tried the following:
Test1
p <- ggplot(c1members2, aes(x,y)) + 
  geom_point() 

p +  geom_line(data = paths, aes(group = trip_id),inherit.aes = FALSE) + #Error in FUN(X[[i]], ...) : object 'x' not found
  ggtitle(paste0("Optimal route with cost: ", round(objective_value(result), 2)))

Test2
ggplot(c1members2, aes(x,y)) + 
  geom_point() +
geom_line(data = paths, aes(group = trip_id),inherit.aes = TRUE) + #Error in FUN(X[[i]], ...) : object 'x' not found
  ggtitle(paste0("Optimal route with cost: ", round(objective_value(result), 2)))

Here are the data involved.
str(c1members2)
'data.frame':	7 obs. of  3 variables:
 $ id: int  1 2 3 4 5 6 7
 $ x : num  95.5 90.2 46.8 40.9 43.3 ...
 $ y : num  4.65 47.57 8.07 38.43 28.59 ...

str(paths)
data.frame':	14 obs. of  6 variables:
 $ trip_id  : int  1 2 3 4 5 6 7 1 2 3 ...
 $ property : chr  "from" "from" "from" "from" ...
 $ idx_val  : int  3 1 5 7 6 4 2 1 2 3 ...
 $ Longitude: num  -122 -122 -122 -122 -122 ...
 $ Latitude : num  47.3 47.2 47.3 47.6 47.6 ...
 $ city     : Factor w/ 1337 levels "ABBOTTSTOWN",..: 1169 1169 1169 1069 1069 1069 1069 1169 1069 1169 ...

Here are the dput()'s

I hope I have provided enough information to elicit a response.

Thank you.

WHP

dput(c1members2)
structure(list(id = 1:7, x = c(95.521828085515, 90.2272877161736, 
46.8465689660691, 40.8959155605829, 43.2591531526978, 23.7485886058345, 
64.0406297698986), y = c(4.64803485876031, 47.5678629996171, 
8.0689847051367, 38.4256264258875, 28.5930051512296, 43.5934562642672, 
42.415566619041)), class = "data.frame", row.names = c(NA, -7L

dput(paths) #Although I am only using city %in% c("SEATTLE","TACOMA")) from the original 2353 rows = 7 rows in c1members2
structure(list(trip_id = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 1L, 2L, 
3L, 4L, 5L, 6L, 7L), property = c("from", "from", "from", "from", 
"from", "from", "from", "to", "to", "to", "to", "to", "to", "to"
), idx_val = c(3L, 1L, 5L, 7L, 6L, 4L, 2L, 1L, 2L, 3L, 4L, 5L, 
6L, 7L), Longitude = c(-122.490417, -122.429323, -122.358115, 
-122.37003, -122.41282, -122.299261, -122.390815, -122.429323, 
-122.390815, -122.490417, -122.299261, -122.358115, -122.41282, 
-122.37003), Latitude = c(47.274599, 47.176596, 47.276776, 47.565667, 
47.646222, 47.673056, 47.549377, 47.176596, 47.549377, 47.274599, 
47.673056, 47.276776, 47.646222, 47.565667), city = structure(c(1169L, 
1169L, 1169L, 1069L, 1069L, 1069L, 1069L, 1169L, 1069L, 1169L, 
1069L, 1169L, 1069L, 1069L), .Label = c("ABBOTTSTOWN", "ABILENE", 
"ACWORTH", "ADAMS", "ADDISON", "ADKINS", "ALBANY", "ALBIA", "ALBUQUERQUE", 
"ALEXANDRIA", "ALFRED", "ALIQUIPPA", "ALISO VIEJO", "ALLEN PARK", 
"ALLENTOWN", "ALPHA", "ALPHARETTA", "ALPINE", "ALTOONA", "AMARILLO", 
"AMBLER", "AMBRIDGE", "AMHERST", "AMITYVILLE", "ANAHIEM", "ANDERSON", 
"ANDOVER", "ANGIER", "ANGLETON", "ANKENY", "ANN ARBOR", "ANZA", 
"APEX", "APOLLO", "ARCADIA", "ARCHDALE", "ARDMORE", "ARGYLE", 
"ARKANSAS CITY", "ARLINGTON", "ARNOLD", "ARTHUR", "ASHEBORO", 
"ASHEVILLE", "ASHLAND", "ASHLAND CITY", "ASHTON", "ASTON", "ATHENS", 
"ATLANTA", "ATLANTIC BCH", "AUBORN", "AUGUSTA", "AURORA", "AUSTELL", 
"AUSTIN", "AVENTURA", "AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN CITY", 
"BALDWIN PLACE", "BALLWIN", "BANGOR", "BARBOURSVILLE", "BARNEGAT", 
"BARTLETT", "BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", "BATESBURG", 
"BATON ROUGE", "BAXTER SPRINGS", "BAY CITY", "BAYONNE", "BAYSIDE", 
"BAYTOWN", "BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER CITY", 
"BELFAIR", "BELLE VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR", 
"BELLMORE", "BELLWOOD", "BELMAR", "BELMONT", "BELVIDERE", "BENNET", 
"BENTON", "BENTONVILLE", "BERLIN", "BESSEMER CITY", "BETHANY", 
"BETHEL", "BETHEL PARK", "BETHESDA", "BETHLEHEM", "BETTENDORF", 
"BIGLERVILLE", "BINGHAMTON", "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN", 
"BLACKSBURG", "BLAINE", "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD", 
"BLUE BELL", "BLUE GRASS", "BLUE POINT", "BLUE SPRINGS", "BOCA RATON", 
"BODFISH", "BOILING SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS", 
"BONNEY LAKE", "BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING GREEN", 
"BOYERTOWN", "BOYNTON BEACH", "BRADENTON", "BRADENVILLE", "BRANDAMORE", 
"BRANDON", "BRANFORD", "BRANSON", "BRASELTON", "BREEZEWOOD", 
"BRENTON", "BRENTWOOD", "BREWSTER", "BRICK", "BRIDGEPORT", "BRIDGETON", 
"BRIGHAM CITY", "BROADDUS", "BROCKPORT", "BROGUE", "BROKEN ARROW", 
"BRONX", "BROOKFIELD", "BROOKHAVEN", "BROOKLYN", "BROWNS MILLS", 
"BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", "BRYAN", "BUCKSPORT", 
"BUDD LAKE", "BUFFALO", "BULVERDE", "BUNKERVILLE", "BURBANK", 
"BURFORD", "BURIEN", "BURLINGTON", "BURR RIDGE", "BURTON", "BUSKIRK", 
"BUTLER", "BUXTON", "BYRON", "CALERA", "CAMBRIDGE", "CAMP HILL", 
"CAMPBELL", "CANAAN", "CANAL WINCHESTER", "CANASTOTA", "CANTON", 
"CARDIFF BY THE SEA", "CARL JUNCTION", "CARLISLE", "CARLTON", 
"CAROL STREAM", "CARROLLTON", "CARSON CITY", "CARTERSVILLE", 
"CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", "CASWELL", "CATO", 
"CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER POINT", "CENTERTON", 
"CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN FALLS", "CHALFONT", 
"CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", "CHARLESTON", 
"CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", "CHESHIRE", 
"CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", "CHILLICOTHE", 
"CHOCTAW", "CICERO", "CINCINNATI", "CLAY", "CLEARWATER", "CLEARWATER BEACH", 
"CLEMMONS", "CLENDENIN", "CLEVELAND", "CLEVELAND HTS", "CLEVES", 
"CLIFFSIDE PARK", "CLIFTON", "CLIFTON PARK", "CLIFTON SPGS", 
"CLINTON", "COACHELLA", "COAL TOWNSHIP", "COATESVILLE", "COLLEGE STATION", 
"COLLINSVILLE", "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE", 
"CONCORD", "CONCORD TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY", 
"CONYERS", "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA", 
"CORNELIUS", "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY", 
"COTTAGE HILLS", "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN", 
"CRANBERRY TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER", 
"CUMBERLAND FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS", 
"DALLAS", "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT", 
"DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR", 
"DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH", 
"DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY", 
"DES MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER", 
"DIXON", "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER", 
"DOYLESTOWN", "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH", 
"DUNCAN", "DUNCANNON", "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE", 
"DURHAM", "DUTTON", "EAGLE GROVE", "EAST AURORA", "EAST CHICAGO", 
"EAST EARL", "EAST HANOVER", "EAST HARTFORD", "EAST HARTLAND", 
"EAST LEROY", "EAST LIVERPOOL", "EAST MEADOW", "EAST NORTHPORT", 
"EAST ORANGE", "EAST PEORIA", "EAST SAINT LOUIS", "EASTON", "EDGERTON", 
"EDISON", "EDMOND", "EGG HBR TWP", "EL DORADO", "EL PASO", "ELDRIDGE", 
"ELGIN", "ELIZABETHVILLE", "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY", 
"ELMHURST", "ELMWOOD PARK", "ELOY", "ELY", "EMERSON", "EMORY", 
"ENDICOTT", "ENGLEWOOD", "ENID", "ENNICE", "ENOLA", "ENUMCLAW", 
"EPHRATA", "ERIE", "ESTERO", "EUREKA SPRINGS", "EVANSVILLE", 
"EVERETT", "EXCELSIOR SPRINGS", "EXTON", "FAIRFAX", "FAIRFIELD", 
"FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", "FALMOUTH", "FAR HILLS", 
"FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", "FAYETTE", "FAYETTEVILLE", 
"FELTON", "FENTON", "FERGUSON", "FIELDALE", "FINDLAY", "FINGERVILLE", 
"FLEMINGTON", "FLINT", "FLORENCE", "FLORESVILLE", "FLORISSANT", 
"FLOVILLA", "FLOWER MOUND", "FLUSHING", "FOGELSVILLE", "FOLSOM", 
"FONTANELLE", "FOREST HILLS", "FOREST PARK", "FORT DODGE", "FORT FAIRFIELD", 
"FORT GAY", "FORT LAUDERDALE", "FORT LEE", "FORT MILL", "FORT MOHAVE", 
"FORT PAYNE", "FORT PIERCE", "FORT SMITH", "FORT WAYNE", "FORT WORTH", 
"FOSTORIA", "FOUNTAIN INN", "FRANKFORT", "FRANKLIN", "FREEDOM", 
"FREEHOLD", "FREEPORT", "FREMONT", "FULLERTON", "FULSHEAR", "FULTON", 
"GADSDEN", "GAFFNEY", "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE", 
"GARDNERS", "GARNETT VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA", 
"GETTYSBURG", "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE", 
"GLEN CARBON", "GLEN COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW", 
"GLENWOOD", "GOLDEN CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM", 
"GOSHEN", "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS", 
"GRANDVIEW", "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS", 
"GRASS VALLEY", "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER", 
"GREENLAWN", "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE", 
"GREER", "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD", 
"HADLEY", "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON", 
"HAMBURG", "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE", 
"HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON", 
"HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL", 
"HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE", 
"HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH", 
"HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE", 
"HIGH HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS", 
"HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE", 
"HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD", 
"HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS", 
"HOT SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON", 
"HUFFMAN", "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON", 
"HUNTINGTON BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE", 
"HURST", "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL", 
"INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY", 
"IRMO", "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP", 
"ISSAQUAH", "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS", 
"JACKSONVILLE", "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE", 
"JEFFERSON", "JEFFERSON CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN", 
"JONESBORO", "JONESPORT", "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE", 
"KANNAPOLIS", "KANSAS CITY", "KATY", "KEARNEY", "KELLERTON", 
"KEMAH", "KENMORE", "KENNESAW", "KENT", "KINGMAN", "KINGS MOUNTAIN", 
"KINGSLAND", "KINGSTON", "KINGWOOD", "KIRKLAND", "KUTZTOWN", 
"LA QUINTA", "LA RUE", "LA VERNE", "LAFAYETTE", "LAKE BLUFF", 
"LAKE IN THE HILL", "LAKE ISABELLA", "LAKE PLACID", "LAKE SAINT LOUIS", 
"LAKE TAPPS", "LAKE VILLAGE", "LAKE WAUKOMIS", "LAKE WORTH", 
"LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD RCH", "LAMBERTVILLE", 
"LANCASTER", "LANDISVILLE", "LANSDALE", "LAREDO", "LARGO", "LAS VEGAS", 
"LATROBE", "LAUDERHILL", "LAWNDALE", "LAWRENCE", "LAWRENCEVILLE", 
"LE MARS", "LE ROY", "LEAGUE CITY", "LEAVENWORTH", "LEAWOOD", 
"LEBANON", "LECOMPTON", "LEECHBURG", "LEES SUMMIT", "LEESBURG", 
"LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", "LEVITTOWN", "LEWES", 
"LEWISTOWN", "LEXINGTON", "LIBERTY TWP", "LILLINGTON", "LINCOLN", 
"LINDENHURST", "LINTON", "LISBON", "LITCHFIELD PK", "LITHIA", 
"LITITZ", "LITTLE EGG HARBOR TO", "LK FOREST PK", "LK PEEKSKILL", 
"LOCKHART", "LOCKWOOD", "LOGAN", "LOMA LINDA", "LONE JACK", "LONG BEACH", 
"LONG LANE", "LONGVIEW", "LORAIN", "LORTON", "LOS ANGELES", "LOUISBURG", 
"LOVES PARK", "LOWER BURRELL", "LOWRY CITY", "LUBBOCK", "LUGOFF", 
"LUMBERTON", "LYNBROOK", "LYNNWOOD", "MACHESNEY PARK", "MACON", 
"MAGNOLIA", "MAHOPAC", "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE", 
"MANASSAS", "MANCHACA", "MANCHESTER", "MANCHESTER TOWNSHIP", 
"MANCHESTER TW", "MANHEIM", "MANITO", "MANLIUS", "MANNINGTON", 
"MANSFIELD", "MANSURA", "MAPLE PARK", "MARBLE FALLS", "MARCELLUS", 
"MARICOPA", "MARIETTA", "MARION", "MARKLEYSBURG", "MARS HILL", 
"MARSHALLTOWN", "MARSHFIELD", "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE", 
"MASON CITY", "MASSEY", "MASSILLON", "MASURY", "MATAWAN", "MATEWAN", 
"MATTAPOISETT", "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND", 
"MC DONALD", "MC KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN", 
"MCALESTER", "MCDONOUGH", "MCKEESPORT", "MCKINNEY", "MECHANICSBURG", 
"MECHANICSVILLE", "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS", 
"MERIDEN", "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA", 
"MESQUITE", "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN", 
"MIDLAND", "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK", 
"MILLEDGEVILLE", "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE", 
"MINEOLA", "MINERAL SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION", 
"MISSION HILLS", "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY", 
"MOLINE", "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE", 
"MONTCLAIR", "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON", 
"MORGANTOWN", "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE", 
"MOUNT JOY", "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO", 
"MOUNT UNION", "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT", 
"MULVANE", "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT", 
"MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS", 
"NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES", 
"NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG", 
"NEW BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK", 
"NEW CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON", 
"NEW LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE", 
"NEW ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK", 
"NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT", 
"NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA", 
"NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK", 
"NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON", 
"NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND HILLS", 
"NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE", 
"NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON", 
"OAK RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE", 
"OMAHA", "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION", 
"ORONO", "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO", 
"OTTAWA", "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD", 
"OYSTER BAY", "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR", 
"PALMDALE", "PALMYRA", "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE", 
"PARLIN", "PARMA", "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY", 
"PAXTON", "PEA RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN", 
"PELION", "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE", 
"PENNSBURG", "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS", 
"PERRYSBURG", "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI", 
"PHOENIX", "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN", 
"PINE BLUFF", "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY", 
"PITMAN", "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY", 
"PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO LAKE", 
"POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", "PORT CHARLOTTE", 
"PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", "PORT WASHINGTON", 
"PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", "POTTSVILLE", 
"POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE VILLAGE", 
"PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT PLEASANT", 
"PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", "QUINCY", 
"RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", "READLYN", 
"RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO PARK", 
"REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", "RICH HILL", 
"RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", "RINCON", 
"RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", "RIVERSIDE", 
"ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", "ROCKAWAY BCH", 
"ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", "ROGERS", 
"ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE", 
"ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT", 
"ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN", 
"RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO", 
"SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR", 
"SAINT CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS", 
"SAINT PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY", 
"SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO", 
"SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN", 
"SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", "SARASOTA", 
"SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", "SAYVILLE", 
"SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", "SCOTT", "SCOTTSDALE", 
"SEAFORD", "SEALE", "SEATTLE", "SEDONA", "SEGUIN", "SELDEN", 
"SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", "SHARON", "SHARPSBURG", 
"SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", "SHELLSBURG", "SHELTON", 
"SHENANDOAH", "SHORELINE", "SHOREWOOD", "SHREVEPORT", "SICKLERVILLE", 
"SILER CITY", "SILOAM SPRINGS", "SIMON", "SIMPSONVILLE", "SIMSBURY", 
"SIOUX CITY", "SIOUX FALLS", "SKOKIE", "SKOWHEGAN", "SLATINGTON", 
"SLIDELL", "SMITHFIELD", "SN BERNRDNO", "SNELLVILLE", "SOCORRO", 
"SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH BELOIT", "SOUTH BEND", 
"SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH PARK", "SOUTH PLAINFIELD", 
"SOUTH SIOUX CITY", "SOUTHAMPTON", "SOUTHBURY", "SOUTHGATE", 
"SPARKS", "SPARROW BUSH", "SPENCER", "SPRING", "SPRING BRANCH", 
"SPRING GROVE", "SPRING HILL", "SPRING HOPE", "SPRING VALLEY", 
"SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD GARDENS", "SPRINGHILL", 
"SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", "ST PETERSBURG", 
"ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", "STANLEY", 
"STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", "STILLMAN VALLEY", 
"STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", "STRAFFORD", 
"STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", "SUFFIELD", 
"SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", "SUMMIT", 
"SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", "SWEENY", 
"SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", "TAFTVILLE", 
"TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", "TAYLORVILLE", 
"TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", "THOMASVILLE", 
"TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY PARK", 
"TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", "TOPEKA", 
"TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS RST", 
"TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER", 
"TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY", 
"UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA", 
"URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO", 
"VAN BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO", 
"VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA", 
"VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH", 
"W TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN", 
"WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG", 
"WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA", 
"WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE", 
"WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER", 
"WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO", 
"WEST CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN", 
"WEST HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH", 
"WEST POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD", 
"WEST VALLEY CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER", 
"WESTMORELAND CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD", 
"WHARTON", "WHEELING", "WHITE PLAINS", "WHITEHALL", "WHITESTONE", 
"WHITESTOWN", "WHITING", "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD", 
"WILLARD", "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK", 
"WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS", 
"WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON", 
"WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE", 
"WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS", 
"WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH", 
"YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS", 
"YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor")), row.names = c(NA, 
-14L), class = "data.frame")






Proprietary

NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}


From Po||ngW @end|ng |rom @etn@@com  Tue May 19 15:04:25 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Tue, 19 May 2020 13:04:25 +0000
Subject: [R] Help with ggplot error: #Error in FUN(X[[i]],
 ...) : object 'x' not found
In-Reply-To: <CALrbzg1bPZGtUK9BYAZUWQB3pFE_C-783xmDR=Zf7AfgFaVROQ@mail.gmail.com>
References: <BYAPR06MB53835FCA0A4E629E6C4048EAAEB90@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CALrbzg1bPZGtUK9BYAZUWQB3pFE_C-783xmDR=Zf7AfgFaVROQ@mail.gmail.com>
Message-ID: <BYAPR06MB53836469C280B465C60075C1AEB90@BYAPR06MB5383.namprd06.prod.outlook.com>


Hello Ben and thank you for your response.

I thought that was what I was doing when I ran this version:
Test1
p <- ggplot(c1members2, aes(x,y)) + 
  geom_point() 

p +  geom_line(data = paths, aes(group = trip_id),inherit.aes = FALSE) + #Error in FUN(X[[i]], ...) : object 'x' not found
  ggtitle(paste0("Optimal route with cost: ", round(objective_value(result), 2)))

Is this not what you mean?	

str(p)
List of 9
 $ data       :'data.frame':	7 obs. of  3 variables:
  ..$ id: int [1:7] 1 2 3 4 5 6 7
  ..$ x : num [1:7] 95.5 90.2 46.8 40.9 43.3 ...
  ..$ y : num [1:7] 4.65 47.57 8.07 38.43 28.59 ...
 $ layers     :List of 1
  ..$ :Classes 'LayerInstance', 'Layer', 'ggproto', 'gg' <ggproto object: Class LayerInstance, Layer, gg>
    aes_params: list
    compute_aesthetics: function
    compute_geom_1: function
    compute_geom_2: function
    compute_position: function
    compute_statistic: function
    data: waiver
    draw_geom: function
    finish_statistics: function
    geom: <ggproto object: Class GeomPoint, Geom, gg>
        aesthetics: function
        default_aes: uneval
        draw_group: function
        draw_key: function
        draw_layer: function
        draw_panel: function
        extra_params: na.rm
        handle_na: function
        non_missing_aes: size shape colour
        optional_aes: 
        parameters: function
        required_aes: x y
        setup_data: function
        setup_params: function
        use_defaults: function
        super:  <ggproto object: Class Geom, gg>
    geom_params: list
    inherit.aes: TRUE
    layer_data: function
    map_statistic: function
    mapping: NULL
    position: <ggproto object: Class PositionIdentity, Position, gg>
        compute_layer: function
        compute_panel: function
        required_aes: 
        setup_data: function
        setup_params: function
        super:  <ggproto object: Class Position, gg>
    print: function
    setup_layer: function
    show.legend: NA
    stat: <ggproto object: Class StatIdentity, Stat, gg>
        aesthetics: function
        compute_group: function
        compute_layer: function
        compute_panel: function
        default_aes: uneval
        extra_params: na.rm
        finish_layer: function
        non_missing_aes: 
        parameters: function
        required_aes: 
        retransform: TRUE
        setup_data: function
        setup_params: function
        super:  <ggproto object: Class Stat, gg>
    stat_params: list
    super:  <ggproto object: Class Layer, gg> 
 $ scales     :Classes 'ScalesList', 'ggproto', 'gg' <ggproto object: Class ScalesList, gg>
    add: function
    clone: function
    find: function
    get_scales: function
    has_scale: function
    input: function
    n: function
    non_position_scales: function
    scales: list
    super:  <ggproto object: Class ScalesList, gg> 
 $ mapping    :List of 2
  ..$ x: language ~x
  .. ..- attr(*, ".Environment")=<environment: R_GlobalEnv> 
  ..$ y: language ~y
  .. ..- attr(*, ".Environment")=<environment: R_GlobalEnv> 
  ..- attr(*, "class")= chr "uneval"
 $ theme      : list()
 $ coordinates:Classes 'CoordCartesian', 'Coord', 'ggproto', 'gg' <ggproto object: Class CoordCartesian, Coord, gg>
    aspect: function
    backtransform_range: function
    clip: on
    default: TRUE
    distance: function
    expand: TRUE
    is_free: function
    is_linear: function
    labels: function
    limits: list
    modify_scales: function
    range: function
    render_axis_h: function
    render_axis_v: function
    render_bg: function
    render_fg: function
    setup_data: function
    setup_layout: function
    setup_panel_guides: function
    setup_panel_params: function
    setup_params: function
    train_panel_guides: function
    transform: function
    super:  <ggproto object: Class CoordCartesian, Coord, gg> 
 $ facet      :Classes 'FacetNull', 'Facet', 'ggproto', 'gg' <ggproto object: Class FacetNull, Facet, gg>
    compute_layout: function
    draw_back: function
    draw_front: function
    draw_labels: function
    draw_panels: function
    finish_data: function
    init_scales: function
    map_data: function
    params: list
    setup_data: function
    setup_params: function
    shrink: TRUE
    train_scales: function
    vars: function
    super:  <ggproto object: Class FacetNull, Facet, gg> 
 $ plot_env   :<environment: R_GlobalEnv> 
 $ labels     :List of 2
  ..$ x: chr "x"
  ..$ y: chr "y"
 - attr(*, "class")= chr [1:2] "gg" "ggplot"














From: Ben Tupper <btupper at bigelow.org> 
Sent: Tuesday, May 19, 2020 7:46 AM
To: Poling, William <PolingW at aetna.com>
Cc: r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with ggplot error: #Error in FUN(X[[i]], ...) : object 'x' not found

**** External Email - Use Caution ****
Hi,

When you add a new layer to a plot and you provide new data, wouldn't it make sense to provide aes() with some indication of the names of the variables to use for coordinate plotting?? You have provided a grouping variable for the paths, but what of coordinates x and y?? How could geom_path() know that your lon/lat variables are to be used as x and y coordinates in the plot??

Cheers,
Ben



On Tue, May 19, 2020 at 8:29 AM Poling, William via R-help <mailto:r-help at r-project.org> wrote:
#RStudio Version Version 1.2.1335 
sessionInfo() 
# R version 4.0.0 Patched (2020-05-03 r78349)
#Platform: x86_64-w64-mingw32/x64 (64-bit)
#Running under: Windows 10 x64 (build 17763)

Good morning.

I am testing a small sample of my data using an example found here:
#https://urldefense.proofpoint.com/v2/url?u=https-3A__www.r-2Dorms.org_mixed-2Dinteger-2Dlinear-2Dprogramming_practicals_problem-2Dtsp_&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=N45-UMvb1jEdpZi0xmhOt4rv3lQf2DBkFap0cMXXvWc&s=mETBFqF-RTLN5awPNRy_CxYYqxMrEu99FK68PoNMKq0&e=

If needed I can provide my complete script which is simply the same as the example only using a few rows from my data, let me know, as I think this is just a ggplot issue.

I am getting this error when I use ggplot
#Error in FUN(X[[i]], ...) : object 'x' not found

1. I have reviewed the following with no success.
#Sources referred to for help
#https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_business-2Dscience_anomalize_issues_2&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=N45-UMvb1jEdpZi0xmhOt4rv3lQf2DBkFap0cMXXvWc&s=wXtp8KhCuHjmSyGkQ4ff3vFbBf2Ve4GawN3qTE2lK_Q&e=
#https://urldefense.proofpoint.com/v2/url?u=https-3A__stackoverflow.com_questions_38988028_error-2Din-2Dfunxi-2Dobject-2Dx-2Dnot-2Dfound-23https-3A__community.rstudio.com_t_error-2Din-2Dfun-2Dx-2Di-2Dobject-2Dvariable-2Dnot-2Dfound_62532_3-2523https-3A__www.neonscience.org_packages-2Din-2Dr&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=N45-UMvb1jEdpZi0xmhOt4rv3lQf2DBkFap0cMXXvWc&s=_Z-vYnJW4sxansRa74mUatDLP6O0k5BB55JadgWCIyM&e=

2. I have rebooted and re-installed ggplot2 for updates

3. These are the only libraries running
library(knitr)
library(dplyr)
library(ggplot2)
library(ggmap)
library(ompr)
library(ompr.roi)
library(ROI.plugin.glpk)? ?

Here is the original plot attempt:
ggplot(c1members2, aes(x , y)) + 
? geom_point() + 
? geom_line(data = paths, aes(group = trip_id)) + #Error in FUN(X[[i]], ...) : object 'x' not found
? ggtitle(paste0("Optimal route with cost: ", round(objective_value(result), 2)))

#I have gone through each of the steps one by one and error occurs at --> geom_line(data = paths, aes(group = trip_id))

I have also tried the following:
Test1
p <- ggplot(c1members2, aes(x,y)) + 
? geom_point() 

p +? geom_line(data = paths, aes(group = trip_id),inherit.aes = FALSE) + #Error in FUN(X[[i]], ...) : object 'x' not found
? ggtitle(paste0("Optimal route with cost: ", round(objective_value(result), 2)))

Test2
ggplot(c1members2, aes(x,y)) + 
? geom_point() +
geom_line(data = paths, aes(group = trip_id),inherit.aes = TRUE) + #Error in FUN(X[[i]], ...) : object 'x' not found
? ggtitle(paste0("Optimal route with cost: ", round(objective_value(result), 2)))

Here are the data involved.
str(c1members2)
'data.frame':? ?7 obs. of? 3 variables:
?$ id: int? 1 2 3 4 5 6 7
?$ x : num? 95.5 90.2 46.8 40.9 43.3 ...
?$ y : num? 4.65 47.57 8.07 38.43 28.59 ...

str(paths)
data.frame':? ? 14 obs. of? 6 variables:
?$ trip_id? : int? 1 2 3 4 5 6 7 1 2 3 ...
?$ property : chr? "from" "from" "from" "from" ...
?$ idx_val? : int? 3 1 5 7 6 4 2 1 2 3 ...
?$ Longitude: num? -122 -122 -122 -122 -122 ...
?$ Latitude : num? 47.3 47.2 47.3 47.6 47.6 ...
?$ city? ? ?: Factor w/ 1337 levels "ABBOTTSTOWN",..: 1169 1169 1169 1069 1069 1069 1069 1169 1069 1169 ...

Here are the dput()'s

I hope I have provided enough information to elicit a response.

Thank you.

WHP

dput(c1members2)
structure(list(id = 1:7, x = c(95.521828085515, 90.2272877161736, 
46.8465689660691, 40.8959155605829, 43.2591531526978, 23.7485886058345, 
64.0406297698986), y = c(4.64803485876031, 47.5678629996171, 
8.0689847051367, 38.4256264258875, 28.5930051512296, 43.5934562642672, 
42.415566619041)), class = "data.frame", row.names = c(NA, -7L

dput(paths) #Although I am only using city %in% c("SEATTLE","TACOMA")) from the original 2353 rows = 7 rows in c1members2
structure(list(trip_id = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 1L, 2L, 
3L, 4L, 5L, 6L, 7L), property = c("from", "from", "from", "from", 
"from", "from", "from", "to", "to", "to", "to", "to", "to", "to"
), idx_val = c(3L, 1L, 5L, 7L, 6L, 4L, 2L, 1L, 2L, 3L, 4L, 5L, 
6L, 7L), Longitude = c(-122.490417, -122.429323, -122.358115, 
-122.37003, -122.41282, -122.299261, -122.390815, -122.429323, 
-122.390815, -122.490417, -122.299261, -122.358115, -122.41282, 
-122.37003), Latitude = c(47.274599, 47.176596, 47.276776, 47.565667, 
47.646222, 47.673056, 47.549377, 47.176596, 47.549377, 47.274599, 
47.673056, 47.276776, 47.646222, 47.565667), city = structure(c(1169L, 
1169L, 1169L, 1069L, 1069L, 1069L, 1069L, 1169L, 1069L, 1169L, 
1069L, 1169L, 1069L, 1069L), .Label = c("ABBOTTSTOWN", "ABILENE", 
"ACWORTH", "ADAMS", "ADDISON", "ADKINS", "ALBANY", "ALBIA", "ALBUQUERQUE", 
"ALEXANDRIA", "ALFRED", "ALIQUIPPA", "ALISO VIEJO", "ALLEN PARK", 
"ALLENTOWN", "ALPHA", "ALPHARETTA", "ALPINE", "ALTOONA", "AMARILLO", 
"AMBLER", "AMBRIDGE", "AMHERST", "AMITYVILLE", "ANAHIEM", "ANDERSON", 
"ANDOVER", "ANGIER", "ANGLETON", "ANKENY", "ANN ARBOR", "ANZA", 
"APEX", "APOLLO", "ARCADIA", "ARCHDALE", "ARDMORE", "ARGYLE", 
"ARKANSAS CITY", "ARLINGTON", "ARNOLD", "ARTHUR", "ASHEBORO", 
"ASHEVILLE", "ASHLAND", "ASHLAND CITY", "ASHTON", "ASTON", "ATHENS", 
"ATLANTA", "ATLANTIC BCH", "AUBORN", "AUGUSTA", "AURORA", "AUSTELL", 
"AUSTIN", "AVENTURA", "AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN CITY", 
"BALDWIN PLACE", "BALLWIN", "BANGOR", "BARBOURSVILLE", "BARNEGAT", 
"BARTLETT", "BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", "BATESBURG", 
"BATON ROUGE", "BAXTER SPRINGS", "BAY CITY", "BAYONNE", "BAYSIDE", 
"BAYTOWN", "BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER CITY", 
"BELFAIR", "BELLE VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR", 
"BELLMORE", "BELLWOOD", "BELMAR", "BELMONT", "BELVIDERE", "BENNET", 
"BENTON", "BENTONVILLE", "BERLIN", "BESSEMER CITY", "BETHANY", 
"BETHEL", "BETHEL PARK", "BETHESDA", "BETHLEHEM", "BETTENDORF", 
"BIGLERVILLE", "BINGHAMTON", "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN", 
"BLACKSBURG", "BLAINE", "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD", 
"BLUE BELL", "BLUE GRASS", "BLUE POINT", "BLUE SPRINGS", "BOCA RATON", 
"BODFISH", "BOILING SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS", 
"BONNEY LAKE", "BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING GREEN", 
"BOYERTOWN", "BOYNTON BEACH", "BRADENTON", "BRADENVILLE", "BRANDAMORE", 
"BRANDON", "BRANFORD", "BRANSON", "BRASELTON", "BREEZEWOOD", 
"BRENTON", "BRENTWOOD", "BREWSTER", "BRICK", "BRIDGEPORT", "BRIDGETON", 
"BRIGHAM CITY", "BROADDUS", "BROCKPORT", "BROGUE", "BROKEN ARROW", 
"BRONX", "BROOKFIELD", "BROOKHAVEN", "BROOKLYN", "BROWNS MILLS", 
"BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", "BRYAN", "BUCKSPORT", 
"BUDD LAKE", "BUFFALO", "BULVERDE", "BUNKERVILLE", "BURBANK", 
"BURFORD", "BURIEN", "BURLINGTON", "BURR RIDGE", "BURTON", "BUSKIRK", 
"BUTLER", "BUXTON", "BYRON", "CALERA", "CAMBRIDGE", "CAMP HILL", 
"CAMPBELL", "CANAAN", "CANAL WINCHESTER", "CANASTOTA", "CANTON", 
"CARDIFF BY THE SEA", "CARL JUNCTION", "CARLISLE", "CARLTON", 
"CAROL STREAM", "CARROLLTON", "CARSON CITY", "CARTERSVILLE", 
"CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", "CASWELL", "CATO", 
"CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER POINT", "CENTERTON", 
"CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN FALLS", "CHALFONT", 
"CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", "CHARLESTON", 
"CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", "CHESHIRE", 
"CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", "CHILLICOTHE", 
"CHOCTAW", "CICERO", "CINCINNATI", "CLAY", "CLEARWATER", "CLEARWATER BEACH", 
"CLEMMONS", "CLENDENIN", "CLEVELAND", "CLEVELAND HTS", "CLEVES", 
"CLIFFSIDE PARK", "CLIFTON", "CLIFTON PARK", "CLIFTON SPGS", 
"CLINTON", "COACHELLA", "COAL TOWNSHIP", "COATESVILLE", "COLLEGE STATION", 
"COLLINSVILLE", "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE", 
"CONCORD", "CONCORD TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY", 
"CONYERS", "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA", 
"CORNELIUS", "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY", 
"COTTAGE HILLS", "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN", 
"CRANBERRY TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER", 
"CUMBERLAND FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS", 
"DALLAS", "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT", 
"DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR", 
"DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH", 
"DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY", 
"DES MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER", 
"DIXON", "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER", 
"DOYLESTOWN", "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH", 
"DUNCAN", "DUNCANNON", "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE", 
"DURHAM", "DUTTON", "EAGLE GROVE", "EAST AURORA", "EAST CHICAGO", 
"EAST EARL", "EAST HANOVER", "EAST HARTFORD", "EAST HARTLAND", 
"EAST LEROY", "EAST LIVERPOOL", "EAST MEADOW", "EAST NORTHPORT", 
"EAST ORANGE", "EAST PEORIA", "EAST SAINT LOUIS", "EASTON", "EDGERTON", 
"EDISON", "EDMOND", "EGG HBR TWP", "EL DORADO", "EL PASO", "ELDRIDGE", 
"ELGIN", "ELIZABETHVILLE", "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY", 
"ELMHURST", "ELMWOOD PARK", "ELOY", "ELY", "EMERSON", "EMORY", 
"ENDICOTT", "ENGLEWOOD", "ENID", "ENNICE", "ENOLA", "ENUMCLAW", 
"EPHRATA", "ERIE", "ESTERO", "EUREKA SPRINGS", "EVANSVILLE", 
"EVERETT", "EXCELSIOR SPRINGS", "EXTON", "FAIRFAX", "FAIRFIELD", 
"FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", "FALMOUTH", "FAR HILLS", 
"FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", "FAYETTE", "FAYETTEVILLE", 
"FELTON", "FENTON", "FERGUSON", "FIELDALE", "FINDLAY", "FINGERVILLE", 
"FLEMINGTON", "FLINT", "FLORENCE", "FLORESVILLE", "FLORISSANT", 
"FLOVILLA", "FLOWER MOUND", "FLUSHING", "FOGELSVILLE", "FOLSOM", 
"FONTANELLE", "FOREST HILLS", "FOREST PARK", "FORT DODGE", "FORT FAIRFIELD", 
"FORT GAY", "FORT LAUDERDALE", "FORT LEE", "FORT MILL", "FORT MOHAVE", 
"FORT PAYNE", "FORT PIERCE", "FORT SMITH", "FORT WAYNE", "FORT WORTH", 
"FOSTORIA", "FOUNTAIN INN", "FRANKFORT", "FRANKLIN", "FREEDOM", 
"FREEHOLD", "FREEPORT", "FREMONT", "FULLERTON", "FULSHEAR", "FULTON", 
"GADSDEN", "GAFFNEY", "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE", 
"GARDNERS", "GARNETT VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA", 
"GETTYSBURG", "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE", 
"GLEN CARBON", "GLEN COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW", 
"GLENWOOD", "GOLDEN CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM", 
"GOSHEN", "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS", 
"GRANDVIEW", "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS", 
"GRASS VALLEY", "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER", 
"GREENLAWN", "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE", 
"GREER", "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD", 
"HADLEY", "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON", 
"HAMBURG", "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE", 
"HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON", 
"HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL", 
"HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE", 
"HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH", 
"HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE", 
"HIGH HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS", 
"HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE", 
"HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD", 
"HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS", 
"HOT SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON", 
"HUFFMAN", "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON", 
"HUNTINGTON BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE", 
"HURST", "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL", 
"INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY", 
"IRMO", "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP", 
"ISSAQUAH", "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS", 
"JACKSONVILLE", "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE", 
"JEFFERSON", "JEFFERSON CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN", 
"JONESBORO", "JONESPORT", "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE", 
"KANNAPOLIS", "KANSAS CITY", "KATY", "KEARNEY", "KELLERTON", 
"KEMAH", "KENMORE", "KENNESAW", "KENT", "KINGMAN", "KINGS MOUNTAIN", 
"KINGSLAND", "KINGSTON", "KINGWOOD", "KIRKLAND", "KUTZTOWN", 
"LA QUINTA", "LA RUE", "LA VERNE", "LAFAYETTE", "LAKE BLUFF", 
"LAKE IN THE HILL", "LAKE ISABELLA", "LAKE PLACID", "LAKE SAINT LOUIS", 
"LAKE TAPPS", "LAKE VILLAGE", "LAKE WAUKOMIS", "LAKE WORTH", 
"LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD RCH", "LAMBERTVILLE", 
"LANCASTER", "LANDISVILLE", "LANSDALE", "LAREDO", "LARGO", "LAS VEGAS", 
"LATROBE", "LAUDERHILL", "LAWNDALE", "LAWRENCE", "LAWRENCEVILLE", 
"LE MARS", "LE ROY", "LEAGUE CITY", "LEAVENWORTH", "LEAWOOD", 
"LEBANON", "LECOMPTON", "LEECHBURG", "LEES SUMMIT", "LEESBURG", 
"LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", "LEVITTOWN", "LEWES", 
"LEWISTOWN", "LEXINGTON", "LIBERTY TWP", "LILLINGTON", "LINCOLN", 
"LINDENHURST", "LINTON", "LISBON", "LITCHFIELD PK", "LITHIA", 
"LITITZ", "LITTLE EGG HARBOR TO", "LK FOREST PK", "LK PEEKSKILL", 
"LOCKHART", "LOCKWOOD", "LOGAN", "LOMA LINDA", "LONE JACK", "LONG BEACH", 
"LONG LANE", "LONGVIEW", "LORAIN", "LORTON", "LOS ANGELES", "LOUISBURG", 
"LOVES PARK", "LOWER BURRELL", "LOWRY CITY", "LUBBOCK", "LUGOFF", 
"LUMBERTON", "LYNBROOK", "LYNNWOOD", "MACHESNEY PARK", "MACON", 
"MAGNOLIA", "MAHOPAC", "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE", 
"MANASSAS", "MANCHACA", "MANCHESTER", "MANCHESTER TOWNSHIP", 
"MANCHESTER TW", "MANHEIM", "MANITO", "MANLIUS", "MANNINGTON", 
"MANSFIELD", "MANSURA", "MAPLE PARK", "MARBLE FALLS", "MARCELLUS", 
"MARICOPA", "MARIETTA", "MARION", "MARKLEYSBURG", "MARS HILL", 
"MARSHALLTOWN", "MARSHFIELD", "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE", 
"MASON CITY", "MASSEY", "MASSILLON", "MASURY", "MATAWAN", "MATEWAN", 
"MATTAPOISETT", "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND", 
"MC DONALD", "MC KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN", 
"MCALESTER", "MCDONOUGH", "MCKEESPORT", "MCKINNEY", "MECHANICSBURG", 
"MECHANICSVILLE", "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS", 
"MERIDEN", "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA", 
"MESQUITE", "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN", 
"MIDLAND", "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK", 
"MILLEDGEVILLE", "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE", 
"MINEOLA", "MINERAL SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION", 
"MISSION HILLS", "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY", 
"MOLINE", "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE", 
"MONTCLAIR", "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON", 
"MORGANTOWN", "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE", 
"MOUNT JOY", "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO", 
"MOUNT UNION", "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT", 
"MULVANE", "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT", 
"MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS", 
"NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES", 
"NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG", 
"NEW BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK", 
"NEW CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON", 
"NEW LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE", 
"NEW ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK", 
"NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT", 
"NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA", 
"NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK", 
"NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON", 
"NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND HILLS", 
"NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE", 
"NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON", 
"OAK RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE", 
"OMAHA", "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION", 
"ORONO", "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO", 
"OTTAWA", "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD", 
"OYSTER BAY", "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR", 
"PALMDALE", "PALMYRA", "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE", 
"PARLIN", "PARMA", "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY", 
"PAXTON", "PEA RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN", 
"PELION", "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE", 
"PENNSBURG", "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS", 
"PERRYSBURG", "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI", 
"PHOENIX", "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN", 
"PINE BLUFF", "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY", 
"PITMAN", "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY", 
"PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO LAKE", 
"POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", "PORT CHARLOTTE", 
"PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", "PORT WASHINGTON", 
"PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", "POTTSVILLE", 
"POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE VILLAGE", 
"PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT PLEASANT", 
"PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", "QUINCY", 
"RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", "READLYN", 
"RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO PARK", 
"REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", "RICH HILL", 
"RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", "RINCON", 
"RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", "RIVERSIDE", 
"ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", "ROCKAWAY BCH", 
"ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", "ROGERS", 
"ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE", 
"ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT", 
"ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN", 
"RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO", 
"SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR", 
"SAINT CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS", 
"SAINT PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY", 
"SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO", 
"SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN", 
"SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", "SARASOTA", 
"SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", "SAYVILLE", 
"SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", "SCOTT", "SCOTTSDALE", 
"SEAFORD", "SEALE", "SEATTLE", "SEDONA", "SEGUIN", "SELDEN", 
"SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", "SHARON", "SHARPSBURG", 
"SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", "SHELLSBURG", "SHELTON", 
"SHENANDOAH", "SHORELINE", "SHOREWOOD", "SHREVEPORT", "SICKLERVILLE", 
"SILER CITY", "SILOAM SPRINGS", "SIMON", "SIMPSONVILLE", "SIMSBURY", 
"SIOUX CITY", "SIOUX FALLS", "SKOKIE", "SKOWHEGAN", "SLATINGTON", 
"SLIDELL", "SMITHFIELD", "SN BERNRDNO", "SNELLVILLE", "SOCORRO", 
"SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH BELOIT", "SOUTH BEND", 
"SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH PARK", "SOUTH PLAINFIELD", 
"SOUTH SIOUX CITY", "SOUTHAMPTON", "SOUTHBURY", "SOUTHGATE", 
"SPARKS", "SPARROW BUSH", "SPENCER", "SPRING", "SPRING BRANCH", 
"SPRING GROVE", "SPRING HILL", "SPRING HOPE", "SPRING VALLEY", 
"SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD GARDENS", "SPRINGHILL", 
"SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", "ST PETERSBURG", 
"ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", "STANLEY", 
"STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", "STILLMAN VALLEY", 
"STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", "STRAFFORD", 
"STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", "SUFFIELD", 
"SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", "SUMMIT", 
"SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", "SWEENY", 
"SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", "TAFTVILLE", 
"TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", "TAYLORVILLE", 
"TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", "THOMASVILLE", 
"TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY PARK", 
"TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", "TOPEKA", 
"TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS RST", 
"TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER", 
"TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY", 
"UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA", 
"URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO", 
"VAN BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO", 
"VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA", 
"VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH", 
"W TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN", 
"WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG", 
"WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA", 
"WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE", 
"WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER", 
"WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO", 
"WEST CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN", 
"WEST HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH", 
"WEST POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD", 
"WEST VALLEY CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER", 
"WESTMORELAND CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD", 
"WHARTON", "WHEELING", "WHITE PLAINS", "WHITEHALL", "WHITESTONE", 
"WHITESTOWN", "WHITING", "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD", 
"WILLARD", "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK", 
"WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS", 
"WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON", 
"WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE", 
"WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS", 
"WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH", 
"YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS", 
"YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor")), row.names = c(NA, 
-14L), class = "data.frame")






Proprietary

NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}

______________________________________________
mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=N45-UMvb1jEdpZi0xmhOt4rv3lQf2DBkFap0cMXXvWc&s=2Nwt1YXA0urvwXy8m_Yq8Xlrvn-hfAQmjKk7zXmij-c&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=N45-UMvb1jEdpZi0xmhOt4rv3lQf2DBkFap0cMXXvWc&s=JdUwH82y8yFXsbRMGG_dwcWUBl8gzmy3XPjpkCeleSQ&e=
and provide commented, minimal, self-contained, reproducible code.



-- 
Ben Tupper
Bigelow Laboratory for Ocean Science
East Boothbay, Maine
https://urldefense.proofpoint.com/v2/url?u=http-3A__www.bigelow.org_&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=N45-UMvb1jEdpZi0xmhOt4rv3lQf2DBkFap0cMXXvWc&s=oRaY1hdV6ioa7STQkpTvF_HTzJkFCnJPKJo72SdTbCs&e=
https://urldefense.proofpoint.com/v2/url?u=https-3A__eco.bigelow.org&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=N45-UMvb1jEdpZi0xmhOt4rv3lQf2DBkFap0cMXXvWc&s=wXV4OHk998Q3mHGu8FvYXE5kZerieouEHZ3_1pU5YCM&e=


Proprietary

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From chr|@ho|d @end|ng |rom p@yctc@org  Tue May 19 16:08:15 2020
From: chr|@ho|d @end|ng |rom p@yctc@org (Chris Evans)
Date: Tue, 19 May 2020 15:08:15 +0100 (BST)
Subject: [R] Help with ggplot error: #Error in FUN(X[[i]],
 ...) : object 'x' not found
In-Reply-To: <BYAPR06MB53836469C280B465C60075C1AEB90@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB53835FCA0A4E629E6C4048EAAEB90@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CALrbzg1bPZGtUK9BYAZUWQB3pFE_C-783xmDR=Zf7AfgFaVROQ@mail.gmail.com>
 <BYAPR06MB53836469C280B465C60075C1AEB90@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <1568383192.20416580.1589897295511.JavaMail.zimbra@psyctc.org>



----- Original Message -----
> From: "Poling, William via R-help" <r-help at r-project.org>
> To: "Ben Tupper" <btupper at bigelow.org>
> Cc: r-help at r-project.org
> Sent: Tuesday, 19 May, 2020 15:04:25
> Subject: Re: [R] Help with ggplot error: #Error in FUN(X[[i]], ...) : object 'x' not found

> Hello Ben and thank you for your response.
> 
> I thought that was what I was doing when I ran this version:
> Test1
> p <- ggplot(c1members2, aes(x,y)) +
>  geom_point()
> 
> p +  geom_line(data = paths, aes(group = trip_id),inherit.aes = FALSE) + #Error
> in FUN(X[[i]], ...) : object 'x' not found
>  ggtitle(paste0("Optimal route with cost: ", round(objective_value(result), 2)))
> 
> Is this not what you mean?


I'm no expert but isn't "inherit.aes = FALSE" the culprit?  I think that means that that line only has a group for the aesthetic as you've 
turned off inheritance of the x and y mapping in the earlier aes().  That would seem to fit with the error message.

No time to try it all out but thought I'd throw this in.

Good luck (all!)

Chris

> 
> str(p)
> List of 9
> $ data       :'data.frame':	7 obs. of  3 variables:
>  ..$ id: int [1:7] 1 2 3 4 5 6 7
>  ..$ x : num [1:7] 95.5 90.2 46.8 40.9 43.3 ...
>  ..$ y : num [1:7] 4.65 47.57 8.07 38.43 28.59 ...
> $ layers     :List of 1
>  ..$ :Classes 'LayerInstance', 'Layer', 'ggproto', 'gg' <ggproto object: Class
>  LayerInstance, Layer, gg>
>    aes_params: list
>    compute_aesthetics: function
>    compute_geom_1: function
>    compute_geom_2: function
>    compute_position: function
>    compute_statistic: function
>    data: waiver
>    draw_geom: function
>    finish_statistics: function
>    geom: <ggproto object: Class GeomPoint, Geom, gg>
>        aesthetics: function
>        default_aes: uneval
>        draw_group: function
>        draw_key: function
>        draw_layer: function
>        draw_panel: function
>        extra_params: na.rm
>        handle_na: function
>        non_missing_aes: size shape colour
>        optional_aes:
>        parameters: function
>        required_aes: x y
>        setup_data: function
>        setup_params: function
>        use_defaults: function
>        super:  <ggproto object: Class Geom, gg>
>    geom_params: list
>    inherit.aes: TRUE
>    layer_data: function
>    map_statistic: function
>    mapping: NULL
>    position: <ggproto object: Class PositionIdentity, Position, gg>
>        compute_layer: function
>        compute_panel: function
>        required_aes:
>        setup_data: function
>        setup_params: function
>        super:  <ggproto object: Class Position, gg>
>    print: function
>    setup_layer: function
>    show.legend: NA
>    stat: <ggproto object: Class StatIdentity, Stat, gg>
>        aesthetics: function
>        compute_group: function
>        compute_layer: function
>        compute_panel: function
>        default_aes: uneval
>        extra_params: na.rm
>        finish_layer: function
>        non_missing_aes:
>        parameters: function
>        required_aes:
>        retransform: TRUE
>        setup_data: function
>        setup_params: function
>        super:  <ggproto object: Class Stat, gg>
>    stat_params: list
>    super:  <ggproto object: Class Layer, gg>
> $ scales     :Classes 'ScalesList', 'ggproto', 'gg' <ggproto object: Class
> ScalesList, gg>
>    add: function
>    clone: function
>    find: function
>    get_scales: function
>    has_scale: function
>    input: function
>    n: function
>    non_position_scales: function
>    scales: list
>    super:  <ggproto object: Class ScalesList, gg>
> $ mapping    :List of 2
>  ..$ x: language ~x
>  .. ..- attr(*, ".Environment")=<environment: R_GlobalEnv>
>  ..$ y: language ~y
>  .. ..- attr(*, ".Environment")=<environment: R_GlobalEnv>
>  ..- attr(*, "class")= chr "uneval"
> $ theme      : list()
> $ coordinates:Classes 'CoordCartesian', 'Coord', 'ggproto', 'gg' <ggproto
> object: Class CoordCartesian, Coord, gg>
>    aspect: function
>    backtransform_range: function
>    clip: on
>    default: TRUE
>    distance: function
>    expand: TRUE
>    is_free: function
>    is_linear: function
>    labels: function
>    limits: list
>    modify_scales: function
>    range: function
>    render_axis_h: function
>    render_axis_v: function
>    render_bg: function
>    render_fg: function
>    setup_data: function
>    setup_layout: function
>    setup_panel_guides: function
>    setup_panel_params: function
>    setup_params: function
>    train_panel_guides: function
>    transform: function
>    super:  <ggproto object: Class CoordCartesian, Coord, gg>
> $ facet      :Classes 'FacetNull', 'Facet', 'ggproto', 'gg' <ggproto object:
> Class FacetNull, Facet, gg>
>    compute_layout: function
>    draw_back: function
>    draw_front: function
>    draw_labels: function
>    draw_panels: function
>    finish_data: function
>    init_scales: function
>    map_data: function
>    params: list
>    setup_data: function
>    setup_params: function
>    shrink: TRUE
>    train_scales: function
>    vars: function
>    super:  <ggproto object: Class FacetNull, Facet, gg>
> $ plot_env   :<environment: R_GlobalEnv>
> $ labels     :List of 2
>  ..$ x: chr "x"
>  ..$ y: chr "y"
> - attr(*, "class")= chr [1:2] "gg" "ggplot"
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> From: Ben Tupper <btupper at bigelow.org>
> Sent: Tuesday, May 19, 2020 7:46 AM
> To: Poling, William <PolingW at aetna.com>
> Cc: r-help at r-project.org
> Subject: [EXTERNAL] Re: [R] Help with ggplot error: #Error in FUN(X[[i]], ...) :
> object 'x' not found
> 
> **** External Email - Use Caution ****
> Hi,
> 
> When you add a new layer to a plot and you provide new data, wouldn't it make
> sense to provide aes() with some indication of the names of the variables to
> use for coordinate plotting?? You have provided a grouping variable for the
> paths, but what of coordinates x and y?? How could geom_path() know that your
> lon/lat variables are to be used as x and y coordinates in the plot?
> 
> Cheers,
> Ben
> 
> 
> 
> On Tue, May 19, 2020 at 8:29 AM Poling, William via R-help
> <mailto:r-help at r-project.org> wrote:
> #RStudio Version Version 1.2.1335
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit)
> #Running under: Windows 10 x64 (build 17763)
> 
> Good morning.
> 
> I am testing a small sample of my data using an example found here:
> #https://urldefense.proofpoint.com/v2/url?u=https-3A__www.r-2Dorms.org_mixed-2Dinteger-2Dlinear-2Dprogramming_practicals_problem-2Dtsp_&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=N45-UMvb1jEdpZi0xmhOt4rv3lQf2DBkFap0cMXXvWc&s=mETBFqF-RTLN5awPNRy_CxYYqxMrEu99FK68PoNMKq0&e=
> 
> If needed I can provide my complete script which is simply the same as the
> example only using a few rows from my data, let me know, as I think this is
> just a ggplot issue.
> 
> I am getting this error when I use ggplot
> #Error in FUN(X[[i]], ...) : object 'x' not found
> 
> 1. I have reviewed the following with no success.
> #Sources referred to for help
> #https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_business-2Dscience_anomalize_issues_2&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=N45-UMvb1jEdpZi0xmhOt4rv3lQf2DBkFap0cMXXvWc&s=wXtp8KhCuHjmSyGkQ4ff3vFbBf2Ve4GawN3qTE2lK_Q&e=
> #https://urldefense.proofpoint.com/v2/url?u=https-3A__stackoverflow.com_questions_38988028_error-2Din-2Dfunxi-2Dobject-2Dx-2Dnot-2Dfound-23https-3A__community.rstudio.com_t_error-2Din-2Dfun-2Dx-2Di-2Dobject-2Dvariable-2Dnot-2Dfound_62532_3-2523https-3A__www.neonscience.org_packages-2Din-2Dr&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=N45-UMvb1jEdpZi0xmhOt4rv3lQf2DBkFap0cMXXvWc&s=_Z-vYnJW4sxansRa74mUatDLP6O0k5BB55JadgWCIyM&e=
> 
> 2. I have rebooted and re-installed ggplot2 for updates
> 
> 3. These are the only libraries running
> library(knitr)
> library(dplyr)
> library(ggplot2)
> library(ggmap)
> library(ompr)
> library(ompr.roi)
> library(ROI.plugin.glpk)
> 
> Here is the original plot attempt:
> ggplot(c1members2, aes(x , y)) +
>? geom_point() +
>? geom_line(data = paths, aes(group = trip_id)) + #Error in FUN(X[[i]], ...) :
>? object 'x' not found
>? ggtitle(paste0("Optimal route with cost: ", round(objective_value(result), 2)))
> 
> #I have gone through each of the steps one by one and error occurs at -->
> geom_line(data = paths, aes(group = trip_id))
> 
> I have also tried the following:
> Test1
> p <- ggplot(c1members2, aes(x,y)) +
>? geom_point()
> 
> p +? geom_line(data = paths, aes(group = trip_id),inherit.aes = FALSE) + #Error
> in FUN(X[[i]], ...) : object 'x' not found
>? ggtitle(paste0("Optimal route with cost: ", round(objective_value(result), 2)))
> 
> Test2
> ggplot(c1members2, aes(x,y)) +
>? geom_point() +
> geom_line(data = paths, aes(group = trip_id),inherit.aes = TRUE) + #Error in
> FUN(X[[i]], ...) : object 'x' not found
>? ggtitle(paste0("Optimal route with cost: ", round(objective_value(result), 2)))
> 
> Here are the data involved.
> str(c1members2)
> 'data.frame':? ?7 obs. of? 3 variables:
>?$ id: int? 1 2 3 4 5 6 7
>?$ x : num? 95.5 90.2 46.8 40.9 43.3 ...
>?$ y : num? 4.65 47.57 8.07 38.43 28.59 ...
> 
> str(paths)
> data.frame':? ? 14 obs. of? 6 variables:
>?$ trip_id? : int? 1 2 3 4 5 6 7 1 2 3 ...
>?$ property : chr? "from" "from" "from" "from" ...
>?$ idx_val? : int? 3 1 5 7 6 4 2 1 2 3 ...
>?$ Longitude: num? -122 -122 -122 -122 -122 ...
>?$ Latitude : num? 47.3 47.2 47.3 47.6 47.6 ...
>?$ city? ? ?: Factor w/ 1337 levels "ABBOTTSTOWN",..: 1169 1169 1169 1069 1069
>?1069 1069 1169 1069 1169 ...
> 
> Here are the dput()'s
> 
> I hope I have provided enough information to elicit a response.
> 
> Thank you.
> 
> WHP
> 
> dput(c1members2)
> structure(list(id = 1:7, x = c(95.521828085515, 90.2272877161736,
> 46.8465689660691, 40.8959155605829, 43.2591531526978, 23.7485886058345,
> 64.0406297698986), y = c(4.64803485876031, 47.5678629996171,
> 8.0689847051367, 38.4256264258875, 28.5930051512296, 43.5934562642672,
> 42.415566619041)), class = "data.frame", row.names = c(NA, -7L
> 
> dput(paths) #Although I am only using city %in% c("SEATTLE","TACOMA")) from the
> original 2353 rows = 7 rows in c1members2
> structure(list(trip_id = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 1L, 2L,
> 3L, 4L, 5L, 6L, 7L), property = c("from", "from", "from", "from",
> "from", "from", "from", "to", "to", "to", "to", "to", "to", "to"
> ), idx_val = c(3L, 1L, 5L, 7L, 6L, 4L, 2L, 1L, 2L, 3L, 4L, 5L,
> 6L, 7L), Longitude = c(-122.490417, -122.429323, -122.358115,
> -122.37003, -122.41282, -122.299261, -122.390815, -122.429323,
> -122.390815, -122.490417, -122.299261, -122.358115, -122.41282,
> -122.37003), Latitude = c(47.274599, 47.176596, 47.276776, 47.565667,
> 47.646222, 47.673056, 47.549377, 47.176596, 47.549377, 47.274599,
> 47.673056, 47.276776, 47.646222, 47.565667), city = structure(c(1169L,
> 1169L, 1169L, 1069L, 1069L, 1069L, 1069L, 1169L, 1069L, 1169L,
> 1069L, 1169L, 1069L, 1069L), .Label = c("ABBOTTSTOWN", "ABILENE",
> "ACWORTH", "ADAMS", "ADDISON", "ADKINS", "ALBANY", "ALBIA", "ALBUQUERQUE",
> "ALEXANDRIA", "ALFRED", "ALIQUIPPA", "ALISO VIEJO", "ALLEN PARK",
> "ALLENTOWN", "ALPHA", "ALPHARETTA", "ALPINE", "ALTOONA", "AMARILLO",
> "AMBLER", "AMBRIDGE", "AMHERST", "AMITYVILLE", "ANAHIEM", "ANDERSON",
> "ANDOVER", "ANGIER", "ANGLETON", "ANKENY", "ANN ARBOR", "ANZA",
> "APEX", "APOLLO", "ARCADIA", "ARCHDALE", "ARDMORE", "ARGYLE",
> "ARKANSAS CITY", "ARLINGTON", "ARNOLD", "ARTHUR", "ASHEBORO",
> "ASHEVILLE", "ASHLAND", "ASHLAND CITY", "ASHTON", "ASTON", "ATHENS",
> "ATLANTA", "ATLANTIC BCH", "AUBORN", "AUGUSTA", "AURORA", "AUSTELL",
> "AUSTIN", "AVENTURA", "AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN CITY",
> "BALDWIN PLACE", "BALLWIN", "BANGOR", "BARBOURSVILLE", "BARNEGAT",
> "BARTLETT", "BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", "BATESBURG",
> "BATON ROUGE", "BAXTER SPRINGS", "BAY CITY", "BAYONNE", "BAYSIDE",
> "BAYTOWN", "BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER CITY",
> "BELFAIR", "BELLE VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR",
> "BELLMORE", "BELLWOOD", "BELMAR", "BELMONT", "BELVIDERE", "BENNET",
> "BENTON", "BENTONVILLE", "BERLIN", "BESSEMER CITY", "BETHANY",
> "BETHEL", "BETHEL PARK", "BETHESDA", "BETHLEHEM", "BETTENDORF",
> "BIGLERVILLE", "BINGHAMTON", "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN",
> "BLACKSBURG", "BLAINE", "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD",
> "BLUE BELL", "BLUE GRASS", "BLUE POINT", "BLUE SPRINGS", "BOCA RATON",
> "BODFISH", "BOILING SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS",
> "BONNEY LAKE", "BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING GREEN",
> "BOYERTOWN", "BOYNTON BEACH", "BRADENTON", "BRADENVILLE", "BRANDAMORE",
> "BRANDON", "BRANFORD", "BRANSON", "BRASELTON", "BREEZEWOOD",
> "BRENTON", "BRENTWOOD", "BREWSTER", "BRICK", "BRIDGEPORT", "BRIDGETON",
> "BRIGHAM CITY", "BROADDUS", "BROCKPORT", "BROGUE", "BROKEN ARROW",
> "BRONX", "BROOKFIELD", "BROOKHAVEN", "BROOKLYN", "BROWNS MILLS",
> "BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", "BRYAN", "BUCKSPORT",
> "BUDD LAKE", "BUFFALO", "BULVERDE", "BUNKERVILLE", "BURBANK",
> "BURFORD", "BURIEN", "BURLINGTON", "BURR RIDGE", "BURTON", "BUSKIRK",
> "BUTLER", "BUXTON", "BYRON", "CALERA", "CAMBRIDGE", "CAMP HILL",
> "CAMPBELL", "CANAAN", "CANAL WINCHESTER", "CANASTOTA", "CANTON",
> "CARDIFF BY THE SEA", "CARL JUNCTION", "CARLISLE", "CARLTON",
> "CAROL STREAM", "CARROLLTON", "CARSON CITY", "CARTERSVILLE",
> "CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", "CASWELL", "CATO",
> "CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER POINT", "CENTERTON",
> "CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN FALLS", "CHALFONT",
> "CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", "CHARLESTON",
> "CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", "CHESHIRE",
> "CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", "CHILLICOTHE",
> "CHOCTAW", "CICERO", "CINCINNATI", "CLAY", "CLEARWATER", "CLEARWATER BEACH",
> "CLEMMONS", "CLENDENIN", "CLEVELAND", "CLEVELAND HTS", "CLEVES",
> "CLIFFSIDE PARK", "CLIFTON", "CLIFTON PARK", "CLIFTON SPGS",
> "CLINTON", "COACHELLA", "COAL TOWNSHIP", "COATESVILLE", "COLLEGE STATION",
> "COLLINSVILLE", "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE",
> "CONCORD", "CONCORD TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY",
> "CONYERS", "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA",
> "CORNELIUS", "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY",
> "COTTAGE HILLS", "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN",
> "CRANBERRY TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER",
> "CUMBERLAND FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS",
> "DALLAS", "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT",
> "DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR",
> "DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH",
> "DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY",
> "DES MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER",
> "DIXON", "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER",
> "DOYLESTOWN", "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH",
> "DUNCAN", "DUNCANNON", "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE",
> "DURHAM", "DUTTON", "EAGLE GROVE", "EAST AURORA", "EAST CHICAGO",
> "EAST EARL", "EAST HANOVER", "EAST HARTFORD", "EAST HARTLAND",
> "EAST LEROY", "EAST LIVERPOOL", "EAST MEADOW", "EAST NORTHPORT",
> "EAST ORANGE", "EAST PEORIA", "EAST SAINT LOUIS", "EASTON", "EDGERTON",
> "EDISON", "EDMOND", "EGG HBR TWP", "EL DORADO", "EL PASO", "ELDRIDGE",
> "ELGIN", "ELIZABETHVILLE", "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY",
> "ELMHURST", "ELMWOOD PARK", "ELOY", "ELY", "EMERSON", "EMORY",
> "ENDICOTT", "ENGLEWOOD", "ENID", "ENNICE", "ENOLA", "ENUMCLAW",
> "EPHRATA", "ERIE", "ESTERO", "EUREKA SPRINGS", "EVANSVILLE",
> "EVERETT", "EXCELSIOR SPRINGS", "EXTON", "FAIRFAX", "FAIRFIELD",
> "FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", "FALMOUTH", "FAR HILLS",
> "FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", "FAYETTE", "FAYETTEVILLE",
> "FELTON", "FENTON", "FERGUSON", "FIELDALE", "FINDLAY", "FINGERVILLE",
> "FLEMINGTON", "FLINT", "FLORENCE", "FLORESVILLE", "FLORISSANT",
> "FLOVILLA", "FLOWER MOUND", "FLUSHING", "FOGELSVILLE", "FOLSOM",
> "FONTANELLE", "FOREST HILLS", "FOREST PARK", "FORT DODGE", "FORT FAIRFIELD",
> "FORT GAY", "FORT LAUDERDALE", "FORT LEE", "FORT MILL", "FORT MOHAVE",
> "FORT PAYNE", "FORT PIERCE", "FORT SMITH", "FORT WAYNE", "FORT WORTH",
> "FOSTORIA", "FOUNTAIN INN", "FRANKFORT", "FRANKLIN", "FREEDOM",
> "FREEHOLD", "FREEPORT", "FREMONT", "FULLERTON", "FULSHEAR", "FULTON",
> "GADSDEN", "GAFFNEY", "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE",
> "GARDNERS", "GARNETT VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA",
> "GETTYSBURG", "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE",
> "GLEN CARBON", "GLEN COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW",
> "GLENWOOD", "GOLDEN CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM",
> "GOSHEN", "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS",
> "GRANDVIEW", "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS",
> "GRASS VALLEY", "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER",
> "GREENLAWN", "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE",
> "GREER", "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD",
> "HADLEY", "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON",
> "HAMBURG", "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE",
> "HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON",
> "HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL",
> "HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE",
> "HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH",
> "HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE",
> "HIGH HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS",
> "HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE",
> "HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD",
> "HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS",
> "HOT SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON",
> "HUFFMAN", "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON",
> "HUNTINGTON BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE",
> "HURST", "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL",
> "INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY",
> "IRMO", "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP",
> "ISSAQUAH", "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS",
> "JACKSONVILLE", "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE",
> "JEFFERSON", "JEFFERSON CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN",
> "JONESBORO", "JONESPORT", "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE",
> "KANNAPOLIS", "KANSAS CITY", "KATY", "KEARNEY", "KELLERTON",
> "KEMAH", "KENMORE", "KENNESAW", "KENT", "KINGMAN", "KINGS MOUNTAIN",
> "KINGSLAND", "KINGSTON", "KINGWOOD", "KIRKLAND", "KUTZTOWN",
> "LA QUINTA", "LA RUE", "LA VERNE", "LAFAYETTE", "LAKE BLUFF",
> "LAKE IN THE HILL", "LAKE ISABELLA", "LAKE PLACID", "LAKE SAINT LOUIS",
> "LAKE TAPPS", "LAKE VILLAGE", "LAKE WAUKOMIS", "LAKE WORTH",
> "LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD RCH", "LAMBERTVILLE",
> "LANCASTER", "LANDISVILLE", "LANSDALE", "LAREDO", "LARGO", "LAS VEGAS",
> "LATROBE", "LAUDERHILL", "LAWNDALE", "LAWRENCE", "LAWRENCEVILLE",
> "LE MARS", "LE ROY", "LEAGUE CITY", "LEAVENWORTH", "LEAWOOD",
> "LEBANON", "LECOMPTON", "LEECHBURG", "LEES SUMMIT", "LEESBURG",
> "LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", "LEVITTOWN", "LEWES",
> "LEWISTOWN", "LEXINGTON", "LIBERTY TWP", "LILLINGTON", "LINCOLN",
> "LINDENHURST", "LINTON", "LISBON", "LITCHFIELD PK", "LITHIA",
> "LITITZ", "LITTLE EGG HARBOR TO", "LK FOREST PK", "LK PEEKSKILL",
> "LOCKHART", "LOCKWOOD", "LOGAN", "LOMA LINDA", "LONE JACK", "LONG BEACH",
> "LONG LANE", "LONGVIEW", "LORAIN", "LORTON", "LOS ANGELES", "LOUISBURG",
> "LOVES PARK", "LOWER BURRELL", "LOWRY CITY", "LUBBOCK", "LUGOFF",
> "LUMBERTON", "LYNBROOK", "LYNNWOOD", "MACHESNEY PARK", "MACON",
> "MAGNOLIA", "MAHOPAC", "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE",
> "MANASSAS", "MANCHACA", "MANCHESTER", "MANCHESTER TOWNSHIP",
> "MANCHESTER TW", "MANHEIM", "MANITO", "MANLIUS", "MANNINGTON",
> "MANSFIELD", "MANSURA", "MAPLE PARK", "MARBLE FALLS", "MARCELLUS",
> "MARICOPA", "MARIETTA", "MARION", "MARKLEYSBURG", "MARS HILL",
> "MARSHALLTOWN", "MARSHFIELD", "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE",
> "MASON CITY", "MASSEY", "MASSILLON", "MASURY", "MATAWAN", "MATEWAN",
> "MATTAPOISETT", "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND",
> "MC DONALD", "MC KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN",
> "MCALESTER", "MCDONOUGH", "MCKEESPORT", "MCKINNEY", "MECHANICSBURG",
> "MECHANICSVILLE", "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS",
> "MERIDEN", "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA",
> "MESQUITE", "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN",
> "MIDLAND", "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK",
> "MILLEDGEVILLE", "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE",
> "MINEOLA", "MINERAL SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION",
> "MISSION HILLS", "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY",
> "MOLINE", "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE",
> "MONTCLAIR", "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON",
> "MORGANTOWN", "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE",
> "MOUNT JOY", "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO",
> "MOUNT UNION", "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT",
> "MULVANE", "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT",
> "MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS",
> "NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES",
> "NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG",
> "NEW BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK",
> "NEW CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON",
> "NEW LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE",
> "NEW ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK",
> "NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT",
> "NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA",
> "NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK",
> "NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON",
> "NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND HILLS",
> "NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE",
> "NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON",
> "OAK RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE",
> "OMAHA", "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION",
> "ORONO", "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO",
> "OTTAWA", "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD",
> "OYSTER BAY", "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR",
> "PALMDALE", "PALMYRA", "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE",
> "PARLIN", "PARMA", "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY",
> "PAXTON", "PEA RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN",
> "PELION", "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE",
> "PENNSBURG", "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS",
> "PERRYSBURG", "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI",
> "PHOENIX", "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN",
> "PINE BLUFF", "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY",
> "PITMAN", "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY",
> "PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO LAKE",
> "POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", "PORT CHARLOTTE",
> "PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", "PORT WASHINGTON",
> "PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", "POTTSVILLE",
> "POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE VILLAGE",
> "PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT PLEASANT",
> "PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", "QUINCY",
> "RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", "READLYN",
> "RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO PARK",
> "REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", "RICH HILL",
> "RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", "RINCON",
> "RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", "RIVERSIDE",
> "ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", "ROCKAWAY BCH",
> "ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", "ROGERS",
> "ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE",
> "ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT",
> "ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN",
> "RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO",
> "SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR",
> "SAINT CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS",
> "SAINT PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY",
> "SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO",
> "SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN",
> "SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", "SARASOTA",
> "SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", "SAYVILLE",
> "SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", "SCOTT", "SCOTTSDALE",
> "SEAFORD", "SEALE", "SEATTLE", "SEDONA", "SEGUIN", "SELDEN",
> "SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", "SHARON", "SHARPSBURG",
> "SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", "SHELLSBURG", "SHELTON",
> "SHENANDOAH", "SHORELINE", "SHOREWOOD", "SHREVEPORT", "SICKLERVILLE",
> "SILER CITY", "SILOAM SPRINGS", "SIMON", "SIMPSONVILLE", "SIMSBURY",
> "SIOUX CITY", "SIOUX FALLS", "SKOKIE", "SKOWHEGAN", "SLATINGTON",
> "SLIDELL", "SMITHFIELD", "SN BERNRDNO", "SNELLVILLE", "SOCORRO",
> "SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH BELOIT", "SOUTH BEND",
> "SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH PARK", "SOUTH PLAINFIELD",
> "SOUTH SIOUX CITY", "SOUTHAMPTON", "SOUTHBURY", "SOUTHGATE",
> "SPARKS", "SPARROW BUSH", "SPENCER", "SPRING", "SPRING BRANCH",
> "SPRING GROVE", "SPRING HILL", "SPRING HOPE", "SPRING VALLEY",
> "SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD GARDENS", "SPRINGHILL",
> "SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", "ST PETERSBURG",
> "ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", "STANLEY",
> "STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", "STILLMAN VALLEY",
> "STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", "STRAFFORD",
> "STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", "SUFFIELD",
> "SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", "SUMMIT",
> "SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", "SWEENY",
> "SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", "TAFTVILLE",
> "TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", "TAYLORVILLE",
> "TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", "THOMASVILLE",
> "TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY PARK",
> "TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", "TOPEKA",
> "TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS RST",
> "TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER",
> "TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY",
> "UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA",
> "URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO",
> "VAN BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO",
> "VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA",
> "VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH",
> "W TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN",
> "WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG",
> "WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA",
> "WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE",
> "WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER",
> "WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO",
> "WEST CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN",
> "WEST HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH",
> "WEST POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD",
> "WEST VALLEY CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER",
> "WESTMORELAND CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD",
> "WHARTON", "WHEELING", "WHITE PLAINS", "WHITEHALL", "WHITESTONE",
> "WHITESTOWN", "WHITING", "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD",
> "WILLARD", "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK",
> "WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS",
> "WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON",
> "WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE",
> "WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS",
> "WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH",
> "YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS",
> "YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor")),
> row.names = c(NA,
> -14L), class = "data.frame")
> 
> 
> 
> 
> 
> 
> Proprietary
> 
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}
> 
> ______________________________________________
> mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=N45-UMvb1jEdpZi0xmhOt4rv3lQf2DBkFap0cMXXvWc&s=2Nwt1YXA0urvwXy8m_Yq8Xlrvn-hfAQmjKk7zXmij-c&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=N45-UMvb1jEdpZi0xmhOt4rv3lQf2DBkFap0cMXXvWc&s=JdUwH82y8yFXsbRMGG_dwcWUBl8gzmy3XPjpkCeleSQ&e=
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 
> --
> Ben Tupper
> Bigelow Laboratory for Ocean Science
> East Boothbay, Maine
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.bigelow.org_&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=N45-UMvb1jEdpZi0xmhOt4rv3lQf2DBkFap0cMXXvWc&s=oRaY1hdV6ioa7STQkpTvF_HTzJkFCnJPKJo72SdTbCs&e=
> https://urldefense.proofpoint.com/v2/url?u=https-3A__eco.bigelow.org&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=N45-UMvb1jEdpZi0xmhOt4rv3lQf2DBkFap0cMXXvWc&s=wXV4OHk998Q3mHGu8FvYXE5kZerieouEHZ3_1pU5YCM&e=
> 
> 
> Proprietary
> 
> NOTICE TO RECIPIENT OF INFORMATION:
> This e-mail may contain confidential or privileged information. If you think you
> have received this e-mail in error, please advise the sender by reply e-mail
> and then delete this e-mail immediately.
> This e-mail may also contain protected health information (PHI) with information
> about sensitive medical conditions, including, but not limited to, treatment
> for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This
> type of information may be protected by various federal and/or state laws which
> prohibit any further disclosure without the express written consent of the
> person to whom it pertains or as otherwise permitted by law. Any unauthorized
> further disclosure may be considered a violation of federal and/or state law. A
> general authorization for the release of medical or other information may NOT
> be sufficient consent for release of this type of information.
> Thank you. Aetna
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Small contribution in our coronavirus rigours: 
https://www.coresystemtrust.org.uk/home/free-options-to-replace-paper-core-forms-during-the-coronavirus-pandemic/

Chris Evans <chris at psyctc.org> Visiting Professor, University of Sheffield <chris.evans at sheffield.ac.uk>
I do some consultation work for the University of Roehampton <chris.evans at roehampton.ac.uk> and other places
but <chris at psyctc.org> remains my main Email address.  I have a work web site at:
   https://www.psyctc.org/psyctc/
and a site I manage for CORE and CORE system trust at:
   http://www.coresystemtrust.org.uk/
I have "semigrated" to France, see: 
   https://www.psyctc.org/pelerinage2016/semigrating-to-france/ 
   https://www.psyctc.org/pelerinage2016/register-to-get-updates-from-pelerinage2016/

If you want an Emeeting, I am trying to keep them to Thursdays and my diary is at:
   https://www.psyctc.org/pelerinage2016/ceworkdiary/
Beware: French time, generally an hour ahead of UK.


From Po||ngW @end|ng |rom @etn@@com  Tue May 19 16:10:36 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Tue, 19 May 2020 14:10:36 +0000
Subject: [R] Help with ggplot error: #Error in FUN(X[[i]],
 ...) : object 'x' not found
In-Reply-To: <1568383192.20416580.1589897295511.JavaMail.zimbra@psyctc.org>
References: <BYAPR06MB53835FCA0A4E629E6C4048EAAEB90@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CALrbzg1bPZGtUK9BYAZUWQB3pFE_C-783xmDR=Zf7AfgFaVROQ@mail.gmail.com>
 <BYAPR06MB53836469C280B465C60075C1AEB90@BYAPR06MB5383.namprd06.prod.outlook.com>
 <1568383192.20416580.1589897295511.JavaMail.zimbra@psyctc.org>
Message-ID: <BYAPR06MB5383F1FDE4533424B21F7124AEB90@BYAPR06MB5383.namprd06.prod.outlook.com>

Hello Chris, and thank you for your response.
I tried it both TRUE and FALSE given my confusion with the reference I found that idea in.

It did not help either way.

Glad you noticed though, thank you.

WHP

William H. Poling Ph.D., MPH? |?Senior Data Scientist, Medicare Stars, CVS Health 
p?813-777-5030


Proprietary

-----Original Message-----
From: Chris Evans <chrishold at psyctc.org> 
Sent: Tuesday, May 19, 2020 9:08 AM
To: Poling, William <PolingW at aetna.com>
Cc: Ben Tupper <btupper at bigelow.org>; r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with ggplot error: #Error in FUN(X[[i]], ...) : object 'x' not found

**** External Email - Use Caution ****



----- Original Message -----
> From: "Poling, William via R-help" <r-help at r-project.org>
> To: "Ben Tupper" <btupper at bigelow.org>
> Cc: r-help at r-project.org
> Sent: Tuesday, 19 May, 2020 15:04:25
> Subject: Re: [R] Help with ggplot error: #Error in FUN(X[[i]], ...) : 
> object 'x' not found

> Hello Ben and thank you for your response.
> 
> I thought that was what I was doing when I ran this version:
> Test1
> p <- ggplot(c1members2, aes(x,y)) +
>  geom_point()
> 
> p +  geom_line(data = paths, aes(group = trip_id),inherit.aes = FALSE) 
> + #Error in FUN(X[[i]], ...) : object 'x' not found  
> ggtitle(paste0("Optimal route with cost: ", 
> round(objective_value(result), 2)))
> 
> Is this not what you mean?


I'm no expert but isn't "inherit.aes = FALSE" the culprit?  I think that means that that line only has a group for the aesthetic as you've turned off inheritance of the x and y mapping in the earlier aes().  That would seem to fit with the error message.

No time to try it all out but thought I'd throw this in.

Good luck (all!)

Chris

> 
> str(p)
> List of 9
> $ data       :'data.frame':	7 obs. of  3 variables:
>  ..$ id: int [1:7] 1 2 3 4 5 6 7
>  ..$ x : num [1:7] 95.5 90.2 46.8 40.9 43.3 ...
>  ..$ y : num [1:7] 4.65 47.57 8.07 38.43 28.59 ...
> $ layers     :List of 1
>  ..$ :Classes 'LayerInstance', 'Layer', 'ggproto', 'gg' <ggproto 
> object: Class  LayerInstance, Layer, gg>
>    aes_params: list
>    compute_aesthetics: function
>    compute_geom_1: function
>    compute_geom_2: function
>    compute_position: function
>    compute_statistic: function
>    data: waiver
>    draw_geom: function
>    finish_statistics: function
>    geom: <ggproto object: Class GeomPoint, Geom, gg>
>        aesthetics: function
>        default_aes: uneval
>        draw_group: function
>        draw_key: function
>        draw_layer: function
>        draw_panel: function
>        extra_params: na.rm
>        handle_na: function
>        non_missing_aes: size shape colour
>        optional_aes:
>        parameters: function
>        required_aes: x y
>        setup_data: function
>        setup_params: function
>        use_defaults: function
>        super:  <ggproto object: Class Geom, gg>
>    geom_params: list
>    inherit.aes: TRUE
>    layer_data: function
>    map_statistic: function
>    mapping: NULL
>    position: <ggproto object: Class PositionIdentity, Position, gg>
>        compute_layer: function
>        compute_panel: function
>        required_aes:
>        setup_data: function
>        setup_params: function
>        super:  <ggproto object: Class Position, gg>
>    print: function
>    setup_layer: function
>    show.legend: NA
>    stat: <ggproto object: Class StatIdentity, Stat, gg>
>        aesthetics: function
>        compute_group: function
>        compute_layer: function
>        compute_panel: function
>        default_aes: uneval
>        extra_params: na.rm
>        finish_layer: function
>        non_missing_aes:
>        parameters: function
>        required_aes:
>        retransform: TRUE
>        setup_data: function
>        setup_params: function
>        super:  <ggproto object: Class Stat, gg>
>    stat_params: list
>    super:  <ggproto object: Class Layer, gg>
> $ scales     :Classes 'ScalesList', 'ggproto', 'gg' <ggproto object: Class
> ScalesList, gg>
>    add: function
>    clone: function
>    find: function
>    get_scales: function
>    has_scale: function
>    input: function
>    n: function
>    non_position_scales: function
>    scales: list
>    super:  <ggproto object: Class ScalesList, gg>
> $ mapping    :List of 2
>  ..$ x: language ~x
>  .. ..- attr(*, ".Environment")=<environment: R_GlobalEnv>  ..$ y: 
> language ~y  .. ..- attr(*, ".Environment")=<environment: R_GlobalEnv>
>  ..- attr(*, "class")= chr "uneval"
> $ theme      : list()
> $ coordinates:Classes 'CoordCartesian', 'Coord', 'ggproto', 'gg' 
> <ggproto
> object: Class CoordCartesian, Coord, gg>
>    aspect: function
>    backtransform_range: function
>    clip: on
>    default: TRUE
>    distance: function
>    expand: TRUE
>    is_free: function
>    is_linear: function
>    labels: function
>    limits: list
>    modify_scales: function
>    range: function
>    render_axis_h: function
>    render_axis_v: function
>    render_bg: function
>    render_fg: function
>    setup_data: function
>    setup_layout: function
>    setup_panel_guides: function
>    setup_panel_params: function
>    setup_params: function
>    train_panel_guides: function
>    transform: function
>    super:  <ggproto object: Class CoordCartesian, Coord, gg>
> $ facet      :Classes 'FacetNull', 'Facet', 'ggproto', 'gg' <ggproto object:
> Class FacetNull, Facet, gg>
>    compute_layout: function
>    draw_back: function
>    draw_front: function
>    draw_labels: function
>    draw_panels: function
>    finish_data: function
>    init_scales: function
>    map_data: function
>    params: list
>    setup_data: function
>    setup_params: function
>    shrink: TRUE
>    train_scales: function
>    vars: function
>    super:  <ggproto object: Class FacetNull, Facet, gg>
> $ plot_env   :<environment: R_GlobalEnv>
> $ labels     :List of 2
>  ..$ x: chr "x"
>  ..$ y: chr "y"
> - attr(*, "class")= chr [1:2] "gg" "ggplot"
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> From: Ben Tupper <btupper at bigelow.org>
> Sent: Tuesday, May 19, 2020 7:46 AM
> To: Poling, William <PolingW at aetna.com>
> Cc: r-help at r-project.org
> Subject: [EXTERNAL] Re: [R] Help with ggplot error: #Error in FUN(X[[i]], ...) :
> object 'x' not found
> 
> **** External Email - Use Caution **** Hi,
> 
> When you add a new layer to a plot and you provide new data, wouldn't 
> it make sense to provide aes() with some indication of the names of 
> the variables to use for coordinate plotting?? You have provided a 
> grouping variable for the paths, but what of coordinates x and y?? How 
> could geom_path() know that your lon/lat variables are to be used as x and y coordinates in the plot?
> 
> Cheers,
> Ben
> 
> 
> 
> On Tue, May 19, 2020 at 8:29 AM Poling, William via R-help 
> <mailto:r-help at r-project.org> wrote:
> #RStudio Version Version 1.2.1335
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit) #Running under: Windows 10 
> x64 (build 17763)
> 
> Good morning.
> 
> I am testing a small sample of my data using an example found here:
> #https://urldefense.proofpoint.com/v2/url?u=https-3A__www.r-2Dorms.org
> _mixed-2Dinteger-2Dlinear-2Dprogramming_practicals_problem-2Dtsp_&d=Dw
> MFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2
> mTpmTCtKvneM2ExlYvnUWbsByY&m=N45-UMvb1jEdpZi0xmhOt4rv3lQf2DBkFap0cMXXv
> Wc&s=mETBFqF-RTLN5awPNRy_CxYYqxMrEu99FK68PoNMKq0&e=
> 
> If needed I can provide my complete script which is simply the same as 
> the example only using a few rows from my data, let me know, as I 
> think this is just a ggplot issue.
> 
> I am getting this error when I use ggplot #Error in FUN(X[[i]], ...) : 
> object 'x' not found
> 
> 1. I have reviewed the following with no success.
> #Sources referred to for help
> #https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_busin
> ess-2Dscience_anomalize_issues_2&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0v
> yYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=N45-
> UMvb1jEdpZi0xmhOt4rv3lQf2DBkFap0cMXXvWc&s=wXtp8KhCuHjmSyGkQ4ff3vFbBf2V
> e4GawN3qTE2lK_Q&e= 
> #https://urldefense.proofpoint.com/v2/url?u=https-3A__stackoverflow.co
> m_questions_38988028_error-2Din-2Dfunxi-2Dobject-2Dx-2Dnot-2Dfound-23h
> ttps-3A__community.rstudio.com_t_error-2Din-2Dfun-2Dx-2Di-2Dobject-2Dv
> ariable-2Dnot-2Dfound_62532_3-2523https-3A__www.neonscience.org_packag
> es-2Din-2Dr&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j
> 7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=N45-UMvb1jEdpZi0xmhOt4rv3
> lQf2DBkFap0cMXXvWc&s=_Z-vYnJW4sxansRa74mUatDLP6O0k5BB55JadgWCIyM&e=
> 
> 2. I have rebooted and re-installed ggplot2 for updates
> 
> 3. These are the only libraries running
> library(knitr)
> library(dplyr)
> library(ggplot2)
> library(ggmap)
> library(ompr)
> library(ompr.roi)
> library(ROI.plugin.glpk)
> 
> Here is the original plot attempt:
> ggplot(c1members2, aes(x , y)) +
>? geom_point() +
>? geom_line(data = paths, aes(group = trip_id)) + #Error in FUN(X[[i]], ...) :
>? object 'x' not found
>? ggtitle(paste0("Optimal route with cost: ", 
>round(objective_value(result), 2)))
> 
> #I have gone through each of the steps one by one and error occurs at 
> --> geom_line(data = paths, aes(group = trip_id))
> 
> I have also tried the following:
> Test1
> p <- ggplot(c1members2, aes(x,y)) +
>? geom_point()
> 
> p +? geom_line(data = paths, aes(group = trip_id),inherit.aes = FALSE) 
>+ #Error  in FUN(X[[i]], ...) : object 'x' not found
>? ggtitle(paste0("Optimal route with cost: ", 
>round(objective_value(result), 2)))
> 
> Test2
> ggplot(c1members2, aes(x,y)) +
>? geom_point() +
> geom_line(data = paths, aes(group = trip_id),inherit.aes = TRUE) + 
>#Error in  FUN(X[[i]], ...) : object 'x' not found
>? ggtitle(paste0("Optimal route with cost: ", 
>round(objective_value(result), 2)))
> 
> Here are the data involved.
> str(c1members2)
> 'data.frame':? ?7 obs. of? 3 variables:
>?$ id: int? 1 2 3 4 5 6 7
>?$ x : num? 95.5 90.2 46.8 40.9 43.3 ...
>?$ y : num? 4.65 47.57 8.07 38.43 28.59 ...
> 
> str(paths)
> data.frame':? ? 14 obs. of? 6 variables:
>?$ trip_id? : int? 1 2 3 4 5 6 7 1 2 3 ...
>?$ property : chr? "from" "from" "from" "from" ...
>?$ idx_val? : int? 3 1 5 7 6 4 2 1 2 3 ...
>?$ Longitude: num? -122 -122 -122 -122 -122 ...
>?$ Latitude : num? 47.3 47.2 47.3 47.6 47.6 ...
>?$ city? ? ?: Factor w/ 1337 levels "ABBOTTSTOWN",..: 1169 1169 1169 
>1069 1069
>?1069 1069 1169 1069 1169 ...
> 
> Here are the dput()'s
> 
> I hope I have provided enough information to elicit a response.
> 
> Thank you.
> 
> WHP
> 
> dput(c1members2)
> structure(list(id = 1:7, x = c(95.521828085515, 90.2272877161736, 
> 46.8465689660691, 40.8959155605829, 43.2591531526978, 
> 23.7485886058345, 64.0406297698986), y = c(4.64803485876031, 
> 47.5678629996171, 8.0689847051367, 38.4256264258875, 28.5930051512296, 
> 43.5934562642672, 42.415566619041)), class = "data.frame", row.names = 
> c(NA, -7L
> 
> dput(paths) #Although I am only using city %in% c("SEATTLE","TACOMA")) 
> from the original 2353 rows = 7 rows in c1members2 
> structure(list(trip_id = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 1L, 2L, 3L, 4L, 
> 5L, 6L, 7L), property = c("from", "from", "from", "from", "from", 
> "from", "from", "to", "to", "to", "to", "to", "to", "to"
> ), idx_val = c(3L, 1L, 5L, 7L, 6L, 4L, 2L, 1L, 2L, 3L, 4L, 5L, 6L, 
> 7L), Longitude = c(-122.490417, -122.429323, -122.358115, -122.37003, 
> -122.41282, -122.299261, -122.390815, -122.429323, -122.390815, 
> -122.490417, -122.299261, -122.358115, -122.41282, -122.37003), 
> Latitude = c(47.274599, 47.176596, 47.276776, 47.565667, 47.646222, 
> 47.673056, 47.549377, 47.176596, 47.549377, 47.274599, 47.673056, 
> 47.276776, 47.646222, 47.565667), city = structure(c(1169L, 1169L, 
> 1169L, 1069L, 1069L, 1069L, 1069L, 1169L, 1069L, 1169L, 1069L, 1169L, 
> 1069L, 1069L), .Label = c("ABBOTTSTOWN", "ABILENE", "ACWORTH", 
> "ADAMS", "ADDISON", "ADKINS", "ALBANY", "ALBIA", "ALBUQUERQUE", 
> "ALEXANDRIA", "ALFRED", "ALIQUIPPA", "ALISO VIEJO", "ALLEN PARK", 
> "ALLENTOWN", "ALPHA", "ALPHARETTA", "ALPINE", "ALTOONA", "AMARILLO", 
> "AMBLER", "AMBRIDGE", "AMHERST", "AMITYVILLE", "ANAHIEM", "ANDERSON", 
> "ANDOVER", "ANGIER", "ANGLETON", "ANKENY", "ANN ARBOR", "ANZA", 
> "APEX", "APOLLO", "ARCADIA", "ARCHDALE", "ARDMORE", "ARGYLE", 
> "ARKANSAS CITY", "ARLINGTON", "ARNOLD", "ARTHUR", "ASHEBORO", 
> "ASHEVILLE", "ASHLAND", "ASHLAND CITY", "ASHTON", "ASTON", "ATHENS", 
> "ATLANTA", "ATLANTIC BCH", "AUBORN", "AUGUSTA", "AURORA", "AUSTELL", 
> "AUSTIN", "AVENTURA", "AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN 
> CITY", "BALDWIN PLACE", "BALLWIN", "BANGOR", "BARBOURSVILLE", 
> "BARNEGAT", "BARTLETT", "BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", 
> "BATESBURG", "BATON ROUGE", "BAXTER SPRINGS", "BAY CITY", "BAYONNE", 
> "BAYSIDE", "BAYTOWN", "BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER 
> CITY", "BELFAIR", "BELLE VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR", 
> "BELLMORE", "BELLWOOD", "BELMAR", "BELMONT", "BELVIDERE", "BENNET", 
> "BENTON", "BENTONVILLE", "BERLIN", "BESSEMER CITY", "BETHANY", 
> "BETHEL", "BETHEL PARK", "BETHESDA", "BETHLEHEM", "BETTENDORF", 
> "BIGLERVILLE", "BINGHAMTON", "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN", 
> "BLACKSBURG", "BLAINE", "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD", 
> "BLUE BELL", "BLUE GRASS", "BLUE POINT", "BLUE SPRINGS", "BOCA RATON", 
> "BODFISH", "BOILING SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS", 
> "BONNEY LAKE", "BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING 
> GREEN", "BOYERTOWN", "BOYNTON BEACH", "BRADENTON", "BRADENVILLE", 
> "BRANDAMORE", "BRANDON", "BRANFORD", "BRANSON", "BRASELTON", 
> "BREEZEWOOD", "BRENTON", "BRENTWOOD", "BREWSTER", "BRICK", 
> "BRIDGEPORT", "BRIDGETON", "BRIGHAM CITY", "BROADDUS", "BROCKPORT", 
> "BROGUE", "BROKEN ARROW", "BRONX", "BROOKFIELD", "BROOKHAVEN", 
> "BROOKLYN", "BROWNS MILLS", "BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", 
> "BRYAN", "BUCKSPORT", "BUDD LAKE", "BUFFALO", "BULVERDE", 
> "BUNKERVILLE", "BURBANK", "BURFORD", "BURIEN", "BURLINGTON", "BURR 
> RIDGE", "BURTON", "BUSKIRK", "BUTLER", "BUXTON", "BYRON", "CALERA", 
> "CAMBRIDGE", "CAMP HILL", "CAMPBELL", "CANAAN", "CANAL WINCHESTER", 
> "CANASTOTA", "CANTON", "CARDIFF BY THE SEA", "CARL JUNCTION", 
> "CARLISLE", "CARLTON", "CAROL STREAM", "CARROLLTON", "CARSON CITY", 
> "CARTERSVILLE", "CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", 
> "CASWELL", "CATO", "CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER 
> POINT", "CENTERTON", "CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN 
> FALLS", "CHALFONT", "CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", 
> "CHARLESTON", "CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", 
> "CHESHIRE", "CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", 
> "CHILLICOTHE", "CHOCTAW", "CICERO", "CINCINNATI", "CLAY", 
> "CLEARWATER", "CLEARWATER BEACH", "CLEMMONS", "CLENDENIN", 
> "CLEVELAND", "CLEVELAND HTS", "CLEVES", "CLIFFSIDE PARK", "CLIFTON", 
> "CLIFTON PARK", "CLIFTON SPGS", "CLINTON", "COACHELLA", "COAL 
> TOWNSHIP", "COATESVILLE", "COLLEGE STATION", "COLLINSVILLE", 
> "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE", "CONCORD", "CONCORD 
> TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY", "CONYERS", 
> "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA", "CORNELIUS", 
> "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY", "COTTAGE HILLS", 
> "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN", "CRANBERRY 
> TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER", "CUMBERLAND 
> FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS", "DALLAS", 
> "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT", 
> "DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR", 
> "DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH", 
> "DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY", "DES 
> MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER", "DIXON", 
> "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER", "DOYLESTOWN", 
> "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH", "DUNCAN", "DUNCANNON", 
> "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE", "DURHAM", "DUTTON", "EAGLE 
> GROVE", "EAST AURORA", "EAST CHICAGO", "EAST EARL", "EAST HANOVER", 
> "EAST HARTFORD", "EAST HARTLAND", "EAST LEROY", "EAST LIVERPOOL", 
> "EAST MEADOW", "EAST NORTHPORT", "EAST ORANGE", "EAST PEORIA", "EAST 
> SAINT LOUIS", "EASTON", "EDGERTON", "EDISON", "EDMOND", "EGG HBR TWP", 
> "EL DORADO", "EL PASO", "ELDRIDGE", "ELGIN", "ELIZABETHVILLE", 
> "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY", "ELMHURST", "ELMWOOD PARK", 
> "ELOY", "ELY", "EMERSON", "EMORY", "ENDICOTT", "ENGLEWOOD", "ENID", 
> "ENNICE", "ENOLA", "ENUMCLAW", "EPHRATA", "ERIE", "ESTERO", "EUREKA 
> SPRINGS", "EVANSVILLE", "EVERETT", "EXCELSIOR SPRINGS", "EXTON", 
> "FAIRFAX", "FAIRFIELD", "FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", 
> "FALMOUTH", "FAR HILLS", "FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", 
> "FAYETTE", "FAYETTEVILLE", "FELTON", "FENTON", "FERGUSON", "FIELDALE", 
> "FINDLAY", "FINGERVILLE", "FLEMINGTON", "FLINT", "FLORENCE", 
> "FLORESVILLE", "FLORISSANT", "FLOVILLA", "FLOWER MOUND", "FLUSHING", 
> "FOGELSVILLE", "FOLSOM", "FONTANELLE", "FOREST HILLS", "FOREST PARK", 
> "FORT DODGE", "FORT FAIRFIELD", "FORT GAY", "FORT LAUDERDALE", "FORT 
> LEE", "FORT MILL", "FORT MOHAVE", "FORT PAYNE", "FORT PIERCE", "FORT 
> SMITH", "FORT WAYNE", "FORT WORTH", "FOSTORIA", "FOUNTAIN INN", 
> "FRANKFORT", "FRANKLIN", "FREEDOM", "FREEHOLD", "FREEPORT", "FREMONT", 
> "FULLERTON", "FULSHEAR", "FULTON", "GADSDEN", "GAFFNEY", 
> "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE", "GARDNERS", "GARNETT 
> VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA", "GETTYSBURG", 
> "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE", "GLEN CARBON", "GLEN 
> COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW", "GLENWOOD", "GOLDEN 
> CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM", "GOSHEN", 
> "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS", "GRANDVIEW", 
> "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS", "GRASS VALLEY", 
> "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER", "GREENLAWN", 
> "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE", "GREER", 
> "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD", "HADLEY", 
> "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON", "HAMBURG", 
> "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE", 
> "HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON", 
> "HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL", 
> "HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE", 
> "HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH", 
> "HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE", "HIGH 
> HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS", 
> "HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE", 
> "HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD", 
> "HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS", "HOT 
> SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON", "HUFFMAN", 
> "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON", "HUNTINGTON 
> BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE", "HURST", 
> "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL", 
> "INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY", "IRMO", 
> "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP", "ISSAQUAH", 
> "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS", "JACKSONVILLE", 
> "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE", "JEFFERSON", "JEFFERSON 
> CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN", "JONESBORO", "JONESPORT", 
> "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE", "KANNAPOLIS", "KANSAS 
> CITY", "KATY", "KEARNEY", "KELLERTON", "KEMAH", "KENMORE", "KENNESAW", 
> "KENT", "KINGMAN", "KINGS MOUNTAIN", "KINGSLAND", "KINGSTON", 
> "KINGWOOD", "KIRKLAND", "KUTZTOWN", "LA QUINTA", "LA RUE", "LA VERNE", 
> "LAFAYETTE", "LAKE BLUFF", "LAKE IN THE HILL", "LAKE ISABELLA", "LAKE 
> PLACID", "LAKE SAINT LOUIS", "LAKE TAPPS", "LAKE VILLAGE", "LAKE 
> WAUKOMIS", "LAKE WORTH", "LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD 
> RCH", "LAMBERTVILLE", "LANCASTER", "LANDISVILLE", "LANSDALE", 
> "LAREDO", "LARGO", "LAS VEGAS", "LATROBE", "LAUDERHILL", "LAWNDALE", 
> "LAWRENCE", "LAWRENCEVILLE", "LE MARS", "LE ROY", "LEAGUE CITY", 
> "LEAVENWORTH", "LEAWOOD", "LEBANON", "LECOMPTON", "LEECHBURG", "LEES 
> SUMMIT", "LEESBURG", "LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", 
> "LEVITTOWN", "LEWES", "LEWISTOWN", "LEXINGTON", "LIBERTY TWP", 
> "LILLINGTON", "LINCOLN", "LINDENHURST", "LINTON", "LISBON", 
> "LITCHFIELD PK", "LITHIA", "LITITZ", "LITTLE EGG HARBOR TO", "LK 
> FOREST PK", "LK PEEKSKILL", "LOCKHART", "LOCKWOOD", "LOGAN", "LOMA 
> LINDA", "LONE JACK", "LONG BEACH", "LONG LANE", "LONGVIEW", "LORAIN", 
> "LORTON", "LOS ANGELES", "LOUISBURG", "LOVES PARK", "LOWER BURRELL", 
> "LOWRY CITY", "LUBBOCK", "LUGOFF", "LUMBERTON", "LYNBROOK", 
> "LYNNWOOD", "MACHESNEY PARK", "MACON", "MAGNOLIA", "MAHOPAC", 
> "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE", "MANASSAS", "MANCHACA", 
> "MANCHESTER", "MANCHESTER TOWNSHIP", "MANCHESTER TW", "MANHEIM", 
> "MANITO", "MANLIUS", "MANNINGTON", "MANSFIELD", "MANSURA", "MAPLE 
> PARK", "MARBLE FALLS", "MARCELLUS", "MARICOPA", "MARIETTA", "MARION", 
> "MARKLEYSBURG", "MARS HILL", "MARSHALLTOWN", "MARSHFIELD", 
> "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE", "MASON CITY", "MASSEY", 
> "MASSILLON", "MASURY", "MATAWAN", "MATEWAN", "MATTAPOISETT", 
> "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND", "MC DONALD", "MC 
> KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN", "MCALESTER", "MCDONOUGH", 
> "MCKEESPORT", "MCKINNEY", "MECHANICSBURG", "MECHANICSVILLE", 
> "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS", "MERIDEN", 
> "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA", "MESQUITE", 
> "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN", "MIDLAND", 
> "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK", "MILLEDGEVILLE", 
> "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE", "MINEOLA", "MINERAL 
> SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION", "MISSION HILLS", 
> "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY", "MOLINE", 
> "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE", "MONTCLAIR", 
> "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON", "MORGANTOWN", 
> "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE", "MOUNT JOY", 
> "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO", "MOUNT UNION", 
> "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT", "MULVANE", 
> "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT", 
> "MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS", 
> "NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES", 
> "NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG", "NEW 
> BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK", "NEW 
> CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON", "NEW 
> LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE", "NEW 
> ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK", 
> "NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT", 
> "NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA", 
> "NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK", 
> "NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON", 
> "NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND 
> HILLS", "NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE", 
> "NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON", "OAK 
> RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE", "OMAHA", 
> "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION", "ORONO", 
> "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO", "OTTAWA", 
> "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD", "OYSTER BAY", 
> "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR", "PALMDALE", "PALMYRA", 
> "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE", "PARLIN", "PARMA", 
> "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY", "PAXTON", "PEA 
> RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN", "PELION", 
> "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE", "PENNSBURG", 
> "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS", "PERRYSBURG", 
> "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI", "PHOENIX", 
> "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN", "PINE BLUFF", 
> "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY", "PITMAN", 
> "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY", 
> "PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO 
> LAKE", "POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", 
> "PORT CHARLOTTE", "PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", 
> "PORT WASHINGTON", "PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", 
> "POTTSVILLE", "POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE 
> VILLAGE", "PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT 
> PLEASANT", "PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", 
> "QUINCY", "RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", 
> "READLYN", "RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO 
> PARK", "REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", 
> "RICH HILL", "RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", 
> "RINCON", "RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", 
> "RIVERSIDE", "ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", 
> "ROCKAWAY BCH", "ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", 
> "ROGERS", "ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE", 
> "ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT", 
> "ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN", 
> "RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO", 
> "SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR", "SAINT 
> CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS", "SAINT 
> PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY", 
> "SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO", 
> "SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN", 
> "SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", 
> "SARASOTA", "SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", 
> "SAYVILLE", "SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", 
> "SCOTT", "SCOTTSDALE", "SEAFORD", "SEALE", "SEATTLE", "SEDONA", 
> "SEGUIN", "SELDEN", "SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", 
> "SHARON", "SHARPSBURG", "SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", 
> "SHELLSBURG", "SHELTON", "SHENANDOAH", "SHORELINE", "SHOREWOOD", 
> "SHREVEPORT", "SICKLERVILLE", "SILER CITY", "SILOAM SPRINGS", "SIMON", 
> "SIMPSONVILLE", "SIMSBURY", "SIOUX CITY", "SIOUX FALLS", "SKOKIE", 
> "SKOWHEGAN", "SLATINGTON", "SLIDELL", "SMITHFIELD", "SN BERNRDNO", 
> "SNELLVILLE", "SOCORRO", "SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH 
> BELOIT", "SOUTH BEND", "SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH 
> PARK", "SOUTH PLAINFIELD", "SOUTH SIOUX CITY", "SOUTHAMPTON", 
> "SOUTHBURY", "SOUTHGATE", "SPARKS", "SPARROW BUSH", "SPENCER", 
> "SPRING", "SPRING BRANCH", "SPRING GROVE", "SPRING HILL", "SPRING 
> HOPE", "SPRING VALLEY", "SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD 
> GARDENS", "SPRINGHILL", "SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", 
> "ST PETERSBURG", "ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", 
> "STANLEY", "STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", 
> "STILLMAN VALLEY", "STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", 
> "STRAFFORD", "STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", 
> "SUFFIELD", "SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", 
> "SUMMIT", "SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", 
> "SWEENY", "SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", 
> "TAFTVILLE", "TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", 
> "TAYLORVILLE", "TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", 
> "THOMASVILLE", "TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY 
> PARK", "TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", 
> "TOPEKA", "TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS 
> RST", "TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER", 
> "TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY", 
> "UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA", 
> "URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO", "VAN 
> BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO", 
> "VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA", 
> "VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH", "W 
> TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN", 
> "WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG", 
> "WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA", 
> "WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE", 
> "WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER", 
> "WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO", "WEST 
> CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN", "WEST 
> HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH", "WEST 
> POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD", "WEST VALLEY 
> CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER", "WESTMORELAND 
> CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD", "WHARTON", "WHEELING", 
> "WHITE PLAINS", "WHITEHALL", "WHITESTONE", "WHITESTOWN", "WHITING", 
> "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD", "WILLARD", 
> "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK", 
> "WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS", 
> "WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON", 
> "WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE", 
> "WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS", 
> "WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH", 
> "YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG 
> HARRIS", "YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), 
> class = "factor")), row.names = c(NA, -14L), class = "data.frame")
> 
> 
> 
> 
> 
> 
> Proprietary
> 
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may 
> con...{{dropped:16}}
> 
> ______________________________________________
> mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, 
> see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmv
> OXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=N45-UMvb1jEdpZi
> 0xmhOt4rv3lQf2DBkFap0cMXXvWc&s=2Nwt1YXA0urvwXy8m_Yq8Xlrvn-hfAQmjKk7zXm
> ij-c&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmm
> vOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=N45-UMvb1jEdpZ
> i0xmhOt4rv3lQf2DBkFap0cMXXvWc&s=JdUwH82y8yFXsbRMGG_dwcWUBl8gzmy3XPjpkC
> eleSQ&e= and provide commented, minimal, self-contained, reproducible 
> code.
> 
> 
> 
> --
> Ben Tupper
> Bigelow Laboratory for Ocean Science
> East Boothbay, Maine
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.bigelow.org_&d
> =DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8
> v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=N45-UMvb1jEdpZi0xmhOt4rv3lQf2DBkFap0cM
> XXvWc&s=oRaY1hdV6ioa7STQkpTvF_HTzJkFCnJPKJo72SdTbCs&e=
> https://urldefense.proofpoint.com/v2/url?u=https-3A__eco.bigelow.org&d
> =DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8
> v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=N45-UMvb1jEdpZi0xmhOt4rv3lQf2DBkFap0cM
> XXvWc&s=wXV4OHk998Q3mHGu8FvYXE5kZerieouEHZ3_1pU5YCM&e=
> 
> 
> Proprietary
> 
> NOTICE TO RECIPIENT OF INFORMATION:
> This e-mail may contain confidential or privileged information. If you 
> think you have received this e-mail in error, please advise the sender 
> by reply e-mail and then delete this e-mail immediately.
> This e-mail may also contain protected health information (PHI) with 
> information about sensitive medical conditions, including, but not 
> limited to, treatment for substance use disorders, behavioral health, 
> HIV/AIDS, or pregnancy. This type of information may be protected by 
> various federal and/or state laws which prohibit any further 
> disclosure without the express written consent of the person to whom 
> it pertains or as otherwise permitted by law. Any unauthorized further 
> disclosure may be considered a violation of federal and/or state law. 
> A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
> Thank you. Aetna
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmv
> OXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=Ms9jhe1Ax8v9HRS
> eMeLgOfl2-VPojwjIUTh_M7t4jkU&s=Sn_gDqvKHDX4nwoe-PGsU0bEG4KzZwaPTmjPUEk
> OCcM&e= PLEASE do read the posting guide 
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmm
> vOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=Ms9jhe1Ax8v9HR
> SeMeLgOfl2-VPojwjIUTh_M7t4jkU&s=qSN0aOJ1WTHYnPsWWfSnb5mn0SZBYbFXBqTAbs
> xz1p4&e= and provide commented, minimal, self-contained, reproducible 
> code.

--
Small contribution in our coronavirus rigours: 
https://urldefense.proofpoint.com/v2/url?u=https-3A__www.coresystemtrust.org.uk_home_free-2Doptions-2Dto-2Dreplace-2Dpaper-2Dcore-2Dforms-2Dduring-2Dthe-2Dcoronavirus-2Dpandemic_&d=DwIFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=Ms9jhe1Ax8v9HRSeMeLgOfl2-VPojwjIUTh_M7t4jkU&s=qGokAg12ZZjFi4JCvxxXd26PafvfaoVG0Fqot5EdYcI&e= 

Chris Evans <chris at psyctc.org> Visiting Professor, University of Sheffield <chris.evans at sheffield.ac.uk> I do some consultation work for the University of Roehampton <chris.evans at roehampton.ac.uk> and other places but <chris at psyctc.org> remains my main Email address.  I have a work web site at:
   https://urldefense.proofpoint.com/v2/url?u=https-3A__www.psyctc.org_psyctc_&d=DwIFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=Ms9jhe1Ax8v9HRSeMeLgOfl2-VPojwjIUTh_M7t4jkU&s=FmkVW2HrSfPHwiDRbsrWM20hyvnE64BOPqswXsQaBMU&e=
and a site I manage for CORE and CORE system trust at:
   https://urldefense.proofpoint.com/v2/url?u=http-3A__www.coresystemtrust.org.uk_&d=DwIFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=Ms9jhe1Ax8v9HRSeMeLgOfl2-VPojwjIUTh_M7t4jkU&s=EU5ws_kd2kGJV-z7CrY6ygIvRnsKPk82RyqTMnAKqJo&e=
I have "semigrated" to France, see: 
   https://urldefense.proofpoint.com/v2/url?u=https-3A__www.psyctc.org_pelerinage2016_semigrating-2Dto-2Dfrance_&d=DwIFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=Ms9jhe1Ax8v9HRSeMeLgOfl2-VPojwjIUTh_M7t4jkU&s=YGulZegGSJQmc0ZzTvjCjcy3N9IK257P1EVjL0VbBXE&e=  
   https://urldefense.proofpoint.com/v2/url?u=https-3A__www.psyctc.org_pelerinage2016_register-2Dto-2Dget-2Dupdates-2Dfrom-2Dpelerinage2016_&d=DwIFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=Ms9jhe1Ax8v9HRSeMeLgOfl2-VPojwjIUTh_M7t4jkU&s=wFLK06h0WIUgEO4g4slt43qlF_r6bmuGNhJyKhcCcGk&e= 

If you want an Emeeting, I am trying to keep them to Thursdays and my diary is at:
   https://urldefense.proofpoint.com/v2/url?u=https-3A__www.psyctc.org_pelerinage2016_ceworkdiary_&d=DwIFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=Ms9jhe1Ax8v9HRSeMeLgOfl2-VPojwjIUTh_M7t4jkU&s=CNeFnAvoc0Inh8BGh6S4_d1DlvO0Kyfw-jsEQ32mz5o&e=
Beware: French time, generally an hour ahead of UK.

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Tue May 19 16:25:05 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Tue, 19 May 2020 15:25:05 +0100
Subject: [R] Help with ggplot error: #Error in FUN(X[[i]],
 ...) : object 'x' not found
In-Reply-To: <BYAPR06MB53835FCA0A4E629E6C4048EAAEB90@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB53835FCA0A4E629E6C4048EAAEB90@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <acf61a30-a9a3-03cd-2b54-38d806b6cd3f@sapo.pt>

Hello,

I'm getting a different error:


p <- ggplot(c1members2, aes(x,y)) +
   geom_point()

p +  geom_line(data = paths, aes(group = trip_id)) +
   ggtitle(paste0("Optimal route with cost: ", 
round(objective_value(result), 2)))
#Error in objective_value(result) : object 'result' not found


When I remove the ggtitle line,


p +  geom_line(data = paths, aes(group = trip_id)) #+
#Error: Aesthetics must be either length 1 or the same as the data (14): 
x and y


This makes sense, the data argument has changed, trip_id is in data set 
paths, not in c1members2. So I have added x, y aesthetics to the 
geom_line call.


p +  geom_line(data = paths, aes(Longitude, Latitude, group = trip_id))


No errors. Is this it?


Hope this helps,

Rui Barradas


?s 13:28 de 19/05/20, Poling, William via R-help escreveu:
> #RStudio Version Version 1.2.1335
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit)
> #Running under: Windows 10 x64 (build 17763)
> 
> Good morning.
> 
> I am testing a small sample of my data using an example found here:
> #https://www.r-orms.org/mixed-integer-linear-programming/practicals/problem-tsp/
> 
> If needed I can provide my complete script which is simply the same as the example only using a few rows from my data, let me know, as I think this is just a ggplot issue.
> 
> I am getting this error when I use ggplot
> #Error in FUN(X[[i]], ...) : object 'x' not found
> 
> 1. I have reviewed the following with no success.
> #Sources referred to for help
> #https://github.com/business-science/anomalize/issues/2
> #https://stackoverflow.com/questions/38988028/error-in-funxi-object-x-not-found
> #https://community.rstudio.com/t/error-in-fun-x-i-object-variable-not-found/62532/3
> #https://www.neonscience.org/packages-in-r
> 
> 2. I have rebooted and re-installed ggplot2 for updates
> 
> 3. These are the only libraries running
> library(knitr)
> library(dplyr)
> library(ggplot2)
> library(ggmap)
> library(ompr)
> library(ompr.roi)
> library(ROI.plugin.glpk)
> 
> Here is the original plot attempt:
> ggplot(c1members2, aes(x , y)) +
>    geom_point() +
>    geom_line(data = paths, aes(group = trip_id)) + #Error in FUN(X[[i]], ...) : object 'x' not found
>    ggtitle(paste0("Optimal route with cost: ", round(objective_value(result), 2)))
> 
> #I have gone through each of the steps one by one and error occurs at --> geom_line(data = paths, aes(group = trip_id))
> 
> I have also tried the following:
> Test1
> p <- ggplot(c1members2, aes(x,y)) +
>    geom_point()
> 
> p +  geom_line(data = paths, aes(group = trip_id),inherit.aes = FALSE) + #Error in FUN(X[[i]], ...) : object 'x' not found
>    ggtitle(paste0("Optimal route with cost: ", round(objective_value(result), 2)))
> 
> Test2
> ggplot(c1members2, aes(x,y)) +
>    geom_point() +
> geom_line(data = paths, aes(group = trip_id),inherit.aes = TRUE) + #Error in FUN(X[[i]], ...) : object 'x' not found
>    ggtitle(paste0("Optimal route with cost: ", round(objective_value(result), 2)))
> 
> Here are the data involved.
> str(c1members2)
> 'data.frame':	7 obs. of  3 variables:
>   $ id: int  1 2 3 4 5 6 7
>   $ x : num  95.5 90.2 46.8 40.9 43.3 ...
>   $ y : num  4.65 47.57 8.07 38.43 28.59 ...
> 
> str(paths)
> data.frame':	14 obs. of  6 variables:
>   $ trip_id  : int  1 2 3 4 5 6 7 1 2 3 ...
>   $ property : chr  "from" "from" "from" "from" ...
>   $ idx_val  : int  3 1 5 7 6 4 2 1 2 3 ...
>   $ Longitude: num  -122 -122 -122 -122 -122 ...
>   $ Latitude : num  47.3 47.2 47.3 47.6 47.6 ...
>   $ city     : Factor w/ 1337 levels "ABBOTTSTOWN",..: 1169 1169 1169 1069 1069 1069 1069 1169 1069 1169 ...
> 
> Here are the dput()'s
> 
> I hope I have provided enough information to elicit a response.
> 
> Thank you.
> 
> WHP
> 
> dput(c1members2)
> structure(list(id = 1:7, x = c(95.521828085515, 90.2272877161736,
> 46.8465689660691, 40.8959155605829, 43.2591531526978, 23.7485886058345,
> 64.0406297698986), y = c(4.64803485876031, 47.5678629996171,
> 8.0689847051367, 38.4256264258875, 28.5930051512296, 43.5934562642672,
> 42.415566619041)), class = "data.frame", row.names = c(NA, -7L
> 
> dput(paths) #Although I am only using city %in% c("SEATTLE","TACOMA")) from the original 2353 rows = 7 rows in c1members2
> structure(list(trip_id = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 1L, 2L,
> 3L, 4L, 5L, 6L, 7L), property = c("from", "from", "from", "from",
> "from", "from", "from", "to", "to", "to", "to", "to", "to", "to"
> ), idx_val = c(3L, 1L, 5L, 7L, 6L, 4L, 2L, 1L, 2L, 3L, 4L, 5L,
> 6L, 7L), Longitude = c(-122.490417, -122.429323, -122.358115,
> -122.37003, -122.41282, -122.299261, -122.390815, -122.429323,
> -122.390815, -122.490417, -122.299261, -122.358115, -122.41282,
> -122.37003), Latitude = c(47.274599, 47.176596, 47.276776, 47.565667,
> 47.646222, 47.673056, 47.549377, 47.176596, 47.549377, 47.274599,
> 47.673056, 47.276776, 47.646222, 47.565667), city = structure(c(1169L,
> 1169L, 1169L, 1069L, 1069L, 1069L, 1069L, 1169L, 1069L, 1169L,
> 1069L, 1169L, 1069L, 1069L), .Label = c("ABBOTTSTOWN", "ABILENE",
> "ACWORTH", "ADAMS", "ADDISON", "ADKINS", "ALBANY", "ALBIA", "ALBUQUERQUE",
> "ALEXANDRIA", "ALFRED", "ALIQUIPPA", "ALISO VIEJO", "ALLEN PARK",
> "ALLENTOWN", "ALPHA", "ALPHARETTA", "ALPINE", "ALTOONA", "AMARILLO",
> "AMBLER", "AMBRIDGE", "AMHERST", "AMITYVILLE", "ANAHIEM", "ANDERSON",
> "ANDOVER", "ANGIER", "ANGLETON", "ANKENY", "ANN ARBOR", "ANZA",
> "APEX", "APOLLO", "ARCADIA", "ARCHDALE", "ARDMORE", "ARGYLE",
> "ARKANSAS CITY", "ARLINGTON", "ARNOLD", "ARTHUR", "ASHEBORO",
> "ASHEVILLE", "ASHLAND", "ASHLAND CITY", "ASHTON", "ASTON", "ATHENS",
> "ATLANTA", "ATLANTIC BCH", "AUBORN", "AUGUSTA", "AURORA", "AUSTELL",
> "AUSTIN", "AVENTURA", "AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN CITY",
> "BALDWIN PLACE", "BALLWIN", "BANGOR", "BARBOURSVILLE", "BARNEGAT",
> "BARTLETT", "BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", "BATESBURG",
> "BATON ROUGE", "BAXTER SPRINGS", "BAY CITY", "BAYONNE", "BAYSIDE",
> "BAYTOWN", "BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER CITY",
> "BELFAIR", "BELLE VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR",
> "BELLMORE", "BELLWOOD", "BELMAR", "BELMONT", "BELVIDERE", "BENNET",
> "BENTON", "BENTONVILLE", "BERLIN", "BESSEMER CITY", "BETHANY",
> "BETHEL", "BETHEL PARK", "BETHESDA", "BETHLEHEM", "BETTENDORF",
> "BIGLERVILLE", "BINGHAMTON", "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN",
> "BLACKSBURG", "BLAINE", "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD",
> "BLUE BELL", "BLUE GRASS", "BLUE POINT", "BLUE SPRINGS", "BOCA RATON",
> "BODFISH", "BOILING SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS",
> "BONNEY LAKE", "BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING GREEN",
> "BOYERTOWN", "BOYNTON BEACH", "BRADENTON", "BRADENVILLE", "BRANDAMORE",
> "BRANDON", "BRANFORD", "BRANSON", "BRASELTON", "BREEZEWOOD",
> "BRENTON", "BRENTWOOD", "BREWSTER", "BRICK", "BRIDGEPORT", "BRIDGETON",
> "BRIGHAM CITY", "BROADDUS", "BROCKPORT", "BROGUE", "BROKEN ARROW",
> "BRONX", "BROOKFIELD", "BROOKHAVEN", "BROOKLYN", "BROWNS MILLS",
> "BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", "BRYAN", "BUCKSPORT",
> "BUDD LAKE", "BUFFALO", "BULVERDE", "BUNKERVILLE", "BURBANK",
> "BURFORD", "BURIEN", "BURLINGTON", "BURR RIDGE", "BURTON", "BUSKIRK",
> "BUTLER", "BUXTON", "BYRON", "CALERA", "CAMBRIDGE", "CAMP HILL",
> "CAMPBELL", "CANAAN", "CANAL WINCHESTER", "CANASTOTA", "CANTON",
> "CARDIFF BY THE SEA", "CARL JUNCTION", "CARLISLE", "CARLTON",
> "CAROL STREAM", "CARROLLTON", "CARSON CITY", "CARTERSVILLE",
> "CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", "CASWELL", "CATO",
> "CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER POINT", "CENTERTON",
> "CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN FALLS", "CHALFONT",
> "CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", "CHARLESTON",
> "CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", "CHESHIRE",
> "CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", "CHILLICOTHE",
> "CHOCTAW", "CICERO", "CINCINNATI", "CLAY", "CLEARWATER", "CLEARWATER BEACH",
> "CLEMMONS", "CLENDENIN", "CLEVELAND", "CLEVELAND HTS", "CLEVES",
> "CLIFFSIDE PARK", "CLIFTON", "CLIFTON PARK", "CLIFTON SPGS",
> "CLINTON", "COACHELLA", "COAL TOWNSHIP", "COATESVILLE", "COLLEGE STATION",
> "COLLINSVILLE", "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE",
> "CONCORD", "CONCORD TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY",
> "CONYERS", "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA",
> "CORNELIUS", "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY",
> "COTTAGE HILLS", "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN",
> "CRANBERRY TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER",
> "CUMBERLAND FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS",
> "DALLAS", "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT",
> "DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR",
> "DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH",
> "DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY",
> "DES MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER",
> "DIXON", "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER",
> "DOYLESTOWN", "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH",
> "DUNCAN", "DUNCANNON", "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE",
> "DURHAM", "DUTTON", "EAGLE GROVE", "EAST AURORA", "EAST CHICAGO",
> "EAST EARL", "EAST HANOVER", "EAST HARTFORD", "EAST HARTLAND",
> "EAST LEROY", "EAST LIVERPOOL", "EAST MEADOW", "EAST NORTHPORT",
> "EAST ORANGE", "EAST PEORIA", "EAST SAINT LOUIS", "EASTON", "EDGERTON",
> "EDISON", "EDMOND", "EGG HBR TWP", "EL DORADO", "EL PASO", "ELDRIDGE",
> "ELGIN", "ELIZABETHVILLE", "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY",
> "ELMHURST", "ELMWOOD PARK", "ELOY", "ELY", "EMERSON", "EMORY",
> "ENDICOTT", "ENGLEWOOD", "ENID", "ENNICE", "ENOLA", "ENUMCLAW",
> "EPHRATA", "ERIE", "ESTERO", "EUREKA SPRINGS", "EVANSVILLE",
> "EVERETT", "EXCELSIOR SPRINGS", "EXTON", "FAIRFAX", "FAIRFIELD",
> "FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", "FALMOUTH", "FAR HILLS",
> "FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", "FAYETTE", "FAYETTEVILLE",
> "FELTON", "FENTON", "FERGUSON", "FIELDALE", "FINDLAY", "FINGERVILLE",
> "FLEMINGTON", "FLINT", "FLORENCE", "FLORESVILLE", "FLORISSANT",
> "FLOVILLA", "FLOWER MOUND", "FLUSHING", "FOGELSVILLE", "FOLSOM",
> "FONTANELLE", "FOREST HILLS", "FOREST PARK", "FORT DODGE", "FORT FAIRFIELD",
> "FORT GAY", "FORT LAUDERDALE", "FORT LEE", "FORT MILL", "FORT MOHAVE",
> "FORT PAYNE", "FORT PIERCE", "FORT SMITH", "FORT WAYNE", "FORT WORTH",
> "FOSTORIA", "FOUNTAIN INN", "FRANKFORT", "FRANKLIN", "FREEDOM",
> "FREEHOLD", "FREEPORT", "FREMONT", "FULLERTON", "FULSHEAR", "FULTON",
> "GADSDEN", "GAFFNEY", "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE",
> "GARDNERS", "GARNETT VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA",
> "GETTYSBURG", "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE",
> "GLEN CARBON", "GLEN COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW",
> "GLENWOOD", "GOLDEN CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM",
> "GOSHEN", "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS",
> "GRANDVIEW", "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS",
> "GRASS VALLEY", "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER",
> "GREENLAWN", "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE",
> "GREER", "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD",
> "HADLEY", "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON",
> "HAMBURG", "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE",
> "HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON",
> "HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL",
> "HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE",
> "HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH",
> "HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE",
> "HIGH HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS",
> "HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE",
> "HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD",
> "HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS",
> "HOT SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON",
> "HUFFMAN", "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON",
> "HUNTINGTON BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE",
> "HURST", "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL",
> "INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY",
> "IRMO", "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP",
> "ISSAQUAH", "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS",
> "JACKSONVILLE", "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE",
> "JEFFERSON", "JEFFERSON CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN",
> "JONESBORO", "JONESPORT", "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE",
> "KANNAPOLIS", "KANSAS CITY", "KATY", "KEARNEY", "KELLERTON",
> "KEMAH", "KENMORE", "KENNESAW", "KENT", "KINGMAN", "KINGS MOUNTAIN",
> "KINGSLAND", "KINGSTON", "KINGWOOD", "KIRKLAND", "KUTZTOWN",
> "LA QUINTA", "LA RUE", "LA VERNE", "LAFAYETTE", "LAKE BLUFF",
> "LAKE IN THE HILL", "LAKE ISABELLA", "LAKE PLACID", "LAKE SAINT LOUIS",
> "LAKE TAPPS", "LAKE VILLAGE", "LAKE WAUKOMIS", "LAKE WORTH",
> "LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD RCH", "LAMBERTVILLE",
> "LANCASTER", "LANDISVILLE", "LANSDALE", "LAREDO", "LARGO", "LAS VEGAS",
> "LATROBE", "LAUDERHILL", "LAWNDALE", "LAWRENCE", "LAWRENCEVILLE",
> "LE MARS", "LE ROY", "LEAGUE CITY", "LEAVENWORTH", "LEAWOOD",
> "LEBANON", "LECOMPTON", "LEECHBURG", "LEES SUMMIT", "LEESBURG",
> "LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", "LEVITTOWN", "LEWES",
> "LEWISTOWN", "LEXINGTON", "LIBERTY TWP", "LILLINGTON", "LINCOLN",
> "LINDENHURST", "LINTON", "LISBON", "LITCHFIELD PK", "LITHIA",
> "LITITZ", "LITTLE EGG HARBOR TO", "LK FOREST PK", "LK PEEKSKILL",
> "LOCKHART", "LOCKWOOD", "LOGAN", "LOMA LINDA", "LONE JACK", "LONG BEACH",
> "LONG LANE", "LONGVIEW", "LORAIN", "LORTON", "LOS ANGELES", "LOUISBURG",
> "LOVES PARK", "LOWER BURRELL", "LOWRY CITY", "LUBBOCK", "LUGOFF",
> "LUMBERTON", "LYNBROOK", "LYNNWOOD", "MACHESNEY PARK", "MACON",
> "MAGNOLIA", "MAHOPAC", "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE",
> "MANASSAS", "MANCHACA", "MANCHESTER", "MANCHESTER TOWNSHIP",
> "MANCHESTER TW", "MANHEIM", "MANITO", "MANLIUS", "MANNINGTON",
> "MANSFIELD", "MANSURA", "MAPLE PARK", "MARBLE FALLS", "MARCELLUS",
> "MARICOPA", "MARIETTA", "MARION", "MARKLEYSBURG", "MARS HILL",
> "MARSHALLTOWN", "MARSHFIELD", "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE",
> "MASON CITY", "MASSEY", "MASSILLON", "MASURY", "MATAWAN", "MATEWAN",
> "MATTAPOISETT", "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND",
> "MC DONALD", "MC KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN",
> "MCALESTER", "MCDONOUGH", "MCKEESPORT", "MCKINNEY", "MECHANICSBURG",
> "MECHANICSVILLE", "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS",
> "MERIDEN", "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA",
> "MESQUITE", "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN",
> "MIDLAND", "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK",
> "MILLEDGEVILLE", "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE",
> "MINEOLA", "MINERAL SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION",
> "MISSION HILLS", "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY",
> "MOLINE", "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE",
> "MONTCLAIR", "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON",
> "MORGANTOWN", "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE",
> "MOUNT JOY", "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO",
> "MOUNT UNION", "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT",
> "MULVANE", "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT",
> "MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS",
> "NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES",
> "NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG",
> "NEW BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK",
> "NEW CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON",
> "NEW LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE",
> "NEW ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK",
> "NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT",
> "NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA",
> "NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK",
> "NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON",
> "NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND HILLS",
> "NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE",
> "NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON",
> "OAK RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE",
> "OMAHA", "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION",
> "ORONO", "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO",
> "OTTAWA", "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD",
> "OYSTER BAY", "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR",
> "PALMDALE", "PALMYRA", "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE",
> "PARLIN", "PARMA", "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY",
> "PAXTON", "PEA RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN",
> "PELION", "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE",
> "PENNSBURG", "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS",
> "PERRYSBURG", "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI",
> "PHOENIX", "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN",
> "PINE BLUFF", "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY",
> "PITMAN", "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY",
> "PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO LAKE",
> "POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", "PORT CHARLOTTE",
> "PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", "PORT WASHINGTON",
> "PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", "POTTSVILLE",
> "POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE VILLAGE",
> "PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT PLEASANT",
> "PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", "QUINCY",
> "RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", "READLYN",
> "RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO PARK",
> "REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", "RICH HILL",
> "RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", "RINCON",
> "RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", "RIVERSIDE",
> "ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", "ROCKAWAY BCH",
> "ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", "ROGERS",
> "ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE",
> "ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT",
> "ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN",
> "RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO",
> "SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR",
> "SAINT CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS",
> "SAINT PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY",
> "SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO",
> "SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN",
> "SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", "SARASOTA",
> "SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", "SAYVILLE",
> "SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", "SCOTT", "SCOTTSDALE",
> "SEAFORD", "SEALE", "SEATTLE", "SEDONA", "SEGUIN", "SELDEN",
> "SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", "SHARON", "SHARPSBURG",
> "SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", "SHELLSBURG", "SHELTON",
> "SHENANDOAH", "SHORELINE", "SHOREWOOD", "SHREVEPORT", "SICKLERVILLE",
> "SILER CITY", "SILOAM SPRINGS", "SIMON", "SIMPSONVILLE", "SIMSBURY",
> "SIOUX CITY", "SIOUX FALLS", "SKOKIE", "SKOWHEGAN", "SLATINGTON",
> "SLIDELL", "SMITHFIELD", "SN BERNRDNO", "SNELLVILLE", "SOCORRO",
> "SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH BELOIT", "SOUTH BEND",
> "SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH PARK", "SOUTH PLAINFIELD",
> "SOUTH SIOUX CITY", "SOUTHAMPTON", "SOUTHBURY", "SOUTHGATE",
> "SPARKS", "SPARROW BUSH", "SPENCER", "SPRING", "SPRING BRANCH",
> "SPRING GROVE", "SPRING HILL", "SPRING HOPE", "SPRING VALLEY",
> "SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD GARDENS", "SPRINGHILL",
> "SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", "ST PETERSBURG",
> "ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", "STANLEY",
> "STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", "STILLMAN VALLEY",
> "STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", "STRAFFORD",
> "STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", "SUFFIELD",
> "SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", "SUMMIT",
> "SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", "SWEENY",
> "SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", "TAFTVILLE",
> "TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", "TAYLORVILLE",
> "TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", "THOMASVILLE",
> "TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY PARK",
> "TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", "TOPEKA",
> "TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS RST",
> "TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER",
> "TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY",
> "UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA",
> "URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO",
> "VAN BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO",
> "VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA",
> "VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH",
> "W TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN",
> "WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG",
> "WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA",
> "WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE",
> "WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER",
> "WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO",
> "WEST CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN",
> "WEST HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH",
> "WEST POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD",
> "WEST VALLEY CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER",
> "WESTMORELAND CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD",
> "WHARTON", "WHEELING", "WHITE PLAINS", "WHITEHALL", "WHITESTONE",
> "WHITESTOWN", "WHITING", "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD",
> "WILLARD", "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK",
> "WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS",
> "WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON",
> "WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE",
> "WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS",
> "WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH",
> "YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS",
> "YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor")), row.names = c(NA,
> -14L), class = "data.frame")
> 
> 
> 
> 
> 
> 
> Proprietary
> 
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Tue May 19 18:12:04 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Tue, 19 May 2020 11:12:04 -0500
Subject: [R] text annotation on Manhattn plot in qqman
Message-ID: <CAF9-5jO0=466Q-8VyBCfMTyqSPMzXC9nnxDNtW6RPVieGDnA-Q@mail.gmail.com>

Hello,

I am making manhattan plot with:
library(qqman)
manhattan(a, chr="CHR", bp="BP", snp="SNP", p="P",annotatePval = 0.0001)

and I would like to annotate these two SNPs which are above the
threshold so that they have GENE name beside them:

> a[a$SNP=="rs4081570",]
        SNP            P CHR       BP GENE
1 rs4081570 6.564447e-05  19 15918647 UCA1
> a[a$SNP=="rs11867934",]
            SNP            P CHR       BP GENE
1021 rs11867934 6.738066e-06  17 16933404 FLCN

Right now my plot only has SNP name for those 2, how to add GENE names
(FLCN and UCA1 as well)

Please advise
Ana

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Screen Shot 2020-05-19 at 11.11.43 AM.png
Type: image/png
Size: 82674 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200519/0d923656/attachment.png>

From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Tue May 19 18:21:02 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Tue, 19 May 2020 11:21:02 -0500
Subject: [R] text annotation on Manhattn plot in qqman
In-Reply-To: <6c9143b0-03c3-9b17-b11e-c3fe94128ae4@dewey.myzen.co.uk>
References: <CAF9-5jO0=466Q-8VyBCfMTyqSPMzXC9nnxDNtW6RPVieGDnA-Q@mail.gmail.com>
 <6c9143b0-03c3-9b17-b11e-c3fe94128ae4@dewey.myzen.co.uk>
Message-ID: <CAF9-5jP3W-kJJiXSC+FRev4FgZqN74LvfnvL5VWTgUGnb56wew@mail.gmail.com>

Hi Michael,

can you please send me code how that would be done?

Thanks
Ana

On Tue, May 19, 2020 at 11:18 AM Michael Dewey <lists at dewey.myzen.co.uk> wrote:
>
> Dear Ana
>
> Perhaps paste together SNP and GENE using paste() and then supply that
> as the snp parameter.
>
> Michael
>
> On 19/05/2020 17:12, Ana Marija wrote:
> > Hello,
> >
> > I am making manhattan plot with:
> > library(qqman)
> > manhattan(a, chr="CHR", bp="BP", snp="SNP", p="P",annotatePval = 0.0001)
> >
> > and I would like to annotate these two SNPs which are above the
> > threshold so that they have GENE name beside them:
> >
> >> a[a$SNP=="rs4081570",]
> >          SNP            P CHR       BP GENE
> > 1 rs4081570 6.564447e-05  19 15918647 UCA1
> >> a[a$SNP=="rs11867934",]
> >              SNP            P CHR       BP GENE
> > 1021 rs11867934 6.738066e-06  17 16933404 FLCN
> >
> > Right now my plot only has SNP name for those 2, how to add GENE names
> > (FLCN and UCA1 as well)
> >
> > Please advise
> > Ana
> >
> >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html


From ||@t@ @end|ng |rom dewey@myzen@co@uk  Tue May 19 18:18:39 2020
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Tue, 19 May 2020 17:18:39 +0100
Subject: [R] text annotation on Manhattn plot in qqman
In-Reply-To: <CAF9-5jO0=466Q-8VyBCfMTyqSPMzXC9nnxDNtW6RPVieGDnA-Q@mail.gmail.com>
References: <CAF9-5jO0=466Q-8VyBCfMTyqSPMzXC9nnxDNtW6RPVieGDnA-Q@mail.gmail.com>
Message-ID: <6c9143b0-03c3-9b17-b11e-c3fe94128ae4@dewey.myzen.co.uk>

Dear Ana

Perhaps paste together SNP and GENE using paste() and then supply that 
as the snp parameter.

Michael

On 19/05/2020 17:12, Ana Marija wrote:
> Hello,
> 
> I am making manhattan plot with:
> library(qqman)
> manhattan(a, chr="CHR", bp="BP", snp="SNP", p="P",annotatePval = 0.0001)
> 
> and I would like to annotate these two SNPs which are above the
> threshold so that they have GENE name beside them:
> 
>> a[a$SNP=="rs4081570",]
>          SNP            P CHR       BP GENE
> 1 rs4081570 6.564447e-05  19 15918647 UCA1
>> a[a$SNP=="rs11867934",]
>              SNP            P CHR       BP GENE
> 1021 rs11867934 6.738066e-06  17 16933404 FLCN
> 
> Right now my plot only has SNP name for those 2, how to add GENE names
> (FLCN and UCA1 as well)
> 
> Please advise
> Ana
> 
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Tue May 19 21:51:14 2020
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Tue, 19 May 2020 19:51:14 +0000 (UTC)
Subject: [R] Get a result but an error message as well ?
References: <232159068.980863.1589917874331.ref@mail.yahoo.com>
Message-ID: <232159068.980863.1589917874331@mail.yahoo.com>

Dear R-experts,

Here is my R code, I get a result but I also get an error message so I doubt I can trust the result I get. 
What is going wrong ? Many thanks.

########################################
a<-c(2,4,3,4,6,5,3,1,2,3,4,3,4,5,65)
b<-c(23,45,32,12,23,43,56,44,33,11,12,54,23,34,54)
d<-c(9,4,5,3,2,1,3,4,5,6,4,9,10,11,18)

my.experiment <- function( ) {

OLS <- lm( a ~ b+d )
MSE_OLS<-mean(OLS$residuals^2)
return( c(MSE_OLS) )
}

my.data = t(replicate( 500, my.experiment() ))
colnames(my.data) <- c("MSE_OLS")
mean(my.data)
########################################
?


From rmh @end|ng |rom temp|e@edu  Tue May 19 21:58:52 2020
From: rmh @end|ng |rom temp|e@edu (Richard M. Heiberger)
Date: Tue, 19 May 2020 15:58:52 -0400
Subject: [R] [External]  Get a result but an error message as well ?
In-Reply-To: <232159068.980863.1589917874331@mail.yahoo.com>
References: <232159068.980863.1589917874331.ref@mail.yahoo.com>
 <232159068.980863.1589917874331@mail.yahoo.com>
Message-ID: <CAGx1TMAZA7sJBZ343MEXwD=eZCM0V9_VLTSjw+QEvbniqDCepA@mail.gmail.com>

> dim(my.data)
[1]   1 500

you have a matrix with a single row and 500 columns.
you gave a name to only the first column.

Look at the result of replicate().  it is a vector.  You transposed it into
a one-row matrix.

>  tmp <- replicate( 500, my.experiment() )
> dim(tmp)
NULL
> length(tmp)
[1] 500
> dim(t(tmp))
[1]   1 500


On Tue, May 19, 2020 at 3:51 PM varin sacha via R-help <r-help at r-project.org>
wrote:

> Dear R-experts,
>
> Here is my R code, I get a result but I also get an error message so I
> doubt I can trust the result I get.
> What is going wrong ? Many thanks.
>
> ########################################
> a<-c(2,4,3,4,6,5,3,1,2,3,4,3,4,5,65)
> b<-c(23,45,32,12,23,43,56,44,33,11,12,54,23,34,54)
> d<-c(9,4,5,3,2,1,3,4,5,6,4,9,10,11,18)
>
> my.experiment <- function( ) {
>
> OLS <- lm( a ~ b+d )
> MSE_OLS<-mean(OLS$residuals^2)
> return( c(MSE_OLS) )
> }
>
> my.data = t(replicate( 500, my.experiment() ))
> colnames(my.data) <- c("MSE_OLS")
> mean(my.data)
> ########################################
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Tue May 19 22:38:24 2020
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Tue, 19 May 2020 20:38:24 +0000 (UTC)
Subject: [R] [External]  Get a result but an error message as well ?
In-Reply-To: <CAGx1TMAZA7sJBZ343MEXwD=eZCM0V9_VLTSjw+QEvbniqDCepA@mail.gmail.com>
References: <232159068.980863.1589917874331.ref@mail.yahoo.com>
 <232159068.980863.1589917874331@mail.yahoo.com>
 <CAGx1TMAZA7sJBZ343MEXwD=eZCM0V9_VLTSjw+QEvbniqDCepA@mail.gmail.com>
Message-ID: <1667554344.1012305.1589920704261@mail.yahoo.com>


Hi Richard,

Thanks for your response. 
However, how can I correct my R code knowing that I want, as a result, only one value : the mean of the 500 MSE_OLS values ?







Le mardi 19 mai 2020 ? 21:59:07 UTC+2, Richard M. Heiberger <rmh at temple.edu> a ?crit : 





> dim(my.data)
[1] ? 1 500

you have a matrix with a single row and 500 columns.
you gave a name to only the first column.

Look at the result of replicate().? it is a vector.? You transposed it into a one-row matrix.

> ?tmp <- replicate( 500, my.experiment() )
> dim(tmp)
NULL
> length(tmp)
[1] 500
> dim(t(tmp))
[1] ? 1 500


On Tue, May 19, 2020 at 3:51 PM varin sacha via R-help <r-help at r-project.org> wrote:
> Dear R-experts,
> 
> Here is my R code, I get a result but I also get an error message so I doubt I can trust the result I get. 
> What is going wrong ? Many thanks.
> 
> ########################################
> a<-c(2,4,3,4,6,5,3,1,2,3,4,3,4,5,65)
> b<-c(23,45,32,12,23,43,56,44,33,11,12,54,23,34,54)
> d<-c(9,4,5,3,2,1,3,4,5,6,4,9,10,11,18)
> 
> my.experiment <- function( ) {
> 
> OLS <- lm( a ~ b+d )
> MSE_OLS<-mean(OLS$residuals^2)
> return( c(MSE_OLS) )
> }
> 
> my.data = t(replicate( 500, my.experiment() ))
> colnames(my.data) <- c("MSE_OLS")
> mean(my.data)
> ########################################
> ?
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Tue May 19 23:14:19 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Tue, 19 May 2020 22:14:19 +0100
Subject: [R] [External] Get a result but an error message as well ?
In-Reply-To: <1667554344.1012305.1589920704261@mail.yahoo.com>
References: <232159068.980863.1589917874331.ref@mail.yahoo.com>
 <232159068.980863.1589917874331@mail.yahoo.com>
 <CAGx1TMAZA7sJBZ343MEXwD=eZCM0V9_VLTSjw+QEvbniqDCepA@mail.gmail.com>
 <1667554344.1012305.1589920704261@mail.yahoo.com>
Message-ID: <79e7fdeb-fe14-dcc0-f998-1c7307eb759b@sapo.pt>

Hello,

Inline.

?s 21:38 de 19/05/20, varin sacha via R-help escreveu:
> 
> Hi Richard,
> 
> Thanks for your response.
> However, how can I correct my R code knowing that I want, as a result, only one value : the mean of the 500 MSE_OLS values ?

Just don't transpose the output of replicate?

Hope this helps,

Rui Barradas
> 
> 
> 
> 
> 
> 
> 
> Le mardi 19 mai 2020 ? 21:59:07 UTC+2, Richard M. Heiberger <rmh at temple.edu> a ?crit :
> 
> 
> 
> 
> 
>> dim(my.data)
> [1] ? 1 500
> 
> you have a matrix with a single row and 500 columns.
> you gave a name to only the first column.
> 
> Look at the result of replicate().? it is a vector.? You transposed it into a one-row matrix.
> 
>>  ?tmp <- replicate( 500, my.experiment() )
>> dim(tmp)
> NULL
>> length(tmp)
> [1] 500
>> dim(t(tmp))
> [1] ? 1 500
> 
> 
> On Tue, May 19, 2020 at 3:51 PM varin sacha via R-help <r-help at r-project.org> wrote:
>> Dear R-experts,
>>
>> Here is my R code, I get a result but I also get an error message so I doubt I can trust the result I get.
>> What is going wrong ? Many thanks.
>>
>> ########################################
>> a<-c(2,4,3,4,6,5,3,1,2,3,4,3,4,5,65)
>> b<-c(23,45,32,12,23,43,56,44,33,11,12,54,23,34,54)
>> d<-c(9,4,5,3,2,1,3,4,5,6,4,9,10,11,18)
>>
>> my.experiment <- function( ) {
>>
>> OLS <- lm( a ~ b+d )
>> MSE_OLS<-mean(OLS$residuals^2)
>> return( c(MSE_OLS) )
>> }
>>
>> my.data = t(replicate( 500, my.experiment() ))
>> colnames(my.data) <- c("MSE_OLS")
>> mean(my.data)
>> ########################################
>>   
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Tue May 19 23:25:03 2020
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Tue, 19 May 2020 21:25:03 +0000 (UTC)
Subject: [R] [External] Get a result but an error message as well ?
In-Reply-To: <79e7fdeb-fe14-dcc0-f998-1c7307eb759b@sapo.pt>
References: <232159068.980863.1589917874331.ref@mail.yahoo.com>
 <232159068.980863.1589917874331@mail.yahoo.com>
 <CAGx1TMAZA7sJBZ343MEXwD=eZCM0V9_VLTSjw+QEvbniqDCepA@mail.gmail.com>
 <1667554344.1012305.1589920704261@mail.yahoo.com>
 <79e7fdeb-fe14-dcc0-f998-1c7307eb759b@sapo.pt>
Message-ID: <1253484374.1044092.1589923503225@mail.yahoo.com>

Hi Rui,

If I don't transpose t() the output of the replicate (my R code here below) I still get an error message !!

########################################
a=c(2,4,3,4,6,5,3,1,2,3,4,3,4,5,65)
b=c(23,45,32,12,23,43,56,44,33,11,12,54,23,34,54)
d=c(9,4,5,3,2,1,3,4,5,6,4,9,10,11,18)

my.experiment <- function() {

OLS <- lm( a ~ b+d )

MSE_OLS<-mean(OLS$residuals^2)

return( c(MSE_OLS) )
}

my.data = replicate( 500, my.experiment() )
colnames(my.data) <- c("MSE_OLS")
mean(my.data)
########################################






Le mardi 19 mai 2020 ? 23:14:21 UTC+2, Rui Barradas <ruipbarradas at sapo.pt> a ?crit : 





Hello,

Inline.

?s 21:38 de 19/05/20, varin sacha via R-help escreveu:
> 
> Hi Richard,
> 
> Thanks for your response.
> However, how can I correct my R code knowing that I want, as a result, only one value : the mean of the 500 MSE_OLS values ?

Just don't transpose the output of replicate?

Hope this helps,

Rui Barradas

> 
> 
> 
> 
> 
> 
> 
> Le mardi 19 mai 2020 ? 21:59:07 UTC+2, Richard M. Heiberger <rmh at temple.edu> a ?crit :
> 
> 
> 
> 
> 
>> dim(my.data)
> [1] ? 1 500
> 
> you have a matrix with a single row and 500 columns.
> you gave a name to only the first column.
> 
> Look at the result of replicate().? it is a vector.? You transposed it into a one-row matrix.
> 
>>? ?tmp <- replicate( 500, my.experiment() )
>> dim(tmp)
> NULL
>> length(tmp)
> [1] 500
>> dim(t(tmp))
> [1] ? 1 500
> 
> 
> On Tue, May 19, 2020 at 3:51 PM varin sacha via R-help <r-help at r-project.org> wrote:
>> Dear R-experts,
>>
>> Here is my R code, I get a result but I also get an error message so I doubt I can trust the result I get.
>> What is going wrong ? Many thanks.
>>
>> ########################################
>> a<-c(2,4,3,4,6,5,3,1,2,3,4,3,4,5,65)
>> b<-c(23,45,32,12,23,43,56,44,33,11,12,54,23,34,54)
>> d<-c(9,4,5,3,2,1,3,4,5,6,4,9,10,11,18)
>>
>> my.experiment <- function( ) {
>>
>> OLS <- lm( a ~ b+d )
>> MSE_OLS<-mean(OLS$residuals^2)
>> return( c(MSE_OLS) )
>> }
>>
>> my.data = t(replicate( 500, my.experiment() ))
>> colnames(my.data) <- c("MSE_OLS")
>> mean(my.data)
>> ########################################
>>? 
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From rmh @end|ng |rom temp|e@edu  Tue May 19 23:30:09 2020
From: rmh @end|ng |rom temp|e@edu (Richard M. Heiberger)
Date: Tue, 19 May 2020 17:30:09 -0400
Subject: [R] 
 [External] Re: [External] Get a result but an error message as
 well ?
In-Reply-To: <1253484374.1044092.1589923503225@mail.yahoo.com>
References: <232159068.980863.1589917874331.ref@mail.yahoo.com>
 <232159068.980863.1589917874331@mail.yahoo.com>
 <CAGx1TMAZA7sJBZ343MEXwD=eZCM0V9_VLTSjw+QEvbniqDCepA@mail.gmail.com>
 <1667554344.1012305.1589920704261@mail.yahoo.com>
 <79e7fdeb-fe14-dcc0-f998-1c7307eb759b@sapo.pt>
 <1253484374.1044092.1589923503225@mail.yahoo.com>
Message-ID: <CAGx1TMD+G50EERDDVXFvVmHk9A-cVCS61Re0xs28wKcC1AUCNA@mail.gmail.com>

you need to pay attention to the intermediate structures that you generate.
If all you want is one number, then that is what you should create

mean(replicate( 500, my.experiment() ))

Since you do seem to want to store the intermediate values, then you must
name
the object according to its structure.  A one-dimensional vector takes
names(), not dimnames().
A two-dimensional structure takes dimnames, and the number of row names and
the number of column
names must match the number of rows and columns.


On Tue, May 19, 2020 at 5:25 PM varin sacha <varinsacha at yahoo.fr> wrote:

> Hi Rui,
>
> If I don't transpose t() the output of the replicate (my R code here
> below) I still get an error message !!
>
> ########################################
> a=c(2,4,3,4,6,5,3,1,2,3,4,3,4,5,65)
> b=c(23,45,32,12,23,43,56,44,33,11,12,54,23,34,54)
> d=c(9,4,5,3,2,1,3,4,5,6,4,9,10,11,18)
>
> my.experiment <- function() {
>
> OLS <- lm( a ~ b+d )
>
> MSE_OLS<-mean(OLS$residuals^2)
>
> return( c(MSE_OLS) )
> }
>
> my.data = replicate( 500, my.experiment() )
> colnames(my.data) <- c("MSE_OLS")
> mean(my.data)
> ########################################
>
>
>
>
>
>
> Le mardi 19 mai 2020 ? 23:14:21 UTC+2, Rui Barradas <ruipbarradas at sapo.pt>
> a ?crit :
>
>
>
>
>
> Hello,
>
> Inline.
>
> ?s 21:38 de 19/05/20, varin sacha via R-help escreveu:
> >
> > Hi Richard,
> >
> > Thanks for your response.
> > However, how can I correct my R code knowing that I want, as a result,
> only one value : the mean of the 500 MSE_OLS values ?
>
> Just don't transpose the output of replicate?
>
> Hope this helps,
>
> Rui Barradas
>
> >
> >
> >
> >
> >
> >
> >
> > Le mardi 19 mai 2020 ? 21:59:07 UTC+2, Richard M. Heiberger <
> rmh at temple.edu> a ?crit :
> >
> >
> >
> >
> >
> >> dim(my.data)
> > [1]   1 500
> >
> > you have a matrix with a single row and 500 columns.
> > you gave a name to only the first column.
> >
> > Look at the result of replicate().  it is a vector.  You transposed it
> into a one-row matrix.
> >
> >>   tmp <- replicate( 500, my.experiment() )
> >> dim(tmp)
> > NULL
> >> length(tmp)
> > [1] 500
> >> dim(t(tmp))
> > [1]   1 500
> >
> >
> > On Tue, May 19, 2020 at 3:51 PM varin sacha via R-help <
> r-help at r-project.org> wrote:
> >> Dear R-experts,
> >>
> >> Here is my R code, I get a result but I also get an error message so I
> doubt I can trust the result I get.
> >> What is going wrong ? Many thanks.
> >>
> >> ########################################
> >> a<-c(2,4,3,4,6,5,3,1,2,3,4,3,4,5,65)
> >> b<-c(23,45,32,12,23,43,56,44,33,11,12,54,23,34,54)
> >> d<-c(9,4,5,3,2,1,3,4,5,6,4,9,10,11,18)
> >>
> >> my.experiment <- function( ) {
> >>
> >> OLS <- lm( a ~ b+d )
> >> MSE_OLS<-mean(OLS$residuals^2)
> >> return( c(MSE_OLS) )
> >> }
> >>
> >> my.data = t(replicate( 500, my.experiment() ))
> >> colnames(my.data) <- c("MSE_OLS")
> >> mean(my.data)
> >> ########################################
> >>
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue May 19 23:44:53 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 19 May 2020 14:44:53 -0700
Subject: [R] 
 [External] Re: [External] Get a result but an error message as
 well ?
In-Reply-To: <CAGx1TMD+G50EERDDVXFvVmHk9A-cVCS61Re0xs28wKcC1AUCNA@mail.gmail.com>
References: <232159068.980863.1589917874331.ref@mail.yahoo.com>
 <232159068.980863.1589917874331@mail.yahoo.com>
 <CAGx1TMAZA7sJBZ343MEXwD=eZCM0V9_VLTSjw+QEvbniqDCepA@mail.gmail.com>
 <1667554344.1012305.1589920704261@mail.yahoo.com>
 <79e7fdeb-fe14-dcc0-f998-1c7307eb759b@sapo.pt>
 <1253484374.1044092.1589923503225@mail.yahoo.com>
 <CAGx1TMD+G50EERDDVXFvVmHk9A-cVCS61Re0xs28wKcC1AUCNA@mail.gmail.com>
Message-ID: <54F3846C-3A99-4C41-9F54-B16FE186C6F3@dcn.davis.ca.us>

Works for me.

set.seed( 42 )
a <- c(2,4,3,4,6,5,3,1,2,3,4,3,4,5,65)
b <- c(23,45,32,12,23,43,56,44,33,11,12,54,23,34,54)
d <- c(9,4,5,3,2,1,3,4,5,6,4,9,10,11,18)

my.experiment <- function() {
  a <- a + rnorm( length( a ), 0, 0.05 )
  b <- b + rnorm( length( b ), 0, 0.05 )
  d <- d + rnorm( length( d ), 0, 0.05 )
  OLS <- lm( a ~ b+d )
  MSE_OLS <- mean( OLS$residuals^2 )
  MSE_OLS
}

my.data <- replicate( 500
                    , my.experiment()
                    )
mean( my.data )


On May 19, 2020 2:30:09 PM PDT, "Richard M. Heiberger" <rmh at temple.edu> wrote:
>you need to pay attention to the intermediate structures that you
>generate.
>If all you want is one number, then that is what you should create
>
>mean(replicate( 500, my.experiment() ))
>
>Since you do seem to want to store the intermediate values, then you
>must
>name
>the object according to its structure.  A one-dimensional vector takes
>names(), not dimnames().
>A two-dimensional structure takes dimnames, and the number of row names
>and
>the number of column
>names must match the number of rows and columns.
>
>
>On Tue, May 19, 2020 at 5:25 PM varin sacha <varinsacha at yahoo.fr>
>wrote:
>
>> Hi Rui,
>>
>> If I don't transpose t() the output of the replicate (my R code here
>> below) I still get an error message !!
>>
>> ########################################
>> a=c(2,4,3,4,6,5,3,1,2,3,4,3,4,5,65)
>> b=c(23,45,32,12,23,43,56,44,33,11,12,54,23,34,54)
>> d=c(9,4,5,3,2,1,3,4,5,6,4,9,10,11,18)
>>
>> my.experiment <- function() {
>>
>> OLS <- lm( a ~ b+d )
>>
>> MSE_OLS<-mean(OLS$residuals^2)
>>
>> return( c(MSE_OLS) )
>> }
>>
>> my.data = replicate( 500, my.experiment() )
>> colnames(my.data) <- c("MSE_OLS")
>> mean(my.data)
>> ########################################
>>
>>
>>
>>
>>
>>
>> Le mardi 19 mai 2020 ? 23:14:21 UTC+2, Rui Barradas
><ruipbarradas at sapo.pt>
>> a ?crit :
>>
>>
>>
>>
>>
>> Hello,
>>
>> Inline.
>>
>> ?s 21:38 de 19/05/20, varin sacha via R-help escreveu:
>> >
>> > Hi Richard,
>> >
>> > Thanks for your response.
>> > However, how can I correct my R code knowing that I want, as a
>result,
>> only one value : the mean of the 500 MSE_OLS values ?
>>
>> Just don't transpose the output of replicate?
>>
>> Hope this helps,
>>
>> Rui Barradas
>>
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> > Le mardi 19 mai 2020 ? 21:59:07 UTC+2, Richard M. Heiberger <
>> rmh at temple.edu> a ?crit :
>> >
>> >
>> >
>> >
>> >
>> >> dim(my.data)
>> > [1]   1 500
>> >
>> > you have a matrix with a single row and 500 columns.
>> > you gave a name to only the first column.
>> >
>> > Look at the result of replicate().  it is a vector.  You transposed
>it
>> into a one-row matrix.
>> >
>> >>   tmp <- replicate( 500, my.experiment() )
>> >> dim(tmp)
>> > NULL
>> >> length(tmp)
>> > [1] 500
>> >> dim(t(tmp))
>> > [1]   1 500
>> >
>> >
>> > On Tue, May 19, 2020 at 3:51 PM varin sacha via R-help <
>> r-help at r-project.org> wrote:
>> >> Dear R-experts,
>> >>
>> >> Here is my R code, I get a result but I also get an error message
>so I
>> doubt I can trust the result I get.
>> >> What is going wrong ? Many thanks.
>> >>
>> >> ########################################
>> >> a<-c(2,4,3,4,6,5,3,1,2,3,4,3,4,5,65)
>> >> b<-c(23,45,32,12,23,43,56,44,33,11,12,54,23,34,54)
>> >> d<-c(9,4,5,3,2,1,3,4,5,6,4,9,10,11,18)
>> >>
>> >> my.experiment <- function( ) {
>> >>
>> >> OLS <- lm( a ~ b+d )
>> >> MSE_OLS<-mean(OLS$residuals^2)
>> >> return( c(MSE_OLS) )
>> >> }
>> >>
>> >> my.data = t(replicate( 500, my.experiment() ))
>> >> colnames(my.data) <- c("MSE_OLS")
>> >> mean(my.data)
>> >> ########################################
>> >>
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >>
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From L@urentRHe|p @end|ng |rom |ree@|r  Tue May 19 09:07:38 2020
From: L@urentRHe|p @end|ng |rom |ree@|r (Laurent Rhelp)
Date: Tue, 19 May 2020 09:07:38 +0200
Subject: [R] iterators : checkFunc with ireadLines
In-Reply-To: <E2222FD6-1A53-4C9C-87BA-7AEFDB3C83BB@dcn.davis.ca.us>
References: <72174dd3-a819-b5fd-5a6f-2a6b46342774-2015@free.fr>
 <CAA99HCwtOek_jaNsD6-=CtTRoL9OHYgDqfgdSAfbNBaEZ_w8JQ@mail.gmail.com>
 <a1ad0aec-7457-b516-1a3c-43e9e83d0ae1@free.fr>
 <CAA99HCx_VvNJzTQ=mQ7zoRFX5ZJOA3mCgbDkMiCSPAH_6Zv=QA-7372@mail.gmail.com>
 <E2222FD6-1A53-4C9C-87BA-7AEFDB3C83BB@dcn.davis.ca.us>
Message-ID: <f0a8e687-c79e-c748-564a-f93cc3fdee23-6107@free.fr>


Ok, thank you for the advice I will take some time to see in details 
these packages.


Le 19/05/2020 ? 05:44, Jeff Newmiller a ?crit?:
> Laurent... Bill is suggesting building your own indexed database... but this has been done before, so re-inventing the wheel seems inefficient and risky. It is actually impossible to create such a beast without reading the entire file into memory at least temporarily anyway, so you are better off looking at ways to process the entire file efficiently.
>
> For example, you could load the data into a sqlite database in a couple of lines of code and use SQL directly or use the sqldf data frame interface, or use dplyr to query the database.
>
> Or you could look at read_csv_chunked from readr package.
>
> On May 18, 2020 11:37:46 AM PDT, William Michels via R-help <r-help at r-project.org> wrote:
>> Hi Laurent,
>>
>> Thank you for explaining your size limitations. Below is an example
>> using the read.fwf() function to grab the first column of your input
>> file (in 2000 row chunks). This column is converted to an index, and
>> the index is used to create an iterator useful for skipping lines when
>> reading input with scan(). (You could try processing your large file
>> in successive 2000 line chunks, or whatever number of lines fits into
>> memory). Maybe not as elegant as the approach you were going for, but
>> read.fwf() should be pretty efficient:
>>
>>> sensors <-  c("N053", "N163")
>>> read.fwf("test2.txt", widths=c(4), as.is=TRUE, flush=TRUE, n=2000,
>> skip=0)
>>     V1
>> 1 Time
>> 2 N023
>> 3 N053
>> 4 N123
>> 5 N163
>> 6 N193
>>> first_col <- read.fwf("test2.txt", widths=c(4), as.is=TRUE,
>> flush=TRUE, n=2000, skip=0)
>>> which(first_col$V1 %in% sensors)
>> [1] 3 5
>>> index1 <- which(first_col$V1 %in% sensors)
>>> iter_index1 <- iter(1:2000, checkFunc= function(n) {n %in% index1})
>>> unlist(scan(file="test2.txt",
>> what=list("","","","","","","","","",""), flush=TRUE, multi.line=FALSE,
>> skip=nextElem(iter_index1)-1, nlines=1, quiet=TRUE))
>> [1] "N053"      "-0.014083" "-0.004741" "0.001443"  "-0.010152"
>> "-0.012996" "-0.005337" "-0.008738" "-0.015094" "-0.012104"
>>> unlist(scan(file="test2.txt",
>> what=list("","","","","","","","","",""), flush=TRUE, multi.line=FALSE,
>> skip=nextElem(iter_index1)-1, nlines=1, quiet=TRUE))
>> [1] "N163"      "-0.054023" "-0.049345" "-0.037158" "-0.04112"
>> "-0.044612" "-0.036953" "-0.036061" "-0.044516" "-0.046436"
>> (Note for this email and the previous one, I've deleted the first
>> "hash" character from each line of your test file for clarity).
>>
>> HTH, Bill.
>>
>> W. Michels, Ph.D.
>>
>>
>>
>>
>>
>> On Mon, May 18, 2020 at 3:35 AM Laurent Rhelp <LaurentRHelp at free.fr>
>> wrote:
>>> Dear William,
>>>    Thank you for your answer
>>> My file is very large so I cannot read it in my memory (I cannot use
>>> read.table). So I want to put in memory only the line I need to
>> process.
>>> With readLines, as I did, it works but I would like to use an
>> iterator
>>> and a foreach loop to understand this way to do because I thought
>> that
>>> it was a better solution to write a nice code.
>>>
>>>
>>> Le 18/05/2020 ? 04:54, William Michels a ?crit :
>>>> Apologies, Laurent, for this two-part answer. I misunderstood your
>>>> post where you stated you wanted to "filter(ing) some
>>>> selected lines according to the line name... ." I thought that
>> meant
>>>> you had a separate index (like a series of primes) that you wanted
>> to
>>>> use to only read-in selected line numbers from a file (test file
>> below
>>>> with numbers 1:1000 each on a separate line):
>>>>
>>>>> library(gmp)
>>>>> library(iterators)
>>>>> iprime <- iter(1:100, checkFunc = function(n) isprime(n))
>>>>> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
>>>> Read 1 item
>>>> [1] 2
>>>>> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
>>>> Read 1 item
>>>> [1] 3
>>>>> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
>>>> Read 1 item
>>>> [1] 5
>>>>> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
>>>> Read 1 item
>>>> [1] 7
>>>> However, what it really seems that you want to do is read each line
>> of
>>>> a (possibly enormous) file, test each line "string-wise" to keep or
>>>> discard, and if you're keeping it, append the line to a list. I can
>>>> certainly see the advantage of this strategy for reading in very,
>> very
>>>> large files, but it's not clear to me how the "ireadLines" function
>> (
>>>> in the "iterators" package) will help you, since it doesn't seem to
>>>> generate anything but a sequential index.
>>>>
>>>> Anyway, below is an absolutely standard read-in of your data using
>>>> read.table(). Hopefully some of the code I've posted has been
>> useful
>>>> to you.
>>>>
>>>>> sensors <-  c("N053", "N163")
>>>>> read.table("test2.txt")
>>>>       V1        V2        V3        V4        V5        V6        V7
>>>>      V8        V9       V10
>>>> 1 Time  0.000000  0.000999  0.001999  0.002998  0.003998  0.004997
>>>> 0.005997  0.006996  0.007996
>>>> 2 N023 -0.031323 -0.035026 -0.029759 -0.024886 -0.024464 -0.026816
>>>> -0.033690 -0.041067 -0.038747
>>>> 3 N053 -0.014083 -0.004741  0.001443 -0.010152 -0.012996 -0.005337
>>>> -0.008738 -0.015094 -0.012104
>>>> 4 N123 -0.019008 -0.013494 -0.013180 -0.029208 -0.032748 -0.020243
>>>> -0.015089 -0.014439 -0.011681
>>>> 5 N163 -0.054023 -0.049345 -0.037158 -0.041120 -0.044612 -0.036953
>>>> -0.036061 -0.044516 -0.046436
>>>> 6 N193 -0.022171 -0.022384 -0.022338 -0.023304 -0.022569 -0.021827
>>>> -0.021996 -0.021755 -0.021846
>>>>> Laurent_data <- read.table("test2.txt")
>>>>> Laurent_data[Laurent_data$V1 %in% sensors, ]
>>>>       V1        V2        V3        V4        V5        V6        V7
>>>>      V8        V9       V10
>>>> 3 N053 -0.014083 -0.004741  0.001443 -0.010152 -0.012996 -0.005337
>>>> -0.008738 -0.015094 -0.012104
>>>> 5 N163 -0.054023 -0.049345 -0.037158 -0.041120 -0.044612 -0.036953
>>>> -0.036061 -0.044516 -0.046436
>>>>
>>>> Best, Bill.
>>>>
>>>> W. Michels, Ph.D.
>>>>
>>>>
>>>> On Sun, May 17, 2020 at 5:43 PM Laurent Rhelp
>> <LaurentRHelp at free.fr> wrote:
>>>>> Dear R-Help List,
>>>>>
>>>>>       I would like to use an iterator to read a file filtering some
>>>>> selected lines according to the line name in order to use after a
>>>>> foreach loop. I wanted to use the checkFunc argument as the
>> following
>>>>> example found on internet to select only prime numbers :
>>>>>
>>>>> |                                iprime <- ||iter||(1:100,
>> checkFunc =
>>>>> ||function||(n) ||isprime||(n))|
>>>>>
>>>>> |(https://datawookie.netlify.app/blog/2013/11/iterators-in-r/)
>>>>> <https://datawookie.netlify.app/blog/2013/11/iterators-in-r/>|
>>>>>
>>>>> but the checkFunc argument seems not to be available with the
>> function
>>>>> ireadLines (package iterators). So, I did the code below to solve
>> my
>>>>> problem but I am sure that I miss something to use iterators with
>> files.
>>>>> Since I found nothing on the web about ireadLines and the
>> checkFunc
>>>>> argument, could somebody help me to understand how we have to use
>>>>> iterator (and foreach loop) on files keeping only selected lines ?
>>>>>
>>>>> Thank you very much
>>>>> Laurent
>>>>>
>>>>> Presently here is my code:
>>>>>
>>>>> ##        mock file to read: test.txt
>>>>> ##
>>>>> # Time    0    0.000999    0.001999    0.002998    0.003998
>> 0.004997
>>>>> 0.005997    0.006996    0.007996
>>>>> # N023    -0.031323    -0.035026    -0.029759    -0.024886
>> -0.024464
>>>>> -0.026816    -0.03369    -0.041067    -0.038747
>>>>> # N053    -0.014083    -0.004741    0.001443    -0.010152
>> -0.012996
>>>>> -0.005337    -0.008738    -0.015094    -0.012104
>>>>> # N123    -0.019008    -0.013494    -0.01318    -0.029208
>> -0.032748
>>>>> -0.020243    -0.015089    -0.014439    -0.011681
>>>>> # N163    -0.054023    -0.049345    -0.037158    -0.04112
>> -0.044612
>>>>> -0.036953    -0.036061    -0.044516    -0.046436
>>>>> # N193    -0.022171    -0.022384    -0.022338    -0.023304
>> -0.022569
>>>>> -0.021827    -0.021996    -0.021755    -0.021846
>>>>>
>>>>>
>>>>> # sensors to keep
>>>>>
>>>>> sensors <-  c("N053", "N163")
>>>>>
>>>>>
>>>>> library(iterators)
>>>>>
>>>>> library(rlist)
>>>>>
>>>>>
>>>>> file_name <- "test.txt"
>>>>>
>>>>> con_obj <- file( file_name , "r")
>>>>> ifile <- ireadLines( con_obj , n = 1 )
>>>>>
>>>>>
>>>>> ## I do not do a loop for the example
>>>>>
>>>>> res <- list()
>>>>>
>>>>> r <- get_Lines_iter( ifile , sensors)
>>>>> res <- list.append( res , r )
>>>>> res
>>>>> r <- get_Lines_iter( ifile , sensors)
>>>>> res <- list.append( res , r )
>>>>> res
>>>>> r <- get_Lines_iter( ifile , sensors)
>>>>> do.call("cbind",res)
>>>>>
>>>>> ## the function get_Lines_iter to select and process the line
>>>>>
>>>>> get_Lines_iter  <-  function( iter , sensors, sep = '\t', quiet =
>> FALSE){
>>>>>      ## read the next record in the iterator
>>>>>      r = try( nextElem(iter) )
>>>>>     while(  TRUE ){
>>>>>        if( class(r) == "try-error") {
>>>>>              return( stop("The iterator is empty") )
>>>>>       } else {
>>>>>       ## split the read line according to the separator
>>>>>        r_txt <- textConnection(r)
>>>>>        fields <- scan(file = r_txt, what = "character", sep = sep,
>> quiet =
>>>>> quiet)
>>>>>         ## test if we have to keep the line
>>>>>         if( fields[1] %in% sensors){
>>>>>           ## data processing for the selected line (for the example
>>>>> transformation in dataframe)
>>>>>           n <- length(fields)
>>>>>           x <- data.frame( as.numeric(fields[2:n]) )
>>>>>           names(x) <- fields[1]
>>>>>           ## We return the values
>>>>>           print(paste0("sensor ",fields[1]," ok"))
>>>>>           return( x )
>>>>>         }else{
>>>>>          print(paste0("Sensor ", fields[1] ," not selected"))
>>>>>          r = try(nextElem(iter) )}
>>>>>       }
>>>>> }# end while loop
>>>>> }
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e
>> par le logiciel antivirus Avast.
>>>>> https://www.avast.com/antivirus
>>>>>
>>>>>           [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>> --
>>> L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e par
>> le logiciel antivirus Avast.
>>> https://www.avast.com/antivirus
>>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.



-- 
L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e par le logiciel antivirus Avast.
https://www.avast.com/antivirus


From btupper @end|ng |rom b|ge|ow@org  Tue May 19 14:45:47 2020
From: btupper @end|ng |rom b|ge|ow@org (Ben Tupper)
Date: Tue, 19 May 2020 08:45:47 -0400
Subject: [R] Help with ggplot error: #Error in FUN(X[[i]],
 ...) : object 'x' not found
In-Reply-To: <BYAPR06MB53835FCA0A4E629E6C4048EAAEB90@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB53835FCA0A4E629E6C4048EAAEB90@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <CALrbzg1bPZGtUK9BYAZUWQB3pFE_C-783xmDR=Zf7AfgFaVROQ-4928@mail.gmail.com>

Hi,

When you add a new layer to a plot and you provide new data, wouldn't it
make sense to provide aes() with some indication of the names of the
variables to use for coordinate plotting?  You have provided a grouping
variable for the paths, but what of coordinates x and y?  How could
geom_path() know that your lon/lat variables are to be used as x and y
coordinates in the plot?

Cheers,
Ben



On Tue, May 19, 2020 at 8:29 AM Poling, William via R-help <
r-help at r-project.org> wrote:

> #RStudio Version Version 1.2.1335
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit)
> #Running under: Windows 10 x64 (build 17763)
>
> Good morning.
>
> I am testing a small sample of my data using an example found here:
> #
> https://www.r-orms.org/mixed-integer-linear-programming/practicals/problem-tsp/
>
> If needed I can provide my complete script which is simply the same as the
> example only using a few rows from my data, let me know, as I think this is
> just a ggplot issue.
>
> I am getting this error when I use ggplot
> #Error in FUN(X[[i]], ...) : object 'x' not found
>
> 1. I have reviewed the following with no success.
> #Sources referred to for help
> #https://github.com/business-science/anomalize/issues/2
> #
> https://stackoverflow.com/questions/38988028/error-in-funxi-object-x-not-found
>
> #https://community.rstudio.com/t/error-in-fun-x-i-object-variable-not-found/62532/3
> #https://www.neonscience.org/packages-in-r
> <https://stackoverflow.com/questions/38988028/error-in-funxi-object-x-not-found#https://community.rstudio.com/t/error-in-fun-x-i-object-variable-not-found/62532/3%23https://www.neonscience.org/packages-in-r>
>
> 2. I have rebooted and re-installed ggplot2 for updates
>
> 3. These are the only libraries running
> library(knitr)
> library(dplyr)
> library(ggplot2)
> library(ggmap)
> library(ompr)
> library(ompr.roi)
> library(ROI.plugin.glpk)
>
> Here is the original plot attempt:
> ggplot(c1members2, aes(x , y)) +
>   geom_point() +
>   geom_line(data = paths, aes(group = trip_id)) + #Error in FUN(X[[i]],
> ...) : object 'x' not found
>   ggtitle(paste0("Optimal route with cost: ",
> round(objective_value(result), 2)))
>
> #I have gone through each of the steps one by one and error occurs at -->
> geom_line(data = paths, aes(group = trip_id))
>
> I have also tried the following:
> Test1
> p <- ggplot(c1members2, aes(x,y)) +
>   geom_point()
>
> p +  geom_line(data = paths, aes(group = trip_id),inherit.aes = FALSE) +
> #Error in FUN(X[[i]], ...) : object 'x' not found
>   ggtitle(paste0("Optimal route with cost: ",
> round(objective_value(result), 2)))
>
> Test2
> ggplot(c1members2, aes(x,y)) +
>   geom_point() +
> geom_line(data = paths, aes(group = trip_id),inherit.aes = TRUE) + #Error
> in FUN(X[[i]], ...) : object 'x' not found
>   ggtitle(paste0("Optimal route with cost: ",
> round(objective_value(result), 2)))
>
> Here are the data involved.
> str(c1members2)
> 'data.frame':   7 obs. of  3 variables:
>  $ id: int  1 2 3 4 5 6 7
>  $ x : num  95.5 90.2 46.8 40.9 43.3 ...
>  $ y : num  4.65 47.57 8.07 38.43 28.59 ...
>
> str(paths)
> data.frame':    14 obs. of  6 variables:
>  $ trip_id  : int  1 2 3 4 5 6 7 1 2 3 ...
>  $ property : chr  "from" "from" "from" "from" ...
>  $ idx_val  : int  3 1 5 7 6 4 2 1 2 3 ...
>  $ Longitude: num  -122 -122 -122 -122 -122 ...
>  $ Latitude : num  47.3 47.2 47.3 47.6 47.6 ...
>  $ city     : Factor w/ 1337 levels "ABBOTTSTOWN",..: 1169 1169 1169 1069
> 1069 1069 1069 1169 1069 1169 ...
>
> Here are the dput()'s
>
> I hope I have provided enough information to elicit a response.
>
> Thank you.
>
> WHP
>
> dput(c1members2)
> structure(list(id = 1:7, x = c(95.521828085515, 90.2272877161736,
> 46.8465689660691, 40.8959155605829, 43.2591531526978, 23.7485886058345,
> 64.0406297698986), y = c(4.64803485876031, 47.5678629996171,
> 8.0689847051367, 38.4256264258875, 28.5930051512296, 43.5934562642672,
> 42.415566619041)), class = "data.frame", row.names = c(NA, -7L
>
> dput(paths) #Although I am only using city %in% c("SEATTLE","TACOMA"))
> from the original 2353 rows = 7 rows in c1members2
> structure(list(trip_id = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 1L, 2L,
> 3L, 4L, 5L, 6L, 7L), property = c("from", "from", "from", "from",
> "from", "from", "from", "to", "to", "to", "to", "to", "to", "to"
> ), idx_val = c(3L, 1L, 5L, 7L, 6L, 4L, 2L, 1L, 2L, 3L, 4L, 5L,
> 6L, 7L), Longitude = c(-122.490417, -122.429323, -122.358115,
> -122.37003, -122.41282, -122.299261, -122.390815, -122.429323,
> -122.390815, -122.490417, -122.299261, -122.358115, -122.41282,
> -122.37003), Latitude = c(47.274599, 47.176596, 47.276776, 47.565667,
> 47.646222, 47.673056, 47.549377, 47.176596, 47.549377, 47.274599,
> 47.673056, 47.276776, 47.646222, 47.565667), city = structure(c(1169L,
> 1169L, 1169L, 1069L, 1069L, 1069L, 1069L, 1169L, 1069L, 1169L,
> 1069L, 1169L, 1069L, 1069L), .Label = c("ABBOTTSTOWN", "ABILENE",
> "ACWORTH", "ADAMS", "ADDISON", "ADKINS", "ALBANY", "ALBIA", "ALBUQUERQUE",
> "ALEXANDRIA", "ALFRED", "ALIQUIPPA", "ALISO VIEJO", "ALLEN PARK",
> "ALLENTOWN", "ALPHA", "ALPHARETTA", "ALPINE", "ALTOONA", "AMARILLO",
> "AMBLER", "AMBRIDGE", "AMHERST", "AMITYVILLE", "ANAHIEM", "ANDERSON",
> "ANDOVER", "ANGIER", "ANGLETON", "ANKENY", "ANN ARBOR", "ANZA",
> "APEX", "APOLLO", "ARCADIA", "ARCHDALE", "ARDMORE", "ARGYLE",
> "ARKANSAS CITY", "ARLINGTON", "ARNOLD", "ARTHUR", "ASHEBORO",
> "ASHEVILLE", "ASHLAND", "ASHLAND CITY", "ASHTON", "ASTON", "ATHENS",
> "ATLANTA", "ATLANTIC BCH", "AUBORN", "AUGUSTA", "AURORA", "AUSTELL",
> "AUSTIN", "AVENTURA", "AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN CITY",
> "BALDWIN PLACE", "BALLWIN", "BANGOR", "BARBOURSVILLE", "BARNEGAT",
> "BARTLETT", "BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", "BATESBURG",
> "BATON ROUGE", "BAXTER SPRINGS", "BAY CITY", "BAYONNE", "BAYSIDE",
> "BAYTOWN", "BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER CITY",
> "BELFAIR", "BELLE VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR",
> "BELLMORE", "BELLWOOD", "BELMAR", "BELMONT", "BELVIDERE", "BENNET",
> "BENTON", "BENTONVILLE", "BERLIN", "BESSEMER CITY", "BETHANY",
> "BETHEL", "BETHEL PARK", "BETHESDA", "BETHLEHEM", "BETTENDORF",
> "BIGLERVILLE", "BINGHAMTON", "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN",
> "BLACKSBURG", "BLAINE", "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD",
> "BLUE BELL", "BLUE GRASS", "BLUE POINT", "BLUE SPRINGS", "BOCA RATON",
> "BODFISH", "BOILING SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS",
> "BONNEY LAKE", "BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING GREEN",
> "BOYERTOWN", "BOYNTON BEACH", "BRADENTON", "BRADENVILLE", "BRANDAMORE",
> "BRANDON", "BRANFORD", "BRANSON", "BRASELTON", "BREEZEWOOD",
> "BRENTON", "BRENTWOOD", "BREWSTER", "BRICK", "BRIDGEPORT", "BRIDGETON",
> "BRIGHAM CITY", "BROADDUS", "BROCKPORT", "BROGUE", "BROKEN ARROW",
> "BRONX", "BROOKFIELD", "BROOKHAVEN", "BROOKLYN", "BROWNS MILLS",
> "BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", "BRYAN", "BUCKSPORT",
> "BUDD LAKE", "BUFFALO", "BULVERDE", "BUNKERVILLE", "BURBANK",
> "BURFORD", "BURIEN", "BURLINGTON", "BURR RIDGE", "BURTON", "BUSKIRK",
> "BUTLER", "BUXTON", "BYRON", "CALERA", "CAMBRIDGE", "CAMP HILL",
> "CAMPBELL", "CANAAN", "CANAL WINCHESTER", "CANASTOTA", "CANTON",
> "CARDIFF BY THE SEA", "CARL JUNCTION", "CARLISLE", "CARLTON",
> "CAROL STREAM", "CARROLLTON", "CARSON CITY", "CARTERSVILLE",
> "CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", "CASWELL", "CATO",
> "CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER POINT", "CENTERTON",
> "CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN FALLS", "CHALFONT",
> "CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", "CHARLESTON",
> "CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", "CHESHIRE",
> "CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", "CHILLICOTHE",
> "CHOCTAW", "CICERO", "CINCINNATI", "CLAY", "CLEARWATER", "CLEARWATER
> BEACH",
> "CLEMMONS", "CLENDENIN", "CLEVELAND", "CLEVELAND HTS", "CLEVES",
> "CLIFFSIDE PARK", "CLIFTON", "CLIFTON PARK", "CLIFTON SPGS",
> "CLINTON", "COACHELLA", "COAL TOWNSHIP", "COATESVILLE", "COLLEGE STATION",
> "COLLINSVILLE", "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE",
> "CONCORD", "CONCORD TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY",
> "CONYERS", "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA",
> "CORNELIUS", "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY",
> "COTTAGE HILLS", "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN",
> "CRANBERRY TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER",
> "CUMBERLAND FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS",
> "DALLAS", "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT",
> "DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR",
> "DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH",
> "DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY",
> "DES MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER",
> "DIXON", "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER",
> "DOYLESTOWN", "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH",
> "DUNCAN", "DUNCANNON", "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE",
> "DURHAM", "DUTTON", "EAGLE GROVE", "EAST AURORA", "EAST CHICAGO",
> "EAST EARL", "EAST HANOVER", "EAST HARTFORD", "EAST HARTLAND",
> "EAST LEROY", "EAST LIVERPOOL", "EAST MEADOW", "EAST NORTHPORT",
> "EAST ORANGE", "EAST PEORIA", "EAST SAINT LOUIS", "EASTON", "EDGERTON",
> "EDISON", "EDMOND", "EGG HBR TWP", "EL DORADO", "EL PASO", "ELDRIDGE",
> "ELGIN", "ELIZABETHVILLE", "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY",
> "ELMHURST", "ELMWOOD PARK", "ELOY", "ELY", "EMERSON", "EMORY",
> "ENDICOTT", "ENGLEWOOD", "ENID", "ENNICE", "ENOLA", "ENUMCLAW",
> "EPHRATA", "ERIE", "ESTERO", "EUREKA SPRINGS", "EVANSVILLE",
> "EVERETT", "EXCELSIOR SPRINGS", "EXTON", "FAIRFAX", "FAIRFIELD",
> "FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", "FALMOUTH", "FAR HILLS",
> "FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", "FAYETTE", "FAYETTEVILLE",
> "FELTON", "FENTON", "FERGUSON", "FIELDALE", "FINDLAY", "FINGERVILLE",
> "FLEMINGTON", "FLINT", "FLORENCE", "FLORESVILLE", "FLORISSANT",
> "FLOVILLA", "FLOWER MOUND", "FLUSHING", "FOGELSVILLE", "FOLSOM",
> "FONTANELLE", "FOREST HILLS", "FOREST PARK", "FORT DODGE", "FORT
> FAIRFIELD",
> "FORT GAY", "FORT LAUDERDALE", "FORT LEE", "FORT MILL", "FORT MOHAVE",
> "FORT PAYNE", "FORT PIERCE", "FORT SMITH", "FORT WAYNE", "FORT WORTH",
> "FOSTORIA", "FOUNTAIN INN", "FRANKFORT", "FRANKLIN", "FREEDOM",
> "FREEHOLD", "FREEPORT", "FREMONT", "FULLERTON", "FULSHEAR", "FULTON",
> "GADSDEN", "GAFFNEY", "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE",
> "GARDNERS", "GARNETT VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA",
> "GETTYSBURG", "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE",
> "GLEN CARBON", "GLEN COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW",
> "GLENWOOD", "GOLDEN CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM",
> "GOSHEN", "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS",
> "GRANDVIEW", "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS",
> "GRASS VALLEY", "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER",
> "GREENLAWN", "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE",
> "GREER", "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD",
> "HADLEY", "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON",
> "HAMBURG", "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE",
> "HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON",
> "HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL",
> "HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE",
> "HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH",
> "HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE",
> "HIGH HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS",
> "HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE",
> "HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD",
> "HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS",
> "HOT SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON",
> "HUFFMAN", "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON",
> "HUNTINGTON BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE",
> "HURST", "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL",
> "INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY",
> "IRMO", "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP",
> "ISSAQUAH", "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS",
> "JACKSONVILLE", "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE",
> "JEFFERSON", "JEFFERSON CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN",
> "JONESBORO", "JONESPORT", "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE",
> "KANNAPOLIS", "KANSAS CITY", "KATY", "KEARNEY", "KELLERTON",
> "KEMAH", "KENMORE", "KENNESAW", "KENT", "KINGMAN", "KINGS MOUNTAIN",
> "KINGSLAND", "KINGSTON", "KINGWOOD", "KIRKLAND", "KUTZTOWN",
> "LA QUINTA", "LA RUE", "LA VERNE", "LAFAYETTE", "LAKE BLUFF",
> "LAKE IN THE HILL", "LAKE ISABELLA", "LAKE PLACID", "LAKE SAINT LOUIS",
> "LAKE TAPPS", "LAKE VILLAGE", "LAKE WAUKOMIS", "LAKE WORTH",
> "LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD RCH", "LAMBERTVILLE",
> "LANCASTER", "LANDISVILLE", "LANSDALE", "LAREDO", "LARGO", "LAS VEGAS",
> "LATROBE", "LAUDERHILL", "LAWNDALE", "LAWRENCE", "LAWRENCEVILLE",
> "LE MARS", "LE ROY", "LEAGUE CITY", "LEAVENWORTH", "LEAWOOD",
> "LEBANON", "LECOMPTON", "LEECHBURG", "LEES SUMMIT", "LEESBURG",
> "LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", "LEVITTOWN", "LEWES",
> "LEWISTOWN", "LEXINGTON", "LIBERTY TWP", "LILLINGTON", "LINCOLN",
> "LINDENHURST", "LINTON", "LISBON", "LITCHFIELD PK", "LITHIA",
> "LITITZ", "LITTLE EGG HARBOR TO", "LK FOREST PK", "LK PEEKSKILL",
> "LOCKHART", "LOCKWOOD", "LOGAN", "LOMA LINDA", "LONE JACK", "LONG BEACH",
> "LONG LANE", "LONGVIEW", "LORAIN", "LORTON", "LOS ANGELES", "LOUISBURG",
> "LOVES PARK", "LOWER BURRELL", "LOWRY CITY", "LUBBOCK", "LUGOFF",
> "LUMBERTON", "LYNBROOK", "LYNNWOOD", "MACHESNEY PARK", "MACON",
> "MAGNOLIA", "MAHOPAC", "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE",
> "MANASSAS", "MANCHACA", "MANCHESTER", "MANCHESTER TOWNSHIP",
> "MANCHESTER TW", "MANHEIM", "MANITO", "MANLIUS", "MANNINGTON",
> "MANSFIELD", "MANSURA", "MAPLE PARK", "MARBLE FALLS", "MARCELLUS",
> "MARICOPA", "MARIETTA", "MARION", "MARKLEYSBURG", "MARS HILL",
> "MARSHALLTOWN", "MARSHFIELD", "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE",
> "MASON CITY", "MASSEY", "MASSILLON", "MASURY", "MATAWAN", "MATEWAN",
> "MATTAPOISETT", "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND",
> "MC DONALD", "MC KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN",
> "MCALESTER", "MCDONOUGH", "MCKEESPORT", "MCKINNEY", "MECHANICSBURG",
> "MECHANICSVILLE", "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS",
> "MERIDEN", "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA",
> "MESQUITE", "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN",
> "MIDLAND", "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK",
> "MILLEDGEVILLE", "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE",
> "MINEOLA", "MINERAL SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION",
> "MISSION HILLS", "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY",
> "MOLINE", "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE",
> "MONTCLAIR", "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON",
> "MORGANTOWN", "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE",
> "MOUNT JOY", "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO",
> "MOUNT UNION", "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT",
> "MULVANE", "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT",
> "MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS",
> "NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES",
> "NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG",
> "NEW BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK",
> "NEW CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON",
> "NEW LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE",
> "NEW ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK",
> "NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT",
> "NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA",
> "NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK",
> "NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON",
> "NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND HILLS",
> "NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE",
> "NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON",
> "OAK RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE",
> "OMAHA", "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION",
> "ORONO", "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO",
> "OTTAWA", "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD",
> "OYSTER BAY", "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR",
> "PALMDALE", "PALMYRA", "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE",
> "PARLIN", "PARMA", "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY",
> "PAXTON", "PEA RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN",
> "PELION", "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE",
> "PENNSBURG", "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS",
> "PERRYSBURG", "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI",
> "PHOENIX", "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN",
> "PINE BLUFF", "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY",
> "PITMAN", "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY",
> "PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO LAKE",
> "POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", "PORT
> CHARLOTTE",
> "PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", "PORT WASHINGTON",
> "PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", "POTTSVILLE",
> "POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE VILLAGE",
> "PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT PLEASANT",
> "PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", "QUINCY",
> "RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", "READLYN",
> "RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO PARK",
> "REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", "RICH HILL",
> "RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", "RINCON",
> "RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", "RIVERSIDE",
> "ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", "ROCKAWAY BCH",
> "ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", "ROGERS",
> "ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE",
> "ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT",
> "ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN",
> "RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO",
> "SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR",
> "SAINT CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS",
> "SAINT PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY",
> "SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO",
> "SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN",
> "SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", "SARASOTA",
> "SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", "SAYVILLE",
> "SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", "SCOTT", "SCOTTSDALE",
> "SEAFORD", "SEALE", "SEATTLE", "SEDONA", "SEGUIN", "SELDEN",
> "SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", "SHARON", "SHARPSBURG",
> "SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", "SHELLSBURG", "SHELTON",
> "SHENANDOAH", "SHORELINE", "SHOREWOOD", "SHREVEPORT", "SICKLERVILLE",
> "SILER CITY", "SILOAM SPRINGS", "SIMON", "SIMPSONVILLE", "SIMSBURY",
> "SIOUX CITY", "SIOUX FALLS", "SKOKIE", "SKOWHEGAN", "SLATINGTON",
> "SLIDELL", "SMITHFIELD", "SN BERNRDNO", "SNELLVILLE", "SOCORRO",
> "SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH BELOIT", "SOUTH BEND",
> "SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH PARK", "SOUTH PLAINFIELD",
> "SOUTH SIOUX CITY", "SOUTHAMPTON", "SOUTHBURY", "SOUTHGATE",
> "SPARKS", "SPARROW BUSH", "SPENCER", "SPRING", "SPRING BRANCH",
> "SPRING GROVE", "SPRING HILL", "SPRING HOPE", "SPRING VALLEY",
> "SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD GARDENS", "SPRINGHILL",
> "SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", "ST PETERSBURG",
> "ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", "STANLEY",
> "STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", "STILLMAN VALLEY",
> "STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", "STRAFFORD",
> "STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", "SUFFIELD",
> "SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", "SUMMIT",
> "SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", "SWEENY",
> "SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", "TAFTVILLE",
> "TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", "TAYLORVILLE",
> "TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", "THOMASVILLE",
> "TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY PARK",
> "TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", "TOPEKA",
> "TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS RST",
> "TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER",
> "TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY",
> "UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA",
> "URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO",
> "VAN BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO",
> "VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA",
> "VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH",
> "W TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN",
> "WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG",
> "WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA",
> "WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE",
> "WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER",
> "WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO",
> "WEST CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN",
> "WEST HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH",
> "WEST POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD",
> "WEST VALLEY CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER",
> "WESTMORELAND CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD",
> "WHARTON", "WHEELING", "WHITE PLAINS", "WHITEHALL", "WHITESTONE",
> "WHITESTOWN", "WHITING", "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD",
> "WILLARD", "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK",
> "WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS",
> "WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON",
> "WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE",
> "WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS",
> "WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH",
> "YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS",
> "YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class =
> "factor")), row.names = c(NA,
> -14L), class = "data.frame")
>
>
>
>
>
>
> Proprietary
>
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ben Tupper
Bigelow Laboratory for Ocean Science
East Boothbay, Maine
http://www.bigelow.org/
https://eco.bigelow.org

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue May 19 05:44:25 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 18 May 2020 20:44:25 -0700
Subject: [R] iterators : checkFunc with ireadLines
In-Reply-To: <CAA99HCx_VvNJzTQ=mQ7zoRFX5ZJOA3mCgbDkMiCSPAH_6Zv=QA-7372@mail.gmail.com>
References: <72174dd3-a819-b5fd-5a6f-2a6b46342774-2015@free.fr>
 <CAA99HCwtOek_jaNsD6-=CtTRoL9OHYgDqfgdSAfbNBaEZ_w8JQ@mail.gmail.com>
 <a1ad0aec-7457-b516-1a3c-43e9e83d0ae1@free.fr>
 <CAA99HCx_VvNJzTQ=mQ7zoRFX5ZJOA3mCgbDkMiCSPAH_6Zv=QA-7372@mail.gmail.com>
Message-ID: <E2222FD6-1A53-4C9C-87BA-7AEFDB3C83BB-6096@dcn.davis.ca.us>


Laurent... Bill is suggesting building your own indexed database... but this has been done before, so re-inventing the wheel seems inefficient and risky. It is actually impossible to create such a beast without reading the entire file into memory at least temporarily anyway, so you are better off looking at ways to process the entire file efficiently.

For example, you could load the data into a sqlite database in a couple of lines of code and use SQL directly or use the sqldf data frame interface, or use dplyr to query the database.

Or you could look at read_csv_chunked from readr package.

On May 18, 2020 11:37:46 AM PDT, William Michels via R-help <r-help at r-project.org> wrote:
>
>Hi Laurent,
>
>Thank you for explaining your size limitations. Below is an example
>using the read.fwf() function to grab the first column of your input
>file (in 2000 row chunks). This column is converted to an index, and
>the index is used to create an iterator useful for skipping lines when
>reading input with scan(). (You could try processing your large file
>in successive 2000 line chunks, or whatever number of lines fits into
>memory). Maybe not as elegant as the approach you were going for, but
>read.fwf() should be pretty efficient:
>
>> sensors <-  c("N053", "N163")
>> read.fwf("test2.txt", widths=c(4), as.is=TRUE, flush=TRUE, n=2000,
>skip=0)
>    V1
>1 Time
>2 N023
>3 N053
>4 N123
>5 N163
>6 N193
>> first_col <- read.fwf("test2.txt", widths=c(4), as.is=TRUE,
>flush=TRUE, n=2000, skip=0)
>> which(first_col$V1 %in% sensors)
>[1] 3 5
>> index1 <- which(first_col$V1 %in% sensors)
>> iter_index1 <- iter(1:2000, checkFunc= function(n) {n %in% index1})
>> unlist(scan(file="test2.txt",
>what=list("","","","","","","","","",""), flush=TRUE, multi.line=FALSE,
>skip=nextElem(iter_index1)-1, nlines=1, quiet=TRUE))
> [1] "N053"      "-0.014083" "-0.004741" "0.001443"  "-0.010152"
>"-0.012996" "-0.005337" "-0.008738" "-0.015094" "-0.012104"
>> unlist(scan(file="test2.txt",
>what=list("","","","","","","","","",""), flush=TRUE, multi.line=FALSE,
>skip=nextElem(iter_index1)-1, nlines=1, quiet=TRUE))
> [1] "N163"      "-0.054023" "-0.049345" "-0.037158" "-0.04112"
>"-0.044612" "-0.036953" "-0.036061" "-0.044516" "-0.046436"
>>
>
>(Note for this email and the previous one, I've deleted the first
>"hash" character from each line of your test file for clarity).
>
>HTH, Bill.
>
>W. Michels, Ph.D.
>
>
>
>
>
>On Mon, May 18, 2020 at 3:35 AM Laurent Rhelp <LaurentRHelp at free.fr>
>wrote:
>>
>> Dear William,
>>   Thank you for your answer
>> My file is very large so I cannot read it in my memory (I cannot use
>> read.table). So I want to put in memory only the line I need to
>process.
>> With readLines, as I did, it works but I would like to use an
>iterator
>> and a foreach loop to understand this way to do because I thought
>that
>> it was a better solution to write a nice code.
>>
>>
>> Le 18/05/2020 ? 04:54, William Michels a ?crit :
>> > Apologies, Laurent, for this two-part answer. I misunderstood your
>> > post where you stated you wanted to "filter(ing) some
>> > selected lines according to the line name... ." I thought that
>meant
>> > you had a separate index (like a series of primes) that you wanted
>to
>> > use to only read-in selected line numbers from a file (test file
>below
>> > with numbers 1:1000 each on a separate line):
>> >
>> >> library(gmp)
>> >> library(iterators)
>> >> iprime <- iter(1:100, checkFunc = function(n) isprime(n))
>> >> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
>> > Read 1 item
>> > [1] 2
>> >> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
>> > Read 1 item
>> > [1] 3
>> >> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
>> > Read 1 item
>> > [1] 5
>> >> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1, nlines=1)
>> > Read 1 item
>> > [1] 7
>> > However, what it really seems that you want to do is read each line
>of
>> > a (possibly enormous) file, test each line "string-wise" to keep or
>> > discard, and if you're keeping it, append the line to a list. I can
>> > certainly see the advantage of this strategy for reading in very,
>very
>> > large files, but it's not clear to me how the "ireadLines" function
>(
>> > in the "iterators" package) will help you, since it doesn't seem to
>> > generate anything but a sequential index.
>> >
>> > Anyway, below is an absolutely standard read-in of your data using
>> > read.table(). Hopefully some of the code I've posted has been
>useful
>> > to you.
>> >
>> >> sensors <-  c("N053", "N163")
>> >> read.table("test2.txt")
>> >      V1        V2        V3        V4        V5        V6        V7
>> >     V8        V9       V10
>> > 1 Time  0.000000  0.000999  0.001999  0.002998  0.003998  0.004997
>> > 0.005997  0.006996  0.007996
>> > 2 N023 -0.031323 -0.035026 -0.029759 -0.024886 -0.024464 -0.026816
>> > -0.033690 -0.041067 -0.038747
>> > 3 N053 -0.014083 -0.004741  0.001443 -0.010152 -0.012996 -0.005337
>> > -0.008738 -0.015094 -0.012104
>> > 4 N123 -0.019008 -0.013494 -0.013180 -0.029208 -0.032748 -0.020243
>> > -0.015089 -0.014439 -0.011681
>> > 5 N163 -0.054023 -0.049345 -0.037158 -0.041120 -0.044612 -0.036953
>> > -0.036061 -0.044516 -0.046436
>> > 6 N193 -0.022171 -0.022384 -0.022338 -0.023304 -0.022569 -0.021827
>> > -0.021996 -0.021755 -0.021846
>> >> Laurent_data <- read.table("test2.txt")
>> >> Laurent_data[Laurent_data$V1 %in% sensors, ]
>> >      V1        V2        V3        V4        V5        V6        V7
>> >     V8        V9       V10
>> > 3 N053 -0.014083 -0.004741  0.001443 -0.010152 -0.012996 -0.005337
>> > -0.008738 -0.015094 -0.012104
>> > 5 N163 -0.054023 -0.049345 -0.037158 -0.041120 -0.044612 -0.036953
>> > -0.036061 -0.044516 -0.046436
>> >
>> > Best, Bill.
>> >
>> > W. Michels, Ph.D.
>> >
>> >
>> > On Sun, May 17, 2020 at 5:43 PM Laurent Rhelp
><LaurentRHelp at free.fr> wrote:
>> >> Dear R-Help List,
>> >>
>> >>      I would like to use an iterator to read a file filtering some
>> >> selected lines according to the line name in order to use after a
>> >> foreach loop. I wanted to use the checkFunc argument as the
>following
>> >> example found on internet to select only prime numbers :
>> >>
>> >> |                                iprime <- ||iter||(1:100,
>checkFunc =
>> >> ||function||(n) ||isprime||(n))|
>> >>
>> >> |(https://datawookie.netlify.app/blog/2013/11/iterators-in-r/)
>> >> <https://datawookie.netlify.app/blog/2013/11/iterators-in-r/>|
>> >>
>> >> but the checkFunc argument seems not to be available with the
>function
>> >> ireadLines (package iterators). So, I did the code below to solve
>my
>> >> problem but I am sure that I miss something to use iterators with
>files.
>> >> Since I found nothing on the web about ireadLines and the
>checkFunc
>> >> argument, could somebody help me to understand how we have to use
>> >> iterator (and foreach loop) on files keeping only selected lines ?
>> >>
>> >> Thank you very much
>> >> Laurent
>> >>
>> >> Presently here is my code:
>> >>
>> >> ##        mock file to read: test.txt
>> >> ##
>> >> # Time    0    0.000999    0.001999    0.002998    0.003998
>0.004997
>> >> 0.005997    0.006996    0.007996
>> >> # N023    -0.031323    -0.035026    -0.029759    -0.024886
>-0.024464
>> >> -0.026816    -0.03369    -0.041067    -0.038747
>> >> # N053    -0.014083    -0.004741    0.001443    -0.010152
>-0.012996
>> >> -0.005337    -0.008738    -0.015094    -0.012104
>> >> # N123    -0.019008    -0.013494    -0.01318    -0.029208
>-0.032748
>> >> -0.020243    -0.015089    -0.014439    -0.011681
>> >> # N163    -0.054023    -0.049345    -0.037158    -0.04112
>-0.044612
>> >> -0.036953    -0.036061    -0.044516    -0.046436
>> >> # N193    -0.022171    -0.022384    -0.022338    -0.023304
>-0.022569
>> >> -0.021827    -0.021996    -0.021755    -0.021846
>> >>
>> >>
>> >> # sensors to keep
>> >>
>> >> sensors <-  c("N053", "N163")
>> >>
>> >>
>> >> library(iterators)
>> >>
>> >> library(rlist)
>> >>
>> >>
>> >> file_name <- "test.txt"
>> >>
>> >> con_obj <- file( file_name , "r")
>> >> ifile <- ireadLines( con_obj , n = 1 )
>> >>
>> >>
>> >> ## I do not do a loop for the example
>> >>
>> >> res <- list()
>> >>
>> >> r <- get_Lines_iter( ifile , sensors)
>> >> res <- list.append( res , r )
>> >> res
>> >> r <- get_Lines_iter( ifile , sensors)
>> >> res <- list.append( res , r )
>> >> res
>> >> r <- get_Lines_iter( ifile , sensors)
>> >> do.call("cbind",res)
>> >>
>> >> ## the function get_Lines_iter to select and process the line
>> >>
>> >> get_Lines_iter  <-  function( iter , sensors, sep = '\t', quiet =
>FALSE){
>> >>     ## read the next record in the iterator
>> >>     r = try( nextElem(iter) )
>> >>    while(  TRUE ){
>> >>       if( class(r) == "try-error") {
>> >>             return( stop("The iterator is empty") )
>> >>      } else {
>> >>      ## split the read line according to the separator
>> >>       r_txt <- textConnection(r)
>> >>       fields <- scan(file = r_txt, what = "character", sep = sep,
>quiet =
>> >> quiet)
>> >>        ## test if we have to keep the line
>> >>        if( fields[1] %in% sensors){
>> >>          ## data processing for the selected line (for the example
>> >> transformation in dataframe)
>> >>          n <- length(fields)
>> >>          x <- data.frame( as.numeric(fields[2:n]) )
>> >>          names(x) <- fields[1]
>> >>          ## We return the values
>> >>          print(paste0("sensor ",fields[1]," ok"))
>> >>          return( x )
>> >>        }else{
>> >>         print(paste0("Sensor ", fields[1] ," not selected"))
>> >>         r = try(nextElem(iter) )}
>> >>      }
>> >> }# end while loop
>> >> }
>> >>
>> >>
>> >>
>> >>
>> >>
>> >>
>> >>
>> >> --
>> >> L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e
>par le logiciel antivirus Avast.
>> >> https://www.avast.com/antivirus
>> >>
>> >>          [[alternative HTML version deleted]]
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>> --
>> L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e par
>le logiciel antivirus Avast.
>> https://www.avast.com/antivirus
>>
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed May 20 09:09:43 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 20 May 2020 08:09:43 +0100
Subject: [R] [External] Get a result but an error message as well ?
In-Reply-To: <1253484374.1044092.1589923503225@mail.yahoo.com>
References: <232159068.980863.1589917874331.ref@mail.yahoo.com>
 <232159068.980863.1589917874331@mail.yahoo.com>
 <CAGx1TMAZA7sJBZ343MEXwD=eZCM0V9_VLTSjw+QEvbniqDCepA@mail.gmail.com>
 <1667554344.1012305.1589920704261@mail.yahoo.com>
 <79e7fdeb-fe14-dcc0-f998-1c7307eb759b@sapo.pt>
 <1253484374.1044092.1589923503225@mail.yahoo.com>
Message-ID: <a767ff0b-1f83-c977-b834-4067ef602924@sapo.pt>

Hello,

I get an error but when running colnames(), not mean().
Notes:

1. I have changed the way the residuals are extracted, if there is a 
function resid(), the recommended practice is to use it.
2. c("MSE_OLS") and "MSE_OLS" are the identical() objects. Likewise, to 
return( c(MSE_OLS) ) is to have only MSE_OLS as the last function line. 
I find the latter clearer.


my.experiment <- function() {
   OLS <- lm(a ~ b + d)
   #MSE_OLS <- mean(OLS$residuals^2)
   MSE_OLS <- mean(resid(OLS)^2)
   MSE_OLS
}

my.data <- replicate( 500, my.experiment() )

class(my.data)  # it's a vector, not a matrix
#[1] "numeric"

colnames(my.data) <- "MSE_OLS"
#Error in `colnames<-`(`*tmp*`, value = "MSE_OLS") :
#  attempt to set 'colnames' on an object with less than two dimensions

mean(my.data)
#[1] 105.6951


Hope this helps,

Rui Barradas

?s 22:25 de 19/05/20, varin sacha escreveu:
> Hi Rui,
> 
> If I don't transpose t() the output of the replicate (my R code here below) I still get an error message !!
> 
> ########################################
> a=c(2,4,3,4,6,5,3,1,2,3,4,3,4,5,65)
> b=c(23,45,32,12,23,43,56,44,33,11,12,54,23,34,54)
> d=c(9,4,5,3,2,1,3,4,5,6,4,9,10,11,18)
> 
> my.experiment <- function() {
> 
> OLS <- lm( a ~ b+d )
> 
> MSE_OLS<-mean(OLS$residuals^2)
> 
> return( c(MSE_OLS) )
> }
> 
> my.data = replicate( 500, my.experiment() )
> colnames(my.data) <- c("MSE_OLS")
> mean(my.data)
> ########################################
> 
> 
> 
> 
> 
> 
> Le mardi 19 mai 2020 ? 23:14:21 UTC+2, Rui Barradas <ruipbarradas at sapo.pt> a ?crit :
> 
> 
> 
> 
> 
> Hello,
> 
> Inline.
> 
> ?s 21:38 de 19/05/20, varin sacha via R-help escreveu:
>>
>> Hi Richard,
>>
>> Thanks for your response.
>> However, how can I correct my R code knowing that I want, as a result, only one value : the mean of the 500 MSE_OLS values ?
> 
> Just don't transpose the output of replicate?
> 
> Hope this helps,
> 
> Rui Barradas
> 
>>
>>
>>
>>
>>
>>
>>
>> Le mardi 19 mai 2020 ? 21:59:07 UTC+2, Richard M. Heiberger <rmh at temple.edu> a ?crit :
>>
>>
>>
>>
>>
>>> dim(my.data)
>> [1] ? 1 500
>>
>> you have a matrix with a single row and 500 columns.
>> you gave a name to only the first column.
>>
>> Look at the result of replicate().? it is a vector.? You transposed it into a one-row matrix.
>>
>>>  ? ?tmp <- replicate( 500, my.experiment() )
>>> dim(tmp)
>> NULL
>>> length(tmp)
>> [1] 500
>>> dim(t(tmp))
>> [1] ? 1 500
>>
>>
>> On Tue, May 19, 2020 at 3:51 PM varin sacha via R-help <r-help at r-project.org> wrote:
>>> Dear R-experts,
>>>
>>> Here is my R code, I get a result but I also get an error message so I doubt I can trust the result I get.
>>> What is going wrong ? Many thanks.
>>>
>>> ########################################
>>> a<-c(2,4,3,4,6,5,3,1,2,3,4,3,4,5,65)
>>> b<-c(23,45,32,12,23,43,56,44,33,11,12,54,23,34,54)
>>> d<-c(9,4,5,3,2,1,3,4,5,6,4,9,10,11,18)
>>>
>>> my.experiment <- function( ) {
>>>
>>> OLS <- lm( a ~ b+d )
>>> MSE_OLS<-mean(OLS$residuals^2)
>>> return( c(MSE_OLS) )
>>> }
>>>
>>> my.data = t(replicate( 500, my.experiment() ))
>>> colnames(my.data) <- c("MSE_OLS")
>>> mean(my.data)
>>> ########################################
>>>    
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>


From kry|ov@r00t @end|ng |rom gm@||@com  Wed May 20 09:46:14 2020
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Wed, 20 May 2020 10:46:14 +0300
Subject: [R] iterators : checkFunc with ireadLines
In-Reply-To: <72174dd3-a819-b5fd-5a6f-2a6b46342774-2015@free.fr>
References: <72174dd3-a819-b5fd-5a6f-2a6b46342774-2015@free.fr>
Message-ID: <20200520104614.45876937@Tarkus>

Hi Laurent,

I am not saying this will work every time and I do recognise that this
is very different from a more general solution that you had envisioned,
but if you are on an UNIX-like system or have the relevant utilities
installed and on the %PATH% on Windows, you can filter the input file
line-by-line using a pipe and an external program:

On Sun, 17 May 2020 15:52:30 +0200
Laurent Rhelp <LaurentRHelp at free.fr> wrote:

> # sensors to keep
> sensors <-? c("N053", "N163")

# filter on the beginning of the line
i <- pipe("grep -E '^(N053|N163)' test.txt")
# or:
# filter on the beginning of the given column
# (use $2 for the second column, etc.)
i <- pipe("awk '($1 ~ \"^(N053|N163)\")' test.txt")
# or:
# since your message is full of Unicode non-breaking spaces, I have to
# bring in heavier machinery to handle those correctly;
# only this solution manages to match full column values
# (here you can also use $F[1] for second column and so on)
i <- pipe("perl -CSD -F'\\s+' -lE \\
 'print join qq{\\t}, @F if $F[0] =~ /^(N053|N163)$/' \\
 test.txt
")
lines <- read.table(i) # closes i when done

The downside of this approach is having to shell-escape the command
lines, which can become complicated, and choosing between use of regular
expressions and more wordy programs (Unicode whitespace in the input
doesn't help, either).

-- 
Best regards,
Ivan


From ||@t@ @end|ng |rom dewey@myzen@co@uk  Wed May 20 11:12:42 2020
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Wed, 20 May 2020 10:12:42 +0100
Subject: [R] text annotation on Manhattn plot in qqman
In-Reply-To: <CAF9-5jP3W-kJJiXSC+FRev4FgZqN74LvfnvL5VWTgUGnb56wew@mail.gmail.com>
References: <CAF9-5jO0=466Q-8VyBCfMTyqSPMzXC9nnxDNtW6RPVieGDnA-Q@mail.gmail.com>
 <6c9143b0-03c3-9b17-b11e-c3fe94128ae4@dewey.myzen.co.uk>
 <CAF9-5jP3W-kJJiXSC+FRev4FgZqN74LvfnvL5VWTgUGnb56wew@mail.gmail.com>
Message-ID: <54e1beb2-bcf4-98cf-82d2-da46b71ab1a6@dewey.myzen.co.uk>

a$newname <- paste(a$SNP, a$GENE)
manhattan(a, chr="CHR", bp="BP", snp="newname", p="P",annotatePval = 0.0001)

However note that I do not use manhattan() so you may need to alter the 
parameters as I am assuming a is where it finds the remaining parameters.

You may also need to play with the sep =, and collapse = parameters to 
paste() to get the precise layout you want.

Michael

On 19/05/2020 17:21, Ana Marija wrote:
> Hi Michael,
> 
> can you please send me code how that would be done?
> 
> Thanks
> Ana
> 
> On Tue, May 19, 2020 at 11:18 AM Michael Dewey <lists at dewey.myzen.co.uk> wrote:
>>
>> Dear Ana
>>
>> Perhaps paste together SNP and GENE using paste() and then supply that
>> as the snp parameter.
>>
>> Michael
>>
>> On 19/05/2020 17:12, Ana Marija wrote:
>>> Hello,
>>>
>>> I am making manhattan plot with:
>>> library(qqman)
>>> manhattan(a, chr="CHR", bp="BP", snp="SNP", p="P",annotatePval = 0.0001)
>>>
>>> and I would like to annotate these two SNPs which are above the
>>> threshold so that they have GENE name beside them:
>>>
>>>> a[a$SNP=="rs4081570",]
>>>           SNP            P CHR       BP GENE
>>> 1 rs4081570 6.564447e-05  19 15918647 UCA1
>>>> a[a$SNP=="rs11867934",]
>>>               SNP            P CHR       BP GENE
>>> 1021 rs11867934 6.738066e-06  17 16933404 FLCN
>>>
>>> Right now my plot only has SNP name for those 2, how to add GENE names
>>> (FLCN and UCA1 as well)
>>>
>>> Please advise
>>> Ana
>>>
>>>
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>> --
>> Michael
>> http://www.dewey.myzen.co.uk/home.html
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Wed May 20 17:10:35 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Wed, 20 May 2020 10:10:35 -0500
Subject: [R] text annotation on Manhattn plot in qqman
In-Reply-To: <54e1beb2-bcf4-98cf-82d2-da46b71ab1a6@dewey.myzen.co.uk>
References: <CAF9-5jO0=466Q-8VyBCfMTyqSPMzXC9nnxDNtW6RPVieGDnA-Q@mail.gmail.com>
 <6c9143b0-03c3-9b17-b11e-c3fe94128ae4@dewey.myzen.co.uk>
 <CAF9-5jP3W-kJJiXSC+FRev4FgZqN74LvfnvL5VWTgUGnb56wew@mail.gmail.com>
 <54e1beb2-bcf4-98cf-82d2-da46b71ab1a6@dewey.myzen.co.uk>
Message-ID: <CAF9-5jPCiPU0rX==PO5+wY6w+H4TJJAuPW4rv3UdKQs8qiHjnQ@mail.gmail.com>

HI Michael,

Thank you so much!
That worked!!! Now I am just trying to increase the size of text of
SNP and GENE on plot

I tried this:

a$newname <- paste(a$SNP,"\n", a$GENE)
manhattan(a, chr="CHR", bp="BP", snp="newname", p="P",cex =
0.5,annotatePval = 0.0001)

but I am getting this error:

Error in textxy(topSNPs$pos, -log10(topSNPs$P), offset = 0.625, labs =
topSNPs$SNP,  :
  formal argument "cex" matched by multiple actual arguments

Do you by any chance know how to do this?

Cheers
Ana

On Wed, May 20, 2020 at 4:12 AM Michael Dewey <lists at dewey.myzen.co.uk> wrote:
>
> a$newname <- paste(a$SNP, a$GENE)
> manhattan(a, chr="CHR", bp="BP", snp="newname", p="P",annotatePval = 0.0001)
>
> However note that I do not use manhattan() so you may need to alter the
> parameters as I am assuming a is where it finds the remaining parameters.
>
> You may also need to play with the sep =, and collapse = parameters to
> paste() to get the precise layout you want.
>
> Michael
>
> On 19/05/2020 17:21, Ana Marija wrote:
> > Hi Michael,
> >
> > can you please send me code how that would be done?
> >
> > Thanks
> > Ana
> >
> > On Tue, May 19, 2020 at 11:18 AM Michael Dewey <lists at dewey.myzen.co.uk> wrote:
> >>
> >> Dear Ana
> >>
> >> Perhaps paste together SNP and GENE using paste() and then supply that
> >> as the snp parameter.
> >>
> >> Michael
> >>
> >> On 19/05/2020 17:12, Ana Marija wrote:
> >>> Hello,
> >>>
> >>> I am making manhattan plot with:
> >>> library(qqman)
> >>> manhattan(a, chr="CHR", bp="BP", snp="SNP", p="P",annotatePval = 0.0001)
> >>>
> >>> and I would like to annotate these two SNPs which are above the
> >>> threshold so that they have GENE name beside them:
> >>>
> >>>> a[a$SNP=="rs4081570",]
> >>>           SNP            P CHR       BP GENE
> >>> 1 rs4081570 6.564447e-05  19 15918647 UCA1
> >>>> a[a$SNP=="rs11867934",]
> >>>               SNP            P CHR       BP GENE
> >>> 1021 rs11867934 6.738066e-06  17 16933404 FLCN
> >>>
> >>> Right now my plot only has SNP name for those 2, how to add GENE names
> >>> (FLCN and UCA1 as well)
> >>>
> >>> Please advise
> >>> Ana
> >>>
> >>>
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>
> >> --
> >> Michael
> >> http://www.dewey.myzen.co.uk/home.html
> >
>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html


From ||@t@ @end|ng |rom dewey@myzen@co@uk  Wed May 20 17:23:39 2020
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Wed, 20 May 2020 16:23:39 +0100
Subject: [R] text annotation on Manhattn plot in qqman
In-Reply-To: <CAF9-5jPCiPU0rX==PO5+wY6w+H4TJJAuPW4rv3UdKQs8qiHjnQ@mail.gmail.com>
References: <CAF9-5jO0=466Q-8VyBCfMTyqSPMzXC9nnxDNtW6RPVieGDnA-Q@mail.gmail.com>
 <6c9143b0-03c3-9b17-b11e-c3fe94128ae4@dewey.myzen.co.uk>
 <CAF9-5jP3W-kJJiXSC+FRev4FgZqN74LvfnvL5VWTgUGnb56wew@mail.gmail.com>
 <54e1beb2-bcf4-98cf-82d2-da46b71ab1a6@dewey.myzen.co.uk>
 <CAF9-5jPCiPU0rX==PO5+wY6w+H4TJJAuPW4rv3UdKQs8qiHjnQ@mail.gmail.com>
Message-ID: <33125e2a-f194-b118-e885-ac0b65421899@dewey.myzen.co.uk>

Dear Ana

That looks like something is hard coded in manhattan(). The simplest 
thing might be to contact the maintainer of the package and ask. You can 
make a copy of manhattan or textxy and modify them but I think the 
maintainer is the simplest course of action.

Michael

On 20/05/2020 16:10, Ana Marija wrote:
> HI Michael,
> 
> Thank you so much!
> That worked!!! Now I am just trying to increase the size of text of
> SNP and GENE on plot
> 
> I tried this:
> 
> a$newname <- paste(a$SNP,"\n", a$GENE)
> manhattan(a, chr="CHR", bp="BP", snp="newname", p="P",cex =
> 0.5,annotatePval = 0.0001)
> 
> but I am getting this error:
> 
> Error in textxy(topSNPs$pos, -log10(topSNPs$P), offset = 0.625, labs =
> topSNPs$SNP,  :
>    formal argument "cex" matched by multiple actual arguments
> 
> Do you by any chance know how to do this?
> 
> Cheers
> Ana
> 
> On Wed, May 20, 2020 at 4:12 AM Michael Dewey <lists at dewey.myzen.co.uk> wrote:
>>
>> a$newname <- paste(a$SNP, a$GENE)
>> manhattan(a, chr="CHR", bp="BP", snp="newname", p="P",annotatePval = 0.0001)
>>
>> However note that I do not use manhattan() so you may need to alter the
>> parameters as I am assuming a is where it finds the remaining parameters.
>>
>> You may also need to play with the sep =, and collapse = parameters to
>> paste() to get the precise layout you want.
>>
>> Michael
>>
>> On 19/05/2020 17:21, Ana Marija wrote:
>>> Hi Michael,
>>>
>>> can you please send me code how that would be done?
>>>
>>> Thanks
>>> Ana
>>>
>>> On Tue, May 19, 2020 at 11:18 AM Michael Dewey <lists at dewey.myzen.co.uk> wrote:
>>>>
>>>> Dear Ana
>>>>
>>>> Perhaps paste together SNP and GENE using paste() and then supply that
>>>> as the snp parameter.
>>>>
>>>> Michael
>>>>
>>>> On 19/05/2020 17:12, Ana Marija wrote:
>>>>> Hello,
>>>>>
>>>>> I am making manhattan plot with:
>>>>> library(qqman)
>>>>> manhattan(a, chr="CHR", bp="BP", snp="SNP", p="P",annotatePval = 0.0001)
>>>>>
>>>>> and I would like to annotate these two SNPs which are above the
>>>>> threshold so that they have GENE name beside them:
>>>>>
>>>>>> a[a$SNP=="rs4081570",]
>>>>>            SNP            P CHR       BP GENE
>>>>> 1 rs4081570 6.564447e-05  19 15918647 UCA1
>>>>>> a[a$SNP=="rs11867934",]
>>>>>                SNP            P CHR       BP GENE
>>>>> 1021 rs11867934 6.738066e-06  17 16933404 FLCN
>>>>>
>>>>> Right now my plot only has SNP name for those 2, how to add GENE names
>>>>> (FLCN and UCA1 as well)
>>>>>
>>>>> Please advise
>>>>> Ana
>>>>>
>>>>>
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>
>>>> --
>>>> Michael
>>>> http://www.dewey.myzen.co.uk/home.html
>>>
>>
>> --
>> Michael
>> http://www.dewey.myzen.co.uk/home.html
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From e@rtun2010 @end|ng |rom gm@||@com  Wed May 20 14:01:08 2020
From: e@rtun2010 @end|ng |rom gm@||@com (Ergin Artun)
Date: Wed, 20 May 2020 15:01:08 +0300
Subject: [R] survival anaylsis with tabulated data
Message-ID: <CAFp+oAwyG0vNSYURgFAOxfDH0erY7Y2D4CPcVQEfuu4Ek0twxw@mail.gmail.com>

  Dear R Friends,

I'm a medical doctor with some knowledge about statistic and programming.
I want to analysis and compare data of different countries with survival or
popEpi tools in R.
I couldn't able to make tables with one row for every people of countries
without illness. I can made a data table look like as:

time
country at.risk from0to1 sk zf from0to0
1 AFG 40363639 1 K F 0

I try to prepare for analysis with survtab_ag function of Epi package. When
I run
st <- survtab_ag(time ~ cuntry, data = xxxx, surv.type =
"surv.obs",surv.breaks=list(xxxx$time),
+                  surv.method = "lifetable",
+                  d = "from0to1")
I see just the error message:
Error in sort.int(x, na.last = na.last, decreasing = decreasing, ...) :
  'x' must be atomic

Can any body help me for preparing this kind tabulated data without surv()
to other survival functions?
Best regards from Turkey

Tanju Aktug

	[[alternative HTML version deleted]]


From p@u| @end|ng |rom @t@t@@uck|@nd@@c@nz  Wed May 20 22:49:25 2020
From: p@u| @end|ng |rom @t@t@@uck|@nd@@c@nz (Paul Murrell)
Date: Thu, 21 May 2020 08:49:25 +1200
Subject: [R] [FORGED] Re:  text annotation on Manhattn plot in qqman
In-Reply-To: <CAF9-5jPCiPU0rX==PO5+wY6w+H4TJJAuPW4rv3UdKQs8qiHjnQ@mail.gmail.com>
References: <CAF9-5jO0=466Q-8VyBCfMTyqSPMzXC9nnxDNtW6RPVieGDnA-Q@mail.gmail.com>
 <6c9143b0-03c3-9b17-b11e-c3fe94128ae4@dewey.myzen.co.uk>
 <CAF9-5jP3W-kJJiXSC+FRev4FgZqN74LvfnvL5VWTgUGnb56wew@mail.gmail.com>
 <54e1beb2-bcf4-98cf-82d2-da46b71ab1a6@dewey.myzen.co.uk>
 <CAF9-5jPCiPU0rX==PO5+wY6w+H4TJJAuPW4rv3UdKQs8qiHjnQ@mail.gmail.com>
Message-ID: <08748d65-6ff3-155a-315b-c12b497dc373@stat.auckland.ac.nz>

Hi

Something like this might do what you want ...


## From example(manhattan)
manhattan(gwasResults, annotatePval = 0.0001)

library(gridGraphics)
grid.echo()
## grid.ls()
## The annotations have "text" in their name
labels <- grid.grep("text", grep=TRUE)
grid.edit(labels, gp=gpar(cex=1))


Hope that helps

Paul

On 21/05/20 3:10 am, Ana Marija wrote:
> HI Michael,
> 
> Thank you so much!
> That worked!!! Now I am just trying to increase the size of text of
> SNP and GENE on plot
> 
> I tried this:
> 
> a$newname <- paste(a$SNP,"\n", a$GENE)
> manhattan(a, chr="CHR", bp="BP", snp="newname", p="P",cex =
> 0.5,annotatePval = 0.0001)
> 
> but I am getting this error:
> 
> Error in textxy(topSNPs$pos, -log10(topSNPs$P), offset = 0.625, labs =
> topSNPs$SNP,  :
>    formal argument "cex" matched by multiple actual arguments
> 
> Do you by any chance know how to do this?
> 
> Cheers
> Ana
> 
> On Wed, May 20, 2020 at 4:12 AM Michael Dewey <lists at dewey.myzen.co.uk> wrote:
>>
>> a$newname <- paste(a$SNP, a$GENE)
>> manhattan(a, chr="CHR", bp="BP", snp="newname", p="P",annotatePval = 0.0001)
>>
>> However note that I do not use manhattan() so you may need to alter the
>> parameters as I am assuming a is where it finds the remaining parameters.
>>
>> You may also need to play with the sep =, and collapse = parameters to
>> paste() to get the precise layout you want.
>>
>> Michael
>>
>> On 19/05/2020 17:21, Ana Marija wrote:
>>> Hi Michael,
>>>
>>> can you please send me code how that would be done?
>>>
>>> Thanks
>>> Ana
>>>
>>> On Tue, May 19, 2020 at 11:18 AM Michael Dewey <lists at dewey.myzen.co.uk> wrote:
>>>>
>>>> Dear Ana
>>>>
>>>> Perhaps paste together SNP and GENE using paste() and then supply that
>>>> as the snp parameter.
>>>>
>>>> Michael
>>>>
>>>> On 19/05/2020 17:12, Ana Marija wrote:
>>>>> Hello,
>>>>>
>>>>> I am making manhattan plot with:
>>>>> library(qqman)
>>>>> manhattan(a, chr="CHR", bp="BP", snp="SNP", p="P",annotatePval = 0.0001)
>>>>>
>>>>> and I would like to annotate these two SNPs which are above the
>>>>> threshold so that they have GENE name beside them:
>>>>>
>>>>>> a[a$SNP=="rs4081570",]
>>>>>            SNP            P CHR       BP GENE
>>>>> 1 rs4081570 6.564447e-05  19 15918647 UCA1
>>>>>> a[a$SNP=="rs11867934",]
>>>>>                SNP            P CHR       BP GENE
>>>>> 1021 rs11867934 6.738066e-06  17 16933404 FLCN
>>>>>
>>>>> Right now my plot only has SNP name for those 2, how to add GENE names
>>>>> (FLCN and UCA1 as well)
>>>>>
>>>>> Please advise
>>>>> Ana
>>>>>
>>>>>
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>
>>>> --
>>>> Michael
>>>> http://www.dewey.myzen.co.uk/home.html
>>>
>>
>> --
>> Michael
>> http://www.dewey.myzen.co.uk/home.html
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed May 20 05:01:52 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 19 May 2020 20:01:52 -0700
Subject: [R] iterators : checkFunc with ireadLines
In-Reply-To: <f0a8e687-c79e-c748-564a-f93cc3fdee23@free.fr>
References: <72174dd3-a819-b5fd-5a6f-2a6b46342774-2015@free.fr>
 <CAA99HCwtOek_jaNsD6-=CtTRoL9OHYgDqfgdSAfbNBaEZ_w8JQ@mail.gmail.com>
 <a1ad0aec-7457-b516-1a3c-43e9e83d0ae1@free.fr>
 <CAA99HCx_VvNJzTQ=mQ7zoRFX5ZJOA3mCgbDkMiCSPAH_6Zv=QA-7372@mail.gmail.com>
 <E2222FD6-1A53-4C9C-87BA-7AEFDB3C83BB@dcn.davis.ca.us>
 <f0a8e687-c79e-c748-564a-f93cc3fdee23@free.fr>
Message-ID: <DCB94842-2A78-4E7A-9F61-AFA91C8A6B60-2034@dcn.davis.ca.us>


There is also apparently a package called disk.frame that you might consider.

On May 19, 2020 12:07:38 AM PDT, Laurent Rhelp <LaurentRHelp at free.fr> wrote:
>Ok, thank you for the advice I will take some time to see in details 
>these packages.
>
>
>Le 19/05/2020 ? 05:44, Jeff Newmiller a ?crit?:
>> Laurent... Bill is suggesting building your own indexed database...
>but this has been done before, so re-inventing the wheel seems
>inefficient and risky. It is actually impossible to create such a beast
>without reading the entire file into memory at least temporarily
>anyway, so you are better off looking at ways to process the entire
>file efficiently.
>>
>> For example, you could load the data into a sqlite database in a
>couple of lines of code and use SQL directly or use the sqldf data
>frame interface, or use dplyr to query the database.
>>
>> Or you could look at read_csv_chunked from readr package.
>>
>> On May 18, 2020 11:37:46 AM PDT, William Michels via R-help
><r-help at r-project.org> wrote:
>>> Hi Laurent,
>>>
>>> Thank you for explaining your size limitations. Below is an example
>>> using the read.fwf() function to grab the first column of your input
>>> file (in 2000 row chunks). This column is converted to an index, and
>>> the index is used to create an iterator useful for skipping lines
>when
>>> reading input with scan(). (You could try processing your large file
>>> in successive 2000 line chunks, or whatever number of lines fits
>into
>>> memory). Maybe not as elegant as the approach you were going for,
>but
>>> read.fwf() should be pretty efficient:
>>>
>>>> sensors <-  c("N053", "N163")
>>>> read.fwf("test2.txt", widths=c(4), as.is=TRUE, flush=TRUE, n=2000,
>>> skip=0)
>>>     V1
>>> 1 Time
>>> 2 N023
>>> 3 N053
>>> 4 N123
>>> 5 N163
>>> 6 N193
>>>> first_col <- read.fwf("test2.txt", widths=c(4), as.is=TRUE,
>>> flush=TRUE, n=2000, skip=0)
>>>> which(first_col$V1 %in% sensors)
>>> [1] 3 5
>>>> index1 <- which(first_col$V1 %in% sensors)
>>>> iter_index1 <- iter(1:2000, checkFunc= function(n) {n %in% index1})
>>>> unlist(scan(file="test2.txt",
>>> what=list("","","","","","","","","",""), flush=TRUE,
>multi.line=FALSE,
>>> skip=nextElem(iter_index1)-1, nlines=1, quiet=TRUE))
>>> [1] "N053"      "-0.014083" "-0.004741" "0.001443"  "-0.010152"
>>> "-0.012996" "-0.005337" "-0.008738" "-0.015094" "-0.012104"
>>>> unlist(scan(file="test2.txt",
>>> what=list("","","","","","","","","",""), flush=TRUE,
>multi.line=FALSE,
>>> skip=nextElem(iter_index1)-1, nlines=1, quiet=TRUE))
>>> [1] "N163"      "-0.054023" "-0.049345" "-0.037158" "-0.04112"
>>> "-0.044612" "-0.036953" "-0.036061" "-0.044516" "-0.046436"
>>> (Note for this email and the previous one, I've deleted the first
>>> "hash" character from each line of your test file for clarity).
>>>
>>> HTH, Bill.
>>>
>>> W. Michels, Ph.D.
>>>
>>>
>>>
>>>
>>>
>>> On Mon, May 18, 2020 at 3:35 AM Laurent Rhelp <LaurentRHelp at free.fr>
>>> wrote:
>>>> Dear William,
>>>>    Thank you for your answer
>>>> My file is very large so I cannot read it in my memory (I cannot
>use
>>>> read.table). So I want to put in memory only the line I need to
>>> process.
>>>> With readLines, as I did, it works but I would like to use an
>>> iterator
>>>> and a foreach loop to understand this way to do because I thought
>>> that
>>>> it was a better solution to write a nice code.
>>>>
>>>>
>>>> Le 18/05/2020 ? 04:54, William Michels a ?crit :
>>>>> Apologies, Laurent, for this two-part answer. I misunderstood your
>>>>> post where you stated you wanted to "filter(ing) some
>>>>> selected lines according to the line name... ." I thought that
>>> meant
>>>>> you had a separate index (like a series of primes) that you wanted
>>> to
>>>>> use to only read-in selected line numbers from a file (test file
>>> below
>>>>> with numbers 1:1000 each on a separate line):
>>>>>
>>>>>> library(gmp)
>>>>>> library(iterators)
>>>>>> iprime <- iter(1:100, checkFunc = function(n) isprime(n))
>>>>>> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1,
>nlines=1)
>>>>> Read 1 item
>>>>> [1] 2
>>>>>> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1,
>nlines=1)
>>>>> Read 1 item
>>>>> [1] 3
>>>>>> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1,
>nlines=1)
>>>>> Read 1 item
>>>>> [1] 5
>>>>>> scan(file="one_thou_lines.txt", skip=nextElem(iprime)-1,
>nlines=1)
>>>>> Read 1 item
>>>>> [1] 7
>>>>> However, what it really seems that you want to do is read each
>line
>>> of
>>>>> a (possibly enormous) file, test each line "string-wise" to keep
>or
>>>>> discard, and if you're keeping it, append the line to a list. I
>can
>>>>> certainly see the advantage of this strategy for reading in very,
>>> very
>>>>> large files, but it's not clear to me how the "ireadLines"
>function
>>> (
>>>>> in the "iterators" package) will help you, since it doesn't seem
>to
>>>>> generate anything but a sequential index.
>>>>>
>>>>> Anyway, below is an absolutely standard read-in of your data using
>>>>> read.table(). Hopefully some of the code I've posted has been
>>> useful
>>>>> to you.
>>>>>
>>>>>> sensors <-  c("N053", "N163")
>>>>>> read.table("test2.txt")
>>>>>       V1        V2        V3        V4        V5        V6       
>V7
>>>>>      V8        V9       V10
>>>>> 1 Time  0.000000  0.000999  0.001999  0.002998  0.003998  0.004997
>>>>> 0.005997  0.006996  0.007996
>>>>> 2 N023 -0.031323 -0.035026 -0.029759 -0.024886 -0.024464 -0.026816
>>>>> -0.033690 -0.041067 -0.038747
>>>>> 3 N053 -0.014083 -0.004741  0.001443 -0.010152 -0.012996 -0.005337
>>>>> -0.008738 -0.015094 -0.012104
>>>>> 4 N123 -0.019008 -0.013494 -0.013180 -0.029208 -0.032748 -0.020243
>>>>> -0.015089 -0.014439 -0.011681
>>>>> 5 N163 -0.054023 -0.049345 -0.037158 -0.041120 -0.044612 -0.036953
>>>>> -0.036061 -0.044516 -0.046436
>>>>> 6 N193 -0.022171 -0.022384 -0.022338 -0.023304 -0.022569 -0.021827
>>>>> -0.021996 -0.021755 -0.021846
>>>>>> Laurent_data <- read.table("test2.txt")
>>>>>> Laurent_data[Laurent_data$V1 %in% sensors, ]
>>>>>       V1        V2        V3        V4        V5        V6       
>V7
>>>>>      V8        V9       V10
>>>>> 3 N053 -0.014083 -0.004741  0.001443 -0.010152 -0.012996 -0.005337
>>>>> -0.008738 -0.015094 -0.012104
>>>>> 5 N163 -0.054023 -0.049345 -0.037158 -0.041120 -0.044612 -0.036953
>>>>> -0.036061 -0.044516 -0.046436
>>>>>
>>>>> Best, Bill.
>>>>>
>>>>> W. Michels, Ph.D.
>>>>>
>>>>>
>>>>> On Sun, May 17, 2020 at 5:43 PM Laurent Rhelp
>>> <LaurentRHelp at free.fr> wrote:
>>>>>> Dear R-Help List,
>>>>>>
>>>>>>       I would like to use an iterator to read a file filtering
>some
>>>>>> selected lines according to the line name in order to use after a
>>>>>> foreach loop. I wanted to use the checkFunc argument as the
>>> following
>>>>>> example found on internet to select only prime numbers :
>>>>>>
>>>>>> |                                iprime <- ||iter||(1:100,
>>> checkFunc =
>>>>>> ||function||(n) ||isprime||(n))|
>>>>>>
>>>>>> |(https://datawookie.netlify.app/blog/2013/11/iterators-in-r/)
>>>>>> <https://datawookie.netlify.app/blog/2013/11/iterators-in-r/>|
>>>>>>
>>>>>> but the checkFunc argument seems not to be available with the
>>> function
>>>>>> ireadLines (package iterators). So, I did the code below to solve
>>> my
>>>>>> problem but I am sure that I miss something to use iterators with
>>> files.
>>>>>> Since I found nothing on the web about ireadLines and the
>>> checkFunc
>>>>>> argument, could somebody help me to understand how we have to use
>>>>>> iterator (and foreach loop) on files keeping only selected lines
>?
>>>>>>
>>>>>> Thank you very much
>>>>>> Laurent
>>>>>>
>>>>>> Presently here is my code:
>>>>>>
>>>>>> ##        mock file to read: test.txt
>>>>>> ##
>>>>>> # Time    0    0.000999    0.001999    0.002998    0.003998
>>> 0.004997
>>>>>> 0.005997    0.006996    0.007996
>>>>>> # N023    -0.031323    -0.035026    -0.029759    -0.024886
>>> -0.024464
>>>>>> -0.026816    -0.03369    -0.041067    -0.038747
>>>>>> # N053    -0.014083    -0.004741    0.001443    -0.010152
>>> -0.012996
>>>>>> -0.005337    -0.008738    -0.015094    -0.012104
>>>>>> # N123    -0.019008    -0.013494    -0.01318    -0.029208
>>> -0.032748
>>>>>> -0.020243    -0.015089    -0.014439    -0.011681
>>>>>> # N163    -0.054023    -0.049345    -0.037158    -0.04112
>>> -0.044612
>>>>>> -0.036953    -0.036061    -0.044516    -0.046436
>>>>>> # N193    -0.022171    -0.022384    -0.022338    -0.023304
>>> -0.022569
>>>>>> -0.021827    -0.021996    -0.021755    -0.021846
>>>>>>
>>>>>>
>>>>>> # sensors to keep
>>>>>>
>>>>>> sensors <-  c("N053", "N163")
>>>>>>
>>>>>>
>>>>>> library(iterators)
>>>>>>
>>>>>> library(rlist)
>>>>>>
>>>>>>
>>>>>> file_name <- "test.txt"
>>>>>>
>>>>>> con_obj <- file( file_name , "r")
>>>>>> ifile <- ireadLines( con_obj , n = 1 )
>>>>>>
>>>>>>
>>>>>> ## I do not do a loop for the example
>>>>>>
>>>>>> res <- list()
>>>>>>
>>>>>> r <- get_Lines_iter( ifile , sensors)
>>>>>> res <- list.append( res , r )
>>>>>> res
>>>>>> r <- get_Lines_iter( ifile , sensors)
>>>>>> res <- list.append( res , r )
>>>>>> res
>>>>>> r <- get_Lines_iter( ifile , sensors)
>>>>>> do.call("cbind",res)
>>>>>>
>>>>>> ## the function get_Lines_iter to select and process the line
>>>>>>
>>>>>> get_Lines_iter  <-  function( iter , sensors, sep = '\t', quiet =
>>> FALSE){
>>>>>>      ## read the next record in the iterator
>>>>>>      r = try( nextElem(iter) )
>>>>>>     while(  TRUE ){
>>>>>>        if( class(r) == "try-error") {
>>>>>>              return( stop("The iterator is empty") )
>>>>>>       } else {
>>>>>>       ## split the read line according to the separator
>>>>>>        r_txt <- textConnection(r)
>>>>>>        fields <- scan(file = r_txt, what = "character", sep =
>sep,
>>> quiet =
>>>>>> quiet)
>>>>>>         ## test if we have to keep the line
>>>>>>         if( fields[1] %in% sensors){
>>>>>>           ## data processing for the selected line (for the
>example
>>>>>> transformation in dataframe)
>>>>>>           n <- length(fields)
>>>>>>           x <- data.frame( as.numeric(fields[2:n]) )
>>>>>>           names(x) <- fields[1]
>>>>>>           ## We return the values
>>>>>>           print(paste0("sensor ",fields[1]," ok"))
>>>>>>           return( x )
>>>>>>         }else{
>>>>>>          print(paste0("Sensor ", fields[1] ," not selected"))
>>>>>>          r = try(nextElem(iter) )}
>>>>>>       }
>>>>>> }# end while loop
>>>>>> }
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> --
>>>>>> L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e
>>> par le logiciel antivirus Avast.
>>>>>> https://www.avast.com/antivirus
>>>>>>
>>>>>>           [[alternative HTML version deleted]]
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible
>code.
>>>>
>>>>
>>>> --
>>>> L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e par
>>> le logiciel antivirus Avast.
>>>> https://www.avast.com/antivirus
>>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From petr@p|k@| @end|ng |rom prechez@@cz  Thu May 21 11:52:59 2020
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Thu, 21 May 2020 09:52:59 +0000
Subject: [R] survival anaylsis with tabulated data
In-Reply-To: <CAFp+oAwyG0vNSYURgFAOxfDH0erY7Y2D4CPcVQEfuu4Ek0twxw@mail.gmail.com>
References: <CAFp+oAwyG0vNSYURgFAOxfDH0erY7Y2D4CPcVQEfuu4Ek0twxw@mail.gmail.com>
Message-ID: <590351402cb04c0e91ff48e9f6990b0b@SRVEXCHCM1302.precheza.cz>

Hi

I am not an expert and cannot give you canned solution, but here are few
comments:

Without knowing data structure it is hard to decipher where is the problem.
So posting result of 
str(yourata) or at least part of it hardly anybody could help.
Cuntry is typo?
The error could by result of typo or not properly structured data.

This is plain text mail list, please do not use HTML formating.

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ergin Artun
> Sent: Wednesday, May 20, 2020 2:01 PM
> To: r-help at r-project.org
> Subject: [R] survival anaylsis with tabulated data
> 
>   Dear R Friends,
> 
> I'm a medical doctor with some knowledge about statistic and programming.
> I want to analysis and compare data of different countries with survival
or
> popEpi tools in R.
> I couldn't able to make tables with one row for every people of countries
> without illness. I can made a data table look like as:
> 
> time
> country at.risk from0to1 sk zf from0to0
> 1 AFG 40363639 1 K F 0
> 
> I try to prepare for analysis with survtab_ag function of Epi package.
When
> I run
> st <- survtab_ag(time ~ cuntry, data = xxxx, surv.type =
> "surv.obs",surv.breaks=list(xxxx$time),
> +                  surv.method = "lifetable",
> +                  d = "from0to1")
> I see just the error message:
> Error in sort.int(x, na.last = na.last, decreasing = decreasing, ...) :
>   'x' must be atomic
> 
> Can any body help me for preparing this kind tabulated data without surv()
> to other survival functions?
> Best regards from Turkey
> 
> Tanju Aktug
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From petr@p|k@| @end|ng |rom prechez@@cz  Thu May 21 15:16:26 2020
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Thu, 21 May 2020 13:16:26 +0000
Subject: [R] survival anaylsis with tabulated data
In-Reply-To: <CAFp+oAw6JQrK1hssWbbJnxLQvZ3ab+K2ABkbgHoF=1jKAV8XFg@mail.gmail.com>
References: <CAFp+oAwyG0vNSYURgFAOxfDH0erY7Y2D4CPcVQEfuu4Ek0twxw@mail.gmail.com>
 <590351402cb04c0e91ff48e9f6990b0b@SRVEXCHCM1302.precheza.cz>
 <CAFp+oAw6JQrK1hssWbbJnxLQvZ3ab+K2ABkbgHoF=1jKAV8XFg@mail.gmail.com>
Message-ID: <47dd583c3d0144b7af54e223e376d620@SRVEXCHCM1302.precheza.cz>

Hallo

I am not private consultant so please keep your messages on help list.

>From help page I understand that the function survtab_ag needs to have **specially prepared** data as input. We could only guess if your data are in correct format and most probably they are not.

Without knowing details I wonder if anybody is able to help you.

Cheers
Petr

From: Ergin Artun <eartun2010 at gmail.com> 
Sent: Thursday, May 21, 2020 3:08 PM
To: PIKAL Petr <petr.pikal at precheza.cz>
Subject: Re: [R] survival anaylsis with tabulated data

Dear Pical,
I'm also not  an expert and had a typo error while translating colum names at code.

At my data set time(as day), at.risk(people), from0to1(as people with event), from0to0(censored values for the function) are integers, Country, and the other two columns are charecter strings.

Best regards...
Tanju   

PIKAL Petr <mailto:petr.pikal at precheza.cz>, 21 May 2020 Per, 12:53 tarihinde ?unu yazd?:
Hi

I am not an expert and cannot give you canned solution, but here are few
comments:

Without knowing data structure it is hard to decipher where is the problem.
So posting result of 
str(yourata) or at least part of it hardly anybody could help.
Cuntry is typo?
The error could by result of typo or not properly structured data.

This is plain text mail list, please do not use HTML formating.

Cheers
Petr

> -----Original Message-----
> From: R-help <mailto:r-help-bounces at r-project.org> On Behalf Of Ergin Artun
> Sent: Wednesday, May 20, 2020 2:01 PM
> To: mailto:r-help at r-project.org
> Subject: [R] survival anaylsis with tabulated data
> 
>   Dear R Friends,
> 
> I'm a medical doctor with some knowledge about statistic and programming.
> I want to analysis and compare data of different countries with survival
or
> popEpi tools in R.
> I couldn't able to make tables with one row for every people of countries
> without illness. I can made a data table look like as:
> 
> time
> country at.risk from0to1 sk zf from0to0
> 1 AFG 40363639 1 K F 0
> 
> I try to prepare for analysis with survtab_ag function of Epi package.
When
> I run
> st <- survtab_ag(time ~ cuntry, data = xxxx, surv.type =
> "surv.obs",surv.breaks=list(xxxx$time),
> +                  surv.method = "lifetable",
> +                  d = "from0to1")
> I see just the error message:
> Error in http://sort.int(x, na.last = na.last, decreasing = decreasing, ...) :
>   'x' must be atomic
> 
> Can any body help me for preparing this kind tabulated data without surv()
> to other survival functions?
> Best regards from Turkey
> 
> Tanju Aktug
> 
>       [[alternative HTML version deleted]]
> 
> ______________________________________________
> mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From jrkr|de@u @end|ng |rom gm@||@com  Thu May 21 16:48:58 2020
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Thu, 21 May 2020 10:48:58 -0400
Subject: [R] survival anaylsis with tabulated data
In-Reply-To: <47dd583c3d0144b7af54e223e376d620@SRVEXCHCM1302.precheza.cz>
References: <CAFp+oAwyG0vNSYURgFAOxfDH0erY7Y2D4CPcVQEfuu4Ek0twxw@mail.gmail.com>
 <590351402cb04c0e91ff48e9f6990b0b@SRVEXCHCM1302.precheza.cz>
 <CAFp+oAw6JQrK1hssWbbJnxLQvZ3ab+K2ABkbgHoF=1jKAV8XFg@mail.gmail.com>
 <47dd583c3d0144b7af54e223e376d620@SRVEXCHCM1302.precheza.cz>
Message-ID: <CAKZQJMCs44DeLzKV3xi+-8X-jtYgFUAS123V6XN448X3yL3rMQ@mail.gmail.com>

 http://adv-r.had.co.nz/Reproducibility.html

http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example



On Thu, 21 May 2020 at 09:16, PIKAL Petr <petr.pikal at precheza.cz> wrote:

> Hallo
>
> I am not private consultant so please keep your messages on help list.
>
> From help page I understand that the function survtab_ag needs to have
> **specially prepared** data as input. We could only guess if your data are
> in correct format and most probably they are not.
>
> Without knowing details I wonder if anybody is able to help you.
>
> Cheers
> Petr
>
> From: Ergin Artun <eartun2010 at gmail.com>
> Sent: Thursday, May 21, 2020 3:08 PM
> To: PIKAL Petr <petr.pikal at precheza.cz>
> Subject: Re: [R] survival anaylsis with tabulated data
>
> Dear Pical,
> I'm also not  an expert and had a typo error while translating colum names
> at code.
>
> At my data set time(as day), at.risk(people), from0to1(as people with
> event), from0to0(censored values for the function) are integers, Country,
> and the other two columns are charecter strings.
>
> Best regards...
> Tanju
>
> PIKAL Petr <mailto:petr.pikal at precheza.cz>, 21 May 2020 Per, 12:53
> tarihinde ?unu yazd?:
> Hi
>
> I am not an expert and cannot give you canned solution, but here are few
> comments:
>
> Without knowing data structure it is hard to decipher where is the problem.
> So posting result of
> str(yourata) or at least part of it hardly anybody could help.
> Cuntry is typo?
> The error could by result of typo or not properly structured data.
>
> This is plain text mail list, please do not use HTML formating.
>
> Cheers
> Petr
>
> > -----Original Message-----
> > From: R-help <mailto:r-help-bounces at r-project.org> On Behalf Of Ergin
> Artun
> > Sent: Wednesday, May 20, 2020 2:01 PM
> > To: mailto:r-help at r-project.org
> > Subject: [R] survival anaylsis with tabulated data
> >
> >   Dear R Friends,
> >
> > I'm a medical doctor with some knowledge about statistic and programming.
> > I want to analysis and compare data of different countries with survival
> or
> > popEpi tools in R.
> > I couldn't able to make tables with one row for every people of countries
> > without illness. I can made a data table look like as:
> >
> > time
> > country at.risk from0to1 sk zf from0to0
> > 1 AFG 40363639 1 K F 0
> >
> > I try to prepare for analysis with survtab_ag function of Epi package.
> When
> > I run
> > st <- survtab_ag(time ~ cuntry, data = xxxx, surv.type =
> > "surv.obs",surv.breaks=list(xxxx$time),
> > +                  surv.method = "lifetable",
> > +                  d = "from0to1")
> > I see just the error message:
> > Error in http://sort.int(x, na.last = na.last, decreasing = decreasing,
> ...) :
> >   'x' must be atomic
> >
> > Can any body help me for preparing this kind tabulated data without
> surv()
> > to other survival functions?
> > Best regards from Turkey
> >
> > Tanju Aktug
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
John Kane
Kingston ON Canada

	[[alternative HTML version deleted]]


From dy|@nbe|jer@ @end|ng |rom gm@||@com  Mon May 18 23:22:24 2020
From: dy|@nbe|jer@ @end|ng |rom gm@||@com (Dylan Beijers)
Date: Mon, 18 May 2020 23:22:24 +0200
Subject: [R] [R-pkgs] statespacer: State Space Modelling in 'R'
Message-ID: <CAM3wmzgid+CBVNVje2PH7QXvGi5z+qgnj5K_qzTbcXqE+x5SWg@mail.gmail.com>

Dear All,

The statespacer package is now on CRAN (
https://cran.r-project.org/package=statespacer). This package provides
functionality for modelling and forecasting time series using state space
techniques. For more info, please see
https://dylanb95.github.io/statespacer/

Best,
Dylan Beijers

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From re|chm@nj @end|ng |rom @bcg|ob@|@net  Thu May 21 19:22:12 2020
From: re|chm@nj @end|ng |rom @bcg|ob@|@net (Jeff Reichman)
Date: Thu, 21 May 2020 12:22:12 -0500
Subject: [R] Attribute Combinations
References: <005701d62f94$5e5d81e0$1b1885a0$.ref@sbcglobal.net>
Message-ID: <005701d62f94$5e5d81e0$1b1885a0$@sbcglobal.net>

R-help forum

 

Looking for a function or some guidance for obtaining the percentage of
attribute combinations, for example

 

V1           V2           V3

A             A             B

A             B             C

A             A             D             

A             A             B

A             A             B

A             B             C

A             C             B             

A             C             B

A             B             C

A             C             C

 

Results

A,A,B     0.30

A,B,C     0.30

A,A,D    0.10

A,C,B     0.20 etc

 

Sincerely

 

Jeff Reichman

(314) 457-1966

 


	[[alternative HTML version deleted]]


From dd@|thorp @end|ng |rom u@g@@gov  Thu May 21 19:43:23 2020
From: dd@|thorp @end|ng |rom u@g@@gov (Dalthorp, Daniel)
Date: Thu, 21 May 2020 17:43:23 +0000
Subject: [R] [EXTERNAL]  Attribute Combinations
In-Reply-To: <005701d62f94$5e5d81e0$1b1885a0$@sbcglobal.net>
References: <005701d62f94$5e5d81e0$1b1885a0$.ref@sbcglobal.net>
 <005701d62f94$5e5d81e0$1b1885a0$@sbcglobal.net>
Message-ID: <MW2PR0901MB3801A62EE8B7BECEFB5AE9F2C4B70@MW2PR0901MB3801.namprd09.prod.outlook.com>

# assuming your data frame is named "x", you can get the counts of each combo:

table(do.call(paste0, x)) 

# and to get the proportions:

table(do.call(paste0, x))/nrow(x)


-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Jeff Reichman
Sent: Thursday, May 21, 2020 10:22 AM
To: R-help at r-project.org
Subject: [EXTERNAL] [R] Attribute Combinations

R-help forum

 

Looking for a function or some guidance for obtaining the percentage of attribute combinations, for example

 

V1           V2           V3

A             A             B

A             B             C

A             A             D             

A             A             B

A             A             B

A             B             C

A             C             B             

A             C             B

A             B             C

A             C             C

 

Results

A,A,B     0.30

A,B,C     0.30

A,A,D    0.10

A,C,B     0.20 etc

 

Sincerely

 

Jeff Reichman

(314) 457-1966

 


	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jrkr|de@u @end|ng |rom gm@||@com  Thu May 21 19:43:55 2020
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Thu, 21 May 2020 13:43:55 -0400
Subject: [R] Attribute Combinations
In-Reply-To: <005701d62f94$5e5d81e0$1b1885a0$@sbcglobal.net>
References: <005701d62f94$5e5d81e0$1b1885a0$.ref@sbcglobal.net>
 <005701d62f94$5e5d81e0$1b1885a0$@sbcglobal.net>
Message-ID: <CAKZQJMCRfNxEk-FwgrO53112H5mDaRZdcfP29sW4Lk76hJg6Cg@mail.gmail.com>

One possible way
library(tidyr)

dat1  <-  structure(list(V1 = c("A", "A", "A", "A", "A", "A", "A", "A",
"A", "A"), V2 = c("A", "B", "A", "A", "A", "B", "C", "C", "B",
"C"), V3 = c("B", "C", "D", "B", "B", "C", "B", "B", "C", "C"
)), class = "data.frame", row.names = c(NA, -10L))

dat2  <-  unite(dat1, att, V1, V2, V3, sep = ",")

prop.table(table(dat2$att))

A,A,B A,A,D A,B,C A,C,B A,C,C
  0.3   0.1   0.3   0.2   0.1

On Thu, 21 May 2020 at 13:22, Jeff Reichman <reichmanj at sbcglobal.net> wrote:

> R-help forum
>
>
>
> Looking for a function or some guidance for obtaining the percentage of
> attribute combinations, for example
>
>
>
> V1           V2           V3
>
> A             A             B
>
> A             B             C
>
> A             A             D
>
> A             A             B
>
> A             A             B
>
> A             B             C
>
> A             C             B
>
> A             C             B
>
> A             B             C
>
> A             C             C
>
>
>
> Results
>
> A,A,B     0.30
>
> A,B,C     0.30
>
> A,A,D    0.10
>
> A,C,B     0.20 etc
>
>
>
> Sincerely
>
>
>
> Jeff Reichman
>
> (314) 457-1966
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
John Kane
Kingston ON Canada

	[[alternative HTML version deleted]]


From |@t@z@hn @end|ng |rom gm@||@com  Thu May 21 19:56:22 2020
From: |@t@z@hn @end|ng |rom gm@||@com (Ista Zahn)
Date: Thu, 21 May 2020 13:56:22 -0400
Subject: [R] Attribute Combinations
In-Reply-To: <005701d62f94$5e5d81e0$1b1885a0$@sbcglobal.net>
References: <005701d62f94$5e5d81e0$1b1885a0$.ref@sbcglobal.net>
 <005701d62f94$5e5d81e0$1b1885a0$@sbcglobal.net>
Message-ID: <CA+vqiLEcJKaDpScu_o4BjVpYEOb59CeZ-eWPH6-ho8uBSWxKSA@mail.gmail.com>

Another one just for fun:

prop.table(table(interaction(x)))

or possibly

prop.table(table(droplevels(interaction(x))))

Best,
Ista


On Thu, May 21, 2020 at 1:22 PM Jeff Reichman <reichmanj at sbcglobal.net> wrote:
>
> R-help forum
>
>
>
> Looking for a function or some guidance for obtaining the percentage of
> attribute combinations, for example
>
>
>
> V1           V2           V3
>
> A             A             B
>
> A             B             C
>
> A             A             D
>
> A             A             B
>
> A             A             B
>
> A             B             C
>
> A             C             B
>
> A             C             B
>
> A             B             C
>
> A             C             C
>
>
>
> Results
>
> A,A,B     0.30
>
> A,B,C     0.30
>
> A,A,D    0.10
>
> A,C,B     0.20 etc
>
>
>
> Sincerely
>
>
>
> Jeff Reichman
>
> (314) 457-1966
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r@v|76 @end|ng |rom gm@||@com  Wed May 20 19:05:26 2020
From: r@v|76 @end|ng |rom gm@||@com (Ravi Jeyaraman)
Date: Wed, 20 May 2020 13:05:26 -0400
Subject: [R] Help with Parallel Processing
Message-ID: <0aa401d62ec8$dbdcb5e0$939621a0$@gmail.com>

Dear Friends,

 

I'm trying to run a bunch of tasks in parallel using 'Future' package and
for some reason, it's not able to find the data frames that I want it to
find.  I've created the below sample program to show what I'm doing.  Should
I be exporting the Global data to each child process?  I am not doing that
currently because I read somewhere that it's automatically done when using
the multisession plan.  Any idea what I'm doing wrong?  

 

Thanks

Ravi

 

 

if(!require('sqldf')) install.packages('sqldf')

if(!require('future')) install.packages('future')

if(!require('doFuture')) install.packages('doFuture')

if(!require('future.apply')) install.packages('future.apply')

 

library('sqldf')

library('future')

library("doFuture")

library("future.apply")

 

registerDoFuture()

plan(multisession, globals = TRUE, workers=5)

options(future.globals.maxSize=+Inf)

 

DATA_ASIA <- data.frame(c('NAME1', 'NAME2'))

DATA_EUROPE <- data.frame(c('NAME1', 'NAME2', 'NAME3'))

DATA_USA <- data.frame(c('NAME1', 'NAME2', 'NAME3', 'NAME4'))

DATA_AFRICA <- data.frame(c('NAME1'))

 

LEVEL <- c('ASIA_LEVEL', 'EUROPE_LEVEL', 'USA_LEVEL', 'AFRICA_LEVEL')

R_PROG <- c('SELECT COUNT(*) as COUNT FROM DATA_ASIA', 

            'SELECT COUNT(*) as COUNT FROM DATA_EUROPE', 

            'SELECT COUNT(*) as COUNT FROM DATA_USA', 

            'SELECT COUNT(*) as COUNT FROM DATA_AFRICA')

 

RULES_ALL <- data.frame(LEVEL, R_PROG)

 

RULES_ASIA <- subset(RULES_ALL, LEVEL == 'ASIA_LEVEL')

RESULT_ASIA <- future(data.table::rbindlist(lapply(1:nrow(RULES_ASIA),
function(x) sqldf(RULES_ASIA$R_PROG[x])), use.names = TRUE, fill=TRUE))

 

RULES_EUROPE <- subset(RULES_ALL, LEVEL == 'EUROPE_LEVEL')

RESULT_EUROPE <- future(data.table::rbindlist(lapply(1:nrow(RULES_EUROPE),
function(x) sqldf(RULES_EUROPE$R_PROG[x])), use.names = TRUE, fill=TRUE))

 

RULES_USA <- subset(RULES_ALL, LEVEL == 'USA_LEVEL')

RESULT_USA <- future(data.table::rbindlist(lapply(1:nrow(RULES_USA),
function(x) sqldf(RULES_USA$R_PROG[x])), use.names = TRUE, fill=TRUE))

 

RULES_AFRICA <- subset(RULES_ALL, LEVEL == 'AFRICA_LEVEL')

RESULTS_AFRICA <- future(data.table::rbindlist(lapply(1:nrow(RULES_AFRICA),
function(x) sqldf(RULES_AFRICA$R_PROG[x])), use.names = TRUE, fill=TRUE))

 

RESULT_ASIA <- value(RESULT_ASIA)

RESULT_EUROPE <- value(RESULT_EUROPE)

RESULT_USA <- value(RESULT_USA)

RESULTS_AFRICA <- value(RESULTS_AFRICA)

 

 

 



-- 
This email has been checked for viruses by AVG.
https://www.avg.com

	[[alternative HTML version deleted]]


From @|mon@m|chnow|cz @end|ng |rom mon@@h@edu  Thu May 21 03:46:01 2020
From: @|mon@m|chnow|cz @end|ng |rom mon@@h@edu (Simon Michnowicz)
Date: Thu, 21 May 2020 11:46:01 +1000
Subject: [R] Problem with checks on R/4.0.0
Message-ID: <CACmKtim3TAe=szw1v1Ph7jQV0k=FEHM9=87tRJ8bGftr9WtW6A@mail.gmail.com>

Dear R Group,
I can build a simple R/4.0.0 OK using   gcc/8.1.0, but when I tried to link
it with  the Intel MKL,  'make check' produced this error

tail tests/reg-tests-1d.Rout.fail

> (m <- cbind(0, c(NA, 0), 0:-1))
     [,1] [,2] [,3]
[1,]    0   NA    0
[2,]    0    0   -1
> nTypes <- eval(formals(base::norm)$type) # "O" "I" "F" "M" "2"
> stopifnot(is.na( print(vapply(nTypes, norm, 0., x = m)) )) # print():
show NA *or* NaN
 O  I  F  M  2
NA NA  1 NA NA
Error: is.na(print(vapply(nTypes, norm, 0, x = m))) are not all TRUE
Execution halted

Is this a significant error?
 There may be differences in how NaN are treated between GNU and MKL that
caused this.

regards


*---Simon Michnowicz *
Senior Application Specialist,  High-Performance Computing

*Research Support Services - eSolutions*
*Monash eResearch Centre*
Monash University
15 Innovation Walk, Building 75, Clayton Campus
Wellington Road, VIC 3800
Australia

T:  +61 3 9902 0794
M: +61 3 0418 302 046
E: simon.michnowicz at monash.edu
monash.edu

	[[alternative HTML version deleted]]


From ychen344 @end|ng |rom w|@c@edu  Thu May 21 18:24:28 2020
From: ychen344 @end|ng |rom w|@c@edu (YANJUN CHEN)
Date: Thu, 21 May 2020 16:24:28 +0000
Subject: [R] how to factor in the ID of the imported subtable to R table?
Message-ID: <017349F0-D220-44A9-99EC-1FC97D7BEDE3@wisc.edu>

Dear R community,

I am new to R?did some online tutorials and exercises in R playground. I was wondering if I could seek guidance on the following matter.

I have a set of 403 .csv files. Each.csv file contains the same layouts and distinguished by subject ID and date in the file name. The dataset looks like this:

Sub1-20170305.csv
Sub2-20180214.csv
?
Sub403-20191109.csv

I will use rbind function to combine 403 csv files in a single file (myFile). I will create two new variables (use mutate function) in myFile (subject ID and date). Is there a way to subtract subject ID (shown as ?Sub1, 2,,,403?) and date from the name of the csv file and then place them in ?subject ID? and ?date? in myFile?

Any info on the issue itself or where to look for will be appreciated.

Thanks,

CJ






	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu May 21 22:30:26 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 21 May 2020 13:30:26 -0700
Subject: [R] Problem with checks on R/4.0.0
In-Reply-To: <CACmKtim3TAe=szw1v1Ph7jQV0k=FEHM9=87tRJ8bGftr9WtW6A@mail.gmail.com>
References: <CACmKtim3TAe=szw1v1Ph7jQV0k=FEHM9=87tRJ8bGftr9WtW6A@mail.gmail.com>
Message-ID: <8FCE0BF4-D687-4324-9CC7-A50DDD33B060@dcn.davis.ca.us>

Do read the Posting Guide... you are on the wrong mailing list for this question.

On May 20, 2020 6:46:01 PM PDT, Simon Michnowicz via R-help <r-help at r-project.org> wrote:
>Dear R Group,
>I can build a simple R/4.0.0 OK using   gcc/8.1.0, but when I tried to
>link
>it with  the Intel MKL,  'make check' produced this error
>
>tail tests/reg-tests-1d.Rout.fail
>
>> (m <- cbind(0, c(NA, 0), 0:-1))
>     [,1] [,2] [,3]
>[1,]    0   NA    0
>[2,]    0    0   -1
>> nTypes <- eval(formals(base::norm)$type) # "O" "I" "F" "M" "2"
>> stopifnot(is.na( print(vapply(nTypes, norm, 0., x = m)) )) # print():
>show NA *or* NaN
> O  I  F  M  2
>NA NA  1 NA NA
>Error: is.na(print(vapply(nTypes, norm, 0, x = m))) are not all TRUE
>Execution halted
>
>Is this a significant error?
>There may be differences in how NaN are treated between GNU and MKL
>that
>caused this.
>
>regards
>
>
>*---Simon Michnowicz *
>Senior Application Specialist,  High-Performance Computing
>
>*Research Support Services - eSolutions*
>*Monash eResearch Centre*
>Monash University
>15 Innovation Walk, Building 75, Clayton Campus
>Wellington Road, VIC 3800
>Australia
>
>T:  +61 3 9902 0794
>M: +61 3 0418 302 046
>E: simon.michnowicz at monash.edu
>monash.edu
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From |@t@z@hn @end|ng |rom gm@||@com  Thu May 21 22:51:48 2020
From: |@t@z@hn @end|ng |rom gm@||@com (Ista Zahn)
Date: Thu, 21 May 2020 16:51:48 -0400
Subject: [R] Help with Parallel Processing
In-Reply-To: <0aa401d62ec8$dbdcb5e0$939621a0$@gmail.com>
References: <0aa401d62ec8$dbdcb5e0$939621a0$@gmail.com>
Message-ID: <CA+vqiLGQSf3TZj6RxyvF+535NnxcMLBVU7p31-aGsubdY_+6Nw@mail.gmail.com>

Hi Ravi,

Please read the ?future documentation, the answers to all your
questions are explained there.

Best,
Ista

On Thu, May 21, 2020 at 3:20 PM Ravi Jeyaraman <ravi76 at gmail.com> wrote:
>
> Dear Friends,
>
>
>
> I'm trying to run a bunch of tasks in parallel using 'Future' package and
> for some reason, it's not able to find the data frames that I want it to
> find.  I've created the below sample program to show what I'm doing.  Should
> I be exporting the Global data to each child process?  I am not doing that
> currently because I read somewhere that it's automatically done when using
> the multisession plan.  Any idea what I'm doing wrong?
>
>
>
> Thanks
>
> Ravi
>
>
>
>
>
> if(!require('sqldf')) install.packages('sqldf')
>
> if(!require('future')) install.packages('future')
>
> if(!require('doFuture')) install.packages('doFuture')
>
> if(!require('future.apply')) install.packages('future.apply')
>
>
>
> library('sqldf')
>
> library('future')
>
> library("doFuture")
>
> library("future.apply")
>
>
>
> registerDoFuture()
>
> plan(multisession, globals = TRUE, workers=5)
>
> options(future.globals.maxSize=+Inf)
>
>
>
> DATA_ASIA <- data.frame(c('NAME1', 'NAME2'))
>
> DATA_EUROPE <- data.frame(c('NAME1', 'NAME2', 'NAME3'))
>
> DATA_USA <- data.frame(c('NAME1', 'NAME2', 'NAME3', 'NAME4'))
>
> DATA_AFRICA <- data.frame(c('NAME1'))
>
>
>
> LEVEL <- c('ASIA_LEVEL', 'EUROPE_LEVEL', 'USA_LEVEL', 'AFRICA_LEVEL')
>
> R_PROG <- c('SELECT COUNT(*) as COUNT FROM DATA_ASIA',
>
>             'SELECT COUNT(*) as COUNT FROM DATA_EUROPE',
>
>             'SELECT COUNT(*) as COUNT FROM DATA_USA',
>
>             'SELECT COUNT(*) as COUNT FROM DATA_AFRICA')
>
>
>
> RULES_ALL <- data.frame(LEVEL, R_PROG)
>
>
>
> RULES_ASIA <- subset(RULES_ALL, LEVEL == 'ASIA_LEVEL')
>
> RESULT_ASIA <- future(data.table::rbindlist(lapply(1:nrow(RULES_ASIA),
> function(x) sqldf(RULES_ASIA$R_PROG[x])), use.names = TRUE, fill=TRUE))
>
>
>
> RULES_EUROPE <- subset(RULES_ALL, LEVEL == 'EUROPE_LEVEL')
>
> RESULT_EUROPE <- future(data.table::rbindlist(lapply(1:nrow(RULES_EUROPE),
> function(x) sqldf(RULES_EUROPE$R_PROG[x])), use.names = TRUE, fill=TRUE))
>
>
>
> RULES_USA <- subset(RULES_ALL, LEVEL == 'USA_LEVEL')
>
> RESULT_USA <- future(data.table::rbindlist(lapply(1:nrow(RULES_USA),
> function(x) sqldf(RULES_USA$R_PROG[x])), use.names = TRUE, fill=TRUE))
>
>
>
> RULES_AFRICA <- subset(RULES_ALL, LEVEL == 'AFRICA_LEVEL')
>
> RESULTS_AFRICA <- future(data.table::rbindlist(lapply(1:nrow(RULES_AFRICA),
> function(x) sqldf(RULES_AFRICA$R_PROG[x])), use.names = TRUE, fill=TRUE))
>
>
>
> RESULT_ASIA <- value(RESULT_ASIA)
>
> RESULT_EUROPE <- value(RESULT_EUROPE)
>
> RESULT_USA <- value(RESULT_USA)
>
> RESULTS_AFRICA <- value(RESULTS_AFRICA)
>
>
>
>
>
>
>
>
>
> --
> This email has been checked for viruses by AVG.
> https://www.avg.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dk@w|n@em|u@ @end|ng |rom gm@||@com  Thu May 21 22:00:37 2020
From: dk@w|n@em|u@ @end|ng |rom gm@||@com (David Winsemius)
Date: Thu, 21 May 2020 13:00:37 -0700
Subject: [R] 
 how to factor in the ID of the imported subtable to R table?
In-Reply-To: <017349F0-D220-44A9-99EC-1FC97D7BEDE3@wisc.edu>
References: <017349F0-D220-44A9-99EC-1FC97D7BEDE3@wisc.edu>
Message-ID: <055a1094-8d00-6bad-155f-6edc25acc692@gmail.com>


On 5/21/20 9:24 AM, YANJUN CHEN via R-help wrote:
> Dear R community,
>
> I am new to R?did some online tutorials and exercises in R playground. I was wondering if I could seek guidance on the following matter.
>
> I have a set of 403 .csv files. Each.csv file contains the same layouts and distinguished by subject ID and date in the file name. The dataset looks like this:
>
> Sub1-20170305.csv
> Sub2-20180214.csv
> ?
> Sub403-20191109.csv


Something along the lines of:

?regex ; ?sub

?read.table

?data.frame

?do.call

?rbind

myfiles <- lapply( list.files(your_path) , # each file name will be 
passed to anonymous function

 ?????????????????????????? function(nm) data.frame( subID = sub("-.+", 
"",? nm), # remove chars after "-"

date=sub("^.+-(.{8})[.]csv", "\\1", nm), #extract date as capture class

 ??????????????????????????????????????????? #assuming all files have 
same number of columns with no headers

 ????????????????????????????????????????? ? read.table( 
paste0(your_path, nm) )

big_file <- do.call(rbind, myfiles)

>
> I will use rbind function to combine 403 csv files in a single file (myFile). I will create two new variables (use mutate function) in myFile (subject ID and date). Is there a way to subtract subject ID (shown as ?Sub1, 2,,,403?) and date from the name of the csv file and then place them in ?subject ID? and ?date? in myFile?
>
> Any info on the issue itself or where to look for will be appreciated.


If you search StackOverflow or Rseek with topic terms " stacking 
multiple data files" you should find many worked examples.

> Thanks,
>
> CJ
>
>
>
>
>
>
> 	[[alternative HTML version deleted]]


You should now read the Posting Guide which will explain why you should 
NOT post in HTML.


Best;

David.

> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu May 21 23:02:48 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 21 May 2020 14:02:48 -0700
Subject: [R] Help with Parallel Processing
In-Reply-To: <CA+vqiLGQSf3TZj6RxyvF+535NnxcMLBVU7p31-aGsubdY_+6Nw@mail.gmail.com>
References: <0aa401d62ec8$dbdcb5e0$939621a0$@gmail.com>
 <CA+vqiLGQSf3TZj6RxyvF+535NnxcMLBVU7p31-aGsubdY_+6Nw@mail.gmail.com>
Message-ID: <FC7127B9-37D8-4E28-B940-7FC8A97A0E3C@dcn.davis.ca.us>

More specifically, read the vignettes. Actually, always start with the package vignettes if any are available.

On May 21, 2020 1:51:48 PM PDT, Ista Zahn <istazahn at gmail.com> wrote:
>Hi Ravi,
>
>Please read the ?future documentation, the answers to all your
>questions are explained there.
>
>Best,
>Ista
>
>On Thu, May 21, 2020 at 3:20 PM Ravi Jeyaraman <ravi76 at gmail.com>
>wrote:
>>
>> Dear Friends,
>>
>>
>>
>> I'm trying to run a bunch of tasks in parallel using 'Future' package
>and
>> for some reason, it's not able to find the data frames that I want it
>to
>> find.  I've created the below sample program to show what I'm doing. 
>Should
>> I be exporting the Global data to each child process?  I am not doing
>that
>> currently because I read somewhere that it's automatically done when
>using
>> the multisession plan.  Any idea what I'm doing wrong?
>>
>>
>>
>> Thanks
>>
>> Ravi
>>
>>
>>
>>
>>
>> if(!require('sqldf')) install.packages('sqldf')
>>
>> if(!require('future')) install.packages('future')
>>
>> if(!require('doFuture')) install.packages('doFuture')
>>
>> if(!require('future.apply')) install.packages('future.apply')
>>
>>
>>
>> library('sqldf')
>>
>> library('future')
>>
>> library("doFuture")
>>
>> library("future.apply")
>>
>>
>>
>> registerDoFuture()
>>
>> plan(multisession, globals = TRUE, workers=5)
>>
>> options(future.globals.maxSize=+Inf)
>>
>>
>>
>> DATA_ASIA <- data.frame(c('NAME1', 'NAME2'))
>>
>> DATA_EUROPE <- data.frame(c('NAME1', 'NAME2', 'NAME3'))
>>
>> DATA_USA <- data.frame(c('NAME1', 'NAME2', 'NAME3', 'NAME4'))
>>
>> DATA_AFRICA <- data.frame(c('NAME1'))
>>
>>
>>
>> LEVEL <- c('ASIA_LEVEL', 'EUROPE_LEVEL', 'USA_LEVEL', 'AFRICA_LEVEL')
>>
>> R_PROG <- c('SELECT COUNT(*) as COUNT FROM DATA_ASIA',
>>
>>             'SELECT COUNT(*) as COUNT FROM DATA_EUROPE',
>>
>>             'SELECT COUNT(*) as COUNT FROM DATA_USA',
>>
>>             'SELECT COUNT(*) as COUNT FROM DATA_AFRICA')
>>
>>
>>
>> RULES_ALL <- data.frame(LEVEL, R_PROG)
>>
>>
>>
>> RULES_ASIA <- subset(RULES_ALL, LEVEL == 'ASIA_LEVEL')
>>
>> RESULT_ASIA <-
>future(data.table::rbindlist(lapply(1:nrow(RULES_ASIA),
>> function(x) sqldf(RULES_ASIA$R_PROG[x])), use.names = TRUE,
>fill=TRUE))
>>
>>
>>
>> RULES_EUROPE <- subset(RULES_ALL, LEVEL == 'EUROPE_LEVEL')
>>
>> RESULT_EUROPE <-
>future(data.table::rbindlist(lapply(1:nrow(RULES_EUROPE),
>> function(x) sqldf(RULES_EUROPE$R_PROG[x])), use.names = TRUE,
>fill=TRUE))
>>
>>
>>
>> RULES_USA <- subset(RULES_ALL, LEVEL == 'USA_LEVEL')
>>
>> RESULT_USA <- future(data.table::rbindlist(lapply(1:nrow(RULES_USA),
>> function(x) sqldf(RULES_USA$R_PROG[x])), use.names = TRUE,
>fill=TRUE))
>>
>>
>>
>> RULES_AFRICA <- subset(RULES_ALL, LEVEL == 'AFRICA_LEVEL')
>>
>> RESULTS_AFRICA <-
>future(data.table::rbindlist(lapply(1:nrow(RULES_AFRICA),
>> function(x) sqldf(RULES_AFRICA$R_PROG[x])), use.names = TRUE,
>fill=TRUE))
>>
>>
>>
>> RESULT_ASIA <- value(RESULT_ASIA)
>>
>> RESULT_EUROPE <- value(RESULT_EUROPE)
>>
>> RESULT_USA <- value(RESULT_USA)
>>
>> RESULTS_AFRICA <- value(RESULTS_AFRICA)
>>
>>
>>
>>
>>
>>
>>
>>
>>
>> --
>> This email has been checked for viruses by AVG.
>> https://www.avg.com
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Fri May 22 00:23:51 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Thu, 21 May 2020 17:23:51 -0500
Subject: [R] how to show percentage of individuals for two groups on
 histogram?
Message-ID: <CAF9-5jPS7TS+SEzc6J6sgatS+Lh+k5h3kP_48mFXR4xBLZ2=Cw@mail.gmail.com>

Hello,

I have a data frame like this:
> head(a)
         FID   IID FLASER PLASER DIABDUR HBA1C ESRD   pheno
1 fam1000-03 G1000      1      1      38  10.2    1 control
2 fam1001-03 G1001      1      1      15   7.3    1 control
3 fam1003-03 G1003      1      2      17   7.0    1    case
4 fam1005-03 G1005      1      1      36   7.7    1 control
5 fam1009-03 G1009      1      1      23   7.6    1 control
6 fam1052-03 G1052      1      1      32   7.3    1 control

> dim(a)
[1] 1698    8

I am doing histogram plot via:
ggplot(a, aes(x=HBA1C, fill=pheno)) + geom_histogram(binwidth=.5,
position="dodge")

there is 848 who have "case" in pheno column and 892 who have
"control" in pheno column.

I would like to have on y-axis shown percentage of individuals which
have either "case" or "control" in pheno instead of count.

Please advise,
Ana


From he||yj @end|ng |rom uc@d@edu  Fri May 22 00:35:13 2020
From: he||yj @end|ng |rom uc@d@edu (John Helly)
Date: Thu, 21 May 2020 15:35:13 -0700
Subject: [R] R hangs when attempting open existing *.R file
Message-ID: <44d3d1de-409a-859e-1b27-2f016bc7ce6d@ucsd.edu>

Aloha.

Just installed 'Arbor day' and what was an occasional problem is now
preventing me from loading even a script I was routinely running the day
before.

Anyone have any idea what might be wrong now?? I just get a beachball
and Force Quit tells me R is not responding.

Thank you.
J.

-- 
John Helly, University of California, San Diego / San Diego Supercomputer Center / Scripps Institution of Oceanography / 760 840 8660 mobile / http://www.sdsc.edu/~hellyj
ORCID ID: orcid.org/0000-0002-3779-0603


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri May 22 00:43:58 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 21 May 2020 15:43:58 -0700
Subject: [R] R hangs when attempting open existing *.R file
In-Reply-To: <44d3d1de-409a-859e-1b27-2f016bc7ce6d@ucsd.edu>
References: <44d3d1de-409a-859e-1b27-2f016bc7ce6d@ucsd.edu>
Message-ID: <3F94EDFB-5592-4615-B713-D5BF4A77D5D0@dcn.davis.ca.us>

What do you mean by "open an existing R file"... did you try to load it with the source function or the MacOSX R App? If the latter you may be better off reading the archives of and/or asking in R-sig-mac...

On May 21, 2020 3:35:13 PM PDT, John Helly via R-help <r-help at r-project.org> wrote:
>Aloha.
>
>Just installed 'Arbor day' and what was an occasional problem is now
>preventing me from loading even a script I was routinely running the
>day
>before.
>
>Anyone have any idea what might be wrong now?? I just get a beachball
>and Force Quit tells me R is not responding.
>
>Thank you.
>J.

-- 
Sent from my phone. Please excuse my brevity.


From he||yj @end|ng |rom uc@d@edu  Fri May 22 00:47:18 2020
From: he||yj @end|ng |rom uc@d@edu (John Helly)
Date: Thu, 21 May 2020 15:47:18 -0700
Subject: [R] R hangs when attempting open existing *.R file
In-Reply-To: <3F94EDFB-5592-4615-B713-D5BF4A77D5D0@dcn.davis.ca.us>
References: <44d3d1de-409a-859e-1b27-2f016bc7ce6d@ucsd.edu>
 <3F94EDFB-5592-4615-B713-D5BF4A77D5D0@dcn.davis.ca.us>
Message-ID: <d260fbaf-29d7-19d5-3291-f1be4d887ebb@ucsd.edu>

MacOSX R App.? Am I missing something about using that GUI?

J.

On 5/21/20 15:43, Jeff Newmiller wrote:
> What do you mean by "open an existing R file"... did you try to load it with the source function or the MacOSX R App? If the latter you may be better off reading the archives of and/or asking in R-sig-mac...
>
> On May 21, 2020 3:35:13 PM PDT, John Helly via R-help <r-help at r-project.org> wrote:
>> Aloha.
>>
>> Just installed 'Arbor day' and what was an occasional problem is now
>> preventing me from loading even a script I was routinely running the
>> day
>> before.
>>
>> Anyone have any idea what might be wrong now?? I just get a beachball
>> and Force Quit tells me R is not responding.
>>
>> Thank you.
>> J.

-- 
John Helly, University of California, San Diego / San Diego Supercomputer Center / Scripps Institution of Oceanography / 760 840 8660 mobile / http://www.sdsc.edu/~hellyj
ORCID ID: orcid.org/0000-0002-3779-0603


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Fri May 22 00:52:40 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Thu, 21 May 2020 17:52:40 -0500
Subject: [R] how to show percentage of individuals for two groups on
 histogram?
In-Reply-To: <CAF9-5jPS7TS+SEzc6J6sgatS+Lh+k5h3kP_48mFXR4xBLZ2=Cw@mail.gmail.com>
References: <CAF9-5jPS7TS+SEzc6J6sgatS+Lh+k5h3kP_48mFXR4xBLZ2=Cw@mail.gmail.com>
Message-ID: <CAF9-5jNBY7Yxk4szXSrCci-KEF=d6=L-CXO_U7g4u3ghEm78Jw@mail.gmail.com>

the result would basically look something like this on in attach or
the overlay of those two plots


On Thu, May 21, 2020 at 5:23 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> Hello,
>
> I have a data frame like this:
> > head(a)
>          FID   IID FLASER PLASER DIABDUR HBA1C ESRD   pheno
> 1 fam1000-03 G1000      1      1      38  10.2    1 control
> 2 fam1001-03 G1001      1      1      15   7.3    1 control
> 3 fam1003-03 G1003      1      2      17   7.0    1    case
> 4 fam1005-03 G1005      1      1      36   7.7    1 control
> 5 fam1009-03 G1009      1      1      23   7.6    1 control
> 6 fam1052-03 G1052      1      1      32   7.3    1 control
>
> > dim(a)
> [1] 1698    8
>
> I am doing histogram plot via:
> ggplot(a, aes(x=HBA1C, fill=pheno)) + geom_histogram(binwidth=.5,
> position="dodge")
>
> there is 848 who have "case" in pheno column and 892 who have
> "control" in pheno column.
>
> I would like to have on y-axis shown percentage of individuals which
> have either "case" or "control" in pheno instead of count.
>
> Please advise,
> Ana

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Screen Shot 2020-05-21 at 5.49.37 PM.png
Type: image/png
Size: 52888 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200521/740baf08/attachment.png>

From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri May 22 00:53:19 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 21 May 2020 15:53:19 -0700
Subject: [R] R hangs when attempting open existing *.R file
In-Reply-To: <d260fbaf-29d7-19d5-3291-f1be4d887ebb@ucsd.edu>
References: <44d3d1de-409a-859e-1b27-2f016bc7ce6d@ucsd.edu>
 <3F94EDFB-5592-4615-B713-D5BF4A77D5D0@dcn.davis.ca.us>
 <d260fbaf-29d7-19d5-3291-f1be4d887ebb@ucsd.edu>
Message-ID: <2B1042E8-1DCC-414F-88DD-F3699331E0EB@dcn.davis.ca.us>

R does not exactly "open R files" the way you phrased it... on most platforms you use a text editor for that. R App is a bit of a special case... ergo, you should probably be asking on the specialized list.

On May 21, 2020 3:47:18 PM PDT, John Helly <hellyj at ucsd.edu> wrote:
>MacOSX R App.? Am I missing something about using that GUI?
>
>J.
>
>On 5/21/20 15:43, Jeff Newmiller wrote:
>> What do you mean by "open an existing R file"... did you try to load
>it with the source function or the MacOSX R App? If the latter you may
>be better off reading the archives of and/or asking in R-sig-mac...
>>
>> On May 21, 2020 3:35:13 PM PDT, John Helly via R-help
><r-help at r-project.org> wrote:
>>> Aloha.
>>>
>>> Just installed 'Arbor day' and what was an occasional problem is now
>>> preventing me from loading even a script I was routinely running the
>>> day
>>> before.
>>>
>>> Anyone have any idea what might be wrong now?? I just get a
>beachball
>>> and Force Quit tells me R is not responding.
>>>
>>> Thank you.
>>> J.

-- 
Sent from my phone. Please excuse my brevity.


From he||yj @end|ng |rom uc@d@edu  Fri May 22 00:54:59 2020
From: he||yj @end|ng |rom uc@d@edu (John Helly)
Date: Thu, 21 May 2020 15:54:59 -0700
Subject: [R] R hangs when attempting open existing *.R file
In-Reply-To: <2B1042E8-1DCC-414F-88DD-F3699331E0EB@dcn.davis.ca.us>
References: <44d3d1de-409a-859e-1b27-2f016bc7ce6d@ucsd.edu>
 <3F94EDFB-5592-4615-B713-D5BF4A77D5D0@dcn.davis.ca.us>
 <d260fbaf-29d7-19d5-3291-f1be4d887ebb@ucsd.edu>
 <2B1042E8-1DCC-414F-88DD-F3699331E0EB@dcn.davis.ca.us>
Message-ID: <c82d1123-792e-58c5-91f6-0a574c751351@ucsd.edu>

Ok. Thanks. I will do that.? Sorry for the confusion.
J.

On 5/21/20 15:53, Jeff Newmiller wrote:
> R does not exactly "open R files" the way you phrased it... on most platforms you use a text editor for that. R App is a bit of a special case... ergo, you should probably be asking on the specialized list.
>
> On May 21, 2020 3:47:18 PM PDT, John Helly <hellyj at ucsd.edu> wrote:
>> MacOSX R App.? Am I missing something about using that GUI?
>>
>> J.
>>
>> On 5/21/20 15:43, Jeff Newmiller wrote:
>>> What do you mean by "open an existing R file"... did you try to load
>> it with the source function or the MacOSX R App? If the latter you may
>> be better off reading the archives of and/or asking in R-sig-mac...
>>> On May 21, 2020 3:35:13 PM PDT, John Helly via R-help
>> <r-help at r-project.org> wrote:
>>>> Aloha.
>>>>
>>>> Just installed 'Arbor day' and what was an occasional problem is now
>>>> preventing me from loading even a script I was routinely running the
>>>> day
>>>> before.
>>>>
>>>> Anyone have any idea what might be wrong now?? I just get a
>> beachball
>>>> and Force Quit tells me R is not responding.
>>>>
>>>> Thank you.
>>>> J.

-- 
John Helly, University of California, San Diego / San Diego Supercomputer Center / Scripps Institution of Oceanography / 760 840 8660 mobile / http://www.sdsc.edu/~hellyj
ORCID ID: orcid.org/0000-0002-3779-0603


From drj|m|emon @end|ng |rom gm@||@com  Fri May 22 06:08:19 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 22 May 2020 14:08:19 +1000
Subject: [R] how to show percentage of individuals for two groups on
 histogram?
In-Reply-To: <CAF9-5jNBY7Yxk4szXSrCci-KEF=d6=L-CXO_U7g4u3ghEm78Jw@mail.gmail.com>
References: <CAF9-5jPS7TS+SEzc6J6sgatS+Lh+k5h3kP_48mFXR4xBLZ2=Cw@mail.gmail.com>
 <CAF9-5jNBY7Yxk4szXSrCci-KEF=d6=L-CXO_U7g4u3ghEm78Jw@mail.gmail.com>
Message-ID: <CA+8X3fW-kCsYxnps1LCUWqfoxHaYfhqNG_MOR0_Lu7zuEizFMg@mail.gmail.com>

Hi Ana,
My apologies for the pedestrian graphics, but it may help.

# a bit of fake data
aafd<-data.frame(FID=paste0("fam",1000:2739),
 IID=paste0("G",1000,2739),FLASER=rep(1,1740),
 PLASER=c(rep(1,892),rep(2,848)),
 DIABDUR=sample(10:50,1740,TRUE),
 HBAIC=rnorm(1740,mean=7.45,sd=2),ESRD=rep(1,1740),
 pheno=c(rep("control",892),rep("case",848)))
par(mfrow=c(2,1))
casepct<-table(cut(aafd$HBAIC[aafd$pheno=="case"],breaks=0:15))
controlpct<-table(cut(aafd$HBAIC[aafd$pheno=="control"],breaks=0:15))
par(mar=c(0,4,1,2))
barpos=barplot(100*casehist,names.arg=names(casepct),col="orange",
 space=0,ylab="Percentage",xaxt="n",ylim=c(0,25))
text(mean(barpos),23,
 "Cases: n=848, nulls=26, median=7.3, mean=7.45, sd=1.96")
box()
par(mar=c(3,4,0,2))
barplot(100*controlhist,names.arg=names(controlpct),
 space=0,ylab="Percentage",col="orange",ylim=c(0,25))
text(mean(barpos),23,
 "Controls: n=892, nulls=7, median=7.3, mean=7.45, sd=1.12")
box()

Jim

On Fri, May 22, 2020 at 9:08 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> the result would basically look something like this on in attach or
> the overlay of those two plots
>
>
> On Thu, May 21, 2020 at 5:23 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >
> > Hello,
> >
> > I have a data frame like this:
> > > head(a)
> >          FID   IID FLASER PLASER DIABDUR HBA1C ESRD   pheno
> > 1 fam1000-03 G1000      1      1      38  10.2    1 control
> > 2 fam1001-03 G1001      1      1      15   7.3    1 control
> > 3 fam1003-03 G1003      1      2      17   7.0    1    case
> > 4 fam1005-03 G1005      1      1      36   7.7    1 control
> > 5 fam1009-03 G1009      1      1      23   7.6    1 control
> > 6 fam1052-03 G1052      1      1      32   7.3    1 control
> >
> > > dim(a)
> > [1] 1698    8
> >
> > I am doing histogram plot via:
> > ggplot(a, aes(x=HBA1C, fill=pheno)) + geom_histogram(binwidth=.5,
> > position="dodge")
> >
> > there is 848 who have "case" in pheno column and 892 who have
> > "control" in pheno column.
> >
> > I would like to have on y-axis shown percentage of individuals which
> > have either "case" or "control" in pheno instead of count.
> >
> > Please advise,
> > Ana
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drj|m|emon @end|ng |rom gm@||@com  Fri May 22 06:14:03 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 22 May 2020 14:14:03 +1000
Subject: [R] how to show percentage of individuals for two groups on
 histogram?
In-Reply-To: <CA+8X3fW-kCsYxnps1LCUWqfoxHaYfhqNG_MOR0_Lu7zuEizFMg@mail.gmail.com>
References: <CAF9-5jPS7TS+SEzc6J6sgatS+Lh+k5h3kP_48mFXR4xBLZ2=Cw@mail.gmail.com>
 <CAF9-5jNBY7Yxk4szXSrCci-KEF=d6=L-CXO_U7g4u3ghEm78Jw@mail.gmail.com>
 <CA+8X3fW-kCsYxnps1LCUWqfoxHaYfhqNG_MOR0_Lu7zuEizFMg@mail.gmail.com>
Message-ID: <CA+8X3fWHY5Hf=jT=nERWCdnHvGgD=3j7uJZErkobWOp4UZekZw@mail.gmail.com>

Hi Ana,
Just noticed a typo from a hasty cut-paste. Two lines should read:

casehist<-table(cut(aafd$HBAIC[aafd$pheno=="case"],breaks=0:15))
controlhist<-table(cut(aafd$HBAIC[aafd$pheno=="control"],breaks=0:15))

Jim

On Fri, May 22, 2020 at 2:08 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi Ana,
> My apologies for the pedestrian graphics, but it may help.
>
> # a bit of fake data
> aafd<-data.frame(FID=paste0("fam",1000:2739),
>  IID=paste0("G",1000,2739),FLASER=rep(1,1740),
>  PLASER=c(rep(1,892),rep(2,848)),
>  DIABDUR=sample(10:50,1740,TRUE),
>  HBAIC=rnorm(1740,mean=7.45,sd=2),ESRD=rep(1,1740),
>  pheno=c(rep("control",892),rep("case",848)))
> par(mfrow=c(2,1))
> casepct<-table(cut(aafd$HBAIC[aafd$pheno=="case"],breaks=0:15))
> controlpct<-table(cut(aafd$HBAIC[aafd$pheno=="control"],breaks=0:15))
> par(mar=c(0,4,1,2))
> barpos=barplot(100*casehist,names.arg=names(casepct),col="orange",
>  space=0,ylab="Percentage",xaxt="n",ylim=c(0,25))
> text(mean(barpos),23,
>  "Cases: n=848, nulls=26, median=7.3, mean=7.45, sd=1.96")
> box()
> par(mar=c(3,4,0,2))
> barplot(100*controlhist,names.arg=names(controlpct),
>  space=0,ylab="Percentage",col="orange",ylim=c(0,25))
> text(mean(barpos),23,
>  "Controls: n=892, nulls=7, median=7.3, mean=7.45, sd=1.12")
> box()
>
> Jim
>
> On Fri, May 22, 2020 at 9:08 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >
> > the result would basically look something like this on in attach or
> > the overlay of those two plots
> >
> >
> > On Thu, May 21, 2020 at 5:23 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> > >
> > > Hello,
> > >
> > > I have a data frame like this:
> > > > head(a)
> > >          FID   IID FLASER PLASER DIABDUR HBA1C ESRD   pheno
> > > 1 fam1000-03 G1000      1      1      38  10.2    1 control
> > > 2 fam1001-03 G1001      1      1      15   7.3    1 control
> > > 3 fam1003-03 G1003      1      2      17   7.0    1    case
> > > 4 fam1005-03 G1005      1      1      36   7.7    1 control
> > > 5 fam1009-03 G1009      1      1      23   7.6    1 control
> > > 6 fam1052-03 G1052      1      1      32   7.3    1 control
> > >
> > > > dim(a)
> > > [1] 1698    8
> > >
> > > I am doing histogram plot via:
> > > ggplot(a, aes(x=HBA1C, fill=pheno)) + geom_histogram(binwidth=.5,
> > > position="dodge")
> > >
> > > there is 848 who have "case" in pheno column and 892 who have
> > > "control" in pheno column.
> > >
> > > I would like to have on y-axis shown percentage of individuals which
> > > have either "case" or "control" in pheno instead of count.
> > >
> > > Please advise,
> > > Ana
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From er|cjberger @end|ng |rom gm@||@com  Fri May 22 07:18:27 2020
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Fri, 22 May 2020 08:18:27 +0300
Subject: [R] how to show percentage of individuals for two groups on
 histogram?
In-Reply-To: <CA+8X3fWHY5Hf=jT=nERWCdnHvGgD=3j7uJZErkobWOp4UZekZw@mail.gmail.com>
References: <CAF9-5jPS7TS+SEzc6J6sgatS+Lh+k5h3kP_48mFXR4xBLZ2=Cw@mail.gmail.com>
 <CAF9-5jNBY7Yxk4szXSrCci-KEF=d6=L-CXO_U7g4u3ghEm78Jw@mail.gmail.com>
 <CA+8X3fW-kCsYxnps1LCUWqfoxHaYfhqNG_MOR0_Lu7zuEizFMg@mail.gmail.com>
 <CA+8X3fWHY5Hf=jT=nERWCdnHvGgD=3j7uJZErkobWOp4UZekZw@mail.gmail.com>
Message-ID: <CAGgJW75wUtZwH0=nODwfnnNNWyWYymHW_QrFVU=8N1wfgvCkvA@mail.gmail.com>

Hi Ana,
This is a very common question about ggplot.
A quick search turns up lots of hits that answer your question. Here
are a couple
https://community.rstudio.com/t/trouble-scaling-y-axis-to-percentages-from-counts/42999
https://stackoverflow.com/questions/3695497/show-instead-of-counts-in-charts-of-categorical-variables

>From reading those discussions, the following should work (untested)

ggplot(a, aes(x = HBA1C, fill=pheno)) + geom_histogram(aes(y =
stat(density)), binwidth = 0.5) +
      scale_y_continuous(labels = scales::percent_format())

HTH,
Eric


On Fri, May 22, 2020 at 7:18 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi Ana,
> Just noticed a typo from a hasty cut-paste. Two lines should read:
>
> casehist<-table(cut(aafd$HBAIC[aafd$pheno=="case"],breaks=0:15))
> controlhist<-table(cut(aafd$HBAIC[aafd$pheno=="control"],breaks=0:15))
>
> Jim
>
> On Fri, May 22, 2020 at 2:08 PM Jim Lemon <drjimlemon at gmail.com> wrote:
> >
> > Hi Ana,
> > My apologies for the pedestrian graphics, but it may help.
> >
> > # a bit of fake data
> > aafd<-data.frame(FID=paste0("fam",1000:2739),
> >  IID=paste0("G",1000,2739),FLASER=rep(1,1740),
> >  PLASER=c(rep(1,892),rep(2,848)),
> >  DIABDUR=sample(10:50,1740,TRUE),
> >  HBAIC=rnorm(1740,mean=7.45,sd=2),ESRD=rep(1,1740),
> >  pheno=c(rep("control",892),rep("case",848)))
> > par(mfrow=c(2,1))
> > casepct<-table(cut(aafd$HBAIC[aafd$pheno=="case"],breaks=0:15))
> > controlpct<-table(cut(aafd$HBAIC[aafd$pheno=="control"],breaks=0:15))
> > par(mar=c(0,4,1,2))
> > barpos=barplot(100*casehist,names.arg=names(casepct),col="orange",
> >  space=0,ylab="Percentage",xaxt="n",ylim=c(0,25))
> > text(mean(barpos),23,
> >  "Cases: n=848, nulls=26, median=7.3, mean=7.45, sd=1.96")
> > box()
> > par(mar=c(3,4,0,2))
> > barplot(100*controlhist,names.arg=names(controlpct),
> >  space=0,ylab="Percentage",col="orange",ylim=c(0,25))
> > text(mean(barpos),23,
> >  "Controls: n=892, nulls=7, median=7.3, mean=7.45, sd=1.12")
> > box()
> >
> > Jim
> >
> > On Fri, May 22, 2020 at 9:08 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> > >
> > > the result would basically look something like this on in attach or
> > > the overlay of those two plots
> > >
> > >
> > > On Thu, May 21, 2020 at 5:23 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> > > >
> > > > Hello,
> > > >
> > > > I have a data frame like this:
> > > > > head(a)
> > > >          FID   IID FLASER PLASER DIABDUR HBA1C ESRD   pheno
> > > > 1 fam1000-03 G1000      1      1      38  10.2    1 control
> > > > 2 fam1001-03 G1001      1      1      15   7.3    1 control
> > > > 3 fam1003-03 G1003      1      2      17   7.0    1    case
> > > > 4 fam1005-03 G1005      1      1      36   7.7    1 control
> > > > 5 fam1009-03 G1009      1      1      23   7.6    1 control
> > > > 6 fam1052-03 G1052      1      1      32   7.3    1 control
> > > >
> > > > > dim(a)
> > > > [1] 1698    8
> > > >
> > > > I am doing histogram plot via:
> > > > ggplot(a, aes(x=HBA1C, fill=pheno)) + geom_histogram(binwidth=.5,
> > > > position="dodge")
> > > >
> > > > there is 848 who have "case" in pheno column and 892 who have
> > > > "control" in pheno column.
> > > >
> > > > I would like to have on y-axis shown percentage of individuals which
> > > > have either "case" or "control" in pheno instead of count.
> > > >
> > > > Please advise,
> > > > Ana
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Fri May 22 10:05:58 2020
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 22 May 2020 10:05:58 +0200
Subject: [R] Problem with checks on R/4.0.0
In-Reply-To: <8FCE0BF4-D687-4324-9CC7-A50DDD33B060@dcn.davis.ca.us>
References: <CACmKtim3TAe=szw1v1Ph7jQV0k=FEHM9=87tRJ8bGftr9WtW6A@mail.gmail.com>
 <8FCE0BF4-D687-4324-9CC7-A50DDD33B060@dcn.davis.ca.us>
Message-ID: <24263.34790.474756.311487@stat.math.ethz.ch>

>>>>> Jeff Newmiller 
>>>>>     on Thu, 21 May 2020 13:30:26 -0700 writes:

    > Do read the Posting Guide... you are on the wrong mailing list for this question.
    > On May 20, 2020 6:46:01 PM PDT, Simon Michnowicz via R-help <r-help at r-project.org> wrote:
    >> Dear R Group,
    >> I can build a simple R/4.0.0 OK using   gcc/8.1.0, but when I tried to
    >> link
    >> it with  the Intel MKL,  'make check' produced this error
    >> 
    >> tail tests/reg-tests-1d.Rout.fail
    >> 
    >>> (m <- cbind(0, c(NA, 0), 0:-1))
    >> [,1] [,2] [,3]
    >> [1,]    0   NA    0
    >> [2,]    0    0   -1
    >>> nTypes <- eval(formals(base::norm)$type) # "O" "I" "F" "M" "2"
    >>> stopifnot(is.na( print(vapply(nTypes, norm, 0., x = m)) )) # print():
    >> show NA *or* NaN
    >> O  I  F  M  2
    >> NA NA  1 NA NA
    >> Error: is.na(print(vapply(nTypes, norm, 0, x = m))) are not all TRUE
    >> Execution halted
    >> 
    >> Is this a significant error?
    >> There may be differences in how NaN are treated between GNU and MKL
    >> that
    >> caused this.

Yes, there are such differences.
This one is indeed new bug in that version of Lapack  which
in the mean time has been fixed AFAIK...
definitely has been fixed in R's builtin version of Lapack.

Martin Maechler
ETH Zurich  and   R Core Team


    >> regards
    >> 
    >> 
    >> *---Simon Michnowicz *
    >> Senior Application Specialist,  High-Performance Computing
    >> 
    >> *Research Support Services - eSolutions*
    >> *Monash eResearch Centre*
    >> Monash University
    >> 15 Innovation Walk, Building 75, Clayton Campus
    >> Wellington Road, VIC 3800
    >> Australia
    >> 
    >> T:  +61 3 9902 0794
    >> M: +61 3 0418 302 046
    >> E: simon.michnowicz at monash.edu
    >> monash.edu
    >> 
    >> [[alternative HTML version deleted]]
    >> 
    >> ______________________________________________
    >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    >> https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide
    >> http://www.R-project.org/posting-guide.html
    >> and provide commented, minimal, self-contained, reproducible code.

    > -- 
    > Sent from my phone. Please excuse my brevity.

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Fri May 22 12:16:35 2020
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Fri, 22 May 2020 12:16:35 +0200
Subject: [R] How to use R0 package?
Message-ID: <CAMk+s2Trp1JJPzrKU-1=67EvmHj4ks94q8C9r_izS-bXSUgC8w@mail.gmail.com>

Hello,
I am trying ot get the R0 from the incidence data from China for the
COVID-19. I set the following:
```
library("R0")
x1 <- c(259,   457,   688,   769,  1771,  1459,  1737,  1981,  2099,  2589,
 2825,  3235,  3884,  3694,  3143,
        3385,  2652,  2973,  2467,  2015, 14108,  5090,  2641,  2008,
 2048,  1888,  1749,   391,   889,  823,
        648,   214,   508,   406,   433,   327,   427,   573,   202,   125,
  119,   139,   143,    99,    44,
        40,    19,    24,    15,     8,    11,    20,     0,    16,    13,
   13,    34,    39,    46,    39,
        78,    47,    67,    55,    54,    45,     0,    79,    36,    35,
   31,    19,    30,    39,    32,
        0,    63,    42,    46,    99,   108,    89,    46,    46,    26,
325,    27,    16,    12,    11,
        30,    10,     6,    12,    11,     3,     6,    22,     4,    12,
    1,     3,     3,     1,     2,
        2,     1,     1,    14,    17,     1,     7,     3,     4,     8,
  6,     7)
d1 = c("2020-01-23", "2020-01-24", "2020-01-25", "2020-01-26",
"2020-01-27", "2020-01-28", "2020-01-29",
       "2020-01-30", "2020-01-31", "2020-02-01", "2020-02-02",
"2020-02-03", "2020-02-04", "2020-02-05",
       "2020-02-06", "2020-02-07", "2020-02-08", "2020-02-09",
"2020-02-10", "2020-02-11", "2020-02-12",
       "2020-02-13", "2020-02-14", "2020-02-15", "2020-02-16",
"2020-02-17", "2020-02-18", "2020-02-19",
       "2020-02-20", "2020-02-21", "2020-02-22", "2020-02-23",
"2020-02-24", "2020-02-25", "2020-02-26",
       "2020-02-27", "2020-02-28", "2020-02-29", "2020-03-01",
"2020-03-02", "2020-03-03", "2020-03-04",
       "2020-03-05", "2020-03-06", "2020-03-07", "2020-03-08",
"2020-03-09", "2020-03-10", "2020-03-11",
       "2020-03-12", "2020-03-13", "2020-03-14", "2020-03-15",
"2020-03-16", "2020-03-17", "2020-03-18",
       "2020-03-19", "2020-03-20", "2020-03-21", "2020-03-22",
"2020-03-23", "2020-03-24", "2020-03-25",
       "2020-03-26", "2020-03-27", "2020-03-28", "2020-03-29",
"2020-03-30", "2020-03-31", "2020-04-01",
       "2020-04-02", "2020-04-03", "2020-04-04", "2020-04-05",
"2020-04-06", "2020-04-07", "2020-04-08",
       "2020-04-09", "2020-04-10", "2020-04-11", "2020-04-12",
"2020-04-13", "2020-04-14", "2020-04-15",
       "2020-04-16", "2020-04-17", "2020-04-18", "2020-04-19",
"2020-04-20", "2020-04-21", "2020-04-22",
       "2020-04-23", "2020-04-24", "2020-04-25", "2020-04-26", "2020-04-27"
,"2020-04-28", "2020-04-29",
       "2020-04-30", "2020-05-01", "2020-05-02", "2020-05-03",
"2020-05-04", "2020-05-05", "2020-05-06",
       "2020-05-07", "2020-05-08", "2020-05-09", "2020-05-10",
"2020-05-11", "2020-05-12", "2020-05-13",
       "2020-05-14", "2020-05-15", "2020-05-16", "2020-05-17", "2020-05-18")
names(x1) <- d1
pop = 1438443864
Ts_mean = 5.16
Ts_sd   = 1.49
N=10000
TODAY = Sys.Date()
mGT = generation.time("gamma", c(Ts_mean, Ts_sd))
R0 = estimate.R(x1, t=d1, GT=mGT, begin=as.Date(d1[1]), end=TODAY,
                methods="EG",pop.size=pop, nsim=N)
```
but when I run I get:
```
Error in if (end.nb <= begin.nb) stop("'begin' and 'end' are not
consistent.") :
  argument is of length zero
> as.Date(d1[1])
[1] "2020-01-23"
> TODAY
[1] "2020-05-22"
> str(TODAY)
 Date[1:1], format: "2020-05-22"
> str(as.Date(d1[1]))
 Date[1:1], format: "2020-01-23"
```
Since I provided both start and end in the same format, I don't understand
the error.
Any tips?
Thank you
-- 
Best regards,
Luigi

	[[alternative HTML version deleted]]


From er|cjberger @end|ng |rom gm@||@com  Fri May 22 12:34:52 2020
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Fri, 22 May 2020 13:34:52 +0300
Subject: [R] How to use R0 package?
In-Reply-To: <CAMk+s2Trp1JJPzrKU-1=67EvmHj4ks94q8C9r_izS-bXSUgC8w@mail.gmail.com>
References: <CAMk+s2Trp1JJPzrKU-1=67EvmHj4ks94q8C9r_izS-bXSUgC8w@mail.gmail.com>
Message-ID: <CAGgJW74JBMBtCEKAYLGwKo+GgFSZmdwOLFXydYdAO6t0qGxYAQ@mail.gmail.com>

Hi Luigi,
I am not familiar with the R0 package but I took a quick look.
The example in the documentation sets begin and end to integers.
Try setting begin = 1, end = 121 and see if that works.

HTH,
Eric

On Fri, May 22, 2020 at 1:17 PM Luigi Marongiu <marongiu.luigi at gmail.com> wrote:
>
> Hello,
> I am trying ot get the R0 from the incidence data from China for the
> COVID-19. I set the following:
> ```
> library("R0")
> x1 <- c(259,   457,   688,   769,  1771,  1459,  1737,  1981,  2099,  2589,
>  2825,  3235,  3884,  3694,  3143,
>         3385,  2652,  2973,  2467,  2015, 14108,  5090,  2641,  2008,
>  2048,  1888,  1749,   391,   889,  823,
>         648,   214,   508,   406,   433,   327,   427,   573,   202,   125,
>   119,   139,   143,    99,    44,
>         40,    19,    24,    15,     8,    11,    20,     0,    16,    13,
>    13,    34,    39,    46,    39,
>         78,    47,    67,    55,    54,    45,     0,    79,    36,    35,
>    31,    19,    30,    39,    32,
>         0,    63,    42,    46,    99,   108,    89,    46,    46,    26,
> 325,    27,    16,    12,    11,
>         30,    10,     6,    12,    11,     3,     6,    22,     4,    12,
>     1,     3,     3,     1,     2,
>         2,     1,     1,    14,    17,     1,     7,     3,     4,     8,
>   6,     7)
> d1 = c("2020-01-23", "2020-01-24", "2020-01-25", "2020-01-26",
> "2020-01-27", "2020-01-28", "2020-01-29",
>        "2020-01-30", "2020-01-31", "2020-02-01", "2020-02-02",
> "2020-02-03", "2020-02-04", "2020-02-05",
>        "2020-02-06", "2020-02-07", "2020-02-08", "2020-02-09",
> "2020-02-10", "2020-02-11", "2020-02-12",
>        "2020-02-13", "2020-02-14", "2020-02-15", "2020-02-16",
> "2020-02-17", "2020-02-18", "2020-02-19",
>        "2020-02-20", "2020-02-21", "2020-02-22", "2020-02-23",
> "2020-02-24", "2020-02-25", "2020-02-26",
>        "2020-02-27", "2020-02-28", "2020-02-29", "2020-03-01",
> "2020-03-02", "2020-03-03", "2020-03-04",
>        "2020-03-05", "2020-03-06", "2020-03-07", "2020-03-08",
> "2020-03-09", "2020-03-10", "2020-03-11",
>        "2020-03-12", "2020-03-13", "2020-03-14", "2020-03-15",
> "2020-03-16", "2020-03-17", "2020-03-18",
>        "2020-03-19", "2020-03-20", "2020-03-21", "2020-03-22",
> "2020-03-23", "2020-03-24", "2020-03-25",
>        "2020-03-26", "2020-03-27", "2020-03-28", "2020-03-29",
> "2020-03-30", "2020-03-31", "2020-04-01",
>        "2020-04-02", "2020-04-03", "2020-04-04", "2020-04-05",
> "2020-04-06", "2020-04-07", "2020-04-08",
>        "2020-04-09", "2020-04-10", "2020-04-11", "2020-04-12",
> "2020-04-13", "2020-04-14", "2020-04-15",
>        "2020-04-16", "2020-04-17", "2020-04-18", "2020-04-19",
> "2020-04-20", "2020-04-21", "2020-04-22",
>        "2020-04-23", "2020-04-24", "2020-04-25", "2020-04-26", "2020-04-27"
> ,"2020-04-28", "2020-04-29",
>        "2020-04-30", "2020-05-01", "2020-05-02", "2020-05-03",
> "2020-05-04", "2020-05-05", "2020-05-06",
>        "2020-05-07", "2020-05-08", "2020-05-09", "2020-05-10",
> "2020-05-11", "2020-05-12", "2020-05-13",
>        "2020-05-14", "2020-05-15", "2020-05-16", "2020-05-17", "2020-05-18")
> names(x1) <- d1
> pop = 1438443864
> Ts_mean = 5.16
> Ts_sd   = 1.49
> N=10000
> TODAY = Sys.Date()
> mGT = generation.time("gamma", c(Ts_mean, Ts_sd))
> R0 = estimate.R(x1, t=d1, GT=mGT, begin=as.Date(d1[1]), end=TODAY,
>                 methods="EG",pop.size=pop, nsim=N)
> ```
> but when I run I get:
> ```
> Error in if (end.nb <= begin.nb) stop("'begin' and 'end' are not
> consistent.") :
>   argument is of length zero
> > as.Date(d1[1])
> [1] "2020-01-23"
> > TODAY
> [1] "2020-05-22"
> > str(TODAY)
>  Date[1:1], format: "2020-05-22"
> > str(as.Date(d1[1]))
>  Date[1:1], format: "2020-01-23"
> ```
> Since I provided both start and end in the same format, I don't understand
> the error.
> Any tips?
> Thank you
> --
> Best regards,
> Luigi
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wgu2 @end|ng |rom pr|nceton@edu  Thu May 21 22:13:17 2020
From: wgu2 @end|ng |rom pr|nceton@edu (Will Underwood)
Date: Thu, 21 May 2020 15:13:17 -0500
Subject: [R] [R-pkgs] motifcluster: Motif-Based Spectral Clustering of
 Weighted Directed Networks
Message-ID: <d7d12ea2-dbbb-22a9-9f78-d3aad0f05460@princeton.edu>

Dear all,

I am happy to announce that the R package "motifcluster" is now 
available on CRAN. This package provides tools for spectral clustering 
of weighted directed networks using motif adjacency matrices. Methods 
perform well on large and sparse networks, and random sampling methods 
for generating weighted directed networks are also provided. Based on 
methodology detailed in Underwood, Elliott and Cucuringu (2020) 
<arXiv:2004.01293>.

Feel free to contact me with questions, and I hope you find this package 
to be of use.

CRAN: https://cran.r-project.org/web/packages/motifcluster/index.html
GitHub: https://github.com/WGUNDERWOOD/motifcluster
Issues: https://github.com/WGUNDERWOOD/motifcluster/issues
Reference: https://arxiv.org/abs/2004.01293

Best wishes,

William G. Underwood
ORFE Department
Princeton University


	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From L@urentRHe|p @end|ng |rom |ree@|r  Fri May 22 13:47:40 2020
From: L@urentRHe|p @end|ng |rom |ree@|r (Laurent Rhelp)
Date: Fri, 22 May 2020 13:47:40 +0200
Subject: [R] iterators : checkFunc with ireadLines
In-Reply-To: <20200520104614.45876937@Tarkus>
References: <72174dd3-a819-b5fd-5a6f-2a6b46342774-2015@free.fr>
 <20200520104614.45876937@Tarkus>
Message-ID: <27f44720-d368-c2cc-39c9-8629da885165@free.fr>

Hi Ivan,
 ? Endeed, it is a good idea. I am under MSwindows but I can use the 
bash command I use with git. I will see how to do that with the unix 
command lines.


Le 20/05/2020 ? 09:46, Ivan Krylov a ?crit?:
> Hi Laurent,
>
> I am not saying this will work every time and I do recognise that this
> is very different from a more general solution that you had envisioned,
> but if you are on an UNIX-like system or have the relevant utilities
> installed and on the %PATH% on Windows, you can filter the input file
> line-by-line using a pipe and an external program:
>
> On Sun, 17 May 2020 15:52:30 +0200
> Laurent Rhelp <LaurentRHelp at free.fr> wrote:
>
>> # sensors to keep
>> sensors <-? c("N053", "N163")
> # filter on the beginning of the line
> i <- pipe("grep -E '^(N053|N163)' test.txt")
> # or:
> # filter on the beginning of the given column
> # (use $2 for the second column, etc.)
> i <- pipe("awk '($1 ~ \"^(N053|N163)\")' test.txt")
> # or:
> # since your message is full of Unicode non-breaking spaces, I have to
> # bring in heavier machinery to handle those correctly;
> # only this solution manages to match full column values
> # (here you can also use $F[1] for second column and so on)
> i <- pipe("perl -CSD -F'\\s+' -lE \\
>   'print join qq{\\t}, @F if $F[0] =~ /^(N053|N163)$/' \\
>   test.txt
> ")
> lines <- read.table(i) # closes i when done
>
> The downside of this approach is having to shell-escape the command
> lines, which can become complicated, and choosing between use of regular
> expressions and more wordy programs (Unicode whitespace in the input
> doesn't help, either).
>


-- 
L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e par le logiciel antivirus Avast.
https://www.avast.com/antivirus


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Fri May 22 14:31:48 2020
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Fri, 22 May 2020 14:31:48 +0200
Subject: [R] How to use R0 package?
In-Reply-To: <CAGgJW777jvC4GMzex_TE3UNpsXjbwVY=Zh6VD+iABUc_9cf=Kg@mail.gmail.com>
References: <CAMk+s2Trp1JJPzrKU-1=67EvmHj4ks94q8C9r_izS-bXSUgC8w@mail.gmail.com>
 <CAGgJW74JBMBtCEKAYLGwKo+GgFSZmdwOLFXydYdAO6t0qGxYAQ@mail.gmail.com>
 <CAMk+s2QULpsOSpvJF3rMfjOyvQz0RvDciXbziCTeaamrGxoHYQ@mail.gmail.com>
 <CAGgJW777jvC4GMzex_TE3UNpsXjbwVY=Zh6VD+iABUc_9cf=Kg@mail.gmail.com>
Message-ID: <CAMk+s2RkLoeptNF+9cPZWbUnZmFegHPusHNC=ThEd=D3WChsNA@mail.gmail.com>

In theory, it works
```
> R0 = estimate.R(x1, t=d1, GT=mGT, begin=1, end=117,
                methods="EG",pop.size=pop, nsim=N)
>R0
Reproduction number estimate using  Exponential Growth  method.
R :  0.7425278[ 0.7409297 , 0.7441229 ]
```
but I am not happy because 1. I have to use numbers instead of
variables and 2. numbers instead of dates (which are instead reported in
the examples...).
Even if I force to an integer, I still get an error:
```
> R0 = estimate.R(x1, t=d1, GT=mGT, begin=1, end=length(x1),
+                 methods="EG",pop.size=pop, nsim=N)
Error in integrity.checks(epid, t, GT, begin, end, date.first.obs,
time.step,  :
  If both 'begin'= 1  and 'end'= 117  are provided, they must be of the
same class (dates, character strings or integers).
> int
Error: object 'int' not found
> R0 = estimate.R(x1, t=d1, GT=mGT, begin=1, end=as.integer(length(x1)),
+                 methods="EG",pop.size=pop, nsim=N)
Error in integrity.checks(epid, t, GT, begin, end, date.first.obs,
time.step,  :
  If both 'begin'= 1  and 'end'= 117  are provided, they must be of the
same class (dates, character strings or integers).
```



On Fri, May 22, 2020 at 1:51 PM Eric Berger <ericjberger at gmail.com> wrote:

> Hi Luigi,
> how about begin=1L (to force it to be integer).
> Also please keep the correspondence on the help list.
>
> Best,
> Eric
>
> On Fri, May 22, 2020 at 2:40 PM Luigi Marongiu <marongiu.luigi at gmail.com>
> wrote:
> >
> > Same error:
> > ```
> > > R0 = estimate.R(x1, t=d1, GT=mGT, begin=1, end=length(x1),
> >                 methods="EG",pop.size=pop, nsim=N)
> >  Error in integrity.checks(epid, t, GT, begin, end, date.first.obs,
> time.step,  :
> >   If both 'begin'= 1  and 'end'= 117  are provided, they must be of the
> same class (dates, character strings or integers).
> > > str(length(x1))
> >  int 117
> > ```
> >
> >
> > On Fri, May 22, 2020 at 12:35 PM Eric Berger <ericjberger at gmail.com>
> wrote:
> >>
> >> Hi Luigi,
> >> I am not familiar with the R0 package but I took a quick look.
> >> The example in the documentation sets begin and end to integers.
> >> Try setting begin = 1, end = 121 and see if that works.
> >>
> >> HTH,
> >> Eric
> >>
> >> On Fri, May 22, 2020 at 1:17 PM Luigi Marongiu <
> marongiu.luigi at gmail.com> wrote:
> >> >
> >> > Hello,
> >> > I am trying ot get the R0 from the incidence data from China for the
> >> > COVID-19. I set the following:
> >> > ```
> >> > library("R0")
> >> > x1 <- c(259,   457,   688,   769,  1771,  1459,  1737,  1981,  2099,
> 2589,
> >> >  2825,  3235,  3884,  3694,  3143,
> >> >         3385,  2652,  2973,  2467,  2015, 14108,  5090,  2641,  2008,
> >> >  2048,  1888,  1749,   391,   889,  823,
> >> >         648,   214,   508,   406,   433,   327,   427,   573,   202,
>  125,
> >> >   119,   139,   143,    99,    44,
> >> >         40,    19,    24,    15,     8,    11,    20,     0,    16,
>   13,
> >> >    13,    34,    39,    46,    39,
> >> >         78,    47,    67,    55,    54,    45,     0,    79,    36,
>   35,
> >> >    31,    19,    30,    39,    32,
> >> >         0,    63,    42,    46,    99,   108,    89,    46,    46,
> 26,
> >> > 325,    27,    16,    12,    11,
> >> >         30,    10,     6,    12,    11,     3,     6,    22,     4,
>   12,
> >> >     1,     3,     3,     1,     2,
> >> >         2,     1,     1,    14,    17,     1,     7,     3,     4,
>  8,
> >> >   6,     7)
> >> > d1 = c("2020-01-23", "2020-01-24", "2020-01-25", "2020-01-26",
> >> > "2020-01-27", "2020-01-28", "2020-01-29",
> >> >        "2020-01-30", "2020-01-31", "2020-02-01", "2020-02-02",
> >> > "2020-02-03", "2020-02-04", "2020-02-05",
> >> >        "2020-02-06", "2020-02-07", "2020-02-08", "2020-02-09",
> >> > "2020-02-10", "2020-02-11", "2020-02-12",
> >> >        "2020-02-13", "2020-02-14", "2020-02-15", "2020-02-16",
> >> > "2020-02-17", "2020-02-18", "2020-02-19",
> >> >        "2020-02-20", "2020-02-21", "2020-02-22", "2020-02-23",
> >> > "2020-02-24", "2020-02-25", "2020-02-26",
> >> >        "2020-02-27", "2020-02-28", "2020-02-29", "2020-03-01",
> >> > "2020-03-02", "2020-03-03", "2020-03-04",
> >> >        "2020-03-05", "2020-03-06", "2020-03-07", "2020-03-08",
> >> > "2020-03-09", "2020-03-10", "2020-03-11",
> >> >        "2020-03-12", "2020-03-13", "2020-03-14", "2020-03-15",
> >> > "2020-03-16", "2020-03-17", "2020-03-18",
> >> >        "2020-03-19", "2020-03-20", "2020-03-21", "2020-03-22",
> >> > "2020-03-23", "2020-03-24", "2020-03-25",
> >> >        "2020-03-26", "2020-03-27", "2020-03-28", "2020-03-29",
> >> > "2020-03-30", "2020-03-31", "2020-04-01",
> >> >        "2020-04-02", "2020-04-03", "2020-04-04", "2020-04-05",
> >> > "2020-04-06", "2020-04-07", "2020-04-08",
> >> >        "2020-04-09", "2020-04-10", "2020-04-11", "2020-04-12",
> >> > "2020-04-13", "2020-04-14", "2020-04-15",
> >> >        "2020-04-16", "2020-04-17", "2020-04-18", "2020-04-19",
> >> > "2020-04-20", "2020-04-21", "2020-04-22",
> >> >        "2020-04-23", "2020-04-24", "2020-04-25", "2020-04-26",
> "2020-04-27"
> >> > ,"2020-04-28", "2020-04-29",
> >> >        "2020-04-30", "2020-05-01", "2020-05-02", "2020-05-03",
> >> > "2020-05-04", "2020-05-05", "2020-05-06",
> >> >        "2020-05-07", "2020-05-08", "2020-05-09", "2020-05-10",
> >> > "2020-05-11", "2020-05-12", "2020-05-13",
> >> >        "2020-05-14", "2020-05-15", "2020-05-16", "2020-05-17",
> "2020-05-18")
> >> > names(x1) <- d1
> >> > pop = 1438443864
> >> > Ts_mean = 5.16
> >> > Ts_sd   = 1.49
> >> > N=10000
> >> > TODAY = Sys.Date()
> >> > mGT = generation.time("gamma", c(Ts_mean, Ts_sd))
> >> > R0 = estimate.R(x1, t=d1, GT=mGT, begin=as.Date(d1[1]), end=TODAY,
> >> >                 methods="EG",pop.size=pop, nsim=N)
> >> > ```
> >> > but when I run I get:
> >> > ```
> >> > Error in if (end.nb <= begin.nb) stop("'begin' and 'end' are not
> >> > consistent.") :
> >> >   argument is of length zero
> >> > > as.Date(d1[1])
> >> > [1] "2020-01-23"
> >> > > TODAY
> >> > [1] "2020-05-22"
> >> > > str(TODAY)
> >> >  Date[1:1], format: "2020-05-22"
> >> > > str(as.Date(d1[1]))
> >> >  Date[1:1], format: "2020-01-23"
> >> > ```
> >> > Since I provided both start and end in the same format, I don't
> understand
> >> > the error.
> >> > Any tips?
> >> > Thank you
> >> > --
> >> > Best regards,
> >> > Luigi
> >> >
> >> >         [[alternative HTML version deleted]]
> >> >
> >> > ______________________________________________
> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> > --
> > Best regards,
> > Luigi
>


-- 
Best regards,
Luigi

	[[alternative HTML version deleted]]


From er|cjberger @end|ng |rom gm@||@com  Fri May 22 14:45:15 2020
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Fri, 22 May 2020 15:45:15 +0300
Subject: [R] How to use R0 package?
In-Reply-To: <CAMk+s2RkLoeptNF+9cPZWbUnZmFegHPusHNC=ThEd=D3WChsNA@mail.gmail.com>
References: <CAMk+s2Trp1JJPzrKU-1=67EvmHj4ks94q8C9r_izS-bXSUgC8w@mail.gmail.com>
 <CAGgJW74JBMBtCEKAYLGwKo+GgFSZmdwOLFXydYdAO6t0qGxYAQ@mail.gmail.com>
 <CAMk+s2QULpsOSpvJF3rMfjOyvQz0RvDciXbziCTeaamrGxoHYQ@mail.gmail.com>
 <CAGgJW777jvC4GMzex_TE3UNpsXjbwVY=Zh6VD+iABUc_9cf=Kg@mail.gmail.com>
 <CAMk+s2RkLoeptNF+9cPZWbUnZmFegHPusHNC=ThEd=D3WChsNA@mail.gmail.com>
Message-ID: <CAGgJW77LyKy4RE-tVAwmBkE5RH+EQatf2CC6uobvxOepdVoovg@mail.gmail.com>

class(length(x1))
"integer"

Your problem is thinking that begin=1 means you are passing begin as
an integer.
class(1)
"numeric"
class(1L)
"integer"

You should pass:  begin=1L, end=length(x1)

Best,
Eric

On Fri, May 22, 2020 at 3:31 PM Luigi Marongiu <marongiu.luigi at gmail.com> wrote:
>
> In theory, it works
> ```
> > R0 = estimate.R(x1, t=d1, GT=mGT, begin=1, end=117,
>                 methods="EG",pop.size=pop, nsim=N)
> >R0
> Reproduction number estimate using  Exponential Growth  method.
> R :  0.7425278[ 0.7409297 , 0.7441229 ]
> ```
> but I am not happy because 1. I have to use numbers instead of variables and 2. numbers instead of dates (which are instead reported in the examples...).
> Even if I force to an integer, I still get an error:
> ```
> > R0 = estimate.R(x1, t=d1, GT=mGT, begin=1, end=length(x1),
> +                 methods="EG",pop.size=pop, nsim=N)
> Error in integrity.checks(epid, t, GT, begin, end, date.first.obs, time.step,  :
>   If both 'begin'= 1  and 'end'= 117  are provided, they must be of the same class (dates, character strings or integers).
> > int
> Error: object 'int' not found
> > R0 = estimate.R(x1, t=d1, GT=mGT, begin=1, end=as.integer(length(x1)),
> +                 methods="EG",pop.size=pop, nsim=N)
> Error in integrity.checks(epid, t, GT, begin, end, date.first.obs, time.step,  :
>   If both 'begin'= 1  and 'end'= 117  are provided, they must be of the same class (dates, character strings or integers).
> ```
>
>
>
> On Fri, May 22, 2020 at 1:51 PM Eric Berger <ericjberger at gmail.com> wrote:
>>
>> Hi Luigi,
>> how about begin=1L (to force it to be integer).
>> Also please keep the correspondence on the help list.
>>
>> Best,
>> Eric
>>
>> On Fri, May 22, 2020 at 2:40 PM Luigi Marongiu <marongiu.luigi at gmail.com> wrote:
>> >
>> > Same error:
>> > ```
>> > > R0 = estimate.R(x1, t=d1, GT=mGT, begin=1, end=length(x1),
>> >                 methods="EG",pop.size=pop, nsim=N)
>> >  Error in integrity.checks(epid, t, GT, begin, end, date.first.obs, time.step,  :
>> >   If both 'begin'= 1  and 'end'= 117  are provided, they must be of the same class (dates, character strings or integers).
>> > > str(length(x1))
>> >  int 117
>> > ```
>> >
>> >
>> > On Fri, May 22, 2020 at 12:35 PM Eric Berger <ericjberger at gmail.com> wrote:
>> >>
>> >> Hi Luigi,
>> >> I am not familiar with the R0 package but I took a quick look.
>> >> The example in the documentation sets begin and end to integers.
>> >> Try setting begin = 1, end = 121 and see if that works.
>> >>
>> >> HTH,
>> >> Eric
>> >>
>> >> On Fri, May 22, 2020 at 1:17 PM Luigi Marongiu <marongiu.luigi at gmail.com> wrote:
>> >> >
>> >> > Hello,
>> >> > I am trying ot get the R0 from the incidence data from China for the
>> >> > COVID-19. I set the following:
>> >> > ```
>> >> > library("R0")
>> >> > x1 <- c(259,   457,   688,   769,  1771,  1459,  1737,  1981,  2099,  2589,
>> >> >  2825,  3235,  3884,  3694,  3143,
>> >> >         3385,  2652,  2973,  2467,  2015, 14108,  5090,  2641,  2008,
>> >> >  2048,  1888,  1749,   391,   889,  823,
>> >> >         648,   214,   508,   406,   433,   327,   427,   573,   202,   125,
>> >> >   119,   139,   143,    99,    44,
>> >> >         40,    19,    24,    15,     8,    11,    20,     0,    16,    13,
>> >> >    13,    34,    39,    46,    39,
>> >> >         78,    47,    67,    55,    54,    45,     0,    79,    36,    35,
>> >> >    31,    19,    30,    39,    32,
>> >> >         0,    63,    42,    46,    99,   108,    89,    46,    46,    26,
>> >> > 325,    27,    16,    12,    11,
>> >> >         30,    10,     6,    12,    11,     3,     6,    22,     4,    12,
>> >> >     1,     3,     3,     1,     2,
>> >> >         2,     1,     1,    14,    17,     1,     7,     3,     4,     8,
>> >> >   6,     7)
>> >> > d1 = c("2020-01-23", "2020-01-24", "2020-01-25", "2020-01-26",
>> >> > "2020-01-27", "2020-01-28", "2020-01-29",
>> >> >        "2020-01-30", "2020-01-31", "2020-02-01", "2020-02-02",
>> >> > "2020-02-03", "2020-02-04", "2020-02-05",
>> >> >        "2020-02-06", "2020-02-07", "2020-02-08", "2020-02-09",
>> >> > "2020-02-10", "2020-02-11", "2020-02-12",
>> >> >        "2020-02-13", "2020-02-14", "2020-02-15", "2020-02-16",
>> >> > "2020-02-17", "2020-02-18", "2020-02-19",
>> >> >        "2020-02-20", "2020-02-21", "2020-02-22", "2020-02-23",
>> >> > "2020-02-24", "2020-02-25", "2020-02-26",
>> >> >        "2020-02-27", "2020-02-28", "2020-02-29", "2020-03-01",
>> >> > "2020-03-02", "2020-03-03", "2020-03-04",
>> >> >        "2020-03-05", "2020-03-06", "2020-03-07", "2020-03-08",
>> >> > "2020-03-09", "2020-03-10", "2020-03-11",
>> >> >        "2020-03-12", "2020-03-13", "2020-03-14", "2020-03-15",
>> >> > "2020-03-16", "2020-03-17", "2020-03-18",
>> >> >        "2020-03-19", "2020-03-20", "2020-03-21", "2020-03-22",
>> >> > "2020-03-23", "2020-03-24", "2020-03-25",
>> >> >        "2020-03-26", "2020-03-27", "2020-03-28", "2020-03-29",
>> >> > "2020-03-30", "2020-03-31", "2020-04-01",
>> >> >        "2020-04-02", "2020-04-03", "2020-04-04", "2020-04-05",
>> >> > "2020-04-06", "2020-04-07", "2020-04-08",
>> >> >        "2020-04-09", "2020-04-10", "2020-04-11", "2020-04-12",
>> >> > "2020-04-13", "2020-04-14", "2020-04-15",
>> >> >        "2020-04-16", "2020-04-17", "2020-04-18", "2020-04-19",
>> >> > "2020-04-20", "2020-04-21", "2020-04-22",
>> >> >        "2020-04-23", "2020-04-24", "2020-04-25", "2020-04-26", "2020-04-27"
>> >> > ,"2020-04-28", "2020-04-29",
>> >> >        "2020-04-30", "2020-05-01", "2020-05-02", "2020-05-03",
>> >> > "2020-05-04", "2020-05-05", "2020-05-06",
>> >> >        "2020-05-07", "2020-05-08", "2020-05-09", "2020-05-10",
>> >> > "2020-05-11", "2020-05-12", "2020-05-13",
>> >> >        "2020-05-14", "2020-05-15", "2020-05-16", "2020-05-17", "2020-05-18")
>> >> > names(x1) <- d1
>> >> > pop = 1438443864
>> >> > Ts_mean = 5.16
>> >> > Ts_sd   = 1.49
>> >> > N=10000
>> >> > TODAY = Sys.Date()
>> >> > mGT = generation.time("gamma", c(Ts_mean, Ts_sd))
>> >> > R0 = estimate.R(x1, t=d1, GT=mGT, begin=as.Date(d1[1]), end=TODAY,
>> >> >                 methods="EG",pop.size=pop, nsim=N)
>> >> > ```
>> >> > but when I run I get:
>> >> > ```
>> >> > Error in if (end.nb <= begin.nb) stop("'begin' and 'end' are not
>> >> > consistent.") :
>> >> >   argument is of length zero
>> >> > > as.Date(d1[1])
>> >> > [1] "2020-01-23"
>> >> > > TODAY
>> >> > [1] "2020-05-22"
>> >> > > str(TODAY)
>> >> >  Date[1:1], format: "2020-05-22"
>> >> > > str(as.Date(d1[1]))
>> >> >  Date[1:1], format: "2020-01-23"
>> >> > ```
>> >> > Since I provided both start and end in the same format, I don't understand
>> >> > the error.
>> >> > Any tips?
>> >> > Thank you
>> >> > --
>> >> > Best regards,
>> >> > Luigi
>> >> >
>> >> >         [[alternative HTML version deleted]]
>> >> >
>> >> > ______________________________________________
>> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> > https://stat.ethz.ch/mailman/listinfo/r-help
>> >> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> >> > and provide commented, minimal, self-contained, reproducible code.
>> >
>> >
>> >
>> > --
>> > Best regards,
>> > Luigi
>
>
>
> --
> Best regards,
> Luigi


From o||v|er@crouzet @end|ng |rom un|v-n@nte@@|r  Fri May 22 15:03:14 2020
From: o||v|er@crouzet @end|ng |rom un|v-n@nte@@|r (Olivier Crouzet)
Date: Fri, 22 May 2020 15:03:14 +0200
Subject: [R] How to use R0 package?
In-Reply-To: <CAMk+s2RkLoeptNF+9cPZWbUnZmFegHPusHNC=ThEd=D3WChsNA@mail.gmail.com>
References: <CAMk+s2Trp1JJPzrKU-1=67EvmHj4ks94q8C9r_izS-bXSUgC8w@mail.gmail.com>
 <CAGgJW74JBMBtCEKAYLGwKo+GgFSZmdwOLFXydYdAO6t0qGxYAQ@mail.gmail.com>
 <CAMk+s2QULpsOSpvJF3rMfjOyvQz0RvDciXbziCTeaamrGxoHYQ@mail.gmail.com>
 <CAGgJW777jvC4GMzex_TE3UNpsXjbwVY=Zh6VD+iABUc_9cf=Kg@mail.gmail.com>
 <CAMk+s2RkLoeptNF+9cPZWbUnZmFegHPusHNC=ThEd=D3WChsNA@mail.gmail.com>
Message-ID: <20200522150314.afbfdc4fffebe4818112e699@univ-nantes.fr>

Hi,

you should be able to convert your date variables to integers (usually
viewed as the elapse between 1970/01/01 and today) by using date
conversion to integers:

TODAY="2020-05-22"
as.Date(TODAY)
[1] "2020-05-22"
> as.integer(as.Date(TODAY))
[1] 18404

Doing the same with your reference dates should do the job.

Yours.
Olivier.


On Fri, 22 May 2020
14:31:48 +0200 Luigi Marongiu <marongiu.luigi at gmail.com> wrote:

> In theory, it works
> ```
> > R0 = estimate.R(x1, t=d1, GT=mGT, begin=1, end=117,
>                 methods="EG",pop.size=pop, nsim=N)
> >R0
> Reproduction number estimate using  Exponential Growth  method.
> R :  0.7425278[ 0.7409297 , 0.7441229 ]
> ```
> but I am not happy because 1. I have to use numbers instead of
> variables and 2. numbers instead of dates (which are instead reported
> in the examples...).
> Even if I force to an integer, I still get an error:
> ```
> > R0 = estimate.R(x1, t=d1, GT=mGT, begin=1, end=length(x1),
> +                 methods="EG",pop.size=pop, nsim=N)
> Error in integrity.checks(epid, t, GT, begin, end, date.first.obs,
> time.step,  :
>   If both 'begin'= 1  and 'end'= 117  are provided, they must be of
> the same class (dates, character strings or integers).
> > int
> Error: object 'int' not found
> > R0 = estimate.R(x1, t=d1, GT=mGT, begin=1,
> > end=as.integer(length(x1)),
> +                 methods="EG",pop.size=pop, nsim=N)
> Error in integrity.checks(epid, t, GT, begin, end, date.first.obs,
> time.step,  :
>   If both 'begin'= 1  and 'end'= 117  are provided, they must be of
> the same class (dates, character strings or integers).
> ```
> 
> 
> 
> On Fri, May 22, 2020 at 1:51 PM Eric Berger <ericjberger at gmail.com>
> wrote:
> 
> > Hi Luigi,
> > how about begin=1L (to force it to be integer).
> > Also please keep the correspondence on the help list.
> >
> > Best,
> > Eric
> >
> > On Fri, May 22, 2020 at 2:40 PM Luigi Marongiu
> > <marongiu.luigi at gmail.com> wrote:
> > >
> > > Same error:
> > > ```
> > > > R0 = estimate.R(x1, t=d1, GT=mGT, begin=1, end=length(x1),
> > >                 methods="EG",pop.size=pop, nsim=N)
> > >  Error in integrity.checks(epid, t, GT, begin, end,
> > > date.first.obs,
> > time.step,  :
> > >   If both 'begin'= 1  and 'end'= 117  are provided, they must be
> > > of the
> > same class (dates, character strings or integers).
> > > > str(length(x1))
> > >  int 117
> > > ```
> > >
> > >
> > > On Fri, May 22, 2020 at 12:35 PM Eric Berger
> > > <ericjberger at gmail.com>
> > wrote:
> > >>
> > >> Hi Luigi,
> > >> I am not familiar with the R0 package but I took a quick look.
> > >> The example in the documentation sets begin and end to integers.
> > >> Try setting begin = 1, end = 121 and see if that works.
> > >>
> > >> HTH,
> > >> Eric
> > >>
> > >> On Fri, May 22, 2020 at 1:17 PM Luigi Marongiu <
> > marongiu.luigi at gmail.com> wrote:
> > >> >
> > >> > Hello,
> > >> > I am trying ot get the R0 from the incidence data from China
> > >> > for the COVID-19. I set the following:
> > >> > ```
> > >> > library("R0")
> > >> > x1 <- c(259,   457,   688,   769,  1771,  1459,  1737,  1981,
> > >> > 2099,
> > 2589,
> > >> >  2825,  3235,  3884,  3694,  3143,
> > >> >         3385,  2652,  2973,  2467,  2015, 14108,  5090,
> > >> > 2641,  2008, 2048,  1888,  1749,   391,   889,  823,
> > >> >         648,   214,   508,   406,   433,   327,   427,
> > >> > 573,   202,
> >  125,
> > >> >   119,   139,   143,    99,    44,
> > >> >         40,    19,    24,    15,     8,    11,    20,
> > >> > 0,    16,
> >   13,
> > >> >    13,    34,    39,    46,    39,
> > >> >         78,    47,    67,    55,    54,    45,     0,
> > >> > 79,    36,
> >   35,
> > >> >    31,    19,    30,    39,    32,
> > >> >         0,    63,    42,    46,    99,   108,    89,    46,
> > >> > 46,
> > 26,
> > >> > 325,    27,    16,    12,    11,
> > >> >         30,    10,     6,    12,    11,     3,     6,
> > >> > 22,     4,
> >   12,
> > >> >     1,     3,     3,     1,     2,
> > >> >         2,     1,     1,    14,    17,     1,     7,
> > >> > 3,     4,
> >  8,
> > >> >   6,     7)
> > >> > d1 = c("2020-01-23", "2020-01-24", "2020-01-25", "2020-01-26",
> > >> > "2020-01-27", "2020-01-28", "2020-01-29",
> > >> >        "2020-01-30", "2020-01-31", "2020-02-01", "2020-02-02",
> > >> > "2020-02-03", "2020-02-04", "2020-02-05",
> > >> >        "2020-02-06", "2020-02-07", "2020-02-08", "2020-02-09",
> > >> > "2020-02-10", "2020-02-11", "2020-02-12",
> > >> >        "2020-02-13", "2020-02-14", "2020-02-15", "2020-02-16",
> > >> > "2020-02-17", "2020-02-18", "2020-02-19",
> > >> >        "2020-02-20", "2020-02-21", "2020-02-22", "2020-02-23",
> > >> > "2020-02-24", "2020-02-25", "2020-02-26",
> > >> >        "2020-02-27", "2020-02-28", "2020-02-29", "2020-03-01",
> > >> > "2020-03-02", "2020-03-03", "2020-03-04",
> > >> >        "2020-03-05", "2020-03-06", "2020-03-07", "2020-03-08",
> > >> > "2020-03-09", "2020-03-10", "2020-03-11",
> > >> >        "2020-03-12", "2020-03-13", "2020-03-14", "2020-03-15",
> > >> > "2020-03-16", "2020-03-17", "2020-03-18",
> > >> >        "2020-03-19", "2020-03-20", "2020-03-21", "2020-03-22",
> > >> > "2020-03-23", "2020-03-24", "2020-03-25",
> > >> >        "2020-03-26", "2020-03-27", "2020-03-28", "2020-03-29",
> > >> > "2020-03-30", "2020-03-31", "2020-04-01",
> > >> >        "2020-04-02", "2020-04-03", "2020-04-04", "2020-04-05",
> > >> > "2020-04-06", "2020-04-07", "2020-04-08",
> > >> >        "2020-04-09", "2020-04-10", "2020-04-11", "2020-04-12",
> > >> > "2020-04-13", "2020-04-14", "2020-04-15",
> > >> >        "2020-04-16", "2020-04-17", "2020-04-18", "2020-04-19",
> > >> > "2020-04-20", "2020-04-21", "2020-04-22",
> > >> >        "2020-04-23", "2020-04-24", "2020-04-25", "2020-04-26",
> > "2020-04-27"
> > >> > ,"2020-04-28", "2020-04-29",
> > >> >        "2020-04-30", "2020-05-01", "2020-05-02", "2020-05-03",
> > >> > "2020-05-04", "2020-05-05", "2020-05-06",
> > >> >        "2020-05-07", "2020-05-08", "2020-05-09", "2020-05-10",
> > >> > "2020-05-11", "2020-05-12", "2020-05-13",
> > >> >        "2020-05-14", "2020-05-15", "2020-05-16", "2020-05-17",
> > "2020-05-18")
> > >> > names(x1) <- d1
> > >> > pop = 1438443864
> > >> > Ts_mean = 5.16
> > >> > Ts_sd   = 1.49
> > >> > N=10000
> > >> > TODAY = Sys.Date()
> > >> > mGT = generation.time("gamma", c(Ts_mean, Ts_sd))
> > >> > R0 = estimate.R(x1, t=d1, GT=mGT, begin=as.Date(d1[1]),
> > >> > end=TODAY, methods="EG",pop.size=pop, nsim=N)
> > >> > ```
> > >> > but when I run I get:
> > >> > ```
> > >> > Error in if (end.nb <= begin.nb) stop("'begin' and 'end' are
> > >> > not consistent.") :
> > >> >   argument is of length zero
> > >> > > as.Date(d1[1])
> > >> > [1] "2020-01-23"
> > >> > > TODAY
> > >> > [1] "2020-05-22"
> > >> > > str(TODAY)
> > >> >  Date[1:1], format: "2020-05-22"
> > >> > > str(as.Date(d1[1]))
> > >> >  Date[1:1], format: "2020-01-23"
> > >> > ```
> > >> > Since I provided both start and end in the same format, I don't
> > understand
> > >> > the error.
> > >> > Any tips?
> > >> > Thank you
> > >> > --
> > >> > Best regards,
> > >> > Luigi
> > >> >
> > >> >         [[alternative HTML version deleted]]
> > >> >
> > >> > ______________________________________________
> > >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> > >> > see https://stat.ethz.ch/mailman/listinfo/r-help
> > >> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > >> > and provide commented, minimal, self-contained, reproducible
> > >> > code.
> > >
> > >
> > >
> > > --
> > > Best regards,
> > > Luigi
> >
> 
> 
> -- 
> Best regards,
> Luigi
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.


-- 
  Olivier Crouzet, PhD
  http://olivier.ghostinthemachine.space
  /Ma?tre de Conf?rences/
  @LLING - Laboratoire de Linguistique de Nantes
    UMR6310 CNRS / Universit? de Nantes
  /Guest Researcher/
  @UMCG (University Medical Center Groningen)
    ENT department
    Rijksuniversiteit Groningen


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Fri May 22 15:49:25 2020
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Fri, 22 May 2020 15:49:25 +0200
Subject: [R] How to install R 4.0 on chromebook?
Message-ID: <CAMk+s2Q2RNf5LJNJ1Q8scWYV3kjnQWT1CxSbdhHqMUN0qDVyyg@mail.gmail.com>

Hello,
I have tried to upgrade R to 4.0. I have added `deb
http://cloud.r-project.org/bin/linux/debian buster-cran40/` to
`/etc/apt/sources.list`
(but I removed cran35). Btu when I run `apt-get update; apt-get install
r-base r-base-dev` I get an error. I think it depends on r-base-core, since
it depends on these obsolete libraries:
```
$ sudo apt-get install r-base-core
Reading package lists... Done
Building dependency tree
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:

The following packages have unmet dependencies:
 r-base-core : Depends: libc6 (>= 2.27) but 2.24-11+deb9u4 is to be
installed
               Depends: libcurl4 (>= 7.28.0) but it is not installable
               Depends: libgfortran5 (>= 8) but it is not installable
               Depends: libicu63 (>= 63.1-1~) but it is not installable
               Depends: libpcre2-8-0 (>= 10.32) but it is not going to be
installed
               Recommends: r-recommended but it is not going to be installed
               Recommends: r-base-dev but it is not going to be installed
               Recommends: r-doc-html but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
```
How can I upgrade these libraries on Chromebook?
Thank you

-- 
Best regards,
Luigi

	[[alternative HTML version deleted]]


From m@rc_@chw@rtz @end|ng |rom me@com  Fri May 22 16:00:22 2020
From: m@rc_@chw@rtz @end|ng |rom me@com (Marc Schwartz)
Date: Fri, 22 May 2020 10:00:22 -0400
Subject: [R] How to install R 4.0 on chromebook?
In-Reply-To: <CAMk+s2Q2RNf5LJNJ1Q8scWYV3kjnQWT1CxSbdhHqMUN0qDVyyg@mail.gmail.com>
References: <CAMk+s2Q2RNf5LJNJ1Q8scWYV3kjnQWT1CxSbdhHqMUN0qDVyyg@mail.gmail.com>
Message-ID: <557C168A-2AB2-458E-8D67-6270274EFCDD@me.com>

Hi,

You appear to have installed Debian on a Chromebook, presumably to overcome the limitations otherwise present in Chrome OS.

Since the questions you pose are specific to Debian, there is an R e-mail list focused on the use of R on Debian based computers:

  https://stat.ethz.ch/mailman/listinfo/r-sig-debian

You should subscribe to that list and post your queries there to avail yourself of a focused audience, which may include folks that have installed a Debian based Linux distribution on a Chromebook.

Regards,

Marc Schwartz


> On May 22, 2020, at 9:49 AM, Luigi Marongiu <marongiu.luigi at gmail.com> wrote:
> 
> Hello,
> I have tried to upgrade R to 4.0. I have added `deb
> http://cloud.r-project.org/bin/linux/debian buster-cran40/` to
> `/etc/apt/sources.list`
> (but I removed cran35). Btu when I run `apt-get update; apt-get install
> r-base r-base-dev` I get an error. I think it depends on r-base-core, since
> it depends on these obsolete libraries:
> ```
> $ sudo apt-get install r-base-core
> Reading package lists... Done
> Building dependency tree
> Reading state information... Done
> Some packages could not be installed. This may mean that you have
> requested an impossible situation or if you are using the unstable
> distribution that some required packages have not yet been created
> or been moved out of Incoming.
> The following information may help to resolve the situation:
> 
> The following packages have unmet dependencies:
> r-base-core : Depends: libc6 (>= 2.27) but 2.24-11+deb9u4 is to be
> installed
>               Depends: libcurl4 (>= 7.28.0) but it is not installable
>               Depends: libgfortran5 (>= 8) but it is not installable
>               Depends: libicu63 (>= 63.1-1~) but it is not installable
>               Depends: libpcre2-8-0 (>= 10.32) but it is not going to be
> installed
>               Recommends: r-recommended but it is not going to be installed
>               Recommends: r-base-dev but it is not going to be installed
>               Recommends: r-doc-html but it is not going to be installed
> E: Unable to correct problems, you have held broken packages.
> ```
> How can I upgrade these libraries on Chromebook?
> Thank you
> 
> -- 
> Best regards,
> Luigi


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Fri May 22 16:45:29 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Fri, 22 May 2020 09:45:29 -0500
Subject: [R] how to show percentage of individuals for two groups on
 histogram?
In-Reply-To: <CA+8X3fWHY5Hf=jT=nERWCdnHvGgD=3j7uJZErkobWOp4UZekZw@mail.gmail.com>
References: <CAF9-5jPS7TS+SEzc6J6sgatS+Lh+k5h3kP_48mFXR4xBLZ2=Cw@mail.gmail.com>
 <CAF9-5jNBY7Yxk4szXSrCci-KEF=d6=L-CXO_U7g4u3ghEm78Jw@mail.gmail.com>
 <CA+8X3fW-kCsYxnps1LCUWqfoxHaYfhqNG_MOR0_Lu7zuEizFMg@mail.gmail.com>
 <CA+8X3fWHY5Hf=jT=nERWCdnHvGgD=3j7uJZErkobWOp4UZekZw@mail.gmail.com>
Message-ID: <CAF9-5jMEQvVh-zmSS+0EDcBAZV0X2kYVzZvEdWSq0=fgroKi+g@mail.gmail.com>

HI Jim,

Thank you so much for getting back to me I tried your codes and I got
this in attach,
I think the issue is in calculating percentage per groups (cases or controls)

par(mfrow=c(2,1))
casehist<-table(cut(a$HBA1C[a$pheno=="case"],breaks=0:15))
controlhist<-table(cut(a$HBA1C[a$pheno=="control"],breaks=0:15))

par(mar=c(0,4,1,2))
barpos=barplot(100*casehist,names.arg=names(casehist),col="orange",
               space=0,ylab="Percentage",xaxt="n",ylim=c(0,25))
text(mean(barpos),23,
     "Cases: n=848, nulls=26, median=7.3, mean=7.45, sd=1.96")
box()
par(mar=c(3,4,0,2))
barplot(100*controlhist,names.arg=names(controlhist),
        space=0,ylab="Percentage",col="orange",ylim=c(0,25))
text(mean(barpos),23,
     "Controls: n=892, nulls=7, median=7.3, mean=7.45, sd=1.12")
box()

I can send you the whole dataset if you would like to try with it
On Thu, May 21, 2020 at 11:14 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi Ana,
> Just noticed a typo from a hasty cut-paste. Two lines should read:
>
> casehist<-table(cut(aafd$HBAIC[aafd$pheno=="case"],breaks=0:15))
> controlhist<-table(cut(aafd$HBAIC[aafd$pheno=="control"],breaks=0:15))
>
> Jim
>
> On Fri, May 22, 2020 at 2:08 PM Jim Lemon <drjimlemon at gmail.com> wrote:
> >
> > Hi Ana,
> > My apologies for the pedestrian graphics, but it may help.
> >
> > # a bit of fake data
> > aafd<-data.frame(FID=paste0("fam",1000:2739),
> >  IID=paste0("G",1000,2739),FLASER=rep(1,1740),
> >  PLASER=c(rep(1,892),rep(2,848)),
> >  DIABDUR=sample(10:50,1740,TRUE),
> >  HBAIC=rnorm(1740,mean=7.45,sd=2),ESRD=rep(1,1740),
> >  pheno=c(rep("control",892),rep("case",848)))
> > par(mfrow=c(2,1))
> > casepct<-table(cut(aafd$HBAIC[aafd$pheno=="case"],breaks=0:15))
> > controlpct<-table(cut(aafd$HBAIC[aafd$pheno=="control"],breaks=0:15))
> > par(mar=c(0,4,1,2))
> > barpos=barplot(100*casehist,names.arg=names(casepct),col="orange",
> >  space=0,ylab="Percentage",xaxt="n",ylim=c(0,25))
> > text(mean(barpos),23,
> >  "Cases: n=848, nulls=26, median=7.3, mean=7.45, sd=1.96")
> > box()
> > par(mar=c(3,4,0,2))
> > barplot(100*controlhist,names.arg=names(controlpct),
> >  space=0,ylab="Percentage",col="orange",ylim=c(0,25))
> > text(mean(barpos),23,
> >  "Controls: n=892, nulls=7, median=7.3, mean=7.45, sd=1.12")
> > box()
> >
> > Jim
> >
> > On Fri, May 22, 2020 at 9:08 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> > >
> > > the result would basically look something like this on in attach or
> > > the overlay of those two plots
> > >
> > >
> > > On Thu, May 21, 2020 at 5:23 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> > > >
> > > > Hello,
> > > >
> > > > I have a data frame like this:
> > > > > head(a)
> > > >          FID   IID FLASER PLASER DIABDUR HBA1C ESRD   pheno
> > > > 1 fam1000-03 G1000      1      1      38  10.2    1 control
> > > > 2 fam1001-03 G1001      1      1      15   7.3    1 control
> > > > 3 fam1003-03 G1003      1      2      17   7.0    1    case
> > > > 4 fam1005-03 G1005      1      1      36   7.7    1 control
> > > > 5 fam1009-03 G1009      1      1      23   7.6    1 control
> > > > 6 fam1052-03 G1052      1      1      32   7.3    1 control
> > > >
> > > > > dim(a)
> > > > [1] 1698    8
> > > >
> > > > I am doing histogram plot via:
> > > > ggplot(a, aes(x=HBA1C, fill=pheno)) + geom_histogram(binwidth=.5,
> > > > position="dodge")
> > > >
> > > > there is 848 who have "case" in pheno column and 892 who have
> > > > "control" in pheno column.
> > > >
> > > > I would like to have on y-axis shown percentage of individuals which
> > > > have either "case" or "control" in pheno instead of count.
> > > >
> > > > Please advise,
> > > > Ana
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Screen Shot 2020-05-22 at 9.42.01 AM.png
Type: image/png
Size: 88187 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200522/bb7b7549/attachment.png>

From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Fri May 22 16:48:46 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Fri, 22 May 2020 09:48:46 -0500
Subject: [R] how to show percentage of individuals for two groups on
 histogram?
In-Reply-To: <CAGgJW75wUtZwH0=nODwfnnNNWyWYymHW_QrFVU=8N1wfgvCkvA@mail.gmail.com>
References: <CAF9-5jPS7TS+SEzc6J6sgatS+Lh+k5h3kP_48mFXR4xBLZ2=Cw@mail.gmail.com>
 <CAF9-5jNBY7Yxk4szXSrCci-KEF=d6=L-CXO_U7g4u3ghEm78Jw@mail.gmail.com>
 <CA+8X3fW-kCsYxnps1LCUWqfoxHaYfhqNG_MOR0_Lu7zuEizFMg@mail.gmail.com>
 <CA+8X3fWHY5Hf=jT=nERWCdnHvGgD=3j7uJZErkobWOp4UZekZw@mail.gmail.com>
 <CAGgJW75wUtZwH0=nODwfnnNNWyWYymHW_QrFVU=8N1wfgvCkvA@mail.gmail.com>
Message-ID: <CAF9-5jP=tc9baHwGAYpSTk29HR-HErOdn2t9=gABjuzvHi9ykg@mail.gmail.com>

Hi Eric,

Thank you for getting back to me, I tried those solutions but they
don't do percentage per groups, so if I do
ggplot(data=subset(a, !is.na(pheno)), aes(x=HBA1C, fill=pheno)) +
geom_histogram(aes(y =

stat(density)), binwidth = 0.5) +
  scale_y_continuous(labels = scales::percent_format())

I am getting the plot in attach, while my results should be more in
this range like on the plot here:
https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/variable.cgi?study_id=phs000018.v2.p1&phv=19980&phd=154&pha=2864&pht=62&phvf=&phdf=&phaf=&phtf=&dssp=1&consent=&temp=1


On Fri, May 22, 2020 at 12:18 AM Eric Berger <ericjberger at gmail.com> wrote:
>
> Hi Ana,
> This is a very common question about ggplot.
> A quick search turns up lots of hits that answer your question. Here
> are a couple
> https://community.rstudio.com/t/trouble-scaling-y-axis-to-percentages-from-counts/42999
> https://stackoverflow.com/questions/3695497/show-instead-of-counts-in-charts-of-categorical-variables
>
> From reading those discussions, the following should work (untested)
>
> ggplot(a, aes(x = HBA1C, fill=pheno)) + geom_histogram(aes(y =
> stat(density)), binwidth = 0.5) +
>       scale_y_continuous(labels = scales::percent_format())
>
> HTH,
> Eric
>
>
> On Fri, May 22, 2020 at 7:18 AM Jim Lemon <drjimlemon at gmail.com> wrote:
> >
> > Hi Ana,
> > Just noticed a typo from a hasty cut-paste. Two lines should read:
> >
> > casehist<-table(cut(aafd$HBAIC[aafd$pheno=="case"],breaks=0:15))
> > controlhist<-table(cut(aafd$HBAIC[aafd$pheno=="control"],breaks=0:15))
> >
> > Jim
> >
> > On Fri, May 22, 2020 at 2:08 PM Jim Lemon <drjimlemon at gmail.com> wrote:
> > >
> > > Hi Ana,
> > > My apologies for the pedestrian graphics, but it may help.
> > >
> > > # a bit of fake data
> > > aafd<-data.frame(FID=paste0("fam",1000:2739),
> > >  IID=paste0("G",1000,2739),FLASER=rep(1,1740),
> > >  PLASER=c(rep(1,892),rep(2,848)),
> > >  DIABDUR=sample(10:50,1740,TRUE),
> > >  HBAIC=rnorm(1740,mean=7.45,sd=2),ESRD=rep(1,1740),
> > >  pheno=c(rep("control",892),rep("case",848)))
> > > par(mfrow=c(2,1))
> > > casepct<-table(cut(aafd$HBAIC[aafd$pheno=="case"],breaks=0:15))
> > > controlpct<-table(cut(aafd$HBAIC[aafd$pheno=="control"],breaks=0:15))
> > > par(mar=c(0,4,1,2))
> > > barpos=barplot(100*casehist,names.arg=names(casepct),col="orange",
> > >  space=0,ylab="Percentage",xaxt="n",ylim=c(0,25))
> > > text(mean(barpos),23,
> > >  "Cases: n=848, nulls=26, median=7.3, mean=7.45, sd=1.96")
> > > box()
> > > par(mar=c(3,4,0,2))
> > > barplot(100*controlhist,names.arg=names(controlpct),
> > >  space=0,ylab="Percentage",col="orange",ylim=c(0,25))
> > > text(mean(barpos),23,
> > >  "Controls: n=892, nulls=7, median=7.3, mean=7.45, sd=1.12")
> > > box()
> > >
> > > Jim
> > >
> > > On Fri, May 22, 2020 at 9:08 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> > > >
> > > > the result would basically look something like this on in attach or
> > > > the overlay of those two plots
> > > >
> > > >
> > > > On Thu, May 21, 2020 at 5:23 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> > > > >
> > > > > Hello,
> > > > >
> > > > > I have a data frame like this:
> > > > > > head(a)
> > > > >          FID   IID FLASER PLASER DIABDUR HBA1C ESRD   pheno
> > > > > 1 fam1000-03 G1000      1      1      38  10.2    1 control
> > > > > 2 fam1001-03 G1001      1      1      15   7.3    1 control
> > > > > 3 fam1003-03 G1003      1      2      17   7.0    1    case
> > > > > 4 fam1005-03 G1005      1      1      36   7.7    1 control
> > > > > 5 fam1009-03 G1009      1      1      23   7.6    1 control
> > > > > 6 fam1052-03 G1052      1      1      32   7.3    1 control
> > > > >
> > > > > > dim(a)
> > > > > [1] 1698    8
> > > > >
> > > > > I am doing histogram plot via:
> > > > > ggplot(a, aes(x=HBA1C, fill=pheno)) + geom_histogram(binwidth=.5,
> > > > > position="dodge")
> > > > >
> > > > > there is 848 who have "case" in pheno column and 892 who have
> > > > > "control" in pheno column.
> > > > >
> > > > > I would like to have on y-axis shown percentage of individuals which
> > > > > have either "case" or "control" in pheno instead of count.
> > > > >
> > > > > Please advise,
> > > > > Ana
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Screen Shot 2020-05-22 at 9.42.21 AM.png
Type: image/png
Size: 57079 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200522/ad5e1814/attachment.png>

From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri May 22 16:59:51 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 22 May 2020 07:59:51 -0700
Subject: [R] How to use R0 package?
In-Reply-To: <CAMk+s2RkLoeptNF+9cPZWbUnZmFegHPusHNC=ThEd=D3WChsNA@mail.gmail.com>
References: <CAMk+s2Trp1JJPzrKU-1=67EvmHj4ks94q8C9r_izS-bXSUgC8w@mail.gmail.com>
 <CAGgJW74JBMBtCEKAYLGwKo+GgFSZmdwOLFXydYdAO6t0qGxYAQ@mail.gmail.com>
 <CAMk+s2QULpsOSpvJF3rMfjOyvQz0RvDciXbziCTeaamrGxoHYQ@mail.gmail.com>
 <CAGgJW777jvC4GMzex_TE3UNpsXjbwVY=Zh6VD+iABUc_9cf=Kg@mail.gmail.com>
 <CAMk+s2RkLoeptNF+9cPZWbUnZmFegHPusHNC=ThEd=D3WChsNA@mail.gmail.com>
Message-ID: <CD5E8DE9-D511-49C0-83F9-6E1A8F0117F5@dcn.davis.ca.us>

This is getting off-topic here but R0 is a mathematical parameter unrelated to calendar dates. It arises when analyzing case counts (integers) as a function of the numerical measure of time since some non-trivial number of cases has occurred (conventionally this measure is in days)..

dta$days <- as.numeric( dta$date - startdate, units="days" )

On May 22, 2020 5:31:48 AM PDT, Luigi Marongiu <marongiu.luigi at gmail.com> wrote:
>In theory, it works
>```
>> R0 = estimate.R(x1, t=d1, GT=mGT, begin=1, end=117,
>                methods="EG",pop.size=pop, nsim=N)
>>R0
>Reproduction number estimate using  Exponential Growth  method.
>R :  0.7425278[ 0.7409297 , 0.7441229 ]
>```
>but I am not happy because 1. I have to use numbers instead of
>variables and 2. numbers instead of dates (which are instead reported
>in
>the examples...).
>Even if I force to an integer, I still get an error:
>```
>> R0 = estimate.R(x1, t=d1, GT=mGT, begin=1, end=length(x1),
>+                 methods="EG",pop.size=pop, nsim=N)
>Error in integrity.checks(epid, t, GT, begin, end, date.first.obs,
>time.step,  :
>  If both 'begin'= 1  and 'end'= 117  are provided, they must be of the
>same class (dates, character strings or integers).
>> int
>Error: object 'int' not found
>> R0 = estimate.R(x1, t=d1, GT=mGT, begin=1,
>end=as.integer(length(x1)),
>+                 methods="EG",pop.size=pop, nsim=N)
>Error in integrity.checks(epid, t, GT, begin, end, date.first.obs,
>time.step,  :
>  If both 'begin'= 1  and 'end'= 117  are provided, they must be of the
>same class (dates, character strings or integers).
>```
>
>
>
>On Fri, May 22, 2020 at 1:51 PM Eric Berger <ericjberger at gmail.com>
>wrote:
>
>> Hi Luigi,
>> how about begin=1L (to force it to be integer).
>> Also please keep the correspondence on the help list.
>>
>> Best,
>> Eric
>>
>> On Fri, May 22, 2020 at 2:40 PM Luigi Marongiu
><marongiu.luigi at gmail.com>
>> wrote:
>> >
>> > Same error:
>> > ```
>> > > R0 = estimate.R(x1, t=d1, GT=mGT, begin=1, end=length(x1),
>> >                 methods="EG",pop.size=pop, nsim=N)
>> >  Error in integrity.checks(epid, t, GT, begin, end, date.first.obs,
>> time.step,  :
>> >   If both 'begin'= 1  and 'end'= 117  are provided, they must be of
>the
>> same class (dates, character strings or integers).
>> > > str(length(x1))
>> >  int 117
>> > ```
>> >
>> >
>> > On Fri, May 22, 2020 at 12:35 PM Eric Berger
><ericjberger at gmail.com>
>> wrote:
>> >>
>> >> Hi Luigi,
>> >> I am not familiar with the R0 package but I took a quick look.
>> >> The example in the documentation sets begin and end to integers.
>> >> Try setting begin = 1, end = 121 and see if that works.
>> >>
>> >> HTH,
>> >> Eric
>> >>
>> >> On Fri, May 22, 2020 at 1:17 PM Luigi Marongiu <
>> marongiu.luigi at gmail.com> wrote:
>> >> >
>> >> > Hello,
>> >> > I am trying ot get the R0 from the incidence data from China for
>the
>> >> > COVID-19. I set the following:
>> >> > ```
>> >> > library("R0")
>> >> > x1 <- c(259,   457,   688,   769,  1771,  1459,  1737,  1981, 
>2099,
>> 2589,
>> >> >  2825,  3235,  3884,  3694,  3143,
>> >> >         3385,  2652,  2973,  2467,  2015, 14108,  5090,  2641, 
>2008,
>> >> >  2048,  1888,  1749,   391,   889,  823,
>> >> >         648,   214,   508,   406,   433,   327,   427,   573,  
>202,
>>  125,
>> >> >   119,   139,   143,    99,    44,
>> >> >         40,    19,    24,    15,     8,    11,    20,     0,   
>16,
>>   13,
>> >> >    13,    34,    39,    46,    39,
>> >> >         78,    47,    67,    55,    54,    45,     0,    79,   
>36,
>>   35,
>> >> >    31,    19,    30,    39,    32,
>> >> >         0,    63,    42,    46,    99,   108,    89,    46,   
>46,
>> 26,
>> >> > 325,    27,    16,    12,    11,
>> >> >         30,    10,     6,    12,    11,     3,     6,    22,    
>4,
>>   12,
>> >> >     1,     3,     3,     1,     2,
>> >> >         2,     1,     1,    14,    17,     1,     7,     3,    
>4,
>>  8,
>> >> >   6,     7)
>> >> > d1 = c("2020-01-23", "2020-01-24", "2020-01-25", "2020-01-26",
>> >> > "2020-01-27", "2020-01-28", "2020-01-29",
>> >> >        "2020-01-30", "2020-01-31", "2020-02-01", "2020-02-02",
>> >> > "2020-02-03", "2020-02-04", "2020-02-05",
>> >> >        "2020-02-06", "2020-02-07", "2020-02-08", "2020-02-09",
>> >> > "2020-02-10", "2020-02-11", "2020-02-12",
>> >> >        "2020-02-13", "2020-02-14", "2020-02-15", "2020-02-16",
>> >> > "2020-02-17", "2020-02-18", "2020-02-19",
>> >> >        "2020-02-20", "2020-02-21", "2020-02-22", "2020-02-23",
>> >> > "2020-02-24", "2020-02-25", "2020-02-26",
>> >> >        "2020-02-27", "2020-02-28", "2020-02-29", "2020-03-01",
>> >> > "2020-03-02", "2020-03-03", "2020-03-04",
>> >> >        "2020-03-05", "2020-03-06", "2020-03-07", "2020-03-08",
>> >> > "2020-03-09", "2020-03-10", "2020-03-11",
>> >> >        "2020-03-12", "2020-03-13", "2020-03-14", "2020-03-15",
>> >> > "2020-03-16", "2020-03-17", "2020-03-18",
>> >> >        "2020-03-19", "2020-03-20", "2020-03-21", "2020-03-22",
>> >> > "2020-03-23", "2020-03-24", "2020-03-25",
>> >> >        "2020-03-26", "2020-03-27", "2020-03-28", "2020-03-29",
>> >> > "2020-03-30", "2020-03-31", "2020-04-01",
>> >> >        "2020-04-02", "2020-04-03", "2020-04-04", "2020-04-05",
>> >> > "2020-04-06", "2020-04-07", "2020-04-08",
>> >> >        "2020-04-09", "2020-04-10", "2020-04-11", "2020-04-12",
>> >> > "2020-04-13", "2020-04-14", "2020-04-15",
>> >> >        "2020-04-16", "2020-04-17", "2020-04-18", "2020-04-19",
>> >> > "2020-04-20", "2020-04-21", "2020-04-22",
>> >> >        "2020-04-23", "2020-04-24", "2020-04-25", "2020-04-26",
>> "2020-04-27"
>> >> > ,"2020-04-28", "2020-04-29",
>> >> >        "2020-04-30", "2020-05-01", "2020-05-02", "2020-05-03",
>> >> > "2020-05-04", "2020-05-05", "2020-05-06",
>> >> >        "2020-05-07", "2020-05-08", "2020-05-09", "2020-05-10",
>> >> > "2020-05-11", "2020-05-12", "2020-05-13",
>> >> >        "2020-05-14", "2020-05-15", "2020-05-16", "2020-05-17",
>> "2020-05-18")
>> >> > names(x1) <- d1
>> >> > pop = 1438443864
>> >> > Ts_mean = 5.16
>> >> > Ts_sd   = 1.49
>> >> > N=10000
>> >> > TODAY = Sys.Date()
>> >> > mGT = generation.time("gamma", c(Ts_mean, Ts_sd))
>> >> > R0 = estimate.R(x1, t=d1, GT=mGT, begin=as.Date(d1[1]),
>end=TODAY,
>> >> >                 methods="EG",pop.size=pop, nsim=N)
>> >> > ```
>> >> > but when I run I get:
>> >> > ```
>> >> > Error in if (end.nb <= begin.nb) stop("'begin' and 'end' are not
>> >> > consistent.") :
>> >> >   argument is of length zero
>> >> > > as.Date(d1[1])
>> >> > [1] "2020-01-23"
>> >> > > TODAY
>> >> > [1] "2020-05-22"
>> >> > > str(TODAY)
>> >> >  Date[1:1], format: "2020-05-22"
>> >> > > str(as.Date(d1[1]))
>> >> >  Date[1:1], format: "2020-01-23"
>> >> > ```
>> >> > Since I provided both start and end in the same format, I don't
>> understand
>> >> > the error.
>> >> > Any tips?
>> >> > Thank you
>> >> > --
>> >> > Best regards,
>> >> > Luigi
>> >> >
>> >> >         [[alternative HTML version deleted]]
>> >> >
>> >> > ______________________________________________
>> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>see
>> >> > https://stat.ethz.ch/mailman/listinfo/r-help
>> >> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> >> > and provide commented, minimal, self-contained, reproducible
>code.
>> >
>> >
>> >
>> > --
>> > Best regards,
>> > Luigi
>>

-- 
Sent from my phone. Please excuse my brevity.


From rmh @end|ng |rom temp|e@edu  Fri May 22 18:29:09 2020
From: rmh @end|ng |rom temp|e@edu (Richard M. Heiberger)
Date: Fri, 22 May 2020 12:29:09 -0400
Subject: [R] access for free more than 500 essential Springer Nature
 textbooks
Message-ID: <CAGx1TMBxYGKQsUY354c8iUdEzYdX6+g2NaBNzzHF4BEGpbLnCA@mail.gmail.com>

Springer has just made available free access to many books through July.
This is part of their global program to support educators, students
and academics
affected by coronavirus lockdown.

Their list includes about 20 statistics books in English and 2 in
German.  Several, including mine, have R in the title or subtitle.

This link describes the program:
https://www.springernature.com/gp/librarians/news-events/all-news-articles/industry-news-initiatives/free-access-to-textbooks-for-institutions-affected-by-coronaviru/17855960?sap-outbound-id=07923935E132AFCC90201BAEA7D6755EC6C597DE&utm_source=hybris-campaign&utm_medium=email&utm_campaign=000_BARZ01_0000001531_AEXS_AWA_CB02_GL_txt_covid&utm_content=EN_internal_5917_20200522&mkt-key=42010A0550671EDA9BA73AC34F576EF6

My book is
Statistical Analysis and Data Display, Richard M. Heiberger, Burt
Holland, 2nd ed. 2015
It is supported by the HH package available from CRAN.

Rich


From @purd|e@@ @end|ng |rom gm@||@com  Fri May 22 20:23:46 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Sat, 23 May 2020 06:23:46 +1200
Subject: [R] access for free more than 500 essential Springer Nature
 textbooks
In-Reply-To: <CAGx1TMBxYGKQsUY354c8iUdEzYdX6+g2NaBNzzHF4BEGpbLnCA@mail.gmail.com>
References: <CAGx1TMBxYGKQsUY354c8iUdEzYdX6+g2NaBNzzHF4BEGpbLnCA@mail.gmail.com>
Message-ID: <CAB8pepzHks4aMWWHqzymKks7wmr1biU6Uk18DorDrVaOz=r35A@mail.gmail.com>

That sounds like progress.

However, I was unable to use their website.
All I can find is Excel documents (which I can't open) and MARC (?),
which I don't have time to look into.

Your post might have more value, if you provide a list of the titles
(or a link to a list, in an easy to read open access format), ideally
with a note on where to find open access copies of those texts,
without spending a long time searching.

On Sat, May 23, 2020 at 4:29 AM Richard M. Heiberger <rmh at temple.edu> wrote:
>
> Springer has just made available free access to many books through July.
> This is part of their global program to support educators, students
> and academics
> affected by coronavirus lockdown.
>
> Their list includes about 20 statistics books in English and 2 in
> German.  Several, including mine, have R in the title or subtitle.
>
> This link describes the program:
> https://www.springernature.com/gp/librarians/news-events/all-news-articles/industry-news-initiatives/free-access-to-textbooks-for-institutions-affected-by-coronaviru/17855960?sap-outbound-id=07923935E132AFCC90201BAEA7D6755EC6C597DE&utm_source=hybris-campaign&utm_medium=email&utm_campaign=000_BARZ01_0000001531_AEXS_AWA_CB02_GL_txt_covid&utm_content=EN_internal_5917_20200522&mkt-key=42010A0550671EDA9BA73AC34F576EF6
>
> My book is
> Statistical Analysis and Data Display, Richard M. Heiberger, Burt
> Holland, 2nd ed. 2015
> It is supported by the HH package available from CRAN.
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Fri May 22 20:52:27 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Fri, 22 May 2020 13:52:27 -0500
Subject: [R] How to only show two numbers on bar_plot with ggplot
Message-ID: <CAF9-5jOhpnhfbantZwOX2Xh4AL-NEY80LfChdT7=pdKVCELPdw@mail.gmail.com>

Hello,

I made the plot in attach via:

ed<-ggplot(e) +
  geom_bar(aes(x = ESRD, fill =
factor(pheno,labels=c("control","case"))))+scale_fill_manual(values=c("#56B4E9","#E7B800"))+labs(fill="pheno")

ed

How do I show only 1 and 2 on x axis?

Thanks
Ana

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Screen Shot 2020-05-22 at 1.50.21 PM.png
Type: image/png
Size: 49767 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200522/351f1b95/attachment.png>

From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Fri May 22 21:22:04 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Fri, 22 May 2020 14:22:04 -0500
Subject: [R] How to only show two numbers on bar_plot with ggplot
In-Reply-To: <CAF9-5jOhpnhfbantZwOX2Xh4AL-NEY80LfChdT7=pdKVCELPdw@mail.gmail.com>
References: <CAF9-5jOhpnhfbantZwOX2Xh4AL-NEY80LfChdT7=pdKVCELPdw@mail.gmail.com>
Message-ID: <CAF9-5jPcK6aqnsj=au_XsdgPGiUt8iYMmkUv00-BT+UFx=qhsQ@mail.gmail.com>

I resolved it not elegantly with:

d=as.numeric(as.character(e$pheno))
ed<-ggplot(e) +
  geom_bar(aes(x = ESRD, fill =
factor(pheno,labels=c("control","case"))))+scale_fill_manual(values=c("#56B4E9","#E7B800"))+labs(fill="pheno")+scale_x_continuous(breaks
= unique(d))

ed

where:

> head(e)
  ESRD pheno
1    1     1
2    1     1
3    1     2
4    1     1
5    1     1

> sapply(e,class)
     ESRD     pheno
"integer"  "factor"

On Fri, May 22, 2020 at 1:52 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> Hello,
>
> I made the plot in attach via:
>
> ed<-ggplot(e) +
>   geom_bar(aes(x = ESRD, fill =
> factor(pheno,labels=c("control","case"))))+scale_fill_manual(values=c("#56B4E9","#E7B800"))+labs(fill="pheno")
>
> ed
>
> How do I show only 1 and 2 on x axis?
>
> Thanks
> Ana


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri May 22 22:00:53 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 22 May 2020 21:00:53 +0100
Subject: [R] How to only show two numbers on bar_plot with ggplot
In-Reply-To: <CAF9-5jPcK6aqnsj=au_XsdgPGiUt8iYMmkUv00-BT+UFx=qhsQ@mail.gmail.com>
References: <CAF9-5jOhpnhfbantZwOX2Xh4AL-NEY80LfChdT7=pdKVCELPdw@mail.gmail.com>
 <CAF9-5jPcK6aqnsj=au_XsdgPGiUt8iYMmkUv00-BT+UFx=qhsQ@mail.gmail.com>
Message-ID: <c6e73d89-e5e1-9511-e7f5-44e9c180b148@sapo.pt>

Hello,

Why not remove aes(x = ESRD,...) from the call to geom_bar and instead do

ed <- ggplot(e, aes(x = factor(ESRD)) + etc?


If you are using an aesthetic throughout the plot, put it in the call to 
ggplot().

Hope this helps,

Rui Barradas

?s 20:22 de 22/05/20, Ana Marija escreveu:
> I resolved it not elegantly with:
> 
> d=as.numeric(as.character(e$pheno))
> ed<-ggplot(e) +
>    geom_bar(aes(x = ESRD, fill =
> factor(pheno,labels=c("control","case"))))+scale_fill_manual(values=c("#56B4E9","#E7B800"))+labs(fill="pheno")+scale_x_continuous(breaks
> = unique(d))
> 
> ed
> 
> where:
> 
>> head(e)
>    ESRD pheno
> 1    1     1
> 2    1     1
> 3    1     2
> 4    1     1
> 5    1     1
> 
>> sapply(e,class)
>       ESRD     pheno
> "integer"  "factor"
> 
> On Fri, May 22, 2020 at 1:52 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>>
>> Hello,
>>
>> I made the plot in attach via:
>>
>> ed<-ggplot(e) +
>>    geom_bar(aes(x = ESRD, fill =
>> factor(pheno,labels=c("control","case"))))+scale_fill_manual(values=c("#56B4E9","#E7B800"))+labs(fill="pheno")
>>
>> ed
>>
>> How do I show only 1 and 2 on x axis?
>>
>> Thanks
>> Ana
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rmh @end|ng |rom temp|e@edu  Fri May 22 22:11:32 2020
From: rmh @end|ng |rom temp|e@edu (Richard M. Heiberger)
Date: Fri, 22 May 2020 16:11:32 -0400
Subject: [R] 
 [External] Re: access for free more than 500 essential Springer
 Nature textbooks
In-Reply-To: <CAB8pepzHks4aMWWHqzymKks7wmr1biU6Uk18DorDrVaOz=r35A@mail.gmail.com>
References: <CAGx1TMBxYGKQsUY354c8iUdEzYdX6+g2NaBNzzHF4BEGpbLnCA@mail.gmail.com>
 <CAB8pepzHks4aMWWHqzymKks7wmr1biU6Uk18DorDrVaOz=r35A@mail.gmail.com>
Message-ID: <CAGx1TMDTRP0pSC+O7906u6kdYpwk+KSS2=5tGyCXFoHp=eQi2g@mail.gmail.com>

The Excel file is what you need.
As Fabio remarked, the downloadable links are in column R

I normally read (and write) Excel files into R using library(openxlsx)
There are also several other packages on CRAN for reading and writing Excel.

MARC stands for
MAchine-Readable Cataloging.
Information is at the Library of Congress
 https://www.loc.gov/marc/faq.html

Rich

On Fri, May 22, 2020 at 2:24 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
>
> That sounds like progress.
>
> However, I was unable to use their website.
> All I can find is Excel documents (which I can't open) and MARC (?),
> which I don't have time to look into.
>
> Your post might have more value, if you provide a list of the titles
> (or a link to a list, in an easy to read open access format), ideally
> with a note on where to find open access copies of those texts,
> without spending a long time searching.
>
> On Sat, May 23, 2020 at 4:29 AM Richard M. Heiberger <rmh at temple.edu> wrote:
> >
> > Springer has just made available free access to many books through July.
> > This is part of their global program to support educators, students
> > and academics
> > affected by coronavirus lockdown.
> >
> > Their list includes about 20 statistics books in English and 2 in
> > German.  Several, including mine, have R in the title or subtitle.
> >
> > This link describes the program:
> > https://www.springernature.com/gp/librarians/news-events/all-news-articles/industry-news-initiatives/free-access-to-textbooks-for-institutions-affected-by-coronaviru/17855960?sap-outbound-id=07923935E132AFCC90201BAEA7D6755EC6C597DE&utm_source=hybris-campaign&utm_medium=email&utm_campaign=000_BARZ01_0000001531_AEXS_AWA_CB02_GL_txt_covid&utm_content=EN_internal_5917_20200522&mkt-key=42010A0550671EDA9BA73AC34F576EF6
> >
> > My book is
> > Statistical Analysis and Data Display, Richard M. Heiberger, Burt
> > Holland, 2nd ed. 2015
> > It is supported by the HH package available from CRAN.
> >
> > Rich
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From |r@|nj @end|ng |rom gm@||@com  Sat May 23 00:02:31 2020
From: |r@|nj @end|ng |rom gm@||@com (John C Frain)
Date: Fri, 22 May 2020 23:02:31 +0100
Subject: [R] 
 [External] Re: access for free more than 500 essential Springer
 Nature textbooks
In-Reply-To: <CAGx1TMDTRP0pSC+O7906u6kdYpwk+KSS2=5tGyCXFoHp=eQi2g@mail.gmail.com>
References: <CAGx1TMBxYGKQsUY354c8iUdEzYdX6+g2NaBNzzHF4BEGpbLnCA@mail.gmail.com>
 <CAB8pepzHks4aMWWHqzymKks7wmr1biU6Uk18DorDrVaOz=r35A@mail.gmail.com>
 <CAGx1TMDTRP0pSC+O7906u6kdYpwk+KSS2=5tGyCXFoHp=eQi2g@mail.gmail.com>
Message-ID: <CAHrK517EVkX-au1xsSiq_duu=pJ1sxM+nGd58zL9Vn=ecpx9DQ@mail.gmail.com>

On Fri, 22 May 2020 at 21:15, Richard M. Heiberger <rmh at temple.edu> wrote:

> The Excel file is what you need.
> As Fabio remarked, the downloadable links are in column R
>
> I normally read (and write) Excel files into R using library(openxlsx)
> There are also several other packages on CRAN for reading and writing
> Excel.
>
> I opened the .xlxs file in LibreOffice Calc.  I pasted the links in
column S into my browser and was offered a download in pdf and /or epub.
This worked very well.  Apologies for the double posting


John C Frain
3 Aranleigh Park
Rathfarnham
Dublin 14
Ireland
www.tcd.ie/Economics/staff/frainj/home.html
mailto:frainj at tcd.ie
mailto:frainj at gmail.com

>
>

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Sat May 23 02:03:55 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sat, 23 May 2020 10:03:55 +1000
Subject: [R] how to show percentage of individuals for two groups on
 histogram?
In-Reply-To: <CAF9-5jPLTUgjmev=YaVHP8dTj2BUPikbODuovKJz8pC3NekUhw@mail.gmail.com>
References: <CAF9-5jPS7TS+SEzc6J6sgatS+Lh+k5h3kP_48mFXR4xBLZ2=Cw@mail.gmail.com>
 <CAF9-5jNBY7Yxk4szXSrCci-KEF=d6=L-CXO_U7g4u3ghEm78Jw@mail.gmail.com>
 <CA+8X3fW-kCsYxnps1LCUWqfoxHaYfhqNG_MOR0_Lu7zuEizFMg@mail.gmail.com>
 <CA+8X3fWHY5Hf=jT=nERWCdnHvGgD=3j7uJZErkobWOp4UZekZw@mail.gmail.com>
 <CAF9-5jMEQvVh-zmSS+0EDcBAZV0X2kYVzZvEdWSq0=fgroKi+g@mail.gmail.com>
 <CA+8X3fUf6MhnKEUguBvEx9bDfhNp_fxWuSb5MesFNMXKK8-VPw@mail.gmail.com>
 <CAF9-5jPLTUgjmev=YaVHP8dTj2BUPikbODuovKJz8pC3NekUhw@mail.gmail.com>
Message-ID: <CA+8X3fU8nLGHKTc_eMC9gdJm6bsOABF=ZKZeYvUSqxpDa-jQNQ@mail.gmail.com>

Hi Ana,
I think this is what you want in the panel style of plot. Let me know
if not, or if I have calculated the wrong percentages. The overlaid
histograms definitely use a different calculation.

amsdf<-read.table("pheno_m1_plot",header=TRUE,stringsAsFactors=FALSE)
dim(amsdf)
# find the right breaks for your "cut"
casen<-table(cut(amsdf$HBA1C[amsdf$pheno==2],breaks=3:14))
controln<-table(cut(amsdf$HBA1C[amsdf$pheno==1],breaks=3:14))
# save yourself some typing
HBA1C2<-amsdf$HBA1C[amsdf$pheno==2]
HBA1C1<-amsdf$HBA1C[amsdf$pheno==1]
ncases<-length(HBA1C2)
ncontrols<-length(HBA1C1)
split.screen(matrix(c(0,1,0.6,1,0,1,0,0.6),nrow=2,byrow=TRUE))
par(mar=c(0,4,1,2))
barpos=barplot(100*casen/ncases,names.arg=NA,col="orange",
 space=0,ylab="Percentage",xaxt="n",ylim=c(0,27))
case_text<-sprintf(
 "Cases: n=%d, nulls=%d, median=%.1f, mean=%.1f, sd=%.1f",
 length(HBA1C2),sum(is.na(HBA1C2)),round(median(HBA1C2,na.rm=TRUE),1),
 round(mean(HBA1C2,na.rm=TRUE),1),round(sd(HBA1C2,na.rm=TRUE),1))
text(mean(barpos),25,case_text)
box()
screen(2)
par(mar=c(4,4,0,2))
barplot(100*controln/ncontrols,names.arg=NA,
 space=0,ylab="Percentage",col="orange",ylim=c(0,34))
control_text<-sprintf(
 "Cases: n=%d, nulls=%d, median=%.1f, mean=%.1f, sd=%.1f",
 length(HBA1C1),sum(is.na(HBA1C1)),round(median(HBA1C1,na.rm=TRUE),1),
 round(mean(HBA1C1,na.rm=TRUE),1),round(sd(HBA1C1,na.rm=TRUE),1))
text(mean(barpos),32,control_text)
box()
library(plotrix)
staxlab(1,at=barpos,labels=names(casen))

Jim

On Sat, May 23, 2020 at 9:01 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> Hi Jim,
>
> My data is attached. It is most kind of you for looking into this!
>
> Cheers,
> Ana
>
> On Fri, May 22, 2020 at 5:49 PM Jim Lemon <drjimlemon at gmail.com> wrote:
> >
> > Hi Ana,
> > As I had very little idea what your data looked like, what I made up
> > obviously didn't fit in the plot that well. If you can send the data I
> > can make a better attempt. The other thing is whether you want a plot
> > with two adjacent panels (what I sent) or overlaid histograms (what
> > Eric sent). Let me know.
> >
> > Jim
> >
> > On Sat, May 23, 2020 at 12:45 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> > >
> > > HI Jim,
> > >
> > > Thank you so much for getting back to me I tried your codes and I got
> > > this in attach,
> > > I think the issue is in calculating percentage per groups (cases or controls)
> > > ...
> > > I can send you the whole dataset if you would like to try with it
> > > On Thu, May 21, 2020 at 11:14 PM Jim Lemon <drjimlemon at gmail.com> wrote:

-------------- next part --------------
A non-text attachment was scrubbed...
Name: ams1.png
Type: image/png
Size: 18251 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200523/ee75ea08/attachment.png>

From @purd|e@@ @end|ng |rom gm@||@com  Sat May 23 02:04:29 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Sat, 23 May 2020 12:04:29 +1200
Subject: [R] 
 [External] Re: access for free more than 500 essential Springer
 Nature textbooks
In-Reply-To: <CAGx1TMDTRP0pSC+O7906u6kdYpwk+KSS2=5tGyCXFoHp=eQi2g@mail.gmail.com>
References: <CAGx1TMBxYGKQsUY354c8iUdEzYdX6+g2NaBNzzHF4BEGpbLnCA@mail.gmail.com>
 <CAB8pepzHks4aMWWHqzymKks7wmr1biU6Uk18DorDrVaOz=r35A@mail.gmail.com>
 <CAGx1TMDTRP0pSC+O7906u6kdYpwk+KSS2=5tGyCXFoHp=eQi2g@mail.gmail.com>
Message-ID: <CAB8pepxYn80zZGiZbVRS6iPDaj68ei9jj=OVocOJO3QkkmVQmw@mail.gmail.com>

> The Excel file is what you need.

Well, now I'm in a bad mood.

I went to all the trouble of opening the thing...
And the first two Springer-published books I look for, aren't there.

(1) Programming with Data, John Chambers
(2) Applied Econometrics with R, Z and co.

Next time someone tells me to use an Excel document, I'm adding them
to the spam list.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sat May 23 02:26:34 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 22 May 2020 17:26:34 -0700
Subject: [R] 
 [External] Re: access for free more than 500 essential Springer
 Nature textbooks
In-Reply-To: <CAB8pepxYn80zZGiZbVRS6iPDaj68ei9jj=OVocOJO3QkkmVQmw@mail.gmail.com>
References: <CAGx1TMBxYGKQsUY354c8iUdEzYdX6+g2NaBNzzHF4BEGpbLnCA@mail.gmail.com>
 <CAB8pepzHks4aMWWHqzymKks7wmr1biU6Uk18DorDrVaOz=r35A@mail.gmail.com>
 <CAGx1TMDTRP0pSC+O7906u6kdYpwk+KSS2=5tGyCXFoHp=eQi2g@mail.gmail.com>
 <CAB8pepxYn80zZGiZbVRS6iPDaj68ei9jj=OVocOJO3QkkmVQmw@mail.gmail.com>
Message-ID: <7C1BF9C3-1AB3-4A77-AA8F-86DB30DF16E1@dcn.davis.ca.us>

You are bound to be disappointed if you invert the purpose of the list. This is marketing... think of it as a sale... stores rarely put their entire stock on sale... particularly if the sale price is zero. You have to start with the list and look for interesting titles.

But don't let me dissuade you from adding to your killfile if that seems more useful to you.

On May 22, 2020 5:04:29 PM PDT, Abby Spurdle <spurdle.a at gmail.com> wrote:
>> The Excel file is what you need.
>
>Well, now I'm in a bad mood.
>
>I went to all the trouble of opening the thing...
>And the first two Springer-published books I look for, aren't there.
>
>(1) Programming with Data, John Chambers
>(2) Applied Econometrics with R, Z and co.
>
>Next time someone tells me to use an Excel document, I'm adding them
>to the spam list.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From r@oknz @end|ng |rom gm@||@com  Sat May 23 03:17:50 2020
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Sat, 23 May 2020 13:17:50 +1200
Subject: [R] 
 [External] Re: access for free more than 500 essential Springer
 Nature textbooks
In-Reply-To: <7C1BF9C3-1AB3-4A77-AA8F-86DB30DF16E1@dcn.davis.ca.us>
References: <CAGx1TMBxYGKQsUY354c8iUdEzYdX6+g2NaBNzzHF4BEGpbLnCA@mail.gmail.com>
 <CAB8pepzHks4aMWWHqzymKks7wmr1biU6Uk18DorDrVaOz=r35A@mail.gmail.com>
 <CAGx1TMDTRP0pSC+O7906u6kdYpwk+KSS2=5tGyCXFoHp=eQi2g@mail.gmail.com>
 <CAB8pepxYn80zZGiZbVRS6iPDaj68ei9jj=OVocOJO3QkkmVQmw@mail.gmail.com>
 <7C1BF9C3-1AB3-4A77-AA8F-86DB30DF16E1@dcn.davis.ca.us>
Message-ID: <CABcYAdJzC+PZRnijsRQ97FK91mDN9YHyZwy5P+s4DRtD1n1N3g@mail.gmail.com>

the real pleasure comes from things you weren't looking for but recognise
as just what you needed.

On Sat, 23 May 2020 at 12:34, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> You are bound to be disappointed if you invert the purpose of the list.
> This is marketing... think of it as a sale... stores rarely put their
> entire stock on sale... particularly if the sale price is zero. You have to
> start with the list and look for interesting titles.
>
> But don't let me dissuade you from adding to your killfile if that seems
> more useful to you.
>
> On May 22, 2020 5:04:29 PM PDT, Abby Spurdle <spurdle.a at gmail.com> wrote:
> >> The Excel file is what you need.
> >
> >Well, now I'm in a bad mood.
> >
> >I went to all the trouble of opening the thing...
> >And the first two Springer-published books I look for, aren't there.
> >
> >(1) Programming with Data, John Chambers
> >(2) Applied Econometrics with R, Z and co.
> >
> >Next time someone tells me to use an Excel document, I'm adding them
> >to the spam list.
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@rk|eed@2 @end|ng |rom gm@||@com  Sat May 23 03:45:18 2020
From: m@rk|eed@2 @end|ng |rom gm@||@com (Mark Leeds)
Date: Fri, 22 May 2020 21:45:18 -0400
Subject: [R] 
 [External] Re: access for free more than 500 essential Springer
 Nature textbooks
In-Reply-To: <CABcYAdJzC+PZRnijsRQ97FK91mDN9YHyZwy5P+s4DRtD1n1N3g@mail.gmail.com>
References: <CAGx1TMBxYGKQsUY354c8iUdEzYdX6+g2NaBNzzHF4BEGpbLnCA@mail.gmail.com>
 <CAB8pepzHks4aMWWHqzymKks7wmr1biU6Uk18DorDrVaOz=r35A@mail.gmail.com>
 <CAGx1TMDTRP0pSC+O7906u6kdYpwk+KSS2=5tGyCXFoHp=eQi2g@mail.gmail.com>
 <CAB8pepxYn80zZGiZbVRS6iPDaj68ei9jj=OVocOJO3QkkmVQmw@mail.gmail.com>
 <7C1BF9C3-1AB3-4A77-AA8F-86DB30DF16E1@dcn.davis.ca.us>
 <CABcYAdJzC+PZRnijsRQ97FK91mDN9YHyZwy5P+s4DRtD1n1N3g@mail.gmail.com>
Message-ID: <CAHz+bWbwRpPksaLgyEe-bkFKbRwsUiS7rBuOAdUkvwyMWNgGFQ@mail.gmail.com>

Abby: here's an easier link for seeing what you might like.

https://link.springer.com/search?facet-content-type=%22Book%22&package=mat-covid19_textbooks&%23038;facet-language=%22En%22&%23038;sortOrder=newestFirst&%23038;showAll=true

On Fri, May 22, 2020 at 9:18 PM Richard O'Keefe <raoknz at gmail.com> wrote:

> the real pleasure comes from things you weren't looking for but recognise
> as just what you needed.
>
> On Sat, 23 May 2020 at 12:34, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
>
> > You are bound to be disappointed if you invert the purpose of the list.
> > This is marketing... think of it as a sale... stores rarely put their
> > entire stock on sale... particularly if the sale price is zero. You have
> to
> > start with the list and look for interesting titles.
> >
> > But don't let me dissuade you from adding to your killfile if that seems
> > more useful to you.
> >
> > On May 22, 2020 5:04:29 PM PDT, Abby Spurdle <spurdle.a at gmail.com>
> wrote:
> > >> The Excel file is what you need.
> > >
> > >Well, now I'm in a bad mood.
> > >
> > >I went to all the trouble of opening the thing...
> > >And the first two Springer-published books I look for, aren't there.
> > >
> > >(1) Programming with Data, John Chambers
> > >(2) Applied Econometrics with R, Z and co.
> > >
> > >Next time someone tells me to use an Excel document, I'm adding them
> > >to the spam list.
> > >
> > >______________________________________________
> > >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >https://stat.ethz.ch/mailman/listinfo/r-help
> > >PLEASE do read the posting guide
> > >http://www.R-project.org/posting-guide.html
> > >and provide commented, minimal, self-contained, reproducible code.
> >
> > --
> > Sent from my phone. Please excuse my brevity.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Sat May 23 04:38:17 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sat, 23 May 2020 12:38:17 +1000
Subject: [R] How to use R0 package?
In-Reply-To: <CD5E8DE9-D511-49C0-83F9-6E1A8F0117F5@dcn.davis.ca.us>
References: <CAMk+s2Trp1JJPzrKU-1=67EvmHj4ks94q8C9r_izS-bXSUgC8w@mail.gmail.com>
 <CAGgJW74JBMBtCEKAYLGwKo+GgFSZmdwOLFXydYdAO6t0qGxYAQ@mail.gmail.com>
 <CAMk+s2QULpsOSpvJF3rMfjOyvQz0RvDciXbziCTeaamrGxoHYQ@mail.gmail.com>
 <CAGgJW777jvC4GMzex_TE3UNpsXjbwVY=Zh6VD+iABUc_9cf=Kg@mail.gmail.com>
 <CAMk+s2RkLoeptNF+9cPZWbUnZmFegHPusHNC=ThEd=D3WChsNA@mail.gmail.com>
 <CD5E8DE9-D511-49C0-83F9-6E1A8F0117F5@dcn.davis.ca.us>
Message-ID: <CA+8X3fXrUksd65PXOTL+AsmzG0M0WhWyLcQuwgN-vR0e2GEUvg@mail.gmail.com>

So what if you treat a nuisance as a feature and import your dates as
factors? as.numeric(dates) would have the correct structure or am I,
as usual, missing something?

Jim

On Sat, May 23, 2020 at 1:00 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>
> This is getting off-topic here but R0 is a mathematical parameter unrelated to calendar dates. It arises when analyzing case counts (integers) as a function of the numerical measure of time since some non-trivial number of cases has occurred (conventionally this measure is in days)..
>
> dta$days <- as.numeric( dta$date - startdate, units="days" )
>
> On May 22, 2020 5:31:48 AM PDT, Luigi Marongiu <marongiu.luigi at gmail.com> wrote:
> >In theory, it works
> >```
> >> R0 = estimate.R(x1, t=d1, GT=mGT, begin=1, end=117,
> >                methods="EG",pop.size=pop, nsim=N)
> >>R0
> >Reproduction number estimate using  Exponential Growth  method.
> >R :  0.7425278[ 0.7409297 , 0.7441229 ]
> >```
> >but I am not happy because 1. I have to use numbers instead of
> >variables and 2. numbers instead of dates (which are instead reported
> >in
> >the examples...).
> >Even if I force to an integer, I still get an error:
> >```
> >> R0 = estimate.R(x1, t=d1, GT=mGT, begin=1, end=length(x1),
> >+                 methods="EG",pop.size=pop, nsim=N)
> >Error in integrity.checks(epid, t, GT, begin, end, date.first.obs,
> >time.step,  :
> >  If both 'begin'= 1  and 'end'= 117  are provided, they must be of the
> >same class (dates, character strings or integers).
> >> int
> >Error: object 'int' not found
> >> R0 = estimate.R(x1, t=d1, GT=mGT, begin=1,
> >end=as.integer(length(x1)),
> >+                 methods="EG",pop.size=pop, nsim=N)
> >Error in integrity.checks(epid, t, GT, begin, end, date.first.obs,
> >time.step,  :
> >  If both 'begin'= 1  and 'end'= 117  are provided, they must be of the
> >same class (dates, character strings or integers).
> >```
> >
> >
> >
> >On Fri, May 22, 2020 at 1:51 PM Eric Berger <ericjberger at gmail.com>
> >wrote:
> >
> >> Hi Luigi,
> >> how about begin=1L (to force it to be integer).
> >> Also please keep the correspondence on the help list.
> >>
> >> Best,
> >> Eric
> >>
> >> On Fri, May 22, 2020 at 2:40 PM Luigi Marongiu
> ><marongiu.luigi at gmail.com>
> >> wrote:
> >> >
> >> > Same error:
> >> > ```
> >> > > R0 = estimate.R(x1, t=d1, GT=mGT, begin=1, end=length(x1),
> >> >                 methods="EG",pop.size=pop, nsim=N)
> >> >  Error in integrity.checks(epid, t, GT, begin, end, date.first.obs,
> >> time.step,  :
> >> >   If both 'begin'= 1  and 'end'= 117  are provided, they must be of
> >the
> >> same class (dates, character strings or integers).
> >> > > str(length(x1))
> >> >  int 117
> >> > ```
> >> >
> >> >
> >> > On Fri, May 22, 2020 at 12:35 PM Eric Berger
> ><ericjberger at gmail.com>
> >> wrote:
> >> >>
> >> >> Hi Luigi,
> >> >> I am not familiar with the R0 package but I took a quick look.
> >> >> The example in the documentation sets begin and end to integers.
> >> >> Try setting begin = 1, end = 121 and see if that works.
> >> >>
> >> >> HTH,
> >> >> Eric
> >> >>
> >> >> On Fri, May 22, 2020 at 1:17 PM Luigi Marongiu <
> >> marongiu.luigi at gmail.com> wrote:
> >> >> >
> >> >> > Hello,
> >> >> > I am trying ot get the R0 from the incidence data from China for
> >the
> >> >> > COVID-19. I set the following:
> >> >> > ```
> >> >> > library("R0")
> >> >> > x1 <- c(259,   457,   688,   769,  1771,  1459,  1737,  1981,
> >2099,
> >> 2589,
> >> >> >  2825,  3235,  3884,  3694,  3143,
> >> >> >         3385,  2652,  2973,  2467,  2015, 14108,  5090,  2641,
> >2008,
> >> >> >  2048,  1888,  1749,   391,   889,  823,
> >> >> >         648,   214,   508,   406,   433,   327,   427,   573,
> >202,
> >>  125,
> >> >> >   119,   139,   143,    99,    44,
> >> >> >         40,    19,    24,    15,     8,    11,    20,     0,
> >16,
> >>   13,
> >> >> >    13,    34,    39,    46,    39,
> >> >> >         78,    47,    67,    55,    54,    45,     0,    79,
> >36,
> >>   35,
> >> >> >    31,    19,    30,    39,    32,
> >> >> >         0,    63,    42,    46,    99,   108,    89,    46,
> >46,
> >> 26,
> >> >> > 325,    27,    16,    12,    11,
> >> >> >         30,    10,     6,    12,    11,     3,     6,    22,
> >4,
> >>   12,
> >> >> >     1,     3,     3,     1,     2,
> >> >> >         2,     1,     1,    14,    17,     1,     7,     3,
> >4,
> >>  8,
> >> >> >   6,     7)
> >> >> > d1 = c("2020-01-23", "2020-01-24", "2020-01-25", "2020-01-26",
> >> >> > "2020-01-27", "2020-01-28", "2020-01-29",
> >> >> >        "2020-01-30", "2020-01-31", "2020-02-01", "2020-02-02",
> >> >> > "2020-02-03", "2020-02-04", "2020-02-05",
> >> >> >        "2020-02-06", "2020-02-07", "2020-02-08", "2020-02-09",
> >> >> > "2020-02-10", "2020-02-11", "2020-02-12",
> >> >> >        "2020-02-13", "2020-02-14", "2020-02-15", "2020-02-16",
> >> >> > "2020-02-17", "2020-02-18", "2020-02-19",
> >> >> >        "2020-02-20", "2020-02-21", "2020-02-22", "2020-02-23",
> >> >> > "2020-02-24", "2020-02-25", "2020-02-26",
> >> >> >        "2020-02-27", "2020-02-28", "2020-02-29", "2020-03-01",
> >> >> > "2020-03-02", "2020-03-03", "2020-03-04",
> >> >> >        "2020-03-05", "2020-03-06", "2020-03-07", "2020-03-08",
> >> >> > "2020-03-09", "2020-03-10", "2020-03-11",
> >> >> >        "2020-03-12", "2020-03-13", "2020-03-14", "2020-03-15",
> >> >> > "2020-03-16", "2020-03-17", "2020-03-18",
> >> >> >        "2020-03-19", "2020-03-20", "2020-03-21", "2020-03-22",
> >> >> > "2020-03-23", "2020-03-24", "2020-03-25",
> >> >> >        "2020-03-26", "2020-03-27", "2020-03-28", "2020-03-29",
> >> >> > "2020-03-30", "2020-03-31", "2020-04-01",
> >> >> >        "2020-04-02", "2020-04-03", "2020-04-04", "2020-04-05",
> >> >> > "2020-04-06", "2020-04-07", "2020-04-08",
> >> >> >        "2020-04-09", "2020-04-10", "2020-04-11", "2020-04-12",
> >> >> > "2020-04-13", "2020-04-14", "2020-04-15",
> >> >> >        "2020-04-16", "2020-04-17", "2020-04-18", "2020-04-19",
> >> >> > "2020-04-20", "2020-04-21", "2020-04-22",
> >> >> >        "2020-04-23", "2020-04-24", "2020-04-25", "2020-04-26",
> >> "2020-04-27"
> >> >> > ,"2020-04-28", "2020-04-29",
> >> >> >        "2020-04-30", "2020-05-01", "2020-05-02", "2020-05-03",
> >> >> > "2020-05-04", "2020-05-05", "2020-05-06",
> >> >> >        "2020-05-07", "2020-05-08", "2020-05-09", "2020-05-10",
> >> >> > "2020-05-11", "2020-05-12", "2020-05-13",
> >> >> >        "2020-05-14", "2020-05-15", "2020-05-16", "2020-05-17",
> >> "2020-05-18")
> >> >> > names(x1) <- d1
> >> >> > pop = 1438443864
> >> >> > Ts_mean = 5.16
> >> >> > Ts_sd   = 1.49
> >> >> > N=10000
> >> >> > TODAY = Sys.Date()
> >> >> > mGT = generation.time("gamma", c(Ts_mean, Ts_sd))
> >> >> > R0 = estimate.R(x1, t=d1, GT=mGT, begin=as.Date(d1[1]),
> >end=TODAY,
> >> >> >                 methods="EG",pop.size=pop, nsim=N)
> >> >> > ```
> >> >> > but when I run I get:
> >> >> > ```
> >> >> > Error in if (end.nb <= begin.nb) stop("'begin' and 'end' are not
> >> >> > consistent.") :
> >> >> >   argument is of length zero
> >> >> > > as.Date(d1[1])
> >> >> > [1] "2020-01-23"
> >> >> > > TODAY
> >> >> > [1] "2020-05-22"
> >> >> > > str(TODAY)
> >> >> >  Date[1:1], format: "2020-05-22"
> >> >> > > str(as.Date(d1[1]))
> >> >> >  Date[1:1], format: "2020-01-23"
> >> >> > ```
> >> >> > Since I provided both start and end in the same format, I don't
> >> understand
> >> >> > the error.
> >> >> > Any tips?
> >> >> > Thank you
> >> >> > --
> >> >> > Best regards,
> >> >> > Luigi
> >> >> >
> >> >> >         [[alternative HTML version deleted]]
> >> >> >
> >> >> > ______________________________________________
> >> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >see
> >> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> >> > PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> >> > and provide commented, minimal, self-contained, reproducible
> >code.
> >> >
> >> >
> >> >
> >> > --
> >> > Best regards,
> >> > Luigi
> >>
>
> --
> Sent from my phone. Please excuse my brevity.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sat May 23 04:50:02 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 22 May 2020 19:50:02 -0700
Subject: [R] How to use R0 package?
In-Reply-To: <CA+8X3fXrUksd65PXOTL+AsmzG0M0WhWyLcQuwgN-vR0e2GEUvg@mail.gmail.com>
References: <CAMk+s2Trp1JJPzrKU-1=67EvmHj4ks94q8C9r_izS-bXSUgC8w@mail.gmail.com>
 <CAGgJW74JBMBtCEKAYLGwKo+GgFSZmdwOLFXydYdAO6t0qGxYAQ@mail.gmail.com>
 <CAMk+s2QULpsOSpvJF3rMfjOyvQz0RvDciXbziCTeaamrGxoHYQ@mail.gmail.com>
 <CAGgJW777jvC4GMzex_TE3UNpsXjbwVY=Zh6VD+iABUc_9cf=Kg@mail.gmail.com>
 <CAMk+s2RkLoeptNF+9cPZWbUnZmFegHPusHNC=ThEd=D3WChsNA@mail.gmail.com>
 <CD5E8DE9-D511-49C0-83F9-6E1A8F0117F5@dcn.davis.ca.us>
 <CA+8X3fXrUksd65PXOTL+AsmzG0M0WhWyLcQuwgN-vR0e2GEUvg@mail.gmail.com>
Message-ID: <49B88591-2174-4500-953E-A3C0E2CC58C9@dcn.davis.ca.us>

Because the dates might not be consecutive.  Or in ISO format.

On May 22, 2020 7:38:17 PM PDT, Jim Lemon <drjimlemon at gmail.com> wrote:
>So what if you treat a nuisance as a feature and import your dates as
>factors? as.numeric(dates) would have the correct structure or am I,
>as usual, missing something?
>
>Jim
>
>On Sat, May 23, 2020 at 1:00 AM Jeff Newmiller
><jdnewmil at dcn.davis.ca.us> wrote:
>>
>> This is getting off-topic here but R0 is a mathematical parameter
>unrelated to calendar dates. It arises when analyzing case counts
>(integers) as a function of the numerical measure of time since some
>non-trivial number of cases has occurred (conventionally this measure
>is in days)..
>>
>> dta$days <- as.numeric( dta$date - startdate, units="days" )
>>
>> On May 22, 2020 5:31:48 AM PDT, Luigi Marongiu
><marongiu.luigi at gmail.com> wrote:
>> >In theory, it works
>> >```
>> >> R0 = estimate.R(x1, t=d1, GT=mGT, begin=1, end=117,
>> >                methods="EG",pop.size=pop, nsim=N)
>> >>R0
>> >Reproduction number estimate using  Exponential Growth  method.
>> >R :  0.7425278[ 0.7409297 , 0.7441229 ]
>> >```
>> >but I am not happy because 1. I have to use numbers instead of
>> >variables and 2. numbers instead of dates (which are instead
>reported
>> >in
>> >the examples...).
>> >Even if I force to an integer, I still get an error:
>> >```
>> >> R0 = estimate.R(x1, t=d1, GT=mGT, begin=1, end=length(x1),
>> >+                 methods="EG",pop.size=pop, nsim=N)
>> >Error in integrity.checks(epid, t, GT, begin, end, date.first.obs,
>> >time.step,  :
>> >  If both 'begin'= 1  and 'end'= 117  are provided, they must be of
>the
>> >same class (dates, character strings or integers).
>> >> int
>> >Error: object 'int' not found
>> >> R0 = estimate.R(x1, t=d1, GT=mGT, begin=1,
>> >end=as.integer(length(x1)),
>> >+                 methods="EG",pop.size=pop, nsim=N)
>> >Error in integrity.checks(epid, t, GT, begin, end, date.first.obs,
>> >time.step,  :
>> >  If both 'begin'= 1  and 'end'= 117  are provided, they must be of
>the
>> >same class (dates, character strings or integers).
>> >```
>> >
>> >
>> >
>> >On Fri, May 22, 2020 at 1:51 PM Eric Berger <ericjberger at gmail.com>
>> >wrote:
>> >
>> >> Hi Luigi,
>> >> how about begin=1L (to force it to be integer).
>> >> Also please keep the correspondence on the help list.
>> >>
>> >> Best,
>> >> Eric
>> >>
>> >> On Fri, May 22, 2020 at 2:40 PM Luigi Marongiu
>> ><marongiu.luigi at gmail.com>
>> >> wrote:
>> >> >
>> >> > Same error:
>> >> > ```
>> >> > > R0 = estimate.R(x1, t=d1, GT=mGT, begin=1, end=length(x1),
>> >> >                 methods="EG",pop.size=pop, nsim=N)
>> >> >  Error in integrity.checks(epid, t, GT, begin, end,
>date.first.obs,
>> >> time.step,  :
>> >> >   If both 'begin'= 1  and 'end'= 117  are provided, they must be
>of
>> >the
>> >> same class (dates, character strings or integers).
>> >> > > str(length(x1))
>> >> >  int 117
>> >> > ```
>> >> >
>> >> >
>> >> > On Fri, May 22, 2020 at 12:35 PM Eric Berger
>> ><ericjberger at gmail.com>
>> >> wrote:
>> >> >>
>> >> >> Hi Luigi,
>> >> >> I am not familiar with the R0 package but I took a quick look.
>> >> >> The example in the documentation sets begin and end to
>integers.
>> >> >> Try setting begin = 1, end = 121 and see if that works.
>> >> >>
>> >> >> HTH,
>> >> >> Eric
>> >> >>
>> >> >> On Fri, May 22, 2020 at 1:17 PM Luigi Marongiu <
>> >> marongiu.luigi at gmail.com> wrote:
>> >> >> >
>> >> >> > Hello,
>> >> >> > I am trying ot get the R0 from the incidence data from China
>for
>> >the
>> >> >> > COVID-19. I set the following:
>> >> >> > ```
>> >> >> > library("R0")
>> >> >> > x1 <- c(259,   457,   688,   769,  1771,  1459,  1737,  1981,
>> >2099,
>> >> 2589,
>> >> >> >  2825,  3235,  3884,  3694,  3143,
>> >> >> >         3385,  2652,  2973,  2467,  2015, 14108,  5090, 
>2641,
>> >2008,
>> >> >> >  2048,  1888,  1749,   391,   889,  823,
>> >> >> >         648,   214,   508,   406,   433,   327,   427,   573,
>> >202,
>> >>  125,
>> >> >> >   119,   139,   143,    99,    44,
>> >> >> >         40,    19,    24,    15,     8,    11,    20,     0,
>> >16,
>> >>   13,
>> >> >> >    13,    34,    39,    46,    39,
>> >> >> >         78,    47,    67,    55,    54,    45,     0,    79,
>> >36,
>> >>   35,
>> >> >> >    31,    19,    30,    39,    32,
>> >> >> >         0,    63,    42,    46,    99,   108,    89,    46,
>> >46,
>> >> 26,
>> >> >> > 325,    27,    16,    12,    11,
>> >> >> >         30,    10,     6,    12,    11,     3,     6,    22,
>> >4,
>> >>   12,
>> >> >> >     1,     3,     3,     1,     2,
>> >> >> >         2,     1,     1,    14,    17,     1,     7,     3,
>> >4,
>> >>  8,
>> >> >> >   6,     7)
>> >> >> > d1 = c("2020-01-23", "2020-01-24", "2020-01-25",
>"2020-01-26",
>> >> >> > "2020-01-27", "2020-01-28", "2020-01-29",
>> >> >> >        "2020-01-30", "2020-01-31", "2020-02-01",
>"2020-02-02",
>> >> >> > "2020-02-03", "2020-02-04", "2020-02-05",
>> >> >> >        "2020-02-06", "2020-02-07", "2020-02-08",
>"2020-02-09",
>> >> >> > "2020-02-10", "2020-02-11", "2020-02-12",
>> >> >> >        "2020-02-13", "2020-02-14", "2020-02-15",
>"2020-02-16",
>> >> >> > "2020-02-17", "2020-02-18", "2020-02-19",
>> >> >> >        "2020-02-20", "2020-02-21", "2020-02-22",
>"2020-02-23",
>> >> >> > "2020-02-24", "2020-02-25", "2020-02-26",
>> >> >> >        "2020-02-27", "2020-02-28", "2020-02-29",
>"2020-03-01",
>> >> >> > "2020-03-02", "2020-03-03", "2020-03-04",
>> >> >> >        "2020-03-05", "2020-03-06", "2020-03-07",
>"2020-03-08",
>> >> >> > "2020-03-09", "2020-03-10", "2020-03-11",
>> >> >> >        "2020-03-12", "2020-03-13", "2020-03-14",
>"2020-03-15",
>> >> >> > "2020-03-16", "2020-03-17", "2020-03-18",
>> >> >> >        "2020-03-19", "2020-03-20", "2020-03-21",
>"2020-03-22",
>> >> >> > "2020-03-23", "2020-03-24", "2020-03-25",
>> >> >> >        "2020-03-26", "2020-03-27", "2020-03-28",
>"2020-03-29",
>> >> >> > "2020-03-30", "2020-03-31", "2020-04-01",
>> >> >> >        "2020-04-02", "2020-04-03", "2020-04-04",
>"2020-04-05",
>> >> >> > "2020-04-06", "2020-04-07", "2020-04-08",
>> >> >> >        "2020-04-09", "2020-04-10", "2020-04-11",
>"2020-04-12",
>> >> >> > "2020-04-13", "2020-04-14", "2020-04-15",
>> >> >> >        "2020-04-16", "2020-04-17", "2020-04-18",
>"2020-04-19",
>> >> >> > "2020-04-20", "2020-04-21", "2020-04-22",
>> >> >> >        "2020-04-23", "2020-04-24", "2020-04-25",
>"2020-04-26",
>> >> "2020-04-27"
>> >> >> > ,"2020-04-28", "2020-04-29",
>> >> >> >        "2020-04-30", "2020-05-01", "2020-05-02",
>"2020-05-03",
>> >> >> > "2020-05-04", "2020-05-05", "2020-05-06",
>> >> >> >        "2020-05-07", "2020-05-08", "2020-05-09",
>"2020-05-10",
>> >> >> > "2020-05-11", "2020-05-12", "2020-05-13",
>> >> >> >        "2020-05-14", "2020-05-15", "2020-05-16",
>"2020-05-17",
>> >> "2020-05-18")
>> >> >> > names(x1) <- d1
>> >> >> > pop = 1438443864
>> >> >> > Ts_mean = 5.16
>> >> >> > Ts_sd   = 1.49
>> >> >> > N=10000
>> >> >> > TODAY = Sys.Date()
>> >> >> > mGT = generation.time("gamma", c(Ts_mean, Ts_sd))
>> >> >> > R0 = estimate.R(x1, t=d1, GT=mGT, begin=as.Date(d1[1]),
>> >end=TODAY,
>> >> >> >                 methods="EG",pop.size=pop, nsim=N)
>> >> >> > ```
>> >> >> > but when I run I get:
>> >> >> > ```
>> >> >> > Error in if (end.nb <= begin.nb) stop("'begin' and 'end' are
>not
>> >> >> > consistent.") :
>> >> >> >   argument is of length zero
>> >> >> > > as.Date(d1[1])
>> >> >> > [1] "2020-01-23"
>> >> >> > > TODAY
>> >> >> > [1] "2020-05-22"
>> >> >> > > str(TODAY)
>> >> >> >  Date[1:1], format: "2020-05-22"
>> >> >> > > str(as.Date(d1[1]))
>> >> >> >  Date[1:1], format: "2020-01-23"
>> >> >> > ```
>> >> >> > Since I provided both start and end in the same format, I
>don't
>> >> understand
>> >> >> > the error.
>> >> >> > Any tips?
>> >> >> > Thank you
>> >> >> > --
>> >> >> > Best regards,
>> >> >> > Luigi
>> >> >> >
>> >> >> >         [[alternative HTML version deleted]]
>> >> >> >
>> >> >> > ______________________________________________
>> >> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>> >see
>> >> >> > https://stat.ethz.ch/mailman/listinfo/r-help
>> >> >> > PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> >> > and provide commented, minimal, self-contained, reproducible
>> >code.
>> >> >
>> >> >
>> >> >
>> >> > --
>> >> > Best regards,
>> >> > Luigi
>> >>
>>
>> --
>> Sent from my phone. Please excuse my brevity.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From cry@n @end|ng |rom b|ngh@mton@edu  Sat May 23 06:15:57 2020
From: cry@n @end|ng |rom b|ngh@mton@edu (Christopher W. Ryan)
Date: Sat, 23 May 2020 00:15:57 -0400
Subject: [R] 
 [External Email] Re: [External] Re: access for free more than
 500 essential Springer Nature textbooks
In-Reply-To: <CAHz+bWbwRpPksaLgyEe-bkFKbRwsUiS7rBuOAdUkvwyMWNgGFQ@mail.gmail.com>
References: <CAGx1TMBxYGKQsUY354c8iUdEzYdX6+g2NaBNzzHF4BEGpbLnCA@mail.gmail.com>
 <CAB8pepzHks4aMWWHqzymKks7wmr1biU6Uk18DorDrVaOz=r35A@mail.gmail.com>
 <CAGx1TMDTRP0pSC+O7906u6kdYpwk+KSS2=5tGyCXFoHp=eQi2g@mail.gmail.com>
 <CAB8pepxYn80zZGiZbVRS6iPDaj68ei9jj=OVocOJO3QkkmVQmw@mail.gmail.com>
 <7C1BF9C3-1AB3-4A77-AA8F-86DB30DF16E1@dcn.davis.ca.us>
 <CABcYAdJzC+PZRnijsRQ97FK91mDN9YHyZwy5P+s4DRtD1n1N3g@mail.gmail.com>
 <CAHz+bWbwRpPksaLgyEe-bkFKbRwsUiS7rBuOAdUkvwyMWNgGFQ@mail.gmail.com>
Message-ID: <986aec24-6c46-d025-8aa0-2c151c1001a8@binghamton.edu>

Am I interpreting this offer correctly, that it is for libraries to
obtain access to the e-books for free? It does not seem to me that an
invididual can download one--am I missing that part?

Thanks

--Chris Ryan

Mark Leeds wrote:
> Abby: here's an easier link for seeing what you might like.
> 
> https://link.springer.com/search?facet-content-type=%22Book%22&package=mat-covid19_textbooks&%23038;facet-language=%22En%22&%23038;sortOrder=newestFirst&%23038;showAll=true
> 
> On Fri, May 22, 2020 at 9:18 PM Richard O'Keefe <raoknz at gmail.com> wrote:
> 
>> the real pleasure comes from things you weren't looking for but recognise
>> as just what you needed.
>>
>> On Sat, 23 May 2020 at 12:34, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>> wrote:
>>
>>> You are bound to be disappointed if you invert the purpose of the list.
>>> This is marketing... think of it as a sale... stores rarely put their
>>> entire stock on sale... particularly if the sale price is zero. You have
>> to
>>> start with the list and look for interesting titles.
>>>
>>> But don't let me dissuade you from adding to your killfile if that seems
>>> more useful to you.
>>>
>>> On May 22, 2020 5:04:29 PM PDT, Abby Spurdle <spurdle.a at gmail.com>
>> wrote:
>>>>> The Excel file is what you need.
>>>>
>>>> Well, now I'm in a bad mood.
>>>>
>>>> I went to all the trouble of opening the thing...
>>>> And the first two Springer-published books I look for, aren't there.
>>>>
>>>> (1) Programming with Data, John Chambers
>>>> (2) Applied Econometrics with R, Z and co.
>>>>
>>>> Next time someone tells me to use an Excel document, I'm adding them
>>>> to the spam list.
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> --
>>> Sent from my phone. Please excuse my brevity.
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From m@rk|eed@2 @end|ng |rom gm@||@com  Sat May 23 07:33:18 2020
From: m@rk|eed@2 @end|ng |rom gm@||@com (Mark Leeds)
Date: Sat, 23 May 2020 01:33:18 -0400
Subject: [R] 
 [External Email] Re: [External] Re: access for free more than
 500 essential Springer Nature textbooks
In-Reply-To: <986aec24-6c46-d025-8aa0-2c151c1001a8@binghamton.edu>
References: <CAGx1TMBxYGKQsUY354c8iUdEzYdX6+g2NaBNzzHF4BEGpbLnCA@mail.gmail.com>
 <CAB8pepzHks4aMWWHqzymKks7wmr1biU6Uk18DorDrVaOz=r35A@mail.gmail.com>
 <CAGx1TMDTRP0pSC+O7906u6kdYpwk+KSS2=5tGyCXFoHp=eQi2g@mail.gmail.com>
 <CAB8pepxYn80zZGiZbVRS6iPDaj68ei9jj=OVocOJO3QkkmVQmw@mail.gmail.com>
 <7C1BF9C3-1AB3-4A77-AA8F-86DB30DF16E1@dcn.davis.ca.us>
 <CABcYAdJzC+PZRnijsRQ97FK91mDN9YHyZwy5P+s4DRtD1n1N3g@mail.gmail.com>
 <CAHz+bWbwRpPksaLgyEe-bkFKbRwsUiS7rBuOAdUkvwyMWNgGFQ@mail.gmail.com>
 <986aec24-6c46-d025-8aa0-2c151c1001a8@binghamton.edu>
Message-ID: <CAHz+bWZ5m=DiLqL=1xNqpYgF1Er+8oozN1eZztADY_-3F705dQ@mail.gmail.com>

Hi: I'm not sure about the other link that was sent because I didn't try
it  but, in the case of the link that I sent,
you can obtain the pdfs of a lot of the books shown.


On Sat, May 23, 2020 at 12:16 AM Christopher W. Ryan <cryan at binghamton.edu>
wrote:

> Am I interpreting this offer correctly, that it is for libraries to
> obtain access to the e-books for free? It does not seem to me that an
> invididual can download one--am I missing that part?
>
> Thanks
>
> --Chris Ryan
>
> Mark Leeds wrote:
> > Abby: here's an easier link for seeing what you might like.
> >
> >
> https://link.springer.com/search?facet-content-type=%22Book%22&package=mat-covid19_textbooks&%23038;facet-language=%22En%22&%23038;sortOrder=newestFirst&%23038;showAll=true
> >
> > On Fri, May 22, 2020 at 9:18 PM Richard O'Keefe <raoknz at gmail.com>
> wrote:
> >
> >> the real pleasure comes from things you weren't looking for but
> recognise
> >> as just what you needed.
> >>
> >> On Sat, 23 May 2020 at 12:34, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> >> wrote:
> >>
> >>> You are bound to be disappointed if you invert the purpose of the list.
> >>> This is marketing... think of it as a sale... stores rarely put their
> >>> entire stock on sale... particularly if the sale price is zero. You
> have
> >> to
> >>> start with the list and look for interesting titles.
> >>>
> >>> But don't let me dissuade you from adding to your killfile if that
> seems
> >>> more useful to you.
> >>>
> >>> On May 22, 2020 5:04:29 PM PDT, Abby Spurdle <spurdle.a at gmail.com>
> >> wrote:
> >>>>> The Excel file is what you need.
> >>>>
> >>>> Well, now I'm in a bad mood.
> >>>>
> >>>> I went to all the trouble of opening the thing...
> >>>> And the first two Springer-published books I look for, aren't there.
> >>>>
> >>>> (1) Programming with Data, John Chambers
> >>>> (2) Applied Econometrics with R, Z and co.
> >>>>
> >>>> Next time someone tells me to use an Excel document, I'm adding them
> >>>> to the spam list.
> >>>>
> >>>> ______________________________________________
> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide
> >>>> http://www.R-project.org/posting-guide.html
> >>>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>> --
> >>> Sent from my phone. Please excuse my brevity.
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>

	[[alternative HTML version deleted]]


From m@z@t|@nmex|co @end|ng |rom y@hoo@com  Sat May 23 07:57:53 2020
From: m@z@t|@nmex|co @end|ng |rom y@hoo@com (Felipe Carrillo)
Date: Sat, 23 May 2020 05:57:53 +0000 (UTC)
Subject: [R] 
 [External Email] Re: [External] Re: access for free more than
 500 essential Springer Nature textbooks
In-Reply-To: <CAHz+bWZ5m=DiLqL=1xNqpYgF1Er+8oozN1eZztADY_-3F705dQ@mail.gmail.com>
References: <CAGx1TMBxYGKQsUY354c8iUdEzYdX6+g2NaBNzzHF4BEGpbLnCA@mail.gmail.com>
 <CAB8pepzHks4aMWWHqzymKks7wmr1biU6Uk18DorDrVaOz=r35A@mail.gmail.com>
 <CAGx1TMDTRP0pSC+O7906u6kdYpwk+KSS2=5tGyCXFoHp=eQi2g@mail.gmail.com>
 <CAB8pepxYn80zZGiZbVRS6iPDaj68ei9jj=OVocOJO3QkkmVQmw@mail.gmail.com>
 <7C1BF9C3-1AB3-4A77-AA8F-86DB30DF16E1@dcn.davis.ca.us>
 <CABcYAdJzC+PZRnijsRQ97FK91mDN9YHyZwy5P+s4DRtD1n1N3g@mail.gmail.com>
 <CAHz+bWbwRpPksaLgyEe-bkFKbRwsUiS7rBuOAdUkvwyMWNgGFQ@mail.gmail.com>
 <986aec24-6c46-d025-8aa0-2c151c1001a8@binghamton.edu>
 <CAHz+bWZ5m=DiLqL=1xNqpYgF1Er+8oozN1eZztADY_-3F705dQ@mail.gmail.com>
Message-ID: <1276725452.193779.1590213473253@mail.yahoo.com>

They can be downladed. I saved two of them to my desktop as pdf earlier.

Sent from Yahoo Mail on Android 
 
  On Fri, May 22, 2020 at 10:35 PM, Mark Leeds<markleeds2 at gmail.com> wrote:   Hi: I'm not sure about the other link that was sent because I didn't try
it? but, in the case of the link that I sent,
you can obtain the pdfs of a lot of the books shown.


On Sat, May 23, 2020 at 12:16 AM Christopher W. Ryan <cryan at binghamton.edu>
wrote:

> Am I interpreting this offer correctly, that it is for libraries to
> obtain access to the e-books for free? It does not seem to me that an
> invididual can download one--am I missing that part?
>
> Thanks
>
> --Chris Ryan
>
> Mark Leeds wrote:
> > Abby: here's an easier link for seeing what you might like.
> >
> >
> https://link.springer.com/search?facet-content-type=%22Book%22&package=mat-covid19_textbooks&%23038;facet-language=%22En%22&%23038;sortOrder=newestFirst&%23038;showAll=true
> >
> > On Fri, May 22, 2020 at 9:18 PM Richard O'Keefe <raoknz at gmail.com>
> wrote:
> >
> >> the real pleasure comes from things you weren't looking for but
> recognise
> >> as just what you needed.
> >>
> >> On Sat, 23 May 2020 at 12:34, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> >> wrote:
> >>
> >>> You are bound to be disappointed if you invert the purpose of the list.
> >>> This is marketing... think of it as a sale... stores rarely put their
> >>> entire stock on sale... particularly if the sale price is zero. You
> have
> >> to
> >>> start with the list and look for interesting titles.
> >>>
> >>> But don't let me dissuade you from adding to your killfile if that
> seems
> >>> more useful to you.
> >>>
> >>> On May 22, 2020 5:04:29 PM PDT, Abby Spurdle <spurdle.a at gmail.com>
> >> wrote:
> >>>>> The Excel file is what you need.
> >>>>
> >>>> Well, now I'm in a bad mood.
> >>>>
> >>>> I went to all the trouble of opening the thing...
> >>>> And the first two Springer-published books I look for, aren't there.
> >>>>
> >>>> (1) Programming with Data, John Chambers
> >>>> (2) Applied Econometrics with R, Z and co.
> >>>>
> >>>> Next time someone tells me to use an Excel document, I'm adding them
> >>>> to the spam list.
> >>>>
> >>>> ______________________________________________
> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide
> >>>> http://www.R-project.org/posting-guide.html
> >>>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>> --
> >>> Sent from my phone. Please excuse my brevity.
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>
> >>? ? ? ? [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> >? ? ? [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>

??? [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
  

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Untitled
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200523/1a83266f/attachment.ksh>

From bry@n@m@c24 @end|ng |rom gm@||@com  Sat May 23 02:37:21 2020
From: bry@n@m@c24 @end|ng |rom gm@||@com (Bryan Mac)
Date: Fri, 22 May 2020 17:37:21 -0700
Subject: [R] Wordcloud warnings
Message-ID: <AAA8134F-B55F-4D42-ABC5-4278D77DFB99@gmail.com>

Hi, 

I am having trouble understanding why I am getting this warning and how to fix it. I don?t have any of these functions in my script..

This is the warning:

warning messages:
1: in tm_map.simplecorpus(corpus, tm::removepunctuation) : transformation drops documents 
2: in tm_map.simplecorpus(corpus, function(x) tm::removewords(x, tm::stopwords())) : transformation drops documents 
3: in tm_map.simplecorpus(corpus, tm::removepunctuation) : transformation drops documents 
4: in tm_map.simplecorpus(corpus, function(x) tm::removewords(x, tm::stopwords())) : transformation drops documents 
5: in tm_map.simplecorpus(corpus, tm::removepunctuation) : transformation drops documents 
6: in tm_map.simplecorpus(corpus, function(x) tm::removewords(x, tm::stopwords())) : transformation drops documents


This is the script:

     if (runtype=="native") {
        words <- Corpus(VectorSource(test123b[,2]))
        wordcloud(words, min.freq = minfreq, max.words=300, random.order = FALSE ,colors=brewer.pal(9,"Reds")[0:-3])
        
      }   else {
        wordcloud((as.character(test123[,2])), min.freq = minfreq, max.words=300, random.order = FALSE ,colors=brewer.pal(9,"Reds")[0:-3])
        
      }
      

Best,
Bryan

From @purd|e@@ @end|ng |rom gm@||@com  Sat May 23 09:52:17 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Sat, 23 May 2020 19:52:17 +1200
Subject: [R] access for free more than 500 essential Springer Nature
 textbooks
In-Reply-To: <CAGx1TMBxYGKQsUY354c8iUdEzYdX6+g2NaBNzzHF4BEGpbLnCA@mail.gmail.com>
References: <CAGx1TMBxYGKQsUY354c8iUdEzYdX6+g2NaBNzzHF4BEGpbLnCA@mail.gmail.com>
Message-ID: <CAB8pepx1tibXtDCcJfEvqDp9KBsrmGRvoqcHJmapB1U=w4xCAA@mail.gmail.com>

> My book is
> Statistical Analysis and Data Display, Richard M. Heiberger, Burt
> Holland, 2nd ed. 2015

In all fairness, I thought should look at your book.

I was quite impressed by the chapter on multiple comparisons.
And may look again, later.

In my personal opinion (diverging slightly), with more and more people
using extensive exploratory-style modelling, I think some of the
methods for multiple comparisons could (and should) be adapted to the
interpretation of exploratory plots.

And returning to your book...
There's relatively comprehensive chapters on multiple regression.
And I'm happy you used the term "explanatory" rather than "independent".
When people use the term "independent variables" (outside a
theoretical context) they usually go my how-did-these people-get-a-job
list.

Some comments:
(1) Expanding on the above point, I couldn't see a section on
interpreting regression coefficients, which is something people often
get wrong.
(2) It had a nice objective/frequentist flavor, but it would have been
nicer to see clearer references to robust, semiparametric and more
general nonparametric methods, even if only brief.

In principle, some of these (but not all) follow the same philosophy
as classical statistics, but allow greater flexibility.


From bog@@o@chr|@to|er @end|ng |rom gm@||@com  Sat May 23 11:28:34 2020
From: bog@@o@chr|@to|er @end|ng |rom gm@||@com (Christofer Bogaso)
Date: Sat, 23 May 2020 14:58:34 +0530
Subject: [R] Bayesian estimation with MCMC
Message-ID: <CA+dpOJn=_DgcSxXMfMQ8KaLJecvzr2-JJdsBCm1TFv0z+kwpNQ@mail.gmail.com>

Hi,

In python there is a package called pymc3 for Bayesian parameter
estimation with MCMC.

I am curious if there is any equivalent package available for R.

Any pointer will be highly appreciated.


From v@h|d@borj|65 @end|ng |rom gm@||@com  Sat May 23 11:45:37 2020
From: v@h|d@borj|65 @end|ng |rom gm@||@com (Vahid Borji)
Date: Sat, 23 May 2020 14:15:37 +0430
Subject: [R] The best way for making speciall matrix
Message-ID: <CAEPHqhYyx+8wB0dRkEtb8NADMgcpWQBJbGChy1+tfpj78hykgg@mail.gmail.com>

Hi my friends,

I want to make the below matrix in r:

1 0 0 4

0 2 0 5

0 0 3 6

I used the below code:

matrix(c(1,0,0,0,2,0,0,0,3,4,5,6),nrow=3)

My code works. But I do not like my solution way. I am thinking to find the
simplest way for making this matrix. Do you think my code is the simplest
code for making this matrix? If not, could anyone writes a simpler code
than my one?

	[[alternative HTML version deleted]]


From @@h|mk@poor @end|ng |rom gm@||@com  Sat May 23 12:00:48 2020
From: @@h|mk@poor @end|ng |rom gm@||@com (Ashim Kapoor)
Date: Sat, 23 May 2020 15:30:48 +0530
Subject: [R] The best way for making speciall matrix
In-Reply-To: <CAEPHqhYyx+8wB0dRkEtb8NADMgcpWQBJbGChy1+tfpj78hykgg@mail.gmail.com>
References: <CAEPHqhYyx+8wB0dRkEtb8NADMgcpWQBJbGChy1+tfpj78hykgg@mail.gmail.com>
Message-ID: <CAC8=1eoYKKM_SLdw6ioM1Bsbmu8pgYOhz0dBo8DCPDfo8Tbvnw@mail.gmail.com>

Dear Vahid,

Would this help?

> row1<- c(1,0,0,4)
> row2<- c(0,2,0,5)
> row3<- c(0,0,3,6)
> mymatrix <- rbind(row1,row2,row3)
> mymatrix
     [,1] [,2] [,3] [,4]
row1    1    0    0    4
row2    0    2    0    5
row3    0    0    3    6
>

Best Regards,
Ashim

On Sat, May 23, 2020 at 3:16 PM Vahid Borji <vahid.borji65 at gmail.com> wrote:
>
> Hi my friends,
>
> I want to make the below matrix in r:
>
> 1 0 0 4
>
> 0 2 0 5
>
> 0 0 3 6
>
> I used the below code:
>
> matrix(c(1,0,0,0,2,0,0,0,3,4,5,6),nrow=3)
>
> My code works. But I do not like my solution way. I am thinking to find the
> simplest way for making this matrix. Do you think my code is the simplest
> code for making this matrix? If not, could anyone writes a simpler code
> than my one?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat May 23 12:09:10 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sat, 23 May 2020 11:09:10 +0100
Subject: [R] The best way for making speciall matrix
In-Reply-To: <CAEPHqhYyx+8wB0dRkEtb8NADMgcpWQBJbGChy1+tfpj78hykgg@mail.gmail.com>
References: <CAEPHqhYyx+8wB0dRkEtb8NADMgcpWQBJbGChy1+tfpj78hykgg@mail.gmail.com>
Message-ID: <84264c8a-2738-9ac2-4d79-e6f66d83cdbb@sapo.pt>

Hello,

Use diag() and cbind().


special_mat <- function(n){
   if(n %% 2 != 0) {
     msg <- paste(sQuote(n), 'is not a multiple of 2, will use')
     n <- 2*(n%/% 2)
     msg <- paste(msg, sQuote(n))
     warning(msg)
   }
   x <- diag(n/2)
   diag(x) <- seq.int(n/2)
   cbind(x, (n/2 + 1):n)
}

special_mat(6)
special_mat(8)
special_mat(7)


Hope this helps,

Rui Barradas

?s 10:45 de 23/05/20, Vahid Borji escreveu:
> Hi my friends,
> 
> I want to make the below matrix in r:
> 
> 1 0 0 4
> 
> 0 2 0 5
> 
> 0 0 3 6
> 
> I used the below code:
> 
> matrix(c(1,0,0,0,2,0,0,0,3,4,5,6),nrow=3)
> 
> My code works. But I do not like my solution way. I am thinking to find the
> simplest way for making this matrix. Do you think my code is the simplest
> code for making this matrix? If not, could anyone writes a simpler code
> than my one?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @purd|e@@ @end|ng |rom gm@||@com  Sat May 23 12:12:46 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Sat, 23 May 2020 22:12:46 +1200
Subject: [R] The best way for making speciall matrix
In-Reply-To: <CAEPHqhYyx+8wB0dRkEtb8NADMgcpWQBJbGChy1+tfpj78hykgg@mail.gmail.com>
References: <CAEPHqhYyx+8wB0dRkEtb8NADMgcpWQBJbGChy1+tfpj78hykgg@mail.gmail.com>
Message-ID: <CAB8pepwQZMvbejfCbYFzN7iRwvwsW4zBhAhes7OF7+TEY4Xt2Q@mail.gmail.com>

This sounds like a homework question...
But... numerical linear algebra rocks...

    cbind (diag (1:3), 4:6)


On Sat, May 23, 2020 at 9:46 PM Vahid Borji <vahid.borji65 at gmail.com> wrote:
>
> Hi my friends,
>
> I want to make the below matrix in r:
>
> 1 0 0 4
>
> 0 2 0 5
>
> 0 0 3 6
>
> I used the below code:
>
> matrix(c(1,0,0,0,2,0,0,0,3,4,5,6),nrow=3)
>
> My code works. But I do not like my solution way. I am thinking to find the
> simplest way for making this matrix. Do you think my code is the simplest
> code for making this matrix? If not, could anyone writes a simpler code
> than my one?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat May 23 13:25:36 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sat, 23 May 2020 12:25:36 +0100
Subject: [R] The best way for making speciall matrix
In-Reply-To: <CAEPHqhboVGkqyQ1eRaBoJbwO5_YGXiSB6AWQ_a23gZCKAwWsvg@mail.gmail.com>
References: <CAEPHqhYyx+8wB0dRkEtb8NADMgcpWQBJbGChy1+tfpj78hykgg@mail.gmail.com>
 <84264c8a-2738-9ac2-4d79-e6f66d83cdbb@sapo.pt>
 <CAEPHqhboVGkqyQ1eRaBoJbwO5_YGXiSB6AWQ_a23gZCKAwWsvg@mail.gmail.com>
Message-ID: <e90b2dc9-1327-e071-cb6f-10c7fbb1e756@sapo.pt>

Hello,

Please keep this on the list, R-help is threaded and it becomes part of 
the archives, maybe it will be usefull to others.


You are now asking 2 other different questions.
Are you looking for something like this?

two_values_mat <- function(n, fill = 1, diagonal = 0){
   m <- matrix(fill, nrow = n, ncol = n)
   diag(m) <- diagonal
   m
}
two_values_mat(3)
two_values_mat(3, 0.5, 1)

odd_even_mat <- function(n){
   even <- n %% 2 == 0
   nr <- floor(sqrt(n)) - !even
   nc <- ceiling(n/nr)
   i <- c(0, seq.int(n - even))
   matrix(2*i + !even, nrow = nr, ncol = nc, byrow = TRUE)
}
odd_even_mat(30)
odd_even_mat(17)


Hope this ehlps,

Rui Barradas

?s 11:52 de 23/05/20, Vahid Borji escreveu:
> Thank you Rui,
> Actually, I am making some matrix as follows:
> 0 1 1
> 1 0 1
> 1 1 0
> matrix(rep(c(0,1,1,1),3),nrow=3,ncol=3)
> and
> 1? 3 ? 5? 7? 9? 11
> 13? 15? 17? 19
> 21 ? 23? 25? 27
> 29 31 33 35
> i<-0:17 matrix(2*i+1,nrow=3,byrow=TRUE)
> and
> 1????? 0.5?? 0.5
> 0.5?? ? 1???? 0.5
> 0.5?? 0.5??? 1
> matrix(rep(c(1,0.5,0.5,0.5),3),nrow=3,ncol=3)
> 
> For all the above matrixes I have written a code (as you see below each 
> matrix) . If you want to make the above matrixes, will you use my codes 
> or will you have other simpler ways for making the above matrixes?
> 
> On Sat, May 23, 2020 at 2:39 PM Rui Barradas <ruipbarradas at sapo.pt 
> <mailto:ruipbarradas at sapo.pt>> wrote:
> 
>     Hello,
> 
>     Use diag() and cbind().
> 
> 
>     special_mat <- function(n){
>      ? ?if(n %% 2 != 0) {
>      ? ? ?msg <- paste(sQuote(n), 'is not a multiple of 2, will use')
>      ? ? ?n <- 2*(n%/% 2)
>      ? ? ?msg <- paste(msg, sQuote(n))
>      ? ? ?warning(msg)
>      ? ?}
>      ? ?x <- diag(n/2)
>      ? ?diag(x) <- seq.int <http://seq.int>(n/2)
>      ? ?cbind(x, (n/2 + 1):n)
>     }
> 
>     special_mat(6)
>     special_mat(8)
>     special_mat(7)
> 
> 
>     Hope this helps,
> 
>     Rui Barradas
> 
>     ?s 10:45 de 23/05/20, Vahid Borji escreveu:
>      > Hi my friends,
>      >
>      > I want to make the below matrix in r:
>      >
>      > 1 0 0 4
>      >
>      > 0 2 0 5
>      >
>      > 0 0 3 6
>      >
>      > I used the below code:
>      >
>      > matrix(c(1,0,0,0,2,0,0,0,3,4,5,6),nrow=3)
>      >
>      > My code works. But I do not like my solution way. I am thinking
>     to find the
>      > simplest way for making this matrix. Do you think my code is the
>     simplest
>      > code for making this matrix? If not, could anyone writes a
>     simpler code
>      > than my one?
>      >
>      >? ? ? ?[[alternative HTML version deleted]]
>      >
>      > ______________________________________________
>      > R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>     -- To UNSUBSCRIBE and more, see
>      > https://stat.ethz.ch/mailman/listinfo/r-help
>      > PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>      > and provide commented, minimal, self-contained, reproducible code.
>      >
>


From er|cjberger @end|ng |rom gm@||@com  Sat May 23 13:45:07 2020
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Sat, 23 May 2020 14:45:07 +0300
Subject: [R] Bayesian estimation with MCMC
In-Reply-To: <CA+dpOJn=_DgcSxXMfMQ8KaLJecvzr2-JJdsBCm1TFv0z+kwpNQ@mail.gmail.com>
References: <CA+dpOJn=_DgcSxXMfMQ8KaLJecvzr2-JJdsBCm1TFv0z+kwpNQ@mail.gmail.com>
Message-ID: <CAGgJW75X=qYUkHog6WQnn6V3X6B05jwxmGnJ_C8TJ_6ct8-J-A@mail.gmail.com>

Hi Christofer,
Did you try web search? I entered 'R CRAN Bayesian parameter
estimation with MCMC'
and it came back with the following which seems relevant.

https://cran.r-project.org/web/packages/airGR/vignettes/V02.2_param_mcmc.html

There are other search results, such as:
https://cran.r-project.org/web/packages/deBInfer/index.html

You should note that using R does not exclude using pymc3 (or any
other Python package) from R.
The R package reticulate makes it easy to use python packages from an R session.
See https://cran.r-project.org/web/packages/reticulate/index.html

HTH,
Eric

On Sat, May 23, 2020 at 12:29 PM Christofer Bogaso
<bogaso.christofer at gmail.com> wrote:
>
> Hi,
>
> In python there is a package called pymc3 for Bayesian parameter
> estimation with MCMC.
>
> I am curious if there is any equivalent package available for R.
>
> Any pointer will be highly appreciated.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From kry|ov@r00t @end|ng |rom gm@||@com  Sat May 23 13:48:44 2020
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Sat, 23 May 2020 14:48:44 +0300
Subject: [R] Bayesian estimation with MCMC
In-Reply-To: <CA+dpOJn=_DgcSxXMfMQ8KaLJecvzr2-JJdsBCm1TFv0z+kwpNQ@mail.gmail.com>
References: <CA+dpOJn=_DgcSxXMfMQ8KaLJecvzr2-JJdsBCm1TFv0z+kwpNQ@mail.gmail.com>
Message-ID: <20200523144844.568ef039@trisector>

On Sat, 23 May 2020 14:58:34 +0530
Christofer Bogaso <bogaso.christofer at gmail.com> wrote:

>Any pointer will be highly appreciated.

"Getting help with R" [1] describes several ways of looking for
R-related things. In particular, there is a Bayesian CRAN Task View
[2], which (as of now) has 52 occurrences of "MCMC".

-- 
Best regards,
Ivan

[1] https://www.r-project.org/help.html
[2] https://cran.r-project.org/view=Bayesian


From v@h|d@borj|65 @end|ng |rom gm@||@com  Sat May 23 14:25:54 2020
From: v@h|d@borj|65 @end|ng |rom gm@||@com (Vahid Borji)
Date: Sat, 23 May 2020 16:55:54 +0430
Subject: [R] How to make a vector with string elements without space
Message-ID: <CAEPHqhbViKqSNJQyT9KPhcEE6uq3p2UUTaJ_QcnWYQKcyrbo0w@mail.gmail.com>

Hello my r friends,
I want to make a vector with elements (c1,c2,...,c10).
I wrote the below code:
c(paste("c",1:10))
My code works but it gives me elements like "c 1", "c 2" to "c 10". I mean
there is a space between each c and its corresponding number. I want the
elements of the vector to be like "c1", "c2", to "c10"  (without space
between c and number). How can I fix this problem in my code?

	[[alternative HTML version deleted]]


From bor|@@@te|pe @end|ng |rom utoronto@c@  Sat May 23 14:35:52 2020
From: bor|@@@te|pe @end|ng |rom utoronto@c@ (Boris Steipe)
Date: Sat, 23 May 2020 12:35:52 +0000
Subject: [R] How to make a vector with string elements without space
In-Reply-To: <CAEPHqhbViKqSNJQyT9KPhcEE6uq3p2UUTaJ_QcnWYQKcyrbo0w@mail.gmail.com>
References: <CAEPHqhbViKqSNJQyT9KPhcEE6uq3p2UUTaJ_QcnWYQKcyrbo0w@mail.gmail.com>
Message-ID: <885DCBCB-E0AD-45FF-9CE7-C6EADEF0D54F@utoronto.ca>

The c() is unnecessary. paste() returns a vector.

Paste separates elements with " " by default. Set the separator to "" instead.
paste("c",1:10, sep = "")

... or use paste0(), which has "" as default separator.
paste0("c",1:10)

?paste  is your friend.


B.




> On 2020-05-23, at 22:25, Vahid Borji <vahid.borji65 at gmail.com> wrote:
> 
> Hello my r friends,
> I want to make a vector with elements (c1,c2,...,c10).
> I wrote the below code:
> c(paste("c",1:10))
> My code works but it gives me elements like "c 1", "c 2" to "c 10". I mean
> there is a space between each c and its corresponding number. I want the
> elements of the vector to be like "c1", "c2", to "c10"  (without space
> between c and number). How can I fix this problem in my code?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From v@h|d@borj|65 @end|ng |rom gm@||@com  Sat May 23 16:30:48 2020
From: v@h|d@borj|65 @end|ng |rom gm@||@com (Vahid Borji)
Date: Sat, 23 May 2020 19:00:48 +0430
Subject: [R] min rows multiplication by the max rows
Message-ID: <CAEPHqhZ72gyYCKXp083ygYSXhZVJxKy-bBKpmJ8gLs3=kJFfGw@mail.gmail.com>

Hi my R friends,

I have two matrices as follows:

M<-matrix(c(1,4,1,3,1,4,2,3,1,2,1,2),3)

1    3    2    2
4    1    3    1
1    4    1    2

N<-matrix(c(1,1,2,2,3,4,-2,2,1,4,3,-1),3)

1    2   -2    4
1    3    2    3
2    4    1   -1

I want to find a vector which is a matrix 1*3 and each of its elements is
the multiplication of min element of each row of M by the max element of
the corresponding row of N (for example, the first element of the vector is
the min element of the first row of matrix M, which is 1, multiply by the
max element of the first row of matrix N, which is 4, and so the first
element of the vector is 1*4 which is 4).
The final answer is: (1*4, 1*3,1*4)=(4,3,4)

To find this vector (or matrix) I have written the below code:
c(min(M[1,])*max(N[1,]),min(M[2,])*max(N[2,]),min(M[3,])*max(N[3,]))
But it is so long. could anyone writes a shorter (or simpler, or easier)
code?

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat May 23 16:52:31 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sat, 23 May 2020 15:52:31 +0100
Subject: [R] min rows multiplication by the max rows
In-Reply-To: <CAEPHqhZ72gyYCKXp083ygYSXhZVJxKy-bBKpmJ8gLs3=kJFfGw@mail.gmail.com>
References: <CAEPHqhZ72gyYCKXp083ygYSXhZVJxKy-bBKpmJ8gLs3=kJFfGw@mail.gmail.com>
Message-ID: <11276a28-18f7-86e9-27f9-578c795fb99b@sapo.pt>

Hello,

Use ?apply on each of the matrices.


min_max <- function(X, Y, na.rm = FALSE){
   Min <- apply(X, 1, min, na.rm = na.rm)
   Max <- apply(Y, 1, max, na.rm = na.rm)
   Min*Max
}
min_max(M, N)
#[1] 4 3 4


Hope this helps,

Rui Barradas

?s 15:30 de 23/05/20, Vahid Borji escreveu:
> Hi my R friends,
> 
> I have two matrices as follows:
> 
> M<-matrix(c(1,4,1,3,1,4,2,3,1,2,1,2),3)
> 
> 1    3    2    2
> 4    1    3    1
> 1    4    1    2
> 
> N<-matrix(c(1,1,2,2,3,4,-2,2,1,4,3,-1),3)
> 
> 1    2   -2    4
> 1    3    2    3
> 2    4    1   -1
> 
> I want to find a vector which is a matrix 1*3 and each of its elements is
> the multiplication of min element of each row of M by the max element of
> the corresponding row of N (for example, the first element of the vector is
> the min element of the first row of matrix M, which is 1, multiply by the
> max element of the first row of matrix N, which is 4, and so the first
> element of the vector is 1*4 which is 4).
> The final answer is: (1*4, 1*3,1*4)=(4,3,4)
> 
> To find this vector (or matrix) I have written the below code:
> c(min(M[1,])*max(N[1,]),min(M[2,])*max(N[2,]),min(M[3,])*max(N[3,]))
> But it is so long. could anyone writes a shorter (or simpler, or easier)
> code?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat May 23 16:58:05 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sat, 23 May 2020 15:58:05 +0100
Subject: [R] min rows multiplication by the max rows
In-Reply-To: <11276a28-18f7-86e9-27f9-578c795fb99b@sapo.pt>
References: <CAEPHqhZ72gyYCKXp083ygYSXhZVJxKy-bBKpmJ8gLs3=kJFfGw@mail.gmail.com>
 <11276a28-18f7-86e9-27f9-578c795fb99b@sapo.pt>
Message-ID: <4d66153a-2cd3-bc37-0e32-b9a801388dc1@sapo.pt>

Hello,

I have just found out that you are cross posting [1].
Please do not cross post, ask a question, wait for an answer and after 
or 3 days, if you don't have a (satisfactory) answer, ask somewhere else.

[1] 
https://stackoverflow.com/questions/61973737/min-rows-multiplication-by-the-max-rows


Rui Barradas

?s 15:52 de 23/05/20, Rui Barradas escreveu:
> Hello,
> 
> Use ?apply on each of the matrices.
> 
> 
> min_max <- function(X, Y, na.rm = FALSE){
>  ? Min <- apply(X, 1, min, na.rm = na.rm)
>  ? Max <- apply(Y, 1, max, na.rm = na.rm)
>  ? Min*Max
> }
> min_max(M, N)
> #[1] 4 3 4
> 
> 
> Hope this helps,
> 
> Rui Barradas
> 
> ?s 15:30 de 23/05/20, Vahid Borji escreveu:
>> Hi my R friends,
>>
>> I have two matrices as follows:
>>
>> M<-matrix(c(1,4,1,3,1,4,2,3,1,2,1,2),3)
>>
>> 1??? 3??? 2??? 2
>> 4??? 1??? 3??? 1
>> 1??? 4??? 1??? 2
>>
>> N<-matrix(c(1,1,2,2,3,4,-2,2,1,4,3,-1),3)
>>
>> 1??? 2?? -2??? 4
>> 1??? 3??? 2??? 3
>> 2??? 4??? 1?? -1
>>
>> I want to find a vector which is a matrix 1*3 and each of its elements is
>> the multiplication of min element of each row of M by the max element of
>> the corresponding row of N (for example, the first element of the 
>> vector is
>> the min element of the first row of matrix M, which is 1, multiply by the
>> max element of the first row of matrix N, which is 4, and so the first
>> element of the vector is 1*4 which is 4).
>> The final answer is: (1*4, 1*3,1*4)=(4,3,4)
>>
>> To find this vector (or matrix) I have written the below code:
>> c(min(M[1,])*max(N[1,]),min(M[2,])*max(N[2,]),min(M[3,])*max(N[3,]))
>> But it is so long. could anyone writes a shorter (or simpler, or easier)
>> code?
>>
>> ????[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sat May 23 16:54:08 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 23 May 2020 07:54:08 -0700
Subject: [R] min rows multiplication by the max rows
In-Reply-To: <CAEPHqhZ72gyYCKXp083ygYSXhZVJxKy-bBKpmJ8gLs3=kJFfGw@mail.gmail.com>
References: <CAEPHqhZ72gyYCKXp083ygYSXhZVJxKy-bBKpmJ8gLs3=kJFfGw@mail.gmail.com>
Message-ID: <6396659D-85E4-4857-9F55-949671086AF0@dcn.davis.ca.us>

Please post using plain text format... otherwise what we see can be corrupted as compared to what you saw.

The obvious solution is:

apply( M, 1, min ) * apply( N, 1, max )

but Google sez there is an optimized version useful with large matrices:

library( Rfast )
rowMins( M ) * rowMins( N )

(untested)

On May 23, 2020 7:30:48 AM PDT, Vahid Borji <vahid.borji65 at gmail.com> wrote:
>Hi my R friends,
>
>I have two matrices as follows:
>
>M<-matrix(c(1,4,1,3,1,4,2,3,1,2,1,2),3)
>
>1    3    2    2
>4    1    3    1
>1    4    1    2
>
>N<-matrix(c(1,1,2,2,3,4,-2,2,1,4,3,-1),3)
>
>1    2   -2    4
>1    3    2    3
>2    4    1   -1
>
>I want to find a vector which is a matrix 1*3 and each of its elements
>is
>the multiplication of min element of each row of M by the max element
>of
>the corresponding row of N (for example, the first element of the
>vector is
>the min element of the first row of matrix M, which is 1, multiply by
>the
>max element of the first row of matrix N, which is 4, and so the first
>element of the vector is 1*4 which is 4).
>The final answer is: (1*4, 1*3,1*4)=(4,3,4)
>
>To find this vector (or matrix) I have written the below code:
>c(min(M[1,])*max(N[1,]),min(M[2,])*max(N[2,]),min(M[3,])*max(N[3,]))
>But it is so long. could anyone writes a shorter (or simpler, or
>easier)
>code?
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com  Sat May 23 18:02:58 2020
From: m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com (Patrick (Malone Quantitative))
Date: Sat, 23 May 2020 12:02:58 -0400
Subject: [R] access for free more than 500 essential Springer Nature
 textbooks
In-Reply-To: <CAGx1TMBxYGKQsUY354c8iUdEzYdX6+g2NaBNzzHF4BEGpbLnCA@mail.gmail.com>
References: <CAGx1TMBxYGKQsUY354c8iUdEzYdX6+g2NaBNzzHF4BEGpbLnCA@mail.gmail.com>
Message-ID: <CAJc=yOG-x58NojL1PCtu=2Pe216mpudDeVbXktnLehkMX1wSjw@mail.gmail.com>

Thanks, Rich! I found several books to peruse.

On Fri, May 22, 2020 at 12:29 PM Richard M. Heiberger <rmh at temple.edu>
wrote:

> Springer has just made available free access to many books through July.
> This is part of their global program to support educators, students
> and academics
> affected by coronavirus lockdown.
>
> Their list includes about 20 statistics books in English and 2 in
> German.  Several, including mine, have R in the title or subtitle.
>
> This link describes the program:
>
> https://www.springernature.com/gp/librarians/news-events/all-news-articles/industry-news-initiatives/free-access-to-textbooks-for-institutions-affected-by-coronaviru/17855960?sap-outbound-id=07923935E132AFCC90201BAEA7D6755EC6C597DE&utm_source=hybris-campaign&utm_medium=email&utm_campaign=000_BARZ01_0000001531_AEXS_AWA_CB02_GL_txt_covid&utm_content=EN_internal_5917_20200522&mkt-key=42010A0550671EDA9BA73AC34F576EF6
>
> My book is
> Statistical Analysis and Data Display, Richard M. Heiberger, Burt
> Holland, 2nd ed. 2015
> It is supported by the HH package available from CRAN.
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Patrick S. Malone, Ph.D., Malone Quantitative
NEW Service Models: http://malonequantitative.com

He/Him/His

	[[alternative HTML version deleted]]


From jrkr|de@u @end|ng |rom gm@||@com  Sat May 23 18:17:20 2020
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Sat, 23 May 2020 12:17:20 -0400
Subject: [R] Wordcloud warnings
In-Reply-To: <AAA8134F-B55F-4D42-ABC5-4278D77DFB99@gmail.com>
References: <AAA8134F-B55F-4D42-ABC5-4278D77DFB99@gmail.com>
Message-ID: <CAKZQJMAVs4NCBtCGS76QcOqdm5w13nV8MNQpA0XRhX3ws-eu0Q@mail.gmail.com>

Hi Brian,
i think we may need a bit more information. It looks like you are doing
some text mining; can you tell us what libraries you have loaded and
perhaps provide us with some sample data?
And an example of what you are doing?

See the links below for some suggestions

 http://adv-r.had.co.nz/Reproducibility.html

http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example




On Sat, 23 May 2020 at 02:53, Bryan Mac <bryan.mac24 at gmail.com> wrote:

> Hi,
>
> I am having trouble understanding why I am getting this warning and how to
> fix it. I don?t have any of these functions in my script..
>
> This is the warning:
>
> warning messages:
> 1: in tm_map.simplecorpus(corpus, tm::removepunctuation) : transformation
> drops documents
> 2: in tm_map.simplecorpus(corpus, function(x) tm::removewords(x,
> tm::stopwords())) : transformation drops documents
> 3: in tm_map.simplecorpus(corpus, tm::removepunctuation) : transformation
> drops documents
> 4: in tm_map.simplecorpus(corpus, function(x) tm::removewords(x,
> tm::stopwords())) : transformation drops documents
> 5: in tm_map.simplecorpus(corpus, tm::removepunctuation) : transformation
> drops documents
> 6: in tm_map.simplecorpus(corpus, function(x) tm::removewords(x,
> tm::stopwords())) : transformation drops documents
>
>
> This is the script:
>
>      if (runtype=="native") {
>         words <- Corpus(VectorSource(test123b[,2]))
>         wordcloud(words, min.freq = minfreq, max.words=300, random.order =
> FALSE ,colors=brewer.pal(9,"Reds")[0:-3])
>
>       }   else {
>         wordcloud((as.character(test123[,2])), min.freq = minfreq,
> max.words=300, random.order = FALSE ,colors=brewer.pal(9,"Reds")[0:-3])
>
>       }
>
>
> Best,
> Bryan
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
John Kane
Kingston ON Canada

	[[alternative HTML version deleted]]


From pd@|gd @end|ng |rom gm@||@com  Sat May 23 20:13:27 2020
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Sat, 23 May 2020 20:13:27 +0200
Subject: [R] 
 [External Email] Re: [External] Re: access for free more than
 500 essential Springer Nature textbooks
In-Reply-To: <986aec24-6c46-d025-8aa0-2c151c1001a8@binghamton.edu>
References: <CAGx1TMBxYGKQsUY354c8iUdEzYdX6+g2NaBNzzHF4BEGpbLnCA@mail.gmail.com>
 <CAB8pepzHks4aMWWHqzymKks7wmr1biU6Uk18DorDrVaOz=r35A@mail.gmail.com>
 <CAGx1TMDTRP0pSC+O7906u6kdYpwk+KSS2=5tGyCXFoHp=eQi2g@mail.gmail.com>
 <CAB8pepxYn80zZGiZbVRS6iPDaj68ei9jj=OVocOJO3QkkmVQmw@mail.gmail.com>
 <7C1BF9C3-1AB3-4A77-AA8F-86DB30DF16E1@dcn.davis.ca.us>
 <CABcYAdJzC+PZRnijsRQ97FK91mDN9YHyZwy5P+s4DRtD1n1N3g@mail.gmail.com>
 <CAHz+bWbwRpPksaLgyEe-bkFKbRwsUiS7rBuOAdUkvwyMWNgGFQ@mail.gmail.com>
 <986aec24-6c46-d025-8aa0-2c151c1001a8@binghamton.edu>
Message-ID: <D6F145C8-9ED4-452A-9A05-612D091D6077@gmail.com>

Some of them are generally available. At least they look that way from here....

E.g., (shameless plug...)

https://link.springer.com/book/10.1007/978-0-387-79054-1

-pd


> On 23 May 2020, at 06:15 , Christopher W. Ryan <cryan at binghamton.edu> wrote:
> 
> Am I interpreting this offer correctly, that it is for libraries to
> obtain access to the e-books for free? It does not seem to me that an
> invididual can download one--am I missing that part?
> 
> Thanks
> 
> --Chris Ryan
> 
> Mark Leeds wrote:
>> Abby: here's an easier link for seeing what you might like.
>> 
>> https://link.springer.com/search?facet-content-type=%22Book%22&package=mat-covid19_textbooks&%23038;facet-language=%22En%22&%23038;sortOrder=newestFirst&%23038;showAll=true
>> 
>> On Fri, May 22, 2020 at 9:18 PM Richard O'Keefe <raoknz at gmail.com> wrote:
>> 
>>> the real pleasure comes from things you weren't looking for but recognise
>>> as just what you needed.
>>> 
>>> On Sat, 23 May 2020 at 12:34, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>>> wrote:
>>> 
>>>> You are bound to be disappointed if you invert the purpose of the list.
>>>> This is marketing... think of it as a sale... stores rarely put their
>>>> entire stock on sale... particularly if the sale price is zero. You have
>>> to
>>>> start with the list and look for interesting titles.
>>>> 
>>>> But don't let me dissuade you from adding to your killfile if that seems
>>>> more useful to you.
>>>> 
>>>> On May 22, 2020 5:04:29 PM PDT, Abby Spurdle <spurdle.a at gmail.com>
>>> wrote:
>>>>>> The Excel file is what you need.
>>>>> 
>>>>> Well, now I'm in a bad mood.
>>>>> 
>>>>> I went to all the trouble of opening the thing...
>>>>> And the first two Springer-published books I look for, aren't there.
>>>>> 
>>>>> (1) Programming with Data, John Chambers
>>>>> (2) Applied Econometrics with R, Z and co.
>>>>> 
>>>>> Next time someone tells me to use an Excel document, I'm adding them
>>>>> to the spam list.
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> --
>>>> Sent from my phone. Please excuse my brevity.
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>> 
>>>        [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From Bry@n@M@c @end|ng |rom |p@o@@com  Fri May 22 23:29:29 2020
From: Bry@n@M@c @end|ng |rom |p@o@@com (Bryan Mac)
Date: Fri, 22 May 2020 21:29:29 +0000
Subject: [R] Wordcloud errors
Message-ID: <AC656F97-A7F6-47CE-80B5-FAACA93E4747@ipsos.com>

Hi all,

I am trying to create word clouds using this script but I am getting warnings.

Script:

      if (runtype=="native") {
        words <- Corpus(VectorSource(test123b[,2]))
        wordcloud(words, min.freq = minfreq, max.words=300, random.order = FALSE ,colors=brewer.pal(9,"Reds")[0:-3])

      }   else {
        wordcloud((as.character(test123[,2])), min.freq = minfreq, max.words=300, random.order = FALSE ,colors=brewer.pal(9,"Reds")[0:-3])

warning messages:
1: in tm_map.simplecorpus(corpus, tm::removepunctuation) : transformation drops documents
2: in tm_map.simplecorpus(corpus, function(x) tm::removewords(x, tm::stopwords())) : transformation drops documents
3: in tm_map.simplecorpus(corpus, tm::removepunctuation) : transformation drops documents
4: in tm_map.simplecorpus(corpus, function(x) tm::removewords(x, tm::stopwords())) : transformation drops documents
5: in tm_map.simplecorpus(corpus, tm::removepunctuation) : transformation drops documents
6: in tm_map.simplecorpus(corpus, function(x) tm::removewords(x, tm::stopwords())) : transformation drops documents


t still produces an output but the warning bugs me. R-help, is there a way to fix this script so that there are no warnings??


Thanks,
Bryan

	[[alternative HTML version deleted]]


From wjm1 @end|ng |rom c@@@co|umb|@@edu  Sun May 24 04:34:42 2020
From: wjm1 @end|ng |rom c@@@co|umb|@@edu (William Michels)
Date: Sat, 23 May 2020 19:34:42 -0700
Subject: [R] iterators : checkFunc with ireadLines
In-Reply-To: <27f44720-d368-c2cc-39c9-8629da885165@free.fr>
References: <72174dd3-a819-b5fd-5a6f-2a6b46342774-2015@free.fr>
 <20200520104614.45876937@Tarkus>
 <27f44720-d368-c2cc-39c9-8629da885165@free.fr>
Message-ID: <CAA99HCwONVzHizUuG17DO=+PF7SVVY8W9V+SKhpT5L=MW6_1NQ@mail.gmail.com>

Hi Laurent,

Seeking to give you an "R-only" solution, I thought the read.fwf()
function might be useful (to read-in your first column of data, only).
However Jeff is correct that this is a poor strategy, since read.fwf()
reads the entire file into R (documented in "Fixed-width-format
files", Section 2.2: R Data Import/Export Manual).

Jeff has suggested a number of packages, as well as using a database.
Ivan Krylov has posted answers using grep, awk and perl (perl5--to
disambiguate). [In point of fact, the R Data Import/Export Manual
suggests using perl]. Similar to Ivan, I've posted code below using
the Raku programming language (the language formerly known as Perl6).
Regexes are claimed to be more readable, but are currently very slow
in Raku. However on the plus side, the language is designed to handle
Unicode gracefully:

> # pipe() using raku-grep on Laurent's data (sep=mult whitespace):
> con_obj1 <- pipe(paste("raku -e '.put for lines.grep( / ^^N053 | ^^N163 /, :p );' ", "Laurents.txt"), open="rt");
> p6_import_a <- scan(file=con_obj1, what=list("","","","","","","","","",""), flush=TRUE, multi.line=FALSE, quiet=TRUE);
> close(con_obj1);
> as.data.frame(sapply(p6_import_a, t), stringsAsFactors=FALSE);
  V1   V2        V3        V4        V5        V6        V7        V8
      V9       V10
1  2 N053 -0.014083 -0.004741  0.001443 -0.010152 -0.012996 -0.005337
-0.008738 -0.015094
2  4 N163 -0.054023 -0.049345 -0.037158  -0.04112 -0.044612 -0.036953
-0.036061 -0.044516
>
> # pipe() using raku-grep "starts-with" to find genbankID ( >3GB TSV file)
> # "lines[0..5]" restricts raku to reading first 6 lines!
> # change "lines[0..5]" to "lines" to run raku code on whole file:
> con_obj2 <- pipe(paste("raku -e '.put for lines[0..5].grep( *.starts-with(q[A00145]), :p);' ", "genbankIDs_3GB.tsv"), "rt");
> p6_import_b <- read.table(con_obj2, sep="\t");
> close(con_obj2)
> p6_import_b
  V1     V2       V3          V4 V5
1  4 A00145 A00145.1 IFN-alpha A NA
>
> # unicode test using R's system() function:
> try(system("raku -ne '.grep( /  ??  |  ?????  |  ?????  |  ??????  /, :v ).put;'  hello_7lang.txt", intern = TRUE, ignore.stderr = FALSE))
[1] ""                    ""                    ""
"?? Chinese"
[5] "????? Japanese" "????? Arabic"        "?????? Russian"
>

[special thanks to Brad Gilbert, Joseph Brenner and others on the
perl6-users mailing list. All errors above are my own.]

HTH, Bill.

W. Michels, Ph.D.




On Fri, May 22, 2020 at 4:48 AM Laurent Rhelp <LaurentRHelp at free.fr> wrote:
>
> Hi Ivan,
>    Endeed, it is a good idea. I am under MSwindows but I can use the
> bash command I use with git. I will see how to do that with the unix
> command lines.
>
>
> Le 20/05/2020 ? 09:46, Ivan Krylov a ?crit :
> > Hi Laurent,
> >
> > I am not saying this will work every time and I do recognise that this
> > is very different from a more general solution that you had envisioned,
> > but if you are on an UNIX-like system or have the relevant utilities
> > installed and on the %PATH% on Windows, you can filter the input file
> > line-by-line using a pipe and an external program:
> >
> > On Sun, 17 May 2020 15:52:30 +0200
> > Laurent Rhelp <LaurentRHelp at free.fr> wrote:
> >
> >> # sensors to keep
> >> sensors <-  c("N053", "N163")
> > # filter on the beginning of the line
> > i <- pipe("grep -E '^(N053|N163)' test.txt")
> > # or:
> > # filter on the beginning of the given column
> > # (use $2 for the second column, etc.)
> > i <- pipe("awk '($1 ~ \"^(N053|N163)\")' test.txt")
> > # or:
> > # since your message is full of Unicode non-breaking spaces, I have to
> > # bring in heavier machinery to handle those correctly;
> > # only this solution manages to match full column values
> > # (here you can also use $F[1] for second column and so on)
> > i <- pipe("perl -CSD -F'\\s+' -lE \\
> >   'print join qq{\\t}, @F if $F[0] =~ /^(N053|N163)$/' \\
> >   test.txt
> > ")
> > lines <- read.table(i) # closes i when done
> >
> > The downside of this approach is having to shell-escape the command
> > lines, which can become complicated, and choosing between use of regular
> > expressions and more wordy programs (Unicode whitespace in the input
> > doesn't help, either).
> >
>
>
> --
> L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e par le logiciel antivirus Avast.
> https://www.avast.com/antivirus
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pd@me@ @end|ng |rom cb@@dk  Sun May 24 12:17:10 2020
From: pd@me@ @end|ng |rom cb@@dk (Peter Dalgaard)
Date: Sun, 24 May 2020 10:17:10 +0000
Subject: [R] [Rd] R 4.0.1 scheduled for June 6
Message-ID: <1E4C01BD-8D42-4398-B893-B5C7547873EA@cbs.dk>

Full schedule is available on developer.r-project.org.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel

_______________________________________________
R-announce at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From wjm1 @end|ng |rom c@@@co|umb|@@edu  Sun May 24 20:09:04 2020
From: wjm1 @end|ng |rom c@@@co|umb|@@edu (William Michels)
Date: Sun, 24 May 2020 11:09:04 -0700
Subject: [R] iterators : checkFunc with ireadLines
In-Reply-To: <CAA99HCwONVzHizUuG17DO=+PF7SVVY8W9V+SKhpT5L=MW6_1NQ@mail.gmail.com>
References: <72174dd3-a819-b5fd-5a6f-2a6b46342774-2015@free.fr>
 <20200520104614.45876937@Tarkus>
 <27f44720-d368-c2cc-39c9-8629da885165@free.fr>
 <CAA99HCwONVzHizUuG17DO=+PF7SVVY8W9V+SKhpT5L=MW6_1NQ@mail.gmail.com>
Message-ID: <CAA99HCzEfOw-6+RYKFyrWhTHjqpnoqND7i5BtW_ueBqMPy37jQ@mail.gmail.com>

Strike that one sentence in brackets: "[In point of fact, the R Data
Import/Export Manual suggests using perl]", to pre-process data before
loading into R. The manual's recommendation only pertains to large
fixed width formatted files [see #1], whereas Laurent's data is
whitespace-delimited:

> read.table( "Laurents.txt")
> read.delim( "Laurents.txt", sep="")

Best Regards, Bill.

W. Michels, Ph.D.

Citation:
[#1] https://cran.r-project.org/doc/manuals/r-release/R-data.html#Fixed_002dwidth_002dformat-files


From 538280 @end|ng |rom gm@||@com  Mon May 25 02:28:19 2020
From: 538280 @end|ng |rom gm@||@com (Greg Snow)
Date: Sun, 24 May 2020 18:28:19 -0600
Subject: [R] Bayesian estimation with MCMC
In-Reply-To: <CA+dpOJn=_DgcSxXMfMQ8KaLJecvzr2-JJdsBCm1TFv0z+kwpNQ@mail.gmail.com>
References: <CA+dpOJn=_DgcSxXMfMQ8KaLJecvzr2-JJdsBCm1TFv0z+kwpNQ@mail.gmail.com>
Message-ID: <CAFEqCdzR7QKmAAmJ-c2wW6yLO3quzC5W1Z3=B0EQum2sSTa+YA@mail.gmail.com>

On CRAN there are Task Views which are moderated lists and summaries
of packages available for specific topics.  The first one
(alphabetically) is the Bayesian task view:
https://cran.r-project.org/web/views/Bayesian.html which lists many
packages for doing/learning Bayesian statistics using R.  There are
descriptions, categories, etc.  Looking through that list you will see
many packages that do McMC for Bayesian inference.

On Sat, May 23, 2020 at 3:29 AM Christofer Bogaso
<bogaso.christofer at gmail.com> wrote:
>
> Hi,
>
> In python there is a package called pymc3 for Bayesian parameter
> estimation with MCMC.
>
> I am curious if there is any equivalent package available for R.
>
> Any pointer will be highly appreciated.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From 538280 @end|ng |rom gm@||@com  Mon May 25 02:38:23 2020
From: 538280 @end|ng |rom gm@||@com (Greg Snow)
Date: Sun, 24 May 2020 18:38:23 -0600
Subject: [R] How to make a vector with string elements without space
In-Reply-To: <CAEPHqhbViKqSNJQyT9KPhcEE6uq3p2UUTaJ_QcnWYQKcyrbo0w@mail.gmail.com>
References: <CAEPHqhbViKqSNJQyT9KPhcEE6uq3p2UUTaJ_QcnWYQKcyrbo0w@mail.gmail.com>
Message-ID: <CAFEqCdzaU6X7JO=6-RubVMg2ZWk1e91rzpnpY==QHDcY5dKGfw@mail.gmail.com>

As Borris mentioned, paste0 works well for this.

Another option is the sprintf function:

sprintf("c%i", 1:10)

For this example they do the same thing, but as things become more
complicated sometimes you will want paste0 and sometimes sprintf will
be better.

Compare the above to
sprintf("c%02i", 1:10)

Also, you do not say why you want to do this, but one possible reason
is that you have a set of variables in the global workspace named "c1"
through "c10" that you want to loop through.  If this is the case,
then there are better ways.  You can put the variables into a list,
then use lapply, sapply, or the purrr package to loop through them in
much  better ways.

On Sat, May 23, 2020 at 6:26 AM Vahid Borji <vahid.borji65 at gmail.com> wrote:
>
> Hello my r friends,
> I want to make a vector with elements (c1,c2,...,c10).
> I wrote the below code:
> c(paste("c",1:10))
> My code works but it gives me elements like "c 1", "c 2" to "c 10". I mean
> there is a space between each c and its corresponding number. I want the
> elements of the vector to be like "c1", "c2", to "c10"  (without space
> between c and number). How can I fix this problem in my code?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From J@m|e@Burge@@ @end|ng |rom ||verpoo|@@c@uk  Mon May 25 14:26:46 2020
From: J@m|e@Burge@@ @end|ng |rom ||verpoo|@@c@uk (Burgess, Jamie)
Date: Mon, 25 May 2020 12:26:46 +0000
Subject: [R] Help with sub-setting
Message-ID: <61fc9b2a44374e178634f6a23f58c455@liverpool.ac.uk>

Dear all,

I hope this message finds you well. I am currently trying to subset my data by two variables, so far, I have tried two different ways to stratify participants into groups. I would like to use the ?summary? and ?table? arguments to characterise the data of participants based on the presence of two variables and summarise this sub-set against a third variable.
I have used this method:

dgb001<-subset(data,data$variable==1 & data,data$variable)


However, I get the following error: ?Error: cannot allocate vector of size 16.0 Gb?. Is there another method I can try?


Kind regards,


Jamie Burgess

PhD Student Endocrinology and Diabetes

University of Liverpool

Aintree University Hospital &

The Walton Centre

Institute of Ageing & Chronic Disease

0151 529 5936


	[[alternative HTML version deleted]]


From rmh @end|ng |rom temp|e@edu  Mon May 25 19:19:45 2020
From: rmh @end|ng |rom temp|e@edu (Richard M. Heiberger)
Date: Mon, 25 May 2020 13:19:45 -0400
Subject: [R] [External]  Help with sub-setting
In-Reply-To: <61fc9b2a44374e178634f6a23f58c455@liverpool.ac.uk>
References: <61fc9b2a44374e178634f6a23f58c455@liverpool.ac.uk>
Message-ID: <CAGx1TMABeGaoMP2tHcgx-kbQnRN0dZW-0a8_MF5kK68g7YqKNw@mail.gmail.com>

I think the syntax you are looking for is

datasubset <- data[ data$A ==1 & data$B ==  1 , ] )

This gives the subset of your original data for variable A with value
1 and variable B with value 1.


On Mon, May 25, 2020 at 12:57 PM Burgess, Jamie
<Jamie.Burgess at liverpool.ac.uk> wrote:
>
> Dear all,
>
> I hope this message finds you well. I am currently trying to subset my data by two variables, so far, I have tried two different ways to stratify participants into groups. I would like to use the ?summary? and ?table? arguments to characterise the data of participants based on the presence of two variables and summarise this sub-set against a third variable.
> I have used this method:
>
> dgb001<-subset(data,data$variable==1 & data,data$variable)
>
>
> However, I get the following error: ?Error: cannot allocate vector of size 16.0 Gb?. Is there another method I can try?
>
>
> Kind regards,
>
>
> Jamie Burgess
>
> PhD Student Endocrinology and Diabetes
>
> University of Liverpool
>
> Aintree University Hospital &
>
> The Walton Centre
>
> Institute of Ageing & Chronic Disease
>
> 0151 529 5936
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Mon May 25 19:36:18 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 25 May 2020 10:36:18 -0700
Subject: [R] [External] Help with sub-setting
In-Reply-To: <CAGx1TMABeGaoMP2tHcgx-kbQnRN0dZW-0a8_MF5kK68g7YqKNw@mail.gmail.com>
References: <61fc9b2a44374e178634f6a23f58c455@liverpool.ac.uk>
 <CAGx1TMABeGaoMP2tHcgx-kbQnRN0dZW-0a8_MF5kK68g7YqKNw@mail.gmail.com>
Message-ID: <CAGxFJbTkzkqAciyhjb6R6+u+JQ2pUyMu2npfx8BHAHiXhvzPhA@mail.gmail.com>

Yes. In particular:

data$variable==1 & data

makes no sense (data is a data frame). A typo perhaps? Or as Richard
indicated, consult references/tutorials to learn proper syntax for
(vectorized) predicates.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, May 25, 2020 at 10:20 AM Richard M. Heiberger <rmh at temple.edu>
wrote:

> I think the syntax you are looking for is
>
> datasubset <- data[ data$A ==1 & data$B ==  1 , ] )
>
> This gives the subset of your original data for variable A with value
> 1 and variable B with value 1.
>
>
> On Mon, May 25, 2020 at 12:57 PM Burgess, Jamie
> <Jamie.Burgess at liverpool.ac.uk> wrote:
> >
> > Dear all,
> >
> > I hope this message finds you well. I am currently trying to subset my
> data by two variables, so far, I have tried two different ways to stratify
> participants into groups. I would like to use the ?summary? and ?table?
> arguments to characterise the data of participants based on the presence of
> two variables and summarise this sub-set against a third variable.
> > I have used this method:
> >
> > dgb001<-subset(data,data$variable==1 & data,data$variable)
> >
> >
> > However, I get the following error: ?Error: cannot allocate vector of
> size 16.0 Gb?. Is there another method I can try?
> >
> >
> > Kind regards,
> >
> >
> > Jamie Burgess
> >
> > PhD Student Endocrinology and Diabetes
> >
> > University of Liverpool
> >
> > Aintree University Hospital &
> >
> > The Walton Centre
> >
> > Institute of Ageing & Chronic Disease
> >
> > 0151 529 5936
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Mon May 25 20:27:27 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Mon, 25 May 2020 19:27:27 +0100
Subject: [R] Help with sub-setting
In-Reply-To: <61fc9b2a44374e178634f6a23f58c455@liverpool.ac.uk>
References: <61fc9b2a44374e178634f6a23f58c455@liverpool.ac.uk>
Message-ID: <bc2da920-9107-dfda-c91c-a988a3e66060@sapo.pt>

Hello,

Inline.

?s 13:26 de 25/05/20, Burgess, Jamie escreveu:
> Dear all,
> 
> I hope this message finds you well. I am currently trying to subset my data by two variables, so far, I have tried two different ways to stratify participants into groups. 

I don't understand what you mean by this, do you want to split the data 
set into sub-dataframes by 2 variables? If so try

df_groups <- split(data, list(data$Var1, data$Var2), drop = TRUE)


This produces a list of sub-df's.
To get the group with Var1 == 1 and Var2 == 1

grp_name <- paste(1, 1, sep = '.')
df_groups[[grp_name]]


But if you only want the sub-df with Var1 == 1 and Var2 == 1, any of the 
following will do it.

data[data$Var1 == 1 & data$Var2 == 1, ]

subset(data, Var1 == 1 & Var2 == 1)


Hope this helps,

Rui Barradas


I would like to use the ?summary? and ?table? arguments to characterise 
the data of participants based on the presence of two variables and 
summarise this sub-set against a third variable.
> I have used this method:
> 
> dgb001<-subset(data,data$variable==1 & data,data$variable)
> 
> 
> However, I get the following error: ?Error: cannot allocate vector of size 16.0 Gb?. Is there another method I can try?
> 
> 
> Kind regards,
> 
> 
> Jamie Burgess
> 
> PhD Student Endocrinology and Diabetes
> 
> University of Liverpool
> 
> Aintree University Hospital &
> 
> The Walton Centre
> 
> Institute of Ageing & Chronic Disease
> 
> 0151 529 5936
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From drj|m|emon @end|ng |rom gm@||@com  Tue May 26 01:00:54 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Tue, 26 May 2020 09:00:54 +1000
Subject: [R] Help with sub-setting
In-Reply-To: <61fc9b2a44374e178634f6a23f58c455@liverpool.ac.uk>
References: <61fc9b2a44374e178634f6a23f58c455@liverpool.ac.uk>
Message-ID: <CA+8X3fX7B1gJi6YeLa=MrvBbYbHGZb6Rqsu8zkXtxs8hmf7rTg@mail.gmail.com>

Hi Jamie,
Your seem to want some descriptive statistic applied to subsets of
your data frame "data" (maybe a more imaginative name would help).
I'll guess that your data frame contains variables X, Y and Z among
others. Further, I'll guess that you want the summaries of variable Z
subset by Y and X.

data<-data.frame(X=sample(1:2,100,TRUE),Y=sample(1:2,100,TRUE),
 Z=rnorm(100))
by(data,data[,c("X","Y")],summary)

Jim

On Tue, May 26, 2020 at 2:57 AM Burgess, Jamie
<Jamie.Burgess at liverpool.ac.uk> wrote:
>
> Dear all,
>
> I hope this message finds you well. I am currently trying to subset my data by two variables, so far, I have tried two different ways to stratify participants into groups. I would like to use the ?summary? and ?table? arguments to characterise the data of participants based on the presence of two variables and summarise this sub-set against a third variable.
> I have used this method:
>
> dgb001<-subset(data,data$variable==1 & data,data$variable)
>
>
> However, I get the following error: ?Error: cannot allocate vector of size 16.0 Gb?. Is there another method I can try?
>
>
> Kind regards,
>
>
> Jamie Burgess
>
> PhD Student Endocrinology and Diabetes
>
> University of Liverpool
>
> Aintree University Hospital &
>
> The Walton Centre
>
> Institute of Ageing & Chronic Disease
>
> 0151 529 5936
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drj|m|emon @end|ng |rom gm@||@com  Tue May 26 01:13:26 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Tue, 26 May 2020 09:13:26 +1000
Subject: [R] Help with sub-setting
In-Reply-To: <CA+8X3fX7B1gJi6YeLa=MrvBbYbHGZb6Rqsu8zkXtxs8hmf7rTg@mail.gmail.com>
References: <61fc9b2a44374e178634f6a23f58c455@liverpool.ac.uk>
 <CA+8X3fX7B1gJi6YeLa=MrvBbYbHGZb6Rqsu8zkXtxs8hmf7rTg@mail.gmail.com>
Message-ID: <CA+8X3fWL_tco6RzuC-7NOzu+2ZKOnjFwEj9O8UueLkudYWbWhA@mail.gmail.com>

oops, that should have been:

by(data$Z,data[,c("X","Y")],summary)

Jim

On Tue, May 26, 2020 at 9:00 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi Jamie,
> Your seem to want some descriptive statistic applied to subsets of
> your data frame "data" (maybe a more imaginative name would help).
> I'll guess that your data frame contains variables X, Y and Z among
> others. Further, I'll guess that you want the summaries of variable Z
> subset by Y and X.
>
> data<-data.frame(X=sample(1:2,100,TRUE),Y=sample(1:2,100,TRUE),
>  Z=rnorm(100))
> by(data,data[,c("X","Y")],summary)
>
> Jim
>
> On Tue, May 26, 2020 at 2:57 AM Burgess, Jamie
> <Jamie.Burgess at liverpool.ac.uk> wrote:
> >
> > Dear all,
> >
> > I hope this message finds you well. I am currently trying to subset my data by two variables, so far, I have tried two different ways to stratify participants into groups. I would like to use the ?summary? and ?table? arguments to characterise the data of participants based on the presence of two variables and summarise this sub-set against a third variable.
> > I have used this method:
> >
> > dgb001<-subset(data,data$variable==1 & data,data$variable)
> >
> >
> > However, I get the following error: ?Error: cannot allocate vector of size 16.0 Gb?. Is there another method I can try?
> >
> >
> > Kind regards,
> >
> >
> > Jamie Burgess
> >
> > PhD Student Endocrinology and Diabetes
> >
> > University of Liverpool
> >
> > Aintree University Hospital &
> >
> > The Walton Centre
> >
> > Institute of Ageing & Chronic Disease
> >
> > 0151 529 5936
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Tue May 26 11:33:35 2020
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Tue, 26 May 2020 11:33:35 +0200
Subject: [R] Manage time frame package deSolve
Message-ID: <CAMk+s2T_LtvwpgzkELP-ArDtL7aXm8csyTXZJSCmwo7MB5Ch-g@mail.gmail.com>

Hello,
I have set a model with deSolve's ode. I have set the time as `period
= seq(1, 10, 1)` but the times I get from the ode are like 1,
1.000173...
Is it possible to run ode with the time points provided by `period`?


-- 
Best regards,
Luigi


From J@m|e@Burge@@ @end|ng |rom ||verpoo|@@c@uk  Tue May 26 14:41:18 2020
From: J@m|e@Burge@@ @end|ng |rom ||verpoo|@@c@uk (Burgess, Jamie)
Date: Tue, 26 May 2020 12:41:18 +0000
Subject: [R] [External] Help with sub-setting
In-Reply-To: <CAGxFJbTkzkqAciyhjb6R6+u+JQ2pUyMu2npfx8BHAHiXhvzPhA@mail.gmail.com>
References: <61fc9b2a44374e178634f6a23f58c455@liverpool.ac.uk>
 <CAGx1TMABeGaoMP2tHcgx-kbQnRN0dZW-0a8_MF5kK68g7YqKNw@mail.gmail.com>,
 <CAGxFJbTkzkqAciyhjb6R6+u+JQ2pUyMu2npfx8BHAHiXhvzPhA@mail.gmail.com>
Message-ID: <4712366358ab4d8e8f9d92bb16efced0@liverpool.ac.uk>

Dear all,


Apologies for the late reply - I have just got back from my shift. I am unfortunately a little sleep deprived hehe


Hi Bert,

Thank-you for your reply


Yes, apologies - the syntax was lost in translation whilst changing the names of the groups, imported data-set file name and variables.


data<-data.frame(X=sample(1:2,100,TRUE),Y=sample(1:2,100,TRUE),
>  Z=rnorm(100))

by(data$Z,data[,c("X","Y")],summary)


In your example, if one of my variables recorded integer data and the other continuous data, does "1:2" specify columns, and "100" the number of entries I would like to select?


Dear Richard,


Thank-you for your reply


I have had previous success sub-grouping by one variable using the following:


group1<-subset(dataset1,dataset1$A==1)


I have subsequently been summarising the data using:


table(group1$variable) or summarise(group1$variable)


Using your suggestion I have managed to sub-group using the following:


GroupAB<-subset(data,data$A==1 & is.na (data$B)==FALSE)




I will also try your suggestion datasubset <- data[data$A ==1 & data$B ==  1 ,]) it is much appreciated. Does my entry do the same thing as yours?


I thought the problem was to do with the size of my data-set (4.9GB) and the presence of ~500,000 entries. However, as another command worked I am unsure what the problems was

I am only actually interested in around one third of these which is the reason I wish to sub-group by the two variables I have selected.

I was wondering why this new script worked.



Kind regards,


Jamie



________________________________
From: Bert Gunter <bgunter.4567 at gmail.com>
Sent: 25 May 2020 18:36:18
To: Richard M. Heiberger
Cc: Burgess, Jamie; r-help at r-project.org
Subject: Re: [R] [External] Help with sub-setting

Yes. In particular:

data$variable==1 & data

makes no sense (data is a data frame). A typo perhaps? Or as Richard indicated, consult references/tutorials to learn proper syntax for (vectorized) predicates.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, May 25, 2020 at 10:20 AM Richard M. Heiberger <rmh at temple.edu<mailto:rmh at temple.edu>> wrote:
I think the syntax you are looking for is

datasubset <- data[ data$A ==1 & data$B ==  1 , ] )

This gives the subset of your original data for variable A with value
1 and variable B with value 1.


On Mon, May 25, 2020 at 12:57 PM Burgess, Jamie
<Jamie.Burgess at liverpool.ac.uk<mailto:Jamie.Burgess at liverpool.ac.uk>> wrote:
>
> Dear all,
>
> I hope this message finds you well. I am currently trying to subset my data by two variables, so far, I have tried two different ways to stratify participants into groups. I would like to use the ?summary? and ?table? arguments to characterise the data of participants based on the presence of two variables and summarise this sub-set against a third variable.
> I have used this method:
>
> dgb001<-subset(data,data$variable==1 & data,data$variable)
>
>
> However, I get the following error: ?Error: cannot allocate vector of size 16.0 Gb?. Is there another method I can try?
>
>
> Kind regards,
>
>
> Jamie Burgess
>
> PhD Student Endocrinology and Diabetes
>
> University of Liverpool
>
> Aintree University Hospital &
>
> The Walton Centre
>
> Institute of Ageing & Chronic Disease
>
> 0151 529 5936
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From john@m@h@rro|d @end|ng |rom gm@||@com  Tue May 26 17:59:03 2020
From: john@m@h@rro|d @end|ng |rom gm@||@com (John Harrold)
Date: Tue, 26 May 2020 08:59:03 -0700
Subject: [R] Partial matching list elements in R 4.0
Message-ID: <CANAiAiXk++sowVnKf_VRL4-YtC_3dNNknqO+jjVzeOjU7udCKw@mail.gmail.com>

Hello,


I'm testing some code in R 4.0, and I'm having an issue with the following"

# -------------
rm(list=ls())
graphics.off()
#load("/tmp/post.RData")
var = list();
# If I uncomment this it fixes things:
# var$options = list(mi   = list(),
#                    misc = list())
#
var$options$misc$abc = "123"
var$options$mi$something    = 13
#------------

This is a stripped down example but it exhibits the issue I"m having.
Basically when I create the list element var$options$mi the contents of
var$options$misc move over to var$options$mi. And what was in
var$options$misc become NULL:

So now var$options looks like:

var$options
$misc
$misc$abc
NULL

$mi
$mi$abc
[1] "123"
$mi$something
[1] 13

This worked (still works) in R 3.5.1. I understand partial matching, but is
this normal lists moving over to elements like this? I can uncomment the
text mentioned in the example and it seems to fix it, but I'm wondering if
this is a bug or just my poor programming coming back to bite me.

I've included my sessionInfo() at the bottom.

Thanks
John
:wq


sessionInfo()

R version 4.0.0 (2020-04-24)

Platform: x86_64-apple-darwin17.0 (64-bit)

Running under: macOS Mojave 10.14.5


Matrix products: default

BLAS:
/Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib

LAPACK:
/Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib


locale:

[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8


attached base packages:

[1] stats     graphics  grDevices utils     datasets  methods   base


other attached packages:

[1] gdata_2.18.0  ggplot2_3.3.0 deSolve_1.28


loaded via a namespace (and not attached):

 [1] Rcpp_1.0.4.6     gtools_3.8.2     withr_2.2.0      assertthat_0.2.1

 [5] dplyr_0.8.5      digest_0.6.25    crayon_1.3.4     grid_4.0.0

 [9] R6_2.4.1         lifecycle_0.2.0  gtable_0.3.0     magrittr_1.5

[13] scales_1.1.1     pillar_1.4.4     rlang_0.4.6      vctrs_0.3.0

[17] ellipsis_0.3.1   glue_1.4.1       purrr_0.3.4      munsell_0.5.0

[21] compiler_4.0.0   pkgconfig_2.0.3  colorspace_1.4-1 tidyselect_1.1.0

[25] tibble_3.0.1

	[[alternative HTML version deleted]]


From zem|ero| @end|ng |rom gm@||@com  Tue May 26 19:01:37 2020
From: zem|ero| @end|ng |rom gm@||@com (=?UTF-8?B?0JTQvNC40YLRgNC40Lkg0J/QvtC90L7QvNCw0YDQtdC90LrQvg==?=)
Date: Tue, 26 May 2020 20:01:37 +0300
Subject: [R] 3D NURBS import as B-spline
Message-ID: <CANW1KoVARRWUtkCOpRa5PDp_QXLX3TgjiJQoWm_hEZmyRt905w@mail.gmail.com>

 Dear R-help list,

I have 3D NURBS curves that I would like to import into R as B-splines. The
curves are in Rhinoceros 3dm format but can be converted to IGES format.

Can 3D NURBS curves be imported into R?

Thank you,
Zemleroi

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Tue May 26 20:31:54 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 26 May 2020 11:31:54 -0700
Subject: [R] Partial matching list elements in R 4.0
In-Reply-To: <CANAiAiXk++sowVnKf_VRL4-YtC_3dNNknqO+jjVzeOjU7udCKw@mail.gmail.com>
References: <CANAiAiXk++sowVnKf_VRL4-YtC_3dNNknqO+jjVzeOjU7udCKw@mail.gmail.com>
Message-ID: <CAGxFJbSBshZicnf_-75o0fYcCdfbNA2HajdOFnjZmJDy-AYLgw@mail.gmail.com>

I can't answer your question (about your R programming skills) but the
behavior you complain about is as documented. In particular:

"Thus the default behaviour is to use partial matching only when extracting
from recursive objects (except environments) by $. Even in that case,
warnings can be switched on by options
<http://127.0.0.1:39592/help/library/base/help/options>(warnPartialMatchDollar
= TRUE)."

So the solution is not to use $ for list extraction/replacement. Though
convenient, it is prone to such issues. Instead, the following works (as
does your suggested solution, of course):

> var <- list()
> var[["options"]][["misc"]][["abc"]] <- "123"
> var[["options"]][["mi"]][["something"]] <- 13
> var
$options
$options$misc
$options$misc$abc
[1] "123"


$options$mi
$options$mi$something
[1] 13

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, May 26, 2020 at 10:45 AM John Harrold <john.m.harrold at gmail.com>
wrote:

> Hello,
>
>
> I'm testing some code in R 4.0, and I'm having an issue with the following"
>
> # -------------
> rm(list=ls())
> graphics.off()
> #load("/tmp/post.RData")
> var = list();
> # If I uncomment this it fixes things:
> # var$options = list(mi   = list(),
> #                    misc = list())
> #
> var$options$misc$abc = "123"
> var$options$mi$something    = 13
> #------------
>
> This is a stripped down example but it exhibits the issue I"m having.
> Basically when I create the list element var$options$mi the contents of
> var$options$misc move over to var$options$mi. And what was in
> var$options$misc become NULL:
>
> So now var$options looks like:
>
> var$options
> $misc
> $misc$abc
> NULL
>
> $mi
> $mi$abc
> [1] "123"
> $mi$something
> [1] 13
>
> This worked (still works) in R 3.5.1. I understand partial matching, but is
> this normal lists moving over to elements like this? I can uncomment the
> text mentioned in the example and it seems to fix it, but I'm wondering if
> this is a bug or just my poor programming coming back to bite me.
>
> I've included my sessionInfo() at the bottom.
>
> Thanks
> John
> :wq
>
>
> sessionInfo()
>
> R version 4.0.0 (2020-04-24)
>
> Platform: x86_64-apple-darwin17.0 (64-bit)
>
> Running under: macOS Mojave 10.14.5
>
>
> Matrix products: default
>
> BLAS:
> /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
>
> LAPACK:
> /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
>
>
> locale:
>
> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>
>
> attached base packages:
>
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
>
> other attached packages:
>
> [1] gdata_2.18.0  ggplot2_3.3.0 deSolve_1.28
>
>
> loaded via a namespace (and not attached):
>
>  [1] Rcpp_1.0.4.6     gtools_3.8.2     withr_2.2.0      assertthat_0.2.1
>
>  [5] dplyr_0.8.5      digest_0.6.25    crayon_1.3.4     grid_4.0.0
>
>  [9] R6_2.4.1         lifecycle_0.2.0  gtable_0.3.0     magrittr_1.5
>
> [13] scales_1.1.1     pillar_1.4.4     rlang_0.4.6      vctrs_0.3.0
>
> [17] ellipsis_0.3.1   glue_1.4.1       purrr_0.3.4      munsell_0.5.0
>
> [21] compiler_4.0.0   pkgconfig_2.0.3  colorspace_1.4-1 tidyselect_1.1.0
>
> [25] tibble_3.0.1
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From wdun|@p @end|ng |rom t|bco@com  Tue May 26 20:33:42 2020
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Tue, 26 May 2020 11:33:42 -0700
Subject: [R] Partial matching list elements in R 4.0
In-Reply-To: <CANAiAiXk++sowVnKf_VRL4-YtC_3dNNknqO+jjVzeOjU7udCKw@mail.gmail.com>
References: <CANAiAiXk++sowVnKf_VRL4-YtC_3dNNknqO+jjVzeOjU7udCKw@mail.gmail.com>
Message-ID: <CAF8bMcYVUfgFD-5629Mz=kaw0vcU71g+484D48nLSGiEam+jdg@mail.gmail.com>

Another symptom of this problem is:

> {x <- list(Abc=list(Pqr="Old Abc$Pqr")); x$Ab$Pqr <- "New Ab$Pqr" ; x}
R version 3.6.2 (2019-12-12) | R version 4.0.0 (2020-04-24)
List of 2                    | List of 2
 $ Abc:List of 1             |  $ Abc:List of 1
  ..$ Pqr: chr "Old Abc$Pqr" |   ..$ Pqr: chr "New Ab$Pqr"
 $ Ab :List of 1             |  $ Ab :List of 1
  ..$ Pqr: chr "New Ab$Pqr"  |   ..$ Pqr: chr "New Ab$Pqr"

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Tue, May 26, 2020 at 10:45 AM John Harrold <john.m.harrold at gmail.com>
wrote:

> Hello,
>
>
> I'm testing some code in R 4.0, and I'm having an issue with the following"
>
> # -------------
> rm(list=ls())
> graphics.off()
> #load("/tmp/post.RData")
> var = list();
> # If I uncomment this it fixes things:
> # var$options = list(mi   = list(),
> #                    misc = list())
> #
> var$options$misc$abc = "123"
> var$options$mi$something    = 13
> #------------
>
> This is a stripped down example but it exhibits the issue I"m having.
> Basically when I create the list element var$options$mi the contents of
> var$options$misc move over to var$options$mi. And what was in
> var$options$misc become NULL:
>
> So now var$options looks like:
>
> var$options
> $misc
> $misc$abc
> NULL
>
> $mi
> $mi$abc
> [1] "123"
> $mi$something
> [1] 13
>
> This worked (still works) in R 3.5.1. I understand partial matching, but is
> this normal lists moving over to elements like this? I can uncomment the
> text mentioned in the example and it seems to fix it, but I'm wondering if
> this is a bug or just my poor programming coming back to bite me.
>
> I've included my sessionInfo() at the bottom.
>
> Thanks
> John
> :wq
>
>
> sessionInfo()
>
> R version 4.0.0 (2020-04-24)
>
> Platform: x86_64-apple-darwin17.0 (64-bit)
>
> Running under: macOS Mojave 10.14.5
>
>
> Matrix products: default
>
> BLAS:
> /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
>
> LAPACK:
> /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
>
>
> locale:
>
> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>
>
> attached base packages:
>
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
>
> other attached packages:
>
> [1] gdata_2.18.0  ggplot2_3.3.0 deSolve_1.28
>
>
> loaded via a namespace (and not attached):
>
>  [1] Rcpp_1.0.4.6     gtools_3.8.2     withr_2.2.0      assertthat_0.2.1
>
>  [5] dplyr_0.8.5      digest_0.6.25    crayon_1.3.4     grid_4.0.0
>
>  [9] R6_2.4.1         lifecycle_0.2.0  gtable_0.3.0     magrittr_1.5
>
> [13] scales_1.1.1     pillar_1.4.4     rlang_0.4.6      vctrs_0.3.0
>
> [17] ellipsis_0.3.1   glue_1.4.1       purrr_0.3.4      munsell_0.5.0
>
> [21] compiler_4.0.0   pkgconfig_2.0.3  colorspace_1.4-1 tidyselect_1.1.0
>
> [25] tibble_3.0.1
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue May 26 20:57:57 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 26 May 2020 11:57:57 -0700
Subject: [R] 3D NURBS import as B-spline
In-Reply-To: <CANW1KoVARRWUtkCOpRa5PDp_QXLX3TgjiJQoWm_hEZmyRt905w@mail.gmail.com>
References: <CANW1KoVARRWUtkCOpRa5PDp_QXLX3TgjiJQoWm_hEZmyRt905w@mail.gmail.com>
Message-ID: <808EE7AE-4E50-48B3-A42D-D08E1957262C@dcn.davis.ca.us>

The unhelpful answer is yes, because R is a programming language that just requires someone with an itch. The more helpful answer is that the rgl package could be a good place to investigate making those connections... it looks like that might have been a "future work item" some time back.

On May 26, 2020 10:01:37 AM PDT, "??????? ???????????" <zemleroi at gmail.com> wrote:
> Dear R-help list,
>
>I have 3D NURBS curves that I would like to import into R as B-splines.
>The
>curves are in Rhinoceros 3dm format but can be converted to IGES
>format.
>
>Can 3D NURBS curves be imported into R?
>
>Thank you,
>Zemleroi
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From john@m@h@rro|d @end|ng |rom gm@||@com  Tue May 26 21:54:45 2020
From: john@m@h@rro|d @end|ng |rom gm@||@com (John Harrold)
Date: Tue, 26 May 2020 12:54:45 -0700
Subject: [R] Partial matching list elements in R 4.0
In-Reply-To: <CAGxFJbSBshZicnf_-75o0fYcCdfbNA2HajdOFnjZmJDy-AYLgw@mail.gmail.com>
References: <CANAiAiXk++sowVnKf_VRL4-YtC_3dNNknqO+jjVzeOjU7udCKw@mail.gmail.com>
 <CAGxFJbSBshZicnf_-75o0fYcCdfbNA2HajdOFnjZmJDy-AYLgw@mail.gmail.com>
Message-ID: <CANAiAiURfjd4wa2c+Yv5v+-ajvfs=ueDm_phrmyi1PDRXKt93A@mail.gmail.com>

Hello Bert,

I've read the documentation and I didn't think it applied here. Perhaps
it's my reading of that documentation I'm confused by. I stays *only when
extracting*. What I'm doing here is assigning values. Is the expected
behavior really to create a copy of the "misc" element in "mi" and then set
all of the values in "misc" to NULL?

Thanks,
John

On Tue, May 26, 2020 at 11:32 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> I can't answer your question (about your R programming skills) but the
> behavior you complain about is as documented. In particular:
>
> "Thus the default behaviour is to use partial matching only when
> extracting from recursive objects (except environments) by $. Even in
> that case, warnings can be switched on by options
> <http://127.0.0.1:39592/help/library/base/help/options>(warnPartialMatchDollar
> = TRUE)."
>
> So the solution is not to use $ for list extraction/replacement. Though
> convenient, it is prone to such issues. Instead, the following works (as
> does your suggested solution, of course):
>
> > var <- list()
> > var[["options"]][["misc"]][["abc"]] <- "123"
> > var[["options"]][["mi"]][["something"]] <- 13
> > var
> $options
> $options$misc
> $options$misc$abc
> [1] "123"
>
>
> $options$mi
> $options$mi$something
> [1] 13
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Tue, May 26, 2020 at 10:45 AM John Harrold <john.m.harrold at gmail.com>
> wrote:
>
>> Hello,
>>
>>
>> I'm testing some code in R 4.0, and I'm having an issue with the
>> following"
>>
>> # -------------
>> rm(list=ls())
>> graphics.off()
>> #load("/tmp/post.RData")
>> var = list();
>> # If I uncomment this it fixes things:
>> # var$options = list(mi   = list(),
>> #                    misc = list())
>> #
>> var$options$misc$abc = "123"
>> var$options$mi$something    = 13
>> #------------
>>
>> This is a stripped down example but it exhibits the issue I"m having.
>> Basically when I create the list element var$options$mi the contents of
>> var$options$misc move over to var$options$mi. And what was in
>> var$options$misc become NULL:
>>
>> So now var$options looks like:
>>
>> var$options
>> $misc
>> $misc$abc
>> NULL
>>
>> $mi
>> $mi$abc
>> [1] "123"
>> $mi$something
>> [1] 13
>>
>> This worked (still works) in R 3.5.1. I understand partial matching, but
>> is
>> this normal lists moving over to elements like this? I can uncomment the
>> text mentioned in the example and it seems to fix it, but I'm wondering if
>> this is a bug or just my poor programming coming back to bite me.
>>
>> I've included my sessionInfo() at the bottom.
>>
>> Thanks
>> John
>> :wq
>>
>>
>> sessionInfo()
>>
>> R version 4.0.0 (2020-04-24)
>>
>> Platform: x86_64-apple-darwin17.0 (64-bit)
>>
>> Running under: macOS Mojave 10.14.5
>>
>>
>> Matrix products: default
>>
>> BLAS:
>> /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
>>
>> LAPACK:
>>
>> /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
>>
>>
>> locale:
>>
>> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>>
>>
>> attached base packages:
>>
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>>
>> other attached packages:
>>
>> [1] gdata_2.18.0  ggplot2_3.3.0 deSolve_1.28
>>
>>
>> loaded via a namespace (and not attached):
>>
>>  [1] Rcpp_1.0.4.6     gtools_3.8.2     withr_2.2.0      assertthat_0.2.1
>>
>>  [5] dplyr_0.8.5      digest_0.6.25    crayon_1.3.4     grid_4.0.0
>>
>>  [9] R6_2.4.1         lifecycle_0.2.0  gtable_0.3.0     magrittr_1.5
>>
>> [13] scales_1.1.1     pillar_1.4.4     rlang_0.4.6      vctrs_0.3.0
>>
>> [17] ellipsis_0.3.1   glue_1.4.1       purrr_0.3.4      munsell_0.5.0
>>
>> [21] compiler_4.0.0   pkgconfig_2.0.3  colorspace_1.4-1 tidyselect_1.1.0
>>
>> [25] tibble_3.0.1
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

-- 
John
:wq

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Tue May 26 22:48:03 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 26 May 2020 13:48:03 -0700
Subject: [R] Partial matching list elements in R 4.0
In-Reply-To: <CANAiAiURfjd4wa2c+Yv5v+-ajvfs=ueDm_phrmyi1PDRXKt93A@mail.gmail.com>
References: <CANAiAiXk++sowVnKf_VRL4-YtC_3dNNknqO+jjVzeOjU7udCKw@mail.gmail.com>
 <CAGxFJbSBshZicnf_-75o0fYcCdfbNA2HajdOFnjZmJDy-AYLgw@mail.gmail.com>
 <CANAiAiURfjd4wa2c+Yv5v+-ajvfs=ueDm_phrmyi1PDRXKt93A@mail.gmail.com>
Message-ID: <CAGxFJbToxyks+58z6XPv-6abgG34ioS2=MrSyBM4-4iTs1B3jw@mail.gmail.com>

Hmmm... yes. I read (past tense) that passage as meaning that **when
extracting** partial matching is only done with $. I did not read  it as
also saying that with assignment, partial matching with $ is not done, but
I see how you could. As Bill D.'s example showed, even R seems confused
about how $ should behave on assignment. As I said, best to avoid the issue
altogether by using [[ ]], where matching behavior can be explicitly
controlled and the default is "exact, "which has been recommended here from
time to time by others also. I grant you that such ambiguity is not
desirable, though.

Bert




Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, May 26, 2020 at 12:54 PM John Harrold <john.m.harrold at gmail.com>
wrote:

> Hello Bert,
>
> I've read the documentation and I didn't think it applied here. Perhaps
> it's my reading of that documentation I'm confused by. I stays *only when
> extracting*. What I'm doing here is assigning values. Is the expected
> behavior really to create a copy of the "misc" element in "mi" and then set
> all of the values in "misc" to NULL?
>
> Thanks,
> John
>
> On Tue, May 26, 2020 at 11:32 AM Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
>
>> I can't answer your question (about your R programming skills) but the
>> behavior you complain about is as documented. In particular:
>>
>> "Thus the default behaviour is to use partial matching only when
>> extracting from recursive objects (except environments) by $. Even in
>> that case, warnings can be switched on by options
>> <http://127.0.0.1:39592/help/library/base/help/options>(warnPartialMatchDollar
>> = TRUE)."
>>
>> So the solution is not to use $ for list extraction/replacement. Though
>> convenient, it is prone to such issues. Instead, the following works (as
>> does your suggested solution, of course):
>>
>> > var <- list()
>> > var[["options"]][["misc"]][["abc"]] <- "123"
>> > var[["options"]][["mi"]][["something"]] <- 13
>> > var
>> $options
>> $options$misc
>> $options$misc$abc
>> [1] "123"
>>
>>
>> $options$mi
>> $options$mi$something
>> [1] 13
>>
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming along
>> and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>> On Tue, May 26, 2020 at 10:45 AM John Harrold <john.m.harrold at gmail.com>
>> wrote:
>>
>>> Hello,
>>>
>>>
>>> I'm testing some code in R 4.0, and I'm having an issue with the
>>> following"
>>>
>>> # -------------
>>> rm(list=ls())
>>> graphics.off()
>>> #load("/tmp/post.RData")
>>> var = list();
>>> # If I uncomment this it fixes things:
>>> # var$options = list(mi   = list(),
>>> #                    misc = list())
>>> #
>>> var$options$misc$abc = "123"
>>> var$options$mi$something    = 13
>>> #------------
>>>
>>> This is a stripped down example but it exhibits the issue I"m having.
>>> Basically when I create the list element var$options$mi the contents of
>>> var$options$misc move over to var$options$mi. And what was in
>>> var$options$misc become NULL:
>>>
>>> So now var$options looks like:
>>>
>>> var$options
>>> $misc
>>> $misc$abc
>>> NULL
>>>
>>> $mi
>>> $mi$abc
>>> [1] "123"
>>> $mi$something
>>> [1] 13
>>>
>>> This worked (still works) in R 3.5.1. I understand partial matching, but
>>> is
>>> this normal lists moving over to elements like this? I can uncomment the
>>> text mentioned in the example and it seems to fix it, but I'm wondering
>>> if
>>> this is a bug or just my poor programming coming back to bite me.
>>>
>>> I've included my sessionInfo() at the bottom.
>>>
>>> Thanks
>>> John
>>> :wq
>>>
>>>
>>> sessionInfo()
>>>
>>> R version 4.0.0 (2020-04-24)
>>>
>>> Platform: x86_64-apple-darwin17.0 (64-bit)
>>>
>>> Running under: macOS Mojave 10.14.5
>>>
>>>
>>> Matrix products: default
>>>
>>> BLAS:
>>> /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
>>>
>>> LAPACK:
>>>
>>> /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
>>>
>>>
>>> locale:
>>>
>>> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>>>
>>>
>>> attached base packages:
>>>
>>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>>
>>>
>>> other attached packages:
>>>
>>> [1] gdata_2.18.0  ggplot2_3.3.0 deSolve_1.28
>>>
>>>
>>> loaded via a namespace (and not attached):
>>>
>>>  [1] Rcpp_1.0.4.6     gtools_3.8.2     withr_2.2.0      assertthat_0.2.1
>>>
>>>  [5] dplyr_0.8.5      digest_0.6.25    crayon_1.3.4     grid_4.0.0
>>>
>>>  [9] R6_2.4.1         lifecycle_0.2.0  gtable_0.3.0     magrittr_1.5
>>>
>>> [13] scales_1.1.1     pillar_1.4.4     rlang_0.4.6      vctrs_0.3.0
>>>
>>> [17] ellipsis_0.3.1   glue_1.4.1       purrr_0.3.4      munsell_0.5.0
>>>
>>> [21] compiler_4.0.0   pkgconfig_2.0.3  colorspace_1.4-1 tidyselect_1.1.0
>>>
>>> [25] tibble_3.0.1
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>
> --
> John
> :wq
>

	[[alternative HTML version deleted]]


From r@v|76 @end|ng |rom gm@||@com  Wed May 27 04:20:47 2020
From: r@v|76 @end|ng |rom gm@||@com (Ravi Jeyaraman)
Date: Tue, 26 May 2020 22:20:47 -0400
Subject: [R] read_excel() ignore case of worksheet name?
Message-ID: <002401d633cd$6f390b50$4dab21f0$@gmail.com>

Hello All, Is there any parameter to make read_excel() ignore the case-sensitiveness of the worksheet?   I'm using the below to ready in multiple spreadsheets and it works perfectly fine if the worksheet is named 'Tables', but fails when it's named ' TABLES'.  Any thoughts?

lapply(1:nrow(SIS), function(x) readxl::read_excel(SIS$FULL_FILEPATH[x], sheet='Tables', .name_repair = fixColNames))

Thanks in advance for your response.

Cheers
Ravi



-- 
This email has been checked for viruses by AVG.
https://www.avg.com


From er|nm@hodge@@ @end|ng |rom gm@||@com  Wed May 27 04:55:11 2020
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Tue, 26 May 2020 20:55:11 -0600
Subject: [R] read_excel() ignore case of worksheet name?
In-Reply-To: <002401d633cd$6f390b50$4dab21f0$@gmail.com>
References: <002401d633cd$6f390b50$4dab21f0$@gmail.com>
Message-ID: <CACxE24nFOyeY1bQ9HaWw8ZqJjoH3xviv7FiTV8NtsKKFx-Bjqw@mail.gmail.com>

Here?s a thought, please.  Could you use the tolower function and make them
all lower case?

Thanks,
Erin

On Tue, May 26, 2020 at 8:21 PM Ravi Jeyaraman <ravi76 at gmail.com> wrote:

> Hello All, Is there any parameter to make read_excel() ignore the
> case-sensitiveness of the worksheet?   I'm using the below to ready in
> multiple spreadsheets and it works perfectly fine if the worksheet is named
> 'Tables', but fails when it's named ' TABLES'.  Any thoughts?
>
> lapply(1:nrow(SIS), function(x) readxl::read_excel(SIS$FULL_FILEPATH[x],
> sheet='Tables', .name_repair = fixColNames))
>
> Thanks in advance for your response.
>
> Cheers
> Ravi
>
>
>
> --
> This email has been checked for viruses by AVG.
> https://www.avg.com
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
-- 
Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From r@v|76 @end|ng |rom gm@||@com  Wed May 27 04:59:33 2020
From: r@v|76 @end|ng |rom gm@||@com (Ravi Jeyaraman)
Date: Tue, 26 May 2020 22:59:33 -0400
Subject: [R] read_excel() ignore case of worksheet name?
In-Reply-To: <CACxE24nFOyeY1bQ9HaWw8ZqJjoH3xviv7FiTV8NtsKKFx-Bjqw@mail.gmail.com>
References: <002401d633cd$6f390b50$4dab21f0$@gmail.com>
 <CACxE24nFOyeY1bQ9HaWw8ZqJjoH3xviv7FiTV8NtsKKFx-Bjqw@mail.gmail.com>
Message-ID: <00a601d633d2$d9a55c50$8cf014f0$@gmail.com>

I?ve already tried that and doesn?t work

 

From: Erin Hodgess [mailto:erinm.hodgess at gmail.com] 
Sent: Tuesday, May 26, 2020 10:55 PM
To: Ravi Jeyaraman <ravi76 at gmail.com>
Cc: r-help at r-project.org
Subject: Re: [R] read_excel() ignore case of worksheet name?

 

Here?s a thought, please.  Could you use the tolower function and make them all lower case?

 

Thanks, 

Erin 

 

On Tue, May 26, 2020 at 8:21 PM Ravi Jeyaraman <ravi76 at gmail.com <mailto:ravi76 at gmail.com> > wrote:

Hello All, Is there any parameter to make read_excel() ignore the case-sensitiveness of the worksheet?   I'm using the below to ready in multiple spreadsheets and it works perfectly fine if the worksheet is named 'Tables', but fails when it's named ' TABLES'.  Any thoughts?

lapply(1:nrow(SIS), function(x) readxl::read_excel(SIS$FULL_FILEPATH[x], sheet='Tables', .name_repair = fixColNames))

Thanks in advance for your response.

Cheers
Ravi



-- 
This email has been checked for viruses by AVG.
https://www.avg.com

______________________________________________
R-help at r-project.org <mailto:R-help at r-project.org>  mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

-- 

Erin Hodgess, PhD

mailto: erinm.hodgess at gmail.com <mailto:erinm.hodgess at gmail.com> 


	[[alternative HTML version deleted]]


From er|nm@hodge@@ @end|ng |rom gm@||@com  Wed May 27 05:34:22 2020
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Tue, 26 May 2020 21:34:22 -0600
Subject: [R] read_excel() ignore case of worksheet name?
In-Reply-To: <00a601d633d2$d9a55c50$8cf014f0$@gmail.com>
References: <002401d633cd$6f390b50$4dab21f0$@gmail.com>
 <CACxE24nFOyeY1bQ9HaWw8ZqJjoH3xviv7FiTV8NtsKKFx-Bjqw@mail.gmail.com>
 <00a601d633d2$d9a55c50$8cf014f0$@gmail.com>
Message-ID: <CACxE24nK+i5HmkLuRXmT+kP9muoiSd_tow96t5=F96eyPRhChA@mail.gmail.com>

What about getSheets, please?

That will get the sheet names.

On Tue, May 26, 2020 at 8:59 PM Ravi Jeyaraman <ravi76 at gmail.com> wrote:

> I?ve already tried that and doesn?t work
>
>
>
> *From:* Erin Hodgess [mailto:erinm.hodgess at gmail.com]
> *Sent:* Tuesday, May 26, 2020 10:55 PM
> *To:* Ravi Jeyaraman <ravi76 at gmail.com>
> *Cc:* r-help at r-project.org
> *Subject:* Re: [R] read_excel() ignore case of worksheet name?
>
>
>
> Here?s a thought, please.  Could you use the tolower function and make
> them all lower case?
>
>
>
> Thanks,
>
> Erin
>
>
>
> On Tue, May 26, 2020 at 8:21 PM Ravi Jeyaraman <ravi76 at gmail.com> wrote:
>
> Hello All, Is there any parameter to make read_excel() ignore the
> case-sensitiveness of the worksheet?   I'm using the below to ready in
> multiple spreadsheets and it works perfectly fine if the worksheet is named
> 'Tables', but fails when it's named ' TABLES'.  Any thoughts?
>
> lapply(1:nrow(SIS), function(x) readxl::read_excel(SIS$FULL_FILEPATH[x],
> sheet='Tables', .name_repair = fixColNames))
>
> Thanks in advance for your response.
>
> Cheers
> Ravi
>
>
>
> --
> This email has been checked for viruses by AVG.
> https://www.avg.com
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> --
>
> Erin Hodgess, PhD
>
> mailto: erinm.hodgess at gmail.com
>
>
> <http://www.avg.com/email-signature?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=emailclient> Virus-free.
> www.avg.com
> <http://www.avg.com/email-signature?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=emailclient>
> <#m_-1767373775547039939_DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>
>
-- 
Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From |@t@z@hn @end|ng |rom gm@||@com  Wed May 27 05:37:36 2020
From: |@t@z@hn @end|ng |rom gm@||@com (Ista Zahn)
Date: Tue, 26 May 2020 23:37:36 -0400
Subject: [R] read_excel() ignore case of worksheet name?
In-Reply-To: <00a601d633d2$d9a55c50$8cf014f0$@gmail.com>
References: <002401d633cd$6f390b50$4dab21f0$@gmail.com>
 <CACxE24nFOyeY1bQ9HaWw8ZqJjoH3xviv7FiTV8NtsKKFx-Bjqw@mail.gmail.com>
 <00a601d633d2$d9a55c50$8cf014f0$@gmail.com>
Message-ID: <CA+vqiLF=Nm=dbPBHrK51i1ZDox36nNL=zpOhEGQwbAqd1MdAvA@mail.gmail.com>

How about

read_excel_table <- function(x) {
  readxl::read_excel(
    x,
    sheet=grep("tables",
               excel_sheets(x),
               ignore.case = TRUE,
               value = TRUE),
    .name_repair = fixColNames
  )
}

lapply(SIS$FULL_FILEPATH, read_excel_table)


--Ista

On Tue, May 26, 2020 at 11:05 PM Ravi Jeyaraman <ravi76 at gmail.com> wrote:
>
> I?ve already tried that and doesn?t work
>
>
>
> From: Erin Hodgess [mailto:erinm.hodgess at gmail.com]
> Sent: Tuesday, May 26, 2020 10:55 PM
> To: Ravi Jeyaraman <ravi76 at gmail.com>
> Cc: r-help at r-project.org
> Subject: Re: [R] read_excel() ignore case of worksheet name?
>
>
>
> Here?s a thought, please.  Could you use the tolower function and make them all lower case?
>
>
>
> Thanks,
>
> Erin
>
>
>
> On Tue, May 26, 2020 at 8:21 PM Ravi Jeyaraman <ravi76 at gmail.com <mailto:ravi76 at gmail.com> > wrote:
>
> Hello All, Is there any parameter to make read_excel() ignore the case-sensitiveness of the worksheet?   I'm using the below to ready in multiple spreadsheets and it works perfectly fine if the worksheet is named 'Tables', but fails when it's named ' TABLES'.  Any thoughts?
>
> lapply(1:nrow(SIS), function(x) readxl::read_excel(SIS$FULL_FILEPATH[x], sheet='Tables', .name_repair = fixColNames))
>
> Thanks in advance for your response.
>
> Cheers
> Ravi
>
>
>
> --
> This email has been checked for viruses by AVG.
> https://www.avg.com
>
> ______________________________________________
> R-help at r-project.org <mailto:R-help at r-project.org>  mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> --
>
> Erin Hodgess, PhD
>
> mailto: erinm.hodgess at gmail.com <mailto:erinm.hodgess at gmail.com>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From er|nm@hodge@@ @end|ng |rom gm@||@com  Wed May 27 05:40:17 2020
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Tue, 26 May 2020 21:40:17 -0600
Subject: [R] read_excel() ignore case of worksheet name?
In-Reply-To: <CA+vqiLF=Nm=dbPBHrK51i1ZDox36nNL=zpOhEGQwbAqd1MdAvA@mail.gmail.com>
References: <002401d633cd$6f390b50$4dab21f0$@gmail.com>
 <CACxE24nFOyeY1bQ9HaWw8ZqJjoH3xviv7FiTV8NtsKKFx-Bjqw@mail.gmail.com>
 <00a601d633d2$d9a55c50$8cf014f0$@gmail.com>
 <CA+vqiLF=Nm=dbPBHrK51i1ZDox36nNL=zpOhEGQwbAqd1MdAvA@mail.gmail.com>
Message-ID: <CACxE24nx7KvSdAvqLn28XM5+aWK+7qs+pJmgC05qNaxJ=_-6ww@mail.gmail.com>

Nice!

On Tue, May 26, 2020 at 9:37 PM Ista Zahn <istazahn at gmail.com> wrote:

> How about
>
> read_excel_table <- function(x) {
>   readxl::read_excel(
>     x,
>     sheet=grep("tables",
>                excel_sheets(x),
>                ignore.case = TRUE,
>                value = TRUE),
>     .name_repair = fixColNames
>   )
> }
>
> lapply(SIS$FULL_FILEPATH, read_excel_table)
>
>
> --Ista
>
> On Tue, May 26, 2020 at 11:05 PM Ravi Jeyaraman <ravi76 at gmail.com> wrote:
> >
> > I?ve already tried that and doesn?t work
> >
> >
> >
> > From: Erin Hodgess [mailto:erinm.hodgess at gmail.com]
> > Sent: Tuesday, May 26, 2020 10:55 PM
> > To: Ravi Jeyaraman <ravi76 at gmail.com>
> > Cc: r-help at r-project.org
> > Subject: Re: [R] read_excel() ignore case of worksheet name?
> >
> >
> >
> > Here?s a thought, please.  Could you use the tolower function and make
> them all lower case?
> >
> >
> >
> > Thanks,
> >
> > Erin
> >
> >
> >
> > On Tue, May 26, 2020 at 8:21 PM Ravi Jeyaraman <ravi76 at gmail.com
> <mailto:ravi76 at gmail.com> > wrote:
> >
> > Hello All, Is there any parameter to make read_excel() ignore the
> case-sensitiveness of the worksheet?   I'm using the below to ready in
> multiple spreadsheets and it works perfectly fine if the worksheet is named
> 'Tables', but fails when it's named ' TABLES'.  Any thoughts?
> >
> > lapply(1:nrow(SIS), function(x) readxl::read_excel(SIS$FULL_FILEPATH[x],
> sheet='Tables', .name_repair = fixColNames))
> >
> > Thanks in advance for your response.
> >
> > Cheers
> > Ravi
> >
> >
> >
> > --
> > This email has been checked for viruses by AVG.
> > https://www.avg.com
> >
> > ______________________________________________
> > R-help at r-project.org <mailto:R-help at r-project.org>  mailing list -- To
> UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> > --
> >
> > Erin Hodgess, PhD
> >
> > mailto: erinm.hodgess at gmail.com <mailto:erinm.hodgess at gmail.com>
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
-- 
Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Wed May 27 05:40:48 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Wed, 27 May 2020 15:40:48 +1200
Subject: [R] [FORGED] Re:  read_excel() ignore case of worksheet name?
In-Reply-To: <00a601d633d2$d9a55c50$8cf014f0$@gmail.com>
References: <002401d633cd$6f390b50$4dab21f0$@gmail.com>
 <CACxE24nFOyeY1bQ9HaWw8ZqJjoH3xviv7FiTV8NtsKKFx-Bjqw@mail.gmail.com>
 <00a601d633d2$d9a55c50$8cf014f0$@gmail.com>
Message-ID: <10fb9149-bd40-ab42-6588-5fdc35ee0de2@auckland.ac.nz>


There is a function excel_sheets() in the readxl package which will tell 
you the names of the sheets.

Using that you should probably be able to take the appropriate evasive 
action.

cheers,

Rolf Turner

On 27/05/20 2:59 pm, Ravi Jeyaraman wrote:
> I?ve already tried that and doesn?t work
> 
>   
> 
> From: Erin Hodgess [mailto:erinm.hodgess at gmail.com]
> Sent: Tuesday, May 26, 2020 10:55 PM
> To: Ravi Jeyaraman <ravi76 at gmail.com>
> Cc: r-help at r-project.org
> Subject: Re: [R] read_excel() ignore case of worksheet name?
> 
>   
> 
> Here?s a thought, please.  Could you use the tolower function and make them all lower case?
> 
>   
> 
> Thanks,
> 
> Erin
> 
>   
> 
> On Tue, May 26, 2020 at 8:21 PM Ravi Jeyaraman <ravi76 at gmail.com <mailto:ravi76 at gmail.com> > wrote:
> 
> Hello All, Is there any parameter to make read_excel() ignore the case-sensitiveness of the worksheet?   I'm using the below to ready in multiple spreadsheets and it works perfectly fine if the worksheet is named 'Tables', but fails when it's named ' TABLES'.  Any thoughts?
> 
> lapply(1:nrow(SIS), function(x) readxl::read_excel(SIS$FULL_FILEPATH[x], sheet='Tables', .name_repair = fixColNames))
> 
> Thanks in advance for your response.
> 
> Cheers
> Ravi


From drj|m|emon @end|ng |rom gm@||@com  Wed May 27 05:50:46 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 27 May 2020 13:50:46 +1000
Subject: [R] read_excel() ignore case of worksheet name?
In-Reply-To: <00a601d633d2$d9a55c50$8cf014f0$@gmail.com>
References: <002401d633cd$6f390b50$4dab21f0$@gmail.com>
 <CACxE24nFOyeY1bQ9HaWw8ZqJjoH3xviv7FiTV8NtsKKFx-Bjqw@mail.gmail.com>
 <00a601d633d2$d9a55c50$8cf014f0$@gmail.com>
Message-ID: <CA+8X3fUXEZNntjrfvLtovpNQvf87pw4BF4mrx5dNZNB-5ADS7A@mail.gmail.com>

Hi Ravi,
The simplest way seems to be the excel_sheets function in the readxl
package. If you know that the sheet name will be some form of "Table",
something like this may do it:

getSheetCase<-function(filepath,sheetname) {
 localnames<-c(sheetname,
  paste0(toupper(substr(sheetname,1,1)),substr(sheetname,2,nchar(sheetname))),
  toupper(sheetname),tolower(sheetname))
 xlnames<-readxl::excel_sheets(filepath)
 namepos<-0
 for(pos in 1:length(localnames)) {
  if(length(grep(localnames[pos],xlnames))) namepos<-pos
  cat(localnames[pos],namepos,"\n")
 }
 if(is.null(namepos)) return(NULL)
 else return(read_excel(filepath,
  sheet=localnames[namepos]))
}
getSheetCase("GS_SS2.xlsx","intent")

This example works on an excel spreadsheet I have as in the last line.
Just as I was about to send this, three messages came in. One was
Ista's excellent solution that blew mine away. Maybe next time.

Jim

On Wed, May 27, 2020 at 1:05 PM Ravi Jeyaraman <ravi76 at gmail.com> wrote:
>
> I?ve already tried that and doesn?t work
>
>
>
> From: Erin Hodgess [mailto:erinm.hodgess at gmail.com]
> Sent: Tuesday, May 26, 2020 10:55 PM
> To: Ravi Jeyaraman <ravi76 at gmail.com>
> Cc: r-help at r-project.org
> Subject: Re: [R] read_excel() ignore case of worksheet name?
>
>
>
> Here?s a thought, please.  Could you use the tolower function and make them all lower case?
>
>
>
> Thanks,
>
> Erin
>
>
>
> On Tue, May 26, 2020 at 8:21 PM Ravi Jeyaraman <ravi76 at gmail.com <mailto:ravi76 at gmail.com> > wrote:
>
> Hello All, Is there any parameter to make read_excel() ignore the case-sensitiveness of the worksheet?   I'm using the below to ready in multiple spreadsheets and it works perfectly fine if the worksheet is named 'Tables', but fails when it's named ' TABLES'.  Any thoughts?
>
> lapply(1:nrow(SIS), function(x) readxl::read_excel(SIS$FULL_FILEPATH[x], sheet='Tables', .name_repair = fixColNames))
>
> Thanks in advance for your response.
>
> Cheers
> Ravi
>
>
>
> --
> This email has been checked for viruses by AVG.
> https://www.avg.com
>
> ______________________________________________
> R-help at r-project.org <mailto:R-help at r-project.org>  mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> --
>
> Erin Hodgess, PhD
>
> mailto: erinm.hodgess at gmail.com <mailto:erinm.hodgess at gmail.com>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m|@ojpm @end|ng |rom gm@||@com  Wed May 27 08:21:58 2020
From: m|@ojpm @end|ng |rom gm@||@com (John)
Date: Wed, 27 May 2020 14:21:58 +0800
Subject: [R] Output multiple sheets to Excel files with openxlsx::write.xlsx
Message-ID: <CABcx46B9=bsC0s5uLTtd6-poX84xs90Emx=M-_WVGLfDB+g9dA@mail.gmail.com>

Hi,

   This is my code a few years ago. I was able to output multiple sheet to
an excel file. Nevertheless, the "append" argument appears to be obsolete.
Now I see only one sheet, the latest added sheet, in the output. Is there
any other way to do it with openxlsx::write.xlsx or other
functions/packages?


openxlsx::write.xlsx(df1, file=fl_out, sheetName="a",
                 col.names=TRUE, row.names=FALSE, append=TRUE, showNA=FALSE)

openxlsx::write.xlsx(df2, file=fl_out, sheetName="b",
                     col.names=TRUE, row.names=FALSE,
append=TRUE, showNA=FALSE)

Thanks!!

	[[alternative HTML version deleted]]


From e@ @end|ng |rom enr|co@chum@nn@net  Wed May 27 09:15:13 2020
From: e@ @end|ng |rom enr|co@chum@nn@net (Enrico Schumann)
Date: Wed, 27 May 2020 09:15:13 +0200
Subject: [R] 
 Output multiple sheets to Excel files with openxlsx::write.xlsx
In-Reply-To: <CABcx46B9=bsC0s5uLTtd6-poX84xs90Emx=M-_WVGLfDB+g9dA@mail.gmail.com>
 (John's message of "Wed, 27 May 2020 14:21:58 +0800")
References: <CABcx46B9=bsC0s5uLTtd6-poX84xs90Emx=M-_WVGLfDB+g9dA@mail.gmail.com>
Message-ID: <87imghsvb2.fsf@enricoschumann.net>

On Wed, 27 May 2020, John writes:

> Hi,
>
>    This is my code a few years ago. I was able to output multiple sheet to
> an excel file. Nevertheless, the "append" argument appears to be obsolete.
> Now I see only one sheet, the latest added sheet, in the output. Is there
> any other way to do it with openxlsx::write.xlsx or other
> functions/packages?
>
>
> openxlsx::write.xlsx(df1, file=fl_out, sheetName="a",
>                  col.names=TRUE, row.names=FALSE, append=TRUE, showNA=FALSE)
>
> openxlsx::write.xlsx(df2, file=fl_out, sheetName="b",
>                      col.names=TRUE, row.names=FALSE,
> append=TRUE, showNA=FALSE)
>
> Thanks!!
>

I think you need to create a workbook first, then add
the sheets, and finally write the workbook to a file.
Something like this:

    df <- data.frame(a = 1:3,
                     b = 4:6)
    
    library("openxlsx")
    wb <- createWorkbook()
    
    sheet <- "sheet1"
    addWorksheet(wb, sheet)
    writeData(wb, sheet = sheet, x = df)
    
    sheet <- "sheet2"
    addWorksheet(wb, sheet)
    writeData(wb, sheet = sheet, x = df + 1)
    
    saveWorkbook(wb, file = "~/Desktop/two_sheets.xlsx")



-- 
Enrico Schumann
Lucerne, Switzerland
http://enricoschumann.net


From c@|@ndr@ @end|ng |rom rgzm@de  Wed May 27 09:24:01 2020
From: c@|@ndr@ @end|ng |rom rgzm@de (Ivan Calandra)
Date: Wed, 27 May 2020 09:24:01 +0200
Subject: [R] 
 Output multiple sheets to Excel files with openxlsx::write.xlsx
In-Reply-To: <87imghsvb2.fsf@enricoschumann.net>
References: <CABcx46B9=bsC0s5uLTtd6-poX84xs90Emx=M-_WVGLfDB+g9dA@mail.gmail.com>
 <87imghsvb2.fsf@enricoschumann.net>
Message-ID: <acc11103-9700-28f6-47cf-eb55921bc771@rgzm.de>

Hi,

Rather than creating a workbook as suggested by Enrico, you can simply
supply a list to write.xlsx(); each element will be saved in a separate
sheet:
write.xlsx(list(a = df1, b = df2), file = fl_out)

That is not really appending, but that might work for you.

HTH,
Ivan

--
Dr. Ivan Calandra
TraCEr, laboratory for Traceology and Controlled Experiments
MONREPOS Archaeological Research Centre and
Museum for Human Behavioural Evolution
Schloss Monrepos
56567 Neuwied, Germany
+49 (0) 2631 9772-243
https://www.researchgate.net/profile/Ivan_Calandra

On 27/05/2020 9:15, Enrico Schumann wrote:
> On Wed, 27 May 2020, John writes:
>
>> Hi,
>>
>>    This is my code a few years ago. I was able to output multiple sheet to
>> an excel file. Nevertheless, the "append" argument appears to be obsolete.
>> Now I see only one sheet, the latest added sheet, in the output. Is there
>> any other way to do it with openxlsx::write.xlsx or other
>> functions/packages?
>>
>>
>> openxlsx::write.xlsx(df1, file=fl_out, sheetName="a",
>>                  col.names=TRUE, row.names=FALSE, append=TRUE, showNA=FALSE)
>>
>> openxlsx::write.xlsx(df2, file=fl_out, sheetName="b",
>>                      col.names=TRUE, row.names=FALSE,
>> append=TRUE, showNA=FALSE)
>>
>> Thanks!!
>>
> I think you need to create a workbook first, then add
> the sheets, and finally write the workbook to a file.
> Something like this:
>
>     df <- data.frame(a = 1:3,
>                      b = 4:6)
>     
>     library("openxlsx")
>     wb <- createWorkbook()
>     
>     sheet <- "sheet1"
>     addWorksheet(wb, sheet)
>     writeData(wb, sheet = sheet, x = df)
>     
>     sheet <- "sheet2"
>     addWorksheet(wb, sheet)
>     writeData(wb, sheet = sheet, x = df + 1)
>     
>     saveWorkbook(wb, file = "~/Desktop/two_sheets.xlsx")
>
>
>


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed May 27 09:28:13 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 27 May 2020 00:28:13 -0700
Subject: [R] 
 Output multiple sheets to Excel files with openxlsx::write.xlsx
In-Reply-To: <CABcx46B9=bsC0s5uLTtd6-poX84xs90Emx=M-_WVGLfDB+g9dA@mail.gmail.com>
References: <CABcx46B9=bsC0s5uLTtd6-poX84xs90Emx=M-_WVGLfDB+g9dA@mail.gmail.com>
Message-ID: <C2A13BEA-3E03-4C72-A32E-22ECDE0E910B@dcn.davis.ca.us>

give the function a list of data frames.

On May 26, 2020 11:21:58 PM PDT, John <miaojpm at gmail.com> wrote:
>Hi,
>
>This is my code a few years ago. I was able to output multiple sheet to
>an excel file. Nevertheless, the "append" argument appears to be
>obsolete.
>Now I see only one sheet, the latest added sheet, in the output. Is
>there
>any other way to do it with openxlsx::write.xlsx or other
>functions/packages?
>
>
>openxlsx::write.xlsx(df1, file=fl_out, sheetName="a",
>            col.names=TRUE, row.names=FALSE, append=TRUE, showNA=FALSE)
>
>openxlsx::write.xlsx(df2, file=fl_out, sheetName="b",
>                     col.names=TRUE, row.names=FALSE,
>append=TRUE, showNA=FALSE)
>
>Thanks!!
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From ch@kr@r@hu| @end|ng |rom gm@||@com  Wed May 27 10:24:29 2020
From: ch@kr@r@hu| @end|ng |rom gm@||@com (Rahul Chakraborty)
Date: Wed, 27 May 2020 13:54:29 +0530
Subject: [R] Problem in generating an "Orthogonal fractional design"
Message-ID: <CAEmZPS=d7XgpM2_znpJS=dx_4Qg18yrfQ4nRUPqR00vKtH5v9Q@mail.gmail.com>

Dear all,

Presently I am working on designing a questionnaire for my discrete choice
experiment. I want to generate an orthogonal fractional factorial design
for the following problem-

The respondent has to choose one out of 4 objects (*X1, X2, X3, X4*). Each
of the 4 objects are classified by 10 different attributes. However, the
levels are not the same under each of the objects. The table below displays
the situation.

Attributes No. of Levels  Choices and values
X1 X2 X3 X4
A 5 1 1,2,3 3,4,5 3,4,5
B 4 1 1 1,2 3,4
C 4 1 1 2,4 3,4
D 5 1 1,2,3 1,2,3 1,4,5
E  5 1,2 2,3 3,4 5
F 2 1 1 1,2 1,2
G 2 1 1 1,2 2
H 2 1 1 1,2 1,2
I 4 1 2,3,4 2,3,4 2,3,4
J 3 1 2,3 2,3 2,3
*X* 4 1 2 3 4

The last row denotes the 4 objects.

Now I want to generate the choice sets for my questionnaire. I would like
to use *orthogonal fractional factorial design*. I kept the row with *X* in
order to sort out the redundant combinations from the choice sets.

I have the following questions-
1. *How to decide on the number of runs that one has to chose for
fractional factorial design?*  I used *AlgDesign* to generate the full
factorial which consists of 0.768 million combinations. So, I need a modest
number of runs, but how much should I target? I do not see any document
where one explains how to choose the number of trials/experimental runs.
The papers I am following only tell that they have used N number of runs
instead of the full factorial.

2. Out of 0.768 million combinations in the full factorial, there will be
many which are redundant. For example- I don't want those rows where (X=X1)
and A=(2 or 3 or 4 or 5). There are many other such cases which I don't
want in my design. I have coded all levels for each attribute and that's
why they are in the full factorial. *How do I generate an orthogonal
fractional factorial so that it does not contain such redundant
combinations?* I included the X attribute with the purpose of dropping
those combinations conditioned upon specific values of X and other factors.
Should I execute that and then generate the fractional factorial using
*optFederov* from the remaining data in the dataframe?

I would be highly obliged if you can kindly help me in this regard. I am a
student of Economics, so I do not have very deep understanding of the
statistical procedure of such algorithms. So, my question might sound
extremely naive for which I am sorry.


-- Regards,
Rahul Chakraborty
Research Fellow
National Institute of Public Finance and Policy
New Delhi- 110067

	[[alternative HTML version deleted]]


From L@urentRHe|p @end|ng |rom |ree@|r  Wed May 27 10:56:42 2020
From: L@urentRHe|p @end|ng |rom |ree@|r (Laurent Rhelp)
Date: Wed, 27 May 2020 10:56:42 +0200
Subject: [R] iterators : checkFunc with ireadLines
In-Reply-To: <CAA99HCwONVzHizUuG17DO=+PF7SVVY8W9V+SKhpT5L=MW6_1NQ@mail.gmail.com>
References: <72174dd3-a819-b5fd-5a6f-2a6b46342774-2015@free.fr>
 <20200520104614.45876937@Tarkus>
 <27f44720-d368-c2cc-39c9-8629da885165@free.fr>
 <CAA99HCwONVzHizUuG17DO=+PF7SVVY8W9V+SKhpT5L=MW6_1NQ@mail.gmail.com>
Message-ID: <b27578f2-824f-86bd-f410-2f3781f2da3e@free.fr>

I installed raku on my PC to test your solution:

The command raku -e '.put for lines.grep( / ^^N053 | ^^N163 /, :p );'? 
Laurents.txt works fine when I write it in the bash command but when I 
use the pipe command in R as you say there is nothing in lines with 
lines <- read.table(i)

There is the same problem with Ivan's solution the command grep -E 
'^(N053|N163)' test.txt works fine under the bash command but not i <- 
pipe("grep -E '^(N053|N163)' test.txt"); lines <- read.table(i)

May be it is because I work with MS windows ?

thx
LP




Le 24/05/2020 ? 04:34, William Michels a ?crit?:
> Hi Laurent,
>
> Seeking to give you an "R-only" solution, I thought the read.fwf()
> function might be useful (to read-in your first column of data, only).
> However Jeff is correct that this is a poor strategy, since read.fwf()
> reads the entire file into R (documented in "Fixed-width-format
> files", Section 2.2: R Data Import/Export Manual).
>
> Jeff has suggested a number of packages, as well as using a database.
> Ivan Krylov has posted answers using grep, awk and perl (perl5--to
> disambiguate). [In point of fact, the R Data Import/Export Manual
> suggests using perl]. Similar to Ivan, I've posted code below using
> the Raku programming language (the language formerly known as Perl6).
> Regexes are claimed to be more readable, but are currently very slow
> in Raku. However on the plus side, the language is designed to handle
> Unicode gracefully:
>
>> # pipe() using raku-grep on Laurent's data (sep=mult whitespace):
>> con_obj1 <- pipe(paste("raku -e '.put for lines.grep( / ^^N053 | ^^N163 /, :p );' ", "Laurents.txt"), open="rt");
>> p6_import_a <- scan(file=con_obj1, what=list("","","","","","","","","",""), flush=TRUE, multi.line=FALSE, quiet=TRUE);
>> close(con_obj1);
>> as.data.frame(sapply(p6_import_a, t), stringsAsFactors=FALSE);
>    V1   V2        V3        V4        V5        V6        V7        V8
>        V9       V10
> 1  2 N053 -0.014083 -0.004741  0.001443 -0.010152 -0.012996 -0.005337
> -0.008738 -0.015094
> 2  4 N163 -0.054023 -0.049345 -0.037158  -0.04112 -0.044612 -0.036953
> -0.036061 -0.044516
>> # pipe() using raku-grep "starts-with" to find genbankID ( >3GB TSV file)
>> # "lines[0..5]" restricts raku to reading first 6 lines!
>> # change "lines[0..5]" to "lines" to run raku code on whole file:
>> con_obj2 <- pipe(paste("raku -e '.put for lines[0..5].grep( *.starts-with(q[A00145]), :p);' ", "genbankIDs_3GB.tsv"), "rt");
>> p6_import_b <- read.table(con_obj2, sep="\t");
>> close(con_obj2)
>> p6_import_b
>    V1     V2       V3          V4 V5
> 1  4 A00145 A00145.1 IFN-alpha A NA
>> # unicode test using R's system() function:
>> try(system("raku -ne '.grep( /  ??  |  ?????  |  ?????  |  ??????  /, :v ).put;'  hello_7lang.txt", intern = TRUE, ignore.stderr = FALSE))
> [1] ""                    ""                    ""
> "?? Chinese"
> [5] "????? Japanese" "????? Arabic"        "?????? Russian"
> [special thanks to Brad Gilbert, Joseph Brenner and others on the
> perl6-users mailing list. All errors above are my own.]
>
> HTH, Bill.
>
> W. Michels, Ph.D.
>
>
>
>
> On Fri, May 22, 2020 at 4:48 AM Laurent Rhelp <LaurentRHelp at free.fr> wrote:
>> Hi Ivan,
>>     Endeed, it is a good idea. I am under MSwindows but I can use the
>> bash command I use with git. I will see how to do that with the unix
>> command lines.
>>
>>
>> Le 20/05/2020 ? 09:46, Ivan Krylov a ?crit :
>>> Hi Laurent,
>>>
>>> I am not saying this will work every time and I do recognise that this
>>> is very different from a more general solution that you had envisioned,
>>> but if you are on an UNIX-like system or have the relevant utilities
>>> installed and on the %PATH% on Windows, you can filter the input file
>>> line-by-line using a pipe and an external program:
>>>
>>> On Sun, 17 May 2020 15:52:30 +0200
>>> Laurent Rhelp <LaurentRHelp at free.fr> wrote:
>>>
>>>> # sensors to keep
>>>> sensors <-  c("N053", "N163")
>>> # filter on the beginning of the line
>>> i <- pipe("grep -E '^(N053|N163)' test.txt")
>>> # or:
>>> # filter on the beginning of the given column
>>> # (use $2 for the second column, etc.)
>>> i <- pipe("awk '($1 ~ \"^(N053|N163)\")' test.txt")
>>> # or:
>>> # since your message is full of Unicode non-breaking spaces, I have to
>>> # bring in heavier machinery to handle those correctly;
>>> # only this solution manages to match full column values
>>> # (here you can also use $F[1] for second column and so on)
>>> i <- pipe("perl -CSD -F'\\s+' -lE \\
>>>    'print join qq{\\t}, @F if $F[0] =~ /^(N053|N163)$/' \\
>>>    test.txt
>>> ")
>>> lines <- read.table(i) # closes i when done
>>>
>>> The downside of this approach is having to shell-escape the command
>>> lines, which can become complicated, and choosing between use of regular
>>> expressions and more wordy programs (Unicode whitespace in the input
>>> doesn't help, either).
>>>
>>
>> --
>> L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e par le logiciel antivirus Avast.
>> https://www.avast.com/antivirus
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.



-- 
L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e par le logiciel antivirus Avast.
https://www.avast.com/antivirus


From r@v|76 @end|ng |rom gm@||@com  Wed May 27 13:10:33 2020
From: r@v|76 @end|ng |rom gm@||@com (Ravi Jeyaraman)
Date: Wed, 27 May 2020 07:10:33 -0400
Subject: [R] read_excel() ignore case of worksheet name?
In-Reply-To: <CA+vqiLF=Nm=dbPBHrK51i1ZDox36nNL=zpOhEGQwbAqd1MdAvA@mail.gmail.com>
References: <002401d633cd$6f390b50$4dab21f0$@gmail.com>
 <CACxE24nFOyeY1bQ9HaWw8ZqJjoH3xviv7FiTV8NtsKKFx-Bjqw@mail.gmail.com>
 <00a601d633d2$d9a55c50$8cf014f0$@gmail.com>
 <CA+vqiLF=Nm=dbPBHrK51i1ZDox36nNL=zpOhEGQwbAqd1MdAvA@mail.gmail.com>
Message-ID: <00c801d63417$7114a980$533dfc80$@gmail.com>

Ista, With few tweaks this worked beautifully.  Thank you so much.  

-----Original Message-----
From: Ista Zahn [mailto:istazahn at gmail.com] 
Sent: Tuesday, May 26, 2020 11:38 PM
To: Ravi Jeyaraman <ravi76 at gmail.com>
Cc: Erin Hodgess <erinm.hodgess at gmail.com>; r-help at r-project.org
Subject: Re: [R] read_excel() ignore case of worksheet name?

How about

read_excel_table <- function(x) {
  readxl::read_excel(
    x,
    sheet=grep("tables",
               excel_sheets(x),
               ignore.case = TRUE,
               value = TRUE),
    .name_repair = fixColNames
  )
}

lapply(SIS$FULL_FILEPATH, read_excel_table)


--Ista

On Tue, May 26, 2020 at 11:05 PM Ravi Jeyaraman <ravi76 at gmail.com> wrote:
>
> I?ve already tried that and doesn?t work
>
>
>
> From: Erin Hodgess [mailto:erinm.hodgess at gmail.com]
> Sent: Tuesday, May 26, 2020 10:55 PM
> To: Ravi Jeyaraman <ravi76 at gmail.com>
> Cc: r-help at r-project.org
> Subject: Re: [R] read_excel() ignore case of worksheet name?
>
>
>
> Here?s a thought, please.  Could you use the tolower function and make them all lower case?
>
>
>
> Thanks,
>
> Erin
>
>
>
> On Tue, May 26, 2020 at 8:21 PM Ravi Jeyaraman <ravi76 at gmail.com <mailto:ravi76 at gmail.com> > wrote:
>
> Hello All, Is there any parameter to make read_excel() ignore the case-sensitiveness of the worksheet?   I'm using the below to ready in multiple spreadsheets and it works perfectly fine if the worksheet is named 'Tables', but fails when it's named ' TABLES'.  Any thoughts?
>
> lapply(1:nrow(SIS), function(x) 
> readxl::read_excel(SIS$FULL_FILEPATH[x], sheet='Tables', .name_repair 
> = fixColNames))
>
> Thanks in advance for your response.
>
> Cheers
> Ravi
>
>
>
> --
> This email has been checked for viruses by AVG.
> https://www.avg.com
>
> ______________________________________________
> R-help at r-project.org <mailto:R-help at r-project.org>  mailing list -- To 
> UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> --
>
> Erin Hodgess, PhD
>
> mailto: erinm.hodgess at gmail.com <mailto:erinm.hodgess at gmail.com>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
This email has been checked for viruses by AVG.
https://www.avg.com


From kry|ov@r00t @end|ng |rom gm@||@com  Wed May 27 14:41:43 2020
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Wed, 27 May 2020 15:41:43 +0300
Subject: [R] iterators : checkFunc with ireadLines
In-Reply-To: <b27578f2-824f-86bd-f410-2f3781f2da3e@free.fr>
References: <72174dd3-a819-b5fd-5a6f-2a6b46342774-2015@free.fr>
 <20200520104614.45876937@Tarkus>
 <27f44720-d368-c2cc-39c9-8629da885165@free.fr>
 <CAA99HCwONVzHizUuG17DO=+PF7SVVY8W9V+SKhpT5L=MW6_1NQ@mail.gmail.com>
 <b27578f2-824f-86bd-f410-2f3781f2da3e@free.fr>
Message-ID: <20200527154143.2d2b6e0a@Tarkus>

On Wed, 27 May 2020 10:56:42 +0200
Laurent Rhelp <LaurentRHelp at free.fr> wrote:

> May be it is because I work with MS windows ?

That is probably the case.

On Windows, pipe() invokes "%COMSPEC% /c <description>", and the
rules of command line quoting are different between POSIX shell and
cmd.exe + runtimes of Windows applications [*].

Can you run raku / perl / grep / awk from the cmd prompt? If not, check
your %PATH% variable. Either way, shQuote() is supposed to be able to
handle the quoting madness for us, the inner call performing the
quoting for the runtime and the outer call escaping for the cmd.exe
itself:

pipe(shQuote(
 paste(
  'raku', '-e',
  shQuote('.put for lines.grep( / ^^N053 | ^^N163 /, :p );'),
  'Laurents.txt'
 ),
 type = 'cmd2'
))

pipe(shQuote(
 paste('grep', '-E', shQuote('^(N053|N163)'), 'test.txt'),
 'cmd2'
))

pipe(shQuote(
 paste('awk', shQuote('($1 ~ "^(N053|N163)")'), 'test.txt'),
 'cmd2'
))

pipe(shQuote(
 paste(
  'perl', '-CSD', '-F', shQuote('\\s+'), '-lE',
  shQuote('print join qq{\\t}, @F if $F[0] =~ /^(N053|N163)$/'),
  'test.txt'
 ), 'cmd2'
))

This way, we can even pretend that we are passing an _array_ of command
line arguments to the child process, like K&R intended, and not
building a command _line_ to be interpreted by the command line
interpreter and application runtime.

-- 
Best regards,
Ivan

[*] In POSIX, the command line is an array of NUL-terminated C strings.
In Windows, the command line is a single NUL-terminated C string, so
the runtime of the application is responsible for obtaining an array of
command line arguments from that:
https://docs.microsoft.com/ru-ru/archive/blogs/twistylittlepassagesallalike/everyone-quotes-command-line-arguments-the-wrong-way


From bgunter@4567 @end|ng |rom gm@||@com  Wed May 27 16:14:23 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 27 May 2020 07:14:23 -0700
Subject: [R] Problem in generating an "Orthogonal fractional design"
In-Reply-To: <CAEmZPS=d7XgpM2_znpJS=dx_4Qg18yrfQ4nRUPqR00vKtH5v9Q@mail.gmail.com>
References: <CAEmZPS=d7XgpM2_znpJS=dx_4Qg18yrfQ4nRUPqR00vKtH5v9Q@mail.gmail.com>
Message-ID: <CAGxFJbRaQ-GJCPh0yrcTx4MTZerXNZu+qSd2De_uDrz1A=L0bw@mail.gmail.com>

Sorry, Off topic. This list deals with R programming questions, not
statistical questions. Try stats.stackexchange.com for those.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, May 27, 2020 at 1:25 AM Rahul Chakraborty <chakrarahul at gmail.com>
wrote:

> Dear all,
>
> Presently I am working on designing a questionnaire for my discrete choice
> experiment. I want to generate an orthogonal fractional factorial design
> for the following problem-
>
> The respondent has to choose one out of 4 objects (*X1, X2, X3, X4*). Each
> of the 4 objects are classified by 10 different attributes. However, the
> levels are not the same under each of the objects. The table below displays
> the situation.
>
> Attributes No. of Levels  Choices and values
> X1 X2 X3 X4
> A 5 1 1,2,3 3,4,5 3,4,5
> B 4 1 1 1,2 3,4
> C 4 1 1 2,4 3,4
> D 5 1 1,2,3 1,2,3 1,4,5
> E  5 1,2 2,3 3,4 5
> F 2 1 1 1,2 1,2
> G 2 1 1 1,2 2
> H 2 1 1 1,2 1,2
> I 4 1 2,3,4 2,3,4 2,3,4
> J 3 1 2,3 2,3 2,3
> *X* 4 1 2 3 4
>
> The last row denotes the 4 objects.
>
> Now I want to generate the choice sets for my questionnaire. I would like
> to use *orthogonal fractional factorial design*. I kept the row with *X* in
> order to sort out the redundant combinations from the choice sets.
>
> I have the following questions-
> 1. *How to decide on the number of runs that one has to chose for
> fractional factorial design?*  I used *AlgDesign* to generate the full
> factorial which consists of 0.768 million combinations. So, I need a modest
> number of runs, but how much should I target? I do not see any document
> where one explains how to choose the number of trials/experimental runs.
> The papers I am following only tell that they have used N number of runs
> instead of the full factorial.
>
> 2. Out of 0.768 million combinations in the full factorial, there will be
> many which are redundant. For example- I don't want those rows where (X=X1)
> and A=(2 or 3 or 4 or 5). There are many other such cases which I don't
> want in my design. I have coded all levels for each attribute and that's
> why they are in the full factorial. *How do I generate an orthogonal
> fractional factorial so that it does not contain such redundant
> combinations?* I included the X attribute with the purpose of dropping
> those combinations conditioned upon specific values of X and other factors.
> Should I execute that and then generate the fractional factorial using
> *optFederov* from the remaining data in the dataframe?
>
> I would be highly obliged if you can kindly help me in this regard. I am a
> student of Economics, so I do not have very deep understanding of the
> statistical procedure of such algorithms. So, my question might sound
> extremely naive for which I am sorry.
>
>
> -- Regards,
> Rahul Chakraborty
> Research Fellow
> National Institute of Public Finance and Policy
> New Delhi- 110067
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@@@hton @end|ng |rom endur|ng|nve@tment@@com  Wed May 27 19:46:13 2020
From: m@@@hton @end|ng |rom endur|ng|nve@tment@@com (Michael Ashton)
Date: Wed, 27 May 2020 17:46:13 +0000
Subject: [R] struggling with apply
Message-ID: <74e69784edb54827a033eb7fe3c7a337@MBX084-W1-CA-3.exch084.serverpod.net>

Hi -

I have a matrix of n rows and 4 columns.

I want to cap the value in each column by a different upper bound. So, suppose my matrix is

somematrix <- matrix(c(1,4,3,6,3,9,12,8,5,7,11,11),nrow=3,ncol=4)
> somematrix
     [,1] [,2] [,3] [,4]
[1,]    1    6   12    7
[2,]    4    3    8   11
[3,]    3    9    5   11

Now I want to have the maximum value in each column described by
UB=c(2.5, 5.5, 8.5, 10.5)

So that the right answer will look like:
     [,1]      [,2]    [,3]   [,4]
[1,]    1      5.5     8.5    7
[2,]    2.5    3        8     10.5
[3,]    2.5   5.5      5    10.5

I've tried a few things, like:
newmatrix <- apply(somematrix,c(1,2),function(x) min(UB,x))

but I can't figure out to apply the relevant element of the UB list to the right element of the matrix. When I run the above, for example, it takes min(UB,x) over all UB, so I get:

newmatrix
     [,1] [,2] [,3] [,4]
[1,]  1.0  2.5  2.5  2.5
[2,]  2.5  2.5  2.5  2.5
[3,]  2.5  2.5  2.5  2.5

I'm sure there's a simple and elegant solution but I don't know what it is!

Thanks in advance,

Mike

Michael Ashton, CFA
Managing Principal

Enduring Investments LLC
W: 973.457.4602
C: 551.655.8006
Schedule a Call: https://calendly.com/m-ashton


	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed May 27 19:47:57 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 27 May 2020 10:47:57 -0700
Subject: [R] struggling with apply
In-Reply-To: <74e69784edb54827a033eb7fe3c7a337@MBX084-W1-CA-3.exch084.serverpod.net>
References: <74e69784edb54827a033eb7fe3c7a337@MBX084-W1-CA-3.exch084.serverpod.net>
Message-ID: <80B58388-E4D3-437A-B6A4-ED175F2A23BD@dcn.davis.ca.us>

Use the pmin function, not min, for this purpose.

On May 27, 2020 10:46:13 AM PDT, Michael Ashton <m.ashton at enduringinvestments.com> wrote:
>Hi -
>
>I have a matrix of n rows and 4 columns.
>
>I want to cap the value in each column by a different upper bound. So,
>suppose my matrix is
>
>somematrix <- matrix(c(1,4,3,6,3,9,12,8,5,7,11,11),nrow=3,ncol=4)
>> somematrix
>     [,1] [,2] [,3] [,4]
>[1,]    1    6   12    7
>[2,]    4    3    8   11
>[3,]    3    9    5   11
>
>Now I want to have the maximum value in each column described by
>UB=c(2.5, 5.5, 8.5, 10.5)
>
>So that the right answer will look like:
>     [,1]      [,2]    [,3]   [,4]
>[1,]    1      5.5     8.5    7
>[2,]    2.5    3        8     10.5
>[3,]    2.5   5.5      5    10.5
>
>I've tried a few things, like:
>newmatrix <- apply(somematrix,c(1,2),function(x) min(UB,x))
>
>but I can't figure out to apply the relevant element of the UB list to
>the right element of the matrix. When I run the above, for example, it
>takes min(UB,x) over all UB, so I get:
>
>newmatrix
>     [,1] [,2] [,3] [,4]
>[1,]  1.0  2.5  2.5  2.5
>[2,]  2.5  2.5  2.5  2.5
>[3,]  2.5  2.5  2.5  2.5
>
>I'm sure there's a simple and elegant solution but I don't know what it
>is!
>
>Thanks in advance,
>
>Mike
>
>Michael Ashton, CFA
>Managing Principal
>
>Enduring Investments LLC
>W: 973.457.4602
>C: 551.655.8006
>Schedule a Call: https://calendly.com/m-ashton
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed May 27 19:50:36 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 27 May 2020 18:50:36 +0100
Subject: [R] struggling with apply
In-Reply-To: <74e69784edb54827a033eb7fe3c7a337@MBX084-W1-CA-3.exch084.serverpod.net>
References: <74e69784edb54827a033eb7fe3c7a337@MBX084-W1-CA-3.exch084.serverpod.net>
Message-ID: <b7ff69b9-56dd-6311-293a-15871e03c0a6@sapo.pt>

Hello,

Try pmin. And loop by column/UB index with sapply/seq_along.


sapply(seq_along(UB), function(i) pmin(UB[i], somematrix[,i]))
#     [,1] [,2] [,3] [,4]
#[1,]  1.0  5.5  8.5  7.0
#[2,]  2.5  3.0  8.0 10.5
#[3,]  2.5  5.5  5.0 10.5


Hope this helps,

Rui Barradas


?s 18:46 de 27/05/20, Michael Ashton escreveu:
> Hi -
> 
> I have a matrix of n rows and 4 columns.
> 
> I want to cap the value in each column by a different upper bound. So, suppose my matrix is
> 
> somematrix <- matrix(c(1,4,3,6,3,9,12,8,5,7,11,11),nrow=3,ncol=4)
>> somematrix
>       [,1] [,2] [,3] [,4]
> [1,]    1    6   12    7
> [2,]    4    3    8   11
> [3,]    3    9    5   11
> 
> Now I want to have the maximum value in each column described by
> UB=c(2.5, 5.5, 8.5, 10.5)
> 
> So that the right answer will look like:
>       [,1]      [,2]    [,3]   [,4]
> [1,]    1      5.5     8.5    7
> [2,]    2.5    3        8     10.5
> [3,]    2.5   5.5      5    10.5
> 
> I've tried a few things, like:
> newmatrix <- apply(somematrix,c(1,2),function(x) min(UB,x))
> 
> but I can't figure out to apply the relevant element of the UB list to the right element of the matrix. When I run the above, for example, it takes min(UB,x) over all UB, so I get:
> 
> newmatrix
>       [,1] [,2] [,3] [,4]
> [1,]  1.0  2.5  2.5  2.5
> [2,]  2.5  2.5  2.5  2.5
> [3,]  2.5  2.5  2.5  2.5
> 
> I'm sure there's a simple and elegant solution but I don't know what it is!
> 
> Thanks in advance,
> 
> Mike
> 
> Michael Ashton, CFA
> Managing Principal
> 
> Enduring Investments LLC
> W: 973.457.4602
> C: 551.655.8006
> Schedule a Call: https://calendly.com/m-ashton
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From m@@@hton @end|ng |rom endur|ng|nve@tment@@com  Wed May 27 19:52:36 2020
From: m@@@hton @end|ng |rom endur|ng|nve@tment@@com (Michael Ashton)
Date: Wed, 27 May 2020 17:52:36 +0000
Subject: [R] struggling with apply
In-Reply-To: <b7ff69b9-56dd-6311-293a-15871e03c0a6@sapo.pt>
References: <74e69784edb54827a033eb7fe3c7a337@MBX084-W1-CA-3.exch084.serverpod.net>
 <b7ff69b9-56dd-6311-293a-15871e03c0a6@sapo.pt>
Message-ID: <4b8584b2fbcd46ea9b2457461c2217c0@MBX084-W1-CA-3.exch084.serverpod.net>

That's it! Thanks. Learn something new every day!

Michael Ashton, CFA
Managing Principal

Enduring Investments LLC
W: 973.457.4602
C: 551.655.8006
Schedule a Call: https://calendly.com/m-ashton


-----Original Message-----
From: Rui Barradas [mailto:ruipbarradas at sapo.pt] 
Sent: Wednesday, May 27, 2020 1:51 PM
To: Michael Ashton; r-help at r-project.org
Subject: Re: [R] struggling with apply

Hello,

Try pmin. And loop by column/UB index with sapply/seq_along.


sapply(seq_along(UB), function(i) pmin(UB[i], somematrix[,i]))
#     [,1] [,2] [,3] [,4]
#[1,]  1.0  5.5  8.5  7.0
#[2,]  2.5  3.0  8.0 10.5
#[3,]  2.5  5.5  5.0 10.5


Hope this helps,

Rui Barradas


?s 18:46 de 27/05/20, Michael Ashton escreveu:
> Hi -
> 
> I have a matrix of n rows and 4 columns.
> 
> I want to cap the value in each column by a different upper bound. So, suppose my matrix is
> 
> somematrix <- matrix(c(1,4,3,6,3,9,12,8,5,7,11,11),nrow=3,ncol=4)
>> somematrix
>       [,1] [,2] [,3] [,4]
> [1,]    1    6   12    7
> [2,]    4    3    8   11
> [3,]    3    9    5   11
> 
> Now I want to have the maximum value in each column described by
> UB=c(2.5, 5.5, 8.5, 10.5)
> 
> So that the right answer will look like:
>       [,1]      [,2]    [,3]   [,4]
> [1,]    1      5.5     8.5    7
> [2,]    2.5    3        8     10.5
> [3,]    2.5   5.5      5    10.5
> 
> I've tried a few things, like:
> newmatrix <- apply(somematrix,c(1,2),function(x) min(UB,x))
> 
> but I can't figure out to apply the relevant element of the UB list to the right element of the matrix. When I run the above, for example, it takes min(UB,x) over all UB, so I get:
> 
> newmatrix
>       [,1] [,2] [,3] [,4]
> [1,]  1.0  2.5  2.5  2.5
> [2,]  2.5  2.5  2.5  2.5
> [3,]  2.5  2.5  2.5  2.5
> 
> I'm sure there's a simple and elegant solution but I don't know what it is!
> 
> Thanks in advance,
> 
> Mike
> 
> Michael Ashton, CFA
> Managing Principal
> 
> Enduring Investments LLC
> W: 973.457.4602
> C: 551.655.8006
> Schedule a Call: https://calendly.com/m-ashton
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Wed May 27 19:59:21 2020
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Wed, 27 May 2020 17:59:21 +0000 (UTC)
Subject: [R] [External] Get a result but an error message as well ?
In-Reply-To: <a767ff0b-1f83-c977-b834-4067ef602924@sapo.pt>
References: <232159068.980863.1589917874331.ref@mail.yahoo.com>
 <232159068.980863.1589917874331@mail.yahoo.com>
 <CAGx1TMAZA7sJBZ343MEXwD=eZCM0V9_VLTSjw+QEvbniqDCepA@mail.gmail.com>
 <1667554344.1012305.1589920704261@mail.yahoo.com>
 <79e7fdeb-fe14-dcc0-f998-1c7307eb759b@sapo.pt>
 <1253484374.1044092.1589923503225@mail.yahoo.com>
 <a767ff0b-1f83-c977-b834-4067ef602924@sapo.pt>
Message-ID: <2104852885.1000862.1590602361853@mail.yahoo.com>

Hi Rui, Richard and other,

Many thanks for your responses that solve my problem.

Best,
SV







Le mercredi 20 mai 2020 ? 09:09:49 UTC+2, Rui Barradas <ruipbarradas at sapo.pt> a ?crit : 





Hello,

I get an error but when running colnames(), not mean().
Notes:

1. I have changed the way the residuals are extracted, if there is a 
function resid(), the recommended practice is to use it.
2. c("MSE_OLS") and "MSE_OLS" are the identical() objects. Likewise, to 
return( c(MSE_OLS) ) is to have only MSE_OLS as the last function line. 
I find the latter clearer.


my.experiment <- function() {
? OLS <- lm(a ~ b + d)
? #MSE_OLS <- mean(OLS$residuals^2)
? MSE_OLS <- mean(resid(OLS)^2)
? MSE_OLS
}

my.data <- replicate( 500, my.experiment() )

class(my.data)? # it's a vector, not a matrix
#[1] "numeric"

colnames(my.data) <- "MSE_OLS"
#Error in `colnames<-`(`*tmp*`, value = "MSE_OLS") :
#? attempt to set 'colnames' on an object with less than two dimensions

mean(my.data)
#[1] 105.6951


Hope this helps,

Rui Barradas

?s 22:25 de 19/05/20, varin sacha escreveu:
> Hi Rui,
> 
> If I don't transpose t() the output of the replicate (my R code here below) I still get an error message !!
> 
> ########################################
> a=c(2,4,3,4,6,5,3,1,2,3,4,3,4,5,65)
> b=c(23,45,32,12,23,43,56,44,33,11,12,54,23,34,54)
> d=c(9,4,5,3,2,1,3,4,5,6,4,9,10,11,18)
> 
> my.experiment <- function() {
> 
> OLS <- lm( a ~ b+d )
> 
> MSE_OLS<-mean(OLS$residuals^2)
> 
> return( c(MSE_OLS) )
> }
> 
> my.data = replicate( 500, my.experiment() )
> colnames(my.data) <- c("MSE_OLS")
> mean(my.data)
> ########################################
> 
> 
> 
> 
> 
> 
> Le mardi 19 mai 2020 ? 23:14:21 UTC+2, Rui Barradas <ruipbarradas at sapo.pt> a ?crit :
> 
> 
> 
> 
> 
> Hello,
> 
> Inline.
> 
> ?s 21:38 de 19/05/20, varin sacha via R-help escreveu:
>>
>> Hi Richard,
>>
>> Thanks for your response.
>> However, how can I correct my R code knowing that I want, as a result, only one value : the mean of the 500 MSE_OLS values ?
> 
> Just don't transpose the output of replicate?
> 
> Hope this helps,
> 
> Rui Barradas
> 
>>
>>
>>
>>
>>
>>
>>
>> Le mardi 19 mai 2020 ? 21:59:07 UTC+2, Richard M. Heiberger <rmh at temple.edu> a ?crit :
>>
>>
>>
>>
>>
>>> dim(my.data)
>> [1] ? 1 500
>>
>> you have a matrix with a single row and 500 columns.
>> you gave a name to only the first column.
>>
>> Look at the result of replicate().? it is a vector.? You transposed it into a one-row matrix.
>>
>>>? ? ?tmp <- replicate( 500, my.experiment() )
>>> dim(tmp)
>> NULL
>>> length(tmp)
>> [1] 500
>>> dim(t(tmp))
>> [1] ? 1 500
>>
>>
>> On Tue, May 19, 2020 at 3:51 PM varin sacha via R-help <r-help at r-project.org> wrote:
>>> Dear R-experts,
>>>
>>> Here is my R code, I get a result but I also get an error message so I doubt I can trust the result I get.
>>> What is going wrong ? Many thanks.
>>>
>>> ########################################
>>> a<-c(2,4,3,4,6,5,3,1,2,3,4,3,4,5,65)
>>> b<-c(23,45,32,12,23,43,56,44,33,11,12,54,23,34,54)
>>> d<-c(9,4,5,3,2,1,3,4,5,6,4,9,10,11,18)
>>>
>>> my.experiment <- function( ) {
>>>
>>> OLS <- lm( a ~ b+d )
>>> MSE_OLS<-mean(OLS$residuals^2)
>>> return( c(MSE_OLS) )
>>> }
>>>
>>> my.data = t(replicate( 500, my.experiment() ))
>>> colnames(my.data) <- c("MSE_OLS")
>>> mean(my.data)
>>> ########################################
>>>? ? 
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>


From rmh @end|ng |rom temp|e@edu  Wed May 27 20:03:46 2020
From: rmh @end|ng |rom temp|e@edu (Richard M. Heiberger)
Date: Wed, 27 May 2020 14:03:46 -0400
Subject: [R] [External]  struggling with apply
In-Reply-To: <74e69784edb54827a033eb7fe3c7a337@MBX084-W1-CA-3.exch084.serverpod.net>
References: <74e69784edb54827a033eb7fe3c7a337@MBX084-W1-CA-3.exch084.serverpod.net>
Message-ID: <CAGx1TMD751rsFG0abK663ZtStvn=sG1mqfmtKo2zT+4KM6dWBw@mail.gmail.com>

sapply(1:4, FUN=function(i, x, UB=c(2.5, 5.5, 8.5, 10.5)) {result <-
x[,i]; result[result > UB[i]] <- UB[i]; result}, x=somematrix)

On Wed, May 27, 2020 at 1:46 PM Michael Ashton
<m.ashton at enduringinvestments.com> wrote:
>
> Hi -
>
> I have a matrix of n rows and 4 columns.
>
> I want to cap the value in each column by a different upper bound. So, suppose my matrix is
>
> somematrix <- matrix(c(1,4,3,6,3,9,12,8,5,7,11,11),nrow=3,ncol=4)
> > somematrix
>      [,1] [,2] [,3] [,4]
> [1,]    1    6   12    7
> [2,]    4    3    8   11
> [3,]    3    9    5   11
>
> Now I want to have the maximum value in each column described by
> UB=c(2.5, 5.5, 8.5, 10.5)
>
> So that the right answer will look like:
>      [,1]      [,2]    [,3]   [,4]
> [1,]    1      5.5     8.5    7
> [2,]    2.5    3        8     10.5
> [3,]    2.5   5.5      5    10.5
>
> I've tried a few things, like:
> newmatrix <- apply(somematrix,c(1,2),function(x) min(UB,x))
>
> but I can't figure out to apply the relevant element of the UB list to the right element of the matrix. When I run the above, for example, it takes min(UB,x) over all UB, so I get:
>
> newmatrix
>      [,1] [,2] [,3] [,4]
> [1,]  1.0  2.5  2.5  2.5
> [2,]  2.5  2.5  2.5  2.5
> [3,]  2.5  2.5  2.5  2.5
>
> I'm sure there's a simple and elegant solution but I don't know what it is!
>
> Thanks in advance,
>
> Mike
>
> Michael Ashton, CFA
> Managing Principal
>
> Enduring Investments LLC
> W: 973.457.4602
> C: 551.655.8006
> Schedule a Call: https://calendly.com/m-ashton
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Wed May 27 20:22:06 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 27 May 2020 11:22:06 -0700
Subject: [R] struggling with apply
In-Reply-To: <b7ff69b9-56dd-6311-293a-15871e03c0a6@sapo.pt>
References: <74e69784edb54827a033eb7fe3c7a337@MBX084-W1-CA-3.exch084.serverpod.net>
 <b7ff69b9-56dd-6311-293a-15871e03c0a6@sapo.pt>
Message-ID: <CAGxFJbTUds+nWxdu+HG1=aBSxuA4SEVmDhX8emiq-RCMkUuf+A@mail.gmail.com>

Better, I think (no indexing):

t(apply(somematrix,1,function(x)pmin(x,UB)))


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, May 27, 2020 at 10:56 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> Try pmin. And loop by column/UB index with sapply/seq_along.
>
>
> sapply(seq_along(UB), function(i) pmin(UB[i], somematrix[,i]))
> #     [,1] [,2] [,3] [,4]
> #[1,]  1.0  5.5  8.5  7.0
> #[2,]  2.5  3.0  8.0 10.5
> #[3,]  2.5  5.5  5.0 10.5
>
>
> Hope this helps,
>
> Rui Barradas
>
>
> ?s 18:46 de 27/05/20, Michael Ashton escreveu:
> > Hi -
> >
> > I have a matrix of n rows and 4 columns.
> >
> > I want to cap the value in each column by a different upper bound. So,
> suppose my matrix is
> >
> > somematrix <- matrix(c(1,4,3,6,3,9,12,8,5,7,11,11),nrow=3,ncol=4)
> >> somematrix
> >       [,1] [,2] [,3] [,4]
> > [1,]    1    6   12    7
> > [2,]    4    3    8   11
> > [3,]    3    9    5   11
> >
> > Now I want to have the maximum value in each column described by
> > UB=c(2.5, 5.5, 8.5, 10.5)
> >
> > So that the right answer will look like:
> >       [,1]      [,2]    [,3]   [,4]
> > [1,]    1      5.5     8.5    7
> > [2,]    2.5    3        8     10.5
> > [3,]    2.5   5.5      5    10.5
> >
> > I've tried a few things, like:
> > newmatrix <- apply(somematrix,c(1,2),function(x) min(UB,x))
> >
> > but I can't figure out to apply the relevant element of the UB list to
> the right element of the matrix. When I run the above, for example, it
> takes min(UB,x) over all UB, so I get:
> >
> > newmatrix
> >       [,1] [,2] [,3] [,4]
> > [1,]  1.0  2.5  2.5  2.5
> > [2,]  2.5  2.5  2.5  2.5
> > [3,]  2.5  2.5  2.5  2.5
> >
> > I'm sure there's a simple and elegant solution but I don't know what it
> is!
> >
> > Thanks in advance,
> >
> > Mike
> >
> > Michael Ashton, CFA
> > Managing Principal
> >
> > Enduring Investments LLC
> > W: 973.457.4602
> > C: 551.655.8006
> > Schedule a Call: https://calendly.com/m-ashton
> >
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@@@hton @end|ng |rom endur|ng|nve@tment@@com  Wed May 27 20:25:59 2020
From: m@@@hton @end|ng |rom endur|ng|nve@tment@@com (Michael Ashton)
Date: Wed, 27 May 2020 18:25:59 +0000
Subject: [R] struggling with apply
In-Reply-To: <CAGxFJbTUds+nWxdu+HG1=aBSxuA4SEVmDhX8emiq-RCMkUuf+A@mail.gmail.com>
References: <74e69784edb54827a033eb7fe3c7a337@MBX084-W1-CA-3.exch084.serverpod.net>
 <b7ff69b9-56dd-6311-293a-15871e03c0a6@sapo.pt>
 <CAGxFJbTUds+nWxdu+HG1=aBSxuA4SEVmDhX8emiq-RCMkUuf+A@mail.gmail.com>
Message-ID: <540b99385d724282b0fac3b133ba97e3@MBX084-W1-CA-3.exch084.serverpod.net>

Always amazes me how many ways there are to do these things, none of which I was able to find myself. Thanks! I think the key here was ?pmin,? which I didn?t know before.

Michael Ashton, CFA
Managing Principal

Enduring Investments LLC
W: 973.457.4602
C: 551.655.8006
Schedule a Call: https://calendly.com/m-ashton

From: Bert Gunter [mailto:bgunter.4567 at gmail.com]
Sent: Wednesday, May 27, 2020 2:22 PM
To: Rui Barradas
Cc: Michael Ashton; r-help at r-project.org
Subject: Re: [R] struggling with apply

Better, I think (no indexing):

t(apply(somematrix,1,function(x)pmin(x,UB)))


Bert Gunter

"The trouble with having an open mind is that people keep coming along and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, May 27, 2020 at 10:56 AM Rui Barradas <ruipbarradas at sapo.pt<mailto:ruipbarradas at sapo.pt>> wrote:
Hello,

Try pmin. And loop by column/UB index with sapply/seq_along.


sapply(seq_along(UB), function(i) pmin(UB[i], somematrix[,i]))
#     [,1] [,2] [,3] [,4]
#[1,]  1.0  5.5  8.5  7.0
#[2,]  2.5  3.0  8.0 10.5
#[3,]  2.5  5.5  5.0 10.5


Hope this helps,

Rui Barradas


?s 18:46 de 27/05/20, Michael Ashton escreveu:
> Hi -
>
> I have a matrix of n rows and 4 columns.
>
> I want to cap the value in each column by a different upper bound. So, suppose my matrix is
>
> somematrix <- matrix(c(1,4,3,6,3,9,12,8,5,7,11,11),nrow=3,ncol=4)
>> somematrix
>       [,1] [,2] [,3] [,4]
> [1,]    1    6   12    7
> [2,]    4    3    8   11
> [3,]    3    9    5   11
>
> Now I want to have the maximum value in each column described by
> UB=c(2.5, 5.5, 8.5, 10.5)
>
> So that the right answer will look like:
>       [,1]      [,2]    [,3]   [,4]
> [1,]    1      5.5     8.5    7
> [2,]    2.5    3        8     10.5
> [3,]    2.5   5.5      5    10.5
>
> I've tried a few things, like:
> newmatrix <- apply(somematrix,c(1,2),function(x) min(UB,x))
>
> but I can't figure out to apply the relevant element of the UB list to the right element of the matrix. When I run the above, for example, it takes min(UB,x) over all UB, so I get:
>
> newmatrix
>       [,1] [,2] [,3] [,4]
> [1,]  1.0  2.5  2.5  2.5
> [2,]  2.5  2.5  2.5  2.5
> [3,]  2.5  2.5  2.5  2.5
>
> I'm sure there's a simple and elegant solution but I don't know what it is!
>
> Thanks in advance,
>
> Mike
>
> Michael Ashton, CFA
> Managing Principal
>
> Enduring Investments LLC
> W: 973.457.4602
> C: 551.655.8006
> Schedule a Call: https://calendly.com/m-ashton
>
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed May 27 21:38:21 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 27 May 2020 12:38:21 -0700
Subject: [R] struggling with apply
In-Reply-To: <CAGxFJbTUds+nWxdu+HG1=aBSxuA4SEVmDhX8emiq-RCMkUuf+A@mail.gmail.com>
References: <74e69784edb54827a033eb7fe3c7a337@MBX084-W1-CA-3.exch084.serverpod.net>
 <b7ff69b9-56dd-6311-293a-15871e03c0a6@sapo.pt>
 <CAGxFJbTUds+nWxdu+HG1=aBSxuA4SEVmDhX8emiq-RCMkUuf+A@mail.gmail.com>
Message-ID: <59BB6E6C-6A7E-4BDB-8D01-2838DE1955EC@dcn.davis.ca.us>

Sigh. Transpose?

apply( somematrix, 2, function( x ) pmin( x, UB ) )

On May 27, 2020 11:22:06 AM PDT, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>Better, I think (no indexing):
>
>t(apply(somematrix,1,function(x)pmin(x,UB)))
>
>
>Bert Gunter
>
>"The trouble with having an open mind is that people keep coming along
>and
>sticking things into it."
>-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
>On Wed, May 27, 2020 at 10:56 AM Rui Barradas <ruipbarradas at sapo.pt>
>wrote:
>
>> Hello,
>>
>> Try pmin. And loop by column/UB index with sapply/seq_along.
>>
>>
>> sapply(seq_along(UB), function(i) pmin(UB[i], somematrix[,i]))
>> #     [,1] [,2] [,3] [,4]
>> #[1,]  1.0  5.5  8.5  7.0
>> #[2,]  2.5  3.0  8.0 10.5
>> #[3,]  2.5  5.5  5.0 10.5
>>
>>
>> Hope this helps,
>>
>> Rui Barradas
>>
>>
>> ?s 18:46 de 27/05/20, Michael Ashton escreveu:
>> > Hi -
>> >
>> > I have a matrix of n rows and 4 columns.
>> >
>> > I want to cap the value in each column by a different upper bound.
>So,
>> suppose my matrix is
>> >
>> > somematrix <- matrix(c(1,4,3,6,3,9,12,8,5,7,11,11),nrow=3,ncol=4)
>> >> somematrix
>> >       [,1] [,2] [,3] [,4]
>> > [1,]    1    6   12    7
>> > [2,]    4    3    8   11
>> > [3,]    3    9    5   11
>> >
>> > Now I want to have the maximum value in each column described by
>> > UB=c(2.5, 5.5, 8.5, 10.5)
>> >
>> > So that the right answer will look like:
>> >       [,1]      [,2]    [,3]   [,4]
>> > [1,]    1      5.5     8.5    7
>> > [2,]    2.5    3        8     10.5
>> > [3,]    2.5   5.5      5    10.5
>> >
>> > I've tried a few things, like:
>> > newmatrix <- apply(somematrix,c(1,2),function(x) min(UB,x))
>> >
>> > but I can't figure out to apply the relevant element of the UB list
>to
>> the right element of the matrix. When I run the above, for example,
>it
>> takes min(UB,x) over all UB, so I get:
>> >
>> > newmatrix
>> >       [,1] [,2] [,3] [,4]
>> > [1,]  1.0  2.5  2.5  2.5
>> > [2,]  2.5  2.5  2.5  2.5
>> > [3,]  2.5  2.5  2.5  2.5
>> >
>> > I'm sure there's a simple and elegant solution but I don't know
>what it
>> is!
>> >
>> > Thanks in advance,
>> >
>> > Mike
>> >
>> > Michael Ashton, CFA
>> > Managing Principal
>> >
>> > Enduring Investments LLC
>> > W: 973.457.4602
>> > C: 551.655.8006
>> > Schedule a Call: https://calendly.com/m-ashton
>> >
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From v@h|d@borj|65 @end|ng |rom gm@||@com  Wed May 27 21:43:20 2020
From: v@h|d@borj|65 @end|ng |rom gm@||@com (Vahid Borji)
Date: Thu, 28 May 2020 00:13:20 +0430
Subject: [R] How to fix an error in nonlinear regression
Message-ID: <CAEPHqhYR5hKNrCwQkwHh84o8A4ODce_UY_-iERwnt91B1Eg6Mg@mail.gmail.com>

Hello my R friends,
I am using the below commands in R:

attach(Puromycin)
Puromycin
plot(Puromycin$conc,Puromycin$rate)
mm=function(conc,vmax,k) vmax*conc/(k+conc)
mm
mm1=nls(rate~mm(conc,vmax,k),data=Puromycin,start=c(vmax=50,k=0.05),subset=state==?treated?)
mm1

Unfortunately,  I receive the below error:

Error: unexpected input in
"mm1=nls(rate~mm(conc,vmax,k),data=Puromycin,start=c(vmax=50,k=0.05),subset=state==?"

How can I fix this error?

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Wed May 27 21:56:47 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 27 May 2020 12:56:47 -0700
Subject: [R] struggling with apply
In-Reply-To: <59BB6E6C-6A7E-4BDB-8D01-2838DE1955EC@dcn.davis.ca.us>
References: <74e69784edb54827a033eb7fe3c7a337@MBX084-W1-CA-3.exch084.serverpod.net>
 <b7ff69b9-56dd-6311-293a-15871e03c0a6@sapo.pt>
 <CAGxFJbTUds+nWxdu+HG1=aBSxuA4SEVmDhX8emiq-RCMkUuf+A@mail.gmail.com>
 <59BB6E6C-6A7E-4BDB-8D01-2838DE1955EC@dcn.davis.ca.us>
Message-ID: <CAGxFJbSFUnpp2d7j79ym61+dV_PtnxSGHH=8Z1N3xLsCH-rAAg@mail.gmail.com>

Jeff: Check it!

> somematrix <- matrix(c(1,4,3,6,3,9,12,8,5,7,11,11),nrow=3,ncol=4)
> UB=c(2.5, 5.5, 8.5, 10.5)
> apply( somematrix, 2, function( x ) pmin( x, UB ) )
     [,1] [,2] [,3] [,4]
[1,]    1  2.5  2.5  2.5
[2,]    4  3.0  5.5  5.5
[3,]    3  8.5  5.0  8.5
[4,]    1  6.0 10.5  7.0

Not what was wanted.
Am I missing something?

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, May 27, 2020 at 12:38 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Sigh. Transpose?
>
> apply( somematrix, 2, function( x ) pmin( x, UB ) )
>
> On May 27, 2020 11:22:06 AM PDT, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
> >Better, I think (no indexing):
> >
> >t(apply(somematrix,1,function(x)pmin(x,UB)))
> >
> >
> >Bert Gunter
> >
> >"The trouble with having an open mind is that people keep coming along
> >and
> >sticking things into it."
> >-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> >
> >
> >On Wed, May 27, 2020 at 10:56 AM Rui Barradas <ruipbarradas at sapo.pt>
> >wrote:
> >
> >> Hello,
> >>
> >> Try pmin. And loop by column/UB index with sapply/seq_along.
> >>
> >>
> >> sapply(seq_along(UB), function(i) pmin(UB[i], somematrix[,i]))
> >> #     [,1] [,2] [,3] [,4]
> >> #[1,]  1.0  5.5  8.5  7.0
> >> #[2,]  2.5  3.0  8.0 10.5
> >> #[3,]  2.5  5.5  5.0 10.5
> >>
> >>
> >> Hope this helps,
> >>
> >> Rui Barradas
> >>
> >>
> >> ?s 18:46 de 27/05/20, Michael Ashton escreveu:
> >> > Hi -
> >> >
> >> > I have a matrix of n rows and 4 columns.
> >> >
> >> > I want to cap the value in each column by a different upper bound.
> >So,
> >> suppose my matrix is
> >> >
> >> > somematrix <- matrix(c(1,4,3,6,3,9,12,8,5,7,11,11),nrow=3,ncol=4)
> >> >> somematrix
> >> >       [,1] [,2] [,3] [,4]
> >> > [1,]    1    6   12    7
> >> > [2,]    4    3    8   11
> >> > [3,]    3    9    5   11
> >> >
> >> > Now I want to have the maximum value in each column described by
> >> > UB=c(2.5, 5.5, 8.5, 10.5)
> >> >
> >> > So that the right answer will look like:
> >> >       [,1]      [,2]    [,3]   [,4]
> >> > [1,]    1      5.5     8.5    7
> >> > [2,]    2.5    3        8     10.5
> >> > [3,]    2.5   5.5      5    10.5
> >> >
> >> > I've tried a few things, like:
> >> > newmatrix <- apply(somematrix,c(1,2),function(x) min(UB,x))
> >> >
> >> > but I can't figure out to apply the relevant element of the UB list
> >to
> >> the right element of the matrix. When I run the above, for example,
> >it
> >> takes min(UB,x) over all UB, so I get:
> >> >
> >> > newmatrix
> >> >       [,1] [,2] [,3] [,4]
> >> > [1,]  1.0  2.5  2.5  2.5
> >> > [2,]  2.5  2.5  2.5  2.5
> >> > [3,]  2.5  2.5  2.5  2.5
> >> >
> >> > I'm sure there's a simple and elegant solution but I don't know
> >what it
> >> is!
> >> >
> >> > Thanks in advance,
> >> >
> >> > Mike
> >> >
> >> > Michael Ashton, CFA
> >> > Managing Principal
> >> >
> >> > Enduring Investments LLC
> >> > W: 973.457.4602
> >> > C: 551.655.8006
> >> > Schedule a Call: https://calendly.com/m-ashton
> >> >
> >> >
> >> >       [[alternative HTML version deleted]]
> >> >
> >> > ______________________________________________
> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> > and provide commented, minimal, self-contained, reproducible code.
> >> >
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From john@m@h@rro|d @end|ng |rom gm@||@com  Wed May 27 22:13:25 2020
From: john@m@h@rro|d @end|ng |rom gm@||@com (John Harrold)
Date: Wed, 27 May 2020 13:13:25 -0700
Subject: [R] Partial matching list elements in R 4.0
In-Reply-To: <CAF8bMcYVUfgFD-5629Mz=kaw0vcU71g+484D48nLSGiEam+jdg@mail.gmail.com>
References: <CANAiAiXk++sowVnKf_VRL4-YtC_3dNNknqO+jjVzeOjU7udCKw@mail.gmail.com>
 <CAF8bMcYVUfgFD-5629Mz=kaw0vcU71g+484D48nLSGiEam+jdg@mail.gmail.com>
Message-ID: <CANAiAiUMSB=f0Zez-HD_VsEQgQnrBrLuXjGfRTMasET6bZnLsw@mail.gmail.com>

Thankyou Bert and Bill.

I have one last question. Is there a tool that will recursively compare two
lists to find differences in both their structure and contents?

I'm afraid that in the process of converting code from $ to [[]] formats I
may inadvertently introduce some errors. And I'd like to QC it in some way.

Thanks
John

On Tue, May 26, 2020 at 11:33 AM William Dunlap <wdunlap at tibco.com> wrote:

> Another symptom of this problem is:
>
> > {x <- list(Abc=list(Pqr="Old Abc$Pqr")); x$Ab$Pqr <- "New Ab$Pqr" ; x}
> R version 3.6.2 (2019-12-12) | R version 4.0.0 (2020-04-24)
> List of 2                    | List of 2
>  $ Abc:List of 1             |  $ Abc:List of 1
>   ..$ Pqr: chr "Old Abc$Pqr" |   ..$ Pqr: chr "New Ab$Pqr"
>  $ Ab :List of 1             |  $ Ab :List of 1
>   ..$ Pqr: chr "New Ab$Pqr"  |   ..$ Pqr: chr "New Ab$Pqr"
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>
> On Tue, May 26, 2020 at 10:45 AM John Harrold <john.m.harrold at gmail.com>
> wrote:
>
>> Hello,
>>
>>
>> I'm testing some code in R 4.0, and I'm having an issue with the
>> following"
>>
>> # -------------
>> rm(list=ls())
>> graphics.off()
>> #load("/tmp/post.RData")
>> var = list();
>> # If I uncomment this it fixes things:
>> # var$options = list(mi   = list(),
>> #                    misc = list())
>> #
>> var$options$misc$abc = "123"
>> var$options$mi$something    = 13
>> #------------
>>
>> This is a stripped down example but it exhibits the issue I"m having.
>> Basically when I create the list element var$options$mi the contents of
>> var$options$misc move over to var$options$mi. And what was in
>> var$options$misc become NULL:
>>
>> So now var$options looks like:
>>
>> var$options
>> $misc
>> $misc$abc
>> NULL
>>
>> $mi
>> $mi$abc
>> [1] "123"
>> $mi$something
>> [1] 13
>>
>> This worked (still works) in R 3.5.1. I understand partial matching, but
>> is
>> this normal lists moving over to elements like this? I can uncomment the
>> text mentioned in the example and it seems to fix it, but I'm wondering if
>> this is a bug or just my poor programming coming back to bite me.
>>
>> I've included my sessionInfo() at the bottom.
>>
>> Thanks
>> John
>> :wq
>>
>>
>> sessionInfo()
>>
>> R version 4.0.0 (2020-04-24)
>>
>> Platform: x86_64-apple-darwin17.0 (64-bit)
>>
>> Running under: macOS Mojave 10.14.5
>>
>>
>> Matrix products: default
>>
>> BLAS:
>> /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
>>
>> LAPACK:
>>
>> /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
>>
>>
>> locale:
>>
>> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>>
>>
>> attached base packages:
>>
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>>
>> other attached packages:
>>
>> [1] gdata_2.18.0  ggplot2_3.3.0 deSolve_1.28
>>
>>
>> loaded via a namespace (and not attached):
>>
>>  [1] Rcpp_1.0.4.6     gtools_3.8.2     withr_2.2.0      assertthat_0.2.1
>>
>>  [5] dplyr_0.8.5      digest_0.6.25    crayon_1.3.4     grid_4.0.0
>>
>>  [9] R6_2.4.1         lifecycle_0.2.0  gtable_0.3.0     magrittr_1.5
>>
>> [13] scales_1.1.1     pillar_1.4.4     rlang_0.4.6      vctrs_0.3.0
>>
>> [17] ellipsis_0.3.1   glue_1.4.1       purrr_0.3.4      munsell_0.5.0
>>
>> [21] compiler_4.0.0   pkgconfig_2.0.3  colorspace_1.4-1 tidyselect_1.1.0
>>
>> [25] tibble_3.0.1
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

-- 
John
:wq

	[[alternative HTML version deleted]]


From wdun|@p @end|ng |rom t|bco@com  Wed May 27 22:31:04 2020
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Wed, 27 May 2020 13:31:04 -0700
Subject: [R] Partial matching list elements in R 4.0
In-Reply-To: <CANAiAiUMSB=f0Zez-HD_VsEQgQnrBrLuXjGfRTMasET6bZnLsw@mail.gmail.com>
References: <CANAiAiXk++sowVnKf_VRL4-YtC_3dNNknqO+jjVzeOjU7udCKw@mail.gmail.com>
 <CAF8bMcYVUfgFD-5629Mz=kaw0vcU71g+484D48nLSGiEam+jdg@mail.gmail.com>
 <CANAiAiUMSB=f0Zez-HD_VsEQgQnrBrLuXjGfRTMasET6bZnLsw@mail.gmail.com>
Message-ID: <CAF8bMcYV-=gpsctkxNmR27CyqxaWNmny+cH9nUzNXxKMhfWCpg@mail.gmail.com>

all.equal()

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Wed, May 27, 2020 at 1:13 PM John Harrold <john.m.harrold at gmail.com>
wrote:

> Thankyou Bert and Bill.
>
> I have one last question. Is there a tool that will recursively compare
> two lists to find differences in both their structure and contents?
>
> I'm afraid that in the process of converting code from $ to [[]] formats I
> may inadvertently introduce some errors. And I'd like to QC it in some way.
>
> Thanks
> John
>
> On Tue, May 26, 2020 at 11:33 AM William Dunlap <wdunlap at tibco.com> wrote:
>
>> Another symptom of this problem is:
>>
>> > {x <- list(Abc=list(Pqr="Old Abc$Pqr")); x$Ab$Pqr <- "New Ab$Pqr" ; x}
>> R version 3.6.2 (2019-12-12) | R version 4.0.0 (2020-04-24)
>> List of 2                    | List of 2
>>  $ Abc:List of 1             |  $ Abc:List of 1
>>   ..$ Pqr: chr "Old Abc$Pqr" |   ..$ Pqr: chr "New Ab$Pqr"
>>  $ Ab :List of 1             |  $ Ab :List of 1
>>   ..$ Pqr: chr "New Ab$Pqr"  |   ..$ Pqr: chr "New Ab$Pqr"
>>
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>>
>>
>> On Tue, May 26, 2020 at 10:45 AM John Harrold <john.m.harrold at gmail.com>
>> wrote:
>>
>>> Hello,
>>>
>>>
>>> I'm testing some code in R 4.0, and I'm having an issue with the
>>> following"
>>>
>>> # -------------
>>> rm(list=ls())
>>> graphics.off()
>>> #load("/tmp/post.RData")
>>> var = list();
>>> # If I uncomment this it fixes things:
>>> # var$options = list(mi   = list(),
>>> #                    misc = list())
>>> #
>>> var$options$misc$abc = "123"
>>> var$options$mi$something    = 13
>>> #------------
>>>
>>> This is a stripped down example but it exhibits the issue I"m having.
>>> Basically when I create the list element var$options$mi the contents of
>>> var$options$misc move over to var$options$mi. And what was in
>>> var$options$misc become NULL:
>>>
>>> So now var$options looks like:
>>>
>>> var$options
>>> $misc
>>> $misc$abc
>>> NULL
>>>
>>> $mi
>>> $mi$abc
>>> [1] "123"
>>> $mi$something
>>> [1] 13
>>>
>>> This worked (still works) in R 3.5.1. I understand partial matching, but
>>> is
>>> this normal lists moving over to elements like this? I can uncomment the
>>> text mentioned in the example and it seems to fix it, but I'm wondering
>>> if
>>> this is a bug or just my poor programming coming back to bite me.
>>>
>>> I've included my sessionInfo() at the bottom.
>>>
>>> Thanks
>>> John
>>> :wq
>>>
>>>
>>> sessionInfo()
>>>
>>> R version 4.0.0 (2020-04-24)
>>>
>>> Platform: x86_64-apple-darwin17.0 (64-bit)
>>>
>>> Running under: macOS Mojave 10.14.5
>>>
>>>
>>> Matrix products: default
>>>
>>> BLAS:
>>> /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
>>>
>>> LAPACK:
>>>
>>> /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
>>>
>>>
>>> locale:
>>>
>>> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>>>
>>>
>>> attached base packages:
>>>
>>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>>
>>>
>>> other attached packages:
>>>
>>> [1] gdata_2.18.0  ggplot2_3.3.0 deSolve_1.28
>>>
>>>
>>> loaded via a namespace (and not attached):
>>>
>>>  [1] Rcpp_1.0.4.6     gtools_3.8.2     withr_2.2.0      assertthat_0.2.1
>>>
>>>  [5] dplyr_0.8.5      digest_0.6.25    crayon_1.3.4     grid_4.0.0
>>>
>>>  [9] R6_2.4.1         lifecycle_0.2.0  gtable_0.3.0     magrittr_1.5
>>>
>>> [13] scales_1.1.1     pillar_1.4.4     rlang_0.4.6      vctrs_0.3.0
>>>
>>> [17] ellipsis_0.3.1   glue_1.4.1       purrr_0.3.4      munsell_0.5.0
>>>
>>> [21] compiler_4.0.0   pkgconfig_2.0.3  colorspace_1.4-1 tidyselect_1.1.0
>>>
>>> [25] tibble_3.0.1
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>
> --
> John
> :wq
>

	[[alternative HTML version deleted]]


From mrgu|||oy|e @end|ng |rom gm@||@com  Wed May 27 22:32:34 2020
From: mrgu|||oy|e @end|ng |rom gm@||@com (Mathew Guilfoyle)
Date: Wed, 27 May 2020 21:32:34 +0100
Subject: [R] struggling with apply
In-Reply-To: <CAGxFJbSFUnpp2d7j79ym61+dV_PtnxSGHH=8Z1N3xLsCH-rAAg@mail.gmail.com>
References: <74e69784edb54827a033eb7fe3c7a337@MBX084-W1-CA-3.exch084.serverpod.net>
 <b7ff69b9-56dd-6311-293a-15871e03c0a6@sapo.pt>
 <CAGxFJbTUds+nWxdu+HG1=aBSxuA4SEVmDhX8emiq-RCMkUuf+A@mail.gmail.com>
 <59BB6E6C-6A7E-4BDB-8D01-2838DE1955EC@dcn.davis.ca.us>
 <CAGxFJbSFUnpp2d7j79ym61+dV_PtnxSGHH=8Z1N3xLsCH-rAAg@mail.gmail.com>
Message-ID: <8B2DFB27-CD9C-4881-B666-9FDE8A3A1915@gmail.com>

A bit quicker:

t(pmin(t(somematrix), UB))



> On 27 May 2020, at 20:56, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> Jeff: Check it!
> 
>> somematrix <- matrix(c(1,4,3,6,3,9,12,8,5,7,11,11),nrow=3,ncol=4)
>> UB=c(2.5, 5.5, 8.5, 10.5)
>> apply( somematrix, 2, function( x ) pmin( x, UB ) )
>     [,1] [,2] [,3] [,4]
> [1,]    1  2.5  2.5  2.5
> [2,]    4  3.0  5.5  5.5
> [3,]    3  8.5  5.0  8.5
> [4,]    1  6.0 10.5  7.0
> 
> Not what was wanted.
> Am I missing something?
> 
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> 
> On Wed, May 27, 2020 at 12:38 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us <mailto:jdnewmil at dcn.davis.ca.us>>
> wrote:
> 
>> Sigh. Transpose?
>> 
>> apply( somematrix, 2, function( x ) pmin( x, UB ) )
>> 
>> On May 27, 2020 11:22:06 AM PDT, Bert Gunter <bgunter.4567 at gmail.com>
>> wrote:
>>> Better, I think (no indexing):
>>> 
>>> t(apply(somematrix,1,function(x)pmin(x,UB)))
>>> 
>>> 
>>> Bert Gunter
>>> 
>>> "The trouble with having an open mind is that people keep coming along
>>> and
>>> sticking things into it."
>>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>> 
>>> 
>>> On Wed, May 27, 2020 at 10:56 AM Rui Barradas <ruipbarradas at sapo.pt>
>>> wrote:
>>> 
>>>> Hello,
>>>> 
>>>> Try pmin. And loop by column/UB index with sapply/seq_along.
>>>> 
>>>> 
>>>> sapply(seq_along(UB), function(i) pmin(UB[i], somematrix[,i]))
>>>> #     [,1] [,2] [,3] [,4]
>>>> #[1,]  1.0  5.5  8.5  7.0
>>>> #[2,]  2.5  3.0  8.0 10.5
>>>> #[3,]  2.5  5.5  5.0 10.5
>>>> 
>>>> 
>>>> Hope this helps,
>>>> 
>>>> Rui Barradas
>>>> 
>>>> 
>>>> ?s 18:46 de 27/05/20, Michael Ashton escreveu:
>>>>> Hi -
>>>>> 
>>>>> I have a matrix of n rows and 4 columns.
>>>>> 
>>>>> I want to cap the value in each column by a different upper bound.
>>> So,
>>>> suppose my matrix is
>>>>> 
>>>>> somematrix <- matrix(c(1,4,3,6,3,9,12,8,5,7,11,11),nrow=3,ncol=4)
>>>>>> somematrix
>>>>>      [,1] [,2] [,3] [,4]
>>>>> [1,]    1    6   12    7
>>>>> [2,]    4    3    8   11
>>>>> [3,]    3    9    5   11
>>>>> 
>>>>> Now I want to have the maximum value in each column described by
>>>>> UB=c(2.5, 5.5, 8.5, 10.5)
>>>>> 
>>>>> So that the right answer will look like:
>>>>>      [,1]      [,2]    [,3]   [,4]
>>>>> [1,]    1      5.5     8.5    7
>>>>> [2,]    2.5    3        8     10.5
>>>>> [3,]    2.5   5.5      5    10.5
>>>>> 
>>>>> I've tried a few things, like:
>>>>> newmatrix <- apply(somematrix,c(1,2),function(x) min(UB,x))
>>>>> 
>>>>> but I can't figure out to apply the relevant element of the UB list
>>> to
>>>> the right element of the matrix. When I run the above, for example,
>>> it
>>>> takes min(UB,x) over all UB, so I get:
>>>>> 
>>>>> newmatrix
>>>>>      [,1] [,2] [,3] [,4]
>>>>> [1,]  1.0  2.5  2.5  2.5
>>>>> [2,]  2.5  2.5  2.5  2.5
>>>>> [3,]  2.5  2.5  2.5  2.5
>>>>> 
>>>>> I'm sure there's a simple and elegant solution but I don't know
>>> what it
>>>> is!
>>>>> 
>>>>> Thanks in advance,
>>>>> 
>>>>> Mike
>>>>> 
>>>>> Michael Ashton, CFA
>>>>> Managing Principal
>>>>> 
>>>>> Enduring Investments LLC
>>>>> W: 973.457.4602
>>>>> C: 551.655.8006
>>>>> Schedule a Call: https://calendly.com/m-ashton
>>>>> 
>>>>> 
>>>>>      [[alternative HTML version deleted]]
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>> 
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>> 
>>>      [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> --
>> Sent from my phone. Please excuse my brevity.
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org <mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help <https://stat.ethz.ch/mailman/listinfo/r-help>
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html <http://www.r-project.org/posting-guide.html>
> and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From john@m@h@rro|d @end|ng |rom gm@||@com  Wed May 27 22:40:10 2020
From: john@m@h@rro|d @end|ng |rom gm@||@com (John Harrold)
Date: Wed, 27 May 2020 13:40:10 -0700
Subject: [R] Partial matching list elements in R 4.0
In-Reply-To: <CAF8bMcYV-=gpsctkxNmR27CyqxaWNmny+cH9nUzNXxKMhfWCpg@mail.gmail.com>
References: <CANAiAiXk++sowVnKf_VRL4-YtC_3dNNknqO+jjVzeOjU7udCKw@mail.gmail.com>
 <CAF8bMcYVUfgFD-5629Mz=kaw0vcU71g+484D48nLSGiEam+jdg@mail.gmail.com>
 <CANAiAiUMSB=f0Zez-HD_VsEQgQnrBrLuXjGfRTMasET6bZnLsw@mail.gmail.com>
 <CAF8bMcYV-=gpsctkxNmR27CyqxaWNmny+cH9nUzNXxKMhfWCpg@mail.gmail.com>
Message-ID: <CANAiAiVY93+Y-Tfv_S=s4cyxioB8Fp9oku=E3yCsrXe_ZYjWGg@mail.gmail.com>

Awesome thanks.

On Wed, May 27, 2020 at 1:31 PM William Dunlap <wdunlap at tibco.com> wrote:

> all.equal()
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>
> On Wed, May 27, 2020 at 1:13 PM John Harrold <john.m.harrold at gmail.com>
> wrote:
>
>> Thankyou Bert and Bill.
>>
>> I have one last question. Is there a tool that will recursively compare
>> two lists to find differences in both their structure and contents?
>>
>> I'm afraid that in the process of converting code from $ to [[]] formats
>> I may inadvertently introduce some errors. And I'd like to QC it in some
>> way.
>>
>> Thanks
>> John
>>
>> On Tue, May 26, 2020 at 11:33 AM William Dunlap <wdunlap at tibco.com>
>> wrote:
>>
>>> Another symptom of this problem is:
>>>
>>> > {x <- list(Abc=list(Pqr="Old Abc$Pqr")); x$Ab$Pqr <- "New Ab$Pqr" ; x}
>>> R version 3.6.2 (2019-12-12) | R version 4.0.0 (2020-04-24)
>>> List of 2                    | List of 2
>>>  $ Abc:List of 1             |  $ Abc:List of 1
>>>   ..$ Pqr: chr "Old Abc$Pqr" |   ..$ Pqr: chr "New Ab$Pqr"
>>>  $ Ab :List of 1             |  $ Ab :List of 1
>>>   ..$ Pqr: chr "New Ab$Pqr"  |   ..$ Pqr: chr "New Ab$Pqr"
>>>
>>> Bill Dunlap
>>> TIBCO Software
>>> wdunlap tibco.com
>>>
>>>
>>> On Tue, May 26, 2020 at 10:45 AM John Harrold <john.m.harrold at gmail.com>
>>> wrote:
>>>
>>>> Hello,
>>>>
>>>>
>>>> I'm testing some code in R 4.0, and I'm having an issue with the
>>>> following"
>>>>
>>>> # -------------
>>>> rm(list=ls())
>>>> graphics.off()
>>>> #load("/tmp/post.RData")
>>>> var = list();
>>>> # If I uncomment this it fixes things:
>>>> # var$options = list(mi   = list(),
>>>> #                    misc = list())
>>>> #
>>>> var$options$misc$abc = "123"
>>>> var$options$mi$something    = 13
>>>> #------------
>>>>
>>>> This is a stripped down example but it exhibits the issue I"m having.
>>>> Basically when I create the list element var$options$mi the contents of
>>>> var$options$misc move over to var$options$mi. And what was in
>>>> var$options$misc become NULL:
>>>>
>>>> So now var$options looks like:
>>>>
>>>> var$options
>>>> $misc
>>>> $misc$abc
>>>> NULL
>>>>
>>>> $mi
>>>> $mi$abc
>>>> [1] "123"
>>>> $mi$something
>>>> [1] 13
>>>>
>>>> This worked (still works) in R 3.5.1. I understand partial matching,
>>>> but is
>>>> this normal lists moving over to elements like this? I can uncomment the
>>>> text mentioned in the example and it seems to fix it, but I'm wondering
>>>> if
>>>> this is a bug or just my poor programming coming back to bite me.
>>>>
>>>> I've included my sessionInfo() at the bottom.
>>>>
>>>> Thanks
>>>> John
>>>> :wq
>>>>
>>>>
>>>> sessionInfo()
>>>>
>>>> R version 4.0.0 (2020-04-24)
>>>>
>>>> Platform: x86_64-apple-darwin17.0 (64-bit)
>>>>
>>>> Running under: macOS Mojave 10.14.5
>>>>
>>>>
>>>> Matrix products: default
>>>>
>>>> BLAS:
>>>>
>>>> /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
>>>>
>>>> LAPACK:
>>>>
>>>> /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
>>>>
>>>>
>>>> locale:
>>>>
>>>> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>>>>
>>>>
>>>> attached base packages:
>>>>
>>>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>>>
>>>>
>>>> other attached packages:
>>>>
>>>> [1] gdata_2.18.0  ggplot2_3.3.0 deSolve_1.28
>>>>
>>>>
>>>> loaded via a namespace (and not attached):
>>>>
>>>>  [1] Rcpp_1.0.4.6     gtools_3.8.2     withr_2.2.0      assertthat_0.2.1
>>>>
>>>>  [5] dplyr_0.8.5      digest_0.6.25    crayon_1.3.4     grid_4.0.0
>>>>
>>>>  [9] R6_2.4.1         lifecycle_0.2.0  gtable_0.3.0     magrittr_1.5
>>>>
>>>> [13] scales_1.1.1     pillar_1.4.4     rlang_0.4.6      vctrs_0.3.0
>>>>
>>>> [17] ellipsis_0.3.1   glue_1.4.1       purrr_0.3.4      munsell_0.5.0
>>>>
>>>> [21] compiler_4.0.0   pkgconfig_2.0.3  colorspace_1.4-1 tidyselect_1.1.0
>>>>
>>>> [25] tibble_3.0.1
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>
>> --
>> John
>> :wq
>>
>

-- 
John
:wq

	[[alternative HTML version deleted]]


From jrkr|de@u @end|ng |rom gm@||@com  Wed May 27 22:44:50 2020
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Wed, 27 May 2020 16:44:50 -0400
Subject: [R] How to fix an error in nonlinear regression
In-Reply-To: <CAEPHqhYR5hKNrCwQkwHh84o8A4ODce_UY_-iERwnt91B1Eg6Mg@mail.gmail.com>
References: <CAEPHqhYR5hKNrCwQkwHh84o8A4ODce_UY_-iERwnt91B1Eg6Mg@mail.gmail.com>
Message-ID: <CAKZQJMCKpP0u3tqzURS9EhzwTZUj6rTY=_8N+9r98yMg+gcSAw@mail.gmail.com>

You have subset=state==?treated?). You need to change "treated? to
"treated". Tho " " are formatted in your example.

On Wed, 27 May 2020 at 15:48, Vahid Borji <vahid.borji65 at gmail.com> wrote:

> Hello my R friends,
> I am using the below commands in R:
>
> attach(Puromycin)
> Puromycin
> plot(Puromycin$conc,Puromycin$rate)
> mm=function(conc,vmax,k) vmax*conc/(k+conc)
> mm
>
> mm1=nls(rate~mm(conc,vmax,k),data=Puromycin,start=c(vmax=50,k=0.05),subset=state==?treated?)
> mm1
>
> Unfortunately,  I receive the below error:
>
> Error: unexpected input in
>
> "mm1=nls(rate~mm(conc,vmax,k),data=Puromycin,start=c(vmax=50,k=0.05),subset=state==?"
>
> How can I fix this error?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
John Kane
Kingston ON Canada

	[[alternative HTML version deleted]]


From m@@@hton @end|ng |rom endur|ng|nve@tment@@com  Wed May 27 22:45:09 2020
From: m@@@hton @end|ng |rom endur|ng|nve@tment@@com (Michael Ashton)
Date: Wed, 27 May 2020 20:45:09 +0000
Subject: [R] struggling with apply
In-Reply-To: <8B2DFB27-CD9C-4881-B666-9FDE8A3A1915@gmail.com>
References: <74e69784edb54827a033eb7fe3c7a337@MBX084-W1-CA-3.exch084.serverpod.net>
 <b7ff69b9-56dd-6311-293a-15871e03c0a6@sapo.pt>
 <CAGxFJbTUds+nWxdu+HG1=aBSxuA4SEVmDhX8emiq-RCMkUuf+A@mail.gmail.com>
 <59BB6E6C-6A7E-4BDB-8D01-2838DE1955EC@dcn.davis.ca.us>
 <CAGxFJbSFUnpp2d7j79ym61+dV_PtnxSGHH=8Z1N3xLsCH-rAAg@mail.gmail.com>,
 <8B2DFB27-CD9C-4881-B666-9FDE8A3A1915@gmail.com>
Message-ID: <891EF0A1-5942-454B-BB4B-D47E393CC1D9@enduringinvestments.com>

This is like "Name that Tune." Can anyone do it in FEWER characters? :-)

On May 27, 2020, at 4:32 PM, Mathew Guilfoyle <mrguilfoyle at gmail.com> wrote:

? A bit quicker:

t(pmin(t(somematrix), UB))



On 27 May 2020, at 20:56, Bert Gunter <bgunter.4567 at gmail.com<mailto:bgunter.4567 at gmail.com>> wrote:

Jeff: Check it!

somematrix <- matrix(c(1,4,3,6,3,9,12,8,5,7,11,11),nrow=3,ncol=4)
UB=c(2.5, 5.5, 8.5, 10.5)
apply( somematrix, 2, function( x ) pmin( x, UB ) )
    [,1] [,2] [,3] [,4]
[1,]    1  2.5  2.5  2.5
[2,]    4  3.0  5.5  5.5
[3,]    3  8.5  5.0  8.5
[4,]    1  6.0 10.5  7.0

Not what was wanted.
Am I missing something?

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, May 27, 2020 at 12:38 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us>>
wrote:

Sigh. Transpose?

apply( somematrix, 2, function( x ) pmin( x, UB ) )

On May 27, 2020 11:22:06 AM PDT, Bert Gunter <bgunter.4567 at gmail.com<mailto:bgunter.4567 at gmail.com>>
wrote:
Better, I think (no indexing):

t(apply(somematrix,1,function(x)pmin(x,UB)))


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, May 27, 2020 at 10:56 AM Rui Barradas <ruipbarradas at sapo.pt<mailto:ruipbarradas at sapo.pt>>
wrote:

Hello,

Try pmin. And loop by column/UB index with sapply/seq_along.


sapply(seq_along(UB), function(i) pmin(UB[i], somematrix[,i]))
#     [,1] [,2] [,3] [,4]
#[1,]  1.0  5.5  8.5  7.0
#[2,]  2.5  3.0  8.0 10.5
#[3,]  2.5  5.5  5.0 10.5


Hope this helps,

Rui Barradas


?s 18:46 de 27/05/20, Michael Ashton escreveu:
Hi -

I have a matrix of n rows and 4 columns.

I want to cap the value in each column by a different upper bound.
So,
suppose my matrix is

somematrix <- matrix(c(1,4,3,6,3,9,12,8,5,7,11,11),nrow=3,ncol=4)
somematrix
     [,1] [,2] [,3] [,4]
[1,]    1    6   12    7
[2,]    4    3    8   11
[3,]    3    9    5   11

Now I want to have the maximum value in each column described by
UB=c(2.5, 5.5, 8.5, 10.5)

So that the right answer will look like:
     [,1]      [,2]    [,3]   [,4]
[1,]    1      5.5     8.5    7
[2,]    2.5    3        8     10.5
[3,]    2.5   5.5      5    10.5

I've tried a few things, like:
newmatrix <- apply(somematrix,c(1,2),function(x) min(UB,x))

but I can't figure out to apply the relevant element of the UB list
to
the right element of the matrix. When I run the above, for example,
it
takes min(UB,x) over all UB, so I get:

newmatrix
     [,1] [,2] [,3] [,4]
[1,]  1.0  2.5  2.5  2.5
[2,]  2.5  2.5  2.5  2.5
[3,]  2.5  2.5  2.5  2.5

I'm sure there's a simple and elegant solution but I don't know
what it
is!

Thanks in advance,

Mike

Michael Ashton, CFA
Managing Principal

Enduring Investments LLC
W: 973.457.4602
C: 551.655.8006
Schedule a Call: https://calendly.com/m-ashton


     [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


     [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.


[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html<http://www.r-project.org/posting-guide.html>
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Wed May 27 23:16:23 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 27 May 2020 14:16:23 -0700
Subject: [R] Partial matching list elements in R 4.0
In-Reply-To: <CANAiAiUMSB=f0Zez-HD_VsEQgQnrBrLuXjGfRTMasET6bZnLsw@mail.gmail.com>
References: <CANAiAiXk++sowVnKf_VRL4-YtC_3dNNknqO+jjVzeOjU7udCKw@mail.gmail.com>
 <CAF8bMcYVUfgFD-5629Mz=kaw0vcU71g+484D48nLSGiEam+jdg@mail.gmail.com>
 <CANAiAiUMSB=f0Zez-HD_VsEQgQnrBrLuXjGfRTMasET6bZnLsw@mail.gmail.com>
Message-ID: <CAGxFJbR_0_3yG3j6UjMUEyNQ-P=fRmGjbfWXJ5DzmtiRuW61Uw@mail.gmail.com>

?identical
?rapply might also be useful to get more granular diagnostics. (??)


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, May 27, 2020 at 1:13 PM John Harrold <john.m.harrold at gmail.com>
wrote:

> Thankyou Bert and Bill.
>
> I have one last question. Is there a tool that will recursively compare two
> lists to find differences in both their structure and contents?
>
> I'm afraid that in the process of converting code from $ to [[]] formats I
> may inadvertently introduce some errors. And I'd like to QC it in some way.
>
> Thanks
> John
>
> On Tue, May 26, 2020 at 11:33 AM William Dunlap <wdunlap at tibco.com> wrote:
>
> > Another symptom of this problem is:
> >
> > > {x <- list(Abc=list(Pqr="Old Abc$Pqr")); x$Ab$Pqr <- "New Ab$Pqr" ; x}
> > R version 3.6.2 (2019-12-12) | R version 4.0.0 (2020-04-24)
> > List of 2                    | List of 2
> >  $ Abc:List of 1             |  $ Abc:List of 1
> >   ..$ Pqr: chr "Old Abc$Pqr" |   ..$ Pqr: chr "New Ab$Pqr"
> >  $ Ab :List of 1             |  $ Ab :List of 1
> >   ..$ Pqr: chr "New Ab$Pqr"  |   ..$ Pqr: chr "New Ab$Pqr"
> >
> > Bill Dunlap
> > TIBCO Software
> > wdunlap tibco.com
> >
> >
> > On Tue, May 26, 2020 at 10:45 AM John Harrold <john.m.harrold at gmail.com>
> > wrote:
> >
> >> Hello,
> >>
> >>
> >> I'm testing some code in R 4.0, and I'm having an issue with the
> >> following"
> >>
> >> # -------------
> >> rm(list=ls())
> >> graphics.off()
> >> #load("/tmp/post.RData")
> >> var = list();
> >> # If I uncomment this it fixes things:
> >> # var$options = list(mi   = list(),
> >> #                    misc = list())
> >> #
> >> var$options$misc$abc = "123"
> >> var$options$mi$something    = 13
> >> #------------
> >>
> >> This is a stripped down example but it exhibits the issue I"m having.
> >> Basically when I create the list element var$options$mi the contents of
> >> var$options$misc move over to var$options$mi. And what was in
> >> var$options$misc become NULL:
> >>
> >> So now var$options looks like:
> >>
> >> var$options
> >> $misc
> >> $misc$abc
> >> NULL
> >>
> >> $mi
> >> $mi$abc
> >> [1] "123"
> >> $mi$something
> >> [1] 13
> >>
> >> This worked (still works) in R 3.5.1. I understand partial matching, but
> >> is
> >> this normal lists moving over to elements like this? I can uncomment the
> >> text mentioned in the example and it seems to fix it, but I'm wondering
> if
> >> this is a bug or just my poor programming coming back to bite me.
> >>
> >> I've included my sessionInfo() at the bottom.
> >>
> >> Thanks
> >> John
> >> :wq
> >>
> >>
> >> sessionInfo()
> >>
> >> R version 4.0.0 (2020-04-24)
> >>
> >> Platform: x86_64-apple-darwin17.0 (64-bit)
> >>
> >> Running under: macOS Mojave 10.14.5
> >>
> >>
> >> Matrix products: default
> >>
> >> BLAS:
> >>
> /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
> >>
> >> LAPACK:
> >>
> >>
> /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
> >>
> >>
> >> locale:
> >>
> >> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
> >>
> >>
> >> attached base packages:
> >>
> >> [1] stats     graphics  grDevices utils     datasets  methods   base
> >>
> >>
> >> other attached packages:
> >>
> >> [1] gdata_2.18.0  ggplot2_3.3.0 deSolve_1.28
> >>
> >>
> >> loaded via a namespace (and not attached):
> >>
> >>  [1] Rcpp_1.0.4.6     gtools_3.8.2     withr_2.2.0      assertthat_0.2.1
> >>
> >>  [5] dplyr_0.8.5      digest_0.6.25    crayon_1.3.4     grid_4.0.0
> >>
> >>  [9] R6_2.4.1         lifecycle_0.2.0  gtable_0.3.0     magrittr_1.5
> >>
> >> [13] scales_1.1.1     pillar_1.4.4     rlang_0.4.6      vctrs_0.3.0
> >>
> >> [17] ellipsis_0.3.1   glue_1.4.1       purrr_0.3.4      munsell_0.5.0
> >>
> >> [21] compiler_4.0.0   pkgconfig_2.0.3  colorspace_1.4-1 tidyselect_1.1.0
> >>
> >> [25] tibble_3.0.1
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
>
> --
> John
> :wq
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From v@h|d@borj|65 @end|ng |rom gm@||@com  Wed May 27 23:29:49 2020
From: v@h|d@borj|65 @end|ng |rom gm@||@com (Vahid Borji)
Date: Thu, 28 May 2020 01:59:49 +0430
Subject: [R] How to fix an error in nonlinear regression
In-Reply-To: <CAKZQJMCKpP0u3tqzURS9EhzwTZUj6rTY=_8N+9r98yMg+gcSAw@mail.gmail.com>
References: <CAEPHqhYR5hKNrCwQkwHh84o8A4ODce_UY_-iERwnt91B1Eg6Mg@mail.gmail.com>
 <CAKZQJMCKpP0u3tqzURS9EhzwTZUj6rTY=_8N+9r98yMg+gcSAw@mail.gmail.com>
Message-ID: <CAEPHqhY-f+f8NgzX-o3qzcYpjJ=56aPygy+S591oTdXk3JYAzg@mail.gmail.com>

Thanks a lot.

On Thu, May 28, 2020 at 1:15 AM John Kane <jrkrideau at gmail.com> wrote:

> You have subset=state==?treated?). You need to change "treated? to
> "treated". Tho " " are formatted in your example.
>
> On Wed, 27 May 2020 at 15:48, Vahid Borji <vahid.borji65 at gmail.com> wrote:
>
>> Hello my R friends,
>> I am using the below commands in R:
>>
>> attach(Puromycin)
>> Puromycin
>> plot(Puromycin$conc,Puromycin$rate)
>> mm=function(conc,vmax,k) vmax*conc/(k+conc)
>> mm
>>
>> mm1=nls(rate~mm(conc,vmax,k),data=Puromycin,start=c(vmax=50,k=0.05),subset=state==?treated?)
>> mm1
>>
>> Unfortunately,  I receive the below error:
>>
>> Error: unexpected input in
>>
>> "mm1=nls(rate~mm(conc,vmax,k),data=Puromycin,start=c(vmax=50,k=0.05),subset=state==?"
>>
>> How can I fix this error?
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>
> --
> John Kane
> Kingston ON Canada
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Wed May 27 23:43:07 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 27 May 2020 14:43:07 -0700
Subject: [R] struggling with apply
In-Reply-To: <8B2DFB27-CD9C-4881-B666-9FDE8A3A1915@gmail.com>
References: <74e69784edb54827a033eb7fe3c7a337@MBX084-W1-CA-3.exch084.serverpod.net>
 <b7ff69b9-56dd-6311-293a-15871e03c0a6@sapo.pt>
 <CAGxFJbTUds+nWxdu+HG1=aBSxuA4SEVmDhX8emiq-RCMkUuf+A@mail.gmail.com>
 <59BB6E6C-6A7E-4BDB-8D01-2838DE1955EC@dcn.davis.ca.us>
 <CAGxFJbSFUnpp2d7j79ym61+dV_PtnxSGHH=8Z1N3xLsCH-rAAg@mail.gmail.com>
 <8B2DFB27-CD9C-4881-B666-9FDE8A3A1915@gmail.com>
Message-ID: <CAGxFJbRyDNkDwOsQ0MGU-xY6dJpWBLxorzSVw+eOPOAcOggVyA@mail.gmail.com>

Yes, that's better --- no looping at the interpreted level.

Another version without transposing is:

nr <- 3
matrix(pmin(c(somematrix),rep(UB, e = nr)), nrow = nr)

Both treat the matrix as a vector stored in column major order.

Cheers,
Bert


On Wed, May 27, 2020 at 1:32 PM Mathew Guilfoyle <mrguilfoyle at gmail.com>
wrote:

> A bit quicker:
>
> t(pmin(t(somematrix), UB))
>
>
>
> On 27 May 2020, at 20:56, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
> Jeff: Check it!
>
> somematrix <- matrix(c(1,4,3,6,3,9,12,8,5,7,11,11),nrow=3,ncol=4)
> UB=c(2.5, 5.5, 8.5, 10.5)
> apply( somematrix, 2, function( x ) pmin( x, UB ) )
>
>     [,1] [,2] [,3] [,4]
> [1,]    1  2.5  2.5  2.5
> [2,]    4  3.0  5.5  5.5
> [3,]    3  8.5  5.0  8.5
> [4,]    1  6.0 10.5  7.0
>
> Not what was wanted.
> Am I missing something?
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Wed, May 27, 2020 at 12:38 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
>
> Sigh. Transpose?
>
> apply( somematrix, 2, function( x ) pmin( x, UB ) )
>
> On May 27, 2020 11:22:06 AM PDT, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
>
> Better, I think (no indexing):
>
> t(apply(somematrix,1,function(x)pmin(x,UB)))
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Wed, May 27, 2020 at 10:56 AM Rui Barradas <ruipbarradas at sapo.pt>
> wrote:
>
> Hello,
>
> Try pmin. And loop by column/UB index with sapply/seq_along.
>
>
> sapply(seq_along(UB), function(i) pmin(UB[i], somematrix[,i]))
> #     [,1] [,2] [,3] [,4]
> #[1,]  1.0  5.5  8.5  7.0
> #[2,]  2.5  3.0  8.0 10.5
> #[3,]  2.5  5.5  5.0 10.5
>
>
> Hope this helps,
>
> Rui Barradas
>
>
> ?s 18:46 de 27/05/20, Michael Ashton escreveu:
>
> Hi -
>
> I have a matrix of n rows and 4 columns.
>
> I want to cap the value in each column by a different upper bound.
>
> So,
>
> suppose my matrix is
>
>
> somematrix <- matrix(c(1,4,3,6,3,9,12,8,5,7,11,11),nrow=3,ncol=4)
>
> somematrix
>
>      [,1] [,2] [,3] [,4]
> [1,]    1    6   12    7
> [2,]    4    3    8   11
> [3,]    3    9    5   11
>
> Now I want to have the maximum value in each column described by
> UB=c(2.5, 5.5, 8.5, 10.5)
>
> So that the right answer will look like:
>      [,1]      [,2]    [,3]   [,4]
> [1,]    1      5.5     8.5    7
> [2,]    2.5    3        8     10.5
> [3,]    2.5   5.5      5    10.5
>
> I've tried a few things, like:
> newmatrix <- apply(somematrix,c(1,2),function(x) min(UB,x))
>
> but I can't figure out to apply the relevant element of the UB list
>
> to
>
> the right element of the matrix. When I run the above, for example,
>
> it
>
> takes min(UB,x) over all UB, so I get:
>
>
> newmatrix
>      [,1] [,2] [,3] [,4]
> [1,]  1.0  2.5  2.5  2.5
> [2,]  2.5  2.5  2.5  2.5
> [3,]  2.5  2.5  2.5  2.5
>
> I'm sure there's a simple and elegant solution but I don't know
>
> what it
>
> is!
>
>
> Thanks in advance,
>
> Mike
>
> Michael Ashton, CFA
> Managing Principal
>
> Enduring Investments LLC
> W: 973.457.4602
> C: 551.655.8006
> Schedule a Call: https://calendly.com/m-ashton
>
>
>      [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
>
> http://www.R-project.org/posting-guide.html
>
> and provide commented, minimal, self-contained, reproducible code.
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>      [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
> --
> Sent from my phone. Please excuse my brevity.
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> <http://www.r-project.org/posting-guide.html>
> and provide commented, minimal, self-contained, reproducible code.
>
>
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed May 27 23:56:46 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 27 May 2020 22:56:46 +0100
Subject: [R] How to fix an error in nonlinear regression
In-Reply-To: <CAEPHqhYR5hKNrCwQkwHh84o8A4ODce_UY_-iERwnt91B1Eg6Mg@mail.gmail.com>
References: <CAEPHqhYR5hKNrCwQkwHh84o8A4ODce_UY_-iERwnt91B1Eg6Mg@mail.gmail.com>
Message-ID: <95bbb8cd-2ccf-2d02-0748-9cb6567b57f5@sapo.pt>

Hello,

This is cross posted [1] which is not well seen. Post and wait for an 
answer, which you got and are now asking the same, why? Maybe with time 
other answer will be posted to the first, original, question.


[1] 
https://stackoverflow.com/questions/62051714/how-can-i-fix-an-error-in-nonlinear-regression-nls-function


Rui Barradas

?s 20:43 de 27/05/20, Vahid Borji escreveu:
> Hello my R friends,
> I am using the below commands in R:
> 
> attach(Puromycin)
> Puromycin
> plot(Puromycin$conc,Puromycin$rate)
> mm=function(conc,vmax,k) vmax*conc/(k+conc)
> mm
> mm1=nls(rate~mm(conc,vmax,k),data=Puromycin,start=c(vmax=50,k=0.05),subset=state==?treated?)
> mm1
> 
> Unfortunately,  I receive the below error:
> 
> Error: unexpected input in
> "mm1=nls(rate~mm(conc,vmax,k),data=Puromycin,start=c(vmax=50,k=0.05),subset=state==?"
> 
> How can I fix this error?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From wjm1 @end|ng |rom c@@@co|umb|@@edu  Thu May 28 00:00:00 2020
From: wjm1 @end|ng |rom c@@@co|umb|@@edu (William Michels)
Date: Wed, 27 May 2020 15:00:00 -0700
Subject: [R] iterators : checkFunc with ireadLines
In-Reply-To: <b27578f2-824f-86bd-f410-2f3781f2da3e@free.fr>
References: <72174dd3-a819-b5fd-5a6f-2a6b46342774-2015@free.fr>
 <20200520104614.45876937@Tarkus>
 <27f44720-d368-c2cc-39c9-8629da885165@free.fr>
 <CAA99HCwONVzHizUuG17DO=+PF7SVVY8W9V+SKhpT5L=MW6_1NQ@mail.gmail.com>
 <b27578f2-824f-86bd-f410-2f3781f2da3e@free.fr>
Message-ID: <CAA99HCz=rfBf=1xVD7-5g=g2J7QKgCqeM9qdxFNVS8DbvVBcew@mail.gmail.com>

Hi Laurent,

Off the bat I would have guessed that the problem you're seeing has to
do with 'command line quoting' differences between the Windows system
and the Linux/Mac systems. I've noticed people using Windows having
better command line success with "exterior double-quotes / interior
single-quotes" while Linux/Mac tend to have more success with
"exterior single- quotes / interior double-quotes". The problem is
exacerbated in R by system() or pipe() calls which require another
(exterior) set of quotations.

1. You can print out your connection object to make sure that the
interior code was read properly into R. Also, take a look at the
'connections' help page to see if there are other parameters you need
to explicitly set (like encoding). Here's the first (working) example
from my last post to you:

> ?connections
> con_obj1
                                                              description
"raku -e '.put for lines.grep( / ^^N053 | ^^N163 /, :p );'  Laurents.txt"
                                                                    class
                                                                   "pipe"
                                                                     mode
                                                                     "rt"
                                                                     text
                                                                   "text"
                                                                   opened
                                                                 "opened"
                                                                 can read
                                                                    "yes"
                                                                can write
                                                                     "no"
>

2. You can try 'backslash-escaping' interior quotes in your system()
or pipe() calls. Also, in two of my previous examples I use paste() to
break up complicated quoting into more manageable chunks. You can try
these calls with 'backslash-escaped' interior quotes, and without
paste():

> con_obj1 <- pipe("raku -e \'.put for lines.grep( / ^^N053 | ^^N163 /, :p );\' Laurents.txt", open="rt");
> con_obj1
                                                             description
"raku -e '.put for lines.grep( / ^^N053 | ^^N163 /, :p );' Laurents.txt"
                                                                   class
                                                                  "pipe"
                                                                    mode
                                                                    "rt"
                                                                    text
                                                                  "text"
                                                                  opened
                                                                "opened"
                                                                can read
                                                                   "yes"
                                                               can write
                                                                    "no"
>

3. If R creates your 'con_obj' without throwing an error, then you
should try the most basic functions for reading data into R, something
like readLines(). Again, recreate our 'con_obj' with different
encodings, if necessary. Be careful of reading from the same
connection object with multiple R functions (an unlikely scenario, but
one that should be mentioned). Below it appears that 'con_obj1' gets
consumed by readLines() before the second call to scan():

> rm(con_obj1)
> # note: dropped ':p' adverb below to simplify
> con_obj1 <- pipe("raku -e \'.put for lines.grep( / ^^N053 | ^^N163 / );\' Laurents.txt", open="rt");
> scan(con_obj1)
Error in scan(con_obj1) : scan() expected 'a real', got 'N053'
> con_obj1 <- pipe("raku -e \'.put for lines.grep( / ^^N053 | ^^N163 / );\' Laurents.txt", open="rt");
> readLines(con_obj1)
[1] "N053    -0.014083    -0.004741    0.001443    -0.010152 -0.012996
   -0.005337    -0.008738    -0.015094    -0.012104"
[2] "N163    -0.054023    -0.049345    -0.037158    -0.04112 -0.044612
   -0.036953    -0.036061    -0.044516    -0.046436"
> scan(con_obj1)
Read 0 items
numeric(0)

>

Other than that, you can post here again and we'll try to help. If you
become convinced it's a raku problem, you can check the 'raku-grep'
help page at https://docs.raku.org/routine/grep, or post a question to
the perl6-users mailing list at perl6-users at perl.org .

HTH, Bill.

W. Michels, Ph.D.
On Wed, May 27, 2020 at 1:56 AM Laurent Rhelp <LaurentRHelp at free.fr> wrote:
>
> I installed raku on my PC to test your solution:
>
> The command raku -e '.put for lines.grep( / ^^N053 | ^^N163 /, :p );'
> Laurents.txt works fine when I write it in the bash command but when I
> use the pipe command in R as you say there is nothing in lines with
> lines <- read.table(i)
>
> There is the same problem with Ivan's solution the command grep -E
> '^(N053|N163)' test.txt works fine under the bash command but not i <-
> pipe("grep -E '^(N053|N163)' test.txt"); lines <- read.table(i)
>
> May be it is because I work with MS windows ?
>
> thx
> LP
>
>
>
>
> Le 24/05/2020 ? 04:34, William Michels a ?crit :
> > Hi Laurent,
> >
> > Seeking to give you an "R-only" solution, I thought the read.fwf()
> > function might be useful (to read-in your first column of data, only).
> > However Jeff is correct that this is a poor strategy, since read.fwf()
> > reads the entire file into R (documented in "Fixed-width-format
> > files", Section 2.2: R Data Import/Export Manual).
> >
> > Jeff has suggested a number of packages, as well as using a database.
> > Ivan Krylov has posted answers using grep, awk and perl (perl5--to
> > disambiguate). [In point of fact, the R Data Import/Export Manual
> > suggests using perl]. Similar to Ivan, I've posted code below using
> > the Raku programming language (the language formerly known as Perl6).
> > Regexes are claimed to be more readable, but are currently very slow
> > in Raku. However on the plus side, the language is designed to handle
> > Unicode gracefully:
> >
> >> # pipe() using raku-grep on Laurent's data (sep=mult whitespace):
> >> con_obj1 <- pipe(paste("raku -e '.put for lines.grep( / ^^N053 | ^^N163 /, :p );' ", "Laurents.txt"), open="rt");
> >> p6_import_a <- scan(file=con_obj1, what=list("","","","","","","","","",""), flush=TRUE, multi.line=FALSE, quiet=TRUE);
> >> close(con_obj1);
> >> as.data.frame(sapply(p6_import_a, t), stringsAsFactors=FALSE);
> >    V1   V2        V3        V4        V5        V6        V7        V8
> >        V9       V10
> > 1  2 N053 -0.014083 -0.004741  0.001443 -0.010152 -0.012996 -0.005337
> > -0.008738 -0.015094
> > 2  4 N163 -0.054023 -0.049345 -0.037158  -0.04112 -0.044612 -0.036953
> > -0.036061 -0.044516
> >> # pipe() using raku-grep "starts-with" to find genbankID ( >3GB TSV file)
> >> # "lines[0..5]" restricts raku to reading first 6 lines!
> >> # change "lines[0..5]" to "lines" to run raku code on whole file:
> >> con_obj2 <- pipe(paste("raku -e '.put for lines[0..5].grep( *.starts-with(q[A00145]), :p);' ", "genbankIDs_3GB.tsv"), "rt");
> >> p6_import_b <- read.table(con_obj2, sep="\t");
> >> close(con_obj2)
> >> p6_import_b
> >    V1     V2       V3          V4 V5
> > 1  4 A00145 A00145.1 IFN-alpha A NA
> >> # unicode test using R's system() function:
> >> try(system("raku -ne '.grep( /  ??  |  ?????  |  ?????  |  ??????  /, :v ).put;'  hello_7lang.txt", intern = TRUE, ignore.stderr = FALSE))
> > [1] ""                    ""                    ""
> > "?? Chinese"
> > [5] "????? Japanese" "????? Arabic"        "?????? Russian"
> > [special thanks to Brad Gilbert, Joseph Brenner and others on the
> > perl6-users mailing list. All errors above are my own.]
> >
> > HTH, Bill.
> >
> > W. Michels, Ph.D.
> >
> >
> >
> >
> > On Fri, May 22, 2020 at 4:48 AM Laurent Rhelp <LaurentRHelp at free.fr> wrote:
> >> Hi Ivan,
> >>     Endeed, it is a good idea. I am under MSwindows but I can use the
> >> bash command I use with git. I will see how to do that with the unix
> >> command lines.
> >>
> >>
> >> Le 20/05/2020 ? 09:46, Ivan Krylov a ?crit :
> >>> Hi Laurent,
> >>>
> >>> I am not saying this will work every time and I do recognise that this
> >>> is very different from a more general solution that you had envisioned,
> >>> but if you are on an UNIX-like system or have the relevant utilities
> >>> installed and on the %PATH% on Windows, you can filter the input file
> >>> line-by-line using a pipe and an external program:
> >>>
> >>> On Sun, 17 May 2020 15:52:30 +0200
> >>> Laurent Rhelp <LaurentRHelp at free.fr> wrote:
> >>>
> >>>> # sensors to keep
> >>>> sensors <-  c("N053", "N163")
> >>> # filter on the beginning of the line
> >>> i <- pipe("grep -E '^(N053|N163)' test.txt")
> >>> # or:
> >>> # filter on the beginning of the given column
> >>> # (use $2 for the second column, etc.)
> >>> i <- pipe("awk '($1 ~ \"^(N053|N163)\")' test.txt")
> >>> # or:
> >>> # since your message is full of Unicode non-breaking spaces, I have to
> >>> # bring in heavier machinery to handle those correctly;
> >>> # only this solution manages to match full column values
> >>> # (here you can also use $F[1] for second column and so on)
> >>> i <- pipe("perl -CSD -F'\\s+' -lE \\
> >>>    'print join qq{\\t}, @F if $F[0] =~ /^(N053|N163)$/' \\
> >>>    test.txt
> >>> ")
> >>> lines <- read.table(i) # closes i when done
> >>>
> >>> The downside of this approach is having to shell-escape the command
> >>> lines, which can become complicated, and choosing between use of regular
> >>> expressions and more wordy programs (Unicode whitespace in the input
> >>> doesn't help, either).
> >>>
> >>
> >> --
> >> L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e par le logiciel antivirus Avast.
> >> https://www.avast.com/antivirus
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> L'absence de virus dans ce courrier ?lectronique a ?t? v?rifi?e par le logiciel antivirus Avast.
> https://www.avast.com/antivirus
>


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Thu May 28 00:57:36 2020
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard McGarvey)
Date: Wed, 27 May 2020 18:57:36 -0400 (EDT)
Subject: [R] Plotting shipment routs on a world map
Message-ID: <820853453.54341.1590620257014@connect.xfinity.com>

I have used the ggplot2 package to create a world map, the png file of the output is attached. The code I use is below:

library(ggplot2)
library(mapproj)
long_Min <- -180.0
long_Max <- 180.0
lat_Min <- -50.0
lat_Max <- 80.0
Width <- 12.0
Height <- 2.0*Width*(lat_Max-lat_Min)/(long_Max-long_Min)
windows(width = Width, height = Height,record = TRUE)
World_map_data <- map_data("world")
World_map_dataS <- subset(World_map_data, (long >= long_Min & long <= long_Max) & (lat >= lat_Min & lat <= lat_Max))
#Europe_data <- map_data("europe")
O_lat <- runif(10, min=lat_Min, max=lat_Max)
O_long <- runif(10, min=long_Min, max=long_Max)
D_lat <- runif(10, min=lat_Min, max=lat_Max)
D_long <- runif(10, min=long_Min, max=long_Max)
Routes = data.frame(O_lat,O_long, D_lat,D_long)
g1 <- ggplot(World_map_dataS, aes(x=long, y=lat, group=group, fill=region)) +
  geom_polygon(fill="white", colour="blue") +
  coord_map("mercator")
g1


I have a list of shipments with the lat/long of the Origin cities and destination cities in a dataframe Routes. A test example is shown below:

structure(list(O_lat = c(57.8440704662353, 43.4692783257924, 
34.197948181536, -13.9658199064434, 28.1358464458026, 54.8644948145375, 
54.7105941944756, 0.960986674763262, -9.04949014307931, 67.3708150233142
), O_long = c(-135.800734693184, -139.97953729704, 130.949327526614, 
120.016278969124, -81.3588178250939, 107.188451411203, -166.396527299657, 
6.01017326116562, 119.992827912793, 68.8450227119029), D_lat = c(13.1392757641152, 
-48.4003935428336, -18.0977397086099, 7.47306475648656, 38.8207479682751, 
-6.95962310535833, 21.4299688511528, 77.7941568545066, -39.2934810533188, 
-5.45942842029035), D_long = c(-161.594757176936, -8.55208304710686, 
59.6322040446103, 16.3289373647422, -82.6615255884826, -150.326664168388, 
-86.496180742979, -162.877350552008, -119.206758281216, -110.952316028997
)), class = "data.frame", row.names = c(NA, -10L))


this dataframe consists of 10 shipments. How do I add these shipments as individual lines onto the world map. If I need to I can convert from lat & long to map coordinates using the mercator projection equations. My issue is what ggplot commands to use.

Thanks

Bernard McGarvey


Director, Fort Myers Beach Lions Foundation, Inc.


Retired (Lilly Engineering Fellow).
-------------- next part --------------
A non-text attachment was scrubbed...
Name: World_Map.png
Type: image/png
Size: 33279 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200527/98872835/attachment.png>

From @gent@ @end|ng |rom medd@t@|nc@com  Thu May 28 03:37:45 2020
From: @gent@ @end|ng |rom medd@t@|nc@com (H)
Date: Wed, 27 May 2020 21:37:45 -0400
Subject: [R] Extracting data from list of lists of dataframes
Message-ID: <493853f2-1ad6-86e5-d646-e885f2a37a72@meddatainc.com>

I am a newcomer to R and have been struggling with the following problem:

I have a list1(1:N) containing a list2(1:2) each containing dataframes with the same column names but different number of rows. Both lists are named and the rows and the columns in the dataframe are named. I then would like to use select() and filter() on the dataframes to select data.

I would like to: (1) select among the top-level lists, (2) select among the second-level lists and (3) finally use select() and filter() on the dataframe data.

My attempt so far is:

list1[grepl("myfirstselection", names(list1)][grepl("mysecondselection", names(list1[[]]))]

Needless to say it does not work... And, I have not even gotten to the dataframes yet... ;-)


From bgunter@4567 @end|ng |rom gm@||@com  Thu May 28 04:03:36 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 27 May 2020 19:03:36 -0700
Subject: [R] Extracting data from list of lists of dataframes
In-Reply-To: <493853f2-1ad6-86e5-d646-e885f2a37a72@meddatainc.com>
References: <493853f2-1ad6-86e5-d646-e885f2a37a72@meddatainc.com>
Message-ID: <CAGxFJbT-mhKsJkiXTW3pxDYTVcaVtmrbLSPv1u6-NskHa3Ep7A@mail.gmail.com>

?reproducible example (see posting guide below)
or
https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

However, I would suggest that you spend some time with an R tutorial or two
rather than flailing about with such queries here.  R has basic tools --
indexing by logical expressions, for example -- for such tasks that you
seem completely ignorant about.  You would do better to invest the time to
learn them. Just my opinion of course. Feel free to ignore.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, May 27, 2020 at 6:38 PM H <agents at meddatainc.com> wrote:

> I am a newcomer to R and have been struggling with the following problem:
>
> I have a list1(1:N) containing a list2(1:2) each containing dataframes
> with the same column names but different number of rows. Both lists are
> named and the rows and the columns in the dataframe are named. I then would
> like to use select() and filter() on the dataframes to select data.
>
> I would like to: (1) select among the top-level lists, (2) select among
> the second-level lists and (3) finally use select() and filter() on the
> dataframe data.
>
> My attempt so far is:
>
> list1[grepl("myfirstselection", names(list1)][grepl("mysecondselection",
> names(list1[[]]))]
>
> Needless to say it does not work... And, I have not even gotten to the
> dataframes yet... ;-)
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From john@m@h@rro|d @end|ng |rom gm@||@com  Thu May 28 05:22:56 2020
From: john@m@h@rro|d @end|ng |rom gm@||@com (John Harrold)
Date: Wed, 27 May 2020 20:22:56 -0700
Subject: [R] all.equal and use.names
Message-ID: <CANAiAiW6vfwEsG3WV43fs0mDB9n38wSHJbHX2BQffZnHxji7nw@mail.gmail.com>

Howdy Folks,

I believe I'm having trouble understanding the documentation for all.equal.
If I have two lists like this:

t1 = list(a = c(1,2,3),
          b = c("1", "2", "3"))
t2 = list( b = c("1", "2", "3"),
           a = c(1,2,3))

If I read the documentation correctly, by setting use.names equal to TRUE I
believe this comparison should evaluate as true:

all.equal(t1,t2, use.names=TRUE)

However, I get the following output:

which appears as though it is performing the comparison based on walking
through indices and comparing that way.

[1] "Names: 2 string mismatches"
[2] "Component 1: Modes: numeric, character"
[3] "Component 1: target is numeric, current is character"
[4] "Component 2: Modes: character, numeric"
[5] "Component 2: target is character, current is numeric"

Can someone tell me what I'm doing wrong here?
-- 
John
:wq

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Thu May 28 06:14:25 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 27 May 2020 21:14:25 -0700
Subject: [R] all.equal and use.names
In-Reply-To: <CANAiAiW6vfwEsG3WV43fs0mDB9n38wSHJbHX2BQffZnHxji7nw@mail.gmail.com>
References: <CANAiAiW6vfwEsG3WV43fs0mDB9n38wSHJbHX2BQffZnHxji7nw@mail.gmail.com>
Message-ID: <CAGxFJbSUaDH-Qk=vCC2CziZXCgi3Q3cz6wMdJBmX23KiCO4qSA@mail.gmail.com>

Nope. You misread I think. It says that use.names = TRUE causes mismatches
to be **reported** by name rather than index, not that it is recursing by
name. It still recurses by component indices.

However, I still think that is wrong. It is not reporting mismatches **by**
name -- it is reporting mismatches **in** names as well as in value.


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, May 27, 2020 at 8:23 PM John Harrold <john.m.harrold at gmail.com>
wrote:

> Howdy Folks,
>
> I believe I'm having trouble understanding the documentation for all.equal.
> If I have two lists like this:
>
> t1 = list(a = c(1,2,3),
>           b = c("1", "2", "3"))
> t2 = list( b = c("1", "2", "3"),
>            a = c(1,2,3))
>
> If I read the documentation correctly, by setting use.names equal to TRUE I
> believe this comparison should evaluate as true:
>
> all.equal(t1,t2, use.names=TRUE)
>
> However, I get the following output:
>
> which appears as though it is performing the comparison based on walking
> through indices and comparing that way.
>
> [1] "Names: 2 string mismatches"
> [2] "Component 1: Modes: numeric, character"
> [3] "Component 1: target is numeric, current is character"
> [4] "Component 2: Modes: character, numeric"
> [5] "Component 2: target is character, current is numeric"
>
> Can someone tell me what I'm doing wrong here?
> --
> John
> :wq
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From john@m@h@rro|d @end|ng |rom gm@||@com  Thu May 28 06:52:16 2020
From: john@m@h@rro|d @end|ng |rom gm@||@com (John Harrold)
Date: Wed, 27 May 2020 21:52:16 -0700
Subject: [R] all.equal and use.names
In-Reply-To: <CAGxFJbSUaDH-Qk=vCC2CziZXCgi3Q3cz6wMdJBmX23KiCO4qSA@mail.gmail.com>
References: <CANAiAiW6vfwEsG3WV43fs0mDB9n38wSHJbHX2BQffZnHxji7nw@mail.gmail.com>
 <CAGxFJbSUaDH-Qk=vCC2CziZXCgi3Q3cz6wMdJBmX23KiCO4qSA@mail.gmail.com>
Message-ID: <CANAiAiX=41dRAj5ZHLhuT8yqUO-DXTtwTVjawJ0a24PMYf1r6A@mail.gmail.com>

Is there a way to compare t1 and t2 above such that the name is used
instead of the index?

On Wed, May 27, 2020 at 9:14 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Nope. You misread I think. It says that use.names = TRUE causes mismatches
> to be **reported** by name rather than index, not that it is recursing by
> name. It still recurses by component indices.
>
> However, I still think that is wrong. It is not reporting mismatches
> **by** name -- it is reporting mismatches **in** names as well as in value.
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Wed, May 27, 2020 at 8:23 PM John Harrold <john.m.harrold at gmail.com>
> wrote:
>
>> Howdy Folks,
>>
>> I believe I'm having trouble understanding the documentation for
>> all.equal.
>> If I have two lists like this:
>>
>> t1 = list(a = c(1,2,3),
>>           b = c("1", "2", "3"))
>> t2 = list( b = c("1", "2", "3"),
>>            a = c(1,2,3))
>>
>> If I read the documentation correctly, by setting use.names equal to TRUE
>> I
>> believe this comparison should evaluate as true:
>>
>> all.equal(t1,t2, use.names=TRUE)
>>
>> However, I get the following output:
>>
>> which appears as though it is performing the comparison based on walking
>> through indices and comparing that way.
>>
>> [1] "Names: 2 string mismatches"
>> [2] "Component 1: Modes: numeric, character"
>> [3] "Component 1: target is numeric, current is character"
>> [4] "Component 2: Modes: character, numeric"
>> [5] "Component 2: target is character, current is numeric"
>>
>> Can someone tell me what I'm doing wrong here?
>> --
>> John
>> :wq
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

-- 
John
:wq

	[[alternative HTML version deleted]]


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Thu May 28 09:06:57 2020
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Thu, 28 May 2020 09:06:57 +0200
Subject: [R] How to use msaPrettyPrint in memory efficient mode?
Message-ID: <CAMk+s2Te3CqumDsof-dUwuZK+ygQhPkfBikKgOxWku7_oR3duw@mail.gmail.com>

Hello,
I have an alignment made with the package MSA. I installed latex on my
ubuntu machine with
`sudo apt-get install texlive-full` but I could not find the package
texshade that is mentioned in the MSA's manual.
When I run msaPrettyPrint I get:
```

Multiple alignment written to temporary file
/tmp/RtmpmW9Nnm/seq12f4d6e21e744.fasta
File aln.tex created
This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian)
(preloaded format=pdflatex)
 restricted \write18 enabled.
entering extended mode
(./aln.tex
LaTeX2e <2020-02-02> patch level 2
L3 programming layer <2020-02-14>
(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls
Document Class: article 2019/12/20 v1.4l Standard LaTeX document class
(/usr/share/texlive/texmf-dist/tex/latex/base/size10.clo))
(/home/gigiux/R/x86_64-pc-linux-gnu-library/4.0/msa/tex/texshade.sty
Package `texshade', Version 1.24 of 2011/12/01.
(/usr/share/texlive/texmf-dist/tex/latex/graphics/color.sty
(/usr/share/texlive/texmf-dist/tex/latex/graphics-cfg/color.cfg)
(/usr/share/texlive/texmf-dist/tex/latex/graphics-def/dvips.def)
(/usr/share/texlive/texmf-dist/tex/latex/graphics/dvipsnam.def))
(/usr/share/texlive/texmf-dist/tex/latex/graphics/graphics.sty
(/usr/share/texlive/texmf-dist/tex/latex/graphics/trig.sty)
(/usr/share/texlive/texmf-dist/tex/latex/graphics-cfg/graphics.cfg)
(/usr/share/texlive/texmf-dist/tex/latex/graphics-def/pdftex.def))
(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amssymb.sty
(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amsfonts.sty)))
(/usr/share/texlive/texmf-dist/tex/latex/l3backend/l3backend-pdfmode.def)
No file aln.aux.
(/usr/share/texlive/texmf-dist/tex/context/base/mkii/supp-pdf.mkii
[Loading MPS to PDF converter (version 2006.09.02).]
) (/usr/share/texlive/texmf-dist/tex/latex/epstopdf-pkg/epstopdf-base.sty
(/usr/share/texlive/texmf-dist/tex/latex/latexconfig/epstopdf-sys.cfg))
(/tmp/RtmpmW9Nnm/seq12f4d6e21e744.fasta:
! TeX capacity exceeded, sorry [save size=80000].
\inf@@get ...@ {#5} \else \def \fourth@ {99999999}
                                                   \fi \fi \def \fifth@ {#5}...
l.25 ...e}{/tmp/RtmpmW9Nnm/seq12f4d6e21e744.fasta}

!  ==> Fatal error occurred, no output PDF file produced!
Transcript written on aln.log.
Error in texi2dvi(texfile, quiet = !verbose, pdf = identical(output, "pdf"),  :
  unable to run pdflatex on 'aln.tex'
LaTeX errors:
! TeX capacity exceeded, sorry [save size=80000].
\inf@@get ...@ {#5} \else \def \fourth@ {99999999}
                                                   \fi \fi \def \fifth@ {#5}...
l.25 ...e}{/tmp/RtmpmW9Nnm/seq12f4d6e21e744.fasta}

!  ==> Fatal error occurred, no output PDF file produced!

>
```
Looks like texshade has been installed but there is a problem with
memory. How can I set the memory limit for msaPrettyPrint?
-- 
Best regards,
Luigi


From er|cjberger @end|ng |rom gm@||@com  Thu May 28 10:10:27 2020
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Thu, 28 May 2020 11:10:27 +0300
Subject: [R] all.equal and use.names
In-Reply-To: <CANAiAiX=41dRAj5ZHLhuT8yqUO-DXTtwTVjawJ0a24PMYf1r6A@mail.gmail.com>
References: <CANAiAiW6vfwEsG3WV43fs0mDB9n38wSHJbHX2BQffZnHxji7nw@mail.gmail.com>
 <CAGxFJbSUaDH-Qk=vCC2CziZXCgi3Q3cz6wMdJBmX23KiCO4qSA@mail.gmail.com>
 <CANAiAiX=41dRAj5ZHLhuT8yqUO-DXTtwTVjawJ0a24PMYf1r6A@mail.gmail.com>
Message-ID: <CAGgJW77om2EGrLF4pzbwApOt_J9roWmXhUhYj+Ow1S4JzD5jCg@mail.gmail.com>

Why not just reorder the elements of the list so they should match?

t1 <- t1[ names(t1)[order(names(t1))]]
t2 <- t2[ names(t2)[order(names(t2))]]

identical(t1,t2)




On Thu, May 28, 2020 at 7:52 AM John Harrold <john.m.harrold at gmail.com>
wrote:

> Is there a way to compare t1 and t2 above such that the name is used
> instead of the index?
>
> On Wed, May 27, 2020 at 9:14 PM Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
>
> > Nope. You misread I think. It says that use.names = TRUE causes
> mismatches
> > to be **reported** by name rather than index, not that it is recursing by
> > name. It still recurses by component indices.
> >
> > However, I still think that is wrong. It is not reporting mismatches
> > **by** name -- it is reporting mismatches **in** names as well as in
> value.
> >
> >
> > Bert Gunter
> >
> > "The trouble with having an open mind is that people keep coming along
> and
> > sticking things into it."
> > -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> >
> >
> > On Wed, May 27, 2020 at 8:23 PM John Harrold <john.m.harrold at gmail.com>
> > wrote:
> >
> >> Howdy Folks,
> >>
> >> I believe I'm having trouble understanding the documentation for
> >> all.equal.
> >> If I have two lists like this:
> >>
> >> t1 = list(a = c(1,2,3),
> >>           b = c("1", "2", "3"))
> >> t2 = list( b = c("1", "2", "3"),
> >>            a = c(1,2,3))
> >>
> >> If I read the documentation correctly, by setting use.names equal to
> TRUE
> >> I
> >> believe this comparison should evaluate as true:
> >>
> >> all.equal(t1,t2, use.names=TRUE)
> >>
> >> However, I get the following output:
> >>
> >> which appears as though it is performing the comparison based on walking
> >> through indices and comparing that way.
> >>
> >> [1] "Names: 2 string mismatches"
> >> [2] "Component 1: Modes: numeric, character"
> >> [3] "Component 1: target is numeric, current is character"
> >> [4] "Component 2: Modes: character, numeric"
> >> [5] "Component 2: target is character, current is numeric"
> >>
> >> Can someone tell me what I'm doing wrong here?
> >> --
> >> John
> >> :wq
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
>
> --
> John
> :wq
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Thu May 28 10:53:58 2020
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Thu, 28 May 2020 10:53:58 +0200
Subject: [R] all.equal and use.names
In-Reply-To: <CANAiAiX=41dRAj5ZHLhuT8yqUO-DXTtwTVjawJ0a24PMYf1r6A@mail.gmail.com>
References: <CANAiAiW6vfwEsG3WV43fs0mDB9n38wSHJbHX2BQffZnHxji7nw@mail.gmail.com>
 <CAGxFJbSUaDH-Qk=vCC2CziZXCgi3Q3cz6wMdJBmX23KiCO4qSA@mail.gmail.com>
 <CANAiAiX=41dRAj5ZHLhuT8yqUO-DXTtwTVjawJ0a24PMYf1r6A@mail.gmail.com>
Message-ID: <24271.31782.943312.435830@stat.math.ethz.ch>

Note:  all.equal() with all its S3 methods is implemented entirely in R
       code, so it should not be hard to find out where things happen
       and how.

>>>>> John Harrold 
>>>>>     on Wed, 27 May 2020 21:52:16 -0700 writes:

    > Is there a way to compare t1 and t2 above such that the
    > name is used instead of the index?

I think that may be a reasonable feature request.
If you sit down look at the R codes and muse a bit, you may even get to propose
a new optional argument to the all.equal.list() method.
Note the relevant R code is all in <R>/src/library/base/R/all.equal.R
development version (true source!) at
   https://svn.r-project.org/R/trunk/src/library/base/R/all.equal.R

--> would be a topic for R-devel (rather than R-help) though.

Best,
Martin

    > On Wed, May 27, 2020 at 9:14 PM Bert Gunter
    > <bgunter.4567 at gmail.com> wrote:

    >> Nope. You misread I think. It says that use.names = TRUE
    >> causes mismatches to be **reported** by name rather than
    >> index, not that it is recursing by name. It still
    >> recurses by component indices.
    >> 
    >> However, I still think that is wrong. It is not reporting
    >> mismatches **by** name -- it is reporting mismatches
    >> **in** names as well as in value.
    >> 
    >> 
    >> Bert Gunter
    >> 
    >> "The trouble with having an open mind is that people keep
    >> coming along and sticking things into it."  -- Opus (aka
    >> Berkeley Breathed in his "Bloom County" comic strip )
    >> 
    >> 
    >> On Wed, May 27, 2020 at 8:23 PM John Harrold
    >> <john.m.harrold at gmail.com> wrote:
    >> 
    >>> Howdy Folks,
    >>> 
    >>> I believe I'm having trouble understanding the
    >>> documentation for all.equal.  If I have two lists like
    >>> this:
    >>> 
    >>> t1 = list(a = c(1,2,3), b = c("1", "2", "3")) t2 = list(
    >>> b = c("1", "2", "3"), a = c(1,2,3))
    >>> 
    >>> If I read the documentation correctly, by setting
    >>> use.names equal to TRUE I believe this comparison should
    >>> evaluate as true:
    >>> 
    >>> all.equal(t1,t2, use.names=TRUE)
    >>> 
    >>> However, I get the following output:
    >>> 
    >>> which appears as though it is performing the comparison
    >>> based on walking through indices and comparing that way.
    >>> 
    >>> [1] "Names: 2 string mismatches" [2] "Component 1:
    >>> Modes: numeric, character" [3] "Component 1: target is
    >>> numeric, current is character" [4] "Component 2: Modes:
    >>> character, numeric" [5] "Component 2: target is
    >>> character, current is numeric"
    >>> 
    >>> Can someone tell me what I'm doing wrong here?
    >>> --
    >>> John :wq
    >>> 
    >>> [[alternative HTML version deleted]]
    >>> 
    >>> ______________________________________________
    >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    >>> more, see https://stat.ethz.ch/mailman/listinfo/r-help
    >>> PLEASE do read the posting guide
    >>> http://www.R-project.org/posting-guide.html and provide
    >>> commented, minimal, self-contained, reproducible code.
    >>> 
    >> 

    > -- 
    > John :wq

    > 	[[alternative HTML version deleted]]

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    > more, see https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide
    > http://www.R-project.org/posting-guide.html and provide
    > commented, minimal, self-contained, reproducible code.


From john@m@h@rro|d @end|ng |rom gm@||@com  Thu May 28 17:44:58 2020
From: john@m@h@rro|d @end|ng |rom gm@||@com (John Harrold)
Date: Thu, 28 May 2020 08:44:58 -0700
Subject: [R] all.equal and use.names
In-Reply-To: <24271.31782.943312.435830@stat.math.ethz.ch>
References: <CANAiAiW6vfwEsG3WV43fs0mDB9n38wSHJbHX2BQffZnHxji7nw@mail.gmail.com>
 <CAGxFJbSUaDH-Qk=vCC2CziZXCgi3Q3cz6wMdJBmX23KiCO4qSA@mail.gmail.com>
 <CANAiAiX=41dRAj5ZHLhuT8yqUO-DXTtwTVjawJ0a24PMYf1r6A@mail.gmail.com>
 <24271.31782.943312.435830@stat.math.ethz.ch>
Message-ID: <CANAiAiW5o8GhuO-PvXacM97B2HTjNrvUxaYet3Sy+SMTGORULQ@mail.gmail.com>

Howdy Folks,

Eric - I'm just using this as a toy example to illustrate what I'm trying
to do. The actual lists I'm trying to compare are much larger and more
complex. So sorting them like that is something I'd have to do recursively.
I wanted to take advantage of all.equal because I want to be able to
identify the elements that are missing in one vs the other. And my
understanding is that identical() won't give me that kind of out put.

I think what I'm going to do is flatten both lists using unlist, and then
compare them using the names of the two flattened lists.

Thanks
John

On Thu, May 28, 2020 at 1:54 AM Martin Maechler <maechler at stat.math.ethz.ch>
wrote:

> Note:  all.equal() with all its S3 methods is implemented entirely in R
>        code, so it should not be hard to find out where things happen
>        and how.
>
> >>>>> John Harrold
> >>>>>     on Wed, 27 May 2020 21:52:16 -0700 writes:
>
>     > Is there a way to compare t1 and t2 above such that the
>     > name is used instead of the index?
>
> I think that may be a reasonable feature request.
> If you sit down look at the R codes and muse a bit, you may even get to
> propose
> a new optional argument to the all.equal.list() method.
> Note the relevant R code is all in <R>/src/library/base/R/all.equal.R
> development version (true source!) at
>    https://svn.r-project.org/R/trunk/src/library/base/R/all.equal.R
>
> --> would be a topic for R-devel (rather than R-help) though.
>
> Best,
> Martin
>
>     > On Wed, May 27, 2020 at 9:14 PM Bert Gunter
>     > <bgunter.4567 at gmail.com> wrote:
>
>     >> Nope. You misread I think. It says that use.names = TRUE
>     >> causes mismatches to be **reported** by name rather than
>     >> index, not that it is recursing by name. It still
>     >> recurses by component indices.
>     >>
>     >> However, I still think that is wrong. It is not reporting
>     >> mismatches **by** name -- it is reporting mismatches
>     >> **in** names as well as in value.
>     >>
>     >>
>     >> Bert Gunter
>     >>
>     >> "The trouble with having an open mind is that people keep
>     >> coming along and sticking things into it."  -- Opus (aka
>     >> Berkeley Breathed in his "Bloom County" comic strip )
>     >>
>     >>
>     >> On Wed, May 27, 2020 at 8:23 PM John Harrold
>     >> <john.m.harrold at gmail.com> wrote:
>     >>
>     >>> Howdy Folks,
>     >>>
>     >>> I believe I'm having trouble understanding the
>     >>> documentation for all.equal.  If I have two lists like
>     >>> this:
>     >>>
>     >>> t1 = list(a = c(1,2,3), b = c("1", "2", "3")) t2 = list(
>     >>> b = c("1", "2", "3"), a = c(1,2,3))
>     >>>
>     >>> If I read the documentation correctly, by setting
>     >>> use.names equal to TRUE I believe this comparison should
>     >>> evaluate as true:
>     >>>
>     >>> all.equal(t1,t2, use.names=TRUE)
>     >>>
>     >>> However, I get the following output:
>     >>>
>     >>> which appears as though it is performing the comparison
>     >>> based on walking through indices and comparing that way.
>     >>>
>     >>> [1] "Names: 2 string mismatches" [2] "Component 1:
>     >>> Modes: numeric, character" [3] "Component 1: target is
>     >>> numeric, current is character" [4] "Component 2: Modes:
>     >>> character, numeric" [5] "Component 2: target is
>     >>> character, current is numeric"
>     >>>
>     >>> Can someone tell me what I'm doing wrong here?
>     >>> --
>     >>> John :wq
>     >>>
>     >>> [[alternative HTML version deleted]]
>     >>>
>     >>> ______________________________________________
>     >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
>     >>> more, see https://stat.ethz.ch/mailman/listinfo/r-help
>     >>> PLEASE do read the posting guide
>     >>> http://www.R-project.org/posting-guide.html and provide
>     >>> commented, minimal, self-contained, reproducible code.
>     >>>
>     >>
>
>     > --
>     > John :wq
>
>     >   [[alternative HTML version deleted]]
>
>     > ______________________________________________
>     > R-help at r-project.org mailing list -- To UNSUBSCRIBE and
>     > more, see https://stat.ethz.ch/mailman/listinfo/r-help
>     > PLEASE do read the posting guide
>     > http://www.R-project.org/posting-guide.html and provide
>     > commented, minimal, self-contained, reproducible code.
>


-- 
John
:wq

	[[alternative HTML version deleted]]


From je|| @end|ng |rom @tr@nd@tech  Thu May 28 01:14:08 2020
From: je|| @end|ng |rom @tr@nd@tech (Jeff Enos)
Date: Wed, 27 May 2020 19:14:08 -0400
Subject: [R] [R-pkgs] strand: A Framework for Investment Strategy Simulation
Message-ID: <CADu7Xui2NUcCaRZuYMCfb842_=G6+_F-Z39dDHxKr33t6Xpbpw@mail.gmail.com>

Hi all,

I'm pleased to announce that the 'strand' package is now available on CRAN.

The package provides a framework for performing discrete (share-level)
simulations of investment strategies. Simulated portfolios optimize
exposure to an input signal subject to constraints such as position size,
factor and category exposure limits, and trading limits. Features include
YAML-based configuration, realistic trade filling based on percentage of
actual volume, and automatic loosening of exposure constraints if no
solution is found.

CRAN: https://CRAN.R-project.org/package=strand
GitHub: https://github.com/strand-tech/strand

Please let me know if you have any feedback or questions.

Best,
Jeff Enos

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From dw|n@em|u@ @end|ng |rom comc@@t@net  Fri May 29 01:59:23 2020
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Thu, 28 May 2020 16:59:23 -0700
Subject: [R] Partial matching list elements in R 4.0
In-Reply-To: <CAGxFJbSBshZicnf_-75o0fYcCdfbNA2HajdOFnjZmJDy-AYLgw@mail.gmail.com>
References: <CANAiAiXk++sowVnKf_VRL4-YtC_3dNNknqO+jjVzeOjU7udCKw@mail.gmail.com>
 <CAGxFJbSBshZicnf_-75o0fYcCdfbNA2HajdOFnjZmJDy-AYLgw@mail.gmail.com>
Message-ID: <0742f04b-489e-081d-378a-edc5f643e740@comcast.net>

There is a modifyList function in pkg utils that is used extensively in 
the code for lattice graphics:


 ?var$options <- modifyList(var$options, list( misc=list(abc = "123"), 
mi= list(something??? = 13))
 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?? )

#-------------------

 > var
$options
$options$mi
$options$mi$something
[1] 13


$options$misc
$options$misc$abc
[1] "123"

On 5/26/20 11:31 AM, Bert Gunter wrote:
> I can't answer your question (about your R programming skills) but the
> behavior you complain about is as documented. In particular:
>
> "Thus the default behaviour is to use partial matching only when extracting
> from recursive objects (except environments) by $. Even in that case,
> warnings can be switched on by options
> <http://127.0.0.1:39592/help/library/base/help/options>(warnPartialMatchDollar
> = TRUE)."
>
> So the solution is not to use $ for list extraction/replacement. Though
> convenient, it is prone to such issues. Instead, the following works (as
> does your suggested solution, of course):
>
>> var <- list()
>> var[["options"]][["misc"]][["abc"]] <- "123"
>> var[["options"]][["mi"]][["something"]] <- 13
>> var
> $options
> $options$misc
> $options$misc$abc
> [1] "123"
>
>
> $options$mi
> $options$mi$something
> [1] 13
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Tue, May 26, 2020 at 10:45 AM John Harrold <john.m.harrold at gmail.com>
> wrote:
>
>> Hello,
>>
>>
>> I'm testing some code in R 4.0, and I'm having an issue with the following"
>>
>> # -------------
>> rm(list=ls())
>> graphics.off()
>> #load("/tmp/post.RData")
>> var = list();
>> # If I uncomment this it fixes things:
>> # var$options = list(mi   = list(),
>> #                    misc = list())
>> #
>> var$options$misc$abc = "123"
>> var$options$mi$something    = 13
>> #------------
>>
>> This is a stripped down example but it exhibits the issue I"m having.
>> Basically when I create the list element var$options$mi the contents of
>> var$options$misc move over to var$options$mi. And what was in
>> var$options$misc become NULL:
>>
>> So now var$options looks like:
>>
>> var$options
>> $misc
>> $misc$abc
>> NULL
>>
>> $mi
>> $mi$abc
>> [1] "123"
>> $mi$something
>> [1] 13
>>
>> This worked (still works) in R 3.5.1. I understand partial matching, but is
>> this normal lists moving over to elements like this? I can uncomment the
>> text mentioned in the example and it seems to fix it, but I'm wondering if
>> this is a bug or just my poor programming coming back to bite me.
>>
>> I've included my sessionInfo() at the bottom.
>>
>> Thanks
>> John
>> :wq
>>
>>
>> sessionInfo()
>>
>> R version 4.0.0 (2020-04-24)
>>
>> Platform: x86_64-apple-darwin17.0 (64-bit)
>>
>> Running under: macOS Mojave 10.14.5
>>
>>
>> Matrix products: default
>>
>> BLAS:
>> /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
>>
>> LAPACK:
>> /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
>>
>>
>> locale:
>>
>> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>>
>>
>> attached base packages:
>>
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>>
>> other attached packages:
>>
>> [1] gdata_2.18.0  ggplot2_3.3.0 deSolve_1.28
>>
>>
>> loaded via a namespace (and not attached):
>>
>>   [1] Rcpp_1.0.4.6     gtools_3.8.2     withr_2.2.0      assertthat_0.2.1
>>
>>   [5] dplyr_0.8.5      digest_0.6.25    crayon_1.3.4     grid_4.0.0
>>
>>   [9] R6_2.4.1         lifecycle_0.2.0  gtable_0.3.0     magrittr_1.5
>>
>> [13] scales_1.1.1     pillar_1.4.4     rlang_0.4.6      vctrs_0.3.0
>>
>> [17] ellipsis_0.3.1   glue_1.4.1       purrr_0.3.4      munsell_0.5.0
>>
>> [21] compiler_4.0.0   pkgconfig_2.0.3  colorspace_1.4-1 tidyselect_1.1.0
>>
>> [25] tibble_3.0.1
>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jwd @end|ng |rom @urewe@t@net  Fri May 29 07:03:18 2020
From: jwd @end|ng |rom @urewe@t@net (John)
Date: Thu, 28 May 2020 22:03:18 -0700
Subject: [R] Package installation problem
Message-ID: <20200528220318.34d998ec@Draco>

I'm not certain just what this problem is.  Trying to install the
"curl" package, which "tseries" wants results in the following error:

****
Package libcurl was not found in the pkg-config search path.
Perhaps you should add the directory containing `libcurl.pc'
to the PKG_CONFIG_PATH environment variable
Package 'libcurl', required by 'virtual:world', not found
Package libcurl was not found in the pkg-config search path.
Perhaps you should add the directory containing `libcurl.pc'
to the PKG_CONFIG_PATH environment variable
Package 'libcurl', required by 'virtual:world', not found
Using PKG_CFLAGS=
Using PKG_LIBS=-lcurl
------------------------- ANTICONF ERROR ---------------------------
Configuration failed because libcurl was not found. Try installing:
 * deb: libcurl4-openssl-dev (Debian, Ubuntu, etc)
 * rpm: libcurl-devel (Fedora, CentOS, RHEL)
 * csw: libcurl_dev (Solaris)
If libcurl is already installed, check that 'pkg-config' is in your
PATH and PKG_CONFIG_PATH contains a libcurl.pc file. If pkg-config
is unavailable you can set INCLUDE_DIR and LIB_DIR manually via:
R CMD INSTALL --configure-vars='INCLUDE_DIR=... LIB_DIR=...'
--------------------------------------------------------------------
ERROR: configuration failed for package ?curl?
* removing ?/home/john/R/x86_64-redhat-linux-gnu-library/3.6/curl?
Warning in install.packages :
  installation of package ?curl? had non-zero exit status
****

However, libcurl is installed, installed.  What does seem to missing is
"libcurl.pc".  The ../pkgconfig directory contains a number of
different *.pc files, but not that one.  I'm asking here first because
someone else may have already encountered and solved this.  I'm running
Fedora 32 with kde on a 4 X AMD system.  I just updated Fedora.  

JWDougherty


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri May 29 09:19:03 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 29 May 2020 00:19:03 -0700
Subject: [R] Package installation problem
In-Reply-To: <20200528220318.34d998ec@Draco>
References: <20200528220318.34d998ec@Draco>
Message-ID: <55FE904C-DF61-40B9-BAB0-87E52ADE58BD@dcn.davis.ca.us>

The library alone is not enough... you have to have the development headers for compiling code against the library.

On May 28, 2020 10:03:18 PM PDT, John via R-help <r-help at r-project.org> wrote:
>I'm not certain just what this problem is.  Trying to install the
>"curl" package, which "tseries" wants results in the following error:
>
>****
>Package libcurl was not found in the pkg-config search path.
>Perhaps you should add the directory containing `libcurl.pc'
>to the PKG_CONFIG_PATH environment variable
>Package 'libcurl', required by 'virtual:world', not found
>Package libcurl was not found in the pkg-config search path.
>Perhaps you should add the directory containing `libcurl.pc'
>to the PKG_CONFIG_PATH environment variable
>Package 'libcurl', required by 'virtual:world', not found
>Using PKG_CFLAGS=
>Using PKG_LIBS=-lcurl
>------------------------- ANTICONF ERROR ---------------------------
>Configuration failed because libcurl was not found. Try installing:
> * deb: libcurl4-openssl-dev (Debian, Ubuntu, etc)
> * rpm: libcurl-devel (Fedora, CentOS, RHEL)
> * csw: libcurl_dev (Solaris)
>If libcurl is already installed, check that 'pkg-config' is in your
>PATH and PKG_CONFIG_PATH contains a libcurl.pc file. If pkg-config
>is unavailable you can set INCLUDE_DIR and LIB_DIR manually via:
>R CMD INSTALL --configure-vars='INCLUDE_DIR=... LIB_DIR=...'
>--------------------------------------------------------------------
>ERROR: configuration failed for package ?curl?
>* removing ?/home/john/R/x86_64-redhat-linux-gnu-library/3.6/curl?
>Warning in install.packages :
>  installation of package ?curl? had non-zero exit status
>****
>
>However, libcurl is installed, installed.  What does seem to missing is
>"libcurl.pc".  The ../pkgconfig directory contains a number of
>different *.pc files, but not that one.  I'm asking here first because
>someone else may have already encountered and solved this.  I'm running
>Fedora 32 with kde on a 4 X AMD system.  I just updated Fedora.  
>
>JWDougherty
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From p@p@|ord500 @end|ng |rom gm@||@com  Fri May 29 08:32:24 2020
From: p@p@|ord500 @end|ng |rom gm@||@com (=?UTF-8?Q?Hiram_Sa=C3=BAl_Rebollar_Fuentes?=)
Date: Fri, 29 May 2020 01:32:24 -0500
Subject: [R] Problem with library path
Message-ID: <CABDYY2t-0r1X-F3A2g-YBW7=zFwjYDwrtET6WQkKfPC64nCJRg@mail.gmail.com>

Hi, I have a problem when I try to load a library, and I think it?s because
my username has a special character in it, the library path is
C:/Users/Sa?l/Documents/R/win-library/4.0, but R reads it as C:/Users/Sa??
Documents/R/win-library/4.0

How can this be fixed? Thanks!

	[[alternative HTML version deleted]]


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Fri May 29 15:59:59 2020
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Fri, 29 May 2020 15:59:59 +0200
Subject: [R] Excessive precision requested error in deSolve package
Message-ID: <CAMk+s2Q1n0qm4HrEnp0+YHLi+YAVJPrJZU8Qs_PA5qbkZsw0fw@mail.gmail.com>

Hello,
I have a system of the differential equation for epidemic model that runs as:
```
fun = function(X, Y, P) {
  # starting population
  S = Y[1]
  E = Y[2]
  I = Y[3]
  R = Y[4]
  N = S+E+I+R
  P = (S*I)/N
  # parms
  transm_rate = P["b"]
  incubation_time = P["k"]
  illness_time = P["r"]
  # final population
  dS = -transm_rate  * P
  dE = (transm_rate * P) - (incubation_time * E)
  dI = (incubation_time * E) - (illness_time * I)
  dR = illness_time * I
  list(c(dS, dE, dI, dR))
}
I0 = 1
S0 = 19e6
Tmax = 10
initPop = c(S=S0-I0, I = I0, R = 0, D = 0)
period = seq(1, Tmax, 1)
p = c(b = 0.8, k = 7, r = 10)
mod = ode(y = initPop, times = period, func = fun, parms = p)
```
when I run I get:
```

DLSODA-  At T (=R1), too much accuracy requested
      for precision of machine..  See TOLSF (=R2)
In above message, R1 = 2, R2 = nan

Warning messages:
1: In lsoda(y, times, func, parms, ...) :
  Excessive precision requested.  scale up `rtol' and `atol' e.g by
the factor 10
2: In lsoda(y, times, func, parms, ...) :
  Returning early. Results are accurate, as far as they go
```
What is this error about and how can I solve it?
Thank you

-- 
Best regards,
Luigi


From ||gge@ @end|ng |rom @t@t|@t|k@tu-dortmund@de  Fri May 29 17:54:29 2020
From: ||gge@ @end|ng |rom @t@t|@t|k@tu-dortmund@de (Uwe Ligges)
Date: Fri, 29 May 2020 17:54:29 +0200
Subject: [R] Problem with library path
In-Reply-To: <CABDYY2t-0r1X-F3A2g-YBW7=zFwjYDwrtET6WQkKfPC64nCJRg@mail.gmail.com>
References: <CABDYY2t-0r1X-F3A2g-YBW7=zFwjYDwrtET6WQkKfPC64nCJRg@mail.gmail.com>
Message-ID: <4fc023db-5124-7efd-f92f-d1a736830615@statistik.tu-dortmund.de>



On 29.05.2020 08:32, Hiram Sa?l Rebollar Fuentes wrote:
> Hi, I have a problem when I try to load a library, and I think it?s because
> my username has a special character in it, the library path is
> C:/Users/Sa?l/Documents/R/win-library/4.0, but R reads it as C:/Users/Sa??
> Documents/R/win-library/4.0

Simplest solution is to use a plain ASCII path without blanks.

Best,
Uwe Ligges


> 
> How can this be fixed? Thanks!
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tr@xp|@yer @end|ng |rom gm@||@com  Fri May 29 18:36:39 2020
From: tr@xp|@yer @end|ng |rom gm@||@com (=?UTF-8?Q?Martin_M=C3=B8ller_Skarbiniks_Pedersen?=)
Date: Fri, 29 May 2020 18:36:39 +0200
Subject: [R] Package installation problem
In-Reply-To: <20200528220318.34d998ec@Draco>
References: <20200528220318.34d998ec@Draco>
Message-ID: <CAGAA5beCMUnP-n-2eA6AbR1aD152cmV+1vFGrWNkzyBsEKQ+Zw@mail.gmail.com>

On Fri, 29 May 2020 at 07:03, John via R-help <r-help at r-project.org> wrote:

> I'm not certain just what this problem is.  Trying to install the
> "curl" package, which "tseries" wants results in the following error:
>
> ****
> Package libcurl was not found in the pkg-config search path.
> Perhaps you should add the directory containing `libcurl.pc'
>

Just run:
sudo dnf install libcurl-devel

Regards
Martin

	[[alternative HTML version deleted]]


From Seb@@t|en@B|hore| @end|ng |rom cogn|gencorp@com  Fri May 29 21:00:57 2020
From: Seb@@t|en@B|hore| @end|ng |rom cogn|gencorp@com (Sebastien Bihorel)
Date: Fri, 29 May 2020 19:00:57 +0000
Subject: [R] Creating file from raw connection
Message-ID: <MN2PR19MB3166AD14E13C493F151CDF02928F0@MN2PR19MB3166.namprd19.prod.outlook.com>

Hi,

Let's say I can extract the content of an Excel .xlsx file stored in a database and store it as raw content in an R object. What would be the proper way to "create" a .xlsx file and "transfer" the content of this obj into it? I took the example of an Excel file, but my question would extend to any kind of binary file.

Thank you in advance for your input

Sebastien


	[[alternative HTML version deleted]]


From Seb@@t|en@B|hore| @end|ng |rom cogn|gencorp@com  Fri May 29 21:12:14 2020
From: Seb@@t|en@B|hore| @end|ng |rom cogn|gencorp@com (Sebastien Bihorel)
Date: Fri, 29 May 2020 19:12:14 +0000
Subject: [R] Creating file from raw content
Message-ID: <MN2PR19MB3166B78FE959F876FCEA75A9928F0@MN2PR19MB3166.namprd19.prod.outlook.com>

Hi,

Let's say I can extract the content of an Excel .xlsx file stored in a database and store it as raw content in an R object. What would be the proper way to "create" a .xlsx file and "transfer" the content of this obj into it? I took the example of an Excel file, but my question would extend to any kind of binary file.

Thank you in advance for your input

Sebastien

From murdoch@dunc@n @end|ng |rom gm@||@com  Fri May 29 21:36:07 2020
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Fri, 29 May 2020 15:36:07 -0400
Subject: [R] Creating file from raw connection
In-Reply-To: <MN2PR19MB3166AD14E13C493F151CDF02928F0@MN2PR19MB3166.namprd19.prod.outlook.com>
References: <MN2PR19MB3166AD14E13C493F151CDF02928F0@MN2PR19MB3166.namprd19.prod.outlook.com>
Message-ID: <78e4dce2-2bd4-1867-8e6c-05dcec4a98a1@gmail.com>

On 29/05/2020 3:00 p.m., Sebastien Bihorel via R-help wrote:
> Hi,
> 
> Let's say I can extract the content of an Excel .xlsx file stored in a database and store it as raw content in an R object. What would be the proper way to "create" a .xlsx file and "transfer" the content of this obj into it? I took the example of an Excel file, but my question would extend to any kind of binary file.
> 
> Thank you in advance for your input

It depends on how the .xlsx was put in to the database and then 
extracted into R, but if it's just a copy of a file from disk, 
writeBin() will write it without changes.

Duncan Murdoch


From Seb@@t|en@B|hore| @end|ng |rom cogn|gencorp@com  Fri May 29 22:26:13 2020
From: Seb@@t|en@B|hore| @end|ng |rom cogn|gencorp@com (Sebastien Bihorel)
Date: Fri, 29 May 2020 20:26:13 +0000
Subject: [R] Creating file from raw connection
In-Reply-To: <78e4dce2-2bd4-1867-8e6c-05dcec4a98a1@gmail.com>
References: <MN2PR19MB3166AD14E13C493F151CDF02928F0@MN2PR19MB3166.namprd19.prod.outlook.com>,
 <78e4dce2-2bd4-1867-8e6c-05dcec4a98a1@gmail.com>
Message-ID: <MN2PR19MB316681B8E72F107B1F71C9FC928F0@MN2PR19MB3166.namprd19.prod.outlook.com>


Thanks Duncan

From: Duncan Murdoch <murdoch.duncan at gmail.com>
Sent: Friday, May 29, 2020 15:36
To: Sebastien Bihorel <Sebastien.Bihorel at cognigencorp.com>; r-help at r-project.org <r-help at r-project.org>
Subject: Re: [R] Creating file from raw connection 
?
On 29/05/2020 3:00 p.m., Sebastien Bihorel via R-help wrote:
> Hi,
> 
> Let's say I can extract the content of an Excel .xlsx file stored in a database and store it as raw content in an R object. What would be the proper way to "create" a .xlsx file and "transfer" the content of this obj into it? I took the example of an Excel file, but my question would extend to any kind of binary file.
> 
> Thank you in advance for your input

It depends on how the .xlsx was put in to the database and then 
extracted into R, but if it's just a copy of a file from disk, 
writeBin() will write it without changes.

Duncan Murdoch

From jwd @end|ng |rom @urewe@t@net  Sat May 30 04:43:52 2020
From: jwd @end|ng |rom @urewe@t@net (John)
Date: Fri, 29 May 2020 19:43:52 -0700
Subject: [R] Package installation problem
In-Reply-To: <CAGAA5beCMUnP-n-2eA6AbR1aD152cmV+1vFGrWNkzyBsEKQ+Zw@mail.gmail.com>
References: <20200528220318.34d998ec@Draco>
 <CAGAA5beCMUnP-n-2eA6AbR1aD152cmV+1vFGrWNkzyBsEKQ+Zw@mail.gmail.com>
Message-ID: <20200529194352.1d6a4325@Draco>

On Fri, 29 May 2020 18:36:39 +0200
Martin M?ller Skarbiniks Pedersen <traxplayer at gmail.com> wrote:

> sudo dnf install libcurl-devel

Martin, I was going to say I had already done that more than once,
using both the command line and the new "dnfdragora" package manager,
but then decided, "what the heck. I'll try again." This time the *.pc
file appeared where it belonged. So, thank you.  Evidently, I tweaked
the right thing, whatever it was, right before I gave up and wrote my
request, thinking that if FC32 broke my tseries installation, it would
have broken others. As usual the r-list is quicker, and much more
helpful. I'll pass this along to Fedora, since there seems to be a
problem of some kind.  libcurl-devel was already installed on my
system, the first thing I ckecked.  It was just the *.pc file that was
the problem. Thanks,

JWDougherty


From tr@xp|@yer @end|ng |rom gm@||@com  Sun May 31 02:56:40 2020
From: tr@xp|@yer @end|ng |rom gm@||@com (=?UTF-8?Q?Martin_M=C3=B8ller_Skarbiniks_Pedersen?=)
Date: Sun, 31 May 2020 02:56:40 +0200
Subject: [R] bug or just me
Message-ID: <CAGAA5bebkqDtosWhw0yojVbCqF3qsZ=ZxD2jEwBx8NUR32=Ybg@mail.gmail.com>

Hi,
  Is this a bug or just me doing something stupid?
  solve(m) never returns and eats 100% CPU.

> sessionInfo()
R version 3.6.3 (2020-02-29)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 20.04 LTS

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3
LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/liblapack.so.3

Random number generation:
 RNG:     Mersenne-Twister
 Normal:  Inversion
 Sample:  Rounding

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] compiler_3.6.3
> set.seed(1)
> m <- matrix(sample(1:25), nrow=5)
> solve(m)

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sun May 31 03:37:27 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sat, 30 May 2020 18:37:27 -0700
Subject: [R] bug or just me
In-Reply-To: <CAGAA5bebkqDtosWhw0yojVbCqF3qsZ=ZxD2jEwBx8NUR32=Ybg@mail.gmail.com>
References: <CAGAA5bebkqDtosWhw0yojVbCqF3qsZ=ZxD2jEwBx8NUR32=Ybg@mail.gmail.com>
Message-ID: <CAGxFJbTuNiDB3E0N6UK08jv90xCrKJz0pzdf0ihPC3tQb2L0VA@mail.gmail.com>

No clue.

Worked fine in R 4.0.0
I would try updating R.Maybe your BLAS got corrupted.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, May 30, 2020 at 5:57 PM Martin M?ller Skarbiniks Pedersen <
traxplayer at gmail.com> wrote:

> Hi,
>   Is this a bug or just me doing something stupid?
>   solve(m) never returns and eats 100% CPU.
>
> > sessionInfo()
> R version 3.6.3 (2020-02-29)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Ubuntu 20.04 LTS
>
> Matrix products: default
> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3
> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/liblapack.so.3
>
> Random number generation:
>  RNG:     Mersenne-Twister
>  Normal:  Inversion
>  Sample:  Rounding
>
> locale:
>  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
>  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
>  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
>  [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> loaded via a namespace (and not attached):
> [1] compiler_3.6.3
> > set.seed(1)
> > m <- matrix(sample(1:25), nrow=5)
> > solve(m)
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From er|cjberger @end|ng |rom gm@||@com  Sun May 31 07:20:18 2020
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Sun, 31 May 2020 08:20:18 +0300
Subject: [R] bug or just me
In-Reply-To: <CAGxFJbTuNiDB3E0N6UK08jv90xCrKJz0pzdf0ihPC3tQb2L0VA@mail.gmail.com>
References: <CAGAA5bebkqDtosWhw0yojVbCqF3qsZ=ZxD2jEwBx8NUR32=Ybg@mail.gmail.com>
 <CAGxFJbTuNiDB3E0N6UK08jv90xCrKJz0pzdf0ihPC3tQb2L0VA@mail.gmail.com>
Message-ID: <CAGgJW76hCV2MjWSqZaXHRiwUn4cPXhEiB3igZpD=Q978yWUQAQ@mail.gmail.com>

Hi Martin,
This is a known bug. Definitely related to Ubuntu (debian), libopenblas and
possibly specific hardware.
Here's a bug report on the Debian list

https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=961725

Best,
Eric


On Sun, May 31, 2020 at 4:38 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> No clue.
>
> Worked fine in R 4.0.0
> I would try updating R.Maybe your BLAS got corrupted.
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Sat, May 30, 2020 at 5:57 PM Martin M?ller Skarbiniks Pedersen <
> traxplayer at gmail.com> wrote:
>
> > Hi,
> >   Is this a bug or just me doing something stupid?
> >   solve(m) never returns and eats 100% CPU.
> >
> > > sessionInfo()
> > R version 3.6.3 (2020-02-29)
> > Platform: x86_64-pc-linux-gnu (64-bit)
> > Running under: Ubuntu 20.04 LTS
> >
> > Matrix products: default
> > BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3
> > LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/liblapack.so.3
> >
> > Random number generation:
> >  RNG:     Mersenne-Twister
> >  Normal:  Inversion
> >  Sample:  Rounding
> >
> > locale:
> >  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
> >  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
> >  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
> >  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
> >  [9] LC_ADDRESS=C               LC_TELEPHONE=C
> > [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
> >
> > attached base packages:
> > [1] stats     graphics  grDevices utils     datasets  methods   base
> >
> > loaded via a namespace (and not attached):
> > [1] compiler_3.6.3
> > > set.seed(1)
> > > m <- matrix(sample(1:25), nrow=5)
> > > solve(m)
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From neotrop|c@|@b@t@ @end|ng |rom gm@||@com  Sun May 31 17:05:14 2020
From: neotrop|c@|@b@t@ @end|ng |rom gm@||@com (Neotropical bat risk assessments)
Date: Sun, 31 May 2020 11:05:14 -0400
Subject: [R] Query on 3d plotting pacakges
In-Reply-To: <mailman.359211.1.1590919201.63164.r-help@r-project.org>
References: <mailman.359211.1.1590919201.63164.r-help@r-project.org>
Message-ID: <aaf1cfe1-3631-1112-ef10-1f0ee38f5e78@gmail.com>

Hi all,

Fumbling around trying to find a plot package to do 3D plots.
This will be an aid to separating bats by their vocal signatures.
What I need to do is plot *Fc *against *Sc* with the third dimension 
being the *density* of the data points in the Fc-Sc plot.

Data format is like this abbreviated sample.? Fc is a frequency in kHz 
and Sc is the characteristic slope? (octaves per second) of each call pulse.

Any suggestions, guidance greatly appreciated.
Bruce

Fc 	Sc
26.58 	-5.95
27.03 	-8.2
27.16 	-2.07
26.19 	-7.68
26.62 	-3.99
26.85 	-6.08
26.94 	0
26.1 	-5.74
26.62 	-5.96
26.85 	-4.05
26.98 	-4.09
26.02 	-5.69
26.53 	-7.89
26.62 	-2
26.8 	-4.04
28.73 	7
25.72 	-2.97
26.14 	-5.76
26.32 	-3.89
26.4 	0
26.32 	5.88


	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sun May 31 17:24:57 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 31 May 2020 08:24:57 -0700
Subject: [R] Query on 3d plotting pacakges
In-Reply-To: <aaf1cfe1-3631-1112-ef10-1f0ee38f5e78@gmail.com>
References: <mailman.359211.1.1590919201.63164.r-help@r-project.org>
 <aaf1cfe1-3631-1112-ef10-1f0ee38f5e78@gmail.com>
Message-ID: <CAGxFJbTZEpEJ6amTXR9P5dbaLpoE0PRzn3V8n4Dgp1wfw5H-rw@mail.gmail.com>

1. Search
2. Search!
3. Search!!

"3D plotting" at rseek.org (or R 3-d plotting on google)
CRAN plotting task view:

https://CRAN.R-project.org/view=Graphics

... and you don't even necessarily need 3D plotting if you encode the
density with color ? la heatmaps.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, May 31, 2020 at 8:05 AM Neotropical bat risk assessments <
neotropical.bats at gmail.com> wrote:

> Hi all,
>
> Fumbling around trying to find a plot package to do 3D plots.
> This will be an aid to separating bats by their vocal signatures.
> What I need to do is plot *Fc *against *Sc* with the third dimension
> being the *density* of the data points in the Fc-Sc plot.
>
> Data format is like this abbreviated sample.  Fc is a frequency in kHz
> and Sc is the characteristic slope  (octaves per second) of each call
> pulse.
>
> Any suggestions, guidance greatly appreciated.
> Bruce
>
> Fc      Sc
> 26.58   -5.95
> 27.03   -8.2
> 27.16   -2.07
> 26.19   -7.68
> 26.62   -3.99
> 26.85   -6.08
> 26.94   0
> 26.1    -5.74
> 26.62   -5.96
> 26.85   -4.05
> 26.98   -4.09
> 26.02   -5.69
> 26.53   -7.89
> 26.62   -2
> 26.8    -4.04
> 28.73   7
> 25.72   -2.97
> 26.14   -5.76
> 26.32   -3.89
> 26.4    0
> 26.32   5.88
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From neotrop|c@|@b@t@ @end|ng |rom gm@||@com  Sun May 31 19:16:02 2020
From: neotrop|c@|@b@t@ @end|ng |rom gm@||@com (Neotropical bat risk assessments)
Date: Sun, 31 May 2020 13:16:02 -0400
Subject: [R] Query on contour plots
In-Reply-To: <mailman.359211.1.1590919201.63164.r-help@r-project.org>
References: <mailman.359211.1.1590919201.63164.r-help@r-project.org>
Message-ID: <25a8922f-ca91-3c58-da96-7ad9d28c76a6@gmail.com>

Hi all,

While exploring? packages for 3D plots that several folks suggested (Tnx 
all!)
It seems what I really need is a contour plot.? This is not working int 
he Deducer GUI.

This will be an aid to separating bats by their vocal signatures.
What I need to do is plot *Fc *against *Sc* with the third dimension 
being the *density* of the data points in the Fc-Sc plot.

Data format is like this abbreviated sample.? Fc is a frequency in kHz 
and Sc is the characteristic slope? (octaves per second) of each call pulse.

Any suggestions, guidance greatly appreciated.
Bruce

Fc 	Sc
26.58 	-5.95
27.03 	-8.2
27.16 	-2.07
26.19 	-7.68
26.62 	-3.99
26.85 	-6.08
26.94 	0
26.1 	-5.74
26.62 	-5.96
26.85 	-4.05
26.98 	-4.09
26.02 	-5.69
26.53 	-7.89
26.62 	-2
26.8 	-4.04
28.73 	7
25.72 	-2.97
26.14 	-5.76
26.32 	-3.89
26.4 	0
26.32 	5.88


	[[alternative HTML version deleted]]


From jwd @end|ng |rom @urewe@t@net  Sat May 30 05:07:39 2020
From: jwd @end|ng |rom @urewe@t@net (John)
Date: Fri, 29 May 2020 20:07:39 -0700
Subject: [R] Package installation problem
In-Reply-To: <CAGgJW74jP7Vian183UwZYo-yej6Kj=0V9TCWwQ6qqKEj6_kabQ@mail.gmail.com>
References: <20200528220318.34d998ec@Draco>
 <CAGgJW74jP7Vian183UwZYo-yej6Kj=0V9TCWwQ6qqKEj6_kabQ@mail.gmail.com>
Message-ID: <20200529200739.11f0e6e3-6559@Draco>


On Fri, 29 May 2020 08:47:35 +0300
Eric Berger <ericjberger at gmail.com> wrote:

> Hi John,
> This is a bit off-topic for this mailing list as your issue is a
> linux, specifically Fedora, issue, and not R.
> I don't use Fedora but I did a quick Google search on
> 
>  fedora missing package .pc file
> 
> and that came back with a lot of hits. This one in particular should
> be a good place to start.
> It gives some explanation as to when installation will result in a .pc
> file (seems to relate to -dev versions).
> 
> https://stackoverflow.com/questions/33050517/supply-pc-file-for-pkg-config
> 
> HTH,
> Eric
> 
Thanks Eric.  I wasn't sure where the hick-up was (R or Fedora).  The
library the error message said couldn't be found was right where it
belonged. The first thing I did was check that the *devel file was in
the lib64 directory, and it was. The pc file was the only vagrant, and
it was not on the system at all. I'm going to try to get sense out of
Fedora about this because with "devel" files it should have been.  When
I reinstalled this latest time from the command line, the "pc" file was
placed just as it should have been.

The threads on stackoverflow are four years old, so this has been
around in one form or another for a while.

Again, thanks,
JWDougherty


From er|cjberger @end|ng |rom gm@||@com  Fri May 29 07:47:35 2020
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Fri, 29 May 2020 08:47:35 +0300
Subject: [R] Package installation problem
In-Reply-To: <20200528220318.34d998ec@Draco>
References: <20200528220318.34d998ec@Draco>
Message-ID: <CAGgJW74jP7Vian183UwZYo-yej6Kj=0V9TCWwQ6qqKEj6_kabQ-5716@mail.gmail.com>


Hi John,
This is a bit off-topic for this mailing list as your issue is a
linux, specifically Fedora, issue, and not R.
I don't use Fedora but I did a quick Google search on

 fedora missing package .pc file

and that came back with a lot of hits. This one in particular should
be a good place to start.
It gives some explanation as to when installation will result in a .pc
file (seems to relate to -dev versions).

https://stackoverflow.com/questions/33050517/supply-pc-file-for-pkg-config

HTH,
Eric


On Fri, May 29, 2020 at 8:03 AM John via R-help <r-help at r-project.org> wrote:
>
> I'm not certain just what this problem is.  Trying to install the
> "curl" package, which "tseries" wants results in the following error:
>
> ****
> Package libcurl was not found in the pkg-config search path.
> Perhaps you should add the directory containing `libcurl.pc'
> to the PKG_CONFIG_PATH environment variable
> Package 'libcurl', required by 'virtual:world', not found
> Package libcurl was not found in the pkg-config search path.
> Perhaps you should add the directory containing `libcurl.pc'
> to the PKG_CONFIG_PATH environment variable
> Package 'libcurl', required by 'virtual:world', not found
> Using PKG_CFLAGS=
> Using PKG_LIBS=-lcurl
> ------------------------- ANTICONF ERROR ---------------------------
> Configuration failed because libcurl was not found. Try installing:
>  * deb: libcurl4-openssl-dev (Debian, Ubuntu, etc)
>  * rpm: libcurl-devel (Fedora, CentOS, RHEL)
>  * csw: libcurl_dev (Solaris)
> If libcurl is already installed, check that 'pkg-config' is in your
> PATH and PKG_CONFIG_PATH contains a libcurl.pc file. If pkg-config
> is unavailable you can set INCLUDE_DIR and LIB_DIR manually via:
> R CMD INSTALL --configure-vars='INCLUDE_DIR=... LIB_DIR=...'
> --------------------------------------------------------------------
> ERROR: configuration failed for package ?curl?
> * removing ?/home/john/R/x86_64-redhat-linux-gnu-library/3.6/curl?
> Warning in install.packages :
>   installation of package ?curl? had non-zero exit status
> ****
>
> However, libcurl is installed, installed.  What does seem to missing is
> "libcurl.pc".  The ../pkgconfig directory contains a number of
> different *.pc files, but not that one.  I'm asking here first because
> someone else may have already encountered and solved this.  I'm running
> Fedora 32 with kde on a 4 X AMD system.  I just updated Fedora.
>
> JWDougherty
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


