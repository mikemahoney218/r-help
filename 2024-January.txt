From no@p@m @end|ng |rom ||@@e@NA  Wed Jan  3 13:26:16 2024
From: no@p@m @end|ng |rom ||@@e@NA (Dr Eberhard Lisse)
Date: Wed, 3 Jan 2024 14:26:16 +0200
Subject: [R] 
 Help request: Parsing docx files for key words and appending to
 a spreadsheet
In-Reply-To: <99249107-9adb-a202-0739-59b0eff6525e@gmail.com>
References: <e2368e9b-b11b-89a3-34cd-a7cf51fa8288@gmail.com>
 <CAAxdm-5VwQZ+Cpb+mXZXGTkh5eoTFpgxKwf6D+ZMXosxgTjcBQ@mail.gmail.com>
 <CA+etgPnMiZ_ZcG41Z31ERSv1NJOrRa8bvR2=CjN+H5aCYpBmwQ@mail.gmail.com>
 <umn9v5$la8$1@ciao.gmane.io> <26efc752-9e23-c00c-3881-e04b94852945@gmail.com>
 <umnn2r$13pq$1@ciao.gmane.io>
 <99249107-9adb-a202-0739-59b0eff6525e@gmail.com>
Message-ID: <un3jpa$10vu$1@ciao.gmane.io>

If you do something like this

	for i in  $(pandoc --list-output-formats);
		do pandoc -f docx -t $i -o test.$i Now\ they\ want\ us\ to\ charge\
	our\ electric\ cars\ from\ litter\ bins.docx;
	done

you get approximately 65 formats, from which you can pick one which you can
write a little parser for. The dokuwiki one for example uses long lines
which
makes parsing easier.

el


On 2023-12-30 13:57 , Andy wrote:
> Good idea, El - thanks.
>
> The link is
> https://docs.google.com/document/d/1QwuaWZk6tYlWQXJ3WLczxC8Cda6zVERk/edit?usp=sharing&ouid=103065135255080058813&rtpof=true&sd=true
>
>  This is helpful.
>
> From the article, which is typical of Lexis+ output, I want to
> extract the following fields and append to a Calc/ Excel spreadsheet.
> Given the volume of articles I have to work through, if this can be
> iterative and semi-automatic, that would be a god send and I might be
> able to do some actual research on the articles before I reach my
> pensionable age. :-)
>
> Title Newspaper Date Section and page number Length Byline Subject
> (only if the threshold of coverage for a specific subject is
>> =50% is reached (e.g. Greenwashing (51%)) - if not, enter 'nil' and
>>
> move onto the next article in the folder
>
> This is the ambition. I am clearly a long way short of that though.
>
> Many thanks. Andy


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Thu Jan  4 13:21:39 2024
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Thu, 4 Jan 2024 12:21:39 +0000
Subject: [R] Obtaining a value of pie in a zero inflated model (fm-zinb2)
Message-ID: <DM6PR03MB5049A6587CD8AB9A83A0FE02E2672@DM6PR03MB5049.namprd03.prod.outlook.com>

I am running a zero inflated regression using the zeroinfl function similar to the model below:
  
 fm_zinb2 <- zeroinfl(art ~ . | ., data = bioChemists, dist = "poisson")
summary(fm_zinb2)

I have three questions:

1) How can I obtain a value for the parameter pie, which is the fraction of the population that is in the zero inflated model vs the fraction in the count model? 

2) For any particular subject, how can I determine if the subject is in the portion of the population that contributes a zero count because the subject is in the group of subjects who have structural zero responses vs. the subject being in the portion of the population who can contribute a zero or a non-zero response?

3) zero inflated models can be solved using closed form solutions, or using iterative methods. Which method is used by fm_zinb2?

Thank you,
John

John David Sorkin M.D., Ph.D.
Professor of Medicine, University of Maryland School of Medicine;

Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;?

PI?Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;

Senior Statistician University of Maryland Center for Vascular Research;

Division of Gerontology and Paliative Care,
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
Cell phone 443-418-5382




From ph@edru@v @end|ng |rom gm@||@com  Thu Jan  4 13:59:59 2024
From: ph@edru@v @end|ng |rom gm@||@com (Andy)
Date: Thu, 4 Jan 2024 12:59:59 +0000
Subject: [R] 
 Help request: Parsing docx files for key words and appending to
 a spreadsheet
In-Reply-To: <589890e3-3124-9ea5-1bdb-733005900d3f@gmail.com>
References: <e2368e9b-b11b-89a3-34cd-a7cf51fa8288@gmail.com>
 <2921616F-A02E-4C00-89B3-3E0A6FE077B3@noaa.gov>
 <956d7fcc-e643-0bc4-b438-e13a903bbbbf@gmail.com>
 <20231229235900.2a21ecd5@Tarkus>
 <CA+etgPmSrnsyUNcn0uAiA2ssHj7-XVGvG-omvtckaxBOEK_D0Q@mail.gmail.com>
 <6e613a8d-dd70-d9b7-3f25-c230b453b9b4@gmail.com>
 <CAGgJW7542CpC=+YysfGDFDCc-aT4q1o9b_ffRFfAjJ0j90eJBg@mail.gmail.com>
 <1a74c1cd-02f8-890d-3521-7a3143334a9f@gmail.com>
 <589890e3-3124-9ea5-1bdb-733005900d3f@gmail.com>
Message-ID: <b233190f-cc1e-d334-784c-5d403ab6e212@gmail.com>

Hi folks

Thanks for your help and suggestions - very much appreciated.

I now have some working code, using this file I uploaded for public 
access: 
https://docs.google.com/document/d/1QwuaWZk6tYlWQXJ3WLczxC8Cda6zVERk/edit?usp=sharing&ouid=103065135255080058813&rtpof=true&sd=true 


The small code segment that now works is as follows:

###########

# Load libraries
library(textreadr)
library(tcltk)
library(tidyverse)
#library(officer)
#library(stringr) #for splitting and trimming raw data
#library(tidyr) #for converting to wide format

# I'd like to keep this as it enables more control over the selected 
directories
filepath <- setwd(tk_choose.dir())

# The following correctly lists the names of all 9 files in my test 
directory
files <- list.files(filepath, ".docx")
files
length(files)

# Ideally, I'd like to skip this step by being able to automatically 
read in the name of each file, but one step at a time:
filename <- "Now they want us to charge our electric cars from litter 
bins.docx"

# This produces the file content as output when run, and identifies the 
fields that I want to extract.
read_docx(filename) %>%
 ? str_split(",") %>%
 ? unlist() %>%
 ? str_trim()

###########

What I'd like to try and accomplish next is to extract the data from 
selected fields and append to a spreadsheet (Calc or Excel) under 
specific columns, or if it is easier to write a CSV which I can then use 
later.

The fields I want to extract are illustrated with reference to the above 
file, viz.:

The title: "Now they want us to charge our electric cars from litter bins"
The name of the newspaper: "Mail on Sunday (London)"
The publication date: "September 24, 2023" (in date format, preferably 
separated into month and year (day is not important))
The section: "NEWS"
The page number(s): "16" (as numeric)
The length: "515" (as numeric)
The author: "Anna Mikhailova"
The subject: from the Subject section, but this is to match a value e.g. 
GREENWASHING >= 50% (here this value is 51% so would be included). A 
match moves onto select the highest value under the section "Industry" 
(here it is ELECTRIC MOBILITY (91%)) and appends this text and % value. 
If no match with 'Greenwashing', then appends 'Null' and moves onto the 
next file in the directory.

###########

The theory I am working with is if I can figure out how to extract these 
fields and append correctly, then the rest should just be wrapping this 
up in a for loop.

However, I am struggling to get my head around the extraction and append 
part. If I can get it to work for one of these fields, I suspect that I 
can repeat the basic syntax to extract and append the remaining fields.

Therefore, if someone can either suggest a syntax or point me to a 
useful tutorial, that would be splendid.

Thank you in anticipation.

Best wishes
Andy

<snip>


From cry@n @end|ng |rom b|ngh@mton@edu  Thu Jan  4 15:38:06 2024
From: cry@n @end|ng |rom b|ngh@mton@edu (Christopher W. Ryan)
Date: Thu, 4 Jan 2024 09:38:06 -0500
Subject: [R] 
 Obtaining a value of pie in a zero inflated model (fm-zinb2)
In-Reply-To: <DM6PR03MB5049A6587CD8AB9A83A0FE02E2672@DM6PR03MB5049.namprd03.prod.outlook.com>
References: <DM6PR03MB5049A6587CD8AB9A83A0FE02E2672@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <02c6fe89-ccae-6c7c-c61e-f79cffad4358@binghamton.edu>

Are you referring to the zeroinfl() function in the countreg package? If
so, I think

predict(fm_zinb2, type = "zero", newdata = some.new.data)

will give you pi for each combination of covariate values that you
provide in some.new.data

where pi is the probability to observe a zero from the point mass component.

As to your second question, I'm not sure that's possible, for any
*particular, individual* subject. Others will undoubtedly know better
than I.

--Chris Ryan

Sorkin, John wrote:
> I am running a zero inflated regression using the zeroinfl function similar to the model below:
>   
>  fm_zinb2 <- zeroinfl(art ~ . | ., data = bioChemists, dist = "poisson")
> summary(fm_zinb2)
> 
> I have three questions:
> 
> 1) How can I obtain a value for the parameter pie, which is the fraction of the population that is in the zero inflated model vs the fraction in the count model? 
> 
> 2) For any particular subject, how can I determine if the subject is in the portion of the population that contributes a zero count because the subject is in the group of subjects who have structural zero responses vs. the subject being in the portion of the population who can contribute a zero or a non-zero response?
> 
> 3) zero inflated models can be solved using closed form solutions, or using iterative methods. Which method is used by fm_zinb2?
> 
> Thank you,
> John
> 
> John David Sorkin M.D., Ph.D.
> Professor of Medicine, University of Maryland School of Medicine;
> 
> Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;?
> 
> PI?Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
> 
> Senior Statistician University of Maryland Center for Vascular Research;
> 
> Division of Gerontology and Paliative Care,
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> Cell phone 443-418-5382
> 
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Sat Jan  6 04:17:58 2024
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Sat, 6 Jan 2024 03:17:58 +0000
Subject: [R] Amelia. Imputation of time-series data
Message-ID: <DM6PR03MB50495751D572168AACAECC72E2652@DM6PR03MB5049.namprd03.prod.outlook.com>

Colleagues,

I have started working with Amelia, with the aim of imputing missing data for time-series data. 

Although I have succeeded in getting Amelia to perform the imputation, I have not found any documentation describing how Amelia imputes time-series data. I have read the basic Amelia documentation, but it does not address how time-series data are imputed. The documentation describes general imputation where there is no serial auto correlation of repeated observations from the same subject. Does Amelia incorporate the serial autocorrelation in the imputation procedure? Can someone direct me to documentation that explains the imputation method? 

Thank you,
John


John David Sorkin M.D., Ph.D.
Professor of Medicine, University of Maryland School of Medicine;

Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;?

PI?Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;

Senior Statistician University of Maryland Center for Vascular Research;

Division of Gerontology and Paliative Care,
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
Cell phone 443-418-5382




From ph@edru@v @end|ng |rom gm@||@com  Sat Jan  6 10:47:03 2024
From: ph@edru@v @end|ng |rom gm@||@com (Andy)
Date: Sat, 6 Jan 2024 09:47:03 +0000
Subject: [R] 
 Help request: Parsing docx files for key words and appending to
 a spreadsheet
In-Reply-To: <SA1PR09MB7792A37BA7D15B486708C69FA8662@SA1PR09MB7792.namprd09.prod.outlook.com>
References: <mailman.370760.1.1704452402.37863.r-help@r-project.org>
 <SA1PR09MB7792A37BA7D15B486708C69FA8662@SA1PR09MB7792.namprd09.prod.outlook.com>
Message-ID: <672e10cd-12ee-d53e-1aad-6efdec8b3e0c@gmail.com>

Hi Tim

This is brilliant - thank you!!

I've had to tweak the basePath line a bit (I am on a Linux machine), but 
having done that, the code works as intended. This is a truly helpful 
contribution that gives me ideas about how to work it through for the 
missing fields, which is one of the major sticking points I kept bumping 
up against.

Thank you so much for this.

All the best
Andy

On 05/01/2024 13:59, Howard, Tim G (DEC) wrote:
> Here's a simplified version of how I would do it, using `textreadr` but otherwise base functions. I haven't done it
> all, but have a few examples of finding the correct row then extracting the right data.
> I made a duplicate of the file you provided, so this loops through the two identical files, extracts a few parts,
> then sticks those parts in a data frame.
>
> #####
> library(textreadr)
>
> # recommend not using setwd(), but instead just include the
> # path as follows
> basePath <- file.path("C:","temp")
> files <- list.files(path=basePath, pattern = "docx$")
>
> length(files)
> # 2
>
> # initialize a list to put the data in
> myList <- vector(mode = "list", length = length(files))
>
> for(i in 1:length(files)){
>    fileDat <- read_docx(file.path(basePath, files[[i]]))
>    # get the data you want, here one line per item to make it clearer
>    # assume consistency among articles
>    ttl <- fileDat[[1]]
>    src <- fileDat[[2]]
>    dt <- fileDat[[3]]
>    aut <- fileDat[grepl("Byline:",fileDat)]
>    aut <- trimws(sub("Byline:","",aut), whitespace = "[\\h\\v]")
>    pg <- fileDat[grepl("Pg.",fileDat)]
>    pg <- as.integer(sub(".*Pg. ([[:digit:]]+)","\\1",pg))
>    len <- fileDat[grepl("Length:", fileDat)]
>    len <- as.integer(sub("Length:.{1}([[:digit:]]+) .*","\\1",len))
>    myList[[i]] <- data.frame("title"=ttl,
>                     "source"=src,
>                     "date"=dt,
>                     "author"=aut,
>                     "page"=pg,
>                     "length"=len)
> }
>
> # roll up the list to a data frame. Many ways to do this.
> myDF <- do.call("rbind",myList)
>
> #####
>
> Hope that helps.
> Tim
>
>
>
>> ------------------------------
>>
>> Date: Thu, 4 Jan 2024 12:59:59 +0000
>> From: Andy <phaedrusv at gmail.com>
>> To: r-help at r-project.org
>> Subject: Re: [R]  Help request: Parsing docx files for key words and
>>          appending to a spreadsheet
>> Message-ID: <b233190f-cc1e-d334-784c-5d403ab6e212 at gmail.com>
>> Content-Type: text/plain; charset="utf-8"; Format="flowed"
>>
>> Hi folks
>>
>> Thanks for your help and suggestions - very much appreciated.
>>
>> I now have some working code, using this file I uploaded for public
>> access:
>> https://docs/.
>> google.com%2Fdocument%2Fd%2F1QwuaWZk6tYlWQXJ3WLczxC8Cda6zVER
>> k%2Fedit%3Fusp%3Dsharing%26ouid%3D103065135255080058813%26rtpof%
>> 3Dtrue%26sd%3Dtrue&data=05%7C02%7Ctim.howard%40dec.ny.gov%7C8f2
>> 952a3ae474d4da14908dc0ddd95fd%7Cf46cb8ea79004d108ceb80e8c1c81ee7
>> %7C0%7C0%7C638400492578674983%7CUnknown%7CTWFpbGZsb3d8eyJWIj
>> oiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3
>> 000%7C%7C%7C&sdata=%2BpYrk6cJA%2BDUn9szLbd2Y7R%2F30UNY2TFSJN
>> HcwkHa9Y%3D&reserved=0
>>
>>
>> The small code segment that now works is as follows:
>>
>> ###########
>>
>> # Load libraries
>> library(textreadr)
>> library(tcltk)
>> library(tidyverse)
>> #library(officer)
>> #library(stringr) #for splitting and trimming raw data
>> #library(tidyr) #for converting to wide format
>>
>> # I'd like to keep this as it enables more control over the selected directories
>> filepath <- setwd(tk_choose.dir())
>>
>> # The following correctly lists the names of all 9 files in my test directory files
>> <- list.files(filepath, ".docx") files
>> length(files)
>>
>> # Ideally, I'd like to skip this step by being able to automatically read in the
>> name of each file, but one step at a time:
>> filename <- "Now they want us to charge our electric cars from litter
>> bins.docx"
>>
>> # This produces the file content as output when run, and identifies the fields
>> that I want to extract.
>> read_docx(filename) %>%
>>     str_split(",") %>%
>>     unlist() %>%
>>     str_trim()
>>
>> ###########
>>
>> What I'd like to try and accomplish next is to extract the data from selected
>> fields and append to a spreadsheet (Calc or Excel) under specific columns, or
>> if it is easier to write a CSV which I can then use later.
>>
>> The fields I want to extract are illustrated with reference to the above file,
>> viz.:
>>
>> The title: "Now they want us to charge our electric cars from litter bins"
>> The name of the newspaper: "Mail on Sunday (London)"
>> The publication date: "September 24, 2023" (in date format, preferably
>> separated into month and year (day is not important)) The section: "NEWS"
>> The page number(s): "16" (as numeric)
>> The length: "515" (as numeric)
>> The author: "Anna Mikhailova"
>> The subject: from the Subject section, but this is to match a value e.g.
>> GREENWASHING >= 50% (here this value is 51% so would be included). A
>> match moves onto select the highest value under the section "Industry"
>> (here it is ELECTRIC MOBILITY (91%)) and appends this text and % value.
>> If no match with 'Greenwashing', then appends 'Null' and moves onto the
>> next file in the directory.
>>
>> ###########
>>
>> The theory I am working with is if I can figure out how to extract these fields
>> and append correctly, then the rest should just be wrapping this up in a for
>> loop.
>>
>> However, I am struggling to get my head around the extraction and append
>> part. If I can get it to work for one of these fields, I suspect that I can repeat
>> the basic syntax to extract and append the remaining fields.
>>
>> Therefore, if someone can either suggest a syntax or point me to a useful
>> tutorial, that would be splendid.
>>
>> Thank you in anticipation.
>>
>> Best wishes
>> Andy
>>
>> <snip>
>>
>>
>>
>>
>> ------------------------------
>>
>> Message: 3
>> Date: Thu, 4 Jan 2024 09:38:06 -0500
>> From: "Christopher W. Ryan" <cryan at binghamton.edu>
>> To: "Sorkin, John" <jsorkin at som.umaryland.edu>, "r-help at r-project.org
>>          (r-help at r-project.org)" <r-help at r-project.org>
>> Subject: Re: [R]  Obtaining a value of pie in a zero inflated model
>>          (fm-zinb2)
>> Message-ID: <02c6fe89-ccae-6c7c-c61e-f79cffad4358 at binghamton.edu>
>> Content-Type: text/plain; charset="utf-8"
>>
>> Are you referring to the zeroinfl() function in the countreg package? If so, I
>> think
>>
>> predict(fm_zinb2, type = "zero", newdata = some.new.data)
>>
>> will give you pi for each combination of covariate values that you provide in
>> some.new.data
>>
>> where pi is the probability to observe a zero from the point mass
>> component.
>>
>> As to your second question, I'm not sure that's possible, for any *particular,
>> individual* subject. Others will undoubtedly know better than I.
>>
>> --Chris Ryan
>>
>> Sorkin, John wrote:
>>> I am running a zero inflated regression using the zeroinfl function similar to
>> the model below:
>>>   fm_zinb2 <- zeroinfl(art ~ . | ., data = bioChemists, dist =
>>> "poisson")
>>> summary(fm_zinb2)
>>>
>>> I have three questions:
>>>
>>> 1) How can I obtain a value for the parameter pie, which is the fraction of
>> the population that is in the zero inflated model vs the fraction in the count
>> model?
>>> 2) For any particular subject, how can I determine if the subject is in the
>> portion of the population that contributes a zero count because the subject
>> is in the group of subjects who have structural zero responses vs. the subject
>> being in the portion of the population who can contribute a zero or a non-
>> zero response?
>>> 3) zero inflated models can be solved using closed form solutions, or using
>> iterative methods. Which method is used by fm_zinb2?
>>> Thank you,
>>> John
>>>
>>> John David Sorkin M.D., Ph.D.
>>> Professor of Medicine, University of Maryland School of Medicine;
>>>
>>> Associate Director for Biostatistics and Informatics, Baltimore VA
>>> Medical Center Geriatrics Research, Education, and Clinical Center;
>>>
>>> PI Biostatistics and Informatics Core, University of Maryland School
>>> of Medicine Claude D. Pepper Older Americans Independence Center;
>>>
>>> Senior Statistician University of Maryland Center for Vascular
>>> Research;
>>>
>>> Division of Gerontology and Paliative Care,
>>> 10 North Greene Street
>>> GRECC (BT/18/GR)
>>> Baltimore, MD 21201-1524
>>> Cell phone 443-418-5382
>>>
>>>
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat/
>>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-
>> help&data=05%7C02%7Ctim.howard%40dec
>> .ny.gov%7C8f2952a3ae474d4da14908dc0ddd95fd%7Cf46cb8ea79004d108ceb
>> 80e8c
>> 1c81ee7%7C0%7C0%7C638400492578674983%7CUnknown%7CTWFpbGZsb3d
>> 8eyJWIjoiM
>> C4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000
>> %7C%7C
>> %7C&sdata=Z17L8H5Lv6Q6e9FHxDJauhNSwsL53Qsvh5YQiH8ztmY%3D&reser
>> ved=0
>>> PLEASE do read the posting guide
>>>
>> http://www.r/
>>> -project.org%2Fposting-
>> guide.html&data=05%7C02%7Ctim.howard%40dec.ny.g
>> ov%7C8f2952a3ae474d4da14908dc0ddd95fd%7Cf46cb8ea79004d108ceb80e8c
>> 1c81e
>> e7%7C0%7C0%7C638400492578674983%7CUnknown%7CTWFpbGZsb3d8eyJ
>> WIjoiMC4wLj
>> AwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%
>> 7C%7C&s
>> data=4PSWzIOvJoU%2FvrXXwwquhha8yyEUzC8z7PgdIpXrlGs%3D&reserved
>> =0
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>>
>> ------------------------------
>>
>> Subject: Digest Footer
>>
>> _______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.e/
>> thz.ch%2Fmailman%2Flistinfo%2Fr-
>> help&data=05%7C02%7Ctim.howard%40dec.ny.gov%7C8f2952a3ae474d4da1
>> 4908dc0ddd95fd%7Cf46cb8ea79004d108ceb80e8c1c81ee7%7C0%7C0%7C638
>> 400492578674983%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAi
>> LCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&s
>> data=Z17L8H5Lv6Q6e9FHxDJauhNSwsL53Qsvh5YQiH8ztmY%3D&reserved=0
>> PLEASE do read the posting guide
>> http://www.r/
>> -project.org%2Fposting-
>> guide.html&data=05%7C02%7Ctim.howard%40dec.ny.gov%7C8f2952a3ae474
>> d4da14908dc0ddd95fd%7Cf46cb8ea79004d108ceb80e8c1c81ee7%7C0%7C0%
>> 7C638400492578674983%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAw
>> MDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%
>> 7C&sdata=4PSWzIOvJoU%2FvrXXwwquhha8yyEUzC8z7PgdIpXrlGs%3D&rese
>> rved=0
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>> ------------------------------
>>
>> End of R-help Digest, Vol 251, Issue 2
>> **************************************


From bbo|ker @end|ng |rom gm@||@com  Mon Jan  8 03:04:15 2024
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sun, 7 Jan 2024 21:04:15 -0500
Subject: [R] 
 Obtaining a value of pie in a zero inflated model (fm-zinb2)
In-Reply-To: <02c6fe89-ccae-6c7c-c61e-f79cffad4358@binghamton.edu>
References: <DM6PR03MB5049A6587CD8AB9A83A0FE02E2672@DM6PR03MB5049.namprd03.prod.outlook.com>
 <02c6fe89-ccae-6c7c-c61e-f79cffad4358@binghamton.edu>
Message-ID: <e50e05bb-b340-416f-b6bf-ad2cd2d51099@gmail.com>

    A little more.

   As Christopher Ryan points out, as long as the zero-inflation model 
is non-trivial (i.e. more complex than a single intercept for the whole 
population), there's a pi(i) for every observation i (you could 
certainly average the pi(i) values if you wanted, or compute pi for an 
averaged set of covariates).

library(pscl)
fm_zinb2 <- zeroinfl(art ~ . | ., data = bioChemists, dist = "poisson")

pi_i <- predict(fm_zinb2, type = "zero")
head(pi_i)
##          1          2          3          4          5          6
## 0.13392803 0.21936320 0.21973454 0.24697313 0.01890023 0.34281196


To get the conditional probability that a zero observation is a 
structural or a sampling zero, compute the probability of a sampling 
zero (exp(-mu[i]) for a Poisson model) and use Bayes' rule, assuming 
equal prior probs

## probability of _sampling_ zero

psampz <- exp(-predict(fm_zinb2, type = "count"))
head(psampz)
##          1          2          3          4          5          6
## 0.09507376 0.18361241 0.18688646 0.14774602 0.08992667 0.27235498

pz <- 1 - (1-pi_i)*(1-psampz)  ## prob of zero of either type
##         1         2         3         4         5         6
## 0.2162687 0.3626978 0.3655556 0.3582299 0.1071273 0.5218004

psampz/pz
head(psampz)/head(pz)
##         1         2         3         4         5         6
## 0.4396093 0.5062408 0.5112395 0.4124336 0.8394378 0.5219524

    3. I'm not aware of a closed-form solution for zero-inflation models 
as long as the count and/or structural-zero components depend on 
covariates?  In any case, looking inside zeroinfl() you can see that it 
calls optim() [using BFGS by default, see ?pscl::zeroinfl.control]

   cheers
    Ben Bolker


On 2024-01-04 9:38 a.m., Christopher W. Ryan via R-help wrote:
> Are you referring to the zeroinfl() function in the countreg package? If
> so, I think
> 
> predict(fm_zinb2, type = "zero", newdata = some.new.data)
> 
> will give you pi for each combination of covariate values that you
> provide in some.new.data
> 
> where pi is the probability to observe a zero from the point mass component.
> 
> As to your second question, I'm not sure that's possible, for any
> *particular, individual* subject. Others will undoubtedly know better
> than I.
> 
> --Chris Ryan
> 
> Sorkin, John wrote:
>> I am running a zero inflated regression using the zeroinfl function similar to the model below:
>>    
>>   fm_zinb2 <- zeroinfl(art ~ . | ., data = bioChemists, dist = "poisson")
>> summary(fm_zinb2)
>>
>> I have three questions:
>>
>> 1) How can I obtain a value for the parameter pie, which is the fraction of the population that is in the zero inflated model vs the fraction in the count model?
>>
>> 2) For any particular subject, how can I determine if the subject is in the portion of the population that contributes a zero count because the subject is in the group of subjects who have structural zero responses vs. the subject being in the portion of the population who can contribute a zero or a non-zero response?
>>
>> 3) zero inflated models can be solved using closed form solutions, or using iterative methods. Which method is used by fm_zinb2?
>>
>> Thank you,
>> John
>>
>> John David Sorkin M.D., Ph.D.
>> Professor of Medicine, University of Maryland School of Medicine;
>>
>> Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
>>
>> PI?Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
>>
>> Senior Statistician University of Maryland Center for Vascular Research;
>>
>> Division of Gerontology and Paliative Care,
>> 10 North Greene Street
>> GRECC (BT/18/GR)
>> Baltimore, MD 21201-1524
>> Cell phone 443-418-5382
>>
>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From w@ngzh597 @end|ng |rom m@||2@@y@u@edu@cn  Mon Jan  8 12:43:04 2024
From: w@ngzh597 @end|ng |rom m@||2@@y@u@edu@cn (=?utf-8?B?546L6ZyH?=)
Date: Mon, 8 Jan 2024 19:43:04 +0800
Subject: [R] how to specify uncorrelated random effects in nlme::lme()
Message-ID: <tencent_539E16DC4644F14A681BCCBC@qq.com>

Dear professor,


I'm using package nlme, but I can't find a way to specify two uncorrelated random effects. For example, a random intercept and a random slope. In package lme4, we can specify&nbsp;x + (x ll g) to realize, but how in nlme?


Thanks!








 ????????????????????????
 Zhen Wang
 Graduate student, Department of Medical Statistics, School of Public Health, Sun Yat-sen University
E-mail:&nbsp;wangzh597 at mail2.sysu.edu.cn
??
????????????????2022??????
	[[alternative HTML version deleted]]


From wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n|  Tue Jan  9 13:22:47 2024
From: wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n| (Viechtbauer, Wolfgang (NP))
Date: Tue, 9 Jan 2024 12:22:47 +0000
Subject: [R] how to specify uncorrelated random effects in nlme::lme()
In-Reply-To: <tencent_539E16DC4644F14A681BCCBC@qq.com>
References: <tencent_539E16DC4644F14A681BCCBC@qq.com>
Message-ID: <AS8PR08MB919376A0A8C23538B05D20CF8B6A2@AS8PR08MB9193.eurprd08.prod.outlook.com>

Dear Zhen,

You can use this with pdDiag(). An example:

library(nlme)
res <- lme(distance ~ age*Sex, random = list(Subject = pdDiag(~ age)), data=Orthodont)
summary(res)

Best,
Wolfgang

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of ??
> Sent: Monday, January 8, 2024 12:43
> To: R-help <R-help at r-project.org>
> Subject: [R] how to specify uncorrelated random effects in nlme::lme()
>
> Dear professor,
>
> I'm using package nlme, but I can't find a way to specify two uncorrelated
> random effects. For example, a random intercept and a random slope. In package
> lme4, we can specify&nbsp;x + (x ll g) to realize, but how in nlme?
>
> Thanks!
>
> ????????????????????????
>  Zhen Wang
>  Graduate student, Department of Medical Statistics, School of Public Health,
> Sun Yat-sen University
> E-mail:&nbsp;wangzh597 at mail2.sysu.edu.cn
> ??
> ????????????????2022??????

From n|ckmwr@y @end|ng |rom gm@||@com  Tue Jan  9 17:42:32 2024
From: n|ckmwr@y @end|ng |rom gm@||@com (Nick Wray)
Date: Tue, 9 Jan 2024 16:42:32 +0000
Subject: [R] Truncated plots
Message-ID: <CABxY9BMs71F7DNc5G2ffjBdtLrTQjmzN47LfnNmwjz8hWOXvaQ@mail.gmail.com>

Hello As a postgrad I have been helping an undergraduate student with R
coding but she has a problem with R studio on her laptop which I can't fix
- basically when she runs a plot it appears without a y axis label with the
black line plot frame hard against the plot window and the bottom of the
plot, where you would expect to see the horizontal axis and the x axis
label etc is completely "chopped off" by the bottom edge of the R studio
interface window.  I can't find anything on the net detailing this problem
- can anyone help?  I have a screenshot which could email if anyone needs
to see what it looks like.

Thanks Nick Wray

	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Tue Jan  9 18:13:43 2024
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Tue, 9 Jan 2024 12:13:43 -0500
Subject: [R] Truncated plots
In-Reply-To: <CABxY9BMs71F7DNc5G2ffjBdtLrTQjmzN47LfnNmwjz8hWOXvaQ@mail.gmail.com>
References: <CABxY9BMs71F7DNc5G2ffjBdtLrTQjmzN47LfnNmwjz8hWOXvaQ@mail.gmail.com>
Message-ID: <b7c5ede0-bd44-4293-8a72-bf5c9b18610a@gmail.com>

On 09/01/2024 11:42 a.m., Nick Wray wrote:
> Hello As a postgrad I have been helping an undergraduate student with R
> coding but she has a problem with R studio on her laptop which I can't fix
> - basically when she runs a plot it appears without a y axis label with the
> black line plot frame hard against the plot window and the bottom of the
> plot, where you would expect to see the horizontal axis and the x axis
> label etc is completely "chopped off" by the bottom edge of the R studio
> interface window.  I can't find anything on the net detailing this problem
> - can anyone help?  I have a screenshot which could email if anyone needs
> to see what it looks like.
> 

Does this happen only in RStudio?  She can run `pdf("somefile.pdf")` 
before plotting and `dev.off()` after to save the results to a PDF file 
instead.  If the problem only occurs in RStudio, then you should 
probably contact Posit or one of their community support groups about it.

If it happens in other devices too:  is she using base graphics, or 
ggplot2, or something else?  If she's using base graphics, take a look 
at `par("mar")`.  That gives the size of the margins measured in lines 
of text; they are in the order bottom, left, top, right, with default 
value `c(5.1, 4.1, 4.1, 2.1)`.  Perhaps she has set the 2nd entry to zero.

I'm sure there's something similar in ggplot2 and other graphics 
systems, but I don't know what it would be.

Duncan Murdoch


From tebert @end|ng |rom u||@edu  Tue Jan  9 18:35:27 2024
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Tue, 9 Jan 2024 17:35:27 +0000
Subject: [R] Truncated plots
In-Reply-To: <CABxY9BMs71F7DNc5G2ffjBdtLrTQjmzN47LfnNmwjz8hWOXvaQ@mail.gmail.com>
References: <CABxY9BMs71F7DNc5G2ffjBdtLrTQjmzN47LfnNmwjz8hWOXvaQ@mail.gmail.com>
Message-ID: <CH3PR22MB4514FCEA4A493F4323EFA703CF6A2@CH3PR22MB4514.namprd22.prod.outlook.com>

It would help to have reproducible code. Use dummy data for confidentiality (if you care about that).
My guess is that you set margins somewhere and never returned them to a default value. The first thing I would try is to open a new window in RStudio, copy the smallest piece of code that will give you the graph you want. Clear out the Environment using the broom icon, and then run your code. This makes sure that your results are only from your current code and not getting mixed up with stuff you ran last week/month. This might solve the problem, or it can be what you post as an example to get further help.

There is an RStudio help forum that might also help.

Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Nick Wray
Sent: Tuesday, January 9, 2024 11:43 AM
To: r-help at r-project.org
Subject: [R] Truncated plots

[External Email]

Hello As a postgrad I have been helping an undergraduate student with R coding but she has a problem with R studio on her laptop which I can't fix
- basically when she runs a plot it appears without a y axis label with the black line plot frame hard against the plot window and the bottom of the plot, where you would expect to see the horizontal axis and the x axis label etc is completely "chopped off" by the bottom edge of the R studio interface window.  I can't find anything on the net detailing this problem
- can anyone help?  I have a screenshot which could email if anyone needs to see what it looks like.

Thanks Nick Wray

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.r-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From |kry|ov @end|ng |rom d|@root@org  Tue Jan  9 17:58:57 2024
From: |kry|ov @end|ng |rom d|@root@org (Ivan Krylov)
Date: Tue, 9 Jan 2024 19:58:57 +0300
Subject: [R] Truncated plots
In-Reply-To: <CABxY9BMs71F7DNc5G2ffjBdtLrTQjmzN47LfnNmwjz8hWOXvaQ@mail.gmail.com>
References: <CABxY9BMs71F7DNc5G2ffjBdtLrTQjmzN47LfnNmwjz8hWOXvaQ@mail.gmail.com>
Message-ID: <20240109195857.01b08a3e@Tarkus>

? Tue, 9 Jan 2024 16:42:32 +0000
Nick Wray <nickmwray at gmail.com> ?????:

> she has a problem with R studio on her laptop

Does the problem happen with plain R, without Rstudio?

What's the student's sessionInfo()?

> I have a screenshot which could email if anyone needs to see what it
> looks like.

I think that PNG screenshots are allowed on the mailing list, so it
could be very helpful if you attached an appropriately cropped
screenshot.

-- 
Best regards,
Ivan


From jrkr|de@u @end|ng |rom gm@||@com  Tue Jan  9 20:33:10 2024
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Tue, 9 Jan 2024 14:33:10 -0500
Subject: [R] Truncated plots
In-Reply-To: <20240109195857.01b08a3e@Tarkus>
References: <CABxY9BMs71F7DNc5G2ffjBdtLrTQjmzN47LfnNmwjz8hWOXvaQ@mail.gmail.com>
 <20240109195857.01b08a3e@Tarkus>
Message-ID: <CAKZQJMAhz0DQtajJARmJX7d10ERtAQE0gA2PdP0y1DORC5Ytyg@mail.gmail.com>

If it looks to be a very specific RStudio/ggplot2 problem then
https://community.rstudio.com is probably the place to ask.

What happens if she does as Duncan suggests or if she exports the file?

Come to think of it, is she getting the same result if she clicks on Zoom
in the plot window? The standard plot window in RStudio can distort the
actual image due to size restrictions.

On Tue, 9 Jan 2024 at 12:47, Ivan Krylov via R-help <r-help at r-project.org>
wrote:

> ? Tue, 9 Jan 2024 16:42:32 +0000
> Nick Wray <nickmwray at gmail.com> ?????:
>
> > she has a problem with R studio on her laptop
>
> Does the problem happen with plain R, without Rstudio?
>
> What's the student's sessionInfo()?
>
> > I have a screenshot which could email if anyone needs to see what it
> > looks like.
>
> I think that PNG screenshots are allowed on the mailing list, so it
> could be very helpful if you attached an appropriately cropped
> screenshot.
>
> --
> Best regards,
> Ivan
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
John Kane
Kingston ON Canada

	[[alternative HTML version deleted]]


From @vi@e@gross m@iii@g oii gm@ii@com  Tue Jan  9 21:09:21 2024
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Tue, 9 Jan 2024 15:09:21 -0500
Subject: [R] Truncated plots
In-Reply-To: <CABxY9BMs71F7DNc5G2ffjBdtLrTQjmzN47LfnNmwjz8hWOXvaQ@mail.gmail.com>
References: <CABxY9BMs71F7DNc5G2ffjBdtLrTQjmzN47LfnNmwjz8hWOXvaQ@mail.gmail.com>
Message-ID: <005901da4337$bc541420$34fc3c60$@gmail.com>


Nick, obviously figuring out the problem is best but you may want to deal
with the symptom.

RSTUDIO lets you adjust the sizes of the various windows and enlarging the
window (lower right normally) where the graph is shown may be a first
attempt if the problem is display space.

And note RSTUDIO in that window lets you zoom a pop-out window with the
graph. It also lets you save the graph into files with various formats.

And, as someone else pointed out, you can change the program to choose a
device to save the output in.

Of course, if the real problem is in some aspect of the setup that causes
the clipping, that has to be looked at.

If the problem is in RSTUDIO, not just R, this is not the right forum to
deal with it.

Avi

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Nick Wray
Sent: Tuesday, January 9, 2024 11:43 AM
To: r-help at r-project.org
Subject: [R] Truncated plots

Hello As a postgrad I have been helping an undergraduate student with R
coding but she has a problem with R studio on her laptop which I can't fix
- basically when she runs a plot it appears without a y axis label with the
black line plot frame hard against the plot window and the bottom of the
plot, where you would expect to see the horizontal axis and the x axis
label etc is completely "chopped off" by the bottom edge of the R studio
interface window.  I can't find anything on the net detailing this problem
- can anyone help?  I have a screenshot which could email if anyone needs
to see what it looks like.

Thanks Nick Wray

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From b@@u@15 @end|ng |rom gm@||@com  Thu Jan 11 01:13:19 2024
From: b@@u@15 @end|ng |rom gm@||@com (Deepankar Basu)
Date: Wed, 10 Jan 2024 19:13:19 -0500
Subject: [R] arrow on contour line
Message-ID: <CA+Rxh84ii9JGitkPEp+2=rr-WTiKpg7kq4YSM=LELRpq08Tg7w@mail.gmail.com>

Hello,

I am drawing contour lines for a function of 2 variables at one level of
the value of the function and want to include a small arrow in any
direction of increase of the function. Is there some way to do that?

Below is an example that creates the contour lines. How do I add one small
arrow on each line in the direction of increase of the function (at some
central point of the contour line)? Any direction will do, but perhaps the
direction of the gradient will be the best.

Thanks in advance.
DB

--------------------------------------------

library(tidyverse)

x <- seq(1,2,length.out=100)
y <- seq(1,2,length.out=100)

myf <- function(x,y) {x*y}
myg <- function(x,y) {x^2 + y^2}

d1 <- expand.grid(X1 = x, X2 = y) %>%
  mutate(Z = myf(X1,X2)) %>%
  as.data.frame()

d2 <- expand.grid(X1 = x, X2 = y) %>%
  mutate(Z = myg(X1,X2)) %>%
  as.data.frame()

ggplot(data = d1, aes(x=X1,y=X2,z=Z))+
  stat_contour(breaks = c(2)) +
  stat_contour(data=d2, aes(x=X1,y=X2,z=Z), breaks=c(6))

	[[alternative HTML version deleted]]


From ro||turner @end|ng |rom po@teo@net  Thu Jan 11 09:18:24 2024
From: ro||turner @end|ng |rom po@teo@net (Rolf Turner)
Date: Thu, 11 Jan 2024 08:18:24 +0000
Subject: [R] arrow on contour line
In-Reply-To: <CA+Rxh84ii9JGitkPEp+2=rr-WTiKpg7kq4YSM=LELRpq08Tg7w@mail.gmail.com>
References: <CA+Rxh84ii9JGitkPEp+2=rr-WTiKpg7kq4YSM=LELRpq08Tg7w@mail.gmail.com>
Message-ID: <20240111211824.6781793d@rolf-Latitude-E7470>


On Wed, 10 Jan 2024 19:13:19 -0500
Deepankar Basu <basu.15 at gmail.com> wrote:

> Hello,
> 
> I am drawing contour lines for a function of 2 variables at one level
> of the value of the function and want to include a small arrow in any
> direction of increase of the function. Is there some way to do that?
> 
> Below is an example that creates the contour lines. How do I add one
> small arrow on each line in the direction of increase of the function
> (at some central point of the contour line)? Any direction will do,
> but perhaps the direction of the gradient will be the best.
> 
> Thanks in advance.
> DB
> 
> --------------------------------------------
> 
> library(tidyverse)
> 
> x <- seq(1,2,length.out=100)
> y <- seq(1,2,length.out=100)
> 
> myf <- function(x,y) {x*y}
> myg <- function(x,y) {x^2 + y^2}
> 
> d1 <- expand.grid(X1 = x, X2 = y) %>%
>   mutate(Z = myf(X1,X2)) %>%
>   as.data.frame()
> 
> d2 <- expand.grid(X1 = x, X2 = y) %>%
>   mutate(Z = myg(X1,X2)) %>%
>   as.data.frame()
> 
> ggplot(data = d1, aes(x=X1,y=X2,z=Z))+
>   stat_contour(breaks = c(2)) +
>   stat_contour(data=d2, aes(x=X1,y=X2,z=Z), breaks=c(6))

My solution would be to use base graphics and avoid the obfuscation
induced by ggplot().  The graphics produced by ggplot() are devilish
pretty, but ggplot() is devilish difficult to work with.  IMHO you need
an IQ of at least 200 (which leaves me about 150 points short!) to be
able to cope with its intricate and unintuitive syntax.

I believe that what you want to do is triv in base graphics, but I must
confess that I have not gone through the details.

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Stats. Dep't. (secretaries) phone:
         +64-9-373-7599 ext. 89622
Home phone: +64-9-480-4619


From |kry|ov @end|ng |rom d|@root@org  Thu Jan 11 09:56:03 2024
From: |kry|ov @end|ng |rom d|@root@org (Ivan Krylov)
Date: Thu, 11 Jan 2024 11:56:03 +0300
Subject: [R] arrow on contour line
In-Reply-To: <CA+Rxh84ii9JGitkPEp+2=rr-WTiKpg7kq4YSM=LELRpq08Tg7w@mail.gmail.com>
References: <CA+Rxh84ii9JGitkPEp+2=rr-WTiKpg7kq4YSM=LELRpq08Tg7w@mail.gmail.com>
Message-ID: <20240111115603.35657a7f@Tarkus>

? Wed, 10 Jan 2024 19:13:19 -0500
Deepankar Basu <basu.15 at gmail.com> ?????:

> I am drawing contour lines for a function of 2 variables at one level
> of the value of the function and want to include a small arrow in any
> direction of increase of the function. Is there some way to do that?

Can you use the information about the function (i.e. the myg object),
or are you restricted to the d2 data.frame?

Both ggplot and the core graphics use the algorithm from the
contourLines() function in order to draw the contour lines. (Lattice
does the same, but ports the function from C to R.) This function
returns a list of levels combined with vectors of `x` and `y`
coordinates for drawing the contour.

If you can use the myg function, you can differentiate it numerically
or symbolically, process the contourLines() output, evaluate the
gradient at one point per contour and draw additional arrows there.

If you are limited to the tabulated function values from the d2
data.frame, you have a more complicated problem on your hands. You can
either locate grid points closest to the contour segments at both sides
of the segment, determine the direction of growth, and draw an arrow
perpendicular to the contour segment (not equal to the gradient), or
find a way to estimate the gradient at arbitrary points between the
grid (e.g. by using a Taylor series stencil) and then go to the
previous paragraph.

-- 
Best regards,
Ivan


From |enn@rt@k@@@err@ @end|ng |rom gm@||@com  Thu Jan 11 09:54:04 2024
From: |enn@rt@k@@@err@ @end|ng |rom gm@||@com (Lennart Kasserra)
Date: Thu, 11 Jan 2024 09:54:04 +0100
Subject: [R] arrow on contour line
In-Reply-To: <CA+Rxh84ii9JGitkPEp+2=rr-WTiKpg7kq4YSM=LELRpq08Tg7w@mail.gmail.com>
References: <CA+Rxh84ii9JGitkPEp+2=rr-WTiKpg7kq4YSM=LELRpq08Tg7w@mail.gmail.com>
Message-ID: <a979392e-2a10-4120-bda7-5649ff135b4a@gmail.com>

Hi,

If I understand your intentions correctly, the simplest way would be to 
manually insert arrows using `geom_segment()` with the `arrow` argument:

```

ggplot(data = d1, aes(x=X1,y=X2,z=Z))+
 ? stat_contour(breaks = c(2)) +
 ? stat_contour(data=d2, aes(x=X1,y=X2,z=Z), breaks=c(6)) +
 ? geom_segment(
 ??? aes(x = 1.25, y = 1.50, xend = 1.4, yend = 1.325),
 ??? arrow = arrow(length = unit(0.5, "cm"))
 ? ) +
 ? geom_segment(
 ??? aes(x = 1.6, y = 1.95, xend = 1.8, yend = 1.75),
 ??? arrow = arrow(length = unit(0.5, "cm"))
 ? )

```

If your actual data is organized differently, you may also be able to do 
this in a more elegant programmatic way. Here is a StackOverflow 
conversation about this exact topic, one Google search away: 
https://stackoverflow.com/questions/38008863/how-to-draw-a-nice-arrow-in-ggplot2

Have a nice day,

Lennart Kasserra

Am 11.01.24 um 01:13 schrieb Deepankar Basu:
> Hello,
>
> I am drawing contour lines for a function of 2 variables at one level of
> the value of the function and want to include a small arrow in any
> direction of increase of the function. Is there some way to do that?
>
> Below is an example that creates the contour lines. How do I add one small
> arrow on each line in the direction of increase of the function (at some
> central point of the contour line)? Any direction will do, but perhaps the
> direction of the gradient will be the best.
>
> Thanks in advance.
> DB
>
> --------------------------------------------
>
> library(tidyverse)
>
> x <- seq(1,2,length.out=100)
> y <- seq(1,2,length.out=100)
>
> myf <- function(x,y) {x*y}
> myg <- function(x,y) {x^2 + y^2}
>
> d1 <- expand.grid(X1 = x, X2 = y) %>%
>    mutate(Z = myf(X1,X2)) %>%
>    as.data.frame()
>
> d2 <- expand.grid(X1 = x, X2 = y) %>%
>    mutate(Z = myg(X1,X2)) %>%
>    as.data.frame()
>
> ggplot(data = d1, aes(x=X1,y=X2,z=Z))+
>    stat_contour(breaks = c(2)) +
>    stat_contour(data=d2, aes(x=X1,y=X2,z=Z), breaks=c(6))
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jrkr|de@u @end|ng |rom gm@||@com  Fri Jan 12 17:35:33 2024
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Fri, 12 Jan 2024 11:35:33 -0500
Subject: [R] arrow on contour line
In-Reply-To: <CA+Rxh84ii9JGitkPEp+2=rr-WTiKpg7kq4YSM=LELRpq08Tg7w@mail.gmail.com>
References: <CA+Rxh84ii9JGitkPEp+2=rr-WTiKpg7kq4YSM=LELRpq08Tg7w@mail.gmail.com>
Message-ID: <CAKZQJMC_0-H4fGYmv09h+89ve3hE--9gh8YFBPswHoXOQukSbA@mail.gmail.com>

Something like this shodld worx.  You will need to fiddle around with the
actual co-ordinates etc. I just stuck an arrow in what seemed like a handy
place

On Wed, 10 Jan 2024 at 19:13, Deepankar Basu <basu.15 at gmail.com> wrote:

> Hello,
>
> I am drawing contour lines for a function of 2 variables at one level of
> the value of the function and want to include a small arrow in any
> direction of increase of the function. Is there some way to do that?
>
> Below is an example that creates the contour lines. How do I add one small
> arrow on each line in the direction of increase of the function (at some
> central point of the contour line)? Any direction will do, but perhaps the
> direction of the gradient will be the best.
>
> Thanks in advance.
> DB
>
> --------------------------------------------
>
> library(tidyverse)
>
> x <- seq(1,2,length.out=100)
> y <- seq(1,2,length.out=100)
>
> myf <- function(x,y) {x*y}
> myg <- function(x,y) {x^2 + y^2}
>
> d1 <- expand.grid(X1 = x, X2 = y) %>%
>   mutate(Z = myf(X1,X2)) %>%
>   as.data.frame()
>
> d2 <- expand.grid(X1 = x, X2 = y) %>%
>   mutate(Z = myg(X1,X2)) %>%
>   as.data.frame()
>
> ggplot(data = d1, aes(x=X1,y=X2,z=Z))+
>   stat_contour(breaks = c(2)) +
>   stat_contour(data=d2, aes(x=X1,y=X2,z=Z), breaks=c(6))
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
John Kane
Kingston ON Canada

	[[alternative HTML version deleted]]


From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Sat Jan 13 21:33:47 2024
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Sat, 13 Jan 2024 20:33:47 +0000 (UTC)
Subject: [R] Strange results : bootrstrp CIs
References: <1711614088.1808243.1705178027739.ref@mail.yahoo.com>
Message-ID: <1711614088.1808243.1705178027739@mail.yahoo.com>

Dear R-experts,

Here below, my R code working BUT I get a strange result I was not expecting! Indeed, the?95% percentile bootstrap CIs is (-54.81, -54.81 ). Is anything going wrong?

Best,

##########################################
Score=c(345,564,467,675,432,346,476,512,567,543,234,435,654,411,356,658,432,345,432,345, 345,456,543,501)
?
Country=c("Italy", "Italy", "Italy", "Turkey", "Turkey", "Turkey", "USA", "USA", "USA", "Korea", "Korea", "Korea", "Portugal", "Portugal", "Portugal", "UK", "UK", "UK", "Poland", "Poland", "Poland", "Austria", "Austria", "Austria")
?
Time=c(1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3)
?
e=data.frame(Score, Country, Time)
?
?
library(boot)
func= function(data, idx) {
coef(lm(Score~ Time + factor(Country)),data=data[idx,])
}
B= boot(e, func, R=1000)
?
boot.ci(B, index=2, type="perc")
#############################################################


From murdoch@dunc@n @end|ng |rom gm@||@com  Sat Jan 13 21:50:27 2024
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sat, 13 Jan 2024 15:50:27 -0500
Subject: [R] Strange results : bootrstrp CIs
In-Reply-To: <1711614088.1808243.1705178027739@mail.yahoo.com>
References: <1711614088.1808243.1705178027739.ref@mail.yahoo.com>
 <1711614088.1808243.1705178027739@mail.yahoo.com>
Message-ID: <c1853d90-2ad0-42b5-a72b-c4f4b813422d@gmail.com>

On 13/01/2024 3:33 p.m., varin sacha via R-help wrote:
> Score=c(345,564,467,675,432,346,476,512,567,543,234,435,654,411,356,658,432,345,432,345,
>   345,456,543,501)
>   
> Country=c("Italy", "Italy", "Italy", "Turkey", "Turkey", "Turkey",
> "USA", "USA", "USA", "Korea", "Korea", "Korea", "Portugal", "Portugal",
> "Portugal", "UK", "UK", "UK", "Poland", "Poland", "Poland", "Austria",
> "Austria", "Austria")
>   
> Time=c(1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3)
>   
> e=data.frame(Score, Country, Time)
>   
>   
> library(boot)
> func= function(data, idx) {
> coef(lm(Score~ Time + factor(Country)),data=data[idx,])
> }
> B= boot(e, func, R=1000)
>   
> boot.ci(B, index=2, type="perc")

Your function ignores the data, because it passes data[idx,] to coef(), 
not to lm().  coef() ignores it.  So the function is using the global 
variables you created earlier, not the ones in e.

Duncan Murdoch


From |kry|ov @end|ng |rom d|@root@org  Sat Jan 13 21:56:56 2024
From: |kry|ov @end|ng |rom d|@root@org (Ivan Krylov)
Date: Sat, 13 Jan 2024 23:56:56 +0300
Subject: [R] Strange results : bootrstrp CIs
In-Reply-To: <1711614088.1808243.1705178027739@mail.yahoo.com>
References: <1711614088.1808243.1705178027739.ref@mail.yahoo.com>
 <1711614088.1808243.1705178027739@mail.yahoo.com>
Message-ID: <20240113235656.57bec978@Tarkus>

? Sat, 13 Jan 2024 20:33:47 +0000 (UTC)
varin sacha via R-help <r-help at r-project.org> ?????:

> coef(lm(Score~ Time + factor(Country)),data=data[idx,])

Wrong place for the data=... argument. You meant to give it to lm(...),
but in the end it went to coef(...). Without the data=... argument, the
formula passed to lm() picks up the global variables inherited by the
func() closure.

Unfortunately, S3 methods really do have to ignore extra arguments they
don't understand if the class is to be extended, so coef.lm isn't
allowed to complain to you about it.

-- 
Best regards,
Ivan


From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Sat Jan 13 23:22:59 2024
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Sat, 13 Jan 2024 22:22:59 +0000 (UTC)
Subject: [R] Strange results : bootrstrp CIs
In-Reply-To: <20240113235656.57bec978@Tarkus>
References: <1711614088.1808243.1705178027739.ref@mail.yahoo.com>
 <1711614088.1808243.1705178027739@mail.yahoo.com>
 <20240113235656.57bec978@Tarkus>
Message-ID: <1336818939.1840583.1705184579275@mail.yahoo.com>

Dear Duncan,
Dear Ivan,

I really thank you a lot for your response.
So, if I correctly understand your answers the problem is coming from this line:

coef(lm(Score~ Time + factor(Country)),data=data[idx,])

This line should be:
coef(lm(Score~ Time + factor(Country),data=data[idx,]))

If yes, now I get an error message (code here below)! So, it still does not work.

Error in t.star[r, ] <- res[[r]] :
? number of items to replace is not a multiple of replacement length


##########################################
Score=c(345,564,467,675,432,346,476,512,567,543,234,435,654,411,356,658,432,345,432,345, 345,456,543,501)
?
Country=c("Italy", "Italy", "Italy", "Turkey", "Turkey", "Turkey", "USA", "USA", "USA", "Korea", "Korea", "Korea", "Portugal", "Portugal", "Portugal", "UK", "UK", "UK", "Poland", "Poland", "Poland", "Austria", "Austria", "Austria")
?
Time=c(1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3)
?
e=data.frame(Score, Country, Time)
?
?
library(boot)
func= function(data, idx) {
coef(lm(Score~ Time + factor(Country),data=data[idx,]))
}
B= boot(e, func, R=1000)
?
boot.ci(B, index=2, type="perc")
#############################################################








Le samedi 13 janvier 2024 ? 21:56:58 UTC+1, Ivan Krylov <ikrylov at disroot.org> a ?crit : 





? Sat, 13 Jan 2024 20:33:47 +0000 (UTC)

varin sacha via R-help <r-help at r-project.org> ?????:

> coef(lm(Score~ Time + factor(Country)),data=data[idx,])


Wrong place for the data=... argument. You meant to give it to lm(...),
but in the end it went to coef(...). Without the data=... argument, the
formula passed to lm() picks up the global variables inherited by the
func() closure.

Unfortunately, S3 methods really do have to ignore extra arguments they
don't understand if the class is to be extended, so coef.lm isn't
allowed to complain to you about it.

-- 
Best regards,
Ivan


From bbo|ker @end|ng |rom gm@||@com  Sat Jan 13 23:51:24 2024
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sat, 13 Jan 2024 17:51:24 -0500
Subject: [R] Strange results : bootrstrp CIs
In-Reply-To: <1336818939.1840583.1705184579275@mail.yahoo.com>
References: <1711614088.1808243.1705178027739.ref@mail.yahoo.com>
 <1711614088.1808243.1705178027739@mail.yahoo.com>
 <20240113235656.57bec978@Tarkus>
 <1336818939.1840583.1705184579275@mail.yahoo.com>
Message-ID: <78c206d2-61df-48ea-a134-d2e89de7a210@gmail.com>

   It took me a little while to figure this out, but: the problem is 
that if your resampling leaves out any countries (which is very likely), 
your model applied to the bootstrapped data will have fewer coefficients 
than your original model.

I tried this:

cc <- unique(e$Country)
func <- function(data, idx) {
    coef(lm(Score~ Time + factor(Country, levels =cc),data=data[idx,]))
}

but lm() automatically drops coefficients for missing countries (I 
didn't think about it too hard, but I thought they might get returned as 
NA and that boot() might be able to handle that ...)

   If you want to do this I think you'll have to find a way to do a 
*stratified* bootstrap, restricting the bootstrap samples so that they 
always contain at least one sample from each country ... (I would have 
expected "strata = as.numeric(e$Country)" to do this, but it doesn't 
work the way I thought ... it tries to compute the statistics for *each* 
stratum ...)



====

  Debugging attempts:

set.seed(101)
options(error=recover)
B= boot(e, func, R=1000)


Error in t.star[r, ] <- res[[r]] :
   number of items to replace is not a multiple of replacement length

Enter a frame number, or 0 to exit

1: boot(e, func, R = 1000)

<enter 1>

Selection: 1
Called from: top level
Browse[1]> print(r)
[1] 2
Browse[1]> t.star[r,]
[1] NA NA NA NA NA NA NA NA NA

i[2,]
  [1] 14 15 22 22 21 14  8  2 12 22 10 15  9  7  9 13 12 23  1 20 15  7 
5 10




On 2024-01-13 5:22 p.m., varin sacha via R-help wrote:
> Dear Duncan,
> Dear Ivan,
> 
> I really thank you a lot for your response.
> So, if I correctly understand your answers the problem is coming from this line:
> 
> coef(lm(Score~ Time + factor(Country)),data=data[idx,])
> 
> This line should be:
> coef(lm(Score~ Time + factor(Country),data=data[idx,]))
> 
> If yes, now I get an error message (code here below)! So, it still does not work.
> 
> Error in t.star[r, ] <- res[[r]] :
>  ? number of items to replace is not a multiple of replacement length
> 
> 
> ##########################################
> Score=c(345,564,467,675,432,346,476,512,567,543,234,435,654,411,356,658,432,345,432,345, 345,456,543,501)
>   
> Country=c("Italy", "Italy", "Italy", "Turkey", "Turkey", "Turkey", "USA", "USA", "USA", "Korea", "Korea", "Korea", "Portugal", "Portugal", "Portugal", "UK", "UK", "UK", "Poland", "Poland", "Poland", "Austria", "Austria", "Austria")
>   
> Time=c(1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3)
>   
> e=data.frame(Score, Country, Time)
>   
>   
> library(boot)
> func= function(data, idx) {
> coef(lm(Score~ Time + factor(Country),data=data[idx,]))
> }
> B= boot(e, func, R=1000)
>   
> boot.ci(B, index=2, type="perc")
> #############################################################
> 
> 
> 
> 
> 
> 
> 
> 
> Le samedi 13 janvier 2024 ? 21:56:58 UTC+1, Ivan Krylov <ikrylov at disroot.org> a ?crit :
> 
> 
> 
> 
> 
> ? Sat, 13 Jan 2024 20:33:47 +0000 (UTC)
> 
> varin sacha via R-help <r-help at r-project.org> ?????:
> 
>> coef(lm(Score~ Time + factor(Country)),data=data[idx,])
> 
> 
> Wrong place for the data=... argument. You meant to give it to lm(...),
> but in the end it went to coef(...). Without the data=... argument, the
> formula passed to lm() picks up the global variables inherited by the
> func() closure.
> 
> Unfortunately, S3 methods really do have to ignore extra arguments they
> don't understand if the class is to be extended, so coef.lm isn't
> allowed to complain to you about it.
>


From murdoch@dunc@n @end|ng |rom gm@||@com  Sat Jan 13 23:59:16 2024
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sat, 13 Jan 2024 17:59:16 -0500
Subject: [R] Fwd:  Strange results : bootrstrp CIs
In-Reply-To: <3293ab1c-4b8c-4121-b345-ac229bf53da3@gmail.com>
References: <3293ab1c-4b8c-4121-b345-ac229bf53da3@gmail.com>
Message-ID: <7884da76-8b6a-418c-9d92-64b564dbfefe@gmail.com>

Sorry, didn't cc this to the list.


-------- Forwarded Message --------
Subject: Re: [R] Strange results : bootrstrp CIs
Date: Sat, 13 Jan 2024 17:37:19 -0500
From: Duncan Murdoch <murdoch.duncan at gmail.com>
To: varin sacha <varinsacha at yahoo.fr>

You can debug things like this by setting options(error = recover). That 
will drop into the debugger when the error occurs.  Examine t.star, r, 
and res[[r]] and you will likely see what the problem was.

My guess is that one of the bootstrap samples had a different selection 
of countries, so factor(Country) had different levels, and that would 
really mess things up.

You'll need to decide how to handle that:  If you are trying to estimate 
the coefficient for Italy in a sample that contains no data from Italy, 
what should the coefficient be?  (This is easier for Bayesians to 
handle:  we don't need data!)

Duncan

On 13/01/2024 5:22 p.m., varin sacha via R-help wrote:
> Dear Duncan,
> Dear Ivan,
> 
> I really thank you a lot for your response.
> So, if I correctly understand your answers the problem is coming from this line:
> 
> coef(lm(Score~ Time + factor(Country)),data=data[idx,])
> 
> This line should be:
> coef(lm(Score~ Time + factor(Country),data=data[idx,]))
> 
> If yes, now I get an error message (code here below)! So, it still does not work.
> 
> Error in t.star[r, ] <- res[[r]] :
>  ? number of items to replace is not a multiple of replacement length
> 
> 
> ##########################################
> Score=c(345,564,467,675,432,346,476,512,567,543,234,435,654,411,356,658,432,345,432,345, 345,456,543,501)
>   
> Country=c("Italy", "Italy", "Italy", "Turkey", "Turkey", "Turkey", "USA", "USA", "USA", "Korea", "Korea", "Korea", "Portugal", "Portugal", "Portugal", "UK", "UK", "UK", "Poland", "Poland", "Poland", "Austria", "Austria", "Austria")
>   
> Time=c(1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3)
>   
> e=data.frame(Score, Country, Time)
>   
>   
> library(boot)
> func= function(data, idx) {
> coef(lm(Score~ Time + factor(Country),data=data[idx,]))
> }
> B= boot(e, func, R=1000)
>   
> boot.ci(B, index=2, type="perc")
> #############################################################
> 
> 
> 
> 
> 
> 
> 
> 
> Le samedi 13 janvier 2024 ? 21:56:58 UTC+1, Ivan Krylov <ikrylov at disroot.org> a ?crit :
> 
> 
> 
> 
> 
> ? Sat, 13 Jan 2024 20:33:47 +0000 (UTC)
> 
> varin sacha via R-help <r-help at r-project.org> ?????:
> 
>> coef(lm(Score~ Time + factor(Country)),data=data[idx,])
> 
> 
> Wrong place for the data=... argument. You meant to give it to lm(...),
> but in the end it went to coef(...). Without the data=... argument, the
> formula passed to lm() picks up the global variables inherited by the
> func() closure.
> 
> Unfortunately, S3 methods really do have to ignore extra arguments they
> don't understand if the class is to be extended, so coef.lm isn't
> allowed to complain to you about it.
>


From bgunter@4567 @end|ng |rom gm@||@com  Sun Jan 14 01:54:01 2024
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sat, 13 Jan 2024 16:54:01 -0800
Subject: [R] Strange results : bootrstrp CIs
In-Reply-To: <78c206d2-61df-48ea-a134-d2e89de7a210@gmail.com>
References: <1711614088.1808243.1705178027739.ref@mail.yahoo.com>
 <1711614088.1808243.1705178027739@mail.yahoo.com>
 <20240113235656.57bec978@Tarkus>
 <1336818939.1840583.1705184579275@mail.yahoo.com>
 <78c206d2-61df-48ea-a134-d2e89de7a210@gmail.com>
Message-ID: <CAGxFJbR06n2FtN9bCcSzLRiVX7HpFyzFAGQQ5Zu4kKshMU3kjQ@mail.gmail.com>

Well, this would seem to work:

e <- data.frame(Score = Score
             , Country = factor(Country)
             , Time = Time)

ncountry <- nlevels(e$Country)
func= function(dat,idx) {
   if(length(unique(dat[idx,'Country'])) < ncountry) NA
   else coef(lm(Score~ Time + Country,data = dat[idx,]))
}
B <-  boot(e, func, R=1000)

boot.ci(B, index=2, type="perc")

Caveats:
1) boot.ci handles the NA's by omitting them, which of course gives a
smaller resample and longer CI's than the value of R specified in the call
to boot().

2) I do not know if the *nice* statistical properties of the nonparametric
bootstrap, e.g. asymptotic correctness, hold when bootstrap samples are
produced in this way.  I leave that to wiser heads than me.

Cheers,
Bert

On Sat, Jan 13, 2024 at 2:51?PM Ben Bolker <bbolker at gmail.com> wrote:

>    It took me a little while to figure this out, but: the problem is
> that if your resampling leaves out any countries (which is very likely),
> your model applied to the bootstrapped data will have fewer coefficients
> than your original model.
>
> I tried this:
>
> cc <- unique(e$Country)
> func <- function(data, idx) {
>     coef(lm(Score~ Time + factor(Country, levels =cc),data=data[idx,]))
> }
>
> but lm() automatically drops coefficients for missing countries (I
> didn't think about it too hard, but I thought they might get returned as
> NA and that boot() might be able to handle that ...)
>
>    If you want to do this I think you'll have to find a way to do a
> *stratified* bootstrap, restricting the bootstrap samples so that they
> always contain at least one sample from each country ... (I would have
> expected "strata = as.numeric(e$Country)" to do this, but it doesn't
> work the way I thought ... it tries to compute the statistics for *each*
> stratum ...)
>
>
>
> ====
>
>   Debugging attempts:
>
> set.seed(101)
> options(error=recover)
> B= boot(e, func, R=1000)
>
>
> Error in t.star[r, ] <- res[[r]] :
>    number of items to replace is not a multiple of replacement length
>
> Enter a frame number, or 0 to exit
>
> 1: boot(e, func, R = 1000)
>
> <enter 1>
>
> Selection: 1
> Called from: top level
> Browse[1]> print(r)
> [1] 2
> Browse[1]> t.star[r,]
> [1] NA NA NA NA NA NA NA NA NA
>
> i[2,]
>   [1] 14 15 22 22 21 14  8  2 12 22 10 15  9  7  9 13 12 23  1 20 15  7
> 5 10
>
>
>
>
> On 2024-01-13 5:22 p.m., varin sacha via R-help wrote:
> > Dear Duncan,
> > Dear Ivan,
> >
> > I really thank you a lot for your response.
> > So, if I correctly understand your answers the problem is coming from
> this line:
> >
> > coef(lm(Score~ Time + factor(Country)),data=data[idx,])
> >
> > This line should be:
> > coef(lm(Score~ Time + factor(Country),data=data[idx,]))
> >
> > If yes, now I get an error message (code here below)! So, it still does
> not work.
> >
> > Error in t.star[r, ] <- res[[r]] :
> >    number of items to replace is not a multiple of replacement length
> >
> >
> > ##########################################
> >
> Score=c(345,564,467,675,432,346,476,512,567,543,234,435,654,411,356,658,432,345,432,345,
> 345,456,543,501)
> >
> > Country=c("Italy", "Italy", "Italy", "Turkey", "Turkey", "Turkey",
> "USA", "USA", "USA", "Korea", "Korea", "Korea", "Portugal", "Portugal",
> "Portugal", "UK", "UK", "UK", "Poland", "Poland", "Poland", "Austria",
> "Austria", "Austria")
> >
> > Time=c(1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3)
> >
> > e=data.frame(Score, Country, Time)
> >
> >
> > library(boot)
> > func= function(data, idx) {
> > coef(lm(Score~ Time + factor(Country),data=data[idx,]))
> > }
> > B= boot(e, func, R=1000)
> >
> > boot.ci(B, index=2, type="perc")
> > #############################################################
> >
> >
> >
> >
> >
> >
> >
> >
> > Le samedi 13 janvier 2024 ? 21:56:58 UTC+1, Ivan Krylov <
> ikrylov at disroot.org> a ?crit :
> >
> >
> >
> >
> >
> > ? Sat, 13 Jan 2024 20:33:47 +0000 (UTC)
> >
> > varin sacha via R-help <r-help at r-project.org> ?????:
> >
> >> coef(lm(Score~ Time + factor(Country)),data=data[idx,])
> >
> >
> > Wrong place for the data=... argument. You meant to give it to lm(...),
> > but in the end it went to coef(...). Without the data=... argument, the
> > formula passed to lm() picks up the global variables inherited by the
> > func() closure.
> >
> > Unfortunately, S3 methods really do have to ignore extra arguments they
> > don't understand if the class is to be extended, so coef.lm isn't
> > allowed to complain to you about it.
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ro||turner @end|ng |rom po@teo@net  Sun Jan 14 02:58:49 2024
From: ro||turner @end|ng |rom po@teo@net (Rolf Turner)
Date: Sun, 14 Jan 2024 01:58:49 +0000
Subject: [R] Fwd:  Strange results : bootrstrp CIs
In-Reply-To: <7884da76-8b6a-418c-9d92-64b564dbfefe@gmail.com>
References: <3293ab1c-4b8c-4121-b345-ac229bf53da3@gmail.com>
 <7884da76-8b6a-418c-9d92-64b564dbfefe@gmail.com>
Message-ID: <20240114145849.12608d85@rolf-Latitude-E7470>

On Sat, 13 Jan 2024 17:59:16 -0500
Duncan Murdoch <murdoch.duncan at gmail.com> wrote:

<SNIP>

> My guess is that one of the bootstrap samples had a different
> selection of countries, so factor(Country) had different levels, and
> that would really mess things up.
> 
> You'll need to decide how to handle that:  If you are trying to
> estimate the coefficient for Italy in a sample that contains no data
> from Italy, what should the coefficient be?

Perhaps NA?  Ben Bolker conjectured that boot() might be able to handle
this.  Getting the NAs into the coefficients is a bit of a fag, but.  I
tried:

func <- function(data, idx) {
clyde <- coef(lm(Score~ Time + factor(Country),data=data))
ccc <- coef(lm(Score~ Time + factor(Country),data=data[idx,]))
urk <- rep(NA,length(clyde))
names(urk) <-names(clyde)
urk[names(ccc)] <- ccc
urk
}

It produced a result:

> > set.seed(42)
> > B= boot(e, func, R=1000)
> B
> 
> ORDINARY NONPARAMETRIC BOOTSTRAP
> 
> 
> Call:
> boot(data = e, statistic = func, R = 1000)
> 
> 
> Bootstrap Statistics :
>       original     bias    std. error
> t1*  609.62500  3.6204052    95.39452
> t2*  -54.81250 -1.6624704    36.32911
> t3*  -41.33333 -2.7337992   100.72113
> t4*  -96.00000 -1.0995718    99.78864
> t5* -126.00000 -0.6548886    63.47076
> t6*  -26.33333 -1.6516683    87.80483
> t7*  -15.66667 -0.8391170    91.72467
> t8*  -21.66667 -5.4544013    83.69211
> t9*   18.33333 -0.7711001    85.57278

However I have no idea if the result is correct, or even meaningful. I
have no idea what I'm doing.  Just hammering and hoping. ??

<SNIP>

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Stats. Dep't. (secretaries) phone:
         +64-9-373-7599 ext. 89622
Home phone: +64-9-480-4619


From ro||turner @end|ng |rom po@teo@net  Sun Jan 14 03:33:49 2024
From: ro||turner @end|ng |rom po@teo@net (Rolf Turner)
Date: Sun, 14 Jan 2024 02:33:49 +0000
Subject: [R] Strange results : bootrstrp CIs
In-Reply-To: <CAGxFJbR06n2FtN9bCcSzLRiVX7HpFyzFAGQQ5Zu4kKshMU3kjQ@mail.gmail.com>
References: <1711614088.1808243.1705178027739.ref@mail.yahoo.com>
 <1711614088.1808243.1705178027739@mail.yahoo.com>
 <20240113235656.57bec978@Tarkus>
 <1336818939.1840583.1705184579275@mail.yahoo.com>
 <78c206d2-61df-48ea-a134-d2e89de7a210@gmail.com>
 <CAGxFJbR06n2FtN9bCcSzLRiVX7HpFyzFAGQQ5Zu4kKshMU3kjQ@mail.gmail.com>
Message-ID: <20240114153349.6295ac5a@rolf-Latitude-E7470>


On Sat, 13 Jan 2024 16:54:01 -0800
Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Well, this would seem to work:
> 
> e <- data.frame(Score = Score
>              , Country = factor(Country)
>              , Time = Time)
> 
> ncountry <- nlevels(e$Country)
> func= function(dat,idx) {
>    if(length(unique(dat[idx,'Country'])) < ncountry) NA
>    else coef(lm(Score~ Time + Country,data = dat[idx,]))
> }
> B <-  boot(e, func, R=1000)
> 
> boot.ci(B, index=2, type="perc")
> 
> Caveats:
> 1) boot.ci handles the NA's by omitting them, which of course gives a
> smaller resample and longer CI's than the value of R specified in the
> call to boot().
> 
> 2) I do not know if the *nice* statistical properties of the
> nonparametric bootstrap, e.g. asymptotic correctness, hold when
> bootstrap samples are produced in this way.  I leave that to wiser
> heads than me.

<SNIP>

It seems to me that my shaganappi idea causes func() to return a vector
of coefficients with NAs corresponding to any missing levels of the
"Country" factor, whereas your idea causes it to return a scalar NA
whenever one or more of the levels of the "Country" factor is missing.

I have no idea what the implications of this are.  As I said before, I
have no idea what I am doing!

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Stats. Dep't. (secretaries) phone:
         +64-9-373-7599 ext. 89622
Home phone: +64-9-480-4619


From murdoch@dunc@n @end|ng |rom gm@||@com  Sun Jan 14 10:21:36 2024
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sun, 14 Jan 2024 04:21:36 -0500
Subject: [R] Fwd: Strange results : bootrstrp CIs
In-Reply-To: <20240114145849.12608d85@rolf-Latitude-E7470>
References: <3293ab1c-4b8c-4121-b345-ac229bf53da3@gmail.com>
 <7884da76-8b6a-418c-9d92-64b564dbfefe@gmail.com>
 <20240114145849.12608d85@rolf-Latitude-E7470>
Message-ID: <4e11dfe3-e797-4a72-9e44-e43c29b59bfd@gmail.com>

On 13/01/2024 8:58 p.m., Rolf Turner wrote:
> On Sat, 13 Jan 2024 17:59:16 -0500
> Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> 
> <SNIP>
> 
>> My guess is that one of the bootstrap samples had a different
>> selection of countries, so factor(Country) had different levels, and
>> that would really mess things up.
>>
>> You'll need to decide how to handle that:  If you are trying to
>> estimate the coefficient for Italy in a sample that contains no data
>> from Italy, what should the coefficient be?
> 
> Perhaps NA?  Ben Bolker conjectured that boot() might be able to handle
> this.  Getting the NAs into the coefficients is a bit of a fag, but.  I
> tried:

My question was really intended as a statistical question.  From a 
statistical perspective, if I have a sampling scheme that sometimes 
generates sample size 0, should my CI be (-Inf, Inf) for high enough 
confidence level?

A Bayesian might say that inference should be entirely based on the 
prior in the case of no relevant data.  You could get similar numerical 
results by adding some fake data to every bootstrap sample, e.g. a 
single weighted observation for each country at your prior mean for that 
country, with weight chosen to match the strength of the prior.  But 
Bayesian methods don't give confidence intervals, they give credible 
intervals, and those aren't the same thing even if they are sometimes 
numerically similar.

Duncan Murdoch


> func <- function(data, idx) {
> clyde <- coef(lm(Score~ Time + factor(Country),data=data))
> ccc <- coef(lm(Score~ Time + factor(Country),data=data[idx,]))
> urk <- rep(NA,length(clyde))
> names(urk) <-names(clyde)
> urk[names(ccc)] <- ccc
> urk
> }
> 
> It produced a result:
> 
>>> set.seed(42)
>>> B= boot(e, func, R=1000)
>> B
>>
>> ORDINARY NONPARAMETRIC BOOTSTRAP
>>
>>
>> Call:
>> boot(data = e, statistic = func, R = 1000)
>>
>>
>> Bootstrap Statistics :
>>        original     bias    std. error
>> t1*  609.62500  3.6204052    95.39452
>> t2*  -54.81250 -1.6624704    36.32911
>> t3*  -41.33333 -2.7337992   100.72113
>> t4*  -96.00000 -1.0995718    99.78864
>> t5* -126.00000 -0.6548886    63.47076
>> t6*  -26.33333 -1.6516683    87.80483
>> t7*  -15.66667 -0.8391170    91.72467
>> t8*  -21.66667 -5.4544013    83.69211
>> t9*   18.33333 -0.7711001    85.57278
> 
> However I have no idea if the result is correct, or even meaningful. I
> have no idea what I'm doing.  Just hammering and hoping. ??
> 
> <SNIP>
> 
> cheers,
> 
> Rolf
>


From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Sun Jan 14 20:38:47 2024
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Sun, 14 Jan 2024 19:38:47 +0000 (UTC)
Subject: [R] Fwd: Strange results : bootrstrp CIs
In-Reply-To: <4e11dfe3-e797-4a72-9e44-e43c29b59bfd@gmail.com>
References: <3293ab1c-4b8c-4121-b345-ac229bf53da3@gmail.com>
 <7884da76-8b6a-418c-9d92-64b564dbfefe@gmail.com>
 <20240114145849.12608d85@rolf-Latitude-E7470>
 <4e11dfe3-e797-4a72-9e44-e43c29b59bfd@gmail.com>
Message-ID: <1473931666.2169292.1705261127244@mail.yahoo.com>

Dear R-experts,

I really thank you all for your responses.

Best,



Le dimanche 14 janvier 2024 ? 10:22:12 UTC+1, Duncan Murdoch <murdoch.duncan at gmail.com> a ?crit : 





On 13/01/2024 8:58 p.m., Rolf Turner wrote:
> On Sat, 13 Jan 2024 17:59:16 -0500
> Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> 
> <SNIP>
> 
>> My guess is that one of the bootstrap samples had a different
>> selection of countries, so factor(Country) had different levels, and
>> that would really mess things up.
>>
>> You'll need to decide how to handle that:? If you are trying to
>> estimate the coefficient for Italy in a sample that contains no data
>> from Italy, what should the coefficient be?
> 
> Perhaps NA?? Ben Bolker conjectured that boot() might be able to handle
> this.? Getting the NAs into the coefficients is a bit of a fag, but.? I
> tried:

My question was really intended as a statistical question.? From a 
statistical perspective, if I have a sampling scheme that sometimes 
generates sample size 0, should my CI be (-Inf, Inf) for high enough 
confidence level?

A Bayesian might say that inference should be entirely based on the 
prior in the case of no relevant data.? You could get similar numerical 
results by adding some fake data to every bootstrap sample, e.g. a 
single weighted observation for each country at your prior mean for that 
country, with weight chosen to match the strength of the prior.? But 
Bayesian methods don't give confidence intervals, they give credible 
intervals, and those aren't the same thing even if they are sometimes 
numerically similar.


Duncan Murdoch


> func <- function(data, idx) {
> clyde <- coef(lm(Score~ Time + factor(Country),data=data))
> ccc <- coef(lm(Score~ Time + factor(Country),data=data[idx,]))
> urk <- rep(NA,length(clyde))
> names(urk) <-names(clyde)
> urk[names(ccc)] <- ccc
> urk
> }
> 
> It produced a result:
> 
>>> set.seed(42)
>>> B= boot(e, func, R=1000)
>> B
>>
>> ORDINARY NONPARAMETRIC BOOTSTRAP
>>
>>
>> Call:
>> boot(data = e, statistic = func, R = 1000)
>>
>>
>> Bootstrap Statistics :
>>? ? ? ? original? ? bias? ? std. error
>> t1*? 609.62500? 3.6204052? ? 95.39452
>> t2*? -54.81250 -1.6624704? ? 36.32911
>> t3*? -41.33333 -2.7337992? 100.72113
>> t4*? -96.00000 -1.0995718? ? 99.78864
>> t5* -126.00000 -0.6548886? ? 63.47076
>> t6*? -26.33333 -1.6516683? ? 87.80483
>> t7*? -15.66667 -0.8391170? ? 91.72467
>> t8*? -21.66667 -5.4544013? ? 83.69211
>> t9*? 18.33333 -0.7711001? ? 85.57278
> 
> However I have no idea if the result is correct, or even meaningful. I
> have no idea what I'm doing.? Just hammering and hoping. ??
> 
> <SNIP>
> 
> cheers,
> 
> Rolf
>

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Sun Jan 14 20:59:28 2024
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Sun, 14 Jan 2024 19:59:28 +0000 (UTC)
Subject: [R] Plotting extrapolation with R like AUTOBOX does
References: <1020257293.2174036.1705262368538.ref@mail.yahoo.com>
Message-ID: <1020257293.2174036.1705262368538@mail.yahoo.com>

Dear R-experts,

I write to you to know if somebody is aware of a R package (or function) able to plot graphs for extrapolation.

I need to be clear on what extrapolation really is to me. It is when we use the model for X variables outside the range of X variables that were used to construct the model and estimates.?

What I am really looking for is that beyond confidence intervals for predictions for the one estimated model, I want intervals that reflect the uncertainty over the models that could have fit the data as well. I know that Clive Granger worried about this a lot, and wrote a few papers (he called them 'thick' confidence intervals, taking into account model uncertainty).

I found AUTOBOX software (https://autobox.com/cms/) allowing multiple time series to be modeled as possible inputs without requiring time as a possible predictor thus it can reproduce ordinary regression.

More precisely, I am looking for an R package (or function) able to produce the same 4 graphs (done with AUTOBOX) in the answer here :?https://stats.stackexchange.com/questions/327454/right-way-to-extrapolate-data

Best,


From p@tr|ck@g|r@udoux @end|ng |rom un|v-|comte@|r  Tue Jan 16 09:19:40 2024
From: p@tr|ck@g|r@udoux @end|ng |rom un|v-|comte@|r (Patrick Giraudoux)
Date: Tue, 16 Jan 2024 09:19:40 +0100
Subject: [R] different results between cor and ccf
Message-ID: <6fdaed51-00da-4ac9-8c0e-e320c4fdc218@univ-fcomte.fr>

Dear listers,

I am working on a time series but find that for a given non-zero time 
lag correlations obtained by ccf and cor are different.

x <- c(0.85472102802704641, 1.6008990694641689, 2.5019632258894835, 
2.514654801253164, 3.3359198688206368, 3.5401357138398208, 
2.6304117871193538, 3.6694074965420009, 3.9125153101706776, 
4.4006592535478566, 3.0208991912866829, 2.959090589344433, 
3.8434635568566056, 2.1683644330520457, 2.3060571563512973, 
1.4680350663043942, 2.0346918622459054, 2.3674524446877538)

y <- c(2.3085729270534765, 2.0809088217491416, 1.6249456563631131, 
1.5133386666933177, 0.66754156827555422, 0.3080839731181978, 
0.52653042555599394, 0.89070463020837132, 0.71600791432232669, 
0.82152341002975027, 0.22200290782700527, 0.6608410635137173, 
0.90715232876618945, 0.45624062770725898, 0.35074487486980244, 
1.1681750562971052, 1.6976462236079737, 0.88950230250556417)

cc<-ccf(x,y)

> cc Autocorrelations of series ?X?, by lag -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 
2 0.098 0.139 0.127 -0.043 -0.049 0.069 -0.237 -0.471 -0.668 -0.595 
-0.269 -0.076 3 4 5 6 7 8 9 -0.004 0.123 0.272 0.283 0.401 0.435 0.454

cor(x,y) [1] -0.5948694

So far so good, but when I lag one of the series, I cannot find the same 
correlation as with ccf

> cor(x[1:(length(x)-1)],y[2:length(y)]) [1] -0.7903428

... where I expect -0.668 based on ccf

Can anyone explain why ?

Best,

Patrick

	[[alternative HTML version deleted]]


From berw|n@tur|@ch @end|ng |rom gm@||@com  Tue Jan 16 11:32:51 2024
From: berw|n@tur|@ch @end|ng |rom gm@||@com (Berwin A Turlach)
Date: Tue, 16 Jan 2024 18:32:51 +0800
Subject: [R] different results between cor and ccf
In-Reply-To: <6fdaed51-00da-4ac9-8c0e-e320c4fdc218@univ-fcomte.fr>
References: <6fdaed51-00da-4ac9-8c0e-e320c4fdc218@univ-fcomte.fr>
Message-ID: <20240116183251.4f54eaff@dep59159.uniwa.uwa.edu.au>

G'day Patrick,

On Tue, 16 Jan 2024 09:19:40 +0100
Patrick Giraudoux <patrick.giraudoux at univ-fcomte.fr> wrote:

[...]
> So far so good, but when I lag one of the series, I cannot find the
> same correlation as with ccf
> 
> > cor(x[1:(length(x)-1)],y[2:length(y)]) [1] -0.7903428  
> 
> ... where I expect -0.668 based on ccf
> 
> Can anyone explain why ?

The difference is explained by cff() seeing the complete data on x and
y and calculating the sample means only once, which are then used in
the calculations for each lag.  cor() sees only the data you pass down,
so calculates different estimates for the means of the two sequences.

To illustrate:

[...first execute your code...]
R> xx <- x-mean(x)
R> yy <- y-mean(y)
R> n <- length(x)
R> vx <- sum(xx^2)/n
R> vy <- sum(yy^2)/n
R> (c0 <- sum(xx*yy)/n/sqrt(vx*vy))
[1] -0.5948694
R> xx <- x[1:(length(x)-1)] - mean(x)
R> yy <- y[2:length(y)] - mean(y)
R> (c1 <- sum(xx*yy)/n/sqrt(vx*vy))
[1] -0.6676418


The help page of cff() points to MASS, 4ed, the more specific reference
is p 389ff. :)

Cheers,

	Berwin


From mkz@m@n@m @end|ng |rom gm@||@com  Wed Jan 17 04:21:12 2024
From: mkz@m@n@m @end|ng |rom gm@||@com (Md. Kamruzzaman)
Date: Wed, 17 Jan 2024 13:51:12 +1030
Subject: [R] Is there any design based two proportions z test?
Message-ID: <CAGbxoeHn5B=4=JkXUZiXayZLzdhvGPqrdhDfErTEwK2PEuFhQw@mail.gmail.com>

Hello Everyone,
I was analysing big survey data using survey packages on RStudio. Survey
package allows survey data analysis with the design effect.The survey
package included functions for all other statistical analysis except
two-proportion z tests.

I was trying to calculate the difference in prevalence of Diabetes and
Prediabetes between the year 2011 and 2017 (with 95%CI). I was able to
calculate the weighted prevalence of diabetes and prediabetes in the Year
2011 and 2017 and just subtracted the prevalence of 2011 from the
prevalence of 2017 to get the difference in prevalence. But I could not
calculate the 95%CI of the difference in prevalence considering the weight
of the survey data.

I was also trying to see if this difference in prevalence is statistically
significant. I could do it using the simple two-proportion z test without
considering the weight of the sample. But I want to do it considering the
weight of the sample.


Example: Prevalence of Diabetes:
                                                     2011: 11.0 (95%CI
10.1-11.9)
                                                     2017: 10.1 (95%CI
9.4-10.9)
                                                     Diff: 0.9% (95%CI: ??)
                                                     Proportion Z test P
Value: ??
Your cooperation will be highly appreciated.

Thanks in advance.

With Regards

*--------------------------------*

*Md Kamruzzaman*

*PhD **Research Fellow (**Medicine**)*
Discipline of Medicine and Centre of Research Excellence in Translating
Nutritional Science to Good Health
Adelaide Medical School | Faculty of Health and Medical Sciences
The University of Adelaide
Adelaide SA 5005

	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Wed Jan 17 15:14:39 2024
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Wed, 17 Jan 2024 09:14:39 -0500
Subject: [R] Is there any design based two proportions z test?
In-Reply-To: <CAGbxoeHn5B=4=JkXUZiXayZLzdhvGPqrdhDfErTEwK2PEuFhQw@mail.gmail.com>
References: <CAGbxoeHn5B=4=JkXUZiXayZLzdhvGPqrdhDfErTEwK2PEuFhQw@mail.gmail.com>
Message-ID: <77544b52-ebfb-409c-8b45-507b4ecf201a@mcmaster.ca>

Dear Md Kamruzzaman,

To answer your second question first, you could just use the svychisq() 
function. The difference-of-proportion test is equivalent to a chisquare 
test for the 2-by-2 table.

You don't say how you computed the confidence intervals for the two 
separate proportions, but if you have their standard errors (and if not, 
you should be able to infer them from the confidence intervals) you can 
compute the variance of the difference as the sum of the variances 
(squared standard errors), because the two proportions are independent, 
and from that the confidence interval for their difference.

I hope this helps,
John
-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://www.john-fox.ca/

On 2024-01-16 10:21 p.m., Md. Kamruzzaman wrote:
> [You don't often get email from mkzaman.m at gmail.com. Learn why this is important at https://aka.ms/LearnAboutSenderIdentification ]
> 
> Caution: External email.
> 
> 
> Hello Everyone,
> I was analysing big survey data using survey packages on RStudio. Survey
> package allows survey data analysis with the design effect.The survey
> package included functions for all other statistical analysis except
> two-proportion z tests.
> 
> I was trying to calculate the difference in prevalence of Diabetes and
> Prediabetes between the year 2011 and 2017 (with 95%CI). I was able to
> calculate the weighted prevalence of diabetes and prediabetes in the Year
> 2011 and 2017 and just subtracted the prevalence of 2011 from the
> prevalence of 2017 to get the difference in prevalence. But I could not
> calculate the 95%CI of the difference in prevalence considering the weight
> of the survey data.
> 
> I was also trying to see if this difference in prevalence is statistically
> significant. I could do it using the simple two-proportion z test without
> considering the weight of the sample. But I want to do it considering the
> weight of the sample.
> 
> 
> Example: Prevalence of Diabetes:
>                                                       2011: 11.0 (95%CI
> 10.1-11.9)
>                                                       2017: 10.1 (95%CI
> 9.4-10.9)
>                                                       Diff: 0.9% (95%CI: ??)
>                                                       Proportion Z test P
> Value: ??
> Your cooperation will be highly appreciated.
> 
> Thanks in advance.
> 
> With Regards
> 
> *--------------------------------*
> 
> *Md Kamruzzaman*
> 
> *PhD **Research Fellow (**Medicine**)*
> Discipline of Medicine and Centre of Research Excellence in Translating
> Nutritional Science to Good Health
> Adelaide Medical School | Faculty of Health and Medical Sciences
> The University of Adelaide
> Adelaide SA 5005
> 
>          [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @jd@m|co @end|ng |rom gm@||@com  Wed Jan 17 19:20:12 2024
From: @jd@m|co @end|ng |rom gm@||@com (Anthony Damico)
Date: Wed, 17 Jan 2024 13:20:12 -0500
Subject: [R] Is there any design based two proportions z test?
In-Reply-To: <77544b52-ebfb-409c-8b45-507b4ecf201a@mcmaster.ca>
References: <CAGbxoeHn5B=4=JkXUZiXayZLzdhvGPqrdhDfErTEwK2PEuFhQw@mail.gmail.com>
 <77544b52-ebfb-409c-8b45-507b4ecf201a@mcmaster.ca>
Message-ID: <CAOwvMDyidPJ8ez_rns++D-kX5KWqxqJZx3Yn-JM6A--QXiGnrg@mail.gmail.com>

hi, this guide to analyzing changes in prevalence rates over time with
complex survey data might also help?  thanks

http://asdfree.com/trend-analysis-of-complex-survey-data.html



On Wed, Jan 17, 2024, 9:15 AM John Fox <jfox at mcmaster.ca> wrote:

> Dear Md Kamruzzaman,
>
> To answer your second question first, you could just use the svychisq()
> function. The difference-of-proportion test is equivalent to a chisquare
> test for the 2-by-2 table.
>
> You don't say how you computed the confidence intervals for the two
> separate proportions, but if you have their standard errors (and if not,
> you should be able to infer them from the confidence intervals) you can
> compute the variance of the difference as the sum of the variances
> (squared standard errors), because the two proportions are independent,
> and from that the confidence interval for their difference.
>
> I hope this helps,
> John
> --
> John Fox, Professor Emeritus
> McMaster University
> Hamilton, Ontario, Canada
> web: https://www.john-fox.ca/
>
> On 2024-01-16 10:21 p.m., Md. Kamruzzaman wrote:
> > [You don't often get email from mkzaman.m at gmail.com. Learn why this is
> important at https://aka.ms/LearnAboutSenderIdentification ]
> >
> > Caution: External email.
> >
> >
> > Hello Everyone,
> > I was analysing big survey data using survey packages on RStudio. Survey
> > package allows survey data analysis with the design effect.The survey
> > package included functions for all other statistical analysis except
> > two-proportion z tests.
> >
> > I was trying to calculate the difference in prevalence of Diabetes and
> > Prediabetes between the year 2011 and 2017 (with 95%CI). I was able to
> > calculate the weighted prevalence of diabetes and prediabetes in the Year
> > 2011 and 2017 and just subtracted the prevalence of 2011 from the
> > prevalence of 2017 to get the difference in prevalence. But I could not
> > calculate the 95%CI of the difference in prevalence considering the
> weight
> > of the survey data.
> >
> > I was also trying to see if this difference in prevalence is
> statistically
> > significant. I could do it using the simple two-proportion z test without
> > considering the weight of the sample. But I want to do it considering the
> > weight of the sample.
> >
> >
> > Example: Prevalence of Diabetes:
> >                                                       2011: 11.0 (95%CI
> > 10.1-11.9)
> >                                                       2017: 10.1 (95%CI
> > 9.4-10.9)
> >                                                       Diff: 0.9% (95%CI:
> ??)
> >                                                       Proportion Z test P
> > Value: ??
> > Your cooperation will be highly appreciated.
> >
> > Thanks in advance.
> >
> > With Regards
> >
> > *--------------------------------*
> >
> > *Md Kamruzzaman*
> >
> > *PhD **Research Fellow (**Medicine**)*
> > Discipline of Medicine and Centre of Research Excellence in Translating
> > Nutritional Science to Good Health
> > Adelaide Medical School | Faculty of Health and Medical Sciences
> > The University of Adelaide
> > Adelaide SA 5005
> >
> >          [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Thu Jan 18 15:44:11 2024
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Thu, 18 Jan 2024 09:44:11 -0500
Subject: [R] Is there any design based two proportions z test?
In-Reply-To: <CAGbxoeGXYbf5hUNOC2DiEJ2TiyDN2otcbfhfckTSk1Ommwhdeg@mail.gmail.com>
References: <CAGbxoeHn5B=4=JkXUZiXayZLzdhvGPqrdhDfErTEwK2PEuFhQw@mail.gmail.com>
 <77544b52-ebfb-409c-8b45-507b4ecf201a@mcmaster.ca>
 <CAGbxoeGXYbf5hUNOC2DiEJ2TiyDN2otcbfhfckTSk1Ommwhdeg@mail.gmail.com>
Message-ID: <09731002-b29d-467e-aa7b-4f9f507b4354@mcmaster.ca>

Dear Md Kamruzzaman,

I've copied this response to the r-help list, where you originally asked 
your question. That way, other people can follow the conversation, if 
they're interested and there will be a record of the solution. Please 
keep r-help in the loop

See below:

On 2024-01-17 9:47 p.m., Md. Kamruzzaman wrote:
> 	
> Caution: External email.
> 
> 
> Dear John
> Thank you so much for your reply.
> 
> I have calculated the 95%CI of the separate?two proportions by using the 
> survey package. The code is given below.
> 
> svyby(~Diabetes_Cate, ~Year, nhc, svymean, na=TRUE)
> 
> Here: nhc is the weighted survey data.
> 
> 
> I understand your point that it is possible?to calculate the 95%CI of 
> the proportional?difference manually.? It is time consuming, that's why 
> I was looking for a function with a design effect to calculate this 
> easily.? I couldn't find this kind of function.
> 
> 
> However, it will be okay for me to calculate this manually, if there are 
> no functions like this.

If you intend to do this computation once, it's not terribly time 
consuming. If you intend to do it repeatedly, you can write a simple 
function to do the calculation, probably in less time than it takes to 
search for one.

> 
> 
> For manual calculation, could you please share the formula? to calculate 
> the 95%CI of proportional difference.

Here's a simple function to compute the confidence interval, assuming 
that the normal distribution is used. The formula is based on the 
elementary result that the variance of the difference of two independent 
random variables is the sum of their variances, plus the observation 
that the width of the confidence interval is 2*z*SE, where z is the 
normal quantile corresponding to the confidence level (e.g., 1.96 for a 
95% CI).

ciDiff <- function(ci1, ci2, level=0.95){
   p1 <- mean(ci1)
   p2 <- mean(ci2)
   z <- qnorm((1 - level)/2, lower.tail=FALSE)
   se1 <- (ci1[2] - ci1[1])/(2*z)
   se2 <- (ci2[2] - ci2[1])/(2*z)
   seDiff <- sqrt(se1^2 + se2^2)
   (p1 - p2) + c(-z, z)*seDiff
}


> 
> Example: Prevalence of Diabetes:
>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?2011:?11.0 (95%CI 
> 10.1-11.9)
>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?2017:?10.1 (95%CI 
> 9.4-10.9)
>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?Diff: 0.9% (95%CI: ??)

These are percentages, not proportions, but you can use either:

 > ciDiff(c(10.1, 11.9), c(9.4, 10.9))
[1] -0.3215375  2.0215375

 > ciDiff(c(.101, .119), c(.094, .109))
[1] -0.003215375  0.020215375

You'll want more significant digits in the inputs to get sufficiently 
precise results.

Since I did this quickly, if I were you I'd check the results manually.

Best,
  John

> With Kind Regards
> 
> -------------------------
> 
> */Md Kamruzzaman/*
> 
> 
> 
> On Thu, Jan 18, 2024 at 12:44?AM John Fox <jfox at mcmaster.ca 
> <mailto:jfox at mcmaster.ca>> wrote:
> 
>     Dear Md Kamruzzaman,
> 
>     To answer your second question first, you could just use the svychisq()
>     function. The difference-of-proportion test is equivalent to a
>     chisquare
>     test for the 2-by-2 table.
> 
>     You don't say how you computed the confidence intervals for the two
>     separate proportions, but if you have their standard errors (and if
>     not,
>     you should be able to infer them from the confidence intervals) you can
>     compute the variance of the difference as the sum of the variances
>     (squared standard errors), because the two proportions are independent,
>     and from that the confidence interval for their difference.
> 
>     I hope this helps,
>     John
>     -- 
>     John Fox, Professor Emeritus
>     McMaster University
>     Hamilton, Ontario, Canada
>     web: https://www.john-fox.ca/ <https://www.john-fox.ca/>
> 
>     On 2024-01-16 10:21 p.m., Md. Kamruzzaman wrote:
>      > [You don't often get email from mkzaman.m at gmail.com
>     <mailto:mkzaman.m at gmail.com>. Learn why this is important at
>     https://aka.ms/LearnAboutSenderIdentification
>     <https://aka.ms/LearnAboutSenderIdentification> ]
>      >
>      > Caution: External email.
>      >
>      >
>      > Hello Everyone,
>      > I was analysing big survey data using survey packages on RStudio.
>     Survey
>      > package allows survey data analysis with the design effect.The survey
>      > package included functions for all other statistical analysis except
>      > two-proportion z tests.
>      >
>      > I was trying to calculate the difference in prevalence of
>     Diabetes and
>      > Prediabetes between the year 2011 and 2017 (with 95%CI). I was
>     able to
>      > calculate the weighted prevalence of diabetes and prediabetes in
>     the Year
>      > 2011 and 2017 and just subtracted the prevalence of 2011 from the
>      > prevalence of 2017 to get the difference in prevalence. But I
>     could not
>      > calculate the 95%CI of the difference in prevalence considering
>     the weight
>      > of the survey data.
>      >
>      > I was also trying to see if this difference in prevalence is
>     statistically
>      > significant. I could do it using the simple two-proportion z test
>     without
>      > considering the weight of the sample. But I want to do it
>     considering the
>      > weight of the sample.
>      >
>      >
>      > Example: Prevalence of Diabetes:
>      >? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?2011: 11.0
>     (95%CI
>      > 10.1-11.9)
>      >? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?2017: 10.1
>     (95%CI
>      > 9.4-10.9)
>      >? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?Diff: 0.9%
>     (95%CI: ??)
>      >? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?Proportion
>     Z test P
>      > Value: ??
>      > Your cooperation will be highly appreciated.
>      >
>      > Thanks in advance.
>      >
>      > With Regards
>      >
>      > *--------------------------------*
>      >
>      > *Md Kamruzzaman*
>      >
>      > *PhD **Research Fellow (**Medicine**)*
>      > Discipline of Medicine and Centre of Research Excellence in
>     Translating
>      > Nutritional Science to Good Health
>      > Adelaide Medical School | Faculty of Health and Medical Sciences
>      > The University of Adelaide
>      > Adelaide SA 5005
>      >
>      >? ? ? ? ? [[alternative HTML version deleted]]
>      >
>      > ______________________________________________
>      > R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>     -- To UNSUBSCRIBE and more, see
>      > https://stat.ethz.ch/mailman/listinfo/r-help
>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>      > PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     <http://www.R-project.org/posting-guide.html>
>      > and provide commented, minimal, self-contained, reproducible code.
>


From mzb@deve| @end|ng |rom gm@||@com  Mon Jan 22 03:06:10 2024
From: mzb@deve| @end|ng |rom gm@||@com (Mauricio Zambrano-Bigiarini)
Date: Sun, 21 Jan 2024 23:06:10 -0300
Subject: [R] [R-pkgs] hydroTSM back on CRAN (v0.7-0 released)
Message-ID: <CAJ44-AkUGpQKQ+8Geuje+Hu8NeemG3DqMyv=jkZVEQKfNH=0uQ@mail.gmail.com>

Dear all,

After being archived on CRAN on 2023-10-1, hydroTSM is finally back on CRAN
since January 18th: https://cran.r-project.org/package=hydroTSM.

This new version 0.7-0 has several new functions, improvements, bugfixes,
and a new dataset, mostly devoted to work with sub-daily and sub-hourly
time series.

*) New functions: baseflow, plot_pq, calendarHeatmap, subhourly2hourly,
subhourly2nminutes, daily2weekly, subdaily2weekly, cmv, si.

*) Improvements: climograph, (sub)daily2monthly, subdaily2daily,
daily2annual, daily2monthly, subdaily2annual, hip, hydroplot, sname2plot,
izoo2rzoo.

A detailed description of the previous changes can be found in the new
News.md file:
https://cran.r-project.org/web/packages/hydroTSM/news/news.html

Two vignettes illustrate the usage of the some of the main functions:

1) Analysis of daily precipitation data:
https://cloud.r-project.org/web/packages/hydroTSM/vignettes/hydroTSM_Daily_P_Vignette-knitr.pdf

2) 1) Analysis of daily streamflow data:
https://cloud.r-project.org/web/packages/hydroTSM/vignettes/hydroTSM_Daily_Q_Vignette-knitr.pdf

If you use this package, please cite it correctly as:

Zambrano-Bigiarini, Mauricio (2024). hydroTSM: Time Series Management and
Analysis for Hydrological Modelling. R package version 0.7-0. URL:
https://cran.r-project.org/package=hydroTSM. doi:110.5281/zenodo.839565

Issues and enhancements can be requested on:
https://github.com/hzambran/hydroTSM/issues

All the best,

Mauricio


Mauricio Zambrano-Bigiarini, PhD
Associate Professor, Universidad de La Frontera
Associate Researcher, (CR)2 FONDAP Center
Phone: +56 45 259 2812 <+56+45+259+2812>
e-mail: mauricio.zambrano at ufrontera.cl
[image: webpage] <https://hzambran.github.io/> [image: ORCID]
<https://orcid.org/0000-0002-9536-643X> [image: Github]
<https://github.com/hzambran> [image: Linkedin]
<http://www.linkedin.com/in/hzambran/>

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From mzb@deve| @end|ng |rom gm@||@com  Mon Jan 22 06:30:00 2024
From: mzb@deve| @end|ng |rom gm@||@com (Mauricio Zambrano-Bigiarini)
Date: Mon, 22 Jan 2024 02:30:00 -0300
Subject: [R] [R-pkgs] hydroGOF back on CRAN (v0.5-4 released)
Message-ID: <CAJ44-Amtui+ZZCtM4JRMMLJW3d3EkUkBkbVx=b=epWep9v_C=A@mail.gmail.com>

Dear all,

After being archived on CRAN on  2023-10-16 , hydroGOF is finally back on
CRAN since January 21th: https://cran.r-project.org/package=hydroGOF.

This new version 0.5-4 includes:

*) the following new functions:

    -) KGElf (Garc?a et al., 2017),
    -) sKGE (Fowler et al., 2018),
    -) KGEnp (Pool et al., 2018),
    -) dr (Willmott et al., 2012),
    -) ubRMSE (Entekhabi et al., 2010),
    -) wNSE (Hundecha and Bardossy, 2004),
    -) rSpearman,
    -) R2

*) improvements: all the functions allow you to apply a user-defined
function (e.g., log, sqrt) to both 'sim' and 'obs' before computing the
goodness-of-fit measure.

A detailed description of the previous changes can be found for the 0.5-0
version in the new News.md file:
https://cran.r-project.org/web/packages/hydroGOF/news/news.html

A much improved vignette illustrate the usage of some of the main functions:

*) Analysis of daily precipitation data:
https://cloud.r-project.org/web/packages/hydroGOF/vignettes/hydroGOF_Vignette.pdf

If you use this package, please cite it correctly as:

Zambrano-Bigiarini, Mauricio (2024). hydroGOF: Goodness-of-fit functions
for comparison of simulated and observed hydrological time series. R
package version 0.5-4. URL:https://cran.r-project.org/package=hydroGOF.
doi:10.5281/zenodo.839854.

Issues and enhancements can be requested on:
https://github.com/hzambran/hydroGOF/issues

All the best,

Mauricio


Mauricio Zambrano-Bigiarini, PhD
Associate Professor, Universidad de La Frontera
Associate Researcher, (CR)2 FONDAP Center
Phone: +56 45 259 2812 <+56+45+259+2812>
e-mail: mauricio.zambrano at ufrontera.cl
[image: webpage] <http://hzambran.github.io/> [image: ORCID]
<https://orcid.org/0000-0002-9536-643X> [image: Github]
<https://github.com/hzambran> [image: Linkedin]
<http://www.linkedin.com/in/hzambran/>

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Mon Jan 22 16:45:31 2024
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Mon, 22 Jan 2024 07:45:31 -0800 (PST)
Subject: [R] Use of geometric mean for geochemical concentrations
Message-ID: <b77a1d4b-6e6f-87e7-aea0-5315a180fd8e@appl-ecosys.com>

A statistical question, not specific to R.

I'm asking for a pointer for a source of definitive descriptions of what
types of data are best summarized by the arithmetic, geometric, and harmonic
means.

As an aquatic ecologist I see regulators apply the geometric mean to
geochemical concentrations rather than using the arithmetic mean. I want to
know whether the geometric mean of a set of chemical concentrations (e.g.,
in mg/L) is an appropriate representation of the expected value. If not, I
want to explain this to non-technical decision-makers; if so, I want to
understand why my assumption is wrong.

TIA,

Rich


From bgunter@4567 @end|ng |rom gm@||@com  Mon Jan 22 16:48:52 2024
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 22 Jan 2024 07:48:52 -0800
Subject: [R] Use of geometric mean for geochemical concentrations
In-Reply-To: <b77a1d4b-6e6f-87e7-aea0-5315a180fd8e@appl-ecosys.com>
References: <b77a1d4b-6e6f-87e7-aea0-5315a180fd8e@appl-ecosys.com>
Message-ID: <CAGxFJbQxqixOWK9hQgGnuYCrgyuT9zj+dtBDYxFp3ERbuWMwGQ@mail.gmail.com>

better posted on r-sig-ecology? -- or maybe even stack exchange?

Cheers,
Bert

On Mon, Jan 22, 2024 at 7:45?AM Rich Shepard <rshepard at appl-ecosys.com>
wrote:

> A statistical question, not specific to R.
>
> I'm asking for a pointer for a source of definitive descriptions of what
> types of data are best summarized by the arithmetic, geometric, and
> harmonic
> means.
>
> As an aquatic ecologist I see regulators apply the geometric mean to
> geochemical concentrations rather than using the arithmetic mean. I want to
> know whether the geometric mean of a set of chemical concentrations (e.g.,
> in mg/L) is an appropriate representation of the expected value. If not, I
> want to explain this to non-technical decision-makers; if so, I want to
> understand why my assumption is wrong.
>
> TIA,
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Mon Jan 22 17:05:19 2024
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Mon, 22 Jan 2024 08:05:19 -0800 (PST)
Subject: [R] Use of geometric mean for geochemical concentrations
In-Reply-To: <CAGxFJbQxqixOWK9hQgGnuYCrgyuT9zj+dtBDYxFp3ERbuWMwGQ@mail.gmail.com>
References: <b77a1d4b-6e6f-87e7-aea0-5315a180fd8e@appl-ecosys.com>
 <CAGxFJbQxqixOWK9hQgGnuYCrgyuT9zj+dtBDYxFp3ERbuWMwGQ@mail.gmail.com>
Message-ID: <b2fbcfc7-9d4f-c52b-6db1-bcfa3774ef7c@appl-ecosys.com>

On Mon, 22 Jan 2024, Bert Gunter wrote:

> better posted on r-sig-ecology? -- or maybe even stack exchange?

Bert,

Okay.

Regards,

Rich


From bbo|ker @end|ng |rom gm@||@com  Mon Jan 22 17:28:52 2024
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 22 Jan 2024 11:28:52 -0500
Subject: [R] Use of geometric mean for geochemical concentrations
In-Reply-To: <b2fbcfc7-9d4f-c52b-6db1-bcfa3774ef7c@appl-ecosys.com>
References: <b77a1d4b-6e6f-87e7-aea0-5315a180fd8e@appl-ecosys.com>
 <CAGxFJbQxqixOWK9hQgGnuYCrgyuT9zj+dtBDYxFp3ERbuWMwGQ@mail.gmail.com>
 <b2fbcfc7-9d4f-c52b-6db1-bcfa3774ef7c@appl-ecosys.com>
Message-ID: <91753de6-347d-49d1-895a-2e601e704f18@gmail.com>

   I think https://stats.stackexchange.com would be best: r-sig-ecology 
is pretty quiet these days

On 2024-01-22 11:05 a.m., Rich Shepard wrote:
> On Mon, 22 Jan 2024, Bert Gunter wrote:
> 
>> better posted on r-sig-ecology? -- or maybe even stack exchange?
> 
> Bert,
> 
> Okay.
> 
> Regards,
> 
> Rich
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Mon Jan 22 17:31:30 2024
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Mon, 22 Jan 2024 08:31:30 -0800 (PST)
Subject: [R] Use of geometric mean for geochemical concentrations
In-Reply-To: <91753de6-347d-49d1-895a-2e601e704f18@gmail.com>
References: <b77a1d4b-6e6f-87e7-aea0-5315a180fd8e@appl-ecosys.com>
 <CAGxFJbQxqixOWK9hQgGnuYCrgyuT9zj+dtBDYxFp3ERbuWMwGQ@mail.gmail.com>
 <b2fbcfc7-9d4f-c52b-6db1-bcfa3774ef7c@appl-ecosys.com>
 <91753de6-347d-49d1-895a-2e601e704f18@gmail.com>
Message-ID: <72ff7657-438-68f3-8bf8-d6bd23132e29@appl-ecosys.com>

On Mon, 22 Jan 2024, Ben Bolker wrote:

>  I think https://stats.stackexchange.com would be best: r-sig-ecology is 
> pretty quiet these days

Okay, Ben.

Thanks,

Rich


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Mon Jan 22 18:18:36 2024
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Mon, 22 Jan 2024 18:18:36 +0100
Subject: [R] Use of geometric mean .. in good data analysis
In-Reply-To: <b77a1d4b-6e6f-87e7-aea0-5315a180fd8e@appl-ecosys.com>
References: <b77a1d4b-6e6f-87e7-aea0-5315a180fd8e@appl-ecosys.com>
Message-ID: <26030.41836.34478.66358@stat.math.ethz.ch>

>>>>> Rich Shepard 
>>>>>     on Mon, 22 Jan 2024 07:45:31 -0800 (PST) writes:

    > A statistical question, not specific to R.  I'm asking for
    > a pointer for a source of definitive descriptions of what
    > types of data are best summarized by the arithmetic,
    > geometric, and harmonic means.

In spite of  off-topic:

I think it is a good question, not really only about
geo-chemistry, but about statistics in applied sciences (and
engineering for that matter).

Something I sure good applied statisticians in the 1980's and
1990's would all know the answer of :

To use the geometric mean instead of the arithmetic mean
is basically  *equivalent* to  first log-transform the data
and then work with that transformed data:
Not just for computing average, but for more relevant modelling,
inference, etc.

John W Tukey (and several other of the grands of the time)
had the log transform among the  "First aid transformations":

If the data for a continuous variable must all be positive it is
also typically the case that the distribution is considerably
skewed to the right.
In such a case behave as a good human who sees another human in
health distress: apply First Aid -- do the things you learned to
do quickly without too much thought, because things must happen
fast ---to hopefully save the other's life.

Here: Do log transform all such variables with further ado,
and only afterwards start your (exploratory and more) data analysis.

Now,  mean(log(y)) = log(geometricmean(y)), 
where mean() is the arithmetic mean as in R
{mathematically; on the computer you need all.equal(), not '==' !!}

I.e., according to Tukey and all the other experienced applied
statisticians of the past, the geometric mean is the "best thing" 
to do for such positive right-skewed data   in the same sense
that the log-transform is the best "a priori" transformation for
such data -- with the one advantage even that you need to fiddle
with zeroes when log-transforming, whereas the geometric mean
works already for zeroes.

Martin


    > As an aquatic ecologist I see regulators apply the
    > geometric mean to geochemical concentrations rather than
    > using the arithmetic mean. I want to know whether the
    > geometric mean of a set of chemical concentrations (e.g.,
    > in mg/L) is an appropriate representation of the expected
    > value. If not, I want to explain this to non-technical
    > decision-makers; if so, I want to understand why my
    > assumption is wrong.

    > TIA,

    > Rich

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    > more, see https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide
    > http://www.R-project.org/posting-guide.html and provide
    > commented, minimal, self-contained, reproducible code.


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Mon Jan 22 18:28:53 2024
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Mon, 22 Jan 2024 09:28:53 -0800 (PST)
Subject: [R] Use of geometric mean .. in good data analysis
In-Reply-To: <26030.41836.34478.66358@stat.math.ethz.ch>
References: <b77a1d4b-6e6f-87e7-aea0-5315a180fd8e@appl-ecosys.com>
 <26030.41836.34478.66358@stat.math.ethz.ch>
Message-ID: <6c6baf6e-8ac1-a3cf-3474-1c1a44df5618@appl-ecosys.com>

On Mon, 22 Jan 2024, Martin Maechler wrote:

> I think it is a good question, not really only about geo-chemistry, but
> about statistics in applied sciences (and engineering for that matter).

> John W Tukey (and several other of the grands of the time) had the log
> transform among the "First aid transformations":
>
> If the data for a continuous variable must all be positive it is also
> typically the case that the distribution is considerably skewed to the
> right. In such a case behave as a good human who sees another human in
> health distress: apply First Aid -- do the things you learned to do
> quickly without too much thought, because things must happen fast ---to
> hopefully save the other's life.

Martin,

Thanks very much. I will look further into this because toxic metals and
organic compounds in geochemical collections almost always have censored lab
results (below method dection limits) that range from about 15% to 80% or
more, and there almost always are very high extreme values.

I'll learn to understand what benefits log transforms have over
compositional data analyses.

Best regards,

Rich


From j|ox @end|ng |rom mcm@@ter@c@  Mon Jan 22 18:36:40 2024
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Mon, 22 Jan 2024 12:36:40 -0500
Subject: [R] Use of geometric mean .. in good data analysis
In-Reply-To: <26030.41836.34478.66358@stat.math.ethz.ch>
References: <b77a1d4b-6e6f-87e7-aea0-5315a180fd8e@appl-ecosys.com>
 <26030.41836.34478.66358@stat.math.ethz.ch>
Message-ID: <6374eada-c18b-4e4f-a49c-5eeebbd555fe@mcmaster.ca>

Dear Martin,

Helpful general advice, although it's perhaps worth mentioning that the 
geometric mean, defined e.g. naively as prod(x)^(1/length(x)), is 
necessarily 0 if there are any 0 values in x. That is, the geometric 
mean "works" in this case but isn't really informative.

Best,
  John
-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://www.john-fox.ca/

On 2024-01-22 12:18 p.m., Martin Maechler wrote:
> Caution: External email.
> 
> 
>>>>>> Rich Shepard
>>>>>>      on Mon, 22 Jan 2024 07:45:31 -0800 (PST) writes:
> 
>      > A statistical question, not specific to R.  I'm asking for
>      > a pointer for a source of definitive descriptions of what
>      > types of data are best summarized by the arithmetic,
>      > geometric, and harmonic means.
> 
> In spite of  off-topic:
> 
> I think it is a good question, not really only about
> geo-chemistry, but about statistics in applied sciences (and
> engineering for that matter).
> 
> Something I sure good applied statisticians in the 1980's and
> 1990's would all know the answer of :
> 
> To use the geometric mean instead of the arithmetic mean
> is basically  *equivalent* to  first log-transform the data
> and then work with that transformed data:
> Not just for computing average, but for more relevant modelling,
> inference, etc.
> 
> John W Tukey (and several other of the grands of the time)
> had the log transform among the  "First aid transformations":
> 
> If the data for a continuous variable must all be positive it is
> also typically the case that the distribution is considerably
> skewed to the right.
> In such a case behave as a good human who sees another human in
> health distress: apply First Aid -- do the things you learned to
> do quickly without too much thought, because things must happen
> fast ---to hopefully save the other's life.
> 
> Here: Do log transform all such variables with further ado,
> and only afterwards start your (exploratory and more) data analysis.
> 
> Now,  mean(log(y)) = log(geometricmean(y)),
> where mean() is the arithmetic mean as in R
> {mathematically; on the computer you need all.equal(), not '==' !!}
> 
> I.e., according to Tukey and all the other experienced applied
> statisticians of the past, the geometric mean is the "best thing"
> to do for such positive right-skewed data   in the same sense
> that the log-transform is the best "a priori" transformation for
> such data -- with the one advantage even that you need to fiddle
> with zeroes when log-transforming, whereas the geometric mean
> works already for zeroes.
> 
> Martin
> 
> 
>      > As an aquatic ecologist I see regulators apply the
>      > geometric mean to geochemical concentrations rather than
>      > using the arithmetic mean. I want to know whether the
>      > geometric mean of a set of chemical concentrations (e.g.,
>      > in mg/L) is an appropriate representation of the expected
>      > value. If not, I want to explain this to non-technical
>      > decision-makers; if so, I want to understand why my
>      > assumption is wrong.
> 
>      > TIA,
> 
>      > Rich
> 
>      > ______________________________________________
>      > R-help at r-project.org mailing list -- To UNSUBSCRIBE and
>      > more, see https://stat.ethz.ch/mailman/listinfo/r-help
>      > PLEASE do read the posting guide
>      > http://www.R-project.org/posting-guide.html and provide
>      > commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Mon Jan 22 21:23:20 2024
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 22 Jan 2024 12:23:20 -0800
Subject: [R] Use of geometric mean .. in good data analysis
In-Reply-To: <6c6baf6e-8ac1-a3cf-3474-1c1a44df5618@appl-ecosys.com>
References: <b77a1d4b-6e6f-87e7-aea0-5315a180fd8e@appl-ecosys.com>
 <26030.41836.34478.66358@stat.math.ethz.ch>
 <6c6baf6e-8ac1-a3cf-3474-1c1a44df5618@appl-ecosys.com>
Message-ID: <CAGxFJbSTkYzZUmexoWYRKsFX+0VgJQdAAPYvGqwLeKdrK7mQgg@mail.gmail.com>

Ah.... LOD's, typically LLOD's ("lower limits of detection").

Disclaimer: I am *NOT* in any sense an expert on such matters. What follows
are just some comments based on my personal experience. Please filter
accordingly. Also, while I kept it on list as Martin suggested it might be
useful to do so, most folks probably can safely ignore the rant that
follows as off topic and not of interest. So you've been warned!!

The rant:
My experience is: data that contain a "bunch" of values that are, e.g.
below a LLOD, are frequently reported and/or analyzed by various ad hoc,
and imho, uniformly bad methods. e.g.:

1) The censored values are recorded and analyzed as at the LLOD;
2) The censored values are recorded and analyzed at some arbitrary value
below the LLOD, like LLOD/2;
3) The censored values are are "imputed" by ad hoc methods, e.g. uniform
random values between 0 and the LLOD for left censoring.

To repeat, *IMO*, all of this is junk and will produced misleading
statistical results. Whether they mislead enough to substantively affect
the science or regulatory decisions depend on the specifics of the
circumstances. I accept no general claim as to their innocuousness.

Further:

a) When you have a "lot" of values -- 50%? 75%?, 25%? -- face facts: you
have (practically) no useful information from the values that you do have
to infer what the distribution of values that you don't have looks like.
All one can sensibly do is say that x% of the values are below a LOD and
here's the distribution of what lies above. Presumably, if you have such
data conditional on covariates with the obvious intent to determine the
relationship to those covariates, you could analyze the percentages of
LLOD's and known values separately. There are undoubtedly more
sophisticated methods out there, so this is where you need to go to the
literature to see what might suit; though I think it will still have to
come down to looking at these separately (e.g. with extra parameters to
account for unmeasurable values). Another way of saying this is: any
analysis which treats all the data as arising from a single distribution
will depend more on the assumptions you make than on the data. So good luck
with that!

b) If you have a "modest" amount of (known) censoring -- 5%?, 20%? 10%? --
methods for the analysis of censored data should be useful. My
understanding is that MI (multiple imputation) is regarded as a generally
useful approach, and there are many R packages that can do various flavors
of this. Again, you should consult the literature: there are very likely
nontechnical reviews of this topic, too, as well as online discussions and
tutorials.

So if you are serious about dealing with this and have a lot of data with
these issues, my advice would be to stop looking for ad hoc advice and dig
into the literature: it's one of the many areas of "data science" where
seemingly simple but pervasive questions require complex answers.

And, again, heed my personal caveats.

Thus endeth my rant.

Cheers to all,
Bert



On Mon, Jan 22, 2024 at 9:29?AM Rich Shepard <rshepard at appl-ecosys.com>
wrote:

> On Mon, 22 Jan 2024, Martin Maechler wrote:
>
> > I think it is a good question, not really only about geo-chemistry, but
> > about statistics in applied sciences (and engineering for that matter).
>
> > John W Tukey (and several other of the grands of the time) had the log
> > transform among the "First aid transformations":
> >
> > If the data for a continuous variable must all be positive it is also
> > typically the case that the distribution is considerably skewed to the
> > right. In such a case behave as a good human who sees another human in
> > health distress: apply First Aid -- do the things you learned to do
> > quickly without too much thought, because things must happen fast ---to
> > hopefully save the other's life.
>
> Martin,
>
> Thanks very much. I will look further into this because toxic metals and
> organic compounds in geochemical collections almost always have censored
> lab
> results (below method dection limits) that range from about 15% to 80% or
> more, and there almost always are very high extreme values.
>
> I'll learn to understand what benefits log transforms have over
> compositional data analyses.
>
> Best regards,
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Mon Jan 22 21:47:53 2024
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 22 Jan 2024 12:47:53 -0800
Subject: [R] Use of geometric mean .. in good data analysis
In-Reply-To: <CAGxFJbSTkYzZUmexoWYRKsFX+0VgJQdAAPYvGqwLeKdrK7mQgg@mail.gmail.com>
References: <b77a1d4b-6e6f-87e7-aea0-5315a180fd8e@appl-ecosys.com>
 <26030.41836.34478.66358@stat.math.ethz.ch>
 <6c6baf6e-8ac1-a3cf-3474-1c1a44df5618@appl-ecosys.com>
 <CAGxFJbSTkYzZUmexoWYRKsFX+0VgJQdAAPYvGqwLeKdrK7mQgg@mail.gmail.com>
Message-ID: <CAGxFJbTuhAWb8JO1CkXO+6f3diqb3AEf_bxmGJOgCG+qoW7+mQ@mail.gmail.com>

In the spirit of Martin's comments, it is perhaps worthwhile to note one of
John Tukey's (who I actually knew) pertinent quotes:
"The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
<https://www.azquotes.com/quote/603406>"

"Sunset Salvo" by John Tukey in The American Statistician, Volume 40, No. 1
(pp. 72-76), www.jstor.org. February 1986.

Cheers,
Bert

<https://www.azquotes.com/author/14847-John_Tukey>

On Mon, Jan 22, 2024 at 12:23?PM Bert Gunter <bgunter.4567 at gmail.com> wrote:

>
> Ah.... LOD's, typically LLOD's ("lower limits of detection").
>
> Disclaimer: I am *NOT* in any sense an expert on such matters. What
> follows are just some comments based on my personal experience. Please
> filter accordingly. Also, while I kept it on list as Martin suggested it
> might be useful to do so, most folks probably can safely ignore the rant
> that follows as off topic and not of interest. So you've been warned!!
>
> The rant:
> My experience is: data that contain a "bunch" of values that are, e.g.
> below a LLOD, are frequently reported and/or analyzed by various ad hoc,
> and imho, uniformly bad methods. e.g.:
>
> 1) The censored values are recorded and analyzed as at the LLOD;
> 2) The censored values are recorded and analyzed at some arbitrary value
> below the LLOD, like LLOD/2;
> 3) The censored values are are "imputed" by ad hoc methods, e.g. uniform
> random values between 0 and the LLOD for left censoring.
>
> To repeat, *IMO*, all of this is junk and will produced misleading
> statistical results. Whether they mislead enough to substantively affect
> the science or regulatory decisions depend on the specifics of the
> circumstances. I accept no general claim as to their innocuousness.
>
> Further:
>
> a) When you have a "lot" of values -- 50%? 75%?, 25%? -- face facts: you
> have (practically) no useful information from the values that you do have
> to infer what the distribution of values that you don't have looks like.
> All one can sensibly do is say that x% of the values are below a LOD and
> here's the distribution of what lies above. Presumably, if you have such
> data conditional on covariates with the obvious intent to determine the
> relationship to those covariates, you could analyze the percentages of
> LLOD's and known values separately. There are undoubtedly more
> sophisticated methods out there, so this is where you need to go to the
> literature to see what might suit; though I think it will still have to
> come down to looking at these separately (e.g. with extra parameters to
> account for unmeasurable values). Another way of saying this is: any
> analysis which treats all the data as arising from a single distribution
> will depend more on the assumptions you make than on the data. So good luck
> with that!
>
> b) If you have a "modest" amount of (known) censoring -- 5%?, 20%? 10%? --
> methods for the analysis of censored data should be useful. My
> understanding is that MI (multiple imputation) is regarded as a generally
> useful approach, and there are many R packages that can do various flavors
> of this. Again, you should consult the literature: there are very likely
> nontechnical reviews of this topic, too, as well as online discussions and
> tutorials.
>
> So if you are serious about dealing with this and have a lot of data with
> these issues, my advice would be to stop looking for ad hoc advice and dig
> into the literature: it's one of the many areas of "data science" where
> seemingly simple but pervasive questions require complex answers.
>
> And, again, heed my personal caveats.
>
> Thus endeth my rant.
>
> Cheers to all,
> Bert
>
>
>
> On Mon, Jan 22, 2024 at 9:29?AM Rich Shepard <rshepard at appl-ecosys.com>
> wrote:
>
>> On Mon, 22 Jan 2024, Martin Maechler wrote:
>>
>> > I think it is a good question, not really only about geo-chemistry, but
>> > about statistics in applied sciences (and engineering for that matter).
>>
>> > John W Tukey (and several other of the grands of the time) had the log
>> > transform among the "First aid transformations":
>> >
>> > If the data for a continuous variable must all be positive it is also
>> > typically the case that the distribution is considerably skewed to the
>> > right. In such a case behave as a good human who sees another human in
>> > health distress: apply First Aid -- do the things you learned to do
>> > quickly without too much thought, because things must happen fast ---to
>> > hopefully save the other's life.
>>
>> Martin,
>>
>> Thanks very much. I will look further into this because toxic metals and
>> organic compounds in geochemical collections almost always have censored
>> lab
>> results (below method dection limits) that range from about 15% to 80% or
>> more, and there almost always are very high extreme values.
>>
>> I'll learn to understand what benefits log transforms have over
>> compositional data analyses.
>>
>> Best regards,
>>
>> Rich
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon Jan 22 21:57:34 2024
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 22 Jan 2024 12:57:34 -0800
Subject: [R] Use of geometric mean .. in good data analysis
In-Reply-To: <CAGxFJbSTkYzZUmexoWYRKsFX+0VgJQdAAPYvGqwLeKdrK7mQgg@mail.gmail.com>
References: <b77a1d4b-6e6f-87e7-aea0-5315a180fd8e@appl-ecosys.com>
 <26030.41836.34478.66358@stat.math.ethz.ch>
 <6c6baf6e-8ac1-a3cf-3474-1c1a44df5618@appl-ecosys.com>
 <CAGxFJbSTkYzZUmexoWYRKsFX+0VgJQdAAPYvGqwLeKdrK7mQgg@mail.gmail.com>
Message-ID: <FF9906E9-E97A-4300-B1A5-D52CEF8FA15D@dcn.davis.ca.us>

Still OT... but here is my own (I think previously mentioned here) rant on people thrashing about with log transformation and an all-too-common kludge to deal with zeros mixed among small numbers... https://gist.github.com/jdnewmil/99301a88de702ad2fcbaef33326b08b4

OP perhaps posting a link here to your question posed wherever you end up with it will help shorten this thread.

On January 22, 2024 12:23:20 PM PST, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>Ah.... LOD's, typically LLOD's ("lower limits of detection").
>
>Disclaimer: I am *NOT* in any sense an expert on such matters. What follows
>are just some comments based on my personal experience. Please filter
>accordingly. Also, while I kept it on list as Martin suggested it might be
>useful to do so, most folks probably can safely ignore the rant that
>follows as off topic and not of interest. So you've been warned!!
>
>The rant:
>My experience is: data that contain a "bunch" of values that are, e.g.
>below a LLOD, are frequently reported and/or analyzed by various ad hoc,
>and imho, uniformly bad methods. e.g.:
>
>1) The censored values are recorded and analyzed as at the LLOD;
>2) The censored values are recorded and analyzed at some arbitrary value
>below the LLOD, like LLOD/2;
>3) The censored values are are "imputed" by ad hoc methods, e.g. uniform
>random values between 0 and the LLOD for left censoring.
>
>To repeat, *IMO*, all of this is junk and will produced misleading
>statistical results. Whether they mislead enough to substantively affect
>the science or regulatory decisions depend on the specifics of the
>circumstances. I accept no general claim as to their innocuousness.
>
>Further:
>
>a) When you have a "lot" of values -- 50%? 75%?, 25%? -- face facts: you
>have (practically) no useful information from the values that you do have
>to infer what the distribution of values that you don't have looks like.
>All one can sensibly do is say that x% of the values are below a LOD and
>here's the distribution of what lies above. Presumably, if you have such
>data conditional on covariates with the obvious intent to determine the
>relationship to those covariates, you could analyze the percentages of
>LLOD's and known values separately. There are undoubtedly more
>sophisticated methods out there, so this is where you need to go to the
>literature to see what might suit; though I think it will still have to
>come down to looking at these separately (e.g. with extra parameters to
>account for unmeasurable values). Another way of saying this is: any
>analysis which treats all the data as arising from a single distribution
>will depend more on the assumptions you make than on the data. So good luck
>with that!
>
>b) If you have a "modest" amount of (known) censoring -- 5%?, 20%? 10%? --
>methods for the analysis of censored data should be useful. My
>understanding is that MI (multiple imputation) is regarded as a generally
>useful approach, and there are many R packages that can do various flavors
>of this. Again, you should consult the literature: there are very likely
>nontechnical reviews of this topic, too, as well as online discussions and
>tutorials.
>
>So if you are serious about dealing with this and have a lot of data with
>these issues, my advice would be to stop looking for ad hoc advice and dig
>into the literature: it's one of the many areas of "data science" where
>seemingly simple but pervasive questions require complex answers.
>
>And, again, heed my personal caveats.
>
>Thus endeth my rant.
>
>Cheers to all,
>Bert
>
>
>
>On Mon, Jan 22, 2024 at 9:29?AM Rich Shepard <rshepard at appl-ecosys.com>
>wrote:
>
>> On Mon, 22 Jan 2024, Martin Maechler wrote:
>>
>> > I think it is a good question, not really only about geo-chemistry, but
>> > about statistics in applied sciences (and engineering for that matter).
>>
>> > John W Tukey (and several other of the grands of the time) had the log
>> > transform among the "First aid transformations":
>> >
>> > If the data for a continuous variable must all be positive it is also
>> > typically the case that the distribution is considerably skewed to the
>> > right. In such a case behave as a good human who sees another human in
>> > health distress: apply First Aid -- do the things you learned to do
>> > quickly without too much thought, because things must happen fast ---to
>> > hopefully save the other's life.
>>
>> Martin,
>>
>> Thanks very much. I will look further into this because toxic metals and
>> organic compounds in geochemical collections almost always have censored
>> lab
>> results (below method dection limits) that range from about 15% to 80% or
>> more, and there almost always are very high extreme values.
>>
>> I'll learn to understand what benefits log transforms have over
>> compositional data analyses.
>>
>> Best regards,
>>
>> Rich
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @pyqqqd|@ @end|ng |rom y@hoo@com  Tue Jan 23 16:53:50 2024
From: @pyqqqd|@ @end|ng |rom y@hoo@com (Michael Meyer)
Date: Tue, 23 Jan 2024 15:53:50 +0000 (UTC)
Subject: [R] Quantiles of sums of independent discrete random variables
References: <1405340866.1174038.1706025230256.ref@mail.yahoo.com>
Message-ID: <1405340866.1174038.1706025230256@mail.yahoo.com>

Greetings,
I have the following?
Problem:
 Given k (=10) discrete independent random variables X_i with n_i (= 5 to 20) values each,compute quantiles of the distribution of the sum X = X_1+...+X_k.

Here X has n=n_1 x n_2 ... n_k distinct values which is too large to list them all together with
their probabilities.

I tried several approaches: 

(A) Convolution: 

each X_j is approximated with Y_j=X_j+Z, where Z is
an N(0,sigma) variable with small sigma. Then Y_j is a probability mixture of the normal
variables N(x_j,sigma), where the x_j runs over all values of X_j, and has a highly oscillatory density.

The density of Y=\sum Y_j is the convolution of the densities of the Y_j.

I need this density at a sizeable number of points and this turns out to be too slow.
The issue seems to be the convergence of the convolution integrals slowed down by the oscillatory nature
of the densities of the Y_j. When the densities are behaved better (e.g. normal RVs), the computation 
of such a convolution is quite fast. 

(B) Characteristic function: 

X will be approximated with Y=X+Z, where Z is normal N(0,sigma) with small sigma.
Y has a density (which it is impossible to compute directly) but the characteristic function 
(_continuous_ Fourier transform) cf_Y of Y can easily be computed analytically (without knowing
the density of Y)

Now let s be a numeric vector. I want to get the density f_Y(s) of Y evaluated along s.
The proper way of doing this would be to apply the inverse continuous Fourier transform to the function cf_Y at each point in s. 

This is far too slow. That's why I tried to apply the inverse _discrete_ Fourier transform to the vector of values cf_Y(s) 
and that does not yield anything reasonable.

This baffles me since I was under the impression that the discrete Fourier transform is an approximation to the continuous 
Fourier transform and so should yield the values of the latter, if the value grid s is fine enough.

Should this work? 

Note that this inversion would definitely work if I could compute the _discrete_ Fourier transform of the density f_Y
along s, but regrettably this is not possible since 

(a) the density of f_Y is far too complicated, and

(b) the discrete Fourier transform of a sum Y = Y_1+Y_2+...+Y_k of independent 
random variables Y_j is not the product of the discrete Fourier transforms of the Y_j.

Any ideas how I could approach this problem with the tools of R?



Thanks in advance for all replies,

Michael Meyer
	[[alternative HTML version deleted]]


From jwd @end|ng |rom @urewe@t@net  Wed Jan 24 01:22:40 2024
From: jwd @end|ng |rom @urewe@t@net (John)
Date: Tue, 23 Jan 2024 16:22:40 -0800
Subject: [R] Use of geometric mean .. in good data analysis
In-Reply-To: <FF9906E9-E97A-4300-B1A5-D52CEF8FA15D@dcn.davis.ca.us>
References: <b77a1d4b-6e6f-87e7-aea0-5315a180fd8e@appl-ecosys.com>
 <26030.41836.34478.66358@stat.math.ethz.ch>
 <6c6baf6e-8ac1-a3cf-3474-1c1a44df5618@appl-ecosys.com>
 <CAGxFJbSTkYzZUmexoWYRKsFX+0VgJQdAAPYvGqwLeKdrK7mQgg@mail.gmail.com>
 <FF9906E9-E97A-4300-B1A5-D52CEF8FA15D@dcn.davis.ca.us>
Message-ID: <ad78cafd-fa80-47fc-9180-f5c88b65365f@surewest.net>

I've advised people consulting me that if their data is loaded with 
zeros, while they are absolutely certain that something should be where 
the zeros are, then they either need a better measuring tool, or to 
carefully document the results of limits on detectability and then note 
what fraction of the data is really below instrument limits.? It's 
important information as it stands, but they don't want to go writing 
fairy tales based on things not seen.

On 1/22/24 12:57, Jeff Newmiller via R-help wrote:

> Still OT... but here is my own (I think previously mentioned here) rant on people thrashing about with log transformation and an all-too-common kludge to deal with zeros mixed among small numbers...https://gist.github.com/jdnewmil/99301a88de702ad2fcbaef33326b08b4
>
> OP perhaps posting a link here to your question posed wherever you end up with it will help shorten this thread.
>
> On January 22, 2024 12:23:20 PM PST, Bert Gunter<bgunter.4567 at gmail.com>  wrote:
>> Ah.... LOD's, typically LLOD's ("lower limits of detection").
>>
>> Disclaimer: I am *NOT* in any sense an expert on such matters. What follows
>> are just some comments based on my personal experience. Please filter
>> accordingly. Also, while I kept it on list as Martin suggested it might be
>> useful to do so, most folks probably can safely ignore the rant that
>> follows as off topic and not of interest. So you've been warned!!
>>
>> The rant:
>> My experience is: data that contain a "bunch" of values that are, e.g.
>> below a LLOD, are frequently reported and/or analyzed by various ad hoc,
>> and imho, uniformly bad methods. e.g.:
>>
>> 1) The censored values are recorded and analyzed as at the LLOD;
>> 2) The censored values are recorded and analyzed at some arbitrary value
>> below the LLOD, like LLOD/2;
>> 3) The censored values are are "imputed" by ad hoc methods, e.g. uniform
>> random values between 0 and the LLOD for left censoring.
>>
>> To repeat, *IMO*, all of this is junk and will produced misleading
>> statistical results. Whether they mislead enough to substantively affect
>> the science or regulatory decisions depend on the specifics of the
>> circumstances. I accept no general claim as to their innocuousness.
>>
>> Further:
>>
>> a) When you have a "lot" of values -- 50%? 75%?, 25%? -- face facts: you
>> have (practically) no useful information from the values that you do have
>> to infer what the distribution of values that you don't have looks like.
>> All one can sensibly do is say that x% of the values are below a LOD and
>> here's the distribution of what lies above. Presumably, if you have such
>> data conditional on covariates with the obvious intent to determine the
>> relationship to those covariates, you could analyze the percentages of
>> LLOD's and known values separately. There are undoubtedly more
>> sophisticated methods out there, so this is where you need to go to the
>> literature to see what might suit; though I think it will still have to
>> come down to looking at these separately (e.g. with extra parameters to
>> account for unmeasurable values). Another way of saying this is: any
>> analysis which treats all the data as arising from a single distribution
>> will depend more on the assumptions you make than on the data. So good luck
>> with that!
>>
>> b) If you have a "modest" amount of (known) censoring -- 5%?, 20%? 10%? --
>> methods for the analysis of censored data should be useful. My
>> understanding is that MI (multiple imputation) is regarded as a generally
>> useful approach, and there are many R packages that can do various flavors
>> of this. Again, you should consult the literature: there are very likely
>> nontechnical reviews of this topic, too, as well as online discussions and
>> tutorials.
>>
>> So if you are serious about dealing with this and have a lot of data with
>> these issues, my advice would be to stop looking for ad hoc advice and dig
>> into the literature: it's one of the many areas of "data science" where
>> seemingly simple but pervasive questions require complex answers.
>>
>> And, again, heed my personal caveats.
>>
>> Thus endeth my rant.
>>
>> Cheers to all,
>> Bert
>>
>>
>>
>> On Mon, Jan 22, 2024 at 9:29?AM Rich Shepard<rshepard at appl-ecosys.com>
>> wrote:
>>
>>> On Mon, 22 Jan 2024, Martin Maechler wrote:
>>>
>>>> I think it is a good question, not really only about geo-chemistry, but
>>>> about statistics in applied sciences (and engineering for that matter).
>>>> John W Tukey (and several other of the grands of the time) had the log
>>>> transform among the "First aid transformations":
>>>>
>>>> If the data for a continuous variable must all be positive it is also
>>>> typically the case that the distribution is considerably skewed to the
>>>> right. In such a case behave as a good human who sees another human in
>>>> health distress: apply First Aid -- do the things you learned to do
>>>> quickly without too much thought, because things must happen fast ---to
>>>> hopefully save the other's life.
>>> Martin,
>>>
>>> Thanks very much. I will look further into this because toxic metals and
>>> organic compounds in geochemical collections almost always have censored
>>> lab
>>> results (below method dection limits) that range from about 15% to 80% or
>>> more, and there almost always are very high extreme values.
>>>
>>> I'll learn to understand what benefits log transforms have over
>>> compositional data analyses.
>>>
>>> Best regards,
>>>
>>> Rich
>>>
>>> ______________________________________________
>>> R-help at r-project.org  mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org  mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guidehttp://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
	[[alternative HTML version deleted]]


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Wed Jan 24 18:24:43 2024
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Wed, 24 Jan 2024 09:24:43 -0800 (PST)
Subject: [R] 
 Use of geometric mean for geochemical concentrations [RESOLVED]
In-Reply-To: <b77a1d4b-6e6f-87e7-aea0-5315a180fd8e@appl-ecosys.com>
References: <b77a1d4b-6e6f-87e7-aea0-5315a180fd8e@appl-ecosys.com>
Message-ID: <d6d3f691-894-f921-c292-188c7b96e9c@appl-ecosys.com>

On Mon, 22 Jan 2024, Rich Shepard wrote:

> As an aquatic ecologist I see regulators apply the geometric mean to
> geochemical concentrations rather than using the arithmetic mean. I want to
> know whether the geometric mean of a set of chemical concentrations (e.g.,
> in mg/L) is an appropriate representation of the expected value. If not, I
> want to explain this to non-technical decision-makers; if so, I want to
> understand why my assumption is wrong.

Many of you provided excellent comments, and so did a couple of folks on
StackExchange. Rather than responding to individual posts I've waited until
the thread petered out to provide an overall response.

I've two points to make: one on mean calculations and the second on the
context I didn't sufficiently provide when I posted my question.

Responses confirmed that the appropriate model for calculating means depends
on the data set and the question(s) the data are to answer. So the summary
answer to my question (as stated) is: it depends. :-) Thank you.

What prompted my thread-starting message is that I work in the realm of
environmental regulation compliance, including the Clean Water Act and the
Endangered Species Act. There is one state environmental regulator that
provides state-wide point source storm water discharges under a General
permit for smaller industrial activities. The permit monitoring requirements
are 4 samples per year, one each quarter for a small set of water chemical
and physical constituents (really!) and the reporting requirements are to
use the geometric mean to summarize the four data points. I have my clients
calculate an arithmetic mean in addition. (For the record, if you have an
Agriculture Department General Storm Water Discharge Permit for a point
source such as a livestock feed lot you need only a single sample (after the
rains start) to comply with the permit. Feh!

Germane to Bert's comments about all the wrong ways to treat
non-detected/censored water chemical analyses, I discovered Dennis Helsel by
his 2005 article in Environmental Science & Technology (Oct. 16th). Bought
his book when it was published in 2012 and have used survival analyses on
censored data ever since. (Also presented a Continuing Legal Education talk
in 2016 with a nice thank-you email from a state district judge who
attended.)

I greatly appreciate all your comments and apologize for not better
explaining the context of my question when I posted my first message.

Regards,

Rich


From @pyqqqd|@ @end|ng |rom y@hoo@com  Wed Jan 24 10:37:31 2024
From: @pyqqqd|@ @end|ng |rom y@hoo@com (Michael Meyer)
Date: Wed, 24 Jan 2024 09:37:31 +0000 (UTC)
Subject: [R] Use of geometric mean .. in good data analysis
References: <225401338.204789.1706089051288.ref@mail.yahoo.com>
Message-ID: <225401338.204789.1706089051288@mail.yahoo.com>

By the Strong Law of Large Numbers applied to log(X) the geometric mean of X_1,...,X_n > 0 and IID like X converges toexp(E[log(X)]] which, by Jensen's inequality, is always? <= E[X] and is strictly less than E[X] except in trivial extreme cases. 

In short: by using the geometric mean all asymptotic results no longer apply.

Michael Meyer
	[[alternative HTML version deleted]]


From @pyqqqd|@ @end|ng |rom y@hoo@com  Wed Jan 24 09:47:09 2024
From: @pyqqqd|@ @end|ng |rom y@hoo@com (Michael Meyer)
Date: Wed, 24 Jan 2024 08:47:09 +0000 (UTC)
Subject: [R] Quantiles of sums of independent discrete random variables
References: <304306336.187622.1706086029641.ref@mail.yahoo.com>
Message-ID: <304306336.187622.1706086029641@mail.yahoo.com>

Greetings,
Minimal reproducible example as requested by the technical expert Jeff Newmiller:
library(bayesmeta)

# density of $(1/10)*\sum_{j=1}{10}N(j,0.01$ 
# (convex sum of normal distributions)
#
f <- Vectorize(function(s) sum(vapply(1:10,
?? FUN = function(j) dnorm(s,mean=j,sd=0.01)/10, FUN.VALUE=0
)))
g <- function(s) dnorm(s,mean=0,sd=0.01)

cat("\n\n")
for(i in 1:5){
? ?
? cat("Doing convolution ",i,"\n")
? g <- convolve(g,f)$density
}
cat("\nConvolutions finished, plotting density.")
s <- seq(0,100,length.out=1024)
matplot(s,g(s),type="l")
?? 
?
Michael Meyer
	[[alternative HTML version deleted]]


From net @end|ng |rom pme@n@com  Fri Jan 26 17:09:20 2024
From: net @end|ng |rom pme@n@com (Simon, Stephen D.)
Date: Fri, 26 Jan 2024 10:09:20 -0600
Subject: [R] Use of geometric mean .. in good data analysis
In-Reply-To: <mailman.370812.0.1706266801.15369.r-help@r-project.org>
References: <mailman.370812.0.1706266801.15369.r-help@r-project.org>
Message-ID: <7aac639d-576d-4dbf-8e85-2f0d48d83c34@pmean.com>

Sorry to prolong a thread on something that is clearly off topic, but when Michael Meyer wrote

 >by using the geometric mean all asymptotic results no longer apply.

that is flat our wrong. It's true that the geometric mean converges to something different that E[X], but it does indeed have an asymptotic distribution and one that makes sense in some contexts. There is no reason that converging to E[X] is desirable over other alternatives.

Reasons you might prefer the geometric mean.

1. You are interested in ratios instead of differences. A confidence interval for a ratio of geometric means is much simpler than a confidence interval for a ratio of arithmetic means.

2. Your data has issues with unequal variances caused by groups with larger means tending to have larger standard deviations.

3. You want to reduce the influence of outliers.

4. If your data follows a log normal distribution, then exp(E[log(X)]) is very good estimate of the population median.

An analysis using geometric means is different than an analysis using arithmetic means, but different does not mean inferior.


From @pyqqqd|@ @end|ng |rom y@hoo@com  Fri Jan 26 16:38:41 2024
From: @pyqqqd|@ @end|ng |rom y@hoo@com (Michael Meyer)
Date: Fri, 26 Jan 2024 15:38:41 +0000 (UTC)
Subject: [R] DescTools::Quantile
References: <930947001.831290.1706283521871.ref@mail.yahoo.com>
Message-ID: <930947001.831290.1706283521871@mail.yahoo.com>

Greetings,

I am having a problem with DescTools::Quantile
(a function computing quantiles from weighted samples):

# these sum to one
probWeights = c(
     0.0043, 0.0062, 0.0087, 0.0119, 0.0157, 0.0204, 0.0257, 0.0315, 0.0378, 
     0.0441, 0.0501, 0.0556, 0.06, 0.0632, 0.0648, 0.0648, 0.0632, 0.06, 
     0.0556, 0.0501, 0.0441, 0.0378, 0.0315, 0.0257, 0.0204, 0.0157, 0.0119, 
     0.0087, 0.0062, 0.0043
  )
  x = seq(-100,100,length.out=length(probWeights))

  qtls <- DescTools::Quantile(x, weights=probWeights, probs=c(0.1,0.9))
  
cat("\nQuantiles:\n")
  print(qtls)


Both quantiles are equal to 100!
Is this function working or am I not using it correctly? 

Michael Meyer


From murdoch@dunc@n @end|ng |rom gm@||@com  Sat Jan 27 18:47:59 2024
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sat, 27 Jan 2024 12:47:59 -0500
Subject: [R] DescTools::Quantile
In-Reply-To: <930947001.831290.1706283521871@mail.yahoo.com>
References: <930947001.831290.1706283521871.ref@mail.yahoo.com>
 <930947001.831290.1706283521871@mail.yahoo.com>
Message-ID: <92b0a7b2-0e91-41bc-8b94-435daed38153@gmail.com>

On 26/01/2024 10:38 a.m., Michael Meyer via R-help wrote:
> Greetings,
> 
> I am having a problem with DescTools::Quantile
> (a function computing quantiles from weighted samples):
> 
> # these sum to one
> probWeights = c(
>       0.0043, 0.0062, 0.0087, 0.0119, 0.0157, 0.0204, 0.0257, 0.0315, 0.0378,
>       0.0441, 0.0501, 0.0556, 0.06, 0.0632, 0.0648, 0.0648, 0.0632, 0.06,
>       0.0556, 0.0501, 0.0441, 0.0378, 0.0315, 0.0257, 0.0204, 0.0157, 0.0119,
>       0.0087, 0.0062, 0.0043
>    )
>    x = seq(-100,100,length.out=length(probWeights))
> 
>    qtls <- DescTools::Quantile(x, weights=probWeights, probs=c(0.1,0.9))
>    
> cat("\nQuantiles:\n")
>    print(qtls)
> 
> 
> Both quantiles are equal to 100!
> Is this function working or am I not using it correctly?

There's an obvious bug in that function:

     n <- sum(weights)          # this sets n to 1 in your data
     ord <- 1 + (n - 1) * probs # This sets ord to c(1,1)

     low <- pmax(floor(ord), 1)
     high <- pmin(low + 1, n)
     ord <- ord%%1
     allq <- approx(cumsum(weights), x, xout = c(low, high), method = 
"constant",
         f = 1, rule = 2)$y
     k <- length(probs)
     qs <- (1 - ord) * allq[1:k] + ord * allq[-(1:k)]

This bug was reported on the package website 6 months ago 
(https://github.com/AndriSignorell/DescTools/issues/123), and hasn't 
been addressed.  I'd suggest the best action is to find a different package.

Duncan Murdoch


From r@oknz @end|ng |rom gm@||@com  Sun Jan 28 12:04:50 2024
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Mon, 29 Jan 2024 00:04:50 +1300
Subject: [R] 
 Use of geometric mean for geochemical concentrations [RESOLVED]
In-Reply-To: <d6d3f691-894-f921-c292-188c7b96e9c@appl-ecosys.com>
References: <b77a1d4b-6e6f-87e7-aea0-5315a180fd8e@appl-ecosys.com>
 <d6d3f691-894-f921-c292-188c7b96e9c@appl-ecosys.com>
Message-ID: <CABcYAdKgX_CiLAnwFQPbyDpSgn7yT+H-gjpU53-ac57vDz1mHg@mail.gmail.com>

Suppose we measure the concentration of urea in four samples.
Suppose the results are 1, 2, 3, and 4 ?mol?L?1,
How should we summarise this?

If you collect a predetermined volume V of water and measure the
number of moles M(V) of urea then calculated
C(V) = M(V)/V, and then you did that four times, then pooling the four
samples gives you
(M(V1)+M(V2)+M(V3)+M(V4))/(V1+V2+V3+V4) =
(V1*C(V1)+V2*C(V2)+V3*C(V3)+V4*C(V4))/(V1+V2+V3+V4)
and this just *is* the concentration of urea in the pooled samples.
If the samples are all the same volume, this reduces to the arithmetic mean.
In this case, 2.5 ?mol?L?1,

If you predetermine a number of moles M of urea and collect water
until the volume V(M) contains M moles,
then calculated C(M) = M/V(M), and then you did that four times,
pooling the samples gives you
(M1+M2+M3+M4)/(M1/C(M1) + M2/C(M2) + M3/C(M3) + M4/C(M4))
and this just *is* the concentration of urea in the pooled samples.
If the samples all contain the same amount of urea, this reduces to
the geometric mean.
In this case, 1.92 ?mol?L?1,

Really, it's not a statistical question but a semantic one: what
physical reality do the data reflect and what combinations of the
numbers make sense in/of the real world?

On Thu, 25 Jan 2024 at 06:25, Rich Shepard <rshepard at appl-ecosys.com> wrote:
>
> On Mon, 22 Jan 2024, Rich Shepard wrote:
>
> > As an aquatic ecologist I see regulators apply the geometric mean to
> > geochemical concentrations rather than using the arithmetic mean. I want to
> > know whether the geometric mean of a set of chemical concentrations (e.g.,
> > in mg/L) is an appropriate representation of the expected value. If not, I
> > want to explain this to non-technical decision-makers; if so, I want to
> > understand why my assumption is wrong.
>
> Many of you provided excellent comments, and so did a couple of folks on
> StackExchange. Rather than responding to individual posts I've waited until
> the thread petered out to provide an overall response.
>
> I've two points to make: one on mean calculations and the second on the
> context I didn't sufficiently provide when I posted my question.
>
> Responses confirmed that the appropriate model for calculating means depends
> on the data set and the question(s) the data are to answer. So the summary
> answer to my question (as stated) is: it depends. :-) Thank you.
>
> What prompted my thread-starting message is that I work in the realm of
> environmental regulation compliance, including the Clean Water Act and the
> Endangered Species Act. There is one state environmental regulator that
> provides state-wide point source storm water discharges under a General
> permit for smaller industrial activities. The permit monitoring requirements
> are 4 samples per year, one each quarter for a small set of water chemical
> and physical constituents (really!) and the reporting requirements are to
> use the geometric mean to summarize the four data points. I have my clients
> calculate an arithmetic mean in addition. (For the record, if you have an
> Agriculture Department General Storm Water Discharge Permit for a point
> source such as a livestock feed lot you need only a single sample (after the
> rains start) to comply with the permit. Feh!
>
> Germane to Bert's comments about all the wrong ways to treat
> non-detected/censored water chemical analyses, I discovered Dennis Helsel by
> his 2005 article in Environmental Science & Technology (Oct. 16th). Bought
> his book when it was published in 2012 and have used survival analyses on
> censored data ever since. (Also presented a Continuing Legal Education talk
> in 2016 with a nice thank-you email from a state district judge who
> attended.)
>
> I greatly appreciate all your comments and apologize for not better
> explaining the context of my question when I posted my first message.
>
> Regards,
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Ach|m@Ze||e|@ @end|ng |rom u|bk@@c@@t  Sun Jan 28 23:33:48 2024
From: Ach|m@Ze||e|@ @end|ng |rom u|bk@@c@@t (Achim Zeileis)
Date: Sun, 28 Jan 2024 23:33:48 +0100 (CET)
Subject: [R] 2SLS with Fixed Effects and Control Variables
In-Reply-To: <CAOJ6AG3y-QeXk70Sw+npwSxZmgqbS9vA3GvUwVVm5ex4iQY3oQ@mail.gmail.com>
References: <CAOJ6AG3y-QeXk70Sw+npwSxZmgqbS9vA3GvUwVVm5ex4iQY3oQ@mail.gmail.com>
Message-ID: <4678ad6c-feee-08a1-e15e-1981d195bac3@uibk.ac.at>

Kelis,

thanks for your interest. It's hard to say what exactly goes wrong based 
on the information you provide. However, I would recommend that you first 
process the data:

- Store all variables as the appropriate types (numeric, factor, etc.) 
in the data frame. Then you don't have to put these things into the model 
formula.

- Employ variable names without spaces, then you don't have to use 
backquotes.

Hopefully, this helps you to resolve the problem.

If the problem persists and you believe that this is an error within 
ivreg, please provide a minimal self-contained and reproducible example.

Best regards,
Achim


On Sun, 28 Jan 2024, Kelis Wong wrote:

> Dear John Fox, Christian Kleiber, and Achim Zeileis,
> 
> I am attempting to run various independent variable parameters to assess
> their suitability. Unfortunately, I hit a snag and couldn't get the tests to
> run properly. When I used ivreg, I got an error message saying: "Error in
> eval(predvars, data, env) : object 'WageInequality' not found."?
> Can you please help?
> Model: numeric(WageInequality) = numeric(EffectiveMinimum) +
> numeric(EffectiveMinimum2)
> (whereas EffectiveMinimum2 is the quadratic form of EffectiveMinimum)
> 
> Instruments: numeric(`Log Real Minimum Wage`) + numeric(`Log Real Minimum
> Wage2`) + numeric(IVinteration)
> (whereas Real Minimum Wage2 is the quadratic form of Real Minimum Wage)
> 
> Time and Region Fixed Effects: factor(Region) + numeric(Year) + Region:Year
> (whereas Region:Year is and interaction term of Region and Year)
> 
> Control Variables: factor(Region), numeric(`Woman Percentage`),
> numeric(`Noneducated Percentage`)
> 
> Parameter 1:
> ivreg(WageInequality ~ EffectiveMinimum + EffectiveMinimum2 + Region | `Log
> Real Minimum Wage` + `Log Real Minimum Wage2` + IVinteration + Region,
> dataset))
> 
> Parameter 2:
> ivreg(WageInequality ~ EffectiveMinimum + EffectiveMinimum2 + Region + Year
> + Region:Year | `Log Real Minimum Wage` + `Log Real Minimum Wage2` +
> IVinteration + Region + Year + Region:Year, dataset))
> 
> Parameter 3:
> ivreg(WageInequality ~ EffectiveMinimum + EffectiveMinimum2 + Region + Year
> + Region:Year + `Woman Percentage` + `Noneducated Percentage` | `Log Real
> Minimum Wage` + `Log Real Minimum Wage2` + IVinteration + Region + Year +
> Region:Year + `Woman Percentage` + `Noneducated Percentage`, dataset))
> 
> --
> Peace,
> 
> Kelis Wong
> 
>

