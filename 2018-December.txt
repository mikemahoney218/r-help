From p_connolly @ending from @ling@hot@co@nz  Sat Dec  1 07:38:18 2018
From: p_connolly @ending from @ling@hot@co@nz (Patrick Connolly)
Date: Sat, 1 Dec 2018 19:38:18 +1300
Subject: [R] 
 Corrupting files while copying (was Re: saveRDS() and readRDS()
 Why? [solved, pretty much anyway])
In-Reply-To: <20181118084438.GJ8540@slingshot.co.nz>
References: <9795CBFC-5F8C-4084-9704-98614BA2A626@gmail.com>
 <46f6c0217cf594e544eef3257376d598@slingshot.co.nz>
 <CAF8bMcaejV7GdwwFFB7fro2AHD9FfqHJ2J26EZG56kyQ08UP=A@mail.gmail.com>
 <3b49eb752bb2b108a72683df1d30f50b@slingshot.co.nz>
 <CAF8bMcYaXYB=PAZ9D2sDHqP1oH2To8u4A1EMrk0EGGdLdCMb=g@mail.gmail.com>
 <d036a9b8893a1ade51c78e7325d8c89b@slingshot.co.nz>
 <CAF8bMcY_fauLbmKNY-yb0JJJu2c-Oz0tngHhHkdyFwo7ajVDyg@mail.gmail.com>
 <20181114073555.GI8540@slingshot.co.nz>
 <CA+vqiLE2D2R-AYfNA8EPZFPtxuwnUcsCCuADTqCq8c=0xeVPng@mail.gmail.com>
 <20181118084438.GJ8540@slingshot.co.nz>
Message-ID: <20181201063817.GK8540@slingshot.co.nz>

I'd be interested to know if anyone can replicate what I experienced.

VBox version: 5.1.22 r115126 (Qt5.6.2) Versions of R and OSes below:


On Sun, 18-Nov-2018 at 09:44PM +1300, Patrick Connolly wrote:

|> Sequence of steps:
|> 
|> Using R-3.5.1 and Windows 7 with the latest Rstudio, shared directory
|> as working directory on Virtual Box host machine:
|> 
|> > x <- airquality
|> > saveRDS(x, file = "x.rds")
|> > saveRDS(x, file = "y.rds")
|> 
|> On guest machine Mint Linux 17.3, KDE desktop, copy x.rds & y.rds to
|> working directory PWD using file manager Dolphin.
|> 
|> (Don't have the precise version of VirtualBox right now.)

Update:
VBox version: 5.1.22 r115126 (Qt5.6.2)

|> 
|> > x <- readRDS(file =  "x.rds")
|> Error in readRDS(file = "x.rds") : error reading from connection
|> > x <- readRDS(file =  "y.rds")
|> 
|> >   tools::md5sum(c("x.rds", "y.rds"))
|>                              x.rds                              y.rds
|> "5fef054848f39b4be02b7c54f1c71a20" "978a64d1dd342d16a381c9ca728d3665"
|> 
|> Yet, if instead of using Dolphin, use bash commands from the shared
|> directory 
|> 
|> $ cp *.rds ~/PWD/
|> 
|> no error reading from the connection or other differences between
|> x.rds and y.rds.
|> 
|> head(x)
|> 
|> > head(datasets::airquality)
|>   Ozone Solar.R Wind Temp Month Day
|> 1    41     190  7.4   67     5   1
|> 2    36     118  8.0   72     5   2
|> 3    12     149 12.6   74     5   3
|> 4    18     313 11.5   62     5   4
|> 5    NA      NA 14.3   56     5   5
|> 6    28      NA 14.9   66     5   6
|> > 
|> 
|> On Thu, 15-Nov-2018 at 09:53AM -0500, Ista Zahn wrote:
|> 
|> |> Hi Patrick,
|> |> 
|> |> I think it would help to start from the beginning and give complete
|> |> (but concise!) replication instructions, including telling us what
|> |> host and gest operating systems you are using (including the
|> |> versions), the version
|> |>  of virtualbox you used, and exactly what steps are needed to
|> |> reproduce the surprising behavior.
|> |> 
|> |> Best,
|> |> Ista
|> |> On Wed, Nov 14, 2018 at 2:36 AM Patrick Connolly
|> |> <p_connolly at slingshot.co.nz> wrote:
|> |> >
|> |> > Thanks William,
|> |> >
|> |> > I've used Dolphin for years and never encountered that phenomenon.
|> |> > Even so, that description doesn't fit what's going on here.  1.7
|> |> > kilobytes is hardly a 'large directory'.
|> |> >
|> |> > The problem seems to be with the way VirtualBox mounts directories
|> |> > which isn't an R issue, nor is the fact that copying from Linux to
|> |> > Windows isn't affected.  But the fact that it happens only with rds
|> |> > files that use the name of the R object as part of their own names
|> |> > must be an R issue (that surfaces only when other conditions are
|> |> > present).
|> |> >
|> |> > Theories short of divine intervention appreciated.
|> |> >
|> |> >
|> |> >
|> |> > On Tue, 13-Nov-2018 at 02:22PM -0800, William Dunlap wrote:
|> |> >
|> |> > |> Perhaps you got bitten by Dolphin's non-modal dialogs, as described in
|> |> > |> https://userbase.kde.org/Dolphin/File_Management:
|> |> > |>
|> |> > |> Non Modal Dialogs
|> |> > |>
|> |> > |> When Moving, Copying or Deleting files/directories the dialog disappears
|> |> > |> even when the operation has not yet completed. A progress bar then appears
|> |> > |> in the bottom right of the screen, this then disappears also, if you want
|> |> > |> see the progress you need to click a small (i) information icon in the
|> |> > |> system tray.
|> |> > |>
|> |> > |>
|> |> > |> Warning
|> |> > |> New users who are not used to this way of working (and even experienced
|> |> > |> users) can get caught out by this, if you are Moving, Copying or Deleting
|> |> > |> large directories then you need to use the icon to monitor the progress of
|> |> > |> your operation. If you don't then any subsequent actions you do, may well
|> |> > |> use an incomplete file structure resulting in corrupted files. You have
|> |> > |> been warned!
|> |> > |>
|> |> > |> Bill Dunlap
|> |> > |> TIBCO Software
|> |> > |> wdunlap tibco.com
|> |> > |>
|> |> > |> On Tue, Nov 13, 2018 at 2:10 PM, p_connolly <p_connolly at slingshot.co.nz>
|> |> > |> wrote:
|> |> > |>
|> |> > |> > This is getting more strange.
|> |> > |> >
|> |> > |> > I normally copy from the shared folder to the appropriate directory using
|> |> > |> > Dolphin, the KDE file manager.  If instead I use the standard bash cp
|> |> > |> > command, no corruption happens -- at least with the limited testing I have
|> |> > |> > done.  There also seems to be no problem copying from Linux to Windows.  I
|> |> > |> > installed R-3.5.1 for Windows just to eliminate that possible issue.
|> |> > |> >
|> |> > |> > However, R has *something* to do with it because it was used to make the
|> |> > |> > .rds file.  Just how the relationship between the name of the R object and
|> |> > |> > the name of the .rds file comes into it, I can't imagine.
|> |> > |> >
|> |> > |> > Thanks for the suggestion William.
|> |> > |> >
|> |> > |> >
|> |> > |> > On 2018-11-14 06:26, William Dunlap wrote:
|> |> > |> >
|> |> > |> >> It seems like copying the files corrupted them. How did you copy them
|> |> > |> >> (with R
|> |> > |> >> or cp or copy or ftp, etc.)?  I don't see how this has anything to do
|> |> > |> >> with R.
|> |> > |> >>
|> |> > |> >> Bill Dunlap
|> |> > |> >> TIBCO Software
|> |> > |> >> wdunlap tibco.com [1]
|> |> > |> >> On Mon, Nov 12, 2018 at 7:10 PM, p_connolly
|> |> > |> >> <p_connolly at slingshot.co.nz> wrote:
|> |> > |> >>
|> |> > |> > [...]
|> |> > |> >
|> |> > |> >
|> |> >
|> |> > --
|> |> > ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
|> |> >    ___    Patrick Connolly
|> |> >  {~._.~}                   Great minds discuss ideas
|> |> >  _( Y )_                 Average minds discuss events
|> |> > (:_~*~_:)                  Small minds discuss people
|> |> >  (_)-(_)                              ..... Eleanor Roosevelt
|> |> >
|> |> > ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
|> |> >
|> |> > ______________________________________________
|> |> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
|> |> > https://stat.ethz.ch/mailman/listinfo/r-help
|> |> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
|> |> > and provide commented, minimal, self-contained, reproducible code.
|> 
|> -- 
|> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
|>    ___    Patrick Connolly   
|>  {~._.~}                   Great minds discuss ideas    
|>  _( Y )_  	         Average minds discuss events 
|> (:_~*~_:)                  Small minds discuss people  
|>  (_)-(_)  	                      ..... Eleanor Roosevelt
|> 	  
|> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
|> 
|> ______________________________________________
|> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
|> https://stat.ethz.ch/mailman/listinfo/r-help
|> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
|> and provide commented, minimal, self-contained, reproducible code.

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}                   Great minds discuss ideas    
 _( Y )_  	         Average minds discuss events 
(:_~*~_:)                  Small minds discuss people  
 (_)-(_)  	                      ..... Eleanor Roosevelt
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From jeremieju@te @ending from gm@il@com  Sat Dec  1 09:52:54 2018
From: jeremieju@te @ending from gm@il@com (Jeremie Juste)
Date: Sat, 01 Dec 2018 09:52:54 +0100
Subject: [R] High dimensional optimization in R
In-Reply-To: <b285a022-f203-3def-cdb1-e839ef4bce70@yahoo.fr> (Marc Girondot
 via's message of "Fri, 30 Nov 2018 09:26:05 +0100")
References: <CAGZkDQhiVVuyJ1c6D3-HOxqash19dWsOfnHbnFrbx6TLcp5nYA@mail.gmail.com>
 <CAM_vjun9NAUs=yTqQeQQRYu09OM6JiiRYJXWD1u0fYUqEC62hQ@mail.gmail.com>
 <0acb74a5-e7f7-0c5e-ad98-72ded5120ce6@kfupm.edu.sa>
 <b285a022-f203-3def-cdb1-e839ef4bce70@yahoo.fr>
Message-ID: <87o9a5r5ll.fsf@gmail.com>


Hello,

Genetic algorithm can prove handy as well here. see for instance
https://cran.r-project.org/web/packages/GA/vignettes/GA.html

with non-convex objective functions I usually try a genetic algorithm for
a few rounds then finish using nlminb


Best regards,
Jeremie

Marc Girondot via R-help <r-help at r-project.org> writes:

> I fit also model with many variables (>100) and I get good result when
> I mix several method iteratively, for example: 500 iterations of
> Nelder-Mead followed by 500 iterations of BFGS followed by 500
> iterations of Nelder-Mead followed by 500 iterations of BFGS
> etc. until it stabilized. It can take several days.
> I use or several rounds of optimx or simply succession of optim.
>
> Marc
>
> Le 28/11/2018 ? 09:29, Ruben a ?crit?:
>> Hi,
>>
>> Sarah Goslee (jn reply to? Basic optimization question (I'm a
>> rookie)):? "R is quite good at optimization."
>>
>> I wonder what is the experience of the R user community with high
>> dimensional problems, various objective functions and various
>> numerical methods in R.
>>
>> In my experience with my package CatDyn (which depends on optimx), I
>> have fitted nonlinear models with nearly 50 free parameters using
>> normal, lognormal, gamma, Poisson and negative binomial exact
>> loglikelihoods, and adjusted profile normal and adjusted profile
>> lognormal approximate loglikelihoods.
>>
>> Most numerical methods crash, but CG and spg often, and BFGS,
>> bobyqa, newuoa and Nelder-Mead sometimes, do yield good results (all
>> numerical gradients less than 1)? after 1 day or more running in a
>> normal 64 bit PC with Ubuntu 16.04 or Windows 7.
>>
>> Ruben
>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drjimlemon @ending from gm@il@com  Sat Dec  1 10:42:24 2018
From: drjimlemon @ending from gm@il@com (Jim Lemon)
Date: Sat, 1 Dec 2018 20:42:24 +1100
Subject: [R] 
 Seek help - plm package (Error message: duplicate 'row.names'
 are not allowed)
In-Reply-To: <HK0PR03MB417818C8D4F4BC7B022C0EFDB7D30@HK0PR03MB4178.apcprd03.prod.outlook.com>
References: <HK0PR03MB417818C8D4F4BC7B022C0EFDB7D30@HK0PR03MB4178.apcprd03.prod.outlook.com>
Message-ID: <CA+8X3fXtjg+9WNReGJM7cJvY2kcG+mrV8iOHxO6funiWaxAoMw@mail.gmail.com>

Hi david,
The formatting of the data frame looks like the Province and Year
columns have gotten stuck together. This probably has something to do
with your Excel spreadsheet or the function that you are using to read
it in. If there is are fewer column names than columns, this error is
likely to happen. As your data did not get through, this is a guess,
but it might help.

Jim

On Sat, Dec 1, 2018 at 1:15 AM Wong David <david-wong912 at hotmail.com> wrote:
>
> Dear Madam/ Sir,
>
> When I used the 'plm' package and import data into r last week, I found that everything was running smooth. However, when I used the 'plm' package today, I found the following error message:
>
> > pdata <- pdata.frame(mydata, index=c("Province","Year"))
> > pooling <- plm(Y~X, data=pdata,model="pooling")
> Error in `row.names<-.data.frame`(`*tmp*`, value = c("Anhui-2006", "Anhui-2007",  :
>   duplicate 'row.names' are not allowed
> In addition: Warning message:
> non-unique values when setting 'row.names':
>
> I did not find out the above error message when running panel data regression over one year. Moreover, I attempted to manipulate the data many times in the excel data file, for example, deleting the duplicated row, and convert the data into csv file. The r program shows the same result. Please kindly advise as this problem has bothered for the whole day.
>
> Enclosed is the data set and the respective result. Please kindly assist.
>
> Thanks and regards,
> David Wong
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From profjcn@@h @ending from gm@il@com  Sat Dec  1 18:05:17 2018
From: profjcn@@h @ending from gm@il@com (J C Nash)
Date: Sat, 1 Dec 2018 12:05:17 -0500
Subject: [R] High dimensional optimization in R
In-Reply-To: <87o9a5r5ll.fsf@gmail.com>
References: <CAGZkDQhiVVuyJ1c6D3-HOxqash19dWsOfnHbnFrbx6TLcp5nYA@mail.gmail.com>
 <CAM_vjun9NAUs=yTqQeQQRYu09OM6JiiRYJXWD1u0fYUqEC62hQ@mail.gmail.com>
 <0acb74a5-e7f7-0c5e-ad98-72ded5120ce6@kfupm.edu.sa>
 <b285a022-f203-3def-cdb1-e839ef4bce70@yahoo.fr> <87o9a5r5ll.fsf@gmail.com>
Message-ID: <86a76849-a616-7e9c-8288-ffee5b343e85@gmail.com>

The postings about polyalgorithms don't mention that optimx has a
tool called polyopt() for this. Though I included it in the package,
it has not been widely tested or applied, and more experience with such
approaches would certainly be of interest to a number of workers, though
I suspect the results are rather context-dependent.

JN

On 2018-12-01 3:52 a.m., Jeremie Juste wrote:
> 
> Hello,
> 
> Genetic algorithm can prove handy as well here. see for instance
> https://cran.r-project.org/web/packages/GA/vignettes/GA.html
> 
> with non-convex objective functions I usually try a genetic algorithm for
> a few rounds then finish using nlminb
> 
> 
> Best regards,
> Jeremie
> 
> Marc Girondot via R-help <r-help at r-project.org> writes:
> 
>> I fit also model with many variables (>100) and I get good result when
>> I mix several method iteratively, for example: 500 iterations of
>> Nelder-Mead followed by 500 iterations of BFGS followed by 500
>> iterations of Nelder-Mead followed by 500 iterations of BFGS
>> etc. until it stabilized. It can take several days.
>> I use or several rounds of optimx or simply succession of optim.
>>
>> Marc
>>
>> Le 28/11/2018 ? 09:29, Ruben a ?crit?:
>>> Hi,
>>>
>>> Sarah Goslee (jn reply to? Basic optimization question (I'm a
>>> rookie)):? "R is quite good at optimization."
>>>
>>> I wonder what is the experience of the R user community with high
>>> dimensional problems, various objective functions and various
>>> numerical methods in R.
>>>
>>> In my experience with my package CatDyn (which depends on optimx), I
>>> have fitted nonlinear models with nearly 50 free parameters using
>>> normal, lognormal, gamma, Poisson and negative binomial exact
>>> loglikelihoods, and adjusted profile normal and adjusted profile
>>> lognormal approximate loglikelihoods.
>>>
>>> Most numerical methods crash, but CG and spg often, and BFGS,
>>> bobyqa, newuoa and Nelder-Mead sometimes, do yield good results (all
>>> numerical gradients less than 1)? after 1 day or more running in a
>>> normal 64 bit PC with Ubuntu 16.04 or Windows 7.
>>>
>>> Ruben
>>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dx@gu@ltero10 @ending from uni@nde@@edu@co  Sat Dec  1 18:00:02 2018
From: dx@gu@ltero10 @ending from uni@nde@@edu@co (=?iso-8859-1?Q?Daniela_Ximena_Gualtero_Brice=F1o?=)
Date: Sat, 1 Dec 2018 17:00:02 +0000
Subject: [R] Question about rmgarch package in R
Message-ID: <BN6PR08MB24180ED6E17B675188AAD8D6ADAC0@BN6PR08MB2418.namprd08.prod.outlook.com>

Good morning, my name is Daniela from Colombia,


I am working with rmgarch package, specifically with the cgarchspec function.

One argument of this function is the "transformation", which is the type of transformation to apply to the marginal innovations of the GARCH fitted models (transformation can be parametric, empirical or semi-parametric). Do you know how this transformations work?. Specially with the parametric transformation, which distribution is assumed? (normal, t student?), how do I access to this estimated parameters? This is the code of the function:



    cgarchspec(uspec, dccOrder = c(1, 1), asymmetric = FALSE,

    distribution.model = list(copula = c("mvnorm", "mvt"),

    method = c("Kendall", "ML"), time.varying = FALSE,

    transformation = c("parametric", "empirical", "spd"))


Thanks,


Daniela Gualtero


	[[alternative HTML version deleted]]


From f@tm@@m@ci @ending from gm@il@com  Sun Dec  2 11:50:00 2018
From: f@tm@@m@ci @ending from gm@il@com (Fatma Ell)
Date: Sun, 2 Dec 2018 11:50:00 +0100
Subject: [R] Does the correlations of component makes the correlation of one
 phenomena ?
Message-ID: <CAH3AuNDu90UykuWQ-asAnjjjVj8zA6toqbHDABSa4Dt_6y-yjQ@mail.gmail.com>

Hi,

I have the following dataset Mesures. It contains test which is a given
context, Space is portion of this following context test. For each test we
have twelve Space and an empirical measure of a behavior Behavior_empirical and
a mesure of simulated behavior Behavior_simulated.

Mesures=structure(list(test = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 10L, 10L, 10L,
10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L), Space = c(1L, 2L, 3L,
4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,
9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L,
1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L,
6L, 7L, 8L, 9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,
10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 1L,
2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L,
7L, 8L, 9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
11L, 12L), Behavior_empirical = c(3.02040816326531, 7.95918367346939,
10.6162790697674, 4.64150943396226, 1.86538461538462, 1.125,
1.01020408163265, 1.2093023255814, 0.292452830188679, 0, 0, 0, 0,
1.3265306122449, 0, 3.09433962264151, 0, 1.6875, 2.02040816326531,
1.2093023255814, 1.75471698113208, 1.79347826086957,
0.243589743589744, 0, 0.377551020408163, 1.98979591836735,
6.75581395348837, 6.18867924528302, 7.46153846153846, 0.75, 0, 0,
0.292452830188679, 0, 0, 0, 0, 1.3265306122449, 1.93023255813953,
10.8301886792453, 3.73076923076923, 0, 2.69387755102041,
0.604651162790698, 1.75471698113208, 0, 0, 0, 1.51020408163265,
2.6530612244898, 3.86046511627907, 1.54716981132075, 1.86538461538462,
1.875, 2.35714285714286, 1.2093023255814, 0.292452830188679, 0, 0,
0.823529411764706, 6.79591836734694, 15.2551020408163,
5.7906976744186, 1.54716981132075, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0.773584905660377, 0, 0, 0.673469387755102, 1.81395348837209,
1.75471698113208, 2.51086956521739, 3.10576923076923,
3.70588235294118, 3.77551020408163, 9.28571428571428,
3.86046511627907, 1.54716981132075, 0, 0, 0, 0, 1.4622641509434, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0.673469387755102, 0, 0.292452830188679,
4.30434782608696, 1.09615384615385, 5.76470588235294, 0, 0,
1.93023255813953, 4.64150943396226, 3.73076923076923, 2.625,
0.673469387755102, 0.604651162790698, 0, 0, 0, 0), Behavior_simulated
= c(18, 61, 129, 198, 128, 57, 44, 80, 36, 8, 0, 0, 0, 0, 0, 49, 50,
194, 211, 353, 352, 214, 120, 15, 10, 74, 145, 224, 158, 99, 26, 19,
7, 2, 0, 0, 180, 89, 47, 36, 34, 56, 51, 65, 44, 4, 0, 0, 116, 133,
131, 103, 74, 132, 75, 44, 0, 0, 0, 0, 532, 165, 18, 5, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 1, 0, 0, 6, 47, 164, 193, 185, 91, 239, 219, 168,
83, 1, 14, 45, 136, 129, 89, 5, 0, 0, 0, 0, 0, 0, 0, 0, 6, 17, 92,
280, 273, 0, 6, 25, 108, 129, 285, 171, 181, 39, 2, 0, 0)), .Names =
c("test", "Space", "Behavior_empirical", "Behavior_simulated"),
row.names = c(NA, 120L), class = "data.frame")

For each test we study correlation between Behavior_empirical
Behavior_simulatedelation

Correlation <- character()for(i in 1:10){Mes=Mesures[(Mesures$test==i),]
co=data.frame(test=i,value=cor(Mes$Behavior_empirical,Mes$Behavior_simulated))Correlation
<- rbind(Correlation, as.data.frame(co))
i=i+1}

which give us for each test many good correlation values :

    test      value1     1  0.55086832     2  0.43690913     3
0.90498064     4 -0.10627145     5  0.84101656     6  0.55608257     7
 0.80880348     8  0.77212329     9  0.708862410   10  0.5116938

Now , we want to conclude that, if the we have good values of
Behavior_simulated for each test. It could build the final distribution
which is the sum of Behavior_simulated and then compare with the sum of
Behavior_empirical.

Mesures_aggregated<- Mesures %>% group_by(Space) %>%
summarize(Sum_Behavior_empirical=sum(Behavior_empirical),Sum_Behavior_simulated=sum(Behavior_simulated))

I may think that my final correlation result should be good. But it is not
the case

> cor(Mesures_aggregated$ Sum_Behavior_empirical,Mesures_aggregated$Sum_Behavior_simulated)[1] 0.07710804

Is correlation could be a result of correlations of the component of one
phenomena ? and How to evaluate the contribution of each component test in
building the 'Sum`?


Thanks  a lot for your help.


Lenny

	[[alternative HTML version deleted]]


From m@h1@khl@ghi @ending from gm@il@com  Sun Dec  2 12:51:47 2018
From: m@h1@khl@ghi @ending from gm@il@com (mahboobe akhlaghi)
Date: Sun, 2 Dec 2018 15:21:47 +0330
Subject: [R] (no subject)
Message-ID: <CANjNGqpsq=Od-ywbuO5zE5Ky=1KxxBC03E-bFjTZPJHL-y695g@mail.gmail.com>

Hi,
I have a question about R Studio. I have a database in SQL. now, I combined
SQL and R Studio. the levels of a variable is in farsi. so when I want to
read some variables, in R Studio are "???????" . I want to show them in
farsi. what should I do to have right words in R Studio?
Thanks.

-- 
Best Regards,
Mahboobe Akhlaghi
PhD Student in Biostatistics
Isfahan University of Medical Sciences

	[[alternative HTML version deleted]]


From murdoch@dunc@n @ending from gm@il@com  Sun Dec  2 21:45:28 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Sun, 2 Dec 2018 15:45:28 -0500
Subject: [R] (no subject)
In-Reply-To: <CANjNGqpsq=Od-ywbuO5zE5Ky=1KxxBC03E-bFjTZPJHL-y695g@mail.gmail.com>
References: <CANjNGqpsq=Od-ywbuO5zE5Ky=1KxxBC03E-bFjTZPJHL-y695g@mail.gmail.com>
Message-ID: <dba34cd0-716f-9392-06d9-84cdfc369ab1@gmail.com>

On 02/12/2018 6:51 AM, mahboobe akhlaghi wrote:
> Hi,
> I have a question about R Studio. I have a database in SQL. now, I combined
> SQL and R Studio. the levels of a variable is in farsi. so when I want to
> read some variables, in R Studio are "???????" . I want to show them in
> farsi. what should I do to have right words in R Studio?

You'll probably need to contact RStudio support about this.  This 
mailing list is about R; RStudio is a separate project that provides a 
front end to R.

When you contact them, let them know details about your system:  are you 
running Windows?  What version?  What locale?

Duncan Murdoch


From jdnewmil @ending from dcn@d@vi@@c@@u@  Sun Dec  2 23:48:38 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Sun, 02 Dec 2018 14:48:38 -0800
Subject: [R] (no subject)
In-Reply-To: <dba34cd0-716f-9392-06d9-84cdfc369ab1@gmail.com>
References: <CANjNGqpsq=Od-ywbuO5zE5Ky=1KxxBC03E-bFjTZPJHL-y695g@mail.gmail.com>
 <dba34cd0-716f-9392-06d9-84cdfc369ab1@gmail.com>
Message-ID: <724BDFB3-97EC-44AB-A234-5AB97AC31AD4@dcn.davis.ca.us>

It is possible that this is not an RStudio-specific issue in spite of the initial phrasing. 

Mahboobe:
If when you use Rgui (Windows) or R.app (Mac) or plain R at the command line (any OS) you still have problems, then you may need to ask for help here or in R-sig-mac or R-sig-debian or R-sig-fedora. Follow the recommendations in the Posting Guide to help your helper reproduce your problem without depending on RStudio. You should also describe what steps you have already tried... in particular read about Locales  and Encoding in the R Data Import/Export  and Installation/Administration manuals and describe what you have done to follow the advice given there.

FWIW I am an English-only speaker so have very limited experience with locales and encodings... be sure to direct your questions to whichever mailing list is most specific to your operating system rather than me.

Also, if it is RStudio-specific, there is a good RStudio blog post [1] you can start with.

[1] https://support.rstudio.com/hc/en-us/articles/200532197-Character-Encoding


On December 2, 2018 12:45:28 PM PST, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>On 02/12/2018 6:51 AM, mahboobe akhlaghi wrote:
>> Hi,
>> I have a question about R Studio. I have a database in SQL. now, I
>combined
>> SQL and R Studio. the levels of a variable is in farsi. so when I
>want to
>> read some variables, in R Studio are "???????" . I want to show them
>in
>> farsi. what should I do to have right words in R Studio?
>
>You'll probably need to contact RStudio support about this.  This 
>mailing list is about R; RStudio is a separate project that provides a 
>front end to R.
>
>When you contact them, let them know details about your system:  are
>you 
>running Windows?  What version?  What locale?
>
>Duncan Murdoch
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From dc@rl@on @ending from t@mu@edu  Mon Dec  3 01:43:20 2018
From: dc@rl@on @ending from t@mu@edu (David L Carlson)
Date: Mon, 3 Dec 2018 00:43:20 +0000
Subject: [R] 
 Does the correlations of component makes the correlation of one
 phenomena ?
In-Reply-To: <CAH3AuNDu90UykuWQ-asAnjjjVj8zA6toqbHDABSa4Dt_6y-yjQ@mail.gmail.com>
References: <CAH3AuNDu90UykuWQ-asAnjjjVj8zA6toqbHDABSa4Dt_6y-yjQ@mail.gmail.com>
Message-ID: <6ee48ca07dce470a830253a82706d94f@tamu.edu>

This is really a statistics question rather than an R question, but you did provide reproducible data. You have some moderate correlations for some of the tests, but they are all different relationships. You used a combination of base R and dplyr code, but I'll just stick with base R:

> Mesures.split <- split(Mesures, Mesures$test)
> Corrs <- sapply(Mesures.split, function(x) cor(x[, 3], x[, 4]))
> options(digits=3)
> Corrs
     1      2      3      4      5      6      7      8      9     10 
 0.551  0.437  0.905 -0.106  0.841  0.556  0.809  0.772  0.709  0.512 

> sapply(Mesures.split, function(x) coef(lm(x[, 3]~x[, 4])))
                 1      2       3        4      5      6      7
(Intercept) 0.6875 0.6530 -0.2597  2.24313 0.3498 1.4436 0.4103
x[, 4]      0.0309 0.0034  0.0353 -0.00668 0.0171 0.0168 0.0137
                  8      9      10
(Intercept) -0.7379 0.2929 0.48115
x[, 4]       0.0255 0.0129 0.00891

This gives you the intercept and slope for the regression lines for each test. Notice that they vary considerably. The slope value for predicting behavior from simulated varies from -0.007 to .031. When you average over space you effectively eliminate the correlations at the test level:

> Mesures_aggregated <- aggregate(Mesures[, 3:4], by=list(Mesures$Space), sum)
> cor(Mesures_aggregated[, 2:3])[1, 2]
[1] 0.0771

If you sum predicted values for empirical behavior using the 10 regression equations and compare that to the summed empirical value, things work out better.

> pred <- rowSums(sapply(Mesures.split, function(x) predict(lm(x[, 3]~x[, 4]))))
> cor(Mesures_aggregated[, 2], pred)
[1] 0.776

Without knowing where the simulated values come from, especially if they are completely independent of the empirical values, I can't say if this approach is wise.

---------------------------------------
David L. Carlson
Department of Anthropology
Texas A&M University


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Fatma Ell
Sent: Sunday, December 2, 2018 4:50 AM
To: r-help at r-project.org
Subject: [R] Does the correlations of component makes the correlation of one phenomena ?

Hi,

I have the following dataset Mesures. It contains test which is a given
context, Space is portion of this following context test. For each test we
have twelve Space and an empirical measure of a behavior Behavior_empirical and
a mesure of simulated behavior Behavior_simulated.

Mesures=structure(list(test = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 10L, 10L, 10L,
10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L), Space = c(1L, 2L, 3L,
4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,
9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L,
1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L,
6L, 7L, 8L, 9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,
10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 1L,
2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L,
7L, 8L, 9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
11L, 12L), Behavior_empirical = c(3.02040816326531, 7.95918367346939,
10.6162790697674, 4.64150943396226, 1.86538461538462, 1.125,
1.01020408163265, 1.2093023255814, 0.292452830188679, 0, 0, 0, 0,
1.3265306122449, 0, 3.09433962264151, 0, 1.6875, 2.02040816326531,
1.2093023255814, 1.75471698113208, 1.79347826086957,
0.243589743589744, 0, 0.377551020408163, 1.98979591836735,
6.75581395348837, 6.18867924528302, 7.46153846153846, 0.75, 0, 0,
0.292452830188679, 0, 0, 0, 0, 1.3265306122449, 1.93023255813953,
10.8301886792453, 3.73076923076923, 0, 2.69387755102041,
0.604651162790698, 1.75471698113208, 0, 0, 0, 1.51020408163265,
2.6530612244898, 3.86046511627907, 1.54716981132075, 1.86538461538462,
1.875, 2.35714285714286, 1.2093023255814, 0.292452830188679, 0, 0,
0.823529411764706, 6.79591836734694, 15.2551020408163,
5.7906976744186, 1.54716981132075, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0.773584905660377, 0, 0, 0.673469387755102, 1.81395348837209,
1.75471698113208, 2.51086956521739, 3.10576923076923,
3.70588235294118, 3.77551020408163, 9.28571428571428,
3.86046511627907, 1.54716981132075, 0, 0, 0, 0, 1.4622641509434, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0.673469387755102, 0, 0.292452830188679,
4.30434782608696, 1.09615384615385, 5.76470588235294, 0, 0,
1.93023255813953, 4.64150943396226, 3.73076923076923, 2.625,
0.673469387755102, 0.604651162790698, 0, 0, 0, 0), Behavior_simulated
= c(18, 61, 129, 198, 128, 57, 44, 80, 36, 8, 0, 0, 0, 0, 0, 49, 50,
194, 211, 353, 352, 214, 120, 15, 10, 74, 145, 224, 158, 99, 26, 19,
7, 2, 0, 0, 180, 89, 47, 36, 34, 56, 51, 65, 44, 4, 0, 0, 116, 133,
131, 103, 74, 132, 75, 44, 0, 0, 0, 0, 532, 165, 18, 5, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 1, 0, 0, 6, 47, 164, 193, 185, 91, 239, 219, 168,
83, 1, 14, 45, 136, 129, 89, 5, 0, 0, 0, 0, 0, 0, 0, 0, 6, 17, 92,
280, 273, 0, 6, 25, 108, 129, 285, 171, 181, 39, 2, 0, 0)), .Names =
c("test", "Space", "Behavior_empirical", "Behavior_simulated"),
row.names = c(NA, 120L), class = "data.frame")

For each test we study correlation between Behavior_empirical
Behavior_simulatedelation

Correlation <- character()for(i in 1:10){Mes=Mesures[(Mesures$test==i),]
co=data.frame(test=i,value=cor(Mes$Behavior_empirical,Mes$Behavior_simulated))Correlation
<- rbind(Correlation, as.data.frame(co))
i=i+1}

which give us for each test many good correlation values :

    test      value1     1  0.55086832     2  0.43690913     3
0.90498064     4 -0.10627145     5  0.84101656     6  0.55608257     7
 0.80880348     8  0.77212329     9  0.708862410   10  0.5116938

Now , we want to conclude that, if the we have good values of
Behavior_simulated for each test. It could build the final distribution
which is the sum of Behavior_simulated and then compare with the sum of
Behavior_empirical.

Mesures_aggregated<- Mesures %>% group_by(Space) %>%
summarize(Sum_Behavior_empirical=sum(Behavior_empirical),Sum_Behavior_simulated=sum(Behavior_simulated))

I may think that my final correlation result should be good. But it is not
the case

> cor(Mesures_aggregated$ Sum_Behavior_empirical,Mesures_aggregated$Sum_Behavior_simulated)[1] 0.07710804

Is correlation could be a result of correlations of the component of one
phenomena ? and How to evaluate the contribution of each component test in
building the 'Sum`?


Thanks  a lot for your help.


Lenny

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From @purdle@@ @ending from gm@il@com  Mon Dec  3 02:59:28 2018
From: @purdle@@ @ending from gm@il@com (Abs Spurdle)
Date: Mon, 3 Dec 2018 14:59:28 +1300
Subject: [R] [R-pkgs] Introducing empirical: Probability Distributions as
 Models of Data
Message-ID: <CAB8pepz4BS-vb5VqUaDN8fBE=3bOT+0bV9zc29aQWPHYNo6sVg@mail.gmail.com>

hi all

I would like to introduce my R package:
empirical: Probability Distributions as Models of Data

The description is:
Computes continuous (not step) empirical (and nonparametric) probability
density, cumulative distribution and quantile functions. Supports
univariate, multivariate and conditional probability distributions, some
kernel smoothing features and weighted data (possibly useful mixed with
fuzzy clustering). Can compute multivariate and conditional probabilities.
Also, can compute conditional medians, quantiles and modes.

Notes:
(1) I'm planning to support categorical variables in the future.
(2) There are some problems with univariate models (but not multivariate),
especially PDFs.
(3) Contrary to what the name empirical suggests, currently multivariate
models use kernel smoothing.
(4) I'm interested in implementing a hybrid Kernel-Quantile method, which I
suspect may be more robust to outliers. If I succeed, then I may rewrite
the univariate implementation.

The URL is:
https://cran.r-project.org/package=empirical

I've written a vignette which describes the package in more detail.
It's URL is:
https://cran.r-project.org/web/packages/empirical/vignettes/empirical.pdf


kind regards
Abs

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From m@@ternh@ttt @ending from gm@il@com  Mon Dec  3 14:03:33 2018
From: m@@ternh@ttt @ending from gm@il@com (Thanh Tran)
Date: Mon, 3 Dec 2018 22:03:33 +0900
Subject: [R] D-optimum design - optimization model
Message-ID: <CAHjanSC3+baNB2-1x_arv7410D0eJX6=6LS4eMRPNqiZVYR1+g@mail.gmail.com>

Dear all,



I'm trying to use the D-optimum design. In my data, the response is KIC,
and 4 factors are AC, AV, T, and Temp. A typical second-degree response
modeling is as follows:



> data<-read.csv("2.csv", header =T)

> mod <- lm(KIC~AC+I(AC^2)+AV+I(AV^2)+T+I(T^2)+Temp+I(Temp^2)+AC:AV+AC:T+AC:Temp+AV:T+AV:Temp+T:Temp,

+ data = data)



The result of the model:



KIC = 4.85 ? 2.9AC +0.151 AV + 0.1094T

          + 0.0091Temp + 0.324 AC^2-0.0156V^2

          - 10.00106T^2 - 0.0009Temp^2 + 0.0071AC?AV

          - 0.00087AC?T -0.00083AC?Temp ? 0.0018AV?T

         +0.0015AV?Temp ? 0.000374 AV ? T



Based on the above response modelling, I want to determine levels of the
AC, AV, T, and Temp to have the Maximum value of KIC. The result running in
Minitab as is shown in Figure 1. In R, I try to compute an D-optimum design
with the following codes:



> attach(data)

> F.trig <- F.cube

> F.trip <-
F.cube(KIC~AC+I(AC^2)+AV+I(AV^2)+T+I(T^2)+Temp+I(Temp^2)+AC:AV+AC:T+AC:Temp+AV:T+AV:Temp+T:Temp,

+ c(4,4,30,5), # Smalesst values of AC,AV,T, and Temp

+ c(5,7,50,25), # Highest values of AC,AV,T, and Temp

+ c(3,3,3,3)) # Numbers of levels ofAC,AV,T, and Temp

> res.trip.D <- od.AA(F.trip,1,alg = "doom", crit = "D",

+ graph =1:7, t.max = 4)





I have the result as shown in Figure 2 but I cannot find out the optimum
design as shown in Figure 1 using Minitab.



If anyone has any experience about what would be the reason for error or
how I can solve it? I really appreciate your support and help.



Best regards,

Nhat Tran



Ps: I also added a CSV file for practicing R.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 2.png
Type: image/png
Size: 14594 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20181203/0c9d6427/attachment.png>

From wolfg@ng@lenh@rd @ending from uni-wuerzburg@de  Mon Dec  3 12:03:44 2018
From: wolfg@ng@lenh@rd @ending from uni-wuerzburg@de (Wolfgang Lenhard)
Date: Mon, 3 Dec 2018 12:03:44 +0100
Subject: [R] [R-pkgs] New package cNORM: Continuous norming
Message-ID: <3600bded-9af7-061e-0b1b-14f206595419@uni-wuerzburg.de>

Hi everyone,
I would like to introduce a new package: cNORM. It aims at solving 
problems with percentile estimation / norm score generation in 
biometrics and psychometrics, f. e. BMI growth curves, IQ tests ... 
Conventional methods for producing standard scores in psychometrics or 
biometrics are often plagued with "jumps" or "gaps" (i.e., 
discontinuities) in norm tables and low confidence for assessing extreme 
scores. The continuous norming method introduced by A. Lenhard et al. 
(2016), <doi:10.1177/1073191116656437>, generates continuous test norm 
scores on the basis of the raw data from standardization samples, 
without requiring assumptions about the distribution of the raw data: 
Norm scores are directly established from raw data by modeling the 
latter ones as a function of both percentile scores and an explanatory 
variable (e.g., age). The method minimizes bias arising from sampling 
and measurement error, while handling marked deviations from normality, 
addressing bottom or ceiling effects and capturing almost all of the 
variance in the original norm data sample.

The R package is available via 
https://cran.r-project.org/web/packages/cNORM/
Comprehensive online tutorial: https://www.psychometrica.de/cNorm_en.html
If you like to access the developmental version: 
https://github.com/WLenhard/cNORM

And finally the original article and project updates via Researchgate:
- https://www.researchgate.net/project/Continuous-Norming
- 
https://www.researchgate.net/publication/303785307_A_Continuous_Solution_to_the_Norming_Problem

Best regards,
 ??? Wolfgang Lenhard

-- 
Prof. Dr. Wolfgang Lenhard
Institute for Psychologie IV
D-97070 W?rzburg, Germany
URL:  https://go.uniwue.de/lenhard

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From jeremieju@te @ending from gm@il@com  Mon Dec  3 21:13:00 2018
From: jeremieju@te @ending from gm@il@com (Jeremie Juste)
Date: Mon, 03 Dec 2018 21:13:00 +0100
Subject: [R] (no subject)
In-Reply-To: <CANjNGqpsq=Od-ywbuO5zE5Ky=1KxxBC03E-bFjTZPJHL-y695g@mail.gmail.com>
 (mahboobe akhlaghi's message of "Sun, 2 Dec 2018 15:21:47 +0330")
References: <CANjNGqpsq=Od-ywbuO5zE5Ky=1KxxBC03E-bFjTZPJHL-y695g@mail.gmail.com>
Message-ID: <87tvjuid2r.fsf@gmail.com>

Hello,

Can you provide more information?
What is your operating system?
What is your database management system (sqlite, mySQL,...)
What package do you use to import the data in R from the database?
What is the default language on your computer?
Are the variables not in farsi fine?

My guess is that Rstudion is a front end and have nothing to do with
your problem. But to be sure you can open R directly and check from there.

Some lines of codes would be helpful.

Best regards,
Jeremie




> Hi,
> I have a question about R Studio. I have a database in SQL. now, I combined
> SQL and R Studio. the levels of a variable is in farsi. so when I want to
> read some variables, in R Studio are "???????" . I want to show them in
> farsi. what should I do to have right words in R Studio?
> Thanks.


From g@d@@br@h@m @ending from gm@il@com  Tue Dec  4 00:55:29 2018
From: g@d@@br@h@m @ending from gm@il@com (Gad Abraham)
Date: Tue, 4 Dec 2018 10:55:29 +1100
Subject: [R] survival::survfit.coxph confidence vs prediction intervals
Message-ID: <CAOTmaJEKTKRZ3d+1VJJ8psjsXTCRu3TsVR21p5X7d30LjmAejQ@mail.gmail.com>

Dear list,

I'm trying to understand what kind of uncertainty intervals are
plotted for survival::survfit.coxph when 'conf.int=TRUE' is called,
e.g.:

f <- coxph(Surv(futime, fustat) ~ age, data = ovarian)
s <- survfit(f, newdata=data.frame(age=c(30, 60)))
plot(s, conf.int=TRUE)

Are these bands the confidence intervals for the survivor function or
prediction intervals? Is it possible to get prediction intervals?

Regards,
Gad


From g@d@@br@h@m @ending from gm@il@com  Tue Dec  4 03:59:41 2018
From: g@d@@br@h@m @ending from gm@il@com (Gad Abraham)
Date: Tue, 4 Dec 2018 13:59:41 +1100
Subject: [R] survival::survfit.coxph confidence vs prediction intervals
 r-help x
Message-ID: <CAOTmaJH4Vb7b9yXgvvug52tUb-77YPFJ0fF7KvOdjKAAOSC9iw@mail.gmail.com>

Dear list,

I'm trying to understand what kind of uncertainty intervals are
plotted for survival::survfit.coxph when 'conf.int=TRUE' is called,
e.g.:

f <- coxph(Surv(futime, fustat) ~ age, data = ovarian)
s <- survfit(f, newdata=data.frame(age=c(30, 60)))
plot(s, conf.int=TRUE)

Are these bands the confidence intervals for the survivor function or
prediction intervals? Is it possible to get prediction intervals?

Regards,
Gad


From zi@d@elmou@ly @ending from k@nt@r@com  Tue Dec  4 12:18:56 2018
From: zi@d@elmou@ly @ending from k@nt@r@com (Elmously, Ziad (TSHHM))
Date: Tue, 4 Dec 2018 11:18:56 +0000
Subject: [R] Weighting Document for Text Clouds
Message-ID: <AM5PR0101MB2402035526D445203E7266B696AF0@AM5PR0101MB2402.eurprd01.prod.exchangelabs.com>

Hello All,

The link below shows the 5 steps required to build a text cloud from a text file of documents (e.g., posts, tweets, etc.).

http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know

The R script in the link loads the text into a character vector, "text".  Basically, it is a vector of documents.

Is there an easy way to apply document weights (if attached to the raw documents)?  The weighting options in the documentation for  "TermDocumentMatrix" do not seem to be relevant.  One option is to use the weights to physically replicate the documents in "text".

However, I seek a more elegant solution.

Sincerely,

Ziad Elmously


Kantar Disclaimer<http://www.kantar.com/disclaimer.html>

	[[alternative HTML version deleted]]


From m@@ternh@ttt @ending from gm@il@com  Tue Dec  4 12:27:02 2018
From: m@@ternh@ttt @ending from gm@il@com (Thanh Tran)
Date: Tue, 4 Dec 2018 20:27:02 +0900
Subject: [R] D-optimum design - optimization model in R
Message-ID: <CAHjanSDxnAsoyShMRRZMz3LLi4NAheGrwd9g3n4DmfJAB67FMw@mail.gmail.com>

Dear all,



I'm trying to use the D-optimum design. In my data, the response is KIC,
and 4 factors are AC, AV, T, and Temp. A typical second-degree response
modeling is as follows:



> data<-read.csv("2.csv", header =T)

> mod <-
lm(KIC~AC+I(AC^2)+AV+I(AV^2)+T+I(T^2)+Temp+I(Temp^2)+AC:AV+AC:T+AC:Temp+AV:T+AV:Temp+T:Temp,

+ data = data)



The result of the model:



KIC = 4.85 ? 2.9AC +0.151 AV + 0.1094T

          + 0.0091Temp + 0.324 AC^2-0.0156V^2

          - 10.00106T^2 - 0.0009Temp^2 + 0.0071AC?AV

          - 0.00087AC?T -0.00083AC?Temp ? 0.0018AV?T

         +0.0015AV?Temp ? 0.000374 AV ? T



Based on the above response modelling, I want to determine levels of the
AC, AV, T, and Temp to have the Maximum value of KIC. The result running in
Minitab as is shown in Figure 1. In R, I try to compute an D-optimum design
with the following codes:



> attach(data)

> F.trig <- F.cube

> F.trip <-
F.cube(KIC~AC+I(AC^2)+AV+I(AV^2)+T+I(T^2)+Temp+I(Temp^2)+AC:AV+AC:T+AC:Temp+AV:T+AV:Temp+T:Temp,

+ c(4,4,30,5), # Smalesst values of AC,AV,T, and Temp

+ c(5,7,50,25), # Highest values of AC,AV,T, and Temp

+ c(3,3,3,3)) # Numbers of levels ofAC,AV,T, and Temp

> res.trip.D <- od.AA(F.trip,1,alg = "doom", crit = "D",

+ graph =1:7, t.max = 4)



I have the result as shown in Figure 2 but I cannot find out the optimum
design as shown in Figure 1 using Minitab.



If anyone has any experience about what would be the reason for error or
how I can solve it? I really appreciate your support and help.



Best regards,

Nhat Tran



Ps: I also added a CSV file for practicing R.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 2.png
Type: image/png
Size: 14594 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20181204/8168a093/attachment.png>

From bog@@o@chri@tofer @ending from gm@il@com  Tue Dec  4 13:51:03 2018
From: bog@@o@chri@tofer @ending from gm@il@com (Christofer Bogaso)
Date: Tue, 4 Dec 2018 18:21:03 +0530
Subject: [R] POSIXct format
Message-ID: <CA+dpOJmZgUaPAsPU8B3DsX6rOf6ozgOAOcbrtAWtGBKWuB7AkA@mail.gmail.com>

Hi,

I am trying to format my Character strings to POSIXct as below :

> as.POSIXct('2018-11-2700:00:00', format = "%Y-%m-%d%H:%M:%S", tz = 'UTC')
[1] "2018-11-27 UTC"
> as.POSIXct('2018-11-2701:00:00', format = "%Y-%m-%d%H:%M:%S", tz = 'UTC')
[1] "2018-11-27 01:00:00 UTC"

For the first case, I wanted to see "2018-11-27 00:00:00 UTC", but the
Hour/Min/Sec part was missing in R output. Is it possible to get consistent
formatting in both cases?

Thanks for your help. Regards

	[[alternative HTML version deleted]]


From btupper @ending from bigelow@org  Tue Dec  4 14:04:07 2018
From: btupper @ending from bigelow@org (Ben Tupper)
Date: Tue, 4 Dec 2018 08:04:07 -0500
Subject: [R] POSIXct format
In-Reply-To: <CA+dpOJmZgUaPAsPU8B3DsX6rOf6ozgOAOcbrtAWtGBKWuB7AkA@mail.gmail.com>
References: <CA+dpOJmZgUaPAsPU8B3DsX6rOf6ozgOAOcbrtAWtGBKWuB7AkA@mail.gmail.com>
Message-ID: <C8A24D90-B7AE-400A-8E6C-BAA50F4C7C10@bigelow.org>

Hi,

It is because the print method for POSIXct likely tries to simplify.  To see it in the format you desire, you need to be explicit when calling it.

x = as.POSIXct('2018-11-2700:00:00', format = "%Y-%m-%d%H:%M:%S", tz = 'UTC')
format(x, "%Y-%m-%d %H:%M:%S %Z")
[1] "2018-11-27 00:00:00 UTC"

Cheers,
Ben

> On Dec 4, 2018, at 7:51 AM, Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
> 
> Hi,
> 
> I am trying to format my Character strings to POSIXct as below :
> 
>> as.POSIXct('2018-11-2700:00:00', format = "%Y-%m-%d%H:%M:%S", tz = 'UTC')
> [1] "2018-11-27 UTC"
>> as.POSIXct('2018-11-2701:00:00', format = "%Y-%m-%d%H:%M:%S", tz = 'UTC')
> [1] "2018-11-27 01:00:00 UTC"
> 
> For the first case, I wanted to see "2018-11-27 00:00:00 UTC", but the
> Hour/Min/Sec part was missing in R output. Is it possible to get consistent
> formatting in both cases?
> 
> Thanks for your help. Regards
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org

Ecological Forecasting: https://eco.bigelow.org/


From bgunter@4567 @ending from gm@il@com  Tue Dec  4 15:16:36 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Tue, 4 Dec 2018 06:16:36 -0800
Subject: [R] D-optimum design - optimization model in R
In-Reply-To: <CAHjanSDxnAsoyShMRRZMz3LLi4NAheGrwd9g3n4DmfJAB67FMw@mail.gmail.com>
References: <CAHjanSDxnAsoyShMRRZMz3LLi4NAheGrwd9g3n4DmfJAB67FMw@mail.gmail.com>
Message-ID: <CAGxFJbQVpbQBJP_e9V1Ukq9aEKortkGJBV_Sf+PYyj+L5rVt1w@mail.gmail.com>

Please do not re-post. To increase your chance of getting a useful answer,
read and follow the posting guide below, which you have not yet done. For
example, what is "F.cube", what packages are you using? Also, this list is
about R programming, not statistics, which is more what your query seems to
be about. Finally, search yourself! -- e,g, "d-optimal designs" on rseek.org
brought up what appeared to be many relevant hits, including a CRAN task
view on experimental design.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Dec 4, 2018 at 3:29 AM Thanh Tran <masternhattt at gmail.com> wrote:

> Dear all,
>
>
>
> I'm trying to use the D-optimum design. In my data, the response is KIC,
> and 4 factors are AC, AV, T, and Temp. A typical second-degree response
> modeling is as follows:
>
>
>
> > data<-read.csv("2.csv", header =T)
>
> > mod <-
>
> lm(KIC~AC+I(AC^2)+AV+I(AV^2)+T+I(T^2)+Temp+I(Temp^2)+AC:AV+AC:T+AC:Temp+AV:T+AV:Temp+T:Temp,
>
> + data = data)
>
>
>
> The result of the model:
>
>
>
> KIC = 4.85 ? 2.9AC +0.151 AV + 0.1094T
>
>           + 0.0091Temp + 0.324 AC^2-0.0156V^2
>
>           - 10.00106T^2 - 0.0009Temp^2 + 0.0071AC?AV
>
>           - 0.00087AC?T -0.00083AC?Temp ? 0.0018AV?T
>
>          +0.0015AV?Temp ? 0.000374 AV ? T
>
>
>
> Based on the above response modelling, I want to determine levels of the
> AC, AV, T, and Temp to have the Maximum value of KIC. The result running in
> Minitab as is shown in Figure 1. In R, I try to compute an D-optimum design
> with the following codes:
>
>
>
> > attach(data)
>
> > F.trig <- F.cube
>
> > F.trip <-
>
> F.cube(KIC~AC+I(AC^2)+AV+I(AV^2)+T+I(T^2)+Temp+I(Temp^2)+AC:AV+AC:T+AC:Temp+AV:T+AV:Temp+T:Temp,
>
> + c(4,4,30,5), # Smalesst values of AC,AV,T, and Temp
>
> + c(5,7,50,25), # Highest values of AC,AV,T, and Temp
>
> + c(3,3,3,3)) # Numbers of levels ofAC,AV,T, and Temp
>
> > res.trip.D <- od.AA(F.trip,1,alg = "doom", crit = "D",
>
> + graph =1:7, t.max = 4)
>
>
>
> I have the result as shown in Figure 2 but I cannot find out the optimum
> design as shown in Figure 1 using Minitab.
>
>
>
> If anyone has any experience about what would be the reason for error or
> how I can solve it? I really appreciate your support and help.
>
>
>
> Best regards,
>
> Nhat Tran
>
>
>
> Ps: I also added a CSV file for practicing R.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From f@tm@@m@ci @ending from gm@il@com  Tue Dec  4 21:47:04 2018
From: f@tm@@m@ci @ending from gm@il@com (Fatma Ell)
Date: Tue, 4 Dec 2018 21:47:04 +0100
Subject: [R] Select behavior dependant with other factors and its
 formalization
Message-ID: <CAH3AuNB9U_SHMtF52tgPEoPSw2Gd+oBtutgzd4jivGZ1eM0LbQ@mail.gmail.com>

Hi,

I'm studying occurence of Behavior11, Behavior12,Behavior2,Behavior3 according
three variables :

Times : task time

Time_interval :task time in interval

Frequency:Frequency of the task

For this purpose, I use GLM

attach(datas)
an11=anova(glm(Behavior11 ~  Times + Frequency    ,
family=binomial),test="Chisq")
an12=anova(glm(Behavior12 ~  Times_interval+ Frequency   ,
family=binomial),test="Chisq")
an3=anova(glm(Behavior3 ~  Times_interval+ Frequency   ,
family=binomial),test="Chisq")
an2=anova(glm(Behavior2 ~  Times_interval+ Frequency   ,
family=binomial),test="Chisq")

I have different significant effect for every behavior. Odds value reveals
the direction of dependence exemple:

model1=glm(Behavior12 ~  Times_interval+ Frequency, family=binomial)
summary(model1)
exp(model1$coefficients)

Coefficients:
                      Estimate Std. Error z value Pr(>|z|)
(Intercept)           -2.84731    0.53984  -5.274 1.33e-07 ***
Times_interval(10,20]  0.20841    0.37449   0.557   0.5778
Times_interval(20,30]  0.65672    0.40334   1.628   0.1035
Times_interval(30,40]  0.49951    0.60114   0.831   0.4060
Times_interval(40,50] -0.18637    0.83938  -0.222   0.8243
Times_interval(50,60]  1.16718    0.70834   1.648   0.0994 .
Frequency              0.02714    0.01380   1.967   0.0492 *


exp(model1$coefficients)
 Frequency
 1.02750856

As frequency grows Behavior12 grows too at a given rate.

Now, my aim is to have something like equation predicting occurence of
behavior. My input data are Times , Time_intervaland Frequency. We will
have always the same percentage of each Frequencynumber and Times too,
exactly like the dataset

My output should be a generated behavior. If I execute n=1000 times eth
'equation'. *The ouput would be the same percentage of behavior than in the
dataset.*

How to obtain probalities values of behavior in link with input data, and
to formalize them in one equation ?

Thanks a lot

    datas=structure(list(Behavior2 = c(0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0), Behavior3 = c(1, 1, 1, 1, 1, 1, 1, 0, 1,
0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
1, 0, 0, 0, 0, 1, 0, 0, 0, 0), Behavior12 = c(0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0), Behavior11 = c(0, 0,
0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0), Frequency = c(11L,
11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 12L, 12L,
12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 11L, 11L, 11L,
11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 9L, 9L, 12L, 12L, 12L,
12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 31L, 31L, 31L, 31L,
31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L,
31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L,
31L, 31L, 31L, 31L, 31L, 31L, 12L, 12L, 12L, 12L, 12L, 12L, 12L,
12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 6L,
6L, 6L, 9L, 9L, 9L, 9L, 9L, 9L, 36L, 36L, 36L, 36L, 36L, 36L,
36L, 36L, 36L, 36L, 36L, 36L, 36L, 36L, 36L, 36L, 36L, 36L, 36L,
36L, 36L, 36L, 36L, 36L, 36L, 36L, 36L, 36L, 36L, 36L, 36L, 36L,
36L, 36L, 36L, 36L, 36L, 36L, 36L, 36L, 36L, 36L, 36L, 36L, 36L,
36L, 36L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 1L, 21L, 21L, 21L, 21L,
21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L,
21L, 21L, 21L, 21L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L,
11L, 11L, 6L, 6L, 6L, 6L, 6L, 6L, 5L, 5L, 5L, 5L, 5L, 18L, 18L,
18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L,
18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L,
18L, 18L, 18L, 42L, 42L, 42L, 42L, 42L, 42L, 42L, 42L, 42L, 42L,
42L, 42L, 42L, 42L, 42L, 42L, 42L, 42L, 42L, 42L, 42L, 42L, 42L,
42L, 42L, 42L, 42L, 42L, 42L, 42L, 42L, 42L, 42L, 42L, 42L, 42L,
42L, 42L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L,
12L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L,
18L, 18L, 18L, 11L, 11L, 11L, 11L, 11L, 11L, 9L, 9L, 9L, 12L,
12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L,
12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 6L, 6L, 6L, 6L, 6L, 6L,
11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L,
11L, 11L, 11L, 11L, 11L, 9L, 11L, 11L, 6L, 6L, 6L, 6L, 6L, 12L,
12L, 12L, 12L, 12L, 12L, 12L, 16L, 16L, 16L, 16L, 16L, 16L, 16L,
16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L,
16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L,
16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L,
16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L,
16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L,
16L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L,
25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L,
25L, 25L, 25L, 25L, 25L, 3L, 3L, 3L, 18L, 18L, 18L, 18L, 18L,
18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L,
43L, 43L, 43L, 43L, 43L, 43L, 43L, 43L, 43L, 43L, 43L, 43L, 43L,
43L, 43L, 43L, 43L, 43L, 43L, 43L, 43L, 43L, 43L, 43L, 43L, 43L,
43L, 43L, 43L, 43L, 43L, 43L, 43L, 43L, 43L, 43L, 43L, 43L, 43L,
43L, 43L, 43L, 43L), Times = c(15, 15, 15, 15, 15, 15, 15, 15,
15, 15, 15, 15, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,
17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 47, 47, 33, 33, 33,
33, 33, 33, 33, 33, 33, 33, 33, 33, 28, 28, 28, 28, 28, 28, 28,
28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,
28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 43, 44, 44,
44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 39,
29, 29, 29, 32, 32, 32, 32, 32, 32, 9, 9, 9, 9, 9, 9, 9, 9, 9,
9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 58, 58, 58,
58, 58, 58, 58, 38, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 51, 51, 51, 51, 51, 51,
51, 51, 51, 51, 51, 43, 43, 43, 43, 43, 43, 28, 28, 28, 28, 28,
4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 14, 14, 14, 14, 14, 14, 14, 14,
14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,
14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 33, 33,
33, 33, 33, 36, 36, 36, 36, 36, 36, 36, 9, 9, 9, 9, 9, 9, 9,
9, 9, 9, 9, 9, 9, 9, 9, 16, 16, 16, 16, 16, 16, 35, 35, 35, 30,
30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,
30, 30, 30, 30, 30, 37, 37, 37, 32, 32, 32, 18, 18, 18, 18, 18,
18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 30, 22, 22,
25, 25, 25, 25, 25, 50, 50, 50, 50, 50, 50, 50, 26, 26, 26, 26,
26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,
26, 26, 26, 26, 26, 26, 26, 26, 26, 23, 23, 23, 23, 23, 23, 23,
23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,
23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,
23, 23, 23, 23, 23, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,
17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,
17, 17, 17, 52, 52, 52, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,
11, 11, 11, 11, 11, 11, 11, 11, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6), Times_interval = structure(c(2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 4L, 4L, 4L, 4L, 4L,
4L, 4L, 4L, 4L, 4L, 4L, 4L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 5L, 5L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 3L, 3L, 3L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 4L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 4L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 5L, 5L, 5L, 5L, 5L, 5L, 3L, 3L, 3L,
3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
4L, 4L, 4L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 4L, 4L, 4L, 3L, 3L, 3L, 3L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 4L, 4L, 4L, 4L, 4L, 4L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 6L,
6L, 6L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), .Label =
c("(0,10]",
"(10,20]", "(20,30]", "(30,40]", "(40,50]", "(50,60]"), class =
"factor")), .Names = c("Behavior2",
"Behavior3", "Behavior12", "Behavior11", "Frequency", "Times",
"Times_interval"), class = "data.frame", row.names = c(1L, 2L,
3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L,
17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L,
30L, 31L, 32L, 33L, 34L, 35L, 36L, 37L, 38L, 39L, 40L, 41L, 42L,
43L, 44L, 45L, 46L, 47L, 48L, 49L, 50L, 51L, 52L, 53L, 54L, 55L,
56L, 57L, 58L, 59L, 60L, 61L, 62L, 63L, 64L, 65L, 66L, 67L, 68L,
69L, 70L, 71L, 72L, 73L, 74L, 75L, 76L, 77L, 78L, 79L, 80L, 81L,
82L, 83L, 84L, 85L, 86L, 87L, 88L, 89L, 90L, 91L, 92L, 93L, 94L,
95L, 96L, 97L, 98L, 99L, 100L, 101L, 102L, 103L, 104L, 105L,
106L, 107L, 108L, 109L, 110L, 111L, 112L, 113L, 114L, 115L, 116L,
117L, 118L, 119L, 120L, 121L, 122L, 123L, 124L, 125L, 126L, 127L,
128L, 129L, 130L, 131L, 132L, 133L, 134L, 135L, 136L, 137L, 138L,
139L, 140L, 141L, 142L, 143L, 144L, 145L, 146L, 147L, 148L, 149L,
150L, 151L, 152L, 153L, 154L, 155L, 156L, 157L, 158L, 159L, 160L,
161L, 162L, 163L, 164L, 165L, 166L, 167L, 168L, 169L, 170L, 171L,
172L, 173L, 174L, 175L, 176L, 177L, 178L, 179L, 180L, 181L, 182L,
183L, 184L, 185L, 186L, 187L, 188L, 189L, 190L, 191L, 192L, 193L,
194L, 195L, 196L, 197L, 198L, 199L, 200L, 201L, 202L, 203L, 204L,
205L, 206L, 207L, 208L, 209L, 210L, 211L, 212L, 213L, 214L, 215L,
216L, 217L, 218L, 219L, 220L, 221L, 222L, 223L, 224L, 225L, 226L,
227L, 228L, 229L, 230L, 231L, 232L, 233L, 234L, 235L, 236L, 237L,
238L, 239L, 240L, 241L, 242L, 243L, 244L, 245L, 246L, 247L, 248L,
249L, 250L, 251L, 252L, 253L, 254L, 255L, 256L, 257L, 258L, 259L,
260L, 261L, 262L, 263L, 264L, 265L, 266L, 267L, 268L, 269L, 270L,
271L, 272L, 273L, 274L, 275L, 276L, 277L, 278L, 279L, 280L, 281L,
282L, 283L, 284L, 285L, 286L, 287L, 288L, 289L, 290L, 291L, 292L,
293L, 294L, 295L, 296L, 297L, 298L, 299L, 300L, 301L, 302L, 303L,
304L, 305L, 306L, 307L, 308L, 309L, 310L, 311L, 312L, 313L, 314L,
315L, 316L, 317L, 318L, 319L, 320L, 321L, 322L, 323L, 324L, 325L,
326L, 327L, 328L, 329L, 330L, 331L, 332L, 333L, 334L, 335L, 336L,
337L, 338L, 339L, 340L, 341L, 342L, 343L, 344L, 345L, 346L, 347L,
348L, 349L, 350L, 351L, 352L, 353L, 354L, 355L, 356L, 357L, 358L,
359L, 360L, 361L, 362L, 363L, 364L, 365L, 366L, 367L, 368L, 369L,
370L, 371L, 372L, 373L, 374L, 375L, 376L, 377L, 378L, 379L, 380L,
381L, 382L, 383L, 384L, 385L, 386L, 387L, 388L, 389L, 390L, 391L,
392L, 393L, 394L, 395L, 396L, 397L, 398L, 399L, 400L, 401L, 402L,
403L, 404L, 405L, 406L, 407L, 408L, 409L, 410L, 411L, 412L, 413L,
414L, 415L, 416L, 417L, 418L, 419L, 420L, 421L, 422L, 423L, 424L,
425L, 426L, 427L, 428L, 429L, 430L, 431L, 432L, 433L, 434L, 435L,
436L, 437L, 438L, 439L, 440L, 441L, 442L, 443L, 444L, 445L, 446L,
447L, 448L, 449L, 450L, 451L, 452L, 453L, 454L, 455L, 456L, 457L,
458L, 459L, 460L, 461L, 462L, 463L, 464L, 465L, 466L, 467L, 468L,
469L, 470L, 471L, 472L, 473L, 474L, 475L, 476L, 477L, 478L, 479L,
480L, 481L, 482L, 483L, 485L, 486L, 487L, 488L, 489L, 490L, 491L,
492L, 493L, 494L, 495L, 496L, 497L, 498L, 499L, 500L, 501L, 502L,
503L, 504L, 505L, 506L, 507L, 508L, 509L, 510L, 511L, 512L, 513L,
514L, 515L, 516L, 517L, 518L, 519L, 520L, 521L, 522L, 523L, 524L,
525L, 526L, 527L, 528L, 529L, 530L, 531L, 532L, 533L, 534L, 535L,
536L, 537L, 538L, 539L, 540L, 541L, 542L, 543L, 544L, 545L))

	[[alternative HTML version deleted]]


From ro@iefleming @ending from hotm@il@co@uk  Tue Dec  4 23:06:43 2018
From: ro@iefleming @ending from hotm@il@co@uk (rosie fleming)
Date: Tue, 4 Dec 2018 22:06:43 +0000
Subject: [R] Random projection
Message-ID: <HE1PR0801MB27143270020903A3708D590D92AF0@HE1PR0801MB2714.eurprd08.prod.outlook.com>

Hello I was wondering if anyone could help. I am using R to perform random projection on Gaussians. 
I need to take k Gaussian random vectors U1,...,Uk in R(reals) 
Then for any vector v in d dimensions, perform the random projection: 
f(v) = v.U1, v.U2, ..., v.Uk which reduces v from d dimensions to k dimensions 
So i need v1,...vn so n vectors 

and then the basic idea is that the relative orders will be preserved between the vectors v after projection ie the nearet neighbours for each vectors will be the same before projection (v1,v2,...,vn) and after projection f(v1),...f(vk) 

I'm struggling to produce this in R. I've managed to make a matrix for U and V 
However I now need to use the scalar product to perform the projection, taking each row of v (vi) and multiplying it with every column of the U matrix. I need to perform this for n vectors and have a matrix as the outcome. 

This would then allow me to find the distances between each f(vi)! 

Any help would be greatly appreciated, thankyou!

From jdnewmil @ending from dcn@d@vi@@c@@u@  Wed Dec  5 00:32:36 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Tue, 04 Dec 2018 15:32:36 -0800
Subject: [R] Random projection
In-Reply-To: <HE1PR0801MB27143270020903A3708D590D92AF0@HE1PR0801MB2714.eurprd08.prod.outlook.com>
References: <HE1PR0801MB27143270020903A3708D590D92AF0@HE1PR0801MB2714.eurprd08.prod.outlook.com>
Message-ID: <37A69264-4308-4798-A2B4-C9FF90263316@dcn.davis.ca.us>

Your multiplication description sounds more like matrix multiplication than scalar multiplication. R supports matrix multiplication with the %*% operator.

I didn't follow the rest of your description... are you doing a homework assignment? There is a no homework policy on this list...

On December 4, 2018 2:06:43 PM PST, rosie fleming <rosiefleming at hotmail.co.uk> wrote:
>Hello I was wondering if anyone could help. I am using R to perform
>random projection on Gaussians. 
>I need to take k Gaussian random vectors U1,...,Uk in R(reals) 
>Then for any vector v in d dimensions, perform the random projection: 
>f(v) = v.U1, v.U2, ..., v.Uk which reduces v from d dimensions to k
>dimensions 
>So i need v1,...vn so n vectors 
>
>and then the basic idea is that the relative orders will be preserved
>between the vectors v after projection ie the nearet neighbours for
>each vectors will be the same before projection (v1,v2,...,vn) and
>after projection f(v1),...f(vk) 
>
>I'm struggling to produce this in R. I've managed to make a matrix for
>U and V 
>However I now need to use the scalar product to perform the projection,
>taking each row of v (vi) and multiplying it with every column of the U
>matrix. I need to perform this for n vectors and have a matrix as the
>outcome. 
>
>This would then allow me to find the distances between each f(vi)! 
>
>Any help would be greatly appreciated, thankyou!
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From m@@ternh@ttt @ending from gm@il@com  Wed Dec  5 00:44:25 2018
From: m@@ternh@ttt @ending from gm@il@com (Thanh Tran)
Date: Wed, 5 Dec 2018 08:44:25 +0900
Subject: [R] D-optimum design - optimization model in R
In-Reply-To: <CAGxFJbQVpbQBJP_e9V1Ukq9aEKortkGJBV_Sf+PYyj+L5rVt1w@mail.gmail.com>
References: <CAHjanSDxnAsoyShMRRZMz3LLi4NAheGrwd9g3n4DmfJAB67FMw@mail.gmail.com>
 <CAGxFJbQVpbQBJP_e9V1Ukq9aEKortkGJBV_Sf+PYyj+L5rVt1w@mail.gmail.com>
Message-ID: <CAHjanSCNVnSON-Vkwqu3xbcwJFpCgQMzxeAXNUr+KmBNbwL82Q@mail.gmail.com>

Dear  Bert Gunter,

Thank you for your advice. I will take care in the next post.

Best regards,
Nhat Tran.

V?o Th 3, 4 thg 12, 2018 va?o lu?c 23:16 Bert Gunter <bgunter.4567 at gmail.com>
?? vi?t:

> Please do not re-post. To increase your chance of getting a useful answer,
> read and follow the posting guide below, which you have not yet done. For
> example, what is "F.cube", what packages are you using? Also, this list is
> about R programming, not statistics, which is more what your query seems to
> be about. Finally, search yourself! -- e,g, "d-optimal designs" on
> rseek.org brought up what appeared to be many relevant hits, including a
> CRAN task view on experimental design.
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Tue, Dec 4, 2018 at 3:29 AM Thanh Tran <masternhattt at gmail.com> wrote:
>
>> Dear all,
>>
>>
>>
>> I'm trying to use the D-optimum design. In my data, the response is KIC,
>> and 4 factors are AC, AV, T, and Temp. A typical second-degree response
>> modeling is as follows:
>>
>>
>>
>> > data<-read.csv("2.csv", header =T)
>>
>> > mod <-
>>
>> lm(KIC~AC+I(AC^2)+AV+I(AV^2)+T+I(T^2)+Temp+I(Temp^2)+AC:AV+AC:T+AC:Temp+AV:T+AV:Temp+T:Temp,
>>
>> + data = data)
>>
>>
>>
>> The result of the model:
>>
>>
>>
>> KIC = 4.85 ? 2.9AC +0.151 AV + 0.1094T
>>
>>           + 0.0091Temp + 0.324 AC^2-0.0156V^2
>>
>>           - 10.00106T^2 - 0.0009Temp^2 + 0.0071AC?AV
>>
>>           - 0.00087AC?T -0.00083AC?Temp ? 0.0018AV?T
>>
>>          +0.0015AV?Temp ? 0.000374 AV ? T
>>
>>
>>
>> Based on the above response modelling, I want to determine levels of the
>> AC, AV, T, and Temp to have the Maximum value of KIC. The result running
>> in
>> Minitab as is shown in Figure 1. In R, I try to compute an D-optimum
>> design
>> with the following codes:
>>
>>
>>
>> > attach(data)
>>
>> > F.trig <- F.cube
>>
>> > F.trip <-
>>
>> F.cube(KIC~AC+I(AC^2)+AV+I(AV^2)+T+I(T^2)+Temp+I(Temp^2)+AC:AV+AC:T+AC:Temp+AV:T+AV:Temp+T:Temp,
>>
>> + c(4,4,30,5), # Smalesst values of AC,AV,T, and Temp
>>
>> + c(5,7,50,25), # Highest values of AC,AV,T, and Temp
>>
>> + c(3,3,3,3)) # Numbers of levels ofAC,AV,T, and Temp
>>
>> > res.trip.D <- od.AA(F.trip,1,alg = "doom", crit = "D",
>>
>> + graph =1:7, t.max = 4)
>>
>>
>>
>> I have the result as shown in Figure 2 but I cannot find out the optimum
>> design as shown in Figure 1 using Minitab.
>>
>>
>>
>> If anyone has any experience about what would be the reason for error or
>> how I can solve it? I really appreciate your support and help.
>>
>>
>>
>> Best regards,
>>
>> Nhat Tran
>>
>>
>>
>> Ps: I also added a CSV file for practicing R.
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From y@@@@_m@lik@ @ending from y@hoo@fr  Wed Dec  5 15:30:40 2018
From: y@@@@_m@lik@ @ending from y@hoo@fr (malika yassa)
Date: Wed, 5 Dec 2018 14:30:40 +0000 (UTC)
Subject: [R] sustraction of two vectors of matrix
References: <8564099.3999832.1544020240398.ref@mail.yahoo.com>
Message-ID: <8564099.3999832.1544020240398@mail.yahoo.com>

helloplease?? I want to make a sustration of two vectors of a matrix
i have this program

aa<-matrix(outer(0:3,0:4,function(x,y) x+y*2),nrow=4,ncol=5)
for(i in 1:4)
+ {for(j in 2:5)
+ {bb[i,j-1]=aa[i,j]-aa[i,j-1]
+ }
+ }
at the end i obtain the bb=matrix( nrow=4,ncol=4)
but i cann't obtain this matrix
thank you very much




	[[alternative HTML version deleted]]


From b@ye@i@nlogic@1 @ending from gm@il@com  Wed Dec  5 15:36:26 2018
From: b@ye@i@nlogic@1 @ending from gm@il@com (Jan Galkowski)
Date: Wed, 05 Dec 2018 09:36:26 -0500
Subject: [R] Random projection
Message-ID: <1544020586.1684289.1599740488.6624ED95@webmail.messagingengine.com>

Ms Fleming,

This blog post (of mine) may be of interest and hopefully of help:

https://667-per-cm.net/2018/11/20/the-johnson-lindenstrauss-lemma-and-the-paradoxical-power-of-random-linear-operators-part-1/
Cheers!

--
Jan Galkowski (o?)
Westwood, MA

(pronouns: *he, him, his*)

*Keep your energy local*.  --John Farrell, *ILSR[1]*



Links:

  1. http://ilsr.org

	[[alternative HTML version deleted]]


From ruipb@rr@d@@ @ending from @@po@pt  Wed Dec  5 17:51:16 2018
From: ruipb@rr@d@@ @ending from @@po@pt (Rui Barradas)
Date: Wed, 5 Dec 2018 16:51:16 +0000
Subject: [R] sustraction of two vectors of matrix
In-Reply-To: <8564099.3999832.1544020240398@mail.yahoo.com>
References: <8564099.3999832.1544020240398.ref@mail.yahoo.com>
 <8564099.3999832.1544020240398@mail.yahoo.com>
Message-ID: <d2e7c79e-bc34-c3b4-bce9-8e95bff25153@sapo.pt>

Hello,

1) You don't need matrix(outer(etc)), outer already returns a matrix.
2) You need to create bb first.

aa <- outer(0:3, 0:4, function(x,y) x + y*2)

bb <- matrix(nrow = 4, ncol = 4)

for(i in 1:4){
   for(j in 2:5){
     bb[i, j - 1] <- aa[i, j] - aa[i, j - 1]
   }
}

bb


Hope this helps,

Rui Barradas


?s 14:30 de 05/12/2018, malika yassa via R-help escreveu:
> helloplease?? I want to make a sustration of two vectors of a matrix
> i have this program
> 
> aa<-matrix(outer(0:3,0:4,function(x,y) x+y*2),nrow=4,ncol=5)
> for(i in 1:4)
> + {for(j in 2:5)
> + {bb[i,j-1]=aa[i,j]-aa[i,j-1]
> + }
> + }
> at the end i obtain the bb=matrix( nrow=4,ncol=4)
> but i cann't obtain this matrix
> thank you very much
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jdnewmil @ending from dcn@d@vi@@c@@u@  Wed Dec  5 19:07:18 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Wed, 05 Dec 2018 10:07:18 -0800
Subject: [R] sustraction of two vectors of matrix
In-Reply-To: <d2e7c79e-bc34-c3b4-bce9-8e95bff25153@sapo.pt>
References: <8564099.3999832.1544020240398.ref@mail.yahoo.com>
 <8564099.3999832.1544020240398@mail.yahoo.com>
 <d2e7c79e-bc34-c3b4-bce9-8e95bff25153@sapo.pt>
Message-ID: <938F3C2D-BDA8-464A-A6D7-1C257173E5D7@dcn.davis.ca.us>

or with no loops and no preallocation:

bb <- aa[ , 2:5 ] - aa[ , 1:4 ]


On December 5, 2018 8:51:16 AM PST, Rui Barradas <ruipbarradas at sapo.pt> wrote:
>Hello,
>
>1) You don't need matrix(outer(etc)), outer already returns a matrix.
>2) You need to create bb first.
>
>aa <- outer(0:3, 0:4, function(x,y) x + y*2)
>
>bb <- matrix(nrow = 4, ncol = 4)
>
>for(i in 1:4){
>   for(j in 2:5){
>     bb[i, j - 1] <- aa[i, j] - aa[i, j - 1]
>   }
>}
>
>bb
>
>
>Hope this helps,
>
>Rui Barradas
>
>
>?s 14:30 de 05/12/2018, malika yassa via R-help escreveu:
>> helloplease?? I want to make a sustration of two vectors of a matrix
>> i have this program
>> 
>> aa<-matrix(outer(0:3,0:4,function(x,y) x+y*2),nrow=4,ncol=5)
>> for(i in 1:4)
>> + {for(j in 2:5)
>> + {bb[i,j-1]=aa[i,j]-aa[i,j-1]
>> + }
>> + }
>> at the end i obtain the bb=matrix( nrow=4,ncol=4)
>> but i cann't obtain this matrix
>> thank you very much
>> 
>> 
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From cory@c@@c@lheir@ @ending from gm@il@com  Wed Dec  5 23:21:51 2018
From: cory@c@@c@lheir@ @ending from gm@il@com (Cory C)
Date: Wed, 5 Dec 2018 17:21:51 -0500
Subject: [R] Simple-Effects Test for Unbalanced Mixed ANOVA
Message-ID: <CAGtWZ=0wv+qGTYcaB_7mLmyRoc7-TArhPq=9p37NS0b59DP6Hg@mail.gmail.com>

Hello everyone.

I am working on an Open Stats Lab (https://sites.trinity.edu/osl/) activity
for factorial ANOVA and have posted my analysis on GitHub (link below).

Once I have a significant interaction, how do I conduct a simple-effects
test in R?

The mixed design is unbalanced as follows:

1. within-subjects factor: Time 1, Time 2;
2. between-subjects factor: Condition 1 (ordinary; n = 64), Condition 2
(extraordinary; n = 66).

I imported the data and transformed it into long format.

    library(broom)
    library(car)
    library(nlme)
    library(tidyverse)

    > zhang_long[, 1:4]
    # A tibble: 260 x 4
       condition subject_id extra_time extra_rating
       <fct>     <fct>      <fct>             <dbl>
     1 ordinary  1          t1_extra              1
     2 ordinary  2          t1_extra              3
     3 ordinary  3          t1_extra              1
     4 ordinary  4          t1_extra              1
     5 ordinary  5          t1_extra              5
     6 ordinary  6          t1_extra              2
     7 ordinary  7          t1_extra              2
     8 ordinary  8          t1_extra              3
     9 ordinary  9          t1_extra              4
    10 ordinary  10         t1_extra              6
    # ... with 250 more rows

Since this is a practice activity, I can test my analyses against the
published paper. I was able to calculate the appropriate F-value using
car::Anova and nlme::lme.

    # Create linear mixed-effects model
    extra_lme <- lme(extra_rating ~ condition*extra_time, random =
~1|subject_id,
                     data = zhang_long)

    # Print ANOVA summary using type III sum of squares
    options(contrasts = c("contr.sum", "contr.poly"))
    Anova(extra_lme, type = "III")

But now I am stuck. How do I conduct a simple-effects test for the
interaction between time and condition?

I tried to subset the data by condition.

    # Split data by condition
    ordinary <- zhang_long %>% filter(condition == "ordinary")

    # Simple-effects test
    Anova(lme(extra_rating ~ extra_time, random = ~1|subject_id, data =
ordinary), type = "III")

This got me very close to the F-value reported in the paper, which is F(1,
128) = **39.86**.

    Analysis of Deviance Table (Type III tests)

    Response: extra_rating
                 Chisq Df Pr(>Chisq)
    (Intercept) 610.25  1  < 2.2e-16 ***
    extra_time   37.81  1  7.797e-10 ***
    ---
    Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

What am I doing wrong?

I uploaded a script and Rmarkdown document to GitHub:
https://github.com/corycaaz/osl-zhang-et-al-2014

Any help would be greatly appreciated!

Thank you,

Cory J. Cascalheira

	[[alternative HTML version deleted]]


From rdi@z02 @ending from gm@il@com  Thu Dec  6 01:33:52 2018
From: rdi@z02 @ending from gm@il@com (Ramon Diaz-Uriarte)
Date: Thu, 06 Dec 2018 01:33:52 +0100
Subject: [R] Strange degrees of freedom and SS from car::Anova with type II
 SS?
Message-ID: <87a7ljlci7.fsf@gmail.com>


Dear All,

I do not understand the degrees of freedom returned by car::Anova under
some models. They seem to be too many (e.g., numerical variables getting
more than 1 df, factors getting more df than levels there are).

This is a reproducible example:

library(car)
data(Prestige)

## Make sure no issues from NAs in comparisons of SS below
prestige_nona <- na.omit(Prestige)

Anova(lm(prestige ~ women * type * income * education,
         data = prestige_nona))

## Notice how women, a numerical variable, has 3 df
## and type (factor with 3 levels) has 4 df.


## In contrast this seems to get the df right:
Anova(lm(prestige ~ women * type * income * education,
         data = prestige_nona), type = "III")

## And also gives the df I'd expect
anova(lm(prestige ~ women * type * income * education,
         data = prestige_nona))



## Type II SS for women in the above model I do not understand either.
m_1 <- lm(prestige ~ type * income * education, data = prestige_nona)
m_2 <- lm(prestige ~ type * income * education + women, data = prestige_nona)
## Does not match women SS
sum(residuals(m_1)^2) - sum(residuals(m_2)^2)

## See [1] below for examples where they match.


Looking at the code, I do not understand what the call from
linearHypothesis returns here (specially compared to other models), and the
problem seems to be in the return from ConjComp, possibly due to the the
vcov of the model? (But this is over my head).


I understand this is not a reasonable model to fit, and there are possibly
serious collinearity problems. But I was surprised by the dfs in the
absence of any warning of something gone wrong. So I think there is
something very basic I do not understand.



Thanks,


R.


[1] In contrast, in other models I see what I'd expect. For example:

## 1 df for women, 2 for type
Anova(lm(prestige ~ type * income * women, data = prestige_nona))
m_1 <- lm(prestige ~ type * income, data = prestige_nona)
m_2 <- lm(prestige ~ type * income + women, data = prestige_nona)
## Type II SS for women
sum(residuals(m_1)^2) - sum(residuals(m_2)^2)

## 1 df for women, income, education
Anova(lm(prestige ~ education * income * women, data = prestige_nona))
m_1 <- lm(prestige ~ education * income, data = prestige_nona)
m_2 <- lm(prestige ~ education * income + women, data = prestige_nona)
## Type II SS for women
sum(residuals(m_1)^2) - sum(residuals(m_2)^2)




--
Ramon Diaz-Uriarte
Department of Biochemistry, Lab B-25
Facultad de Medicina
Universidad Aut?noma de Madrid
Arzobispo Morcillo, 4
28029 Madrid
Spain

Phone: +34-91-497-2412

Email: rdiaz02 at gmail.com
       ramon.diaz at iib.uam.es

http://ligarto.org/rdiaz


From jfox @ending from mcm@@ter@c@  Thu Dec  6 03:43:06 2018
From: jfox @ending from mcm@@ter@c@ (Fox, John)
Date: Thu, 6 Dec 2018 02:43:06 +0000
Subject: [R] 
 Strange degrees of freedom and SS from car::Anova with type II SS?
In-Reply-To: <20557_1544061329_wB61tSdm019325_87a7ljlci7.fsf@gmail.com>
References: <20557_1544061329_wB61tSdm019325_87a7ljlci7.fsf@gmail.com>
Message-ID: <D00CDC93-4C1D-4A8E-ACD7-8AA64E868227@mcmaster.ca>

Dear R.,

The problem you constructed is too ill-conditioned for the method that Anova() uses to compute type-II sums of squares and the associated degrees of freedom, with an immense condition number of the coefficient covariance matrix:

> library(car)
Loading required package: carData

> mod <- lm(prestige ~ women * type * income * education, data=Prestige)
> e <- eigen(vcov(mod))$values
> max(e)/min(e)
[1] 2.776205e+17

Simply centering the numerical predictors reduces the condition number by a factor of 10^3, which allows Anova() to work, even though the problem is still extremely ill-conditioned:

> Prestige.c <- within(Prestige, {
+   income <- income - mean(income)
+   education <- education - mean(education)
+   women <- women - mean(women)
+ })
> mod.c <- lm(prestige ~ women * type * income * education, data=Prestige.c)
> e.c <- eigen(vcov(mod.c))$values
> max(e)/min(e)
[1] 2.776205e+17

> Anova(mod.c)
Anova Table (Type II tests)

Response: prestige
                             Sum Sq Df F value    Pr(>F)    
women                        167.29  1  4.9516 0.0291142 *  
type                         744.30  2 11.0150 6.494e-05 ***
income                       789.00  1 23.3529 7.112e-06 ***
education                    699.54  1 20.7050 2.057e-05 ***
women:type                   140.32  2  2.0766 0.1326023    
women:income                  33.14  1  0.9807 0.3252424    
type:income                  653.40  2  9.6697 0.0001859 ***
women:education               30.36  1  0.8986 0.3462316    
type:education                 0.72  2  0.0107 0.9893462    
income:education               7.88  1  0.2331 0.6306681    
women:type:income            136.80  2  2.0245 0.1393087    
women:type:education         140.18  2  2.0745 0.1328633    
women:income:education       100.42  1  2.9722 0.0888832 .  
type:income:education         82.02  2  1.2138 0.3029069    
women:type:income:education    2.05  2  0.0303 0.9701334    
Residuals                   2500.16 74                      
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

> mod.c.2 <- update(mod.c, . ~ . - women:type:income:education)
> sum(residuals(mod.c.2)^2) - sum(residuals(mod.c)^2)
[1] 2.049735

Beyond demonstrating that the algorithm that Anova() uses can be made to fail if the coefficient covariance matrix is sufficiently ill-conditioned problem, I?m not sure what the point of this is. I suppose that we could try to detect this condition, which falls in the small region between where lm() detects a singularity and the projections used by Anova() break down.

Best,
 John

-------------------------------------------------
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
Web: http::/socserv.mcmaster.ca/jfox

> On Dec 5, 2018, at 7:33 PM, Ramon Diaz-Uriarte <rdiaz02 at gmail.com> wrote:
> 
> 
> Dear All,
> 
> I do not understand the degrees of freedom returned by car::Anova under
> some models. They seem to be too many (e.g., numerical variables getting
> more than 1 df, factors getting more df than levels there are).
> 
> This is a reproducible example:
> 
> library(car)
> data(Prestige)
> 
> ## Make sure no issues from NAs in comparisons of SS below
> prestige_nona <- na.omit(Prestige)
> 
> Anova(lm(prestige ~ women * type * income * education,
>         data = prestige_nona))
> 
> ## Notice how women, a numerical variable, has 3 df
> ## and type (factor with 3 levels) has 4 df.
> 
> 
> ## In contrast this seems to get the df right:
> Anova(lm(prestige ~ women * type * income * education,
>         data = prestige_nona), type = "III")
> 
> ## And also gives the df I'd expect
> anova(lm(prestige ~ women * type * income * education,
>         data = prestige_nona))
> 
> 
> 
> ## Type II SS for women in the above model I do not understand either.
> m_1 <- lm(prestige ~ type * income * education, data = prestige_nona)
> m_2 <- lm(prestige ~ type * income * education + women, data = prestige_nona)
> ## Does not match women SS
> sum(residuals(m_1)^2) - sum(residuals(m_2)^2)
> 
> ## See [1] below for examples where they match.
> 
> 
> Looking at the code, I do not understand what the call from
> linearHypothesis returns here (specially compared to other models), and the
> problem seems to be in the return from ConjComp, possibly due to the the
> vcov of the model? (But this is over my head).
> 
> 
> I understand this is not a reasonable model to fit, and there are possibly
> serious collinearity problems. But I was surprised by the dfs in the
> absence of any warning of something gone wrong. So I think there is
> something very basic I do not understand.
> 
> 
> 
> Thanks,
> 
> 
> R.
> 
> 
> [1] In contrast, in other models I see what I'd expect. For example:
> 
> ## 1 df for women, 2 for type
> Anova(lm(prestige ~ type * income * women, data = prestige_nona))
> m_1 <- lm(prestige ~ type * income, data = prestige_nona)
> m_2 <- lm(prestige ~ type * income + women, data = prestige_nona)
> ## Type II SS for women
> sum(residuals(m_1)^2) - sum(residuals(m_2)^2)
> 
> ## 1 df for women, income, education
> Anova(lm(prestige ~ education * income * women, data = prestige_nona))
> m_1 <- lm(prestige ~ education * income, data = prestige_nona)
> m_2 <- lm(prestige ~ education * income + women, data = prestige_nona)
> ## Type II SS for women
> sum(residuals(m_1)^2) - sum(residuals(m_2)^2)
> 
> 
> 
> 
> --
> Ramon Diaz-Uriarte
> Department of Biochemistry, Lab B-25
> Facultad de Medicina
> Universidad Aut?noma de Madrid
> Arzobispo Morcillo, 4
> 28029 Madrid
> Spain
> 
> Phone: +34-91-497-2412
> 
> Email: rdiaz02 at gmail.com
>       ramon.diaz at iib.uam.es
> 
> http://ligarto.org/rdiaz
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m@rc@lpl@n@ @ending from gm@il@com  Thu Dec  6 07:07:01 2018
From: m@rc@lpl@n@ @ending from gm@il@com (Marcal Plans)
Date: Thu, 6 Dec 2018 00:07:01 -0600
Subject: [R] Protein profile by NMF
Message-ID: <CAL6qZrL_u4r3z75VaF-QAHZ_NamYnxmYj9d4ynxVgOYnFDAcXQ@mail.gmail.com>

Hi community,
I have an interesting challenge that i think that it can be resolved using
NMF.
I have a total of 52 product with different protein profiles. I would like
to make two protein premixes (two unique profiles) to create all the SKU
using different concentrations of them (mixing). I have tried
the  following code and it partially works. The main problem is that the
levels that i have in the spec are the minimum protein concentrations
[minimum guarantee].
The solution that I got is the average that minimize the error, but with
this profile of proteins and concentrations I have profiles that do not
reach the minimum requirements. We can overdose, but we cannot under dose.

Code:

Data=read.table("clipboard", header=T, sep="\t", dec=".")
res=nmf(Data[,-1],2, nrun=30, seed=12345)
C=as.matrix(.coef(res)*100000)
S=as.matrix(basis(res)/100000)

#write.csv(S%*%C, "Test1.csv")
DD=(Data[,-1]-S%*%C)/(Data[,-1])*100+100
summary(DD)

Is there any way to add some restrictions to the NMF? for example W%*%H>
Data
Is there a better algorithm to factorize this data set?

Thank you so much for your help!
Cheers,
Marcal

From rdi@z02 @ending from gm@il@com  Thu Dec  6 10:44:46 2018
From: rdi@z02 @ending from gm@il@com (Ramon Diaz-Uriarte)
Date: Thu, 06 Dec 2018 10:44:46 +0100
Subject: [R] 
 Strange degrees of freedom and SS from car::Anova with type II SS?
In-Reply-To: <D00CDC93-4C1D-4A8E-ACD7-8AA64E868227@mcmaster.ca>
References: <20557_1544061329_wB61tSdm019325_87a7ljlci7.fsf@gmail.com>
 <D00CDC93-4C1D-4A8E-ACD7-8AA64E868227@mcmaster.ca>
Message-ID: <878t13kn01.fsf@gmail.com>

Dear John,

Thank you very much for your reply.


On Thu, 06-December-2018, at 03:43:06, Fox, John <jfox at mcmaster.ca> wrote:
> Dear R.,
>
> The problem you constructed is too ill-conditioned for the method that
> Anova() uses to compute type-II sums of squares and the associated
> degrees of freedom, with an immense condition number of the coefficient
> covariance matrix:

As I said, I understand this is not a sensible model to fit (this was a
quick example that I put together on the fly during a class when talking
about models with high-order interactions involving factors and numerical
variables). I forgot to mention that I also checked the eigenvalues of vcov
as well as the VIFs and they all indicated likely trouble.


>
>> library(car)
> Loading required package: carData
>
>> mod <- lm(prestige ~ women * type * income * education, data=Prestige)
>> e <- eigen(vcov(mod))$values
>> max(e)/min(e)
> [1] 2.776205e+17
>
> Simply centering the numerical predictors reduces the condition number by
> a factor of 10^3, which allows Anova() to work, even though the problem
> is still extremely ill-conditioned:

Thanks for pointing that out. I should have done that!


>
>> Prestige.c <- within(Prestige, {
> +   income <- income - mean(income)
> +   education <- education - mean(education)
> +   women <- women - mean(women)
> + })
>> mod.c <- lm(prestige ~ women * type * income * education, data=Prestige.c)
>> e.c <- eigen(vcov(mod.c))$values
>> max(e)/min(e)
> [1] 2.776205e+17
>
>> Anova(mod.c)
> Anova Table (Type II tests)
>
> Response: prestige
>                              Sum Sq Df F value    Pr(>F)
> women                        167.29  1  4.9516 0.0291142 *
> type                         744.30  2 11.0150 6.494e-05 ***
> income                       789.00  1 23.3529 7.112e-06 ***
> education                    699.54  1 20.7050 2.057e-05 ***
> women:type                   140.32  2  2.0766 0.1326023
> women:income                  33.14  1  0.9807 0.3252424
> type:income                  653.40  2  9.6697 0.0001859 ***
> women:education               30.36  1  0.8986 0.3462316
> type:education                 0.72  2  0.0107 0.9893462
> income:education               7.88  1  0.2331 0.6306681
> women:type:income            136.80  2  2.0245 0.1393087
> women:type:education         140.18  2  2.0745 0.1328633
> women:income:education       100.42  1  2.9722 0.0888832 .
> type:income:education         82.02  2  1.2138 0.3029069
> women:type:income:education    2.05  2  0.0303 0.9701334
> Residuals                   2500.16 74
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
>> mod.c.2 <- update(mod.c, . ~ . - women:type:income:education)
>> sum(residuals(mod.c.2)^2) - sum(residuals(mod.c)^2)
> [1] 2.049735
>
> Beyond demonstrating that the algorithm that Anova() uses can be made to
> fail if the coefficient covariance matrix is sufficiently ill-conditioned
> problem, I?m not sure what the point of this is. I suppose that we could
> try to detect this condition, which falls in the small region between
> where lm() detects a singularity and the projections used by Anova()
> break down.

The point of this: I was surprised that the only immediate hint I saw of
something gone wrong were the dfs (some numerical variables with > 1 df,
factors with more dfs than actual levels) with no additional warning. 

But then, I am not sure if a simple check of dfs is something simple to add
to the code, or more generally whether trying to detect this condition is
worth it, as this might just be too borderline a case (as you say "the
small region between where lm() detects a singularity and the projections
used by Anova() break down.").


Again, thanks for clarifying my confusion.


Best,


Ramon

>
> Best,
>  John
>
> -------------------------------------------------
> John Fox, Professor Emeritus
> McMaster University
> Hamilton, Ontario, Canada
> Web: http::/socserv.mcmaster.ca/jfox
>
>> On Dec 5, 2018, at 7:33 PM, Ramon Diaz-Uriarte <rdiaz02 at gmail.com> wrote:
>>
>>
>> Dear All,
>>
>> I do not understand the degrees of freedom returned by car::Anova under
>> some models. They seem to be too many (e.g., numerical variables getting
>> more than 1 df, factors getting more df than levels there are).
>>
>> This is a reproducible example:
>>>
>> library(car)
>> data(Prestige)
>>
>> ## Make sure no issues from NAs in comparisons of SS below
>> prestige_nona <- na.omit(Prestige)
>>
>> Anova(lm(prestige ~ women * type * income * education,
>>         data = prestige_nona))
>>
>> ## Notice how women, a numerical variable, has 3 df
>> ## and type (factor with 3 levels) has 4 df.
>>
>>
>> ## In contrast this seems to get the df right:
>> Anova(lm(prestige ~ women * type * income * education,
>>         data = prestige_nona), type = "III")
>>
>> ## And also gives the df I'd expect
>> anova(lm(prestige ~ women * type * income * education,
>>         data = prestige_nona))
>>
>>
>>
>> ## Type II SS for women in the above model I do not understand either.
>> m_1 <- lm(prestige ~ type * income * education, data = prestige_nona)
>> m_2 <- lm(prestige ~ type * income * education + women, data = prestige_nona)
>> ## Does not match women SS
>> sum(residuals(m_1)^2) - sum(residuals(m_2)^2)
>>
>> ## See [1] below for examples where they match.
>>
>>
>> Looking at the code, I do not understand what the call from
>> linearHypothesis returns here (specially compared to other models), and the
>> problem seems to be in the return from ConjComp, possibly due to the the
>> vcov of the model? (But this is over my head).
>>
>>
>> I understand this is not a reasonable model to fit, and there are possibly
>> serious collinearity problems. But I was surprised by the dfs in the
>> absence of any warning of something gone wrong. So I think there is
>> something very basic I do not understand.
>>
>>
>>
>> Thanks,
>>
>>
>> R.
>>
>>
>> [1] In contrast, in other models I see what I'd expect. For example:
>>
>> ## 1 df for women, 2 for type
>> Anova(lm(prestige ~ type * income * women, data = prestige_nona))
>> m_1 <- lm(prestige ~ type * income, data = prestige_nona)
>> m_2 <- lm(prestige ~ type * income + women, data = prestige_nona)
>> ## Type II SS for women
>> sum(residuals(m_1)^2) - sum(residuals(m_2)^2)
>>
>> ## 1 df for women, income, education
>> Anova(lm(prestige ~ education * income * women, data = prestige_nona))
>> m_1 <- lm(prestige ~ education * income, data = prestige_nona)
>> m_2 <- lm(prestige ~ education * income + women, data = prestige_nona)
>> ## Type II SS for women
>> sum(residuals(m_1)^2) - sum(residuals(m_2)^2)
>>
>>
>>
>>
>> --
>> Ramon Diaz-Uriarte
>> Department of Biochemistry, Lab B-25
>> Facultad de Medicina
>> Universidad Aut?noma de Madrid
>> Arzobispo Morcillo, 4
>> 28029 Madrid
>> Spain
>>
>> Phone: +34-91-497-2412
>>
>> Email: rdiaz02 at gmail.com
>>       ramon.diaz at iib.uam.es
>>
>> http://ligarto.org/rdiaz
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


--
Ramon Diaz-Uriarte
Department of Biochemistry, Lab B-25
Facultad de Medicina
Universidad Aut?noma de Madrid
Arzobispo Morcillo, 4
28029 Madrid
Spain

Phone: +34-91-497-2412

Email: rdiaz02 at gmail.com
       ramon.diaz at iib.uam.es

http://ligarto.org/rdiaz


From k@tem@to @ending from gm@il@com  Thu Dec  6 13:45:35 2018
From: k@tem@to @ending from gm@il@com (Kate Stone)
Date: Thu, 6 Dec 2018 13:45:35 +0100
Subject: [R] Bug (?): reading binary files in Windows 10
Message-ID: <CA+w2qYnxD5DHs0Q=k0T=5d8u3EVmHWtFNBtqoz0bb1ZRkODbOA@mail.gmail.com>

Hello r-help,

Could you help me determine whether this is an R bug or not?

I've been trying to read this binary file in R:

download.file("ftp://ftp.fieldtriptoolbox.org/pub/fieldtrip/tutorial/preprocessing_erp/s04.eeg","s04.eeg")

and I get a different length file (i.e. much longer) in Windows  >= 8
x64 (build 9200) than in Ubuntu. I've tested it with different R
versions in Windows and different package versions with the same
incorrect result. Other colleagues have tested it on the same
Windows/Ubuntu builds and got the correct length.

I'm not sure whether this is an R problem or something to do with my
OS specifically, or even with the file itself. Any ideas?? I've
attached a small script demonstrating the issue.

Many thanks,
Kate

-- 
Kate Stone
PhD candidate
Vasishth Lab | Department of Linguistics
Potsdam University, 14467 Potsdam, Germany
https://auskate.github.io

From @lk@uffm @ending from f@@tm@il@fm  Thu Dec  6 14:50:57 2018
From: @lk@uffm @ending from f@@tm@il@fm (Albrecht Kauffmann)
Date: Thu, 06 Dec 2018 14:50:57 +0100
Subject: [R] Bug (?): reading binary files in Windows 10
In-Reply-To: <CA+w2qYnxD5DHs0Q=k0T=5d8u3EVmHWtFNBtqoz0bb1ZRkODbOA@mail.gmail.com>
References: <CA+w2qYnxD5DHs0Q=k0T=5d8u3EVmHWtFNBtqoz0bb1ZRkODbOA@mail.gmail.com>
Message-ID: <1544104257.652595.1600906384.7288E270@webmail.messagingengine.com>

Dear Kate,

I cannot find your small script, but I downloaded the file using your command line. It has the size of  142773760 bytes (136.2 MB).

Hth,
Albrecht

-- 
  Albrecht Kauffmann
  alkauffm at fastmail.fm

Am Do, 6. Dez 2018, um 13:45, schrieb Kate Stone:
> Hello r-help,
> 
> Could you help me determine whether this is an R bug or not?
> 
> I've been trying to read this binary file in R:
> 
> download.file("ftp://ftp.fieldtriptoolbox.org/pub/fieldtrip/tutorial/preprocessing_erp/s04.eeg","s04.eeg")
> 
> and I get a different length file (i.e. much longer) in Windows  >= 8
> x64 (build 9200) than in Ubuntu. I've tested it with different R
> versions in Windows and different package versions with the same
> incorrect result. Other colleagues have tested it on the same
> Windows/Ubuntu builds and got the correct length.
> 
> I'm not sure whether this is an R problem or something to do with my
> OS specifically, or even with the file itself. Any ideas?? I've
> attached a small script demonstrating the issue.
> 
> Many thanks,
> Kate
> 
> -- 
> Kate Stone
> PhD candidate
> Vasishth Lab | Department of Linguistics
> Potsdam University, 14467 Potsdam, Germany
> https://auskate.github.io
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From om@@gonz@le@ @ending from gm@il@com  Thu Dec  6 15:02:24 2018
From: om@@gonz@le@ @ending from gm@il@com (=?UTF-8?B?T21hciBBbmRyw6kgR29uesOhbGVzIETDrWF6?=)
Date: Thu, 6 Dec 2018 09:02:24 -0500
Subject: [R] Bug (?): reading binary files in Windows 10
In-Reply-To: <1544104257.652595.1600906384.7288E270@webmail.messagingengine.com>
References: <CA+w2qYnxD5DHs0Q=k0T=5d8u3EVmHWtFNBtqoz0bb1ZRkODbOA@mail.gmail.com>
 <1544104257.652595.1600906384.7288E270@webmail.messagingengine.com>
Message-ID: <CAM-xyZiTdjzztyjOUjP=fc+vWtYAJd9mY1h5rhrYScRFVpHMcw@mail.gmail.com>

Hi,

this is what i got, just with base R:

> a <- download.file("
ftp://ftp.fieldtriptoolbox.org/pub/fieldtrip/tutorial/preprocessing_erp/s04.eeg
","s04.eeg")
probando la URL '
ftp://ftp.fieldtriptoolbox.org/pub/fieldtrip/tutorial/preprocessing_erp/s04.eeg
'
Content type 'unknown' length 142773760 bytes (136.2 MB)
==================================================
> a
[1] 0
> length(a)
[1] 1

Information about the session:

> sessionInfo()
R version 3.4.4 (2018-03-15)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 18.04.1 LTS

Matrix products: default
BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1
LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.7.1

locale:
 [1] LC_CTYPE=es_ES.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=es_ES.UTF-8        LC_COLLATE=es_ES.UTF-8
 [5] LC_MONETARY=es_ES.UTF-8    LC_MESSAGES=es_ES.UTF-8
 [7] LC_PAPER=es_ES.UTF-8       LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=es_ES.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] compiler_3.4.4 tools_3.4.4    yaml_2.2.0


El jue., 6 dic. 2018 a las 8:51, Albrecht Kauffmann (<alkauffm at fastmail.fm>)
escribi?:

> Dear Kate,
>
> I cannot find your small script, but I downloaded the file using your
> command line. It has the size of  142773760 bytes (136.2 MB).
>
> Hth,
> Albrecht
>
> --
>   Albrecht Kauffmann
>   alkauffm at fastmail.fm
>
> Am Do, 6. Dez 2018, um 13:45, schrieb Kate Stone:
> > Hello r-help,
> >
> > Could you help me determine whether this is an R bug or not?
> >
> > I've been trying to read this binary file in R:
> >
> > download.file("
> ftp://ftp.fieldtriptoolbox.org/pub/fieldtrip/tutorial/preprocessing_erp/s04.eeg
> ","s04.eeg")
> >
> > and I get a different length file (i.e. much longer) in Windows  >= 8
> > x64 (build 9200) than in Ubuntu. I've tested it with different R
> > versions in Windows and different package versions with the same
> > incorrect result. Other colleagues have tested it on the same
> > Windows/Ubuntu builds and got the correct length.
> >
> > I'm not sure whether this is an R problem or something to do with my
> > OS specifically, or even with the file itself. Any ideas?? I've
> > attached a small script demonstrating the issue.
> >
> > Many thanks,
> > Kate
> >
> > --
> > Kate Stone
> > PhD candidate
> > Vasishth Lab | Department of Linguistics
> > Potsdam University, 14467 Potsdam, Germany
> > https://auskate.github.io
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From murdoch@dunc@n @ending from gm@il@com  Thu Dec  6 16:03:48 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Thu, 6 Dec 2018 10:03:48 -0500
Subject: [R] Bug (?): reading binary files in Windows 10
In-Reply-To: <CA+w2qYnxD5DHs0Q=k0T=5d8u3EVmHWtFNBtqoz0bb1ZRkODbOA@mail.gmail.com>
References: <CA+w2qYnxD5DHs0Q=k0T=5d8u3EVmHWtFNBtqoz0bb1ZRkODbOA@mail.gmail.com>
Message-ID: <65036ca8-71f0-915b-8572-637fde417b0c@gmail.com>

On 06/12/2018 7:45 AM, Kate Stone wrote:
> Hello r-help,
> 
> Could you help me determine whether this is an R bug or not?
> 
> I've been trying to read this binary file in R:
> 
> download.file("ftp://ftp.fieldtriptoolbox.org/pub/fieldtrip/tutorial/preprocessing_erp/s04.eeg","s04.eeg")
> 
> and I get a different length file (i.e. much longer) in Windows  >= 8
> x64 (build 9200) than in Ubuntu. I've tested it with different R
> versions in Windows and different package versions with the same
> incorrect result. Other colleagues have tested it on the same
> Windows/Ubuntu builds and got the correct length.
> 
> I'm not sure whether this is an R problem or something to do with my
> OS specifically, or even with the file itself. Any ideas?? I've
> attached a small script demonstrating the issue.

On Windows, the `mode = "wb"` argument to download.file() is important, 
otherwise it is assumed to be a text file, and LF is changed to CR LF. 
There may also be handling of EOF marks, I forget.

Duncan Murdoch


From jdnewmil @ending from dcn@d@vi@@c@@u@  Thu Dec  6 16:41:33 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Thu, 06 Dec 2018 07:41:33 -0800
Subject: [R] Bug (?): reading binary files in Windows 10
In-Reply-To: <65036ca8-71f0-915b-8572-637fde417b0c@gmail.com>
References: <CA+w2qYnxD5DHs0Q=k0T=5d8u3EVmHWtFNBtqoz0bb1ZRkODbOA@mail.gmail.com>
 <65036ca8-71f0-915b-8572-637fde417b0c@gmail.com>
Message-ID: <E1799EDE-577A-4393-B3E2-02050A23796D@dcn.davis.ca.us>

AFAIK this receiver-side responsibility to specify the text/binary status of the file is particularly a problem with the "ftp://" protocol because it does not use MIME file encoding (which "http://" uses). MIME allows the sending end of the connection to communicate whether the file is text or binary, though it uses more bandwidth for the transfer. If the server offers you a choice in these days of high bandwidth connections, you may be better off sticking with http/https.

Note that MIME is not magic... if the sender is improperly configured then the client can potentially receive corrupt data. Fortunately the most typical MIME misconfigurations cause the file to be unchanged in all cases, leaving it to the receiver to deal with any text file newline decoding choice/task after the file transfer is completed.

On December 6, 2018 7:03:48 AM PST, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>On 06/12/2018 7:45 AM, Kate Stone wrote:
>> Hello r-help,
>> 
>> Could you help me determine whether this is an R bug or not?
>> 
>> I've been trying to read this binary file in R:
>> 
>>
>download.file("ftp://ftp.fieldtriptoolbox.org/pub/fieldtrip/tutorial/preprocessing_erp/s04.eeg","s04.eeg")
>> 
>> and I get a different length file (i.e. much longer) in Windows  >= 8
>> x64 (build 9200) than in Ubuntu. I've tested it with different R
>> versions in Windows and different package versions with the same
>> incorrect result. Other colleagues have tested it on the same
>> Windows/Ubuntu builds and got the correct length.
>> 
>> I'm not sure whether this is an R problem or something to do with my
>> OS specifically, or even with the file itself. Any ideas?? I've
>> attached a small script demonstrating the issue.
>
>On Windows, the `mode = "wb"` argument to download.file() is important,
>
>otherwise it is assumed to be a text file, and LF is changed to CR LF. 
>There may also be handling of EOF marks, I forget.
>
>Duncan Murdoch
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @nnij@nh @ending from gm@il@com  Thu Dec  6 18:45:13 2018
From: @nnij@nh @ending from gm@il@com (Janh Anni)
Date: Thu, 6 Dec 2018 12:45:13 -0500
Subject: [R] Monte Carlo Simulations for Human Health & Ecological Risk
 Assessment
Message-ID: <CAFCoDdD=WY_ZcRNrtiswu0SJPr=06AAXrJqQRe39X6dJ7JWV-w@mail.gmail.com>

Dear R Experts!

I would really love to perform probabilistic risk assessment for human
health and ecological using Monte Carlo. I am knowledgeable in the risk
assessment part but have no idea how to incorporate Monte Carlo simulation
using R.  Is there anyone out there in the wide wide world of R who has
actually used Monte Carlo for this type of risk assessment and kind enough
to show me the way?

Many thanks in anticipation!

Janh

	[[alternative HTML version deleted]]


From m@k@hholly @ending from gm@il@com  Thu Dec  6 22:38:00 2018
From: m@k@hholly @ending from gm@il@com (greg holly)
Date: Thu, 6 Dec 2018 15:38:00 -0600
Subject: [R] meta analysis for sensitivity and specificity
Message-ID: <CAM9Qe4hKgx-jaX7Qv8=8fsVX4WTkXsqA6BATR0xNmCaeYW2p0g@mail.gmail.com>

Does anyone know any R library that runs meta-analysis in SAS differently
for  Sensitivity and Specificity if I have only the following info?

Regards,

Greg

specificity sample_size Sensitivity Sample_size
1 21 0.66 57
1 70 0.55 33
1 19 0.76 17
1 10 0.4 30
1 16 0.46 11

	[[alternative HTML version deleted]]


From d@gm@r@cimiotti @ending from ftz-we@t@uni-kiel@de  Fri Dec  7 09:36:48 2018
From: d@gm@r@cimiotti @ending from ftz-we@t@uni-kiel@de (Dagmar Cimiotti)
Date: Fri, 7 Dec 2018 09:36:48 +0100
Subject: [R] sample (randomly select) from successive days
Message-ID: <033f3a57-3a23-688f-b3aa-954cf041e5db@ftz-west.uni-kiel.de>

Dear all,

I have data from a time span like this:

myframe <- data.frame (Timestamp=c("24.09.2012 09:00:00", "24.09.2012 10:00:00","25.09.2012 09:00:00",
                                    "25.09.2012 09:00:00","24.09.2012 09:00:00", "24.09.2012 10:00:00"),
                         Event=c(50,60,30,40,42,54) )
myframe


I want to create a new dataframe which includes in this example the data from two successive days (in my real data I have a big time span and want data from 25 consecutive days). I understand that I can do a simple sample like this

mysample <- myframe[sample(1:nrow(myframe), 4,replace=FALSE),]
mysample

But I need the data from consecutive days in my random sample. Can anyone help me with this?


Many thanks in advance,
Dagmar


From drjimlemon @ending from gm@il@com  Fri Dec  7 10:30:03 2018
From: drjimlemon @ending from gm@il@com (Jim Lemon)
Date: Fri, 7 Dec 2018 20:30:03 +1100
Subject: [R] sample (randomly select) from successive days
In-Reply-To: <033f3a57-3a23-688f-b3aa-954cf041e5db@ftz-west.uni-kiel.de>
References: <033f3a57-3a23-688f-b3aa-954cf041e5db@ftz-west.uni-kiel.de>
Message-ID: <CA+8X3fW28GenU-nWP1JAsbyT5pEoC8WF_H+AODeFw3UBLkjVRA@mail.gmail.com>

Hi Dagmar,
This will probably involve creating a variable to differentiate the
two days in each data.frame:

myframe$day<-as.Date(as.character(myframe$Timestamp),"%d.%m.%Y %H:%M:%S")
days<-unique(myframe$day)

Then just sample the two subsets and concatenate them:

myframe[c(sample(which(myframe$day==days[1]),2),
 sample(which(myframe$day==days[2]),2)),]

Jim


On Fri, Dec 7, 2018 at 8:08 PM Dagmar Cimiotti
<dagmar.cimiotti at ftz-west.uni-kiel.de> wrote:
>
> Dear all,
>
> I have data from a time span like this:
>
> myframe <- data.frame (Timestamp=c("24.09.2012 09:00:00", "24.09.2012 10:00:00","25.09.2012 09:00:00",
>                                     "25.09.2012 09:00:00","24.09.2012 09:00:00", "24.09.2012 10:00:00"),
>                          Event=c(50,60,30,40,42,54) )
> myframe
>
>
> I want to create a new dataframe which includes in this example the data from two successive days (in my real data I have a big time span and want data from 25 consecutive days). I understand that I can do a simple sample like this
>
> mysample <- myframe[sample(1:nrow(myframe), 4,replace=FALSE),]
> mysample
>
> But I need the data from consecutive days in my random sample. Can anyone help me with this?
>
>
> Many thanks in advance,
> Dagmar
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From d@gm@r@cimiotti @ending from ftz-we@t@uni-kiel@de  Fri Dec  7 11:24:11 2018
From: d@gm@r@cimiotti @ending from ftz-we@t@uni-kiel@de (Dagmar Cimiotti)
Date: Fri, 7 Dec 2018 11:24:11 +0100
Subject: [R] sample (randomly select) from successive days
In-Reply-To: <CA+8X3fW28GenU-nWP1JAsbyT5pEoC8WF_H+AODeFw3UBLkjVRA@mail.gmail.com>
References: <033f3a57-3a23-688f-b3aa-954cf041e5db@ftz-west.uni-kiel.de>
 <CA+8X3fW28GenU-nWP1JAsbyT5pEoC8WF_H+AODeFw3UBLkjVRA@mail.gmail.com>
Message-ID: <b2821de1-3f02-cb28-6b8f-ef5399fe4b4e@ftz-west.uni-kiel.de>

Hi Jim and everyone else,

Mhm, no this is not what I am looking for. I think in your way I would 
randomly sample two values of day 1 and of day 2. But I want the 
opposite: I want to randomly draw two successive (!) days and put those 
values in a new dataframe to continue working with them.

In my real data I do have a huge time span and I want to draw 25 
consecutive days. So maybe my example was a little misleading. And now 
that I read it again my text was, too. Sorry about that!

Good try though and I am very gratefull for your good will to help me 
:-)?? Would anyone give another try?

Dagmar

Am 07.12.2018 um 10:30 schrieb Jim Lemon:
> Hi Dagmar,
> This will probably involve creating a variable to differentiate the
> two days in each data.frame:
>
> myframe$day<-as.Date(as.character(myframe$Timestamp),"%d.%m.%Y %H:%M:%S")
> days<-unique(myframe$day)
>
> Then just sample the two subsets and concatenate them:
>
> myframe[c(sample(which(myframe$day==days[1]),2),
>   sample(which(myframe$day==days[2]),2)),]
>
> Jim
>
>
> On Fri, Dec 7, 2018 at 8:08 PM Dagmar Cimiotti
> <dagmar.cimiotti at ftz-west.uni-kiel.de> wrote:
>> Dear all,
>>
>> I have data from a time span like this:
>>
>> myframe <- data.frame (Timestamp=c("24.09.2012 09:00:00", "24.09.2012 10:00:00","25.09.2012 09:00:00",
>>                                      "25.09.2012 09:00:00","24.09.2012 09:00:00", "24.09.2012 10:00:00"),
>>                           Event=c(50,60,30,40,42,54) )
>> myframe
>>
>>
>> I want to create a new dataframe which includes in this example the data from two successive days (in my real data I have a big time span and want data from 25 consecutive days). I understand that I can do a simple sample like this
>>
>> mysample <- myframe[sample(1:nrow(myframe), 4,replace=FALSE),]
>> mysample
>>
>> But I need the data from consecutive days in my random sample. Can anyone help me with this?
>>
>>
>> Many thanks in advance,
>> Dagmar
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From li@t@ @ending from dewey@myzen@co@uk  Fri Dec  7 13:07:19 2018
From: li@t@ @ending from dewey@myzen@co@uk (Michael Dewey)
Date: Fri, 7 Dec 2018 12:07:19 +0000
Subject: [R] meta analysis for sensitivity and specificity
In-Reply-To: <CAM9Qe4hKgx-jaX7Qv8=8fsVX4WTkXsqA6BATR0xNmCaeYW2p0g@mail.gmail.com>
References: <CAM9Qe4hKgx-jaX7Qv8=8fsVX4WTkXsqA6BATR0xNmCaeYW2p0g@mail.gmail.com>
Message-ID: <2e3a0b89-4cd0-db93-c39c-7a9f1e210009@dewey.myzen.co.uk>

Dear Greg

I think you are going to need to supply more information. WHat do you 
mean by "in SAS differently"? If you just want to do an analysis using 
the Reitsma model then there are options in R of course.

https://CRAN.R-project.org/view=MetaAnalysis

for further questions may I suggest using the mailing list dedicated to 
meta-analysis in R

https://stat.ethz.ch/mailman/listinfo/r-sig-meta-analysis//

Michael


On 06/12/2018 21:38, greg holly wrote:
> Does anyone know any R library that runs meta-analysis in SAS differently
> for  Sensitivity and Specificity if I have only the following info?
> 
> Regards,
> 
> Greg
> 
> specificity sample_size Sensitivity Sample_size
> 1 21 0.66 57
> 1 70 0.55 33
> 1 19 0.76 17
> 1 10 0.4 30
> 1 16 0.46 11
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From wolfg@ng@viechtb@uer @ending from m@@@trichtuniver@ity@nl  Fri Dec  7 13:15:53 2018
From: wolfg@ng@viechtb@uer @ending from m@@@trichtuniver@ity@nl (Viechtbauer, Wolfgang (SP))
Date: Fri, 7 Dec 2018 12:15:53 +0000
Subject: [R] meta analysis for sensitivity and specificity
In-Reply-To: <CAM9Qe4hKgx-jaX7Qv8=8fsVX4WTkXsqA6BATR0xNmCaeYW2p0g@mail.gmail.com>
References: <CAM9Qe4hKgx-jaX7Qv8=8fsVX4WTkXsqA6BATR0xNmCaeYW2p0g@mail.gmail.com>
Message-ID: <b6e6ce6cf7a14948914c9614eb03ba30@UM-MAIL3214.unimaas.nl>

Dear Greg,

I am not sure if I understand your question. If you are asking how to do this in R, then one could use the metafor or meta package for this. The specificity and sensitivity values are proportions, so one would usually meta-analyze them after a logit transformation. But all of the specificity values are equal to 1, so this is pretty pointless. For sensitivity:

dat <- data.frame(pi = c(.66, .55, .76, .40, .46), ni = c(57, 33, 17, 30, 11))
dat$xi <- round(dat$pi * dat$ni)

library(metafor)

dat <- escalc(measure="PLO", xi=xi, ni=ni, data=dat)
res <- rma(yi, vi, data=dat)
res
predict(res, transf=transf.ilogit)

One could also use a logistic mixed-effects model for this:

res <- rma.glmm(measure="PLO", xi=xi, ni=ni, data=dat)
res
predict(res, transf=transf.ilogit)

If you want to analyze the specificity and sensitivity together, then you would want to use a bivariate model. There are some specific packages for this. See the Meta-Analysis Task View (https://cran.r-project.org/web/views/MetaAnalysis.html). I just saw that Michael also replied with the same suggestion (and the note about the mailing list).

Best,
Wolfgang

>-----Original Message-----
>From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of greg
>holly
>Sent: Thursday, 06 December, 2018 22:38
>To: r-help mailing list
>Subject: [R] meta analysis for sensitivity and specificity
>
>Does anyone know any R library that runs meta-analysis in SAS differently
>for  Sensitivity and Specificity if I have only the following info?
>
>Regards,
>
>Greg
>
>specificity sample_size Sensitivity Sample_size
>1 21 0.66 57
>1 70 0.55 33
>1 19 0.76 17
>1 10 0.4 30
>1 16 0.46 11


From jdnewmil @ending from dcn@d@vi@@c@@u@  Fri Dec  7 15:41:35 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Fri, 07 Dec 2018 06:41:35 -0800
Subject: [R] sample (randomly select) from successive days
In-Reply-To: <b2821de1-3f02-cb28-6b8f-ef5399fe4b4e@ftz-west.uni-kiel.de>
References: <033f3a57-3a23-688f-b3aa-954cf041e5db@ftz-west.uni-kiel.de>
 <CA+8X3fW28GenU-nWP1JAsbyT5pEoC8WF_H+AODeFw3UBLkjVRA@mail.gmail.com>
 <b2821de1-3f02-cb28-6b8f-ef5399fe4b4e@ftz-west.uni-kiel.de>
Message-ID: <4799FBBA-5B1F-4246-B34D-FAD4EE406D4A@dcn.davis.ca.us>

myfirst <- sample( seq.int( nrow(myframe)-1 ), 1 )
mysample <- myframe[seq( myfirst, myfirst+1),]
mysample

On December 7, 2018 2:24:11 AM PST, Dagmar Cimiotti <dagmar.cimiotti at ftz-west.uni-kiel.de> wrote:
>Hi Jim and everyone else,
>
>Mhm, no this is not what I am looking for. I think in your way I would 
>randomly sample two values of day 1 and of day 2. But I want the 
>opposite: I want to randomly draw two successive (!) days and put those
>
>values in a new dataframe to continue working with them.
>
>In my real data I do have a huge time span and I want to draw 25 
>consecutive days. So maybe my example was a little misleading. And now 
>that I read it again my text was, too. Sorry about that!
>
>Good try though and I am very gratefull for your good will to help me 
>:-)?? Would anyone give another try?
>
>Dagmar
>
>Am 07.12.2018 um 10:30 schrieb Jim Lemon:
>> Hi Dagmar,
>> This will probably involve creating a variable to differentiate the
>> two days in each data.frame:
>>
>> myframe$day<-as.Date(as.character(myframe$Timestamp),"%d.%m.%Y
>%H:%M:%S")
>> days<-unique(myframe$day)
>>
>> Then just sample the two subsets and concatenate them:
>>
>> myframe[c(sample(which(myframe$day==days[1]),2),
>>   sample(which(myframe$day==days[2]),2)),]
>>
>> Jim
>>
>>
>> On Fri, Dec 7, 2018 at 8:08 PM Dagmar Cimiotti
>> <dagmar.cimiotti at ftz-west.uni-kiel.de> wrote:
>>> Dear all,
>>>
>>> I have data from a time span like this:
>>>
>>> myframe <- data.frame (Timestamp=c("24.09.2012 09:00:00",
>"24.09.2012 10:00:00","25.09.2012 09:00:00",
>>>                                      "25.09.2012
>09:00:00","24.09.2012 09:00:00", "24.09.2012 10:00:00"),
>>>                           Event=c(50,60,30,40,42,54) )
>>> myframe
>>>
>>>
>>> I want to create a new dataframe which includes in this example the
>data from two successive days (in my real data I have a big time span
>and want data from 25 consecutive days). I understand that I can do a
>simple sample like this
>>>
>>> mysample <- myframe[sample(1:nrow(myframe), 4,replace=FALSE),]
>>> mysample
>>>
>>> But I need the data from consecutive days in my random sample. Can
>anyone help me with this?
>>>
>>>
>>> Many thanks in advance,
>>> Dagmar
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From m@k@hholly @ending from gm@il@com  Fri Dec  7 16:10:43 2018
From: m@k@hholly @ending from gm@il@com (greg holly)
Date: Fri, 7 Dec 2018 09:10:43 -0600
Subject: [R] meta analysis for sensitivity and specificity
In-Reply-To: <b6e6ce6cf7a14948914c9614eb03ba30@UM-MAIL3214.unimaas.nl>
References: <CAM9Qe4hKgx-jaX7Qv8=8fsVX4WTkXsqA6BATR0xNmCaeYW2p0g@mail.gmail.com>
 <b6e6ce6cf7a14948914c9614eb03ba30@UM-MAIL3214.unimaas.nl>
Message-ID: <CAM9Qe4i1KzjZ2jiS6m7eTthJ7OVCzmsVpRYmbvUB-rQHQb4P4Q@mail.gmail.com>

Dear All;

I sincerely apologize for TYPOS. My question is that:

Does anyone know any R library that runs meta-analysis differently for
Sensitivity and Specificity if I have only the following info in my data
set?
Once again my apologies for the mistake in my earlier email.

Regards,

Greg

specificity sample_size Sensitivity Sample_size
1 21 0.66 57
1 70 0.55 33
1 19 0.76 17
1 10 0.4 30
1 16 0.46 11

On Fri, Dec 7, 2018 at 6:16 AM Viechtbauer, Wolfgang (SP) <
wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:

> Dear Greg,
>
> I am not sure if I understand your question. If you are asking how to do
> this in R, then one could use the metafor or meta package for this. The
> specificity and sensitivity values are proportions, so one would usually
> meta-analyze them after a logit transformation. But all of the specificity
> values are equal to 1, so this is pretty pointless. For sensitivity:
>
> dat <- data.frame(pi = c(.66, .55, .76, .40, .46), ni = c(57, 33, 17, 30,
> 11))
> dat$xi <- round(dat$pi * dat$ni)
>
> library(metafor)
>
> dat <- escalc(measure="PLO", xi=xi, ni=ni, data=dat)
> res <- rma(yi, vi, data=dat)
> res
> predict(res, transf=transf.ilogit)
>
> One could also use a logistic mixed-effects model for this:
>
> res <- rma.glmm(measure="PLO", xi=xi, ni=ni, data=dat)
> res
> predict(res, transf=transf.ilogit)
>
> If you want to analyze the specificity and sensitivity together, then you
> would want to use a bivariate model. There are some specific packages for
> this. See the Meta-Analysis Task View (
> https://cran.r-project.org/web/views/MetaAnalysis.html). I just saw that
> Michael also replied with the same suggestion (and the note about the
> mailing list).
>
> Best,
> Wolfgang
>
> >-----Original Message-----
> >From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of greg
> >holly
> >Sent: Thursday, 06 December, 2018 22:38
> >To: r-help mailing list
> >Subject: [R] meta analysis for sensitivity and specificity
> >
> >Does anyone know any R library that runs meta-analysis in SAS differently
> >for  Sensitivity and Specificity if I have only the following info?
> >
> >Regards,
> >
> >Greg
> >
> >specificity sample_size Sensitivity Sample_size
> >1 21 0.66 57
> >1 70 0.55 33
> >1 19 0.76 17
> >1 10 0.4 30
> >1 16 0.46 11
>

	[[alternative HTML version deleted]]


From m@k@hholly @ending from gm@il@com  Fri Dec  7 16:21:35 2018
From: m@k@hholly @ending from gm@il@com (greg holly)
Date: Fri, 7 Dec 2018 09:21:35 -0600
Subject: [R] meta analysis for sensitivity and specificity
In-Reply-To: <b6e6ce6cf7a14948914c9614eb03ba30@UM-MAIL3214.unimaas.nl>
References: <CAM9Qe4hKgx-jaX7Qv8=8fsVX4WTkXsqA6BATR0xNmCaeYW2p0g@mail.gmail.com>
 <b6e6ce6cf7a14948914c9614eb03ba30@UM-MAIL3214.unimaas.nl>
Message-ID: <CAM9Qe4gu56OEDvVvLEbqoLCui_UCza6THrKLh+Wm+OV+NQ3eVw@mail.gmail.com>

Hi Viechtbauer and Micheal;


Thanks so much for writing. It is much appreciated.

Regards,
Greg

On Fri, Dec 7, 2018 at 6:16 AM Viechtbauer, Wolfgang (SP) <
wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:

> Dear Greg,
>
> I am not sure if I understand your question. If you are asking how to do
> this in R, then one could use the metafor or meta package for this. The
> specificity and sensitivity values are proportions, so one would usually
> meta-analyze them after a logit transformation. But all of the specificity
> values are equal to 1, so this is pretty pointless. For sensitivity:
>
> dat <- data.frame(pi = c(.66, .55, .76, .40, .46), ni = c(57, 33, 17, 30,
> 11))
> dat$xi <- round(dat$pi * dat$ni)
>
> library(metafor)
>
> dat <- escalc(measure="PLO", xi=xi, ni=ni, data=dat)
> res <- rma(yi, vi, data=dat)
> res
> predict(res, transf=transf.ilogit)
>
> One could also use a logistic mixed-effects model for this:
>
> res <- rma.glmm(measure="PLO", xi=xi, ni=ni, data=dat)
> res
> predict(res, transf=transf.ilogit)
>
> If you want to analyze the specificity and sensitivity together, then you
> would want to use a bivariate model. There are some specific packages for
> this. See the Meta-Analysis Task View (
> https://cran.r-project.org/web/views/MetaAnalysis.html). I just saw that
> Michael also replied with the same suggestion (and the note about the
> mailing list).
>
> Best,
> Wolfgang
>
> >-----Original Message-----
> >From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of greg
> >holly
> >Sent: Thursday, 06 December, 2018 22:38
> >To: r-help mailing list
> >Subject: [R] meta analysis for sensitivity and specificity
> >
> >Does anyone know any R library that runs meta-analysis in SAS differently
> >for  Sensitivity and Specificity if I have only the following info?
> >
> >Regards,
> >
> >Greg
> >
> >specificity sample_size Sensitivity Sample_size
> >1 21 0.66 57
> >1 70 0.55 33
> >1 19 0.76 17
> >1 10 0.4 30
> >1 16 0.46 11
>

	[[alternative HTML version deleted]]


From k@tem@to @ending from gm@il@com  Fri Dec  7 13:33:12 2018
From: k@tem@to @ending from gm@il@com (Kate Stone)
Date: Fri, 7 Dec 2018 13:33:12 +0100
Subject: [R] Bug (?): reading binary files in Windows 10
In-Reply-To: <E1799EDE-577A-4393-B3E2-02050A23796D@dcn.davis.ca.us>
References: <CA+w2qYnxD5DHs0Q=k0T=5d8u3EVmHWtFNBtqoz0bb1ZRkODbOA@mail.gmail.com>
 <65036ca8-71f0-915b-8572-637fde417b0c@gmail.com>
 <E1799EDE-577A-4393-B3E2-02050A23796D@dcn.davis.ca.us>
Message-ID: <CA+w2qYn+pkXWjYus=M1bvnnMFpKmvfYduVz_vGocOD8J-=tavg@mail.gmail.com>

Ah wow, that answers many questions, thanks!

On Thu, Dec 6, 2018 at 4:41 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> AFAIK this receiver-side responsibility to specify the text/binary status
> of the file is particularly a problem with the "ftp://" protocol because
> it does not use MIME file encoding (which "http://" uses). MIME allows
> the sending end of the connection to communicate whether the file is text
> or binary, though it uses more bandwidth for the transfer. If the server
> offers you a choice in these days of high bandwidth connections, you may be
> better off sticking with http/https.
>
> Note that MIME is not magic... if the sender is improperly configured then
> the client can potentially receive corrupt data. Fortunately the most
> typical MIME misconfigurations cause the file to be unchanged in all cases,
> leaving it to the receiver to deal with any text file newline decoding
> choice/task after the file transfer is completed.
>
> On December 6, 2018 7:03:48 AM PST, Duncan Murdoch <
> murdoch.duncan at gmail.com> wrote:
> >On 06/12/2018 7:45 AM, Kate Stone wrote:
> >> Hello r-help,
> >>
> >> Could you help me determine whether this is an R bug or not?
> >>
> >> I've been trying to read this binary file in R:
> >>
> >>
> >download.file("
> ftp://ftp.fieldtriptoolbox.org/pub/fieldtrip/tutorial/preprocessing_erp/s04.eeg
> ","s04.eeg")
> >>
> >> and I get a different length file (i.e. much longer) in Windows  >= 8
> >> x64 (build 9200) than in Ubuntu. I've tested it with different R
> >> versions in Windows and different package versions with the same
> >> incorrect result. Other colleagues have tested it on the same
> >> Windows/Ubuntu builds and got the correct length.
> >>
> >> I'm not sure whether this is an R problem or something to do with my
> >> OS specifically, or even with the file itself. Any ideas?? I've
> >> attached a small script demonstrating the issue.
> >
> >On Windows, the `mode = "wb"` argument to download.file() is important,
> >
> >otherwise it is assumed to be a text file, and LF is changed to CR LF.
> >There may also be handling of EOF marks, I forget.
> >
> >Duncan Murdoch
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>


-- 
Kate Stone
PhD candidate
Vasishth Lab | Department of Linguistics
Potsdam University, 14467 Potsdam, Germany
https://auskate.github.io

	[[alternative HTML version deleted]]


From d@gm@r@cimiotti @ending from ftz-we@t@uni-kiel@de  Sat Dec  8 01:18:26 2018
From: d@gm@r@cimiotti @ending from ftz-we@t@uni-kiel@de (Dagmar Cimiotti)
Date: Sat, 8 Dec 2018 01:18:26 +0100
Subject: [R] sample (randomly select) to get a number of successive days
Message-ID: <dbc2d925-7f97-15b2-8a2a-e265bf93495d@ftz-west.uni-kiel.de>

Hi Jim and everyone else,

Mhm, no this is not what I am looking for. I think in your way I would 
randomly sample two values of day 1 and of day 2. But I want the 
opposite: I want to randomly draw two successive (!) days and put those 
values in a new dataframe to continue working with them.

In my real data I do have a huge time span and I want to draw 25 
consecutive days. So maybe my example was a little misleading. And now 
that I read it again my text was, too. Sorry about that!

Good try though and I am very gratefull for your good will to help me ?? 
Would anyone give another try?

Dagmar

Am 07.12.2018 um 10:30 schrieb Jim Lemon:
> Hi Dagmar,
> This will probably involve creating a variable to differentiate the
> two days in each data.frame:
>
> myframe$day<-as.Date(as.character(myframe$Timestamp),"%d.%m.%Y %H:%M:%S")
> days<-unique(myframe$day)
>
> Then just sample the two subsets and concatenate them:
>
> myframe[c(sample(which(myframe$day==days[1]),2),
> ? sample(which(myframe$day==days[2]),2)),]
>
> Jim
>
>
> On Fri, Dec 7, 2018 at 8:08 PM Dagmar Cimiotti
> <dagmar.cimiotti at ftz-west.uni-kiel.de> wrote:
>> Dear all,
>>
>> I have data from a time span like this:
>>
>> myframe <- data.frame (Timestamp=c("24.09.2012 09:00:00", "24.09.2012 
>> 10:00:00","25.09.2012 09:00:00",
>> ???????????????????????????????????? "25.09.2012 
>> 09:00:00","24.09.2012 09:00:00", "24.09.2012 10:00:00"),
>> ????????????????????????? Event=c(50,60,30,40,42,54) )
>> myframe
>>
>>
>> I want to create a new dataframe which includes in this example the 
>> data from two successive days (in my real data I have a big time span 
>> and want data from 25 consecutive days). I understand that I can do a 
>> simple sample like this
>>
>> mysample <- myframe[sample(1:nrow(myframe), 4,replace=FALSE),]
>> mysample
>>
>> But I need the data from consecutive days in my random sample. Can 
>> anyone help me with this?
>>
>>
>> Many thanks in advance,
>> Dagmar
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From m@rc_@chw@rtz @ending from me@com  Sat Dec  8 02:26:47 2018
From: m@rc_@chw@rtz @ending from me@com (Marc Schwartz)
Date: Fri, 7 Dec 2018 20:26:47 -0500
Subject: [R] sample (randomly select) to get a number of successive days
In-Reply-To: <dbc2d925-7f97-15b2-8a2a-e265bf93495d@ftz-west.uni-kiel.de>
References: <dbc2d925-7f97-15b2-8a2a-e265bf93495d@ftz-west.uni-kiel.de>
Message-ID: <DE4E6769-8679-4D8D-8283-691A789E9DA4@me.com>

Hi,

I am confused.

As far as I can tell, only the first day is selected randomly from your dataset. The subsequent 24 days are deterministic, since they need to be consecutive days from the first day, for a total of 25 consecutive days. 

Thus, all you need to do is to randomly select 1 day from within the time range of your dataset to be the first day, that is also far enough from the maximum date, to allow you to then select the data from the additional 24 consecutive days.

So randomly pick your first day and set a range of values, covering the 25 days, to use to then subset your full dataset.

What am I missing?

Regards,

Marc Schwartz


> On Dec 7, 2018, at 7:18 PM, Dagmar Cimiotti <dagmar.cimiotti at ftz-west.uni-kiel.de> wrote:
> 
> Hi Jim and everyone else,
> 
> Mhm, no this is not what I am looking for. I think in your way I would 
> randomly sample two values of day 1 and of day 2. But I want the 
> opposite: I want to randomly draw two successive (!) days and put those 
> values in a new dataframe to continue working with them.
> 
> In my real data I do have a huge time span and I want to draw 25 
> consecutive days. So maybe my example was a little misleading. And now 
> that I read it again my text was, too. Sorry about that!
> 
> Good try though and I am very gratefull for your good will to help me    
> Would anyone give another try?
> 
> Dagmar
> 
> Am 07.12.2018 um 10:30 schrieb Jim Lemon:
>> Hi Dagmar,
>> This will probably involve creating a variable to differentiate the
>> two days in each data.frame:
>> 
>> myframe$day<-as.Date(as.character(myframe$Timestamp),"%d.%m.%Y %H:%M:%S")
>> days<-unique(myframe$day)
>> 
>> Then just sample the two subsets and concatenate them:
>> 
>> myframe[c(sample(which(myframe$day==days[1]),2),
>>   sample(which(myframe$day==days[2]),2)),]
>> 
>> Jim
>> 
>> 
>> On Fri, Dec 7, 2018 at 8:08 PM Dagmar Cimiotti
>> <dagmar.cimiotti at ftz-west.uni-kiel.de> wrote:
>>> Dear all,
>>> 
>>> I have data from a time span like this:
>>> 
>>> myframe <- data.frame (Timestamp=c("24.09.2012 09:00:00", "24.09.2012 
>>> 10:00:00","25.09.2012 09:00:00",
>>>                                      "25.09.2012 
>>> 09:00:00","24.09.2012 09:00:00", "24.09.2012 10:00:00"),
>>>                           Event=c(50,60,30,40,42,54) )
>>> myframe
>>> 
>>> 
>>> I want to create a new dataframe which includes in this example the 
>>> data from two successive days (in my real data I have a big time span 
>>> and want data from 25 consecutive days). I understand that I can do a 
>>> simple sample like this
>>> 
>>> mysample <- myframe[sample(1:nrow(myframe), 4,replace=FALSE),]
>>> mysample
>>> 
>>> But I need the data from consecutive days in my random sample. Can 
>>> anyone help me with this?
>>> 
>>> 
>>> Many thanks in advance,
>>> Dagmar


From Bill@Poling @ending from zeli@@com  Sat Dec  8 16:03:19 2018
From: Bill@Poling @ending from zeli@@com (Bill Poling)
Date: Sat, 8 Dec 2018 15:03:19 +0000
Subject: [R] Help with K-Means output
Message-ID: <BN7PR02MB50734F1F311A1C3BB8AB781BEAAB0@BN7PR02MB5073.namprd02.prod.outlook.com>

Good afternoon. I hope I have provided enough info to get my question answered.

I am running windows 10 -- R3.5.1 -- RStudio Version 1.1.456

When running a K-Means clustering routine is it possible to get the actual data from each cluster into a DF?

I have reviewed a number of tutorials and unless I missed it somewhere I would like to know if it is possible.

https://www.datacamp.com/community/tutorials/k-means-clustering-r
https://www.guru99.com/r-k-means-clustering.html
https://datascienceplus.com/k-means-clustering-in-r/
https://datascienceplus.com/finding-optimal-number-of-clusters/
http://enhancedatascience.com/2017/10/24/machine-learning-explained-kmeans/
http://enhancedatascience.com/2017/04/30/r-basics-k-means-r/

For example:

I ran the below and get K-means clustering with 10 clusters of sizes 1511, 1610, 702, 926, 996, 1076, 580, 2429, 728, 3797
Can the 1511 values of SavingsReversed and ProviderID , 1610 values of SavingsReversed and ProviderID, etc.. be run out into DF's?

Thank you for your help.

WHP

str(rr0)
Classes 'data.table' and 'data.frame':14355 obs. of  2 variables:
 $ SavingsReversed: num  0 0 61 128 160 ...
 $ ProviderID     : num  113676 113676 116494 116641 116641 ...
 - attr(*, ".internal.selfref")=<externalptr>

head(rr0, n=35)
    SavingsReversed ProviderID
 1:            0.00     113676
 2:            0.00     113676
 3:           61.00     116494
 4:          128.25     116641
 5:          159.60     116641
 6:          372.66     119316
 7:           18.79     121319
 8:           15.64     121319
 9:            0.00     121319
10:           18.79     121319
11:           23.00     121319
12:           18.79     121319
13:            0.00     121319
14:           25.86     121319
15:           14.00     121319
16:          113.00     121545
17:           50.00     121545
18:         1155.32     121545
19:          113.00     121545
20:          197.20     121545
21:            0.00     121780
22:           36.00     122536
23:         1171.32     125198
24:         1171.32     125198
25:           43.00     125303
26:            0.00     125881
27:           69.64     128435
28:          420.18     128435
29:          175.18     128435
30:           71.54     128435
31:           99.85     128435
32:            0.00     128435
33:           42.75     128435
34:          175.18     128435
35:          846.45     128435

set.seed(213)
rr0a <- kmeans(rr0, 10)
View(rr0a)
summary(rr0a)
# Length Class  Mode
# cluster      14355  -none- numeric
# centers         20  -none- numeric
# totss            1  -none- numeric
# withinss        10  -none- numeric
# tot.withinss     1  -none- numeric
# betweenss        1  -none- numeric
# size            10  -none- numeric
# iter             1  -none- numeric
# ifault           1  -none- numeric

x1 <- as.data.frame(rr0a$centers)
sort(x1)
#SavingsReversed ProviderID
# 2         75.19665  2773789.2
# 3         99.31959  4147091.6
# 5        101.21070  3558532.7
# 4        103.41147  3893274.4
# 1        105.38310  2241031.2
# 8        114.61562  3240701.5
# 10       121.14184  4718727.6
# 9        153.70536  4470878.9
# 6        156.84426  5560636.6
# 7        185.09745   173732.9
print(rr0a)
# K-means clustering with 10 clusters of sizes 1511, 1610, 702, 926, 996, 1076, 580, 2429, 728, 3797
#
# Cluster means:
#   SavingsReversed ProviderID
# 1        105.38310  2241031.2
# 2         75.19665  2773789.2
# 3         99.31959  4147091.6
# 4        103.41147  3893274.4
# 5        101.21070  3558532.7
# 6        156.84426  5560636.6
# 7        185.09745   173732.9
# 8        114.61562  3240701.5
# 9        153.70536  4470878.9
# 10       121.14184  4718727.6
#Within cluster sum of squares by cluster:
# [1] 74529288379846 25846368411171  4692898666512  6277704963344  8428785199973 90824041558798  1468798013919 12143462193009  5483877005233
# [10] 51547955737867
# (between_SS / total_SS =  98.7 %)
#
# Available components:
#
#   [1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss" "betweenss"    "size"         "iter"         "ifault"









Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}


From bgunter@4567 @ending from gm@il@com  Sat Dec  8 16:46:23 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Sat, 8 Dec 2018 07:46:23 -0800
Subject: [R] Help with K-Means output
In-Reply-To: <BN7PR02MB50734F1F311A1C3BB8AB781BEAAB0@BN7PR02MB5073.namprd02.prod.outlook.com>
References: <BN7PR02MB50734F1F311A1C3BB8AB781BEAAB0@BN7PR02MB5073.namprd02.prod.outlook.com>
Message-ID: <CAGxFJbT_G4_=k-RmZmrrRPNVfFisK60b+mtUZ8dtASOQDkvLsQ@mail.gmail.com>

Please see ?kmeans and note the "cluster" component of the returned value
that would appear to provide the info you seek.

-- Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Dec 8, 2018 at 7:03 AM Bill Poling <Bill.Poling at zelis.com> wrote:

> Good afternoon. I hope I have provided enough info to get my question
> answered.
>
> I am running windows 10 -- R3.5.1 -- RStudio Version 1.1.456
>
> When running a K-Means clustering routine is it possible to get the actual
> data from each cluster into a DF?
>
> I have reviewed a number of tutorials and unless I missed it somewhere I
> would like to know if it is possible.
>
> https://www.datacamp.com/community/tutorials/k-means-clustering-r
> https://www.guru99.com/r-k-means-clustering.html
> https://datascienceplus.com/k-means-clustering-in-r/
> https://datascienceplus.com/finding-optimal-number-of-clusters/
> http://enhancedatascience.com/2017/10/24/machine-learning-explained-kmeans/
> http://enhancedatascience.com/2017/04/30/r-basics-k-means-r/
>
> For example:
>
> I ran the below and get K-means clustering with 10 clusters of sizes 1511,
> 1610, 702, 926, 996, 1076, 580, 2429, 728, 3797
> Can the 1511 values of SavingsReversed and ProviderID , 1610 values of
> SavingsReversed and ProviderID, etc.. be run out into DF's?
>
> Thank you for your help.
>
> WHP
>
> str(rr0)
> Classes 'data.table' and 'data.frame':14355 obs. of  2 variables:
>  $ SavingsReversed: num  0 0 61 128 160 ...
>  $ ProviderID     : num  113676 113676 116494 116641 116641 ...
>  - attr(*, ".internal.selfref")=<externalptr>
>
> head(rr0, n=35)
>     SavingsReversed ProviderID
>  1:            0.00     113676
>  2:            0.00     113676
>  3:           61.00     116494
>  4:          128.25     116641
>  5:          159.60     116641
>  6:          372.66     119316
>  7:           18.79     121319
>  8:           15.64     121319
>  9:            0.00     121319
> 10:           18.79     121319
> 11:           23.00     121319
> 12:           18.79     121319
> 13:            0.00     121319
> 14:           25.86     121319
> 15:           14.00     121319
> 16:          113.00     121545
> 17:           50.00     121545
> 18:         1155.32     121545
> 19:          113.00     121545
> 20:          197.20     121545
> 21:            0.00     121780
> 22:           36.00     122536
> 23:         1171.32     125198
> 24:         1171.32     125198
> 25:           43.00     125303
> 26:            0.00     125881
> 27:           69.64     128435
> 28:          420.18     128435
> 29:          175.18     128435
> 30:           71.54     128435
> 31:           99.85     128435
> 32:            0.00     128435
> 33:           42.75     128435
> 34:          175.18     128435
> 35:          846.45     128435
>
> set.seed(213)
> rr0a <- kmeans(rr0, 10)
> View(rr0a)
> summary(rr0a)
> # Length Class  Mode
> # cluster      14355  -none- numeric
> # centers         20  -none- numeric
> # totss            1  -none- numeric
> # withinss        10  -none- numeric
> # tot.withinss     1  -none- numeric
> # betweenss        1  -none- numeric
> # size            10  -none- numeric
> # iter             1  -none- numeric
> # ifault           1  -none- numeric
>
> x1 <- as.data.frame(rr0a$centers)
> sort(x1)
> #SavingsReversed ProviderID
> # 2         75.19665  2773789.2
> # 3         99.31959  4147091.6
> # 5        101.21070  3558532.7
> # 4        103.41147  3893274.4
> # 1        105.38310  2241031.2
> # 8        114.61562  3240701.5
> # 10       121.14184  4718727.6
> # 9        153.70536  4470878.9
> # 6        156.84426  5560636.6
> # 7        185.09745   173732.9
> print(rr0a)
> # K-means clustering with 10 clusters of sizes 1511, 1610, 702, 926, 996,
> 1076, 580, 2429, 728, 3797
> #
> # Cluster means:
> #   SavingsReversed ProviderID
> # 1        105.38310  2241031.2
> # 2         75.19665  2773789.2
> # 3         99.31959  4147091.6
> # 4        103.41147  3893274.4
> # 5        101.21070  3558532.7
> # 6        156.84426  5560636.6
> # 7        185.09745   173732.9
> # 8        114.61562  3240701.5
> # 9        153.70536  4470878.9
> # 10       121.14184  4718727.6
> #Within cluster sum of squares by cluster:
> # [1] 74529288379846 25846368411171  4692898666512  6277704963344
> 8428785199973 90824041558798  1468798013919 12143462193009  5483877005233
> # [10] 51547955737867
> # (between_SS / total_SS =  98.7 %)
> #
> # Available components:
> #
> #   [1] "cluster"      "centers"      "totss"        "withinss"
>  "tot.withinss" "betweenss"    "size"         "iter"         "ifault"
>
>
>
>
>
>
>
>
>
> Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dc@rl@on @ending from t@mu@edu  Sat Dec  8 17:11:42 2018
From: dc@rl@on @ending from t@mu@edu (David L Carlson)
Date: Sat, 8 Dec 2018 16:11:42 +0000
Subject: [R] Help with K-Means output
In-Reply-To: <CAGxFJbT_G4_=k-RmZmrrRPNVfFisK60b+mtUZ8dtASOQDkvLsQ@mail.gmail.com>
References: <BN7PR02MB50734F1F311A1C3BB8AB781BEAAB0@BN7PR02MB5073.namprd02.prod.outlook.com>
 <CAGxFJbT_G4_=k-RmZmrrRPNVfFisK60b+mtUZ8dtASOQDkvLsQ@mail.gmail.com>
Message-ID: <0baa3281338346b5b21187b75ea59fff@tamu.edu>

You should also read the manual page for ?split and learn how to work with lists:

# Split the data according to cluster membership
# to create a list of data frames
rr0.clus <- split(rr0, rr0a$cluster)

# The data frame for cluster 1:
rr0.clus[[1]]

--------------------------------------------------------
David L. Carlson
Department of Anthropology
Texas A&M University

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Bert Gunter
Sent: Saturday, December 8, 2018 9:46 AM
To: Bill.Poling at zelis.com
Cc: R-help <r-help at r-project.org>
Subject: Re: [R] Help with K-Means output

Please see ?kmeans and note the "cluster" component of the returned value
that would appear to provide the info you seek.

-- Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Dec 8, 2018 at 7:03 AM Bill Poling <Bill.Poling at zelis.com> wrote:

> Good afternoon. I hope I have provided enough info to get my question
> answered.
>
> I am running windows 10 -- R3.5.1 -- RStudio Version 1.1.456
>
> When running a K-Means clustering routine is it possible to get the actual
> data from each cluster into a DF?
>
> I have reviewed a number of tutorials and unless I missed it somewhere I
> would like to know if it is possible.
>
> https://www.datacamp.com/community/tutorials/k-means-clustering-r
> https://www.guru99.com/r-k-means-clustering.html
> https://datascienceplus.com/k-means-clustering-in-r/
> https://datascienceplus.com/finding-optimal-number-of-clusters/
> http://enhancedatascience.com/2017/10/24/machine-learning-explained-kmeans/
> http://enhancedatascience.com/2017/04/30/r-basics-k-means-r/
>
> For example:
>
> I ran the below and get K-means clustering with 10 clusters of sizes 1511,
> 1610, 702, 926, 996, 1076, 580, 2429, 728, 3797
> Can the 1511 values of SavingsReversed and ProviderID , 1610 values of
> SavingsReversed and ProviderID, etc.. be run out into DF's?
>
> Thank you for your help.
>
> WHP
>
> str(rr0)
> Classes 'data.table' and 'data.frame':14355 obs. of  2 variables:
>  $ SavingsReversed: num  0 0 61 128 160 ...
>  $ ProviderID     : num  113676 113676 116494 116641 116641 ...
>  - attr(*, ".internal.selfref")=<externalptr>
>
> head(rr0, n=35)
>     SavingsReversed ProviderID
>  1:            0.00     113676
>  2:            0.00     113676
>  3:           61.00     116494
>  4:          128.25     116641
>  5:          159.60     116641
>  6:          372.66     119316
>  7:           18.79     121319
>  8:           15.64     121319
>  9:            0.00     121319
> 10:           18.79     121319
> 11:           23.00     121319
> 12:           18.79     121319
> 13:            0.00     121319
> 14:           25.86     121319
> 15:           14.00     121319
> 16:          113.00     121545
> 17:           50.00     121545
> 18:         1155.32     121545
> 19:          113.00     121545
> 20:          197.20     121545
> 21:            0.00     121780
> 22:           36.00     122536
> 23:         1171.32     125198
> 24:         1171.32     125198
> 25:           43.00     125303
> 26:            0.00     125881
> 27:           69.64     128435
> 28:          420.18     128435
> 29:          175.18     128435
> 30:           71.54     128435
> 31:           99.85     128435
> 32:            0.00     128435
> 33:           42.75     128435
> 34:          175.18     128435
> 35:          846.45     128435
>
> set.seed(213)
> rr0a <- kmeans(rr0, 10)
> View(rr0a)
> summary(rr0a)
> # Length Class  Mode
> # cluster      14355  -none- numeric
> # centers         20  -none- numeric
> # totss            1  -none- numeric
> # withinss        10  -none- numeric
> # tot.withinss     1  -none- numeric
> # betweenss        1  -none- numeric
> # size            10  -none- numeric
> # iter             1  -none- numeric
> # ifault           1  -none- numeric
>
> x1 <- as.data.frame(rr0a$centers)
> sort(x1)
> #SavingsReversed ProviderID
> # 2         75.19665  2773789.2
> # 3         99.31959  4147091.6
> # 5        101.21070  3558532.7
> # 4        103.41147  3893274.4
> # 1        105.38310  2241031.2
> # 8        114.61562  3240701.5
> # 10       121.14184  4718727.6
> # 9        153.70536  4470878.9
> # 6        156.84426  5560636.6
> # 7        185.09745   173732.9
> print(rr0a)
> # K-means clustering with 10 clusters of sizes 1511, 1610, 702, 926, 996,
> 1076, 580, 2429, 728, 3797
> #
> # Cluster means:
> #   SavingsReversed ProviderID
> # 1        105.38310  2241031.2
> # 2         75.19665  2773789.2
> # 3         99.31959  4147091.6
> # 4        103.41147  3893274.4
> # 5        101.21070  3558532.7
> # 6        156.84426  5560636.6
> # 7        185.09745   173732.9
> # 8        114.61562  3240701.5
> # 9        153.70536  4470878.9
> # 10       121.14184  4718727.6
> #Within cluster sum of squares by cluster:
> # [1] 74529288379846 25846368411171  4692898666512  6277704963344
> 8428785199973 90824041558798  1468798013919 12143462193009  5483877005233
> # [10] 51547955737867
> # (between_SS / total_SS =  98.7 %)
> #
> # Available components:
> #
> #   [1] "cluster"      "centers"      "totss"        "withinss"
>  "tot.withinss" "betweenss"    "size"         "iter"         "ifault"
>
>
>
>
>
>
>
>
>
> Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Bill@Poling @ending from zeli@@com  Sat Dec  8 17:43:37 2018
From: Bill@Poling @ending from zeli@@com (Bill Poling)
Date: Sat, 8 Dec 2018 16:43:37 +0000
Subject: [R] Help with K-Means output
In-Reply-To: <CAGxFJbT_G4_=k-RmZmrrRPNVfFisK60b+mtUZ8dtASOQDkvLsQ@mail.gmail.com>
References: <BN7PR02MB50734F1F311A1C3BB8AB781BEAAB0@BN7PR02MB5073.namprd02.prod.outlook.com>
 <CAGxFJbT_G4_=k-RmZmrrRPNVfFisK60b+mtUZ8dtASOQDkvLsQ@mail.gmail.com>
Message-ID: <BN7PR02MB5073758FD538D785AC14A90AEAAB0@BN7PR02MB5073.namprd02.prod.outlook.com>

Thank you Bert, I see, so I think this is the process?

set.seed(213)
rr0a1 <- kmeans(rr0, 10)

summary(rr0a1) #Just the cluster
#Length Class  Mode
#cluster      14355  -none- numeric

head(rr0a1$cluster, n=35)
# [1] 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7

Xcluster <- as.data.frame(rr0a1$cluster)

head(Xcluster, n=5)
#rr0a1$cluster
# 1             7
# 2             7
# 3             7
# 4             7
# 5             7

tail(Xcluster, n=5)
#rr0a1$cluster
# 14351             6
# 14352             6
# 14353             6
# 14354             6
# 14355             6

And I can just join this DF with my original DF used for the KMean, correct?
The vertical order is the same?

WHP


From: Bert Gunter <bgunter.4567 at gmail.com>
Sent: Saturday, December 8, 2018 10:46 AM
To: Bill Poling <Bill.Poling at zelis.com>
Cc: R-help <r-help at r-project.org>
Subject: Re: [R] Help with K-Means output

Please see ?kmeans and note the "cluster" component of the returned value that would appear to provide the info you seek.

-- Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Dec 8, 2018 at 7:03 AM Bill Poling <mailto:Bill.Poling at zelis.com> wrote:
Good afternoon. I hope I have provided enough info to get my question answered.

I am running windows 10 -- R3.5.1 -- RStudio Version 1.1.456

When running a K-Means clustering routine is it possible to get the actual data from each cluster into a DF?

I have reviewed a number of tutorials and unless I missed it somewhere I would like to know if it is possible.

https://www.datacamp.com/community/tutorials/k-means-clustering-r
https://www.guru99.com/r-k-means-clustering.html
https://datascienceplus.com/k-means-clustering-in-r/
https://datascienceplus.com/finding-optimal-number-of-clusters/
http://enhancedatascience.com/2017/10/24/machine-learning-explained-kmeans/
http://enhancedatascience.com/2017/04/30/r-basics-k-means-r/

For example:

I ran the below and get K-means clustering with 10 clusters of sizes 1511, 1610, 702, 926, 996, 1076, 580, 2429, 728, 3797
Can the 1511 values of SavingsReversed and ProviderID , 1610 values of SavingsReversed and ProviderID, etc.. be run out into DF's?

Thank you for your help.

WHP

str(rr0)
Classes 'data.table' and 'data.frame':14355 obs. of  2 variables:
 $ SavingsReversed: num  0 0 61 128 160 ...
 $ ProviderID     : num  113676 113676 116494 116641 116641 ...
 - attr(*, ".internal.selfref")=<externalptr>

head(rr0, n=35)
    SavingsReversed ProviderID
 1:            0.00     113676
 2:            0.00     113676
 3:           61.00     116494
 4:          128.25     116641
 5:          159.60     116641
 6:          372.66     119316
 7:           18.79     121319
 8:           15.64     121319
 9:            0.00     121319
10:           18.79     121319
11:           23.00     121319
12:           18.79     121319
13:            0.00     121319
14:           25.86     121319
15:           14.00     121319
16:          113.00     121545
17:           50.00     121545
18:         1155.32     121545
19:          113.00     121545
20:          197.20     121545
21:            0.00     121780
22:           36.00     122536
23:         1171.32     125198
24:         1171.32     125198
25:           43.00     125303
26:            0.00     125881
27:           69.64     128435
28:          420.18     128435
29:          175.18     128435
30:           71.54     128435
31:           99.85     128435
32:            0.00     128435
33:           42.75     128435
34:          175.18     128435
35:          846.45     128435

set.seed(213)
rr0a <- kmeans(rr0, 10)
View(rr0a)
summary(rr0a)
# Length Class  Mode
# cluster      14355  -none- numeric
# centers         20  -none- numeric
# totss            1  -none- numeric
# withinss        10  -none- numeric
# tot.withinss     1  -none- numeric
# betweenss        1  -none- numeric
# size            10  -none- numeric
# iter             1  -none- numeric
# ifault           1  -none- numeric

x1 <- as.data.frame(rr0a$centers)
sort(x1)
#SavingsReversed ProviderID
# 2         75.19665  2773789.2
# 3         99.31959  4147091.6
# 5        101.21070  3558532.7
# 4        103.41147  3893274.4
# 1        105.38310  2241031.2
# 8        114.61562  3240701.5
# 10       121.14184  4718727.6
# 9        153.70536  4470878.9
# 6        156.84426  5560636.6
# 7        185.09745   173732.9
print(rr0a)
# K-means clustering with 10 clusters of sizes 1511, 1610, 702, 926, 996, 1076, 580, 2429, 728, 3797
#
# Cluster means:
#   SavingsReversed ProviderID
# 1        105.38310  2241031.2
# 2         75.19665  2773789.2
# 3         99.31959  4147091.6
# 4        103.41147  3893274.4
# 5        101.21070  3558532.7
# 6        156.84426  5560636.6
# 7        185.09745   173732.9
# 8        114.61562  3240701.5
# 9        153.70536  4470878.9
# 10       121.14184  4718727.6
#Within cluster sum of squares by cluster:
# [1] 74529288379846 25846368411171  4692898666512  6277704963344  8428785199973 90824041558798  1468798013919 12143462193009  5483877005233
# [10] 51547955737867
# (between_SS / total_SS =  98.7 %)
#
# Available components:
#
#   [1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss" "betweenss"    "size"         "iter"         "ifault"









Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}

______________________________________________
mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

Confidentiality Notice This message is sent from Zelis. This transmission may contain information which is privileged and confidential and is intended for the personal and confidential use of the named recipient only. Such information may be protected by applicable State and Federal laws from this disclosure or unauthorized use. If the reader of this message is not the intended recipient, or the employee or agent responsible for delivering the message to the intended recipient, you are hereby notified that any disclosure, review, discussion, copying, or taking any action in reliance on the contents of this transmission is strictly prohibited. If you have received this transmission in error, please contact the sender immediately. Zelis, 2018.

From em@ni@m@il@92 @ending from gm@il@com  Sat Dec  8 13:50:34 2018
From: em@ni@m@il@92 @ending from gm@il@com (=?UTF-8?B?2KXZitmF2KfZhiDYpdiz2YXYp9i52YrZhCDZhdit2YXYrw==?=)
Date: Sat, 8 Dec 2018 14:50:34 +0200
Subject: [R] is okay
Message-ID: <CAFiZHK61yU93GS2ivaYK=cg2aEkkmLSB7=cerp_hhFDLtak9EA@mail.gmail.com>

confirm b336fdc157205f369b39177d930c6e6ac826be46

	[[alternative HTML version deleted]]


From em@ni@m@il@92 @ending from gm@il@com  Sat Dec  8 13:54:34 2018
From: em@ni@m@il@92 @ending from gm@il@com (=?UTF-8?B?2KXZitmF2KfZhiDYpdiz2YXYp9i52YrZhCDZhdit2YXYrw==?=)
Date: Sat, 8 Dec 2018 14:54:34 +0200
Subject: [R] Confirm Adding to mailing list
Message-ID: <CAFiZHK6m3YDt9X1ic3tYya36_v0SOrMEvYcQ6o6E2k9Fo7jqPA@mail.gmail.com>

Please I need to post in R-help
I need to confirm first adding to mailing list
Any help ??

	[[alternative HTML version deleted]]


From em@ni@m@il@92 @ending from gm@il@com  Sat Dec  8 14:03:37 2018
From: em@ni@m@il@92 @ending from gm@il@com (=?UTF-8?B?2KXZitmF2KfZhiDYpdiz2YXYp9i52YrZhCDZhdit2YXYrw==?=)
Date: Sat, 8 Dec 2018 15:03:37 +0200
Subject: [R] Generate Range of Correlations Matrix Bernoulli
Message-ID: <CAFiZHK4E0=yqKBMY=gBrSGKc-FTzo2gx7wdH-0LEk3JvK38arw@mail.gmail.com>

Hi all,
I was wondering how can I construct range of correlations matrix that cover
all space
from dependent Multivariate Bernoulli (known Marginal Probabilities but
unknown correlations)
I have P's for every variable but unknown correlation between each pair
I want to try range of applicable correlations

Why my post rejected?

	[[alternative HTML version deleted]]


From bgunter@4567 @ending from gm@il@com  Sat Dec  8 18:18:42 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Sat, 8 Dec 2018 09:18:42 -0800
Subject: [R] Help with K-Means output
In-Reply-To: <BN7PR02MB5073758FD538D785AC14A90AEAAB0@BN7PR02MB5073.namprd02.prod.outlook.com>
References: <BN7PR02MB50734F1F311A1C3BB8AB781BEAAB0@BN7PR02MB5073.namprd02.prod.outlook.com>
 <CAGxFJbT_G4_=k-RmZmrrRPNVfFisK60b+mtUZ8dtASOQDkvLsQ@mail.gmail.com>
 <BN7PR02MB5073758FD538D785AC14A90AEAAB0@BN7PR02MB5073.namprd02.prod.outlook.com>
Message-ID: <CAGxFJbRPDWOBYhZvAJ+FUNKT3Z017L_hizQ4tJ9TnGTNABH1Ow@mail.gmail.com>

See David Carlson's reply -- and his advice for learning about how to use
lists.

"And I can just join this DF with my original DF used for the KMean,
correct?"

Define "join" . See, e.g.
http://desktop.arcgis.com/en/arcmap/10.3/manage-data/tables/essentials-of-joining-tables.htm
See also ?merge

I consider it to be your job to learn how to work with R's data structures.
There are numerous web tutorials to help you do so. Others may disagree and
reply to such queries.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Dec 8, 2018 at 8:43 AM Bill Poling <Bill.Poling at zelis.com> wrote:

> Thank you Bert, I see, so I think this is the process?
>
> set.seed(213)
> rr0a1 <- kmeans(rr0, 10)
>
> summary(rr0a1) #Just the cluster
> #Length Class  Mode
> #cluster      14355  -none- numeric
>
> head(rr0a1$cluster, n=35)
> # [1] 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7
>
> Xcluster <- as.data.frame(rr0a1$cluster)
>
> head(Xcluster, n=5)
> #rr0a1$cluster
> # 1             7
> # 2             7
> # 3             7
> # 4             7
> # 5             7
>
> tail(Xcluster, n=5)
> #rr0a1$cluster
> # 14351             6
> # 14352             6
> # 14353             6
> # 14354             6
> # 14355             6
>
> And I can just join this DF with my original DF used for the KMean,
> correct?
> The vertical order is the same?
>
> WHP
>
>
> From: Bert Gunter <bgunter.4567 at gmail.com>
> Sent: Saturday, December 8, 2018 10:46 AM
> To: Bill Poling <Bill.Poling at zelis.com>
> Cc: R-help <r-help at r-project.org>
> Subject: Re: [R] Help with K-Means output
>
> Please see ?kmeans and note the "cluster" component of the returned value
> that would appear to provide the info you seek.
>
> -- Bert
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Sat, Dec 8, 2018 at 7:03 AM Bill Poling <mailto:Bill.Poling at zelis.com>
> wrote:
> Good afternoon. I hope I have provided enough info to get my question
> answered.
>
> I am running windows 10 -- R3.5.1 -- RStudio Version 1.1.456
>
> When running a K-Means clustering routine is it possible to get the actual
> data from each cluster into a DF?
>
> I have reviewed a number of tutorials and unless I missed it somewhere I
> would like to know if it is possible.
>
> https://www.datacamp.com/community/tutorials/k-means-clustering-r
> https://www.guru99.com/r-k-means-clustering.html
> https://datascienceplus.com/k-means-clustering-in-r/
> https://datascienceplus.com/finding-optimal-number-of-clusters/
> http://enhancedatascience.com/2017/10/24/machine-learning-explained-kmeans/
> http://enhancedatascience.com/2017/04/30/r-basics-k-means-r/
>
> For example:
>
> I ran the below and get K-means clustering with 10 clusters of sizes 1511,
> 1610, 702, 926, 996, 1076, 580, 2429, 728, 3797
> Can the 1511 values of SavingsReversed and ProviderID , 1610 values of
> SavingsReversed and ProviderID, etc.. be run out into DF's?
>
> Thank you for your help.
>
> WHP
>
> str(rr0)
> Classes 'data.table' and 'data.frame':14355 obs. of  2 variables:
>  $ SavingsReversed: num  0 0 61 128 160 ...
>  $ ProviderID     : num  113676 113676 116494 116641 116641 ...
>  - attr(*, ".internal.selfref")=<externalptr>
>
> head(rr0, n=35)
>     SavingsReversed ProviderID
>  1:            0.00     113676
>  2:            0.00     113676
>  3:           61.00     116494
>  4:          128.25     116641
>  5:          159.60     116641
>  6:          372.66     119316
>  7:           18.79     121319
>  8:           15.64     121319
>  9:            0.00     121319
> 10:           18.79     121319
> 11:           23.00     121319
> 12:           18.79     121319
> 13:            0.00     121319
> 14:           25.86     121319
> 15:           14.00     121319
> 16:          113.00     121545
> 17:           50.00     121545
> 18:         1155.32     121545
> 19:          113.00     121545
> 20:          197.20     121545
> 21:            0.00     121780
> 22:           36.00     122536
> 23:         1171.32     125198
> 24:         1171.32     125198
> 25:           43.00     125303
> 26:            0.00     125881
> 27:           69.64     128435
> 28:          420.18     128435
> 29:          175.18     128435
> 30:           71.54     128435
> 31:           99.85     128435
> 32:            0.00     128435
> 33:           42.75     128435
> 34:          175.18     128435
> 35:          846.45     128435
>
> set.seed(213)
> rr0a <- kmeans(rr0, 10)
> View(rr0a)
> summary(rr0a)
> # Length Class  Mode
> # cluster      14355  -none- numeric
> # centers         20  -none- numeric
> # totss            1  -none- numeric
> # withinss        10  -none- numeric
> # tot.withinss     1  -none- numeric
> # betweenss        1  -none- numeric
> # size            10  -none- numeric
> # iter             1  -none- numeric
> # ifault           1  -none- numeric
>
> x1 <- as.data.frame(rr0a$centers)
> sort(x1)
> #SavingsReversed ProviderID
> # 2         75.19665  2773789.2
> # 3         99.31959  4147091.6
> # 5        101.21070  3558532.7
> # 4        103.41147  3893274.4
> # 1        105.38310  2241031.2
> # 8        114.61562  3240701.5
> # 10       121.14184  4718727.6
> # 9        153.70536  4470878.9
> # 6        156.84426  5560636.6
> # 7        185.09745   173732.9
> print(rr0a)
> # K-means clustering with 10 clusters of sizes 1511, 1610, 702, 926, 996,
> 1076, 580, 2429, 728, 3797
> #
> # Cluster means:
> #   SavingsReversed ProviderID
> # 1        105.38310  2241031.2
> # 2         75.19665  2773789.2
> # 3         99.31959  4147091.6
> # 4        103.41147  3893274.4
> # 5        101.21070  3558532.7
> # 6        156.84426  5560636.6
> # 7        185.09745   173732.9
> # 8        114.61562  3240701.5
> # 9        153.70536  4470878.9
> # 10       121.14184  4718727.6
> #Within cluster sum of squares by cluster:
> # [1] 74529288379846 25846368411171  4692898666512  6277704963344
> 8428785199973 90824041558798  1468798013919 12143462193009  5483877005233
> # [10] 51547955737867
> # (between_SS / total_SS =  98.7 %)
> #
> # Available components:
> #
> #   [1] "cluster"      "centers"      "totss"        "withinss"
>  "tot.withinss" "betweenss"    "size"         "iter"         "ifault"
>
>
>
>
>
>
>
>
>
> Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}
>
> ______________________________________________
> mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> Confidentiality Notice This message is sent from Zelis. This transmission
> may contain information which is privileged and confidential and is
> intended for the personal and confidential use of the named recipient only.
> Such information may be protected by applicable State and Federal laws from
> this disclosure or unauthorized use. If the reader of this message is not
> the intended recipient, or the employee or agent responsible for delivering
> the message to the intended recipient, you are hereby notified that any
> disclosure, review, discussion, copying, or taking any action in reliance
> on the contents of this transmission is strictly prohibited. If you have
> received this transmission in error, please contact the sender immediately.
> Zelis, 2018.
>

	[[alternative HTML version deleted]]


From Bill@Poling @ending from zeli@@com  Sat Dec  8 18:23:27 2018
From: Bill@Poling @ending from zeli@@com (Bill Poling)
Date: Sat, 8 Dec 2018 17:23:27 +0000
Subject: [R] Help with K-Means output
In-Reply-To: <0baa3281338346b5b21187b75ea59fff@tamu.edu>
References: <BN7PR02MB50734F1F311A1C3BB8AB781BEAAB0@BN7PR02MB5073.namprd02.prod.outlook.com>
 <CAGxFJbT_G4_=k-RmZmrrRPNVfFisK60b+mtUZ8dtASOQDkvLsQ@mail.gmail.com>
 <0baa3281338346b5b21187b75ea59fff@tamu.edu>
Message-ID: <BN7PR02MB5073B83D92DDC829F09EA3F2EAAB0@BN7PR02MB5073.namprd02.prod.outlook.com>

Thank you David I will try that as well.

WHP

From: David L Carlson <dcarlson at tamu.edu>
Sent: Saturday, December 8, 2018 11:12 AM
To: Bert Gunter <bgunter.4567 at gmail.com>; Bill Poling <Bill.Poling at zelis.com>
Cc: R-help <r-help at r-project.org>
Subject: RE: [R] Help with K-Means output

You should also read the manual page for ?split and learn how to work with lists:

# Split the data according to cluster membership
# to create a list of data frames
rr0.clus <- split(rr0, rr0a$cluster)

# The data frame for cluster 1:
rr0.clus[[1]]

--------------------------------------------------------
David L. Carlson
Department of Anthropology
Texas A&M University

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Bert Gunter
Sent: Saturday, December 8, 2018 9:46 AM
To: mailto:Bill.Poling at zelis.com
Cc: R-help <mailto:r-help at r-project.org>
Subject: Re: [R] Help with K-Means output

Please see ?kmeans and note the "cluster" component of the returned value
that would appear to provide the info you seek.

-- Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Dec 8, 2018 at 7:03 AM Bill Poling <mailto:Bill.Poling at zelis.com> wrote:

> Good afternoon. I hope I have provided enough info to get my question
> answered.
>
> I am running windows 10 -- R3.5.1 -- RStudio Version 1.1.456
>
> When running a K-Means clustering routine is it possible to get the actual
> data from each cluster into a DF?
>
> I have reviewed a number of tutorials and unless I missed it somewhere I
> would like to know if it is possible.
>
> https://www.datacamp.com/community/tutorials/k-means-clustering-r
> https://www.guru99.com/r-k-means-clustering.html
> https://datascienceplus.com/k-means-clustering-in-r/
> https://datascienceplus.com/finding-optimal-number-of-clusters/
> http://enhancedatascience.com/2017/10/24/machine-learning-explained-kmeans/
> http://enhancedatascience.com/2017/04/30/r-basics-k-means-r/
>
> For example:
>
> I ran the below and get K-means clustering with 10 clusters of sizes 1511,
> 1610, 702, 926, 996, 1076, 580, 2429, 728, 3797
> Can the 1511 values of SavingsReversed and ProviderID , 1610 values of
> SavingsReversed and ProviderID, etc.. be run out into DF's?
>
> Thank you for your help.
>
> WHP
>
> str(rr0)
> Classes 'data.table' and 'data.frame':14355 obs. of 2 variables:
> $ SavingsReversed: num 0 0 61 128 160 ...
> $ ProviderID : num 113676 113676 116494 116641 116641 ...
> - attr(*, ".internal.selfref")=<externalptr>
>
> head(rr0, n=35)
> SavingsReversed ProviderID
> 1: 0.00 113676
> 2: 0.00 113676
> 3: 61.00 116494
> 4: 128.25 116641
> 5: 159.60 116641
> 6: 372.66 119316
> 7: 18.79 121319
> 8: 15.64 121319
> 9: 0.00 121319
> 10: 18.79 121319
> 11: 23.00 121319
> 12: 18.79 121319
> 13: 0.00 121319
> 14: 25.86 121319
> 15: 14.00 121319
> 16: 113.00 121545
> 17: 50.00 121545
> 18: 1155.32 121545
> 19: 113.00 121545
> 20: 197.20 121545
> 21: 0.00 121780
> 22: 36.00 122536
> 23: 1171.32 125198
> 24: 1171.32 125198
> 25: 43.00 125303
> 26: 0.00 125881
> 27: 69.64 128435
> 28: 420.18 128435
> 29: 175.18 128435
> 30: 71.54 128435
> 31: 99.85 128435
> 32: 0.00 128435
> 33: 42.75 128435
> 34: 175.18 128435
> 35: 846.45 128435
>
> set.seed(213)
> rr0a <- kmeans(rr0, 10)
> View(rr0a)
> summary(rr0a)
> # Length Class Mode
> # cluster 14355 -none- numeric
> # centers 20 -none- numeric
> # totss 1 -none- numeric
> # withinss 10 -none- numeric
> # tot.withinss 1 -none- numeric
> # betweenss 1 -none- numeric
> # size 10 -none- numeric
> # iter 1 -none- numeric
> # ifault 1 -none- numeric
>
> x1 <- as.data.frame(rr0a$centers)
> sort(x1)
> #SavingsReversed ProviderID
> # 2 75.19665 2773789.2
> # 3 99.31959 4147091.6
> # 5 101.21070 3558532.7
> # 4 103.41147 3893274.4
> # 1 105.38310 2241031.2
> # 8 114.61562 3240701.5
> # 10 121.14184 4718727.6
> # 9 153.70536 4470878.9
> # 6 156.84426 5560636.6
> # 7 185.09745 173732.9
> print(rr0a)
> # K-means clustering with 10 clusters of sizes 1511, 1610, 702, 926, 996,
> 1076, 580, 2429, 728, 3797
> #
> # Cluster means:
> # SavingsReversed ProviderID
> # 1 105.38310 2241031.2
> # 2 75.19665 2773789.2
> # 3 99.31959 4147091.6
> # 4 103.41147 3893274.4
> # 5 101.21070 3558532.7
> # 6 156.84426 5560636.6
> # 7 185.09745 173732.9
> # 8 114.61562 3240701.5
> # 9 153.70536 4470878.9
> # 10 121.14184 4718727.6
> #Within cluster sum of squares by cluster:
> # [1] 74529288379846 25846368411171 4692898666512 6277704963344
> 8428785199973 90824041558798 1468798013919 12143462193009 5483877005233
> # [10] 51547955737867
> # (between_SS / total_SS = 98.7 %)
> #
> # Available components:
> #
> # [1] "cluster" "centers" "totss" "withinss"
> "tot.withinss" "betweenss" "size" "iter" "ifault"
>
>
>
>
>
>
>
>
>
> Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}
>
> ______________________________________________
> mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

[[alternative HTML version deleted]]

______________________________________________
mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}


From Bill@Poling @ending from zeli@@com  Sat Dec  8 18:34:30 2018
From: Bill@Poling @ending from zeli@@com (Bill Poling)
Date: Sat, 8 Dec 2018 17:34:30 +0000
Subject: [R] Help with K-Means output
In-Reply-To: <0baa3281338346b5b21187b75ea59fff@tamu.edu>
References: <BN7PR02MB50734F1F311A1C3BB8AB781BEAAB0@BN7PR02MB5073.namprd02.prod.outlook.com>
 <CAGxFJbT_G4_=k-RmZmrrRPNVfFisK60b+mtUZ8dtASOQDkvLsQ@mail.gmail.com>
 <0baa3281338346b5b21187b75ea59fff@tamu.edu>
Message-ID: <BN7PR02MB507363F4847DAFF9FF449D6CEAAB0@BN7PR02MB5073.namprd02.prod.outlook.com>

Terrific David, that's got it thanks again!

From: David L Carlson <dcarlson at tamu.edu>
Sent: Saturday, December 8, 2018 11:12 AM
To: Bert Gunter <bgunter.4567 at gmail.com>; Bill Poling <Bill.Poling at zelis.com>
Cc: R-help <r-help at r-project.org>
Subject: RE: [R] Help with K-Means output

You should also read the manual page for ?split and learn how to work with lists:

# Split the data according to cluster membership
# to create a list of data frames
rr0.clus <- split(rr0, rr0a$cluster)

# The data frame for cluster 1:
rr0.clus[[1]]

--------------------------------------------------------
David L. Carlson
Department of Anthropology
Texas A&M University

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Bert Gunter
Sent: Saturday, December 8, 2018 9:46 AM
To: mailto:Bill.Poling at zelis.com
Cc: R-help <mailto:r-help at r-project.org>
Subject: Re: [R] Help with K-Means output

Please see ?kmeans and note the "cluster" component of the returned value
that would appear to provide the info you seek.

-- Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Dec 8, 2018 at 7:03 AM Bill Poling <mailto:Bill.Poling at zelis.com> wrote:

> Good afternoon. I hope I have provided enough info to get my question
> answered.
>
> I am running windows 10 -- R3.5.1 -- RStudio Version 1.1.456
>
> When running a K-Means clustering routine is it possible to get the actual
> data from each cluster into a DF?
>
> I have reviewed a number of tutorials and unless I missed it somewhere I
> would like to know if it is possible.
>
> https://www.datacamp.com/community/tutorials/k-means-clustering-r
> https://www.guru99.com/r-k-means-clustering.html
> https://datascienceplus.com/k-means-clustering-in-r/
> https://datascienceplus.com/finding-optimal-number-of-clusters/
> http://enhancedatascience.com/2017/10/24/machine-learning-explained-kmeans/
> http://enhancedatascience.com/2017/04/30/r-basics-k-means-r/
>
> For example:
>
> I ran the below and get K-means clustering with 10 clusters of sizes 1511,
> 1610, 702, 926, 996, 1076, 580, 2429, 728, 3797
> Can the 1511 values of SavingsReversed and ProviderID , 1610 values of
> SavingsReversed and ProviderID, etc.. be run out into DF's?
>
> Thank you for your help.
>
> WHP
>
> str(rr0)
> Classes 'data.table' and 'data.frame':14355 obs. of 2 variables:
> $ SavingsReversed: num 0 0 61 128 160 ...
> $ ProviderID : num 113676 113676 116494 116641 116641 ...
> - attr(*, ".internal.selfref")=<externalptr>
>
> head(rr0, n=35)
> SavingsReversed ProviderID
> 1: 0.00 113676
> 2: 0.00 113676
> 3: 61.00 116494
> 4: 128.25 116641
> 5: 159.60 116641
> 6: 372.66 119316
> 7: 18.79 121319
> 8: 15.64 121319
> 9: 0.00 121319
> 10: 18.79 121319
> 11: 23.00 121319
> 12: 18.79 121319
> 13: 0.00 121319
> 14: 25.86 121319
> 15: 14.00 121319
> 16: 113.00 121545
> 17: 50.00 121545
> 18: 1155.32 121545
> 19: 113.00 121545
> 20: 197.20 121545
> 21: 0.00 121780
> 22: 36.00 122536
> 23: 1171.32 125198
> 24: 1171.32 125198
> 25: 43.00 125303
> 26: 0.00 125881
> 27: 69.64 128435
> 28: 420.18 128435
> 29: 175.18 128435
> 30: 71.54 128435
> 31: 99.85 128435
> 32: 0.00 128435
> 33: 42.75 128435
> 34: 175.18 128435
> 35: 846.45 128435
>
> set.seed(213)
> rr0a <- kmeans(rr0, 10)
> View(rr0a)
> summary(rr0a)
> # Length Class Mode
> # cluster 14355 -none- numeric
> # centers 20 -none- numeric
> # totss 1 -none- numeric
> # withinss 10 -none- numeric
> # tot.withinss 1 -none- numeric
> # betweenss 1 -none- numeric
> # size 10 -none- numeric
> # iter 1 -none- numeric
> # ifault 1 -none- numeric
>
> x1 <- as.data.frame(rr0a$centers)
> sort(x1)
> #SavingsReversed ProviderID
> # 2 75.19665 2773789.2
> # 3 99.31959 4147091.6
> # 5 101.21070 3558532.7
> # 4 103.41147 3893274.4
> # 1 105.38310 2241031.2
> # 8 114.61562 3240701.5
> # 10 121.14184 4718727.6
> # 9 153.70536 4470878.9
> # 6 156.84426 5560636.6
> # 7 185.09745 173732.9
> print(rr0a)
> # K-means clustering with 10 clusters of sizes 1511, 1610, 702, 926, 996,
> 1076, 580, 2429, 728, 3797
> #
> # Cluster means:
> # SavingsReversed ProviderID
> # 1 105.38310 2241031.2
> # 2 75.19665 2773789.2
> # 3 99.31959 4147091.6
> # 4 103.41147 3893274.4
> # 5 101.21070 3558532.7
> # 6 156.84426 5560636.6
> # 7 185.09745 173732.9
> # 8 114.61562 3240701.5
> # 9 153.70536 4470878.9
> # 10 121.14184 4718727.6
> #Within cluster sum of squares by cluster:
> # [1] 74529288379846 25846368411171 4692898666512 6277704963344
> 8428785199973 90824041558798 1468798013919 12143462193009 5483877005233
> # [10] 51547955737867
> # (between_SS / total_SS = 98.7 %)
> #
> # Available components:
> #
> # [1] "cluster" "centers" "totss" "withinss"
> "tot.withinss" "betweenss" "size" "iter" "ifault"
>
>
>
>
>
>
>
>
>
> Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}
>
> ______________________________________________
> mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

[[alternative HTML version deleted]]

______________________________________________
mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}


From bgunter@4567 @ending from gm@il@com  Sat Dec  8 18:35:58 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Sat, 8 Dec 2018 09:35:58 -0800
Subject: [R] Generate Range of Correlations Matrix Bernoulli
In-Reply-To: <CAFiZHK4E0=yqKBMY=gBrSGKc-FTzo2gx7wdH-0LEk3JvK38arw@mail.gmail.com>
References: <CAFiZHK4E0=yqKBMY=gBrSGKc-FTzo2gx7wdH-0LEk3JvK38arw@mail.gmail.com>
Message-ID: <CAGxFJbQSMho9WCSTBdHDyFhQQV23QO0f0whXXvTAsQ2o6zcC_A@mail.gmail.com>

I have no idea why your post was "rejected," nor even quite what you mean
by that. But I believe your post may not receive any replies because you
have failed to follow the posting guide linked below. I find it
incomprehensible, but maybe others will be able to understand what you
want. It may also be off topic -- this list is about R programming, not
statistics (though they do sometimes intersect) -- but that may just be
because I didn't understand your query.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


?On Sat, Dec 8, 2018 at 8:55 AM ?????? ??????? ?????? <
emanismail.92 at gmail.com> wrote:?

> Hi all,
> I was wondering how can I construct range of correlations matrix that cover
> all space
> from dependent Multivariate Bernoulli (known Marginal Probabilities but
> unknown correlations)
> I have P's for every variable but unknown correlation between each pair
> I want to try range of applicable correlations
>
> Why my post rejected?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From Bill@Poling @ending from zeli@@com  Sat Dec  8 18:39:30 2018
From: Bill@Poling @ending from zeli@@com (Bill Poling)
Date: Sat, 8 Dec 2018 17:39:30 +0000
Subject: [R] Help with K-Means output
In-Reply-To: <CAGxFJbRPDWOBYhZvAJ+FUNKT3Z017L_hizQ4tJ9TnGTNABH1Ow@mail.gmail.com>
References: <BN7PR02MB50734F1F311A1C3BB8AB781BEAAB0@BN7PR02MB5073.namprd02.prod.outlook.com>
 <CAGxFJbT_G4_=k-RmZmrrRPNVfFisK60b+mtUZ8dtASOQDkvLsQ@mail.gmail.com>
 <BN7PR02MB5073758FD538D785AC14A90AEAAB0@BN7PR02MB5073.namprd02.prod.outlook.com>
 <CAGxFJbRPDWOBYhZvAJ+FUNKT3Z017L_hizQ4tJ9TnGTNABH1Ow@mail.gmail.com>
Message-ID: <BN7PR02MB507395A13CAFF95E4B9F5C22EAAB0@BN7PR02MB5073.namprd02.prod.outlook.com>

Thank you Bert.


From: Bert Gunter <bgunter.4567 at gmail.com>
Sent: Saturday, December 8, 2018 12:19 PM
To: Bill Poling <Bill.Poling at zelis.com>
Cc: R-help <r-help at r-project.org>
Subject: Re: [R] Help with K-Means output

See David Carlson's reply -- and his advice for learning about how to use lists.

"And I can just join this DF with my original DF used for the KMean, correct?"

Define "join" . See, e.g. http://desktop.arcgis.com/en/arcmap/10.3/manage-data/tables/essentials-of-joining-tables.htm
See also ?merge

I consider it to be your job to learn how to work with R's data structures. There are numerous web tutorials to help you do so. Others may disagree and reply to such queries.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Dec 8, 2018 at 8:43 AM Bill Poling <mailto:Bill.Poling at zelis.com> wrote:
Thank you Bert, I see, so I think this is the process?

set.seed(213)
rr0a1 <- kmeans(rr0, 10)

summary(rr0a1) #Just the cluster
#Length Class  Mode
#cluster      14355  -none- numeric

head(rr0a1$cluster, n=35)
# [1] 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7

Xcluster <- as.data.frame(rr0a1$cluster)

head(Xcluster, n=5)
#rr0a1$cluster
# 1             7
# 2             7
# 3             7
# 4             7
# 5             7

tail(Xcluster, n=5)
#rr0a1$cluster
# 14351             6
# 14352             6
# 14353             6
# 14354             6
# 14355             6

And I can just join this DF with my original DF used for the KMean, correct?
The vertical order is the same?

WHP


From: Bert Gunter <mailto:bgunter.4567 at gmail.com>
Sent: Saturday, December 8, 2018 10:46 AM
To: Bill Poling <mailto:Bill.Poling at zelis.com>
Cc: R-help <mailto:r-help at r-project.org>
Subject: Re: [R] Help with K-Means output

Please see ?kmeans and note the "cluster" component of the returned value that would appear to provide the info you seek.

-- Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Dec 8, 2018 at 7:03 AM Bill Poling <mailto:mailto:Bill.Poling at zelis.com> wrote:
Good afternoon. I hope I have provided enough info to get my question answered.

I am running windows 10 -- R3.5.1 -- RStudio Version 1.1.456

When running a K-Means clustering routine is it possible to get the actual data from each cluster into a DF?

I have reviewed a number of tutorials and unless I missed it somewhere I would like to know if it is possible.

https://www.datacamp.com/community/tutorials/k-means-clustering-r
https://www.guru99.com/r-k-means-clustering.html
https://datascienceplus.com/k-means-clustering-in-r/
https://datascienceplus.com/finding-optimal-number-of-clusters/
http://enhancedatascience.com/2017/10/24/machine-learning-explained-kmeans/
http://enhancedatascience.com/2017/04/30/r-basics-k-means-r/

For example:

I ran the below and get K-means clustering with 10 clusters of sizes 1511, 1610, 702, 926, 996, 1076, 580, 2429, 728, 3797
Can the 1511 values of SavingsReversed and ProviderID , 1610 values of SavingsReversed and ProviderID, etc.. be run out into DF's?

Thank you for your help.

WHP

str(rr0)
Classes 'data.table' and 'data.frame':14355 obs. of  2 variables:
 $ SavingsReversed: num  0 0 61 128 160 ...
 $ ProviderID     : num  113676 113676 116494 116641 116641 ...
 - attr(*, ".internal.selfref")=<externalptr>

head(rr0, n=35)
    SavingsReversed ProviderID
 1:            0.00     113676
 2:            0.00     113676
 3:           61.00     116494
 4:          128.25     116641
 5:          159.60     116641
 6:          372.66     119316
 7:           18.79     121319
 8:           15.64     121319
 9:            0.00     121319
10:           18.79     121319
11:           23.00     121319
12:           18.79     121319
13:            0.00     121319
14:           25.86     121319
15:           14.00     121319
16:          113.00     121545
17:           50.00     121545
18:         1155.32     121545
19:          113.00     121545
20:          197.20     121545
21:            0.00     121780
22:           36.00     122536
23:         1171.32     125198
24:         1171.32     125198
25:           43.00     125303
26:            0.00     125881
27:           69.64     128435
28:          420.18     128435
29:          175.18     128435
30:           71.54     128435
31:           99.85     128435
32:            0.00     128435
33:           42.75     128435
34:          175.18     128435
35:          846.45     128435

set.seed(213)
rr0a <- kmeans(rr0, 10)
View(rr0a)
summary(rr0a)
# Length Class  Mode
# cluster      14355  -none- numeric
# centers         20  -none- numeric
# totss            1  -none- numeric
# withinss        10  -none- numeric
# tot.withinss     1  -none- numeric
# betweenss        1  -none- numeric
# size            10  -none- numeric
# iter             1  -none- numeric
# ifault           1  -none- numeric

x1 <- as.data.frame(rr0a$centers)
sort(x1)
#SavingsReversed ProviderID
# 2         75.19665  2773789.2
# 3         99.31959  4147091.6
# 5        101.21070  3558532.7
# 4        103.41147  3893274.4
# 1        105.38310  2241031.2
# 8        114.61562  3240701.5
# 10       121.14184  4718727.6
# 9        153.70536  4470878.9
# 6        156.84426  5560636.6
# 7        185.09745   173732.9
print(rr0a)
# K-means clustering with 10 clusters of sizes 1511, 1610, 702, 926, 996, 1076, 580, 2429, 728, 3797
#
# Cluster means:
#   SavingsReversed ProviderID
# 1        105.38310  2241031.2
# 2         75.19665  2773789.2
# 3         99.31959  4147091.6
# 4        103.41147  3893274.4
# 5        101.21070  3558532.7
# 6        156.84426  5560636.6
# 7        185.09745   173732.9
# 8        114.61562  3240701.5
# 9        153.70536  4470878.9
# 10       121.14184  4718727.6
#Within cluster sum of squares by cluster:
# [1] 74529288379846 25846368411171  4692898666512  6277704963344  8428785199973 90824041558798  1468798013919 12143462193009  5483877005233
# [10] 51547955737867
# (between_SS / total_SS =  98.7 %)
#
# Available components:
#
#   [1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss" "betweenss"    "size"         "iter"         "ifault"









Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}

______________________________________________
mailto:mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

Confidentiality Notice This message is sent from Zelis. This transmission may contain information which is privileged and confidential and is intended for the personal and confidential use of the named recipient only. Such information may be protected by applicable State and Federal laws from this disclosure or unauthorized use. If the reader of this message is not the intended recipient, or the employee or agent responsible for delivering the message to the intended recipient, you are hereby notified that any disclosure, review, discussion, copying, or taking any action in reliance on the contents of this transmission is strictly prohibited. If you have received this transmission in error, please contact the sender immediately. Zelis, 2018.

Confidentiality Notice This message is sent from Zelis. This transmission may contain information which is privileged and confidential and is intended for the personal and confidential use of the named recipient only. Such information may be protected by applicable State and Federal laws from this disclosure or unauthorized use. If the reader of this message is not the intended recipient, or the employee or agent responsible for delivering the message to the intended recipient, you are hereby notified that any disclosure, review, discussion, copying, or taking any action in reliance on the contents of this transmission is strictly prohibited. If you have received this transmission in error, please contact the sender immediately. Zelis, 2018.

From bgunter@4567 @ending from gm@il@com  Sat Dec  8 21:13:19 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Sat, 8 Dec 2018 12:13:19 -0800
Subject: [R] Generate Range of Correlations Matrix Bernoulli
In-Reply-To: <CAFiZHK5127Zg82Xp4kJdmrZe8soR38kfC_QoiKLkF6Zi_EJ=Ew@mail.gmail.com>
References: <CAFiZHK4E0=yqKBMY=gBrSGKc-FTzo2gx7wdH-0LEk3JvK38arw@mail.gmail.com>
 <CAGxFJbQSMho9WCSTBdHDyFhQQV23QO0f0whXXvTAsQ2o6zcC_A@mail.gmail.com>
 <CAFiZHK5127Zg82Xp4kJdmrZe8soR38kfC_QoiKLkF6Zi_EJ=Ew@mail.gmail.com>
Message-ID: <CAGxFJbTHaNBu6Akok-TW4fZjNW0J8DnUFiuSXLZZLi0m_xDBWg@mail.gmail.com>

Unless you are sending a private message, always include the list (which I
have cc'ed) in your reply, because, as here, individuals do not do private
consulting and may be unwilling or unable to provide the help you seek.

Bert Gunter



?On Sat, Dec 8, 2018 at 10:33 AM ?????? ??????? ?????? <
emanismail.92 at gmail.com> wrote:?

> I want to simulate data from Multivariate Bernoulli
> I have probability for each variable but unknown correlation for each pair
> I want to try all possible correlation value that made me cover all space
> if you could have a look on attached paper
> I am looking on implementation in R to that paper
> I appreciate if you could help me with any references or links
>
> Thanks in advance
>
> ??? ?????? 8 ?????? 2018 ?? 7:36 ? ??? ????? ?? ??? ?????? ?Bert Gunter??
> <?bgunter.4567 at gmail.com??>:?
>
>> I have no idea why your post was "rejected," nor even quite what you mean
>> by that. But I believe your post may not receive any replies because you
>> have failed to follow the posting guide linked below. I find it
>> incomprehensible, but maybe others will be able to understand what you
>> want. It may also be off topic -- this list is about R programming, not
>> statistics (though they do sometimes intersect) -- but that may just be
>> because I didn't understand your query.
>>
>> Cheers,
>> Bert
>>
>>
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming along
>> and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>> ?On Sat, Dec 8, 2018 at 8:55 AM ?????? ??????? ?????? <
>> emanismail.92 at gmail.com> wrote:?
>>
>>> Hi all,
>>> I was wondering how can I construct range of correlations matrix that
>>> cover
>>> all space
>>> from dependent Multivariate Bernoulli (known Marginal Probabilities but
>>> unknown correlations)
>>> I have P's for every variable but unknown correlation between each pair
>>> I want to try range of applicable correlations
>>>
>>> Why my post rejected?
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>

	[[alternative HTML version deleted]]


From dwin@emiu@ @ending from comc@@t@net  Sat Dec  8 22:29:34 2018
From: dwin@emiu@ @ending from comc@@t@net (David Winsemius)
Date: Sat, 8 Dec 2018 13:29:34 -0800
Subject: [R] Confirm Adding to mailing list
In-Reply-To: <CAFiZHK6m3YDt9X1ic3tYya36_v0SOrMEvYcQ6o6E2k9Fo7jqPA@mail.gmail.com>
References: <CAFiZHK6m3YDt9X1ic3tYya36_v0SOrMEvYcQ6o6E2k9Fo7jqPA@mail.gmail.com>
Message-ID: <60452538-43a7-34cf-424a-33313882308c@comcast.net>

I cannot tell what is being asked but it seems clear that you are 
confused about how mailing lists work.. I've looked in the pending posts 
in the moderation queue and none of them appear to be from you. I saw an 
earlier misdirected "confirm" message that should have gone to the 
mailserver-bot and not to the mailing list.

David.


On 12/8/18 4:54 AM, ????? ??????? ???? wrote:
> Please I need to post in R-help
> I need to confirm first adding to mailing list
> Any help ??
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From gh@d@mhed@t77 @ending from gm@il@com  Sat Dec  8 11:46:32 2018
From: gh@d@mhed@t77 @ending from gm@il@com (ghada mhedat)
Date: Sat, 8 Dec 2018 12:46:32 +0200
Subject: [R] General information
Message-ID: <CAPOduybAFuu=5fpu7fR7fn1p_abdQaN8QHUM1mAb8z3+yZLegg@mail.gmail.com>

Hello

I am graduate student I make a research about data mining tool ...one of
them R
I need to answer theses questions and I need your help :
1.what language does R wrote in?
2.how much users of R ?
3.year of create?
4. What is the appropriate platform?
5.file format?

Pleas I need the answer of these questions as soon as possible
Thank you

	[[alternative HTML version deleted]]


From jdnewmil @ending from dcn@d@vi@@c@@u@  Sat Dec  8 22:54:14 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Sat, 08 Dec 2018 13:54:14 -0800
Subject: [R] General information
In-Reply-To: <CAPOduybAFuu=5fpu7fR7fn1p_abdQaN8QHUM1mAb8z3+yZLegg@mail.gmail.com>
References: <CAPOduybAFuu=5fpu7fR7fn1p_abdQaN8QHUM1mAb8z3+yZLegg@mail.gmail.com>
Message-ID: <863CBEBD-92DF-406F-A35A-377249602C06@dcn.davis.ca.us>

Please use a search engine, as all of the answers to these questions are already written down online, and this mailing list is primarily handled by volunteers who would have to do your searching for you to answer your questions.

On December 8, 2018 2:46:32 AM PST, ghada mhedat <ghadamhedat77 at gmail.com> wrote:
>Hello
>
>I am graduate student I make a research about data mining tool ...one
>of
>them R
>I need to answer theses questions and I need your help :
>1.what language does R wrote in?
>2.how much users of R ?
>3.year of create?
>4. What is the appropriate platform?
>5.file format?
>
>Pleas I need the answer of these questions as soon as possible
>Thank you
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From em@ni@m@il@92 @ending from gm@il@com  Sat Dec  8 22:10:28 2018
From: em@ni@m@il@92 @ending from gm@il@com (=?UTF-8?B?2KXZitmF2KfZhiDYpdiz2YXYp9i52YrZhCDZhdit2YXYrw==?=)
Date: Sat, 8 Dec 2018 23:10:28 +0200
Subject: [R] Generate Range of Correlations Matrix Bernoulli
In-Reply-To: <CAGxFJbTHaNBu6Akok-TW4fZjNW0J8DnUFiuSXLZZLi0m_xDBWg@mail.gmail.com>
References: <CAFiZHK4E0=yqKBMY=gBrSGKc-FTzo2gx7wdH-0LEk3JvK38arw@mail.gmail.com>
 <CAGxFJbQSMho9WCSTBdHDyFhQQV23QO0f0whXXvTAsQ2o6zcC_A@mail.gmail.com>
 <CAFiZHK5127Zg82Xp4kJdmrZe8soR38kfC_QoiKLkF6Zi_EJ=Ew@mail.gmail.com>
 <CAGxFJbTHaNBu6Akok-TW4fZjNW0J8DnUFiuSXLZZLi0m_xDBWg@mail.gmail.com>
Message-ID: <CAFiZHK7QP-Qn_5aQ8-+P4hcFz-qzARzvbj_-8XdvNzUb0XWDXg@mail.gmail.com>

Thank you for the clarification.

??? ?????? 8 ?????? 2018 ?? 10:13 ? ??? ????? ?? ??? ?????? ?Bert Gunter??
<?bgunter.4567 at gmail.com??>:?

> Unless you are sending a private message, always include the list (which I
> have cc'ed) in your reply, because, as here, individuals do not do private
> consulting and may be unwilling or unable to provide the help you seek.
>
> Bert Gunter
>
>
>
> ?On Sat, Dec 8, 2018 at 10:33 AM ?????? ??????? ?????? <
> emanismail.92 at gmail.com> wrote:?
>
>> I want to simulate data from Multivariate Bernoulli
>> I have probability for each variable but unknown correlation for each pair
>> I want to try all possible correlation value that made me cover all space
>> if you could have a look on attached paper
>> I am looking on implementation in R to that paper
>> I appreciate if you could help me with any references or links
>>
>> Thanks in advance
>>
>> ??? ?????? 8 ?????? 2018 ?? 7:36 ? ??? ????? ?? ??? ?????? ?Bert Gunter??
>> <?bgunter.4567 at gmail.com??>:?
>>
>>> I have no idea why your post was "rejected," nor even quite what you
>>> mean by that. But I believe your post may not receive any replies because
>>> you have failed to follow the posting guide linked below. I find it
>>> incomprehensible, but maybe others will be able to understand what you
>>> want. It may also be off topic -- this list is about R programming, not
>>> statistics (though they do sometimes intersect) -- but that may just be
>>> because I didn't understand your query.
>>>
>>> Cheers,
>>> Bert
>>>
>>>
>>> Bert Gunter
>>>
>>> "The trouble with having an open mind is that people keep coming along
>>> and sticking things into it."
>>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>>
>>>
>>> ?On Sat, Dec 8, 2018 at 8:55 AM ?????? ??????? ?????? <
>>> emanismail.92 at gmail.com> wrote:?
>>>
>>>> Hi all,
>>>> I was wondering how can I construct range of correlations matrix that
>>>> cover
>>>> all space
>>>> from dependent Multivariate Bernoulli (known Marginal Probabilities but
>>>> unknown correlations)
>>>> I have P's for every variable but unknown correlation between each pair
>>>> I want to try range of applicable correlations
>>>>
>>>> Why my post rejected?
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>

	[[alternative HTML version deleted]]


From dwin@emiu@ @ending from comc@@t@net  Sun Dec  9 01:28:03 2018
From: dwin@emiu@ @ending from comc@@t@net (David Winsemius)
Date: Sat, 8 Dec 2018 16:28:03 -0800
Subject: [R] General information
In-Reply-To: <CAPOduybAFuu=5fpu7fR7fn1p_abdQaN8QHUM1mAb8z3+yZLegg@mail.gmail.com>
References: <CAPOduybAFuu=5fpu7fR7fn1p_abdQaN8QHUM1mAb8z3+yZLegg@mail.gmail.com>
Message-ID: <3609799d-ac6c-8a4b-133b-85a87149c193@comcast.net>

Most of those answers are in the R-FAQ. It ships with every distribution 
copy from CRTAN and is also available online.


Estimating the number of R users would be difficult. There are probably 
20,000+ subscriblers to Rhelp.


Not certain what is meant by #5.


David.


On 12/8/18 2:46 AM, ghada mhedat wrote:
> Hello
>
> I am graduate student I make a research about data mining tool ...one of
> them R
> I need to answer theses questions and I need your help :
> 1.what language does R wrote in?
> 2.how much users of R ?
> 3.year of create?
> 4. What is the appropriate platform?
> 5.file format?
>
> Pleas I need the answer of these questions as soon as possible
> Thank you
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @@@h@@w@ng2017 @ending from y@ndex@com  Sun Dec  9 07:23:36 2018
From: @@@h@@w@ng2017 @ending from y@ndex@com (Jiayue Wang)
Date: Sun, 9 Dec 2018 14:23:36 +0800
Subject: [R] "subscript out of bounds" error when using koRpus+Tree Tagger
Message-ID: <73d9024b-7008-877c-cd56-c2ec5ce38b58@yandex.com>

Hi,

I'm trying to do text corpus processing on some novels, with koRpus 
package and Tree Tagger. The script lists all txt files (11 in all) in a 
dir, and processes it one by one.

##########
rm(list=ls())
library(koRpus)
library(koRpus.lang.en)
set.kRp.env(TT.cmd = "/pathto/tree-tagger-english", lang = "en")
outdir <- "/pathto/corpora"
corpdir <- paste0(outdir,"/","morrison11")

files <- list.files(path=corpdir, pattern = "*.txt", full.names = F)
n <- length(files)

output <- file(paste0(outdir,"/calc_results_morrison11.txt"), open="at")
for (i in 1:n) {
   cat(i," - ",files[i],"\n", file = output)
   tagged.results <- treetag(paste0(corpdir,'/',files[i]),
      treetagger="kRp.env")
   capture.output(flesch(tagged.results), file = output)
   cat("\n", file=output)
   capture.output(TTR(tagged.results), file = output)
   cat("\n", file=output)
   capture.output(textFeatures(tagged.results), file=output)
   cat("\n===========================\n", file = output)
}
close(output)
#########

The problem is, the script always throws the following error when it 
works on the last txt file and prematurely exits:

??Error in all.patterns[[word.length]] : subscript out of bounds

I can't figure out what this message means. the dir's are correct; 
there's no problem with Tree Tagger installation; n and files have the 
correct values.

Please help, many thanks!

Jiayue


From ruipb@rr@d@@ @ending from @@po@pt  Sun Dec  9 12:55:03 2018
From: ruipb@rr@d@@ @ending from @@po@pt (Rui Barradas)
Date: Sun, 9 Dec 2018 11:55:03 +0000
Subject: [R] General information
In-Reply-To: <3609799d-ac6c-8a4b-133b-85a87149c193@comcast.net>
References: <CAPOduybAFuu=5fpu7fR7fn1p_abdQaN8QHUM1mAb8z3+yZLegg@mail.gmail.com>
 <3609799d-ac6c-8a4b-133b-85a87149c193@comcast.net>
Message-ID: <a0ca8071-b457-cd21-85aa-0ddc0fb66be6@sapo.pt>

Hello,


?s 00:28 de 09/12/2018, David Winsemius escreveu:
> Most of those answers are in the R-FAQ. It ships with every distribution 
> copy from CRTAN and is also available online.
> 
> 
> Estimating the number of R users would be difficult. There are probably 
> 20,000+ subscriblers to Rhelp.

The OP asks "how much users of R ?" with an obvious error. Correct it, 
google "how many users of R ?"  and get 1,000,000 to 2,000,000 user 
worldwide.
According to [1]: "R has more than 2 million users worldwide (Oracle 
estimate, February 2012)".
The 1,000,000 comes from a 2009 New York Times article [2], [3]. Clearly 
outdated.

[1] 
https://blog.revolutionanalytics.com/2014/04/seven-quick-facts-about-r.html
[2] https://www.quora.com/How-many-people-use-R
[3] 
https://bits.blogs.nytimes.com/2009/01/08/r-you-ready-for-r/?_php=true&_r=0&_type=blogs

Rui Barradas

> 
> 
> Not certain what is meant by #5.
> 
> 
> David.
> 
> 
> On 12/8/18 2:46 AM, ghada mhedat wrote:
>> Hello
>>
>> I am graduate student I make a research about data mining tool ...one of
>> them R
>> I need to answer theses questions and I need your help :
>> 1.what language does R wrote in?
>> 2.how much users of R ?
>> 3.year of create?
>> 4. What is the appropriate platform?
>> 5.file format?
>>
>> Pleas I need the answer of these questions as soon as possible
>> Thank you
>>
>> ????[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From j@zh@o @ending from ye@h@net  Sun Dec  9 16:05:05 2018
From: j@zh@o @ending from ye@h@net (Jinsong Zhao)
Date: Sun, 9 Dec 2018 23:05:05 +0800 (CST)
Subject: [R] how to keep colnames of matrix when put it into a data frame
Message-ID: <5bbaefdc.b07e.16793801c35.Coremail.jszhao@yeah.net>

Hi there,

In the following mini-example, I hope to keep the column names of mat, but failed.

# mini-example
> mat <- matrix(1:9, nrow = 3)
> colnames(mat) <- paste("(", 1:3, ")", sep = "")
> mat
     (1) (2) (3)
[1,]   1   4   7
[2,]   2   5   8
[3,]   3   6   9
> data.frame(x = 1:3, mat)
  x X.1. X.2. X.3.
1 1    1    4    7
2 2    2    5    8
3 3    3    6    9

Any hints will be really appreciated.

Best,
Jinsong
	[[alternative HTML version deleted]]


From li@t@ @ending from dewey@myzen@co@uk  Sun Dec  9 16:11:52 2018
From: li@t@ @ending from dewey@myzen@co@uk (Michael Dewey)
Date: Sun, 9 Dec 2018 15:11:52 +0000
Subject: [R] 
 how to keep colnames of matrix when put it into a data frame
In-Reply-To: <5bbaefdc.b07e.16793801c35.Coremail.jszhao@yeah.net>
References: <5bbaefdc.b07e.16793801c35.Coremail.jszhao@yeah.net>
Message-ID: <0907571a-93bf-54ba-9608-3e8eda18af62@dewey.myzen.co.uk>

Dear Jinsong

Try cbind(x = 1:3, mat)
and see if that helps

Michael

On 09/12/2018 15:05, Jinsong Zhao wrote:
> Hi there,
> 
> In the following mini-example, I hope to keep the column names of mat, but failed.
> 
> # mini-example
>> mat <- matrix(1:9, nrow = 3)
>> colnames(mat) <- paste("(", 1:3, ")", sep = "")
>> mat
>       (1) (2) (3)
> [1,]   1   4   7
> [2,]   2   5   8
> [3,]   3   6   9
>> data.frame(x = 1:3, mat)
>    x X.1. X.2. X.3.
> 1 1    1    4    7
> 2 2    2    5    8
> 3 3    3    6    9
> 
> Any hints will be really appreciated.
> 
> Best,
> Jinsong
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From bgunter@4567 @ending from gm@il@com  Sun Dec  9 16:29:26 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Sun, 9 Dec 2018 07:29:26 -0800
Subject: [R] 
 how to keep colnames of matrix when put it into a data frame
In-Reply-To: <5bbaefdc.b07e.16793801c35.Coremail.jszhao@yeah.net>
References: <5bbaefdc.b07e.16793801c35.Coremail.jszhao@yeah.net>
Message-ID: <CAGxFJbQmXi-k-DYQH53-0VRtzzU8A575k8-6gmjeXkBxbVUNuQ@mail.gmail.com>

Your names are not syntactically valid.

Consider:

> mat <- matrix(1:9, nrow = 3)
> colnames(mat) <- letters[1:3]
> mat
     a b c
[1,] 1 4 7
[2,] 2 5 8
[3,] 3 6 9
> data.frame(x=1:3,mat)
  x a b c
1 1 1 4 7
2 2 2 5 8
3 3 3 6 9

See ?make.names, and the "Value" section of ?data.frame for how names are
constructed.

Michael's suggestion produces a matrix, not  a data frame. dimnames of
matrices apparently have different rules for validity.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, Dec 9, 2018 at 7:05 AM Jinsong Zhao <jszhao at yeah.net> wrote:

> Hi there,
>
> In the following mini-example, I hope to keep the column names of mat, but
> failed.
>
> # mini-example
> > mat <- matrix(1:9, nrow = 3)
> > colnames(mat) <- paste("(", 1:3, ")", sep = "")
> > mat
>      (1) (2) (3)
> [1,]   1   4   7
> [2,]   2   5   8
> [3,]   3   6   9
> > data.frame(x = 1:3, mat)
>   x X.1. X.2. X.3.
> 1 1    1    4    7
> 2 2    2    5    8
> 3 3    3    6    9
>
> Any hints will be really appreciated.
>
> Best,
> Jinsong
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewmil @ending from dcn@d@vi@@c@@u@  Sun Dec  9 16:31:25 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Sun, 09 Dec 2018 07:31:25 -0800
Subject: [R] 
 how to keep colnames of matrix when put it into a data frame
In-Reply-To: <0907571a-93bf-54ba-9608-3e8eda18af62@dewey.myzen.co.uk>
References: <5bbaefdc.b07e.16793801c35.Coremail.jszhao@yeah.net>
 <0907571a-93bf-54ba-9608-3e8eda18af62@dewey.myzen.co.uk>
Message-ID: <5CBFAC02-2D1F-430B-B35F-B8E5B11E73E9@dcn.davis.ca.us>

Read ?data.frame 

In particular, notice the check.names argument.

On December 9, 2018 7:11:52 AM PST, Michael Dewey <lists at dewey.myzen.co.uk> wrote:
>Dear Jinsong
>
>Try cbind(x = 1:3, mat)
>and see if that helps
>
>Michael
>
>On 09/12/2018 15:05, Jinsong Zhao wrote:
>> Hi there,
>> 
>> In the following mini-example, I hope to keep the column names of
>mat, but failed.
>> 
>> # mini-example
>>> mat <- matrix(1:9, nrow = 3)
>>> colnames(mat) <- paste("(", 1:3, ")", sep = "")
>>> mat
>>       (1) (2) (3)
>> [1,]   1   4   7
>> [2,]   2   5   8
>> [3,]   3   6   9
>>> data.frame(x = 1:3, mat)
>>    x X.1. X.2. X.3.
>> 1 1    1    4    7
>> 2 2    2    5    8
>> 3 3    3    6    9
>> 
>> Any hints will be really appreciated.
>> 
>> Best,
>> Jinsong
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 

-- 
Sent from my phone. Please excuse my brevity.


From giftedlife2014 @ending from gm@il@com  Sun Dec  9 16:40:58 2018
From: giftedlife2014 @ending from gm@il@com (Ogbos Okike)
Date: Sun, 9 Dec 2018 16:40:58 +0100
Subject: [R] Plotting Very Large lat-lon data in x-y axes graph
Message-ID: <CAC8ss33ZS-8NqHf+6Q3Rk=qt+Z7+O7sVFV9Ahi+X8U=DqxhOsw@mail.gmail.com>

Dear Contributors,

I have a data of the form:
Lat          Lon
30.1426 104.7854
30.5622 105.0837
30.0966 104.6213
29.9795 104.8430
39.2802 147.7295
30.2469 104.6543
26.4428 157.7293
29.4782 104.5590
32.3839 105.3293
26.4746 157.8411
25.1014 159.6959
25.1242 159.6558
30.1607 104.9100
31.4900 -71.8919
43.3655 -74.9994
30.0811 104.8462
29.0912 -85.5138
26.6204 -80.9342
31.5462 -71.9638
26.8619 97.3844
30.2534 104.6134
29.9311 -85.3434
26.1524 159.6806
26.5112 158.0233
26.5441 158.0565
27.8901 -105.8554
30.3175 104.7135
26.4822 157.6127
30.1887 104.5986
29.5058 104.5661
26.4010 157.5749
30.2281 104.7585
31.4556 110.5619
30.1700 104.5861
26.3911 157.4776
30.6493 104.9949
30.2209 104.6629
26.0488 97.3608
30.2142 104.8023
30.1806 104.8158
25.2107 160.1690
30.6708 104.9385
30.4152 104.7002
30.2446 104.7804
29.5760 -85.1535
26.4484 92.4312
26.3914 157.4189
26.3986 157.4421
30.4903 -88.2271
30.6727 104.8768
30.2518 104.6466
41.6979 -78.4136
33.7575 72.1089
26.8333 -80.9485
25.3103 124.0978
30.1742 104.7554
30.6345 104.9739
30.2075 104.7960
30.2226 104.7517
30.5948 105.0532.
The record is for lightning flashes in the continental U.S. and
surrounding waters within the latitudinal band between
258 and 458N.

I want to display the result in x-y co-ordinate plot. However, the
data is very large such that when plotted, everything just appeared
blurred.


Is there a way of using color codes to  indicate the regions of higher
or lower flash densities?

I can attach the plot I generated but I am not sure if the moderator
will allow it to go with this.

I will send it in a separate email if required.

Thank you so much for sparing your time.

Best
Ogbos


From ruipb@rr@d@@ @ending from @@po@pt  Sun Dec  9 16:56:13 2018
From: ruipb@rr@d@@ @ending from @@po@pt (Rui Barradas)
Date: Sun, 9 Dec 2018 15:56:13 +0000
Subject: [R] 
 how to keep colnames of matrix when put it into a data frame
In-Reply-To: <0907571a-93bf-54ba-9608-3e8eda18af62@dewey.myzen.co.uk>
References: <5bbaefdc.b07e.16793801c35.Coremail.jszhao@yeah.net>
 <0907571a-93bf-54ba-9608-3e8eda18af62@dewey.myzen.co.uk>
Message-ID: <a8cfb947-080c-a480-1b9f-1967b5e53320@sapo.pt>

Hello,

cbind alone will return a matrix, cbind.data.frame is probably what the 
OP wants.

class(cbind(x = 1:3, mat))
#[1] "matrix"

cbind.data.frame(x = 1:3, mat)
#  x (1) (2) (3)
#1 1   1   4   7
#2 2   2   5   8
#3 3   3   6   9


Hope this helps,

Rui Barradas


?s 15:11 de 09/12/2018, Michael Dewey escreveu:
> Dear Jinsong
> 
> Try cbind(x = 1:3, mat)
> and see if that helps
> 
> Michael
> 
> On 09/12/2018 15:05, Jinsong Zhao wrote:
>> Hi there,
>>
>> In the following mini-example, I hope to keep the column names of mat, 
>> but failed.
>>
>> # mini-example
>>> mat <- matrix(1:9, nrow = 3)
>>> colnames(mat) <- paste("(", 1:3, ")", sep = "")
>>> mat
>> ????? (1) (2) (3)
>> [1,]?? 1?? 4?? 7
>> [2,]?? 2?? 5?? 8
>> [3,]?? 3?? 6?? 9
>>> data.frame(x = 1:3, mat)
>> ?? x X.1. X.2. X.3.
>> 1 1??? 1??? 4??? 7
>> 2 2??? 2??? 5??? 8
>> 3 3??? 3??? 6??? 9
>>
>> Any hints will be really appreciated.
>>
>> Best,
>> Jinsong
>> ????[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


From bgunter@4567 @ending from gm@il@com  Sun Dec  9 17:04:35 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Sun, 9 Dec 2018 08:04:35 -0800
Subject: [R] Plotting Very Large lat-lon data in x-y axes graph
In-Reply-To: <CAC8ss33ZS-8NqHf+6Q3Rk=qt+Z7+O7sVFV9Ahi+X8U=DqxhOsw@mail.gmail.com>
References: <CAC8ss33ZS-8NqHf+6Q3Rk=qt+Z7+O7sVFV9Ahi+X8U=DqxhOsw@mail.gmail.com>
Message-ID: <CAGxFJbS9GVmrgNZ4V352+hj1FdrBQy3e-bQED11WbCOSbF0nDg@mail.gmail.com>

Yes, there are many ways to do this. Search rseek.org for "2d density
plots". Also check the CRAN "Spatial" task view. Also see the kde2d
function in the MASS package and especially the examples there that use the
image() function to plot densities.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, Dec 9, 2018 at 7:50 AM Ogbos Okike <giftedlife2014 at gmail.com> wrote:

> Dear Contributors,
>
> I have a data of the form:
> Lat          Lon
> 30.1426 104.7854
> 30.5622 105.0837
> 30.0966 104.6213
> 29.9795 104.8430
> 39.2802 147.7295
> 30.2469 104.6543
> 26.4428 157.7293
> 29.4782 104.5590
> 32.3839 105.3293
> 26.4746 157.8411
> 25.1014 159.6959
> 25.1242 159.6558
> 30.1607 104.9100
> 31.4900 -71.8919
> 43.3655 -74.9994
> 30.0811 104.8462
> 29.0912 -85.5138
> 26.6204 -80.9342
> 31.5462 -71.9638
> 26.8619 97.3844
> 30.2534 104.6134
> 29.9311 -85.3434
> 26.1524 159.6806
> 26.5112 158.0233
> 26.5441 158.0565
> 27.8901 -105.8554
> 30.3175 104.7135
> 26.4822 157.6127
> 30.1887 104.5986
> 29.5058 104.5661
> 26.4010 157.5749
> 30.2281 104.7585
> 31.4556 110.5619
> 30.1700 104.5861
> 26.3911 157.4776
> 30.6493 104.9949
> 30.2209 104.6629
> 26.0488 97.3608
> 30.2142 104.8023
> 30.1806 104.8158
> 25.2107 160.1690
> 30.6708 104.9385
> 30.4152 104.7002
> 30.2446 104.7804
> 29.5760 -85.1535
> 26.4484 92.4312
> 26.3914 157.4189
> 26.3986 157.4421
> 30.4903 -88.2271
> 30.6727 104.8768
> 30.2518 104.6466
> 41.6979 -78.4136
> 33.7575 72.1089
> 26.8333 -80.9485
> 25.3103 124.0978
> 30.1742 104.7554
> 30.6345 104.9739
> 30.2075 104.7960
> 30.2226 104.7517
> 30.5948 105.0532.
> The record is for lightning flashes in the continental U.S. and
> surrounding waters within the latitudinal band between
> 258 and 458N.
>
> I want to display the result in x-y co-ordinate plot. However, the
> data is very large such that when plotted, everything just appeared
> blurred.
>
>
> Is there a way of using color codes to  indicate the regions of higher
> or lower flash densities?
>
> I can attach the plot I generated but I am not sure if the moderator
> will allow it to go with this.
>
> I will send it in a separate email if required.
>
> Thank you so much for sparing your time.
>
> Best
> Ogbos
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drjimlemon @ending from gm@il@com  Sun Dec  9 21:46:43 2018
From: drjimlemon @ending from gm@il@com (Jim Lemon)
Date: Mon, 10 Dec 2018 07:46:43 +1100
Subject: [R] Plotting Very Large lat-lon data in x-y axes graph
In-Reply-To: <CAC8ss33ZS-8NqHf+6Q3Rk=qt+Z7+O7sVFV9Ahi+X8U=DqxhOsw@mail.gmail.com>
References: <CAC8ss33ZS-8NqHf+6Q3Rk=qt+Z7+O7sVFV9Ahi+X8U=DqxhOsw@mail.gmail.com>
Message-ID: <CA+8X3fU04pA_nh2DvXmY9vUWGr7bsj8AwRBp9Xd=qFvGmj6RMQ@mail.gmail.com>

Hi Ogbos,
Here is a slight modification of a method I use to display trip
density on a map:

oolt<-read.table(text="Lat Lon
30.1426 104.7854
30.5622 105.0837
30.0966 104.6213
29.9795 104.8430
39.2802 147.7295
30.2469 104.6543
26.4428 157.7293
29.4782 104.5590
32.3839 105.3293
26.4746 157.8411
25.1014 159.6959
25.1242 159.6558
30.1607 104.9100
31.4900 -71.8919
43.3655 -74.9994
30.0811 104.8462
29.0912 -85.5138
26.6204 -80.9342
31.5462 -71.9638
26.8619 97.3844
30.2534 104.6134
29.9311 -85.3434
26.1524 159.6806
26.5112 158.0233
26.5441 158.0565
27.8901 -105.8554
30.3175 104.7135
26.4822 157.6127
30.1887 104.5986
29.5058 104.5661
26.4010 157.5749
30.2281 104.7585
31.4556 110.5619
30.1700 104.5861
26.3911 157.4776
30.6493 104.9949
30.2209 104.6629
26.0488 97.3608
30.2142 104.8023
30.1806 104.8158
25.2107 160.1690
30.6708 104.9385
30.4152 104.7002
30.2446 104.7804
29.5760 -85.1535
26.4484 92.4312
26.3914 157.4189
26.3986 157.4421
30.4903 -88.2271
30.6727 104.8768
30.2518 104.6466
41.6979 -78.4136
33.7575 72.1089
26.8333 -80.9485
25.3103 124.0978
30.1742 104.7554
30.6345 104.9739
30.2075 104.7960
30.2226 104.7517
30.5948 105.0532",
header=TRUE)
latlim<-c(20,45)
lonlim<-c(-90,160)
latbreaks<-seq(latlim[1],latlim[2],by=5)
lonbreaks<-seq(lonlim[1],lonlim[2],by=10)

mids<-function(x) {
 lenx<-length(x)
 return((x[1:(lenx-1)]+x[2:lenx])/2)
}
lonmids<-mids(lonbreaks)
latmids<-mids(latbreaks)
oolt$loncuts<-cut(oolt$Lon,lonbreaks)
oolt$latcuts<-cut(oolt$Lat,latbreaks)
counts<-table(oolt$latcuts,oolt$loncuts)
library(plotrix)
countcol<-color.scale(counts,extremes=c("blue","red"))
map("world",xlim=c(-90,160),ylim=c(20,45))
for(lon in 1:length(lonmids)) {
 for(lat in 1:length(latmids)) {
  if(counts[lat,lon] > 0)
   draw.circle(lonmids[lon],latmids[lat],radius=sqrt(counts[lat,lon]),
    border=countcol[lat,lon],col=countcol[lat,lon])
 }
}

If you have very large counts in some places you may need to adjust
the radius of the circles.

Jim
On Mon, Dec 10, 2018 at 2:50 AM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>
> Dear Contributors,
>
> I have a data of the form:
> Lat          Lon
> 30.1426 104.7854
> 30.5622 105.0837
> 30.0966 104.6213
> 29.9795 104.8430
> 39.2802 147.7295
> 30.2469 104.6543
> 26.4428 157.7293
> 29.4782 104.5590
> 32.3839 105.3293
> 26.4746 157.8411
> 25.1014 159.6959
> 25.1242 159.6558
> 30.1607 104.9100
> 31.4900 -71.8919
> 43.3655 -74.9994
> 30.0811 104.8462
> 29.0912 -85.5138
> 26.6204 -80.9342
> 31.5462 -71.9638
> 26.8619 97.3844
> 30.2534 104.6134
> 29.9311 -85.3434
> 26.1524 159.6806
> 26.5112 158.0233
> 26.5441 158.0565
> 27.8901 -105.8554
> 30.3175 104.7135
> 26.4822 157.6127
> 30.1887 104.5986
> 29.5058 104.5661
> 26.4010 157.5749
> 30.2281 104.7585
> 31.4556 110.5619
> 30.1700 104.5861
> 26.3911 157.4776
> 30.6493 104.9949
> 30.2209 104.6629
> 26.0488 97.3608
> 30.2142 104.8023
> 30.1806 104.8158
> 25.2107 160.1690
> 30.6708 104.9385
> 30.4152 104.7002
> 30.2446 104.7804
> 29.5760 -85.1535
> 26.4484 92.4312
> 26.3914 157.4189
> 26.3986 157.4421
> 30.4903 -88.2271
> 30.6727 104.8768
> 30.2518 104.6466
> 41.6979 -78.4136
> 33.7575 72.1089
> 26.8333 -80.9485
> 25.3103 124.0978
> 30.1742 104.7554
> 30.6345 104.9739
> 30.2075 104.7960
> 30.2226 104.7517
> 30.5948 105.0532.
> The record is for lightning flashes in the continental U.S. and
> surrounding waters within the latitudinal band between
> 258 and 458N.
>
> I want to display the result in x-y co-ordinate plot. However, the
> data is very large such that when plotted, everything just appeared
> blurred.
>
>
> Is there a way of using color codes to  indicate the regions of higher
> or lower flash densities?
>
> I can attach the plot I generated but I am not sure if the moderator
> will allow it to go with this.
>
> I will send it in a separate email if required.
>
> Thank you so much for sparing your time.
>
> Best
> Ogbos
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From kmezhoud @ending from gm@il@com  Sun Dec  9 22:06:51 2018
From: kmezhoud @ending from gm@il@com (Karim Mezhoud)
Date: Sun, 9 Dec 2018 22:06:51 +0100
Subject: [R] Spark DataFrame: replace NULL cell by NA
Message-ID: <CALJKBv_DrtA_SsY3HhvFWhj4GChWk9DfzbDYu3pTq1ootRYw2Q@mail.gmail.com>

Dear All,
## function to relpace empty cell by NA
empty_as_na <- function(x){
  if("factor" %in% class(x)) x <- as.character(x) ## since ifelse wont work
with factors
  ifelse(as.character(x)!="", x, NA)
}

## connect to spark local
sc <- spark_connect(master = "local")
# load an example of dataframe taht has empty cells (needs cgdsr package)
clinicalData <- cgdsr::getClinicalData(cgds, "gbm_tcga_pub_all")
## copy to spark
clinicalData_tbl <- dplyr::copy_to(sc, clinicalData, overwrite = TRUE)

 # This works
clinicalData %>% mutate_all(funs(empty_as_na))
# This Does not works
clinicalData_tbl %>% mutate_all(funs(empty_as_na))
Thanks,
Karim

	[[alternative HTML version deleted]]


From bgunter@4567 @ending from gm@il@com  Sun Dec  9 22:47:00 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Sun, 9 Dec 2018 13:47:00 -0800
Subject: [R] Spark DataFrame: replace NULL cell by NA
In-Reply-To: <CALJKBv_DrtA_SsY3HhvFWhj4GChWk9DfzbDYu3pTq1ootRYw2Q@mail.gmail.com>
References: <CALJKBv_DrtA_SsY3HhvFWhj4GChWk9DfzbDYu3pTq1ootRYw2Q@mail.gmail.com>
Message-ID: <CAGxFJbQFB66wp5T3SaoqvnL6T7uHcEjAyJA7yupESfc9rtuD9g@mail.gmail.com>

"...   if("factor" %in% class(x)) x <- as.character(x) ## since ifelse wont
work with factors  "
Nonsense!

> x <- factor(c("a","", "b"))
> x
[1] a   b
Levels:  a b

> levels(x)
[1] ""  "a" "b"

> x <- factor(ifelse(x==(""),NA,x))
> x
[1] 2    <NA> 3
Levels: 2 3


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, Dec 9, 2018 at 1:07 PM Karim Mezhoud <kmezhoud at gmail.com> wrote:

> Dear All,
> ## function to relpace empty cell by NA
> empty_as_na <- function(x){
>   if("factor" %in% class(x)) x <- as.character(x) ## since ifelse wont work
> with factors
>   ifelse(as.character(x)!="", x, NA)
> }
>
> ## connect to spark local
> sc <- spark_connect(master = "local")
> # load an example of dataframe taht has empty cells (needs cgdsr package)
> clinicalData <- cgdsr::getClinicalData(cgds, "gbm_tcga_pub_all")
> ## copy to spark
> clinicalData_tbl <- dplyr::copy_to(sc, clinicalData, overwrite = TRUE)
>
>  # This works
> clinicalData %>% mutate_all(funs(empty_as_na))
> # This Does not works
> clinicalData_tbl %>% mutate_all(funs(empty_as_na))
> Thanks,
> Karim
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From giftedlife2014 @ending from gm@il@com  Mon Dec 10 06:25:04 2018
From: giftedlife2014 @ending from gm@il@com (Ogbos Okike)
Date: Mon, 10 Dec 2018 06:25:04 +0100
Subject: [R] Plotting Very Large lat-lon data in x-y axes graph
In-Reply-To: <CA+8X3fU04pA_nh2DvXmY9vUWGr7bsj8AwRBp9Xd=qFvGmj6RMQ@mail.gmail.com>
References: <CAC8ss33ZS-8NqHf+6Q3Rk=qt+Z7+O7sVFV9Ahi+X8U=DqxhOsw@mail.gmail.com>
 <CA+8X3fU04pA_nh2DvXmY9vUWGr7bsj8AwRBp9Xd=qFvGmj6RMQ@mail.gmail.com>
Message-ID: <CAC8ss32fAjoh1Q2+7x9i25EyXCGAbeqjYOtG4r3WuWi2S6st4A@mail.gmail.com>

Dear Jim,

I used a bit of my data in my attempt to follow the code.

I got the attached plot after some tweaks.

I would like to add horizontal color bar legend that could be used to
explain the number of lightning counts at different points on the
latitude band as plotted.

Thank you
Warm regards
Ogbos

On Sun, Dec 9, 2018 at 9:46 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi Ogbos,
> Here is a slight modification of a method I use to display trip
> density on a map:
>
> oolt<-read.table(text="Lat Lon
> 30.1426 104.7854
> 30.5622 105.0837
> 30.0966 104.6213
> 29.9795 104.8430
> 39.2802 147.7295
> 30.2469 104.6543
> 26.4428 157.7293
> 29.4782 104.5590
> 32.3839 105.3293
> 26.4746 157.8411
> 25.1014 159.6959
> 25.1242 159.6558
> 30.1607 104.9100
> 31.4900 -71.8919
> 43.3655 -74.9994
> 30.0811 104.8462
> 29.0912 -85.5138
> 26.6204 -80.9342
> 31.5462 -71.9638
> 26.8619 97.3844
> 30.2534 104.6134
> 29.9311 -85.3434
> 26.1524 159.6806
> 26.5112 158.0233
> 26.5441 158.0565
> 27.8901 -105.8554
> 30.3175 104.7135
> 26.4822 157.6127
> 30.1887 104.5986
> 29.5058 104.5661
> 26.4010 157.5749
> 30.2281 104.7585
> 31.4556 110.5619
> 30.1700 104.5861
> 26.3911 157.4776
> 30.6493 104.9949
> 30.2209 104.6629
> 26.0488 97.3608
> 30.2142 104.8023
> 30.1806 104.8158
> 25.2107 160.1690
> 30.6708 104.9385
> 30.4152 104.7002
> 30.2446 104.7804
> 29.5760 -85.1535
> 26.4484 92.4312
> 26.3914 157.4189
> 26.3986 157.4421
> 30.4903 -88.2271
> 30.6727 104.8768
> 30.2518 104.6466
> 41.6979 -78.4136
> 33.7575 72.1089
> 26.8333 -80.9485
> 25.3103 124.0978
> 30.1742 104.7554
> 30.6345 104.9739
> 30.2075 104.7960
> 30.2226 104.7517
> 30.5948 105.0532",
> header=TRUE)
> latlim<-c(20,45)
> lonlim<-c(-90,160)
> latbreaks<-seq(latlim[1],latlim[2],by=5)
> lonbreaks<-seq(lonlim[1],lonlim[2],by=10)
>
> mids<-function(x) {
>  lenx<-length(x)
>  return((x[1:(lenx-1)]+x[2:lenx])/2)
> }
> lonmids<-mids(lonbreaks)
> latmids<-mids(latbreaks)
> oolt$loncuts<-cut(oolt$Lon,lonbreaks)
> oolt$latcuts<-cut(oolt$Lat,latbreaks)
> counts<-table(oolt$latcuts,oolt$loncuts)
> library(plotrix)
> countcol<-color.scale(counts,extremes=c("blue","red"))
> map("world",xlim=c(-90,160),ylim=c(20,45))
> for(lon in 1:length(lonmids)) {
>  for(lat in 1:length(latmids)) {
>   if(counts[lat,lon] > 0)
>    draw.circle(lonmids[lon],latmids[lat],radius=sqrt(counts[lat,lon]),
>     border=countcol[lat,lon],col=countcol[lat,lon])
>  }
> }
>
> If you have very large counts in some places you may need to adjust
> the radius of the circles.
>
> Jim
> On Mon, Dec 10, 2018 at 2:50 AM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
> >
> > Dear Contributors,
> >
> > I have a data of the form:
> > Lat          Lon
> > 30.1426 104.7854
> > 30.5622 105.0837
> > 30.0966 104.6213
> > 29.9795 104.8430
> > 39.2802 147.7295
> > 30.2469 104.6543
> > 26.4428 157.7293
> > 29.4782 104.5590
> > 32.3839 105.3293
> > 26.4746 157.8411
> > 25.1014 159.6959
> > 25.1242 159.6558
> > 30.1607 104.9100
> > 31.4900 -71.8919
> > 43.3655 -74.9994
> > 30.0811 104.8462
> > 29.0912 -85.5138
> > 26.6204 -80.9342
> > 31.5462 -71.9638
> > 26.8619 97.3844
> > 30.2534 104.6134
> > 29.9311 -85.3434
> > 26.1524 159.6806
> > 26.5112 158.0233
> > 26.5441 158.0565
> > 27.8901 -105.8554
> > 30.3175 104.7135
> > 26.4822 157.6127
> > 30.1887 104.5986
> > 29.5058 104.5661
> > 26.4010 157.5749
> > 30.2281 104.7585
> > 31.4556 110.5619
> > 30.1700 104.5861
> > 26.3911 157.4776
> > 30.6493 104.9949
> > 30.2209 104.6629
> > 26.0488 97.3608
> > 30.2142 104.8023
> > 30.1806 104.8158
> > 25.2107 160.1690
> > 30.6708 104.9385
> > 30.4152 104.7002
> > 30.2446 104.7804
> > 29.5760 -85.1535
> > 26.4484 92.4312
> > 26.3914 157.4189
> > 26.3986 157.4421
> > 30.4903 -88.2271
> > 30.6727 104.8768
> > 30.2518 104.6466
> > 41.6979 -78.4136
> > 33.7575 72.1089
> > 26.8333 -80.9485
> > 25.3103 124.0978
> > 30.1742 104.7554
> > 30.6345 104.9739
> > 30.2075 104.7960
> > 30.2226 104.7517
> > 30.5948 105.0532.
> > The record is for lightning flashes in the continental U.S. and
> > surrounding waters within the latitudinal band between
> > 258 and 458N.
> >
> > I want to display the result in x-y co-ordinate plot. However, the
> > data is very large such that when plotted, everything just appeared
> > blurred.
> >
> >
> > Is there a way of using color codes to  indicate the regions of higher
> > or lower flash densities?
> >
> > I can attach the plot I generated but I am not sure if the moderator
> > will allow it to go with this.
> >
> > I will send it in a separate email if required.
> >
> > Thank you so much for sparing your time.
> >
> > Best
> > Ogbos
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: test.png
Type: image/png
Size: 53400 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20181210/addbf450/attachment.png>

From i@ubir@n@ @ending from imim@e@  Mon Dec 10 07:49:00 2018
From: i@ubir@n@ @ending from imim@e@ (Subirana Cachinero, Isaac)
Date: Mon, 10 Dec 2018 06:49:00 +0000
Subject: [R] repeating the same variable in formula
Message-ID: <04FA967CF3C1A14AB593785C317F043506D265B5@HERMES4.imim.es>

I am using terms.formula function to substract the variables from a formula environment.

Concretely, with the attr(*, "term.labels") I get the right side terms specified in the formula.

However, when introducing the same variable two or more times, this appears only once.

For example, when typing



>f <- y ~ cholesterol + age + age

>attr(terms(f),"term.labels")



I get



[1] "cholesterol" "age"



and I would like to get



[1] "cholesterol" "age" "age"





Is it possible with terms.formula function to obtain the variables and the times that they are specified in the formula? Or may I use another function?



Thanks in advance.



Isaac.


	[[alternative HTML version deleted]]


From d@gm@r@cimiotti @ending from ftz-we@t@uni-kiel@de  Mon Dec 10 08:37:13 2018
From: d@gm@r@cimiotti @ending from ftz-we@t@uni-kiel@de (Dagmar Cimiotti)
Date: Mon, 10 Dec 2018 08:37:13 +0100
Subject: [R] sample (randomly select) to get a number of successive days
Message-ID: <d0d47d4b-5423-89f0-e030-87ed7e9ea201@ftz-west.uni-kiel.de>

Hi Marc,

Yes, you got it to the point! That is exactly what I want. But I do not know how to do that. I know how to randomly pick the first day but I do not know how to set a range of values which cover the 25 days starting from that random value.

Best,
Dagmar


Hi,

I am confused.

As far as I can tell, only the first day is selected randomly from your dataset. The subsequent 24 days are deterministic, since they need to be consecutive days from the first day, for a total of 25 consecutive days.

Thus, all you need to do is to randomly select 1 day from within the time range of your dataset to be the first day, that is also far enough from the maximum date, to allow you to then select the data from the additional 24 consecutive days.

So randomly pick your first day and set a range of values, covering the 25 days, to use to then subset your full dataset.

What am I missing?

Regards,

Marc Schwartz


> On Dec 7, 2018, at 7:18 PM, Dagmar Cimiotti<dagmar.cimiotti at ftz-west.uni-kiel.de>  wrote:
>
> Hi Jim and everyone else,
>
> Mhm, no this is not what I am looking for. I think in your way I would
> randomly sample two values of day 1 and of day 2. But I want the
> opposite: I want to randomly draw two successive (!) days and put those
> values in a new dataframe to continue working with them.
>
> In my real data I do have a huge time span and I want to draw 25
> consecutive days. So maybe my example was a little misleading. And now
> that I read it again my text was, too. Sorry about that!
>
> Good try though and I am very gratefull for your good will to help me
> Would anyone give another try?
>
> Dagmar
>
> Am 07.12.2018 um 10:30 schrieb Jim Lemon:
>> Hi Dagmar,
>> This will probably involve creating a variable to differentiate the
>> two days in each data.frame:
>>
>> myframe$day<-as.Date(as.character(myframe$Timestamp),"%d.%m.%Y %H:%M:%S")
>> days<-unique(myframe$day)
>>
>> Then just sample the two subsets and concatenate them:
>>
>> myframe[c(sample(which(myframe$day==days[1]),2),
>>    sample(which(myframe$day==days[2]),2)),]
>>
>> Jim
>>
>>
>> On Fri, Dec 7, 2018 at 8:08 PM Dagmar Cimiotti
>> <dagmar.cimiotti at ftz-west.uni-kiel.de>  wrote:
>>> Dear all,
>>>
>>> I have data from a time span like this:
>>>
>>> myframe <- data.frame (Timestamp=c("24.09.2012 09:00:00", "24.09.2012
>>> 10:00:00","25.09.2012 09:00:00",
>>>                                       "25.09.2012
>>> 09:00:00","24.09.2012 09:00:00", "24.09.2012 10:00:00"),
>>>                            Event=c(50,60,30,40,42,54) )
>>> myframe
>>>
>>>
>>> I want to create a new dataframe which includes in this example the
>>> data from two successive days (in my real data I have a big time span
>>> and want data from 25 consecutive days). I understand that I can do a
>>> simple sample like this
>>>
>>> mysample <- myframe[sample(1:nrow(myframe), 4,replace=FALSE),]
>>> mysample
>>>
>>> But I need the data from consecutive days in my random sample. Can
>>> anyone help me with this?
>>>
>>>
>>> Many thanks in advance,
>>> Dagmar


	[[alternative HTML version deleted]]


From ruipb@rr@d@@ @ending from @@po@pt  Mon Dec 10 11:26:57 2018
From: ruipb@rr@d@@ @ending from @@po@pt (Rui Barradas)
Date: Mon, 10 Dec 2018 10:26:57 +0000
Subject: [R] repeating the same variable in formula
In-Reply-To: <04FA967CF3C1A14AB593785C317F043506D265B5@HERMES4.imim.es>
References: <04FA967CF3C1A14AB593785C317F043506D265B5@HERMES4.imim.es>
Message-ID: <d44a5f47-5dd5-e652-1c43-d6390438f60b@sapo.pt>

Hello,

The formulas

y ~ cholesterol + age + age

and

y ~ cholesterol + age

are the same formula.
If you want 'age' twice, maybe

g <- y ~ cholesterol + I(age + age)
attr(terms(g), "term.labels")
#[1] "cholesterol"  "I(age + age)"


Hope this helps,

Rui Barradas

?s 06:49 de 10/12/2018, Subirana Cachinero, Isaac escreveu:
> y ~ cholesterol + age + age


From i@ubir@n@ @ending from imim@e@  Mon Dec 10 11:35:03 2018
From: i@ubir@n@ @ending from imim@e@ (Subirana Cachinero, Isaac)
Date: Mon, 10 Dec 2018 10:35:03 +0000
Subject: [R] repeating the same variable in formula
In-Reply-To: <d44a5f47-5dd5-e652-1c43-d6390438f60b@sapo.pt>
References: <04FA967CF3C1A14AB593785C317F043506D265B5@HERMES4.imim.es>
 <d44a5f47-5dd5-e652-1c43-d6390438f60b@sapo.pt>
Message-ID: <04FA967CF3C1A14AB593785C317F043506D2680D@HERMES4.imim.es>

Thank you for your response.
In fact, I use the formula environament to select variables, as part of the code of another function.
I would like to allow the user to select the same variable more than once.
The use of I() may partly solve the problem. However, I would like attr(terms(g),"term.labels") would return a vector with as many components as variables including repetitions. And using I() function, there would be some work remaining to split "I(age + age)" into "age", "age", taking into account that in other examples variable may include white spaces or other characters within ` `.

Isaac.

-----Mensaje original-----
De: Rui Barradas [mailto:ruipbarradas at sapo.pt] 
Enviado el: lunes, 10 de diciembre de 2018 11:27
Para: Subirana Cachinero, Isaac; r-help at r-project.org
Asunto: Re: [R] repeating the same variable in formula

Hello,

The formulas

y ~ cholesterol + age + age

and

y ~ cholesterol + age

are the same formula.
If you want 'age' twice, maybe

g <- y ~ cholesterol + I(age + age)
attr(terms(g), "term.labels")
#[1] "cholesterol"  "I(age + age)"


Hope this helps,

Rui Barradas

?s 06:49 de 10/12/2018, Subirana Cachinero, Isaac escreveu:
> y ~ cholesterol + age + age


From m@rc_@chw@rtz @ending from me@com  Mon Dec 10 14:53:44 2018
From: m@rc_@chw@rtz @ending from me@com (Marc Schwartz)
Date: Mon, 10 Dec 2018 08:53:44 -0500
Subject: [R] sample (randomly select) to get a number of successive days
In-Reply-To: <d0d47d4b-5423-89f0-e030-87ed7e9ea201@ftz-west.uni-kiel.de>
References: <d0d47d4b-5423-89f0-e030-87ed7e9ea201@ftz-west.uni-kiel.de>
Message-ID: <68B5C020-F580-4FD2-853E-45D7859060EF@me.com>

Hi,

Given that your original data frame example is:

myframe <- data.frame (Timestamp=c("24.09.2012 09:00:00", "24.09.2012 10:00:00","25.09.2012 09:00:00",
                                   "25.09.2012 09:00:00","24.09.2012 09:00:00", "24.09.2012 10:00:00"),
                       Event=c(50,60,30,40,42,54))

> str(myframe)
'data.frame':	6 obs. of  2 variables:
 $ Timestamp: Factor w/ 3 levels "24.09.2012 09:00:00",..: 1 2 3 3 1 2
 $ Event    : num  50 60 30 40 42 54


Your Timestamp variable is a factor, not a datetime variable. So you first need to coerce it to one, in order to be able to define a range of dates.

Thus:

## See ?as.POSIXlt and the See Also links therein for more information on how R handles dates/times

myframe$Timestamp <- as.POSIXct(myframe$Timestamp, format = "%d.%m.%Y %H:%M:%S")

> str(myframe)
'data.frame':	6 obs. of  2 variables:
 $ Timestamp: POSIXct, format: "2012-09-24 09:00:00" ...
 $ Event    : num  50 60 30 40 42 54


So, to keep it simple, since you appear to be only concerned during the range selection process with the day and not the time, let's use the day part of the datetime as the basis for defining your interval. So, for clarity, let's create a new column in the data frame that is just the date:

myframe$day <- as.Date(myframe$Timestamp)

> str(myframe)
'data.frame':	6 obs. of  3 variables:
 $ Timestamp: POSIXct, format: "2012-09-24 09:00:00" ...
 $ Event    : num  50 60 30 40 42 54
 $ day      : Date, format: "2012-09-24" ...


> myframe
            Timestamp Event        day
1 2012-09-24 09:00:00    50 2012-09-24
2 2012-09-24 10:00:00    60 2012-09-24
3 2012-09-25 09:00:00    30 2012-09-25
4 2012-09-25 09:00:00    40 2012-09-25
5 2012-09-24 09:00:00    42 2012-09-24
6 2012-09-24 10:00:00    54 2012-09-24


With that in place, let's presume that you selected 2012-09-24 as your starting date. You can then use ?seq.Date to define the range:

set.seed(1)
start <- sample(myframe$day, 1)

> start
[1] "2012-09-24"

> str(start)
 Date[1:1], format: "2012-09-24"


So, create the range of 25 dates:

> seq(start, length.out = 25, by = "day")
 [1] "2012-09-24" "2012-09-25" "2012-09-26" "2012-09-27" "2012-09-28"
 [6] "2012-09-29" "2012-09-30" "2012-10-01" "2012-10-02" "2012-10-03"
[11] "2012-10-04" "2012-10-05" "2012-10-06" "2012-10-07" "2012-10-08"
[16] "2012-10-09" "2012-10-10" "2012-10-11" "2012-10-12" "2012-10-13"
[21] "2012-10-14" "2012-10-15" "2012-10-16" "2012-10-17" "2012-10-18"


Now, use the result of the above to subset your data frame. See ?subset and ?"%in%":

myframe.rand <- subset(myframe, day %in% seq(start, length.out = 25, by = "day"))


In your example, all rows will be returned, but from your larger dataset, you will only get the rows that have dates within the range defined.

Given the above, I will leave it to you to define the truncated date range from your full dataset, so that your initial starting date is sufficiently before your 'max' date, so that you can select 25 consecutive days.

Regards,

Marc Schwartz


> On Dec 10, 2018, at 2:37 AM, Dagmar Cimiotti <dagmar.cimiotti at ftz-west.uni-kiel.de> wrote:
> 
> Hi Marc,
> 
> Yes, you got it to the point! That is exactly what I want. But I do not know how to do that. I know how to randomly pick the first day but I do not know how to set a range of values which cover the 25 days starting from that random value.
> 
> Best,
> Dagmar
> 
> 
> Hi,
> 
> I am confused.
> 
> As far as I can tell, only the first day is selected randomly from your dataset. The subsequent 24 days are deterministic, since they need to be consecutive days from the first day, for a total of 25 consecutive days.
> 
> Thus, all you need to do is to randomly select 1 day from within the time range of your dataset to be the first day, that is also far enough from the maximum date, to allow you to then select the data from the additional 24 consecutive days.
> 
> So randomly pick your first day and set a range of values, covering the 25 days, to use to then subset your full dataset.
> 
> What am I missing?
> 
> Regards,
> 
> Marc Schwartz
> 
> 
>> On Dec 7, 2018, at 7:18 PM, Dagmar Cimiotti<dagmar.cimiotti at ftz-west.uni-kiel.de>  wrote:
>> 
>> Hi Jim and everyone else,
>> 
>> Mhm, no this is not what I am looking for. I think in your way I would
>> randomly sample two values of day 1 and of day 2. But I want the
>> opposite: I want to randomly draw two successive (!) days and put those
>> values in a new dataframe to continue working with them.
>> 
>> In my real data I do have a huge time span and I want to draw 25
>> consecutive days. So maybe my example was a little misleading. And now
>> that I read it again my text was, too. Sorry about that!
>> 
>> Good try though and I am very gratefull for your good will to help me
>> Would anyone give another try?
>> 
>> Dagmar
>> 
>> Am 07.12.2018 um 10:30 schrieb Jim Lemon:
>>> Hi Dagmar,
>>> This will probably involve creating a variable to differentiate the
>>> two days in each data.frame:
>>> 
>>> myframe$day<-as.Date(as.character(myframe$Timestamp),"%d.%m.%Y %H:%M:%S")
>>> days<-unique(myframe$day)
>>> 
>>> Then just sample the two subsets and concatenate them:
>>> 
>>> myframe[c(sample(which(myframe$day==days[1]),2),
>>>   sample(which(myframe$day==days[2]),2)),]
>>> 
>>> Jim
>>> 
>>> 
>>> On Fri, Dec 7, 2018 at 8:08 PM Dagmar Cimiotti
>>> <dagmar.cimiotti at ftz-west.uni-kiel.de>  wrote:
>>>> Dear all,
>>>> 
>>>> I have data from a time span like this:
>>>> 
>>>> myframe <- data.frame (Timestamp=c("24.09.2012 09:00:00", "24.09.2012
>>>> 10:00:00","25.09.2012 09:00:00",
>>>>                                      "25.09.2012
>>>> 09:00:00","24.09.2012 09:00:00", "24.09.2012 10:00:00"),
>>>>                           Event=c(50,60,30,40,42,54) )
>>>> myframe
>>>> 
>>>> 
>>>> I want to create a new dataframe which includes in this example the
>>>> data from two successive days (in my real data I have a big time span
>>>> and want data from 25 consecutive days). I understand that I can do a
>>>> simple sample like this
>>>> 
>>>> mysample <- myframe[sample(1:nrow(myframe), 4,replace=FALSE),]
>>>> mysample
>>>> 
>>>> But I need the data from consecutive days in my random sample. Can
>>>> anyone help me with this?
>>>> 
>>>> 
>>>> Many thanks in advance,
>>>> Dagmar


From pd@lgd @ending from gm@il@com  Mon Dec 10 15:09:57 2018
From: pd@lgd @ending from gm@il@com (peter dalgaard)
Date: Mon, 10 Dec 2018 15:09:57 +0100
Subject: [R] repeating the same variable in formula
In-Reply-To: <04FA967CF3C1A14AB593785C317F043506D2680D@HERMES4.imim.es>
References: <04FA967CF3C1A14AB593785C317F043506D265B5@HERMES4.imim.es>
 <d44a5f47-5dd5-e652-1c43-d6390438f60b@sapo.pt>
 <04FA967CF3C1A14AB593785C317F043506D2680D@HERMES4.imim.es>
Message-ID: <D2B6A8D8-C8F3-4DB4-B7A7-CE60A65DF43A@gmail.com>

You might be looking for this:

> all.vars(~chol+age+age, unique=FALSE)
[1] "chol" "age"  "age" 


-pd


> On 10 Dec 2018, at 11:35 , Subirana Cachinero, Isaac <isubirana at imim.es> wrote:
> 
> Thank you for your response.
> In fact, I use the formula environament to select variables, as part of the code of another function.
> I would like to allow the user to select the same variable more than once.
> The use of I() may partly solve the problem. However, I would like attr(terms(g),"term.labels") would return a vector with as many components as variables including repetitions. And using I() function, there would be some work remaining to split "I(age + age)" into "age", "age", taking into account that in other examples variable may include white spaces or other characters within ` `.
> 
> Isaac.
> 
> -----Mensaje original-----
> De: Rui Barradas [mailto:ruipbarradas at sapo.pt] 
> Enviado el: lunes, 10 de diciembre de 2018 11:27
> Para: Subirana Cachinero, Isaac; r-help at r-project.org
> Asunto: Re: [R] repeating the same variable in formula
> 
> Hello,
> 
> The formulas
> 
> y ~ cholesterol + age + age
> 
> and
> 
> y ~ cholesterol + age
> 
> are the same formula.
> If you want 'age' twice, maybe
> 
> g <- y ~ cholesterol + I(age + age)
> attr(terms(g), "term.labels")
> #[1] "cholesterol"  "I(age + age)"
> 
> 
> Hope this helps,
> 
> Rui Barradas
> 
> ?s 06:49 de 10/12/2018, Subirana Cachinero, Isaac escreveu:
>> y ~ cholesterol + age + age
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From i@ubir@n@ @ending from imim@e@  Mon Dec 10 16:13:28 2018
From: i@ubir@n@ @ending from imim@e@ (Subirana Cachinero, Isaac)
Date: Mon, 10 Dec 2018 15:13:28 +0000
Subject: [R] repeating the same variable in formula
In-Reply-To: <D2B6A8D8-C8F3-4DB4-B7A7-CE60A65DF43A@gmail.com>
References: <04FA967CF3C1A14AB593785C317F043506D265B5@HERMES4.imim.es>
 <d44a5f47-5dd5-e652-1c43-d6390438f60b@sapo.pt>
 <04FA967CF3C1A14AB593785C317F043506D2680D@HERMES4.imim.es>
 <D2B6A8D8-C8F3-4DB4-B7A7-CE60A65DF43A@gmail.com>
Message-ID: <04FA967CF3C1A14AB593785C317F043506D2686F@HERMES4.imim.es>

Thank you very much.
This is exactly what I needed.

Isaac.

-----Mensaje original-----
De: peter dalgaard [mailto:pdalgd at gmail.com] 
Enviado el: lunes, 10 de diciembre de 2018 15:10
Para: Subirana Cachinero, Isaac
CC: Rui Barradas; r-help at r-project.org
Asunto: Re: [R] repeating the same variable in formula

You might be looking for this:

> all.vars(~chol+age+age, unique=FALSE)
[1] "chol" "age"  "age" 


-pd


> On 10 Dec 2018, at 11:35 , Subirana Cachinero, Isaac <isubirana at imim.es> wrote:
> 
> Thank you for your response.
> In fact, I use the formula environament to select variables, as part of the code of another function.
> I would like to allow the user to select the same variable more than once.
> The use of I() may partly solve the problem. However, I would like attr(terms(g),"term.labels") would return a vector with as many components as variables including repetitions. And using I() function, there would be some work remaining to split "I(age + age)" into "age", "age", taking into account that in other examples variable may include white spaces or other characters within ` `.
> 
> Isaac.
> 
> -----Mensaje original-----
> De: Rui Barradas [mailto:ruipbarradas at sapo.pt] 
> Enviado el: lunes, 10 de diciembre de 2018 11:27
> Para: Subirana Cachinero, Isaac; r-help at r-project.org
> Asunto: Re: [R] repeating the same variable in formula
> 
> Hello,
> 
> The formulas
> 
> y ~ cholesterol + age + age
> 
> and
> 
> y ~ cholesterol + age
> 
> are the same formula.
> If you want 'age' twice, maybe
> 
> g <- y ~ cholesterol + I(age + age)
> attr(terms(g), "term.labels")
> #[1] "cholesterol"  "I(age + age)"
> 
> 
> Hope this helps,
> 
> Rui Barradas
> 
> ?s 06:49 de 10/12/2018, Subirana Cachinero, Isaac escreveu:
>> y ~ cholesterol + age + age
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com











From f@tm@@m@ci @ending from gm@il@com  Mon Dec 10 22:17:32 2018
From: f@tm@@m@ci @ending from gm@il@com (Fatma Ell)
Date: Mon, 10 Dec 2018 22:17:32 +0100
Subject: [R] ks.test ;
 impossible to calculate exact exact value with ex-aequos
Message-ID: <CAH3AuNAx=4yPNPK9LNZ+8V+gEiGZ8UxiNXnvv5BU4PYs_XpuHg@mail.gmail.com>

Dear all,

I'm trying to use ks.test in order to compare two curve. I've 0 values i
think this is why I have the follonwing warnings :impossible to calculate
exact exact value with  ex-aequos


a=c(3.02040816326531, 7.95918367346939, 10.6162790697674, 4.64150943396226,
1.86538461538462, 1.125, 1.01020408163265, 1.2093023255814,
0.292452830188679,
0, 0, 0)
b=c(2.30769230769231, 4.19252873563218, 5.81924882629108, 6.2248243559719,
5.02682926829268, 4.50728862973761, 3.61741424802111, 5.05479452054795,
3.68095238095238, 1.875, 5.25, 0)

ks.test(a,b)

data:  a and b
D = 0.58333, p-value = 0.0337
alternative hypothesis: two-sided

Warning message:
In ks.test(a, b) :
impossible to calculate exact exact value with ex-aequos

Does the p-value is correct ? Otherwise, how to solve this issue ?

Thanks a lot.

	[[alternative HTML version deleted]]


From ruipb@rr@d@@ @ending from @@po@pt  Mon Dec 10 23:34:19 2018
From: ruipb@rr@d@@ @ending from @@po@pt (Rui Barradas)
Date: Mon, 10 Dec 2018 22:34:19 +0000
Subject: [R] ks.test ;
 impossible to calculate exact exact value with ex-aequos
In-Reply-To: <CAH3AuNAx=4yPNPK9LNZ+8V+gEiGZ8UxiNXnvv5BU4PYs_XpuHg@mail.gmail.com>
References: <CAH3AuNAx=4yPNPK9LNZ+8V+gEiGZ8UxiNXnvv5BU4PYs_XpuHg@mail.gmail.com>
Message-ID: <37cf958c-59a1-af93-dc99-ba821997b56c@sapo.pt>

Hello,

That is a warning, not an error.
And it documented. In ?ks.test, section Details, the relevant part is


The presence of ties always generates a warning, since continuous 
distributions do not generate them. If the ties arose from rounding the 
tests may be approximately valid, but even modest amounts of rounding 
can have a significant effect on the calculated statistic.


It then adds, three paragraphs down:

Exact p-values are not available for the two-sample case if one-sided or 
in the presence of ties.


You can use exact = FALSE but it will still issue a warning.

ks.test(a, b, exact = FALSE)

	Two-sample Kolmogorov-Smirnov test

data:  a and b
D = 0.58333, p-value = 0.0337
alternative hypothesis: two-sided

Warning message:
In ks.test(a, b, exact = FALSE) :
   p-value will be approximate in the presence of ties



Hope this helps,

Rui Barradas
?s 21:17 de 10/12/2018, Fatma Ell escreveu:
> Dear all,
> 
> I'm trying to use ks.test in order to compare two curve. I've 0 values i
> think this is why I have the follonwing warnings :impossible to calculate
> exact exact value with  ex-aequos
> 
> 
> a=c(3.02040816326531, 7.95918367346939, 10.6162790697674, 4.64150943396226,
> 1.86538461538462, 1.125, 1.01020408163265, 1.2093023255814,
> 0.292452830188679,
> 0, 0, 0)
> b=c(2.30769230769231, 4.19252873563218, 5.81924882629108, 6.2248243559719,
> 5.02682926829268, 4.50728862973761, 3.61741424802111, 5.05479452054795,
> 3.68095238095238, 1.875, 5.25, 0)
> 
> ks.test(a,b)
> 
> data:  a and b
> D = 0.58333, p-value = 0.0337
> alternative hypothesis: two-sided
> 
> Warning message:
> In ks.test(a, b) :
> impossible to calculate exact exact value with ex-aequos
> 
> Does the p-value is correct ? Otherwise, how to solve this issue ?
> 
> Thanks a lot.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ted@h@rding @ending from wl@ndre@@net  Mon Dec 10 23:44:57 2018
From: ted@h@rding @ending from wl@ndre@@net (Ted Harding)
Date: Mon, 10 Dec 2018 22:44:57 +0000
Subject: [R] ks.test ;
	impossible to calculate exact exact value with	ex-aequos
In-Reply-To: <CAH3AuNAx=4yPNPK9LNZ+8V+gEiGZ8UxiNXnvv5BU4PYs_XpuHg@mail.gmail.com>
References: <CAH3AuNAx=4yPNPK9LNZ+8V+gEiGZ8UxiNXnvv5BU4PYs_XpuHg@mail.gmail.com>
Message-ID: <1544481897.3868.41.camel@deb2.fort.knox.uk>

On Mon, 2018-12-10 at 22:17 +0100, Fatma Ell wrote:
> Dear all,
> I'm trying to use ks.test in order to compare two curve. I've 0 values i
> think this is why I have the follonwing warnings :impossible to calculate
> exact exact value with  ex-aequos
> 
> a=c(3.02040816326531, 7.95918367346939, 10.6162790697674, 4.64150943396226,
> 1.86538461538462, 1.125, 1.01020408163265, 1.2093023255814,
> 0.292452830188679,
> 0, 0, 0)
> b=c(2.30769230769231, 4.19252873563218, 5.81924882629108, 6.2248243559719,
> 5.02682926829268, 4.50728862973761, 3.61741424802111, 5.05479452054795,
> 3.68095238095238, 1.875, 5.25, 0)
> 
> ks.test(a,b)
> 
> data:  a and b
> D = 0.58333, p-value = 0.0337
> alternative hypothesis: two-sided
> 
> Warning message:
> In ks.test(a, b) :
> impossible to calculate exact exact value with ex-aequos
> 
> Does the p-value is correct ? Otherwise, how to solve this issue ?
> Thanks a lot.

The warning arises, not because you have "0" values as such,
but because there are repeated values (which happen to be 0).

The K-S test is designed for continuous random variables, for
which the probability of repeated values is (theoretically) zero:
theoretically, they can't happen.

>From the help page ?ks.test :

"The presence of ties always generates a warning, since continuous
distributions do not generate them. If the ties arose from
rounding the tests may be approximately valid, but even modest
amounts of rounding can have a significant effect on the
calculated statistic."



in view of the fact that your sample 'a' has three zeros along with
nine other vauwes which are all different from 0 (and all *very*
different from 0 except for 0.292452830188679), along with the fact
that your sample 'b' has 11 values all *very* different from 0.
and pne finall value equal to 0; together also with the fact that
in each sample the '0' values occur at the end, stringly suggests
that the data source is not such that a K-D test is auitasble.

The K-S test is a non-parametric test for whether
  a) a given sample comes from na given kind of distribiution;
or
  v) two samples are drwn from the same distribition.
In either case, it is assumed that the sample values are drawn
independently of each other; if there is some reason why they
may not be independent of each other, the test os not valid.

You say "I'm trying to use ks.test in order to compare two curve".
When I ezecute
  plot(a)
  plot(b)
on your data, I see (approximately) in each case a rise from a
medium vale (~2 or ~3) to a higher vale {~6 or ~10) followed
by a decline down to an exact 0.

This is not the sort of situation that the K-S test is for!
Hoping this helps,
Ted.


From f@tm@@m@ci @ending from gm@il@com  Tue Dec 11 00:36:38 2018
From: f@tm@@m@ci @ending from gm@il@com (Fatma Ell)
Date: Tue, 11 Dec 2018 00:36:38 +0100
Subject: [R] ks.test ;
 impossible to calculate exact exact value with ex-aequos
In-Reply-To: <1544481897.3868.41.camel@deb2.fort.knox.uk>
References: <CAH3AuNAx=4yPNPK9LNZ+8V+gEiGZ8UxiNXnvv5BU4PYs_XpuHg@mail.gmail.com>
 <1544481897.3868.41.camel@deb2.fort.knox.uk>
Message-ID: <CAH3AuNDzWbvG6843MTHESR7-rqCXtNZ8K8ot8GLFJqsJ2AJinA@mail.gmail.com>

Thanks a lot for this reply

'a' is a simulated data while 'b' is empirical data.
Other than correlation, how to check ressemblence between these two curve
in terms of :
Amplitude in each row 1...12
Evolution and variability from 1 to 12

Thanks !


Le lundi 10 d?cembre 2018, Ted Harding <ted.harding at wlandres.net> a ?crit :

> On Mon, 2018-12-10 at 22:17 +0100, Fatma Ell wrote:
> > Dear all,
> > I'm trying to use ks.test in order to compare two curve. I've 0 values i
> > think this is why I have the follonwing warnings :impossible to calculate
> > exact exact value with  ex-aequos
> >
> > a=c(3.02040816326531, 7.95918367346939, 10.6162790697674,
> 4.64150943396226,
> > 1.86538461538462, 1.125, 1.01020408163265, 1.2093023255814,
> > 0.292452830188679,
> > 0, 0, 0)
> > b=c(2.30769230769231, 4.19252873563218, 5.81924882629108,
> 6.2248243559719,
> > 5.02682926829268, 4.50728862973761, 3.61741424802111, 5.05479452054795,
> > 3.68095238095238, 1.875, 5.25, 0)
> >
> > ks.test(a,b)
> >
> > data:  a and b
> > D = 0.58333, p-value = 0.0337
> > alternative hypothesis: two-sided
> >
> > Warning message:
> > In ks.test(a, b) :
> > impossible to calculate exact exact value with ex-aequos
> >
> > Does the p-value is correct ? Otherwise, how to solve this issue ?
> > Thanks a lot.
>
> The warning arises, not because you have "0" values as such,
> but because there are repeated values (which happen to be 0).
>
> The K-S test is designed for continuous random variables, for
> which the probability of repeated values is (theoretically) zero:
> theoretically, they can't happen.
>
> >From the help page ?ks.test :
>
> "The presence of ties always generates a warning, since continuous
> distributions do not generate them. If the ties arose from
> rounding the tests may be approximately valid, but even modest
> amounts of rounding can have a significant effect on the
> calculated statistic."
>
>
>
> in view of the fact that your sample 'a' has three zeros along with
> nine other vauwes which are all different from 0 (and all *very*
> different from 0 except for 0.292452830188679), along with the fact
> that your sample 'b' has 11 values all *very* different from 0.
> and pne finall value equal to 0; together also with the fact that
> in each sample the '0' values occur at the end, stringly suggests
> that the data source is not such that a K-D test is auitasble.
>
> The K-S test is a non-parametric test for whether
>   a) a given sample comes from na given kind of distribiution;
> or
>   v) two samples are drwn from the same distribition.
> In either case, it is assumed that the sample values are drawn
> independently of each other; if there is some reason why they
> may not be independent of each other, the test os not valid.
>
> You say "I'm trying to use ks.test in order to compare two curve".
> When I ezecute
>   plot(a)
>   plot(b)
> on your data, I see (approximately) in each case a rise from a
> medium vale (~2 or ~3) to a higher vale {~6 or ~10) followed
> by a decline down to an exact 0.
>
> This is not the sort of situation that the K-S test is for!
> Hoping this helps,
> Ted.
>
>

	[[alternative HTML version deleted]]


From bgunter@4567 @ending from gm@il@com  Tue Dec 11 01:02:15 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Mon, 10 Dec 2018 16:02:15 -0800
Subject: [R] ks.test ;
 impossible to calculate exact exact value with ex-aequos
In-Reply-To: <CAH3AuNDzWbvG6843MTHESR7-rqCXtNZ8K8ot8GLFJqsJ2AJinA@mail.gmail.com>
References: <CAH3AuNAx=4yPNPK9LNZ+8V+gEiGZ8UxiNXnvv5BU4PYs_XpuHg@mail.gmail.com>
 <1544481897.3868.41.camel@deb2.fort.knox.uk>
 <CAH3AuNDzWbvG6843MTHESR7-rqCXtNZ8K8ot8GLFJqsJ2AJinA@mail.gmail.com>
Message-ID: <CAGxFJbRxnbU7rWyNA19ZHX4cohsNGom45=+Q-4ec2N4PxWTNwg@mail.gmail.com>

"Other than correlation, how to check ressemblence between these two curve"

(As Ted Indicated) Graph them... and look!

There is nothing magical about statistics, which seems to be what you seek.


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Dec 10, 2018 at 3:36 PM Fatma Ell <fatma.msci at gmail.com> wrote:

> Thanks a lot for this reply
>
> 'a' is a simulated data while 'b' is empirical data.
> Other than correlation, how to check ressemblence between these two curve
> in terms of :
> Amplitude in each row 1...12
> Evolution and variability from 1 to 12
>
> Thanks !
>
>
> Le lundi 10 d?cembre 2018, Ted Harding <ted.harding at wlandres.net> a ?crit
> :
>
> > On Mon, 2018-12-10 at 22:17 +0100, Fatma Ell wrote:
> > > Dear all,
> > > I'm trying to use ks.test in order to compare two curve. I've 0 values
> i
> > > think this is why I have the follonwing warnings :impossible to
> calculate
> > > exact exact value with  ex-aequos
> > >
> > > a=c(3.02040816326531, 7.95918367346939, 10.6162790697674,
> > 4.64150943396226,
> > > 1.86538461538462, 1.125, 1.01020408163265, 1.2093023255814,
> > > 0.292452830188679,
> > > 0, 0, 0)
> > > b=c(2.30769230769231, 4.19252873563218, 5.81924882629108,
> > 6.2248243559719,
> > > 5.02682926829268, 4.50728862973761, 3.61741424802111, 5.05479452054795,
> > > 3.68095238095238, 1.875, 5.25, 0)
> > >
> > > ks.test(a,b)
> > >
> > > data:  a and b
> > > D = 0.58333, p-value = 0.0337
> > > alternative hypothesis: two-sided
> > >
> > > Warning message:
> > > In ks.test(a, b) :
> > > impossible to calculate exact exact value with ex-aequos
> > >
> > > Does the p-value is correct ? Otherwise, how to solve this issue ?
> > > Thanks a lot.
> >
> > The warning arises, not because you have "0" values as such,
> > but because there are repeated values (which happen to be 0).
> >
> > The K-S test is designed for continuous random variables, for
> > which the probability of repeated values is (theoretically) zero:
> > theoretically, they can't happen.
> >
> > >From the help page ?ks.test :
> >
> > "The presence of ties always generates a warning, since continuous
> > distributions do not generate them. If the ties arose from
> > rounding the tests may be approximately valid, but even modest
> > amounts of rounding can have a significant effect on the
> > calculated statistic."
> >
> >
> >
> > in view of the fact that your sample 'a' has three zeros along with
> > nine other vauwes which are all different from 0 (and all *very*
> > different from 0 except for 0.292452830188679), along with the fact
> > that your sample 'b' has 11 values all *very* different from 0.
> > and pne finall value equal to 0; together also with the fact that
> > in each sample the '0' values occur at the end, stringly suggests
> > that the data source is not such that a K-D test is auitasble.
> >
> > The K-S test is a non-parametric test for whether
> >   a) a given sample comes from na given kind of distribiution;
> > or
> >   v) two samples are drwn from the same distribition.
> > In either case, it is assumed that the sample values are drawn
> > independently of each other; if there is some reason why they
> > may not be independent of each other, the test os not valid.
> >
> > You say "I'm trying to use ks.test in order to compare two curve".
> > When I ezecute
> >   plot(a)
> >   plot(b)
> > on your data, I see (approximately) in each case a rise from a
> > medium vale (~2 or ~3) to a higher vale {~6 or ~10) followed
> > by a decline down to an exact 0.
> >
> > This is not the sort of situation that the K-S test is for!
> > Hoping this helps,
> > Ted.
> >
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From d@gm@r@cimiotti @ending from ftz-we@t@uni-kiel@de  Tue Dec 11 08:46:26 2018
From: d@gm@r@cimiotti @ending from ftz-we@t@uni-kiel@de (Dagmar Cimiotti)
Date: Tue, 11 Dec 2018 08:46:26 +0100
Subject: [R] sample (randomly select) to get a number of successive days
Message-ID: <d13d42b0-51f6-88f2-0ab0-e21f59681374@ftz-west.uni-kiel.de>

Thank you so much Marc,
that is exactly what I need. That will save me weeks of work and additionally I learned a lot.
:-)
Have a great day!
Dagmar



Hi,

Given that your original data frame example is:

myframe <- data.frame (Timestamp=c("24.09.2012 09:00:00", "24.09.2012 10:00:00","25.09.2012 09:00:00",
                                    "25.09.2012 09:00:00","24.09.2012 09:00:00", "24.09.2012 10:00:00"),
                        Event=c(50,60,30,40,42,54))

> str(myframe)

'data.frame':	6 obs. of  2 variables:
  $ Timestamp: Factor w/ 3 levels "24.09.2012 09:00:00",..: 1 2 3 3 1 2
  $ Event    : num  50 60 30 40 42 54


Your Timestamp variable is a factor, not a datetime variable. So you first need to coerce it to one, in order to be able to define a range of dates.

Thus:

## See ?as.POSIXlt and the See Also links therein for more information on how R handles dates/times

myframe$Timestamp <- as.POSIXct(myframe$Timestamp, format = "%d.%m.%Y %H:%M:%S")

> str(myframe)

'data.frame':	6 obs. of  2 variables:
  $ Timestamp: POSIXct, format: "2012-09-24 09:00:00" ...
  $ Event    : num  50 60 30 40 42 54


So, to keep it simple, since you appear to be only concerned during the range selection process with the day and not the time, let's use the day part of the datetime as the basis for defining your interval. So, for clarity, let's create a new column in the data frame that is just the date:

myframe$day <- as.Date(myframe$Timestamp)

> str(myframe)

'data.frame':	6 obs. of  3 variables:
  $ Timestamp: POSIXct, format: "2012-09-24 09:00:00" ...
  $ Event    : num  50 60 30 40 42 54
  $ day      : Date, format: "2012-09-24" ...


> myframe

             Timestamp Event        day
1 2012-09-24 09:00:00    50 2012-09-24
2 2012-09-24 10:00:00    60 2012-09-24
3 2012-09-25 09:00:00    30 2012-09-25
4 2012-09-25 09:00:00    40 2012-09-25
5 2012-09-24 09:00:00    42 2012-09-24
6 2012-09-24 10:00:00    54 2012-09-24


With that in place, let's presume that you selected 2012-09-24 as your starting date. You can then use ?seq.Date to define the range:

set.seed(1)
start <- sample(myframe$day, 1)

> start

[1] "2012-09-24"

> str(start)

  Date[1:1], format: "2012-09-24"


So, create the range of 25 dates:

> seq(start, length.out = 25, by = "day")

  [1] "2012-09-24" "2012-09-25" "2012-09-26" "2012-09-27" "2012-09-28"
  [6] "2012-09-29" "2012-09-30" "2012-10-01" "2012-10-02" "2012-10-03"
[11] "2012-10-04" "2012-10-05" "2012-10-06" "2012-10-07" "2012-10-08"
[16] "2012-10-09" "2012-10-10" "2012-10-11" "2012-10-12" "2012-10-13"
[21] "2012-10-14" "2012-10-15" "2012-10-16" "2012-10-17" "2012-10-18"


Now, use the result of the above to subset your data frame. See ?subset and ?"%in%":

myframe.rand <- subset(myframe, day %in% seq(start, length.out = 25, by = "day"))


In your example, all rows will be returned, but from your larger dataset, you will only get the rows that have dates within the range defined.

Given the above, I will leave it to you to define the truncated date range from your full dataset, so that your initial starting date is sufficiently before your 'max' date, so that you can select 25 consecutive days.

Regards,

Marc Schwartz


> On Dec 10, 2018, at 2:37 AM, Dagmar Cimiotti<dagmar.cimiotti at ftz-west.uni-kiel.de>  wrote:
>
> Hi Marc,
>
> Yes, you got it to the point! That is exactly what I want. But I do not know how to do that. I know how to randomly pick the first day but I do not know how to set a range of values which cover the 25 days starting from that random value.
>
> Best,
> Dagmar
>
>
> Hi,
>
> I am confused.
>
> As far as I can tell, only the first day is selected randomly from your dataset. The subsequent 24 days are deterministic, since they need to be consecutive days from the first day, for a total of 25 consecutive days.
>
> Thus, all you need to do is to randomly select 1 day from within the time range of your dataset to be the first day, that is also far enough from the maximum date, to allow you to then select the data from the additional 24 consecutive days.
>
> So randomly pick your first day and set a range of values, covering the 25 days, to use to then subset your full dataset.
>
> What am I missing?
>
> Regards,
>
> Marc Schwartz
>
>
>> On Dec 7, 2018, at 7:18 PM, Dagmar Cimiotti<dagmar.cimiotti at ftz-west.uni-kiel.de>   wrote:
>>
>> Hi Jim and everyone else,
>>
>> Mhm, no this is not what I am looking for. I think in your way I would
>> randomly sample two values of day 1 and of day 2. But I want the
>> opposite: I want to randomly draw two successive (!) days and put those
>> values in a new dataframe to continue working with them.
>>
>> In my real data I do have a huge time span and I want to draw 25
>> consecutive days. So maybe my example was a little misleading. And now
>> that I read it again my text was, too. Sorry about that!
>>
>> Good try though and I am very gratefull for your good will to help me
>> Would anyone give another try?
>>
>> Dagmar
>>
>> Am 07.12.2018 um 10:30 schrieb Jim Lemon:
>>> Hi Dagmar,
>>> This will probably involve creating a variable to differentiate the
>>> two days in each data.frame:
>>>
>>> myframe$day<-as.Date(as.character(myframe$Timestamp),"%d.%m.%Y %H:%M:%S")
>>> days<-unique(myframe$day)
>>>
>>> Then just sample the two subsets and concatenate them:
>>>
>>> myframe[c(sample(which(myframe$day==days[1]),2),
>>>    sample(which(myframe$day==days[2]),2)),]
>>>
>>> Jim
>>>
>>>
>>> On Fri, Dec 7, 2018 at 8:08 PM Dagmar Cimiotti
>>> <dagmar.cimiotti at ftz-west.uni-kiel.de>   wrote:
>>>> Dear all,
>>>>
>>>> I have data from a time span like this:
>>>>
>>>> myframe <- data.frame (Timestamp=c("24.09.2012 09:00:00", "24.09.2012
>>>> 10:00:00","25.09.2012 09:00:00",
>>>>                                       "25.09.2012
>>>> 09:00:00","24.09.2012 09:00:00", "24.09.2012 10:00:00"),
>>>>                            Event=c(50,60,30,40,42,54) )
>>>> myframe
>>>>
>>>>
>>>> I want to create a new dataframe which includes in this example the
>>>> data from two successive days (in my real data I have a big time span
>>>> and want data from 25 consecutive days). I understand that I can do a
>>>> simple sample like this
>>>>
>>>> mysample <- myframe[sample(1:nrow(myframe), 4,replace=FALSE),]
>>>> mysample
>>>>
>>>> But I need the data from consecutive days in my random sample. Can
>>>> anyone help me with this?
>>>>
>>>>
>>>> Many thanks in advance,
>>>> Dagmar


	[[alternative HTML version deleted]]


From r@cow@n @ending from m@@@trichtuniver@ity@nl  Tue Dec 11 09:22:38 2018
From: r@cow@n @ending from m@@@trichtuniver@ity@nl (cowan robin)
Date: Tue, 11 Dec 2018 09:22:38 +0100
Subject: [R] Different performance with different R versions
Message-ID: <78104315-645E-4192-86D6-FA4F6040BECE@maastrichtuniversity.nl>

I am running a small simulation, and getting very different run times when I use different versions of R. 
Two set-ups using the same machine (MacBook Pro 2013 vintage)

1. R version 3.1.3  running on system OS X 10.9.5

> system.time(source("simulationR-R.R"))

  user  system elapsed 
  3.890   0.061   3.965 



Compared to


2. R version 3.5.1  running on system OS X 10.12.6

> system.time(source("simulationR-R.R"))

  user  system elapsed 
277.924   2.087 280.841 


The source code is identical. This is a pretty big difference running the same code on the same hardware.
Before submitting the code, is this a known issue?


From drjimlemon @ending from gm@il@com  Tue Dec 11 11:14:39 2018
From: drjimlemon @ending from gm@il@com (Jim Lemon)
Date: Tue, 11 Dec 2018 21:14:39 +1100
Subject: [R] Plotting Very Large lat-lon data in x-y axes graph
In-Reply-To: <CAC8ss33wF7Tp-Uh86S4XMUVxbA56-1atk4kEj67Tg+Ps-3LN-A@mail.gmail.com>
References: <CAC8ss33ZS-8NqHf+6Q3Rk=qt+Z7+O7sVFV9Ahi+X8U=DqxhOsw@mail.gmail.com>
 <CA+8X3fU04pA_nh2DvXmY9vUWGr7bsj8AwRBp9Xd=qFvGmj6RMQ@mail.gmail.com>
 <CAC8ss32fAjoh1Q2+7x9i25EyXCGAbeqjYOtG4r3WuWi2S6st4A@mail.gmail.com>
 <CAC8ss33wF7Tp-Uh86S4XMUVxbA56-1atk4kEj67Tg+Ps-3LN-A@mail.gmail.com>
Message-ID: <CA+8X3fUuz0y70LNc2H8TuR0y65h728vR2GA55o+iE1DRGK+o5Q@mail.gmail.com>

Hi Ogbos,
I have been off the air for a couple of days. Look at the color.legend
function in the plotrix package.

Jim
On Tue, Dec 11, 2018 at 12:39 PM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>
> Dear Jim,
> I am still having trouble with the colour code. I await your help when
> you are less busy.
>
> Thank you.
> Best
> Ogbos
> On Mon, Dec 10, 2018 at 6:25 AM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
> >
> > Dear Jim,
> >
> > I used a bit of my data in my attempt to follow the code.
> >
> > I got the attached plot after some tweaks.
> >
> > I would like to add horizontal color bar legend that could be used to
> > explain the number of lightning counts at different points on the
> > latitude band as plotted.
> >
> > Thank you
> > Warm regards
> > Ogbos
> >
> > On Sun, Dec 9, 2018 at 9:46 PM Jim Lemon <drjimlemon at gmail.com> wrote:
> > >
> > > Hi Ogbos,
> > > Here is a slight modification of a method I use to display trip
> > > density on a map:
> > >
> > > oolt<-read.table(text="Lat Lon
> > > 30.1426 104.7854
> > > 30.5622 105.0837
> > > 30.0966 104.6213
> > > 29.9795 104.8430
> > > 39.2802 147.7295
> > > 30.2469 104.6543
> > > 26.4428 157.7293
> > > 29.4782 104.5590
> > > 32.3839 105.3293
> > > 26.4746 157.8411
> > > 25.1014 159.6959
> > > 25.1242 159.6558
> > > 30.1607 104.9100
> > > 31.4900 -71.8919
> > > 43.3655 -74.9994
> > > 30.0811 104.8462
> > > 29.0912 -85.5138
> > > 26.6204 -80.9342
> > > 31.5462 -71.9638
> > > 26.8619 97.3844
> > > 30.2534 104.6134
> > > 29.9311 -85.3434
> > > 26.1524 159.6806
> > > 26.5112 158.0233
> > > 26.5441 158.0565
> > > 27.8901 -105.8554
> > > 30.3175 104.7135
> > > 26.4822 157.6127
> > > 30.1887 104.5986
> > > 29.5058 104.5661
> > > 26.4010 157.5749
> > > 30.2281 104.7585
> > > 31.4556 110.5619
> > > 30.1700 104.5861
> > > 26.3911 157.4776
> > > 30.6493 104.9949
> > > 30.2209 104.6629
> > > 26.0488 97.3608
> > > 30.2142 104.8023
> > > 30.1806 104.8158
> > > 25.2107 160.1690
> > > 30.6708 104.9385
> > > 30.4152 104.7002
> > > 30.2446 104.7804
> > > 29.5760 -85.1535
> > > 26.4484 92.4312
> > > 26.3914 157.4189
> > > 26.3986 157.4421
> > > 30.4903 -88.2271
> > > 30.6727 104.8768
> > > 30.2518 104.6466
> > > 41.6979 -78.4136
> > > 33.7575 72.1089
> > > 26.8333 -80.9485
> > > 25.3103 124.0978
> > > 30.1742 104.7554
> > > 30.6345 104.9739
> > > 30.2075 104.7960
> > > 30.2226 104.7517
> > > 30.5948 105.0532",
> > > header=TRUE)
> > > latlim<-c(20,45)
> > > lonlim<-c(-90,160)
> > > latbreaks<-seq(latlim[1],latlim[2],by=5)
> > > lonbreaks<-seq(lonlim[1],lonlim[2],by=10)
> > >
> > > mids<-function(x) {
> > >  lenx<-length(x)
> > >  return((x[1:(lenx-1)]+x[2:lenx])/2)
> > > }
> > > lonmids<-mids(lonbreaks)
> > > latmids<-mids(latbreaks)
> > > oolt$loncuts<-cut(oolt$Lon,lonbreaks)
> > > oolt$latcuts<-cut(oolt$Lat,latbreaks)
> > > counts<-table(oolt$latcuts,oolt$loncuts)
> > > library(plotrix)
> > > countcol<-color.scale(counts,extremes=c("blue","red"))
> > > map("world",xlim=c(-90,160),ylim=c(20,45))
> > > for(lon in 1:length(lonmids)) {
> > >  for(lat in 1:length(latmids)) {
> > >   if(counts[lat,lon] > 0)
> > >    draw.circle(lonmids[lon],latmids[lat],radius=sqrt(counts[lat,lon]),
> > >     border=countcol[lat,lon],col=countcol[lat,lon])
> > >  }
> > > }
> > >
> > > If you have very large counts in some places you may need to adjust
> > > the radius of the circles.
> > >
> > > Jim
> > > On Mon, Dec 10, 2018 at 2:50 AM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
> > > >
> > > > Dear Contributors,
> > > >
> > > > I have a data of the form:
> > > > Lat          Lon
> > > > 30.1426 104.7854
> > > > 30.5622 105.0837
> > > > 30.0966 104.6213
> > > > 29.9795 104.8430
> > > > 39.2802 147.7295
> > > > 30.2469 104.6543
> > > > 26.4428 157.7293
> > > > 29.4782 104.5590
> > > > 32.3839 105.3293
> > > > 26.4746 157.8411
> > > > 25.1014 159.6959
> > > > 25.1242 159.6558
> > > > 30.1607 104.9100
> > > > 31.4900 -71.8919
> > > > 43.3655 -74.9994
> > > > 30.0811 104.8462
> > > > 29.0912 -85.5138
> > > > 26.6204 -80.9342
> > > > 31.5462 -71.9638
> > > > 26.8619 97.3844
> > > > 30.2534 104.6134
> > > > 29.9311 -85.3434
> > > > 26.1524 159.6806
> > > > 26.5112 158.0233
> > > > 26.5441 158.0565
> > > > 27.8901 -105.8554
> > > > 30.3175 104.7135
> > > > 26.4822 157.6127
> > > > 30.1887 104.5986
> > > > 29.5058 104.5661
> > > > 26.4010 157.5749
> > > > 30.2281 104.7585
> > > > 31.4556 110.5619
> > > > 30.1700 104.5861
> > > > 26.3911 157.4776
> > > > 30.6493 104.9949
> > > > 30.2209 104.6629
> > > > 26.0488 97.3608
> > > > 30.2142 104.8023
> > > > 30.1806 104.8158
> > > > 25.2107 160.1690
> > > > 30.6708 104.9385
> > > > 30.4152 104.7002
> > > > 30.2446 104.7804
> > > > 29.5760 -85.1535
> > > > 26.4484 92.4312
> > > > 26.3914 157.4189
> > > > 26.3986 157.4421
> > > > 30.4903 -88.2271
> > > > 30.6727 104.8768
> > > > 30.2518 104.6466
> > > > 41.6979 -78.4136
> > > > 33.7575 72.1089
> > > > 26.8333 -80.9485
> > > > 25.3103 124.0978
> > > > 30.1742 104.7554
> > > > 30.6345 104.9739
> > > > 30.2075 104.7960
> > > > 30.2226 104.7517
> > > > 30.5948 105.0532.
> > > > The record is for lightning flashes in the continental U.S. and
> > > > surrounding waters within the latitudinal band between
> > > > 258 and 458N.
> > > >
> > > > I want to display the result in x-y co-ordinate plot. However, the
> > > > data is very large such that when plotted, everything just appeared
> > > > blurred.
> > > >
> > > >
> > > > Is there a way of using color codes to  indicate the regions of higher
> > > > or lower flash densities?
> > > >
> > > > I can attach the plot I generated but I am not sure if the moderator
> > > > will allow it to go with this.
> > > >
> > > > I will send it in a separate email if required.
> > > >
> > > > Thank you so much for sparing your time.
> > > >
> > > > Best
> > > > Ogbos
> > > >
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.


From f@tm@@m@ci @ending from gm@il@com  Tue Dec 11 11:02:48 2018
From: f@tm@@m@ci @ending from gm@il@com (Fatma Ell)
Date: Tue, 11 Dec 2018 11:02:48 +0100
Subject: [R] 
 Does the correlations of component makes the correlation of one
 phenomena ?
In-Reply-To: <6ee48ca07dce470a830253a82706d94f@tamu.edu>
References: <CAH3AuNDu90UykuWQ-asAnjjjVj8zA6toqbHDABSa4Dt_6y-yjQ@mail.gmail.com>
 <6ee48ca07dce470a830253a82706d94f@tamu.edu>
Message-ID: <CAH3AuNAU92GrBTjkFCMhjpytk49EUwC-AK-0SZ9dP+Mok2STng@mail.gmail.com>

Thanks a lot David for this extended answer

The aim is to say: if simulated vs emprical correlate one by one, the sum
of both should correlate also

I want to be sure that I understood correctly:
What you have done
1) building the model ( the fittingness) according empirical vs simulated
value and predict value from this model
2) compare predicted value of the fittingness model with the sum of
empirical value, isnt ?

Thanks a lot


Le lundi 3 d?cembre 2018, David L Carlson <dcarlson at tamu.edu> a ?crit :

> This is really a statistics question rather than an R question, but you
> did provide reproducible data. You have some moderate correlations for some
> of the tests, but they are all different relationships. You used a
> combination of base R and dplyr code, but I'll just stick with base R:
>
> > Mesures.split <- split(Mesures, Mesures$test)
> > Corrs <- sapply(Mesures.split, function(x) cor(x[, 3], x[, 4]))
> > options(digits=3)
> > Corrs
>      1      2      3      4      5      6      7      8      9     10
>  0.551  0.437  0.905 -0.106  0.841  0.556  0.809  0.772  0.709  0.512
>
> > sapply(Mesures.split, function(x) coef(lm(x[, 3]~x[, 4])))
>                  1      2       3        4      5      6      7
> (Intercept) 0.6875 0.6530 -0.2597  2.24313 0.3498 1.4436 0.4103
> x[, 4]      0.0309 0.0034  0.0353 -0.00668 0.0171 0.0168 0.0137
>                   8      9      10
> (Intercept) -0.7379 0.2929 0.48115
> x[, 4]       0.0255 0.0129 0.00891
>
> This gives you the intercept and slope for the regression lines for each
> test. Notice that they vary considerably. The slope value for predicting
> behavior from simulated varies from -0.007 to .031. When you average over
> space you effectively eliminate the correlations at the test level:
>
> > Mesures_aggregated <- aggregate(Mesures[, 3:4], by=list(Mesures$Space),
> sum)
> > cor(Mesures_aggregated[, 2:3])[1, 2]
> [1] 0.0771
>
> If you sum predicted values for empirical behavior using the 10 regression
> equations and compare that to the summed empirical value, things work out
> better.
>
> > pred <- rowSums(sapply(Mesures.split, function(x) predict(lm(x[, 3]~x[,
> 4]))))
> > cor(Mesures_aggregated[, 2], pred)
> [1] 0.776
>
> Without knowing where the simulated values come from, especially if they
> are completely independent of the empirical values, I can't say if this
> approach is wise.
>
> ---------------------------------------
> David L. Carlson
> Department of Anthropology
> Texas A&M University
>
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Fatma Ell
> Sent: Sunday, December 2, 2018 4:50 AM
> To: r-help at r-project.org
> Subject: [R] Does the correlations of component makes the correlation of
> one phenomena ?
>
> Hi,
>
> I have the following dataset Mesures. It contains test which is a given
> context, Space is portion of this following context test. For each test we
> have twelve Space and an empirical measure of a behavior
> Behavior_empirical and
> a mesure of simulated behavior Behavior_simulated.
>
> Mesures=structure(list(test = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L,
> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
> 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
> 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L,
> 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
> 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 10L, 10L, 10L,
> 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L), Space = c(1L, 2L, 3L,
> 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,
> 9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L,
> 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L,
> 6L, 7L, 8L, 9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,
> 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 1L,
> 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L,
> 7L, 8L, 9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
> 11L, 12L), Behavior_empirical = c(3.02040816326531, 7.95918367346939,
> 10.6162790697674, 4.64150943396226, 1.86538461538462, 1.125,
> 1.01020408163265, 1.2093023255814, 0.292452830188679, 0, 0, 0, 0,
> 1.3265306122449, 0, 3.09433962264151, 0, 1.6875, 2.02040816326531,
> 1.2093023255814, 1.75471698113208, 1.79347826086957,
> 0.243589743589744, 0, 0.377551020408163, 1.98979591836735,
> 6.75581395348837, 6.18867924528302, 7.46153846153846, 0.75, 0, 0,
> 0.292452830188679, 0, 0, 0, 0, 1.3265306122449, 1.93023255813953,
> 10.8301886792453, 3.73076923076923, 0, 2.69387755102041,
> 0.604651162790698, 1.75471698113208, 0, 0, 0, 1.51020408163265,
> 2.6530612244898, 3.86046511627907, 1.54716981132075, 1.86538461538462,
> 1.875, 2.35714285714286, 1.2093023255814, 0.292452830188679, 0, 0,
> 0.823529411764706, 6.79591836734694, 15.2551020408163,
> 5.7906976744186, 1.54716981132075, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> 0.773584905660377, 0, 0, 0.673469387755102, 1.81395348837209,
> 1.75471698113208, 2.51086956521739, 3.10576923076923,
> 3.70588235294118, 3.77551020408163, 9.28571428571428,
> 3.86046511627907, 1.54716981132075, 0, 0, 0, 0, 1.4622641509434, 0, 0,
> 0, 0, 0, 0, 0, 0, 0, 0.673469387755102, 0, 0.292452830188679,
> 4.30434782608696, 1.09615384615385, 5.76470588235294, 0, 0,
> 1.93023255813953, 4.64150943396226, 3.73076923076923, 2.625,
> 0.673469387755102, 0.604651162790698, 0, 0, 0, 0), Behavior_simulated
> = c(18, 61, 129, 198, 128, 57, 44, 80, 36, 8, 0, 0, 0, 0, 0, 49, 50,
> 194, 211, 353, 352, 214, 120, 15, 10, 74, 145, 224, 158, 99, 26, 19,
> 7, 2, 0, 0, 180, 89, 47, 36, 34, 56, 51, 65, 44, 4, 0, 0, 116, 133,
> 131, 103, 74, 132, 75, 44, 0, 0, 0, 0, 532, 165, 18, 5, 0, 0, 0, 0, 0,
> 0, 0, 0, 0, 0, 0, 1, 0, 0, 6, 47, 164, 193, 185, 91, 239, 219, 168,
> 83, 1, 14, 45, 136, 129, 89, 5, 0, 0, 0, 0, 0, 0, 0, 0, 6, 17, 92,
> 280, 273, 0, 6, 25, 108, 129, 285, 171, 181, 39, 2, 0, 0)), .Names =
> c("test", "Space", "Behavior_empirical", "Behavior_simulated"),
> row.names = c(NA, 120L), class = "data.frame")
>
> For each test we study correlation between Behavior_empirical
> Behavior_simulatedelation
>
> Correlation <- character()for(i in 1:10){Mes=Mesures[(Mesures$test==i),]
> co=data.frame(test=i,value=cor(Mes$Behavior_empirical,
> Mes$Behavior_simulated))Correlation
> <- rbind(Correlation, as.data.frame(co))
> i=i+1}
>
> which give us for each test many good correlation values :
>
>     test      value1     1  0.55086832     2  0.43690913     3
> 0.90498064     4 -0.10627145     5  0.84101656     6  0.55608257     7
>  0.80880348     8  0.77212329     9  0.708862410   10  0.5116938
>
> Now , we want to conclude that, if the we have good values of
> Behavior_simulated for each test. It could build the final distribution
> which is the sum of Behavior_simulated and then compare with the sum of
> Behavior_empirical.
>
> Mesures_aggregated<- Mesures %>% group_by(Space) %>%
> summarize(Sum_Behavior_empirical=sum(Behavior_empirical),Sum_Behavior_
> simulated=sum(Behavior_simulated))
>
> I may think that my final correlation result should be good. But it is not
> the case
>
> > cor(Mesures_aggregated$ Sum_Behavior_empirical,Mesures_aggregated$Sum_Behavior_simulated)[1]
> 0.07710804
>
> Is correlation could be a result of correlations of the component of one
> phenomena ? and How to evaluate the contribution of each component test in
> building the 'Sum`?
>
>
> Thanks  a lot for your help.
>
>
> Lenny
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @veekm @ending from y@hoo@co@in  Tue Dec 11 11:14:44 2018
From: @veekm @ending from y@hoo@co@in (aveek)
Date: Tue, 11 Dec 2018 10:14:44 +0000 (UTC)
Subject: [R] CRAN package NlcOptim query
In-Reply-To: <2000317032.1615237.1543918890940@mail.yahoo.com>
References: <afe5c2e520d942adbcceff37f7ea7a01@INW20010891.ADRES.HSBC>
 <2000317032.1615237.1543918890940@mail.yahoo.com>
Message-ID: <456046261.2086478.1544523285070@mail.yahoo.com>

Hi All,
I am facing an issue with an optimization problem which I am trying to solve using NlcOptim package in R. I have tried reaching out to the package maintainer but not received any response, hence posting this here.?

 
Below is the code snippet I am using:
 
 ?
 
#Optimization
 
? obj_F <- function(vect_mat){
 
??? return (sum((c(InputTM) - vect_mat)^2))
 
? }
 
? 
 
??numel = nrow(InputTM)*ncol(InputTM)
 
? opt_vect = solnl(X=c(InputTM), objfun=obj_F, A=-constr_new, B=-x_than0, Aeq=as.matrix(aeq2), Beq=beq2, lb=c(rep(0,numel)),ub=c(rep(1,numel)),tolX = 0)
 
 ?
 
I am attaching in the email the data being used as function arguments.
 
 ?
 
Input_TM is a 9*9 matrix
 
Constr_new is a 120*81 matrix
 
x_than0 is a 120*1 matrix
 
aeq2 is a 17*81 matrix
 
beq2 is a 17*1 matrix
 
 ?
 
Below is the error I am getting :
 
 ?
 
R> ??opt_vect = solnl(X=c(InputTM), objfun=obj_F, A=-constr_new, B=-x_than0, Aeq=as.matrix(aeq2), Beq=beq2, lb=c(rep(0,numel)),ub=c(rep(1,numel)),tolX = 0)
 
Error in as.matrix(A %*% Xtarget) - matrix(B, ncol = 1) :
 
??non-conformable arrays
 
Calls: solnl -> rbind -> rbind
 
In addition: Warning message:
 
In rbind(rbind(lbright, ubright), B) :
 
? number of columns of result is not a multiple of vector length (arg 2)
 
Calls: solnl -> rbind
 
 ?
 
Enter a frame number, or 0 to exit??
 
 ?
 
1: solnl(X = c(InputTM), objfun = obj_F, A = -constr_new, B = -x_than0, Aeq = as.matrix(aeq2), Beq = beq2, lb =
 
2: rbind(rbind(rbind(Aeq %*% Xtarget - Beq, as.matrix(nceq)), as.matrix(A %*% Xtarget) - matrix(B, ncol = 1)), a
 
3: rbind(rbind(Aeq %*% Xtarget - Beq, as.matrix(nceq)), as.matrix(A %*% Xtarget) - matrix(B, ncol = 1))
 
 ?
 
 ?
 
 ?
 
Can you kindly help with this? I am mostly sure that the constraint matrices have been correctly formulated. Am I going wrong with the way I am specifying the arguments?
 
Thanks a lot for any help any of you can offer.
 
 ?
 
Thanks and Regards,
 
Aveek Mukhopadhyay
 


  
<!--#yiv2239857533 _filtered #yiv2239857533 {font-family:"Cambria Math";panose-1:2 4 5 3 5 4 6 3 2 4;} _filtered #yiv2239857533 {font-family:Calibri;panose-1:2 15 5 2 2 2 4 3 2 4;} _filtered #yiv2239857533 {font-family:"Lucida Console";panose-1:2 11 6 9 4 5 4 2 2 4;}#yiv2239857533 #yiv2239857533 p.yiv2239857533MsoNormal, #yiv2239857533 li.yiv2239857533MsoNormal, #yiv2239857533 div.yiv2239857533MsoNormal {margin:0in;margin-bottom:.0001pt;font-size:11.0pt;font-family:"Calibri", sans-serif;}#yiv2239857533 a:link, #yiv2239857533 span.yiv2239857533MsoHyperlink {color:#0563C1;text-decoration:underline;}#yiv2239857533 a:visited, #yiv2239857533 span.yiv2239857533MsoHyperlinkFollowed {color:#954F72;text-decoration:underline;}#yiv2239857533 pre {margin:0in;margin-bottom:.0001pt;font-size:10.0pt;font-family:"Courier New";}#yiv2239857533 span.yiv2239857533HTMLPreformattedChar {font-family:"Courier New";}#yiv2239857533 span.yiv2239857533EmailStyle19 {font-family:"Calibri", sans-serif;color:windowtext;}#yiv2239857533 span.yiv2239857533gnkrckgcmsb {}#yiv2239857533 span.yiv2239857533gnkrckgcmrb {}#yiv2239857533 span.yiv2239857533gnkrckgcasb {}#yiv2239857533 span.yiv2239857533gnkrckgcgsb {}#yiv2239857533 span.yiv2239857533EmailStyle24 {font-family:"Calibri", sans-serif;color:#1F497D;}#yiv2239857533 span.yiv2239857533EmailStyle25 {font-family:"Calibri", sans-serif;color:windowtext;}#yiv2239857533 .yiv2239857533MsoChpDefault {font-size:10.0pt;} _filtered #yiv2239857533 {margin:1.0in 1.0in 1.0in 1.0in;}#yiv2239857533 div.yiv2239857533WordSection1 {}-->  

From ericjberger @ending from gm@il@com  Tue Dec 11 13:54:43 2018
From: ericjberger @ending from gm@il@com (Eric Berger)
Date: Tue, 11 Dec 2018 14:54:43 +0200
Subject: [R] CRAN package NlcOptim query
In-Reply-To: <456046261.2086478.1544523285070@mail.yahoo.com>
References: <afe5c2e520d942adbcceff37f7ea7a01@INW20010891.ADRES.HSBC>
 <2000317032.1615237.1543918890940@mail.yahoo.com>
 <456046261.2086478.1544523285070@mail.yahoo.com>
Message-ID: <CAGgJW75Xo-XOZO2qOniUseaLaM591F7DJGsVN56=F5Eors3xkg@mail.gmail.com>

Hi Aveek,
1. This is an "all-text" mailing list. Your attachment did not come
through.
    You can check out the posting guide (see the link at the bottom of your
email)
     and/or
     use dput(...) on your structures and paste them into your email so
that members of the list can try to reproduce the problem.
2. One way to check out whether you are using a package correctly is to try
a tiny example that you can calculate by hand, and see if you can reproduce
the solution via the package.
    e.g. instead of a 9x9 matrix (hence 81 dimensional problem in your
case), try a 2x2 matrix with maybe just one or two constraints.

HTH,
Eric

On Tue, Dec 11, 2018 at 1:18 PM aveek via R-help <r-help at r-project.org>
wrote:

> Hi All,
> I am facing an issue with an optimization problem which I am trying to
> solve using NlcOptim package in R. I have tried reaching out to the package
> maintainer but not received any response, hence posting this here.
>
>
> Below is the code snippet I am using:
>
>
>
> #Optimization
>
>   obj_F <- function(vect_mat){
>
>     return (sum((c(InputTM) - vect_mat)^2))
>
>   }
>
>
>
>   numel = nrow(InputTM)*ncol(InputTM)
>
>   opt_vect = solnl(X=c(InputTM), objfun=obj_F, A=-constr_new, B=-x_than0,
> Aeq=as.matrix(aeq2), Beq=beq2, lb=c(rep(0,numel)),ub=c(rep(1,numel)),tolX =
> 0)
>
>
>
> I am attaching in the email the data being used as function arguments.
>
>
>
> Input_TM is a 9*9 matrix
>
> Constr_new is a 120*81 matrix
>
> x_than0 is a 120*1 matrix
>
> aeq2 is a 17*81 matrix
>
> beq2 is a 17*1 matrix
>
>
>
> Below is the error I am getting :
>
>
>
> R>   opt_vect = solnl(X=c(InputTM), objfun=obj_F, A=-constr_new,
> B=-x_than0, Aeq=as.matrix(aeq2), Beq=beq2,
> lb=c(rep(0,numel)),ub=c(rep(1,numel)),tolX = 0)
>
> Error in as.matrix(A %*% Xtarget) - matrix(B, ncol = 1) :
>
>   non-conformable arrays
>
> Calls: solnl -> rbind -> rbind
>
> In addition: Warning message:
>
> In rbind(rbind(lbright, ubright), B) :
>
>   number of columns of result is not a multiple of vector length (arg 2)
>
> Calls: solnl -> rbind
>
>
>
> Enter a frame number, or 0 to exit
>
>
>
> 1: solnl(X = c(InputTM), objfun = obj_F, A = -constr_new, B = -x_than0,
> Aeq = as.matrix(aeq2), Beq = beq2, lb =
>
> 2: rbind(rbind(rbind(Aeq %*% Xtarget - Beq, as.matrix(nceq)), as.matrix(A
> %*% Xtarget) - matrix(B, ncol = 1)), a
>
> 3: rbind(rbind(Aeq %*% Xtarget - Beq, as.matrix(nceq)), as.matrix(A %*%
> Xtarget) - matrix(B, ncol = 1))
>
>
>
>
>
>
>
> Can you kindly help with this? I am mostly sure that the constraint
> matrices have been correctly formulated. Am I going wrong with the way I am
> specifying the arguments?
>
> Thanks a lot for any help any of you can offer.
>
>
>
> Thanks and Regards,
>
> Aveek Mukhopadhyay
>
>
>
>
> <!--#yiv2239857533 _filtered #yiv2239857533 {font-family:"Cambria
> Math";panose-1:2 4 5 3 5 4 6 3 2 4;} _filtered #yiv2239857533
> {font-family:Calibri;panose-1:2 15 5 2 2 2 4 3 2 4;} _filtered
> #yiv2239857533 {font-family:"Lucida Console";panose-1:2 11 6 9 4 5 4 2 2
> 4;}#yiv2239857533 #yiv2239857533 p.yiv2239857533MsoNormal, #yiv2239857533
> li.yiv2239857533MsoNormal, #yiv2239857533 div.yiv2239857533MsoNormal
> {margin:0in;margin-bottom:.0001pt;font-size:11.0pt;font-family:"Calibri",
> sans-serif;}#yiv2239857533 a:link, #yiv2239857533
> span.yiv2239857533MsoHyperlink
> {color:#0563C1;text-decoration:underline;}#yiv2239857533 a:visited,
> #yiv2239857533 span.yiv2239857533MsoHyperlinkFollowed
> {color:#954F72;text-decoration:underline;}#yiv2239857533 pre
> {margin:0in;margin-bottom:.0001pt;font-size:10.0pt;font-family:"Courier
> New";}#yiv2239857533 span.yiv2239857533HTMLPreformattedChar
> {font-family:"Courier New";}#yiv2239857533 span.yiv2239857533EmailStyle19
> {font-family:"Calibri", sans-serif;color:windowtext;}#yiv2239857533
> span.yiv2239857533gnkrckgcmsb {}#yiv2239857533
> span.yiv2239857533gnkrckgcmrb {}#yiv2239857533
> span.yiv2239857533gnkrckgcasb {}#yiv2239857533
> span.yiv2239857533gnkrckgcgsb {}#yiv2239857533
> span.yiv2239857533EmailStyle24 {font-family:"Calibri",
> sans-serif;color:#1F497D;}#yiv2239857533 span.yiv2239857533EmailStyle25
> {font-family:"Calibri", sans-serif;color:windowtext;}#yiv2239857533
> .yiv2239857533MsoChpDefault {font-size:10.0pt;} _filtered #yiv2239857533
> {margin:1.0in 1.0in 1.0in 1.0in;}#yiv2239857533
> div.yiv2239857533WordSection1 {}-->
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From giftedlife2014 @ending from gm@il@com  Tue Dec 11 15:09:10 2018
From: giftedlife2014 @ending from gm@il@com (Ogbos Okike)
Date: Tue, 11 Dec 2018 15:09:10 +0100
Subject: [R] Plotting Very Large lat-lon data in x-y axes graph
In-Reply-To: <CA+8X3fUuz0y70LNc2H8TuR0y65h728vR2GA55o+iE1DRGK+o5Q@mail.gmail.com>
References: <CAC8ss33ZS-8NqHf+6Q3Rk=qt+Z7+O7sVFV9Ahi+X8U=DqxhOsw@mail.gmail.com>
 <CA+8X3fU04pA_nh2DvXmY9vUWGr7bsj8AwRBp9Xd=qFvGmj6RMQ@mail.gmail.com>
 <CAC8ss32fAjoh1Q2+7x9i25EyXCGAbeqjYOtG4r3WuWi2S6st4A@mail.gmail.com>
 <CAC8ss33wF7Tp-Uh86S4XMUVxbA56-1atk4kEj67Tg+Ps-3LN-A@mail.gmail.com>
 <CA+8X3fUuz0y70LNc2H8TuR0y65h728vR2GA55o+iE1DRGK+o5Q@mail.gmail.com>
Message-ID: <CAC8ss31zwzNH1VnksDjnJxQZgUVx0KecNydhP5ExNy6hwVAyXA@mail.gmail.com>

Dear All,

Thank you for your advice.

I have looked at the plotrix package and with what I gathered, I
tinkered with the code below and obtain the attached graph:
oolt<-read.table("QUERY2",col.names=c("Lat","Lon"))
 latlim<-c(20,45)
lonlim<-c(-180,180)
latbreaks<-seq(latlim[1],latlim[2],by=5)
lonbreaks<-seq(lonlim[1],lonlim[2],by=10)

mids<-function(x) {
 lenx<-length(x)
 return((x[1:(lenx-1)]+x[2:lenx])/2)
}
lonmids<-mids(lonbreaks)
latmids<-mids(latbreaks)
oolt$loncuts<-cut(oolt$Lon,lonbreaks)
oolt$latcuts<-cut(oolt$Lat,latbreaks)
counts<-table(oolt$latcuts,oolt$loncuts)
png("test.png")
library(plotrix)
countcol<-color.scale(counts,extremes=c("blue","red"))
library(maps)
map("world",xlim=c(-180,180),ylim=c(20,45))
w = map("world")
for(lon in 1:length(lonmids)) {
 for(lat in 1:length(latmids)) {
  if(counts[lat,lon] > 0)
   draw.circle(lonmids[lon],latmids[lat],radius=sqrt(counts[lat,lon]),
    border=countcol[lat,lon],col=countcol[lat,lon])
 }
}
box()
axis(1)
axis(2)
 x<-seq(-180,180,30)
 y<-seq(-90,90,30)
nx = 12
 ny = 6
grid(nx,ny,col = "red",lty="dotted",lwd=par("lwd"),equilogs=TRUE)
testcol<-color.gradient(c(10,30),0,c(30,10),nslices=5)
color.legend(-180,-130,100,-150,countcol,testcol,gradient="x")

I now have the horizontal color legend.

But I have two major problems still.
(1) The black line on top of the color bar is meaningless and I want it go.
(2) I want the numerical values plotted to appear on the color bar
(see the color.legend.png attached please) such that it can be used to
interpret the map.

I would be most grateful for any further assistance.

Very best wishes
Ogbos
On Tue, Dec 11, 2018 at 11:14 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi Ogbos,
> I have been off the air for a couple of days. Look at the color.legend
> function in the plotrix package.
>
> Jim
> On Tue, Dec 11, 2018 at 12:39 PM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
> >
> > Dear Jim,
> > I am still having trouble with the colour code. I await your help when
> > you are less busy.
> >
> > Thank you.
> > Best
> > Ogbos
> > On Mon, Dec 10, 2018 at 6:25 AM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
> > >
> > > Dear Jim,
> > >
> > > I used a bit of my data in my attempt to follow the code.
> > >
> > > I got the attached plot after some tweaks.
> > >
> > > I would like to add horizontal color bar legend that could be used to
> > > explain the number of lightning counts at different points on the
> > > latitude band as plotted.
> > >
> > > Thank you
> > > Warm regards
> > > Ogbos
> > >
> > > On Sun, Dec 9, 2018 at 9:46 PM Jim Lemon <drjimlemon at gmail.com> wrote:
> > > >
> > > > Hi Ogbos,
> > > > Here is a slight modification of a method I use to display trip
> > > > density on a map:
> > > >
> > > > oolt<-read.table(text="Lat Lon
> > > > 30.1426 104.7854
> > > > 30.5622 105.0837
> > > > 30.0966 104.6213
> > > > 29.9795 104.8430
> > > > 39.2802 147.7295
> > > > 30.2469 104.6543
> > > > 26.4428 157.7293
> > > > 29.4782 104.5590
> > > > 32.3839 105.3293
> > > > 26.4746 157.8411
> > > > 25.1014 159.6959
> > > > 25.1242 159.6558
> > > > 30.1607 104.9100
> > > > 31.4900 -71.8919
> > > > 43.3655 -74.9994
> > > > 30.0811 104.8462
> > > > 29.0912 -85.5138
> > > > 26.6204 -80.9342
> > > > 31.5462 -71.9638
> > > > 26.8619 97.3844
> > > > 30.2534 104.6134
> > > > 29.9311 -85.3434
> > > > 26.1524 159.6806
> > > > 26.5112 158.0233
> > > > 26.5441 158.0565
> > > > 27.8901 -105.8554
> > > > 30.3175 104.7135
> > > > 26.4822 157.6127
> > > > 30.1887 104.5986
> > > > 29.5058 104.5661
> > > > 26.4010 157.5749
> > > > 30.2281 104.7585
> > > > 31.4556 110.5619
> > > > 30.1700 104.5861
> > > > 26.3911 157.4776
> > > > 30.6493 104.9949
> > > > 30.2209 104.6629
> > > > 26.0488 97.3608
> > > > 30.2142 104.8023
> > > > 30.1806 104.8158
> > > > 25.2107 160.1690
> > > > 30.6708 104.9385
> > > > 30.4152 104.7002
> > > > 30.2446 104.7804
> > > > 29.5760 -85.1535
> > > > 26.4484 92.4312
> > > > 26.3914 157.4189
> > > > 26.3986 157.4421
> > > > 30.4903 -88.2271
> > > > 30.6727 104.8768
> > > > 30.2518 104.6466
> > > > 41.6979 -78.4136
> > > > 33.7575 72.1089
> > > > 26.8333 -80.9485
> > > > 25.3103 124.0978
> > > > 30.1742 104.7554
> > > > 30.6345 104.9739
> > > > 30.2075 104.7960
> > > > 30.2226 104.7517
> > > > 30.5948 105.0532",
> > > > header=TRUE)
> > > > latlim<-c(20,45)
> > > > lonlim<-c(-90,160)
> > > > latbreaks<-seq(latlim[1],latlim[2],by=5)
> > > > lonbreaks<-seq(lonlim[1],lonlim[2],by=10)
> > > >
> > > > mids<-function(x) {
> > > >  lenx<-length(x)
> > > >  return((x[1:(lenx-1)]+x[2:lenx])/2)
> > > > }
> > > > lonmids<-mids(lonbreaks)
> > > > latmids<-mids(latbreaks)
> > > > oolt$loncuts<-cut(oolt$Lon,lonbreaks)
> > > > oolt$latcuts<-cut(oolt$Lat,latbreaks)
> > > > counts<-table(oolt$latcuts,oolt$loncuts)
> > > > library(plotrix)
> > > > countcol<-color.scale(counts,extremes=c("blue","red"))
> > > > map("world",xlim=c(-90,160),ylim=c(20,45))
> > > > for(lon in 1:length(lonmids)) {
> > > >  for(lat in 1:length(latmids)) {
> > > >   if(counts[lat,lon] > 0)
> > > >    draw.circle(lonmids[lon],latmids[lat],radius=sqrt(counts[lat,lon]),
> > > >     border=countcol[lat,lon],col=countcol[lat,lon])
> > > >  }
> > > > }
> > > >
> > > > If you have very large counts in some places you may need to adjust
> > > > the radius of the circles.
> > > >
> > > > Jim
> > > > On Mon, Dec 10, 2018 at 2:50 AM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
> > > > >
> > > > > Dear Contributors,
> > > > >
> > > > > I have a data of the form:
> > > > > Lat          Lon
> > > > > 30.1426 104.7854
> > > > > 30.5622 105.0837
> > > > > 30.0966 104.6213
> > > > > 29.9795 104.8430
> > > > > 39.2802 147.7295
> > > > > 30.2469 104.6543
> > > > > 26.4428 157.7293
> > > > > 29.4782 104.5590
> > > > > 32.3839 105.3293
> > > > > 26.4746 157.8411
> > > > > 25.1014 159.6959
> > > > > 25.1242 159.6558
> > > > > 30.1607 104.9100
> > > > > 31.4900 -71.8919
> > > > > 43.3655 -74.9994
> > > > > 30.0811 104.8462
> > > > > 29.0912 -85.5138
> > > > > 26.6204 -80.9342
> > > > > 31.5462 -71.9638
> > > > > 26.8619 97.3844
> > > > > 30.2534 104.6134
> > > > > 29.9311 -85.3434
> > > > > 26.1524 159.6806
> > > > > 26.5112 158.0233
> > > > > 26.5441 158.0565
> > > > > 27.8901 -105.8554
> > > > > 30.3175 104.7135
> > > > > 26.4822 157.6127
> > > > > 30.1887 104.5986
> > > > > 29.5058 104.5661
> > > > > 26.4010 157.5749
> > > > > 30.2281 104.7585
> > > > > 31.4556 110.5619
> > > > > 30.1700 104.5861
> > > > > 26.3911 157.4776
> > > > > 30.6493 104.9949
> > > > > 30.2209 104.6629
> > > > > 26.0488 97.3608
> > > > > 30.2142 104.8023
> > > > > 30.1806 104.8158
> > > > > 25.2107 160.1690
> > > > > 30.6708 104.9385
> > > > > 30.4152 104.7002
> > > > > 30.2446 104.7804
> > > > > 29.5760 -85.1535
> > > > > 26.4484 92.4312
> > > > > 26.3914 157.4189
> > > > > 26.3986 157.4421
> > > > > 30.4903 -88.2271
> > > > > 30.6727 104.8768
> > > > > 30.2518 104.6466
> > > > > 41.6979 -78.4136
> > > > > 33.7575 72.1089
> > > > > 26.8333 -80.9485
> > > > > 25.3103 124.0978
> > > > > 30.1742 104.7554
> > > > > 30.6345 104.9739
> > > > > 30.2075 104.7960
> > > > > 30.2226 104.7517
> > > > > 30.5948 105.0532.
> > > > > The record is for lightning flashes in the continental U.S. and
> > > > > surrounding waters within the latitudinal band between
> > > > > 258 and 458N.
> > > > >
> > > > > I want to display the result in x-y co-ordinate plot. However, the
> > > > > data is very large such that when plotted, everything just appeared
> > > > > blurred.
> > > > >
> > > > >
> > > > > Is there a way of using color codes to  indicate the regions of higher
> > > > > or lower flash densities?
> > > > >
> > > > > I can attach the plot I generated but I am not sure if the moderator
> > > > > will allow it to go with this.
> > > > >
> > > > > I will send it in a separate email if required.
> > > > >
> > > > > Thank you so much for sparing your time.
> > > > >
> > > > > Best
> > > > > Ogbos
> > > > >
> > > > > ______________________________________________
> > > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > > and provide commented, minimal, self-contained, reproducible code.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: test.png
Type: image/png
Size: 54480 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20181211/787cd39d/attachment.png>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: color.legend.png
Type: image/png
Size: 11630 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20181211/787cd39d/attachment-0001.png>

From petr@pik@l @ending from prechez@@cz  Tue Dec 11 16:09:23 2018
From: petr@pik@l @ending from prechez@@cz (PIKAL Petr)
Date: Tue, 11 Dec 2018 15:09:23 +0000
Subject: [R] R and factorytalk historian
Message-ID: <f104645d3ae54c25b4ebe038f2123d43@SRVEXCHCM1302.precheza.cz>

Hallo all

Does anybody know if R could be used directly with FactoryTalk Historian programme from Rockwell automation.

It is probably possible to use Excel as interface but I would prefer not to.

Best regards.

Petr Pikal
Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner's personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


	[[alternative HTML version deleted]]


From jdnewmil @ending from dcn@d@vi@@c@@u@  Tue Dec 11 17:18:08 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Tue, 11 Dec 2018 08:18:08 -0800
Subject: [R] R and factorytalk historian
In-Reply-To: <f104645d3ae54c25b4ebe038f2123d43@SRVEXCHCM1302.precheza.cz>
References: <f104645d3ae54c25b4ebe038f2123d43@SRVEXCHCM1302.precheza.cz>
Message-ID: <BE34F84E-3229-4CAB-8B11-E43F0E52D1D4@dcn.davis.ca.us>

R supports a wide range of data transfer methods, Petr... you know for example that if you export data from Excel in CSV format then you can import it to R, yet this solution does not satisfy all users in all cases. What expectations do you have?

Why don't you do some legwork and identify what data formats/mechanisms this "FactoryTalk Historian" understands? E.g. does it understand csv? ODBC? COM? HTTP API?

On December 11, 2018 7:09:23 AM PST, PIKAL Petr <petr.pikal at precheza.cz> wrote:
>Hallo all
>
>Does anybody know if R could be used directly with FactoryTalk
>Historian programme from Rockwell automation.
>
>It is probably possible to use Excel as interface but I would prefer
>not to.
>
>Best regards.
>
>Petr Pikal
>Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj?
>obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na:
>https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information
>about processing and protection of business partner's personal data are
>available on website:
>https://www.precheza.cz/en/personal-data-protection-principles/
>D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou
>d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en?
>odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any
>documents attached to it may be confidential and are subject to the
>legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @uror@@field@ @ending from yellowwhitecont@ct@@info  Tue Dec 11 22:54:45 2018
From: @uror@@field@ @ending from yellowwhitecont@ct@@info (Aurora Fields)
Date: Wed, 12 Dec 2018 03:24:45 +0530
Subject: [R] R-Project
Message-ID: <0e6401d4919c$24bdef20$6e39cd60$@fields@yellowwhitecontacts.info>

 

 

Hi,

 

I am following up to confirm, if you are interested acquiring the attendee
list.

 

2019 Triological Society Combined Sections Meeting

 

Coronado, California

 

24-26 Jan 2019

 

Counts: 1,000

 

Let me know your thoughts so that I can send cost & additional information.

 

Regards,

 

Aurora Fields
Sr. Business Analyst

 



---
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus

	[[alternative HTML version deleted]]


From @veekm @ending from y@hoo@co@in  Wed Dec 12 07:44:14 2018
From: @veekm @ending from y@hoo@co@in (aveek)
Date: Wed, 12 Dec 2018 06:44:14 +0000 (UTC)
Subject: [R] CRAN package NlcOptim query
In-Reply-To: <CAGgJW75Xo-XOZO2qOniUseaLaM591F7DJGsVN56=F5Eors3xkg@mail.gmail.com>
References: <afe5c2e520d942adbcceff37f7ea7a01@INW20010891.ADRES.HSBC>
 <2000317032.1615237.1543918890940@mail.yahoo.com>
 <456046261.2086478.1544523285070@mail.yahoo.com>
 <CAGgJW75Xo-XOZO2qOniUseaLaM591F7DJGsVN56=F5Eors3xkg@mail.gmail.com>
Message-ID: <1691260324.2450334.1544597054487@mail.yahoo.com>


Hello Eric,

Thanks for your response and suggestions.?

I have used dput() on the R objects - sharing below so that it is possible for anyone to recreate the situation.

I have still kept it as a 9*9 matrix but for simplicity we now only have 2 equality and 2 non equality constraints.

Thanks again for your help.

InputTM


structure(c(0.813231189406663, 0.0199464964676128, 0.00100552815128915,

0.000465771436428336, 0.000736922016196076, 0.00203037431732662,

0.000596998285890709, 0.0011699714577823, 0, 0.103116692172408,

0.751775368068589, 0.0160957427707042, 0.00285542569941823, 0.0020295541916448,

0.00954562743564027, 0.00173399818906894, 0.00292299139663608,

0, 0.0481959576543631, 0.177032393868544, 0.811609524051149,

0.146703962329218, 0.0698423269415636, 0.168241524872922, 0.0505757280338206,

0.0324917017565673, 0, 0.026504623193874, 0.038430496838613,

0.134709744786799, 0.758322716164413, 0.176013939161559, 0.234265359508999,

0.108188555487004, 0.0476663548325017, 0, 0.00520614395937929,

0.00868690292550468, 0.0223895752360805, 0.0581458712447338,

0.681496895121054, 0.0733970775224908, 0.0508491985259732, 0.0268876385360338,

0, 0.000749001395622802, 0.00181690494827145, 0.00317194515883476,

0.00705434604769267, 0.0211989316284324, 0.464732208379131, 0.0165146818291576,

0.00721872506710652, 0, 0.000960493069403903, 0.00138444384054219,

0.00703528202498607, 0.0163983255053438, 0.0301780379843763,

0.0280699612529658, 0.491157627745315, 0.0235353949469527, 0,

0.00112628575287418, 0.000477300396419488, 0.00223238360574478,

0.00521558462566306, 0.00925149338117537, 0.00878272484051914,

0.163654071683595, 0.611806567272906, 0, 0.000909613395411595,

0.000449692645903539, 0.00175027421441265, 0.00483799694708872,

0.00925189957399783, 0.0109351418700064, 0.116729140220175, 0.246300654733514,

1), .Dim = c(9L, 9L))

?

?

Constr_new

structure(c(1, 0, -1, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,

0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,

0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,

0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,

0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,

0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,

0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,

0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), .Dim = c(2L,

81L), .Dimnames = list(NULL, c("X11", "X12", "X13", "X14", "X15",

"X16", "X17", "X18", "X19", "X21", "X22", "X23", "X24", "X25",

"X26", "X27", "X28", "X29", "X31", "X32", "X33", "X34", "X35",

"X36", "X37", "X38", "X39", "X41", "X42", "X43", "X44", "X45",

"X46", "X47", "X48", "X49", "X51", "X52", "X53", "X54", "X55",

"X56", "X57", "X58", "X59", "X61", "X62", "X63", "X64", "X65",

"X66", "X67", "X68", "X69", "X71", "X72", "X73", "X74", "X75",

"X76", "X77", "X78", "X79", "X81", "X82", "X83", "X84", "X85",

"X86", "X87", "X88", "X89", "X91", "X92", "X93", "X94", "X95",

"X96", "X97", "X98", "X99")))

?

?

x_than0

c(1e-04, 1e-04)

?

aeq2

structure(list(X1 = c(1, 0), X2 = c(1, 0), X3 = c(1, 0), X4 = c(1,

0), X5 = c(1, 0), X6 = c(1, 0), X7 = c(1, 0), X8 = c(1, 0), X9 = c(1,

0), X10 = c(0, 1), X11 = c(0, 1), X12 = c(0, 1), X13 = c(0, 1

), X14 = c(0, 1), X15 = c(0, 1), X16 = c(0, 1), X17 = c(0, 1),

????X18 = c(0, 1), X19 = c(0, 0), X20 = c(0, 0), X21 = c(0, 0

??? ), X22 = c(0, 0), X23 = c(0, 0), X24 = c(0, 0), X25 = c(0,

????0), X26 = c(0, 0), X27 = c(0, 0), X28 = c(0, 0), X29 = c(0,

????0), X30 = c(0, 0), X31 = c(0, 0), X32 = c(0, 0), X33 = c(0,

????0), X34 = c(0, 0), X35 = c(0, 0), X36 = c(0, 0), X37 = c(0,

????0), X38 = c(0, 0), X39 = c(0, 0), X40 = c(0, 0), X41 = c(0,

????0), X42 = c(0, 0), X43 = c(0, 0), X44 = c(0, 0), X45 = c(0,

????0), X46 = c(0, 0), X47 = c(0, 0), X48 = c(0, 0), X49 = c(0,

????0), X50 = c(0, 0), X51 = c(0, 0), X52 = c(0, 0), X53 = c(0,

????0), X54 = c(0, 0), X55 = c(0, 0), X56 = c(0, 0), X57 = c(0,

????0), X58 = c(0, 0), X59 = c(0, 0), X60 = c(0, 0), X61 = c(0,

????0), X62 = c(0, 0), X63 = c(0, 0), X64 = c(0, 0), X65 = c(0,

????0), X66 = c(0, 0), X67 = c(0, 0), X68 = c(0, 0), X69 = c(0,

????0), X70 = c(0, 0), X71 = c(0, 0), X72 = c(0, 0), X73 = c(0,

????0), X74 = c(0, 0), X75 = c(0, 0), X76 = c(0, 0), X77 = c(0,

????0), X78 = c(0, 0), X79 = c(0, 0), X80 = c(0, 0), X81 = c(0,

????0)), .Names = c("X1", "X2", "X3", "X4", "X5", "X6", "X7",

"X8", "X9", "X10", "X11", "X12", "X13", "X14", "X15", "X16",

"X17", "X18", "X19", "X20", "X21", "X22", "X23", "X24", "X25",

"X26", "X27", "X28", "X29", "X30", "X31", "X32", "X33", "X34",

"X35", "X36", "X37", "X38", "X39", "X40", "X41", "X42", "X43",

"X44", "X45", "X46", "X47", "X48", "X49", "X50", "X51", "X52",

"X53", "X54", "X55", "X56", "X57", "X58", "X59", "X60", "X61",

"X62", "X63", "X64", "X65", "X66", "X67", "X68", "X69", "X70",

"X71", "X72", "X73", "X74", "X75", "X76", "X77", "X78", "X79",

"X80", "X81"), row.names = 1:2, class = "data.frame")

?

beq2

c(1, 1)




Regards,

Aveek

Sent from Yahoo Mail on Android 
 
  On Tue, Dec 11, 2018 at 6:25 PM, Eric Berger<ericjberger at gmail.com> wrote:   Hi Aveek,1. This is an "all-text" mailing list. Your attachment did not come through.?? ? You can check out the posting guide (see the link at the bottom of your email)?? ? ?and/or??? ? ?use dput(...) on your structures and paste them into your email so that members of the list can try to reproduce the problem.2. One way to check out whether you are using a package correctly is to try a tiny example that you can calculate by hand, and see if you can reproduce the solution via the package.? ? e.g. instead of a 9x9 matrix (hence 81 dimensional problem in your case), try a 2x2 matrix with maybe just one or two constraints.?
HTH,Eric
On Tue, Dec 11, 2018 at 1:18 PM aveek via R-help <r-help at r-project.org> wrote:

Hi All,
I am facing an issue with an optimization problem which I am trying to solve using NlcOptim package in R. I have tried reaching out to the package maintainer but not received any response, hence posting this here.?


Below is the code snippet I am using:

??

#Optimization

? obj_F <- function(vect_mat){

??? return (sum((c(InputTM) - vect_mat)^2))

? }

? 

??numel = nrow(InputTM)*ncol(InputTM)

? opt_vect = solnl(X=c(InputTM), objfun=obj_F, A=-constr_new, B=-x_than0, Aeq=as.matrix(aeq2), Beq=beq2, lb=c(rep(0,numel)),ub=c(rep(1,numel)),tolX = 0)

??

I am attaching in the email the data being used as function arguments.

??

Input_TM is a 9*9 matrix

Constr_new is a 120*81 matrix

x_than0 is a 120*1 matrix

aeq2 is a 17*81 matrix

beq2 is a 17*1 matrix

??

Below is the error I am getting :

??

R> ??opt_vect = solnl(X=c(InputTM), objfun=obj_F, A=-constr_new, B=-x_than0, Aeq=as.matrix(aeq2), Beq=beq2, lb=c(rep(0,numel)),ub=c(rep(1,numel)),tolX = 0)

Error in as.matrix(A %*% Xtarget) - matrix(B, ncol = 1) :

??non-conformable arrays

Calls: solnl -> rbind -> rbind

In addition: Warning message:

In rbind(rbind(lbright, ubright), B) :

? number of columns of result is not a multiple of vector length (arg 2)

Calls: solnl -> rbind

??

Enter a frame number, or 0 to exit??

??

1: solnl(X = c(InputTM), objfun = obj_F, A = -constr_new, B = -x_than0, Aeq = as.matrix(aeq2), Beq = beq2, lb =

2: rbind(rbind(rbind(Aeq %*% Xtarget - Beq, as.matrix(nceq)), as.matrix(A %*% Xtarget) - matrix(B, ncol = 1)), a

3: rbind(rbind(Aeq %*% Xtarget - Beq, as.matrix(nceq)), as.matrix(A %*% Xtarget) - matrix(B, ncol = 1))

??

??

??

Can you kindly help with this? I am mostly sure that the constraint matrices have been correctly formulated. Am I going wrong with the way I am specifying the arguments?

Thanks a lot for any help any of you can offer.

??

Thanks and Regards,

Aveek Mukhopadhyay




<!--#yiv2239857533 _filtered #yiv2239857533 {font-family:"Cambria Math";panose-1:2 4 5 3 5 4 6 3 2 4;} _filtered #yiv2239857533 {font-family:Calibri;panose-1:2 15 5 2 2 2 4 3 2 4;} _filtered #yiv2239857533 {font-family:"Lucida Console";panose-1:2 11 6 9 4 5 4 2 2 4;}#yiv2239857533 #yiv2239857533 p.yiv2239857533MsoNormal, #yiv2239857533 li.yiv2239857533MsoNormal, #yiv2239857533 div.yiv2239857533MsoNormal {margin:0in;margin-bottom:.0001pt;font-size:11.0pt;font-family:"Calibri", sans-serif;}#yiv2239857533 a:link, #yiv2239857533 span.yiv2239857533MsoHyperlink {color:#0563C1;text-decoration:underline;}#yiv2239857533 a:visited, #yiv2239857533 span.yiv2239857533MsoHyperlinkFollowed {color:#954F72;text-decoration:underline;}#yiv2239857533 pre {margin:0in;margin-bottom:.0001pt;font-size:10.0pt;font-family:"Courier New";}#yiv2239857533 span.yiv2239857533HTMLPreformattedChar {font-family:"Courier New";}#yiv2239857533 span.yiv2239857533EmailStyle19 {font-family:"Calibri", sans-serif;color:windowtext;}#yiv2239857533 span.yiv2239857533gnkrckgcmsb {}#yiv2239857533 span.yiv2239857533gnkrckgcmrb {}#yiv2239857533 span.yiv2239857533gnkrckgcasb {}#yiv2239857533 span.yiv2239857533gnkrckgcgsb {}#yiv2239857533 span.yiv2239857533EmailStyle24 {font-family:"Calibri", sans-serif;color:#1F497D;}#yiv2239857533 span.yiv2239857533EmailStyle25 {font-family:"Calibri", sans-serif;color:windowtext;}#yiv2239857533 .yiv2239857533MsoChpDefault {font-size:10.0pt;} _filtered #yiv2239857533 {margin:1.0in 1.0in 1.0in 1.0in;}#yiv2239857533 div.yiv2239857533WordSection1 {}-->? 
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

  

	[[alternative HTML version deleted]]


From petr@pik@l @ending from prechez@@cz  Wed Dec 12 10:53:40 2018
From: petr@pik@l @ending from prechez@@cz (PIKAL Petr)
Date: Wed, 12 Dec 2018 09:53:40 +0000
Subject: [R] R and factorytalk historian
In-Reply-To: <BE34F84E-3229-4CAB-8B11-E43F0E52D1D4@dcn.davis.ca.us>
References: <f104645d3ae54c25b4ebe038f2123d43@SRVEXCHCM1302.precheza.cz>
 <BE34F84E-3229-4CAB-8B11-E43F0E52D1D4@dcn.davis.ca.us>
Message-ID: <6b7383eb395b4ab7bf0b0003491e27f4@SRVEXCHCM1302.precheza.cz>

Hi Jeff,

FactoryTalk Historian should be system for operation data management. I am aware of some R capabilities in transfering data and during our training I will ask about the way FTH can export data. From my current knowledge export to Excel via integrated module is possible.

My question was mainly driven by broad expertise among R users and hope that maybe some R user already performed such task and I could learn from his/her experience.

S pozdravem | Best Regards
RNDr. Petr PIKAL
Vedouc? V?zkumu a v?voje | Research Manager
PRECHEZA a.s.
n?b?. Dr. Edvarda Bene?e 1170/24 | 750 02 P?erov | Czech Republic
Tel: +420 581 252 256 | GSM: +420 724 008 364
petr.pikal at precheza.cz | www.precheza.cz

Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/

> -----Original Message-----
> From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> Sent: Tuesday, December 11, 2018 5:18 PM
> To: r-help at r-project.org; PIKAL Petr <petr.pikal at precheza.cz>; r-help <r-
> help at r-project.org>
> Subject: Re: [R] R and factorytalk historian
> 
> R supports a wide range of data transfer methods, Petr... you know for example
> that if you export data from Excel in CSV format then you can import it to R,
> yet this solution does not satisfy all users in all cases. What expectations do you
> have?
> 
> Why don't you do some legwork and identify what data formats/mechanisms
> this "FactoryTalk Historian" understands? E.g. does it understand csv? ODBC?
> COM? HTTP API?
> 
> On December 11, 2018 7:09:23 AM PST, PIKAL Petr <petr.pikal at precheza.cz>
> wrote:
> >Hallo all
> >
> >Does anybody know if R could be used directly with FactoryTalk
> >Historian programme from Rockwell automation.
> >
> >It is probably possible to use Excel as interface but I would prefer
> >not to.
> >
> >Best regards.
> >
> >Petr Pikal
> >Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj?
> >obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na:
> >https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information
> >about processing and protection of business partner's personal data are
> >available on website:
> >https://www.precheza.cz/en/personal-data-protection-principles/
> >D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou
> >d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en?
> >odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any
> >documents attached to it may be confidential and are subject to the
> >legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/
> >
> >
> >	[[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
> 
> --
> Sent from my phone. Please excuse my brevity.

From wolfg@ng@viechtb@uer @ending from m@@@trichtuniver@ity@nl  Wed Dec 12 11:20:45 2018
From: wolfg@ng@viechtb@uer @ending from m@@@trichtuniver@ity@nl (Viechtbauer, Wolfgang (SP))
Date: Wed, 12 Dec 2018 10:20:45 +0000
Subject: [R] R and factorytalk historian
In-Reply-To: <f104645d3ae54c25b4ebe038f2123d43@SRVEXCHCM1302.precheza.cz>
References: <f104645d3ae54c25b4ebe038f2123d43@SRVEXCHCM1302.precheza.cz>
Message-ID: <d960f025e1b1485c93f875eab7749cce@UM-MAIL3214.unimaas.nl>

Dear Petr,

Sorry, no experience with the FTH, but related to this, has anybody gotten R to interface nicely with the Retro Encabulator, providing live read outs of the synchronizing cardinal grammeters?

(my apologies, I just couldn't resist)

Best,
Wolfgang

>-----Original Message-----
>From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of PIKAL
>Petr
>Sent: Tuesday, 11 December, 2018 16:09
>To: r-help
>Subject: [R] R and factorytalk historian
>
>Hallo all
>
>Does anybody know if R could be used directly with FactoryTalk Historian
>programme from Rockwell automation.
>
>It is probably possible to use Excel as interface but I would prefer not
>to.
>
>Best regards.
>
>Petr Pikal


From hwborcher@ @ending from gm@il@com  Wed Dec 12 12:45:06 2018
From: hwborcher@ @ending from gm@il@com (Hans W Borchers)
Date: Wed, 12 Dec 2018 12:45:06 +0100
Subject: [R] CRAN package NlcOptim query
Message-ID: <CAML4n3P0w_fb+51vPco5Lv9R80iVMjsDL4MtWLCNo6=Ly+Qtkw@mail.gmail.com>

This is still not complete: `x_than0` is missing.
`Constr_new` is written with a capital 'C'.
And aeq2 is a list of column vectors, not a matrix.
Setting the tolerance to 0 does not seem to be a good idea.

Making aeq2 a matrix and adding `x_than0 <- matrix(c(1, 1))`, then

    aeq2 <- as.matrix(aeq2)
    x_than0 <- matrix(c(1, 1))

    NlcOptim::solnl(X=c(InputTM), objfun=obj_F, A=-Constr_new, B=-x_than0,
                Aeq=as.matrix(aeq2), Beq=beq2,
                lb=c(rep(0,numel)),ub=c(rep(1,numel)), tolX = 0)

will indeed return in the same error, while it runs without error if you
either leave out the inequality constraints or the bounds constraints. So
I guess there may be a bug when the function internally combines these
constraints and the bounds.

You could / should write to the maintainer. I know he is very responsive.

For the moment, you can combine the bounds constraints and the lower and
upper bounds yourself:

    myA <- rbind(-Constr_new, diag(-1,numel), diag(1,numel))
    myB <- c(-x_than0, rep(0,numel), rep(1,numel))

    NlcOptim::solnl(X=c(InputTM), objfun=obj_F, A=myA, B=myB,
                    Aeq=as.matrix(aeq2), Beq=beq2)

returns "constraints are inconsistent, no solution!", but that may be the
case because I don't know your `x_than` value.


From bog@@o@chri@tofer @ending from gm@il@com  Wed Dec 12 12:55:37 2018
From: bog@@o@chri@tofer @ending from gm@il@com (Christofer Bogaso)
Date: Wed, 12 Dec 2018 17:25:37 +0530
Subject: [R] Width of a text
Message-ID: <CA+dpOJ==ejgiVLuitA1kUPeGBzhC-h3MrxkYnXDU8NdJhQ43Ag@mail.gmail.com>

Hi,

In HTML there is a way to measure the width of a Text before printing it on
screen as in https://www.w3schools.com/tags/canvas_measuretext.asp

In R we have nchar() function which just measures the number of letters in
a Text, but I wonder if we can measure the width of text as well.

I have a shiny app where I need to print a text in an Area onto the screen
which has a specific width, so some the texts are not showing within that
area (overflowing), where some are though all of them have the equal number
of letters.

Thanks for any input.

	[[alternative HTML version deleted]]


From btupper @ending from bigelow@org  Wed Dec 12 14:32:05 2018
From: btupper @ending from bigelow@org (Ben Tupper)
Date: Wed, 12 Dec 2018 08:32:05 -0500
Subject: [R] Width of a text
In-Reply-To: <CA+dpOJ==ejgiVLuitA1kUPeGBzhC-h3MrxkYnXDU8NdJhQ43Ag@mail.gmail.com>
References: <CA+dpOJ==ejgiVLuitA1kUPeGBzhC-h3MrxkYnXDU8NdJhQ43Ag@mail.gmail.com>
Message-ID: <19E85258-851B-4904-AEEC-2CCD0E18D087@bigelow.org>

Hi,

Does strwidth() do the trick?

https://www.rdocumentation.org/packages/graphics/versions/3.5.1/topics/strwidth <https://www.rdocumentation.org/packages/graphics/versions/3.5.1/topics/strwidth>

Ben

> On Dec 12, 2018, at 6:55 AM, Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
> 
> Hi,
> 
> In HTML there is a way to measure the width of a Text before printing it on
> screen as in https://www.w3schools.com/tags/canvas_measuretext.asp
> 
> In R we have nchar() function which just measures the number of letters in
> a Text, but I wonder if we can measure the width of text as well.
> 
> I have a shiny app where I need to print a text in an Area onto the screen
> which has a specific width, so some the texts are not showing within that
> area (overflowing), where some are though all of them have the equal number
> of letters.
> 
> Thanks for any input.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org

Ecological Forecasting: https://eco.bigelow.org/






	[[alternative HTML version deleted]]


From petr@pik@l @ending from prechez@@cz  Wed Dec 12 16:17:08 2018
From: petr@pik@l @ending from prechez@@cz (PIKAL Petr)
Date: Wed, 12 Dec 2018 15:17:08 +0000
Subject: [R] R and factorytalk historian
In-Reply-To: <d960f025e1b1485c93f875eab7749cce@UM-MAIL3214.unimaas.nl>
References: <f104645d3ae54c25b4ebe038f2123d43@SRVEXCHCM1302.precheza.cz>
 <d960f025e1b1485c93f875eab7749cce@UM-MAIL3214.unimaas.nl>
Message-ID: <a696a745489c450b90529df2fabfb722@SRVEXCHCM1302.precheza.cz>

Hi

Well, the final answer is that data from FTH could be transfered to other software **only** through Excel.

Regarding RetroEncabulator, readings could be enahanced by extensive and elaborate use of nanoputian molecules chained together.

Cheers
Petr

> -----Original Message-----
> From: Viechtbauer, Wolfgang (SP)
> <wolfgang.viechtbauer at maastrichtuniversity.nl>
> Sent: Wednesday, December 12, 2018 11:21 AM
> To: PIKAL Petr <petr.pikal at precheza.cz>; r-help <r-help at r-project.org>
> Subject: RE: R and factorytalk historian
>
> Dear Petr,
>
> Sorry, no experience with the FTH, but related to this, has anybody gotten R to
> interface nicely with the Retro Encabulator, providing live read outs of the
> synchronizing cardinal grammeters?
>
> (my apologies, I just couldn't resist)
>
> Best,
> Wolfgang
>
> >-----Original Message-----
> >From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of PIKAL
> >Petr
> >Sent: Tuesday, 11 December, 2018 16:09
> >To: r-help
> >Subject: [R] R and factorytalk historian
> >
> >Hallo all
> >
> >Does anybody know if R could be used directly with FactoryTalk
> >Historian programme from Rockwell automation.
> >
> >It is probably possible to use Excel as interface but I would prefer
> >not to.
> >
> >Best regards.
> >
> >Petr Pikal
Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


From m@rongiu@luigi @ending from gm@il@com  Wed Dec 12 16:19:01 2018
From: m@rongiu@luigi @ending from gm@il@com (Luigi Marongiu)
Date: Wed, 12 Dec 2018 16:19:01 +0100
Subject: [R] R plot split screen in uneven panels
Message-ID: <CAMk+s2RX94-=HQ7XjzxO7_3dub5huYKudSHuZaq4hF-AociWPQ@mail.gmail.com>

Dear all,
I would like to draw two plots in the same device so that there is a
single row and two columns, with the first column being 1/3 of the
device's width.
I am creating a PNG object with width = 30 and height = 20 cm.
I know that I should use split.screen or layout but I am lost with the
matrix to pass to the functions.
For istance, I tried:
# distance in arbitrary units (so let's say cm) from of corners
# left, right, bottom, and top counting from bottom left corner
# that is first panel has the bottom right corner 20 cm from the bottom left?
> m = matrix(c(0,20,40,0, 20,60,40,0), byrow=T, ncol=4)
> m
     [,1] [,2] [,3] [,4]
[1,]    0   20   40    0
[2,]   20   60   40    0
> split.screen(m)
Error in par(split.screens[[cur.screen]]) :
  invalid value specified for graphical parameter "fig"
> m[1,]
[1]  0 20 40  0
> split.screen(m[1,])
Error in split.screen(m[1, ]) : 'figs' must specify at least one screen

What should be the syntax for this task?

-- 
Best regards,
Luigi


From m@cqueen1 @ending from llnl@gov  Wed Dec 12 17:17:26 2018
From: m@cqueen1 @ending from llnl@gov (MacQueen, Don)
Date: Wed, 12 Dec 2018 16:17:26 +0000
Subject: [R] Different performance with different R versions
In-Reply-To: <78104315-645E-4192-86D6-FA4F6040BECE@maastrichtuniversity.nl>
References: <78104315-645E-4192-86D6-FA4F6040BECE@maastrichtuniversity.nl>
Message-ID: <97C86EB8-0C2B-49C7-A118-F14A07A5D605@llnl.gov>

Probably more appropriate for R-SIG-Mac

--
Don MacQueen
Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062
Lab cell 925-724-7509
 
 

?On 12/11/18, 12:22 AM, "R-help on behalf of cowan robin" <r-help-bounces at r-project.org on behalf of r.cowan at maastrichtuniversity.nl> wrote:

    I am running a small simulation, and getting very different run times when I use different versions of R. 
    Two set-ups using the same machine (MacBook Pro 2013 vintage)
    
    1. R version 3.1.3  running on system OS X 10.9.5
    
    > system.time(source("simulationR-R.R"))
    
      user  system elapsed 
      3.890   0.061   3.965 
    
    
    
    Compared to
    
    
    2. R version 3.5.1  running on system OS X 10.12.6
    
    > system.time(source("simulationR-R.R"))
    
      user  system elapsed 
    277.924   2.087 280.841 
    
    
    The source code is identical. This is a pretty big difference running the same code on the same hardware.
    Before submitting the code, is this a known issue?
    
    ______________________________________________
    R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    https://stat.ethz.ch/mailman/listinfo/r-help
    PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    and provide commented, minimal, self-contained, reproducible code.
    


From bgunter@4567 @ending from gm@il@com  Wed Dec 12 17:39:37 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Wed, 12 Dec 2018 08:39:37 -0800
Subject: [R] R plot split screen in uneven panels
In-Reply-To: <CAMk+s2RX94-=HQ7XjzxO7_3dub5huYKudSHuZaq4hF-AociWPQ@mail.gmail.com>
References: <CAMk+s2RX94-=HQ7XjzxO7_3dub5huYKudSHuZaq4hF-AociWPQ@mail.gmail.com>
Message-ID: <CAGxFJbSGqniB1BmALa1z_CDa7o3be8j-GOf8fmTJ44LDimfZCg@mail.gmail.com>

?layout
Please read the Help file **carefully** and work through the **examples**.
I cannot explain better than they.
Here is code using layout() that I think does what you want:

m <- matrix(1:2, nrow =1)
layout(m, widths = c(1,2))
plot(1:10, type = "p",main = "The First Plot")
plot(10:1, type = "l", main ="The Second Plot")

Note that both the lattice package and ggplot2 can also do this sort of
thing much more flexibly(and therefore requiring more effort to learn).

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Dec 12, 2018 at 7:19 AM Luigi Marongiu <marongiu.luigi at gmail.com>
wrote:

> Dear all,
> I would like to draw two plots in the same device so that there is a
> single row and two columns, with the first column being 1/3 of the
> device's width.
> I am creating a PNG object with width = 30 and height = 20 cm.
> I know that I should use split.screen or layout but I am lost with the
> matrix to pass to the functions.
> For istance, I tried:
> # distance in arbitrary units (so let's say cm) from of corners
> # left, right, bottom, and top counting from bottom left corner
> # that is first panel has the bottom right corner 20 cm from the bottom
> left?
> > m = matrix(c(0,20,40,0, 20,60,40,0), byrow=T, ncol=4)
> > m
>      [,1] [,2] [,3] [,4]
> [1,]    0   20   40    0
> [2,]   20   60   40    0
> > split.screen(m)
> Error in par(split.screens[[cur.screen]]) :
>   invalid value specified for graphical parameter "fig"
> > m[1,]
> [1]  0 20 40  0
> > split.screen(m[1,])
> Error in split.screen(m[1, ]) : 'figs' must specify at least one screen
>
> What should be the syntax for this task?
>
> --
> Best regards,
> Luigi
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bret@chr @ending from x@4@ll@nl  Wed Dec 12 17:45:31 2018
From: bret@chr @ending from x@4@ll@nl (Franklin Bretschneider)
Date: Wed, 12 Dec 2018 17:45:31 +0100
Subject: [R] R plot split screen in uneven panels
In-Reply-To: <CAMk+s2RX94-=HQ7XjzxO7_3dub5huYKudSHuZaq4hF-AociWPQ@mail.gmail.com>
References: <CAMk+s2RX94-=HQ7XjzxO7_3dub5huYKudSHuZaq4hF-AociWPQ@mail.gmail.com>
Message-ID: <0C6EAE89-B329-4F1D-9F7A-39F972E073DE@xs4all.nl>

Dear Luigi Marongiu,


Re: 

> Dear all,
> I would like to draw two plots in the same device so that there is a
> single row and two columns, with the first column being 1/3 of the
> device's width.
> I am creating a PNG object with width = 30 and height = 20 cm.
> I know that I should use split.screen or layout but I am lost with the
> matrix to pass to the functions.
> For istance, I tried:
> # distance in arbitrary units (so let's say cm) from of corners
> # left, right, bottom, and top counting from bottom left corner
> # that is first panel has the bottom right corner 20 cm from the bottom left?
>> m = matrix(c(0,20,40,0, 20,60,40,0), byrow=T, ncol=4)
>> m
>     [,1] [,2] [,3] [,4]
> [1,]    0   20   40    0
> [2,]   20   60   40    0
>> split.screen(m)
> Error in par(split.screens[[cur.screen]]) :
>  invalid value specified for graphical parameter "fig"
>> m[1,]
> [1]  0 20 40  0
>> split.screen(m[1,])
> Error in split.screen(m[1, ]) : 'figs' must specify at least one screen
> 
> What should be the syntax for this task?
> 
> -- 
> Best regards,
> Luigi
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Screen parrts shoeld be fractions (0 ...1), not percentages.
And bottom first, so ...

m = matrix(c(0,20,0,40, 20,60,0,40)/100, byrow=T, ncol=4)

 ... will work.

Success and
Best regards,

Franklin
---

Franklin Bretschneider
Dept of Biology
Utrecht University
bretschr at xs4all.nl


From bgunter@4567 @ending from gm@il@com  Wed Dec 12 17:51:04 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Wed, 12 Dec 2018 08:51:04 -0800
Subject: [R] R plot split screen in uneven panels
In-Reply-To: <CAGxFJbSGqniB1BmALa1z_CDa7o3be8j-GOf8fmTJ44LDimfZCg@mail.gmail.com>
References: <CAMk+s2RX94-=HQ7XjzxO7_3dub5huYKudSHuZaq4hF-AociWPQ@mail.gmail.com>
 <CAGxFJbSGqniB1BmALa1z_CDa7o3be8j-GOf8fmTJ44LDimfZCg@mail.gmail.com>
Message-ID: <CAGxFJbRcL36E02LwrPezH_CSwyLwaZCC2R6bDKArRjCvf4bsYg@mail.gmail.com>

Incidentally, here is another way to do what (I think) you asked using
layout():

m <- matrix(c(1,2,2), nrow =1)
layout(m)
plot(1:10, type = "p",main = "The First Plot")
plot(10:1, type = "l", main ="The Second Plot")

On my device, the plots use different size fonts, point sizes, etc. and so
aesthetically differ. I do not know why and am too lazy to delve into the
code.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Dec 12, 2018 at 8:39 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> ?layout
> Please read the Help file **carefully** and work through the **examples**.
> I cannot explain better than they.
> Here is code using layout() that I think does what you want:
>
> m <- matrix(1:2, nrow =1)
> layout(m, widths = c(1,2))
> plot(1:10, type = "p",main = "The First Plot")
> plot(10:1, type = "l", main ="The Second Plot")
>
> Note that both the lattice package and ggplot2 can also do this sort of
> thing much more flexibly(and therefore requiring more effort to learn).
>
> Cheers,
> Bert
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Wed, Dec 12, 2018 at 7:19 AM Luigi Marongiu <marongiu.luigi at gmail.com>
> wrote:
>
>> Dear all,
>> I would like to draw two plots in the same device so that there is a
>> single row and two columns, with the first column being 1/3 of the
>> device's width.
>> I am creating a PNG object with width = 30 and height = 20 cm.
>> I know that I should use split.screen or layout but I am lost with the
>> matrix to pass to the functions.
>> For istance, I tried:
>> # distance in arbitrary units (so let's say cm) from of corners
>> # left, right, bottom, and top counting from bottom left corner
>> # that is first panel has the bottom right corner 20 cm from the bottom
>> left?
>> > m = matrix(c(0,20,40,0, 20,60,40,0), byrow=T, ncol=4)
>> > m
>>      [,1] [,2] [,3] [,4]
>> [1,]    0   20   40    0
>> [2,]   20   60   40    0
>> > split.screen(m)
>> Error in par(split.screens[[cur.screen]]) :
>>   invalid value specified for graphical parameter "fig"
>> > m[1,]
>> [1]  0 20 40  0
>> > split.screen(m[1,])
>> Error in split.screen(m[1, ]) : 'figs' must specify at least one screen
>>
>> What should be the syntax for this task?
>>
>> --
>> Best regards,
>> Luigi
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From murdoch@dunc@n @ending from gm@il@com  Wed Dec 12 19:42:32 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Wed, 12 Dec 2018 13:42:32 -0500
Subject: [R] Width of a text
In-Reply-To: <CA+dpOJ==ejgiVLuitA1kUPeGBzhC-h3MrxkYnXDU8NdJhQ43Ag@mail.gmail.com>
References: <CA+dpOJ==ejgiVLuitA1kUPeGBzhC-h3MrxkYnXDU8NdJhQ43Ag@mail.gmail.com>
Message-ID: <ad051864-0c67-8cd0-8826-19330b382cdb@gmail.com>

On 12/12/2018 6:55 AM, Christofer Bogaso wrote:
> Hi,
> 
> In HTML there is a way to measure the width of a Text before printing it on
> screen as in https://www.w3schools.com/tags/canvas_measuretext.asp
> 
> In R we have nchar() function which just measures the number of letters in
> a Text, but I wonder if we can measure the width of text as well.
> 
> I have a shiny app where I need to print a text in an Area onto the screen
> which has a specific width, so some the texts are not showing within that
> area (overflowing), where some are though all of them have the equal number
> of letters.

Since Shiny displays in a web browser, it will be up to the web browser 
to measure the text.  (How would R know the font, magnification, etc?) 
You'll have to do this in Javascript.

If your R code needs to know the answer, Shiny provides a way to send 
the result back to R.  It's not trivial, but you can read how to do it 
here:  <https://shiny.rstudio.com/articles/js-send-message.html>.

Duncan Murdoch

> 
> Thanks for any input.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From h@@@n@diw@n @ending from gm@il@com  Wed Dec 12 20:21:07 2018
From: h@@@n@diw@n @ending from gm@il@com (Hasan Diwan)
Date: Wed, 12 Dec 2018 11:21:07 -0800
Subject: [R] R and factorytalk historian
In-Reply-To: <a696a745489c450b90529df2fabfb722@SRVEXCHCM1302.precheza.cz>
References: <f104645d3ae54c25b4ebe038f2123d43@SRVEXCHCM1302.precheza.cz>
 <d960f025e1b1485c93f875eab7749cce@UM-MAIL3214.unimaas.nl>
 <a696a745489c450b90529df2fabfb722@SRVEXCHCM1302.precheza.cz>
Message-ID: <CAP+bYWB8=q4yoUogyZyzBQB422aVg2FFVp3TQ7jcNnTb6wdH2w@mail.gmail.com>

Could you not script Excel to export automatically to CSV --
https://stackoverflow.com/a/10803229/783412, for example -- and import the
result into R? -- H

On Wed, 12 Dec 2018 at 07:17, PIKAL Petr <petr.pikal at precheza.cz> wrote:

> Hi
>
> Well, the final answer is that data from FTH could be transfered to other
> software **only** through Excel.
>
> Regarding RetroEncabulator, readings could be enahanced by extensive and
> elaborate use of nanoputian molecules chained together.
>
> Cheers
> Petr
>
> > -----Original Message-----
> > From: Viechtbauer, Wolfgang (SP)
> > <wolfgang.viechtbauer at maastrichtuniversity.nl>
> > Sent: Wednesday, December 12, 2018 11:21 AM
> > To: PIKAL Petr <petr.pikal at precheza.cz>; r-help <r-help at r-project.org>
> > Subject: RE: R and factorytalk historian
> >
> > Dear Petr,
> >
> > Sorry, no experience with the FTH, but related to this, has anybody
> gotten R to
> > interface nicely with the Retro Encabulator, providing live read outs of
> the
> > synchronizing cardinal grammeters?
> >
> > (my apologies, I just couldn't resist)
> >
> > Best,
> > Wolfgang
> >
> > >-----Original Message-----
> > >From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of PIKAL
> > >Petr
> > >Sent: Tuesday, 11 December, 2018 16:09
> > >To: r-help
> > >Subject: [R] R and factorytalk historian
> > >
> > >Hallo all
> > >
> > >Does anybody know if R could be used directly with FactoryTalk
> > >Historian programme from Rockwell automation.
> > >
> > >It is probably possible to use Excel as interface but I would prefer
> > >not to.
> > >
> > >Best regards.
> > >
> > >Petr Pikal
> Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch
> partner? PRECHEZA a.s. jsou zve?ejn?ny na:
> https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information
> about processing and protection of business partner?s personal data are
> available on website:
> https://www.precheza.cz/en/personal-data-protection-principles/
> D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou
> d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en?
> odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any
> documents attached to it may be confidential and are subject to the legally
> binding disclaimer: https://www.precheza.cz/en/01-disclaimer/
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
OpenPGP:
https://sks-keyservers.net/pks/lookup?op=get&search=0xFEBAD7FFD041BBA1
If you wish to request my time, please do so using
*bit.ly/hd1AppointmentRequest
<http://bit.ly/hd1AppointmentRequest>*.
Si vous voudrais faire connnaisance, allez a *bit.ly/hd1AppointmentRequest
<http://bit.ly/hd1AppointmentRequest>*.

<https://sks-keyservers.net/pks/lookup?op=get&search=0xFEBAD7FFD041BBA1>Sent
from my mobile device
Envoye de mon portable

	[[alternative HTML version deleted]]


From petr@pik@l @ending from prechez@@cz  Thu Dec 13 09:01:56 2018
From: petr@pik@l @ending from prechez@@cz (PIKAL Petr)
Date: Thu, 13 Dec 2018 08:01:56 +0000
Subject: [R] R and factorytalk historian
In-Reply-To: <CAP+bYWB8=q4yoUogyZyzBQB422aVg2FFVp3TQ7jcNnTb6wdH2w@mail.gmail.com>
References: <f104645d3ae54c25b4ebe038f2123d43@SRVEXCHCM1302.precheza.cz>
 <d960f025e1b1485c93f875eab7749cce@UM-MAIL3214.unimaas.nl>
 <a696a745489c450b90529df2fabfb722@SRVEXCHCM1302.precheza.cz>
 <CAP+bYWB8=q4yoUogyZyzBQB422aVg2FFVp3TQ7jcNnTb6wdH2w@mail.gmail.com>
Message-ID: <06c39e5a7bd0499a9937b0a267683d0b@SRVEXCHCM1302.precheza.cz>

Hi

Yes, there should not be any problem with transfering data from Excel to R.

However my trust to Excel is not big, therefore I wanted to avoid such unnecessary intermediate step. But according to FTH lecturers they do not know about any option for direct export to csv or other suitable data format.

It seems that nobody has experience with direct connection FTH to R, so I would like to thank all for their answers and I would consider this topic closed.

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Hasan Diwan
> Sent: Wednesday, December 12, 2018 8:21 PM
> To: R Project Help <r-help at r-project.org>
> Subject: Re: [R] R and factorytalk historian
>
> Could you not script Excel to export automatically to CSV --
> https://stackoverflow.com/a/10803229/783412, for example -- and import the
> result into R? -- H
>
> On Wed, 12 Dec 2018 at 07:17, PIKAL Petr <petr.pikal at precheza.cz> wrote:
>
> > Hi
> >
> > Well, the final answer is that data from FTH could be transfered to
> > other software **only** through Excel.
> >
> > Regarding RetroEncabulator, readings could be enahanced by extensive
> > and elaborate use of nanoputian molecules chained together.
> >
> > Cheers
> > Petr
> >
> > > -----Original Message-----
> > > From: Viechtbauer, Wolfgang (SP)
> > > <wolfgang.viechtbauer at maastrichtuniversity.nl>
> > > Sent: Wednesday, December 12, 2018 11:21 AM
> > > To: PIKAL Petr <petr.pikal at precheza.cz>; r-help
> > > <r-help at r-project.org>
> > > Subject: RE: R and factorytalk historian
> > >
> > > Dear Petr,
> > >
> > > Sorry, no experience with the FTH, but related to this, has anybody
> > gotten R to
> > > interface nicely with the Retro Encabulator, providing live read
> > > outs of
> > the
> > > synchronizing cardinal grammeters?
> > >
> > > (my apologies, I just couldn't resist)
> > >
> > > Best,
> > > Wolfgang
> > >
> > > >-----Original Message-----
> > > >From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> > > >PIKAL Petr
> > > >Sent: Tuesday, 11 December, 2018 16:09
> > > >To: r-help
> > > >Subject: [R] R and factorytalk historian
> > > >
> > > >Hallo all
> > > >
> > > >Does anybody know if R could be used directly with FactoryTalk
> > > >Historian programme from Rockwell automation.
> > > >
> > > >It is probably possible to use Excel as interface but I would
> > > >prefer not to.
> > > >
> > > >Best regards.
> > > >
> > > >Petr Pikal
> > Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj?
> > obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na:
> > https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information
> > about processing and protection of business partner?s personal data
> > are available on website:
> > https://www.precheza.cz/en/personal-data-protection-principles/
> > D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou
> > d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en?
> > odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any
> > documents attached to it may be confidential and are subject to the
> > legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> --
> OpenPGP:
> https://sks-keyservers.net/pks/lookup?op=get&search=0xFEBAD7FFD041BBA1
> If you wish to request my time, please do so using
> *bit.ly/hd1AppointmentRequest <http://bit.ly/hd1AppointmentRequest>*.
> Si vous voudrais faire connnaisance, allez a *bit.ly/hd1AppointmentRequest
> <http://bit.ly/hd1AppointmentRequest>*.
>
> <https://sks-
> keyservers.net/pks/lookup?op=get&search=0xFEBAD7FFD041BBA1>Sent
> from my mobile device
> Envoye de mon portable
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


From wewol@ki @ending from gm@il@com  Thu Dec 13 10:52:38 2018
From: wewol@ki @ending from gm@il@com (Witold E Wolski)
Date: Thu, 13 Dec 2018 10:52:38 +0100
Subject: [R] Visualizing contrasts for lmer models
Message-ID: <CAAjnpdiFi_Hq5BtCGfaXTDZh7XCsRVC3qp32B4RxLPfS0bTVbQ@mail.gmail.com>

Hello,

Is there an R-package which implements visualizations of estimated
coefficients and the data for lmer models similar to those shown here:

http://genomicsclass.github.io/book/pages/interactions_and_contrasts.html
in sections:

Examining the estimated coefficients
(ideally with ggplot)

???

I find these visualizations (especially those for contrasts) very
helpful but despite R -package searches I failed to find a generic
implementation.

best regards
Witek

Witold Eryk Wolski


From m@echler @ending from @t@t@m@th@ethz@ch  Thu Dec 13 11:14:12 2018
From: m@echler @ending from @t@t@m@th@ethz@ch (Martin Maechler)
Date: Thu, 13 Dec 2018 11:14:12 +0100
Subject: [R] R plot split screen in uneven panels
In-Reply-To: <CAGxFJbRcL36E02LwrPezH_CSwyLwaZCC2R6bDKArRjCvf4bsYg@mail.gmail.com>
References: <CAMk+s2RX94-=HQ7XjzxO7_3dub5huYKudSHuZaq4hF-AociWPQ@mail.gmail.com>
 <CAGxFJbSGqniB1BmALa1z_CDa7o3be8j-GOf8fmTJ44LDimfZCg@mail.gmail.com>
 <CAGxFJbRcL36E02LwrPezH_CSwyLwaZCC2R6bDKArRjCvf4bsYg@mail.gmail.com>
Message-ID: <23570.12532.531977.36978@stat.math.ethz.ch>

>>>>> Bert Gunter 
>>>>>     on Wed, 12 Dec 2018 08:51:04 -0800 writes:

    > Incidentally, here is another way to do what (I think) you asked using
    > layout():

    > m <- matrix(c(1,2,2), nrow =1)
    > layout(m)
    > plot(1:10, type = "p", main ="The First Plot")
    > plot(10:1, type = "l", main ="The Second Plot")

    > On my device, the plots use different size fonts, point sizes, etc. and so
    > aesthetically differ. 

Really? -- Not at all for me [Linux default device =
X11(type="cairo")], where all these are identical on the left
and the right plot.

Such behavior seems like a bogous graphics device  or
bogous interaction with underlying libraries or ??

    > I do not know why and am too lazy to delve into the code.
    > Bert Gunter

Can you at least tell us a bit more, e.g.

  dev.capabilities()
?

Best, Martin


From m@rongiu@luigi @ending from gm@il@com  Thu Dec 13 13:44:24 2018
From: m@rongiu@luigi @ending from gm@il@com (Luigi Marongiu)
Date: Thu, 13 Dec 2018 13:44:24 +0100
Subject: [R] R plot split screen in uneven panels
In-Reply-To: <CAGxFJbRcL36E02LwrPezH_CSwyLwaZCC2R6bDKArRjCvf4bsYg@mail.gmail.com>
References: <CAMk+s2RX94-=HQ7XjzxO7_3dub5huYKudSHuZaq4hF-AociWPQ@mail.gmail.com>
 <CAGxFJbSGqniB1BmALa1z_CDa7o3be8j-GOf8fmTJ44LDimfZCg@mail.gmail.com>
 <CAGxFJbRcL36E02LwrPezH_CSwyLwaZCC2R6bDKArRjCvf4bsYg@mail.gmail.com>
Message-ID: <CAMk+s2TK10uwumeCm52sK6Wn-06eg7vQg_tsb2pWFjv0yhtD8Q@mail.gmail.com>

Thank you, that worked good. I tried to read the help for
layout/split.screen but I found it confusing.
On Wed, Dec 12, 2018 at 5:51 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
> Incidentally, here is another way to do what (I think) you asked using layout():
>
> m <- matrix(c(1,2,2), nrow =1)
> layout(m)
> plot(1:10, type = "p",main = "The First Plot")
> plot(10:1, type = "l", main ="The Second Plot")
>
> On my device, the plots use different size fonts, point sizes, etc. and so aesthetically differ. I do not know why and am too lazy to delve into the code.
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Wed, Dec 12, 2018 at 8:39 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>
>> ?layout
>> Please read the Help file **carefully** and work through the **examples**. I cannot explain better than they.
>> Here is code using layout() that I think does what you want:
>>
>> m <- matrix(1:2, nrow =1)
>> layout(m, widths = c(1,2))
>> plot(1:10, type = "p",main = "The First Plot")
>> plot(10:1, type = "l", main ="The Second Plot")
>>
>> Note that both the lattice package and ggplot2 can also do this sort of thing much more flexibly(and therefore requiring more effort to learn).
>>
>> Cheers,
>> Bert
>>
>>
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming along and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>> On Wed, Dec 12, 2018 at 7:19 AM Luigi Marongiu <marongiu.luigi at gmail.com> wrote:
>>>
>>> Dear all,
>>> I would like to draw two plots in the same device so that there is a
>>> single row and two columns, with the first column being 1/3 of the
>>> device's width.
>>> I am creating a PNG object with width = 30 and height = 20 cm.
>>> I know that I should use split.screen or layout but I am lost with the
>>> matrix to pass to the functions.
>>> For istance, I tried:
>>> # distance in arbitrary units (so let's say cm) from of corners
>>> # left, right, bottom, and top counting from bottom left corner
>>> # that is first panel has the bottom right corner 20 cm from the bottom left?
>>> > m = matrix(c(0,20,40,0, 20,60,40,0), byrow=T, ncol=4)
>>> > m
>>>      [,1] [,2] [,3] [,4]
>>> [1,]    0   20   40    0
>>> [2,]   20   60   40    0
>>> > split.screen(m)
>>> Error in par(split.screens[[cur.screen]]) :
>>>   invalid value specified for graphical parameter "fig"
>>> > m[1,]
>>> [1]  0 20 40  0
>>> > split.screen(m[1,])
>>> Error in split.screen(m[1, ]) : 'figs' must specify at least one screen
>>>
>>> What should be the syntax for this task?
>>>
>>> --
>>> Best regards,
>>> Luigi
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.



-- 
Best regards,
Luigi


From @pbr@ckett20 @ending from @@intjo@ephh@@com  Thu Dec 13 14:00:19 2018
From: @pbr@ckett20 @ending from @@intjo@ephh@@com (Spencer Brackett)
Date: Thu, 13 Dec 2018 08:00:19 -0500
Subject: [R] In need of TCGA Workflow to identify portion expression
Message-ID: <CAPQaxLOQHrE_5WXnRK468pXVHtigtDWOEMkNcQqKCfnqF=po_A@mail.gmail.com>

Good morning,

  I am looking for a workflow compatible with a TCGA file, and one that can
essentially identify the protein expression involved with TMZ-induced MGMT
methylation. Any advice or information that could lead me to finding such a
workflow would be greatly appreciated!

Best,

Spencer Brackett

	[[alternative HTML version deleted]]


From @@r@h@go@lee @ending from gm@il@com  Thu Dec 13 15:00:09 2018
From: @@r@h@go@lee @ending from gm@il@com (Sarah Goslee)
Date: Thu, 13 Dec 2018 09:00:09 -0500
Subject: [R] In need of TCGA Workflow to identify portion expression
In-Reply-To: <CAPQaxLOQHrE_5WXnRK468pXVHtigtDWOEMkNcQqKCfnqF=po_A@mail.gmail.com>
References: <CAPQaxLOQHrE_5WXnRK468pXVHtigtDWOEMkNcQqKCfnqF=po_A@mail.gmail.com>
Message-ID: <CAM_vjunoc2zUVW9ziR0EbuBw4tNri+F9d43qmz+kXBuS-V067Q@mail.gmail.com>

Spencer, you will find far more knowledgeable help on the Bioconductor
mailing list.

But here's a starting poing:
https://www.bioconductor.org/packages/release/workflows/html/TCGAWorkflow.html

Sarah
On Thu, Dec 13, 2018 at 8:01 AM Spencer Brackett
<spbrackett20 at saintjosephhs.com> wrote:
>
> Good morning,
>
>   I am looking for a workflow compatible with a TCGA file, and one that can
> essentially identify the protein expression involved with TMZ-induced MGMT
> methylation. Any advice or information that could lead me to finding such a
> workflow would be greatly appreciated!
>
> Best,
>
> Spencer Brackett
>

-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From @h@wnmille @ending from p@@gov  Thu Dec 13 18:28:37 2018
From: @h@wnmille @ending from p@@gov (Miller, Shawn)
Date: Thu, 13 Dec 2018 17:28:37 +0000
Subject: [R] help
Message-ID: <BL0PR0901MB3218E057E4CEEF92FCC3DBADB9A00@BL0PR0901MB3218.namprd09.prod.outlook.com>

Need help with R studio. Code is to pull data from todays date plus 5 years 4 months from now. I am missing the last 3 months of data. Can you please help?

Shawn Miller | Aquatic Biologist II | Assessment Section
Environmental Protection | Clean Water
Rachel Carson State Office Building
400 Market Street | Harrisburg, PA 17101
Phone: 717.772.2185 | Fax: 717.772.3249
www.depweb.state.pa.us<https://webmail.state.pa.us/OWA/redir.aspx?C=t4jGxr3_mkC5mWY30vM0D8N-9RdJ8s9IgIGFizoEzsd1aNOJaDwGjjpEh4RqLFX24CIJXV9M2ic.&URL=http%3a%2f%2fwww.depweb.state.pa.us%2f>


	[[alternative HTML version deleted]]


From dc@rl@on @ending from t@mu@edu  Thu Dec 13 20:12:24 2018
From: dc@rl@on @ending from t@mu@edu (David L Carlson)
Date: Thu, 13 Dec 2018 19:12:24 +0000
Subject: [R] help
In-Reply-To: <BL0PR0901MB3218E057E4CEEF92FCC3DBADB9A00@BL0PR0901MB3218.namprd09.prod.outlook.com>
References: <BL0PR0901MB3218E057E4CEEF92FCC3DBADB9A00@BL0PR0901MB3218.namprd09.prod.outlook.com>
Message-ID: <ce8dd23c4c2644f0a496582fe1c9a1a3@tamu.edu>

You need Santa Claus not r-help. You haven't given us a fraction of the information we would need to help. You don't show us your code. You don't tell us where the information is coming from except "today's date." You don't tell us what data you want. You don't seem to know the difference between R and R-Studio.

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352


-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Miller, Shawn
Sent: Thursday, December 13, 2018 11:29 AM
To: R-help at r-project.org
Subject: [R] help

Need help with R studio. Code is to pull data from todays date plus 5 years 4 months from now. I am missing the last 3 months of data. Can you please help?

Shawn Miller | Aquatic Biologist II | Assessment Section
Environmental Protection | Clean Water
Rachel Carson State Office Building
400 Market Street | Harrisburg, PA 17101
Phone: 717.772.2185 | Fax: 717.772.3249
www.depweb.state.pa.us<https://webmail.state.pa.us/OWA/redir.aspx?C=t4jGxr3_mkC5mWY30vM0D8N-9RdJ8s9IgIGFizoEzsd1aNOJaDwGjjpEh4RqLFX24CIJXV9M2ic.&URL=http%3a%2f%2fwww.depweb.state.pa.us%2f>


	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dc@rl@on @ending from t@mu@edu  Thu Dec 13 20:32:38 2018
From: dc@rl@on @ending from t@mu@edu (David L Carlson)
Date: Thu, 13 Dec 2018 19:32:38 +0000
Subject: [R] [External] RE: help
In-Reply-To: <BL0PR0901MB3218831CD4CEF3768663214BB9A00@BL0PR0901MB3218.namprd09.prod.outlook.com>
References: <BL0PR0901MB3218E057E4CEEF92FCC3DBADB9A00@BL0PR0901MB3218.namprd09.prod.outlook.com>
 <ce8dd23c4c2644f0a496582fe1c9a1a3@tamu.edu>
 <BL0PR0901MB3218831CD4CEF3768663214BB9A00@BL0PR0901MB3218.namprd09.prod.outlook.com>
Message-ID: <e9b83dfd00fe4e81894b5b5741809fb0@tamu.edu>

Use Reply-All to keep the thread on the list.

If the most recent WQN SIS Data Download table is 14June2018_WQN_Data_Download, does it contain any data for the months July, August, September, October, November, December? After extracting the data you create the file "Anti-deg_requests/test.xlsx". Have you confirmed that all of the dates you need are present in that file?

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352

-----Original Message-----
From: Miller, Shawn <shawnmille at pa.gov> 
Sent: Thursday, December 13, 2018 1:15 PM
To: David L Carlson <dcarlson at tamu.edu>
Subject: RE: [External] RE: help

setwd("c:/users/shawnmille/Desktop/Anti-deg_requests/")

library(plyr)
library(reshape2)
library(RODBC)
library(dplyr)



## MAKE SURE YOU ARE USING THE 32-bit VERSION OF R (See Tools>Global Options>General in RStudio to set this)
## READING DATA FROM EXISTING DATABASES (EXAMPLE: ACCESS 2010)
chan <- odbcConnectAccess("WQN Datadumps.mdb")

# Show list of tables in database
sqlTables(chan,tableType='TABLE')
# Fetch the most recent WQN SIS Data Download table, this takes a couple minutes
allwqn <- sqlFetch(chan,'14June2018_WQN_Data_Download',stringsAsFactors=FALSE)
names(allwqn)
## subset to the desired fields and values, choose your WQN by modifying command

wqn.specific <- subset(allwqn,MONITORING_POINT_ALIAS_ID =='WQN0735',select=c('DATE_COLLECTED','MONITORING_POINT_ALIAS_ID','QUALITY_ASSURANCE_TYPE_DESC',
                                                                                                                  'TEST_CODE','TEST_SHORT_DESC','READING_INDICATOR_CODE','FINAL_AMOUNT','ABBREVIATION'))  

###VIEW ENTIRE DATASET, this coding can be used to view any created datasets in excel#####
library(xlsx)
write.xlsx(wqn.specific,"c:/users/shawnmille/Desktop/Anti-deg_requests/test.xlsx") 

###VIEW ENTIRE DATASET#####

# FOR ACTIVE WQN STATION GET TODAY'S DATE and date of 5 years ago + 4 months or put in last date in command#
today <- Sys.Date()
TODAY.5<-today-1949
## Select data where DATA_COLLECTED >= TODAY.5##
wqn.specific$DATE_COLLECTED <- as.Date(wqn.specific$DATE_COLLECTED ,"%m/%d/%y")
wqn.last5yrs <- subset(wqn.specific,DATE_COLLECTED >= TODAY.5, select=c('DATE_COLLECTED','MONITORING_POINT_ALIAS_ID','QUALITY_ASSURANCE_TYPE_DESC','TEST_CODE','TEST_SHORT_DESC','READING_INDICATOR_CODE','FINAL_AMOUNT','ABBREVIATION'))


#OR FOR INACTIVE WQN STATION ENTER THE LAST DATE OF RECORD BELOW AND GET PAST 5 YEARS OF DATA IN SUBSET#
lastdate <- as.Date("11/01/2010", format = "%m/%d/%Y")
firstdate <- lastdate-1949
wqn.specific$DATE_COLLECTED <- as.Date(wqn.specific$DATE_COLLECTED ,"%m/%d/%y")
wqn.last5yrs <- subset(wqn.specific,DATE_COLLECTED >= firstdate & DATE_COLLECTED <= lastdate, select=c('DATE_COLLECTED','MONITORING_POINT_ALIAS_ID','QUALITY_ASSURANCE_TYPE_DESC','TEST_CODE','TEST_SHORT_DESC','READING_INDICATOR_CODE','FINAL_AMOUNT','ABBREVIATION'))

#### Delete Blanks and QA column #####
wqn.removeblanks <-subset(wqn.last5yrs,QUALITY_ASSURANCE_TYPE_DESC=="Duplicate"|is.na(QUALITY_ASSURANCE_TYPE_DESC),select=c('DATE_COLLECTED','MONITORING_POINT_ALIAS_ID',
                                                                                                                            'TEST_CODE','TEST_SHORT_DESC','READING_INDICATOR_CODE','FINAL_AMOUNT','ABBREVIATION')) 
#### Delete Null Results####
wqn.removenull <- subset(wqn.removeblanks,FINAL_AMOUNT!="NA",select=c('DATE_COLLECTED','MONITORING_POINT_ALIAS_ID','TEST_CODE','TEST_SHORT_DESC','READING_INDICATOR_CODE','FINAL_AMOUNT','ABBREVIATION')) 

### Remove non-relevant parameters ####

wqn.removeparm <- subset(wqn.removenull,TEST_CODE!= "571"&TEST_CODE!="572"&
                           TEST_CODE!="573"& TEST_CODE!="543"& TEST_CODE!="561"& TEST_CODE!="546"
                         & TEST_CODE!="595"& TEST_CODE!="596"& TEST_CODE!="597"& TEST_CODE!="598"
                         & TEST_CODE!="599"& TEST_CODE!="600"& TEST_CODE!="71946"& TEST_CODE!="71940"
                         & TEST_CODE!="587"& TEST_CODE!="593"& TEST_CODE!="71939"& TEST_CODE!="71937"
                         & TEST_CODE!="574"& TEST_CODE!="549"& TEST_CODE!="99014"& TEST_CODE!="547"
                         & TEST_CODE!="298"& TEST_CODE!="551"& TEST_CODE!="71930"& TEST_CODE!="70508"
                         & TEST_CODE!="564"& TEST_CODE!="709"& TEST_CODE!="111"& TEST_CODE!="592"
                         & TEST_CODE!="MMTECMF"& TEST_CODE!="588"& TEST_CODE!="589"& TEST_CODE!="590"
                         & TEST_CODE!="594"& TEST_CODE!="71936"& TEST_CODE!="71945"& TEST_CODE!="F00061"
                         & TEST_CODE!="71947"& TEST_CODE!="555",
                         select=c('DATE_COLLECTED','MONITORING_POINT_ALIAS_ID','TEST_CODE','TEST_SHORT_DESC','READING_INDICATOR_CODE','FINAL_AMOUNT','ABBREVIATION'))

###  Combine test codes that are measuring the same analyte, format testcode to include first 5 digits###


wqn.removeparm$TEST_CODE<-substr(wqn.removeparm$TEST_CODE,1,5)


######Creates a table (wqn.det) with list of max detection limits for each parameter,FOR EACH TEST_CODE BRING ALL  < TO THAT VALUE############

n.det <- subset(wqn.removeparm,READING_INDICATOR_CODE=='<')
wqn.det<- aggregate(n.det$FINAL_AMOUNT , by=list(n.det$MONITORING_POINT_ALIAS_ID,n.det$TEST_CODE),FUN=max)
colnames(wqn.det) <- c("MONITORING_POINT_ALIAS_ID","TEST_CODE","DET_LIMIT")

wqn.mer<-merge(wqn.removeparm,wqn.det, by=c("MONITORING_POINT_ALIAS_ID","TEST_CODE"),all=TRUE)
wqn.mer$Amount<- ifelse(is.na(wqn.mer$'READING_INDICATOR_CODE'),wqn.mer$FINAL_AMOUNT*1,wqn.mer$DET_LIMIT*1)


######DIVIDE '<' BY HALF #####
wqn.mer$Result <- ifelse(is.na(wqn.mer$'READING_INDICATOR_CODE'),wqn.mer$Amount*1,wqn.mer$Amount/2)

###### AVERAGE AND GROUP BY TO REMOVE DUPLICATES ########

wqn.agg<-aggregate(wqn.mer$Result, by=list(MONITORING_POINT_ALIAS_ID=wqn.mer$'MONITORING_POINT_ALIAS_ID',DATE_COLLECTED=wqn.mer$'DATE_COLLECTED',TEST_CODE=wqn.mer$'TEST_CODE'),data=wqn.mer, FUN="mean",na.rm=TRUE)
colnames(wqn.agg) <- c("MONITORING_POINT_ALIAS_ID","DATE_COLLECTED","TEST_CODE","Result")

###### Calculate Median and 95% CI, alpha/2 for tailed2 (pH)..similar to ci.median function in asbio package, using alpha for funtion "tailed1"(other parameters)###


tailed2<-function(x){
  n <- nrow(as.matrix(x))
  L <- qbinom(.025,n,0.5)
  U <- n-L+1
  order.x <-sort(x)
  lower <- (order.x[L])
  upper <-order.x[n - 
                    L + 1]
  median <- median(x)
  coverage <- 1-(2*pbinom(qbinom(.025,n,.5)-1,n,0.5))
  y<-list(c(lower, median, upper,n,coverage))

  
  
  
  
  
} 
tailed1<-function(x){
  n <- nrow(as.matrix(x))
  L <- qbinom(.05,n,0.5)
  U <- n-L+1
  order.x <-sort(x)
  lower <- (order.x[L])
  upper <-order.x[n - 
                    L+1]
  median <- median(x)
  coverage <- 1-(2*pbinom(qbinom(.05,n,.5)-1,n,0.5))
  y<-list(c(lower, median, upper,n,coverage))
  
  
  
  
} 


###Create two data sets, one with all the parameters except pH and one with just pH#####
wqn.ph <- subset(wqn.agg,TEST_CODE=='00403'|TEST_CODE=="F0040",select=c("MONITORING_POINT_ALIAS_ID","DATE_COLLECTED","TEST_CODE","Result"))
wqn.noph <-subset(wqn.agg,TEST_CODE!='00403'& TEST_CODE!="F0040",select=c("MONITORING_POINT_ALIAS_ID","DATE_COLLECTED","TEST_CODE","Result"))                  
                   
###Run tailed1 function on all data except for pH###                   
                   
stats<-aggregate(wqn.noph$Result, by=list(wqn.noph$MONITORING_POINT_ALIAS_ID,wqn.noph$TEST_CODE), FUN=tailed1)
colnames(stats) <- c("MONITORING_POINT_ALIAS_ID","TEST_CODE","x")

stats$lower <-unlist(lapply(stats$x, '[[', 1))
stats$median<-unlist(lapply(stats$x, '[[', 2))
stats$upper <-unlist(lapply(stats$x, '[[', 3))
stats$n <-unlist(lapply(stats$x, '[[', 4)) 
stats1 <- stats[order(stats$"MONITORING_POINT_ALIAS_ID",stats$"TEST_CODE"),] 

###Run tailed2 function on pH (lab and field) data####
ph.stats<-aggregate(wqn.ph$Result, by=list(wqn.ph$MONITORING_POINT_ALIAS_ID,wqn.ph$TEST_CODE), FUN=tailed2)
colnames(ph.stats) <- c("MONITORING_POINT_ALIAS_ID","TEST_CODE","x")

ph.stats$lower <-unlist(lapply(ph.stats$x, '[[', 1))
ph.stats$median<-unlist(lapply(ph.stats$x, '[[', 2))
ph.stats$upper <-unlist(lapply(ph.stats$x, '[[', 3))
ph.stats$n <-unlist(lapply(ph.stats$x, '[[', 4)) 
ph.stats1 <- ph.stats[order(ph.stats$"MONITORING_POINT_ALIAS_ID",ph.stats$"TEST_CODE"),] 

###Combine datasets vertically###
stats2 <- rbind(stats1,ph.stats1)


####PeriodofRecordTable####

library(dplyr)

my.dt<-(format(as.Date(wqn.removeparm$"DATE_COLLECTED"),"%m/%d/%Y"))

my.dt1<-aggregate(my.dt, by=list(wqn.removeparm$MONITORING_POINT_ALIAS_ID,wqn.removeparm$TEST_CODE), FUN=first)
colnames(my.dt1) <- c("MONITORING_POINT_ALIAS_ID","TEST_CODE","FirstDate")

my.dtlast<-aggregate(my.dt, by=list(wqn.removeparm$MONITORING_POINT_ALIAS_ID,wqn.removeparm$TEST_CODE), FUN=last)
colnames(my.dtlast) <- c("MONITORING_POINT_ALIAS_ID","TEST_CODE","LastDate")



####Merging Tables Dates and Stats####################
all.dt <- merge(my.dt1,my.dtlast,by=c("MONITORING_POINT_ALIAS_ID","TEST_CODE"))

dt.stats<-merge(all.dt,stats2,by=c("MONITORING_POINT_ALIAS_ID","TEST_CODE"))


####Calculate Period Of record in Table, Old coding 2 lines: dt.stats$pdofrecord<-(mdy(dt.stats$"LastDate"))-(mdy(dt.stats$"FirstDate"))
###dt.stats$pd<-round(dt.stats$pdofrecord/31563000, digits=1), lubridate is glitchy test new code on this step more######


dt.stats$pd <- round((as.Date(dt.stats$LastDate,format = "%m/%d/%Y")-as.Date(dt.stats$FirstDate,format = "%m/%d/%Y"))/365,digits=1)



####Retrieve Test Descriptions and Merge######
t1 <- subset(dt.stats,select=c('MONITORING_POINT_ALIAS_ID','TEST_CODE','FirstDate','LastDate','pd','lower','median','upper','n'))
t2<-subset(wqn.removeparm,select=c('TEST_CODE','TEST_SHORT_DESC','ABBREVIATION'))
t3 <- unique( t2 )

t4<-merge(t1,t3, by='TEST_CODE',all.t3=TRUE)
######ReorderColumns,sort, rename columns#######
t5 <- t4[ ,c(2,1,10,3,4,5,9,6,7,8,11)]
t6 <- t5[order(t5$"MONITORING_POINT_ALIAS_ID",t5$"TEST_SHORT_DESC"),] 
names(t6)

####This does not need to be run. It produces an error. You still get the correct results.
library(plyr)
library(dplyr)
library(reshape2)
final.tab<-rename(t6, c("FirstDate"="FIRST_DATE", "LastDate"="LAST_DATE","pd"="PERIOD_OF_RECORD(yrs)","n"="SAMPLE_SIZE","lower"
             ="L_95_CI","median"="MEDIAN_","upper"="U_95_CI","ABBREVIATION"="UNITS"))

####BRING VALUES UP TO DETECTION LIMITS merge with wqn.det######
less<-function(x){
  sprintf("< %3.2f", x) 
} 

wqn.final <-merge(t6,wqn.det,by=c("MONITORING_POINT_ALIAS_ID","TEST_CODE"),all=TRUE)
wqn.final$DET_LIMIT[is.na(wqn.final$DET_LIMIT)] <- 0
wqn.final$LOW_95_CL <- ifelse(wqn.final$lower < wqn.final$DET_LIM,less(wqn.final$DET_LIM*1),wqn.final$lower*1)
wqn.final$MEDIAN <- ifelse(wqn.final$median < wqn.final$DET_LIM,less(wqn.final$DET_LIM*1),wqn.final$median*1)
wqn.final$UPP_95_CL <- ifelse(wqn.final$upper < wqn.final$DET_LIM,less(wqn.final$DET_LIM*1),wqn.final$upper*1)

####ONLY REPORT MINIMUM FOR ALKALINITY, PH, DO, TEMP, SPECIFIC cONDUCTANCE######

wqn.final$LOWER_95_CL <- ifelse(wqn.final$TEST_CODE=="00403"|wqn.final$TEST_CODE=="00410"|wqn.final$TEST_CODE=="F0040"|
                                    wqn.final$TEST_CODE=="F0030"|wqn.final$TEST_CODE=="F0043",wqn.final$LOW_95_CL,"NA")

wqn.final$UPPER_95_CL <- ifelse(wqn.final$TEST_CODE=="00410"|wqn.final$TEST_CODE=="F0043"|
                                  wqn.final$TEST_CODE=="F0030","NA",wqn.final$UPP_95_CL)


names(wqn.final)                                  
final.1<-subset(wqn.final, select=c('MONITORING_POINT_ALIAS_ID','TEST_CODE','TEST_SHORT_DESC','FirstDate',
                                    'LastDate', 'pd',"n","LOWER_95_CL",'MEDIAN',
                                    'UPPER_95_CL','ABBREVIATION'))                                    
final.2 <- final.1[order(final.1$"MONITORING_POINT_ALIAS_ID",final.1$"TEST_SHORT_DESC"),]                                     

###VIEW IN EXCEL##
library(xlsx)
write.xlsx(final.2,"c:/users/shawnmille/Desktop/Anti-deg_requests/final.2.xlsx") 


-----Original Message-----
From: David L Carlson [mailto:dcarlson at tamu.edu] 
Sent: Thursday, December 13, 2018 2:12 PM
To: Miller, Shawn <shawnmille at pa.gov>; R-help at r-project.org
Subject: [External] RE: help

ATTENTION: This email message is from an external sender. Do not open links or attachments from unknown sources. To report suspicious email, forward the message as an attachment to CWOPA_SPAM at pa.gov.

You need Santa Claus not r-help. You haven't given us a fraction of the information we would need to help. You don't show us your code. You don't tell us where the information is coming from except "today's date." You don't tell us what data you want. You don't seem to know the difference between R and R-Studio.

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352


-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Miller, Shawn
Sent: Thursday, December 13, 2018 11:29 AM
To: R-help at r-project.org
Subject: [R] help

Need help with R studio. Code is to pull data from todays date plus 5 years 4 months from now. I am missing the last 3 months of data. Can you please help?

Shawn Miller | Aquatic Biologist II | Assessment Section Environmental Protection | Clean Water Rachel Carson State Office Building
400 Market Street | Harrisburg, PA 17101
Phone: 717.772.2185 | Fax: 717.772.3249
https://na01.safelinks.protection.outlook.com/?url=www.depweb.state.pa.us&amp;data=02%7C01%7Cshawnmille%40pa.gov%7Cdfe052e5375647a1c32f08d6612eec13%7C418e284101284dd59b6c47fc5a9a1bde%7C1%7C1%7C636803251493279248&amp;sdata=ScL5NIrFc1A4g9wKVAYxhZ6Hrwnb7qvgjijh2WHEENY%3D&amp;reserved=0<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwebmail.state.pa.us%2FOWA%2Fredir.aspx%3FC%3Dt4jGxr3_mkC5mWY30vM0D8N-9RdJ8s9IgIGFizoEzsd1aNOJaDwGjjpEh4RqLFX24CIJXV9M2ic.%26URL%3Dhttp%253a%252f%252fwww.depweb.state.pa.us%252f&amp;data=02%7C01%7Cshawnmille%40pa.gov%7Cdfe052e5375647a1c32f08d6612eec13%7C418e284101284dd59b6c47fc5a9a1bde%7C1%7C1%7C636803251493279248&amp;sdata=H6VzxlE9wQCOszIoDOZXOoI5%2BZmsu2qVfP1Y%2BEU50SU%3D&amp;reserved=0>


        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=02%7C01%7Cshawnmille%40pa.gov%7Cdfe052e5375647a1c32f08d6612eec13%7C418e284101284dd59b6c47fc5a9a1bde%7C1%7C1%7C636803251493279248&amp;sdata=WM4AQs45%2FgKgmoxl4bS%2B5sTSaWYhCt0YhmkVSlwqgeY%3D&amp;reserved=0
PLEASE do read the posting guide https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.R-project.org%2Fposting-guide.html&amp;data=02%7C01%7Cshawnmille%40pa.gov%7Cdfe052e5375647a1c32f08d6612eec13%7C418e284101284dd59b6c47fc5a9a1bde%7C1%7C1%7C636803251493279248&amp;sdata=BSVgu1ome5ob4rGe%2Bck3hcInxJn6tF0N%2F1CBJezIKG4%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.


From r@turner @ending from @uckl@nd@@c@nz  Thu Dec 13 21:06:14 2018
From: r@turner @ending from @uckl@nd@@c@nz (Rolf Turner)
Date: Fri, 14 Dec 2018 09:06:14 +1300
Subject: [R] [FORGED] Re:  R plot split screen in uneven panels
In-Reply-To: <CAMk+s2TK10uwumeCm52sK6Wn-06eg7vQg_tsb2pWFjv0yhtD8Q@mail.gmail.com>
References: <CAMk+s2RX94-=HQ7XjzxO7_3dub5huYKudSHuZaq4hF-AociWPQ@mail.gmail.com>
 <CAGxFJbSGqniB1BmALa1z_CDa7o3be8j-GOf8fmTJ44LDimfZCg@mail.gmail.com>
 <CAGxFJbRcL36E02LwrPezH_CSwyLwaZCC2R6bDKArRjCvf4bsYg@mail.gmail.com>
 <CAMk+s2TK10uwumeCm52sK6Wn-06eg7vQg_tsb2pWFjv0yhtD8Q@mail.gmail.com>
Message-ID: <76f8ff59-e161-1032-957f-fab137d67b9e@auckland.ac.nz>


On 12/14/18 1:44 AM, Luigi Marongiu wrote:

> Thank you, that worked good. I tried to read the help for
> layout/split.screen but I found it confusing.

<SNIP>

Me too.  I conjecture that *everybody* finds it confusing, except maybe 
Paul M., and I'm not so sure about him! :-)

However it *is possible* to fight your way through the help. 
Experimenting is useful!  And when you succeed in fighting your way 
through, it works *well*.  (Not "good"!!!)

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From dc@rl@on @ending from t@mu@edu  Thu Dec 13 21:43:28 2018
From: dc@rl@on @ending from t@mu@edu (David L Carlson)
Date: Thu, 13 Dec 2018 20:43:28 +0000
Subject: [R] [External] RE: help
In-Reply-To: <BL0PR0901MB32182A49CCA33756BB0785E3B9A00@BL0PR0901MB3218.namprd09.prod.outlook.com>
References: <BL0PR0901MB3218E057E4CEEF92FCC3DBADB9A00@BL0PR0901MB3218.namprd09.prod.outlook.com>
 <ce8dd23c4c2644f0a496582fe1c9a1a3@tamu.edu>
 <BL0PR0901MB3218831CD4CEF3768663214BB9A00@BL0PR0901MB3218.namprd09.prod.outlook.com>
 <e9b83dfd00fe4e81894b5b5741809fb0@tamu.edu>
 <BL0PR0901MB32182A49CCA33756BB0785E3B9A00@BL0PR0901MB3218.namprd09.prod.outlook.com>
Message-ID: <bf9f763481c64d48b02270cb47c5ce85@tamu.edu>

OK. Follow the chain of subsets to figure out where the dates go missing.

Scanning the code I see the following subsets: wqn.last5yrs, wqn.removeblanks, wqn.removenull, wqn.removeparm, n.det, wqn.ph, wqn.noph. Use the range function to find out where the observations get dropped out. 

range(wqn.last5yrs$DATE_COLLECTED)

David C

-----Original Message-----
From: Miller, Shawn <shawnmille at pa.gov> 
Sent: Thursday, December 13, 2018 1:42 PM
To: David L Carlson <dcarlson at tamu.edu>
Cc: R-help at r-project.org
Subject: RE: [External] RE: help

Yes I confirmed all the dates I need are in the test.xlsx table. The final.2.xlsx table does not have the dates  for July, August, September.

-----Original Message-----
From: David L Carlson [mailto:dcarlson at tamu.edu] 
Sent: Thursday, December 13, 2018 2:33 PM
To: Miller, Shawn <shawnmille at pa.gov>
Cc: R-help at r-project.org
Subject: RE: [External] RE: help

Use Reply-All to keep the thread on the list.

If the most recent WQN SIS Data Download table is 14June2018_WQN_Data_Download, does it contain any data for the months July, August, September, October, November, December? After extracting the data you create the file "Anti-deg_requests/test.xlsx". Have you confirmed that all of the dates you need are present in that file?

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352

-----Original Message-----
From: Miller, Shawn <shawnmille at pa.gov>
Sent: Thursday, December 13, 2018 1:15 PM
To: David L Carlson <dcarlson at tamu.edu>
Subject: RE: [External] RE: help

setwd("c:/users/shawnmille/Desktop/Anti-deg_requests/")

library(plyr)
library(reshape2)
library(RODBC)
library(dplyr)



## MAKE SURE YOU ARE USING THE 32-bit VERSION OF R (See Tools>Global Options>General in RStudio to set this) ## READING DATA FROM EXISTING DATABASES (EXAMPLE: ACCESS 2010) chan <- odbcConnectAccess("WQN Datadumps.mdb")

# Show list of tables in database
sqlTables(chan,tableType='TABLE')
# Fetch the most recent WQN SIS Data Download table, this takes a couple minutes allwqn <- sqlFetch(chan,'14June2018_WQN_Data_Download',stringsAsFactors=FALSE)
names(allwqn)
## subset to the desired fields and values, choose your WQN by modifying command

wqn.specific <- subset(allwqn,MONITORING_POINT_ALIAS_ID =='WQN0735',select=c('DATE_COLLECTED','MONITORING_POINT_ALIAS_ID','QUALITY_ASSURANCE_TYPE_DESC',
                                                                                                                  'TEST_CODE','TEST_SHORT_DESC','READING_INDICATOR_CODE','FINAL_AMOUNT','ABBREVIATION'))  

###VIEW ENTIRE DATASET, this coding can be used to view any created datasets in excel#####
library(xlsx)
write.xlsx(wqn.specific,"c:/users/shawnmille/Desktop/Anti-deg_requests/test.xlsx") 

###VIEW ENTIRE DATASET#####

# FOR ACTIVE WQN STATION GET TODAY'S DATE and date of 5 years ago + 4 months or put in last date in command# today <- Sys.Date()
TODAY.5<-today-1949
## Select data where DATA_COLLECTED >= TODAY.5## wqn.specific$DATE_COLLECTED <- as.Date(wqn.specific$DATE_COLLECTED ,"%m/%d/%y") wqn.last5yrs <- subset(wqn.specific,DATE_COLLECTED >= TODAY.5, select=c('DATE_COLLECTED','MONITORING_POINT_ALIAS_ID','QUALITY_ASSURANCE_TYPE_DESC','TEST_CODE','TEST_SHORT_DESC','READING_INDICATOR_CODE','FINAL_AMOUNT','ABBREVIATION'))


#OR FOR INACTIVE WQN STATION ENTER THE LAST DATE OF RECORD BELOW AND GET PAST 5 YEARS OF DATA IN SUBSET# lastdate <- as.Date("11/01/2010", format = "%m/%d/%Y") firstdate <- lastdate-1949 wqn.specific$DATE_COLLECTED <- as.Date(wqn.specific$DATE_COLLECTED ,"%m/%d/%y") wqn.last5yrs <- subset(wqn.specific,DATE_COLLECTED >= firstdate & DATE_COLLECTED <= lastdate, select=c('DATE_COLLECTED','MONITORING_POINT_ALIAS_ID','QUALITY_ASSURANCE_TYPE_DESC','TEST_CODE','TEST_SHORT_DESC','READING_INDICATOR_CODE','FINAL_AMOUNT','ABBREVIATION'))

#### Delete Blanks and QA column #####
wqn.removeblanks <-subset(wqn.last5yrs,QUALITY_ASSURANCE_TYPE_DESC=="Duplicate"|is.na(QUALITY_ASSURANCE_TYPE_DESC),select=c('DATE_COLLECTED','MONITORING_POINT_ALIAS_ID',
                                                                                                                            'TEST_CODE','TEST_SHORT_DESC','READING_INDICATOR_CODE','FINAL_AMOUNT','ABBREVIATION'))
#### Delete Null Results####
wqn.removenull <- subset(wqn.removeblanks,FINAL_AMOUNT!="NA",select=c('DATE_COLLECTED','MONITORING_POINT_ALIAS_ID','TEST_CODE','TEST_SHORT_DESC','READING_INDICATOR_CODE','FINAL_AMOUNT','ABBREVIATION')) 

### Remove non-relevant parameters ####

wqn.removeparm <- subset(wqn.removenull,TEST_CODE!= "571"&TEST_CODE!="572"&
                           TEST_CODE!="573"& TEST_CODE!="543"& TEST_CODE!="561"& TEST_CODE!="546"
                         & TEST_CODE!="595"& TEST_CODE!="596"& TEST_CODE!="597"& TEST_CODE!="598"
                         & TEST_CODE!="599"& TEST_CODE!="600"& TEST_CODE!="71946"& TEST_CODE!="71940"
                         & TEST_CODE!="587"& TEST_CODE!="593"& TEST_CODE!="71939"& TEST_CODE!="71937"
                         & TEST_CODE!="574"& TEST_CODE!="549"& TEST_CODE!="99014"& TEST_CODE!="547"
                         & TEST_CODE!="298"& TEST_CODE!="551"& TEST_CODE!="71930"& TEST_CODE!="70508"
                         & TEST_CODE!="564"& TEST_CODE!="709"& TEST_CODE!="111"& TEST_CODE!="592"
                         & TEST_CODE!="MMTECMF"& TEST_CODE!="588"& TEST_CODE!="589"& TEST_CODE!="590"
                         & TEST_CODE!="594"& TEST_CODE!="71936"& TEST_CODE!="71945"& TEST_CODE!="F00061"
                         & TEST_CODE!="71947"& TEST_CODE!="555",
                         select=c('DATE_COLLECTED','MONITORING_POINT_ALIAS_ID','TEST_CODE','TEST_SHORT_DESC','READING_INDICATOR_CODE','FINAL_AMOUNT','ABBREVIATION'))

###  Combine test codes that are measuring the same analyte, format testcode to include first 5 digits###


wqn.removeparm$TEST_CODE<-substr(wqn.removeparm$TEST_CODE,1,5)


######Creates a table (wqn.det) with list of max detection limits for each parameter,FOR EACH TEST_CODE BRING ALL  < TO THAT VALUE############

n.det <- subset(wqn.removeparm,READING_INDICATOR_CODE=='<')
wqn.det<- aggregate(n.det$FINAL_AMOUNT , by=list(n.det$MONITORING_POINT_ALIAS_ID,n.det$TEST_CODE),FUN=max)
colnames(wqn.det) <- c("MONITORING_POINT_ALIAS_ID","TEST_CODE","DET_LIMIT")

wqn.mer<-merge(wqn.removeparm,wqn.det, by=c("MONITORING_POINT_ALIAS_ID","TEST_CODE"),all=TRUE)
wqn.mer$Amount<- ifelse(is.na(wqn.mer$'READING_INDICATOR_CODE'),wqn.mer$FINAL_AMOUNT*1,wqn.mer$DET_LIMIT*1)


######DIVIDE '<' BY HALF #####
wqn.mer$Result <- ifelse(is.na(wqn.mer$'READING_INDICATOR_CODE'),wqn.mer$Amount*1,wqn.mer$Amount/2)

###### AVERAGE AND GROUP BY TO REMOVE DUPLICATES ########

wqn.agg<-aggregate(wqn.mer$Result, by=list(MONITORING_POINT_ALIAS_ID=wqn.mer$'MONITORING_POINT_ALIAS_ID',DATE_COLLECTED=wqn.mer$'DATE_COLLECTED',TEST_CODE=wqn.mer$'TEST_CODE'),data=wqn.mer, FUN="mean",na.rm=TRUE)
colnames(wqn.agg) <- c("MONITORING_POINT_ALIAS_ID","DATE_COLLECTED","TEST_CODE","Result")

###### Calculate Median and 95% CI, alpha/2 for tailed2 (pH)..similar to ci.median function in asbio package, using alpha for funtion "tailed1"(other parameters)###


tailed2<-function(x){
  n <- nrow(as.matrix(x))
  L <- qbinom(.025,n,0.5)
  U <- n-L+1
  order.x <-sort(x)
  lower <- (order.x[L])
  upper <-order.x[n - 
                    L + 1]
  median <- median(x)
  coverage <- 1-(2*pbinom(qbinom(.025,n,.5)-1,n,0.5))
  y<-list(c(lower, median, upper,n,coverage))

  
  
  
  
  
}
tailed1<-function(x){
  n <- nrow(as.matrix(x))
  L <- qbinom(.05,n,0.5)
  U <- n-L+1
  order.x <-sort(x)
  lower <- (order.x[L])
  upper <-order.x[n - 
                    L+1]
  median <- median(x)
  coverage <- 1-(2*pbinom(qbinom(.05,n,.5)-1,n,0.5))
  y<-list(c(lower, median, upper,n,coverage))
  
  
  
  
} 


###Create two data sets, one with all the parameters except pH and one with just pH##### wqn.ph <- subset(wqn.agg,TEST_CODE=='00403'|TEST_CODE=="F0040",select=c("MONITORING_POINT_ALIAS_ID","DATE_COLLECTED","TEST_CODE","Result")) 
wqn.noph <-subset(wqn.agg,TEST_CODE!='00403'& TEST_CODE!="F0040",select=c("MONITORING_POINT_ALIAS_ID","DATE_COLLECTED","TEST_CODE","Result"))                  
                   
###Run tailed1 function on all data except for pH###                   
                   
stats<-aggregate(wqn.noph$Result, by=list(wqn.noph$MONITORING_POINT_ALIAS_ID,wqn.noph$TEST_CODE), FUN=tailed1)
colnames(stats) <- c("MONITORING_POINT_ALIAS_ID","TEST_CODE","x")

stats$lower <-unlist(lapply(stats$x, '[[', 1)) stats$median<-unlist(lapply(stats$x, '[[', 2)) stats$upper <-unlist(lapply(stats$x, '[[', 3)) stats$n <-unlist(lapply(stats$x, '[[', 4))
stats1 <- stats[order(stats$"MONITORING_POINT_ALIAS_ID",stats$"TEST_CODE"),] 

###Run tailed2 function on pH (lab and field) data#### ph.stats<-aggregate(wqn.ph$Result, by=list(wqn.ph$MONITORING_POINT_ALIAS_ID,wqn.ph$TEST_CODE), FUN=tailed2)
colnames(ph.stats) <- c("MONITORING_POINT_ALIAS_ID","TEST_CODE","x")

ph.stats$lower <-unlist(lapply(ph.stats$x, '[[', 1)) ph.stats$median<-unlist(lapply(ph.stats$x, '[[', 2)) ph.stats$upper <-unlist(lapply(ph.stats$x, '[[', 3)) ph.stats$n <-unlist(lapply(ph.stats$x, '[[', 4))
ph.stats1 <- ph.stats[order(ph.stats$"MONITORING_POINT_ALIAS_ID",ph.stats$"TEST_CODE"),] 

###Combine datasets vertically###
stats2 <- rbind(stats1,ph.stats1)


####PeriodofRecordTable####

library(dplyr)

my.dt<-(format(as.Date(wqn.removeparm$"DATE_COLLECTED"),"%m/%d/%Y"))

my.dt1<-aggregate(my.dt, by=list(wqn.removeparm$MONITORING_POINT_ALIAS_ID,wqn.removeparm$TEST_CODE), FUN=first)
colnames(my.dt1) <- c("MONITORING_POINT_ALIAS_ID","TEST_CODE","FirstDate")

my.dtlast<-aggregate(my.dt, by=list(wqn.removeparm$MONITORING_POINT_ALIAS_ID,wqn.removeparm$TEST_CODE), FUN=last)
colnames(my.dtlast) <- c("MONITORING_POINT_ALIAS_ID","TEST_CODE","LastDate")



####Merging Tables Dates and Stats#################### all.dt <- merge(my.dt1,my.dtlast,by=c("MONITORING_POINT_ALIAS_ID","TEST_CODE"))

dt.stats<-merge(all.dt,stats2,by=c("MONITORING_POINT_ALIAS_ID","TEST_CODE"))


####Calculate Period Of record in Table, Old coding 2 lines: dt.stats$pdofrecord<-(mdy(dt.stats$"LastDate"))-(mdy(dt.stats$"FirstDate"))
###dt.stats$pd<-round(dt.stats$pdofrecord/31563000, digits=1), lubridate is glitchy test new code on this step more######


dt.stats$pd <- round((as.Date(dt.stats$LastDate,format = "%m/%d/%Y")-as.Date(dt.stats$FirstDate,format = "%m/%d/%Y"))/365,digits=1)



####Retrieve Test Descriptions and Merge######
t1 <- subset(dt.stats,select=c('MONITORING_POINT_ALIAS_ID','TEST_CODE','FirstDate','LastDate','pd','lower','median','upper','n'))
t2<-subset(wqn.removeparm,select=c('TEST_CODE','TEST_SHORT_DESC','ABBREVIATION'))
t3 <- unique( t2 )

t4<-merge(t1,t3, by='TEST_CODE',all.t3=TRUE) ######ReorderColumns,sort, rename columns#######
t5 <- t4[ ,c(2,1,10,3,4,5,9,6,7,8,11)]
t6 <- t5[order(t5$"MONITORING_POINT_ALIAS_ID",t5$"TEST_SHORT_DESC"),]
names(t6)

####This does not need to be run. It produces an error. You still get the correct results.
library(plyr)
library(dplyr)
library(reshape2)
final.tab<-rename(t6, c("FirstDate"="FIRST_DATE", "LastDate"="LAST_DATE","pd"="PERIOD_OF_RECORD(yrs)","n"="SAMPLE_SIZE","lower"
             ="L_95_CI","median"="MEDIAN_","upper"="U_95_CI","ABBREVIATION"="UNITS"))

####BRING VALUES UP TO DETECTION LIMITS merge with wqn.det###### less<-function(x){
  sprintf("< %3.2f", x)
} 

wqn.final <-merge(t6,wqn.det,by=c("MONITORING_POINT_ALIAS_ID","TEST_CODE"),all=TRUE)
wqn.final$DET_LIMIT[is.na(wqn.final$DET_LIMIT)] <- 0 wqn.final$LOW_95_CL <- ifelse(wqn.final$lower < wqn.final$DET_LIM,less(wqn.final$DET_LIM*1),wqn.final$lower*1)
wqn.final$MEDIAN <- ifelse(wqn.final$median < wqn.final$DET_LIM,less(wqn.final$DET_LIM*1),wqn.final$median*1)
wqn.final$UPP_95_CL <- ifelse(wqn.final$upper < wqn.final$DET_LIM,less(wqn.final$DET_LIM*1),wqn.final$upper*1)

####ONLY REPORT MINIMUM FOR ALKALINITY, PH, DO, TEMP, SPECIFIC cONDUCTANCE######

wqn.final$LOWER_95_CL <- ifelse(wqn.final$TEST_CODE=="00403"|wqn.final$TEST_CODE=="00410"|wqn.final$TEST_CODE=="F0040"|
                                    wqn.final$TEST_CODE=="F0030"|wqn.final$TEST_CODE=="F0043",wqn.final$LOW_95_CL,"NA")

wqn.final$UPPER_95_CL <- ifelse(wqn.final$TEST_CODE=="00410"|wqn.final$TEST_CODE=="F0043"|
                                  wqn.final$TEST_CODE=="F0030","NA",wqn.final$UPP_95_CL)


names(wqn.final)                                  
final.1<-subset(wqn.final, select=c('MONITORING_POINT_ALIAS_ID','TEST_CODE','TEST_SHORT_DESC','FirstDate',
                                    'LastDate', 'pd',"n","LOWER_95_CL",'MEDIAN',
                                    'UPPER_95_CL','ABBREVIATION'))                                    
final.2 <- final.1[order(final.1$"MONITORING_POINT_ALIAS_ID",final.1$"TEST_SHORT_DESC"),]                                     

###VIEW IN EXCEL##
library(xlsx)
write.xlsx(final.2,"c:/users/shawnmille/Desktop/Anti-deg_requests/final.2.xlsx") 


-----Original Message-----
From: David L Carlson [mailto:dcarlson at tamu.edu]
Sent: Thursday, December 13, 2018 2:12 PM
To: Miller, Shawn <shawnmille at pa.gov>; R-help at r-project.org
Subject: [External] RE: help

ATTENTION: This email message is from an external sender. Do not open links or attachments from unknown sources. To report suspicious email, forward the message as an attachment to CWOPA_SPAM at pa.gov.

You need Santa Claus not r-help. You haven't given us a fraction of the information we would need to help. You don't show us your code. You don't tell us where the information is coming from except "today's date." You don't tell us what data you want. You don't seem to know the difference between R and R-Studio.

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352


-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Miller, Shawn
Sent: Thursday, December 13, 2018 11:29 AM
To: R-help at r-project.org
Subject: [R] help

Need help with R studio. Code is to pull data from todays date plus 5 years 4 months from now. I am missing the last 3 months of data. Can you please help?

Shawn Miller | Aquatic Biologist II | Assessment Section Environmental Protection | Clean Water Rachel Carson State Office Building
400 Market Street | Harrisburg, PA 17101
Phone: 717.772.2185 | Fax: 717.772.3249
https://na01.safelinks.protection.outlook.com/?url=www.depweb.state.pa.us&amp;data=02%7C01%7Cshawnmille%40pa.gov%7Ce275c28afad54d99b5d608d66131bec5%7C418e284101284dd59b6c47fc5a9a1bde%7C1%7C0%7C636803263627811233&amp;sdata=SetioLDNtYwPWgXTfgp4XNoH2ptEpbMkVCkYBcBFJAE%3D&amp;reserved=0<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwebmail.state.pa.us%2FOWA%2Fredir.aspx%3FC%3Dt4jGxr3_mkC5mWY30vM0D8N-9RdJ8s9IgIGFizoEzsd1aNOJaDwGjjpEh4RqLFX24CIJXV9M2ic.%26URL%3Dhttp%253a%252f%252fwww.depweb.state.pa.us%252f&amp;data=02%7C01%7Cshawnmille%40pa.gov%7Ce275c28afad54d99b5d608d66131bec5%7C418e284101284dd59b6c47fc5a9a1bde%7C1%7C0%7C636803263627811233&amp;sdata=8mPY9gVwTHUVb6EhpRueVUcHQIyTyd7QYYctDdbwBsM%3D&amp;reserved=0>


        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=02%7C01%7Cshawnmille%40pa.gov%7Ce275c28afad54d99b5d608d66131bec5%7C418e284101284dd59b6c47fc5a9a1bde%7C1%7C0%7C636803263627811233&amp;sdata=FEb6F5f1w9Y7%2BPPPfHvCkQNVZjNJJjHod%2F0%2BX4IifUw%3D&amp;reserved=0
PLEASE do read the posting guide https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.R-project.org%2Fposting-guide.html&amp;data=02%7C01%7Cshawnmille%40pa.gov%7Ce275c28afad54d99b5d608d66131bec5%7C418e284101284dd59b6c47fc5a9a1bde%7C1%7C0%7C636803263627811233&amp;sdata=ZemT7Y8LV%2FFIi5J46qF1rN48UlM1OxelnJYFhcmvLRc%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.


From drjimlemon @ending from gm@il@com  Thu Dec 13 22:03:50 2018
From: drjimlemon @ending from gm@il@com (Jim Lemon)
Date: Fri, 14 Dec 2018 08:03:50 +1100
Subject: [R] Plotting Very Large lat-lon data in x-y axes graph
In-Reply-To: <CAC8ss31zwzNH1VnksDjnJxQZgUVx0KecNydhP5ExNy6hwVAyXA@mail.gmail.com>
References: <CAC8ss33ZS-8NqHf+6Q3Rk=qt+Z7+O7sVFV9Ahi+X8U=DqxhOsw@mail.gmail.com>
 <CA+8X3fU04pA_nh2DvXmY9vUWGr7bsj8AwRBp9Xd=qFvGmj6RMQ@mail.gmail.com>
 <CAC8ss32fAjoh1Q2+7x9i25EyXCGAbeqjYOtG4r3WuWi2S6st4A@mail.gmail.com>
 <CAC8ss33wF7Tp-Uh86S4XMUVxbA56-1atk4kEj67Tg+Ps-3LN-A@mail.gmail.com>
 <CA+8X3fUuz0y70LNc2H8TuR0y65h728vR2GA55o+iE1DRGK+o5Q@mail.gmail.com>
 <CAC8ss31zwzNH1VnksDjnJxQZgUVx0KecNydhP5ExNy6hwVAyXA@mail.gmail.com>
Message-ID: <CA+8X3fWFB6KxwmN1c0hfPpJTw15iM2X2OUAxSudnvuHK2KepOQ@mail.gmail.com>

Hi Ogbos,
Back on the air after a few days off. I don't have your data ("QUERY
2"), but I think this will fix your problem.

library(maps)
map("world")
box()
library(plotrix)
color.legend(-180,-150,100,-130,legend=c(0,25000,50000,75000,100000),
 rect.col=color.scale(1:5,extremes=c("blue","red")),gradient="x")

Notice that I have swapped the "yb" and "yt" values so that they are
in increasing order. If they are reversed, the numbers will appear
within the color bar. Also you don't need to call color.gradient, just
pass the output of a five increment color scale from blue to red (or
whatever you like) to the rect.col argument.

Jim


From @h@wnmille @ending from p@@gov  Thu Dec 13 20:42:29 2018
From: @h@wnmille @ending from p@@gov (Miller, Shawn)
Date: Thu, 13 Dec 2018 19:42:29 +0000
Subject: [R] [External] RE: help
In-Reply-To: <e9b83dfd00fe4e81894b5b5741809fb0@tamu.edu>
References: <BL0PR0901MB3218E057E4CEEF92FCC3DBADB9A00@BL0PR0901MB3218.namprd09.prod.outlook.com>
 <ce8dd23c4c2644f0a496582fe1c9a1a3@tamu.edu>
 <BL0PR0901MB3218831CD4CEF3768663214BB9A00@BL0PR0901MB3218.namprd09.prod.outlook.com>
 <e9b83dfd00fe4e81894b5b5741809fb0@tamu.edu>
Message-ID: <BL0PR0901MB32182A49CCA33756BB0785E3B9A00@BL0PR0901MB3218.namprd09.prod.outlook.com>

Yes I confirmed all the dates I need are in the test.xlsx table. The final.2.xlsx table does not have the dates  for July, August, September.

-----Original Message-----
From: David L Carlson [mailto:dcarlson at tamu.edu] 
Sent: Thursday, December 13, 2018 2:33 PM
To: Miller, Shawn <shawnmille at pa.gov>
Cc: R-help at r-project.org
Subject: RE: [External] RE: help

Use Reply-All to keep the thread on the list.

If the most recent WQN SIS Data Download table is 14June2018_WQN_Data_Download, does it contain any data for the months July, August, September, October, November, December? After extracting the data you create the file "Anti-deg_requests/test.xlsx". Have you confirmed that all of the dates you need are present in that file?

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352

-----Original Message-----
From: Miller, Shawn <shawnmille at pa.gov>
Sent: Thursday, December 13, 2018 1:15 PM
To: David L Carlson <dcarlson at tamu.edu>
Subject: RE: [External] RE: help

setwd("c:/users/shawnmille/Desktop/Anti-deg_requests/")

library(plyr)
library(reshape2)
library(RODBC)
library(dplyr)



## MAKE SURE YOU ARE USING THE 32-bit VERSION OF R (See Tools>Global Options>General in RStudio to set this) ## READING DATA FROM EXISTING DATABASES (EXAMPLE: ACCESS 2010) chan <- odbcConnectAccess("WQN Datadumps.mdb")

# Show list of tables in database
sqlTables(chan,tableType='TABLE')
# Fetch the most recent WQN SIS Data Download table, this takes a couple minutes allwqn <- sqlFetch(chan,'14June2018_WQN_Data_Download',stringsAsFactors=FALSE)
names(allwqn)
## subset to the desired fields and values, choose your WQN by modifying command

wqn.specific <- subset(allwqn,MONITORING_POINT_ALIAS_ID =='WQN0735',select=c('DATE_COLLECTED','MONITORING_POINT_ALIAS_ID','QUALITY_ASSURANCE_TYPE_DESC',
                                                                                                                  'TEST_CODE','TEST_SHORT_DESC','READING_INDICATOR_CODE','FINAL_AMOUNT','ABBREVIATION'))  

###VIEW ENTIRE DATASET, this coding can be used to view any created datasets in excel#####
library(xlsx)
write.xlsx(wqn.specific,"c:/users/shawnmille/Desktop/Anti-deg_requests/test.xlsx") 

###VIEW ENTIRE DATASET#####

# FOR ACTIVE WQN STATION GET TODAY'S DATE and date of 5 years ago + 4 months or put in last date in command# today <- Sys.Date()
TODAY.5<-today-1949
## Select data where DATA_COLLECTED >= TODAY.5## wqn.specific$DATE_COLLECTED <- as.Date(wqn.specific$DATE_COLLECTED ,"%m/%d/%y") wqn.last5yrs <- subset(wqn.specific,DATE_COLLECTED >= TODAY.5, select=c('DATE_COLLECTED','MONITORING_POINT_ALIAS_ID','QUALITY_ASSURANCE_TYPE_DESC','TEST_CODE','TEST_SHORT_DESC','READING_INDICATOR_CODE','FINAL_AMOUNT','ABBREVIATION'))


#OR FOR INACTIVE WQN STATION ENTER THE LAST DATE OF RECORD BELOW AND GET PAST 5 YEARS OF DATA IN SUBSET# lastdate <- as.Date("11/01/2010", format = "%m/%d/%Y") firstdate <- lastdate-1949 wqn.specific$DATE_COLLECTED <- as.Date(wqn.specific$DATE_COLLECTED ,"%m/%d/%y") wqn.last5yrs <- subset(wqn.specific,DATE_COLLECTED >= firstdate & DATE_COLLECTED <= lastdate, select=c('DATE_COLLECTED','MONITORING_POINT_ALIAS_ID','QUALITY_ASSURANCE_TYPE_DESC','TEST_CODE','TEST_SHORT_DESC','READING_INDICATOR_CODE','FINAL_AMOUNT','ABBREVIATION'))

#### Delete Blanks and QA column #####
wqn.removeblanks <-subset(wqn.last5yrs,QUALITY_ASSURANCE_TYPE_DESC=="Duplicate"|is.na(QUALITY_ASSURANCE_TYPE_DESC),select=c('DATE_COLLECTED','MONITORING_POINT_ALIAS_ID',
                                                                                                                            'TEST_CODE','TEST_SHORT_DESC','READING_INDICATOR_CODE','FINAL_AMOUNT','ABBREVIATION'))
#### Delete Null Results####
wqn.removenull <- subset(wqn.removeblanks,FINAL_AMOUNT!="NA",select=c('DATE_COLLECTED','MONITORING_POINT_ALIAS_ID','TEST_CODE','TEST_SHORT_DESC','READING_INDICATOR_CODE','FINAL_AMOUNT','ABBREVIATION')) 

### Remove non-relevant parameters ####

wqn.removeparm <- subset(wqn.removenull,TEST_CODE!= "571"&TEST_CODE!="572"&
                           TEST_CODE!="573"& TEST_CODE!="543"& TEST_CODE!="561"& TEST_CODE!="546"
                         & TEST_CODE!="595"& TEST_CODE!="596"& TEST_CODE!="597"& TEST_CODE!="598"
                         & TEST_CODE!="599"& TEST_CODE!="600"& TEST_CODE!="71946"& TEST_CODE!="71940"
                         & TEST_CODE!="587"& TEST_CODE!="593"& TEST_CODE!="71939"& TEST_CODE!="71937"
                         & TEST_CODE!="574"& TEST_CODE!="549"& TEST_CODE!="99014"& TEST_CODE!="547"
                         & TEST_CODE!="298"& TEST_CODE!="551"& TEST_CODE!="71930"& TEST_CODE!="70508"
                         & TEST_CODE!="564"& TEST_CODE!="709"& TEST_CODE!="111"& TEST_CODE!="592"
                         & TEST_CODE!="MMTECMF"& TEST_CODE!="588"& TEST_CODE!="589"& TEST_CODE!="590"
                         & TEST_CODE!="594"& TEST_CODE!="71936"& TEST_CODE!="71945"& TEST_CODE!="F00061"
                         & TEST_CODE!="71947"& TEST_CODE!="555",
                         select=c('DATE_COLLECTED','MONITORING_POINT_ALIAS_ID','TEST_CODE','TEST_SHORT_DESC','READING_INDICATOR_CODE','FINAL_AMOUNT','ABBREVIATION'))

###  Combine test codes that are measuring the same analyte, format testcode to include first 5 digits###


wqn.removeparm$TEST_CODE<-substr(wqn.removeparm$TEST_CODE,1,5)


######Creates a table (wqn.det) with list of max detection limits for each parameter,FOR EACH TEST_CODE BRING ALL  < TO THAT VALUE############

n.det <- subset(wqn.removeparm,READING_INDICATOR_CODE=='<')
wqn.det<- aggregate(n.det$FINAL_AMOUNT , by=list(n.det$MONITORING_POINT_ALIAS_ID,n.det$TEST_CODE),FUN=max)
colnames(wqn.det) <- c("MONITORING_POINT_ALIAS_ID","TEST_CODE","DET_LIMIT")

wqn.mer<-merge(wqn.removeparm,wqn.det, by=c("MONITORING_POINT_ALIAS_ID","TEST_CODE"),all=TRUE)
wqn.mer$Amount<- ifelse(is.na(wqn.mer$'READING_INDICATOR_CODE'),wqn.mer$FINAL_AMOUNT*1,wqn.mer$DET_LIMIT*1)


######DIVIDE '<' BY HALF #####
wqn.mer$Result <- ifelse(is.na(wqn.mer$'READING_INDICATOR_CODE'),wqn.mer$Amount*1,wqn.mer$Amount/2)

###### AVERAGE AND GROUP BY TO REMOVE DUPLICATES ########

wqn.agg<-aggregate(wqn.mer$Result, by=list(MONITORING_POINT_ALIAS_ID=wqn.mer$'MONITORING_POINT_ALIAS_ID',DATE_COLLECTED=wqn.mer$'DATE_COLLECTED',TEST_CODE=wqn.mer$'TEST_CODE'),data=wqn.mer, FUN="mean",na.rm=TRUE)
colnames(wqn.agg) <- c("MONITORING_POINT_ALIAS_ID","DATE_COLLECTED","TEST_CODE","Result")

###### Calculate Median and 95% CI, alpha/2 for tailed2 (pH)..similar to ci.median function in asbio package, using alpha for funtion "tailed1"(other parameters)###


tailed2<-function(x){
  n <- nrow(as.matrix(x))
  L <- qbinom(.025,n,0.5)
  U <- n-L+1
  order.x <-sort(x)
  lower <- (order.x[L])
  upper <-order.x[n - 
                    L + 1]
  median <- median(x)
  coverage <- 1-(2*pbinom(qbinom(.025,n,.5)-1,n,0.5))
  y<-list(c(lower, median, upper,n,coverage))

  
  
  
  
  
}
tailed1<-function(x){
  n <- nrow(as.matrix(x))
  L <- qbinom(.05,n,0.5)
  U <- n-L+1
  order.x <-sort(x)
  lower <- (order.x[L])
  upper <-order.x[n - 
                    L+1]
  median <- median(x)
  coverage <- 1-(2*pbinom(qbinom(.05,n,.5)-1,n,0.5))
  y<-list(c(lower, median, upper,n,coverage))
  
  
  
  
} 


###Create two data sets, one with all the parameters except pH and one with just pH##### wqn.ph <- subset(wqn.agg,TEST_CODE=='00403'|TEST_CODE=="F0040",select=c("MONITORING_POINT_ALIAS_ID","DATE_COLLECTED","TEST_CODE","Result")) 
wqn.noph <-subset(wqn.agg,TEST_CODE!='00403'& TEST_CODE!="F0040",select=c("MONITORING_POINT_ALIAS_ID","DATE_COLLECTED","TEST_CODE","Result"))                  
                   
###Run tailed1 function on all data except for pH###                   
                   
stats<-aggregate(wqn.noph$Result, by=list(wqn.noph$MONITORING_POINT_ALIAS_ID,wqn.noph$TEST_CODE), FUN=tailed1)
colnames(stats) <- c("MONITORING_POINT_ALIAS_ID","TEST_CODE","x")

stats$lower <-unlist(lapply(stats$x, '[[', 1)) stats$median<-unlist(lapply(stats$x, '[[', 2)) stats$upper <-unlist(lapply(stats$x, '[[', 3)) stats$n <-unlist(lapply(stats$x, '[[', 4))
stats1 <- stats[order(stats$"MONITORING_POINT_ALIAS_ID",stats$"TEST_CODE"),] 

###Run tailed2 function on pH (lab and field) data#### ph.stats<-aggregate(wqn.ph$Result, by=list(wqn.ph$MONITORING_POINT_ALIAS_ID,wqn.ph$TEST_CODE), FUN=tailed2)
colnames(ph.stats) <- c("MONITORING_POINT_ALIAS_ID","TEST_CODE","x")

ph.stats$lower <-unlist(lapply(ph.stats$x, '[[', 1)) ph.stats$median<-unlist(lapply(ph.stats$x, '[[', 2)) ph.stats$upper <-unlist(lapply(ph.stats$x, '[[', 3)) ph.stats$n <-unlist(lapply(ph.stats$x, '[[', 4))
ph.stats1 <- ph.stats[order(ph.stats$"MONITORING_POINT_ALIAS_ID",ph.stats$"TEST_CODE"),] 

###Combine datasets vertically###
stats2 <- rbind(stats1,ph.stats1)


####PeriodofRecordTable####

library(dplyr)

my.dt<-(format(as.Date(wqn.removeparm$"DATE_COLLECTED"),"%m/%d/%Y"))

my.dt1<-aggregate(my.dt, by=list(wqn.removeparm$MONITORING_POINT_ALIAS_ID,wqn.removeparm$TEST_CODE), FUN=first)
colnames(my.dt1) <- c("MONITORING_POINT_ALIAS_ID","TEST_CODE","FirstDate")

my.dtlast<-aggregate(my.dt, by=list(wqn.removeparm$MONITORING_POINT_ALIAS_ID,wqn.removeparm$TEST_CODE), FUN=last)
colnames(my.dtlast) <- c("MONITORING_POINT_ALIAS_ID","TEST_CODE","LastDate")



####Merging Tables Dates and Stats#################### all.dt <- merge(my.dt1,my.dtlast,by=c("MONITORING_POINT_ALIAS_ID","TEST_CODE"))

dt.stats<-merge(all.dt,stats2,by=c("MONITORING_POINT_ALIAS_ID","TEST_CODE"))


####Calculate Period Of record in Table, Old coding 2 lines: dt.stats$pdofrecord<-(mdy(dt.stats$"LastDate"))-(mdy(dt.stats$"FirstDate"))
###dt.stats$pd<-round(dt.stats$pdofrecord/31563000, digits=1), lubridate is glitchy test new code on this step more######


dt.stats$pd <- round((as.Date(dt.stats$LastDate,format = "%m/%d/%Y")-as.Date(dt.stats$FirstDate,format = "%m/%d/%Y"))/365,digits=1)



####Retrieve Test Descriptions and Merge######
t1 <- subset(dt.stats,select=c('MONITORING_POINT_ALIAS_ID','TEST_CODE','FirstDate','LastDate','pd','lower','median','upper','n'))
t2<-subset(wqn.removeparm,select=c('TEST_CODE','TEST_SHORT_DESC','ABBREVIATION'))
t3 <- unique( t2 )

t4<-merge(t1,t3, by='TEST_CODE',all.t3=TRUE) ######ReorderColumns,sort, rename columns#######
t5 <- t4[ ,c(2,1,10,3,4,5,9,6,7,8,11)]
t6 <- t5[order(t5$"MONITORING_POINT_ALIAS_ID",t5$"TEST_SHORT_DESC"),]
names(t6)

####This does not need to be run. It produces an error. You still get the correct results.
library(plyr)
library(dplyr)
library(reshape2)
final.tab<-rename(t6, c("FirstDate"="FIRST_DATE", "LastDate"="LAST_DATE","pd"="PERIOD_OF_RECORD(yrs)","n"="SAMPLE_SIZE","lower"
             ="L_95_CI","median"="MEDIAN_","upper"="U_95_CI","ABBREVIATION"="UNITS"))

####BRING VALUES UP TO DETECTION LIMITS merge with wqn.det###### less<-function(x){
  sprintf("< %3.2f", x)
} 

wqn.final <-merge(t6,wqn.det,by=c("MONITORING_POINT_ALIAS_ID","TEST_CODE"),all=TRUE)
wqn.final$DET_LIMIT[is.na(wqn.final$DET_LIMIT)] <- 0 wqn.final$LOW_95_CL <- ifelse(wqn.final$lower < wqn.final$DET_LIM,less(wqn.final$DET_LIM*1),wqn.final$lower*1)
wqn.final$MEDIAN <- ifelse(wqn.final$median < wqn.final$DET_LIM,less(wqn.final$DET_LIM*1),wqn.final$median*1)
wqn.final$UPP_95_CL <- ifelse(wqn.final$upper < wqn.final$DET_LIM,less(wqn.final$DET_LIM*1),wqn.final$upper*1)

####ONLY REPORT MINIMUM FOR ALKALINITY, PH, DO, TEMP, SPECIFIC cONDUCTANCE######

wqn.final$LOWER_95_CL <- ifelse(wqn.final$TEST_CODE=="00403"|wqn.final$TEST_CODE=="00410"|wqn.final$TEST_CODE=="F0040"|
                                    wqn.final$TEST_CODE=="F0030"|wqn.final$TEST_CODE=="F0043",wqn.final$LOW_95_CL,"NA")

wqn.final$UPPER_95_CL <- ifelse(wqn.final$TEST_CODE=="00410"|wqn.final$TEST_CODE=="F0043"|
                                  wqn.final$TEST_CODE=="F0030","NA",wqn.final$UPP_95_CL)


names(wqn.final)                                  
final.1<-subset(wqn.final, select=c('MONITORING_POINT_ALIAS_ID','TEST_CODE','TEST_SHORT_DESC','FirstDate',
                                    'LastDate', 'pd',"n","LOWER_95_CL",'MEDIAN',
                                    'UPPER_95_CL','ABBREVIATION'))                                    
final.2 <- final.1[order(final.1$"MONITORING_POINT_ALIAS_ID",final.1$"TEST_SHORT_DESC"),]                                     

###VIEW IN EXCEL##
library(xlsx)
write.xlsx(final.2,"c:/users/shawnmille/Desktop/Anti-deg_requests/final.2.xlsx") 


-----Original Message-----
From: David L Carlson [mailto:dcarlson at tamu.edu]
Sent: Thursday, December 13, 2018 2:12 PM
To: Miller, Shawn <shawnmille at pa.gov>; R-help at r-project.org
Subject: [External] RE: help

ATTENTION: This email message is from an external sender. Do not open links or attachments from unknown sources. To report suspicious email, forward the message as an attachment to CWOPA_SPAM at pa.gov.

You need Santa Claus not r-help. You haven't given us a fraction of the information we would need to help. You don't show us your code. You don't tell us where the information is coming from except "today's date." You don't tell us what data you want. You don't seem to know the difference between R and R-Studio.

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352


-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Miller, Shawn
Sent: Thursday, December 13, 2018 11:29 AM
To: R-help at r-project.org
Subject: [R] help

Need help with R studio. Code is to pull data from todays date plus 5 years 4 months from now. I am missing the last 3 months of data. Can you please help?

Shawn Miller | Aquatic Biologist II | Assessment Section Environmental Protection | Clean Water Rachel Carson State Office Building
400 Market Street | Harrisburg, PA 17101
Phone: 717.772.2185 | Fax: 717.772.3249
https://na01.safelinks.protection.outlook.com/?url=www.depweb.state.pa.us&amp;data=02%7C01%7Cshawnmille%40pa.gov%7Ce275c28afad54d99b5d608d66131bec5%7C418e284101284dd59b6c47fc5a9a1bde%7C1%7C0%7C636803263627811233&amp;sdata=SetioLDNtYwPWgXTfgp4XNoH2ptEpbMkVCkYBcBFJAE%3D&amp;reserved=0<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwebmail.state.pa.us%2FOWA%2Fredir.aspx%3FC%3Dt4jGxr3_mkC5mWY30vM0D8N-9RdJ8s9IgIGFizoEzsd1aNOJaDwGjjpEh4RqLFX24CIJXV9M2ic.%26URL%3Dhttp%253a%252f%252fwww.depweb.state.pa.us%252f&amp;data=02%7C01%7Cshawnmille%40pa.gov%7Ce275c28afad54d99b5d608d66131bec5%7C418e284101284dd59b6c47fc5a9a1bde%7C1%7C0%7C636803263627811233&amp;sdata=8mPY9gVwTHUVb6EhpRueVUcHQIyTyd7QYYctDdbwBsM%3D&amp;reserved=0>


        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=02%7C01%7Cshawnmille%40pa.gov%7Ce275c28afad54d99b5d608d66131bec5%7C418e284101284dd59b6c47fc5a9a1bde%7C1%7C0%7C636803263627811233&amp;sdata=FEb6F5f1w9Y7%2BPPPfHvCkQNVZjNJJjHod%2F0%2BX4IifUw%3D&amp;reserved=0
PLEASE do read the posting guide https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.R-project.org%2Fposting-guide.html&amp;data=02%7C01%7Cshawnmille%40pa.gov%7Ce275c28afad54d99b5d608d66131bec5%7C418e284101284dd59b6c47fc5a9a1bde%7C1%7C0%7C636803263627811233&amp;sdata=ZemT7Y8LV%2FFIi5J46qF1rN48UlM1OxelnJYFhcmvLRc%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.


From ccberry @ending from uc@d@edu  Fri Dec 14 03:03:36 2018
From: ccberry @ending from uc@d@edu (Berry, Charles)
Date: Fri, 14 Dec 2018 02:03:36 +0000
Subject: [R] fortune nomination WAS:Re: help
In-Reply-To: <ce8dd23c4c2644f0a496582fe1c9a1a3@tamu.edu>
References: <BL0PR0901MB3218E057E4CEEF92FCC3DBADB9A00@BL0PR0901MB3218.namprd09.prod.outlook.com>
 <ce8dd23c4c2644f0a496582fe1c9a1a3@tamu.edu>
Message-ID: <01DA978A-B121-452F-BAB0-13FFFEA5FC32@ucsd.edu>


"You need Santa Claus not r-help."

in response to an unrealistic and poorly posed request for help.

Best,

Chuck

> On Dec 13, 2018, at 11:12 AM, David L Carlson <dcarlson at tamu.edu> wrote:
> 
> You need Santa Claus not r-help. You haven't given us a fraction of the information we would need to help. You don't show us your code. You don't tell us where the information is coming from except "today's date." You don't tell us what data you want. You don't seem to know the difference between R and R-Studio.
> 
> ----------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77843-4352
> 
> 
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Miller, Shawn
> Sent: Thursday, December 13, 2018 11:29 AM
> To: R-help at r-project.org
> Subject: [R] help
> 
> Need help with R studio. Code is to pull data from todays date plus 5 years 4 months from now. I am missing the last 3 months of data. Can you please help?
> 
> Shawn Miller | Aquatic Biologist II | Assessment Section
> Environmental Protection | Clean Water
> Rachel Carson State Office Building
> 400 Market Street | Harrisburg, PA 17101
> Phone: 717.772.2185 | Fax: 717.772.3249
> www.depweb.state.pa.us<https://webmail.state.pa.us/OWA/redir.aspx?C=t4jGxr3_mkC5mWY30vM0D8N-9RdJ8s9IgIGFizoEzsd1aNOJaDwGjjpEh4RqLFX24CIJXV9M2ic.&URL=http%3a%2f%2fwww.depweb.state.pa.us%2f>
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From DSmith @ending from mednet@ucl@@edu  Fri Dec 14 05:33:00 2018
From: DSmith @ending from mednet@ucl@@edu (Smith, Desmond)
Date: Fri, 14 Dec 2018 04:33:00 +0000
Subject: [R] Quoting smooth random terms mccv::gam
Message-ID: <EFC6792B-87C7-42A1-8C03-E5FC83819B2A@mednet.ucla.edu>

Dear All,

I have a mgcv::gam model of the form:

m1 <- gam(Y ~ A + s(B, bs = "re"), data = dataframe, family = gaussian, method = "REML")

The random term is quoted in summary(m1) as, for example,

Approximate significance of smooth terms:
           # edf Ref.df      F p-value
s(B)  4.486      5 97.195 6.7e-08 ***

My question is, how would I quote this result (statistic and P value) in a formal document?

For example, one possibility is F[4.486,5] = 97.195, P = 6.7e-08. However, arguing against this, ?reverse engineering? of the result using

pf(q= 97.195, df1= 4.486, df2= 5, lower.tail=FALSE)

gives an incorrect p value:

[1] 0.1435508

I would be very grateful for your advice. Many thanks for your help!

________________________________

UCLA HEALTH SCIENCES IMPORTANT WARNING: This email (and any attachments) is only intended for the use of the person or entity to which it is addressed, and may contain information that is privileged and confidential. You, the recipient, are obligated to maintain it in a safe, secure and confidential manner. Unauthorized redisclosure or failure to maintain confidentiality may subject you to federal and state penalties. If you are not the intended recipient, please immediately notify us by return email, and delete this message from your computer.

	[[alternative HTML version deleted]]


From @ubh@mitr@@p@tr@ @ending from gm@il@com  Fri Dec 14 05:37:29 2018
From: @ubh@mitr@@p@tr@ @ending from gm@il@com (Subhamitra Patra)
Date: Fri, 14 Dec 2018 10:07:29 +0530
Subject: [R] [R studio] How to define date column and plot that date column
 as x axis in R studio
Message-ID: <CAOFE=kOcxHsA-eC+3g_Dp+pSknvApr=So6p_5OQQuEJgxRUe3w@mail.gmail.com>

 Dear R users,

I have half-hourly time series data; 48 data points in a day. The total
time period starts from 4/6/2018 (i.e day/month/year) to 2/12/2018. I want
to create a time series object. I am using ts function.

However, I am unable to define the frequency. As I know, frequency=1 for
yearly data, but unable to define in my half-hourly data.

After creating a time series object (i.e. date column), I want to plot it
with the corresponding data series.

How to plot my result (in R object) on Y-axis and time period on the
X-axis? For your convenience, I am providing my code as follows.

R <- function(x){
return(FDWhittle(x, method="discrete", sdf.method="multitaper"))
}
plot(R[,i],type="l",col = "Black", xlab="Time",
   ylab="Return",main=names(R)[,i])

When I am plotting by using the above code, the results in the object "R"
is coming on the Y-axis, and some values like (100, 200, 300--------) are
coming on the X-axis. I want to get the time period on my X axis, but
unable to define the intra-day date column having 48 data points in a day.

Further, kindly suggest me what can be the frequency level for defining the
date column of the daily data?

A kind help is highly appreciated.

Thanks in advance.


-- 
*Best Regards,*
*Subhamitra Patra*
*Phd. Research Scholar*
*Department of Humanities and Social Sciences*
*Indian Institute of Technology, Kharagpur*
*INDIA*

	[[alternative HTML version deleted]]


From @ubh@mitr@@p@tr@ @ending from gm@il@com  Fri Dec 14 06:22:03 2018
From: @ubh@mitr@@p@tr@ @ending from gm@il@com (Subhamitra Patra)
Date: Fri, 14 Dec 2018 10:52:03 +0530
Subject: [R] 
 [R studio] Plotting of line chart for each columns at 1 page
In-Reply-To: <CAOFE=kN_tdNCzu50RhVM4-k7d6psEd=vZu7pD0htdczBKSn-HQ@mail.gmail.com>
References: <CAOFE=kOWXrnRnUVtL7A0vLKjL03571WQy5W_865hTFeBz28+tA@mail.gmail.com>
 <CA+8X3fW3Op8icpvA68O2sYPpBi=rpO2DWVXwL5UrPOHtDqDzpw@mail.gmail.com>
 <CAOFE=kNV6P4jRtrYS9K1E76pn+tW_aVpQ4Kq25ZHtQ6v-m=TkQ@mail.gmail.com>
 <CA+8X3fXM9KEb8=U1y-E_DFy9UuXk7NXDyRo6VppnEEAbAtGDDQ@mail.gmail.com>
 <CAOFE=kO20RsVsvVL2ZXpsH51aw9bzgCc3tDJ7DX8fM-tgkkbdQ@mail.gmail.com>
 <CA+8X3fUh8bug_m8iGBGcdPdb=PeGXY0xG48NTwBHC2XzXrtyhg@mail.gmail.com>
 <CAOFE=kNda7BE7cnkUg3nTZNAZbG0-dq=ir2ddXy_1pRMwySpcg@mail.gmail.com>
 <CA+8X3fVsMos2cafXXk7seO=fpM+JFgufmQpcUhGqJ9nNm4BFmQ@mail.gmail.com>
 <CAOFE=kOiTHOzcQ+dL2Tak2WnGBx1JAXpryj5CbLefdofa-7x5Q@mail.gmail.com>
 <CA+8X3fVYFJ9GUzY5ujH38zEQhWKKrkUpRzf1UjC-Cc=GHMe0VA@mail.gmail.com>
 <CAOFE=kOBPM-6bdaQKC3NXTXuAn_9OOA8oEvOGKhrGdUxBUAVkw@mail.gmail.com>
 <CA+8X3fUSY13o=khzBy9RA1_qsP1vkVqNAm=UP-xAd_cyLNrAVw@mail.gmail.com>
 <CAOFE=kPwv6LspZ=4RvL9x2JE7YSE9Ad+4Rrdhpmj0hyEHmVhxA@mail.gmail.com>
 <CAOFE=kN_tdNCzu50RhVM4-k7d6psEd=vZu7pD0htdczBKSn-HQ@mail.gmail.com>
Message-ID: <CAOFE=kN6uZ9mKCkfDkz+xXLMxs3+WBu=A3YFAP-0UWt_b5V1FA@mail.gmail.com>

Dear Sir,

The method you suggested for arranging multiple plots in 1 pdf page has
finally worked.

Thank you very much.

But, I have 3 more queries as follows.

1. Is there any way that I can merge the respective plots from the
different excel files into 1? For instance, I have 3 excel files (i.e. EMs
1, EMs 2 and EMs 3) having 4 plots each. I want to merge 1st plots from 3
excel files into 1, 2nd plots from 3 excel files into 1, similarly 3rd and
4th plots from each respective files into 1?
2. Is there any way to range the scale of Y-axis for all plots because I
think if the range of the Y-axis for each plot is same, it can be easy to
interpret and compare the results. I used Ylim function. But, after using
the Ylim function, the plot is looking unclear than before. For your
convenience, I am providing my code.

pdf("EMs.pdf",width=20,height=20)
par(mfrow=c(5,4))
# import your first sheet here (38 columns)
ncolumns<-ncol(EMs1)
for(i in 1:ncolumns)
  plot(EMs1[,i],type="l",col = "Red", xlab="Time",
       ylab="APEn", ylim=c(0.1,2), main=names(EMs1)[i])
library(zoo)
*APEn=zoo(EMs1, seq(from = as.Date("1994-01-01"), to =
as.Date("2017-08-03"), by = 1))*
#import your second sheet here, (10 columns)
ncolumns<-ncol(EMs2)
for(i in 1:ncolumns)
  plot(EMs2[,i],type="l",col = "Red", xlab="Time",
       ylab="APEn",ylim=c(0.1,2), main=names(EMs2)[i])
*APEn=zoo(EMs2, seq(from = as.Date("1994-01-01"), to =
as.Date("2017-08-03"), by = 1))*
# import your Third sheet here, (2 columns)
ncolumns<-ncol(EMs3)
for(i in 1:ncolumns)
  plot(EMs3[,i],type="l",col = "Red", xlab="Time",
       ylab="APEn", ylim=c(0.1,2), main=names(EMs3)[i])
*APEn=zoo(EMs3, seq(from = as.Date("1996-01-01"), to =
as.Date("2017-08-03"), by = 1))  *
# finish plotting
dev.off()

3. I want the corresponding date column (which is the daily date) on X-axis
of each plot. Thus, I added 1 line after the plot in the above (i.e. the
bold line). Please suggest me Is it the correct way that I am doing?


Sir, I am new in R and doing by learning.

Kindly suggest me regarding my previous queries for which I shall be always
grateful to you.

Thank you very much for educating a new R learner.




On Wed, Nov 21, 2018 at 3:20 PM Subhamitra Patra <subhamitra.patra at gmail.com>
wrote:

> Thank you very much for your suggestions and help.
>
> [image: Mailtrack]
> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
> notified by
> Mailtrack
> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 11/21/18,
> 3:19:55 PM
>
> On Wed, Nov 21, 2018 at 3:09 PM Subhamitra Patra <
> subhamitra.patra at gmail.com> wrote:
>
>> OK, Sir.  I will try as per your suggestions.
>>
>> Thank you very much for your kind help.
>>
>> [image: Mailtrack]
>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
>> notified by
>> Mailtrack
>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 11/21/18,
>> 3:07:47 PM
>>
>> On Wed, Nov 21, 2018 at 2:41 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>>
>>> 1. xaxt="n" means "Don't display the X axis". See the help for "par" in
>>> the graphics package
>>>
>>> 2. axis(1,at=1:nrows,labels=names(MPG3))
>>> This means, "Display the bottom axis (1) with ticks at 1 to the number
>>> of rows in the data frame"
>>> "Use the values of MPG$Year as labels for the ticks". see the help for
>>> "axis" in the graphics package
>>> Note that this should be in the same loop as "plot"
>>>
>>> Now I can see that my guess at the structure of the data was wrong. What
>>> you could do is to collapse the daily records into the means for the years.
>>> As I don't know what your spreadsheet looks like, I could only guess a
>>> method for this.
>>>
>>> You seem to be saying that you plot all 5655 values, but you want the
>>> axis to show just the years.Rather than tell you to convert your data to a
>>> time series, I'll suggest a quick hack.
>>>
>>> axis(1,at=seq(1,5655,by=365),labels=1994:2014)
>>>
>>> This _may_ work for you. I offer it because I can see that you do not
>>> have a lot of experience in R and you want to get the job done. If you
>>> can't get it to work, I apologize and you can blamelessly move to something
>>> else.
>>>
>>> Jim
>>>
>>> PS - If you don't know how to start HTML help - help.start()
>>>
>>> On Wed, Nov 21, 2018 at 7:26 PM Subhamitra Patra <
>>> subhamitra.patra at gmail.com> wrote:
>>>
>>>>  Sir, in the bold portion of the below code, I have some confusion
>>>> which I am mentioning below that
>>>>
>>>> "ylab="MPG",main=names(MPG3)[i],*xaxt="n"*)
>>>>  axis(*1*,at=1:nrows,*labels=MPG3$Year*)"
>>>>
>>>> 1. Here, what *xaxt="n"* indicates? I think it indicates the no. of
>>>> rows, right?
>>>> 2. 1 in the 2nd line represents the no. of graphs. Let suppose, 38
>>>> plots are having the same row, I need to mention them as *axis(38,
>>>> at=1:nrows)*, right?
>>>> 3. *labels=**MPG3$Year *will give the name of all years in the X-axis,
>>>> right?
>>>>
>>>> Kindly correct me if I am wrong.
>>>>
>>>> Sir, here one thing I would like to ask, my data frequency is not
>>>> yearly. I obtained results from the daily data of the period from
>>>> 1994-2017 (that means the no. of rows will be 5655). But, as the daily
>>>> period is very unclear to mention in the X-axis, I wanted to give year name
>>>> as the name of the X-axis (that means, 1995, 1997, 1999 with the increment
>>>> of 2 years up to 2017).
>>>>
>>>> Sir, please suggest me how to proceed with this?
>>>>
>>>>
>>
>> --
>> *Best Regards,*
>> *Subhamitra Patra*
>> *Phd. Research Scholar*
>> *Department of Humanities and Social Sciences*
>> *Indian Institute of Technology, Kharagpur*
>> *INDIA*
>>
>
>
> --
> *Best Regards,*
> *Subhamitra Patra*
> *Phd. Research Scholar*
> *Department of Humanities and Social Sciences*
> *Indian Institute of Technology, Kharagpur*
> *INDIA*
>


-- 
*Best Regards,*
*Subhamitra Patra*
*Phd. Research Scholar*
*Department of Humanities and Social Sciences*
*Indian Institute of Technology, Kharagpur*
*INDIA*

	[[alternative HTML version deleted]]


From drjimlemon @ending from gm@il@com  Fri Dec 14 10:18:35 2018
From: drjimlemon @ending from gm@il@com (Jim Lemon)
Date: Fri, 14 Dec 2018 20:18:35 +1100
Subject: [R] 
 [R studio] Plotting of line chart for each columns at 1 page
In-Reply-To: <CAOFE=kMgKxo--vVCLhaPuY-4+5LD=HCZuyhBXBDFzUt0DG3ThA@mail.gmail.com>
References: <CAOFE=kOWXrnRnUVtL7A0vLKjL03571WQy5W_865hTFeBz28+tA@mail.gmail.com>
 <CA+8X3fW3Op8icpvA68O2sYPpBi=rpO2DWVXwL5UrPOHtDqDzpw@mail.gmail.com>
 <CAOFE=kNV6P4jRtrYS9K1E76pn+tW_aVpQ4Kq25ZHtQ6v-m=TkQ@mail.gmail.com>
 <CA+8X3fXM9KEb8=U1y-E_DFy9UuXk7NXDyRo6VppnEEAbAtGDDQ@mail.gmail.com>
 <CAOFE=kO20RsVsvVL2ZXpsH51aw9bzgCc3tDJ7DX8fM-tgkkbdQ@mail.gmail.com>
 <CA+8X3fUh8bug_m8iGBGcdPdb=PeGXY0xG48NTwBHC2XzXrtyhg@mail.gmail.com>
 <CAOFE=kNda7BE7cnkUg3nTZNAZbG0-dq=ir2ddXy_1pRMwySpcg@mail.gmail.com>
 <CA+8X3fVsMos2cafXXk7seO=fpM+JFgufmQpcUhGqJ9nNm4BFmQ@mail.gmail.com>
 <CAOFE=kOiTHOzcQ+dL2Tak2WnGBx1JAXpryj5CbLefdofa-7x5Q@mail.gmail.com>
 <CA+8X3fVYFJ9GUzY5ujH38zEQhWKKrkUpRzf1UjC-Cc=GHMe0VA@mail.gmail.com>
 <CAOFE=kOBPM-6bdaQKC3NXTXuAn_9OOA8oEvOGKhrGdUxBUAVkw@mail.gmail.com>
 <CA+8X3fUSY13o=khzBy9RA1_qsP1vkVqNAm=UP-xAd_cyLNrAVw@mail.gmail.com>
 <CAOFE=kPwv6LspZ=4RvL9x2JE7YSE9Ad+4Rrdhpmj0hyEHmVhxA@mail.gmail.com>
 <CAOFE=kN_tdNCzu50RhVM4-k7d6psEd=vZu7pD0htdczBKSn-HQ@mail.gmail.com>
 <CAOFE=kN6uZ9mKCkfDkz+xXLMxs3+WBu=A3YFAP-0UWt_b5V1FA@mail.gmail.com>
 <CAOFE=kMgKxo--vVCLhaPuY-4+5LD=HCZuyhBXBDFzUt0DG3ThA@mail.gmail.com>
Message-ID: <CA+8X3fUTof6=q9JBNnMNpbEqF_7Jxi4WnTPF+2-c62ihMqmSaQ@mail.gmail.com>

Hi Subhamitra,
As before, I don't have your data, so I cannot run your code. Similarly,
when you say the plot is looking unclear, I have almost no idea what that
means in terms of a plot that I could see and possibly correct. Let's start
at the top anyway. You set up an array of 20 plots, then plot 38 series.
This is going to cycle through your array almost twice, as each time you
plot, you step forward one plot in the array. At the end of the first Excel
data sheet, you will be at plot 18 out of the 20. You then display another
10 plots, leaving you at plot 8 in the array. Each time you plot into the
same section of the array, you will wipe out the previous plot. Maybe this
is why you are not getting what you want. Finally, you display a further
two plots, leaving you at plot 10. If I am correct, you will have plots 31
to 38 from sheet 1 in the bottom two rows, with plots 1 and 2 from sheet
two in positions 19 and 20, then 3 to 10 from sheet 2 in the top rows,
finishing off with the two plots from sheet 3 in the 9th and 10th positions
in the second row of the plot array. Of course I can't see what you
actually have plotted, so this is but a desperate guess. I apologize for
the complicated answer which is probably no use whatever, but without data
and hopefully the output of your code, I am unable to read your mind.

Jim


On Fri, Dec 14, 2018 at 4:26 PM Subhamitra Patra <subhamitra.patra at gmail.com>
wrote:

> Sorry not previous queries, the 3 queries which I mentioned in my last
> email.
>
> On Fri, Dec 14, 2018 at 10:52 AM Subhamitra Patra <
> subhamitra.patra at gmail.com> wrote:
>
>> Dear Sir,
>>
>> The method you suggested for arranging multiple plots in 1 pdf page has
>> finally worked.
>>
>> Thank you very much.
>>
>> But, I have 3 more queries as follows.
>>
>> 1. Is there any way that I can merge the respective plots from the
>> different excel files into 1? For instance, I have 3 excel files (i.e. EMs
>> 1, EMs 2 and EMs 3) having 4 plots each. I want to merge 1st plots from 3
>> excel files into 1, 2nd plots from 3 excel files into 1, similarly 3rd and
>> 4th plots from each respective files into 1?
>> 2. Is there any way to range the scale of Y-axis for all plots because I
>> think if the range of the Y-axis for each plot is same, it can be easy to
>> interpret and compare the results. I used Ylim function. But, after using
>> the Ylim function, the plot is looking unclear than before. For your
>> convenience, I am providing my code.
>>
>> pdf("EMs.pdf",width=20,height=20)
>> par(mfrow=c(5,4))
>> # import your first sheet here (38 columns)
>> ncolumns<-ncol(EMs1)
>> for(i in 1:ncolumns)
>>   plot(EMs1[,i],type="l",col = "Red", xlab="Time",
>>        ylab="APEn", ylim=c(0.1,2), main=names(EMs1)[i])
>> library(zoo)
>> *APEn=zoo(EMs1, seq(from = as.Date("1994-01-01"), to =
>> as.Date("2017-08-03"), by = 1))*
>> #import your second sheet here, (10 columns)
>> ncolumns<-ncol(EMs2)
>> for(i in 1:ncolumns)
>>   plot(EMs2[,i],type="l",col = "Red", xlab="Time",
>>        ylab="APEn",ylim=c(0.1,2), main=names(EMs2)[i])
>> *APEn=zoo(EMs2, seq(from = as.Date("1994-01-01"), to =
>> as.Date("2017-08-03"), by = 1))*
>> # import your Third sheet here, (2 columns)
>> ncolumns<-ncol(EMs3)
>> for(i in 1:ncolumns)
>>   plot(EMs3[,i],type="l",col = "Red", xlab="Time",
>>        ylab="APEn", ylim=c(0.1,2), main=names(EMs3)[i])
>> *APEn=zoo(EMs3, seq(from = as.Date("1996-01-01"), to =
>> as.Date("2017-08-03"), by = 1))  *
>> # finish plotting
>> dev.off()
>>
>> 3. I want the corresponding date column (which is the daily date) on
>> X-axis of each plot. Thus, I added 1 line after the plot in the above (i.e.
>> the bold line). Please suggest me Is it the correct way that I am doing?
>>
>>
>> Sir, I am new in R and doing by learning.
>>
>> Kindly suggest me regarding my previous queries for which I shall be
>> always grateful to you.
>>
>> Thank you very much for educating a new R learner.
>>
>>
>>
>>
>> On Wed, Nov 21, 2018 at 3:20 PM Subhamitra Patra <
>> subhamitra.patra at gmail.com> wrote:
>>
>>> Thank you very much for your suggestions and help.
>>>
>>> [image: Mailtrack]
>>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
>>> notified by
>>> Mailtrack
>>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 11/21/18,
>>> 3:19:55 PM
>>>
>>> On Wed, Nov 21, 2018 at 3:09 PM Subhamitra Patra <
>>> subhamitra.patra at gmail.com> wrote:
>>>
>>>> OK, Sir.  I will try as per your suggestions.
>>>>
>>>> Thank you very much for your kind help.
>>>>
>>>> [image: Mailtrack]
>>>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
>>>> notified by
>>>> Mailtrack
>>>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 11/21/18,
>>>> 3:07:47 PM
>>>>
>>>> On Wed, Nov 21, 2018 at 2:41 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>>>>
>>>>> 1. xaxt="n" means "Don't display the X axis". See the help for "par"
>>>>> in the graphics package
>>>>>
>>>>> 2. axis(1,at=1:nrows,labels=names(MPG3))
>>>>> This means, "Display the bottom axis (1) with ticks at 1 to the number
>>>>> of rows in the data frame"
>>>>> "Use the values of MPG$Year as labels for the ticks". see the help for
>>>>> "axis" in the graphics package
>>>>> Note that this should be in the same loop as "plot"
>>>>>
>>>>> Now I can see that my guess at the structure of the data was wrong.
>>>>> What you could do is to collapse the daily records into the means for the
>>>>> years. As I don't know what your spreadsheet looks like, I could only guess
>>>>> a method for this.
>>>>>
>>>>> You seem to be saying that you plot all 5655 values, but you want the
>>>>> axis to show just the years.Rather than tell you to convert your data to a
>>>>> time series, I'll suggest a quick hack.
>>>>>
>>>>> axis(1,at=seq(1,5655,by=365),labels=1994:2014)
>>>>>
>>>>> This _may_ work for you. I offer it because I can see that you do not
>>>>> have a lot of experience in R and you want to get the job done. If you
>>>>> can't get it to work, I apologize and you can blamelessly move to something
>>>>> else.
>>>>>
>>>>> Jim
>>>>>
>>>>> PS - If you don't know how to start HTML help - help.start()
>>>>>
>>>>> On Wed, Nov 21, 2018 at 7:26 PM Subhamitra Patra <
>>>>> subhamitra.patra at gmail.com> wrote:
>>>>>
>>>>>>  Sir, in the bold portion of the below code, I have some confusion
>>>>>> which I am mentioning below that
>>>>>>
>>>>>> "ylab="MPG",main=names(MPG3)[i],*xaxt="n"*)
>>>>>>  axis(*1*,at=1:nrows,*labels=MPG3$Year*)"
>>>>>>
>>>>>> 1. Here, what *xaxt="n"* indicates? I think it indicates the no. of
>>>>>> rows, right?
>>>>>> 2. 1 in the 2nd line represents the no. of graphs. Let suppose, 38
>>>>>> plots are having the same row, I need to mention them as *axis(38,
>>>>>> at=1:nrows)*, right?
>>>>>> 3. *labels=**MPG3$Year *will give the name of all years in the
>>>>>> X-axis, right?
>>>>>>
>>>>>> Kindly correct me if I am wrong.
>>>>>>
>>>>>> Sir, here one thing I would like to ask, my data frequency is not
>>>>>> yearly. I obtained results from the daily data of the period from
>>>>>> 1994-2017 (that means the no. of rows will be 5655). But, as the daily
>>>>>> period is very unclear to mention in the X-axis, I wanted to give year name
>>>>>> as the name of the X-axis (that means, 1995, 1997, 1999 with the increment
>>>>>> of 2 years up to 2017).
>>>>>>
>>>>>> Sir, please suggest me how to proceed with this?
>>>>>>
>>>>>>
>>>>
>>>> --
>>>> *Best Regards,*
>>>> *Subhamitra Patra*
>>>> *Phd. Research Scholar*
>>>> *Department of Humanities and Social Sciences*
>>>> *Indian Institute of Technology, Kharagpur*
>>>> *INDIA*
>>>>
>>>
>>>
>>> --
>>> *Best Regards,*
>>> *Subhamitra Patra*
>>> *Phd. Research Scholar*
>>> *Department of Humanities and Social Sciences*
>>> *Indian Institute of Technology, Kharagpur*
>>> *INDIA*
>>>
>>
>>
>> --
>> *Best Regards,*
>> *Subhamitra Patra*
>> *Phd. Research Scholar*
>> *Department of Humanities and Social Sciences*
>> *Indian Institute of Technology, Kharagpur*
>> *INDIA*
>>
>
>
> --
> *Best Regards,*
> *Subhamitra Patra*
> *Phd. Research Scholar*
> *Department of Humanities and Social Sciences*
> *Indian Institute of Technology, Kharagpur*
> *INDIA*
>

	[[alternative HTML version deleted]]


From giftedlife2014 @ending from gm@il@com  Fri Dec 14 12:38:33 2018
From: giftedlife2014 @ending from gm@il@com (Ogbos Okike)
Date: Fri, 14 Dec 2018 11:38:33 +0000
Subject: [R] Plotting Very Large lat-lon data in x-y axes graph: SOLVED
In-Reply-To: <CA+8X3fWFB6KxwmN1c0hfPpJTw15iM2X2OUAxSudnvuHK2KepOQ@mail.gmail.com>
References: <CAC8ss33ZS-8NqHf+6Q3Rk=qt+Z7+O7sVFV9Ahi+X8U=DqxhOsw@mail.gmail.com>
 <CA+8X3fU04pA_nh2DvXmY9vUWGr7bsj8AwRBp9Xd=qFvGmj6RMQ@mail.gmail.com>
 <CAC8ss32fAjoh1Q2+7x9i25EyXCGAbeqjYOtG4r3WuWi2S6st4A@mail.gmail.com>
 <CAC8ss33wF7Tp-Uh86S4XMUVxbA56-1atk4kEj67Tg+Ps-3LN-A@mail.gmail.com>
 <CA+8X3fUuz0y70LNc2H8TuR0y65h728vR2GA55o+iE1DRGK+o5Q@mail.gmail.com>
 <CAC8ss31zwzNH1VnksDjnJxQZgUVx0KecNydhP5ExNy6hwVAyXA@mail.gmail.com>
 <CA+8X3fWFB6KxwmN1c0hfPpJTw15iM2X2OUAxSudnvuHK2KepOQ@mail.gmail.com>
Message-ID: <CAC8ss31bxHyG_gt+jP1LX-fNiDwx=kw7D8JdFo=PWtdsqe4owA@mail.gmail.com>

Dear Jim,
Good news to me!! Welcome.

I am fine. The code elegantly displayed the color.

I also tried to adjust the line:
draw.circle(lonmids[lon],latmids[lat],radius=sqrt(counts[lat,lon])/100,
    border=countcol[lat,lon],col=countcol[lat,lon]) in order to reduce
the radius of the circle in order to plot the whole data.

I am really really grateful. I will immediately include the result in
a draft manuscript to Astrophysical Journal.

With the warmest regards
Ogbos

On Thu, Dec 13, 2018 at 9:04 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi Ogbos,
> Back on the air after a few days off. I don't have your data ("QUERY
> 2"), but I think this will fix your problem.
>
> library(maps)
> map("world")
> box()
> library(plotrix)
> color.legend(-180,-150,100,-130,legend=c(0,25000,50000,75000,100000),
>  rect.col=color.scale(1:5,extremes=c("blue","red")),gradient="x")
>
> Notice that I have swapped the "yb" and "yt" values so that they are
> in increasing order. If they are reversed, the numbers will appear
> within the color bar. Also you don't need to call color.gradient, just
> pass the output of a five increment color scale from blue to red (or
> whatever you like) to the rect.col argument.
>
> Jim


From D@vid@M@Smith @ending from micro@oft@com  Fri Dec 14 16:04:00 2018
From: D@vid@M@Smith @ending from micro@oft@com (David Smith (CDA))
Date: Fri, 14 Dec 2018 15:04:00 +0000
Subject: [R] Revolutions blog: November 2018 roundup
Message-ID: <DM5PR2101MB10967D3171C60F6B4EBBEB77B9A10@DM5PR2101MB1096.namprd21.prod.outlook.com>

For more than 10 years, Microsoft staff and guests have written about R at the
Revolutions blog (http://blog.revolutionanalytics.com) and every month I post a
summary of articles from the previous month of particular interest to readers of
r-help.

In case you missed them, here are some articles related to R from the
month of November:

David Gerard assesses the plausibility of a key plot point in 'Jurassic Park'
with simulations in R:
https://blog.revolutionanalytics.com/2018/11/jurassic-park.html

In-database R is available in Azure SQL Database for private preview:
https://blog.revolutionanalytics.com/2018/11/r-support-in-azure-sql-database.html

Introducing AzureR, a new suite of R packages for managing Azure resources in R:
https://blog.revolutionanalytics.com/2018/11/azurer-intro.html 

The AzureRMR package provides an interface for Resource Manager.
https://blog.revolutionanalytics.com/2018/11/azurermr-azure-resource-manager.html

Roundup of AI, Machine Learning and Data Science news from November 2018:
https://blog.revolutionanalytics.com/2018/11/ai-roundup-nov-2018.html

You can now use the AI capabilities of Microsoft Cognitive Services within a
container you host:
https://blog.revolutionanalytics.com/2018/11/cognitive-services-updates.html

A look back at some of the R applications presented at the EARL conference in
Seattle:
https://blog.revolutionanalytics.com/2018/11/report-from-earl-seattle.html

Slides and notebooks from my ODSC workshop, AI for Good:
https://blog.revolutionanalytics.com/2018/11/workshop-ai-for-good.html

T-Mobile uses AI models implemented with R to streamline customer service:
https://blog.revolutionanalytics.com/2018/11/t-mobile-uses-r.html

A guide to R packages for importing and working with US Census data:
https://blog.revolutionanalytics.com/2018/11/working-with-us-census-data-in-r.html

Azure Machine Learning Studio, the online drag-and-drop data analysis tool,
upgrades its R support:
https://blog.revolutionanalytics.com/2018/11/azure-ml-studio-r-34.html

And some general interest stories (not necessarily related to R):

* How the planets would look in the sky, if they were as close as the Moon:
  https://blog.revolutionanalytics.com/2018/11/because-its-friday-if-planets-were.html

* Pavarotti and Freddie Mercury, impersonated by the same performer:
https://blog.revolutionanalytics.com/2018/11/because-its-friday-pavarotti-v-mercury.html

* The real-world physics of the sci-fi series The Expanse:
https://blog.revolutionanalytics.com/2018/11/because-its-friday-the-physics-of-the-expanse.html

* Real robots and simulated people, dancing:
https://blog.revolutionanalytics.com/2018/11/because-its-friday-robot-dance.html

* The different ways human languages write animal sounds:
https://blog.revolutionanalytics.com/2018/11/because-its-friday-animal-sounds.html

As always, thanks for the comments and please keep sending suggestions to me at
davidsmi at microsoft.com or via Twitter (I'm @revodavid).

Cheers,
# David

-- 
David M Smith <davidsmi at microsoft.com>
Cloud Advocate, Microsoft Cloud & AI Developer Relations
Tel: +1 (312) 9205766 (Chicago IL, USA)
Twitter: @revodavid | Blog: ?http://blog.revolutionanalytics.com


From bh@t699 @ending from gm@il@com  Fri Dec 14 17:27:07 2018
From: bh@t699 @ending from gm@il@com (Madhavi Bhat)
Date: Fri, 14 Dec 2018 11:27:07 -0500
Subject: [R] Need help with R studio
Message-ID: <CAFrGcS-GdREWXjEr3DZeGjig=8_7XPoGh8D88tgontyGgLA7oQ@mail.gmail.com>

I am using HO Spectre windows and I have downloaded latest version of R
studio 3.5.1 for windows but its not working. I need help to fix this
issue. I am attaching screen shot of my R studio. Please help me in this
regard.
Thank you
Madhavi Bhat

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Screenshot (1).png
Type: image/png
Size: 247234 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20181214/39ab4c4e/attachment.png>

From jdnewmil @ending from dcn@d@vi@@c@@u@  Fri Dec 14 17:58:12 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Fri, 14 Dec 2018 08:58:12 -0800
Subject: [R] Need help with R studio
In-Reply-To: <CAFrGcS-GdREWXjEr3DZeGjig=8_7XPoGh8D88tgontyGgLA7oQ@mail.gmail.com>
References: <CAFrGcS-GdREWXjEr3DZeGjig=8_7XPoGh8D88tgontyGgLA7oQ@mail.gmail.com>
Message-ID: <A6FB9B27-2A37-494B-97D8-ADBE5A1C113F@dcn.davis.ca.us>

Looks fine to me, though you seem to be confused between what R is (on topic in this mailing list) and what RStudio is (a fine software package that relies on R but is not really on topic here).

Do find the link in the footer below about posting in R-help and read it before posting again.

On December 14, 2018 8:27:07 AM PST, Madhavi Bhat <bhat699 at gmail.com> wrote:
>I am using HO Spectre windows and I have downloaded latest version of R
>studio 3.5.1 for windows but its not working. I need help to fix this
>issue. I am attaching screen shot of my R studio. Please help me in
>this
>regard.
>Thank you
>Madhavi Bhat

-- 
Sent from my phone. Please excuse my brevity.


From bgunter@4567 @ending from gm@il@com  Fri Dec 14 18:23:09 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Fri, 14 Dec 2018 09:23:09 -0800
Subject: [R] Need help with R studio
In-Reply-To: <A6FB9B27-2A37-494B-97D8-ADBE5A1C113F@dcn.davis.ca.us>
References: <CAFrGcS-GdREWXjEr3DZeGjig=8_7XPoGh8D88tgontyGgLA7oQ@mail.gmail.com>
 <A6FB9B27-2A37-494B-97D8-ADBE5A1C113F@dcn.davis.ca.us>
Message-ID: <CAGxFJbSziaUnZA3GtVq0hK-oSR5TeTGtJy7gusFMFZFSjMThrA@mail.gmail.com>

... and R GUI is an interactive and command driven language. This means you
have to learn how how to use it like a programming language. Please spend
some time with R tutorials to do this -- there are many on the web.

... or see here for a GUI interface to some of R's basic, but most widely
used, functionality:

https://www.rdocumentation.org/packages/Rcmdr/versions/2.5-1


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Dec 14, 2018 at 8:58 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Looks fine to me, though you seem to be confused between what R is (on
> topic in this mailing list) and what RStudio is (a fine software package
> that relies on R but is not really on topic here).
>
> Do find the link in the footer below about posting in R-help and read it
> before posting again.
>
> On December 14, 2018 8:27:07 AM PST, Madhavi Bhat <bhat699 at gmail.com>
> wrote:
> >I am using HO Spectre windows and I have downloaded latest version of R
> >studio 3.5.1 for windows but its not working. I need help to fix this
> >issue. I am attaching screen shot of my R studio. Please help me in
> >this
> >regard.
> >Thank you
> >Madhavi Bhat
>
> --
> Sent from my phone. Please excuse my brevity.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bh@t699 @ending from gm@il@com  Fri Dec 14 18:02:48 2018
From: bh@t699 @ending from gm@il@com (Madhavi Bhat)
Date: Fri, 14 Dec 2018 12:02:48 -0500
Subject: [R] Need help with R studio
In-Reply-To: <A6FB9B27-2A37-494B-97D8-ADBE5A1C113F@dcn.davis.ca.us>
References: <CAFrGcS-GdREWXjEr3DZeGjig=8_7XPoGh8D88tgontyGgLA7oQ@mail.gmail.com>
 <A6FB9B27-2A37-494B-97D8-ADBE5A1C113F@dcn.davis.ca.us>
Message-ID: <CAFrGcS_TWH_fn3_CeajWm4z56brMfu372WFe=uyXLzSM2rm2+g@mail.gmail.com>

Thanks for the tip!

On Fri, Dec 14, 2018 at 11:58 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Looks fine to me, though you seem to be confused between what R is (on
> topic in this mailing list) and what RStudio is (a fine software package
> that relies on R but is not really on topic here).
>
> Do find the link in the footer below about posting in R-help and read it
> before posting again.
>
> On December 14, 2018 8:27:07 AM PST, Madhavi Bhat <bhat699 at gmail.com>
> wrote:
> >I am using HO Spectre windows and I have downloaded latest version of R
> >studio 3.5.1 for windows but its not working. I need help to fix this
> >issue. I am attaching screen shot of my R studio. Please help me in
> >this
> >regard.
> >Thank you
> >Madhavi Bhat
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From giftedlife2014 @ending from gm@il@com  Fri Dec 14 20:28:22 2018
From: giftedlife2014 @ending from gm@il@com (Ogbos Okike)
Date: Fri, 14 Dec 2018 19:28:22 +0000
Subject: [R] Plotting Very Large lat-lon data in x-y axes graph
In-Reply-To: <CA+8X3fWFB6KxwmN1c0hfPpJTw15iM2X2OUAxSudnvuHK2KepOQ@mail.gmail.com>
References: <CAC8ss33ZS-8NqHf+6Q3Rk=qt+Z7+O7sVFV9Ahi+X8U=DqxhOsw@mail.gmail.com>
 <CA+8X3fU04pA_nh2DvXmY9vUWGr7bsj8AwRBp9Xd=qFvGmj6RMQ@mail.gmail.com>
 <CAC8ss32fAjoh1Q2+7x9i25EyXCGAbeqjYOtG4r3WuWi2S6st4A@mail.gmail.com>
 <CAC8ss33wF7Tp-Uh86S4XMUVxbA56-1atk4kEj67Tg+Ps-3LN-A@mail.gmail.com>
 <CA+8X3fUuz0y70LNc2H8TuR0y65h728vR2GA55o+iE1DRGK+o5Q@mail.gmail.com>
 <CAC8ss31zwzNH1VnksDjnJxQZgUVx0KecNydhP5ExNy6hwVAyXA@mail.gmail.com>
 <CA+8X3fWFB6KxwmN1c0hfPpJTw15iM2X2OUAxSudnvuHK2KepOQ@mail.gmail.com>
Message-ID: <CAC8ss30K30cFYL09-vN0+QzFjuFCUA+jKGAeyyOt0UyAEM0ZsA@mail.gmail.com>

Dear Jim,
Attached is a plot generated by the code for one year lightning data.

Instead of lightning flashes happening randomly at all points over the
globe, there is a kind of constraint making lightning to be aligned
straight on the longitudes.

I suspect the problem is coming from seq function in the code:
latbreaks<-seq(latlim[1],latlim[2],by=2)
lonbreaks<-seq(lonlim[1],lonlim[2],by=10).

I tried to remove them but found it difficult.

Part of the data (see below) shows that lightning happens at all
latitudes and longitudes and as such, cannot be plotted to appear only
on straight lines:
0.8312   95.2156
 -3.3851  -65.3236
 -3.2696  -65.2364
 -3.2679  -65.2349
-17.6404  164.7025
 -4.8285  148.8214
  2.6477  -67.6568
 -0.2350  -73.4833
-16.8655   40.9587
  8.1179  -61.6474
 -0.2755   93.3401
-17.1733  119.9011
  1.1245  -69.7640
-20.0035 -149.3088
 -3.4200  177.8753
  3.0133  -67.5590
 15.4684  -21.9331
-17.6166  120.2656
-17.3888  165.9368
-17.6953  164.7335
 -1.8623  -74.0017
 -3.3562  -71.1195
-11.5775  130.1496
-11.5720  130.4004
 18.7159  -21.0251
 -9.3788  -75.0815
 -2.8341  -71.9095
 -2.8276  -71.9380
-19.0845  166.5715
  1.3692  -69.8615
  1.0989  -69.8881
  1.0351  -69.9390
  1.3306  -69.9484
 13.4238  -83.6240
-18.2490  165.4406
  2.8831  -67.6648
  3.0405  -67.4912
  1.2068  -70.0177
-34.6024  -67.2998
 -8.6378  -46.0881
-17.7834  119.7202
  2.5582  -69.4447
 -6.8894  -47.9581
-16.6939   40.7143
  1.4648  -69.7636
  4.5290  -62.5343
  4.6965  -62.6023
-18.9731  166.2414
 -3.6024  173.9586
 -3.4901  173.8333
 -3.8061 -176.4053
-16.7814  -53.0986
 -0.6284  -70.7341
 -3.1878  -71.8722
 -4.3205  176.7693
 -4.3616  176.8269
-16.8295  119.7462
  2.1829  -62.9190
-16.6656   42.0181
  3.5589  -61.7756
  1.3026  -69.9409
 33.3995   13.4344
  3.0125  -67.6869
  3.0155  -67.7028
 -7.3156  -48.0353
-17.5709  164.6291
 33.0699   14.3376
 -5.6291  -48.6772
 -2.8796  -71.8747
 -2.6448  -71.5548
 -3.0977  -71.5027
  1.1513  -69.8094
  1.1024  -69.8070
 -3.6739  -65.2433
 25.9860  -51.3312
 26.0156  -51.4377
 -6.1894  -56.5574
  3.2794   95.4703
 18.6850  -20.8606
  2.8957  -68.6864
  1.0930  -69.7733
  1.4467  -69.7631
-17.8441   27.0904
  1.4039  -69.9049
  0.9384  -69.8649
 -2.2356  170.8400
  2.7273  -67.6561
  5.2676  -62.7957
-16.9645  119.2204
 33.2544   13.6508
 -6.8735  -48.0914
-17.5725  120.1941
 10.8718  -59.0204
  1.9115  -62.8357
  2.4337   -4.8253
-17.2323  119.7706
 -7.4806  -47.9645
-13.0036  166.4029
-16.6861  -53.0932
-17.5506  164.3730
  2.2578  158.9620
 -2.9222  177.1345
  5.2906  -62.6111
  5.0903  -62.5855
  2.2920   91.7130
 -6.2985  -55.5143
  2.9048  -67.6558
  3.0313  -67.6569
 -1.5240  -71.0068
  1.4910  -69.3833
  3.0779  -67.7509
  2.8380  -67.7203
Thanks for more inputs.
Ogbos
On Thu, Dec 13, 2018 at 9:04 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi Ogbos,
> Back on the air after a few days off. I don't have your data ("QUERY
> 2"), but I think this will fix your problem.
>
> library(maps)
> map("world")
> box()
> library(plotrix)
> color.legend(-180,-150,100,-130,legend=c(0,25000,50000,75000,100000),
>  rect.col=color.scale(1:5,extremes=c("blue","red")),gradient="x")
>
> Notice that I have swapped the "yb" and "yt" values so that they are
> in increasing order. If they are reversed, the numbers will appear
> within the color bar. Also you don't need to call color.gradient, just
> pass the output of a five increment color scale from blue to red (or
> whatever you like) to the rect.col argument.
>
> Jim

-------------- next part --------------
A non-text attachment was scrubbed...
Name: testB2.png
Type: image/png
Size: 107473 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20181214/c584bd3d/attachment.png>

From profjcn@@h @ending from gm@il@com  Sat Dec 15 01:33:25 2018
From: profjcn@@h @ending from gm@il@com (J C Nash)
Date: Fri, 14 Dec 2018 19:33:25 -0500
Subject: [R] listing all files in a directory ending in ".xxx" in a script
Message-ID: <ac9701a3-5435-260d-9736-e17c478531fa@gmail.com>

When in a console (I was in Rstudio) I can run

dir("../provenance-of-rootfinding/", pattern="\\.Rmd")

to list all Rmd files in the specified directory.

However, when I try to run this in a script under

   Rscript --vanilla

I don't get the files to list.

Am I missing something obvious that I should know about?

JN


From profjcn@@h @ending from gm@il@com  Sat Dec 15 01:53:14 2018
From: profjcn@@h @ending from gm@il@com (J C Nash)
Date: Fri, 14 Dec 2018 19:53:14 -0500
Subject: [R] 
 listing all files in a directory ending in ".xxx" in a script
In-Reply-To: <ac9701a3-5435-260d-9736-e17c478531fa@gmail.com>
References: <ac9701a3-5435-260d-9736-e17c478531fa@gmail.com>
Message-ID: <89cdf269-4882-6c45-40b0-8d851e8f2e60@gmail.com>

Apologies for list noise. I just found a subtle typo so the command
was different from what it appeared to be in the actual use-case.
dir() seems to be working OK once that was fixed.

JN



-------- Forwarded Message --------
Subject: listing all files in a directory ending in ".xxx" in a script
Date: Fri, 14 Dec 2018 19:33:25 -0500
From: J C Nash <profjcnash at gmail.com>
To: r-help <r-help at R-project.org>

When in a console (I was in Rstudio) I can run

dir("../provenance-of-rootfinding/", pattern="\\.Rmd")

to list all Rmd files in the specified directory.

However, when I try to run this in a script under

   Rscript --vanilla

I don't get the files to list.

Am I missing something obvious that I should know about?

JN


From @ubh@mitr@@p@tr@ @ending from gm@il@com  Sat Dec 15 06:15:58 2018
From: @ubh@mitr@@p@tr@ @ending from gm@il@com (Subhamitra Patra)
Date: Sat, 15 Dec 2018 10:45:58 +0530
Subject: [R] 
 [R studio] Plotting of line chart for each columns at 1 page
In-Reply-To: <CA+8X3fUTof6=q9JBNnMNpbEqF_7Jxi4WnTPF+2-c62ihMqmSaQ@mail.gmail.com>
References: <CAOFE=kOWXrnRnUVtL7A0vLKjL03571WQy5W_865hTFeBz28+tA@mail.gmail.com>
 <CA+8X3fW3Op8icpvA68O2sYPpBi=rpO2DWVXwL5UrPOHtDqDzpw@mail.gmail.com>
 <CAOFE=kNV6P4jRtrYS9K1E76pn+tW_aVpQ4Kq25ZHtQ6v-m=TkQ@mail.gmail.com>
 <CA+8X3fXM9KEb8=U1y-E_DFy9UuXk7NXDyRo6VppnEEAbAtGDDQ@mail.gmail.com>
 <CAOFE=kO20RsVsvVL2ZXpsH51aw9bzgCc3tDJ7DX8fM-tgkkbdQ@mail.gmail.com>
 <CA+8X3fUh8bug_m8iGBGcdPdb=PeGXY0xG48NTwBHC2XzXrtyhg@mail.gmail.com>
 <CAOFE=kNda7BE7cnkUg3nTZNAZbG0-dq=ir2ddXy_1pRMwySpcg@mail.gmail.com>
 <CA+8X3fVsMos2cafXXk7seO=fpM+JFgufmQpcUhGqJ9nNm4BFmQ@mail.gmail.com>
 <CAOFE=kOiTHOzcQ+dL2Tak2WnGBx1JAXpryj5CbLefdofa-7x5Q@mail.gmail.com>
 <CA+8X3fVYFJ9GUzY5ujH38zEQhWKKrkUpRzf1UjC-Cc=GHMe0VA@mail.gmail.com>
 <CAOFE=kOBPM-6bdaQKC3NXTXuAn_9OOA8oEvOGKhrGdUxBUAVkw@mail.gmail.com>
 <CA+8X3fUSY13o=khzBy9RA1_qsP1vkVqNAm=UP-xAd_cyLNrAVw@mail.gmail.com>
 <CAOFE=kPwv6LspZ=4RvL9x2JE7YSE9Ad+4Rrdhpmj0hyEHmVhxA@mail.gmail.com>
 <CAOFE=kN_tdNCzu50RhVM4-k7d6psEd=vZu7pD0htdczBKSn-HQ@mail.gmail.com>
 <CAOFE=kN6uZ9mKCkfDkz+xXLMxs3+WBu=A3YFAP-0UWt_b5V1FA@mail.gmail.com>
 <CAOFE=kMgKxo--vVCLhaPuY-4+5LD=HCZuyhBXBDFzUt0DG3ThA@mail.gmail.com>
 <CA+8X3fUTof6=q9JBNnMNpbEqF_7Jxi4WnTPF+2-c62ihMqmSaQ@mail.gmail.com>
Message-ID: <CAOFE=kOUbBpUidnAxQjSzN4yhUBiaCC2GRgG8ovqm1BeuJ86ig@mail.gmail.com>

Hello Sir,

I am extremely Sorry for the late reply.

Ok now, I am sending my data and output, and would like to discuss my
queries one by one.

This is my final code.

pdf("EMs.pdf",width=20,height=20)
par(mfrow=c(5,4))
# import your first sheet here (16 columns)
ncolumns<-ncol(EMs1.1)
for(i in 1:ncolumns)
  plot(EMs1.1[,i],type="l",col = "Red", xlab="Time",
       ylab="APEn", main=names(EMs1.1)[i])
#import your second sheet here, (1 column)
ncolumns<-ncol(EMs2.1)
for(i in 1:ncolumns)
  plot(EMs2.1[,i],type="l",col = "Red", xlab="Time",
       ylab="APEn", main=names(EMs2.1)[i])
# import your Third sheet here, (1 column)
ncolumns<-ncol(EMs3.1)
for(i in 1:ncolumns)
  plot(EMs3.1[,i],type="l",col = "Red", xlab="Time",
       ylab="APEn", main=names(EMs3.1)[i])
# import your fourth sheet here, (1 column)
ncolumns<-ncol(EMs4.1)
for(i in 1:ncolumns)
  plot(EMs4.1[,i],type="l",col = "Red", xlab="Time",
       ylab="APEn", main=names(EMs4.1)[i])
# finish plotting
dev.off()

With this code, I found the following results. I attached The data files
(in rar file containing 4 excel files) and the output of my result.

*My First query is :*
I am having daily data, and without defining the date column, I obtained
the results. Therefore, I found the no. of observation in my X-axis of the
plots (in the attached result Pdf file). Now, I need the date column in my
X-axis with the corresponding data. I considered 03-01-1994 to 03-08-2017
(Date-Month-Year) by excluding 2 non-trading days per week.

I know that the frequency for defining yearly data is 1. So, I tried with
the following code being attached to the plot code, but not sure that
whether It is giving the appropriate plot or not?

*library(zoo)*
*y=zoo(EMs1, seq(from = as.Date("1994-01-01"), to = as.Date("2017-08-03"),
by = 1))*

*Therefore, kindly suggest me instead of getting the no. of observations in
the X-axis, how to get the date column in X-axis? *




[image: Mailtrack]
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
Sender
notified by
Mailtrack
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
12/15/18,
10:39:06 AM

On Fri, Dec 14, 2018 at 2:48 PM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Subhamitra,
> As before, I don't have your data, so I cannot run your code. Similarly,
> when you say the plot is looking unclear, I have almost no idea what that
> means in terms of a plot that I could see and possibly correct. Let's start
> at the top anyway. You set up an array of 20 plots, then plot 38 series.
> This is going to cycle through your array almost twice, as each time you
> plot, you step forward one plot in the array. At the end of the first Excel
> data sheet, you will be at plot 18 out of the 20. You then display another
> 10 plots, leaving you at plot 8 in the array. Each time you plot into the
> same section of the array, you will wipe out the previous plot. Maybe this
> is why you are not getting what you want. Finally, you display a further
> two plots, leaving you at plot 10. If I am correct, you will have plots 31
> to 38 from sheet 1 in the bottom two rows, with plots 1 and 2 from sheet
> two in positions 19 and 20, then 3 to 10 from sheet 2 in the top rows,
> finishing off with the two plots from sheet 3 in the 9th and 10th positions
> in the second row of the plot array. Of course I can't see what you
> actually have plotted, so this is but a desperate guess. I apologize for
> the complicated answer which is probably no use whatever, but without data
> and hopefully the output of your code, I am unable to read your mind.
>
> Jim
>
>
> On Fri, Dec 14, 2018 at 4:26 PM Subhamitra Patra <
> subhamitra.patra at gmail.com> wrote:
>
>> Sorry not previous queries, the 3 queries which I mentioned in my last
>> email.
>>
>> On Fri, Dec 14, 2018 at 10:52 AM Subhamitra Patra <
>> subhamitra.patra at gmail.com> wrote:
>>
>>> Dear Sir,
>>>
>>> The method you suggested for arranging multiple plots in 1 pdf page has
>>> finally worked.
>>>
>>> Thank you very much.
>>>
>>> But, I have 3 more queries as follows.
>>>
>>> 1. Is there any way that I can merge the respective plots from the
>>> different excel files into 1? For instance, I have 3 excel files (i.e. EMs
>>> 1, EMs 2 and EMs 3) having 4 plots each. I want to merge 1st plots from 3
>>> excel files into 1, 2nd plots from 3 excel files into 1, similarly 3rd and
>>> 4th plots from each respective files into 1?
>>> 2. Is there any way to range the scale of Y-axis for all plots because I
>>> think if the range of the Y-axis for each plot is same, it can be easy to
>>> interpret and compare the results. I used Ylim function. But, after using
>>> the Ylim function, the plot is looking unclear than before. For your
>>> convenience, I am providing my code.
>>>
>>> pdf("EMs.pdf",width=20,height=20)
>>> par(mfrow=c(5,4))
>>> # import your first sheet here (38 columns)
>>> ncolumns<-ncol(EMs1)
>>> for(i in 1:ncolumns)
>>>   plot(EMs1[,i],type="l",col = "Red", xlab="Time",
>>>        ylab="APEn", ylim=c(0.1,2), main=names(EMs1)[i])
>>> library(zoo)
>>> *APEn=zoo(EMs1, seq(from = as.Date("1994-01-01"), to =
>>> as.Date("2017-08-03"), by = 1))*
>>> #import your second sheet here, (10 columns)
>>> ncolumns<-ncol(EMs2)
>>> for(i in 1:ncolumns)
>>>   plot(EMs2[,i],type="l",col = "Red", xlab="Time",
>>>        ylab="APEn",ylim=c(0.1,2), main=names(EMs2)[i])
>>> *APEn=zoo(EMs2, seq(from = as.Date("1994-01-01"), to =
>>> as.Date("2017-08-03"), by = 1))*
>>> # import your Third sheet here, (2 columns)
>>> ncolumns<-ncol(EMs3)
>>> for(i in 1:ncolumns)
>>>   plot(EMs3[,i],type="l",col = "Red", xlab="Time",
>>>        ylab="APEn", ylim=c(0.1,2), main=names(EMs3)[i])
>>> *APEn=zoo(EMs3, seq(from = as.Date("1996-01-01"), to =
>>> as.Date("2017-08-03"), by = 1))  *
>>> # finish plotting
>>> dev.off()
>>>
>>> 3. I want the corresponding date column (which is the daily date) on
>>> X-axis of each plot. Thus, I added 1 line after the plot in the above (i.e.
>>> the bold line). Please suggest me Is it the correct way that I am doing?
>>>
>>>
>>> Sir, I am new in R and doing by learning.
>>>
>>> Kindly suggest me regarding my previous queries for which I shall be
>>> always grateful to you.
>>>
>>> Thank you very much for educating a new R learner.
>>>
>>>
>>>
>>>
>>> On Wed, Nov 21, 2018 at 3:20 PM Subhamitra Patra <
>>> subhamitra.patra at gmail.com> wrote:
>>>
>>>> Thank you very much for your suggestions and help.
>>>>
>>>> [image: Mailtrack]
>>>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
>>>> notified by
>>>> Mailtrack
>>>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 11/21/18,
>>>> 3:19:55 PM
>>>>
>>>> On Wed, Nov 21, 2018 at 3:09 PM Subhamitra Patra <
>>>> subhamitra.patra at gmail.com> wrote:
>>>>
>>>>> OK, Sir.  I will try as per your suggestions.
>>>>>
>>>>> Thank you very much for your kind help.
>>>>>
>>>>> [image: Mailtrack]
>>>>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
>>>>> notified by
>>>>> Mailtrack
>>>>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 11/21/18,
>>>>> 3:07:47 PM
>>>>>
>>>>> On Wed, Nov 21, 2018 at 2:41 PM Jim Lemon <drjimlemon at gmail.com>
>>>>> wrote:
>>>>>
>>>>>> 1. xaxt="n" means "Don't display the X axis". See the help for "par"
>>>>>> in the graphics package
>>>>>>
>>>>>> 2. axis(1,at=1:nrows,labels=names(MPG3))
>>>>>> This means, "Display the bottom axis (1) with ticks at 1 to the
>>>>>> number of rows in the data frame"
>>>>>> "Use the values of MPG$Year as labels for the ticks". see the help
>>>>>> for "axis" in the graphics package
>>>>>> Note that this should be in the same loop as "plot"
>>>>>>
>>>>>> Now I can see that my guess at the structure of the data was wrong.
>>>>>> What you could do is to collapse the daily records into the means for the
>>>>>> years. As I don't know what your spreadsheet looks like, I could only guess
>>>>>> a method for this.
>>>>>>
>>>>>> You seem to be saying that you plot all 5655 values, but you want the
>>>>>> axis to show just the years.Rather than tell you to convert your data to a
>>>>>> time series, I'll suggest a quick hack.
>>>>>>
>>>>>> axis(1,at=seq(1,5655,by=365),labels=1994:2014)
>>>>>>
>>>>>> This _may_ work for you. I offer it because I can see that you do not
>>>>>> have a lot of experience in R and you want to get the job done. If you
>>>>>> can't get it to work, I apologize and you can blamelessly move to something
>>>>>> else.
>>>>>>
>>>>>> Jim
>>>>>>
>>>>>> PS - If you don't know how to start HTML help - help.start()
>>>>>>
>>>>>> On Wed, Nov 21, 2018 at 7:26 PM Subhamitra Patra <
>>>>>> subhamitra.patra at gmail.com> wrote:
>>>>>>
>>>>>>>  Sir, in the bold portion of the below code, I have some confusion
>>>>>>> which I am mentioning below that
>>>>>>>
>>>>>>> "ylab="MPG",main=names(MPG3)[i],*xaxt="n"*)
>>>>>>>  axis(*1*,at=1:nrows,*labels=MPG3$Year*)"
>>>>>>>
>>>>>>> 1. Here, what *xaxt="n"* indicates? I think it indicates the no. of
>>>>>>> rows, right?
>>>>>>> 2. 1 in the 2nd line represents the no. of graphs. Let suppose, 38
>>>>>>> plots are having the same row, I need to mention them as *axis(38,
>>>>>>> at=1:nrows)*, right?
>>>>>>> 3. *labels=**MPG3$Year *will give the name of all years in the
>>>>>>> X-axis, right?
>>>>>>>
>>>>>>> Kindly correct me if I am wrong.
>>>>>>>
>>>>>>> Sir, here one thing I would like to ask, my data frequency is not
>>>>>>> yearly. I obtained results from the daily data of the period from
>>>>>>> 1994-2017 (that means the no. of rows will be 5655). But, as the daily
>>>>>>> period is very unclear to mention in the X-axis, I wanted to give year name
>>>>>>> as the name of the X-axis (that means, 1995, 1997, 1999 with the increment
>>>>>>> of 2 years up to 2017).
>>>>>>>
>>>>>>> Sir, please suggest me how to proceed with this?
>>>>>>>
>>>>>>>
>>>>>
>>>>> --
>>>>> *Best Regards,*
>>>>> *Subhamitra Patra*
>>>>> *Phd. Research Scholar*
>>>>> *Department of Humanities and Social Sciences*
>>>>> *Indian Institute of Technology, Kharagpur*
>>>>> *INDIA*
>>>>>
>>>>
>>>>
>>>> --
>>>> *Best Regards,*
>>>> *Subhamitra Patra*
>>>> *Phd. Research Scholar*
>>>> *Department of Humanities and Social Sciences*
>>>> *Indian Institute of Technology, Kharagpur*
>>>> *INDIA*
>>>>
>>>
>>>
>>> --
>>> *Best Regards,*
>>> *Subhamitra Patra*
>>> *Phd. Research Scholar*
>>> *Department of Humanities and Social Sciences*
>>> *Indian Institute of Technology, Kharagpur*
>>> *INDIA*
>>>
>>
>>
>> --
>> *Best Regards,*
>> *Subhamitra Patra*
>> *Phd. Research Scholar*
>> *Department of Humanities and Social Sciences*
>> *Indian Institute of Technology, Kharagpur*
>> *INDIA*
>>
>

-- 
*Best Regards,*
*Subhamitra Patra*
*Phd. Research Scholar*
*Department of Humanities and Social Sciences*
*Indian Institute of Technology, Kharagpur*
*INDIA*

-------------- next part --------------
A non-text attachment was scrubbed...
Name: EMs.pdf
Type: application/pdf
Size: 542618 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20181215/56c6cb99/attachment-0001.pdf>

From p@trick@gir@udoux @ending from univ-fcomte@fr  Sat Dec 15 14:37:19 2018
From: p@trick@gir@udoux @ending from univ-fcomte@fr (Patrick Giraudoux)
Date: Sat, 15 Dec 2018 14:37:19 +0100
Subject: [R] Read text files with Chinese characters
Message-ID: <d811bcf4-e15c-e093-13b3-196df7126147@univ-fcomte.fr>

Dear listers,

There is number of requests about reading Chinese characters from Excel 
or text files. I had to cope with the issue and wrote a small manual 
about it. It might not be an optimal solution, but at least it works :-)

One can download the pdf at: 
https://chrono-environnement.univ-fcomte.fr/personnes/annuaire/article/giraudoux-patrick?lang=en#chinese

Cheers,

Patrick



	[[alternative HTML version deleted]]


From profjcn@@h @ending from gm@il@com  Sat Dec 15 16:31:55 2018
From: profjcn@@h @ending from gm@il@com (J C Nash)
Date: Sat, 15 Dec 2018 10:31:55 -0500
Subject: [R] Dealing with special characters at end of line in file
Message-ID: <8e3ad1af-3f15-ab1f-3b08-6ac2b6b0b6a3@gmail.com>

I am trying to fix up some image files (jpg) that have comments in them.
Unfortunately, many have had extra special characters encoded.

rdjpgcom, called from an R script, returns a comment e.g.,

"In  Alvarez Cabral street by no. 105.\\000"

I want to get rid of "\\000", but sub seems
to be giving trouble.

> sub("\\000", "", ctxt)
[1] "In  Alvarez Cabral street by no. 105.\\0"

Anyone know how to resolve this?

JN


From dc@rl@on @ending from t@mu@edu  Sat Dec 15 16:40:08 2018
From: dc@rl@on @ending from t@mu@edu (David L Carlson)
Date: Sat, 15 Dec 2018 15:40:08 +0000
Subject: [R] Dealing with special characters at end of line in file
In-Reply-To: <8e3ad1af-3f15-ab1f-3b08-6ac2b6b0b6a3@gmail.com>
References: <8e3ad1af-3f15-ab1f-3b08-6ac2b6b0b6a3@gmail.com>
Message-ID: <00c61ca0aa4e4ab796e2bf9d04e26686@tamu.edu>

Each of the backslashes need to be escaped with a backslash:

> ctxt <- "In  Alvarez Cabral street by no. 105.\\000"
> sub("\\\\000", "", ctxt)
[1] "In  Alvarez Cabral street by no. 105."

-------------------------------------------------------
David L. Carlson
Department of Anthropology
Texas A&M University

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of J C Nash
Sent: Saturday, December 15, 2018 9:32 AM
To: r-help <r-help at R-project.org>
Subject: [R] Dealing with special characters at end of line in file

I am trying to fix up some image files (jpg) that have comments in them.
Unfortunately, many have had extra special characters encoded.

rdjpgcom, called from an R script, returns a comment e.g.,

"In  Alvarez Cabral street by no. 105.\\000"

I want to get rid of "\\000", but sub seems
to be giving trouble.

> sub("\\000", "", ctxt)
[1] "In  Alvarez Cabral street by no. 105.\\0"

Anyone know how to resolve this?

JN

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @ending from gm@il@com  Sat Dec 15 16:45:06 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Sat, 15 Dec 2018 07:45:06 -0800
Subject: [R] Dealing with special characters at end of line in file
In-Reply-To: <8e3ad1af-3f15-ab1f-3b08-6ac2b6b0b6a3@gmail.com>
References: <8e3ad1af-3f15-ab1f-3b08-6ac2b6b0b6a3@gmail.com>
Message-ID: <CAGxFJbRqUGOrE+ee433CAPUKwbb_B_JPWApU=op4o9+NAKXwDg@mail.gmail.com>

... or used the fixed = TRUE argument.

> z <-"In  Alvarez Cabral street by no. 105.\\000"

> sub("\\000","", z, fixed = TRUE)
[1] "In  Alvarez Cabral street by no. 105."


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Dec 15, 2018 at 7:32 AM J C Nash <profjcnash at gmail.com> wrote:

> I am trying to fix up some image files (jpg) that have comments in them.
> Unfortunately, many have had extra special characters encoded.
>
> rdjpgcom, called from an R script, returns a comment e.g.,
>
> "In  Alvarez Cabral street by no. 105.\\000"
>
> I want to get rid of "\\000", but sub seems
> to be giving trouble.
>
> > sub("\\000", "", ctxt)
> [1] "In  Alvarez Cabral street by no. 105.\\0"
>
> Anyone know how to resolve this?
>
> JN
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From profjcn@@h @ending from gm@il@com  Sat Dec 15 16:47:56 2018
From: profjcn@@h @ending from gm@il@com (J C Nash)
Date: Sat, 15 Dec 2018 10:47:56 -0500
Subject: [R] Dealing with special characters at end of line in file
In-Reply-To: <CAGxFJbRqUGOrE+ee433CAPUKwbb_B_JPWApU=op4o9+NAKXwDg@mail.gmail.com>
References: <8e3ad1af-3f15-ab1f-3b08-6ac2b6b0b6a3@gmail.com>
 <CAGxFJbRqUGOrE+ee433CAPUKwbb_B_JPWApU=op4o9+NAKXwDg@mail.gmail.com>
Message-ID: <d877aa21-6d84-f11d-9591-bdcdc6879433@gmail.com>

Many thanks. I'd misread the doc re fixed, and misunderstood the number of escapes. Sigh.

JN


On 2018-12-15 10:45 a.m., Bert Gunter wrote:
> ... or used the fixed = TRUE argument.
> 
>> z <-"In? Alvarez Cabral street by no. 105.\\000"
> 
>> sub("\\000","", z, fixed = TRUE)
> [1] "In? Alvarez Cabral street by no. 105."
> 
> 
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> 
> On Sat, Dec 15, 2018 at 7:32 AM J C Nash <profjcnash at gmail.com <mailto:profjcnash at gmail.com>> wrote:
> 
>     I am trying to fix up some image files (jpg) that have comments in them.
>     Unfortunately, many have had extra special characters encoded.
> 
>     rdjpgcom, called from an R script, returns a comment e.g.,
> 
>     "In? Alvarez Cabral street by no. 105.\\000"
> 
>     I want to get rid of "\\000", but sub seems
>     to be giving trouble.
> 
>     > sub("\\000", "", ctxt)
>     [1] "In? Alvarez Cabral street by no. 105.\\0"
> 
>     Anyone know how to resolve this?
> 
>     JN
> 
>     ______________________________________________
>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>     and provide commented, minimal, self-contained, reproducible code.
>


From drjimlemon @ending from gm@il@com  Sun Dec 16 03:40:10 2018
From: drjimlemon @ending from gm@il@com (Jim Lemon)
Date: Sun, 16 Dec 2018 13:40:10 +1100
Subject: [R] 
 [R studio] Plotting of line chart for each columns at 1 page
In-Reply-To: <CAOFE=kOUbBpUidnAxQjSzN4yhUBiaCC2GRgG8ovqm1BeuJ86ig@mail.gmail.com>
References: <CAOFE=kOWXrnRnUVtL7A0vLKjL03571WQy5W_865hTFeBz28+tA@mail.gmail.com>
 <CA+8X3fW3Op8icpvA68O2sYPpBi=rpO2DWVXwL5UrPOHtDqDzpw@mail.gmail.com>
 <CAOFE=kNV6P4jRtrYS9K1E76pn+tW_aVpQ4Kq25ZHtQ6v-m=TkQ@mail.gmail.com>
 <CA+8X3fXM9KEb8=U1y-E_DFy9UuXk7NXDyRo6VppnEEAbAtGDDQ@mail.gmail.com>
 <CAOFE=kO20RsVsvVL2ZXpsH51aw9bzgCc3tDJ7DX8fM-tgkkbdQ@mail.gmail.com>
 <CA+8X3fUh8bug_m8iGBGcdPdb=PeGXY0xG48NTwBHC2XzXrtyhg@mail.gmail.com>
 <CAOFE=kNda7BE7cnkUg3nTZNAZbG0-dq=ir2ddXy_1pRMwySpcg@mail.gmail.com>
 <CA+8X3fVsMos2cafXXk7seO=fpM+JFgufmQpcUhGqJ9nNm4BFmQ@mail.gmail.com>
 <CAOFE=kOiTHOzcQ+dL2Tak2WnGBx1JAXpryj5CbLefdofa-7x5Q@mail.gmail.com>
 <CA+8X3fVYFJ9GUzY5ujH38zEQhWKKrkUpRzf1UjC-Cc=GHMe0VA@mail.gmail.com>
 <CAOFE=kOBPM-6bdaQKC3NXTXuAn_9OOA8oEvOGKhrGdUxBUAVkw@mail.gmail.com>
 <CA+8X3fUSY13o=khzBy9RA1_qsP1vkVqNAm=UP-xAd_cyLNrAVw@mail.gmail.com>
 <CAOFE=kPwv6LspZ=4RvL9x2JE7YSE9Ad+4Rrdhpmj0hyEHmVhxA@mail.gmail.com>
 <CAOFE=kN_tdNCzu50RhVM4-k7d6psEd=vZu7pD0htdczBKSn-HQ@mail.gmail.com>
 <CAOFE=kN6uZ9mKCkfDkz+xXLMxs3+WBu=A3YFAP-0UWt_b5V1FA@mail.gmail.com>
 <CAOFE=kMgKxo--vVCLhaPuY-4+5LD=HCZuyhBXBDFzUt0DG3ThA@mail.gmail.com>
 <CA+8X3fUTof6=q9JBNnMNpbEqF_7Jxi4WnTPF+2-c62ihMqmSaQ@mail.gmail.com>
 <CAOFE=kOUbBpUidnAxQjSzN4yhUBiaCC2GRgG8ovqm1BeuJ86ig@mail.gmail.com>
Message-ID: <CA+8X3fWHutE7ibNvBWzdZWYCqQc1zpKr3EAidnN=oaGUgOmRKw@mail.gmail.com>

Hi Subhamitra,
Thanks. Now I can provide some assistance instead of just complaining. Your
first problem is the temporal extent of the data. There are 8613 days and
6512 weekdays between the two dates you list, but only 5655 observations in
your data. Therefore it is unlikely that you have a complete data series,
or perhaps you have the wrong dates. For the moment I'll assume that there
are missing observations. What I am going to do is to match the 24 years
(1994-2017) to their approximate positions in the time series. This will
give you the x-axis labels that you want, close enough for this
illustration. I doubt that you will need anything more accurate. You have a
span of 24.58 years, which means that if your missing observations are
uniformly distributed, you will have almost exactly 226 observations per
year. When i tried this, I got too many intervals, so I increased the
increment to 229 and that worked. To get the positions for the middle of
each year in the indices of the data:

year_mids<-seq(182,5655,by=229)

Now I suppress the x-axis by adding xaxt="n" to each call to plot. Then I
add a command to display the years at the positions I have calculated:

axis(1,at=year_mids,labels=1994:2017)

Also note that I have added braces to the "for" loop. Putting it all
together:

year_mids<-seq(182,5655,by=229)
pdf("EMs.pdf",width=20,height=20)
par(mfrow=c(5,4))
# import your first sheet here (16 columns)
EMs1.1<-read.csv("EMs1.1.csv")
ncolumns<-ncol(EMs1.1)
for(i in 1:ncolumns) {
  plot(EMs1.1[,i],type="l",col = "Red", xlab="Time",
       ylab="APEn", main=names(EMs1.1)[i],xaxt="n")
 axis(1,at=year_mids,labels=1994:2017)
}
#import your second sheet here, (1 column)
EMs2.1<-read.csv("EMs2.1.csv")
ncolumns<-ncol(EMs2.1)
for(i in 1:ncolumns) {
  plot(EMs2.1[,i],type="l",col = "Red", xlab="Time",
       ylab="APEn", main=names(EMs2.1)[i],xaxt="n")
 axis(1,at=year_mids,labels=1994:2017)
}
# import your Third sheet here, (1 column)
EMs3.1<-read.csv("EMs3.1.csv")
ncolumns<-ncol(EMs3.1)
for(i in 1:ncolumns) {
  plot(EMs3.1[,i],type="l",col = "Red", xlab="Time",
       ylab="APEn", main=names(EMs3.1)[i],xaxt="n")
 axis(1,at=year_mids,labels=1994:2017)
}
# import your fourth sheet here, (1 column)
EMs4.1<-read.csv("EMs4.1.csv")
ncolumns<-ncol(EMs4.1)
for(i in 1:ncolumns) {
  plot(EMs4.1[,i],type="l",col = "Red", xlab="Time",
       ylab="APEn", main=names(EMs4.1)[i],xaxt="n")
 axis(1,at=year_mids,labels=1994:2017)
}
# finish plotting
dev.off()

With any luck, you are now okay. Remember, this is a hack to deal with data
that are not what you think they are.

Jim

	[[alternative HTML version deleted]]


From e@@wiek @ending from gm@il@com  Sun Dec 16 04:28:01 2018
From: e@@wiek @ending from gm@il@com (Ek Esawi)
Date: Sat, 15 Dec 2018 22:28:01 -0500
Subject: [R] Combine lists into a data frame or append them to a text file
Message-ID: <CA+ZkTxvozwNSzB5qrT3xSK_fQiQTnsf15j1DBT5stcy-H_iEow@mail.gmail.com>

Hi All,

I have an R object that is made up of N number of lists which are all
of different number of columns and rows.  I want to combine the N
lists into a single data frame or write (append) them into text file.
I hope the question is clear and doesn?t require an example. I am
hoping to accomplish this using base R functions.
Below is what I tried but both gave me the same error which I do
understand, I think, but I don?t know how to fix it. My R object is
MyTables

lapply(MyTables, function(x) write.table(x, file = "Temp.txt",append = TRUE ))
OR
for (i in 1:length(MyTables)) {
write.table(MyTables[i], file = "Temp.txt",append = TRUE,quote = TRUE)

the error
Error in (function (..., row.names = NULL, check.rows = FALSE,
check.names = TRUE,  :
  arguments imply differing number of rows: 51, 8, 30

Thanks--EK


From bgunter@4567 @ending from gm@il@com  Sun Dec 16 06:04:18 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Sat, 15 Dec 2018 21:04:18 -0800
Subject: [R] 
 Combine lists into a data frame or append them to a text file
In-Reply-To: <CA+ZkTxvozwNSzB5qrT3xSK_fQiQTnsf15j1DBT5stcy-H_iEow@mail.gmail.com>
References: <CA+ZkTxvozwNSzB5qrT3xSK_fQiQTnsf15j1DBT5stcy-H_iEow@mail.gmail.com>
Message-ID: <CAGxFJbQN4ufKtmVJL0kj18MYAajKqS6d0oPFUn+U8nuASTikbw@mail.gmail.com>

FWIW, I had no trouble writing a test case to a file with either version of
your code. As we have no idea what your data look like, I don't know how
anyone can diagnose the problem. But maybe I'm wrong and someone else will
recognize the issue.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Dec 15, 2018 at 7:28 PM Ek Esawi <esawiek at gmail.com> wrote:

> Hi All,
>
> I have an R object that is made up of N number of lists which are all
> of different number of columns and rows.  I want to combine the N
> lists into a single data frame or write (append) them into text file.
> I hope the question is clear and doesn?t require an example. I am
> hoping to accomplish this using base R functions.
> Below is what I tried but both gave me the same error which I do
> understand, I think, but I don?t know how to fix it. My R object is
> MyTables
>
> lapply(MyTables, function(x) write.table(x, file = "Temp.txt",append =
> TRUE ))
> OR
> for (i in 1:length(MyTables)) {
> write.table(MyTables[i], file = "Temp.txt",append = TRUE,quote = TRUE)
>
> the error
> Error in (function (..., row.names = NULL, check.rows = FALSE,
> check.names = TRUE,  :
>   arguments imply differing number of rows: 51, 8, 30
>
> Thanks--EK
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drjimlemon @ending from gm@il@com  Sun Dec 16 07:03:27 2018
From: drjimlemon @ending from gm@il@com (Jim Lemon)
Date: Sun, 16 Dec 2018 17:03:27 +1100
Subject: [R] 
 Combine lists into a data frame or append them to a text file
In-Reply-To: <CA+ZkTxvozwNSzB5qrT3xSK_fQiQTnsf15j1DBT5stcy-H_iEow@mail.gmail.com>
References: <CA+ZkTxvozwNSzB5qrT3xSK_fQiQTnsf15j1DBT5stcy-H_iEow@mail.gmail.com>
Message-ID: <CA+8X3fVSN7+raqt0Uo8UtR65TsykgZsAwoWPqKKzDCftjjT8QQ@mail.gmail.com>

Hi Ek,
I thought there would be a simple fix for this, but had to write a
little function:

fillList<-function(x) {
 maxrows<-max(unlist(lapply(x,length)))
 return(lapply(x,"[",1:maxrows))
}

that fills up the rows of each list with NAs. I got the expected result with:

testlist<-list(a=1:8,b=1:9,c=1:10)
as.data.frame(fillList(testlist))

so:

for (i in 1:length(MyTables)) {
write.table(as.data.frame(fillList(MyTables[i])),
 file = "Temp.txt",append = TRUE,quote = TRUE)

may do the job.

Jim

On Sun, Dec 16, 2018 at 2:28 PM Ek Esawi <esawiek at gmail.com> wrote:
>
> Hi All,
>
> I have an R object that is made up of N number of lists which are all
> of different number of columns and rows.  I want to combine the N
> lists into a single data frame or write (append) them into text file.
> I hope the question is clear and doesn?t require an example. I am
> hoping to accomplish this using base R functions.
> Below is what I tried but both gave me the same error which I do
> understand, I think, but I don?t know how to fix it. My R object is
> MyTables
>
> lapply(MyTables, function(x) write.table(x, file = "Temp.txt",append = TRUE ))
> OR
> for (i in 1:length(MyTables)) {
> write.table(MyTables[i], file = "Temp.txt",append = TRUE,quote = TRUE)
>
> the error
> Error in (function (..., row.names = NULL, check.rows = FALSE,
> check.names = TRUE,  :
>   arguments imply differing number of rows: 51, 8, 30
>
> Thanks--EK
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @ubh@mitr@@p@tr@ @ending from gm@il@com  Sun Dec 16 07:54:41 2018
From: @ubh@mitr@@p@tr@ @ending from gm@il@com (Subhamitra Patra)
Date: Sun, 16 Dec 2018 12:24:41 +0530
Subject: [R] 
 [R studio] Plotting of line chart for each columns at 1 page
In-Reply-To: <CA+8X3fWHutE7ibNvBWzdZWYCqQc1zpKr3EAidnN=oaGUgOmRKw@mail.gmail.com>
References: <CAOFE=kOWXrnRnUVtL7A0vLKjL03571WQy5W_865hTFeBz28+tA@mail.gmail.com>
 <CA+8X3fW3Op8icpvA68O2sYPpBi=rpO2DWVXwL5UrPOHtDqDzpw@mail.gmail.com>
 <CAOFE=kNV6P4jRtrYS9K1E76pn+tW_aVpQ4Kq25ZHtQ6v-m=TkQ@mail.gmail.com>
 <CA+8X3fXM9KEb8=U1y-E_DFy9UuXk7NXDyRo6VppnEEAbAtGDDQ@mail.gmail.com>
 <CAOFE=kO20RsVsvVL2ZXpsH51aw9bzgCc3tDJ7DX8fM-tgkkbdQ@mail.gmail.com>
 <CA+8X3fUh8bug_m8iGBGcdPdb=PeGXY0xG48NTwBHC2XzXrtyhg@mail.gmail.com>
 <CAOFE=kNda7BE7cnkUg3nTZNAZbG0-dq=ir2ddXy_1pRMwySpcg@mail.gmail.com>
 <CA+8X3fVsMos2cafXXk7seO=fpM+JFgufmQpcUhGqJ9nNm4BFmQ@mail.gmail.com>
 <CAOFE=kOiTHOzcQ+dL2Tak2WnGBx1JAXpryj5CbLefdofa-7x5Q@mail.gmail.com>
 <CA+8X3fVYFJ9GUzY5ujH38zEQhWKKrkUpRzf1UjC-Cc=GHMe0VA@mail.gmail.com>
 <CAOFE=kOBPM-6bdaQKC3NXTXuAn_9OOA8oEvOGKhrGdUxBUAVkw@mail.gmail.com>
 <CA+8X3fUSY13o=khzBy9RA1_qsP1vkVqNAm=UP-xAd_cyLNrAVw@mail.gmail.com>
 <CAOFE=kPwv6LspZ=4RvL9x2JE7YSE9Ad+4Rrdhpmj0hyEHmVhxA@mail.gmail.com>
 <CAOFE=kN_tdNCzu50RhVM4-k7d6psEd=vZu7pD0htdczBKSn-HQ@mail.gmail.com>
 <CAOFE=kN6uZ9mKCkfDkz+xXLMxs3+WBu=A3YFAP-0UWt_b5V1FA@mail.gmail.com>
 <CAOFE=kMgKxo--vVCLhaPuY-4+5LD=HCZuyhBXBDFzUt0DG3ThA@mail.gmail.com>
 <CA+8X3fUTof6=q9JBNnMNpbEqF_7Jxi4WnTPF+2-c62ihMqmSaQ@mail.gmail.com>
 <CAOFE=kOUbBpUidnAxQjSzN4yhUBiaCC2GRgG8ovqm1BeuJ86ig@mail.gmail.com>
 <CA+8X3fWHutE7ibNvBWzdZWYCqQc1zpKr3EAidnN=oaGUgOmRKw@mail.gmail.com>
Message-ID: <CAOFE=kOatoPvDrY6bFBqnLHojJ08HL6qMrngV4dLYpyW+Eewkg@mail.gmail.com>

Thank you very much sir. Actually, I excluded all the non-trading days.
Therefore, Each year will have 226 observations and total 6154 observations
for each column. The data which I plotted is not rough data. I obtained the
rolling observations of window 500 from my original data. So, the no. of
observations for each resulted column is (6154-500)+1=5655. So, It is not
accurate as per the days of calculations of each year.

Ok, Sir, I will go through your suggestion, obtain the results for each
column of my data and would like to discuss the results with you. After
solving of this problem, I would like to discuss another 2 queries.

Thank you very much Sir for educating a new R learner.

[image: Mailtrack]
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
Sender
notified by
Mailtrack
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
12/16/18,
12:20:17 PM

On Sun, Dec 16, 2018 at 8:10 AM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Subhamitra,
> Thanks. Now I can provide some assistance instead of just complaining.
> Your first problem is the temporal extent of the data. There are 8613 days
> and 6512 weekdays between the two dates you list, but only 5655
> observations in your data. Therefore it is unlikely that you have a
> complete data series, or perhaps you have the wrong dates. For the moment
> I'll assume that there are missing observations. What I am going to do is
> to match the 24 years (1994-2017) to their approximate positions in the
> time series. This will give you the x-axis labels that you want, close
> enough for this illustration. I doubt that you will need anything more
> accurate. You have a span of 24.58 years, which means that if your missing
> observations are uniformly distributed, you will have almost exactly 226
> observations per year. When i tried this, I got too many intervals, so I
> increased the increment to 229 and that worked. To get the positions for
> the middle of each year in the indices of the data:
>
> year_mids<-seq(182,5655,by=229)
>
> Now I suppress the x-axis by adding xaxt="n" to each call to plot. Then I
> add a command to display the years at the positions I have calculated:
>
> axis(1,at=year_mids,labels=1994:2017)
>
> Also note that I have added braces to the "for" loop. Putting it all
> together:
>
> year_mids<-seq(182,5655,by=229)
> pdf("EMs.pdf",width=20,height=20)
> par(mfrow=c(5,4))
> # import your first sheet here (16 columns)
> EMs1.1<-read.csv("EMs1.1.csv")
> ncolumns<-ncol(EMs1.1)
> for(i in 1:ncolumns) {
>   plot(EMs1.1[,i],type="l",col = "Red", xlab="Time",
>        ylab="APEn", main=names(EMs1.1)[i],xaxt="n")
>  axis(1,at=year_mids,labels=1994:2017)
> }
> #import your second sheet here, (1 column)
> EMs2.1<-read.csv("EMs2.1.csv")
> ncolumns<-ncol(EMs2.1)
> for(i in 1:ncolumns) {
>   plot(EMs2.1[,i],type="l",col = "Red", xlab="Time",
>        ylab="APEn", main=names(EMs2.1)[i],xaxt="n")
>  axis(1,at=year_mids,labels=1994:2017)
> }
> # import your Third sheet here, (1 column)
> EMs3.1<-read.csv("EMs3.1.csv")
> ncolumns<-ncol(EMs3.1)
> for(i in 1:ncolumns) {
>   plot(EMs3.1[,i],type="l",col = "Red", xlab="Time",
>        ylab="APEn", main=names(EMs3.1)[i],xaxt="n")
>  axis(1,at=year_mids,labels=1994:2017)
> }
> # import your fourth sheet here, (1 column)
> EMs4.1<-read.csv("EMs4.1.csv")
> ncolumns<-ncol(EMs4.1)
> for(i in 1:ncolumns) {
>   plot(EMs4.1[,i],type="l",col = "Red", xlab="Time",
>        ylab="APEn", main=names(EMs4.1)[i],xaxt="n")
>  axis(1,at=year_mids,labels=1994:2017)
> }
> # finish plotting
> dev.off()
>
> With any luck, you are now okay. Remember, this is a hack to deal with
> data that are not what you think they are.
>
> Jim
>
>

-- 
*Best Regards,*
*Subhamitra Patra*
*Phd. Research Scholar*
*Department of Humanities and Social Sciences*
*Indian Institute of Technology, Kharagpur*
*INDIA*

	[[alternative HTML version deleted]]


From @ubh@mitr@@p@tr@ @ending from gm@il@com  Sun Dec 16 09:03:35 2018
From: @ubh@mitr@@p@tr@ @ending from gm@il@com (Subhamitra Patra)
Date: Sun, 16 Dec 2018 13:33:35 +0530
Subject: [R] 
 [R studio] Plotting of line chart for each columns at 1 page
In-Reply-To: <CAOFE=kOatoPvDrY6bFBqnLHojJ08HL6qMrngV4dLYpyW+Eewkg@mail.gmail.com>
References: <CAOFE=kOWXrnRnUVtL7A0vLKjL03571WQy5W_865hTFeBz28+tA@mail.gmail.com>
 <CA+8X3fW3Op8icpvA68O2sYPpBi=rpO2DWVXwL5UrPOHtDqDzpw@mail.gmail.com>
 <CAOFE=kNV6P4jRtrYS9K1E76pn+tW_aVpQ4Kq25ZHtQ6v-m=TkQ@mail.gmail.com>
 <CA+8X3fXM9KEb8=U1y-E_DFy9UuXk7NXDyRo6VppnEEAbAtGDDQ@mail.gmail.com>
 <CAOFE=kO20RsVsvVL2ZXpsH51aw9bzgCc3tDJ7DX8fM-tgkkbdQ@mail.gmail.com>
 <CA+8X3fUh8bug_m8iGBGcdPdb=PeGXY0xG48NTwBHC2XzXrtyhg@mail.gmail.com>
 <CAOFE=kNda7BE7cnkUg3nTZNAZbG0-dq=ir2ddXy_1pRMwySpcg@mail.gmail.com>
 <CA+8X3fVsMos2cafXXk7seO=fpM+JFgufmQpcUhGqJ9nNm4BFmQ@mail.gmail.com>
 <CAOFE=kOiTHOzcQ+dL2Tak2WnGBx1JAXpryj5CbLefdofa-7x5Q@mail.gmail.com>
 <CA+8X3fVYFJ9GUzY5ujH38zEQhWKKrkUpRzf1UjC-Cc=GHMe0VA@mail.gmail.com>
 <CAOFE=kOBPM-6bdaQKC3NXTXuAn_9OOA8oEvOGKhrGdUxBUAVkw@mail.gmail.com>
 <CA+8X3fUSY13o=khzBy9RA1_qsP1vkVqNAm=UP-xAd_cyLNrAVw@mail.gmail.com>
 <CAOFE=kPwv6LspZ=4RvL9x2JE7YSE9Ad+4Rrdhpmj0hyEHmVhxA@mail.gmail.com>
 <CAOFE=kN_tdNCzu50RhVM4-k7d6psEd=vZu7pD0htdczBKSn-HQ@mail.gmail.com>
 <CAOFE=kN6uZ9mKCkfDkz+xXLMxs3+WBu=A3YFAP-0UWt_b5V1FA@mail.gmail.com>
 <CAOFE=kMgKxo--vVCLhaPuY-4+5LD=HCZuyhBXBDFzUt0DG3ThA@mail.gmail.com>
 <CA+8X3fUTof6=q9JBNnMNpbEqF_7Jxi4WnTPF+2-c62ihMqmSaQ@mail.gmail.com>
 <CAOFE=kOUbBpUidnAxQjSzN4yhUBiaCC2GRgG8ovqm1BeuJ86ig@mail.gmail.com>
 <CA+8X3fWHutE7ibNvBWzdZWYCqQc1zpKr3EAidnN=oaGUgOmRKw@mail.gmail.com>
 <CAOFE=kOatoPvDrY6bFBqnLHojJ08HL6qMrngV4dLYpyW+Eewkg@mail.gmail.com>
Message-ID: <CAOFE=kMPdr6RDDJCKV_Uw73uEUKV=RgOatrRaxnwsZgDT1rrOw@mail.gmail.com>

Hello Sir,

I have three queries regarding your suggested code.

*1. *In my last email, I mentioned why there are missing observations in my
data series. In the line, *year_mids<-seq(182,5655,by=229), *

*A. what 182 indicates and what is the logic behind the consideration of
229 increments, although there are 226 observations per year?*
*B.  Each excel file is having different observations depending on the
variation of starting dates. So, is it required to add  **year_mids in the
loop? I think I need to justify **year_mids object each time after
importing the individual excel files. If I am wrong, kindly correct me.*

2. Further, in the command* axis(1,at=year_mids,labels=1994:2017), 1
indicates the no. of increments of year name, right?*

Kindly clarify my queries Sir for which I shall be always grateful to you.

Thank you very much.

[image: Mailtrack]
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
Sender
notified by
Mailtrack
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
12/16/18,
1:29:05 PM

On Sun, Dec 16, 2018 at 12:24 PM Subhamitra Patra <
subhamitra.patra at gmail.com> wrote:

> Thank you very much sir. Actually, I excluded all the non-trading days.
> Therefore, Each year will have 226 observations and total 6154 observations
> for each column. The data which I plotted is not rough data. I obtained the
> rolling observations of window 500 from my original data. So, the no. of
> observations for each resulted column is (6154-500)+1=5655. So, It is not
> accurate as per the days of calculations of each year.
>
> Ok, Sir, I will go through your suggestion, obtain the results for each
> column of my data and would like to discuss the results with you. After
> solving of this problem, I would like to discuss another 2 queries.
>
> Thank you very much Sir for educating a new R learner.
>
> [image: Mailtrack]
> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
> notified by
> Mailtrack
> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 12/16/18,
> 12:20:17 PM
>
> On Sun, Dec 16, 2018 at 8:10 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>
>> Hi Subhamitra,
>> Thanks. Now I can provide some assistance instead of just complaining.
>> Your first problem is the temporal extent of the data. There are 8613 days
>> and 6512 weekdays between the two dates you list, but only 5655
>> observations in your data. Therefore it is unlikely that you have a
>> complete data series, or perhaps you have the wrong dates. For the moment
>> I'll assume that there are missing observations. What I am going to do is
>> to match the 24 years (1994-2017) to their approximate positions in the
>> time series. This will give you the x-axis labels that you want, close
>> enough for this illustration. I doubt that you will need anything more
>> accurate. You have a span of 24.58 years, which means that if your missing
>> observations are uniformly distributed, you will have almost exactly 226
>> observations per year. When i tried this, I got too many intervals, so I
>> increased the increment to 229 and that worked. To get the positions for
>> the middle of each year in the indices of the data:
>>
>> year_mids<-seq(182,5655,by=229)
>>
>> Now I suppress the x-axis by adding xaxt="n" to each call to plot. Then I
>> add a command to display the years at the positions I have calculated:
>>
>> axis(1,at=year_mids,labels=1994:2017)
>>
>> Also note that I have added braces to the "for" loop. Putting it all
>> together:
>>
>> year_mids<-seq(182,5655,by=229)
>> pdf("EMs.pdf",width=20,height=20)
>> par(mfrow=c(5,4))
>> # import your first sheet here (16 columns)
>> EMs1.1<-read.csv("EMs1.1.csv")
>> ncolumns<-ncol(EMs1.1)
>> for(i in 1:ncolumns) {
>>   plot(EMs1.1[,i],type="l",col = "Red", xlab="Time",
>>        ylab="APEn", main=names(EMs1.1)[i],xaxt="n")
>>  axis(1,at=year_mids,labels=1994:2017)
>> }
>> #import your second sheet here, (1 column)
>> EMs2.1<-read.csv("EMs2.1.csv")
>> ncolumns<-ncol(EMs2.1)
>> for(i in 1:ncolumns) {
>>   plot(EMs2.1[,i],type="l",col = "Red", xlab="Time",
>>        ylab="APEn", main=names(EMs2.1)[i],xaxt="n")
>>  axis(1,at=year_mids,labels=1994:2017)
>> }
>> # import your Third sheet here, (1 column)
>> EMs3.1<-read.csv("EMs3.1.csv")
>> ncolumns<-ncol(EMs3.1)
>> for(i in 1:ncolumns) {
>>   plot(EMs3.1[,i],type="l",col = "Red", xlab="Time",
>>        ylab="APEn", main=names(EMs3.1)[i],xaxt="n")
>>  axis(1,at=year_mids,labels=1994:2017)
>> }
>> # import your fourth sheet here, (1 column)
>> EMs4.1<-read.csv("EMs4.1.csv")
>> ncolumns<-ncol(EMs4.1)
>> for(i in 1:ncolumns) {
>>   plot(EMs4.1[,i],type="l",col = "Red", xlab="Time",
>>        ylab="APEn", main=names(EMs4.1)[i],xaxt="n")
>>  axis(1,at=year_mids,labels=1994:2017)
>> }
>> # finish plotting
>> dev.off()
>>
>> With any luck, you are now okay. Remember, this is a hack to deal with
>> data that are not what you think they are.
>>
>> Jim
>>
>>
>
> --
> *Best Regards,*
> *Subhamitra Patra*
> *Phd. Research Scholar*
> *Department of Humanities and Social Sciences*
> *Indian Institute of Technology, Kharagpur*
> *INDIA*
>


-- 
*Best Regards,*
*Subhamitra Patra*
*Phd. Research Scholar*
*Department of Humanities and Social Sciences*
*Indian Institute of Technology, Kharagpur*
*INDIA*

	[[alternative HTML version deleted]]


From drjimlemon @ending from gm@il@com  Sun Dec 16 10:28:37 2018
From: drjimlemon @ending from gm@il@com (Jim Lemon)
Date: Sun, 16 Dec 2018 20:28:37 +1100
Subject: [R] 
 [R studio] Plotting of line chart for each columns at 1 page
In-Reply-To: <CAOFE=kMPdr6RDDJCKV_Uw73uEUKV=RgOatrRaxnwsZgDT1rrOw@mail.gmail.com>
References: <CAOFE=kOWXrnRnUVtL7A0vLKjL03571WQy5W_865hTFeBz28+tA@mail.gmail.com>
 <CA+8X3fW3Op8icpvA68O2sYPpBi=rpO2DWVXwL5UrPOHtDqDzpw@mail.gmail.com>
 <CAOFE=kNV6P4jRtrYS9K1E76pn+tW_aVpQ4Kq25ZHtQ6v-m=TkQ@mail.gmail.com>
 <CA+8X3fXM9KEb8=U1y-E_DFy9UuXk7NXDyRo6VppnEEAbAtGDDQ@mail.gmail.com>
 <CAOFE=kO20RsVsvVL2ZXpsH51aw9bzgCc3tDJ7DX8fM-tgkkbdQ@mail.gmail.com>
 <CA+8X3fUh8bug_m8iGBGcdPdb=PeGXY0xG48NTwBHC2XzXrtyhg@mail.gmail.com>
 <CAOFE=kNda7BE7cnkUg3nTZNAZbG0-dq=ir2ddXy_1pRMwySpcg@mail.gmail.com>
 <CA+8X3fVsMos2cafXXk7seO=fpM+JFgufmQpcUhGqJ9nNm4BFmQ@mail.gmail.com>
 <CAOFE=kOiTHOzcQ+dL2Tak2WnGBx1JAXpryj5CbLefdofa-7x5Q@mail.gmail.com>
 <CA+8X3fVYFJ9GUzY5ujH38zEQhWKKrkUpRzf1UjC-Cc=GHMe0VA@mail.gmail.com>
 <CAOFE=kOBPM-6bdaQKC3NXTXuAn_9OOA8oEvOGKhrGdUxBUAVkw@mail.gmail.com>
 <CA+8X3fUSY13o=khzBy9RA1_qsP1vkVqNAm=UP-xAd_cyLNrAVw@mail.gmail.com>
 <CAOFE=kPwv6LspZ=4RvL9x2JE7YSE9Ad+4Rrdhpmj0hyEHmVhxA@mail.gmail.com>
 <CAOFE=kN_tdNCzu50RhVM4-k7d6psEd=vZu7pD0htdczBKSn-HQ@mail.gmail.com>
 <CAOFE=kN6uZ9mKCkfDkz+xXLMxs3+WBu=A3YFAP-0UWt_b5V1FA@mail.gmail.com>
 <CAOFE=kMgKxo--vVCLhaPuY-4+5LD=HCZuyhBXBDFzUt0DG3ThA@mail.gmail.com>
 <CA+8X3fUTof6=q9JBNnMNpbEqF_7Jxi4WnTPF+2-c62ihMqmSaQ@mail.gmail.com>
 <CAOFE=kOUbBpUidnAxQjSzN4yhUBiaCC2GRgG8ovqm1BeuJ86ig@mail.gmail.com>
 <CA+8X3fWHutE7ibNvBWzdZWYCqQc1zpKr3EAidnN=oaGUgOmRKw@mail.gmail.com>
 <CAOFE=kOatoPvDrY6bFBqnLHojJ08HL6qMrngV4dLYpyW+Eewkg@mail.gmail.com>
 <CAOFE=kMPdr6RDDJCKV_Uw73uEUKV=RgOatrRaxnwsZgDT1rrOw@mail.gmail.com>
Message-ID: <CA+8X3fWe5k2_QF4AwOKdyqqU0U62m-eSNriwPqny=Hh06sU2nw@mail.gmail.com>

Hi Subhamitra,
As I said, the code I sent is an approximation to get your year labels in
about the correct places. You are welcome to improve the calculations.

182 days is about half a year, so that the first "tick" will fall around
the end of June (i.e. the middle of the year). If you specify the increment
as 226, you get one too many labels. 229 is what is known as a kludge (a
clumsy solution that works)

Yes, I mistakenly thought that the observations were the same throughout
the four files. As you know this (and I didn't) you can do a better job of
placing the year labels by changing the sequence for each of the CSV (not
Excel) files. The best method of all would be to have a date for each
observation. You could then discard all these approximations I have made to
get the plots to work.

No, the arguments of the axis function are:

axis(<side of plot>, <position of ticks>, <labels for the ticks>)

The first argument is; 1=bottom, 2=left, 3=top, 4=right. The next two
arguments must be the same length. If not, you will get an error. As you
can see, only every other tick has a label to avoid crowding. There are
ways to get more tick labels on an axis.

Jim


On Sun, Dec 16, 2018 at 7:03 PM Subhamitra Patra <subhamitra.patra at gmail.com>
wrote:

> Hello Sir,
>
> I have three queries regarding your suggested code.
>
> *1. *In my last email, I mentioned why there are missing observations in
> my data series. In the line, *year_mids<-seq(182,5655,by=229), *
>
> *A. what 182 indicates and what is the logic behind the consideration of
> 229 increments, although there are 226 observations per year?*
> *B.  Each excel file is having different observations depending on the
> variation of starting dates. So, is it required to add  **year_mids in
> the loop? I think I need to justify **year_mids object each time after
> importing the individual excel files. If I am wrong, kindly correct me.*
>
> 2. Further, in the command* axis(1,at=year_mids,labels=1994:2017), 1
> indicates the no. of increments of year name, right?*
>
> Kindly clarify my queries Sir for which I shall be always grateful to you.
>
> Thank you very much.
>
> [image: Mailtrack]
> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
> notified by
> Mailtrack
> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 12/16/18,
> 1:29:05 PM
>
> On Sun, Dec 16, 2018 at 12:24 PM Subhamitra Patra <
> subhamitra.patra at gmail.com> wrote:
>
>> Thank you very much sir. Actually, I excluded all the non-trading days.
>> Therefore, Each year will have 226 observations and total 6154 observations
>> for each column. The data which I plotted is not rough data. I obtained the
>> rolling observations of window 500 from my original data. So, the no. of
>> observations for each resulted column is (6154-500)+1=5655. So, It is
>> not accurate as per the days of calculations of each year.
>>
>> Ok, Sir, I will go through your suggestion, obtain the results for each
>> column of my data and would like to discuss the results with you. After
>> solving of this problem, I would like to discuss another 2 queries.
>>
>> Thank you very much Sir for educating a new R learner.
>>
>> [image: Mailtrack]
>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
>> notified by
>> Mailtrack
>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 12/16/18,
>> 12:20:17 PM
>>
>> On Sun, Dec 16, 2018 at 8:10 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>>
>>> Hi Subhamitra,
>>> Thanks. Now I can provide some assistance instead of just complaining.
>>> Your first problem is the temporal extent of the data. There are 8613 days
>>> and 6512 weekdays between the two dates you list, but only 5655
>>> observations in your data. Therefore it is unlikely that you have a
>>> complete data series, or perhaps you have the wrong dates. For the moment
>>> I'll assume that there are missing observations. What I am going to do is
>>> to match the 24 years (1994-2017) to their approximate positions in the
>>> time series. This will give you the x-axis labels that you want, close
>>> enough for this illustration. I doubt that you will need anything more
>>> accurate. You have a span of 24.58 years, which means that if your missing
>>> observations are uniformly distributed, you will have almost exactly 226
>>> observations per year. When i tried this, I got too many intervals, so I
>>> increased the increment to 229 and that worked. To get the positions for
>>> the middle of each year in the indices of the data:
>>>
>>> year_mids<-seq(182,5655,by=229)
>>>
>>> Now I suppress the x-axis by adding xaxt="n" to each call to plot. Then
>>> I add a command to display the years at the positions I have calculated:
>>>
>>> axis(1,at=year_mids,labels=1994:2017)
>>>
>>> Also note that I have added braces to the "for" loop. Putting it all
>>> together:
>>>
>>> year_mids<-seq(182,5655,by=229)
>>> pdf("EMs.pdf",width=20,height=20)
>>> par(mfrow=c(5,4))
>>> # import your first sheet here (16 columns)
>>> EMs1.1<-read.csv("EMs1.1.csv")
>>> ncolumns<-ncol(EMs1.1)
>>> for(i in 1:ncolumns) {
>>>   plot(EMs1.1[,i],type="l",col = "Red", xlab="Time",
>>>        ylab="APEn", main=names(EMs1.1)[i],xaxt="n")
>>>  axis(1,at=year_mids,labels=1994:2017)
>>> }
>>> #import your second sheet here, (1 column)
>>> EMs2.1<-read.csv("EMs2.1.csv")
>>> ncolumns<-ncol(EMs2.1)
>>> for(i in 1:ncolumns) {
>>>   plot(EMs2.1[,i],type="l",col = "Red", xlab="Time",
>>>        ylab="APEn", main=names(EMs2.1)[i],xaxt="n")
>>>  axis(1,at=year_mids,labels=1994:2017)
>>> }
>>> # import your Third sheet here, (1 column)
>>> EMs3.1<-read.csv("EMs3.1.csv")
>>> ncolumns<-ncol(EMs3.1)
>>> for(i in 1:ncolumns) {
>>>   plot(EMs3.1[,i],type="l",col = "Red", xlab="Time",
>>>        ylab="APEn", main=names(EMs3.1)[i],xaxt="n")
>>>  axis(1,at=year_mids,labels=1994:2017)
>>> }
>>> # import your fourth sheet here, (1 column)
>>> EMs4.1<-read.csv("EMs4.1.csv")
>>> ncolumns<-ncol(EMs4.1)
>>> for(i in 1:ncolumns) {
>>>   plot(EMs4.1[,i],type="l",col = "Red", xlab="Time",
>>>        ylab="APEn", main=names(EMs4.1)[i],xaxt="n")
>>>  axis(1,at=year_mids,labels=1994:2017)
>>> }
>>> # finish plotting
>>> dev.off()
>>>
>>> With any luck, you are now okay. Remember, this is a hack to deal with
>>> data that are not what you think they are.
>>>
>>> Jim
>>>
>>>
>>
>> --
>> *Best Regards,*
>> *Subhamitra Patra*
>> *Phd. Research Scholar*
>> *Department of Humanities and Social Sciences*
>> *Indian Institute of Technology, Kharagpur*
>> *INDIA*
>>
>
>
> --
> *Best Regards,*
> *Subhamitra Patra*
> *Phd. Research Scholar*
> *Department of Humanities and Social Sciences*
> *Indian Institute of Technology, Kharagpur*
> *INDIA*
>

	[[alternative HTML version deleted]]


From e@@wiek @ending from gm@il@com  Sun Dec 16 19:29:30 2018
From: e@@wiek @ending from gm@il@com (Ek Esawi)
Date: Sun, 16 Dec 2018 13:29:30 -0500
Subject: [R] 
 Combine lists into a data frame or append them to a text file
In-Reply-To: <CA+ZkTxtjLSYQJ+-3mX_NAnOUJ--9EL_dkmj3pWEaSdEbauVqgg@mail.gmail.com>
References: <CA+ZkTxvozwNSzB5qrT3xSK_fQiQTnsf15j1DBT5stcy-H_iEow@mail.gmail.com>
 <CA+8X3fVSN7+raqt0Uo8UtR65TsykgZsAwoWPqKKzDCftjjT8QQ@mail.gmail.com>
 <CA+ZkTxtjLSYQJ+-3mX_NAnOUJ--9EL_dkmj3pWEaSdEbauVqgg@mail.gmail.com>
Message-ID: <CA+ZkTxshnFf2uZP-5H9Ybx9ZtEvtuG-X6vo299tnt0NkAtjP_A@mail.gmail.com>

I tried Jim's function and it works. But here is an example just in case.

AA <- list(a=c(1,2,3,4),b = c("a","b","c"))
BB <- list(c=c(1,2,3,4,5),d=c("a","b","c","d","e"))
mylist <- (list(AA,BB))

lapply(mylist,function(x) write.table(x,file = test.txt))
 Show Traceback

 Error in (function (..., row.names = NULL, check.rows = FALSE,
check.names = TRUE,  :
  arguments imply differing number of rows: 4, 3

On Sun, Dec 16, 2018 at 8:45 AM Ek Esawi <esawiek at gmail.com> wrote:
>
> Thank you Jim and Bert,
>
> I tried Jim's function and it works. But here is an example just in case.
>
> AA <- list(a=c(1,2,3,4),b = c("a","b","c"))
> BB <- list(c=c(1,2,3,4,5),d=c("a","b","c","d","e"))
> mylist <- (list(AA,BB))
>
> lapply(mylist,function(x) write.table(x,file = test.txt))
>  Show Traceback
>
>  Error in (function (..., row.names = NULL, check.rows = FALSE,
> check.names = TRUE,  :
>   arguments imply differing number of rows: 4, 3
>
> On Sun, Dec 16, 2018 at 1:03 AM Jim Lemon <drjimlemon at gmail.com> wrote:
> >
> > Hi Ek,
> > I thought there would be a simple fix for this, but had to write a
> > little function:
> >
> > fillList<-function(x) {
> >  maxrows<-max(unlist(lapply(x,length)))
> >  return(lapply(x,"[",1:maxrows))
> > }
> >
> > that fills up the rows of each list with NAs. I got the expected result with:
> >
> > testlist<-list(a=1:8,b=1:9,c=1:10)
> > as.data.frame(fillList(testlist))
> >
> > so:
> >
> > for (i in 1:length(MyTables)) {
> > write.table(as.data.frame(fillList(MyTables[i])),
> >  file = "Temp.txt",append = TRUE,quote = TRUE)
> >
> > may do the job.
> >
> > Jim
> >
> > On Sun, Dec 16, 2018 at 2:28 PM Ek Esawi <esawiek at gmail.com> wrote:
> > >
> > > Hi All,
> > >
> > > I have an R object that is made up of N number of lists which are all
> > > of different number of columns and rows.  I want to combine the N
> > > lists into a single data frame or write (append) them into text file.
> > > I hope the question is clear and doesn?t require an example. I am
> > > hoping to accomplish this using base R functions.
> > > Below is what I tried but both gave me the same error which I do
> > > understand, I think, but I don?t know how to fix it. My R object is
> > > MyTables
> > >
> > > lapply(MyTables, function(x) write.table(x, file = "Temp.txt",append = TRUE ))
> > > OR
> > > for (i in 1:length(MyTables)) {
> > > write.table(MyTables[i], file = "Temp.txt",append = TRUE,quote = TRUE)
> > >
> > > the error
> > > Error in (function (..., row.names = NULL, check.rows = FALSE,
> > > check.names = TRUE,  :
> > >   arguments imply differing number of rows: 51, 8, 30
> > >
> > > Thanks--EK
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.


From n@th@n@f@p@r@on@ @ending from gm@il@com  Sun Dec 16 20:48:44 2018
From: n@th@n@f@p@r@on@ @ending from gm@il@com (Nathan Parsons)
Date: Sun, 16 Dec 2018 11:48:44 -0800
Subject: [R] error = FALSE causes knit2wp to throw duplicate label error
Message-ID: <CANdqJqeawL2fdbeZi3jqiNSs1VJORG5e=XnRFWo2J4EGkJhZvQ@mail.gmail.com>

Goal: post from R to Wordpress installation on server.

Problem: R keeps returning the error ?Error in parse_block(g[-1],
g[1], params.src) : duplicate label 'setup?? if error = FALSE in the
knitr options or in an r chunk. It works fine if error = TRUE. I could
just go through each post each time and remove any returned errors
manually, but I'd like to find a more permanent solution.

I don't have any duplicate labels; is knit2wp somehow introducing a
duplicate label in the .Rmd
-> .md / upload process?

My code:

```{r setup, include=FALSE}
## Set the global chunk options for knitting reports
  knitr::opts_chunk$set(
    echo = TRUE,
    eval = TRUE,
    message = TRUE,
    error = FALSE,
    warning = TRUE,
    highlight = TRUE,
    prompt = FALSE
  )

## Load and activate libraries using 'pacman' package
  if (!require(pacman)) {
    install.packages("pacman", repos = "http://cran.us.r-project.org")
  require(pacman)
  }

  pacman::p_load_gh("duncantl/XMLRPC",
    "duncantl/RWordPress")
  pacman::p_load("knitr")
```

```{r chunk1, echo = FALSE}
## post information
  fileName <- "fancy_post.Rmd"
  postTitle <- "Fancy Post Title"

```

blah blah blah...

```{r chunk2, echo = FALSE}
## Set working directory to correct location
  last_dir <- getwd()
  setwd("~/Sites/posts")

## Tell knitr to create the html code and upload it to your wordpress site
  knit2wp(input = fileName,
    title = postTitle,
    publish = FALSE,
    action = 'newPost')

  setwd(last_dir)
```


Traceback:
Error in parse_block(g[-1], g[1], params.src) : duplicate label 'setup'
26. stop("duplicate label '", label, "'")
25. parse_block(g[-1], g[1], params.src)
24. FUN(X[[i]], ...)
23. lapply(groups, function(g) { block = grepl(chunk.begin, g[1]) if
(!set.preamble && !parent_mode()) { return(if (block) "" else g) ...
22. split_file(lines = text)
21. process_file(text, output)
20. knit(input, encoding = encoding, envir = envir)
19. knit2wp(input = fileName, title = postTitle, publish = FALSE,
action = "newPost")
18. eval(expr, envir, enclos)
17. eval(expr, envir, enclos)
16. withVisible(eval(expr, envir, enclos))
15. withCallingHandlers(withVisible(eval(expr, envir, enclos)),
warning = wHandler, error = eHandler, message = mHandler)
14. handle(ev <- withCallingHandlers(withVisible(eval(expr, envir,
enclos)), warning = wHandler, error = eHandler, message = mHandler))
13. timing_fn(handle(ev <- withCallingHandlers(withVisible(eval(expr,
envir, enclos)), warning = wHandler, error = eHandler, message =
mHandler)))
12. evaluate_call(expr, parsed$src[[i]], envir = envir, enclos =
enclos, debug = debug, last = i == length(out), use_try =
stop_on_error != 2L, keep_warning = keep_warning, keep_message =
keep_message, output_handler = output_handler, include_timing =
include_timing)
11. evaluate::evaluate(...)
10. evaluate(code, envir = env, new_device = FALSE, keep_warning =
!isFALSE(options$warning), keep_message = !isFALSE(options$message),
stop_on_error = if (options$error && options$include) 0L else 2L,
output_handler = knit_handlers(options$render, options))
9. in_dir(input_dir(), evaluate(code, envir = env, new_device = FALSE,
keep_warning = !isFALSE(options$warning), keep_message =
!isFALSE(options$message), stop_on_error = if (options$error &&
options$include) 0L else 2L, output_handler =
knit_handlers(options$render, options)))
8. block_exec(params)
7. call_block(x)
6. process_group.block(group)
5. process_group(group)
4. withCallingHandlers(if (tangle) process_tangle(group) else
process_group(group), error = function(e) { setwd(wd) cat(res, sep =
"\n", file = output %n% "") ...
3. process_file(text, output)
2. knit(input, encoding = encoding, envir = envir)
1. knit2wp(input = fileName, title = postTitle, publish = FALSE,
action = "newPost")


From murdoch@dunc@n @ending from gm@il@com  Sun Dec 16 21:44:44 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Sun, 16 Dec 2018 15:44:44 -0500
Subject: [R] error = FALSE causes knit2wp to throw duplicate label error
In-Reply-To: <CANdqJqeawL2fdbeZi3jqiNSs1VJORG5e=XnRFWo2J4EGkJhZvQ@mail.gmail.com>
References: <CANdqJqeawL2fdbeZi3jqiNSs1VJORG5e=XnRFWo2J4EGkJhZvQ@mail.gmail.com>
Message-ID: <b75dd37d-cceb-4a05-4972-f39f6b9acd3f@gmail.com>

On 16/12/2018 2:48 PM, Nathan Parsons wrote:
> Goal: post from R to Wordpress installation on server.
> 
> Problem: R keeps returning the error ?Error in parse_block(g[-1],
> g[1], params.src) : duplicate label 'setup?? if error = FALSE in the
> knitr options or in an r chunk. It works fine if error = TRUE. I could
> just go through each post each time and remove any returned errors
> manually, but I'd like to find a more permanent solution.
> 
> I don't have any duplicate labels; is knit2wp somehow introducing a
> duplicate label in the .Rmd
> -> .md / upload process?

That looks like a knitr issue.  I don't know if Yihui reads this list; 
you'd probably be better off on StackOverflow or filing an issue as 
described at https://yihui.name/knitr/faq/.

Duncan Murdoch

> 
> My code:
> 
> ```{r setup, include=FALSE}
> ## Set the global chunk options for knitting reports
>    knitr::opts_chunk$set(
>      echo = TRUE,
>      eval = TRUE,
>      message = TRUE,
>      error = FALSE,
>      warning = TRUE,
>      highlight = TRUE,
>      prompt = FALSE
>    )
> 
> ## Load and activate libraries using 'pacman' package
>    if (!require(pacman)) {
>      install.packages("pacman", repos = "http://cran.us.r-project.org")
>    require(pacman)
>    }
> 
>    pacman::p_load_gh("duncantl/XMLRPC",
>      "duncantl/RWordPress")
>    pacman::p_load("knitr")
> ```
> 
> ```{r chunk1, echo = FALSE}
> ## post information
>    fileName <- "fancy_post.Rmd"
>    postTitle <- "Fancy Post Title"
> 
> ```
> 
> blah blah blah...
> 
> ```{r chunk2, echo = FALSE}
> ## Set working directory to correct location
>    last_dir <- getwd()
>    setwd("~/Sites/posts")
> 
> ## Tell knitr to create the html code and upload it to your wordpress site
>    knit2wp(input = fileName,
>      title = postTitle,
>      publish = FALSE,
>      action = 'newPost')
> 
>    setwd(last_dir)
> ```
> 
> 
> Traceback:
> Error in parse_block(g[-1], g[1], params.src) : duplicate label 'setup'
> 26. stop("duplicate label '", label, "'")
> 25. parse_block(g[-1], g[1], params.src)
> 24. FUN(X[[i]], ...)
> 23. lapply(groups, function(g) { block = grepl(chunk.begin, g[1]) if
> (!set.preamble && !parent_mode()) { return(if (block) "" else g) ...
> 22. split_file(lines = text)
> 21. process_file(text, output)
> 20. knit(input, encoding = encoding, envir = envir)
> 19. knit2wp(input = fileName, title = postTitle, publish = FALSE,
> action = "newPost")
> 18. eval(expr, envir, enclos)
> 17. eval(expr, envir, enclos)
> 16. withVisible(eval(expr, envir, enclos))
> 15. withCallingHandlers(withVisible(eval(expr, envir, enclos)),
> warning = wHandler, error = eHandler, message = mHandler)
> 14. handle(ev <- withCallingHandlers(withVisible(eval(expr, envir,
> enclos)), warning = wHandler, error = eHandler, message = mHandler))
> 13. timing_fn(handle(ev <- withCallingHandlers(withVisible(eval(expr,
> envir, enclos)), warning = wHandler, error = eHandler, message =
> mHandler)))
> 12. evaluate_call(expr, parsed$src[[i]], envir = envir, enclos =
> enclos, debug = debug, last = i == length(out), use_try =
> stop_on_error != 2L, keep_warning = keep_warning, keep_message =
> keep_message, output_handler = output_handler, include_timing =
> include_timing)
> 11. evaluate::evaluate(...)
> 10. evaluate(code, envir = env, new_device = FALSE, keep_warning =
> !isFALSE(options$warning), keep_message = !isFALSE(options$message),
> stop_on_error = if (options$error && options$include) 0L else 2L,
> output_handler = knit_handlers(options$render, options))
> 9. in_dir(input_dir(), evaluate(code, envir = env, new_device = FALSE,
> keep_warning = !isFALSE(options$warning), keep_message =
> !isFALSE(options$message), stop_on_error = if (options$error &&
> options$include) 0L else 2L, output_handler =
> knit_handlers(options$render, options)))
> 8. block_exec(params)
> 7. call_block(x)
> 6. process_group.block(group)
> 5. process_group(group)
> 4. withCallingHandlers(if (tangle) process_tangle(group) else
> process_group(group), error = function(e) { setwd(wd) cat(res, sep =
> "\n", file = output %n% "") ...
> 3. process_file(text, output)
> 2. knit(input, encoding = encoding, envir = envir)
> 1. knit2wp(input = fileName, title = postTitle, publish = FALSE,
> action = "newPost")
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jdnewmil @ending from dcn@d@vi@@c@@u@  Sun Dec 16 22:46:03 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Sun, 16 Dec 2018 13:46:03 -0800
Subject: [R] error = FALSE causes knit2wp to throw duplicate label error
In-Reply-To: <CANdqJqeawL2fdbeZi3jqiNSs1VJORG5e=XnRFWo2J4EGkJhZvQ@mail.gmail.com>
References: <CANdqJqeawL2fdbeZi3jqiNSs1VJORG5e=XnRFWo2J4EGkJhZvQ@mail.gmail.com>
Message-ID: <3428E4A8-43DF-4EF8-9838-F006BF7F7C15@dcn.davis.ca.us>

This seems a bit deep into knitr for R-help... you might have better luck on StackExchange. I also suggest that posting an incomplete example is usually the kiss of death for getting constructive assistance online.

FWIW my guess is that executing knitr from within an Rmarkdown document is a bad idea unless you are building using child documents. Try manipulating your markdown from an R file.

On December 16, 2018 11:48:44 AM PST, Nathan Parsons <nathan.f.parsons at gmail.com> wrote:
>Goal: post from R to Wordpress installation on server.
>
>Problem: R keeps returning the error ?Error in parse_block(g[-1],
>g[1], params.src) : duplicate label 'setup?? if error = FALSE in the
>knitr options or in an r chunk. It works fine if error = TRUE. I could
>just go through each post each time and remove any returned errors
>manually, but I'd like to find a more permanent solution.
>
>I don't have any duplicate labels; is knit2wp somehow introducing a
>duplicate label in the .Rmd
>-> .md / upload process?
>
>My code:
>
>```{r setup, include=FALSE}
>## Set the global chunk options for knitting reports
>  knitr::opts_chunk$set(
>    echo = TRUE,
>    eval = TRUE,
>    message = TRUE,
>    error = FALSE,
>    warning = TRUE,
>    highlight = TRUE,
>    prompt = FALSE
>  )
>
>## Load and activate libraries using 'pacman' package
>  if (!require(pacman)) {
>    install.packages("pacman", repos = "http://cran.us.r-project.org")
>  require(pacman)
>  }
>
>  pacman::p_load_gh("duncantl/XMLRPC",
>    "duncantl/RWordPress")
>  pacman::p_load("knitr")
>```
>
>```{r chunk1, echo = FALSE}
>## post information
>  fileName <- "fancy_post.Rmd"
>  postTitle <- "Fancy Post Title"
>
>```
>
>blah blah blah...
>
>```{r chunk2, echo = FALSE}
>## Set working directory to correct location
>  last_dir <- getwd()
>  setwd("~/Sites/posts")
>
>## Tell knitr to create the html code and upload it to your wordpress
>site
>  knit2wp(input = fileName,
>    title = postTitle,
>    publish = FALSE,
>    action = 'newPost')
>
>  setwd(last_dir)
>```
>
>
>Traceback:
>Error in parse_block(g[-1], g[1], params.src) : duplicate label 'setup'
>26. stop("duplicate label '", label, "'")
>25. parse_block(g[-1], g[1], params.src)
>24. FUN(X[[i]], ...)
>23. lapply(groups, function(g) { block = grepl(chunk.begin, g[1]) if
>(!set.preamble && !parent_mode()) { return(if (block) "" else g) ...
>22. split_file(lines = text)
>21. process_file(text, output)
>20. knit(input, encoding = encoding, envir = envir)
>19. knit2wp(input = fileName, title = postTitle, publish = FALSE,
>action = "newPost")
>18. eval(expr, envir, enclos)
>17. eval(expr, envir, enclos)
>16. withVisible(eval(expr, envir, enclos))
>15. withCallingHandlers(withVisible(eval(expr, envir, enclos)),
>warning = wHandler, error = eHandler, message = mHandler)
>14. handle(ev <- withCallingHandlers(withVisible(eval(expr, envir,
>enclos)), warning = wHandler, error = eHandler, message = mHandler))
>13. timing_fn(handle(ev <- withCallingHandlers(withVisible(eval(expr,
>envir, enclos)), warning = wHandler, error = eHandler, message =
>mHandler)))
>12. evaluate_call(expr, parsed$src[[i]], envir = envir, enclos =
>enclos, debug = debug, last = i == length(out), use_try =
>stop_on_error != 2L, keep_warning = keep_warning, keep_message =
>keep_message, output_handler = output_handler, include_timing =
>include_timing)
>11. evaluate::evaluate(...)
>10. evaluate(code, envir = env, new_device = FALSE, keep_warning =
>!isFALSE(options$warning), keep_message = !isFALSE(options$message),
>stop_on_error = if (options$error && options$include) 0L else 2L,
>output_handler = knit_handlers(options$render, options))
>9. in_dir(input_dir(), evaluate(code, envir = env, new_device = FALSE,
>keep_warning = !isFALSE(options$warning), keep_message =
>!isFALSE(options$message), stop_on_error = if (options$error &&
>options$include) 0L else 2L, output_handler =
>knit_handlers(options$render, options)))
>8. block_exec(params)
>7. call_block(x)
>6. process_group.block(group)
>5. process_group(group)
>4. withCallingHandlers(if (tangle) process_tangle(group) else
>process_group(group), error = function(e) { setwd(wd) cat(res, sep =
>"\n", file = output %n% "") ...
>3. process_file(text, output)
>2. knit(input, encoding = encoding, envir = envir)
>1. knit2wp(input = fileName, title = postTitle, publish = FALSE,
>action = "newPost")
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From e@@wiek @ending from gm@il@com  Mon Dec 17 04:10:40 2018
From: e@@wiek @ending from gm@il@com (Ek Esawi)
Date: Sun, 16 Dec 2018 22:10:40 -0500
Subject: [R] 
 Combine lists into a data frame or append them to a text file
In-Reply-To: <CA+8X3fVRgdJ4Bwos409xqLNKAsjpgLKCSgixFMsj+267asq8UQ@mail.gmail.com>
References: <CA+ZkTxvozwNSzB5qrT3xSK_fQiQTnsf15j1DBT5stcy-H_iEow@mail.gmail.com>
 <CA+8X3fVSN7+raqt0Uo8UtR65TsykgZsAwoWPqKKzDCftjjT8QQ@mail.gmail.com>
 <CA+ZkTxtjLSYQJ+-3mX_NAnOUJ--9EL_dkmj3pWEaSdEbauVqgg@mail.gmail.com>
 <CA+8X3fVRgdJ4Bwos409xqLNKAsjpgLKCSgixFMsj+267asq8UQ@mail.gmail.com>
Message-ID: <CA+ZkTxsrd_40mx=spz6yZfJ61vJTA-Z5vdDyuCyrH0w3u6+P0g@mail.gmail.com>

Hi Jim,

Thanks again. Actually i changed my code where the lists are not
nested. Your code works, as you said for the example, but still is not
working for my lists (30). My lists have different columns and rows
and several are NULL; plus there are many blank space which i suppose
don't make much difference, but not sure. I will try to send an actual
output..

Thanks again---EK.

> for (i in 1:length(MyTables)) {
+     write.table(as.data.frame(fillList(MyTables[i])),
+                 file = "Temp.txt",append = TRUE,quote = TRUE)}
Error in (function (..., row.names = NULL, check.rows = FALSE,
check.names = TRUE,  :
  arguments imply differing number of rows: 4, 50, 53, 8, 20
In addition: There were 20 warnings (use warnings() to see t

On Sun, Dec 16, 2018 at 4:10 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi Ek,
> Okay, you got me. I didn't write the function to handle nested lists.
> If you combine your initial lists into a single level list, I think it
> will work:
>
> fillList<-function(x) {
>  maxrows<-max(unlist(lapply(x,length)))
>  return(lapply(x,"[",1:maxrows))
> }
> AA <- list(a=c(1,2,3,4),b = c("a","b","c"))
> BB <- list(c=c(1,2,3,4,5),d=c("a","b","c","d","e"))
> mylist <- c(AA,BB)
> mydf<as.data.frame(fillList(mylist))
>
> Jim
>
> On Mon, Dec 17, 2018 at 12:45 AM Ek Esawi <esawiek at gmail.com> wrote:
> >
> > Thank you Jim and Bert,
> >
> > I tried Jim's function and it works. But here is an example just in case.
> >
> > AA <- list(a=c(1,2,3,4),b = c("a","b","c"))
> > BB <- list(c=c(1,2,3,4,5),d=c("a","b","c","d","e"))
> > mylist <- (list(AA,BB))
> >
> > lapply(mylist,function(x) write.table(x,file = test.txt))
> >  Show Traceback
> >
> >  Error in (function (..., row.names = NULL, check.rows = FALSE,
> > check.names = TRUE,  :
> >   arguments imply differing number of rows: 4, 3
> >
> > On Sun, Dec 16, 2018 at 1:03 AM Jim Lemon <drjimlemon at gmail.com> wrote:
> > >
> > > Hi Ek,
> > > I thought there would be a simple fix for this, but had to write a
> > > little function:
> > >
> > > fillList<-function(x) {
> > >  maxrows<-max(unlist(lapply(x,length)))
> > >  return(lapply(x,"[",1:maxrows))
> > > }
> > >
> > > that fills up the rows of each list with NAs. I got the expected result with:
> > >
> > > testlist<-list(a=1:8,b=1:9,c=1:10)
> > > as.data.frame(fillList(testlist))
> > >
> > > so:
> > >
> > > for (i in 1:length(MyTables)) {
> > > write.table(as.data.frame(fillList(MyTables[i])),
> > >  file = "Temp.txt",append = TRUE,quote = TRUE)
> > >
> > > may do the job.
> > >
> > > Jim
> > >
> > > On Sun, Dec 16, 2018 at 2:28 PM Ek Esawi <esawiek at gmail.com> wrote:
> > > >
> > > > Hi All,
> > > >
> > > > I have an R object that is made up of N number of lists which are all
> > > > of different number of columns and rows.  I want to combine the N
> > > > lists into a single data frame or write (append) them into text file.
> > > > I hope the question is clear and doesn?t require an example. I am
> > > > hoping to accomplish this using base R functions.
> > > > Below is what I tried but both gave me the same error which I do
> > > > understand, I think, but I don?t know how to fix it. My R object is
> > > > MyTables
> > > >
> > > > lapply(MyTables, function(x) write.table(x, file = "Temp.txt",append = TRUE ))
> > > > OR
> > > > for (i in 1:length(MyTables)) {
> > > > write.table(MyTables[i], file = "Temp.txt",append = TRUE,quote = TRUE)
> > > >
> > > > the error
> > > > Error in (function (..., row.names = NULL, check.rows = FALSE,
> > > > check.names = TRUE,  :
> > > >   arguments imply differing number of rows: 51, 8, 30
> > > >
> > > > Thanks--EK
> > > >
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.


From @ubh@mitr@@p@tr@ @ending from gm@il@com  Mon Dec 17 08:13:00 2018
From: @ubh@mitr@@p@tr@ @ending from gm@il@com (Subhamitra Patra)
Date: Mon, 17 Dec 2018 12:43:00 +0530
Subject: [R] 
 [R studio] Plotting of line chart for each columns at 1 page
In-Reply-To: <CA+8X3fWe5k2_QF4AwOKdyqqU0U62m-eSNriwPqny=Hh06sU2nw@mail.gmail.com>
References: <CAOFE=kOWXrnRnUVtL7A0vLKjL03571WQy5W_865hTFeBz28+tA@mail.gmail.com>
 <CA+8X3fW3Op8icpvA68O2sYPpBi=rpO2DWVXwL5UrPOHtDqDzpw@mail.gmail.com>
 <CAOFE=kNV6P4jRtrYS9K1E76pn+tW_aVpQ4Kq25ZHtQ6v-m=TkQ@mail.gmail.com>
 <CA+8X3fXM9KEb8=U1y-E_DFy9UuXk7NXDyRo6VppnEEAbAtGDDQ@mail.gmail.com>
 <CAOFE=kO20RsVsvVL2ZXpsH51aw9bzgCc3tDJ7DX8fM-tgkkbdQ@mail.gmail.com>
 <CA+8X3fUh8bug_m8iGBGcdPdb=PeGXY0xG48NTwBHC2XzXrtyhg@mail.gmail.com>
 <CAOFE=kNda7BE7cnkUg3nTZNAZbG0-dq=ir2ddXy_1pRMwySpcg@mail.gmail.com>
 <CA+8X3fVsMos2cafXXk7seO=fpM+JFgufmQpcUhGqJ9nNm4BFmQ@mail.gmail.com>
 <CAOFE=kOiTHOzcQ+dL2Tak2WnGBx1JAXpryj5CbLefdofa-7x5Q@mail.gmail.com>
 <CA+8X3fVYFJ9GUzY5ujH38zEQhWKKrkUpRzf1UjC-Cc=GHMe0VA@mail.gmail.com>
 <CAOFE=kOBPM-6bdaQKC3NXTXuAn_9OOA8oEvOGKhrGdUxBUAVkw@mail.gmail.com>
 <CA+8X3fUSY13o=khzBy9RA1_qsP1vkVqNAm=UP-xAd_cyLNrAVw@mail.gmail.com>
 <CAOFE=kPwv6LspZ=4RvL9x2JE7YSE9Ad+4Rrdhpmj0hyEHmVhxA@mail.gmail.com>
 <CAOFE=kN_tdNCzu50RhVM4-k7d6psEd=vZu7pD0htdczBKSn-HQ@mail.gmail.com>
 <CAOFE=kN6uZ9mKCkfDkz+xXLMxs3+WBu=A3YFAP-0UWt_b5V1FA@mail.gmail.com>
 <CAOFE=kMgKxo--vVCLhaPuY-4+5LD=HCZuyhBXBDFzUt0DG3ThA@mail.gmail.com>
 <CA+8X3fUTof6=q9JBNnMNpbEqF_7Jxi4WnTPF+2-c62ihMqmSaQ@mail.gmail.com>
 <CAOFE=kOUbBpUidnAxQjSzN4yhUBiaCC2GRgG8ovqm1BeuJ86ig@mail.gmail.com>
 <CA+8X3fWHutE7ibNvBWzdZWYCqQc1zpKr3EAidnN=oaGUgOmRKw@mail.gmail.com>
 <CAOFE=kOatoPvDrY6bFBqnLHojJ08HL6qMrngV4dLYpyW+Eewkg@mail.gmail.com>
 <CAOFE=kMPdr6RDDJCKV_Uw73uEUKV=RgOatrRaxnwsZgDT1rrOw@mail.gmail.com>
 <CA+8X3fWe5k2_QF4AwOKdyqqU0U62m-eSNriwPqny=Hh06sU2nw@mail.gmail.com>
Message-ID: <CAOFE=kPQDaqAKc-gMiUvEO=Zbo1+Ca2Hqm-KYX2DjeM6FnZqKw@mail.gmail.com>

Hello Sir,

Thank you very much for your excellent guidance to a new R learner.

I tried with your suggested code and got the expected results, but for the
2 CSV files (i.e. EMs2.1. and EMs.3.1), the date column is not coming in
the X-axis (shown in the last row of the attached result Pdf file).  I
think I need to increase more or less than 229 in the year-mids because for
both the CSV files, starting date is 03-01-2002 and 04-07-2001
(date-month-year) for EMs 2.1. and EMs 3.1. respectively. *Sir, hence I am
quite confused for the logic behind the fixing of year_mids*. For your
convenience, I am attaching both the code and result file.

pdf("EMs1.pdf",width=20,height=20)
par(mfrow=c(5,4))
# import your first sheet here (16 columns)
EMs1.1<-read.csv("EMs1.1.csv")
ncolumns<-ncol(EMs1.1)
for(i in 1:ncolumns) {
  plot(EMs1.1[,i],type="l",col = "Red", xlab="Time",
       ylab="APEn", main=names(EMs1.1)[i],xaxt="n")
  year_mids<-seq(182,5655,by=229)
  axis(1,at=year_mids,labels=1994:2017)
}
#import your second sheet here, (1 column)
EMs2.1<-read.csv("EMs2.1.csv")
ncolumns<-ncol(EMs2.1)
for(i in 1:ncolumns) {
  plot(EMs2.1[,i],type="l",col = "Red", xlab="Time",
       ylab="APEn", main=names(EMs2.1)[i],xaxt="n")
  year_mids<-seq(182,3567,by=229)
  axis(1,at=year_mids,labels=2002:2017)
}
# import your Third sheet here, (1 column)
EMs3.1<-read.csv("EMs3.1.csv")
ncolumns<-ncol(EMs3.1)
for(i in 1:ncolumns) {
  plot(EMs3.1[,i],type="l",col = "Red", xlab="Time",
       ylab="APEn", main=names(EMs3.1)[i],xaxt="n")
  year_mids<-seq(182,3698,by=229)
  axis(1,at=year_mids,labels=2001:2017)
}
# import your fourth sheet here, (1 column)
EMs4.1<-read.csv("EMs4.1.csv")
ncolumns<-ncol(EMs4.1)
for(i in 1:ncolumns) {
  plot(EMs4.1[,i],type="l",col = "Red", xlab="Time",
       ylab="APEn", main=names(EMs4.1)[i],xaxt="n")
  year_mids<-seq(182,5265,by=229)
  axis(1,at=year_mids,labels=1995:2017)
}
# finish plotting
dev.off()


Sir, According to your suggestion, *"** you can do a better job of placing
the year labels by changing the sequence for each of the CSV (not Excel)
files. The best method of all would be to have a date for each observation.
You could then discard all these approximations I have made to get the
plots to work.**" , *when I am adding the date (i.e. date-month-year) in
the sequence *(**year_mids<-seq(182,5655,by=229)*
*
  axis(1,at=year_mids,labels=03-01-1994:03-08-2017)*
*
  })*
*I am getting the error.*

Kindly suggest.

Thank you very much.





[image: Mailtrack]
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
Sender
notified by
Mailtrack
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
12/17/18,
12:25:26 PM

On Sun, Dec 16, 2018 at 2:58 PM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Subhamitra,
> As I said, the code I sent is an approximation to get your year labels in
> about the correct places. You are welcome to improve the calculations.
>
> 182 days is about half a year, so that the first "tick" will fall around
> the end of June (i.e. the middle of the year). If you specify the increment
> as 226, you get one too many labels. 229 is what is known as a kludge (a
> clumsy solution that works)
>
> Yes, I mistakenly thought that the observations were the same throughout
> the four files. As you know this (and I didn't) you can do a better job of
> placing the year labels by changing the sequence for each of the CSV (not
> Excel) files. The best method of all would be to have a date for each
> observation. You could then discard all these approximations I have made to
> get the plots to work.
>
> No, the arguments of the axis function are:
>
> axis(<side of plot>, <position of ticks>, <labels for the ticks>)
>
> The first argument is; 1=bottom, 2=left, 3=top, 4=right. The next two
> arguments must be the same length. If not, you will get an error. As you
> can see, only every other tick has a label to avoid crowding. There are
> ways to get more tick labels on an axis.
>
> Jim
>
>
> On Sun, Dec 16, 2018 at 7:03 PM Subhamitra Patra <
> subhamitra.patra at gmail.com> wrote:
>
>> Hello Sir,
>>
>> I have three queries regarding your suggested code.
>>
>> *1. *In my last email, I mentioned why there are missing observations in
>> my data series. In the line, *year_mids<-seq(182,5655,by=229), *
>>
>> *A. what 182 indicates and what is the logic behind the consideration of
>> 229 increments, although there are 226 observations per year?*
>> *B.  Each excel file is having different observations depending on the
>> variation of starting dates. So, is it required to add  **year_mids in
>> the loop? I think I need to justify **year_mids object each time after
>> importing the individual excel files. If I am wrong, kindly correct me.*
>>
>> 2. Further, in the command* axis(1,at=year_mids,labels=1994:2017), 1
>> indicates the no. of increments of year name, right?*
>>
>> Kindly clarify my queries Sir for which I shall be always grateful to you.
>>
>> Thank you very much.
>>
>> [image: Mailtrack]
>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
>> notified by
>> Mailtrack
>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 12/16/18,
>> 1:29:05 PM
>>
>> On Sun, Dec 16, 2018 at 12:24 PM Subhamitra Patra <
>> subhamitra.patra at gmail.com> wrote:
>>
>>> Thank you very much sir. Actually, I excluded all the non-trading days.
>>> Therefore, Each year will have 226 observations and total 6154 observations
>>> for each column. The data which I plotted is not rough data. I obtained the
>>> rolling observations of window 500 from my original data. So, the no. of
>>> observations for each resulted column is (6154-500)+1=5655. So, It is
>>> not accurate as per the days of calculations of each year.
>>>
>>> Ok, Sir, I will go through your suggestion, obtain the results for each
>>> column of my data and would like to discuss the results with you. After
>>> solving of this problem, I would like to discuss another 2 queries.
>>>
>>> Thank you very much Sir for educating a new R learner.
>>>
>>> [image: Mailtrack]
>>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
>>> notified by
>>> Mailtrack
>>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 12/16/18,
>>> 12:20:17 PM
>>>
>>> On Sun, Dec 16, 2018 at 8:10 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>>>
>>>> Hi Subhamitra,
>>>> Thanks. Now I can provide some assistance instead of just complaining.
>>>> Your first problem is the temporal extent of the data. There are 8613 days
>>>> and 6512 weekdays between the two dates you list, but only 5655
>>>> observations in your data. Therefore it is unlikely that you have a
>>>> complete data series, or perhaps you have the wrong dates. For the moment
>>>> I'll assume that there are missing observations. What I am going to do is
>>>> to match the 24 years (1994-2017) to their approximate positions in the
>>>> time series. This will give you the x-axis labels that you want, close
>>>> enough for this illustration. I doubt that you will need anything more
>>>> accurate. You have a span of 24.58 years, which means that if your missing
>>>> observations are uniformly distributed, you will have almost exactly 226
>>>> observations per year. When i tried this, I got too many intervals, so I
>>>> increased the increment to 229 and that worked. To get the positions for
>>>> the middle of each year in the indices of the data:
>>>>
>>>> year_mids<-seq(182,5655,by=229)
>>>>
>>>> Now I suppress the x-axis by adding xaxt="n" to each call to plot. Then
>>>> I add a command to display the years at the positions I have calculated:
>>>>
>>>> axis(1,at=year_mids,labels=1994:2017)
>>>>
>>>> Also note that I have added braces to the "for" loop. Putting it all
>>>> together:
>>>>
>>>> year_mids<-seq(182,5655,by=229)
>>>> pdf("EMs.pdf",width=20,height=20)
>>>> par(mfrow=c(5,4))
>>>> # import your first sheet here (16 columns)
>>>> EMs1.1<-read.csv("EMs1.1.csv")
>>>> ncolumns<-ncol(EMs1.1)
>>>> for(i in 1:ncolumns) {
>>>>   plot(EMs1.1[,i],type="l",col = "Red", xlab="Time",
>>>>        ylab="APEn", main=names(EMs1.1)[i],xaxt="n")
>>>>  axis(1,at=year_mids,labels=1994:2017)
>>>> }
>>>> #import your second sheet here, (1 column)
>>>> EMs2.1<-read.csv("EMs2.1.csv")
>>>> ncolumns<-ncol(EMs2.1)
>>>> for(i in 1:ncolumns) {
>>>>   plot(EMs2.1[,i],type="l",col = "Red", xlab="Time",
>>>>        ylab="APEn", main=names(EMs2.1)[i],xaxt="n")
>>>>  axis(1,at=year_mids,labels=1994:2017)
>>>> }
>>>> # import your Third sheet here, (1 column)
>>>> EMs3.1<-read.csv("EMs3.1.csv")
>>>> ncolumns<-ncol(EMs3.1)
>>>> for(i in 1:ncolumns) {
>>>>   plot(EMs3.1[,i],type="l",col = "Red", xlab="Time",
>>>>        ylab="APEn", main=names(EMs3.1)[i],xaxt="n")
>>>>  axis(1,at=year_mids,labels=1994:2017)
>>>> }
>>>> # import your fourth sheet here, (1 column)
>>>> EMs4.1<-read.csv("EMs4.1.csv")
>>>> ncolumns<-ncol(EMs4.1)
>>>> for(i in 1:ncolumns) {
>>>>   plot(EMs4.1[,i],type="l",col = "Red", xlab="Time",
>>>>        ylab="APEn", main=names(EMs4.1)[i],xaxt="n")
>>>>  axis(1,at=year_mids,labels=1994:2017)
>>>> }
>>>> # finish plotting
>>>> dev.off()
>>>>
>>>> With any luck, you are now okay. Remember, this is a hack to deal with
>>>> data that are not what you think they are.
>>>>
>>>> Jim
>>>>
>>>>
>>>
>>> --
>>> *Best Regards,*
>>> *Subhamitra Patra*
>>> *Phd. Research Scholar*
>>> *Department of Humanities and Social Sciences*
>>> *Indian Institute of Technology, Kharagpur*
>>> *INDIA*
>>>
>>
>>
>> --
>> *Best Regards,*
>> *Subhamitra Patra*
>> *Phd. Research Scholar*
>> *Department of Humanities and Social Sciences*
>> *Indian Institute of Technology, Kharagpur*
>> *INDIA*
>>
>

-- 
*Best Regards,*
*Subhamitra Patra*
*Phd. Research Scholar*
*Department of Humanities and Social Sciences*
*Indian Institute of Technology, Kharagpur*
*INDIA*

-------------- next part --------------
A non-text attachment was scrubbed...
Name: EMs.pdf
Type: application/pdf
Size: 545317 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20181217/96de6a51/attachment.pdf>

From t@n@@@ @ending from gm@il@com  Mon Dec 17 10:41:43 2018
From: t@n@@@ @ending from gm@il@com (Bogdan Tanasa)
Date: Mon, 17 Dec 2018 01:41:43 -0800
Subject: [R] display of ECDF
Message-ID: <CA+JEM01qM_WjU1J==ezpvVoF0UkqYd=Jb7rdQkAL6yfDJhpnqQ@mail.gmail.com>

Dear all,

please could you advise me on the following : I would like to display a few
CDF data (the R code is below), by using a set of numerical BREAKS on a X
axis to be shown at EQUAL DISTANCE from each other (although numerically,
the BREAKS are on log10 axis and do not reflecting an equal distance):

df <- data.frame(
  x = c(rnorm(100, 0, 3), rnorm(100, 0, 10)),
  g = gl(2, 100)
)

breaks=c(0.001, 0.01, 0.1, 1, 5, 10, 20, 30, 100)

ggplot(df, aes(x, colour = g)) + stat_ecdf()  +
scale_x_log10(breaks=breaks),

how shall I do it ? thanks a lot !

-- bogdan

	[[alternative HTML version deleted]]


From petr@pik@l @ending from prechez@@cz  Mon Dec 17 11:12:11 2018
From: petr@pik@l @ending from prechez@@cz (PIKAL Petr)
Date: Mon, 17 Dec 2018 10:12:11 +0000
Subject: [R] display of ECDF
In-Reply-To: <CA+JEM01qM_WjU1J==ezpvVoF0UkqYd=Jb7rdQkAL6yfDJhpnqQ@mail.gmail.com>
References: <CA+JEM01qM_WjU1J==ezpvVoF0UkqYd=Jb7rdQkAL6yfDJhpnqQ@mail.gmail.com>
Message-ID: <ecdb22af2875489083aa8bb84707133c@SRVEXCHCM1302.precheza.cz>

Hi

You should add limits and maybe some rotation to x axis

ggplot(df, aes(x, colour = g)) + stat_ecdf()  +
scale_x_log10(breaks=breaks, limits=c(0.01, 100))+
theme(axis.text.x = element_text(size=8, angle=45))

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Bogdan Tanasa
> Sent: Monday, December 17, 2018 10:42 AM
> To: r-help <r-help at r-project.org>
> Subject: [R] display of ECDF
>
> Dear all,
>
> please could you advise me on the following : I would like to display a few CDF
> data (the R code is below), by using a set of numerical BREAKS on a X axis to be
> shown at EQUAL DISTANCE from each other (although numerically, the BREAKS
> are on log10 axis and do not reflecting an equal distance):
>
> df <- data.frame(
>   x = c(rnorm(100, 0, 3), rnorm(100, 0, 10)),
>   g = gl(2, 100)
> )
>
> breaks=c(0.001, 0.01, 0.1, 1, 5, 10, 20, 30, 100)
>
> ggplot(df, aes(x, colour = g)) + stat_ecdf()  + scale_x_log10(breaks=breaks),
>
> how shall I do it ? thanks a lot !
>
> -- bogdan
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


From peter m@ili@g off mybetterlife@co@uk  Mon Dec 17 15:41:01 2018
From: peter m@ili@g off mybetterlife@co@uk (peter m@ili@g off mybetterlife@co@uk)
Date: Mon, 17 Dec 2018 14:41:01 -0000
Subject: [R] RTerm.exe
Message-ID: <07fd01d49616$8b5b9970$a212cc50$@mybetterlife.co.uk>

Hi,

 

I have recently downloaded R and accessed.  I  am using Zorro (a development
platform for ForeX) and requested to add my path for RTerm.exe

 

I can't find RTerm.exe and confirm that I appear to load R using "C:\Program
Files\R\R-3.5.1\bin\x64\Rgui.exe"

 

 

 

 

Regards

Peter H. Williams

 

113 Skelmorlie Castle Road

Skelmorlie

Ayrshire

PA17 5AL

TEL:- 01475 529946

Mobile:- 07773 348117

Skype:-   peterhwuk

 

 

 


From pjmiller_57 @ending from y@hoo@com  Mon Dec 17 16:27:51 2018
From: pjmiller_57 @ending from y@hoo@com (Paul Miller)
Date: Mon, 17 Dec 2018 15:27:51 +0000 (UTC)
Subject: [R] R code for if-then-do code blocks
In-Reply-To: <mailman.352837.1.1545044401.60322.r-help@r-project.org>
References: <mailman.352837.1.1545044401.60322.r-help@r-project.org>
Message-ID: <2088363583.889377.1545060471869@mail.yahoo.com>

Hello All,

Season's greetings!

 Am trying to replicate some SAS code in R. The SAS code uses if-then-do code blocks. I've been trying to do likewise in R as that seems to be the most reliable way to get the same result. 

Below is some toy data and some code that does work. There are some things I don't necessarily like about the code though. So I was hoping some people could help make it better. One thing I don't like is that the within function reverses the order of the computed columns such that test1:test5 becomes test5:test1. I've used a mutate to overcome that but would prefer not to have to do so. 

 Another, perhaps very small thing, is the need to calculate an ID variable that becomes the basis for a grouping.?

I did considerable Internet searching for R code that conditionally computes blocks of code. I didn't find much though and so am wondering if my search terms were not sufficient or if there is some other reason. It occurred to me that maybe if-then-do code blocks like we often see in SAS as are frowned upon and therefore not much implemented. 

I'd be interested in seeing more R-compatible approaches if this is the case. I've learned that it's a mistake to try and make R be like SAS. It's better to let R be R. Trouble is I'm not always sure how to do that. 

Thanks,

Paul


d1 <- data.frame(workshop=rep(1:2,4),
??????????????? gender=rep(c("f","m"),each=4))

library(tibble)
library(plyr)

d2 <- d1 %>%
? rownames_to_column("ID") %>%
? mutate(test1 = NA, test2 = NA, test4 = NA, test5 = NA) %>%
? ddply("ID",
??????? within,
??????? if (gender == "f" & workshop == 1) {
????????? test1 <- 1
????????? test1 <- 6 + test1
????????? test2 <- 2 + test1
????????? test4 <- 1
????????? test5 <- 1
??????? } else {
????????? test1 <- test2 <- test4 <- test5 <- 0
??????? })


From murdoch@dunc@n @ending from gm@il@com  Mon Dec 17 16:29:28 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Mon, 17 Dec 2018 10:29:28 -0500
Subject: [R] RTerm.exe
In-Reply-To: <07fd01d49616$8b5b9970$a212cc50$@mybetterlife.co.uk>
References: <07fd01d49616$8b5b9970$a212cc50$@mybetterlife.co.uk>
Message-ID: <6b801d1e-5aa0-e21e-61e2-50d3d41ef53c@gmail.com>

On 17/12/2018 9:41 AM, peter at mybetterlife.co.uk wrote:
> Hi,
> 
>   
> 
> I have recently downloaded R and accessed.  I  am using Zorro (a development
> platform for ForeX) and requested to add my path for RTerm.exe
> 
>   
> 
> I can't find RTerm.exe and confirm that I appear to load R using "C:\Program
> Files\R\R-3.5.1\bin\x64\Rgui.exe"
> 
>   

Not sure what your question is, but the executable is called Rterm.exe, 
and should be in the same directory as Rgui.exe.

Duncan Murdoch


From @imon@wood @ending from b@th@edu  Mon Dec 17 17:23:15 2018
From: @imon@wood @ending from b@th@edu (Simon Wood)
Date: Mon, 17 Dec 2018 16:23:15 +0000
Subject: [R] Quoting smooth random terms mccv::gam
In-Reply-To: <EFC6792B-87C7-42A1-8C03-E5FC83819B2A@mednet.ucla.edu>
References: <EFC6792B-87C7-42A1-8C03-E5FC83819B2A@mednet.ucla.edu>
Message-ID: <b5270dd1-5d3e-ed59-4c40-998ea76941fc@bath.edu>

I would quote the p-value, but not the statistic (as it is not a 
standard F stat). The actual statistic is given here:

https://academic.oup.com/biomet/article-pdf/100/4/1005/566200/ast038.pdf

On 14/12/2018 04:33, Smith, Desmond wrote:
> Dear All,
>
> I have a mgcv::gam model of the form:
>
> m1 <- gam(Y ~ A + s(B, bs = "re"), data = dataframe, family = gaussian, method = "REML")
>
> The random term is quoted in summary(m1) as, for example,
>
> Approximate significance of smooth terms:
>             # edf Ref.df      F p-value
> s(B)  4.486      5 97.195 6.7e-08 ***
>
> My question is, how would I quote this result (statistic and P value) in a formal document?
>
> For example, one possibility is F[4.486,5] = 97.195, P = 6.7e-08. However, arguing against this, ?reverse engineering? of the result using
>
> pf(q= 97.195, df1= 4.486, df2= 5, lower.tail=FALSE)
>
> gives an incorrect p value:
>
> [1] 0.1435508
>
> I would be very grateful for your advice. Many thanks for your help!
>
> ________________________________
>
> UCLA HEALTH SCIENCES IMPORTANT WARNING: This email (and any attachments) is only intended for the use of the person or entity to which it is addressed, and may contain information that is privileged and confidential. You, the recipient, are obligated to maintain it in a safe, secure and confidential manner. Unauthorized redisclosure or failure to maintain confidentiality may subject you to federal and state penalties. If you are not the intended recipient, please immediately notify us by return email, and delete this message from your computer.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From sour@ m@ili@g off i@st@te@edu  Mon Dec 17 17:50:09 2018
From: sour@ m@ili@g off i@st@te@edu (sour@ m@ili@g off i@st@te@edu)
Date: Mon, 17 Dec 2018 10:50:09 -0600
Subject: [R] Functional data anlysis for unequal length and unequal width
 time series
Message-ID: <9da3593489e7b0c95c183e6a9a2b723ed9ad1224.camel@iastate.edu>

Dear All,
            I apologize if you have already seen in Stack Overflow. I
have not got any response from there so I am posting for help here.

I have data on 1318 time series. Many of these series are of unequal
length. Apart from this also quite a few time points for each of the
series are observed at different time points. For example consider the
following four series

t1 <- c(24.51, 24.67, 24.91, 24.95, 25.10, 25.35, 25.50, 25.55, 25.67)
V1 <- c(-0.1710, -0.0824, -0.0419, -0.0416, -0.0216, -0.0792, -0.0656,-
0.0273, -0.0589)
ser1 <- cbind(t1, V1)

t2 <- c(24.5, 24.67, 24.91, 24.98, 25.14, 25.38)
V2 <- c(-0.0280, -0.1980, -0.2556, 0.3131, 0.3231, 0.2264)
ser2 <- cbind(t2, V2)

t3 <- c(24.51, 24.67, 24.91, 24.95, 25.10, 25.35, 25.50, 25.55, 25.65,
25.88, 25.97, 25.99)
V3 <- c(0.0897, -0.0533, -0.3497, -0.5684, -0.4294, -0.1109, 0.0352,
0.0550, -0.0536, 0.0185, -0.0295, -0.0324)
ser3 <- cbind(t3, V3)

t4 <- c(24.5, 24.67, 24.71, 24.98, 25.17)
V4 <- c(-0.0280, -0.1980, -0.2556, 0.3131, 0.3231)
ser4 <- cbind(t4, V4)

Here t1, t2, t3, t4 are the time points and V1, V2, V3, V4 are the
observations made at over those time points. The time points in the
actual data are Julian dates so they look like these, just that they
are much larger decimal figures like 2452450.6225.

I am trying to cluster these time series using functional data approach
for which I am using the "funFEM" package in R. Th examples present are
for equispaced and equal length time series so I am not sure how to use
the package for my data. Initially I tried by making all the time
series equal in length to the time series having the highest number of
observations (here equal to ser3) by adding NA's to the time series. So
following this example I made ser2 as

t2_n <- c(24.5, 24.67, 24.91, 24.98, 25.14, 25.38, 25.50, 25.55, 25.65,
25.88, 25.97, 25.99)
V2_na <- c(V2, rep(NA, 6))
ser2_na <- cbind(t2_n, V2_na)

Note that to make t2 equal to length of t3 I grabbed the last 6 time
points from t3. To make V2 equal in length to V3 I added NA's.

Then I created my data matrix as

dat <- rbind(V1_na, V2_na, V3, V4_na).

The code I used was

require(funFEM)
basis<- create.fourier.basis(c(min(t3), max(t3)), nbasis = 25) 
fdobj <- smooth.basis(c(min(t3), max(t3)) ,dat, basis)$fd

Note that the range is constructed using the maximum and minumum time
point of ser_3 series.

res <- funFEM(fdobj, K = 2:9, model = "all", crit = "bic", init =
"random") 

But this gives me an error

Error in svd(X) : infinite or missing values in 'x'.

Can anyone tell please help me on how to deal with this dataset for
this package or any alternative package?

Sincerly,
Souradeep


From thierry@onkelinx @ending from inbo@be  Mon Dec 17 18:14:33 2018
From: thierry@onkelinx @ending from inbo@be (Thierry Onkelinx)
Date: Mon, 17 Dec 2018 18:14:33 +0100
Subject: [R] R code for if-then-do code blocks
In-Reply-To: <2088363583.889377.1545060471869@mail.yahoo.com>
References: <mailman.352837.1.1545044401.60322.r-help@r-project.org>
 <2088363583.889377.1545060471869@mail.yahoo.com>
Message-ID: <CAJuCY5w9+YNW9HtOf8DRmMh0rjby5Z-5N9OjUUMS8Mxx3MhPyQ@mail.gmail.com>

Dear Paul,

R's power is that is works vectorised. Unlike SAS which is rowbased. Using
R in a SAS way will lead to very slow code.

Your examples can be written vectorised

d1 %>%
  rownames_to_column("ID") %>%
  mutate(
    test1 = ifelse(gender == "f" & workshop == 1, 7, 0),
    test2 = ifelse(gender == "f" & workshop == 1, test1 + 2, 0),
    test4 = ifelse(gender == "f" & workshop == 1, 1, 0),
    test5 = test4
  )

Here is a speed comparison.

library(microbenchmark)
microbenchmark(
  vector = {d1 %>%
    rownames_to_column("ID") %>%
    mutate(
      test1 = ifelse(gender == "f" & workshop == 1, 7, 0),
      test2 = ifelse(gender == "f" & workshop == 1, test1 + 2, 0),
      test4 = ifelse(gender == "f" & workshop == 1, 1, 0),
      test5 = test4
    ) },
  rowbased = {d1 %>%
  rownames_to_column("ID") %>%
  mutate(test1 = NA, test2 = NA, test4 = NA, test5 = NA) %>%
  ddply("ID",
        within,
        if (gender == "f" & workshop == 1) {
          test1 <- 1
          test1 <- 6 + test1
          test2 <- 2 + test1
          test4 <- 1
          test5 <- 1
        } else {
          test1 <- test2 <- test4 <- test5 <- 0
        })}
)


Best regards,

Thierry

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op ma 17 dec. 2018 om 16:30 schreef Paul Miller via R-help <
r-help at r-project.org>:

> Hello All,
>
> Season's greetings!
>
>  Am trying to replicate some SAS code in R. The SAS code uses if-then-do
> code blocks. I've been trying to do likewise in R as that seems to be the
> most reliable way to get the same result.
>
> Below is some toy data and some code that does work. There are some things
> I don't necessarily like about the code though. So I was hoping some people
> could help make it better. One thing I don't like is that the within
> function reverses the order of the computed columns such that test1:test5
> becomes test5:test1. I've used a mutate to overcome that but would prefer
> not to have to do so.
>
>  Another, perhaps very small thing, is the need to calculate an ID
> variable that becomes the basis for a grouping.
>
> I did considerable Internet searching for R code that conditionally
> computes blocks of code. I didn't find much though and so am wondering if
> my search terms were not sufficient or if there is some other reason. It
> occurred to me that maybe if-then-do code blocks like we often see in SAS
> as are frowned upon and therefore not much implemented.
>
> I'd be interested in seeing more R-compatible approaches if this is the
> case. I've learned that it's a mistake to try and make R be like SAS. It's
> better to let R be R. Trouble is I'm not always sure how to do that.
>
> Thanks,
>
> Paul
>
>
> d1 <- data.frame(workshop=rep(1:2,4),
>                 gender=rep(c("f","m"),each=4))
>
> library(tibble)
> library(plyr)
>
> d2 <- d1 %>%
>   rownames_to_column("ID") %>%
>   mutate(test1 = NA, test2 = NA, test4 = NA, test5 = NA) %>%
>   ddply("ID",
>         within,
>         if (gender == "f" & workshop == 1) {
>           test1 <- 1
>           test1 <- 6 + test1
>           test2 <- 2 + test1
>           test4 <- 1
>           test5 <- 1
>         } else {
>           test1 <- test2 <- test4 <- test5 <- 0
>         })
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From twoolm@n @ending from ont@rgettek@com  Mon Dec 17 18:33:47 2018
From: twoolm@n @ending from ont@rgettek@com (Tom Woolman)
Date: Mon, 17 Dec 2018 11:33:47 -0600
Subject: [R] Trying to fix code that will find highest 5 column names and
 their associated values for each row in a data frame in R
Message-ID: <20181217113347.Horde.HiAl-rAhWHuIZ4EX7S6E6y-@www.ontargettek.com>


I have a data frame each with 10 variables of integer data for various
  attributes about each row of data, and I need to know the highest 5  
variables related to each of
  row in this data frame and output that to a new data frame. In addition to
  the 5 highest variable names, I also need to know the corresponding 5
  highest variable values for each row.

  A simple code example to generate a sample data frame for this is:

  set.seed(1)
  DF <- matrix(sample(1:9,9),ncol=10,nrow=9)
  DF <- as.data.frame.matrix(DF)


This would result in an example data frame like this:

  #   V1 V2 V3 V4 V5 V6 V7 V8 V9 V10
  # 1  3  2  5  6  5  2  6  8  1   3
  # 2  1  4  7  8  7  7  3  4  2   9
  # 3  2  3  4  7  5  8  9  1  3   5
  # 4  3  8  3  4  5  6  7  4  6   5
  # 5  6  2  3  7  2  1  8  3  2   4
  # 6  8  2  4  8  3  2  9  7  6   5
  # 7  1  5  3  6  8  3  8  9  1   3
  # 8  9  3  5  8  4  9  7  8  1   2
  # 9  1  2  4  8  3  2  1  2  5   6


  My ideal output would be something like this:


  #      V1   V2   V3   V4   V5
  # 1  V2:9 V7:8 V8:7 V4:6 V3:5
  # 2  V9:9 V3:8 V5:7 V7:6 V4:5
  # 3  V5:9 V3:8 V2:7 V9:6 V7:5
  # 4  V8:9 V4:8 V2:7 V5:6 V9:5
  # 5  V9:9 V1:8 V6:7 V3:6 V5:5
  # 6  V8:9 V1:8 V5:7 V9:6 V4:5
  # 7  V2:9 V8:8 V7:7 V5:6 V9:5
  # 8  V4:9 V7:8 V9:7 V2:6 V8:5
  # 9  V3:9 V7:8 V8:7 V4:6 V5:5
  # 10 V6:9 V8:8 V1:7 V9:6 V4:5


  I was trying to use code, but this doesn't seem to work:

  out <- t(apply(DF, 1, function(x){
    o <- head(order(-x), 5)
    paste0(names(x[o]), ':', x[o])
  }))
  as.data.frame(out)



  Thanks everyone!


From rmh @ending from temple@edu  Mon Dec 17 18:49:42 2018
From: rmh @ending from temple@edu (Richard M. Heiberger)
Date: Mon, 17 Dec 2018 12:49:42 -0500
Subject: [R] R code for if-then-do code blocks
In-Reply-To: <CAJuCY5w9+YNW9HtOf8DRmMh0rjby5Z-5N9OjUUMS8Mxx3MhPyQ@mail.gmail.com>
References: <mailman.352837.1.1545044401.60322.r-help@r-project.org>
 <2088363583.889377.1545060471869@mail.yahoo.com>
 <CAJuCY5w9+YNW9HtOf8DRmMh0rjby5Z-5N9OjUUMS8Mxx3MhPyQ@mail.gmail.com>
Message-ID: <CAGx1TMBoajqpggJ-+dVFLr0sey6=Ec4D_gho4Pb3WzGyCZGF3A@mail.gmail.com>

this can be dome even faster, and I think more easily read, using only base R

d1 <- data.frame(workshop=rep(1:2,4),
                gender=rep(c("f","m"),each=4))

## needed by vector and rowbased, not needed by rmh
library(tibble)
library(plyr)
library(magrittr)

microbenchmark(
  vector = {d1 %>%
    rownames_to_column("ID") %>%
    mutate(
      test1 = ifelse(gender == "f" & workshop == 1, 7, 0),
      test2 = ifelse(gender == "f" & workshop == 1, test1 + 2, 0),
      test4 = ifelse(gender == "f" & workshop == 1, 1, 0),
      test5 = test4
    ) },
  rowbased = {d1 %>%
  rownames_to_column("ID") %>%
  mutate(test1 = NA, test2 = NA, test4 = NA, test5 = NA) %>%
  ddply("ID",
        within,
        if (gender == "f" & workshop == 1) {
          test1 <- 1
          test1 <- 6 + test1
          test2 <- 2 + test1
          test4 <- 1
          test5 <- 1
        } else {
          test1 <- test2 <- test4 <- test5 <- 0
        })},
  rmh={
    data.frame(ID=rownames(d1),
               d1,
               test1=0,
               test2=0,
               test4=0,
               test5=0)
    myRowSubset <- d3$gender=="f" & d3$workshop==1
    test1 <- 1
    d3[myRowSubset, "test1"] <- test1 + 6
    d3[myRowSubset, "test2"] <- test1 + 6 + 2
    d3[myRowSubset, c("test4", "test5")] <- test1
  }
)

Unit: microseconds
     expr      min       lq      mean   median        uq        max neval cld
   vector 1281.994 1468.102  1669.266 1573.043  1750.354   3171.777   100  a
 rowbased 8131.230 8691.899 10894.700 9219.882 10435.642 133293.034   100   b
      rmh  925.571 1056.530  1167.568 1116.425  1221.457   1968.199   100  a
On Mon, Dec 17, 2018 at 12:15 PM Thierry Onkelinx via R-help
<r-help at r-project.org> wrote:
>
> Dear Paul,
>
> R's power is that is works vectorised. Unlike SAS which is rowbased. Using
> R in a SAS way will lead to very slow code.
>
> Your examples can be written vectorised
>
> d1 %>%
>   rownames_to_column("ID") %>%
>   mutate(
>     test1 = ifelse(gender == "f" & workshop == 1, 7, 0),
>     test2 = ifelse(gender == "f" & workshop == 1, test1 + 2, 0),
>     test4 = ifelse(gender == "f" & workshop == 1, 1, 0),
>     test5 = test4
>   )
>
> Here is a speed comparison.
>
> library(microbenchmark)
> microbenchmark(
>   vector = {d1 %>%
>     rownames_to_column("ID") %>%
>     mutate(
>       test1 = ifelse(gender == "f" & workshop == 1, 7, 0),
>       test2 = ifelse(gender == "f" & workshop == 1, test1 + 2, 0),
>       test4 = ifelse(gender == "f" & workshop == 1, 1, 0),
>       test5 = test4
>     ) },
>   rowbased = {d1 %>%
>   rownames_to_column("ID") %>%
>   mutate(test1 = NA, test2 = NA, test4 = NA, test5 = NA) %>%
>   ddply("ID",
>         within,
>         if (gender == "f" & workshop == 1) {
>           test1 <- 1
>           test1 <- 6 + test1
>           test2 <- 2 + test1
>           test4 <- 1
>           test5 <- 1
>         } else {
>           test1 <- test2 <- test4 <- test5 <- 0
>         })}
> )
>
>
> Best regards,
>
> Thierry
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op ma 17 dec. 2018 om 16:30 schreef Paul Miller via R-help <
> r-help at r-project.org>:
>
> > Hello All,
> >
> > Season's greetings!
> >
> >  Am trying to replicate some SAS code in R. The SAS code uses if-then-do
> > code blocks. I've been trying to do likewise in R as that seems to be the
> > most reliable way to get the same result.
> >
> > Below is some toy data and some code that does work. There are some things
> > I don't necessarily like about the code though. So I was hoping some people
> > could help make it better. One thing I don't like is that the within
> > function reverses the order of the computed columns such that test1:test5
> > becomes test5:test1. I've used a mutate to overcome that but would prefer
> > not to have to do so.
> >
> >  Another, perhaps very small thing, is the need to calculate an ID
> > variable that becomes the basis for a grouping.
> >
> > I did considerable Internet searching for R code that conditionally
> > computes blocks of code. I didn't find much though and so am wondering if
> > my search terms were not sufficient or if there is some other reason. It
> > occurred to me that maybe if-then-do code blocks like we often see in SAS
> > as are frowned upon and therefore not much implemented.
> >
> > I'd be interested in seeing more R-compatible approaches if this is the
> > case. I've learned that it's a mistake to try and make R be like SAS. It's
> > better to let R be R. Trouble is I'm not always sure how to do that.
> >
> > Thanks,
> >
> > Paul
> >
> >
> > d1 <- data.frame(workshop=rep(1:2,4),
> >                 gender=rep(c("f","m"),each=4))
> >
> > library(tibble)
> > library(plyr)
> >
> > d2 <- d1 %>%
> >   rownames_to_column("ID") %>%
> >   mutate(test1 = NA, test2 = NA, test4 = NA, test5 = NA) %>%
> >   ddply("ID",
> >         within,
> >         if (gender == "f" & workshop == 1) {
> >           test1 <- 1
> >           test1 <- 6 + test1
> >           test2 <- 2 + test1
> >           test4 <- 1
> >           test5 <- 1
> >         } else {
> >           test1 <- test2 <- test4 <- test5 <- 0
> >         })
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dc@rl@on @ending from t@mu@edu  Mon Dec 17 21:55:40 2018
From: dc@rl@on @ending from t@mu@edu (David L Carlson)
Date: Mon, 17 Dec 2018 20:55:40 +0000
Subject: [R] 
 Trying to fix code that will find highest 5 column names and
 their associated values for each row in a data frame in R
In-Reply-To: <20181217113347.Horde.HiAl-rAhWHuIZ4EX7S6E6y-@www.ontargettek.com>
References: <20181217113347.Horde.HiAl-rAhWHuIZ4EX7S6E6y-@www.ontargettek.com>
Message-ID: <d9e067b03b0144c99dfbf85655321d40@tamu.edu>

There are some problems with your example. Your code does not produce anything like your example data frame because you draw only 9 values without replacement. Your code produces 10 columns, each with the same permutation of the values 1:9.

Then your desired output does not make sense in terms of your example data. The first entry is V2:9 but 9 does not appear in row 1.

Using your posted example:
DF <- structure(list(V1 = c(3L, 1L, 2L, 3L, 6L, 8L, 1L, 9L, 1L),
V2 = c(2L, 4L, 3L, 8L, 2L, 2L, 5L, 3L, 2L), V3 = c(5L, 7L, 4L, 3L,
3L, 4L, 3L, 5L, 4L), V4 = c(6L, 8L, 7L, 4L, 7L, 8L, 6L, 8L, 8L),
V5 = c(5L, 7L, 5L, 5L, 2L, 3L, 8L, 4L, 3L), V6 = c(2L, 7L, 8L, 6L,
1L, 2L, 3L, 9L, 2L), V7 = c(6L, 3L, 9L, 7L, 8L, 9L, 8L, 7L, 1L),
V8 = c(8L, 4L, 1L, 4L, 3L, 7L, 9L, 8L, 2L), V9 = c(1L, 2L, 3L, 6L,
2L, 6L, 1L, 1L, 5L), V10 = c(3L, 9L, 5L, 5L, 4L, 5L, 3L, 2L, 6L)), 
class = "data.frame", row.names = c(NA, -9L))

Your code produces:

     V1    V2   V3    V4    V5
1  V8:8  V4:6 V7:6  V3:5  V5:5
2 V10:9  V4:8 V3:7  V5:7  V6:7
3  V7:9  V6:8 V4:7  V5:5 V10:5
4  V2:8  V7:7 V6:6  V9:6  V5:5
5  V7:8  V4:7 V1:6 V10:4  V3:3
6  V7:9  V1:8 V4:8  V8:7  V9:6
7  V8:9  V5:8 V7:8  V4:6  V2:5
8  V1:9  V6:9 V4:8  V8:8  V7:7
9  V4:8 V10:6 V9:5  V3:4  V5:3

Which seems to be what you wanted.

---------------------------------------------
David L. Carlson
Department of Anthropology
Texas A&M University


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Tom Woolman
Sent: Monday, December 17, 2018 11:34 AM
To: r-help at r-project.org
Subject: [R] Trying to fix code that will find highest 5 column names and their associated values for each row in a data frame in R


I have a data frame each with 10 variables of integer data for various
  attributes about each row of data, and I need to know the highest 5  
variables related to each of
  row in this data frame and output that to a new data frame. In addition to
  the 5 highest variable names, I also need to know the corresponding 5
  highest variable values for each row.

  A simple code example to generate a sample data frame for this is:

  set.seed(1)
  DF <- matrix(sample(1:9,9),ncol=10,nrow=9)
  DF <- as.data.frame.matrix(DF)


This would result in an example data frame like this:

  #   V1 V2 V3 V4 V5 V6 V7 V8 V9 V10
  # 1  3  2  5  6  5  2  6  8  1   3
  # 2  1  4  7  8  7  7  3  4  2   9
  # 3  2  3  4  7  5  8  9  1  3   5
  # 4  3  8  3  4  5  6  7  4  6   5
  # 5  6  2  3  7  2  1  8  3  2   4
  # 6  8  2  4  8  3  2  9  7  6   5
  # 7  1  5  3  6  8  3  8  9  1   3
  # 8  9  3  5  8  4  9  7  8  1   2
  # 9  1  2  4  8  3  2  1  2  5   6


  My ideal output would be something like this:


  #      V1   V2   V3   V4   V5
  # 1  V2:9 V7:8 V8:7 V4:6 V3:5
  # 2  V9:9 V3:8 V5:7 V7:6 V4:5
  # 3  V5:9 V3:8 V2:7 V9:6 V7:5
  # 4  V8:9 V4:8 V2:7 V5:6 V9:5
  # 5  V9:9 V1:8 V6:7 V3:6 V5:5
  # 6  V8:9 V1:8 V5:7 V9:6 V4:5
  # 7  V2:9 V8:8 V7:7 V5:6 V9:5
  # 8  V4:9 V7:8 V9:7 V2:6 V8:5
  # 9  V3:9 V7:8 V8:7 V4:6 V5:5
  # 10 V6:9 V8:8 V1:7 V9:6 V4:5


  I was trying to use code, but this doesn't seem to work:

  out <- t(apply(DF, 1, function(x){
    o <- head(order(-x), 5)
    paste0(names(x[o]), ':', x[o])
  }))
  as.data.frame(out)



  Thanks everyone!

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From DSmith @ending from mednet@ucl@@edu  Mon Dec 17 23:26:07 2018
From: DSmith @ending from mednet@ucl@@edu (Smith, Desmond)
Date: Mon, 17 Dec 2018 22:26:07 +0000
Subject: [R] Quoting smooth random terms mccv::gam
In-Reply-To: <b5270dd1-5d3e-ed59-4c40-998ea76941fc@bath.edu>
References: <EFC6792B-87C7-42A1-8C03-E5FC83819B2A@mednet.ucla.edu>
 <b5270dd1-5d3e-ed59-4c40-998ea76941fc@bath.edu>
Message-ID: <C56F0D96-D264-4C95-9C12-A967B6AFF0CC@mednet.ucla.edu>

Thanks very much, Simon!

?On 12/17/18, 8:23 AM, "Simon Wood" <simon.wood at bath.edu> wrote:

    I would quote the p-value, but not the statistic (as it is not a 
    standard F stat). The actual statistic is given here:
    
    https://urldefense.proofpoint.com/v2/url?u=https-3A__academic.oup.com_biomet_article-2Dpdf_100_4_1005_566200_ast038.pdf&d=DwIDaQ&c=UXmaowRpu5bLSLEQRunJ2z-YIUZuUoa9Rw_x449Hd_Y&r=y6YM-SSv8WOxR70LMzwwFohC41WMNU4ZGFcHpTmGWLo&m=CqMTgm500nmgBrPfC2cLIc45sZ6h0I2odVPYT8qCmYA&s=epymthgL8_opS9pITmfKRJnH_uzCCzEWTYU9qEwQ_q0&e=
    
    On 14/12/2018 04:33, Smith, Desmond wrote:
    > Dear All,
    >
    > I have a mgcv::gam model of the form:
    >
    > m1 <- gam(Y ~ A + s(B, bs = "re"), data = dataframe, family = gaussian, method = "REML")
    >
    > The random term is quoted in summary(m1) as, for example,
    >
    > Approximate significance of smooth terms:
    >             # edf Ref.df      F p-value
    > s(B)  4.486      5 97.195 6.7e-08 ***
    >
    > My question is, how would I quote this result (statistic and P value) in a formal document?
    >
    > For example, one possibility is F[4.486,5] = 97.195, P = 6.7e-08. However, arguing against this, ?reverse engineering? of the result using
    >
    > pf(q= 97.195, df1= 4.486, df2= 5, lower.tail=FALSE)
    >
    > gives an incorrect p value:
    >
    > [1] 0.1435508
    >
    > I would be very grateful for your advice. Many thanks for your help!
    >
    > ________________________________
    >
    > UCLA HEALTH SCIENCES IMPORTANT WARNING: This email (and any attachments) is only intended for the use of the person or entity to which it is addressed, and may contain information that is privileged and confidential. You, the recipient, are obligated to maintain it in a safe, secure and confidential manner. Unauthorized redisclosure or failure to maintain confidentiality may subject you to federal and state penalties. If you are not the intended recipient, please immediately notify us by return email, and delete this message from your computer.
    >
    > 	[[alternative HTML version deleted]]
    >
    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIDaQ&c=UXmaowRpu5bLSLEQRunJ2z-YIUZuUoa9Rw_x449Hd_Y&r=y6YM-SSv8WOxR70LMzwwFohC41WMNU4ZGFcHpTmGWLo&m=CqMTgm500nmgBrPfC2cLIc45sZ6h0I2odVPYT8qCmYA&s=XCgKsfTTCyRoegz5hqMvtEt9m0KRm-qD0HtpXGYdxzg&e=
    > PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIDaQ&c=UXmaowRpu5bLSLEQRunJ2z-YIUZuUoa9Rw_x449Hd_Y&r=y6YM-SSv8WOxR70LMzwwFohC41WMNU4ZGFcHpTmGWLo&m=CqMTgm500nmgBrPfC2cLIc45sZ6h0I2odVPYT8qCmYA&s=hyi1_CVKTW4c_7lyb3TlmtPnSpsQfnI7BhRjVOLhXdI&e=
    > and provide commented, minimal, self-contained, reproducible code.
    


From rmh @ending from temple@edu  Tue Dec 18 01:02:07 2018
From: rmh @ending from temple@edu (Richard M. Heiberger)
Date: Mon, 17 Dec 2018 19:02:07 -0500
Subject: [R] R code for if-then-do code blocks
In-Reply-To: <CAGx1TMBoajqpggJ-+dVFLr0sey6=Ec4D_gho4Pb3WzGyCZGF3A@mail.gmail.com>
References: <mailman.352837.1.1545044401.60322.r-help@r-project.org>
 <2088363583.889377.1545060471869@mail.yahoo.com>
 <CAJuCY5w9+YNW9HtOf8DRmMh0rjby5Z-5N9OjUUMS8Mxx3MhPyQ@mail.gmail.com>
 <CAGx1TMBoajqpggJ-+dVFLr0sey6=Ec4D_gho4Pb3WzGyCZGF3A@mail.gmail.com>
Message-ID: <CAGx1TMAnyV1c=88vPDuK5p+4Pp8d8+AvjJwtoBb1HNa2egiN3g@mail.gmail.com>

I got another 10% savings with this example by using only one
subscripting adjustment.
I also fixed a typo in my previous posting (which didn't affect the timing).



microbenchmark(
 rmh={
    d3 <-data.frame(ID=rownames(d1),
               d1,
               test1=0,
               test2=0,
               test4=0,
               test5=0)
    myRowSubset <- d3$gender=="f" & d3$workshop==1
    test1 <- 1
    d3[myRowSubset, "test1"] <- test1 + 6
    d3[myRowSubset, "test2"] <- test1 + 6 + 2
    d3[myRowSubset, c("test4", "test5")] <- test1
  },
 rmh4={
   d4 <- data.frame(ID=rownames(d1),
                    d1,
                    test1=0,
                    test2=0,
                    test4=0,
                    test5=0)
   myRowSubset <- d4$gender=="f" & d4$workshop==1
   test1 <- 1
   d4[myRowSubset, c("test1", "test2", "test4", "test5")] <-
     matrix(test1 + c(6, 6+2, 0, 0), nrow=sum(myRowSubset), ncol=4, byrow=TRUE)
 }
)

Unit: microseconds
 expr     min       lq     mean   median       uq      max neval cld
  rmh 956.187 1183.304 1538.012 1617.985 1865.149 2177.071   100   b
 rmh4 850.729 1042.997 1380.842 1416.476 1700.307 2448.545   100  a


On Mon, Dec 17, 2018 at 12:49 PM Richard M. Heiberger <rmh at temple.edu> wrote:
>
> this can be dome even faster, and I think more easily read, using only base R
>
> d1 <- data.frame(workshop=rep(1:2,4),
>                 gender=rep(c("f","m"),each=4))
>
> ## needed by vector and rowbased, not needed by rmh
> library(tibble)
> library(plyr)
> library(magrittr)
>
> microbenchmark(
>   vector = {d1 %>%
>     rownames_to_column("ID") %>%
>     mutate(
>       test1 = ifelse(gender == "f" & workshop == 1, 7, 0),
>       test2 = ifelse(gender == "f" & workshop == 1, test1 + 2, 0),
>       test4 = ifelse(gender == "f" & workshop == 1, 1, 0),
>       test5 = test4
>     ) },
>   rowbased = {d1 %>%
>   rownames_to_column("ID") %>%
>   mutate(test1 = NA, test2 = NA, test4 = NA, test5 = NA) %>%
>   ddply("ID",
>         within,
>         if (gender == "f" & workshop == 1) {
>           test1 <- 1
>           test1 <- 6 + test1
>           test2 <- 2 + test1
>           test4 <- 1
>           test5 <- 1
>         } else {
>           test1 <- test2 <- test4 <- test5 <- 0
>         })},
>   rmh={
>     data.frame(ID=rownames(d1),
>                d1,
>                test1=0,
>                test2=0,
>                test4=0,
>                test5=0)
>     myRowSubset <- d3$gender=="f" & d3$workshop==1
>     test1 <- 1
>     d3[myRowSubset, "test1"] <- test1 + 6
>     d3[myRowSubset, "test2"] <- test1 + 6 + 2
>     d3[myRowSubset, c("test4", "test5")] <- test1
>   }
> )
>
> Unit: microseconds
>      expr      min       lq      mean   median        uq        max neval cld
>    vector 1281.994 1468.102  1669.266 1573.043  1750.354   3171.777   100  a
>  rowbased 8131.230 8691.899 10894.700 9219.882 10435.642 133293.034   100   b
>       rmh  925.571 1056.530  1167.568 1116.425  1221.457   1968.199   100  a
> On Mon, Dec 17, 2018 at 12:15 PM Thierry Onkelinx via R-help
> <r-help at r-project.org> wrote:
> >
> > Dear Paul,
> >
> > R's power is that is works vectorised. Unlike SAS which is rowbased. Using
> > R in a SAS way will lead to very slow code.
> >
> > Your examples can be written vectorised
> >
> > d1 %>%
> >   rownames_to_column("ID") %>%
> >   mutate(
> >     test1 = ifelse(gender == "f" & workshop == 1, 7, 0),
> >     test2 = ifelse(gender == "f" & workshop == 1, test1 + 2, 0),
> >     test4 = ifelse(gender == "f" & workshop == 1, 1, 0),
> >     test5 = test4
> >   )
> >
> > Here is a speed comparison.
> >
> > library(microbenchmark)
> > microbenchmark(
> >   vector = {d1 %>%
> >     rownames_to_column("ID") %>%
> >     mutate(
> >       test1 = ifelse(gender == "f" & workshop == 1, 7, 0),
> >       test2 = ifelse(gender == "f" & workshop == 1, test1 + 2, 0),
> >       test4 = ifelse(gender == "f" & workshop == 1, 1, 0),
> >       test5 = test4
> >     ) },
> >   rowbased = {d1 %>%
> >   rownames_to_column("ID") %>%
> >   mutate(test1 = NA, test2 = NA, test4 = NA, test5 = NA) %>%
> >   ddply("ID",
> >         within,
> >         if (gender == "f" & workshop == 1) {
> >           test1 <- 1
> >           test1 <- 6 + test1
> >           test2 <- 2 + test1
> >           test4 <- 1
> >           test5 <- 1
> >         } else {
> >           test1 <- test2 <- test4 <- test5 <- 0
> >         })}
> > )
> >
> >
> > Best regards,
> >
> > Thierry
> >
> > ir. Thierry Onkelinx
> > Statisticus / Statistician
> >
> > Vlaamse Overheid / Government of Flanders
> > INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> > FOREST
> > Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> > thierry.onkelinx at inbo.be
> > Havenlaan 88 bus 73, 1000 Brussel
> > www.inbo.be
> >
> > ///////////////////////////////////////////////////////////////////////////////////////////
> > To call in the statistician after the experiment is done may be no more
> > than asking him to perform a post-mortem examination: he may be able to say
> > what the experiment died of. ~ Sir Ronald Aylmer Fisher
> > The plural of anecdote is not data. ~ Roger Brinner
> > The combination of some data and an aching desire for an answer does not
> > ensure that a reasonable answer can be extracted from a given body of data.
> > ~ John Tukey
> > ///////////////////////////////////////////////////////////////////////////////////////////
> >
> > <https://www.inbo.be>
> >
> >
> > Op ma 17 dec. 2018 om 16:30 schreef Paul Miller via R-help <
> > r-help at r-project.org>:
> >
> > > Hello All,
> > >
> > > Season's greetings!
> > >
> > >  Am trying to replicate some SAS code in R. The SAS code uses if-then-do
> > > code blocks. I've been trying to do likewise in R as that seems to be the
> > > most reliable way to get the same result.
> > >
> > > Below is some toy data and some code that does work. There are some things
> > > I don't necessarily like about the code though. So I was hoping some people
> > > could help make it better. One thing I don't like is that the within
> > > function reverses the order of the computed columns such that test1:test5
> > > becomes test5:test1. I've used a mutate to overcome that but would prefer
> > > not to have to do so.
> > >
> > >  Another, perhaps very small thing, is the need to calculate an ID
> > > variable that becomes the basis for a grouping.
> > >
> > > I did considerable Internet searching for R code that conditionally
> > > computes blocks of code. I didn't find much though and so am wondering if
> > > my search terms were not sufficient or if there is some other reason. It
> > > occurred to me that maybe if-then-do code blocks like we often see in SAS
> > > as are frowned upon and therefore not much implemented.
> > >
> > > I'd be interested in seeing more R-compatible approaches if this is the
> > > case. I've learned that it's a mistake to try and make R be like SAS. It's
> > > better to let R be R. Trouble is I'm not always sure how to do that.
> > >
> > > Thanks,
> > >
> > > Paul
> > >
> > >
> > > d1 <- data.frame(workshop=rep(1:2,4),
> > >                 gender=rep(c("f","m"),each=4))
> > >
> > > library(tibble)
> > > library(plyr)
> > >
> > > d2 <- d1 %>%
> > >   rownames_to_column("ID") %>%
> > >   mutate(test1 = NA, test2 = NA, test4 = NA, test5 = NA) %>%
> > >   ddply("ID",
> > >         within,
> > >         if (gender == "f" & workshop == 1) {
> > >           test1 <- 1
> > >           test1 <- 6 + test1
> > >           test2 <- 2 + test1
> > >           test4 <- 1
> > >           test5 <- 1
> > >         } else {
> > >           test1 <- test2 <- test4 <- test5 <- 0
> > >         })
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From drjimlemon @ending from gm@il@com  Tue Dec 18 02:56:20 2018
From: drjimlemon @ending from gm@il@com (Jim Lemon)
Date: Tue, 18 Dec 2018 12:56:20 +1100
Subject: [R] 
 [R studio] Plotting of line chart for each columns at 1 page
In-Reply-To: <CAOFE=kPQDaqAKc-gMiUvEO=Zbo1+Ca2Hqm-KYX2DjeM6FnZqKw@mail.gmail.com>
References: <CAOFE=kOWXrnRnUVtL7A0vLKjL03571WQy5W_865hTFeBz28+tA@mail.gmail.com>
 <CA+8X3fW3Op8icpvA68O2sYPpBi=rpO2DWVXwL5UrPOHtDqDzpw@mail.gmail.com>
 <CAOFE=kNV6P4jRtrYS9K1E76pn+tW_aVpQ4Kq25ZHtQ6v-m=TkQ@mail.gmail.com>
 <CA+8X3fXM9KEb8=U1y-E_DFy9UuXk7NXDyRo6VppnEEAbAtGDDQ@mail.gmail.com>
 <CAOFE=kO20RsVsvVL2ZXpsH51aw9bzgCc3tDJ7DX8fM-tgkkbdQ@mail.gmail.com>
 <CA+8X3fUh8bug_m8iGBGcdPdb=PeGXY0xG48NTwBHC2XzXrtyhg@mail.gmail.com>
 <CAOFE=kNda7BE7cnkUg3nTZNAZbG0-dq=ir2ddXy_1pRMwySpcg@mail.gmail.com>
 <CA+8X3fVsMos2cafXXk7seO=fpM+JFgufmQpcUhGqJ9nNm4BFmQ@mail.gmail.com>
 <CAOFE=kOiTHOzcQ+dL2Tak2WnGBx1JAXpryj5CbLefdofa-7x5Q@mail.gmail.com>
 <CA+8X3fVYFJ9GUzY5ujH38zEQhWKKrkUpRzf1UjC-Cc=GHMe0VA@mail.gmail.com>
 <CAOFE=kOBPM-6bdaQKC3NXTXuAn_9OOA8oEvOGKhrGdUxBUAVkw@mail.gmail.com>
 <CA+8X3fUSY13o=khzBy9RA1_qsP1vkVqNAm=UP-xAd_cyLNrAVw@mail.gmail.com>
 <CAOFE=kPwv6LspZ=4RvL9x2JE7YSE9Ad+4Rrdhpmj0hyEHmVhxA@mail.gmail.com>
 <CAOFE=kN_tdNCzu50RhVM4-k7d6psEd=vZu7pD0htdczBKSn-HQ@mail.gmail.com>
 <CAOFE=kN6uZ9mKCkfDkz+xXLMxs3+WBu=A3YFAP-0UWt_b5V1FA@mail.gmail.com>
 <CAOFE=kMgKxo--vVCLhaPuY-4+5LD=HCZuyhBXBDFzUt0DG3ThA@mail.gmail.com>
 <CA+8X3fUTof6=q9JBNnMNpbEqF_7Jxi4WnTPF+2-c62ihMqmSaQ@mail.gmail.com>
 <CAOFE=kOUbBpUidnAxQjSzN4yhUBiaCC2GRgG8ovqm1BeuJ86ig@mail.gmail.com>
 <CA+8X3fWHutE7ibNvBWzdZWYCqQc1zpKr3EAidnN=oaGUgOmRKw@mail.gmail.com>
 <CAOFE=kOatoPvDrY6bFBqnLHojJ08HL6qMrngV4dLYpyW+Eewkg@mail.gmail.com>
 <CAOFE=kMPdr6RDDJCKV_Uw73uEUKV=RgOatrRaxnwsZgDT1rrOw@mail.gmail.com>
 <CA+8X3fWe5k2_QF4AwOKdyqqU0U62m-eSNriwPqny=Hh06sU2nw@mail.gmail.com>
 <CAOFE=kPQDaqAKc-gMiUvEO=Zbo1+Ca2Hqm-KYX2DjeM6FnZqKw@mail.gmail.com>
Message-ID: <CA+8X3fWnA=4PYGqxM8hq-Fm-OEJ0qhqezx1x=a+LWQ7C=CrF1w@mail.gmail.com>

Hi Subhamitra,
As for the error that you mention, it was probably:

Error in axis(1, at = year_mids, labels = 3 - 1 - 1994:3 - 8 - 2017) :
  'at' and 'labels' lengths differ, 24 != 1992

Anything more than a passing glance reveals that you didn't read the
explanation I sent about the arguments passed to the "axis" function.
Perhaps it will be rewarding to read the help page for the "axis" function
in the "graphics" package.

Your confusion about the logic (really simple arithmetic) of assigning
positions for the year labels may be allayed by the following. Think back
to those grade school problems that read:

 "If I have m apples to give to n people, how many must I give each person
so that all will receive the same number and I will have the fewest apples
left?"

I'm sure that you remember that this can be solved in a number of ways. You
can divide m/n and drop the remainder. So, from 03-01-2002 to 03-08-2017 in
EMs2.1:

diff(as.Date(c("03-01-2002","03-08-2017"),"%d-%m-%Y"))
Time difference of 5691 days
# plus 1 for all of the days included
# calculate the number of years
5692/365.25
[1] 15.58385

So if there had been an observation each day, you would have the trivial
task of dividing the number of days by the number of years to get the tick
increments:

5692/15.58385
365.2499

Of course you don't have that many observations and you are trying to get
the number of observations, not days, in each year. By making the
assumption that the missing observations are spread evenly over the years,
you can simply replace the number of days with the number of observations.
At the moment I don't have that as I unrared your data at home. But you do
have it and I will call it nobs:

# this calculates the number of observations per year
nobs/15.58385
<obs_per_year>

will yield the number of observations in each year. So you have your tick
increments. Now for the offset. If you want the year ticks to appear at the
middle of each year, you will want to start at 182 minus the two days
missing in January or 180. So your new year_mids will be:

year_mids<-seq(180,nobs,obs_per_year)

Your years are 2002:2017 for EMs2.1, so:

axis(1,year_mids,2002:2017)

may well be what you want for axis ticks. As you can see, the "m apples to
n people" approach gives you the answer. The only missing part was the
offset, or where to start handing out apples. You might want to have
another look at the help pages for "axis" and "seq" (or ":") which will
show you why your axis command failed badly. Good luck.

Jim

On Mon, Dec 17, 2018 at 6:12 PM Subhamitra Patra <subhamitra.patra at gmail.com>
wrote:

> Hello Sir,
>
> Thank you very much for your excellent guidance to a new R learner.
>
> I tried with your suggested code and got the expected results, but for the
> 2 CSV files (i.e. EMs2.1. and EMs.3.1), the date column is not coming in
> the X-axis (shown in the last row of the attached result Pdf file).  I
> think I need to increase more or less than 229 in the year-mids because for
> both the CSV files, starting date is 03-01-2002 and 04-07-2001
> (date-month-year) for EMs 2.1. and EMs 3.1. respectively. *Sir, hence I
> am quite confused for the logic behind the fixing of year_mids*. For your
> convenience, I am attaching both the code and result file.
>

	[[alternative HTML version deleted]]


From n@th@n@f@p@r@on@ @ending from gm@il@com  Tue Dec 18 04:27:42 2018
From: n@th@n@f@p@r@on@ @ending from gm@il@com (Nathan Parsons)
Date: Mon, 17 Dec 2018 19:27:42 -0800
Subject: [R] error = FALSE causes knit2wp to throw duplicate label error
In-Reply-To: <3428E4A8-43DF-4EF8-9838-F006BF7F7C15@dcn.davis.ca.us>
References: <CANdqJqeawL2fdbeZi3jqiNSs1VJORG5e=XnRFWo2J4EGkJhZvQ@mail.gmail.com>
 <3428E4A8-43DF-4EF8-9838-F006BF7F7C15@dcn.davis.ca.us>
Message-ID: <CANdqJqdRaJ_7mxXUpSrZ2bXDsgcvsJzy5K-trbpLBzrcQuD3jg@mail.gmail.com>

Thanks for getting me pointed in the right direction. If I happen upon
a satisfactory solution, I will report back!

Nate Parsons
Pronouns: He, Him, His
Graduate Teaching Assistant
Department of Sociology
Portland State University
Portland, Oregon

Schedule an appointment: https://calendly.com/nate-parsons

503-893-8281
503-725-3957 FAX


Nate Parsons
Pronouns: He, Him, His
Graduate Teaching Assistant
Department of Sociology
Portland State University
Portland, Oregon

Schedule an appointment: https://calendly.com/nate-parsons

503-893-8281
503-725-3957 FAX


On Sun, Dec 16, 2018 at 1:46 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>
> This seems a bit deep into knitr for R-help... you might have better luck on StackExchange. I also suggest that posting an incomplete example is usually the kiss of death for getting constructive assistance online.
>
> FWIW my guess is that executing knitr from within an Rmarkdown document is a bad idea unless you are building using child documents. Try manipulating your markdown from an R file.
>
> On December 16, 2018 11:48:44 AM PST, Nathan Parsons <nathan.f.parsons at gmail.com> wrote:
> >Goal: post from R to Wordpress installation on server.
> >
> >Problem: R keeps returning the error ?Error in parse_block(g[-1],
> >g[1], params.src) : duplicate label 'setup?? if error = FALSE in the
> >knitr options or in an r chunk. It works fine if error = TRUE. I could
> >just go through each post each time and remove any returned errors
> >manually, but I'd like to find a more permanent solution.
> >
> >I don't have any duplicate labels; is knit2wp somehow introducing a
> >duplicate label in the .Rmd
> >-> .md / upload process?
> >
> >My code:
> >
> >```{r setup, include=FALSE}
> >## Set the global chunk options for knitting reports
> >  knitr::opts_chunk$set(
> >    echo = TRUE,
> >    eval = TRUE,
> >    message = TRUE,
> >    error = FALSE,
> >    warning = TRUE,
> >    highlight = TRUE,
> >    prompt = FALSE
> >  )
> >
> >## Load and activate libraries using 'pacman' package
> >  if (!require(pacman)) {
> >    install.packages("pacman", repos = "http://cran.us.r-project.org")
> >  require(pacman)
> >  }
> >
> >  pacman::p_load_gh("duncantl/XMLRPC",
> >    "duncantl/RWordPress")
> >  pacman::p_load("knitr")
> >```
> >
> >```{r chunk1, echo = FALSE}
> >## post information
> >  fileName <- "fancy_post.Rmd"
> >  postTitle <- "Fancy Post Title"
> >
> >```
> >
> >blah blah blah...
> >
> >```{r chunk2, echo = FALSE}
> >## Set working directory to correct location
> >  last_dir <- getwd()
> >  setwd("~/Sites/posts")
> >
> >## Tell knitr to create the html code and upload it to your wordpress
> >site
> >  knit2wp(input = fileName,
> >    title = postTitle,
> >    publish = FALSE,
> >    action = 'newPost')
> >
> >  setwd(last_dir)
> >```
> >
> >
> >Traceback:
> >Error in parse_block(g[-1], g[1], params.src) : duplicate label 'setup'
> >26. stop("duplicate label '", label, "'")
> >25. parse_block(g[-1], g[1], params.src)
> >24. FUN(X[[i]], ...)
> >23. lapply(groups, function(g) { block = grepl(chunk.begin, g[1]) if
> >(!set.preamble && !parent_mode()) { return(if (block) "" else g) ...
> >22. split_file(lines = text)
> >21. process_file(text, output)
> >20. knit(input, encoding = encoding, envir = envir)
> >19. knit2wp(input = fileName, title = postTitle, publish = FALSE,
> >action = "newPost")
> >18. eval(expr, envir, enclos)
> >17. eval(expr, envir, enclos)
> >16. withVisible(eval(expr, envir, enclos))
> >15. withCallingHandlers(withVisible(eval(expr, envir, enclos)),
> >warning = wHandler, error = eHandler, message = mHandler)
> >14. handle(ev <- withCallingHandlers(withVisible(eval(expr, envir,
> >enclos)), warning = wHandler, error = eHandler, message = mHandler))
> >13. timing_fn(handle(ev <- withCallingHandlers(withVisible(eval(expr,
> >envir, enclos)), warning = wHandler, error = eHandler, message =
> >mHandler)))
> >12. evaluate_call(expr, parsed$src[[i]], envir = envir, enclos =
> >enclos, debug = debug, last = i == length(out), use_try =
> >stop_on_error != 2L, keep_warning = keep_warning, keep_message =
> >keep_message, output_handler = output_handler, include_timing =
> >include_timing)
> >11. evaluate::evaluate(...)
> >10. evaluate(code, envir = env, new_device = FALSE, keep_warning =
> >!isFALSE(options$warning), keep_message = !isFALSE(options$message),
> >stop_on_error = if (options$error && options$include) 0L else 2L,
> >output_handler = knit_handlers(options$render, options))
> >9. in_dir(input_dir(), evaluate(code, envir = env, new_device = FALSE,
> >keep_warning = !isFALSE(options$warning), keep_message =
> >!isFALSE(options$message), stop_on_error = if (options$error &&
> >options$include) 0L else 2L, output_handler =
> >knit_handlers(options$render, options)))
> >8. block_exec(params)
> >7. call_block(x)
> >6. process_group.block(group)
> >5. process_group(group)
> >4. withCallingHandlers(if (tangle) process_tangle(group) else
> >process_group(group), error = function(e) { setwd(wd) cat(res, sep =
> >"\n", file = output %n% "") ...
> >3. process_file(text, output)
> >2. knit(input, encoding = encoding, envir = envir)
> >1. knit2wp(input = fileName, title = postTitle, publish = FALSE,
> >action = "newPost")
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.


From bgunter@4567 @ending from gm@il@com  Tue Dec 18 04:40:14 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Mon, 17 Dec 2018 19:40:14 -0800
Subject: [R] 
 Functional data anlysis for unequal length and unequal width
 time series
In-Reply-To: <9da3593489e7b0c95c183e6a9a2b723ed9ad1224.camel@iastate.edu>
References: <9da3593489e7b0c95c183e6a9a2b723ed9ad1224.camel@iastate.edu>
Message-ID: <CAGxFJbS8QZre-ViPTad53m6EdaSiVVnL15+UxXHeLspu-xoMfQ@mail.gmail.com>

Specialized: Probably need to email the maintainer. See ?maintainer

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Dec 17, 2018 at 9:27 AM <soura at iastate.edu> wrote:

> Dear All,
>             I apologize if you have already seen in Stack Overflow. I
> have not got any response from there so I am posting for help here.
>
> I have data on 1318 time series. Many of these series are of unequal
> length. Apart from this also quite a few time points for each of the
> series are observed at different time points. For example consider the
> following four series
>
> t1 <- c(24.51, 24.67, 24.91, 24.95, 25.10, 25.35, 25.50, 25.55, 25.67)
> V1 <- c(-0.1710, -0.0824, -0.0419, -0.0416, -0.0216, -0.0792, -0.0656,-
> 0.0273, -0.0589)
> ser1 <- cbind(t1, V1)
>
> t2 <- c(24.5, 24.67, 24.91, 24.98, 25.14, 25.38)
> V2 <- c(-0.0280, -0.1980, -0.2556, 0.3131, 0.3231, 0.2264)
> ser2 <- cbind(t2, V2)
>
> t3 <- c(24.51, 24.67, 24.91, 24.95, 25.10, 25.35, 25.50, 25.55, 25.65,
> 25.88, 25.97, 25.99)
> V3 <- c(0.0897, -0.0533, -0.3497, -0.5684, -0.4294, -0.1109, 0.0352,
> 0.0550, -0.0536, 0.0185, -0.0295, -0.0324)
> ser3 <- cbind(t3, V3)
>
> t4 <- c(24.5, 24.67, 24.71, 24.98, 25.17)
> V4 <- c(-0.0280, -0.1980, -0.2556, 0.3131, 0.3231)
> ser4 <- cbind(t4, V4)
>
> Here t1, t2, t3, t4 are the time points and V1, V2, V3, V4 are the
> observations made at over those time points. The time points in the
> actual data are Julian dates so they look like these, just that they
> are much larger decimal figures like 2452450.6225.
>
> I am trying to cluster these time series using functional data approach
> for which I am using the "funFEM" package in R. Th examples present are
> for equispaced and equal length time series so I am not sure how to use
> the package for my data. Initially I tried by making all the time
> series equal in length to the time series having the highest number of
> observations (here equal to ser3) by adding NA's to the time series. So
> following this example I made ser2 as
>
> t2_n <- c(24.5, 24.67, 24.91, 24.98, 25.14, 25.38, 25.50, 25.55, 25.65,
> 25.88, 25.97, 25.99)
> V2_na <- c(V2, rep(NA, 6))
> ser2_na <- cbind(t2_n, V2_na)
>
> Note that to make t2 equal to length of t3 I grabbed the last 6 time
> points from t3. To make V2 equal in length to V3 I added NA's.
>
> Then I created my data matrix as
>
> dat <- rbind(V1_na, V2_na, V3, V4_na).
>
> The code I used was
>
> require(funFEM)
> basis<- create.fourier.basis(c(min(t3), max(t3)), nbasis = 25)
> fdobj <- smooth.basis(c(min(t3), max(t3)) ,dat, basis)$fd
>
> Note that the range is constructed using the maximum and minumum time
> point of ser_3 series.
>
> res <- funFEM(fdobj, K = 2:9, model = "all", crit = "bic", init =
> "random")
>
> But this gives me an error
>
> Error in svd(X) : infinite or missing values in 'x'.
>
> Can anyone tell please help me on how to deal with this dataset for
> this package or any alternative package?
>
> Sincerly,
> Souradeep
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ggrothendieck @ending from gm@il@com  Tue Dec 18 04:55:57 2018
From: ggrothendieck @ending from gm@il@com (Gabor Grothendieck)
Date: Mon, 17 Dec 2018 22:55:57 -0500
Subject: [R] R code for if-then-do code blocks
In-Reply-To: <2088363583.889377.1545060471869@mail.yahoo.com>
References: <mailman.352837.1.1545044401.60322.r-help@r-project.org>
 <2088363583.889377.1545060471869@mail.yahoo.com>
Message-ID: <CAP01uRn0AyT=oRq13TMjz4e+-iLiKtyxfwrj_tQBmTKZ88KXKQ@mail.gmail.com>

There is some discussion of approaches to this here:

https://stackoverflow.com/questions/34096162/dplyr-mutate-replace-on-a-subset-of-rows/34096575#34096575


On Mon, Dec 17, 2018 at 10:30 AM Paul Miller via R-help
<r-help at r-project.org> wrote:
>
> Hello All,
>
> Season's greetings!
>
>  Am trying to replicate some SAS code in R. The SAS code uses if-then-do code blocks. I've been trying to do likewise in R as that seems to be the most reliable way to get the same result.
>
> Below is some toy data and some code that does work. There are some things I don't necessarily like about the code though. So I was hoping some people could help make it better. One thing I don't like is that the within function reverses the order of the computed columns such that test1:test5 becomes test5:test1. I've used a mutate to overcome that but would prefer not to have to do so.
>
>  Another, perhaps very small thing, is the need to calculate an ID variable that becomes the basis for a grouping.
>
> I did considerable Internet searching for R code that conditionally computes blocks of code. I didn't find much though and so am wondering if my search terms were not sufficient or if there is some other reason. It occurred to me that maybe if-then-do code blocks like we often see in SAS as are frowned upon and therefore not much implemented.
>
> I'd be interested in seeing more R-compatible approaches if this is the case. I've learned that it's a mistake to try and make R be like SAS. It's better to let R be R. Trouble is I'm not always sure how to do that.
>
> Thanks,
>
> Paul
>
>
> d1 <- data.frame(workshop=rep(1:2,4),
>                 gender=rep(c("f","m"),each=4))
>
> library(tibble)
> library(plyr)
>
> d2 <- d1 %>%
>   rownames_to_column("ID") %>%
>   mutate(test1 = NA, test2 = NA, test4 = NA, test5 = NA) %>%
>   ddply("ID",
>         within,
>         if (gender == "f" & workshop == 1) {
>           test1 <- 1
>           test1 <- 6 + test1
>           test2 <- 2 + test1
>           test4 <- 1
>           test5 <- 1
>         } else {
>           test1 <- test2 <- test4 <- test5 <- 0
>         })
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From jdnewmil @ending from dcn@d@vi@@c@@u@  Tue Dec 18 07:53:14 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Mon, 17 Dec 2018 22:53:14 -0800
Subject: [R] 
 Functional data anlysis for unequal length and unequal width
 time series
In-Reply-To: <9da3593489e7b0c95c183e6a9a2b723ed9ad1224.camel@iastate.edu>
References: <9da3593489e7b0c95c183e6a9a2b723ed9ad1224.camel@iastate.edu>
Message-ID: <010DBD22-9CE2-4A45-9514-02D2B0E9A93A@dcn.davis.ca.us>

You will learn something useful if you search for "rolling join". The zoo package can handle this, as can the data.table package (read the vignette).

Your decision to pad with NA at the end was ill-considered... the first point of your first series is between the first two points of your second series... you need to interleave the points somehow.

You will need to decide whether you want to use piecewise linear approximation (as with the base "approx" function) or the more stable last-observation-carried-forward ("locf") or cubic splines or something more exotic like Fourier interpolation to identify the new interpolated "y" values in each series.

You can avoid the rolling join if you intend to resample the series to have points at regular intervals.  Just apply your preferred interpolation technique with your intended mesh of regular time values to each of your series in turn and then use cbind with the results.

I don't know anything about the package you mention, but getting time series data aligned is a common preprocessing step for many time series analysis.

Oh, and to you should probably be familiar with that CRAN Time Series Task View [1].

PS you should provide a link back to your original posting when moving the conversation to a different venue in case the discussion doesn't stay dead there.

[1] https://cran.r-project.org/web/views/TimeSeries.html

On December 17, 2018 8:50:09 AM PST, soura at iastate.edu wrote:
>Dear All,
>            I apologize if you have already seen in Stack Overflow. I
>have not got any response from there so I am posting for help here.
>
>I have data on 1318 time series. Many of these series are of unequal
>length. Apart from this also quite a few time points for each of the
>series are observed at different time points. For example consider the
>following four series
>
>t1 <- c(24.51, 24.67, 24.91, 24.95, 25.10, 25.35, 25.50, 25.55, 25.67)
>V1 <- c(-0.1710, -0.0824, -0.0419, -0.0416, -0.0216, -0.0792, -0.0656,-
>0.0273, -0.0589)
>ser1 <- cbind(t1, V1)
>
>t2 <- c(24.5, 24.67, 24.91, 24.98, 25.14, 25.38)
>V2 <- c(-0.0280, -0.1980, -0.2556, 0.3131, 0.3231, 0.2264)
>ser2 <- cbind(t2, V2)
>
>t3 <- c(24.51, 24.67, 24.91, 24.95, 25.10, 25.35, 25.50, 25.55, 25.65,
>25.88, 25.97, 25.99)
>V3 <- c(0.0897, -0.0533, -0.3497, -0.5684, -0.4294, -0.1109, 0.0352,
>0.0550, -0.0536, 0.0185, -0.0295, -0.0324)
>ser3 <- cbind(t3, V3)
>
>t4 <- c(24.5, 24.67, 24.71, 24.98, 25.17)
>V4 <- c(-0.0280, -0.1980, -0.2556, 0.3131, 0.3231)
>ser4 <- cbind(t4, V4)
>
>Here t1, t2, t3, t4 are the time points and V1, V2, V3, V4 are the
>observations made at over those time points. The time points in the
>actual data are Julian dates so they look like these, just that they
>are much larger decimal figures like 2452450.6225.
>
>I am trying to cluster these time series using functional data approach
>for which I am using the "funFEM" package in R. Th examples present are
>for equispaced and equal length time series so I am not sure how to use
>the package for my data. Initially I tried by making all the time
>series equal in length to the time series having the highest number of
>observations (here equal to ser3) by adding NA's to the time series. So
>following this example I made ser2 as
>
>t2_n <- c(24.5, 24.67, 24.91, 24.98, 25.14, 25.38, 25.50, 25.55, 25.65,
>25.88, 25.97, 25.99)
>V2_na <- c(V2, rep(NA, 6))
>ser2_na <- cbind(t2_n, V2_na)
>
>Note that to make t2 equal to length of t3 I grabbed the last 6 time
>points from t3. To make V2 equal in length to V3 I added NA's.
>
>Then I created my data matrix as
>
>dat <- rbind(V1_na, V2_na, V3, V4_na).
>
>The code I used was
>
>require(funFEM)
>basis<- create.fourier.basis(c(min(t3), max(t3)), nbasis = 25) 
>fdobj <- smooth.basis(c(min(t3), max(t3)) ,dat, basis)$fd
>
>Note that the range is constructed using the maximum and minumum time
>point of ser_3 series.
>
>res <- funFEM(fdobj, K = 2:9, model = "all", crit = "bic", init =
>"random") 
>
>But this gives me an error
>
>Error in svd(X) : infinite or missing values in 'x'.
>
>Can anyone tell please help me on how to deal with this dataset for
>this package or any alternative package?
>
>Sincerly,
>Souradeep
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From hen@_4567 @ending from y@hoo@com  Tue Dec 18 04:20:58 2018
From: hen@_4567 @ending from y@hoo@com (Rehena Sultana)
Date: Tue, 18 Dec 2018 03:20:58 +0000 (UTC)
Subject: [R] AUC/Volume under the surface
References: <338898667.5927861.1545103258414.ref@mail.yahoo.com>
Message-ID: <338898667.5927861.1545103258414@mail.yahoo.com>

Dear Members,I am having trouble in plotting a 3D ROC curve based on multinomial logistic regression. I am also interested in finding AUC/Volume under the surface based on final multivariate model. I do have an interaction term in my final model.?
I tried using "HUM" library. But I failed to figure out how to include interaction term in the model.?
Looking forward for any help. Appreciate your help in advance.?
Regards,rehena
	[[alternative HTML version deleted]]


From jdnewmil @ending from dcn@d@vi@@c@@u@  Tue Dec 18 08:42:58 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Mon, 17 Dec 2018 23:42:58 -0800
Subject: [R] AUC/Volume under the surface
In-Reply-To: <338898667.5927861.1545103258414@mail.yahoo.com>
References: <338898667.5927861.1545103258414.ref@mail.yahoo.com>
 <338898667.5927861.1545103258414@mail.yahoo.com>
Message-ID: <131C0A27-0464-402A-AAF6-32C56188683B@dcn.davis.ca.us>

You need to grasp two concepts:

1) Models in R conventionally have predict methods. To plot your model, predict the dependent variable based on the model object and a grid of your independent variable(s). Whether you have interactions or logistic regression shouldn't be relevant to getting a plot. The "expand.grid" function is helpful with preparing your inputs for the predict method.

2) Surfaces generally need a regular grid of input values independent of the input values you built your model from. This grid is often input to the plotting functions with the set of unique x-values and the set of unique y-values, with your z-values in a matrix. That is, for most 3d surface plotting functions you do not provide the gridded x and y values that you gave to the predict function.

There are many functions that can do this plotting... the rgl package is quite nice, but the scatterplot3d package can be simpler to publish with. Search for tutorials... there are many. If you get stuck, post a reproducible example data set and what you did with it here and someone will likely offer some more specific assistance.


On December 17, 2018 7:20:58 PM PST, Rehena Sultana via R-help <r-help at r-project.org> wrote:
>Dear Members,I am having trouble in plotting a 3D ROC curve based on
>multinomial logistic regression. I am also interested in finding
>AUC/Volume under the surface based on final multivariate model. I do
>have an interaction term in my final model.?
>I tried using "HUM" library. But I failed to figure out how to include
>interaction term in the model.?
>Looking forward for any help. Appreciate your help in advance.?
>Regards,rehena
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From ph@edru@v @ending from gm@il@com  Tue Dec 18 09:56:23 2018
From: ph@edru@v @ending from gm@il@com (Andrew)
Date: Tue, 18 Dec 2018 08:56:23 +0000
Subject: [R] Help with if else branching print out
Message-ID: <f44ac76f-8d59-d353-20cb-fc4a79405a49@gmail.com>

Hi all

Playing around with some branching using if, if else, and else, I wrote 
the following very basic script:


[code block]

## Take single value input from user:
# Note 'as.integer' to convert character vector to numeric for testing

nums <- (as.numeric(readline("Please enter a number between 1 and 15: ")))

## Truth test routine:
# Set conditional values and test input against conditions and then 
print outcome

if ((nums > 15) || (nums < 1))
{
 ? print(c(nums, "is not between 1 or 15"))
} else if ((nums > 5) && (nums < 10) || (nums == 12))
{
 ? print(c(nums, "falls within 6 - 9 inclusively, or is 12"))
}? else
{
 ? print( c(nums, "is *not* 6 - 9 inclusive, nor is it 12"))
}

[/ code block]


However, it prints out like this:

[output]

Please enter a number between 1 and 15: 17
[1] "17"???????????????????? "is not between 1 or 15"

Please enter a number between 1 and 15: 9
[1] "9"
[2] "falls within 6 - 9 inclusively, or is 12"

Please enter a number between 1 and 15: 13
[1] "13"???????????????????????????????????? "is *not* 6 - 9 inclusive, 
nor is it 12"

[/ output]


How do I:

(a) reduce the gap between the reported number (i.e., 17, 9, 13) in each 
of the lines? and

(b) ensure that in the case of the second run using 9 as the input, the 
print is not over two lines?

I will try the switches function soon, which may yield a different 
output format, but wanted to get my head around why this is printing out 
the way it is.

Many thanks for any help.

Andrew


	[[alternative HTML version deleted]]


From ericjberger @ending from gm@il@com  Tue Dec 18 10:09:58 2018
From: ericjberger @ending from gm@il@com (Eric Berger)
Date: Tue, 18 Dec 2018 11:09:58 +0200
Subject: [R] Help with if else branching print out
In-Reply-To: <f44ac76f-8d59-d353-20cb-fc4a79405a49@gmail.com>
References: <f44ac76f-8d59-d353-20cb-fc4a79405a49@gmail.com>
Message-ID: <CAGgJW7697+cC6Unyeyi+cWQ0MHyQHPdcCJMxATb6FemVyFDUpg@mail.gmail.com>

I often use sprintf() to control formatting of text strings. e.g.

sprintf("%s is not between 1 or 15\n",nums)

See ?sprintf for details

HTH,
Eric


On Tue, Dec 18, 2018 at 10:56 AM Andrew <phaedrusv at gmail.com> wrote:

> Hi all
>
> Playing around with some branching using if, if else, and else, I wrote
> the following very basic script:
>
>
> [code block]
>
> ## Take single value input from user:
> # Note 'as.integer' to convert character vector to numeric for testing
>
> nums <- (as.numeric(readline("Please enter a number between 1 and 15: ")))
>
> ## Truth test routine:
> # Set conditional values and test input against conditions and then
> print outcome
>
> if ((nums > 15) || (nums < 1))
> {
>    print(c(nums, "is not between 1 or 15"))
> } else if ((nums > 5) && (nums < 10) || (nums == 12))
> {
>    print(c(nums, "falls within 6 - 9 inclusively, or is 12"))
> }  else
> {
>    print( c(nums, "is *not* 6 - 9 inclusive, nor is it 12"))
> }
>
> [/ code block]
>
>
> However, it prints out like this:
>
> [output]
>
> Please enter a number between 1 and 15: 17
> [1] "17"                     "is not between 1 or 15"
>
> Please enter a number between 1 and 15: 9
> [1] "9"
> [2] "falls within 6 - 9 inclusively, or is 12"
>
> Please enter a number between 1 and 15: 13
> [1] "13"                                     "is *not* 6 - 9 inclusive,
> nor is it 12"
>
> [/ output]
>
>
> How do I:
>
> (a) reduce the gap between the reported number (i.e., 17, 9, 13) in each
> of the lines? and
>
> (b) ensure that in the case of the second run using 9 as the input, the
> print is not over two lines?
>
> I will try the switches function soon, which may yield a different
> output format, but wanted to get my head around why this is printing out
> the way it is.
>
> Many thanks for any help.
>
> Andrew
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From krylov@r00t @ending from gm@il@com  Tue Dec 18 10:25:24 2018
From: krylov@r00t @ending from gm@il@com (Ivan Krylov)
Date: Tue, 18 Dec 2018 12:25:24 +0300
Subject: [R] Help with if else branching print out
In-Reply-To: <f44ac76f-8d59-d353-20cb-fc4a79405a49@gmail.com>
References: <f44ac76f-8d59-d353-20cb-fc4a79405a49@gmail.com>
Message-ID: <20181218122524.61065a8a@trisector>

On Tue, 18 Dec 2018 08:56:23 +0000
Andrew <phaedrusv at gmail.com> wrote:

> How do I:
> 
> (a) reduce the gap between the reported number (i.e., 17, 9, 13) in
> each of the lines? and
> 
> (b) ensure that in the case of the second run using 9 as the input,
> the print is not over two lines?

Build a single string from your string parts. ?sprintf has already been
mentioned; another option is ?paste.

To prevent strings from being printed in quotes, use ?message.

-- 
Best regards,
Ivan


From drjimlemon @ending from gm@il@com  Tue Dec 18 11:01:33 2018
From: drjimlemon @ending from gm@il@com (Jim Lemon)
Date: Tue, 18 Dec 2018 21:01:33 +1100
Subject: [R] 
 [R studio] Plotting of line chart for each columns at 1 page
In-Reply-To: <CA+8X3fWnA=4PYGqxM8hq-Fm-OEJ0qhqezx1x=a+LWQ7C=CrF1w@mail.gmail.com>
References: <CAOFE=kOWXrnRnUVtL7A0vLKjL03571WQy5W_865hTFeBz28+tA@mail.gmail.com>
 <CA+8X3fW3Op8icpvA68O2sYPpBi=rpO2DWVXwL5UrPOHtDqDzpw@mail.gmail.com>
 <CAOFE=kNV6P4jRtrYS9K1E76pn+tW_aVpQ4Kq25ZHtQ6v-m=TkQ@mail.gmail.com>
 <CA+8X3fXM9KEb8=U1y-E_DFy9UuXk7NXDyRo6VppnEEAbAtGDDQ@mail.gmail.com>
 <CAOFE=kO20RsVsvVL2ZXpsH51aw9bzgCc3tDJ7DX8fM-tgkkbdQ@mail.gmail.com>
 <CA+8X3fUh8bug_m8iGBGcdPdb=PeGXY0xG48NTwBHC2XzXrtyhg@mail.gmail.com>
 <CAOFE=kNda7BE7cnkUg3nTZNAZbG0-dq=ir2ddXy_1pRMwySpcg@mail.gmail.com>
 <CA+8X3fVsMos2cafXXk7seO=fpM+JFgufmQpcUhGqJ9nNm4BFmQ@mail.gmail.com>
 <CAOFE=kOiTHOzcQ+dL2Tak2WnGBx1JAXpryj5CbLefdofa-7x5Q@mail.gmail.com>
 <CA+8X3fVYFJ9GUzY5ujH38zEQhWKKrkUpRzf1UjC-Cc=GHMe0VA@mail.gmail.com>
 <CAOFE=kOBPM-6bdaQKC3NXTXuAn_9OOA8oEvOGKhrGdUxBUAVkw@mail.gmail.com>
 <CA+8X3fUSY13o=khzBy9RA1_qsP1vkVqNAm=UP-xAd_cyLNrAVw@mail.gmail.com>
 <CAOFE=kPwv6LspZ=4RvL9x2JE7YSE9Ad+4Rrdhpmj0hyEHmVhxA@mail.gmail.com>
 <CAOFE=kN_tdNCzu50RhVM4-k7d6psEd=vZu7pD0htdczBKSn-HQ@mail.gmail.com>
 <CAOFE=kN6uZ9mKCkfDkz+xXLMxs3+WBu=A3YFAP-0UWt_b5V1FA@mail.gmail.com>
 <CAOFE=kMgKxo--vVCLhaPuY-4+5LD=HCZuyhBXBDFzUt0DG3ThA@mail.gmail.com>
 <CA+8X3fUTof6=q9JBNnMNpbEqF_7Jxi4WnTPF+2-c62ihMqmSaQ@mail.gmail.com>
 <CAOFE=kOUbBpUidnAxQjSzN4yhUBiaCC2GRgG8ovqm1BeuJ86ig@mail.gmail.com>
 <CA+8X3fWHutE7ibNvBWzdZWYCqQc1zpKr3EAidnN=oaGUgOmRKw@mail.gmail.com>
 <CAOFE=kOatoPvDrY6bFBqnLHojJ08HL6qMrngV4dLYpyW+Eewkg@mail.gmail.com>
 <CAOFE=kMPdr6RDDJCKV_Uw73uEUKV=RgOatrRaxnwsZgDT1rrOw@mail.gmail.com>
 <CA+8X3fWe5k2_QF4AwOKdyqqU0U62m-eSNriwPqny=Hh06sU2nw@mail.gmail.com>
 <CAOFE=kPQDaqAKc-gMiUvEO=Zbo1+Ca2Hqm-KYX2DjeM6FnZqKw@mail.gmail.com>
 <CA+8X3fWnA=4PYGqxM8hq-Fm-OEJ0qhqezx1x=a+LWQ7C=CrF1w@mail.gmail.com>
Message-ID: <CA+8X3fUURVr7-FxdTt6qKyhO2cV4Y-s=RiH8WvCVj3ZsyOg00g@mail.gmail.com>

Hi Subhamitra,
My apologies, I caught a mistake. To have the first tick in the middle of
the first year, you want half of the _observations_ in a year, not half of
the days. As I now have your data at my fingertips:

3567/15.58385
[1] 228.8908

Almost exactly what was calculated for the first series. Your increment
remains 229 and your offset is 114, so

year_mids<-seq(114,3567,229)

Jim

	[[alternative HTML version deleted]]


From ruipb@rr@d@@ @ending from @@po@pt  Tue Dec 18 11:55:54 2018
From: ruipb@rr@d@@ @ending from @@po@pt (Rui Barradas)
Date: Tue, 18 Dec 2018 10:55:54 +0000
Subject: [R] Help with if else branching print out
In-Reply-To: <20181218122524.61065a8a@trisector>
References: <f44ac76f-8d59-d353-20cb-fc4a79405a49@gmail.com>
 <20181218122524.61065a8a@trisector>
Message-ID: <2a9623fe-7e95-42ec-6f6b-ae0d41adae15@sapo.pt>

Hello,

Yet another option is ?cat.


msg <- c("is not between 1 or 15",
          "is *not* 6 - 9 inclusive, nor is it 12",
          "falls within 6 - 9 inclusively, or is 12",
          "is *not* 6 - 9 inclusive, nor is it 12",
          "is not between 1 or 15")

nums <- as.numeric(readline("Please enter a number between 1 and 15: "))

i <- if(nums == 12) 2 else findInterval(nums, c(-Inf, 1, 6, 9, 15), 
rightmost.closed = TRUE)
cat(nums, msg[i], "\n")


Hope this helps,

Rui Barradas

?s 09:25 de 18/12/2018, Ivan Krylov escreveu:
> On Tue, 18 Dec 2018 08:56:23 +0000
> Andrew <phaedrusv at gmail.com> wrote:
> 
>> How do I:
>>
>> (a) reduce the gap between the reported number (i.e., 17, 9, 13) in
>> each of the lines? and
>>
>> (b) ensure that in the case of the second run using 9 as the input,
>> the print is not over two lines?
> 
> Build a single string from your string parts. ?sprintf has already been
> mentioned; another option is ?paste.
> 
> To prevent strings from being printed in quotes, use ?message.
>


From @ubh@mitr@@p@tr@ @ending from gm@il@com  Tue Dec 18 13:26:23 2018
From: @ubh@mitr@@p@tr@ @ending from gm@il@com (Subhamitra Patra)
Date: Tue, 18 Dec 2018 17:56:23 +0530
Subject: [R] 
 [R studio] Plotting of line chart for each columns at 1 page
In-Reply-To: <CA+8X3fUURVr7-FxdTt6qKyhO2cV4Y-s=RiH8WvCVj3ZsyOg00g@mail.gmail.com>
References: <CAOFE=kOWXrnRnUVtL7A0vLKjL03571WQy5W_865hTFeBz28+tA@mail.gmail.com>
 <CA+8X3fW3Op8icpvA68O2sYPpBi=rpO2DWVXwL5UrPOHtDqDzpw@mail.gmail.com>
 <CAOFE=kNV6P4jRtrYS9K1E76pn+tW_aVpQ4Kq25ZHtQ6v-m=TkQ@mail.gmail.com>
 <CA+8X3fXM9KEb8=U1y-E_DFy9UuXk7NXDyRo6VppnEEAbAtGDDQ@mail.gmail.com>
 <CAOFE=kO20RsVsvVL2ZXpsH51aw9bzgCc3tDJ7DX8fM-tgkkbdQ@mail.gmail.com>
 <CA+8X3fUh8bug_m8iGBGcdPdb=PeGXY0xG48NTwBHC2XzXrtyhg@mail.gmail.com>
 <CAOFE=kNda7BE7cnkUg3nTZNAZbG0-dq=ir2ddXy_1pRMwySpcg@mail.gmail.com>
 <CA+8X3fVsMos2cafXXk7seO=fpM+JFgufmQpcUhGqJ9nNm4BFmQ@mail.gmail.com>
 <CAOFE=kOiTHOzcQ+dL2Tak2WnGBx1JAXpryj5CbLefdofa-7x5Q@mail.gmail.com>
 <CA+8X3fVYFJ9GUzY5ujH38zEQhWKKrkUpRzf1UjC-Cc=GHMe0VA@mail.gmail.com>
 <CAOFE=kOBPM-6bdaQKC3NXTXuAn_9OOA8oEvOGKhrGdUxBUAVkw@mail.gmail.com>
 <CA+8X3fUSY13o=khzBy9RA1_qsP1vkVqNAm=UP-xAd_cyLNrAVw@mail.gmail.com>
 <CAOFE=kPwv6LspZ=4RvL9x2JE7YSE9Ad+4Rrdhpmj0hyEHmVhxA@mail.gmail.com>
 <CAOFE=kN_tdNCzu50RhVM4-k7d6psEd=vZu7pD0htdczBKSn-HQ@mail.gmail.com>
 <CAOFE=kN6uZ9mKCkfDkz+xXLMxs3+WBu=A3YFAP-0UWt_b5V1FA@mail.gmail.com>
 <CAOFE=kMgKxo--vVCLhaPuY-4+5LD=HCZuyhBXBDFzUt0DG3ThA@mail.gmail.com>
 <CA+8X3fUTof6=q9JBNnMNpbEqF_7Jxi4WnTPF+2-c62ihMqmSaQ@mail.gmail.com>
 <CAOFE=kOUbBpUidnAxQjSzN4yhUBiaCC2GRgG8ovqm1BeuJ86ig@mail.gmail.com>
 <CA+8X3fWHutE7ibNvBWzdZWYCqQc1zpKr3EAidnN=oaGUgOmRKw@mail.gmail.com>
 <CAOFE=kOatoPvDrY6bFBqnLHojJ08HL6qMrngV4dLYpyW+Eewkg@mail.gmail.com>
 <CAOFE=kMPdr6RDDJCKV_Uw73uEUKV=RgOatrRaxnwsZgDT1rrOw@mail.gmail.com>
 <CA+8X3fWe5k2_QF4AwOKdyqqU0U62m-eSNriwPqny=Hh06sU2nw@mail.gmail.com>
 <CAOFE=kPQDaqAKc-gMiUvEO=Zbo1+Ca2Hqm-KYX2DjeM6FnZqKw@mail.gmail.com>
 <CA+8X3fWnA=4PYGqxM8hq-Fm-OEJ0qhqezx1x=a+LWQ7C=CrF1w@mail.gmail.com>
 <CA+8X3fUURVr7-FxdTt6qKyhO2cV4Y-s=RiH8WvCVj3ZsyOg00g@mail.gmail.com>
Message-ID: <CAOFE=kMbDYdGtNMv_sxSOd0myGk1DV80Z+KOK-_TyZkc-5P7LA@mail.gmail.com>

Hello Sir,

It is really great learning for me while discussing with you.

As per your suggestion, I also read the axis function in the graphics
package, and now completely understand your logic. I will apply the same
logic for my rest variable and would like to discuss after the successful
generation of all plots.

Thank you very much, Sir, for pointing me to the right path.



[image: Mailtrack]
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
Sender
notified by
Mailtrack
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
12/18/18,
5:54:53 PM

On Tue, Dec 18, 2018 at 3:31 PM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Subhamitra,
> My apologies, I caught a mistake. To have the first tick in the middle of
> the first year, you want half of the _observations_ in a year, not half of
> the days. As I now have your data at my fingertips:
>
> 3567/15.58385
> [1] 228.8908
>
> Almost exactly what was calculated for the first series. Your increment
> remains 229 and your offset is 114, so
>
> year_mids<-seq(114,3567,229)
>
> Jim
>
>

-- 
*Best Regards,*
*Subhamitra Patra*
*Phd. Research Scholar*
*Department of Humanities and Social Sciences*
*Indian Institute of Technology, Kharagpur*
*INDIA*

	[[alternative HTML version deleted]]


From bog@@o@chri@tofer @ending from gm@il@com  Tue Dec 18 13:37:18 2018
From: bog@@o@chri@tofer @ending from gm@il@com (Christofer Bogaso)
Date: Tue, 18 Dec 2018 18:07:18 +0530
Subject: [R] Webshot failed to take snapshot in Ubuntu machine
Message-ID: <CA+dpOJnZwhzUt=mPN+hKacuTjU71nyGggts69hvh55uyBPO5sQ@mail.gmail.com>

Hi,

I was using webshot package to take snapshot of a webpage as below:

library(webshot)
webshot('
https://www.bseindia.com/stock-share-price/asian-paints-ltd/asianpaint/500820/',
'bb.pdf')

However what I see is a Blank PDF file is saved.

However if I use the same code in my windows machine it is able to produce
correct snapshot.

Below is my system information
> Sys.info()
                                       sysname
                                       "Linux"
                                       release
                           "4.4.0-139-generic"
                                       version
"#165-Ubuntu SMP Wed Oct 24 10:58:50 UTC 2018"
                                      nodename
                  "ubuntu-s-2vcpu-4gb-blr1-01"
                                       machine
                                      "x86_64"
                                         login
                                        "root"
                                          user
                                        "root"
                                effective_user
                                        "root"

Any idea what went wrong would be highly helpful.

Thanks,

	[[alternative HTML version deleted]]


From bog@@o@chri@tofer @ending from gm@il@com  Tue Dec 18 13:39:08 2018
From: bog@@o@chri@tofer @ending from gm@il@com (Christofer Bogaso)
Date: Tue, 18 Dec 2018 18:09:08 +0530
Subject: [R] Webshot failed to take snapshot in Ubuntu machine
In-Reply-To: <CA+dpOJnZwhzUt=mPN+hKacuTjU71nyGggts69hvh55uyBPO5sQ@mail.gmail.com>
References: <CA+dpOJnZwhzUt=mPN+hKacuTjU71nyGggts69hvh55uyBPO5sQ@mail.gmail.com>
Message-ID: <CA+dpOJmud_BN9AZFYrnTdcO8TmN6p+pDiHe56G9ErVQ5dSBoiw@mail.gmail.com>

Also the Session information.

> sessionInfo()
R version 3.4.4 (2018-03-15)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 16.04.4 LTS

Matrix products: default
BLAS: /usr/lib/libblas/libblas.so.3.6.0
LAPACK: /usr/lib/lapack/liblapack.so.3.6.0

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] pdftools_2.0  webshot_0.5.1

loaded via a namespace (and not attached):
 [1] Rcpp_0.12.19     ps_1.1.0         crayon_1.3.4     assertthat_0.2.0
 [5] R6_2.3.0         jsonlite_1.5     magrittr_1.5     pillar_1.3.0
 [9] rlang_0.2.2      debugme_1.1.0    callr_3.0.0      tools_3.4.4
[13] compiler_3.4.4   processx_3.2.0   base64enc_0.1-3  tibble_1.4.2

On Tue, Dec 18, 2018 at 6:07 PM Christofer Bogaso <
bogaso.christofer at gmail.com> wrote:

> Hi,
>
> I was using webshot package to take snapshot of a webpage as below:
>
> library(webshot)
> webshot('
> https://www.bseindia.com/stock-share-price/asian-paints-ltd/asianpaint/500820/',
> 'bb.pdf')
>
> However what I see is a Blank PDF file is saved.
>
> However if I use the same code in my windows machine it is able to produce
> correct snapshot.
>
> Below is my system information
> > Sys.info()
>                                        sysname
>                                        "Linux"
>                                        release
>                            "4.4.0-139-generic"
>                                        version
> "#165-Ubuntu SMP Wed Oct 24 10:58:50 UTC 2018"
>                                       nodename
>                   "ubuntu-s-2vcpu-4gb-blr1-01"
>                                        machine
>                                       "x86_64"
>                                          login
>                                         "root"
>                                           user
>                                         "root"
>                                 effective_user
>                                         "root"
>
> Any idea what went wrong would be highly helpful.
>
> Thanks,
>

	[[alternative HTML version deleted]]


From m@rc_grt @ending from y@hoo@fr  Tue Dec 18 13:53:34 2018
From: m@rc_grt @ending from y@hoo@fr (Marc Girondot)
Date: Tue, 18 Dec 2018 13:53:34 +0100
Subject: [R] Webshot failed to take snapshot in Ubuntu machine
In-Reply-To: <CA+dpOJnZwhzUt=mPN+hKacuTjU71nyGggts69hvh55uyBPO5sQ@mail.gmail.com>
References: <CA+dpOJnZwhzUt=mPN+hKacuTjU71nyGggts69hvh55uyBPO5sQ@mail.gmail.com>
Message-ID: <85df464f-1892-5500-799b-f5d73a61015a@yahoo.fr>

Hi Christofer,
I just try on MacOSX and ubuntu and it works on both:

For ubuntu:
 > Sys.info()
 ????????????????????????????????????? sysname
 ????????????????????????????????????? "Linux"
 ????????????????????????????????????? release
 ????????????????????????? "4.15.0-42-generic"
 ????????????????????????????????????? version
"#45-Ubuntu SMP Thu Nov 15 19:32:57 UTC 2018"
 ???????????????????????????????????? nodename
 ?????????????????????????????? "lepidochelys"
 ????????????????????????????????????? machine
 ???????????????????????????????????? "x86_64"

Not sure what to do...

Marc

Le 18/12/2018 ? 13:37, Christofer Bogaso a ?crit?:
> Hi,
>
> I was using webshot package to take snapshot of a webpage as below:
>
> library(webshot)
> webshot('
> https://www.bseindia.com/stock-share-price/asian-paints-ltd/asianpaint/500820/',
> 'bb.pdf')
>
> However what I see is a Blank PDF file is saved.
>
> However if I use the same code in my windows machine it is able to produce
> correct snapshot.
>
> Below is my system information
>> Sys.info()
>                                         sysname
>                                         "Linux"
>                                         release
>                             "4.4.0-139-generic"
>                                         version
> "#165-Ubuntu SMP Wed Oct 24 10:58:50 UTC 2018"
>                                        nodename
>                    "ubuntu-s-2vcpu-4gb-blr1-01"
>                                         machine
>                                        "x86_64"
>                                           login
>                                          "root"
>                                            user
>                                          "root"
>                                  effective_user
>                                          "root"
>
> Any idea what went wrong would be highly helpful.
>
> Thanks,
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r@herry8 @ending from comc@@t@net  Tue Dec 18 15:17:00 2018
From: r@herry8 @ending from comc@@t@net (rsherry8)
Date: Tue, 18 Dec 2018 09:17:00 -0500
Subject: [R] Problem with Plotting in R
Message-ID: <5C19015C.8080100@comcast.net>


Please consider the following R statements:

     > x = seq(1:1632)
     > length( MyData$NWorth )
     [1] 1632
     > length( MyData$NWorthSm )
     [1] 1632
     > plot( x, MyData$NWorth, type="l" )
     > plot( x, MyData$NWorthSm, type="l" )
     > plot( x, MyData$NWorth, MyData$NWorthSm, type="l" )

All of the above statements work except for the last one. The last one 
produces the following message:

     Error in plot.window(...) : invalid 'xlim' value

So I then tired this:

     > xlim1 = c(0, 5000)
     >plot( x, MyData$NWorth, MyData$NWorthSm, type="l", xlim = xlim1 )

Which produced the following error message:
     Error in plot.window(...) : invalid 'ylim' value

So, I tired this:
     > ylim1 = c(0,9000)
     > plot( x, MyData$NWorth, MyData$NWorthSm, type="l", xlim = xlim1, 
ylim = ylim1 )

Which produced the following error message:
     Error in strsplit(log, NULL) : non-character argument

I would like to know what I am doing wrong.

Thank you,
Bob


From li@t@ @ending from dewey@myzen@co@uk  Tue Dec 18 15:34:11 2018
From: li@t@ @ending from dewey@myzen@co@uk (Michael Dewey)
Date: Tue, 18 Dec 2018 14:34:11 +0000
Subject: [R] Problem with Plotting in R
In-Reply-To: <5C19015C.8080100@comcast.net>
References: <5C19015C.8080100@comcast.net>
Message-ID: <6c7a27b8-0650-5588-f64a-39f392f38480@dewey.myzen.co.uk>

Dear Bob

We do not have your data so it is hard to be sure but plot() takes two 
parameters for the data x and y so when you give it three you are 
confusing it into thinking one of them is something else.

What exactly were you trying to do with the failed command?

On 18/12/2018 14:17, rsherry8 wrote:
> 
> Please consider the following R statements:
> 
>  ??? > x = seq(1:1632)
>  ??? > length( MyData$NWorth )
>  ??? [1] 1632
>  ??? > length( MyData$NWorthSm )
>  ??? [1] 1632
>  ??? > plot( x, MyData$NWorth, type="l" )
>  ??? > plot( x, MyData$NWorthSm, type="l" )
>  ??? > plot( x, MyData$NWorth, MyData$NWorthSm, type="l" )
> 
> All of the above statements work except for the last one. The last one 
> produces the following message:
> 
>  ??? Error in plot.window(...) : invalid 'xlim' value
> 
> So I then tired this:
> 
>  ??? > xlim1 = c(0, 5000)
>  ??? >plot( x, MyData$NWorth, MyData$NWorthSm, type="l", xlim = xlim1 )
> 
> Which produced the following error message:
>  ??? Error in plot.window(...) : invalid 'ylim' value
> 
> So, I tired this:
>  ??? > ylim1 = c(0,9000)
>  ??? > plot( x, MyData$NWorth, MyData$NWorthSm, type="l", xlim = xlim1, 
> ylim = ylim1 )
> 
> Which produced the following error message:
>  ??? Error in strsplit(log, NULL) : non-character argument
> 
> I would like to know what I am doing wrong.
> 
> Thank you,
> Bob
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From ph@edru@v @ending from gm@il@com  Tue Dec 18 16:01:02 2018
From: ph@edru@v @ending from gm@il@com (Andrew)
Date: Tue, 18 Dec 2018 15:01:02 +0000
Subject: [R] [Solved] Re:  Help with if else branching print out
In-Reply-To: <20181218122524.61065a8a@trisector>
References: <f44ac76f-8d59-d353-20cb-fc4a79405a49@gmail.com>
 <20181218122524.61065a8a@trisector>
Message-ID: <5f983d46-da98-09c3-6f6f-00614058d359@gmail.com>

Dear Eric, Ivan & Rui

Thanks all for your suggestions. FWIW, I've opted to use paste0 and 
after a bit of playing around found it formatted the output nicely.

Eric, I wasn't aware of sprintf - seems a handy function to format text 
with, so will try to remember that.
Ivan, I'll dig into 'message' a little more, so that's a good one to know.
Rui, I liked the idea of the output vector and indexing - that will be 
useful for longer/ more complicated branching

Thank you all again.

Best wishes
Andrew


On 18/12/2018 09:25, Ivan Krylov wrote:
> On Tue, 18 Dec 2018 08:56:23 +0000
> Andrew <phaedrusv at gmail.com> wrote:
>
>> How do I:
>>
>> (a) reduce the gap between the reported number (i.e., 17, 9, 13) in
>> each of the lines? and
>>
>> (b) ensure that in the case of the second run using 9 as the input,
>> the print is not over two lines?
> Build a single string from your string parts. ?sprintf has already been
> mentioned; another option is ?paste.
>
> To prevent strings from being printed in quotes, use ?message.
>


From ruipb@rr@d@@ @ending from @@po@pt  Tue Dec 18 16:18:26 2018
From: ruipb@rr@d@@ @ending from @@po@pt (Rui Barradas)
Date: Tue, 18 Dec 2018 15:18:26 +0000
Subject: [R] Problem with Plotting in R
In-Reply-To: <5C19015C.8080100@comcast.net>
References: <5C19015C.8080100@comcast.net>
Message-ID: <554e5119-c79e-04a0-67ee-0a20e2ecc18e@sapo.pt>

Hello,

You are calling plot.default with 4 arguments.
The first 2 are x and y.
The 3rd is type.
So MyData$NWorthSm becomes the 4th, xlim.
When you pass xlim a value, MyData$NWorthSm becomes the next one, ylim.
Etc, etc, etc.

It will throw the errors in the order of the arguments you can see in 
?plot.default:

## Default S3 method:
plot(x, y = NULL, type = "p",  xlim = NULL, ylim = NULL,
      log = "", main = NULL, etc, etc, etc)


So now if you pass a log = <something>, it's the time for argument main.

Revise the reason why you are passing MyData$NWorthSm.


Hope this helps,

Rui Barradas


?s 14:17 de 18/12/2018, rsherry8 escreveu:
> 
> Please consider the following R statements:
> 
>  ??? > x = seq(1:1632)
>  ??? > length( MyData$NWorth )
>  ??? [1] 1632
>  ??? > length( MyData$NWorthSm )
>  ??? [1] 1632
>  ??? > plot( x, MyData$NWorth, type="l" )
>  ??? > plot( x, MyData$NWorthSm, type="l" )
>  ??? > plot( x, MyData$NWorth, MyData$NWorthSm, type="l" )
> 
> All of the above statements work except for the last one. The last one 
> produces the following message:
> 
>  ??? Error in plot.window(...) : invalid 'xlim' value
> 
> So I then tired this:
> 
>  ??? > xlim1 = c(0, 5000)
>  ??? >plot( x, MyData$NWorth, MyData$NWorthSm, type="l", xlim = xlim1 )
> 
> Which produced the following error message:
>  ??? Error in plot.window(...) : invalid 'ylim' value
> 
> So, I tired this:
>  ??? > ylim1 = c(0,9000)
>  ??? > plot( x, MyData$NWorth, MyData$NWorthSm, type="l", xlim = xlim1, 
> ylim = ylim1 )
> 
> Which produced the following error message:
>  ??? Error in strsplit(log, NULL) : non-character argument
> 
> I would like to know what I am doing wrong.
> 
> Thank you,
> Bob
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bog@@o@chri@tofer @ending from gm@il@com  Tue Dec 18 17:45:00 2018
From: bog@@o@chri@tofer @ending from gm@il@com (Christofer Bogaso)
Date: Tue, 18 Dec 2018 22:15:00 +0530
Subject: [R] Webshot failed to take snapshot in Ubuntu machine
In-Reply-To: <85df464f-1892-5500-799b-f5d73a61015a@yahoo.fr>
References: <CA+dpOJnZwhzUt=mPN+hKacuTjU71nyGggts69hvh55uyBPO5sQ@mail.gmail.com>
 <85df464f-1892-5500-799b-f5d73a61015a@yahoo.fr>
Message-ID: <CA+dpOJ=ZcSq-_GU3sVjVa=YNi-8z6Ng+LaembihaT5R_ndBBvg@mail.gmail.com>

Is there any alternative to webshot?

On Tue, Dec 18, 2018 at 6:23 PM Marc Girondot <marc_grt at yahoo.fr> wrote:

> Hi Christofer,
> I just try on MacOSX and ubuntu and it works on both:
>
> For ubuntu:
>  > Sys.info()
>                                        sysname
>                                        "Linux"
>                                        release
>                            "4.15.0-42-generic"
>                                        version
> "#45-Ubuntu SMP Thu Nov 15 19:32:57 UTC 2018"
>                                       nodename
>                                 "lepidochelys"
>                                        machine
>                                       "x86_64"
>
> Not sure what to do...
>
> Marc
>
> Le 18/12/2018 ? 13:37, Christofer Bogaso a ?crit :
> > Hi,
> >
> > I was using webshot package to take snapshot of a webpage as below:
> >
> > library(webshot)
> > webshot('
> >
> https://www.bseindia.com/stock-share-price/asian-paints-ltd/asianpaint/500820/
> ',
> > 'bb.pdf')
> >
> > However what I see is a Blank PDF file is saved.
> >
> > However if I use the same code in my windows machine it is able to
> produce
> > correct snapshot.
> >
> > Below is my system information
> >> Sys.info()
> >                                         sysname
> >                                         "Linux"
> >                                         release
> >                             "4.4.0-139-generic"
> >                                         version
> > "#165-Ubuntu SMP Wed Oct 24 10:58:50 UTC 2018"
> >                                        nodename
> >                    "ubuntu-s-2vcpu-4gb-blr1-01"
> >                                         machine
> >                                        "x86_64"
> >                                           login
> >                                          "root"
> >                                            user
> >                                          "root"
> >                                  effective_user
> >                                          "root"
> >
> > Any idea what went wrong would be highly helpful.
> >
> > Thanks,
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>

	[[alternative HTML version deleted]]


From t@@h@@eileen @ending from gm@il@com  Tue Dec 18 18:17:24 2018
From: t@@h@@eileen @ending from gm@il@com (Tasha O'Hara)
Date: Tue, 18 Dec 2018 12:17:24 -0500
Subject: [R] Plotting rgb proportions in R
Message-ID: <CAKpKZ1zwnNcOeF8YwqNc8PxyRmhLwhS6p7CW5rORhey69KGNpg@mail.gmail.com>

Hello,

I am trying to plot specific rgb color proportions of a marine specimen in
a stacked plot using R and I was looking for some help. I have several
rgb proportions per specimen (an example of one is below).  I've run into
different examples of people using vegan or grDevices. Can anyone help with
this?

Red    Green  Blue   %
249 158 37 56.311
249 158 68 4.319
249 158 98 0.058
249 128 7 13.965
249 128 37 12.87
188 128 37 0.029
249 128 68 0.161
188 128 68 0.015
188 98 7 0.029
219 128 7 2.773
219 128 37 2.583
188 98 68 0.058
219 128 68 0.525
249 188 37 0.876
249 188 68 1.08
219 98 7 0.482
249 188 98 0.015
249 158 7 3.852

	[[alternative HTML version deleted]]


From jdnewmil @ending from dcn@d@vi@@c@@u@  Tue Dec 18 22:27:33 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Tue, 18 Dec 2018 13:27:33 -0800
Subject: [R] Plotting rgb proportions in R
In-Reply-To: <CAKpKZ1zwnNcOeF8YwqNc8PxyRmhLwhS6p7CW5rORhey69KGNpg@mail.gmail.com>
References: <CAKpKZ1zwnNcOeF8YwqNc8PxyRmhLwhS6p7CW5rORhey69KGNpg@mail.gmail.com>
Message-ID: <B9A4A91C-ED8E-4FF1-87C1-391E91CD7312@dcn.davis.ca.us>

I can't figure out what message you are hoping to convey in your plot from your posting... what are you comparing to what? Comments like "I've run into different examples of people using vegan (an analysis package with some diagnostic display functionality) or grDevices (a package supporting different device and graphic file formats)" fail to inform us as to where we can catch up with your reading or whether you liked what they did or not. Is there some missing analysis step or do you just want to present the raw data? Even a hand drawn sketch in PNG format would be some improvement in clarity.

Note that this is the wrong place to ask for help on theory... if you want to know what type of analysis you should attempt then you came here too soon... this is where we try to help you apply R to the problem once you know what your analysis strategy will be.

On December 18, 2018 9:17:24 AM PST, Tasha O'Hara <tasha.eileen at gmail.com> wrote:
>Hello,
>
>I am trying to plot specific rgb color proportions of a marine specimen
>in
>a stacked plot using R and I was looking for some help. I have several
>rgb proportions per specimen (an example of one is below).  I've run
>into
>different examples of people using vegan or grDevices. Can anyone help
>with
>this?
>
>Red    Green  Blue   %
>249 158 37 56.311
>249 158 68 4.319
>249 158 98 0.058
>249 128 7 13.965
>249 128 37 12.87
>188 128 37 0.029
>249 128 68 0.161
>188 128 68 0.015
>188 98 7 0.029
>219 128 7 2.773
>219 128 37 2.583
>188 98 68 0.058
>219 128 68 0.525
>249 188 37 0.876
>249 188 68 1.08
>219 98 7 0.482
>249 188 98 0.015
>249 158 7 3.852
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From drjimlemon @ending from gm@il@com  Wed Dec 19 00:04:15 2018
From: drjimlemon @ending from gm@il@com (Jim Lemon)
Date: Wed, 19 Dec 2018 10:04:15 +1100
Subject: [R] Plotting rgb proportions in R
In-Reply-To: <CAKpKZ1zwnNcOeF8YwqNc8PxyRmhLwhS6p7CW5rORhey69KGNpg@mail.gmail.com>
References: <CAKpKZ1zwnNcOeF8YwqNc8PxyRmhLwhS6p7CW5rORhey69KGNpg@mail.gmail.com>
Message-ID: <CA+8X3fUaF_BUw9QUg3QZ57GiNAX8n+CZuRhW9kJQxVf4FEFZuQ@mail.gmail.com>

Hi Tasha,
I may be right off the track, but you could plot RGB proportions on a
3D plot. The easiest way I can think if would be to convert your 0-255
values to proportions:

rgb_prop<-read.table(text="Red Green Blue pct
249 158 37 56.311
249 158 68 4.319
249 158 98 0.058
249 128 7 13.965
249 128 37 12.87
188 128 37 0.029
249 128 68 0.161
188 128 68 0.015
188 98 7 0.029
219 128 7 2.773
219 128 37 2.583
188 98 68 0.058
219 128 68 0.525
249 188 37 0.876
249 188 68 1.08
219 98 7 0.482
249 188 98 0.015
249 158 7 3.852",header=TRUE)
rgb_prop$Red<-rgb_prop$Red/255
rgb_prop$Green<-rgb_prop$Green/255
rgb_prop$Blue<-rgb_prop$Blue/255
library(scatterplot3d)
scatterplot3d(rgb_prop[,1:3],cex.symbols=sqrt(rgb_prop[,4]),
 color=rgb(rgb_prop[,1],rgb_prop[,2],rgb_prop[,3]),pch=19)

then plot the RGB values on a 3D scatterplot. I have included
arguments to make the symbols the actual RGB colors that you specify
and their size proportional to the square root of the percentages.

Jim

On Wed, Dec 19, 2018 at 5:17 AM Tasha O'Hara <tasha.eileen at gmail.com> wrote:
>
> Hello,
>
> I am trying to plot specific rgb color proportions of a marine specimen in
> a stacked plot using R and I was looking for some help. I have several
> rgb proportions per specimen (an example of one is below).  I've run into
> different examples of people using vegan or grDevices. Can anyone help with
> this?
>
> Red    Green  Blue   %
> 249 158 37 56.311
> 249 158 68 4.319
> 249 158 98 0.058
> 249 128 7 13.965
> 249 128 37 12.87
> 188 128 37 0.029
> 249 128 68 0.161
> 188 128 68 0.015
> 188 98 7 0.029
> 219 128 7 2.773
> 219 128 37 2.583
> 188 98 68 0.058
> 219 128 68 0.525
> 249 188 37 0.876
> 249 188 68 1.08
> 219 98 7 0.482
> 249 188 98 0.015
> 249 158 7 3.852
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r@herry8 @ending from comc@@t@net  Wed Dec 19 00:31:43 2018
From: r@herry8 @ending from comc@@t@net (rsherry8)
Date: Tue, 18 Dec 2018 18:31:43 -0500
Subject: [R] Problem with LM
Message-ID: <5C19835F.4070106@comcast.net>

The values read into z2 came from a CSV file. Please consider this R 
session:

 > length(x2)
[1] 1632
 > length(x)
[1] 1632
 > length(z2)
[1] 1632
 > head(z2)
[1] 28914.0 28960.5 28994.5 29083.0 29083.0 29083.0
 > tail(z2)
[1] 32729.65 32751.85 32386.05 32379.75 32379.15 31977.15
 > lm ( y ~ x2 + x, z2 )
Error in eval(predvars, data, env) :
   numeric 'envir' arg not of length one
 > lm ( y ~ x2 + x, as.data.frme(z2) )
Error in as.data.frme(z2) : could not find function "as.data.frme"
 > lm ( y ~ x2 + x, as.data.frame(z2) )
Error in eval(predvars, data, env) :
   numeric 'envir' arg not of length one
lm(formula = y ~ x2 + x, data = as.data.frame(z2))

Coefficients:
(Intercept)           x2            x
  -1.475e-09    1.000e+00    6.044e-13

 > min(z2)
[1] 24420
 > max(z2)
[1] 35524.85
 > class(z2)
[1] "numeric"
 >

where x is set to x = seq(1:1632)
and x2 is set to x^2

I am looking for an interpolating polynomial of the form:
     Ax^2 + Bx + C
I do not think the results I got make sense. I believe that I have a 
data type error.  I do not understand why
I need to convert z2 to a data frame if it is already numeric.

Thanks,
Bob


From rmh @ending from temple@edu  Wed Dec 19 01:10:10 2018
From: rmh @ending from temple@edu (Richard M. Heiberger)
Date: Tue, 18 Dec 2018 19:10:10 -0500
Subject: [R] Problem with LM
In-Reply-To: <5C19835F.4070106@comcast.net>
References: <5C19835F.4070106@comcast.net>
Message-ID: <CAGx1TMC+ijZh9tG8hx_3Gw5fYPFe2y_0w3ORxGBnDbk19vm8Zg@mail.gmail.com>

## This example, with your variable names, works correctly.

z2 <- data.frame(y=1:5, x=c(1,5,2,3,5), x2=c(1,5,2,3,5)^2)
z2
class(z2)
length(z2)
dim(z2)

lm(y ~ x + x2, data=z2)

## note that that variable names y, x, x2 are column names of the
## data.frame z2

## please review the definitions and examples of data.frame in ?data.frame
## also the argument requirements for lm in ?lm

On Tue, Dec 18, 2018 at 6:32 PM rsherry8 <rsherry8 at comcast.net> wrote:
>
> The values read into z2 came from a CSV file. Please consider this R
> session:
>
>  > length(x2)
> [1] 1632
>  > length(x)
> [1] 1632
>  > length(z2)
> [1] 1632
>  > head(z2)
> [1] 28914.0 28960.5 28994.5 29083.0 29083.0 29083.0
>  > tail(z2)
> [1] 32729.65 32751.85 32386.05 32379.75 32379.15 31977.15
>  > lm ( y ~ x2 + x, z2 )
> Error in eval(predvars, data, env) :
>    numeric 'envir' arg not of length one
>  > lm ( y ~ x2 + x, as.data.frme(z2) )
> Error in as.data.frme(z2) : could not find function "as.data.frme"
>  > lm ( y ~ x2 + x, as.data.frame(z2) )
> Error in eval(predvars, data, env) :
>    numeric 'envir' arg not of length one
> lm(formula = y ~ x2 + x, data = as.data.frame(z2))
>
> Coefficients:
> (Intercept)           x2            x
>   -1.475e-09    1.000e+00    6.044e-13
>
>  > min(z2)
> [1] 24420
>  > max(z2)
> [1] 35524.85
>  > class(z2)
> [1] "numeric"
>  >
>
> where x is set to x = seq(1:1632)
> and x2 is set to x^2
>
> I am looking for an interpolating polynomial of the form:
>      Ax^2 + Bx + C
> I do not think the results I got make sense. I believe that I have a
> data type error.  I do not understand why
> I need to convert z2 to a data frame if it is already numeric.
>
> Thanks,
> Bob
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r@herry8 @ending from comc@@t@net  Wed Dec 19 03:14:32 2018
From: r@herry8 @ending from comc@@t@net (rsherry8)
Date: Tue, 18 Dec 2018 21:14:32 -0500
Subject: [R] Problem with LM
In-Reply-To: <CAGx1TMC+ijZh9tG8hx_3Gw5fYPFe2y_0w3ORxGBnDbk19vm8Zg@mail.gmail.com>
References: <5C19835F.4070106@comcast.net>
 <CAGx1TMC+ijZh9tG8hx_3Gw5fYPFe2y_0w3ORxGBnDbk19vm8Zg@mail.gmail.com>
Message-ID: <5C19A988.2000402@comcast.net>

Richard,

It is now working.

Thank you very much.

Bob
On 12/18/2018 7:10 PM, Richard M. Heiberger wrote:
> ## This example, with your variable names, works correctly.
>
> z2 <- data.frame(y=1:5, x=c(1,5,2,3,5), x2=c(1,5,2,3,5)^2)
> z2
> class(z2)
> length(z2)
> dim(z2)
>
> lm(y ~ x + x2, data=z2)
>
> ## note that that variable names y, x, x2 are column names of the
> ## data.frame z2
>
> ## please review the definitions and examples of data.frame in ?data.frame
> ## also the argument requirements for lm in ?lm
>
> On Tue, Dec 18, 2018 at 6:32 PM rsherry8 <rsherry8 at comcast.net> wrote:
>> The values read into z2 came from a CSV file. Please consider this R
>> session:
>>
>>   > length(x2)
>> [1] 1632
>>   > length(x)
>> [1] 1632
>>   > length(z2)
>> [1] 1632
>>   > head(z2)
>> [1] 28914.0 28960.5 28994.5 29083.0 29083.0 29083.0
>>   > tail(z2)
>> [1] 32729.65 32751.85 32386.05 32379.75 32379.15 31977.15
>>   > lm ( y ~ x2 + x, z2 )
>> Error in eval(predvars, data, env) :
>>     numeric 'envir' arg not of length one
>>   > lm ( y ~ x2 + x, as.data.frme(z2) )
>> Error in as.data.frme(z2) : could not find function "as.data.frme"
>>   > lm ( y ~ x2 + x, as.data.frame(z2) )
>> Error in eval(predvars, data, env) :
>>     numeric 'envir' arg not of length one
>> lm(formula = y ~ x2 + x, data = as.data.frame(z2))
>>
>> Coefficients:
>> (Intercept)           x2            x
>>    -1.475e-09    1.000e+00    6.044e-13
>>
>>   > min(z2)
>> [1] 24420
>>   > max(z2)
>> [1] 35524.85
>>   > class(z2)
>> [1] "numeric"
>>   >
>>
>> where x is set to x = seq(1:1632)
>> and x2 is set to x^2
>>
>> I am looking for an interpolating polynomial of the form:
>>       Ax^2 + Bx + C
>> I do not think the results I got make sense. I believe that I have a
>> data type error.  I do not understand why
>> I need to convert z2 to a data frame if it is already numeric.
>>
>> Thanks,
>> Bob
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @ending from gm@il@com  Wed Dec 19 05:19:33 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Tue, 18 Dec 2018 20:19:33 -0800
Subject: [R] Problem with LM
In-Reply-To: <5C19A988.2000402@comcast.net>
References: <5C19835F.4070106@comcast.net>
 <CAGx1TMC+ijZh9tG8hx_3Gw5fYPFe2y_0w3ORxGBnDbk19vm8Zg@mail.gmail.com>
 <5C19A988.2000402@comcast.net>
Message-ID: <CAGxFJbTcOz-CjiBEFmupdmDYpj8Am+DL5kp+E8h5yU3xO6_FuQ@mail.gmail.com>

... Perhaps worth adding is the use of poly() rather than separately
created  terms for (non/orthogonal)  polynomials:

lm(y ~ poly(x, degree =2)  #orthogonal polyomial of degree 2

see ?poly for details.

-- Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Dec 18, 2018 at 6:14 PM rsherry8 <rsherry8 at comcast.net> wrote:

> Richard,
>
> It is now working.
>
> Thank you very much.
>
> Bob
> On 12/18/2018 7:10 PM, Richard M. Heiberger wrote:
> > ## This example, with your variable names, works correctly.
> >
> > z2 <- data.frame(y=1:5, x=c(1,5,2,3,5), x2=c(1,5,2,3,5)^2)
> > z2
> > class(z2)
> > length(z2)
> > dim(z2)
> >
> > lm(y ~ x + x2, data=z2)
> >
> > ## note that that variable names y, x, x2 are column names of the
> > ## data.frame z2
> >
> > ## please review the definitions and examples of data.frame in
> ?data.frame
> > ## also the argument requirements for lm in ?lm
> >
> > On Tue, Dec 18, 2018 at 6:32 PM rsherry8 <rsherry8 at comcast.net> wrote:
> >> The values read into z2 came from a CSV file. Please consider this R
> >> session:
> >>
> >>   > length(x2)
> >> [1] 1632
> >>   > length(x)
> >> [1] 1632
> >>   > length(z2)
> >> [1] 1632
> >>   > head(z2)
> >> [1] 28914.0 28960.5 28994.5 29083.0 29083.0 29083.0
> >>   > tail(z2)
> >> [1] 32729.65 32751.85 32386.05 32379.75 32379.15 31977.15
> >>   > lm ( y ~ x2 + x, z2 )
> >> Error in eval(predvars, data, env) :
> >>     numeric 'envir' arg not of length one
> >>   > lm ( y ~ x2 + x, as.data.frme(z2) )
> >> Error in as.data.frme(z2) : could not find function "as.data.frme"
> >>   > lm ( y ~ x2 + x, as.data.frame(z2) )
> >> Error in eval(predvars, data, env) :
> >>     numeric 'envir' arg not of length one
> >> lm(formula = y ~ x2 + x, data = as.data.frame(z2))
> >>
> >> Coefficients:
> >> (Intercept)           x2            x
> >>    -1.475e-09    1.000e+00    6.044e-13
> >>
> >>   > min(z2)
> >> [1] 24420
> >>   > max(z2)
> >> [1] 35524.85
> >>   > class(z2)
> >> [1] "numeric"
> >>   >
> >>
> >> where x is set to x = seq(1:1632)
> >> and x2 is set to x^2
> >>
> >> I am looking for an interpolating polynomial of the form:
> >>       Ax^2 + Bx + C
> >> I do not think the results I got make sense. I believe that I have a
> >> data type error.  I do not understand why
> >> I need to convert z2 to a data frame if it is already numeric.
> >>
> >> Thanks,
> >> Bob
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @ending from gm@il@com  Wed Dec 19 05:26:06 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Tue, 18 Dec 2018 20:26:06 -0800
Subject: [R] Plotting rgb proportions in R
In-Reply-To: <CA+8X3fUaF_BUw9QUg3QZ57GiNAX8n+CZuRhW9kJQxVf4FEFZuQ@mail.gmail.com>
References: <CAKpKZ1zwnNcOeF8YwqNc8PxyRmhLwhS6p7CW5rORhey69KGNpg@mail.gmail.com>
 <CA+8X3fUaF_BUw9QUg3QZ57GiNAX8n+CZuRhW9kJQxVf4FEFZuQ@mail.gmail.com>
Message-ID: <CAGxFJbQODiuWSL_QTeYT=sHKJnR+ZpvRca233rT4-mEXisK9_g@mail.gmail.com>

3-d Proportions must sum to 1and are thus actually 2-d and should preferaby
be plotted as a ternary plot. Several r packages will do this for you, e.g.
package Ternary. Search "ternary plots" on rseek.org for others.

-- Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Dec 18, 2018 at 3:10 PM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Tasha,
> I may be right off the track, but you could plot RGB proportions on a
> 3D plot. The easiest way I can think if would be to convert your 0-255
> values to proportions:
>
> rgb_prop<-read.table(text="Red Green Blue pct
> 249 158 37 56.311
> 249 158 68 4.319
> 249 158 98 0.058
> 249 128 7 13.965
> 249 128 37 12.87
> 188 128 37 0.029
> 249 128 68 0.161
> 188 128 68 0.015
> 188 98 7 0.029
> 219 128 7 2.773
> 219 128 37 2.583
> 188 98 68 0.058
> 219 128 68 0.525
> 249 188 37 0.876
> 249 188 68 1.08
> 219 98 7 0.482
> 249 188 98 0.015
> 249 158 7 3.852",header=TRUE)
> rgb_prop$Red<-rgb_prop$Red/255
> rgb_prop$Green<-rgb_prop$Green/255
> rgb_prop$Blue<-rgb_prop$Blue/255
> library(scatterplot3d)
> scatterplot3d(rgb_prop[,1:3],cex.symbols=sqrt(rgb_prop[,4]),
>  color=rgb(rgb_prop[,1],rgb_prop[,2],rgb_prop[,3]),pch=19)
>
> then plot the RGB values on a 3D scatterplot. I have included
> arguments to make the symbols the actual RGB colors that you specify
> and their size proportional to the square root of the percentages.
>
> Jim
>
> On Wed, Dec 19, 2018 at 5:17 AM Tasha O'Hara <tasha.eileen at gmail.com>
> wrote:
> >
> > Hello,
> >
> > I am trying to plot specific rgb color proportions of a marine specimen
> in
> > a stacked plot using R and I was looking for some help. I have several
> > rgb proportions per specimen (an example of one is below).  I've run into
> > different examples of people using vegan or grDevices. Can anyone help
> with
> > this?
> >
> > Red    Green  Blue   %
> > 249 158 37 56.311
> > 249 158 68 4.319
> > 249 158 98 0.058
> > 249 128 7 13.965
> > 249 128 37 12.87
> > 188 128 37 0.029
> > 249 128 68 0.161
> > 188 128 68 0.015
> > 188 98 7 0.029
> > 219 128 7 2.773
> > 219 128 37 2.583
> > 188 98 68 0.058
> > 219 128 68 0.525
> > 249 188 37 0.876
> > 249 188 68 1.08
> > 219 98 7 0.482
> > 249 188 98 0.015
> > 249 158 7 3.852
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From mi@ojpm @ending from gm@il@com  Wed Dec 19 07:05:07 2018
From: mi@ojpm @ending from gm@il@com (John)
Date: Wed, 19 Dec 2018 14:05:07 +0800
Subject: [R] geom_ribbon function in ggplot2 package
Message-ID: <CABcx46DkoZd2ky3dmLn-3398GMJYkP9uka186cY0QtLHkaV+PQ@mail.gmail.com>

Hi,

   When using the geom_ribbon function in gglot2 package, I got the text
"fill" above the legend "A" and "B". How can I get rid of the text "fill"
above the legend?

   Thanks!

The code is as follows:


df<-data.frame(x=c(1,2), y=c(1,2), z=c(3,5))
> ggplot(df,
aes(1:2))+geom_ribbon(aes(ymin=0,ymax=df[,"y"],fill="A"),alpha=0.5)+geom_ribbon(aes(ymin=0,ymax=df[,"z"],fill="B"),alpha=0.5)

	[[alternative HTML version deleted]]


From drjimlemon @ending from gm@il@com  Wed Dec 19 07:17:14 2018
From: drjimlemon @ending from gm@il@com (Jim Lemon)
Date: Wed, 19 Dec 2018 17:17:14 +1100
Subject: [R] Plotting rgb proportions in R
In-Reply-To: <CAGxFJbQODiuWSL_QTeYT=sHKJnR+ZpvRca233rT4-mEXisK9_g@mail.gmail.com>
References: <CAKpKZ1zwnNcOeF8YwqNc8PxyRmhLwhS6p7CW5rORhey69KGNpg@mail.gmail.com>
 <CA+8X3fUaF_BUw9QUg3QZ57GiNAX8n+CZuRhW9kJQxVf4FEFZuQ@mail.gmail.com>
 <CAGxFJbQODiuWSL_QTeYT=sHKJnR+ZpvRca233rT4-mEXisK9_g@mail.gmail.com>
Message-ID: <CA+8X3fWWVXkrfQ4RBwuWVkhCvFsiDa8z8=hD91wZnGso=UK9Pg@mail.gmail.com>

I was probably a bit obscure and should have written "convert your
0-255 values to 0-1". As Bert notes, you can't get a sensible ternary
plot if your values do not sum to 1. scatterplot3d does a pretty good
job, if you run the example.


Jim

On Wed, Dec 19, 2018 at 3:26 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
> 3-d Proportions must sum to 1and are thus actually 2-d and should preferaby be plotted as a ternary plot. Several r packages will do this for you, e.g. package Ternary. Search "ternary plots" on rseek.org for others.
>
> -- Bert
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Tue, Dec 18, 2018 at 3:10 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>>
>> Hi Tasha,
>> I may be right off the track, but you could plot RGB proportions on a
>> 3D plot. The easiest way I can think if would be to convert your 0-255
>> values to proportions:
>>
>> rgb_prop<-read.table(text="Red Green Blue pct
>> 249 158 37 56.311
>> 249 158 68 4.319
>> 249 158 98 0.058
>> 249 128 7 13.965
>> 249 128 37 12.87
>> 188 128 37 0.029
>> 249 128 68 0.161
>> 188 128 68 0.015
>> 188 98 7 0.029
>> 219 128 7 2.773
>> 219 128 37 2.583
>> 188 98 68 0.058
>> 219 128 68 0.525
>> 249 188 37 0.876
>> 249 188 68 1.08
>> 219 98 7 0.482
>> 249 188 98 0.015
>> 249 158 7 3.852",header=TRUE)
>> rgb_prop$Red<-rgb_prop$Red/255
>> rgb_prop$Green<-rgb_prop$Green/255
>> rgb_prop$Blue<-rgb_prop$Blue/255
>> library(scatterplot3d)
>> scatterplot3d(rgb_prop[,1:3],cex.symbols=sqrt(rgb_prop[,4]),
>>  color=rgb(rgb_prop[,1],rgb_prop[,2],rgb_prop[,3]),pch=19)
>>
>> then plot the RGB values on a 3D scatterplot. I have included
>> arguments to make the symbols the actual RGB colors that you specify
>> and their size proportional to the square root of the percentages.
>>
>> Jim
>>
>> On Wed, Dec 19, 2018 at 5:17 AM Tasha O'Hara <tasha.eileen at gmail.com> wrote:
>> >
>> > Hello,
>> >
>> > I am trying to plot specific rgb color proportions of a marine specimen in
>> > a stacked plot using R and I was looking for some help. I have several
>> > rgb proportions per specimen (an example of one is below).  I've run into
>> > different examples of people using vegan or grDevices. Can anyone help with
>> > this?
>> >
>> > Red    Green  Blue   %
>> > 249 158 37 56.311
>> > 249 158 68 4.319
>> > 249 158 98 0.058
>> > 249 128 7 13.965
>> > 249 128 37 12.87
>> > 188 128 37 0.029
>> > 249 128 68 0.161
>> > 188 128 68 0.015
>> > 188 98 7 0.029
>> > 219 128 7 2.773
>> > 219 128 37 2.583
>> > 188 98 68 0.058
>> > 219 128 68 0.525
>> > 249 188 37 0.876
>> > 249 188 68 1.08
>> > 219 98 7 0.482
>> > 249 188 98 0.015
>> > 249 158 7 3.852
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From tring @ending from gvdnet@dk  Wed Dec 19 10:03:09 2018
From: tring @ending from gvdnet@dk (Troels Ring)
Date: Wed, 19 Dec 2018 10:03:09 +0100
Subject: [R] uniroot problem
Message-ID: <00b501d49779$aa2a4180$fe7ec480$@gvdnet.dk>

Dear friends and helpers - in the script below, uniroot is called with a
function CHB that calls a function, Charge. On its own, CHB apparently does
what is expected, but from within uniroot, problems appear. An error is
thrown 

Error in f(lower, ...) : could not find function "f"

So CHB is not seen or understood from within uniroot?

 

I'm on windows 10, 64 bit R version 3.5.1 (2018-07-02)

All best wishes

Troels

 

 

kw <- 1e-14

TOT <- 1

Pk1 <- 10^-2.16

Pk2 <- 10^-7.21

Pk3 <- 10^-12.32 

 

K <- c(Pk1,Pk2,Pk3)

f <- c(1,1,1)

H <- 10^-7.4

 

Charge <- function(TOT,f,K,H) 

{

  num <- c() 

  num[1] <- K[1]/(f[1]^2*H)

  for (i in 2:length(K)) num[i] <- i*prod(K[1:i])/(f[1]^i*f[i]*H^i)

  num <- sum(num) 

  denum <- c()

  denum[1] <- 1+ K[1]/(f[1]^2*H)

  for (i in 2:length(K)) denum[i] <- prod(K[1:i])/(f[1]^i*f[i]*H^i)

  denum <- sum(denum)

  num/denum

}

 

Na <- 0.140

Cl <- 0.1

 

CHB  <- function(Na,Cl,H,K,f,TOT) {Na-Cl+H-kw/(f[1]^2*H)-Charge(TOT,f,K,H)}

 

H <- uniroot(CHB,interval=c(1e-19,5),tol=.Machine$double.eps,maxiter=100000,

             Na=Na,Cl=Cl,K=K,TOT=TOT,f=f)$root

#Error in f(lower, ...) : could not find function "f"

 

 

CHB(Na,Cl,10^-7.4,K,f,TOT) # -1.567668 OK right!


	[[alternative HTML version deleted]]


From E@Vettor@zzi @ending from uke@de  Wed Dec 19 10:11:57 2018
From: E@Vettor@zzi @ending from uke@de (Eik Vettorazzi)
Date: Wed, 19 Dec 2018 10:11:57 +0100
Subject: [R] geom_ribbon function in ggplot2 package
In-Reply-To: <CABcx46DkoZd2ky3dmLn-3398GMJYkP9uka186cY0QtLHkaV+PQ@mail.gmail.com>
References: <CABcx46DkoZd2ky3dmLn-3398GMJYkP9uka186cY0QtLHkaV+PQ@mail.gmail.com>
Message-ID: <b0305007-3639-34bc-574d-c5fa1ca74e96@uke.de>

Hi,
just add +scale_fill_discrete(name=NULL)

Cheers

Am 19.12.2018 um 07:05 schrieb John:
> Hi,
> 
>     When using the geom_ribbon function in gglot2 package, I got the text
> "fill" above the legend "A" and "B". How can I get rid of the text "fill"
> above the legend?
> 
>     Thanks!
> 
> The code is as follows:
> 
> 
> df<-data.frame(x=c(1,2), y=c(1,2), z=c(3,5))
>> ggplot(df,
> aes(1:2))+geom_ribbon(aes(ymin=0,ymax=df[,"y"],fill="A"),alpha=0.5)+geom_ribbon(aes(ymin=0,ymax=df[,"z"],fill="B"),alpha=0.5)
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Eik Vettorazzi

Department of Medical Biometry and Epidemiology
University Medical Center Hamburg-Eppendorf

Martinistrasse 52
building W 34
20246 Hamburg

Phone: +49 (0) 40 7410 - 58243
Fax:   +49 (0) 40 7410 - 57790
Web: www.uke.de/imbe
--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING

From tring @ending from gvdnet@dk  Wed Dec 19 10:53:49 2018
From: tring @ending from gvdnet@dk (Troels Ring)
Date: Wed, 19 Dec 2018 10:53:49 +0100
Subject: [R] uniroot problem
In-Reply-To: ZY8kgUQiTtXDiZY8lgPzos
References: <00b501d49779$aa2a4180$fe7ec480$@gvdnet.dk> ZY8kgUQiTtXDiZY8lgPzos
Message-ID: <00c201d49780$bd87f900$3897eb00$@gvdnet.dk>

Thanks a lot - f was renamed FF and things are OK
BW
Troels

-----Oprindelig meddelelse-----
Fra: Berwin A Turlach <berwin.turlach at gmail.com> 
Sendt: 19. december 2018 10:27
Til: Troels Ring <tring at gvdnet.dk>
Emne: Re: [R] uniroot problem

G'day Troels,

On Wed, 19 Dec 2018 10:03:09 +0100
"Troels Ring" <tring at gvdnet.dk> wrote:

> Dear friends and helpers - in the script below, uniroot is called with 
> a function CHB that calls a function, Charge. On its own, CHB 
> apparently does what is expected, but from within uniroot, problems 
> appear. An error is thrown
> 
> Error in f(lower, ...) : could not find function "f"
> 
> So CHB is not seen or understood from within uniroot?

Read the help page of uniroot.  

The first argument is called "f", it is the function for which the root is
searched.  In your call:

> uniroot(CHB,interval=c(1e-19,5),tol=.Machine$double.eps,maxiter=100000,
>              Na=Na,Cl=Cl,K=K,TOT=TOT,f=f)$root

You pass the object "f" (a vector "f <- c(1,1,1)" created earlier in your
code) to the argument "f".  Presumably there is no function called "f" in
your search path, and so R correctly complains that it cannot find the
function whose root you are looking for.

In R, arguments are passed first by exact matching of actual and formal
arguments, then by partial matching and then by position.

The easiest fix is probably to rename the object "f" to something else.

Cheers,

	Berwin


From petr@pik@l @ending from prechez@@cz  Wed Dec 19 11:28:15 2018
From: petr@pik@l @ending from prechez@@cz (PIKAL Petr)
Date: Wed, 19 Dec 2018 10:28:15 +0000
Subject: [R] Plotting rgb proportions in R
In-Reply-To: <CAGxFJbQODiuWSL_QTeYT=sHKJnR+ZpvRca233rT4-mEXisK9_g@mail.gmail.com>
References: <CAKpKZ1zwnNcOeF8YwqNc8PxyRmhLwhS6p7CW5rORhey69KGNpg@mail.gmail.com>
 <CA+8X3fUaF_BUw9QUg3QZ57GiNAX8n+CZuRhW9kJQxVf4FEFZuQ@mail.gmail.com>
 <CAGxFJbQODiuWSL_QTeYT=sHKJnR+ZpvRca233rT4-mEXisK9_g@mail.gmail.com>
Message-ID: <f858814ee44443b4a66015138a5312fb@SRVEXCHCM1302.precheza.cz>

Hi

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Bert Gunter
> Sent: Wednesday, December 19, 2018 5:26 AM
> To: Jim Lemon <drjimlemon at gmail.com>
> Cc: R-help <r-help at r-project.org>
> Subject: Re: [R] Plotting rgb proportions in R
>
> 3-d Proportions must sum to 1and are thus actually 2-d and should preferaby
> be plotted as a ternary plot. Several r packages will do this for you, e.g.
> package Ternary. Search "ternary plots" on rseek.org for others.

From which I would recommend ggtern. Sometimes a bit tricky but quite handy, especially when you consider some grouping.

Cheers
Petr

>
> -- Bert
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Tue, Dec 18, 2018 at 3:10 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> > Hi Tasha,
> > I may be right off the track, but you could plot RGB proportions on a
> > 3D plot. The easiest way I can think if would be to convert your 0-255
> > values to proportions:
> >
> > rgb_prop<-read.table(text="Red Green Blue pct
> > 249 158 37 56.311
> > 249 158 68 4.319
> > 249 158 98 0.058
> > 249 128 7 13.965
> > 249 128 37 12.87
> > 188 128 37 0.029
> > 249 128 68 0.161
> > 188 128 68 0.015
> > 188 98 7 0.029
> > 219 128 7 2.773
> > 219 128 37 2.583
> > 188 98 68 0.058
> > 219 128 68 0.525
> > 249 188 37 0.876
> > 249 188 68 1.08
> > 219 98 7 0.482
> > 249 188 98 0.015
> > 249 158 7 3.852",header=TRUE)
> > rgb_prop$Red<-rgb_prop$Red/255
> > rgb_prop$Green<-rgb_prop$Green/255
> > rgb_prop$Blue<-rgb_prop$Blue/255
> > library(scatterplot3d)
> > scatterplot3d(rgb_prop[,1:3],cex.symbols=sqrt(rgb_prop[,4]),
> >  color=rgb(rgb_prop[,1],rgb_prop[,2],rgb_prop[,3]),pch=19)
> >
> > then plot the RGB values on a 3D scatterplot. I have included
> > arguments to make the symbols the actual RGB colors that you specify
> > and their size proportional to the square root of the percentages.
> >
> > Jim
> >
> > On Wed, Dec 19, 2018 at 5:17 AM Tasha O'Hara <tasha.eileen at gmail.com>
> > wrote:
> > >
> > > Hello,
> > >
> > > I am trying to plot specific rgb color proportions of a marine
> > > specimen
> > in
> > > a stacked plot using R and I was looking for some help. I have
> > > several rgb proportions per specimen (an example of one is below).
> > > I've run into different examples of people using vegan or grDevices.
> > > Can anyone help
> > with
> > > this?
> > >
> > > Red    Green  Blue   %
> > > 249 158 37 56.311
> > > 249 158 68 4.319
> > > 249 158 98 0.058
> > > 249 128 7 13.965
> > > 249 128 37 12.87
> > > 188 128 37 0.029
> > > 249 128 68 0.161
> > > 188 128 68 0.015
> > > 188 98 7 0.029
> > > 219 128 7 2.773
> > > 219 128 37 2.583
> > > 188 98 68 0.058
> > > 219 128 68 0.525
> > > 249 188 37 0.876
> > > 249 188 68 1.08
> > > 219 98 7 0.482
> > > 249 188 98 0.015
> > > 249 158 7 3.852
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


From m@rongiu@luigi @ending from gm@il@com  Wed Dec 19 11:58:46 2018
From: m@rongiu@luigi @ending from gm@il@com (Luigi Marongiu)
Date: Wed, 19 Dec 2018 11:58:46 +0100
Subject: [R] convert columns of dataframe to same factor levels
Message-ID: <CAMk+s2QqC+bWVitSyB7TNLT5fskz57wDFdvD=q1nehA3hZX_Dw@mail.gmail.com>

Dear all,
I have a data frame with character values where each character is a
level; however, not all columns of the data frame have the same
characters thus, when generating the data frame with stringsAsFactors
= TRUE, the levels are different for each column.
Is there a way to provide a single vector of levels and assign the
characters so that they match such vector?
Is there a way to do that not only when setting the data frame but
also when reading data from a file with read.table()?

For instance, I have:
column_1 = c("A", "B", "C", "D", "E")
column_2 = c("B", "B", "C", "E", "E")
column_3 = c("C", "C", "D", "D", "C")
my.data <- data.frame(column_1, column_2, column_3, stringsAsFactors = TRUE)
> str(my.data)
'data.frame': 5 obs. of  3 variables:
 $ column_1: Factor w/ 5 levels "A","B","C","D",..: 1 2 3 4 5
 $ column_2: Factor w/ 3 levels "B","C","E": 1 1 2 3 3
 $ column_3: Factor w/ 2 levels "C","D": 1 1 2 2 1

Thank you
-- 
Best regards,
Luigi


From murdoch@dunc@n @ending from gm@il@com  Wed Dec 19 12:19:06 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Wed, 19 Dec 2018 06:19:06 -0500
Subject: [R] convert columns of dataframe to same factor levels
In-Reply-To: <CAMk+s2QqC+bWVitSyB7TNLT5fskz57wDFdvD=q1nehA3hZX_Dw@mail.gmail.com>
References: <CAMk+s2QqC+bWVitSyB7TNLT5fskz57wDFdvD=q1nehA3hZX_Dw@mail.gmail.com>
Message-ID: <1e6721db-71aa-30ca-d4b5-a8d4b60fd767@gmail.com>

On 19/12/2018 5:58 AM, Luigi Marongiu wrote:
> Dear all,
> I have a data frame with character values where each character is a
> level; however, not all columns of the data frame have the same
> characters thus, when generating the data frame with stringsAsFactors
> = TRUE, the levels are different for each column.
> Is there a way to provide a single vector of levels and assign the
> characters so that they match such vector?
> Is there a way to do that not only when setting the data frame but
> also when reading data from a file with read.table()?
> 
> For instance, I have:
> column_1 = c("A", "B", "C", "D", "E")
> column_2 = c("B", "B", "C", "E", "E")
> column_3 = c("C", "C", "D", "D", "C")
> my.data <- data.frame(column_1, column_2, column_3, stringsAsFactors = TRUE)
>> str(my.data)
> 'data.frame': 5 obs. of  3 variables:
>   $ column_1: Factor w/ 5 levels "A","B","C","D",..: 1 2 3 4 5
>   $ column_2: Factor w/ 3 levels "B","C","E": 1 1 2 3 3
>   $ column_3: Factor w/ 2 levels "C","D": 1 1 2 2 1
> 
> Thank you
> 

I don't think read.table() can do it for you automatically.  To do it 
yourself, you need to get a vector of the levels.  If you know this, 
just assign it to a variable; if you don't know it, compute it as

   thelevels <- unique(unlist(lapply(my.data, levels)))

Then set the levels of each column to thelevels:

   my.data.new <- as.data.frame(lapply(my.data, function(x) {levels(x) 
<- thelevels; x}))

Duncan Murdoch


From neotropic@l@b@t@ @ending from gm@il@com  Wed Dec 19 12:38:43 2018
From: neotropic@l@b@t@ @ending from gm@il@com (Neotropical bat risk assessments)
Date: Wed, 19 Dec 2018 06:38:43 -0500
Subject: [R] rewrite code from Deducer to run on R-Studio
In-Reply-To: <mailman.352884.1.1545217201.26200.r-help@r-project.org>
References: <mailman.352884.1.1545217201.26200.r-help@r-project.org>
Message-ID: <68caf3fe-4c55-aa90-509c-34b9f6a70c25@gmail.com>

Hi all,

Apparently Deducer (a GUI for R) is no longer supported or being updated.

What is the easiest way to re-write code to run using R-Studio?

An example:

BatStats<- Deducer::descriptive.table (vars = d 
(Dur,Fmin,Fmax,Fmean,Fk,Fc,Sc,Pmc),data= BatStats,func.names =c("Valid 
N","Minimum","Maximum","Mean","St. Deviation","25th Percentile","75th 
Percentile"),func.additional= list(p10=function(x) quantile(x, c(0.10), 
na.rm=TRUE),p90=function(x) quantile(x, c(0.90), na.rm=TRUE)))


So I will be on a new learning curve.


Tnx for any suggestions

Bruce


-- 
Neotropical bat projects
Making call Fact Sheets and interactive ID keys of the New World bat vocal signature freely available to all.
Pioneering acoustic identifications of Neotropical bats since 1995
Bruce W. Miller, PhD.
Conservation Fellow, Wildlife Conservation Society


From m@rongiu@luigi @ending from gm@il@com  Wed Dec 19 12:48:09 2018
From: m@rongiu@luigi @ending from gm@il@com (Luigi Marongiu)
Date: Wed, 19 Dec 2018 12:48:09 +0100
Subject: [R] convert columns of dataframe to same factor levels
In-Reply-To: <1e6721db-71aa-30ca-d4b5-a8d4b60fd767@gmail.com>
References: <CAMk+s2QqC+bWVitSyB7TNLT5fskz57wDFdvD=q1nehA3hZX_Dw@mail.gmail.com>
 <1e6721db-71aa-30ca-d4b5-a8d4b60fd767@gmail.com>
Message-ID: <CAMk+s2SZ8mCZVzx3WvrofcW9eFSiZvcacJt2H2udF9V5vFvOSQ@mail.gmail.com>

Thank you,
that worked fine for me.
Best wishes of merry Christmas and happy new year,
Luigi

On Wed, Dec 19, 2018 at 12:19 PM Duncan Murdoch
<murdoch.duncan at gmail.com> wrote:
>
> On 19/12/2018 5:58 AM, Luigi Marongiu wrote:
> > Dear all,
> > I have a data frame with character values where each character is a
> > level; however, not all columns of the data frame have the same
> > characters thus, when generating the data frame with stringsAsFactors
> > = TRUE, the levels are different for each column.
> > Is there a way to provide a single vector of levels and assign the
> > characters so that they match such vector?
> > Is there a way to do that not only when setting the data frame but
> > also when reading data from a file with read.table()?
> >
> > For instance, I have:
> > column_1 = c("A", "B", "C", "D", "E")
> > column_2 = c("B", "B", "C", "E", "E")
> > column_3 = c("C", "C", "D", "D", "C")
> > my.data <- data.frame(column_1, column_2, column_3, stringsAsFactors = TRUE)
> >> str(my.data)
> > 'data.frame': 5 obs. of  3 variables:
> >   $ column_1: Factor w/ 5 levels "A","B","C","D",..: 1 2 3 4 5
> >   $ column_2: Factor w/ 3 levels "B","C","E": 1 1 2 3 3
> >   $ column_3: Factor w/ 2 levels "C","D": 1 1 2 2 1
> >
> > Thank you
> >
>
> I don't think read.table() can do it for you automatically.  To do it
> yourself, you need to get a vector of the levels.  If you know this,
> just assign it to a variable; if you don't know it, compute it as
>
>    thelevels <- unique(unlist(lapply(my.data, levels)))
>
> Then set the levels of each column to thelevels:
>
>    my.data.new <- as.data.frame(lapply(my.data, function(x) {levels(x)
> <- thelevels; x}))
>
> Duncan Murdoch



-- 
Best regards,
Luigi


From petr@pik@l @ending from prechez@@cz  Wed Dec 19 13:14:15 2018
From: petr@pik@l @ending from prechez@@cz (PIKAL Petr)
Date: Wed, 19 Dec 2018 12:14:15 +0000
Subject: [R] 
 Trying to fix code that will find highest 5 column names and
 their associated values for each row in a data frame in R
In-Reply-To: <20181217113347.Horde.HiAl-rAhWHuIZ4EX7S6E6y-@www.ontargettek.com>
References: <20181217113347.Horde.HiAl-rAhWHuIZ4EX7S6E6y-@www.ontargettek.com>
Message-ID: <62e52acc69aa4979b3ae8ccaa000ae0f@SRVEXCHCM1302.precheza.cz>

Hi

generated DF is not what you expect it is

>   set.seed(1)
>   DF <- matrix(sample(1:9,9),ncol=10,nrow=9)
> DF
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
 [1,]    3    3    3    3    3    3    3    3    3     3
 [2,]    9    9    9    9    9    9    9    9    9     9
 [3,]    5    5    5    5    5    5    5    5    5     5
 [4,]    6    6    6    6    6    6    6    6    6     6
 [5,]    2    2    2    2    2    2    2    2    2     2
 [6,]    4    4    4    4    4    4    4    4    4     4
 [7,]    8    8    8    8    8    8    8    8    8     8
 [8,]    7    7    7    7    7    7    7    7    7     7
 [9,]    1    1    1    1    1    1    1    1    1     1
>

with slight input modification

> set.seed(1)
> DF <- matrix(sample(1:9,90, replace=T), ncol=10, nrow=9)
>   DF <- as.data.frame.matrix(DF)
>

> out <- t(apply(DF, 1, function(x){
+     o <- head(order(-x), 5)
+     paste0(names(x[o]), ':', x[o])
+   }))
>   as.data.frame(out)
    V1   V2    V3   V4    V5
1 V5:8 V6:8 V10:7 V3:4  V4:4
2 V4:8 V3:7  V8:6 V1:4  V9:4
3 V3:9 V5:7  V1:6 V6:5  V9:5
4 V1:9 V9:9  V2:7 V6:7 V10:7
5 V5:8 V9:8  V6:7 V8:7  V3:6
6 V1:9 V2:7 V10:7 V5:6  V4:5
7 V1:9 V7:9  V5:8 V6:8  V8:8
8 V9:9 V4:8  V2:7 V1:6  V5:5
9 V2:9 V8:8  V4:7 V1:6  V5:5

your code seems to work.
Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Tom Woolman
> Sent: Monday, December 17, 2018 6:34 PM
> To: r-help at r-project.org
> Subject: [R] Trying to fix code that will find highest 5 column names and their
> associated values for each row in a data frame in R
>
>
> I have a data frame each with 10 variables of integer data for various
>   attributes about each row of data, and I need to know the highest 5 variables
> related to each of
>   row in this data frame and output that to a new data frame. In addition to
>   the 5 highest variable names, I also need to know the corresponding 5
>   highest variable values for each row.
>
>   A simple code example to generate a sample data frame for this is:
>
>   set.seed(1)
>   DF <- matrix(sample(1:9,9),ncol=10,nrow=9)
>   DF <- as.data.frame.matrix(DF)
>
>
> This would result in an example data frame like this:
>
>   #   V1 V2 V3 V4 V5 V6 V7 V8 V9 V10
>   # 1  3  2  5  6  5  2  6  8  1   3
>   # 2  1  4  7  8  7  7  3  4  2   9
>   # 3  2  3  4  7  5  8  9  1  3   5
>   # 4  3  8  3  4  5  6  7  4  6   5
>   # 5  6  2  3  7  2  1  8  3  2   4
>   # 6  8  2  4  8  3  2  9  7  6   5
>   # 7  1  5  3  6  8  3  8  9  1   3
>   # 8  9  3  5  8  4  9  7  8  1   2
>   # 9  1  2  4  8  3  2  1  2  5   6
>
>
>   My ideal output would be something like this:
>
>
>   #      V1   V2   V3   V4   V5
>   # 1  V2:9 V7:8 V8:7 V4:6 V3:5
>   # 2  V9:9 V3:8 V5:7 V7:6 V4:5
>   # 3  V5:9 V3:8 V2:7 V9:6 V7:5
>   # 4  V8:9 V4:8 V2:7 V5:6 V9:5
>   # 5  V9:9 V1:8 V6:7 V3:6 V5:5
>   # 6  V8:9 V1:8 V5:7 V9:6 V4:5
>   # 7  V2:9 V8:8 V7:7 V5:6 V9:5
>   # 8  V4:9 V7:8 V9:7 V2:6 V8:5
>   # 9  V3:9 V7:8 V8:7 V4:6 V5:5
>   # 10 V6:9 V8:8 V1:7 V9:6 V4:5
>
>
>   I was trying to use code, but this doesn't seem to work:
>
>   out <- t(apply(DF, 1, function(x){
>     o <- head(order(-x), 5)
>     paste0(names(x[o]), ':', x[o])
>   }))
>   as.data.frame(out)
>
>
>
>   Thanks everyone!
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


From murdoch@dunc@n @ending from gm@il@com  Wed Dec 19 14:01:47 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Wed, 19 Dec 2018 08:01:47 -0500
Subject: [R] convert columns of dataframe to same factor levels
In-Reply-To: <CAMk+s2SZ8mCZVzx3WvrofcW9eFSiZvcacJt2H2udF9V5vFvOSQ@mail.gmail.com>
References: <CAMk+s2QqC+bWVitSyB7TNLT5fskz57wDFdvD=q1nehA3hZX_Dw@mail.gmail.com>
 <1e6721db-71aa-30ca-d4b5-a8d4b60fd767@gmail.com>
 <CAMk+s2SZ8mCZVzx3WvrofcW9eFSiZvcacJt2H2udF9V5vFvOSQ@mail.gmail.com>
Message-ID: <1dc6ac0d-2be3-5fb2-f1d5-92edc8618759@gmail.com>

On 19/12/2018 6:48 AM, Luigi Marongiu wrote:
> Thank you,
> that worked fine for me.
> Best wishes of merry Christmas and happy new year,
> Luigi
> 

Actually it's wrong!  Sorry about that.

If you look at my.data.new$column_2, you'll see that the levels have 
changed:

 > my.data
   column_1 column_2 column_3
1        A        B        A
2        B        B        A
3        C        C        B
4        D        E        B
5        E        E        A


 > my.data.new
   column_1 column_2 column_3
1        A        A        A
2        B        A        A
3        C        B        B
4        D        C        B
5        E        C        A

What you want is this instead:

my.data.new <- as.data.frame(lapply(my.data, function(x) {factor(x, 
levels = thelevels)}))

The last example in the ?levels help page does this too.  I wonder if 
that is intentional?

levels> ## we can add levels this way:
levels> f <- factor(c("a","b"))

levels> levels(f) <- c("c", "a", "b")

levels> f
[1] c a
Levels: c a b

levels> f <- factor(c("a","b"))

levels> levels(f) <- list(C = "C", A = "a", B = "b")

levels> f
[1] A B
Levels: C A B

Duncan Murdoch

> On Wed, Dec 19, 2018 at 12:19 PM Duncan Murdoch
> <murdoch.duncan at gmail.com> wrote:
>>
>> On 19/12/2018 5:58 AM, Luigi Marongiu wrote:
>>> Dear all,
>>> I have a data frame with character values where each character is a
>>> level; however, not all columns of the data frame have the same
>>> characters thus, when generating the data frame with stringsAsFactors
>>> = TRUE, the levels are different for each column.
>>> Is there a way to provide a single vector of levels and assign the
>>> characters so that they match such vector?
>>> Is there a way to do that not only when setting the data frame but
>>> also when reading data from a file with read.table()?
>>>
>>> For instance, I have:
>>> column_1 = c("A", "B", "C", "D", "E")
>>> column_2 = c("B", "B", "C", "E", "E")
>>> column_3 = c("C", "C", "D", "D", "C")
>>> my.data <- data.frame(column_1, column_2, column_3, stringsAsFactors = TRUE)
>>>> str(my.data)
>>> 'data.frame': 5 obs. of  3 variables:
>>>    $ column_1: Factor w/ 5 levels "A","B","C","D",..: 1 2 3 4 5
>>>    $ column_2: Factor w/ 3 levels "B","C","E": 1 1 2 3 3
>>>    $ column_3: Factor w/ 2 levels "C","D": 1 1 2 2 1
>>>
>>> Thank you
>>>
>>
>> I don't think read.table() can do it for you automatically.  To do it
>> yourself, you need to get a vector of the levels.  If you know this,
>> just assign it to a variable; if you don't know it, compute it as
>>
>>     thelevels <- unique(unlist(lapply(my.data, levels)))
>>
>> Then set the levels of each column to thelevels:
>>
>>     my.data.new <- as.data.frame(lapply(my.data, function(x) {levels(x)
>> <- thelevels; x}))
>>
>> Duncan Murdoch
> 
> 
>


From dc@rl@on @ending from t@mu@edu  Wed Dec 19 18:21:12 2018
From: dc@rl@on @ending from t@mu@edu (David L Carlson)
Date: Wed, 19 Dec 2018 17:21:12 +0000
Subject: [R] rewrite code from Deducer to run on R-Studio
In-Reply-To: <68caf3fe-4c55-aa90-509c-34b9f6a70c25@gmail.com>
References: <mailman.352884.1.1545217201.26200.r-help@r-project.org>
 <68caf3fe-4c55-aa90-509c-34b9f6a70c25@gmail.com>
Message-ID: <e55d24f181024daca12ec4669a0ed8bc@tamu.edu>

Since R is open source, you can download the source package file for Deducer, extract the code for the descriptive.table function, and continue to use it without installing the whole package.

Alternatively, there are descriptive stats functions in many R packages. You might try numSummary() in package RcmdrMisc, Desc() in DescTools, describe() in Hmisc, stat.desc() in pastecs, or describe() in psych. Try several until you find one that formats the results the way you want.

-------------------------------
David L. Carlson
Department of Anthropology
Texas A&M University


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Neotropical bat risk assessments
Sent: Wednesday, December 19, 2018 5:39 AM
To: r-help at r-project.org
Subject: [R] rewrite code from Deducer to run on R-Studio

Hi all,

Apparently Deducer (a GUI for R) is no longer supported or being updated.

What is the easiest way to re-write code to run using R-Studio?

An example:

BatStats<- Deducer::descriptive.table (vars = d 
(Dur,Fmin,Fmax,Fmean,Fk,Fc,Sc,Pmc),data= BatStats,func.names =c("Valid
N","Minimum","Maximum","Mean","St. Deviation","25th Percentile","75th 
Percentile"),func.additional= list(p10=function(x) quantile(x, c(0.10),
na.rm=TRUE),p90=function(x) quantile(x, c(0.90), na.rm=TRUE)))


So I will be on a new learning curve.


Tnx for any suggestions

Bruce


-- 
Neotropical bat projects
Making call Fact Sheets and interactive ID keys of the New World bat vocal signature freely available to all.
Pioneering acoustic identifications of Neotropical bats since 1995
Bruce W. Miller, PhD.
Conservation Fellow, Wildlife Conservation Society

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From pjmiller_57 @ending from y@hoo@com  Wed Dec 19 18:28:35 2018
From: pjmiller_57 @ending from y@hoo@com (Paul Miller)
Date: Wed, 19 Dec 2018 17:28:35 +0000 (UTC)
Subject: [R] R code for if-then-do code blocks
In-Reply-To: <CAP01uRn0AyT=oRq13TMjz4e+-iLiKtyxfwrj_tQBmTKZ88KXKQ@mail.gmail.com>
References: <mailman.352837.1.1545044401.60322.r-help@r-project.org>
 <2088363583.889377.1545060471869@mail.yahoo.com>
 <CAP01uRn0AyT=oRq13TMjz4e+-iLiKtyxfwrj_tQBmTKZ88KXKQ@mail.gmail.com>
Message-ID: <1693217539.259893.1545240515559@mail.yahoo.com>

Hi Gabor, Richard, and Thierry, 

Thanks very much for your replies. Turns out I had already hit on Gabor's idea of "factor out" in writing an initial draft of the code converting from SAS to R. Below is the link Gabor sent describing this and other approaches. 

https://stackoverflow.com/questions/34096162/dplyr-mutate-replace-on-a-subset-of-rows/34096575#34096575

At the end of this email are some new test data plus a snippet of my initial R code. The R code I have replicates the result from SAS but is quite verbose. That should be obvious from the snippet. I know I can make the code less verbose with a subsequent draft but wonder if I can simplify to the point where the factor out approach gets a fair test. I'd appreciate it if people could share some ways to make the factor out approach less verbose. I'd also like to see how well some of the other approaches might work with these data. I spent considerable time looking at the link Gabor sent as well as the other responses I received. The mutate_cond function in the link seems promising but it wasn't clear to me how I could avoid having to repeat the various conditions using that approach. 

Thanks again.

Paul

library(magrittr)
library(dplyr)
?
test_data <-
? structure(
??? list(
?? ?? intPatientId = c("3", "37", "48", "6", "6", "5"),
????? intSurveySessionId = c(1L, 10996L, 19264L, 2841L, 28L, 34897L),
????? a_CCMA02 = c(NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_),
????? a_CCMA69 = c(7, NA, 0, 2, NA, 0),
????? a_CCMA70 = c(7, 0, NA, 10, NA, NA),
????? a_CCMA72 = c(7, 2, 3, NA, NA, NA),
????? CCMA2 = c(NA_integer_, NA_integer_, NA_integer_, NA_integer_,NA_integer_, NA_integer_),
????? a_CCMA05 = c(NA, NA, NA, NA, NA, 0),
????? a_CCMA43 = c(5, 0, 6, 5, NA, NA),
????? a_CCMA44 = c(5, 0, 0, 5, 0, NA),
????? CCMA5 = c(NA, NA, NA, NA, NA, 0)
??? ),
??? class = "data.frame",
??? row.names = c(NA,-6L)
? )

factor_out <- test_data %>%
? mutate(
??? CCMA2_cond = case_when(
????? (is.na(a_CCMA02) | a_CCMA02 < 0 | a_CCMA02 > 10) &
??????? (!is.na(a_CCMA69) & between(a_CCMA69, 0, 10) &
?????????? !is.na(a_CCMA70) & between(a_CCMA70, 0, 10) &
?????????? !is.na(a_CCMA72) & between(a_CCMA72, 0, 10)) ~ "A",
????? (is.na(a_CCMA02) | a_CCMA02 < 0 | a_CCMA02 > 10) &
??????? (is.na(a_CCMA69) | a_CCMA69 < 0 | a_CCMA69 >= 10) &
??????? !is.na(a_CCMA70) & between(a_CCMA70, 0, 10) &
??????? !is.na(a_CCMA72) & between(a_CCMA72, 0, 10) ~ "B",
????? (is.na(a_CCMA02) | a_CCMA02 < 0 | a_CCMA02 > 10) &
??????? (is.na(a_CCMA70) | a_CCMA70 < 0 | a_CCMA70 >= 10) &
??????? between(a_CCMA69, 0, 10) & between(a_CCMA72, 0, 10) ~ "C",
????? (is.na(a_CCMA02) | a_CCMA02 < 0 | a_CCMA02 > 10) &
??????? (is.na(a_CCMA72) | a_CCMA72 < 0 | a_CCMA72 >= 10) &
??????? between(a_CCMA69, 0, 10) & between(a_CCMA70, 0, 10) ~ "D")
? ) %>%
? mutate(
??? CCMA2 = case_when(
????? CCMA2_cond == "A" & 0.614 + (0.065 * a_CCMA69) + (-0.012 * a_CCMA70) + (0.504 * a_CCMA72) < 0? ~ 0,
????? CCMA2_cond == "A" & 0.614 + (0.065 * a_CCMA69) + (-0.012 * a_CCMA70) + (0.504 * a_CCMA72) > 10 ~ 10,
????? CCMA2_cond == "A" ~ 0.614 + (0.065 * a_CCMA69) + (-0.012 * a_CCMA70) + (0.504 * a_CCMA72),
????? TRUE ~ as.double(CCMA2)
??? ),
??? CCMA2 = case_when(
????? CCMA2_cond == "B" & 0.614 + (0.065 * (a_CCMA70 + a_CCMA72) / 2) + (-0.012 * a_CCMA70) + (0.504 * a_CCMA72) < 0? ~ 0,
????? CCMA2_cond == "B" & 0.614 + (0.065 * (a_CCMA70 + a_CCMA72) / 2) + (-0.012 * a_CCMA70) + (0.504 * a_CCMA72) > 10 ~ 10,
????? CCMA2_cond == "B" ~ 0.614 + (0.065 * (a_CCMA70 + a_CCMA72) / 2) + (-0.012 * a_CCMA70) + (0.504 * a_CCMA72),
????? TRUE ~ as.double(CCMA2)
??? ),
??? CCMA2 = case_when(
????? CCMA2_cond == "C" & 0.614 + (0.065 * a_CCMA69) + (-0.012 *(a_CCMA72 + a_CCMA69) / 2 ) + (0.504 * a_CCMA72) < 0? ~ 0,
????? CCMA2_cond == "C" & 0.614 + (0.065 * a_CCMA69) + (-0.012 *(a_CCMA72 + a_CCMA69) / 2 ) + (0.504 * a_CCMA72) > 10 ~ 10,
????? CCMA2_cond == "C" ~ 0.614 + (0.065 * a_CCMA69) + (-0.012 *(a_CCMA72 + a_CCMA69) / 2 ) + (0.504 * a_CCMA72),
????? TRUE ~ as.double(CCMA2)
??? ),
??? CCMA2 = case_when(
????? CCMA2_cond == "D" & 0.614 + (0.065 * a_CCMA69) + (-0.012 * a_CCMA70 ) + (0.504 *(a_CCMA70 + a_CCMA69) / 2) < 0? ~ 0,
????? CCMA2_cond == "D" & 0.614 + (0.065 * a_CCMA69) + (-0.012 * a_CCMA70 ) + (0.504 *(a_CCMA70 + a_CCMA69) / 2) > 10 ~ 10,
????? CCMA2_cond == "D" ~ 0.614 + (0.065 * a_CCMA69) + (-0.012 * a_CCMA70 ) + (0.504 *(a_CCMA70 + a_CCMA69) / 2),
????? TRUE ~ as.double(CCMA2)
??? )
? ) %>%
? select(-CCMA2_cond) %>%
? mutate(
??? CCMA5_condA = if_else(
????? (is.na(a_CCMA05) | a_CCMA05 < 0 | a_CCMA05 > 10),
????? 1, 0
??? ),
??? CCMA5 = ifelse(CCMA5_condA == 1 & between(a_CCMA43, 0, 10) & between(a_CCMA44, 0, 10),
?????????????????? 0.216 + (0.257 * a_CCMA43) + (0.828 * a_CCMA44),
?????????????????? CCMA5),
??? CCMA5 = ifelse(CCMA5_condA == 1 & between(a_CCMA43, 0, 10) & (is.na(a_CCMA44) | a_CCMA44 < 0 | a_CCMA44 > 10),
?????????????????? 0.216 + (0.257 * a_CCMA43) + (0.828 * a_CCMA43),
?????????????????? CCMA5),
??? CCMA5 = ifelse(CCMA5_condA == 1 & between(a_CCMA44, 0, 10) & (is.na(a_CCMA43) | a_CCMA43 < 0 | a_CCMA43 > 10),
?????????????????? 0.216 + (0.257 * a_CCMA44) + (0.828 * a_CCMA44),
?????????????????? CCMA5),
??? CCMA5 = ifelse(CCMA5_condA == 1 & !is.na(CCMA5) & CCMA5 < 0,
?????????????????? 0,
?????????????????? CCMA5),
??? CCMA5 = ifelse(CCMA5_condA == 1 & CCMA5 > 10,
?????????????????? 10,
?????????????????? CCMA5)
? ) %>%
? select(-CCMA5_condA)


From wdunl@p @ending from tibco@com  Wed Dec 19 18:50:35 2018
From: wdunl@p @ending from tibco@com (William Dunlap)
Date: Wed, 19 Dec 2018 09:50:35 -0800
Subject: [R] convert columns of dataframe to same factor levels
In-Reply-To: <1e6721db-71aa-30ca-d4b5-a8d4b60fd767@gmail.com>
References: <CAMk+s2QqC+bWVitSyB7TNLT5fskz57wDFdvD=q1nehA3hZX_Dw@mail.gmail.com>
 <1e6721db-71aa-30ca-d4b5-a8d4b60fd767@gmail.com>
Message-ID: <CAF8bMcYR2em6OrfLWjuC=W2_KJA2nNPgZOdaxpoh_grt51DnOg@mail.gmail.com>

You can abuse the S4 class system to do this.

setClass("Size") # no representation, no prototype
setAs(from="character", to="Size", # nothing but a coercion method
  function(from){
    ret <- factor(from, levels=c("Small","Medium","Large"), ordered=TRUE)
    class(ret) <- c("Size", class(ret))
    ret
  })
z <- read.table(colClasses=c("integer", "Size"), text="7 Medium\n5 Large\n3
Large")
dput(z)
#structure(list(V1 = c(7L, 5L, 3L), V2 = structure(c(2L, 3L, 3L
#), .Label = c("Small", "Medium", "Large"), class = c("Size",
#"ordered", "factor"))), class = "data.frame", row.names = c(NA,
#-3L))

I wonder if this behavior is intended or if there is a more sanctioned way
to get read.table(colClasses=...) to make a factor with a specified set of
levels.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Wed, Dec 19, 2018 at 3:19 AM Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 19/12/2018 5:58 AM, Luigi Marongiu wrote:
> > Dear all,
> > I have a data frame with character values where each character is a
> > level; however, not all columns of the data frame have the same
> > characters thus, when generating the data frame with stringsAsFactors
> > = TRUE, the levels are different for each column.
> > Is there a way to provide a single vector of levels and assign the
> > characters so that they match such vector?
> > Is there a way to do that not only when setting the data frame but
> > also when reading data from a file with read.table()?
> >
> > For instance, I have:
> > column_1 = c("A", "B", "C", "D", "E")
> > column_2 = c("B", "B", "C", "E", "E")
> > column_3 = c("C", "C", "D", "D", "C")
> > my.data <- data.frame(column_1, column_2, column_3, stringsAsFactors =
> TRUE)
> >> str(my.data)
> > 'data.frame': 5 obs. of  3 variables:
> >   $ column_1: Factor w/ 5 levels "A","B","C","D",..: 1 2 3 4 5
> >   $ column_2: Factor w/ 3 levels "B","C","E": 1 1 2 3 3
> >   $ column_3: Factor w/ 2 levels "C","D": 1 1 2 2 1
> >
> > Thank you
> >
>
> I don't think read.table() can do it for you automatically.  To do it
> yourself, you need to get a vector of the levels.  If you know this,
> just assign it to a variable; if you don't know it, compute it as
>
>    thelevels <- unique(unlist(lapply(my.data, levels)))
>
> Then set the levels of each column to thelevels:
>
>    my.data.new <- as.data.frame(lapply(my.data, function(x) {levels(x)
> <- thelevels; x}))
>
> Duncan Murdoch
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@cqueen1 @ending from llnl@gov  Wed Dec 19 21:10:39 2018
From: m@cqueen1 @ending from llnl@gov (MacQueen, Don)
Date: Wed, 19 Dec 2018 20:10:39 +0000
Subject: [R] Problem with Plotting in R
In-Reply-To: <5C19015C.8080100@comcast.net>
References: <5C19015C.8080100@comcast.net>
Message-ID: <FA896036-2393-4937-A58E-47D9979EBF7F@llnl.gov>

You haven't described what you are trying to get with the command that doesn't work. My guess is that this might be what you want:

         plot( x, MyData$NWorth, type="l" )
        lines( x, MyData$NWorthSm)

However, you might also have to calculate and supply a for the ylim argument to plot().

--
Don MacQueen
Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062
Lab cell 925-724-7509
 
 

?On 12/18/18, 6:17 AM, "R-help on behalf of rsherry8" <r-help-bounces at r-project.org on behalf of rsherry8 at comcast.net> wrote:

    
    Please consider the following R statements:
    
         > x = seq(1:1632)
         > length( MyData$NWorth )
         [1] 1632
         > length( MyData$NWorthSm )
         [1] 1632
         > plot( x, MyData$NWorth, type="l" )
         > plot( x, MyData$NWorthSm, type="l" )
         > plot( x, MyData$NWorth, MyData$NWorthSm, type="l" )
    
    All of the above statements work except for the last one. The last one 
    produces the following message:
    
         Error in plot.window(...) : invalid 'xlim' value
    
    So I then tired this:
    
         > xlim1 = c(0, 5000)
         >plot( x, MyData$NWorth, MyData$NWorthSm, type="l", xlim = xlim1 )
    
    Which produced the following error message:
         Error in plot.window(...) : invalid 'ylim' value
    
    So, I tired this:
         > ylim1 = c(0,9000)
         > plot( x, MyData$NWorth, MyData$NWorthSm, type="l", xlim = xlim1, 
    ylim = ylim1 )
    
    Which produced the following error message:
         Error in strsplit(log, NULL) : non-character argument
    
    I would like to know what I am doing wrong.
    
    Thank you,
    Bob
    
    ______________________________________________
    R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    https://stat.ethz.ch/mailman/listinfo/r-help
    PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    and provide commented, minimal, self-contained, reproducible code.
    


From @@ti@h@v@dl@m@ni @ending from gm@il@com  Wed Dec 19 21:13:28 2018
From: @@ti@h@v@dl@m@ni @ending from gm@il@com (Satish Vadlamani)
Date: Wed, 19 Dec 2018 14:13:28 -0600
Subject: [R] Filtering data with dplyr or grep and losing data?
Message-ID: <CAK3+11TE70XxxnaWwLs0oOuBw6HKBHkeFAv7kjoGovbgE7mKQw@mail.gmail.com>

Hello Experts:

I have this log file that has about 1200 characters (max) on a line. What I
want to do is read this first and then extract certain portions of the file
into new columns. I want to extract rows that contain the text ?[DF_API:
input string]?. When I read it and then filter based on the rows that I am
interested, it almost seems like I am losing data. I tried this using the
dplyr filter and using standard grep with the same result.

Not sure why this is the case. Appreciate your help with this. The code and
the data is there at the following link. Satish

Code is given below

library(dplyr)
setwd("C:/Users/satis/Documents/VF/df_issue_dec01")

sec1 <- read.delim(file="secondary1_aa_small.log")
head(sec1)
names(sec1) <- c("V1")
sec1_test <- filter(sec1,str_detect(V1,"DF_API: input string")==TRUE)
head(sec1_test)

sec1_test2 = sec1[grep("DF_API: input string",sec1$V1, perl = TRUE),]
head(sec1_test2)

write.csv(sec1_test, file = "test_out.txt", row.names = F, quote = F)
write.csv(sec1_test2, file = "test2_out.txt", row.names = F, quote = F)

Data (and code) is given at the link below. Sorry, I should have used dput.

https://spaces.hightail.com/space/arJlYkgIev


Satish Vadlamani

	[[alternative HTML version deleted]]


From @@r@h@go@lee @ending from gm@il@com  Wed Dec 19 21:56:44 2018
From: @@r@h@go@lee @ending from gm@il@com (Sarah Goslee)
Date: Wed, 19 Dec 2018 15:56:44 -0500
Subject: [R] Filtering data with dplyr or grep and losing data?
In-Reply-To: <CAK3+11TE70XxxnaWwLs0oOuBw6HKBHkeFAv7kjoGovbgE7mKQw@mail.gmail.com>
References: <CAK3+11TE70XxxnaWwLs0oOuBw6HKBHkeFAv7kjoGovbgE7mKQw@mail.gmail.com>
Message-ID: <CAM_vjunk4LEab1RTi4FZ6GathH7Lyois=Tv+6_2r4+St59C03A@mail.gmail.com>

Hi,

What does, "it almost seems like I am losing data mean?

Are you losing data? If so, what rows are being excluded that you
think should be included?

There are 90 rows in the test file that meet your criterion, as far as
I can tell, and 90 rows in my R output.

So apparently "almost seems like I am losing data" means "I am not
losing data but am confused."

So we need to know more about what you are expecting but not getting.

Sarah

On Wed, Dec 19, 2018 at 3:21 PM Satish Vadlamani
<satish.vadlamani at gmail.com> wrote:
>
> Hello Experts:
>
> I have this log file that has about 1200 characters (max) on a line. What I
> want to do is read this first and then extract certain portions of the file
> into new columns. I want to extract rows that contain the text ?[DF_API:
> input string]?. When I read it and then filter based on the rows that I am
> interested, it almost seems like I am losing data. I tried this using the
> dplyr filter and using standard grep with the same result.
>
> Not sure why this is the case. Appreciate your help with this. The code and
> the data is there at the following link. Satish
>
> Code is given below
>
> library(dplyr)
> setwd("C:/Users/satis/Documents/VF/df_issue_dec01")
>
> sec1 <- read.delim(file="secondary1_aa_small.log")
> head(sec1)
> names(sec1) <- c("V1")
> sec1_test <- filter(sec1,str_detect(V1,"DF_API: input string")==TRUE)
> head(sec1_test)
>
> sec1_test2 = sec1[grep("DF_API: input string",sec1$V1, perl = TRUE),]
> head(sec1_test2)
>
> write.csv(sec1_test, file = "test_out.txt", row.names = F, quote = F)
> write.csv(sec1_test2, file = "test2_out.txt", row.names = F, quote = F)
>
> Data (and code) is given at the link below. Sorry, I should have used dput.
>
> https://spaces.hightail.com/space/arJlYkgIev
>
>
> Satish Vadlamani
>


-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From e@@wiek @ending from gm@il@com  Thu Dec 20 02:13:10 2018
From: e@@wiek @ending from gm@il@com (Ek Esawi)
Date: Wed, 19 Dec 2018 20:13:10 -0500
Subject: [R] Combine recursive lists in a single list or data frame and
 write it to file
Message-ID: <CA+ZkTxuOGp__prbkn9xtr4yyXUcYdo=e4WHTQP7RZL17H_2tgw@mail.gmail.com>

Hi All?

 I am using the R tabulizer package to extract tables from pdf files.
The output is a set of lists of matrices. The package extracts tables
and a lot of extra stuff which is nearly impossible to clean with
RegEx. So, I want to clean it manually.
To do so I need to (1) combine all lists in a single list or data
frame and (2) then write the single entity to a text file to edit it.
I could not figure out how.

I tried something like this but did not work.
lapply(MyTables, function(x)
lapply(x,write.table(file="temp.txt",append = TRUE)))

 Any help is greatly appreciated.

 Here is my code:

install.packages("rJava")    ;library(rJava)
install.packages("tabulizer");library(tabulizer)
MyPath <- "C:/Users/name/Documents/tEMP"
ExtTable <- function (Path,CalOrd){
  FileNames <- dir(Path, pattern =".(pdf|PDF)",full.names = TRUE)
  MyFiles <- lapply(FileNames, function(i) extract_tables(i,method = "stream"))
  if(CalOrd == "Yes"){
    MyOFiles <- gsub("(\\s.*)|(.pdf|.PDF)","",basename(FileNames))
    MyOFiles <- match(MyOFiles,month.name)
    MyNFiles <- MyFiles[order(MyOFiles)]}
  else
    MyFiles
}
MyTables <- ExtTable(Path=MyPath,CalOrd = "No")

Here is cleaned portion of the output: The whole output consists of 3
lists, each contains 12, 15, and 12 sub-lists.

 [[2]][[2]]
 [,1]        [,2]    [,3]    [,4]  [,5]    [,6]    [,7]    [,8]    [,9]  [,10]
 [1,] ""          "Avg."  "+_ lo" "n"   "Med."  ""      "Avg."  "+_
lo" "n"   "Med."
 [2,] "SiOz"      "44.0"  "1.26"  "375" "44.1"  "Nb"    "4.8"   "6.3"
 "58"  "2.7"
 [3,] "T i O  2"  "0.09"  "0.09"  "561" "0.09"  "Mo(b)" "50"    "30"
 "3"   "35"
 [4,] "A1203"     "2.27"  "1.10"  "375" "2.20"  "Ru(b)" "12.4"  "4.1"
 "3"   "12"
 [5,] "FeO total" "8.43"  "1.14"  "375" "8.19"  "Pd(b)" "3.9"   "2.1"
 "19"  "4.1"
 [6,] "MnO"       "0.14"  "0.03"  "366" "0.14"  "Ag(b)" "6.8"   "8.3"
 "17"  "4.8"
 [7,] "MgO"       "41.4"  "3.00"  "375" "41.2"  "Cd(b)" "41"    "14"
 "16"  "37"
 [8,] "CaO"       "2.15"  "1.11"  "374" "2.20"  "In(b)" "12"    "4"
 "19"  "12"
 [9,] "Na20"      "0.24"  "0.16"  "341" "0.21"  "Sn(b)" "54"    "31"
 "6"   "36"
[10,] "K20"       "0.054" "0.11"  "330" "0.028" "Sb(b)" "3.9"   "3.9"
 "11"  "3.2"
[11,] "P205"      "0.056" "0.11"  "233" "0.030" "Te(b)" "11"    "4"
 "18"  "10"
[12,] "Total"     "98.88" ""      ""    "98.43" "Cs(b)" "10"    "16"
 "17"  "1.5"
[13,] ""          ""      ""      ""    ""      "Ba"    "33"    "52"
 "75"  "17"
[14,] "Mg-value"  "89.8"  "1.1"   "375" "90.0"  "La"    "2.60"  "5.70"
 "208" "0.77"
[15,] "Ca/AI"     "1.28"  "1.6"   "374" "1.35"  "Ce"    "6.29"  "11.7"
 "197" "2.08"
[16,] "AI/Ti"     "22"    "29"    "361" "22"    "Pr"    "0.56"  "0.87"
 "40"  "0.21"
[17,] "F e / M n" "60"    "10"    "366" "59"    "Nd"    "2.67"  "4.31"
 "162" "1.52"
[18,] ""          ""      ""      ""    ""      "Sm"    "0.47"  "0.69"
 "214" "0.25"
[19,] "Li"        "1.5"   "0.3"   "6"   "1.5"   "Eu"    "0.16"  "0.21"
 "201" "0.097"
[20,] "B"         "0.53"  "0.07"  "6"   "0.55"  "Gd"    "0.60"  "0.83"
 "67"  "0.31"
[21,] "C"         "110"   "50"    "13"  "93"    "Tb"    "0.070"
"0.064" "146" "0.056"
[22,] "F"         "88"    "71"    "15"  "100"   "Dy"    "0.51"  "0.35"
 "58"  "0.47"
[23,] "S"         "157"   "77"    "22"  "152"   "Ho"    "0.12"  "0.14"
 "54"  "0.090"
[24,] "C1"        "53"    "45"    "15"  "75"    "Er"    "0.30"  "0.22"
 "52"  "0.28"
[25,] "Sc"        "12.2"  "6.4"   "220" "12.0"  "Tm"    "0.038"
"0.026" "40"  "0.035"
[26,] "V"         "56"    "21"    "132" "53"    "Yb"    "0.26"  "0.14"
 "201" "0.27"
[27,] "Cr"        "2690"  "705"   "325" "2690"  "Lu"    "0.043"
"0.023" "172" "0.045"
[28,] "Co"        "112"   "10"    "166" "111"   "Hf"    "0.27"  "0.30"
 "71"  "0.17"
[29,] "Ni"        "2160"  "304"   "308" "2140"  "Ta"    "0.40"  "0.51"
 "38"  "0.23"
[30,] "Cu"        "11"    "9"     "94"  "9"     "W(b)"  "7.2"   "5.2"
 "6"   "4.0"
[31,] "Zn"        "65"    "20"    "129" "60"    "Re(b)" "0.13"  "0.11"
 "18"  "0.09"
[32,] "Ga"        "2.4"   "1.3"   "49"  "2.4"   "Os(b)" "4.0"   "1.8"
 "18"  "3.7"
[33,] "Ge"        "0.96"  "0.19"  "19"  "0.92"  "Ir(b)" "3.7"   "0.9"
 "34"  "3.0"
[34,] "As"        "0.11"  "0.07"  "7"   "0.10"  "Pt(b)" "7"     "-"
 "1"   "-"
[35,] "Se"        "0.041" "0.056" "18"  "0.025" "Au(b)" "0.65"  "0.53"
 "30"  "0.5"
[36,] "Br"        "0.01"  "0.01"  "6"   "0.01"  "Tl(b)" "1.2"   "1.0"
 "13"  "0.9"
[37,] "Rb"        "1,9"   "4.8"   "97"  "0.38"  "Pb"    "0.16"  "0.11"
 "17"  "0.16"
[38,] "Sr"        "49"    "60"    "110" "20"    "Bi(b)" "1.7"   "0.7"
 "13"  "1.6"
[39,] "Y"         "4.4"   "5.5"   "86"  "3.1"   "Th*"   "0.71"  "1.2"
 "71"  "0.22"
[40,] "Zr"        "21"    "42"    "82"  "8.0"   "U"     "0.12"  "0.23"
 "48"  "0.040"
[[2]][[4]]
[,1]       [,2]                 [,3]     [,4]                  [,5]
 [,6]
 [1,] ""         "Spinel peridotites" ""       "Garnet  peridotites"
""       "Primitive"
 [2,] ""         "Avg. Meal."         "M-A sp" "M-A gt B-M"
"Jordan" "mantle"
 [3,] "SiO 2"    "44.0 44.1"          "44.15"  "44.99 45.00"
"45.55"  "44.8"
 [4,] "TiO 2"    "0.09 0.09"          "0.07"   "0.06 0.08"
"0.11"   "0.21"
 [5,] "A1203"    "2.27 2.20"          "1.96"   "1.40 1.31"
"1.43"   "4.45"
 [6,] "Cr203"    "0.39 0.39"          "0.44"   "0.32 0.38"
"0.34"   "0.43"
 [7,] "FeOtotal" "8.43 8.19"          "8.28"   "7.89 6.97"
"7.61"   "8.40"
 [8,] "Mn O"     "0.14 0.14"          "0.12"   "0.11 0.13"
"0.11"   "0.14"
 [9,] "MgO"      "41.4 41.2"          "42.25"  "42.60 44.86"
"43.55"  "37.2"
[10,] "NiO"      "0.27 0.27"          "0.27"   "0.26 0.29"
"-"      "0.24"
[11,] "CaO"      "2.15 2.20"          "2.08"   "0.82 0.77"
"1.05"   "3.60"
[12,] "Na  20"   "0.24 0.21"          "0.18"   "0.11 0.09"
"0.14"   "0.34"
[13,] "K 2 0"    "0.054 0.028"        "0.05"   "0.04 0.10"
"0.11"   "0.028"
[14,] "P205"     "0.056 0.030"        "0.02"   "- 0.01"
"-"      "0.022"
[15,] "Total"    "99.49 99.05"        "99.87"  "98.60 100.00"
"100.00" "99.86"
[16,] "Mg-value" "89.8 90.0"          "90.1"   "90.6 92.0"
"91.1"   "88.8"
[17,] "olivine"  "62 63"              "67"     "65 68"
"66"     "56 57"
[18,] "opx"      "24 24"              "22"     "28 25"
"28"     "22 17"
[19,] "cpx"      "12 11"              "9"      "3 2"
"3"      "19 10"
[20,] "spinel"   "2 2"                "2"      "- -"
"-"      "3 -"

Here is portion of the output for str(MyTables):

str(MyTables)

List of 3
 $ :List of 12
$ : chr [1:3, 1:2] "south of the artificial lake Lokka. Intrusive
complexes" "of alkaline rocks are found at Sokli (phosphorite-bear-"
"ing and a possible Nb-occurrence) in Finland, and at" "(Eriksson,
1992). During this period, Northern Europe" ...
  ..$ : chr [1:55, 1:15] "Element" "Ag" "Al" "Al_XRF" ...
  ..$ : chr [1:56, 1:2] "in the till is mainly of local origin,
although some cob-" "bles and boulders may have been transported over
sev-" "eral kilometres. The moraine formations in the study" "area are
mostly gravelly and sandy tills, locally hum-" ...
  ..$ : chr [1:53, 1:2] "requisites. PCA accounts for maximum variance
of all" "variables, while FA is based on the correlation structure"
"of the variables. The model of factor analysis allows that" "the
common factors do not explain the total variation of" ...
  ..$ : chr [1:54, 1:7] "lished examples of the use of factor
analysis, it is neglec-" "ted that regional geochemical (and
environmental) data" "almost never follow a normal distribution.
Continuing Method" "with factor analysis in such a case must lead to
biased" ...
  ..$ : chr [1:16, 1:2] "shows the factor loadings of the different
variables" "entering each factor. Names of variables with an abso-"
"lute value of the loadings <0.3 are not plotted. Fig. 5" "shows 8
results of factor analyses using a selection of all" ...
  ..$ : chr [1:21, 1:2] "pretable results, notwithstanding the fact
that on the" "basis of the foregoing discussion it should probably
not" "be used with these data. Do these results warrant the use" "of a
quite work-intensive method? Unfortunately not," ...
  ..$ : chr [1:55, 1:8] "" "Ag" "Al" "Al_XRF" ...
  ..$ : chr [1:23, 1:2] "addition, geochemical reasoning (e.g.
geochemical asso-" "ciations and/or pathfinder elements for different
types of" "ore deposits) was used to select further sub-sets of vari-"
"ables. In geochemistry, the selection of elements entered" ...
  ..$ : chr [1:55, 1:2] "Fig. 10C cuts several geological units, and
is most likely" "indicative of alteration processes related to a
deep-" "seated fault. It was revealed again in a factor analysis"
"carried out with all those elements extracted by aqua" ...
  ..$ : chr [1:50, 1:2] "well justified in stating that it is not very
scientific to" "play with the selection of elements and number of
fac-" "tors extracted until one
?\200\230?\200\230finds?\200\231?\200\231 an
?\200\230?\200\230interesting?\200\231?\200\231 result." "On the other
hand, even all the different results pre-" ...
  ..$ : chr [1:24, 1:2] "Niemel??, J., Ekman, I., Lukashov, A. (Eds.),
1993. Quaternary" "Deposits of Finland and Northwestern Part of
Russian Fed-" "eration and Their Resources 1:1,000,000. Geological
Survey" "of Finland, Espoo, Finland." ...
 $ :List of 15


From mi@ojpm @ending from gm@il@com  Thu Dec 20 02:19:09 2018
From: mi@ojpm @ending from gm@il@com (John)
Date: Thu, 20 Dec 2018 09:19:09 +0800
Subject: [R] geom_ribbon function in ggplot2 package
In-Reply-To: <b0305007-3639-34bc-574d-c5fa1ca74e96@uke.de>
References: <CABcx46DkoZd2ky3dmLn-3398GMJYkP9uka186cY0QtLHkaV+PQ@mail.gmail.com>
 <b0305007-3639-34bc-574d-c5fa1ca74e96@uke.de>
Message-ID: <CABcx46A_fJeC-mQjxmtdeE69Rf6ci358id_7k9C0J9xFb=Y=vQ@mail.gmail.com>

Thanks, Eik!

Eik Vettorazzi <E.Vettorazzi at uke.de> ? 2018?12?19? ?? ??5:12???

> Hi,
> just add +scale_fill_discrete(name=NULL)
>
> Cheers
>
> Am 19.12.2018 um 07:05 schrieb John:
> > Hi,
> >
> >     When using the geom_ribbon function in gglot2 package, I got the text
> > "fill" above the legend "A" and "B". How can I get rid of the text "fill"
> > above the legend?
> >
> >     Thanks!
> >
> > The code is as follows:
> >
> >
> > df<-data.frame(x=c(1,2), y=c(1,2), z=c(3,5))
> >> ggplot(df,
> >
> aes(1:2))+geom_ribbon(aes(ymin=0,ymax=df[,"y"],fill="A"),alpha=0.5)+geom_ribbon(aes(ymin=0,ymax=df[,"z"],fill="B"),alpha=0.5)
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> --
> Eik Vettorazzi
>
> Department of Medical Biometry and Epidemiology
> University Medical Center Hamburg-Eppendorf
>
> Martinistrasse 52
> building W 34
> 20246 Hamburg
>
> Phone: +49 (0) 40 7410 - 58243
> Fax:   +49 (0) 40 7410 - 57790
> Web: www.uke.de/imbe
> --
>
> _____________________________________________________________________
>
> Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen
> Rechts; Gerichtsstand: Hamburg | www.uke.de
> Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr.
> Uwe Koch-Gromus, Joachim Pr?l?, Marya Verdel
> _____________________________________________________________________
>
> SAVE PAPER - THINK BEFORE PRINTING
>

	[[alternative HTML version deleted]]


From bgunter@4567 @ending from gm@il@com  Thu Dec 20 03:10:33 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Wed, 19 Dec 2018 18:10:33 -0800
Subject: [R] Combine recursive lists in a single list or data frame and
 write it to file
In-Reply-To: <CA+ZkTxuOGp__prbkn9xtr4yyXUcYdo=e4WHTQP7RZL17H_2tgw@mail.gmail.com>
References: <CA+ZkTxuOGp__prbkn9xtr4yyXUcYdo=e4WHTQP7RZL17H_2tgw@mail.gmail.com>
Message-ID: <CAGxFJbQBP8JQXEpQEmpU_PedQLvJLRrhs5SpSAvVVp1iLqQN+A@mail.gmail.com>

Does ?unlist not help? Why not?

Bert


On Wed, Dec 19, 2018, 5:13 PM Ek Esawi <esawiek at gmail.com wrote:

> Hi All?
>
>  I am using the R tabulizer package to extract tables from pdf files.
> The output is a set of lists of matrices. The package extracts tables
> and a lot of extra stuff which is nearly impossible to clean with
> RegEx. So, I want to clean it manually.
> To do so I need to (1) combine all lists in a single list or data
> frame and (2) then write the single entity to a text file to edit it.
> I could not figure out how.
>
> I tried something like this but did not work.
> lapply(MyTables, function(x)
> lapply(x,write.table(file="temp.txt",append = TRUE)))
>
>  Any help is greatly appreciated.
>
>  Here is my code:
>
> install.packages("rJava")    ;library(rJava)
> install.packages("tabulizer");library(tabulizer)
> MyPath <- "C:/Users/name/Documents/tEMP"
> ExtTable <- function (Path,CalOrd){
>   FileNames <- dir(Path, pattern =".(pdf|PDF)",full.names = TRUE)
>   MyFiles <- lapply(FileNames, function(i) extract_tables(i,method =
> "stream"))
>   if(CalOrd == "Yes"){
>     MyOFiles <- gsub("(\\s.*)|(.pdf|.PDF)","",basename(FileNames))
>     MyOFiles <- match(MyOFiles,month.name)
>     MyNFiles <- MyFiles[order(MyOFiles)]}
>   else
>     MyFiles
> }
> MyTables <- ExtTable(Path=MyPath,CalOrd = "No")
>
> Here is cleaned portion of the output: The whole output consists of 3
> lists, each contains 12, 15, and 12 sub-lists.
>
>  [[2]][[2]]
>  [,1]        [,2]    [,3]    [,4]  [,5]    [,6]    [,7]    [,8]    [,9]
> [,10]
>  [1,] ""          "Avg."  "+_ lo" "n"   "Med."  ""      "Avg."  "+_
> lo" "n"   "Med."
>  [2,] "SiOz"      "44.0"  "1.26"  "375" "44.1"  "Nb"    "4.8"   "6.3"
>  "58"  "2.7"
>  [3,] "T i O  2"  "0.09"  "0.09"  "561" "0.09"  "Mo(b)" "50"    "30"
>  "3"   "35"
>  [4,] "A1203"     "2.27"  "1.10"  "375" "2.20"  "Ru(b)" "12.4"  "4.1"
>  "3"   "12"
>  [5,] "FeO total" "8.43"  "1.14"  "375" "8.19"  "Pd(b)" "3.9"   "2.1"
>  "19"  "4.1"
>  [6,] "MnO"       "0.14"  "0.03"  "366" "0.14"  "Ag(b)" "6.8"   "8.3"
>  "17"  "4.8"
>  [7,] "MgO"       "41.4"  "3.00"  "375" "41.2"  "Cd(b)" "41"    "14"
>  "16"  "37"
>  [8,] "CaO"       "2.15"  "1.11"  "374" "2.20"  "In(b)" "12"    "4"
>  "19"  "12"
>  [9,] "Na20"      "0.24"  "0.16"  "341" "0.21"  "Sn(b)" "54"    "31"
>  "6"   "36"
> [10,] "K20"       "0.054" "0.11"  "330" "0.028" "Sb(b)" "3.9"   "3.9"
>  "11"  "3.2"
> [11,] "P205"      "0.056" "0.11"  "233" "0.030" "Te(b)" "11"    "4"
>  "18"  "10"
> [12,] "Total"     "98.88" ""      ""    "98.43" "Cs(b)" "10"    "16"
>  "17"  "1.5"
> [13,] ""          ""      ""      ""    ""      "Ba"    "33"    "52"
>  "75"  "17"
> [14,] "Mg-value"  "89.8"  "1.1"   "375" "90.0"  "La"    "2.60"  "5.70"
>  "208" "0.77"
> [15,] "Ca/AI"     "1.28"  "1.6"   "374" "1.35"  "Ce"    "6.29"  "11.7"
>  "197" "2.08"
> [16,] "AI/Ti"     "22"    "29"    "361" "22"    "Pr"    "0.56"  "0.87"
>  "40"  "0.21"
> [17,] "F e / M n" "60"    "10"    "366" "59"    "Nd"    "2.67"  "4.31"
>  "162" "1.52"
> [18,] ""          ""      ""      ""    ""      "Sm"    "0.47"  "0.69"
>  "214" "0.25"
> [19,] "Li"        "1.5"   "0.3"   "6"   "1.5"   "Eu"    "0.16"  "0.21"
>  "201" "0.097"
> [20,] "B"         "0.53"  "0.07"  "6"   "0.55"  "Gd"    "0.60"  "0.83"
>  "67"  "0.31"
> [21,] "C"         "110"   "50"    "13"  "93"    "Tb"    "0.070"
> "0.064" "146" "0.056"
> [22,] "F"         "88"    "71"    "15"  "100"   "Dy"    "0.51"  "0.35"
>  "58"  "0.47"
> [23,] "S"         "157"   "77"    "22"  "152"   "Ho"    "0.12"  "0.14"
>  "54"  "0.090"
> [24,] "C1"        "53"    "45"    "15"  "75"    "Er"    "0.30"  "0.22"
>  "52"  "0.28"
> [25,] "Sc"        "12.2"  "6.4"   "220" "12.0"  "Tm"    "0.038"
> "0.026" "40"  "0.035"
> [26,] "V"         "56"    "21"    "132" "53"    "Yb"    "0.26"  "0.14"
>  "201" "0.27"
> [27,] "Cr"        "2690"  "705"   "325" "2690"  "Lu"    "0.043"
> "0.023" "172" "0.045"
> [28,] "Co"        "112"   "10"    "166" "111"   "Hf"    "0.27"  "0.30"
>  "71"  "0.17"
> [29,] "Ni"        "2160"  "304"   "308" "2140"  "Ta"    "0.40"  "0.51"
>  "38"  "0.23"
> [30,] "Cu"        "11"    "9"     "94"  "9"     "W(b)"  "7.2"   "5.2"
>  "6"   "4.0"
> [31,] "Zn"        "65"    "20"    "129" "60"    "Re(b)" "0.13"  "0.11"
>  "18"  "0.09"
> [32,] "Ga"        "2.4"   "1.3"   "49"  "2.4"   "Os(b)" "4.0"   "1.8"
>  "18"  "3.7"
> [33,] "Ge"        "0.96"  "0.19"  "19"  "0.92"  "Ir(b)" "3.7"   "0.9"
>  "34"  "3.0"
> [34,] "As"        "0.11"  "0.07"  "7"   "0.10"  "Pt(b)" "7"     "-"
>  "1"   "-"
> [35,] "Se"        "0.041" "0.056" "18"  "0.025" "Au(b)" "0.65"  "0.53"
>  "30"  "0.5"
> [36,] "Br"        "0.01"  "0.01"  "6"   "0.01"  "Tl(b)" "1.2"   "1.0"
>  "13"  "0.9"
> [37,] "Rb"        "1,9"   "4.8"   "97"  "0.38"  "Pb"    "0.16"  "0.11"
>  "17"  "0.16"
> [38,] "Sr"        "49"    "60"    "110" "20"    "Bi(b)" "1.7"   "0.7"
>  "13"  "1.6"
> [39,] "Y"         "4.4"   "5.5"   "86"  "3.1"   "Th*"   "0.71"  "1.2"
>  "71"  "0.22"
> [40,] "Zr"        "21"    "42"    "82"  "8.0"   "U"     "0.12"  "0.23"
>  "48"  "0.040"
> [[2]][[4]]
> [,1]       [,2]                 [,3]     [,4]                  [,5]
>  [,6]
>  [1,] ""         "Spinel peridotites" ""       "Garnet  peridotites"
> ""       "Primitive"
>  [2,] ""         "Avg. Meal."         "M-A sp" "M-A gt B-M"
> "Jordan" "mantle"
>  [3,] "SiO 2"    "44.0 44.1"          "44.15"  "44.99 45.00"
> "45.55"  "44.8"
>  [4,] "TiO 2"    "0.09 0.09"          "0.07"   "0.06 0.08"
> "0.11"   "0.21"
>  [5,] "A1203"    "2.27 2.20"          "1.96"   "1.40 1.31"
> "1.43"   "4.45"
>  [6,] "Cr203"    "0.39 0.39"          "0.44"   "0.32 0.38"
> "0.34"   "0.43"
>  [7,] "FeOtotal" "8.43 8.19"          "8.28"   "7.89 6.97"
> "7.61"   "8.40"
>  [8,] "Mn O"     "0.14 0.14"          "0.12"   "0.11 0.13"
> "0.11"   "0.14"
>  [9,] "MgO"      "41.4 41.2"          "42.25"  "42.60 44.86"
> "43.55"  "37.2"
> [10,] "NiO"      "0.27 0.27"          "0.27"   "0.26 0.29"
> "-"      "0.24"
> [11,] "CaO"      "2.15 2.20"          "2.08"   "0.82 0.77"
> "1.05"   "3.60"
> [12,] "Na  20"   "0.24 0.21"          "0.18"   "0.11 0.09"
> "0.14"   "0.34"
> [13,] "K 2 0"    "0.054 0.028"        "0.05"   "0.04 0.10"
> "0.11"   "0.028"
> [14,] "P205"     "0.056 0.030"        "0.02"   "- 0.01"
> "-"      "0.022"
> [15,] "Total"    "99.49 99.05"        "99.87"  "98.60 100.00"
> "100.00" "99.86"
> [16,] "Mg-value" "89.8 90.0"          "90.1"   "90.6 92.0"
> "91.1"   "88.8"
> [17,] "olivine"  "62 63"              "67"     "65 68"
> "66"     "56 57"
> [18,] "opx"      "24 24"              "22"     "28 25"
> "28"     "22 17"
> [19,] "cpx"      "12 11"              "9"      "3 2"
> "3"      "19 10"
> [20,] "spinel"   "2 2"                "2"      "- -"
> "-"      "3 -"
>
> Here is portion of the output for str(MyTables):
>
> str(MyTables)
>
> List of 3
>  $ :List of 12
> $ : chr [1:3, 1:2] "south of the artificial lake Lokka. Intrusive
> complexes" "of alkaline rocks are found at Sokli (phosphorite-bear-"
> "ing and a possible Nb-occurrence) in Finland, and at" "(Eriksson,
> 1992). During this period, Northern Europe" ...
>   ..$ : chr [1:55, 1:15] "Element" "Ag" "Al" "Al_XRF" ...
>   ..$ : chr [1:56, 1:2] "in the till is mainly of local origin,
> although some cob-" "bles and boulders may have been transported over
> sev-" "eral kilometres. The moraine formations in the study" "area are
> mostly gravelly and sandy tills, locally hum-" ...
>   ..$ : chr [1:53, 1:2] "requisites. PCA accounts for maximum variance
> of all" "variables, while FA is based on the correlation structure"
> "of the variables. The model of factor analysis allows that" "the
> common factors do not explain the total variation of" ...
>   ..$ : chr [1:54, 1:7] "lished examples of the use of factor
> analysis, it is neglec-" "ted that regional geochemical (and
> environmental) data" "almost never follow a normal distribution.
> Continuing Method" "with factor analysis in such a case must lead to
> biased" ...
>   ..$ : chr [1:16, 1:2] "shows the factor loadings of the different
> variables" "entering each factor. Names of variables with an abso-"
> "lute value of the loadings <0.3 are not plotted. Fig. 5" "shows 8
> results of factor analyses using a selection of all" ...
>   ..$ : chr [1:21, 1:2] "pretable results, notwithstanding the fact
> that on the" "basis of the foregoing discussion it should probably
> not" "be used with these data. Do these results warrant the use" "of a
> quite work-intensive method? Unfortunately not," ...
>   ..$ : chr [1:55, 1:8] "" "Ag" "Al" "Al_XRF" ...
>   ..$ : chr [1:23, 1:2] "addition, geochemical reasoning (e.g.
> geochemical asso-" "ciations and/or pathfinder elements for different
> types of" "ore deposits) was used to select further sub-sets of vari-"
> "ables. In geochemistry, the selection of elements entered" ...
>   ..$ : chr [1:55, 1:2] "Fig. 10C cuts several geological units, and
> is most likely" "indicative of alteration processes related to a
> deep-" "seated fault. It was revealed again in a factor analysis"
> "carried out with all those elements extracted by aqua" ...
>   ..$ : chr [1:50, 1:2] "well justified in stating that it is not very
> scientific to" "play with the selection of elements and number of
> fac-" "tors extracted until one
> ?\200\230?\200\230finds?\200\231?\200\231 an
> ?\200\230?\200\230interesting?\200\231?\200\231 result." "On the other
> hand, even all the different results pre-" ...
>   ..$ : chr [1:24, 1:2] "Niemel??, J., Ekman, I., Lukashov, A. (Eds.),
> 1993. Quaternary" "Deposits of Finland and Northwestern Part of
> Russian Fed-" "eration and Their Resources 1:1,000,000. Geological
> Survey" "of Finland, Espoo, Finland." ...
>  $ :List of 15
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From e@@wiek @ending from gm@il@com  Thu Dec 20 03:32:54 2018
From: e@@wiek @ending from gm@il@com (Ek Esawi)
Date: Wed, 19 Dec 2018 21:32:54 -0500
Subject: [R] Combine recursive lists in a single list or data frame and
 write it to file
In-Reply-To: <CAGxFJbQBP8JQXEpQEmpU_PedQLvJLRrhs5SpSAvVVp1iLqQN+A@mail.gmail.com>
References: <CA+ZkTxuOGp__prbkn9xtr4yyXUcYdo=e4WHTQP7RZL17H_2tgw@mail.gmail.com>
 <CAGxFJbQBP8JQXEpQEmpU_PedQLvJLRrhs5SpSAvVVp1iLqQN+A@mail.gmail.com>
Message-ID: <CA+ZkTxuVoFdd9dvR1S1YY5Lec-bDU=_mx11pc8cJ9fd6i54A4w@mail.gmail.com>

Thank you Bert. I don't see how unlist will help. I want to combine
them but keep the "rectangular structure",e.g. list, data frame,
matrix  because i want to get the tables in their original form.
Unlist converts the whole output to a single vector; unless i am
missing something.

On Wed, Dec 19, 2018 at 9:10 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
> Does ?unlist not help? Why not?
>
> Bert
>
>
> On Wed, Dec 19, 2018, 5:13 PM Ek Esawi <esawiek at gmail.com wrote:
>>
>> Hi All?
>>
>>  I am using the R tabulizer package to extract tables from pdf files.
>> The output is a set of lists of matrices. The package extracts tables
>> and a lot of extra stuff which is nearly impossible to clean with
>> RegEx. So, I want to clean it manually.
>> To do so I need to (1) combine all lists in a single list or data
>> frame and (2) then write the single entity to a text file to edit it.
>> I could not figure out how.
>>
>> I tried something like this but did not work.
>> lapply(MyTables, function(x)
>> lapply(x,write.table(file="temp.txt",append = TRUE)))
>>
>>  Any help is greatly appreciated.
>>
>>  Here is my code:
>>
>> install.packages("rJava")    ;library(rJava)
>> install.packages("tabulizer");library(tabulizer)
>> MyPath <- "C:/Users/name/Documents/tEMP"
>> ExtTable <- function (Path,CalOrd){
>>   FileNames <- dir(Path, pattern =".(pdf|PDF)",full.names = TRUE)
>>   MyFiles <- lapply(FileNames, function(i) extract_tables(i,method = "stream"))
>>   if(CalOrd == "Yes"){
>>     MyOFiles <- gsub("(\\s.*)|(.pdf|.PDF)","",basename(FileNames))
>>     MyOFiles <- match(MyOFiles,month.name)
>>     MyNFiles <- MyFiles[order(MyOFiles)]}
>>   else
>>     MyFiles
>> }
>> MyTables <- ExtTable(Path=MyPath,CalOrd = "No")
>>
>> Here is cleaned portion of the output: The whole output consists of 3
>> lists, each contains 12, 15, and 12 sub-lists.
>>
>>  [[2]][[2]]
>>  [,1]        [,2]    [,3]    [,4]  [,5]    [,6]    [,7]    [,8]    [,9]  [,10]
>>  [1,] ""          "Avg."  "+_ lo" "n"   "Med."  ""      "Avg."  "+_
>> lo" "n"   "Med."
>>  [2,] "SiOz"      "44.0"  "1.26"  "375" "44.1"  "Nb"    "4.8"   "6.3"
>>  "58"  "2.7"
>>  [3,] "T i O  2"  "0.09"  "0.09"  "561" "0.09"  "Mo(b)" "50"    "30"
>>  "3"   "35"
>>  [4,] "A1203"     "2.27"  "1.10"  "375" "2.20"  "Ru(b)" "12.4"  "4.1"
>>  "3"   "12"
>>  [5,] "FeO total" "8.43"  "1.14"  "375" "8.19"  "Pd(b)" "3.9"   "2.1"
>>  "19"  "4.1"
>>  [6,] "MnO"       "0.14"  "0.03"  "366" "0.14"  "Ag(b)" "6.8"   "8.3"
>>  "17"  "4.8"
>>  [7,] "MgO"       "41.4"  "3.00"  "375" "41.2"  "Cd(b)" "41"    "14"
>>  "16"  "37"
>>  [8,] "CaO"       "2.15"  "1.11"  "374" "2.20"  "In(b)" "12"    "4"
>>  "19"  "12"
>>  [9,] "Na20"      "0.24"  "0.16"  "341" "0.21"  "Sn(b)" "54"    "31"
>>  "6"   "36"
>> [10,] "K20"       "0.054" "0.11"  "330" "0.028" "Sb(b)" "3.9"   "3.9"
>>  "11"  "3.2"
>> [11,] "P205"      "0.056" "0.11"  "233" "0.030" "Te(b)" "11"    "4"
>>  "18"  "10"
>> [12,] "Total"     "98.88" ""      ""    "98.43" "Cs(b)" "10"    "16"
>>  "17"  "1.5"
>> [13,] ""          ""      ""      ""    ""      "Ba"    "33"    "52"
>>  "75"  "17"
>> [14,] "Mg-value"  "89.8"  "1.1"   "375" "90.0"  "La"    "2.60"  "5.70"
>>  "208" "0.77"
>> [15,] "Ca/AI"     "1.28"  "1.6"   "374" "1.35"  "Ce"    "6.29"  "11.7"
>>  "197" "2.08"
>> [16,] "AI/Ti"     "22"    "29"    "361" "22"    "Pr"    "0.56"  "0.87"
>>  "40"  "0.21"
>> [17,] "F e / M n" "60"    "10"    "366" "59"    "Nd"    "2.67"  "4.31"
>>  "162" "1.52"
>> [18,] ""          ""      ""      ""    ""      "Sm"    "0.47"  "0.69"
>>  "214" "0.25"
>> [19,] "Li"        "1.5"   "0.3"   "6"   "1.5"   "Eu"    "0.16"  "0.21"
>>  "201" "0.097"
>> [20,] "B"         "0.53"  "0.07"  "6"   "0.55"  "Gd"    "0.60"  "0.83"
>>  "67"  "0.31"
>> [21,] "C"         "110"   "50"    "13"  "93"    "Tb"    "0.070"
>> "0.064" "146" "0.056"
>> [22,] "F"         "88"    "71"    "15"  "100"   "Dy"    "0.51"  "0.35"
>>  "58"  "0.47"
>> [23,] "S"         "157"   "77"    "22"  "152"   "Ho"    "0.12"  "0.14"
>>  "54"  "0.090"
>> [24,] "C1"        "53"    "45"    "15"  "75"    "Er"    "0.30"  "0.22"
>>  "52"  "0.28"
>> [25,] "Sc"        "12.2"  "6.4"   "220" "12.0"  "Tm"    "0.038"
>> "0.026" "40"  "0.035"
>> [26,] "V"         "56"    "21"    "132" "53"    "Yb"    "0.26"  "0.14"
>>  "201" "0.27"
>> [27,] "Cr"        "2690"  "705"   "325" "2690"  "Lu"    "0.043"
>> "0.023" "172" "0.045"
>> [28,] "Co"        "112"   "10"    "166" "111"   "Hf"    "0.27"  "0.30"
>>  "71"  "0.17"
>> [29,] "Ni"        "2160"  "304"   "308" "2140"  "Ta"    "0.40"  "0.51"
>>  "38"  "0.23"
>> [30,] "Cu"        "11"    "9"     "94"  "9"     "W(b)"  "7.2"   "5.2"
>>  "6"   "4.0"
>> [31,] "Zn"        "65"    "20"    "129" "60"    "Re(b)" "0.13"  "0.11"
>>  "18"  "0.09"
>> [32,] "Ga"        "2.4"   "1.3"   "49"  "2.4"   "Os(b)" "4.0"   "1.8"
>>  "18"  "3.7"
>> [33,] "Ge"        "0.96"  "0.19"  "19"  "0.92"  "Ir(b)" "3.7"   "0.9"
>>  "34"  "3.0"
>> [34,] "As"        "0.11"  "0.07"  "7"   "0.10"  "Pt(b)" "7"     "-"
>>  "1"   "-"
>> [35,] "Se"        "0.041" "0.056" "18"  "0.025" "Au(b)" "0.65"  "0.53"
>>  "30"  "0.5"
>> [36,] "Br"        "0.01"  "0.01"  "6"   "0.01"  "Tl(b)" "1.2"   "1.0"
>>  "13"  "0.9"
>> [37,] "Rb"        "1,9"   "4.8"   "97"  "0.38"  "Pb"    "0.16"  "0.11"
>>  "17"  "0.16"
>> [38,] "Sr"        "49"    "60"    "110" "20"    "Bi(b)" "1.7"   "0.7"
>>  "13"  "1.6"
>> [39,] "Y"         "4.4"   "5.5"   "86"  "3.1"   "Th*"   "0.71"  "1.2"
>>  "71"  "0.22"
>> [40,] "Zr"        "21"    "42"    "82"  "8.0"   "U"     "0.12"  "0.23"
>>  "48"  "0.040"
>> [[2]][[4]]
>> [,1]       [,2]                 [,3]     [,4]                  [,5]
>>  [,6]
>>  [1,] ""         "Spinel peridotites" ""       "Garnet  peridotites"
>> ""       "Primitive"
>>  [2,] ""         "Avg. Meal."         "M-A sp" "M-A gt B-M"
>> "Jordan" "mantle"
>>  [3,] "SiO 2"    "44.0 44.1"          "44.15"  "44.99 45.00"
>> "45.55"  "44.8"
>>  [4,] "TiO 2"    "0.09 0.09"          "0.07"   "0.06 0.08"
>> "0.11"   "0.21"
>>  [5,] "A1203"    "2.27 2.20"          "1.96"   "1.40 1.31"
>> "1.43"   "4.45"
>>  [6,] "Cr203"    "0.39 0.39"          "0.44"   "0.32 0.38"
>> "0.34"   "0.43"
>>  [7,] "FeOtotal" "8.43 8.19"          "8.28"   "7.89 6.97"
>> "7.61"   "8.40"
>>  [8,] "Mn O"     "0.14 0.14"          "0.12"   "0.11 0.13"
>> "0.11"   "0.14"
>>  [9,] "MgO"      "41.4 41.2"          "42.25"  "42.60 44.86"
>> "43.55"  "37.2"
>> [10,] "NiO"      "0.27 0.27"          "0.27"   "0.26 0.29"
>> "-"      "0.24"
>> [11,] "CaO"      "2.15 2.20"          "2.08"   "0.82 0.77"
>> "1.05"   "3.60"
>> [12,] "Na  20"   "0.24 0.21"          "0.18"   "0.11 0.09"
>> "0.14"   "0.34"
>> [13,] "K 2 0"    "0.054 0.028"        "0.05"   "0.04 0.10"
>> "0.11"   "0.028"
>> [14,] "P205"     "0.056 0.030"        "0.02"   "- 0.01"
>> "-"      "0.022"
>> [15,] "Total"    "99.49 99.05"        "99.87"  "98.60 100.00"
>> "100.00" "99.86"
>> [16,] "Mg-value" "89.8 90.0"          "90.1"   "90.6 92.0"
>> "91.1"   "88.8"
>> [17,] "olivine"  "62 63"              "67"     "65 68"
>> "66"     "56 57"
>> [18,] "opx"      "24 24"              "22"     "28 25"
>> "28"     "22 17"
>> [19,] "cpx"      "12 11"              "9"      "3 2"
>> "3"      "19 10"
>> [20,] "spinel"   "2 2"                "2"      "- -"
>> "-"      "3 -"
>>
>> Here is portion of the output for str(MyTables):
>>
>> str(MyTables)
>>
>> List of 3
>>  $ :List of 12
>> $ : chr [1:3, 1:2] "south of the artificial lake Lokka. Intrusive
>> complexes" "of alkaline rocks are found at Sokli (phosphorite-bear-"
>> "ing and a possible Nb-occurrence) in Finland, and at" "(Eriksson,
>> 1992). During this period, Northern Europe" ...
>>   ..$ : chr [1:55, 1:15] "Element" "Ag" "Al" "Al_XRF" ...
>>   ..$ : chr [1:56, 1:2] "in the till is mainly of local origin,
>> although some cob-" "bles and boulders may have been transported over
>> sev-" "eral kilometres. The moraine formations in the study" "area are
>> mostly gravelly and sandy tills, locally hum-" ...
>>   ..$ : chr [1:53, 1:2] "requisites. PCA accounts for maximum variance
>> of all" "variables, while FA is based on the correlation structure"
>> "of the variables. The model of factor analysis allows that" "the
>> common factors do not explain the total variation of" ...
>>   ..$ : chr [1:54, 1:7] "lished examples of the use of factor
>> analysis, it is neglec-" "ted that regional geochemical (and
>> environmental) data" "almost never follow a normal distribution.
>> Continuing Method" "with factor analysis in such a case must lead to
>> biased" ...
>>   ..$ : chr [1:16, 1:2] "shows the factor loadings of the different
>> variables" "entering each factor. Names of variables with an abso-"
>> "lute value of the loadings <0.3 are not plotted. Fig. 5" "shows 8
>> results of factor analyses using a selection of all" ...
>>   ..$ : chr [1:21, 1:2] "pretable results, notwithstanding the fact
>> that on the" "basis of the foregoing discussion it should probably
>> not" "be used with these data. Do these results warrant the use" "of a
>> quite work-intensive method? Unfortunately not," ...
>>   ..$ : chr [1:55, 1:8] "" "Ag" "Al" "Al_XRF" ...
>>   ..$ : chr [1:23, 1:2] "addition, geochemical reasoning (e.g.
>> geochemical asso-" "ciations and/or pathfinder elements for different
>> types of" "ore deposits) was used to select further sub-sets of vari-"
>> "ables. In geochemistry, the selection of elements entered" ...
>>   ..$ : chr [1:55, 1:2] "Fig. 10C cuts several geological units, and
>> is most likely" "indicative of alteration processes related to a
>> deep-" "seated fault. It was revealed again in a factor analysis"
>> "carried out with all those elements extracted by aqua" ...
>>   ..$ : chr [1:50, 1:2] "well justified in stating that it is not very
>> scientific to" "play with the selection of elements and number of
>> fac-" "tors extracted until one
>> ?\200\230?\200\230finds?\200\231?\200\231 an
>> ?\200\230?\200\230interesting?\200\231?\200\231 result." "On the other
>> hand, even all the different results pre-" ...
>>   ..$ : chr [1:24, 1:2] "Niemel??, J., Ekman, I., Lukashov, A. (Eds.),
>> 1993. Quaternary" "Deposits of Finland and Northwestern Part of
>> Russian Fed-" "eration and Their Resources 1:1,000,000. Geological
>> Survey" "of Finland, Espoo, Finland." ...
>>  $ :List of 15
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From drjimlemon @ending from gm@il@com  Thu Dec 20 03:35:56 2018
From: drjimlemon @ending from gm@il@com (Jim Lemon)
Date: Thu, 20 Dec 2018 13:35:56 +1100
Subject: [R] Combine recursive lists in a single list or data frame and
 write it to file
In-Reply-To: <CA+ZkTxuVoFdd9dvR1S1YY5Lec-bDU=_mx11pc8cJ9fd6i54A4w@mail.gmail.com>
References: <CA+ZkTxuOGp__prbkn9xtr4yyXUcYdo=e4WHTQP7RZL17H_2tgw@mail.gmail.com>
 <CAGxFJbQBP8JQXEpQEmpU_PedQLvJLRrhs5SpSAvVVp1iLqQN+A@mail.gmail.com>
 <CA+ZkTxuVoFdd9dvR1S1YY5Lec-bDU=_mx11pc8cJ9fd6i54A4w@mail.gmail.com>
Message-ID: <CA+8X3fUBZFDfrB8sQU4TMr=pvR5vk+1xsJT8Fa5+Dvn=V8Gg1Q@mail.gmail.com>

Hi Ek,
Look at unlist and the argument "recursive". You can step down through
the levels or a nested list to convert it to a single level list.

Jim

On Thu, Dec 20, 2018 at 1:33 PM Ek Esawi <esawiek at gmail.com> wrote:
>
> Thank you Bert. I don't see how unlist will help. I want to combine
> them but keep the "rectangular structure",e.g. list, data frame,
> matrix  because i want to get the tables in their original form.
> Unlist converts the whole output to a single vector; unless i am
> missing something.
>
> On Wed, Dec 19, 2018 at 9:10 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:
> >
> > Does ?unlist not help? Why not?
> >
> > Bert
> >
> >
> > On Wed, Dec 19, 2018, 5:13 PM Ek Esawi <esawiek at gmail.com wrote:
> >>
> >> Hi All?
> >>
> >>  I am using the R tabulizer package to extract tables from pdf files.
> >> The output is a set of lists of matrices. The package extracts tables
> >> and a lot of extra stuff which is nearly impossible to clean with
> >> RegEx. So, I want to clean it manually.
> >> To do so I need to (1) combine all lists in a single list or data
> >> frame and (2) then write the single entity to a text file to edit it.
> >> I could not figure out how.
> >>
> >> I tried something like this but did not work.
> >> lapply(MyTables, function(x)
> >> lapply(x,write.table(file="temp.txt",append = TRUE)))
> >>
> >>  Any help is greatly appreciated.
> >>
> >>  Here is my code:
> >>
> >> install.packages("rJava")    ;library(rJava)
> >> install.packages("tabulizer");library(tabulizer)
> >> MyPath <- "C:/Users/name/Documents/tEMP"
> >> ExtTable <- function (Path,CalOrd){
> >>   FileNames <- dir(Path, pattern =".(pdf|PDF)",full.names = TRUE)
> >>   MyFiles <- lapply(FileNames, function(i) extract_tables(i,method = "stream"))
> >>   if(CalOrd == "Yes"){
> >>     MyOFiles <- gsub("(\\s.*)|(.pdf|.PDF)","",basename(FileNames))
> >>     MyOFiles <- match(MyOFiles,month.name)
> >>     MyNFiles <- MyFiles[order(MyOFiles)]}
> >>   else
> >>     MyFiles
> >> }
> >> MyTables <- ExtTable(Path=MyPath,CalOrd = "No")
> >>
> >> Here is cleaned portion of the output: The whole output consists of 3
> >> lists, each contains 12, 15, and 12 sub-lists.
> >>
> >>  [[2]][[2]]
> >>  [,1]        [,2]    [,3]    [,4]  [,5]    [,6]    [,7]    [,8]    [,9]  [,10]
> >>  [1,] ""          "Avg."  "+_ lo" "n"   "Med."  ""      "Avg."  "+_
> >> lo" "n"   "Med."
> >>  [2,] "SiOz"      "44.0"  "1.26"  "375" "44.1"  "Nb"    "4.8"   "6.3"
> >>  "58"  "2.7"
> >>  [3,] "T i O  2"  "0.09"  "0.09"  "561" "0.09"  "Mo(b)" "50"    "30"
> >>  "3"   "35"
> >>  [4,] "A1203"     "2.27"  "1.10"  "375" "2.20"  "Ru(b)" "12.4"  "4.1"
> >>  "3"   "12"
> >>  [5,] "FeO total" "8.43"  "1.14"  "375" "8.19"  "Pd(b)" "3.9"   "2.1"
> >>  "19"  "4.1"
> >>  [6,] "MnO"       "0.14"  "0.03"  "366" "0.14"  "Ag(b)" "6.8"   "8.3"
> >>  "17"  "4.8"
> >>  [7,] "MgO"       "41.4"  "3.00"  "375" "41.2"  "Cd(b)" "41"    "14"
> >>  "16"  "37"
> >>  [8,] "CaO"       "2.15"  "1.11"  "374" "2.20"  "In(b)" "12"    "4"
> >>  "19"  "12"
> >>  [9,] "Na20"      "0.24"  "0.16"  "341" "0.21"  "Sn(b)" "54"    "31"
> >>  "6"   "36"
> >> [10,] "K20"       "0.054" "0.11"  "330" "0.028" "Sb(b)" "3.9"   "3.9"
> >>  "11"  "3.2"
> >> [11,] "P205"      "0.056" "0.11"  "233" "0.030" "Te(b)" "11"    "4"
> >>  "18"  "10"
> >> [12,] "Total"     "98.88" ""      ""    "98.43" "Cs(b)" "10"    "16"
> >>  "17"  "1.5"
> >> [13,] ""          ""      ""      ""    ""      "Ba"    "33"    "52"
> >>  "75"  "17"
> >> [14,] "Mg-value"  "89.8"  "1.1"   "375" "90.0"  "La"    "2.60"  "5.70"
> >>  "208" "0.77"
> >> [15,] "Ca/AI"     "1.28"  "1.6"   "374" "1.35"  "Ce"    "6.29"  "11.7"
> >>  "197" "2.08"
> >> [16,] "AI/Ti"     "22"    "29"    "361" "22"    "Pr"    "0.56"  "0.87"
> >>  "40"  "0.21"
> >> [17,] "F e / M n" "60"    "10"    "366" "59"    "Nd"    "2.67"  "4.31"
> >>  "162" "1.52"
> >> [18,] ""          ""      ""      ""    ""      "Sm"    "0.47"  "0.69"
> >>  "214" "0.25"
> >> [19,] "Li"        "1.5"   "0.3"   "6"   "1.5"   "Eu"    "0.16"  "0.21"
> >>  "201" "0.097"
> >> [20,] "B"         "0.53"  "0.07"  "6"   "0.55"  "Gd"    "0.60"  "0.83"
> >>  "67"  "0.31"
> >> [21,] "C"         "110"   "50"    "13"  "93"    "Tb"    "0.070"
> >> "0.064" "146" "0.056"
> >> [22,] "F"         "88"    "71"    "15"  "100"   "Dy"    "0.51"  "0.35"
> >>  "58"  "0.47"
> >> [23,] "S"         "157"   "77"    "22"  "152"   "Ho"    "0.12"  "0.14"
> >>  "54"  "0.090"
> >> [24,] "C1"        "53"    "45"    "15"  "75"    "Er"    "0.30"  "0.22"
> >>  "52"  "0.28"
> >> [25,] "Sc"        "12.2"  "6.4"   "220" "12.0"  "Tm"    "0.038"
> >> "0.026" "40"  "0.035"
> >> [26,] "V"         "56"    "21"    "132" "53"    "Yb"    "0.26"  "0.14"
> >>  "201" "0.27"
> >> [27,] "Cr"        "2690"  "705"   "325" "2690"  "Lu"    "0.043"
> >> "0.023" "172" "0.045"
> >> [28,] "Co"        "112"   "10"    "166" "111"   "Hf"    "0.27"  "0.30"
> >>  "71"  "0.17"
> >> [29,] "Ni"        "2160"  "304"   "308" "2140"  "Ta"    "0.40"  "0.51"
> >>  "38"  "0.23"
> >> [30,] "Cu"        "11"    "9"     "94"  "9"     "W(b)"  "7.2"   "5.2"
> >>  "6"   "4.0"
> >> [31,] "Zn"        "65"    "20"    "129" "60"    "Re(b)" "0.13"  "0.11"
> >>  "18"  "0.09"
> >> [32,] "Ga"        "2.4"   "1.3"   "49"  "2.4"   "Os(b)" "4.0"   "1.8"
> >>  "18"  "3.7"
> >> [33,] "Ge"        "0.96"  "0.19"  "19"  "0.92"  "Ir(b)" "3.7"   "0.9"
> >>  "34"  "3.0"
> >> [34,] "As"        "0.11"  "0.07"  "7"   "0.10"  "Pt(b)" "7"     "-"
> >>  "1"   "-"
> >> [35,] "Se"        "0.041" "0.056" "18"  "0.025" "Au(b)" "0.65"  "0.53"
> >>  "30"  "0.5"
> >> [36,] "Br"        "0.01"  "0.01"  "6"   "0.01"  "Tl(b)" "1.2"   "1.0"
> >>  "13"  "0.9"
> >> [37,] "Rb"        "1,9"   "4.8"   "97"  "0.38"  "Pb"    "0.16"  "0.11"
> >>  "17"  "0.16"
> >> [38,] "Sr"        "49"    "60"    "110" "20"    "Bi(b)" "1.7"   "0.7"
> >>  "13"  "1.6"
> >> [39,] "Y"         "4.4"   "5.5"   "86"  "3.1"   "Th*"   "0.71"  "1.2"
> >>  "71"  "0.22"
> >> [40,] "Zr"        "21"    "42"    "82"  "8.0"   "U"     "0.12"  "0.23"
> >>  "48"  "0.040"
> >> [[2]][[4]]
> >> [,1]       [,2]                 [,3]     [,4]                  [,5]
> >>  [,6]
> >>  [1,] ""         "Spinel peridotites" ""       "Garnet  peridotites"
> >> ""       "Primitive"
> >>  [2,] ""         "Avg. Meal."         "M-A sp" "M-A gt B-M"
> >> "Jordan" "mantle"
> >>  [3,] "SiO 2"    "44.0 44.1"          "44.15"  "44.99 45.00"
> >> "45.55"  "44.8"
> >>  [4,] "TiO 2"    "0.09 0.09"          "0.07"   "0.06 0.08"
> >> "0.11"   "0.21"
> >>  [5,] "A1203"    "2.27 2.20"          "1.96"   "1.40 1.31"
> >> "1.43"   "4.45"
> >>  [6,] "Cr203"    "0.39 0.39"          "0.44"   "0.32 0.38"
> >> "0.34"   "0.43"
> >>  [7,] "FeOtotal" "8.43 8.19"          "8.28"   "7.89 6.97"
> >> "7.61"   "8.40"
> >>  [8,] "Mn O"     "0.14 0.14"          "0.12"   "0.11 0.13"
> >> "0.11"   "0.14"
> >>  [9,] "MgO"      "41.4 41.2"          "42.25"  "42.60 44.86"
> >> "43.55"  "37.2"
> >> [10,] "NiO"      "0.27 0.27"          "0.27"   "0.26 0.29"
> >> "-"      "0.24"
> >> [11,] "CaO"      "2.15 2.20"          "2.08"   "0.82 0.77"
> >> "1.05"   "3.60"
> >> [12,] "Na  20"   "0.24 0.21"          "0.18"   "0.11 0.09"
> >> "0.14"   "0.34"
> >> [13,] "K 2 0"    "0.054 0.028"        "0.05"   "0.04 0.10"
> >> "0.11"   "0.028"
> >> [14,] "P205"     "0.056 0.030"        "0.02"   "- 0.01"
> >> "-"      "0.022"
> >> [15,] "Total"    "99.49 99.05"        "99.87"  "98.60 100.00"
> >> "100.00" "99.86"
> >> [16,] "Mg-value" "89.8 90.0"          "90.1"   "90.6 92.0"
> >> "91.1"   "88.8"
> >> [17,] "olivine"  "62 63"              "67"     "65 68"
> >> "66"     "56 57"
> >> [18,] "opx"      "24 24"              "22"     "28 25"
> >> "28"     "22 17"
> >> [19,] "cpx"      "12 11"              "9"      "3 2"
> >> "3"      "19 10"
> >> [20,] "spinel"   "2 2"                "2"      "- -"
> >> "-"      "3 -"
> >>
> >> Here is portion of the output for str(MyTables):
> >>
> >> str(MyTables)
> >>
> >> List of 3
> >>  $ :List of 12
> >> $ : chr [1:3, 1:2] "south of the artificial lake Lokka. Intrusive
> >> complexes" "of alkaline rocks are found at Sokli (phosphorite-bear-"
> >> "ing and a possible Nb-occurrence) in Finland, and at" "(Eriksson,
> >> 1992). During this period, Northern Europe" ...
> >>   ..$ : chr [1:55, 1:15] "Element" "Ag" "Al" "Al_XRF" ...
> >>   ..$ : chr [1:56, 1:2] "in the till is mainly of local origin,
> >> although some cob-" "bles and boulders may have been transported over
> >> sev-" "eral kilometres. The moraine formations in the study" "area are
> >> mostly gravelly and sandy tills, locally hum-" ...
> >>   ..$ : chr [1:53, 1:2] "requisites. PCA accounts for maximum variance
> >> of all" "variables, while FA is based on the correlation structure"
> >> "of the variables. The model of factor analysis allows that" "the
> >> common factors do not explain the total variation of" ...
> >>   ..$ : chr [1:54, 1:7] "lished examples of the use of factor
> >> analysis, it is neglec-" "ted that regional geochemical (and
> >> environmental) data" "almost never follow a normal distribution.
> >> Continuing Method" "with factor analysis in such a case must lead to
> >> biased" ...
> >>   ..$ : chr [1:16, 1:2] "shows the factor loadings of the different
> >> variables" "entering each factor. Names of variables with an abso-"
> >> "lute value of the loadings <0.3 are not plotted. Fig. 5" "shows 8
> >> results of factor analyses using a selection of all" ...
> >>   ..$ : chr [1:21, 1:2] "pretable results, notwithstanding the fact
> >> that on the" "basis of the foregoing discussion it should probably
> >> not" "be used with these data. Do these results warrant the use" "of a
> >> quite work-intensive method? Unfortunately not," ...
> >>   ..$ : chr [1:55, 1:8] "" "Ag" "Al" "Al_XRF" ...
> >>   ..$ : chr [1:23, 1:2] "addition, geochemical reasoning (e.g.
> >> geochemical asso-" "ciations and/or pathfinder elements for different
> >> types of" "ore deposits) was used to select further sub-sets of vari-"
> >> "ables. In geochemistry, the selection of elements entered" ...
> >>   ..$ : chr [1:55, 1:2] "Fig. 10C cuts several geological units, and
> >> is most likely" "indicative of alteration processes related to a
> >> deep-" "seated fault. It was revealed again in a factor analysis"
> >> "carried out with all those elements extracted by aqua" ...
> >>   ..$ : chr [1:50, 1:2] "well justified in stating that it is not very
> >> scientific to" "play with the selection of elements and number of
> >> fac-" "tors extracted until one
> >> ?\200\230?\200\230finds?\200\231?\200\231 an
> >> ?\200\230?\200\230interesting?\200\231?\200\231 result." "On the other
> >> hand, even all the different results pre-" ...
> >>   ..$ : chr [1:24, 1:2] "Niemel??, J., Ekman, I., Lukashov, A. (Eds.),
> >> 1993. Quaternary" "Deposits of Finland and Northwestern Part of
> >> Russian Fed-" "eration and Their Resources 1:1,000,000. Geological
> >> Survey" "of Finland, Espoo, Finland." ...
> >>  $ :List of 15
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From e@@wiek @ending from gm@il@com  Thu Dec 20 04:22:17 2018
From: e@@wiek @ending from gm@il@com (Ek Esawi)
Date: Wed, 19 Dec 2018 22:22:17 -0500
Subject: [R] Combine recursive lists in a single list or data frame and
 write it to file
In-Reply-To: <CA+8X3fUBZFDfrB8sQU4TMr=pvR5vk+1xsJT8Fa5+Dvn=V8Gg1Q@mail.gmail.com>
References: <CA+ZkTxuOGp__prbkn9xtr4yyXUcYdo=e4WHTQP7RZL17H_2tgw@mail.gmail.com>
 <CAGxFJbQBP8JQXEpQEmpU_PedQLvJLRrhs5SpSAvVVp1iLqQN+A@mail.gmail.com>
 <CA+ZkTxuVoFdd9dvR1S1YY5Lec-bDU=_mx11pc8cJ9fd6i54A4w@mail.gmail.com>
 <CA+8X3fUBZFDfrB8sQU4TMr=pvR5vk+1xsJT8Fa5+Dvn=V8Gg1Q@mail.gmail.com>
Message-ID: <CA+ZkTxtzQnnYSwDTn7H26tmAbYJi-KP8VNoQHsk47hkUoxKoRQ@mail.gmail.com>

Thank you Jim. I did use unlist with the recursive option which
converted the 3 levels list to a list of 38 matrices. I tried your
earlier function to join the 38 matrices, all of which have different
number of columns and rows, but i kept getting an error.

fillList<-function(x) {
+     maxrows<-max(unlist(lapply(x,length)))
+     return(lapply(x,"[",1:maxrows))
+ }
>
> for (i in 1:length(MyTables)) {
+         write.table(as.data.frame(fillList(MyTables[i])),
+                     file = "Temp.txt",append = TRUE,quote = TRUE)}
Error in (function (..., row.names = NULL, check.rows = FALSE,
check.names = TRUE,  :
  arguments imply differing number of rows: 3, 55, 56, 53, 54, 16, 21,
23, 50, 24


On Wed, Dec 19, 2018 at 9:36 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi Ek,
> Look at unlist and the argument "recursive". You can step down through
> the levels or a nested list to convert it to a single level list.
>
> Jim
>
> On Thu, Dec 20, 2018 at 1:33 PM Ek Esawi <esawiek at gmail.com> wrote:
> >
> > Thank you Bert. I don't see how unlist will help. I want to combine
> > them but keep the "rectangular structure",e.g. list, data frame,
> > matrix  because i want to get the tables in their original form.
> > Unlist converts the whole output to a single vector; unless i am
> > missing something.
> >
> > On Wed, Dec 19, 2018 at 9:10 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:
> > >
> > > Does ?unlist not help? Why not?
> > >
> > > Bert
> > >
> > >
> > > On Wed, Dec 19, 2018, 5:13 PM Ek Esawi <esawiek at gmail.com wrote:
> > >>
> > >> Hi All?
> > >>
> > >>  I am using the R tabulizer package to extract tables from pdf files.
> > >> The output is a set of lists of matrices. The package extracts tables
> > >> and a lot of extra stuff which is nearly impossible to clean with
> > >> RegEx. So, I want to clean it manually.
> > >> To do so I need to (1) combine all lists in a single list or data
> > >> frame and (2) then write the single entity to a text file to edit it.
> > >> I could not figure out how.
> > >>
> > >> I tried something like this but did not work.
> > >> lapply(MyTables, function(x)
> > >> lapply(x,write.table(file="temp.txt",append = TRUE)))
> > >>
> > >>  Any help is greatly appreciated.
> > >>
> > >>  Here is my code:
> > >>
> > >> install.packages("rJava")    ;library(rJava)
> > >> install.packages("tabulizer");library(tabulizer)
> > >> MyPath <- "C:/Users/name/Documents/tEMP"
> > >> ExtTable <- function (Path,CalOrd){
> > >>   FileNames <- dir(Path, pattern =".(pdf|PDF)",full.names = TRUE)
> > >>   MyFiles <- lapply(FileNames, function(i) extract_tables(i,method = "stream"))
> > >>   if(CalOrd == "Yes"){
> > >>     MyOFiles <- gsub("(\\s.*)|(.pdf|.PDF)","",basename(FileNames))
> > >>     MyOFiles <- match(MyOFiles,month.name)
> > >>     MyNFiles <- MyFiles[order(MyOFiles)]}
> > >>   else
> > >>     MyFiles
> > >> }
> > >> MyTables <- ExtTable(Path=MyPath,CalOrd = "No")
> > >>
> > >> Here is cleaned portion of the output: The whole output consists of 3
> > >> lists, each contains 12, 15, and 12 sub-lists.
> > >>
> > >>  [[2]][[2]]
> > >>  [,1]        [,2]    [,3]    [,4]  [,5]    [,6]    [,7]    [,8]    [,9]  [,10]
> > >>  [1,] ""          "Avg."  "+_ lo" "n"   "Med."  ""      "Avg."  "+_
> > >> lo" "n"   "Med."
> > >>  [2,] "SiOz"      "44.0"  "1.26"  "375" "44.1"  "Nb"    "4.8"   "6.3"
> > >>  "58"  "2.7"
> > >>  [3,] "T i O  2"  "0.09"  "0.09"  "561" "0.09"  "Mo(b)" "50"    "30"
> > >>  "3"   "35"
> > >>  [4,] "A1203"     "2.27"  "1.10"  "375" "2.20"  "Ru(b)" "12.4"  "4.1"
> > >>  "3"   "12"
> > >>  [5,] "FeO total" "8.43"  "1.14"  "375" "8.19"  "Pd(b)" "3.9"   "2.1"
> > >>  "19"  "4.1"
> > >>  [6,] "MnO"       "0.14"  "0.03"  "366" "0.14"  "Ag(b)" "6.8"   "8.3"
> > >>  "17"  "4.8"
> > >>  [7,] "MgO"       "41.4"  "3.00"  "375" "41.2"  "Cd(b)" "41"    "14"
> > >>  "16"  "37"
> > >>  [8,] "CaO"       "2.15"  "1.11"  "374" "2.20"  "In(b)" "12"    "4"
> > >>  "19"  "12"
> > >>  [9,] "Na20"      "0.24"  "0.16"  "341" "0.21"  "Sn(b)" "54"    "31"
> > >>  "6"   "36"
> > >> [10,] "K20"       "0.054" "0.11"  "330" "0.028" "Sb(b)" "3.9"   "3.9"
> > >>  "11"  "3.2"
> > >> [11,] "P205"      "0.056" "0.11"  "233" "0.030" "Te(b)" "11"    "4"
> > >>  "18"  "10"
> > >> [12,] "Total"     "98.88" ""      ""    "98.43" "Cs(b)" "10"    "16"
> > >>  "17"  "1.5"
> > >> [13,] ""          ""      ""      ""    ""      "Ba"    "33"    "52"
> > >>  "75"  "17"
> > >> [14,] "Mg-value"  "89.8"  "1.1"   "375" "90.0"  "La"    "2.60"  "5.70"
> > >>  "208" "0.77"
> > >> [15,] "Ca/AI"     "1.28"  "1.6"   "374" "1.35"  "Ce"    "6.29"  "11.7"
> > >>  "197" "2.08"
> > >> [16,] "AI/Ti"     "22"    "29"    "361" "22"    "Pr"    "0.56"  "0.87"
> > >>  "40"  "0.21"
> > >> [17,] "F e / M n" "60"    "10"    "366" "59"    "Nd"    "2.67"  "4.31"
> > >>  "162" "1.52"
> > >> [18,] ""          ""      ""      ""    ""      "Sm"    "0.47"  "0.69"
> > >>  "214" "0.25"
> > >> [19,] "Li"        "1.5"   "0.3"   "6"   "1.5"   "Eu"    "0.16"  "0.21"
> > >>  "201" "0.097"
> > >> [20,] "B"         "0.53"  "0.07"  "6"   "0.55"  "Gd"    "0.60"  "0.83"
> > >>  "67"  "0.31"
> > >> [21,] "C"         "110"   "50"    "13"  "93"    "Tb"    "0.070"
> > >> "0.064" "146" "0.056"
> > >> [22,] "F"         "88"    "71"    "15"  "100"   "Dy"    "0.51"  "0.35"
> > >>  "58"  "0.47"
> > >> [23,] "S"         "157"   "77"    "22"  "152"   "Ho"    "0.12"  "0.14"
> > >>  "54"  "0.090"
> > >> [24,] "C1"        "53"    "45"    "15"  "75"    "Er"    "0.30"  "0.22"
> > >>  "52"  "0.28"
> > >> [25,] "Sc"        "12.2"  "6.4"   "220" "12.0"  "Tm"    "0.038"
> > >> "0.026" "40"  "0.035"
> > >> [26,] "V"         "56"    "21"    "132" "53"    "Yb"    "0.26"  "0.14"
> > >>  "201" "0.27"
> > >> [27,] "Cr"        "2690"  "705"   "325" "2690"  "Lu"    "0.043"
> > >> "0.023" "172" "0.045"
> > >> [28,] "Co"        "112"   "10"    "166" "111"   "Hf"    "0.27"  "0.30"
> > >>  "71"  "0.17"
> > >> [29,] "Ni"        "2160"  "304"   "308" "2140"  "Ta"    "0.40"  "0.51"
> > >>  "38"  "0.23"
> > >> [30,] "Cu"        "11"    "9"     "94"  "9"     "W(b)"  "7.2"   "5.2"
> > >>  "6"   "4.0"
> > >> [31,] "Zn"        "65"    "20"    "129" "60"    "Re(b)" "0.13"  "0.11"
> > >>  "18"  "0.09"
> > >> [32,] "Ga"        "2.4"   "1.3"   "49"  "2.4"   "Os(b)" "4.0"   "1.8"
> > >>  "18"  "3.7"
> > >> [33,] "Ge"        "0.96"  "0.19"  "19"  "0.92"  "Ir(b)" "3.7"   "0.9"
> > >>  "34"  "3.0"
> > >> [34,] "As"        "0.11"  "0.07"  "7"   "0.10"  "Pt(b)" "7"     "-"
> > >>  "1"   "-"
> > >> [35,] "Se"        "0.041" "0.056" "18"  "0.025" "Au(b)" "0.65"  "0.53"
> > >>  "30"  "0.5"
> > >> [36,] "Br"        "0.01"  "0.01"  "6"   "0.01"  "Tl(b)" "1.2"   "1.0"
> > >>  "13"  "0.9"
> > >> [37,] "Rb"        "1,9"   "4.8"   "97"  "0.38"  "Pb"    "0.16"  "0.11"
> > >>  "17"  "0.16"
> > >> [38,] "Sr"        "49"    "60"    "110" "20"    "Bi(b)" "1.7"   "0.7"
> > >>  "13"  "1.6"
> > >> [39,] "Y"         "4.4"   "5.5"   "86"  "3.1"   "Th*"   "0.71"  "1.2"
> > >>  "71"  "0.22"
> > >> [40,] "Zr"        "21"    "42"    "82"  "8.0"   "U"     "0.12"  "0.23"
> > >>  "48"  "0.040"
> > >> [[2]][[4]]
> > >> [,1]       [,2]                 [,3]     [,4]                  [,5]
> > >>  [,6]
> > >>  [1,] ""         "Spinel peridotites" ""       "Garnet  peridotites"
> > >> ""       "Primitive"
> > >>  [2,] ""         "Avg. Meal."         "M-A sp" "M-A gt B-M"
> > >> "Jordan" "mantle"
> > >>  [3,] "SiO 2"    "44.0 44.1"          "44.15"  "44.99 45.00"
> > >> "45.55"  "44.8"
> > >>  [4,] "TiO 2"    "0.09 0.09"          "0.07"   "0.06 0.08"
> > >> "0.11"   "0.21"
> > >>  [5,] "A1203"    "2.27 2.20"          "1.96"   "1.40 1.31"
> > >> "1.43"   "4.45"
> > >>  [6,] "Cr203"    "0.39 0.39"          "0.44"   "0.32 0.38"
> > >> "0.34"   "0.43"
> > >>  [7,] "FeOtotal" "8.43 8.19"          "8.28"   "7.89 6.97"
> > >> "7.61"   "8.40"
> > >>  [8,] "Mn O"     "0.14 0.14"          "0.12"   "0.11 0.13"
> > >> "0.11"   "0.14"
> > >>  [9,] "MgO"      "41.4 41.2"          "42.25"  "42.60 44.86"
> > >> "43.55"  "37.2"
> > >> [10,] "NiO"      "0.27 0.27"          "0.27"   "0.26 0.29"
> > >> "-"      "0.24"
> > >> [11,] "CaO"      "2.15 2.20"          "2.08"   "0.82 0.77"
> > >> "1.05"   "3.60"
> > >> [12,] "Na  20"   "0.24 0.21"          "0.18"   "0.11 0.09"
> > >> "0.14"   "0.34"
> > >> [13,] "K 2 0"    "0.054 0.028"        "0.05"   "0.04 0.10"
> > >> "0.11"   "0.028"
> > >> [14,] "P205"     "0.056 0.030"        "0.02"   "- 0.01"
> > >> "-"      "0.022"
> > >> [15,] "Total"    "99.49 99.05"        "99.87"  "98.60 100.00"
> > >> "100.00" "99.86"
> > >> [16,] "Mg-value" "89.8 90.0"          "90.1"   "90.6 92.0"
> > >> "91.1"   "88.8"
> > >> [17,] "olivine"  "62 63"              "67"     "65 68"
> > >> "66"     "56 57"
> > >> [18,] "opx"      "24 24"              "22"     "28 25"
> > >> "28"     "22 17"
> > >> [19,] "cpx"      "12 11"              "9"      "3 2"
> > >> "3"      "19 10"
> > >> [20,] "spinel"   "2 2"                "2"      "- -"
> > >> "-"      "3 -"
> > >>
> > >> Here is portion of the output for str(MyTables):
> > >>
> > >> str(MyTables)
> > >>
> > >> List of 3
> > >>  $ :List of 12
> > >> $ : chr [1:3, 1:2] "south of the artificial lake Lokka. Intrusive
> > >> complexes" "of alkaline rocks are found at Sokli (phosphorite-bear-"
> > >> "ing and a possible Nb-occurrence) in Finland, and at" "(Eriksson,
> > >> 1992). During this period, Northern Europe" ...
> > >>   ..$ : chr [1:55, 1:15] "Element" "Ag" "Al" "Al_XRF" ...
> > >>   ..$ : chr [1:56, 1:2] "in the till is mainly of local origin,
> > >> although some cob-" "bles and boulders may have been transported over
> > >> sev-" "eral kilometres. The moraine formations in the study" "area are
> > >> mostly gravelly and sandy tills, locally hum-" ...
> > >>   ..$ : chr [1:53, 1:2] "requisites. PCA accounts for maximum variance
> > >> of all" "variables, while FA is based on the correlation structure"
> > >> "of the variables. The model of factor analysis allows that" "the
> > >> common factors do not explain the total variation of" ...
> > >>   ..$ : chr [1:54, 1:7] "lished examples of the use of factor
> > >> analysis, it is neglec-" "ted that regional geochemical (and
> > >> environmental) data" "almost never follow a normal distribution.
> > >> Continuing Method" "with factor analysis in such a case must lead to
> > >> biased" ...
> > >>   ..$ : chr [1:16, 1:2] "shows the factor loadings of the different
> > >> variables" "entering each factor. Names of variables with an abso-"
> > >> "lute value of the loadings <0.3 are not plotted. Fig. 5" "shows 8
> > >> results of factor analyses using a selection of all" ...
> > >>   ..$ : chr [1:21, 1:2] "pretable results, notwithstanding the fact
> > >> that on the" "basis of the foregoing discussion it should probably
> > >> not" "be used with these data. Do these results warrant the use" "of a
> > >> quite work-intensive method? Unfortunately not," ...
> > >>   ..$ : chr [1:55, 1:8] "" "Ag" "Al" "Al_XRF" ...
> > >>   ..$ : chr [1:23, 1:2] "addition, geochemical reasoning (e.g.
> > >> geochemical asso-" "ciations and/or pathfinder elements for different
> > >> types of" "ore deposits) was used to select further sub-sets of vari-"
> > >> "ables. In geochemistry, the selection of elements entered" ...
> > >>   ..$ : chr [1:55, 1:2] "Fig. 10C cuts several geological units, and
> > >> is most likely" "indicative of alteration processes related to a
> > >> deep-" "seated fault. It was revealed again in a factor analysis"
> > >> "carried out with all those elements extracted by aqua" ...
> > >>   ..$ : chr [1:50, 1:2] "well justified in stating that it is not very
> > >> scientific to" "play with the selection of elements and number of
> > >> fac-" "tors extracted until one
> > >> ?\200\230?\200\230finds?\200\231?\200\231 an
> > >> ?\200\230?\200\230interesting?\200\231?\200\231 result." "On the other
> > >> hand, even all the different results pre-" ...
> > >>   ..$ : chr [1:24, 1:2] "Niemel??, J., Ekman, I., Lukashov, A. (Eds.),
> > >> 1993. Quaternary" "Deposits of Finland and Northwestern Part of
> > >> Russian Fed-" "eration and Their Resources 1:1,000,000. Geological
> > >> Survey" "of Finland, Espoo, Finland." ...
> > >>  $ :List of 15
> > >>
> > >> ______________________________________________
> > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > >> and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From fr@@ch@rtier @ending from gm@il@com  Thu Dec 20 03:07:48 2018
From: fr@@ch@rtier @ending from gm@il@com (Francois Chartier)
Date: Wed, 19 Dec 2018 21:07:48 -0500
Subject: [R] 3D interpolation with R
Message-ID: <CALyCC-8QAaetagdYxZn3+2Pryg_nLM+rv7+zKLQFQ+znTeNJmw@mail.gmail.com>

Hi,

I would like to create a 3D interpolation of soil texture with a data set
of 280K points.  What package exist for 3D interpolation?
Can R do 3D kriging?

thanks
F

	[[alternative HTML version deleted]]


From drjimlemon @ending from gm@il@com  Thu Dec 20 06:28:08 2018
From: drjimlemon @ending from gm@il@com (Jim Lemon)
Date: Thu, 20 Dec 2018 16:28:08 +1100
Subject: [R] Combine recursive lists in a single list or data frame and
 write it to file
In-Reply-To: <CA+ZkTxtzQnnYSwDTn7H26tmAbYJi-KP8VNoQHsk47hkUoxKoRQ@mail.gmail.com>
References: <CA+ZkTxuOGp__prbkn9xtr4yyXUcYdo=e4WHTQP7RZL17H_2tgw@mail.gmail.com>
 <CAGxFJbQBP8JQXEpQEmpU_PedQLvJLRrhs5SpSAvVVp1iLqQN+A@mail.gmail.com>
 <CA+ZkTxuVoFdd9dvR1S1YY5Lec-bDU=_mx11pc8cJ9fd6i54A4w@mail.gmail.com>
 <CA+8X3fUBZFDfrB8sQU4TMr=pvR5vk+1xsJT8Fa5+Dvn=V8Gg1Q@mail.gmail.com>
 <CA+ZkTxtzQnnYSwDTn7H26tmAbYJi-KP8VNoQHsk47hkUoxKoRQ@mail.gmail.com>
Message-ID: <CA+8X3fXU9qQ_v=2pOkNey9wJWTF3q5ChXVV=jJnOGpWz4raLOw@mail.gmail.com>

Hi Ek,
It looks to me as though you are not joining the lists into a single
list, then calling FillList and then converting to a data frame. If
you can send some data (if it's not too big) I can test it and make
sure that it works, as it did every time for me.

Jim

On Thu, Dec 20, 2018 at 2:22 PM Ek Esawi <esawiek at gmail.com> wrote:
>
> Thank you Jim. I did use unlist with the recursive option which
> converted the 3 levels list to a list of 38 matrices. I tried your
> earlier function to join the 38 matrices, all of which have different
> number of columns and rows, but i kept getting an error.
>
> fillList<-function(x) {
> +     maxrows<-max(unlist(lapply(x,length)))
> +     return(lapply(x,"[",1:maxrows))
> + }
> >
> > for (i in 1:length(MyTables)) {
> +         write.table(as.data.frame(fillList(MyTables[i])),
> +                     file = "Temp.txt",append = TRUE,quote = TRUE)}
> Error in (function (..., row.names = NULL, check.rows = FALSE,
> check.names = TRUE,  :
>   arguments imply differing number of rows: 3, 55, 56, 53, 54, 16, 21,
> 23, 50, 24
>
>
> On Wed, Dec 19, 2018 at 9:36 PM Jim Lemon <drjimlemon at gmail.com> wrote:
> >
> > Hi Ek,
> > Look at unlist and the argument "recursive". You can step down through
> > the levels or a nested list to convert it to a single level list.
> >
> > Jim
> >
> > On Thu, Dec 20, 2018 at 1:33 PM Ek Esawi <esawiek at gmail.com> wrote:
> > >
> > > Thank you Bert. I don't see how unlist will help. I want to combine
> > > them but keep the "rectangular structure",e.g. list, data frame,
> > > matrix  because i want to get the tables in their original form.
> > > Unlist converts the whole output to a single vector; unless i am
> > > missing something.
> > >
> > > On Wed, Dec 19, 2018 at 9:10 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:
> > > >
> > > > Does ?unlist not help? Why not?
> > > >
> > > > Bert
> > > >
> > > >
> > > > On Wed, Dec 19, 2018, 5:13 PM Ek Esawi <esawiek at gmail.com wrote:
> > > >>
> > > >> Hi All?
> > > >>
> > > >>  I am using the R tabulizer package to extract tables from pdf files.
> > > >> The output is a set of lists of matrices. The package extracts tables
> > > >> and a lot of extra stuff which is nearly impossible to clean with
> > > >> RegEx. So, I want to clean it manually.
> > > >> To do so I need to (1) combine all lists in a single list or data
> > > >> frame and (2) then write the single entity to a text file to edit it.
> > > >> I could not figure out how.
> > > >>
> > > >> I tried something like this but did not work.
> > > >> lapply(MyTables, function(x)
> > > >> lapply(x,write.table(file="temp.txt",append = TRUE)))
> > > >>
> > > >>  Any help is greatly appreciated.
> > > >>
> > > >>  Here is my code:
> > > >>
> > > >> install.packages("rJava")    ;library(rJava)
> > > >> install.packages("tabulizer");library(tabulizer)
> > > >> MyPath <- "C:/Users/name/Documents/tEMP"
> > > >> ExtTable <- function (Path,CalOrd){
> > > >>   FileNames <- dir(Path, pattern =".(pdf|PDF)",full.names = TRUE)
> > > >>   MyFiles <- lapply(FileNames, function(i) extract_tables(i,method = "stream"))
> > > >>   if(CalOrd == "Yes"){
> > > >>     MyOFiles <- gsub("(\\s.*)|(.pdf|.PDF)","",basename(FileNames))
> > > >>     MyOFiles <- match(MyOFiles,month.name)
> > > >>     MyNFiles <- MyFiles[order(MyOFiles)]}
> > > >>   else
> > > >>     MyFiles
> > > >> }
> > > >> MyTables <- ExtTable(Path=MyPath,CalOrd = "No")
> > > >>
> > > >> Here is cleaned portion of the output: The whole output consists of 3
> > > >> lists, each contains 12, 15, and 12 sub-lists.
> > > >>
> > > >>  [[2]][[2]]
> > > >>  [,1]        [,2]    [,3]    [,4]  [,5]    [,6]    [,7]    [,8]    [,9]  [,10]
> > > >>  [1,] ""          "Avg."  "+_ lo" "n"   "Med."  ""      "Avg."  "+_
> > > >> lo" "n"   "Med."
> > > >>  [2,] "SiOz"      "44.0"  "1.26"  "375" "44.1"  "Nb"    "4.8"   "6.3"
> > > >>  "58"  "2.7"
> > > >>  [3,] "T i O  2"  "0.09"  "0.09"  "561" "0.09"  "Mo(b)" "50"    "30"
> > > >>  "3"   "35"
> > > >>  [4,] "A1203"     "2.27"  "1.10"  "375" "2.20"  "Ru(b)" "12.4"  "4.1"
> > > >>  "3"   "12"
> > > >>  [5,] "FeO total" "8.43"  "1.14"  "375" "8.19"  "Pd(b)" "3.9"   "2.1"
> > > >>  "19"  "4.1"
> > > >>  [6,] "MnO"       "0.14"  "0.03"  "366" "0.14"  "Ag(b)" "6.8"   "8.3"
> > > >>  "17"  "4.8"
> > > >>  [7,] "MgO"       "41.4"  "3.00"  "375" "41.2"  "Cd(b)" "41"    "14"
> > > >>  "16"  "37"
> > > >>  [8,] "CaO"       "2.15"  "1.11"  "374" "2.20"  "In(b)" "12"    "4"
> > > >>  "19"  "12"
> > > >>  [9,] "Na20"      "0.24"  "0.16"  "341" "0.21"  "Sn(b)" "54"    "31"
> > > >>  "6"   "36"
> > > >> [10,] "K20"       "0.054" "0.11"  "330" "0.028" "Sb(b)" "3.9"   "3.9"
> > > >>  "11"  "3.2"
> > > >> [11,] "P205"      "0.056" "0.11"  "233" "0.030" "Te(b)" "11"    "4"
> > > >>  "18"  "10"
> > > >> [12,] "Total"     "98.88" ""      ""    "98.43" "Cs(b)" "10"    "16"
> > > >>  "17"  "1.5"
> > > >> [13,] ""          ""      ""      ""    ""      "Ba"    "33"    "52"
> > > >>  "75"  "17"
> > > >> [14,] "Mg-value"  "89.8"  "1.1"   "375" "90.0"  "La"    "2.60"  "5.70"
> > > >>  "208" "0.77"
> > > >> [15,] "Ca/AI"     "1.28"  "1.6"   "374" "1.35"  "Ce"    "6.29"  "11.7"
> > > >>  "197" "2.08"
> > > >> [16,] "AI/Ti"     "22"    "29"    "361" "22"    "Pr"    "0.56"  "0.87"
> > > >>  "40"  "0.21"
> > > >> [17,] "F e / M n" "60"    "10"    "366" "59"    "Nd"    "2.67"  "4.31"
> > > >>  "162" "1.52"
> > > >> [18,] ""          ""      ""      ""    ""      "Sm"    "0.47"  "0.69"
> > > >>  "214" "0.25"
> > > >> [19,] "Li"        "1.5"   "0.3"   "6"   "1.5"   "Eu"    "0.16"  "0.21"
> > > >>  "201" "0.097"
> > > >> [20,] "B"         "0.53"  "0.07"  "6"   "0.55"  "Gd"    "0.60"  "0.83"
> > > >>  "67"  "0.31"
> > > >> [21,] "C"         "110"   "50"    "13"  "93"    "Tb"    "0.070"
> > > >> "0.064" "146" "0.056"
> > > >> [22,] "F"         "88"    "71"    "15"  "100"   "Dy"    "0.51"  "0.35"
> > > >>  "58"  "0.47"
> > > >> [23,] "S"         "157"   "77"    "22"  "152"   "Ho"    "0.12"  "0.14"
> > > >>  "54"  "0.090"
> > > >> [24,] "C1"        "53"    "45"    "15"  "75"    "Er"    "0.30"  "0.22"
> > > >>  "52"  "0.28"
> > > >> [25,] "Sc"        "12.2"  "6.4"   "220" "12.0"  "Tm"    "0.038"
> > > >> "0.026" "40"  "0.035"
> > > >> [26,] "V"         "56"    "21"    "132" "53"    "Yb"    "0.26"  "0.14"
> > > >>  "201" "0.27"
> > > >> [27,] "Cr"        "2690"  "705"   "325" "2690"  "Lu"    "0.043"
> > > >> "0.023" "172" "0.045"
> > > >> [28,] "Co"        "112"   "10"    "166" "111"   "Hf"    "0.27"  "0.30"
> > > >>  "71"  "0.17"
> > > >> [29,] "Ni"        "2160"  "304"   "308" "2140"  "Ta"    "0.40"  "0.51"
> > > >>  "38"  "0.23"
> > > >> [30,] "Cu"        "11"    "9"     "94"  "9"     "W(b)"  "7.2"   "5.2"
> > > >>  "6"   "4.0"
> > > >> [31,] "Zn"        "65"    "20"    "129" "60"    "Re(b)" "0.13"  "0.11"
> > > >>  "18"  "0.09"
> > > >> [32,] "Ga"        "2.4"   "1.3"   "49"  "2.4"   "Os(b)" "4.0"   "1.8"
> > > >>  "18"  "3.7"
> > > >> [33,] "Ge"        "0.96"  "0.19"  "19"  "0.92"  "Ir(b)" "3.7"   "0.9"
> > > >>  "34"  "3.0"
> > > >> [34,] "As"        "0.11"  "0.07"  "7"   "0.10"  "Pt(b)" "7"     "-"
> > > >>  "1"   "-"
> > > >> [35,] "Se"        "0.041" "0.056" "18"  "0.025" "Au(b)" "0.65"  "0.53"
> > > >>  "30"  "0.5"
> > > >> [36,] "Br"        "0.01"  "0.01"  "6"   "0.01"  "Tl(b)" "1.2"   "1.0"
> > > >>  "13"  "0.9"
> > > >> [37,] "Rb"        "1,9"   "4.8"   "97"  "0.38"  "Pb"    "0.16"  "0.11"
> > > >>  "17"  "0.16"
> > > >> [38,] "Sr"        "49"    "60"    "110" "20"    "Bi(b)" "1.7"   "0.7"
> > > >>  "13"  "1.6"
> > > >> [39,] "Y"         "4.4"   "5.5"   "86"  "3.1"   "Th*"   "0.71"  "1.2"
> > > >>  "71"  "0.22"
> > > >> [40,] "Zr"        "21"    "42"    "82"  "8.0"   "U"     "0.12"  "0.23"
> > > >>  "48"  "0.040"
> > > >> [[2]][[4]]
> > > >> [,1]       [,2]                 [,3]     [,4]                  [,5]
> > > >>  [,6]
> > > >>  [1,] ""         "Spinel peridotites" ""       "Garnet  peridotites"
> > > >> ""       "Primitive"
> > > >>  [2,] ""         "Avg. Meal."         "M-A sp" "M-A gt B-M"
> > > >> "Jordan" "mantle"
> > > >>  [3,] "SiO 2"    "44.0 44.1"          "44.15"  "44.99 45.00"
> > > >> "45.55"  "44.8"
> > > >>  [4,] "TiO 2"    "0.09 0.09"          "0.07"   "0.06 0.08"
> > > >> "0.11"   "0.21"
> > > >>  [5,] "A1203"    "2.27 2.20"          "1.96"   "1.40 1.31"
> > > >> "1.43"   "4.45"
> > > >>  [6,] "Cr203"    "0.39 0.39"          "0.44"   "0.32 0.38"
> > > >> "0.34"   "0.43"
> > > >>  [7,] "FeOtotal" "8.43 8.19"          "8.28"   "7.89 6.97"
> > > >> "7.61"   "8.40"
> > > >>  [8,] "Mn O"     "0.14 0.14"          "0.12"   "0.11 0.13"
> > > >> "0.11"   "0.14"
> > > >>  [9,] "MgO"      "41.4 41.2"          "42.25"  "42.60 44.86"
> > > >> "43.55"  "37.2"
> > > >> [10,] "NiO"      "0.27 0.27"          "0.27"   "0.26 0.29"
> > > >> "-"      "0.24"
> > > >> [11,] "CaO"      "2.15 2.20"          "2.08"   "0.82 0.77"
> > > >> "1.05"   "3.60"
> > > >> [12,] "Na  20"   "0.24 0.21"          "0.18"   "0.11 0.09"
> > > >> "0.14"   "0.34"
> > > >> [13,] "K 2 0"    "0.054 0.028"        "0.05"   "0.04 0.10"
> > > >> "0.11"   "0.028"
> > > >> [14,] "P205"     "0.056 0.030"        "0.02"   "- 0.01"
> > > >> "-"      "0.022"
> > > >> [15,] "Total"    "99.49 99.05"        "99.87"  "98.60 100.00"
> > > >> "100.00" "99.86"
> > > >> [16,] "Mg-value" "89.8 90.0"          "90.1"   "90.6 92.0"
> > > >> "91.1"   "88.8"
> > > >> [17,] "olivine"  "62 63"              "67"     "65 68"
> > > >> "66"     "56 57"
> > > >> [18,] "opx"      "24 24"              "22"     "28 25"
> > > >> "28"     "22 17"
> > > >> [19,] "cpx"      "12 11"              "9"      "3 2"
> > > >> "3"      "19 10"
> > > >> [20,] "spinel"   "2 2"                "2"      "- -"
> > > >> "-"      "3 -"
> > > >>
> > > >> Here is portion of the output for str(MyTables):
> > > >>
> > > >> str(MyTables)
> > > >>
> > > >> List of 3
> > > >>  $ :List of 12
> > > >> $ : chr [1:3, 1:2] "south of the artificial lake Lokka. Intrusive
> > > >> complexes" "of alkaline rocks are found at Sokli (phosphorite-bear-"
> > > >> "ing and a possible Nb-occurrence) in Finland, and at" "(Eriksson,
> > > >> 1992). During this period, Northern Europe" ...
> > > >>   ..$ : chr [1:55, 1:15] "Element" "Ag" "Al" "Al_XRF" ...
> > > >>   ..$ : chr [1:56, 1:2] "in the till is mainly of local origin,
> > > >> although some cob-" "bles and boulders may have been transported over
> > > >> sev-" "eral kilometres. The moraine formations in the study" "area are
> > > >> mostly gravelly and sandy tills, locally hum-" ...
> > > >>   ..$ : chr [1:53, 1:2] "requisites. PCA accounts for maximum variance
> > > >> of all" "variables, while FA is based on the correlation structure"
> > > >> "of the variables. The model of factor analysis allows that" "the
> > > >> common factors do not explain the total variation of" ...
> > > >>   ..$ : chr [1:54, 1:7] "lished examples of the use of factor
> > > >> analysis, it is neglec-" "ted that regional geochemical (and
> > > >> environmental) data" "almost never follow a normal distribution.
> > > >> Continuing Method" "with factor analysis in such a case must lead to
> > > >> biased" ...
> > > >>   ..$ : chr [1:16, 1:2] "shows the factor loadings of the different
> > > >> variables" "entering each factor. Names of variables with an abso-"
> > > >> "lute value of the loadings <0.3 are not plotted. Fig. 5" "shows 8
> > > >> results of factor analyses using a selection of all" ...
> > > >>   ..$ : chr [1:21, 1:2] "pretable results, notwithstanding the fact
> > > >> that on the" "basis of the foregoing discussion it should probably
> > > >> not" "be used with these data. Do these results warrant the use" "of a
> > > >> quite work-intensive method? Unfortunately not," ...
> > > >>   ..$ : chr [1:55, 1:8] "" "Ag" "Al" "Al_XRF" ...
> > > >>   ..$ : chr [1:23, 1:2] "addition, geochemical reasoning (e.g.
> > > >> geochemical asso-" "ciations and/or pathfinder elements for different
> > > >> types of" "ore deposits) was used to select further sub-sets of vari-"
> > > >> "ables. In geochemistry, the selection of elements entered" ...
> > > >>   ..$ : chr [1:55, 1:2] "Fig. 10C cuts several geological units, and
> > > >> is most likely" "indicative of alteration processes related to a
> > > >> deep-" "seated fault. It was revealed again in a factor analysis"
> > > >> "carried out with all those elements extracted by aqua" ...
> > > >>   ..$ : chr [1:50, 1:2] "well justified in stating that it is not very
> > > >> scientific to" "play with the selection of elements and number of
> > > >> fac-" "tors extracted until one
> > > >> ?\200\230?\200\230finds?\200\231?\200\231 an
> > > >> ?\200\230?\200\230interesting?\200\231?\200\231 result." "On the other
> > > >> hand, even all the different results pre-" ...
> > > >>   ..$ : chr [1:24, 1:2] "Niemel??, J., Ekman, I., Lukashov, A. (Eds.),
> > > >> 1993. Quaternary" "Deposits of Finland and Northwestern Part of
> > > >> Russian Fed-" "eration and Their Resources 1:1,000,000. Geological
> > > >> Survey" "of Finland, Espoo, Finland." ...
> > > >>  $ :List of 15
> > > >>
> > > >> ______________________________________________
> > > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > > >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > >> and provide commented, minimal, self-contained, reproducible code.
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.


From @rikri@hn@moh@n @ending from gm@il@com  Thu Dec 20 08:07:18 2018
From: @rikri@hn@moh@n @ending from gm@il@com (km)
Date: Thu, 20 Dec 2018 12:37:18 +0530
Subject: [R] test of independence
Message-ID: <CAPV1RAAi_NkUmUvRMahOban1o9RHrc42TwCxvriJ9iuYJeJP-A@mail.gmail.com>

Dear All,

How do I do a test of independence with 16x16 table of counts.
Please suggest.

Regards,
KM

	[[alternative HTML version deleted]]


From ericjberger @ending from gm@il@com  Thu Dec 20 08:34:57 2018
From: ericjberger @ending from gm@il@com (Eric Berger)
Date: Thu, 20 Dec 2018 09:34:57 +0200
Subject: [R] 3D interpolation with R
In-Reply-To: <CALyCC-8QAaetagdYxZn3+2Pryg_nLM+rv7+zKLQFQ+znTeNJmw@mail.gmail.com>
References: <CALyCC-8QAaetagdYxZn3+2Pryg_nLM+rv7+zKLQFQ+znTeNJmw@mail.gmail.com>
Message-ID: <CAGgJW76czmigcYOyvKB+ObMV+Vj2ui8cZQW=LSfQXbLRz0jYXQ@mail.gmail.com>

You can do a web search that focuses on R related hits via
step 1: go to rseek.org
step 2: do your search there - e.g. 3d krig

This returns a lot of "hits" that seem to address your question.

HTH,
Eric


On Thu, Dec 20, 2018 at 5:56 AM Francois Chartier <fra.chartier at gmail.com>
wrote:

> Hi,
>
> I would like to create a 3D interpolation of soil texture with a data set
> of 280K points.  What package exist for 3D interpolation?
> Can R do 3D kriging?
>
> thanks
> F
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From petr@pik@l @ending from prechez@@cz  Thu Dec 20 09:32:43 2018
From: petr@pik@l @ending from prechez@@cz (PIKAL Petr)
Date: Thu, 20 Dec 2018 08:32:43 +0000
Subject: [R] test of independence
In-Reply-To: <CAPV1RAAi_NkUmUvRMahOban1o9RHrc42TwCxvriJ9iuYJeJP-A@mail.gmail.com>
References: <CAPV1RAAi_NkUmUvRMahOban1o9RHrc42TwCxvriJ9iuYJeJP-A@mail.gmail.com>
Message-ID: <32d52b0a85154eed87923270bd3b10f6@SRVEXCHCM1302.precheza.cz>

Hi

Did you search CRAN? I got **many** results for

test of independence

which may or may not provide you with suitable procedures.

Cheers
Petr


> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of km
> Sent: Thursday, December 20, 2018 8:07 AM
> To: r-help at r-project.org
> Subject: [R] test of independence
>
> Dear All,
>
> How do I do a test of independence with 16x16 table of counts.
> Please suggest.
>
> Regards,
> KM
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


From pd@me@ @ending from cb@@dk  Thu Dec 20 09:33:30 2018
From: pd@me@ @ending from cb@@dk (Peter Dalgaard)
Date: Thu, 20 Dec 2018 08:33:30 +0000
Subject: [R] R 3.5.2 is released
Message-ID: <6C5303DF-7435-4C86-B46F-9BF6BB7C1EB4@cbs.dk>

The build system rolled up R-3.5.2.tar.gz (codename "Eggshell Igloo") this morning.

The list below details the changes in this release.

You can get the source code from

http://cran.r-project.org/src/base/R-3/R-3.5.2.tar.gz

or wait for it to be mirrored at a CRAN site nearer to you.

Binaries for various platforms will appear in due course.


For the R Core Team,

Peter Dalgaard


These are the checksums (md5 and SHA-256) for the freshly created files, in case you wish
to check that they are uncorrupted:

MD5 (AUTHORS) = b9c44f9f78cab3184ad9898bebc854b4
MD5 (COPYING) = eb723b61539feef013de476e68b5c50a
MD5 (COPYING.LIB) = a6f89e2100d9b6cdffcea4f398e37343
MD5 (FAQ) = 3bba37aa1dd06de3f781200a8081302f
MD5 (INSTALL) = 7893f754308ca31f1ccf62055090ad7b
MD5 (NEWS) = 2e406d65b35c6e2f6f0cf07ce12f1697
MD5 (NEWS.0) = bfcd7c147251b5474d96848c6f57e5a8
MD5 (NEWS.1) = eb78c4d053ec9c32b815cf0c2ebea801
MD5 (NEWS.2) = 591dcf615162127f904e4e461f330ce9
MD5 (R-latest.tar.gz) = 3e4b40b2bbd4a2f8133ac45dbef6a485
MD5 (README) = f468f281c919665e276a1b691decbbe6
MD5 (RESOURCES) = 529223fd3ffef95731d0a87353108435
MD5 (THANKS) = 08158353102084599797db8c9ccf8e2a
MD5 (VERSION-INFO.dcf) = 2f0bb13198cfc16ce69d0c61d8cd57ad
MD5 (R-3/R-3.5.2.tar.gz) = 3e4b40b2bbd4a2f8133ac45dbef6a485

2cde824a7b18958e5f06b391c801c8288be0f84fa8934b7ddefef23c67e60c09  AUTHORS
e6d6a009505e345fe949e1310334fcb0747f28dae2856759de102ab66b722cb4  COPYING
6095e9ffa777dd22839f7801aa845b31c9ed07f3d6bf8a26dc5d2dec8ccc0ef3  COPYING.LIB
98df47801c33cc4f4a4de98447cb2bd40e09c0920195f540a981ceed874714f2  FAQ
f87461be6cbaecc4dce44ac58e5bd52364b0491ccdadaf846cb9b452e9550f31  INSTALL
c1bb84ce9fb277e742e2545506c8d1233deb8d397c65329b9325162675a2516c  NEWS
4e21b62f515b749f80997063fceab626d7258c7d650e81a662ba8e0640f12f62  NEWS.0
12b30c724117b1b2b11484673906a6dcd48a361f69fc420b36194f9218692d01  NEWS.1
ca04f78ffe54afa326fe3ed40e7e1411aca0000ed2fa5ead97ddf51c6aa5b7bc  NEWS.2
e53d8c3cf20f2b8d7a9c1631b6f6a22874506fb392034758b3bb341c586c5b62  R-latest.tar.gz
2fdd3e90f23f32692d4b3a0c0452f2c219a10882033d1774f8cadf25886c3ddc  README
408737572ecc6e1135fdb2cf7a9dbb1a6cb27967c757f1771b8c39d1fd2f1ab9  RESOURCES
2d2e85e85574c4430951f6b070c08cd5aff1602abfd1bb162bed6d89c436b11f  THANKS
9d0e6e7f442952001c174cfc235cc4e6ec97f6e73865f605da30abf34f116fb0  VERSION-INFO.dcf
e53d8c3cf20f2b8d7a9c1631b6f6a22874506fb392034758b3bb341c586c5b62  R-3/R-3.5.2.tar.gz

This is the relevant part of the NEWS file:

CHANGES IN R 3.5.2:

  PACKAGE INSTALLATION:

    * New macro CXX_VISIBILITY analogous to C_VISIBILITY (which several
      packages have been misusing for C++ code) for the default C++
      compiler (but not necessarily one used for non-default C++
      dialects like C++14).

  TESTING:

    * The random number generator tests in tests/p-r-random-tests.R no
      longer fail occasionally as they now randomly sample from
      "certified" random seeds.

  BUG FIXES:

    * The "glm" method of drop1() miscalculated the score test
      (test="Rao") when the model contained an offset.

    * Linear multiple empty models such as lm(y ~ 0) now have a
      correctly dimensioned empty coefficient matrix; reported by Brett
      Presnell.

    * vcov(<empty mlm>) and hence confint() now work (via a consistency
      change in summary.lm()).

    * confint(<multiple lm()>) now works correctly; reported on R-devel
      by Steven Pav.

    * quade.test() now also works correctly when its arguments are not
      yet sorted along groups, fixing PR#15842.

    * Installation on a Unix-alike tries harder to link to the pthread
      library where required (rather than relying on OpenMP to provide
      it: configuring with --disable-openmp was failing on some Linux
      systems).

    * The data.frame method for print(x) is fast now also for large
      data frames x and got an optional argument max, thanks to
      suggestions by Juan Telleria.

    * hist() no longer integer overflows in very rare cases, fixing
      PR#17450.

    * untar() ignored a character compressed argument: however many
      external tar programs ignore the flags which should have been set
      and automagically choose the compression type, and if appropriate
      gzip or bzip2 compression would have been chosen from the magic
      header of the tarball.

    * zapsmall(x) now works for more "number-like" objects.

    * The tools-internal function called from R CMD INSTALL now gets a
      warnOption = 1 argument and only sets options(warn = warnOption)
      when that increases the warning level (PR#17453).

    * Analogously, the tools-internal function called from R CMD check
      gets a warnOption = 1 argument and uses the larger of that and
      getOption("warn"), also allowing to be run with increased warning
      level.

    * Parse data now have deterministic parent nodes (PR#16041).

    * Calling match() with length one x and POSIXlt table gave a
      segfault (PR#17459).

    * Fork clusters could hang due to a race condition in cluster
      initialization (makeCluster()).

    * nextn(n) now also works for larger n and no longer loops
      infinitely for e.g, n <- 214e7.

    * cooks.distance() and rstandard() now work correctly for multiple
      linear models ("mlm").

    * polym() and corresponding lm() prediction now also work for a
      boundary "vector" case fixing PR#17474, reported by Alexandre
      Courtiol.

    * With a very large number of variables terms() could segfault
      (PR#17480).

    * cut(rep(0, 7)) now works, thanks to Joey Reid and Benjamin Tyner
      (PR#16802).

    * download.file(*, method = "curl", cacheOK = FALSE) should work
      now on Windows, thanks to Kevin Ushey's patch in PR#17323.

    * duplicated(<dataframe with 'f'>) now works, too, thanks to
      Andreas Kersting's PR#17485; ditto for anyDuplicated().

    * legend(*, cex = 1:2) now works less badly.

    * The print() method for POSIXct and POSIXlt now correctly obeys
      getOption("max.print"), fixing a long-standing typo, and it also
      gets a corresponding optional max argument.

    * Unserialization of raw vectors serialized in ASCII representation
      now works correctly.

    * <data frame>[TRUE, <new>] <- list(c1, c2) now works correctly,
      thanks to Suharto Anggono's PR#15362 and Emil Bode's patch in
      PR#17504.

    * seq.int(*, by=by, length=n) no longer wrongly "drops fractional
      parts" when by is integer, thanks to Suharto Anggono's report
      PR#17506.

    * Buffering is disabled for file() connections to non-regular files
      (like sockets), as well as fifo() and pipe() connections.  Fixes
      PR#17470, reported by Chris Culnane.


-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com

_______________________________________________
R-announce at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From m@rongiu@luigi @ending from gm@il@com  Thu Dec 20 11:15:58 2018
From: m@rongiu@luigi @ending from gm@il@com (Luigi Marongiu)
Date: Thu, 20 Dec 2018 11:15:58 +0100
Subject: [R] extract hyperplan from e1071 model
Message-ID: <CAMk+s2SQPNv4BUSpyo71Jmr1k+Vj5s2BjD97rowoBQfs_o977g@mail.gmail.com>

Dear all,
I am using the package e1071 for modeling SVM. I obtain a model from
the data and I can plot the results; the plot shows the support
vectors (as 'X's) and the shaded areas as it should be.
However, I don't like the plot generated from the model and I would
like instead to have more control upon the plotting; in particular, I
would like to draw the hyperplane on plots I have already made from
the data available.
Is there a way to extract the values that are used to draw the hyperplane?
That is: plot(model) -- where model is obtained from svm() -- draws an
area in blue and one in red based on some values provided by model;
can I get these values so I can plot a line in a pre-existing plot?
Also, it is possible to extract the positions of the support vectors?
The names of the model generated by svm() are:
> names(mod)
 [1] "call"            "type"            "kernel"          "cost"
 [5] "degree"          "gamma"           "coef0"           "nu"
 [9] "epsilon"         "sparse"          "scaled"          "x.scale"
[13] "y.scale"         "nclasses"        "levels"          "tot.nSV"
[17] "nSV"             "labels"          "SV"              "index"
[21] "rho"             "compprob"        "probA"           "probB"
[25] "sigma"           "coefs"           "na.action"       "fitted"
[29] "decision.values" "terms"

Which one should I look at?
Thank you
-- 
Best regards,
Luigi


From r-p@ck@ge@ @ending from r-project@org  Wed Dec 19 17:17:21 2018
From: r-p@ck@ge@ @ending from r-project@org (mohammed ibrahim via R-packages)
Date: Wed, 19 Dec 2018 16:17:21 +0000 (UTC)
Subject: [R] [R-pkgs] dbparser: 'DrugBank' Database XML Parser
References: <494687108.6752524.1545236241993.ref@mail.yahoo.com>
Message-ID: <494687108.6752524.1545236241993@mail.yahoo.com>

Hello,
Kindly?find this new package for parsing the 'DrugBank' XML database <http://drugbank.ca/>. The parsed data are then returned in a proper 'R' dataframe with the ability to save them in a given database.
CRAN:?<https://cran.r-project.org/web/packages/dbparser/index.html>
Vignettes:?<https://cran.r-project.org/web/packages/dbparser/vignettes/dbparser.html>
Github:?<https://github.com/Dainanahan/dbparser>




It will be great to know your feedback :). Thank you

Best Regards,
Mohammed Ali
MSc in Applied Data Science & Big Data
mohammed.ali at edu.dsti.institute
+20 01000481973
eg.linkedin.com/in/mohammedali85

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From m@echler @ending from @t@t@m@th@ethz@ch  Thu Dec 20 13:00:11 2018
From: m@echler @ending from @t@t@m@th@ethz@ch (Martin Maechler)
Date: Thu, 20 Dec 2018 13:00:11 +0100
Subject: [R] Webshot failed to take snapshot in Ubuntu machine
In-Reply-To: <85df464f-1892-5500-799b-f5d73a61015a@yahoo.fr>
References: <CA+dpOJnZwhzUt=mPN+hKacuTjU71nyGggts69hvh55uyBPO5sQ@mail.gmail.com>
 <85df464f-1892-5500-799b-f5d73a61015a@yahoo.fr>
Message-ID: <23579.33867.614153.433173@stat.math.ethz.ch>

>>>>> Marc Girondot via R-help 
>>>>>     on Tue, 18 Dec 2018 13:53:34 +0100 writes:

    > Hi Christofer, I just try on MacOSX and ubuntu and it
    > works on both:

    > For ubuntu:
    >> Sys.info()
    >  ????????????????????????????????????? sysname
    > ????????????????????????????????????? "Linux"
    > ????????????????????????????????????? release
    > ????????????????????????? "4.15.0-42-generic"
    > ????????????????????????????????????? version "#45-Ubuntu
    > SMP Thu Nov 15 19:32:57 UTC 2018"
    > ???????????????????????????????????? nodename
    > ?????????????????????????????? "lepidochelys"
    > ????????????????????????????????????? machine
    > ???????????????????????????????????? "x86_64"

    > Not sure what to do...
    > Marc

Hmm, if I try it (on my Linux desktop), I get

  > library(webshot)
  > url <- "https://www.bseindia.com/stock-share-price/asian-paints-ltd/asianpaint/500820/"
  > webshot(url, 'bb.pdf')
  PhantomJS not found. You can install it with webshot::install_phantomjs(). If it is installed, please make sure the phantomjs executable can be found via the PATH variable.
  NULL

So, it is clear this relies on extra javascript based software
being available on your computer, *and* having that correctly in
your PATH.

On my linux system, I then did
   webshot::install_phantomjs()
and that downloaded things and installed a 67 Megabyte
executable in my PATH ... which then subsequently worked.

On that Linux system it did *not* work, try

  system("which phantomjs")

and you should see that it gets a version of 'phantomjs' on your
computer, i.e., the one that  webshot() will then try to use and
somehow fails.

I'd recommend you run   webshot::install_phantomjs()
which then should install a "better" version of the 'phantomjs'
executable that then *should* work ..

Let us know if this helped (or why not).

Best,
Martin Maechler
ETH Zurich

    > Le 18/12/2018 ? 13:37, Christofer Bogaso a ?crit?:
    >> Hi,
    >> 
    >> I was using webshot package to take snapshot of a webpage
    >> as below:
    >> 
    >> library(webshot) webshot('
    >> https://www.bseindia.com/stock-share-price/asian-paints-ltd/asianpaint/500820/',
    >> 'bb.pdf')
    >> 
    >> However what I see is a Blank PDF file is saved.
    >> 
    >> However if I use the same code in my windows machine it
    >> is able to produce correct snapshot.
    >> 
    >> Below is my system information
    >>> Sys.info()
    >> sysname "Linux" release "4.4.0-139-generic" version
    >> "#165-Ubuntu SMP Wed Oct 24 10:58:50 UTC 2018" nodename
    >> "ubuntu-s-2vcpu-4gb-blr1-01" machine "x86_64" login
    >> "root" user "root" effective_user "root"
    >> 
    >> Any idea what went wrong would be highly helpful.
    >> 
    >> Thanks,
    >> 
    >> [[alternative HTML version deleted]]
    >> 
    >> ______________________________________________
    >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    >> more, see https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide
    >> http://www.R-project.org/posting-guide.html and provide
    >> commented, minimal, self-contained, reproducible code.

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    > more, see https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide
    > http://www.R-project.org/posting-guide.html and provide
    > commented, minimal, self-contained, reproducible code.


From e@@wiek @ending from gm@il@com  Thu Dec 20 14:27:39 2018
From: e@@wiek @ending from gm@il@com (Ek Esawi)
Date: Thu, 20 Dec 2018 08:27:39 -0500
Subject: [R] Combine recursive lists in a single list or data frame and
 write it to file
In-Reply-To: <CA+8X3fXU9qQ_v=2pOkNey9wJWTF3q5ChXVV=jJnOGpWz4raLOw@mail.gmail.com>
References: <CA+ZkTxuOGp__prbkn9xtr4yyXUcYdo=e4WHTQP7RZL17H_2tgw@mail.gmail.com>
 <CAGxFJbQBP8JQXEpQEmpU_PedQLvJLRrhs5SpSAvVVp1iLqQN+A@mail.gmail.com>
 <CA+ZkTxuVoFdd9dvR1S1YY5Lec-bDU=_mx11pc8cJ9fd6i54A4w@mail.gmail.com>
 <CA+8X3fUBZFDfrB8sQU4TMr=pvR5vk+1xsJT8Fa5+Dvn=V8Gg1Q@mail.gmail.com>
 <CA+ZkTxtzQnnYSwDTn7H26tmAbYJi-KP8VNoQHsk47hkUoxKoRQ@mail.gmail.com>
 <CA+8X3fXU9qQ_v=2pOkNey9wJWTF3q5ChXVV=jJnOGpWz4raLOw@mail.gmail.com>
Message-ID: <CA+ZkTxvR-iEiCfaoZkcg0NvUJ2MBBz4JQmxN6rj6x0BLbJiphQ@mail.gmail.com>

Thanks again Jim. The links below are for 2 files (papers) i
downloaded from Google Scholar for testing. You can use either both or
any other pdf files with tables. Thanks again-EK.

https://pdfs.semanticscholar.org/50a4/2b8146f08161b1036457fe0d241b6b898974.pdf
https://pdfs.semanticscholar.org/50a4/2b8146f08161b1036457fe0d241b6b898974.pdf


The code:
install.packages("rJava")    ;library(rJava)
install.packages("tabulizer");library(tabulizer)
MyPath &lt;- "C:/Users/name/Documents/Temp"
ExtTable &lt;- function (Path,CalOrd){
  FileNames &lt;- dir(Path, pattern =".(pdf|PDF)",full.names = TRUE)
  MyFiles &lt;- lapply(FileNames, function(i) extract_tables(i,method
= "stream"))
  if(CalOrd == "Yes"){
    MyOFiles &lt;- gsub("(\\s.*)|(.pdf|.PDF)","",basename(FileNames))
    MyOFiles &lt;- match(MyOFiles,month.name)
    MyNFiles &lt;- MyFiles[order(MyOFiles)]}
  else
    MyFiles
}
MyTables &lt;- ExtTable(Path=MyPath,CalOrd = "Yes")

On Thu, Dec 20, 2018 at 12:28 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi Ek,
> It looks to me as though you are not joining the lists into a single
> list, then calling FillList and then converting to a data frame. If
> you can send some data (if it's not too big) I can test it and make
> sure that it works, as it did every time for me.
>
> Jim
>
> On Thu, Dec 20, 2018 at 2:22 PM Ek Esawi <esawiek at gmail.com> wrote:
> >
> > Thank you Jim. I did use unlist with the recursive option which
> > converted the 3 levels list to a list of 38 matrices. I tried your
> > earlier function to join the 38 matrices, all of which have different
> > number of columns and rows, but i kept getting an error.
> >
> > fillList<-function(x) {
> > +     maxrows<-max(unlist(lapply(x,length)))
> > +     return(lapply(x,"[",1:maxrows))
> > + }
> > >
> > > for (i in 1:length(MyTables)) {
> > +         write.table(as.data.frame(fillList(MyTables[i])),
> > +                     file = "Temp.txt",append = TRUE,quote = TRUE)}
> > Error in (function (..., row.names = NULL, check.rows = FALSE,
> > check.names = TRUE,  :
> >   arguments imply differing number of rows: 3, 55, 56, 53, 54, 16, 21,
> > 23, 50, 24
> >
> >
> > On Wed, Dec 19, 2018 at 9:36 PM Jim Lemon <drjimlemon at gmail.com> wrote:
> > >
> > > Hi Ek,
> > > Look at unlist and the argument "recursive". You can step down through
> > > the levels or a nested list to convert it to a single level list.
> > >
> > > Jim
> > >
> > > On Thu, Dec 20, 2018 at 1:33 PM Ek Esawi <esawiek at gmail.com> wrote:
> > > >
> > > > Thank you Bert. I don't see how unlist will help. I want to combine
> > > > them but keep the "rectangular structure",e.g. list, data frame,
> > > > matrix  because i want to get the tables in their original form.
> > > > Unlist converts the whole output to a single vector; unless i am
> > > > missing something.
> > > >
> > > > On Wed, Dec 19, 2018 at 9:10 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:
> > > > >
> > > > > Does ?unlist not help? Why not?
> > > > >
> > > > > Bert
> > > > >
> > > > >
> > > > > On Wed, Dec 19, 2018, 5:13 PM Ek Esawi <esawiek at gmail.com wrote:
> > > > >>
> > > > >> Hi All?
> > > > >>
> > > > >>  I am using the R tabulizer package to extract tables from pdf files.
> > > > >> The output is a set of lists of matrices. The package extracts tables
> > > > >> and a lot of extra stuff which is nearly impossible to clean with
> > > > >> RegEx. So, I want to clean it manually.
> > > > >> To do so I need to (1) combine all lists in a single list or data
> > > > >> frame and (2) then write the single entity to a text file to edit it.
> > > > >> I could not figure out how.
> > > > >>
> > > > >> I tried something like this but did not work.
> > > > >> lapply(MyTables, function(x)
> > > > >> lapply(x,write.table(file="temp.txt",append = TRUE)))
> > > > >>
> > > > >>  Any help is greatly appreciated.
> > > > >>
> > > > >>  Here is my code:
> > > > >>
> > > > >> install.packages("rJava")    ;library(rJava)
> > > > >> install.packages("tabulizer");library(tabulizer)
> > > > >> MyPath <- "C:/Users/name/Documents/tEMP"
> > > > >> ExtTable <- function (Path,CalOrd){
> > > > >>   FileNames <- dir(Path, pattern =".(pdf|PDF)",full.names = TRUE)
> > > > >>   MyFiles <- lapply(FileNames, function(i) extract_tables(i,method = "stream"))
> > > > >>   if(CalOrd == "Yes"){
> > > > >>     MyOFiles <- gsub("(\\s.*)|(.pdf|.PDF)","",basename(FileNames))
> > > > >>     MyOFiles <- match(MyOFiles,month.name)
> > > > >>     MyNFiles <- MyFiles[order(MyOFiles)]}
> > > > >>   else
> > > > >>     MyFiles
> > > > >> }
> > > > >> MyTables <- ExtTable(Path=MyPath,CalOrd = "No")
> > > > >>
> > > > >> Here is cleaned portion of the output: The whole output consists of 3
> > > > >> lists, each contains 12, 15, and 12 sub-lists.
> > > > >>
> > > > >>  [[2]][[2]]
> > > > >>  [,1]        [,2]    [,3]    [,4]  [,5]    [,6]    [,7]    [,8]    [,9]  [,10]
> > > > >>  [1,] ""          "Avg."  "+_ lo" "n"   "Med."  ""      "Avg."  "+_
> > > > >> lo" "n"   "Med."
> > > > >>  [2,] "SiOz"      "44.0"  "1.26"  "375" "44.1"  "Nb"    "4.8"   "6.3"
> > > > >>  "58"  "2.7"
> > > > >>  [3,] "T i O  2"  "0.09"  "0.09"  "561" "0.09"  "Mo(b)" "50"    "30"
> > > > >>  "3"   "35"
> > > > >>  [4,] "A1203"     "2.27"  "1.10"  "375" "2.20"  "Ru(b)" "12.4"  "4.1"
> > > > >>  "3"   "12"
> > > > >>  [5,] "FeO total" "8.43"  "1.14"  "375" "8.19"  "Pd(b)" "3.9"   "2.1"
> > > > >>  "19"  "4.1"
> > > > >>  [6,] "MnO"       "0.14"  "0.03"  "366" "0.14"  "Ag(b)" "6.8"   "8.3"
> > > > >>  "17"  "4.8"
> > > > >>  [7,] "MgO"       "41.4"  "3.00"  "375" "41.2"  "Cd(b)" "41"    "14"
> > > > >>  "16"  "37"
> > > > >>  [8,] "CaO"       "2.15"  "1.11"  "374" "2.20"  "In(b)" "12"    "4"
> > > > >>  "19"  "12"
> > > > >>  [9,] "Na20"      "0.24"  "0.16"  "341" "0.21"  "Sn(b)" "54"    "31"
> > > > >>  "6"   "36"
> > > > >> [10,] "K20"       "0.054" "0.11"  "330" "0.028" "Sb(b)" "3.9"   "3.9"
> > > > >>  "11"  "3.2"
> > > > >> [11,] "P205"      "0.056" "0.11"  "233" "0.030" "Te(b)" "11"    "4"
> > > > >>  "18"  "10"
> > > > >> [12,] "Total"     "98.88" ""      ""    "98.43" "Cs(b)" "10"    "16"
> > > > >>  "17"  "1.5"
> > > > >> [13,] ""          ""      ""      ""    ""      "Ba"    "33"    "52"
> > > > >>  "75"  "17"
> > > > >> [14,] "Mg-value"  "89.8"  "1.1"   "375" "90.0"  "La"    "2.60"  "5.70"
> > > > >>  "208" "0.77"
> > > > >> [15,] "Ca/AI"     "1.28"  "1.6"   "374" "1.35"  "Ce"    "6.29"  "11.7"
> > > > >>  "197" "2.08"
> > > > >> [16,] "AI/Ti"     "22"    "29"    "361" "22"    "Pr"    "0.56"  "0.87"
> > > > >>  "40"  "0.21"
> > > > >> [17,] "F e / M n" "60"    "10"    "366" "59"    "Nd"    "2.67"  "4.31"
> > > > >>  "162" "1.52"
> > > > >> [18,] ""          ""      ""      ""    ""      "Sm"    "0.47"  "0.69"
> > > > >>  "214" "0.25"
> > > > >> [19,] "Li"        "1.5"   "0.3"   "6"   "1.5"   "Eu"    "0.16"  "0.21"
> > > > >>  "201" "0.097"
> > > > >> [20,] "B"         "0.53"  "0.07"  "6"   "0.55"  "Gd"    "0.60"  "0.83"
> > > > >>  "67"  "0.31"
> > > > >> [21,] "C"         "110"   "50"    "13"  "93"    "Tb"    "0.070"
> > > > >> "0.064" "146" "0.056"
> > > > >> [22,] "F"         "88"    "71"    "15"  "100"   "Dy"    "0.51"  "0.35"
> > > > >>  "58"  "0.47"
> > > > >> [23,] "S"         "157"   "77"    "22"  "152"   "Ho"    "0.12"  "0.14"
> > > > >>  "54"  "0.090"
> > > > >> [24,] "C1"        "53"    "45"    "15"  "75"    "Er"    "0.30"  "0.22"
> > > > >>  "52"  "0.28"
> > > > >> [25,] "Sc"        "12.2"  "6.4"   "220" "12.0"  "Tm"    "0.038"
> > > > >> "0.026" "40"  "0.035"
> > > > >> [26,] "V"         "56"    "21"    "132" "53"    "Yb"    "0.26"  "0.14"
> > > > >>  "201" "0.27"
> > > > >> [27,] "Cr"        "2690"  "705"   "325" "2690"  "Lu"    "0.043"
> > > > >> "0.023" "172" "0.045"
> > > > >> [28,] "Co"        "112"   "10"    "166" "111"   "Hf"    "0.27"  "0.30"
> > > > >>  "71"  "0.17"
> > > > >> [29,] "Ni"        "2160"  "304"   "308" "2140"  "Ta"    "0.40"  "0.51"
> > > > >>  "38"  "0.23"
> > > > >> [30,] "Cu"        "11"    "9"     "94"  "9"     "W(b)"  "7.2"   "5.2"
> > > > >>  "6"   "4.0"
> > > > >> [31,] "Zn"        "65"    "20"    "129" "60"    "Re(b)" "0.13"  "0.11"
> > > > >>  "18"  "0.09"
> > > > >> [32,] "Ga"        "2.4"   "1.3"   "49"  "2.4"   "Os(b)" "4.0"   "1.8"
> > > > >>  "18"  "3.7"
> > > > >> [33,] "Ge"        "0.96"  "0.19"  "19"  "0.92"  "Ir(b)" "3.7"   "0.9"
> > > > >>  "34"  "3.0"
> > > > >> [34,] "As"        "0.11"  "0.07"  "7"   "0.10"  "Pt(b)" "7"     "-"
> > > > >>  "1"   "-"
> > > > >> [35,] "Se"        "0.041" "0.056" "18"  "0.025" "Au(b)" "0.65"  "0.53"
> > > > >>  "30"  "0.5"
> > > > >> [36,] "Br"        "0.01"  "0.01"  "6"   "0.01"  "Tl(b)" "1.2"   "1.0"
> > > > >>  "13"  "0.9"
> > > > >> [37,] "Rb"        "1,9"   "4.8"   "97"  "0.38"  "Pb"    "0.16"  "0.11"
> > > > >>  "17"  "0.16"
> > > > >> [38,] "Sr"        "49"    "60"    "110" "20"    "Bi(b)" "1.7"   "0.7"
> > > > >>  "13"  "1.6"
> > > > >> [39,] "Y"         "4.4"   "5.5"   "86"  "3.1"   "Th*"   "0.71"  "1.2"
> > > > >>  "71"  "0.22"
> > > > >> [40,] "Zr"        "21"    "42"    "82"  "8.0"   "U"     "0.12"  "0.23"
> > > > >>  "48"  "0.040"
> > > > >> [[2]][[4]]
> > > > >> [,1]       [,2]                 [,3]     [,4]                  [,5]
> > > > >>  [,6]
> > > > >>  [1,] ""         "Spinel peridotites" ""       "Garnet  peridotites"
> > > > >> ""       "Primitive"
> > > > >>  [2,] ""         "Avg. Meal."         "M-A sp" "M-A gt B-M"
> > > > >> "Jordan" "mantle"
> > > > >>  [3,] "SiO 2"    "44.0 44.1"          "44.15"  "44.99 45.00"
> > > > >> "45.55"  "44.8"
> > > > >>  [4,] "TiO 2"    "0.09 0.09"          "0.07"   "0.06 0.08"
> > > > >> "0.11"   "0.21"
> > > > >>  [5,] "A1203"    "2.27 2.20"          "1.96"   "1.40 1.31"
> > > > >> "1.43"   "4.45"
> > > > >>  [6,] "Cr203"    "0.39 0.39"          "0.44"   "0.32 0.38"
> > > > >> "0.34"   "0.43"
> > > > >>  [7,] "FeOtotal" "8.43 8.19"          "8.28"   "7.89 6.97"
> > > > >> "7.61"   "8.40"
> > > > >>  [8,] "Mn O"     "0.14 0.14"          "0.12"   "0.11 0.13"
> > > > >> "0.11"   "0.14"
> > > > >>  [9,] "MgO"      "41.4 41.2"          "42.25"  "42.60 44.86"
> > > > >> "43.55"  "37.2"
> > > > >> [10,] "NiO"      "0.27 0.27"          "0.27"   "0.26 0.29"
> > > > >> "-"      "0.24"
> > > > >> [11,] "CaO"      "2.15 2.20"          "2.08"   "0.82 0.77"
> > > > >> "1.05"   "3.60"
> > > > >> [12,] "Na  20"   "0.24 0.21"          "0.18"   "0.11 0.09"
> > > > >> "0.14"   "0.34"
> > > > >> [13,] "K 2 0"    "0.054 0.028"        "0.05"   "0.04 0.10"
> > > > >> "0.11"   "0.028"
> > > > >> [14,] "P205"     "0.056 0.030"        "0.02"   "- 0.01"
> > > > >> "-"      "0.022"
> > > > >> [15,] "Total"    "99.49 99.05"        "99.87"  "98.60 100.00"
> > > > >> "100.00" "99.86"
> > > > >> [16,] "Mg-value" "89.8 90.0"          "90.1"   "90.6 92.0"
> > > > >> "91.1"   "88.8"
> > > > >> [17,] "olivine"  "62 63"              "67"     "65 68"
> > > > >> "66"     "56 57"
> > > > >> [18,] "opx"      "24 24"              "22"     "28 25"
> > > > >> "28"     "22 17"
> > > > >> [19,] "cpx"      "12 11"              "9"      "3 2"
> > > > >> "3"      "19 10"
> > > > >> [20,] "spinel"   "2 2"                "2"      "- -"
> > > > >> "-"      "3 -"
> > > > >>
> > > > >> Here is portion of the output for str(MyTables):
> > > > >>
> > > > >> str(MyTables)
> > > > >>
> > > > >> List of 3
> > > > >>  $ :List of 12
> > > > >> $ : chr [1:3, 1:2] "south of the artificial lake Lokka. Intrusive
> > > > >> complexes" "of alkaline rocks are found at Sokli (phosphorite-bear-"
> > > > >> "ing and a possible Nb-occurrence) in Finland, and at" "(Eriksson,
> > > > >> 1992). During this period, Northern Europe" ...
> > > > >>   ..$ : chr [1:55, 1:15] "Element" "Ag" "Al" "Al_XRF" ...
> > > > >>   ..$ : chr [1:56, 1:2] "in the till is mainly of local origin,
> > > > >> although some cob-" "bles and boulders may have been transported over
> > > > >> sev-" "eral kilometres. The moraine formations in the study" "area are
> > > > >> mostly gravelly and sandy tills, locally hum-" ...
> > > > >>   ..$ : chr [1:53, 1:2] "requisites. PCA accounts for maximum variance
> > > > >> of all" "variables, while FA is based on the correlation structure"
> > > > >> "of the variables. The model of factor analysis allows that" "the
> > > > >> common factors do not explain the total variation of" ...
> > > > >>   ..$ : chr [1:54, 1:7] "lished examples of the use of factor
> > > > >> analysis, it is neglec-" "ted that regional geochemical (and
> > > > >> environmental) data" "almost never follow a normal distribution.
> > > > >> Continuing Method" "with factor analysis in such a case must lead to
> > > > >> biased" ...
> > > > >>   ..$ : chr [1:16, 1:2] "shows the factor loadings of the different
> > > > >> variables" "entering each factor. Names of variables with an abso-"
> > > > >> "lute value of the loadings <0.3 are not plotted. Fig. 5" "shows 8
> > > > >> results of factor analyses using a selection of all" ...
> > > > >>   ..$ : chr [1:21, 1:2] "pretable results, notwithstanding the fact
> > > > >> that on the" "basis of the foregoing discussion it should probably
> > > > >> not" "be used with these data. Do these results warrant the use" "of a
> > > > >> quite work-intensive method? Unfortunately not," ...
> > > > >>   ..$ : chr [1:55, 1:8] "" "Ag" "Al" "Al_XRF" ...
> > > > >>   ..$ : chr [1:23, 1:2] "addition, geochemical reasoning (e.g.
> > > > >> geochemical asso-" "ciations and/or pathfinder elements for different
> > > > >> types of" "ore deposits) was used to select further sub-sets of vari-"
> > > > >> "ables. In geochemistry, the selection of elements entered" ...
> > > > >>   ..$ : chr [1:55, 1:2] "Fig. 10C cuts several geological units, and
> > > > >> is most likely" "indicative of alteration processes related to a
> > > > >> deep-" "seated fault. It was revealed again in a factor analysis"
> > > > >> "carried out with all those elements extracted by aqua" ...
> > > > >>   ..$ : chr [1:50, 1:2] "well justified in stating that it is not very
> > > > >> scientific to" "play with the selection of elements and number of
> > > > >> fac-" "tors extracted until one
> > > > >> ?\200\230?\200\230finds?\200\231?\200\231 an
> > > > >> ?\200\230?\200\230interesting?\200\231?\200\231 result." "On the other
> > > > >> hand, even all the different results pre-" ...
> > > > >>   ..$ : chr [1:24, 1:2] "Niemel??, J., Ekman, I., Lukashov, A. (Eds.),
> > > > >> 1993. Quaternary" "Deposits of Finland and Northwestern Part of
> > > > >> Russian Fed-" "eration and Their Resources 1:1,000,000. Geological
> > > > >> Survey" "of Finland, Espoo, Finland." ...
> > > > >>  $ :List of 15
> > > > >>
> > > > >> ______________________________________________
> > > > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > > > >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > >> and provide commented, minimal, self-contained, reproducible code.
> > > >
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.


From @@r@h@go@lee @ending from gm@il@com  Thu Dec 20 14:59:43 2018
From: @@r@h@go@lee @ending from gm@il@com (Sarah Goslee)
Date: Thu, 20 Dec 2018 08:59:43 -0500
Subject: [R] extract hyperplan from e1071 model
In-Reply-To: <CAMk+s2SQPNv4BUSpyo71Jmr1k+Vj5s2BjD97rowoBQfs_o977g@mail.gmail.com>
References: <CAMk+s2SQPNv4BUSpyo71Jmr1k+Vj5s2BjD97rowoBQfs_o977g@mail.gmail.com>
Message-ID: <CAM_vjunBJbNY4M6mkEkx5SB7omWL7ux1CsuWMxJW0eBjz+dajw@mail.gmail.com>

Hi,

According to the help for svm, which you probably should have started
with, SV contains the support vectors, and index contains the position
of the support vectors in the data matrix.

As for plotting, plot.svm lets you pass additional options to plot so
that you can customize the plot to your tastes. I'm not sure if
add=TRUE is a useful option there, but you should try it. If that
doesn't meet your needs, R is open source - you can easily look at the
code for plot.svm and see what you need (basically the predicted
values on agrid, if I'm understanding your goal correctly).

Sarah

On Thu, Dec 20, 2018 at 5:16 AM Luigi Marongiu <marongiu.luigi at gmail.com> wrote:
>
> Dear all,
> I am using the package e1071 for modeling SVM. I obtain a model from
> the data and I can plot the results; the plot shows the support
> vectors (as 'X's) and the shaded areas as it should be.
> However, I don't like the plot generated from the model and I would
> like instead to have more control upon the plotting; in particular, I
> would like to draw the hyperplane on plots I have already made from
> the data available.
> Is there a way to extract the values that are used to draw the hyperplane?
> That is: plot(model) -- where model is obtained from svm() -- draws an
> area in blue and one in red based on some values provided by model;
> can I get these values so I can plot a line in a pre-existing plot?
> Also, it is possible to extract the positions of the support vectors?
> The names of the model generated by svm() are:
> > names(mod)
>  [1] "call"            "type"            "kernel"          "cost"
>  [5] "degree"          "gamma"           "coef0"           "nu"
>  [9] "epsilon"         "sparse"          "scaled"          "x.scale"
> [13] "y.scale"         "nclasses"        "levels"          "tot.nSV"
> [17] "nSV"             "labels"          "SV"              "index"
> [21] "rho"             "compprob"        "probA"           "probB"
> [25] "sigma"           "coefs"           "na.action"       "fitted"
> [29] "decision.values" "terms"
>
> Which one should I look at?
> Thank you
> --
> Best regards,
> Luigi
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Sarah Goslee (she/her)
http://www.sarahgoslee.com


From @@r@h@go@lee @ending from gm@il@com  Thu Dec 20 15:14:21 2018
From: @@r@h@go@lee @ending from gm@il@com (Sarah Goslee)
Date: Thu, 20 Dec 2018 09:14:21 -0500
Subject: [R] extract hyperplan from e1071 model
In-Reply-To: <CAMk+s2R2FSXitJfzXPVmEauSfmDvgkiqFrGprcb6hhRU+HGj5g@mail.gmail.com>
References: <CAMk+s2SQPNv4BUSpyo71Jmr1k+Vj5s2BjD97rowoBQfs_o977g@mail.gmail.com>
 <CAM_vjunBJbNY4M6mkEkx5SB7omWL7ux1CsuWMxJW0eBjz+dajw@mail.gmail.com>
 <CAMk+s2R2FSXitJfzXPVmEauSfmDvgkiqFrGprcb6hhRU+HGj5g@mail.gmail.com>
Message-ID: <CAM_vjum63bu3QuejJOm1oi7tZ-Uh=67eWmz=KfuQAmBDrU7wBw@mail.gmail.com>

Hi,

Please don't forget to copy R-help on your reply - other people are
likely to have more insight.

My understanding is that you want to replicate the shaded polygons
produced by plot.svm on your own plot. That's why I suggested you
first try add=TRUE, and if that doesn't work, then look at the code
for plot.svm so you can see how the predicted values are calculated on
a grid to be plotted. R is open source - you can extract the section
from that function that does what you want, and make your own code to
add it to another plot.

I have no idea what "the tutorial" is; I suggested two things you
could try based on the help and my understanding of R.

Sarah

On Thu, Dec 20, 2018 at 9:07 AM Luigi Marongiu <marongiu.luigi at gmail.com> wrote:
>
> Dear Sarah,
> I have looked at the tutorial but I did not find what I was looking
> for. Essentially, I would like to extract the data that make the
> hyperplane (it should be a matrix of X,Y value, for a bidimensional
> plane, I guess) so I can plot a line in another plot. I tried with
> model$SV but it is not the right one.
> Luigi
>
> On Thu, Dec 20, 2018 at 2:59 PM Sarah Goslee <sarah.goslee at gmail.com> wrote:
> >
> > Hi,
> >
> > According to the help for svm, which you probably should have started
> > with, SV contains the support vectors, and index contains the position
> > of the support vectors in the data matrix.
> >
> > As for plotting, plot.svm lets you pass additional options to plot so
> > that you can customize the plot to your tastes. I'm not sure if
> > add=TRUE is a useful option there, but you should try it. If that
> > doesn't meet your needs, R is open source - you can easily look at the
> > code for plot.svm and see what you need (basically the predicted
> > values on agrid, if I'm understanding your goal correctly).
> >
> > Sarah
> >
> > On Thu, Dec 20, 2018 at 5:16 AM Luigi Marongiu <marongiu.luigi at gmail.com> wrote:
> > >
> > > Dear all,
> > > I am using the package e1071 for modeling SVM. I obtain a model from
> > > the data and I can plot the results; the plot shows the support
> > > vectors (as 'X's) and the shaded areas as it should be.
> > > However, I don't like the plot generated from the model and I would
> > > like instead to have more control upon the plotting; in particular, I
> > > would like to draw the hyperplane on plots I have already made from
> > > the data available.
> > > Is there a way to extract the values that are used to draw the hyperplane?
> > > That is: plot(model) -- where model is obtained from svm() -- draws an
> > > area in blue and one in red based on some values provided by model;
> > > can I get these values so I can plot a line in a pre-existing plot?
> > > Also, it is possible to extract the positions of the support vectors?
> > > The names of the model generated by svm() are:
> > > > names(mod)
> > >  [1] "call"            "type"            "kernel"          "cost"
> > >  [5] "degree"          "gamma"           "coef0"           "nu"
> > >  [9] "epsilon"         "sparse"          "scaled"          "x.scale"
> > > [13] "y.scale"         "nclasses"        "levels"          "tot.nSV"
> > > [17] "nSV"             "labels"          "SV"              "index"
> > > [21] "rho"             "compprob"        "probA"           "probB"
> > > [25] "sigma"           "coefs"           "na.action"       "fitted"
> > > [29] "decision.values" "terms"
> > >
> > > Which one should I look at?
> > > Thank you
> > > --
> > > Best regards,
> > > Luigi
> > >



-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From @lobo@leu @ending from gm@il@com  Thu Dec 20 12:00:04 2018
From: @lobo@leu @ending from gm@il@com (Agustin Lobo)
Date: Thu, 20 Dec 2018 12:00:04 +0100
Subject: [R] Problem with system() and source on linux
Message-ID: <CALPC6DN7=3J_A8a6871sLHzsjFbNYFLEKxY8ga9p25jenDG1dQ@mail.gmail.com>

Hi!
I quite often use system() to run other programs from within R, but
have just hitted
a problem:

For a given program, I need to set up its environment, which I normally do with
source /home/alobo/OTB-6.6.0-Linux64/otbenv.profile
from the terminal.
Now, when I try to do the same from within R, I get:

> system("source /home/alobo/OTB-6.6.0-Linux64/otbenv.profile", intern=TRUE)
sh: 1: source: not found
Error in system("source /home/alobo/OTB-6.6.0-Linux64/otbenv.profile",  :
  error in running command

I need this command to set the environment before I actually run the
program. My idea was saving a simple script from within R  in which
the first line would be
source /home/alobo/OTB-6.6.0-Linux64/otbenv.profile

and then run the script with system(), but I get that odd error with
source. I thought source was just
a plain linux command, how can it be "not found" from within system()?

Any help much appreciated,
Thanks


-- 
Agustin Lobo
aloboaleu at gmail.com


From p@ulbern@l07 @ending from gm@il@com  Thu Dec 20 16:11:20 2018
From: p@ulbern@l07 @ending from gm@il@com (Paul Bernal)
Date: Thu, 20 Dec 2018 10:11:20 -0500
Subject: [R] Reformatting output of Forecasts generated by mlp model
Message-ID: <CAMOcQfP_7n94EfJFn9ZmoezEECG5f=Qkj3AEjEAkuowdi2tg_w@mail.gmail.com>

Dear friends,

Hope you are doing great. I am using the multiple layer perceptron model
(provided in R?s mlp() function for time series forecasting, but I don?t
know how to reformat the output forecasts generated.

mydata <- dput(datframe$Transits)

> dput(datframe$Transits)
c(77L, 75L, 85L, 74L, 73L, 96L, 82L, 90L, 91L, 81L, 81L, 77L,
84L, 81L, 82L, 86L, 81L, 81L, 83L, 88L, 88L, 92L, 97L, 89L, 96L,
94L, 94L, 95L, 92L, 94L, 95L, 95L, 87L, 102L, 94L, 91L, 93L,
86L, 96L, 85L, 81L, 84L, 88L, 91L, 89L, 89L, 93L, 83L, 92L, 92L,
76L, 98L, 80L, 95L, 89L, 92L, 96L, 86L, 98L, 84L, 90L, 95L, 90L,
99L, 85L, 91L, 90L, 88L, 97L, 93L, 97L, 87L, 92L, 87L, 86L, 85L,
82L, 90L, 89L, 101L, 94L, 92L, 109L, 101L, 103L, 96L, 89L, 102L,
87L, 101L, 100L, 99L, 101L, 98L, 101L, 90L, 106L, 90L, 99L, 105L,
91L, 96L, 91L, 96L, 93L, 101L, 105L, 98L, 110L, 100L, 101L, 106L,
99L, 111L, 114L, 112L, 113L, 120L, 105L, 111L, 114L, 111L, 118L,
115L, 108L, 120L, 119L, 120L, 118L, 117L, 121L, 111L, 114L, 107L,
121L, 109L, 106L, 116L, 105L, 119L, 120L, 123L, 126L, 117L, 127L,
128L, 132L, 138L, 120L, 132L, 134L, 136L, 144L, 152L, 155L, 146L,
155L, 138L, 141L, 146L, 123L, 133L, 123L, 137L, 133L, 143L, 132L,
126L, 134L, 129L, 138L, 134L, 132L, 139L, 130L, 152L, 150L, 153L,
161L, 152L, 154L, 154L, 138L, 149L, 137L, 144L, 146L, 152L, 140L,
151L, 168L, 148L, 157L, 152L, 153L, 166L, 157L, 156L, 166L, 168L,
179L, 188L, 190L, 185L, 184L, 185L, 202L, 191L, 175L, 197L, 187L,
195L, 204L, 218L, 220L, 212L, 220L, 211L, 221L, 204L, 196L, 209L,
205L, 217L, 211L, 212L, 224L, 206L, 225L, 206L, 219L, 232L, 220L,
242L, 241L, 261L, 252L, 261L, 269L, 251L, 264L, 261L, 266L, 274L,
236L, 270L, 263L, 276L, 276L, 300L, 303L, 301L, 318L, 294L, 308L,
308L, 269L, 303L, 302L, 318L, 282L, 311L, 305L, 304L, 309L, 298L,
295L, 295L, 281L, 280L, 287L, 313L, 276L, 296L, 307L, 307L, 309L,
287L, 286L, 290L, 261L, 285L, 279L, 286L, 284L, 267L, 271L, 259L,
268L, 243L, 242L, 237L, 208L, 250L, 237L, 267L, 257L, 276L, 277L,
269L, 282L, 264L, 270L, 270L, 251L, 272L, 271L, 288L, 266L, 283L,
266L, 270L, 282L, 272L, 264L, 269L, 253L, 269L, 283L, 288L, 275L,
301L, 292L, 283L, 287L, 261L, 265L, 269L, 234L, 251L, 261L, 262L,
249L, 256L, 255L, 253L, 253L, 233L, 234L, 235L, 217L, 244L, 232L,
261L, 236L, 252L, 242L, 252L, 251L, 230L, 240L, 254L, 226L, 267L,
245L, 263L, 261L, 286L, 281L, 265L, 274L, 250L, 260L, 265L, 242L,
251L, 249L, 251L, 247L, 248L, 234L, 206L, 219L, 194L, 218L, 209L,
192L, 207L, 200L, 208L, 208L, 209L, 213L, 216L, 219L, 195L, 217L,
217L, 197L, 210L, 211L, 229L, 232L, 227L, 233L, 217L)

TransitsDat <- ts(mydata, start=(1985,10), end=c(2018,9), frequency=12)

Model <- mlp(TransitsDat)

ModelForecasts <- forecast(Model, h=10)

And the output I get is this:

           Jan         Feb         Mar         Apr         May
Jun         Jul Aug Sep         Oct         Nov         Dec
2018
224.9970134 221.7932717 220.2698789
2019 223.8440115 219.3309631 221.5382052 221.5720276 222.0963057
223.8392450 224.0982199

But I would like to have the results in tabular form, where the first
column is the date with format mmm-yyyy (for example jan-2019) and the
second row are the actual forecasts.

Like this
Date           TransitForecast
Jan-2019     230
Feb-2019    217
etc.

Any guidance will be greatly appreciated,

Best regards,

Paul

	[[alternative HTML version deleted]]


From @@r@h@go@lee @ending from gm@il@com  Thu Dec 20 16:13:38 2018
From: @@r@h@go@lee @ending from gm@il@com (Sarah Goslee)
Date: Thu, 20 Dec 2018 10:13:38 -0500
Subject: [R] Problem with system() and source on linux
In-Reply-To: <CALPC6DN7=3J_A8a6871sLHzsjFbNYFLEKxY8ga9p25jenDG1dQ@mail.gmail.com>
References: <CALPC6DN7=3J_A8a6871sLHzsjFbNYFLEKxY8ga9p25jenDG1dQ@mail.gmail.com>
Message-ID: <CAM_vju=G4LtGGWN8EawiqkVODr84s7WH_QS2Vwfj0Z2Cbv0B5A@mail.gmail.com>

Hi,

I can tell you what the problem is:

You're probably running bash at the terminal command line, as I am:

[sarahg at localhost]$ echo $0
bash

but the R system function uses sh

&gt; system("echo $0")
sh

The bash shell has a source command; the sh shell doesn't. See here
for a possible solution:

https://stackoverflow.com/questions/4732200/replacement-for-source-in-sh


I don't know if there's a way to specify bash shell in system(); a
very cursory googling didn't find anything. If you find a way, please
report back.

Sarah

On Thu, Dec 20, 2018 at 10:00 AM Agustin Lobo <aloboaleu at gmail.com> wrote:
>
> Hi!
> I quite often use system() to run other programs from within R, but
> have just hitted
> a problem:
>
> For a given program, I need to set up its environment, which I normally do with
> source /home/alobo/OTB-6.6.0-Linux64/otbenv.profile
> from the terminal.
> Now, when I try to do the same from within R, I get:
>
> > system("source /home/alobo/OTB-6.6.0-Linux64/otbenv.profile", intern=TRUE)
> sh: 1: source: not found
> Error in system("source /home/alobo/OTB-6.6.0-Linux64/otbenv.profile",  :
>   error in running command
>
> I need this command to set the environment before I actually run the
> program. My idea was saving a simple script from within R  in which
> the first line would be
> source /home/alobo/OTB-6.6.0-Linux64/otbenv.profile
>
> and then run the script with system(), but I get that odd error with
> source. I thought source was just
> a plain linux command, how can it be "not found" from within system()?
>
> Any help much appreciated,
> Thanks
>
>
> --
> Agustin Lobo
> aloboaleu at gmail.com
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From wdunl@p @ending from tibco@com  Thu Dec 20 16:15:02 2018
From: wdunl@p @ending from tibco@com (William Dunlap)
Date: Thu, 20 Dec 2018 07:15:02 -0800
Subject: [R] Problem with system() and source on linux
In-Reply-To: <CALPC6DN7=3J_A8a6871sLHzsjFbNYFLEKxY8ga9p25jenDG1dQ@mail.gmail.com>
References: <CALPC6DN7=3J_A8a6871sLHzsjFbNYFLEKxY8ga9p25jenDG1dQ@mail.gmail.com>
Message-ID: <CAF8bMcZkB5CrjxCZmV3dKXviyL_W4PPU2n3n6pC0Qc-OOoqYbQ@mail.gmail.com>

Isn't 'source' a csh (tcsh, etc.) command?  The sh (bash, etc.) command is
a period, but
you probably will need to use sh constructs in the file (like
VAR=value;exportVAR)
instead of csh constructs (like setenv VAR value).

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Thu, Dec 20, 2018 at 7:00 AM Agustin Lobo <aloboaleu at gmail.com> wrote:

> Hi!
> I quite often use system() to run other programs from within R, but
> have just hitted
> a problem:
>
> For a given program, I need to set up its environment, which I normally do
> with
> source /home/alobo/OTB-6.6.0-Linux64/otbenv.profile
> from the terminal.
> Now, when I try to do the same from within R, I get:
>
> > system("source /home/alobo/OTB-6.6.0-Linux64/otbenv.profile",
> intern=TRUE)
> sh: 1: source: not found
> Error in system("source /home/alobo/OTB-6.6.0-Linux64/otbenv.profile",  :
>   error in running command
>
> I need this command to set the environment before I actually run the
> program. My idea was saving a simple script from within R  in which
> the first line would be
> source /home/alobo/OTB-6.6.0-Linux64/otbenv.profile
>
> and then run the script with system(), but I get that odd error with
> source. I thought source was just
> a plain linux command, how can it be "not found" from within system()?
>
> Any help much appreciated,
> Thanks
>
>
> --
> Agustin Lobo
> aloboaleu at gmail.com
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @@r@h@go@lee @ending from gm@il@com  Thu Dec 20 16:16:25 2018
From: @@r@h@go@lee @ending from gm@il@com (Sarah Goslee)
Date: Thu, 20 Dec 2018 10:16:25 -0500
Subject: [R] Problem with system() and source on linux
In-Reply-To: <CAM_vju=G4LtGGWN8EawiqkVODr84s7WH_QS2Vwfj0Z2Cbv0B5A@mail.gmail.com>
References: <CALPC6DN7=3J_A8a6871sLHzsjFbNYFLEKxY8ga9p25jenDG1dQ@mail.gmail.com>
 <CAM_vju=G4LtGGWN8EawiqkVODr84s7WH_QS2Vwfj0Z2Cbv0B5A@mail.gmail.com>
Message-ID: <CAM_vjukkQnXguzSJ1jqQ2NgksmhcHTO6yOB98k3hZ_RnVb9g4g@mail.gmail.com>

Actually, here's another possibility:

system('bash -c "source filename"')

On Thu, Dec 20, 2018 at 10:13 AM Sarah Goslee <sarah.goslee at gmail.com> wrote:
>
> Hi,
>
> I can tell you what the problem is:
>
> You're probably running bash at the terminal command line, as I am:
>
> [sarahg at localhost]$ echo $0
> bash
>
> but the R system function uses sh
>
> &gt; system("echo $0")
> sh
>
> The bash shell has a source command; the sh shell doesn't. See here
> for a possible solution:
>
> https://stackoverflow.com/questions/4732200/replacement-for-source-in-sh
>
>
> I don't know if there's a way to specify bash shell in system(); a
> very cursory googling didn't find anything. If you find a way, please
> report back.
>
> Sarah
>
> On Thu, Dec 20, 2018 at 10:00 AM Agustin Lobo <aloboaleu at gmail.com> wrote:
> >
> > Hi!
> > I quite often use system() to run other programs from within R, but
> > have just hitted
> > a problem:
> >
> > For a given program, I need to set up its environment, which I normally do with
> > source /home/alobo/OTB-6.6.0-Linux64/otbenv.profile
> > from the terminal.
> > Now, when I try to do the same from within R, I get:
> >
> > > system("source /home/alobo/OTB-6.6.0-Linux64/otbenv.profile", intern=TRUE)
> > sh: 1: source: not found
> > Error in system("source /home/alobo/OTB-6.6.0-Linux64/otbenv.profile",  :
> >   error in running command
> >
> > I need this command to set the environment before I actually run the
> > program. My idea was saving a simple script from within R  in which
> > the first line would be
> > source /home/alobo/OTB-6.6.0-Linux64/otbenv.profile
> >
> > and then run the script with system(), but I get that odd error with
> > source. I thought source was just
> > a plain linux command, how can it be "not found" from within system()?
> >
> > Any help much appreciated,
> > Thanks
> >
> >
> > --
> > Agustin Lobo
> > aloboaleu at gmail.com
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Sarah Goslee (she/her)
> http://www.numberwright.com



-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From jdnewmil @ending from dcn@d@vi@@c@@u@  Thu Dec 20 16:13:06 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Thu, 20 Dec 2018 07:13:06 -0800
Subject: [R] Problem with system() and source on linux
In-Reply-To: <CALPC6DN7=3J_A8a6871sLHzsjFbNYFLEKxY8ga9p25jenDG1dQ@mail.gmail.com>
References: <CALPC6DN7=3J_A8a6871sLHzsjFbNYFLEKxY8ga9p25jenDG1dQ@mail.gmail.com>
Message-ID: <CA3435DE-69D4-49AF-AD5F-0AC0A9858AC0@dcn.davis.ca.us>

Read the man page for bash... source is built-in to the shell interpreter. You should invoke a single shell instance that first sources the profile and then executes the program... but this has nothing to do with R... this would be true for any program invoking an external program on a POSIX-compatible operating system (i.e. not Windows), and so is OT here.

On December 20, 2018 3:00:04 AM PST, Agustin Lobo <aloboaleu at gmail.com> wrote:
>Hi!
>I quite often use system() to run other programs from within R, but
>have just hitted
>a problem:
>
>For a given program, I need to set up its environment, which I normally
>do with
>source /home/alobo/OTB-6.6.0-Linux64/otbenv.profile
>from the terminal.
>Now, when I try to do the same from within R, I get:
>
>> system("source /home/alobo/OTB-6.6.0-Linux64/otbenv.profile",
>intern=TRUE)
>sh: 1: source: not found
>Error in system("source /home/alobo/OTB-6.6.0-Linux64/otbenv.profile", 
>:
>  error in running command
>
>I need this command to set the environment before I actually run the
>program. My idea was saving a simple script from within R  in which
>the first line would be
>source /home/alobo/OTB-6.6.0-Linux64/otbenv.profile
>
>and then run the script with system(), but I get that odd error with
>source. I thought source was just
>a plain linux command, how can it be "not found" from within system()?
>
>Any help much appreciated,
>Thanks

-- 
Sent from my phone. Please excuse my brevity.


From krylov@r00t @ending from gm@il@com  Thu Dec 20 16:19:47 2018
From: krylov@r00t @ending from gm@il@com (Ivan Krylov)
Date: Thu, 20 Dec 2018 18:19:47 +0300
Subject: [R] Problem with system() and source on linux
In-Reply-To: <CALPC6DN7=3J_A8a6871sLHzsjFbNYFLEKxY8ga9p25jenDG1dQ@mail.gmail.com>
References: <CALPC6DN7=3J_A8a6871sLHzsjFbNYFLEKxY8ga9p25jenDG1dQ@mail.gmail.com>
Message-ID: <20181220181947.6e0ba6ad@trisector>

On Thu, 20 Dec 2018 12:00:04 +0100
Agustin Lobo <aloboaleu at gmail.com> wrote:

> For a given program, I need to set up its environment, which I
> normally do with source /home/alobo/OTB-6.6.0-Linux64/otbenv.profile
> from the terminal.

The problem with this approach is that in Unix-like systems, child
processes cannot directly modify environment variables of their
parents (or any other processes besides those they are about to spawn).
In /bin/bash, `source` is a special builtin command that runs without
spawning child processes - therefore being able to modify the
environment of the *current* process.

> Now, when I try to do the same from within R, I get:
> 
> > system("source /home/alobo/OTB-6.6.0-Linux64/otbenv.profile",
> > intern=TRUE)  
> sh: 1: source: not found

Moreover, `source` is a bash-ism and R launches the `system` commands
using /bin/sh, which might be different from /bin/bash. The
POSIX-correct way to include a batch file in the current shell session
is a dot: `. /home/alobo/OTB-6.6.0-Linux64/otbenv.profile`
http://pubs.opengroup.org/onlinepubs/9699919799.2018edition/utilities/V3_chap02.html#dot

Since this way you can only modify the environment of the temporary
shell process spawned by `system`, you have set up the environment and
launch the executable in a single `system` command:

system(". /home/alobo/OTB-6.6.0-Linux64/otbenv.profile; exec
otbcli_something")

-- 
Best regards,
Ivan


From 538280 @ending from gm@il@com  Thu Dec 20 18:10:08 2018
From: 538280 @ending from gm@il@com (Greg Snow)
Date: Thu, 20 Dec 2018 10:10:08 -0700
Subject: [R] test of independence
In-Reply-To: <CAPV1RAAi_NkUmUvRMahOban1o9RHrc42TwCxvriJ9iuYJeJP-A@mail.gmail.com>
References: <CAPV1RAAi_NkUmUvRMahOban1o9RHrc42TwCxvriJ9iuYJeJP-A@mail.gmail.com>
Message-ID: <CAFEqCdxOWGBPexrCs0HXe6mutA2N+3OaNTm4Uq6-1LLxz=gWFQ@mail.gmail.com>

The basic test of independence for a table based on the Chi-squared
distribution can be done using the `chisq.test` function.  This is in
the stats package which is installed and loaded by default, so you
don't need to do anything additional.  There is also the `fisher.test`
function for Fisher's exact test (similar hypotheses, different
methodology and assumptions, may be really slow on your table).

If you need more than the basics provided in those functions, then a
search of CRAN may be helpful, or give us more detail to be able to
help.

On Thu, Dec 20, 2018 at 12:08 AM km <srikrishnamohan at gmail.com> wrote:
>
> Dear All,
>
> How do I do a test of independence with 16x16 table of counts.
> Please suggest.
>
> Regards,
> KM
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From bgunter@4567 @ending from gm@il@com  Thu Dec 20 23:37:15 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Thu, 20 Dec 2018 14:37:15 -0800
Subject: [R] Reformatting output of Forecasts generated by mlp model
In-Reply-To: <CAMOcQfP_7n94EfJFn9ZmoezEECG5f=Qkj3AEjEAkuowdi2tg_w@mail.gmail.com>
References: <CAMOcQfP_7n94EfJFn9ZmoezEECG5f=Qkj3AEjEAkuowdi2tg_w@mail.gmail.com>
Message-ID: <CAGxFJbR-JMLcf4s74_Pa_vVBpK5ykiq0s+oJ7W+vNSDAyztgGQ@mail.gmail.com>

The printed output you have shown is meaningless (and maybe mangled, since
you posted in HTML): it is the result of a call to a print method for the
forecast object. You need to examine that object (e.g. via str()) and/or
the print method to see how to extract the data you want in whatever form
you want them. If these remarks don't make sense to you, I suggest you
spend some time with an R tutorial or two to learn about (S3, I assume)
objects and methods and how R uses these for printing output. The important
point is: what you see in printed output may not at "look like" the
structure of the object from which the output is obtained.

You have also failed to tell us what package(s) you are using, which is
part of the requested minimal info. There appear to be at least two that
seem relevant.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Dec 20, 2018 at 7:12 AM Paul Bernal <paulbernal07 at gmail.com> wrote:

> Dear friends,
>
> Hope you are doing great. I am using the multiple layer perceptron model
> (provided in R?s mlp() function for time series forecasting, but I don?t
> know how to reformat the output forecasts generated.
>
> mydata <- dput(datframe$Transits)
>
> > dput(datframe$Transits)
> c(77L, 75L, 85L, 74L, 73L, 96L, 82L, 90L, 91L, 81L, 81L, 77L,
> 84L, 81L, 82L, 86L, 81L, 81L, 83L, 88L, 88L, 92L, 97L, 89L, 96L,
> 94L, 94L, 95L, 92L, 94L, 95L, 95L, 87L, 102L, 94L, 91L, 93L,
> 86L, 96L, 85L, 81L, 84L, 88L, 91L, 89L, 89L, 93L, 83L, 92L, 92L,
> 76L, 98L, 80L, 95L, 89L, 92L, 96L, 86L, 98L, 84L, 90L, 95L, 90L,
> 99L, 85L, 91L, 90L, 88L, 97L, 93L, 97L, 87L, 92L, 87L, 86L, 85L,
> 82L, 90L, 89L, 101L, 94L, 92L, 109L, 101L, 103L, 96L, 89L, 102L,
> 87L, 101L, 100L, 99L, 101L, 98L, 101L, 90L, 106L, 90L, 99L, 105L,
> 91L, 96L, 91L, 96L, 93L, 101L, 105L, 98L, 110L, 100L, 101L, 106L,
> 99L, 111L, 114L, 112L, 113L, 120L, 105L, 111L, 114L, 111L, 118L,
> 115L, 108L, 120L, 119L, 120L, 118L, 117L, 121L, 111L, 114L, 107L,
> 121L, 109L, 106L, 116L, 105L, 119L, 120L, 123L, 126L, 117L, 127L,
> 128L, 132L, 138L, 120L, 132L, 134L, 136L, 144L, 152L, 155L, 146L,
> 155L, 138L, 141L, 146L, 123L, 133L, 123L, 137L, 133L, 143L, 132L,
> 126L, 134L, 129L, 138L, 134L, 132L, 139L, 130L, 152L, 150L, 153L,
> 161L, 152L, 154L, 154L, 138L, 149L, 137L, 144L, 146L, 152L, 140L,
> 151L, 168L, 148L, 157L, 152L, 153L, 166L, 157L, 156L, 166L, 168L,
> 179L, 188L, 190L, 185L, 184L, 185L, 202L, 191L, 175L, 197L, 187L,
> 195L, 204L, 218L, 220L, 212L, 220L, 211L, 221L, 204L, 196L, 209L,
> 205L, 217L, 211L, 212L, 224L, 206L, 225L, 206L, 219L, 232L, 220L,
> 242L, 241L, 261L, 252L, 261L, 269L, 251L, 264L, 261L, 266L, 274L,
> 236L, 270L, 263L, 276L, 276L, 300L, 303L, 301L, 318L, 294L, 308L,
> 308L, 269L, 303L, 302L, 318L, 282L, 311L, 305L, 304L, 309L, 298L,
> 295L, 295L, 281L, 280L, 287L, 313L, 276L, 296L, 307L, 307L, 309L,
> 287L, 286L, 290L, 261L, 285L, 279L, 286L, 284L, 267L, 271L, 259L,
> 268L, 243L, 242L, 237L, 208L, 250L, 237L, 267L, 257L, 276L, 277L,
> 269L, 282L, 264L, 270L, 270L, 251L, 272L, 271L, 288L, 266L, 283L,
> 266L, 270L, 282L, 272L, 264L, 269L, 253L, 269L, 283L, 288L, 275L,
> 301L, 292L, 283L, 287L, 261L, 265L, 269L, 234L, 251L, 261L, 262L,
> 249L, 256L, 255L, 253L, 253L, 233L, 234L, 235L, 217L, 244L, 232L,
> 261L, 236L, 252L, 242L, 252L, 251L, 230L, 240L, 254L, 226L, 267L,
> 245L, 263L, 261L, 286L, 281L, 265L, 274L, 250L, 260L, 265L, 242L,
> 251L, 249L, 251L, 247L, 248L, 234L, 206L, 219L, 194L, 218L, 209L,
> 192L, 207L, 200L, 208L, 208L, 209L, 213L, 216L, 219L, 195L, 217L,
> 217L, 197L, 210L, 211L, 229L, 232L, 227L, 233L, 217L)
>
> TransitsDat <- ts(mydata, start=(1985,10), end=c(2018,9), frequency=12)
>
> Model <- mlp(TransitsDat)
>
> ModelForecasts <- forecast(Model, h=10)
>
> And the output I get is this:
>
>            Jan         Feb         Mar         Apr         May
> Jun         Jul Aug Sep         Oct         Nov         Dec
> 2018
> 224.9970134 221.7932717 220.2698789
> 2019 223.8440115 219.3309631 221.5382052 221.5720276 222.0963057
> 223.8392450 224.0982199
>
> But I would like to have the results in tabular form, where the first
> column is the date with format mmm-yyyy (for example jan-2019) and the
> second row are the actual forecasts.
>
> Like this
> Date           TransitForecast
> Jan-2019     230
> Feb-2019    217
> etc.
>
> Any guidance will be greatly appreciated,
>
> Best regards,
>
> Paul
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From giucilli@ @ending from gm@il@com  Fri Dec 21 16:37:54 2018
From: giucilli@ @ending from gm@il@com (Giuseppe Cillis)
Date: Fri, 21 Dec 2018 16:37:54 +0100
Subject: [R] =?utf-8?q?Problem_with_Kruskal=E2=80=93Wallis_test?=
Message-ID: <CAE43vpgKp0ThqqHm7YBB=XL66q3zUf=8f_XhgHPBMqsShgG1gQ@mail.gmail.com>

Dear all,
I am a beginner with R (and also with the statistics) for which I hope to
be clear.
I should do this non-parametric test on data I extracted from maps.
In practice I have a column that represents the landscape Dynamics of a
certain time period (there are 3 dynamics, each of them marked by the
number 1, 2 or 3) and the other column with the values of a topographic
variable (for example the slope) . In all, there are more than 90,000 pairs
of values.
Going to do the test in R, for all the dynamics and for all the variables,
I get out of the values of chi-square elevated (even in the order of
thousands) and a p-value always <2.2e-16 .... why? Where can the error be? in
the script or in the test approach?
Thanks in advance

	[[alternative HTML version deleted]]


From jenny@liu00 @ending from gm@il@com  Sat Dec 22 05:58:56 2018
From: jenny@liu00 @ending from gm@il@com (Jenny Liu)
Date: Sat, 22 Dec 2018 04:58:56 +0000
Subject: [R] Glitch in Kruskal-Wallis test?
Message-ID: <d1f02404-b0ed-9264-b1db-bf6c833c5828@mixmax.com>

Hi everyone,
I have been running a K-W test with the attached data, PupMort1. My code:
kruskal.test(Prop~Temp,data=PupMort1)
However, I found that I get the exact same result when I change the x-values, as
in the attached data PupMort2.
Test run with PupMort1Kruskal-Wallis rank sum testdata:? Prop by Temp
Kruskal-Wallis chi-squared = 6, df = 6, p-value = 0.4232
Test run with PupMort2Kruskal-Wallis rank sum testdata:??Prop by Temp
Kruskal-Wallis chi-squared = 6, df = 6, p-value = 0.4232
Does anybody know why this is happening?
Thank you!
Jenny
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: PupMort1.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20181222/10ab57ac/attachment.txt>

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: PupMort2.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20181222/10ab57ac/attachment-0001.txt>

From li@t@ @ending from dewey@myzen@co@uk  Sat Dec 22 13:34:45 2018
From: li@t@ @ending from dewey@myzen@co@uk (Michael Dewey)
Date: Sat, 22 Dec 2018 12:34:45 +0000
Subject: [R] =?utf-8?q?Problem_with_Kruskal=E2=80=93Wallis_test?=
In-Reply-To: <CAE43vpgKp0ThqqHm7YBB=XL66q3zUf=8f_XhgHPBMqsShgG1gQ@mail.gmail.com>
References: <CAE43vpgKp0ThqqHm7YBB=XL66q3zUf=8f_XhgHPBMqsShgG1gQ@mail.gmail.com>
Message-ID: <ff12c212-7e48-e65c-4009-fa5a692b822e@dewey.myzen.co.uk>

Dear Giuseppe

If I understand you correctly you have a very large sample size so it is 
not surprising that you get very small p-values. Eevn a scientifically 
uninteresting difference can become statistically significant with large 
samples. You probably need to define a metric for meaningful differences 
between groups and calculate a confidence interval for it.

Michael

On 21/12/2018 15:37, Giuseppe Cillis wrote:
> Dear all,
> I am a beginner with R (and also with the statistics) for which I hope to
> be clear.
> I should do this non-parametric test on data I extracted from maps.
> In practice I have a column that represents the landscape Dynamics of a
> certain time period (there are 3 dynamics, each of them marked by the
> number 1, 2 or 3) and the other column with the values of a topographic
> variable (for example the slope) . In all, there are more than 90,000 pairs
> of values.
> Going to do the test in R, for all the dynamics and for all the variables,
> I get out of the values of chi-square elevated (even in the order of
> thousands) and a p-value always <2.2e-16 .... why? Where can the error be? in
> the script or in the test approach?
> Thanks in advance
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From li@t@ @ending from dewey@myzen@co@uk  Sat Dec 22 13:38:46 2018
From: li@t@ @ending from dewey@myzen@co@uk (Michael Dewey)
Date: Sat, 22 Dec 2018 12:38:46 +0000
Subject: [R] Glitch in Kruskal-Wallis test?
In-Reply-To: <d1f02404-b0ed-9264-b1db-bf6c833c5828@mixmax.com>
References: <d1f02404-b0ed-9264-b1db-bf6c833c5828@mixmax.com>
Message-ID: <13b1abe7-8209-b12b-8d70-f4e0b1ff8751@dewey.myzen.co.uk>

Dear Jenny

What exactly do you think you are testing here? You are telling K-W you 
have seven groups each with a single value which is not the usual 
situation for K-W.

Michael

On 22/12/2018 04:58, Jenny Liu wrote:
> Hi everyone,
> I have been running a K-W test with the attached data, PupMort1. My code:
> kruskal.test(Prop~Temp,data=PupMort1)
> However, I found that I get the exact same result when I change the x-values, as
> in the attached data PupMort2.
> Test run with PupMort1Kruskal-Wallis rank sum testdata:? Prop by Temp
> Kruskal-Wallis chi-squared = 6, df = 6, p-value = 0.4232
> Test run with PupMort2Kruskal-Wallis rank sum testdata:??Prop by Temp
> Kruskal-Wallis chi-squared = 6, df = 6, p-value = 0.4232
> Does anybody know why this is happening?
> Thank you!
> Jenny
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From jenny@liu00 @ending from gm@il@com  Sat Dec 22 14:09:38 2018
From: jenny@liu00 @ending from gm@il@com (Jenny Liu)
Date: Sat, 22 Dec 2018 08:09:38 -0500
Subject: [R] Glitch in Kruskal-Wallis test?
In-Reply-To: <13b1abe7-8209-b12b-8d70-f4e0b1ff8751@dewey.myzen.co.uk>
References: <d1f02404-b0ed-9264-b1db-bf6c833c5828@mixmax.com>
 <13b1abe7-8209-b12b-8d70-f4e0b1ff8751@dewey.myzen.co.uk>
Message-ID: <CANmGPfC02e+C_hPVAJCefbgdNCSJ3=ptKnwTgax13c1rTCeR7A@mail.gmail.com>

Hi Michael,

Thank you for your reply! I'm testing the difference in proportions. Temp
is temperature, and Prop is the proportion of insect pupae that survived at
that temperature. I was told by a statistician that the K-W was appropriate
for testing proportions, but perhaps you know of an alternative? I have
already tested for heteroscedasticity using the Breusch-Pagan test.

Thanks again,
Jenny



On Dec 22, 2018 7:38 AM, "Michael Dewey" <lists at dewey.myzen.co.uk> wrote:

Dear Jenny

What exactly do you think you are testing here? You are telling K-W you
have seven groups each with a single value which is not the usual
situation for K-W.

Michael


On 22/12/2018 04:58, Jenny Liu wrote:
> Hi everyone,
> I have been running a K-W test with the attached data, PupMort1. My code:
> kruskal.test(Prop~Temp,data=PupMort1)
> However, I found that I get the exact same result when I change the
x-values, as
> in the attached data PupMort2.
> Test run with PupMort1Kruskal-Wallis rank sum testdata:  Prop by Temp

> Kruskal-Wallis chi-squared = 6, df = 6, p-value = 0.4232
> Test run with PupMort2Kruskal-Wallis rank sum testdata:  Prop by Temp

> Kruskal-Wallis chi-squared = 6, df = 6, p-value = 0.4232
> Does anybody know why this is happening?
> Thank you!
> Jenny
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html

	[[alternative HTML version deleted]]


From @tyen @ending from ntu@edu@tw  Sat Dec 22 15:31:52 2018
From: @tyen @ending from ntu@edu@tw (Steven Yen)
Date: Sat, 22 Dec 2018 22:31:52 +0800
Subject: [R] Printing with cat in a procedure
Message-ID: <958f64c0-a0ab-33dc-f851-69558817610a@ntu.edu.tw>

How do I print a matrix running a procedure? In the code below, I print 
with the cat command and get a vector (from A and C).

A<-matrix(rpois(16,lambda=5),nrow=4,byrow=T)
B<-diag(4)

try5<-function(A,B){
  C<-A+B
  cat("\nA =",A,"\nC = ",C)
structure(list(A=A,B=B,C=C))
}

v<-try5(A,B)
v$C

-- 
styen at ntu.edu.tw (S.T. Yen)


	[[alternative HTML version deleted]]


From ruipb@rr@d@@ @ending from @@po@pt  Sat Dec 22 15:39:11 2018
From: ruipb@rr@d@@ @ending from @@po@pt (Rui Barradas)
Date: Sat, 22 Dec 2018 14:39:11 +0000
Subject: [R] Printing with cat in a procedure
In-Reply-To: <958f64c0-a0ab-33dc-f851-69558817610a@ntu.edu.tw>
References: <958f64c0-a0ab-33dc-f851-69558817610a@ntu.edu.tw>
Message-ID: <d77d9c1b-c2a3-846a-dd86-ef78f5e07aaf@sapo.pt>

Hello,

Use print(A) and print(C). cat is meant for simpler objects.

Hope this helps,

Rui Barradas

?s 14:31 de 22/12/2018, Steven Yen escreveu:
> How do I print a matrix running a procedure? In the code below, I print
> with the cat command and get a vector (from A and C).
> 
> A<-matrix(rpois(16,lambda=5),nrow=4,byrow=T)
> B<-diag(4)
> 
> try5<-function(A,B){
>    C<-A+B
>    cat("\nA =",A,"\nC = ",C)
> structure(list(A=A,B=B,C=C))
> }
> 
> v<-try5(A,B)
> v$C
>


From ericjberger @ending from gm@il@com  Sat Dec 22 15:36:52 2018
From: ericjberger @ending from gm@il@com (Eric Berger)
Date: Sat, 22 Dec 2018 16:36:52 +0200
Subject: [R] Printing with cat in a procedure
In-Reply-To: <958f64c0-a0ab-33dc-f851-69558817610a@ntu.edu.tw>
References: <958f64c0-a0ab-33dc-f851-69558817610a@ntu.edu.tw>
Message-ID: <CAGgJW77PBVeK63wBDYV5wv+a0_OLw-2hqYSwp2vaLpUPtz83Ng@mail.gmail.com>

Hi Steven,
Here's one way, using print

try5<-function(A,B){
  C<-A+B
  #cat("\nA =",A,"\nC = ",C)
  cat("\nA = ")
  print(A)
  cat("\nC = ")
  print(C)
  structure(list(A=A,B=B,C=C))
}

HTH,
Eric


On Sat, Dec 22, 2018 at 4:32 PM Steven Yen <styen at ntu.edu.tw> wrote:

> How do I print a matrix running a procedure? In the code below, I print
> with the cat command and get a vector (from A and C).
>
> A<-matrix(rpois(16,lambda=5),nrow=4,byrow=T)
> B<-diag(4)
>
> try5<-function(A,B){
>   C<-A+B
>   cat("\nA =",A,"\nC = ",C)
> structure(list(A=A,B=B,C=C))
> }
>
> v<-try5(A,B)
> v$C
>
> --
> styen at ntu.edu.tw (S.T. Yen)
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewmil @ending from dcn@d@vi@@c@@u@  Sat Dec 22 15:49:05 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Sat, 22 Dec 2018 06:49:05 -0800
Subject: [R] Printing with cat in a procedure
In-Reply-To: <958f64c0-a0ab-33dc-f851-69558817610a@ntu.edu.tw>
References: <958f64c0-a0ab-33dc-f851-69558817610a@ntu.edu.tw>
Message-ID: <E442B1C8-522C-4747-A5E8-3E5C92D10606@dcn.davis.ca.us>

Try using print instead of cat [1], and please read about what the arguments are in the help file [2][3] for any function you are using before posting a question.

[1] https://stackoverflow.com/questions/31843662/what-is-the-difference-between-cat-and-print
[2] ?cat
[3] ?print

On December 22, 2018 6:31:52 AM PST, Steven Yen <styen at ntu.edu.tw> wrote:
>How do I print a matrix running a procedure? In the code below, I print
>
>with the cat command and get a vector (from A and C).
>
>A<-matrix(rpois(16,lambda=5),nrow=4,byrow=T)
>B<-diag(4)
>
>try5<-function(A,B){
>  C<-A+B
>  cat("\nA =",A,"\nC = ",C)
>structure(list(A=A,B=B,C=C))
>}
>
>v<-try5(A,B)
>v$C

-- 
Sent from my phone. Please excuse my brevity.


From @tyen @ending from ntu@edu@tw  Sat Dec 22 16:13:47 2018
From: @tyen @ending from ntu@edu@tw (Steven Yen)
Date: Sat, 22 Dec 2018 23:13:47 +0800
Subject: [R] Printing with cat in a procedure
In-Reply-To: <CAGgJW77PBVeK63wBDYV5wv+a0_OLw-2hqYSwp2vaLpUPtz83Ng@mail.gmail.com>
References: <958f64c0-a0ab-33dc-f851-69558817610a@ntu.edu.tw>
 <CAGgJW77PBVeK63wBDYV5wv+a0_OLw-2hqYSwp2vaLpUPtz83Ng@mail.gmail.com>
Message-ID: <8070c645-2671-c47a-b15e-05c767cfb1c8@ntu.edu.tw>

Thank you all - print works wonders.

On 12/22/2018 10:36 PM, Eric Berger wrote:
> Hi Steven,
> Here's one way, using print
>
> try5<-function(A,B){
>   C<-A+B
>   #cat("\nA =",A,"\nC = ",C)
>   cat("\nA = ")
>   print(A)
>   cat("\nC = ")
>   print(C)
>   structure(list(A=A,B=B,C=C))
> }
>
> HTH,
> Eric
>
>
> On Sat, Dec 22, 2018 at 4:32 PM Steven Yen <styen at ntu.edu.tw 
> <mailto:styen at ntu.edu.tw>> wrote:
>
>     How do I print a matrix running a procedure? In the code below, I
>     print
>     with the cat command and get a vector (from A and C).
>
>     A<-matrix(rpois(16,lambda=5),nrow=4,byrow=T)
>     B<-diag(4)
>
>     try5<-function(A,B){
>       C<-A+B
>       cat("\nA =",A,"\nC = ",C)
>     structure(list(A=A,B=B,C=C))
>     }
>
>     v<-try5(A,B)
>     v$C
>
>     -- 
>     styen at ntu.edu.tw <mailto:styen at ntu.edu.tw> (S.T. Yen)
>
>
>             [[alternative HTML version deleted]]
>
>     ______________________________________________
>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>     To UNSUBSCRIBE and more, see
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     and provide commented, minimal, self-contained, reproducible code.
>

-- 
styen at ntu.edu.tw (S.T. Yen)


	[[alternative HTML version deleted]]


From dc@rl@on @ending from t@mu@edu  Sat Dec 22 16:28:56 2018
From: dc@rl@on @ending from t@mu@edu (David L Carlson)
Date: Sat, 22 Dec 2018 15:28:56 +0000
Subject: [R] Glitch in Kruskal-Wallis test?
In-Reply-To: <CANmGPfC02e+C_hPVAJCefbgdNCSJ3=ptKnwTgax13c1rTCeR7A@mail.gmail.com>
References: <d1f02404-b0ed-9264-b1db-bf6c833c5828@mixmax.com>
 <13b1abe7-8209-b12b-8d70-f4e0b1ff8751@dewey.myzen.co.uk>
 <CANmGPfC02e+C_hPVAJCefbgdNCSJ3=ptKnwTgax13c1rTCeR7A@mail.gmail.com>
Message-ID: <085fa7e98a3a4f47893b3ecf35074fd3@tamu.edu>

You may need to spend some more time with the statistician who needs to see your data. It is not clear if you have a two sample test or a paired sample test. Kruskall-Wallis expects data for each observation, not grouped data. Without the observations, the test cannot compute the sample size and the degrees of freedom. You have run kruskal.test separately on each sample. The kruskal.test is designed for comparing two or more samples. 

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Jenny Liu
Sent: Saturday, December 22, 2018 7:10 AM
To: Michael Dewey <lists at dewey.myzen.co.uk>
Cc: r-help at r-project.org
Subject: Re: [R] Glitch in Kruskal-Wallis test?

Hi Michael,

Thank you for your reply! I'm testing the difference in proportions. Temp
is temperature, and Prop is the proportion of insect pupae that survived at
that temperature. I was told by a statistician that the K-W was appropriate
for testing proportions, but perhaps you know of an alternative? I have
already tested for heteroscedasticity using the Breusch-Pagan test.

Thanks again,
Jenny



On Dec 22, 2018 7:38 AM, "Michael Dewey" <lists at dewey.myzen.co.uk> wrote:

Dear Jenny

What exactly do you think you are testing here? You are telling K-W you
have seven groups each with a single value which is not the usual
situation for K-W.

Michael


On 22/12/2018 04:58, Jenny Liu wrote:
> Hi everyone,
> I have been running a K-W test with the attached data, PupMort1. My code:
> kruskal.test(Prop~Temp,data=PupMort1)
> However, I found that I get the exact same result when I change the
x-values, as
> in the attached data PupMort2.
> Test run with PupMort1Kruskal-Wallis rank sum testdata:  Prop by Temp

> Kruskal-Wallis chi-squared = 6, df = 6, p-value = 0.4232
> Test run with PupMort2Kruskal-Wallis rank sum testdata:  Prop by Temp

> Kruskal-Wallis chi-squared = 6, df = 6, p-value = 0.4232
> Does anybody know why this is happening?
> Thank you!
> Jenny
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @ending from gm@il@com  Sat Dec 22 16:36:19 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Sat, 22 Dec 2018 07:36:19 -0800
Subject: [R] Glitch in Kruskal-Wallis test?
In-Reply-To: <085fa7e98a3a4f47893b3ecf35074fd3@tamu.edu>
References: <d1f02404-b0ed-9264-b1db-bf6c833c5828@mixmax.com>
 <13b1abe7-8209-b12b-8d70-f4e0b1ff8751@dewey.myzen.co.uk>
 <CANmGPfC02e+C_hPVAJCefbgdNCSJ3=ptKnwTgax13c1rTCeR7A@mail.gmail.com>
 <085fa7e98a3a4f47893b3ecf35074fd3@tamu.edu>
Message-ID: <CAGxFJbQ4aOde7NwxRBoDW3sWawraY+GbPPWmHXa9ck4BZ9LnBA@mail.gmail.com>

... Moreover, you should not analyze proportions in this way, which treats
.5 = 2/4 or .5 = 2000/4000 identically. As David said, you need to work
with a statistician.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Dec 22, 2018 at 7:32 AM David L Carlson <dcarlson at tamu.edu> wrote:

> You may need to spend some more time with the statistician who needs to
> see your data. It is not clear if you have a two sample test or a paired
> sample test. Kruskall-Wallis expects data for each observation, not grouped
> data. Without the observations, the test cannot compute the sample size and
> the degrees of freedom. You have run kruskal.test separately on each
> sample. The kruskal.test is designed for comparing two or more samples.
>
> ----------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77843-4352
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Jenny Liu
> Sent: Saturday, December 22, 2018 7:10 AM
> To: Michael Dewey <lists at dewey.myzen.co.uk>
> Cc: r-help at r-project.org
> Subject: Re: [R] Glitch in Kruskal-Wallis test?
>
> Hi Michael,
>
> Thank you for your reply! I'm testing the difference in proportions. Temp
> is temperature, and Prop is the proportion of insect pupae that survived at
> that temperature. I was told by a statistician that the K-W was appropriate
> for testing proportions, but perhaps you know of an alternative? I have
> already tested for heteroscedasticity using the Breusch-Pagan test.
>
> Thanks again,
> Jenny
>
>
>
> On Dec 22, 2018 7:38 AM, "Michael Dewey" <lists at dewey.myzen.co.uk> wrote:
>
> Dear Jenny
>
> What exactly do you think you are testing here? You are telling K-W you
> have seven groups each with a single value which is not the usual
> situation for K-W.
>
> Michael
>
>
> On 22/12/2018 04:58, Jenny Liu wrote:
> > Hi everyone,
> > I have been running a K-W test with the attached data, PupMort1. My code:
> > kruskal.test(Prop~Temp,data=PupMort1)
> > However, I found that I get the exact same result when I change the
> x-values, as
> > in the attached data PupMort2.
> > Test run with PupMort1Kruskal-Wallis rank sum testdata:  Prop by Temp
>
> > Kruskal-Wallis chi-squared = 6, df = 6, p-value = 0.4232
> > Test run with PupMort2Kruskal-Wallis rank sum testdata:  Prop by Temp
>
> > Kruskal-Wallis chi-squared = 6, df = 6, p-value = 0.4232
> > Does anybody know why this is happening?
> > Thank you!
> > Jenny
> >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @nnij@nh @ending from gm@il@com  Sun Dec 23 02:10:27 2018
From: @nnij@nh @ending from gm@il@com (Janh Anni)
Date: Sat, 22 Dec 2018 20:10:27 -0500
Subject: [R] Issues with R3.5.2
Message-ID: <CAFCoDdBa3XxZMQZJamNOUCjOcY+p5umsn95mPW2MTbqqR=Ps7g@mail.gmail.com>

Dear R Experts,

I use Windows 10 and just installed the new R version, R3.5.2 but when I
tried to load a data file using read.table, I got an error message like
this:

*Error in file(file, "rt") : cannot open the connection*
*In addition: Warning message:*
*In file(file, "rt") :*
*  cannot open file 'StreamPCB.dat': No such file or directory*

Also, I couldn't install packages using install.packages as usual, unless I
run R as Administrator

I wonder if anyone else had the same issues and any suggestions how to fix?

Thanks a lot
Janh

	[[alternative HTML version deleted]]


From jdnewmil @ending from dcn@d@vi@@c@@u@  Sun Dec 23 02:26:01 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Sat, 22 Dec 2018 17:26:01 -0800
Subject: [R] Issues with R3.5.2
In-Reply-To: <CAFCoDdBa3XxZMQZJamNOUCjOcY+p5umsn95mPW2MTbqqR=Ps7g@mail.gmail.com>
References: <CAFCoDdBa3XxZMQZJamNOUCjOcY+p5umsn95mPW2MTbqqR=Ps7g@mail.gmail.com>
Message-ID: <61D9034F-5E36-404C-869D-2E3A4E6DC9EC@dcn.davis.ca.us>

Did you by any chance use Run As Administrator to install R? If so then you need to uninstall it and delete all files created by it (e.g. Documents/R/win-lib/3.5/) and re-install using UAC as prompted.

On December 22, 2018 5:10:27 PM PST, Janh Anni <annijanh at gmail.com> wrote:
>Dear R Experts,
>
>I use Windows 10 and just installed the new R version, R3.5.2 but when
>I
>tried to load a data file using read.table, I got an error message like
>this:
>
>*Error in file(file, "rt") : cannot open the connection*
>*In addition: Warning message:*
>*In file(file, "rt") :*
>*  cannot open file 'StreamPCB.dat': No such file or directory*
>
>Also, I couldn't install packages using install.packages as usual,
>unless I
>run R as Administrator
>
>I wonder if anyone else had the same issues and any suggestions how to
>fix?
>
>Thanks a lot
>Janh
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @tyen @ending from ntu@edu@tw  Sun Dec 23 02:33:39 2018
From: @tyen @ending from ntu@edu@tw (Steven Yen)
Date: Sun, 23 Dec 2018 09:33:39 +0800
Subject: [R] Random seed
Message-ID: <1061f17a-3ab1-f651-5a14-0e5376e70dd6@ntu.edu.tw>

I have known from the old days to set a random seed of a LARGE ODD 
NUMBER. Now I read instructions of set.seed and it requires ANY INTEGER. 
Any idea? Or, does it matter. Thanks.

-- 
styen at ntu.edu.tw (S.T. Yen)


	[[alternative HTML version deleted]]


From @nnij@nh @ending from gm@il@com  Sun Dec 23 03:01:44 2018
From: @nnij@nh @ending from gm@il@com (Janh Anni)
Date: Sat, 22 Dec 2018 21:01:44 -0500
Subject: [R] Issues with R3.5.2
In-Reply-To: <61D9034F-5E36-404C-869D-2E3A4E6DC9EC@dcn.davis.ca.us>
References: <CAFCoDdBa3XxZMQZJamNOUCjOcY+p5umsn95mPW2MTbqqR=Ps7g@mail.gmail.com>
 <61D9034F-5E36-404C-869D-2E3A4E6DC9EC@dcn.davis.ca.us>
Message-ID: <CAFCoDdATK3V_V2qBOR2fMTX40QUwsdSsRJgO=azZBVkjG+NBBQ@mail.gmail.com>

Hi Jeff,

No, during the installation, there was not an option to Run as
Administration.  But *after *installation, I found that if I selected Run
as Administrator, then I could install packages using install.packages as
usual without problems.

Thanks
Janh

On Sat, Dec 22, 2018 at 8:26 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Did you by any chance use Run As Administrator to install R? If so then
> you need to uninstall it and delete all files created by it (e.g.
> Documents/R/win-lib/3.5/) and re-install using UAC as prompted.
>
> On December 22, 2018 5:10:27 PM PST, Janh Anni <annijanh at gmail.com> wrote:
> >Dear R Experts,
> >
> >I use Windows 10 and just installed the new R version, R3.5.2 but when
> >I
> >tried to load a data file using read.table, I got an error message like
> >this:
> >
> >*Error in file(file, "rt") : cannot open the connection*
> >*In addition: Warning message:*
> >*In file(file, "rt") :*
> >*  cannot open file 'StreamPCB.dat': No such file or directory*
> >
> >Also, I couldn't install packages using install.packages as usual,
> >unless I
> >run R as Administrator
> >
> >I wonder if anyone else had the same issues and any suggestions how to
> >fix?
> >
> >Thanks a lot
> >Janh
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From jdnewmil @ending from dcn@d@vi@@c@@u@  Sun Dec 23 05:06:43 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Sat, 22 Dec 2018 20:06:43 -0800
Subject: [R] Issues with R3.5.2
In-Reply-To: <CAFCoDdATK3V_V2qBOR2fMTX40QUwsdSsRJgO=azZBVkjG+NBBQ@mail.gmail.com>
References: <CAFCoDdBa3XxZMQZJamNOUCjOcY+p5umsn95mPW2MTbqqR=Ps7g@mail.gmail.com>
 <61D9034F-5E36-404C-869D-2E3A4E6DC9EC@dcn.davis.ca.us>
 <CAFCoDdATK3V_V2qBOR2fMTX40QUwsdSsRJgO=azZBVkjG+NBBQ@mail.gmail.com>
Message-ID: <221C51C8-C5C7-45A4-BF9F-D5DEE0461662@dcn.davis.ca.us>

That normally only occurs if you have at some time used elevated permissions, beyond which point you fall into a downward spiral of more permissions trouble. You are apparently already in trouble, whether it was of your own making or due to a bug in the installer.

Also, never update the system R package library... always use a personal library.

On December 22, 2018 6:01:44 PM PST, Janh Anni <annijanh at gmail.com> wrote:
>Hi Jeff,
>
>No, during the installation, there was not an option to Run as
>Administration.  But *after *installation, I found that if I selected
>Run
>as Administrator, then I could install packages using install.packages
>as
>usual without problems.
>
>Thanks
>Janh
>
>On Sat, Dec 22, 2018 at 8:26 PM Jeff Newmiller
><jdnewmil at dcn.davis.ca.us>
>wrote:
>
>> Did you by any chance use Run As Administrator to install R? If so
>then
>> you need to uninstall it and delete all files created by it (e.g.
>> Documents/R/win-lib/3.5/) and re-install using UAC as prompted.
>>
>> On December 22, 2018 5:10:27 PM PST, Janh Anni <annijanh at gmail.com>
>wrote:
>> >Dear R Experts,
>> >
>> >I use Windows 10 and just installed the new R version, R3.5.2 but
>when
>> >I
>> >tried to load a data file using read.table, I got an error message
>like
>> >this:
>> >
>> >*Error in file(file, "rt") : cannot open the connection*
>> >*In addition: Warning message:*
>> >*In file(file, "rt") :*
>> >*  cannot open file 'StreamPCB.dat': No such file or directory*
>> >
>> >Also, I couldn't install packages using install.packages as usual,
>> >unless I
>> >run R as Administrator
>> >
>> >I wonder if anyone else had the same issues and any suggestions how
>to
>> >fix?
>> >
>> >Thanks a lot
>> >Janh
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> >______________________________________________
>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >PLEASE do read the posting guide
>> >http://www.R-project.org/posting-guide.html
>> >and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Sent from my phone. Please excuse my brevity.
>>

-- 
Sent from my phone. Please excuse my brevity.


From @nnij@nh @ending from gm@il@com  Sun Dec 23 05:16:11 2018
From: @nnij@nh @ending from gm@il@com (Janh Anni)
Date: Sat, 22 Dec 2018 23:16:11 -0500
Subject: [R] Issues with R3.5.2
In-Reply-To: <221C51C8-C5C7-45A4-BF9F-D5DEE0461662@dcn.davis.ca.us>
References: <CAFCoDdBa3XxZMQZJamNOUCjOcY+p5umsn95mPW2MTbqqR=Ps7g@mail.gmail.com>
 <61D9034F-5E36-404C-869D-2E3A4E6DC9EC@dcn.davis.ca.us>
 <CAFCoDdATK3V_V2qBOR2fMTX40QUwsdSsRJgO=azZBVkjG+NBBQ@mail.gmail.com>
 <221C51C8-C5C7-45A4-BF9F-D5DEE0461662@dcn.davis.ca.us>
Message-ID: <CAFCoDdBW38f5CazbqA-3xzfJHKPSN39tj73rGkhg=bdGn-aXBw@mail.gmail.com>

This issue only came up after I installed R3.5.2. Never had any problems
with previous installations.  So it is likely a bug in the current
version.  Any suggestions what to do now?

On Sat, Dec 22, 2018 at 11:06 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> That normally only occurs if you have at some time used elevated
> permissions, beyond which point you fall into a downward spiral of more
> permissions trouble. You are apparently already in trouble, whether it was
> of your own making or due to a bug in the installer.
>
> Also, never update the system R package library... always use a personal
> library.
>
> On December 22, 2018 6:01:44 PM PST, Janh Anni <annijanh at gmail.com> wrote:
> >Hi Jeff,
> >
> >No, during the installation, there was not an option to Run as
> >Administration.  But *after *installation, I found that if I selected
> >Run
> >as Administrator, then I could install packages using install.packages
> >as
> >usual without problems.
> >
> >Thanks
> >Janh
> >
> >On Sat, Dec 22, 2018 at 8:26 PM Jeff Newmiller
> ><jdnewmil at dcn.davis.ca.us>
> >wrote:
> >
> >> Did you by any chance use Run As Administrator to install R? If so
> >then
> >> you need to uninstall it and delete all files created by it (e.g.
> >> Documents/R/win-lib/3.5/) and re-install using UAC as prompted.
> >>
> >> On December 22, 2018 5:10:27 PM PST, Janh Anni <annijanh at gmail.com>
> >wrote:
> >> >Dear R Experts,
> >> >
> >> >I use Windows 10 and just installed the new R version, R3.5.2 but
> >when
> >> >I
> >> >tried to load a data file using read.table, I got an error message
> >like
> >> >this:
> >> >
> >> >*Error in file(file, "rt") : cannot open the connection*
> >> >*In addition: Warning message:*
> >> >*In file(file, "rt") :*
> >> >*  cannot open file 'StreamPCB.dat': No such file or directory*
> >> >
> >> >Also, I couldn't install packages using install.packages as usual,
> >> >unless I
> >> >run R as Administrator
> >> >
> >> >I wonder if anyone else had the same issues and any suggestions how
> >to
> >> >fix?
> >> >
> >> >Thanks a lot
> >> >Janh
> >> >
> >> >       [[alternative HTML version deleted]]
> >> >
> >> >______________________________________________
> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >https://stat.ethz.ch/mailman/listinfo/r-help
> >> >PLEASE do read the posting guide
> >> >http://www.R-project.org/posting-guide.html
> >> >and provide commented, minimal, self-contained, reproducible code.
> >>
> >> --
> >> Sent from my phone. Please excuse my brevity.
> >>
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From jdnewmil @ending from dcn@d@vi@@c@@u@  Sun Dec 23 05:09:42 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Sat, 22 Dec 2018 20:09:42 -0800
Subject: [R] Random seed
In-Reply-To: <1061f17a-3ab1-f651-5a14-0e5376e70dd6@ntu.edu.tw>
References: <1061f17a-3ab1-f651-5a14-0e5376e70dd6@ntu.edu.tw>
Message-ID: <D2F5B9A7-649C-48AC-8425-4AFDC458CF40@dcn.davis.ca.us>

It doesn't matter. The whole point is to make the pseudo-random sequence repeatable... unless you have a specific reason to avoid repeatability.

On December 22, 2018 5:33:39 PM PST, Steven Yen <styen at ntu.edu.tw> wrote:
>I have known from the old days to set a random seed of a LARGE ODD 
>NUMBER. Now I read instructions of set.seed and it requires ANY
>INTEGER. 
>Any idea? Or, does it matter. Thanks.

-- 
Sent from my phone. Please excuse my brevity.


From jdnewmil @ending from dcn@d@vi@@c@@u@  Sun Dec 23 07:31:54 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Sat, 22 Dec 2018 22:31:54 -0800
Subject: [R] Issues with R3.5.2
In-Reply-To: <CAFCoDdBW38f5CazbqA-3xzfJHKPSN39tj73rGkhg=bdGn-aXBw@mail.gmail.com>
References: <CAFCoDdBa3XxZMQZJamNOUCjOcY+p5umsn95mPW2MTbqqR=Ps7g@mail.gmail.com>
 <61D9034F-5E36-404C-869D-2E3A4E6DC9EC@dcn.davis.ca.us>
 <CAFCoDdATK3V_V2qBOR2fMTX40QUwsdSsRJgO=azZBVkjG+NBBQ@mail.gmail.com>
 <221C51C8-C5C7-45A4-BF9F-D5DEE0461662@dcn.davis.ca.us>
 <CAFCoDdBW38f5CazbqA-3xzfJHKPSN39tj73rGkhg=bdGn-aXBw@mail.gmail.com>
Message-ID: <FB9E284F-94F5-4A58-8B6E-5DCDA107313B@dcn.davis.ca.us>

You could delete your 3.5 personal package library (using the File Explorer with Run as Admin if necessary) and re-install your packages without running as Admin. If that does not work try uninstalling R and re-installing 3.5.1.

On December 22, 2018 8:16:11 PM PST, Janh Anni <annijanh at gmail.com> wrote:
>This issue only came up after I installed R3.5.2. Never had any
>problems
>with previous installations.  So it is likely a bug in the current
>version.  Any suggestions what to do now?
>
>On Sat, Dec 22, 2018 at 11:06 PM Jeff Newmiller
><jdnewmil at dcn.davis.ca.us>
>wrote:
>
>> That normally only occurs if you have at some time used elevated
>> permissions, beyond which point you fall into a downward spiral of
>more
>> permissions trouble. You are apparently already in trouble, whether
>it was
>> of your own making or due to a bug in the installer.
>>
>> Also, never update the system R package library... always use a
>personal
>> library.
>>
>> On December 22, 2018 6:01:44 PM PST, Janh Anni <annijanh at gmail.com>
>wrote:
>> >Hi Jeff,
>> >
>> >No, during the installation, there was not an option to Run as
>> >Administration.  But *after *installation, I found that if I
>selected
>> >Run
>> >as Administrator, then I could install packages using
>install.packages
>> >as
>> >usual without problems.
>> >
>> >Thanks
>> >Janh
>> >
>> >On Sat, Dec 22, 2018 at 8:26 PM Jeff Newmiller
>> ><jdnewmil at dcn.davis.ca.us>
>> >wrote:
>> >
>> >> Did you by any chance use Run As Administrator to install R? If so
>> >then
>> >> you need to uninstall it and delete all files created by it (e.g.
>> >> Documents/R/win-lib/3.5/) and re-install using UAC as prompted.
>> >>
>> >> On December 22, 2018 5:10:27 PM PST, Janh Anni
><annijanh at gmail.com>
>> >wrote:
>> >> >Dear R Experts,
>> >> >
>> >> >I use Windows 10 and just installed the new R version, R3.5.2 but
>> >when
>> >> >I
>> >> >tried to load a data file using read.table, I got an error
>message
>> >like
>> >> >this:
>> >> >
>> >> >*Error in file(file, "rt") : cannot open the connection*
>> >> >*In addition: Warning message:*
>> >> >*In file(file, "rt") :*
>> >> >*  cannot open file 'StreamPCB.dat': No such file or directory*
>> >> >
>> >> >Also, I couldn't install packages using install.packages as
>usual,
>> >> >unless I
>> >> >run R as Administrator
>> >> >
>> >> >I wonder if anyone else had the same issues and any suggestions
>how
>> >to
>> >> >fix?
>> >> >
>> >> >Thanks a lot
>> >> >Janh
>> >> >
>> >> >       [[alternative HTML version deleted]]
>> >> >
>> >> >______________________________________________
>> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >> >PLEASE do read the posting guide
>> >> >http://www.R-project.org/posting-guide.html
>> >> >and provide commented, minimal, self-contained, reproducible
>code.
>> >>
>> >> --
>> >> Sent from my phone. Please excuse my brevity.
>> >>
>>
>> --
>> Sent from my phone. Please excuse my brevity.
>>

-- 
Sent from my phone. Please excuse my brevity.


From jenny@liu00 @ending from gm@il@com  Sun Dec 23 02:59:14 2018
From: jenny@liu00 @ending from gm@il@com (Jenny Liu)
Date: Sat, 22 Dec 2018 20:59:14 -0500
Subject: [R] Glitch in Kruskal-Wallis test?
In-Reply-To: <CAGxFJbQ4aOde7NwxRBoDW3sWawraY+GbPPWmHXa9ck4BZ9LnBA@mail.gmail.com>
References: <d1f02404-b0ed-9264-b1db-bf6c833c5828@mixmax.com>
 <13b1abe7-8209-b12b-8d70-f4e0b1ff8751@dewey.myzen.co.uk>
 <CANmGPfC02e+C_hPVAJCefbgdNCSJ3=ptKnwTgax13c1rTCeR7A@mail.gmail.com>
 <085fa7e98a3a4f47893b3ecf35074fd3@tamu.edu>
 <CAGxFJbQ4aOde7NwxRBoDW3sWawraY+GbPPWmHXa9ck4BZ9LnBA@mail.gmail.com>
Message-ID: <CANmGPfBtdFMgC6N9oK1cOjb8FBB4Md4rEkAbcKxJofGKFnSGNw@mail.gmail.com>

Thank you very much Michael and Bert! I'll have to look for some other
statistical test. Have a great holiday!

On Sat, Dec 22, 2018, 10:36 AM Bert Gunter <bgunter.4567 at gmail.com wrote:

> ... Moreover, you should not analyze proportions in this way, which treats
> .5 = 2/4 or .5 = 2000/4000 identically. As David said, you need to work
> with a statistician.
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Sat, Dec 22, 2018 at 7:32 AM David L Carlson <dcarlson at tamu.edu> wrote:
>
>> You may need to spend some more time with the statistician who needs to
>> see your data. It is not clear if you have a two sample test or a paired
>> sample test. Kruskall-Wallis expects data for each observation, not grouped
>> data. Without the observations, the test cannot compute the sample size and
>> the degrees of freedom. You have run kruskal.test separately on each
>> sample. The kruskal.test is designed for comparing two or more samples.
>>
>> ----------------------------------------
>> David L Carlson
>> Department of Anthropology
>> Texas A&M University
>> College Station, TX 77843-4352
>>
>> -----Original Message-----
>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Jenny Liu
>> Sent: Saturday, December 22, 2018 7:10 AM
>> To: Michael Dewey <lists at dewey.myzen.co.uk>
>> Cc: r-help at r-project.org
>> Subject: Re: [R] Glitch in Kruskal-Wallis test?
>>
>> Hi Michael,
>>
>> Thank you for your reply! I'm testing the difference in proportions. Temp
>> is temperature, and Prop is the proportion of insect pupae that survived
>> at
>> that temperature. I was told by a statistician that the K-W was
>> appropriate
>> for testing proportions, but perhaps you know of an alternative? I have
>> already tested for heteroscedasticity using the Breusch-Pagan test.
>>
>> Thanks again,
>> Jenny
>>
>>
>>
>> On Dec 22, 2018 7:38 AM, "Michael Dewey" <lists at dewey.myzen.co.uk> wrote:
>>
>> Dear Jenny
>>
>> What exactly do you think you are testing here? You are telling K-W you
>> have seven groups each with a single value which is not the usual
>> situation for K-W.
>>
>> Michael
>>
>>
>> On 22/12/2018 04:58, Jenny Liu wrote:
>> > Hi everyone,
>> > I have been running a K-W test with the attached data, PupMort1. My
>> code:
>> > kruskal.test(Prop~Temp,data=PupMort1)
>> > However, I found that I get the exact same result when I change the
>> x-values, as
>> > in the attached data PupMort2.
>> > Test run with PupMort1Kruskal-Wallis rank sum testdata:  Prop by Temp
>>
>> > Kruskal-Wallis chi-squared = 6, df = 6, p-value = 0.4232
>> > Test run with PupMort2Kruskal-Wallis rank sum testdata:  Prop by Temp
>>
>> > Kruskal-Wallis chi-squared = 6, df = 6, p-value = 0.4232
>> > Does anybody know why this is happening?
>> > Thank you!
>> > Jenny
>> >
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>
>> --
>> Michael
>> http://www.dewey.myzen.co.uk/home.html
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From li@t@ @ending from dewey@myzen@co@uk  Sun Dec 23 12:51:07 2018
From: li@t@ @ending from dewey@myzen@co@uk (Michael Dewey)
Date: Sun, 23 Dec 2018 11:51:07 +0000
Subject: [R] =?utf-8?q?Problem_with_Kruskal=E2=80=93Wallis_test?=
In-Reply-To: <CAE43vpiqQvQOqe80wq1bPs+s8nRt=0pK8JjY0fxqEbQ6iEeyVA@mail.gmail.com>
References: <CAE43vpgKp0ThqqHm7YBB=XL66q3zUf=8f_XhgHPBMqsShgG1gQ@mail.gmail.com>
 <ff12c212-7e48-e65c-4009-fa5a692b822e@dewey.myzen.co.uk>
 <CAE43vpiqQvQOqe80wq1bPs+s8nRt=0pK8JjY0fxqEbQ6iEeyVA@mail.gmail.com>
Message-ID: <81941d31-1140-ab40-5a92-3ba068767a9f@dewey.myzen.co.uk>

I think you need to talk to someone who uses your sort of geographic 
data to find out what an appropriate metric for comparing your variables 
is. Only then will you know what might be a suitable way forward.

Michael

On 22/12/2018 17:27, Giuseppe Cillis wrote:
> Dear Michael,
> Thanks for your answer.
> So, I'm not an expert in R and statistics, how can I create this 
> interval of confidence of groups?
> Thanks
> Gc
> 
> Il giorno sab 22 dic 2018, 13:34 Michael Dewey <lists at dewey.myzen.co.uk 
> <mailto:lists at dewey.myzen.co.uk>> ha scritto:
> 
>     Dear Giuseppe
> 
>     If I understand you correctly you have a very large sample size so
>     it is
>     not surprising that you get very small p-values. Eevn a scientifically
>     uninteresting difference can become statistically significant with
>     large
>     samples. You probably need to define a metric for meaningful
>     differences
>     between groups and calculate a confidence interval for it.
> 
>     Michael
> 
>     On 21/12/2018 15:37, Giuseppe Cillis wrote:
>      > Dear all,
>      > I am a beginner with R (and also with the statistics) for which I
>     hope to
>      > be clear.
>      > I should do this non-parametric test on data I extracted from maps.
>      > In practice I have a column that represents the landscape
>     Dynamics of a
>      > certain time period (there are 3 dynamics, each of them marked by the
>      > number 1, 2 or 3) and the other column with the values of a
>     topographic
>      > variable (for example the slope) . In all, there are more than
>     90,000 pairs
>      > of values.
>      > Going to do the test in R, for all the dynamics and for all the
>     variables,
>      > I get out of the values of chi-square elevated (even in the order of
>      > thousands) and a p-value always <2.2e-16 .... why? Where can the
>     error be? in
>      > the script or in the test approach?
>      > Thanks in advance
>      >
>      >? ? ? ?[[alternative HTML version deleted]]
>      >
>      > ______________________________________________
>      > R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>     -- To UNSUBSCRIBE and more, see
>      > https://stat.ethz.ch/mailman/listinfo/r-help
>      > PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>      > and provide commented, minimal, self-contained, reproducible code.
>      >
> 
>     -- 
>     Michael
>     http://www.dewey.myzen.co.uk/home.html
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From m@echler @ending from @t@t@m@th@ethz@ch  Sun Dec 23 16:21:08 2018
From: m@echler @ending from @t@t@m@th@ethz@ch (Martin Maechler)
Date: Sun, 23 Dec 2018 16:21:08 +0100
Subject: [R] Issues with R3.5.2
In-Reply-To: <FB9E284F-94F5-4A58-8B6E-5DCDA107313B@dcn.davis.ca.us>
References: <CAFCoDdBa3XxZMQZJamNOUCjOcY+p5umsn95mPW2MTbqqR=Ps7g@mail.gmail.com>
 <61D9034F-5E36-404C-869D-2E3A4E6DC9EC@dcn.davis.ca.us>
 <CAFCoDdATK3V_V2qBOR2fMTX40QUwsdSsRJgO=azZBVkjG+NBBQ@mail.gmail.com>
 <221C51C8-C5C7-45A4-BF9F-D5DEE0461662@dcn.davis.ca.us>
 <CAFCoDdBW38f5CazbqA-3xzfJHKPSN39tj73rGkhg=bdGn-aXBw@mail.gmail.com>
 <FB9E284F-94F5-4A58-8B6E-5DCDA107313B@dcn.davis.ca.us>
Message-ID: <23583.42980.468376.733361@stat.math.ethz.ch>

>>>>> Jeff Newmiller 
>>>>>     on Sat, 22 Dec 2018 22:31:54 -0800 writes:

    > You could delete your 3.5 personal package library (using
    > the File Explorer with Run as Admin if necessary) and
    > re-install your packages without running as Admin. If that
    > does not work try uninstalling R and re-installing 3.5.1.

I tend to think that Jeff is right and you, Janh, *have* changed
something on your Windows box, or inadvertently during
installation of R.

I've tried our Windows Server installation (where I consciously do install
things as admin for all win server users to use it), and I can
install packages fine as simple user -- into the personal
library that is created after asking for user confirmation.

Also, given the number of R users and the (unfortunately still) high
percentage of Windows users among them,  I strongly assume we
would would have heard of it if this was a common problem..
(But then I may be wrong if the Windows users are "slow" to
 update their R version)

Martin

Merry Christmas!

    > On December 22, 2018 8:16:11 PM PST, Janh Anni
    > <annijanh at gmail.com> wrote:
    >> This issue only came up after I installed R3.5.2. Never
    >> had any problems with previous installations.  So it is
    >> likely a bug in the current version.  Any suggestions
    >> what to do now?
    >> 
    >> On Sat, Dec 22, 2018 at 11:06 PM Jeff Newmiller
    >> <jdnewmil at dcn.davis.ca.us> wrote:
    >> 
    >>> That normally only occurs if you have at some time used
    >>> elevated permissions, beyond which point you fall into a
    >>> downward spiral of
    >> more
    >>> permissions trouble. You are apparently already in
    >>> trouble, whether
    >> it was
    >>> of your own making or due to a bug in the installer.
    >>> 
    >>> Also, never update the system R package
    >>> library... always use a
    >> personal
    >>> library.
    >>> 
    >>> On December 22, 2018 6:01:44 PM PST, Janh Anni
    >>> <annijanh at gmail.com>
    >> wrote:
    >>> >Hi Jeff,
    >>> >
    >>> >No, during the installation, there was not an option to
    >>> Run as >Administration.  But *after *installation, I
    >>> found that if I
    >> selected
    >>> >Run >as Administrator, then I could install packages
    >>> using
    >> install.packages
    >>> >as >usual without problems.
    >>> >
    >>> >Thanks >Janh
    >>> >
    >>> >On Sat, Dec 22, 2018 at 8:26 PM Jeff Newmiller
    >>> ><jdnewmil at dcn.davis.ca.us> >wrote:
    >>> >
    >>> >> Did you by any chance use Run As Administrator to
    >>> install R? If so >then >> you need to uninstall it and
    >>> delete all files created by it (e.g.  >>
    >>> Documents/R/win-lib/3.5/) and re-install using UAC as
    >>> prompted.
    >>> >>
    >>> >> On December 22, 2018 5:10:27 PM PST, Janh Anni
    >> <annijanh at gmail.com>
    >>> >wrote: >> >Dear R Experts,
    >>> >> >
    >>> >> >I use Windows 10 and just installed the new R
    >>> version, R3.5.2 but >when >> >I >> >tried to load a data
    >>> file using read.table, I got an error
    >> message
    >>> >like >> >this:
    >>> >> >
    >>> >> >*Error in file(file, "rt") : cannot open the
    >>> connection* >> >*In addition: Warning message:* >> >*In
    >>> file(file, "rt") :* >> >* cannot open file
    >>> 'StreamPCB.dat': No such file or directory*
    >>> >> >
    >>> >> >Also, I couldn't install packages using
    >>> install.packages as
    >> usual,
    >>> >> >unless I >> >run R as Administrator
    >>> >> >
    >>> >> >I wonder if anyone else had the same issues and any
    >>> suggestions
    >> how
    >>> >to >> >fix?
    >>> >> >
    >>> >> >Thanks a lot >> >Janh
    >>> >> >
    >>> >> > [[alternative HTML version deleted]]
    >>> >> >
    >>> >> >______________________________________________ >>
    >>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    >>> more, see >>
    >>> >https://stat.ethz.ch/mailman/listinfo/r-help >> >PLEASE
    >>> do read the posting guide >>
    >>> >http://www.R-project.org/posting-guide.html >> >and
    >>> provide commented, minimal, self-contained, reproducible
    >> code.
    >>> >>
    >>> >> --
    >>> >> Sent from my phone. Please excuse my brevity.
    >>> >>
    >>> 
    >>> --
    >>> Sent from my phone. Please excuse my brevity.
    >>> 

    > -- 
    > Sent from my phone. Please excuse my brevity.

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    > more, see https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide
    > http://www.R-project.org/posting-guide.html and provide
    > commented, minimal, self-contained, reproducible code.


From m@iliP@dpo@t @ending from gm@il@com  Sun Dec 23 21:01:12 2018
From: m@iliP@dpo@t @ending from gm@il@com (Medic)
Date: Sun, 23 Dec 2018 23:01:12 +0300
Subject: [R] Problems with library(survival)
Message-ID: <CAH6117LyYDxTiuGO_zm5V6pu14Y6EvrQKc57szqCK+zDEKRM2A@mail.gmail.com>

> install.packages("survival")
package ?survival? successfully unpacked and MD5 sums checked
> library(survival)
Error: package or namespace load failed for ?survival? in get(Info[i,
1], envir = env):
 lazy-load database 'C:/Program
Files/R/R-3.5.1/library/lattice/R/lattice.rdb' is corrupt
In addition: Warning message:
In get(Info[i, 1], envir = env) : internal error -3 in R_decompress1
---
MY QUESTION IS: what does this mean? what is the problem here?


From jdnewmil @ending from dcn@d@vi@@c@@u@  Sun Dec 23 21:25:47 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Sun, 23 Dec 2018 12:25:47 -0800
Subject: [R] Problems with library(survival)
In-Reply-To: <CAH6117LyYDxTiuGO_zm5V6pu14Y6EvrQKc57szqCK+zDEKRM2A@mail.gmail.com>
References: <CAH6117LyYDxTiuGO_zm5V6pu14Y6EvrQKc57szqCK+zDEKRM2A@mail.gmail.com>
Message-ID: <ABA0F2CB-68B9-4185-B088-81317808D112@dcn.davis.ca.us>

It means that one of the packages installed during R installation (lattice) is damaged. You should probably re-install R.

On December 23, 2018 12:01:12 PM PST, Medic <mailiPadpost at gmail.com> wrote:
>> install.packages("survival")
>package ?survival? successfully unpacked and MD5 sums checked
>> library(survival)
>Error: package or namespace load failed for ?survival? in get(Info[i,
>1], envir = env):
> lazy-load database 'C:/Program
>Files/R/R-3.5.1/library/lattice/R/lattice.rdb' is corrupt
>In addition: Warning message:
>In get(Info[i, 1], envir = env) : internal error -3 in R_decompress1
>---
>MY QUESTION IS: what does this mean? what is the problem here?
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From jdnewmil @ending from dcn@d@vi@@c@@u@  Mon Dec 24 00:45:31 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Sun, 23 Dec 2018 15:45:31 -0800
Subject: [R] Random seed
In-Reply-To: <D2F5B9A7-649C-48AC-8425-4AFDC458CF40@dcn.davis.ca.us>
References: <1061f17a-3ab1-f651-5a14-0e5376e70dd6@ntu.edu.tw>
 <D2F5B9A7-649C-48AC-8425-4AFDC458CF40@dcn.davis.ca.us>
Message-ID: <0BC04E01-FE4B-47C5-8C5A-71156EF372E6@dcn.davis.ca.us>

For the record, low bitcount RNGs are relatively sensitive to initialization [1] such that rules like the one you reference might make sense for specific designs, but the default RNG used in R is much much more sophisticated than that [2].

[1] http://daviddeley.com/random/random4.htm
[2] ?Random (https://stat.ethz.ch/R-manual/R-devel/library/base/html/Random.html)

On December 22, 2018 8:09:42 PM PST, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>It doesn't matter. The whole point is to make the pseudo-random
>sequence repeatable... unless you have a specific reason to avoid
>repeatability.
>
>On December 22, 2018 5:33:39 PM PST, Steven Yen <styen at ntu.edu.tw>
>wrote:
>>I have known from the old days to set a random seed of a LARGE ODD 
>>NUMBER. Now I read instructions of set.seed and it requires ANY
>>INTEGER. 
>>Any idea? Or, does it matter. Thanks.

-- 
Sent from my phone. Please excuse my brevity.


From m@iliP@dpo@t @ending from gm@il@com  Mon Dec 24 10:30:13 2018
From: m@iliP@dpo@t @ending from gm@il@com (Medic)
Date: Mon, 24 Dec 2018 12:30:13 +0300
Subject: [R] Installation Re:  Problems with library...
Message-ID: <CAH6117+jLvTBsDhsRFr9XSj=_xq4-qUDVPqaNJ71qOA2kt9pAA@mail.gmail.com>

1. Jeff, thank you for your explanation!
<Jeff Newmiller: It means that one of the packages installed during R
installation is damaged>
2 Kum-Hoe, thank you for appointed treatment!
<Kum-Hoe Hwang: Try to reinstall lattice library>
---
QUESTION!
I will also (with R) reinstall the RStudio. Allow me to ask a QUESTION
(about packages installing) -- which variant is right:
1) installing all packages only in R (RStudio will take them automatically)
2) installing all packages only in RStudio (R will take them automatically)
3) installing all packages in R and in RStudio (i.e. in each of them)
Thanks, Medic


From fr@inj @ending from gm@il@com  Mon Dec 24 11:41:48 2018
From: fr@inj @ending from gm@il@com (John C Frain)
Date: Mon, 24 Dec 2018 10:41:48 +0000
Subject: [R] Installation Re: Problems with library...
In-Reply-To: <CAH6117+jLvTBsDhsRFr9XSj=_xq4-qUDVPqaNJ71qOA2kt9pAA@mail.gmail.com>
References: <CAH6117+jLvTBsDhsRFr9XSj=_xq4-qUDVPqaNJ71qOA2kt9pAA@mail.gmail.com>
Message-ID: <CAHrK515E7FPm4LPOXQNs-1iA+0KmuUmSN6TQFKp7vgg=R9xxWQ@mail.gmail.com>

If I update in R the updates are visible in Rstudio and visa versa.  If you
use the default windows installation of both there should be no problems.
Check that Rstudio is not using a different version of R to the version
that contains packages updated in R and visa versa.

On 24 Dec 2018 09:31, "Medic" <mailiPadpost at gmail.com> wrote:

1. Jeff, thank you for your explanation!
<Jeff Newmiller: It means that one of the packages installed during R
installation is damaged>
2 Kum-Hoe, thank you for appointed treatment!
<Kum-Hoe Hwang: Try to reinstall lattice library>
---
QUESTION!
I will also (with R) reinstall the RStudio. Allow me to ask a QUESTION
(about packages installing) -- which variant is right:
1) installing all packages only in R (RStudio will take them automatically)
2) installing all packages only in RStudio (R will take them automatically)
3) installing all packages in R and in RStudio (i.e. in each of them)
Thanks, Medic

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From murdoch@dunc@n @ending from gm@il@com  Mon Dec 24 12:30:56 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Mon, 24 Dec 2018 06:30:56 -0500
Subject: [R] Installation Re: Problems with library...
In-Reply-To: <CAH6117+jLvTBsDhsRFr9XSj=_xq4-qUDVPqaNJ71qOA2kt9pAA@mail.gmail.com>
References: <CAH6117+jLvTBsDhsRFr9XSj=_xq4-qUDVPqaNJ71qOA2kt9pAA@mail.gmail.com>
Message-ID: <80b9fadf-0b36-5100-4537-beaae64e47c6@gmail.com>

On 24/12/2018 4:30 a.m., Medic wrote:
> 1. Jeff, thank you for your explanation!
> <Jeff Newmiller: It means that one of the packages installed during R
> installation is damaged>
> 2 Kum-Hoe, thank you for appointed treatment!
> <Kum-Hoe Hwang: Try to reinstall lattice library>
> ---
> QUESTION!
> I will also (with R) reinstall the RStudio. Allow me to ask a QUESTION
> (about packages installing) -- which variant is right:
> 1) installing all packages only in R (RStudio will take them automatically)
> 2) installing all packages only in RStudio (R will take them automatically)
> 3) installing all packages in R and in RStudio (i.e. in each of them)

1 and 2 are the same.  The principle is that all installation happens in 
R; RStudio is just a wrapper around R.

Duncan Murdoch


From m@iliP@dpo@t @ending from gm@il@com  Mon Dec 24 17:44:05 2018
From: m@iliP@dpo@t @ending from gm@il@com (Medic)
Date: Mon, 24 Dec 2018 19:44:05 +0300
Subject: [R] THANKS! Re:  Installation...
Message-ID: <CAH6117+2jWhmSx94Dh49f693R7RTGweMGXHPidnMGZ_HQo4zyQ@mail.gmail.com>

1. Dear John, thank you for the important point!
<John C Frain: ... Check that Rstudio is not using a different version
of R to the version that contains packages updated in R and visa
versa>
2. Dear Duncan, thank you for the clear clarification!
<Duncan Murdoch: 1) ... All installation happens in R; 2) RStudio is
just a wrapper around R>


From @nnij@nh @ending from gm@il@com  Mon Dec 24 20:14:40 2018
From: @nnij@nh @ending from gm@il@com (Janh Anni)
Date: Mon, 24 Dec 2018 14:14:40 -0500
Subject: [R] Issues with R3.5.2
In-Reply-To: <FB9E284F-94F5-4A58-8B6E-5DCDA107313B@dcn.davis.ca.us>
References: <CAFCoDdBa3XxZMQZJamNOUCjOcY+p5umsn95mPW2MTbqqR=Ps7g@mail.gmail.com>
 <61D9034F-5E36-404C-869D-2E3A4E6DC9EC@dcn.davis.ca.us>
 <CAFCoDdATK3V_V2qBOR2fMTX40QUwsdSsRJgO=azZBVkjG+NBBQ@mail.gmail.com>
 <221C51C8-C5C7-45A4-BF9F-D5DEE0461662@dcn.davis.ca.us>
 <CAFCoDdBW38f5CazbqA-3xzfJHKPSN39tj73rGkhg=bdGn-aXBw@mail.gmail.com>
 <FB9E284F-94F5-4A58-8B6E-5DCDA107313B@dcn.davis.ca.us>
Message-ID: <CAFCoDdBDKiKVpJwdavAh_zBPhYJ95_c84FiiT5c3rzQsKwkpCw@mail.gmail.com>

Hello Jeff, Martin,

I deleted 3.5.2 as suggested and tried 3.5.1 but still had the same
problems.  I still couldn't use read.table to load a data file and still
had an error message when I tried to install a package. Usually after
installing a new version of R, I would go to the R icon on the desktop,
right click on it, click on Properties and then specify the folder that
contains my data files in the "Start in" box, so that R automatically has
access to my data files.  Could that possibly be causing problems with
these newer versions of R? Also, there?s never a prompt during the
installation to Run as Administrator, so that could not possibly be the
cause

 I also just tried Version 3.4.4, and had no problems whatsoever either
with using read.table to load data files or downloading packages.  So there
must be some changes from version 3.5 onward that created the issues.
Hopefully this will be looked at more closely by the team with  a view to
resolving the issues

Thanks,

Janh

On Sun, Dec 23, 2018 at 1:31 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> You could delete your 3.5 personal package library (using the File
> Explorer with Run as Admin if necessary) and re-install your packages
> without running as Admin. If that does not work try uninstalling R and
> re-installing 3.5.1.
>
> On December 22, 2018 8:16:11 PM PST, Janh Anni <annijanh at gmail.com> wrote:
> >This issue only came up after I installed R3.5.2. Never had any
> >problems
> >with previous installations.  So it is likely a bug in the current
> >version.  Any suggestions what to do now?
> >
> >On Sat, Dec 22, 2018 at 11:06 PM Jeff Newmiller
> ><jdnewmil at dcn.davis.ca.us>
> >wrote:
> >
> >> That normally only occurs if you have at some time used elevated
> >> permissions, beyond which point you fall into a downward spiral of
> >more
> >> permissions trouble. You are apparently already in trouble, whether
> >it was
> >> of your own making or due to a bug in the installer.
> >>
> >> Also, never update the system R package library... always use a
> >personal
> >> library.
> >>
> >> On December 22, 2018 6:01:44 PM PST, Janh Anni <annijanh at gmail.com>
> >wrote:
> >> >Hi Jeff,
> >> >
> >> >No, during the installation, there was not an option to Run as
> >> >Administration.  But *after *installation, I found that if I
> >selected
> >> >Run
> >> >as Administrator, then I could install packages using
> >install.packages
> >> >as
> >> >usual without problems.
> >> >
> >> >Thanks
> >> >Janh
> >> >
> >> >On Sat, Dec 22, 2018 at 8:26 PM Jeff Newmiller
> >> ><jdnewmil at dcn.davis.ca.us>
> >> >wrote:
> >> >
> >> >> Did you by any chance use Run As Administrator to install R? If so
> >> >then
> >> >> you need to uninstall it and delete all files created by it (e.g.
> >> >> Documents/R/win-lib/3.5/) and re-install using UAC as prompted.
> >> >>
> >> >> On December 22, 2018 5:10:27 PM PST, Janh Anni
> ><annijanh at gmail.com>
> >> >wrote:
> >> >> >Dear R Experts,
> >> >> >
> >> >> >I use Windows 10 and just installed the new R version, R3.5.2 but
> >> >when
> >> >> >I
> >> >> >tried to load a data file using read.table, I got an error
> >message
> >> >like
> >> >> >this:
> >> >> >
> >> >> >*Error in file(file, "rt") : cannot open the connection*
> >> >> >*In addition: Warning message:*
> >> >> >*In file(file, "rt") :*
> >> >> >*  cannot open file 'StreamPCB.dat': No such file or directory*
> >> >> >
> >> >> >Also, I couldn't install packages using install.packages as
> >usual,
> >> >> >unless I
> >> >> >run R as Administrator
> >> >> >
> >> >> >I wonder if anyone else had the same issues and any suggestions
> >how
> >> >to
> >> >> >fix?
> >> >> >
> >> >> >Thanks a lot
> >> >> >Janh
> >> >> >
> >> >> >       [[alternative HTML version deleted]]
> >> >> >
> >> >> >______________________________________________
> >> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >> >https://stat.ethz.ch/mailman/listinfo/r-help
> >> >> >PLEASE do read the posting guide
> >> >> >http://www.R-project.org/posting-guide.html
> >> >> >and provide commented, minimal, self-contained, reproducible
> >code.
> >> >>
> >> >> --
> >> >> Sent from my phone. Please excuse my brevity.
> >> >>
> >>
> >> --
> >> Sent from my phone. Please excuse my brevity.
> >>
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From jdnewmil @ending from dcn@d@vi@@c@@u@  Mon Dec 24 22:28:00 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Mon, 24 Dec 2018 13:28:00 -0800
Subject: [R] Issues with R3.5.2
In-Reply-To: <CAFCoDdBDKiKVpJwdavAh_zBPhYJ95_c84FiiT5c3rzQsKwkpCw@mail.gmail.com>
References: <CAFCoDdBa3XxZMQZJamNOUCjOcY+p5umsn95mPW2MTbqqR=Ps7g@mail.gmail.com>
 <61D9034F-5E36-404C-869D-2E3A4E6DC9EC@dcn.davis.ca.us>
 <CAFCoDdATK3V_V2qBOR2fMTX40QUwsdSsRJgO=azZBVkjG+NBBQ@mail.gmail.com>
 <221C51C8-C5C7-45A4-BF9F-D5DEE0461662@dcn.davis.ca.us>
 <CAFCoDdBW38f5CazbqA-3xzfJHKPSN39tj73rGkhg=bdGn-aXBw@mail.gmail.com>
 <FB9E284F-94F5-4A58-8B6E-5DCDA107313B@dcn.davis.ca.us>
 <CAFCoDdBDKiKVpJwdavAh_zBPhYJ95_c84FiiT5c3rzQsKwkpCw@mail.gmail.com>
Message-ID: <B4E32426-5B5B-41FF-9303-84CAEE041797@dcn.davis.ca.us>



On December 24, 2018 11:14:40 AM PST, Janh Anni <annijanh at gmail.com> wrote:
>Hello Jeff, Martin,
>
>I deleted 3.5.2 as suggested and tried 3.5.1 but still had the same
>problems.  I still couldn't use read.table to load a data file and
>still
>had an error message when I tried to install a package. Usually after
>installing a new version of R, I would go to the R icon on the desktop,
>right click on it, click on Properties and then specify the folder that
>contains my data files in the "Start in" box, so that R automatically
>has
>access to my data files.  Could that possibly be causing problems with
>these newer versions of R? 

No.

> Also, there?s never a prompt during the
>installation to Run as Administrator, so that could not possibly be the
>cause

Yes, you have to go out of your way to Run As Administrator (RAA). However, regardless of how you initially encountered a problem, once you did that there could be any number of files contaminated with permissions issues.

Specifically, I said to delete your 3.5 personal package library, but your description is not specific so I suspect you may not have done that. However, any file modified by you intentionally or not while using RAA could be causing your problems now, so now it is up to you to find those files somehow.

>
> I also just tried Version 3.4.4, and had no problems whatsoever either
>with using read.table to load data files or downloading packages.  So
>there
>must be some changes from version 3.5 onward that created the issues.
>Hopefully this will be looked at more closely by the team with  a view
>to
>resolving the issues

I doubt the "team" will spend much time looking at this based on your descriptions so far. They need something reproducible, and the fact that you already used RAA to "fix" the problem makes anything you did prior to that almost impossible to reproduce.

fortunes:::fortune(337)

>
>Thanks,
>
>Janh
>
>On Sun, Dec 23, 2018 at 1:31 AM Jeff Newmiller
><jdnewmil at dcn.davis.ca.us>
>wrote:
>
>> You could delete your 3.5 personal package library (using the File
>> Explorer with Run as Admin if necessary) and re-install your packages
>> without running as Admin. If that does not work try uninstalling R
>and
>> re-installing 3.5.1.
>>
>> On December 22, 2018 8:16:11 PM PST, Janh Anni <annijanh at gmail.com>
>wrote:
>> >This issue only came up after I installed R3.5.2. Never had any
>> >problems
>> >with previous installations.  So it is likely a bug in the current
>> >version.  Any suggestions what to do now?
>> >
>> >On Sat, Dec 22, 2018 at 11:06 PM Jeff Newmiller
>> ><jdnewmil at dcn.davis.ca.us>
>> >wrote:
>> >
>> >> That normally only occurs if you have at some time used elevated
>> >> permissions, beyond which point you fall into a downward spiral of
>> >more
>> >> permissions trouble. You are apparently already in trouble,
>whether
>> >it was
>> >> of your own making or due to a bug in the installer.
>> >>
>> >> Also, never update the system R package library... always use a
>> >personal
>> >> library.
>> >>
>> >> On December 22, 2018 6:01:44 PM PST, Janh Anni
><annijanh at gmail.com>
>> >wrote:
>> >> >Hi Jeff,
>> >> >
>> >> >No, during the installation, there was not an option to Run as
>> >> >Administration.  But *after *installation, I found that if I
>> >selected
>> >> >Run
>> >> >as Administrator, then I could install packages using
>> >install.packages
>> >> >as
>> >> >usual without problems.
>> >> >
>> >> >Thanks
>> >> >Janh
>> >> >
>> >> >On Sat, Dec 22, 2018 at 8:26 PM Jeff Newmiller
>> >> ><jdnewmil at dcn.davis.ca.us>
>> >> >wrote:
>> >> >
>> >> >> Did you by any chance use Run As Administrator to install R? If
>so
>> >> >then
>> >> >> you need to uninstall it and delete all files created by it
>(e.g.
>> >> >> Documents/R/win-lib/3.5/) and re-install using UAC as prompted.
>> >> >>
>> >> >> On December 22, 2018 5:10:27 PM PST, Janh Anni
>> ><annijanh at gmail.com>
>> >> >wrote:
>> >> >> >Dear R Experts,
>> >> >> >
>> >> >> >I use Windows 10 and just installed the new R version, R3.5.2
>but
>> >> >when
>> >> >> >I
>> >> >> >tried to load a data file using read.table, I got an error
>> >message
>> >> >like
>> >> >> >this:
>> >> >> >
>> >> >> >*Error in file(file, "rt") : cannot open the connection*
>> >> >> >*In addition: Warning message:*
>> >> >> >*In file(file, "rt") :*
>> >> >> >*  cannot open file 'StreamPCB.dat': No such file or
>directory*
>> >> >> >
>> >> >> >Also, I couldn't install packages using install.packages as
>> >usual,
>> >> >> >unless I
>> >> >> >run R as Administrator
>> >> >> >
>> >> >> >I wonder if anyone else had the same issues and any
>suggestions
>> >how
>> >> >to
>> >> >> >fix?
>> >> >> >
>> >> >> >Thanks a lot
>> >> >> >Janh
>> >> >> >
>> >> >> >       [[alternative HTML version deleted]]
>> >> >> >
>> >> >> >______________________________________________
>> >> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>see
>> >> >> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >> >> >PLEASE do read the posting guide
>> >> >> >http://www.R-project.org/posting-guide.html
>> >> >> >and provide commented, minimal, self-contained, reproducible
>> >code.
>> >> >>
>> >> >> --
>> >> >> Sent from my phone. Please excuse my brevity.
>> >> >>
>> >>
>> >> --
>> >> Sent from my phone. Please excuse my brevity.
>> >>
>>
>> --
>> Sent from my phone. Please excuse my brevity.
>>

-- 
Sent from my phone. Please excuse my brevity.


From @nnij@nh @ending from gm@il@com  Mon Dec 24 23:43:24 2018
From: @nnij@nh @ending from gm@il@com (Janh Anni)
Date: Mon, 24 Dec 2018 17:43:24 -0500
Subject: [R] Issues with R3.5.2
In-Reply-To: <B4E32426-5B5B-41FF-9303-84CAEE041797@dcn.davis.ca.us>
References: <CAFCoDdBa3XxZMQZJamNOUCjOcY+p5umsn95mPW2MTbqqR=Ps7g@mail.gmail.com>
 <61D9034F-5E36-404C-869D-2E3A4E6DC9EC@dcn.davis.ca.us>
 <CAFCoDdATK3V_V2qBOR2fMTX40QUwsdSsRJgO=azZBVkjG+NBBQ@mail.gmail.com>
 <221C51C8-C5C7-45A4-BF9F-D5DEE0461662@dcn.davis.ca.us>
 <CAFCoDdBW38f5CazbqA-3xzfJHKPSN39tj73rGkhg=bdGn-aXBw@mail.gmail.com>
 <FB9E284F-94F5-4A58-8B6E-5DCDA107313B@dcn.davis.ca.us>
 <CAFCoDdBDKiKVpJwdavAh_zBPhYJ95_c84FiiT5c3rzQsKwkpCw@mail.gmail.com>
 <B4E32426-5B5B-41FF-9303-84CAEE041797@dcn.davis.ca.us>
Message-ID: <CAFCoDdBhdKmBDMjRqbZbkO4=+zfPr7_WVM8Q-TFRbE1pLaE_uw@mail.gmail.com>

I am sorry I forgot to mention - I just looked in the
Documents\R\win-Library directory and only found folders for previous R
versions, specifically R3.0, 3.1 and 3.4.  So I must have deleted the R3.5
folder as you initially advised or it was removed during the
un-installation.  Unless I am misunderstanding what you mean by the R3.5
personal package library?  Thanks again

On Mon, Dec 24, 2018 at 4:28 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

>
>
> On December 24, 2018 11:14:40 AM PST, Janh Anni <annijanh at gmail.com>
> wrote:
> >Hello Jeff, Martin,
> >
> >I deleted 3.5.2 as suggested and tried 3.5.1 but still had the same
> >problems.  I still couldn't use read.table to load a data file and
> >still
> >had an error message when I tried to install a package. Usually after
> >installing a new version of R, I would go to the R icon on the desktop,
> >right click on it, click on Properties and then specify the folder that
> >contains my data files in the "Start in" box, so that R automatically
> >has
> >access to my data files.  Could that possibly be causing problems with
> >these newer versions of R?
>
> No.
>
> > Also, there?s never a prompt during the
> >installation to Run as Administrator, so that could not possibly be the
> >cause
>
> Yes, you have to go out of your way to Run As Administrator (RAA).
> However, regardless of how you initially encountered a problem, once you
> did that there could be any number of files contaminated with permissions
> issues.
>
> Specifically, I said to delete your 3.5 personal package library, but your
> description is not specific so I suspect you may not have done that.
> However, any file modified by you intentionally or not while using RAA
> could be causing your problems now, so now it is up to you to find those
> files somehow.
>
> >
> > I also just tried Version 3.4.4, and had no problems whatsoever either
> >with using read.table to load data files or downloading packages.  So
> >there
> >must be some changes from version 3.5 onward that created the issues.
> >Hopefully this will be looked at more closely by the team with  a view
> >to
> >resolving the issues
>
> I doubt the "team" will spend much time looking at this based on your
> descriptions so far. They need something reproducible, and the fact that
> you already used RAA to "fix" the problem makes anything you did prior to
> that almost impossible to reproduce.
>
> fortunes:::fortune(337)
>
> >
> >Thanks,
> >
> >Janh
> >
> >On Sun, Dec 23, 2018 at 1:31 AM Jeff Newmiller
> ><jdnewmil at dcn.davis.ca.us>
> >wrote:
> >
> >> You could delete your 3.5 personal package library (using the File
> >> Explorer with Run as Admin if necessary) and re-install your packages
> >> without running as Admin. If that does not work try uninstalling R
> >and
> >> re-installing 3.5.1.
> >>
> >> On December 22, 2018 8:16:11 PM PST, Janh Anni <annijanh at gmail.com>
> >wrote:
> >> >This issue only came up after I installed R3.5.2. Never had any
> >> >problems
> >> >with previous installations.  So it is likely a bug in the current
> >> >version.  Any suggestions what to do now?
> >> >
> >> >On Sat, Dec 22, 2018 at 11:06 PM Jeff Newmiller
> >> ><jdnewmil at dcn.davis.ca.us>
> >> >wrote:
> >> >
> >> >> That normally only occurs if you have at some time used elevated
> >> >> permissions, beyond which point you fall into a downward spiral of
> >> >more
> >> >> permissions trouble. You are apparently already in trouble,
> >whether
> >> >it was
> >> >> of your own making or due to a bug in the installer.
> >> >>
> >> >> Also, never update the system R package library... always use a
> >> >personal
> >> >> library.
> >> >>
> >> >> On December 22, 2018 6:01:44 PM PST, Janh Anni
> ><annijanh at gmail.com>
> >> >wrote:
> >> >> >Hi Jeff,
> >> >> >
> >> >> >No, during the installation, there was not an option to Run as
> >> >> >Administration.  But *after *installation, I found that if I
> >> >selected
> >> >> >Run
> >> >> >as Administrator, then I could install packages using
> >> >install.packages
> >> >> >as
> >> >> >usual without problems.
> >> >> >
> >> >> >Thanks
> >> >> >Janh
> >> >> >
> >> >> >On Sat, Dec 22, 2018 at 8:26 PM Jeff Newmiller
> >> >> ><jdnewmil at dcn.davis.ca.us>
> >> >> >wrote:
> >> >> >
> >> >> >> Did you by any chance use Run As Administrator to install R? If
> >so
> >> >> >then
> >> >> >> you need to uninstall it and delete all files created by it
> >(e.g.
> >> >> >> Documents/R/win-lib/3.5/) and re-install using UAC as prompted.
> >> >> >>
> >> >> >> On December 22, 2018 5:10:27 PM PST, Janh Anni
> >> ><annijanh at gmail.com>
> >> >> >wrote:
> >> >> >> >Dear R Experts,
> >> >> >> >
> >> >> >> >I use Windows 10 and just installed the new R version, R3.5.2
> >but
> >> >> >when
> >> >> >> >I
> >> >> >> >tried to load a data file using read.table, I got an error
> >> >message
> >> >> >like
> >> >> >> >this:
> >> >> >> >
> >> >> >> >*Error in file(file, "rt") : cannot open the connection*
> >> >> >> >*In addition: Warning message:*
> >> >> >> >*In file(file, "rt") :*
> >> >> >> >*  cannot open file 'StreamPCB.dat': No such file or
> >directory*
> >> >> >> >
> >> >> >> >Also, I couldn't install packages using install.packages as
> >> >usual,
> >> >> >> >unless I
> >> >> >> >run R as Administrator
> >> >> >> >
> >> >> >> >I wonder if anyone else had the same issues and any
> >suggestions
> >> >how
> >> >> >to
> >> >> >> >fix?
> >> >> >> >
> >> >> >> >Thanks a lot
> >> >> >> >Janh
> >> >> >> >
> >> >> >> >       [[alternative HTML version deleted]]
> >> >> >> >
> >> >> >> >______________________________________________
> >> >> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >see
> >> >> >> >https://stat.ethz.ch/mailman/listinfo/r-help
> >> >> >> >PLEASE do read the posting guide
> >> >> >> >http://www.R-project.org/posting-guide.html
> >> >> >> >and provide commented, minimal, self-contained, reproducible
> >> >code.
> >> >> >>
> >> >> >> --
> >> >> >> Sent from my phone. Please excuse my brevity.
> >> >> >>
> >> >>
> >> >> --
> >> >> Sent from my phone. Please excuse my brevity.
> >> >>
> >>
> >> --
> >> Sent from my phone. Please excuse my brevity.
> >>
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From jdnewmil @ending from dcn@d@vi@@c@@u@  Tue Dec 25 00:09:43 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Mon, 24 Dec 2018 15:09:43 -0800
Subject: [R] Issues with R3.5.2
In-Reply-To: <CAFCoDdBhdKmBDMjRqbZbkO4=+zfPr7_WVM8Q-TFRbE1pLaE_uw@mail.gmail.com>
References: <CAFCoDdBa3XxZMQZJamNOUCjOcY+p5umsn95mPW2MTbqqR=Ps7g@mail.gmail.com>
 <61D9034F-5E36-404C-869D-2E3A4E6DC9EC@dcn.davis.ca.us>
 <CAFCoDdATK3V_V2qBOR2fMTX40QUwsdSsRJgO=azZBVkjG+NBBQ@mail.gmail.com>
 <221C51C8-C5C7-45A4-BF9F-D5DEE0461662@dcn.davis.ca.us>
 <CAFCoDdBW38f5CazbqA-3xzfJHKPSN39tj73rGkhg=bdGn-aXBw@mail.gmail.com>
 <FB9E284F-94F5-4A58-8B6E-5DCDA107313B@dcn.davis.ca.us>
 <CAFCoDdBDKiKVpJwdavAh_zBPhYJ95_c84FiiT5c3rzQsKwkpCw@mail.gmail.com>
 <B4E32426-5B5B-41FF-9303-84CAEE041797@dcn.davis.ca.us>
 <CAFCoDdBhdKmBDMjRqbZbkO4=+zfPr7_WVM8Q-TFRbE1pLaE_uw@mail.gmail.com>
Message-ID: <3AD6F3D6-D6F2-4510-B24D-84B5D0FC5562@dcn.davis.ca.us>

Yes, that would be the personal library.

There is one question in the installer that asks if you would like to create a personal library... I have always said yes, but I could imagine that saying no could lead to problems.

On December 24, 2018 2:43:24 PM PST, Janh Anni <annijanh at gmail.com> wrote:
>I am sorry I forgot to mention - I just looked in the
>Documents\R\win-Library directory and only found folders for previous R
>versions, specifically R3.0, 3.1 and 3.4.  So I must have deleted the
>R3.5
>folder as you initially advised or it was removed during the
>un-installation.  Unless I am misunderstanding what you mean by the
>R3.5
>personal package library?  Thanks again
>
>On Mon, Dec 24, 2018 at 4:28 PM Jeff Newmiller
><jdnewmil at dcn.davis.ca.us>
>wrote:
>
>>
>>
>> On December 24, 2018 11:14:40 AM PST, Janh Anni <annijanh at gmail.com>
>> wrote:
>> >Hello Jeff, Martin,
>> >
>> >I deleted 3.5.2 as suggested and tried 3.5.1 but still had the same
>> >problems.  I still couldn't use read.table to load a data file and
>> >still
>> >had an error message when I tried to install a package. Usually
>after
>> >installing a new version of R, I would go to the R icon on the
>desktop,
>> >right click on it, click on Properties and then specify the folder
>that
>> >contains my data files in the "Start in" box, so that R
>automatically
>> >has
>> >access to my data files.  Could that possibly be causing problems
>with
>> >these newer versions of R?
>>
>> No.
>>
>> > Also, there?s never a prompt during the
>> >installation to Run as Administrator, so that could not possibly be
>the
>> >cause
>>
>> Yes, you have to go out of your way to Run As Administrator (RAA).
>> However, regardless of how you initially encountered a problem, once
>you
>> did that there could be any number of files contaminated with
>permissions
>> issues.
>>
>> Specifically, I said to delete your 3.5 personal package library, but
>your
>> description is not specific so I suspect you may not have done that.
>> However, any file modified by you intentionally or not while using
>RAA
>> could be causing your problems now, so now it is up to you to find
>those
>> files somehow.
>>
>> >
>> > I also just tried Version 3.4.4, and had no problems whatsoever
>either
>> >with using read.table to load data files or downloading packages. 
>So
>> >there
>> >must be some changes from version 3.5 onward that created the
>issues.
>> >Hopefully this will be looked at more closely by the team with  a
>view
>> >to
>> >resolving the issues
>>
>> I doubt the "team" will spend much time looking at this based on your
>> descriptions so far. They need something reproducible, and the fact
>that
>> you already used RAA to "fix" the problem makes anything you did
>prior to
>> that almost impossible to reproduce.
>>
>> fortunes:::fortune(337)
>>
>> >
>> >Thanks,
>> >
>> >Janh
>> >
>> >On Sun, Dec 23, 2018 at 1:31 AM Jeff Newmiller
>> ><jdnewmil at dcn.davis.ca.us>
>> >wrote:
>> >
>> >> You could delete your 3.5 personal package library (using the File
>> >> Explorer with Run as Admin if necessary) and re-install your
>packages
>> >> without running as Admin. If that does not work try uninstalling R
>> >and
>> >> re-installing 3.5.1.
>> >>
>> >> On December 22, 2018 8:16:11 PM PST, Janh Anni
><annijanh at gmail.com>
>> >wrote:
>> >> >This issue only came up after I installed R3.5.2. Never had any
>> >> >problems
>> >> >with previous installations.  So it is likely a bug in the
>current
>> >> >version.  Any suggestions what to do now?
>> >> >
>> >> >On Sat, Dec 22, 2018 at 11:06 PM Jeff Newmiller
>> >> ><jdnewmil at dcn.davis.ca.us>
>> >> >wrote:
>> >> >
>> >> >> That normally only occurs if you have at some time used
>elevated
>> >> >> permissions, beyond which point you fall into a downward spiral
>of
>> >> >more
>> >> >> permissions trouble. You are apparently already in trouble,
>> >whether
>> >> >it was
>> >> >> of your own making or due to a bug in the installer.
>> >> >>
>> >> >> Also, never update the system R package library... always use a
>> >> >personal
>> >> >> library.
>> >> >>
>> >> >> On December 22, 2018 6:01:44 PM PST, Janh Anni
>> ><annijanh at gmail.com>
>> >> >wrote:
>> >> >> >Hi Jeff,
>> >> >> >
>> >> >> >No, during the installation, there was not an option to Run as
>> >> >> >Administration.  But *after *installation, I found that if I
>> >> >selected
>> >> >> >Run
>> >> >> >as Administrator, then I could install packages using
>> >> >install.packages
>> >> >> >as
>> >> >> >usual without problems.
>> >> >> >
>> >> >> >Thanks
>> >> >> >Janh
>> >> >> >
>> >> >> >On Sat, Dec 22, 2018 at 8:26 PM Jeff Newmiller
>> >> >> ><jdnewmil at dcn.davis.ca.us>
>> >> >> >wrote:
>> >> >> >
>> >> >> >> Did you by any chance use Run As Administrator to install R?
>If
>> >so
>> >> >> >then
>> >> >> >> you need to uninstall it and delete all files created by it
>> >(e.g.
>> >> >> >> Documents/R/win-lib/3.5/) and re-install using UAC as
>prompted.
>> >> >> >>
>> >> >> >> On December 22, 2018 5:10:27 PM PST, Janh Anni
>> >> ><annijanh at gmail.com>
>> >> >> >wrote:
>> >> >> >> >Dear R Experts,
>> >> >> >> >
>> >> >> >> >I use Windows 10 and just installed the new R version,
>R3.5.2
>> >but
>> >> >> >when
>> >> >> >> >I
>> >> >> >> >tried to load a data file using read.table, I got an error
>> >> >message
>> >> >> >like
>> >> >> >> >this:
>> >> >> >> >
>> >> >> >> >*Error in file(file, "rt") : cannot open the connection*
>> >> >> >> >*In addition: Warning message:*
>> >> >> >> >*In file(file, "rt") :*
>> >> >> >> >*  cannot open file 'StreamPCB.dat': No such file or
>> >directory*
>> >> >> >> >
>> >> >> >> >Also, I couldn't install packages using install.packages as
>> >> >usual,
>> >> >> >> >unless I
>> >> >> >> >run R as Administrator
>> >> >> >> >
>> >> >> >> >I wonder if anyone else had the same issues and any
>> >suggestions
>> >> >how
>> >> >> >to
>> >> >> >> >fix?
>> >> >> >> >
>> >> >> >> >Thanks a lot
>> >> >> >> >Janh
>> >> >> >> >
>> >> >> >> >       [[alternative HTML version deleted]]
>> >> >> >> >
>> >> >> >> >______________________________________________
>> >> >> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and
>more,
>> >see
>> >> >> >> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >> >> >> >PLEASE do read the posting guide
>> >> >> >> >http://www.R-project.org/posting-guide.html
>> >> >> >> >and provide commented, minimal, self-contained,
>reproducible
>> >> >code.
>> >> >> >>
>> >> >> >> --
>> >> >> >> Sent from my phone. Please excuse my brevity.
>> >> >> >>
>> >> >>
>> >> >> --
>> >> >> Sent from my phone. Please excuse my brevity.
>> >> >>
>> >>
>> >> --
>> >> Sent from my phone. Please excuse my brevity.
>> >>
>>
>> --
>> Sent from my phone. Please excuse my brevity.
>>

-- 
Sent from my phone. Please excuse my brevity.


From @nnij@nh @ending from gm@il@com  Tue Dec 25 00:36:20 2018
From: @nnij@nh @ending from gm@il@com (Janh Anni)
Date: Mon, 24 Dec 2018 18:36:20 -0500
Subject: [R] Issues with R3.5.2
In-Reply-To: <3AD6F3D6-D6F2-4510-B24D-84B5D0FC5562@dcn.davis.ca.us>
References: <CAFCoDdBa3XxZMQZJamNOUCjOcY+p5umsn95mPW2MTbqqR=Ps7g@mail.gmail.com>
 <61D9034F-5E36-404C-869D-2E3A4E6DC9EC@dcn.davis.ca.us>
 <CAFCoDdATK3V_V2qBOR2fMTX40QUwsdSsRJgO=azZBVkjG+NBBQ@mail.gmail.com>
 <221C51C8-C5C7-45A4-BF9F-D5DEE0461662@dcn.davis.ca.us>
 <CAFCoDdBW38f5CazbqA-3xzfJHKPSN39tj73rGkhg=bdGn-aXBw@mail.gmail.com>
 <FB9E284F-94F5-4A58-8B6E-5DCDA107313B@dcn.davis.ca.us>
 <CAFCoDdBDKiKVpJwdavAh_zBPhYJ95_c84FiiT5c3rzQsKwkpCw@mail.gmail.com>
 <B4E32426-5B5B-41FF-9303-84CAEE041797@dcn.davis.ca.us>
 <CAFCoDdBhdKmBDMjRqbZbkO4=+zfPr7_WVM8Q-TFRbE1pLaE_uw@mail.gmail.com>
 <3AD6F3D6-D6F2-4510-B24D-84B5D0FC5562@dcn.davis.ca.us>
Message-ID: <CAFCoDdCH=QKYj+uCwWpgYkKPEu+XVym7usSKcavN4=dx2qsxJQ@mail.gmail.com>

Oddly enough, I must have done a dozen installations and re-installations
since this issue arose but don't recall ever being asked if I wished to
create a personal library.  I do recall being offered the option to install
R to a different folder other than the usual Program Files folder, or to
choose custom installation rather than the usual defaults, or to create a
desk shortcut or quick launch shortcut and so on.  But this whole RAA issue
is sobering - the fact that clicking Run as Administrator at some point or
the other, either intentionally or inadvertently can suddenly render R
inoperable ...........

On Mon, Dec 24, 2018 at 6:09 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Yes, that would be the personal library.
>
> There is one question in the installer that asks if you would like to
> create a personal library... I have always said yes, but I could imagine
> that saying no could lead to problems.
>
> On December 24, 2018 2:43:24 PM PST, Janh Anni <annijanh at gmail.com> wrote:
> >I am sorry I forgot to mention - I just looked in the
> >Documents\R\win-Library directory and only found folders for previous R
> >versions, specifically R3.0, 3.1 and 3.4.  So I must have deleted the
> >R3.5
> >folder as you initially advised or it was removed during the
> >un-installation.  Unless I am misunderstanding what you mean by the
> >R3.5
> >personal package library?  Thanks again
> >
> >On Mon, Dec 24, 2018 at 4:28 PM Jeff Newmiller
> ><jdnewmil at dcn.davis.ca.us>
> >wrote:
> >
> >>
> >>
> >> On December 24, 2018 11:14:40 AM PST, Janh Anni <annijanh at gmail.com>
> >> wrote:
> >> >Hello Jeff, Martin,
> >> >
> >> >I deleted 3.5.2 as suggested and tried 3.5.1 but still had the same
> >> >problems.  I still couldn't use read.table to load a data file and
> >> >still
> >> >had an error message when I tried to install a package. Usually
> >after
> >> >installing a new version of R, I would go to the R icon on the
> >desktop,
> >> >right click on it, click on Properties and then specify the folder
> >that
> >> >contains my data files in the "Start in" box, so that R
> >automatically
> >> >has
> >> >access to my data files.  Could that possibly be causing problems
> >with
> >> >these newer versions of R?
> >>
> >> No.
> >>
> >> > Also, there?s never a prompt during the
> >> >installation to Run as Administrator, so that could not possibly be
> >the
> >> >cause
> >>
> >> Yes, you have to go out of your way to Run As Administrator (RAA).
> >> However, regardless of how you initially encountered a problem, once
> >you
> >> did that there could be any number of files contaminated with
> >permissions
> >> issues.
> >>
> >> Specifically, I said to delete your 3.5 personal package library, but
> >your
> >> description is not specific so I suspect you may not have done that.
> >> However, any file modified by you intentionally or not while using
> >RAA
> >> could be causing your problems now, so now it is up to you to find
> >those
> >> files somehow.
> >>
> >> >
> >> > I also just tried Version 3.4.4, and had no problems whatsoever
> >either
> >> >with using read.table to load data files or downloading packages.
> >So
> >> >there
> >> >must be some changes from version 3.5 onward that created the
> >issues.
> >> >Hopefully this will be looked at more closely by the team with  a
> >view
> >> >to
> >> >resolving the issues
> >>
> >> I doubt the "team" will spend much time looking at this based on your
> >> descriptions so far. They need something reproducible, and the fact
> >that
> >> you already used RAA to "fix" the problem makes anything you did
> >prior to
> >> that almost impossible to reproduce.
> >>
> >> fortunes:::fortune(337)
> >>
> >> >
> >> >Thanks,
> >> >
> >> >Janh
> >> >
> >> >On Sun, Dec 23, 2018 at 1:31 AM Jeff Newmiller
> >> ><jdnewmil at dcn.davis.ca.us>
> >> >wrote:
> >> >
> >> >> You could delete your 3.5 personal package library (using the File
> >> >> Explorer with Run as Admin if necessary) and re-install your
> >packages
> >> >> without running as Admin. If that does not work try uninstalling R
> >> >and
> >> >> re-installing 3.5.1.
> >> >>
> >> >> On December 22, 2018 8:16:11 PM PST, Janh Anni
> ><annijanh at gmail.com>
> >> >wrote:
> >> >> >This issue only came up after I installed R3.5.2. Never had any
> >> >> >problems
> >> >> >with previous installations.  So it is likely a bug in the
> >current
> >> >> >version.  Any suggestions what to do now?
> >> >> >
> >> >> >On Sat, Dec 22, 2018 at 11:06 PM Jeff Newmiller
> >> >> ><jdnewmil at dcn.davis.ca.us>
> >> >> >wrote:
> >> >> >
> >> >> >> That normally only occurs if you have at some time used
> >elevated
> >> >> >> permissions, beyond which point you fall into a downward spiral
> >of
> >> >> >more
> >> >> >> permissions trouble. You are apparently already in trouble,
> >> >whether
> >> >> >it was
> >> >> >> of your own making or due to a bug in the installer.
> >> >> >>
> >> >> >> Also, never update the system R package library... always use a
> >> >> >personal
> >> >> >> library.
> >> >> >>
> >> >> >> On December 22, 2018 6:01:44 PM PST, Janh Anni
> >> ><annijanh at gmail.com>
> >> >> >wrote:
> >> >> >> >Hi Jeff,
> >> >> >> >
> >> >> >> >No, during the installation, there was not an option to Run as
> >> >> >> >Administration.  But *after *installation, I found that if I
> >> >> >selected
> >> >> >> >Run
> >> >> >> >as Administrator, then I could install packages using
> >> >> >install.packages
> >> >> >> >as
> >> >> >> >usual without problems.
> >> >> >> >
> >> >> >> >Thanks
> >> >> >> >Janh
> >> >> >> >
> >> >> >> >On Sat, Dec 22, 2018 at 8:26 PM Jeff Newmiller
> >> >> >> ><jdnewmil at dcn.davis.ca.us>
> >> >> >> >wrote:
> >> >> >> >
> >> >> >> >> Did you by any chance use Run As Administrator to install R?
> >If
> >> >so
> >> >> >> >then
> >> >> >> >> you need to uninstall it and delete all files created by it
> >> >(e.g.
> >> >> >> >> Documents/R/win-lib/3.5/) and re-install using UAC as
> >prompted.
> >> >> >> >>
> >> >> >> >> On December 22, 2018 5:10:27 PM PST, Janh Anni
> >> >> ><annijanh at gmail.com>
> >> >> >> >wrote:
> >> >> >> >> >Dear R Experts,
> >> >> >> >> >
> >> >> >> >> >I use Windows 10 and just installed the new R version,
> >R3.5.2
> >> >but
> >> >> >> >when
> >> >> >> >> >I
> >> >> >> >> >tried to load a data file using read.table, I got an error
> >> >> >message
> >> >> >> >like
> >> >> >> >> >this:
> >> >> >> >> >
> >> >> >> >> >*Error in file(file, "rt") : cannot open the connection*
> >> >> >> >> >*In addition: Warning message:*
> >> >> >> >> >*In file(file, "rt") :*
> >> >> >> >> >*  cannot open file 'StreamPCB.dat': No such file or
> >> >directory*
> >> >> >> >> >
> >> >> >> >> >Also, I couldn't install packages using install.packages as
> >> >> >usual,
> >> >> >> >> >unless I
> >> >> >> >> >run R as Administrator
> >> >> >> >> >
> >> >> >> >> >I wonder if anyone else had the same issues and any
> >> >suggestions
> >> >> >how
> >> >> >> >to
> >> >> >> >> >fix?
> >> >> >> >> >
> >> >> >> >> >Thanks a lot
> >> >> >> >> >Janh
> >> >> >> >> >
> >> >> >> >> >       [[alternative HTML version deleted]]
> >> >> >> >> >
> >> >> >> >> >______________________________________________
> >> >> >> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and
> >more,
> >> >see
> >> >> >> >> >https://stat.ethz.ch/mailman/listinfo/r-help
> >> >> >> >> >PLEASE do read the posting guide
> >> >> >> >> >http://www.R-project.org/posting-guide.html
> >> >> >> >> >and provide commented, minimal, self-contained,
> >reproducible
> >> >> >code.
> >> >> >> >>
> >> >> >> >> --
> >> >> >> >> Sent from my phone. Please excuse my brevity.
> >> >> >> >>
> >> >> >>
> >> >> >> --
> >> >> >> Sent from my phone. Please excuse my brevity.
> >> >> >>
> >> >>
> >> >> --
> >> >> Sent from my phone. Please excuse my brevity.
> >> >>
> >>
> >> --
> >> Sent from my phone. Please excuse my brevity.
> >>
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From prof@@mitmitt@l @ending from gm@il@com  Tue Dec 25 02:38:15 2018
From: prof@@mitmitt@l @ending from gm@il@com (prof.amitmittal)
Date: Tue, 25 Dec 2018 07:08:15 +0530
Subject: [R] Issues with R3.5.2
Message-ID: <1gtq5gcvhk673dlif3sh023o.1545701895566@email.android.com>


    
you can just use libpath to specify which packages to use instead of deleting librariesalso use sessionInfo () at the beginning to see what is loadedSent from my Samsung device

-------- Original message --------
From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us> 
Date: 25/12/2018  4:39 am  (GMT+05:30) 
To: Janh Anni <annijanh at gmail.com> 
Cc: r-help at r-project.org 
Subject: Re: [R] Issues with R3.5.2 

Yes, that would be the personal library.There is one question in the installer that asks if you would like to create a personal library... I have always said yes, but I could imagine that saying no could lead to problems.On December 24, 2018 2:43:24 PM PST, Janh Anni <annijanh at gmail.com> wrote:>I am sorry I forgot to mention - I just looked in the>Documents\R\win-Library directory and only found folders for previous R>versions, specifically R3.0, 3.1 and 3.4.? So I must have deleted the>R3.5>folder as you initially advised or it was removed during the>un-installation.? Unless I am misunderstanding what you mean by the>R3.5>personal package library?? Thanks again>>On Mon, Dec 24, 2018 at 4:28 PM Jeff Newmiller><jdnewmil at dcn.davis.ca.us>>wrote:>>>>>>> On December 24, 2018 11:14:40 AM PST, Janh Anni <annijanh at gmail.com>>> wrote:>> >Hello Jeff, Martin,>> >>> >I deleted 3.5.2 as suggested and tried 3.5.1 but still had the same>> >problems.? I still couldn't use read.table to load a data file and>> >still>> >had an error message when I tried to install a package. Usually>after>> >installing a new version of R, I would go to the R icon on the>desktop,>> >right click on it, click on Properties and then specify the folder>that>> >contains my data files in the "Start in" box, so that R>automatically>> >has>> >access to my data files.? Could that possibly be causing problems>with>> >these newer versions of R?>>>> No.>>>> > Also, there?s never a prompt during the>> >installation to Run as Administrator, so that could not possibly be>the>> >cause>>>> Yes, you have to go out of your way to Run As Administrator (RAA).>> However, regardless of how you initially encountered a problem, once>you>> did that there could be any number of files contaminated with>permissions>> issues.>>>> Specifically, I said to delete your 3.5 personal package library, but>your>> description is not specific so I suspect you may not have done that.>> However, any file modified by you intentionally or not while using>RAA>> could be causing your problems now, so now it is up to you to find>those>> files somehow.>>>> >>> > I also just tried Version 3.4.4, and had no problems whatsoever>either>> >with using read.table to load data files or downloading packages. >So>> >there>> >must be some changes from version 3.5 onward that created the>issues.>> >Hopefully this will be looked at more closely by the team with? a>view>> >to>> >resolving the issues>>>> I doubt the "team" will spend much time looking at this based on your>> descriptions so far. They need something reproducible, and the fact>that>> you already used RAA to "fix" the problem makes anything you did>prior to>> that almost impossible to reproduce.>>>> fortunes:::fortune(337)>>>> >>> >Thanks,>> >>> >Janh>> >>> >On Sun, Dec 23, 2018 at 1:31 AM Jeff Newmiller>> ><jdnewmil at dcn.davis.ca.us>>> >wrote:>> >>> >> You could delete your 3.5 personal package library (using the File>> >> Explorer with Run as Admin if necessary) and re-install your>packages>> >> without running as Admin. If that does not work try uninstalling R>> >and>> >> re-installing 3.5.1.>> >>>> >> On December 22, 2018 8:16:11 PM PST, Janh Anni><annijanh at gmail.com>>> >wrote:>> >> >This issue only came up after I installed R3.5.2. Never had any>> >> >problems>> >> >with previous installations.? So it is likely a bug in the>current>> >> >version.? Any suggestions what to do now?>> >> >>> >> >On Sat, Dec 22, 2018 at 11:06 PM Jeff Newmiller>> >> ><jdnewmil at dcn.davis.ca.us>>> >> >wrote:>> >> >>> >> >> That normally only occurs if you have at some time used>elevated>> >> >> permissions, beyond which point you fall into a downward spiral>of>> >> >more>> >> >> permissions trouble. You are apparently already in trouble,>> >whether>> >> >it was>> >> >> of your own making or due to a bug in the installer.>> >> >>>> >> >> Also, never update the system R package library... always use a>> >> >personal>> >> >> library.>> >> >>>> >> >> On December 22, 2018 6:01:44 PM PST, Janh Anni>> ><annijanh at gmail.com>>> >> >wrote:>> >> >> >Hi Jeff,>> >> >> >>> >> >> >No, during the installation, there was not an option to Run as>> >> >> >Administration.? But *after *installation, I found that if I>> >> >selected>> >> >> >Run>> >> >> >as Administrator, then I could install packages using>> >> >install.packages>> >> >> >as>> >> >> >usual without problems.>> >> >> >>> >> >> >Thanks>> >> >> >Janh>> >> >> >>> >> >> >On Sat, Dec 22, 2018 at 8:26 PM Jeff Newmiller>> >> >> ><jdnewmil at dcn.davis.ca.us>>> >> >> >wrote:>> >> >> >>> >> >> >> Did you by any chance use Run As Administrator to install R?>If>> >so>> >> >> >then>> >> >> >> you need to uninstall it and delete all files created by it>> >(e.g.>> >> >> >> Documents/R/win-lib/3.5/) and re-install using UAC as>prompted.>> >> >> >>>> >> >> >> On December 22, 2018 5:10:27 PM PST, Janh Anni>> >> ><annijanh at gmail.com>>> >> >> >wrote:>> >> >> >> >Dear R Experts,>> >> >> >> >>> >> >> >> >I use Windows 10 and just installed the new R version,>R3.5.2>> >but>> >> >> >when>> >> >> >> >I>> >> >> >> >tried to load a data file using read.table, I got an error>> >> >message>> >> >> >like>> >> >> >> >this:>> >> >> >> >>> >> >> >> >*Error in file(file, "rt") : cannot open the connection*>> >> >> >> >*In addition: Warning message:*>> >> >> >> >*In file(file, "rt") :*>> >> >> >> >*? cannot open file 'StreamPCB.dat': No such file or>> >directory*>> >> >> >> >>> >> >> >> >Also, I couldn't install packages using install.packages as>> >> >usual,>> >> >> >> >unless I>> >> >> >> >run R as Administrator>> >> >> >> >>> >> >> >> >I wonder if anyone else had the same issues and any>> >suggestions>> >> >how>> >> >> >to>> >> >> >> >fix?>> >> >> >> >>> >> >> >> >Thanks a lot>> >> >> >> >Janh>> >> >> >> >>> >> >> >> >?????? [[alternative HTML version deleted]]>> >> >> >> >>> >> >> >> >______________________________________________>> >> >> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and>more,>> >see>> >> >> >> >https://stat.ethz.ch/mailman/listinfo/r-help>> >> >> >> >PLEASE do read the posting guide>> >> >> >> >http://www.R-project.org/posting-guide.html>> >> >> >> >and provide commented, minimal, self-contained,>reproducible>> >> >code.>> >> >> >>>> >> >> >> -->> >> >> >> Sent from my phone. Please excuse my brevity.>> >> >> >>>> >> >>>> >> >> -->> >> >> Sent from my phone. Please excuse my brevity.>> >> >>>> >>>> >> -->> >> Sent from my phone. Please excuse my brevity.>> >>>>>> -->> Sent from my phone. Please excuse my brevity.>>-- Sent from my phone. Please excuse my brevity.______________________________________________R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, seehttps://stat.ethz.ch/mailman/listinfo/r-helpPLEASE do read the posting guide http://www.R-project.org/posting-guide.htmland provide commented, minimal, self-contained, reproducible code.
	[[alternative HTML version deleted]]


From mzp3769 @ending from gm@il@com  Tue Dec 25 03:12:49 2018
From: mzp3769 @ending from gm@il@com (M P)
Date: Mon, 24 Dec 2018 19:12:49 -0700
Subject: [R] fields package question
Message-ID: <CAARDZMXZ0Lcdatg6xY0qqOyyqZw0HgW3CcPVTOMYyX7xkZrcag@mail.gmail.com>

Hello,
I used commands below to obtain a surface, can plot it and all looks as
expected.
How do I evaluate values at new point. I tried as below but that produces
errors.
Thanks for suggestions/help.

x <- log(lambda)
y <- rh
z <- qext[,,2]

grid.l <- list(abcissa=x,ordinate=y)
xg <- make.surface.grid(grid.l)
out.p <- as.surface(xg,z)
plot.surface(out.p,type="p")

tried:
grid_new.l <- list(abcissa=c(-15.0,-10.),ordinate=y)
xg_new <- make.surface.grid(grid_new.l)

out_new.p <- predict.surface(out.p,xg_new)

results in this prompt:
predict.surface is now the function predictSurface>

	[[alternative HTML version deleted]]


From @nnij@nh @ending from gm@il@com  Tue Dec 25 03:14:48 2018
From: @nnij@nh @ending from gm@il@com (Janh Anni)
Date: Mon, 24 Dec 2018 21:14:48 -0500
Subject: [R] Issues with R3.5.2
In-Reply-To: <1gtq5gcvhk673dlif3sh023o.1545701895566@email.android.com>
References: <1gtq5gcvhk673dlif3sh023o.1545701895566@email.android.com>
Message-ID: <CAFCoDdDUnbuADTrMjab9W6BWhPrp4OVdNg1ur74=+f7RiP7O7g@mail.gmail.com>

Thanks!

On Mon, Dec 24, 2018 at 8:38 PM prof.amitmittal <prof.amitmittal at gmail.com>
wrote:

> you can just use libpath to specify which packages to use instead of
> deleting libraries
>
> also use sessionInfo () at the beginning to see what is loaded
>
>
>
> Sent from my Samsung device
>
>
> -------- Original message --------
> From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> Date: 25/12/2018 4:39 am (GMT+05:30)
> To: Janh Anni <annijanh at gmail.com>
> Cc: r-help at r-project.org
> Subject: Re: [R] Issues with R3.5.2
>
> Yes, that would be the personal library.
>
> There is one question in the installer that asks if you would like to
> create a personal library... I have always said yes, but I could imagine
> that saying no could lead to problems.
>
> On December 24, 2018 2:43:24 PM PST, Janh Anni <annijanh at gmail.com> wrote:
> >I am sorry I forgot to mention - I just looked in the
> >Documents\R\win-Library directory and only found folders for previous R
> >versions, specifically R3.0, 3.1 and 3.4.  So I must have deleted the
> >R3.5
> >folder as you initially advised or it was removed during the
> >un-installation.  Unless I am misunderstanding what you mean by the
> >R3.5
> >personal package library?  Thanks again
> >
> >On Mon, Dec 24, 2018 at 4:28 PM Jeff Newmiller
> ><jdnewmil at dcn.davis.ca.us>
> >wrote:
> >
> >>
> >>
> >> On December 24, 2018 11:14:40 AM PST, Janh Anni <annijanh at gmail.com>
> >> wrote:
> >> >Hello Jeff, Martin,
> >> >
> >> >I deleted 3.5.2 as suggested and tried 3.5.1 but still had the same
> >> >problems.  I still couldn't use read.table to load a data file and
> >> >still
> >> >had an error message when I tried to install a package. Usually
> >after
> >> >installing a new version of R, I would go to the R icon on the
> >desktop,
> >> >right click on it, click on Properties and then specify the folder
> >that
> >> >contains my data files in the "Start in" box, so that R
> >automatically
> >> >has
> >> >access to my data files.  Could that possibly be causing problems
> >with
> >> >these newer versions of R?
> >>
> >> No.
> >>
> >> > Also, there?s never a prompt during the
> >> >installation to Run as Administrator, so that could not possibly be
> >the
> >> >cause
> >>
> >> Yes, you have to go out of your way to Run As Administrator (RAA).
> >> However, regardless of how you initially encountered a problem, once
> >you
> >> did that there could be any number of files contaminated with
> >permissions
> >> issues.
> >>
> >> Specifically, I said to delete your 3.5 personal package library, but
> >your
> >> description is not specific so I suspect you may not have done that.
> >> However, any file modified by you intentionally or not while using
> >RAA
> >> could be causing your problems now, so now it is up to you to find
> >those
> >> files somehow.
> >>
> >> >
> >> > I also just tried Version 3.4.4, and had no problems whatsoever
> >either
> >> >with using read.table to load data files or downloading packages.
> >So
> >> >there
> >> >must be some changes from version 3.5 onward that created the
> >issues.
> >> >Hopefully this will be looked at more closely by the team with  a
> >view
> >> >to
> >> >resolving the issues
> >>
> >> I doubt the "team" will spend much time looking at this based on your
> >> descriptions so far. They need something reproducible, and the fact
> >that
> >> you already used RAA to "fix" the problem makes anything you did
> >prior to
> >> that almost impossible to reproduce.
> >>
> >> fortunes:::fortune(337)
> >>
> >> >
> >> >Thanks,
> >> >
> >> >Janh
> >> >
> >> >On Sun, Dec 23, 2018 at 1:31 AM Jeff Newmiller
> >> ><jdnewmil at dcn.davis.ca.us>
> >> >wrote:
> >> >
> >> >> You could delete your 3.5 personal package library (using the File
> >> >> Explorer with Run as Admin if necessary) and re-install your
> >packages
> >> >> without running as Admin. If that does not work try uninstalling R
> >> >and
> >> >> re-installing 3.5.1.
> >> >>
> >> >> On December 22, 2018 8:16:11 PM PST, Janh Anni
> ><annijanh at gmail.com>
> >> >wrote:
> >> >> >This issue only came up after I installed R3.5.2. Never had any
> >> >> >problems
> >> >> >with previous installations.  So it is likely a bug in the
> >current
> >> >> >version.  Any suggestions what to do now?
> >> >> >
> >> >> >On Sat, Dec 22, 2018 at 11:06 PM Jeff Newmiller
> >> >> ><jdnewmil at dcn.davis.ca.us>
> >> >> >wrote:
> >> >> >
> >> >> >> That normally only occurs if you have at some time used
> >elevated
> >> >> >> permissions, beyond which point you fall into a downward spiral
> >of
> >> >> >more
> >> >> >> permissions trouble. You are apparently already in trouble,
> >> >whether
> >> >> >it was
> >> >> >> of your own making or due to a bug in the installer.
> >> >> >>
> >> >> >> Also, never update the system R package library... always use a
> >> >> >personal
> >> >> >> library.
> >> >> >>
> >> >> >> On December 22, 2018 6:01:44 PM PST, Janh Anni
> >> ><annijanh at gmail.com>
> >> >> >wrote:
> >> >> >> >Hi Jeff,
> >> >> >> >
> >> >> >> >No, during the installation, there was not an option to Run as
> >> >> >> >Administration.  But *after *installation, I found that if I
> >> >> >selected
> >> >> >> >Run
> >> >> >> >as Administrator, then I could install packages using
> >> >> >install.packages
> >> >> >> >as
> >> >> >> >usual without problems.
> >> >> >> >
> >> >> >> >Thanks
> >> >> >> >Janh
> >> >> >> >
> >> >> >> >On Sat, Dec 22, 2018 at 8:26 PM Jeff Newmiller
> >> >> >> ><jdnewmil at dcn.davis.ca.us>
> >> >> >> >wrote:
> >> >> >> >
> >> >> >> >> Did you by any chance use Run As Administrator to install R?
> >If
> >> >so
> >> >> >> >then
> >> >> >> >> you need to uninstall it and delete all files created by it
> >> >(e.g.
> >> >> >> >> Documents/R/win-lib/3.5/) and re-install using UAC as
> >prompted.
> >> >> >> >>
> >> >> >> >> On December 22, 2018 5:10:27 PM PST, Janh Anni
> >> >> ><annijanh at gmail.com>
> >> >> >> >wrote:
> >> >> >> >> >Dear R Experts,
> >> >> >> >> >
> >> >> >> >> >I use Windows 10 and just installed the new R version,
> >R3.5.2
> >> >but
> >> >> >> >when
> >> >> >> >> >I
> >> >> >> >> >tried to load a data file using read.table, I got an error
> >> >> >message
> >> >> >> >like
> >> >> >> >> >this:
> >> >> >> >> >
> >> >> >> >> >*Error in file(file, "rt") : cannot open the connection*
> >> >> >> >> >*In addition: Warning message:*
> >> >> >> >> >*In file(file, "rt") :*
> >> >> >> >> >*  cannot open file 'StreamPCB.dat': No such file or
> >> >directory*
> >> >> >> >> >
> >> >> >> >> >Also, I couldn't install packages using install.packages as
> >> >> >usual,
> >> >> >> >> >unless I
> >> >> >> >> >run R as Administrator
> >> >> >> >> >
> >> >> >> >> >I wonder if anyone else had the same issues and any
> >> >suggestions
> >> >> >how
> >> >> >> >to
> >> >> >> >> >fix?
> >> >> >> >> >
> >> >> >> >> >Thanks a lot
> >> >> >> >> >Janh
> >> >> >> >> >
> >> >> >> >> >       [[alternative HTML version deleted]]
> >> >> >> >> >
> >> >> >> >> >______________________________________________
> >> >> >> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and
> >more,
> >> >see
> >> >> >> >> >https://stat.ethz.ch/mailman/listinfo/r-help
> >> >> >> >> >PLEASE do read the posting guide
> >> >> >> >> >http://www.R-project.org/posting-guide.html
> >> >> >> >> >and provide commented, minimal, self-contained,
> >reproducible
> >> >> >code.
> >> >> >> >>
> >> >> >> >> --
> >> >> >> >> Sent from my phone. Please excuse my brevity.
> >> >> >> >>
> >> >> >>
> >> >> >> --
> >> >> >> Sent from my phone. Please excuse my brevity.
> >> >> >>
> >> >>
> >> >> --
> >> >> Sent from my phone. Please excuse my brevity.
> >> >>
> >>
> >> --
> >> Sent from my phone. Please excuse my brevity.
> >>
>
> --
> Sent from my phone. Please excuse my brevity.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From prof@@mit@mitt@l @ending from gm@il@com  Tue Dec 25 05:21:45 2018
From: prof@@mit@mitt@l @ending from gm@il@com (Amit Mittal)
Date: Tue, 25 Dec 2018 09:51:45 +0530
Subject: [R] Issues with R3.5.2
In-Reply-To: <CAFCoDdCH=QKYj+uCwWpgYkKPEu+XVym7usSKcavN4=dx2qsxJQ@mail.gmail.com>
References: <CAFCoDdBa3XxZMQZJamNOUCjOcY+p5umsn95mPW2MTbqqR=Ps7g@mail.gmail.com>
 <61D9034F-5E36-404C-869D-2E3A4E6DC9EC@dcn.davis.ca.us>
 <CAFCoDdATK3V_V2qBOR2fMTX40QUwsdSsRJgO=azZBVkjG+NBBQ@mail.gmail.com>
 <221C51C8-C5C7-45A4-BF9F-D5DEE0461662@dcn.davis.ca.us>
 <CAFCoDdBW38f5CazbqA-3xzfJHKPSN39tj73rGkhg=bdGn-aXBw@mail.gmail.com>
 <FB9E284F-94F5-4A58-8B6E-5DCDA107313B@dcn.davis.ca.us>
 <CAFCoDdBDKiKVpJwdavAh_zBPhYJ95_c84FiiT5c3rzQsKwkpCw@mail.gmail.com>
 <B4E32426-5B5B-41FF-9303-84CAEE041797@dcn.davis.ca.us>
 <CAFCoDdBhdKmBDMjRqbZbkO4=+zfPr7_WVM8Q-TFRbE1pLaE_uw@mail.gmail.com>
 <3AD6F3D6-D6F2-4510-B24D-84B5D0FC5562@dcn.davis.ca.us>
 <CAFCoDdCH=QKYj+uCwWpgYkKPEu+XVym7usSKcavN4=dx2qsxJQ@mail.gmail.com>
Message-ID: <5c21b05a.1c69fb81.332e4.0cf4@mx.google.com>

It asks you when you install packages after loading the current workspace TYPICALLY WHEN CORE PACKAGES ARE NOT SUFFICIENT

BR
Amit



Amit Mittal 
5th year - PhD in Finance and Accounting 
IIM Lucknow 
http://ssrn.com/author=2665511 
*Top 10%, downloaded author since July 2017



From: Janh Anni
Sent: Tuesday, December 25, 2018 5:07 AM
To: Jeff Newmiller
Cc: r-help at r-project.org
Subject: Re: [R] Issues with R3.5.2

Oddly enough, I must have done a dozen installations and re-installations
since this issue arose but don't recall ever being asked if I wished to
create a personal library.  I do recall being offered the option to install
R to a different folder other than the usual Program Files folder, or to
choose custom installation rather than the usual defaults, or to create a
desk shortcut or quick launch shortcut and so on.  But this whole RAA issue
is sobering - the fact that clicking Run as Administrator at some point or
the other, either intentionally or inadvertently can suddenly render R
inoperable ...........

On Mon, Dec 24, 2018 at 6:09 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Yes, that would be the personal library.
>
> There is one question in the installer that asks if you would like to
> create a personal library... I have always said yes, but I could imagine
> that saying no could lead to problems.
>
> On December 24, 2018 2:43:24 PM PST, Janh Anni <annijanh at gmail.com> wrote:
> >I am sorry I forgot to mention - I just looked in the
> >Documents\R\win-Library directory and only found folders for previous R
> >versions, specifically R3.0, 3.1 and 3.4.  So I must have deleted the
> >R3.5
> >folder as you initially advised or it was removed during the
> >un-installation.  Unless I am misunderstanding what you mean by the
> >R3.5
> >personal package library?  Thanks again
> >
> >On Mon, Dec 24, 2018 at 4:28 PM Jeff Newmiller
> ><jdnewmil at dcn.davis.ca.us>
> >wrote:
> >
> >>
> >>
> >> On December 24, 2018 11:14:40 AM PST, Janh Anni <annijanh at gmail.com>
> >> wrote:
> >> >Hello Jeff, Martin,
> >> >
> >> >I deleted 3.5.2 as suggested and tried 3.5.1 but still had the same
> >> >problems.  I still couldn't use read.table to load a data file and
> >> >still
> >> >had an error message when I tried to install a package. Usually
> >after
> >> >installing a new version of R, I would go to the R icon on the
> >desktop,
> >> >right click on it, click on Properties and then specify the folder
> >that
> >> >contains my data files in the "Start in" box, so that R
> >automatically
> >> >has
> >> >access to my data files.  Could that possibly be causing problems
> >with
> >> >these newer versions of R?
> >>
> >> No.
> >>
> >> > Also, there?s never a prompt during the
> >> >installation to Run as Administrator, so that could not possibly be
> >the
> >> >cause
> >>
> >> Yes, you have to go out of your way to Run As Administrator (RAA).
> >> However, regardless of how you initially encountered a problem, once
> >you
> >> did that there could be any number of files contaminated with
> >permissions
> >> issues.
> >>
> >> Specifically, I said to delete your 3.5 personal package library, but
> >your
> >> description is not specific so I suspect you may not have done that.
> >> However, any file modified by you intentionally or not while using
> >RAA
> >> could be causing your problems now, so now it is up to you to find
> >those
> >> files somehow.
> >>
> >> >
> >> > I also just tried Version 3.4.4, and had no problems whatsoever
> >either
> >> >with using read.table to load data files or downloading packages.
> >So
> >> >there
> >> >must be some changes from version 3.5 onward that created the
> >issues.
> >> >Hopefully this will be looked at more closely by the team with  a
> >view
> >> >to
> >> >resolving the issues
> >>
> >> I doubt the "team" will spend much time looking at this based on your
> >> descriptions so far. They need something reproducible, and the fact
> >that
> >> you already used RAA to "fix" the problem makes anything you did
> >prior to
> >> that almost impossible to reproduce.
> >>
> >> fortunes:::fortune(337)
> >>
> >> >
> >> >Thanks,
> >> >
> >> >Janh
> >> >
> >> >On Sun, Dec 23, 2018 at 1:31 AM Jeff Newmiller
> >> ><jdnewmil at dcn.davis.ca.us>
> >> >wrote:
> >> >
> >> >> You could delete your 3.5 personal package library (using the File
> >> >> Explorer with Run as Admin if necessary) and re-install your
> >packages
> >> >> without running as Admin. If that does not work try uninstalling R
> >> >and
> >> >> re-installing 3.5.1.
> >> >>
> >> >> On December 22, 2018 8:16:11 PM PST, Janh Anni
> ><annijanh at gmail.com>
> >> >wrote:
> >> >> >This issue only came up after I installed R3.5.2. Never had any
> >> >> >problems
> >> >> >with previous installations.  So it is likely a bug in the
> >current
> >> >> >version.  Any suggestions what to do now?
> >> >> >
> >> >> >On Sat, Dec 22, 2018 at 11:06 PM Jeff Newmiller
> >> >> ><jdnewmil at dcn.davis.ca.us>
> >> >> >wrote:
> >> >> >
> >> >> >> That normally only occurs if you have at some time used
> >elevated
> >> >> >> permissions, beyond which point you fall into a downward spiral
> >of
> >> >> >more
> >> >> >> permissions trouble. You are apparently already in trouble,
> >> >whether
> >> >> >it was
> >> >> >> of your own making or due to a bug in the installer.
> >> >> >>
> >> >> >> Also, never update the system R package library... always use a
> >> >> >personal
> >> >> >> library.
> >> >> >>
> >> >> >> On December 22, 2018 6:01:44 PM PST, Janh Anni
> >> ><annijanh at gmail.com>
> >> >> >wrote:
> >> >> >> >Hi Jeff,
> >> >> >> >
> >> >> >> >No, during the installation, there was not an option to Run as
> >> >> >> >Administration.  But *after *installation, I found that if I
> >> >> >selected
> >> >> >> >Run
> >> >> >> >as Administrator, then I could install packages using
> >> >> >install.packages
> >> >> >> >as
> >> >> >> >usual without problems.
> >> >> >> >
> >> >> >> >Thanks
> >> >> >> >Janh
> >> >> >> >
> >> >> >> >On Sat, Dec 22, 2018 at 8:26 PM Jeff Newmiller
> >> >> >> ><jdnewmil at dcn.davis.ca.us>
> >> >> >> >wrote:
> >> >> >> >
> >> >> >> >> Did you by any chance use Run As Administrator to install R?
> >If
> >> >so
> >> >> >> >then
> >> >> >> >> you need to uninstall it and delete all files created by it
> >> >(e.g.
> >> >> >> >> Documents/R/win-lib/3.5/) and re-install using UAC as
> >prompted.
> >> >> >> >>
> >> >> >> >> On December 22, 2018 5:10:27 PM PST, Janh Anni
> >> >> ><annijanh at gmail.com>
> >> >> >> >wrote:
> >> >> >> >> >Dear R Experts,
> >> >> >> >> >
> >> >> >> >> >I use Windows 10 and just installed the new R version,
> >R3.5.2
> >> >but
> >> >> >> >when
> >> >> >> >> >I
> >> >> >> >> >tried to load a data file using read.table, I got an error
> >> >> >message
> >> >> >> >like
> >> >> >> >> >this:
> >> >> >> >> >
> >> >> >> >> >*Error in file(file, "rt") : cannot open the connection*
> >> >> >> >> >*In addition: Warning message:*
> >> >> >> >> >*In file(file, "rt") :*
> >> >> >> >> >*  cannot open file 'StreamPCB.dat': No such file or
> >> >directory*
> >> >> >> >> >
> >> >> >> >> >Also, I couldn't install packages using install.packages as
> >> >> >usual,
> >> >> >> >> >unless I
> >> >> >> >> >run R as Administrator
> >> >> >> >> >
> >> >> >> >> >I wonder if anyone else had the same issues and any
> >> >suggestions
> >> >> >how
> >> >> >> >to
> >> >> >> >> >fix?
> >> >> >> >> >
> >> >> >> >> >Thanks a lot
> >> >> >> >> >Janh
> >> >> >> >> >
> >> >> >> >> >       [[alternative HTML version deleted]]
> >> >> >> >> >
> >> >> >> >> >______________________________________________
> >> >> >> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and
> >more,
> >> >see
> >> >> >> >> >https://stat.ethz.ch/mailman/listinfo/r-help
> >> >> >> >> >PLEASE do read the posting guide
> >> >> >> >> >http://www.R-project.org/posting-guide.html
> >> >> >> >> >and provide commented, minimal, self-contained,
> >reproducible
> >> >> >code.
> >> >> >> >>
> >> >> >> >> --
> >> >> >> >> Sent from my phone. Please excuse my brevity.
> >> >> >> >>
> >> >> >>
> >> >> >> --
> >> >> >> Sent from my phone. Please excuse my brevity.
> >> >> >>
> >> >>
> >> >> --
> >> >> Sent from my phone. Please excuse my brevity.
> >> >>
> >>
> >> --
> >> Sent from my phone. Please excuse my brevity.
> >>
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From ericjberger @ending from gm@il@com  Tue Dec 25 08:44:33 2018
From: ericjberger @ending from gm@il@com (Eric Berger)
Date: Tue, 25 Dec 2018 09:44:33 +0200
Subject: [R] fields package question
In-Reply-To: <CAARDZMXZ0Lcdatg6xY0qqOyyqZw0HgW3CcPVTOMYyX7xkZrcag@mail.gmail.com>
References: <CAARDZMXZ0Lcdatg6xY0qqOyyqZw0HgW3CcPVTOMYyX7xkZrcag@mail.gmail.com>
Message-ID: <CAGgJW75kArnOq7O1k1R8nJEu5H-A-rG1yp0D-+SfZRi_9ACBJw@mail.gmail.com>

Since you don't provide lambda, rh or qext it is impossible to reproduce
what you are seeing.
Also note that in this mailing list HTML formatted emails are not passed
along.



On Tue, Dec 25, 2018 at 4:13 AM M P <mzp3769 at gmail.com> wrote:

> Hello,
> I used commands below to obtain a surface, can plot it and all looks as
> expected.
> How do I evaluate values at new point. I tried as below but that produces
> errors.
> Thanks for suggestions/help.
>
> x <- log(lambda)
> y <- rh
> z <- qext[,,2]
>
> grid.l <- list(abcissa=x,ordinate=y)
> xg <- make.surface.grid(grid.l)
> out.p <- as.surface(xg,z)
> plot.surface(out.p,type="p")
>
> tried:
> grid_new.l <- list(abcissa=c(-15.0,-10.),ordinate=y)
> xg_new <- make.surface.grid(grid_new.l)
>
> out_new.p <- predict.surface(out.p,xg_new)
>
> results in this prompt:
> predict.surface is now the function predictSurface>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From giucilli@ @ending from gm@il@com  Sat Dec 22 09:58:46 2018
From: giucilli@ @ending from gm@il@com (Giuseppe Cillis)
Date: Sat, 22 Dec 2018 09:58:46 +0100
Subject: [R] =?utf-8?q?Fwd=3A_Problem_with_Kruskal=E2=80=93Wallis_test?=
In-Reply-To: <CAE43vpgKp0ThqqHm7YBB=XL66q3zUf=8f_XhgHPBMqsShgG1gQ@mail.gmail.com>
References: <CAE43vpgKp0ThqqHm7YBB=XL66q3zUf=8f_XhgHPBMqsShgG1gQ@mail.gmail.com>
Message-ID: <CAE43vpjW8bif3-5eRdqgQddhAn2qCk-oyrQy8MYcvVWWSHiY1Q@mail.gmail.com>

Dear all,
I am a beginner with R (and also with the statistics) for which I hope to
be clear.
I should do this non-parametric test on data I extracted from maps.
In practice I have a column that represents the landscape Dynamics of a
certain time period (there are 3 dynamics, each of them marked by the
number 1, 2 or 3) and the other column with the values of a topographic
variable (for example the slope) . In all, there are more than 90,000 pairs
of values.
Going to do the test in R, for all the dynamics and for all the variables,
I get out of the values of chi-square elevated (even in the order of
thousands) and a p-value always <2.2e-16 .... why? Where can the error be? in
the script or in the test approach?
Thanks in advance

	[[alternative HTML version deleted]]


From giucilli@ @ending from gm@il@com  Sat Dec 22 18:27:41 2018
From: giucilli@ @ending from gm@il@com (Giuseppe Cillis)
Date: Sat, 22 Dec 2018 18:27:41 +0100
Subject: [R] =?utf-8?q?Problem_with_Kruskal=E2=80=93Wallis_test?=
In-Reply-To: <ff12c212-7e48-e65c-4009-fa5a692b822e@dewey.myzen.co.uk>
References: <CAE43vpgKp0ThqqHm7YBB=XL66q3zUf=8f_XhgHPBMqsShgG1gQ@mail.gmail.com>
 <ff12c212-7e48-e65c-4009-fa5a692b822e@dewey.myzen.co.uk>
Message-ID: <CAE43vpiqQvQOqe80wq1bPs+s8nRt=0pK8JjY0fxqEbQ6iEeyVA@mail.gmail.com>

Dear Michael,
Thanks for your answer.
So, I'm not an expert in R and statistics, how can I create this interval
of confidence of groups?
Thanks
Gc

Il giorno sab 22 dic 2018, 13:34 Michael Dewey <lists at dewey.myzen.co.uk> ha
scritto:

> Dear Giuseppe
>
> If I understand you correctly you have a very large sample size so it is
> not surprising that you get very small p-values. Eevn a scientifically
> uninteresting difference can become statistically significant with large
> samples. You probably need to define a metric for meaningful differences
> between groups and calculate a confidence interval for it.
>
> Michael
>
> On 21/12/2018 15:37, Giuseppe Cillis wrote:
> > Dear all,
> > I am a beginner with R (and also with the statistics) for which I hope to
> > be clear.
> > I should do this non-parametric test on data I extracted from maps.
> > In practice I have a column that represents the landscape Dynamics of a
> > certain time period (there are 3 dynamics, each of them marked by the
> > number 1, 2 or 3) and the other column with the values of a topographic
> > variable (for example the slope) . In all, there are more than 90,000
> pairs
> > of values.
> > Going to do the test in R, for all the dynamics and for all the
> variables,
> > I get out of the values of chi-square elevated (even in the order of
> > thousands) and a p-value always <2.2e-16 .... why? Where can the error
> be? in
> > the script or in the test approach?
> > Thanks in advance
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html
>

	[[alternative HTML version deleted]]


From @tyen @ending from ntu@edu@tw  Tue Dec 25 14:42:13 2018
From: @tyen @ending from ntu@edu@tw (Steven Yen)
Date: Tue, 25 Dec 2018 21:42:13 +0800
Subject: [R] Retrievable results in a procedure
Message-ID: <40ac1e02-8a7a-6ef6-09e3-edf6b396e0fa@ntu.edu.tw>

I would like to suppressed printing of retrievable results in a 
procedure and to print only when retrieved.

In line 10 below I call procedure "try" and get matrices A,B,C all 
printed upon a call to the procedure. I get around this unwanted 
printing by calling with v<-try(A,B) as in line 11.

Any way to suppress printing of the retrievable results listed in the 
structure command? Thank you, and Merry Christmas to all.


A<-matrix(rpois(16,lambda=5),nrow=4,byrow=T)
B<-diag(4)

try<-function(A,B){
  C<-A+B
  cat("\nC:\n"); print(C)
structure(list(A=A,B=B,C=C))
}

try(A,B)    # line 10
v<-try(A,B) # line 11

-- 
styen at ntu.edu.tw (S.T. Yen)


	[[alternative HTML version deleted]]


From @@r@h@go@lee @ending from gm@il@com  Tue Dec 25 16:10:41 2018
From: @@r@h@go@lee @ending from gm@il@com (Sarah Goslee)
Date: Tue, 25 Dec 2018 10:10:41 -0500
Subject: [R] Retrievable results in a procedure
In-Reply-To: <40ac1e02-8a7a-6ef6-09e3-edf6b396e0fa@ntu.edu.tw>
References: <40ac1e02-8a7a-6ef6-09e3-edf6b396e0fa@ntu.edu.tw>
Message-ID: <CAM_vju=0cNP2SE1ZdZszS+yJ-8D1GdM6gGC0Fc1BLEnFY0CuYw@mail.gmail.com>

I'm a bit confused about what you actually want, but I think invisible()
might be the answer.

Note that there's already a base function try() so that's not a great name
for test functions.

Sarah

On Tue, Dec 25, 2018 at 8:47 AM Steven Yen <styen at ntu.edu.tw> wrote:

> I would like to suppressed printing of retrievable results in a
> procedure and to print only when retrieved.
>
> In line 10 below I call procedure "try" and get matrices A,B,C all
> printed upon a call to the procedure. I get around this unwanted
> printing by calling with v<-try(A,B) as in line 11.
>
> Any way to suppress printing of the retrievable results listed in the
> structure command? Thank you, and Merry Christmas to all.
>
>
> A<-matrix(rpois(16,lambda=5),nrow=4,byrow=T)
> B<-diag(4)
>
> try<-function(A,B){
>   C<-A+B
>   cat("\nC:\n"); print(C)
> structure(list(A=A,B=B,C=C))
> }
>
> try(A,B)    # line 10
> v<-try(A,B) # line 11
>
> --
> styen at ntu.edu.tw (S.T. Yen)
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
-- 
Sarah Goslee (she/her)
http://www.sarahgoslee.com

	[[alternative HTML version deleted]]


From bgunter@4567 @ending from gm@il@com  Tue Dec 25 16:22:10 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Tue, 25 Dec 2018 07:22:10 -0800
Subject: [R] =?utf-8?q?Problem_with_Kruskal=E2=80=93Wallis_test?=
In-Reply-To: <CAE43vpiqQvQOqe80wq1bPs+s8nRt=0pK8JjY0fxqEbQ6iEeyVA@mail.gmail.com>
References: <CAE43vpgKp0ThqqHm7YBB=XL66q3zUf=8f_XhgHPBMqsShgG1gQ@mail.gmail.com>
 <ff12c212-7e48-e65c-4009-fa5a692b822e@dewey.myzen.co.uk>
 <CAE43vpiqQvQOqe80wq1bPs+s8nRt=0pK8JjY0fxqEbQ6iEeyVA@mail.gmail.com>
Message-ID: <CAGxFJbSeWkGSNYSaOyBe2bqP+UguWKOK3aLRds9_droQd53vSg@mail.gmail.com>

"So, I'm not an expert in R and statistics" ....

So you need to seek local help from someone who is. Statistics is usually
off-topic for this list -- it is about R programming primarily. And online
is probably not a good venue for the sort of discussion you need anyway.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Dec 25, 2018 at 5:37 AM Giuseppe Cillis <giucillis at gmail.com> wrote:

> Dear Michael,
> Thanks for your answer.
> So, I'm not an expert in R and statistics, how can I create this interval
> of confidence of groups?
> Thanks
> Gc
>
> Il giorno sab 22 dic 2018, 13:34 Michael Dewey <lists at dewey.myzen.co.uk>
> ha
> scritto:
>
> > Dear Giuseppe
> >
> > If I understand you correctly you have a very large sample size so it is
> > not surprising that you get very small p-values. Eevn a scientifically
> > uninteresting difference can become statistically significant with large
> > samples. You probably need to define a metric for meaningful differences
> > between groups and calculate a confidence interval for it.
> >
> > Michael
> >
> > On 21/12/2018 15:37, Giuseppe Cillis wrote:
> > > Dear all,
> > > I am a beginner with R (and also with the statistics) for which I hope
> to
> > > be clear.
> > > I should do this non-parametric test on data I extracted from maps.
> > > In practice I have a column that represents the landscape Dynamics of a
> > > certain time period (there are 3 dynamics, each of them marked by the
> > > number 1, 2 or 3) and the other column with the values of a topographic
> > > variable (for example the slope) . In all, there are more than 90,000
> > pairs
> > > of values.
> > > Going to do the test in R, for all the dynamics and for all the
> > variables,
> > > I get out of the values of chi-square elevated (even in the order of
> > > thousands) and a p-value always <2.2e-16 .... why? Where can the error
> > be? in
> > > the script or in the test approach?
> > > Thanks in advance
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > --
> > Michael
> > http://www.dewey.myzen.co.uk/home.html
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewmil @ending from dcn@d@vi@@c@@u@  Tue Dec 25 23:37:21 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Tue, 25 Dec 2018 14:37:21 -0800
Subject: [R] Retrievable results in a procedure
In-Reply-To: <40ac1e02-8a7a-6ef6-09e3-edf6b396e0fa@ntu.edu.tw>
References: <40ac1e02-8a7a-6ef6-09e3-edf6b396e0fa@ntu.edu.tw>
Message-ID: <534BDC69-B3B5-4715-9093-E601747FE9D6@dcn.davis.ca.us>

You can use `capture.output`, but a far, far better solution is to remove the output statements from your computation functions entirely and let the caller decide whether to print the results.

You can, for example, add a `debug` parameter to the function, and if true it can return a list of as many intermediate results as you like that you can examine as you wish.

Of course, if debugging is your goal then learning to use the debug function to mark functions for single-stepping as needed is even better.

But no matter what, making functions that do both computation and output is really poor practice... do one or the other.

On December 25, 2018 5:42:13 AM PST, Steven Yen <styen at ntu.edu.tw> wrote:
>I would like to suppressed printing of retrievable results in a 
>procedure and to print only when retrieved.
>
>In line 10 below I call procedure "try" and get matrices A,B,C all 
>printed upon a call to the procedure. I get around this unwanted 
>printing by calling with v<-try(A,B) as in line 11.
>
>Any way to suppress printing of the retrievable results listed in the 
>structure command? Thank you, and Merry Christmas to all.
>
>
>A<-matrix(rpois(16,lambda=5),nrow=4,byrow=T)
>B<-diag(4)
>
>try<-function(A,B){
>  C<-A+B
>  cat("\nC:\n"); print(C)
>structure(list(A=A,B=B,C=C))
>}
>
>try(A,B)    # line 10
>v<-try(A,B) # line 11

-- 
Sent from my phone. Please excuse my brevity.


From mzp3769 @ending from gm@il@com  Wed Dec 26 00:41:20 2018
From: mzp3769 @ending from gm@il@com (M P)
Date: Tue, 25 Dec 2018 16:41:20 -0700
Subject: [R] fields package question
In-Reply-To: <CAGgJW75kArnOq7O1k1R8nJEu5H-A-rG1yp0D-+SfZRi_9ACBJw@mail.gmail.com>
References: <CAARDZMXZ0Lcdatg6xY0qqOyyqZw0HgW3CcPVTOMYyX7xkZrcag@mail.gmail.com>
 <CAGgJW75kArnOq7O1k1R8nJEu5H-A-rG1yp0D-+SfZRi_9ACBJw@mail.gmail.com>
Message-ID: <CAARDZMVUrxFE4i_e6uCwSNhXr38WkzJG7G7+jOka+Qvw8wykGw@mail.gmail.com>

Thanks, Eric, for looking into that.
The values are below and since I subset the new abcissa  is smaller range
grid_new.l <- list(abcissa=c(-15.0,-14.),ordinate=y)
I am emailing form gmail - don't know why is using html to format when all
is in ascii

x
 [1] -15.20180 -15.01948 -14.86533 -14.73180 -14.61402 -14.50866 -14.41335
 [8] -14.32634 -14.24629 -14.17219
y
 [1] 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45
z
[5,] 0.6467642 0.6467642 0.6467642 0.6467642 0.6467642 0.6467642 0.6467642
 [6,] 0.5597143 0.5597143 0.5597143 0.5597143 0.5597143 0.5597143 0.5597143
 [7,] 0.4854133 0.4854133 0.4854133 0.4854133 0.4854133 0.4854133 0.4854133
 [8,] 0.4278326 0.4278326 0.4278326 0.4278326 0.4278326 0.4278326 0.4278326
 [9,] 0.3834149 0.3834149 0.3834149 0.3834149 0.3834149 0.3834149 0.3834149
[10,] 0.3433031 0.3433031 0.3433031 0.3433031 0.3433031 0.3433031 0.3433031
           [,8]      [,9]     [,10]
 [1,] 1.1900951 1.1900951 1.1900951
 [2,] 1.0636935 1.0636935 1.0636935
 [3,] 0.8927228 0.8927228 0.8927228
 [4,] 0.7554456 0.7554456 0.7554456
 [5,] 0.6467642 0.6467642 0.6467642
 [6,] 0.5597143 0.5597143 0.5597143
 [7,] 0.4854133 0.4854133 0.4854133
 [8,] 0.4278326 0.4278326 0.4278326
 [9,] 0.3834149 0.3834149 0.3834149
[10,] 0.3433031 0.3433031 0.3433031



On Tue, Dec 25, 2018 at 12:45 AM Eric Berger <ericjberger at gmail.com> wrote:

> Since you don't provide lambda, rh or qext it is impossible to reproduce
> what you are seeing.
> Also note that in this mailing list HTML formatted emails are not passed
> along.
>
>
>
> On Tue, Dec 25, 2018 at 4:13 AM M P <mzp3769 at gmail.com> wrote:
>
>> Hello,
>> I used commands below to obtain a surface, can plot it and all looks as
>> expected.
>> How do I evaluate values at new point. I tried as below but that produces
>> errors.
>> Thanks for suggestions/help.
>>
>> x <- log(lambda)
>> y <- rh
>> z <- qext[,,2]
>>
>> grid.l <- list(abcissa=x,ordinate=y)
>> xg <- make.surface.grid(grid.l)
>> out.p <- as.surface(xg,z)
>> plot.surface(out.p,type="p")
>>
>> tried:
>> grid_new.l <- list(abcissa=c(-15.0,-10.),ordinate=y)
>> xg_new <- make.surface.grid(grid_new.l)
>>
>> out_new.p <- predict.surface(out.p,xg_new)
>>
>> results in this prompt:
>> predict.surface is now the function predictSurface>
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From mzp3769 @ending from gm@il@com  Wed Dec 26 00:50:36 2018
From: mzp3769 @ending from gm@il@com (M P)
Date: Tue, 25 Dec 2018 16:50:36 -0700
Subject: [R] fields package question
In-Reply-To: <CAARDZMVUrxFE4i_e6uCwSNhXr38WkzJG7G7+jOka+Qvw8wykGw@mail.gmail.com>
References: <CAARDZMXZ0Lcdatg6xY0qqOyyqZw0HgW3CcPVTOMYyX7xkZrcag@mail.gmail.com>
 <CAGgJW75kArnOq7O1k1R8nJEu5H-A-rG1yp0D-+SfZRi_9ACBJw@mail.gmail.com>
 <CAARDZMVUrxFE4i_e6uCwSNhXr38WkzJG7G7+jOka+Qvw8wykGw@mail.gmail.com>
Message-ID: <CAARDZMVT4_DGHHdEt8nSzeqAjD31s0SSJHW45ukr6V2nX5mNoA@mail.gmail.com>

Actually, let's set it
grid_new.l <- list(abcissa=c(-15.0,-14.5),ordinate=y)
to avoid out of bounds

On Tue, Dec 25, 2018 at 4:41 PM M P <mzp3769 at gmail.com> wrote:

> Thanks, Eric, for looking into that.
> The values are below and since I subset the new abcissa  is smaller range
> grid_new.l <- list(abcissa=c(-15.0,-14.),ordinate=y)
> I am emailing form gmail - don't know why is using html to format when all
> is in ascii
>
> x
>  [1] -15.20180 -15.01948 -14.86533 -14.73180 -14.61402 -14.50866 -14.41335
>  [8] -14.32634 -14.24629 -14.17219
> y
>  [1] 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45
> z
> [5,] 0.6467642 0.6467642 0.6467642 0.6467642 0.6467642 0.6467642 0.6467642
>  [6,] 0.5597143 0.5597143 0.5597143 0.5597143 0.5597143 0.5597143 0.5597143
>  [7,] 0.4854133 0.4854133 0.4854133 0.4854133 0.4854133 0.4854133 0.4854133
>  [8,] 0.4278326 0.4278326 0.4278326 0.4278326 0.4278326 0.4278326 0.4278326
>  [9,] 0.3834149 0.3834149 0.3834149 0.3834149 0.3834149 0.3834149 0.3834149
> [10,] 0.3433031 0.3433031 0.3433031 0.3433031 0.3433031 0.3433031 0.3433031
>            [,8]      [,9]     [,10]
>  [1,] 1.1900951 1.1900951 1.1900951
>  [2,] 1.0636935 1.0636935 1.0636935
>  [3,] 0.8927228 0.8927228 0.8927228
>  [4,] 0.7554456 0.7554456 0.7554456
>  [5,] 0.6467642 0.6467642 0.6467642
>  [6,] 0.5597143 0.5597143 0.5597143
>  [7,] 0.4854133 0.4854133 0.4854133
>  [8,] 0.4278326 0.4278326 0.4278326
>  [9,] 0.3834149 0.3834149 0.3834149
> [10,] 0.3433031 0.3433031 0.3433031
>
>
>
> On Tue, Dec 25, 2018 at 12:45 AM Eric Berger <ericjberger at gmail.com>
> wrote:
>
>> Since you don't provide lambda, rh or qext it is impossible to reproduce
>> what you are seeing.
>> Also note that in this mailing list HTML formatted emails are not passed
>> along.
>>
>>
>>
>> On Tue, Dec 25, 2018 at 4:13 AM M P <mzp3769 at gmail.com> wrote:
>>
>>> Hello,
>>> I used commands below to obtain a surface, can plot it and all looks as
>>> expected.
>>> How do I evaluate values at new point. I tried as below but that produces
>>> errors.
>>> Thanks for suggestions/help.
>>>
>>> x <- log(lambda)
>>> y <- rh
>>> z <- qext[,,2]
>>>
>>> grid.l <- list(abcissa=x,ordinate=y)
>>> xg <- make.surface.grid(grid.l)
>>> out.p <- as.surface(xg,z)
>>> plot.surface(out.p,type="p")
>>>
>>> tried:
>>> grid_new.l <- list(abcissa=c(-15.0,-10.),ordinate=y)
>>> xg_new <- make.surface.grid(grid_new.l)
>>>
>>> out_new.p <- predict.surface(out.p,xg_new)
>>>
>>> results in this prompt:
>>> predict.surface is now the function predictSurface>
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>

	[[alternative HTML version deleted]]


From @tyen @ending from ntu@edu@tw  Wed Dec 26 02:32:29 2018
From: @tyen @ending from ntu@edu@tw (Steven Yen)
Date: Wed, 26 Dec 2018 09:32:29 +0800
Subject: [R] Retrievable results in a procedure
In-Reply-To: <CAM_vju=0cNP2SE1ZdZszS+yJ-8D1GdM6gGC0Fc1BLEnFY0CuYw@mail.gmail.com>
References: <40ac1e02-8a7a-6ef6-09e3-edf6b396e0fa@ntu.edu.tw>
 <CAM_vju=0cNP2SE1ZdZszS+yJ-8D1GdM6gGC0Fc1BLEnFY0CuYw@mail.gmail.com>
Message-ID: <ee38462f-e227-2c66-5940-ecf7d422208a@ntu.edu.tw>

Thanks Sarah. Below, replacing "structure" with "invisible" does 
wonders--that serves my need. What I want is quite simple - I call a 
procedure and it does two things: (1) display results for all; (2) save 
retrievable results for use in further analysis, e.g., in knitr. 
Earlier, with "structure" (or with results<-list(...)) it spits out the 
main results, with components repeated (printed) in a painfully long 
list. Yet, as I said, calling with foo<-try(...) prints the main results 
with the list suppressed. I am just looking for option to NOT have to 
call with foo<- always. There must be more ways to do this, but I am 
happy with invisible. Thanks again.


On 12/25/2018 11:10 PM, Sarah Goslee wrote:
> I'm a bit confused about what you actually want, but I think 
> invisible() might be the answer.
>
> Note that there's already a base function try() so that's not a great 
> name for test functions.
>
> Sarah
>
> On Tue, Dec 25, 2018 at 8:47 AM Steven Yen <styen at ntu.edu.tw 
> <mailto:styen at ntu.edu.tw>> wrote:
>
>     I would like to suppressed printing of retrievable results in a
>     procedure and to print only when retrieved.
>
>     In line 10 below I call procedure "try" and get matrices A,B,C all
>     printed upon a call to the procedure. I get around this unwanted
>     printing by calling with v<-try(A,B) as in line 11.
>
>     Any way to suppress printing of the retrievable results listed in the
>     structure command? Thank you, and Merry Christmas to all.
>
>
>     A<-matrix(rpois(16,lambda=5),nrow=4,byrow=T)
>     B<-diag(4)
>
>     try<-function(A,B){
>       C<-A+B
>       cat("\nC:\n"); print(C)
>     structure(list(A=A,B=B,C=C))
>     }
>
>     try(A,B)    # line 10
>     v<-try(A,B) # line 11
>
>     -- 
>     styen at ntu.edu.tw <mailto:styen at ntu.edu.tw> (S.T. Yen)
>
>
>             [[alternative HTML version deleted]]
>
>     ______________________________________________
>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>     To UNSUBSCRIBE and more, see
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     and provide commented, minimal, self-contained, reproducible code.
>
> -- 
> Sarah Goslee (she/her)
> http://www.sarahgoslee.com

-- 
styen at ntu.edu.tw (S.T. Yen)


	[[alternative HTML version deleted]]


From m@@ternh@ttt @ending from gm@il@com  Wed Dec 26 07:22:24 2018
From: m@@ternh@ttt @ending from gm@il@com (Thanh Tran)
Date: Wed, 26 Dec 2018 15:22:24 +0900
Subject: [R] Determination of variables for optimizing one response using
 the desirability function
Message-ID: <CAHjanSDvrauHA1WQjKiVCa4SOD5WNyhgzF=UWNvBXME=KS+96g@mail.gmail.com>

Dear all,



I'm trying to use response surface methodology (rsm package) to . In my
data, the response is KIC, and 4 factors are AC, AV, T, and Temp. A typical
second-degree response modeling is as follows:



> data<-read.csv("2.csv", header =T)

> library(rsm)

> f.quad <- rsm(KIC~SO(AC, AV, T, Temp), data = data)

> summary(f.quad)



I summary the results as follows:



KIC = 4.85 ? 2.9AC +0.151 AV + 0.1094T

          + 0.0091Temp + 0.324 AC^2-0.0156AV^2

          - 10.00106T^2 - 0.0009Temp^2 + 0.0071AC?AV

          - 0.00087AC?T -0.00083AC?Temp ? 0.0018AV?T

         +0.0015AV?Temp ? 0.000374 AV ? T



Stationary point of response surface:

       AC        AV          T                Temp

 4.502353|  2.753002 | 48.278146 | -4.246307



Eigenanalysis:

eigen() decomposition

$`values`

[1]  0.324323665 -0.000736292 -0.001210406 -0.015776132



Based on the above response modeling and ranges of the factors (4<AC<5;
4<AV<7; 30<T<50; 5<Temp<25), I want to determine levels of the AC, AV, T,
and Temp to have the Maximum value of KIC. Because the stationary point is
an outside region of experiments, I try the desirability packages in R, but
it does not work because these packages almost focus on different responses
rather than variables.



I believe that there is a package which can solve this problem because,
with Minitab, the result using the desirability function can be shown in
Figure 1.



If anyone has any experience practicing the desirability function or
suggests any potential package, I appreciate your support and help.



Best regards,

Nhat Tran



Ps: I also added a CSV file for practicing R.

From ericjberger @ending from gm@il@com  Wed Dec 26 09:23:44 2018
From: ericjberger @ending from gm@il@com (Eric Berger)
Date: Wed, 26 Dec 2018 10:23:44 +0200
Subject: [R] fields package question
In-Reply-To: <CAARDZMVT4_DGHHdEt8nSzeqAjD31s0SSJHW45ukr6V2nX5mNoA@mail.gmail.com>
References: <CAARDZMXZ0Lcdatg6xY0qqOyyqZw0HgW3CcPVTOMYyX7xkZrcag@mail.gmail.com>
 <CAGgJW75kArnOq7O1k1R8nJEu5H-A-rG1yp0D-+SfZRi_9ACBJw@mail.gmail.com>
 <CAARDZMVUrxFE4i_e6uCwSNhXr38WkzJG7G7+jOka+Qvw8wykGw@mail.gmail.com>
 <CAARDZMVT4_DGHHdEt8nSzeqAjD31s0SSJHW45ukr6V2nX5mNoA@mail.gmail.com>
Message-ID: <CAGgJW74iXwBHH+KPYP4shuByaGJMNMUu1TcBvDEgB1tUyd5w2w@mail.gmail.com>

In your printing out of z your copy-paste effort seems to have missed
columns 1-7 of rows 1-4.
It would be better if you would provide the data by using the dput()
function, as in:
dput(y)
dput(z)

then copy-paste the output from that.


On Wed, Dec 26, 2018 at 1:50 AM M P <mzp3769 at gmail.com> wrote:

> Actually, let's set it
> grid_new.l <- list(abcissa=c(-15.0,-14.5),ordinate=y)
> to avoid out of bounds
>
> On Tue, Dec 25, 2018 at 4:41 PM M P <mzp3769 at gmail.com> wrote:
>
>> Thanks, Eric, for looking into that.
>> The values are below and since I subset the new abcissa  is smaller range
>> grid_new.l <- list(abcissa=c(-15.0,-14.),ordinate=y)
>> I am emailing form gmail - don't know why is using html to format when
>> all is in ascii
>>
>> x
>>  [1] -15.20180 -15.01948 -14.86533 -14.73180 -14.61402 -14.50866 -14.41335
>>  [8] -14.32634 -14.24629 -14.17219
>> y
>>  [1] 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45
>> z
>> [5,] 0.6467642 0.6467642 0.6467642 0.6467642 0.6467642 0.6467642 0.6467642
>>  [6,] 0.5597143 0.5597143 0.5597143 0.5597143 0.5597143 0.5597143
>> 0.5597143
>>  [7,] 0.4854133 0.4854133 0.4854133 0.4854133 0.4854133 0.4854133
>> 0.4854133
>>  [8,] 0.4278326 0.4278326 0.4278326 0.4278326 0.4278326 0.4278326
>> 0.4278326
>>  [9,] 0.3834149 0.3834149 0.3834149 0.3834149 0.3834149 0.3834149
>> 0.3834149
>> [10,] 0.3433031 0.3433031 0.3433031 0.3433031 0.3433031 0.3433031
>> 0.3433031
>>            [,8]      [,9]     [,10]
>>  [1,] 1.1900951 1.1900951 1.1900951
>>  [2,] 1.0636935 1.0636935 1.0636935
>>  [3,] 0.8927228 0.8927228 0.8927228
>>  [4,] 0.7554456 0.7554456 0.7554456
>>  [5,] 0.6467642 0.6467642 0.6467642
>>  [6,] 0.5597143 0.5597143 0.5597143
>>  [7,] 0.4854133 0.4854133 0.4854133
>>  [8,] 0.4278326 0.4278326 0.4278326
>>  [9,] 0.3834149 0.3834149 0.3834149
>> [10,] 0.3433031 0.3433031 0.3433031
>>
>>
>>
>> On Tue, Dec 25, 2018 at 12:45 AM Eric Berger <ericjberger at gmail.com>
>> wrote:
>>
>>> Since you don't provide lambda, rh or qext it is impossible to reproduce
>>> what you are seeing.
>>> Also note that in this mailing list HTML formatted emails are not passed
>>> along.
>>>
>>>
>>>
>>> On Tue, Dec 25, 2018 at 4:13 AM M P <mzp3769 at gmail.com> wrote:
>>>
>>>> Hello,
>>>> I used commands below to obtain a surface, can plot it and all looks as
>>>> expected.
>>>> How do I evaluate values at new point. I tried as below but that
>>>> produces
>>>> errors.
>>>> Thanks for suggestions/help.
>>>>
>>>> x <- log(lambda)
>>>> y <- rh
>>>> z <- qext[,,2]
>>>>
>>>> grid.l <- list(abcissa=x,ordinate=y)
>>>> xg <- make.surface.grid(grid.l)
>>>> out.p <- as.surface(xg,z)
>>>> plot.surface(out.p,type="p")
>>>>
>>>> tried:
>>>> grid_new.l <- list(abcissa=c(-15.0,-10.),ordinate=y)
>>>> xg_new <- make.surface.grid(grid_new.l)
>>>>
>>>> out_new.p <- predict.surface(out.p,xg_new)
>>>>
>>>> results in this prompt:
>>>> predict.surface is now the function predictSurface>
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>

	[[alternative HTML version deleted]]


From nd@ly @ending from hotm@il@com  Wed Dec 26 06:29:25 2018
From: nd@ly @ending from hotm@il@com (nigel daly)
Date: Wed, 26 Dec 2018 05:29:25 +0000
Subject: [R] Data formatting issue for using sirt package for a Q3 analysis
 of testlets...?
Message-ID: <DM5PR22MB09063E034DB5ECF0BE0C1C00B1B50@DM5PR22MB0906.namprd22.prod.outlook.com>

I am having some trouble with setting up a Q3 testlet analysis in R and I was hoping someone may have some advice.


At this link (https://rdrr.io/cran/sirt/man/Q3.testlet.html) there is code for a testlet analysis of 12 items in 4 testlets (although the output has only the 3 categories of A,B, and C....?).


##############

#############################################################################
# EXAMPLE 1: data.read. The 12 items are arranged in 4 testlets
#############################################################################
data<https://rdrr.io/r/utils/data.html>(data.read<https://rdrr.io/cran/sirt/man/data.read.html>)

# Yen's Q3 statistic with testlets
items <- colnames<https://rdrr.io/r/base/colnames.html>(data.read<https://rdrr.io/cran/sirt/man/data.read.html>)
testlet.matrix <- cbind<https://rdrr.io/r/base/cbind.html>( substring<https://rdrr.io/r/base/substr.html>(  items,1,1), items )
mod.testletq3 <- sirt<https://rdrr.io/cran/sirt/man/sirt-package.html>::Q3.testlet<https://rdrr.io/cran/sirt/man/Q3.testlet.html>( q3.res=mod.q3,testlet.matrix=testlet.matrix)
mod.testletq3

##############


My data set consists of 90 items comprising 30 testlets of 3 items per testlet; the are 3 subtests (or levels of difficulty) that contain 10 testlets each. Instructions for the Q3 testlet analysis suggest 2 columns - one with testlet code and the other for item number. So, I formatted the data like so:


[https://lh3.googleusercontent.com/-S5z5wBwaYN52WFl7mES80FI5-a8BQE9Nw5S6aVGUq4F6GUDWhgsRPLQjby4mRgAbJA9xM2YzsQFjcUlrVNlKi2nYJa-drypfZeU4FA0zdmlo_A-GFOOIFNqHQ1P_hsb35jSWcmM]


But after running the code from the above link, the output was still in an A,B,C matrix.


Any advice would be much appreciated.


[[elided Hotmail spam]]


Nigel



	[[alternative HTML version deleted]]


From ericjberger @ending from gm@il@com  Wed Dec 26 17:52:46 2018
From: ericjberger @ending from gm@il@com (Eric Berger)
Date: Wed, 26 Dec 2018 18:52:46 +0200
Subject: [R] Fwd:  fields package question
In-Reply-To: <CAARDZMUyX0mq-Jd6X3914mTy+M9BEr5Y96x0W3nSNBW=FAsgLQ@mail.gmail.com>
References: <CAARDZMXZ0Lcdatg6xY0qqOyyqZw0HgW3CcPVTOMYyX7xkZrcag@mail.gmail.com>
 <CAGgJW75kArnOq7O1k1R8nJEu5H-A-rG1yp0D-+SfZRi_9ACBJw@mail.gmail.com>
 <CAARDZMVUrxFE4i_e6uCwSNhXr38WkzJG7G7+jOka+Qvw8wykGw@mail.gmail.com>
 <CAARDZMVT4_DGHHdEt8nSzeqAjD31s0SSJHW45ukr6V2nX5mNoA@mail.gmail.com>
 <CAGgJW74iXwBHH+KPYP4shuByaGJMNMUu1TcBvDEgB1tUyd5w2w@mail.gmail.com>
 <CAARDZMUyX0mq-Jd6X3914mTy+M9BEr5Y96x0W3nSNBW=FAsgLQ@mail.gmail.com>
Message-ID: <CAGgJW75MjjZoAhDGuCJv7ZYMFhWe3fS=rzJDYx62TC2b-1+-Fg@mail.gmail.com>

Forwarding to the full R-help list.
The protocol is for everyone to see the full set of questions and answers.
This allows for:
    1. more people to contribute, which generally improves the quality of
the answer
    2. people with similar questions can find this thread via search and
see the full discussion

---------- Forwarded message ---------
From: M P <mzp3769 at gmail.com>
Date: Wed, Dec 26, 2018 at 6:32 PM
Subject: Re: [R] fields package question
To: Eric Berger <ericjberger at gmail.com>


Thanks, dput outputs below. I'd like to evaluate surface e.g. at (-10.5,
0.935)
> dput(x)
structure(c(-10.9251387646973, -10.8977397823155, -10.8197783096724,
-10.7568035566262, -10.701995233594, -10.5966347583582, -10.4868838826637,
-10.4143132015642, -10.2601624697659, -10.1266311291125), .Dim = 10L)
> dput(y)
structure(c(0.899999976158142, 0.910000026226044, 0.920000016689301,
0.930000007152557, 0.939999997615814, 0.949999988079071, 0.959999978542328,
0.970000028610229, 0.980000019073486, 0.990000009536743), .Dim = 10L)
> dput(z)
structure(c(0.0170888844877481, 0.0163554549217224, 0.0143800554797053,
0.0132564017549157, 0.0123229259625077, 0.0108034228906035,
0.00943510234355927,
0.00868543982505798, 0.00763010373339057, 0.00705513963475823,
0.0173761900514364, 0.0166240241378546, 0.0145960496738553,
0.0134453792124987,
0.0124927284196019, 0.0109430542215705, 0.00954837258905172,
0.00878583826124668, 0.00772488862276077, 0.00716213695704937,
0.0178007110953331, 0.0170208644121885, 0.0149147948250175,
0.0137240244075656,
0.0127429272979498, 0.0111485198140144, 0.0097147086635232,
0.00893292296677828,
0.00786342192441225, 0.00731897540390491, 0.0180797930806875,
0.0172817632555962, 0.0151241403073072, 0.0139069128781557,
0.012907050549984,
0.0112831527367234, 0.0098235122859478, 0.00902892742305994,
0.00795362330973148, 0.00742132356390357, 0.0186295621097088,
0.0177957788109779, 0.015536243095994, 0.014266736805439,
0.0132297882810235,
0.0115476427599788, 0.0100369155406952, 0.00921681523323059,
0.00812961626797915, 0.00762135302647948, 0.0190353523939848,
0.0181752592325211, 0.0158403068780899, 0.0145321246236563,
0.0134677225723863,
0.0117424847558141, 0.0101939048618078, 0.00935473665595055,
0.00825834088027477, 0.00776780024170876, 0.0190353523939848,
0.0181752592325211, 0.0158403068780899, 0.0145321246236563,
0.0134677225723863,
0.0117424847558141, 0.0101939048618078, 0.00935473665595055,
0.00825834088027477, 0.00776780024170876, 0.0190353523939848,
0.0181752592325211, 0.0158403068780899, 0.0145321246236563,
0.0134677225723863,
0.0117424847558141, 0.0101939048618078, 0.00935473665595055,
0.00825834088027477, 0.00776780024170876, 0.0190353523939848,
0.0181752592325211, 0.0158403068780899, 0.0145321246236563,
0.0134677225723863,
0.0117424847558141, 0.0101939048618078, 0.00935473665595055,
0.00825834088027477, 0.00776780024170876, 0.0190353523939848,
0.0181752592325211, 0.0158403068780899, 0.0145321246236563,
0.0134677225723863,
0.0117424847558141, 0.0101939048618078, 0.00935473665595055,
0.00825834088027477, 0.00776780024170876), .Dim = c(10L, 10L))

On Wed, Dec 26, 2018 at 1:24 AM Eric Berger <ericjberger at gmail.com> wrote:
>
> In your printing out of z your copy-paste effort seems to have missed
columns 1-7 of rows 1-4.
> It would be better if you would provide the data by using the dput()
function, as in:
> dput(y)
> dput(z)
>
> then copy-paste the output from that.
>
>
> On Wed, Dec 26, 2018 at 1:50 AM M P <mzp3769 at gmail.com> wrote:
>>
>> Actually, let's set it
>> grid_new.l <- list(abcissa=c(-15.0,-14.5),ordinate=y)
>> to avoid out of bounds
>>
>> On Tue, Dec 25, 2018 at 4:41 PM M P <mzp3769 at gmail.com> wrote:
>>>
>>> Thanks, Eric, for looking into that.
>>> The values are below and since I subset the new abcissa  is smaller
range
>>> grid_new.l <- list(abcissa=c(-15.0,-14.),ordinate=y)
>>> I am emailing form gmail - don't know why is using html to format when
all is in ascii
>>>
>>> x
>>>  [1] -15.20180 -15.01948 -14.86533 -14.73180 -14.61402 -14.50866
-14.41335
>>>  [8] -14.32634 -14.24629 -14.17219
>>> y
>>>  [1] 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45
>>> z
>>> [5,] 0.6467642 0.6467642 0.6467642 0.6467642 0.6467642 0.6467642
0.6467642
>>>  [6,] 0.5597143 0.5597143 0.5597143 0.5597143 0.5597143 0.5597143
0.5597143
>>>  [7,] 0.4854133 0.4854133 0.4854133 0.4854133 0.4854133 0.4854133
0.4854133
>>>  [8,] 0.4278326 0.4278326 0.4278326 0.4278326 0.4278326 0.4278326
0.4278326
>>>  [9,] 0.3834149 0.3834149 0.3834149 0.3834149 0.3834149 0.3834149
0.3834149
>>> [10,] 0.3433031 0.3433031 0.3433031 0.3433031 0.3433031 0.3433031
0.3433031
>>>            [,8]      [,9]     [,10]
>>>  [1,] 1.1900951 1.1900951 1.1900951
>>>  [2,] 1.0636935 1.0636935 1.0636935
>>>  [3,] 0.8927228 0.8927228 0.8927228
>>>  [4,] 0.7554456 0.7554456 0.7554456
>>>  [5,] 0.6467642 0.6467642 0.6467642
>>>  [6,] 0.5597143 0.5597143 0.5597143
>>>  [7,] 0.4854133 0.4854133 0.4854133
>>>  [8,] 0.4278326 0.4278326 0.4278326
>>>  [9,] 0.3834149 0.3834149 0.3834149
>>> [10,] 0.3433031 0.3433031 0.3433031
>>>
>>>
>>>
>>> On Tue, Dec 25, 2018 at 12:45 AM Eric Berger <ericjberger at gmail.com>
wrote:
>>>>
>>>> Since you don't provide lambda, rh or qext it is impossible to
reproduce what you are seeing.
>>>> Also note that in this mailing list HTML formatted emails are not
passed along.
>>>>
>>>>
>>>>
>>>> On Tue, Dec 25, 2018 at 4:13 AM M P <mzp3769 at gmail.com> wrote:
>>>>>
>>>>> Hello,
>>>>> I used commands below to obtain a surface, can plot it and all looks
as
>>>>> expected.
>>>>> How do I evaluate values at new point. I tried as below but that
produces
>>>>> errors.
>>>>> Thanks for suggestions/help.
>>>>>
>>>>> x <- log(lambda)
>>>>> y <- rh
>>>>> z <- qext[,,2]
>>>>>
>>>>> grid.l <- list(abcissa=x,ordinate=y)
>>>>> xg <- make.surface.grid(grid.l)
>>>>> out.p <- as.surface(xg,z)
>>>>> plot.surface(out.p,type="p")
>>>>>
>>>>> tried:
>>>>> grid_new.l <- list(abcissa=c(-15.0,-10.),ordinate=y)
>>>>> xg_new <- make.surface.grid(grid_new.l)
>>>>>
>>>>> out_new.p <- predict.surface(out.p,xg_new)
>>>>>
>>>>> results in this prompt:
>>>>> predict.surface is now the function predictSurface>
>>>>>
>>>>>         [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From @pbr@ckett20 @ending from @@intjo@ephh@@com  Thu Dec 27 00:03:51 2018
From: @pbr@ckett20 @ending from @@intjo@ephh@@com (Spencer Brackett)
Date: Wed, 26 Dec 2018 18:03:51 -0500
Subject: [R] Help converting .txt to .csv file
Message-ID: <CAPQaxLN2t=y8t_NgWpSUU_erg=44GC=8+PedewUq7OYabx7jjg@mail.gmail.com>

Good evening,

I am attempting to anaylze the protein expression data contained within
these two ICGC, TCGA datasets (one for GBM and the other for LGG)

*File for GBM  protein expression*:
https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22GBM-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D

*File for LGG protein expression:*


*https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
<https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D>*

  When I tried to transfer the files from .txt (via Notepad) to .csv (via
Excel), the data appeared in the columns as unorganized and random
script... not like how a typical csv should be arranged at all. I need the
dataset to be converted into .csv in order to analyze it in R, which is why
I am hoping someone here might help me in doing that. If not, is there
perhaps some other way that I could analyze the datatsets on R, which again
is downloaded from the dataportal ICGC?

Best,

Spencer Brackett

	[[alternative HTML version deleted]]


From bgunter@4567 @ending from gm@il@com  Thu Dec 27 00:26:43 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Wed, 26 Dec 2018 15:26:43 -0800
Subject: [R] Help converting .txt to .csv file
In-Reply-To: <CAPQaxLN2t=y8t_NgWpSUU_erg=44GC=8+PedewUq7OYabx7jjg@mail.gmail.com>
References: <CAPQaxLN2t=y8t_NgWpSUU_erg=44GC=8+PedewUq7OYabx7jjg@mail.gmail.com>
Message-ID: <CAGxFJbTSEq3CLxz4YwC61AnE7cMC4AvBSy30F1e4A7Y44m4gLA@mail.gmail.com>

Inline.

-- Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Dec 26, 2018 at 3:04 PM Spencer Brackett <
spbrackett20 at saintjosephhs.com> wrote:

> Good evening,
>
> I am attempting to anaylze the protein expression data contained within
> these two ICGC, TCGA datasets (one for GBM and the other for LGG)
>
> ...
>   When I tried to transfer the files from .txt (via Notepad) to .csv (via
> Excel), the data appeared in the columns as unorganized and random
> script... not like how a typical csv should be arranged at all. I need the
> dataset to be converted into .csv in order to analyze it in R,


Huh?? Why do you think this? A csv is just a comma delimited text file.

R can input pretty much any kind of file, ONCE YOU KNOW THE FORMAT OF WHAT
YOU ARE INPUTTING. This should be provided by the links that you gave. Then
see ?read.table or, more generally, ?scan for how to read the (text) file
into R into whatever data structure you need. See also the R data
import/export manual. Or possibly post to the Bioconductor list where they
specialize in this sort of thing and may already have packages that can
access the repositories and bring in the data in the form you need them.
They also have lots of software there for analysis, too.

Cheers,
Bert






> which is why
> I am hoping someone here might help me in doing that. If not, is there
> perhaps some other way that I could analyze the datatsets on R, which again
> is downloaded from the dataportal ICGC?
>
> Best,
>
> Spencer Brackett
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From rmh @ending from temple@edu  Thu Dec 27 00:34:19 2018
From: rmh @ending from temple@edu (Richard M. Heiberger)
Date: Wed, 26 Dec 2018 18:34:19 -0500
Subject: [R] Help converting .txt to .csv file
In-Reply-To: <CAPQaxLN2t=y8t_NgWpSUU_erg=44GC=8+PedewUq7OYabx7jjg@mail.gmail.com>
References: <CAPQaxLN2t=y8t_NgWpSUU_erg=44GC=8+PedewUq7OYabx7jjg@mail.gmail.com>
Message-ID: <CAGx1TMAnF7V7jEig0mGUgciYY70FfufCuV8GzU0TnPOLOYhEfw@mail.gmail.com>

I looked at the first file.  It gives an option to download as TSV
(tab separated values).
That is the same as CSV except with tabs instead of commas.
You do not need any external software to read it.  Read the downloaded
file directly into R.

read.delim looks as if it would work directly on the downloaded file.
?read.delim
The notation "\t" means the tab character.

As an aside, stay away from notepad. it is too naive for almost
anything interesting.
The specific case I often see is people reading linux-style text files
with notepad, which doesn't
understand NL terminated lines.  nicely formatted text files become illegible.

On Wed, Dec 26, 2018 at 6:04 PM Spencer Brackett
<spbrackett20 at saintjosephhs.com> wrote:
>
> Good evening,
>
> I am attempting to anaylze the protein expression data contained within
> these two ICGC, TCGA datasets (one for GBM and the other for LGG)
>
> *File for GBM  protein expression*:
> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22GBM-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>
> *File for LGG protein expression:*
>
>
> *https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
> <https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D>*
>
>   When I tried to transfer the files from .txt (via Notepad) to .csv (via
> Excel), the data appeared in the columns as unorganized and random
> script... not like how a typical csv should be arranged at all. I need the
> dataset to be converted into .csv in order to analyze it in R, which is why
> I am hoping someone here might help me in doing that. If not, is there
> perhaps some other way that I could analyze the datatsets on R, which again
> is downloaded from the dataportal ICGC?
>
> Best,
>
> Spencer Brackett
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @pbr@ckett20 @ending from @@intjo@ephh@@com  Thu Dec 27 00:42:22 2018
From: @pbr@ckett20 @ending from @@intjo@ephh@@com (Spencer Brackett)
Date: Wed, 26 Dec 2018 18:42:22 -0500
Subject: [R] Help converting .txt to .csv file
In-Reply-To: <CAGx1TMAnF7V7jEig0mGUgciYY70FfufCuV8GzU0TnPOLOYhEfw@mail.gmail.com>
References: <CAPQaxLN2t=y8t_NgWpSUU_erg=44GC=8+PedewUq7OYabx7jjg@mail.gmail.com>
 <CAGx1TMAnF7V7jEig0mGUgciYY70FfufCuV8GzU0TnPOLOYhEfw@mail.gmail.com>
Message-ID: <CAPQaxLPbFTFZyTGahAH0_j=06mfovgN4mAcMEQJo87BoGRmRjQ@mail.gmail.com>

Mr. Heiberger,

 Thank you for the insight! I will try out suggestion.

Best,

Spencer Brackett

On Wed, Dec 26, 2018 at 6:34 PM Richard M. Heiberger <rmh at temple.edu> wrote:

> I looked at the first file.  It gives an option to download as TSV
> (tab separated values).
> That is the same as CSV except with tabs instead of commas.
> You do not need any external software to read it.  Read the downloaded
> file directly into R.
>
> read.delim looks as if it would work directly on the downloaded file.
> ?read.delim
> The notation "\t" means the tab character.
>
> As an aside, stay away from notepad. it is too naive for almost
> anything interesting.
> The specific case I often see is people reading linux-style text files
> with notepad, which doesn't
> understand NL terminated lines.  nicely formatted text files become
> illegible.
>
> On Wed, Dec 26, 2018 at 6:04 PM Spencer Brackett
> <spbrackett20 at saintjosephhs.com> wrote:
> >
> > Good evening,
> >
> > I am attempting to anaylze the protein expression data contained within
> > these two ICGC, TCGA datasets (one for GBM and the other for LGG)
> >
> > *File for GBM  protein expression*:
> >
> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22GBM-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
> >
> > *File for LGG protein expression:*
> >
> >
> > *
> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
> > <
> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
> >*
> >
> >   When I tried to transfer the files from .txt (via Notepad) to .csv (via
> > Excel), the data appeared in the columns as unorganized and random
> > script... not like how a typical csv should be arranged at all. I need
> the
> > dataset to be converted into .csv in order to analyze it in R, which is
> why
> > I am hoping someone here might help me in doing that. If not, is there
> > perhaps some other way that I could analyze the datatsets on R, which
> again
> > is downloaded from the dataportal ICGC?
> >
> > Best,
> >
> > Spencer Brackett
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @pbr@ckett20 @ending from @@intjo@ephh@@com  Thu Dec 27 03:57:27 2018
From: @pbr@ckett20 @ending from @@intjo@ephh@@com (Spencer Brackett)
Date: Wed, 26 Dec 2018 21:57:27 -0500
Subject: [R] Help converting .txt to .csv file
In-Reply-To: <CAPQaxLPbFTFZyTGahAH0_j=06mfovgN4mAcMEQJo87BoGRmRjQ@mail.gmail.com>
References: <CAPQaxLN2t=y8t_NgWpSUU_erg=44GC=8+PedewUq7OYabx7jjg@mail.gmail.com>
 <CAGx1TMAnF7V7jEig0mGUgciYY70FfufCuV8GzU0TnPOLOYhEfw@mail.gmail.com>
 <CAPQaxLPbFTFZyTGahAH0_j=06mfovgN4mAcMEQJo87BoGRmRjQ@mail.gmail.com>
Message-ID: <CAPQaxLPmCfXHjJgXbV98kvNfHEGPbqKB9a-LiVNTWXu0_QgwSw@mail.gmail.com>

Hello again,

I worked on directly downloading the file into R as was suggested, but have
thus far been unsuccessful. This is what  I generated on my second
attempt...

 GBM protein_expression<-(file.choose(), header=TRUE, sep="\t")
Error: unexpected symbol in "GBM protein_expression"
> GBM
protein_expression<-(file.choose(GBM_protein_expression.xlsx),header=TRUE,
sep="\t")
Error: unexpected symbol in "GBM protein_expression"
>

What part of the argument is in error?

Also I tried importing the dataset as an excel file on RStudio to see if I
could solve my problem that way. However, my imported excel file has been
stuck in the 'retrieving preview data' and no data is appearing. Is the
data file prehaps too large or in the wrong format?



On Wed, Dec 26, 2018 at 6:42 PM Spencer Brackett <
spbrackett20 at saintjosephhs.com> wrote:

> Mr. Heiberger,
>
>  Thank you for the insight! I will try out suggestion.
>
> Best,
>
> Spencer Brackett
>
> On Wed, Dec 26, 2018 at 6:34 PM Richard M. Heiberger <rmh at temple.edu>
> wrote:
>
>> I looked at the first file.  It gives an option to download as TSV
>> (tab separated values).
>> That is the same as CSV except with tabs instead of commas.
>> You do not need any external software to read it.  Read the downloaded
>> file directly into R.
>>
>> read.delim looks as if it would work directly on the downloaded file.
>> ?read.delim
>> The notation "\t" means the tab character.
>>
>> As an aside, stay away from notepad. it is too naive for almost
>> anything interesting.
>> The specific case I often see is people reading linux-style text files
>> with notepad, which doesn't
>> understand NL terminated lines.  nicely formatted text files become
>> illegible.
>>
>> On Wed, Dec 26, 2018 at 6:04 PM Spencer Brackett
>> <spbrackett20 at saintjosephhs.com> wrote:
>> >
>> > Good evening,
>> >
>> > I am attempting to anaylze the protein expression data contained within
>> > these two ICGC, TCGA datasets (one for GBM and the other for LGG)
>> >
>> > *File for GBM  protein expression*:
>> >
>> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22GBM-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>> >
>> > *File for LGG protein expression:*
>> >
>> >
>> > *
>> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>> > <
>> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>> >*
>> >
>> >   When I tried to transfer the files from .txt (via Notepad) to .csv
>> (via
>> > Excel), the data appeared in the columns as unorganized and random
>> > script... not like how a typical csv should be arranged at all. I need
>> the
>> > dataset to be converted into .csv in order to analyze it in R, which is
>> why
>> > I am hoping someone here might help me in doing that. If not, is there
>> > perhaps some other way that I could analyze the datatsets on R, which
>> again
>> > is downloaded from the dataportal ICGC?
>> >
>> > Best,
>> >
>> > Spencer Brackett
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From @rk@y7777 @ending from gm@il@com  Wed Dec 26 17:47:44 2018
From: @rk@y7777 @ending from gm@il@com (RK)
Date: Wed, 26 Dec 2018 10:47:44 -0600
Subject: [R] Long input lines in R.exe
Message-ID: <CAP3sJ3eSBarDSquzdw+pWbEHDOfub20_8bwh8r7_4b0ZEa=Hig@mail.gmail.com>

Hi,

When I run R.exe (or Rterm.exe) from cmd.exe and enter in a long input
line, e.g.

print("12345678901234567890123456789012345678901234567890123456789012345678901234567890")

the visible portion of the input is truncated to:

print("123456789012345678901234567890123456789012345678901234567890123456789$

In effect, I cannot see the full input line all at once. The visible
portion of the input, including the $ symbol is 77 characters which
must be a terminal width setting. Scrolling to the other end of the
line, allows me to see the hidden portion but this is not ideal.
Changing options(width=132) within R.exe/cmd.exe shows long output
lines, which tells me that terminal width is not a fundamental
limitation.

This behavior does not occur when using RGui.exe, RStudio, etc. where
the visible portion of the input lines is limited by screen width.

Is there any way to make R.exe show long input lines when using
cmd.exe? I prefer to use R.exe when I am working off the Windows
console.

I am using R 3.5.2 on Win10, 64-bit.

Thanks,
RK


From @pbr@ckett20 @ending from @@intjo@ephh@@com  Thu Dec 27 04:26:35 2018
From: @pbr@ckett20 @ending from @@intjo@ephh@@com (Spencer Brackett)
Date: Wed, 26 Dec 2018 22:26:35 -0500
Subject: [R] Fwd:  UPDATE
In-Reply-To: <CAPQaxLPmeKTuMjtU4ENGAc5gyfoQSEcMkCPFHK4czyFvV0N+CA@mail.gmail.com>
References: <CAPQaxLN2t=y8t_NgWpSUU_erg=44GC=8+PedewUq7OYabx7jjg@mail.gmail.com>
 <CAGx1TMAnF7V7jEig0mGUgciYY70FfufCuV8GzU0TnPOLOYhEfw@mail.gmail.com>
 <CAPQaxLPbFTFZyTGahAH0_j=06mfovgN4mAcMEQJo87BoGRmRjQ@mail.gmail.com>
 <CAPQaxLPmCfXHjJgXbV98kvNfHEGPbqKB9a-LiVNTWXu0_QgwSw@mail.gmail.com>
 <CAPQaxLNVU86dmGPdiz43g1iw3as+LR=TjzgjGQnFF_xju5_76g@mail.gmail.com>
 <CAPQaxLPmeKTuMjtU4ENGAc5gyfoQSEcMkCPFHK4czyFvV0N+CA@mail.gmail.com>
Message-ID: <CAPQaxLP+PFjbX9200g9oi8Xk2-2Rhvfqj=PGmLBMnO-LOBBXbA@mail.gmail.com>

I tried importing the file without preview and recieved the following....

library(readxl)
> GBM_protein_expression <- read_excel("C:/Users/Spencer/Desktop/GBM
protein_expression.csv")
Error: Can't establish that the input is either xls or xlsx.
> View(GBM_protein_expression)
Error in View : object 'GBM_protein_expression' not found
Error in gzfile(file, mode) : cannot open the connection
In addition: Warning message:
In gzfile(file, mode) :
  cannot open compressed file
'C:/Users/Spencer/AppData/Local/Temp/RtmpQNQrMh/input147c61fc5b52.rds',
probable reason 'No such file or directory'
> library(readxl)
> GBM_protein_expression <-
read_excel("C:/Users/Spencer/Desktop/GBM_protein_ expression.xlsx")
readxl works best with a newer version of the tibble package.
You currently have tibble v1.4.2.
Falling back to column name repair from tibble <= v1.4.2.
Message displays once per session.
> View(GBM_protein_expression)

Also, the area above my console says that no data is available in the
table. Is this perhaps the result of lack of preview or the fact that the
excel file itself contains no numerical data, but only TRUE or FALSE
entries?

On Wed, Dec 26, 2018 at 9:57 PM Spencer Brackett <
spbrackett20 at saintjosephhs.com> wrote:

> Hello again,
>
> I worked on directly downloading the file into R as was suggested, but
> have thus far been unsuccessful. This is what  I generated on my second
> attempt...
>
>  GBM protein_expression<-(file.choose(), header=TRUE, sep="\t")
> Error: unexpected symbol in "GBM protein_expression"
> > GBM
> protein_expression<-(file.choose(GBM_protein_expression.xlsx),header=TRUE,
> sep="\t")
> Error: unexpected symbol in "GBM protein_expression"
> >
>
> What part of the argument is in error?
>
> Also I tried importing the dataset as an excel file on RStudio to see if I
> could solve my problem that way. However, my imported excel file has been
> stuck in the 'retrieving preview data' and no data is appearing. Is the
> data file prehaps too large or in the wrong format?
>
>
>
> On Wed, Dec 26, 2018 at 6:42 PM Spencer Brackett <
> spbrackett20 at saintjosephhs.com> wrote:
>
>> Mr. Heiberger,
>>
>>  Thank you for the insight! I will try out suggestion.
>>
>> Best,
>>
>> Spencer Brackett
>>
>> On Wed, Dec 26, 2018 at 6:34 PM Richard M. Heiberger <rmh at temple.edu>
>> wrote:
>>
>>> I looked at the first file.  It gives an option to download as TSV
>>> (tab separated values).
>>> That is the same as CSV except with tabs instead of commas.
>>> You do not need any external software to read it.  Read the downloaded
>>> file directly into R.
>>>
>>> read.delim looks as if it would work directly on the downloaded file.
>>> ?read.delim
>>> The notation "\t" means the tab character.
>>>
>>> As an aside, stay away from notepad. it is too naive for almost
>>> anything interesting.
>>> The specific case I often see is people reading linux-style text files
>>> with notepad, which doesn't
>>> understand NL terminated lines.  nicely formatted text files become
>>> illegible.
>>>
>>> On Wed, Dec 26, 2018 at 6:04 PM Spencer Brackett
>>> <spbrackett20 at saintjosephhs.com> wrote:
>>> >
>>> > Good evening,
>>> >
>>> > I am attempting to anaylze the protein expression data contained within
>>> > these two ICGC, TCGA datasets (one for GBM and the other for LGG)
>>> >
>>> > *File for GBM  protein expression*:
>>> >
>>> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22GBM-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>>> >
>>> > *File for LGG protein expression:*
>>> >
>>> >
>>> > *
>>> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>>> > <
>>> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>>> >*
>>> >
>>> >   When I tried to transfer the files from .txt (via Notepad) to .csv
>>> (via
>>> > Excel), the data appeared in the columns as unorganized and random
>>> > script... not like how a typical csv should be arranged at all. I need
>>> the
>>> > dataset to be converted into .csv in order to analyze it in R, which
>>> is why
>>> > I am hoping someone here might help me in doing that. If not, is there
>>> > perhaps some other way that I could analyze the datatsets on R, which
>>> again
>>> > is downloaded from the dataportal ICGC?
>>> >
>>> > Best,
>>> >
>>> > Spencer Brackett
>>> >
>>> >         [[alternative HTML version deleted]]
>>> >
>>> > ______________________________________________
>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>> > PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> > and provide commented, minimal, self-contained, reproducible code.
>>>
>>

	[[alternative HTML version deleted]]


From jdnewmil @ending from dcn@d@vi@@c@@u@  Thu Dec 27 04:35:53 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Wed, 26 Dec 2018 19:35:53 -0800
Subject: [R] Fwd:  UPDATE
In-Reply-To: <CAPQaxLP+PFjbX9200g9oi8Xk2-2Rhvfqj=PGmLBMnO-LOBBXbA@mail.gmail.com>
References: <CAPQaxLN2t=y8t_NgWpSUU_erg=44GC=8+PedewUq7OYabx7jjg@mail.gmail.com>
 <CAGx1TMAnF7V7jEig0mGUgciYY70FfufCuV8GzU0TnPOLOYhEfw@mail.gmail.com>
 <CAPQaxLPbFTFZyTGahAH0_j=06mfovgN4mAcMEQJo87BoGRmRjQ@mail.gmail.com>
 <CAPQaxLPmCfXHjJgXbV98kvNfHEGPbqKB9a-LiVNTWXu0_QgwSw@mail.gmail.com>
 <CAPQaxLNVU86dmGPdiz43g1iw3as+LR=TjzgjGQnFF_xju5_76g@mail.gmail.com>
 <CAPQaxLPmeKTuMjtU4ENGAc5gyfoQSEcMkCPFHK4czyFvV0N+CA@mail.gmail.com>
 <CAPQaxLP+PFjbX9200g9oi8Xk2-2Rhvfqj=PGmLBMnO-LOBBXbA@mail.gmail.com>
Message-ID: <D3662A59-E568-4A7F-9665-D24F42865BB2@dcn.davis.ca.us>

CSV and TSV are not Excel files. Yes, I know Excel will open them, but that does not make them Excel files.

Read a TSV file with read.table or read.csv, setting the sep argument to "\t".

On December 26, 2018 7:26:35 PM PST, Spencer Brackett <spbrackett20 at saintjosephhs.com> wrote:
>I tried importing the file without preview and recieved the
>following....
>
>library(readxl)
>> GBM_protein_expression <- read_excel("C:/Users/Spencer/Desktop/GBM
>protein_expression.csv")
>Error: Can't establish that the input is either xls or xlsx.
>> View(GBM_protein_expression)
>Error in View : object 'GBM_protein_expression' not found
>Error in gzfile(file, mode) : cannot open the connection
>In addition: Warning message:
>In gzfile(file, mode) :
>  cannot open compressed file
>'C:/Users/Spencer/AppData/Local/Temp/RtmpQNQrMh/input147c61fc5b52.rds',
>probable reason 'No such file or directory'
>> library(readxl)
>> GBM_protein_expression <-
>read_excel("C:/Users/Spencer/Desktop/GBM_protein_ expression.xlsx")
>readxl works best with a newer version of the tibble package.
>You currently have tibble v1.4.2.
>Falling back to column name repair from tibble <= v1.4.2.
>Message displays once per session.
>> View(GBM_protein_expression)
>
>Also, the area above my console says that no data is available in the
>table. Is this perhaps the result of lack of preview or the fact that
>the
>excel file itself contains no numerical data, but only TRUE or FALSE
>entries?
>
>On Wed, Dec 26, 2018 at 9:57 PM Spencer Brackett <
>spbrackett20 at saintjosephhs.com> wrote:
>
>> Hello again,
>>
>> I worked on directly downloading the file into R as was suggested,
>but
>> have thus far been unsuccessful. This is what  I generated on my
>second
>> attempt...
>>
>>  GBM protein_expression<-(file.choose(), header=TRUE, sep="\t")
>> Error: unexpected symbol in "GBM protein_expression"
>> > GBM
>>
>protein_expression<-(file.choose(GBM_protein_expression.xlsx),header=TRUE,
>> sep="\t")
>> Error: unexpected symbol in "GBM protein_expression"
>> >
>>
>> What part of the argument is in error?
>>
>> Also I tried importing the dataset as an excel file on RStudio to see
>if I
>> could solve my problem that way. However, my imported excel file has
>been
>> stuck in the 'retrieving preview data' and no data is appearing. Is
>the
>> data file prehaps too large or in the wrong format?
>>
>>
>>
>> On Wed, Dec 26, 2018 at 6:42 PM Spencer Brackett <
>> spbrackett20 at saintjosephhs.com> wrote:
>>
>>> Mr. Heiberger,
>>>
>>>  Thank you for the insight! I will try out suggestion.
>>>
>>> Best,
>>>
>>> Spencer Brackett
>>>
>>> On Wed, Dec 26, 2018 at 6:34 PM Richard M. Heiberger
><rmh at temple.edu>
>>> wrote:
>>>
>>>> I looked at the first file.  It gives an option to download as TSV
>>>> (tab separated values).
>>>> That is the same as CSV except with tabs instead of commas.
>>>> You do not need any external software to read it.  Read the
>downloaded
>>>> file directly into R.
>>>>
>>>> read.delim looks as if it would work directly on the downloaded
>file.
>>>> ?read.delim
>>>> The notation "\t" means the tab character.
>>>>
>>>> As an aside, stay away from notepad. it is too naive for almost
>>>> anything interesting.
>>>> The specific case I often see is people reading linux-style text
>files
>>>> with notepad, which doesn't
>>>> understand NL terminated lines.  nicely formatted text files become
>>>> illegible.
>>>>
>>>> On Wed, Dec 26, 2018 at 6:04 PM Spencer Brackett
>>>> <spbrackett20 at saintjosephhs.com> wrote:
>>>> >
>>>> > Good evening,
>>>> >
>>>> > I am attempting to anaylze the protein expression data contained
>within
>>>> > these two ICGC, TCGA datasets (one for GBM and the other for LGG)
>>>> >
>>>> > *File for GBM  protein expression*:
>>>> >
>>>>
>https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22GBM-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>>>> >
>>>> > *File for LGG protein expression:*
>>>> >
>>>> >
>>>> > *
>>>>
>https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>>>> > <
>>>>
>https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>>>> >*
>>>> >
>>>> >   When I tried to transfer the files from .txt (via Notepad) to
>.csv
>>>> (via
>>>> > Excel), the data appeared in the columns as unorganized and
>random
>>>> > script... not like how a typical csv should be arranged at all. I
>need
>>>> the
>>>> > dataset to be converted into .csv in order to analyze it in R,
>which
>>>> is why
>>>> > I am hoping someone here might help me in doing that. If not, is
>there
>>>> > perhaps some other way that I could analyze the datatsets on R,
>which
>>>> again
>>>> > is downloaded from the dataportal ICGC?
>>>> >
>>>> > Best,
>>>> >
>>>> > Spencer Brackett
>>>> >
>>>> >         [[alternative HTML version deleted]]
>>>> >
>>>> > ______________________________________________
>>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>>> > PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> > and provide commented, minimal, self-contained, reproducible
>code.
>>>>
>>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From jdnewmil @ending from dcn@d@vi@@c@@u@  Thu Dec 27 04:48:38 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Wed, 26 Dec 2018 19:48:38 -0800
Subject: [R] Fwd: UPDATE
In-Reply-To: <CAPQaxLMG1oF-79iyReu3W-L6ZiE9H8t1dkxmsDL4N4MawphW+w@mail.gmail.com>
References: <CAPQaxLN2t=y8t_NgWpSUU_erg=44GC=8+PedewUq7OYabx7jjg@mail.gmail.com>
 <CAGx1TMAnF7V7jEig0mGUgciYY70FfufCuV8GzU0TnPOLOYhEfw@mail.gmail.com>
 <CAPQaxLPbFTFZyTGahAH0_j=06mfovgN4mAcMEQJo87BoGRmRjQ@mail.gmail.com>
 <CAPQaxLPmCfXHjJgXbV98kvNfHEGPbqKB9a-LiVNTWXu0_QgwSw@mail.gmail.com>
 <CAPQaxLNVU86dmGPdiz43g1iw3as+LR=TjzgjGQnFF_xju5_76g@mail.gmail.com>
 <CAPQaxLPmeKTuMjtU4ENGAc5gyfoQSEcMkCPFHK4czyFvV0N+CA@mail.gmail.com>
 <CAPQaxLP+PFjbX9200g9oi8Xk2-2Rhvfqj=PGmLBMnO-LOBBXbA@mail.gmail.com>
 <D3662A59-E568-4A7F-9665-D24F42865BB2@dcn.davis.ca.us>
 <CAPQaxLMG1oF-79iyReu3W-L6ZiE9H8t1dkxmsDL4N4MawphW+w@mail.gmail.com>
Message-ID: <BAAF4725-B100-4B1C-814B-C5A2C0985947@dcn.davis.ca.us>

Please always reply-all to keep the list involved.

If you used Save As to change the data format to Excel AND the file extension to xlsx, then yes, you should be able to read with readxl. I don't recommend it, though... Excel often changes data silently and in irregularly located places in your file.

On December 26, 2018 7:38:16 PM PST, Spencer Brackett <spbrackett20 at saintjosephhs.com> wrote:
>So even if I imported the file form ICGC to my desktop as an excel
>file,
>and can view and saved the data as such, it is still a TSV?
>
>On Wed, Dec 26, 2018 at 10:35 PM Jeff Newmiller
><jdnewmil at dcn.davis.ca.us>
>wrote:
>
>> CSV and TSV are not Excel files. Yes, I know Excel will open them,
>but
>> that does not make them Excel files.
>>
>> Read a TSV file with read.table or read.csv, setting the sep argument
>to
>> "\t".
>>
>> On December 26, 2018 7:26:35 PM PST, Spencer Brackett <
>> spbrackett20 at saintjosephhs.com> wrote:
>> >I tried importing the file without preview and recieved the
>> >following....
>> >
>> >library(readxl)
>> >> GBM_protein_expression <- read_excel("C:/Users/Spencer/Desktop/GBM
>> >protein_expression.csv")
>> >Error: Can't establish that the input is either xls or xlsx.
>> >> View(GBM_protein_expression)
>> >Error in View : object 'GBM_protein_expression' not found
>> >Error in gzfile(file, mode) : cannot open the connection
>> >In addition: Warning message:
>> >In gzfile(file, mode) :
>> >  cannot open compressed file
>>
>>'C:/Users/Spencer/AppData/Local/Temp/RtmpQNQrMh/input147c61fc5b52.rds',
>> >probable reason 'No such file or directory'
>> >> library(readxl)
>> >> GBM_protein_expression <-
>> >read_excel("C:/Users/Spencer/Desktop/GBM_protein_ expression.xlsx")
>> >readxl works best with a newer version of the tibble package.
>> >You currently have tibble v1.4.2.
>> >Falling back to column name repair from tibble <= v1.4.2.
>> >Message displays once per session.
>> >> View(GBM_protein_expression)
>> >
>> >Also, the area above my console says that no data is available in
>the
>> >table. Is this perhaps the result of lack of preview or the fact
>that
>> >the
>> >excel file itself contains no numerical data, but only TRUE or FALSE
>> >entries?
>> >
>> >On Wed, Dec 26, 2018 at 9:57 PM Spencer Brackett <
>> >spbrackett20 at saintjosephhs.com> wrote:
>> >
>> >> Hello again,
>> >>
>> >> I worked on directly downloading the file into R as was suggested,
>> >but
>> >> have thus far been unsuccessful. This is what  I generated on my
>> >second
>> >> attempt...
>> >>
>> >>  GBM protein_expression<-(file.choose(), header=TRUE, sep="\t")
>> >> Error: unexpected symbol in "GBM protein_expression"
>> >> > GBM
>> >>
>>
>>protein_expression<-(file.choose(GBM_protein_expression.xlsx),header=TRUE,
>> >> sep="\t")
>> >> Error: unexpected symbol in "GBM protein_expression"
>> >> >
>> >>
>> >> What part of the argument is in error?
>> >>
>> >> Also I tried importing the dataset as an excel file on RStudio to
>see
>> >if I
>> >> could solve my problem that way. However, my imported excel file
>has
>> >been
>> >> stuck in the 'retrieving preview data' and no data is appearing.
>Is
>> >the
>> >> data file prehaps too large or in the wrong format?
>> >>
>> >>
>> >>
>> >> On Wed, Dec 26, 2018 at 6:42 PM Spencer Brackett <
>> >> spbrackett20 at saintjosephhs.com> wrote:
>> >>
>> >>> Mr. Heiberger,
>> >>>
>> >>>  Thank you for the insight! I will try out suggestion.
>> >>>
>> >>> Best,
>> >>>
>> >>> Spencer Brackett
>> >>>
>> >>> On Wed, Dec 26, 2018 at 6:34 PM Richard M. Heiberger
>> ><rmh at temple.edu>
>> >>> wrote:
>> >>>
>> >>>> I looked at the first file.  It gives an option to download as
>TSV
>> >>>> (tab separated values).
>> >>>> That is the same as CSV except with tabs instead of commas.
>> >>>> You do not need any external software to read it.  Read the
>> >downloaded
>> >>>> file directly into R.
>> >>>>
>> >>>> read.delim looks as if it would work directly on the downloaded
>> >file.
>> >>>> ?read.delim
>> >>>> The notation "\t" means the tab character.
>> >>>>
>> >>>> As an aside, stay away from notepad. it is too naive for almost
>> >>>> anything interesting.
>> >>>> The specific case I often see is people reading linux-style text
>> >files
>> >>>> with notepad, which doesn't
>> >>>> understand NL terminated lines.  nicely formatted text files
>become
>> >>>> illegible.
>> >>>>
>> >>>> On Wed, Dec 26, 2018 at 6:04 PM Spencer Brackett
>> >>>> <spbrackett20 at saintjosephhs.com> wrote:
>> >>>> >
>> >>>> > Good evening,
>> >>>> >
>> >>>> > I am attempting to anaylze the protein expression data
>contained
>> >within
>> >>>> > these two ICGC, TCGA datasets (one for GBM and the other for
>LGG)
>> >>>> >
>> >>>> > *File for GBM  protein expression*:
>> >>>> >
>> >>>>
>> >
>>
>https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22GBM-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>> >>>> >
>> >>>> > *File for LGG protein expression:*
>> >>>> >
>> >>>> >
>> >>>> > *
>> >>>>
>> >
>>
>https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>> >>>> > <
>> >>>>
>> >
>>
>https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>> >>>> >*
>> >>>> >
>> >>>> >   When I tried to transfer the files from .txt (via Notepad)
>to
>> >.csv
>> >>>> (via
>> >>>> > Excel), the data appeared in the columns as unorganized and
>> >random
>> >>>> > script... not like how a typical csv should be arranged at
>all. I
>> >need
>> >>>> the
>> >>>> > dataset to be converted into .csv in order to analyze it in R,
>> >which
>> >>>> is why
>> >>>> > I am hoping someone here might help me in doing that. If not,
>is
>> >there
>> >>>> > perhaps some other way that I could analyze the datatsets on
>R,
>> >which
>> >>>> again
>> >>>> > is downloaded from the dataportal ICGC?
>> >>>> >
>> >>>> > Best,
>> >>>> >
>> >>>> > Spencer Brackett
>> >>>> >
>> >>>> >         [[alternative HTML version deleted]]
>> >>>> >
>> >>>> > ______________________________________________
>> >>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>see
>> >>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>> > PLEASE do read the posting guide
>> >>>> http://www.R-project.org/posting-guide.html
>> >>>> > and provide commented, minimal, self-contained, reproducible
>> >code.
>> >>>>
>> >>>
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> >______________________________________________
>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >PLEASE do read the posting guide
>> >http://www.R-project.org/posting-guide.html
>> >and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Sent from my phone. Please excuse my brevity.
>>

-- 
Sent from my phone. Please excuse my brevity.


From jdnewmil @ending from dcn@d@vi@@c@@u@  Thu Dec 27 05:02:38 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Wed, 26 Dec 2018 20:02:38 -0800
Subject: [R] Long input lines in R.exe
In-Reply-To: <CAP3sJ3eSBarDSquzdw+pWbEHDOfub20_8bwh8r7_4b0ZEa=Hig@mail.gmail.com>
References: <CAP3sJ3eSBarDSquzdw+pWbEHDOfub20_8bwh8r7_4b0ZEa=Hig@mail.gmail.com>
Message-ID: <176D6CF9-A661-42B2-ADA0-92F194227EED@dcn.davis.ca.us>

No. This is a limitation of CMD that R is not designed to work around.

On December 26, 2018 8:47:44 AM PST, RK <arkay7777 at gmail.com> wrote:
>Hi,
>
>When I run R.exe (or Rterm.exe) from cmd.exe and enter in a long input
>line, e.g.
>
>print("12345678901234567890123456789012345678901234567890123456789012345678901234567890")
>
>the visible portion of the input is truncated to:
>
>print("123456789012345678901234567890123456789012345678901234567890123456789$
>
>In effect, I cannot see the full input line all at once. The visible
>portion of the input, including the $ symbol is 77 characters which
>must be a terminal width setting. Scrolling to the other end of the
>line, allows me to see the hidden portion but this is not ideal.
>Changing options(width=132) within R.exe/cmd.exe shows long output
>lines, which tells me that terminal width is not a fundamental
>limitation.
>
>This behavior does not occur when using RGui.exe, RStudio, etc. where
>the visible portion of the input lines is limited by screen width.
>
>Is there any way to make R.exe show long input lines when using
>cmd.exe? I prefer to use R.exe when I am working off the Windows
>console.
>
>I am using R 3.5.2 on Win10, 64-bit.
>
>Thanks,
>RK
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @pbr@ckett20 @ending from @@intjo@ephh@@com  Thu Dec 27 05:07:32 2018
From: @pbr@ckett20 @ending from @@intjo@ephh@@com (Spencer Brackett)
Date: Wed, 26 Dec 2018 23:07:32 -0500
Subject: [R] Fwd: UPDATE
In-Reply-To: <BAAF4725-B100-4B1C-814B-C5A2C0985947@dcn.davis.ca.us>
References: <CAPQaxLN2t=y8t_NgWpSUU_erg=44GC=8+PedewUq7OYabx7jjg@mail.gmail.com>
 <CAGx1TMAnF7V7jEig0mGUgciYY70FfufCuV8GzU0TnPOLOYhEfw@mail.gmail.com>
 <CAPQaxLPbFTFZyTGahAH0_j=06mfovgN4mAcMEQJo87BoGRmRjQ@mail.gmail.com>
 <CAPQaxLPmCfXHjJgXbV98kvNfHEGPbqKB9a-LiVNTWXu0_QgwSw@mail.gmail.com>
 <CAPQaxLNVU86dmGPdiz43g1iw3as+LR=TjzgjGQnFF_xju5_76g@mail.gmail.com>
 <CAPQaxLPmeKTuMjtU4ENGAc5gyfoQSEcMkCPFHK4czyFvV0N+CA@mail.gmail.com>
 <CAPQaxLP+PFjbX9200g9oi8Xk2-2Rhvfqj=PGmLBMnO-LOBBXbA@mail.gmail.com>
 <D3662A59-E568-4A7F-9665-D24F42865BB2@dcn.davis.ca.us>
 <CAPQaxLMG1oF-79iyReu3W-L6ZiE9H8t1dkxmsDL4N4MawphW+w@mail.gmail.com>
 <BAAF4725-B100-4B1C-814B-C5A2C0985947@dcn.davis.ca.us>
Message-ID: <CAPQaxLO15X39qio1u_OXkO6o5m0M55F8d==suNVf0eER+Z494A@mail.gmail.com>

Sorry, my mistake.

So I could still use read.table and should I try using a .txt version of
the file to avoid the silent changes you described?

Also, when I tried to simply this process by downloading the dataset onto
RStudio opposed to R (Gui) I received the following...
 library(readxl)
> GBM_protein_expression <- read_excel("C:/Users/Spencer/Desktop/GBM
protein_expression.csv")
Error: Can't establish that the input is either xls or xlsx.
> View(GBM_protein_expression)
Error in View : object 'GBM_protein_expression' not found
Error in gzfile(file, mode) : cannot open the connection
In addition: Warning message:
In gzfile(file, mode) :
  cannot open compressed file
'C:/Users/Spencer/AppData/Local/Temp/RtmpQNQrMh/input147c61fc5b52.rds',
probable reason 'No such file or directory'
> library(readxl)
> GBM_protein_expression <-
read_excel("C:/Users/Spencer/Desktop/GBM_protein_ expression.xlsx")
readxl works best with a newer version of the tibble package.
You currently have tibble v1.4.2.
Falling back to column name repair from tibble <= v1.4.2.
Message displays once per session.
> View(GBM_protein_expression)


Is this perhaps the result of lack of preview (which I did not complete at
the time I hit import as the preview failed to load), or the fact that the
excel file itself contains no numerical data, but only TRUE or FALSE
entries?

On Wed, Dec 26, 2018 at 10:59 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Please always reply-all to keep the list involved.
>
> If you used Save As to change the data format to Excel AND the file
> extension to xlsx, then yes, you should be able to read with readxl. I
> don't recommend it, though... Excel often changes data silently and in
> irregularly located places in your file.
>
> On December 26, 2018 7:38:16 PM PST, Spencer Brackett <
> spbrackett20 at saintjosephhs.com> wrote:
> >So even if I imported the file form ICGC to my desktop as an excel
> >file,
> >and can view and saved the data as such, it is still a TSV?
> >
> >On Wed, Dec 26, 2018 at 10:35 PM Jeff Newmiller
> ><jdnewmil at dcn.davis.ca.us>
> >wrote:
> >
> >> CSV and TSV are not Excel files. Yes, I know Excel will open them,
> >but
> >> that does not make them Excel files.
> >>
> >> Read a TSV file with read.table or read.csv, setting the sep argument
> >to
> >> "\t".
> >>
> >> On December 26, 2018 7:26:35 PM PST, Spencer Brackett <
> >> spbrackett20 at saintjosephhs.com> wrote:
> >> >I tried importing the file without preview and recieved the
> >> >following....
> >> >
> >> >library(readxl)
> >> >> GBM_protein_expression <- read_excel("C:/Users/Spencer/Desktop/GBM
> >> >protein_expression.csv")
> >> >Error: Can't establish that the input is either xls or xlsx.
> >> >> View(GBM_protein_expression)
> >> >Error in View : object 'GBM_protein_expression' not found
> >> >Error in gzfile(file, mode) : cannot open the connection
> >> >In addition: Warning message:
> >> >In gzfile(file, mode) :
> >> >  cannot open compressed file
> >>
> >>'C:/Users/Spencer/AppData/Local/Temp/RtmpQNQrMh/input147c61fc5b52.rds',
> >> >probable reason 'No such file or directory'
> >> >> library(readxl)
> >> >> GBM_protein_expression <-
> >> >read_excel("C:/Users/Spencer/Desktop/GBM_protein_ expression.xlsx")
> >> >readxl works best with a newer version of the tibble package.
> >> >You currently have tibble v1.4.2.
> >> >Falling back to column name repair from tibble <= v1.4.2.
> >> >Message displays once per session.
> >> >> View(GBM_protein_expression)
> >> >
> >> >Also, the area above my console says that no data is available in
> >the
> >> >table. Is this perhaps the result of lack of preview or the fact
> >that
> >> >the
> >> >excel file itself contains no numerical data, but only TRUE or FALSE
> >> >entries?
> >> >
> >> >On Wed, Dec 26, 2018 at 9:57 PM Spencer Brackett <
> >> >spbrackett20 at saintjosephhs.com> wrote:
> >> >
> >> >> Hello again,
> >> >>
> >> >> I worked on directly downloading the file into R as was suggested,
> >> >but
> >> >> have thus far been unsuccessful. This is what  I generated on my
> >> >second
> >> >> attempt...
> >> >>
> >> >>  GBM protein_expression<-(file.choose(), header=TRUE, sep="\t")
> >> >> Error: unexpected symbol in "GBM protein_expression"
> >> >> > GBM
> >> >>
> >>
>
> >>protein_expression<-(file.choose(GBM_protein_expression.xlsx),header=TRUE,
> >> >> sep="\t")
> >> >> Error: unexpected symbol in "GBM protein_expression"
> >> >> >
> >> >>
> >> >> What part of the argument is in error?
> >> >>
> >> >> Also I tried importing the dataset as an excel file on RStudio to
> >see
> >> >if I
> >> >> could solve my problem that way. However, my imported excel file
> >has
> >> >been
> >> >> stuck in the 'retrieving preview data' and no data is appearing.
> >Is
> >> >the
> >> >> data file prehaps too large or in the wrong format?
> >> >>
> >> >>
> >> >>
> >> >> On Wed, Dec 26, 2018 at 6:42 PM Spencer Brackett <
> >> >> spbrackett20 at saintjosephhs.com> wrote:
> >> >>
> >> >>> Mr. Heiberger,
> >> >>>
> >> >>>  Thank you for the insight! I will try out suggestion.
> >> >>>
> >> >>> Best,
> >> >>>
> >> >>> Spencer Brackett
> >> >>>
> >> >>> On Wed, Dec 26, 2018 at 6:34 PM Richard M. Heiberger
> >> ><rmh at temple.edu>
> >> >>> wrote:
> >> >>>
> >> >>>> I looked at the first file.  It gives an option to download as
> >TSV
> >> >>>> (tab separated values).
> >> >>>> That is the same as CSV except with tabs instead of commas.
> >> >>>> You do not need any external software to read it.  Read the
> >> >downloaded
> >> >>>> file directly into R.
> >> >>>>
> >> >>>> read.delim looks as if it would work directly on the downloaded
> >> >file.
> >> >>>> ?read.delim
> >> >>>> The notation "\t" means the tab character.
> >> >>>>
> >> >>>> As an aside, stay away from notepad. it is too naive for almost
> >> >>>> anything interesting.
> >> >>>> The specific case I often see is people reading linux-style text
> >> >files
> >> >>>> with notepad, which doesn't
> >> >>>> understand NL terminated lines.  nicely formatted text files
> >become
> >> >>>> illegible.
> >> >>>>
> >> >>>> On Wed, Dec 26, 2018 at 6:04 PM Spencer Brackett
> >> >>>> <spbrackett20 at saintjosephhs.com> wrote:
> >> >>>> >
> >> >>>> > Good evening,
> >> >>>> >
> >> >>>> > I am attempting to anaylze the protein expression data
> >contained
> >> >within
> >> >>>> > these two ICGC, TCGA datasets (one for GBM and the other for
> >LGG)
> >> >>>> >
> >> >>>> > *File for GBM  protein expression*:
> >> >>>> >
> >> >>>>
> >> >
> >>
> >
> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22GBM-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
> >> >>>> >
> >> >>>> > *File for LGG protein expression:*
> >> >>>> >
> >> >>>> >
> >> >>>> > *
> >> >>>>
> >> >
> >>
> >
> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
> >> >>>> > <
> >> >>>>
> >> >
> >>
> >
> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
> >> >>>> >*
> >> >>>> >
> >> >>>> >   When I tried to transfer the files from .txt (via Notepad)
> >to
> >> >.csv
> >> >>>> (via
> >> >>>> > Excel), the data appeared in the columns as unorganized and
> >> >random
> >> >>>> > script... not like how a typical csv should be arranged at
> >all. I
> >> >need
> >> >>>> the
> >> >>>> > dataset to be converted into .csv in order to analyze it in R,
> >> >which
> >> >>>> is why
> >> >>>> > I am hoping someone here might help me in doing that. If not,
> >is
> >> >there
> >> >>>> > perhaps some other way that I could analyze the datatsets on
> >R,
> >> >which
> >> >>>> again
> >> >>>> > is downloaded from the dataportal ICGC?
> >> >>>> >
> >> >>>> > Best,
> >> >>>> >
> >> >>>> > Spencer Brackett
> >> >>>> >
> >> >>>> >         [[alternative HTML version deleted]]
> >> >>>> >
> >> >>>> > ______________________________________________
> >> >>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >see
> >> >>>> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> >>>> > PLEASE do read the posting guide
> >> >>>> http://www.R-project.org/posting-guide.html
> >> >>>> > and provide commented, minimal, self-contained, reproducible
> >> >code.
> >> >>>>
> >> >>>
> >> >
> >> >       [[alternative HTML version deleted]]
> >> >
> >> >______________________________________________
> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >https://stat.ethz.ch/mailman/listinfo/r-help
> >> >PLEASE do read the posting guide
> >> >http://www.R-project.org/posting-guide.html
> >> >and provide commented, minimal, self-contained, reproducible code.
> >>
> >> --
> >> Sent from my phone. Please excuse my brevity.
> >>
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From rmh @ending from temple@edu  Thu Dec 27 05:23:27 2018
From: rmh @ending from temple@edu (Richard M. Heiberger)
Date: Wed, 26 Dec 2018 23:23:27 -0500
Subject: [R] Fwd: UPDATE
In-Reply-To: <CAPQaxLO15X39qio1u_OXkO6o5m0M55F8d==suNVf0eER+Z494A@mail.gmail.com>
References: <CAPQaxLN2t=y8t_NgWpSUU_erg=44GC=8+PedewUq7OYabx7jjg@mail.gmail.com>
 <CAGx1TMAnF7V7jEig0mGUgciYY70FfufCuV8GzU0TnPOLOYhEfw@mail.gmail.com>
 <CAPQaxLPbFTFZyTGahAH0_j=06mfovgN4mAcMEQJo87BoGRmRjQ@mail.gmail.com>
 <CAPQaxLPmCfXHjJgXbV98kvNfHEGPbqKB9a-LiVNTWXu0_QgwSw@mail.gmail.com>
 <CAPQaxLNVU86dmGPdiz43g1iw3as+LR=TjzgjGQnFF_xju5_76g@mail.gmail.com>
 <CAPQaxLPmeKTuMjtU4ENGAc5gyfoQSEcMkCPFHK4czyFvV0N+CA@mail.gmail.com>
 <CAPQaxLP+PFjbX9200g9oi8Xk2-2Rhvfqj=PGmLBMnO-LOBBXbA@mail.gmail.com>
 <D3662A59-E568-4A7F-9665-D24F42865BB2@dcn.davis.ca.us>
 <CAPQaxLMG1oF-79iyReu3W-L6ZiE9H8t1dkxmsDL4N4MawphW+w@mail.gmail.com>
 <BAAF4725-B100-4B1C-814B-C5A2C0985947@dcn.davis.ca.us>
 <CAPQaxLO15X39qio1u_OXkO6o5m0M55F8d==suNVf0eER+Z494A@mail.gmail.com>
Message-ID: <CAGx1TMB=_g2oF8Xiu-OzNWe7k4SjYkMUOTnQWvO7YHghqoAgMw@mail.gmail.com>

this is wrong because the file is a csv file.  read_excel is designed
for xls files.
GBM_protein_expression <- read_excel("C:/Users/Spencer/Desktop/GBM
protein_expression.csv")

How did you get a csv? it downloads as tsv.

the statement you should use is in base, no library() statement is needed.

GBM_protein_expression <- read.delim("C:/Users/Spencer/Desktop/GBM
protein_expression.csv")

read.delim is the same as read.csv except that it sets the sep
argument to "\t".



On Wed, Dec 26, 2018 at 11:11 PM Spencer Brackett
<spbrackett20 at saintjosephhs.com> wrote:
>
> Sorry, my mistake.
>
> So I could still use read.table and should I try using a .txt version of
> the file to avoid the silent changes you described?
>
> Also, when I tried to simply this process by downloading the dataset onto
> RStudio opposed to R (Gui) I received the following...
>  library(readxl)
> > GBM_protein_expression <- read_excel("C:/Users/Spencer/Desktop/GBM
> protein_expression.csv")
> Error: Can't establish that the input is either xls or xlsx.
> > View(GBM_protein_expression)
> Error in View : object 'GBM_protein_expression' not found
> Error in gzfile(file, mode) : cannot open the connection
> In addition: Warning message:
> In gzfile(file, mode) :
>   cannot open compressed file
> 'C:/Users/Spencer/AppData/Local/Temp/RtmpQNQrMh/input147c61fc5b52.rds',
> probable reason 'No such file or directory'
> > library(readxl)
> > GBM_protein_expression <-
> read_excel("C:/Users/Spencer/Desktop/GBM_protein_ expression.xlsx")
> readxl works best with a newer version of the tibble package.
> You currently have tibble v1.4.2.
> Falling back to column name repair from tibble <= v1.4.2.
> Message displays once per session.
> > View(GBM_protein_expression)
>
>
> Is this perhaps the result of lack of preview (which I did not complete at
> the time I hit import as the preview failed to load), or the fact that the
> excel file itself contains no numerical data, but only TRUE or FALSE
> entries?
>
> On Wed, Dec 26, 2018 at 10:59 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
>
> > Please always reply-all to keep the list involved.
> >
> > If you used Save As to change the data format to Excel AND the file
> > extension to xlsx, then yes, you should be able to read with readxl. I
> > don't recommend it, though... Excel often changes data silently and in
> > irregularly located places in your file.
> >
> > On December 26, 2018 7:38:16 PM PST, Spencer Brackett <
> > spbrackett20 at saintjosephhs.com> wrote:
> > >So even if I imported the file form ICGC to my desktop as an excel
> > >file,
> > >and can view and saved the data as such, it is still a TSV?
> > >
> > >On Wed, Dec 26, 2018 at 10:35 PM Jeff Newmiller
> > ><jdnewmil at dcn.davis.ca.us>
> > >wrote:
> > >
> > >> CSV and TSV are not Excel files. Yes, I know Excel will open them,
> > >but
> > >> that does not make them Excel files.
> > >>
> > >> Read a TSV file with read.table or read.csv, setting the sep argument
> > >to
> > >> "\t".
> > >>
> > >> On December 26, 2018 7:26:35 PM PST, Spencer Brackett <
> > >> spbrackett20 at saintjosephhs.com> wrote:
> > >> >I tried importing the file without preview and recieved the
> > >> >following....
> > >> >
> > >> >library(readxl)
> > >> >> GBM_protein_expression <- read_excel("C:/Users/Spencer/Desktop/GBM
> > >> >protein_expression.csv")
> > >> >Error: Can't establish that the input is either xls or xlsx.
> > >> >> View(GBM_protein_expression)
> > >> >Error in View : object 'GBM_protein_expression' not found
> > >> >Error in gzfile(file, mode) : cannot open the connection
> > >> >In addition: Warning message:
> > >> >In gzfile(file, mode) :
> > >> >  cannot open compressed file
> > >>
> > >>'C:/Users/Spencer/AppData/Local/Temp/RtmpQNQrMh/input147c61fc5b52.rds',
> > >> >probable reason 'No such file or directory'
> > >> >> library(readxl)
> > >> >> GBM_protein_expression <-
> > >> >read_excel("C:/Users/Spencer/Desktop/GBM_protein_ expression.xlsx")
> > >> >readxl works best with a newer version of the tibble package.
> > >> >You currently have tibble v1.4.2.
> > >> >Falling back to column name repair from tibble <= v1.4.2.
> > >> >Message displays once per session.
> > >> >> View(GBM_protein_expression)
> > >> >
> > >> >Also, the area above my console says that no data is available in
> > >the
> > >> >table. Is this perhaps the result of lack of preview or the fact
> > >that
> > >> >the
> > >> >excel file itself contains no numerical data, but only TRUE or FALSE
> > >> >entries?
> > >> >
> > >> >On Wed, Dec 26, 2018 at 9:57 PM Spencer Brackett <
> > >> >spbrackett20 at saintjosephhs.com> wrote:
> > >> >
> > >> >> Hello again,
> > >> >>
> > >> >> I worked on directly downloading the file into R as was suggested,
> > >> >but
> > >> >> have thus far been unsuccessful. This is what  I generated on my
> > >> >second
> > >> >> attempt...
> > >> >>
> > >> >>  GBM protein_expression<-(file.choose(), header=TRUE, sep="\t")
> > >> >> Error: unexpected symbol in "GBM protein_expression"
> > >> >> > GBM
> > >> >>
> > >>
> >
> > >>protein_expression<-(file.choose(GBM_protein_expression.xlsx),header=TRUE,
> > >> >> sep="\t")
> > >> >> Error: unexpected symbol in "GBM protein_expression"
> > >> >> >
> > >> >>
> > >> >> What part of the argument is in error?
> > >> >>
> > >> >> Also I tried importing the dataset as an excel file on RStudio to
> > >see
> > >> >if I
> > >> >> could solve my problem that way. However, my imported excel file
> > >has
> > >> >been
> > >> >> stuck in the 'retrieving preview data' and no data is appearing.
> > >Is
> > >> >the
> > >> >> data file prehaps too large or in the wrong format?
> > >> >>
> > >> >>
> > >> >>
> > >> >> On Wed, Dec 26, 2018 at 6:42 PM Spencer Brackett <
> > >> >> spbrackett20 at saintjosephhs.com> wrote:
> > >> >>
> > >> >>> Mr. Heiberger,
> > >> >>>
> > >> >>>  Thank you for the insight! I will try out suggestion.
> > >> >>>
> > >> >>> Best,
> > >> >>>
> > >> >>> Spencer Brackett
> > >> >>>
> > >> >>> On Wed, Dec 26, 2018 at 6:34 PM Richard M. Heiberger
> > >> ><rmh at temple.edu>
> > >> >>> wrote:
> > >> >>>
> > >> >>>> I looked at the first file.  It gives an option to download as
> > >TSV
> > >> >>>> (tab separated values).
> > >> >>>> That is the same as CSV except with tabs instead of commas.
> > >> >>>> You do not need any external software to read it.  Read the
> > >> >downloaded
> > >> >>>> file directly into R.
> > >> >>>>
> > >> >>>> read.delim looks as if it would work directly on the downloaded
> > >> >file.
> > >> >>>> ?read.delim
> > >> >>>> The notation "\t" means the tab character.
> > >> >>>>
> > >> >>>> As an aside, stay away from notepad. it is too naive for almost
> > >> >>>> anything interesting.
> > >> >>>> The specific case I often see is people reading linux-style text
> > >> >files
> > >> >>>> with notepad, which doesn't
> > >> >>>> understand NL terminated lines.  nicely formatted text files
> > >become
> > >> >>>> illegible.
> > >> >>>>
> > >> >>>> On Wed, Dec 26, 2018 at 6:04 PM Spencer Brackett
> > >> >>>> <spbrackett20 at saintjosephhs.com> wrote:
> > >> >>>> >
> > >> >>>> > Good evening,
> > >> >>>> >
> > >> >>>> > I am attempting to anaylze the protein expression data
> > >contained
> > >> >within
> > >> >>>> > these two ICGC, TCGA datasets (one for GBM and the other for
> > >LGG)
> > >> >>>> >
> > >> >>>> > *File for GBM  protein expression*:
> > >> >>>> >
> > >> >>>>
> > >> >
> > >>
> > >
> > https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22GBM-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
> > >> >>>> >
> > >> >>>> > *File for LGG protein expression:*
> > >> >>>> >
> > >> >>>> >
> > >> >>>> > *
> > >> >>>>
> > >> >
> > >>
> > >
> > https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
> > >> >>>> > <
> > >> >>>>
> > >> >
> > >>
> > >
> > https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
> > >> >>>> >*
> > >> >>>> >
> > >> >>>> >   When I tried to transfer the files from .txt (via Notepad)
> > >to
> > >> >.csv
> > >> >>>> (via
> > >> >>>> > Excel), the data appeared in the columns as unorganized and
> > >> >random
> > >> >>>> > script... not like how a typical csv should be arranged at
> > >all. I
> > >> >need
> > >> >>>> the
> > >> >>>> > dataset to be converted into .csv in order to analyze it in R,
> > >> >which
> > >> >>>> is why
> > >> >>>> > I am hoping someone here might help me in doing that. If not,
> > >is
> > >> >there
> > >> >>>> > perhaps some other way that I could analyze the datatsets on
> > >R,
> > >> >which
> > >> >>>> again
> > >> >>>> > is downloaded from the dataportal ICGC?
> > >> >>>> >
> > >> >>>> > Best,
> > >> >>>> >
> > >> >>>> > Spencer Brackett
> > >> >>>> >
> > >> >>>> >         [[alternative HTML version deleted]]
> > >> >>>> >
> > >> >>>> > ______________________________________________
> > >> >>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> > >see
> > >> >>>> > https://stat.ethz.ch/mailman/listinfo/r-help
> > >> >>>> > PLEASE do read the posting guide
> > >> >>>> http://www.R-project.org/posting-guide.html
> > >> >>>> > and provide commented, minimal, self-contained, reproducible
> > >> >code.
> > >> >>>>
> > >> >>>
> > >> >
> > >> >       [[alternative HTML version deleted]]
> > >> >
> > >> >______________________________________________
> > >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >> >https://stat.ethz.ch/mailman/listinfo/r-help
> > >> >PLEASE do read the posting guide
> > >> >http://www.R-project.org/posting-guide.html
> > >> >and provide commented, minimal, self-contained, reproducible code.
> > >>
> > >> --
> > >> Sent from my phone. Please excuse my brevity.
> > >>
> >
> > --
> > Sent from my phone. Please excuse my brevity.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bioprogr@mmer @ending from gm@il@com  Thu Dec 27 05:55:34 2018
From: bioprogr@mmer @ending from gm@il@com (Caitlin Gibbons)
Date: Wed, 26 Dec 2018 21:55:34 -0700
Subject: [R] Fwd: UPDATE
In-Reply-To: <CAGx1TMB=_g2oF8Xiu-OzNWe7k4SjYkMUOTnQWvO7YHghqoAgMw@mail.gmail.com>
References: <CAPQaxLN2t=y8t_NgWpSUU_erg=44GC=8+PedewUq7OYabx7jjg@mail.gmail.com>
 <CAGx1TMAnF7V7jEig0mGUgciYY70FfufCuV8GzU0TnPOLOYhEfw@mail.gmail.com>
 <CAPQaxLPbFTFZyTGahAH0_j=06mfovgN4mAcMEQJo87BoGRmRjQ@mail.gmail.com>
 <CAPQaxLPmCfXHjJgXbV98kvNfHEGPbqKB9a-LiVNTWXu0_QgwSw@mail.gmail.com>
 <CAPQaxLNVU86dmGPdiz43g1iw3as+LR=TjzgjGQnFF_xju5_76g@mail.gmail.com>
 <CAPQaxLPmeKTuMjtU4ENGAc5gyfoQSEcMkCPFHK4czyFvV0N+CA@mail.gmail.com>
 <CAPQaxLP+PFjbX9200g9oi8Xk2-2Rhvfqj=PGmLBMnO-LOBBXbA@mail.gmail.com>
 <D3662A59-E568-4A7F-9665-D24F42865BB2@dcn.davis.ca.us>
 <CAPQaxLMG1oF-79iyReu3W-L6ZiE9H8t1dkxmsDL4N4MawphW+w@mail.gmail.com>
 <BAAF4725-B100-4B1C-814B-C5A2C0985947@dcn.davis.ca.us>
 <CAPQaxLO15X39qio1u_OXkO6o5m0M55F8d==suNVf0eER+Z494A@mail.gmail.com>
 <CAGx1TMB=_g2oF8Xiu-OzNWe7k4SjYkMUOTnQWvO7YHghqoAgMw@mail.gmail.com>
Message-ID: <DF19FBD9-25AD-48D5-A0D8-BB4D9F9D9C87@gmail.com>

Does this help Spencer? The read.delim() function assumes a tab character by default, but I specifically included it using the read.csv function. The downloaded file is NOT an Excel file so this should help. 

GBM_protein_expression <- read.csv("C:/Users/Spencer/Desktop/GBM
protein_expression.tsv", sep=?\t?)

Sent from my iPhone

> On Dec 26, 2018, at 9:23 PM, Richard M. Heiberger <rmh at temple.edu> wrote:
> 
> this is wrong because the file is a csv file.  read_excel is designed
> for xls files.
> GBM_protein_expression <- read_excel("C:/Users/Spencer/Desktop/GBM
> protein_expression.csv")
> 
> How did you get a csv? it downloads as tsv.
> 
> the statement you should use is in base, no library() statement is needed.
> 
> GBM_protein_expression <- read.delim("C:/Users/Spencer/Desktop/GBM
> protein_expression.csv")
> 
> read.delim is the same as read.csv except that it sets the sep
> argument to "\t".
> 
> 
> 
> On Wed, Dec 26, 2018 at 11:11 PM Spencer Brackett
> <spbrackett20 at saintjosephhs.com> wrote:
>> 
>> Sorry, my mistake.
>> 
>> So I could still use read.table and should I try using a .txt version of
>> the file to avoid the silent changes you described?
>> 
>> Also, when I tried to simply this process by downloading the dataset onto
>> RStudio opposed to R (Gui) I received the following...
>> library(readxl)
>>> GBM_protein_expression <- read_excel("C:/Users/Spencer/Desktop/GBM
>> protein_expression.csv")
>> Error: Can't establish that the input is either xls or xlsx.
>>> View(GBM_protein_expression)
>> Error in View : object 'GBM_protein_expression' not found
>> Error in gzfile(file, mode) : cannot open the connection
>> In addition: Warning message:
>> In gzfile(file, mode) :
>>  cannot open compressed file
>> 'C:/Users/Spencer/AppData/Local/Temp/RtmpQNQrMh/input147c61fc5b52.rds',
>> probable reason 'No such file or directory'
>>> library(readxl)
>>> GBM_protein_expression <-
>> read_excel("C:/Users/Spencer/Desktop/GBM_protein_ expression.xlsx")
>> readxl works best with a newer version of the tibble package.
>> You currently have tibble v1.4.2.
>> Falling back to column name repair from tibble <= v1.4.2.
>> Message displays once per session.
>>> View(GBM_protein_expression)
>> 
>> 
>> Is this perhaps the result of lack of preview (which I did not complete at
>> the time I hit import as the preview failed to load), or the fact that the
>> excel file itself contains no numerical data, but only TRUE or FALSE
>> entries?
>> 
>> On Wed, Dec 26, 2018 at 10:59 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>> wrote:
>> 
>>> Please always reply-all to keep the list involved.
>>> 
>>> If you used Save As to change the data format to Excel AND the file
>>> extension to xlsx, then yes, you should be able to read with readxl. I
>>> don't recommend it, though... Excel often changes data silently and in
>>> irregularly located places in your file.
>>> 
>>> On December 26, 2018 7:38:16 PM PST, Spencer Brackett <
>>> spbrackett20 at saintjosephhs.com> wrote:
>>>> So even if I imported the file form ICGC to my desktop as an excel
>>>> file,
>>>> and can view and saved the data as such, it is still a TSV?
>>>> 
>>>> On Wed, Dec 26, 2018 at 10:35 PM Jeff Newmiller
>>>> <jdnewmil at dcn.davis.ca.us>
>>>> wrote:
>>>> 
>>>>> CSV and TSV are not Excel files. Yes, I know Excel will open them,
>>>> but
>>>>> that does not make them Excel files.
>>>>> 
>>>>> Read a TSV file with read.table or read.csv, setting the sep argument
>>>> to
>>>>> "\t".
>>>>> 
>>>>> On December 26, 2018 7:26:35 PM PST, Spencer Brackett <
>>>>> spbrackett20 at saintjosephhs.com> wrote:
>>>>>> I tried importing the file without preview and recieved the
>>>>>> following....
>>>>>> 
>>>>>> library(readxl)
>>>>>>> GBM_protein_expression <- read_excel("C:/Users/Spencer/Desktop/GBM
>>>>>> protein_expression.csv")
>>>>>> Error: Can't establish that the input is either xls or xlsx.
>>>>>>> View(GBM_protein_expression)
>>>>>> Error in View : object 'GBM_protein_expression' not found
>>>>>> Error in gzfile(file, mode) : cannot open the connection
>>>>>> In addition: Warning message:
>>>>>> In gzfile(file, mode) :
>>>>>> cannot open compressed file
>>>>> 
>>>>> 'C:/Users/Spencer/AppData/Local/Temp/RtmpQNQrMh/input147c61fc5b52.rds',
>>>>>> probable reason 'No such file or directory'
>>>>>>> library(readxl)
>>>>>>> GBM_protein_expression <-
>>>>>> read_excel("C:/Users/Spencer/Desktop/GBM_protein_ expression.xlsx")
>>>>>> readxl works best with a newer version of the tibble package.
>>>>>> You currently have tibble v1.4.2.
>>>>>> Falling back to column name repair from tibble <= v1.4.2.
>>>>>> Message displays once per session.
>>>>>>> View(GBM_protein_expression)
>>>>>> 
>>>>>> Also, the area above my console says that no data is available in
>>>> the
>>>>>> table. Is this perhaps the result of lack of preview or the fact
>>>> that
>>>>>> the
>>>>>> excel file itself contains no numerical data, but only TRUE or FALSE
>>>>>> entries?
>>>>>> 
>>>>>> On Wed, Dec 26, 2018 at 9:57 PM Spencer Brackett <
>>>>>> spbrackett20 at saintjosephhs.com> wrote:
>>>>>> 
>>>>>>> Hello again,
>>>>>>> 
>>>>>>> I worked on directly downloading the file into R as was suggested,
>>>>>> but
>>>>>>> have thus far been unsuccessful. This is what  I generated on my
>>>>>> second
>>>>>>> attempt...
>>>>>>> 
>>>>>>> GBM protein_expression<-(file.choose(), header=TRUE, sep="\t")
>>>>>>> Error: unexpected symbol in "GBM protein_expression"
>>>>>>>> GBM
>>>>>>> 
>>>>> 
>>> 
>>>>> protein_expression<-(file.choose(GBM_protein_expression.xlsx),header=TRUE,
>>>>>>> sep="\t")
>>>>>>> Error: unexpected symbol in "GBM protein_expression"
>>>>>>>> 
>>>>>>> 
>>>>>>> What part of the argument is in error?
>>>>>>> 
>>>>>>> Also I tried importing the dataset as an excel file on RStudio to
>>>> see
>>>>>> if I
>>>>>>> could solve my problem that way. However, my imported excel file
>>>> has
>>>>>> been
>>>>>>> stuck in the 'retrieving preview data' and no data is appearing.
>>>> Is
>>>>>> the
>>>>>>> data file prehaps too large or in the wrong format?
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>> On Wed, Dec 26, 2018 at 6:42 PM Spencer Brackett <
>>>>>>> spbrackett20 at saintjosephhs.com> wrote:
>>>>>>> 
>>>>>>>> Mr. Heiberger,
>>>>>>>> 
>>>>>>>> Thank you for the insight! I will try out suggestion.
>>>>>>>> 
>>>>>>>> Best,
>>>>>>>> 
>>>>>>>> Spencer Brackett
>>>>>>>> 
>>>>>>>> On Wed, Dec 26, 2018 at 6:34 PM Richard M. Heiberger
>>>>>> <rmh at temple.edu>
>>>>>>>> wrote:
>>>>>>>> 
>>>>>>>>> I looked at the first file.  It gives an option to download as
>>>> TSV
>>>>>>>>> (tab separated values).
>>>>>>>>> That is the same as CSV except with tabs instead of commas.
>>>>>>>>> You do not need any external software to read it.  Read the
>>>>>> downloaded
>>>>>>>>> file directly into R.
>>>>>>>>> 
>>>>>>>>> read.delim looks as if it would work directly on the downloaded
>>>>>> file.
>>>>>>>>> ?read.delim
>>>>>>>>> The notation "\t" means the tab character.
>>>>>>>>> 
>>>>>>>>> As an aside, stay away from notepad. it is too naive for almost
>>>>>>>>> anything interesting.
>>>>>>>>> The specific case I often see is people reading linux-style text
>>>>>> files
>>>>>>>>> with notepad, which doesn't
>>>>>>>>> understand NL terminated lines.  nicely formatted text files
>>>> become
>>>>>>>>> illegible.
>>>>>>>>> 
>>>>>>>>> On Wed, Dec 26, 2018 at 6:04 PM Spencer Brackett
>>>>>>>>> <spbrackett20 at saintjosephhs.com> wrote:
>>>>>>>>>> 
>>>>>>>>>> Good evening,
>>>>>>>>>> 
>>>>>>>>>> I am attempting to anaylze the protein expression data
>>>> contained
>>>>>> within
>>>>>>>>>> these two ICGC, TCGA datasets (one for GBM and the other for
>>>> LGG)
>>>>>>>>>> 
>>>>>>>>>> *File for GBM  protein expression*:
>>>>>>>>>> 
>>>>>>>>> 
>>>>>> 
>>>>> 
>>>> 
>>> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22GBM-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>>>>>>>>>> 
>>>>>>>>>> *File for LGG protein expression:*
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> *
>>>>>>>>> 
>>>>>> 
>>>>> 
>>>> 
>>> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>>>>>>>>>> <
>>>>>>>>> 
>>>>>> 
>>>>> 
>>>> 
>>> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>>>>>>>>>> *
>>>>>>>>>> 
>>>>>>>>>>  When I tried to transfer the files from .txt (via Notepad)
>>>> to
>>>>>> .csv
>>>>>>>>> (via
>>>>>>>>>> Excel), the data appeared in the columns as unorganized and
>>>>>> random
>>>>>>>>>> script... not like how a typical csv should be arranged at
>>>> all. I
>>>>>> need
>>>>>>>>> the
>>>>>>>>>> dataset to be converted into .csv in order to analyze it in R,
>>>>>> which
>>>>>>>>> is why
>>>>>>>>>> I am hoping someone here might help me in doing that. If not,
>>>> is
>>>>>> there
>>>>>>>>>> perhaps some other way that I could analyze the datatsets on
>>>> R,
>>>>>> which
>>>>>>>>> again
>>>>>>>>>> is downloaded from the dataportal ICGC?
>>>>>>>>>> 
>>>>>>>>>> Best,
>>>>>>>>>> 
>>>>>>>>>> Spencer Brackett
>>>>>>>>>> 
>>>>>>>>>>        [[alternative HTML version deleted]]
>>>>>>>>>> 
>>>>>>>>>> ______________________________________________
>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>>> see
>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>>> code.
>>>>>>>>> 
>>>>>>>> 
>>>>>> 
>>>>>>      [[alternative HTML version deleted]]
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>> 
>>>>> --
>>>>> Sent from my phone. Please excuse my brevity.
>>>>> 
>>> 
>>> --
>>> Sent from my phone. Please excuse my brevity.
>>> 
>> 
>>        [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @pbr@ckett20 @ending from @@intjo@ephh@@com  Thu Dec 27 06:10:15 2018
From: @pbr@ckett20 @ending from @@intjo@ephh@@com (Spencer Brackett)
Date: Thu, 27 Dec 2018 00:10:15 -0500
Subject: [R] Fwd: UPDATE
In-Reply-To: <DF19FBD9-25AD-48D5-A0D8-BB4D9F9D9C87@gmail.com>
References: <CAPQaxLN2t=y8t_NgWpSUU_erg=44GC=8+PedewUq7OYabx7jjg@mail.gmail.com>
 <CAGx1TMAnF7V7jEig0mGUgciYY70FfufCuV8GzU0TnPOLOYhEfw@mail.gmail.com>
 <CAPQaxLPbFTFZyTGahAH0_j=06mfovgN4mAcMEQJo87BoGRmRjQ@mail.gmail.com>
 <CAPQaxLPmCfXHjJgXbV98kvNfHEGPbqKB9a-LiVNTWXu0_QgwSw@mail.gmail.com>
 <CAPQaxLNVU86dmGPdiz43g1iw3as+LR=TjzgjGQnFF_xju5_76g@mail.gmail.com>
 <CAPQaxLPmeKTuMjtU4ENGAc5gyfoQSEcMkCPFHK4czyFvV0N+CA@mail.gmail.com>
 <CAPQaxLP+PFjbX9200g9oi8Xk2-2Rhvfqj=PGmLBMnO-LOBBXbA@mail.gmail.com>
 <D3662A59-E568-4A7F-9665-D24F42865BB2@dcn.davis.ca.us>
 <CAPQaxLMG1oF-79iyReu3W-L6ZiE9H8t1dkxmsDL4N4MawphW+w@mail.gmail.com>
 <BAAF4725-B100-4B1C-814B-C5A2C0985947@dcn.davis.ca.us>
 <CAPQaxLO15X39qio1u_OXkO6o5m0M55F8d==suNVf0eER+Z494A@mail.gmail.com>
 <CAGx1TMB=_g2oF8Xiu-OzNWe7k4SjYkMUOTnQWvO7YHghqoAgMw@mail.gmail.com>
 <DF19FBD9-25AD-48D5-A0D8-BB4D9F9D9C87@gmail.com>
Message-ID: <CAPQaxLP+QFWgsvDeLeQP=BFdstttAo-4r7ZdMrQs7kS+-QQSsw@mail.gmail.com>

Caitlin,

  I tried your command in both RGui and RStudio but both came up as errors.
I believe I made a mistake somewhere I labeling/downloading the files,
which is the source of the confusion in R. I will re-examine the files
saved on my desktop to determine the error. Regardless, would it be better
to use a read.table or read.csv function when attempting to download my
datasets? I tried using read.xl on RStudio as this process seemed much
easier, however, it would seem that my proclivity to error prevents such.

Best,

Spencer

On Wed, Dec 26, 2018 at 11:55 PM Caitlin Gibbons <bioprogrammer at gmail.com>
wrote:

> Does this help Spencer? The read.delim() function assumes a tab character
> by default, but I specifically included it using the read.csv function. The
> downloaded file is NOT an Excel file so this should help.
>
> GBM_protein_expression <- read.csv("C:/Users/Spencer/Desktop/GBM
> protein_expression.tsv", sep=?\t?)
>
> Sent from my iPhone
>
> > On Dec 26, 2018, at 9:23 PM, Richard M. Heiberger <rmh at temple.edu>
> wrote:
> >
> > this is wrong because the file is a csv file.  read_excel is designed
> > for xls files.
> > GBM_protein_expression <- read_excel("C:/Users/Spencer/Desktop/GBM
> > protein_expression.csv")
> >
> > How did you get a csv? it downloads as tsv.
> >
> > the statement you should use is in base, no library() statement is
> needed.
> >
> > GBM_protein_expression <- read.delim("C:/Users/Spencer/Desktop/GBM
> > protein_expression.csv")
> >
> > read.delim is the same as read.csv except that it sets the sep
> > argument to "\t".
> >
> >
> >
> > On Wed, Dec 26, 2018 at 11:11 PM Spencer Brackett
> > <spbrackett20 at saintjosephhs.com> wrote:
> >>
> >> Sorry, my mistake.
> >>
> >> So I could still use read.table and should I try using a .txt version of
> >> the file to avoid the silent changes you described?
> >>
> >> Also, when I tried to simply this process by downloading the dataset
> onto
> >> RStudio opposed to R (Gui) I received the following...
> >> library(readxl)
> >>> GBM_protein_expression <- read_excel("C:/Users/Spencer/Desktop/GBM
> >> protein_expression.csv")
> >> Error: Can't establish that the input is either xls or xlsx.
> >>> View(GBM_protein_expression)
> >> Error in View : object 'GBM_protein_expression' not found
> >> Error in gzfile(file, mode) : cannot open the connection
> >> In addition: Warning message:
> >> In gzfile(file, mode) :
> >>  cannot open compressed file
> >> 'C:/Users/Spencer/AppData/Local/Temp/RtmpQNQrMh/input147c61fc5b52.rds',
> >> probable reason 'No such file or directory'
> >>> library(readxl)
> >>> GBM_protein_expression <-
> >> read_excel("C:/Users/Spencer/Desktop/GBM_protein_ expression.xlsx")
> >> readxl works best with a newer version of the tibble package.
> >> You currently have tibble v1.4.2.
> >> Falling back to column name repair from tibble <= v1.4.2.
> >> Message displays once per session.
> >>> View(GBM_protein_expression)
> >>
> >>
> >> Is this perhaps the result of lack of preview (which I did not complete
> at
> >> the time I hit import as the preview failed to load), or the fact that
> the
> >> excel file itself contains no numerical data, but only TRUE or FALSE
> >> entries?
> >>
> >> On Wed, Dec 26, 2018 at 10:59 PM Jeff Newmiller <
> jdnewmil at dcn.davis.ca.us>
> >> wrote:
> >>
> >>> Please always reply-all to keep the list involved.
> >>>
> >>> If you used Save As to change the data format to Excel AND the file
> >>> extension to xlsx, then yes, you should be able to read with readxl. I
> >>> don't recommend it, though... Excel often changes data silently and in
> >>> irregularly located places in your file.
> >>>
> >>> On December 26, 2018 7:38:16 PM PST, Spencer Brackett <
> >>> spbrackett20 at saintjosephhs.com> wrote:
> >>>> So even if I imported the file form ICGC to my desktop as an excel
> >>>> file,
> >>>> and can view and saved the data as such, it is still a TSV?
> >>>>
> >>>> On Wed, Dec 26, 2018 at 10:35 PM Jeff Newmiller
> >>>> <jdnewmil at dcn.davis.ca.us>
> >>>> wrote:
> >>>>
> >>>>> CSV and TSV are not Excel files. Yes, I know Excel will open them,
> >>>> but
> >>>>> that does not make them Excel files.
> >>>>>
> >>>>> Read a TSV file with read.table or read.csv, setting the sep argument
> >>>> to
> >>>>> "\t".
> >>>>>
> >>>>> On December 26, 2018 7:26:35 PM PST, Spencer Brackett <
> >>>>> spbrackett20 at saintjosephhs.com> wrote:
> >>>>>> I tried importing the file without preview and recieved the
> >>>>>> following....
> >>>>>>
> >>>>>> library(readxl)
> >>>>>>> GBM_protein_expression <- read_excel("C:/Users/Spencer/Desktop/GBM
> >>>>>> protein_expression.csv")
> >>>>>> Error: Can't establish that the input is either xls or xlsx.
> >>>>>>> View(GBM_protein_expression)
> >>>>>> Error in View : object 'GBM_protein_expression' not found
> >>>>>> Error in gzfile(file, mode) : cannot open the connection
> >>>>>> In addition: Warning message:
> >>>>>> In gzfile(file, mode) :
> >>>>>> cannot open compressed file
> >>>>>
> >>>>>
> 'C:/Users/Spencer/AppData/Local/Temp/RtmpQNQrMh/input147c61fc5b52.rds',
> >>>>>> probable reason 'No such file or directory'
> >>>>>>> library(readxl)
> >>>>>>> GBM_protein_expression <-
> >>>>>> read_excel("C:/Users/Spencer/Desktop/GBM_protein_ expression.xlsx")
> >>>>>> readxl works best with a newer version of the tibble package.
> >>>>>> You currently have tibble v1.4.2.
> >>>>>> Falling back to column name repair from tibble <= v1.4.2.
> >>>>>> Message displays once per session.
> >>>>>>> View(GBM_protein_expression)
> >>>>>>
> >>>>>> Also, the area above my console says that no data is available in
> >>>> the
> >>>>>> table. Is this perhaps the result of lack of preview or the fact
> >>>> that
> >>>>>> the
> >>>>>> excel file itself contains no numerical data, but only TRUE or FALSE
> >>>>>> entries?
> >>>>>>
> >>>>>> On Wed, Dec 26, 2018 at 9:57 PM Spencer Brackett <
> >>>>>> spbrackett20 at saintjosephhs.com> wrote:
> >>>>>>
> >>>>>>> Hello again,
> >>>>>>>
> >>>>>>> I worked on directly downloading the file into R as was suggested,
> >>>>>> but
> >>>>>>> have thus far been unsuccessful. This is what  I generated on my
> >>>>>> second
> >>>>>>> attempt...
> >>>>>>>
> >>>>>>> GBM protein_expression<-(file.choose(), header=TRUE, sep="\t")
> >>>>>>> Error: unexpected symbol in "GBM protein_expression"
> >>>>>>>> GBM
> >>>>>>>
> >>>>>
> >>>
> >>>>>
> protein_expression<-(file.choose(GBM_protein_expression.xlsx),header=TRUE,
> >>>>>>> sep="\t")
> >>>>>>> Error: unexpected symbol in "GBM protein_expression"
> >>>>>>>>
> >>>>>>>
> >>>>>>> What part of the argument is in error?
> >>>>>>>
> >>>>>>> Also I tried importing the dataset as an excel file on RStudio to
> >>>> see
> >>>>>> if I
> >>>>>>> could solve my problem that way. However, my imported excel file
> >>>> has
> >>>>>> been
> >>>>>>> stuck in the 'retrieving preview data' and no data is appearing.
> >>>> Is
> >>>>>> the
> >>>>>>> data file prehaps too large or in the wrong format?
> >>>>>>>
> >>>>>>>
> >>>>>>>
> >>>>>>> On Wed, Dec 26, 2018 at 6:42 PM Spencer Brackett <
> >>>>>>> spbrackett20 at saintjosephhs.com> wrote:
> >>>>>>>
> >>>>>>>> Mr. Heiberger,
> >>>>>>>>
> >>>>>>>> Thank you for the insight! I will try out suggestion.
> >>>>>>>>
> >>>>>>>> Best,
> >>>>>>>>
> >>>>>>>> Spencer Brackett
> >>>>>>>>
> >>>>>>>> On Wed, Dec 26, 2018 at 6:34 PM Richard M. Heiberger
> >>>>>> <rmh at temple.edu>
> >>>>>>>> wrote:
> >>>>>>>>
> >>>>>>>>> I looked at the first file.  It gives an option to download as
> >>>> TSV
> >>>>>>>>> (tab separated values).
> >>>>>>>>> That is the same as CSV except with tabs instead of commas.
> >>>>>>>>> You do not need any external software to read it.  Read the
> >>>>>> downloaded
> >>>>>>>>> file directly into R.
> >>>>>>>>>
> >>>>>>>>> read.delim looks as if it would work directly on the downloaded
> >>>>>> file.
> >>>>>>>>> ?read.delim
> >>>>>>>>> The notation "\t" means the tab character.
> >>>>>>>>>
> >>>>>>>>> As an aside, stay away from notepad. it is too naive for almost
> >>>>>>>>> anything interesting.
> >>>>>>>>> The specific case I often see is people reading linux-style text
> >>>>>> files
> >>>>>>>>> with notepad, which doesn't
> >>>>>>>>> understand NL terminated lines.  nicely formatted text files
> >>>> become
> >>>>>>>>> illegible.
> >>>>>>>>>
> >>>>>>>>> On Wed, Dec 26, 2018 at 6:04 PM Spencer Brackett
> >>>>>>>>> <spbrackett20 at saintjosephhs.com> wrote:
> >>>>>>>>>>
> >>>>>>>>>> Good evening,
> >>>>>>>>>>
> >>>>>>>>>> I am attempting to anaylze the protein expression data
> >>>> contained
> >>>>>> within
> >>>>>>>>>> these two ICGC, TCGA datasets (one for GBM and the other for
> >>>> LGG)
> >>>>>>>>>>
> >>>>>>>>>> *File for GBM  protein expression*:
> >>>>>>>>>>
> >>>>>>>>>
> >>>>>>
> >>>>>
> >>>>
> >>>
> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22GBM-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
> >>>>>>>>>>
> >>>>>>>>>> *File for LGG protein expression:*
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> *
> >>>>>>>>>
> >>>>>>
> >>>>>
> >>>>
> >>>
> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
> >>>>>>>>>> <
> >>>>>>>>>
> >>>>>>
> >>>>>
> >>>>
> >>>
> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
> >>>>>>>>>> *
> >>>>>>>>>>
> >>>>>>>>>>  When I tried to transfer the files from .txt (via Notepad)
> >>>> to
> >>>>>> .csv
> >>>>>>>>> (via
> >>>>>>>>>> Excel), the data appeared in the columns as unorganized and
> >>>>>> random
> >>>>>>>>>> script... not like how a typical csv should be arranged at
> >>>> all. I
> >>>>>> need
> >>>>>>>>> the
> >>>>>>>>>> dataset to be converted into .csv in order to analyze it in R,
> >>>>>> which
> >>>>>>>>> is why
> >>>>>>>>>> I am hoping someone here might help me in doing that. If not,
> >>>> is
> >>>>>> there
> >>>>>>>>>> perhaps some other way that I could analyze the datatsets on
> >>>> R,
> >>>>>> which
> >>>>>>>>> again
> >>>>>>>>>> is downloaded from the dataportal ICGC?
> >>>>>>>>>>
> >>>>>>>>>> Best,
> >>>>>>>>>>
> >>>>>>>>>> Spencer Brackett
> >>>>>>>>>>
> >>>>>>>>>>        [[alternative HTML version deleted]]
> >>>>>>>>>>
> >>>>>>>>>> ______________________________________________
> >>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >>>> see
> >>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>>> PLEASE do read the posting guide
> >>>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>>> and provide commented, minimal, self-contained, reproducible
> >>>>>> code.
> >>>>>>>>>
> >>>>>>>>
> >>>>>>
> >>>>>>      [[alternative HTML version deleted]]
> >>>>>>
> >>>>>> ______________________________________________
> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>> PLEASE do read the posting guide
> >>>>>> http://www.R-project.org/posting-guide.html
> >>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>
> >>>>> --
> >>>>> Sent from my phone. Please excuse my brevity.
> >>>>>
> >>>
> >>> --
> >>> Sent from my phone. Please excuse my brevity.
> >>>
> >>
> >>        [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @pbr@ckett20 @ending from @@intjo@ephh@@com  Thu Dec 27 06:14:11 2018
From: @pbr@ckett20 @ending from @@intjo@ephh@@com (Spencer Brackett)
Date: Thu, 27 Dec 2018 00:14:11 -0500
Subject: [R] Fwd: UPDATE
In-Reply-To: <CAPQaxLP+QFWgsvDeLeQP=BFdstttAo-4r7ZdMrQs7kS+-QQSsw@mail.gmail.com>
References: <CAPQaxLN2t=y8t_NgWpSUU_erg=44GC=8+PedewUq7OYabx7jjg@mail.gmail.com>
 <CAGx1TMAnF7V7jEig0mGUgciYY70FfufCuV8GzU0TnPOLOYhEfw@mail.gmail.com>
 <CAPQaxLPbFTFZyTGahAH0_j=06mfovgN4mAcMEQJo87BoGRmRjQ@mail.gmail.com>
 <CAPQaxLPmCfXHjJgXbV98kvNfHEGPbqKB9a-LiVNTWXu0_QgwSw@mail.gmail.com>
 <CAPQaxLNVU86dmGPdiz43g1iw3as+LR=TjzgjGQnFF_xju5_76g@mail.gmail.com>
 <CAPQaxLPmeKTuMjtU4ENGAc5gyfoQSEcMkCPFHK4czyFvV0N+CA@mail.gmail.com>
 <CAPQaxLP+PFjbX9200g9oi8Xk2-2Rhvfqj=PGmLBMnO-LOBBXbA@mail.gmail.com>
 <D3662A59-E568-4A7F-9665-D24F42865BB2@dcn.davis.ca.us>
 <CAPQaxLMG1oF-79iyReu3W-L6ZiE9H8t1dkxmsDL4N4MawphW+w@mail.gmail.com>
 <BAAF4725-B100-4B1C-814B-C5A2C0985947@dcn.davis.ca.us>
 <CAPQaxLO15X39qio1u_OXkO6o5m0M55F8d==suNVf0eER+Z494A@mail.gmail.com>
 <CAGx1TMB=_g2oF8Xiu-OzNWe7k4SjYkMUOTnQWvO7YHghqoAgMw@mail.gmail.com>
 <DF19FBD9-25AD-48D5-A0D8-BB4D9F9D9C87@gmail.com>
 <CAPQaxLP+QFWgsvDeLeQP=BFdstttAo-4r7ZdMrQs7kS+-QQSsw@mail.gmail.com>
Message-ID: <CAPQaxLN5yWnoa+YPOd1t4TKJUNyCibXFrkek1HhkC51PmOCXPw@mail.gmail.com>

Follow up,

Would read.txt also work, as I am certain that I have both datasets in .txt
files? As to a previous users question concern the .csv nature of the
supposed excel file, I am uncertain as to how this was translated as such.
The file is most certainly in excel.


On Thu, Dec 27, 2018 at 12:10 AM Spencer Brackett <
spbrackett20 at saintjosephhs.com> wrote:

> Caitlin,
>
>   I tried your command in both RGui and RStudio but both came up as
> errors. I believe I made a mistake somewhere I labeling/downloading the
> files, which is the source of the confusion in R. I will re-examine the
> files saved on my desktop to determine the error. Regardless, would it be
> better to use a read.table or read.csv function when attempting to download
> my datasets? I tried using read.xl on RStudio as this process seemed much
> easier, however, it would seem that my proclivity to error prevents such.
>
> Best,
>
> Spencer
>
> On Wed, Dec 26, 2018 at 11:55 PM Caitlin Gibbons <bioprogrammer at gmail.com>
> wrote:
>
>> Does this help Spencer? The read.delim() function assumes a tab character
>> by default, but I specifically included it using the read.csv function. The
>> downloaded file is NOT an Excel file so this should help.
>>
>> GBM_protein_expression <- read.csv("C:/Users/Spencer/Desktop/GBM
>> protein_expression.tsv", sep=?\t?)
>>
>> Sent from my iPhone
>>
>> > On Dec 26, 2018, at 9:23 PM, Richard M. Heiberger <rmh at temple.edu>
>> wrote:
>> >
>> > this is wrong because the file is a csv file.  read_excel is designed
>> > for xls files.
>> > GBM_protein_expression <- read_excel("C:/Users/Spencer/Desktop/GBM
>> > protein_expression.csv")
>> >
>> > How did you get a csv? it downloads as tsv.
>> >
>> > the statement you should use is in base, no library() statement is
>> needed.
>> >
>> > GBM_protein_expression <- read.delim("C:/Users/Spencer/Desktop/GBM
>> > protein_expression.csv")
>> >
>> > read.delim is the same as read.csv except that it sets the sep
>> > argument to "\t".
>> >
>> >
>> >
>> > On Wed, Dec 26, 2018 at 11:11 PM Spencer Brackett
>> > <spbrackett20 at saintjosephhs.com> wrote:
>> >>
>> >> Sorry, my mistake.
>> >>
>> >> So I could still use read.table and should I try using a .txt version
>> of
>> >> the file to avoid the silent changes you described?
>> >>
>> >> Also, when I tried to simply this process by downloading the dataset
>> onto
>> >> RStudio opposed to R (Gui) I received the following...
>> >> library(readxl)
>> >>> GBM_protein_expression <- read_excel("C:/Users/Spencer/Desktop/GBM
>> >> protein_expression.csv")
>> >> Error: Can't establish that the input is either xls or xlsx.
>> >>> View(GBM_protein_expression)
>> >> Error in View : object 'GBM_protein_expression' not found
>> >> Error in gzfile(file, mode) : cannot open the connection
>> >> In addition: Warning message:
>> >> In gzfile(file, mode) :
>> >>  cannot open compressed file
>> >> 'C:/Users/Spencer/AppData/Local/Temp/RtmpQNQrMh/input147c61fc5b52.rds',
>> >> probable reason 'No such file or directory'
>> >>> library(readxl)
>> >>> GBM_protein_expression <-
>> >> read_excel("C:/Users/Spencer/Desktop/GBM_protein_ expression.xlsx")
>> >> readxl works best with a newer version of the tibble package.
>> >> You currently have tibble v1.4.2.
>> >> Falling back to column name repair from tibble <= v1.4.2.
>> >> Message displays once per session.
>> >>> View(GBM_protein_expression)
>> >>
>> >>
>> >> Is this perhaps the result of lack of preview (which I did not
>> complete at
>> >> the time I hit import as the preview failed to load), or the fact that
>> the
>> >> excel file itself contains no numerical data, but only TRUE or FALSE
>> >> entries?
>> >>
>> >> On Wed, Dec 26, 2018 at 10:59 PM Jeff Newmiller <
>> jdnewmil at dcn.davis.ca.us>
>> >> wrote:
>> >>
>> >>> Please always reply-all to keep the list involved.
>> >>>
>> >>> If you used Save As to change the data format to Excel AND the file
>> >>> extension to xlsx, then yes, you should be able to read with readxl. I
>> >>> don't recommend it, though... Excel often changes data silently and in
>> >>> irregularly located places in your file.
>> >>>
>> >>> On December 26, 2018 7:38:16 PM PST, Spencer Brackett <
>> >>> spbrackett20 at saintjosephhs.com> wrote:
>> >>>> So even if I imported the file form ICGC to my desktop as an excel
>> >>>> file,
>> >>>> and can view and saved the data as such, it is still a TSV?
>> >>>>
>> >>>> On Wed, Dec 26, 2018 at 10:35 PM Jeff Newmiller
>> >>>> <jdnewmil at dcn.davis.ca.us>
>> >>>> wrote:
>> >>>>
>> >>>>> CSV and TSV are not Excel files. Yes, I know Excel will open them,
>> >>>> but
>> >>>>> that does not make them Excel files.
>> >>>>>
>> >>>>> Read a TSV file with read.table or read.csv, setting the sep
>> argument
>> >>>> to
>> >>>>> "\t".
>> >>>>>
>> >>>>> On December 26, 2018 7:26:35 PM PST, Spencer Brackett <
>> >>>>> spbrackett20 at saintjosephhs.com> wrote:
>> >>>>>> I tried importing the file without preview and recieved the
>> >>>>>> following....
>> >>>>>>
>> >>>>>> library(readxl)
>> >>>>>>> GBM_protein_expression <- read_excel("C:/Users/Spencer/Desktop/GBM
>> >>>>>> protein_expression.csv")
>> >>>>>> Error: Can't establish that the input is either xls or xlsx.
>> >>>>>>> View(GBM_protein_expression)
>> >>>>>> Error in View : object 'GBM_protein_expression' not found
>> >>>>>> Error in gzfile(file, mode) : cannot open the connection
>> >>>>>> In addition: Warning message:
>> >>>>>> In gzfile(file, mode) :
>> >>>>>> cannot open compressed file
>> >>>>>
>> >>>>>
>> 'C:/Users/Spencer/AppData/Local/Temp/RtmpQNQrMh/input147c61fc5b52.rds',
>> >>>>>> probable reason 'No such file or directory'
>> >>>>>>> library(readxl)
>> >>>>>>> GBM_protein_expression <-
>> >>>>>> read_excel("C:/Users/Spencer/Desktop/GBM_protein_ expression.xlsx")
>> >>>>>> readxl works best with a newer version of the tibble package.
>> >>>>>> You currently have tibble v1.4.2.
>> >>>>>> Falling back to column name repair from tibble <= v1.4.2.
>> >>>>>> Message displays once per session.
>> >>>>>>> View(GBM_protein_expression)
>> >>>>>>
>> >>>>>> Also, the area above my console says that no data is available in
>> >>>> the
>> >>>>>> table. Is this perhaps the result of lack of preview or the fact
>> >>>> that
>> >>>>>> the
>> >>>>>> excel file itself contains no numerical data, but only TRUE or
>> FALSE
>> >>>>>> entries?
>> >>>>>>
>> >>>>>> On Wed, Dec 26, 2018 at 9:57 PM Spencer Brackett <
>> >>>>>> spbrackett20 at saintjosephhs.com> wrote:
>> >>>>>>
>> >>>>>>> Hello again,
>> >>>>>>>
>> >>>>>>> I worked on directly downloading the file into R as was suggested,
>> >>>>>> but
>> >>>>>>> have thus far been unsuccessful. This is what  I generated on my
>> >>>>>> second
>> >>>>>>> attempt...
>> >>>>>>>
>> >>>>>>> GBM protein_expression<-(file.choose(), header=TRUE, sep="\t")
>> >>>>>>> Error: unexpected symbol in "GBM protein_expression"
>> >>>>>>>> GBM
>> >>>>>>>
>> >>>>>
>> >>>
>> >>>>>
>> protein_expression<-(file.choose(GBM_protein_expression.xlsx),header=TRUE,
>> >>>>>>> sep="\t")
>> >>>>>>> Error: unexpected symbol in "GBM protein_expression"
>> >>>>>>>>
>> >>>>>>>
>> >>>>>>> What part of the argument is in error?
>> >>>>>>>
>> >>>>>>> Also I tried importing the dataset as an excel file on RStudio to
>> >>>> see
>> >>>>>> if I
>> >>>>>>> could solve my problem that way. However, my imported excel file
>> >>>> has
>> >>>>>> been
>> >>>>>>> stuck in the 'retrieving preview data' and no data is appearing.
>> >>>> Is
>> >>>>>> the
>> >>>>>>> data file prehaps too large or in the wrong format?
>> >>>>>>>
>> >>>>>>>
>> >>>>>>>
>> >>>>>>> On Wed, Dec 26, 2018 at 6:42 PM Spencer Brackett <
>> >>>>>>> spbrackett20 at saintjosephhs.com> wrote:
>> >>>>>>>
>> >>>>>>>> Mr. Heiberger,
>> >>>>>>>>
>> >>>>>>>> Thank you for the insight! I will try out suggestion.
>> >>>>>>>>
>> >>>>>>>> Best,
>> >>>>>>>>
>> >>>>>>>> Spencer Brackett
>> >>>>>>>>
>> >>>>>>>> On Wed, Dec 26, 2018 at 6:34 PM Richard M. Heiberger
>> >>>>>> <rmh at temple.edu>
>> >>>>>>>> wrote:
>> >>>>>>>>
>> >>>>>>>>> I looked at the first file.  It gives an option to download as
>> >>>> TSV
>> >>>>>>>>> (tab separated values).
>> >>>>>>>>> That is the same as CSV except with tabs instead of commas.
>> >>>>>>>>> You do not need any external software to read it.  Read the
>> >>>>>> downloaded
>> >>>>>>>>> file directly into R.
>> >>>>>>>>>
>> >>>>>>>>> read.delim looks as if it would work directly on the downloaded
>> >>>>>> file.
>> >>>>>>>>> ?read.delim
>> >>>>>>>>> The notation "\t" means the tab character.
>> >>>>>>>>>
>> >>>>>>>>> As an aside, stay away from notepad. it is too naive for almost
>> >>>>>>>>> anything interesting.
>> >>>>>>>>> The specific case I often see is people reading linux-style text
>> >>>>>> files
>> >>>>>>>>> with notepad, which doesn't
>> >>>>>>>>> understand NL terminated lines.  nicely formatted text files
>> >>>> become
>> >>>>>>>>> illegible.
>> >>>>>>>>>
>> >>>>>>>>> On Wed, Dec 26, 2018 at 6:04 PM Spencer Brackett
>> >>>>>>>>> <spbrackett20 at saintjosephhs.com> wrote:
>> >>>>>>>>>>
>> >>>>>>>>>> Good evening,
>> >>>>>>>>>>
>> >>>>>>>>>> I am attempting to anaylze the protein expression data
>> >>>> contained
>> >>>>>> within
>> >>>>>>>>>> these two ICGC, TCGA datasets (one for GBM and the other for
>> >>>> LGG)
>> >>>>>>>>>>
>> >>>>>>>>>> *File for GBM  protein expression*:
>> >>>>>>>>>>
>> >>>>>>>>>
>> >>>>>>
>> >>>>>
>> >>>>
>> >>>
>> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22GBM-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>> >>>>>>>>>>
>> >>>>>>>>>> *File for LGG protein expression:*
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>> *
>> >>>>>>>>>
>> >>>>>>
>> >>>>>
>> >>>>
>> >>>
>> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>> >>>>>>>>>> <
>> >>>>>>>>>
>> >>>>>>
>> >>>>>
>> >>>>
>> >>>
>> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>> >>>>>>>>>> *
>> >>>>>>>>>>
>> >>>>>>>>>>  When I tried to transfer the files from .txt (via Notepad)
>> >>>> to
>> >>>>>> .csv
>> >>>>>>>>> (via
>> >>>>>>>>>> Excel), the data appeared in the columns as unorganized and
>> >>>>>> random
>> >>>>>>>>>> script... not like how a typical csv should be arranged at
>> >>>> all. I
>> >>>>>> need
>> >>>>>>>>> the
>> >>>>>>>>>> dataset to be converted into .csv in order to analyze it in R,
>> >>>>>> which
>> >>>>>>>>> is why
>> >>>>>>>>>> I am hoping someone here might help me in doing that. If not,
>> >>>> is
>> >>>>>> there
>> >>>>>>>>>> perhaps some other way that I could analyze the datatsets on
>> >>>> R,
>> >>>>>> which
>> >>>>>>>>> again
>> >>>>>>>>>> is downloaded from the dataportal ICGC?
>> >>>>>>>>>>
>> >>>>>>>>>> Best,
>> >>>>>>>>>>
>> >>>>>>>>>> Spencer Brackett
>> >>>>>>>>>>
>> >>>>>>>>>>        [[alternative HTML version deleted]]
>> >>>>>>>>>>
>> >>>>>>>>>> ______________________________________________
>> >>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>> >>>> see
>> >>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>>>>>>>> PLEASE do read the posting guide
>> >>>>>>>>> http://www.R-project.org/posting-guide.html
>> >>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>> >>>>>> code.
>> >>>>>>>>>
>> >>>>>>>>
>> >>>>>>
>> >>>>>>      [[alternative HTML version deleted]]
>> >>>>>>
>> >>>>>> ______________________________________________
>> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>>>> PLEASE do read the posting guide
>> >>>>>> http://www.R-project.org/posting-guide.html
>> >>>>>> and provide commented, minimal, self-contained, reproducible code.
>> >>>>>
>> >>>>> --
>> >>>>> Sent from my phone. Please excuse my brevity.
>> >>>>>
>> >>>
>> >>> --
>> >>> Sent from my phone. Please excuse my brevity.
>> >>>
>> >>
>> >>        [[alternative HTML version deleted]]
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From bioprogr@mmer @ending from gm@il@com  Thu Dec 27 06:51:46 2018
From: bioprogr@mmer @ending from gm@il@com (Caitlin)
Date: Wed, 26 Dec 2018 22:51:46 -0700
Subject: [R] Fwd: UPDATE
In-Reply-To: <CAPQaxLN5yWnoa+YPOd1t4TKJUNyCibXFrkek1HhkC51PmOCXPw@mail.gmail.com>
References: <CAPQaxLN2t=y8t_NgWpSUU_erg=44GC=8+PedewUq7OYabx7jjg@mail.gmail.com>
 <CAGx1TMAnF7V7jEig0mGUgciYY70FfufCuV8GzU0TnPOLOYhEfw@mail.gmail.com>
 <CAPQaxLPbFTFZyTGahAH0_j=06mfovgN4mAcMEQJo87BoGRmRjQ@mail.gmail.com>
 <CAPQaxLPmCfXHjJgXbV98kvNfHEGPbqKB9a-LiVNTWXu0_QgwSw@mail.gmail.com>
 <CAPQaxLNVU86dmGPdiz43g1iw3as+LR=TjzgjGQnFF_xju5_76g@mail.gmail.com>
 <CAPQaxLPmeKTuMjtU4ENGAc5gyfoQSEcMkCPFHK4czyFvV0N+CA@mail.gmail.com>
 <CAPQaxLP+PFjbX9200g9oi8Xk2-2Rhvfqj=PGmLBMnO-LOBBXbA@mail.gmail.com>
 <D3662A59-E568-4A7F-9665-D24F42865BB2@dcn.davis.ca.us>
 <CAPQaxLMG1oF-79iyReu3W-L6ZiE9H8t1dkxmsDL4N4MawphW+w@mail.gmail.com>
 <BAAF4725-B100-4B1C-814B-C5A2C0985947@dcn.davis.ca.us>
 <CAPQaxLO15X39qio1u_OXkO6o5m0M55F8d==suNVf0eER+Z494A@mail.gmail.com>
 <CAGx1TMB=_g2oF8Xiu-OzNWe7k4SjYkMUOTnQWvO7YHghqoAgMw@mail.gmail.com>
 <DF19FBD9-25AD-48D5-A0D8-BB4D9F9D9C87@gmail.com>
 <CAPQaxLP+QFWgsvDeLeQP=BFdstttAo-4r7ZdMrQs7kS+-QQSsw@mail.gmail.com>
 <CAPQaxLN5yWnoa+YPOd1t4TKJUNyCibXFrkek1HhkC51PmOCXPw@mail.gmail.com>
Message-ID: <CABDKo+ww101e0DWCGgXyDTDqxDzSZigkeHqpZKDLWhTbkn_MtA@mail.gmail.com>

Is the file being saved as .xls, .xlsx, .csv, .tsv, or .txt?


On Wed, Dec 26, 2018 at 10:14 PM Spencer Brackett <
spbrackett20 at saintjosephhs.com> wrote:

> Follow up,
>
> Would read.txt also work, as I am certain that I have both datasets in
> .txt files? As to a previous users question concern the .csv nature of the
> supposed excel file, I am uncertain as to how this was translated as such.
> The file is most certainly in excel.
>
>
> On Thu, Dec 27, 2018 at 12:10 AM Spencer Brackett <
> spbrackett20 at saintjosephhs.com> wrote:
>
>> Caitlin,
>>
>>   I tried your command in both RGui and RStudio but both came up as
>> errors. I believe I made a mistake somewhere I labeling/downloading the
>> files, which is the source of the confusion in R. I will re-examine the
>> files saved on my desktop to determine the error. Regardless, would it be
>> better to use a read.table or read.csv function when attempting to download
>> my datasets? I tried using read.xl on RStudio as this process seemed much
>> easier, however, it would seem that my proclivity to error prevents such.
>>
>> Best,
>>
>> Spencer
>>
>> On Wed, Dec 26, 2018 at 11:55 PM Caitlin Gibbons <bioprogrammer at gmail.com>
>> wrote:
>>
>>> Does this help Spencer? The read.delim() function assumes a tab
>>> character by default, but I specifically included it using the read.csv
>>> function. The downloaded file is NOT an Excel file so this should help.
>>>
>>> GBM_protein_expression <- read.csv("C:/Users/Spencer/Desktop/GBM
>>> protein_expression.tsv", sep=?\t?)
>>>
>>> Sent from my iPhone
>>>
>>> > On Dec 26, 2018, at 9:23 PM, Richard M. Heiberger <rmh at temple.edu>
>>> wrote:
>>> >
>>> > this is wrong because the file is a csv file.  read_excel is designed
>>> > for xls files.
>>> > GBM_protein_expression <- read_excel("C:/Users/Spencer/Desktop/GBM
>>> > protein_expression.csv")
>>> >
>>> > How did you get a csv? it downloads as tsv.
>>> >
>>> > the statement you should use is in base, no library() statement is
>>> needed.
>>> >
>>> > GBM_protein_expression <- read.delim("C:/Users/Spencer/Desktop/GBM
>>> > protein_expression.csv")
>>> >
>>> > read.delim is the same as read.csv except that it sets the sep
>>> > argument to "\t".
>>> >
>>> >
>>> >
>>> > On Wed, Dec 26, 2018 at 11:11 PM Spencer Brackett
>>> > <spbrackett20 at saintjosephhs.com> wrote:
>>> >>
>>> >> Sorry, my mistake.
>>> >>
>>> >> So I could still use read.table and should I try using a .txt version
>>> of
>>> >> the file to avoid the silent changes you described?
>>> >>
>>> >> Also, when I tried to simply this process by downloading the dataset
>>> onto
>>> >> RStudio opposed to R (Gui) I received the following...
>>> >> library(readxl)
>>> >>> GBM_protein_expression <- read_excel("C:/Users/Spencer/Desktop/GBM
>>> >> protein_expression.csv")
>>> >> Error: Can't establish that the input is either xls or xlsx.
>>> >>> View(GBM_protein_expression)
>>> >> Error in View : object 'GBM_protein_expression' not found
>>> >> Error in gzfile(file, mode) : cannot open the connection
>>> >> In addition: Warning message:
>>> >> In gzfile(file, mode) :
>>> >>  cannot open compressed file
>>> >>
>>> 'C:/Users/Spencer/AppData/Local/Temp/RtmpQNQrMh/input147c61fc5b52.rds',
>>> >> probable reason 'No such file or directory'
>>> >>> library(readxl)
>>> >>> GBM_protein_expression <-
>>> >> read_excel("C:/Users/Spencer/Desktop/GBM_protein_ expression.xlsx")
>>> >> readxl works best with a newer version of the tibble package.
>>> >> You currently have tibble v1.4.2.
>>> >> Falling back to column name repair from tibble <= v1.4.2.
>>> >> Message displays once per session.
>>> >>> View(GBM_protein_expression)
>>> >>
>>> >>
>>> >> Is this perhaps the result of lack of preview (which I did not
>>> complete at
>>> >> the time I hit import as the preview failed to load), or the fact
>>> that the
>>> >> excel file itself contains no numerical data, but only TRUE or FALSE
>>> >> entries?
>>> >>
>>> >> On Wed, Dec 26, 2018 at 10:59 PM Jeff Newmiller <
>>> jdnewmil at dcn.davis.ca.us>
>>> >> wrote:
>>> >>
>>> >>> Please always reply-all to keep the list involved.
>>> >>>
>>> >>> If you used Save As to change the data format to Excel AND the file
>>> >>> extension to xlsx, then yes, you should be able to read with readxl.
>>> I
>>> >>> don't recommend it, though... Excel often changes data silently and
>>> in
>>> >>> irregularly located places in your file.
>>> >>>
>>> >>> On December 26, 2018 7:38:16 PM PST, Spencer Brackett <
>>> >>> spbrackett20 at saintjosephhs.com> wrote:
>>> >>>> So even if I imported the file form ICGC to my desktop as an excel
>>> >>>> file,
>>> >>>> and can view and saved the data as such, it is still a TSV?
>>> >>>>
>>> >>>> On Wed, Dec 26, 2018 at 10:35 PM Jeff Newmiller
>>> >>>> <jdnewmil at dcn.davis.ca.us>
>>> >>>> wrote:
>>> >>>>
>>> >>>>> CSV and TSV are not Excel files. Yes, I know Excel will open them,
>>> >>>> but
>>> >>>>> that does not make them Excel files.
>>> >>>>>
>>> >>>>> Read a TSV file with read.table or read.csv, setting the sep
>>> argument
>>> >>>> to
>>> >>>>> "\t".
>>> >>>>>
>>> >>>>> On December 26, 2018 7:26:35 PM PST, Spencer Brackett <
>>> >>>>> spbrackett20 at saintjosephhs.com> wrote:
>>> >>>>>> I tried importing the file without preview and recieved the
>>> >>>>>> following....
>>> >>>>>>
>>> >>>>>> library(readxl)
>>> >>>>>>> GBM_protein_expression <-
>>> read_excel("C:/Users/Spencer/Desktop/GBM
>>> >>>>>> protein_expression.csv")
>>> >>>>>> Error: Can't establish that the input is either xls or xlsx.
>>> >>>>>>> View(GBM_protein_expression)
>>> >>>>>> Error in View : object 'GBM_protein_expression' not found
>>> >>>>>> Error in gzfile(file, mode) : cannot open the connection
>>> >>>>>> In addition: Warning message:
>>> >>>>>> In gzfile(file, mode) :
>>> >>>>>> cannot open compressed file
>>> >>>>>
>>> >>>>>
>>> 'C:/Users/Spencer/AppData/Local/Temp/RtmpQNQrMh/input147c61fc5b52.rds',
>>> >>>>>> probable reason 'No such file or directory'
>>> >>>>>>> library(readxl)
>>> >>>>>>> GBM_protein_expression <-
>>> >>>>>> read_excel("C:/Users/Spencer/Desktop/GBM_protein_
>>> expression.xlsx")
>>> >>>>>> readxl works best with a newer version of the tibble package.
>>> >>>>>> You currently have tibble v1.4.2.
>>> >>>>>> Falling back to column name repair from tibble <= v1.4.2.
>>> >>>>>> Message displays once per session.
>>> >>>>>>> View(GBM_protein_expression)
>>> >>>>>>
>>> >>>>>> Also, the area above my console says that no data is available in
>>> >>>> the
>>> >>>>>> table. Is this perhaps the result of lack of preview or the fact
>>> >>>> that
>>> >>>>>> the
>>> >>>>>> excel file itself contains no numerical data, but only TRUE or
>>> FALSE
>>> >>>>>> entries?
>>> >>>>>>
>>> >>>>>> On Wed, Dec 26, 2018 at 9:57 PM Spencer Brackett <
>>> >>>>>> spbrackett20 at saintjosephhs.com> wrote:
>>> >>>>>>
>>> >>>>>>> Hello again,
>>> >>>>>>>
>>> >>>>>>> I worked on directly downloading the file into R as was
>>> suggested,
>>> >>>>>> but
>>> >>>>>>> have thus far been unsuccessful. This is what  I generated on my
>>> >>>>>> second
>>> >>>>>>> attempt...
>>> >>>>>>>
>>> >>>>>>> GBM protein_expression<-(file.choose(), header=TRUE, sep="\t")
>>> >>>>>>> Error: unexpected symbol in "GBM protein_expression"
>>> >>>>>>>> GBM
>>> >>>>>>>
>>> >>>>>
>>> >>>
>>> >>>>>
>>> protein_expression<-(file.choose(GBM_protein_expression.xlsx),header=TRUE,
>>> >>>>>>> sep="\t")
>>> >>>>>>> Error: unexpected symbol in "GBM protein_expression"
>>> >>>>>>>>
>>> >>>>>>>
>>> >>>>>>> What part of the argument is in error?
>>> >>>>>>>
>>> >>>>>>> Also I tried importing the dataset as an excel file on RStudio to
>>> >>>> see
>>> >>>>>> if I
>>> >>>>>>> could solve my problem that way. However, my imported excel file
>>> >>>> has
>>> >>>>>> been
>>> >>>>>>> stuck in the 'retrieving preview data' and no data is appearing.
>>> >>>> Is
>>> >>>>>> the
>>> >>>>>>> data file prehaps too large or in the wrong format?
>>> >>>>>>>
>>> >>>>>>>
>>> >>>>>>>
>>> >>>>>>> On Wed, Dec 26, 2018 at 6:42 PM Spencer Brackett <
>>> >>>>>>> spbrackett20 at saintjosephhs.com> wrote:
>>> >>>>>>>
>>> >>>>>>>> Mr. Heiberger,
>>> >>>>>>>>
>>> >>>>>>>> Thank you for the insight! I will try out suggestion.
>>> >>>>>>>>
>>> >>>>>>>> Best,
>>> >>>>>>>>
>>> >>>>>>>> Spencer Brackett
>>> >>>>>>>>
>>> >>>>>>>> On Wed, Dec 26, 2018 at 6:34 PM Richard M. Heiberger
>>> >>>>>> <rmh at temple.edu>
>>> >>>>>>>> wrote:
>>> >>>>>>>>
>>> >>>>>>>>> I looked at the first file.  It gives an option to download as
>>> >>>> TSV
>>> >>>>>>>>> (tab separated values).
>>> >>>>>>>>> That is the same as CSV except with tabs instead of commas.
>>> >>>>>>>>> You do not need any external software to read it.  Read the
>>> >>>>>> downloaded
>>> >>>>>>>>> file directly into R.
>>> >>>>>>>>>
>>> >>>>>>>>> read.delim looks as if it would work directly on the downloaded
>>> >>>>>> file.
>>> >>>>>>>>> ?read.delim
>>> >>>>>>>>> The notation "\t" means the tab character.
>>> >>>>>>>>>
>>> >>>>>>>>> As an aside, stay away from notepad. it is too naive for almost
>>> >>>>>>>>> anything interesting.
>>> >>>>>>>>> The specific case I often see is people reading linux-style
>>> text
>>> >>>>>> files
>>> >>>>>>>>> with notepad, which doesn't
>>> >>>>>>>>> understand NL terminated lines.  nicely formatted text files
>>> >>>> become
>>> >>>>>>>>> illegible.
>>> >>>>>>>>>
>>> >>>>>>>>> On Wed, Dec 26, 2018 at 6:04 PM Spencer Brackett
>>> >>>>>>>>> <spbrackett20 at saintjosephhs.com> wrote:
>>> >>>>>>>>>>
>>> >>>>>>>>>> Good evening,
>>> >>>>>>>>>>
>>> >>>>>>>>>> I am attempting to anaylze the protein expression data
>>> >>>> contained
>>> >>>>>> within
>>> >>>>>>>>>> these two ICGC, TCGA datasets (one for GBM and the other for
>>> >>>> LGG)
>>> >>>>>>>>>>
>>> >>>>>>>>>> *File for GBM  protein expression*:
>>> >>>>>>>>>>
>>> >>>>>>>>>
>>> >>>>>>
>>> >>>>>
>>> >>>>
>>> >>>
>>> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22GBM-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>>> >>>>>>>>>>
>>> >>>>>>>>>> *File for LGG protein expression:*
>>> >>>>>>>>>>
>>> >>>>>>>>>>
>>> >>>>>>>>>> *
>>> >>>>>>>>>
>>> >>>>>>
>>> >>>>>
>>> >>>>
>>> >>>
>>> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>>> >>>>>>>>>> <
>>> >>>>>>>>>
>>> >>>>>>
>>> >>>>>
>>> >>>>
>>> >>>
>>> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>>> >>>>>>>>>> *
>>> >>>>>>>>>>
>>> >>>>>>>>>>  When I tried to transfer the files from .txt (via Notepad)
>>> >>>> to
>>> >>>>>> .csv
>>> >>>>>>>>> (via
>>> >>>>>>>>>> Excel), the data appeared in the columns as unorganized and
>>> >>>>>> random
>>> >>>>>>>>>> script... not like how a typical csv should be arranged at
>>> >>>> all. I
>>> >>>>>> need
>>> >>>>>>>>> the
>>> >>>>>>>>>> dataset to be converted into .csv in order to analyze it in R,
>>> >>>>>> which
>>> >>>>>>>>> is why
>>> >>>>>>>>>> I am hoping someone here might help me in doing that. If not,
>>> >>>> is
>>> >>>>>> there
>>> >>>>>>>>>> perhaps some other way that I could analyze the datatsets on
>>> >>>> R,
>>> >>>>>> which
>>> >>>>>>>>> again
>>> >>>>>>>>>> is downloaded from the dataportal ICGC?
>>> >>>>>>>>>>
>>> >>>>>>>>>> Best,
>>> >>>>>>>>>>
>>> >>>>>>>>>> Spencer Brackett
>>> >>>>>>>>>>
>>> >>>>>>>>>>        [[alternative HTML version deleted]]
>>> >>>>>>>>>>
>>> >>>>>>>>>> ______________________________________________
>>> >>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>> >>>> see
>>> >>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> >>>>>>>>>> PLEASE do read the posting guide
>>> >>>>>>>>> http://www.R-project.org/posting-guide.html
>>> >>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>> >>>>>> code.
>>> >>>>>>>>>
>>> >>>>>>>>
>>> >>>>>>
>>> >>>>>>      [[alternative HTML version deleted]]
>>> >>>>>>
>>> >>>>>> ______________________________________________
>>> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> >>>>>> PLEASE do read the posting guide
>>> >>>>>> http://www.R-project.org/posting-guide.html
>>> >>>>>> and provide commented, minimal, self-contained, reproducible code.
>>> >>>>>
>>> >>>>> --
>>> >>>>> Sent from my phone. Please excuse my brevity.
>>> >>>>>
>>> >>>
>>> >>> --
>>> >>> Sent from my phone. Please excuse my brevity.
>>> >>>
>>> >>
>>> >>        [[alternative HTML version deleted]]
>>> >>
>>> >> ______________________________________________
>>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>>> >> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> >> and provide commented, minimal, self-contained, reproducible code.
>>> >
>>> > ______________________________________________
>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>> > PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> > and provide commented, minimal, self-contained, reproducible code.
>>>
>>

	[[alternative HTML version deleted]]


From jwd @ending from @urewe@t@net  Thu Dec 27 06:32:40 2018
From: jwd @ending from @urewe@t@net (John)
Date: Wed, 26 Dec 2018 21:32:40 -0800
Subject: [R] =?utf-8?q?Problem_with_Kruskal=E2=80=93Wallis_test?=
In-Reply-To: <CAE43vpgKp0ThqqHm7YBB=XL66q3zUf=8f_XhgHPBMqsShgG1gQ@mail.gmail.com>
References: <CAE43vpgKp0ThqqHm7YBB=XL66q3zUf=8f_XhgHPBMqsShgG1gQ@mail.gmail.com>
Message-ID: <20181226213240.11c53436@Draco.localdomain>

On Fri, 21 Dec 2018 16:37:54 +0100
Giuseppe Cillis <giucillis at gmail.com> wrote:

> Dear all,
> I am a beginner with R (and also with the statistics) for which I
> hope to be clear.
> I should do this non-parametric test on data I extracted from maps.
> In practice I have a column that represents the landscape Dynamics of
> a certain time period (there are 3 dynamics, each of them marked by
> the number 1, 2 or 3) and the other column with the values of a
> topographic variable (for example the slope) . In all, there are more
> than 90,000 pairs of values.
> Going to do the test in R, for all the dynamics and for all the
> variables, I get out of the values of chi-square elevated (even in
> the order of thousands) and a p-value always <2.2e-16 .... why? Where
> can the error be? in the script or in the test approach?
> Thanks in advance
> 
Your question and your problem is not about R, but rather about why your
results do not match your expectations.  There are a lot of questions
you should be considering like, why use Kruskal-Wallis, before you even
start wondering about R?  

JWDougherty


From petr@pik@l @ending from prechez@@cz  Thu Dec 27 09:27:50 2018
From: petr@pik@l @ending from prechez@@cz (PIKAL Petr)
Date: Thu, 27 Dec 2018 08:27:50 +0000
Subject: [R] Help converting .txt to .csv file
In-Reply-To: <CAPQaxLPmCfXHjJgXbV98kvNfHEGPbqKB9a-LiVNTWXu0_QgwSw@mail.gmail.com>
References: <CAPQaxLN2t=y8t_NgWpSUU_erg=44GC=8+PedewUq7OYabx7jjg@mail.gmail.com>
 <CAGx1TMAnF7V7jEig0mGUgciYY70FfufCuV8GzU0TnPOLOYhEfw@mail.gmail.com>
 <CAPQaxLPbFTFZyTGahAH0_j=06mfovgN4mAcMEQJo87BoGRmRjQ@mail.gmail.com>
 <CAPQaxLPmCfXHjJgXbV98kvNfHEGPbqKB9a-LiVNTWXu0_QgwSw@mail.gmail.com>
Message-ID: <a598040e004f4f778efb153ae9faa9fe@SRVEXCHCM1302.precheza.cz>

Hi

See inline

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Spencer Brackett
> Sent: Thursday, December 27, 2018 3:57 AM
> To: Richard M. Heiberger <rmh at temple.edu>
> Cc: R-help <r-help at r-project.org>
> Subject: Re: [R] Help converting .txt to .csv file
>
> Hello again,
>
> I worked on directly downloading the file into R as was suggested, but have
> thus far been unsuccessful. This is what  I generated on my second attempt...
>
>  GBM protein_expression<-(file.choose(), header=TRUE, sep="\t")
> Error: unexpected symbol in "GBM protein_expression"
> > GBM

You forgot to add read.* function.

something like
protein_expression <- read.delim(file.choose())

or

protein_expression <- read.table(file.choose(), header=TRUE, sep="\t")

If your files are tab delimited as Richard suggested.

Cheers
Petr

> protein_expression<-(file.choose(GBM_protein_expression.xlsx),header=TRUE,
> sep="\t")
> Error: unexpected symbol in "GBM protein_expression"
> >
>
> What part of the argument is in error?
>
> Also I tried importing the dataset as an excel file on RStudio to see if I could
> solve my problem that way. However, my imported excel file has been stuck in
> the 'retrieving preview data' and no data is appearing. Is the data file prehaps
> too large or in the wrong format?
>
>
>
> On Wed, Dec 26, 2018 at 6:42 PM Spencer Brackett <
> spbrackett20 at saintjosephhs.com> wrote:
>
> > Mr. Heiberger,
> >
> >  Thank you for the insight! I will try out suggestion.
> >
> > Best,
> >
> > Spencer Brackett
> >
> > On Wed, Dec 26, 2018 at 6:34 PM Richard M. Heiberger <rmh at temple.edu>
> > wrote:
> >
> >> I looked at the first file.  It gives an option to download as TSV
> >> (tab separated values).
> >> That is the same as CSV except with tabs instead of commas.
> >> You do not need any external software to read it.  Read the
> >> downloaded file directly into R.
> >>
> >> read.delim looks as if it would work directly on the downloaded file.
> >> ?read.delim
> >> The notation "\t" means the tab character.
> >>
> >> As an aside, stay away from notepad. it is too naive for almost
> >> anything interesting.
> >> The specific case I often see is people reading linux-style text
> >> files with notepad, which doesn't understand NL terminated lines.
> >> nicely formatted text files become illegible.
> >>
> >> On Wed, Dec 26, 2018 at 6:04 PM Spencer Brackett
> >> <spbrackett20 at saintjosephhs.com> wrote:
> >> >
> >> > Good evening,
> >> >
> >> > I am attempting to anaylze the protein expression data contained
> >> > within these two ICGC, TCGA datasets (one for GBM and the other for
> >> > LGG)
> >> >
> >> > *File for GBM  protein expression*:
> >> >
> >> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22
> >> :%7B%22is%22:%5B%22GBM-
> US%22%5D%7D,%22availableDataTypes%22:%7B%22is%
> >> 22:%5B%22pexp%22%5D%7D%7D%7D
> >> >
> >> > *File for LGG protein expression:*
> >> >
> >> >
> >> > *
> >> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22
> >> :%7B%22is%22:%5B%22LGG-
> US%22%5D%7D,%22availableDataTypes%22:%7B%22is%
> >> 22:%5B%22pexp%22%5D%7D%7D%7D
> >> > <
> >> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22
> >> :%7B%22is%22:%5B%22LGG-
> US%22%5D%7D,%22availableDataTypes%22:%7B%22is%
> >> 22:%5B%22pexp%22%5D%7D%7D%7D
> >> >*
> >> >
> >> >   When I tried to transfer the files from .txt (via Notepad) to
> >> > .csv
> >> (via
> >> > Excel), the data appeared in the columns as unorganized and random
> >> > script... not like how a typical csv should be arranged at all. I
> >> > need
> >> the
> >> > dataset to be converted into .csv in order to analyze it in R,
> >> > which is
> >> why
> >> > I am hoping someone here might help me in doing that. If not, is
> >> > there perhaps some other way that I could analyze the datatsets on
> >> > R, which
> >> again
> >> > is downloaded from the dataportal ICGC?
> >> >
> >> > Best,
> >> >
> >> > Spencer Brackett
> >> >
> >> >         [[alternative HTML version deleted]]
> >> >
> >> > ______________________________________________
> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> > and provide commented, minimal, self-contained, reproducible code.
> >>
> >
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


From @pbr@ckett20 @ending from @@intjo@ephh@@com  Thu Dec 27 04:24:06 2018
From: @pbr@ckett20 @ending from @@intjo@ephh@@com (Spencer Brackett)
Date: Wed, 26 Dec 2018 22:24:06 -0500
Subject: [R] Fwd:  UPDATE
In-Reply-To: <CAPQaxLNVU86dmGPdiz43g1iw3as+LR=TjzgjGQnFF_xju5_76g@mail.gmail.com>
References: <CAPQaxLN2t=y8t_NgWpSUU_erg=44GC=8+PedewUq7OYabx7jjg@mail.gmail.com>
 <CAGx1TMAnF7V7jEig0mGUgciYY70FfufCuV8GzU0TnPOLOYhEfw@mail.gmail.com>
 <CAPQaxLPbFTFZyTGahAH0_j=06mfovgN4mAcMEQJo87BoGRmRjQ@mail.gmail.com>
 <CAPQaxLPmCfXHjJgXbV98kvNfHEGPbqKB9a-LiVNTWXu0_QgwSw@mail.gmail.com>
 <CAPQaxLNVU86dmGPdiz43g1iw3as+LR=TjzgjGQnFF_xju5_76g@mail.gmail.com>
Message-ID: <CAPQaxLPmeKTuMjtU4ENGAc5gyfoQSEcMkCPFHK4czyFvV0N+CA@mail.gmail.com>

I tried importing the file without preview and recieved the following....

library(readxl)
> GBM_protein_expression <- read_excel("C:/Users/Spencer/Desktop/GBM
protein_expression.csv")
Error: Can't establish that the input is either xls or xlsx.
> View(GBM_protein_expression)
Error in View : object 'GBM_protein_expression' not found
Error in gzfile(file, mode) : cannot open the connection
In addition: Warning message:
In gzfile(file, mode) :
  cannot open compressed file
'C:/Users/Spencer/AppData/Local/Temp/RtmpQNQrMh/input147c61fc5b52.rds',
probable reason 'No such file or directory'
> library(readxl)
> GBM_protein_expression <-
read_excel("C:/Users/Spencer/Desktop/GBM_protein_ expression.xlsx")
readxl works best with a newer version of the tibble package.
You currently have tibble v1.4.2.
Falling back to column name repair from tibble <= v1.4.2.
Message displays once per session.
> View(GBM_protein_expression)

Also, the area above my console says that no data is available in the
table. Is this perhaps the result of lack of preview or the fact that the
excel file itself contains no numerical data, but only TRUE or FALSE
entries?

On Wed, Dec 26, 2018 at 9:57 PM Spencer Brackett <
spbrackett20 at saintjosephhs.com> wrote:

> Hello again,
>
> I worked on directly downloading the file into R as was suggested, but
> have thus far been unsuccessful. This is what  I generated on my second
> attempt...
>
>  GBM protein_expression<-(file.choose(), header=TRUE, sep="\t")
> Error: unexpected symbol in "GBM protein_expression"
> > GBM
> protein_expression<-(file.choose(GBM_protein_expression.xlsx),header=TRUE,
> sep="\t")
> Error: unexpected symbol in "GBM protein_expression"
> >
>
> What part of the argument is in error?
>
> Also I tried importing the dataset as an excel file on RStudio to see if I
> could solve my problem that way. However, my imported excel file has been
> stuck in the 'retrieving preview data' and no data is appearing. Is the
> data file prehaps too large or in the wrong format?
>
>
>
> On Wed, Dec 26, 2018 at 6:42 PM Spencer Brackett <
> spbrackett20 at saintjosephhs.com> wrote:
>
>> Mr. Heiberger,
>>
>>  Thank you for the insight! I will try out suggestion.
>>
>> Best,
>>
>> Spencer Brackett
>>
>> On Wed, Dec 26, 2018 at 6:34 PM Richard M. Heiberger <rmh at temple.edu>
>> wrote:
>>
>>> I looked at the first file.  It gives an option to download as TSV
>>> (tab separated values).
>>> That is the same as CSV except with tabs instead of commas.
>>> You do not need any external software to read it.  Read the downloaded
>>> file directly into R.
>>>
>>> read.delim looks as if it would work directly on the downloaded file.
>>> ?read.delim
>>> The notation "\t" means the tab character.
>>>
>>> As an aside, stay away from notepad. it is too naive for almost
>>> anything interesting.
>>> The specific case I often see is people reading linux-style text files
>>> with notepad, which doesn't
>>> understand NL terminated lines.  nicely formatted text files become
>>> illegible.
>>>
>>> On Wed, Dec 26, 2018 at 6:04 PM Spencer Brackett
>>> <spbrackett20 at saintjosephhs.com> wrote:
>>> >
>>> > Good evening,
>>> >
>>> > I am attempting to anaylze the protein expression data contained within
>>> > these two ICGC, TCGA datasets (one for GBM and the other for LGG)
>>> >
>>> > *File for GBM  protein expression*:
>>> >
>>> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22GBM-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>>> >
>>> > *File for LGG protein expression:*
>>> >
>>> >
>>> > *
>>> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>>> > <
>>> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22LGG-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>>> >*
>>> >
>>> >   When I tried to transfer the files from .txt (via Notepad) to .csv
>>> (via
>>> > Excel), the data appeared in the columns as unorganized and random
>>> > script... not like how a typical csv should be arranged at all. I need
>>> the
>>> > dataset to be converted into .csv in order to analyze it in R, which
>>> is why
>>> > I am hoping someone here might help me in doing that. If not, is there
>>> > perhaps some other way that I could analyze the datatsets on R, which
>>> again
>>> > is downloaded from the dataportal ICGC?
>>> >
>>> > Best,
>>> >
>>> > Spencer Brackett
>>> >
>>> >         [[alternative HTML version deleted]]
>>> >
>>> > ______________________________________________
>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>> > PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> > and provide commented, minimal, self-contained, reproducible code.
>>>
>>

From @pbr@ckett20 @ending from @@intjo@ephh@@com  Thu Dec 27 20:02:59 2018
From: @pbr@ckett20 @ending from @@intjo@ephh@@com (Spencer Brackett)
Date: Thu, 27 Dec 2018 14:02:59 -0500
Subject: [R] you are making it far too difficult
In-Reply-To: <C727FD36-8DBD-4BC1-8F92-E60B1E05F03C@kuleuven.be>
References: <C727FD36-8DBD-4BC1-8F92-E60B1E05F03C@kuleuven.be>
Message-ID: <CAPQaxLOgDhzX_njV_ukp4POR6PV89DiCjvMNMMwfj+cLsBFEmA@mail.gmail.com>

Thank you for the help! I tried using the read.table command in my RStudio
using the following argument, and managed to open the file.

GBM_protein_expression<-read.table(file.choose(), header=TRUE, sep=?/t?)

However, my data did not unpack as yours did. I again only received a table
of true and flase distinctions per column, and my environment tab says that
there is 0 observations upon 0 variables.

I believe I should be getting data similar to what you got, as it would
appear that your?s actually contains relevant gene/protein expression info.

On Thu, Dec 27, 2018 at 6:21 AM Federico Calboli <
federico.calboli at kuleuven.be> wrote:

> Once you have your TSV files just use something as
>
> x = read.table('protein_expression.tsv', h = T, sep = '\t')
>
> Do not copy paste the code of this email because it is formatted and would
> not work in R.
>
>
> Best
>
> F
>
> PS the data looks like this to me
>
> head(x)
>   icgc_donor_id project_code icgc_specimen_id icgc_sample_id
>  submitted_sample_id analysis_id        antibody_id gene_name
> 1       DO12370       GBM-US          SP26475       SA131594
> TCGA-19-5960-01A-13-1900-20       97765 14-3-3_epsilon-M-C     YWHAE
> 2       DO12370       GBM-US          SP26475       SA131594
> TCGA-19-5960-01A-13-1900-20       97765         4E-BP1-R-V  EIF4EBP1
> 3       DO12370       GBM-US          SP26475       SA131594
> TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pS65-R-V  EIF4EBP1
> 4       DO12370       GBM-US          SP26475       SA131594
> TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pT37-R-V  EIF4EBP1
> 5       DO12370       GBM-US          SP26475       SA131594
> TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pT70-R-C  EIF4EBP1
> 6       DO12370       GBM-US          SP26475       SA131594
> TCGA-19-5960-01A-13-1900-20       97765          53BP1-R-C   TP53BP1
>   gene_stable_id gene_build_version normalized_expression_level
> verification_status verification_platform
> 1             NA                 NA                  -1.1636330
> not tested                    NA
> 2             NA                 NA                  -1.7969721
> not tested                    NA
> 3             NA                 NA                  -0.7256390
> not tested                    NA
> 4             NA                 NA                   0.6498421
> not tested                    NA
> 5             NA                 NA                  -1.0262844
> not tested                    NA
> 6             NA                 NA                   1.5186400
> not tested                    NA
>                                         platform
> 1 M.D. Anderson Reverse Phase Protein Array Core
> 2 M.D. Anderson Reverse Phase Protein Array Core
> 3 M.D. Anderson Reverse Phase Protein Array Core
> 4 M.D. Anderson Reverse Phase Protein Array Core
> 5 M.D. Anderson Reverse Phase Protein Array Core
> 6 M.D. Anderson Reverse Phase Protein Array Core
>
>
>
> experimental_protocol
> 1 MDA_RPPA_Core
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> 2 MDA_RPPA_Core
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> 3 MDA_RPPA_Core
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> 4 MDA_RPPA_Core
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> 5 MDA_RPPA_Core
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> 6 MDA_RPPA_Core
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
>   raw_data_repository          raw_data_accession
> 1                TCGA TCGA-19-5960-01A-13-1900-20
> 2                TCGA TCGA-19-5960-01A-13-1900-20
> 3                TCGA TCGA-19-5960-01A-13-1900-20
> 4                TCGA TCGA-19-5960-01A-13-1900-20
> 5                TCGA TCGA-19-5960-01A-13-1900-20
> 6                TCGA TCGA-19-5960-01A-13-1900-20
>
> --
> Federico Calboli
> LBEG - Laboratory of Biodiversity and Evolutionary Genomics
> Charles Deberiotstraat 32
> <https://maps.google.com/?q=Charles+Deberiotstraat+32&entry=gmail&source=g>
> box 2439
> 3000 Leuven
> +32 16 32 87 67
>
>
>
>
>
>

	[[alternative HTML version deleted]]


From @pbr@ckett20 @ending from @@intjo@ephh@@com  Thu Dec 27 21:28:01 2018
From: @pbr@ckett20 @ending from @@intjo@ephh@@com (Spencer Brackett)
Date: Thu, 27 Dec 2018 15:28:01 -0500
Subject: [R] you are making it far too difficult
In-Reply-To: <8773122F-1F09-4F43-9C9F-834610A96FED@kuleuven.be>
References: <C727FD36-8DBD-4BC1-8F92-E60B1E05F03C@kuleuven.be>
 <CAPQaxLOgDhzX_njV_ukp4POR6PV89DiCjvMNMMwfj+cLsBFEmA@mail.gmail.com>
 <8773122F-1F09-4F43-9C9F-834610A96FED@kuleuven.be>
Message-ID: <CAPQaxLOS8SQq+ONXL_W+4O+S4F-rTqvzqdxR-5dSYPTgfe-7Bg@mail.gmail.com>

Mr. Calboli,

After beginning to unpack the GBM file you sent me via directly importing
it unit my console, I received the following:

View(GBM_PEXP.tsv)

**Note that I named the file GBM_PEXP.tsv)**

  Upon downloading, my script now contains a 2 by 2 table, with the x
column still containing encoded script. As for my Data summary to the
 right, this new file reports that 2 objects are acting upon 1 variable.
How should I proceed?

-Spencer

On Thu, Dec 27, 2018 at 3:12 PM Federico Calboli <
federico.calboli at kuleuven.be> wrote:

> Unpack these files.
>
> F
>
>

	[[alternative HTML version deleted]]


From @@r@h@go@lee @ending from gm@il@com  Thu Dec 27 22:02:28 2018
From: @@r@h@go@lee @ending from gm@il@com (Sarah Goslee)
Date: Thu, 27 Dec 2018 16:02:28 -0500
Subject: [R] you are making it far too difficult
In-Reply-To: <CAPQaxLOgDhzX_njV_ukp4POR6PV89DiCjvMNMMwfj+cLsBFEmA@mail.gmail.com>
References: <C727FD36-8DBD-4BC1-8F92-E60B1E05F03C@kuleuven.be>
 <CAPQaxLOgDhzX_njV_ukp4POR6PV89DiCjvMNMMwfj+cLsBFEmA@mail.gmail.com>
Message-ID: <CAM_vjukUQ4hKRC+v77gShb+_sHgU9SiXNTqC83XeXesAHZCx-g@mail.gmail.com>

On Thu, Dec 27, 2018 at 2:03 PM Spencer Brackett
<spbrackett20 at saintjosephhs.com> wrote:
>
> Thank you for the help! I tried using the read.table command in my RStudio
> using the following argument, and managed to open the file.
>
> GBM_protein_expression<-read.table(file.choose(), header=TRUE, sep=?/t?)

Note that sep="/t" is NOT the same thing as the sep="\t" you were
advised to use.




>
> However, my data did not unpack as yours did. I again only received a table
> of true and flase distinctions per column, and my environment tab says that
> there is 0 observations upon 0 variables.
>
> I believe I should be getting data similar to what you got, as it would
> appear that your?s actually contains relevant gene/protein expression info.
>
> On Thu, Dec 27, 2018 at 6:21 AM Federico Calboli <
> federico.calboli at kuleuven.be> wrote:
>
> > Once you have your TSV files just use something as
> >
> > x = read.table('protein_expression.tsv', h = T, sep = '\t')
> >
> > Do not copy paste the code of this email because it is formatted and would
> > not work in R.
> >
> >
> > Best
> >
> > F
> >
> > PS the data looks like this to me
> >
> > head(x)
> >   icgc_donor_id project_code icgc_specimen_id icgc_sample_id
> >  submitted_sample_id analysis_id        antibody_id gene_name
> > 1       DO12370       GBM-US          SP26475       SA131594
> > TCGA-19-5960-01A-13-1900-20       97765 14-3-3_epsilon-M-C     YWHAE
> > 2       DO12370       GBM-US          SP26475       SA131594
> > TCGA-19-5960-01A-13-1900-20       97765         4E-BP1-R-V  EIF4EBP1
> > 3       DO12370       GBM-US          SP26475       SA131594
> > TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pS65-R-V  EIF4EBP1
> > 4       DO12370       GBM-US          SP26475       SA131594
> > TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pT37-R-V  EIF4EBP1
> > 5       DO12370       GBM-US          SP26475       SA131594
> > TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pT70-R-C  EIF4EBP1
> > 6       DO12370       GBM-US          SP26475       SA131594
> > TCGA-19-5960-01A-13-1900-20       97765          53BP1-R-C   TP53BP1
> >   gene_stable_id gene_build_version normalized_expression_level
> > verification_status verification_platform
> > 1             NA                 NA                  -1.1636330
> > not tested                    NA
> > 2             NA                 NA                  -1.7969721
> > not tested                    NA
> > 3             NA                 NA                  -0.7256390
> > not tested                    NA
> > 4             NA                 NA                   0.6498421
> > not tested                    NA
> > 5             NA                 NA                  -1.0262844
> > not tested                    NA
> > 6             NA                 NA                   1.5186400
> > not tested                    NA
> >                                         platform
> > 1 M.D. Anderson Reverse Phase Protein Array Core
> > 2 M.D. Anderson Reverse Phase Protein Array Core
> > 3 M.D. Anderson Reverse Phase Protein Array Core
> > 4 M.D. Anderson Reverse Phase Protein Array Core
> > 5 M.D. Anderson Reverse Phase Protein Array Core
> > 6 M.D. Anderson Reverse Phase Protein Array Core
> >
> >
> >
> > experimental_protocol
> > 1 MDA_RPPA_Core
> > http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> > 2 MDA_RPPA_Core
> > http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> > 3 MDA_RPPA_Core
> > http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> > 4 MDA_RPPA_Core
> > http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> > 5 MDA_RPPA_Core
> > http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> > 6 MDA_RPPA_Core
> > http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> >   raw_data_repository          raw_data_accession
> > 1                TCGA TCGA-19-5960-01A-13-1900-20
> > 2                TCGA TCGA-19-5960-01A-13-1900-20
> > 3                TCGA TCGA-19-5960-01A-13-1900-20
> > 4                TCGA TCGA-19-5960-01A-13-1900-20
> > 5                TCGA TCGA-19-5960-01A-13-1900-20
> > 6                TCGA TCGA-19-5960-01A-13-1900-20
> >


-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From rmh @ending from temple@edu  Thu Dec 27 22:05:29 2018
From: rmh @ending from temple@edu (Richard M. Heiberger)
Date: Thu, 27 Dec 2018 16:05:29 -0500
Subject: [R] you are making it far too difficult
In-Reply-To: <CAPQaxLOS8SQq+ONXL_W+4O+S4F-rTqvzqdxR-5dSYPTgfe-7Bg@mail.gmail.com>
References: <C727FD36-8DBD-4BC1-8F92-E60B1E05F03C@kuleuven.be>
 <CAPQaxLOgDhzX_njV_ukp4POR6PV89DiCjvMNMMwfj+cLsBFEmA@mail.gmail.com>
 <8773122F-1F09-4F43-9C9F-834610A96FED@kuleuven.be>
 <CAPQaxLOS8SQq+ONXL_W+4O+S4F-rTqvzqdxR-5dSYPTgfe-7Bg@mail.gmail.com>
Message-ID: <CAGx1TMB6LHbR4NwRtU7E9+XsjUf3kMrKCkHEdevTwHxV292G+Q@mail.gmail.com>

I downloaded the Donors dataset

https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22GBM-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D

by clicking "Export table as TSV".

Then I read it with

donors <- read.delim("~/Downloads/donors_2018_12_27_03_52_03.tsv")

Here is the transcript.

> donors <- read.delim("~/Downloads/donors_2018_12_27_03_52_03.tsv")
> donors
   Donor.ID Project.Code Primary.Site Gender Age.at.Diagnosis
1   DO10892       GBM-US        Brain Female               45
2   DO12328       GBM-US        Brain   Male               56
3   DO11657       GBM-US        Brain Female               73
4   DO13510       GBM-US        Brain Female               63
5   DO12670       GBM-US        Brain Female               63
6   DO11501       GBM-US        Brain Female               59
7   DO13809       GBM-US        Brain Female               74
8   DO13647       GBM-US        Brain   Male               56
9   DO11645       GBM-US        Brain   Male               73
10  DO14145       GBM-US        Brain Female               85
   Tumor.Stage.at.Diagnosis Survival.Time..days.  SSM CNSM  STSM   SGV
METH.A
1                        NA                   NA True True False False
 True
2                        NA                  154 True True False False
 True
3                        NA                   NA True True False False
 True
4                        NA                 1448 True True False False
 True
5                        NA                  772 True True False False
 True
6                        NA                   NA True True False False
 True
7                        NA                  213 True True False False
 True
8                        NA                  383 True True False False
 True
9                        NA                  113 True True False False
 True
10                       NA                   94 True True False False
 True
   METH.S EXP.A EXP.S PEXP miRNA.S   JCN Mutations Mutated.Genes
1   False  True  True True   False False       269           392
2   False  True False True   False False       192           263
3   False  True False True   False False       128           209
4   False  True  True True   False False       130           199
5   False  True  True True   False False       142           194
6   False  True  True True   False False       129           190
7   False  True False True   False False       130           178
8   False  True False True   False False       116           175
9   False  True False True   False False       125           174
10  False  True  True True   False False       108           169
>

I don't know how to get the download of the whole file.  It looks like you
could page through it with the page menu at the bottom of the webpage.  If
you do that, set it for 50 at a time instead of the default 10.

For the Genes and the two types of Mutation files, it will be more nuisance
this way because there are about 10000 rows for each of those three files,
thus about 200 of these statements per dataset.

I think it is time to move to the bioconductor list for specific guidance
on this type of dataset.


On Thu, Dec 27, 2018 at 3:28 PM Spencer Brackett <
spbrackett20 at saintjosephhs.com> wrote:

> Mr. Calboli,
>
> After beginning to unpack the GBM file you sent me via directly importing
> it unit my console, I received the following:
>
> View(GBM_PEXP.tsv)
>
> **Note that I named the file GBM_PEXP.tsv)**
>
>   Upon downloading, my script now contains a 2 by 2 table, with the x
> column still containing encoded script. As for my Data summary to the
>  right, this new file reports that 2 objects are acting upon 1 variable.
> How should I proceed?
>
> -Spencer
>
> On Thu, Dec 27, 2018 at 3:12 PM Federico Calboli <
> federico.calboli at kuleuven.be> wrote:
>
> > Unpack these files.
> >
> > F
> >
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @pbr@ckett20 @ending from @@intjo@ephh@@com  Thu Dec 27 22:09:21 2018
From: @pbr@ckett20 @ending from @@intjo@ephh@@com (Spencer Brackett)
Date: Thu, 27 Dec 2018 16:09:21 -0500
Subject: [R] you are making it far too difficult
In-Reply-To: <CAM_vjukUQ4hKRC+v77gShb+_sHgU9SiXNTqC83XeXesAHZCx-g@mail.gmail.com>
References: <C727FD36-8DBD-4BC1-8F92-E60B1E05F03C@kuleuven.be>
 <CAPQaxLOgDhzX_njV_ukp4POR6PV89DiCjvMNMMwfj+cLsBFEmA@mail.gmail.com>
 <CAM_vjukUQ4hKRC+v77gShb+_sHgU9SiXNTqC83XeXesAHZCx-g@mail.gmail.com>
Message-ID: <CAPQaxLPh3fNXaoc=nnkN1BBegd-sWZRVx8S_qUjoPYWf2jcYCw@mail.gmail.com>

What is the significance of using / or \ ?

On Thu, Dec 27, 2018 at 4:02 PM Sarah Goslee <sarah.goslee at gmail.com> wrote:

> On Thu, Dec 27, 2018 at 2:03 PM Spencer Brackett
> <spbrackett20 at saintjosephhs.com> wrote:
> >
> > Thank you for the help! I tried using the read.table command in my
> RStudio
> > using the following argument, and managed to open the file.
> >
> > GBM_protein_expression<-read.table(file.choose(), header=TRUE, sep=?/t?)
>
> Note that sep="/t" is NOT the same thing as the sep="\t" you were
> advised to use.
>
>
>
>
> >
> > However, my data did not unpack as yours did. I again only received a
> table
> > of true and flase distinctions per column, and my environment tab says
> that
> > there is 0 observations upon 0 variables.
> >
> > I believe I should be getting data similar to what you got, as it would
> > appear that your?s actually contains relevant gene/protein expression
> info.
> >
> > On Thu, Dec 27, 2018 at 6:21 AM Federico Calboli <
> > federico.calboli at kuleuven.be> wrote:
> >
> > > Once you have your TSV files just use something as
> > >
> > > x = read.table('protein_expression.tsv', h = T, sep = '\t')
> > >
> > > Do not copy paste the code of this email because it is formatted and
> would
> > > not work in R.
> > >
> > >
> > > Best
> > >
> > > F
> > >
> > > PS the data looks like this to me
> > >
> > > head(x)
> > >   icgc_donor_id project_code icgc_specimen_id icgc_sample_id
> > >  submitted_sample_id analysis_id        antibody_id gene_name
> > > 1       DO12370       GBM-US          SP26475       SA131594
> > > TCGA-19-5960-01A-13-1900-20       97765 14-3-3_epsilon-M-C     YWHAE
> > > 2       DO12370       GBM-US          SP26475       SA131594
> > > TCGA-19-5960-01A-13-1900-20       97765         4E-BP1-R-V  EIF4EBP1
> > > 3       DO12370       GBM-US          SP26475       SA131594
> > > TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pS65-R-V  EIF4EBP1
> > > 4       DO12370       GBM-US          SP26475       SA131594
> > > TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pT37-R-V  EIF4EBP1
> > > 5       DO12370       GBM-US          SP26475       SA131594
> > > TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pT70-R-C  EIF4EBP1
> > > 6       DO12370       GBM-US          SP26475       SA131594
> > > TCGA-19-5960-01A-13-1900-20       97765          53BP1-R-C   TP53BP1
> > >   gene_stable_id gene_build_version normalized_expression_level
> > > verification_status verification_platform
> > > 1             NA                 NA                  -1.1636330
> > > not tested                    NA
> > > 2             NA                 NA                  -1.7969721
> > > not tested                    NA
> > > 3             NA                 NA                  -0.7256390
> > > not tested                    NA
> > > 4             NA                 NA                   0.6498421
> > > not tested                    NA
> > > 5             NA                 NA                  -1.0262844
> > > not tested                    NA
> > > 6             NA                 NA                   1.5186400
> > > not tested                    NA
> > >                                         platform
> > > 1 M.D. Anderson Reverse Phase Protein Array Core
> > > 2 M.D. Anderson Reverse Phase Protein Array Core
> > > 3 M.D. Anderson Reverse Phase Protein Array Core
> > > 4 M.D. Anderson Reverse Phase Protein Array Core
> > > 5 M.D. Anderson Reverse Phase Protein Array Core
> > > 6 M.D. Anderson Reverse Phase Protein Array Core
> > >
> > >
> > >
> > > experimental_protocol
> > > 1 MDA_RPPA_Core
> > >
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> > > 2 MDA_RPPA_Core
> > >
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> > > 3 MDA_RPPA_Core
> > >
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> > > 4 MDA_RPPA_Core
> > >
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> > > 5 MDA_RPPA_Core
> > >
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> > > 6 MDA_RPPA_Core
> > >
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> > >   raw_data_repository          raw_data_accession
> > > 1                TCGA TCGA-19-5960-01A-13-1900-20
> > > 2                TCGA TCGA-19-5960-01A-13-1900-20
> > > 3                TCGA TCGA-19-5960-01A-13-1900-20
> > > 4                TCGA TCGA-19-5960-01A-13-1900-20
> > > 5                TCGA TCGA-19-5960-01A-13-1900-20
> > > 6                TCGA TCGA-19-5960-01A-13-1900-20
> > >
>
>
> --
> Sarah Goslee (she/her)
> http://www.numberwright.com
>

	[[alternative HTML version deleted]]


From bioprogr@mmer @ending from gm@il@com  Thu Dec 27 22:30:27 2018
From: bioprogr@mmer @ending from gm@il@com (Caitlin Gibbons)
Date: Thu, 27 Dec 2018 14:30:27 -0700
Subject: [R] you are making it far too difficult
In-Reply-To: <CAPQaxLPh3fNXaoc=nnkN1BBegd-sWZRVx8S_qUjoPYWf2jcYCw@mail.gmail.com>
References: <C727FD36-8DBD-4BC1-8F92-E60B1E05F03C@kuleuven.be>
 <CAPQaxLOgDhzX_njV_ukp4POR6PV89DiCjvMNMMwfj+cLsBFEmA@mail.gmail.com>
 <CAM_vjukUQ4hKRC+v77gShb+_sHgU9SiXNTqC83XeXesAHZCx-g@mail.gmail.com>
 <CAPQaxLPh3fNXaoc=nnkN1BBegd-sWZRVx8S_qUjoPYWf2jcYCw@mail.gmail.com>
Message-ID: <08D26B35-0F29-4EE6-8B5C-3075C694EA5B@gmail.com>

?\t? is an escape sequence which signifies one tab character. ?/t? is NOT an escape sequence, and to R, looks like a very brief file path. 

Sent from my iPhone

> On Dec 27, 2018, at 2:09 PM, Spencer Brackett <spbrackett20 at saintjosephhs.com> wrote:
> 
> What is the significance of using / or \ ?
> 
>> On Thu, Dec 27, 2018 at 4:02 PM Sarah Goslee <sarah.goslee at gmail.com> wrote:
>> 
>> On Thu, Dec 27, 2018 at 2:03 PM Spencer Brackett
>> <spbrackett20 at saintjosephhs.com> wrote:
>>> 
>>> Thank you for the help! I tried using the read.table command in my
>> RStudio
>>> using the following argument, and managed to open the file.
>>> 
>>> GBM_protein_expression<-read.table(file.choose(), header=TRUE, sep=?/t?)
>> 
>> Note that sep="/t" is NOT the same thing as the sep="\t" you were
>> advised to use.
>> 
>> 
>> 
>> 
>>> 
>>> However, my data did not unpack as yours did. I again only received a
>> table
>>> of true and flase distinctions per column, and my environment tab says
>> that
>>> there is 0 observations upon 0 variables.
>>> 
>>> I believe I should be getting data similar to what you got, as it would
>>> appear that your?s actually contains relevant gene/protein expression
>> info.
>>> 
>>> On Thu, Dec 27, 2018 at 6:21 AM Federico Calboli <
>>> federico.calboli at kuleuven.be> wrote:
>>> 
>>>> Once you have your TSV files just use something as
>>>> 
>>>> x = read.table('protein_expression.tsv', h = T, sep = '\t')
>>>> 
>>>> Do not copy paste the code of this email because it is formatted and
>> would
>>>> not work in R.
>>>> 
>>>> 
>>>> Best
>>>> 
>>>> F
>>>> 
>>>> PS the data looks like this to me
>>>> 
>>>> head(x)
>>>>  icgc_donor_id project_code icgc_specimen_id icgc_sample_id
>>>> submitted_sample_id analysis_id        antibody_id gene_name
>>>> 1       DO12370       GBM-US          SP26475       SA131594
>>>> TCGA-19-5960-01A-13-1900-20       97765 14-3-3_epsilon-M-C     YWHAE
>>>> 2       DO12370       GBM-US          SP26475       SA131594
>>>> TCGA-19-5960-01A-13-1900-20       97765         4E-BP1-R-V  EIF4EBP1
>>>> 3       DO12370       GBM-US          SP26475       SA131594
>>>> TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pS65-R-V  EIF4EBP1
>>>> 4       DO12370       GBM-US          SP26475       SA131594
>>>> TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pT37-R-V  EIF4EBP1
>>>> 5       DO12370       GBM-US          SP26475       SA131594
>>>> TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pT70-R-C  EIF4EBP1
>>>> 6       DO12370       GBM-US          SP26475       SA131594
>>>> TCGA-19-5960-01A-13-1900-20       97765          53BP1-R-C   TP53BP1
>>>>  gene_stable_id gene_build_version normalized_expression_level
>>>> verification_status verification_platform
>>>> 1             NA                 NA                  -1.1636330
>>>> not tested                    NA
>>>> 2             NA                 NA                  -1.7969721
>>>> not tested                    NA
>>>> 3             NA                 NA                  -0.7256390
>>>> not tested                    NA
>>>> 4             NA                 NA                   0.6498421
>>>> not tested                    NA
>>>> 5             NA                 NA                  -1.0262844
>>>> not tested                    NA
>>>> 6             NA                 NA                   1.5186400
>>>> not tested                    NA
>>>>                                        platform
>>>> 1 M.D. Anderson Reverse Phase Protein Array Core
>>>> 2 M.D. Anderson Reverse Phase Protein Array Core
>>>> 3 M.D. Anderson Reverse Phase Protein Array Core
>>>> 4 M.D. Anderson Reverse Phase Protein Array Core
>>>> 5 M.D. Anderson Reverse Phase Protein Array Core
>>>> 6 M.D. Anderson Reverse Phase Protein Array Core
>>>> 
>>>> 
>>>> 
>>>> experimental_protocol
>>>> 1 MDA_RPPA_Core
>>>> 
>> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
>>>> 2 MDA_RPPA_Core
>>>> 
>> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
>>>> 3 MDA_RPPA_Core
>>>> 
>> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
>>>> 4 MDA_RPPA_Core
>>>> 
>> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
>>>> 5 MDA_RPPA_Core
>>>> 
>> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
>>>> 6 MDA_RPPA_Core
>>>> 
>> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
>>>>  raw_data_repository          raw_data_accession
>>>> 1                TCGA TCGA-19-5960-01A-13-1900-20
>>>> 2                TCGA TCGA-19-5960-01A-13-1900-20
>>>> 3                TCGA TCGA-19-5960-01A-13-1900-20
>>>> 4                TCGA TCGA-19-5960-01A-13-1900-20
>>>> 5                TCGA TCGA-19-5960-01A-13-1900-20
>>>> 6                TCGA TCGA-19-5960-01A-13-1900-20
>>>> 
>> 
>> 
>> --
>> Sarah Goslee (she/her)
>> http://www.numberwright.com
>> 
> 
>    [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @pbr@ckett20 @ending from @@intjo@ephh@@com  Thu Dec 27 22:30:42 2018
From: @pbr@ckett20 @ending from @@intjo@ephh@@com (Spencer Brackett)
Date: Thu, 27 Dec 2018 16:30:42 -0500
Subject: [R] you are making it far too difficult
In-Reply-To: <CAGx1TMB6LHbR4NwRtU7E9+XsjUf3kMrKCkHEdevTwHxV292G+Q@mail.gmail.com>
References: <C727FD36-8DBD-4BC1-8F92-E60B1E05F03C@kuleuven.be>
 <CAPQaxLOgDhzX_njV_ukp4POR6PV89DiCjvMNMMwfj+cLsBFEmA@mail.gmail.com>
 <8773122F-1F09-4F43-9C9F-834610A96FED@kuleuven.be>
 <CAPQaxLOS8SQq+ONXL_W+4O+S4F-rTqvzqdxR-5dSYPTgfe-7Bg@mail.gmail.com>
 <CAGx1TMB6LHbR4NwRtU7E9+XsjUf3kMrKCkHEdevTwHxV292G+Q@mail.gmail.com>
Message-ID: <CAPQaxLOGzKGJbP+T-utVxf_CH=L21eTP5OH6-e+AepeAyiv+RA@mail.gmail.com>

Mr. Heiberger,

  I followed your argument and it works. I received the same data. And yes,
ICGC breakes their datasets into separate files based on data type. Thank
you for the pointer on selecting all 50 rows, as I assumed that the
entirety of gene expression data within the data set would be downloaded
through the process you followed, as it does when directly downloading the
file via a tsv.gz download.

On Thu, Dec 27, 2018 at 4:05 PM Richard M. Heiberger <rmh at temple.edu> wrote:

> I downloaded the Donors dataset
>
>
> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22GBM-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>
> by clicking "Export table as TSV".
>
> Then I read it with
>
> donors <- read.delim("~/Downloads/donors_2018_12_27_03_52_03.tsv")
>
> Here is the transcript.
>
> > donors <- read.delim("~/Downloads/donors_2018_12_27_03_52_03.tsv")
> > donors
>    Donor.ID Project.Code Primary.Site Gender Age.at.Diagnosis
> 1   DO10892       GBM-US        Brain Female               45
> 2   DO12328       GBM-US        Brain   Male               56
> 3   DO11657       GBM-US        Brain Female               73
> 4   DO13510       GBM-US        Brain Female               63
> 5   DO12670       GBM-US        Brain Female               63
> 6   DO11501       GBM-US        Brain Female               59
> 7   DO13809       GBM-US        Brain Female               74
> 8   DO13647       GBM-US        Brain   Male               56
> 9   DO11645       GBM-US        Brain   Male               73
> 10  DO14145       GBM-US        Brain Female               85
>    Tumor.Stage.at.Diagnosis Survival.Time..days.  SSM CNSM  STSM   SGV
> METH.A
> 1                        NA                   NA True True False False
>  True
> 2                        NA                  154 True True False False
>  True
> 3                        NA                   NA True True False False
>  True
> 4                        NA                 1448 True True False False
>  True
> 5                        NA                  772 True True False False
>  True
> 6                        NA                   NA True True False False
>  True
> 7                        NA                  213 True True False False
>  True
> 8                        NA                  383 True True False False
>  True
> 9                        NA                  113 True True False False
>  True
> 10                       NA                   94 True True False False
>  True
>    METH.S EXP.A EXP.S PEXP miRNA.S   JCN Mutations Mutated.Genes
> 1   False  True  True True   False False       269           392
> 2   False  True False True   False False       192           263
> 3   False  True False True   False False       128           209
> 4   False  True  True True   False False       130           199
> 5   False  True  True True   False False       142           194
> 6   False  True  True True   False False       129           190
> 7   False  True False True   False False       130           178
> 8   False  True False True   False False       116           175
> 9   False  True False True   False False       125           174
> 10  False  True  True True   False False       108           169
> >
>
> I don't know how to get the download of the whole file.  It looks like you
> could page through it with the page menu at the bottom of the webpage.  If
> you do that, set it for 50 at a time instead of the default 10.
>
> For the Genes and the two types of Mutation files, it will be more
> nuisance this way because there are about 10000 rows for each of those
> three files, thus about 200 of these statements per dataset.
>
> I think it is time to move to the bioconductor list for specific guidance
> on this type of dataset.
>
>
> On Thu, Dec 27, 2018 at 3:28 PM Spencer Brackett <
> spbrackett20 at saintjosephhs.com> wrote:
>
>> Mr. Calboli,
>>
>> After beginning to unpack the GBM file you sent me via directly importing
>> it unit my console, I received the following:
>>
>> View(GBM_PEXP.tsv)
>>
>> **Note that I named the file GBM_PEXP.tsv)**
>>
>>   Upon downloading, my script now contains a 2 by 2 table, with the x
>> column still containing encoded script. As for my Data summary to the
>>  right, this new file reports that 2 objects are acting upon 1 variable.
>> How should I proceed?
>>
>> -Spencer
>>
>> On Thu, Dec 27, 2018 at 3:12 PM Federico Calboli <
>> federico.calboli at kuleuven.be> wrote:
>>
>> > Unpack these files.
>> >
>> > F
>> >
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From @@r@h@go@lee @ending from gm@il@com  Thu Dec 27 22:43:11 2018
From: @@r@h@go@lee @ending from gm@il@com (Sarah Goslee)
Date: Thu, 27 Dec 2018 16:43:11 -0500
Subject: [R] you are making it far too difficult
In-Reply-To: <CAPQaxLPh3fNXaoc=nnkN1BBegd-sWZRVx8S_qUjoPYWf2jcYCw@mail.gmail.com>
References: <C727FD36-8DBD-4BC1-8F92-E60B1E05F03C@kuleuven.be>
 <CAPQaxLOgDhzX_njV_ukp4POR6PV89DiCjvMNMMwfj+cLsBFEmA@mail.gmail.com>
 <CAM_vjukUQ4hKRC+v77gShb+_sHgU9SiXNTqC83XeXesAHZCx-g@mail.gmail.com>
 <CAPQaxLPh3fNXaoc=nnkN1BBegd-sWZRVx8S_qUjoPYWf2jcYCw@mail.gmail.com>
Message-ID: <CAM_vjukcT4mXH0ZNnd4t3E6rx4qFbhg6aNxjKFW-=Lqx4W4Y4Q@mail.gmail.com>

\t is the symbol for a tab.
/t is two characters just as it seems. It's highly unlikely your file
is delimited with /t, which would look like
1/t2/t3

The help for read.table mentions this tangentially as part of
read.delim(), and you can find out more under ?regex - see the section
about escaping non-metacharacters with a backslash.

Sarah

On Thu, Dec 27, 2018 at 4:09 PM Spencer Brackett
<spbrackett20 at saintjosephhs.com> wrote:
>
> What is the significance of using / or \ ?
>
> On Thu, Dec 27, 2018 at 4:02 PM Sarah Goslee <sarah.goslee at gmail.com> wrote:
>>
>> On Thu, Dec 27, 2018 at 2:03 PM Spencer Brackett
>> <spbrackett20 at saintjosephhs.com> wrote:
>> >
>> > Thank you for the help! I tried using the read.table command in my RStudio
>> > using the following argument, and managed to open the file.
>> >
>> > GBM_protein_expression<-read.table(file.choose(), header=TRUE, sep=?/t?)
>>
>> Note that sep="/t" is NOT the same thing as the sep="\t" you were
>> advised to use.
>>
>>
>>
>>
>> >
>> > However, my data did not unpack as yours did. I again only received a table
>> > of true and flase distinctions per column, and my environment tab says that
>> > there is 0 observations upon 0 variables.
>> >
>> > I believe I should be getting data similar to what you got, as it would
>> > appear that your?s actually contains relevant gene/protein expression info.
>> >
>> > On Thu, Dec 27, 2018 at 6:21 AM Federico Calboli <
>> > federico.calboli at kuleuven.be> wrote:
>> >
>> > > Once you have your TSV files just use something as
>> > >
>> > > x = read.table('protein_expression.tsv', h = T, sep = '\t')
>> > >
>> > > Do not copy paste the code of this email because it is formatted and would
>> > > not work in R.
>> > >
>> > >
>> > > Best
>> > >
>> > > F
>> > >
>> > > PS the data looks like this to me
>> > >
>> > > head(x)
>> > >   icgc_donor_id project_code icgc_specimen_id icgc_sample_id
>> > >  submitted_sample_id analysis_id        antibody_id gene_name
>> > > 1       DO12370       GBM-US          SP26475       SA131594
>> > > TCGA-19-5960-01A-13-1900-20       97765 14-3-3_epsilon-M-C     YWHAE
>> > > 2       DO12370       GBM-US          SP26475       SA131594
>> > > TCGA-19-5960-01A-13-1900-20       97765         4E-BP1-R-V  EIF4EBP1
>> > > 3       DO12370       GBM-US          SP26475       SA131594
>> > > TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pS65-R-V  EIF4EBP1
>> > > 4       DO12370       GBM-US          SP26475       SA131594
>> > > TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pT37-R-V  EIF4EBP1
>> > > 5       DO12370       GBM-US          SP26475       SA131594
>> > > TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pT70-R-C  EIF4EBP1
>> > > 6       DO12370       GBM-US          SP26475       SA131594
>> > > TCGA-19-5960-01A-13-1900-20       97765          53BP1-R-C   TP53BP1
>> > >   gene_stable_id gene_build_version normalized_expression_level
>> > > verification_status verification_platform
>> > > 1             NA                 NA                  -1.1636330
>> > > not tested                    NA
>> > > 2             NA                 NA                  -1.7969721
>> > > not tested                    NA
>> > > 3             NA                 NA                  -0.7256390
>> > > not tested                    NA
>> > > 4             NA                 NA                   0.6498421
>> > > not tested                    NA
>> > > 5             NA                 NA                  -1.0262844
>> > > not tested                    NA
>> > > 6             NA                 NA                   1.5186400
>> > > not tested                    NA
>> > >                                         platform
>> > > 1 M.D. Anderson Reverse Phase Protein Array Core
>> > > 2 M.D. Anderson Reverse Phase Protein Array Core
>> > > 3 M.D. Anderson Reverse Phase Protein Array Core
>> > > 4 M.D. Anderson Reverse Phase Protein Array Core
>> > > 5 M.D. Anderson Reverse Phase Protein Array Core
>> > > 6 M.D. Anderson Reverse Phase Protein Array Core
>> > >
>> > >
>> > >
>> > > experimental_protocol
>> > > 1 MDA_RPPA_Core
>> > > http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
>> > > 2 MDA_RPPA_Core
>> > > http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
>> > > 3 MDA_RPPA_Core
>> > > http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
>> > > 4 MDA_RPPA_Core
>> > > http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
>> > > 5 MDA_RPPA_Core
>> > > http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
>> > > 6 MDA_RPPA_Core
>> > > http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
>> > >   raw_data_repository          raw_data_accession
>> > > 1                TCGA TCGA-19-5960-01A-13-1900-20
>> > > 2                TCGA TCGA-19-5960-01A-13-1900-20
>> > > 3                TCGA TCGA-19-5960-01A-13-1900-20
>> > > 4                TCGA TCGA-19-5960-01A-13-1900-20
>> > > 5                TCGA TCGA-19-5960-01A-13-1900-20
>> > > 6                TCGA TCGA-19-5960-01A-13-1900-20
>> > >
>>
>>
>> --
>> Sarah Goslee (she/her)
>> http://www.numberwright.com


From @pbr@ckett20 @ending from @@intjo@ephh@@com  Thu Dec 27 22:34:25 2018
From: @pbr@ckett20 @ending from @@intjo@ephh@@com (Spencer Brackett)
Date: Thu, 27 Dec 2018 16:34:25 -0500
Subject: [R] you are making it far too difficult
In-Reply-To: <08D26B35-0F29-4EE6-8B5C-3075C694EA5B@gmail.com>
References: <C727FD36-8DBD-4BC1-8F92-E60B1E05F03C@kuleuven.be>
 <CAPQaxLOgDhzX_njV_ukp4POR6PV89DiCjvMNMMwfj+cLsBFEmA@mail.gmail.com>
 <CAM_vjukUQ4hKRC+v77gShb+_sHgU9SiXNTqC83XeXesAHZCx-g@mail.gmail.com>
 <CAPQaxLPh3fNXaoc=nnkN1BBegd-sWZRVx8S_qUjoPYWf2jcYCw@mail.gmail.com>
 <08D26B35-0F29-4EE6-8B5C-3075C694EA5B@gmail.com>
Message-ID: <CAPQaxLOkDZ-sMvGP2ftWoXuywBakNQ3ajiHvzH2kVkLqj6MjTQ@mail.gmail.com>

So I should use ?\t? proceeding on?

On Thu, Dec 27, 2018 at 4:30 PM Caitlin Gibbons <bioprogrammer at gmail.com>
wrote:

> ?\t? is an escape sequence which signifies one tab character. ?/t? is NOT
> an escape sequence, and to R, looks like a very brief file path.
>
> Sent from my iPhone
>
> > On Dec 27, 2018, at 2:09 PM, Spencer Brackett <
> spbrackett20 at saintjosephhs.com> wrote:
> >
> > What is the significance of using / or \ ?
> >
> >> On Thu, Dec 27, 2018 at 4:02 PM Sarah Goslee <sarah.goslee at gmail.com>
> wrote:
> >>
> >> On Thu, Dec 27, 2018 at 2:03 PM Spencer Brackett
> >> <spbrackett20 at saintjosephhs.com> wrote:
> >>>
> >>> Thank you for the help! I tried using the read.table command in my
> >> RStudio
> >>> using the following argument, and managed to open the file.
> >>>
> >>> GBM_protein_expression<-read.table(file.choose(), header=TRUE,
> sep=?/t?)
> >>
> >> Note that sep="/t" is NOT the same thing as the sep="\t" you were
> >> advised to use.
> >>
> >>
> >>
> >>
> >>>
> >>> However, my data did not unpack as yours did. I again only received a
> >> table
> >>> of true and flase distinctions per column, and my environment tab says
> >> that
> >>> there is 0 observations upon 0 variables.
> >>>
> >>> I believe I should be getting data similar to what you got, as it would
> >>> appear that your?s actually contains relevant gene/protein expression
> >> info.
> >>>
> >>> On Thu, Dec 27, 2018 at 6:21 AM Federico Calboli <
> >>> federico.calboli at kuleuven.be> wrote:
> >>>
> >>>> Once you have your TSV files just use something as
> >>>>
> >>>> x = read.table('protein_expression.tsv', h = T, sep = '\t')
> >>>>
> >>>> Do not copy paste the code of this email because it is formatted and
> >> would
> >>>> not work in R.
> >>>>
> >>>>
> >>>> Best
> >>>>
> >>>> F
> >>>>
> >>>> PS the data looks like this to me
> >>>>
> >>>> head(x)
> >>>>  icgc_donor_id project_code icgc_specimen_id icgc_sample_id
> >>>> submitted_sample_id analysis_id        antibody_id gene_name
> >>>> 1       DO12370       GBM-US          SP26475       SA131594
> >>>> TCGA-19-5960-01A-13-1900-20       97765 14-3-3_epsilon-M-C     YWHAE
> >>>> 2       DO12370       GBM-US          SP26475       SA131594
> >>>> TCGA-19-5960-01A-13-1900-20       97765         4E-BP1-R-V  EIF4EBP1
> >>>> 3       DO12370       GBM-US          SP26475       SA131594
> >>>> TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pS65-R-V  EIF4EBP1
> >>>> 4       DO12370       GBM-US          SP26475       SA131594
> >>>> TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pT37-R-V  EIF4EBP1
> >>>> 5       DO12370       GBM-US          SP26475       SA131594
> >>>> TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pT70-R-C  EIF4EBP1
> >>>> 6       DO12370       GBM-US          SP26475       SA131594
> >>>> TCGA-19-5960-01A-13-1900-20       97765          53BP1-R-C   TP53BP1
> >>>>  gene_stable_id gene_build_version normalized_expression_level
> >>>> verification_status verification_platform
> >>>> 1             NA                 NA                  -1.1636330
> >>>> not tested                    NA
> >>>> 2             NA                 NA                  -1.7969721
> >>>> not tested                    NA
> >>>> 3             NA                 NA                  -0.7256390
> >>>> not tested                    NA
> >>>> 4             NA                 NA                   0.6498421
> >>>> not tested                    NA
> >>>> 5             NA                 NA                  -1.0262844
> >>>> not tested                    NA
> >>>> 6             NA                 NA                   1.5186400
> >>>> not tested                    NA
> >>>>                                        platform
> >>>> 1 M.D. Anderson Reverse Phase Protein Array Core
> >>>> 2 M.D. Anderson Reverse Phase Protein Array Core
> >>>> 3 M.D. Anderson Reverse Phase Protein Array Core
> >>>> 4 M.D. Anderson Reverse Phase Protein Array Core
> >>>> 5 M.D. Anderson Reverse Phase Protein Array Core
> >>>> 6 M.D. Anderson Reverse Phase Protein Array Core
> >>>>
> >>>>
> >>>>
> >>>> experimental_protocol
> >>>> 1 MDA_RPPA_Core
> >>>>
> >>
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> >>>> 2 MDA_RPPA_Core
> >>>>
> >>
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> >>>> 3 MDA_RPPA_Core
> >>>>
> >>
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> >>>> 4 MDA_RPPA_Core
> >>>>
> >>
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> >>>> 5 MDA_RPPA_Core
> >>>>
> >>
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> >>>> 6 MDA_RPPA_Core
> >>>>
> >>
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> >>>>  raw_data_repository          raw_data_accession
> >>>> 1                TCGA TCGA-19-5960-01A-13-1900-20
> >>>> 2                TCGA TCGA-19-5960-01A-13-1900-20
> >>>> 3                TCGA TCGA-19-5960-01A-13-1900-20
> >>>> 4                TCGA TCGA-19-5960-01A-13-1900-20
> >>>> 5                TCGA TCGA-19-5960-01A-13-1900-20
> >>>> 6                TCGA TCGA-19-5960-01A-13-1900-20
> >>>>
> >>
> >>
> >> --
> >> Sarah Goslee (she/her)
> >> http://www.numberwright.com
> >>
> >
> >    [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @pbr@ckett20 @ending from @@intjo@ephh@@com  Thu Dec 27 22:53:03 2018
From: @pbr@ckett20 @ending from @@intjo@ephh@@com (Spencer Brackett)
Date: Thu, 27 Dec 2018 16:53:03 -0500
Subject: [R] you are making it far too difficult
In-Reply-To: <CAM_vjukcT4mXH0ZNnd4t3E6rx4qFbhg6aNxjKFW-=Lqx4W4Y4Q@mail.gmail.com>
References: <C727FD36-8DBD-4BC1-8F92-E60B1E05F03C@kuleuven.be>
 <CAPQaxLOgDhzX_njV_ukp4POR6PV89DiCjvMNMMwfj+cLsBFEmA@mail.gmail.com>
 <CAM_vjukUQ4hKRC+v77gShb+_sHgU9SiXNTqC83XeXesAHZCx-g@mail.gmail.com>
 <CAPQaxLPh3fNXaoc=nnkN1BBegd-sWZRVx8S_qUjoPYWf2jcYCw@mail.gmail.com>
 <CAM_vjukcT4mXH0ZNnd4t3E6rx4qFbhg6aNxjKFW-=Lqx4W4Y4Q@mail.gmail.com>
Message-ID: <CAPQaxLOd5B=DjZmN5hQnFt=DvbYPmW9_g+G9oTh5LqhgocA6eQ@mail.gmail.com>

Thank you!

On Thu, Dec 27, 2018 at 4:43 PM Sarah Goslee <sarah.goslee at gmail.com> wrote:

> \t is the symbol for a tab.
> /t is two characters just as it seems. It's highly unlikely your file
> is delimited with /t, which would look like
> 1/t2/t3
>
> The help for read.table mentions this tangentially as part of
> read.delim(), and you can find out more under ?regex - see the section
> about escaping non-metacharacters with a backslash.
>
> Sarah
>
> On Thu, Dec 27, 2018 at 4:09 PM Spencer Brackett
> <spbrackett20 at saintjosephhs.com> wrote:
> >
> > What is the significance of using / or \ ?
> >
> > On Thu, Dec 27, 2018 at 4:02 PM Sarah Goslee <sarah.goslee at gmail.com>
> wrote:
> >>
> >> On Thu, Dec 27, 2018 at 2:03 PM Spencer Brackett
> >> <spbrackett20 at saintjosephhs.com> wrote:
> >> >
> >> > Thank you for the help! I tried using the read.table command in my
> RStudio
> >> > using the following argument, and managed to open the file.
> >> >
> >> > GBM_protein_expression<-read.table(file.choose(), header=TRUE,
> sep=?/t?)
> >>
> >> Note that sep="/t" is NOT the same thing as the sep="\t" you were
> >> advised to use.
> >>
> >>
> >>
> >>
> >> >
> >> > However, my data did not unpack as yours did. I again only received a
> table
> >> > of true and flase distinctions per column, and my environment tab
> says that
> >> > there is 0 observations upon 0 variables.
> >> >
> >> > I believe I should be getting data similar to what you got, as it
> would
> >> > appear that your?s actually contains relevant gene/protein expression
> info.
> >> >
> >> > On Thu, Dec 27, 2018 at 6:21 AM Federico Calboli <
> >> > federico.calboli at kuleuven.be> wrote:
> >> >
> >> > > Once you have your TSV files just use something as
> >> > >
> >> > > x = read.table('protein_expression.tsv', h = T, sep = '\t')
> >> > >
> >> > > Do not copy paste the code of this email because it is formatted
> and would
> >> > > not work in R.
> >> > >
> >> > >
> >> > > Best
> >> > >
> >> > > F
> >> > >
> >> > > PS the data looks like this to me
> >> > >
> >> > > head(x)
> >> > >   icgc_donor_id project_code icgc_specimen_id icgc_sample_id
> >> > >  submitted_sample_id analysis_id        antibody_id gene_name
> >> > > 1       DO12370       GBM-US          SP26475       SA131594
> >> > > TCGA-19-5960-01A-13-1900-20       97765 14-3-3_epsilon-M-C     YWHAE
> >> > > 2       DO12370       GBM-US          SP26475       SA131594
> >> > > TCGA-19-5960-01A-13-1900-20       97765         4E-BP1-R-V  EIF4EBP1
> >> > > 3       DO12370       GBM-US          SP26475       SA131594
> >> > > TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pS65-R-V  EIF4EBP1
> >> > > 4       DO12370       GBM-US          SP26475       SA131594
> >> > > TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pT37-R-V  EIF4EBP1
> >> > > 5       DO12370       GBM-US          SP26475       SA131594
> >> > > TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pT70-R-C  EIF4EBP1
> >> > > 6       DO12370       GBM-US          SP26475       SA131594
> >> > > TCGA-19-5960-01A-13-1900-20       97765          53BP1-R-C   TP53BP1
> >> > >   gene_stable_id gene_build_version normalized_expression_level
> >> > > verification_status verification_platform
> >> > > 1             NA                 NA                  -1.1636330
> >> > > not tested                    NA
> >> > > 2             NA                 NA                  -1.7969721
> >> > > not tested                    NA
> >> > > 3             NA                 NA                  -0.7256390
> >> > > not tested                    NA
> >> > > 4             NA                 NA                   0.6498421
> >> > > not tested                    NA
> >> > > 5             NA                 NA                  -1.0262844
> >> > > not tested                    NA
> >> > > 6             NA                 NA                   1.5186400
> >> > > not tested                    NA
> >> > >                                         platform
> >> > > 1 M.D. Anderson Reverse Phase Protein Array Core
> >> > > 2 M.D. Anderson Reverse Phase Protein Array Core
> >> > > 3 M.D. Anderson Reverse Phase Protein Array Core
> >> > > 4 M.D. Anderson Reverse Phase Protein Array Core
> >> > > 5 M.D. Anderson Reverse Phase Protein Array Core
> >> > > 6 M.D. Anderson Reverse Phase Protein Array Core
> >> > >
> >> > >
> >> > >
> >> > > experimental_protocol
> >> > > 1 MDA_RPPA_Core
> >> > >
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> >> > > 2 MDA_RPPA_Core
> >> > >
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> >> > > 3 MDA_RPPA_Core
> >> > >
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> >> > > 4 MDA_RPPA_Core
> >> > >
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> >> > > 5 MDA_RPPA_Core
> >> > >
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> >> > > 6 MDA_RPPA_Core
> >> > >
> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
> >> > >   raw_data_repository          raw_data_accession
> >> > > 1                TCGA TCGA-19-5960-01A-13-1900-20
> >> > > 2                TCGA TCGA-19-5960-01A-13-1900-20
> >> > > 3                TCGA TCGA-19-5960-01A-13-1900-20
> >> > > 4                TCGA TCGA-19-5960-01A-13-1900-20
> >> > > 5                TCGA TCGA-19-5960-01A-13-1900-20
> >> > > 6                TCGA TCGA-19-5960-01A-13-1900-20
> >> > >
> >>
> >>
> >> --
> >> Sarah Goslee (she/her)
> >> http://www.numberwright.com
>

	[[alternative HTML version deleted]]


From @pbr@ckett20 @ending from @@intjo@ephh@@com  Thu Dec 27 22:57:33 2018
From: @pbr@ckett20 @ending from @@intjo@ephh@@com (Spencer Brackett)
Date: Thu, 27 Dec 2018 16:57:33 -0500
Subject: [R] you are making it far too difficult
In-Reply-To: <CAPQaxLOGzKGJbP+T-utVxf_CH=L21eTP5OH6-e+AepeAyiv+RA@mail.gmail.com>
References: <C727FD36-8DBD-4BC1-8F92-E60B1E05F03C@kuleuven.be>
 <CAPQaxLOgDhzX_njV_ukp4POR6PV89DiCjvMNMMwfj+cLsBFEmA@mail.gmail.com>
 <8773122F-1F09-4F43-9C9F-834610A96FED@kuleuven.be>
 <CAPQaxLOS8SQq+ONXL_W+4O+S4F-rTqvzqdxR-5dSYPTgfe-7Bg@mail.gmail.com>
 <CAGx1TMB6LHbR4NwRtU7E9+XsjUf3kMrKCkHEdevTwHxV292G+Q@mail.gmail.com>
 <CAPQaxLOGzKGJbP+T-utVxf_CH=L21eTP5OH6-e+AepeAyiv+RA@mail.gmail.com>
Message-ID: <CAPQaxLNHUS6WFF6_nsPOL_XLR-XXJwTyUYaOttWsFemhc21s8Q@mail.gmail.com>

For future reference,

I unpacked the full gene expression file from ICGC by directly downloading
the following link
protein_expression.GBM-US.tsv.gz, and then using the argument Mr. Heiberger
layed out.

Best,

Spencer Brackett

On Thu, Dec 27, 2018 at 4:30 PM Spencer Brackett <
spbrackett20 at saintjosephhs.com> wrote:

> Mr. Heiberger,
>
>   I followed your argument and it works. I received the same data. And
> yes, ICGC breakes their datasets into separate files based on data type.
> Thank you for the pointer on selecting all 50 rows, as I assumed that the
> entirety of gene expression data within the data set would be downloaded
> through the process you followed, as it does when directly downloading the
> file via a tsv.gz download.
>
> On Thu, Dec 27, 2018 at 4:05 PM Richard M. Heiberger <rmh at temple.edu>
> wrote:
>
>> I downloaded the Donors dataset
>>
>>
>> https://dcc.icgc.org/search?filters=%7B%22donor%22:%7B%22projectId%22:%7B%22is%22:%5B%22GBM-US%22%5D%7D,%22availableDataTypes%22:%7B%22is%22:%5B%22pexp%22%5D%7D%7D%7D
>>
>> by clicking "Export table as TSV".
>>
>> Then I read it with
>>
>> donors <- read.delim("~/Downloads/donors_2018_12_27_03_52_03.tsv")
>>
>> Here is the transcript.
>>
>> > donors <- read.delim("~/Downloads/donors_2018_12_27_03_52_03.tsv")
>> > donors
>>    Donor.ID Project.Code Primary.Site Gender Age.at.Diagnosis
>> 1   DO10892       GBM-US        Brain Female               45
>> 2   DO12328       GBM-US        Brain   Male               56
>> 3   DO11657       GBM-US        Brain Female               73
>> 4   DO13510       GBM-US        Brain Female               63
>> 5   DO12670       GBM-US        Brain Female               63
>> 6   DO11501       GBM-US        Brain Female               59
>> 7   DO13809       GBM-US        Brain Female               74
>> 8   DO13647       GBM-US        Brain   Male               56
>> 9   DO11645       GBM-US        Brain   Male               73
>> 10  DO14145       GBM-US        Brain Female               85
>>    Tumor.Stage.at.Diagnosis Survival.Time..days.  SSM CNSM  STSM   SGV
>> METH.A
>> 1                        NA                   NA True True False False
>>  True
>> 2                        NA                  154 True True False False
>>  True
>> 3                        NA                   NA True True False False
>>  True
>> 4                        NA                 1448 True True False False
>>  True
>> 5                        NA                  772 True True False False
>>  True
>> 6                        NA                   NA True True False False
>>  True
>> 7                        NA                  213 True True False False
>>  True
>> 8                        NA                  383 True True False False
>>  True
>> 9                        NA                  113 True True False False
>>  True
>> 10                       NA                   94 True True False False
>>  True
>>    METH.S EXP.A EXP.S PEXP miRNA.S   JCN Mutations Mutated.Genes
>> 1   False  True  True True   False False       269           392
>> 2   False  True False True   False False       192           263
>> 3   False  True False True   False False       128           209
>> 4   False  True  True True   False False       130           199
>> 5   False  True  True True   False False       142           194
>> 6   False  True  True True   False False       129           190
>> 7   False  True False True   False False       130           178
>> 8   False  True False True   False False       116           175
>> 9   False  True False True   False False       125           174
>> 10  False  True  True True   False False       108           169
>> >
>>
>> I don't know how to get the download of the whole file.  It looks like
>> you could page through it with the page menu at the bottom of the webpage.
>> If you do that, set it for 50 at a time instead of the default 10.
>>
>> For the Genes and the two types of Mutation files, it will be more
>> nuisance this way because there are about 10000 rows for each of those
>> three files, thus about 200 of these statements per dataset.
>>
>> I think it is time to move to the bioconductor list for specific guidance
>> on this type of dataset.
>>
>>
>> On Thu, Dec 27, 2018 at 3:28 PM Spencer Brackett <
>> spbrackett20 at saintjosephhs.com> wrote:
>>
>>> Mr. Calboli,
>>>
>>> After beginning to unpack the GBM file you sent me via directly importing
>>> it unit my console, I received the following:
>>>
>>> View(GBM_PEXP.tsv)
>>>
>>> **Note that I named the file GBM_PEXP.tsv)**
>>>
>>>   Upon downloading, my script now contains a 2 by 2 table, with the x
>>> column still containing encoded script. As for my Data summary to the
>>>  right, this new file reports that 2 objects are acting upon 1 variable.
>>> How should I proceed?
>>>
>>> -Spencer
>>>
>>> On Thu, Dec 27, 2018 at 3:12 PM Federico Calboli <
>>> federico.calboli at kuleuven.be> wrote:
>>>
>>> > Unpack these files.
>>> >
>>> > F
>>> >
>>> >
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>

	[[alternative HTML version deleted]]


From bioprogr@mmer @ending from gm@il@com  Thu Dec 27 22:58:39 2018
From: bioprogr@mmer @ending from gm@il@com (Caitlin Gibbons)
Date: Thu, 27 Dec 2018 14:58:39 -0700
Subject: [R] you are making it far too difficult
In-Reply-To: <CAPQaxLOkDZ-sMvGP2ftWoXuywBakNQ3ajiHvzH2kVkLqj6MjTQ@mail.gmail.com>
References: <C727FD36-8DBD-4BC1-8F92-E60B1E05F03C@kuleuven.be>
 <CAPQaxLOgDhzX_njV_ukp4POR6PV89DiCjvMNMMwfj+cLsBFEmA@mail.gmail.com>
 <CAM_vjukUQ4hKRC+v77gShb+_sHgU9SiXNTqC83XeXesAHZCx-g@mail.gmail.com>
 <CAPQaxLPh3fNXaoc=nnkN1BBegd-sWZRVx8S_qUjoPYWf2jcYCw@mail.gmail.com>
 <08D26B35-0F29-4EE6-8B5C-3075C694EA5B@gmail.com>
 <CAPQaxLOkDZ-sMvGP2ftWoXuywBakNQ3ajiHvzH2kVkLqj6MjTQ@mail.gmail.com>
Message-ID: <E496D1BC-9BC8-4E4F-A776-4FCC928C2DB7@gmail.com>

I would :)


Sent from my iPhone

> On Dec 27, 2018, at 2:34 PM, Spencer Brackett <spbrackett20 at saintjosephhs.com> wrote:
> 
> So I should use ?\t? proceeding on? 
> 
>> On Thu, Dec 27, 2018 at 4:30 PM Caitlin Gibbons <bioprogrammer at gmail.com> wrote:
>> ?\t? is an escape sequence which signifies one tab character. ?/t? is NOT an escape sequence, and to R, looks like a very brief file path. 
>> 
>> Sent from my iPhone
>> 
>> > On Dec 27, 2018, at 2:09 PM, Spencer Brackett <spbrackett20 at saintjosephhs.com> wrote:
>> > 
>> > What is the significance of using / or \ ?
>> > 
>> >> On Thu, Dec 27, 2018 at 4:02 PM Sarah Goslee <sarah.goslee at gmail.com> wrote:
>> >> 
>> >> On Thu, Dec 27, 2018 at 2:03 PM Spencer Brackett
>> >> <spbrackett20 at saintjosephhs.com> wrote:
>> >>> 
>> >>> Thank you for the help! I tried using the read.table command in my
>> >> RStudio
>> >>> using the following argument, and managed to open the file.
>> >>> 
>> >>> GBM_protein_expression<-read.table(file.choose(), header=TRUE, sep=?/t?)
>> >> 
>> >> Note that sep="/t" is NOT the same thing as the sep="\t" you were
>> >> advised to use.
>> >> 
>> >> 
>> >> 
>> >> 
>> >>> 
>> >>> However, my data did not unpack as yours did. I again only received a
>> >> table
>> >>> of true and flase distinctions per column, and my environment tab says
>> >> that
>> >>> there is 0 observations upon 0 variables.
>> >>> 
>> >>> I believe I should be getting data similar to what you got, as it would
>> >>> appear that your?s actually contains relevant gene/protein expression
>> >> info.
>> >>> 
>> >>> On Thu, Dec 27, 2018 at 6:21 AM Federico Calboli <
>> >>> federico.calboli at kuleuven.be> wrote:
>> >>> 
>> >>>> Once you have your TSV files just use something as
>> >>>> 
>> >>>> x = read.table('protein_expression.tsv', h = T, sep = '\t')
>> >>>> 
>> >>>> Do not copy paste the code of this email because it is formatted and
>> >> would
>> >>>> not work in R.
>> >>>> 
>> >>>> 
>> >>>> Best
>> >>>> 
>> >>>> F
>> >>>> 
>> >>>> PS the data looks like this to me
>> >>>> 
>> >>>> head(x)
>> >>>>  icgc_donor_id project_code icgc_specimen_id icgc_sample_id
>> >>>> submitted_sample_id analysis_id        antibody_id gene_name
>> >>>> 1       DO12370       GBM-US          SP26475       SA131594
>> >>>> TCGA-19-5960-01A-13-1900-20       97765 14-3-3_epsilon-M-C     YWHAE
>> >>>> 2       DO12370       GBM-US          SP26475       SA131594
>> >>>> TCGA-19-5960-01A-13-1900-20       97765         4E-BP1-R-V  EIF4EBP1
>> >>>> 3       DO12370       GBM-US          SP26475       SA131594
>> >>>> TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pS65-R-V  EIF4EBP1
>> >>>> 4       DO12370       GBM-US          SP26475       SA131594
>> >>>> TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pT37-R-V  EIF4EBP1
>> >>>> 5       DO12370       GBM-US          SP26475       SA131594
>> >>>> TCGA-19-5960-01A-13-1900-20       97765    4E-BP1_pT70-R-C  EIF4EBP1
>> >>>> 6       DO12370       GBM-US          SP26475       SA131594
>> >>>> TCGA-19-5960-01A-13-1900-20       97765          53BP1-R-C   TP53BP1
>> >>>>  gene_stable_id gene_build_version normalized_expression_level
>> >>>> verification_status verification_platform
>> >>>> 1             NA                 NA                  -1.1636330
>> >>>> not tested                    NA
>> >>>> 2             NA                 NA                  -1.7969721
>> >>>> not tested                    NA
>> >>>> 3             NA                 NA                  -0.7256390
>> >>>> not tested                    NA
>> >>>> 4             NA                 NA                   0.6498421
>> >>>> not tested                    NA
>> >>>> 5             NA                 NA                  -1.0262844
>> >>>> not tested                    NA
>> >>>> 6             NA                 NA                   1.5186400
>> >>>> not tested                    NA
>> >>>>                                        platform
>> >>>> 1 M.D. Anderson Reverse Phase Protein Array Core
>> >>>> 2 M.D. Anderson Reverse Phase Protein Array Core
>> >>>> 3 M.D. Anderson Reverse Phase Protein Array Core
>> >>>> 4 M.D. Anderson Reverse Phase Protein Array Core
>> >>>> 5 M.D. Anderson Reverse Phase Protein Array Core
>> >>>> 6 M.D. Anderson Reverse Phase Protein Array Core
>> >>>> 
>> >>>> 
>> >>>> 
>> >>>> experimental_protocol
>> >>>> 1 MDA_RPPA_Core
>> >>>> 
>> >> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
>> >>>> 2 MDA_RPPA_Core
>> >>>> 
>> >> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
>> >>>> 3 MDA_RPPA_Core
>> >>>> 
>> >> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
>> >>>> 4 MDA_RPPA_Core
>> >>>> 
>> >> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
>> >>>> 5 MDA_RPPA_Core
>> >>>> 
>> >> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
>> >>>> 6 MDA_RPPA_Core
>> >>>> 
>> >> http://tcga-data.nci.nih.gov/tcgafiles/ftp_auth/distro_ftpusers/anonymous/tumor/blca/cgcc/mdanderson.org/mda_rppa_core/protein_exp/mdanderson.org_BLCA.MDA_RPPA_Core.mage-tab.1.7.0/mdanderson.org_BLCA.MDA_RPPA_Core.idf.txt
>> >>>>  raw_data_repository          raw_data_accession
>> >>>> 1                TCGA TCGA-19-5960-01A-13-1900-20
>> >>>> 2                TCGA TCGA-19-5960-01A-13-1900-20
>> >>>> 3                TCGA TCGA-19-5960-01A-13-1900-20
>> >>>> 4                TCGA TCGA-19-5960-01A-13-1900-20
>> >>>> 5                TCGA TCGA-19-5960-01A-13-1900-20
>> >>>> 6                TCGA TCGA-19-5960-01A-13-1900-20
>> >>>> 
>> >> 
>> >> 
>> >> --
>> >> Sarah Goslee (she/her)
>> >> http://www.numberwright.com
>> >> 
>> > 
>> >    [[alternative HTML version deleted]]
>> > 
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From m@rongiu@luigi @ending from gm@il@com  Fri Dec 28 14:28:06 2018
From: m@rongiu@luigi @ending from gm@il@com (Luigi Marongiu)
Date: Fri, 28 Dec 2018 14:28:06 +0100
Subject: [R] Kernlab Error in .local(x, ...) when plot model
Message-ID: <CAMk+s2SDvMm_Q+zEhPthfDRkT++zCR9qOz2qsjuijNFu4UwSLw@mail.gmail.com>

Dear all,
I have generated a model with KERNLAB using the following steps:
the data is a dataframe df of two numerical variables x and y, and
each point is assigned a z value that is a factor of two levels:
positive and negative. The data has the strucutre:
> str(df)
'data.frame': 1574 obs. of 3 variables:
$y : num ...
$x : num ...
$x : Factor w/ 2 levels "negative", "positive"...

I set the model with:
mod = ksvm(z ~., data = df, type = "C-bscv", kernel = "rbfdot", kpar =
"automatic", C = 10, prob.model  = TRUE)
which gives me:
> mod
Support Vector Machine object of class "ksvm"

SV type: S-bsvc (classification)
parameter: cost  C = 10
Gaussian Radial basis kernel function.
Hyperparameter: sigma = 45.69...

Number of support vectors: 1436

Objective function value : -12792.1
Training error: 0.388818
Probability model included

But when I try to plot the model I get:
>plot(mod, data = df) OR kernlab::plot(mod, data = df)
Error in .local(x, ...) :
Only plots of classification ksvm object supported

What did I get wrong?
Thank you

-- 
Best regards,
Luigi


From Angu@@Hewitt @ending from dhh@@vic@gov@@u  Fri Dec 28 01:36:42 2018
From: Angu@@Hewitt @ending from dhh@@vic@gov@@u (Angus C Hewitt (DHHS))
Date: Fri, 28 Dec 2018 00:36:42 +0000
Subject: [R] Proposed changes to the R Language
Message-ID: <SYCPR01MB4046920F86CF40C126DC1B9886B70@SYCPR01MB4046.ausprd01.prod.outlook.com>

Hi Team,

Please advise if there are any plans in the pipeline for change the R language to "B"  as proposed in the below mentioned statistical series help at Auckland Uni.

https://www.youtube.com/watch?v=88TftllIjaY


Kind Regards,

Angus Hewitt
Senior Analyst | Decision Support
System Design, Planning & Decision Making |  Health & Well Being
Department of Health and Human Services | 19th floor, 50 Lonsdale Street, Melbourne Victoria 3000
t. 9096 5859  | m. 0468 364 744 | e. Angus.Hewitt at dhhs.vic.gov.au



	[[alternative HTML version deleted]]


From jdnewmil @ending from dcn@d@vi@@c@@u@  Fri Dec 28 16:13:14 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Fri, 28 Dec 2018 07:13:14 -0800
Subject: [R] Proposed changes to the R Language
In-Reply-To: <SYCPR01MB4046920F86CF40C126DC1B9886B70@SYCPR01MB4046.ausprd01.prod.outlook.com>
References: <SYCPR01MB4046920F86CF40C126DC1B9886B70@SYCPR01MB4046.ausprd01.prod.outlook.com>
Message-ID: <23282FF2-A23F-47D1-B2AE-DE8F6B453DB1@dcn.davis.ca.us>

I have no idea. But I hope not... that sounds like a different tool than R, just as C++ is a different tool than C.

On December 27, 2018 4:36:42 PM PST, "Angus C Hewitt (DHHS)" <Angus.Hewitt at dhhs.vic.gov.au> wrote:
>Hi Team,
>
>Please advise if there are any plans in the pipeline for change the R
>language to "B"  as proposed in the below mentioned statistical series
>help at Auckland Uni.
>
>https://www.youtube.com/watch?v=88TftllIjaY
>
>
>Kind Regards,
>
>Angus Hewitt
>Senior Analyst | Decision Support
>System Design, Planning & Decision Making |  Health & Well Being
>Department of Health and Human Services | 19th floor, 50 Lonsdale
>Street, Melbourne Victoria 3000
>t. 9096 5859  | m. 0468 364 744 | e. Angus.Hewitt at dhhs.vic.gov.au
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From bgunter@4567 @ending from gm@il@com  Fri Dec 28 17:04:48 2018
From: bgunter@4567 @ending from gm@il@com (Bert Gunter)
Date: Fri, 28 Dec 2018 08:04:48 -0800
Subject: [R] Proposed changes to the R Language
In-Reply-To: <23282FF2-A23F-47D1-B2AE-DE8F6B453DB1@dcn.davis.ca.us>
References: <SYCPR01MB4046920F86CF40C126DC1B9886B70@SYCPR01MB4046.ausprd01.prod.outlook.com>
 <23282FF2-A23F-47D1-B2AE-DE8F6B453DB1@dcn.davis.ca.us>
Message-ID: <CAGxFJbRsfcmm672irYZ1F=k6oz8Q6VO+d666F9d-2ZOkws-AuQ@mail.gmail.com>

I would assume r-devel is where this sort of query should be posted as we
mere users have nothing to say about this.

However, I've seen discussions and talks about better languages for
scientific (but data science?) programming -- Matlab, Julia, Scipy, etc. --
for at least a decade. But with a library of now over 10,000 packages on
CRAN and yet more on Bioconductor and github -- that's a lot of inertia to
overcome.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Dec 28, 2018 at 7:13 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> I have no idea. But I hope not... that sounds like a different tool than
> R, just as C++ is a different tool than C.
>
> On December 27, 2018 4:36:42 PM PST, "Angus C Hewitt (DHHS)" <
> Angus.Hewitt at dhhs.vic.gov.au> wrote:
> >Hi Team,
> >
> >Please advise if there are any plans in the pipeline for change the R
> >language to "B"  as proposed in the below mentioned statistical series
> >help at Auckland Uni.
> >
> >https://www.youtube.com/watch?v=88TftllIjaY
> >
> >
> >Kind Regards,
> >
> >Angus Hewitt
> >Senior Analyst | Decision Support
> >System Design, Planning & Decision Making |  Health & Well Being
> >Department of Health and Human Services | 19th floor, 50 Lonsdale
> >Street, Melbourne Victoria 3000
> >t. 9096 5859  | m. 0468 364 744 | e. Angus.Hewitt at dhhs.vic.gov.au
> >
> >
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ripon17271 @ending from gm@il@com  Fri Dec 28 04:52:56 2018
From: ripon17271 @ending from gm@il@com (Ripon Saha)
Date: Fri, 28 Dec 2018 12:52:56 +0900
Subject: [R] Regression analysis result compare
Message-ID: <CAMOad-E0zacLTy8S9z5gTq-VPdtKdRKoOJpwkGMrnZF_SOPeyg@mail.gmail.com>

Dear R-help team,
Good afternoon. I need your help regarding the attached file. My questions
are:
1. Is my result analysis right?
2. How can I compare the result between this single and multiple regression?
With thanks and best regards.
-- 
Ripon Kumer Saha
Student of Masters Program in Economic Faculty
Yamaguchi University
Mobile: 080-4916-3453

From jdnewmil @ending from dcn@d@vi@@c@@u@  Fri Dec 28 20:44:15 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Fri, 28 Dec 2018 11:44:15 -0800
Subject: [R] Regression analysis result compare
In-Reply-To: <CAMOad-E0zacLTy8S9z5gTq-VPdtKdRKoOJpwkGMrnZF_SOPeyg@mail.gmail.com>
References: <CAMOad-E0zacLTy8S9z5gTq-VPdtKdRKoOJpwkGMrnZF_SOPeyg@mail.gmail.com>
Message-ID: <AEDF480B-8EEB-4C9A-8D52-65BA0111093F@dcn.davis.ca.us>

You have applied to an inappropriate forum using an inappropriate communication format for your question.

You should read the Posting Guide to fill in your misunderstanding for future use of this from, but more immediately you should check out the CrossValidated web site for help regarding how statistics analysis should be conducted.

On December 27, 2018 7:52:56 PM PST, Ripon Saha <ripon17271 at gmail.com> wrote:
>Dear R-help team,
>Good afternoon. I need your help regarding the attached file. My
>questions
>are:
>1. Is my result analysis right?
>2. How can I compare the result between this single and multiple
>regression?
>With thanks and best regards.

-- 
Sent from my phone. Please excuse my brevity.


From jkfboger@ @ending from gm@il@com  Sat Dec 29 19:48:57 2018
From: jkfboger@ @ending from gm@il@com (jacob bogers)
Date: Sat, 29 Dec 2018 19:48:57 +0100
Subject: [R] Proposed changes to the R Language
In-Reply-To: <SYCPR01MB4046920F86CF40C126DC1B9886B70@SYCPR01MB4046.ausprd01.prod.outlook.com>
References: <SYCPR01MB4046920F86CF40C126DC1B9886B70@SYCPR01MB4046.ausprd01.prod.outlook.com>
Message-ID: <CALuwgN5S0Qz5QQOvm4Unu05=h51_4Mf3KCNMdy1FD2d_dcHC_A@mail.gmail.com>

It has some good ideas, but R (my personal assesment) is not build for
superspeed, but for superease of use.))


On Fri, Dec 28, 2018 at 3:01 PM Angus C Hewitt (DHHS) <
Angus.Hewitt at dhhs.vic.gov.au> wrote:

> Hi Team,
>
> Please advise if there are any plans in the pipeline for change the R
> language to "B"  as proposed in the below mentioned statistical series help
> at Auckland Uni.
>
> https://www.youtube.com/watch?v=88TftllIjaY
>
>
> Kind Regards,
>
> Angus Hewitt
> Senior Analyst | Decision Support
> System Design, Planning & Decision Making |  Health & Well Being
> Department of Health and Human Services | 19th floor, 50 Lonsdale Street,
> Melbourne Victoria 3000
> t. 9096 5859  | m. 0468 364 744 | e. Angus.Hewitt at dhhs.vic.gov.au
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@rc_grt @ending from y@hoo@fr  Sun Dec 30 05:31:31 2018
From: m@rc_grt @ending from y@hoo@fr (Marc Girondot)
Date: Sun, 30 Dec 2018 05:31:31 +0100
Subject: [R] SE for all fixed factor effect in GLMM
Message-ID: <9f92eb28-8fc6-9a34-092a-a08757bb1d74@yahoo.fr>

Dear members,

Let do a example of simple GLMM with x and G as fixed factors and R as 
random factor:

(note that question is the same with GLM or even LM):

x <- rnorm(100)
y <- rnorm(100)
G <- as.factor(sample(c("A", "B", "C", "D"), 100, replace = TRUE))
R <- as.factor(rep(1:25, 4))

library(lme4)

m <- lmer(y ~ x + G + (1 | R))
summary(m)$coefficients

I get the fixed effect fit and their SE

 > summary(m)$coefficients
 ?????????????? Estimate Std. Error??? t value
(Intercept)? 0.07264454? 0.1952380? 0.3720820
x?????????? -0.02519892? 0.1238621 -0.2034433
GB?????????? 0.10969225? 0.3118371? 0.3517614
GC????????? -0.09771555? 0.2705523 -0.3611706
GD????????? -0.12944760? 0.2740012 -0.4724344

The estimate for GA is not shown as it is fixed to 0. Normal, it is the 
reference level.

But is there a way to get SE for GA of is-it non-sense question because 
GA is fixed to 0 ?

______________

I propose here a solution but I don't know if it is correct. It is based 
on reordering levels and averaging se for all reordering:

G <- relevel(G, "A")
m <- lmer(y ~ x + G + (1 | R))
sA <- summary(m)$coefficients

G <- relevel(G, "B")
m <- lmer(y ~ x + G + (1 | R))
sB <- summary(m)$coefficients

G <- relevel(G, "C")
m <- lmer(y ~ x + G + (1 | R))
sC <- summary(m)$coefficients

G <- relevel(G, "D")
m <- lmer(y ~ x + G + (1 | R))
sD <- summary(m)$coefficients

seA <- mean(sB["GA", "Std. Error"], sC["GA", "Std. Error"], sD["GA", 
"Std. Error"])
seB <- mean(sA["GB", "Std. Error"], sC["GB", "Std. Error"], sD["GB", 
"Std. Error"])
seC <- mean(sA["GC", "Std. Error"], sB["GC", "Std. Error"], sD["GC", 
"Std. Error"])
seD <- mean(sA["GD", "Std. Error"], sB["GD", "Std. Error"], sC["GD", 
"Std. Error"])

seA; seB; seC; seD


Thanks,

Marc


From r@turner @ending from @uckl@nd@@c@nz  Sun Dec 30 06:11:55 2018
From: r@turner @ending from @uckl@nd@@c@nz (Rolf Turner)
Date: Sun, 30 Dec 2018 18:11:55 +1300
Subject: [R] [FORGED]  SE for all fixed factor effect in GLMM
In-Reply-To: <9f92eb28-8fc6-9a34-092a-a08757bb1d74@yahoo.fr>
References: <9f92eb28-8fc6-9a34-092a-a08757bb1d74@yahoo.fr>
Message-ID: <8946b486-47e7-fd34-c373-e32bd893b536@auckland.ac.nz>


On 12/30/18 5:31 PM, Marc Girondot via R-help wrote:

> Dear members,
> 
> Let do a example of simple GLMM with x and G as fixed factors and R as 
> random factor:
> 
> (note that question is the same with GLM or even LM):
> 
> x <- rnorm(100)
> y <- rnorm(100)
> G <- as.factor(sample(c("A", "B", "C", "D"), 100, replace = TRUE))
> R <- as.factor(rep(1:25, 4))
> 
> library(lme4)
> 
> m <- lmer(y ~ x + G + (1 | R))
> summary(m)$coefficients
> 
> I get the fixed effect fit and their SE
> 
>  > summary(m)$coefficients
>  ?????????????? Estimate Std. Error??? t value
> (Intercept)? 0.07264454? 0.1952380? 0.3720820
> x?????????? -0.02519892? 0.1238621 -0.2034433
> GB?????????? 0.10969225? 0.3118371? 0.3517614
> GC????????? -0.09771555? 0.2705523 -0.3611706
> GD????????? -0.12944760? 0.2740012 -0.4724344
> 
> The estimate for GA is not shown as it is fixed to 0. Normal, it is the 
> reference level.
> 
> But is there a way to get SE for GA of is-it non-sense question because 
> GA is fixed to 0 ?

In a way, yes it's a nonsense question, as you say.

If you really want an SE for GA then re-parametrise so that GA is 
meaningful:

m2 <- lmer(y ~ x + 0 + G + (1 | R))

Note that with this formulation GA will be there, "(Intercept)" will 
disappear, and GB, GC and GD will now mean something different.

GA from m2 = (Intercept) from m
GB from m2 = (Intercept) + GB from m
GC from m2 = (Intercept) + GC from m
GD from m2 = (Intercept) + GD from m

I haven't followed what you've done below, but I think that you are 
making things unnecessarily complicated and life difficult for yourself.

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276
> 
> ______________
> 
> I propose here a solution but I don't know if it is correct. It is based 
> on reordering levels and averaging se for all reordering:
> 
> G <- relevel(G, "A")
> m <- lmer(y ~ x + G + (1 | R))
> sA <- summary(m)$coefficients
> 
> G <- relevel(G, "B")
> m <- lmer(y ~ x + G + (1 | R))
> sB <- summary(m)$coefficients
> 
> G <- relevel(G, "C")
> m <- lmer(y ~ x + G + (1 | R))
> sC <- summary(m)$coefficients
> 
> G <- relevel(G, "D")
> m <- lmer(y ~ x + G + (1 | R))
> sD <- summary(m)$coefficients
> 
> seA <- mean(sB["GA", "Std. Error"], sC["GA", "Std. Error"], sD["GA", 
> "Std. Error"])
> seB <- mean(sA["GB", "Std. Error"], sC["GB", "Std. Error"], sD["GB", 
> "Std. Error"])
> seC <- mean(sA["GC", "Std. Error"], sB["GC", "Std. Error"], sD["GC", 
> "Std. Error"])
> seD <- mean(sA["GD", "Std. Error"], sB["GD", "Std. Error"], sC["GD", 
> "Std. Error"])
> 
> seA; seB; seC; seD
> 
> 
> Thanks,
> 
> Marc


From jdnewmil @ending from dcn@d@vi@@c@@u@  Sun Dec 30 07:57:50 2018
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Sat, 29 Dec 2018 22:57:50 -0800
Subject: [R] Proposed changes to the R Language
In-Reply-To: <PS2P216MB0964FD2C8C4C3622ABD6D0C0EDB10@PS2P216MB0964.KORP216.PROD.OUTLOOK.COM>
References: <SYCPR01MB4046920F86CF40C126DC1B9886B70@SYCPR01MB4046.ausprd01.prod.outlook.com>,
 <23282FF2-A23F-47D1-B2AE-DE8F6B453DB1@dcn.davis.ca.us>
 <PS2P216MB0964FD2C8C4C3622ABD6D0C0EDB10@PS2P216MB0964.KORP216.PROD.OUTLOOK.COM>
Message-ID: <681AC01A-FFCE-4425-B33F-933E891944D6@dcn.davis.ca.us>

I don't think you can assume that any of this _research_ was ever on any R-core burner at all. But as Bert said, this would not be where such discussion occurred if it was... go to r-devel to find out, or better yet contact the researchers themselves.

On December 29, 2018 9:53:01 PM PST, angus hewitt <angus_hewitt at hotmail.com> wrote:
>Hi Jeff,
>
>The suggested language looks like it is shifting to some stricter
>programming idioms such as defining object's data structure. this video
>was shoot in 2017 so I'm not sure if these changes have been put on the
>back burier.
>
>AH
>________________________________
>From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>Sent: Friday, 28 December 2018 3:13 PM
>To: r-help at r-project.org; Angus C Hewitt (DHHS)
>Cc: angus_hewitt at hotmail.com
>Subject: Re: [R] Proposed changes to the R Language
>
>I have no idea. But I hope not... that sounds like a different tool
>than R, just as C++ is a different tool than C.
>
>On December 27, 2018 4:36:42 PM PST, "Angus C Hewitt (DHHS)"
><Angus.Hewitt at dhhs.vic.gov.au> wrote:
>>Hi Team,
>>
>>Please advise if there are any plans in the pipeline for change the R
>>language to "B"  as proposed in the below mentioned statistical series
>>help at Auckland Uni.
>>
>>https://www.youtube.com/watch?v=88TftllIjaY
>>
>>
>>Kind Regards,
>>
>>Angus Hewitt
>>Senior Analyst | Decision Support
>>System Design, Planning & Decision Making |  Health & Well Being
>>Department of Health and Human Services | 19th floor, 50 Lonsdale
>>Street, Melbourne Victoria 3000
>>t. 9096 5859  | m. 0468 364 744 | e. Angus.Hewitt at dhhs.vic.gov.au
>>
>>
>>
>>       [[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
>--
>Sent from my phone. Please excuse my brevity.

-- 
Sent from my phone. Please excuse my brevity.


From tuechler @ending from gmx@@t  Sun Dec 30 08:58:09 2018
From: tuechler @ending from gmx@@t (Heinz Tuechler)
Date: Sun, 30 Dec 2018 08:58:09 +0100
Subject: [R] SE for all fixed factor effect in GLMM
In-Reply-To: <9f92eb28-8fc6-9a34-092a-a08757bb1d74@yahoo.fr>
References: <9f92eb28-8fc6-9a34-092a-a08757bb1d74@yahoo.fr>
Message-ID: <8b093767-cc9f-8340-a49a-156a2e9aef2a@gmx.at>

maybe qvcalc https://cran.r-project.org/web/packages/qvcalc/index.html 
is useful for you.

Marc Girondot via R-help wrote/hat geschrieben on/am 30.12.2018 05:31:
> Dear members,
>
> Let do a example of simple GLMM with x and G as fixed factors and R as
> random factor:
>
> (note that question is the same with GLM or even LM):
>
> x <- rnorm(100)
> y <- rnorm(100)
> G <- as.factor(sample(c("A", "B", "C", "D"), 100, replace = TRUE))
> R <- as.factor(rep(1:25, 4))
>
> library(lme4)
>
> m <- lmer(y ~ x + G + (1 | R))
> summary(m)$coefficients
>
> I get the fixed effect fit and their SE
>
>> summary(m)$coefficients
>                Estimate Std. Error    t value
> (Intercept)  0.07264454  0.1952380  0.3720820
> x           -0.02519892  0.1238621 -0.2034433
> GB           0.10969225  0.3118371  0.3517614
> GC          -0.09771555  0.2705523 -0.3611706
> GD          -0.12944760  0.2740012 -0.4724344
>
> The estimate for GA is not shown as it is fixed to 0. Normal, it is the
> reference level.
>
> But is there a way to get SE for GA of is-it non-sense question because
> GA is fixed to 0 ?
>
> ______________
>
> I propose here a solution but I don't know if it is correct. It is based
> on reordering levels and averaging se for all reordering:
>
> G <- relevel(G, "A")
> m <- lmer(y ~ x + G + (1 | R))
> sA <- summary(m)$coefficients
>
> G <- relevel(G, "B")
> m <- lmer(y ~ x + G + (1 | R))
> sB <- summary(m)$coefficients
>
> G <- relevel(G, "C")
> m <- lmer(y ~ x + G + (1 | R))
> sC <- summary(m)$coefficients
>
> G <- relevel(G, "D")
> m <- lmer(y ~ x + G + (1 | R))
> sD <- summary(m)$coefficients
>
> seA <- mean(sB["GA", "Std. Error"], sC["GA", "Std. Error"], sD["GA",
> "Std. Error"])
> seB <- mean(sA["GB", "Std. Error"], sC["GB", "Std. Error"], sD["GB",
> "Std. Error"])
> seC <- mean(sA["GC", "Std. Error"], sB["GC", "Std. Error"], sD["GC",
> "Std. Error"])
> seD <- mean(sA["GD", "Std. Error"], sB["GD", "Std. Error"], sC["GD",
> "Std. Error"])
>
> seA; seB; seC; seD
>
>
> Thanks,
>
> Marc
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From profjcn@@h @ending from gm@il@com  Sun Dec 30 15:15:24 2018
From: profjcn@@h @ending from gm@il@com (J C Nash)
Date: Sun, 30 Dec 2018 09:15:24 -0500
Subject: [R] Proposed changes to the R Language
In-Reply-To: <681AC01A-FFCE-4425-B33F-933E891944D6@dcn.davis.ca.us>
References: <SYCPR01MB4046920F86CF40C126DC1B9886B70@SYCPR01MB4046.ausprd01.prod.outlook.com>
 <23282FF2-A23F-47D1-B2AE-DE8F6B453DB1@dcn.davis.ca.us>
 <PS2P216MB0964FD2C8C4C3622ABD6D0C0EDB10@PS2P216MB0964.KORP216.PROD.OUTLOOK.COM>
 <681AC01A-FFCE-4425-B33F-933E891944D6@dcn.davis.ca.us>
Message-ID: <55c97178-bd4a-f6c0-5298-d784d6edf6ab@gmail.com>

My friend Morven Gentleman who died recently was for some time chair of the computer
faculty at Waterloo and (Fortune nomination!) once said "The response of many computer
scientists to any problem is to invent a new programming language."

Looking at Ross Ihaka's video, I got the impression he wants to preserve R syntax as
much as possible while improving speed and predictability. His example

  x=10
  f=function(){
   if (runif(1) > .5) x=20
   x
  }

where the local/global status of x is unclear underlines just one issue.

The temptation with programming languages is to allow them to expand. That makes
it very much more expensive and difficult to preserve the package ecosystem in a
healthy and verifiable state. A robust dialog between users and language
developers is, I believe, the best way to move to a streamlining of R. If -- and
that is the big question -- Ross' ideas are workable to provide a simplified
R (whatever we call it) that allows a high proportion of existing codes to run
satisfactorily or with minimal/automated changes, the R community would benefit.


John Nash


From @ngu@_hewitt @ending from hotm@il@com  Sun Dec 30 06:53:01 2018
From: @ngu@_hewitt @ending from hotm@il@com (angus hewitt)
Date: Sun, 30 Dec 2018 05:53:01 +0000
Subject: [R] Proposed changes to the R Language
In-Reply-To: <23282FF2-A23F-47D1-B2AE-DE8F6B453DB1@dcn.davis.ca.us>
References: <SYCPR01MB4046920F86CF40C126DC1B9886B70@SYCPR01MB4046.ausprd01.prod.outlook.com>,
 <23282FF2-A23F-47D1-B2AE-DE8F6B453DB1@dcn.davis.ca.us>
Message-ID: <PS2P216MB0964FD2C8C4C3622ABD6D0C0EDB10@PS2P216MB0964.KORP216.PROD.OUTLOOK.COM>

Hi Jeff,

The suggested language looks like it is shifting to some stricter programming idioms such as defining object's data structure. this video was shoot in 2017 so I'm not sure if these changes have been put on the back burier.

AH
________________________________
From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
Sent: Friday, 28 December 2018 3:13 PM
To: r-help at r-project.org; Angus C Hewitt (DHHS)
Cc: angus_hewitt at hotmail.com
Subject: Re: [R] Proposed changes to the R Language

I have no idea. But I hope not... that sounds like a different tool than R, just as C++ is a different tool than C.

On December 27, 2018 4:36:42 PM PST, "Angus C Hewitt (DHHS)" <Angus.Hewitt at dhhs.vic.gov.au> wrote:
>Hi Team,
>
>Please advise if there are any plans in the pipeline for change the R
>language to "B"  as proposed in the below mentioned statistical series
>help at Auckland Uni.
>
>https://www.youtube.com/watch?v=88TftllIjaY
>
>
>Kind Regards,
>
>Angus Hewitt
>Senior Analyst | Decision Support
>System Design, Planning & Decision Making |  Health & Well Being
>Department of Health and Human Services | 19th floor, 50 Lonsdale
>Street, Melbourne Victoria 3000
>t. 9096 5859  | m. 0468 364 744 | e. Angus.Hewitt at dhhs.vic.gov.au
>
>
>
>       [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.

	[[alternative HTML version deleted]]


From @@himk@poor @ending from gm@il@com  Mon Dec 31 07:54:47 2018
From: @@himk@poor @ending from gm@il@com (Ashim Kapoor)
Date: Mon, 31 Dec 2018 12:24:47 +0530
Subject: [R] Reproducing the results from package dlm
Message-ID: <CAC8=1erzu7uX_pV8dPZ9W6c5==DXNdeNw-T6OKE0yXX2-XeFAw@mail.gmail.com>

Dear all,

I have a query with regards to the package dlm. My query is : will dlm
return the same results if I give it the same data set ?

Here is a MWE ( created from :
https://sites.ualberta.ca/~sfossati/e509/files/other/dlm_ex.R )

library(dlm)

# simulate AR(1) process
# phi = .8, sig2 = .25
nobs <- 250
yt <- arima.sim(n=nobs,list(ar=.8,ma=0),sd=.5)

# estimate AR(1) for comparison
model10 <- Arima(yt,order=c(1,0,0),method="ML",include.mean=FALSE)
model10

# set parameter restrictions
parm_rest <- function(parm){
     return( c(exp(parm[1])/(1+exp(parm[1])),exp(parm[2])) )
    }

# set up SS model
ssm1 <- function(parm){
    parm <- parm_rest(parm)
    return( dlm(FF=1,V=0,GG=parm[1],W=parm[2],
                m0=0,C0=solve(1-parm[1]^2)*parm[2]) )
    }
# estimate parameters
fit1 <- dlmMLE(y=yt,parm=c(0,1),build=ssm1,hessian=T)

# get estimates
coef <- parm_rest(fit1$par)
# get standard errors using delta method
dg1 <- exp(fit1$par[1])/(1+exp(fit1$par[1]))^2
dg2 <- exp(fit1$par[2])
dg <- diag(c(dg1,dg2))
var <- dg%*%solve(fit1$hessian)%*%dg
# print results
coef; sqrt(diag(var))

# Here are 2 runs of the latter part of the code: -
> fit1 <- dlmMLE(y=yt,parm=c(0,1),build=ssm1,hessian=T)
>
> # get estimates
> coef <- parm_rest(fit1$par)
> # get standard errors using delta method
> dg1 <- exp(fit1$par[1])/(1+exp(fit1$par[1]))^2
> dg2 <- exp(fit1$par[2])
> dg <- diag(c(dg1,dg2))
> var <- dg%*%solve(fit1$hessian)%*%dg
> # print results
> coef; sqrt(diag(var))
[1] 0.8442885 0.2513462
[1] 0.03361245 0.02248196
> fit1 <- dlmMLE(y=yt,parm=c(0,1),build=ssm1,hessian=T)
>
> # get estimates
> coef <- parm_rest(fit1$par)
> # get standard errors using delta method
> dg1 <- exp(fit1$par[1])/(1+exp(fit1$par[1]))^2
> dg2 <- exp(fit1$par[2])
> dg <- diag(c(dg1,dg2))
> var <- dg%*%solve(fit1$hessian)%*%dg
> # print results
> coef; sqrt(diag(var))
[1] 0.8442885 0.2513462
[1] 0.03361245 0.02248196
>

Seems to me that even without set.seed, dlm reports the SAME results. Is
that true? I have written a big program where I have slightly different
results from dlm and I wonder if I have made a mistake. I have created a
MWE above to verify my doubt.

Do we need a set.seed for reproducing results from dlm ? Please clarify.

Best Regards,
Ashim

	[[alternative HTML version deleted]]


