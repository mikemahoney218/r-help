From ch@|@b|@e|@he @end|ng |rom y@hoo@de  Fri Feb  1 10:31:31 2019
From: ch@|@b|@e|@he @end|ng |rom y@hoo@de (Elahe chalabi)
Date: Fri, 1 Feb 2019 09:31:31 +0000 (UTC)
Subject: [R] create a network for a small text df
In-Reply-To: <BN7PR02MB5073BA57D048076AE1251251EA910@BN7PR02MB5073.namprd02.prod.outlook.com>
References: <1870388490.241973.1548843344984.ref@mail.yahoo.com>
 <1870388490.241973.1548843344984@mail.yahoo.com>
 <BN7PR02MB5073BA57D048076AE1251251EA910@BN7PR02MB5073.namprd02.prod.outlook.com>
Message-ID: <397532265.2446469.1549013492003@mail.yahoo.com>

Thanks for your reply but it does not solve the problem. Since I have 7 text rows in my df then I have to run something like 
net2=graph(c("account","block","block","solve","solve","problem")) for each, I'm looking for a way to bring all the relations in one single plot 



On Thursday, January 31, 2019 3:24 PM, Bill Poling <Bill.Poling at zelis.com> wrote:



Hello Elahe, this probably does not help but my idea may move you forward?


################################################################# NEW TOPIC ##############################################################################
#From: R-help <r-help-bounces at r-project.org> On Behalf Of Elahe chalabi via R-help Sent: Wednesday, January 30, 2019 5:16 AM

# I ran this
net1 <- structure(list(text = structure(c(1L, 7L, 3L, 4L, 5L, 6L, 2L), .Label = c("account block solv problem",
"exactly problem morning", "investigate similar problem", "matched problem control vec", "problem also accour yesterday", "same problem jj ", "same problem uk"
), class = "factor")), class = "data.frame", row.names = c(NA,
-7L))

#And I ran this

library(igraph)
net2=graph(c("account","block","block","solve","solve","problem"))
plot(net2)

# I do not get 7 plots, only one? See attached.

WHP

From: R-help <r-help-bounces at r-project.org> On Behalf Of Elahe chalabi via R-help
Sent: Wednesday, January 30, 2019 5:16 AM
To: R-help Mailing List <r-help at r-project.org>
Subject: [R] create a network for a small text df

Hi all,

I have a small dataframe and I would like to show in a network plot how words are related to the word "problem" with arrows (keeping the order of the words in sentences).
Here's the df:


dput(df)
structure(list(text = structure(c(1L, 7L, 3L, 4L, 5L, 6L, 2L), .Label = c("account block solv problem",
"exactly problem morning", "investigate similar problem", "matched problem control vec", "problem also accour yesterday", "same problem jj ", "same problem uk"
), class = "factor")), class = "data.frame", row.names = c(NA,
-7L))

So far I have tried plotting each row a a network as following:



library(igraph)
net=graph(c("account","block","block","solve","solve","problem"))
plot(net)

but I will end up having 7 plots, is there a better way?
thanks for any help.
Elahe.

______________________________________________
mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see

https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

Confidentiality Notice This message is sent from Zelis. ...{{dropped:7}}


From n|cho|@@@wr@y @end|ng |rom nt|wor|d@com  Fri Feb  1 12:00:18 2019
From: n|cho|@@@wr@y @end|ng |rom nt|wor|d@com (Nick Wray)
Date: Fri, 1 Feb 2019 11:00:18 +0000 (GMT)
Subject: [R] load.wave q
Message-ID: <2092049486.429802.1549018818200@mail2.virginmedia.com>

Please delete my last question as I have worked out what is going on myself

Thanks Nick Wray
	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Fri Feb  1 12:38:55 2019
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 1 Feb 2019 12:38:55 +0100
Subject: [R] load.wave q
In-Reply-To: <2092049486.429802.1549018818200@mail2.virginmedia.com>
References: <2092049486.429802.1549018818200@mail2.virginmedia.com>
Message-ID: <23636.12239.36842.273233@stat.math.ethz.ch>

>>>>> Nick Wray via R-help 
>>>>>     on Fri, 1 Feb 2019 11:00:18 +0000 writes:

    > Please delete my last question as I have worked out what is going on myself
    > Thanks Nick Wray

    > [[alternative HTML version deleted]]

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.

In both of the above URLs it is mentioned that R-help is
archived (in many places): 

Questions and their answers are there for the R community (of
the future) who will find and be able to learn from them.

Best,
Martin Maechler
ETH Zurich and R Core team


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri Feb  1 17:14:17 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 01 Feb 2019 08:14:17 -0800
Subject: [R] create a network for a small text df
In-Reply-To: <397532265.2446469.1549013492003@mail.yahoo.com>
References: <1870388490.241973.1548843344984.ref@mail.yahoo.com>
 <1870388490.241973.1548843344984@mail.yahoo.com>
 <BN7PR02MB5073BA57D048076AE1251251EA910@BN7PR02MB5073.namprd02.prod.outlook.com>
 <397532265.2446469.1549013492003@mail.yahoo.com>
Message-ID: <BBED05D1-E54C-4805-8050-E55FCF6C821D@dcn.davis.ca.us>

I an finding your question very opaque. For one thing, why aren't you specifying any edges? Since I don't do this kind of analysis normally I don't know what your combined graph should look like. Can you provide a link you what you hope to end up with?

Note that the Posting Guide emphasizes that the topic here is R... not the theory behind what specific packages or analyses require... so you need to be more explicit here than you have been so far.

On February 1, 2019 1:31:31 AM PST, Elahe chalabi via R-help <r-help at r-project.org> wrote:
>Thanks for your reply but it does not solve the problem. Since I have 7
>text rows in my df then I have to run something like 
>net2=graph(c("account","block","block","solve","solve","problem")) for
>each, I'm looking for a way to bring all the relations in one single
>plot 
>
>
>
>On Thursday, January 31, 2019 3:24 PM, Bill Poling
><Bill.Poling at zelis.com> wrote:
>
>
>
>Hello Elahe, this probably does not help but my idea may move you
>forward?
>
>
>################################################################# NEW
>TOPIC
>##############################################################################
>#From: R-help <r-help-bounces at r-project.org> On Behalf Of Elahe chalabi
>via R-help Sent: Wednesday, January 30, 2019 5:16 AM
>
># I ran this
>net1 <- structure(list(text = structure(c(1L, 7L, 3L, 4L, 5L, 6L, 2L),
>.Label = c("account block solv problem",
>"exactly problem morning", "investigate similar problem", "matched
>problem control vec", "problem also accour yesterday", "same problem jj
>", "same problem uk"
>), class = "factor")), class = "data.frame", row.names = c(NA,
>-7L))
>
>#And I ran this
>
>library(igraph)
>net2=graph(c("account","block","block","solve","solve","problem"))
>plot(net2)
>
># I do not get 7 plots, only one? See attached.
>
>WHP
>
>From: R-help <r-help-bounces at r-project.org> On Behalf Of Elahe chalabi
>via R-help
>Sent: Wednesday, January 30, 2019 5:16 AM
>To: R-help Mailing List <r-help at r-project.org>
>Subject: [R] create a network for a small text df
>
>Hi all,
>
>I have a small dataframe and I would like to show in a network plot how
>words are related to the word "problem" with arrows (keeping the order
>of the words in sentences).
>Here's the df:
>
>
>dput(df)
>structure(list(text = structure(c(1L, 7L, 3L, 4L, 5L, 6L, 2L), .Label =
>c("account block solv problem",
>"exactly problem morning", "investigate similar problem", "matched
>problem control vec", "problem also accour yesterday", "same problem jj
>", "same problem uk"
>), class = "factor")), class = "data.frame", row.names = c(NA,
>-7L))
>
>So far I have tried plotting each row a a network as following:
>
>
>
>library(igraph)
>net=graph(c("account","block","block","solve","solve","problem"))
>plot(net)
>
>but I will end up having 7 plots, is there a better way?
>thanks for any help.
>Elahe.
>
>______________________________________________
>mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>see
>
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>Confidentiality Notice This message is sent from Zelis.
>...{{dropped:7}}
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From md@umner @end|ng |rom gm@||@com  Fri Feb  1 21:17:16 2019
From: md@umner @end|ng |rom gm@||@com (Michael Sumner)
Date: Sat, 2 Feb 2019 07:17:16 +1100
Subject: [R] Extract the coordinates of a Polylines
In-Reply-To: <b16cc562-cb80-09b1-18d1-d4d5ae2b9d01@comcast.net>
References: <CANTxAmKurP05q8kWq_fSoRyPhNm2G7pUB-34bUD4oxOO0z8uKg@mail.gmail.com>
 <CANTxAm+o1dCKiPc7wdHO9jVB=frN4h=2E5-iq4zT7ARoKNG_XA@mail.gmail.com>
 <b16cc562-cb80-09b1-18d1-d4d5ae2b9d01@comcast.net>
Message-ID: <CAAcGz98R9zqLqBbRg6RtG3uVNmyFmJm8BmMd5B6Zd52_P7dPyQ@mail.gmail.com>

Use ggplot2::fortify - there's several other ways but no "native" support.
If fortify doesn't work try spbabel::sptable.

All the best

On Fri, Feb 1, 2019, 05:46 David Winsemius <dwinsemius at comcast.net> wrote:

>
> On 1/30/19 7:12 AM, javad bayat wrote:
> > Dear all;
> > Back to my previous question, I am trying to add X and Y coordinates
> > to every row of the data.
> >
> > topo =
> >
> readOGR("E:/New/Modelling_Water/MIKE/BathyMetry/GIS_Armator/Chitgar_Topo.shp")#Read
>
> > shape file of the topo as polylines
> > plot(topo)
> > cords = topo at lines[[1]]@Lines[[1]]@coords###Extracting X and Y
> > coordinates of the polylines of topo
> >
> > head(topo at data)
> >   FID_     Entity       Layer Color   Linetype Elevation
> >     0 LWPolyline C-TOPO-MAJR     9 Continuous    1258.0
> >     0 LWPolyline C-TOPO-MAJR     9 Continuous    1258.5
> >     0 LWPolyline C-TOPO-MAJR     9 Continuous    1258.5
> >     0 LWPolyline C-TOPO-MAJR     9 Continuous    1258.5
> >     0 LWPolyline C-TOPO-MAJR     9 Continuous    1258.5
> >     0 LWPolyline C-TOPO-MAJR     9 Continuous    1258.5
>
>
> I did a google-search on "lwpolyline" and it appears to be a structure
> that is created with AutoCAD. So it's unlikely that people who are
> primarily users of R's spatial data structures will know how rgdal
> (which is a layer on top of GDAL) will have implemented the importation
> of such a structure into R. You should instead provide the output of
> `dput (head(topo at data) )` if you want more informed comments.
>
>
> David.
>
> >
> > How can I do this.
> > Sincerely.
> >
> > On Sat, Jan 26, 2019 at 9:37 AM javad bayat <j.bayat194 at gmail.com
> > <mailto:j.bayat194 at gmail.com>> wrote:
> >
> >     Dear R users;
> >     I am trying to extract the X and Y coordinates of a polylines
> >     along with Elevation data. I have extracted the Elevations as Z,
> >     but I do not know how to extract the X and Y of these Elevations.
> >     Is it possible to extract X and Y of the Elevation and create a
> >     data frame with three variables?
> >
> >     line = readOGR("E:/......../Topo.shp")
> >     Z = line at data$Elevation
> >
> >     Sincerely.
> >
> >     --
> >     Best Regards
> >     Javad Bayat
> >     M.Sc. Environment Engineering
> >     Alternative Mail: bayat194 at yahoo.com <mailto:bayat194 at yahoo.com>
> >
> >
> >
> > --
> > Best Regards
> > Javad Bayat
> > M.Sc. Environment Engineering
> > Alternative Mail: bayat194 at yahoo.com <mailto:bayat194 at yahoo.com>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
-- 
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From n|cho|@@@wr@y @end|ng |rom nt|wor|d@com  Fri Feb  1 11:20:57 2019
From: n|cho|@@@wr@y @end|ng |rom nt|wor|d@com (Nick Wray)
Date: Fri, 1 Feb 2019 10:20:57 +0000 (GMT)
Subject: [R] load.wave
Message-ID: <820804675.427905.1549016457255@mail2.virginmedia.com>

I have been given a wav file of train locomotive noise - literally something you can play back and hear.  Using the audio package and the load.wave function I have got a 1.5 million element vector which visually at least has some periodicity in certain parts and does not seem to be completely random.  Most elements (99%) are within a range of about -0.14 to +0.14 with occasional outliers.  Beneath is a typical short segment.


This is the head:

sample rate: 16000Hz, mono, 16-bits
[1] -3.051851e-05  6.103516e-05 -6.103702e-05  3.051758e-05  3.051758e-05 -1.220740e-04

Most elements (99%) are within a range of about -0.14 to +0.14 with occasional outliers

This is the same kind of output as is illustrated in the documentation: 
https://cran.r-project.org/web/packages/seewave/vignettes/seewave_IO.pdf

What I am not sure about, and I can't find any clear explanation, is what these elements actually stand for? 
I would have thought that one needed as a minimum both volume and frequency ie a two dimensional vector but as far as I can tell
there is only one single vector.  I'm aware that this question is pushing the envelope of R help but...

Thanks, Nick Wray

From |@uren@||d|er1997 @end|ng |rom hotm@||@co@uk  Fri Feb  1 12:17:50 2019
From: |@uren@||d|er1997 @end|ng |rom hotm@||@co@uk (lauren fidler)
Date: Fri, 1 Feb 2019 11:17:50 +0000
Subject: [R] How to fix offset warning running a GAM in R's MGCV package?
Message-ID: <CWXP265MB1239453D346C0D125D7492F1AB920@CWXP265MB1239.GBRP265.PROD.OUTLOOK.COM>

Dear all,

I am trying to run a GAM using MGCV. When I include offset:

gam1 <- gam(Presence~s(Spd)+offset(effort_m), family=binomial, data=porp)

I receive the following warning:

Warning messages:

1: In newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L,  :
  Fitting terminated with step failure - check results carefully
2: glm.fit: fitted probabilities numerically 0 or 1 occurred
3: glm.fit: fitted probabilities numerically 0 or 1 occurred

How can I correct this?

Kind regards,
Lauren

	[[alternative HTML version deleted]]


From y@@@@_m@||k@ @end|ng |rom y@hoo@|r  Fri Feb  1 12:41:54 2019
From: y@@@@_m@||k@ @end|ng |rom y@hoo@|r (malika yassa)
Date: Fri, 1 Feb 2019 11:41:54 +0000 (UTC)
Subject: [R] (no subject)
References: <793972785.2617566.1549021314851.ref@mail.yahoo.com>
Message-ID: <793972785.2617566.1549021314851@mail.yahoo.com>

Please, can you help me I have a equation to solve by newton method but I can not do it
for example

f<-function(x) {


2+X2-X3=0}
this equation have un solution in [1,2]
is there a function in R for solve it or i have to programme it




	[[alternative HTML version deleted]]


From tmr@g11 @end|ng |rom gm@||@com  Sat Feb  2 04:45:44 2019
From: tmr@g11 @end|ng |rom gm@||@com (C W)
Date: Fri, 1 Feb 2019 22:45:44 -0500
Subject: [R] Why is there error in as.POSIXlt.character when using
 strftime()?
Message-ID: <CAE2FW2mTHsv6aPxFxymmPiagbaeeoHST7Vq8-jw0QY8oSHkPwA@mail.gmail.com>

Dear R community,

I am working with dates. And I get the following error:
> strftime(dat[20], format="%H:%M")
Error in as.POSIXlt.character(as.character(x), ...) :
  character string is not in a standard unambiguous format

Here's the original data:
dat <- structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L,
13L, 14L, 15L, 16L, 17L, 18L, 19L, 23L, 24L, 25L, 26L, 27L, 28L,
29L, 30L, 31L, 32L, 33L, 34L, 35L, 36L, 37L, 38L, 39L, 40L, 20L,
21L, 22L), .Label = c("7/12/15 11:32", "7/12/15 11:42", "7/12/15 12:17",
"7/12/15 12:31", "7/12/15 12:50", "7/12/15 14:10", "7/12/15 14:19",
"7/12/15 14:59", "7/12/15 15:57", "7/12/15 16:00", "7/12/15 16:46",
"7/12/15 16:51", "7/12/15 17:35", "7/12/15 17:59", "7/12/15 18:17",
"7/12/15 19:07", "7/12/15 19:08", "7/12/15 19:31", "7/12/15 21:21",
"7/13/15 10:01", "7/13/15 10:03", "7/13/15 10:05", "7/13/15 6:00",
"7/13/15 6:20", "7/13/15 6:37", "7/13/15 6:40", "7/13/15 6:46",
"7/13/15 7:20", "7/13/15 7:47", "7/13/15 7:50", "7/13/15 7:54",
"7/13/15 8:11", "7/13/15 8:23", "7/13/15 8:31", "7/13/15 8:33",
"7/13/15 8:43", "7/13/15 9:04", "7/13/15 9:09", "7/13/15 9:30",
"7/13/15 9:59"), class = "factor")

It seems like things work fine for the first 19 elements,
> strftime(dat[1:19], format="%H:%M")
 [1] "11:32" "11:42" "12:17" "12:31" "12:50" "14:10" "14:19" "14:59"
 [9] "15:57" "16:00" "16:46" "16:51" "17:35" "17:59" "18:17" "19:07"
[17] "19:08" "19:31" "21:21"

But why not element 20 and on? They *look* the same to me.

What is going on?

Thanks very much!

	[[alternative HTML version deleted]]


From h@@@n@d|w@n @end|ng |rom gm@||@com  Sat Feb  2 16:06:28 2019
From: h@@@n@d|w@n @end|ng |rom gm@||@com (Hasan Diwan)
Date: Sat, 2 Feb 2019 07:06:28 -0800
Subject: [R] (no subject)
In-Reply-To: <793972785.2617566.1549021314851@mail.yahoo.com>
References: <793972785.2617566.1549021314851.ref@mail.yahoo.com>
 <793972785.2617566.1549021314851@mail.yahoo.com>
Message-ID: <CAP+bYWAJ40PEQKSOnoqVWYxO+MQnHQdCqOJOF2rZo98yT8iyFA@mail.gmail.com>

Look at the rootSolve package[1] for what you need. Hope it helps... -- H

On Sat, 2 Feb 2019 at 06:46, malika yassa via R-help <r-help at r-project.org>
wrote:

> Please, can you help me I have a equation to solve by newton method but I
> can not do it
> for example
>
> f<-function(x) {
>
>
> 2+X2-X3=0}
> this equation have un solution in [1,2]
> is there a function in R for solve it or i have to programme it
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
OpenPGP:
https://sks-keyservers.net/pks/lookup?op=get&search=0xFEBAD7FFD041BBA1
If you wish to request my time, please do so using
*bit.ly/hd1AppointmentRequest
<http://bit.ly/hd1AppointmentRequest>*.
Si vous voudrais faire connnaisance, allez a *bit.ly/hd1AppointmentRequest
<http://bit.ly/hd1AppointmentRequest>*.

<https://sks-keyservers.net/pks/lookup?op=get&search=0xFEBAD7FFD041BBA1>Sent
from my mobile device
Envoye de mon portable

	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Sat Feb  2 16:09:46 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sat, 2 Feb 2019 10:09:46 -0500
Subject: [R] Why is there error in as.POSIXlt.character when using
 strftime()?
In-Reply-To: <CAE2FW2mTHsv6aPxFxymmPiagbaeeoHST7Vq8-jw0QY8oSHkPwA@mail.gmail.com>
References: <CAE2FW2mTHsv6aPxFxymmPiagbaeeoHST7Vq8-jw0QY8oSHkPwA@mail.gmail.com>
Message-ID: <0885ec11-fae7-7bc0-20e6-c6856c12c6dc@gmail.com>

On 01/02/2019 10:45 p.m., C W wrote:
> Dear R community,
> 
> I am working with dates. And I get the following error:
>> strftime(dat[20], format="%H:%M")
> Error in as.POSIXlt.character(as.character(x), ...) :
>    character string is not in a standard unambiguous format

You are using the wrong function:  strftime() formats a time object as a 
character string.  You want strptime() to convert character (or factor 
in your case) to a time object.

But you need to give the format for the full string, not just the time 
at the end.

If you really were intending to extract times from dat, then you need 
both conversions:

 > strftime(strptime(dat, format="%m/%d/%y %H:%M"), format = "%H:%M")
  [1] "11:32" "11:42" "12:17" "12:31" "12:50" "14:10" "14:19" "14:59" 
"15:57" "16:00" "16:46" "16:51" "17:35" "17:59" "18:17" "19:07" "19:08"
[18] "19:31" "21:21" "06:00" "06:20" "06:37" "06:40" "06:46" "07:20" 
"07:47" "07:50" "07:54" "08:11" "08:23" "08:31" "08:33" "08:43" "09:04"
[35] "09:09" "09:30" "09:59" "10:01" "10:03" "10:05"

Duncan Murdoch


From wdun|@p @end|ng |rom t|bco@com  Sat Feb  2 16:40:29 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Sat, 2 Feb 2019 07:40:29 -0800
Subject: [R] Why is there error in as.POSIXlt.character when using
 strftime()?
In-Reply-To: <CAE2FW2mTHsv6aPxFxymmPiagbaeeoHST7Vq8-jw0QY8oSHkPwA@mail.gmail.com>
References: <CAE2FW2mTHsv6aPxFxymmPiagbaeeoHST7Vq8-jw0QY8oSHkPwA@mail.gmail.com>
Message-ID: <CAF8bMcaJPnJMSYgWor2L2mTLYGOqE0Rh6XC=J5wpK=Ju=tK61w@mail.gmail.com>

Note that the first unparsable element is the first with a 13 in the second
field, which is out of range for the month entry.  If you look at the the
whole date/time output by the first 19 elements you will see that you need
to tell it the order of the year, month, and day

> as.POSIXlt(dat[1:19])
 [1] "0007-12-15 11:32:00 LMT" "0007-12-15 11:42:00 LMT"
 [3] "0007-12-15 12:17:00 LMT" "0007-12-15 12:31:00 LMT"
 [5] "0007-12-15 12:50:00 LMT" "0007-12-15 14:10:00 LMT"
 [7] "0007-12-15 14:19:00 LMT" "0007-12-15 14:59:00 LMT"
 [9] "0007-12-15 15:57:00 LMT" "0007-12-15 16:00:00 LMT"
[11] "0007-12-15 16:46:00 LMT" "0007-12-15 16:51:00 LMT"
[13] "0007-12-15 17:35:00 LMT" "0007-12-15 17:59:00 LMT"
[15] "0007-12-15 18:17:00 LMT" "0007-12-15 19:07:00 LMT"
[17] "0007-12-15 19:08:00 LMT" "0007-12-15 19:31:00 LMT"
[19] "0007-12-15 21:21:00 LMT"
> as.POSIXlt(dat, format="%m/%d/%y %H:%M")
 [1] "2015-07-12 11:32:00 PDT" "2015-07-12 11:42:00 PDT"
 [3] "2015-07-12 12:17:00 PDT" "2015-07-12 12:31:00 PDT"
 [5] "2015-07-12 12:50:00 PDT" "2015-07-12 14:10:00 PDT"
 [7] "2015-07-12 14:19:00 PDT" "2015-07-12 14:59:00 PDT"
 [9] "2015-07-12 15:57:00 PDT" "2015-07-12 16:00:00 PDT"
[11] "2015-07-12 16:46:00 PDT" "2015-07-12 16:51:00 PDT"
[13] "2015-07-12 17:35:00 PDT" "2015-07-12 17:59:00 PDT"
[15] "2015-07-12 18:17:00 PDT" "2015-07-12 19:07:00 PDT"
[17] "2015-07-12 19:08:00 PDT" "2015-07-12 19:31:00 PDT"
[19] "2015-07-12 21:21:00 PDT" "2015-07-13 06:00:00 PDT"
[21] "2015-07-13 06:20:00 PDT" "2015-07-13 06:37:00 PDT"
[23] "2015-07-13 06:40:00 PDT" "2015-07-13 06:46:00 PDT"
[25] "2015-07-13 07:20:00 PDT" "2015-07-13 07:47:00 PDT"
[27] "2015-07-13 07:50:00 PDT" "2015-07-13 07:54:00 PDT"
[29] "2015-07-13 08:11:00 PDT" "2015-07-13 08:23:00 PDT"
[31] "2015-07-13 08:31:00 PDT" "2015-07-13 08:33:00 PDT"
[33] "2015-07-13 08:43:00 PDT" "2015-07-13 09:04:00 PDT"
[35] "2015-07-13 09:09:00 PDT" "2015-07-13 09:30:00 PDT"
[37] "2015-07-13 09:59:00 PDT" "2015-07-13 10:01:00 PDT"
[39] "2015-07-13 10:03:00 PDT" "2015-07-13 10:05:00 PDT"

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Sat, Feb 2, 2019 at 6:46 AM C W <tmrsg11 at gmail.com> wrote:

> Dear R community,
>
> I am working with dates. And I get the following error:
> > strftime(dat[20], format="%H:%M")
> Error in as.POSIXlt.character(as.character(x), ...) :
>   character string is not in a standard unambiguous format
>
> Here's the original data:
> dat <- structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L,
> 13L, 14L, 15L, 16L, 17L, 18L, 19L, 23L, 24L, 25L, 26L, 27L, 28L,
> 29L, 30L, 31L, 32L, 33L, 34L, 35L, 36L, 37L, 38L, 39L, 40L, 20L,
> 21L, 22L), .Label = c("7/12/15 11:32", "7/12/15 11:42", "7/12/15 12:17",
> "7/12/15 12:31", "7/12/15 12:50", "7/12/15 14:10", "7/12/15 14:19",
> "7/12/15 14:59", "7/12/15 15:57", "7/12/15 16:00", "7/12/15 16:46",
> "7/12/15 16:51", "7/12/15 17:35", "7/12/15 17:59", "7/12/15 18:17",
> "7/12/15 19:07", "7/12/15 19:08", "7/12/15 19:31", "7/12/15 21:21",
> "7/13/15 10:01", "7/13/15 10:03", "7/13/15 10:05", "7/13/15 6:00",
> "7/13/15 6:20", "7/13/15 6:37", "7/13/15 6:40", "7/13/15 6:46",
> "7/13/15 7:20", "7/13/15 7:47", "7/13/15 7:50", "7/13/15 7:54",
> "7/13/15 8:11", "7/13/15 8:23", "7/13/15 8:31", "7/13/15 8:33",
> "7/13/15 8:43", "7/13/15 9:04", "7/13/15 9:09", "7/13/15 9:30",
> "7/13/15 9:59"), class = "factor")
>
> It seems like things work fine for the first 19 elements,
> > strftime(dat[1:19], format="%H:%M")
>  [1] "11:32" "11:42" "12:17" "12:31" "12:50" "14:10" "14:19" "14:59"
>  [9] "15:57" "16:00" "16:46" "16:51" "17:35" "17:59" "18:17" "19:07"
> [17] "19:08" "19:31" "21:21"
>
> But why not element 20 and on? They *look* the same to me.
>
> What is going on?
>
> Thanks very much!
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bhh @end|ng |rom x@4@||@n|  Sat Feb  2 17:07:05 2019
From: bhh @end|ng |rom x@4@||@n| (Berend Hasselman)
Date: Sat, 2 Feb 2019 17:07:05 +0100
Subject: [R] (no subject)
In-Reply-To: <793972785.2617566.1549021314851@mail.yahoo.com>
References: <793972785.2617566.1549021314851.ref@mail.yahoo.com>
 <793972785.2617566.1549021314851@mail.yahoo.com>
Message-ID: <EBC44F78-2D10-4686-B923-9940CB60CA1A@xs4all.nl>



> On 1 Feb 2019, at 12:41, malika yassa via R-help <r-help at r-project.org> wrote:
> 
> Please, can you help me I have a equation to solve by newton method but I can not do it
> for example
> 
> f<-function(x) {
> 
> 
> 2+X2-X3=0}
> this equation have un solution in [1,2]
> is there a function in R for solve it or i have to programme it
> 

What is the relation between the function argument x and variables X2 and X3?

As it stands this is incomprehensible. The function argument x is not used anywhere in the function body.

Berend Hasselman

> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sat Feb  2 16:41:31 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 02 Feb 2019 07:41:31 -0800
Subject: [R] Why is there error in as.POSIXlt.character when using
 strftime()?
In-Reply-To: <0885ec11-fae7-7bc0-20e6-c6856c12c6dc@gmail.com>
References: <CAE2FW2mTHsv6aPxFxymmPiagbaeeoHST7Vq8-jw0QY8oSHkPwA@mail.gmail.com>
 <0885ec11-fae7-7bc0-20e6-c6856c12c6dc@gmail.com>
Message-ID: <3767EC7C-9841-436F-B75B-210DD61A878C@dcn.davis.ca.us>

... and in general, you need to specify the time zone to avoid surprises. In many cases this can be as simple as

Sys.setenv(TZ="GMT")

but it can be specific to your data set also.

On February 2, 2019 7:09:46 AM PST, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>On 01/02/2019 10:45 p.m., C W wrote:
>> Dear R community,
>> 
>> I am working with dates. And I get the following error:
>>> strftime(dat[20], format="%H:%M")
>> Error in as.POSIXlt.character(as.character(x), ...) :
>>    character string is not in a standard unambiguous format
>
>You are using the wrong function:  strftime() formats a time object as
>a 
>character string.  You want strptime() to convert character (or factor 
>in your case) to a time object.
>
>But you need to give the format for the full string, not just the time 
>at the end.
>
>If you really were intending to extract times from dat, then you need 
>both conversions:
>
> > strftime(strptime(dat, format="%m/%d/%y %H:%M"), format = "%H:%M")
>  [1] "11:32" "11:42" "12:17" "12:31" "12:50" "14:10" "14:19" "14:59" 
>"15:57" "16:00" "16:46" "16:51" "17:35" "17:59" "18:17" "19:07" "19:08"
>[18] "19:31" "21:21" "06:00" "06:20" "06:37" "06:40" "06:46" "07:20" 
>"07:47" "07:50" "07:54" "08:11" "08:23" "08:31" "08:33" "08:43" "09:04"
>[35] "09:09" "09:30" "09:59" "10:01" "10:03" "10:05"
>
>Duncan Murdoch
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sat Feb  2 18:02:51 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 02 Feb 2019 09:02:51 -0800
Subject: [R] Why is there error in as.POSIXlt.character when using
 strftime()?
In-Reply-To: <CAE2FW2=sUYi5G-LL6BzvQG84q6XN1169AKBN03Mm6jnQoxLdgw@mail.gmail.com>
References: <CAE2FW2mTHsv6aPxFxymmPiagbaeeoHST7Vq8-jw0QY8oSHkPwA@mail.gmail.com>
 <0885ec11-fae7-7bc0-20e6-c6856c12c6dc@gmail.com>
 <3767EC7C-9841-436F-B75B-210DD61A878C@dcn.davis.ca.us>
 <CAE2FW2=sUYi5G-LL6BzvQG84q6XN1169AKBN03Mm6jnQoxLdgw@mail.gmail.com>
Message-ID: <0F9B4A80-DE0B-48FB-9F3C-539FE9568C6C@dcn.davis.ca.us>

strptime = string parse time (convert from string to POSIXlt, name of function as specified in POSIX standard)

strftime = string format time (make string, name of function as specified in POSIX standard)

as.POSIXlt = convert to POSIX list time format (same as strptime but name fits type conversion function naming pattern in R)

as.POSIXct = convert to POSIX continuous time (seconds since epoch, typically 1970-01-01 00:00:00 GMT, useful for computations)

?DateTimeClasses

On February 2, 2019 8:51:28 AM PST, C W <tmrsg11 at gmail.com> wrote:
>Thank you all very much, very helpful! :)
>
>I'm just curious, what does strptime(), strftime(), as.POSIXlt.(), and
>as.POSIXct() stand for?
>
>On Sat, Feb 2, 2019 at 10:41 AM Jeff Newmiller
><jdnewmil at dcn.davis.ca.us>
>wrote:
>
>> ... and in general, you need to specify the time zone to avoid
>surprises.
>> In many cases this can be as simple as
>>
>> Sys.setenv(TZ="GMT")
>>
>> but it can be specific to your data set also.
>>
>> On February 2, 2019 7:09:46 AM PST, Duncan Murdoch <
>> murdoch.duncan at gmail.com> wrote:
>> >On 01/02/2019 10:45 p.m., C W wrote:
>> >> Dear R community,
>> >>
>> >> I am working with dates. And I get the following error:
>> >>> strftime(dat[20], format="%H:%M")
>> >> Error in as.POSIXlt.character(as.character(x), ...) :
>> >>    character string is not in a standard unambiguous format
>> >
>> >You are using the wrong function:  strftime() formats a time object
>as
>> >a
>> >character string.  You want strptime() to convert character (or
>factor
>> >in your case) to a time object.
>> >
>> >But you need to give the format for the full string, not just the
>time
>> >at the end.
>> >
>> >If you really were intending to extract times from dat, then you
>need
>> >both conversions:
>> >
>> > > strftime(strptime(dat, format="%m/%d/%y %H:%M"), format =
>"%H:%M")
>> >  [1] "11:32" "11:42" "12:17" "12:31" "12:50" "14:10" "14:19"
>"14:59"
>> >"15:57" "16:00" "16:46" "16:51" "17:35" "17:59" "18:17" "19:07"
>"19:08"
>> >[18] "19:31" "21:21" "06:00" "06:20" "06:37" "06:40" "06:46" "07:20"
>> >"07:47" "07:50" "07:54" "08:11" "08:23" "08:31" "08:33" "08:43"
>"09:04"
>> >[35] "09:09" "09:30" "09:59" "10:01" "10:03" "10:05"
>> >
>> >Duncan Murdoch
>> >
>> >______________________________________________
>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >PLEASE do read the posting guide
>> >http://www.R-project.org/posting-guide.html
>> >and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Sent from my phone. Please excuse my brevity.
>>

-- 
Sent from my phone. Please excuse my brevity.


From @|@n|00 @end|ng |rom comc@@t@net  Sat Feb  2 19:29:29 2019
From: @|@n|00 @end|ng |rom comc@@t@net (Alan Feuerbacher)
Date: Sat, 2 Feb 2019 11:29:29 -0700
Subject: [R] [FORGED] Newbie Question on R versus Matlab/Octave versus C
In-Reply-To: <alpine.BSF.2.00.1901292221200.83151@pedal.dcn.davis.ca.us>
References: <eafdc33a-61ce-03b8-2e02-3a467eda2d84@comcast.net>
 <8b2f75fe-4979-e4ad-b86f-4c9a50a93271@auckland.ac.nz>
 <bb0af392-125a-5a53-96a5-ab825756ee28@comcast.net>
 <24250968-5B39-4CEC-8649-406716F9780E@dcn.davis.ca.us>
 <235c61aa-b59c-1a9c-c5d3-ffa0614d7a65@comcast.net>
 <alpine.BSF.2.00.1901292221200.83151@pedal.dcn.davis.ca.us>
Message-ID: <dfb68d74-a7ed-cd8d-d69d-10aaf565f8a7@comcast.net>

On 1/29/2019 11:50 PM, Jeff Newmiller wrote:
> On Tue, 29 Jan 2019, Alan Feuerbacher wrote:
> 
>> After my failed attempt at using Octave, I realized that most likely 
>> the main contributing factor was that I was not able to figure out an 
>> efficient data structure to model one person. But C lent itself 
>> perfectly to my idea of how to go about programming my simulation. So 
>> here's a simplified pseudocode sort of example of what I did:
> 
> Don't model one person... model an array of people.
> 
>> To model a single reproducing woman I used this C construct:
>>
>> typedef struct woman {
>> ?int isAlive;
>> ?int isPregnant;
>> ?double age;
>> ?. . .
>> } WOMAN;
> 
> # e.g.
> Nwomen <- 100
> women <- data.frame( isAlive = rep( TRUE, Nwomen )
>  ?????????????????? , isPregnant = rep( FALSE, Nwomen )
>  ?????????????????? , age = rep( 20, Nwomen )
>  ?????????????????? )
> 
>> Then I allocated memory for a big array of these things, using the C 
>> malloc() function, which gave me the equivalent of this statement:
>>
>> WOMAN women[NWOMEN];? /* An array of NWOMEN woman-structs */
>>
>> After some initialization I set up two loops:
>>
>> for( j=0; j<numberOfYears; j++) {
>> ?for(i=1; i< numberOfWomen; i++) {
>> ?? updateWomen();
>> ?}
>> }
> 
> for ( j in seq.int( numberOfYears ) {
>  ? # let vectorized data storage automatically handle the other for loop
>  ? women <- updateWomen( women )
> }
> 
>> The function updateWomen() figures out things like whether the woman 
>> becomes pregnant or gives birth on a given day, dies, etc.
> 
> You can use your "fixed size" allocation strategy with flags indicating 
> whether specific rows are in use, or you can only work with valid rows 
> and add rows as needed for children... best to compute a logical vector 
> that identifies all of the birthing mothers as a subset of the data 
> frame, and build a set of children rows using the birthing mothers data 
> frame as input, and then rbind the new rows to the updated women 
> dataframe as appropriate. The most clear approach for individual 
> decision calculations is the use of the vectorized "ifelse" function, 
> though under certain circumstances putting an indexed subset on the left 
> side of an assignment can modify memory "in place" (the 
> functional-programming restriction against this is probably a foreign 
> idea to a dyed-in-the-wool C programmer, but R usually prevents you from 
> modifying the variable that was input to a function, automatically 
> making a local copy of the input as needed in order to prevent such 
> backwash into the caller's context).

Hi Jeff,

I'm well along in implementing your suggestions, but I don't understand 
the last paragraph. Here is part of the experimenting I've done so far:

*=======*=======*=======*=======*=======*=======*
updatePerson <- function() {
   ifelse( women$isAlive,
     {
# Check whether to kill off this person, if she's pregnant whether
# to give birth, whether to make her pregnant again.
       women$age = women$age + timeStep
# Check if the person has reached maxAge
     }
   )
}

calculatePopulation <- function() {
   lastDate = 0
   jd = 0
   while( jd < maxDate ) {
     for( i in seq_len( nWomen ) ) {
       updatePerson();
     }
     todaysDateInt = floor(jd/dpy)
     NAlive[todaysDateInt] = nWomen - nDead
# Do various other things
     todaysDate = todaysDate + timeStep
     jd = jd + timeStep
   }
}

nWomen <- 5
numberOfYears <- 30
women <- data.frame( isAlive = rep_len( TRUE, nWomen )
                    , isPregnant = rep_len( FALSE, nWomen )
                    , nChildren = rep_len( 0L, nWomen )
                    , ageInt = rep_len( 0L, nWomen )
                    , age = rep_len( 0, nWomen )
                    , dateOfPregnancy = rep_len( 0, nWomen )
                    , endDateLastPregnancy = rep_len( 0.0, nWomen )
                    , minBirthAge = rep_len( 0, nWomen )
                    , maxBirthAge = rep_len( 0, nWomen )
                    )

# . . .

   calculatePopulation()

*=======*=======*=======*=======*=======*=======*

The above code (in its complete form) executes without errors. I don't 
understand at least two things:

In the updatePerson function, in the ifelse statement, how do I change 
the appropriate values in the women dataframe?

I don't understand most of your last paragraph at all.

Thanks so much for your help in learning R!

Alan

---
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sat Feb  2 19:33:27 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 02 Feb 2019 10:33:27 -0800
Subject: [R] Why is there error in as.POSIXlt.character when using
 strftime()?
In-Reply-To: <CAE2FW2=Uzd98pz=kCrFHaJWMFWL+iHd0EnJP3X7FXMged1eDSw@mail.gmail.com>
References: <CAE2FW2mTHsv6aPxFxymmPiagbaeeoHST7Vq8-jw0QY8oSHkPwA@mail.gmail.com>
 <0885ec11-fae7-7bc0-20e6-c6856c12c6dc@gmail.com>
 <3767EC7C-9841-436F-B75B-210DD61A878C@dcn.davis.ca.us>
 <CAE2FW2=sUYi5G-LL6BzvQG84q6XN1169AKBN03Mm6jnQoxLdgw@mail.gmail.com>
 <CAE2FW2=Uzd98pz=kCrFHaJWMFWL+iHd0EnJP3X7FXMged1eDSw@mail.gmail.com>
Message-ID: <81A3E377-6A75-4589-A7D5-54228B937951@dcn.davis.ca.us>

If there are no time-of-day values in your data set the the Date type is great. However, it can be messy if you work with both Date and POSIXt types in the same analysis... I recommend sticking with one or the other.

The trunc.POSIXt function is more appropriate for getting POSIXt dates than converting to character and back.

On February 2, 2019 9:03:31 AM PST, C W <tmrsg11 at gmail.com> wrote:
>Also, I was able to extract date using
>> as.Date(dat, "%m/%d/%y")
> [1] "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-12"
> [6] "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-12"
>[11] "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-12"
>[16] "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-13"
>[21] "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13"
>[26] "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13"
>[31] "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13"
>[36] "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13"
>
>> class(as.Date(dat, "%m/%d/%y"))
>[1] "Date"
>> class(strftime(strptime(dat, format="%m/%d/%y %H:%M"), format =
>"%m/%d/%y"))
>[1] "character"
>
>If I want to date caluculation, would as.Date() be more desirable? Or
>does
>strftime() do the same thing?
>
>
>
>On Sat, Feb 2, 2019 at 11:51 AM C W <tmrsg11 at gmail.com> wrote:
>
>> Thank you all very much, very helpful! :)
>>
>> I'm just curious, what does strptime(), strftime(), as.POSIXlt.(),
>and
>> as.POSIXct() stand for?
>>
>> On Sat, Feb 2, 2019 at 10:41 AM Jeff Newmiller
><jdnewmil at dcn.davis.ca.us>
>> wrote:
>>
>>> ... and in general, you need to specify the time zone to avoid
>surprises.
>>> In many cases this can be as simple as
>>>
>>> Sys.setenv(TZ="GMT")
>>>
>>> but it can be specific to your data set also.
>>>
>>> On February 2, 2019 7:09:46 AM PST, Duncan Murdoch <
>>> murdoch.duncan at gmail.com> wrote:
>>> >On 01/02/2019 10:45 p.m., C W wrote:
>>> >> Dear R community,
>>> >>
>>> >> I am working with dates. And I get the following error:
>>> >>> strftime(dat[20], format="%H:%M")
>>> >> Error in as.POSIXlt.character(as.character(x), ...) :
>>> >>    character string is not in a standard unambiguous format
>>> >
>>> >You are using the wrong function:  strftime() formats a time object
>as
>>> >a
>>> >character string.  You want strptime() to convert character (or
>factor
>>> >in your case) to a time object.
>>> >
>>> >But you need to give the format for the full string, not just the
>time
>>> >at the end.
>>> >
>>> >If you really were intending to extract times from dat, then you
>need
>>> >both conversions:
>>> >
>>> > > strftime(strptime(dat, format="%m/%d/%y %H:%M"), format =
>"%H:%M")
>>> >  [1] "11:32" "11:42" "12:17" "12:31" "12:50" "14:10" "14:19"
>"14:59"
>>> >"15:57" "16:00" "16:46" "16:51" "17:35" "17:59" "18:17" "19:07"
>"19:08"
>>> >[18] "19:31" "21:21" "06:00" "06:20" "06:37" "06:40" "06:46"
>"07:20"
>>> >"07:47" "07:50" "07:54" "08:11" "08:23" "08:31" "08:33" "08:43"
>"09:04"
>>> >[35] "09:09" "09:30" "09:59" "10:01" "10:03" "10:05"
>>> >
>>> >Duncan Murdoch
>>> >
>>> >______________________________________________
>>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> >https://stat.ethz.ch/mailman/listinfo/r-help
>>> >PLEASE do read the posting guide
>>> >http://www.R-project.org/posting-guide.html
>>> >and provide commented, minimal, self-contained, reproducible code.
>>>
>>> --
>>> Sent from my phone. Please excuse my brevity.
>>>
>>

-- 
Sent from my phone. Please excuse my brevity.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sat Feb  2 19:51:15 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 02 Feb 2019 10:51:15 -0800
Subject: [R] load.wave
In-Reply-To: <820804675.427905.1549016457255@mail2.virginmedia.com>
References: <820804675.427905.1549016457255@mail2.virginmedia.com>
Message-ID: <FC953FA2-90A4-4FEA-9EF7-1F8D6196B098@dcn.davis.ca.us>

You aren't pushing any envelope... you slit it open and fell out somewhere on the sidewalk. I tossed your question into Google and it came back with [1] and [2]. Please do that yourself instead whenever you are tempted to go off topic.

[1] https://stackoverflow.com/questions/25940376/whats-the-actual-data-in-a-wav-file
[2] https://en.m.wikipedia.org/wiki/Digital_audio

On February 1, 2019 2:20:57 AM PST, Nick Wray via R-help <r-help at r-project.org> wrote:
>I have been given a wav file of train locomotive noise - literally
>something you can play back and hear.  Using the audio package and the
>load.wave function I have got a 1.5 million element vector which
>visually at least has some periodicity in certain parts and does not
>seem to be completely random.  Most elements (99%) are within a range
>of about -0.14 to +0.14 with occasional outliers.  Beneath is a typical
>short segment.
>
>
>This is the head:
>
>sample rate: 16000Hz, mono, 16-bits
>[1] -3.051851e-05  6.103516e-05 -6.103702e-05  3.051758e-05 
>3.051758e-05 -1.220740e-04
>
>Most elements (99%) are within a range of about -0.14 to +0.14 with
>occasional outliers
>
>This is the same kind of output as is illustrated in the documentation:
>
>https://cran.r-project.org/web/packages/seewave/vignettes/seewave_IO.pdf
>
>What I am not sure about, and I can't find any clear explanation, is
>what these elements actually stand for? 
>I would have thought that one needed as a minimum both volume and
>frequency ie a two dimensional vector but as far as I can tell
>there is only one single vector.  I'm aware that this question is
>pushing the envelope of R help but...
>
>Thanks, Nick Wray
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From kry|ov@r00t @end|ng |rom gm@||@com  Sat Feb  2 20:11:57 2019
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Sat, 2 Feb 2019 22:11:57 +0300
Subject: [R] load.wave
In-Reply-To: <820804675.427905.1549016457255@mail2.virginmedia.com>
References: <820804675.427905.1549016457255@mail2.virginmedia.com>
Message-ID: <20190202221157.441ca02d@Tarkus>

Hello Nick Wray,

Let me offer a simplified explanation of what's going on. Sorry if it's
unnecessary.

Sound is waves of pressure in the air. Devices like microphones can
measure the changing pressure by converting it into voltage. Voltage
can then be sampled by an analog-to-digital converter inside a sound
card and stored as numbers in computer memory.

On Fri, 1 Feb 2019 10:20:57 +0000 (GMT)
Nick Wray via R-help <r-help at r-project.org> wrote:

> What I am not sure about, and I can't find any clear explanation, is
> what these elements actually stand for?

Digital sound works by measuring "pressure" a few tens of thousands of
times per second and then recreating the corresponding signal
elsewhere. According to the sampling theorem, sound sampled N times per
second would be losslessly reproduced if it didn't contain frequencies
above N/2 Hz.

To reiterate, these numbers are just audio samples. Feed them to the
sound card at the original sample rate, and you hear the same sound
that had been recorded.

This part is explained well in two 30-minute video lectures here:
https://xiph.org/video/vid1.shtml https://xiph.org/video/vid2.shtml
(I wouldn't normally recommend video lectures, but these are really
good.)

> I would have thought that one needed as a minimum both volume and
> frequency ie a two dimensional vector but as far as I can tell there
> is only one single vector.

You are describing a spectrogram: a surface showing the "volume" of
each individual frequency in the sound recording, over time. How to get
it? If you run a Fourier transform over the original vector, you will
get only one vector showing the magnitudes and phases of all frequencies
through the whole length of the clip.

To get a two-dimensional spectrogram, you should take overlapping parts
of the original vector of samples, multiply them by a special window
function, then take a Fourier transform over that and combine
resulting vectors into a matrix. Computing a spectrogram involves
choosing a lot of parameters: size of the overlapping window, step
between overlapping windows, the window function itself and its own
parameters.

Problems like these should be described in books about digital signal
processing.

Jeff Newmiller sent more useful links while I was typing this, and I
guess I should posting off-topic.

-- 
Best regards,
Ivan


From r@turner @end|ng |rom @uck|@nd@@c@nz  Sat Feb  2 22:52:23 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sun, 3 Feb 2019 10:52:23 +1300
Subject: [R] [FORGED]  (no subject)
In-Reply-To: <793972785.2617566.1549021314851@mail.yahoo.com>
References: <793972785.2617566.1549021314851.ref@mail.yahoo.com>
 <793972785.2617566.1549021314851@mail.yahoo.com>
Message-ID: <6f90c913-d484-221d-a77b-9a9915413782@auckland.ac.nz>


On 2/2/19 12:41 AM, malika yassa via R-help wrote:

> Please, can you help me I have a equation to solve by newton method but I can not do it
> for example
> 
> f<-function(x) {
> 
> 
> 2+X2-X3=0}
> this equation have un solution in [1,2]
> is there a function in R for solve it or i have to programme it

(a) Read the posting guide.

(b) This looks like homework (perhaps a homework question that you have 
completely misunderstood) and this list has a no-homework policy.

(c) As Berend Hasselman has pointed out, your question makes no real 
sense anyway.

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From tmr@g11 @end|ng |rom gm@||@com  Sat Feb  2 17:51:28 2019
From: tmr@g11 @end|ng |rom gm@||@com (C W)
Date: Sat, 2 Feb 2019 11:51:28 -0500
Subject: [R] Why is there error in as.POSIXlt.character when using
 strftime()?
In-Reply-To: <3767EC7C-9841-436F-B75B-210DD61A878C@dcn.davis.ca.us>
References: <CAE2FW2mTHsv6aPxFxymmPiagbaeeoHST7Vq8-jw0QY8oSHkPwA@mail.gmail.com>
 <0885ec11-fae7-7bc0-20e6-c6856c12c6dc@gmail.com>
 <3767EC7C-9841-436F-B75B-210DD61A878C@dcn.davis.ca.us>
Message-ID: <CAE2FW2=sUYi5G-LL6BzvQG84q6XN1169AKBN03Mm6jnQoxLdgw@mail.gmail.com>

Thank you all very much, very helpful! :)

I'm just curious, what does strptime(), strftime(), as.POSIXlt.(), and
as.POSIXct() stand for?

On Sat, Feb 2, 2019 at 10:41 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> ... and in general, you need to specify the time zone to avoid surprises.
> In many cases this can be as simple as
>
> Sys.setenv(TZ="GMT")
>
> but it can be specific to your data set also.
>
> On February 2, 2019 7:09:46 AM PST, Duncan Murdoch <
> murdoch.duncan at gmail.com> wrote:
> >On 01/02/2019 10:45 p.m., C W wrote:
> >> Dear R community,
> >>
> >> I am working with dates. And I get the following error:
> >>> strftime(dat[20], format="%H:%M")
> >> Error in as.POSIXlt.character(as.character(x), ...) :
> >>    character string is not in a standard unambiguous format
> >
> >You are using the wrong function:  strftime() formats a time object as
> >a
> >character string.  You want strptime() to convert character (or factor
> >in your case) to a time object.
> >
> >But you need to give the format for the full string, not just the time
> >at the end.
> >
> >If you really were intending to extract times from dat, then you need
> >both conversions:
> >
> > > strftime(strptime(dat, format="%m/%d/%y %H:%M"), format = "%H:%M")
> >  [1] "11:32" "11:42" "12:17" "12:31" "12:50" "14:10" "14:19" "14:59"
> >"15:57" "16:00" "16:46" "16:51" "17:35" "17:59" "18:17" "19:07" "19:08"
> >[18] "19:31" "21:21" "06:00" "06:20" "06:37" "06:40" "06:46" "07:20"
> >"07:47" "07:50" "07:54" "08:11" "08:23" "08:31" "08:33" "08:43" "09:04"
> >[35] "09:09" "09:30" "09:59" "10:01" "10:03" "10:05"
> >
> >Duncan Murdoch
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From tmr@g11 @end|ng |rom gm@||@com  Sat Feb  2 18:03:31 2019
From: tmr@g11 @end|ng |rom gm@||@com (C W)
Date: Sat, 2 Feb 2019 12:03:31 -0500
Subject: [R] Why is there error in as.POSIXlt.character when using
 strftime()?
In-Reply-To: <CAE2FW2=sUYi5G-LL6BzvQG84q6XN1169AKBN03Mm6jnQoxLdgw@mail.gmail.com>
References: <CAE2FW2mTHsv6aPxFxymmPiagbaeeoHST7Vq8-jw0QY8oSHkPwA@mail.gmail.com>
 <0885ec11-fae7-7bc0-20e6-c6856c12c6dc@gmail.com>
 <3767EC7C-9841-436F-B75B-210DD61A878C@dcn.davis.ca.us>
 <CAE2FW2=sUYi5G-LL6BzvQG84q6XN1169AKBN03Mm6jnQoxLdgw@mail.gmail.com>
Message-ID: <CAE2FW2=Uzd98pz=kCrFHaJWMFWL+iHd0EnJP3X7FXMged1eDSw@mail.gmail.com>

Also, I was able to extract date using
> as.Date(dat, "%m/%d/%y")
 [1] "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-12"
 [6] "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-12"
[11] "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-12"
[16] "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-13"
[21] "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13"
[26] "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13"
[31] "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13"
[36] "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13"

> class(as.Date(dat, "%m/%d/%y"))
[1] "Date"
> class(strftime(strptime(dat, format="%m/%d/%y %H:%M"), format =
"%m/%d/%y"))
[1] "character"

If I want to date caluculation, would as.Date() be more desirable? Or does
strftime() do the same thing?



On Sat, Feb 2, 2019 at 11:51 AM C W <tmrsg11 at gmail.com> wrote:

> Thank you all very much, very helpful! :)
>
> I'm just curious, what does strptime(), strftime(), as.POSIXlt.(), and
> as.POSIXct() stand for?
>
> On Sat, Feb 2, 2019 at 10:41 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
>
>> ... and in general, you need to specify the time zone to avoid surprises.
>> In many cases this can be as simple as
>>
>> Sys.setenv(TZ="GMT")
>>
>> but it can be specific to your data set also.
>>
>> On February 2, 2019 7:09:46 AM PST, Duncan Murdoch <
>> murdoch.duncan at gmail.com> wrote:
>> >On 01/02/2019 10:45 p.m., C W wrote:
>> >> Dear R community,
>> >>
>> >> I am working with dates. And I get the following error:
>> >>> strftime(dat[20], format="%H:%M")
>> >> Error in as.POSIXlt.character(as.character(x), ...) :
>> >>    character string is not in a standard unambiguous format
>> >
>> >You are using the wrong function:  strftime() formats a time object as
>> >a
>> >character string.  You want strptime() to convert character (or factor
>> >in your case) to a time object.
>> >
>> >But you need to give the format for the full string, not just the time
>> >at the end.
>> >
>> >If you really were intending to extract times from dat, then you need
>> >both conversions:
>> >
>> > > strftime(strptime(dat, format="%m/%d/%y %H:%M"), format = "%H:%M")
>> >  [1] "11:32" "11:42" "12:17" "12:31" "12:50" "14:10" "14:19" "14:59"
>> >"15:57" "16:00" "16:46" "16:51" "17:35" "17:59" "18:17" "19:07" "19:08"
>> >[18] "19:31" "21:21" "06:00" "06:20" "06:37" "06:40" "06:46" "07:20"
>> >"07:47" "07:50" "07:54" "08:11" "08:23" "08:31" "08:33" "08:43" "09:04"
>> >[35] "09:09" "09:30" "09:59" "10:01" "10:03" "10:05"
>> >
>> >Duncan Murdoch
>> >
>> >______________________________________________
>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >PLEASE do read the posting guide
>> >http://www.R-project.org/posting-guide.html
>> >and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Sent from my phone. Please excuse my brevity.
>>
>

	[[alternative HTML version deleted]]


From tmr@g11 @end|ng |rom gm@||@com  Sun Feb  3 02:19:47 2019
From: tmr@g11 @end|ng |rom gm@||@com (C W)
Date: Sat, 2 Feb 2019 20:19:47 -0500
Subject: [R] Why is there error in as.POSIXlt.character when using
 strftime()?
In-Reply-To: <81A3E377-6A75-4589-A7D5-54228B937951@dcn.davis.ca.us>
References: <CAE2FW2mTHsv6aPxFxymmPiagbaeeoHST7Vq8-jw0QY8oSHkPwA@mail.gmail.com>
 <0885ec11-fae7-7bc0-20e6-c6856c12c6dc@gmail.com>
 <3767EC7C-9841-436F-B75B-210DD61A878C@dcn.davis.ca.us>
 <CAE2FW2=sUYi5G-LL6BzvQG84q6XN1169AKBN03Mm6jnQoxLdgw@mail.gmail.com>
 <CAE2FW2=Uzd98pz=kCrFHaJWMFWL+iHd0EnJP3X7FXMged1eDSw@mail.gmail.com>
 <81A3E377-6A75-4589-A7D5-54228B937951@dcn.davis.ca.us>
Message-ID: <CAE2FW2mS=aJic5A1gu8U_ecq_xtyotsboWUZ4OzNr0aHbW-b9g@mail.gmail.com>

Thanks lots Jeff, very clear explanation.

Yes, I think I will stick with the POSIXt family of functions.

On Sat, Feb 2, 2019 at 1:33 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> If there are no time-of-day values in your data set the the Date type is
> great. However, it can be messy if you work with both Date and POSIXt types
> in the same analysis... I recommend sticking with one or the other.
>
> The trunc.POSIXt function is more appropriate for getting POSIXt dates
> than converting to character and back.
>
> On February 2, 2019 9:03:31 AM PST, C W <tmrsg11 at gmail.com> wrote:
> >Also, I was able to extract date using
> >> as.Date(dat, "%m/%d/%y")
> > [1] "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-12"
> > [6] "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-12"
> >[11] "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-12"
> >[16] "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-12" "2015-07-13"
> >[21] "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13"
> >[26] "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13"
> >[31] "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13"
> >[36] "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13" "2015-07-13"
> >
> >> class(as.Date(dat, "%m/%d/%y"))
> >[1] "Date"
> >> class(strftime(strptime(dat, format="%m/%d/%y %H:%M"), format =
> >"%m/%d/%y"))
> >[1] "character"
> >
> >If I want to date caluculation, would as.Date() be more desirable? Or
> >does
> >strftime() do the same thing?
> >
> >
> >
> >On Sat, Feb 2, 2019 at 11:51 AM C W <tmrsg11 at gmail.com> wrote:
> >
> >> Thank you all very much, very helpful! :)
> >>
> >> I'm just curious, what does strptime(), strftime(), as.POSIXlt.(),
> >and
> >> as.POSIXct() stand for?
> >>
> >> On Sat, Feb 2, 2019 at 10:41 AM Jeff Newmiller
> ><jdnewmil at dcn.davis.ca.us>
> >> wrote:
> >>
> >>> ... and in general, you need to specify the time zone to avoid
> >surprises.
> >>> In many cases this can be as simple as
> >>>
> >>> Sys.setenv(TZ="GMT")
> >>>
> >>> but it can be specific to your data set also.
> >>>
> >>> On February 2, 2019 7:09:46 AM PST, Duncan Murdoch <
> >>> murdoch.duncan at gmail.com> wrote:
> >>> >On 01/02/2019 10:45 p.m., C W wrote:
> >>> >> Dear R community,
> >>> >>
> >>> >> I am working with dates. And I get the following error:
> >>> >>> strftime(dat[20], format="%H:%M")
> >>> >> Error in as.POSIXlt.character(as.character(x), ...) :
> >>> >>    character string is not in a standard unambiguous format
> >>> >
> >>> >You are using the wrong function:  strftime() formats a time object
> >as
> >>> >a
> >>> >character string.  You want strptime() to convert character (or
> >factor
> >>> >in your case) to a time object.
> >>> >
> >>> >But you need to give the format for the full string, not just the
> >time
> >>> >at the end.
> >>> >
> >>> >If you really were intending to extract times from dat, then you
> >need
> >>> >both conversions:
> >>> >
> >>> > > strftime(strptime(dat, format="%m/%d/%y %H:%M"), format =
> >"%H:%M")
> >>> >  [1] "11:32" "11:42" "12:17" "12:31" "12:50" "14:10" "14:19"
> >"14:59"
> >>> >"15:57" "16:00" "16:46" "16:51" "17:35" "17:59" "18:17" "19:07"
> >"19:08"
> >>> >[18] "19:31" "21:21" "06:00" "06:20" "06:37" "06:40" "06:46"
> >"07:20"
> >>> >"07:47" "07:50" "07:54" "08:11" "08:23" "08:31" "08:33" "08:43"
> >"09:04"
> >>> >[35] "09:09" "09:30" "09:59" "10:01" "10:03" "10:05"
> >>> >
> >>> >Duncan Murdoch
> >>> >
> >>> >______________________________________________
> >>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> >https://stat.ethz.ch/mailman/listinfo/r-help
> >>> >PLEASE do read the posting guide
> >>> >http://www.R-project.org/posting-guide.html
> >>> >and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>> --
> >>> Sent from my phone. Please excuse my brevity.
> >>>
> >>
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sun Feb  3 15:21:52 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sun, 3 Feb 2019 14:21:52 +0000
Subject: [R] (no subject)
In-Reply-To: <EBC44F78-2D10-4686-B923-9940CB60CA1A@xs4all.nl>
References: <793972785.2617566.1549021314851.ref@mail.yahoo.com>
 <793972785.2617566.1549021314851@mail.yahoo.com>
 <EBC44F78-2D10-4686-B923-9940CB60CA1A@xs4all.nl>
Message-ID: <8adb8b2a-eeb6-db1b-59a3-dd9db0797abb@sapo.pt>

Hello,

It is just a guess but I bet that what the OP means is


f <- function(x) {
   2 + x^2 - x^3
}

uniroot(f, lower = 1, upper = 2)


To the OP: please learn how to write math in R *before* trying to solve 
problems.

Hope this helps,

Rui Barradas


?s 16:07 de 02/02/2019, Berend Hasselman escreveu:
> 
> 
>> On 1 Feb 2019, at 12:41, malika yassa via R-help <r-help at r-project.org> wrote:
>>
>> Please, can you help me I have a equation to solve by newton method but I can not do it
>> for example
>>
>> f<-function(x) {
>>
>>
>> 2+X2-X3=0}
>> this equation have un solution in [1,2]
>> is there a function in R for solve it or i have to programme it
>>
> 
> What is the relation between the function argument x and variables X2 and X3?
> 
> As it stands this is incomprehensible. The function argument x is not used anywhere in the function body.
> 
> Berend Hasselman
> 
>>
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon Feb  4 03:09:57 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sun, 3 Feb 2019 18:09:57 -0800 (PST)
Subject: [R] [FORGED] Newbie Question on R versus Matlab/Octave versus C
In-Reply-To: <dfb68d74-a7ed-cd8d-d69d-10aaf565f8a7@comcast.net>
References: <eafdc33a-61ce-03b8-2e02-3a467eda2d84@comcast.net>
 <8b2f75fe-4979-e4ad-b86f-4c9a50a93271@auckland.ac.nz>
 <bb0af392-125a-5a53-96a5-ab825756ee28@comcast.net>
 <24250968-5B39-4CEC-8649-406716F9780E@dcn.davis.ca.us>
 <235c61aa-b59c-1a9c-c5d3-ffa0614d7a65@comcast.net>
 <alpine.BSF.2.00.1901292221200.83151@pedal.dcn.davis.ca.us>
 <dfb68d74-a7ed-cd8d-d69d-10aaf565f8a7@comcast.net>
Message-ID: <alpine.BSF.2.00.1902031722160.40568@pedal.dcn.davis.ca.us>

Your code seems to be attempting to modify global variables from within 
functions... R purposely makes this hard to do. Don't fight it. Instead, 
use function arguments and produce function outputs with your functions.

Also, the ifelse function does not control flow of execution of code... it 
selects values between two vectors according to the state of the logical 
input vector. Note that all values in both possible input values must be 
computed when using ifelse before it can do its magic, so ifelse can be 
significantly slower than assigning into an indexed vector if a small 
fraction of the vector will be changing.

Below is some proof-of-concept code. It mostly modifies values in-place 
within the data frame rather than using ifelse.

You might want to read the Intro to R document available through the R 
console via:

RShowDoc("R-intro")

to look up numeric indexing and logical indexing syntax while reading 
through this.

#####################
makeNewWomen <- function( nWomen ) {
   data.frame( isAlive = rep_len( TRUE, nWomen )
             , isPregnant = rep_len( FALSE, nWomen )
             , nChildren = rep_len( 0L, nWomen )
             , age = rep_len( 0, nWomen )
             , dateOfPregnancy = rep_len( 0, nWomen )
             , endDateLastPregnancy = rep_len( 0.0, nWomen )
             )
}

updateWomen <- function( DF
                        , jd
                        , maxAge
                        , timeStep
                        , pregProb
                        , gestation
                        , minBirthAge
                        , maxBirthAge
                        ) {
   DF$isAlive[ maxAge <= DF$age ] <- FALSE
   fertileIdx <- with( DF, isAlive & !isPregnant & minBirthAge <= age & age <= maxBirthAge )
   conceiveIdx <- fertileIdx
   conceiveIdx[ conceiveIdx ] <- sample( c( FALSE, TRUE )
                                       , size = sum( fertileIdx )
                                       , replace = TRUE
                                       , prob = c( 1-pregProb, pregProb )
                                       )
   DF$isPregnant[ conceiveIdx ] <- TRUE
   DF$dateOfPregnancy[ conceiveIdx ] <- jd
   birthIdx <- with( DF, isAlive & isPregnant & ( dateOfPregnancy + gestation ) <= jd )
   femalechild <- sample( c( FALSE, TRUE )
                        , size = sum( birthIdx )  # random within birthing group
                        , replace = TRUE
                        , prob = c( 0.5, 0.5 )
                        )
   DF$isPregnant[ birthIdx ] <- FALSE # pregnancy over
   birthIdx[ birthIdx ] <- femalechild # track births further only where female
   # DF$age <- ifelse( DF$isAlive
   #                 , DF$age + timeStep
   #                 , DF$age
   #                 )
   DF$age[ DF$isAlive ] <- DF$age[ DF$isAlive ] + timeStep
   numNotAlive <- sum( !DF$isAlive )
   numBirths <- sum( birthIdx )
   if ( 0 < numBirths ) { # if needed, start female babies in existing or new rows
     if ( 0 < numNotAlive ) {
       reuseidx <- which( !DF$isAlive )
       if ( numBirths <= numNotAlive ) {
         # can fit all new births into existing DF
         reuseidx <- reuseidx[ seq.int( numBirths ) ]
         DF[ reuseidx, ] <- makeNewWomen( numBirths )
       } else {
         DF[ reuseidx, ] <- makeNewWomen( length( reuseidx ) )
         DF <- rbind( DF
                    , makeNewWomen( numBirths - length( reuseidx ) )
                    )
       }
     } else { # no empty rows in DF
       DF <- rbind( DF
                  , makeNewWomen( numBirths )
                  )
     }
   }
   DF  # return the updated data frame to the caller
}

calculatePopulation <- function( nWomen
                                , maxDate
                                , dpy
                                , pregProb
                                , maxAge
                                , timeStep
                                , gestation
                                , minBirthAge
                                , maxBirthAge
                                , prealloc
                                ) {
   jd <- 0
   nextSampleJd <- jd + dpy
   numSamples <- maxDate %/% dpy
   result <- data.frame( jd = rep( NA, numSamples )
                       , NAlive = rep( NA, numSamples )
                       , NPreg = rep( NA, numSamples )
                       , NNotAlive = rep( NA, numSamples )
                       )
   i <- 1L
   DF <- makeNewWomen( prealloc )
   DF$isAlive <- seq.int( prealloc ) <= nWomen # leave most entries "dead"
   while( jd < maxDate ) {
     DF <- updateWomen( DF
                      , jd
                      , maxAge
                      , timeStep
                      , pregProb
                      , gestation
                      , minBirthAge
                      , maxBirthAge
                      )
     if ( nextSampleJd <= jd ) {
       result$jd[ i ] <- jd
       result$NAlive[ i ] <- sum( DF$isAlive )
       result$NPreg[ i ] <- sum( DF$isPregnant )
       result$NNotAlive <- sum( !DF$isAlive )
       nextSampleJd <- nextSampleJd + dpy
       i <- i + 1L
     }
     # Do various other things
     jd <- jd + timeStep
   }
   result
}

nWomen <- 5
numberOfYears <- 30
maxDate <- 300 * 365
dpy <- 365
pregProb <- 0.01
maxAge <- 50 * 365
minBirthAge <- 18 * 365
maxBirthAge <- 45 * 365
timeStep <- 30
gestation <- 30 * 9
prealloc <- 10000
set.seed(42)
simresult <- calculatePopulation( nWomen
                                 , maxDate
                                 , dpy
                                 , pregProb
                                 , maxAge
                                 , timeStep
                                 , gestation
                                 , minBirthAge
                                 , maxBirthAge
                                 , prealloc
                                 )

plot( simresult$jd/365, simresult$NAlive )
plot( simresult$jd/365, simresult$NNotAlive )
plot( simresult$jd/365, simresult$NPreg )
#####################

On Sat, 2 Feb 2019, Alan Feuerbacher wrote:

> On 1/29/2019 11:50 PM, Jeff Newmiller wrote:
>> On Tue, 29 Jan 2019, Alan Feuerbacher wrote:
>> 
>>> After my failed attempt at using Octave, I realized that most likely the 
>>> main contributing factor was that I was not able to figure out an 
>>> efficient data structure to model one person. But C lent itself perfectly 
>>> to my idea of how to go about programming my simulation. So here's a 
>>> simplified pseudocode sort of example of what I did:
>> 
>> Don't model one person... model an array of people.
>> 
>>> To model a single reproducing woman I used this C construct:
>>> 
>>> typedef struct woman {
>>> ?int isAlive;
>>> ?int isPregnant;
>>> ?double age;
>>> ?. . .
>>> } WOMAN;
>> 
>> # e.g.
>> Nwomen <- 100
>> women <- data.frame( isAlive = rep( TRUE, Nwomen )
>>  ?????????????????? , isPregnant = rep( FALSE, Nwomen )
>>  ?????????????????? , age = rep( 20, Nwomen )
>>  ?????????????????? )
>> 
>>> Then I allocated memory for a big array of these things, using the C 
>>> malloc() function, which gave me the equivalent of this statement:
>>> 
>>> WOMAN women[NWOMEN];? /* An array of NWOMEN woman-structs */
>>> 
>>> After some initialization I set up two loops:
>>> 
>>> for( j=0; j<numberOfYears; j++) {
>>> ?for(i=1; i< numberOfWomen; i++) {
>>> ?? updateWomen();
>>> ?}
>>> }
>> 
>> for ( j in seq.int( numberOfYears ) {
>>  ? # let vectorized data storage automatically handle the other for loop
>>  ? women <- updateWomen( women )
>> }
>> 
>>> The function updateWomen() figures out things like whether the woman 
>>> becomes pregnant or gives birth on a given day, dies, etc.
>> 
>> You can use your "fixed size" allocation strategy with flags indicating 
>> whether specific rows are in use, or you can only work with valid rows and 
>> add rows as needed for children... best to compute a logical vector that 
>> identifies all of the birthing mothers as a subset of the data frame, and 
>> build a set of children rows using the birthing mothers data frame as 
>> input, and then rbind the new rows to the updated women dataframe as 
>> appropriate. The most clear approach for individual decision calculations 
>> is the use of the vectorized "ifelse" function, though under certain 
>> circumstances putting an indexed subset on the left side of an assignment 
>> can modify memory "in place" (the functional-programming restriction 
>> against this is probably a foreign idea to a dyed-in-the-wool C programmer, 
>> but R usually prevents you from modifying the variable that was input to a 
>> function, automatically making a local copy of the input as needed in order 
>> to prevent such backwash into the caller's context).
>
> Hi Jeff,
>
> I'm well along in implementing your suggestions, but I don't understand the 
> last paragraph. Here is part of the experimenting I've done so far:
>
> *=======*=======*=======*=======*=======*=======*
> updatePerson <- function() {
>  ifelse( women$isAlive,
>    {
> # Check whether to kill off this person, if she's pregnant whether
> # to give birth, whether to make her pregnant again.
>      women$age = women$age + timeStep
> # Check if the person has reached maxAge
>    }
>  )
> }
>
> calculatePopulation <- function() {
>  lastDate = 0
>  jd = 0
>  while( jd < maxDate ) {
>    for( i in seq_len( nWomen ) ) {
>      updatePerson();
>    }
>    todaysDateInt = floor(jd/dpy)
>    NAlive[todaysDateInt] = nWomen - nDead
> # Do various other things
>    todaysDate = todaysDate + timeStep
>    jd = jd + timeStep
>  }
> }
>
> nWomen <- 5
> numberOfYears <- 30
> women <- data.frame( isAlive = rep_len( TRUE, nWomen )
>                   , isPregnant = rep_len( FALSE, nWomen )
>                   , nChildren = rep_len( 0L, nWomen )
>                   , ageInt = rep_len( 0L, nWomen )
>                   , age = rep_len( 0, nWomen )
>                   , dateOfPregnancy = rep_len( 0, nWomen )
>                   , endDateLastPregnancy = rep_len( 0.0, nWomen )
>                   , minBirthAge = rep_len( 0, nWomen )
>                   , maxBirthAge = rep_len( 0, nWomen )
>                   )
>
> # . . .
>
>  calculatePopulation()
>
> *=======*=======*=======*=======*=======*=======*
>
> The above code (in its complete form) executes without errors. I don't 
> understand at least two things:
>
> In the updatePerson function, in the ifelse statement, how do I change the 
> appropriate values in the women dataframe?
>
> I don't understand most of your last paragraph at all.
>
> Thanks so much for your help in learning R!
>
> Alan
>
> ---
> This email has been checked for viruses by Avast antivirus software.
> https://www.avast.com/antivirus
>
>

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
---------------------------------------------------------------------------

From pd@|gd @end|ng |rom gm@||@com  Mon Feb  4 09:57:37 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Mon, 4 Feb 2019 09:57:37 +0100
Subject: [R] Why is there error in as.POSIXlt.character when using
 strftime()?
In-Reply-To: <0F9B4A80-DE0B-48FB-9F3C-539FE9568C6C@dcn.davis.ca.us>
References: <CAE2FW2mTHsv6aPxFxymmPiagbaeeoHST7Vq8-jw0QY8oSHkPwA@mail.gmail.com>
 <0885ec11-fae7-7bc0-20e6-c6856c12c6dc@gmail.com>
 <3767EC7C-9841-436F-B75B-210DD61A878C@dcn.davis.ca.us>
 <CAE2FW2=sUYi5G-LL6BzvQG84q6XN1169AKBN03Mm6jnQoxLdgw@mail.gmail.com>
 <0F9B4A80-DE0B-48FB-9F3C-539FE9568C6C@dcn.davis.ca.us>
Message-ID: <83C771B2-A5A1-4093-9772-1932A53BB038@gmail.com>

* localtime format, I believe. 

> On 2 Feb 2019, at 18:02 , Jeff Newmiller <jdnewmil at dcn.davis.CA.us> wrote:
> 
> as.POSIXlt = convert to POSIX list time format (same as strptime but name fits type conversion function naming pattern in R)

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From motyoc@k@ @end|ng |rom y@hoo@com  Mon Feb  4 22:01:06 2019
From: motyoc@k@ @end|ng |rom y@hoo@com (Andras Farkas)
Date: Mon, 4 Feb 2019 21:01:06 +0000 (UTC)
Subject: [R] list with list function
References: <1814264961.2897866.1549314066076.ref@mail.yahoo.com>
Message-ID: <1814264961.2897866.1549314066076@mail.yahoo.com>

Hello everyone,

wonder if you would have a thought on a function for the following:


we have

a<-sample(seq(as.Date('1999/01/01'), as.Date('2000/01/01'), by="day"),5)
b<-sample(seq(as.Date('1999/01/01'), as.Date('2000/01/01'), by="day"), 4)
c<-sample(seq(as.Date('1999/01/01'), as.Date('2000/01/01'), by="day"), 3)

d<-c(1,3,5)
e<-c(1,4)
f<-c(1,2)

listA<-list(a,b,c)
listB<-list(d,e,f)


what I would like to do with a function (my real listA and listB can be of any length but always equal length, but their components like a,b,and c those can be unequal) as opposed to manually is to derive the following answer

listfinal<-list(a[-d],b[-e],c[-f])
listfinal


essentially the elements in listB serve as identifying the position of corresponding list element in listA and removing it from listA.?

these lists listA and listB in practice are columns of a data frame that I am trying to work with and were generated with a function using lapply...

appreciate any thoughts you may have to make this functional...

thanks,

Andras?


From kry|ov@r00t @end|ng |rom gm@||@com  Mon Feb  4 22:21:09 2019
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Tue, 5 Feb 2019 00:21:09 +0300
Subject: [R] list with list function
In-Reply-To: <1814264961.2897866.1549314066076@mail.yahoo.com>
References: <1814264961.2897866.1549314066076.ref@mail.yahoo.com>
 <1814264961.2897866.1549314066076@mail.yahoo.com>
Message-ID: <20190205002109.501d748f@Tarkus>

On Mon, 4 Feb 2019 21:01:06 +0000 (UTC)
Andras Farkas via R-help <r-help at r-project.org> wrote:

> listA<-list(a,b,c)
> listB<-list(d,e,f)
> 
> what I would like to do with a function <...> as opposed to manually
> is to derive the following answer
> 
> listfinal<-list(a[-d],b[-e],c[-f])

The `Map` function, unlike `lapply`, iterates over its arguments
simultaneously:

Map(function(dt, idx) dt[-idx], listA, listB)
# [[1]]
# [1] "1999-07-31" "1999-06-12"
#
# [[2]]
# [1] "1999-03-10" "1999-04-04"
#
# [[3]]
# [1] "1999-08-07"


-- 
Best regards,
Ivan


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Mon Feb  4 22:18:32 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Mon, 4 Feb 2019 21:18:32 +0000
Subject: [R] list with list function
In-Reply-To: <1814264961.2897866.1549314066076@mail.yahoo.com>
References: <1814264961.2897866.1549314066076.ref@mail.yahoo.com>
 <1814264961.2897866.1549314066076@mail.yahoo.com>
Message-ID: <02f5caab-5ab5-185c-a621-8197819a9d44@sapo.pt>

Hello,

Like this?


Map('[', listA, lapply(listB, '*', -1))


Hope this helps,

Rui Barradas

?s 21:01 de 04/02/2019, Andras Farkas via R-help escreveu:
> Hello everyone,
> 
> wonder if you would have a thought on a function for the following:
> 
> 
> we have
> 
> a<-sample(seq(as.Date('1999/01/01'), as.Date('2000/01/01'), by="day"),5)
> b<-sample(seq(as.Date('1999/01/01'), as.Date('2000/01/01'), by="day"), 4)
> c<-sample(seq(as.Date('1999/01/01'), as.Date('2000/01/01'), by="day"), 3)
> 
> d<-c(1,3,5)
> e<-c(1,4)
> f<-c(1,2)
> 
> listA<-list(a,b,c)
> listB<-list(d,e,f)
> 
> 
> what I would like to do with a function (my real listA and listB can be of any length but always equal length, but their components like a,b,and c those can be unequal) as opposed to manually is to derive the following answer
> 
> listfinal<-list(a[-d],b[-e],c[-f])
> listfinal
> 
> 
> essentially the elements in listB serve as identifying the position of corresponding list element in listA and removing it from listA.
> 
> these lists listA and listB in practice are columns of a data frame that I am trying to work with and were generated with a function using lapply...
> 
> appreciate any thoughts you may have to make this functional...
> 
> thanks,
> 
> Andras
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jkk@p|@n6 @end|ng |rom gm@||@com  Mon Feb  4 15:05:37 2019
From: jkk@p|@n6 @end|ng |rom gm@||@com (Jacob)
Date: Mon, 4 Feb 2019 09:05:37 -0500
Subject: [R] [R-pkgs] Major Update to asciiSetupReader Package
Message-ID: <CAE-oFUdnZ=g8TDZyhtZ2cmudWt-Lhk4nLnF_P_hTc9rkgXbe-A@mail.gmail.com>

 The latest version of the package asciiSetupReader includes a number of
major improvements and bug fixes. This package lets you read in .dat+.sps
and .dat+.sas pair files.

See the following for an overview of the package:
https://jacobkap.github.io/asciiSetupReader/


For a list of changes please see here:
https://cran.r-project.org/web/packages/asciiSetupReader/news/news.html

Best,
Jacob

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From motyoc@k@ @end|ng |rom y@hoo@com  Tue Feb  5 18:11:41 2019
From: motyoc@k@ @end|ng |rom y@hoo@com (Andras Farkas)
Date: Tue, 5 Feb 2019 17:11:41 +0000 (UTC)
Subject: [R] list with list function
In-Reply-To: <02f5caab-5ab5-185c-a621-8197819a9d44@sapo.pt>
References: <1814264961.2897866.1549314066076.ref@mail.yahoo.com>
 <1814264961.2897866.1549314066076@mail.yahoo.com>
 <02f5caab-5ab5-185c-a621-8197819a9d44@sapo.pt>
Message-ID: <1858884259.3386345.1549386701112@mail.yahoo.com>

Thanks Rui and Ivan, works perfectly...
Andras
    On Monday, February 4, 2019, 4:18:39 PM EST, Rui Barradas <ruipbarradas at sapo.pt> wrote:  
 
 Hello,

Like this?


Map('[', listA, lapply(listB, '*', -1))


Hope this helps,

Rui Barradas

?s 21:01 de 04/02/2019, Andras Farkas via R-help escreveu:
> Hello everyone,
> 
> wonder if you would have a thought on a function for the following:
> 
> 
> we have
> 
> a<-sample(seq(as.Date('1999/01/01'), as.Date('2000/01/01'), by="day"),5)
> b<-sample(seq(as.Date('1999/01/01'), as.Date('2000/01/01'), by="day"), 4)
> c<-sample(seq(as.Date('1999/01/01'), as.Date('2000/01/01'), by="day"), 3)
> 
> d<-c(1,3,5)
> e<-c(1,4)
> f<-c(1,2)
> 
> listA<-list(a,b,c)
> listB<-list(d,e,f)
> 
> 
> what I would like to do with a function (my real listA and listB can be of any length but always equal length, but their components like a,b,and c those can be unequal) as opposed to manually is to derive the following answer
> 
> listfinal<-list(a[-d],b[-e],c[-f])
> listfinal
> 
> 
> essentially the elements in listB serve as identifying the position of corresponding list element in listA and removing it from listA.
> 
> these lists listA and listB in practice are columns of a data frame that I am trying to work with and were generated with a function using lapply...
> 
> appreciate any thoughts you may have to make this functional...
> 
> thanks,
> 
> Andras
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
  
	[[alternative HTML version deleted]]


From joh@mo| @end|ng |rom @tud@ntnu@no  Tue Feb  5 15:06:15 2019
From: joh@mo| @end|ng |rom @tud@ntnu@no (=?Windows-1252?Q?Johannes_M=F8llerhagen?=)
Date: Tue, 5 Feb 2019 14:06:15 +0000
Subject: [R] Help converting file to XTS
Message-ID: <DB6PR0501MB27104572451305156ED51D978E6E0@DB6PR0501MB2710.eurprd05.prod.outlook.com>

Hello there! I am a master student working on my master thesis, and I am trying to convert some data to xts so I can apply a highfrequency package to it.

At the moment I am trying to use a POSIXct function. I am quite new at this program and I am having some issue. The file is  attached.


The current coding is:


dat<-read_csv("TEL5minint.csv")
xts(dat,order.by=as.POSIXct(dat),"%d/%m/%Y %H:%M")


And the error is:

Error in as.POSIXct.default(dat) :
  do not know how to convert 'dat' to class ?POSIXct?


Any help is appreciated!


Kind Regards

Johannes

From bgunter@4567 @end|ng |rom gm@||@com  Tue Feb  5 18:43:53 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 5 Feb 2019 09:43:53 -0800
Subject: [R] Help converting file to XTS
In-Reply-To: <DB6PR0501MB27104572451305156ED51D978E6E0@DB6PR0501MB2710.eurprd05.prod.outlook.com>
References: <DB6PR0501MB27104572451305156ED51D978E6E0@DB6PR0501MB2710.eurprd05.prod.outlook.com>
Message-ID: <CAGxFJbQR7wbWmKtZBaUR7zDEyXYnK5Mdoxx=dAnLciLtJ6C_hA@mail.gmail.com>

Hard to say without knowing what dat looks like.

Can you show us a small sample, perhaps via dput( head( dat))  ?
See ?dput, ?head for details.

A guess would be that dat is a data frame and not a character string, but
????


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Feb 5, 2019 at 9:24 AM Johannes M?llerhagen <johamol at stud.ntnu.no>
wrote:

> Hello there! I am a master student working on my master thesis, and I am
> trying to convert some data to xts so I can apply a highfrequency package
> to it.
>
> At the moment I am trying to use a POSIXct function. I am quite new at
> this program and I am having some issue. The file is  attached.
>
>
> The current coding is:
>
>
> dat<-read_csv("TEL5minint.csv")
> xts(dat,order.by=as.POSIXct(dat),"%d/%m/%Y %H:%M")
>
>
> And the error is:
>
> Error in as.POSIXct.default(dat) :
>   do not know how to convert 'dat' to class ?POSIXct?
>
>
> Any help is appreciated!
>
>
> Kind Regards
>
> Johannes
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Tue Feb  5 18:49:37 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Tue, 5 Feb 2019 09:49:37 -0800
Subject: [R] Help converting file to XTS
In-Reply-To: <DB6PR0501MB27104572451305156ED51D978E6E0@DB6PR0501MB2710.eurprd05.prod.outlook.com>
References: <DB6PR0501MB27104572451305156ED51D978E6E0@DB6PR0501MB2710.eurprd05.prod.outlook.com>
Message-ID: <70a3b1bd-099f-dd35-bb52-d89f9a79714f@comcast.net>


On 2/5/19 6:06 AM, Johannes M?llerhagen wrote:
> Hello there! I am a master student working on my master thesis, and I am trying to convert some data to xts so I can apply a highfrequency package to it.
>
> At the moment I am trying to use a POSIXct function. I am quite new at this program and I am having some issue. The file is  attached.
>
>
> The current coding is:
>
>
> dat<-read_csv("TEL5minint.csv")


So dat is now a dataframe

> xts(dat,order.by=as.POSIXct(dat),"%d/%m/%Y %H:%M")


And you are supplying a list (which is what dataframes are) to the 
order.by parameter.

>
>
> And the error is:
>
> Error in as.POSIXct.default(dat) :
>    do not know how to convert 'dat' to class ?POSIXct?

The function is telling you that dat is the wrong type of object to be 
converted to the rownames of a matrix (which is what xts objects are.


-- 

David

>
>
> Any help is appreciated!
>
>
> Kind Regards
>
> Johannes
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @@@@@ko@@n|c @end|ng |rom gm@||@com  Tue Feb  5 18:56:37 2019
From: @@@@@ko@@n|c @end|ng |rom gm@||@com (sasa kosanic)
Date: Tue, 5 Feb 2019 18:56:37 +0100
Subject: [R] problems when merging two data sets
Message-ID: <CAJanvzFpsoYZb43k4FnQOy_tz_KMoL3Xv-egWUcne0s-yMywgA@mail.gmail.com>

Dear All,

I would like to merge two data sets however I am doing something wrong...
1 data set contains 2 columns of  'species occurrence'(1 column) in Germany
and  'species names' (2 column).
and the second one names of 'Red list species'(1 column) and 'species
status' (2 column).
so I would like to merge Red list species with species names from the first
table and to sign the  species status
I have tried with merge function but got this an error:" 'by' must specify
a uniquely valid column"
I also tried with the function left_join, however no success.

Also columns in two data sets are different in size. 1 table has 7189 rows
and 2 table just 426 rows as we do not have much Red list Species.

I would appreciate your help.

Kind regards,
Sasha


Dr Sasha Kosanic
Ecology Lab (Biology Department)
Room M842
University of Konstanz
Universit?tsstra?e 10
D-78464 Konstanz
Phone: +49 7531 883321 & +49 (0)175 9172503

http://cms.uni-konstanz.de/vkleunen/
https://tinyurl.com/y8u5wyoj
https://tinyurl.com/cgec6tu

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue Feb  5 19:39:14 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 05 Feb 2019 10:39:14 -0800
Subject: [R] Help converting file to XTS
In-Reply-To: <DB6PR0501MB27104572451305156ED51D978E6E0@DB6PR0501MB2710.eurprd05.prod.outlook.com>
References: <DB6PR0501MB27104572451305156ED51D978E6E0@DB6PR0501MB2710.eurprd05.prod.outlook.com>
Message-ID: <9AF113E3-B3E0-4337-BC50-772281537133@dcn.davis.ca.us>

One thing about POSIXct or POSIXlt... you always need to address the issue of timezone. If your timestamp data are simple (no daylight savings) you may be able to get away with a simple

Sys.setenv( TZ="GMT" )

sometime in each R session prior to converting anything to this type (e.g at the beginning of your script).

You should read the help pages for read_csv

?read_csv

to find out that the object created by that function is a tibble (a variety of data frame). Then read

?xts

to find out that the order.by must be a time-based class (which requirement POSIXct meets), and then

?as.POSIXct

which sadly only says that x has to be an R object. There are a variety of specializations for this function, e.g. as.POSIXct.Date, as.POSIXct.numeric, as.POSIXct.POSIXlt, and as.POSIXct.default, the last of which handles conversion from character or factor data types. Note that none of these options include converting an entire data frame.

Since dat is a tibble you will need to use $ or `[[` indexing to extract the one column that contains the timestamps:

dat[[ 1 ]]

and convert it

as.POSIXct( dat[[ 1 ]], format="%d/%m/%Y %H:%M" )

and use that as the order.by argument

result <- xts( dat[-1] ,order.by= as.POSIXct( dat[[ 1 ]], format="%d/%m/%Y %H:%M" ) )

[1] https://stackoverflow.com/questions/9327700/read-data-from-excel-to-r-and-convert-to-xts

On February 5, 2019 6:06:15 AM PST, "Johannes M?llerhagen" <johamol at stud.ntnu.no> wrote:
>Hello there! I am a master student working on my master thesis, and I
>am trying to convert some data to xts so I can apply a highfrequency
>package to it.
>
>At the moment I am trying to use a POSIXct function. I am quite new at
>this program and I am having some issue. The file is  attached.
>
>
>The current coding is:
>
>
>dat<-read_csv("TEL5minint.csv")
>xts(dat,order.by=as.POSIXct(dat),"%d/%m/%Y %H:%M")
>
>
>And the error is:
>
>Error in as.POSIXct.default(dat) :
>  do not know how to convert 'dat' to class ?POSIXct?
>
>
>Any help is appreciated!
>
>
>Kind Regards
>
>Johannes
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue Feb  5 19:46:39 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 05 Feb 2019 10:46:39 -0800
Subject: [R] problems when merging two data sets
In-Reply-To: <CAJanvzFpsoYZb43k4FnQOy_tz_KMoL3Xv-egWUcne0s-yMywgA@mail.gmail.com>
References: <CAJanvzFpsoYZb43k4FnQOy_tz_KMoL3Xv-egWUcne0s-yMywgA@mail.gmail.com>
Message-ID: <072761C7-839C-4FAC-A563-F138A99B4361@dcn.davis.ca.us>

There are many examples of how to do this properly on the web, and many ways you could have failed to follow those examples. You need to be much more specific (using actual R code) about what you did in order for us to help you get past your specific error. [1][2][3]

You will also avoid the what-we-see-is-different-than-what-you-saw problems with your email if you read the Posting Guide and insure that your email client is configured to send plain text format rather than HTML- format email to the mailing list.

[1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

[2] http://adv-r.had.co.nz/Reproducibility.html

[3] https://cran.r-project.org/web/packages/reprex/index.html (read the vignette) 


On February 5, 2019 9:56:37 AM PST, sasa kosanic <sasa.kosanic at gmail.com> wrote:
>Dear All,
>
>I would like to merge two data sets however I am doing something
>wrong...
>1 data set contains 2 columns of  'species occurrence'(1 column) in
>Germany
>and  'species names' (2 column).
>and the second one names of 'Red list species'(1 column) and 'species
>status' (2 column).
>so I would like to merge Red list species with species names from the
>first
>table and to sign the  species status
>I have tried with merge function but got this an error:" 'by' must
>specify
>a uniquely valid column"
>I also tried with the function left_join, however no success.
>
>Also columns in two data sets are different in size. 1 table has 7189
>rows
>and 2 table just 426 rows as we do not have much Red list Species.
>
>I would appreciate your help.
>
>Kind regards,
>Sasha
>
>
>Dr Sasha Kosanic
>Ecology Lab (Biology Department)
>Room M842
>University of Konstanz
>Universit?tsstra?e 10
>D-78464 Konstanz
>Phone: +49 7531 883321 & +49 (0)175 9172503
>
>http://cms.uni-konstanz.de/vkleunen/
>https://tinyurl.com/y8u5wyoj
>https://tinyurl.com/cgec6tu
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From bgunter@4567 @end|ng |rom gm@||@com  Tue Feb  5 19:49:41 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 5 Feb 2019 10:49:41 -0800
Subject: [R] problems when merging two data sets
In-Reply-To: <CAJanvzFpsoYZb43k4FnQOy_tz_KMoL3Xv-egWUcne0s-yMywgA@mail.gmail.com>
References: <CAJanvzFpsoYZb43k4FnQOy_tz_KMoL3Xv-egWUcne0s-yMywgA@mail.gmail.com>
Message-ID: <CAGxFJbR4mDMwf9MmhaJzbuAp2EpVkSxfnLt00sK5DqpSwGLCKQ@mail.gmail.com>

Show us your code! (as the posting guide below requests. Please read the
posting guide).


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Feb 5, 2019 at 10:04 AM sasa kosanic <sasa.kosanic at gmail.com> wrote:

> Dear All,
>
> I would like to merge two data sets however I am doing something wrong...
> 1 data set contains 2 columns of  'species occurrence'(1 column) in Germany
> and  'species names' (2 column).
> and the second one names of 'Red list species'(1 column) and 'species
> status' (2 column).
> so I would like to merge Red list species with species names from the first
> table and to sign the  species status
> I have tried with merge function but got this an error:" 'by' must specify
> a uniquely valid column"
> I also tried with the function left_join, however no success.
>
> Also columns in two data sets are different in size. 1 table has 7189 rows
> and 2 table just 426 rows as we do not have much Red list Species.
>
> I would appreciate your help.
>
> Kind regards,
> Sasha
>
>
> Dr Sasha Kosanic
> Ecology Lab (Biology Department)
> Room M842
> University of Konstanz
> Universit?tsstra?e 10
> D-78464 Konstanz
> Phone: +49 7531 883321 & +49 (0)175 9172503
>
> http://cms.uni-konstanz.de/vkleunen/
> https://tinyurl.com/y8u5wyoj
> https://tinyurl.com/cgec6tu
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Tue Feb  5 23:22:24 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Wed, 6 Feb 2019 11:22:24 +1300
Subject: [R] data.frame() versus as.data.frame() applied to a matrix.
Message-ID: <32f3ee5a-2683-58ad-1336-c3c9bc156668@auckland.ac.nz>


Consider the following:

set.seed(42)
X <- matrix(runif(40),10,4)
colnames(X) <- c("a","b","a:x","b:x") # Imitating the output
                                       # of model.matrix().
D1 <- as.data.frame(X)
D2 <- data.frame(X)
names(D1)
[1] "a"   "b"   "a:x" "b:x"
names(D2)
[1] "a"   "b"   "a.x" "b.x"

The names of D2 are syntactically valid; those of D1 are not.

Why should I have expected this phenomenon? :-)

The as.data.frame() syntax seems to me much more natural for converting 
a matrix to a data frame, yet it doesn't get it quite right, sometimes,
in respect of the names.

Is there some reason that as.data.frame() does not apply make.names()?
Or was this just an oversight?

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From reichm@@j m@iii@g oii sbcgiob@i@@et  Tue Feb  5 23:28:28 2019
From: reichm@@j m@iii@g oii sbcgiob@i@@et (reichm@@j m@iii@g oii sbcgiob@i@@et)
Date: Tue, 5 Feb 2019 16:28:28 -0600
Subject: [R] Calendar Heat Map
Message-ID: <003001d4bda2$1e8b8a40$5ba29ec0$@sbcglobal.net>

r-Help Form

 

I'm working on a "Time-Series Calendar Heatmap" using the following code.

 

ggplot(myData, aes(monthweek, weekdayf, fill = myData $adjusted)) + 

geom_tile(colour = "white") + facet_grid(year(myData $date)~monthf) + 

scale_fill_gradient(low="red", high="green") + 

xlab("Week of Month") + ylab("") + 

ggtitle("Time-Series Calendar Heatmap ") + labs(fill = "Price")

 

While the ggplot commands do (almost) what I want I can't figure out how to
change my color scaling. While scale_fill_gradient(low="red", high="green")
does what I ask, that is create a color gradient from red to green it not
what I thought it would be. What I need is discreet colors something like 0
- grey; 1:5 - blue; 6:10 - green etc.  How to I set discrete colors for
groups of values. A color ramp would work but I need to separately color
those cells with 0 counts.

 

Jeff Reichman


	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Feb  6 00:27:26 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 05 Feb 2019 15:27:26 -0800
Subject: [R] data.frame() versus as.data.frame() applied to a matrix.
In-Reply-To: <32f3ee5a-2683-58ad-1336-c3c9bc156668@auckland.ac.nz>
References: <32f3ee5a-2683-58ad-1336-c3c9bc156668@auckland.ac.nz>
Message-ID: <71FC15E0-53A2-4689-8079-B4FC2C9BEECC@dcn.davis.ca.us>

I have no idea about "why it is this way" but there are many cases where I would rather have to use backticks around syntactically-invalid names than deal with arbitrary rules for mapping column names as they were supplied to column names as R wants them to be. From that perspective, making the conversion function leave the names alone and limit the name-mashing to one function sounds great to me. You can always call make.names yourself.

On February 5, 2019 2:22:24 PM PST, Rolf Turner <r.turner at auckland.ac.nz> wrote:
>
>Consider the following:
>
>set.seed(42)
>X <- matrix(runif(40),10,4)
>colnames(X) <- c("a","b","a:x","b:x") # Imitating the output
>                                       # of model.matrix().
>D1 <- as.data.frame(X)
>D2 <- data.frame(X)
>names(D1)
>[1] "a"   "b"   "a:x" "b:x"
>names(D2)
>[1] "a"   "b"   "a.x" "b.x"
>
>The names of D2 are syntactically valid; those of D1 are not.
>
>Why should I have expected this phenomenon? :-)
>
>The as.data.frame() syntax seems to me much more natural for converting
>
>a matrix to a data frame, yet it doesn't get it quite right, sometimes,
>in respect of the names.
>
>Is there some reason that as.data.frame() does not apply make.names()?
>Or was this just an oversight?
>
>cheers,
>
>Rolf Turner

-- 
Sent from my phone. Please excuse my brevity.


From r@turner @end|ng |rom @uck|@nd@@c@nz  Wed Feb  6 00:52:46 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Wed, 6 Feb 2019 12:52:46 +1300
Subject: [R] data.frame() versus as.data.frame() applied to a matrix.
In-Reply-To: <71FC15E0-53A2-4689-8079-B4FC2C9BEECC@dcn.davis.ca.us>
References: <32f3ee5a-2683-58ad-1336-c3c9bc156668@auckland.ac.nz>
 <71FC15E0-53A2-4689-8079-B4FC2C9BEECC@dcn.davis.ca.us>
Message-ID: <f750e4b7-3247-4ccd-797e-03f568d0484c@auckland.ac.nz>


On 2/6/19 12:27 PM, Jeff Newmiller wrote:

> I have no idea about "why it is this way" but there are many cases
> where I would rather have to use backticks around
> syntactically-invalid names than deal with arbitrary rules for
> mapping column names as they were supplied to column names as R wants
> them to be. From that perspective, making the conversion function
> leave the names alone and limit the name-mashing to one function
> sounds great to me. You can always call make.names yourself.

Fair enough.  My real problem was getting ambushed by the fact that 
*different* names arise depending on whether one uses data.frame(X)
or as.data.frame(X).  I'll spare you the details. :-)

cheers,

Rolf

> 
> On February 5, 2019 2:22:24 PM PST, Rolf Turner
> <r.turner at auckland.ac.nz> wrote:
>> 
>> Consider the following:
>> 
>> set.seed(42) X <- matrix(runif(40),10,4) colnames(X) <-
>> c("a","b","a:x","b:x") # Imitating the output # of model.matrix(). 
>> D1 <- as.data.frame(X) D2 <- data.frame(X) names(D1) [1] "a"   "b"
>> "a:x" "b:x" names(D2) [1] "a"   "b"   "a.x" "b.x"
>> 
>> The names of D2 are syntactically valid; those of D1 are not.
>> 
>> Why should I have expected this phenomenon? :-)
>> 
>> The as.data.frame() syntax seems to me much more natural for
>> converting
>> 
>> a matrix to a data frame, yet it doesn't get it quite right,
>> sometimes, in respect of the names.
>> 
>> Is there some reason that as.data.frame() does not apply
>> make.names()? Or was this just an oversight?


From drj|m|emon @end|ng |rom gm@||@com  Wed Feb  6 01:10:15 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 6 Feb 2019 11:10:15 +1100
Subject: [R] problems when merging two data sets
In-Reply-To: <CAJanvzFpsoYZb43k4FnQOy_tz_KMoL3Xv-egWUcne0s-yMywgA@mail.gmail.com>
References: <CAJanvzFpsoYZb43k4FnQOy_tz_KMoL3Xv-egWUcne0s-yMywgA@mail.gmail.com>
Message-ID: <CA+8X3fX1inXPr91a61DcyHwZf7Ymczt09BBwF5exdwwJ41jHbA@mail.gmail.com>

Hi Sasha,
I'll take a wild guess that your column names have periods (.)
replacing the spaces in the names you use:

species occurrence -> species.occurrence

The error message means that R can't find the variable name you have
used in the "by" argument. The second wild guess is that your column
names for the species names are different and you must use the "by.x"
and "by.y" arguments instead of just "by".

Jim

On Wed, Feb 6, 2019 at 5:04 AM sasa kosanic <sasa.kosanic at gmail.com> wrote:
>
> Dear All,
>
> I would like to merge two data sets however I am doing something wrong...
> 1 data set contains 2 columns of  'species occurrence'(1 column) in Germany
> and  'species names' (2 column).
> and the second one names of 'Red list species'(1 column) and 'species
> status' (2 column).
> so I would like to merge Red list species with species names from the first
> table and to sign the  species status
> I have tried with merge function but got this an error:" 'by' must specify
> a uniquely valid column"
> I also tried with the function left_join, however no success.
>
> Also columns in two data sets are different in size. 1 table has 7189 rows
> and 2 table just 426 rows as we do not have much Red list Species.
>
> I would appreciate your help.
>
> Kind regards,
> Sasha
>
>
> Dr Sasha Kosanic
> Ecology Lab (Biology Department)
> Room M842
> University of Konstanz
> Universit?tsstra?e 10
> D-78464 Konstanz
> Phone: +49 7531 883321 & +49 (0)175 9172503
>
> http://cms.uni-konstanz.de/vkleunen/
> https://tinyurl.com/y8u5wyoj
> https://tinyurl.com/cgec6tu
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rmh @end|ng |rom temp|e@edu  Wed Feb  6 01:18:54 2019
From: rmh @end|ng |rom temp|e@edu (Richard M. Heiberger)
Date: Tue, 5 Feb 2019 19:18:54 -0500
Subject: [R] data.frame() versus as.data.frame() applied to a matrix.
In-Reply-To: <f750e4b7-3247-4ccd-797e-03f568d0484c@auckland.ac.nz>
References: <32f3ee5a-2683-58ad-1336-c3c9bc156668@auckland.ac.nz>
 <71FC15E0-53A2-4689-8079-B4FC2C9BEECC@dcn.davis.ca.us>
 <f750e4b7-3247-4ccd-797e-03f568d0484c@auckland.ac.nz>
Message-ID: <CAGx1TMAe4fFdZ3SwBMGv5=hfkPGY+qN2zTF5aQg1d-LkXCYXLQ@mail.gmail.com>

To me the interesting difference between matrix() and as.matrix() is
that as.matrix() retains the argument names as the rows names of the
result.
> tmp <- structure(1:3, names=letters[1:3])
> tmp
a b c
1 2 3
> matrix(tmp)
     [,1]
[1,]    1
[2,]    2
[3,]    3
> as.matrix(tmp)
  [,1]
a    1
b    2
c    3
>

On Tue, Feb 5, 2019 at 6:53 PM Rolf Turner <r.turner at auckland.ac.nz> wrote:
>
>
> On 2/6/19 12:27 PM, Jeff Newmiller wrote:
>
> > I have no idea about "why it is this way" but there are many cases
> > where I would rather have to use backticks around
> > syntactically-invalid names than deal with arbitrary rules for
> > mapping column names as they were supplied to column names as R wants
> > them to be. From that perspective, making the conversion function
> > leave the names alone and limit the name-mashing to one function
> > sounds great to me. You can always call make.names yourself.
>
> Fair enough.  My real problem was getting ambushed by the fact that
> *different* names arise depending on whether one uses data.frame(X)
> or as.data.frame(X).  I'll spare you the details. :-)
>
> cheers,
>
> Rolf
>
> >
> > On February 5, 2019 2:22:24 PM PST, Rolf Turner
> > <r.turner at auckland.ac.nz> wrote:
> >>
> >> Consider the following:
> >>
> >> set.seed(42) X <- matrix(runif(40),10,4) colnames(X) <-
> >> c("a","b","a:x","b:x") # Imitating the output # of model.matrix().
> >> D1 <- as.data.frame(X) D2 <- data.frame(X) names(D1) [1] "a"   "b"
> >> "a:x" "b:x" names(D2) [1] "a"   "b"   "a.x" "b.x"
> >>
> >> The names of D2 are syntactically valid; those of D1 are not.
> >>
> >> Why should I have expected this phenomenon? :-)
> >>
> >> The as.data.frame() syntax seems to me much more natural for
> >> converting
> >>
> >> a matrix to a data frame, yet it doesn't get it quite right,
> >> sometimes, in respect of the names.
> >>
> >> Is there some reason that as.data.frame() does not apply
> >> make.names()? Or was this just an oversight?
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wdun|@p @end|ng |rom t|bco@com  Wed Feb  6 02:41:02 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Tue, 5 Feb 2019 17:41:02 -0800
Subject: [R] data.frame() versus as.data.frame() applied to a matrix.
In-Reply-To: <32f3ee5a-2683-58ad-1336-c3c9bc156668@auckland.ac.nz>
References: <32f3ee5a-2683-58ad-1336-c3c9bc156668@auckland.ac.nz>
Message-ID: <CAF8bMcYVhEYL99g4wyQzS4z-nBZmBdt+hvfuKCQ8vi+Bn5mDuA@mail.gmail.com>

I think of the methods of as.data.frame as a helper functions for
data.frame and don't usually call as.data.frame directly.  data.frame()
will call as.data.frame for each of its arguments and then put together the
the results into one big data.frame.

> for(method in
c("as.data.frame.list","as.data.frame.character","as.data.frame.integer","as.data.frame.numeric","as.data.frame.matrix"))
trace(method, quote(str(x)))
Tracing function "as.data.frame.list" in package "base"
Tracing function "as.data.frame.character" in package "base"
Tracing function "as.data.frame.integer" in package "base"
Tracing function "as.data.frame.numeric" in package "base"
Tracing function "as.data.frame.matrix" in package "base"
> d <-
data.frame(Mat=cbind(m1=11:12,M2=13:14),Num=c(15.5,16.6),Int=17:18,List=list(L1=19:20,L2=c(20.2,21.2)))
Tracing as.data.frame.matrix(x[[i]], optional = TRUE) on entry
 int [1:2, 1:2] 11 12 13 14
 - attr(*, "dimnames")=List of 2
  ..$ : NULL
  ..$ : chr [1:2] "m1" "M2"
Tracing as.data.frame.numeric(x[[i]], optional = TRUE) on entry
 num [1:2] 15.5 16.6
Tracing as.data.frame.integer(x[[i]], optional = TRUE) on entry
 int [1:2] 17 18
Tracing as.data.frame.list(x[[i]], optional = TRUE, stringsAsFactors =
stringsAsFactors) on entry
List of 2
 $ L1: int [1:2] 19 20
 $ L2: num [1:2] 20.2 21.2
Tracing as.data.frame.integer(x[[i]], optional = TRUE) on entry
 int [1:2] 19 20
Tracing as.data.frame.numeric(x[[i]], optional = TRUE) on entry
 num [1:2] 20.2 21.2

If I recall correctly, that is how S did things and Splus tried to use
something like as.data.frameAux for the name of the helper function to
avoid some of the frustration you describe.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Tue, Feb 5, 2019 at 2:22 PM Rolf Turner <r.turner at auckland.ac.nz> wrote:

>
> Consider the following:
>
> set.seed(42)
> X <- matrix(runif(40),10,4)
> colnames(X) <- c("a","b","a:x","b:x") # Imitating the output
>                                        # of model.matrix().
> D1 <- as.data.frame(X)
> D2 <- data.frame(X)
> names(D1)
> [1] "a"   "b"   "a:x" "b:x"
> names(D2)
> [1] "a"   "b"   "a.x" "b.x"
>
> The names of D2 are syntactically valid; those of D1 are not.
>
> Why should I have expected this phenomenon? :-)
>
> The as.data.frame() syntax seems to me much more natural for converting
> a matrix to a data frame, yet it doesn't get it quite right, sometimes,
> in respect of the names.
>
> Is there some reason that as.data.frame() does not apply make.names()?
> Or was this just an oversight?
>
> cheers,
>
> Rolf Turner
>
> --
> Honorary Research Fellow
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From |@nch@co|||n @end|ng |rom gm@||@com  Tue Feb  5 22:26:52 2019
From: |@nch@co|||n @end|ng |rom gm@||@com (Francois COLLIN)
Date: Tue, 5 Feb 2019 22:26:52 +0100
Subject: [R] problems when merging two data sets
In-Reply-To: <CAGxFJbR4mDMwf9MmhaJzbuAp2EpVkSxfnLt00sK5DqpSwGLCKQ@mail.gmail.com>
References: <CAJanvzFpsoYZb43k4FnQOy_tz_KMoL3Xv-egWUcne0s-yMywgA@mail.gmail.com>
 <CAGxFJbR4mDMwf9MmhaJzbuAp2EpVkSxfnLt00sK5DqpSwGLCKQ@mail.gmail.com>
Message-ID: <e4f57119-aedb-daf5-76c4-010c7d44fbc0@gmail.com>

Quite agree with Jeff Newmiller and Bert Gunter.

The error you get (" 'by' must specify a uniquely valid column") is a 
very common mistake when the function merge is misused. Although, the 
function merge is the good choice. Have you read the manual of the 
function sending the command `?merge`. That is always a good start.

Hereafter is what the function call look like:

`merge(x, y, by = intersect(names(x), names(y)), by.x = by, by.y = by, 
all = FALSE, all.x = all, all.y = all, sort = TRUE, suffixes = 
c(".x",".y"), no.dups = TRUE, incomparables = NULL, ...)`

For your matter, you probably need only 4 arguments:

`merge(x = dataset1, y = dataset2, by.x = "key1", by.y = "key2")`

In the example, key1 correspond to the column name in the dataset1 that 
should match the column name in the dataset2. Likewise for key2.

Again, read the manual to understand the other arguments, I would 
especially advise you to look at the arguments suffixes, all.x, all.y 
which will help you doing exactly what you want.

Cheers,

Francois COLLIN

On 05/02/2019 19:49, Bert Gunter wrote:
> Show us your code! (as the posting guide below requests. Please read the
> posting guide).
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Tue, Feb 5, 2019 at 10:04 AM sasa kosanic <sasa.kosanic at gmail.com> wrote:
>
>> Dear All,
>>
>> I would like to merge two data sets however I am doing something wrong...
>> 1 data set contains 2 columns of  'species occurrence'(1 column) in Germany
>> and  'species names' (2 column).
>> and the second one names of 'Red list species'(1 column) and 'species
>> status' (2 column).
>> so I would like to merge Red list species with species names from the first
>> table and to sign the  species status
>> I have tried with merge function but got this an error:" 'by' must specify
>> a uniquely valid column"
>> I also tried with the function left_join, however no success.
>>
>> Also columns in two data sets are different in size. 1 table has 7189 rows
>> and 2 table just 426 rows as we do not have much Red list Species.
>>
>> I would appreciate your help.
>>
>> Kind regards,
>> Sasha
>>
>>
>> Dr Sasha Kosanic
>> Ecology Lab (Biology Department)
>> Room M842
>> University of Konstanz
>> Universit?tsstra?e 10
>> D-78464 Konstanz
>> Phone: +49 7531 883321 & +49 (0)175 9172503
>>
>> http://cms.uni-konstanz.de/vkleunen/
>> https://tinyurl.com/y8u5wyoj
>> https://tinyurl.com/cgec6tu
>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@r||nkcox @end|ng |rom gm@||@com  Wed Feb  6 06:52:04 2019
From: m@r||nkcox @end|ng |rom gm@||@com (Marlin Keith Cox)
Date: Tue, 5 Feb 2019 20:52:04 -0900
Subject: [R] dates by week multiple years
Message-ID: <CAHskWAViyOeka60iKcLe8JsGW29so5TXuJ0RDpkObMDOWr_8ZQ@mail.gmail.com>

Hello all, I did not attach data as this is probably simple answer that I
cannot find.

I am collecting daily data and would like to plot the data by week but now
that we are into the new year, my plots started over with 01 after the last
week of the year.  How can I continuously keep adding weeks such as 53, 54,
55, 56, etc. instead of starting over?

Below has worked for me until we moved into 2019.

data$Date    <- as.Date(data$TIMESTAMP,format="%m/%d/%y  %H:%M")
data$Week1    <- strftime(data$Date,format = "%V")
data$Week<-as.factor(data$Week1)

Keith

M. Keith Cox, Ph.D.
Principal
MKConsulting
17415 Christine Ave.
Juneau, AK 99801
U.S. 907.957.4606

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Wed Feb  6 07:16:51 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 6 Feb 2019 17:16:51 +1100
Subject: [R] dates by week multiple years
In-Reply-To: <CAHskWAViyOeka60iKcLe8JsGW29so5TXuJ0RDpkObMDOWr_8ZQ@mail.gmail.com>
References: <CAHskWAViyOeka60iKcLe8JsGW29so5TXuJ0RDpkObMDOWr_8ZQ@mail.gmail.com>
Message-ID: <CA+8X3fXBuqB=0ovWf8Jx1X2o1N2c3Wx2a=b-UHHhfDdg8PAi+g@mail.gmail.com>

Hi Keith,
Perhaps you do not want to go with calendar weeks:

365/7 = 52.14

as there are not an even number of weeks in a year, you may want to
plot by week from your initial observation. As you are using as.Date,
you could simply calculate weeks as:

data$Week<-1+as.numeric(data$Date - data$Date[1])%/%7

Jim

On Wed, Feb 6, 2019 at 4:52 PM Marlin Keith Cox <marlinkcox at gmail.com> wrote:
>
> Hello all, I did not attach data as this is probably simple answer that I
> cannot find.
>
> I am collecting daily data and would like to plot the data by week but now
> that we are into the new year, my plots started over with 01 after the last
> week of the year.  How can I continuously keep adding weeks such as 53, 54,
> 55, 56, etc. instead of starting over?
>
> Below has worked for me until we moved into 2019.
>
> data$Date    <- as.Date(data$TIMESTAMP,format="%m/%d/%y  %H:%M")
> data$Week1    <- strftime(data$Date,format = "%V")
> data$Week<-as.factor(data$Week1)
>
> Keith
>
> M. Keith Cox, Ph.D.
> Principal
> MKConsulting
> 17415 Christine Ave.
> Juneau, AK 99801
> U.S. 907.957.4606
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m|korym @end|ng |rom protonm@||@com  Wed Feb  6 13:10:54 2019
From: m|korym @end|ng |rom protonm@||@com (Phillip-Jan van Zyl)
Date: Wed, 06 Feb 2019 12:10:54 +0000
Subject: [R] readxl::excel_sheets in tryCatch() doesn't catch error
Message-ID: <l8pOGf17vsxgXnl8uG6JNJchE8ysQavR8FqRJbRLx0w9OP782FNXtGdiQ36BEtRSqsAY8xKPLPqxjTwkBhxKyMALVWyE3uRviyx_RfK1Kys=@protonmail.com>

Hi R programmers

I am reading multiple .xls and .xlsx files from a directory using readxl from tidyverse. When reading fails, the code should continue on to the next file.

However, when I call the custom function readExcelSheets (in a loop and with the tryCatch function) I get an error for some files and the code then stops executing. How can I force my code to continue on to the next files?

Here is the function:

readExcelSheets <- function(curPath) {
  out <- tryCatch(
    {
      message("This is the 'try' part")
      dat <- excel_sheets(curPath)
    },
    error=function(cond) {
      message(paste("Error in opening Excel file with readxl read sheets:", curPath))
      message("Here's the original error message:")
      message(cond)
    },
    warning=function(cond) {
      message(paste("readxl caused a warning en reading sheets:", curPath))
      message("Here's the original warning message:")
      message(cond)
    },
    finally={
      message(paste("Processed file for sheets:", curPath))
      message("End of processing file for sheets.")
    }
  )
  return(out)
}

The loop looks like this:

listLength <- length(excelList)
for (excel_file in excelList) {
  curPath <- excel_file
  sheetNames <- NULL
  sheetNames <- withTimeout({readExcelSheets(curPath)}, timeout = 5, onTimeout="silent")
  if(is.null(sheetNames)){next}
  for (sheetName in sheetNames){
    # do something
  }
}

The problem is that I get an error:

Error: Evaluation error: zip file '<the path to the file>' cannot be opened.

And then execution of the loop stops without progressing to the next Excel file. Note that for the first n=+-20 files the code works as expected. I think that there may be an error in the full path name (such as a text encoding error), but my point is that it should exit silently and progress to the next Excel file even if the path is not found.

Best regards
Phillip
	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Feb  6 13:55:39 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 06 Feb 2019 04:55:39 -0800
Subject: [R] Help converting file to XTS
In-Reply-To: <DB6PR0501MB2710F13CB3CD58F2BC547A4E8E6F0@DB6PR0501MB2710.eurprd05.prod.outlook.com>
References: <DB6PR0501MB27104572451305156ED51D978E6E0@DB6PR0501MB2710.eurprd05.prod.outlook.com>,
 <9AF113E3-B3E0-4337-BC50-772281537133@dcn.davis.ca.us>
 <DB6PR0501MB2710F13CB3CD58F2BC547A4E8E6F0@DB6PR0501MB2710.eurprd05.prod.outlook.com>
Message-ID: <624EE409-E9E2-4751-98BD-692DE35969B1@dcn.davis.ca.us>

Please use "reply-all" to keep the mailing list included in the conversation.

You need to closely examine your format string ... read

?strptime

and adjust your format to match the data you have. In particular the %Y you are using is wrong, but only you can tell in what order the day, month and year should be.

On February 6, 2019 3:31:05 AM PST, "Johannes M?llerhagen" <johamol at stud.ntnu.no> wrote:
>Thank you guys! I think I'm getting it. The problem now is that when I
>import my data it is given as:
>
>print(dat)
>
>                         V1   V2
>1   02.01.03 10:00 26.8
>2   02.01.03 10:05 26.8
>3   02.01.03 10:10 26.9
>4   02.01.03 10:15 27.0
>5   02.01.03 10:20 26.9
>6   02.01.03 10:25 26.9
>7   02.01.03 10:30 26.9
>8   02.01.03 10:35 26.8
>9   02.01.03 10:40 26.7
>10  02.01.03 10:45 26.6
>11  02.01.03 10:50 26.6
>....
>
>
>However, when I call: as.POSIXct( dat[[ 1]], format="%d/%m/%Y %H:%M" )
>I get:
>
>[1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
>NA NA NA NA NA NA NA
>[30] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
>NA NA NA NA NA NA NA
>[59] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
>NA NA NA NA NA NA NA
>[88] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
>NA NA NA NA NA NA NA
>[117] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
>NA NA NA NA NA NA NA
>[146] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
>NA NA NA NA NA NA NA
>[175] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
>NA NA NA NA NA NA NA
>[204] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
>NA NA NA NA NA NA NA
>[233] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
>NA NA NA NA NA NA N
>
>
>It seems like the indexing is somehow wrong, or am I missing something?
>:)
>
>________________________________
>Fra: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>Sendt: tirsdag 5. februar 2019 19.39.14
>Til: r-help at r-project.org; Johannes M?llerhagen; r-help at r-project.org
>Emne: Re: [R] Help converting file to XTS
>
>One thing about POSIXct or POSIXlt... you always need to address the
>issue of timezone. If your timestamp data are simple (no daylight
>savings) you may be able to get away with a simple
>
>Sys.setenv( TZ="GMT" )
>
>sometime in each R session prior to converting anything to this type
>(e.g at the beginning of your script).
>
>You should read the help pages for read_csv
>
>?read_csv
>
>to find out that the object created by that function is a tibble (a
>variety of data frame). Then read
>
>?xts
>
>to find out that the order.by must be a time-based class (which
>requirement POSIXct meets), and then
>
>?as.POSIXct
>
>which sadly only says that x has to be an R object. There are a variety
>of specializations for this function, e.g. as.POSIXct.Date,
>as.POSIXct.numeric, as.POSIXct.POSIXlt, and as.POSIXct.default, the
>last of which handles conversion from character or factor data types.
>Note that none of these options include converting an entire data
>frame.
>
>Since dat is a tibble you will need to use $ or `[[` indexing to
>extract the one column that contains the timestamps:
>
>dat[[ 1 ]]
>
>and convert it
>
>as.POSIXct( dat[[ 1 ]], format="%d/%m/%Y %H:%M" )
>
>and use that as the order.by argument
>
>result <- xts( dat[-1] ,order.by= as.POSIXct( dat[[ 1 ]],
>format="%d/%m/%Y %H:%M" ) )
>
>[1]
>https://stackoverflow.com/questions/9327700/read-data-from-excel-to-r-and-convert-to-xts
>
>On February 5, 2019 6:06:15 AM PST, "Johannes M?llerhagen"
><johamol at stud.ntnu.no> wrote:
>>Hello there! I am a master student working on my master thesis, and I
>>am trying to convert some data to xts so I can apply a highfrequency
>>package to it.
>>
>>At the moment I am trying to use a POSIXct function. I am quite new at
>>this program and I am having some issue. The file is  attached.
>>
>>
>>The current coding is:
>>
>>
>>dat<-read_csv("TEL5minint.csv")
>>xts(dat,order.by=as.POSIXct(dat),"%d/%m/%Y %H:%M")
>>
>>
>>And the error is:
>>
>>Error in as.POSIXct.default(dat) :
>>  do not know how to convert 'dat' to class ?POSIXct?
>>
>>
>>Any help is appreciated!
>>
>>
>>Kind Regards
>>
>>Johannes
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
>--
>Sent from my phone. Please excuse my brevity.

-- 
Sent from my phone. Please excuse my brevity.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Feb  6 14:09:08 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 06 Feb 2019 05:09:08 -0800
Subject: [R] Calendar Heat Map
In-Reply-To: <003001d4bda2$1e8b8a40$5ba29ec0$@sbcglobal.net>
References: <003001d4bda2$1e8b8a40$5ba29ec0$@sbcglobal.net>
Message-ID: <9D1FAA52-1CBA-4B1C-9530-733135054D2A@dcn.davis.ca.us>

ggplot automatically chooses continuous or discrete scales depending on the type of column you give it... so don't give it a numeric column (integers are a subset of numeric)... give it a character (lazy) or factor (better for controlling what the output looks like) value for your colour specification.

Read

?cut

for one clean way to make a new factor column in your data set before you give it to ggplot. Note that if you don't like the labels it generates automatically you can specify them yourself.

On February 5, 2019 2:28:28 PM PST, reichmanj at sbcglobal.net wrote:
>r-Help Form
>
> 
>
>I'm working on a "Time-Series Calendar Heatmap" using the following
>code.
>
> 
>
>ggplot(myData, aes(monthweek, weekdayf, fill = myData $adjusted)) + 
>
>geom_tile(colour = "white") + facet_grid(year(myData $date)~monthf) + 
>
>scale_fill_gradient(low="red", high="green") + 
>
>xlab("Week of Month") + ylab("") + 
>
>ggtitle("Time-Series Calendar Heatmap ") + labs(fill = "Price")
>
> 
>
>While the ggplot commands do (almost) what I want I can't figure out
>how to
>change my color scaling. While scale_fill_gradient(low="red",
>high="green")
>does what I ask, that is create a color gradient from red to green it
>not
>what I thought it would be. What I need is discreet colors something
>like 0
>- grey; 1:5 - blue; 6:10 - green etc.  How to I set discrete colors for
>groups of values. A color ramp would work but I need to separately
>color
>those cells with 0 counts.
>
> 
>
>Jeff Reichman
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From |eroy @end|ng |rom |cmpe@cnr@@|r  Wed Feb  6 09:37:28 2019
From: |eroy @end|ng |rom |cmpe@cnr@@|r (Eric Leroy)
Date: Wed, 6 Feb 2019 09:37:28 +0100
Subject: [R] Nearest neighbors of a of 3D points
Message-ID: <5f0a040b-eabf-bc78-6dad-d15af4dabcc4@icmpe.cnrs.fr>

Hi,

I have a text file that contains the 3D coordinates of points and I want 
to plot the histogram of the nearest neighbors distance. I can import 
the xyz coordinates in R and each value x, y, z is stored in a numerical 
array. I discovered the nndist.pp3 function from the spatstat package 
that seems to do what I want. My problem is to create a pp3 object from 
the xyz values that I have.

Do you know how to do that ?

Best regards,

-- 

*Eric Leroy*

/Responsable de la plateforme microscopie ?lectronique/

ICMPE - UMR 7182 - CNRS - UPEC

2/8, rue Henri Dunant

94320 Thiais

T?l : 01.49.78.12.09

Fax : 01.49.78.12.03

courriel : eric.leroy at icmpe.cnrs.fr <mailto:eric.leroy at icmpe.cnrs.fr>

Page Web : : http://www.icmpe.cnrs.fr


	[[alternative HTML version deleted]]


From @@|@hm@@d@wy @end|ng |rom gm@||@com  Wed Feb  6 13:03:04 2019
From: @@|@hm@@d@wy @end|ng |rom gm@||@com (salah maadawy)
Date: Wed, 6 Feb 2019 15:03:04 +0300
Subject: [R] very slow code execution
Message-ID: <CANcGnwxCDyRKRRmX8+XN6XygOYasvFBLZ+JCdedGjZGMOAcQ_Q@mail.gmail.com>

i am a beginner regarding R but i am trying to do a simple thing, but it is
taking too much time and i am asking if there is any way to achieve what i
need, i have a time series data set with 730 data points, i detected 7, 354
and 365 seasonality periods. i am trying to use Fourier terms for
seasonality and for loop to get the K value for each while minimizing AICc,
my code is

    AICc<- data.table(matrix(nrow = 96642, ncol = 4))for (i in 1:3) {
  for (j in 1:177) {
    for (k in 182) {                     #i,j and k values are choosen
with regad that K cannot exceed seasonality period/2
      z1 <- fourier(ts(demand,frequency = 7), K=i)
      z2 <- fourier(ts(demand,frequency=354), K=j)
      z3 <- fourier(ts(demand,frequency = 365),K=k)
      fit <- auto.arima(demand, xreg =cbind(z1,z2,z3),
         seasonal = FALSE)
      fit$aicc
      AICc[,1]<-i
      AICc[,2]<-j
      AICc[,3]<-k
      AICc[,4]<-fit$aicc
    }

  }
}
  AICc

i have created a data table to store AICc values from all possible i,j,k
combinations so that i can find later the minimum AICc value. the problem
now is that it is taking forever to do so not only to iterate all
combinations but also due to the large K values.

, is there any possible solution for this? thank you in advance

	[[alternative HTML version deleted]]


From ||@t@ @end|ng |rom dewey@myzen@co@uk  Wed Feb  6 14:59:53 2019
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Wed, 6 Feb 2019 13:59:53 +0000
Subject: [R] very slow code execution
In-Reply-To: <CANcGnwxCDyRKRRmX8+XN6XygOYasvFBLZ+JCdedGjZGMOAcQ_Q@mail.gmail.com>
References: <CANcGnwxCDyRKRRmX8+XN6XygOYasvFBLZ+JCdedGjZGMOAcQ_Q@mail.gmail.com>
Message-ID: <d3530209-f284-90f5-3c2a-d17fcff0883d@dewey.myzen.co.uk>

This is not an answer to your speed problem but are your assignments to 
AICc[,1] and so on doing what you hope they are doing?

Michael

On 06/02/2019 12:03, salah maadawy wrote:
> i am a beginner regarding R but i am trying to do a simple thing, but it is
> taking too much time and i am asking if there is any way to achieve what i
> need, i have a time series data set with 730 data points, i detected 7, 354
> and 365 seasonality periods. i am trying to use Fourier terms for
> seasonality and for loop to get the K value for each while minimizing AICc,
> my code is
> 
>      AICc<- data.table(matrix(nrow = 96642, ncol = 4))for (i in 1:3) {
>    for (j in 1:177) {
>      for (k in 182) {                     #i,j and k values are choosen
> with regad that K cannot exceed seasonality period/2
>        z1 <- fourier(ts(demand,frequency = 7), K=i)
>        z2 <- fourier(ts(demand,frequency=354), K=j)
>        z3 <- fourier(ts(demand,frequency = 365),K=k)
>        fit <- auto.arima(demand, xreg =cbind(z1,z2,z3),
>           seasonal = FALSE)
>        fit$aicc
>        AICc[,1]<-i
>        AICc[,2]<-j
>        AICc[,3]<-k
>        AICc[,4]<-fit$aicc
>      }
> 
>    }
> }
>    AICc
> 
> i have created a data table to store AICc values from all possible i,j,k
> combinations so that i can find later the minimum AICc value. the problem
> now is that it is taking forever to do so not only to iterate all
> combinations but also due to the large K values.
> 
> , is there any possible solution for this? thank you in advance
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From dc@r|@on @end|ng |rom t@mu@edu  Wed Feb  6 17:06:48 2019
From: dc@r|@on @end|ng |rom t@mu@edu (David L Carlson)
Date: Wed, 6 Feb 2019 16:06:48 +0000
Subject: [R] Nearest neighbors of a of 3D points
In-Reply-To: <5f0a040b-eabf-bc78-6dad-d15af4dabcc4@icmpe.cnrs.fr>
References: <5f0a040b-eabf-bc78-6dad-d15af4dabcc4@icmpe.cnrs.fr>
Message-ID: <33de316ce9104bd8bea21637335ea92c@tamu.edu>

What have you tried so far? Have you installed the spatstat package and read the manual page for pp3 objects? The website spatstat.org has additional support including a quick reference guide. There are always multiple ways to do something in R, but without more details it is hard to be specific. Nearest neighbor distances are also provided in several other packages, e.g. packages FNN, distances, and RANN.

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Eric Leroy
Sent: Wednesday, February 6, 2019 2:37 AM
To: r-help at r-project.org
Subject: [R] Nearest neighbors of a of 3D points

Hi,

I have a text file that contains the 3D coordinates of points and I want 
to plot the histogram of the nearest neighbors distance. I can import 
the xyz coordinates in R and each value x, y, z is stored in a numerical 
array. I discovered the nndist.pp3 function from the spatstat package 
that seems to do what I want. My problem is to create a pp3 object from 
the xyz values that I have.

Do you know how to do that ?

Best regards,

-- 

*Eric Leroy*

/Responsable de la plateforme microscopie ?lectronique/

ICMPE - UMR 7182 - CNRS - UPEC

2/8, rue Henri Dunant

94320 Thiais

T?l : 01.49.78.12.09

Fax : 01.49.78.12.03

courriel : eric.leroy at icmpe.cnrs.fr <mailto:eric.leroy at icmpe.cnrs.fr>

Page Web : : http://www.icmpe.cnrs.fr


	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Wed Feb  6 17:36:36 2019
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Wed, 6 Feb 2019 08:36:36 -0800 (PST)
Subject: [R] CRAN Task View: Optimization and Mathematical Programming
Message-ID: <alpine.LNX.2.20.1902060834530.22634@salmo.appl-ecosys.com>

The task view lists many R packages for optimization and I would appreciate
suggestions from those familiar with the available tools for packages that
might be suitable for water management.

Limited water resources need to be apportioned among various competing users
(e.g,, agriculture, fish and wildlife, Tribes, potable human water
supplies). I'm looking for R packages that can be applied to this type of
constrained optimization problem.

Pointers to other resources also welcome.

Thanks in advance,

Rich


From S@E|||@on @end|ng |rom LGCGroup@com  Wed Feb  6 17:48:15 2019
From: S@E|||@on @end|ng |rom LGCGroup@com (S Ellison)
Date: Wed, 6 Feb 2019 16:48:15 +0000
Subject: [R] CRAN Task View: Optimization and Mathematical Programming
In-Reply-To: <alpine.LNX.2.20.1902060834530.22634@salmo.appl-ecosys.com>
References: <alpine.LNX.2.20.1902060834530.22634@salmo.appl-ecosys.com>
Message-ID: <a38a78e472d8419ab3b9e9b61b1f5f50@GBDCVPEXC08.corp.lgc-group.com>

> Limited water resources need to be apportioned among various competing
> users
> (e.g,, agriculture, fish and wildlife, Tribes, potable human water
> supplies). 
Water management is definitely not my field, but for interest - and maybe to help other folk respond - can I ask what the loss function would look like for this kind of problem? That may suggest particular optimisation approaches.

Steve Ellison



*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Feb  6 19:02:10 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 6 Feb 2019 10:02:10 -0800 (PST)
Subject: [R] CRAN Task View: Optimization and Mathematical Programming
In-Reply-To: <a38a78e472d8419ab3b9e9b61b1f5f50@GBDCVPEXC08.corp.lgc-group.com>
References: <alpine.LNX.2.20.1902060834530.22634@salmo.appl-ecosys.com>
 <a38a78e472d8419ab3b9e9b61b1f5f50@GBDCVPEXC08.corp.lgc-group.com>
Message-ID: <alpine.BSF.2.00.1902060918200.89303@pedal.dcn.davis.ca.us>

Rich's email did not reach me, so tagging off of S Ellison's reply...

Take a look at [1] for an example analysis that maximizes the firm yield 
(amount that can be promised and subsequently delivered for irrigation 
delivery each year) subject to required minimum capacity levels (for flood 
protection) and available water input from precipitation. (Yes, it assumes 
an omniscient knowledge of water flows, but estimating future firm yield 
based on historical patterns is standard practice.) This is a classic type 
of problem in civil engineering graduate school... it should not take much 
work to find lots of literature on the topic of water management that 
mentions various optimization techniques that you can then look up via 
such resources as the subject CRAN Task View.

And no, I am not a water specialist either... this was a targeted set 
of materials for illustrating the use of R to an audience of civil 
engineers who were already familiar with these specific problems.

[1] https://github.com/jdnewmil/eci298sp2016 KernClimate.html

On Wed, 6 Feb 2019, S Ellison wrote:

>> Limited water resources need to be apportioned among various competing
>> users
>> (e.g,, agriculture, fish and wildlife, Tribes, potable human water
>> supplies).
> Water management is definitely not my field, but for interest - and 
> maybe to help other folk respond - can I ask what the loss function 
> would look like for this kind of problem? That may suggest particular 
> optimisation approaches.
>
> Steve Ellison
>
> *******************************************************************
> This email and any attachments are confidential. Any u...{{dropped:17}}


From dech@nt @end|ng |rom d@|@c@  Wed Feb  6 18:07:35 2019
From: dech@nt @end|ng |rom d@|@c@ (Oliver Dechant)
Date: Wed, 6 Feb 2019 17:07:35 +0000
Subject: [R] CRAN Task View: Optimization and Mathematical Programming
In-Reply-To: <alpine.LNX.2.20.1902060834530.22634@salmo.appl-ecosys.com>
References: <alpine.LNX.2.20.1902060834530.22634@salmo.appl-ecosys.com>
Message-ID: <4e74ca8c-5d19-15bc-af7e-9af6bc63ce06@dal.ca>

On 2019-02-06 12:36 p.m., Rich Shepard wrote:
> The task view lists many R packages for optimization and I would appreciate
> suggestions from those familiar with the available tools for packages that
> might be suitable for water management.
> 
> Limited water resources need to be apportioned among various competing
> users
> (e.g,, agriculture, fish and wildlife, Tribes, potable human water
> supplies). I'm looking for R packages that can be applied to this type of
> constrained optimization problem.
> 
> Pointers to other resources also welcome.
> 
> Thanks in advance,
> 
> Rich
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
Hello,

For other resources, I was recently looking at MDP research and several
packages. I found one which originates from ecological modeling
specifically so I thought to share it with you.

[1] https://cran.r-project.org/web/packages/MDPtoolbox/index.html

Other than reading the papers I have no practical experience applying
that package.

Hope that helps.
-- 
Oliver Dechant

From te@3rd @end|ng |rom gm@||@com  Wed Feb  6 19:48:53 2019
From: te@3rd @end|ng |rom gm@||@com (Thomas Adams)
Date: Wed, 6 Feb 2019 13:48:53 -0500
Subject: [R] CRAN Task View: Optimization and Mathematical Programming
In-Reply-To: <alpine.LNX.2.20.1902060834530.22634@salmo.appl-ecosys.com>
References: <alpine.LNX.2.20.1902060834530.22634@salmo.appl-ecosys.com>
Message-ID: <CAGxgkWhQbaLCYeN5RhK9zzs3LChur0fnxKVhfznfYsdkgwSC4A@mail.gmail.com>

Did you Google "R stat water resources optimization" or "r stats
optimization"?

It seems that applying some of the references below with R packages in the
optimization task view probably gets you where you want to go.

https://cran.r-project.org/web/packages/reservoir/reservoir.pdf
https://www.researchgate.net/publication/317213418_Modern_Optimization_Methods_in_Water_Resources_Planning_Engineering_and_Management
https://link.springer.com/article/10.1007/s10666-018-9628-0
https://math.nyu.edu/faculty/tabak/publications/WaterResources.pdf

https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=6&ved=2ahUKEwjHwLz34KfgAhVvg-AKHVy_DRcQFjAFegQICRAC&url=https%3A%2F%2Fwww.jstatsoft.org%2Farticle%2Fview%2Fv060i01%2Fv60i01.pdf&usg=AOvVaw31QSrEIhxLDZrbHyMVcAds

https://www.jstatsoft.org/article/view/v060i02

http://abouthydrology.blogspot.com/2012/08/r-resources-for-hydrologists.html

https://www.sciencedirect.com/science/article/pii/S0309170896000619
https://acwi.gov/monitoring/conference/2016/2_wednesday_may4/F4/NMC2016SessionF4RybergEtAl_secure.pdf
https://www.epa.gov/sites/production/files/2015-06/documents/twri4a3-new.pdf

Tom


On Wed, Feb 6, 2019 at 11:36 AM Rich Shepard <rshepard at appl-ecosys.com>
wrote:

> The task view lists many R packages for optimization and I would appreciate
> suggestions from those familiar with the available tools for packages that
> might be suitable for water management.
>
> Limited water resources need to be apportioned among various competing
> users
> (e.g,, agriculture, fish and wildlife, Tribes, potable human water
> supplies). I'm looking for R packages that can be applied to this type of
> constrained optimization problem.
>
> Pointers to other resources also welcome.
>
> Thanks in advance,
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Wed Feb  6 20:40:04 2019
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Wed, 6 Feb 2019 11:40:04 -0800 (PST)
Subject: [R] CRAN Task View: Optimization and Mathematical Programming
In-Reply-To: <CAGxgkWhQbaLCYeN5RhK9zzs3LChur0fnxKVhfznfYsdkgwSC4A@mail.gmail.com>
References: <alpine.LNX.2.20.1902060834530.22634@salmo.appl-ecosys.com>
 <CAGxgkWhQbaLCYeN5RhK9zzs3LChur0fnxKVhfznfYsdkgwSC4A@mail.gmail.com>
Message-ID: <alpine.LNX.2.20.1902061138580.22634@salmo.appl-ecosys.com>

On Wed, 6 Feb 2019, Thomas Adams wrote:

> Did you Google "R stat water resources optimization" or "r stats
> optimization"?

Tom,

No. I went directly to the CRAN task views.

Thanks very much for the pointers.

Regards,

Rich


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Wed Feb  6 21:29:13 2019
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Wed, 6 Feb 2019 12:29:13 -0800 (PST)
Subject: [R] CRAN Task View: Optimization and Mathematical Programming
In-Reply-To: <a38a78e472d8419ab3b9e9b61b1f5f50@GBDCVPEXC08.corp.lgc-group.com>
References: <alpine.LNX.2.20.1902060834530.22634@salmo.appl-ecosys.com>
 <a38a78e472d8419ab3b9e9b61b1f5f50@GBDCVPEXC08.corp.lgc-group.com>
Message-ID: <alpine.LNX.2.20.1902061228330.22634@salmo.appl-ecosys.com>

On Wed, 6 Feb 2019, S Ellison wrote:

> Water management is definitely not my field, but for interest - and maybe
> to help other folk respond - can I ask what the loss function would look
> like for this kind of problem? That may suggest particular optimisation
> approaches.

Steve,

I'll go learn more about loss functions.

Thanks for the suggestion,

Rich


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Wed Feb  6 21:30:17 2019
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Wed, 6 Feb 2019 12:30:17 -0800 (PST)
Subject: [R] CRAN Task View: Optimization and Mathematical Programming
In-Reply-To: <alpine.BSF.2.00.1902060918200.89303@pedal.dcn.davis.ca.us>
References: <alpine.LNX.2.20.1902060834530.22634@salmo.appl-ecosys.com>
 <a38a78e472d8419ab3b9e9b61b1f5f50@GBDCVPEXC08.corp.lgc-group.com>
 <alpine.BSF.2.00.1902060918200.89303@pedal.dcn.davis.ca.us>
Message-ID: <alpine.LNX.2.20.1902061229190.22634@salmo.appl-ecosys.com>

On Wed, 6 Feb 2019, Jeff Newmiller wrote:

> Take a look at [1] for an example analysis that maximizes the firm yield 
> (amount that can be promised and subsequently delivered for irrigation 
> delivery each year) subject to required minimum capacity levels (for flood 
> protection) and available water input from precipitation.

Jeff,

I'll definitely look at the reference. Thanks for the URL.

Best regards,

Rich


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Feb  6 22:03:20 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 6 Feb 2019 13:03:20 -0800 (PST)
Subject: [R] very slow code execution
In-Reply-To: <CANcGnwxCDyRKRRmX8+XN6XygOYasvFBLZ+JCdedGjZGMOAcQ_Q@mail.gmail.com>
References: <CANcGnwxCDyRKRRmX8+XN6XygOYasvFBLZ+JCdedGjZGMOAcQ_Q@mail.gmail.com>
Message-ID: <alpine.BSF.2.00.1902061229130.96142@pedal.dcn.davis.ca.us>

This seems like an odd analysis to me, but I don't have time to look 
closer at it. It certainly doesn't look simple to me... I hope you have 
some good theoretical guidance in tackling this (which this mailing list 
is not).

One obvious thing is that z1 only depends on i, so you should only have to 
compute z1 3 times... but you are computing it 177*182*3 times. Make three 
lists of fourier results using three separate preparation loops before 
diving into your triple loop. Then just retrieve the relevant z1, z2 and 
z3 from these lists when you run the auto.arima calculation.

The easy way to make lists is to use lapply rather than a for loop:

demand7 <- ts( demand, frequency = 7 ) # don't repeat the ts conversion
z1List <- lapply( 1:3, function(i) { fourier( demand7, K = i ) } )

then you can retrieve whichever z1 you want with

z1 <- z1List[[ i ]]

rather than recalculating it.

Also, data.table is implemented by a contributed package. It is a special 
type of data frame. It is not at all clear that you need its special 
properties here, but most people who create a matrix and then immediately 
converts the matrix to a (type of) data frame aren't using the data frame 
properly anyway. Here is one way to assemble your results:

AICc <- expand.grid( k = 1:181, j = 1:177, i = 1:3 )
AICc$fit <- NA
idx <- 1
for ( i in 1:3 ) {
   for ( j in 1:177 ) {
     for ( k in 1:181 ) {
       # compute fit
       AICc$fit[ idx ] <- fit$aicc
       idx <- idx + 1
     }
   }
}

On Wed, 6 Feb 2019, salah maadawy wrote:

> i am a beginner regarding R but i am trying to do a simple thing, but it is
> taking too much time and i am asking if there is any way to achieve what i
> need, i have a time series data set with 730 data points, i detected 7, 354
> and 365 seasonality periods. i am trying to use Fourier terms for
> seasonality and for loop to get the K value for each while minimizing AICc,
> my code is
>
>    AICc<- data.table(matrix(nrow = 96642, ncol = 4))for (i in 1:3) {
>  for (j in 1:177) {
>    for (k in 182) {                     #i,j and k values are choosen
> with regad that K cannot exceed seasonality period/2
>      z1 <- fourier(ts(demand,frequency = 7), K=i)
>      z2 <- fourier(ts(demand,frequency=354), K=j)
>      z3 <- fourier(ts(demand,frequency = 365),K=k)
>      fit <- auto.arima(demand, xreg =cbind(z1,z2,z3),
>         seasonal = FALSE)
>      fit$aicc
>      AICc[,1]<-i
>      AICc[,2]<-j
>      AICc[,3]<-k
>      AICc[,4]<-fit$aicc
>    }
>
>  }
> }
>  AICc
>
> i have created a data table to store AICc values from all possible i,j,k
> combinations so that i can find later the minimum AICc value. the problem
> now is that it is taking forever to do so not only to iterate all
> combinations but also due to the large K values.
>
> , is there any possible solution for this? thank you in advance
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From h@rry@ho|t07 @end|ng |rom gm@||@com  Wed Feb  6 17:20:54 2019
From: h@rry@ho|t07 @end|ng |rom gm@||@com (Harry Holt)
Date: Wed, 6 Feb 2019 17:20:54 +0100
Subject: [R] Daily Prices, need VaR for longer periods
Message-ID: <022201d4be37$f043f050$d0cbd0f0$@gmail.com>

I am using performance analytics and Quantmod packages.  The data is daily
stock returns, I am calculating VaR (Port.returns, p=0.95, weights =
weights, portfolio_method = "Component", method="modified").  This gives me
the Cornish Fisher VaR - daily risk.  Is there a way still using daily
prices and get longer risk periods (VaR), such as one month, or quarter?
Thanks

 


	[[alternative HTML version deleted]]


From reichm@@j m@iii@g oii sbcgiob@i@@et  Wed Feb  6 22:40:38 2019
From: reichm@@j m@iii@g oii sbcgiob@i@@et (reichm@@j m@iii@g oii sbcgiob@i@@et)
Date: Wed, 6 Feb 2019 15:40:38 -0600
Subject: [R] Calendar Heat Map
In-Reply-To: <9D1FAA52-1CBA-4B1C-9530-733135054D2A@dcn.davis.ca.us>
References: <003001d4bda2$1e8b8a40$5ba29ec0$@sbcglobal.net>
 <9D1FAA52-1CBA-4B1C-9530-733135054D2A@dcn.davis.ca.us>
Message-ID: <002201d4be64$9a4e5960$ceeb0c20$@sbcglobal.net>

Jeff

Thanks - that?s easy enough

Jeff

-----Original Message-----
From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us> 
Sent: Wednesday, February 6, 2019 7:09 AM
To: r-help at r-project.org; reichmanj at sbcglobal.net
Subject: Re: [R] Calendar Heat Map

ggplot automatically chooses continuous or discrete scales depending on the type of column you give it... so don't give it a numeric column (integers are a subset of numeric)... give it a character (lazy) or factor (better for controlling what the output looks like) value for your colour specification.

Read

?cut

for one clean way to make a new factor column in your data set before you give it to ggplot. Note that if you don't like the labels it generates automatically you can specify them yourself.

On February 5, 2019 2:28:28 PM PST, reichmanj at sbcglobal.net wrote:
>r-Help Form
>
> 
>
>I'm working on a "Time-Series Calendar Heatmap" using the following 
>code.
>
> 
>
>ggplot(myData, aes(monthweek, weekdayf, fill = myData $adjusted)) +
>
>geom_tile(colour = "white") + facet_grid(year(myData $date)~monthf) +
>
>scale_fill_gradient(low="red", high="green") +
>
>xlab("Week of Month") + ylab("") +
>
>ggtitle("Time-Series Calendar Heatmap ") + labs(fill = "Price")
>
> 
>
>While the ggplot commands do (almost) what I want I can't figure out 
>how to change my color scaling. While scale_fill_gradient(low="red",
>high="green")
>does what I ask, that is create a color gradient from red to green it 
>not what I thought it would be. What I need is discreet colors 
>something like 0
>- grey; 1:5 - blue; 6:10 - green etc.  How to I set discrete colors for 
>groups of values. A color ramp would work but I need to separately 
>color those cells with 0 counts.
>
> 
>
>Jeff Reichman
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.


From ||@t@ @end|ng |rom dewey@myzen@co@uk  Thu Feb  7 10:38:30 2019
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Thu, 7 Feb 2019 09:38:30 +0000
Subject: [R] very slow code execution
In-Reply-To: <CANcGnwzfyhT7V2F_6X4Yo7EKS868UGZbDvSsMvxTkxOKNZ6YJQ@mail.gmail.com>
References: <CANcGnwxCDyRKRRmX8+XN6XygOYasvFBLZ+JCdedGjZGMOAcQ_Q@mail.gmail.com>
 <d3530209-f284-90f5-3c2a-d17fcff0883d@dewey.myzen.co.uk>
 <CANcGnwzfyhT7V2F_6X4Yo7EKS868UGZbDvSsMvxTkxOKNZ6YJQ@mail.gmail.com>
Message-ID: <7975b7dd-8256-81a4-9e77-9f5b3ded9d53@dewey.myzen.co.uk>

Well I do not know about data.table but in standard R if you go
AICc[,1] <- 3
it fills the whole column with 3 so you will end up with a table with 
the last value of AICc stored in every row which is almost certainly not 
what you want.

Michael

On 06/02/2019 14:15, salah maadawy wrote:
> Hi Micheal, Maybe there is a simple way but i wanted to get the lowest 
> aicc ana i could not find a way to do so, that's why i created the 
>  ?table to store all possible outcomes and then i can easily get the 
> minimum value and the values of (i,j and k) used for that minimum value. 
> The first column in the table is AICc[,1] to store i and second column 
> for j and so on. Maybe i am mistaken and this won't give me what i want, 
> the code been running for 5 hours now. So i am waiting
> 
> On Wed, Feb 6, 2019 at 4:59 PM Michael Dewey <lists at dewey.myzen.co.uk 
> <mailto:lists at dewey.myzen.co.uk>> wrote:
> 
>     This is not an answer to your speed problem but are your assignments to
>     AICc[,1] and so on doing what you hope they are doing?
> 
>     Michael
> 
>     On 06/02/2019 12:03, salah maadawy wrote:
>      > i am a beginner regarding R but i am trying to do a simple thing,
>     but it is
>      > taking too much time and i am asking if there is any way to
>     achieve what i
>      > need, i have a time series data set with 730 data points, i
>     detected 7, 354
>      > and 365 seasonality periods. i am trying to use Fourier terms for
>      > seasonality and for loop to get the K value for each while
>     minimizing AICc,
>      > my code is
>      >
>      >? ? ? AICc<- data.table(matrix(nrow = 96642, ncol = 4))for (i in
>     1:3) {
>      >? ? for (j in 1:177) {
>      >? ? ? for (k in 182) {? ? ? ? ? ? ? ? ? ? ?#i,j and k values are
>     choosen
>      > with regad that K cannot exceed seasonality period/2
>      >? ? ? ? z1 <- fourier(ts(demand,frequency = 7), K=i)
>      >? ? ? ? z2 <- fourier(ts(demand,frequency=354), K=j)
>      >? ? ? ? z3 <- fourier(ts(demand,frequency = 365),K=k)
>      >? ? ? ? fit <- auto.arima(demand, xreg =cbind(z1,z2,z3),
>      >? ? ? ? ? ?seasonal = FALSE)
>      >? ? ? ? fit$aicc
>      >? ? ? ? AICc[,1]<-i
>      >? ? ? ? AICc[,2]<-j
>      >? ? ? ? AICc[,3]<-k
>      >? ? ? ? AICc[,4]<-fit$aicc
>      >? ? ? }
>      >
>      >? ? }
>      > }
>      >? ? AICc
>      >
>      > i have created a data table to store AICc values from all
>     possible i,j,k
>      > combinations so that i can find later the minimum AICc value. the
>     problem
>      > now is that it is taking forever to do so not only to iterate all
>      > combinations but also due to the large K values.
>      >
>      > , is there any possible solution for this? thank you in advance
>      >
>      >? ? ? ?[[alternative HTML version deleted]]
>      >
>      > ______________________________________________
>      > R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>     -- To UNSUBSCRIBE and more, see
>      > https://stat.ethz.ch/mailman/listinfo/r-help
>      > PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>      > and provide commented, minimal, self-contained, reproducible code.
>      >
> 
>     -- 
>     Michael
>     http://www.dewey.myzen.co.uk/home.html
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From @@|@hm@@d@wy @end|ng |rom gm@||@com  Wed Feb  6 15:15:59 2019
From: @@|@hm@@d@wy @end|ng |rom gm@||@com (salah maadawy)
Date: Wed, 6 Feb 2019 17:15:59 +0300
Subject: [R] very slow code execution
In-Reply-To: <d3530209-f284-90f5-3c2a-d17fcff0883d@dewey.myzen.co.uk>
References: <CANcGnwxCDyRKRRmX8+XN6XygOYasvFBLZ+JCdedGjZGMOAcQ_Q@mail.gmail.com>
 <d3530209-f284-90f5-3c2a-d17fcff0883d@dewey.myzen.co.uk>
Message-ID: <CANcGnwzfyhT7V2F_6X4Yo7EKS868UGZbDvSsMvxTkxOKNZ6YJQ@mail.gmail.com>

Hi Micheal, Maybe there is a simple way but i wanted to get the lowest aicc
ana i could not find a way to do so, that's why i created the  table to
store all possible outcomes and then i can easily get the minimum value and
the values of (i,j and k) used for that minimum value. The first column in
the table is AICc[,1] to store i and second column for j and so on. Maybe i
am mistaken and this won't give me what i want, the code been running for 5
hours now. So i am waiting

On Wed, Feb 6, 2019 at 4:59 PM Michael Dewey <lists at dewey.myzen.co.uk>
wrote:

> This is not an answer to your speed problem but are your assignments to
> AICc[,1] and so on doing what you hope they are doing?
>
> Michael
>
> On 06/02/2019 12:03, salah maadawy wrote:
> > i am a beginner regarding R but i am trying to do a simple thing, but it
> is
> > taking too much time and i am asking if there is any way to achieve what
> i
> > need, i have a time series data set with 730 data points, i detected 7,
> 354
> > and 365 seasonality periods. i am trying to use Fourier terms for
> > seasonality and for loop to get the K value for each while minimizing
> AICc,
> > my code is
> >
> >      AICc<- data.table(matrix(nrow = 96642, ncol = 4))for (i in 1:3) {
> >    for (j in 1:177) {
> >      for (k in 182) {                     #i,j and k values are choosen
> > with regad that K cannot exceed seasonality period/2
> >        z1 <- fourier(ts(demand,frequency = 7), K=i)
> >        z2 <- fourier(ts(demand,frequency=354), K=j)
> >        z3 <- fourier(ts(demand,frequency = 365),K=k)
> >        fit <- auto.arima(demand, xreg =cbind(z1,z2,z3),
> >           seasonal = FALSE)
> >        fit$aicc
> >        AICc[,1]<-i
> >        AICc[,2]<-j
> >        AICc[,3]<-k
> >        AICc[,4]<-fit$aicc
> >      }
> >
> >    }
> > }
> >    AICc
> >
> > i have created a data table to store AICc values from all possible i,j,k
> > combinations so that i can find later the minimum AICc value. the problem
> > now is that it is taking forever to do so not only to iterate all
> > combinations but also due to the large K values.
> >
> > , is there any possible solution for this? thank you in advance
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html
>

	[[alternative HTML version deleted]]


From |eroy @end|ng |rom |cmpe@cnr@@|r  Wed Feb  6 17:55:37 2019
From: |eroy @end|ng |rom |cmpe@cnr@@|r (Eric Leroy)
Date: Wed, 6 Feb 2019 17:55:37 +0100
Subject: [R] Nearest neighbors of a of 3D points
In-Reply-To: <33de316ce9104bd8bea21637335ea92c@tamu.edu>
References: <5f0a040b-eabf-bc78-6dad-d15af4dabcc4@icmpe.cnrs.fr>
 <33de316ce9104bd8bea21637335ea92c@tamu.edu>
Message-ID: <8cd66d15-20ef-06a5-9486-8e72ef1573d4@icmpe.cnrs.fr>

I tried the spatstat package. In the meanwhile, I think that I found the 
solution: with the pp3 command, I was able to create a pp3 object that 
is recognized by nndist.

I will give a look at the other packages you mentioned seeing what are 
the differences.

Anyway, thank you for your answer.

*Eric Leroy*

/Responsable de la plateforme microscopie ?lectronique/

ICMPE - UMR 7182 - CNRS - UPEC

2/8, rue Henri Dunant

94320 Thiais

T?l : 01.49.78.12.09

Fax : 01.49.78.12.03

courriel : eric.leroy at icmpe.cnrs.fr <mailto:eric.leroy at icmpe.cnrs.fr>

Page Web : : http://www.icmpe.cnrs.fr

Le 06/02/2019 ? 17:06, David L Carlson a ?crit?:
> What have you tried so far? Have you installed the spatstat package and read the manual page for pp3 objects? The website spatstat.org has additional support including a quick reference guide. There are always multiple ways to do something in R, but without more details it is hard to be specific. Nearest neighbor distances are also provided in several other packages, e.g. packages FNN, distances, and RANN.
>
> ----------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77843-4352
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Eric Leroy
> Sent: Wednesday, February 6, 2019 2:37 AM
> To: r-help at r-project.org
> Subject: [R] Nearest neighbors of a of 3D points
>
> Hi,
>
> I have a text file that contains the 3D coordinates of points and I want
> to plot the histogram of the nearest neighbors distance. I can import
> the xyz coordinates in R and each value x, y, z is stored in a numerical
> array. I discovered the nndist.pp3 function from the spatstat package
> that seems to do what I want. My problem is to create a pp3 object from
> the xyz values that I have.
>
> Do you know how to do that ?
>
> Best regards,
>

From p@@c@|@n|k|@u@ @end|ng |rom |eu@uzh@ch  Wed Feb  6 19:48:49 2019
From: p@@c@|@n|k|@u@ @end|ng |rom |eu@uzh@ch (Pascal A. Niklaus)
Date: Wed, 6 Feb 2019 19:48:49 +0100
Subject: [R] Spacing around \cdot (%.%) unequal in pdf
Message-ID: <e4a77efe-24cd-e369-f74a-71aa424202e4@ieu.uzh.ch>

I am struggling labeling an axis in a plot.

Here is a minimal example illustrating the point:


pdf("mve.pdf",width=5,height=5)
ypos <- c(1:3) * 1e6
ylabs <- sapply(ypos/1e6, function(i) as.expression(bquote(.(i)%.%10^6)))
plot(rep(1,3), ypos, yaxt="n", ylab="")
axis(2, at=ypos, labels=ylabs, las=1)
dev.off()


First, I find the space left and right of the \cdot operator rather 
larger. Is there a way to decrease it?

Second, and more important, in the generated PDF, the space left of the 
dot is much bigger, it looks as if it was typeset as e.g. "3 *10^6".

Thanks for any hint

Pascal


From t@v@r|ch@norberto @end|ng |rom gm@||@com  Wed Feb  6 22:40:52 2019
From: t@v@r|ch@norberto @end|ng |rom gm@||@com (Norberto Hernandez)
Date: Wed, 6 Feb 2019 15:40:52 -0600
Subject: [R] using ggplot2 geom_path() in a grouped variable
Message-ID: <CADRcKWx4-OFQNJtTeDWtCXw-cLUAoLNXPDtY=0djVmDUpcQb4Q@mail.gmail.com>

Hi! I am trying to make a scatter/path graph in one variable that is
divided in two groups (variable Control), but when I use the geom_path()
option, the line continues from the group one to the group two, and I
wasn't able to avoid it.I need the path draws over the group one and then
draws over the group two avoiding the connection between the last value of
group one and the first value of group two.

This is my code

library(ggplot2)
plot <- ggplot(data, aes(Pretrend, Outcome)) + geom_point()
plot + geom_point(aes(colour=factor(Control)))+labs(x="Date", y="Cases",
title="Intoxication Cases")+geom_path()

Could you please bring be some advice.
Regards
Norberto Francisco Hern?ndez

From bruce@@w|h@rt @end|ng |rom gm@||@com  Tue Feb  5 18:54:20 2019
From: bruce@@w|h@rt @end|ng |rom gm@||@com (Bruce Swihart)
Date: Tue, 5 Feb 2019 12:54:20 -0500
Subject: [R] [R-pkgs] Jim Lindsey's packages back on CRAN
Message-ID: <CAAHpScqk1jhXDomtjU_Z+M-gX7Qpk1UWuLHBX-N8DZa2z=KGPQ@mail.gmail.com>

R Users,

Jim Lindsey's R packages:

  * event: Event History Procedures and Models
  * gnlm: Generalized Nonlinear Regression Models
  * growth: Multivariate Normal and Elliptically-Contoured Repeated
Measurements Models
  * repeated: Non-Normal Repeated Measurements Models
  * rmutil: Utilities for Nonlinear Regression and Repeated Measurements
Models
  * stable: Probability Functions and Generalized Regression Models for
Stable Distributions

are now being maintained on CRAN and developed with a Github repo
workflow.   There are still off-CRAN versions and supporting materials
available at his website (http://www.commanster.eu/rcode.html).  I have
added some code to help equate and transform the pm=0, pm=1 Nolan
parameterizations with the parameterization provided in stable (see more at
https://github.com/swihart/stable).

All the best,
Bruce

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From rn|@boh @end|ng |rom gm@||@com  Thu Feb  7 14:09:50 2019
From: rn|@boh @end|ng |rom gm@||@com (Bob O'Hara)
Date: Thu, 7 Feb 2019 14:09:50 +0100
Subject: [R] very slow code execution
In-Reply-To: <CANcGnwzfyhT7V2F_6X4Yo7EKS868UGZbDvSsMvxTkxOKNZ6YJQ@mail.gmail.com>
References: <CANcGnwxCDyRKRRmX8+XN6XygOYasvFBLZ+JCdedGjZGMOAcQ_Q@mail.gmail.com>
 <d3530209-f284-90f5-3c2a-d17fcff0883d@dewey.myzen.co.uk>
 <CANcGnwzfyhT7V2F_6X4Yo7EKS868UGZbDvSsMvxTkxOKNZ6YJQ@mail.gmail.com>
Message-ID: <CAN-Z0xWfVkDMTJ+Ou27dWE7Srd=mhgFTF_GX9Zk6V0bcRQQzbg@mail.gmail.com>

With 96k model fits it's going to be slow, so you might want to think
first about whether you need to do them all. Beyond that, I think this
is more in the R style, so might be quicker (I don't know how much the
loops are slowing you down), and even if not it should be easier to
adapt.

The other thing to think about is parallelising the code - the
parallel package should help.

FitModel <- function(K, data) {
  z1 <- fourier(ts(data,frequency = 7), K=K["i"])
  z2 <- fourier(ts(data,frequency=354), K=K["j"])
  z3 <- fourier(ts(data,,frequency = 365),K=K["k"])
  fit <- auto.arima(data,, xreg =cbind(z1,z2,z3), seasonal = FALSE)
  fit$aicc
}

# smaller MaxOrders used so if you run it like this, it won't take 5 hours
MaxOrders <- expand.grid(i = 1:3, j=1=7, k=1:8)
AICc <- apply(MaxOrders, FitModel, data=demand)

Bob

      AICc<- data.table(matrix(nrow = 96642, ncol = 4))for (i in 1:3) {
> >    for (j in 1:177) {
> >      for (k in 182) {                     #i,j and k values are choosen
> > with regad that K cannot exceed seasonality period/2
> >        z1 <- fourier(ts(demand,frequency = 7), K=i)
> >        z2 <- fourier(ts(demand,frequency=354), K=j)
> >        z3 <- fourier(ts(demand,frequency = 365),K=k)
> >        fit <- auto.arima(demand, xreg =cbind(z1,z2,z3),
> >           seasonal = FALSE)
> >        fit$aicc
> >        AICc[,1]<-i
> >        AICc[,2]<-j
> >        AICc[,3]<-k
> >        AICc[,4]<-fit$aicc
> >      }
> >change
> >    }
> > }
> >    AICc

On Thu, 7 Feb 2019 at 13:44, salah maadawy <salahmaadawy at gmail.com> wrote:
>
> Hi Micheal, Maybe there is a simple way but i wanted to get the lowest aicc
> ana i could not find a way to do so, that's why i created the  table to
> store all possible outcomes and then i can easily get the minimum value and
> the values of (i,j and k) used for that minimum value. The first column in
> the table is AICc[,1] to store i and second column for j and so on. Maybe i
> am mistaken and this won't give me what i want, the code been running for 5
> hours now. So i am waiting
>
> On Wed, Feb 6, 2019 at 4:59 PM Michael Dewey <lists at dewey.myzen.co.uk>
> wrote:
>
> > This is not an answer to your speed problem but are your assignments to
> > AICc[,1] and so on doing what you hope they are doing?
> >
> > Michael
> >
> > On 06/02/2019 12:03, salah maadawy wrote:
> > > i am a beginner regarding R but i am trying to do a simple thing, but it
> > is
> > > taking too much time and i am asking if there is any way to achieve what
> > i
> > > need, i have a time series data set with 730 data points, i detected 7,
> > 354
> > > and 365 seasonality periods. i am trying to use Fourier terms for
> > > seasonality and for loop to get the K value for each while minimizing
> > AICc,
> > > my code is
> > >
> > >      AICc<- data.table(matrix(nrow = 96642, ncol = 4))for (i in 1:3) {
> > >    for (j in 1:177) {
> > >      for (k in 182) {                     #i,j and k values are choosen
> > > with regad that K cannot exceed seasonality period/2
> > >        z1 <- fourier(ts(demand,frequency = 7), K=i)
> > >        z2 <- fourier(ts(demand,frequency=354), K=j)
> > >        z3 <- fourier(ts(demand,frequency = 365),K=k)
> > >        fit <- auto.arima(demand, xreg =cbind(z1,z2,z3),
> > >           seasonal = FALSE)
> > >        fit$aicc
> > >        AICc[,1]<-i
> > >        AICc[,2]<-j
> > >        AICc[,3]<-k
> > >        AICc[,4]<-fit$aicc
> > >      }
> > >
> > >    }
> > > }
> > >    AICc
> > >
> > > i have created a data table to store AICc values from all possible i,j,k
> > > combinations so that i can find later the minimum AICc value. the problem
> > > now is that it is taking forever to do so not only to iterate all
> > > combinations but also due to the large K values.
> > >
> > > , is there any possible solution for this? thank you in advance
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > --
> > Michael
> > http://www.dewey.myzen.co.uk/home.html
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Bob O'Hara
Institutt for matematiske fag
NTNU
7491 Trondheim
Norway

Mobile: +47 915 54 416
Journal of Negative Results - EEB: www.jnr-eeb.org


From roy@mende|@@ohn @end|ng |rom no@@@gov  Fri Feb  8 02:51:08 2019
From: roy@mende|@@ohn @end|ng |rom no@@@gov (Roy Mendelssohn - NOAA Federal)
Date: Thu, 7 Feb 2019 17:51:08 -0800
Subject: [R] Two gganimate questions.
Message-ID: <C9A90E5B-627A-401A-B916-845BE7F3B5E6@noaa.gov>

I have two gganimate questions that I have made some headway on but not too much,  and they are actually related.  The questions are:

1.  Suppose I have a list where each element of the list is a pre-defined ggplots2 graphic  (in my case each is a map).  Is there a way to animate this,  and if so,  what is the best way?  I have tried using gganimate::transition_layers()   and while it recognized the lists and produced an animation  (albeit very slowly).

2. is there a way to build up an animation as we go?  it is a long story,  but each map in the list above has to be calculated separated,  it it done using geom_sf.  can an initial animation be defined and then frames added to it as new maps are made?

Thanks for any help.

-Roy



**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new street address***
110 McAllister Way
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Fri Feb  8 11:27:46 2019
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 8 Feb 2019 11:27:46 +0100
Subject: [R] Spacing around \cdot (%.%) unequal in pdf
In-Reply-To: <e4a77efe-24cd-e369-f74a-71aa424202e4@ieu.uzh.ch>
References: <e4a77efe-24cd-e369-f74a-71aa424202e4@ieu.uzh.ch>
Message-ID: <23645.22946.721942.255153@stat.math.ethz.ch>

>>>>> Pascal A Niklaus 
>>>>>     on Wed, 6 Feb 2019 19:48:49 +0100 writes:

    > I am struggling labeling an axis in a plot.
    > Here is a minimal example illustrating the point:


    > pdf("mve.pdf",width=5,height=5)
    > ypos <- c(1:3) * 1e6
    > ylabs <- sapply(ypos/1e6, function(i) as.expression(bquote(.(i)%.%10^6)))
    > plot(rep(1,3), ypos, yaxt="n", ylab="")
    > axis(2, at=ypos, labels=ylabs, las=1)
    > dev.off()


    > First, I find the space left and right of the \cdot operator rather 
    > larger. Is there a way to decrease it?

    > Second, and more important, in the generated PDF, the space left of the 
    > dot is much bigger, it looks as if it was typeset as e.g. "3 *10^6".

Vaguely related:   I have advertized my  eaxis()  and
pretty10exp() functions in package 'sfsmisc' for doing what you do above more
generally and automatically:

if(!require("sfsmisc")) {
  install.packages("sfsmisc") ; library("sfsmisc")
}	

ypos <- c(1:3) * 1e6
plot(rep(1,3), ypos, yaxt="n", ylab="")
## e.g.,
eaxis(2, at = ypos, labels = pretty10exp(ypos, drop.1=TRUE))

gives a nice plot for me, not using 'cdot' but rather

  > pretty10exp(ypos, drop.1=TRUE)
  expression(10^6, 2 %*% 10^6, 3 %*% 10^6)
  > 

    > Thanks for any hint
    > Pascal

You are welcome!

Martin Maechler
ETH Zurich and R Core Team


From meyner@@m @end|ng |rom pg@com  Fri Feb  8 16:31:26 2019
From: meyner@@m @end|ng |rom pg@com (Meyners, Michael)
Date: Fri, 8 Feb 2019 15:31:26 +0000
Subject: [R] Randomization Test
In-Reply-To: <CAC8ss32K6KNVYUgiJmmw6UParhgjyQJAp9t8q2CdMjKakzHOrg@mail.gmail.com>
References: <CAC8ss32K6KNVYUgiJmmw6UParhgjyQJAp9t8q2CdMjKakzHOrg@mail.gmail.com>
Message-ID: <BL0PR01MB4132FD4418FADCA82CBDC3349A690@BL0PR01MB4132.prod.exchangelabs.com>

Ogbos,

You do not seem to have received a reply over the list yet, which might be due to the fact that this seems rather a stats than an R question. Neither got your attachment (Figure) through - see posting guide. 

I'm not familiar with epoch analysis, so not sure what exactly you are doing / trying to achieve, but some general thoughts: 

* You do NOT want to restrict your re-randomizations in a way that "none of the dates corresponds with the ones in the real event" - actually, as a general principle, the true data must be an admissible re-randomization as well. You seem to have excluded that (and a lot of other randomizations at the same time which might have occurred, i.e. dates 1 and 2 reversed but all others the same), thereby rendering the test invalid. Any restrictions you have on your re-randomizations must've applied to the original randomization as well.
* If you have rather observational data (which I suspect, but not sure), Edgington & Onghena (2007) would rather refer to this as a permutation test - the difference being that you have to make strong assumptions (similar to parametric tests) on the nature of the data, which are designed-in to be true for randomization tests. It might be a merely linguistic discrimination, but it is important to note which assumptions have to be (implicitly) made.
* I'm not sure what you mean by "mean differences" of the events - is that two groups you are comparing? If so, that seems reasonable, but just make sure the test statistic you use is reasonable and sensitive against the alternatives you are mostly interested in. The randomization/permutation test will never proof that, e.g., means are significantly different, but only that there is SOME difference. By selecting the appropriate test statistic, you can influence what will pop up more easily and what not, but you can never be sure (unless you make strong assumptions about everything else, like in many parametric tests).
* For any test statistic, you would then determine the proportion of its values among the 5000 samples where it is as large or larger than the one observed (or as small or smaller, or either, depending on the nature of the test statistic and whether you aim for a one- or a two-sided test). That is your p value. If small enough, conclude significance. At least conceptually important: The observed test statistic is always part of the re-randomization (i.e. your 5000) - so you truly only do 4999 plus the one you observed. Otherwise the test may be more or less liberal. Your p value is hence no smaller than 1/n, where n is the total number of samples you looked at (including the observed one), a p value of 0 is not possible in randomization tests (nor in other tests, of course).

I hope this is helpful, but you will need to go through these and refer to your own setup to check whether you adhered to the principles or not, which is impossible for me to judge based on the information provided (and I won't be able to look at excessive code to check either).

Michael

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ogbos Okike
> Sent: Montag, 28. Januar 2019 19:42
> To: r-help <r-help at r-project.org>
> Subject: [R] Randomization Test
> 
> Dear Contributors,
> 
> I conducting epoch analysis. I tried to test the significance of my result using
> randomization test.
> 
> Since I have 71 events, I randomly selected another 71 events, making sure
> that none of the dates in the random events corresponds with the ones in
> the real event.
> 
> Following the code I found here
> (https://www.uvm.edu/~dhowell/StatPages/R/RandomizationTestsWithR/R
> andom2Sample/TwoIndependentSamplesR.html),
> I combined these two data set and used them to generate another 5000
> events. I then plotted the graph of the mean differences for the 5000
> randomly generated events. On the graph, I indicated the region of the
> mean difference between the real 71 epoch and the randomly selected 71
> epoch.
> 
> Since the two tail test shows that the mean difference falls at the extreme of
> the randomly selected events, I concluded that my result is statistically
> significant.
> 
> 
> 
> I am attaching the graph to assistance you in you suggestions.
> 
> I can attach both my code and the real and randomly generated events if you
> ask for it.
> 
> My request is that you help me to understand if I am on the right track or no.
> This is the first time I am doing this and except the experts decide, I am not
> quite sure whether I am right or not.
> 
> Many thanks for your kind concern.
> 
> Best
> Ogbos
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From petr@p|k@| @end|ng |rom prechez@@cz  Fri Feb  8 10:53:58 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Fri, 8 Feb 2019 09:53:58 +0000
Subject: [R] pattern evaluation in electron microscopy images
Message-ID: <fb6c47fe3525477c9d63e53e9a633855@SRVEXCHCM1302.precheza.cz>

Dear all

I enclose 3 electron microscope images in which I would like to evaluate plane spacing.

Before I start to dig deeper and use trial and error in trying to find some packages/functions for such pattern evaluation in electron microscopy pictures I would like to ask if anybody could point me to suitable packages/functions.

I am aware of EBImage package for general purpose image manipulation, but it does not have such functionality.

Best regards
Petr

If images did not came through please use this link:
St?hnout soubory<https://uschovna.agrofert.cz/dshosts/getfiles.aspx?fip=2475efa6-a77b-4ff6-8155-d9302e7b151b>.

Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner's personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


From tgozdz @end|ng |rom gm@||@com  Fri Feb  8 19:24:16 2019
From: tgozdz @end|ng |rom gm@||@com (Tony Gozdz)
Date: Fri, 8 Feb 2019 13:24:16 -0500
Subject: [R] why standardize the variables to perform LDA?
Message-ID: <CAO_D832BtyU=5T2j74NcFauf6ymjznE57iVf7SmOEENWNrOnAQ@mail.gmail.com>

I understand the need to standardize the variables to perform PCA, but is
this a recommendation or necessity before running LDA?

	[[alternative HTML version deleted]]


From @@|@hm@@d@wy @end|ng |rom gm@||@com  Sat Feb  9 10:21:43 2019
From: @@|@hm@@d@wy @end|ng |rom gm@||@com (salah maadawy)
Date: Sat, 9 Feb 2019 12:21:43 +0300
Subject: [R] very slow code execution
In-Reply-To: <CAN-Z0xWfVkDMTJ+Ou27dWE7Srd=mhgFTF_GX9Zk6V0bcRQQzbg@mail.gmail.com>
References: <CANcGnwxCDyRKRRmX8+XN6XygOYasvFBLZ+JCdedGjZGMOAcQ_Q@mail.gmail.com>
 <d3530209-f284-90f5-3c2a-d17fcff0883d@dewey.myzen.co.uk>
 <CANcGnwzfyhT7V2F_6X4Yo7EKS868UGZbDvSsMvxTkxOKNZ6YJQ@mail.gmail.com>
 <CAN-Z0xWfVkDMTJ+Ou27dWE7Srd=mhgFTF_GX9Zk6V0bcRQQzbg@mail.gmail.com>
Message-ID: <CANcGnwwAZc765KCZH8J-C91RTXGiYLKXL19=t-kBWZGdO2hyZg@mail.gmail.com>

thank you all for your suggestions and feedback, with further search and
experimentation, the problem is with the auto.arima function with large k
values, it takes 4 min to compute one model with k=30 and the time
increases with K, so i used your suggestions for collecting the output but
i limited my loop (i=3,j=25 and k=25) , it took around 17 hours to finish
already (1875 models). thank you again.

On Thu, Feb 7, 2019 at 4:10 PM Bob O'Hara <rni.boh at gmail.com> wrote:

> With 96k model fits it's going to be slow, so you might want to think
> first about whether you need to do them all. Beyond that, I think this
> is more in the R style, so might be quicker (I don't know how much the
> loops are slowing you down), and even if not it should be easier to
> adapt.
>
> The other thing to think about is parallelising the code - the
> parallel package should help.
>
> FitModel <- function(K, data) {
>   z1 <- fourier(ts(data,frequency = 7), K=K["i"])
>   z2 <- fourier(ts(data,frequency=354), K=K["j"])
>   z3 <- fourier(ts(data,,frequency = 365),K=K["k"])
>   fit <- auto.arima(data,, xreg =cbind(z1,z2,z3), seasonal = FALSE)
>   fit$aicc
> }
>
> # smaller MaxOrders used so if you run it like this, it won't take 5 hours
> MaxOrders <- expand.grid(i = 1:3, j=1=7, k=1:8)
> AICc <- apply(MaxOrders, FitModel, data=demand)
>
> Bob
>
>       AICc<- data.table(matrix(nrow = 96642, ncol = 4))for (i in 1:3) {
> > >    for (j in 1:177) {
> > >      for (k in 182) {                     #i,j and k values are choosen
> > > with regad that K cannot exceed seasonality period/2
> > >        z1 <- fourier(ts(demand,frequency = 7), K=i)
> > >        z2 <- fourier(ts(demand,frequency=354), K=j)
> > >        z3 <- fourier(ts(demand,frequency = 365),K=k)
> > >        fit <- auto.arima(demand, xreg =cbind(z1,z2,z3),
> > >           seasonal = FALSE)
> > >        fit$aicc
> > >        AICc[,1]<-i
> > >        AICc[,2]<-j
> > >        AICc[,3]<-k
> > >        AICc[,4]<-fit$aicc
> > >      }
> > >change
> > >    }
> > > }
> > >    AICc
>
> On Thu, 7 Feb 2019 at 13:44, salah maadawy <salahmaadawy at gmail.com> wrote:
> >
> > Hi Micheal, Maybe there is a simple way but i wanted to get the lowest
> aicc
> > ana i could not find a way to do so, that's why i created the  table to
> > store all possible outcomes and then i can easily get the minimum value
> and
> > the values of (i,j and k) used for that minimum value. The first column
> in
> > the table is AICc[,1] to store i and second column for j and so on.
> Maybe i
> > am mistaken and this won't give me what i want, the code been running
> for 5
> > hours now. So i am waiting
> >
> > On Wed, Feb 6, 2019 at 4:59 PM Michael Dewey <lists at dewey.myzen.co.uk>
> > wrote:
> >
> > > This is not an answer to your speed problem but are your assignments to
> > > AICc[,1] and so on doing what you hope they are doing?
> > >
> > > Michael
> > >
> > > On 06/02/2019 12:03, salah maadawy wrote:
> > > > i am a beginner regarding R but i am trying to do a simple thing,
> but it
> > > is
> > > > taking too much time and i am asking if there is any way to achieve
> what
> > > i
> > > > need, i have a time series data set with 730 data points, i detected
> 7,
> > > 354
> > > > and 365 seasonality periods. i am trying to use Fourier terms for
> > > > seasonality and for loop to get the K value for each while minimizing
> > > AICc,
> > > > my code is
> > > >
> > > >      AICc<- data.table(matrix(nrow = 96642, ncol = 4))for (i in 1:3)
> {
> > > >    for (j in 1:177) {
> > > >      for (k in 182) {                     #i,j and k values are
> choosen
> > > > with regad that K cannot exceed seasonality period/2
> > > >        z1 <- fourier(ts(demand,frequency = 7), K=i)
> > > >        z2 <- fourier(ts(demand,frequency=354), K=j)
> > > >        z3 <- fourier(ts(demand,frequency = 365),K=k)
> > > >        fit <- auto.arima(demand, xreg =cbind(z1,z2,z3),
> > > >           seasonal = FALSE)
> > > >        fit$aicc
> > > >        AICc[,1]<-i
> > > >        AICc[,2]<-j
> > > >        AICc[,3]<-k
> > > >        AICc[,4]<-fit$aicc
> > > >      }
> > > >
> > > >    }
> > > > }
> > > >    AICc
> > > >
> > > > i have created a data table to store AICc values from all possible
> i,j,k
> > > > combinations so that i can find later the minimum AICc value. the
> problem
> > > > now is that it is taking forever to do so not only to iterate all
> > > > combinations but also due to the large K values.
> > > >
> > > > , is there any possible solution for this? thank you in advance
> > > >
> > > >       [[alternative HTML version deleted]]
> > > >
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > > >
> > >
> > > --
> > > Michael
> > > http://www.dewey.myzen.co.uk/home.html
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Bob O'Hara
> Institutt for matematiske fag
> NTNU
> 7491 Trondheim
> Norway
>
> Mobile: +47 915 54 416
> Journal of Negative Results - EEB: www.jnr-eeb.org
>

	[[alternative HTML version deleted]]


From v@|kremk @end|ng |rom gm@||@com  Sat Feb  9 19:05:40 2019
From: v@|kremk @end|ng |rom gm@||@com (Val)
Date: Sat, 9 Feb 2019 12:05:40 -0600
Subject: [R] character comp
Message-ID: <CAJOiR6bkWri5o6ebm7D3cs9V2KbdoBwbdMJkzVA=a4A_J4PFHw@mail.gmail.com>

Hi  All,
In a given data frame I  want to compare character values of two columns.
My sample data looks like as follow,

mydataframe <- read.table( text='ID  var1 var2
  R1   AA  AAA
  R2   AAA AAA
  R3    A  AAAA
  R4   AA   A
  R5   A  AAA', header = TRUE, as.is = TRUE )

For each ID, I want  create the third column "dvar" as  difference
between var1 and var2
 Row1( R1)   the "dvar" value will be -1 and the complete  desired out
put looks like as follow.

 ID    var1 var2   dvar
 R1   AA    AAA    -1
 R2  AAA  AAA      0
 R3    A    AAAA    -3
 R4   AA       A        1
 R5    A     AAA      -2

How do i do this? Any help please?
Thank you


From er|nm@hodge@@ @end|ng |rom gm@||@com  Sat Feb  9 19:21:56 2019
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Sat, 9 Feb 2019 11:21:56 -0700
Subject: [R] character comp
In-Reply-To: <CAJOiR6bkWri5o6ebm7D3cs9V2KbdoBwbdMJkzVA=a4A_J4PFHw@mail.gmail.com>
References: <CAJOiR6bkWri5o6ebm7D3cs9V2KbdoBwbdMJkzVA=a4A_J4PFHw@mail.gmail.com>
Message-ID: <CACxE24kfBrH5Re3xmpccRdJumw-5DydOO9ktJB-vDY_LNKg5DA@mail.gmail.com>

Will it always be A?s or will there be a mix please?

On Sat, Feb 9, 2019 at 11:06 AM Val <valkremk at gmail.com> wrote:

> Hi  All,
> In a given data frame I  want to compare character values of two columns.
> My sample data looks like as follow,
>
> mydataframe <- read.table( text='ID  var1 var2
>   R1   AA  AAA
>   R2   AAA AAA
>   R3    A  AAAA
>   R4   AA   A
>   R5   A  AAA', header = TRUE, as.is = TRUE )
>
> For each ID, I want  create the third column "dvar" as  difference
> between var1 and var2
>  Row1( R1)   the "dvar" value will be -1 and the complete  desired out
> put looks like as follow.
>
>  ID    var1 var2   dvar
>  R1   AA    AAA    -1
>  R2  AAA  AAA      0
>  R3    A    AAAA    -3
>  R4   AA       A        1
>  R5    A     AAA      -2
>
> How do i do this? Any help please?
> Thank you
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
-- 
Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From v@|kremk @end|ng |rom gm@||@com  Sat Feb  9 19:28:39 2019
From: v@|kremk @end|ng |rom gm@||@com (Val)
Date: Sat, 9 Feb 2019 12:28:39 -0600
Subject: [R] character comp
In-Reply-To: <CACxE24kfBrH5Re3xmpccRdJumw-5DydOO9ktJB-vDY_LNKg5DA@mail.gmail.com>
References: <CAJOiR6bkWri5o6ebm7D3cs9V2KbdoBwbdMJkzVA=a4A_J4PFHw@mail.gmail.com>
 <CACxE24kfBrH5Re3xmpccRdJumw-5DydOO9ktJB-vDY_LNKg5DA@mail.gmail.com>
Message-ID: <CAJOiR6Z0QoygNDt+9Lnos-WTFvXZq1p73sqq4oDUoJX2UEHDfA@mail.gmail.com>

Hi Erin,  Yes, it is always  A's.

On Sat, Feb 9, 2019 at 12:22 PM Erin Hodgess <erinm.hodgess at gmail.com> wrote:
>
> Will it always be A?s or will there be a mix please?
>
> On Sat, Feb 9, 2019 at 11:06 AM Val <valkremk at gmail.com> wrote:
>>
>> Hi  All,
>> In a given data frame I  want to compare character values of two columns.
>> My sample data looks like as follow,
>>
>> mydataframe <- read.table( text='ID  var1 var2
>>   R1   AA  AAA
>>   R2   AAA AAA
>>   R3    A  AAAA
>>   R4   AA   A
>>   R5   A  AAA', header = TRUE, as.is = TRUE )
>>
>> For each ID, I want  create the third column "dvar" as  difference
>> between var1 and var2
>>  Row1( R1)   the "dvar" value will be -1 and the complete  desired out
>> put looks like as follow.
>>
>>  ID    var1 var2   dvar
>>  R1   AA    AAA    -1
>>  R2  AAA  AAA      0
>>  R3    A    AAAA    -3
>>  R4   AA       A        1
>>  R5    A     AAA      -2
>>
>> How do i do this? Any help please?
>> Thank you
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> --
> Erin Hodgess, PhD
> mailto: erinm.hodgess at gmail.com


From er|nm@hodge@@ @end|ng |rom gm@||@com  Sat Feb  9 19:31:19 2019
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Sat, 9 Feb 2019 11:31:19 -0700
Subject: [R] character comp
In-Reply-To: <CACxE24kfBrH5Re3xmpccRdJumw-5DydOO9ktJB-vDY_LNKg5DA@mail.gmail.com>
References: <CAJOiR6bkWri5o6ebm7D3cs9V2KbdoBwbdMJkzVA=a4A_J4PFHw@mail.gmail.com>
 <CACxE24kfBrH5Re3xmpccRdJumw-5DydOO9ktJB-vDY_LNKg5DA@mail.gmail.com>
Message-ID: <CACxE24mPBZP30KfFtNxaCdAUdE5a5FbB8TOOrpGNFXL9hEcV1g@mail.gmail.com>

Ok.  Try something like
nchar(mydataframe[,1])-nchar(mydataframe[,2])


On Sat, Feb 9, 2019 at 11:21 AM Erin Hodgess <erinm.hodgess at gmail.com>
wrote:

> Will it always be A?s or will there be a mix please?
>
> On Sat, Feb 9, 2019 at 11:06 AM Val <valkremk at gmail.com> wrote:
>
>> Hi  All,
>> In a given data frame I  want to compare character values of two columns.
>> My sample data looks like as follow,
>>
>> mydataframe <- read.table( text='ID  var1 var2
>>   R1   AA  AAA
>>   R2   AAA AAA
>>   R3    A  AAAA
>>   R4   AA   A
>>   R5   A  AAA', header = TRUE, as.is = TRUE )
>>
>> For each ID, I want  create the third column "dvar" as  difference
>> between var1 and var2
>>  Row1( R1)   the "dvar" value will be -1 and the complete  desired out
>> put looks like as follow.
>>
>>  ID    var1 var2   dvar
>>  R1   AA    AAA    -1
>>  R2  AAA  AAA      0
>>  R3    A    AAAA    -3
>>  R4   AA       A        1
>>  R5    A     AAA      -2
>>
>> How do i do this? Any help please?
>> Thank you
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> --
> Erin Hodgess, PhD
> mailto: erinm.hodgess at gmail.com
>
-- 
Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat Feb  9 19:55:02 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sat, 9 Feb 2019 18:55:02 +0000
Subject: [R] character comp
In-Reply-To: <CAJOiR6bkWri5o6ebm7D3cs9V2KbdoBwbdMJkzVA=a4A_J4PFHw@mail.gmail.com>
References: <CAJOiR6bkWri5o6ebm7D3cs9V2KbdoBwbdMJkzVA=a4A_J4PFHw@mail.gmail.com>
Message-ID: <116db21f-bd8f-66c6-e553-1756221cd033@sapo.pt>

Hello,

The following will do it.

mydataframe$dvar <- c(sapply(mydataframe[-1], nchar) %*% c(1, -1))


Hope this helps,

Rui Barradas

?s 18:05 de 09/02/2019, Val escreveu:
> Hi  All,
> In a given data frame I  want to compare character values of two columns.
> My sample data looks like as follow,
> 
> mydataframe <- read.table( text='ID  var1 var2
>    R1   AA  AAA
>    R2   AAA AAA
>    R3    A  AAAA
>    R4   AA   A
>    R5   A  AAA', header = TRUE, as.is = TRUE )
> 
> For each ID, I want  create the third column "dvar" as  difference
> between var1 and var2
>   Row1( R1)   the "dvar" value will be -1 and the complete  desired out
> put looks like as follow.
> 
>   ID    var1 var2   dvar
>   R1   AA    AAA    -1
>   R2  AAA  AAA      0
>   R3    A    AAAA    -3
>   R4   AA       A        1
>   R5    A     AAA      -2
> 
> How do i do this? Any help please?
> Thank you
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat Feb  9 19:56:47 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sat, 9 Feb 2019 18:56:47 +0000
Subject: [R] character comp
In-Reply-To: <CACxE24mPBZP30KfFtNxaCdAUdE5a5FbB8TOOrpGNFXL9hEcV1g@mail.gmail.com>
References: <CAJOiR6bkWri5o6ebm7D3cs9V2KbdoBwbdMJkzVA=a4A_J4PFHw@mail.gmail.com>
 <CACxE24kfBrH5Re3xmpccRdJumw-5DydOO9ktJB-vDY_LNKg5DA@mail.gmail.com>
 <CACxE24mPBZP30KfFtNxaCdAUdE5a5FbB8TOOrpGNFXL9hEcV1g@mail.gmail.com>
Message-ID: <8bfcd025-17e0-2551-0284-8e10cfdfd917@sapo.pt>

Hello,

It should be

nchar(mydataframe[,2])-nchar(mydataframe[,3])


since the first column is ID.

Hope this helps,

Rui Barradas


?s 18:31 de 09/02/2019, Erin Hodgess escreveu:
> Ok.  Try something like
> nchar(mydataframe[,1])-nchar(mydataframe[,2])
> 
> 
> On Sat, Feb 9, 2019 at 11:21 AM Erin Hodgess <erinm.hodgess at gmail.com>
> wrote:
> 
>> Will it always be A?s or will there be a mix please?
>>
>> On Sat, Feb 9, 2019 at 11:06 AM Val <valkremk at gmail.com> wrote:
>>
>>> Hi  All,
>>> In a given data frame I  want to compare character values of two columns.
>>> My sample data looks like as follow,
>>>
>>> mydataframe <- read.table( text='ID  var1 var2
>>>    R1   AA  AAA
>>>    R2   AAA AAA
>>>    R3    A  AAAA
>>>    R4   AA   A
>>>    R5   A  AAA', header = TRUE, as.is = TRUE )
>>>
>>> For each ID, I want  create the third column "dvar" as  difference
>>> between var1 and var2
>>>   Row1( R1)   the "dvar" value will be -1 and the complete  desired out
>>> put looks like as follow.
>>>
>>>   ID    var1 var2   dvar
>>>   R1   AA    AAA    -1
>>>   R2  AAA  AAA      0
>>>   R3    A    AAAA    -3
>>>   R4   AA       A        1
>>>   R5    A     AAA      -2
>>>
>>> How do i do this? Any help please?
>>> Thank you
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>> --
>> Erin Hodgess, PhD
>> mailto: erinm.hodgess at gmail.com
>>


From er|nm@hodge@@ @end|ng |rom gm@||@com  Sat Feb  9 20:08:07 2019
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Sat, 9 Feb 2019 12:08:07 -0700
Subject: [R] character comp
In-Reply-To: <116db21f-bd8f-66c6-e553-1756221cd033@sapo.pt>
References: <CAJOiR6bkWri5o6ebm7D3cs9V2KbdoBwbdMJkzVA=a4A_J4PFHw@mail.gmail.com>
 <116db21f-bd8f-66c6-e553-1756221cd033@sapo.pt>
Message-ID: <CACxE24m7gwxRBnynYnR1n29qyZ6nHc4oVHu0+1r7ZCmU-rHdVg@mail.gmail.com>

Nice, Rui!  Thanks

On Sat, Feb 9, 2019 at 11:55 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> The following will do it.
>
> mydataframe$dvar <- c(sapply(mydataframe[-1], nchar) %*% c(1, -1))
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 18:05 de 09/02/2019, Val escreveu:
> > Hi  All,
> > In a given data frame I  want to compare character values of two columns.
> > My sample data looks like as follow,
> >
> > mydataframe <- read.table( text='ID  var1 var2
> >    R1   AA  AAA
> >    R2   AAA AAA
> >    R3    A  AAAA
> >    R4   AA   A
> >    R5   A  AAA', header = TRUE, as.is = TRUE )
> >
> > For each ID, I want  create the third column "dvar" as  difference
> > between var1 and var2
> >   Row1( R1)   the "dvar" value will be -1 and the complete  desired out
> > put looks like as follow.
> >
> >   ID    var1 var2   dvar
> >   R1   AA    AAA    -1
> >   R2  AAA  AAA      0
> >   R3    A    AAAA    -3
> >   R4   AA       A        1
> >   R5    A     AAA      -2
> >
> > How do i do this? Any help please?
> > Thank you
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
-- 
Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat Feb  9 20:10:03 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sat, 9 Feb 2019 19:10:03 +0000
Subject: [R] character comp
In-Reply-To: <CACxE24m7gwxRBnynYnR1n29qyZ6nHc4oVHu0+1r7ZCmU-rHdVg@mail.gmail.com>
References: <CAJOiR6bkWri5o6ebm7D3cs9V2KbdoBwbdMJkzVA=a4A_J4PFHw@mail.gmail.com>
 <116db21f-bd8f-66c6-e553-1756221cd033@sapo.pt>
 <CACxE24m7gwxRBnynYnR1n29qyZ6nHc4oVHu0+1r7ZCmU-rHdVg@mail.gmail.com>
Message-ID: <a292feb3-dcfe-dc64-26df-c64c5f7cb13b@sapo.pt>

After correcting the typo I tested both and yours is over twice as fast 
as mine.

Rui Barradas

?s 19:08 de 09/02/2019, Erin Hodgess escreveu:
> Nice, Rui!? Thanks
> 
> On Sat, Feb 9, 2019 at 11:55 AM Rui Barradas <ruipbarradas at sapo.pt 
> <mailto:ruipbarradas at sapo.pt>> wrote:
> 
>     Hello,
> 
>     The following will do it.
> 
>     mydataframe$dvar <- c(sapply(mydataframe[-1], nchar) %*% c(1, -1))
> 
> 
>     Hope this helps,
> 
>     Rui Barradas
> 
>     ?s 18:05 de 09/02/2019, Val escreveu:
>      > Hi? All,
>      > In a given data frame I? want to compare character values of two
>     columns.
>      > My sample data looks like as follow,
>      >
>      > mydataframe <- read.table( text='ID? var1 var2
>      >? ? R1? ?AA? AAA
>      >? ? R2? ?AAA AAA
>      >? ? R3? ? A? AAAA
>      >? ? R4? ?AA? ?A
>      >? ? R5? ?A? AAA', header = TRUE, as.is <http://as.is> = TRUE )
>      >
>      > For each ID, I want? create the third column "dvar" as? difference
>      > between var1 and var2
>      >? ?Row1( R1)? ?the "dvar" value will be -1 and the complete 
>     desired out
>      > put looks like as follow.
>      >
>      >? ?ID? ? var1 var2? ?dvar
>      >? ?R1? ?AA? ? AAA? ? -1
>      >? ?R2? AAA? AAA? ? ? 0
>      >? ?R3? ? A? ? AAAA? ? -3
>      >? ?R4? ?AA? ? ? ?A? ? ? ? 1
>      >? ?R5? ? A? ? ?AAA? ? ? -2
>      >
>      > How do i do this? Any help please?
>      > Thank you
>      >
>      > ______________________________________________
>      > R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>     -- To UNSUBSCRIBE and more, see
>      > https://stat.ethz.ch/mailman/listinfo/r-help
>      > PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>      > and provide commented, minimal, self-contained, reproducible code.
>      >
> 
>     ______________________________________________
>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>     To UNSUBSCRIBE and more, see
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     and provide commented, minimal, self-contained, reproducible code.
> 
> -- 
> Erin Hodgess, PhD
> mailto: erinm.hodgess at gmail.com <mailto:erinm.hodgess at gmail.com>


From v@|kremk @end|ng |rom gm@||@com  Sat Feb  9 22:51:33 2019
From: v@|kremk @end|ng |rom gm@||@com (Val)
Date: Sat, 9 Feb 2019 15:51:33 -0600
Subject: [R] character comp
In-Reply-To: <CACxE24m7gwxRBnynYnR1n29qyZ6nHc4oVHu0+1r7ZCmU-rHdVg@mail.gmail.com>
References: <CAJOiR6bkWri5o6ebm7D3cs9V2KbdoBwbdMJkzVA=a4A_J4PFHw@mail.gmail.com>
 <116db21f-bd8f-66c6-e553-1756221cd033@sapo.pt>
 <CACxE24m7gwxRBnynYnR1n29qyZ6nHc4oVHu0+1r7ZCmU-rHdVg@mail.gmail.com>
Message-ID: <CAJOiR6aeWpZdCgVrdcVG=htepBs_HdBqSLU-R8i_OB9Hc5zZHw@mail.gmail.com>

Thank you Erin and Rui!


On Sat, Feb 9, 2019 at 1:08 PM Erin Hodgess <erinm.hodgess at gmail.com> wrote:
>
> Nice, Rui!  Thanks
>
> On Sat, Feb 9, 2019 at 11:55 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>>
>> Hello,
>>
>> The following will do it.
>>
>> mydataframe$dvar <- c(sapply(mydataframe[-1], nchar) %*% c(1, -1))
>>
>>
>> Hope this helps,
>>
>> Rui Barradas
>>
>> ?s 18:05 de 09/02/2019, Val escreveu:
>> > Hi  All,
>> > In a given data frame I  want to compare character values of two columns.
>> > My sample data looks like as follow,
>> >
>> > mydataframe <- read.table( text='ID  var1 var2
>> >    R1   AA  AAA
>> >    R2   AAA AAA
>> >    R3    A  AAAA
>> >    R4   AA   A
>> >    R5   A  AAA', header = TRUE, as.is = TRUE )
>> >
>> > For each ID, I want  create the third column "dvar" as  difference
>> > between var1 and var2
>> >   Row1( R1)   the "dvar" value will be -1 and the complete  desired out
>> > put looks like as follow.
>> >
>> >   ID    var1 var2   dvar
>> >   R1   AA    AAA    -1
>> >   R2  AAA  AAA      0
>> >   R3    A    AAAA    -3
>> >   R4   AA       A        1
>> >   R5    A     AAA      -2
>> >
>> > How do i do this? Any help please?
>> > Thank you
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> --
> Erin Hodgess, PhD
> mailto: erinm.hodgess at gmail.com


From @boue|m@k@r|m1962 @end|ng |rom gm@||@com  Sun Feb 10 10:35:05 2019
From: @boue|m@k@r|m1962 @end|ng |rom gm@||@com (AbouEl-Makarim Aboueissa)
Date: Sun, 10 Feb 2019 04:35:05 -0500
Subject: [R] How to rum Multiple ANOVA and Multiple T-test between the same
 groups?
Message-ID: <CAE9stmcNuE-N-8KogigRysV8Qi=BQVtg80+eY3UK9uPFi-bZbA@mail.gmail.com>

Dear All: good morning





*Re:* How to rum Multiple ANOVA and Multiple T-test between the same groups.



Your help will be highly appreciated.





*1.*  is there a way to run multiple t-tests on different variables between
the same two groups.





*Data for t-tests:*



The data frame ?dataTtest?  has 5 variables (x1,x2,x3,x4,x5) and one factor
(factor1) with 2 levels (group1, group2).





x1<-rnorm(20,1,1)

x2<-rnorm(20,2,1)

x3<-rnorm(20,3,1)

x4<-rnorm(20,4,1)

x5<-rnorm(20,5,1)

factor1<-rep(c("group1", "group2"), each = 10)

dataTtest<-data.frame(x1,x2,x3,x4,x5,factor1)

dataTtest









*2.* is there a way to run *multiple ANOVA* and multiple comparisons *Tukey
tests* on different variables between the same groups.





*Data for ANOVA tests:*



The data frame ?dataANOVA?  has 6 variables (x1,x2,x3,x4,x5,x6) and one
factor (factor2) with 5 levels (group1, group2, group3, group4, group5).







x1<-rnorm(40,1,1)

x2<-rnorm(40,2,1)

x3<-rnorm(40,3,1)

x4<-rnorm(40,4,1)

x5<-rnorm(40,5,1)

x6<-rnorm(40,6,1)

factor2<-rep(c("group1", "group2", "group3", "group4", "group5"), each = 8)

dataANOVA<-data.frame(x1,x2,x3,x4,x5,x6,factor2)

dataANOVA





with many thanks

abou
______________________


*AbouEl-Makarim Aboueissa, PhD*

*Professor, Statistics and Data Science*
*Graduate Coordinator*

*Department of Mathematics and Statistics*
*University of Southern Maine*

	[[alternative HTML version deleted]]


From g||ted|||e2014 @end|ng |rom gm@||@com  Sun Feb 10 15:55:33 2019
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Sun, 10 Feb 2019 15:55:33 +0100
Subject: [R] Randomization Test
In-Reply-To: <BL0PR01MB4132FD4418FADCA82CBDC3349A690@BL0PR01MB4132.prod.exchangelabs.com>
References: <CAC8ss32K6KNVYUgiJmmw6UParhgjyQJAp9t8q2CdMjKakzHOrg@mail.gmail.com>
 <BL0PR01MB4132FD4418FADCA82CBDC3349A690@BL0PR01MB4132.prod.exchangelabs.com>
Message-ID: <CAC8ss32HYyCKXXy-U9R8i_eceXxrSVsjSezo=ELF7xchoGiL5w@mail.gmail.com>

Dear Michael,
This is great! Thank you.

I have not really got any response other than yours.

I have long before now included what I have in a paper submitted to a journal.

I am awaiting the feedback of the reviewer. I will compare the
comments with your input here and determine the corrections to make
and probably return to the list for additional help.

Best wishes
Ogbos

On Fri, Feb 8, 2019 at 4:31 PM Meyners, Michael <meyners.m at pg.com> wrote:
>
> Ogbos,
>
> You do not seem to have received a reply over the list yet, which might be due to the fact that this seems rather a stats than an R question. Neither got your attachment (Figure) through - see posting guide.
>
> I'm not familiar with epoch analysis, so not sure what exactly you are doing / trying to achieve, but some general thoughts:
>
> * You do NOT want to restrict your re-randomizations in a way that "none of the dates corresponds with the ones in the real event" - actually, as a general principle, the true data must be an admissible re-randomization as well. You seem to have excluded that (and a lot of other randomizations at the same time which might have occurred, i.e. dates 1 and 2 reversed but all others the same), thereby rendering the test invalid. Any restrictions you have on your re-randomizations must've applied to the original randomization as well.
> * If you have rather observational data (which I suspect, but not sure), Edgington & Onghena (2007) would rather refer to this as a permutation test - the difference being that you have to make strong assumptions (similar to parametric tests) on the nature of the data, which are designed-in to be true for randomization tests. It might be a merely linguistic discrimination, but it is important to note which assumptions have to be (implicitly) made.
> * I'm not sure what you mean by "mean differences" of the events - is that two groups you are comparing? If so, that seems reasonable, but just make sure the test statistic you use is reasonable and sensitive against the alternatives you are mostly interested in. The randomization/permutation test will never proof that, e.g., means are significantly different, but only that there is SOME difference. By selecting the appropriate test statistic, you can influence what will pop up more easily and what not, but you can never be sure (unless you make strong assumptions about everything else, like in many parametric tests).
> * For any test statistic, you would then determine the proportion of its values among the 5000 samples where it is as large or larger than the one observed (or as small or smaller, or either, depending on the nature of the test statistic and whether you aim for a one- or a two-sided test). That is your p value. If small enough, conclude significance. At least conceptually important: The observed test statistic is always part of the re-randomization (i.e. your 5000) - so you truly only do 4999 plus the one you observed. Otherwise the test may be more or less liberal. Your p value is hence no smaller than 1/n, where n is the total number of samples you looked at (including the observed one), a p value of 0 is not possible in randomization tests (nor in other tests, of course).
>
> I hope this is helpful, but you will need to go through these and refer to your own setup to check whether you adhered to the principles or not, which is impossible for me to judge based on the information provided (and I won't be able to look at excessive code to check either).
>
> Michael
>
> > -----Original Message-----
> > From: R-help <r-help-bounces at r-project.org> On Behalf Of Ogbos Okike
> > Sent: Montag, 28. Januar 2019 19:42
> > To: r-help <r-help at r-project.org>
> > Subject: [R] Randomization Test
> >
> > Dear Contributors,
> >
> > I conducting epoch analysis. I tried to test the significance of my result using
> > randomization test.
> >
> > Since I have 71 events, I randomly selected another 71 events, making sure
> > that none of the dates in the random events corresponds with the ones in
> > the real event.
> >
> > Following the code I found here
> > (https://www.uvm.edu/~dhowell/StatPages/R/RandomizationTestsWithR/R
> > andom2Sample/TwoIndependentSamplesR.html),
> > I combined these two data set and used them to generate another 5000
> > events. I then plotted the graph of the mean differences for the 5000
> > randomly generated events. On the graph, I indicated the region of the
> > mean difference between the real 71 epoch and the randomly selected 71
> > epoch.
> >
> > Since the two tail test shows that the mean difference falls at the extreme of
> > the randomly selected events, I concluded that my result is statistically
> > significant.
> >
> >
> >
> > I am attaching the graph to assistance you in you suggestions.
> >
> > I can attach both my code and the real and randomly generated events if you
> > ask for it.
> >
> > My request is that you help me to understand if I am on the right track or no.
> > This is the first time I am doing this and except the experts decide, I am not
> > quite sure whether I am right or not.
> >
> > Many thanks for your kind concern.
> >
> > Best
> > Ogbos
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From M@tth|@@@Koh| @end|ng |rom @t@m@t@@de  Sun Feb 10 16:52:40 2019
From: M@tth|@@@Koh| @end|ng |rom @t@m@t@@de (Prof. Dr. Matthias Kohl)
Date: Sun, 10 Feb 2019 16:52:40 +0100
Subject: [R] 
 How to rum Multiple ANOVA and Multiple T-test between the same
 groups?
In-Reply-To: <CAE9stmcNuE-N-8KogigRysV8Qi=BQVtg80+eY3UK9uPFi-bZbA@mail.gmail.com>
References: <CAE9stmcNuE-N-8KogigRysV8Qi=BQVtg80+eY3UK9uPFi-bZbA@mail.gmail.com>
Message-ID: <584da88d-1fb8-aae2-efda-e0e9e34beffb@stamats.de>

Have a look at Bioconductor package genefilter, especially functions 
colttests and colFtests.
Best Matthias

Am 10.02.19 um 10:35 schrieb AbouEl-Makarim Aboueissa:
> Dear All: good morning
> 
> 
> 
> 
> 
> *Re:* How to rum Multiple ANOVA and Multiple T-test between the same groups.
> 
> 
> 
> Your help will be highly appreciated.
> 
> 
> 
> 
> 
> *1.*  is there a way to run multiple t-tests on different variables between
> the same two groups.
> 
> 
> 
> 
> 
> *Data for t-tests:*
> 
> 
> 
> The data frame ?dataTtest?  has 5 variables (x1,x2,x3,x4,x5) and one factor
> (factor1) with 2 levels (group1, group2).
> 
> 
> 
> 
> 
> x1<-rnorm(20,1,1)
> 
> x2<-rnorm(20,2,1)
> 
> x3<-rnorm(20,3,1)
> 
> x4<-rnorm(20,4,1)
> 
> x5<-rnorm(20,5,1)
> 
> factor1<-rep(c("group1", "group2"), each = 10)
> 
> dataTtest<-data.frame(x1,x2,x3,x4,x5,factor1)
> 
> dataTtest
> 
> 
> 
> 
> 
> 
> 
> 
> 
> *2.* is there a way to run *multiple ANOVA* and multiple comparisons *Tukey
> tests* on different variables between the same groups.
> 
> 
> 
> 
> 
> *Data for ANOVA tests:*
> 
> 
> 
> The data frame ?dataANOVA?  has 6 variables (x1,x2,x3,x4,x5,x6) and one
> factor (factor2) with 5 levels (group1, group2, group3, group4, group5).
> 
> 
> 
> 
> 
> 
> 
> x1<-rnorm(40,1,1)
> 
> x2<-rnorm(40,2,1)
> 
> x3<-rnorm(40,3,1)
> 
> x4<-rnorm(40,4,1)
> 
> x5<-rnorm(40,5,1)
> 
> x6<-rnorm(40,6,1)
> 
> factor2<-rep(c("group1", "group2", "group3", "group4", "group5"), each = 8)
> 
> dataANOVA<-data.frame(x1,x2,x3,x4,x5,x6,factor2)
> 
> dataANOVA
> 
> 
> 
> 
> 
> with many thanks
> 
> abou
> ______________________
> 
> 
> *AbouEl-Makarim Aboueissa, PhD*
> 
> *Professor, Statistics and Data Science*
> *Graduate Coordinator*
> 
> *Department of Mathematics and Statistics*
> *University of Southern Maine*
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Prof. Dr. Matthias Kohl
www.stamats.de


From |n||n|te@monkey@@w|th@keybo@rd@ @end|ng |rom gm@||@com  Sun Feb 10 16:22:17 2019
From: |n||n|te@monkey@@w|th@keybo@rd@ @end|ng |rom gm@||@com (Aaron Lun)
Date: Sun, 10 Feb 2019 15:22:17 +0000
Subject: [R] Unexpected errors in sparse Matrix arithmetic with zero-length
 dimensions
Message-ID: <1549812137.3935.16.camel@gmail.com>

Dear list,

The Matrix package exhibits some unexpected behaviour in its arithmetic
methods for the edge case of a sparse matrix with a dimension of zero
length. The example below is the most illustrative, where changing the
contents of the vector causes the subtraction to fail for a sparse
matrix with no columns:?
????
> library(Matrix)
> x <- rsparsematrix(10, 0, density=0.1)
>?
> x - rep(1, nrow(x)) # OK?
> x - rep(0, nrow(x)) # fails
Error in .Ops.recycle.ind(e1, len = l2) :?
? vector too long in Matrix - vector operation

This is presumably because Matrix recognizes that subtraction of zero
preserves sparsity and thus uses a different method in the second case.
However, I would have expected subtraction of a zero vector to work if
subtraction of a general vector is permissible. This is accompanied by
a host of related errors for sparsity-preserving arithmetic:

> x / 1 # OK
> x / rep(1, nrow(x)) # fails?
Error in .Ops.recycle.ind(e1, len = l2) :?
? vector too long in Matrix - vector operation
>?
> x * 1 # OK
> x * rep(1, nrow(x)) # fails
Error in .Ops.recycle.ind(e1, len = l2) :?
? vector too long in Matrix - vector operation
??????
A different error is raised for a sparse matrix with no rows:

> y <- rsparsematrix(0, 10, density=0.1)
>?
> y - numeric(1) # OK
> y - numeric(0) # fails
Error in y - numeric(0) : <Matrix> - numeric(0) is undefined

I would have expected to just get 'y' back, given that the same code
works fine for other Matrix classes:

> z <- as(y, "dgeMatrix")
> z - numeric(0) # OK

Correct behaviour of zero-dimension sparse matrices is practically
important to me; I develop a number of packages that rely on Matrix
classes, and in those packages, I do a lot of unit testing with zero-
dimension inputs. This ensures that my functions return sensible
results or fail gracefully in edge cases that might be encountered by
users. The current behaviour of sparse Matrix arithmetic causes my unit
tests to fail for no (obvious) good reason.

Best,

Aaron Lun

Research Associate
CRUK Cambridge Institute
University of Cambridge

> sessionInfo()
R Under development (unstable) (2019-01-14 r75992)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 16.04.5 LTS

Matrix products: default
BLAS: /home/cri.camres.org/lun01/Software/R/trunk/lib/libRblas.so
LAPACK: /home/cri.camres.org/lun01/Software/R/trunk/lib/libRlapack.so

locale:
?[1] LC_CTYPE=en_GB.UTF-8???????LC_NUMERIC=C??????????????
?[3] LC_TIME=en_GB.UTF-8????????LC_COLLATE=en_GB.UTF-8????
?[5] LC_MONETARY=en_GB.UTF-8????LC_MESSAGES=en_GB.UTF-8???
?[7] LC_PAPER=en_GB.UTF-8???????LC_NAME=C?????????????????
?[9] LC_ADDRESS=C???????????????LC_TELEPHONE=C????????????
[11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C???????

attached base packages:
[1] stats?????graphics??grDevices
utils?????datasets??methods???base?????

other attached packages:
[1] Matrix_1.2-15

loaded via a namespace (and not attached):
[1] compiler_3.6.0??grid_3.6.0??????lattice_0.20-38


From d@m|ro1089 @end|ng |rom gm@||@com  Sun Feb 10 21:39:16 2019
From: d@m|ro1089 @end|ng |rom gm@||@com (Diego Miro)
Date: Sun, 10 Feb 2019 18:39:16 -0200
Subject: [R] (no subject)
Message-ID: <CAKZ33Q9x=BKvXeEL40TgbPo=2VHgMnggj_1Ox=38dxnuQ0bMXg@mail.gmail.com>

4 xxx ff

	[[alternative HTML version deleted]]


From h@@@n@d|w@n @end|ng |rom gm@||@com  Sun Feb 10 21:41:04 2019
From: h@@@n@d|w@n @end|ng |rom gm@||@com (Hasan Diwan)
Date: Sun, 10 Feb 2019 12:41:04 -0800
Subject: [R] (no subject)
In-Reply-To: <CAKZ33Q9x=BKvXeEL40TgbPo=2VHgMnggj_1Ox=38dxnuQ0bMXg@mail.gmail.com>
References: <CAKZ33Q9x=BKvXeEL40TgbPo=2VHgMnggj_1Ox=38dxnuQ0bMXg@mail.gmail.com>
Message-ID: <CAP+bYWC78_VeRVOVf395=pMD31yOHnHFFxPxRg781gBuVWiq6g@mail.gmail.com>

This is spam, right? -- H

On Sun, 10 Feb 2019 at 12:36, Diego Miro <d.miro1089 at gmail.com> wrote:

> 4 xxx ff
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
OpenPGP:
https://sks-keyservers.net/pks/lookup?op=get&search=0xFEBAD7FFD041BBA1
If you wish to request my time, please do so using
*bit.ly/hd1AppointmentRequest
<http://bit.ly/hd1AppointmentRequest>*.
Si vous voudrais faire connnaisance, allez a *bit.ly/hd1AppointmentRequest
<http://bit.ly/hd1AppointmentRequest>*.

<https://sks-keyservers.net/pks/lookup?op=get&search=0xFEBAD7FFD041BBA1>Sent
from my mobile device
Envoye de mon portable

	[[alternative HTML version deleted]]


From or|o|eb@|t|more @end|ng |rom gm@||@com  Sun Feb 10 21:49:31 2019
From: or|o|eb@|t|more @end|ng |rom gm@||@com (Adrian Johnson)
Date: Sun, 10 Feb 2019 15:49:31 -0500
Subject: [R] (no subject)
Message-ID: <CAL2fYnMbKV5rhjOxBC1a1tprTa_6duBvxjAUVg2ar0fnW3appA@mail.gmail.com>

Dear group,

I have two large matrices.

Matrix one: is 24776 x 76 (example toy1 dput object given below)

Matrix two: is 12913 x 76 (example toy2 dput object given below)

Column names of both matrices are identical.

My aim is:

a. Take each row of toy2 and transform vector into UP (>0)  and DN (
<0 ) categories. (kc)
b  Test association between kc and every row of toy1.

My code, given below, although this works but is very slow.

I gave dput objects for toy1, toy2 and result matrix.

Could you suggest/help me how I can make this faster.  Also, how can I
select values in result column that are less than 0.001 (p < 0.001).

Appreciate your help. Thank you.

Code:
===============================================================================



result <- matrix(NA,nrow=nrow(toy1),ncol=nrow(toy2))

rownames(result) <- rownames(toy1)
colnames(result) <- rownames(toy2)

for(i in 1:nrow(toy2)){
for(j in 1:nrow(toy1)){
kx = toy2[i,]
kc <- rep('NC',length(kx))
kc[ kx >0] <- 'UP'
kc[ kx <=0 ] <- 'DN'
xpv <- fisher.test(table(kc,toy1[j,]),simulate.p.value = TRUE)$p.value
result[j,i] <- xpv
}
}

===============================================================================


===============================================================================


> dput(toy1)
structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1,
-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
-1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1,
-1, -1, -1, -1, -1), .Dim = c(10L, 7L), .Dimnames = list(c("ACAP3",
"ACTRT2", "AGRN", "ANKRD65", "ATAD3A", "ATAD3B", "ATAD3C", "AURKAIP1",
"B3GALT6", "C1orf159"), c("a", "b", "c", "d", "e", "f", "g")))



> dput(toy2)
structure(c(-0.242891119688613, -0.0514058216682132, 0.138447212993773,
-0.312576648033122, 0.271489918720452, -0.281196468299486, -0.0407160143344565,
-0.328353812845287, 0.151667836674511, 0.408596843743938, -0.049351944902924,
0.238586287349249, 0.200571558784821, -0.0737604184858411, 0.245971526254877,
0.24740263959845, -0.161528943131908, 0.197521973013793, 0.0402668125708444,
0.376323735212088, 0.0731550871764204, 0.385270176969893, 0.28953042756208,
0.062587289401188, -0.281187168932979, -0.0202298984561554, -0.0848696970309447,
0.0349676726358973, -0.520484215644868, -0.481991414222996,
-0.00698099201388211,
0.135503878341873, 0.156983081312087, 0.320223832092661, 0.34582193394074,
0.0844455960468667, -0.157825604090972, 0.204758250510969, 0.261796072978612,
-0.19510450641405, 0.43196474472874, -0.211155577453175, -0.0921641871215187,
0.420950361292263, 0.390261862151936, -0.422273930504427, 0.344653684951627,
0.0378273248838503, 0.197782027324611, 0.0963124876309569, 0.332093167080656,
0.128036554821915, -0.41338065859335, -0.409470440033177, 0.371490567256253,
-0.0912549189140141, -0.247451812684234, 0.127741739114639, 0.0856254238844557,
0.515282940316031, -0.25675759521248, 0.333943163209869, 0.604141413840881,
0.0824942299510931, -0.179605710473021, -0.275604207054643, -0.113251154591898,
0.172897837449258, -0.329808795076691, -0.239255324324506), .Dim = c(10L,
7L), .Dimnames = list(c("chr5q23", "chr16q24", "chr8q24", "chr13q11",
"chr7p21", "chr10q23", "chr13q13", "chr10q21", "chr1p13", "chrxp21"
), c("a", "b", "c", "d", "e", "f", "g")))
>


> dput(result)
structure(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.532733633183408,
0.511244377811094, 0.528235882058971, 0.526736631684158, 0.51424287856072,
0.530734632683658, 0.513243378310845, 0.533233383308346, 0.542228885557221,
0.517241379310345, 0.532733633183408, 0.521739130434783, 0.529235382308846,
0.530234882558721, 0.548725637181409, 0.525737131434283, 0.527236381809095,
0.532733633183408, 0.530234882558721, 0.520739630184908, 0.15592203898051,
0.142928535732134, 0.140929535232384, 0.150924537731134, 0.160419790104948,
0.139430284857571, 0.152923538230885, 0.146426786606697, 0.149425287356322,
0.145427286356822, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.282358820589705,
0.293853073463268, 0.262868565717141, 0.290854572713643, 0.276861569215392,
0.288855572213893, 0.282358820589705, 0.292853573213393, 0.286356821589205,
0.271364317841079, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1), .Dim = c(10L, 10L), .Dimnames = list(c("ACAP3",
"ACTRT2", "AGRN", "ANKRD65", "ATAD3A", "ATAD3B", "ATAD3C", "AURKAIP1",
"B3GALT6", "C1orf159"), c("chr5q23", "chr16q24", "chr8q24", "chr13q11",
"chr7p21", "chr10q23", "chr13q13", "chr10q21", "chr1p13", "chrxp21"
)))


From @boue|m@k@r|m1962 @end|ng |rom gm@||@com  Sun Feb 10 23:44:25 2019
From: @boue|m@k@r|m1962 @end|ng |rom gm@||@com (AbouEl-Makarim Aboueissa)
Date: Sun, 10 Feb 2019 17:44:25 -0500
Subject: [R] 
 How to rum Multiple ANOVA and Multiple T-test between the same
 groups?
In-Reply-To: <584da88d-1fb8-aae2-efda-e0e9e34beffb@stamats.de>
References: <CAE9stmcNuE-N-8KogigRysV8Qi=BQVtg80+eY3UK9uPFi-bZbA@mail.gmail.com>
 <584da88d-1fb8-aae2-efda-e0e9e34beffb@stamats.de>
Message-ID: <CAE9stmc=d7HmSy=hMc_U+_FMjBExuQsBTcNS3zoa8ePs=Q9sKA@mail.gmail.com>

Dear Prof Kohl:


I am trying to install the "genefilter" package, but I got the following
error message(s). I am not sure which R version should be used for this
package.


*For R.3.5.2:*

> install.packages("genefilter")
Installing package into ?C:/Users/aaboueissa/Documents/R/win-library/3.5?
(as ?lib? is unspecified)
Warning message:
package ?genefilter? is not available (for R version 3.5.2)


*For R.3.3.2:*

> install.packages("Bioconductor")
Installing package into ?C:/Users/aaboueissa/Documents/R/win-library/3.3?
(as ?lib? is unspecified)
--- Please select a CRAN mirror for use in this session ---
Warning message:
package ?Bioconductor? is not available (for R version 3.3.2)


with many thanks
abou
______________________


*AbouEl-Makarim Aboueissa, PhD*

*Professor, Statistics and Data Science*
*Graduate Coordinator*

*Department of Mathematics and Statistics*
*University of Southern Maine*



On Sun, Feb 10, 2019 at 10:52 AM Prof. Dr. Matthias Kohl <
Matthias.Kohl at stamats.de> wrote:

> Have a look at Bioconductor package genefilter, especially functions
> colttests and colFtests.
> Best Matthias
>
> Am 10.02.19 um 10:35 schrieb AbouEl-Makarim Aboueissa:
> > Dear All: good morning
> >
> >
> >
> >
> >
> > *Re:* How to rum Multiple ANOVA and Multiple T-test between the same
> groups.
> >
> >
> >
> > Your help will be highly appreciated.
> >
> >
> >
> >
> >
> > *1.*  is there a way to run multiple t-tests on different variables
> between
> > the same two groups.
> >
> >
> >
> >
> >
> > *Data for t-tests:*
> >
> >
> >
> > The data frame ?dataTtest?  has 5 variables (x1,x2,x3,x4,x5) and one
> factor
> > (factor1) with 2 levels (group1, group2).
> >
> >
> >
> >
> >
> > x1<-rnorm(20,1,1)
> >
> > x2<-rnorm(20,2,1)
> >
> > x3<-rnorm(20,3,1)
> >
> > x4<-rnorm(20,4,1)
> >
> > x5<-rnorm(20,5,1)
> >
> > factor1<-rep(c("group1", "group2"), each = 10)
> >
> > dataTtest<-data.frame(x1,x2,x3,x4,x5,factor1)
> >
> > dataTtest
> >
> >
> >
> >
> >
> >
> >
> >
> >
> > *2.* is there a way to run *multiple ANOVA* and multiple comparisons
> *Tukey
> > tests* on different variables between the same groups.
> >
> >
> >
> >
> >
> > *Data for ANOVA tests:*
> >
> >
> >
> > The data frame ?dataANOVA?  has 6 variables (x1,x2,x3,x4,x5,x6) and one
> > factor (factor2) with 5 levels (group1, group2, group3, group4, group5).
> >
> >
> >
> >
> >
> >
> >
> > x1<-rnorm(40,1,1)
> >
> > x2<-rnorm(40,2,1)
> >
> > x3<-rnorm(40,3,1)
> >
> > x4<-rnorm(40,4,1)
> >
> > x5<-rnorm(40,5,1)
> >
> > x6<-rnorm(40,6,1)
> >
> > factor2<-rep(c("group1", "group2", "group3", "group4", "group5"), each =
> 8)
> >
> > dataANOVA<-data.frame(x1,x2,x3,x4,x5,x6,factor2)
> >
> > dataANOVA
> >
> >
> >
> >
> >
> > with many thanks
> >
> > abou
> > ______________________
> >
> >
> > *AbouEl-Makarim Aboueissa, PhD*
> >
> > *Professor, Statistics and Data Science*
> > *Graduate Coordinator*
> >
> > *Department of Mathematics and Statistics*
> > *University of Southern Maine*
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> --
> Prof. Dr. Matthias Kohl
> www.stamats.de
>

	[[alternative HTML version deleted]]


From bor|@@@te|pe @end|ng |rom utoronto@c@  Mon Feb 11 02:57:16 2019
From: bor|@@@te|pe @end|ng |rom utoronto@c@ (Boris Steipe)
Date: Mon, 11 Feb 2019 01:57:16 +0000
Subject: [R] 
 How to rum Multiple ANOVA and Multiple T-test between the same
 groups?
In-Reply-To: <CAE9stmc=d7HmSy=hMc_U+_FMjBExuQsBTcNS3zoa8ePs=Q9sKA@mail.gmail.com>
References: <CAE9stmcNuE-N-8KogigRysV8Qi=BQVtg80+eY3UK9uPFi-bZbA@mail.gmail.com>
 <584da88d-1fb8-aae2-efda-e0e9e34beffb@stamats.de>
 <CAE9stmc=d7HmSy=hMc_U+_FMjBExuQsBTcNS3zoa8ePs=Q9sKA@mail.gmail.com>
Message-ID: <FB997165-06B8-4C4A-8447-B200175A1481@utoronto.ca>

You need to spend more time getting clear on the fundamentals, what the Bioconductor project is and why its packages are useful in our domain. Bioconductor packages are not installed with the install.packages() function; that is for packages on CRAN. Instead, you use ...

  install.packages("BiocManager")     # BiocManagere IS hosted on CRAN
  BiocManager::install("genefilter")  # BiocManagerinstalls "genefilter" from bioconductor.org

Now, _do_ read the documentation, and pay special attention to the vignettes!

https://cran.r-project.org/web/packages/BiocManager/index.html
https://bioconductor.org/packages/release/bioc/html/genefilter.html



B.




> On 2019-02-10, at 17:44, AbouEl-Makarim Aboueissa <abouelmakarim1962 at gmail.com> wrote:
> 
> Dear Prof Kohl:
> 
> 
> I am trying to install the "genefilter" package, but I got the following
> error message(s). I am not sure which R version should be used for this
> package.
> 
> 
> *For R.3.5.2:*
> 
>> install.packages("genefilter")
> Installing package into ?C:/Users/aaboueissa/Documents/R/win-library/3.5?
> (as ?lib? is unspecified)
> Warning message:
> package ?genefilter? is not available (for R version 3.5.2)
> 
> 
> *For R.3.3.2:*
> 
>> install.packages("Bioconductor")
> Installing package into ?C:/Users/aaboueissa/Documents/R/win-library/3.3?
> (as ?lib? is unspecified)
> --- Please select a CRAN mirror for use in this session ---
> Warning message:
> package ?Bioconductor? is not available (for R version 3.3.2)
> 
> 
> with many thanks
> abou
> ______________________
> 
> 
> *AbouEl-Makarim Aboueissa, PhD*
> 
> *Professor, Statistics and Data Science*
> *Graduate Coordinator*
> 
> *Department of Mathematics and Statistics*
> *University of Southern Maine*
> 
> 
> 
> On Sun, Feb 10, 2019 at 10:52 AM Prof. Dr. Matthias Kohl <
> Matthias.Kohl at stamats.de> wrote:
> 
>> Have a look at Bioconductor package genefilter, especially functions
>> colttests and colFtests.
>> Best Matthias
>> 
>> Am 10.02.19 um 10:35 schrieb AbouEl-Makarim Aboueissa:
>>> Dear All: good morning
>>> 
>>> 
>>> 
>>> 
>>> 
>>> *Re:* How to rum Multiple ANOVA and Multiple T-test between the same
>> groups.
>>> 
>>> 
>>> 
>>> Your help will be highly appreciated.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> *1.*  is there a way to run multiple t-tests on different variables
>> between
>>> the same two groups.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> *Data for t-tests:*
>>> 
>>> 
>>> 
>>> The data frame ?dataTtest?  has 5 variables (x1,x2,x3,x4,x5) and one
>> factor
>>> (factor1) with 2 levels (group1, group2).
>>> 
>>> 
>>> 
>>> 
>>> 
>>> x1<-rnorm(20,1,1)
>>> 
>>> x2<-rnorm(20,2,1)
>>> 
>>> x3<-rnorm(20,3,1)
>>> 
>>> x4<-rnorm(20,4,1)
>>> 
>>> x5<-rnorm(20,5,1)
>>> 
>>> factor1<-rep(c("group1", "group2"), each = 10)
>>> 
>>> dataTtest<-data.frame(x1,x2,x3,x4,x5,factor1)
>>> 
>>> dataTtest
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> *2.* is there a way to run *multiple ANOVA* and multiple comparisons
>> *Tukey
>>> tests* on different variables between the same groups.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> *Data for ANOVA tests:*
>>> 
>>> 
>>> 
>>> The data frame ?dataANOVA?  has 6 variables (x1,x2,x3,x4,x5,x6) and one
>>> factor (factor2) with 5 levels (group1, group2, group3, group4, group5).
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> x1<-rnorm(40,1,1)
>>> 
>>> x2<-rnorm(40,2,1)
>>> 
>>> x3<-rnorm(40,3,1)
>>> 
>>> x4<-rnorm(40,4,1)
>>> 
>>> x5<-rnorm(40,5,1)
>>> 
>>> x6<-rnorm(40,6,1)
>>> 
>>> factor2<-rep(c("group1", "group2", "group3", "group4", "group5"), each =
>> 8)
>>> 
>>> dataANOVA<-data.frame(x1,x2,x3,x4,x5,x6,factor2)
>>> 
>>> dataANOVA
>>> 
>>> 
>>> 
>>> 
>>> 
>>> with many thanks
>>> 
>>> abou
>>> ______________________
>>> 
>>> 
>>> *AbouEl-Makarim Aboueissa, PhD*
>>> 
>>> *Professor, Statistics and Data Science*
>>> *Graduate Coordinator*
>>> 
>>> *Department of Mathematics and Statistics*
>>> *University of Southern Maine*
>>> 
>>>      [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
>> --
>> Prof. Dr. Matthias Kohl
>> www.stamats.de
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From R|m@@ec @end|ng |rom hotm@||@com  Sun Feb 10 17:59:41 2019
From: R|m@@ec @end|ng |rom hotm@||@com (Rima El-zein)
Date: Sun, 10 Feb 2019 16:59:41 +0000
Subject: [R] Recreate for loop without using for loop
Message-ID: <VI1PR0301MB2207439A81621C7457601AA0F06B0@VI1PR0301MB2207.eurprd03.prod.outlook.com>

Hi.



Can someone please help me recreate this code without using a for loop? Idk if I'm supposed to use a map function or something else.



qprob <- function(pp) {

  qq <- 1 - pp -1

  stotal <- 0.0

  for (i in 1:length(pp))

    stotal <- stotal + pp[i] * prod(qq[-i])

  return(stotal)

}

Best regards,
Rima


Sendt fra Mail<https://go.microsoft.com/fwlink/?LinkId=550986> til Windows 10


	[[alternative HTML version deleted]]


From ro@@@|@mo|den @end|ng |rom gm@||@com  Sun Feb 10 18:06:38 2019
From: ro@@@|@mo|den @end|ng |rom gm@||@com (Ross Molden)
Date: Sun, 10 Feb 2019 17:06:38 +0000
Subject: [R] Merge multiple google xls files
Message-ID: <7C225BC9-6647-464D-B3BC-FDD2FC815C7A@gmail.com>


I am trying to merge a list of .xls files in google drive. I have now managed to create a list of all the files I need, but for some reason I still can't manage to merge them, this is the code I have so far:

library(googledrive) inputfiles <- drive_ls(path = "Email It In", pattern = "*PDOL_dataexport", n_max = 50)

library(readxl) df.list<- lapply(inputfiles,function(x) read_xls(x)) library(dplyr) consolidated_data<-bind_rows(df.list)

The second part of the code throws up the following error: 

Error: path must be a string 

I must be entering the path (inputfiles) incorrectly for lapply, can someone please help?
	[[alternative HTML version deleted]]


From donghw79 @end|ng |rom hotm@||@com  Sun Feb 10 18:31:40 2019
From: donghw79 @end|ng |rom hotm@||@com (Hongwei Dong)
Date: Sun, 10 Feb 2019 17:31:40 +0000
Subject: [R] evaluate model performance when using 'withReplicates' in
 'survey' package
Message-ID: <BYAPR02MB5767A5562EFB680D0F002500DC6B0@BYAPR02MB5767.namprd02.prod.outlook.com>

Dear all,

I am using the 'withReplicates' function in the 'survey' package to estimate a zero-inflated Poisson model. The 'withReplicates' function gives me the coefficients, but no model performance indicators (e.g. log likelihood, pseudo R-squared, etc). Is there a way that I can evaluate the goodness-of-fit of a model when using 'withReplicates'?

My code looks like this:

zip_model<-withReplicates(design=mydesign, scale.weights=TRUE, quote(
                                                   coef(zeroinfl(Y ~ X1 + X2 + X3 | X1 + X2, weights=.weights))
                                                  ))
zip_model

I am aware that the 'fit.svyglm' function in the 'poliscidata' package can calculate R-squared for models that are estimated via 'svyglm'. Unfortunately, the 'svyglm' function estimates regular Poisson models only and cannot handle zero-inflated Poisson models.

Any advice is appreciated.

Best
Gary

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon Feb 11 05:23:28 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sun, 10 Feb 2019 20:23:28 -0800
Subject: [R] Recreate for loop without using for loop
In-Reply-To: <VI1PR0301MB2207439A81621C7457601AA0F06B0@VI1PR0301MB2207.eurprd03.prod.outlook.com>
References: <VI1PR0301MB2207439A81621C7457601AA0F06B0@VI1PR0301MB2207.eurprd03.prod.outlook.com>
Message-ID: <D48682E6-7D2E-4CA6-B0AA-3A4009220A91@dcn.davis.ca.us>

There is a no-homework policy stated in the Posting Guide.

On February 10, 2019 8:59:41 AM PST, Rima El-zein <Rima.ec at hotmail.com> wrote:
>Hi.
>
>
>
>Can someone please help me recreate this code without using a for loop?
>Idk if I'm supposed to use a map function or something else.
>
>
>
>qprob <- function(pp) {
>
>  qq <- 1 - pp -1
>
>  stotal <- 0.0
>
>  for (i in 1:length(pp))
>
>    stotal <- stotal + pp[i] * prod(qq[-i])
>
>  return(stotal)
>
>}
>
>Best regards,
>Rima
>
>
>Sendt fra Mail<https://go.microsoft.com/fwlink/?LinkId=550986> til
>Windows 10
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From r@turner @end|ng |rom @uck|@nd@@c@nz  Mon Feb 11 05:37:29 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Mon, 11 Feb 2019 17:37:29 +1300
Subject: [R] [FORGED]  Recreate for loop without using for loop
In-Reply-To: <VI1PR0301MB2207439A81621C7457601AA0F06B0@VI1PR0301MB2207.eurprd03.prod.outlook.com>
References: <VI1PR0301MB2207439A81621C7457601AA0F06B0@VI1PR0301MB2207.eurprd03.prod.outlook.com>
Message-ID: <13f5b505-6928-64d1-ca65-237991ed0b49@auckland.ac.nz>


This list has a "no homework" policy.

On 2/11/19 5:59 AM, Rima El-zein wrote:
> Hi.
> 
> 
> 
> Can someone please help me recreate this code without using a for loop? Idk if I'm supposed to use a map function or something else.

Supposed by whom?  What do you mean by "map function"?

> qprob <- function(pp) {
> 
>    qq <- 1 - pp -1
> 
>    stotal <- 0.0
> 
>    for (i in 1:length(pp))
> 
>      stotal <- stotal + pp[i] * prod(qq[-i])
> 
>    return(stotal)
> 
> }

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From murrjohn321 @end|ng |rom gm@||@com  Sun Feb 10 15:57:13 2019
From: murrjohn321 @end|ng |rom gm@||@com (Murris Johnson)
Date: Sun, 10 Feb 2019 14:57:13 +0000
Subject: [R] I'm Working On A Data Security Article - Quick Question
Message-ID: <c357e2a78eef4d6b941ca2156d1977df_CAG9bbvn5E4m4bZ3mzr_VeiViv=74+pS+ZGmaFgWyfCdA+g06-g@mail.gmail.com>



Hi,


I have a quick question for you about a page on your site that briefly 
mentions a data security topic, https://stat.ethz.ch/pipermail/r-help/2008-May/162561.html. 


I plan on writing a few guest authored articles in the next month or so 
that talk about data security & breaches, are you ok with me possibly 
linking to your above-mentioned webpage? 


I'm not entirely sure which sites I may link to or reference in the 
articles yet, but I'd like to have a few different options to choose 
from... so please let me know what you think when you have a moment to let 
me know. Whatever you decide, thanks for your time and I don't expect a 
response if you're not interested. Thanks.


Murris



PS: If this question was a disturbance, click this 
<http://ec2-52-26-194-35.us-west-2.compute.amazonaws.com/x/u?u=35c7bbaa-48f6-4968-a738-87d4e024a4b9> and I won't email you again.

	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Mon Feb 11 10:26:13 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Mon, 11 Feb 2019 22:26:13 +1300
Subject: [R] [FORGED] I'm Working On A Data Security Article - Quick
 Question
In-Reply-To: <c357e2a78eef4d6b941ca2156d1977df_CAG9bbvn5E4m4bZ3mzr_VeiViv=74+pS+ZGmaFgWyfCdA+g06-g@mail.gmail.com>
References: <c357e2a78eef4d6b941ca2156d1977df_CAG9bbvn5E4m4bZ3mzr_VeiViv=74+pS+ZGmaFgWyfCdA+g06-g@mail.gmail.com>
Message-ID: <35281b52-bf56-41de-1966-204b49afad91@auckland.ac.nz>

On 2/11/19 3:57 AM, Murris Johnson wrote:
> 
> 
> Hi,
> 
> 
> I have a quick question for you about a page on your site that briefly
> mentions a data security topic, https://stat.ethz.ch/pipermail/r-help/2008-May/162561.html.
> 
> 
> I plan on writing a few guest authored articles in the next month or so
> that talk about data security & breaches, are you ok with me possibly
> linking to your above-mentioned webpage?
> 
> 
> I'm not entirely sure which sites I may link to or reference in the
> articles yet, but I'd like to have a few different options to choose
> from... so please let me know what you think when you have a moment to let
> me know. Whatever you decide, thanks for your time and I don't expect a
> response if you're not interested. Thanks.

Are you pulling our legs?

Personally I haven't got a clue what you are asking.

Also, please note that r-help is a world wide "community" with hundreds 
of thousands of members.  (I am but one of this large and unruly mob.) 
It is not a single entity or "site".  (There is a formal organisation 
that runs the R show, but you are not really addressing this formal 
organisation.)

The URL that you refer to is a joke!  Apparently some idiotic Windoze 
antiviral software once identified the R executable for Windoze as being 
a virus.  This is not to taken seriously.

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From em@n|@m@||@92 @end|ng |rom gm@||@com  Mon Feb 11 10:51:40 2019
From: em@n|@m@||@92 @end|ng |rom gm@||@com (=?UTF-8?B?2KXZitmF2KfZhiDYpdiz2YXYp9i52YrZhCDZhdit2YXYrw==?=)
Date: Mon, 11 Feb 2019 11:51:40 +0200
Subject: [R] Question about bindata lib in high dimensions
Message-ID: <CAFiZHK6ir3NR9P4aK41UPNnoUwrB9ZMS74xJqr20gH1wKTFP+A@mail.gmail.com>

I need to simulate data for 2000 binary variables given a vector of
marginal probabilities and a correlation matrix. I used bindata library,
but it give me

 Not all probabilities are between 0 and 1.
Error in Element ( i , j ): Admissible values are in [.....].
Error in commonprob2sigma(commonprob, simulvals) :
  Matrix commonprob not admissible.

 and I tried to get the elements within range but still have the same problem

How can I fix the correlation matrix or how to track error ??

Thanks
Eman

	[[alternative HTML version deleted]]


From bhh @end|ng |rom x@4@||@n|  Mon Feb 11 10:53:21 2019
From: bhh @end|ng |rom x@4@||@n| (Berend Hasselman)
Date: Mon, 11 Feb 2019 10:53:21 +0100
Subject: [R] I'm Working On A Data Security Article - Quick Question
In-Reply-To: <c357e2a78eef4d6b941ca2156d1977df_CAG9bbvn5E4m4bZ3mzr_VeiViv=74+pS+ZGmaFgWyfCdA+g06-g@mail.gmail.com>
References: <c357e2a78eef4d6b941ca2156d1977df_CAG9bbvn5E4m4bZ3mzr_VeiViv=74+pS+ZGmaFgWyfCdA+g06-g@mail.gmail.com>
Message-ID: <EEA027BD-C6FD-4CEE-A556-56E882ED3BC6@xs4all.nl>


Just read the complete thread. 
That will clarify.

Nothing wrong with R or RGUI; the virus checker is wrong.
regards,

Berend Hasselman

> On 10 Feb 2019, at 15:57, Murris Johnson <murrjohn321 at gmail.com> wrote:
> 
> 
> 
> Hi,
> 
> 
> I have a quick question for you about a page on your site that briefly 
> mentions a data security topic, https://stat.ethz.ch/pipermail/r-help/2008-May/162561.html. 
> 
> 
> I plan on writing a few guest authored articles in the next month or so 
> that talk about data security & breaches, are you ok with me possibly 
> linking to your above-mentioned webpage? 
> 
> 
> I'm not entirely sure which sites I may link to or reference in the 
> articles yet, but I'd like to have a few different options to choose 
> from... so please let me know what you think when you have a moment to let 
> me know. Whatever you decide, thanks for your time and I don't expect a 
> response if you're not interested. Thanks.
> 
> 
> Murris
> 
> 
> 
> PS: If this question was a disturbance, click this 
> <http://ec2-52-26-194-35.us-west-2.compute.amazonaws.com/x/u?u=35c7bbaa-48f6-4968-a738-87d4e024a4b9> and I won't email you again.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From er|cjberger @end|ng |rom gm@||@com  Mon Feb 11 11:22:40 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Mon, 11 Feb 2019 12:22:40 +0200
Subject: [R] Question about bindata lib in high dimensions
In-Reply-To: <CAFiZHK6ir3NR9P4aK41UPNnoUwrB9ZMS74xJqr20gH1wKTFP+A@mail.gmail.com>
References: <CAFiZHK6ir3NR9P4aK41UPNnoUwrB9ZMS74xJqr20gH1wKTFP+A@mail.gmail.com>
Message-ID: <CAGgJW75dg6VczWn6J7XTSPPZMg3TOqtXZeLa1FSQsFeXJbt0KQ@mail.gmail.com>

Hi Eman,
It helps if you create a small example that reproduces the problem and then
post the code with your question.
This will help people determine what is causing the problem.

Best,
Eric


?On Mon, Feb 11, 2019 at 11:52 AM ?????? ??????? ?????? <
emanismail.92 at gmail.com> wrote:?

> I need to simulate data for 2000 binary variables given a vector of
> marginal probabilities and a correlation matrix. I used bindata library,
> but it give me
>
>  Not all probabilities are between 0 and 1.
> Error in Element ( i , j ): Admissible values are in [.....].
> Error in commonprob2sigma(commonprob, simulvals) :
>   Matrix commonprob not admissible.
>
>  and I tried to get the elements within range but still have the same
> problem
>
> How can I fix the correlation matrix or how to track error ??
>
> Thanks
> Eman
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@rco@be@o48 @end|ng |rom gm@||@com  Mon Feb 11 11:24:54 2019
From: m@rco@be@o48 @end|ng |rom gm@||@com (Marco Besozzi)
Date: Mon, 11 Feb 2019 11:24:54 +0100
Subject: [R] Siegel nonparametric regression / mblm package
Message-ID: <CAO=Vm8qoXsPR+WTtX=wepHj69n6fWiVW86pdvvNmhYTg-xUYqw@mail.gmail.com>

I employed the "galton" set of data included in the package "psych". With
the package "mblm" I obtained the Theil-Sen nonparametric regression and
the Siegel non parametric regression, and compared them with the ordinary
least square regression line.
The results of standard regression and Theil-Sen regression are practically
identical. But the Siegel regression seems to have a bias that I cannot
understand. May I ask for a possible explanation? The bias may be related
to the number of ties in the set of data? Here's the code and the image.

Best regards.

Marco Besozzi
# Theil-Sen and Siegel nonparametric regression with package mblm
# comparison with ordinary least squares (parametric) regression
# on galton set of data included in the package psych
#
library(psych)
attach(galton)
library(mblm)
#
reglin_yx <- lm(child ~ parent, data=galton) # ordinary least squares
(parametric) regression
a_yx <- reglin_yx$coefficients[1] # intercept a
b_yx <- reglin_yx$coefficients[2] # slope b
#
regnonTS <- mblm(child ~ parent, data=galton, repeated=FALSE) # Theil-Sen
nonparametric regression (wait a few minutes!)
a_TS <- regnonTS$coefficients[1] # intercept a
b_TS <- regnonTS$coefficients[2] # slope b
#
regnonS = mblm(child ~ parent, data=galton, repeated=TRUE) # Siegel
nonparametric regression
a_S <- regnonS$coefficients[1] # intercept a
b_S <- regnonS$coefficients[2] # slope b
#
# xy plot of data and regression lines
#
windows() # open a new window
plot(parent, child, xlim = c(60,80), ylim = c(60,80), pch=1, xlab="Parent
heigt (inch)", ylab="Chile height (inch)", main="Regression lines
comparison", cex.main = 0.9) # data plot
abline(a_yx, b_yx, col="green", lty=1) # ordinary least squares
(parametric) regression line
abline(a_TS, b_TS, col="blue", lty=1) # Theil-Sen nonparametric regression
line
abline(a_S, b_S, col="red", lty=1) # Siegel nonparametric regression
legend(60, 80, legend=c("Ordinary least squares regression", "Theil-Sen
nonparametric regression","Siegel nonparametric regression"),
col=c("green", "blue", "red"), lty=c(4,4,1), cex=0.8) # add a legend
#

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Siegel.PNG
Type: image/png
Size: 81522 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190211/ac668c7b/attachment.png>

From @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org  Mon Feb 11 11:36:56 2019
From: @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org (Spencer Graves)
Date: Mon, 11 Feb 2019 04:36:56 -0600
Subject: [R] I'm Working On A Data Security Article - Quick Question
In-Reply-To: <EEA027BD-C6FD-4CEE-A556-56E882ED3BC6@xs4all.nl>
References: <c357e2a78eef4d6b941ca2156d1977df_CAG9bbvn5E4m4bZ3mzr_VeiViv=74+pS+ZGmaFgWyfCdA+g06-g@mail.gmail.com>
 <EEA027BD-C6FD-4CEE-A556-56E882ED3BC6@xs4all.nl>
Message-ID: <c281fa08-d4e3-6a28-b58c-677b03022c13@effectivedefense.org>

 ????? To your question about, "are you ok with me possibly linking to" 
a 2008 thread on r-help subject: "[R] R is a virus, spyware or malware 
(gasp!)", I think that would be considered a friendly gesture PROVIDED 
you honestly portrayed the conclusion that any checker for malicious 
software that flags R or RGUI (or RStudio, now, also), is likely wrong:? 
This is free, open-source software with probably millions of users 
worldwide and currently almost 14,000 contributed packages available on 
the Comprehensive R Archive Network (CRAN), and all the code is checked 
on multiple platforms several times per week, including for malicious 
code.? Any such code is isolated and removed.? "CRAN Repository 
Policy"[1] includes, "The code and examples provided in a package should 
never do anything which might be regarded as malicious or anti-social", 
and provides several "examples from past experience."


 ????? Hope this helps.
 ????? Spencer Graves


[1] https://cran.r-project.org/web/packages/policies.html


On 2019-02-11 03:53, Berend Hasselman wrote:
> Just read the complete thread.
> That will clarify.
>
> Nothing wrong with R or RGUI; the virus checker is wrong.
> regards,
>
> Berend Hasselman
>
>> On 10 Feb 2019, at 15:57, Murris Johnson <murrjohn321 at gmail.com> wrote:
>>
>>
>>
>> Hi,
>>
>>
>> I have a quick question for you about a page on your site that briefly
>> mentions a data security topic, https://stat.ethz.ch/pipermail/r-help/2008-May/162561.html.
>>
>>
>> I plan on writing a few guest authored articles in the next month or so
>> that talk about data security & breaches, are you ok with me possibly
>> linking to your above-mentioned webpage?
>>
>>
>> I'm not entirely sure which sites I may link to or reference in the
>> articles yet, but I'd like to have a few different options to choose
>> from... so please let me know what you think when you have a moment to let
>> me know. Whatever you decide, thanks for your time and I don't expect a
>> response if you're not interested. Thanks.
>>
>>
>> Murris
>>
>>
>>
>> PS: If this question was a disturbance, click this
>> <http://ec2-52-26-194-35.us-west-2.compute.amazonaws.com/x/u?u=35c7bbaa-48f6-4968-a738-87d4e024a4b9> and I won't email you again.
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From em@n|@m@||@92 @end|ng |rom gm@||@com  Mon Feb 11 12:10:55 2019
From: em@n|@m@||@92 @end|ng |rom gm@||@com (=?UTF-8?B?2KXZitmF2KfZhiDYpdiz2YXYp9i52YrZhCDZhdit2YXYrw==?=)
Date: Mon, 11 Feb 2019 13:10:55 +0200
Subject: [R] Question about bindata lib in high dimensions
In-Reply-To: <CAGgJW75dg6VczWn6J7XTSPPZMg3TOqtXZeLa1FSQsFeXJbt0KQ@mail.gmail.com>
References: <CAFiZHK6ir3NR9P4aK41UPNnoUwrB9ZMS74xJqr20gH1wKTFP+A@mail.gmail.com>
 <CAGgJW75dg6VczWn6J7XTSPPZMg3TOqtXZeLa1FSQsFeXJbt0KQ@mail.gmail.com>
Message-ID: <CAFiZHK7vteBS9qVU4fJYShDYFxiQiajqF1iSnWSxvYrFYmQy7g@mail.gmail.com>

*Here Sample of Code for 10 variables:*
> probs_10 = probs[1:10]
> probs_10
 [1] 9.795272e-01 9.331778e-01 6.764349e-01 9.884067e-02 9.522222e-05
3.499417e-03 2.380556e-05 9.826457e-01 9.628633e-01 8.874949e-01
> corr_mat_10 = corr_mat[1:10,1:10]
> corr_mat_10
               [,1]         [,2]         [,3]         [,4]
[,5]          [,6]          [,7]          [,8]          [,9]       [,10]
 [1,]  1.0000000000  0.540258943  0.209031764  0.047879233 -6.750092e-02
0.0085672057  7.053822e-04  0.7840635867  0.6694665745  0.40604770
 [2,]  0.5402589429  1.000000000  0.386910326  0.088622750 -3.646798e-02
-0.0454879132  1.305637e-03  0.4929722619  0.6613106007  0.61159373
 [3,]  0.2090317635  0.386910326  1.000000000  0.229052428 -1.410984e-02
-0.0434598161 -7.054666e-03  0.1909793458  0.2831488805  0.49337866
 [4,]  0.0478792330  0.088622750  0.229052428  1.000000000 -3.231892e-03
-0.0101705338 -1.615888e-03  0.0434012259  0.0646190283  0.11766286
 [5,] -0.0675009217 -0.036467977 -0.014109837 -0.003231892  1.000000e+00
-0.0005782943 -4.761395e-05 -0.0734320072 -0.0496901947 -0.02740859
 [6,]  0.0085672057 -0.045487913 -0.043459816 -0.010170534 -5.782943e-04
1.0000000000  8.233515e-02  0.0078752345  0.0095061395 -0.03886223
 [7,]  0.0007053822  0.001305637 -0.007054666 -0.001615888 -4.761395e-05
0.0823351499  1.000000e+00  0.0006484086  0.0009582161  0.00173719
 [8,]  0.7840635867  0.492972262  0.190979346  0.043401226 -7.343201e-02
0.0078752345  6.484086e-04  1.0000000000  0.6766830516  0.37325133
 [9,]  0.6694665745  0.661310601  0.283148881  0.064619028 -4.969019e-02
0.0095061395  9.582161e-04  0.6766830516  1.0000000000  0.55158959
[10,]  0.4060477004  0.611593731  0.493378657  0.117662862 -2.740859e-02
-0.0388622278  1.737190e-03  0.3732513255  0.5515895878  1.00000000
> library(bindata)
> r <- rmvbin(10,margprob = probs_10, bincorr = corr_mat_10)
Not all probabilities are between 0 and 1.
Error in Element ( 1 , 5 ): Admissible values are in [ 0 ,
9.5222224867284e-05 ].
Error in Element ( 1 , 7 ): Admissible values are in [ 0 ,
2.3805556216821e-05 ].
Error in Element ( 3 , 7 ): Admissible values are in [ 0 ,
2.3805556216821e-05 ].
Error in Element ( 4 , 7 ): Admissible values are in [ 0 ,
2.3805556216821e-05 ].
Error in Element ( 5 , 7 ): Admissible values are in [ 0 ,
2.3805556216821e-05 ].
Error in Element ( 6 , 7 ): Admissible values are in [ 0 ,
2.3805556216821e-05 ].
Error in Element ( 7 , 3 ): Admissible values are in [ 0 ,
2.3805556216821e-05 ].
Error in Element ( 7 , 4 ): Admissible values are in [ 0 ,
2.3805556216821e-05 ].
Error in Element ( 7 , 5 ): Admissible values are in [ 0 ,
2.3805556216821e-05 ].
Error in Element ( 7 , 6 ): Admissible values are in [ 0 ,
2.3805556216821e-05 ].
Error in Element ( 7 , 8 ): Admissible values are in [ 0 ,
2.3805556216821e-05 ].
Error in Element ( 7 , 9 ): Admissible values are in [ 0 ,
2.3805556216821e-05 ].
Error in Element ( 7 , 10 ): Admissible values are in [ 0 ,
2.3805556216821e-05 ].
Error in Element ( 8 , 7 ): Admissible values are in [ 0 ,
2.3805556216821e-05 ].
Error in Element ( 9 , 7 ): Admissible values are in [ 0 ,
2.3805556216821e-05 ].
Error in Element ( 9 , 10 ): Admissible values are in [ 0.850358273621063 ,
0.887494941319304 ].
Error in commonprob2sigma(commonprob, simulvals) :
  Matrix commonprob not admissible.

*Then I tried to fix wrong values to be within range with the following:*
> corr_mat_10[1,5]=runif(1,min=0,max=9.5222224867284e-05)
> corr_mat_10[1,5]
[1] 7.915036e-05
> corr_mat_10[5,1]=7.915036e-05

*and did the same for all elements but the same error raised:*
> r <- rmvbin(10,margprob = probs_10, bincorr = corr_mat_10)
Error in Element ( 9 , 10 ): Admissible values are in [ 0.850358273621063 ,
0.887494941319304 ].
Error in commonprob2sigma(commonprob, simulvals) :
  Matrix commonprob not admissible.

*Finally I check the value of (9,10) it is within range but the same error
raised :*
> corr_mat_10[9,10]
[1] 0.8793437

I don't know How to fix it?

On Mon, 11 Feb 2019 at 12:23, Eric Berger <ericjberger at gmail.com> wrote:

> Hi Eman,
> It helps if you create a small example that reproduces the problem and
> then post the code with your question.
> This will help people determine what is causing the problem.
>
> Best,
> Eric
>
>
> ?On Mon, Feb 11, 2019 at 11:52 AM ?????? ??????? ?????? <
> emanismail.92 at gmail.com> wrote:?
>
>> I need to simulate data for 2000 binary variables given a vector of
>> marginal probabilities and a correlation matrix. I used bindata library,
>> but it give me
>>
>>  Not all probabilities are between 0 and 1.
>> Error in Element ( i , j ): Admissible values are in [.....].
>> Error in commonprob2sigma(commonprob, simulvals) :
>>   Matrix commonprob not admissible.
>>
>>  and I tried to get the elements within range but still have the same
>> problem
>>
>> How can I fix the correlation matrix or how to track error ??
>>
>> Thanks
>> Eman
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Mon Feb 11 12:37:44 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Mon, 11 Feb 2019 06:37:44 -0500
Subject: [R] [FORGED] I'm Working On A Data Security Article - Quick
 Question
In-Reply-To: <35281b52-bf56-41de-1966-204b49afad91@auckland.ac.nz>
References: <c357e2a78eef4d6b941ca2156d1977df_CAG9bbvn5E4m4bZ3mzr_VeiViv=74+pS+ZGmaFgWyfCdA+g06-g@mail.gmail.com>
 <35281b52-bf56-41de-1966-204b49afad91@auckland.ac.nz>
Message-ID: <0e1380f4-6c51-2056-681e-924356f9534c@gmail.com>

On 11/02/2019 4:26 a.m., Rolf Turner wrote:
> On 2/11/19 3:57 AM, Murris Johnson wrote:
>>
>>
>> Hi,
>>
>>
>> I have a quick question for you about a page on your site that briefly
>> mentions a data security topic, https://stat.ethz.ch/pipermail/r-help/2008-May/162561.html.
>>
>>
>> I plan on writing a few guest authored articles in the next month or so
>> that talk about data security & breaches, are you ok with me possibly
>> linking to your above-mentioned webpage?
>>
>>
>> I'm not entirely sure which sites I may link to or reference in the
>> articles yet, but I'd like to have a few different options to choose
>> from... so please let me know what you think when you have a moment to let
>> me know. Whatever you decide, thanks for your time and I don't expect a
>> response if you're not interested. Thanks.
> 
> Are you pulling our legs?

No, he's a spammer who clearly hasn't read the link he's asking about. 
There are probably thousands of them.

Duncan Murdoch

> 
> Personally I haven't got a clue what you are asking.
> 
> Also, please note that r-help is a world wide "community" with hundreds
> of thousands of members.  (I am but one of this large and unruly mob.)
> It is not a single entity or "site".  (There is a formal organisation
> that runs the R show, but you are not really addressing this formal
> organisation.)
> 
> The URL that you refer to is a joke!  Apparently some idiotic Windoze
> antiviral software once identified the R executable for Windoze as being
> a virus.  This is not to taken seriously.
> 
> cheers,
> 
> Rolf Turner
>


From S@E|||@on @end|ng |rom LGCGroup@com  Mon Feb 11 12:53:30 2019
From: S@E|||@on @end|ng |rom LGCGroup@com (S Ellison)
Date: Mon, 11 Feb 2019 11:53:30 +0000
Subject: [R] pattern evaluation in electron microscopy images
In-Reply-To: <fb6c47fe3525477c9d63e53e9a633855@SRVEXCHCM1302.precheza.cz>
References: <fb6c47fe3525477c9d63e53e9a633855@SRVEXCHCM1302.precheza.cz>
Message-ID: <295313c4660e49278e3abc8c0b6eb444@GBDCVPEXC08.corp.lgc-group.com>

Not really my field, but would you not approach this using FFT on selected regions?

I think IMageJ has some capability in that area; see example at https://imagej.nih.gov/ij/docs/examples/tem/.

Steve Ellison



> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of PIKAL Petr
> Sent: 08 February 2019 09:54
> To: r-help at r-project.org
> Subject: [R] pattern evaluation in electron microscopy images
> 
> Dear all
> 
> I enclose 3 electron microscope images in which I would like to evaluate
> plane spacing.
> 
> Before I start to dig deeper and use trial and error in trying to find some
> packages/functions for such pattern evaluation in electron microscopy
> pictures I would like to ask if anybody could point me to suitable
> packages/functions.
> 
> I am aware of EBImage package for general purpose image manipulation, but
> it does not have such functionality.
> 
> Best regards
> Petr
> 
> If images did not came through please use this link:
> St?hnout
> soubory<https://uschovna.agrofert.cz/dshosts/getfiles.aspx?fip=2475efa6-
> a77b-4ff6-8155-d9302e7b151b>.
> 
> Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch
> partner? PRECHEZA a.s. jsou zve?ejn?ny na:
> https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information
> about processing and protection of business partner's personal data are
> available on website: https://www.precheza.cz/en/personal-data-
> protection-principles/
> D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou
> d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en?
> odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any
> documents attached to it may be confidential and are subject to the legally
> binding disclaimer: https://www.precheza.cz/en/01-disclaimer/
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From rkoenker @end|ng |rom ||||no|@@edu  Mon Feb 11 12:54:16 2019
From: rkoenker @end|ng |rom ||||no|@@edu (Roger Koenker)
Date: Mon, 11 Feb 2019 11:54:16 +0000
Subject: [R] Siegel nonparametric regression / mblm package
In-Reply-To: <8e35f428ff824a85be630717b4e0bf4f@BYAPR11MB3141.namprd11.prod.outlook.com>
References: <8e35f428ff824a85be630717b4e0bf4f@BYAPR11MB3141.namprd11.prod.outlook.com>
Message-ID: <DC4A2B8D-3C8C-4D84-8203-CE2A3BF1334B@illinois.edu>

My first thought was also that this was an artifact of the ties, but dithering the data
n <- length(child)
child <- child + runif(n,-.5,.5)
parent <- parent + runif(n,-.5,.5)

and rerunning yields the same discrepancy between the Siegel and other fits. Curiously, both
lmsreg and ltsreg from MASS produce lines that are more steeply sloped than those
of the other methods.  Since I stupidly forgot to set.seed(), YMMV.

> On Feb 11, 2019, at 10:24 AM, Marco Besozzi <marco.beso48 at gmail.com> wrote:
> 
> I employed the "galton" set of data included in the package "psych". With
> the package "mblm" I obtained the Theil-Sen nonparametric regression and
> the Siegel non parametric regression, and compared them with the ordinary
> least square regression line.
> The results of standard regression and Theil-Sen regression are practically
> identical. But the Siegel regression seems to have a bias that I cannot
> understand. May I ask for a possible explanation? The bias may be related
> to the number of ties in the set of data? Here's the code and the image.
> 
> Best regards.
> 
> Marco Besozzi
> # Theil-Sen and Siegel nonparametric regression with package mblm
> # comparison with ordinary least squares (parametric) regression
> # on galton set of data included in the package psych
> #
> library(psych)
> attach(galton)
> library(mblm)
> #
> reglin_yx <- lm(child ~ parent, data=galton) # ordinary least squares
> (parametric) regression
> a_yx <- reglin_yx$coefficients[1] # intercept a
> b_yx <- reglin_yx$coefficients[2] # slope b
> #
> regnonTS <- mblm(child ~ parent, data=galton, repeated=FALSE) # Theil-Sen
> nonparametric regression (wait a few minutes!)
> a_TS <- regnonTS$coefficients[1] # intercept a
> b_TS <- regnonTS$coefficients[2] # slope b
> #
> regnonS = mblm(child ~ parent, data=galton, repeated=TRUE) # Siegel
> nonparametric regression
> a_S <- regnonS$coefficients[1] # intercept a
> b_S <- regnonS$coefficients[2] # slope b
> #
> # xy plot of data and regression lines
> #
> windows() # open a new window
> plot(parent, child, xlim = c(60,80), ylim = c(60,80), pch=1, xlab="Parent
> heigt (inch)", ylab="Chile height (inch)", main="Regression lines
> comparison", cex.main = 0.9) # data plot
> abline(a_yx, b_yx, col="green", lty=1) # ordinary least squares
> (parametric) regression line
> abline(a_TS, b_TS, col="blue", lty=1) # Theil-Sen nonparametric regression
> line
> abline(a_S, b_S, col="red", lty=1) # Siegel nonparametric regression
> legend(60, 80, legend=c("Ordinary least squares regression", "Theil-Sen
> nonparametric regression","Siegel nonparametric regression"),
> col=c("green", "blue", "red"), lty=c(4,4,1), cex=0.8) # add a legend
> #
> <Siegel.PNG>______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From or|o|eb@|t|more @end|ng |rom gm@||@com  Sun Feb 10 21:50:28 2019
From: or|o|eb@|t|more @end|ng |rom gm@||@com (Adrian Johnson)
Date: Sun, 10 Feb 2019 15:50:28 -0500
Subject: [R] (no subject)
In-Reply-To: <CAL2fYnMbKV5rhjOxBC1a1tprTa_6duBvxjAUVg2ar0fnW3appA@mail.gmail.com>
References: <CAL2fYnMbKV5rhjOxBC1a1tprTa_6duBvxjAUVg2ar0fnW3appA@mail.gmail.com>
Message-ID: <CAL2fYnPrvoMmCop1kHM+OzxsYJv2_PtaSZdtCsEPCYjY7crh9Q@mail.gmail.com>

Pardon me, I forgot to add subject line.
-Adrian.

On Sun, Feb 10, 2019 at 3:49 PM Adrian Johnson
<oriolebaltimore at gmail.com> wrote:
>
> Dear group,
>
> I have two large matrices.
>
> Matrix one: is 24776 x 76 (example toy1 dput object given below)
>
> Matrix two: is 12913 x 76 (example toy2 dput object given below)
>
> Column names of both matrices are identical.
>
> My aim is:
>
> a. Take each row of toy2 and transform vector into UP (>0)  and DN (
> <0 ) categories. (kc)
> b  Test association between kc and every row of toy1.
>
> My code, given below, although this works but is very slow.
>
> I gave dput objects for toy1, toy2 and result matrix.
>
> Could you suggest/help me how I can make this faster.  Also, how can I
> select values in result column that are less than 0.001 (p < 0.001).
>
> Appreciate your help. Thank you.
>
> Code:
> ===============================================================================
>
>
>
> result <- matrix(NA,nrow=nrow(toy1),ncol=nrow(toy2))
>
> rownames(result) <- rownames(toy1)
> colnames(result) <- rownames(toy2)
>
> for(i in 1:nrow(toy2)){
> for(j in 1:nrow(toy1)){
> kx = toy2[i,]
> kc <- rep('NC',length(kx))
> kc[ kx >0] <- 'UP'
> kc[ kx <=0 ] <- 'DN'
> xpv <- fisher.test(table(kc,toy1[j,]),simulate.p.value = TRUE)$p.value
> result[j,i] <- xpv
> }
> }
>
> ===============================================================================
>
>
> ===============================================================================
>
>
> > dput(toy1)
> structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1,
> -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
> -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
> -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1,
> -1, -1, -1, -1, -1), .Dim = c(10L, 7L), .Dimnames = list(c("ACAP3",
> "ACTRT2", "AGRN", "ANKRD65", "ATAD3A", "ATAD3B", "ATAD3C", "AURKAIP1",
> "B3GALT6", "C1orf159"), c("a", "b", "c", "d", "e", "f", "g")))
>
>
>
> > dput(toy2)
> structure(c(-0.242891119688613, -0.0514058216682132, 0.138447212993773,
> -0.312576648033122, 0.271489918720452, -0.281196468299486, -0.0407160143344565,
> -0.328353812845287, 0.151667836674511, 0.408596843743938, -0.049351944902924,
> 0.238586287349249, 0.200571558784821, -0.0737604184858411, 0.245971526254877,
> 0.24740263959845, -0.161528943131908, 0.197521973013793, 0.0402668125708444,
> 0.376323735212088, 0.0731550871764204, 0.385270176969893, 0.28953042756208,
> 0.062587289401188, -0.281187168932979, -0.0202298984561554, -0.0848696970309447,
> 0.0349676726358973, -0.520484215644868, -0.481991414222996,
> -0.00698099201388211,
> 0.135503878341873, 0.156983081312087, 0.320223832092661, 0.34582193394074,
> 0.0844455960468667, -0.157825604090972, 0.204758250510969, 0.261796072978612,
> -0.19510450641405, 0.43196474472874, -0.211155577453175, -0.0921641871215187,
> 0.420950361292263, 0.390261862151936, -0.422273930504427, 0.344653684951627,
> 0.0378273248838503, 0.197782027324611, 0.0963124876309569, 0.332093167080656,
> 0.128036554821915, -0.41338065859335, -0.409470440033177, 0.371490567256253,
> -0.0912549189140141, -0.247451812684234, 0.127741739114639, 0.0856254238844557,
> 0.515282940316031, -0.25675759521248, 0.333943163209869, 0.604141413840881,
> 0.0824942299510931, -0.179605710473021, -0.275604207054643, -0.113251154591898,
> 0.172897837449258, -0.329808795076691, -0.239255324324506), .Dim = c(10L,
> 7L), .Dimnames = list(c("chr5q23", "chr16q24", "chr8q24", "chr13q11",
> "chr7p21", "chr10q23", "chr13q13", "chr10q21", "chr1p13", "chrxp21"
> ), c("a", "b", "c", "d", "e", "f", "g")))
> >
>
>
> > dput(result)
> structure(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.532733633183408,
> 0.511244377811094, 0.528235882058971, 0.526736631684158, 0.51424287856072,
> 0.530734632683658, 0.513243378310845, 0.533233383308346, 0.542228885557221,
> 0.517241379310345, 0.532733633183408, 0.521739130434783, 0.529235382308846,
> 0.530234882558721, 0.548725637181409, 0.525737131434283, 0.527236381809095,
> 0.532733633183408, 0.530234882558721, 0.520739630184908, 0.15592203898051,
> 0.142928535732134, 0.140929535232384, 0.150924537731134, 0.160419790104948,
> 0.139430284857571, 0.152923538230885, 0.146426786606697, 0.149425287356322,
> 0.145427286356822, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.282358820589705,
> 0.293853073463268, 0.262868565717141, 0.290854572713643, 0.276861569215392,
> 0.288855572213893, 0.282358820589705, 0.292853573213393, 0.286356821589205,
> 0.271364317841079, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1), .Dim = c(10L, 10L), .Dimnames = list(c("ACAP3",
> "ACTRT2", "AGRN", "ANKRD65", "ATAD3A", "ATAD3B", "ATAD3C", "AURKAIP1",
> "B3GALT6", "C1orf159"), c("chr5q23", "chr16q24", "chr8q24", "chr13q11",
> "chr7p21", "chr10q23", "chr13q13", "chr10q21", "chr1p13", "chrxp21"
> )))
>
>
> ===============================================================================


From rkoenker @end|ng |rom ||||no|@@edu  Mon Feb 11 14:39:26 2019
From: rkoenker @end|ng |rom ||||no|@@edu (Roger Koenker)
Date: Mon, 11 Feb 2019 13:39:26 +0000
Subject: [R] Siegel nonparametric regression / mblm package
In-Reply-To: <f682ad0fbf1d4486aa64e30ffbdae85e@BYAPR11MB3141.namprd11.prod.outlook.com>
References: <8e35f428ff824a85be630717b4e0bf4f@BYAPR11MB3141.namprd11.prod.outlook.com>
 <DC4A2B8D-3C8C-4D84-8203-CE2A3BF1334B@illinois.edu>
 <f682ad0fbf1d4486aa64e30ffbdae85e@BYAPR11MB3141.namprd11.prod.outlook.com>
Message-ID: <ECCF4BF9-216E-406C-9777-02FF13D5BC6A@illinois.edu>

A quick look at the code for Siegel in mblm reveals that it is extremely inefficient, but it seems to be correct.
One ?explanation? for this behavior, presuming that we haven?t overlooked something more basic, is that such
high breakdown estimates sacrifice some efficiency, that is to say, they are more variable than other methods
when the data is well behaved, and of course, the Galton data is famously ?almost Gaussian?.

> On Feb 11, 2019, at 12:47 PM, Marco Besozzi <marco.beso48 at gmail.com> wrote:
> 
> Thank you very much for your reply.
> If I have well understood, unfortunately in this way I have lost the only idea I had...
> Do you believe that a problem in the R algorithm employed in the package mblm for Siegel regression is possible?
> And do you know if Siegel regression is available in a different package? I was unable to find it.
> Thanks again!
> Best regards.
> 
> P.S.: sorry for my bad english...
> 
> Il giorno lun 11 feb 2019 alle ore 12:54 Roger Koenker <rkoenker at illinois.edu <mailto:rkoenker at illinois.edu>> ha scritto:
> My first thought was also that this was an artifact of the ties, but dithering the data
> n <- length(child)
> child <- child + runif(n,-.5,.5)
> parent <- parent + runif(n,-.5,.5)
> 
> and rerunning yields the same discrepancy between the Siegel and other fits. Curiously, both
> lmsreg and ltsreg from MASS produce lines that are more steeply sloped than those
> of the other methods.  Since I stupidly forgot to set.seed(), YMMV.
> 
> > On Feb 11, 2019, at 10:24 AM, Marco Besozzi <marco.beso48 at gmail.com <mailto:marco.beso48 at gmail.com>> wrote:
> > 
> > I employed the "galton" set of data included in the package "psych". With
> > the package "mblm" I obtained the Theil-Sen nonparametric regression and
> > the Siegel non parametric regression, and compared them with the ordinary
> > least square regression line.
> > The results of standard regression and Theil-Sen regression are practically
> > identical. But the Siegel regression seems to have a bias that I cannot
> > understand. May I ask for a possible explanation? The bias may be related
> > to the number of ties in the set of data? Here's the code and the image.
> > 
> > Best regards.
> > 
> > Marco Besozzi
> > # Theil-Sen and Siegel nonparametric regression with package mblm
> > # comparison with ordinary least squares (parametric) regression
> > # on galton set of data included in the package psych
> > #
> > library(psych)
> > attach(galton)
> > library(mblm)
> > #
> > reglin_yx <- lm(child ~ parent, data=galton) # ordinary least squares
> > (parametric) regression
> > a_yx <- reglin_yx$coefficients[1] # intercept a
> > b_yx <- reglin_yx$coefficients[2] # slope b
> > #
> > regnonTS <- mblm(child ~ parent, data=galton, repeated=FALSE) # Theil-Sen
> > nonparametric regression (wait a few minutes!)
> > a_TS <- regnonTS$coefficients[1] # intercept a
> > b_TS <- regnonTS$coefficients[2] # slope b
> > #
> > regnonS = mblm(child ~ parent, data=galton, repeated=TRUE) # Siegel
> > nonparametric regression
> > a_S <- regnonS$coefficients[1] # intercept a
> > b_S <- regnonS$coefficients[2] # slope b
> > #
> > # xy plot of data and regression lines
> > #
> > windows() # open a new window
> > plot(parent, child, xlim = c(60,80), ylim = c(60,80), pch=1, xlab="Parent
> > heigt (inch)", ylab="Chile height (inch)", main="Regression lines
> > comparison", cex.main = 0.9) # data plot
> > abline(a_yx, b_yx, col="green", lty=1) # ordinary least squares
> > (parametric) regression line
> > abline(a_TS, b_TS, col="blue", lty=1) # Theil-Sen nonparametric regression
> > line
> > abline(a_S, b_S, col="red", lty=1) # Siegel nonparametric regression
> > legend(60, 80, legend=c("Ordinary least squares regression", "Theil-Sen
> > nonparametric regression","Siegel nonparametric regression"),
> > col=c("green", "blue", "red"), lty=c(4,4,1), cex=0.8) # add a legend
> > #
> > <Siegel.PNG>______________________________________________
> > R-help at r-project.org <mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help <https://stat.ethz.ch/mailman/listinfo/r-help>
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html <http://www.r-project.org/posting-guide.html>
> > and provide commented, minimal, self-contained, reproducible code.
> 


	[[alternative HTML version deleted]]


From m@rco@be@o48 @end|ng |rom gm@||@com  Mon Feb 11 14:48:37 2019
From: m@rco@be@o48 @end|ng |rom gm@||@com (Marco Besozzi)
Date: Mon, 11 Feb 2019 14:48:37 +0100
Subject: [R] Siegel nonparametric regression / mblm package
In-Reply-To: <ECCF4BF9-216E-406C-9777-02FF13D5BC6A@illinois.edu>
References: <8e35f428ff824a85be630717b4e0bf4f@BYAPR11MB3141.namprd11.prod.outlook.com>
 <DC4A2B8D-3C8C-4D84-8203-CE2A3BF1334B@illinois.edu>
 <f682ad0fbf1d4486aa64e30ffbdae85e@BYAPR11MB3141.namprd11.prod.outlook.com>
 <ECCF4BF9-216E-406C-9777-02FF13D5BC6A@illinois.edu>
Message-ID: <CAO=Vm8q3N_7stKCgKTa7fj1XUYKxXoUeEeAE0DM0xVzaY3Y+Dg@mail.gmail.com>

Thanks a lot!


Il giorno lun 11 feb 2019 alle ore 14:39 Roger Koenker <
rkoenker at illinois.edu> ha scritto:

> A quick look at the code for Siegel in mblm reveals that it is extremely
> inefficient, but it seems to be correct.
> One ?explanation? for this behavior, presuming that we haven?t overlooked
> something more basic, is that such
> high breakdown estimates sacrifice some efficiency, that is to say, they
> are more variable than other methods
> when the data is well behaved, and of course, the Galton data is famously
> ?almost Gaussian?.
>
> On Feb 11, 2019, at 12:47 PM, Marco Besozzi <marco.beso48 at gmail.com>
> wrote:
>
> Thank you very much for your reply.
> If I have well understood, unfortunately in this way I have lost the only
> idea I had...
> Do you believe that a problem in the R algorithm employed in the package
> mblm for Siegel regression is possible?
> And do you know if Siegel regression is available in a different package?
> I was unable to find it.
> Thanks again!
> Best regards.
>
> P.S.: sorry for my bad english...
>
> Il giorno lun 11 feb 2019 alle ore 12:54 Roger Koenker <
> rkoenker at illinois.edu> ha scritto:
>
>> My first thought was also that this was an artifact of the ties, but
>> dithering the data
>> n <- length(child)
>> child <- child + runif(n,-.5,.5)
>> parent <- parent + runif(n,-.5,.5)
>>
>> and rerunning yields the same discrepancy between the Siegel and other
>> fits. Curiously, both
>> lmsreg and ltsreg from MASS produce lines that are more steeply sloped
>> than those
>> of the other methods.  Since I stupidly forgot to set.seed(), YMMV.
>>
>> > On Feb 11, 2019, at 10:24 AM, Marco Besozzi <marco.beso48 at gmail.com>
>> wrote:
>> >
>> > I employed the "galton" set of data included in the package "psych".
>> With
>> > the package "mblm" I obtained the Theil-Sen nonparametric regression and
>> > the Siegel non parametric regression, and compared them with the
>> ordinary
>> > least square regression line.
>> > The results of standard regression and Theil-Sen regression are
>> practically
>> > identical. But the Siegel regression seems to have a bias that I cannot
>> > understand. May I ask for a possible explanation? The bias may be
>> related
>> > to the number of ties in the set of data? Here's the code and the image.
>> >
>> > Best regards.
>> >
>> > Marco Besozzi
>> > # Theil-Sen and Siegel nonparametric regression with package mblm
>> > # comparison with ordinary least squares (parametric) regression
>> > # on galton set of data included in the package psych
>> > #
>> > library(psych)
>> > attach(galton)
>> > library(mblm)
>> > #
>> > reglin_yx <- lm(child ~ parent, data=galton) # ordinary least squares
>> > (parametric) regression
>> > a_yx <- reglin_yx$coefficients[1] # intercept a
>> > b_yx <- reglin_yx$coefficients[2] # slope b
>> > #
>> > regnonTS <- mblm(child ~ parent, data=galton, repeated=FALSE) #
>> Theil-Sen
>> > nonparametric regression (wait a few minutes!)
>> > a_TS <- regnonTS$coefficients[1] # intercept a
>> > b_TS <- regnonTS$coefficients[2] # slope b
>> > #
>> > regnonS = mblm(child ~ parent, data=galton, repeated=TRUE) # Siegel
>> > nonparametric regression
>> > a_S <- regnonS$coefficients[1] # intercept a
>> > b_S <- regnonS$coefficients[2] # slope b
>> > #
>> > # xy plot of data and regression lines
>> > #
>> > windows() # open a new window
>> > plot(parent, child, xlim = c(60,80), ylim = c(60,80), pch=1,
>> xlab="Parent
>> > heigt (inch)", ylab="Chile height (inch)", main="Regression lines
>> > comparison", cex.main = 0.9) # data plot
>> > abline(a_yx, b_yx, col="green", lty=1) # ordinary least squares
>> > (parametric) regression line
>> > abline(a_TS, b_TS, col="blue", lty=1) # Theil-Sen nonparametric
>> regression
>> > line
>> > abline(a_S, b_S, col="red", lty=1) # Siegel nonparametric regression
>> > legend(60, 80, legend=c("Ordinary least squares regression", "Theil-Sen
>> > nonparametric regression","Siegel nonparametric regression"),
>> > col=c("green", "blue", "red"), lty=c(4,4,1), cex=0.8) # add a legend
>> > #
>> > <Siegel.PNG>______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> <http://www.r-project.org/posting-guide.html>
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>>
>

	[[alternative HTML version deleted]]


From ro@@@|@mo|den @end|ng |rom gm@||@com  Mon Feb 11 14:11:29 2019
From: ro@@@|@mo|den @end|ng |rom gm@||@com (Ross Molden)
Date: Mon, 11 Feb 2019 13:11:29 +0000
Subject: [R] Help needed with my code for merging multiple xls files from
 google drive
Message-ID: <F00E7103-40E9-4520-A868-32EF477917D9@gmail.com>

Hi guys,

I am trying to merge a list of .xls files in google drive. I have now managed to create a list of all the files I need, but for some reason I still can't manage to merge them, this is the code I have so far:

library(googledrive) inputfiles <- drive_ls(path = "Email It In", pattern = "*PDOL_dataexport", n_max = 50)

library(readxl) df.list<- lapply(inputfiles,function(x) read_xls(x)) library(dplyr) consolidated_data<-bind_rows(df.list)

The second part of the code throws up the following error: 

Error: path must be a string 

I must be entering the path (inputfiles) incorrectly for lapply, can someone please help?

Thank in advance!
Ross
	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon Feb 11 18:17:25 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 11 Feb 2019 09:17:25 -0800
Subject: [R] 
 Help needed with my code for merging multiple xls files from
 google drive
In-Reply-To: <F00E7103-40E9-4520-A868-32EF477917D9@gmail.com>
References: <F00E7103-40E9-4520-A868-32EF477917D9@gmail.com>
Message-ID: <F9861A9E-8676-4DFE-9BA2-E14283B32412@dcn.davis.ca.us>

Your example is not reproducible [1][2][3], you are reposting a copy of an email in a fresh thread (instead of replying to the first one), and you are using HTML email format on a text-only mailing list (what you see is really not what we see). Please read the Posting Guide to find out what the basic conventions are for using this mailing list. If you make the reader's job too hard they are going to ignore you.

Regarding your question... you don't seem to be executing your code one line at a time in order to debug it... you can also try executing just the read_xls function with one of the elements of the inputfiles variable to confirm whether read_xls is the problem (I think so) or whether lapply is the problem (seems unlikely). You should read the examples in the googledrive package... I suspect that the readxl package is not designed to handle google docs, but there is an as_dribble function in googledrive that may be useful for getting data out of it.

[1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

[2] http://adv-r.had.co.nz/Reproducibility.html

[3] https://cran.r-project.org/web/packages/reprex/index.html (read the vignette)

On February 11, 2019 5:11:29 AM PST, Ross Molden <ross.l.molden at gmail.com> wrote:
>Hi guys,
>
>I am trying to merge a list of .xls files in google drive. I have now
>managed to create a list of all the files I need, but for some reason I
>still can't manage to merge them, this is the code I have so far:
>
>library(googledrive) inputfiles <- drive_ls(path = "Email It In",
>pattern = "*PDOL_dataexport", n_max = 50)
>
>library(readxl) df.list<- lapply(inputfiles,function(x) read_xls(x))
>library(dplyr) consolidated_data<-bind_rows(df.list)
>
>The second part of the code throws up the following error: 
>
>Error: path must be a string 
>
>I must be entering the path (inputfiles) incorrectly for lapply, can
>someone please help?
>
>Thank in advance!
>Ross
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Mon Feb 11 21:01:16 2019
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard McGarvey)
Date: Mon, 11 Feb 2019 15:01:16 -0500 (EST)
Subject: [R] Difficulty with "\\" in string functions....
Message-ID: <1602927782.166029.1549915277240@connect.xfinity.com>

I am using the file.choose() function to choose a file from the dialog box and once I get it, I want to be able to split the full name into the folder part and the file name part. So for example, when I have closed the file choose dialog, the name for the file I get is


Fname1
[1] "D:\\Data\\OneDrive\\ISTA Documents\\QT_App\\QT Analysis Input Data Example WorkBook.xlsx"


where the "\\" is used to split the folder and sub-folder and file names. R see this "\\" as a single \ backslash character.


Now I try to split it using


str_split(Fname1,"\\")


but this returns an error


Error in stri_split_regex(string, pattern, n = n, simplify = simplify, :
Unrecognized backslash escape sequence in pattern. (U_REGEX_BAD_ESCAPE_SEQUENCE)


I know its got something to do with the \\ because it is treated as a single backslash character. But replacing the str_split with


str_split(Fname1,"\")


does not work either. 


Any ideas on how I can handle the \\ and split the full name into its pieces?



Lion Bernard McGarvey

Director, Fort Myers Beach Lions Foundation, Inc.

Retired (Lilly Engineering Fellow).



	[[alternative HTML version deleted]]


From kry|ov@r00t @end|ng |rom gm@||@com  Mon Feb 11 21:13:57 2019
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Mon, 11 Feb 2019 23:13:57 +0300
Subject: [R] Difficulty with "\\" in string functions....
In-Reply-To: <1602927782.166029.1549915277240@connect.xfinity.com>
References: <1602927782.166029.1549915277240@connect.xfinity.com>
Message-ID: <20190211231357.27cb7fce@Tarkus>

On Mon, 11 Feb 2019 15:01:16 -0500 (EST)
Bernard McGarvey <mcgarvey.bernard at comcast.net> wrote:

> Now I try to split it using
> 
> 
> str_split(Fname1,"\\")
> 
> 
> but this returns an error
> 
> 
> Error in stri_split_regex(string, pattern, n = n, simplify =
> simplify, : Unrecognized backslash escape sequence in pattern.
> (U_REGEX_BAD_ESCAPE_SEQUENCE)

This happens because the second parameter of str_split is by default a
regular expression, and a backslash has a special meaning in regular
expressions: when preceding other characters, it may change the way
they are interpreted. (For example, w means a literal "w"
character, while \w means "any alphanumeric character". On the
other hand, [ starts a character group, but \[ means just an opening
square bracket.) See ?regex for more info on that.

Since you want a literal backslash, you need to escape it with another
backslash: \\

But to write a string literal of a double-backslash in R, you need to
escape both backslash characters, each with their own backslash: "\\\\"

## fname <- "D:\\Data\\OneDrive\\ISTA Documents\\QT_App\\QT Analysis
Input Data Example WorkBook.xlsx"
## message("\\\\")
\\
## str_split(fname, "\\\\")
[[1]]
[1] "D:"                                         
[2] "Data"                                       
[3] "OneDrive"                                   
[4] "ISTA Documents"                             
[5] "QT_App"                                     
[6] "QT AnalysisInput Data Example WorkBook.xlsx"

You can also avoid all layers of the backslash hell (except the first)
if you choose to split by fixed strings instead of regular expressions
by using stringr::fixed:

## str_split(fname, fixed("\\"))

-- 
Best regards,
Ivan


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Mon Feb 11 21:19:15 2019
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard McGarvey)
Date: Mon, 11 Feb 2019 15:19:15 -0500 (EST)
Subject: [R] Difficulty with "\\" in string functions....
In-Reply-To: <20190211231357.27cb7fce@Tarkus>
References: <1602927782.166029.1549915277240@connect.xfinity.com>
 <20190211231357.27cb7fce@Tarkus>
Message-ID: <1204842114.166403.1549916356475@connect.xfinity.com>

Brilliant! Thanks a million Ivan.

Lion Bernard McGarvey


Director, Fort Myers Beach Lions Foundation, Inc.


Retired (Lilly Engineering Fellow).


> On February 11, 2019 at 3:13 PM Ivan Krylov <krylov.r00t at gmail.com> wrote:
> 
> 
> On Mon, 11 Feb 2019 15:01:16 -0500 (EST)
> Bernard McGarvey <mcgarvey.bernard at comcast.net> wrote:
> 
> > Now I try to split it using
> > 
> > 
> > str_split(Fname1,"\\")
> > 
> > 
> > but this returns an error
> > 
> > 
> > Error in stri_split_regex(string, pattern, n = n, simplify =
> > simplify, : Unrecognized backslash escape sequence in pattern.
> > (U_REGEX_BAD_ESCAPE_SEQUENCE)
> 
> This happens because the second parameter of str_split is by default a
> regular expression, and a backslash has a special meaning in regular
> expressions: when preceding other characters, it may change the way
> they are interpreted. (For example, w means a literal "w"
> character, while \w means "any alphanumeric character". On the
> other hand, [ starts a character group, but \[ means just an opening
> square bracket.) See ?regex for more info on that.
> 
> Since you want a literal backslash, you need to escape it with another
> backslash: \\
> 
> But to write a string literal of a double-backslash in R, you need to
> escape both backslash characters, each with their own backslash: "\\\\"
> 
> ## fname <- "D:\\Data\\OneDrive\\ISTA Documents\\QT_App\\QT Analysis
> Input Data Example WorkBook.xlsx"
> ## message("\\\\")
> \\
> ## str_split(fname, "\\\\")
> [[1]]
> [1] "D:"                                         
> [2] "Data"                                       
> [3] "OneDrive"                                   
> [4] "ISTA Documents"                             
> [5] "QT_App"                                     
> [6] "QT AnalysisInput Data Example WorkBook.xlsx"
> 
> You can also avoid all layers of the backslash hell (except the first)
> if you choose to split by fixed strings instead of regular expressions
> by using stringr::fixed:
> 
> ## str_split(fname, fixed("\\"))
> 
> -- 
> Best regards,
> Ivan


From wdun|@p @end|ng |rom t|bco@com  Mon Feb 11 22:28:53 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Mon, 11 Feb 2019 13:28:53 -0800
Subject: [R] Difficulty with "\\" in string functions....
In-Reply-To: <1204842114.166403.1549916356475@connect.xfinity.com>
References: <1602927782.166029.1549915277240@connect.xfinity.com>
 <20190211231357.27cb7fce@Tarkus>
 <1204842114.166403.1549916356475@connect.xfinity.com>
Message-ID: <CAF8bMcY8jtYPLxXypioR50Ue6uCnidCmhJQC8TGqG1BSxbBJsA@mail.gmail.com>

You can also avoid the issue by using the basename and dirname functions.

> Fname1 <- "D:\\Data\\OneDrive\\ISTA Documents\\QT_App\\QT Analysis Input
Data Example WorkBook.xlsx"
> basename(Fname1)
[1] "QT Analysis Input Data Example WorkBook.xlsx"
> dirname(Fname1)
[1] "D:/Data/OneDrive/ISTA Documents/QT_App"

Use normalizePath if you need to convert those / to \ on Windows.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Mon, Feb 11, 2019 at 12:26 PM Bernard McGarvey <
mcgarvey.bernard at comcast.net> wrote:

> Brilliant! Thanks a million Ivan.
>
> Lion Bernard McGarvey
>
>
> Director, Fort Myers Beach Lions Foundation, Inc.
>
>
> Retired (Lilly Engineering Fellow).
>
>
> > On February 11, 2019 at 3:13 PM Ivan Krylov <krylov.r00t at gmail.com>
> wrote:
> >
> >
> > On Mon, 11 Feb 2019 15:01:16 -0500 (EST)
> > Bernard McGarvey <mcgarvey.bernard at comcast.net> wrote:
> >
> > > Now I try to split it using
> > >
> > >
> > > str_split(Fname1,"\\")
> > >
> > >
> > > but this returns an error
> > >
> > >
> > > Error in stri_split_regex(string, pattern, n = n, simplify =
> > > simplify, : Unrecognized backslash escape sequence in pattern.
> > > (U_REGEX_BAD_ESCAPE_SEQUENCE)
> >
> > This happens because the second parameter of str_split is by default a
> > regular expression, and a backslash has a special meaning in regular
> > expressions: when preceding other characters, it may change the way
> > they are interpreted. (For example, w means a literal "w"
> > character, while \w means "any alphanumeric character". On the
> > other hand, [ starts a character group, but \[ means just an opening
> > square bracket.) See ?regex for more info on that.
> >
> > Since you want a literal backslash, you need to escape it with another
> > backslash: \\
> >
> > But to write a string literal of a double-backslash in R, you need to
> > escape both backslash characters, each with their own backslash: "\\\\"
> >
> > ## fname <- "D:\\Data\\OneDrive\\ISTA Documents\\QT_App\\QT Analysis
> > Input Data Example WorkBook.xlsx"
> > ## message("\\\\")
> > \\
> > ## str_split(fname, "\\\\")
> > [[1]]
> > [1] "D:"
> > [2] "Data"
> > [3] "OneDrive"
> > [4] "ISTA Documents"
> > [5] "QT_App"
> > [6] "QT AnalysisInput Data Example WorkBook.xlsx"
> >
> > You can also avoid all layers of the backslash hell (except the first)
> > if you choose to split by fixed strings instead of regular expressions
> > by using stringr::fixed:
> >
> > ## str_split(fname, fixed("\\"))
> >
> > --
> > Best regards,
> > Ivan
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Mon Feb 11 22:33:59 2019
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard)
Date: Mon, 11 Feb 2019 16:33:59 -0500
Subject: [R] Difficulty with "\\" in string functions....
Message-ID: <63d068ff-a0a9-4f82-9f55-3519969c315f@Bernards-iPhone>

Simple when you know how!

Thanks

Sent from mobile device - please excuse any spelling mistakes.



------ Original Message ------

From: William Dunlap
To: Bernard McGarvey
Cc: Ivan Krylov, r-help at r-project.org
Sent: February 11, 2019 at 4:29 PM
Subject: Re: [R] Difficulty with "\\" in string functions....

You can also avoid the issue by using the basename and dirname functions.

>Fname1<- "D:\\Data\\OneDrive\\ISTA Documents\\QT_App\\QT Analysis Input Data Example WorkBook.xlsx"
>basename(Fname1)
[1] "QT Analysis Input Data Example WorkBook.xlsx"
>dirname(Fname1)
[1] "D:/Data/OneDrive/ISTA Documents/QT_App"


Use normalizePath if you need to convert those / to \ on Windows.

Bill Dunlap
TIBCO Software
wdunlaptibco.com(http://tibco.com)

On Mon, Feb 11, 2019 at 12:26 PM Bernard McGarvey<mcgarvey.bernard at comcast.net(mailto:mcgarvey.bernard at comcast.net)>wrote:
> Brilliant! Thanks a million Ivan.
> 
> Lion Bernard McGarvey
> 
> 
> Director, Fort Myers Beach Lions Foundation, Inc.
> 
> 
> Retired (Lilly Engineering Fellow).
> 
> 
> >On February 11, 2019 at 3:13 PM Ivan Krylov<krylov.r00t at gmail.com(mailto:krylov.r00t at gmail.com)>wrote:
> >
> >
> >On Mon, 11 Feb 2019 15:01:16 -0500 (EST)
> >Bernard McGarvey<mcgarvey.bernard at comcast.net(mailto:mcgarvey.bernard at comcast.net)>wrote:
> >
> >>Now I try to split it using
> >>
> >>
> >>str_split(Fname1,"\\")
> >>
> >>
> >>but this returns an error
> >>
> >>
> >>Error in stri_split_regex(string, pattern, n = n, simplify =
> >>simplify, : Unrecognized backslash escape sequence in pattern.
> >>(U_REGEX_BAD_ESCAPE_SEQUENCE)
> >
> >This happens because the second parameter of str_split is by default a
> >regular expression, and a backslash has a special meaning in regular
> >expressions: when preceding other characters, it may change the way
> >they are interpreted. (For example, w means a literal "w"
> >character, while \w means "any alphanumeric character". On the
> >other hand, [ starts a character group, but \[ means just an opening
> >square bracket.) See ?regex for more info on that.
> >
> >Since you want a literal backslash, you need to escape it with another
> >backslash: \\
> >
> >But to write a string literal of a double-backslash in R, you need to
> >escape both backslash characters, each with their own backslash: "\\\\"
> >
> >## fname<- "D:\\Data\\OneDrive\\ISTA Documents\\QT_App\\QT Analysis
> >Input Data Example WorkBook.xlsx"
> >## message("\\\\")
> >\\
> >## str_split(fname, "\\\\")
> >[[1]]
> >[1] "D:"
> >[2] "Data"
> >[3] "OneDrive"
> >[4] "ISTA Documents"
> >[5] "QT_App"
> >[6] "QT AnalysisInput Data Example WorkBook.xlsx"
> >
> >You can also avoid all layers of the backslash hell (except the first)
> >if you choose to split by fixed strings instead of regular expressions
> >by using stringr::fixed:
> >
> >## str_split(fname, fixed("\\"))
> >
> >--
> >Best regards,
> >Ivan
> 
> ______________________________________________
> R-help at r-project.org(mailto:R-help at r-project.org)mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guidehttp://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From em@n|@m@||@92 @end|ng |rom gm@||@com  Mon Feb 11 22:42:35 2019
From: em@n|@m@||@92 @end|ng |rom gm@||@com (=?UTF-8?B?2KXZitmF2KfZhiDYpdiz2YXYp9i52YrZhCDZhdit2YXYrw==?=)
Date: Mon, 11 Feb 2019 23:42:35 +0200
Subject: [R] Question about bindata lib in high dimensions
In-Reply-To: <CAFiZHK7vteBS9qVU4fJYShDYFxiQiajqF1iSnWSxvYrFYmQy7g@mail.gmail.com>
References: <CAFiZHK6ir3NR9P4aK41UPNnoUwrB9ZMS74xJqr20gH1wKTFP+A@mail.gmail.com>
 <CAGgJW75dg6VczWn6J7XTSPPZMg3TOqtXZeLa1FSQsFeXJbt0KQ@mail.gmail.com>
 <CAFiZHK7vteBS9qVU4fJYShDYFxiQiajqF1iSnWSxvYrFYmQy7g@mail.gmail.com>
Message-ID: <CAFiZHK5MFW3nQAkV_iXTu0ssLMEHCgBtEdcxBQ5h5u8sdVNHLQ@mail.gmail.com>

even I tried to use another library mipfp to generate multivariate Bernoulli
*using the following:*
> p.joint <- ObtainMultBinaryDist(corr = corr_mat[1:10,1:10], marg.probs =
probs[1:10])
*it Shows:*
Problematic pairs:
     row col
[1,]  10   9
[2,]   9  10
Warning messages:
1: In Corr2PairProbs(corr, marg.probs) :
  Correlation exceeds constrains set by marg.probs, i.e. pair.proba[i, j]
<= marg.probs[i]

2: In Ipfp(seed = seed, target.list = target.list, target.data =
target.data,  :
  Missing values allowed in the target margins.
             Computation of the covariance matrices set to FALSE!
3: In Ipfp(seed = seed, target.list = target.list, target.data =
target.data,  :
  IPFP did not converged after 1000 iteration(s)!
            This migh be due to 0 cells in the seed, maximum number
            of iteration too low or tolerance too small

*and even if I fix the problematic pair (9,10) with the following:*

> corr_mat[9,10]=runif(1,max = min(probs[9],probs[10]),min  =
max(0,probs[10]+probs[9]-1))
> corr_mat[9,10]
[1] 0.8551618
> corr_mat[10,9]=0.8551618

*it still give me the same error.*

> p.joint <- ObtainMultBinaryDist(corr = corr_mat[1:10,1:10], marg.probs =
probs[1:10])
Problematic pairs:
     row col
[1,]  10   9
[2,]   9  10
Warning messages:
1: In Corr2PairProbs(corr, marg.probs) :
  Correlation exceeds constrains set by marg.probs, i.e. pair.proba[i, j]
<= marg.probs[i]

2: In Ipfp(seed = seed, target.list = target.list, target.data =
target.data,  :
  Missing values allowed in the target margins.
             Computation of the covariance matrices set to FALSE!
3: In Ipfp(seed = seed, target.list = target.list, target.data =
target.data,  :
  IPFP did not converged after 1000 iteration(s)!
            This migh be due to 0 cells in the seed, maximum number
            of iteration too low or tolerance too small
Could anyone help me please?

?On Mon, 11 Feb 2019 at 13:10, ?????? ??????? ?????? <
emanismail.92 at gmail.com> wrote:?

> *Here Sample of Code for 10 variables:*
> > probs_10 = probs[1:10]
> > probs_10
>  [1] 9.795272e-01 9.331778e-01 6.764349e-01 9.884067e-02 9.522222e-05
> 3.499417e-03 2.380556e-05 9.826457e-01 9.628633e-01 8.874949e-01
> > corr_mat_10 = corr_mat[1:10,1:10]
> > corr_mat_10
>                [,1]         [,2]         [,3]         [,4]
> [,5]          [,6]          [,7]          [,8]          [,9]       [,10]
>  [1,]  1.0000000000  0.540258943  0.209031764  0.047879233 -6.750092e-02
> 0.0085672057  7.053822e-04  0.7840635867  0.6694665745  0.40604770
>  [2,]  0.5402589429  1.000000000  0.386910326  0.088622750 -3.646798e-02
> -0.0454879132  1.305637e-03  0.4929722619  0.6613106007  0.61159373
>  [3,]  0.2090317635  0.386910326  1.000000000  0.229052428 -1.410984e-02
> -0.0434598161 -7.054666e-03  0.1909793458  0.2831488805  0.49337866
>  [4,]  0.0478792330  0.088622750  0.229052428  1.000000000 -3.231892e-03
> -0.0101705338 -1.615888e-03  0.0434012259  0.0646190283  0.11766286
>  [5,] -0.0675009217 -0.036467977 -0.014109837 -0.003231892  1.000000e+00
> -0.0005782943 -4.761395e-05 -0.0734320072 -0.0496901947 -0.02740859
>  [6,]  0.0085672057 -0.045487913 -0.043459816 -0.010170534 -5.782943e-04
> 1.0000000000  8.233515e-02  0.0078752345  0.0095061395 -0.03886223
>  [7,]  0.0007053822  0.001305637 -0.007054666 -0.001615888 -4.761395e-05
> 0.0823351499  1.000000e+00  0.0006484086  0.0009582161  0.00173719
>  [8,]  0.7840635867  0.492972262  0.190979346  0.043401226 -7.343201e-02
> 0.0078752345  6.484086e-04  1.0000000000  0.6766830516  0.37325133
>  [9,]  0.6694665745  0.661310601  0.283148881  0.064619028 -4.969019e-02
> 0.0095061395  9.582161e-04  0.6766830516  1.0000000000  0.55158959
> [10,]  0.4060477004  0.611593731  0.493378657  0.117662862 -2.740859e-02
> -0.0388622278  1.737190e-03  0.3732513255  0.5515895878  1.00000000
> > library(bindata)
> > r <- rmvbin(10,margprob = probs_10, bincorr = corr_mat_10)
> Not all probabilities are between 0 and 1.
> Error in Element ( 1 , 5 ): Admissible values are in [ 0 ,
> 9.5222224867284e-05 ].
> Error in Element ( 1 , 7 ): Admissible values are in [ 0 ,
> 2.3805556216821e-05 ].
> Error in Element ( 3 , 7 ): Admissible values are in [ 0 ,
> 2.3805556216821e-05 ].
> Error in Element ( 4 , 7 ): Admissible values are in [ 0 ,
> 2.3805556216821e-05 ].
> Error in Element ( 5 , 7 ): Admissible values are in [ 0 ,
> 2.3805556216821e-05 ].
> Error in Element ( 6 , 7 ): Admissible values are in [ 0 ,
> 2.3805556216821e-05 ].
> Error in Element ( 7 , 3 ): Admissible values are in [ 0 ,
> 2.3805556216821e-05 ].
> Error in Element ( 7 , 4 ): Admissible values are in [ 0 ,
> 2.3805556216821e-05 ].
> Error in Element ( 7 , 5 ): Admissible values are in [ 0 ,
> 2.3805556216821e-05 ].
> Error in Element ( 7 , 6 ): Admissible values are in [ 0 ,
> 2.3805556216821e-05 ].
> Error in Element ( 7 , 8 ): Admissible values are in [ 0 ,
> 2.3805556216821e-05 ].
> Error in Element ( 7 , 9 ): Admissible values are in [ 0 ,
> 2.3805556216821e-05 ].
> Error in Element ( 7 , 10 ): Admissible values are in [ 0 ,
> 2.3805556216821e-05 ].
> Error in Element ( 8 , 7 ): Admissible values are in [ 0 ,
> 2.3805556216821e-05 ].
> Error in Element ( 9 , 7 ): Admissible values are in [ 0 ,
> 2.3805556216821e-05 ].
> Error in Element ( 9 , 10 ): Admissible values are in [ 0.850358273621063
> , 0.887494941319304 ].
> Error in commonprob2sigma(commonprob, simulvals) :
>   Matrix commonprob not admissible.
>
> *Then I tried to fix wrong values to be within range with the following:*
> > corr_mat_10[1,5]=runif(1,min=0,max=9.5222224867284e-05)
> > corr_mat_10[1,5]
> [1] 7.915036e-05
> > corr_mat_10[5,1]=7.915036e-05
>
> *and did the same for all elements but the same error raised:*
> > r <- rmvbin(10,margprob = probs_10, bincorr = corr_mat_10)
> Error in Element ( 9 , 10 ): Admissible values are in [ 0.850358273621063
> , 0.887494941319304 ].
> Error in commonprob2sigma(commonprob, simulvals) :
>   Matrix commonprob not admissible.
>
> *Finally I check the value of (9,10) it is within range but the same error
> raised :*
> > corr_mat_10[9,10]
> [1] 0.8793437
>
> I don't know How to fix it?
>
> On Mon, 11 Feb 2019 at 12:23, Eric Berger <ericjberger at gmail.com> wrote:
>
>> Hi Eman,
>> It helps if you create a small example that reproduces the problem and
>> then post the code with your question.
>> This will help people determine what is causing the problem.
>>
>> Best,
>> Eric
>>
>>
>> ?On Mon, Feb 11, 2019 at 11:52 AM ?????? ??????? ?????? <
>> emanismail.92 at gmail.com> wrote:?
>>
>>> I need to simulate data for 2000 binary variables given a vector of
>>> marginal probabilities and a correlation matrix. I used bindata library,
>>> but it give me
>>>
>>>  Not all probabilities are between 0 and 1.
>>> Error in Element ( i , j ): Admissible values are in [.....].
>>> Error in commonprob2sigma(commonprob, simulvals) :
>>>   Matrix commonprob not admissible.
>>>
>>>  and I tried to get the elements within range but still have the same
>>> problem
>>>
>>> How can I fix the correlation matrix or how to track error ??
>>>
>>> Thanks
>>> Eman
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>

	[[alternative HTML version deleted]]


From v@|kremk @end|ng |rom gm@||@com  Mon Feb 11 23:35:39 2019
From: v@|kremk @end|ng |rom gm@||@com (Val)
Date: Mon, 11 Feb 2019 16:35:39 -0600
Subject: [R] Select
Message-ID: <CAJOiR6bB7FGFV3yuQoL_dwGAZ+SVsBDvP_jY=Ug-AAEDMBRsgw@mail.gmail.com>

Hi all,

I have a data frame  with tow variables  group and its size.
mydat<- read.table( text='group  count
G1 25
G2 15
G3 12
G4 31
G5 10' , header = TRUE, as.is = TRUE )

I want to select   group ID randomly (without replacement)  until  the
sum of count reaches 40.
So, in  the first case, the data frame could be
   G4 31
   65 10

In other case, it could be
  G5 10
  G2 15
  G3 12

How do I put sum of count variable   is  a minimum of 40 restriction?

Than k you in advance






I want to select group  ids randomly until I reach the


From dc@r|@on @end|ng |rom t@mu@edu  Mon Feb 11 23:51:56 2019
From: dc@r|@on @end|ng |rom t@mu@edu (David L Carlson)
Date: Mon, 11 Feb 2019 22:51:56 +0000
Subject: [R] Select
In-Reply-To: <CAJOiR6bB7FGFV3yuQoL_dwGAZ+SVsBDvP_jY=Ug-AAEDMBRsgw@mail.gmail.com>
References: <CAJOiR6bB7FGFV3yuQoL_dwGAZ+SVsBDvP_jY=Ug-AAEDMBRsgw@mail.gmail.com>
Message-ID: <7850e860f2d74c7190d81c87f9b64cb9@tamu.edu>

First expand your data frame into a vector where G1 is repeated 25 times, G2 is repeated 15 times, etc. Then draw random samples of 40 from that vector:

> grp <- rep(mydat$group, mydat$count)
> grp.sam <- sample(grp, 40)
> table(grp.sam)
grp.sam
G1 G2 G3 G4 G5 
10  9  5 13  3

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352


-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
Sent: Monday, February 11, 2019 4:36 PM
To: r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
Subject: [R] Select

Hi all,

I have a data frame  with tow variables  group and its size.
mydat<- read.table( text='group  count
G1 25
G2 15
G3 12
G4 31
G5 10' , header = TRUE, as.is = TRUE )

I want to select   group ID randomly (without replacement)  until  the
sum of count reaches 40.
So, in  the first case, the data frame could be
   G4 31
   65 10

In other case, it could be
  G5 10
  G2 15
  G3 12

How do I put sum of count variable   is  a minimum of 40 restriction?

Than k you in advance






I want to select group  ids randomly until I reach the

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From v@|kremk @end|ng |rom gm@||@com  Tue Feb 12 00:00:15 2019
From: v@|kremk @end|ng |rom gm@||@com (Val)
Date: Mon, 11 Feb 2019 17:00:15 -0600
Subject: [R] Select
In-Reply-To: <7850e860f2d74c7190d81c87f9b64cb9@tamu.edu>
References: <CAJOiR6bB7FGFV3yuQoL_dwGAZ+SVsBDvP_jY=Ug-AAEDMBRsgw@mail.gmail.com>
 <7850e860f2d74c7190d81c87f9b64cb9@tamu.edu>
Message-ID: <CAJOiR6ZjpcaXH0f0Ls+EbYpjvZGg15CaUrpySGh2R+DXJPwABg@mail.gmail.com>

Thank you David.

However, this will not work for me. If the group ID selected then all
of its observation should be included.

On Mon, Feb 11, 2019 at 4:51 PM David L Carlson <dcarlson at tamu.edu> wrote:
>
> First expand your data frame into a vector where G1 is repeated 25 times, G2 is repeated 15 times, etc. Then draw random samples of 40 from that vector:
>
> > grp <- rep(mydat$group, mydat$count)
> > grp.sam <- sample(grp, 40)
> > table(grp.sam)
> grp.sam
> G1 G2 G3 G4 G5
> 10  9  5 13  3
>
> ----------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77843-4352
>
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
> Sent: Monday, February 11, 2019 4:36 PM
> To: r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
> Subject: [R] Select
>
> Hi all,
>
> I have a data frame  with tow variables  group and its size.
> mydat<- read.table( text='group  count
> G1 25
> G2 15
> G3 12
> G4 31
> G5 10' , header = TRUE, as.is = TRUE )
>
> I want to select   group ID randomly (without replacement)  until  the
> sum of count reaches 40.
> So, in  the first case, the data frame could be
>    G4 31
>    65 10
>
> In other case, it could be
>   G5 10
>   G2 15
>   G3 12
>
> How do I put sum of count variable   is  a minimum of 40 restriction?
>
> Than k you in advance
>
>
>
>
>
>
> I want to select group  ids randomly until I reach the
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue Feb 12 00:09:12 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 11 Feb 2019 15:09:12 -0800
Subject: [R] Select
In-Reply-To: <CAJOiR6ZjpcaXH0f0Ls+EbYpjvZGg15CaUrpySGh2R+DXJPwABg@mail.gmail.com>
References: <CAJOiR6bB7FGFV3yuQoL_dwGAZ+SVsBDvP_jY=Ug-AAEDMBRsgw@mail.gmail.com>
 <7850e860f2d74c7190d81c87f9b64cb9@tamu.edu>
 <CAJOiR6ZjpcaXH0f0Ls+EbYpjvZGg15CaUrpySGh2R+DXJPwABg@mail.gmail.com>
Message-ID: <3C4CA8B7-0D22-4A2D-9A3C-5B32A1E7D71F@dcn.davis.ca.us>

This constraint was not clear in your original sample data set. Can you expand the data set to clarify how this requirement REALLY works?

On February 11, 2019 3:00:15 PM PST, Val <valkremk at gmail.com> wrote:
>Thank you David.
>
>However, this will not work for me. If the group ID selected then all
>of its observation should be included.
>
>On Mon, Feb 11, 2019 at 4:51 PM David L Carlson <dcarlson at tamu.edu>
>wrote:
>>
>> First expand your data frame into a vector where G1 is repeated 25
>times, G2 is repeated 15 times, etc. Then draw random samples of 40
>from that vector:
>>
>> > grp <- rep(mydat$group, mydat$count)
>> > grp.sam <- sample(grp, 40)
>> > table(grp.sam)
>> grp.sam
>> G1 G2 G3 G4 G5
>> 10  9  5 13  3
>>
>> ----------------------------------------
>> David L Carlson
>> Department of Anthropology
>> Texas A&M University
>> College Station, TX 77843-4352
>>
>>
>> -----Original Message-----
>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
>> Sent: Monday, February 11, 2019 4:36 PM
>> To: r-help at R-project.org (r-help at r-project.org)
><r-help at r-project.org>
>> Subject: [R] Select
>>
>> Hi all,
>>
>> I have a data frame  with tow variables  group and its size.
>> mydat<- read.table( text='group  count
>> G1 25
>> G2 15
>> G3 12
>> G4 31
>> G5 10' , header = TRUE, as.is = TRUE )
>>
>> I want to select   group ID randomly (without replacement)  until 
>the
>> sum of count reaches 40.
>> So, in  the first case, the data frame could be
>>    G4 31
>>    65 10
>>
>> In other case, it could be
>>   G5 10
>>   G2 15
>>   G3 12
>>
>> How do I put sum of count variable   is  a minimum of 40 restriction?
>>
>> Than k you in advance
>>
>>
>>
>>
>>
>>
>> I want to select group  ids randomly until I reach the
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From gor@n@bro@trom @end|ng |rom umu@@e  Tue Feb 12 00:28:56 2019
From: gor@n@bro@trom @end|ng |rom umu@@e (=?UTF-8?Q?G=c3=b6ran_Brostr=c3=b6m?=)
Date: Tue, 12 Feb 2019 00:28:56 +0100
Subject: [R] Select
In-Reply-To: <CAJOiR6bB7FGFV3yuQoL_dwGAZ+SVsBDvP_jY=Ug-AAEDMBRsgw@mail.gmail.com>
References: <CAJOiR6bB7FGFV3yuQoL_dwGAZ+SVsBDvP_jY=Ug-AAEDMBRsgw@mail.gmail.com>
Message-ID: <e552afed-5ff5-f541-9ca4-1799829a7f80@umu.se>



On 2019-02-11 23:35, Val wrote:
> Hi all,
> 
> I have a data frame  with tow variables  group and its size.
> mydat<- read.table( text='group  count
> G1 25
> G2 15
> G3 12
> G4 31
> G5 10' , header = TRUE, as.is = TRUE )
> 

How about

x <- sample(1:5)

total <- mydat$count[x[1]]
i <- 1
while (total < 40){
     i <- i + 1
     total <- total + mydat$count[x[i]]
}

print(mydat$group[x[1:i]])

G?ran


> I want to select   group ID randomly (without replacement)  until  the
> sum of count reaches 40.
> So, in  the first case, the data frame could be
>     G4 31
>     65 10
> 
> In other case, it could be
>    G5 10
>    G2 15
>    G3 12
> 
> How do I put sum of count variable   is  a minimum of 40 restriction?
> 
> Than k you in advance
> 
> 
> 
> 
> 
> 
> I want to select group  ids randomly until I reach the
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From v@|kremk @end|ng |rom gm@||@com  Tue Feb 12 00:31:27 2019
From: v@|kremk @end|ng |rom gm@||@com (Val)
Date: Mon, 11 Feb 2019 17:31:27 -0600
Subject: [R] Select
In-Reply-To: <3C4CA8B7-0D22-4A2D-9A3C-5B32A1E7D71F@dcn.davis.ca.us>
References: <CAJOiR6bB7FGFV3yuQoL_dwGAZ+SVsBDvP_jY=Ug-AAEDMBRsgw@mail.gmail.com>
 <7850e860f2d74c7190d81c87f9b64cb9@tamu.edu>
 <CAJOiR6ZjpcaXH0f0Ls+EbYpjvZGg15CaUrpySGh2R+DXJPwABg@mail.gmail.com>
 <3C4CA8B7-0D22-4A2D-9A3C-5B32A1E7D71F@dcn.davis.ca.us>
Message-ID: <CAJOiR6b-zeEufpJUfkbOBY-qh0F-_mHJ1=+Kzt20-EyR7eOyFA@mail.gmail.com>

Sorry Jeff and David  for not being clear!

The total sample size should be at least 40, but the selection should
be based on group ID.  A different combination of Group ID could give
 at least  40.
If I select  group G1   with 25  count and  G2  and with 15  counts
then   I can get  a minimum of 40  counts.   So G1 and G2 are
selected.
G1  25
G2  15

In another scenario, if G2, G3 and G4  are  selected  then the total
count will be 58 which is  greater than 40. So G2 , G3 and G4  could
be selected.
 G2 15
 G3 12
 G4 31

So the restriction is to  find group IDs  that give a minim of  40.
Once, I reached a minim of 40 then stop selecting group  and output
the data..

I am hope this helps




On Mon, Feb 11, 2019 at 5:09 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>
> This constraint was not clear in your original sample data set. Can you expand the data set to clarify how this requirement REALLY works?
>
> On February 11, 2019 3:00:15 PM PST, Val <valkremk at gmail.com> wrote:
> >Thank you David.
> >
> >However, this will not work for me. If the group ID selected then all
> >of its observation should be included.
> >
> >On Mon, Feb 11, 2019 at 4:51 PM David L Carlson <dcarlson at tamu.edu>
> >wrote:
> >>
> >> First expand your data frame into a vector where G1 is repeated 25
> >times, G2 is repeated 15 times, etc. Then draw random samples of 40
> >from that vector:
> >>
> >> > grp <- rep(mydat$group, mydat$count)
> >> > grp.sam <- sample(grp, 40)
> >> > table(grp.sam)
> >> grp.sam
> >> G1 G2 G3 G4 G5
> >> 10  9  5 13  3
> >>
> >> ----------------------------------------
> >> David L Carlson
> >> Department of Anthropology
> >> Texas A&M University
> >> College Station, TX 77843-4352
> >>
> >>
> >> -----Original Message-----
> >> From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
> >> Sent: Monday, February 11, 2019 4:36 PM
> >> To: r-help at R-project.org (r-help at r-project.org)
> ><r-help at r-project.org>
> >> Subject: [R] Select
> >>
> >> Hi all,
> >>
> >> I have a data frame  with tow variables  group and its size.
> >> mydat<- read.table( text='group  count
> >> G1 25
> >> G2 15
> >> G3 12
> >> G4 31
> >> G5 10' , header = TRUE, as.is = TRUE )
> >>
> >> I want to select   group ID randomly (without replacement)  until
> >the
> >> sum of count reaches 40.
> >> So, in  the first case, the data frame could be
> >>    G4 31
> >>    65 10
> >>
> >> In other case, it could be
> >>   G5 10
> >>   G2 15
> >>   G3 12
> >>
> >> How do I put sum of count variable   is  a minimum of 40 restriction?
> >>
> >> Than k you in advance
> >>
> >>
> >>
> >>
> >>
> >>
> >> I want to select group  ids randomly until I reach the
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue Feb 12 01:26:17 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 11 Feb 2019 16:26:17 -0800 (PST)
Subject: [R] Select
In-Reply-To: <CAJOiR6b-zeEufpJUfkbOBY-qh0F-_mHJ1=+Kzt20-EyR7eOyFA@mail.gmail.com>
References: <CAJOiR6bB7FGFV3yuQoL_dwGAZ+SVsBDvP_jY=Ug-AAEDMBRsgw@mail.gmail.com>
 <7850e860f2d74c7190d81c87f9b64cb9@tamu.edu>
 <CAJOiR6ZjpcaXH0f0Ls+EbYpjvZGg15CaUrpySGh2R+DXJPwABg@mail.gmail.com>
 <3C4CA8B7-0D22-4A2D-9A3C-5B32A1E7D71F@dcn.davis.ca.us>
 <CAJOiR6b-zeEufpJUfkbOBY-qh0F-_mHJ1=+Kzt20-EyR7eOyFA@mail.gmail.com>
Message-ID: <alpine.BSF.2.00.1902111623480.75683@pedal.dcn.davis.ca.us>

N <- 8 # however many times you want to do this
ans <- lapply( seq.int( N )
              , function( n ) {
                  idx <- sample( nrow( mydat ) )
                  mydat[ idx[ seq.int( which( 40 < cumsum( mydat[ idx, "count" ] ) )[ 1 ] ) ], ]
                }
              )


On Mon, 11 Feb 2019, Val wrote:

> Sorry Jeff and David  for not being clear!
>
> The total sample size should be at least 40, but the selection should
> be based on group ID.  A different combination of Group ID could give
> at least  40.
> If I select  group G1   with 25  count and  G2  and with 15  counts
> then   I can get  a minimum of 40  counts.   So G1 and G2 are
> selected.
> G1  25
> G2  15
>
> In another scenario, if G2, G3 and G4  are  selected  then the total
> count will be 58 which is  greater than 40. So G2 , G3 and G4  could
> be selected.
> G2 15
> G3 12
> G4 31
>
> So the restriction is to  find group IDs  that give a minim of  40.
> Once, I reached a minim of 40 then stop selecting group  and output
> the data..
>
> I am hope this helps
>
>
>
>
> On Mon, Feb 11, 2019 at 5:09 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>
>> This constraint was not clear in your original sample data set. Can you expand the data set to clarify how this requirement REALLY works?
>>
>> On February 11, 2019 3:00:15 PM PST, Val <valkremk at gmail.com> wrote:
>>> Thank you David.
>>>
>>> However, this will not work for me. If the group ID selected then all
>>> of its observation should be included.
>>>
>>> On Mon, Feb 11, 2019 at 4:51 PM David L Carlson <dcarlson at tamu.edu>
>>> wrote:
>>>>
>>>> First expand your data frame into a vector where G1 is repeated 25
>>> times, G2 is repeated 15 times, etc. Then draw random samples of 40
>>> from that vector:
>>>>
>>>>> grp <- rep(mydat$group, mydat$count)
>>>>> grp.sam <- sample(grp, 40)
>>>>> table(grp.sam)
>>>> grp.sam
>>>> G1 G2 G3 G4 G5
>>>> 10  9  5 13  3
>>>>
>>>> ----------------------------------------
>>>> David L Carlson
>>>> Department of Anthropology
>>>> Texas A&M University
>>>> College Station, TX 77843-4352
>>>>
>>>>
>>>> -----Original Message-----
>>>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
>>>> Sent: Monday, February 11, 2019 4:36 PM
>>>> To: r-help at R-project.org (r-help at r-project.org)
>>> <r-help at r-project.org>
>>>> Subject: [R] Select
>>>>
>>>> Hi all,
>>>>
>>>> I have a data frame  with tow variables  group and its size.
>>>> mydat<- read.table( text='group  count
>>>> G1 25
>>>> G2 15
>>>> G3 12
>>>> G4 31
>>>> G5 10' , header = TRUE, as.is = TRUE )
>>>>
>>>> I want to select   group ID randomly (without replacement)  until
>>> the
>>>> sum of count reaches 40.
>>>> So, in  the first case, the data frame could be
>>>>    G4 31
>>>>    65 10
>>>>
>>>> In other case, it could be
>>>>   G5 10
>>>>   G2 15
>>>>   G3 12
>>>>
>>>> How do I put sum of count variable   is  a minimum of 40 restriction?
>>>>
>>>> Than k you in advance
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> I want to select group  ids randomly until I reach the
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Sent from my phone. Please excuse my brevity.
>

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From v@|kremk @end|ng |rom gm@||@com  Tue Feb 12 02:17:41 2019
From: v@|kremk @end|ng |rom gm@||@com (Val)
Date: Mon, 11 Feb 2019 19:17:41 -0600
Subject: [R] Select
In-Reply-To: <alpine.BSF.2.00.1902111623480.75683@pedal.dcn.davis.ca.us>
References: <CAJOiR6bB7FGFV3yuQoL_dwGAZ+SVsBDvP_jY=Ug-AAEDMBRsgw@mail.gmail.com>
 <7850e860f2d74c7190d81c87f9b64cb9@tamu.edu>
 <CAJOiR6ZjpcaXH0f0Ls+EbYpjvZGg15CaUrpySGh2R+DXJPwABg@mail.gmail.com>
 <3C4CA8B7-0D22-4A2D-9A3C-5B32A1E7D71F@dcn.davis.ca.us>
 <CAJOiR6b-zeEufpJUfkbOBY-qh0F-_mHJ1=+Kzt20-EyR7eOyFA@mail.gmail.com>
 <alpine.BSF.2.00.1902111623480.75683@pedal.dcn.davis.ca.us>
Message-ID: <CAJOiR6aeDifkbURRQwmWT5V_Zmvz_9TY+OgqyD4=YjQ2Td3Ggw@mail.gmail.com>

Thank you very much Jeff, Goran and David  for your help.


On Mon, Feb 11, 2019 at 6:22 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>
> N <- 8 # however many times you want to do this
> ans <- lapply( seq.int( N )
>               , function( n ) {
>                   idx <- sample( nrow( mydat ) )
>                   mydat[ idx[ seq.int( which( 40 < cumsum( mydat[ idx, "count" ] ) )[ 1 ] ) ], ]
>                 }
>               )
>
>
> On Mon, 11 Feb 2019, Val wrote:
>
> > Sorry Jeff and David  for not being clear!
> >
> > The total sample size should be at least 40, but the selection should
> > be based on group ID.  A different combination of Group ID could give
> > at least  40.
> > If I select  group G1   with 25  count and  G2  and with 15  counts
> > then   I can get  a minimum of 40  counts.   So G1 and G2 are
> > selected.
> > G1  25
> > G2  15
> >
> > In another scenario, if G2, G3 and G4  are  selected  then the total
> > count will be 58 which is  greater than 40. So G2 , G3 and G4  could
> > be selected.
> > G2 15
> > G3 12
> > G4 31
> >
> > So the restriction is to  find group IDs  that give a minim of  40.
> > Once, I reached a minim of 40 then stop selecting group  and output
> > the data..
> >
> > I am hope this helps
> >
> >
> >
> >
> > On Mon, Feb 11, 2019 at 5:09 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> >>
> >> This constraint was not clear in your original sample data set. Can you expand the data set to clarify how this requirement REALLY works?
> >>
> >> On February 11, 2019 3:00:15 PM PST, Val <valkremk at gmail.com> wrote:
> >>> Thank you David.
> >>>
> >>> However, this will not work for me. If the group ID selected then all
> >>> of its observation should be included.
> >>>
> >>> On Mon, Feb 11, 2019 at 4:51 PM David L Carlson <dcarlson at tamu.edu>
> >>> wrote:
> >>>>
> >>>> First expand your data frame into a vector where G1 is repeated 25
> >>> times, G2 is repeated 15 times, etc. Then draw random samples of 40
> >>> from that vector:
> >>>>
> >>>>> grp <- rep(mydat$group, mydat$count)
> >>>>> grp.sam <- sample(grp, 40)
> >>>>> table(grp.sam)
> >>>> grp.sam
> >>>> G1 G2 G3 G4 G5
> >>>> 10  9  5 13  3
> >>>>
> >>>> ----------------------------------------
> >>>> David L Carlson
> >>>> Department of Anthropology
> >>>> Texas A&M University
> >>>> College Station, TX 77843-4352
> >>>>
> >>>>
> >>>> -----Original Message-----
> >>>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
> >>>> Sent: Monday, February 11, 2019 4:36 PM
> >>>> To: r-help at R-project.org (r-help at r-project.org)
> >>> <r-help at r-project.org>
> >>>> Subject: [R] Select
> >>>>
> >>>> Hi all,
> >>>>
> >>>> I have a data frame  with tow variables  group and its size.
> >>>> mydat<- read.table( text='group  count
> >>>> G1 25
> >>>> G2 15
> >>>> G3 12
> >>>> G4 31
> >>>> G5 10' , header = TRUE, as.is = TRUE )
> >>>>
> >>>> I want to select   group ID randomly (without replacement)  until
> >>> the
> >>>> sum of count reaches 40.
> >>>> So, in  the first case, the data frame could be
> >>>>    G4 31
> >>>>    65 10
> >>>>
> >>>> In other case, it could be
> >>>>   G5 10
> >>>>   G2 15
> >>>>   G3 12
> >>>>
> >>>> How do I put sum of count variable   is  a minimum of 40 restriction?
> >>>>
> >>>> Than k you in advance
> >>>>
> >>>>
> >>>>
> >>>>
> >>>>
> >>>>
> >>>> I want to select group  ids randomly until I reach the
> >>>>
> >>>> ______________________________________________
> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >> --
> >> Sent from my phone. Please excuse my brevity.
> >
>
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>                                        Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------


From or|o|eb@|t|more @end|ng |rom gm@||@com  Tue Feb 12 02:26:35 2019
From: or|o|eb@|t|more @end|ng |rom gm@||@com (Adrian Johnson)
Date: Mon, 11 Feb 2019 20:26:35 -0500
Subject: [R] faster execution of for loop in Fishers test
Message-ID: <CAL2fYnMkVOvHz19FNEsU6KDS_vqKMkSCyvtB_S9t4TmKN5wzUg@mail.gmail.com>

Dear group,

I have two large matrices.

Matrix one: is 24776 x 76 (example toy1 dput object given below)

Matrix two: is 12913 x 76 (example toy2 dput object given below)

Column names of both matrices are identical.

My aim is:

a. Take each row of toy2 and transform vector into UP (>0)  and DN (
<0 ) categories. (kc)
b  Test association between kc and every row of toy1.

My code, given below, although this works but is very slow.

I gave dput objects for toy1, toy2 and result matrix.

Could you suggest/help me how I can make this faster.  Also, how can I
select values in result column that are less than 0.001 (p < 0.001).

Appreciate your help. Thank you.
-Adrian

Code:
===============================================================================



result <- matrix(NA,nrow=nrow(toy1),ncol=nrow(toy2))

rownames(result) <- rownames(toy1)
colnames(result) <- rownames(toy2)

for(i in 1:nrow(toy2)){
for(j in 1:nrow(toy1)){
kx = toy2[i,]
kc <- rep('NC',length(kx))
kc[ kx >0] <- 'UP'
kc[ kx <=0 ] <- 'DN'
xpv <- fisher.test(table(kc,toy1[j,]),simulate.p.value = TRUE)$p.value
result[j,i] <- xpv
}
}

===============================================================================


===============================================================================


> dput(toy1)
structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1,
-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
-1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1,
-1, -1, -1, -1, -1), .Dim = c(10L, 7L), .Dimnames = list(c("ACAP3",
"ACTRT2", "AGRN", "ANKRD65", "ATAD3A", "ATAD3B", "ATAD3C", "AURKAIP1",
"B3GALT6", "C1orf159"), c("a", "b", "c", "d", "e", "f", "g")))



> dput(toy2)
structure(c(-0.242891119688613, -0.0514058216682132, 0.138447212993773,
-0.312576648033122, 0.271489918720452, -0.281196468299486, -0.0407160143344565,
-0.328353812845287, 0.151667836674511, 0.408596843743938, -0.049351944902924,
0.238586287349249, 0.200571558784821, -0.0737604184858411, 0.245971526254877,
0.24740263959845, -0.161528943131908, 0.197521973013793, 0.0402668125708444,
0.376323735212088, 0.0731550871764204, 0.385270176969893, 0.28953042756208,
0.062587289401188, -0.281187168932979, -0.0202298984561554, -0.0848696970309447,
0.0349676726358973, -0.520484215644868, -0.481991414222996,
-0.00698099201388211,
0.135503878341873, 0.156983081312087, 0.320223832092661, 0.34582193394074,
0.0844455960468667, -0.157825604090972, 0.204758250510969, 0.261796072978612,
-0.19510450641405, 0.43196474472874, -0.211155577453175, -0.0921641871215187,
0.420950361292263, 0.390261862151936, -0.422273930504427, 0.344653684951627,
0.0378273248838503, 0.197782027324611, 0.0963124876309569, 0.332093167080656,
0.128036554821915, -0.41338065859335, -0.409470440033177, 0.371490567256253,
-0.0912549189140141, -0.247451812684234, 0.127741739114639, 0.0856254238844557,
0.515282940316031, -0.25675759521248, 0.333943163209869, 0.604141413840881,
0.0824942299510931, -0.179605710473021, -0.275604207054643, -0.113251154591898,
0.172897837449258, -0.329808795076691, -0.239255324324506), .Dim = c(10L,
7L), .Dimnames = list(c("chr5q23", "chr16q24", "chr8q24", "chr13q11",
"chr7p21", "chr10q23", "chr13q13", "chr10q21", "chr1p13", "chrxp21"
), c("a", "b", "c", "d", "e", "f", "g")))
>


> dput(result)
structure(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.532733633183408,
0.511244377811094, 0.528235882058971, 0.526736631684158, 0.51424287856072,
0.530734632683658, 0.513243378310845, 0.533233383308346, 0.542228885557221,
0.517241379310345, 0.532733633183408, 0.521739130434783, 0.529235382308846,
0.530234882558721, 0.548725637181409, 0.525737131434283, 0.527236381809095,
0.532733633183408, 0.530234882558721, 0.520739630184908, 0.15592203898051,
0.142928535732134, 0.140929535232384, 0.150924537731134, 0.160419790104948,
0.139430284857571, 0.152923538230885, 0.146426786606697, 0.149425287356322,
0.145427286356822, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.282358820589705,
0.293853073463268, 0.262868565717141, 0.290854572713643, 0.276861569215392,
0.288855572213893, 0.282358820589705, 0.292853573213393, 0.286356821589205,
0.271364317841079, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1), .Dim = c(10L, 10L), .Dimnames = list(c("ACAP3",
"ACTRT2", "AGRN", "ANKRD65", "ATAD3A", "ATAD3B", "ATAD3C", "AURKAIP1",
"B3GALT6", "C1orf159"), c("chr5q23", "chr16q24", "chr8q24", "chr13q11",
"chr7p21", "chr10q23", "chr13q13", "chr10q21", "chr1p13", "chrxp21"
)))


From bgunter@4567 @end|ng |rom gm@||@com  Tue Feb 12 02:45:44 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 11 Feb 2019 17:45:44 -0800
Subject: [R] faster execution of for loop in Fishers test
In-Reply-To: <CAL2fYnMkVOvHz19FNEsU6KDS_vqKMkSCyvtB_S9t4TmKN5wzUg@mail.gmail.com>
References: <CAL2fYnMkVOvHz19FNEsU6KDS_vqKMkSCyvtB_S9t4TmKN5wzUg@mail.gmail.com>
Message-ID: <CAGxFJbTnmJNPEM1_4VeOnuM3Vc7JExGVu578UzetNWb2cn_azA@mail.gmail.com>

1. I believe Fisher's exact test is computationally intensive and takes a
lot of time for large structures, so I would say what you see is what you
should expect! (As I'm not an expert on this, confirmation or contradiction
by those who are would be appreciated).

2. Your second question on how to select results based on values in another
vector/column is very basic R. So it appears that you need to spend some
time with an R tutorial or two to learn the basics (unless I have
misinterpreted).

3. Please do not repost further. No one is obligated to respond to your
posts. Following the posting guide, which you appear to have done,
increases the likelihood, but is of course no guarantee.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Feb 11, 2019 at 5:28 PM Adrian Johnson <oriolebaltimore at gmail.com>
wrote:

> Dear group,
>
> I have two large matrices.
>
> Matrix one: is 24776 x 76 (example toy1 dput object given below)
>
> Matrix two: is 12913 x 76 (example toy2 dput object given below)
>
> Column names of both matrices are identical.
>
> My aim is:
>
> a. Take each row of toy2 and transform vector into UP (>0)  and DN (
> <0 ) categories. (kc)
> b  Test association between kc and every row of toy1.
>
> My code, given below, although this works but is very slow.
>
> I gave dput objects for toy1, toy2 and result matrix.
>
> Could you suggest/help me how I can make this faster.  Also, how can I
> select values in result column that are less than 0.001 (p < 0.001).
>
> Appreciate your help. Thank you.
> -Adrian
>
> Code:
>
> ===============================================================================
>
>
>
> result <- matrix(NA,nrow=nrow(toy1),ncol=nrow(toy2))
>
> rownames(result) <- rownames(toy1)
> colnames(result) <- rownames(toy2)
>
> for(i in 1:nrow(toy2)){
> for(j in 1:nrow(toy1)){
> kx = toy2[i,]
> kc <- rep('NC',length(kx))
> kc[ kx >0] <- 'UP'
> kc[ kx <=0 ] <- 'DN'
> xpv <- fisher.test(table(kc,toy1[j,]),simulate.p.value = TRUE)$p.value
> result[j,i] <- xpv
> }
> }
>
>
> ===============================================================================
>
>
>
> ===============================================================================
>
>
> > dput(toy1)
> structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1,
> -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
> -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
> -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1,
> -1, -1, -1, -1, -1), .Dim = c(10L, 7L), .Dimnames = list(c("ACAP3",
> "ACTRT2", "AGRN", "ANKRD65", "ATAD3A", "ATAD3B", "ATAD3C", "AURKAIP1",
> "B3GALT6", "C1orf159"), c("a", "b", "c", "d", "e", "f", "g")))
>
>
>
> > dput(toy2)
> structure(c(-0.242891119688613, -0.0514058216682132, 0.138447212993773,
> -0.312576648033122, 0.271489918720452, -0.281196468299486,
> -0.0407160143344565,
> -0.328353812845287, 0.151667836674511, 0.408596843743938,
> -0.049351944902924,
> 0.238586287349249, 0.200571558784821, -0.0737604184858411,
> 0.245971526254877,
> 0.24740263959845, -0.161528943131908, 0.197521973013793,
> 0.0402668125708444,
> 0.376323735212088, 0.0731550871764204, 0.385270176969893, 0.28953042756208,
> 0.062587289401188, -0.281187168932979, -0.0202298984561554,
> -0.0848696970309447,
> 0.0349676726358973, -0.520484215644868, -0.481991414222996,
> -0.00698099201388211,
> 0.135503878341873, 0.156983081312087, 0.320223832092661, 0.34582193394074,
> 0.0844455960468667, -0.157825604090972, 0.204758250510969,
> 0.261796072978612,
> -0.19510450641405, 0.43196474472874, -0.211155577453175,
> -0.0921641871215187,
> 0.420950361292263, 0.390261862151936, -0.422273930504427,
> 0.344653684951627,
> 0.0378273248838503, 0.197782027324611, 0.0963124876309569,
> 0.332093167080656,
> 0.128036554821915, -0.41338065859335, -0.409470440033177,
> 0.371490567256253,
> -0.0912549189140141, -0.247451812684234, 0.127741739114639,
> 0.0856254238844557,
> 0.515282940316031, -0.25675759521248, 0.333943163209869, 0.604141413840881,
> 0.0824942299510931, -0.179605710473021, -0.275604207054643,
> -0.113251154591898,
> 0.172897837449258, -0.329808795076691, -0.239255324324506), .Dim = c(10L,
> 7L), .Dimnames = list(c("chr5q23", "chr16q24", "chr8q24", "chr13q11",
> "chr7p21", "chr10q23", "chr13q13", "chr10q21", "chr1p13", "chrxp21"
> ), c("a", "b", "c", "d", "e", "f", "g")))
> >
>
>
> > dput(result)
> structure(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.532733633183408,
> 0.511244377811094, 0.528235882058971, 0.526736631684158, 0.51424287856072,
> 0.530734632683658, 0.513243378310845, 0.533233383308346, 0.542228885557221,
> 0.517241379310345, 0.532733633183408, 0.521739130434783, 0.529235382308846,
> 0.530234882558721, 0.548725637181409, 0.525737131434283, 0.527236381809095,
> 0.532733633183408, 0.530234882558721, 0.520739630184908, 0.15592203898051,
> 0.142928535732134, 0.140929535232384, 0.150924537731134, 0.160419790104948,
> 0.139430284857571, 0.152923538230885, 0.146426786606697, 0.149425287356322,
> 0.145427286356822, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.282358820589705,
> 0.293853073463268, 0.262868565717141, 0.290854572713643, 0.276861569215392,
> 0.288855572213893, 0.282358820589705, 0.292853573213393, 0.286356821589205,
> 0.271364317841079, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1), .Dim = c(10L, 10L), .Dimnames = list(c("ACAP3",
> "ACTRT2", "AGRN", "ANKRD65", "ATAD3A", "ATAD3B", "ATAD3C", "AURKAIP1",
> "B3GALT6", "C1orf159"), c("chr5q23", "chr16q24", "chr8q24", "chr13q11",
> "chr7p21", "chr10q23", "chr13q13", "chr10q21", "chr1p13", "chrxp21"
> )))
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com  Tue Feb 12 03:18:14 2019
From: m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com (Patrick (Malone Quantitative))
Date: Mon, 11 Feb 2019 21:18:14 -0500
Subject: [R] faster execution of for loop in Fishers test
In-Reply-To: <CAGxFJbTnmJNPEM1_4VeOnuM3Vc7JExGVu578UzetNWb2cn_azA@mail.gmail.com>
References: <CAL2fYnMkVOvHz19FNEsU6KDS_vqKMkSCyvtB_S9t4TmKN5wzUg@mail.gmail.com>
 <CAGxFJbTnmJNPEM1_4VeOnuM3Vc7JExGVu578UzetNWb2cn_azA@mail.gmail.com>
Message-ID: <DB6730D7-70AB-4894-9E42-3018869476EE@malonequantitative.com>

Point 1 confirmed. It's an exhaustive permutation test.

?On 2/11/19, 8:46 PM, "R-help on behalf of Bert Gunter" <r-help-bounces at r-project.org on behalf of bgunter.4567 at gmail.com> wrote:

    1. I believe Fisher's exact test is computationally intensive and takes a
    lot of time for large structures, so I would say what you see is what you
    should expect! (As I'm not an expert on this, confirmation or contradiction
    by those who are would be appreciated).
    
    2. Your second question on how to select results based on values in another
    vector/column is very basic R. So it appears that you need to spend some
    time with an R tutorial or two to learn the basics (unless I have
    misinterpreted).
    
    3. Please do not repost further. No one is obligated to respond to your
    posts. Following the posting guide, which you appear to have done,
    increases the likelihood, but is of course no guarantee.
    
    Cheers,
    Bert
    
    
    Bert Gunter
    
    "The trouble with having an open mind is that people keep coming along and
    sticking things into it."
    -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
    
    
    On Mon, Feb 11, 2019 at 5:28 PM Adrian Johnson <oriolebaltimore at gmail.com>
    wrote:
    
    > Dear group,
    >
    > I have two large matrices.
    >
    > Matrix one: is 24776 x 76 (example toy1 dput object given below)
    >
    > Matrix two: is 12913 x 76 (example toy2 dput object given below)
    >
    > Column names of both matrices are identical.
    >
    > My aim is:
    >
    > a. Take each row of toy2 and transform vector into UP (>0)  and DN (
    > <0 ) categories. (kc)
    > b  Test association between kc and every row of toy1.
    >
    > My code, given below, although this works but is very slow.
    >
    > I gave dput objects for toy1, toy2 and result matrix.
    >
    > Could you suggest/help me how I can make this faster.  Also, how can I
    > select values in result column that are less than 0.001 (p < 0.001).
    >
    > Appreciate your help. Thank you.
    > -Adrian
    >
    > Code:
    >
    > ===============================================================================
    >
    >
    >
    > result <- matrix(NA,nrow=nrow(toy1),ncol=nrow(toy2))
    >
    > rownames(result) <- rownames(toy1)
    > colnames(result) <- rownames(toy2)
    >
    > for(i in 1:nrow(toy2)){
    > for(j in 1:nrow(toy1)){
    > kx = toy2[i,]
    > kc <- rep('NC',length(kx))
    > kc[ kx >0] <- 'UP'
    > kc[ kx <=0 ] <- 'DN'
    > xpv <- fisher.test(table(kc,toy1[j,]),simulate.p.value = TRUE)$p.value
    > result[j,i] <- xpv
    > }
    > }
    >
    >
    > ===============================================================================
    >
    >
    >
    > ===============================================================================
    >
    >
    > > dput(toy1)
    > structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1,
    > -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
    > -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
    > -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1,
    > -1, -1, -1, -1, -1), .Dim = c(10L, 7L), .Dimnames = list(c("ACAP3",
    > "ACTRT2", "AGRN", "ANKRD65", "ATAD3A", "ATAD3B", "ATAD3C", "AURKAIP1",
    > "B3GALT6", "C1orf159"), c("a", "b", "c", "d", "e", "f", "g")))
    >
    >
    >
    > > dput(toy2)
    > structure(c(-0.242891119688613, -0.0514058216682132, 0.138447212993773,
    > -0.312576648033122, 0.271489918720452, -0.281196468299486,
    > -0.0407160143344565,
    > -0.328353812845287, 0.151667836674511, 0.408596843743938,
    > -0.049351944902924,
    > 0.238586287349249, 0.200571558784821, -0.0737604184858411,
    > 0.245971526254877,
    > 0.24740263959845, -0.161528943131908, 0.197521973013793,
    > 0.0402668125708444,
    > 0.376323735212088, 0.0731550871764204, 0.385270176969893, 0.28953042756208,
    > 0.062587289401188, -0.281187168932979, -0.0202298984561554,
    > -0.0848696970309447,
    > 0.0349676726358973, -0.520484215644868, -0.481991414222996,
    > -0.00698099201388211,
    > 0.135503878341873, 0.156983081312087, 0.320223832092661, 0.34582193394074,
    > 0.0844455960468667, -0.157825604090972, 0.204758250510969,
    > 0.261796072978612,
    > -0.19510450641405, 0.43196474472874, -0.211155577453175,
    > -0.0921641871215187,
    > 0.420950361292263, 0.390261862151936, -0.422273930504427,
    > 0.344653684951627,
    > 0.0378273248838503, 0.197782027324611, 0.0963124876309569,
    > 0.332093167080656,
    > 0.128036554821915, -0.41338065859335, -0.409470440033177,
    > 0.371490567256253,
    > -0.0912549189140141, -0.247451812684234, 0.127741739114639,
    > 0.0856254238844557,
    > 0.515282940316031, -0.25675759521248, 0.333943163209869, 0.604141413840881,
    > 0.0824942299510931, -0.179605710473021, -0.275604207054643,
    > -0.113251154591898,
    > 0.172897837449258, -0.329808795076691, -0.239255324324506), .Dim = c(10L,
    > 7L), .Dimnames = list(c("chr5q23", "chr16q24", "chr8q24", "chr13q11",
    > "chr7p21", "chr10q23", "chr13q13", "chr10q21", "chr1p13", "chrxp21"
    > ), c("a", "b", "c", "d", "e", "f", "g")))
    > >
    >
    >
    > > dput(result)
    > structure(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.532733633183408,
    > 0.511244377811094, 0.528235882058971, 0.526736631684158, 0.51424287856072,
    > 0.530734632683658, 0.513243378310845, 0.533233383308346, 0.542228885557221,
    > 0.517241379310345, 0.532733633183408, 0.521739130434783, 0.529235382308846,
    > 0.530234882558721, 0.548725637181409, 0.525737131434283, 0.527236381809095,
    > 0.532733633183408, 0.530234882558721, 0.520739630184908, 0.15592203898051,
    > 0.142928535732134, 0.140929535232384, 0.150924537731134, 0.160419790104948,
    > 0.139430284857571, 0.152923538230885, 0.146426786606697, 0.149425287356322,
    > 0.145427286356822, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
    > 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.282358820589705,
    > 0.293853073463268, 0.262868565717141, 0.290854572713643, 0.276861569215392,
    > 0.288855572213893, 0.282358820589705, 0.292853573213393, 0.286356821589205,
    > 0.271364317841079, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
    > 1, 1, 1, 1, 1, 1), .Dim = c(10L, 10L), .Dimnames = list(c("ACAP3",
    > "ACTRT2", "AGRN", "ANKRD65", "ATAD3A", "ATAD3B", "ATAD3C", "AURKAIP1",
    > "B3GALT6", "C1orf159"), c("chr5q23", "chr16q24", "chr8q24", "chr13q11",
    > "chr7p21", "chr10q23", "chr13q13", "chr10q21", "chr1p13", "chrxp21"
    > )))
    >
    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide
    > http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.
    >
    
    	[[alternative HTML version deleted]]
    
    ______________________________________________
    R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    https://stat.ethz.ch/mailman/listinfo/r-help
    PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    and provide commented, minimal, self-contained, reproducible code.
    


From pd@|gd @end|ng |rom gm@||@com  Tue Feb 12 13:10:53 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Tue, 12 Feb 2019 13:10:53 +0100
Subject: [R] faster execution of for loop in Fishers test
In-Reply-To: <CAGxFJbTnmJNPEM1_4VeOnuM3Vc7JExGVu578UzetNWb2cn_azA@mail.gmail.com>
References: <CAL2fYnMkVOvHz19FNEsU6KDS_vqKMkSCyvtB_S9t4TmKN5wzUg@mail.gmail.com>
 <CAGxFJbTnmJNPEM1_4VeOnuM3Vc7JExGVu578UzetNWb2cn_azA@mail.gmail.com>
Message-ID: <0FFAB494-8DE0-4818-BD0F-EB838C2647D9@gmail.com>



> On 12 Feb 2019, at 02:45 , Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> 1. I believe Fisher's exact test is computationally intensive and takes a
> lot of time for large structures, so I would say what you see is what you
> should expect! (As I'm not an expert on this, confirmation or contradiction
> by those who are would be appreciated).
> 


As I read it, it is mainly 24776 * 12913 = "a lot" of 3x2 tables (320 million of them). Fisher.test has a fair amount of red-tape overhead, so brute force would take a while.

Some observations: All tables have a total of 76, so there is only a limited number of possible tables (but will kx always have only three possible values?), so there could be scope for using lookup tables. Also, if it is always 3x2, I think simulation is slower than exact computation.

-pd 

> 2. Your second question on how to select results based on values in another
> vector/column is very basic R. So it appears that you need to spend some
> time with an R tutorial or two to learn the basics (unless I have
> misinterpreted).
> 
> 3. Please do not repost further. No one is obligated to respond to your
> posts. Following the posting guide, which you appear to have done,
> increases the likelihood, but is of course no guarantee.
> 
> Cheers,
> Bert
> 
> 
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> 
> On Mon, Feb 11, 2019 at 5:28 PM Adrian Johnson <oriolebaltimore at gmail.com>
> wrote:
> 
>> Dear group,
>> 
>> I have two large matrices.
>> 
>> Matrix one: is 24776 x 76 (example toy1 dput object given below)
>> 
>> Matrix two: is 12913 x 76 (example toy2 dput object given below)
>> 
>> Column names of both matrices are identical.
>> 
>> My aim is:
>> 
>> a. Take each row of toy2 and transform vector into UP (>0)  and DN (
>> <0 ) categories. (kc)
>> b  Test association between kc and every row of toy1.
>> 
>> My code, given below, although this works but is very slow.
>> 
>> I gave dput objects for toy1, toy2 and result matrix.
>> 
>> Could you suggest/help me how I can make this faster.  Also, how can I
>> select values in result column that are less than 0.001 (p < 0.001).
>> 
>> Appreciate your help. Thank you.
>> -Adrian
>> 
>> Code:
>> 
>> ===============================================================================
>> 
>> 
>> 
>> result <- matrix(NA,nrow=nrow(toy1),ncol=nrow(toy2))
>> 
>> rownames(result) <- rownames(toy1)
>> colnames(result) <- rownames(toy2)
>> 
>> for(i in 1:nrow(toy2)){
>> for(j in 1:nrow(toy1)){
>> kx = toy2[i,]
>> kc <- rep('NC',length(kx))
>> kc[ kx >0] <- 'UP'
>> kc[ kx <=0 ] <- 'DN'
>> xpv <- fisher.test(table(kc,toy1[j,]),simulate.p.value = TRUE)$p.value
>> result[j,i] <- xpv
>> }
>> }
>> 
>> 
>> ===============================================================================
>> 
>> 
>> 
>> ===============================================================================
>> 
>> 
>>> dput(toy1)
>> structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1,
>> -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
>> -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
>> -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1,
>> -1, -1, -1, -1, -1), .Dim = c(10L, 7L), .Dimnames = list(c("ACAP3",
>> "ACTRT2", "AGRN", "ANKRD65", "ATAD3A", "ATAD3B", "ATAD3C", "AURKAIP1",
>> "B3GALT6", "C1orf159"), c("a", "b", "c", "d", "e", "f", "g")))
>> 
>> 
>> 
>>> dput(toy2)
>> structure(c(-0.242891119688613, -0.0514058216682132, 0.138447212993773,
>> -0.312576648033122, 0.271489918720452, -0.281196468299486,
>> -0.0407160143344565,
>> -0.328353812845287, 0.151667836674511, 0.408596843743938,
>> -0.049351944902924,
>> 0.238586287349249, 0.200571558784821, -0.0737604184858411,
>> 0.245971526254877,
>> 0.24740263959845, -0.161528943131908, 0.197521973013793,
>> 0.0402668125708444,
>> 0.376323735212088, 0.0731550871764204, 0.385270176969893, 0.28953042756208,
>> 0.062587289401188, -0.281187168932979, -0.0202298984561554,
>> -0.0848696970309447,
>> 0.0349676726358973, -0.520484215644868, -0.481991414222996,
>> -0.00698099201388211,
>> 0.135503878341873, 0.156983081312087, 0.320223832092661, 0.34582193394074,
>> 0.0844455960468667, -0.157825604090972, 0.204758250510969,
>> 0.261796072978612,
>> -0.19510450641405, 0.43196474472874, -0.211155577453175,
>> -0.0921641871215187,
>> 0.420950361292263, 0.390261862151936, -0.422273930504427,
>> 0.344653684951627,
>> 0.0378273248838503, 0.197782027324611, 0.0963124876309569,
>> 0.332093167080656,
>> 0.128036554821915, -0.41338065859335, -0.409470440033177,
>> 0.371490567256253,
>> -0.0912549189140141, -0.247451812684234, 0.127741739114639,
>> 0.0856254238844557,
>> 0.515282940316031, -0.25675759521248, 0.333943163209869, 0.604141413840881,
>> 0.0824942299510931, -0.179605710473021, -0.275604207054643,
>> -0.113251154591898,
>> 0.172897837449258, -0.329808795076691, -0.239255324324506), .Dim = c(10L,
>> 7L), .Dimnames = list(c("chr5q23", "chr16q24", "chr8q24", "chr13q11",
>> "chr7p21", "chr10q23", "chr13q13", "chr10q21", "chr1p13", "chrxp21"
>> ), c("a", "b", "c", "d", "e", "f", "g")))
>>> 
>> 
>> 
>>> dput(result)
>> structure(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.532733633183408,
>> 0.511244377811094, 0.528235882058971, 0.526736631684158, 0.51424287856072,
>> 0.530734632683658, 0.513243378310845, 0.533233383308346, 0.542228885557221,
>> 0.517241379310345, 0.532733633183408, 0.521739130434783, 0.529235382308846,
>> 0.530234882558721, 0.548725637181409, 0.525737131434283, 0.527236381809095,
>> 0.532733633183408, 0.530234882558721, 0.520739630184908, 0.15592203898051,
>> 0.142928535732134, 0.140929535232384, 0.150924537731134, 0.160419790104948,
>> 0.139430284857571, 0.152923538230885, 0.146426786606697, 0.149425287356322,
>> 0.145427286356822, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
>> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.282358820589705,
>> 0.293853073463268, 0.262868565717141, 0.290854572713643, 0.276861569215392,
>> 0.288855572213893, 0.282358820589705, 0.292853573213393, 0.286356821589205,
>> 0.271364317841079, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
>> 1, 1, 1, 1, 1, 1), .Dim = c(10L, 10L), .Dimnames = list(c("ACAP3",
>> "ACTRT2", "AGRN", "ANKRD65", "ATAD3A", "ATAD3B", "ATAD3C", "AURKAIP1",
>> "B3GALT6", "C1orf159"), c("chr5q23", "chr16q24", "chr8q24", "chr13q11",
>> "chr7p21", "chr10q23", "chr13q13", "chr10q21", "chr1p13", "chrxp21"
>> )))
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From Ch|@Zh@ng @end|ng |rom UGent@be  Tue Feb 12 10:28:11 2019
From: Ch|@Zh@ng @end|ng |rom UGent@be (Chi Zhang)
Date: Tue, 12 Feb 2019 09:28:11 +0000
Subject: [R] Should I use full models when using Powersim?
In-Reply-To: <mailman.353549.1.1549710001.59326.r-help@r-project.org>
References: <mailman.353549.1.1549710001.59326.r-help@r-project.org>
Message-ID: <1549963628295.89016@UGent.be>

I tried using powersim from R package simr to estimate the number of participants that I need for an experiment. I performed the simulation based on the data from my pilot study. The model I used is sketched below:

fit <- glmer(B ~ a+b+a:b
             (1+a+b+a:b|Subject) +
             (1+a+b+a:b|Item),
           family = binomial(link="logit"),
           data = data,
           control = glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=50000),
                                  tol = .0001))

in which Subject and Item mean the distinct id of subjects and items from the pilot study. I want to know test how the power of the interaction term (a:b) changes with the growth of the number of participants. The code I am using is:

fit2<- extend(fit, along="Subject", n = 84)
sim <- powerCurve(fit2, test = fcompare(~a+b), along = "Subject", breaks=c(48,60,72,84), nsim = 5000)
print(sim)

But the results of the simulation was rather bizarre. To begin with, the power of the interaction grew smaller when the number of participants increased from 72 to 84, which I believe is incompatible with the normal observation that the power increases with the number of participants. Second, I tried using the full random model to perform the simulation, but it is really slow (it took me weeks to get just one result). I was wondering if I can use a simpler random model to perform the simulation.

To reiterate my question: first, why my simulated power decreased with the increase of the number of participants? Is there something wrong with my code? Second, can I use a simpler random model for the simulation in order to save time? Thanks in advance!
------------------------
Chi Zhang

PhD Student
Department of Experimental Psychology
Ghent University
Henri Dunantlaan 2, B-9000 Gent, Belgium
Tel: +32 465386530
E-mail: chi.zhang at ugent.be

________________________________________
From: R-help <r-help-bounces at r-project.org> on behalf of r-help-request at r-project.org <r-help-request at r-project.org>
Sent: Saturday, February 9, 2019 12:00
To: r-help at r-project.org
Subject: R-help Digest, Vol 192, Issue 9

Send R-help mailing list submissions to
        r-help at r-project.org

To subscribe or unsubscribe via the World Wide Web, visit
        https://stat.ethz.ch/mailman/listinfo/r-help
or, via email, send a message with subject or body 'help' to
        r-help-request at r-project.org

You can reach the person managing the list at
        r-help-owner at r-project.org

When replying, please edit your Subject line so it is more specific
than "Re: Contents of R-help digest..."


Today's Topics:

   1. Re: Randomization Test (Meyners, Michael)
   2. pattern evaluation in electron microscopy images (PIKAL Petr)
   3. why standardize the variables to perform LDA? (Tony Gozdz)

----------------------------------------------------------------------

Message: 1
Date: Fri, 8 Feb 2019 15:31:26 +0000
From: "Meyners, Michael" <meyners.m at pg.com>
To: Ogbos Okike <giftedlife2014 at gmail.com>, r-help
        <r-help at r-project.org>
Subject: Re: [R] Randomization Test
Message-ID:
        <BL0PR01MB4132FD4418FADCA82CBDC3349A690 at BL0PR01MB4132.prod.exchangelabs.com>

Content-Type: text/plain; charset="us-ascii"

Ogbos,

You do not seem to have received a reply over the list yet, which might be due to the fact that this seems rather a stats than an R question. Neither got your attachment (Figure) through - see posting guide.

I'm not familiar with epoch analysis, so not sure what exactly you are doing / trying to achieve, but some general thoughts:

* You do NOT want to restrict your re-randomizations in a way that "none of the dates corresponds with the ones in the real event" - actually, as a general principle, the true data must be an admissible re-randomization as well. You seem to have excluded that (and a lot of other randomizations at the same time which might have occurred, i.e. dates 1 and 2 reversed but all others the same), thereby rendering the test invalid. Any restrictions you have on your re-randomizations must've applied to the original randomization as well.
* If you have rather observational data (which I suspect, but not sure), Edgington & Onghena (2007) would rather refer to this as a permutation test - the difference being that you have to make strong assumptions (similar to parametric tests) on the nature of the data, which are designed-in to be true for randomization tests. It might be a merely linguistic discrimination, but it is important to note which assumptions have to be (implicitly) made.
* I'm not sure what you mean by "mean differences" of the events - is that two groups you are comparing? If so, that seems reasonable, but just make sure the test statistic you use is reasonable and sensitive against the alternatives you are mostly interested in. The randomization/permutation test will never proof that, e.g., means are significantly different, but only that there is SOME difference. By selecting the appropriate test statistic, you can influence what will pop up more easily and what not, but you can never be sure (unless you make strong assumptions about everything else, like in many parametric tests).
* For any test statistic, you would then determine the proportion of its values among the 5000 samples where it is as large or larger than the one observed (or as small or smaller, or either, depending on the nature of the test statistic and whether you aim for a one- or a two-sided test). That is your p value. If small enough, conclude significance. At least conceptually important: The observed test statistic is always part of the re-randomization (i.e. your 5000) - so you truly only do 4999 plus the one you observed. Otherwise the test may be more or less liberal. Your p value is hence no smaller than 1/n, where n is the total number of samples you looked at (including the observed one), a p value of 0 is not possible in randomization tests (nor in other tests, of course).

I hope this is helpful, but you will need to go through these and refer to your own setup to check whether you adhered to the principles or not, which is impossible for me to judge based on the information provided (and I won't be able to look at excessive code to check either).

Michael

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ogbos Okike
> Sent: Montag, 28. Januar 2019 19:42
> To: r-help <r-help at r-project.org>
> Subject: [R] Randomization Test
>
> Dear Contributors,
>
> I conducting epoch analysis. I tried to test the significance of my result using
> randomization test.
>
> Since I have 71 events, I randomly selected another 71 events, making sure
> that none of the dates in the random events corresponds with the ones in
> the real event.
>
> Following the code I found here
> (https://www.uvm.edu/~dhowell/StatPages/R/RandomizationTestsWithR/R
> andom2Sample/TwoIndependentSamplesR.html),
> I combined these two data set and used them to generate another 5000
> events. I then plotted the graph of the mean differences for the 5000
> randomly generated events. On the graph, I indicated the region of the
> mean difference between the real 71 epoch and the randomly selected 71
> epoch.
>
> Since the two tail test shows that the mean difference falls at the extreme of
> the randomly selected events, I concluded that my result is statistically
> significant.
>
>
>
> I am attaching the graph to assistance you in you suggestions.
>
> I can attach both my code and the real and randomly generated events if you
> ask for it.
>
> My request is that you help me to understand if I am on the right track or no.
> This is the first time I am doing this and except the experts decide, I am not
> quite sure whether I am right or not.
>
> Many thanks for your kind concern.
>
> Best
> Ogbos
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.




------------------------------

Message: 2
Date: Fri, 8 Feb 2019 09:53:58 +0000
From: PIKAL Petr <petr.pikal at precheza.cz>
To: "r-help at r-project.org" <r-help at r-project.org>
Subject: [R] pattern evaluation in electron microscopy images
Message-ID:
        <fb6c47fe3525477c9d63e53e9a633855 at SRVEXCHCM1302.precheza.cz>
Content-Type: text/plain; charset="iso-8859-2"

Dear all

I enclose 3 electron microscope images in which I would like to evaluate plane spacing.

Before I start to dig deeper and use trial and error in trying to find some packages/functions for such pattern evaluation in electron microscopy pictures I would like to ask if anybody could point me to suitable packages/functions.

I am aware of EBImage package for general purpose image manipulation, but it does not have such functionality.

Best regards
Petr

If images did not came through please use this link:
St?hnout soubory<https://uschovna.agrofert.cz/dshosts/getfiles.aspx?fip=2475efa6-a77b-4ff6-8155-d9302e7b151b>.

Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner's personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/




------------------------------

Message: 3
Date: Fri, 8 Feb 2019 13:24:16 -0500
From: Tony Gozdz <tgozdz at gmail.com>
To: r-help at r-project.org
Subject: [R] why standardize the variables to perform LDA?
Message-ID:
        <CAO_D832BtyU=5T2j74NcFauf6ymjznE57iVf7SmOEENWNrOnAQ at mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

I understand the need to standardize the variables to perform PCA, but is
this a recommendation or necessity before running LDA?

        [[alternative HTML version deleted]]




------------------------------

Subject: Digest Footer

_______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


------------------------------

End of R-help Digest, Vol 192, Issue 9
**************************************


From @z@deh_moh@@eb @end|ng |rom y@hoo@com  Tue Feb 12 11:23:04 2019
From: @z@deh_moh@@eb @end|ng |rom y@hoo@com (azadeh mohaseb)
Date: Tue, 12 Feb 2019 10:23:04 +0000 (UTC)
Subject: [R] Define semi landmarks in 2d GMM data
References: <1009109935.2195378.1549966984471.ref@mail.yahoo.com>
Message-ID: <1009109935.2195378.1549966984471@mail.yahoo.com>

HelloI have a question about the geometric morphometris in R. I use generally the geomorph package for my analyses.I have a set of 2D landmarks. 
I want to define my semi landmarks. Here is the list of my landmarks and semi landmarks.
22 semilandmarks between landmarks 1 and 220 semilandmarks between landmarks 2 and 317 semilandmarks between landmarks 3 and 423 semilandmarks between landmarks 4 and 529 semilandmarks between landmarks 5 and 627 semilandmarks between landmarks 6 and 710 semilandmarks between landmarks 7 and 822 semilandmarks between landmarks 8 and 1
I tried define.sliders but I can only choos one semi landmark between two landmarks. 

Is there any other solution which is more simple?
Could you please help me with this? 

Thanks in advanceAzadeh Mohaseb


?  
	[[alternative HTML version deleted]]


From t@v@r|ch@norberto @end|ng |rom gm@||@com  Tue Feb 12 21:09:52 2019
From: t@v@r|ch@norberto @end|ng |rom gm@||@com (Norberto Hernandez)
Date: Tue, 12 Feb 2019 14:09:52 -0600
Subject: [R] Using ggplot2 geom_path() in a grouped variable
Message-ID: <CADRcKWwwv7YKQisHQ39Bo-ToDDbg_0x_AX=OoZacgfpC7jTsmg@mail.gmail.com>

Hi! I am trying to make a scatter/path graph in one variable that is
divided in two groups (variable Control), but when I use the
geom_path() option, the line continues from the group one to the group
two, and I wasn't able to avoid it.I need the path draws over the
group one and then draws over the group two avoiding the connection
between the last value of group one and the first value of group two.

This is my code

library(ggplot2)
plot <- ggplot(data, aes(Pretrend, Outcome)) + geom_point()
plot + geom_point(aes(colour=factor(Control)))+labs(x="Date",
y="Cases", title="Intoxication Cases")+geom_path()

Could you please bring be some advice.
Regards
Norberto Francisco Hern?ndez

From dw|n@em|u@ @end|ng |rom comc@@t@net  Tue Feb 12 23:02:05 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Tue, 12 Feb 2019 16:02:05 -0600
Subject: [R] Using ggplot2 geom_path() in a grouped variable
In-Reply-To: <CADRcKWwwv7YKQisHQ39Bo-ToDDbg_0x_AX=OoZacgfpC7jTsmg@mail.gmail.com>
References: <CADRcKWwwv7YKQisHQ39Bo-ToDDbg_0x_AX=OoZacgfpC7jTsmg@mail.gmail.com>
Message-ID: <DE320FD4-7291-436D-B3B6-C97D78232FCE@comcast.net>



> On Feb 12, 2019, at 2:09 PM, Norberto Hernandez <tavarich.norberto at gmail.com> wrote:
> 
> Hi! I am trying to make a scatter/path graph in one variable that is
> divided in two groups (variable Control), but when I use the
> geom_path() option, the line continues from the group one to the group
> two, and I wasn't able to avoid it.I need the path draws over the
> group one and then draws over the group two avoiding the connection
> between the last value of group one and the first value of group two.
> 
> This is my code
> 
> library(ggplot2)
> plot <- ggplot(data, aes(Pretrend, Outcome)) + geom_point()
> plot + geom_point(aes(colour=factor(Control)))+labs(x="Date",
> y="Cases", title="Intoxication Cases")+geom_path()
> 
> Could you please bring be some advice.

The first bit of advice would be to include the unsuccessful code that used geom_path. Otherwise we hav no way of knowing what you gave as the grouping argument. The second bit of advice would be to include a dataset that illustrates the problem. (These are both bits of advice that you should have found in the Posting Guide.

-- 
David.

> Regards
> Norberto Francisco Hern?ndez
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Tue Feb 12 23:22:38 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Tue, 12 Feb 2019 22:22:38 +0000
Subject: [R] Using ggplot2 geom_path() in a grouped variable
In-Reply-To: <CADRcKWwwv7YKQisHQ39Bo-ToDDbg_0x_AX=OoZacgfpC7jTsmg@mail.gmail.com>
References: <CADRcKWwwv7YKQisHQ39Bo-ToDDbg_0x_AX=OoZacgfpC7jTsmg@mail.gmail.com>
Message-ID: <02e6380b-ddb7-177a-5175-2ce456b58c81@sapo.pt>

Hello,

I am not understanding the problem.
With a made up dataset everything seems right.

set.seed(1234)    # make the results reproducible
n <- 10
data <- data.frame(Pretrend = rep(1:10, 2),
                    Outcome = 1:10 + rnorm(2*n),
                    Control = rep(1:2, each = n))


library(ggplot2)

p <- ggplot(data, aes(Pretrend, Outcome, colour = factor(Control))) +
   geom_point() +
   geom_path() +
   labs(x = "Date", y = "Cases", title = "Intoxication Cases")

p


I have changed the plot name because 'plot' is the name of a base R 
function.

What is the problem with the graph above? Is that what you were looking 
for? (Or similar.)


Hope this helps,

Rui Barradas



?s 20:09 de 12/02/2019, Norberto Hernandez escreveu:
> Hi! I am trying to make a scatter/path graph in one variable that is
> divided in two groups (variable Control), but when I use the
> geom_path() option, the line continues from the group one to the group
> two, and I wasn't able to avoid it.I need the path draws over the
> group one and then draws over the group two avoiding the connection
> between the last value of group one and the first value of group two.
> 
> This is my code
> 
> library(ggplot2)
> plot <- ggplot(data, aes(Pretrend, Outcome)) + geom_point()
> plot + geom_point(aes(colour=factor(Control)))+labs(x="Date",
> y="Cases", title="Intoxication Cases")+geom_path()
> 
> Could you please bring be some advice.
> Regards
> Norberto Francisco Hern?ndez
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dw|n@em|u@ @end|ng |rom comc@@t@net  Wed Feb 13 00:25:09 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Tue, 12 Feb 2019 17:25:09 -0600
Subject: [R] Using ggplot2 geom_path() in a grouped variable
In-Reply-To: <02e6380b-ddb7-177a-5175-2ce456b58c81@sapo.pt>
References: <CADRcKWwwv7YKQisHQ39Bo-ToDDbg_0x_AX=OoZacgfpC7jTsmg@mail.gmail.com>
 <02e6380b-ddb7-177a-5175-2ce456b58c81@sapo.pt>
Message-ID: <E1FCB2E4-CD91-4F81-95D3-C04FF92EDE7F@comcast.net>

To Norberto;

Your code _probably_ would have succeeded if you had used color as a grouping argument, or you could have used group or linetype or probably others, but I find locating the listing of ggplot2 "behavioral" parameters rather frustrating. These details are not to be found in any of ?aes , ?ggplot , or ?geom_path. I was able to find those alternate parameters illustrated in the Data Visualization cheatsheet: https://github.com/rstudio/cheatsheets/blob/master/data-visualization-2.1.pdf

   ... + +geom_path(aes(color=factor(Control)))

However, one never knows about error semantics unless one has the data-object.

-- 
David

> On Feb 12, 2019, at 4:22 PM, Rui Barradas <ruipbarradas at sapo.pt> wrote:
> 
> Hello,
> 
> I am not understanding the problem.
> With a made up dataset everything seems right.
> 
> set.seed(1234)    # make the results reproducible
> n <- 10
> data <- data.frame(Pretrend = rep(1:10, 2),
>                   Outcome = 1:10 + rnorm(2*n),
>                   Control = rep(1:2, each = n))
> 
> 
> library(ggplot2)
> 
> p <- ggplot(data, aes(Pretrend, Outcome, colour = factor(Control))) +
>  geom_point() +
>  geom_path() +
>  labs(x = "Date", y = "Cases", title = "Intoxication Cases")
> 
> p
> 
> 
> I have changed the plot name because 'plot' is the name of a base R function.
> 
> What is the problem with the graph above? Is that what you were looking for? (Or similar.)
> 
> 
> Hope this helps,
> 
> Rui Barradas
> 
> 
> 
> ?s 20:09 de 12/02/2019, Norberto Hernandez escreveu:
>> Hi! I am trying to make a scatter/path graph in one variable that is
>> divided in two groups (variable Control), but when I use the
>> geom_path() option, the line continues from the group one to the group
>> two, and I wasn't able to avoid it.I need the path draws over the
>> group one and then draws over the group two avoiding the connection
>> between the last value of group one and the first value of group two.
>> This is my code
>> library(ggplot2)
>> plot <- ggplot(data, aes(Pretrend, Outcome)) + geom_point()
>> plot + geom_point(aes(colour=factor(Control)))+labs(x="Date",
>> y="Cases", title="Intoxication Cases")+geom_path()
>> Could you please bring be some advice.
>> Regards
>> Norberto Francisco Hern?ndez
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m@rt|nbe@ed@ @end|ng |rom @ezn@m@cz  Wed Feb 13 00:33:36 2019
From: m@rt|nbe@ed@ @end|ng |rom @ezn@m@cz (Martin Beseda)
Date: Wed, 13 Feb 2019 00:33:36 +0100
Subject: [R] How to fit the following data and get the equation describing
 them?
Message-ID: <8395c7f6-c774-a4d5-e3af-d9361171134a@seznam.cz>

Hello everybody,

I have a following data-set:

 ??? > data$R
 ???? [1] 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 
1.30 1.35 1.40
 ??? [16] 1.45 1.50 1.55 1.60 1.65 1.70 1.75 1.80 1.85 1.90 1.95 2.00 
2.05 2.10 2.15
 ??? [31] 2.20 2.25 2.30 2.35 2.40 2.45 2.50 2.60 2.65 2.70 2.75 2.80 
2.85 2.90 2.95
 ??? [46] 3.00 3.05 3.10 3.15 3.20 3.25 3.30 3.35 3.40 3.45 3.50 3.55 
3.60 3.65 3.70
 ??? [61] 3.75 3.80 3.85 3.90 3.95 4.00 4.05 4.10 4.15 4.20 4.25 4.30 
4.35 4.40 4.45
 ??? [76] 4.50 4.55 4.90 4.95
 ??? > data$MRCI8
 ???? [1] 69.10108 56.65276 47.82651 41.64975 37.41315 33.68440 31.29635 
29.75257
 ???? [9] 28.87221 28.34471 27.71358 26.05035 24.95444 24.03100 23.29969 
22.71869
 ??? [17] 22.25380 21.87805 21.56874 21.30299 21.01756 20.61259 20.22289 
19.88496
 ??? [25] 19.59455 19.34637 19.11511 18.94022 18.79525 18.67703 18.58289 
18.51252
 ??? [33] 18.45438 18.41333 18.38730 18.37395 18.37071 18.38427 18.39670 
18.41077
 ??? [41] 18.42544 18.44003 18.45415 18.46754 18.48009 18.49176 18.50254 
18.51246
 ??? [49] 18.52155 18.52988 18.53748 18.54440 18.55068 18.55633 18.56108 
18.59038
 ??? [57] 18.59568 18.60022 18.60429 18.60789 18.61100 18.61363 18.61581 
18.61763
 ??? [65] 18.61914 18.62043 18.62155 18.62256 18.62349 18.62436 18.62520 
18.62602
 ??? [73] 18.62682 18.62761 18.62839 18.62915 18.62990 18.67205 18.67038


I need to fit the data and to get the equation describing the line.

1) I know, I can fit the points using smooth.spline but I have no idea, 
if it's possible to get the equation as y(x) = c_0 + c_1x + c_2x^2.

2) If it's not possible, then I'd like to fit the data with 
Lennard-Jones potential 
(https://en.wikipedia.org/wiki/Lennard-Jones_potential), i.e. with the 
formula

V(r) = 4e * ( (s/r)^12 - (s/r)^6 ),

where 'e' and 's' are the coefficients to be fitted. I have no idea, if 
it's possible to specify the exact position of fitted coefficients, i.e. 
inside the fraction like here.



Could you, please, advise me, how to do any of these things?

Thank you very much.


Best regards,

Martin Beseda


From r@turner @end|ng |rom @uck|@nd@@c@nz  Wed Feb 13 02:16:12 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Wed, 13 Feb 2019 14:16:12 +1300
Subject: [R] [FORGED] How to fit the following data and get the equation
 describing them?
In-Reply-To: <8395c7f6-c774-a4d5-e3af-d9361171134a@seznam.cz>
References: <8395c7f6-c774-a4d5-e3af-d9361171134a@seznam.cz>
Message-ID: <38dfa1ba-4885-7f7a-1ca0-aa0e71701cb1@auckland.ac.nz>

On 2/13/19 12:33 PM, Martin Beseda wrote:
> Hello everybody,
> 
> I have a following data-set:
> 
>  ??? > data$R
>  ???? [1] 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 
> 1.30 1.35 1.40
>  ??? [16] 1.45 1.50 1.55 1.60 1.65 1.70 1.75 1.80 1.85 1.90 1.95 2.00 
> 2.05 2.10 2.15
>  ??? [31] 2.20 2.25 2.30 2.35 2.40 2.45 2.50 2.60 2.65 2.70 2.75 2.80 
> 2.85 2.90 2.95
>  ??? [46] 3.00 3.05 3.10 3.15 3.20 3.25 3.30 3.35 3.40 3.45 3.50 3.55 
> 3.60 3.65 3.70
>  ??? [61] 3.75 3.80 3.85 3.90 3.95 4.00 4.05 4.10 4.15 4.20 4.25 4.30 
> 4.35 4.40 4.45
>  ??? [76] 4.50 4.55 4.90 4.95
>  ??? > data$MRCI8
>  ???? [1] 69.10108 56.65276 47.82651 41.64975 37.41315 33.68440 31.29635 
> 29.75257
>  ???? [9] 28.87221 28.34471 27.71358 26.05035 24.95444 24.03100 23.29969 
> 22.71869
>  ??? [17] 22.25380 21.87805 21.56874 21.30299 21.01756 20.61259 20.22289 
> 19.88496
>  ??? [25] 19.59455 19.34637 19.11511 18.94022 18.79525 18.67703 18.58289 
> 18.51252
>  ??? [33] 18.45438 18.41333 18.38730 18.37395 18.37071 18.38427 18.39670 
> 18.41077
>  ??? [41] 18.42544 18.44003 18.45415 18.46754 18.48009 18.49176 18.50254 
> 18.51246
>  ??? [49] 18.52155 18.52988 18.53748 18.54440 18.55068 18.55633 18.56108 
> 18.59038
>  ??? [57] 18.59568 18.60022 18.60429 18.60789 18.61100 18.61363 18.61581 
> 18.61763
>  ??? [65] 18.61914 18.62043 18.62155 18.62256 18.62349 18.62436 18.62520 
> 18.62602
>  ??? [73] 18.62682 18.62761 18.62839 18.62915 18.62990 18.67205 18.67038
> 
> 
> I need to fit the data and to get the equation describing the line.
> 
> 1) I know, I can fit the points using smooth.spline but I have no idea, 
> if it's possible to get the equation as y(x) = c_0 + c_1x + c_2x^2.
> 
> 2) If it's not possible, then I'd like to fit the data with 
> Lennard-Jones potential 
> (https://en.wikipedia.org/wiki/Lennard-Jones_potential), i.e. with the 
> formula
> 
> V(r) = 4e * ( (s/r)^12 - (s/r)^6 ),
> 
> where 'e' and 's' are the coefficients to be fitted. I have no idea, if 
> it's possible to specify the exact position of fitted coefficients, i.e. 
> inside the fraction like here.
> 
> 
> 
> Could you, please, advise me, how to do any of these things?
> 
> Thank you very much.

You seem to be way out of your depth.  You would be well advised to seek 
local help from a qualified statistician.

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From t@v@r|ch@norberto @end|ng |rom gm@||@com  Wed Feb 13 03:35:49 2019
From: t@v@r|ch@norberto @end|ng |rom gm@||@com (Norberto Hernandez)
Date: Tue, 12 Feb 2019 20:35:49 -0600
Subject: [R] Using ggplot2 geom_path() in a grouped variable
In-Reply-To: <02e6380b-ddb7-177a-5175-2ce456b58c81@sapo.pt>
References: <CADRcKWwwv7YKQisHQ39Bo-ToDDbg_0x_AX=OoZacgfpC7jTsmg@mail.gmail.com>
 <02e6380b-ddb7-177a-5175-2ce456b58c81@sapo.pt>
Message-ID: <CADRcKWw0hGtvaSzZgQssL0m0o9RbDq2MR9DZpW_aPBOFv3C0dg@mail.gmail.com>

Rui:

Thank you very much for your help! It works the way you do. My problem
was with the time I declare the control group:

My code:
p <- ggplot(data, aes(Pretrend, Outcome))
+ geom_point(aes(colour=factor(Control))

Your solution:
p <- ggplot(data, aes(Pretrend, Outcome, colour = factor(Control)))
+ geom_point()

I believe that the time you are declaring the control group (inside
the ggplot() function instead the geom_point()) makes your code do the
right way and that my code links the last observation of group zero
with the first observation of group one.

Thank you Rui, and thanks all of you guys for take the time to read
and answer my post.

Regards
Norberto

El mar., 12 feb. 2019 a las 16:22, Rui Barradas
(<ruipbarradas at sapo.pt>) escribi?:
>
> Hello,
>
> I am not understanding the problem.
> With a made up dataset everything seems right.
>
> set.seed(1234)    # make the results reproducible
> n <- 10
> data <- data.frame(Pretrend = rep(1:10, 2),
>                     Outcome = 1:10 + rnorm(2*n),
>                     Control = rep(1:2, each = n))
>
>
> library(ggplot2)
>
> p <- ggplot(data, aes(Pretrend, Outcome, colour = factor(Control))) +
>    geom_point() +
>    geom_path() +
>    labs(x = "Date", y = "Cases", title = "Intoxication Cases")
>
> p
>
>
> I have changed the plot name because 'plot' is the name of a base R
> function.
>
> What is the problem with the graph above? Is that what you were looking
> for? (Or similar.)
>
>
> Hope this helps,
>
> Rui Barradas
>
>
>
> ?s 20:09 de 12/02/2019, Norberto Hernandez escreveu:
> > Hi! I am trying to make a scatter/path graph in one variable that is
> > divided in two groups (variable Control), but when I use the
> > geom_path() option, the line continues from the group one to the group
> > two, and I wasn't able to avoid it.I need the path draws over the
> > group one and then draws over the group two avoiding the connection
> > between the last value of group one and the first value of group two.
> >
> > This is my code
> >
> > library(ggplot2)
> > plot <- ggplot(data, aes(Pretrend, Outcome)) + geom_point()
> > plot + geom_point(aes(colour=factor(Control)))+labs(x="Date",
> > y="Cases", title="Intoxication Cases")+geom_path()
> >
> > Could you please bring be some advice.
> > Regards
> > Norberto Francisco Hern?ndez
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Feb 13 08:10:04 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 12 Feb 2019 23:10:04 -0800
Subject: [R] [FORGED] How to fit the following data and get the equation
 describing them?
In-Reply-To: <38dfa1ba-4885-7f7a-1ca0-aa0e71701cb1@auckland.ac.nz>
References: <8395c7f6-c774-a4d5-e3af-d9361171134a@seznam.cz>
 <38dfa1ba-4885-7f7a-1ca0-aa0e71701cb1@auckland.ac.nz>
Message-ID: <62D6084B-C2D7-4CC0-9B09-BE13C928AF41@dcn.davis.ca.us>

To elaborate on Rolf's suggestion... the data you have does not look like the LJ plot on the referenced web page... You should be investigating the nature of this data and how it is really supposed to relate to potential energy physics before putting it into a numerical meat grinder. Garbage in gets you garbage out.

As for an alternative to splines... linear or nonlinear regression is one of the more common uses of statistics and R, but it is not without its pitfalls. You _could_ read the Introduction to R document that comes with R... but it would only tell you the mechanics of getting R to perform regression without informing you how to judge whether the result makes any sense at all. For that you need some local help and/or a college course on regression.

On February 12, 2019 5:16:12 PM PST, Rolf Turner <r.turner at auckland.ac.nz> wrote:
>On 2/13/19 12:33 PM, Martin Beseda wrote:
>> Hello everybody,
>> 
>> I have a following data-set:
>> 
>>  ??? > data$R
>>  ???? [1] 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25
>
>> 1.30 1.35 1.40
>>  ??? [16] 1.45 1.50 1.55 1.60 1.65 1.70 1.75 1.80 1.85 1.90 1.95 2.00
>
>> 2.05 2.10 2.15
>>  ??? [31] 2.20 2.25 2.30 2.35 2.40 2.45 2.50 2.60 2.65 2.70 2.75 2.80
>
>> 2.85 2.90 2.95
>>  ??? [46] 3.00 3.05 3.10 3.15 3.20 3.25 3.30 3.35 3.40 3.45 3.50 3.55
>
>> 3.60 3.65 3.70
>>  ??? [61] 3.75 3.80 3.85 3.90 3.95 4.00 4.05 4.10 4.15 4.20 4.25 4.30
>
>> 4.35 4.40 4.45
>>  ??? [76] 4.50 4.55 4.90 4.95
>>  ??? > data$MRCI8
>>  ???? [1] 69.10108 56.65276 47.82651 41.64975 37.41315 33.68440
>31.29635 
>> 29.75257
>>  ???? [9] 28.87221 28.34471 27.71358 26.05035 24.95444 24.03100
>23.29969 
>> 22.71869
>>  ??? [17] 22.25380 21.87805 21.56874 21.30299 21.01756 20.61259
>20.22289 
>> 19.88496
>>  ??? [25] 19.59455 19.34637 19.11511 18.94022 18.79525 18.67703
>18.58289 
>> 18.51252
>>  ??? [33] 18.45438 18.41333 18.38730 18.37395 18.37071 18.38427
>18.39670 
>> 18.41077
>>  ??? [41] 18.42544 18.44003 18.45415 18.46754 18.48009 18.49176
>18.50254 
>> 18.51246
>>  ??? [49] 18.52155 18.52988 18.53748 18.54440 18.55068 18.55633
>18.56108 
>> 18.59038
>>  ??? [57] 18.59568 18.60022 18.60429 18.60789 18.61100 18.61363
>18.61581 
>> 18.61763
>>  ??? [65] 18.61914 18.62043 18.62155 18.62256 18.62349 18.62436
>18.62520 
>> 18.62602
>>  ??? [73] 18.62682 18.62761 18.62839 18.62915 18.62990 18.67205
>18.67038
>> 
>> 
>> I need to fit the data and to get the equation describing the line.
>> 
>> 1) I know, I can fit the points using smooth.spline but I have no
>idea, 
>> if it's possible to get the equation as y(x) = c_0 + c_1x + c_2x^2.
>> 
>> 2) If it's not possible, then I'd like to fit the data with 
>> Lennard-Jones potential 
>> (https://en.wikipedia.org/wiki/Lennard-Jones_potential), i.e. with
>the 
>> formula
>> 
>> V(r) = 4e * ( (s/r)^12 - (s/r)^6 ),
>> 
>> where 'e' and 's' are the coefficients to be fitted. I have no idea,
>if 
>> it's possible to specify the exact position of fitted coefficients,
>i.e. 
>> inside the fraction like here.
>> 
>> 
>> 
>> Could you, please, advise me, how to do any of these things?
>> 
>> Thank you very much.
>
>You seem to be way out of your depth.  You would be well advised to
>seek 
>local help from a qualified statistician.
>
>cheers,
>
>Rolf Turner

-- 
Sent from my phone. Please excuse my brevity.


From y@e|@|nb@r@new @end|ng |rom gm@||@com  Wed Feb 13 22:14:12 2019
From: y@e|@|nb@r@new @end|ng |rom gm@||@com (Yael Inbar)
Date: Wed, 13 Feb 2019 16:14:12 -0500
Subject: [R] Help with Zelig package
Message-ID: <061f01d4c3e1$11e400c0$35ac0240$@gmail.com>

Hi!
 
I'm conducting a research which involves matching in R.
I'm running now an old code, written by a colleague, who passed away  last
year.
I'm trying to continue his research, but I encountered some issues while
reviving his code.
 
The main problem is that I'm not familiar with Zelig functionalities, so I
can't even tell if the problems that I encountered are due to changes in
Zelig packages over the years, or not. 
In his original code, my colleague wrote:
 
z.treat <- zelig(OF, data=CNTRL_G, model="ls")
  x.treat <- setx(z.treat, data=TREAT_G, cond=TRUE)
  s.treat <- sim(z.treat, x=x.treat)
 
  z.control <- zelig(OF, data=TREAT_G, model="ls")
  x.control <- setx(z.treat, data=CNTRL_G, cond=TRUE)
  s.control <- sim(z.control, x=x.control)
  
  att <- s.treat$qi$att.ev
  ate <- c(s.treat$qi$att.ev, -s.control$qi$att.ev)
 
 
However, when I try to run this code, I get the following error:
Error in s.treat$qi$att.ev : object of type 'closure' is not subsettable
 
I tried to install an old version of Zelig, but this resulted in hundreds of
warning and no success in running the abovementioned methods.
I understand that Zelig package has gone through some changes with regards
to retrieving ATT, however, I'm afraid to harm the original code's meaning
and would like to know what is the exact corresponding code in the new
version, so that my colleague's results will remain as originally computed.
 
Could you please help me with this issue? I would really appreciate your
advice regarding the replacement of the code with a code which will produce
similar results, with the new format of the newer Zelig package.
 
Thanks in advance,
 
Yael
 

	[[alternative HTML version deleted]]


From @pbr@ckett20 @end|ng |rom @@|ntjo@ephh@@com  Thu Feb 14 05:31:32 2019
From: @pbr@ckett20 @end|ng |rom @@|ntjo@ephh@@com (Spencer Brackett)
Date: Wed, 13 Feb 2019 23:31:32 -0500
Subject: [R] R Data
Message-ID: <CAPQaxLPPzu9p-bUkqRbCA_Wc5Jp6T5QF9PEs5pB3RcVeV0kXvw@mail.gmail.com>

Hello everyone,

The following is a portion of coding that a colleague sent. Given my lack
of experience in R, I am not quite sure what the significance of the
following arguments. Could anyone help me translate? For context, I am
aware of the downloading portion of the script... library(data.table) etc.,
but am not familiar with the portion pertaining to an1 .

library(data.table)
anno = as.data.frame(fread(file =
"/rsrch1/bcb/kchen_group/v_mohanty/data/TCGA/450K/mapper.txt", sep ="\t",
header = T))
meth = read.table(file =
"/rsrch1/bcb/kchen_group/v_mohanty/data/TCGA/27K/GBM.txt", sep  ="\t",
header = T, row.names = 1)
meth = as.matrix(meth)
""" the loop just formats the methylation column names to match format"""
colnames(meth) = sapply(colnames(meth), function(i){
  c1 = strsplit(i,split = '.', fixed = T)[[1]]
  c1[4] = paste(strsplit(c1[4],split = "",fixed = T)[[1]][1:2],collapse =
"")
  paste(c1,collapse = ".")
})
exp = read.table(file =
"/rsrch1/bcb/kchen_group/v_mohanty/data/TCGA/RNAseq/GBM.txt", sep = "\t",
header = T, row.names = 1)
exp = as.matrix(exp)
c = intersect(colnames(exp),colnames(meth))
exp = exp[,c]
meth = meth[,c]
m = apply(meth, 1, function(i){
  log2(i/(1-i))
})
m = t(as.matrix(m))
an = anno[anno$probe %in% rownames(m),]
an = an[an$gene %in% rownames(exp),]
an = an[an$location %in% c("TSS200","TSS1500"),]

p = apply(an,1,function(i){
  tryCatch(summary(lm(exp[as.character(i[2]),] ~
m[as.character(i[1]),]))$coefficient[2,4], error= function(e)NA)
})
t = apply(an,1,function(i){
  tryCatch(summary(lm(exp[as.character(i[2]),] ~
m[as.character(i[1]),]))$coefficient[2,3], error= function(e)NA)
})
an1 =cbind(an,p)
an1 = cbind(an1,t)
an1$q = p.adjust(as.numeric(an1$p))
summary(lm(exp["MAOB",] ~ m["cg00121904",]$coefficient[2,c(3:4)]
###############################################

	[[alternative HTML version deleted]]


From M@rk@Fow|er @end|ng |rom d|o-mpo@gc@c@  Thu Feb 14 13:09:51 2019
From: M@rk@Fow|er @end|ng |rom d|o-mpo@gc@c@ (Fowler, Mark)
Date: Thu, 14 Feb 2019 12:09:51 +0000
Subject: [R] R Data
In-Reply-To: <CAPQaxLPPzu9p-bUkqRbCA_Wc5Jp6T5QF9PEs5pB3RcVeV0kXvw@mail.gmail.com>
References: <CAPQaxLPPzu9p-bUkqRbCA_Wc5Jp6T5QF9PEs5pB3RcVeV0kXvw@mail.gmail.com>
Message-ID: <88388BFE50A61F408122CBAEB917FD57502B1D1B@SVNSBIOMBX02.ENT.dfo-mpo.ca>

Hi Spencer,

The an1 syntax is adding regression coefficients (or NAs where a regression could not be done) to the downloaded and processed data, which ends up a matrix. The cbind function adds the regression coefficients to the last column of the matrix (i.e. bind the columns of the inputs in the order given). Simple example below. Not actually any need for the separate cbind commands, could have just used an1=cbind(an,p,t). The cbind function expects all the columns to be of the same length, hence the use of the tryCatch function to capture NA's for failed regression attempts, ensuring that p and t correspond row by row with the matrix.

 x=seq(1,5)
 y=seq(6,10)
 z=seq(1,5)
xyz=cbind(x,y,z)
xyz
   x  y z
[1,] 1  6 1
[2,] 2  7 2
[3,] 3  8 3
[4,] 4  9 4
[5,] 5 10 5
dangs=rep(NA,5)
xyzd=cbind(xyz,dangs)
xyzd
     x  y z dangs
[1,] 1  6 1    NA
[2,] 2  7 2    NA
[3,] 3  8 3    NA
[4,] 4  9 4    NA
[5,] 5 10 5    NA

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Spencer Brackett
Sent: February 14, 2019 12:32 AM
To: R-help <r-help at r-project.org>; Sarah Goslee <sarah.goslee at gmail.com>; Caitlin Gibbons <bioprogrammer at gmail.com>; Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
Subject: [R] R Data

Hello everyone,

The following is a portion of coding that a colleague sent. Given my lack of experience in R, I am not quite sure what the significance of the following arguments. Could anyone help me translate? For context, I am aware of the downloading portion of the script... library(data.table) etc., but am not familiar with the portion pertaining to an1 .

library(data.table)
anno = as.data.frame(fread(file =
"/rsrch1/bcb/kchen_group/v_mohanty/data/TCGA/450K/mapper.txt", sep ="\t", header = T)) meth = read.table(file = "/rsrch1/bcb/kchen_group/v_mohanty/data/TCGA/27K/GBM.txt", sep  ="\t", header = T, row.names = 1) meth = as.matrix(meth) """ the loop just formats the methylation column names to match format"""
colnames(meth) = sapply(colnames(meth), function(i){
  c1 = strsplit(i,split = '.', fixed = T)[[1]]
  c1[4] = paste(strsplit(c1[4],split = "",fixed = T)[[1]][1:2],collapse =
"")
  paste(c1,collapse = ".")
})
exp = read.table(file =
"/rsrch1/bcb/kchen_group/v_mohanty/data/TCGA/RNAseq/GBM.txt", sep = "\t", header = T, row.names = 1) exp = as.matrix(exp) c = intersect(colnames(exp),colnames(meth))
exp = exp[,c]
meth = meth[,c]
m = apply(meth, 1, function(i){
  log2(i/(1-i))
})
m = t(as.matrix(m))
an = anno[anno$probe %in% rownames(m),]
an = an[an$gene %in% rownames(exp),]
an = an[an$location %in% c("TSS200","TSS1500"),]

p = apply(an,1,function(i){
  tryCatch(summary(lm(exp[as.character(i[2]),] ~ m[as.character(i[1]),]))$coefficient[2,4], error= function(e)NA)
})
t = apply(an,1,function(i){
  tryCatch(summary(lm(exp[as.character(i[2]),] ~ m[as.character(i[1]),]))$coefficient[2,3], error= function(e)NA)
})
an1 =cbind(an,p)
an1 = cbind(an1,t)
an1$q = p.adjust(as.numeric(an1$p))
summary(lm(exp["MAOB",] ~ m["cg00121904",]$coefficient[2,c(3:4)]
###############################################

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From petr@p|k@| @end|ng |rom prechez@@cz  Thu Feb 14 13:19:05 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Thu, 14 Feb 2019 12:19:05 +0000
Subject: [R] pattern evaluation in electron microscopy images
In-Reply-To: <295313c4660e49278e3abc8c0b6eb444@GBDCVPEXC08.corp.lgc-group.com>
References: <fb6c47fe3525477c9d63e53e9a633855@SRVEXCHCM1302.precheza.cz>
 <295313c4660e49278e3abc8c0b6eb444@GBDCVPEXC08.corp.lgc-group.com>
Message-ID: <c47d873d292c40daa2d2daa741574b59@SRVEXCHCM1302.precheza.cz>

Hallo Steve.

Thank you for pointing to ImageJ, I will try to inspect it. I expected that somebody in R comunity automated such procedure within some R package, but it is probably too specialised.

S pozdravem | Best Regards
RNDr. Petr PIKAL
Vedouc? V?zkumu a v?voje | Research Manager
PRECHEZA a.s.
n?b?. Dr. Edvarda Bene?e 1170/24 | 750 02 P?erov | Czech Republic
Tel: +420 581 252 256 | GSM: +420 724 008 364
petr.pikal at precheza.cz | www.precheza.cz

Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner's personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/

> -----Original Message-----
> From: S Ellison <S.Ellison at LGCGroup.com>
> Sent: Monday, February 11, 2019 12:54 PM
> To: PIKAL Petr <petr.pikal at precheza.cz>; r-help at r-project.org
> Subject: RE: pattern evaluation in electron microscopy images
> 
> Not really my field, but would you not approach this using FFT on selected
> regions?
> 
> I think IMageJ has some capability in that area; see example at
> https://imagej.nih.gov/ij/docs/examples/tem/.
> 
> Steve Ellison
> 
> 
> 
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of PIKAL
> > Petr
> > Sent: 08 February 2019 09:54
> > To: r-help at r-project.org
> > Subject: [R] pattern evaluation in electron microscopy images
> >
> > Dear all
> >
> > I enclose 3 electron microscope images in which I would like to
> > evaluate plane spacing.
> >
> > Before I start to dig deeper and use trial and error in trying to find
> > some packages/functions for such pattern evaluation in electron
> > microscopy pictures I would like to ask if anybody could point me to
> > suitable packages/functions.
> >
> > I am aware of EBImage package for general purpose image manipulation,
> > but it does not have such functionality.
> >
> > Best regards
> > Petr
> >
> > If images did not came through please use this link:
> > St?hnout
> > soubory<https://uschovna.agrofert.cz/dshosts/getfiles.aspx?fip=2475efa
> > 6-
> > a77b-4ff6-8155-d9302e7b151b>.
> >
> > Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj?
> > obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na:
> > https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information
> > about processing and protection of business partner's personal data
> > are available on website: https://www.precheza.cz/en/personal-data-
> > protection-principles/
> > D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou
> > d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en?
> > odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any
> > documents attached to it may be confidential and are subject to the
> > legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html and provide commented, minimal, self-contained,
> > reproducible code.
> 
> 
> *****************************************************************
> **
> This email and any attachments are confidential. Any u...{{dropped:10}}


From @pbr@ckett20 @end|ng |rom @@|ntjo@ephh@@com  Thu Feb 14 14:20:39 2019
From: @pbr@ckett20 @end|ng |rom @@|ntjo@ephh@@com (Spencer Brackett)
Date: Thu, 14 Feb 2019 08:20:39 -0500
Subject: [R] R Data
In-Reply-To: <88388BFE50A61F408122CBAEB917FD57502B1D1B@SVNSBIOMBX02.ENT.dfo-mpo.ca>
References: <CAPQaxLPPzu9p-bUkqRbCA_Wc5Jp6T5QF9PEs5pB3RcVeV0kXvw@mail.gmail.com>
 <88388BFE50A61F408122CBAEB917FD57502B1D1B@SVNSBIOMBX02.ENT.dfo-mpo.ca>
Message-ID: <CAPQaxLNsF7p_=Wcy9aF0nF2L6_y1p65TxzgVUbjPdR6+FECmrQ@mail.gmail.com>

Mr. Fowler,

Thank you! This information is most helpful. So from my understanding, I
can use the regression coefficients shown (via the coding I originally
sent, to generate a continuous distribution with what is essentially a line
of best fit? The data added here had some 30,000 variables (it is genomic
data from TCGA), does this mean that any none NA data is being accounted
for in said distribution?

Best,

Spencer Brackett



On Thursday, February 14, 2019, Fowler, Mark <Mark.Fowler at dfo-mpo.gc.ca>
wrote:

> Hi Spencer,
>
> The an1 syntax is adding regression coefficients (or NAs where a
> regression could not be done) to the downloaded and processed data, which
> ends up a matrix. The cbind function adds the regression coefficients to
> the last column of the matrix (i.e. bind the columns of the inputs in the
> order given). Simple example below. Not actually any need for the separate
> cbind commands, could have just used an1=cbind(an,p,t). The cbind function
> expects all the columns to be of the same length, hence the use of the
> tryCatch function to capture NA's for failed regression attempts, ensuring
> that p and t correspond row by row with the matrix.
>
>  x=seq(1,5)
>  y=seq(6,10)
>  z=seq(1,5)
> xyz=cbind(x,y,z)
> xyz
>    x  y z
> [1,] 1  6 1
> [2,] 2  7 2
> [3,] 3  8 3
> [4,] 4  9 4
> [5,] 5 10 5
> dangs=rep(NA,5)
> xyzd=cbind(xyz,dangs)
> xyzd
>      x  y z dangs
> [1,] 1  6 1    NA
> [2,] 2  7 2    NA
> [3,] 3  8 3    NA
> [4,] 4  9 4    NA
> [5,] 5 10 5    NA
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Spencer Brackett
> Sent: February 14, 2019 12:32 AM
> To: R-help <r-help at r-project.org>; Sarah Goslee <sarah.goslee at gmail.com>;
> Caitlin Gibbons <bioprogrammer at gmail.com>; Jeff Newmiller <
> jdnewmil at dcn.davis.ca.us>
> Subject: [R] R Data
>
> Hello everyone,
>
> The following is a portion of coding that a colleague sent. Given my lack
> of experience in R, I am not quite sure what the significance of the
> following arguments. Could anyone help me translate? For context, I am
> aware of the downloading portion of the script... library(data.table) etc.,
> but am not familiar with the portion pertaining to an1 .
>
> library(data.table)
> anno = as.data.frame(fread(file =
> "/rsrch1/bcb/kchen_group/v_mohanty/data/TCGA/450K/mapper.txt", sep ="\t",
> header = T)) meth = read.table(file = "/rsrch1/bcb/kchen_group/v_
> mohanty/data/TCGA/27K/GBM.txt", sep  ="\t", header = T, row.names = 1)
> meth = as.matrix(meth) """ the loop just formats the methylation column
> names to match format"""
> colnames(meth) = sapply(colnames(meth), function(i){
>   c1 = strsplit(i,split = '.', fixed = T)[[1]]
>   c1[4] = paste(strsplit(c1[4],split = "",fixed = T)[[1]][1:2],collapse =
> "")
>   paste(c1,collapse = ".")
> })
> exp = read.table(file =
> "/rsrch1/bcb/kchen_group/v_mohanty/data/TCGA/RNAseq/GBM.txt", sep = "\t",
> header = T, row.names = 1) exp = as.matrix(exp) c = intersect(colnames(exp),
> colnames(meth))
> exp = exp[,c]
> meth = meth[,c]
> m = apply(meth, 1, function(i){
>   log2(i/(1-i))
> })
> m = t(as.matrix(m))
> an = anno[anno$probe %in% rownames(m),]
> an = an[an$gene %in% rownames(exp),]
> an = an[an$location %in% c("TSS200","TSS1500"),]
>
> p = apply(an,1,function(i){
>   tryCatch(summary(lm(exp[as.character(i[2]),] ~ m[as.character(i[1]),]))$coefficient[2,4],
> error= function(e)NA)
> })
> t = apply(an,1,function(i){
>   tryCatch(summary(lm(exp[as.character(i[2]),] ~ m[as.character(i[1]),]))$coefficient[2,3],
> error= function(e)NA)
> })
> an1 =cbind(an,p)
> an1 = cbind(an1,t)
> an1$q = p.adjust(as.numeric(an1$p))
> summary(lm(exp["MAOB",] ~ m["cg00121904",]$coefficient[2,c(3:4)]
> ###############################################
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From M@rk@Fow|er @end|ng |rom d|o-mpo@gc@c@  Thu Feb 14 14:44:07 2019
From: M@rk@Fow|er @end|ng |rom d|o-mpo@gc@c@ (Fowler, Mark)
Date: Thu, 14 Feb 2019 13:44:07 +0000
Subject: [R] R Data
In-Reply-To: <CAPQaxLNsF7p_=Wcy9aF0nF2L6_y1p65TxzgVUbjPdR6+FECmrQ@mail.gmail.com>
References: <CAPQaxLPPzu9p-bUkqRbCA_Wc5Jp6T5QF9PEs5pB3RcVeV0kXvw@mail.gmail.com>
 <88388BFE50A61F408122CBAEB917FD57502B1D1B@SVNSBIOMBX02.ENT.dfo-mpo.ca>
 <CAPQaxLNsF7p_=Wcy9aF0nF2L6_y1p65TxzgVUbjPdR6+FECmrQ@mail.gmail.com>
Message-ID: <88388BFE50A61F408122CBAEB917FD57502B1D65@SVNSBIOMBX02.ENT.dfo-mpo.ca>

I am not sure I would use the word ?accounted?, more like discounted (tossed out).

From: Spencer Brackett <spbrackett20 at saintjosephhs.com>
Sent: February 14, 2019 9:21 AM
To: Fowler, Mark <Mark.Fowler at dfo-mpo.gc.ca>
Cc: R-help <r-help at r-project.org>; Sarah Goslee <sarah.goslee at gmail.com>; Caitlin Gibbons <bioprogrammer at gmail.com>; Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
Subject: Re: R Data

Mr. Fowler,

Thank you! This information is most helpful. So from my understanding, I can use the regression coefficients shown (via the coding I originally sent, to generate a continuous distribution with what is essentially a line of best fit? The data added here had some 30,000 variables (it is genomic data from TCGA), does this mean that any none NA data is being accounted for in said distribution?

Best,

Spencer Brackett



On Thursday, February 14, 2019, Fowler, Mark <Mark.Fowler at dfo-mpo.gc.ca<mailto:Mark.Fowler at dfo-mpo.gc.ca>> wrote:
Hi Spencer,

The an1 syntax is adding regression coefficients (or NAs where a regression could not be done) to the downloaded and processed data, which ends up a matrix. The cbind function adds the regression coefficients to the last column of the matrix (i.e. bind the columns of the inputs in the order given). Simple example below. Not actually any need for the separate cbind commands, could have just used an1=cbind(an,p,t). The cbind function expects all the columns to be of the same length, hence the use of the tryCatch function to capture NA's for failed regression attempts, ensuring that p and t correspond row by row with the matrix.

 x=seq(1,5)
 y=seq(6,10)
 z=seq(1,5)
xyz=cbind(x,y,z)
xyz
   x  y z
[1,] 1  6 1
[2,] 2  7 2
[3,] 3  8 3
[4,] 4  9 4
[5,] 5 10 5
dangs=rep(NA,5)
xyzd=cbind(xyz,dangs)
xyzd
     x  y z dangs
[1,] 1  6 1    NA
[2,] 2  7 2    NA
[3,] 3  8 3    NA
[4,] 4  9 4    NA
[5,] 5 10 5    NA

-----Original Message-----
From: R-help <r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>> On Behalf Of Spencer Brackett
Sent: February 14, 2019 12:32 AM
To: R-help <r-help at r-project.org<mailto:r-help at r-project.org>>; Sarah Goslee <sarah.goslee at gmail.com<mailto:sarah.goslee at gmail.com>>; Caitlin Gibbons <bioprogrammer at gmail.com<mailto:bioprogrammer at gmail.com>>; Jeff Newmiller <jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us>>
Subject: [R] R Data

Hello everyone,

The following is a portion of coding that a colleague sent. Given my lack of experience in R, I am not quite sure what the significance of the following arguments. Could anyone help me translate? For context, I am aware of the downloading portion of the script... library(data.table) etc., but am not familiar with the portion pertaining to an1 .

library(data.table)
anno = as.data.frame(fread(file =
"/rsrch1/bcb/kchen_group/v_mohanty/data/TCGA/450K/mapper.txt", sep ="\t", header = T)) meth = read.table(file = "/rsrch1/bcb/kchen_group/v_mohanty/data/TCGA/27K/GBM.txt", sep  ="\t", header = T, row.names = 1) meth = as.matrix(meth) """ the loop just formats the methylation column names to match format"""
colnames(meth) = sapply(colnames(meth), function(i){
  c1 = strsplit(i,split = '.', fixed = T)[[1]]
  c1[4] = paste(strsplit(c1[4],split = "",fixed = T)[[1]][1:2],collapse =
"")
  paste(c1,collapse = ".")
})
exp = read.table(file =
"/rsrch1/bcb/kchen_group/v_mohanty/data/TCGA/RNAseq/GBM.txt", sep = "\t", header = T, row.names = 1) exp = as.matrix(exp) c = intersect(colnames(exp),colnames(meth))
exp = exp[,c]
meth = meth[,c]
m = apply(meth, 1, function(i){
  log2(i/(1-i))
})
m = t(as.matrix(m))
an = anno[anno$probe %in% rownames(m),]
an = an[an$gene %in% rownames(exp),]
an = an[an$location %in% c("TSS200","TSS1500"),]

p = apply(an,1,function(i){
  tryCatch(summary(lm(exp[as.character(i[2]),] ~ m[as.character(i[1]),]))$coefficient[2,4], error= function(e)NA)
})
t = apply(an,1,function(i){
  tryCatch(summary(lm(exp[as.character(i[2]),] ~ m[as.character(i[1]),]))$coefficient[2,3], error= function(e)NA)
})
an1 =cbind(an,p)
an1 = cbind(an1,t)
an1$q = p.adjust(as.numeric(an1$p))
summary(lm(exp["MAOB",] ~ m["cg00121904",]$coefficient[2,c(3:4)]
###############################################

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From |hb @end|ng |rom k@u@edu  Thu Feb 14 15:31:38 2019
From: |hb @end|ng |rom k@u@edu (Isaac Barnhart)
Date: Thu, 14 Feb 2019 14:31:38 +0000
Subject: [R] Finding the Mean of a Specific Set of Columns
Message-ID: <BN6PR05MB338093AAA324C660B6D33140AE670@BN6PR05MB3380.namprd05.prod.outlook.com>

I am having trouble finding the mean of a specific part of my dataset. Here is a sample of it:

plot lai leaf
1 104 82 1
2 104 167 2
3 104 248 3
4 104 343 4
5 104 377 5
6 105 64 1
7 105 139 2
8 105 211 3
9 105 296 4
10 105 348 5
11 106 94 1
12 106 167 2
13 106 243 3
14 106 281 4
15 106 332 5
16 108 83 1
17 108 382 2
18 108 320 3
19 108 146 4
20 108 129 5

I have many different plot numbers, none of which follow any kind of specific numeric sequence (even though I have sorted them from smallest to largest). I need to take the average (mean) of the LAI for each plot, and was wondering if there was a way to specify the code to do this. For example: I need the average of all the LAI measurements for each leaf of plot 104, 105, etc. Any help would be appreciated. Thanks!

Get Outlook for iOS<https://aka.ms/o0ukef>

	[[alternative HTML version deleted]]


From er|cjberger @end|ng |rom gm@||@com  Thu Feb 14 17:36:50 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Thu, 14 Feb 2019 18:36:50 +0200
Subject: [R] Finding the Mean of a Specific Set of Columns
In-Reply-To: <BN6PR05MB338093AAA324C660B6D33140AE670@BN6PR05MB3380.namprd05.prod.outlook.com>
References: <BN6PR05MB338093AAA324C660B6D33140AE670@BN6PR05MB3380.namprd05.prod.outlook.com>
Message-ID: <CAGgJW76veDLZAPGB56_JKFoVD0cM=1-xR2gBWhJx50aQJJxHUA@mail.gmail.com>

Hi Isaac,
I am sure you will get lots of answers to this. Here is one using the dplyr
package.
Assuming that your data frame is called 'a', then
library(dplyr)
b <- dplyr::group_by(a,plot) %>% dplyr::summarise( mean(lai) )
b
# A tibble: 4 x 2
   plot `mean(lai)`
  <int>       <dbl>
1   104        243.
2   105        212.
3   106        223.
4   108        212

HTH,
Eric



On Thu, Feb 14, 2019 at 6:20 PM Isaac Barnhart <ihb at ksu.edu> wrote:

> I am having trouble finding the mean of a specific part of my dataset.
> Here is a sample of it:
>
> plot lai leaf
> 1 104 82 1
> 2 104 167 2
> 3 104 248 3
> 4 104 343 4
> 5 104 377 5
> 6 105 64 1
> 7 105 139 2
> 8 105 211 3
> 9 105 296 4
> 10 105 348 5
> 11 106 94 1
> 12 106 167 2
> 13 106 243 3
> 14 106 281 4
> 15 106 332 5
> 16 108 83 1
> 17 108 382 2
> 18 108 320 3
> 19 108 146 4
> 20 108 129 5
>
> I have many different plot numbers, none of which follow any kind of
> specific numeric sequence (even though I have sorted them from smallest to
> largest). I need to take the average (mean) of the LAI for each plot, and
> was wondering if there was a way to specify the code to do this. For
> example: I need the average of all the LAI measurements for each leaf of
> plot 104, 105, etc. Any help would be appreciated. Thanks!
>
> Get Outlook for iOS<https://aka.ms/o0ukef>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From btupper @end|ng |rom b|ge|ow@org  Thu Feb 14 17:38:29 2019
From: btupper @end|ng |rom b|ge|ow@org (Ben Tupper)
Date: Thu, 14 Feb 2019 11:38:29 -0500
Subject: [R] Finding the Mean of a Specific Set of Columns
In-Reply-To: <BN6PR05MB338093AAA324C660B6D33140AE670@BN6PR05MB3380.namprd05.prod.outlook.com>
References: <BN6PR05MB338093AAA324C660B6D33140AE670@BN6PR05MB3380.namprd05.prod.outlook.com>
Message-ID: <4CA08086-8CED-434D-8219-3732FE561DCA@bigelow.org>

Hi,

You might try your hand at the tidyverse collection of tools which are veddy nice for this kind of wrangling. https://www.tidyverse.org/

Does this do the trick?

## START
library(readr)
library(dplyr)

txt <- "row plot lai leaf
1 104 82 1
2 104 167 2
3 104 248 3
4 104 343 4
5 104 377 5
6 105 64 1
7 105 139 2
8 105 211 3
9 105 296 4
10 105 348 5
11 106 94 1
12 106 167 2
13 106 243 3
14 106 281 4
15 106 332 5
16 108 83 1
17 108 382 2
18 108 320 3
19 108 146 4
20 108 129 5"

x <- readr::read_delim(txt, delim = " ") %>%
	dplyr::group_by(plot) %>%
	dplyr::summarize(mean_lai = mean(lai))
	
x
# A tibble: 4 x 2
   # plot mean_lai
  # <dbl>    <dbl>
# 1   104     243.
# 2   105     212.
# 3   106     223.
# 4   108     212 

## END
 
Cheers,
Ben

> 	[[alternative HTML version deleted]]


P.S.  Don't forget that HTML formatted emails get stripped of formatting on this list - so be sure to change your email client to send plain text.


> On Feb 14, 2019, at 9:31 AM, Isaac Barnhart <ihb at ksu.edu> wrote:
> 
> I am having trouble finding the mean of a specific part of my dataset. Here is a sample of it:
> 
> plot lai leaf
> 1 104 82 1
> 2 104 167 2
> 3 104 248 3
> 4 104 343 4
> 5 104 377 5
> 6 105 64 1
> 7 105 139 2
> 8 105 211 3
> 9 105 296 4
> 10 105 348 5
> 11 106 94 1
> 12 106 167 2
> 13 106 243 3
> 14 106 281 4
> 15 106 332 5
> 16 108 83 1
> 17 108 382 2
> 18 108 320 3
> 19 108 146 4
> 20 108 129 5
> 
> I have many different plot numbers, none of which follow any kind of specific numeric sequence (even though I have sorted them from smallest to largest). I need to take the average (mean) of the LAI for each plot, and was wondering if there was a way to specify the code to do this. For example: I need the average of all the LAI measurements for each leaf of plot 104, 105, etc. Any help would be appreciated. Thanks!
> 
> Get Outlook for iOS<https://aka.ms/o0ukef>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org

Ecological Forecasting: https://eco.bigelow.org/


From m@rc_@chw@rtz @end|ng |rom me@com  Thu Feb 14 18:05:23 2019
From: m@rc_@chw@rtz @end|ng |rom me@com (Marc Schwartz)
Date: Thu, 14 Feb 2019 12:05:23 -0500
Subject: [R] Finding the Mean of a Specific Set of Columns
In-Reply-To: <BN6PR05MB338093AAA324C660B6D33140AE670@BN6PR05MB3380.namprd05.prod.outlook.com>
References: <BN6PR05MB338093AAA324C660B6D33140AE670@BN6PR05MB3380.namprd05.prod.outlook.com>
Message-ID: <3344F7AE-92EB-4E3E-A2FA-30D231861891@me.com>

On Feb 14, 2019, at 9:31 AM, Isaac Barnhart <ihb at ksu.edu> wrote:
> 
> I am having trouble finding the mean of a specific part of my dataset. Here is a sample of it:
> 
> plot lai leaf
> 1 104 82 1
> 2 104 167 2
> 3 104 248 3
> 4 104 343 4
> 5 104 377 5
> 6 105 64 1
> 7 105 139 2
> 8 105 211 3
> 9 105 296 4
> 10 105 348 5
> 11 106 94 1
> 12 106 167 2
> 13 106 243 3
> 14 106 281 4
> 15 106 332 5
> 16 108 83 1
> 17 108 382 2
> 18 108 320 3
> 19 108 146 4
> 20 108 129 5
> 
> I have many different plot numbers, none of which follow any kind of specific numeric sequence (even though I have sorted them from smallest to largest). I need to take the average (mean) of the LAI for each plot, and was wondering if there was a way to specify the code to do this. For example: I need the average of all the LAI measurements for each leaf of plot 104, 105, etc. Any help would be appreciated. Thanks!


Hi, 

This is easy using base R functions. See ?aggregate, ?by and ?tapply for a starting place.

For example:

> aggregate(lai ~ plot, data = DF, FUN = mean)
  plot   lai
1  104 243.4
2  105 211.6
3  106 223.4
4  108 212.0

Regards,

Marc Schwartz


From p@u|@newe|| @end|ng |rom meto|||ce@gov@uk  Thu Feb 14 16:28:50 2019
From: p@u|@newe|| @end|ng |rom meto|||ce@gov@uk (Newell, Paul)
Date: Thu, 14 Feb 2019 15:28:50 +0000
Subject: [R] POSIXlt class and lapply
Message-ID: <VI1PR01MB4416CFC4412E1FC7256C217CD9670@VI1PR01MB4416.eurprd01.prod.exchangelabs.com>

Dear R-helpers,

We have recently upgraded from R-3.3.1 to R-3.5.2.

It seems there has been a change in behaviour of `lapply` and the `POSIXlt` class that I cannot find explicitly documented.


In R-3.3.1:

> lapply(as.POSIXlt(Sys.Date()), length)
$sec
[1] 1
$min
[1] 1
$hour
[1] 1
$mday
[1] 1
$mon
[1] 1
$year
[1] 1
$wday
[1] 1
$yday
[1] 1
$isdst
[1] 1


whereas, in R-3.5.2:

> lapply(as.POSIXlt(Sys.Date()), length)
[[1]]
[1] 1


Is this change in behaviour intentional?

Realistically, I cannot see anything documented to say that `lapply` should behave as per R-3.3.1 on a `POSIXlt` object, so it is/was perhaps unwise to rely on it.


Best wishes,
Paul Newell

From th|e|en @end|ng |rom e@hpm@eur@n|  Thu Feb 14 17:36:43 2019
From: th|e|en @end|ng |rom e@hpm@eur@n| (Frederick Thielen)
Date: Thu, 14 Feb 2019 16:36:43 +0000
Subject: [R] Finding the Mean of a Specific Set of Columns
In-Reply-To: <BN6PR05MB338093AAA324C660B6D33140AE670@BN6PR05MB3380.namprd05.prod.outlook.com>
References: <BN6PR05MB338093AAA324C660B6D33140AE670@BN6PR05MB3380.namprd05.prod.outlook.com>
Message-ID: <0D5B55609F8AC74BBD4A2BFB08414FD9B514F9D1@stff-mb04.staff.eur.nl>

Hi,

try

library("dplyr")
plot <- c(104, 104 ,104 ,104 ,104 ,105 ,105 ,105 ,105 ,105,106,
  106,106,  106,  106,108,  108,108,108,108)

lai <- c(82, 167, 248, 343, 377, 64, 139, 211, 296, 348,
         94, 167,243,281,332,83, 382,320,146,129)

leaf <- c(1,2, 3,4,5,1,2,3,4,5,1,2,3,4,5,1,2,3,4,5)

df <- cbind(plot, lai, leaf) %>% data.frame()

df %>% dplyr::group_by(plot) %>% 
        dplyr::summarise(mean = mean(lai, na.rm = T))


Best,
Frederick

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Isaac Barnhart
Sent: donderdag 14 februari 2019 15:32
To: r-help at r-project.org
Subject: [R] Finding the Mean of a Specific Set of Columns

I am having trouble finding the mean of a specific part of my dataset. Here is a sample of it:

plot lai leaf
1 104 82 1
2 104 167 2
3 104 248 3
4 104 343 4
5 104 377 5
6 105 64 1
7 105 139 2
8 105 211 3
9 105 296 4
10 105 348 5
11 106 94 1
12 106 167 2
13 106 243 3
14 106 281 4
15 106 332 5
16 108 83 1
17 108 382 2
18 108 320 3
19 108 146 4
20 108 129 5

I have many different plot numbers, none of which follow any kind of specific numeric sequence (even though I have sorted them from smallest to largest). I need to take the average (mean) of the LAI for each plot, and was wondering if there was a way to specify the code to do this. For example: I need the average of all the LAI measurements for each leaf of plot 104, 105, etc. Any help would be appreciated. Thanks!

Get Outlook for iOS<https://aka.ms/o0ukef>

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From wdun|@p @end|ng |rom t|bco@com  Thu Feb 14 21:03:55 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Thu, 14 Feb 2019 12:03:55 -0800
Subject: [R] POSIXlt class and lapply
In-Reply-To: <VI1PR01MB4416CFC4412E1FC7256C217CD9670@VI1PR01MB4416.eurprd01.prod.exchangelabs.com>
References: <VI1PR01MB4416CFC4412E1FC7256C217CD9670@VI1PR01MB4416.eurprd01.prod.exchangelabs.com>
Message-ID: <CAF8bMcb0CitdMRt1md00c2pe9G8uEXYuex_+wOySZ_ra_MaxCA@mail.gmail.com>

Somewhere between R-3.3.3 and R-3.5.2 a POSIXlt method for as.list() was
added, and lapply probably calls as.list().

> RCompare(methods("as.list"))
R version 3.3.3 (2017-03-06)                        | R version 3.5.1
(2018-07-02)
[1] as.list.data.frame      as.list.Date            | [1]
as.list.data.frame      as.list.Date
[3] as.list.default         as.list.environment     | [3] as.list.default
       as.list.environment
[5] as.list.factor          as.list.function        | [5] as.list.factor
      as.list.function
[7] as.list.numeric_version as.list.POSIXct         | [7]
as.list.numeric_version as.list.POSIXct
see '?methods' for accessing help and source code   | [9] as.list.POSIXlt
                                                    | see '?methods' for
accessing help and source code


Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Thu, Feb 14, 2019 at 9:45 AM Newell, Paul <paul.newell at metoffice.gov.uk>
wrote:

> Dear R-helpers,
>
> We have recently upgraded from R-3.3.1 to R-3.5.2.
>
> It seems there has been a change in behaviour of `lapply` and the
> `POSIXlt` class that I cannot find explicitly documented.
>
>
> In R-3.3.1:
>
> > lapply(as.POSIXlt(Sys.Date()), length)
> $sec
> [1] 1
> $min
> [1] 1
> $hour
> [1] 1
> $mday
> [1] 1
> $mon
> [1] 1
> $year
> [1] 1
> $wday
> [1] 1
> $yday
> [1] 1
> $isdst
> [1] 1
>
>
> whereas, in R-3.5.2:
>
> > lapply(as.POSIXlt(Sys.Date()), length)
> [[1]]
> [1] 1
>
>
> Is this change in behaviour intentional?
>
> Realistically, I cannot see anything documented to say that `lapply`
> should behave as per R-3.3.1 on a `POSIXlt` object, so it is/was perhaps
> unwise to rely on it.
>
>
> Best wishes,
> Paul Newell
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From |hb @end|ng |rom k@u@edu  Thu Feb 14 20:34:57 2019
From: |hb @end|ng |rom k@u@edu (Isaac Barnhart)
Date: Thu, 14 Feb 2019 19:34:57 +0000
Subject: [R] Finding the Mean of a Specific Set of Columns
In-Reply-To: <3344F7AE-92EB-4E3E-A2FA-30D231861891@me.com>
References: <BN6PR05MB338093AAA324C660B6D33140AE670@BN6PR05MB3380.namprd05.prod.outlook.com>,
 <3344F7AE-92EB-4E3E-A2FA-30D231861891@me.com>
Message-ID: <BN6PR05MB33802603B26D487B4A34E7DFAE670@BN6PR05MB3380.namprd05.prod.outlook.com>

Thanks for the help!


Isaac



________________________________
From: Marc Schwartz <marc_schwartz at me.com>
Sent: Thursday, February 14, 2019 11:05:23 AM
To: Isaac Barnhart
Cc: R-help
Subject: Re: [R] Finding the Mean of a Specific Set of Columns

On Feb 14, 2019, at 9:31 AM, Isaac Barnhart <ihb at ksu.edu> wrote:
>
> I am having trouble finding the mean of a specific part of my dataset. Here is a sample of it:
>
> plot lai leaf
> 1 104 82 1
> 2 104 167 2
> 3 104 248 3
> 4 104 343 4
> 5 104 377 5
> 6 105 64 1
> 7 105 139 2
> 8 105 211 3
> 9 105 296 4
> 10 105 348 5
> 11 106 94 1
> 12 106 167 2
> 13 106 243 3
> 14 106 281 4
> 15 106 332 5
> 16 108 83 1
> 17 108 382 2
> 18 108 320 3
> 19 108 146 4
> 20 108 129 5
>
> I have many different plot numbers, none of which follow any kind of specific numeric sequence (even though I have sorted them from smallest to largest). I need to take the average (mean) of the LAI for each plot, and was wondering if there was a way to specify the code to do this. For example: I need the average of all the LAI measurements for each leaf of plot 104, 105, etc. Any help would be appreciated. Thanks!


Hi,

This is easy using base R functions. See ?aggregate, ?by and ?tapply for a starting place.

For example:

> aggregate(lai ~ plot, data = DF, FUN = mean)
  plot   lai
1  104 243.4
2  105 211.6
3  106 223.4
4  108 212.0

Regards,

Marc Schwartz




	[[alternative HTML version deleted]]


From goure@hk @end|ng |rom |c|oud@com  Thu Feb 14 20:47:04 2019
From: goure@hk @end|ng |rom |c|oud@com (Gouresh Kamble)
Date: Thu, 14 Feb 2019 20:47:04 +0100
Subject: [R] R help
Message-ID: <49DC8004-EE92-4DD4-85D4-59A942D91E35@icloud.com>

Dear to whom it may concern on the R team, 



I am having an issue with creating a code in which i can hold information such as the author of a paper, the year of publication, and the title. Also would like to add into this data frame a logical variable which would show some keywords I used to find the data. I keep getting stuck on how to create specific characters for the table. 




Kind Regards, 
Gouresh Kamble 

From @@r@h@go@|ee @end|ng |rom gm@||@com  Fri Feb 15 00:08:00 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Thu, 14 Feb 2019 18:08:00 -0500
Subject: [R] R help
In-Reply-To: <49DC8004-EE92-4DD4-85D4-59A942D91E35@icloud.com>
References: <49DC8004-EE92-4DD4-85D4-59A942D91E35@icloud.com>
Message-ID: <CAM_vju=ChHXW+iZLW_xcSku+CBM=d7X8fiEN+Lo9K_izt_gfSg@mail.gmail.com>

There's a bibtex parser for R: you could adapt that for your use,
rather than trying to reinvent the equivalent tool.

https://cran.r-project.org/web/packages/bibtex/index.html

Sarah

On Thu, Feb 14, 2019 at 5:57 PM Gouresh Kamble via R-help
<r-help at r-project.org> wrote:
>
> Dear to whom it may concern on the R team,
>
>
>
> I am having an issue with creating a code in which i can hold information such as the author of a paper, the year of publication, and the title. Also would like to add into this data frame a logical variable which would show some keywords I used to find the data. I keep getting stuck on how to create specific characters for the table.
>
>
>

-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From g||ted|||e2014 @end|ng |rom gm@||@com  Fri Feb 15 07:29:58 2019
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Fri, 15 Feb 2019 07:29:58 +0100
Subject: [R] Extending my code
Message-ID: <CAC8ss32JwM+VXDDkRNLi5frjxzhu-UsgQ+THbeTaSNG2BhpuJg@mail.gmail.com>

Dear List,
I have a simple code with which I convert year, month, and day to a date format.
My data looks like:
67 01 26    18464
67 01 26    18472
67 01 26    18408
67 01 26    18360
67 01 26    18328
67 01 26    18320
67 01 26    18296

while my code is:


data <- read.table("CALG.txt", col.names = c("year", "month", "day", "counts"))
 new.century <- data$year < 50
data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)
data$date <- as.Date(ISOdate(data$year, data$month, data$day))
x = data$date
 y = data$counts

I now wish to extend this code to be able to include hour for another
data of the format:
05 01 01 00    4009
05 01 01 01    3969
05 01 01 02    3946
05 01 01 03    3975
05 01 01 04    3960
05 01 01 05    3974
05 01 01 06    3971
05 01 01 07    3970
That is, I now would like to include hour in:
data <- read.table("CALG.txt", col.names = c("year", "month", "day", "counts")).

I am aware there are many other way of conversion but I have a
specific interest here. This code is a preamble to a larger code and
changing it to another format other than what I have will not be
compatible with the general code. Or will rather be difficult for me
to get another format fit into my main code.

So if you would be kind enough to assist me to run the read.table in the format:

data <- read.table("CALG.txt", col.names = c("year", "month",
"day","hour", "counts"))

and then run the rest as:

new.century <- data$year < 50
data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)
data$date <- as.Date(ISOdate(data$year, data$month, data$day,data$hour))
x = data$date

such that year, month, day and hour will be stored in x,

I will be very thankful.

Thank you so much for your kind assistance.
Best regards
Ogbos


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri Feb 15 07:51:06 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 14 Feb 2019 22:51:06 -0800
Subject: [R] Extending my code
In-Reply-To: <CAC8ss32JwM+VXDDkRNLi5frjxzhu-UsgQ+THbeTaSNG2BhpuJg@mail.gmail.com>
References: <CAC8ss32JwM+VXDDkRNLi5frjxzhu-UsgQ+THbeTaSNG2BhpuJg@mail.gmail.com>
Message-ID: <A4FD44B7-A286-4568-882F-19F350A74182@dcn.davis.ca.us>

The Date class is not designed to handle time... you need to use the ISOdatetime function and convert to POSIXct instead of Date. Just be sure to set your timezone to some appropriate value before you convert any times into datetime types.

Sys.setenv( TZ="GMT" )
# avoid using `data` as that is the name of a base R function
dta <- read.table("CALG.txt", col.names = c("year", "month", "day", "hour", "counts"))
dta$year <- with( dta, ifelse(year < 50, year + 2000, year + 1900)
dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year, month,day,hour,0,0)))

I don't see why you feel obliged to copy the timestamp out of the data frame into x, but that is your business.

Appropriate timezone values can be reviewed with the OlsonNames() function. 


On February 14, 2019 10:29:58 PM PST, Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>Dear List,
>I have a simple code with which I convert year, month, and day to a
>date format.
>My data looks like:
>67 01 26    18464
>67 01 26    18472
>67 01 26    18408
>67 01 26    18360
>67 01 26    18328
>67 01 26    18320
>67 01 26    18296
>
>while my code is:
>
>
>data <- read.table("CALG.txt", col.names = c("year", "month", "day",
>"counts"))
> new.century <- data$year < 50
>data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)
>data$date <- as.Date(ISOdate(data$year, data$month, data$day))
>x = data$date
> y = data$counts
>
>I now wish to extend this code to be able to include hour for another
>data of the format:
>05 01 01 00    4009
>05 01 01 01    3969
>05 01 01 02    3946
>05 01 01 03    3975
>05 01 01 04    3960
>05 01 01 05    3974
>05 01 01 06    3971
>05 01 01 07    3970
>That is, I now would like to include hour in:
>data <- read.table("CALG.txt", col.names = c("year", "month", "day",
>"counts")).
>
>I am aware there are many other way of conversion but I have a
>specific interest here. This code is a preamble to a larger code and
>changing it to another format other than what I have will not be
>compatible with the general code. Or will rather be difficult for me
>to get another format fit into my main code.
>
>So if you would be kind enough to assist me to run the read.table in
>the format:
>
>data <- read.table("CALG.txt", col.names = c("year", "month",
>"day","hour", "counts"))
>
>and then run the rest as:
>
>new.century <- data$year < 50
>data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)
>data$date <- as.Date(ISOdate(data$year, data$month,
>data$day,data$hour))
>x = data$date
>
>such that year, month, day and hour will be stored in x,
>
>I will be very thankful.
>
>Thank you so much for your kind assistance.
>Best regards
>Ogbos
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From g||ted|||e2014 @end|ng |rom gm@||@com  Fri Feb 15 08:15:02 2019
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Fri, 15 Feb 2019 08:15:02 +0100
Subject: [R] Extending my code
In-Reply-To: <A4FD44B7-A286-4568-882F-19F350A74182@dcn.davis.ca.us>
References: <CAC8ss32JwM+VXDDkRNLi5frjxzhu-UsgQ+THbeTaSNG2BhpuJg@mail.gmail.com>
 <A4FD44B7-A286-4568-882F-19F350A74182@dcn.davis.ca.us>
Message-ID: <CAC8ss31oh-QiyVLNxnBX+KHnnnDHZW0SUL4SQMDf9jN=6R1TyA@mail.gmail.com>

Dear Jeff,

Thank you so much.

I ran the code but got an error message. I then try to  run them line by line.

The problem is in:
dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year, month,day,hour,0,0)))
Error in with(dta, as.POSIXct(ISOdatetime(year, month, day, hour, 0, 0))) :
  object 'dta' not found

Thanks for another time.
Best
Ogbos

On Fri, Feb 15, 2019 at 7:51 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>
> The Date class is not designed to handle time... you need to use the ISOdatetime function and convert to POSIXct instead of Date. Just be sure to set your timezone to some appropriate value before you convert any times into datetime types.
>
> Sys.setenv( TZ="GMT" )
> # avoid using `data` as that is the name of a base R function
> dta <- read.table("CALG.txt", col.names = c("year", "month", "day", "hour", "counts"))
> dta$year <- with( dta, ifelse(year < 50, year + 2000, year + 1900)
> dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year, month,day,hour,0,0)))
>
> I don't see why you feel obliged to copy the timestamp out of the data frame into x, but that is your business.
>
> Appropriate timezone values can be reviewed with the OlsonNames() function.
>
>
> On February 14, 2019 10:29:58 PM PST, Ogbos Okike <giftedlife2014 at gmail.com> wrote:
> >Dear List,
> >I have a simple code with which I convert year, month, and day to a
> >date format.
> >My data looks like:
> >67 01 26    18464
> >67 01 26    18472
> >67 01 26    18408
> >67 01 26    18360
> >67 01 26    18328
> >67 01 26    18320
> >67 01 26    18296
> >
> >while my code is:
> >
> >
> >data <- read.table("CALG.txt", col.names = c("year", "month", "day",
> >"counts"))
> > new.century <- data$year < 50
> >data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)
> >data$date <- as.Date(ISOdate(data$year, data$month, data$day))
> >x = data$date
> > y = data$counts
> >
> >I now wish to extend this code to be able to include hour for another
> >data of the format:
> >05 01 01 00    4009
> >05 01 01 01    3969
> >05 01 01 02    3946
> >05 01 01 03    3975
> >05 01 01 04    3960
> >05 01 01 05    3974
> >05 01 01 06    3971
> >05 01 01 07    3970
> >That is, I now would like to include hour in:
> >data <- read.table("CALG.txt", col.names = c("year", "month", "day",
> >"counts")).
> >
> >I am aware there are many other way of conversion but I have a
> >specific interest here. This code is a preamble to a larger code and
> >changing it to another format other than what I have will not be
> >compatible with the general code. Or will rather be difficult for me
> >to get another format fit into my main code.
> >
> >So if you would be kind enough to assist me to run the read.table in
> >the format:
> >
> >data <- read.table("CALG.txt", col.names = c("year", "month",
> >"day","hour", "counts"))
> >
> >and then run the rest as:
> >
> >new.century <- data$year < 50
> >data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)
> >data$date <- as.Date(ISOdate(data$year, data$month,
> >data$day,data$hour))
> >x = data$date
> >
> >such that year, month, day and hour will be stored in x,
> >
> >I will be very thankful.
> >
> >Thank you so much for your kind assistance.
> >Best regards
> >Ogbos
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.


From g||ted|||e2014 @end|ng |rom gm@||@com  Fri Feb 15 08:25:11 2019
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Fri, 15 Feb 2019 08:25:11 +0100
Subject: [R] Extending my code
In-Reply-To: <CAC8ss31oh-QiyVLNxnBX+KHnnnDHZW0SUL4SQMDf9jN=6R1TyA@mail.gmail.com>
References: <CAC8ss32JwM+VXDDkRNLi5frjxzhu-UsgQ+THbeTaSNG2BhpuJg@mail.gmail.com>
 <A4FD44B7-A286-4568-882F-19F350A74182@dcn.davis.ca.us>
 <CAC8ss31oh-QiyVLNxnBX+KHnnnDHZW0SUL4SQMDf9jN=6R1TyA@mail.gmail.com>
Message-ID: <CAC8ss32CM=vbWTWA-13qa-cCUY8ADK3Z7N-XYo-K8D6w-AoiCQ@mail.gmail.com>

Dear Jeff,

Please hold.
It is begging to work. There was an error somewhere. One ")" is
missing and as I went back to check the lines one by one with cursor,
I stubbed on non matching bracket.

I completed, run the code again and got some result.

Will get back to you once I am through.

Thanks in a hurry.
Best regards
Ogbos


On Fri, Feb 15, 2019 at 8:15 AM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>
> Dear Jeff,
>
> Thank you so much.
>
> I ran the code but got an error message. I then try to  run them line by line.
>
> The problem is in:
> dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year, month,day,hour,0,0)))
> Error in with(dta, as.POSIXct(ISOdatetime(year, month, day, hour, 0, 0))) :
>   object 'dta' not found
>
> Thanks for another time.
> Best
> Ogbos
>
> On Fri, Feb 15, 2019 at 7:51 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> >
> > The Date class is not designed to handle time... you need to use the ISOdatetime function and convert to POSIXct instead of Date. Just be sure to set your timezone to some appropriate value before you convert any times into datetime types.
> >
> > Sys.setenv( TZ="GMT" )
> > # avoid using `data` as that is the name of a base R function
> > dta <- read.table("CALG.txt", col.names = c("year", "month", "day", "hour", "counts"))
> > dta$year <- with( dta, ifelse(year < 50, year + 2000, year + 1900)
> > dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year, month,day,hour,0,0)))
> >
> > I don't see why you feel obliged to copy the timestamp out of the data frame into x, but that is your business.
> >
> > Appropriate timezone values can be reviewed with the OlsonNames() function.
> >
> >
> > On February 14, 2019 10:29:58 PM PST, Ogbos Okike <giftedlife2014 at gmail.com> wrote:
> > >Dear List,
> > >I have a simple code with which I convert year, month, and day to a
> > >date format.
> > >My data looks like:
> > >67 01 26    18464
> > >67 01 26    18472
> > >67 01 26    18408
> > >67 01 26    18360
> > >67 01 26    18328
> > >67 01 26    18320
> > >67 01 26    18296
> > >
> > >while my code is:
> > >
> > >
> > >data <- read.table("CALG.txt", col.names = c("year", "month", "day",
> > >"counts"))
> > > new.century <- data$year < 50
> > >data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)
> > >data$date <- as.Date(ISOdate(data$year, data$month, data$day))
> > >x = data$date
> > > y = data$counts
> > >
> > >I now wish to extend this code to be able to include hour for another
> > >data of the format:
> > >05 01 01 00    4009
> > >05 01 01 01    3969
> > >05 01 01 02    3946
> > >05 01 01 03    3975
> > >05 01 01 04    3960
> > >05 01 01 05    3974
> > >05 01 01 06    3971
> > >05 01 01 07    3970
> > >That is, I now would like to include hour in:
> > >data <- read.table("CALG.txt", col.names = c("year", "month", "day",
> > >"counts")).
> > >
> > >I am aware there are many other way of conversion but I have a
> > >specific interest here. This code is a preamble to a larger code and
> > >changing it to another format other than what I have will not be
> > >compatible with the general code. Or will rather be difficult for me
> > >to get another format fit into my main code.
> > >
> > >So if you would be kind enough to assist me to run the read.table in
> > >the format:
> > >
> > >data <- read.table("CALG.txt", col.names = c("year", "month",
> > >"day","hour", "counts"))
> > >
> > >and then run the rest as:
> > >
> > >new.century <- data$year < 50
> > >data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)
> > >data$date <- as.Date(ISOdate(data$year, data$month,
> > >data$day,data$hour))
> > >x = data$date
> > >
> > >such that year, month, day and hour will be stored in x,
> > >
> > >I will be very thankful.
> > >
> > >Thank you so much for your kind assistance.
> > >Best regards
> > >Ogbos
> > >
> > >______________________________________________
> > >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >https://stat.ethz.ch/mailman/listinfo/r-help
> > >PLEASE do read the posting guide
> > >http://www.R-project.org/posting-guide.html
> > >and provide commented, minimal, self-contained, reproducible code.
> >
> > --
> > Sent from my phone. Please excuse my brevity.


From g||ted|||e2014 @end|ng |rom gm@||@com  Fri Feb 15 08:52:31 2019
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Fri, 15 Feb 2019 08:52:31 +0100
Subject: [R] Extending my code: SOLVED
In-Reply-To: <CAC8ss32CM=vbWTWA-13qa-cCUY8ADK3Z7N-XYo-K8D6w-AoiCQ@mail.gmail.com>
References: <CAC8ss32JwM+VXDDkRNLi5frjxzhu-UsgQ+THbeTaSNG2BhpuJg@mail.gmail.com>
 <A4FD44B7-A286-4568-882F-19F350A74182@dcn.davis.ca.us>
 <CAC8ss31oh-QiyVLNxnBX+KHnnnDHZW0SUL4SQMDf9jN=6R1TyA@mail.gmail.com>
 <CAC8ss32CM=vbWTWA-13qa-cCUY8ADK3Z7N-XYo-K8D6w-AoiCQ@mail.gmail.com>
Message-ID: <CAC8ss30DR+oL5NLWj-4nq8h8DLWNa4ewgBiRtDQX85864gJ0Cw@mail.gmail.com>

Dear Jeff,
I am alright now!!!!

Please accept my indebtedness!!!

Warmest regards
Ogbos
On Fri, Feb 15, 2019 at 8:25 AM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>
> Dear Jeff,
>
> Please hold.
> It is begging to work. There was an error somewhere. One ")" is
> missing and as I went back to check the lines one by one with cursor,
> I stubbed on non matching bracket.
>
> I completed, run the code again and got some result.
>
> Will get back to you once I am through.
>
> Thanks in a hurry.
> Best regards
> Ogbos
>
>
> On Fri, Feb 15, 2019 at 8:15 AM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
> >
> > Dear Jeff,
> >
> > Thank you so much.
> >
> > I ran the code but got an error message. I then try to  run them line by line.
> >
> > The problem is in:
> > dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year, month,day,hour,0,0)))
> > Error in with(dta, as.POSIXct(ISOdatetime(year, month, day, hour, 0, 0))) :
> >   object 'dta' not found
> >
> > Thanks for another time.
> > Best
> > Ogbos
> >
> > On Fri, Feb 15, 2019 at 7:51 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> > >
> > > The Date class is not designed to handle time... you need to use the ISOdatetime function and convert to POSIXct instead of Date. Just be sure to set your timezone to some appropriate value before you convert any times into datetime types.
> > >
> > > Sys.setenv( TZ="GMT" )
> > > # avoid using `data` as that is the name of a base R function
> > > dta <- read.table("CALG.txt", col.names = c("year", "month", "day", "hour", "counts"))
> > > dta$year <- with( dta, ifelse(year < 50, year + 2000, year + 1900)
> > > dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year, month,day,hour,0,0)))
> > >
> > > I don't see why you feel obliged to copy the timestamp out of the data frame into x, but that is your business.
> > >
> > > Appropriate timezone values can be reviewed with the OlsonNames() function.
> > >
> > >
> > > On February 14, 2019 10:29:58 PM PST, Ogbos Okike <giftedlife2014 at gmail.com> wrote:
> > > >Dear List,
> > > >I have a simple code with which I convert year, month, and day to a
> > > >date format.
> > > >My data looks like:
> > > >67 01 26    18464
> > > >67 01 26    18472
> > > >67 01 26    18408
> > > >67 01 26    18360
> > > >67 01 26    18328
> > > >67 01 26    18320
> > > >67 01 26    18296
> > > >
> > > >while my code is:
> > > >
> > > >
> > > >data <- read.table("CALG.txt", col.names = c("year", "month", "day",
> > > >"counts"))
> > > > new.century <- data$year < 50
> > > >data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)
> > > >data$date <- as.Date(ISOdate(data$year, data$month, data$day))
> > > >x = data$date
> > > > y = data$counts
> > > >
> > > >I now wish to extend this code to be able to include hour for another
> > > >data of the format:
> > > >05 01 01 00    4009
> > > >05 01 01 01    3969
> > > >05 01 01 02    3946
> > > >05 01 01 03    3975
> > > >05 01 01 04    3960
> > > >05 01 01 05    3974
> > > >05 01 01 06    3971
> > > >05 01 01 07    3970
> > > >That is, I now would like to include hour in:
> > > >data <- read.table("CALG.txt", col.names = c("year", "month", "day",
> > > >"counts")).
> > > >
> > > >I am aware there are many other way of conversion but I have a
> > > >specific interest here. This code is a preamble to a larger code and
> > > >changing it to another format other than what I have will not be
> > > >compatible with the general code. Or will rather be difficult for me
> > > >to get another format fit into my main code.
> > > >
> > > >So if you would be kind enough to assist me to run the read.table in
> > > >the format:
> > > >
> > > >data <- read.table("CALG.txt", col.names = c("year", "month",
> > > >"day","hour", "counts"))
> > > >
> > > >and then run the rest as:
> > > >
> > > >new.century <- data$year < 50
> > > >data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)
> > > >data$date <- as.Date(ISOdate(data$year, data$month,
> > > >data$day,data$hour))
> > > >x = data$date
> > > >
> > > >such that year, month, day and hour will be stored in x,
> > > >
> > > >I will be very thankful.
> > > >
> > > >Thank you so much for your kind assistance.
> > > >Best regards
> > > >Ogbos
> > > >
> > > >______________________________________________
> > > >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > >https://stat.ethz.ch/mailman/listinfo/r-help
> > > >PLEASE do read the posting guide
> > > >http://www.R-project.org/posting-guide.html
> > > >and provide commented, minimal, self-contained, reproducible code.
> > >
> > > --
> > > Sent from my phone. Please excuse my brevity.


From em@n|@m@||@92 @end|ng |rom gm@||@com  Fri Feb 15 09:32:24 2019
From: em@n|@m@||@92 @end|ng |rom gm@||@com (Eman)
Date: Fri, 15 Feb 2019 10:32:24 +0200
Subject: [R] Simulate High Dimensional Correlated Binary Data
Message-ID: <B827CFF9-2892-4A93-8B0E-802EED66BBBC@gmail.com>

Hi all,
I have correlated binary data which have around 2000 columns. I need to simulate data like them I have marginal probability vector and correlation matrix.
I tried bindata lib but didn?t work with me as l have some negative correlation and I tried also mipfp but give error with using all 2000 columns.
Could anyone help me with that,please ?
Suggestions to fix errors or use another way.
I appreciate any help thanks

From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Fri Feb 15 10:25:05 2019
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 15 Feb 2019 10:25:05 +0100
Subject: [R] 
 Unexpected errors in sparse Matrix arithmetic with zero-length
 dimensions
In-Reply-To: <1549812137.3935.16.camel@gmail.com>
References: <1549812137.3935.16.camel@gmail.com>
Message-ID: <23654.34161.308207.463239@stat.math.ethz.ch>

>>>>> Aaron Lun 
>>>>>     on Sun, 10 Feb 2019 15:22:17 +0000 writes:

    > Dear list,
    > The Matrix package exhibits some unexpected behaviour in its arithmetic
    > methods for the edge case of a sparse matrix with a dimension of zero
    > length. The example below is the most illustrative, where changing the
    > contents of the vector causes the subtraction to fail for a sparse
    > matrix with no columns:?
    > ????
    >> library(Matrix)
    >> x <- rsparsematrix(10, 0, density=0.1)
    >> ?
    >> x - rep(1, nrow(x)) # OK?
    >> x - rep(0, nrow(x)) # fails
    > Error in .Ops.recycle.ind(e1, len = l2) :?
    > ? vector too long in Matrix - vector operation

This is indeed clearly a lapsus of us / mine  as well as the
next examples:  Will all be fixed  "around" .Ops.recycle.ind() 

    > This is presumably because Matrix recognizes that subtraction of zero
    > preserves sparsity and thus uses a different method in the second case.
    > However, I would have expected subtraction of a zero vector to work if
    > subtraction of a general vector is permissible. This is accompanied by
    > a host of related errors for sparsity-preserving arithmetic:

    >> x / 1 # OK
    >> x / rep(1, nrow(x)) # fails?
    > Error in .Ops.recycle.ind(e1, len = l2) :?
    > ? vector too long in Matrix - vector operation
    >> ?
    >> x * 1 # OK
    >> x * rep(1, nrow(x)) # fails
    > Error in .Ops.recycle.ind(e1, len = l2) :?
    > ? vector too long in Matrix - vector operation
    > ??????
    > A different error is raised for a sparse matrix with no rows:

    >> y <- rsparsematrix(0, 10, density=0.1)
    >> ?
    >> y - numeric(1) # OK
    >> y - numeric(0) # fails
    > Error in y - numeric(0) : <Matrix> - numeric(0) is undefined

Thank you, that's another lapsus, I will fix before the next
release of Matrix.

    > I would have expected to just get 'y' back, given that the same code
    > works fine for other Matrix classes:

    >> z <- as(y, "dgeMatrix")
    >> z - numeric(0) # OK

sure.

    > Correct behaviour of zero-dimension sparse matrices is practically
    > important to me; I develop a number of packages that rely on Matrix
    > classes, and in those packages, I do a lot of unit testing with zero-
    > dimension inputs. This ensures that my functions return sensible
    > results or fail gracefully in edge cases that might be encountered by
    > users. The current behaviour of sparse Matrix arithmetic causes my unit
    > tests to fail for no (obvious) good reason.

Interesting that you need 0-dim sparse matrices.  I agree they
should work, too... and will fix
(but it seems they haven't been used much by others;  else I
would have expected these cases to have been reported long ago).

Further note that in R,

  maintainer("Matrix")

gives a nice address to send such findings.
(similarly for all other R packages !)

Thank you very much once more!
Martin

    > Best,
    > Aaron Lun

    > Research Associate
    > CRUK Cambridge Institute
    > University of Cambridge

--
Martin <Maechler at ....>   
Seminar f?r Statistik, ETH Z?rich   HG G 16         R?mistrasse 101
CH-8092 Zurich, SWITZERLAND


From p@u|@newe|| @end|ng |rom meto|||ce@gov@uk  Fri Feb 15 13:15:38 2019
From: p@u|@newe|| @end|ng |rom meto|||ce@gov@uk (Newell, Paul)
Date: Fri, 15 Feb 2019 12:15:38 +0000
Subject: [R] POSIXlt class and lapply
In-Reply-To: <CAF8bMcb0CitdMRt1md00c2pe9G8uEXYuex_+wOySZ_ra_MaxCA@mail.gmail.com>
References: <VI1PR01MB4416CFC4412E1FC7256C217CD9670@VI1PR01MB4416.eurprd01.prod.exchangelabs.com>,
 <CAF8bMcb0CitdMRt1md00c2pe9G8uEXYuex_+wOySZ_ra_MaxCA@mail.gmail.com>
Message-ID: <VI1PR01MB441623E25C023B140D9017BAD9600@VI1PR01MB4416.eurprd01.prod.exchangelabs.com>

Many thanks Bill Dunlap.

You are correct that `lapply` calls `as.list`, which I should have seen if I had looked a little harder.

Whether that would have led me to locate `as.list.POSIXlt` is another matter.

Best wishes.



From: William Dunlap <wdunlap at tibco.com>
Sent: 14 February 2019 20:03
To: Newell, Paul
Cc: r-help at r-project.org
Subject: Re: [R] POSIXlt class and lapply
?
Somewhere between R-3.3.3 and R-3.5.2 a POSIXlt method for as.list() was added, and lapply probably calls as.list().


> RCompare(methods("as.list"))
R version 3.3.3 (2017-03-06)? ? ? ? ? ? ? ? ? ? ? ? | R version 3.5.1 (2018-07-02)
[1] as.list.data.frame? ? ? as.list.Date? ? ? ? ? ? | [1] as.list.data.frame? ? ? as.list.Date
[3] as.list.default? ? ? ? ?as.list.environment? ? ?| [3] as.list.default? ? ? ? ?as.list.environment
[5] as.list.factor? ? ? ? ? as.list.function? ? ? ? | [5] as.list.factor? ? ? ? ? as.list.function
[7] as.list.numeric_version as.list.POSIXct? ? ? ? ?| [7] as.list.numeric_version as.list.POSIXct
see '?methods' for accessing help and source code? ?| [9] as.list.POSIXlt
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? | see '?methods' for accessing help and source code




Bill Dunlap
TIBCO Software
wdunlap tibco.com




On Thu, Feb 14, 2019 at 9:45 AM Newell, Paul <paul.newell at metoffice.gov.uk> wrote:

Dear R-helpers,

We have recently upgraded from R-3.3.1 to R-3.5.2.

It seems there has been a change in behaviour of `lapply` and the `POSIXlt` class that I cannot find explicitly documented.


In R-3.3.1:

> lapply(as.POSIXlt(Sys.Date()), length)
$sec
[1] 1
$min
[1] 1
$hour
[1] 1
$mday
[1] 1
$mon
[1] 1
$year
[1] 1
$wday
[1] 1
$yday
[1] 1
$isdst
[1] 1


whereas, in R-3.5.2:

> lapply(as.POSIXlt(Sys.Date()), length)
[[1]]
[1] 1


Is this change in behaviour intentional?

Realistically, I cannot see anything documented to say that `lapply` should behave as per R-3.3.1 on a `POSIXlt` object, so it is/was perhaps unwise to rely on it.


Best wishes,
Paul Newell
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From B|||@Po||ng @end|ng |rom ze||@@com  Fri Feb 15 14:37:25 2019
From: B|||@Po||ng @end|ng |rom ze||@@com (Bill Poling)
Date: Fri, 15 Feb 2019 13:37:25 +0000
Subject: [R] Help with Cluster Tutorial Error
Message-ID: <BN7PR02MB50736C89D44733ACB0B46DA8EA600@BN7PR02MB5073.namprd02.prod.outlook.com>

sessionInfo()
#R version 3.5.2 (2018-12-20)
#Platform: x86_64-w64-mingw32/x64 (64-bit)
#Running under: Windows >= 8 x64 (build 9200)

Hello I am working through this tutorial https://www.r-bloggers.com/10-tips-for-choosing-the-optimal-number-of-clusters/
And I run into an error almost immediately at the point below like this:

 mammals <- raw_mammals %>% select(-name) # set rownames
#Error in select(., -name) : object 'p_links' not found

I have googled the error " R object 'p_links' not found"
https://stats.stackexchange.com/questions/113907/error-object-descr-not-found
However, the links I have found seem to be specific to an object that has been declared by the Op, and in my case I do not know where this object is supposed to be coming from?

I think this stems from the use of the dplyr Pkg?

https://dplyr.tidyverse.org/reference/select.html

Here is what I have so far:

#Data Set
# I will be using a lesser known data set from the cluster package: all.mammals.milk.1956, one which I haven't looked at before.
#
# This small dataset contains a list of 25 mammals and the constituents of their milk (water, protein, fat, lactose, ash percentages) from John Hartigan, Clustering Algorithms, Wiley, 1975.
#
# First let's load the required packages. Some of these are already in my library, those that are not were installed

library(tidyverse)
library(magrittr)
library(cluster)
install.packages("cluster.datasets")
install.packages("NbClust")
install.packages("clValid")
install.packages("ggfortify")
install.packages("clustree")
install.packages("ggiraphExtra")
library(cowplot)
library(cluster.datasets)
library(NbClust)
library(clValid)
library(ggfortify)
library(clustree)
library(ggiraphExtra)
library(dendextend)
library(factoextra)
library(FactoMineR)
library(corrplot)
library(GGally)
library(knitr)
library(kableExtra)


data("all.mammals.milk.1956")
raw_mammals <- all.mammals.milk.1956
str(raw_mammals)  #----------------------KNOW THY DATA

# 'data.frame':25 obs. of  6 variables:
# $ name   : chr  "Horse" "Orangutan" "Monkey" "Donkey" ...
# $ water  : num  90.1 88.5 88.4 90.3 90.4 87.7 86.9 82.1 81.9 81.6 ...
# $ protein: num  2.6 1.4 2.2 1.7 0.6 3.5 4.8 5.9 7.4 10.1 ...
# $ fat    : num  1 3.5 2.7 1.4 4.5 3.4 1.7 7.9 7.2 6.3 ...
# $ lactose: num  6.9 6 6.4 6.2 4.4 4.8 5.7 4.7 2.7 4.4 ...
# $ ash    : num  0.35 0.24 0.18 0.4 0.1 0.71 0.9 0.78 0.85 0.75 ...

#raw_mammals <- fread("Animal Milk Constituent Percentages.csv",header=TRUE, stringsAsFactors=TRUE)

# subset dataset
mammals <- raw_mammals %>% select(-name) # set rownames
#Error in select(., -name) : object 'p_links' not found  <--The Error

I hope this is enough information to help answer my question.

Thank you

WHP




Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}


From |@t@z@hn @end|ng |rom gm@||@com  Fri Feb 15 14:55:15 2019
From: |@t@z@hn @end|ng |rom gm@||@com (Ista Zahn)
Date: Fri, 15 Feb 2019 08:55:15 -0500
Subject: [R] POSIXlt class and lapply
In-Reply-To: <VI1PR01MB441623E25C023B140D9017BAD9600@VI1PR01MB4416.eurprd01.prod.exchangelabs.com>
References: <VI1PR01MB4416CFC4412E1FC7256C217CD9670@VI1PR01MB4416.eurprd01.prod.exchangelabs.com>
 <CAF8bMcb0CitdMRt1md00c2pe9G8uEXYuex_+wOySZ_ra_MaxCA@mail.gmail.com>
 <VI1PR01MB441623E25C023B140D9017BAD9600@VI1PR01MB4416.eurprd01.prod.exchangelabs.com>
Message-ID: <CA+vqiLGDM-ayhbTqYPurdMGWRNU+boL9P=oE9XxV9uHiKR40VQ@mail.gmail.com>

As a practical matter, you can't treat POSIXlt as a list. The
documentation could be clearer about this.  ?DateTimeClasses says

"Class ?"POSIXlt"? is a named list of vectors", and then later, "Note
that the internal list structure is somewhat hidden, as many methods
(including ?length(x)?, ?print()? and ?str?) apply to the abstract
date-time vector, as for ?"POSIXct"?."

In other words, "POSIXct" is internally a list, but you can't really
treat it as one. If you want to access the internal list structure
directly, unclass it for first. For example:

> x = as.POSIXlt(Sys.time() - 1:20
>
> length(x)
[1] 20
> length(unclass(x))
[1] 11
> str(x)
 POSIXlt[1:20], format: "2019-02-15 08:53:16" "2019-02-15 08:53:15"
"2019-02-15 08:53:14" ...
> str(unclass(x))
List of 11
 $ sec   : num [1:20] 16.9 15.9 14.9 13.9 12.9 ...
 $ min   : int [1:20] 53 53 53 53 53 53 53 53 53 53 ...
 $ hour  : int [1:20] 8 8 8 8 8 8 8 8 8 8 ...
 $ mday  : int [1:20] 15 15 15 15 15 15 15 15 15 15 ...
 $ mon   : int [1:20] 1 1 1 1 1 1 1 1 1 1 ...
 $ year  : int [1:20] 119 119 119 119 119 119 119 119 119 119 ...
 $ wday  : int [1:20] 5 5 5 5 5 5 5 5 5 5 ...
 $ yday  : int [1:20] 45 45 45 45 45 45 45 45 45 45 ...
 $ isdst : int [1:20] 0 0 0 0 0 0 0 0 0 0 ...
 $ zone  : chr [1:20] "EST" "EST" "EST" "EST" ...
 $ gmtoff: int [1:20] -18000 -18000 -18000 -18000 -18000 -18000 -18000
-18000 -18000 -18000 ...
 - attr(*, "tzone")= chr [1:3] "" "EST" "EDT"
> sapply(x, length)
 [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
> sapply(unclass(x), length)
   sec    min   hour   mday    mon   year   wday   yday  isdst   zone gmtoff
    20     20     20     20     20     20     20     20     20     20     20

Best,
Ista

On Fri, Feb 15, 2019 at 7:15 AM Newell, Paul
<paul.newell at metoffice.gov.uk> wrote:
>
> Many thanks Bill Dunlap.
>
> You are correct that `lapply` calls `as.list`, which I should have seen if I had looked a little harder.
>
> Whether that would have led me to locate `as.list.POSIXlt` is another matter.
>
> Best wishes.
>
>
>
> From: William Dunlap <wdunlap at tibco.com>
> Sent: 14 February 2019 20:03
> To: Newell, Paul
> Cc: r-help at r-project.org
> Subject: Re: [R] POSIXlt class and lapply
>
> Somewhere between R-3.3.3 and R-3.5.2 a POSIXlt method for as.list() was added, and lapply probably calls as.list().
>
>
> > RCompare(methods("as.list"))
> R version 3.3.3 (2017-03-06)                        | R version 3.5.1 (2018-07-02)
> [1] as.list.data.frame      as.list.Date            | [1] as.list.data.frame      as.list.Date
> [3] as.list.default         as.list.environment     | [3] as.list.default         as.list.environment
> [5] as.list.factor          as.list.function        | [5] as.list.factor          as.list.function
> [7] as.list.numeric_version as.list.POSIXct         | [7] as.list.numeric_version as.list.POSIXct
> see '?methods' for accessing help and source code   | [9] as.list.POSIXlt
>                                                     | see '?methods' for accessing help and source code
>
>
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>
>
>
> On Thu, Feb 14, 2019 at 9:45 AM Newell, Paul <paul.newell at metoffice.gov.uk> wrote:
>
> Dear R-helpers,
>
> We have recently upgraded from R-3.3.1 to R-3.5.2.
>
> It seems there has been a change in behaviour of `lapply` and the `POSIXlt` class that I cannot find explicitly documented.
>
>
> In R-3.3.1:
>
> > lapply(as.POSIXlt(Sys.Date()), length)
> $sec
> [1] 1
> $min
> [1] 1
> $hour
> [1] 1
> $mday
> [1] 1
> $mon
> [1] 1
> $year
> [1] 1
> $wday
> [1] 1
> $yday
> [1] 1
> $isdst
> [1] 1
>
>
> whereas, in R-3.5.2:
>
> > lapply(as.POSIXlt(Sys.Date()), length)
> [[1]]
> [1] 1
>
>
> Is this change in behaviour intentional?
>
> Realistically, I cannot see anything documented to say that `lapply` should behave as per R-3.3.1 on a `POSIXlt` object, so it is/was perhaps unwise to rely on it.
>
>
> Best wishes,
> Paul Newell
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From S@E|||@on @end|ng |rom LGCGroup@com  Fri Feb 15 15:56:10 2019
From: S@E|||@on @end|ng |rom LGCGroup@com (S Ellison)
Date: Fri, 15 Feb 2019 14:56:10 +0000
Subject: [R] R help
In-Reply-To: <49DC8004-EE92-4DD4-85D4-59A942D91E35@icloud.com>
References: <49DC8004-EE92-4DD4-85D4-59A942D91E35@icloud.com>
Message-ID: <6544bdb1b1cd483e99ed95dd473d36ff@GBDCVPEXC08.corp.lgc-group.com>

> I am having an issue with creating a code in which i can hold information such
> as the author of a paper, the year of publication, and the title. 
This doesn't really tell me what the trouble is. But  ...

> Also would like
> to add into this data frame a logical variable which would show some
> keywords I used to find the data. 
A logical variable cannot include keywords ('cos it's a logical).
You can add a column for each keyword, though, and that could be logical.
So, for example, you could search text for keywords like "cat" and "mouse", and your data frame could be, say
text                                                cat        mouse
The cat ate the mouse           TRUE   TRUE
The mouse ate the cheese  FALSE  TRUE

and so on.
You can also add a text comment (see ?comment) giving the list of keywords as a single text value, or another attribute (see ?attr) that held a vector of keywords. Those would then be accessible if you passed yourt data frame to a function that needed the keyword list. You could even include the 'keyword' columns above as such an attribute; attributes can be any object. 


> I keep getting stuck on how to create
> specific characters for the table.
'fraid that's not sufficient to comment on.
Try sending an example of what you want to see; someone may be able to work out how to make it happen. 





*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From dc@r|@on @end|ng |rom t@mu@edu  Fri Feb 15 16:05:44 2019
From: dc@r|@on @end|ng |rom t@mu@edu (David L Carlson)
Date: Fri, 15 Feb 2019 15:05:44 +0000
Subject: [R] Help with Cluster Tutorial Error
In-Reply-To: <BN7PR02MB50736C89D44733ACB0B46DA8EA600@BN7PR02MB5073.namprd02.prod.outlook.com>
References: <BN7PR02MB50736C89D44733ACB0B46DA8EA600@BN7PR02MB5073.namprd02.prod.outlook.com>
Message-ID: <49f6653d18c94d0bb86d10eeb48c5da4@tamu.edu>

I'm not getting any error on that line in Windows 10. I did not try running anything past that line.

Have you tried restarting R and clearing your environment?

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Bill Poling
Sent: Friday, February 15, 2019 7:37 AM
To: r-help (r-help at r-project.org) <r-help at r-project.org>
Subject: [R] Help with Cluster Tutorial Error

sessionInfo()
#R version 3.5.2 (2018-12-20)
#Platform: x86_64-w64-mingw32/x64 (64-bit)
#Running under: Windows >= 8 x64 (build 9200)

Hello I am working through this tutorial https://www.r-bloggers.com/10-tips-for-choosing-the-optimal-number-of-clusters/
And I run into an error almost immediately at the point below like this:

 mammals <- raw_mammals %>% select(-name) # set rownames
#Error in select(., -name) : object 'p_links' not found

I have googled the error " R object 'p_links' not found"
https://stats.stackexchange.com/questions/113907/error-object-descr-not-found
However, the links I have found seem to be specific to an object that has been declared by the Op, and in my case I do not know where this object is supposed to be coming from?

I think this stems from the use of the dplyr Pkg?

https://dplyr.tidyverse.org/reference/select.html

Here is what I have so far:

#Data Set
# I will be using a lesser known data set from the cluster package: all.mammals.milk.1956, one which I haven't looked at before.
#
# This small dataset contains a list of 25 mammals and the constituents of their milk (water, protein, fat, lactose, ash percentages) from John Hartigan, Clustering Algorithms, Wiley, 1975.
#
# First let's load the required packages. Some of these are already in my library, those that are not were installed

library(tidyverse)
library(magrittr)
library(cluster)
install.packages("cluster.datasets")
install.packages("NbClust")
install.packages("clValid")
install.packages("ggfortify")
install.packages("clustree")
install.packages("ggiraphExtra")
library(cowplot)
library(cluster.datasets)
library(NbClust)
library(clValid)
library(ggfortify)
library(clustree)
library(ggiraphExtra)
library(dendextend)
library(factoextra)
library(FactoMineR)
library(corrplot)
library(GGally)
library(knitr)
library(kableExtra)


data("all.mammals.milk.1956")
raw_mammals <- all.mammals.milk.1956
str(raw_mammals)  #----------------------KNOW THY DATA

# 'data.frame':25 obs. of  6 variables:
# $ name   : chr  "Horse" "Orangutan" "Monkey" "Donkey" ...
# $ water  : num  90.1 88.5 88.4 90.3 90.4 87.7 86.9 82.1 81.9 81.6 ...
# $ protein: num  2.6 1.4 2.2 1.7 0.6 3.5 4.8 5.9 7.4 10.1 ...
# $ fat    : num  1 3.5 2.7 1.4 4.5 3.4 1.7 7.9 7.2 6.3 ...
# $ lactose: num  6.9 6 6.4 6.2 4.4 4.8 5.7 4.7 2.7 4.4 ...
# $ ash    : num  0.35 0.24 0.18 0.4 0.1 0.71 0.9 0.78 0.85 0.75 ...

#raw_mammals <- fread("Animal Milk Constituent Percentages.csv",header=TRUE, stringsAsFactors=TRUE)

# subset dataset
mammals <- raw_mammals %>% select(-name) # set rownames
#Error in select(., -name) : object 'p_links' not found  <--The Error

I hope this is enough information to help answer my question.

Thank you

WHP




Confidentiality Notice This message is sent from Zelis. ...{{dropped:9}}


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri Feb 15 16:48:02 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 15 Feb 2019 07:48:02 -0800
Subject: [R] Help with Cluster Tutorial Error
In-Reply-To: <49f6653d18c94d0bb86d10eeb48c5da4@tamu.edu>
References: <BN7PR02MB50736C89D44733ACB0B46DA8EA600@BN7PR02MB5073.namprd02.prod.outlook.com>
 <49f6653d18c94d0bb86d10eeb48c5da4@tamu.edu>
Message-ID: <ECF50315-681F-4D88-9AC9-7CCCFFE1BF65@dcn.davis.ca.us>

Another possible issue could be some outdated packages... be sure to update all packages.

On February 15, 2019 7:05:44 AM PST, David L Carlson <dcarlson at tamu.edu> wrote:
>I'm not getting any error on that line in Windows 10. I did not try
>running anything past that line.
>
>Have you tried restarting R and clearing your environment?
>
>----------------------------------------
>David L Carlson
>Department of Anthropology
>Texas A&M University
>College Station, TX 77843-4352
>
>-----Original Message-----
>From: R-help <r-help-bounces at r-project.org> On Behalf Of Bill Poling
>Sent: Friday, February 15, 2019 7:37 AM
>To: r-help (r-help at r-project.org) <r-help at r-project.org>
>Subject: [R] Help with Cluster Tutorial Error
>
>sessionInfo()
>#R version 3.5.2 (2018-12-20)
>#Platform: x86_64-w64-mingw32/x64 (64-bit)
>#Running under: Windows >= 8 x64 (build 9200)
>
>Hello I am working through this tutorial
>https://www.r-bloggers.com/10-tips-for-choosing-the-optimal-number-of-clusters/
>And I run into an error almost immediately at the point below like
>this:
>
> mammals <- raw_mammals %>% select(-name) # set rownames
>#Error in select(., -name) : object 'p_links' not found
>
>I have googled the error " R object 'p_links' not found"
>https://stats.stackexchange.com/questions/113907/error-object-descr-not-found
>However, the links I have found seem to be specific to an object that
>has been declared by the Op, and in my case I do not know where this
>object is supposed to be coming from?
>
>I think this stems from the use of the dplyr Pkg?
>
>https://dplyr.tidyverse.org/reference/select.html
>
>Here is what I have so far:
>
>#Data Set
># I will be using a lesser known data set from the cluster package:
>all.mammals.milk.1956, one which I haven't looked at before.
>#
># This small dataset contains a list of 25 mammals and the constituents
>of their milk (water, protein, fat, lactose, ash percentages) from John
>Hartigan, Clustering Algorithms, Wiley, 1975.
>#
># First let's load the required packages. Some of these are already in
>my library, those that are not were installed
>
>library(tidyverse)
>library(magrittr)
>library(cluster)
>install.packages("cluster.datasets")
>install.packages("NbClust")
>install.packages("clValid")
>install.packages("ggfortify")
>install.packages("clustree")
>install.packages("ggiraphExtra")
>library(cowplot)
>library(cluster.datasets)
>library(NbClust)
>library(clValid)
>library(ggfortify)
>library(clustree)
>library(ggiraphExtra)
>library(dendextend)
>library(factoextra)
>library(FactoMineR)
>library(corrplot)
>library(GGally)
>library(knitr)
>library(kableExtra)
>
>
>data("all.mammals.milk.1956")
>raw_mammals <- all.mammals.milk.1956
>str(raw_mammals)  #----------------------KNOW THY DATA
>
># 'data.frame':25 obs. of  6 variables:
># $ name   : chr  "Horse" "Orangutan" "Monkey" "Donkey" ...
># $ water  : num  90.1 88.5 88.4 90.3 90.4 87.7 86.9 82.1 81.9 81.6 ...
># $ protein: num  2.6 1.4 2.2 1.7 0.6 3.5 4.8 5.9 7.4 10.1 ...
># $ fat    : num  1 3.5 2.7 1.4 4.5 3.4 1.7 7.9 7.2 6.3 ...
># $ lactose: num  6.9 6 6.4 6.2 4.4 4.8 5.7 4.7 2.7 4.4 ...
># $ ash    : num  0.35 0.24 0.18 0.4 0.1 0.71 0.9 0.78 0.85 0.75 ...
>
>#raw_mammals <- fread("Animal Milk Constituent
>Percentages.csv",header=TRUE, stringsAsFactors=TRUE)
>
># subset dataset
>mammals <- raw_mammals %>% select(-name) # set rownames
>#Error in select(., -name) : object 'p_links' not found  <--The Error
>
>I hope this is enough information to help answer my question.
>
>Thank you
>
>WHP
>
>
>
>
>Confidentiality Notice This message is sent from Zelis.
>...{{dropped:9}}
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Fri Feb 15 16:57:41 2019
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Fri, 15 Feb 2019 07:57:41 -0800 (PST)
Subject: [R] Package updates fail: how to fix the causes
Message-ID: <alpine.LNX.2.20.1902150747380.31662@salmo.appl-ecosys.com>

Running R-3.5.2 on Slackware-14.2, using my script that updates installed
packages found four that failed. My web searches did not find relevant hits,
and only the last build failure is explained by the build failure of a
specific dependency. The results displayed are:

ERROR: dependency ?sf? is not available for package ?spdep?
* removing ?/usr/lib/R/library/spdep?

The downloaded source packages are in
 	?/tmp/RtmpzEBBCY/downloaded_packages?
Updating HTML index of packages in '.Library'
Making 'packages.html' ... done
Warning messages:
1: In install.packages(update[instlib == l, "Package"], l, contriburl = contriburl,  :
   installation of package ?units? had non-zero exit status
2: In install.packages(update[instlib == l, "Package"], l, contriburl = contriburl,  :
   installation of package ?later? had non-zero exit status
3: In install.packages(update[instlib == l, "Package"], l, contriburl = contriburl,  :
   installation of package ?sf? had non-zero exit status
4: In install.packages(update[instlib == l, "Package"], l, contriburl = contriburl,  :
   installation of package ?spdep? had non-zero exit status

Starting at the top, trying to install 'units' resulted in identifying a
missing library:

Configuration failed because libudunits2.so was not found. Try installing:
     * deb: libudunits2-dev (Debian, Ubuntu, ...)
     * rpm: udunits2-devel (Fedora, EPEL, ...)
     * brew: udunits (OSX)
   If udunits2 is already installed in a non-standard location, use:
     --configure-args='--with-udunits2-lib=/usr/local/lib'
   if the library was not found, and/or:
     --configure-args='--with-udunits2-include=/usr/include/udunits2'
   if the header was not found, replacing paths with appropriate values.
   You can alternatively set UDUNITS2_INCLUDE and UDUNITS2_LIBS manually.
--------------------------------------------------------------------------------
See `config.log' for more details
ERROR: configuration failed for package ?units?
* removing ?/usr/lib/R/library/units?

The downloaded source packages are in
 	?/tmp/RtmprbnasH/downloaded_packages?
Updating HTML index of packages in '.Library'
Making 'packages.html' ... done
Warning message:
In install.packages("units") :
   installation of package ?units? had non-zero exit status

Please advise me on how to proceed so these four packages are eventually
updated or re-installed.

TIA,

Rich


From bgunter@4567 @end|ng |rom gm@||@com  Fri Feb 15 17:05:30 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 15 Feb 2019 08:05:30 -0800
Subject: [R] Package updates fail: how to fix the causes
In-Reply-To: <alpine.LNX.2.20.1902150747380.31662@salmo.appl-ecosys.com>
References: <alpine.LNX.2.20.1902150747380.31662@salmo.appl-ecosys.com>
Message-ID: <CAGxFJbRTfKrM+p9m4kPGNuaO6MVCfpCOMWquWsq6y8AhS84YGw@mail.gmail.com>

You *might* do better posting this on r-sig-debian and/or r-sig-fedora,
especially as this is not a question about R programming per se, which
makes it off topic for this list, but more on topic for those lists.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Feb 15, 2019 at 7:58 AM Rich Shepard <rshepard at appl-ecosys.com>
wrote:

> Running R-3.5.2 on Slackware-14.2, using my script that updates installed
> packages found four that failed. My web searches did not find relevant
> hits,
> and only the last build failure is explained by the build failure of a
> specific dependency. The results displayed are:
>
> ERROR: dependency ?sf? is not available for package ?spdep?
> * removing ?/usr/lib/R/library/spdep?
>
> The downloaded source packages are in
>         ?/tmp/RtmpzEBBCY/downloaded_packages?
> Updating HTML index of packages in '.Library'
> Making 'packages.html' ... done
> Warning messages:
> 1: In install.packages(update[instlib == l, "Package"], l, contriburl =
> contriburl,  :
>    installation of package ?units? had non-zero exit status
> 2: In install.packages(update[instlib == l, "Package"], l, contriburl =
> contriburl,  :
>    installation of package ?later? had non-zero exit status
> 3: In install.packages(update[instlib == l, "Package"], l, contriburl =
> contriburl,  :
>    installation of package ?sf? had non-zero exit status
> 4: In install.packages(update[instlib == l, "Package"], l, contriburl =
> contriburl,  :
>    installation of package ?spdep? had non-zero exit status
>
> Starting at the top, trying to install 'units' resulted in identifying a
> missing library:
>
> Configuration failed because libudunits2.so was not found. Try installing:
>      * deb: libudunits2-dev (Debian, Ubuntu, ...)
>      * rpm: udunits2-devel (Fedora, EPEL, ...)
>      * brew: udunits (OSX)
>    If udunits2 is already installed in a non-standard location, use:
>      --configure-args='--with-udunits2-lib=/usr/local/lib'
>    if the library was not found, and/or:
>      --configure-args='--with-udunits2-include=/usr/include/udunits2'
>    if the header was not found, replacing paths with appropriate values.
>    You can alternatively set UDUNITS2_INCLUDE and UDUNITS2_LIBS manually.
>
> --------------------------------------------------------------------------------
> See `config.log' for more details
> ERROR: configuration failed for package ?units?
> * removing ?/usr/lib/R/library/units?
>
> The downloaded source packages are in
>         ?/tmp/RtmprbnasH/downloaded_packages?
> Updating HTML index of packages in '.Library'
> Making 'packages.html' ... done
> Warning message:
> In install.packages("units") :
>    installation of package ?units? had non-zero exit status
>
> Please advise me on how to proceed so these four packages are eventually
> updated or re-installed.
>
> TIA,
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From |@t@z@hn @end|ng |rom gm@||@com  Fri Feb 15 17:17:48 2019
From: |@t@z@hn @end|ng |rom gm@||@com (Ista Zahn)
Date: Fri, 15 Feb 2019 11:17:48 -0500
Subject: [R] Package updates fail: how to fix the causes
In-Reply-To: <alpine.LNX.2.20.1902150747380.31662@salmo.appl-ecosys.com>
References: <alpine.LNX.2.20.1902150747380.31662@salmo.appl-ecosys.com>
Message-ID: <CA+vqiLE-Ehjt1WgCapuWjj=Yq=tEfP5spQzExW0v2afKb1QK8Q@mail.gmail.com>

Hi Rich,

Install udunits. If you don't know how to do that in slackware go ask
on a slackware forum.

Best,
Ista

On Fri, Feb 15, 2019 at 10:58 AM Rich Shepard <rshepard at appl-ecosys.com> wrote:
>
> Running R-3.5.2 on Slackware-14.2, using my script that updates installed
> packages found four that failed. My web searches did not find relevant hits,
> and only the last build failure is explained by the build failure of a
> specific dependency. The results displayed are:
>
> ERROR: dependency ?sf? is not available for package ?spdep?
> * removing ?/usr/lib/R/library/spdep?
>
> The downloaded source packages are in
>         ?/tmp/RtmpzEBBCY/downloaded_packages?
> Updating HTML index of packages in '.Library'
> Making 'packages.html' ... done
> Warning messages:
> 1: In install.packages(update[instlib == l, "Package"], l, contriburl = contriburl,  :
>    installation of package ?units? had non-zero exit status
> 2: In install.packages(update[instlib == l, "Package"], l, contriburl = contriburl,  :
>    installation of package ?later? had non-zero exit status
> 3: In install.packages(update[instlib == l, "Package"], l, contriburl = contriburl,  :
>    installation of package ?sf? had non-zero exit status
> 4: In install.packages(update[instlib == l, "Package"], l, contriburl = contriburl,  :
>    installation of package ?spdep? had non-zero exit status
>
> Starting at the top, trying to install 'units' resulted in identifying a
> missing library:
>
> Configuration failed because libudunits2.so was not found. Try installing:
>      * deb: libudunits2-dev (Debian, Ubuntu, ...)
>      * rpm: udunits2-devel (Fedora, EPEL, ...)
>      * brew: udunits (OSX)
>    If udunits2 is already installed in a non-standard location, use:
>      --configure-args='--with-udunits2-lib=/usr/local/lib'
>    if the library was not found, and/or:
>      --configure-args='--with-udunits2-include=/usr/include/udunits2'
>    if the header was not found, replacing paths with appropriate values.
>    You can alternatively set UDUNITS2_INCLUDE and UDUNITS2_LIBS manually.
> --------------------------------------------------------------------------------
> See `config.log' for more details
> ERROR: configuration failed for package ?units?
> * removing ?/usr/lib/R/library/units?
>
> The downloaded source packages are in
>         ?/tmp/RtmprbnasH/downloaded_packages?
> Updating HTML index of packages in '.Library'
> Making 'packages.html' ... done
> Warning message:
> In install.packages("units") :
>    installation of package ?units? had non-zero exit status
>
> Please advise me on how to proceed so these four packages are eventually
> updated or re-installed.
>
> TIA,
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Fri Feb 15 17:31:25 2019
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Fri, 15 Feb 2019 08:31:25 -0800 (PST)
Subject: [R] Package updates fail: how to fix the causes
In-Reply-To: <CAGxFJbRTfKrM+p9m4kPGNuaO6MVCfpCOMWquWsq6y8AhS84YGw@mail.gmail.com>
References: <alpine.LNX.2.20.1902150747380.31662@salmo.appl-ecosys.com>
 <CAGxFJbRTfKrM+p9m4kPGNuaO6MVCfpCOMWquWsq6y8AhS84YGw@mail.gmail.com>
Message-ID: <alpine.LNX.2.20.1902150830140.31662@salmo.appl-ecosys.com>

On Fri, 15 Feb 2019, Bert Gunter wrote:

> You *might* do better posting this on r-sig-debian and/or r-sig-fedora,
> especially as this is not a question about R programming per se, which
> makes it off topic for this list, but more on topic for those lists.

Bert,

   Since I run only Slackware I'm not seeing how posting on a debian or
fedora mail list would help. Those two distributions were part of the error
messasge when 'units' didn't build.

Regards,

Rich


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Fri Feb 15 17:34:10 2019
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Fri, 15 Feb 2019 08:34:10 -0800 (PST)
Subject: [R] Package updates fail: how to fix the causes
In-Reply-To: <CA+vqiLE-Ehjt1WgCapuWjj=Yq=tEfP5spQzExW0v2afKb1QK8Q@mail.gmail.com>
References: <alpine.LNX.2.20.1902150747380.31662@salmo.appl-ecosys.com>
 <CA+vqiLE-Ehjt1WgCapuWjj=Yq=tEfP5spQzExW0v2afKb1QK8Q@mail.gmail.com>
Message-ID: <alpine.LNX.2.20.1902150832390.31662@salmo.appl-ecosys.com>

On Fri, 15 Feb 2019, Ista Zahn wrote:

> Install udunits. If you don't know how to do that in slackware go ask on a
> slackware forum.

Ista,

Interesting. This must be a new dependency because prior versions of units
didn't require it. However, SlackBuilds.org has that package so I'll install
it then work my way down the list of failed updates.

Thanks for the pointer,

Rich


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Fri Feb 15 17:42:05 2019
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Fri, 15 Feb 2019 08:42:05 -0800 (PST)
Subject: [R] Package updates fail: how to fix the causes [FIXED]
In-Reply-To: <CA+vqiLE-Ehjt1WgCapuWjj=Yq=tEfP5spQzExW0v2afKb1QK8Q@mail.gmail.com>
References: <alpine.LNX.2.20.1902150747380.31662@salmo.appl-ecosys.com>
 <CA+vqiLE-Ehjt1WgCapuWjj=Yq=tEfP5spQzExW0v2afKb1QK8Q@mail.gmail.com>
Message-ID: <alpine.LNX.2.20.1902150841000.31662@salmo.appl-ecosys.com>

On Fri, 15 Feb 2019, Ista Zahn wrote:

> Install udunits.

Ista,

Yep. Installing that package allowed the chain to build. Much appreciated.

Best regards,

Rich


From B|||@Po||ng @end|ng |rom ze||@@com  Fri Feb 15 18:57:50 2019
From: B|||@Po||ng @end|ng |rom ze||@@com (Bill Poling)
Date: Fri, 15 Feb 2019 17:57:50 +0000
Subject: [R] Help with Cluster Tutorial Error
Message-ID: <BN7PR02MB5073718F36D461037BB44F09EA600@BN7PR02MB5073.namprd02.prod.outlook.com>

Hi Jeff and David.

Yes, updating all my Pkgs has done the trick.

I will remember to try that first next time.

As always I appreciate your help.

Thank you.

WHP



From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
Sent: Friday, February 15, 2019 10:48 AM
To: r-help at r-project.org; David L Carlson <dcarlson at tamu.edu>; Bill Poling <Bill.Poling at zelis.com>; r-help (r-help at r-project.org) <r-help at r-project.org>
Subject: Re: [R] Help with Cluster Tutorial Error

Another possible issue could be some outdated packages... be sure to update all packages.

On February 15, 2019 7:05:44 AM PST, David L Carlson <mailto:dcarlson at tamu.edu> wrote:
>I'm not getting any error on that line in Windows 10. I did not try
>running anything past that line.
>
>Have you tried restarting R and clearing your environment?
>
>----------------------------------------
>David L Carlson
>Department of Anthropology
>Texas A&M University
>College Station, TX 77843-4352
>
>-----Original Message-----
>From: R-help <mailto:r-help-bounces at r-project.org> On Behalf Of Bill Poling
>Sent: Friday, February 15, 2019 7:37 AM
>To: r-help (mailto:r-help at r-project.org) <mailto:r-help at r-project.org>
>Subject: [R] Help with Cluster Tutorial Error
>
>sessionInfo()
>#R version 3.5.2 (2018-12-20)
>#Platform: x86_64-w64-mingw32/x64 (64-bit)
>#Running under: Windows >= 8 x64 (build 9200)
>
>Hello I am working through this tutorial
>https://www.r-bloggers.com/10-tips-for-choosing-the-optimal-number-of-clusters/
>And I run into an error almost immediately at the point below like
>this:
>
> mammals <- raw_mammals %>% select(-name) # set rownames
>#Error in select(., -name) : object 'p_links' not found
>
>I have googled the error " R object 'p_links' not found"
>https://stats.stackexchange.com/questions/113907/error-object-descr-not-found
>However, the links I have found seem to be specific to an object that
>has been declared by the Op, and in my case I do not know where this
>object is supposed to be coming from?
>
>I think this stems from the use of the dplyr Pkg?
>
>https://dplyr.tidyverse.org/reference/select.html
>
>Here is what I have so far:
>
>#Data Set
># I will be using a lesser known data set from the cluster package:
>all.mammals.milk.1956, one which I haven't looked at before.
>#
># This small dataset contains a list of 25 mammals and the constituents
>of their milk (water, protein, fat, lactose, ash percentages) from John
>Hartigan, Clustering Algorithms, Wiley, 1975.
>#
># First let's load the required packages. Some of these are already in
>my library, those that are not were installed
>
>library(tidyverse)
>library(magrittr)
>library(cluster)
>install.packages("cluster.datasets")
>install.packages("NbClust")
>install.packages("clValid")
>install.packages("ggfortify")
>install.packages("clustree")
>install.packages("ggiraphExtra")
>library(cowplot)
>library(cluster.datasets)
>library(NbClust)
>library(clValid)
>library(ggfortify)
>library(clustree)
>library(ggiraphExtra)
>library(dendextend)
>library(factoextra)
>library(FactoMineR)
>library(corrplot)
>library(GGally)
>library(knitr)
>library(kableExtra)
>
>
>data("all.mammals.milk.1956")
>raw_mammals <- all.mammals.milk.1956
>str(raw_mammals) #----------------------KNOW THY DATA
>
># 'data.frame':25 obs. of 6 variables:
># $ name : chr "Horse" "Orangutan" "Monkey" "Donkey" ...
># $ water : num 90.1 88.5 88.4 90.3 90.4 87.7 86.9 82.1 81.9 81.6 ...
># $ protein: num 2.6 1.4 2.2 1.7 0.6 3.5 4.8 5.9 7.4 10.1 ...
># $ fat : num 1 3.5 2.7 1.4 4.5 3.4 1.7 7.9 7.2 6.3 ...
># $ lactose: num 6.9 6 6.4 6.2 4.4 4.8 5.7 4.7 2.7 4.4 ...
># $ ash : num 0.35 0.24 0.18 0.4 0.1 0.71 0.9 0.78 0.85 0.75 ...
>
>#raw_mammals <- fread("Animal Milk Constituent
>Percentages.csv",header=TRUE, stringsAsFactors=TRUE)
>
># subset dataset
>mammals <- raw_mammals %>% select(-name) # set rownames
>#Error in select(., -name) : object 'p_links' not found <--The Error
>
>I hope this is enough information to help answer my question.
>
>Thank you
>
>WHP
>
>
>
>
>Confidentiality Notice This message is sent from Zelis.
>...{{dropped:9}}
>
>______________________________________________
>mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.

Confidentiality Notice This message is sent from Zelis. This transmission may contain information which is privileged and confidential and is intended for the personal and confidential use of the named recipient only. Such information may be protected by applicable State and Federal laws from this disclosure or unauthorized use. If the reader of this message is not the intended recipient, or the employee or agent responsible for delivering the message to the intended recipient, you are hereby notified that any disclosure, review, discussion, copying, or taking any action in reliance on the contents of this transmission is strictly prohibited. If you have received this transmission in error, please contact the sender immediately. Zelis, 2018.

From |hb @end|ng |rom k@u@edu  Fri Feb 15 16:06:46 2019
From: |hb @end|ng |rom k@u@edu (Isaac Barnhart)
Date: Fri, 15 Feb 2019 15:06:46 +0000
Subject: [R] Taking the Average of a subset of data
Message-ID: <BN6PR05MB3380994F75A26E8477089275AE600@BN6PR05MB3380.namprd05.prod.outlook.com>

Hello all, I have another question. I'm working with the following dataset:






plot    plant   leaf_number     sen_score       plot_lai        plant_lai       lai_score       leaf_num
104     5       1       90      104     1       82      1
104     5       2       90      104     1       167     2
104     5       3       95      104     1       248     3
104     5       4       100     104     1       343     4
104     6       1       95      104     1       377     5
104     6       2       85      104     1       372     6
104     6       3       90      104     1       335     7
104     6       4       90      104     1       221     8
105     5       1       90      104     1       162     9
105     5       2       95      104     2       145     1
105     5       3       100     104     2       235     2
105     5       4       100     104     2       310     3
105     6       1       70      104     2       393     4
105     6       2       80      104     2       455     5
105     6       3       90      104     2       472     6
105     6       4       80      104     2       445     7
106     5       1       100     104     2       330     8
106     5       2       90      104     2       292     9
106     5       3       100     105     1       64      1
106     5       4       100     105     1       139     2
106     5       10      0       105     1       211     3
106     6       1       100     105     1       296     4
106     6       2       30      105     1       348     5
106     6       3       100     105     1       392     6
106     6       4       40      105     1       405     7
108     5       1       100     105     1       379     8
108     5       2       100     105     1       278     9
108     5       3       100     105     2       64      1
108     5       4       100     105     2       209     2

(Note: 'plant' and 'leaf' column should be separated. '51' means plant 5, leaf 1).


This dataset shows two datasets: The left 4 columns are of one  measurement (leaf senescence), and the right 4 columns are of another (leaf area index). I have a large amount of plots, and several plants, more than what is listed.


I need to sort both datasets (senescence and leaf area index) so that each plot has the same number of leaves.


This is hard because sometimes plots in the 'senescence' dataset have more leaves, and sometimes plots in the 'leaf area index'. Is there a way to sort both datasets so that this requirement is met? Like I said, there is no way to tell which dataset has the plot with the minimum amount of leaves; it can be either one in any case.


Any help would be appreciated!


Isaac


	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Fri Feb 15 21:43:52 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 15 Feb 2019 12:43:52 -0800
Subject: [R] Taking the Average of a subset of data
In-Reply-To: <BN6PR05MB3380994F75A26E8477089275AE600@BN6PR05MB3380.namprd05.prod.outlook.com>
References: <BN6PR05MB3380994F75A26E8477089275AE600@BN6PR05MB3380.namprd05.prod.outlook.com>
Message-ID: <CAGxFJbSgHCC2zqWfi=F=48Jh_2YJHM-B=NFtVC7Brdb2CKjrDA@mail.gmail.com>

Read the posting guide, please, paying particular attention to how to
provide reproducible data, e.g. via ?dput. You are much more likely to get
useful help if you do what it recommends and provide data for people to
work with.

You also should provide code showing us what you tried. You appear not to
have done much homework of your own -- have you gone through some R
tutorials, for example? Which ones?

Cheers,
Bert





Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Feb 15, 2019 at 12:26 PM Isaac Barnhart <ihb at ksu.edu> wrote:

> Hello all, I have another question. I'm working with the following dataset:
>
>
>
>
>
>
> plot    plant   leaf_number     sen_score       plot_lai        plant_lai
>      lai_score       leaf_num
> 104     5       1       90      104     1       82      1
> 104     5       2       90      104     1       167     2
> 104     5       3       95      104     1       248     3
> 104     5       4       100     104     1       343     4
> 104     6       1       95      104     1       377     5
> 104     6       2       85      104     1       372     6
> 104     6       3       90      104     1       335     7
> 104     6       4       90      104     1       221     8
> 105     5       1       90      104     1       162     9
> 105     5       2       95      104     2       145     1
> 105     5       3       100     104     2       235     2
> 105     5       4       100     104     2       310     3
> 105     6       1       70      104     2       393     4
> 105     6       2       80      104     2       455     5
> 105     6       3       90      104     2       472     6
> 105     6       4       80      104     2       445     7
> 106     5       1       100     104     2       330     8
> 106     5       2       90      104     2       292     9
> 106     5       3       100     105     1       64      1
> 106     5       4       100     105     1       139     2
> 106     5       10      0       105     1       211     3
> 106     6       1       100     105     1       296     4
> 106     6       2       30      105     1       348     5
> 106     6       3       100     105     1       392     6
> 106     6       4       40      105     1       405     7
> 108     5       1       100     105     1       379     8
> 108     5       2       100     105     1       278     9
> 108     5       3       100     105     2       64      1
> 108     5       4       100     105     2       209     2
>
> (Note: 'plant' and 'leaf' column should be separated. '51' means plant 5,
> leaf 1).
>
>
> This dataset shows two datasets: The left 4 columns are of one
> measurement (leaf senescence), and the right 4 columns are of another (leaf
> area index). I have a large amount of plots, and several plants, more than
> what is listed.
>
>
> I need to sort both datasets (senescence and leaf area index) so that each
> plot has the same number of leaves.
>
>
> This is hard because sometimes plots in the 'senescence' dataset have more
> leaves, and sometimes plots in the 'leaf area index'. Is there a way to
> sort both datasets so that this requirement is met? Like I said, there is
> no way to tell which dataset has the plot with the minimum amount of
> leaves; it can be either one in any case.
>
>
> Any help would be appreciated!
>
>
> Isaac
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @t@@|nom @end|ng |rom @t@||@|ondonmet@@c@uk  Fri Feb 15 12:37:54 2019
From: @t@@|nom @end|ng |rom @t@||@|ondonmet@@c@uk (Dimitrios Stasinopoulos)
Date: Fri, 15 Feb 2019 11:37:54 +0000
Subject: [R] Saving and reloading function in a package
Message-ID: <01BD4885-79C7-427C-AFEE-0AEACB896C14@staff.londonmet.ac.uk>

 I would like to put a graphic background to a model diagnostic plot.
The background is created with plot()/lines() but it takes time. 
My solution was to save the plots as functions using splinefun(). 
Those saved function can be put in a .RData file using load()  or  .rds using saveRDS().

My question is how I can put those files  in a package and load them within a function of the package.
Any suggestion please?  


Prof Dimitrios Stasinopoulos
stasinom at staff.londonmet.ac.uk




-- 
London Metropolitan University is a limited company registered in England 
and Wales with registered number 974438 and VAT registered number GB 447 
2190 51. Our registered office is at 166-220 Holloway Road, London N7 8DB. 
London Metropolitan University is an exempt charity under the Charities Act 
2011. Its registration number with HMRC is X6880.


	[[alternative HTML version deleted]]


From j@b@y@t194 @end|ng |rom gm@||@com  Fri Feb 15 18:24:53 2019
From: j@b@y@t194 @end|ng |rom gm@||@com (javad bayat)
Date: Fri, 15 Feb 2019 20:54:53 +0330
Subject: [R] Problem with combining 2 data frame
Message-ID: <CANTxAmLBHMQCgXbZNLtxsGEq7eGsr+ZQSacMBg8A5K6KqKsehw@mail.gmail.com>

Dear R users;
I am trying to combine 2 dataframes with different rows, 26 and 6 rows. The
first column of both dataframe has something in common, and I want to
compare the first column of the df1 with first column of the df2 to see if
they are same or not. Then if they were same, the second column of the df1
fill by the value of the second column of df2.

df1 = data.frame(x1 = letters[1:26],x2 = NA)
df2 = data.frame(x1 = letters[10:15],x2 = c("1a","2a","3a","4a","5a","6a"))

f = function(x,y){
          for (i in 1:nrow(df1))
           ifelse(df1$x1 == df2$x1, df1$x2==df2$x2, "NA")}
f(df1,df2)
Error in Ops.factor(df1$x1, df2$x1) : level sets of factors are different

Is there anyone to help me to solve this problem?
Sincerely.










-- 
Best Regards
Javad Bayat
M.Sc. Environment Engineering
Alternative Mail: bayat194 at yahoo.com

	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Sat Feb 16 03:15:07 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Fri, 15 Feb 2019 21:15:07 -0500
Subject: [R] Saving and reloading function in a package
In-Reply-To: <01BD4885-79C7-427C-AFEE-0AEACB896C14@staff.londonmet.ac.uk>
References: <01BD4885-79C7-427C-AFEE-0AEACB896C14@staff.londonmet.ac.uk>
Message-ID: <6EF5DEFB-27B8-41AB-9D85-ACF8817BAEF4@comcast.net>

Isn't this much more on topic with the package development list?

-- 
David


> On Feb 15, 2019, at 6:37 AM, Dimitrios Stasinopoulos <stasinom at staff.londonmet.ac.uk> wrote:
> 
> I would like to put a graphic background to a model diagnostic plot.
> The background is created with plot()/lines() but it takes time. 
> My solution was to save the plots as functions using splinefun(). 
> Those saved function can be put in a .RData file using load()  or  .rds using saveRDS().
> 
> My question is how I can put those files  in a package and load them within a function of the package.
> Any suggestion please?  
> 
> 
> Prof Dimitrios Stasinopoulos
> stasinom at staff.londonmet.ac.uk
> 
> 
> 
> 
> -- 
> London Metropolitan University is a limited company registered in England 
> and Wales with registered number 974438 and VAT registered number GB 447 
> 2190 51. Our registered office is at 166-220 Holloway Road, London N7 8DB. 
> London Metropolitan University is an exempt charity under the Charities Act 
> 2011. Its registration number with HMRC is X6880.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Sat Feb 16 03:19:56 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 15 Feb 2019 18:19:56 -0800
Subject: [R] Saving and reloading function in a package
In-Reply-To: <01BD4885-79C7-427C-AFEE-0AEACB896C14@staff.londonmet.ac.uk>
References: <01BD4885-79C7-427C-AFEE-0AEACB896C14@staff.londonmet.ac.uk>
Message-ID: <CAGxFJbRqzD0JoGspw6Uq3ZQFsx78YDzZn-FfQDWOzOY4uypqag@mail.gmail.com>

Do what we all do and learn how to create packages by reading the "Writing
R Extensions" manual; or spend time with a tutorial that shows how various
packages or IDE's such as the RStudio IDE simplify the process. See also
?package.skeleton and other R functions that can be used to assist you.

-- Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Feb 15, 2019 at 6:10 PM Dimitrios Stasinopoulos <
stasinom at staff.londonmet.ac.uk> wrote:

>  I would like to put a graphic background to a model diagnostic plot.
> The background is created with plot()/lines() but it takes time.
> My solution was to save the plots as functions using splinefun().
> Those saved function can be put in a .RData file using load()  or  .rds
> using saveRDS().
>
> My question is how I can put those files  in a package and load them
> within a function of the package.
> Any suggestion please?
>
>
> Prof Dimitrios Stasinopoulos
> stasinom at staff.londonmet.ac.uk
>
>
>
>
> --
> London Metropolitan University is a limited company registered in England
> and Wales with registered number 974438 and VAT registered number GB 447
> 2190 51. Our registered office is at 166-220 Holloway Road, London N7 8DB.
> London Metropolitan University is an exempt charity under the Charities
> Act
> 2011. Its registration number with HMRC is X6880.
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From er|cjberger @end|ng |rom gm@||@com  Sat Feb 16 06:43:17 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Sat, 16 Feb 2019 07:43:17 +0200
Subject: [R] Problem with combining 2 data frame
In-Reply-To: <CANTxAmLBHMQCgXbZNLtxsGEq7eGsr+ZQSacMBg8A5K6KqKsehw@mail.gmail.com>
References: <CANTxAmLBHMQCgXbZNLtxsGEq7eGsr+ZQSacMBg8A5K6KqKsehw@mail.gmail.com>
Message-ID: <CAGgJW77HgJ_nT61YeeTC6Q5ieTftWfpd6X_S3GF2XqEqZ388cg@mail.gmail.com>

Hi Javad,
You have a number of problems with your code, such as:
1. you should set df1 and df2 without factors
2. you define a function f(x,y) but the body of the function never refers
to x and y

The following code does what I think you are looking for:

df1 = data.frame(x1 = letters[1:26],x2 = NA,stringsAsFactors = FALSE)
df2 = data.frame(x1 = letters[10:15],x2 = c("1a","2a","3a","4a","5a","6a"),
stringsAsFactors = FALSE)

aa <- sapply( 1:nrow(df2), function(i) { df1$x2[ df1$x1==df2$x1[i] ] <<-
df2$x2[i] } )

HTH,
Eric


On Sat, Feb 16, 2019 at 4:11 AM javad bayat <j.bayat194 at gmail.com> wrote:

> Dear R users;
> I am trying to combine 2 dataframes with different rows, 26 and 6 rows. The
> first column of both dataframe has something in common, and I want to
> compare the first column of the df1 with first column of the df2 to see if
> they are same or not. Then if they were same, the second column of the df1
> fill by the value of the second column of df2.
>
> df1 = data.frame(x1 = letters[1:26],x2 = NA)
> df2 = data.frame(x1 = letters[10:15],x2 = c("1a","2a","3a","4a","5a","6a"))
>
> f = function(x,y){
>           for (i in 1:nrow(df1))
>            ifelse(df1$x1 == df2$x1, df1$x2==df2$x2, "NA")}
> f(df1,df2)
> Error in Ops.factor(df1$x1, df2$x1) : level sets of factors are different
>
> Is there anyone to help me to solve this problem?
> Sincerely.
>
>
>
>
>
>
>
>
>
>
> --
> Best Regards
> Javad Bayat
> M.Sc. Environment Engineering
> Alternative Mail: bayat194 at yahoo.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @boue|m@k@r|m1962 @end|ng |rom gm@||@com  Sat Feb 16 16:05:19 2019
From: @boue|m@k@r|m1962 @end|ng |rom gm@||@com (AbouEl-Makarim Aboueissa)
Date: Sat, 16 Feb 2019 10:05:19 -0500
Subject: [R] Remove cases with -Inf from a data frame
Message-ID: <CAE9stmeV-LE4ZNDZs3bon1CpcRxO9gcDJ+yPLk-0=DqyuvP4Gg@mail.gmail.com>

Dear All: good morning


I have a log-transformed data frame with some *-Inf* data values.

*my question: *how to remove all rows with *-Inf* data value from that data
frame?


with many thanks
abou
______________________


*AbouEl-Makarim Aboueissa, PhD*

*Professor, Statistics and Data Science*
*Graduate Coordinator*

*Department of Mathematics and Statistics*
*University of Southern Maine*

	[[alternative HTML version deleted]]


From ||@t@ @end|ng |rom dewey@myzen@co@uk  Sat Feb 16 17:13:37 2019
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Sat, 16 Feb 2019 16:13:37 +0000
Subject: [R] Remove cases with -Inf from a data frame
In-Reply-To: <CAE9stmeV-LE4ZNDZs3bon1CpcRxO9gcDJ+yPLk-0=DqyuvP4Gg@mail.gmail.com>
References: <CAE9stmeV-LE4ZNDZs3bon1CpcRxO9gcDJ+yPLk-0=DqyuvP4Gg@mail.gmail.com>
Message-ID: <ebec4ef6-1fe2-2332-5dbe-8a68ccb0fb16@dewey.myzen.co.uk>

Dear Abou

Depends on exact details of your variables but

?is.finite

Gives you the basic tool.

On 16/02/2019 15:05, AbouEl-Makarim Aboueissa wrote:
> Dear All: good morning
> 
> 
> I have a log-transformed data frame with some *-Inf* data values.
> 
> *my question: *how to remove all rows with *-Inf* data value from that data
> frame?
> 
> 
> with many thanks
> abou
> ______________________
> 
> 
> *AbouEl-Makarim Aboueissa, PhD*
> 
> *Professor, Statistics and Data Science*
> *Graduate Coordinator*
> 
> *Department of Mathematics and Statistics*
> *University of Southern Maine*
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From tr@xp|@yer @end|ng |rom gm@||@com  Sat Feb 16 17:14:33 2019
From: tr@xp|@yer @end|ng |rom gm@||@com (=?UTF-8?Q?Martin_M=C3=B8ller_Skarbiniks_Pedersen?=)
Date: Sat, 16 Feb 2019 17:14:33 +0100
Subject: [R] Remove cases with -Inf from a data frame
In-Reply-To: <CAE9stmeV-LE4ZNDZs3bon1CpcRxO9gcDJ+yPLk-0=DqyuvP4Gg@mail.gmail.com>
References: <CAE9stmeV-LE4ZNDZs3bon1CpcRxO9gcDJ+yPLk-0=DqyuvP4Gg@mail.gmail.com>
Message-ID: <CAGAA5bfCAKYWM_h=mqxUYTHs3BfU5LfJreYnxzu_q3ByVHdBig@mail.gmail.com>

On Sat, 16 Feb 2019 at 16:07, AbouEl-Makarim Aboueissa <
abouelmakarim1962 at gmail.com> wrote:
>
> I have a log-transformed data frame with some *-Inf* data values.
>
> *my question: *how to remove all rows with *-Inf* data value from that
data
> frame?


Hi,
  Here is a solution which uses apply.

First a data-frame as input:

set.seed(1)
df <- data.frame(w = sample(c(-Inf,1:20), 10),
                 x = sample(c(-Inf,1:20), 10),
                 y = sample(c(-Inf,1:20), 10),
                 z = sample(c(-Inf,1:20), 10))

df <- df[-(unlist(apply(df, 2, function(x) which(x == -Inf)))),]

Regards
Martin

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sat Feb 16 17:25:06 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sat, 16 Feb 2019 08:25:06 -0800
Subject: [R] Remove cases with -Inf from a data frame
In-Reply-To: <CAE9stmeV-LE4ZNDZs3bon1CpcRxO9gcDJ+yPLk-0=DqyuvP4Gg@mail.gmail.com>
References: <CAE9stmeV-LE4ZNDZs3bon1CpcRxO9gcDJ+yPLk-0=DqyuvP4Gg@mail.gmail.com>
Message-ID: <CAGxFJbSHU_LGUWcW=jf1DW0RLGz+nBV-0qQGsW-naeve50m2Mw@mail.gmail.com>

Many ways. I assume you know that Inf and -Inf are (special) numeric values
that can be treated like other numerics. i.e.

> 1 == - Inf
[1] FALSE

So straightforward indexing (selection) would do it.
But there is also ?is.infinite and ?is.finite, so

apply(yourdat, 1, function(x)all(is.finite(x)))

would produce the index vector to keep rows with only finite values
assuming yourdat contains only numeric data. If this is not the case, just
select the numeric columns to index on, i.e.

apply(yourdat[sapply(yourdat,is.numeric)], 1, function(x) all(is.finite(x)))

One possible problem here is handling of NA's:

is.finite(c(-Inf,NA))
[1] FALSE FALSE
... so rows containing NA's but no -Inf's would also get removed. If you
wish to keep rows with NA's but no -Inf's, then

function(x)(is.finite(x) | is.na(x) )

could be used.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Feb 16, 2019 at 7:07 AM AbouEl-Makarim Aboueissa <
abouelmakarim1962 at gmail.com> wrote:

> Dear All: good morning
>
>
> I have a log-transformed data frame with some *-Inf* data values.
>
> *my question: *how to remove all rows with *-Inf* data value from that data
> frame?
>
>
> with many thanks
> abou
> ______________________
>
>
> *AbouEl-Makarim Aboueissa, PhD*
>
> *Professor, Statistics and Data Science*
> *Graduate Coordinator*
>
> *Department of Mathematics and Statistics*
> *University of Southern Maine*
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sat Feb 16 17:29:19 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sat, 16 Feb 2019 08:29:19 -0800
Subject: [R] Remove cases with -Inf from a data frame
In-Reply-To: <CAGxFJbSHU_LGUWcW=jf1DW0RLGz+nBV-0qQGsW-naeve50m2Mw@mail.gmail.com>
References: <CAE9stmeV-LE4ZNDZs3bon1CpcRxO9gcDJ+yPLk-0=DqyuvP4Gg@mail.gmail.com>
 <CAGxFJbSHU_LGUWcW=jf1DW0RLGz+nBV-0qQGsW-naeve50m2Mw@mail.gmail.com>
Message-ID: <CAGxFJbQkhkiEw4m_Lv80VyM_mgBJYCKAtomOMZV2MsHuARj7kw@mail.gmail.com>

Sorry, that's

function(x)all(is.finite(x) | is.na(x) )

of course.


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Feb 16, 2019 at 8:25 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Many ways. I assume you know that Inf and -Inf are (special) numeric
> values that can be treated like other numerics. i.e.
>
> > 1 == - Inf
> [1] FALSE
>
> So straightforward indexing (selection) would do it.
> But there is also ?is.infinite and ?is.finite, so
>
> apply(yourdat, 1, function(x)all(is.finite(x)))
>
> would produce the index vector to keep rows with only finite values
> assuming yourdat contains only numeric data. If this is not the case, just
> select the numeric columns to index on, i.e.
>
> apply(yourdat[sapply(yourdat,is.numeric)], 1, function(x)
> all(is.finite(x)))
>
> One possible problem here is handling of NA's:
>
> is.finite(c(-Inf,NA))
> [1] FALSE FALSE
> ... so rows containing NA's but no -Inf's would also get removed. If you
> wish to keep rows with NA's but no -Inf's, then
>
> function(x)(is.finite(x) | is.na(x) )
>
> could be used.
>
> Cheers,
> Bert
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Sat, Feb 16, 2019 at 7:07 AM AbouEl-Makarim Aboueissa <
> abouelmakarim1962 at gmail.com> wrote:
>
>> Dear All: good morning
>>
>>
>> I have a log-transformed data frame with some *-Inf* data values.
>>
>> *my question: *how to remove all rows with *-Inf* data value from that
>> data
>> frame?
>>
>>
>> with many thanks
>> abou
>> ______________________
>>
>>
>> *AbouEl-Makarim Aboueissa, PhD*
>>
>> *Professor, Statistics and Data Science*
>> *Graduate Coordinator*
>>
>> *Department of Mathematics and Statistics*
>> *University of Southern Maine*
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat Feb 16 17:36:31 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sat, 16 Feb 2019 16:36:31 +0000
Subject: [R] Remove cases with -Inf from a data frame
In-Reply-To: <CAGAA5bfCAKYWM_h=mqxUYTHs3BfU5LfJreYnxzu_q3ByVHdBig@mail.gmail.com>
References: <CAE9stmeV-LE4ZNDZs3bon1CpcRxO9gcDJ+yPLk-0=DqyuvP4Gg@mail.gmail.com>
 <CAGAA5bfCAKYWM_h=mqxUYTHs3BfU5LfJreYnxzu_q3ByVHdBig@mail.gmail.com>
Message-ID: <35cf6934-f4ae-d3b5-3a6b-28446f3231a2@sapo.pt>

Hello,

An alternative, same dataset.

df[apply(df, 1, function(x) all(is.finite(x))), ]


Hope this helps,

Rui Barradas

?s 16:14 de 16/02/2019, Martin M?ller Skarbiniks Pedersen escreveu:
> On Sat, 16 Feb 2019 at 16:07, AbouEl-Makarim Aboueissa <
> abouelmakarim1962 at gmail.com> wrote:
>>
>> I have a log-transformed data frame with some *-Inf* data values.
>>
>> *my question: *how to remove all rows with *-Inf* data value from that
> data
>> frame?
> 
> 
> Hi,
>    Here is a solution which uses apply.
> 
> First a data-frame as input:
> 
> set.seed(1)
> df <- data.frame(w = sample(c(-Inf,1:20), 10),
>                   x = sample(c(-Inf,1:20), 10),
>                   y = sample(c(-Inf,1:20), 10),
>                   z = sample(c(-Inf,1:20), 10))
> 
> df <- df[-(unlist(apply(df, 2, function(x) which(x == -Inf)))),]
> 
> Regards
> Martin
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From g||ted|||e2014 @end|ng |rom gm@||@com  Sat Feb 16 22:08:38 2019
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Sat, 16 Feb 2019 22:08:38 +0100
Subject: [R] Extending my code
In-Reply-To: <CAC8ss32CM=vbWTWA-13qa-cCUY8ADK3Z7N-XYo-K8D6w-AoiCQ@mail.gmail.com>
References: <CAC8ss32JwM+VXDDkRNLi5frjxzhu-UsgQ+THbeTaSNG2BhpuJg@mail.gmail.com>
 <A4FD44B7-A286-4568-882F-19F350A74182@dcn.davis.ca.us>
 <CAC8ss31oh-QiyVLNxnBX+KHnnnDHZW0SUL4SQMDf9jN=6R1TyA@mail.gmail.com>
 <CAC8ss32CM=vbWTWA-13qa-cCUY8ADK3Z7N-XYo-K8D6w-AoiCQ@mail.gmail.com>
Message-ID: <CAC8ss32pkxBF+9VZ0wSu+FceB3cWm9YDg3gGoJXVHPDAc2cSog@mail.gmail.com>

Dear Jeff,
One more problem please.

When I used as.Date(ISOdate(dta$year, dta$month, dta$day,dta$hour)) to
handle date, I could use text(as.Date("2005-03-13"),-9,"b") to label
my plot.

Now that I am using as.POSIXct(ISOdatetime(year,
month,day,hour,0,0))), can you please tell me how to text "b" on the
point corresponding with 1960-05-04 09:00:00 on my plot.

Many thanks for your extra time.

Best wishes
Ogbos

On Fri, Feb 15, 2019 at 8:25 AM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>
> Dear Jeff,
>
> Please hold.
> It is begging to work. There was an error somewhere. One ")" is
> missing and as I went back to check the lines one by one with cursor,
> I stubbed on non matching bracket.
>
> I completed, run the code again and got some result.
>
> Will get back to you once I am through.
>
> Thanks in a hurry.
> Best regards
> Ogbos
>
>
> On Fri, Feb 15, 2019 at 8:15 AM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
> >
> > Dear Jeff,
> >
> > Thank you so much.
> >
> > I ran the code but got an error message. I then try to  run them line by line.
> >
> > The problem is in:
> > dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year, month,day,hour,0,0)))
> > Error in with(dta, as.POSIXct(ISOdatetime(year, month, day, hour, 0, 0))) :
> >   object 'dta' not found
> >
> > Thanks for another time.
> > Best
> > Ogbos
> >
> > On Fri, Feb 15, 2019 at 7:51 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> > >
> > > The Date class is not designed to handle time... you need to use the ISOdatetime function and convert to POSIXct instead of Date. Just be sure to set your timezone to some appropriate value before you convert any times into datetime types.
> > >
> > > Sys.setenv( TZ="GMT" )
> > > # avoid using `data` as that is the name of a base R function
> > > dta <- read.table("CALG.txt", col.names = c("year", "month", "day", "hour", "counts"))
> > > dta$year <- with( dta, ifelse(year < 50, year + 2000, year + 1900)
> > > dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year, month,day,hour,0,0)))
> > >
> > > I don't see why you feel obliged to copy the timestamp out of the data frame into x, but that is your business.
> > >
> > > Appropriate timezone values can be reviewed with the OlsonNames() function.
> > >
> > >
> > > On February 14, 2019 10:29:58 PM PST, Ogbos Okike <giftedlife2014 at gmail.com> wrote:
> > > >Dear List,
> > > >I have a simple code with which I convert year, month, and day to a
> > > >date format.
> > > >My data looks like:
> > > >67 01 26    18464
> > > >67 01 26    18472
> > > >67 01 26    18408
> > > >67 01 26    18360
> > > >67 01 26    18328
> > > >67 01 26    18320
> > > >67 01 26    18296
> > > >
> > > >while my code is:
> > > >
> > > >
> > > >data <- read.table("CALG.txt", col.names = c("year", "month", "day",
> > > >"counts"))
> > > > new.century <- data$year < 50
> > > >data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)
> > > >data$date <- as.Date(ISOdate(data$year, data$month, data$day))
> > > >x = data$date
> > > > y = data$counts
> > > >
> > > >I now wish to extend this code to be able to include hour for another
> > > >data of the format:
> > > >05 01 01 00    4009
> > > >05 01 01 01    3969
> > > >05 01 01 02    3946
> > > >05 01 01 03    3975
> > > >05 01 01 04    3960
> > > >05 01 01 05    3974
> > > >05 01 01 06    3971
> > > >05 01 01 07    3970
> > > >That is, I now would like to include hour in:
> > > >data <- read.table("CALG.txt", col.names = c("year", "month", "day",
> > > >"counts")).
> > > >
> > > >I am aware there are many other way of conversion but I have a
> > > >specific interest here. This code is a preamble to a larger code and
> > > >changing it to another format other than what I have will not be
> > > >compatible with the general code. Or will rather be difficult for me
> > > >to get another format fit into my main code.
> > > >
> > > >So if you would be kind enough to assist me to run the read.table in
> > > >the format:
> > > >
> > > >data <- read.table("CALG.txt", col.names = c("year", "month",
> > > >"day","hour", "counts"))
> > > >
> > > >and then run the rest as:
> > > >
> > > >new.century <- data$year < 50
> > > >data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)
> > > >data$date <- as.Date(ISOdate(data$year, data$month,
> > > >data$day,data$hour))
> > > >x = data$date
> > > >
> > > >such that year, month, day and hour will be stored in x,
> > > >
> > > >I will be very thankful.
> > > >
> > > >Thank you so much for your kind assistance.
> > > >Best regards
> > > >Ogbos
> > > >
> > > >______________________________________________
> > > >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > >https://stat.ethz.ch/mailman/listinfo/r-help
> > > >PLEASE do read the posting guide
> > > >http://www.R-project.org/posting-guide.html
> > > >and provide commented, minimal, self-contained, reproducible code.
> > >
> > > --
> > > Sent from my phone. Please excuse my brevity.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sat Feb 16 22:25:01 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 16 Feb 2019 13:25:01 -0800
Subject: [R] Extending my code
In-Reply-To: <CAC8ss32pkxBF+9VZ0wSu+FceB3cWm9YDg3gGoJXVHPDAc2cSog@mail.gmail.com>
References: <CAC8ss32JwM+VXDDkRNLi5frjxzhu-UsgQ+THbeTaSNG2BhpuJg@mail.gmail.com>
 <A4FD44B7-A286-4568-882F-19F350A74182@dcn.davis.ca.us>
 <CAC8ss31oh-QiyVLNxnBX+KHnnnDHZW0SUL4SQMDf9jN=6R1TyA@mail.gmail.com>
 <CAC8ss32CM=vbWTWA-13qa-cCUY8ADK3Z7N-XYo-K8D6w-AoiCQ@mail.gmail.com>
 <CAC8ss32pkxBF+9VZ0wSu+FceB3cWm9YDg3gGoJXVHPDAc2cSog@mail.gmail.com>
Message-ID: <F31D629D-4464-4C2F-9C6C-F7AA0C0E7EF7@dcn.davis.ca.us>

I have no idea how text(as.Date("2005-03-13"),-9,"b") would mark your plot anywhere near 1960-05-04 09:00:00. Perhaps someone else does. Or perhaps you can provide an actual minimal working example of what you had working before you changed to POSIXct.

On February 16, 2019 1:08:38 PM PST, Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>Dear Jeff,
>One more problem please.
>
>When I used as.Date(ISOdate(dta$year, dta$month, dta$day,dta$hour)) to
>handle date, I could use text(as.Date("2005-03-13"),-9,"b") to label
>my plot.
>
>Now that I am using as.POSIXct(ISOdatetime(year,
>month,day,hour,0,0))), can you please tell me how to text "b" on the
>point corresponding with 1960-05-04 09:00:00 on my plot.
>
>Many thanks for your extra time.
>
>Best wishes
>Ogbos
>
>On Fri, Feb 15, 2019 at 8:25 AM Ogbos Okike <giftedlife2014 at gmail.com>
>wrote:
>>
>> Dear Jeff,
>>
>> Please hold.
>> It is begging to work. There was an error somewhere. One ")" is
>> missing and as I went back to check the lines one by one with cursor,
>> I stubbed on non matching bracket.
>>
>> I completed, run the code again and got some result.
>>
>> Will get back to you once I am through.
>>
>> Thanks in a hurry.
>> Best regards
>> Ogbos
>>
>>
>> On Fri, Feb 15, 2019 at 8:15 AM Ogbos Okike
><giftedlife2014 at gmail.com> wrote:
>> >
>> > Dear Jeff,
>> >
>> > Thank you so much.
>> >
>> > I ran the code but got an error message. I then try to  run them
>line by line.
>> >
>> > The problem is in:
>> > dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year,
>month,day,hour,0,0)))
>> > Error in with(dta, as.POSIXct(ISOdatetime(year, month, day, hour,
>0, 0))) :
>> >   object 'dta' not found
>> >
>> > Thanks for another time.
>> > Best
>> > Ogbos
>> >
>> > On Fri, Feb 15, 2019 at 7:51 AM Jeff Newmiller
><jdnewmil at dcn.davis.ca.us> wrote:
>> > >
>> > > The Date class is not designed to handle time... you need to use
>the ISOdatetime function and convert to POSIXct instead of Date. Just
>be sure to set your timezone to some appropriate value before you
>convert any times into datetime types.
>> > >
>> > > Sys.setenv( TZ="GMT" )
>> > > # avoid using `data` as that is the name of a base R function
>> > > dta <- read.table("CALG.txt", col.names = c("year", "month",
>"day", "hour", "counts"))
>> > > dta$year <- with( dta, ifelse(year < 50, year + 2000, year +
>1900)
>> > > dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year,
>month,day,hour,0,0)))
>> > >
>> > > I don't see why you feel obliged to copy the timestamp out of the
>data frame into x, but that is your business.
>> > >
>> > > Appropriate timezone values can be reviewed with the OlsonNames()
>function.
>> > >
>> > >
>> > > On February 14, 2019 10:29:58 PM PST, Ogbos Okike
><giftedlife2014 at gmail.com> wrote:
>> > > >Dear List,
>> > > >I have a simple code with which I convert year, month, and day
>to a
>> > > >date format.
>> > > >My data looks like:
>> > > >67 01 26    18464
>> > > >67 01 26    18472
>> > > >67 01 26    18408
>> > > >67 01 26    18360
>> > > >67 01 26    18328
>> > > >67 01 26    18320
>> > > >67 01 26    18296
>> > > >
>> > > >while my code is:
>> > > >
>> > > >
>> > > >data <- read.table("CALG.txt", col.names = c("year", "month",
>"day",
>> > > >"counts"))
>> > > > new.century <- data$year < 50
>> > > >data$year <- ifelse(new.century, data$year + 2000, data$year +
>1900)
>> > > >data$date <- as.Date(ISOdate(data$year, data$month, data$day))
>> > > >x = data$date
>> > > > y = data$counts
>> > > >
>> > > >I now wish to extend this code to be able to include hour for
>another
>> > > >data of the format:
>> > > >05 01 01 00    4009
>> > > >05 01 01 01    3969
>> > > >05 01 01 02    3946
>> > > >05 01 01 03    3975
>> > > >05 01 01 04    3960
>> > > >05 01 01 05    3974
>> > > >05 01 01 06    3971
>> > > >05 01 01 07    3970
>> > > >That is, I now would like to include hour in:
>> > > >data <- read.table("CALG.txt", col.names = c("year", "month",
>"day",
>> > > >"counts")).
>> > > >
>> > > >I am aware there are many other way of conversion but I have a
>> > > >specific interest here. This code is a preamble to a larger code
>and
>> > > >changing it to another format other than what I have will not be
>> > > >compatible with the general code. Or will rather be difficult
>for me
>> > > >to get another format fit into my main code.
>> > > >
>> > > >So if you would be kind enough to assist me to run the
>read.table in
>> > > >the format:
>> > > >
>> > > >data <- read.table("CALG.txt", col.names = c("year", "month",
>> > > >"day","hour", "counts"))
>> > > >
>> > > >and then run the rest as:
>> > > >
>> > > >new.century <- data$year < 50
>> > > >data$year <- ifelse(new.century, data$year + 2000, data$year +
>1900)
>> > > >data$date <- as.Date(ISOdate(data$year, data$month,
>> > > >data$day,data$hour))
>> > > >x = data$date
>> > > >
>> > > >such that year, month, day and hour will be stored in x,
>> > > >
>> > > >I will be very thankful.
>> > > >
>> > > >Thank you so much for your kind assistance.
>> > > >Best regards
>> > > >Ogbos
>> > > >
>> > > >______________________________________________
>> > > >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>see
>> > > >https://stat.ethz.ch/mailman/listinfo/r-help
>> > > >PLEASE do read the posting guide
>> > > >http://www.R-project.org/posting-guide.html
>> > > >and provide commented, minimal, self-contained, reproducible
>code.
>> > >
>> > > --
>> > > Sent from my phone. Please excuse my brevity.

-- 
Sent from my phone. Please excuse my brevity.


From dc@r|@on @end|ng |rom t@mu@edu  Sun Feb 17 01:49:21 2019
From: dc@r|@on @end|ng |rom t@mu@edu (David L Carlson)
Date: Sun, 17 Feb 2019 00:49:21 +0000
Subject: [R] Problem with combining 2 data frame
In-Reply-To: <CAGgJW77HgJ_nT61YeeTC6Q5ieTftWfpd6X_S3GF2XqEqZ388cg@mail.gmail.com>
References: <CANTxAmLBHMQCgXbZNLtxsGEq7eGsr+ZQSacMBg8A5K6KqKsehw@mail.gmail.com>
 <CAGgJW77HgJ_nT61YeeTC6Q5ieTftWfpd6X_S3GF2XqEqZ388cg@mail.gmail.com>
Message-ID: <0ad8385b0403402e93f8d6d0f8485d05@tamu.edu>

This is another approach using match():

df1 = data.frame(x1 = letters[1:26],x2 = NA, stringsAsFactors=FALSE)
df2 = data.frame(x1 = letters[10:15],x2 = c("1a","2a","3a","4a","5a","6a"),
     stringsAsFactors =FALSE)

df1$x2[match(df2$x1, df1$x1)] <- df2$x2

---------------------------------------------
David L. Carlson
Department of Anthropology
Texas A&M University

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Eric Berger
Sent: Friday, February 15, 2019 11:43 PM
To: javad bayat <j.bayat194 at gmail.com>
Cc: R mailing list <R-help at r-project.org>; r-help-owner at r-project.org
Subject: Re: [R] Problem with combining 2 data frame

Hi Javad,
You have a number of problems with your code, such as:
1. you should set df1 and df2 without factors
2. you define a function f(x,y) but the body of the function never refers
to x and y

The following code does what I think you are looking for:

df1 = data.frame(x1 = letters[1:26],x2 = NA,stringsAsFactors = FALSE)
df2 = data.frame(x1 = letters[10:15],x2 = c("1a","2a","3a","4a","5a","6a"),
stringsAsFactors = FALSE)

aa <- sapply( 1:nrow(df2), function(i) { df1$x2[ df1$x1==df2$x1[i] ] <<-
df2$x2[i] } )

HTH,
Eric


On Sat, Feb 16, 2019 at 4:11 AM javad bayat <j.bayat194 at gmail.com> wrote:

> Dear R users;
> I am trying to combine 2 dataframes with different rows, 26 and 6 rows. The
> first column of both dataframe has something in common, and I want to
> compare the first column of the df1 with first column of the df2 to see if
> they are same or not. Then if they were same, the second column of the df1
> fill by the value of the second column of df2.
>
> df1 = data.frame(x1 = letters[1:26],x2 = NA)
> df2 = data.frame(x1 = letters[10:15],x2 = c("1a","2a","3a","4a","5a","6a"))
>
> f = function(x,y){
>           for (i in 1:nrow(df1))
>            ifelse(df1$x1 == df2$x1, df1$x2==df2$x2, "NA")}
> f(df1,df2)
> Error in Ops.factor(df1$x1, df2$x1) : level sets of factors are different
>
> Is there anyone to help me to solve this problem?
> Sincerely.
>
>
>
>
>
>
>
>
>
>
> --
> Best Regards
> Javad Bayat
> M.Sc. Environment Engineering
> Alternative Mail: bayat194 at yahoo.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From drj|m|emon @end|ng |rom gm@||@com  Sun Feb 17 02:21:50 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sun, 17 Feb 2019 12:21:50 +1100
Subject: [R] Extending my code
In-Reply-To: <F31D629D-4464-4C2F-9C6C-F7AA0C0E7EF7@dcn.davis.ca.us>
References: <CAC8ss32JwM+VXDDkRNLi5frjxzhu-UsgQ+THbeTaSNG2BhpuJg@mail.gmail.com>
 <A4FD44B7-A286-4568-882F-19F350A74182@dcn.davis.ca.us>
 <CAC8ss31oh-QiyVLNxnBX+KHnnnDHZW0SUL4SQMDf9jN=6R1TyA@mail.gmail.com>
 <CAC8ss32CM=vbWTWA-13qa-cCUY8ADK3Z7N-XYo-K8D6w-AoiCQ@mail.gmail.com>
 <CAC8ss32pkxBF+9VZ0wSu+FceB3cWm9YDg3gGoJXVHPDAc2cSog@mail.gmail.com>
 <F31D629D-4464-4C2F-9C6C-F7AA0C0E7EF7@dcn.davis.ca.us>
Message-ID: <CA+8X3fUcMN=zasRWSewwuz3=Mz2DOWFxz_L34WbWs_o0rTZE3w@mail.gmail.com>

Hi Ogbos,
It may be easier to use strptime:

dta<-data.frame(year=rep(2005,5),month=rep("05",5),
 day=c("01","06","11","16","21"),
 hour=c(2,4,6,8,10),minute=rep(0,5),second=rep(0,5),value=1:5)
dta$Ptime<-strptime(paste(paste(dta$year,dta$month,dta$day,sep="-"),
 paste(dta$hour,dta$minute,dta$second,sep=":")),"%Y-%m-%d %H:%M:%S")
plot(dta$Ptime,dta$value)
text(strptime("2005-05-04 09:00:00","%Y-%m-%d %H:%M:%S"),4,"b")

Jim

> On February 16, 2019 1:08:38 PM PST, Ogbos Okike <giftedlife2014 at gmail.com> wrote:
> >Dear Jeff,
> >One more problem please.
> >
> >When I used as.Date(ISOdate(dta$year, dta$month, dta$day,dta$hour)) to
> >handle date, I could use text(as.Date("2005-03-13"),-9,"b") to label
> >my plot.
> >
> >Now that I am using as.POSIXct(ISOdatetime(year,
> >month,day,hour,0,0))), can you please tell me how to text "b" on the
> >point corresponding with 1960-05-04 09:00:00 on my plot.
> >
> >Many thanks for your extra time.
> >
> >Best wishes
> >Ogbos


From g||ted|||e2014 @end|ng |rom gm@||@com  Sun Feb 17 02:41:29 2019
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Sun, 17 Feb 2019 02:41:29 +0100
Subject: [R] Extending my code
In-Reply-To: <F31D629D-4464-4C2F-9C6C-F7AA0C0E7EF7@dcn.davis.ca.us>
References: <CAC8ss32JwM+VXDDkRNLi5frjxzhu-UsgQ+THbeTaSNG2BhpuJg@mail.gmail.com>
 <A4FD44B7-A286-4568-882F-19F350A74182@dcn.davis.ca.us>
 <CAC8ss31oh-QiyVLNxnBX+KHnnnDHZW0SUL4SQMDf9jN=6R1TyA@mail.gmail.com>
 <CAC8ss32CM=vbWTWA-13qa-cCUY8ADK3Z7N-XYo-K8D6w-AoiCQ@mail.gmail.com>
 <CAC8ss32pkxBF+9VZ0wSu+FceB3cWm9YDg3gGoJXVHPDAc2cSog@mail.gmail.com>
 <F31D629D-4464-4C2F-9C6C-F7AA0C0E7EF7@dcn.davis.ca.us>
Message-ID: <CAC8ss32h6x5Ey1iXW8pFyg=VdEJavDcqMHfwAzC_1JZsvWzOkg@mail.gmail.com>

Dear Jeff,
My error please and sorry about that.

 Not "1960-05-04 09:00:00".
I meant to write as.Date("2005-03-13"),-9,"b") and "2005-03-13 09:00:00".
My problem is the additional time factor.

I can text anywhere on my plot when dealing with yyyy/mm/dd but I can't
handle the new yyyy/mm/dd/hh/mm/ss.

Thank you for your patience.

Best regards
Ogbos.

I have no idea how text(as.Date("2005-03-13"),-9,"b") would mark your plot
> anywhere near 1960-05-04 09:00:00. Perhaps someone else does. Or perhaps
> you can provide an actual minimal working example of what you had working
> before you changed to POSIXct.
>
> On February 16, 2019 1:08:38 PM PST, Ogbos Okike <giftedlife2014 at gmail.com>
> wrote:
> >Dear Jeff,
> >One more problem please.
> >
> >When I used as.Date(ISOdate(dta$year, dta$month, dta$day,dta$hour)) to
> >handle date, I could use text(as.Date("2005-03-13"),-9,"b") to label
> >my plot.
> >
> >Now that I am using as.POSIXct(ISOdatetime(year,
> >month,day,hour,0,0))), can you please tell me how to text "b" on the
> >point corresponding with 1960-05-04 09:00:00 on my plot.
> >
> >Many thanks for your extra time.
> >
> >Best wishes
> >Ogbos
> >
> >On Fri, Feb 15, 2019 at 8:25 AM Ogbos Okike <giftedlife2014 at gmail.com>
> >wrote:
> >>
> >> Dear Jeff,
> >>
> >> Please hold.
> >> It is begging to work. There was an error somewhere. One ")" is
> >> missing and as I went back to check the lines one by one with cursor,
> >> I stubbed on non matching bracket.
> >>
> >> I completed, run the code again and got some result.
> >>
> >> Will get back to you once I am through.
> >>
> >> Thanks in a hurry.
> >> Best regards
> >> Ogbos
> >>
> >>
> >> On Fri, Feb 15, 2019 at 8:15 AM Ogbos Okike
> ><giftedlife2014 at gmail.com> wrote:
> >> >
> >> > Dear Jeff,
> >> >
> >> > Thank you so much.
> >> >
> >> > I ran the code but got an error message. I then try to  run them
> >line by line.
> >> >
> >> > The problem is in:
> >> > dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year,
> >month,day,hour,0,0)))
> >> > Error in with(dta, as.POSIXct(ISOdatetime(year, month, day, hour,
> >0, 0))) :
> >> >   object 'dta' not found
> >> >
> >> > Thanks for another time.
> >> > Best
> >> > Ogbos
> >> >
> >> > On Fri, Feb 15, 2019 at 7:51 AM Jeff Newmiller
> ><jdnewmil at dcn.davis.ca.us> wrote:
> >> > >
> >> > > The Date class is not designed to handle time... you need to use
> >the ISOdatetime function and convert to POSIXct instead of Date. Just
> >be sure to set your timezone to some appropriate value before you
> >convert any times into datetime types.
> >> > >
> >> > > Sys.setenv( TZ="GMT" )
> >> > > # avoid using `data` as that is the name of a base R function
> >> > > dta <- read.table("CALG.txt", col.names = c("year", "month",
> >"day", "hour", "counts"))
> >> > > dta$year <- with( dta, ifelse(year < 50, year + 2000, year +
> >1900)
> >> > > dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year,
> >month,day,hour,0,0)))
> >> > >
> >> > > I don't see why you feel obliged to copy the timestamp out of the
> >data frame into x, but that is your business.
> >> > >
> >> > > Appropriate timezone values can be reviewed with the OlsonNames()
> >function.
> >> > >
> >> > >
> >> > > On February 14, 2019 10:29:58 PM PST, Ogbos Okike
> ><giftedlife2014 at gmail.com> wrote:
> >> > > >Dear List,
> >> > > >I have a simple code with which I convert year, month, and day
> >to a
> >> > > >date format.
> >> > > >My data looks like:
> >> > > >67 01 26    18464
> >> > > >67 01 26    18472
> >> > > >67 01 26    18408
> >> > > >67 01 26    18360
> >> > > >67 01 26    18328
> >> > > >67 01 26    18320
> >> > > >67 01 26    18296
> >> > > >
> >> > > >while my code is:
> >> > > >
> >> > > >
> >> > > >data <- read.table("CALG.txt", col.names = c("year", "month",
> >"day",
> >> > > >"counts"))
> >> > > > new.century <- data$year < 50
> >> > > >data$year <- ifelse(new.century, data$year + 2000, data$year +
> >1900)
> >> > > >data$date <- as.Date(ISOdate(data$year, data$month, data$day))
> >> > > >x = data$date
> >> > > > y = data$counts
> >> > > >
> >> > > >I now wish to extend this code to be able to include hour for
> >another
> >> > > >data of the format:
> >> > > >05 01 01 00    4009
> >> > > >05 01 01 01    3969
> >> > > >05 01 01 02    3946
> >> > > >05 01 01 03    3975
> >> > > >05 01 01 04    3960
> >> > > >05 01 01 05    3974
> >> > > >05 01 01 06    3971
> >> > > >05 01 01 07    3970
> >> > > >That is, I now would like to include hour in:
> >> > > >data <- read.table("CALG.txt", col.names = c("year", "month",
> >"day",
> >> > > >"counts")).
> >> > > >
> >> > > >I am aware there are many other way of conversion but I have a
> >> > > >specific interest here. This code is a preamble to a larger code
> >and
> >> > > >changing it to another format other than what I have will not be
> >> > > >compatible with the general code. Or will rather be difficult
> >for me
> >> > > >to get another format fit into my main code.
> >> > > >
> >> > > >So if you would be kind enough to assist me to run the
> >read.table in
> >> > > >the format:
> >> > > >
> >> > > >data <- read.table("CALG.txt", col.names = c("year", "month",
> >> > > >"day","hour", "counts"))
> >> > > >
> >> > > >and then run the rest as:
> >> > > >
> >> > > >new.century <- data$year < 50
> >> > > >data$year <- ifelse(new.century, data$year + 2000, data$year +
> >1900)
> >> > > >data$date <- as.Date(ISOdate(data$year, data$month,
> >> > > >data$day,data$hour))
> >> > > >x = data$date
> >> > > >
> >> > > >such that year, month, day and hour will be stored in x,
> >> > > >
> >> > > >I will be very thankful.
> >> > > >
> >> > > >Thank you so much for your kind assistance.
> >> > > >Best regards
> >> > > >Ogbos
> >> > > >
> >> > > >______________________________________________
> >> > > >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >see
> >> > > >https://stat.ethz.ch/mailman/listinfo/r-help
> >> > > >PLEASE do read the posting guide
> >> > > >http://www.R-project.org/posting-guide.html
> >> > > >and provide commented, minimal, self-contained, reproducible
> >code.
> >> > >
> >> > > --
> >> > > Sent from my phone. Please excuse my brevity.
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sun Feb 17 02:59:13 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 16 Feb 2019 17:59:13 -0800
Subject: [R] Extending my code
In-Reply-To: <CAC8ss32h6x5Ey1iXW8pFyg=VdEJavDcqMHfwAzC_1JZsvWzOkg@mail.gmail.com>
References: <CAC8ss32JwM+VXDDkRNLi5frjxzhu-UsgQ+THbeTaSNG2BhpuJg@mail.gmail.com>
 <A4FD44B7-A286-4568-882F-19F350A74182@dcn.davis.ca.us>
 <CAC8ss31oh-QiyVLNxnBX+KHnnnDHZW0SUL4SQMDf9jN=6R1TyA@mail.gmail.com>
 <CAC8ss32CM=vbWTWA-13qa-cCUY8ADK3Z7N-XYo-K8D6w-AoiCQ@mail.gmail.com>
 <CAC8ss32pkxBF+9VZ0wSu+FceB3cWm9YDg3gGoJXVHPDAc2cSog@mail.gmail.com>
 <F31D629D-4464-4C2F-9C6C-F7AA0C0E7EF7@dcn.davis.ca.us>
 <CAC8ss32h6x5Ey1iXW8pFyg=VdEJavDcqMHfwAzC_1JZsvWzOkg@mail.gmail.com>
Message-ID: <857CA90F-EB1D-4621-AB67-E1028638778A@dcn.davis.ca.us>

Jim has pointed out the strptime function, but you can use as.POSIXct function also... so 

as.POSIXct( "2005-03-13 09:00:00" )
as.POSIXct( "03/13/2005 9:00", format="%m/%d/%Y %H:%M" )

or other variations should work.

On February 16, 2019 5:41:29 PM PST, Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>Dear Jeff,
>My error please and sorry about that.
>
> Not "1960-05-04 09:00:00".
>I meant to write as.Date("2005-03-13"),-9,"b") and "2005-03-13
>09:00:00".
>My problem is the additional time factor.
>
>I can text anywhere on my plot when dealing with yyyy/mm/dd but I can't
>handle the new yyyy/mm/dd/hh/mm/ss.
>
>Thank you for your patience.
>
>Best regards
>Ogbos.
>
>I have no idea how text(as.Date("2005-03-13"),-9,"b") would mark your
>plot
>> anywhere near 1960-05-04 09:00:00. Perhaps someone else does. Or
>perhaps
>> you can provide an actual minimal working example of what you had
>working
>> before you changed to POSIXct.
>>
>> On February 16, 2019 1:08:38 PM PST, Ogbos Okike
><giftedlife2014 at gmail.com>
>> wrote:
>> >Dear Jeff,
>> >One more problem please.
>> >
>> >When I used as.Date(ISOdate(dta$year, dta$month, dta$day,dta$hour))
>to
>> >handle date, I could use text(as.Date("2005-03-13"),-9,"b") to label
>> >my plot.
>> >
>> >Now that I am using as.POSIXct(ISOdatetime(year,
>> >month,day,hour,0,0))), can you please tell me how to text "b" on the
>> >point corresponding with 1960-05-04 09:00:00 on my plot.
>> >
>> >Many thanks for your extra time.
>> >
>> >Best wishes
>> >Ogbos
>> >
>> >On Fri, Feb 15, 2019 at 8:25 AM Ogbos Okike
><giftedlife2014 at gmail.com>
>> >wrote:
>> >>
>> >> Dear Jeff,
>> >>
>> >> Please hold.
>> >> It is begging to work. There was an error somewhere. One ")" is
>> >> missing and as I went back to check the lines one by one with
>cursor,
>> >> I stubbed on non matching bracket.
>> >>
>> >> I completed, run the code again and got some result.
>> >>
>> >> Will get back to you once I am through.
>> >>
>> >> Thanks in a hurry.
>> >> Best regards
>> >> Ogbos
>> >>
>> >>
>> >> On Fri, Feb 15, 2019 at 8:15 AM Ogbos Okike
>> ><giftedlife2014 at gmail.com> wrote:
>> >> >
>> >> > Dear Jeff,
>> >> >
>> >> > Thank you so much.
>> >> >
>> >> > I ran the code but got an error message. I then try to  run them
>> >line by line.
>> >> >
>> >> > The problem is in:
>> >> > dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year,
>> >month,day,hour,0,0)))
>> >> > Error in with(dta, as.POSIXct(ISOdatetime(year, month, day,
>hour,
>> >0, 0))) :
>> >> >   object 'dta' not found
>> >> >
>> >> > Thanks for another time.
>> >> > Best
>> >> > Ogbos
>> >> >
>> >> > On Fri, Feb 15, 2019 at 7:51 AM Jeff Newmiller
>> ><jdnewmil at dcn.davis.ca.us> wrote:
>> >> > >
>> >> > > The Date class is not designed to handle time... you need to
>use
>> >the ISOdatetime function and convert to POSIXct instead of Date.
>Just
>> >be sure to set your timezone to some appropriate value before you
>> >convert any times into datetime types.
>> >> > >
>> >> > > Sys.setenv( TZ="GMT" )
>> >> > > # avoid using `data` as that is the name of a base R function
>> >> > > dta <- read.table("CALG.txt", col.names = c("year", "month",
>> >"day", "hour", "counts"))
>> >> > > dta$year <- with( dta, ifelse(year < 50, year + 2000, year +
>> >1900)
>> >> > > dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year,
>> >month,day,hour,0,0)))
>> >> > >
>> >> > > I don't see why you feel obliged to copy the timestamp out of
>the
>> >data frame into x, but that is your business.
>> >> > >
>> >> > > Appropriate timezone values can be reviewed with the
>OlsonNames()
>> >function.
>> >> > >
>> >> > >
>> >> > > On February 14, 2019 10:29:58 PM PST, Ogbos Okike
>> ><giftedlife2014 at gmail.com> wrote:
>> >> > > >Dear List,
>> >> > > >I have a simple code with which I convert year, month, and
>day
>> >to a
>> >> > > >date format.
>> >> > > >My data looks like:
>> >> > > >67 01 26    18464
>> >> > > >67 01 26    18472
>> >> > > >67 01 26    18408
>> >> > > >67 01 26    18360
>> >> > > >67 01 26    18328
>> >> > > >67 01 26    18320
>> >> > > >67 01 26    18296
>> >> > > >
>> >> > > >while my code is:
>> >> > > >
>> >> > > >
>> >> > > >data <- read.table("CALG.txt", col.names = c("year", "month",
>> >"day",
>> >> > > >"counts"))
>> >> > > > new.century <- data$year < 50
>> >> > > >data$year <- ifelse(new.century, data$year + 2000, data$year
>+
>> >1900)
>> >> > > >data$date <- as.Date(ISOdate(data$year, data$month,
>data$day))
>> >> > > >x = data$date
>> >> > > > y = data$counts
>> >> > > >
>> >> > > >I now wish to extend this code to be able to include hour for
>> >another
>> >> > > >data of the format:
>> >> > > >05 01 01 00    4009
>> >> > > >05 01 01 01    3969
>> >> > > >05 01 01 02    3946
>> >> > > >05 01 01 03    3975
>> >> > > >05 01 01 04    3960
>> >> > > >05 01 01 05    3974
>> >> > > >05 01 01 06    3971
>> >> > > >05 01 01 07    3970
>> >> > > >That is, I now would like to include hour in:
>> >> > > >data <- read.table("CALG.txt", col.names = c("year", "month",
>> >"day",
>> >> > > >"counts")).
>> >> > > >
>> >> > > >I am aware there are many other way of conversion but I have
>a
>> >> > > >specific interest here. This code is a preamble to a larger
>code
>> >and
>> >> > > >changing it to another format other than what I have will not
>be
>> >> > > >compatible with the general code. Or will rather be difficult
>> >for me
>> >> > > >to get another format fit into my main code.
>> >> > > >
>> >> > > >So if you would be kind enough to assist me to run the
>> >read.table in
>> >> > > >the format:
>> >> > > >
>> >> > > >data <- read.table("CALG.txt", col.names = c("year", "month",
>> >> > > >"day","hour", "counts"))
>> >> > > >
>> >> > > >and then run the rest as:
>> >> > > >
>> >> > > >new.century <- data$year < 50
>> >> > > >data$year <- ifelse(new.century, data$year + 2000, data$year
>+
>> >1900)
>> >> > > >data$date <- as.Date(ISOdate(data$year, data$month,
>> >> > > >data$day,data$hour))
>> >> > > >x = data$date
>> >> > > >
>> >> > > >such that year, month, day and hour will be stored in x,
>> >> > > >
>> >> > > >I will be very thankful.
>> >> > > >
>> >> > > >Thank you so much for your kind assistance.
>> >> > > >Best regards
>> >> > > >Ogbos
>> >> > > >
>> >> > > >______________________________________________
>> >> > > >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>> >see
>> >> > > >https://stat.ethz.ch/mailman/listinfo/r-help
>> >> > > >PLEASE do read the posting guide
>> >> > > >http://www.R-project.org/posting-guide.html
>> >> > > >and provide commented, minimal, self-contained, reproducible
>> >code.
>> >> > >
>> >> > > --
>> >> > > Sent from my phone. Please excuse my brevity.
>>
>> --
>> Sent from my phone. Please excuse my brevity.
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From drj|m|emon @end|ng |rom gm@||@com  Sun Feb 17 03:04:52 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sun, 17 Feb 2019 13:04:52 +1100
Subject: [R] Extending my code
In-Reply-To: <CAC8ss32msSgP_=_9f=-J6ADHJ2emt6DXApaKZMFcEDwL0xFjrA@mail.gmail.com>
References: <CAC8ss32JwM+VXDDkRNLi5frjxzhu-UsgQ+THbeTaSNG2BhpuJg@mail.gmail.com>
 <A4FD44B7-A286-4568-882F-19F350A74182@dcn.davis.ca.us>
 <CAC8ss31oh-QiyVLNxnBX+KHnnnDHZW0SUL4SQMDf9jN=6R1TyA@mail.gmail.com>
 <CAC8ss32CM=vbWTWA-13qa-cCUY8ADK3Z7N-XYo-K8D6w-AoiCQ@mail.gmail.com>
 <CAC8ss32pkxBF+9VZ0wSu+FceB3cWm9YDg3gGoJXVHPDAc2cSog@mail.gmail.com>
 <F31D629D-4464-4C2F-9C6C-F7AA0C0E7EF7@dcn.davis.ca.us>
 <CA+8X3fUcMN=zasRWSewwuz3=Mz2DOWFxz_L34WbWs_o0rTZE3w@mail.gmail.com>
 <CAC8ss32msSgP_=_9f=-J6ADHJ2emt6DXApaKZMFcEDwL0xFjrA@mail.gmail.com>
Message-ID: <CA+8X3fW9vAjHTWSxGMtKTBd0Fu_ZwfpJSCdstDrNYbwxO4ZsBA@mail.gmail.com>

Hi Ogbos,
Remember that there is an underlying numeric representation of dates.
as.Date assigns dates a number of days from an agreed origin
(1970-01-01) while POSIX dates assign a number of seconds. If you plot
in one system and then try to add points in another, it's not going to
work.

Jim

On Sun, Feb 17, 2019 at 12:55 PM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>
> Dear Jim,
> Thank you and welcome back. It seems you have been away as you have not been responding to people's questions as before.
>
> I have just made a correction on my original question. Please have a look and check if I can still go the way u have suggested or if the correction would call for another approach.
> Best regards
> Ogbos
>
>
> On Sun, Feb 17, 2019, 02:22 Jim Lemon <drjimlemon at gmail.com> wrote:
>>
>> Hi Ogbos,
>> It may be easier to use strptime:
>>
>> dta<-data.frame(year=rep(2005,5),month=rep("05",5),
>>  day=c("01","06","11","16","21"),
>>  hour=c(2,4,6,8,10),minute=rep(0,5),second=rep(0,5),value=1:5)
>> dta$Ptime<-strptime(paste(paste(dta$year,dta$month,dta$day,sep="-"),
>>  paste(dta$hour,dta$minute,dta$second,sep=":")),"%Y-%m-%d %H:%M:%S")
>> plot(dta$Ptime,dta$value)
>> text(strptime("2005-05-04 09:00:00","%Y-%m-%d %H:%M:%S"),4,"b")
>>
>> Jim
>>
>> > On February 16, 2019 1:08:38 PM PST, Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>> > >Dear Jeff,
>> > >One more problem please.
>> > >
>> > >When I used as.Date(ISOdate(dta$year, dta$month, dta$day,dta$hour)) to
>> > >handle date, I could use text(as.Date("2005-03-13"),-9,"b") to label
>> > >my plot.
>> > >
>> > >Now that I am using as.POSIXct(ISOdatetime(year,
>> > >month,day,hour,0,0))), can you please tell me how to text "b" on the
>> > >point corresponding with 1960-05-04 09:00:00 on my plot.
>> > >
>> > >Many thanks for your extra time.
>> > >
>> > >Best wishes
>> > >Ogbos


From g||ted|||e2014 @end|ng |rom gm@||@com  Sun Feb 17 03:37:25 2019
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Sun, 17 Feb 2019 03:37:25 +0100
Subject: [R] Extending my code: RESOLVED
Message-ID: <CAC8ss33VadmJOQ+TeUnFtwt06tS6GgYZOS0kZXKq+yTezrz6rQ@mail.gmail.com>

Dear Jeff and Jim,
Thank you for sorting out this for me.
Replacing text(as.Date("2005-03-13"),5900,"b") with:
text(as.POSIXct(("2005-03-13 09:00:00"),5900,"b") resolved the problem.
Warmest regards
Ogbos
On Sun, Feb 17, 2019 at 3:05 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi Ogbos,
> Remember that there is an underlying numeric representation of dates.
> as.Date assigns dates a number of days from an agreed origin
> (1970-01-01) while POSIX dates assign a number of seconds. If you plot
> in one system and then try to add points in another, it's not going to
> work.
>
> Jim
>
> On Sun, Feb 17, 2019 at 12:55 PM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
> >
> > Dear Jim,
> > Thank you and welcome back. It seems you have been away as you have not been responding to people's questions as before.
> >
> > I have just made a correction on my original question. Please have a look and check if I can still go the way u have suggested or if the correction would call for another approach.
> > Best regards
> > Ogbos
> >
> >
> > On Sun, Feb 17, 2019, 02:22 Jim Lemon <drjimlemon at gmail.com> wrote:
> >>
> >> Hi Ogbos,
> >> It may be easier to use strptime:
> >>
> >> dta<-data.frame(year=rep(2005,5),month=rep("05",5),
> >>  day=c("01","06","11","16","21"),
> >>  hour=c(2,4,6,8,10),minute=rep(0,5),second=rep(0,5),value=1:5)
> >> dta$Ptime<-strptime(paste(paste(dta$year,dta$month,dta$day,sep="-"),
> >>  paste(dta$hour,dta$minute,dta$second,sep=":")),"%Y-%m-%d %H:%M:%S")
> >> plot(dta$Ptime,dta$value)
> >> text(strptime("2005-05-04 09:00:00","%Y-%m-%d %H:%M:%S"),4,"b")
> >>
> >> Jim
> >>
> >> > On February 16, 2019 1:08:38 PM PST, Ogbos Okike <giftedlife2014 at gmail.com> wrote:
> >> > >Dear Jeff,
> >> > >One more problem please.
> >> > >
> >> > >When I used as.Date(ISOdate(dta$year, dta$month, dta$day,dta$hour)) to
> >> > >handle date, I could use text(as.Date("2005-03-13"),-9,"b") to label
> >> > >my plot.
> >> > >
> >> > >Now that I am using as.POSIXct(ISOdatetime(year,
> >> > >month,day,hour,0,0))), can you please tell me how to text "b" on the
> >> > >point corresponding with 1960-05-04 09:00:00 on my plot.
> >> > >
> >> > >Many thanks for your extra time.
> >> > >
> >> > >Best wishes
> >> > >Ogbos


From r@i@1290 m@iii@g oii @im@com  Sat Feb 16 18:33:20 2019
From: r@i@1290 m@iii@g oii @im@com (r@i@1290 m@iii@g oii @im@com)
Date: Sat, 16 Feb 2019 17:33:20 +0000 (UTC)
Subject: [R] Problems trying to place a global map with Ncdf data plot
References: <964633593.296394.1550338400710.ref@mail.yahoo.com>
Message-ID: <964633593.296394.1550338400710@mail.yahoo.com>

Hello there,

I am trying to overlay a global map with ncdf data of precipitation for a
particular location (using specific coordinates). The file is in ncdf format
(commonly used to store away climate data), and I am currently attempting to
place a global map on plotted precipitation values. However, I am having
difficulty placing a global map on this plot and am encountering errors. I
will show you what I have done:

#To create a plot of precipitation data using the following ncdf file - the
following works fine and provides the distributions global precipitation
values (Land+Water values):

library(ncdf4)
Can<-"MaxPrecCCCMACanESM2rcp45.nc"


>Model<-nc_open(Can)
>print(Model)
>attributes(Model$var)
>$names
>dat<-ncvar_get(Model, "onedaymax")
>dat[128,50,1] #View onedaymax for selected latitude, longitude and Year
>nc_lat<-ncvar_get(Model,attributes(Model$dim)$names[2]) #Retrieve latitude
>nc_lon<-ncvar_get(Model,attributes(Model$dim)$names[3]) #Retrieve longitude
>print(paste(dim(nc_lat), "latitudes and", dim(nc_lon), "longitudes"))
>library(maptools)
>map<-dat[,,5] #Precipitation for all longitudes, latitudes, and Year 5
>grid<-expand.grid(nc_lon=nc_lon, nc_lat=nc_lat)
>image(nc_lon,nc_lat,map, ylab="Latitude", xlab="Longitude", main="One-day
maximum precipitation")
>levelplot(map~nc_lon*nc_lat, data=grid, at=cutpoints, cuts=11,
ylab="Latitude", xlab="Longitude", >main="Year 5 one-day maximum
precipitation (mm/day) for CanESM2 under RCP4.5", pretty=T,
col.regions=(rev(brewer.pal(10, "Spectral"))))

#To place a global map on the map that map that returns using the above
code. *This is where errors begin:

>ggplot()+geom_point(aes(x=nc_lon,y=nc_lat,color="onedaymax"),
size=0.8)+borders("world",
colour="black")+scale_color_viridis(name="onedaymax")+theme_void()+coord_quickmap()
*Error: Aesthetics must be either length 1 or the same as the data (128): x,
y, colour*


Why doesn't this work? Could it be that I am not including the "time"
dimension in the ggplot function? The problem, though, is when I try to
obtain the "time" dimension, like I did for latitude and longitude, I
receive the following error:

t<-ncvar_get(Model,"time")
*Error in nc$dim[[idobj$list_index]] : 
? attempt to select more than one element*

If it helps, this is what the variables and dimensions look like in the ncdf
file:

/File MaxPrecCCCMACanESM2rcp45.nc (NC_FORMAT_NETCDF4):

? ? 3 variables (excluding dimension variables):
? ? ? ? double onedaymax[lon,lat,time]? (Contiguous storage)? 
? ? ? ? ? ? units: mm/day
? ? ? ? double fivedaymax[lon,lat,time]? (Contiguous storage)? 
? ? ? ? ? ? units: mm/day
? ? ? ? short Year[time]? (Contiguous storage)? 

? ? 3 dimensions:
? ? ? ? time? Size:95
? ? ? ? lat? Size:64
? ? ? ? ? ? units: degree North
? ? ? ? lon? Size:128
? ? ? ? ? ? units: degree East/

Any help would be greatly appreciated!!!!

Thanks,
	[[alternative HTML version deleted]]


From @boue|m@k@r|m1962 @end|ng |rom gm@||@com  Sun Feb 17 13:29:52 2019
From: @boue|m@k@r|m1962 @end|ng |rom gm@||@com (AbouEl-Makarim Aboueissa)
Date: Sun, 17 Feb 2019 07:29:52 -0500
Subject: [R] Remove cases with -Inf from a data frame
In-Reply-To: <35cf6934-f4ae-d3b5-3a6b-28446f3231a2@sapo.pt>
References: <CAE9stmeV-LE4ZNDZs3bon1CpcRxO9gcDJ+yPLk-0=DqyuvP4Gg@mail.gmail.com>
 <CAGAA5bfCAKYWM_h=mqxUYTHs3BfU5LfJreYnxzu_q3ByVHdBig@mail.gmail.com>
 <35cf6934-f4ae-d3b5-3a6b-28446f3231a2@sapo.pt>
Message-ID: <CAE9stmfVdeu1WMcyO3k3x1R6sTCUasK_YqFmpMzKVq0nRytzqQ@mail.gmail.com>

Dear Rui and All:

thank you very much for your very helpful responses.

with many thanks
abou
______________________


*AbouEl-Makarim Aboueissa, PhD*

*Professor, Statistics and Data Science*
*Graduate Coordinator*

*Department of Mathematics and Statistics*
*University of Southern Maine*



On Sat, Feb 16, 2019 at 11:36 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> An alternative, same dataset.
>
> df[apply(df, 1, function(x) all(is.finite(x))), ]
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 16:14 de 16/02/2019, Martin M?ller Skarbiniks Pedersen escreveu:
> > On Sat, 16 Feb 2019 at 16:07, AbouEl-Makarim Aboueissa <
> > abouelmakarim1962 at gmail.com> wrote:
> >>
> >> I have a log-transformed data frame with some *-Inf* data values.
> >>
> >> *my question: *how to remove all rows with *-Inf* data value from that
> > data
> >> frame?
> >
> >
> > Hi,
> >    Here is a solution which uses apply.
> >
> > First a data-frame as input:
> >
> > set.seed(1)
> > df <- data.frame(w = sample(c(-Inf,1:20), 10),
> >                   x = sample(c(-Inf,1:20), 10),
> >                   y = sample(c(-Inf,1:20), 10),
> >                   z = sample(c(-Inf,1:20), 10))
> >
> > df <- df[-(unlist(apply(df, 2, function(x) which(x == -Inf)))),]
> >
> > Regards
> > Martin
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>

	[[alternative HTML version deleted]]


From em@n|@m@||@92 @end|ng |rom gm@||@com  Sun Feb 17 15:48:25 2019
From: em@n|@m@||@92 @end|ng |rom gm@||@com (=?UTF-8?B?2KXZitmF2KfZhiDYpdiz2YXYp9i52YrZhCDZhdit2YXYrw==?=)
Date: Sun, 17 Feb 2019 16:48:25 +0200
Subject: [R] Question about bindata
Message-ID: <CAFiZHK7azgK5AfiishStgaR1GRf6EB+=gAK_KwqE14Sr70i1FQ@mail.gmail.com>

Hi all,
I am wondering that I can't find the following functions:

   - higher.cor
   - input.cor
   - pairprob
   - pdef

in bindata lib as mentioned in the following paper:
http://epub.wu.ac.at/486/1/document.pdf

How can I reach these functions??

Thanks

	[[alternative HTML version deleted]]


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Sun Feb 17 16:02:16 2019
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard Comcast)
Date: Sun, 17 Feb 2019 10:02:16 -0500
Subject: [R] Weather station data
Message-ID: <712FC03A-4C00-4837-95EA-2FCD272862C6@comcast.net>

Is anyone aware of any R capability to access data at weather stations around the globe? An R package perhaps?

Thanks

Bernard
Sent from my iPhone so please excuse the spelling!"

From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sun Feb 17 16:11:20 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sun, 17 Feb 2019 07:11:20 -0800
Subject: [R] Question about bindata
In-Reply-To: <CAFiZHK7azgK5AfiishStgaR1GRf6EB+=gAK_KwqE14Sr70i1FQ@mail.gmail.com>
References: <CAFiZHK7azgK5AfiishStgaR1GRf6EB+=gAK_KwqE14Sr70i1FQ@mail.gmail.com>
Message-ID: <5EA8AB40-190F-4116-A49E-EA8F5322ADB8@dcn.davis.ca.us>

I would guess in an archived version of the package [1], though they may have been moved to another package. Such an old package probably will not work on a modern version of R... you can extract the functions and see if they still run. 

Note that those functions may have theoretical issues or may have been moved to another package... you might try contacting the authors of the paper come up to date before rummaging around in archives.

[1] https://cran.r-project.org/src/contrib/Archive/bindata/

On February 17, 2019 6:48:25 AM PST, "????? ??????? ????" <emanismail.92 at gmail.com> wrote:
>Hi all,
>I am wondering that I can't find the following functions:
>
>   - higher.cor
>   - input.cor
>   - pairprob
>   - pdef
>
>in bindata lib as mentioned in the following paper:
>http://epub.wu.ac.at/486/1/document.pdf
>
>How can I reach these functions??
>
>Thanks
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From bgunter@4567 @end|ng |rom gm@||@com  Sun Feb 17 16:38:18 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 17 Feb 2019 07:38:18 -0800
Subject: [R] Problems trying to place a global map with Ncdf data plot
In-Reply-To: <964633593.296394.1550338400710@mail.yahoo.com>
References: <964633593.296394.1550338400710.ref@mail.yahoo.com>
 <964633593.296394.1550338400710@mail.yahoo.com>
Message-ID: <CAGxFJbRecWPonahPPg5CNz3ftPjbHNUiaNCvMrp1R--q-5aSeA@mail.gmail.com>

The r-sig-geo list would probably be a better place to post this, as they
specialize in this sort of thing.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, Feb 17, 2019 at 3:48 AM rain1290--- via R-help <r-help at r-project.org>
wrote:

> Hello there,
>
> I am trying to overlay a global map with ncdf data of precipitation for a
> particular location (using specific coordinates). The file is in ncdf
> format
> (commonly used to store away climate data), and I am currently attempting
> to
> place a global map on plotted precipitation values. However, I am having
> difficulty placing a global map on this plot and am encountering errors. I
> will show you what I have done:
>
> #To create a plot of precipitation data using the following ncdf file - the
> following works fine and provides the distributions global precipitation
> values (Land+Water values):
>
> library(ncdf4)
> Can<-"MaxPrecCCCMACanESM2rcp45.nc"
>
>
> >Model<-nc_open(Can)
> >print(Model)
> >attributes(Model$var)
> >$names
> >dat<-ncvar_get(Model, "onedaymax")
> >dat[128,50,1] #View onedaymax for selected latitude, longitude and Year
> >nc_lat<-ncvar_get(Model,attributes(Model$dim)$names[2]) #Retrieve latitude
> >nc_lon<-ncvar_get(Model,attributes(Model$dim)$names[3]) #Retrieve
> longitude
> >print(paste(dim(nc_lat), "latitudes and", dim(nc_lon), "longitudes"))
> >library(maptools)
> >map<-dat[,,5] #Precipitation for all longitudes, latitudes, and Year 5
> >grid<-expand.grid(nc_lon=nc_lon, nc_lat=nc_lat)
> >image(nc_lon,nc_lat,map, ylab="Latitude", xlab="Longitude", main="One-day
> maximum precipitation")
> >levelplot(map~nc_lon*nc_lat, data=grid, at=cutpoints, cuts=11,
> ylab="Latitude", xlab="Longitude", >main="Year 5 one-day maximum
> precipitation (mm/day) for CanESM2 under RCP4.5", pretty=T,
> col.regions=(rev(brewer.pal(10, "Spectral"))))
>
> #To place a global map on the map that map that returns using the above
> code. *This is where errors begin:
>
> >ggplot()+geom_point(aes(x=nc_lon,y=nc_lat,color="onedaymax"),
> size=0.8)+borders("world",
>
> colour="black")+scale_color_viridis(name="onedaymax")+theme_void()+coord_quickmap()
> *Error: Aesthetics must be either length 1 or the same as the data (128):
> x,
> y, colour*
>
>
> Why doesn't this work? Could it be that I am not including the "time"
> dimension in the ggplot function? The problem, though, is when I try to
> obtain the "time" dimension, like I did for latitude and longitude, I
> receive the following error:
>
> t<-ncvar_get(Model,"time")
> *Error in nc$dim[[idobj$list_index]] :
>   attempt to select more than one element*
>
> If it helps, this is what the variables and dimensions look like in the
> ncdf
> file:
>
> /File MaxPrecCCCMACanESM2rcp45.nc (NC_FORMAT_NETCDF4):
>
>     3 variables (excluding dimension variables):
>         double onedaymax[lon,lat,time]  (Contiguous storage)
>             units: mm/day
>         double fivedaymax[lon,lat,time]  (Contiguous storage)
>             units: mm/day
>         short Year[time]  (Contiguous storage)
>
>     3 dimensions:
>         time  Size:95
>         lat  Size:64
>             units: degree North
>         lon  Size:128
>             units: degree East/
>
> Any help would be greatly appreciated!!!!
>
> Thanks,
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sun Feb 17 16:40:10 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 17 Feb 2019 07:40:10 -0800
Subject: [R] Weather station data
In-Reply-To: <712FC03A-4C00-4837-95EA-2FCD272862C6@comcast.net>
References: <712FC03A-4C00-4837-95EA-2FCD272862C6@comcast.net>
Message-ID: <CAGxFJbSpUNTa+ev5rRbMcejHh7wt8wgk5UKSo3G4ZojRZDAJ1w@mail.gmail.com>

Search!

"r-package weather station data"

immediately brought up what looked like several relevant hits.


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, Feb 17, 2019 at 7:02 AM Bernard Comcast <
mcgarvey.bernard at comcast.net> wrote:

> Is anyone aware of any R capability to access data at weather stations
> around the globe? An R package perhaps?
>
> Thanks
>
> Bernard
> Sent from my iPhone so please excuse the spelling!"
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From roy@mende|@@ohn @end|ng |rom no@@@gov  Sun Feb 17 17:22:00 2019
From: roy@mende|@@ohn @end|ng |rom no@@@gov (Roy Mendelssohn - NOAA Federal)
Date: Sun, 17 Feb 2019 08:22:00 -0800
Subject: [R] Problems trying to place a global map with Ncdf data plot
In-Reply-To: <964633593.296394.1550338400710@mail.yahoo.com>
References: <964633593.296394.1550338400710.ref@mail.yahoo.com>
 <964633593.296394.1550338400710@mail.yahoo.com>
Message-ID: <7E084C5D-BBB5-4515-A8AF-729DC4510DC2@noaa.gov>

Hi:

> On Feb 16, 2019, at 9:33 AM, rain1290--- via R-help <r-help at r-project.org> wrote:
> 
>> ggplot()+geom_point(aes(x=nc_lon,y=nc_lat,color="onedaymax"),
> size=0.8)+borders("world",
> colour="black")+scale_color_viridis(name="onedaymax")+theme_void()+coord_quickmap()
> *Error: Aesthetics must be either length 1 or the same as the data (128): x,
> y, colour*

Maybe I am missing something (i am old and it is early on a Sunday),  but I don't see whee the dataset is defined in either ggplot or geom_point. You must have a data frame defined that contains both your expanded grid and the precipitation data and that has to be defined in either the ggplot() call or the geom_point() cal.

HTH,

-Roy

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new street address***
110 McAllister Way
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From |vo@we|ch @end|ng |rom gm@||@com  Sun Feb 17 18:50:31 2019
From: |vo@we|ch @end|ng |rom gm@||@com (Ivo Welch)
Date: Sun, 17 Feb 2019 09:50:31 -0800
Subject: [R] Learning to Write R Packages (Libraries) with Documentation
Message-ID: <CACi4-JmrP0CjUY2V+YejuenDoymQzYR-ch1mfa_h-G7XQ8yQ1Q@mail.gmail.com>

I would like to put together a set of my collected utility functions and
share them internally.  (I don't think they are of any broader interest.)
To do this, I still want to follow good practice.  I am particularly
confused about writing docs.

* for documentation, how do I refer to '@'-type documentation rather than
the latex-like format?  I have read descriptions where both are referred to
as roxygen-type.  I believe that devtools::document() translates the more
convenient @-type into the latex-like format.

* where do I find current good examples of R functions documented properly
with the '@' format.   What should be taken from the function itself (name?
usage?) so as to not repeat myself?

* when I run `document()`, does devtools create a set of documentation
files that I can also easily import by itself into another R session?  I am
asking because I want to put a few functions into my .Rprofile, generate
the documentation, and import it by hand.

* my utility functions currently live in their own environment to avoid
name conflicts ( such as mywork$read.csv <- cmpfun(function()
message("specialized")) ).

  - is keeping function collections in environments a good or bad idea in a
library?
  - will generating a package automatically compile all the functions, so
that I should lose the `cmpfun`s ?
  - to export the functions for others' uses, presumably I should place an
"#` @export" just before the function.

* is there integration between Rmd and R documentation?  Can/should I use
Rmd for writing documentation for my functions and have this become
available through the built-in help system?  Or are the two really separate.

/iaw

PS: Yes, I tried to do my homework.  apparently, the R ecosystem has been
moving fast.  I start reading something, it seems great, but then I find
out that it does not work.  For example, I tried the "Object Documentation"
example from Hadley's book from 2015, but I think it is outdated.  (My
`document()` run seems to want an explicit @name.  Hilary Parker's nice
tutorial is outdated, too, as are many others.  The popular load.Rd example
is already in the latex format. etc.)  where should I look for definitive
documentation for the *current* package writing ecosystem?

	[[alternative HTML version deleted]]


From e@@w|ek @end|ng |rom gm@||@com  Sun Feb 17 20:34:51 2019
From: e@@w|ek @end|ng |rom gm@||@com (Ek Esawi)
Date: Sun, 17 Feb 2019 14:34:51 -0500
Subject: [R] Remove cases with -Inf from a data frame
In-Reply-To: <CAE9stmeV-LE4ZNDZs3bon1CpcRxO9gcDJ+yPLk-0=DqyuvP4Gg@mail.gmail.com>
References: <CAE9stmeV-LE4ZNDZs3bon1CpcRxO9gcDJ+yPLk-0=DqyuvP4Gg@mail.gmail.com>
Message-ID: <CA+ZkTxvXmQoDphjjcGOSS3i77JjRUrj6cv7QaBYHgBf5=s8SCw@mail.gmail.com>

This is a similar versions of other answers.

df[apply(apply(df,2,is.finite),1,sum)==4,]

BOL---EK

On Sat, Feb 16, 2019 at 10:07 AM AbouEl-Makarim Aboueissa
<abouelmakarim1962 at gmail.com> wrote:
>
> Dear All: good morning
>
>
> I have a log-transformed data frame with some *-Inf* data values.
>
> *my question: *how to remove all rows with *-Inf* data value from that data
> frame?
>
>
> with many thanks
> abou
> ______________________
>
>
> *AbouEl-Makarim Aboueissa, PhD*
>
> *Professor, Statistics and Data Science*
> *Graduate Coordinator*
>
> *Department of Mathematics and Statistics*
> *University of Southern Maine*
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Sun Feb 17 20:35:17 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 17 Feb 2019 11:35:17 -0800
Subject: [R] Learning to Write R Packages (Libraries) with Documentation
In-Reply-To: <CACi4-JmrP0CjUY2V+YejuenDoymQzYR-ch1mfa_h-G7XQ8yQ1Q@mail.gmail.com>
References: <CACi4-JmrP0CjUY2V+YejuenDoymQzYR-ch1mfa_h-G7XQ8yQ1Q@mail.gmail.com>
Message-ID: <CAGxFJbSM-5TEuPOqX8xBqP=yyhcPSzoUbQSRYYWP2uHEN2JiZA@mail.gmail.com>

This is off topic for this list. Post to r-package-devel for questions
about writing r packages, package docs, etc. Note especially the use of
namespaces to avoid name clashes.

-- Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, Feb 17, 2019 at 11:27 AM Ivo Welch <ivo.welch at gmail.com> wrote:

> I would like to put together a set of my collected utility functions and
> share them internally.  (I don't think they are of any broader interest.)
> To do this, I still want to follow good practice.  I am particularly
> confused about writing docs.
>
> * for documentation, how do I refer to '@'-type documentation rather than
> the latex-like format?  I have read descriptions where both are referred to
> as roxygen-type.  I believe that devtools::document() translates the more
> convenient @-type into the latex-like format.
>
> * where do I find current good examples of R functions documented properly
> with the '@' format.   What should be taken from the function itself (name?
> usage?) so as to not repeat myself?
>
> * when I run `document()`, does devtools create a set of documentation
> files that I can also easily import by itself into another R session?  I am
> asking because I want to put a few functions into my .Rprofile, generate
> the documentation, and import it by hand.
>
> * my utility functions currently live in their own environment to avoid
> name conflicts ( such as mywork$read.csv <- cmpfun(function()
> message("specialized")) ).
>
>   - is keeping function collections in environments a good or bad idea in a
> library?
>   - will generating a package automatically compile all the functions, so
> that I should lose the `cmpfun`s ?
>   - to export the functions for others' uses, presumably I should place an
> "#` @export" just before the function.
>
> * is there integration between Rmd and R documentation?  Can/should I use
> Rmd for writing documentation for my functions and have this become
> available through the built-in help system?  Or are the two really
> separate.
>
> /iaw
>
> PS: Yes, I tried to do my homework.  apparently, the R ecosystem has been
> moving fast.  I start reading something, it seems great, but then I find
> out that it does not work.  For example, I tried the "Object Documentation"
> example from Hadley's book from 2015, but I think it is outdated.  (My
> `document()` run seems to want an explicit @name.  Hilary Parker's nice
> tutorial is outdated, too, as are many others.  The popular load.Rd example
> is already in the latex format. etc.)  where should I look for definitive
> documentation for the *current* package writing ecosystem?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sun Feb 17 20:38:43 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 17 Feb 2019 11:38:43 -0800
Subject: [R] Learning to Write R Packages (Libraries) with Documentation
In-Reply-To: <CAGxFJbSM-5TEuPOqX8xBqP=yyhcPSzoUbQSRYYWP2uHEN2JiZA@mail.gmail.com>
References: <CACi4-JmrP0CjUY2V+YejuenDoymQzYR-ch1mfa_h-G7XQ8yQ1Q@mail.gmail.com>
 <CAGxFJbSM-5TEuPOqX8xBqP=yyhcPSzoUbQSRYYWP2uHEN2JiZA@mail.gmail.com>
Message-ID: <CAGxFJbQ1-cTPDxUj7h0bL3qTb3RpfXkDWveivnvK1yySfyfSkQ@mail.gmail.com>

Oh, I also would assume that the authoritative, current doc for R package
development is "Writing R Extensions," which of course is part of all R
distros.

-- Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, Feb 17, 2019 at 11:35 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> This is off topic for this list. Post to r-package-devel for questions
> about writing r packages, package docs, etc. Note especially the use of
> namespaces to avoid name clashes.
>
> -- Bert
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Sun, Feb 17, 2019 at 11:27 AM Ivo Welch <ivo.welch at gmail.com> wrote:
>
>> I would like to put together a set of my collected utility functions and
>> share them internally.  (I don't think they are of any broader interest.)
>> To do this, I still want to follow good practice.  I am particularly
>> confused about writing docs.
>>
>> * for documentation, how do I refer to '@'-type documentation rather than
>> the latex-like format?  I have read descriptions where both are referred
>> to
>> as roxygen-type.  I believe that devtools::document() translates the more
>> convenient @-type into the latex-like format.
>>
>> * where do I find current good examples of R functions documented properly
>> with the '@' format.   What should be taken from the function itself
>> (name?
>> usage?) so as to not repeat myself?
>>
>> * when I run `document()`, does devtools create a set of documentation
>> files that I can also easily import by itself into another R session?  I
>> am
>> asking because I want to put a few functions into my .Rprofile, generate
>> the documentation, and import it by hand.
>>
>> * my utility functions currently live in their own environment to avoid
>> name conflicts ( such as mywork$read.csv <- cmpfun(function()
>> message("specialized")) ).
>>
>>   - is keeping function collections in environments a good or bad idea in
>> a
>> library?
>>   - will generating a package automatically compile all the functions, so
>> that I should lose the `cmpfun`s ?
>>   - to export the functions for others' uses, presumably I should place an
>> "#` @export" just before the function.
>>
>> * is there integration between Rmd and R documentation?  Can/should I use
>> Rmd for writing documentation for my functions and have this become
>> available through the built-in help system?  Or are the two really
>> separate.
>>
>> /iaw
>>
>> PS: Yes, I tried to do my homework.  apparently, the R ecosystem has been
>> moving fast.  I start reading something, it seems great, but then I find
>> out that it does not work.  For example, I tried the "Object
>> Documentation"
>> example from Hadley's book from 2015, but I think it is outdated.  (My
>> `document()` run seems to want an explicit @name.  Hilary Parker's nice
>> tutorial is outdated, too, as are many others.  The popular load.Rd
>> example
>> is already in the latex format. etc.)  where should I look for definitive
>> documentation for the *current* package writing ecosystem?
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From em@n|@m@||@92 @end|ng |rom gm@||@com  Sun Feb 17 20:54:12 2019
From: em@n|@m@||@92 @end|ng |rom gm@||@com (=?UTF-8?B?2KXZitmF2KfZhiDYpdiz2YXYp9i52YrZhCDZhdit2YXYrw==?=)
Date: Sun, 17 Feb 2019 21:54:12 +0200
Subject: [R] Question about bindata
In-Reply-To: <5EA8AB40-190F-4116-A49E-EA8F5322ADB8@dcn.davis.ca.us>
References: <CAFiZHK7azgK5AfiishStgaR1GRf6EB+=gAK_KwqE14Sr70i1FQ@mail.gmail.com>
 <5EA8AB40-190F-4116-A49E-EA8F5322ADB8@dcn.davis.ca.us>
Message-ID: <CAFiZHK4L0_GsVy9KYmMQ3X5szf=x1P9mFz_XziBQEpF6-aNDsg@mail.gmail.com>

Thanks a lot Jeff
I sent to authors and I hope can find these function or even alternatives


On Sun, 17 Feb 2019 at 17:11, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> I would guess in an archived version of the package [1], though they may
> have been moved to another package. Such an old package probably will not
> work on a modern version of R... you can extract the functions and see if
> they still run.
>
> Note that those functions may have theoretical issues or may have been
> moved to another package... you might try contacting the authors of the
> paper come up to date before rummaging around in archives.
>
> [1] https://cran.r-project.org/src/contrib/Archive/bindata/
>
> On February 17, 2019 6:48:25 AM PST, "????? ??????? ????" <
> emanismail.92 at gmail.com> wrote:
> >Hi all,
> >I am wondering that I can't find the following functions:
> >
> >   - higher.cor
> >   - input.cor
> >   - pairprob
> >   - pdef
> >
> >in bindata lib as mentioned in the following paper:
> >http://epub.wu.ac.at/486/1/document.pdf
> >
> >How can I reach these functions??
> >
> >Thanks
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sun Feb 17 21:09:36 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sun, 17 Feb 2019 12:09:36 -0800
Subject: [R] Learning to Write R Packages (Libraries) with Documentation
In-Reply-To: <CACi4-JmrP0CjUY2V+YejuenDoymQzYR-ch1mfa_h-G7XQ8yQ1Q@mail.gmail.com>
References: <CACi4-JmrP0CjUY2V+YejuenDoymQzYR-ch1mfa_h-G7XQ8yQ1Q@mail.gmail.com>
Message-ID: <230A3EC8-A194-465C-8B8E-93CE8FEA0390@dcn.davis.ca.us>

Wow. Did you consider reading the Posting Guide, which indicates that this is not the right list for these questions? Please follow up in the right mailing list, but maybe this will get you started.

Your missive is full of value judgements... I can only suppose that is what you get for believing every random post you see on the Internet.

My first suggestion is to remember that there are many perspectives on what is the "right" way to construct packages, but like it or not they all have to lead to a result that conforms to the Writing R Extensions (WRE) document maintained by R Core... stop asking what that means and read that first. Yes it is long, but it IS the reference.

That said, you may find that using roxygen  can reduce some busywork (some hold the opinion that the more traditional approach leads to more useful documentation).  If the CONTRIBUTED PACKAGE roxygen changes how it works over time... well, read its documentation, and keep in mind that whatever is does is only window dressing intended to generate valid files for R to work with according to WRE. Stop depending on blog posts... when they help, great... but when they don't, go to the official documentation.

Finally, Rmd is usable for vignettes, but not any other help files. Rd has its own syntax that is not latex and not Rmd, because it is translated to various other formats as needed in a more comprehensive way than Rmd files are handled.

As for examples... there are thousands, and "good" is much too subjective to sort by. If you want examples of using roxygen, look for the dependency in CRAN.

On February 17, 2019 9:50:31 AM PST, Ivo Welch <ivo.welch at gmail.com> wrote:
>I would like to put together a set of my collected utility functions
>and
>share them internally.  (I don't think they are of any broader
>interest.)
>To do this, I still want to follow good practice.  I am particularly
>confused about writing docs.
>
>* for documentation, how do I refer to '@'-type documentation rather
>than
>the latex-like format?  I have read descriptions where both are
>referred to
>as roxygen-type.  I believe that devtools::document() translates the
>more
>convenient @-type into the latex-like format.
>
>* where do I find current good examples of R functions documented
>properly
>with the '@' format.   What should be taken from the function itself
>(name?
>usage?) so as to not repeat myself?
>
>* when I run `document()`, does devtools create a set of documentation
>files that I can also easily import by itself into another R session? 
>I am
>asking because I want to put a few functions into my .Rprofile,
>generate
>the documentation, and import it by hand.
>
>* my utility functions currently live in their own environment to avoid
>name conflicts ( such as mywork$read.csv <- cmpfun(function()
>message("specialized")) ).
>
>- is keeping function collections in environments a good or bad idea in
>a
>library?
>- will generating a package automatically compile all the functions, so
>that I should lose the `cmpfun`s ?
>- to export the functions for others' uses, presumably I should place
>an
>"#` @export" just before the function.
>
>* is there integration between Rmd and R documentation?  Can/should I
>use
>Rmd for writing documentation for my functions and have this become
>available through the built-in help system?  Or are the two really
>separate.
>
>/iaw
>
>PS: Yes, I tried to do my homework.  apparently, the R ecosystem has
>been
>moving fast.  I start reading something, it seems great, but then I
>find
>out that it does not work.  For example, I tried the "Object
>Documentation"
>example from Hadley's book from 2015, but I think it is outdated.  (My
>`document()` run seems to want an explicit @name.  Hilary Parker's nice
>tutorial is outdated, too, as are many others.  The popular load.Rd
>example
>is already in the latex format. etc.)  where should I look for
>definitive
>documentation for the *current* package writing ecosystem?
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @@r@h@go@|ee @end|ng |rom gm@||@com  Sun Feb 17 23:09:58 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Sun, 17 Feb 2019 17:09:58 -0500
Subject: [R] Weather station data
In-Reply-To: <712FC03A-4C00-4837-95EA-2FCD272862C6@comcast.net>
References: <712FC03A-4C00-4837-95EA-2FCD272862C6@comcast.net>
Message-ID: <CAM_vjumC2kvzPL3Qy-Gj6yAnbtcoHWc9Ka9ibwWTG1+4r5SW5Q@mail.gmail.com>

The VFS package has tools to process GHCN data
(https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-ghcn).

Sarah

On Sun, Feb 17, 2019 at 10:02 AM Bernard Comcast
<mcgarvey.bernard at comcast.net> wrote:
>
> Is anyone aware of any R capability to access data at weather stations around the globe? An R package perhaps?
>
> Thanks
>
> Bernard


-- 
Sarah Goslee (she/her)
http://www.sarahgoslee.com


From drj|m|emon @end|ng |rom gm@||@com  Mon Feb 18 01:32:29 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Mon, 18 Feb 2019 11:32:29 +1100
Subject: [R] Problems trying to place a global map with Ncdf data plot
In-Reply-To: <964633593.296394.1550338400710@mail.yahoo.com>
References: <964633593.296394.1550338400710.ref@mail.yahoo.com>
 <964633593.296394.1550338400710@mail.yahoo.com>
Message-ID: <CA+8X3fW8kq7DGGKR_QFRh5kMmNd_JK1cv+R6F2dmv-dLFNNQBg@mail.gmail.com>

Hi rain1290,
I have recently been working on a similar project, building a grid of
event densities for geographic coordinates. If you are stuck, I may be
able to provide some assistance.

Jim

On Sun, Feb 17, 2019 at 10:49 PM rain1290--- via R-help
<r-help at r-project.org> wrote:
>
> Hello there,
>
> I am trying to overlay a global map with ncdf data of precipitation for a
> particular location (using specific coordinates). The file is in ncdf format
> (commonly used to store away climate data), and I am currently attempting to
> place a global map on plotted precipitation values. However, I am having
> difficulty placing a global map on this plot and am encountering errors. I
> will show you what I have done:
>
> #To create a plot of precipitation data using the following ncdf file - the
> following works fine and provides the distributions global precipitation
> values (Land+Water values):
>
> library(ncdf4)
> Can<-"MaxPrecCCCMACanESM2rcp45.nc"
>
>
> >Model<-nc_open(Can)
> >print(Model)
> >attributes(Model$var)
> >$names
> >dat<-ncvar_get(Model, "onedaymax")
> >dat[128,50,1] #View onedaymax for selected latitude, longitude and Year
> >nc_lat<-ncvar_get(Model,attributes(Model$dim)$names[2]) #Retrieve latitude
> >nc_lon<-ncvar_get(Model,attributes(Model$dim)$names[3]) #Retrieve longitude
> >print(paste(dim(nc_lat), "latitudes and", dim(nc_lon), "longitudes"))
> >library(maptools)
> >map<-dat[,,5] #Precipitation for all longitudes, latitudes, and Year 5
> >grid<-expand.grid(nc_lon=nc_lon, nc_lat=nc_lat)
> >image(nc_lon,nc_lat,map, ylab="Latitude", xlab="Longitude", main="One-day
> maximum precipitation")
> >levelplot(map~nc_lon*nc_lat, data=grid, at=cutpoints, cuts=11,
> ylab="Latitude", xlab="Longitude", >main="Year 5 one-day maximum
> precipitation (mm/day) for CanESM2 under RCP4.5", pretty=T,
> col.regions=(rev(brewer.pal(10, "Spectral"))))
>
> #To place a global map on the map that map that returns using the above
> code. *This is where errors begin:
>
> >ggplot()+geom_point(aes(x=nc_lon,y=nc_lat,color="onedaymax"),
> size=0.8)+borders("world",
> colour="black")+scale_color_viridis(name="onedaymax")+theme_void()+coord_quickmap()
> *Error: Aesthetics must be either length 1 or the same as the data (128): x,
> y, colour*
>
>
> Why doesn't this work? Could it be that I am not including the "time"
> dimension in the ggplot function? The problem, though, is when I try to
> obtain the "time" dimension, like I did for latitude and longitude, I
> receive the following error:
>
> t<-ncvar_get(Model,"time")
> *Error in nc$dim[[idobj$list_index]] :
>   attempt to select more than one element*
>
> If it helps, this is what the variables and dimensions look like in the ncdf
> file:
>
> /File MaxPrecCCCMACanESM2rcp45.nc (NC_FORMAT_NETCDF4):
>
>     3 variables (excluding dimension variables):
>         double onedaymax[lon,lat,time]  (Contiguous storage)
>             units: mm/day
>         double fivedaymax[lon,lat,time]  (Contiguous storage)
>             units: mm/day
>         short Year[time]  (Contiguous storage)
>
>     3 dimensions:
>         time  Size:95
>         lat  Size:64
>             units: degree North
>         lon  Size:128
>             units: degree East/
>
> Any help would be greatly appreciated!!!!
>
> Thanks,
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m||uj|@b @end|ng |rom gm@||@com  Mon Feb 18 09:57:38 2019
From: m||uj|@b @end|ng |rom gm@||@com (Miluji Sb)
Date: Mon, 18 Feb 2019 09:57:38 +0100
Subject: [R] Plot coordinates on world map with Robinson CRS - ggplot2
Message-ID: <CAMLwc7PQvGEzjEsZtTzCHtoi61M6OLR7yozOPpYSsrtqswa-Ng@mail.gmail.com>

Dear all,

I am trying to plot coordinates on a world map with Robinson CRS. While the
world map is generated without any issues, when I try to plot the points -
I only get a single point.

The code I am using and the coordinates data is below. What am I doing
wrong? Any help/suggestions will be highly appreciated.

library(data.table)
library(foreign)
library(readstata13)
library(rgdal)
library(maptools)
library(ggplot2)
library(dplyr)

load(url("
https://github.com/valentinitnelav/RandomScripts/blob/master/NaturalEarth.RData?raw=true
"))

PROJ <- "+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84
+units=m +no_defs"

NE_countries_rob  <- spTransform(NE_countries, CRSobj = PROJ)
NE_graticules_rob <- spTransform(NE_graticules, CRSobj = PROJ)
NE_box_rob        <- spTransform(NE_box, CRSobj = PROJ)

# project long-lat coordinates for graticule label data frames
# (two extra columns with projected XY are created)
prj.coord <- project(cbind(lbl.Y$lon, lbl.Y$lat), proj=PROJ)
lbl.Y.prj <- cbind(prj.coord, lbl.Y)
names(lbl.Y.prj)[1:2] <- c("X.prj","Y.prj")

prj.coord <- project(cbind(lbl.X$lon, lbl.X$lat), proj=PROJ)
lbl.X.prj <- cbind(prj.coord, lbl.X)
names(lbl.X.prj)[1:2] <- c("X.prj","Y.prj")

m <- ggplot() +
  # add Natural Earth countries projected to Robinson, give black border
and fill with gray
  geom_polygon(data=NE_countries_rob, aes(long,lat, group=group),
colour="black", fill="gray80", size = 0.25) +
  # Note: "Regions defined for each Polygons" warning has to do with
fortify transformation. Might get deprecated in future!
  # alternatively, use use map_data(NE_countries) to transform to data
frame and then use project() to change to desired projection.
  # add Natural Earth box projected to Robinson
  geom_polygon(data=NE_box_rob, aes(x=long, y=lat), colour="black",
fill="transparent", size = 0.25) +
  # add graticules projected to Robinson
  geom_path(data=NE_graticules_rob, aes(long, lat, group=group),
linetype="dotted", color="grey50", size = 0.25) +
  # add graticule labels - latitude and longitude
  geom_text(data = lbl.Y.prj, aes(x = X.prj, y = Y.prj, label = lbl),
color="grey50", size=2) +
  geom_text(data = lbl.X.prj, aes(x = X.prj, y = Y.prj, label = lbl),
color="grey50", size=2) +
  # the default, ratio = 1 in coord_fixed ensures that one unit on the
x-axis is the same length as one unit on the y-axis
  coord_fixed(ratio = 1) +
  geom_point(data=df,
             aes(x=lon, y=lat), colour="Deep Pink",
             fill="Pink",pch=21, size=2, alpha=I(0.7))
  # remove the background and default gridlines
  theme_void()

## coordinates dataframe
dput(df)
structure(list(lon = c(2.67569724621467, 17.5766416259819,
28.4126232192772,
23.8147674538232, 29.8917589327105), lat = c(28.1503115976162,
-12.3388787380201, 9.78891068739477, -22.1873831176644, -3.36546931479253
)), class = "data.frame", row.names = c(NA, -5L))

Sincerely,

Milu

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon Feb 18 10:33:41 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 18 Feb 2019 01:33:41 -0800
Subject: [R] Plot coordinates on world map with Robinson CRS - ggplot2
In-Reply-To: <CAMLwc7PQvGEzjEsZtTzCHtoi61M6OLR7yozOPpYSsrtqswa-Ng@mail.gmail.com>
References: <CAMLwc7PQvGEzjEsZtTzCHtoi61M6OLR7yozOPpYSsrtqswa-Ng@mail.gmail.com>
Message-ID: <C5A1B821-0E11-4ABF-BAFF-7D69A51D09CB@dcn.davis.ca.us>

This is the wrong place for this question.

https://stat.ethz.ch/mailman/listinfo/r-sig-geo

On February 18, 2019 12:57:38 AM PST, Miluji Sb <milujisb at gmail.com> wrote:
>Dear all,
>
>I am trying to plot coordinates on a world map with Robinson CRS. While
>the
>world map is generated without any issues, when I try to plot the
>points -
>I only get a single point.
>
>The code I am using and the coordinates data is below. What am I doing
>wrong? Any help/suggestions will be highly appreciated.
>
>library(data.table)
>library(foreign)
>library(readstata13)
>library(rgdal)
>library(maptools)
>library(ggplot2)
>library(dplyr)
>
>load(url("
>https://github.com/valentinitnelav/RandomScripts/blob/master/NaturalEarth.RData?raw=true
>"))
>
>PROJ <- "+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84
>+units=m +no_defs"
>
>NE_countries_rob  <- spTransform(NE_countries, CRSobj = PROJ)
>NE_graticules_rob <- spTransform(NE_graticules, CRSobj = PROJ)
>NE_box_rob        <- spTransform(NE_box, CRSobj = PROJ)
>
># project long-lat coordinates for graticule label data frames
># (two extra columns with projected XY are created)
>prj.coord <- project(cbind(lbl.Y$lon, lbl.Y$lat), proj=PROJ)
>lbl.Y.prj <- cbind(prj.coord, lbl.Y)
>names(lbl.Y.prj)[1:2] <- c("X.prj","Y.prj")
>
>prj.coord <- project(cbind(lbl.X$lon, lbl.X$lat), proj=PROJ)
>lbl.X.prj <- cbind(prj.coord, lbl.X)
>names(lbl.X.prj)[1:2] <- c("X.prj","Y.prj")
>
>m <- ggplot() +
> # add Natural Earth countries projected to Robinson, give black border
>and fill with gray
>  geom_polygon(data=NE_countries_rob, aes(long,lat, group=group),
>colour="black", fill="gray80", size = 0.25) +
>  # Note: "Regions defined for each Polygons" warning has to do with
>fortify transformation. Might get deprecated in future!
>  # alternatively, use use map_data(NE_countries) to transform to data
>frame and then use project() to change to desired projection.
>  # add Natural Earth box projected to Robinson
>  geom_polygon(data=NE_box_rob, aes(x=long, y=lat), colour="black",
>fill="transparent", size = 0.25) +
>  # add graticules projected to Robinson
>  geom_path(data=NE_graticules_rob, aes(long, lat, group=group),
>linetype="dotted", color="grey50", size = 0.25) +
>  # add graticule labels - latitude and longitude
>  geom_text(data = lbl.Y.prj, aes(x = X.prj, y = Y.prj, label = lbl),
>color="grey50", size=2) +
>  geom_text(data = lbl.X.prj, aes(x = X.prj, y = Y.prj, label = lbl),
>color="grey50", size=2) +
>  # the default, ratio = 1 in coord_fixed ensures that one unit on the
>x-axis is the same length as one unit on the y-axis
>  coord_fixed(ratio = 1) +
>  geom_point(data=df,
>             aes(x=lon, y=lat), colour="Deep Pink",
>             fill="Pink",pch=21, size=2, alpha=I(0.7))
>  # remove the background and default gridlines
>  theme_void()
>
>## coordinates dataframe
>dput(df)
>structure(list(lon = c(2.67569724621467, 17.5766416259819,
>28.4126232192772,
>23.8147674538232, 29.8917589327105), lat = c(28.1503115976162,
>-12.3388787380201, 9.78891068739477, -22.1873831176644,
>-3.36546931479253
>)), class = "data.frame", row.names = c(NA, -5L))
>
>Sincerely,
>
>Milu
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From petr@p|k@| @end|ng |rom prechez@@cz  Mon Feb 18 11:26:02 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Mon, 18 Feb 2019 10:26:02 +0000
Subject: [R] Problem with combining 2 data frame
In-Reply-To: <0ad8385b0403402e93f8d6d0f8485d05@tamu.edu>
References: <CANTxAmLBHMQCgXbZNLtxsGEq7eGsr+ZQSacMBg8A5K6KqKsehw@mail.gmail.com>
 <CAGgJW77HgJ_nT61YeeTC6Q5ieTftWfpd6X_S3GF2XqEqZ388cg@mail.gmail.com>
 <0ad8385b0403402e93f8d6d0f8485d05@tamu.edu>
Message-ID: <ce84a558f46649ca82c776b6b607b607@SRVEXCHCM1302.precheza.cz>

Hi

and if you consider stringsAsFactor option a nuisance you probably could use as.character

df1$x2[match(df2$x1, df1$x1)] <- as.character(df2$x2)

Slightly different approach is merge
merge(df1, df2, by=c("x1"), all=T)

which gives you additional column from df2 with NA values together with merged x2 values.

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of David L Carlson
> Sent: Sunday, February 17, 2019 1:49 AM
> To: Eric Berger <ericjberger at gmail.com>; javad bayat
> <j.bayat194 at gmail.com>
> Cc: R mailing list <R-help at r-project.org>; r-help-owner at r-project.org
> Subject: Re: [R] Problem with combining 2 data frame
>
> This is another approach using match():
>
> df1 = data.frame(x1 = letters[1:26],x2 = NA, stringsAsFactors=FALSE)
> df2 = data.frame(x1 = letters[10:15],x2 = c("1a","2a","3a","4a","5a","6a"),
>      stringsAsFactors =FALSE)
>
> df1$x2[match(df2$x1, df1$x1)] <- df2$x2
>
> ---------------------------------------------
> David L. Carlson
> Department of Anthropology
> Texas A&M University
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Eric Berger
> Sent: Friday, February 15, 2019 11:43 PM
> To: javad bayat <j.bayat194 at gmail.com>
> Cc: R mailing list <R-help at r-project.org>; r-help-owner at r-project.org
> Subject: Re: [R] Problem with combining 2 data frame
>
> Hi Javad,
> You have a number of problems with your code, such as:
> 1. you should set df1 and df2 without factors 2. you define a function f(x,y) but
> the body of the function never refers to x and y
>
> The following code does what I think you are looking for:
>
> df1 = data.frame(x1 = letters[1:26],x2 = NA,stringsAsFactors = FALSE)
> df2 = data.frame(x1 = letters[10:15],x2 = c("1a","2a","3a","4a","5a","6a"),
> stringsAsFactors = FALSE)
>
> aa <- sapply( 1:nrow(df2), function(i) { df1$x2[ df1$x1==df2$x1[i] ] <<-
> df2$x2[i] } )
>
> HTH,
> Eric
>
>
> On Sat, Feb 16, 2019 at 4:11 AM javad bayat <j.bayat194 at gmail.com> wrote:
>
> > Dear R users;
> > I am trying to combine 2 dataframes with different rows, 26 and 6
> > rows. The first column of both dataframe has something in common, and
> > I want to compare the first column of the df1 with first column of the
> > df2 to see if they are same or not. Then if they were same, the second
> > column of the df1 fill by the value of the second column of df2.
> >
> > df1 = data.frame(x1 = letters[1:26],x2 = NA)
> > df2 = data.frame(x1 = letters[10:15],x2 =
> > c("1a","2a","3a","4a","5a","6a"))
> >
> > f = function(x,y){
> >           for (i in 1:nrow(df1))
> >            ifelse(df1$x1 == df2$x1, df1$x2==df2$x2, "NA")}
> > f(df1,df2)
> > Error in Ops.factor(df1$x1, df2$x1) : level sets of factors are
> > different
> >
> > Is there anyone to help me to solve this problem?
> > Sincerely.
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> > --
> > Best Regards
> > Javad Bayat
> > M.Sc. Environment Engineering
> > Alternative Mail: bayat194 at yahoo.com
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


From petr@p|k@| @end|ng |rom prechez@@cz  Mon Feb 18 11:47:21 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Mon, 18 Feb 2019 10:47:21 +0000
Subject: [R] Taking the Average of a subset of data
In-Reply-To: <BN6PR05MB3380994F75A26E8477089275AE600@BN6PR05MB3380.namprd05.prod.outlook.com>
References: <BN6PR05MB3380994F75A26E8477089275AE600@BN6PR05MB3380.namprd05.prod.outlook.com>
Message-ID: <ec14de74d24c400cb1352340b905da6e@SRVEXCHCM1302.precheza.cz>

Hi

Could you show what is your intention with your data? What do you mean by sort data to have the same number of leaves? Do you want to trim excessive rows in both data.frames to meet such condition?

I would suggest using merge.

merge(test1, test2, by.x=c("plot", "plant"), by.y=c("plot_lai", "plant_lai"), all=TRUE)

which gives you one data.frame with rows corresponding to each plot and line

After that you could remove all rows having NA in respective columns, which ensures that there is same number of leaves in each column.

Cheers
Petr

> dput(test1)
structure(list(plot = c(104L, 104L, 104L, 104L, 104L, 104L, 104L,
104L, 105L, 105L, 105L, 105L, 105L, 105L, 105L, 105L, 106L, 106L,
106L, 106L, 106L, 106L, 106L, 106L, 106L, 108L, 108L, 108L, 108L
), plant = c(5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 5L, 5L, 5L, 5L,
6L, 6L, 6L, 6L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 5L, 5L, 5L,
5L), leaf_number = c(1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L,
3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 10L, 1L, 2L, 3L, 4L,
1L, 2L, 3L, 4L), sen_score = c(90L, 90L, 95L, 100L, 95L, 85L,
90L, 90L, 90L, 95L, 100L, 100L, 70L, 80L, 90L, 80L, 100L, 90L,
100L, 100L, 0L, 100L, 30L, 100L, 40L, 100L, 100L, 100L, 100L)), class = "data.frame", row.names = c(NA,
-29L))
> dput(test2)
structure(list(plot_lai = c(104L, 104L, 104L, 104L, 104L, 104L,
104L, 104L, 104L, 104L, 104L, 104L, 104L, 104L, 104L, 104L, 104L,
104L, 105L, 105L, 105L, 105L, 105L, 105L, 105L, 105L, 105L, 105L,
105L), plant_lai = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 2L, 2L), lai_score = c(82L, 167L, 248L, 343L, 377L, 372L,
335L, 221L, 162L, 145L, 235L, 310L, 393L, 455L, 472L, 445L, 330L,
292L, 64L, 139L, 211L, 296L, 348L, 392L, 405L, 379L, 278L, 64L,
209L), leaf_num = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 1L, 2L,
3L, 4L, 5L, 6L, 7L, 8L, 9L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,
1L, 2L)), class = "data.frame", row.names = c(NA, -29L))
>



> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Isaac Barnhart
> Sent: Friday, February 15, 2019 4:07 PM
> To: r-help at r-project.org
> Subject: [R] Taking the Average of a subset of data
>
> Hello all, I have another question. I'm working with the following dataset:
>
> plot    plant   leaf_number     sen_score       plot_lai        plant_lai       lai_score
> leaf_num
> 104     5       1       90      104     1       82      1
> 104     5       2       90      104     1       167     2
> 104     5       3       95      104     1       248     3
> 104     5       4       100     104     1       343     4
> 104     6       1       95      104     1       377     5
> 104     6       2       85      104     1       372     6
> 104     6       3       90      104     1       335     7
> 104     6       4       90      104     1       221     8
> 105     5       1       90      104     1       162     9
> 105     5       2       95      104     2       145     1
> 105     5       3       100     104     2       235     2
> 105     5       4       100     104     2       310     3
> 105     6       1       70      104     2       393     4
> 105     6       2       80      104     2       455     5
> 105     6       3       90      104     2       472     6
> 105     6       4       80      104     2       445     7
> 106     5       1       100     104     2       330     8
> 106     5       2       90      104     2       292     9
> 106     5       3       100     105     1       64      1
> 106     5       4       100     105     1       139     2
> 106     5       10      0       105     1       211     3
> 106     6       1       100     105     1       296     4
> 106     6       2       30      105     1       348     5
> 106     6       3       100     105     1       392     6
> 106     6       4       40      105     1       405     7
> 108     5       1       100     105     1       379     8
> 108     5       2       100     105     1       278     9
> 108     5       3       100     105     2       64      1
> 108     5       4       100     105     2       209     2
>
> (Note: 'plant' and 'leaf' column should be separated. '51' means plant 5, leaf
> 1).
>
>
> This dataset shows two datasets: The left 4 columns are of one  measurement
> (leaf senescence), and the right 4 columns are of another (leaf area index). I
> have a large amount of plots, and several plants, more than what is listed.
>
>
> I need to sort both datasets (senescence and leaf area index) so that each plot
> has the same number of leaves.
>
>
> This is hard because sometimes plots in the 'senescence' dataset have more
> leaves, and sometimes plots in the 'leaf area index'. Is there a way to sort both
> datasets so that this requirement is met? Like I said, there is no way to tell
> which dataset has the plot with the minimum amount of leaves; it can be
> either one in any case.
>
>
> Any help would be appreciated!
>
>
> Isaac
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


From bog@@o@chr|@to|er @end|ng |rom gm@||@com  Mon Feb 18 18:47:03 2019
From: bog@@o@chr|@to|er @end|ng |rom gm@||@com (Christofer Bogaso)
Date: Mon, 18 Feb 2019 23:17:03 +0530
Subject: [R] Memory usage
Message-ID: <CA+dpOJmgniBmaf8V-M38pXNJ+vBzeuCj8RUWx4T7tbZ9rH1cfg@mail.gmail.com>

Hi,

I have below lines of code to understand how R manages memory.

> library(pryr)

*Warning message:*

*package ?pryr? was built under R version 3.4.3 *

> mem_change(x <- 1:1e6)

4.01 MB

> mem_change(y <- x)

976 B

> mem_change(x[100] < NA)

976 B

> mem_change(rm(x))

864 B

> mem_change(rm(y))

-4 MB

>

I do understand why there is only 976 B positive change in the 3rd line.
This is because now y and x both points to the same block of memory that
holds 1:1e6.

But I dont understand below

> mem_change(rm(x))

864 B
Why memory consumption increased here while deleting an object, although by
a small amount?

Any detailed explanation will be appreciated. Thanks,

	[[alternative HTML version deleted]]


From Ev@n@L|ndenberger @end|ng |rom jwu@edu  Mon Feb 18 18:32:04 2019
From: Ev@n@L|ndenberger @end|ng |rom jwu@edu (Evan Lindenberger)
Date: Mon, 18 Feb 2019 17:32:04 +0000
Subject: [R] R Software
Message-ID: <DM6PR06MB5977819DC2087D8E879E1AE4FB630@DM6PR06MB5977.namprd06.prod.outlook.com>

Hello,

My name is Evan Lindenberger and I work at the Johnson & Wales information security office. We received a request for R Software, but I have a few questions before we start using R, such as:

- What information does R collect?
- Does the R Foundation have a written information security policy (WISP)?
- Is R compliant with GDPR and ADA?

If someone could get back to me, that would be greatly appreciated.

Thank you.

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Mon Feb 18 23:11:02 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Mon, 18 Feb 2019 22:11:02 +0000
Subject: [R] R Software
In-Reply-To: <DM6PR06MB5977819DC2087D8E879E1AE4FB630@DM6PR06MB5977.namprd06.prod.outlook.com>
References: <DM6PR06MB5977819DC2087D8E879E1AE4FB630@DM6PR06MB5977.namprd06.prod.outlook.com>
Message-ID: <5303c9de-b479-3f88-3c1e-e2bb8ac110f8@sapo.pt>

Hello,

I do not speak for the R Foundation but I believe you are not aware that 
R is a computer language for statistics biostatistics and (scientific) 
graphics.

- R itself does not collect data.
- Security policies are left to the users.
- You can program whatever you want since R is Turing equivalent, GDPR 
or ADA compliant or not. It's up to the users/developers to comply to 
laws. (I hope they do.)

Regarding this, R is pretty much the same as, for instance, C, C++, 
Fortran, etc. And just like those languages R is used by companies and 
other institutions, government or private, that enforce strong security 
policies.


Hope this helps,

Rui Barradas

?s 17:32 de 18/02/2019, Evan Lindenberger escreveu:
> Hello,
> 
> My name is Evan Lindenberger and I work at the Johnson & Wales information security office. We received a request for R Software, but I have a few questions before we start using R, such as:
> 
> - What information does R collect?
> - Does the R Foundation have a written information security policy (WISP)?
> - Is R compliant with GDPR and ADA?
> 
> If someone could get back to me, that would be greatly appreciated.
> 
> Thank you.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bgunter@4567 @end|ng |rom gm@||@com  Mon Feb 18 23:45:07 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 18 Feb 2019 14:45:07 -0800
Subject: [R] R Software
In-Reply-To: <5303c9de-b479-3f88-3c1e-e2bb8ac110f8@sapo.pt>
References: <DM6PR06MB5977819DC2087D8E879E1AE4FB630@DM6PR06MB5977.namprd06.prod.outlook.com>
 <5303c9de-b479-3f88-3c1e-e2bb8ac110f8@sapo.pt>
Message-ID: <CAGxFJbQDtw9sA-egoiDeCDS+djK6kLubT81aGSuwbhcxmQ=TSA@mail.gmail.com>

To add to what Rui said, go here:
https://www.r-project.org/

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Feb 18, 2019 at 2:11 PM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> I do not speak for the R Foundation but I believe you are not aware that
> R is a computer language for statistics biostatistics and (scientific)
> graphics.
>
> - R itself does not collect data.
> - Security policies are left to the users.
> - You can program whatever you want since R is Turing equivalent, GDPR
> or ADA compliant or not. It's up to the users/developers to comply to
> laws. (I hope they do.)
>
> Regarding this, R is pretty much the same as, for instance, C, C++,
> Fortran, etc. And just like those languages R is used by companies and
> other institutions, government or private, that enforce strong security
> policies.
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 17:32 de 18/02/2019, Evan Lindenberger escreveu:
> > Hello,
> >
> > My name is Evan Lindenberger and I work at the Johnson & Wales
> information security office. We received a request for R Software, but I
> have a few questions before we start using R, such as:
> >
> > - What information does R collect?
> > - Does the R Foundation have a written information security policy
> (WISP)?
> > - Is R compliant with GDPR and ADA?
> >
> > If someone could get back to me, that would be greatly appreciated.
> >
> > Thank you.
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From B|||@G|e@@ner @end|ng |rom cwu@edu  Mon Feb 18 21:17:02 2019
From: B|||@G|e@@ner @end|ng |rom cwu@edu (Bill Glessner)
Date: Mon, 18 Feb 2019 20:17:02 +0000
Subject: [R] Most recent version of R for IBM Power8 Ubuntu environment
Message-ID: <MWHPR07MB2830E295614864549615F44CF3630@MWHPR07MB2830.namprd07.prod.outlook.com>

Good afternoon,

What is the most recent version of R known to run reliably in an environment of IBM HPC cluster Power8 systems, Ubuntu 16.04.1 operating system, with gpfs as the cluster file-system? It would seem to be R version 3.2.3; however, that one is not current enough for all the packages that are required by a number of research projects at the university. R version 3.5.1 has been built and run through 'make check' without incident; but, when actual project work began, the cluster-node running the R application became hung, requiring a reboot to re-enable the gpfs daemon and communication with the cluster storage-node.
Thank you for your assistance.

Bill Glessner       - Central Washington University


	[[alternative HTML version deleted]]


From |ou|@ebj @end|ng |rom drcmr@dk  Mon Feb 18 15:58:18 2019
From: |ou|@ebj @end|ng |rom drcmr@dk (=?utf-8?Q?Louise_Baru=C3=ABl_Johansen?=)
Date: Mon, 18 Feb 2019 15:58:18 +0100
Subject: [R] Interaction effects with GAMM
Message-ID: <263FD529-00C5-4A93-AF14-67EC2E8199E5@drcmr.dk>

I have a question on how to model interaction terms including smooths in a GAMM model (using the mgcv and nlme packages in R).

We have collected longitudinal behavioral and brain imaging data from ~100 subjects across ~6 time points, and I would like to model main effects of age, sex, brain as well as to-way interaction terms (and maybe three-way interaction terms), while correcting for education level and taking random effects into account. Is using the ti() setup the way to do this:

M = gamm(behav ~ ti(age) + sex + education + ti(age, by = sex) + brain + ti(brain, by = age), random = list(subjectID = ~1+age), data = data)

All help will be appreciated. 

Thanks, Louise


	[[alternative HTML version deleted]]


From |@r@@ve|ten @end|ng |rom emb|@de  Mon Feb 18 20:51:11 2019
From: |@r@@ve|ten @end|ng |rom emb|@de (Lars Velten)
Date: Mon, 18 Feb 2019 19:51:11 +0000
Subject: [R] Save creates huge files, dump doesn't
Message-ID: <2feb543facba484e5d1e10abcc62a149@webmail.embl.de>

Dear list,
I noticed an extremely odd behavior... I have a rather complex shiny app which allows the user to store his/her state which internally obviously triggers as call to save as follows
save(list=c("plots","gates","populations","cg", "genelists","colorscores",  "proj", "actds"),file=fname)
this was all working fine until some time ago (?!?) files created by this command became several hundred MBs big... even thought the cumulative size of all objects in memory after load() is in the 10s of kB. 
Changing to
dump(list=c("plots","gates","populations","cg", "genelists","colorscores",  "proj", "actds"),file=fname)
solved the problem, output was then only 10s of kB.
(Why/when) is this behavior intended?
Best wishes,
Lars

	[[alternative HTML version deleted]]


From bountym|nd@ @end|ng |rom gm@||@com  Tue Feb 19 07:54:52 2019
From: bountym|nd@ @end|ng |rom gm@||@com (Floriane Kg)
Date: Tue, 19 Feb 2019 01:54:52 -0500
Subject: [R] suprising mm %*% coef(object) : non-conformable arguments with
 predict
Message-ID: <CAEpHxwFpnAGAGyX8GYE-uD5jNO85_PQPT9+FM5g3Xb7e+GcCUA@mail.gmail.com>

Hi,
A possible bug here?
My code worked yesterday, but today I have an error, and still don't know
why.
I joined a reproducible code and data files.

Error in mm %*% coef(object) : non-conformable arguments
Even if the data used to predict are the same as the one used to fit the
regression(has the same levels and number of covariates)

```r
## load package
library("prediction")
library(survey)

options(survey.lonely.psu = "adjust")
  my.svydesign <- survey::svydesign(ids = ~clust+idh, strata = ~reg+ur,
nest = T, weights = ~wt,
                                    data = ladies )

  DAT<-subset(ladies,region=="Sahel")

  tmp<-subset(my.svydesign,(region=="Sahel"))

X<-c((-1), names(COVARIATES)) #COVARIATES HAVE BEEN EXTRACTED FROM data DAT
  formula <- as.formula(paste("MCuser ~", paste(X, collapse= "+")))
  glm.ob5<-svyglm(formula,design=tmp,family=quasibinomial, maxit = 50)
  summary(glm.ob5)
  pred5<-predict(glm.ob5,DAT,type="response", se.fit=TRUE)#*100)

Error in mm %*% coef(object) : non-conformable arguments

#Even using the dataframe COVARIATES it does not work anymore
  pred5<-predict(glm.ob5,COVARIATES,type="response", se.fit=TRUE)#*100)


Error in mm %*% coef(object) : non-conformable arguments

Thank you
COVARIATES.RData
<http://r.789695.n4.nabble.com/file/t387811/COVARIATES.RData>
DAT.RData <http://r.789695.n4.nabble.com/file/t387811/DAT.RData>
ladies.RData <http://r.789695.n4.nabble.com/file/t387811/ladies.RData>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue Feb 19 18:58:26 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 19 Feb 2019 09:58:26 -0800
Subject: [R] Most recent version of R for IBM Power8 Ubuntu environment
In-Reply-To: <MWHPR07MB2830E295614864549615F44CF3630@MWHPR07MB2830.namprd07.prod.outlook.com>
References: <MWHPR07MB2830E295614864549615F44CF3630@MWHPR07MB2830.namprd07.prod.outlook.com>
Message-ID: <FA7EDAA4-E814-49CB-8AF6-A24298F544D9@dcn.davis.ca.us>

Y'know, there is a mailing list for that (kind of question)... R-sig-debian...

On February 18, 2019 12:17:02 PM PST, Bill Glessner <Bill.Glessner at cwu.edu> wrote:
>Good afternoon,
>
>What is the most recent version of R known to run reliably in an
>environment of IBM HPC cluster Power8 systems, Ubuntu 16.04.1 operating
>system, with gpfs as the cluster file-system? It would seem to be R
>version 3.2.3; however, that one is not current enough for all the
>packages that are required by a number of research projects at the
>university. R version 3.5.1 has been built and run through 'make check'
>without incident; but, when actual project work began, the cluster-node
>running the R application became hung, requiring a reboot to re-enable
>the gpfs daemon and communication with the cluster storage-node.
>Thank you for your assistance.
>
>Bill Glessner       - Central Washington University
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From HDor@n @end|ng |rom @|r@org  Tue Feb 19 19:26:02 2019
From: HDor@n @end|ng |rom @|r@org (Doran, Harold)
Date: Tue, 19 Feb 2019 18:26:02 +0000
Subject: [R] Save creates huge files, dump doesn't
In-Reply-To: <2feb543facba484e5d1e10abcc62a149@webmail.embl.de>
References: <2feb543facba484e5d1e10abcc62a149@webmail.embl.de>
Message-ID: <BN7PR05MB58576280B97F4FE0F6DCA7F9CA7C0@BN7PR05MB5857.namprd05.prod.outlook.com>

Lars

Typically answers regarding shiny are not answered here (I do wish there was a SIG, however). With that said, are you saying that the object size is increasing for objects that previously were much smaller? Or, are other things now being saved into the workspace that were not there previously?

In other words, suppose you have some object called tmp. If you do object.size(tmp) on your older version and on the newer version are they the same size? Are the objects of the same class?



-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Lars Velten
Sent: Monday, February 18, 2019 2:51 PM
To: r-help at r-project.org
Subject: [R] Save creates huge files, dump doesn't

Dear list,
I noticed an extremely odd behavior... I have a rather complex shiny app which allows the user to store his/her state which internally obviously triggers as call to save as follows save(list=c("plots","gates","populations","cg", "genelists","colorscores",  "proj", "actds"),file=fname) this was all working fine until some time ago (?!?) files created by this command became several hundred MBs big... even thought the cumulative size of all objects in memory after load() is in the 10s of kB. 
Changing to
dump(list=c("plots","gates","populations","cg", "genelists","colorscores",  "proj", "actds"),file=fname) solved the problem, output was then only 10s of kB.
(Why/when) is this behavior intended?
Best wishes,
Lars

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Tue Feb 19 19:32:45 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 19 Feb 2019 10:32:45 -0800
Subject: [R] Interaction effects with GAMM
In-Reply-To: <263FD529-00C5-4A93-AF14-67EC2E8199E5@drcmr.dk>
References: <263FD529-00C5-4A93-AF14-67EC2E8199E5@drcmr.dk>
Message-ID: <CAGxFJbS=pReZ215s1BV77Fh+g_4Ubf5rhG+aLO3Ufjmmc9769A@mail.gmail.com>

Wrong list. This list is about R programming, not statistical questions on
mixed models. Post on the r-sig-mixed-models list for that.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Feb 19, 2019 at 10:07 AM Louise Baru?l Johansen <louisebj at drcmr.dk>
wrote:

> I have a question on how to model interaction terms including smooths in a
> GAMM model (using the mgcv and nlme packages in R).
>
> We have collected longitudinal behavioral and brain imaging data from ~100
> subjects across ~6 time points, and I would like to model main effects of
> age, sex, brain as well as to-way interaction terms (and maybe three-way
> interaction terms), while correcting for education level and taking random
> effects into account. Is using the ti() setup the way to do this:
>
> M = gamm(behav ~ ti(age) + sex + education + ti(age, by = sex) + brain +
> ti(brain, by = age), random = list(subjectID = ~1+age), data = data)
>
> All help will be appreciated.
>
> Thanks, Louise
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bret@chr @end|ng |rom x@4@||@n|  Tue Feb 19 19:39:17 2019
From: bret@chr @end|ng |rom x@4@||@n| (bretschr)
Date: Tue, 19 Feb 2019 19:39:17 +0100
Subject: [R] character set problem
Message-ID: <7358F75D-3176-49DD-9429-D3A42A3C4036@xs4all.nl>

Dear R-users,


Last week I installed R 3.5.2 on a new MacBook Air.

I got error messages for the wrong locale (character set).
And simple math proved not to work:
Upon typing this, I got:
> 2?2
Error: unexpected input in "2?"
> 

The character visible as a caret is apparently coded as something very different.

Then I changed things according to the FAQ, chapter 7, (switching all settings to English),
and executed the recommended line:

defaults write org.R-project.R force.LANG en_US.UTF-8

The error messages disappeared, but the problem remained.

A fresh install of R 3.5.2 also didn't help: 

Here its startup messages, then a line testing the caret:

> R version 3.5.2 (2018-12-20) -- "Eggshell Igloo"
> Copyright (C) 2018 The R Foundation for Statistical Computing
> Platform: x86_64-apple-darwin15.6.0 (64-bit)
> 
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type 'license()' or 'licence()' for distribution details.
> 
> Natural language support but running in an English locale
> 
> R is a collaborative project with many contributors.
> Type 'contributors()' for more information and
> 'citation()' on how to cite R or R packages in publications.
> 
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for an HTML browser interface to help.
> Type 'q()' to quit R.
> 
> [R.app GUI 1.70 (7612) x86_64-apple-darwin15.6.0]
> 
> [History restored from /Users/fb/.Rapp.history]
> 
>> 2?2
> Error: unexpected input in "2?"
>> 



Does anyone know how to get R (R.app) to interpret a caret as a caret?

Thanks in advance,



Franklin Bretschneider
Utrecht University
Utrecht, The Netherlands


From kev|nu@hey @end|ng |rom gm@||@com  Tue Feb 19 20:16:44 2019
From: kev|nu@hey @end|ng |rom gm@||@com (Kevin Ushey)
Date: Tue, 19 Feb 2019 11:16:44 -0800
Subject: [R] [R-SIG-Mac] character set problem
In-Reply-To: <7358F75D-3176-49DD-9429-D3A42A3C4036@xs4all.nl>
References: <7358F75D-3176-49DD-9429-D3A42A3C4036@xs4all.nl>
Message-ID: <CAJXgQP3QmtjGoEwrEphw-hMi_V6zcm=RdoAXyFTUfyFA4Nkt7g@mail.gmail.com>

If I understand correctly, the problem is that the character your keyboard
is inserting is not a regular caret (^, \u0053); rather, it's a 'modifier
character circumflex accent' (?, \u02c6).

How are you inserting the carat on your laptop? For what it's worth, I get
a 'regular' caret with Shift + 6, and that particular accent character with
Alt + I.

On Tue, Feb 19, 2019 at 10:40 AM bretschr <bretschr at xs4all.nl> wrote:

> Dear R-users,
>
>
> Last week I installed R 3.5.2 on a new MacBook Air.
>
> I got error messages for the wrong locale (character set).
> And simple math proved not to work:
> Upon typing this, I got:
> > 2?2
> Error: unexpected input in "2?"
> >
>
> The character visible as a caret is apparently coded as something very
> different.
>
> Then I changed things according to the FAQ, chapter 7, (switching all
> settings to English),
> and executed the recommended line:
>
> defaults write org.R-project.R force.LANG en_US.UTF-8
>
> The error messages disappeared, but the problem remained.
>
> A fresh install of R 3.5.2 also didn't help:
>
> Here its startup messages, then a line testing the caret:
>
> > R version 3.5.2 (2018-12-20) -- "Eggshell Igloo"
> > Copyright (C) 2018 The R Foundation for Statistical Computing
> > Platform: x86_64-apple-darwin15.6.0 (64-bit)
> >
> > R is free software and comes with ABSOLUTELY NO WARRANTY.
> > You are welcome to redistribute it under certain conditions.
> > Type 'license()' or 'licence()' for distribution details.
> >
> > Natural language support but running in an English locale
> >
> > R is a collaborative project with many contributors.
> > Type 'contributors()' for more information and
> > 'citation()' on how to cite R or R packages in publications.
> >
> > Type 'demo()' for some demos, 'help()' for on-line help, or
> > 'help.start()' for an HTML browser interface to help.
> > Type 'q()' to quit R.
> >
> > [R.app GUI 1.70 (7612) x86_64-apple-darwin15.6.0]
> >
> > [History restored from /Users/fb/.Rapp.history]
> >
> >> 2?2
> > Error: unexpected input in "2?"
> >>
>
>
>
> Does anyone know how to get R (R.app) to interpret a caret as a caret?
>
> Thanks in advance,
>
>
>
> Franklin Bretschneider
> Utrecht University
> Utrecht, The Netherlands
>
> _______________________________________________
> R-SIG-Mac mailing list
> R-SIG-Mac at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-mac
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Tue Feb 19 20:36:09 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 19 Feb 2019 11:36:09 -0800
Subject: [R] character set problem
In-Reply-To: <7358F75D-3176-49DD-9429-D3A42A3C4036@xs4all.nl>
References: <7358F75D-3176-49DD-9429-D3A42A3C4036@xs4all.nl>
Message-ID: <CAGxFJbSKnyUDGJ2mNf3NBnfBn+zcAw3jexMYJyKaXwVMfuT2ag@mail.gmail.com>

You might try posting this on r-sig-mac if you don't get resolution here.

-- Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Feb 19, 2019 at 10:55 AM bretschr <bretschr at xs4all.nl> wrote:

> Dear R-users,
>
>
> Last week I installed R 3.5.2 on a new MacBook Air.
>
> I got error messages for the wrong locale (character set).
> And simple math proved not to work:
> Upon typing this, I got:
> > 2?2
> Error: unexpected input in "2?"
> >
>
> The character visible as a caret is apparently coded as something very
> different.
>
> Then I changed things according to the FAQ, chapter 7, (switching all
> settings to English),
> and executed the recommended line:
>
> defaults write org.R-project.R force.LANG en_US.UTF-8
>
> The error messages disappeared, but the problem remained.
>
> A fresh install of R 3.5.2 also didn't help:
>
> Here its startup messages, then a line testing the caret:
>
> > R version 3.5.2 (2018-12-20) -- "Eggshell Igloo"
> > Copyright (C) 2018 The R Foundation for Statistical Computing
> > Platform: x86_64-apple-darwin15.6.0 (64-bit)
> >
> > R is free software and comes with ABSOLUTELY NO WARRANTY.
> > You are welcome to redistribute it under certain conditions.
> > Type 'license()' or 'licence()' for distribution details.
> >
> > Natural language support but running in an English locale
> >
> > R is a collaborative project with many contributors.
> > Type 'contributors()' for more information and
> > 'citation()' on how to cite R or R packages in publications.
> >
> > Type 'demo()' for some demos, 'help()' for on-line help, or
> > 'help.start()' for an HTML browser interface to help.
> > Type 'q()' to quit R.
> >
> > [R.app GUI 1.70 (7612) x86_64-apple-darwin15.6.0]
> >
> > [History restored from /Users/fb/.Rapp.history]
> >
> >> 2?2
> > Error: unexpected input in "2?"
> >>
>
>
>
> Does anyone know how to get R (R.app) to interpret a caret as a caret?
>
> Thanks in advance,
>
>
>
> Franklin Bretschneider
> Utrecht University
> Utrecht, The Netherlands
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Tue Feb 19 20:37:38 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 19 Feb 2019 11:37:38 -0800
Subject: [R] character set problem
In-Reply-To: <CAGxFJbSKnyUDGJ2mNf3NBnfBn+zcAw3jexMYJyKaXwVMfuT2ag@mail.gmail.com>
References: <7358F75D-3176-49DD-9429-D3A42A3C4036@xs4all.nl>
 <CAGxFJbSKnyUDGJ2mNf3NBnfBn+zcAw3jexMYJyKaXwVMfuT2ag@mail.gmail.com>
Message-ID: <CAGxFJbS1OojpW67ym=enZQcj9M2cC0ccQjzRzqnv2020_OioFw@mail.gmail.com>

Oops, you already seem to have done this! My bad.

-- Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Feb 19, 2019 at 11:36 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> You might try posting this on r-sig-mac if you don't get resolution here.
>
> -- Bert
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Tue, Feb 19, 2019 at 10:55 AM bretschr <bretschr at xs4all.nl> wrote:
>
>> Dear R-users,
>>
>>
>> Last week I installed R 3.5.2 on a new MacBook Air.
>>
>> I got error messages for the wrong locale (character set).
>> And simple math proved not to work:
>> Upon typing this, I got:
>> > 2?2
>> Error: unexpected input in "2?"
>> >
>>
>> The character visible as a caret is apparently coded as something very
>> different.
>>
>> Then I changed things according to the FAQ, chapter 7, (switching all
>> settings to English),
>> and executed the recommended line:
>>
>> defaults write org.R-project.R force.LANG en_US.UTF-8
>>
>> The error messages disappeared, but the problem remained.
>>
>> A fresh install of R 3.5.2 also didn't help:
>>
>> Here its startup messages, then a line testing the caret:
>>
>> > R version 3.5.2 (2018-12-20) -- "Eggshell Igloo"
>> > Copyright (C) 2018 The R Foundation for Statistical Computing
>> > Platform: x86_64-apple-darwin15.6.0 (64-bit)
>> >
>> > R is free software and comes with ABSOLUTELY NO WARRANTY.
>> > You are welcome to redistribute it under certain conditions.
>> > Type 'license()' or 'licence()' for distribution details.
>> >
>> > Natural language support but running in an English locale
>> >
>> > R is a collaborative project with many contributors.
>> > Type 'contributors()' for more information and
>> > 'citation()' on how to cite R or R packages in publications.
>> >
>> > Type 'demo()' for some demos, 'help()' for on-line help, or
>> > 'help.start()' for an HTML browser interface to help.
>> > Type 'q()' to quit R.
>> >
>> > [R.app GUI 1.70 (7612) x86_64-apple-darwin15.6.0]
>> >
>> > [History restored from /Users/fb/.Rapp.history]
>> >
>> >> 2?2
>> > Error: unexpected input in "2?"
>> >>
>>
>>
>>
>> Does anyone know how to get R (R.app) to interpret a caret as a caret?
>>
>> Thanks in advance,
>>
>>
>>
>> Franklin Bretschneider
>> Utrecht University
>> Utrecht, The Netherlands
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From bret@chr @end|ng |rom x@4@||@n|  Tue Feb 19 20:58:39 2019
From: bret@chr @end|ng |rom x@4@||@n| (bretschr)
Date: Tue, 19 Feb 2019 20:58:39 +0100
Subject: [R] [R-SIG-Mac] character set problem
In-Reply-To: <CAJXgQP3QmtjGoEwrEphw-hMi_V6zcm=RdoAXyFTUfyFA4Nkt7g@mail.gmail.com>
References: <7358F75D-3176-49DD-9429-D3A42A3C4036@xs4all.nl>
 <CAJXgQP3QmtjGoEwrEphw-hMi_V6zcm=RdoAXyFTUfyFA4Nkt7g@mail.gmail.com>
Message-ID: <E9D6F747-B3FA-48F6-B0F2-67F75FFCD60D@xs4all.nl>

Dear Kevin Ushey,

Re:

> On 19 Feb 2019, at 20:16, Kevin Ushey <kevinushey at gmail.com> wrote:
> 
> If I understand correctly, the problem is that the character your keyboard is inserting is not a regular caret (^, \u0053); rather, it's a 'modifier character circumflex accent' (?, \u02c6).
> 
> How are you inserting the carat on your laptop? For what it's worth, I get a 'regular' caret with Shift + 6, and that particular accent character with Alt + I.


Thanks.  Indeed I just type the normal shift-6, and get this strange circumflex character. With alt-i I get this same character.

I can produce a normal caret by typing 

rawToChar(as.raw(94))

in the console, but that's of course rather clumsy.

How to "tame" the editor to produce the normal ASCII caret?
The other symbols in that row, like % and &, are the normal ASCII characters.
Strange why this single character on my keyboard is a two-byte character.

Any clue appreciated.


Franklin




Franklin Bretschneider
Utrecht University
Utrecht, The Netherlands


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue Feb 19 20:59:24 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 19 Feb 2019 11:59:24 -0800
Subject: [R] Save creates huge files, dump doesn't
In-Reply-To: <2feb543facba484e5d1e10abcc62a149@webmail.embl.de>
References: <2feb543facba484e5d1e10abcc62a149@webmail.embl.de>
Message-ID: <DF23AC8D-5EB1-4E1B-8FDD-099C77C89662@dcn.davis.ca.us>

Make a reproducible example that focuses on the save/load aspect of the size problem. You may need to experiment with which variables need to be in the save file in order to trigger the behavior. Your example might have to involve sending us a link to a large file, but that size may dissuade busy experts from tackling it so paring it down by experimentation could be in your best interest.

There is some expected behavior that can lead to larger files than the original in-memory data, but offhand I am unaware of any explanation for those files then using less space when re-loaded into memory than they occupy on disk.

On February 18, 2019 11:51:11 AM PST, Lars Velten <lars.velten at embl.de> wrote:
>Dear list,
>I noticed an extremely odd behavior... I have a rather complex shiny
>app which allows the user to store his/her state which internally
>obviously triggers as call to save as follows
>save(list=c("plots","gates","populations","cg",
>"genelists","colorscores",  "proj", "actds"),file=fname)
>this was all working fine until some time ago (?!?) files created by
>this command became several hundred MBs big... even thought the
>cumulative size of all objects in memory after load() is in the 10s of
>kB. 
>Changing to
>dump(list=c("plots","gates","populations","cg",
>"genelists","colorscores",  "proj", "actds"),file=fname)
>solved the problem, output was then only 10s of kB.
>(Why/when) is this behavior intended?
>Best wishes,
>Lars
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From wdun|@p @end|ng |rom t|bco@com  Tue Feb 19 21:30:01 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Tue, 19 Feb 2019 12:30:01 -0800
Subject: [R] Save creates huge files, dump doesn't
In-Reply-To: <DF23AC8D-5EB1-4E1B-8FDD-099C77C89662@dcn.davis.ca.us>
References: <2feb543facba484e5d1e10abcc62a149@webmail.embl.de>
 <DF23AC8D-5EB1-4E1B-8FDD-099C77C89662@dcn.davis.ca.us>
Message-ID: <CAF8bMcbxPTCVukHipm7LtFzF0cXnFtobbAY5YDHrgznK2LOOzA@mail.gmail.com>

One reason save() makes bigger files than dump() is that save() saves
environments associated with functions that are saved and those
environments may contain large datasets that are not really needed.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Tue, Feb 19, 2019 at 11:59 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Make a reproducible example that focuses on the save/load aspect of the
> size problem. You may need to experiment with which variables need to be in
> the save file in order to trigger the behavior. Your example might have to
> involve sending us a link to a large file, but that size may dissuade busy
> experts from tackling it so paring it down by experimentation could be in
> your best interest.
>
> There is some expected behavior that can lead to larger files than the
> original in-memory data, but offhand I am unaware of any explanation for
> those files then using less space when re-loaded into memory than they
> occupy on disk.
>
> On February 18, 2019 11:51:11 AM PST, Lars Velten <lars.velten at embl.de>
> wrote:
> >Dear list,
> >I noticed an extremely odd behavior... I have a rather complex shiny
> >app which allows the user to store his/her state which internally
> >obviously triggers as call to save as follows
> >save(list=c("plots","gates","populations","cg",
> >"genelists","colorscores",  "proj", "actds"),file=fname)
> >this was all working fine until some time ago (?!?) files created by
> >this command became several hundred MBs big... even thought the
> >cumulative size of all objects in memory after load() is in the 10s of
> >kB.
> >Changing to
> >dump(list=c("plots","gates","populations","cg",
> >"genelists","colorscores",  "proj", "actds"),file=fname)
> >solved the problem, output was then only 10s of kB.
> >(Why/when) is this behavior intended?
> >Best wishes,
> >Lars
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dr@ke@go@@| @end|ng |rom gm@||@com  Tue Feb 19 21:30:20 2019
From: dr@ke@go@@| @end|ng |rom gm@||@com (Drake Gossi)
Date: Tue, 19 Feb 2019 12:30:20 -0800
Subject: [R] help getting a research project started on regulations.gov
Message-ID: <CAPSTy5dbH416rts4i5wUprkfOh7OYjz6+bGhvJcRAfHSq=dT6w@mail.gmail.com>

Hello everyone,

I will be using R to manipulate this data
<https://www.regulations.gov/docketBrowser?rpp=25&so=DESC&sb=commentDueDate&po=0&dct=PS&D=ED-2018-OCR-0064>.
Specifically, it's proposed changes to Title IX--over 11,000 publicly
available comments. So, the end goal is for me to tabulate each of these
11,000 comments in a csv file, so I can begin to manipulate and visualize
the data.

But I'm not there yet. I just put in for an API key and, while I have one,
I'm waiting for it to be activated. After that, though, I'm a little lost.
Do I need to scrape the comments from the site? Or does having the API
render that unnecessary? There is this interface
<https://regulationsgov.github.io/developers/console/> that works with the
API, but I don't know if, though it, I can get the data I need. I'm still
trying to figure out what JSON is.

Or, if I have to scrape the comments, can I do that with R? I can't get a
straight answer from the python people. I can't tell if I need to do this
through beautiful soup or through scrapy (or even if I need to do it at
all, as I said...). The trouble with the comments is, they are each on
their own URL, so--and again this is assuming that I will have to scrape
them--I don't know how to code in order to grab all of the comments from
all of the URLs.

I also am trying to figure out how to isolate the essence of the comments
in the html. From the python people, I've heard the following:

scrapy fetch 'url'
will download the raw page you are interested in. And you can look at
the raw source code. Important to appreciate that what you see in the
browser is often processed in your browser before you see it.

Of course, a scraper can do the same processing, but it's complicated.
So, start by looking at the raw source code. Maybe you can grab what you
need with simple parsing like Beautiful Soup does. Maybe you need to do
more. Scrapy is your friend.

 Beautiful soup is your friend here.  It can analyze the data within
the html tags on your scraped page. But often javascript is used on
'modern' web pages so the page is actually not just html, but
javascript that changes the html.  For this you need another tool -- i
think one is called scrapy.  Others here probably have experience with
that.

I think part of my problem relates to that yellow part. I was saying things
like

I think what I might be looking for is a div  class = GIY1LSJIXD, since
that's where the hierarchy seems to taper off in the html for the comment
I'm looking to scrape.


What I'm trying to do here is, locate the comment in the html so I can tell
the request function to extract it.

Any help anyone could offer here would be much appreciated. I'm very lost.

Drake

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Wed Feb 20 00:16:28 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 19 Feb 2019 15:16:28 -0800
Subject: [R] help getting a research project started on regulations.gov
In-Reply-To: <CAPSTy5dbH416rts4i5wUprkfOh7OYjz6+bGhvJcRAfHSq=dT6w@mail.gmail.com>
References: <CAPSTy5dbH416rts4i5wUprkfOh7OYjz6+bGhvJcRAfHSq=dT6w@mail.gmail.com>
Message-ID: <CAGxFJbTGAGDiJgpQa54XFQWQrCvfYH8zin6cyYyFAmA7otfzXA@mail.gmail.com>

Please search yourself first!

"scrape JSON from web" at the rseek.org site produced what appeared to be
several relevant hits,
especially this CRAN task view:
https://cran.r-project.org/web/views/WebTechnologies.html


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Feb 19, 2019 at 3:07 PM Drake Gossi <drake.gossi at gmail.com> wrote:

> Hello everyone,
>
> I will be using R to manipulate this data
> <
> https://www.regulations.gov/docketBrowser?rpp=25&so=DESC&sb=commentDueDate&po=0&dct=PS&D=ED-2018-OCR-0064
> >.
> Specifically, it's proposed changes to Title IX--over 11,000 publicly
> available comments. So, the end goal is for me to tabulate each of these
> 11,000 comments in a csv file, so I can begin to manipulate and visualize
> the data.
>
> But I'm not there yet. I just put in for an API key and, while I have one,
> I'm waiting for it to be activated. After that, though, I'm a little lost.
> Do I need to scrape the comments from the site? Or does having the API
> render that unnecessary? There is this interface
> <https://regulationsgov.github.io/developers/console/> that works with the
> API, but I don't know if, though it, I can get the data I need. I'm still
> trying to figure out what JSON is.
>
> Or, if I have to scrape the comments, can I do that with R? I can't get a
> straight answer from the python people. I can't tell if I need to do this
> through beautiful soup or through scrapy (or even if I need to do it at
> all, as I said...). The trouble with the comments is, they are each on
> their own URL, so--and again this is assuming that I will have to scrape
> them--I don't know how to code in order to grab all of the comments from
> all of the URLs.
>
> I also am trying to figure out how to isolate the essence of the comments
> in the html. From the python people, I've heard the following:
>
> scrapy fetch 'url'
> will download the raw page you are interested in. And you can look at
> the raw source code. Important to appreciate that what you see in the
> browser is often processed in your browser before you see it.
>
> Of course, a scraper can do the same processing, but it's complicated.
> So, start by looking at the raw source code. Maybe you can grab what you
> need with simple parsing like Beautiful Soup does. Maybe you need to do
> more. Scrapy is your friend.
>
>  Beautiful soup is your friend here.  It can analyze the data within
> the html tags on your scraped page. But often javascript is used on
> 'modern' web pages so the page is actually not just html, but
> javascript that changes the html.  For this you need another tool -- i
> think one is called scrapy.  Others here probably have experience with
> that.
>
> I think part of my problem relates to that yellow part. I was saying things
> like
>
> I think what I might be looking for is a div  class = GIY1LSJIXD, since
> that's where the hierarchy seems to taper off in the html for the comment
> I'm looking to scrape.
>
>
> What I'm trying to do here is, locate the comment in the html so I can tell
> the request function to extract it.
>
> Any help anyone could offer here would be much appreciated. I'm very lost.
>
> Drake
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bret@chr @end|ng |rom x@4@||@n|  Wed Feb 20 09:29:48 2019
From: bret@chr @end|ng |rom x@4@||@n| (bretschr)
Date: Wed, 20 Feb 2019 09:29:48 +0100
Subject: [R] [R-SIG-Mac] character set problem
In-Reply-To: <0fec0584-d68e-5262-e462-9524d07d0c65@kit.edu>
References: <7358F75D-3176-49DD-9429-D3A42A3C4036@xs4all.nl>
 <CAJXgQP3QmtjGoEwrEphw-hMi_V6zcm=RdoAXyFTUfyFA4Nkt7g@mail.gmail.com>
 <E9D6F747-B3FA-48F6-B0F2-67F75FFCD60D@xs4all.nl>
 <7a4d37e3-5161-01f0-e6df-6bc814a6100b@kit.edu>
 <0fec0584-d68e-5262-e462-9524d07d0c65@kit.edu>
Message-ID: <B4D69D43-2162-4499-A0DB-B69A10E3BC1C@xs4all.nl>

Dear Peter Anthoni,


Re:

> On 20 Feb 2019, at 07:22, Peter Anthoni <peter.anthoni at kit.edu> wrote:
> 
> is your keyboard type US. International -PC, that will insert the weird caret.
> 
> <bfmoeahenojamlch.png>
> 
> cheers
> 
> Peter



That 's it!!!  What I thought to be the most universal keyboard proved to be the culprit.
Changed to simple "US" (SystemPreferences, Language and region, Keyboard preferences) and all is OK.
Thanks a lot.
With best regards,

Franklin
- - - - - 


Franklin Bretschneider
Utrecht University
Utrecht, The Netherlands

From petr@p|k@| @end|ng |rom prechez@@cz  Wed Feb 20 12:16:17 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Wed, 20 Feb 2019 11:16:17 +0000
Subject: [R] particle count probability
Message-ID: <bba66e7514c34d7ca772eefc40871a06@SRVEXCHCM1302.precheza.cz>

Dear all

Sorry, this is probably the most off-topic mail I have ever sent to this help list. However maybe somebody could point me to right direction or give some advice.

In microscopy particle counting you have finite viewing field and some particles could be partly outside of this field. My problem/question is:

Do bigger particles have also bigger probability that they will be partly outside this viewing field than smaller ones?

Saying it differently, although there is equal count of bigger (white) and smaller (black) particles in enclosed picture (8), due to the fact that more bigger particles are on the edge I count more small particles (6) than big (4).

Is it possible to evaluate this feature exactly i.e. calculate some bias towards smaller particles based on particle size distribution, mean particle size and/or image magnification?

Best regards
Petr Pikal
Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner's personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


-------------- next part --------------
A non-text attachment was scrubbed...
Name: particle.pdf
Type: application/pdf
Size: 43446 bytes
Desc: particle.pdf
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190220/59b4dc10/attachment.pdf>

From rkoenker @end|ng |rom ||||no|@@edu  Wed Feb 20 12:32:34 2019
From: rkoenker @end|ng |rom ||||no|@@edu (Roger Koenker)
Date: Wed, 20 Feb 2019 11:32:34 +0000
Subject: [R] particle count probability
In-Reply-To: <583b90075489448795e8b4562f744879@BYAPR11MB3141.namprd11.prod.outlook.com>
References: <583b90075489448795e8b4562f744879@BYAPR11MB3141.namprd11.prod.outlook.com>
Message-ID: <8E122956-EC6A-4A0E-B3E4-8D942890DF0F@illinois.edu>


Somewhere, buried in the vast literature on the Wicksell problem, there is probably an answer, or at least a hint.

> On Feb 20, 2019, at 11:16 AM, PIKAL Petr <petr.pikal at precheza.cz> wrote:
> 
> Dear all
> 
> Sorry, this is probably the most off-topic mail I have ever sent to this help list. However maybe somebody could point me to right direction or give some advice.
> 
> In microscopy particle counting you have finite viewing field and some particles could be partly outside of this field. My problem/question is:
> 
> Do bigger particles have also bigger probability that they will be partly outside this viewing field than smaller ones?
> 
> Saying it differently, although there is equal count of bigger (white) and smaller (black) particles in enclosed picture (8), due to the fact that more bigger particles are on the edge I count more small particles (6) than big (4).
> 
> Is it possible to evaluate this feature exactly i.e. calculate some bias towards smaller particles based on particle size distribution, mean particle size and/or image magnification?
> 
> Best regards
> Petr Pikal
> Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner's personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
> D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/
> 
> <particle.pdf>______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wdun|@p @end|ng |rom t|bco@com  Wed Feb 20 16:47:39 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Wed, 20 Feb 2019 07:47:39 -0800
Subject: [R] Save creates huge files, dump doesn't
In-Reply-To: <05DA66FD-04D9-493E-B797-3E0DD8DAD6CD@embl.de>
References: <2feb543facba484e5d1e10abcc62a149@webmail.embl.de>
 <DF23AC8D-5EB1-4E1B-8FDD-099C77C89662@dcn.davis.ca.us>
 <CAF8bMcbxPTCVukHipm7LtFzF0cXnFtobbAY5YDHrgznK2LOOzA@mail.gmail.com>
 <05DA66FD-04D9-493E-B797-3E0DD8DAD6CD@embl.de>
Message-ID: <CAF8bMcawj8nk4sfxTQq39KxRmOcN8=R2dkP8rf1KFA0Z7wVyNQ@mail.gmail.com>

object at transforms@transforms$PC1.all at f
function(x) x
<environment: 0x3314db8>
Do you know how to 'see' what's in 0x3314db8 ?

ls.str(all=TRUE, environment(object at transforms@transforms$PC1.all at f)

will list the names, types, summaries, etc. of the objects in that
environment.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Wed, Feb 20, 2019 at 12:20 AM Lars Velten <lars.velten at embl.de> wrote:
>
> Dear Bill, dear all,
>
> yes that seems to be it.  The problem orginates from objects of class
transformMap from package flowCore
>
> > object_size(object at transforms@transforms$PC1.all at f)
> 174 MB
> > object.size(object at transforms@transforms$PC1.all at f)
> 1160 bytes
>
> object at transforms@transforms$PC1.all at f
>
> function(x) x
> <environment: 0x3314db8>
>
> Do you know how to 'see' what's in 0x3314db8 ? Might then drop a line to
flowCore's developer, this behavior cannot be intended - especially here
where f literally is just identity :-)
>
> Best wishes,
>
> Lars
>
> On 19. Feb 2019, at 21:30, William Dunlap <wdunlap at tibco.com> wrote:
>
> One reason save() makes bigger files than dump() is that save() saves
environments associated with functions that are saved and those
environments may contain large datasets that are not really needed.
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>
> On Tue, Feb 19, 2019 at 11:59 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:
>>
>> Make a reproducible example that focuses on the save/load aspect of the
size problem. You may need to experiment with which variables need to be in
the save file in order to trigger the behavior. Your example might have to
involve sending us a link to a large file, but that size may dissuade busy
experts from tackling it so paring it down by experimentation could be in
your best interest.
>>
>> There is some expected behavior that can lead to larger files than the
original in-memory data, but offhand I am unaware of any explanation for
those files then using less space when re-loaded into memory than they
occupy on disk.
>>
>> On February 18, 2019 11:51:11 AM PST, Lars Velten <lars.velten at embl.de>
wrote:
>> >Dear list,
>> >I noticed an extremely odd behavior... I have a rather complex shiny
>> >app which allows the user to store his/her state which internally
>> >obviously triggers as call to save as follows
>> >save(list=c("plots","gates","populations","cg",
>> >"genelists","colorscores",  "proj", "actds"),file=fname)
>> >this was all working fine until some time ago (?!?) files created by
>> >this command became several hundred MBs big... even thought the
>> >cumulative size of all objects in memory after load() is in the 10s of
>> >kB.
>> >Changing to
>> >dump(list=c("plots","gates","populations","cg",
>> >"genelists","colorscores",  "proj", "actds"),file=fname)
>> >solved the problem, output was then only 10s of kB.
>> >(Why/when) is this behavior intended?
>> >Best wishes,
>> >Lars
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> >______________________________________________
>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >PLEASE do read the posting guide
>> >http://www.R-project.org/posting-guide.html
>> >and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Sent from my phone. Please excuse my brevity.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From wdun|@p @end|ng |rom t|bco@com  Wed Feb 20 16:50:14 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Wed, 20 Feb 2019 07:50:14 -0800
Subject: [R] Save creates huge files, dump doesn't
In-Reply-To: <CAF8bMcawj8nk4sfxTQq39KxRmOcN8=R2dkP8rf1KFA0Z7wVyNQ@mail.gmail.com>
References: <2feb543facba484e5d1e10abcc62a149@webmail.embl.de>
 <DF23AC8D-5EB1-4E1B-8FDD-099C77C89662@dcn.davis.ca.us>
 <CAF8bMcbxPTCVukHipm7LtFzF0cXnFtobbAY5YDHrgznK2LOOzA@mail.gmail.com>
 <05DA66FD-04D9-493E-B797-3E0DD8DAD6CD@embl.de>
 <CAF8bMcawj8nk4sfxTQq39KxRmOcN8=R2dkP8rf1KFA0Z7wVyNQ@mail.gmail.com>
Message-ID: <CAF8bMcZdJ7jsGUifE87P7+sLkn5DsRZRywVE5-Sir=1Hh+mBiA@mail.gmail.com>

Also, note that the function
   function(x) x
   <environment: 0x3314db8>
has no free variables so it doesn't matter what environment encloses it.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Wed, Feb 20, 2019 at 7:47 AM William Dunlap <wdunlap at tibco.com> wrote:

> object at transforms@transforms$PC1.all at f
> function(x) x
> <environment: 0x3314db8>
> Do you know how to 'see' what's in 0x3314db8 ?
>
> ls.str(all=TRUE, environment(object at transforms@transforms$PC1.all at f)
>
> will list the names, types, summaries, etc. of the objects in that
> environment.
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>
> On Wed, Feb 20, 2019 at 12:20 AM Lars Velten <lars.velten at embl.de> wrote:
> >
> > Dear Bill, dear all,
> >
> > yes that seems to be it.  The problem orginates from objects of class
> transformMap from package flowCore
> >
> > > object_size(object at transforms@transforms$PC1.all at f)
> > 174 MB
> > > object.size(object at transforms@transforms$PC1.all at f)
> > 1160 bytes
> >
> > object at transforms@transforms$PC1.all at f
> >
> > function(x) x
> > <environment: 0x3314db8>
> >
> > Do you know how to 'see' what's in 0x3314db8 ? Might then drop a line to
> flowCore's developer, this behavior cannot be intended - especially here
> where f literally is just identity :-)
> >
> > Best wishes,
> >
> > Lars
> >
> > On 19. Feb 2019, at 21:30, William Dunlap <wdunlap at tibco.com> wrote:
> >
> > One reason save() makes bigger files than dump() is that save() saves
> environments associated with functions that are saved and those
> environments may contain large datasets that are not really needed.
> >
> > Bill Dunlap
> > TIBCO Software
> > wdunlap tibco.com
> >
> >
> > On Tue, Feb 19, 2019 at 11:59 AM Jeff Newmiller <
> jdnewmil at dcn.davis.ca.us> wrote:
> >>
> >> Make a reproducible example that focuses on the save/load aspect of the
> size problem. You may need to experiment with which variables need to be in
> the save file in order to trigger the behavior. Your example might have to
> involve sending us a link to a large file, but that size may dissuade busy
> experts from tackling it so paring it down by experimentation could be in
> your best interest.
> >>
> >> There is some expected behavior that can lead to larger files than the
> original in-memory data, but offhand I am unaware of any explanation for
> those files then using less space when re-loaded into memory than they
> occupy on disk.
> >>
> >> On February 18, 2019 11:51:11 AM PST, Lars Velten <lars.velten at embl.de>
> wrote:
> >> >Dear list,
> >> >I noticed an extremely odd behavior... I have a rather complex shiny
> >> >app which allows the user to store his/her state which internally
> >> >obviously triggers as call to save as follows
> >> >save(list=c("plots","gates","populations","cg",
> >> >"genelists","colorscores",  "proj", "actds"),file=fname)
> >> >this was all working fine until some time ago (?!?) files created by
> >> >this command became several hundred MBs big... even thought the
> >> >cumulative size of all objects in memory after load() is in the 10s of
> >> >kB.
> >> >Changing to
> >> >dump(list=c("plots","gates","populations","cg",
> >> >"genelists","colorscores",  "proj", "actds"),file=fname)
> >> >solved the problem, output was then only 10s of kB.
> >> >(Why/when) is this behavior intended?
> >> >Best wishes,
> >> >Lars
> >> >
> >> >       [[alternative HTML version deleted]]
> >> >
> >> >______________________________________________
> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >https://stat.ethz.ch/mailman/listinfo/r-help
> >> >PLEASE do read the posting guide
> >> >http://www.R-project.org/posting-guide.html
> >> >and provide commented, minimal, self-contained, reproducible code.
> >>
> >> --
> >> Sent from my phone. Please excuse my brevity.
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From |@r@@ve|ten @end|ng |rom emb|@de  Wed Feb 20 09:20:40 2019
From: |@r@@ve|ten @end|ng |rom emb|@de (Lars Velten)
Date: Wed, 20 Feb 2019 09:20:40 +0100
Subject: [R] Save creates huge files, dump doesn't
In-Reply-To: <CAF8bMcbxPTCVukHipm7LtFzF0cXnFtobbAY5YDHrgznK2LOOzA@mail.gmail.com>
References: <2feb543facba484e5d1e10abcc62a149@webmail.embl.de>
 <DF23AC8D-5EB1-4E1B-8FDD-099C77C89662@dcn.davis.ca.us>
 <CAF8bMcbxPTCVukHipm7LtFzF0cXnFtobbAY5YDHrgznK2LOOzA@mail.gmail.com>
Message-ID: <05DA66FD-04D9-493E-B797-3E0DD8DAD6CD@embl.de>

Dear Bill, dear all,

yes that seems to be it.  The problem orginates from objects of class transformMap from package flowCore

> object_size(object at transforms@transforms$PC1.all at f)
174 MB
> object.size(object at transforms@transforms$PC1.all at f)
1160 bytes

object at transforms@transforms$PC1.all at f

function(x) x
<environment: 0x3314db8>

Do you know how to 'see' what's in 0x3314db8 ? Might then drop a line to flowCore's developer, this behavior cannot be intended - especially here where f literally is just identity :-)

Best wishes,

Lars

> On 19. Feb 2019, at 21:30, William Dunlap <wdunlap at tibco.com> wrote:
> 
> One reason save() makes bigger files than dump() is that save() saves environments associated with functions that are saved and those environments may contain large datasets that are not really needed.
> 
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com <http://tibco.com/>
> 
> On Tue, Feb 19, 2019 at 11:59 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us <mailto:jdnewmil at dcn.davis.ca.us>> wrote:
> Make a reproducible example that focuses on the save/load aspect of the size problem. You may need to experiment with which variables need to be in the save file in order to trigger the behavior. Your example might have to involve sending us a link to a large file, but that size may dissuade busy experts from tackling it so paring it down by experimentation could be in your best interest.
> 
> There is some expected behavior that can lead to larger files than the original in-memory data, but offhand I am unaware of any explanation for those files then using less space when re-loaded into memory than they occupy on disk.
> 
> On February 18, 2019 11:51:11 AM PST, Lars Velten <lars.velten at embl.de <mailto:lars.velten at embl.de>> wrote:
> >Dear list,
> >I noticed an extremely odd behavior... I have a rather complex shiny
> >app which allows the user to store his/her state which internally
> >obviously triggers as call to save as follows
> >save(list=c("plots","gates","populations","cg",
> >"genelists","colorscores",  "proj", "actds"),file=fname)
> >this was all working fine until some time ago (?!?) files created by
> >this command became several hundred MBs big... even thought the
> >cumulative size of all objects in memory after load() is in the 10s of
> >kB. 
> >Changing to
> >dump(list=c("plots","gates","populations","cg",
> >"genelists","colorscores",  "proj", "actds"),file=fname)
> >solved the problem, output was then only 10s of kB.
> >(Why/when) is this behavior intended?
> >Best wishes,
> >Lars
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org <mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help <https://stat.ethz.ch/mailman/listinfo/r-help>
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html <http://www.r-project.org/posting-guide.html>
> >and provide commented, minimal, self-contained, reproducible code.
> 
> -- 
> Sent from my phone. Please excuse my brevity.
> 
> ______________________________________________
> R-help at r-project.org <mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help <https://stat.ethz.ch/mailman/listinfo/r-help>
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html <http://www.r-project.org/posting-guide.html>
> and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From r|@h|@d@@roy @end|ng |rom gm@||@com  Wed Feb 20 10:23:53 2019
From: r|@h|@d@@roy @end|ng |rom gm@||@com (=?UTF-8?B?4KaL4Ka34Ka/ICAoIOCki+Ckt+CkvyAvIHJJc0hpICk=?=)
Date: Wed, 20 Feb 2019 11:23:53 +0200
Subject: [R] matrix subset problem with factors
Message-ID: <CAODjwAG8+iPFEB26CsiJ7T2+of1jymZF1A1WGBtBzez7aEZ9pg@mail.gmail.com>

Hi All,

I like to report this bug related to matrix subset by rownames when passed
as factors. Now factors are may not be safe to use but then it should
generate a warning message. Since many time we use values returned by some
packages as factor to subset a matrix and which may result in a wrong
calculation.

I wish if "factor" is not expected in matrix operation then it should throw
an error/warning message.

Below are the codes to reproduce it.

> x <- matrix(1:9, nrow = 3, dimnames = list(c("X","Y","Z"),
c("A","B","C")))
>
> rNames <- as.factor(c("X","Z"))
> # As some functions from different packages return factors and which
could be overlooked
> rNames
[1] X Z
Levels: X Z
>
> x[rNames,]
  A B C
X 1 4 7
Y 2 5 8
>
> ## The intended matrix should return X and Z rows instead of X and Y
>
> sessionInfo()
R version 3.4.1 (2017-06-30)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 14.04.5 LTS

Matrix products: default
BLAS: /usr/lib/atlas-base/atlas/libblas.so.3.0
LAPACK: /usr/lib/lapack/liblapack.so.3.0

locale:
 [1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_GB.UTF-8        LC_COLLATE=en_GB.UTF-8
 [5] LC_MONETARY=en_GB.UTF-8    LC_MESSAGES=en_GB.UTF-8
 [7] LC_PAPER=en_GB.UTF-8       LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] compiler_3.4.1
>





-- 



With regards
Rishi Das Roy

	[[alternative HTML version deleted]]


From m@rc_@chw@rtz @end|ng |rom me@com  Wed Feb 20 21:59:54 2019
From: m@rc_@chw@rtz @end|ng |rom me@com (Marc Schwartz)
Date: Wed, 20 Feb 2019 15:59:54 -0500
Subject: [R] matrix subset problem with factors
In-Reply-To: <CAODjwAG8+iPFEB26CsiJ7T2+of1jymZF1A1WGBtBzez7aEZ9pg@mail.gmail.com>
References: <CAODjwAG8+iPFEB26CsiJ7T2+of1jymZF1A1WGBtBzez7aEZ9pg@mail.gmail.com>
Message-ID: <EF6FC8B9-B828-4C54-81CE-7004C5AB7024@me.com>

Hi,

I get the same behavior in R 3.5.2 on macOS.

Others may feel differently, but I am not so sure that this is a bug, as opposed to perhaps the need to clarify in ?Extract, that the following, which is found under Atomic vectors:

"The index object i can be numeric, logical, character or empty. Indexing by factors is allowed and is equivalent to indexing by the numeric codes (see factor) and not by the character values which are printed (for which use [as.character(i)])."

also applies to the indexing of matrices and arrays.

Since matrices and arrays in R are vectors with 'dim' attributes, the behavior is essentially consistent as described above.

Thus, perhaps just add the second sentence above or similar wording to the section for Matrices and arrays.

Regards,

Marc Schwartz

> On Feb 20, 2019, at 4:23 AM, ??? ( ??? / rIsHi ) <rishi.dasroy at gmail.com> wrote:
> 
> Hi All,
> 
> I like to report this bug related to matrix subset by rownames when passed
> as factors. Now factors are may not be safe to use but then it should
> generate a warning message. Since many time we use values returned by some
> packages as factor to subset a matrix and which may result in a wrong
> calculation.
> 
> I wish if "factor" is not expected in matrix operation then it should throw
> an error/warning message.
> 
> Below are the codes to reproduce it.
> 
>> x <- matrix(1:9, nrow = 3, dimnames = list(c("X","Y","Z"),
> c("A","B","C")))
>> 
>> rNames <- as.factor(c("X","Z"))
>> # As some functions from different packages return factors and which
> could be overlooked
>> rNames
> [1] X Z
> Levels: X Z
>> 
>> x[rNames,]
>  A B C
> X 1 4 7
> Y 2 5 8
>> 
>> ## The intended matrix should return X and Z rows instead of X and Y
>> 
>> sessionInfo()
> R version 3.4.1 (2017-06-30)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Ubuntu 14.04.5 LTS
> 
> Matrix products: default
> BLAS: /usr/lib/atlas-base/atlas/libblas.so.3.0
> LAPACK: /usr/lib/lapack/liblapack.so.3.0
> 
> locale:
> [1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C
> [3] LC_TIME=en_GB.UTF-8        LC_COLLATE=en_GB.UTF-8
> [5] LC_MONETARY=en_GB.UTF-8    LC_MESSAGES=en_GB.UTF-8
> [7] LC_PAPER=en_GB.UTF-8       LC_NAME=C
> [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> loaded via a namespace (and not attached):
> [1] compiler_3.4.1
>> 
> 
> 
> 
> With regards
> Rishi Das Roy


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Feb 20 22:38:01 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 20 Feb 2019 13:38:01 -0800
Subject: [R] matrix subset problem with factors
In-Reply-To: <EF6FC8B9-B828-4C54-81CE-7004C5AB7024@me.com>
References: <CAODjwAG8+iPFEB26CsiJ7T2+of1jymZF1A1WGBtBzez7aEZ9pg@mail.gmail.com>
 <EF6FC8B9-B828-4C54-81CE-7004C5AB7024@me.com>
Message-ID: <519D33F2-B5CB-413E-A266-5E9551A4EB9B@dcn.davis.ca.us>

With on official weight, I second the opinion that the existing behavior is appropriate and not a bug.

Functions should not "unexpectedly" return factors... a common example are the read.table family of functions that by default return factors, but the behaviour is deterministic and controllable with the as.is or stringsAsFactors arguments. If you have functions that randomly return different types then the bug is in those functions.

Don't confuse factors and character data types... they are distinct and used for different purposes.

On February 20, 2019 12:59:54 PM PST, Marc Schwartz via R-help <r-help at r-project.org> wrote:
>Hi,
>
>I get the same behavior in R 3.5.2 on macOS.
>
>Others may feel differently, but I am not so sure that this is a bug,
>as opposed to perhaps the need to clarify in ?Extract, that the
>following, which is found under Atomic vectors:
>
>"The index object i can be numeric, logical, character or empty.
>Indexing by factors is allowed and is equivalent to indexing by the
>numeric codes (see factor) and not by the character values which are
>printed (for which use [as.character(i)])."
>
>also applies to the indexing of matrices and arrays.
>
>Since matrices and arrays in R are vectors with 'dim' attributes, the
>behavior is essentially consistent as described above.
>
>Thus, perhaps just add the second sentence above or similar wording to
>the section for Matrices and arrays.
>
>Regards,
>
>Marc Schwartz
>
>> On Feb 20, 2019, at 4:23 AM, ??? ( ??? / rIsHi )
><rishi.dasroy at gmail.com> wrote:
>> 
>> Hi All,
>> 
>> I like to report this bug related to matrix subset by rownames when
>passed
>> as factors. Now factors are may not be safe to use but then it should
>> generate a warning message. Since many time we use values returned by
>some
>> packages as factor to subset a matrix and which may result in a wrong
>> calculation.
>> 
>> I wish if "factor" is not expected in matrix operation then it should
>throw
>> an error/warning message.
>> 
>> Below are the codes to reproduce it.
>> 
>>> x <- matrix(1:9, nrow = 3, dimnames = list(c("X","Y","Z"),
>> c("A","B","C")))
>>> 
>>> rNames <- as.factor(c("X","Z"))
>>> # As some functions from different packages return factors and which
>> could be overlooked
>>> rNames
>> [1] X Z
>> Levels: X Z
>>> 
>>> x[rNames,]
>>  A B C
>> X 1 4 7
>> Y 2 5 8
>>> 
>>> ## The intended matrix should return X and Z rows instead of X and Y
>>> 
>>> sessionInfo()
>> R version 3.4.1 (2017-06-30)
>> Platform: x86_64-pc-linux-gnu (64-bit)
>> Running under: Ubuntu 14.04.5 LTS
>> 
>> Matrix products: default
>> BLAS: /usr/lib/atlas-base/atlas/libblas.so.3.0
>> LAPACK: /usr/lib/lapack/liblapack.so.3.0
>> 
>> locale:
>> [1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C
>> [3] LC_TIME=en_GB.UTF-8        LC_COLLATE=en_GB.UTF-8
>> [5] LC_MONETARY=en_GB.UTF-8    LC_MESSAGES=en_GB.UTF-8
>> [7] LC_PAPER=en_GB.UTF-8       LC_NAME=C
>> [9] LC_ADDRESS=C               LC_TELEPHONE=C
>> [11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C
>> 
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>> 
>> loaded via a namespace (and not attached):
>> [1] compiler_3.4.1
>>> 
>> 
>> 
>> 
>> With regards
>> Rishi Das Roy
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From drj|m|emon @end|ng |rom gm@||@com  Wed Feb 20 22:43:57 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Thu, 21 Feb 2019 08:43:57 +1100
Subject: [R] particle count probability
In-Reply-To: <8E122956-EC6A-4A0E-B3E4-8D942890DF0F@illinois.edu>
References: <583b90075489448795e8b4562f744879@BYAPR11MB3141.namprd11.prod.outlook.com>
 <8E122956-EC6A-4A0E-B3E4-8D942890DF0F@illinois.edu>
Message-ID: <CA+8X3fW3xC3g6VtPuT4T3_MC0fYsEdNU5imtARmAta3DsZoz0A@mail.gmail.com>

Hi Petr,
This is off the top of my head, but I assume that the shape of the
particle is not considered in counting. Assume particles are uniformly
distributed in the viewing field. If all particles entirely within the
field are counted, large particles will be undercounted. If all
particles within or intruding on the field are counted, large
particles will be overcounted. I do not have an analytic proof of
this, but the average of the two counts may provide a true count.

Hi Roger,
The Wicksell problem looks fascinating, although I don't think it
solves Petr's problem. I will study it further.

Jim

On Wed, Feb 20, 2019 at 10:34 PM Roger Koenker <rkoenker at illinois.edu> wrote:
>
>
> Somewhere, buried in the vast literature on the Wicksell problem, there is probably an answer, or at least a hint.
>
> > On Feb 20, 2019, at 11:16 AM, PIKAL Petr <petr.pikal at precheza.cz> wrote:
> >
> > Dear all
> >
> > Sorry, this is probably the most off-topic mail I have ever sent to this help list. However maybe somebody could point me to right direction or give some advice.
> >
> > In microscopy particle counting you have finite viewing field and some particles could be partly outside of this field. My problem/question is:
> >
> > Do bigger particles have also bigger probability that they will be partly outside this viewing field than smaller ones?
> >
> > Saying it differently, although there is equal count of bigger (white) and smaller (black) particles in enclosed picture (8), due to the fact that more bigger particles are on the edge I count more small particles (6) than big (4).
> >
> > Is it possible to evaluate this feature exactly i.e. calculate some bias towards smaller particles based on particle size distribution, mean particle size and/or image magnification?
> >
> > Best regards
> > Petr Pikal
> > Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner's personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
> > D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/
> >
> > <particle.pdf>______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From roy@mende|@@ohn @end|ng |rom no@@@gov  Wed Feb 20 22:50:46 2019
From: roy@mende|@@ohn @end|ng |rom no@@@gov (Roy Mendelssohn - NOAA Federal)
Date: Wed, 20 Feb 2019 13:50:46 -0800
Subject: [R] cmocean color palette
Message-ID: <C38C3435-08C5-4262-8824-6A9E1CD050C9@noaa.gov>

Hi All:

If it would be of use to anyone,  I have the latest version of the Kristen Thyng's beautiful cmocean color palettes  (see https://matplotlib.org/cmocean/ ) converted to be used in R.  These colormaps have been carefully designed given the latest ideas of what makes for a good palette,  and to make palettes that are really specific to the type of parameters in oceanography.  They are quite popular in the oceanographic community.

The file is a small .RData file.  If you are interested,  email me off-line so as not to spam the entire mail-list.

-Roy M.


**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new street address***
110 McAllister Way
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From r@turner @end|ng |rom @uck|@nd@@c@nz  Wed Feb 20 23:22:35 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Thu, 21 Feb 2019 11:22:35 +1300
Subject: [R] particle count probability
In-Reply-To: <bba66e7514c34d7ca772eefc40871a06@SRVEXCHCM1302.precheza.cz>
References: <bba66e7514c34d7ca772eefc40871a06@SRVEXCHCM1302.precheza.cz>
Message-ID: <76dd733a-31ec-b7ff-8e6a-c8c6e45ff0bb@auckland.ac.nz>

On 2/21/19 12:16 AM, PIKAL Petr wrote:
> Dear all
> 
> Sorry, this is probably the most off-topic mail I have ever sent to
> this help list. However maybe somebody could point me to right
> direction or give some advice.
> 
> In microscopy particle counting you have finite viewing field and
> some particles could be partly outside of this field. My
> problem/question is:
> 
> Do bigger particles have also bigger probability that they will be
> partly outside this viewing field than smaller ones?
> 
> Saying it differently, although there is equal count of bigger
> (white) and smaller (black) particles in enclosed picture (8), due to
> the fact that more bigger particles are on the edge I count more
> small particles (6) than big (4).
> 
> Is it possible to evaluate this feature exactly i.e. calculate some
> bias towards smaller particles based on particle size distribution,
> mean particle size and/or image magnification?

This is fundamentally a stereology problem (or so it seems to me) and as 
such twists my head.  Stereology is tricky and can be full of apparent 
paradoxes.

"Generally speaking" it surely must be the case that larger particles 
have a larger probability of intersecting the complement of the window,
but to say something solid, some assumptions would have to be made.  I'm 
not sure what.

To take a simple case:  If the particles are discs whose centres are 
uniformly distributed on the window W which is an (a x b) rectangle,
the probability that a particle, whose radius is R, intersects the 
complement of W is

    1 - (a-R)(b-R)/ab

for R <= min{a,b}, and is 1 otherwise.  I think!  (I could be muddling 
things up, as I so often do; check my reasoning.)

This is an increasing function of R for R in [0,min{a,b}].

I hope this helps a bit.

Should you wish to learn more about stereology, may I recommend:

> @Book{baddvede05,
>   author =       {A. Baddeley and E.B. Vedel Jensen},
>   title =        {Stereology for Statisticians},
>   publisher =    {Chapman and Hall/CRC},
>   year =         2005,
>   address =      {Boca Raton},
>   note =         {{ISBN} 1-58488-405-3}
> }

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From drj|m|emon @end|ng |rom gm@||@com  Thu Feb 21 00:24:18 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Thu, 21 Feb 2019 10:24:18 +1100
Subject: [R] particle count probability
In-Reply-To: <76dd733a-31ec-b7ff-8e6a-c8c6e45ff0bb@auckland.ac.nz>
References: <bba66e7514c34d7ca772eefc40871a06@SRVEXCHCM1302.precheza.cz>
 <76dd733a-31ec-b7ff-8e6a-c8c6e45ff0bb@auckland.ac.nz>
Message-ID: <CA+8X3fV4btB94bEOv6yorQVf1+oHjBJ7Q=Bcty1CAmhQ8V57dg@mail.gmail.com>

Okay, suppose the viewing field is circular and we consider two
particles as in the attached image.

Probability of being within the field:
R0 > sqrt((x1+R1-x0)^2 + (y1+R1-y0)^2)
Probability of being outside the field:
R0 < sqrt((x2-R1-x0)^2 + (y2-R1-y0)^2)

Since these are the limiting cases, it looks like the averaging I
suggested will work.

Jim

On Thu, Feb 21, 2019 at 9:23 AM Rolf Turner <r.turner at auckland.ac.nz> wrote:
>
> On 2/21/19 12:16 AM, PIKAL Petr wrote:
> > Dear all
> >
> > Sorry, this is probably the most off-topic mail I have ever sent to
> > this help list. However maybe somebody could point me to right
> > direction or give some advice.
> >
> > In microscopy particle counting you have finite viewing field and
> > some particles could be partly outside of this field. My
> > problem/question is:
> >
> > Do bigger particles have also bigger probability that they will be
> > partly outside this viewing field than smaller ones?
> >
> > Saying it differently, although there is equal count of bigger
> > (white) and smaller (black) particles in enclosed picture (8), due to
> > the fact that more bigger particles are on the edge I count more
> > small particles (6) than big (4).
> >
> > Is it possible to evaluate this feature exactly i.e. calculate some
> > bias towards smaller particles based on particle size distribution,
> > mean particle size and/or image magnification?
>
> This is fundamentally a stereology problem (or so it seems to me) and as
> such twists my head.  Stereology is tricky and can be full of apparent
> paradoxes.
>
> "Generally speaking" it surely must be the case that larger particles
> have a larger probability of intersecting the complement of the window,
> but to say something solid, some assumptions would have to be made.  I'm
> not sure what.
>
> To take a simple case:  If the particles are discs whose centres are
> uniformly distributed on the window W which is an (a x b) rectangle,
> the probability that a particle, whose radius is R, intersects the
> complement of W is
>
>     1 - (a-R)(b-R)/ab
>
> for R <= min{a,b}, and is 1 otherwise.  I think!  (I could be muddling
> things up, as I so often do; check my reasoning.)
>
> This is an increasing function of R for R in [0,min{a,b}].
>
> I hope this helps a bit.
>
> Should you wish to learn more about stereology, may I recommend:
>
> > @Book{baddvede05,
> >   author =       {A. Baddeley and E.B. Vedel Jensen},
> >   title =        {Stereology for Statisticians},
> >   publisher =    {Chapman and Hall/CRC},
> >   year =         2005,
> >   address =      {Boca Raton},
> >   note =         {{ISBN} 1-58488-405-3}
> > }
>
> cheers,
>
> Rolf
>
> --
> Honorary Research Fellow
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: particles.png
Type: image/png
Size: 2402 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190221/52104c6a/attachment.png>

From robertz|mb@rdo @end|ng |rom gm@||@com  Thu Feb 21 05:12:09 2019
From: robertz|mb@rdo @end|ng |rom gm@||@com (Robert Zimbardo)
Date: Wed, 20 Feb 2019 20:12:09 -0800
Subject: [R] Dotchart and its arguments
Message-ID: <CAGJyvhGkL34UmdzrJ+CHzOX=kERL+1G11yqJLq9GjaqHzH80-w@mail.gmail.com>

Hi all

I was recently trying to customise a dotchart of a matrix

dats <- matrix(1:6, nrow=2, dimnames=list(R=letters[1:2], C=letters[14:16]))
dotchart(dats)

with pch and pt.cex and noticed some irregularities, namely that R
doesn't use the values in the positions it uses for plotting also for
the arguments of the dotchart function:

dotchart(dats, pch=as.character(dats))       # wrong
dotchart(dats, pch=as.character(dats[,3:1])) # right
dotchart(dats, pch=as.character(dats[,3:1]), pt.cex=dats)       # wrong
dotchart(dats, pch=as.character(dats[,3:1]), pt.cex=dats[,3:1]) # right

Is this a bug or a feature (whose purpose then I don't get)?

Thanks


From petr@p|k@| @end|ng |rom prechez@@cz  Thu Feb 21 09:52:50 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Thu, 21 Feb 2019 08:52:50 +0000
Subject: [R] particle count probability
In-Reply-To: <CA+8X3fV4btB94bEOv6yorQVf1+oHjBJ7Q=Bcty1CAmhQ8V57dg@mail.gmail.com>
References: <bba66e7514c34d7ca772eefc40871a06@SRVEXCHCM1302.precheza.cz>
 <76dd733a-31ec-b7ff-8e6a-c8c6e45ff0bb@auckland.ac.nz>
 <CA+8X3fV4btB94bEOv6yorQVf1+oHjBJ7Q=Bcty1CAmhQ8V57dg@mail.gmail.com>
Message-ID: <16de42eb4a9c479ba317acec878d89f9@SRVEXCHCM1302.precheza.cz>

Hallo

Thanks all for valuable suggestions. As always, people here are generous and clever. I will try to think through all your suggestions, including recommended literature.

Jim. Standard practice in particle measurement is to count (and mesure) only particles which are fully inside viewing area. So using your equation I could compare probability for let say particles with R1 = c(0.1, 1). But I probably misunderstand something. Having x0, y0 = 0 and x1 =10 and y1 = 0 I get

> sqrt((10+c(0.1, 1)-0)^2 + (0+c(0.1,1)-0)^2)
[1] 10.10050 11.04536

which gives in contrary higher value for bigger particle.

OTOH, if I take your first reasoning I get quite satisfactory values.

> 1-(10-c(0.1, 1))* (10-c(0.1,1))/(10^2)
[1] 0.0199 0.1900

Cheers.
Petr

> -----Original Message-----
> From: Jim Lemon <drjimlemon at gmail.com>
> Sent: Thursday, February 21, 2019 12:24 AM
> To: Rolf Turner <r.turner at auckland.ac.nz>
> Cc: PIKAL Petr <petr.pikal at precheza.cz>; r-help at r-project.org
> Subject: Re: [R] particle count probability
>
> Okay, suppose the viewing field is circular and we consider two particles as in
> the attached image.
>
> Probability of being within the field:
> R0 > sqrt((x1+R1-x0)^2 + (y1+R1-y0)^2)
> Probability of being outside the field:
> R0 < sqrt((x2-R1-x0)^2 + (y2-R1-y0)^2)
>
> Since these are the limiting cases, it looks like the averaging I suggested will
> work.
>
> Jim
>
> On Thu, Feb 21, 2019 at 9:23 AM Rolf Turner <r.turner at auckland.ac.nz>
> wrote:
> >
> > On 2/21/19 12:16 AM, PIKAL Petr wrote:
> > > Dear all
> > >
> > > Sorry, this is probably the most off-topic mail I have ever sent to
> > > this help list. However maybe somebody could point me to right
> > > direction or give some advice.
> > >
> > > In microscopy particle counting you have finite viewing field and
> > > some particles could be partly outside of this field. My
> > > problem/question is:
> > >
> > > Do bigger particles have also bigger probability that they will be
> > > partly outside this viewing field than smaller ones?
> > >
> > > Saying it differently, although there is equal count of bigger
> > > (white) and smaller (black) particles in enclosed picture (8), due
> > > to the fact that more bigger particles are on the edge I count more
> > > small particles (6) than big (4).
> > >
> > > Is it possible to evaluate this feature exactly i.e. calculate some
> > > bias towards smaller particles based on particle size distribution,
> > > mean particle size and/or image magnification?
> >
> > This is fundamentally a stereology problem (or so it seems to me) and
> > as such twists my head.  Stereology is tricky and can be full of
> > apparent paradoxes.
> >
> > "Generally speaking" it surely must be the case that larger particles
> > have a larger probability of intersecting the complement of the
> > window, but to say something solid, some assumptions would have to be
> > made.  I'm not sure what.
> >
> > To take a simple case:  If the particles are discs whose centres are
> > uniformly distributed on the window W which is an (a x b) rectangle,
> > the probability that a particle, whose radius is R, intersects the
> > complement of W is
> >
> >     1 - (a-R)(b-R)/ab
> >
> > for R <= min{a,b}, and is 1 otherwise.  I think!  (I could be muddling
> > things up, as I so often do; check my reasoning.)
> >
> > This is an increasing function of R for R in [0,min{a,b}].
> >
> > I hope this helps a bit.
> >
> > Should you wish to learn more about stereology, may I recommend:
> >
> > > @Book{baddvede05,
> > >   author =       {A. Baddeley and E.B. Vedel Jensen},
> > >   title =        {Stereology for Statisticians},
> > >   publisher =    {Chapman and Hall/CRC},
> > >   year =         2005,
> > >   address =      {Boca Raton},
> > >   note =         {{ISBN} 1-58488-405-3}
> > > }
> >
> > cheers,
> >
> > Rolf
> >
> > --
> > Honorary Research Fellow
> > Department of Statistics
> > University of Auckland
> > Phone: +64-9-373-7599 ext. 88276
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


From drj|m|emon @end|ng |rom gm@||@com  Thu Feb 21 11:35:55 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Thu, 21 Feb 2019 21:35:55 +1100
Subject: [R] particle count probability
In-Reply-To: <16de42eb4a9c479ba317acec878d89f9@SRVEXCHCM1302.precheza.cz>
References: <bba66e7514c34d7ca772eefc40871a06@SRVEXCHCM1302.precheza.cz>
 <76dd733a-31ec-b7ff-8e6a-c8c6e45ff0bb@auckland.ac.nz>
 <CA+8X3fV4btB94bEOv6yorQVf1+oHjBJ7Q=Bcty1CAmhQ8V57dg@mail.gmail.com>
 <16de42eb4a9c479ba317acec878d89f9@SRVEXCHCM1302.precheza.cz>
Message-ID: <CA+8X3fUOV36jA_na_P9kYmHS5N5Z1e2GFOV=whmB2D8TogM6zg@mail.gmail.com>

Hi Petr,
My second message was to show that if you take the limiting cases of
"just inside" and "just outside" - which should have been:

just inside the field:
R0 = sqrt((x1+R1-x0)^2 + (y1+R1-y0)^2)
just outside the field:
R0 = sqrt((x2-R1-x0)^2 + (y2-R1-y0)^2)

the two differences are equal along any radius, supporting the
averaging strategy.

Jim

On Thu, Feb 21, 2019 at 7:53 PM PIKAL Petr <petr.pikal at precheza.cz> wrote:
>
> Hallo
>
> Thanks all for valuable suggestions. As always, people here are generous and clever. I will try to think through all your suggestions, including recommended literature.
>
> Jim. Standard practice in particle measurement is to count (and mesure) only particles which are fully inside viewing area. So using your equation I could compare probability for let say particles with R1 = c(0.1, 1). But I probably misunderstand something. Having x0, y0 = 0 and x1 =10 and y1 = 0 I get
>
> > sqrt((10+c(0.1, 1)-0)^2 + (0+c(0.1,1)-0)^2)
> [1] 10.10050 11.04536
>
> which gives in contrary higher value for bigger particle.
>
> OTOH, if I take your first reasoning I get quite satisfactory values.
>
> > 1-(10-c(0.1, 1))* (10-c(0.1,1))/(10^2)
> [1] 0.0199 0.1900
>
> Cheers.
> Petr
>
> > -----Original Message-----
> > From: Jim Lemon <drjimlemon at gmail.com>
> > Sent: Thursday, February 21, 2019 12:24 AM
> > To: Rolf Turner <r.turner at auckland.ac.nz>
> > Cc: PIKAL Petr <petr.pikal at precheza.cz>; r-help at r-project.org
> > Subject: Re: [R] particle count probability
> >
> > Okay, suppose the viewing field is circular and we consider two particles as in
> > the attached image.
> >
> > Probability of being within the field:
> > R0 > sqrt((x1+R1-x0)^2 + (y1+R1-y0)^2)
> > Probability of being outside the field:
> > R0 < sqrt((x2-R1-x0)^2 + (y2-R1-y0)^2)
> >
> > Since these are the limiting cases, it looks like the averaging I suggested will
> > work.
> >
> > Jim
> >
> > On Thu, Feb 21, 2019 at 9:23 AM Rolf Turner <r.turner at auckland.ac.nz>
> > wrote:
> > >
> > > On 2/21/19 12:16 AM, PIKAL Petr wrote:
> > > > Dear all
> > > >
> > > > Sorry, this is probably the most off-topic mail I have ever sent to
> > > > this help list. However maybe somebody could point me to right
> > > > direction or give some advice.
> > > >
> > > > In microscopy particle counting you have finite viewing field and
> > > > some particles could be partly outside of this field. My
> > > > problem/question is:
> > > >
> > > > Do bigger particles have also bigger probability that they will be
> > > > partly outside this viewing field than smaller ones?
> > > >
> > > > Saying it differently, although there is equal count of bigger
> > > > (white) and smaller (black) particles in enclosed picture (8), due
> > > > to the fact that more bigger particles are on the edge I count more
> > > > small particles (6) than big (4).
> > > >
> > > > Is it possible to evaluate this feature exactly i.e. calculate some
> > > > bias towards smaller particles based on particle size distribution,
> > > > mean particle size and/or image magnification?
> > >
> > > This is fundamentally a stereology problem (or so it seems to me) and
> > > as such twists my head.  Stereology is tricky and can be full of
> > > apparent paradoxes.
> > >
> > > "Generally speaking" it surely must be the case that larger particles
> > > have a larger probability of intersecting the complement of the
> > > window, but to say something solid, some assumptions would have to be
> > > made.  I'm not sure what.
> > >
> > > To take a simple case:  If the particles are discs whose centres are
> > > uniformly distributed on the window W which is an (a x b) rectangle,
> > > the probability that a particle, whose radius is R, intersects the
> > > complement of W is
> > >
> > >     1 - (a-R)(b-R)/ab
> > >
> > > for R <= min{a,b}, and is 1 otherwise.  I think!  (I could be muddling
> > > things up, as I so often do; check my reasoning.)
> > >
> > > This is an increasing function of R for R in [0,min{a,b}].
> > >
> > > I hope this helps a bit.
> > >
> > > Should you wish to learn more about stereology, may I recommend:
> > >
> > > > @Book{baddvede05,
> > > >   author =       {A. Baddeley and E.B. Vedel Jensen},
> > > >   title =        {Stereology for Statisticians},
> > > >   publisher =    {Chapman and Hall/CRC},
> > > >   year =         2005,
> > > >   address =      {Boca Raton},
> > > >   note =         {{ISBN} 1-58488-405-3}
> > > > }
> > >
> > > cheers,
> > >
> > > Rolf
> > >
> > > --
> > > Honorary Research Fellow
> > > Department of Statistics
> > > University of Auckland
> > > Phone: +64-9-373-7599 ext. 88276
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
> D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/
>


From petr@p|k@| @end|ng |rom prechez@@cz  Thu Feb 21 11:58:37 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Thu, 21 Feb 2019 10:58:37 +0000
Subject: [R] particle count probability
In-Reply-To: <CA+8X3fUOV36jA_na_P9kYmHS5N5Z1e2GFOV=whmB2D8TogM6zg@mail.gmail.com>
References: <bba66e7514c34d7ca772eefc40871a06@SRVEXCHCM1302.precheza.cz>
 <76dd733a-31ec-b7ff-8e6a-c8c6e45ff0bb@auckland.ac.nz>
 <CA+8X3fV4btB94bEOv6yorQVf1+oHjBJ7Q=Bcty1CAmhQ8V57dg@mail.gmail.com>
 <16de42eb4a9c479ba317acec878d89f9@SRVEXCHCM1302.precheza.cz>
 <CA+8X3fUOV36jA_na_P9kYmHS5N5Z1e2GFOV=whmB2D8TogM6zg@mail.gmail.com>
Message-ID: <38061d3834aa438bbee39ea5ff085b55@SRVEXCHCM1302.precheza.cz>

OK. I got it.

Thanks.

Petr

> -----Original Message-----
> From: Jim Lemon <drjimlemon at gmail.com>
> Sent: Thursday, February 21, 2019 11:36 AM
> To: PIKAL Petr <petr.pikal at precheza.cz>
> Cc: Rolf Turner <r.turner at auckland.ac.nz>; r-help at r-project.org
> Subject: Re: [R] particle count probability
>
> Hi Petr,
> My second message was to show that if you take the limiting cases of "just
> inside" and "just outside" - which should have been:
>
> just inside the field:
> R0 = sqrt((x1+R1-x0)^2 + (y1+R1-y0)^2)
> just outside the field:
> R0 = sqrt((x2-R1-x0)^2 + (y2-R1-y0)^2)
>
> the two differences are equal along any radius, supporting the averaging
> strategy.
>
> Jim
>
> On Thu, Feb 21, 2019 at 7:53 PM PIKAL Petr <petr.pikal at precheza.cz> wrote:
> >
> > Hallo
> >
> > Thanks all for valuable suggestions. As always, people here are generous and
> clever. I will try to think through all your suggestions, including recommended
> literature.
> >
> > Jim. Standard practice in particle measurement is to count (and
> > mesure) only particles which are fully inside viewing area. So using
> > your equation I could compare probability for let say particles with
> > R1 = c(0.1, 1). But I probably misunderstand something. Having x0, y0
> > = 0 and x1 =10 and y1 = 0 I get
> >
> > > sqrt((10+c(0.1, 1)-0)^2 + (0+c(0.1,1)-0)^2)
> > [1] 10.10050 11.04536
> >
> > which gives in contrary higher value for bigger particle.
> >
> > OTOH, if I take your first reasoning I get quite satisfactory values.
> >
> > > 1-(10-c(0.1, 1))* (10-c(0.1,1))/(10^2)
> > [1] 0.0199 0.1900
> >
> > Cheers.
> > Petr
> >
> > > -----Original Message-----
> > > From: Jim Lemon <drjimlemon at gmail.com>
> > > Sent: Thursday, February 21, 2019 12:24 AM
> > > To: Rolf Turner <r.turner at auckland.ac.nz>
> > > Cc: PIKAL Petr <petr.pikal at precheza.cz>; r-help at r-project.org
> > > Subject: Re: [R] particle count probability
> > >
> > > Okay, suppose the viewing field is circular and we consider two
> > > particles as in the attached image.
> > >
> > > Probability of being within the field:
> > > R0 > sqrt((x1+R1-x0)^2 + (y1+R1-y0)^2) Probability of being outside
> > > the field:
> > > R0 < sqrt((x2-R1-x0)^2 + (y2-R1-y0)^2)
> > >
> > > Since these are the limiting cases, it looks like the averaging I
> > > suggested will work.
> > >
> > > Jim
> > >
> > > On Thu, Feb 21, 2019 at 9:23 AM Rolf Turner
> > > <r.turner at auckland.ac.nz>
> > > wrote:
> > > >
> > > > On 2/21/19 12:16 AM, PIKAL Petr wrote:
> > > > > Dear all
> > > > >
> > > > > Sorry, this is probably the most off-topic mail I have ever sent
> > > > > to this help list. However maybe somebody could point me to
> > > > > right direction or give some advice.
> > > > >
> > > > > In microscopy particle counting you have finite viewing field
> > > > > and some particles could be partly outside of this field. My
> > > > > problem/question is:
> > > > >
> > > > > Do bigger particles have also bigger probability that they will
> > > > > be partly outside this viewing field than smaller ones?
> > > > >
> > > > > Saying it differently, although there is equal count of bigger
> > > > > (white) and smaller (black) particles in enclosed picture (8),
> > > > > due to the fact that more bigger particles are on the edge I
> > > > > count more small particles (6) than big (4).
> > > > >
> > > > > Is it possible to evaluate this feature exactly i.e. calculate
> > > > > some bias towards smaller particles based on particle size
> > > > > distribution, mean particle size and/or image magnification?
> > > >
> > > > This is fundamentally a stereology problem (or so it seems to me)
> > > > and as such twists my head.  Stereology is tricky and can be full
> > > > of apparent paradoxes.
> > > >
> > > > "Generally speaking" it surely must be the case that larger
> > > > particles have a larger probability of intersecting the complement
> > > > of the window, but to say something solid, some assumptions would
> > > > have to be made.  I'm not sure what.
> > > >
> > > > To take a simple case:  If the particles are discs whose centres
> > > > are uniformly distributed on the window W which is an (a x b)
> > > > rectangle, the probability that a particle, whose radius is R,
> > > > intersects the complement of W is
> > > >
> > > >     1 - (a-R)(b-R)/ab
> > > >
> > > > for R <= min{a,b}, and is 1 otherwise.  I think!  (I could be
> > > > muddling things up, as I so often do; check my reasoning.)
> > > >
> > > > This is an increasing function of R for R in [0,min{a,b}].
> > > >
> > > > I hope this helps a bit.
> > > >
> > > > Should you wish to learn more about stereology, may I recommend:
> > > >
> > > > > @Book{baddvede05,
> > > > >   author =       {A. Baddeley and E.B. Vedel Jensen},
> > > > >   title =        {Stereology for Statisticians},
> > > > >   publisher =    {Chapman and Hall/CRC},
> > > > >   year =         2005,
> > > > >   address =      {Boca Raton},
> > > > >   note =         {{ISBN} 1-58488-405-3}
> > > > > }
> > > >
> > > > cheers,
> > > >
> > > > Rolf
> > > >
> > > > --
> > > > Honorary Research Fellow
> > > > Department of Statistics
> > > > University of Auckland
> > > > Phone: +64-9-373-7599 ext. 88276
> > > >
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj?
> > obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na:
> > https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information
> > about processing and protection of business partner?s personal data
> > are available on website:
> > https://www.precheza.cz/en/personal-data-protection-principles/
> > D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou
> > d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en?
> > odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any
> > documents attached to it may be confidential and are subject to the
> > legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/
> >
Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


From motyoc@k@ @end|ng |rom y@hoo@com  Thu Feb 21 14:22:46 2019
From: motyoc@k@ @end|ng |rom y@hoo@com (Andras Farkas)
Date: Thu, 21 Feb 2019 13:22:46 +0000 (UTC)
Subject: [R] reduce and intersect question (maybe)?
References: <1212477924.2903809.1550755366835.ref@mail.yahoo.com>
Message-ID: <1212477924.2903809.1550755366835@mail.yahoo.com>

Hello All,

wonder if you have a suggestion for the following:

we have
a<-data.frame(ID=c(1,2,3,4,5,6,7),date=as.POSIXct(seq(as.Date('2011-01-01'),as.Date('2011-01-07'),by = 1),format='%m/%d/%Y %H:%M'),z=rnorm(7,1,1))
b<-data.frame(ID=c(1,2,3,11,12,13,14,15),date=as.POSIXct(seq(as.Date('2011-01-01'),as.Date('2011-01-08'),by = 1),format='%m/%d/%Y %H:%M'),z=rnorm(8,1,1))
c<-data.frame(ID=c(1,2,3,4,5,6,7,8,9,10),date=as.POSIXct(c(seq(as.Date('2011-01-01'),as.Date('2011-01-05'),by = 1),seq(as.Date('2011-01-11'),as.Date('2011-01-15'),by = 1)),format='%m/%d/%Y %H:%M'),z=rnorm(10,1,1))
d<-data.frame(ID=c(1,2,3,21,22,23,24,25,26,27,28),date=as.POSIXct(c(as.Date('2011-01-01'),as.Date('2011-11-01'),as.Date('2011-01-03'),seq(as.Date('2011-01-01'),as.Date('2011-01-08'),by = 1)),format='%m/%d/%Y %H:%M'),z=rnorm(11,1,1))


#this function will do the obvious and give the IDs that are in all of the data frames based on the ID column

intersect_all <- function(a,b,...){
? Reduce(intersect, list(a,b,...))
}

intersect_all(a$ID,b$ID,c$ID,d$ID)


#I would like to extend this (or use another function) where the function would give all the rows (ie based on both columns as a condition) that are in all of the data frames, so the result should be as below as these 2 rows are in all of the data frames (the fact that the rows that are common in all data frames ie 1 and 3 in my example are I only set up for the sake of convenience, in reality their row number in each of the data frames may be different) . The value of z is of no particular importance,?but once the common rows are identified I would want to subset the data frames to get these results:

a[c(1,3),]
b[c(1,3),]
c[c(1,3),]
d[c(1,3),]

much appreciate your input,

thanks

Andras?


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Feb 21 14:47:48 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 21 Feb 2019 05:47:48 -0800
Subject: [R] reduce and intersect question (maybe)?
In-Reply-To: <1212477924.2903809.1550755366835@mail.yahoo.com>
References: <1212477924.2903809.1550755366835.ref@mail.yahoo.com>
 <1212477924.2903809.1550755366835@mail.yahoo.com>
Message-ID: <CE79F5FA-61A8-4631-A16B-64F20E81E09C@dcn.davis.ca.us>

Use ?merge instead of intersect.

On February 21, 2019 5:22:46 AM PST, Andras Farkas via R-help <r-help at r-project.org> wrote:
>Hello All,
>
>wonder if you have a suggestion for the following:
>
>we have
>a<-data.frame(ID=c(1,2,3,4,5,6,7),date=as.POSIXct(seq(as.Date('2011-01-01'),as.Date('2011-01-07'),by
>= 1),format='%m/%d/%Y %H:%M'),z=rnorm(7,1,1))
>b<-data.frame(ID=c(1,2,3,11,12,13,14,15),date=as.POSIXct(seq(as.Date('2011-01-01'),as.Date('2011-01-08'),by
>= 1),format='%m/%d/%Y %H:%M'),z=rnorm(8,1,1))
>c<-data.frame(ID=c(1,2,3,4,5,6,7,8,9,10),date=as.POSIXct(c(seq(as.Date('2011-01-01'),as.Date('2011-01-05'),by
>= 1),seq(as.Date('2011-01-11'),as.Date('2011-01-15'),by =
>1)),format='%m/%d/%Y %H:%M'),z=rnorm(10,1,1))
>d<-data.frame(ID=c(1,2,3,21,22,23,24,25,26,27,28),date=as.POSIXct(c(as.Date('2011-01-01'),as.Date('2011-11-01'),as.Date('2011-01-03'),seq(as.Date('2011-01-01'),as.Date('2011-01-08'),by
>= 1)),format='%m/%d/%Y %H:%M'),z=rnorm(11,1,1))
>
>
>#this function will do the obvious and give the IDs that are in all of
>the data frames based on the ID column
>
>intersect_all <- function(a,b,...){
>? Reduce(intersect, list(a,b,...))
>}
>
>intersect_all(a$ID,b$ID,c$ID,d$ID)
>
>
>#I would like to extend this (or use another function) where the
>function would give all the rows (ie based on both columns as a
>condition) that are in all of the data frames, so the result should be
>as below as these 2 rows are in all of the data frames (the fact that
>the rows that are common in all data frames ie 1 and 3 in my example
>are I only set up for the sake of convenience, in reality their row
>number in each of the data frames may be different) . The value of z is
>of no particular importance,?but once the common rows are identified I
>would want to subset the data frames to get these results:
>
>a[c(1,3),]
>b[c(1,3),]
>c[c(1,3),]
>d[c(1,3),]
>
>much appreciate your input,
>
>thanks
>
>Andras?
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From bgunter@4567 @end|ng |rom gm@||@com  Thu Feb 21 16:41:30 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 21 Feb 2019 07:41:30 -0800
Subject: [R] R Software
In-Reply-To: <DM6PR06MB59776F42FA378F05DC333812FB7E0@DM6PR06MB5977.namprd06.prod.outlook.com>
References: <DM6PR06MB5977819DC2087D8E879E1AE4FB630@DM6PR06MB5977.namprd06.prod.outlook.com>
 <5303c9de-b479-3f88-3c1e-e2bb8ac110f8@sapo.pt>
 <CAGxFJbQDtw9sA-egoiDeCDS+djK6kLubT81aGSuwbhcxmQ=TSA@mail.gmail.com>
 <DM6PR06MB59776F42FA378F05DC333812FB7E0@DM6PR06MB5977.namprd06.prod.outlook.com>
Message-ID: <CAGxFJbSeUrDXWLakFVoitwjpi=jfmBs58OS3hMe7Ai3yoODNOg@mail.gmail.com>

Please stop these silly posts. R is open source software, and its open
source licensing requirements are explained on its website and referenced
links. As stated there, it comes with NO guarantees. The R Foundation is
*not* a company.


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Feb 21, 2019 at 7:26 AM Evan Lindenberger <Evan.Lindenberger at jwu.edu>
wrote:

> Hello!
>
> Thanks for getting back to me, I just need to ask these question while
> reviewing a software for the school, but just to clarify, the R Foundation
> itself does not need to abide by GDPR?
>
> Also, the WISP mentioned would be how the R Foundation handles internal
> information regarding the company.
>
> Sincerely,
> Evan Lindenberger
> ------------------------------
> *From:* Bert Gunter <bgunter.4567 at gmail.com>
> *Sent:* Monday, February 18, 2019 5:45:07 PM
> *To:* Rui Barradas
> *Cc:* Evan Lindenberger; r-help at r-project.org
> *Subject:* Re: [R] R Software
>
>
> *WARNING:* This email originated from *outside* of Johnson & Wales
> University.
> *Do not click links or open attachments* unless you recognize the sender
> & are expecting the message.
> To add to what Rui said, go here:
> https://www.r-project.org/
> <https://clicktime.symantec.com/37vc7To4aANtsQh834ruXha7Vc?u=https%3A%2F%2Fwww.r-project.org%2F>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Mon, Feb 18, 2019 at 2:11 PM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
> Hello,
>
> I do not speak for the R Foundation but I believe you are not aware that
> R is a computer language for statistics biostatistics and (scientific)
> graphics.
>
> - R itself does not collect data.
> - Security policies are left to the users.
> - You can program whatever you want since R is Turing equivalent, GDPR
> or ADA compliant or not. It's up to the users/developers to comply to
> laws. (I hope they do.)
>
> Regarding this, R is pretty much the same as, for instance, C, C++,
> Fortran, etc. And just like those languages R is used by companies and
> other institutions, government or private, that enforce strong security
> policies.
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 17:32 de 18/02/2019, Evan Lindenberger escreveu:
> > Hello,
> >
> > My name is Evan Lindenberger and I work at the Johnson & Wales
> information security office. We received a request for R Software, but I
> have a few questions before we start using R, such as:
> >
> > - What information does R collect?
> > - Does the R Foundation have a written information security policy
> (WISP)?
> > - Is R compliant with GDPR and ADA?
> >
> > If someone could get back to me, that would be greatly appreciated.
> >
> > Thank you.
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> <https://clicktime.symantec.com/3BUgvpFLdSWDRFa1j2F2m5m7Vc?u=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help>
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> <https://clicktime.symantec.com/3Lh2L3VsqTjGFeFxPyQotrx7Vc?u=http%3A%2F%2Fwww.R-project.org%2Fposting-guide.html>
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> <https://clicktime.symantec.com/3BUgvpFLdSWDRFa1j2F2m5m7Vc?u=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help>
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> <https://clicktime.symantec.com/3Lh2L3VsqTjGFeFxPyQotrx7Vc?u=http%3A%2F%2Fwww.R-project.org%2Fposting-guide.html>
> and provide commented, minimal, self-contained, reproducible code.
>
>

	[[alternative HTML version deleted]]


From P@@c@|@N|k|@u@ @end|ng |rom |eu@uzh@ch  Thu Feb 21 15:10:04 2019
From: P@@c@|@N|k|@u@ @end|ng |rom |eu@uzh@ch (Pascal A. Niklaus)
Date: Thu, 21 Feb 2019 15:10:04 +0100
Subject: [R] data.table: reference column in "i"-part by column name in
 string object
Message-ID: <95637f81-525d-2538-f9e9-ed1920a41f5c@ieu.uzh.ch>

I am converting data.frame-based code to data.table, for performance 
reasons.

Is there a way to refer to columns using expressions in the "i"-part?

Here is an example:

a <- data.table(x=rep(LETTERS[1:10],each=2), y=1:20)
v <- "x"

For the j-part, I can access the column whose name is stored in v as

a[,..v]

However, for the i-part I did not find a good way to achieve the same. 
For a simple case, the following works

xx <- "B"
a[xx, on=v]

However, I can't see how this is easily expanded to more complex logical 
expressions.

Pascal


From Ev@n@L|ndenberger @end|ng |rom jwu@edu  Thu Feb 21 16:26:07 2019
From: Ev@n@L|ndenberger @end|ng |rom jwu@edu (Evan Lindenberger)
Date: Thu, 21 Feb 2019 15:26:07 +0000
Subject: [R] R Software
In-Reply-To: <CAGxFJbQDtw9sA-egoiDeCDS+djK6kLubT81aGSuwbhcxmQ=TSA@mail.gmail.com>
References: <DM6PR06MB5977819DC2087D8E879E1AE4FB630@DM6PR06MB5977.namprd06.prod.outlook.com>
 <5303c9de-b479-3f88-3c1e-e2bb8ac110f8@sapo.pt>,
 <CAGxFJbQDtw9sA-egoiDeCDS+djK6kLubT81aGSuwbhcxmQ=TSA@mail.gmail.com>
Message-ID: <DM6PR06MB59776F42FA378F05DC333812FB7E0@DM6PR06MB5977.namprd06.prod.outlook.com>

Hello!

Thanks for getting back to me, I just need to ask these question while reviewing a software for the school, but just to clarify, the R Foundation itself does not need to abide by GDPR?

Also, the WISP mentioned would be how the R Foundation handles internal information regarding the company.

Sincerely,
Evan Lindenberger

________________________________
From: Bert Gunter <bgunter.4567 at gmail.com>
Sent: Monday, February 18, 2019 5:45:07 PM
To: Rui Barradas
Cc: Evan Lindenberger; r-help at r-project.org
Subject: Re: [R] R Software


WARNING: This email originated from outside of Johnson & Wales University.
Do not click links or open attachments unless you recognize the sender & are expecting the message.

To add to what Rui said, go here:
https://www.r-project.org/<https://clicktime.symantec.com/37vc7To4aANtsQh834ruXha7Vc?u=https%3A%2F%2Fwww.r-project.org%2F>

Bert Gunter

"The trouble with having an open mind is that people keep coming along and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Feb 18, 2019 at 2:11 PM Rui Barradas <ruipbarradas at sapo.pt<mailto:ruipbarradas at sapo.pt>> wrote:
Hello,

I do not speak for the R Foundation but I believe you are not aware that
R is a computer language for statistics biostatistics and (scientific)
graphics.

- R itself does not collect data.
- Security policies are left to the users.
- You can program whatever you want since R is Turing equivalent, GDPR
or ADA compliant or not. It's up to the users/developers to comply to
laws. (I hope they do.)

Regarding this, R is pretty much the same as, for instance, C, C++,
Fortran, etc. And just like those languages R is used by companies and
other institutions, government or private, that enforce strong security
policies.


Hope this helps,

Rui Barradas

?s 17:32 de 18/02/2019, Evan Lindenberger escreveu:
> Hello,
>
> My name is Evan Lindenberger and I work at the Johnson & Wales information security office. We received a request for R Software, but I have a few questions before we start using R, such as:
>
> - What information does R collect?
> - Does the R Foundation have a written information security policy (WISP)?
> - Is R compliant with GDPR and ADA?
>
> If someone could get back to me, that would be greatly appreciated.
>
> Thank you.
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help<https://clicktime.symantec.com/3BUgvpFLdSWDRFa1j2F2m5m7Vc?u=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help>
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html<https://clicktime.symantec.com/3Lh2L3VsqTjGFeFxPyQotrx7Vc?u=http%3A%2F%2Fwww.R-project.org%2Fposting-guide.html>
> and provide commented, minimal, self-contained, reproducible code.
>

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help<https://clicktime.symantec.com/3BUgvpFLdSWDRFa1j2F2m5m7Vc?u=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help>
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html<https://clicktime.symantec.com/3Lh2L3VsqTjGFeFxPyQotrx7Vc?u=http%3A%2F%2Fwww.R-project.org%2Fposting-guide.html>
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From motyoc@k@ @end|ng |rom y@hoo@com  Thu Feb 21 19:06:00 2019
From: motyoc@k@ @end|ng |rom y@hoo@com (Andras Farkas)
Date: Thu, 21 Feb 2019 18:06:00 +0000 (UTC)
Subject: [R] reduce and intersect question (maybe)?
In-Reply-To: <CE79F5FA-61A8-4631-A16B-64F20E81E09C@dcn.davis.ca.us>
References: <1212477924.2903809.1550755366835.ref@mail.yahoo.com>
 <1212477924.2903809.1550755366835@mail.yahoo.com>
 <CE79F5FA-61A8-4631-A16B-64F20E81E09C@dcn.davis.ca.us>
Message-ID: <879583611.3068260.1550772360309@mail.yahoo.com>

Works well! Thanks!
Andras? 

    On Thursday, February 21, 2019, 8:47:51 AM EST, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:  
 
 Use ?merge instead of intersect.

On February 21, 2019 5:22:46 AM PST, Andras Farkas via R-help <r-help at r-project.org> wrote:
>Hello All,
>
>wonder if you have a suggestion for the following:
>
>we have
>a<-data.frame(ID=c(1,2,3,4,5,6,7),date=as.POSIXct(seq(as.Date('2011-01-01'),as.Date('2011-01-07'),by
>= 1),format='%m/%d/%Y %H:%M'),z=rnorm(7,1,1))
>b<-data.frame(ID=c(1,2,3,11,12,13,14,15),date=as.POSIXct(seq(as.Date('2011-01-01'),as.Date('2011-01-08'),by
>= 1),format='%m/%d/%Y %H:%M'),z=rnorm(8,1,1))
>c<-data.frame(ID=c(1,2,3,4,5,6,7,8,9,10),date=as.POSIXct(c(seq(as.Date('2011-01-01'),as.Date('2011-01-05'),by
>= 1),seq(as.Date('2011-01-11'),as.Date('2011-01-15'),by =
>1)),format='%m/%d/%Y %H:%M'),z=rnorm(10,1,1))
>d<-data.frame(ID=c(1,2,3,21,22,23,24,25,26,27,28),date=as.POSIXct(c(as.Date('2011-01-01'),as.Date('2011-11-01'),as.Date('2011-01-03'),seq(as.Date('2011-01-01'),as.Date('2011-01-08'),by
>= 1)),format='%m/%d/%Y %H:%M'),z=rnorm(11,1,1))
>
>
>#this function will do the obvious and give the IDs that are in all of
>the data frames based on the ID column
>
>intersect_all <- function(a,b,...){
>? Reduce(intersect, list(a,b,...))
>}
>
>intersect_all(a$ID,b$ID,c$ID,d$ID)
>
>
>#I would like to extend this (or use another function) where the
>function would give all the rows (ie based on both columns as a
>condition) that are in all of the data frames, so the result should be
>as below as these 2 rows are in all of the data frames (the fact that
>the rows that are common in all data frames ie 1 and 3 in my example
>are I only set up for the sake of convenience, in reality their row
>number in each of the data frames may be different) . The value of z is
>of no particular importance,?but once the common rows are identified I
>would want to subset the data frames to get these results:
>
>a[c(1,3),]
>b[c(1,3),]
>c[c(1,3),]
>d[c(1,3),]
>
>much appreciate your input,
>
>thanks
>
>Andras?
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.  
	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Feb 21 19:28:48 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 21 Feb 2019 10:28:48 -0800
Subject: [R] R Software
In-Reply-To: <CAGxFJbSeUrDXWLakFVoitwjpi=jfmBs58OS3hMe7Ai3yoODNOg@mail.gmail.com>
References: <DM6PR06MB5977819DC2087D8E879E1AE4FB630@DM6PR06MB5977.namprd06.prod.outlook.com>
 <5303c9de-b479-3f88-3c1e-e2bb8ac110f8@sapo.pt>
 <CAGxFJbQDtw9sA-egoiDeCDS+djK6kLubT81aGSuwbhcxmQ=TSA@mail.gmail.com>
 <DM6PR06MB59776F42FA378F05DC333812FB7E0@DM6PR06MB5977.namprd06.prod.outlook.com>
 <CAGxFJbSeUrDXWLakFVoitwjpi=jfmBs58OS3hMe7Ai3yoODNOg@mail.gmail.com>
Message-ID: <72220D68-0B5B-4F56-B61C-AAEF0B262813@dcn.davis.ca.us>

IANAL (nor an R Core developer) but I think GDPR applies to organizations, particularly ones that handle personally-identifiable data, presumably through the use of software. As R does not by design collect such data without being given explicit instructions by the user to do so and where to store it, it seems unlikely to me that this should be an issue. [1] However, R exists because of individuals and organizations that contribute to it, so anyone who needs to pursue this question should be communicating with those entities... e.g. [2][3][4], since this forum is primarily populated by users and is not really suitable for legal queries. Be warned that there are a lot of them when you consider the number of contributed packages (that you might elect to install but are separate from R) out there, and IMO it will be unlikely to be worth your while to bother them unless your review of their open source code reveals phone-home behaviour.

[1] https://termsfeed.com/blog/gdpr-open-source
[2] https://www.r-project.org/foundation/board.html
[3] https://mran.microsoft.com/contact
[4] https://www.rstudio.com/about/

On February 21, 2019 7:41:30 AM PST, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>Please stop these silly posts. R is open source software, and its open
>source licensing requirements are explained on its website and
>referenced
>links. As stated there, it comes with NO guarantees. The R Foundation
>is
>*not* a company.
>
>
>Bert Gunter
>
>"The trouble with having an open mind is that people keep coming along
>and
>sticking things into it."
>-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
>On Thu, Feb 21, 2019 at 7:26 AM Evan Lindenberger
><Evan.Lindenberger at jwu.edu>
>wrote:
>
>> Hello!
>>
>> Thanks for getting back to me, I just need to ask these question
>while
>> reviewing a software for the school, but just to clarify, the R
>Foundation
>> itself does not need to abide by GDPR?
>>
>> Also, the WISP mentioned would be how the R Foundation handles
>internal
>> information regarding the company.
>>
>> Sincerely,
>> Evan Lindenberger
>> ------------------------------
>> *From:* Bert Gunter <bgunter.4567 at gmail.com>
>> *Sent:* Monday, February 18, 2019 5:45:07 PM
>> *To:* Rui Barradas
>> *Cc:* Evan Lindenberger; r-help at r-project.org
>> *Subject:* Re: [R] R Software
>>
>>
>> *WARNING:* This email originated from *outside* of Johnson & Wales
>> University.
>> *Do not click links or open attachments* unless you recognize the
>sender
>> & are expecting the message.
>> To add to what Rui said, go here:
>> https://www.r-project.org/
>>
><https://clicktime.symantec.com/37vc7To4aANtsQh834ruXha7Vc?u=https%3A%2F%2Fwww.r-project.org%2F>
>>
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming
>along and
>> sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>> On Mon, Feb 18, 2019 at 2:11 PM Rui Barradas <ruipbarradas at sapo.pt>
>wrote:
>>
>> Hello,
>>
>> I do not speak for the R Foundation but I believe you are not aware
>that
>> R is a computer language for statistics biostatistics and
>(scientific)
>> graphics.
>>
>> - R itself does not collect data.
>> - Security policies are left to the users.
>> - You can program whatever you want since R is Turing equivalent,
>GDPR
>> or ADA compliant or not. It's up to the users/developers to comply to
>> laws. (I hope they do.)
>>
>> Regarding this, R is pretty much the same as, for instance, C, C++,
>> Fortran, etc. And just like those languages R is used by companies
>and
>> other institutions, government or private, that enforce strong
>security
>> policies.
>>
>>
>> Hope this helps,
>>
>> Rui Barradas
>>
>> ?s 17:32 de 18/02/2019, Evan Lindenberger escreveu:
>> > Hello,
>> >
>> > My name is Evan Lindenberger and I work at the Johnson & Wales
>> information security office. We received a request for R Software,
>but I
>> have a few questions before we start using R, such as:
>> >
>> > - What information does R collect?
>> > - Does the R Foundation have a written information security policy
>> (WISP)?
>> > - Is R compliant with GDPR and ADA?
>> >
>> > If someone could get back to me, that would be greatly appreciated.
>> >
>> > Thank you.
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>
><https://clicktime.symantec.com/3BUgvpFLdSWDRFa1j2F2m5m7Vc?u=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help>
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>
><https://clicktime.symantec.com/3Lh2L3VsqTjGFeFxPyQotrx7Vc?u=http%3A%2F%2Fwww.R-project.org%2Fposting-guide.html>
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>>
><https://clicktime.symantec.com/3BUgvpFLdSWDRFa1j2F2m5m7Vc?u=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help>
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>
><https://clicktime.symantec.com/3Lh2L3VsqTjGFeFxPyQotrx7Vc?u=http%3A%2F%2Fwww.R-project.org%2Fposting-guide.html>
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu Feb 21 20:43:57 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 21 Feb 2019 19:43:57 +0000
Subject: [R] data.table: reference column in "i"-part by column name in
 string object
In-Reply-To: <95637f81-525d-2538-f9e9-ed1920a41f5c@ieu.uzh.ch>
References: <95637f81-525d-2538-f9e9-ed1920a41f5c@ieu.uzh.ch>
Message-ID: <616b6e11-7f7d-5921-f424-df25a0f3a136@sapo.pt>

Hello,

I don't understand the question.
Like this?

xx <- "B"
yy <- "D"
a[xx, on = v]
a[c(xx, yy), on = v]


Note that .(xx, yy) doesn't work. It outputs something else.

Could you give an example of more complicated expressions you have 
doubts with?

Hope this helps,

Rui Barradas

?s 14:10 de 21/02/2019, Pascal A. Niklaus escreveu:
> I am converting data.frame-based code to data.table, for performance 
> reasons.
> 
> Is there a way to refer to columns using expressions in the "i"-part?
> 
> Here is an example:
> 
> a <- data.table(x=rep(LETTERS[1:10],each=2), y=1:20)
> v <- "x"
> 
> For the j-part, I can access the column whose name is stored in v as
> 
> a[,..v]
> 
> However, for the i-part I did not find a good way to achieve the same. 
> For a simple case, the following works
> 
> xx <- "B"
> a[xx, on=v]
> 
> However, I can't see how this is easily expanded to more complex logical 
> expressions.
> 
> Pascal
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From p@@c@|@n|k|@u@ @end|ng |rom |eu@uzh@ch  Thu Feb 21 23:09:31 2019
From: p@@c@|@n|k|@u@ @end|ng |rom |eu@uzh@ch (Pascal A. Niklaus)
Date: Thu, 21 Feb 2019 23:09:31 +0100
Subject: [R] data.table: reference column in "i"-part by column name in
 string object
In-Reply-To: <616b6e11-7f7d-5921-f424-df25a0f3a136@sapo.pt>
References: <95637f81-525d-2538-f9e9-ed1920a41f5c@ieu.uzh.ch>
 <616b6e11-7f7d-5921-f424-df25a0f3a136@sapo.pt>
Message-ID: <817a6660-48e1-03b5-7cea-b898f947d5dc@ieu.uzh.ch>

On 21.02.19 20:43, Rui Barradas wrote:
> Hello,
>
> I don't understand the question.
> Like this?
>
> xx <- "B"
> yy <- "D"
> a[xx, on = v]
> a[c(xx, yy), on = v]
>
> Note that .(xx, yy) doesn't work. It outputs something else.
>
> Could you give an example of more complicated expressions you have 
> doubts with?

Let's assume I'd like to create a subset like this, in data.frame syntax:

d[ (d[[v]] == a | d$b == "c") & d$c>2, ]

Here I have a column referenced by its name (stored in v), and compared 
to a value stored in a, and the whole thing is part of a more complex 
logical expression. I could write it in this conventional way, but I 
wondered whether there is a better way that would leverage some of 
data.tables syntax.

Pascal


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Fri Feb 22 08:44:37 2019
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Fri, 22 Feb 2019 07:44:37 +0000
Subject: [R] Obtaining values of estimates from a regression;
 How do I get values from a list?
Message-ID: <BN7PR03MB3730597B2C241C1011A4BF8AE27F0@BN7PR03MB3730.namprd03.prod.outlook.com>

I am trying to obtain the coefficients from a regression (performed using lm).  I would like to get the value for the slope (i.e. estimate) for pre from the following regression:


fitchange <- lm(post-pre~pre,data=mydata2)


I have tried the following without any success:


zz <- summary(fitchange)["coefficients"]
class(zz)
print(zz)
zz[[2,1]]
zz[2,1]
zz["pre","Estimate"]


I clearly don't know how to select elements from the list returned by the summary function.


A reproducible version of my code follows:


mydata2 <-structure(list(pre = c(71.3302299440613, 86.2703384845455, 120.941698468568,
                              80.9020778388552, 84.9927752038908, 77.9108032451793, 111.007107108483,
                              93.288442414475, 126.097826796255, 111.63734644637),
                         post = c(45.9294556667686,
                                114.661937978585, 138.501558726477, 55.355775963925, 97.7906200355594,
                                71.1008233796004, 149.308274695789, 122.828428213951, 143.690814568562,
                                116.607579975539)), class = "data.frame", row.names = c(NA, -10L))

fitchange <- lm(post-pre~pre,data=mydata2)
zz <- summary(fitchange)["coefficients"]
class(zz)
print(zz)
zz[[2,1]]
zz[2,1]
zz["pre","Estimate"]


Any help you can offer would be appreciated.






John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)


	[[alternative HTML version deleted]]


From er|cjberger @end|ng |rom gm@||@com  Fri Feb 22 08:50:26 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Fri, 22 Feb 2019 09:50:26 +0200
Subject: [R] Obtaining values of estimates from a regression;
 How do I get values from a list?
In-Reply-To: <BN7PR03MB3730597B2C241C1011A4BF8AE27F0@BN7PR03MB3730.namprd03.prod.outlook.com>
References: <BN7PR03MB3730597B2C241C1011A4BF8AE27F0@BN7PR03MB3730.namprd03.prod.outlook.com>
Message-ID: <CAGgJW74_Ew628p9pvUJtWvMw508fRYdDpV-P-ca0xo_cC0ag7Q@mail.gmail.com>

You have some choices

fitchange$coefficients[2]

zz$coefficients[2,1]

Note that class(zz$coefficients) shows that it is a matrix.

HTH,
Eric


On Fri, Feb 22, 2019 at 9:45 AM Sorkin, John <jsorkin at som.umaryland.edu>
wrote:

> I am trying to obtain the coefficients from a regression (performed using
> lm).  I would like to get the value for the slope (i.e. estimate) for pre
> from the following regression:
>
>
> fitchange <- lm(post-pre~pre,data=mydata2)
>
>
> I have tried the following without any success:
>
>
> zz <- summary(fitchange)["coefficients"]
> class(zz)
> print(zz)
> zz[[2,1]]
> zz[2,1]
> zz["pre","Estimate"]
>
>
> I clearly don't know how to select elements from the list returned by the
> summary function.
>
>
> A reproducible version of my code follows:
>
>
> mydata2 <-structure(list(pre = c(71.3302299440613, 86.2703384845455,
> 120.941698468568,
>                               80.9020778388552, 84.9927752038908,
> 77.9108032451793, 111.007107108483,
>                               93.288442414475, 126.097826796255,
> 111.63734644637),
>                          post = c(45.9294556667686,
>                                 114.661937978585, 138.501558726477,
> 55.355775963925, 97.7906200355594,
>                                 71.1008233796004, 149.308274695789,
> 122.828428213951, 143.690814568562,
>                                 116.607579975539)), class = "data.frame",
> row.names = c(NA, -10L))
>
> fitchange <- lm(post-pre~pre,data=mydata2)
> zz <- summary(fitchange)["coefficients"]
> class(zz)
> print(zz)
> zz[[2,1]]
> zz[2,1]
> zz["pre","Estimate"]
>
>
> Any help you can offer would be appreciated.
>
>
>
>
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and
> Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From c@|@ndr@ @end|ng |rom rgzm@de  Fri Feb 22 08:58:47 2019
From: c@|@ndr@ @end|ng |rom rgzm@de (Ivan Calandra)
Date: Fri, 22 Feb 2019 08:58:47 +0100 (CET)
Subject: [R] Obtaining values of estimates from a regression;
 How do I get values from a list?
In-Reply-To: <CAGgJW74_Ew628p9pvUJtWvMw508fRYdDpV-P-ca0xo_cC0ag7Q@mail.gmail.com>
References: <BN7PR03MB3730597B2C241C1011A4BF8AE27F0@BN7PR03MB3730.namprd03.prod.outlook.com>
 <CAGgJW74_Ew628p9pvUJtWvMw508fRYdDpV-P-ca0xo_cC0ag7Q@mail.gmail.com>
Message-ID: <33458627.1264513.1550822328009.JavaMail.open-xchange@mail.rgzm.de>

I find that the str() function is really helpful to understand how an object is
structured, and therefore how to extract part(s) of it.

Try for example:
str(zz)
and it might help you understand why zz$coefficients[2,1] is what you were
looking for.

HTH
Ivan

--
Dr. Ivan Calandra
TraCEr, laboratory for Traceology and Controlled Experiments
MONREPOS Archaeological Research Centre and
Museum for Human Behavioural Evolution
Schloss Monrepos
56567 Neuwied, Germany
+49 (0) 2631 9772-243
https://www.researchgate.net/profile/Ivan_Calandra

On February 22, 2019 at 8:50 AM Eric Berger <ericjberger at gmail.com> wrote:
> You have some choices
>
> fitchange$coefficients[2]
>
> zz$coefficients[2,1]
>
> Note that class(zz$coefficients) shows that it is a matrix.
>
> HTH,
> Eric
>
>
> On Fri, Feb 22, 2019 at 9:45 AM Sorkin, John <jsorkin at som.umaryland.edu>
> wrote:
>
> > I am trying to obtain the coefficients from a regression (performed using
> > lm). I would like to get the value for the slope (i.e. estimate) for pre
> > from the following regression:
> >
> >
> > fitchange <- lm(post-pre~pre,data=mydata2)
> >
> >
> > I have tried the following without any success:
> >
> >
> > zz <- summary(fitchange)["coefficients"]
> > class(zz)
> > print(zz)
> > zz[[2,1]]
> > zz[2,1]
> > zz["pre","Estimate"]
> >
> >
> > I clearly don't know how to select elements from the list returned by the
> > summary function.
> >
> >
> > A reproducible version of my code follows:
> >
> >
> > mydata2 <-structure(list(pre = c(71.3302299440613, 86.2703384845455,
> > 120.941698468568,
> > 80.9020778388552, 84.9927752038908,
> > 77.9108032451793, 111.007107108483,
> > 93.288442414475, 126.097826796255,
> > 111.63734644637),
> > post = c(45.9294556667686,
> > 114.661937978585, 138.501558726477,
> > 55.355775963925, 97.7906200355594,
> > 71.1008233796004, 149.308274695789,
> > 122.828428213951, 143.690814568562,
> > 116.607579975539)), class = "data.frame",
> > row.names = c(NA, -10L))
> >
> > fitchange <- lm(post-pre~pre,data=mydata2)
> > zz <- summary(fitchange)["coefficients"]
> > class(zz)
> > print(zz)
> > zz[[2,1]]
> > zz[2,1]
> > zz["pre","Estimate"]
> >
> >
> > Any help you can offer would be appreciated.
> >
> >
> >
> >
> >
> >
> > John David Sorkin M.D., Ph.D.
> > Professor of Medicine
> > Chief, Biostatistics and Informatics
> > University of Maryland School of Medicine Division of Gerontology and
> > Geriatric Medicine
> > Baltimore VA Medical Center
> > 10 North Greene Street
> > GRECC (BT/18/GR)
> > Baltimore, MD 21201-1524
> > (Phone) 410-605-7119
> > (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> >
> >
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
	[[alternative HTML version deleted]]


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Fri Feb 22 09:26:14 2019
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Fri, 22 Feb 2019 08:26:14 +0000
Subject: [R] Obtaining values of estimates from a regression;
 How do I get values from a list?
In-Reply-To: <33458627.1264513.1550822328009.JavaMail.open-xchange@mail.rgzm.de>
References: <BN7PR03MB3730597B2C241C1011A4BF8AE27F0@BN7PR03MB3730.namprd03.prod.outlook.com>
 <CAGgJW74_Ew628p9pvUJtWvMw508fRYdDpV-P-ca0xo_cC0ag7Q@mail.gmail.com>,
 <33458627.1264513.1550822328009.JavaMail.open-xchange@mail.rgzm.de>
Message-ID: <BN7PR03MB3730CE01454B2C15F4F9144CE27F0@BN7PR03MB3730.namprd03.prod.outlook.com>

Problem solved:

summary(fitchange)$coefficients[2,1]



John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)


________________________________
From: R-help <r-help-bounces at r-project.org> on behalf of Ivan Calandra <calandra at rgzm.de>
Sent: Friday, February 22, 2019 2:58:47 AM
To: r-help at r-project.org
Subject: Re: [R] Obtaining values of estimates from a regression; How do I get values from a list?

I find that the str() function is really helpful to understand how an object is
structured, and therefore how to extract part(s) of it.

Try for example:
str(zz)
and it might help you understand why zz$coefficients[2,1] is what you were
looking for.

HTH
Ivan

--
Dr. Ivan Calandra
TraCEr, laboratory for Traceology and Controlled Experiments
MONREPOS Archaeological Research Centre and
Museum for Human Behavioural Evolution
Schloss Monrepos
56567 Neuwied, Germany
+49 (0) 2631 9772-243
https://www.researchgate.net/profile/Ivan_Calandra

On February 22, 2019 at 8:50 AM Eric Berger <ericjberger at gmail.com> wrote:
> You have some choices
>
> fitchange$coefficients[2]
>
> zz$coefficients[2,1]
>
> Note that class(zz$coefficients) shows that it is a matrix.
>
> HTH,
> Eric
>
>
> On Fri, Feb 22, 2019 at 9:45 AM Sorkin, John <jsorkin at som.umaryland.edu>
> wrote:
>
> > I am trying to obtain the coefficients from a regression (performed using
> > lm). I would like to get the value for the slope (i.e. estimate) for pre
> > from the following regression:
> >
> >
> > fitchange <- lm(post-pre~pre,data=mydata2)
> >
> >
> > I have tried the following without any success:
> >
> >
> > zz <- summary(fitchange)["coefficients"]
> > class(zz)
> > print(zz)
> > zz[[2,1]]
> > zz[2,1]
> > zz["pre","Estimate"]
> >
> >
> > I clearly don't know how to select elements from the list returned by the
> > summary function.
> >
> >
> > A reproducible version of my code follows:
> >
> >
> > mydata2 <-structure(list(pre = c(71.3302299440613, 86.2703384845455,
> > 120.941698468568,
> > 80.9020778388552, 84.9927752038908,
> > 77.9108032451793, 111.007107108483,
> > 93.288442414475, 126.097826796255,
> > 111.63734644637),
> > post = c(45.9294556667686,
> > 114.661937978585, 138.501558726477,
> > 55.355775963925, 97.7906200355594,
> > 71.1008233796004, 149.308274695789,
> > 122.828428213951, 143.690814568562,
> > 116.607579975539)), class = "data.frame",
> > row.names = c(NA, -10L))
> >
> > fitchange <- lm(post-pre~pre,data=mydata2)
> > zz <- summary(fitchange)["coefficients"]
> > class(zz)
> > print(zz)
> > zz[[2,1]]
> > zz[2,1]
> > zz["pre","Estimate"]
> >
> >
> > Any help you can offer would be appreciated.
> >
> >
> >
> >
> >
> >
> > John David Sorkin M.D., Ph.D.
> > Professor of Medicine
> > Chief, Biostatistics and Informatics
> > University of Maryland School of Medicine Division of Gerontology and
> > Geriatric Medicine
> > Baltimore VA Medical Center
> > 10 North Greene Street
> > GRECC (BT/18/GR)
> > Baltimore, MD 21201-1524
> > (Phone) 410-605-7119
> > (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> >
> >
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Fri Feb 22 14:56:53 2019
From: j|ox @end|ng |rom mcm@@ter@c@ (Fox, John)
Date: Fri, 22 Feb 2019 13:56:53 +0000
Subject: [R] Obtaining values of estimates from a regression;
 How do I get values from a list?
In-Reply-To: <32697_1550824010_x1M8QogD027804_BN7PR03MB3730CE01454B2C15F4F9144CE27F0@BN7PR03MB3730.namprd03.prod.outlook.com>
References: <BN7PR03MB3730597B2C241C1011A4BF8AE27F0@BN7PR03MB3730.namprd03.prod.outlook.com>
 <CAGgJW74_Ew628p9pvUJtWvMw508fRYdDpV-P-ca0xo_cC0ag7Q@mail.gmail.com>
 <33458627.1264513.1550822328009.JavaMail.open-xchange@mail.rgzm.de>
 <32697_1550824010_x1M8QogD027804_BN7PR03MB3730CE01454B2C15F4F9144CE27F0@BN7PR03MB3730.namprd03.prod.outlook.com>
Message-ID: <7A72E234-3A40-48AF-89FA-DEA1C214B548@mcmaster.ca>

Dear John,

This seems to be more complicated than it needs to be. One normally uses coef() to extract coefficients from a model object. Thus

> coef(fitchange)
(Intercept)         pre 
-54.1010158   0.6557661 
> coef(fitchange)[2]
      pre 
0.6557661 

Best,
 John

  -------------------------------------------------
  John Fox, Professor Emeritus
  McMaster University
  Hamilton, Ontario, Canada
  Web: http::/socserv.mcmaster.ca/jfox

> On Feb 22, 2019, at 3:26 AM, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
> 
> Problem solved:
> 
> summary(fitchange)$coefficients[2,1]
> 
> 
> 
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> 
> 
> ________________________________
> From: R-help <r-help-bounces at r-project.org> on behalf of Ivan Calandra <calandra at rgzm.de>
> Sent: Friday, February 22, 2019 2:58:47 AM
> To: r-help at r-project.org
> Subject: Re: [R] Obtaining values of estimates from a regression; How do I get values from a list?
> 
> I find that the str() function is really helpful to understand how an object is
> structured, and therefore how to extract part(s) of it.
> 
> Try for example:
> str(zz)
> and it might help you understand why zz$coefficients[2,1] is what you were
> looking for.
> 
> HTH
> Ivan
> 
> --
> Dr. Ivan Calandra
> TraCEr, laboratory for Traceology and Controlled Experiments
> MONREPOS Archaeological Research Centre and
> Museum for Human Behavioural Evolution
> Schloss Monrepos
> 56567 Neuwied, Germany
> +49 (0) 2631 9772-243
> https://www.researchgate.net/profile/Ivan_Calandra
> 
> On February 22, 2019 at 8:50 AM Eric Berger <ericjberger at gmail.com> wrote:
>> You have some choices
>> 
>> fitchange$coefficients[2]
>> 
>> zz$coefficients[2,1]
>> 
>> Note that class(zz$coefficients) shows that it is a matrix.
>> 
>> HTH,
>> Eric
>> 
>> 
>> On Fri, Feb 22, 2019 at 9:45 AM Sorkin, John <jsorkin at som.umaryland.edu>
>> wrote:
>> 
>>> I am trying to obtain the coefficients from a regression (performed using
>>> lm). I would like to get the value for the slope (i.e. estimate) for pre
>>> from the following regression:
>>> 
>>> 
>>> fitchange <- lm(post-pre~pre,data=mydata2)
>>> 
>>> 
>>> I have tried the following without any success:
>>> 
>>> 
>>> zz <- summary(fitchange)["coefficients"]
>>> class(zz)
>>> print(zz)
>>> zz[[2,1]]
>>> zz[2,1]
>>> zz["pre","Estimate"]
>>> 
>>> 
>>> I clearly don't know how to select elements from the list returned by the
>>> summary function.
>>> 
>>> 
>>> A reproducible version of my code follows:
>>> 
>>> 
>>> mydata2 <-structure(list(pre = c(71.3302299440613, 86.2703384845455,
>>> 120.941698468568,
>>> 80.9020778388552, 84.9927752038908,
>>> 77.9108032451793, 111.007107108483,
>>> 93.288442414475, 126.097826796255,
>>> 111.63734644637),
>>> post = c(45.9294556667686,
>>> 114.661937978585, 138.501558726477,
>>> 55.355775963925, 97.7906200355594,
>>> 71.1008233796004, 149.308274695789,
>>> 122.828428213951, 143.690814568562,
>>> 116.607579975539)), class = "data.frame",
>>> row.names = c(NA, -10L))
>>> 
>>> fitchange <- lm(post-pre~pre,data=mydata2)
>>> zz <- summary(fitchange)["coefficients"]
>>> class(zz)
>>> print(zz)
>>> zz[[2,1]]
>>> zz[2,1]
>>> zz["pre","Estimate"]
>>> 
>>> 
>>> Any help you can offer would be appreciated.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> John David Sorkin M.D., Ph.D.
>>> Professor of Medicine
>>> Chief, Biostatistics and Informatics
>>> University of Maryland School of Medicine Division of Gerontology and
>>> Geriatric Medicine
>>> Baltimore VA Medical Center
>>> 10 North Greene Street
>>> GRECC (BT/18/GR)
>>> Baltimore, MD 21201-1524
>>> (Phone) 410-605-7119
>>> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>>> 
>>> 
>>> [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
>> [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>        [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m@rc_@chw@rtz @end|ng |rom me@com  Fri Feb 22 15:21:25 2019
From: m@rc_@chw@rtz @end|ng |rom me@com (Marc Schwartz)
Date: Fri, 22 Feb 2019 09:21:25 -0500
Subject: [R] Obtaining values of estimates from a regression;
 How do I get values from a list?
In-Reply-To: <7A72E234-3A40-48AF-89FA-DEA1C214B548@mcmaster.ca>
References: <BN7PR03MB3730597B2C241C1011A4BF8AE27F0@BN7PR03MB3730.namprd03.prod.outlook.com>
 <CAGgJW74_Ew628p9pvUJtWvMw508fRYdDpV-P-ca0xo_cC0ag7Q@mail.gmail.com>
 <33458627.1264513.1550822328009.JavaMail.open-xchange@mail.rgzm.de>
 <32697_1550824010_x1M8QogD027804_BN7PR03MB3730CE01454B2C15F4F9144CE27F0@BN7PR03MB3730.namprd03.prod.outlook.com>
 <7A72E234-3A40-48AF-89FA-DEA1C214B548@mcmaster.ca>
Message-ID: <C93F6085-89A6-4E1D-B9FA-800922F75C1C@me.com>

Hi,

I was just about to reply with coef() when I saw John's reply come through.

Note that this is covered in An Introduction to R:

  https://cran.r-project.org/doc/manuals/r-release/R-intro.html#Generic-functions-for-extracting-model-information

If your model object is 'MOD', note that you can use:

  coef(MOD)

and 

  coef(summary(MOD))

depending upon the specific information that you need.

John provides the output of coef(fitchange) below. To contrast that with:

> coef(summary(fitchange))
               Estimate Std. Error   t value   Pr(>|t|)
(Intercept) -54.1010158 32.7217093 -1.653368 0.13685562
pre           0.6557661  0.3332963  1.967517 0.08466821


These extractor functions are also covered in some of the "See Also" sections of the help pages for the modeling functions such as ?lm.

A key advantage of using the extractor functions, is that they are independent of the underlying object structure. So, while it may be unlikely that the structure of these objects would change in the future, that is not guaranteed. Thus, using the extractor functions is more "future-proof" than directly accessing object components directly.

Regards,

Marc Schwartz


> On Feb 22, 2019, at 8:56 AM, Fox, John <jfox at mcmaster.ca> wrote:
> 
> Dear John,
> 
> This seems to be more complicated than it needs to be. One normally uses coef() to extract coefficients from a model object. Thus
> 
>> coef(fitchange)
> (Intercept)         pre 
> -54.1010158   0.6557661 
>> coef(fitchange)[2]
>      pre 
> 0.6557661 
> 
> Best,
> John
> 
>  -------------------------------------------------
>  John Fox, Professor Emeritus
>  McMaster University
>  Hamilton, Ontario, Canada
>  Web: http::/socserv.mcmaster.ca/jfox
> 
>> On Feb 22, 2019, at 3:26 AM, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
>> 
>> Problem solved:
>> 
>> summary(fitchange)$coefficients[2,1]
>> 
>> 
>> 
>> John David Sorkin M.D., Ph.D.
>> Professor of Medicine
>> Chief, Biostatistics and Informatics
>> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
>> Baltimore VA Medical Center
>> 10 North Greene Street
>> GRECC (BT/18/GR)
>> Baltimore, MD 21201-1524
>> (Phone) 410-605-7119
>> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>> 
>> 
>> ________________________________
>> From: R-help <r-help-bounces at r-project.org> on behalf of Ivan Calandra <calandra at rgzm.de>
>> Sent: Friday, February 22, 2019 2:58:47 AM
>> To: r-help at r-project.org
>> Subject: Re: [R] Obtaining values of estimates from a regression; How do I get values from a list?
>> 
>> I find that the str() function is really helpful to understand how an object is
>> structured, and therefore how to extract part(s) of it.
>> 
>> Try for example:
>> str(zz)
>> and it might help you understand why zz$coefficients[2,1] is what you were
>> looking for.
>> 
>> HTH
>> Ivan
>> 
>> --
>> Dr. Ivan Calandra
>> TraCEr, laboratory for Traceology and Controlled Experiments
>> MONREPOS Archaeological Research Centre and
>> Museum for Human Behavioural Evolution
>> Schloss Monrepos
>> 56567 Neuwied, Germany
>> +49 (0) 2631 9772-243
>> https://www.researchgate.net/profile/Ivan_Calandra
>> 
>> On February 22, 2019 at 8:50 AM Eric Berger <ericjberger at gmail.com> wrote:
>>> You have some choices
>>> 
>>> fitchange$coefficients[2]
>>> 
>>> zz$coefficients[2,1]
>>> 
>>> Note that class(zz$coefficients) shows that it is a matrix.
>>> 
>>> HTH,
>>> Eric
>>> 
>>> 
>>> On Fri, Feb 22, 2019 at 9:45 AM Sorkin, John <jsorkin at som.umaryland.edu>
>>> wrote:
>>> 
>>>> I am trying to obtain the coefficients from a regression (performed using
>>>> lm). I would like to get the value for the slope (i.e. estimate) for pre
>>>> from the following regression:
>>>> 
>>>> 
>>>> fitchange <- lm(post-pre~pre,data=mydata2)
>>>> 
>>>> 
>>>> I have tried the following without any success:
>>>> 
>>>> 
>>>> zz <- summary(fitchange)["coefficients"]
>>>> class(zz)
>>>> print(zz)
>>>> zz[[2,1]]
>>>> zz[2,1]
>>>> zz["pre","Estimate"]
>>>> 
>>>> 
>>>> I clearly don't know how to select elements from the list returned by the
>>>> summary function.
>>>> 
>>>> 
>>>> A reproducible version of my code follows:
>>>> 
>>>> 
>>>> mydata2 <-structure(list(pre = c(71.3302299440613, 86.2703384845455,
>>>> 120.941698468568,
>>>> 80.9020778388552, 84.9927752038908,
>>>> 77.9108032451793, 111.007107108483,
>>>> 93.288442414475, 126.097826796255,
>>>> 111.63734644637),
>>>> post = c(45.9294556667686,
>>>> 114.661937978585, 138.501558726477,
>>>> 55.355775963925, 97.7906200355594,
>>>> 71.1008233796004, 149.308274695789,
>>>> 122.828428213951, 143.690814568562,
>>>> 116.607579975539)), class = "data.frame",
>>>> row.names = c(NA, -10L))
>>>> 
>>>> fitchange <- lm(post-pre~pre,data=mydata2)
>>>> zz <- summary(fitchange)["coefficients"]
>>>> class(zz)
>>>> print(zz)
>>>> zz[[2,1]]
>>>> zz[2,1]
>>>> zz["pre","Estimate"]
>>>> 
>>>> 
>>>> Any help you can offer would be appreciated.
>>>> 
>>>> 
>>>> 
>>>> 
>>>> 
>>>> 
>>>> John David Sorkin M.D., Ph.D.
>>>> Professor of Medicine
>>>> Chief, Biostatistics and Informatics
>>>> University of Maryland School of Medicine Division of Gerontology and
>>>> Geriatric Medicine
>>>> Baltimore VA Medical Center
>>>> 10 North Greene Street
>>>> GRECC (BT/18/GR)
>>>> Baltimore, MD 21201-1524
>>>> (Phone) 410-605-7119
>>>> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>>>> 


From j@@on@hern@ndez74 @end|ng |rom y@hoo@com  Fri Feb 22 22:10:18 2019
From: j@@on@hern@ndez74 @end|ng |rom y@hoo@com (Jason Hernandez)
Date: Fri, 22 Feb 2019 21:10:18 +0000 (UTC)
Subject: [R] Cannot reproduce tutorial results
References: <1491240453.3676052.1550869818788.ref@mail.yahoo.com>
Message-ID: <1491240453.3676052.1550869818788@mail.yahoo.com>

I have come back to trying to learn R after a long time away, and have begun with the YouTube tutorial videos by David Langer, as seen here?Introduction to Data Science with R - Data Analysis Part 1. I am using R Studio with?R version 3.4.4 (2018-03-15) -- "Someone to Lean On"
Around 1:16:00 in the video Langer creates a new variable using an else if loop:
extractTitle <- function(name) {? ? ?name <- as.character(name)? ? ?if(length(grep("Miss.", name)) > 0) {? ? ? ?return("Miss.")? ? ?} else if(length(grep("Master.", name)) > 0) {? ? ? ?return("Master.")? ? ?} else if(length(grep("Mrs.", name)) > 0) {? ? ? ?return("Mrs.")? ? ?} else if(length(grep("Mr.", name)) > 0) {? ? ? ?return("Mr.")? ? ?} else { return("Other.") }}
titles <- NULLfor(i in 1:nrow(data.combined)) {? ? ?titles <- c(title, extractTitle(data.combined[i, "name"])) }
data.combined$title <- as.factor(titles)
There are two problems I see in my attempt to replicate this. First, the data.combined set contains 1309 names, but when I try to create the variable "titles" using this code, it creates a list of 2. When I use View(titles), what comes up is the first item on the list is?
function (main = NULL, sub = NULL, xlab = NULL, ylab = NULL, line = NA, outer = FALSE, ...)
and the second item in the list is just the title "Master."
The second problem is that after I enter?data.combined$title <- as.factor(titles) I get the error message
Error in sort.list(y) : 'x' must be atomic for 'sort.list'Have you called 'sort' on a list?

If I try changing the as.factor to as.vector, I get 
Error in `$<-.data.frame`(`*tmp*`, title, value = list(function (main = NULL, : replacement has 2 rows, data has 1309

I have checked and rechecked my code, and it is identical to Langer's. What is wrong here?

| 
| 
| 
|  |  |

 |

 |
| 
|  | 
Introduction to Data Science with R - Data Analysis Part 1

Part 1 in a in-depth hands-on tutorial introducing the viewer to Data Science with R programming. The video prov...
 |

 |

 |




	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Fri Feb 22 22:47:14 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 22 Feb 2019 13:47:14 -0800
Subject: [R] Cannot reproduce tutorial results
In-Reply-To: <1491240453.3676052.1550869818788@mail.yahoo.com>
References: <1491240453.3676052.1550869818788.ref@mail.yahoo.com>
 <1491240453.3676052.1550869818788@mail.yahoo.com>
Message-ID: <CAGxFJbR1Mr6mWRkxs4S2FAW_34v8gvApAKajFGR_d-XT=t7z+w@mail.gmail.com>

This is a plain text list and your html post below is pretty mangled and
difficult to read. If you re-post in plain text, you are more likely to get
a response.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Feb 22, 2019 at 1:11 PM Jason Hernandez via R-help <
r-help at r-project.org> wrote:

> I have come back to trying to learn R after a long time away, and have
> begun with the YouTube tutorial videos by David Langer, as seen
> here Introduction to Data Science with R - Data Analysis Part 1. I am using
> R Studio with R version 3.4.4 (2018-03-15) -- "Someone to Lean On"
> Around 1:16:00 in the video Langer creates a new variable using an else if
> loop:
> extractTitle <- function(name) {     name <- as.character(name)
>  if(length(grep("Miss.", name)) > 0) {       return("Miss.")     } else
> if(length(grep("Master.", name)) > 0) {       return("Master.")     } else
> if(length(grep("Mrs.", name)) > 0) {       return("Mrs.")     } else
> if(length(grep("Mr.", name)) > 0) {       return("Mr.")     } else {
> return("Other.") }}
> titles <- NULLfor(i in 1:nrow(data.combined)) {     titles <- c(title,
> extractTitle(data.combined[i, "name"])) }
> data.combined$title <- as.factor(titles)
> There are two problems I see in my attempt to replicate this. First, the
> data.combined set contains 1309 names, but when I try to create the
> variable "titles" using this code, it creates a list of 2. When I use
> View(titles), what comes up is the first item on the list is
> function (main = NULL, sub = NULL, xlab = NULL, ylab = NULL, line = NA,
> outer = FALSE, ...)
> and the second item in the list is just the title "Master."
> The second problem is that after I enter data.combined$title <-
> as.factor(titles) I get the error message
> Error in sort.list(y) : 'x' must be atomic for 'sort.list'Have you called
> 'sort' on a list?
>
> If I try changing the as.factor to as.vector, I get
> Error in `$<-.data.frame`(`*tmp*`, title, value = list(function (main =
> NULL, : replacement has 2 rows, data has 1309
>
> I have checked and rechecked my code, and it is identical to Langer's.
> What is wrong here?
>
> |
> |
> |
> |  |  |
>
>  |
>
>  |
> |
> |  |
> Introduction to Data Science with R - Data Analysis Part 1
>
> Part 1 in a in-depth hands-on tutorial introducing the viewer to Data
> Science with R programming. The video prov...
>  |
>
>  |
>
>  |
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From y@@@@_m@||k@ @end|ng |rom y@hoo@|r  Sun Feb 24 14:25:24 2019
From: y@@@@_m@||k@ @end|ng |rom y@hoo@|r (malika yassa)
Date: Sun, 24 Feb 2019 13:25:24 +0000 (UTC)
Subject: [R] Newton-RaphsonMethod
References: <1853547470.6862241.1551014724023.ref@mail.yahoo.com>
Message-ID: <1853547470.6862241.1551014724023@mail.yahoo.com>

HELLOplease I want to approximate the solution of the equation f(x)=x*(x-2)+log(x)=0
for that i did this program

f <- function(x){x*(x-2)+log(x)}
x <- c(1 : 2)
f(x)
h <- 1e-7
df.dx <- function(x){(f(x + h) - f(x)) / h}
df.dx(3/2);df.dx(2)
?newton <- function(f, tol = 1e-7, x0 = 3/2, N = 100){
?h = 1e-7
?i = 1; x1 = x0
?p = numeric(N)
?while (i <= N) {
?df.dx = (f(x + h) - f(x)) / h
?x1 = (x0 - (f(x0) / df.dx))
?p[1] = x1
?i = i + 1
?if (abs(x1 - x0) < tol) break
?x0 = x1
?}
?return(p[1 : (i-1)])
?}
?app <- newton(f, x0 = 3/2)

but i cann't find this approximation
please can you help me?






	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sun Feb 24 18:53:55 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 24 Feb 2019 09:53:55 -0800
Subject: [R] Newton-RaphsonMethod
In-Reply-To: <1853547470.6862241.1551014724023@mail.yahoo.com>
References: <1853547470.6862241.1551014724023.ref@mail.yahoo.com>
 <1853547470.6862241.1551014724023@mail.yahoo.com>
Message-ID: <CAGxFJbR0mc1h7cS+GOTT9TXucaNPDZeJkuAeB9z9caBn+N+cBg@mail.gmail.com>

This list has a no homework policy.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, Feb 24, 2019 at 9:22 AM malika yassa via R-help <
r-help at r-project.org> wrote:

> HELLOplease I want to approximate the solution of the equation
> f(x)=x*(x-2)+log(x)=0
> for that i did this program
>
> f <- function(x){x*(x-2)+log(x)}
> x <- c(1 : 2)
> f(x)
> h <- 1e-7
> df.dx <- function(x){(f(x + h) - f(x)) / h}
> df.dx(3/2);df.dx(2)
>  newton <- function(f, tol = 1e-7, x0 = 3/2, N = 100){
>  h = 1e-7
>  i = 1; x1 = x0
>  p = numeric(N)
>  while (i <= N) {
>  df.dx = (f(x + h) - f(x)) / h
>  x1 = (x0 - (f(x0) / df.dx))
>  p[1] = x1
>  i = i + 1
>  if (abs(x1 - x0) < tol) break
>  x0 = x1
>  }
>  return(p[1 : (i-1)])
>  }
>  app <- newton(f, x0 = 3/2)
>
> but i cann't find this approximation
> please can you help me?
>
>
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Sun Feb 24 20:08:43 2019
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Sun, 24 Feb 2019 11:08:43 -0800 (PST)
Subject: [R] Which dependency list to build first?
Message-ID: <alpine.LNX.2.20.1902241104040.31842@salmo.appl-ecosys.com>

I apologize for the ambiguous subject; I could not think of a more accurate
one.

Updating packages reported that 'later' did not build, but I did not see
which dependency needs updating. Looking at the CRAN page for this package I
see potentials and would like advice on which are the ones needing explicit
updating:

LinkingTo: 	Rcpp, BH
Suggests: 	knitr, rmarkdown, testthat
...
Reverse imports: 	eplusr, fiery, httpuv, pool, promises, shiny
Reverse linking to: 	httpuv, promises
Reverse suggests: 	blogdown, servr

Learning which package(s) need to be rebuilt here for 'later' will help me
the next time I encounter this situaion.

TIA,

Rich


From djnord|und @end|ng |rom gm@||@com  Sun Feb 24 20:14:16 2019
From: djnord|und @end|ng |rom gm@||@com (Daniel Nordlund)
Date: Sun, 24 Feb 2019 11:14:16 -0800
Subject: [R] Newton-RaphsonMethod
In-Reply-To: <1853547470.6862241.1551014724023@mail.yahoo.com>
References: <1853547470.6862241.1551014724023.ref@mail.yahoo.com>
 <1853547470.6862241.1551014724023@mail.yahoo.com>
Message-ID: <a3fb803e-c054-6701-b999-e1a42de528a2@gmail.com>

On 2/24/2019 5:25 AM, malika yassa via R-help wrote:
> HELLOplease I want to approximate the solution of the equation f(x)=x*(x-2)+log(x)=0
> for that i did this program
>
> f <- function(x){x*(x-2)+log(x)}
> x <- c(1 : 2)
> f(x)
> h <- 1e-7
> df.dx <- function(x){(f(x + h) - f(x)) / h}
> df.dx(3/2);df.dx(2)
>  ?newton <- function(f, tol = 1e-7, x0 = 3/2, N = 100){
>  ?h = 1e-7
>  ?i = 1; x1 = x0
>  ?p = numeric(N)
>  ?while (i <= N) {
>  ?df.dx = (f(x + h) - f(x)) / h
>  ?x1 = (x0 - (f(x0) / df.dx))
>  ?p[1] = x1
>  ?i = i + 1
>  ?if (abs(x1 - x0) < tol) break
>  ?x0 = x1
>  ?}
>  ?return(p[1 : (i-1)])
>  ?}
>  ?app <- newton(f, x0 = 3/2)
>
> but i cann't find this approximation
> please can you help me?
>
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

As Bert pointed out there is a no homework policy on this list. But I 
will point out that when I ran your presented code, I got a lot of 
warnings.? They were not immediately informative (at least to me) 
because of a subtle programming error in your code.? Your main problem 
(but not your only one) is in the calculation of the derivative of the 
function inside the while loop. The following line is the problem:

df.dx = (f(x + h) - f(x)) / h

I will leave the solution of the problem to you.

Dan

-- 
Daniel Nordlund
Port Townsend, WA  USA


From J|n@L| @end|ng |rom g@@gov@@u  Mon Feb 25 01:52:10 2019
From: J|n@L| @end|ng |rom g@@gov@@u (Li Jin)
Date: Mon, 25 Feb 2019 00:52:10 +0000
Subject: [R] =?windows-1252?q?A_new_version_=281=2E2=2E0=29_of_the_=93spm?=
 =?windows-1252?q?=94_package_for_spatial_predictive_modelling__is_now_on_?=
 =?windows-1252?q?CRAN=2E_=5BSEC=3DUNCLASSIFIED=5D?=
Message-ID: <SYAPR01MB2413A812BA896A5E6F77A2D0D07A0@SYAPR01MB2413.ausprd01.prod.outlook.com>

Dear R users,



A new version (1.2.0) of the ?spm? package for spatial predictive modelling  is now available on CRAN.



The introductory vignette is available here:

https://cran.rstudio.com/web/packages/spm/vignettes/spm.html


In this version, two additional functions, avi and rvi have been added; and some typos in the help files have been corrected.

avi: to calculate averaged variable importance (avi) for ranfom forest; and
rvi: to calculate relative variable influence (rvi) for generalised boosted regression modelling (gbm).



As always, if you find any bugs and have any suggestions, please send me an email! Thanks in advance!



Kind regards,

Jin Li, PhD | Spatial Modeller / Computational Statistician
National Earth and Marine Observations | Environmental Geoscience Division
t:  +61 2 6249 9899    www.ga.gov.au<http://www.ga.gov.au/>



-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: mg_info.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190225/0c0f5c13/attachment.txt>

From @@h|mk@poor @end|ng |rom gm@||@com  Mon Feb 25 08:27:53 2019
From: @@h|mk@poor @end|ng |rom gm@||@com (Ashim Kapoor)
Date: Mon, 25 Feb 2019 12:57:53 +0530
Subject: [R] magick package says format is PNG when we load an SVG.
Message-ID: <CAC8=1eqebRAq=cgktMk9XjpxDakbqQ-WROu7GJ4qSmRrP64F3g@mail.gmail.com>

Dear All,

Here is a MWE :

library(magick)
tiger <- image_read_svg('http://jeroen.github.io/images/tiger.svg', width = 400)
print(tiger)

##   format width height colorspace matte filesize density
## 1    PNG   400    400       sRGB  TRUE        0   72x72

Why does the format say PNG ? Should it not say SVG? We just read an SVG file.

Can someone please explain?

Best Regards,

Ashim

	[[alternative HTML version deleted]]


From pd@|gd @end|ng |rom gm@||@com  Mon Feb 25 10:40:01 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Mon, 25 Feb 2019 10:40:01 +0100
Subject: [R] magick package says format is PNG when we load an SVG.
In-Reply-To: <CAC8=1eqebRAq=cgktMk9XjpxDakbqQ-WROu7GJ4qSmRrP64F3g@mail.gmail.com>
References: <CAC8=1eqebRAq=cgktMk9XjpxDakbqQ-WROu7GJ4qSmRrP64F3g@mail.gmail.com>
Message-ID: <9099A631-A9AF-4B41-849A-1E61DB1739B9@gmail.com>

The image format in magick is raster based, not vector based, no? So I'd expect that image_read_svg() converts the file on the fly.

-pd

> On 25 Feb 2019, at 08:27 , Ashim Kapoor <ashimkapoor at gmail.com> wrote:
> 
> Dear All,
> 
> Here is a MWE :
> 
> library(magick)
> tiger <- image_read_svg('http://jeroen.github.io/images/tiger.svg', width = 400)
> print(tiger)
> 
> ##   format width height colorspace matte filesize density
> ## 1    PNG   400    400       sRGB  TRUE        0   72x72
> 
> Why does the format say PNG ? Should it not say SVG? We just read an SVG file.
> 
> Can someone please explain?
> 
> Best Regards,
> 
> Ashim
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From @@h|mk@poor @end|ng |rom gm@||@com  Mon Feb 25 11:01:12 2019
From: @@h|mk@poor @end|ng |rom gm@||@com (Ashim Kapoor)
Date: Mon, 25 Feb 2019 15:31:12 +0530
Subject: [R] magick package says format is PNG when we load an SVG.
In-Reply-To: <9099A631-A9AF-4B41-849A-1E61DB1739B9@gmail.com>
References: <CAC8=1eqebRAq=cgktMk9XjpxDakbqQ-WROu7GJ4qSmRrP64F3g@mail.gmail.com>
 <9099A631-A9AF-4B41-849A-1E61DB1739B9@gmail.com>
Message-ID: <CAC8=1eo-=hFbDA8gA2W5hCxqHORXRyzOUMJVrztrL2HtbyQ98w@mail.gmail.com>

Dear Peter,

So when I convert from PDF to SVG it will first convert to raster and then
back to vector ? Is'nt there a way of preventing this ?

Many thanks,
Ashim

On Mon, Feb 25, 2019 at 3:10 PM peter dalgaard <pdalgd at gmail.com> wrote:

> The image format in magick is raster based, not vector based, no? So I'd
> expect that image_read_svg() converts the file on the fly.
>
> -pd
>
> > On 25 Feb 2019, at 08:27 , Ashim Kapoor <ashimkapoor at gmail.com> wrote:
> >
> > Dear All,
> >
> > Here is a MWE :
> >
> > library(magick)
> > tiger <- image_read_svg('http://jeroen.github.io/images/tiger.svg',
> width = 400)
> > print(tiger)
> >
> > ##   format width height colorspace matte filesize density
> > ## 1    PNG   400    400       sRGB  TRUE        0   72x72
> >
> > Why does the format say PNG ? Should it not say SVG? We just read an SVG
> file.
> >
> > Can someone please explain?
> >
> > Best Regards,
> >
> > Ashim
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>
>

	[[alternative HTML version deleted]]


From petr@p|k@| @end|ng |rom prechez@@cz  Mon Feb 25 11:57:33 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Mon, 25 Feb 2019 10:57:33 +0000
Subject: [R] Cannot reproduce tutorial results
In-Reply-To: <1491240453.3676052.1550869818788@mail.yahoo.com>
References: <1491240453.3676052.1550869818788.ref@mail.yahoo.com>
 <1491240453.3676052.1550869818788@mail.yahoo.com>
Message-ID: <209255d5bf4144bcbc0ce6e33cb9bbcf@SRVEXCHCM1302.precheza.cz>

Hi.

Without going too deep in your messy code:
"title" is function to make titles in plots.

therefore

titles <- c(title, extractTitle(data.combined[i, "name"]))

put in your titles object in the first place this "title" function.

And I believe, that instead of multiple if/else there is better vectorised option using gsub for extracting titles from something like TTT. Name Name. Just find a dot and get rid of evering what is right from the dot.

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Jason Hernandez
> via R-help
> Sent: Friday, February 22, 2019 10:10 PM
> To: r-help at r-project.org
> Subject: [R] Cannot reproduce tutorial results
>
> I have come back to trying to learn R after a long time away, and have begun
> with the YouTube tutorial videos by David Langer, as seen here Introduction to
> Data Science with R - Data Analysis Part 1. I am using R Studio with R version
> 3.4.4 (2018-03-15) -- "Someone to Lean On"
> Around 1:16:00 in the video Langer creates a new variable using an else if loop:
> extractTitle <- function(name) {     name <-
> as.character(name)     if(length(grep("Miss.", name)) > 0)
> {       return("Miss.")     } else if(length(grep("Master.", name)) > 0)
> {       return("Master.")     } else if(length(grep("Mrs.", name)) > 0)
> {       return("Mrs.")     } else if(length(grep("Mr.", name)) > 0)
> {       return("Mr.")     } else { return("Other.") }} titles <- NULLfor(i in
> 1:nrow(data.combined)) {     titles <- c(title, extractTitle(data.combined[i,
> "name"])) } data.combined$title <- as.factor(titles) There are two problems I
> see in my attempt to replicate this. First, the data.combined set contains 1309
> names, but when I try to create the variable "titles" using this code, it creates a
> list of 2. When I use View(titles), what comes up is the first item on the list is
> function (main = NULL, sub = NULL, xlab = NULL, ylab = NULL, line = NA, outer =
> FALSE, ...) and the second item in the list is just the title "Master."
> The second problem is that after I enter data.combined$title <- as.factor(titles)
> I get the error message Error in sort.list(y) : 'x' must be atomic for
> 'sort.list'Have you called 'sort' on a list?
>
> If I try changing the as.factor to as.vector, I get Error in `$<-
> .data.frame`(`*tmp*`, title, value = list(function (main = NULL, : replacement
> has 2 rows, data has 1309
>
> I have checked and rechecked my code, and it is identical to Langer's. What is
> wrong here?
>
> |
> |
> |
> |  |  |
>
>  |
>
>  |
> |
> |  |
> Introduction to Data Science with R - Data Analysis Part 1
>
> Part 1 in a in-depth hands-on tutorial introducing the viewer to Data Science
> with R programming. The video prov...
>  |
>
>  |
>
>  |
>
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


From pd@|gd @end|ng |rom gm@||@com  Mon Feb 25 14:06:06 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Mon, 25 Feb 2019 14:06:06 +0100
Subject: [R] [Rd] R 3.5.3 scheduled for March 11
Message-ID: <5E0AD3BC-059D-4528-991D-B7CF3F2B5716@gmail.com>

Full schedule available on developer.r-project.org (pending auto-update from SVN)

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel

_______________________________________________
R-announce at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From m@k@hho||y @end|ng |rom gm@||@com  Mon Feb 25 18:39:34 2019
From: m@k@hho||y @end|ng |rom gm@||@com (greg holly)
Date: Mon, 25 Feb 2019 11:39:34 -0600
Subject: [R] differences between meat and metafor packages
Message-ID: <CAM9Qe4gp4xG9uKx8L1sANHXhuR-AHttHEQVpn8fVME-boXY3AQ@mail.gmail.com>

Hi all;

I have got different results (CI and Q value for heterogeneity, tau) on the
same data when I run meta and metafor for hazard ratio with a random
effects model. The basic programs for both are given below. What can cause?

Regards,
Greg

Metafor
 res <- rma(HR, SE, data=a)
predict(res, transf=exp)

Meta
 metagen(HR, SE, sm="HR", data=a)

	[[alternative HTML version deleted]]


From n|ge|@montgomery @end|ng |rom roche@com  Mon Feb 25 11:22:10 2019
From: n|ge|@montgomery @end|ng |rom roche@com (Montgomery, Nigel)
Date: Mon, 25 Feb 2019 11:22:10 +0100
Subject: [R] Extracting data from zip file in one directory to a different
 directory.
Message-ID: <CAF0rjiYt5B1NCewP6m_5jqmsQgC6UWK=rRd6XhRcfzGk--8y5A@mail.gmail.com>

Hi there, I've tried several different ways to do the above (see attempts
below) and none have been successful. From my internet searches this would
appear to be a common problem.
Any pointers would be appreciated.

Attempt 1:
zip_file_loc <- "xx/yy/zipfile.zip"
data <- read_csv(unzip(zip_file_loc))

ERROR: error 1 in extracting from zip file

Attempt 2:
temp <- tempfile()
temp <- "xx/yy/zipfile.zip"
unzip(temp)

ERROR: error 1 in extracting from zip file

Regards, Nigel.

	[[alternative HTML version deleted]]


From JLucke m@iii@g oii ri@@buii@io@edu  Mon Feb 25 19:31:46 2019
From: JLucke m@iii@g oii ri@@buii@io@edu (JLucke m@iii@g oii ri@@buii@io@edu)
Date: Mon, 25 Feb 2019 13:31:46 -0500
Subject: [R] Ghost variables
Message-ID: <OF4A4F4FD9.33C2E490-ON852583AC.00634A56-852583AC.0065C8C3@ria.buffalo.edu>

Fellow R-gonauts:

I frequently erase/remove all the objects in my current environment so can 
I re-run scripts to ensure that analyses are complete, error-free, and 
accurate. 
However, sometimes when I re-rerun a script I get warning messages (see 
below for example)  regarding some variables (objects) when these 
variables do not exist in my current environment. 
These ghost variables had existed at one time, but were subsequently 
removed by the rm(list=ls()) command or by the broom icon in RStudio. 

What's happening and how do I exorcise the ghosts?


Warning messages:
1: Unknown or uninitialised column: 'K'. 
2: Unknown or uninitialised column: 'NDAfit'. 
3: Unknown or uninitialised column: 'NDAfit'. 
4: Unknown or uninitialised column: 'NDAfit'. 
5: Unknown or uninitialised column: 'NDAfit'. 
6: Unknown or uninitialised column: 'NDAfit'. 
7: Unknown or uninitialised column: 'NDAobs'.

Joe Lucke
	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon Feb 25 19:35:48 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 25 Feb 2019 10:35:48 -0800
Subject: [R] 
 Extracting data from zip file in one directory to a different
 directory.
In-Reply-To: <CAF0rjiYt5B1NCewP6m_5jqmsQgC6UWK=rRd6XhRcfzGk--8y5A@mail.gmail.com>
References: <CAF0rjiYt5B1NCewP6m_5jqmsQgC6UWK=rRd6XhRcfzGk--8y5A@mail.gmail.com>
Message-ID: <D41F92BC-4629-4151-9231-B5676F8868C5@dcn.davis.ca.us>

My bet is that the zip file is corrupt.

Also, carefully read the documentation for the unzip function... your attempt 1 was not going to work as posed even with a valid file.

On February 25, 2019 2:22:10 AM PST, "Montgomery, Nigel" <nigel.montgomery at roche.com> wrote:
>Hi there, I've tried several different ways to do the above (see
>attempts
>below) and none have been successful. From my internet searches this
>would
>appear to be a common problem.
>Any pointers would be appreciated.
>
>Attempt 1:
>zip_file_loc <- "xx/yy/zipfile.zip"
>data <- read_csv(unzip(zip_file_loc))
>
>ERROR: error 1 in extracting from zip file
>
>Attempt 2:

>temp <- tempfile()
>temp <- "xx/yy/zipfile.zip"
>unzip(temp)
>
>ERROR: error 1 in extracting from zip file
>
>Regards, Nigel.
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon Feb 25 19:39:52 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 25 Feb 2019 10:39:52 -0800
Subject: [R] Ghost variables
In-Reply-To: <OF4A4F4FD9.33C2E490-ON852583AC.00634A56-852583AC.0065C8C3@ria.buffalo.edu>
References: <OF4A4F4FD9.33C2E490-ON852583AC.00634A56-852583AC.0065C8C3@ria.buffalo.edu>
Message-ID: <F3E4D2E8-DE37-4E4B-8B6D-F6C5A84A644E@dcn.davis.ca.us>

Cease and desist with erasing objects... it fails to address loaded packages and attached data. Rather, restart R. There is an option to do this in RStudio, but with Rgui you have to exit and restart.

On February 25, 2019 10:31:46 AM PST, JLucke at ria.buffalo.edu wrote:
>Fellow R-gonauts:
>
>I frequently erase/remove all the objects in my current environment so
>can 
>I re-run scripts to ensure that analyses are complete, error-free, and 
>accurate. 
>However, sometimes when I re-rerun a script I get warning messages (see
>
>below for example)  regarding some variables (objects) when these 
>variables do not exist in my current environment. 
>These ghost variables had existed at one time, but were subsequently 
>removed by the rm(list=ls()) command or by the broom icon in RStudio. 
>
>What's happening and how do I exorcise the ghosts?
>
>
>Warning messages:
>1: Unknown or uninitialised column: 'K'. 
>2: Unknown or uninitialised column: 'NDAfit'. 
>3: Unknown or uninitialised column: 'NDAfit'. 
>4: Unknown or uninitialised column: 'NDAfit'. 
>5: Unknown or uninitialised column: 'NDAfit'. 
>6: Unknown or uninitialised column: 'NDAfit'. 
>7: Unknown or uninitialised column: 'NDAobs'.
>
>Joe Lucke
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From wdun|@p @end|ng |rom t|bco@com  Mon Feb 25 20:30:07 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Mon, 25 Feb 2019 11:30:07 -0800
Subject: [R] Ghost variables
In-Reply-To: <OF4A4F4FD9.33C2E490-ON852583AC.00634A56-852583AC.0065C8C3@ria.buffalo.edu>
References: <OF4A4F4FD9.33C2E490-ON852583AC.00634A56-852583AC.0065C8C3@ria.buffalo.edu>
Message-ID: <CAF8bMcabNF-NZgPK2=pQwo0FQrF1bb=BSKfXa=Ab2SgKQPW5Rw@mail.gmail.com>

Doesn't that mean that your script is incomplete, that it needs to make
those variables?

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Mon, Feb 25, 2019 at 10:32 AM <JLucke at ria.buffalo.edu> wrote:

> Fellow R-gonauts:
>
> I frequently erase/remove all the objects in my current environment so can
> I re-run scripts to ensure that analyses are complete, error-free, and
> accurate.
> However, sometimes when I re-rerun a script I get warning messages (see
> below for example)  regarding some variables (objects) when these
> variables do not exist in my current environment.
> These ghost variables had existed at one time, but were subsequently
> removed by the rm(list=ls()) command or by the broom icon in RStudio.
>
> What's happening and how do I exorcise the ghosts?
>
>
> Warning messages:
> 1: Unknown or uninitialised column: 'K'.
> 2: Unknown or uninitialised column: 'NDAfit'.
> 3: Unknown or uninitialised column: 'NDAfit'.
> 4: Unknown or uninitialised column: 'NDAfit'.
> 5: Unknown or uninitialised column: 'NDAfit'.
> 6: Unknown or uninitialised column: 'NDAfit'.
> 7: Unknown or uninitialised column: 'NDAobs'.
>
> Joe Lucke
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n|  Mon Feb 25 20:34:26 2019
From: wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n| (Viechtbauer, Wolfgang (SP))
Date: Mon, 25 Feb 2019 19:34:26 +0000
Subject: [R] differences between meat and metafor packages
In-Reply-To: <CAM9Qe4gp4xG9uKx8L1sANHXhuR-AHttHEQVpn8fVME-boXY3AQ@mail.gmail.com>
References: <CAM9Qe4gp4xG9uKx8L1sANHXhuR-AHttHEQVpn8fVME-boXY3AQ@mail.gmail.com>
Message-ID: <c6f0e5e7e2eb48ae8dffe63e55f8eb4b@UM-MAIL3214.unimaas.nl>

The second argument (called 'vi') in rma() is for the variances. If you have SEs, then use the 'sei' argument:

res <- rma(HR, sei=SE, data=a)

Best,
Wolfgang

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of greg holly
Sent: Monday, 25 February, 2019 18:40
To: r-help mailing list
Subject: [R] differences between meat and metafor packages

Hi all;

I have got different results (CI and Q value for heterogeneity, tau) on the
same data when I run meta and metafor for hazard ratio with a random
effects model. The basic programs for both are given below. What can cause?

Regards,
Greg

Metafor
 res <- rma(HR, SE, data=a)
predict(res, transf=exp)

Meta
 metagen(HR, SE, sm="HR", data=a)


From j@@on@hern@ndez74 @end|ng |rom y@hoo@com  Mon Feb 25 20:37:43 2019
From: j@@on@hern@ndez74 @end|ng |rom y@hoo@com (Jason Hernandez)
Date: Mon, 25 Feb 2019 19:37:43 +0000 (UTC)
Subject: [R] Second attempt: Cannot reproduce tutorial results
References: <462439758.4829240.1551123463829.ref@mail.yahoo.com>
Message-ID: <462439758.4829240.1551123463829@mail.yahoo.com>

Okay, I switched to plain text. Let's see if people can see this better.
I have come back to trying to learn R after a long time away, and have begun with the YouTube tutorial videos by David Langer, as seen on "Introduction to Data Science with R - Data Analysis Part 1." I am using R Studio with?R version 3.4.4 (2018-03-15) -- "Someone to Lean On"

Around 1:16:00 in the video Langer creates a new variable using an else if loop:

extractTitle <- function(name) {
? ? ?name <- as.character(name)
? ? ?if(length(grep("Miss.", name)) > 0) {
? ? ? ?return("Miss.")
? ? ?} else if(length(grep("Master.", name)) > 0) {
? ? ? ?return("Master.")
? ? ?} else if(length(grep("Mrs.", name)) > 0) {
? ? ? ?return("Mrs.")
? ? ?} else if(length(grep("Mr.", name)) > 0) {
? ? ? ?return("Mr.")
? ? ?} else { return("Other.") }}

titles <- NULL
for(i in 1:nrow(data.combined)) {
? ? ?titles <- c(title, extractTitle(data.combined[i, "name"])) }

data.combined$title <- as.factor(titles)

There are two problems I see in my attempt to replicate this. First, the data.combined set contains 1309 names, but when I try to create the variable "titles" using this code, it creates a list of 2. When I use View(titles), what comes up is the first item on the list is?

function (main = NULL, sub = NULL, xlab = NULL, ylab = NULL, line = NA, outer = FALSE, ...)

and the second item in the list is just the title "Master."

The second problem is that after I enter?data.combined$title <- as.factor(titles) I get the error message

Error in sort.list(y) : 'x' must be atomic for 'sort.list' Have you called 'sort' on a list?

If I try changing the as.factor to as.vector, I get 

Error in `$<-.data.frame`(`*tmp*`, title, value = list(function (main = NULL, : replacement has 2 rows, data has 1309

I have checked and rechecked my code, and it is identical to Langer's. What is wrong here?


From wdun|@p @end|ng |rom t|bco@com  Mon Feb 25 21:06:23 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Mon, 25 Feb 2019 12:06:23 -0800
Subject: [R] Second attempt: Cannot reproduce tutorial results
In-Reply-To: <462439758.4829240.1551123463829@mail.yahoo.com>
References: <462439758.4829240.1551123463829.ref@mail.yahoo.com>
 <462439758.4829240.1551123463829@mail.yahoo.com>
Message-ID: <CAF8bMca=OduF56NyrZtOOcYWs6MV7J=QuQJWOGv1o29rqLcVMQ@mail.gmail.com>

Do you see anything wrong with this line?
   titles <- c(title, extractTitle(data.combined[i, "name"])) }

Hint - plural or singular?

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Mon, Feb 25, 2019 at 11:38 AM Jason Hernandez via R-help <
r-help at r-project.org> wrote:

> Okay, I switched to plain text. Let's see if people can see this better.
> I have come back to trying to learn R after a long time away, and have
> begun with the YouTube tutorial videos by David Langer, as seen on
> "Introduction to Data Science with R - Data Analysis Part 1." I am using R
> Studio with R version 3.4.4 (2018-03-15) -- "Someone to Lean On"
>
> Around 1:16:00 in the video Langer creates a new variable using an else if
> loop:
>
> extractTitle <- function(name) {
>      name <- as.character(name)
>      if(length(grep("Miss.", name)) > 0) {
>        return("Miss.")
>      } else if(length(grep("Master.", name)) > 0) {
>        return("Master.")
>      } else if(length(grep("Mrs.", name)) > 0) {
>        return("Mrs.")
>      } else if(length(grep("Mr.", name)) > 0) {
>        return("Mr.")
>      } else { return("Other.") }}
>
> titles <- NULL
> for(i in 1:nrow(data.combined)) {
>      titles <- c(title, extractTitle(data.combined[i, "name"])) }
>
> data.combined$title <- as.factor(titles)
>
> There are two problems I see in my attempt to replicate this. First, the
> data.combined set contains 1309 names, but when I try to create the
> variable "titles" using this code, it creates a list of 2. When I use
> View(titles), what comes up is the first item on the list is
>
> function (main = NULL, sub = NULL, xlab = NULL, ylab = NULL, line = NA,
> outer = FALSE, ...)
>
> and the second item in the list is just the title "Master."
>
> The second problem is that after I enter data.combined$title <-
> as.factor(titles) I get the error message
>
> Error in sort.list(y) : 'x' must be atomic for 'sort.list' Have you called
> 'sort' on a list?
>
> If I try changing the as.factor to as.vector, I get
>
> Error in `$<-.data.frame`(`*tmp*`, title, value = list(function (main =
> NULL, : replacement has 2 rows, data has 1309
>
> I have checked and rechecked my code, and it is identical to Langer's.
> What is wrong here?
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From JLucke m@iii@g oii ri@@buii@io@edu  Mon Feb 25 21:45:37 2019
From: JLucke m@iii@g oii ri@@buii@io@edu (JLucke m@iii@g oii ri@@buii@io@edu)
Date: Mon, 25 Feb 2019 15:45:37 -0500
Subject: [R] Ghost variables
In-Reply-To: <CAF8bMcabNF-NZgPK2=pQwo0FQrF1bb=BSKfXa=Ab2SgKQPW5Rw@mail.gmail.com>
References: <OF4A4F4FD9.33C2E490-ON852583AC.00634A56-852583AC.0065C8C3@ria.buffalo.edu>
 <CAF8bMcabNF-NZgPK2=pQwo0FQrF1bb=BSKfXa=Ab2SgKQPW5Rw@mail.gmail.com>
Message-ID: <OF1B4432B4.798DF11E-ON852583AC.00714596-852583AC.007209C9@ria.buffalo.edu>

The script is complete.  When I start, the environment is empty. 
The warnings are issued for these "ghost" variables well before they are 
created later in the script. 
Somehow previous incarnations are lingering around and being unhappy even 
after they were "deleted". 
Jeff Newmiller has suggested a solution I have yet to try.




William Dunlap <wdunlap at tibco.com> 
02/25/2019 02:30 PM

To
JLucke at ria.buffalo.edu, 
cc
r-help mailing list <r-help at r-project.org>
Subject
Re: [R] Ghost variables






Doesn't that mean that your script is incomplete, that it needs to make 
those variables?

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Mon, Feb 25, 2019 at 10:32 AM <JLucke at ria.buffalo.edu> wrote:
Fellow R-gonauts:

I frequently erase/remove all the objects in my current environment so can 

I re-run scripts to ensure that analyses are complete, error-free, and 
accurate. 
However, sometimes when I re-rerun a script I get warning messages (see 
below for example)  regarding some variables (objects) when these 
variables do not exist in my current environment. 
These ghost variables had existed at one time, but were subsequently 
removed by the rm(list=ls()) command or by the broom icon in RStudio. 

What's happening and how do I exorcise the ghosts?


Warning messages:
1: Unknown or uninitialised column: 'K'. 
2: Unknown or uninitialised column: 'NDAfit'. 
3: Unknown or uninitialised column: 'NDAfit'. 
4: Unknown or uninitialised column: 'NDAfit'. 
5: Unknown or uninitialised column: 'NDAfit'. 
6: Unknown or uninitialised column: 'NDAfit'. 
7: Unknown or uninitialised column: 'NDAobs'.

Joe Lucke
        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon Feb 25 23:03:52 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 25 Feb 2019 14:03:52 -0800
Subject: [R] Ghost variables
In-Reply-To: <OF1B4432B4.798DF11E-ON852583AC.00714596-852583AC.007209C9@ria.buffalo.edu>
References: <OF4A4F4FD9.33C2E490-ON852583AC.00634A56-852583AC.0065C8C3@ria.buffalo.edu>
 <CAF8bMcabNF-NZgPK2=pQwo0FQrF1bb=BSKfXa=Ab2SgKQPW5Rw@mail.gmail.com>
 <OF1B4432B4.798DF11E-ON852583AC.00714596-852583AC.007209C9@ria.buffalo.edu>
Message-ID: <7E82FD62-DA9F-484A-A263-8A811B0BF0E7@dcn.davis.ca.us>

Some other points:

A) I have seen these kinds of warnings in RStudio related to Rmd files. Try running your script in R rather than using RStudio, at least to identify where the problem is. If they are being generated by RStudio then you need to get help through their recommended forum.

B) Check for and remove any ".RData" file in your working directory when you start R. That is, if there is no text before the period in that file name it will get loaded automatically when you start R and any problems that were saved into it will be restored into your global environment. I recommend deleting such files, or at least renaming it so it won't get automatically reloaded.

C) Your assertion that the script is complete are only as trustworthy as allowed by your skills to confirm that. Pardon us if we reserve judgement on that point until the problem is isolated.

On February 25, 2019 12:45:37 PM PST, JLucke at ria.buffalo.edu wrote:
>The script is complete.  When I start, the environment is empty. 
>The warnings are issued for these "ghost" variables well before they
>are 
>created later in the script. 
>Somehow previous incarnations are lingering around and being unhappy
>even 
>after they were "deleted". 
>Jeff Newmiller has suggested a solution I have yet to try.
>
>
>
>
>William Dunlap <wdunlap at tibco.com> 
>02/25/2019 02:30 PM
>
>To
>JLucke at ria.buffalo.edu, 
>cc
>r-help mailing list <r-help at r-project.org>
>Subject
>Re: [R] Ghost variables
>
>
>
>
>
>
>Doesn't that mean that your script is incomplete, that it needs to make
>
>those variables?
>
>Bill Dunlap
>TIBCO Software
>wdunlap tibco.com
>
>
>On Mon, Feb 25, 2019 at 10:32 AM <JLucke at ria.buffalo.edu> wrote:
>Fellow R-gonauts:
>
>I frequently erase/remove all the objects in my current environment so
>can 
>
>I re-run scripts to ensure that analyses are complete, error-free, and 
>accurate. 
>However, sometimes when I re-rerun a script I get warning messages (see
>
>below for example)  regarding some variables (objects) when these 
>variables do not exist in my current environment. 
>These ghost variables had existed at one time, but were subsequently 
>removed by the rm(list=ls()) command or by the broom icon in RStudio. 
>
>What's happening and how do I exorcise the ghosts?
>
>
>Warning messages:
>1: Unknown or uninitialised column: 'K'. 
>2: Unknown or uninitialised column: 'NDAfit'. 
>3: Unknown or uninitialised column: 'NDAfit'. 
>4: Unknown or uninitialised column: 'NDAfit'. 
>5: Unknown or uninitialised column: 'NDAfit'. 
>6: Unknown or uninitialised column: 'NDAfit'. 
>7: Unknown or uninitialised column: 'NDAobs'.
>
>Joe Lucke
>        [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From murdoch@dunc@n @end|ng |rom gm@||@com  Mon Feb 25 23:13:33 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Mon, 25 Feb 2019 17:13:33 -0500
Subject: [R] Ghost variables
In-Reply-To: <OF1B4432B4.798DF11E-ON852583AC.00714596-852583AC.007209C9@ria.buffalo.edu>
References: <OF4A4F4FD9.33C2E490-ON852583AC.00634A56-852583AC.0065C8C3@ria.buffalo.edu>
 <CAF8bMcabNF-NZgPK2=pQwo0FQrF1bb=BSKfXa=Ab2SgKQPW5Rw@mail.gmail.com>
 <OF1B4432B4.798DF11E-ON852583AC.00714596-852583AC.007209C9@ria.buffalo.edu>
Message-ID: <ee85c30b-0e6b-7d04-0960-e89711f9da1a@gmail.com>

Those messages say that some code (presumably in your script) is 
referring to columns, not variables, with those names.  That isn't a 
message coming from base R, it's coming from some package that you are 
using, I think "tibble".

You can turn those warnings into errors by running

options(warn=2)

before your script, and then the script will die at the first warning.

Duncan Murdoch

On 25/02/2019 3:45 p.m., JLucke at ria.buffalo.edu wrote:
> The script is complete.  When I start, the environment is empty.
> The warnings are issued for these "ghost" variables well before they are
> created later in the script.
> Somehow previous incarnations are lingering around and being unhappy even
> after they were "deleted".
> Jeff Newmiller has suggested a solution I have yet to try.
> 
> 
> 
> 
> William Dunlap <wdunlap at tibco.com>
> 02/25/2019 02:30 PM
> 
> To
> JLucke at ria.buffalo.edu,
> cc
> r-help mailing list <r-help at r-project.org>
> Subject
> Re: [R] Ghost variables
> 
> 
> 
> 
> 
> 
> Doesn't that mean that your script is incomplete, that it needs to make
> those variables?
> 
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
> 
> 
> On Mon, Feb 25, 2019 at 10:32 AM <JLucke at ria.buffalo.edu> wrote:
> Fellow R-gonauts:
> 
> I frequently erase/remove all the objects in my current environment so can
> 
> I re-run scripts to ensure that analyses are complete, error-free, and
> accurate.
> However, sometimes when I re-rerun a script I get warning messages (see
> below for example)  regarding some variables (objects) when these
> variables do not exist in my current environment.
> These ghost variables had existed at one time, but were subsequently
> removed by the rm(list=ls()) command or by the broom icon in RStudio.
> 
> What's happening and how do I exorcise the ghosts?
> 
> 
> Warning messages:
> 1: Unknown or uninitialised column: 'K'.
> 2: Unknown or uninitialised column: 'NDAfit'.
> 3: Unknown or uninitialised column: 'NDAfit'.
> 4: Unknown or uninitialised column: 'NDAfit'.
> 5: Unknown or uninitialised column: 'NDAfit'.
> 6: Unknown or uninitialised column: 'NDAfit'.
> 7: Unknown or uninitialised column: 'NDAobs'.
> 
> Joe Lucke
>          [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Tue Feb 26 11:20:16 2019
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Tue, 26 Feb 2019 11:20:16 +0100
Subject: [R] Which dependency list to build first?
In-Reply-To: <alpine.LNX.2.20.1902241104040.31842@salmo.appl-ecosys.com>
References: <alpine.LNX.2.20.1902241104040.31842@salmo.appl-ecosys.com>
Message-ID: <23669.4832.639467.519820@stat.math.ethz.ch>

>>>>> Rich Shepard 
>>>>>     on Sun, 24 Feb 2019 11:08:43 -0800 writes:

    > I apologize for the ambiguous subject; I could not think of a more accurate
    > one.

    > Updating packages reported that 'later' did not build, but I did not see
    > which dependency needs updating. Looking at the CRAN page for this package I
    > see potentials and would like advice on which are the ones needing explicit
    > updating:

    > LinkingTo: 	Rcpp, BH
    > Suggests: 	knitr, rmarkdown, testthat
    > ...
    > Reverse imports: 	eplusr, fiery, httpuv, pool, promises, shiny
    > Reverse linking to: 	httpuv, promises
    > Reverse suggests: 	blogdown, servr

I'm not the expert on these problems, but as nobody else has
replied:

My experience is that both Rcpp and BH need  "often" to be
re-installed if your system updates in some way.
IIUC, 'LinkingTo:'  is actually a misnomer (as e.g. the Rcpp maintainer
      		     has known and told us about often enough),
here, because it's more a "including C++ header templates from"

So, I'd re-install first  Rcpp, then BH,  and then "your"
package.

Please let us (= the audience) know if that helped.

Best,
Martin

    > Learning which package(s) need to be rebuilt here for 'later' will help me
    > the next time I encounter this situaion.

    > TIA,

    > Rich

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Tue Feb 26 14:47:38 2019
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Tue, 26 Feb 2019 05:47:38 -0800 (PST)
Subject: [R] Which dependency list to build first?
In-Reply-To: <23669.4832.639467.519820@stat.math.ethz.ch>
References: <alpine.LNX.2.20.1902241104040.31842@salmo.appl-ecosys.com>
 <23669.4832.639467.519820@stat.math.ethz.ch>
Message-ID: <alpine.LNX.2.20.1902260532430.16864@salmo.appl-ecosys.com>

On Tue, 26 Feb 2019, Martin Maechler wrote:

> I'm not the expert on these problems, but as nobody else has
> replied:

Martin,

Thanks for responding.

> My experience is that both Rcpp and BH need  "often" to be re-installed if
> your system updates in some way. IIUC, 'LinkingTo:' is actually a misnomer
> (as e.g. the Rcpp maintainer has known and told us about often enough),
> here, because it's more a "including C++ header templates from"
>
> So, I'd re-install first  Rcpp, then BH,  and then "your"
> package.

Did this, and ...

> Please let us (= the audience) know if that helped.

there's still an unresolved issue with package 'later':

** testing if installed package can be loaded
Error: package or namespace load failed for ?later? in dyn.load(file, DLLpath = DLLpath, ...):
  unable to load shared object '/usr/lib/R/library/later/libs/later.so':
   /usr/lib/R/library/later/libs/later.so: undefined symbol: __atomic_fetch_add_8
Error: loading failed
Execution halted
ERROR: loading failed
* removing ?/usr/lib/R/library/later?
* restoring previous ?/usr/lib/R/library/later?

I urge everyone with more ideas to share them.

Regards,

Rich


From n@th@||e@v|@|@ne|x @end|ng |rom |nr@@|r  Mon Feb 25 18:21:30 2019
From: n@th@||e@v|@|@ne|x @end|ng |rom |nr@@|r (Nathalie Vialaneix)
Date: Mon, 25 Feb 2019 18:21:30 +0100
Subject: [R] [useR! 2019] last chance submit your abstracts
Message-ID: <f4b010fc-8366-c8c7-2925-0f2997582add@inra.fr>

Dear R-users,
(with my apologies for multiple posting: this message was already posted 
on R-announce but accidentally not forwarded to R-help)

The list of selected tutorials for useR! 2019 is out
http://user2019.r-project.org/tutorials/. If you want to join them, you
can still submit a proposal for a talk, a lightning talk or a poster.
The submission process closes March 1st. See information at:
http://user2019.r-project.org/abstracts/.

Students (Master, PhD, postdocs) and young employees also have the
opportunity to engage the Data Challenge
https://user2019.r-project.org/datathon/. The winner will be offered a
free ticket for the conference.

Don't miss the opportunity to join!

Best regards,
Nathalie Vialaneix
On behalf of the organization committee for useR! 2019


** new email: nathalie.vialaneix at inra.fr **
-- 
Nathalie Vialaneix
Directrice de Recherche, Statistique
Unit? MIAT, INRA de Toulouse
Email : nathalie.vialaneix at inra.fr
Web : http://www.nathalievialaneix.eu


From verz@n| @end|ng |rom m@th@c@|@cuny@edu  Mon Feb 25 18:11:44 2019
From: verz@n| @end|ng |rom m@th@c@|@cuny@edu (John Verzani)
Date: Mon, 25 Feb 2019 12:11:44 -0500
Subject: [R] New issue of The R Journal
Message-ID: <CADx-4xAMoR2mi3DC2Ae=-bE0E05sqA+WVWZHxwMZWOXf15D_JA@mail.gmail.com>

Dear All,

The latest issue of The R Journal is now available at:

https://journal.r-project.org/archive/2018-2/

Many thanks to all contributors - especially reviewers and authors.

Regards,

John Verzani
CUNY/College of Staten Island

-- 
John Verzani
Department of Mathematics
College of Staten Island, CUNY
verzani at math.csi.cuny.edu

	[[alternative HTML version deleted]]

_______________________________________________
R-announce at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From ju||eey@w @end|ng |rom y@hoo@c@  Tue Feb 26 23:36:58 2019
From: ju||eey@w @end|ng |rom y@hoo@c@ (Julie Lee-Yaw)
Date: Tue, 26 Feb 2019 22:36:58 +0000 (UTC)
Subject: [R] Help formulating gamm with repeated measures and spatial
 autocorrelation using mgcv package
References: <1608878369.5618013.1551220618482.ref@mail.yahoo.com>
Message-ID: <1608878369.5618013.1551220618482@mail.yahoo.com>

(Also posted on StackExchange but submitting to R-help to reach more potential experts)I am using the gamm function in the mgcv package in R to specify a model that predicts abundance with respect to elevation and year based on repeated measures from several sites. My overarching question is how abundance changes over elevation, knowing and accounting for the fact that it also changes over time. GAMM is used because I do not want to restrict the relationship of either elevation or year with abundance to be linear. I expect the data to be spatially autocorrelated within years (sites closer together will have more similar values than sites further apart within each year) and would like to account for this.

I'm having trouble figuring out the proper full formulation of the GAMM.

The variables are: 1) abundance (continuous response) 2) elevation (numeric, continuous predictor) 3) year (numeric, continuous predictor as we expect a gradual change in abundance over time) 4) spatial autocorrelation term (based on the lat/long of the sites where data were collected) 5) site (random factor)

The model formulation I have come up with is:

model<-gamm(abundance ~ s(elevation, bs = "tp")+s(year, bs = "tp"), correlation=corSpher(form = ~ y+x|year), random=list(site=~1), data=input.data)

My questions are:

Is the repeated measures nature of the data adequately accounted for here or do I also need to include year in my random effects (nest site in year)?

Have I properly taken year into account in my spatial autocorrelation term?
Finally, when running this model, I get an error and a warning:Error in corFactor.corSpatial(object):??Na/NaN/Inf in foreign function call (arg 1)?In addition: Warning message:?In min(unlist(attr(object,"covariate"))):?no non-missing arguments to min, returning InfNot sure why this is happening. If I run the GAMM without the random effect of site, it does not return the error...

Not sure why this is happening. If I run the GAMM without the random effect of site, it does not return the error...

Many thanks in advance for any help!


	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Wed Feb 27 02:05:04 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Tue, 26 Feb 2019 17:05:04 -0800
Subject: [R] Help formulating gamm with repeated measures and spatial
 autocorrelation using mgcv package
In-Reply-To: <1608878369.5618013.1551220618482@mail.yahoo.com>
References: <1608878369.5618013.1551220618482.ref@mail.yahoo.com>
 <1608878369.5618013.1551220618482@mail.yahoo.com>
Message-ID: <50b9adc0-e791-bd62-edcf-5638f522b9ec@comcast.net>

Crossposting is explicitly advised against in the rhelp posting guide. 
Furthermore, this is the wrong list for this sort of question within the 
r-project hierarchy of mailing lists. I do not believe an answer is 
possible in the absence of the data (and note the request .... some 
would say demand... for reproducible code and data at the end of every 
posting from the Rhelp server). And finally rhelp is a plain text list, 
so posting with html is also deprecated.

Please read the posting guide. I'm just repeating relevant points that 
are already discussed there at greater length and clarity.

David

On 2/26/19 2:36 PM, Julie Lee-Yaw via R-help wrote:
> (Also posted on StackExchange but submitting to R-help to reach more potential experts)I am using the gamm function in the mgcv package in R to specify a model that predicts abundance with respect to elevation and year based on repeated measures from several sites. My overarching question is how abundance changes over elevation, knowing and accounting for the fact that it also changes over time. GAMM is used because I do not want to restrict the relationship of either elevation or year with abundance to be linear. I expect the data to be spatially autocorrelated within years (sites closer together will have more similar values than sites further apart within each year) and would like to account for this.
>
> I'm having trouble figuring out the proper full formulation of the GAMM.
>
> The variables are: 1) abundance (continuous response) 2) elevation (numeric, continuous predictor) 3) year (numeric, continuous predictor as we expect a gradual change in abundance over time) 4) spatial autocorrelation term (based on the lat/long of the sites where data were collected) 5) site (random factor)
>
> The model formulation I have come up with is:
>
> model<-gamm(abundance ~ s(elevation, bs = "tp")+s(year, bs = "tp"), correlation=corSpher(form = ~ y+x|year), random=list(site=~1), data=input.data)
>
> My questions are:
>
> Is the repeated measures nature of the data adequately accounted for here or do I also need to include year in my random effects (nest site in year)?
>
> Have I properly taken year into account in my spatial autocorrelation term?
> Finally, when running this model, I get an error and a warning:Error in corFactor.corSpatial(object):??Na/NaN/Inf in foreign function call (arg 1)?In addition: Warning message:?In min(unlist(attr(object,"covariate"))):?no non-missing arguments to min, returning InfNot sure why this is happening. If I run the GAMM without the random effect of site, it does not return the error...
>
> Not sure why this is happening. If I run the GAMM without the random effect of site, it does not return the error...
>
> Many thanks in advance for any help!
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m|@ojpm @end|ng |rom gm@||@com  Wed Feb 27 04:34:39 2019
From: m|@ojpm @end|ng |rom gm@||@com (John)
Date: Wed, 27 Feb 2019 11:34:39 +0800
Subject: [R] Replacing each NA with the most recent non-NA prior to it
Message-ID: <CABcx46AtGZgZPqS8OS86thviMg9Uo-3V9a0+eZfVHwytdgjRyw@mail.gmail.com>

If I use the na.locf function to replace each NA with the most recent
non-NA prior to it, then

> na.locf(c(NA,NA,1,4,NA,2))
[1] 1 1 1 4 4 2

I want to keep leading NA's, and this is what I want
 NA NA 1 4 4 2

How can I do it?

The following do not work:

> na.locf(c(NA,NA,1,4,NA,2), na.rm=FALSE)
Error in na.locf(c(NA, NA, 1, 4, NA, 2), na.rm = FALSE) :
  unused argument (na.rm = FALSE)
> na.locf(c(NA,NA,1,4,NA,2), na.rm=TRUE)
Error in na.locf(c(NA, NA, 1, 4, NA, 2), na.rm = TRUE) :
  unused argument (na.rm = TRUE)


Thank you very much!

John

	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Wed Feb 27 05:06:09 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Tue, 26 Feb 2019 23:06:09 -0500
Subject: [R] Replacing each NA with the most recent non-NA prior to it
In-Reply-To: <CABcx46AtGZgZPqS8OS86thviMg9Uo-3V9a0+eZfVHwytdgjRyw@mail.gmail.com>
References: <CABcx46AtGZgZPqS8OS86thviMg9Uo-3V9a0+eZfVHwytdgjRyw@mail.gmail.com>
Message-ID: <d8113e82-0e04-434a-4ba0-0da7adc88079@gmail.com>

On 26/02/2019 10:34 p.m., John wrote:
> If I use the na.locf function to replace each NA with the most recent
> non-NA prior to it, then
> 
>> na.locf(c(NA,NA,1,4,NA,2))
> [1] 1 1 1 4 4 2
> 
> I want to keep leading NA's, and this is what I want
>   NA NA 1 4 4 2
> 
> How can I do it?
> 
> The following do not work:
> 
>> na.locf(c(NA,NA,1,4,NA,2), na.rm=FALSE)
> Error in na.locf(c(NA, NA, 1, 4, NA, 2), na.rm = FALSE) :
>    unused argument (na.rm = FALSE)
>> na.locf(c(NA,NA,1,4,NA,2), na.rm=TRUE)
> Error in na.locf(c(NA, NA, 1, 4, NA, 2), na.rm = TRUE) :
>    unused argument (na.rm = TRUE)
> 
> 
> Thank you very much!


There are at least two packages (zoo and imputeTS) which have na.locf 
functions.  The one in zoo does what you want:

 > zoo::na.locf(c(NA,NA,1,4,NA,2), na.rm=FALSE)
[1] NA NA  1  4  4  2

Duncan Murdoch


From m|@ojpm @end|ng |rom gm@||@com  Wed Feb 27 06:48:24 2019
From: m|@ojpm @end|ng |rom gm@||@com (John)
Date: Wed, 27 Feb 2019 13:48:24 +0800
Subject: [R] Replacing each NA with the most recent non-NA prior to it
In-Reply-To: <d8113e82-0e04-434a-4ba0-0da7adc88079@gmail.com>
References: <CABcx46AtGZgZPqS8OS86thviMg9Uo-3V9a0+eZfVHwytdgjRyw@mail.gmail.com>
 <d8113e82-0e04-434a-4ba0-0da7adc88079@gmail.com>
Message-ID: <CABcx46AhMU+Z9u1DTb1ASnMVRzg=BTW7pS0ba73fL-XwCvuiPQ@mail.gmail.com>

Thanks! That works!!

Duncan Murdoch <murdoch.duncan at gmail.com> ? 2019?2?27? ?? ??12:06???

> On 26/02/2019 10:34 p.m., John wrote:
> > If I use the na.locf function to replace each NA with the most recent
> > non-NA prior to it, then
> >
> >> na.locf(c(NA,NA,1,4,NA,2))
> > [1] 1 1 1 4 4 2
> >
> > I want to keep leading NA's, and this is what I want
> >   NA NA 1 4 4 2
> >
> > How can I do it?
> >
> > The following do not work:
> >
> >> na.locf(c(NA,NA,1,4,NA,2), na.rm=FALSE)
> > Error in na.locf(c(NA, NA, 1, 4, NA, 2), na.rm = FALSE) :
> >    unused argument (na.rm = FALSE)
> >> na.locf(c(NA,NA,1,4,NA,2), na.rm=TRUE)
> > Error in na.locf(c(NA, NA, 1, 4, NA, 2), na.rm = TRUE) :
> >    unused argument (na.rm = TRUE)
> >
> >
> > Thank you very much!
>
>
> There are at least two packages (zoo and imputeTS) which have na.locf
> functions.  The one in zoo does what you want:
>
>  > zoo::na.locf(c(NA,NA,1,4,NA,2), na.rm=FALSE)
> [1] NA NA  1  4  4  2
>
> Duncan Murdoch
>
>
>

	[[alternative HTML version deleted]]


From wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n|  Wed Feb 27 12:42:24 2019
From: wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n| (Viechtbauer, Wolfgang (SP))
Date: Wed, 27 Feb 2019 11:42:24 +0000
Subject: [R] differences between meat and metafor packages
In-Reply-To: <CAM9Qe4hqiO8nftxuVynd60As4WYy+06jEd-n7f4paK31ZpoOVA@mail.gmail.com>
References: <CAM9Qe4gp4xG9uKx8L1sANHXhuR-AHttHEQVpn8fVME-boXY3AQ@mail.gmail.com>
 <c6f0e5e7e2eb48ae8dffe63e55f8eb4b@UM-MAIL3214.unimaas.nl>
 <CAM9Qe4hqiO8nftxuVynd60As4WYy+06jEd-n7f4paK31ZpoOVA@mail.gmail.com>
Message-ID: <37ebdacf13734c5d95408bf5cd85023b@UM-MAIL3214.unimaas.nl>

Hi Greg,

Please cc the mailing list when responding.

No, this is not correct. The argument 'vi' is for the *variances*, not the standard errors. So, either use:

res <- rma(HR, sei=SE, data=a, method="REML", slab=paste(a$study), digits=3)

or

res <- rma(HR, vi=SE^2, data=a, method="REML", slab=paste(a$study), digits=3)

Best,
Wolfgang

-----Original Message-----
From: greg holly [mailto:mak.hholly at gmail.com] 
Sent: Monday, 25 February, 2019 21:20
To: Viechtbauer, Wolfgang (SP)
Subject: Re: [R] differences between meat and metafor packages

Hi?Wolfgang;

Thanks so much for this. It is much appreciated. Essentially, I run the following with metafor. Is not this correct? Still, should I specify sei in program?

Regards,
Greg.
### Spesify log hazard ratios and sampling variances
a$yi <- a$HR
a$vi <- a$SE
### meta-analysis based on all trials
res <- rma(yi, vi, data=a, method="REML", slab=paste(a$study), digits=3)

On Mon, Feb 25, 2019 at 1:34 PM Viechtbauer, Wolfgang (SP) <wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
The second argument (called 'vi') in rma() is for the variances. If you have SEs, then use the 'sei' argument:

res <- rma(HR, sei=SE, data=a)

Best,
Wolfgang

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of greg holly
Sent: Monday, 25 February, 2019 18:40
To: r-help mailing list
Subject: [R] differences between meat and metafor packages

Hi all;

I have got different results (CI and Q value for heterogeneity, tau) on the
same data when I run meta and metafor for hazard ratio with a random
effects model. The basic programs for both are given below. What can cause?

Regards,
Greg

Metafor
?res <- rma(HR, SE, data=a)
predict(res, transf=exp)

Meta
?metagen(HR, SE, sm="HR", data=a)

From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Wed Feb 27 14:38:33 2019
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Wed, 27 Feb 2019 05:38:33 -0800 (PST)
Subject: [R] Which dependency list to build first?
In-Reply-To: <9F750BDE-2792-4294-B10A-0E8687E5B32B@icloud.com>
References: <alpine.LNX.2.20.1902241104040.31842@salmo.appl-ecosys.com>
 <23669.4832.639467.519820@stat.math.ethz.ch>
 <alpine.LNX.2.20.1902260532430.16864@salmo.appl-ecosys.com>
 <9F750BDE-2792-4294-B10A-0E8687E5B32B@icloud.com>
Message-ID: <alpine.LNX.2.20.1902270537130.11593@salmo.appl-ecosys.com>

On Wed, 27 Feb 2019, Rainer Krug wrote:

> I am sure, you have uninstalled the package completely and tried it again?
>
> Have you tried at looking at the shared objects required by `
> /usr/lib/R/library/later/libs/later.so` using ldd ?
>
> I had similar problems on Mac (with other packages) and re-installing some
> home-brew packages fixed it.

Rainer,

In truth, I've ignored the issue for now. :-) Regardless, I will take your
advice and report results when completed.

Thanks very much,

Rich


From m@k@hho||y @end|ng |rom gm@||@com  Wed Feb 27 16:03:56 2019
From: m@k@hho||y @end|ng |rom gm@||@com (greg holly)
Date: Wed, 27 Feb 2019 09:03:56 -0600
Subject: [R] differences between meat and metafor packages
In-Reply-To: <37ebdacf13734c5d95408bf5cd85023b@UM-MAIL3214.unimaas.nl>
References: <CAM9Qe4gp4xG9uKx8L1sANHXhuR-AHttHEQVpn8fVME-boXY3AQ@mail.gmail.com>
 <c6f0e5e7e2eb48ae8dffe63e55f8eb4b@UM-MAIL3214.unimaas.nl>
 <CAM9Qe4hqiO8nftxuVynd60As4WYy+06jEd-n7f4paK31ZpoOVA@mail.gmail.com>
 <37ebdacf13734c5d95408bf5cd85023b@UM-MAIL3214.unimaas.nl>
Message-ID: <CAM9Qe4ga-pHG5XEc8sEZ-R+fCR=Y4pmPq+463Or2fOsAmvKGWg@mail.gmail.com>

H? Wolfgang?

I do appreciate for this info. Very helpful. And I do apologize for the
personal email. It was done mistakenly.

Regards,

Greg

On Wed, Feb 27, 2019 at 5:42 AM Viechtbauer, Wolfgang (SP) <
wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:

> Hi Greg,
>
> Please cc the mailing list when responding.
>
> No, this is not correct. The argument 'vi' is for the *variances*, not the
> standard errors. So, either use:
>
> res <- rma(HR, sei=SE, data=a, method="REML", slab=paste(a$study),
> digits=3)
>
> or
>
> res <- rma(HR, vi=SE^2, data=a, method="REML", slab=paste(a$study),
> digits=3)
>
> Best,
> Wolfgang
>
> -----Original Message-----
> From: greg holly [mailto:mak.hholly at gmail.com]
> Sent: Monday, 25 February, 2019 21:20
> To: Viechtbauer, Wolfgang (SP)
> Subject: Re: [R] differences between meat and metafor packages
>
> Hi Wolfgang;
>
> Thanks so much for this. It is much appreciated. Essentially, I run the
> following with metafor. Is not this correct? Still, should I specify sei in
> program?
>
> Regards,
> Greg.
> ### Spesify log hazard ratios and sampling variances
> a$yi <- a$HR
> a$vi <- a$SE
> ### meta-analysis based on all trials
> res <- rma(yi, vi, data=a, method="REML", slab=paste(a$study), digits=3)
>
> On Mon, Feb 25, 2019 at 1:34 PM Viechtbauer, Wolfgang (SP) <
> wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
> The second argument (called 'vi') in rma() is for the variances. If you
> have SEs, then use the 'sei' argument:
>
> res <- rma(HR, sei=SE, data=a)
>
> Best,
> Wolfgang
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of greg holly
> Sent: Monday, 25 February, 2019 18:40
> To: r-help mailing list
> Subject: [R] differences between meat and metafor packages
>
> Hi all;
>
> I have got different results (CI and Q value for heterogeneity, tau) on the
> same data when I run meta and metafor for hazard ratio with a random
> effects model. The basic programs for both are given below. What can cause?
>
> Regards,
> Greg
>
> Metafor
>  res <- rma(HR, SE, data=a)
> predict(res, transf=exp)
>
> Meta
>  metagen(HR, SE, sm="HR", data=a)
>

	[[alternative HTML version deleted]]


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Wed Feb 27 18:39:51 2019
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Wed, 27 Feb 2019 09:39:51 -0800 (PST)
Subject: [R] Which dependency list to build first?
In-Reply-To: <9F750BDE-2792-4294-B10A-0E8687E5B32B@icloud.com>
References: <alpine.LNX.2.20.1902241104040.31842@salmo.appl-ecosys.com>
 <23669.4832.639467.519820@stat.math.ethz.ch>
 <alpine.LNX.2.20.1902260532430.16864@salmo.appl-ecosys.com>
 <9F750BDE-2792-4294-B10A-0E8687E5B32B@icloud.com>
Message-ID: <alpine.LNX.2.20.1902270937380.11593@salmo.appl-ecosys.com>

On Wed, 27 Feb 2019, Rainer Krug wrote:

> I am sure, you have uninstalled the package completely and tried it again?

Rainer,

Yes. Just did so with the same result: R could not load the shared library.

> Have you tried at looking at the shared objects required by
> `/usr/lib/R/library/later/libs/later.so` using ldd ?

Because later was removed and could not be rebuilt, there is no later.so to
be queried for dependencies.

I'll look again at later's dependencies and ensure they're all rebuilt.

Regards,

Rich


From wdun|@p @end|ng |rom t|bco@com  Wed Feb 27 19:33:11 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Wed, 27 Feb 2019 10:33:11 -0800
Subject: [R] Which dependency list to build first?
In-Reply-To: <alpine.LNX.2.20.1902270937380.11593@salmo.appl-ecosys.com>
References: <alpine.LNX.2.20.1902241104040.31842@salmo.appl-ecosys.com>
 <23669.4832.639467.519820@stat.math.ethz.ch>
 <alpine.LNX.2.20.1902260532430.16864@salmo.appl-ecosys.com>
 <9F750BDE-2792-4294-B10A-0E8687E5B32B@icloud.com>
 <alpine.LNX.2.20.1902270937380.11593@salmo.appl-ecosys.com>
Message-ID: <CAF8bMcbhaR=8WcGLrmTM8=nsCHATooP8vN=fF1=B7gsLXAwvrg@mail.gmail.com>

Add the --no-test-load option to the install command and the unloadable .so
file should be left there so you can look at its dependencies with, e.g.,
'R CMD ldd .../libs/later.so'.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Wed, Feb 27, 2019 at 9:40 AM Rich Shepard <rshepard at appl-ecosys.com>
wrote:

> On Wed, 27 Feb 2019, Rainer Krug wrote:
>
> > I am sure, you have uninstalled the package completely and tried it
> again?
>
> Rainer,
>
> Yes. Just did so with the same result: R could not load the shared library.
>
> > Have you tried at looking at the shared objects required by
> > `/usr/lib/R/library/later/libs/later.so` using ldd ?
>
> Because later was removed and could not be rebuilt, there is no later.so to
> be queried for dependencies.
>
> I'll look again at later's dependencies and ensure they're all rebuilt.
>
> Regards,
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Wed Feb 27 20:13:57 2019
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Wed, 27 Feb 2019 11:13:57 -0800 (PST)
Subject: [R] Which dependency list to build first?
In-Reply-To: <CAF8bMcbhaR=8WcGLrmTM8=nsCHATooP8vN=fF1=B7gsLXAwvrg@mail.gmail.com>
References: <alpine.LNX.2.20.1902241104040.31842@salmo.appl-ecosys.com>
 <23669.4832.639467.519820@stat.math.ethz.ch>
 <alpine.LNX.2.20.1902260532430.16864@salmo.appl-ecosys.com>
 <9F750BDE-2792-4294-B10A-0E8687E5B32B@icloud.com>
 <alpine.LNX.2.20.1902270937380.11593@salmo.appl-ecosys.com>
 <CAF8bMcbhaR=8WcGLrmTM8=nsCHATooP8vN=fF1=B7gsLXAwvrg@mail.gmail.com>
Message-ID: <alpine.LNX.2.20.1902271109100.11593@salmo.appl-ecosys.com>

On Wed, 27 Feb 2019, William Dunlap wrote:

> Add the --no-test-load option to the install command and the unloadable .so
> file should be left there so you can look at its dependencies with, e.g.,
> 'R CMD ldd .../libs/later.so'.

Bill,

Using the syntax 'install.packages("later") --no-test-load' still produces
the same result:

** testing if installed package can be loaded
Error: package or namespace load failed for ?later? in dyn.load(file, DLLpath = DLLpath, ...):
  unable to load shared object '/usr/lib/R/library/later/libs/later.so':
   /usr/lib/R/library/later/libs/later.so: undefined symbol: __atomic_fetch_add_8
Error: loading failed
Execution halted
ERROR: loading failed

The later package was installed so why it's not updating now puzzles me. I'm
running R-3.5.2 on Slackware-14.2.

Regards,

Rich


From wdun|@p @end|ng |rom t|bco@com  Wed Feb 27 20:45:59 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Wed, 27 Feb 2019 11:45:59 -0800
Subject: [R] Which dependency list to build first?
In-Reply-To: <alpine.LNX.2.20.1902271109100.11593@salmo.appl-ecosys.com>
References: <alpine.LNX.2.20.1902241104040.31842@salmo.appl-ecosys.com>
 <23669.4832.639467.519820@stat.math.ethz.ch>
 <alpine.LNX.2.20.1902260532430.16864@salmo.appl-ecosys.com>
 <9F750BDE-2792-4294-B10A-0E8687E5B32B@icloud.com>
 <alpine.LNX.2.20.1902270937380.11593@salmo.appl-ecosys.com>
 <CAF8bMcbhaR=8WcGLrmTM8=nsCHATooP8vN=fF1=B7gsLXAwvrg@mail.gmail.com>
 <alpine.LNX.2.20.1902271109100.11593@salmo.appl-ecosys.com>
Message-ID: <CAF8bMcYuF74ERUsSPZD_xjwFEG81KYoKGm+DRYqBkhzqDN4YTQ@mail.gmail.com>

    Using the syntax 'install.packages("later") --no-test-load'

The syntax is either
    install.packages("later", type="source", INSTALL_opts="--no-test-load")
from within R (perhaps with repos=NULL if from a local directory) or
    R CMD INSTALL --no-test-load later
from outside of R, where 'later' must be a directory.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Wed, Feb 27, 2019 at 11:14 AM Rich Shepard <rshepard at appl-ecosys.com>
wrote:

> On Wed, 27 Feb 2019, William Dunlap wrote:
>
> > Add the --no-test-load option to the install command and the unloadable
> .so
> > file should be left there so you can look at its dependencies with, e.g.,
> > 'R CMD ldd .../libs/later.so'.
>
> Bill,
>
> Using the syntax 'install.packages("later") --no-test-load' still produces
> the same result:
>
> ** testing if installed package can be loaded
> Error: package or namespace load failed for ?later? in dyn.load(file,
> DLLpath = DLLpath, ...):
>   unable to load shared object '/usr/lib/R/library/later/libs/later.so':
>    /usr/lib/R/library/later/libs/later.so: undefined symbol:
> __atomic_fetch_add_8
> Error: loading failed
> Execution halted
> ERROR: loading failed
>
> The later package was installed so why it's not updating now puzzles me.
> I'm
> running R-3.5.2 on Slackware-14.2.
>
> Regards,
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Wed Feb 27 20:55:33 2019
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Wed, 27 Feb 2019 11:55:33 -0800 (PST)
Subject: [R] Which dependency list to build first?
In-Reply-To: <CAF8bMcYuF74ERUsSPZD_xjwFEG81KYoKGm+DRYqBkhzqDN4YTQ@mail.gmail.com>
References: <alpine.LNX.2.20.1902241104040.31842@salmo.appl-ecosys.com>
 <23669.4832.639467.519820@stat.math.ethz.ch>
 <alpine.LNX.2.20.1902260532430.16864@salmo.appl-ecosys.com>
 <9F750BDE-2792-4294-B10A-0E8687E5B32B@icloud.com>
 <alpine.LNX.2.20.1902270937380.11593@salmo.appl-ecosys.com>
 <CAF8bMcbhaR=8WcGLrmTM8=nsCHATooP8vN=fF1=B7gsLXAwvrg@mail.gmail.com>
 <alpine.LNX.2.20.1902271109100.11593@salmo.appl-ecosys.com>
 <CAF8bMcYuF74ERUsSPZD_xjwFEG81KYoKGm+DRYqBkhzqDN4YTQ@mail.gmail.com>
Message-ID: <alpine.LNX.2.20.1902271147240.11593@salmo.appl-ecosys.com>

On Wed, 27 Feb 2019, William Dunlap wrote:

> The syntax is either
>    install.packages("later", type="source", INSTALL_opts="--no-test-load")
> from within R (perhaps with repos=NULL if from a local directory) or
>    R CMD INSTALL --no-test-load later
> from outside of R, where 'later' must be a directory.

Bill,

Thank you very much. I read ?install.packages and saw all the options but
had no idea what to use.

Boy howdy! Removing the load testing allowed the package to install:

installing to /usr/lib/R/library/later/libs
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** installing vignettes
* DONE (later)

Is it worth exploring why testing loading failed here for this package?

Best regards,

Rich


From wdun|@p @end|ng |rom t|bco@com  Wed Feb 27 21:11:05 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Wed, 27 Feb 2019 12:11:05 -0800
Subject: [R] Which dependency list to build first?
In-Reply-To: <alpine.LNX.2.20.1902271147240.11593@salmo.appl-ecosys.com>
References: <alpine.LNX.2.20.1902241104040.31842@salmo.appl-ecosys.com>
 <23669.4832.639467.519820@stat.math.ethz.ch>
 <alpine.LNX.2.20.1902260532430.16864@salmo.appl-ecosys.com>
 <9F750BDE-2792-4294-B10A-0E8687E5B32B@icloud.com>
 <alpine.LNX.2.20.1902270937380.11593@salmo.appl-ecosys.com>
 <CAF8bMcbhaR=8WcGLrmTM8=nsCHATooP8vN=fF1=B7gsLXAwvrg@mail.gmail.com>
 <alpine.LNX.2.20.1902271109100.11593@salmo.appl-ecosys.com>
 <CAF8bMcYuF74ERUsSPZD_xjwFEG81KYoKGm+DRYqBkhzqDN4YTQ@mail.gmail.com>
 <alpine.LNX.2.20.1902271147240.11593@salmo.appl-ecosys.com>
Message-ID: <CAF8bMcaq_zZpy9cfi2nwGoCS2QNhadN7q2sqbrRhA5iu3S1v0g@mail.gmail.com>

The package will not load.  The only reason to do test load is to examine
why the package's .so file cannot be loaded.  We know there is at least one
function or data symbol that it cannot find,  __atomic_fetch_add_8, wihch
may be from boost::atomic.  The ldd command may give some hints about
missing libraries.
Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Wed, Feb 27, 2019 at 11:56 AM Rich Shepard <rshepard at appl-ecosys.com>
wrote:

> On Wed, 27 Feb 2019, William Dunlap wrote:
>
> > The syntax is either
> >    install.packages("later", type="source",
> INSTALL_opts="--no-test-load")
> > from within R (perhaps with repos=NULL if from a local directory) or
> >    R CMD INSTALL --no-test-load later
> > from outside of R, where 'later' must be a directory.
>
> Bill,
>
> Thank you very much. I read ?install.packages and saw all the options but
> had no idea what to use.
>
> Boy howdy! Removing the load testing allowed the package to install:
>
> installing to /usr/lib/R/library/later/libs
> ** R
> ** inst
> ** byte-compile and prepare package for lazy loading
> ** help
> *** installing help indices
> ** building package indices
> ** installing vignettes
> * DONE (later)
>
> Is it worth exploring why testing loading failed here for this package?
>
> Best regards,
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Wed Feb 27 21:51:30 2019
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Wed, 27 Feb 2019 12:51:30 -0800 (PST)
Subject: [R] Which dependency list to build first?
In-Reply-To: <CAF8bMcaq_zZpy9cfi2nwGoCS2QNhadN7q2sqbrRhA5iu3S1v0g@mail.gmail.com>
References: <alpine.LNX.2.20.1902241104040.31842@salmo.appl-ecosys.com>
 <23669.4832.639467.519820@stat.math.ethz.ch>
 <alpine.LNX.2.20.1902260532430.16864@salmo.appl-ecosys.com>
 <9F750BDE-2792-4294-B10A-0E8687E5B32B@icloud.com>
 <alpine.LNX.2.20.1902270937380.11593@salmo.appl-ecosys.com>
 <CAF8bMcbhaR=8WcGLrmTM8=nsCHATooP8vN=fF1=B7gsLXAwvrg@mail.gmail.com>
 <alpine.LNX.2.20.1902271109100.11593@salmo.appl-ecosys.com>
 <CAF8bMcYuF74ERUsSPZD_xjwFEG81KYoKGm+DRYqBkhzqDN4YTQ@mail.gmail.com>
 <alpine.LNX.2.20.1902271147240.11593@salmo.appl-ecosys.com>
 <CAF8bMcaq_zZpy9cfi2nwGoCS2QNhadN7q2sqbrRhA5iu3S1v0g@mail.gmail.com>
Message-ID: <alpine.LNX.2.20.1902271245210.11593@salmo.appl-ecosys.com>

On Wed, 27 Feb 2019, William Dunlap wrote:

> The package will not load. The only reason to do test load is to examine
> why the package's .so file cannot be loaded. We know there is at least one
> function or data symbol that it cannot find, __atomic_fetch_add_8, which
> may be from boost::atomic. The ldd command may give some hints about
> missing libraries.

Bill,

Yes, it does. libR.so is not found:
# ldd later.so
 	linux-gate.so.1 (0xb76df000)
 	libR.so => not found

However, R runs and libR.so is found here:
/usr/lib/R/lib/libR.so
and was last accessed
-rwxr-xr-x 1 root root 3331940 Dec 23 10:00 /usr/lib/R/lib/libR.so*

Should I rebuild and reinstall R-3.5.2?

Thanks again,

Rich


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Wed Feb 27 21:55:27 2019
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard Comcast)
Date: Wed, 27 Feb 2019 15:55:27 -0500
Subject: [R] Error trapping in R
Message-ID: <FD7AB93F-9088-4DA8-A49D-2AB370D27715@comcast.net>

What is the recommended way to trap errors in R? My main need is to be able to trap an error and then skip a section of code if an error has occurred. In VB for Excel I used the ?On Error goto  .....? construct to do this.

Bernard
Sent from my iPhone so please excuse the spelling!"

From murdoch@dunc@n @end|ng |rom gm@||@com  Wed Feb 27 22:05:47 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Wed, 27 Feb 2019 16:05:47 -0500
Subject: [R] Error trapping in R
In-Reply-To: <FD7AB93F-9088-4DA8-A49D-2AB370D27715@comcast.net>
References: <FD7AB93F-9088-4DA8-A49D-2AB370D27715@comcast.net>
Message-ID: <28521902-45a2-822f-ff70-ca9e44856cb3@gmail.com>

On 27/02/2019 3:55 p.m., Bernard Comcast wrote:
> What is the recommended way to trap errors in R? My main need is to be able to trap an error and then skip a section of code if an error has occurred. In VB for Excel I used the ?On Error goto  .....? construct to do this.

The recommended way is to use tryCatch() around the expression you're 
evaluating.  A simpler, less flexible alternative is try().  The Excel 
version sounds a bit more like try().  You'd use it like this:

   value <- try({ x <- 1
                  y <- someFunction(x)
                  someOtherFunction(y)
                })
   if (inherits(value, "try-error")) {
     cat ("something went wrong.  There's information in value about 
what happened.")
   } else {
     cat ("value is fine, there was no error.")
   }

Duncan Murdoch


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed Feb 27 22:13:40 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 27 Feb 2019 21:13:40 +0000
Subject: [R] Error trapping in R
In-Reply-To: <FD7AB93F-9088-4DA8-A49D-2AB370D27715@comcast.net>
References: <FD7AB93F-9088-4DA8-A49D-2AB370D27715@comcast.net>
Message-ID: <a920a825-e402-339f-0c26-08891866820e@sapo.pt>

Hello,

You can trap errors with ?try or ?tryCatch.
Example:


result <- vector(mode = "list", length = 5)
for(i in 1:5){
   result[[i]] <- tryCatch(if(i == 3) stop("This is an error") else 2*i + 1,
            error = function(e) e)
}

result

for(i in seq_along(result)) {
   err <- inherits(result[[i]], "error")
   print(err)
}


Hope this helps,

Rui Barradas

?s 20:55 de 27/02/2019, Bernard Comcast escreveu:
> What is the recommended way to trap errors in R? My main need is to be able to trap an error and then skip a section of code if an error has occurred. In VB for Excel I used the ?On Error goto  .....? construct to do this.
> 
> Bernard
> Sent from my iPhone so please excuse the spelling!"
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From wdun|@p @end|ng |rom t|bco@com  Wed Feb 27 22:24:58 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Wed, 27 Feb 2019 13:24:58 -0800
Subject: [R] Which dependency list to build first?
In-Reply-To: <alpine.LNX.2.20.1902271245210.11593@salmo.appl-ecosys.com>
References: <alpine.LNX.2.20.1902241104040.31842@salmo.appl-ecosys.com>
 <23669.4832.639467.519820@stat.math.ethz.ch>
 <alpine.LNX.2.20.1902260532430.16864@salmo.appl-ecosys.com>
 <9F750BDE-2792-4294-B10A-0E8687E5B32B@icloud.com>
 <alpine.LNX.2.20.1902270937380.11593@salmo.appl-ecosys.com>
 <CAF8bMcbhaR=8WcGLrmTM8=nsCHATooP8vN=fF1=B7gsLXAwvrg@mail.gmail.com>
 <alpine.LNX.2.20.1902271109100.11593@salmo.appl-ecosys.com>
 <CAF8bMcYuF74ERUsSPZD_xjwFEG81KYoKGm+DRYqBkhzqDN4YTQ@mail.gmail.com>
 <alpine.LNX.2.20.1902271147240.11593@salmo.appl-ecosys.com>
 <CAF8bMcaq_zZpy9cfi2nwGoCS2QNhadN7q2sqbrRhA5iu3S1v0g@mail.gmail.com>
 <alpine.LNX.2.20.1902271245210.11593@salmo.appl-ecosys.com>
Message-ID: <CAF8bMcZiO0N6Owfjjbn+q9vsxbxzjz9+xmpo9Lau4ez3H66a_Q@mail.gmail.com>

Did you use 'R CMD ldd .../later.so', as I recommended?

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Wed, Feb 27, 2019 at 12:51 PM Rich Shepard <rshepard at appl-ecosys.com>
wrote:

> On Wed, 27 Feb 2019, William Dunlap wrote:
>
> > The package will not load. The only reason to do test load is to examine
> > why the package's .so file cannot be loaded. We know there is at least
> one
> > function or data symbol that it cannot find, __atomic_fetch_add_8, which
> > may be from boost::atomic. The ldd command may give some hints about
> > missing libraries.
>
> Bill,
>
> Yes, it does. libR.so is not found:
> # ldd later.so
>         linux-gate.so.1 (0xb76df000)
>         libR.so => not found
>
> However, R runs and libR.so is found here:
> /usr/lib/R/lib/libR.so
> and was last accessed
> -rwxr-xr-x 1 root root 3331940 Dec 23 10:00 /usr/lib/R/lib/libR.so*
>
> Should I rebuild and reinstall R-3.5.2?
>
> Thanks again,
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Wed Feb 27 22:44:04 2019
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Wed, 27 Feb 2019 13:44:04 -0800 (PST)
Subject: [R] Which dependency list to build first?
In-Reply-To: <CAF8bMcZiO0N6Owfjjbn+q9vsxbxzjz9+xmpo9Lau4ez3H66a_Q@mail.gmail.com>
References: <alpine.LNX.2.20.1902241104040.31842@salmo.appl-ecosys.com>
 <23669.4832.639467.519820@stat.math.ethz.ch>
 <alpine.LNX.2.20.1902260532430.16864@salmo.appl-ecosys.com>
 <9F750BDE-2792-4294-B10A-0E8687E5B32B@icloud.com>
 <alpine.LNX.2.20.1902270937380.11593@salmo.appl-ecosys.com>
 <CAF8bMcbhaR=8WcGLrmTM8=nsCHATooP8vN=fF1=B7gsLXAwvrg@mail.gmail.com>
 <alpine.LNX.2.20.1902271109100.11593@salmo.appl-ecosys.com>
 <CAF8bMcYuF74ERUsSPZD_xjwFEG81KYoKGm+DRYqBkhzqDN4YTQ@mail.gmail.com>
 <alpine.LNX.2.20.1902271147240.11593@salmo.appl-ecosys.com>
 <CAF8bMcaq_zZpy9cfi2nwGoCS2QNhadN7q2sqbrRhA5iu3S1v0g@mail.gmail.com>
 <alpine.LNX.2.20.1902271245210.11593@salmo.appl-ecosys.com>
 <CAF8bMcZiO0N6Owfjjbn+q9vsxbxzjz9+xmpo9Lau4ez3H66a_Q@mail.gmail.com>
Message-ID: <alpine.LNX.2.20.1902271341260.11593@salmo.appl-ecosys.com>

On Wed, 27 Feb 2019, William Dunlap wrote:

> Did you use 'R CMD ldd .../later.so', as I recommended?

Bill,

I ran ldd externally on /usr/lib/R/library/lib/later/later.so and posted the
missing library. Just now I ran the above command and it did not find
later.so:

# R CMD ldd /usr/lib/R/library/later.so
ldd: /usr/lib/R/library/later.so: No such file or directory

Strange. And confusing to me.

Regards,

Rich


From g||ted|||e2014 @end|ng |rom gm@||@com  Wed Feb 27 22:53:16 2019
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Wed, 27 Feb 2019 22:53:16 +0100
Subject: [R] Randomization Test
In-Reply-To: <CAC8ss32HYyCKXXy-U9R8i_eceXxrSVsjSezo=ELF7xchoGiL5w@mail.gmail.com>
References: <CAC8ss32K6KNVYUgiJmmw6UParhgjyQJAp9t8q2CdMjKakzHOrg@mail.gmail.com>
 <BL0PR01MB4132FD4418FADCA82CBDC3349A690@BL0PR01MB4132.prod.exchangelabs.com>
 <CAC8ss32HYyCKXXy-U9R8i_eceXxrSVsjSezo=ELF7xchoGiL5w@mail.gmail.com>
Message-ID: <CAC8ss31pdJ9SisAcHYVHcp+670tKKdSQvj5V-HKaXnLyE_0J9A@mail.gmail.com>

Dear Kind List,

I am still battling with this. I have, however, made some progress
with the suggestions of Micheal and others. At least, I have a better
picture of what I want to do now as I will attempt a detailed
description here.

I am aware I should show you just a small part of my code and data.
But when I copied out a small portion and run to see what you get when
I send that,  I was not satisfied with the signal displayed. The epoch
analysis averages data and is quite sensitive to leveraging,
especially if a small sample is used.

So please permit/exercise patience  me to display the series of epoch
that give the averaged valued used. You can just run the code and see
the signal of interest. Here is the code and the data:

dta <- read.table( text ="n CR
 -5 8969
-4 8932
-3 8929
-2 8916
-1 8807
0 8449
1 8484
2 8148
3 8282
4 8305
5 8380
6 8530
7 8642
8 8780
9 8890
10 8962
-5 8929
-4 8916
-3 8807
-2 8449
-1 8484
0 8148
1 8282
2 8305
3 8380
4 8530
5 8642
6 8780
7 8890
8 8962
9 8949
10 8974
-5 8744
-4 8786
-3 8828
-2 8807
-1 8716
0 8520
1 8634
2 8640
3 8636
4 8658
5 8699
6 8682
7 8621
8 8626
9 8660
10 8737
-5 8592
-4 8612
-3 8628
-2 8589
-1 8318
0 8264
1 8294
2 8410
3 8442
4 8416
5 8389
6 8412
7 8453
8 8563
9 8581
10 8613
-5 8264
-4 8294
-3 8410
-2 8442
-1 8416
0 8389
1 8412
2 8453
3 8563
4 8581
5 8613
6 8647
7 8613
8 8508
9 7829
10 7499
-5 8613
-4 8647
-3 8613
-2 8508
-1 7829
0 7499
1 8213
2 7993
3 7821
4 8316
5 8460
6 8533
7 8584
8 8586
9 8567
10 8573
-5 8508
-4 7829
-3 7499
-2 8213
-1 7993
0 7821
1 8316
2 8460
3 8533
4 8584
5 8586
6 8567
7 8573
8 8617
9 8591
10 8661
-5 8851
-4 8893
-3 8858
-2 8803
-1 8790
0 8468
1 8545
2 8570
3 8568
4 8624
5 8669
6 8236
7 8190
8 8313
9 8389
10 8421
-5 8803
-4 8790
-3 8468
-2 8545
-1 8570
0 8568
1 8624
2 8669
3 8236
4 8190
5 8313
6 8389
7 8421
8 8468
9 8537
10 8580
-5 8570
-4 8568
-3 8624
-2 8669
-1 8236
0 8190
1 8313
2 8389
3 8421
4 8468
5 8537
6 8580
7 8605
8 8646
9 8690
10 8770
-5 8690
-4 8770
-3 8799
-2 8821
-1 8666
0 8539
1 8633
2 8617
3 8651
4 8693
5 8715
6 8738
7 8716
8 8677
9 8680
10 8700
-5 8756
-4 8632
-3 8662
-2 8596
-1 8552
0 8502
1 8633
2 8702
3 8745
4 8730
5 8708
6 8817
7 8724
8 8688
9 8693
10 8746
-5 8926
-4 8888
-3 8798
-2 8651
-1 8678
0 8578
1 8593
2 8598
3 8526
4 8181
5 8204
6 8373
7 8599
8 8773
9 8784
10 8746
-5 8678
-4 8578
-3 8593
-2 8598
-1 8526
0 8181
1 8204
2 8373
3 8599
4 8773
5 8784
6 8746
7 8747
8 8757
9 8749
10 8767
-5 8757
-4 8749
-3 8767
-2 8754
-1 8695
0 8631
1 8661
2 8653
3 8588
4 8562
5 8613
6 8595
7 8498
8 8404
9 8507
10 8599
-5 8695
-4 8631
-3 8661
-2 8653
-1 8588
0 8562
1 8613
2 8595
3 8498
4 8404
5 8507
6 8599
7 8592
8 8600
9 8637
10 8635
-5 8588
-4 8562
-3 8613
-2 8595
-1 8498
0 8404
1 8507
2 8599
3 8592
4 8600
5 8637
6 8635
7 8632
8 8674
9 8644
10 8687
-5 8595
-4 8498
-3 8404
-2 8507
-1 8599
0 8592
1 8600
2 8637
3 8635
4 8632
5 8674
6 8644
7 8687
8 8721
9 8747
10 8748
-5 8599
-4 8592
-3 8600
-2 8637
-1 8635
0 8632
1 8674
2 8644
3 8687
4 8721
5 8747
6 8748
7 8739
8 8763
9 8792
10 8558
-5 8600
-4 8637
-3 8635
-2 8632
-1 8674
0 8644
1 8687
2 8721
3 8747
4 8748
5 8739
6 8763
7 8792
8 8558
9 8442
10 8555
-5 8748
-4 8739
-3 8763
-2 8792
-1 8558
0 8442
1 8555
2 8622
3 8634
4 8698
5 8732
6 8713
7 8732
8 8681
9 8615
10 8624
-5 8698
-4 8732
-3 8713
-2 8732
-1 8681
0 8615
1 8624
2 8649
3 8656
4 8678
5 8723
6 8693
7 8548
8 7803
9 7801
10 7724
-5 8723
-4 8693
-3 8548
-2 7803
-1 7801
0 7724
1 7910
2 7829
3 7995
4 8156
5 8307
6 8377
7 8465
8 8506
9 8516
10 8536
-5 8548
-4 7803
-3 7801
-2 7724
-1 7910
0 7829
1 7995
2 8156
3 8307
4 8377
5 8465
6 8506
7 8516
8 8536
9 8574
10 8623
-5 8821
-4 8856
-3 8798
-2 8772
-1 8705
0 8682
1 8691
2 8720
3 8727
4 8789
5 8821
6 8811
7 8841
8 8849
9 8849
10 8860
-5 8835
-4 8829
-3 8826
-2 8799
-1 8775
0 8756
1 8793
2 8814
3 8847
4 8838
5 8833
6 8841
7 8847
8 8903
9 8933
10 8918
-5 8890
-4 8875
-3 8874
-2 8865
-1 8891
0 8839
1 8853
2 8888
3 8884
4 8890
5 8889
6 8839
7 8879
8 8908
9 8924
10 8882
-5 8853
-4 8888
-3 8884
-2 8890
-1 8889
0 8839
1 8879
2 8908
3 8924
4 8882
5 8910
6 8903
7 8859
8 8858
9 8863
10 8847
-5 8924
-4 8882
-3 8910
-2 8903
-1 8859
0 8858
1 8863
2 8847
3 8883
4 8869
5 8878
6 8897
7 8922
8 8895
9 8858
10 8858
-5 8910
-4 8903
-3 8859
-2 8858
-1 8863
0 8847
1 8883
2 8869
3 8878
4 8897
5 8922
6 8895
7 8858
8 8858
9 8736
10 8905
-5 8859
-4 8858
-3 8863
-2 8847
-1 8883
0 8869
1 8878
2 8897
3 8922
4 8895
5 8858
6 8858
7 8736
8 8905
9 8935
10 8974
-5 8897
-4 8922
-3 8895
-2 8858
-1 8858
0 8736
1 8905
2 8935
3 8974
4 8946
5 8952
6 9010
7 8980
8 8976
9 8970
10 8961
-5 9376
-4 9336
-3 9311
-2 9287
-1 9221
0 9087
1 9132
2 9175
3 9166
4 9240
5 9264
6 9271
7 9319
8 9324
9 9333
10 9351
-5 9287
-4 9221
-3 9087
-2 9132
-1 9175
0 9166
1 9240
2 9264
3 9271
4 9319
5 9324
6 9333
7 9351
8 9362
9 9385
10 9354
-5 9407
-4 9414
-3 9354
-2 9298
-1 9319
0 9147
1 9178
2 9196
3 9258
4 9303
5 9369
6 9382
7 9375
8 9389
9 9376
10 9264
-5 9386
-4 9396
-3 9424
-2 9391
-1 9284
0 9267
1 9278
2 9318
3 9334
4 9275
5 9306
6 9308
7 9358
8 9335
9 9373
10 9379
-5 9284
-4 9267
-3 9278
-2 9318
-1 9334
0 9275
1 9306
2 9308
3 9358
4 9335
5 9373
6 9379
7 9355
8 9340
9 9327
10 9320
-5 9327
-4 9320
-3 9315
-2 9336
-1 9371
0 9259
1 9330
2 9355
3 9334
4 9353
5 9370
6 9394
7 9400
8 9318
9 9037
10 8994
-5 9394
-4 9400
-3 9318
-2 9037
-1 8994
0 8943
1 8964
2 8997
3 9158
4 8964
5 8564
6 8736
7 8818
8 8938
9 9034
10 9132
-5 8943
-4 8964
-3 8997
-2 9158
-1 8964
0 8564
1 8736
2 8818
3 8938
4 9034
5 9132
6 9167
7 9200
8 9257
9 9266
10 9306
-5 9338
-4 9354
-3 9372
-2 9338
-1 9308
0 9282
1 9324
2 9318
3 9342
4 9370
5 9331
6 9327
7 9338
8 9381
9 9394
10 9332
-5 9372
-4 9338
-3 9308
-2 9282
-1 9324
0 9318
1 9342
2 9370
3 9331
4 9327
5 9338
6 9381
7 9394
8 9332
9 9331
10 9293
-5 9338
-4 9381
-3 9394
-2 9332
-1 9331
0 9293
1 9309
2 9325
3 9406
4 9409
5 9413
6 9426
7 9440
8 9449
9 9512
10 9494
-5 9361
-4 9354
-3 9299
-2 9282
-1 9250
0 9242
1 9254
2 9321
3 9390
4 9414
5 9435
6 9437
7 9426
8 9398
9 9383
10 9354
-5 9365
-4 9421
-3 9416
-2 9355
-1 9338
0 9324
1 9325
2 9322
3 9319
4 9381
5 9315
6 9314
7 9359
8 9403
9 9419
10 9474
-5 9355
-4 9338
-3 9324
-2 9325
-1 9322
0 9319
1 9381
2 9315
3 9314
4 9359
5 9403
6 9419
7 9474
8 9525
9 9501
10 9447
-5 9325
-4 9322
-3 9319
-2 9381
-1 9315
0 9314
1 9359
2 9403
3 9419
4 9474
5 9525
6 9501
7 9447
8 9424
9 9396
10 9388
-5 9447
-4 9424
-3 9396
-2 9388
-1 9396
0 9346
1 9358
2 9353
3 9350
4 9378
5 9372
6 9354
7 9349
8 9392
9 9440
10 9467
-5 9388
-4 9396
-3 9346
-2 9358
-1 9353
0 9350
1 9378
2 9372
3 9354
4 9349
5 9392
6 9440
7 9467
8 9519
9 9550
10 9565
-5 9353
-4 9350
-3 9378
-2 9372
-1 9354
0 9349
1 9392
2 9440
3 9467
4 9519
5 9550
6 9565
7 9565
8 9497
9 9500
10 9472
-5 9522
-4 9529
-3 9492
-2 9432
-1 9382
0 9355
1 9361
2 9350
3 9382
4 9451
5 9491
6 9506
7 9529
8 9543
9 9556
10 9553
-5 9492
-4 9432
-3 9382
-2 9355
-1 9361
0 9350
1 9382
2 9451
3 9491
4 9506
5 9529
6 9543
7 9556
8 9553
9 9502
10 9470
-5 9551
-4 9505
-3 9389
-2 9406
-1 9377
0 9284
1 9365
2 9424
3 9412
4 9403
5 9384
6 9394
7 9404
8 9413
9 9407
10 9405
-5 9579
-4 9576
-3 9543
-2 9451
-1 9421
0 9361
1 9394
2 9400
3 9387
4 9366
5 9346
6 9360
7 9385
8 9435
9 9443
10 9430
-5 9361
-4 9394
-3 9400
-2 9387
-1 9366
0 9346
1 9360
2 9385
3 9435
4 9443
5 9430
6 9454
7 9531
8 9547
9 9581
10 9540
-5 9510
-4 9546
-3 9564
-2 9508
-1 9422
0 9369
1 9395
2 9438
3 9423
4 9392
5 9368
6 9366
7 9348
8 9340
9 9375
10 9391
-5 9423
-4 9392
-3 9368
-2 9366
-1 9348
0 9340
1 9375
2 9391
3 9466
4 9545
5 9574
6 9564
7 9527
8 9513
9 9494
10 9542
-5 9511
-4 9491
-3 9457
-2 9453
-1 9402
0 9382
1 9407
2 9437
3 9403
4 9404
5 9425
6 9486
7 9457
8 9451
9 9423
10 9401
-5 9425
-4 9486
-3 9457
-2 9451
-1 9423
0 9401
1 9429
2 9422
3 9431
4 9462
5 9475
6 9474
7 9487
8 9493
9 9495
10 9499
-5 9404
-4 9385
-3 9363
-2 9399
-1 9411
0 9355
1 9357
2 9363
3 9382
4 9387
5 9408
6 9429
7 9456
8 9487
9 9526
10 9487
-5 9493
-4 9439
-3 9400
-2 9378
-1 9371
0 9369
1 9374
2 9305
3 9298
4 9298
5 9325
6 9381
7 9477
8 9508
9 9496
10 9517
-5 9371
-4 9369
-3 9374
-2 9305
-1 9298
0 9298
1 9325
2 9381
3 9477
4 9508
5 9496
6 9517
7 9561
8 9570
9 9546
10 9544
-5 9510
-4 9506
-3 9530
-2 9441
-1 9427
0 9393
1 9420
2 9444
3 9468
4 9484
5 9525
6 9542
7 9557
8 9548
9 9550
10 9593
-5 9589
-4 9598
-3 9527
-2 9417
-1 9390
0 9374
1 9386
2 9407
3 9453
4 9447
5 9419
6 9386
7 9373
8 9364
9 9376
10 9389
-5 9453
-4 9447
-3 9419
-2 9386
-1 9373
0 9364
1 9376
2 9389
3 9376
4 9375
5 9370
6 9391
7 9458
8 9446
9 9456
10 9463
-5 9364
-4 9376
-3 9389
-2 9376
-1 9375
0 9370
1 9391
2 9458
3 9446
4 9456
5 9463
6 9500
7 9486
8 9474
9 9495
10 9531
-5 9491
-4 9441
-3 9388
-2 9380
-1 9369
0 9354
1 9367
2 9369
3 9341
4 9305
5 9308
6 9324
7 9385
8 9451
9 9496
10 9527
-5 9369
-4 9354
-3 9367
-2 9369
-1 9341
0 9305
1 9308
2 9324
3 9385
4 9451
5 9496
6 9527
7 9544
8 9543
9 9535
10 9536
-5 9586
-4 9583
-3 9572
-2 9533
-1 9454
0 9392
1 9420
2 9451
3 9475
4 9514
5 9561
6 9542
7 9502
8 9461
9 9468
10 9463
-5 9587
-4 9562
-3 9530
-2 9445
-1 9404
0 9395
1 9417
2 9449
3 9467
4 9470
5 9524
6 9512
7 9448
8 9398
9 9431
10 9467
-5 9467
-4 9470
-3 9524
-2 9512
-1 9448
0 9398
1 9431
2 9467
3 9490
4 9517
5 9526
6 9574
7 9573
8 9562
9 9563
10 9566
",header=TRUE)

 data<-matrix(c(dta$CR),ncol=71)
A<-matrix(rep(-5:10,71))
B<-matrix(data)

 oodf<-data.frame(A,B)
 a<--5:10
oodf<-data.frame(A,B)
library(plotrix)
std.error<-function(x) return(sd(x)/(sum(!is.na(x))))
oomean<-as.vector(by(oodf$B,oodf$A,mean))
oose<-as.vector(by(oodf$B,oodf$A,std.error))
plot(-5:10,oomean,type="l",ylim=c(8890,9100),
 )
A<-oomean-1.96*oose
 B<-oomean+1.96*oose
lines(a,A,col="red")
 lines(a,B,col="red")

 My Question:
I wish to conduct a randomization test of significance (90 and 99
percentile) of the reductions/decreases as displayed by the signal.

I am attempting using:
x<-sample(8890:9500,1136,replace=T )

to generate the random numbers, where 8890, 9500 and 1136 are the
minimum  and maximum of the signal and 1136 the length of sample data.
Q1: Please how do I generate many samples as x above, say up to 5000
or 10,000? I manually generated and stored as x1,x2, x3 up to x100.

Q2: Please how do I use this randomly generated numbers to test the
statistical significance level of the signal generated by
plot(-5:10,oomean,type="l",ylim=c(8890,9100),  )?

I wish to test for 90% and 99% percentile.

I am sorry that this is too long.

Many thanks for your kind contributions

Best
Ogbos







On Sun, Feb 10, 2019 at 3:55 PM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>
> Dear Michael,
> This is great! Thank you.
>
> I have not really got any response other than yours.
>
> I have long before now included what I have in a paper submitted to a journal.
>
> I am awaiting the feedback of the reviewer. I will compare the
> comments with your input here and determine the corrections to make
> and probably return to the list for additional help.
>
> Best wishes
> Ogbos
>
> On Fri, Feb 8, 2019 at 4:31 PM Meyners, Michael <meyners.m at pg.com> wrote:
> >
> > Ogbos,
> >
> > You do not seem to have received a reply over the list yet, which might be due to the fact that this seems rather a stats than an R question. Neither got your attachment (Figure) through - see posting guide.
> >
> > I'm not familiar with epoch analysis, so not sure what exactly you are doing / trying to achieve, but some general thoughts:
> >
> > * You do NOT want to restrict your re-randomizations in a way that "none of the dates corresponds with the ones in the real event" - actually, as a general principle, the true data must be an admissible re-randomization as well. You seem to have excluded that (and a lot of other randomizations at the same time which might have occurred, i.e. dates 1 and 2 reversed but all others the same), thereby rendering the test invalid. Any restrictions you have on your re-randomizations must've applied to the original randomization as well.
> > * If you have rather observational data (which I suspect, but not sure), Edgington & Onghena (2007) would rather refer to this as a permutation test - the difference being that you have to make strong assumptions (similar to parametric tests) on the nature of the data, which are designed-in to be true for randomization tests. It might be a merely linguistic discrimination, but it is important to note which assumptions have to be (implicitly) made.
> > * I'm not sure what you mean by "mean differences" of the events - is that two groups you are comparing? If so, that seems reasonable, but just make sure the test statistic you use is reasonable and sensitive against the alternatives you are mostly interested in. The randomization/permutation test will never proof that, e.g., means are significantly different, but only that there is SOME difference. By selecting the appropriate test statistic, you can influence what will pop up more easily and what not, but you can never be sure (unless you make strong assumptions about everything else, like in many parametric tests).
> > * For any test statistic, you would then determine the proportion of its values among the 5000 samples where it is as large or larger than the one observed (or as small or smaller, or either, depending on the nature of the test statistic and whether you aim for a one- or a two-sided test). That is your p value. If small enough, conclude significance. At least conceptually important: The observed test statistic is always part of the re-randomization (i.e. your 5000) - so you truly only do 4999 plus the one you observed. Otherwise the test may be more or less liberal. Your p value is hence no smaller than 1/n, where n is the total number of samples you looked at (including the observed one), a p value of 0 is not possible in randomization tests (nor in other tests, of course).
> >
> > I hope this is helpful, but you will need to go through these and refer to your own setup to check whether you adhered to the principles or not, which is impossible for me to judge based on the information provided (and I won't be able to look at excessive code to check either).
> >
> > Michael
> >
> > > -----Original Message-----
> > > From: R-help <r-help-bounces at r-project.org> On Behalf Of Ogbos Okike
> > > Sent: Montag, 28. Januar 2019 19:42
> > > To: r-help <r-help at r-project.org>
> > > Subject: [R] Randomization Test
> > >
> > > Dear Contributors,
> > >
> > > I conducting epoch analysis. I tried to test the significance of my result using
> > > randomization test.
> > >
> > > Since I have 71 events, I randomly selected another 71 events, making sure
> > > that none of the dates in the random events corresponds with the ones in
> > > the real event.
> > >
> > > Following the code I found here
> > > (https://www.uvm.edu/~dhowell/StatPages/R/RandomizationTestsWithR/R
> > > andom2Sample/TwoIndependentSamplesR.html),
> > > I combined these two data set and used them to generate another 5000
> > > events. I then plotted the graph of the mean differences for the 5000
> > > randomly generated events. On the graph, I indicated the region of the
> > > mean difference between the real 71 epoch and the randomly selected 71
> > > epoch.
> > >
> > > Since the two tail test shows that the mean difference falls at the extreme of
> > > the randomly selected events, I concluded that my result is statistically
> > > significant.
> > >
> > >
> > >
> > > I am attaching the graph to assistance you in you suggestions.
> > >
> > > I can attach both my code and the real and randomly generated events if you
> > > ask for it.
> > >
> > > My request is that you help me to understand if I am on the right track or no.
> > > This is the first time I am doing this and except the experts decide, I am not
> > > quite sure whether I am right or not.
> > >
> > > Many thanks for your kind concern.
> > >
> > > Best
> > > Ogbos
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-
> > > guide.html
> > > and provide commented, minimal, self-contained, reproducible code.


From eb@15242 @end|ng |rom gm@||@com  Thu Feb 28 00:03:27 2019
From: eb@15242 @end|ng |rom gm@||@com (Ed Siefker)
Date: Wed, 27 Feb 2019 17:03:27 -0600
Subject: [R] inverse of which()
Message-ID: <CALRb-odXXpf3Otiz64XHibbD+3r842p5SBV9A2qiXVuC1bkrcA@mail.gmail.com>

Given a vector of booleans, chich() will return indices that are TRUE.

Given a vector of indices, how can I get a vector of booleans?

My intent is to do logical operations on the output of grep().  Maybe
there's a better way to do this?

Thanks
-Ed


From dc@r|@on @end|ng |rom t@mu@edu  Thu Feb 28 00:14:48 2019
From: dc@r|@on @end|ng |rom t@mu@edu (David L Carlson)
Date: Wed, 27 Feb 2019 23:14:48 +0000
Subject: [R] inverse of which()
In-Reply-To: <CALRb-odXXpf3Otiz64XHibbD+3r842p5SBV9A2qiXVuC1bkrcA@mail.gmail.com>
References: <CALRb-odXXpf3Otiz64XHibbD+3r842p5SBV9A2qiXVuC1bkrcA@mail.gmail.com>
Message-ID: <ae069de31ca040d3a267dc723d021416@tamu.edu>

I'm not sure I completely understand your question. Would using grepl() instead of grep() let you do what you want?

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Ed Siefker
Sent: Wednesday, February 27, 2019 5:03 PM
To: r-help <r-help at r-project.org>
Subject: [R] inverse of which()

Given a vector of booleans, chich() will return indices that are TRUE.

Given a vector of indices, how can I get a vector of booleans?

My intent is to do logical operations on the output of grep().  Maybe
there's a better way to do this?

Thanks
-Ed

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Feb 28 00:17:34 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 27 Feb 2019 15:17:34 -0800
Subject: [R] inverse of which()
In-Reply-To: <CALRb-odXXpf3Otiz64XHibbD+3r842p5SBV9A2qiXVuC1bkrcA@mail.gmail.com>
References: <CALRb-odXXpf3Otiz64XHibbD+3r842p5SBV9A2qiXVuC1bkrcA@mail.gmail.com>
Message-ID: <FF6E778B-7B9E-4D2A-9D6C-6327BEB821D0@dcn.davis.ca.us>

use grepl?

On February 27, 2019 3:03:27 PM PST, Ed Siefker <ebs15242 at gmail.com> wrote:
>Given a vector of booleans, chich() will return indices that are TRUE.
>
>Given a vector of indices, how can I get a vector of booleans?
>
>My intent is to do logical operations on the output of grep().  Maybe
>there's a better way to do this?
>
>Thanks
>-Ed
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Thu Feb 28 00:56:31 2019
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard Comcast)
Date: Wed, 27 Feb 2019 18:56:31 -0500
Subject: [R] Error trapping in R
In-Reply-To: <28521902-45a2-822f-ff70-ca9e44856cb3@gmail.com>
References: <FD7AB93F-9088-4DA8-A49D-2AB370D27715@comcast.net>
 <28521902-45a2-822f-ff70-ca9e44856cb3@gmail.com>
Message-ID: <20FCE966-E0C4-4F9D-B473-C014830B29BC@comcast.net>

Thanks

Bernard
Sent from my iPhone so please excuse the spelling!"

> On Feb 27, 2019, at 4:05 PM, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> 
>> On 27/02/2019 3:55 p.m., Bernard Comcast wrote:
>> What is the recommended way to trap errors in R? My main need is to be able to trap an error and then skip a section of code if an error has occurred. In VB for Excel I used the ?On Error goto  .....? construct to do this.
> 
> The recommended way is to use tryCatch() around the expression you're evaluating.  A simpler, less flexible alternative is try().  The Excel version sounds a bit more like try().  You'd use it like this:
> 
>  value <- try({ x <- 1
>                 y <- someFunction(x)
>                 someOtherFunction(y)
>               })
>  if (inherits(value, "try-error")) {
>    cat ("something went wrong.  There's information in value about what happened.")
>  } else {
>    cat ("value is fine, there was no error.")
>  }
> 
> Duncan Murdoch


From btupper @end|ng |rom b|ge|ow@org  Thu Feb 28 01:59:05 2019
From: btupper @end|ng |rom b|ge|ow@org (Ben Tupper)
Date: Wed, 27 Feb 2019 19:59:05 -0500
Subject: [R] Randomization Test
In-Reply-To: <CAC8ss31pdJ9SisAcHYVHcp+670tKKdSQvj5V-HKaXnLyE_0J9A@mail.gmail.com>
References: <CAC8ss32K6KNVYUgiJmmw6UParhgjyQJAp9t8q2CdMjKakzHOrg@mail.gmail.com>
 <BL0PR01MB4132FD4418FADCA82CBDC3349A690@BL0PR01MB4132.prod.exchangelabs.com>
 <CAC8ss32HYyCKXXy-U9R8i_eceXxrSVsjSezo=ELF7xchoGiL5w@mail.gmail.com>
 <CAC8ss31pdJ9SisAcHYVHcp+670tKKdSQvj5V-HKaXnLyE_0J9A@mail.gmail.com>
Message-ID: <94909111-A379-483E-AF12-75F611CF614B@bigelow.org>

Hi,

I'm not very clear on what you are trying to achieve, but I think you could try the following for your Q1...

> Q1: Please how do I generate many samples as x above, say up to 5000
> or 10,000? I manually generated and stored as x1,x2, x3 up to x100.

ndta = nrow(dta)
x0 = 8890
x1 = 9500
xx = seq(from = x0, to = x1, by = 1)
N_many = 50 # make 5000 etc as required
m <- sapply(   seq_len(N_many), function(i) sample(xx, ndta, replace = TRUE))

str(m)
# int [1:1136, 1:50] 9147 8904 9062 8946 9330 9056 9239 9284 9290 9441 ...

summary(as.vector(m))
#   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#   8890    9043    9195    9196    9348    9500 

m[1:5, 1:5]
#     [,1] [,2] [,3] [,4] [,5]
#[1,] 9147 9124 9341 8999 9268
#[2,] 8904 9246 9087 9041 8943
#[3,] 9062 9184 9061 9119 9350
#[4,] 8946 9242 8932 9306 9270
#[5,] 9330 8979 9437 9030 9333

Each sample set of length ndta (in this case ndta = 1136) is found in a column of the matrix.   Is that what you are looking for?

Ben

> On Feb 27, 2019, at 4:53 PM, Ogbos Okike <giftedlife2014 at gmail.com> wrote:
> 
> Dear Kind List,
> 
> I am still battling with this. I have, however, made some progress
> with the suggestions of Micheal and others. At least, I have a better
> picture of what I want to do now as I will attempt a detailed
> description here.
> 
> I am aware I should show you just a small part of my code and data.
> But when I copied out a small portion and run to see what you get when
> I send that,  I was not satisfied with the signal displayed. The epoch
> analysis averages data and is quite sensitive to leveraging,
> especially if a small sample is used.
> 
> So please permit/exercise patience  me to display the series of epoch
> that give the averaged valued used. You can just run the code and see
> the signal of interest. Here is the code and the data:
> 
> dta <- read.table( text ="n CR
> -5 8969
> -4 8932
> -3 8929
> -2 8916
> -1 8807
> 0 8449
> 1 8484
> 2 8148
> 3 8282
> 4 8305
> 5 8380
> 6 8530
> 7 8642
> 8 8780
> 9 8890
> 10 8962
> -5 8929
> -4 8916
> -3 8807
> -2 8449
> -1 8484
> 0 8148
> 1 8282
> 2 8305
> 3 8380
> 4 8530
> 5 8642
> 6 8780
> 7 8890
> 8 8962
> 9 8949
> 10 8974
> -5 8744
> -4 8786
> -3 8828
> -2 8807
> -1 8716
> 0 8520
> 1 8634
> 2 8640
> 3 8636
> 4 8658
> 5 8699
> 6 8682
> 7 8621
> 8 8626
> 9 8660
> 10 8737
> -5 8592
> -4 8612
> -3 8628
> -2 8589
> -1 8318
> 0 8264
> 1 8294
> 2 8410
> 3 8442
> 4 8416
> 5 8389
> 6 8412
> 7 8453
> 8 8563
> 9 8581
> 10 8613
> -5 8264
> -4 8294
> -3 8410
> -2 8442
> -1 8416
> 0 8389
> 1 8412
> 2 8453
> 3 8563
> 4 8581
> 5 8613
> 6 8647
> 7 8613
> 8 8508
> 9 7829
> 10 7499
> -5 8613
> -4 8647
> -3 8613
> -2 8508
> -1 7829
> 0 7499
> 1 8213
> 2 7993
> 3 7821
> 4 8316
> 5 8460
> 6 8533
> 7 8584
> 8 8586
> 9 8567
> 10 8573
> -5 8508
> -4 7829
> -3 7499
> -2 8213
> -1 7993
> 0 7821
> 1 8316
> 2 8460
> 3 8533
> 4 8584
> 5 8586
> 6 8567
> 7 8573
> 8 8617
> 9 8591
> 10 8661
> -5 8851
> -4 8893
> -3 8858
> -2 8803
> -1 8790
> 0 8468
> 1 8545
> 2 8570
> 3 8568
> 4 8624
> 5 8669
> 6 8236
> 7 8190
> 8 8313
> 9 8389
> 10 8421
> -5 8803
> -4 8790
> -3 8468
> -2 8545
> -1 8570
> 0 8568
> 1 8624
> 2 8669
> 3 8236
> 4 8190
> 5 8313
> 6 8389
> 7 8421
> 8 8468
> 9 8537
> 10 8580
> -5 8570
> -4 8568
> -3 8624
> -2 8669
> -1 8236
> 0 8190
> 1 8313
> 2 8389
> 3 8421
> 4 8468
> 5 8537
> 6 8580
> 7 8605
> 8 8646
> 9 8690
> 10 8770
> -5 8690
> -4 8770
> -3 8799
> -2 8821
> -1 8666
> 0 8539
> 1 8633
> 2 8617
> 3 8651
> 4 8693
> 5 8715
> 6 8738
> 7 8716
> 8 8677
> 9 8680
> 10 8700
> -5 8756
> -4 8632
> -3 8662
> -2 8596
> -1 8552
> 0 8502
> 1 8633
> 2 8702
> 3 8745
> 4 8730
> 5 8708
> 6 8817
> 7 8724
> 8 8688
> 9 8693
> 10 8746
> -5 8926
> -4 8888
> -3 8798
> -2 8651
> -1 8678
> 0 8578
> 1 8593
> 2 8598
> 3 8526
> 4 8181
> 5 8204
> 6 8373
> 7 8599
> 8 8773
> 9 8784
> 10 8746
> -5 8678
> -4 8578
> -3 8593
> -2 8598
> -1 8526
> 0 8181
> 1 8204
> 2 8373
> 3 8599
> 4 8773
> 5 8784
> 6 8746
> 7 8747
> 8 8757
> 9 8749
> 10 8767
> -5 8757
> -4 8749
> -3 8767
> -2 8754
> -1 8695
> 0 8631
> 1 8661
> 2 8653
> 3 8588
> 4 8562
> 5 8613
> 6 8595
> 7 8498
> 8 8404
> 9 8507
> 10 8599
> -5 8695
> -4 8631
> -3 8661
> -2 8653
> -1 8588
> 0 8562
> 1 8613
> 2 8595
> 3 8498
> 4 8404
> 5 8507
> 6 8599
> 7 8592
> 8 8600
> 9 8637
> 10 8635
> -5 8588
> -4 8562
> -3 8613
> -2 8595
> -1 8498
> 0 8404
> 1 8507
> 2 8599
> 3 8592
> 4 8600
> 5 8637
> 6 8635
> 7 8632
> 8 8674
> 9 8644
> 10 8687
> -5 8595
> -4 8498
> -3 8404
> -2 8507
> -1 8599
> 0 8592
> 1 8600
> 2 8637
> 3 8635
> 4 8632
> 5 8674
> 6 8644
> 7 8687
> 8 8721
> 9 8747
> 10 8748
> -5 8599
> -4 8592
> -3 8600
> -2 8637
> -1 8635
> 0 8632
> 1 8674
> 2 8644
> 3 8687
> 4 8721
> 5 8747
> 6 8748
> 7 8739
> 8 8763
> 9 8792
> 10 8558
> -5 8600
> -4 8637
> -3 8635
> -2 8632
> -1 8674
> 0 8644
> 1 8687
> 2 8721
> 3 8747
> 4 8748
> 5 8739
> 6 8763
> 7 8792
> 8 8558
> 9 8442
> 10 8555
> -5 8748
> -4 8739
> -3 8763
> -2 8792
> -1 8558
> 0 8442
> 1 8555
> 2 8622
> 3 8634
> 4 8698
> 5 8732
> 6 8713
> 7 8732
> 8 8681
> 9 8615
> 10 8624
> -5 8698
> -4 8732
> -3 8713
> -2 8732
> -1 8681
> 0 8615
> 1 8624
> 2 8649
> 3 8656
> 4 8678
> 5 8723
> 6 8693
> 7 8548
> 8 7803
> 9 7801
> 10 7724
> -5 8723
> -4 8693
> -3 8548
> -2 7803
> -1 7801
> 0 7724
> 1 7910
> 2 7829
> 3 7995
> 4 8156
> 5 8307
> 6 8377
> 7 8465
> 8 8506
> 9 8516
> 10 8536
> -5 8548
> -4 7803
> -3 7801
> -2 7724
> -1 7910
> 0 7829
> 1 7995
> 2 8156
> 3 8307
> 4 8377
> 5 8465
> 6 8506
> 7 8516
> 8 8536
> 9 8574
> 10 8623
> -5 8821
> -4 8856
> -3 8798
> -2 8772
> -1 8705
> 0 8682
> 1 8691
> 2 8720
> 3 8727
> 4 8789
> 5 8821
> 6 8811
> 7 8841
> 8 8849
> 9 8849
> 10 8860
> -5 8835
> -4 8829
> -3 8826
> -2 8799
> -1 8775
> 0 8756
> 1 8793
> 2 8814
> 3 8847
> 4 8838
> 5 8833
> 6 8841
> 7 8847
> 8 8903
> 9 8933
> 10 8918
> -5 8890
> -4 8875
> -3 8874
> -2 8865
> -1 8891
> 0 8839
> 1 8853
> 2 8888
> 3 8884
> 4 8890
> 5 8889
> 6 8839
> 7 8879
> 8 8908
> 9 8924
> 10 8882
> -5 8853
> -4 8888
> -3 8884
> -2 8890
> -1 8889
> 0 8839
> 1 8879
> 2 8908
> 3 8924
> 4 8882
> 5 8910
> 6 8903
> 7 8859
> 8 8858
> 9 8863
> 10 8847
> -5 8924
> -4 8882
> -3 8910
> -2 8903
> -1 8859
> 0 8858
> 1 8863
> 2 8847
> 3 8883
> 4 8869
> 5 8878
> 6 8897
> 7 8922
> 8 8895
> 9 8858
> 10 8858
> -5 8910
> -4 8903
> -3 8859
> -2 8858
> -1 8863
> 0 8847
> 1 8883
> 2 8869
> 3 8878
> 4 8897
> 5 8922
> 6 8895
> 7 8858
> 8 8858
> 9 8736
> 10 8905
> -5 8859
> -4 8858
> -3 8863
> -2 8847
> -1 8883
> 0 8869
> 1 8878
> 2 8897
> 3 8922
> 4 8895
> 5 8858
> 6 8858
> 7 8736
> 8 8905
> 9 8935
> 10 8974
> -5 8897
> -4 8922
> -3 8895
> -2 8858
> -1 8858
> 0 8736
> 1 8905
> 2 8935
> 3 8974
> 4 8946
> 5 8952
> 6 9010
> 7 8980
> 8 8976
> 9 8970
> 10 8961
> -5 9376
> -4 9336
> -3 9311
> -2 9287
> -1 9221
> 0 9087
> 1 9132
> 2 9175
> 3 9166
> 4 9240
> 5 9264
> 6 9271
> 7 9319
> 8 9324
> 9 9333
> 10 9351
> -5 9287
> -4 9221
> -3 9087
> -2 9132
> -1 9175
> 0 9166
> 1 9240
> 2 9264
> 3 9271
> 4 9319
> 5 9324
> 6 9333
> 7 9351
> 8 9362
> 9 9385
> 10 9354
> -5 9407
> -4 9414
> -3 9354
> -2 9298
> -1 9319
> 0 9147
> 1 9178
> 2 9196
> 3 9258
> 4 9303
> 5 9369
> 6 9382
> 7 9375
> 8 9389
> 9 9376
> 10 9264
> -5 9386
> -4 9396
> -3 9424
> -2 9391
> -1 9284
> 0 9267
> 1 9278
> 2 9318
> 3 9334
> 4 9275
> 5 9306
> 6 9308
> 7 9358
> 8 9335
> 9 9373
> 10 9379
> -5 9284
> -4 9267
> -3 9278
> -2 9318
> -1 9334
> 0 9275
> 1 9306
> 2 9308
> 3 9358
> 4 9335
> 5 9373
> 6 9379
> 7 9355
> 8 9340
> 9 9327
> 10 9320
> -5 9327
> -4 9320
> -3 9315
> -2 9336
> -1 9371
> 0 9259
> 1 9330
> 2 9355
> 3 9334
> 4 9353
> 5 9370
> 6 9394
> 7 9400
> 8 9318
> 9 9037
> 10 8994
> -5 9394
> -4 9400
> -3 9318
> -2 9037
> -1 8994
> 0 8943
> 1 8964
> 2 8997
> 3 9158
> 4 8964
> 5 8564
> 6 8736
> 7 8818
> 8 8938
> 9 9034
> 10 9132
> -5 8943
> -4 8964
> -3 8997
> -2 9158
> -1 8964
> 0 8564
> 1 8736
> 2 8818
> 3 8938
> 4 9034
> 5 9132
> 6 9167
> 7 9200
> 8 9257
> 9 9266
> 10 9306
> -5 9338
> -4 9354
> -3 9372
> -2 9338
> -1 9308
> 0 9282
> 1 9324
> 2 9318
> 3 9342
> 4 9370
> 5 9331
> 6 9327
> 7 9338
> 8 9381
> 9 9394
> 10 9332
> -5 9372
> -4 9338
> -3 9308
> -2 9282
> -1 9324
> 0 9318
> 1 9342
> 2 9370
> 3 9331
> 4 9327
> 5 9338
> 6 9381
> 7 9394
> 8 9332
> 9 9331
> 10 9293
> -5 9338
> -4 9381
> -3 9394
> -2 9332
> -1 9331
> 0 9293
> 1 9309
> 2 9325
> 3 9406
> 4 9409
> 5 9413
> 6 9426
> 7 9440
> 8 9449
> 9 9512
> 10 9494
> -5 9361
> -4 9354
> -3 9299
> -2 9282
> -1 9250
> 0 9242
> 1 9254
> 2 9321
> 3 9390
> 4 9414
> 5 9435
> 6 9437
> 7 9426
> 8 9398
> 9 9383
> 10 9354
> -5 9365
> -4 9421
> -3 9416
> -2 9355
> -1 9338
> 0 9324
> 1 9325
> 2 9322
> 3 9319
> 4 9381
> 5 9315
> 6 9314
> 7 9359
> 8 9403
> 9 9419
> 10 9474
> -5 9355
> -4 9338
> -3 9324
> -2 9325
> -1 9322
> 0 9319
> 1 9381
> 2 9315
> 3 9314
> 4 9359
> 5 9403
> 6 9419
> 7 9474
> 8 9525
> 9 9501
> 10 9447
> -5 9325
> -4 9322
> -3 9319
> -2 9381
> -1 9315
> 0 9314
> 1 9359
> 2 9403
> 3 9419
> 4 9474
> 5 9525
> 6 9501
> 7 9447
> 8 9424
> 9 9396
> 10 9388
> -5 9447
> -4 9424
> -3 9396
> -2 9388
> -1 9396
> 0 9346
> 1 9358
> 2 9353
> 3 9350
> 4 9378
> 5 9372
> 6 9354
> 7 9349
> 8 9392
> 9 9440
> 10 9467
> -5 9388
> -4 9396
> -3 9346
> -2 9358
> -1 9353
> 0 9350
> 1 9378
> 2 9372
> 3 9354
> 4 9349
> 5 9392
> 6 9440
> 7 9467
> 8 9519
> 9 9550
> 10 9565
> -5 9353
> -4 9350
> -3 9378
> -2 9372
> -1 9354
> 0 9349
> 1 9392
> 2 9440
> 3 9467
> 4 9519
> 5 9550
> 6 9565
> 7 9565
> 8 9497
> 9 9500
> 10 9472
> -5 9522
> -4 9529
> -3 9492
> -2 9432
> -1 9382
> 0 9355
> 1 9361
> 2 9350
> 3 9382
> 4 9451
> 5 9491
> 6 9506
> 7 9529
> 8 9543
> 9 9556
> 10 9553
> -5 9492
> -4 9432
> -3 9382
> -2 9355
> -1 9361
> 0 9350
> 1 9382
> 2 9451
> 3 9491
> 4 9506
> 5 9529
> 6 9543
> 7 9556
> 8 9553
> 9 9502
> 10 9470
> -5 9551
> -4 9505
> -3 9389
> -2 9406
> -1 9377
> 0 9284
> 1 9365
> 2 9424
> 3 9412
> 4 9403
> 5 9384
> 6 9394
> 7 9404
> 8 9413
> 9 9407
> 10 9405
> -5 9579
> -4 9576
> -3 9543
> -2 9451
> -1 9421
> 0 9361
> 1 9394
> 2 9400
> 3 9387
> 4 9366
> 5 9346
> 6 9360
> 7 9385
> 8 9435
> 9 9443
> 10 9430
> -5 9361
> -4 9394
> -3 9400
> -2 9387
> -1 9366
> 0 9346
> 1 9360
> 2 9385
> 3 9435
> 4 9443
> 5 9430
> 6 9454
> 7 9531
> 8 9547
> 9 9581
> 10 9540
> -5 9510
> -4 9546
> -3 9564
> -2 9508
> -1 9422
> 0 9369
> 1 9395
> 2 9438
> 3 9423
> 4 9392
> 5 9368
> 6 9366
> 7 9348
> 8 9340
> 9 9375
> 10 9391
> -5 9423
> -4 9392
> -3 9368
> -2 9366
> -1 9348
> 0 9340
> 1 9375
> 2 9391
> 3 9466
> 4 9545
> 5 9574
> 6 9564
> 7 9527
> 8 9513
> 9 9494
> 10 9542
> -5 9511
> -4 9491
> -3 9457
> -2 9453
> -1 9402
> 0 9382
> 1 9407
> 2 9437
> 3 9403
> 4 9404
> 5 9425
> 6 9486
> 7 9457
> 8 9451
> 9 9423
> 10 9401
> -5 9425
> -4 9486
> -3 9457
> -2 9451
> -1 9423
> 0 9401
> 1 9429
> 2 9422
> 3 9431
> 4 9462
> 5 9475
> 6 9474
> 7 9487
> 8 9493
> 9 9495
> 10 9499
> -5 9404
> -4 9385
> -3 9363
> -2 9399
> -1 9411
> 0 9355
> 1 9357
> 2 9363
> 3 9382
> 4 9387
> 5 9408
> 6 9429
> 7 9456
> 8 9487
> 9 9526
> 10 9487
> -5 9493
> -4 9439
> -3 9400
> -2 9378
> -1 9371
> 0 9369
> 1 9374
> 2 9305
> 3 9298
> 4 9298
> 5 9325
> 6 9381
> 7 9477
> 8 9508
> 9 9496
> 10 9517
> -5 9371
> -4 9369
> -3 9374
> -2 9305
> -1 9298
> 0 9298
> 1 9325
> 2 9381
> 3 9477
> 4 9508
> 5 9496
> 6 9517
> 7 9561
> 8 9570
> 9 9546
> 10 9544
> -5 9510
> -4 9506
> -3 9530
> -2 9441
> -1 9427
> 0 9393
> 1 9420
> 2 9444
> 3 9468
> 4 9484
> 5 9525
> 6 9542
> 7 9557
> 8 9548
> 9 9550
> 10 9593
> -5 9589
> -4 9598
> -3 9527
> -2 9417
> -1 9390
> 0 9374
> 1 9386
> 2 9407
> 3 9453
> 4 9447
> 5 9419
> 6 9386
> 7 9373
> 8 9364
> 9 9376
> 10 9389
> -5 9453
> -4 9447
> -3 9419
> -2 9386
> -1 9373
> 0 9364
> 1 9376
> 2 9389
> 3 9376
> 4 9375
> 5 9370
> 6 9391
> 7 9458
> 8 9446
> 9 9456
> 10 9463
> -5 9364
> -4 9376
> -3 9389
> -2 9376
> -1 9375
> 0 9370
> 1 9391
> 2 9458
> 3 9446
> 4 9456
> 5 9463
> 6 9500
> 7 9486
> 8 9474
> 9 9495
> 10 9531
> -5 9491
> -4 9441
> -3 9388
> -2 9380
> -1 9369
> 0 9354
> 1 9367
> 2 9369
> 3 9341
> 4 9305
> 5 9308
> 6 9324
> 7 9385
> 8 9451
> 9 9496
> 10 9527
> -5 9369
> -4 9354
> -3 9367
> -2 9369
> -1 9341
> 0 9305
> 1 9308
> 2 9324
> 3 9385
> 4 9451
> 5 9496
> 6 9527
> 7 9544
> 8 9543
> 9 9535
> 10 9536
> -5 9586
> -4 9583
> -3 9572
> -2 9533
> -1 9454
> 0 9392
> 1 9420
> 2 9451
> 3 9475
> 4 9514
> 5 9561
> 6 9542
> 7 9502
> 8 9461
> 9 9468
> 10 9463
> -5 9587
> -4 9562
> -3 9530
> -2 9445
> -1 9404
> 0 9395
> 1 9417
> 2 9449
> 3 9467
> 4 9470
> 5 9524
> 6 9512
> 7 9448
> 8 9398
> 9 9431
> 10 9467
> -5 9467
> -4 9470
> -3 9524
> -2 9512
> -1 9448
> 0 9398
> 1 9431
> 2 9467
> 3 9490
> 4 9517
> 5 9526
> 6 9574
> 7 9573
> 8 9562
> 9 9563
> 10 9566
> ",header=TRUE)
> 
> data<-matrix(c(dta$CR),ncol=71)
> A<-matrix(rep(-5:10,71))
> B<-matrix(data)
> 
> oodf<-data.frame(A,B)
> a<--5:10
> oodf<-data.frame(A,B)
> library(plotrix)
> std.error<-function(x) return(sd(x)/(sum(!is.na(x))))
> oomean<-as.vector(by(oodf$B,oodf$A,mean))
> oose<-as.vector(by(oodf$B,oodf$A,std.error))
> plot(-5:10,oomean,type="l",ylim=c(8890,9100),
> )
> A<-oomean-1.96*oose
> B<-oomean+1.96*oose
> lines(a,A,col="red")
> lines(a,B,col="red")
> 
> My Question:
> I wish to conduct a randomization test of significance (90 and 99
> percentile) of the reductions/decreases as displayed by the signal.
> 
> I am attempting using:
> x<-sample(8890:9500,1136,replace=T )
> 
> to generate the random numbers, where 8890, 9500 and 1136 are the
> minimum  and maximum of the signal and 1136 the length of sample data.
> Q1: Please how do I generate many samples as x above, say up to 5000
> or 10,000? I manually generated and stored as x1,x2, x3 up to x100.
> 
> Q2: Please how do I use this randomly generated numbers to test the
> statistical significance level of the signal generated by
> plot(-5:10,oomean,type="l",ylim=c(8890,9100),  )?
> 
> I wish to test for 90% and 99% percentile.
> 
> I am sorry that this is too long.
> 
> Many thanks for your kind contributions
> 
> Best
> Ogbos
> 
> 
> 
> 
> 
> 
> 
> On Sun, Feb 10, 2019 at 3:55 PM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>> 
>> Dear Michael,
>> This is great! Thank you.
>> 
>> I have not really got any response other than yours.
>> 
>> I have long before now included what I have in a paper submitted to a journal.
>> 
>> I am awaiting the feedback of the reviewer. I will compare the
>> comments with your input here and determine the corrections to make
>> and probably return to the list for additional help.
>> 
>> Best wishes
>> Ogbos
>> 
>> On Fri, Feb 8, 2019 at 4:31 PM Meyners, Michael <meyners.m at pg.com> wrote:
>>> 
>>> Ogbos,
>>> 
>>> You do not seem to have received a reply over the list yet, which might be due to the fact that this seems rather a stats than an R question. Neither got your attachment (Figure) through - see posting guide.
>>> 
>>> I'm not familiar with epoch analysis, so not sure what exactly you are doing / trying to achieve, but some general thoughts:
>>> 
>>> * You do NOT want to restrict your re-randomizations in a way that "none of the dates corresponds with the ones in the real event" - actually, as a general principle, the true data must be an admissible re-randomization as well. You seem to have excluded that (and a lot of other randomizations at the same time which might have occurred, i.e. dates 1 and 2 reversed but all others the same), thereby rendering the test invalid. Any restrictions you have on your re-randomizations must've applied to the original randomization as well.
>>> * If you have rather observational data (which I suspect, but not sure), Edgington & Onghena (2007) would rather refer to this as a permutation test - the difference being that you have to make strong assumptions (similar to parametric tests) on the nature of the data, which are designed-in to be true for randomization tests. It might be a merely linguistic discrimination, but it is important to note which assumptions have to be (implicitly) made.
>>> * I'm not sure what you mean by "mean differences" of the events - is that two groups you are comparing? If so, that seems reasonable, but just make sure the test statistic you use is reasonable and sensitive against the alternatives you are mostly interested in. The randomization/permutation test will never proof that, e.g., means are significantly different, but only that there is SOME difference. By selecting the appropriate test statistic, you can influence what will pop up more easily and what not, but you can never be sure (unless you make strong assumptions about everything else, like in many parametric tests).
>>> * For any test statistic, you would then determine the proportion of its values among the 5000 samples where it is as large or larger than the one observed (or as small or smaller, or either, depending on the nature of the test statistic and whether you aim for a one- or a two-sided test). That is your p value. If small enough, conclude significance. At least conceptually important: The observed test statistic is always part of the re-randomization (i.e. your 5000) - so you truly only do 4999 plus the one you observed. Otherwise the test may be more or less liberal. Your p value is hence no smaller than 1/n, where n is the total number of samples you looked at (including the observed one), a p value of 0 is not possible in randomization tests (nor in other tests, of course).
>>> 
>>> I hope this is helpful, but you will need to go through these and refer to your own setup to check whether you adhered to the principles or not, which is impossible for me to judge based on the information provided (and I won't be able to look at excessive code to check either).
>>> 
>>> Michael
>>> 
>>>> -----Original Message-----
>>>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ogbos Okike
>>>> Sent: Montag, 28. Januar 2019 19:42
>>>> To: r-help <r-help at r-project.org>
>>>> Subject: [R] Randomization Test
>>>> 
>>>> Dear Contributors,
>>>> 
>>>> I conducting epoch analysis. I tried to test the significance of my result using
>>>> randomization test.
>>>> 
>>>> Since I have 71 events, I randomly selected another 71 events, making sure
>>>> that none of the dates in the random events corresponds with the ones in
>>>> the real event.
>>>> 
>>>> Following the code I found here
>>>> (https://www.uvm.edu/~dhowell/StatPages/R/RandomizationTestsWithR/R
>>>> andom2Sample/TwoIndependentSamplesR.html),
>>>> I combined these two data set and used them to generate another 5000
>>>> events. I then plotted the graph of the mean differences for the 5000
>>>> randomly generated events. On the graph, I indicated the region of the
>>>> mean difference between the real 71 epoch and the randomly selected 71
>>>> epoch.
>>>> 
>>>> Since the two tail test shows that the mean difference falls at the extreme of
>>>> the randomly selected events, I concluded that my result is statistically
>>>> significant.
>>>> 
>>>> 
>>>> 
>>>> I am attaching the graph to assistance you in you suggestions.
>>>> 
>>>> I can attach both my code and the real and randomly generated events if you
>>>> ask for it.
>>>> 
>>>> My request is that you help me to understand if I am on the right track or no.
>>>> This is the first time I am doing this and except the experts decide, I am not
>>>> quite sure whether I am right or not.
>>>> 
>>>> Many thanks for your kind concern.
>>>> 
>>>> Best
>>>> Ogbos
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>>> guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org

Ecological Forecasting: https://eco.bigelow.org/


From nev||@@mo@ @end|ng |rom gm@||@com  Thu Feb 28 03:42:50 2019
From: nev||@@mo@ @end|ng |rom gm@||@com (nevil amos)
Date: Thu, 28 Feb 2019 13:42:50 +1100
Subject: [R] profvis function parse_rprof not being loaded
Message-ID: <CAN9eD7mvBUTqxbdkPwoF=FkcaEbih5PwdOyE7RKRDxF=uEOc2Q@mail.gmail.com>

I have loaded the profvis library  but the function parse_ rprof()  is
absent.
below is the session info show the absence of the function ( which is
listed  in the package help for the current version.)


Documentation for package ?profvis? version 0.3.5

DESCRIPTION file.
Help Pages

parse_rprof Parse Rprof output file for use with profvis
pause Pause an R process
print.profvis Print a profvis object
profvis Profile an R expression and visualize profiling data
profvisOutput Widget output function for use in Shiny
renderProfvis Widget render function for use in Shiny


Rsession info:


> library(profvis)
> parse_rprof()
Error in parse_rprof() : could not find function "parse_rprof"
> ls("package:profvis")
[1] "pause"         "profvis"       "profvisOutput" "renderProfvis"
> sessionInfo()
R version 3.5.2 (2018-12-20)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

Matrix products: default

locale:
[1] LC_COLLATE=English_Australia.1252  LC_CTYPE=English_Australia.1252
LC_MONETARY=English_Australia.1252
[4] LC_NUMERIC=C                       LC_TIME=English_Australia.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] profvis_0.3.5

loaded via a namespace (and not attached):
 [1] htmlwidgets_1.3 compiler_3.5.2  magrittr_1.5    htmltools_0.3.6
tools_3.5.2     yaml_2.2.0      Rcpp_1.0.0      stringi_1.2.4
 [9] stringr_1.3.1   digest_0.6.18

	[[alternative HTML version deleted]]


From wdun|@p @end|ng |rom t|bco@com  Thu Feb 28 03:56:10 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Wed, 27 Feb 2019 18:56:10 -0800
Subject: [R] inverse of which()
In-Reply-To: <CALRb-odXXpf3Otiz64XHibbD+3r842p5SBV9A2qiXVuC1bkrcA@mail.gmail.com>
References: <CALRb-odXXpf3Otiz64XHibbD+3r842p5SBV9A2qiXVuC1bkrcA@mail.gmail.com>
Message-ID: <CAF8bMcYiOWnCUVw-gqVvN1C8R-1gxTAib1mJasc-oD-kva0Z+g@mail.gmail.com>

The inverse of which() would have to know the length of the logical vector
to create.  The function could be
   invWhich <- function(whichTrue, length) {
       stopifnot(length <= max(whichTrue), !anyNA(whichTrue))
       v <- logical(length)
       v[whichTrue] <- TRUE
       v
   }
It isn't quite an inverse, as which() treats NA and FALSE the same.

Much of the time dealing with the logical vector (e.g, grepl() instead of
grep()) is less error prone than using which() to convert to indices of the
TRUE values.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Wed, Feb 27, 2019 at 3:04 PM Ed Siefker <ebs15242 at gmail.com> wrote:

> Given a vector of booleans, chich() will return indices that are TRUE.
>
> Given a vector of indices, how can I get a vector of booleans?
>
> My intent is to do logical operations on the output of grep().  Maybe
> there's a better way to do this?
>
> Thanks
> -Ed
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @|m|n@@t@work @end|ng |rom gm@||@com  Thu Feb 28 04:51:32 2019
From: @|m|n@@t@work @end|ng |rom gm@||@com (Aimin Yan)
Date: Wed, 27 Feb 2019 22:51:32 -0500
Subject: [R] color question
Message-ID: <CALn2QVgsHUB0iLSnNDiuSUKe+febefxnSFEwknKk6V3pXjbmng@mail.gmail.com>

I have a question about assigning color based on the value of a matrix

The following is my matrix.

d
             lateRT      earlyRT         NAD       ciLAD
lateRT  1.000000000 0.0000000000 0.006224017 0.001260241
earlyRT 0.000000000 1.0000000000 0.001425649 0.007418436
NAD     0.006224017 0.0014256488 1.000000000 0.064653780
ciLAD   0.001260241 0.0074184361 0.064653780 1.000000000
LAD     0.006969928 0.0007096344 0.393556636 0.002483941
                 LAD
lateRT  0.0069699285
earlyRT 0.0007096344
NAD     0.3935566356
ciLAD   0.0024839407
LAD     1.0000000000

I want to use the following function to get heatmap and dendrogram

> heatmap.2(d,trace="none",margin=c(8, 10))

but it is hard to use color to make  0.001260241 and 0.0074184361 to be
visualized differently.

Does anyone know how to adjust color based on these values in this matrix?

Thank you,

Aimin

	[[alternative HTML version deleted]]


From @|m|n@@t@work @end|ng |rom gm@||@com  Thu Feb 28 04:55:34 2019
From: @|m|n@@t@work @end|ng |rom gm@||@com (Aimin Yan)
Date: Wed, 27 Feb 2019 22:55:34 -0500
Subject: [R] color question
In-Reply-To: <CALn2QVgsHUB0iLSnNDiuSUKe+febefxnSFEwknKk6V3pXjbmng@mail.gmail.com>
References: <CALn2QVgsHUB0iLSnNDiuSUKe+febefxnSFEwknKk6V3pXjbmng@mail.gmail.com>
Message-ID: <CALn2QVgSaQOTRwB9agFMD3M7Ob=yOUH=hBXWdseBzP6o8tjNZg@mail.gmail.com>

The attached is the heatmap.

Thank you,

Aimin

On Wed, Feb 27, 2019 at 10:51 PM Aimin Yan <aimin.at.work at gmail.com> wrote:

> I have a question about assigning color based on the value of a matrix
>
> The following is my matrix.
>
> d
>              lateRT      earlyRT         NAD       ciLAD
> lateRT  1.000000000 0.0000000000 0.006224017 0.001260241
> earlyRT 0.000000000 1.0000000000 0.001425649 0.007418436
> NAD     0.006224017 0.0014256488 1.000000000 0.064653780
> ciLAD   0.001260241 0.0074184361 0.064653780 1.000000000
> LAD     0.006969928 0.0007096344 0.393556636 0.002483941
>                  LAD
> lateRT  0.0069699285
> earlyRT 0.0007096344
> NAD     0.3935566356
> ciLAD   0.0024839407
> LAD     1.0000000000
>
> I want to use the following function to get heatmap and dendrogram
>
> > heatmap.2(d,trace="none",margin=c(8, 10))
>
> but it is hard to use color to make  0.001260241 and 0.0074184361 to be
> visualized differently.
>
> Does anyone know how to adjust color based on these values in this matrix?
>
> Thank you,
>
> Aimin
>
>
>

From m@||||@t@ @end|ng |rom pp@|net@||  Thu Feb 28 08:20:50 2019
From: m@||||@t@ @end|ng |rom pp@|net@|| (K. Elo)
Date: Thu, 28 Feb 2019 09:20:50 +0200
Subject: [R] color question
In-Reply-To: <CALn2QVgsHUB0iLSnNDiuSUKe+febefxnSFEwknKk6V3pXjbmng@mail.gmail.com>
References: <CALn2QVgsHUB0iLSnNDiuSUKe+febefxnSFEwknKk6V3pXjbmng@mail.gmail.com>
Message-ID: <985f55aca99d47fb5ab5eac9d008616455df75b2.camel@pp.inet.fi>

Hi!

2019-02-27 22:51 -0500, Aimin Yan wrote:
> I have a question about assigning color based on the value of a
> matrix
> 
> The following is my matrix.
> 
> d
>              lateRT      earlyRT         NAD       ciLAD
> lateRT  1.000000000 0.0000000000 0.006224017 0.001260241
> earlyRT 0.000000000 1.0000000000 0.001425649 0.007418436
> NAD     0.006224017 0.0014256488 1.000000000 0.064653780
> ciLAD   0.001260241 0.0074184361 0.064653780 1.000000000
> LAD     0.006969928 0.0007096344 0.393556636 0.002483941
>                  LAD
> lateRT  0.0069699285
> earlyRT 0.0007096344
> NAD     0.3935566356
> ciLAD   0.0024839407
> LAD     1.0000000000
> 
> I want to use the following function to get heatmap and dendrogram
> 
> > heatmap.2(d,trace="none",margin=c(8, 10))
> 
> but it is hard to use color to make  0.001260241 and 0.0074184361 to
> be
> visualized differently.
> 
> Does anyone know how to adjust color based on these values in this
> matrix?

Have you tried to adapt the attribute "colorTable"?

You have to store the heatmap in an object (say "hm") and the use
"hm$colorTable".

See the examples here: 
https://www.rdocumentation.org/packages/gplots/versions/3.0.1.1/topics/heatmap.2

Also take a look on the documentation.

HTH,
Kimmo


From drj|m|emon @end|ng |rom gm@||@com  Thu Feb 28 09:01:46 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Thu, 28 Feb 2019 19:01:46 +1100
Subject: [R] color question
In-Reply-To: <CALn2QVgsHUB0iLSnNDiuSUKe+febefxnSFEwknKk6V3pXjbmng@mail.gmail.com>
References: <CALn2QVgsHUB0iLSnNDiuSUKe+febefxnSFEwknKk6V3pXjbmng@mail.gmail.com>
Message-ID: <CA+8X3fXX1gs4pw3h71aCTPVCTJu2ZSdr=xt-cPtY5_Udnt2JvA@mail.gmail.com>

Hi Aimin,
This example uses a log transformation to spread the colors out:

d<-read.table(text="         lateRT      earlyRT         NAD
ciLAD          LAD
 1.000000000 0.0000000000 0.006224017 0.001260241 0.0069699285
 0.000000000 1.0000000000 0.001425649 0.007418436 0.0007096344
 0.006224017 0.0014256488 1.000000000 0.064653780 0.3935566356
 0.001260241 0.0074184361 0.064653780 1.000000000 0.0024839407
 0.006969928 0.0007096344 0.393556636 0.002483941 1.0000000000",
header=TRUE)
rownames(d)<-colnames(d)
d<-as.matrix(d)
diag(d)<-NA
library(plotrix)
color2D.matplot(-log(d+0.0001),extremes=c("red","blue"),
 main="Correlation matrix of d",axes=FALSE)
axis(1,at=seq(0.5,4.5),labels=colnames(d))
axis(2,at=seq(0.5,4.5),labels=rownames(d))
color.legend(0,-0.7,2,-0.5,legend=c(0,0.001,0.007,0.07,0.4),
 rect.col=color.scale(log(c(0.00001,0.001,0.005,0.07,0.4)),
 extremes=c("blue","red")),align="rb")

Jim

On Thu, Feb 28, 2019 at 2:52 PM Aimin Yan <aimin.at.work at gmail.com> wrote:
>
> I have a question about assigning color based on the value of a matrix
>
> The following is my matrix.
>
> d
>              lateRT      earlyRT         NAD       ciLAD
> lateRT  1.000000000 0.0000000000 0.006224017 0.001260241
> earlyRT 0.000000000 1.0000000000 0.001425649 0.007418436
> NAD     0.006224017 0.0014256488 1.000000000 0.064653780
> ciLAD   0.001260241 0.0074184361 0.064653780 1.000000000
> LAD     0.006969928 0.0007096344 0.393556636 0.002483941
>                  LAD
> lateRT  0.0069699285
> earlyRT 0.0007096344
> NAD     0.3935566356
> ciLAD   0.0024839407
> LAD     1.0000000000
>
> I want to use the following function to get heatmap and dendrogram
>
> > heatmap.2(d,trace="none",margin=c(8, 10))
>
> but it is hard to use color to make  0.001260241 and 0.0074184361 to be
> visualized differently.
>
> Does anyone know how to adjust color based on these values in this matrix?
>
> Thank you,
>
> Aimin
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From er|cjberger @end|ng |rom gm@||@com  Thu Feb 28 09:08:54 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Thu, 28 Feb 2019 10:08:54 +0200
Subject: [R] Which dependency list to build first?
In-Reply-To: <alpine.LNX.2.20.1902271341260.11593@salmo.appl-ecosys.com>
References: <alpine.LNX.2.20.1902241104040.31842@salmo.appl-ecosys.com>
 <23669.4832.639467.519820@stat.math.ethz.ch>
 <alpine.LNX.2.20.1902260532430.16864@salmo.appl-ecosys.com>
 <9F750BDE-2792-4294-B10A-0E8687E5B32B@icloud.com>
 <alpine.LNX.2.20.1902270937380.11593@salmo.appl-ecosys.com>
 <CAF8bMcbhaR=8WcGLrmTM8=nsCHATooP8vN=fF1=B7gsLXAwvrg@mail.gmail.com>
 <alpine.LNX.2.20.1902271109100.11593@salmo.appl-ecosys.com>
 <CAF8bMcYuF74ERUsSPZD_xjwFEG81KYoKGm+DRYqBkhzqDN4YTQ@mail.gmail.com>
 <alpine.LNX.2.20.1902271147240.11593@salmo.appl-ecosys.com>
 <CAF8bMcaq_zZpy9cfi2nwGoCS2QNhadN7q2sqbrRhA5iu3S1v0g@mail.gmail.com>
 <alpine.LNX.2.20.1902271245210.11593@salmo.appl-ecosys.com>
 <CAF8bMcZiO0N6Owfjjbn+q9vsxbxzjz9+xmpo9Lau4ez3H66a_Q@mail.gmail.com>
 <alpine.LNX.2.20.1902271341260.11593@salmo.appl-ecosys.com>
Message-ID: <CAGgJW77caM9weCmbZJxm5TO9o0cFt1Pmq5LdeSszN6jC=-Y8-A@mail.gmail.com>

These two pathnames are different

/usr/lib/R/library/lib/later/later.so
/usr/lib/R/library/later.so

Was that your intention?


On Wed, Feb 27, 2019 at 11:45 PM Rich Shepard <rshepard at appl-ecosys.com>
wrote:

> On Wed, 27 Feb 2019, William Dunlap wrote:
>
> > Did you use 'R CMD ldd .../later.so', as I recommended?
>
> Bill,
>
> I ran ldd externally on /usr/lib/R/library/lib/later/later.so and posted
> the
> missing library. Just now I ran the above command and it did not find
> later.so:
>
> # R CMD ldd /usr/lib/R/library/later.so
> ldd: /usr/lib/R/library/later.so: No such file or directory
>
> Strange. And confusing to me.
>
> Regards,
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Thu Feb 28 11:32:04 2019
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Thu, 28 Feb 2019 11:32:04 +0100
Subject: [R] Add point label to lattice cloud plot (3D scatter)
Message-ID: <CAMk+s2RgzRbL3tFTASid6Q=68k2aDutiU5d62Fft4icSz5yVzw@mail.gmail.com>

Dear all,
is there a way to add the labels of the points to a 3D scatter plot
obtained with lattice's cloud?

I can now plot the data, but when trying to add the label I get the error:
`Error in multiple && !outer : invalid 'x' type in 'x && y'`

The script I wrote runs like this:

>>>
df = data.frame(Name = c("A", "B", "C", "D", "E"),
              x_axis = c(-0.591, 0.384, -0.384, -0.032, 0.754),
              y_axis = c(-1.302, 1.652, -1.652, 0.326, 0.652),
              z_axis = c(1.33, 1.33, 2.213, 0.032, -0.754),
              stringsAsFactors = FALSE)

cloud(z_axis ~ x_axis * y_axis, data = df,
      xlab = "X", ylab = "Y", zlab = "Z",
      pch = 16, col = "red", type = "b", cex = 1.5,
      ltext(x=df$x_axis, y=df$y_axis, z=df$z_axis,
            labels=df$Names, pos=1, offset=1, cex=0.8)
)

<<<
Thank you


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Thu Feb 28 11:39:33 2019
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Thu, 28 Feb 2019 11:39:33 +0100
Subject: [R] add points to lattice cloud plot (3D scatter)
Message-ID: <CAMk+s2Te3OtxiXH4uszCcsSiTZiqTBbBvLMm4iFxtYgpv6Q3Bw@mail.gmail.com>

Dear all,
is it possible to add points to a lattice cloud plot (3D scatter)? I
can plot the main data, but what if I wanted to add another point. In
R there is the high level plotting function plot(), then the low level
points() or lines() etc. What is the equivalent for lattice?

Thank you


>>>

df = data.frame(Name = c("A", "B", "C", "D", "E"),
              x_axis = c(-0.591, 0.384, -0.384, -0.032, 0.754),
              y_axis = c(-1.302, 1.652, -1.652, 0.326, 0.652),
              z_axis = c(1.33, 1.33, 2.213, 0.032, -0.754),
              stringsAsFactors = FALSE)

cloud(z_axis ~ x_axis * y_axis, data = df,
      xlab = "X", ylab = "Y", zlab = "Z",
      pch = 16, col = "red", type = "b", cex = 1.5,
      ltext(x=df$x_axis, y=df$y_axis, z=df$z_axis,
            labels=df$Names, pos=1, offset=1, cex=0.8)
)

df2 = data.frame(Name = "F",
                x_axis = 0.891,
                y_axis = 2.302
                z_axis = -1.83,
                stringsAsFactors = FALSE)

-- 
Best regards,
Luigi


From murdoch@dunc@n @end|ng |rom gm@||@com  Thu Feb 28 12:14:24 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Thu, 28 Feb 2019 06:14:24 -0500
Subject: [R] profvis function parse_rprof not being loaded
In-Reply-To: <CAN9eD7mvBUTqxbdkPwoF=FkcaEbih5PwdOyE7RKRDxF=uEOc2Q@mail.gmail.com>
References: <CAN9eD7mvBUTqxbdkPwoF=FkcaEbih5PwdOyE7RKRDxF=uEOc2Q@mail.gmail.com>
Message-ID: <dc3afd85-01c0-8c45-9f08-96b1480451cb@gmail.com>

On 27/02/2019 9:42 p.m., nevil amos wrote:
> I have loaded the profvis library  but the function parse_ rprof()  is
> absent.
> below is the session info show the absence of the function ( which is
> listed  in the package help for the current version.)
> 

Looks as though they forgot to export it.  You can get to it using 
profvis:::parse_rprof, but the maintainer (who is cc'd) might want to 
fix this.

Duncan Murdoch

> 
> Documentation for package ?profvis? version 0.3.5
> 
> DESCRIPTION file.
> Help Pages
> 
> parse_rprof Parse Rprof output file for use with profvis
> pause Pause an R process
> print.profvis Print a profvis object
> profvis Profile an R expression and visualize profiling data
> profvisOutput Widget output function for use in Shiny
> renderProfvis Widget render function for use in Shiny
> 
> 
> Rsession info:
> 
> 
>> library(profvis)
>> parse_rprof()
> Error in parse_rprof() : could not find function "parse_rprof"
>> ls("package:profvis")
> [1] "pause"         "profvis"       "profvisOutput" "renderProfvis"
>> sessionInfo()
> R version 3.5.2 (2018-12-20)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows 7 x64 (build 7601) Service Pack 1
> 
> Matrix products: default
> 
> locale:
> [1] LC_COLLATE=English_Australia.1252  LC_CTYPE=English_Australia.1252
> LC_MONETARY=English_Australia.1252
> [4] LC_NUMERIC=C                       LC_TIME=English_Australia.1252
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> other attached packages:
> [1] profvis_0.3.5
> 
> loaded via a namespace (and not attached):
>   [1] htmlwidgets_1.3 compiler_3.5.2  magrittr_1.5    htmltools_0.3.6
> tools_3.5.2     yaml_2.2.0      Rcpp_1.0.0      stringi_1.2.4
>   [9] stringr_1.3.1   digest_0.6.18
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ||@t@ @end|ng |rom dewey@myzen@co@uk  Thu Feb 28 12:52:31 2019
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Thu, 28 Feb 2019 11:52:31 +0000
Subject: [R] add points to lattice cloud plot (3D scatter)
In-Reply-To: <CAMk+s2Te3OtxiXH4uszCcsSiTZiqTBbBvLMm4iFxtYgpv6Q3Bw@mail.gmail.com>
References: <CAMk+s2Te3OtxiXH4uszCcsSiTZiqTBbBvLMm4iFxtYgpv6Q3Bw@mail.gmail.com>
Message-ID: <33199dca-79b7-fa8d-5f48-9736bceb7bf7@dewey.myzen.co.uk>

Not sure whether this helps but try

library(lattice)
?llines

Note that is indeed a double ll at the start

Michael

On 28/02/2019 10:39, Luigi Marongiu wrote:
> Dear all,
> is it possible to add points to a lattice cloud plot (3D scatter)? I
> can plot the main data, but what if I wanted to add another point. In
> R there is the high level plotting function plot(), then the low level
> points() or lines() etc. What is the equivalent for lattice?
> 
> Thank you
> 
> 
>>>>
> 
> df = data.frame(Name = c("A", "B", "C", "D", "E"),
>                x_axis = c(-0.591, 0.384, -0.384, -0.032, 0.754),
>                y_axis = c(-1.302, 1.652, -1.652, 0.326, 0.652),
>                z_axis = c(1.33, 1.33, 2.213, 0.032, -0.754),
>                stringsAsFactors = FALSE)
> 
> cloud(z_axis ~ x_axis * y_axis, data = df,
>        xlab = "X", ylab = "Y", zlab = "Z",
>        pch = 16, col = "red", type = "b", cex = 1.5,
>        ltext(x=df$x_axis, y=df$y_axis, z=df$z_axis,
>              labels=df$Names, pos=1, offset=1, cex=0.8)
> )
> 
> df2 = data.frame(Name = "F",
>                  x_axis = 0.891,
>                  y_axis = 2.302
>                  z_axis = -1.83,
>                  stringsAsFactors = FALSE)
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Thu Feb 28 13:58:16 2019
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Thu, 28 Feb 2019 13:58:16 +0100
Subject: [R] add points to lattice cloud plot (3D scatter)
In-Reply-To: <33199dca-79b7-fa8d-5f48-9736bceb7bf7@dewey.myzen.co.uk>
References: <CAMk+s2Te3OtxiXH4uszCcsSiTZiqTBbBvLMm4iFxtYgpv6Q3Bw@mail.gmail.com>
 <33199dca-79b7-fa8d-5f48-9736bceb7bf7@dewey.myzen.co.uk>
Message-ID: <CAMk+s2SJe-a8Y3Lns1f9Gk=trNiFFwgz5APteeYWuzRD6AgogA@mail.gmail.com>

Thank uyou, but ?llines gives a method for 2 d plot:

llines(x, y = NULL, ...

when I try to plot 3 variables I get NULL as a result

On Thu, Feb 28, 2019 at 12:52 PM Michael Dewey <lists at dewey.myzen.co.uk> wrote:
>
> Not sure whether this helps but try
>
> library(lattice)
> ?llines
>
> Note that is indeed a double ll at the start
>
> Michael
>
> On 28/02/2019 10:39, Luigi Marongiu wrote:
> > Dear all,
> > is it possible to add points to a lattice cloud plot (3D scatter)? I
> > can plot the main data, but what if I wanted to add another point. In
> > R there is the high level plotting function plot(), then the low level
> > points() or lines() etc. What is the equivalent for lattice?
> >
> > Thank you
> >
> >
> >>>>
> >
> > df = data.frame(Name = c("A", "B", "C", "D", "E"),
> >                x_axis = c(-0.591, 0.384, -0.384, -0.032, 0.754),
> >                y_axis = c(-1.302, 1.652, -1.652, 0.326, 0.652),
> >                z_axis = c(1.33, 1.33, 2.213, 0.032, -0.754),
> >                stringsAsFactors = FALSE)
> >
> > cloud(z_axis ~ x_axis * y_axis, data = df,
> >        xlab = "X", ylab = "Y", zlab = "Z",
> >        pch = 16, col = "red", type = "b", cex = 1.5,
> >        ltext(x=df$x_axis, y=df$y_axis, z=df$z_axis,
> >              labels=df$Names, pos=1, offset=1, cex=0.8)
> > )
> >
> > df2 = data.frame(Name = "F",
> >                  x_axis = 0.891,
> >                  y_axis = 2.302
> >                  z_axis = -1.83,
> >                  stringsAsFactors = FALSE)
> >
>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html



-- 
Best regards,
Luigi


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Thu Feb 28 14:26:34 2019
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Thu, 28 Feb 2019 05:26:34 -0800 (PST)
Subject: [R] Which dependency list to build first?
In-Reply-To: <CAGgJW77caM9weCmbZJxm5TO9o0cFt1Pmq5LdeSszN6jC=-Y8-A@mail.gmail.com>
References: <alpine.LNX.2.20.1902241104040.31842@salmo.appl-ecosys.com>
 <23669.4832.639467.519820@stat.math.ethz.ch>
 <alpine.LNX.2.20.1902260532430.16864@salmo.appl-ecosys.com>
 <9F750BDE-2792-4294-B10A-0E8687E5B32B@icloud.com>
 <alpine.LNX.2.20.1902270937380.11593@salmo.appl-ecosys.com>
 <CAF8bMcbhaR=8WcGLrmTM8=nsCHATooP8vN=fF1=B7gsLXAwvrg@mail.gmail.com>
 <alpine.LNX.2.20.1902271109100.11593@salmo.appl-ecosys.com>
 <CAF8bMcYuF74ERUsSPZD_xjwFEG81KYoKGm+DRYqBkhzqDN4YTQ@mail.gmail.com>
 <alpine.LNX.2.20.1902271147240.11593@salmo.appl-ecosys.com>
 <CAF8bMcaq_zZpy9cfi2nwGoCS2QNhadN7q2sqbrRhA5iu3S1v0g@mail.gmail.com>
 <alpine.LNX.2.20.1902271245210.11593@salmo.appl-ecosys.com>
 <CAF8bMcZiO0N6Owfjjbn+q9vsxbxzjz9+xmpo9Lau4ez3H66a_Q@mail.gmail.com>
 <alpine.LNX.2.20.1902271341260.11593@salmo.appl-ecosys.com>
 <CAGgJW77caM9weCmbZJxm5TO9o0cFt1Pmq5LdeSszN6jC=-Y8-A@mail.gmail.com>
Message-ID: <alpine.LNX.2.20.1902280525270.22815@salmo.appl-ecosys.com>

On Thu, 28 Feb 2019, Eric Berger wrote:

> These two pathnames are different
>
> /usr/lib/R/library/lib/later/later.so
> /usr/lib/R/library/later.so
>
> Was that your intention?

Eric,

No. I apologize for being imprecise. On this host later.so is located at
only /usr/lib/R/library/later/libs/later.so.

Regards,

Rich


From murdoch@dunc@n @end|ng |rom gm@||@com  Thu Feb 28 15:53:17 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Thu, 28 Feb 2019 09:53:17 -0500
Subject: [R] add points to lattice cloud plot (3D scatter)
In-Reply-To: <CAMk+s2Te3OtxiXH4uszCcsSiTZiqTBbBvLMm4iFxtYgpv6Q3Bw@mail.gmail.com>
References: <CAMk+s2Te3OtxiXH4uszCcsSiTZiqTBbBvLMm4iFxtYgpv6Q3Bw@mail.gmail.com>
Message-ID: <b771fd81-b24c-298c-d66d-9adada547cf9@gmail.com>

On 28/02/2019 5:39 a.m., Luigi Marongiu wrote:
> Dear all,
> is it possible to add points to a lattice cloud plot (3D scatter)? I
> can plot the main data, but what if I wanted to add another point. In
> R there is the high level plotting function plot(), then the low level
> points() or lines() etc. What is the equivalent for lattice?

I don't know for sure, but I don't think you can do that in lattice. 
The scatterplot3d::scatterplot3d function returns enough information to 
do this, but I don't think lattice::cloud does.  But even 
scatterplot3d::scatterplot3d won't necessarily get it right if points 
hide others that are behind them.  It uses the "painter's algorithm", 
and that needs everything to be drawn in just the right order, which you 
probably won't get if you draw things in several calls.

You can draw things in arbitrary order using rgl::plot3d or related 
functions, but you'll need to do more work yourself to get an array of 
plots like lattice gives.

Duncan Murdoch


> 
> Thank you
> 
> 
>>>>
> 
> df = data.frame(Name = c("A", "B", "C", "D", "E"),
>                x_axis = c(-0.591, 0.384, -0.384, -0.032, 0.754),
>                y_axis = c(-1.302, 1.652, -1.652, 0.326, 0.652),
>                z_axis = c(1.33, 1.33, 2.213, 0.032, -0.754),
>                stringsAsFactors = FALSE)
> 
> cloud(z_axis ~ x_axis * y_axis, data = df,
>        xlab = "X", ylab = "Y", zlab = "Z",
>        pch = 16, col = "red", type = "b", cex = 1.5,
>        ltext(x=df$x_axis, y=df$y_axis, z=df$z_axis,
>              labels=df$Names, pos=1, offset=1, cex=0.8)
> )
> 
> df2 = data.frame(Name = "F",
>                  x_axis = 0.891,
>                  y_axis = 2.302
>                  z_axis = -1.83,
>                  stringsAsFactors = FALSE)
>


From meyner@@m @end|ng |rom pg@com  Thu Feb 28 15:56:40 2019
From: meyner@@m @end|ng |rom pg@com (Meyners, Michael)
Date: Thu, 28 Feb 2019 14:56:40 +0000
Subject: [R] Randomization Test
In-Reply-To: <CAC8ss31pdJ9SisAcHYVHcp+670tKKdSQvj5V-HKaXnLyE_0J9A@mail.gmail.com>
References: <CAC8ss32K6KNVYUgiJmmw6UParhgjyQJAp9t8q2CdMjKakzHOrg@mail.gmail.com>
 <BL0PR01MB4132FD4418FADCA82CBDC3349A690@BL0PR01MB4132.prod.exchangelabs.com>
 <CAC8ss32HYyCKXXy-U9R8i_eceXxrSVsjSezo=ELF7xchoGiL5w@mail.gmail.com>
 <CAC8ss31pdJ9SisAcHYVHcp+670tKKdSQvj5V-HKaXnLyE_0J9A@mail.gmail.com>
Message-ID: <BL0PR01MB4132A5179F96C1DC16C640169A750@BL0PR01MB4132.prod.exchangelabs.com>

Ogbos,

To share data (in particularly lengthy one as yours), check out ?dput

To replicate sampling, look at ?replicate (will output to a data frame to use further) - that should answer your Q1.

Apart from that (and regarding Q2), the task you are after is getting more and more obscure to me. I don't see what you really want to test - between levels of n (or A in oodf)? If so, you would need to permute levels within each set (are they sets of -5:10, or are all observations completely independent? In the latter case, across all sets). What is the null hypothesis you want to test? And what is the data exactly? I don't understand what the two columns indicate. Then you need to decide on what the test statistic is you want to use. The average? The difference between each pair of averages? Anything like that? Do you want to test all levels simultaneously, or by pairs? 

Clearly, your way of sampling is inappropriate for a randomization test. You are rather simulating data that can take any value between min and max with equal probability. That does not seem to be the right null model to me (it might be, though, depending on your null hypothesis). It would not make a randomization test either way, but rather a Monte Carlo simulation under the null hypothesis. Then you would determine your test statistic(s) (whichever they are) and subsequently repeat that often enough and check whether your observed value is higher than 90 or 99% of the simulated ones. Again, that would be a simulation, not a randomization test.

For the latter, you'd need to permute the data in an appropriate way (again depending on the null hypothesis, and on the structure, i.e. are they are coming in blocks of any kind, or all independent), and then recalculate the test statistic every time, and then proceed as before. I'm not sure whether the data was the result of a randomized experiment - if not, you can still use the idea, but fall into something that some refer to as "permutation testing" - the difference being that you need to make much stronger assumptions on independence etc of the data. Many use the terms equivalently, but just so you are aware. 

I really think you need to look into a textbook (eg. Edgington & Onghena) or some papers to understand the concept of randomization tests, or consult with a statistician with good background in that field. What you are suggesting is not near it, and unless you have a clear hypothesis and a good understanding of how the data was generated, it is impossible for you (and anyone else) to say how such a test might be designed.

Michael 


> -----Original Message-----
> From: Ogbos Okike <giftedlife2014 at gmail.com>
> Sent: Mittwoch, 27. Februar 2019 22:53
> To: Meyners, Michael <meyners.m at pg.com>
> Cc: r-help <r-help at r-project.org>
> Subject: Re: [R] Randomization Test
> 
> Dear Kind List,
> 
> I am still battling with this. I have, however, made some progress with the
> suggestions of Micheal and others. At least, I have a better picture of what I
> want to do now as I will attempt a detailed description here.
> 
> I am aware I should show you just a small part of my code and data.
> But when I copied out a small portion and run to see what you get when I
> send that,  I was not satisfied with the signal displayed. The epoch analysis
> averages data and is quite sensitive to leveraging, especially if a small sample
> is used.
> 
> So please permit/exercise patience  me to display the series of epoch that
> give the averaged valued used. You can just run the code and see the signal
> of interest. Here is the code and the data:
> 
> dta <- read.table( text ="n CR
>  -5 8969
.
SNIP...
.
> 10 9566
> ",header=TRUE)
> 
>  data<-matrix(c(dta$CR),ncol=71)
> A<-matrix(rep(-5:10,71))
> B<-matrix(data)
> 
>  oodf<-data.frame(A,B)
>  a<--5:10
> oodf<-data.frame(A,B)
> library(plotrix)
> std.error<-function(x) return(sd(x)/(sum(!is.na(x))))
> oomean<-as.vector(by(oodf$B,oodf$A,mean))
> oose<-as.vector(by(oodf$B,oodf$A,std.error))
> plot(-5:10,oomean,type="l",ylim=c(8890,9100),
>  )
> A<-oomean-1.96*oose
>  B<-oomean+1.96*oose
> lines(a,A,col="red")
>  lines(a,B,col="red")
> 
>  My Question:
> I wish to conduct a randomization test of significance (90 and 99
> percentile) of the reductions/decreases as displayed by the signal.
> 
> I am attempting using:
> x<-sample(8890:9500,1136,replace=T )
> 
> to generate the random numbers, where 8890, 9500 and 1136 are the
> minimum  and maximum of the signal and 1136 the length of sample data.
> Q1: Please how do I generate many samples as x above, say up to 5000 or
> 10,000? I manually generated and stored as x1,x2, x3 up to x100.
> 
> Q2: Please how do I use this randomly generated numbers to test the
> statistical significance level of the signal generated by plot(-
> 5:10,oomean,type="l",ylim=c(8890,9100),  )?
> 
> I wish to test for 90% and 99% percentile.
> 
> I am sorry that this is too long.
> 
> Many thanks for your kind contributions
> 
> Best
> Ogbos
> 
> 
> 
> 
> 
> 
> 
> On Sun, Feb 10, 2019 at 3:55 PM Ogbos Okike <giftedlife2014 at gmail.com>
> wrote:
> >
> > Dear Michael,
> > This is great! Thank you.
> >
> > I have not really got any response other than yours.
> >
> > I have long before now included what I have in a paper submitted to a
> journal.
> >
> > I am awaiting the feedback of the reviewer. I will compare the
> > comments with your input here and determine the corrections to make
> > and probably return to the list for additional help.
> >
> > Best wishes
> > Ogbos
> >
> > On Fri, Feb 8, 2019 at 4:31 PM Meyners, Michael <meyners.m at pg.com>
> wrote:
> > >
> > > Ogbos,
> > >
> > > You do not seem to have received a reply over the list yet, which might
> be due to the fact that this seems rather a stats than an R question. Neither
> got your attachment (Figure) through - see posting guide.
> > >
> > > I'm not familiar with epoch analysis, so not sure what exactly you are
> doing / trying to achieve, but some general thoughts:
> > >
> > > * You do NOT want to restrict your re-randomizations in a way that "none
> of the dates corresponds with the ones in the real event" - actually, as a
> general principle, the true data must be an admissible re-randomization as
> well. You seem to have excluded that (and a lot of other randomizations at
> the same time which might have occurred, i.e. dates 1 and 2 reversed but all
> others the same), thereby rendering the test invalid. Any restrictions you
> have on your re-randomizations must've applied to the original
> randomization as well.
> > > * If you have rather observational data (which I suspect, but not sure),
> Edgington & Onghena (2007) would rather refer to this as a permutation test
> - the difference being that you have to make strong assumptions (similar to
> parametric tests) on the nature of the data, which are designed-in to be true
> for randomization tests. It might be a merely linguistic discrimination, but it is
> important to note which assumptions have to be (implicitly) made.
> > > * I'm not sure what you mean by "mean differences" of the events - is
> that two groups you are comparing? If so, that seems reasonable, but just
> make sure the test statistic you use is reasonable and sensitive against the
> alternatives you are mostly interested in. The randomization/permutation
> test will never proof that, e.g., means are significantly different, but only that
> there is SOME difference. By selecting the appropriate test statistic, you can
> influence what will pop up more easily and what not, but you can never be
> sure (unless you make strong assumptions about everything else, like in
> many parametric tests).
> > > * For any test statistic, you would then determine the proportion of its
> values among the 5000 samples where it is as large or larger than the one
> observed (or as small or smaller, or either, depending on the nature of the
> test statistic and whether you aim for a one- or a two-sided test). That is your
> p value. If small enough, conclude significance. At least conceptually
> important: The observed test statistic is always part of the re-randomization
> (i.e. your 5000) - so you truly only do 4999 plus the one you observed.
> Otherwise the test may be more or less liberal. Your p value is hence no
> smaller than 1/n, where n is the total number of samples you looked at
> (including the observed one), a p value of 0 is not possible in randomization
> tests (nor in other tests, of course).
> > >
> > > I hope this is helpful, but you will need to go through these and refer to
> your own setup to check whether you adhered to the principles or not,
> which is impossible for me to judge based on the information provided (and I
> won't be able to look at excessive code to check either).
> > >
> > > Michael
> > >
> > > > -----Original Message-----
> > > > From: R-help <r-help-bounces at r-project.org> On Behalf Of Ogbos
> > > > Okike
> > > > Sent: Montag, 28. Januar 2019 19:42
> > > > To: r-help <r-help at r-project.org>
> > > > Subject: [R] Randomization Test
> > > >
> > > > Dear Contributors,
> > > >
> > > > I conducting epoch analysis. I tried to test the significance of
> > > > my result using randomization test.
> > > >
> > > > Since I have 71 events, I randomly selected another 71 events,
> > > > making sure that none of the dates in the random events
> > > > corresponds with the ones in the real event.
> > > >
> > > > Following the code I found here
> > > >
> (https://www.uvm.edu/~dhowell/StatPages/R/RandomizationTestsWithR/
> > > > R andom2Sample/TwoIndependentSamplesR.html),
> > > > I combined these two data set and used them to generate another
> > > > 5000 events. I then plotted the graph of the mean differences for
> > > > the 5000 randomly generated events. On the graph, I indicated the
> > > > region of the mean difference between the real 71 epoch and the
> > > > randomly selected 71 epoch.
> > > >
> > > > Since the two tail test shows that the mean difference falls at
> > > > the extreme of the randomly selected events, I concluded that my
> > > > result is statistically significant.
> > > >
> > > >
> > > >
> > > > I am attaching the graph to assistance you in you suggestions.
> > > >
> > > > I can attach both my code and the real and randomly generated
> > > > events if you ask for it.
> > > >
> > > > My request is that you help me to understand if I am on the right track
> or no.
> > > > This is the first time I am doing this and except the experts
> > > > decide, I am not quite sure whether I am right or not.
> > > >
> > > > Many thanks for your kind concern.
> > > >
> > > > Best
> > > > Ogbos
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide http://www.R-project.org/posting-
> > > > guide.html and provide commented, minimal, self-contained,
> > > > reproducible code.

From @bh|@hek@gho@e@82 @end|ng |rom gm@||@com  Wed Feb 27 23:56:11 2019
From: @bh|@hek@gho@e@82 @end|ng |rom gm@||@com (Abhishek Ghose)
Date: Wed, 27 Feb 2019 14:56:11 -0800
Subject: [R] Behaviour of dfmax in glmnet
Message-ID: <CAOKQjMXaz3m1==tyWkZ9=_UKiUaTeTpCJ8fvtCocbSbcLXbQWw@mail.gmail.com>

Hi,

I am new to <i>glmnet</i>, so I do not yet understand fully what the various

parameters do. I am trying to build a multinomial classifier which restricts

the number of features used in the model. From reading the docs and some

answers on this forum, I understand <i>dfmax</i> is the way to do it. I
played

around with it a bit; I have a couple of questions and would appreciate some

help:

<h3>Setup</h3>

For a particular dataset, I want to restrict the number of features to 3;

the original data has 126 features. Here's what I run:

fit<-glmnet(data.matrix(X), data.matrix(y), family='multinomial', dfmax=3)

d<-data.frame(tidy(fit))

This is the value of <i>d</i> (inserting a screenshot since the table
columns get

disturbed by the formatting):



My questions about the output:


[1] I see multiple values of <i>lambda</i> in there; it looks like glmnet
tries

to fit lambdas that gets the number of terms close to dfmax=3. So its less

like the LARs algorithm (in the sense that we don't move stagewise by adding

variables) and more about getting the right lambdas for regularization that

lead to the intended dfmax. Is this right?

[2] I'm guessing alpha plays a role in how close we can get to dfmax. At

alpha=1, where we're doing lasso, and so its easier to get close to dfmax,

compared to when alpha=0 and we're doing ridge. Is this understanding

correct?

[3] A "neighborhood" of dfmax is the best we can do it'd seem. Or am I

missing a parameter that gets me to the model with the exact dfmax (fyi:

alpha=1 doesn't seem to get me to the precise number of non zero terms

either, at least on this dataset).

[4] what does pmax do?

-------------- next part --------------
A non-text attachment was scrubbed...
Name: dfmax.PNG
Type: image/png
Size: 54147 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190227/92b3a1a1/attachment.png>

From eb@15242 @end|ng |rom gm@||@com  Thu Feb 28 16:26:13 2019
From: eb@15242 @end|ng |rom gm@||@com (Ed Siefker)
Date: Thu, 28 Feb 2019 09:26:13 -0600
Subject: [R] inverse of which()
In-Reply-To: <ae069de31ca040d3a267dc723d021416@tamu.edu>
References: <CALRb-odXXpf3Otiz64XHibbD+3r842p5SBV9A2qiXVuC1bkrcA@mail.gmail.com>
 <ae069de31ca040d3a267dc723d021416@tamu.edu>
Message-ID: <CALRb-oc0f_c3_9GjiPzc_K4OxywHk5o0X3a0efOFvy7c+ZOsEw@mail.gmail.com>

That's exactly what I want! Thanks!
-Ed

On Wed, Feb 27, 2019 at 5:14 PM David L Carlson <dcarlson at tamu.edu> wrote:
>
> I'm not sure I completely understand your question. Would using grepl() instead of grep() let you do what you want?
>
> ----------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77843-4352
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ed Siefker
> Sent: Wednesday, February 27, 2019 5:03 PM
> To: r-help <r-help at r-project.org>
> Subject: [R] inverse of which()
>
> Given a vector of booleans, chich() will return indices that are TRUE.
>
> Given a vector of indices, how can I get a vector of booleans?
>
> My intent is to do logical operations on the output of grep().  Maybe
> there's a better way to do this?
>
> Thanks
> -Ed
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Thu Feb 28 16:38:52 2019
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Thu, 28 Feb 2019 16:38:52 +0100
Subject: [R] add points to lattice cloud plot (3D scatter)
In-Reply-To: <b771fd81-b24c-298c-d66d-9adada547cf9@gmail.com>
References: <CAMk+s2Te3OtxiXH4uszCcsSiTZiqTBbBvLMm4iFxtYgpv6Q3Bw@mail.gmail.com>
 <b771fd81-b24c-298c-d66d-9adada547cf9@gmail.com>
Message-ID: <CAMk+s2T=6kcbJ9VCfR3dQfHHMF0qVVjjTt3UcYXXdnPvUqQaoA@mail.gmail.com>

I see. I have been thinking of superimposing two plots with
par(new=TRUE), but how could I remove all the graphic parameters
(axes, background etc) keeping only the actual points in lattice? (if
possible).
Tx

On Thu, Feb 28, 2019 at 3:53 PM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>
> On 28/02/2019 5:39 a.m., Luigi Marongiu wrote:
> > Dear all,
> > is it possible to add points to a lattice cloud plot (3D scatter)? I
> > can plot the main data, but what if I wanted to add another point. In
> > R there is the high level plotting function plot(), then the low level
> > points() or lines() etc. What is the equivalent for lattice?
>
> I don't know for sure, but I don't think you can do that in lattice.
> The scatterplot3d::scatterplot3d function returns enough information to
> do this, but I don't think lattice::cloud does.  But even
> scatterplot3d::scatterplot3d won't necessarily get it right if points
> hide others that are behind them.  It uses the "painter's algorithm",
> and that needs everything to be drawn in just the right order, which you
> probably won't get if you draw things in several calls.
>
> You can draw things in arbitrary order using rgl::plot3d or related
> functions, but you'll need to do more work yourself to get an array of
> plots like lattice gives.
>
> Duncan Murdoch
>
>
> >
> > Thank you
> >
> >
> >>>>
> >
> > df = data.frame(Name = c("A", "B", "C", "D", "E"),
> >                x_axis = c(-0.591, 0.384, -0.384, -0.032, 0.754),
> >                y_axis = c(-1.302, 1.652, -1.652, 0.326, 0.652),
> >                z_axis = c(1.33, 1.33, 2.213, 0.032, -0.754),
> >                stringsAsFactors = FALSE)
> >
> > cloud(z_axis ~ x_axis * y_axis, data = df,
> >        xlab = "X", ylab = "Y", zlab = "Z",
> >        pch = 16, col = "red", type = "b", cex = 1.5,
> >        ltext(x=df$x_axis, y=df$y_axis, z=df$z_axis,
> >              labels=df$Names, pos=1, offset=1, cex=0.8)
> > )
> >
> > df2 = data.frame(Name = "F",
> >                  x_axis = 0.891,
> >                  y_axis = 2.302
> >                  z_axis = -1.83,
> >                  stringsAsFactors = FALSE)
> >
>


-- 
Best regards,
Luigi


From g||ted|||e2014 @end|ng |rom gm@||@com  Thu Feb 28 16:45:58 2019
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Thu, 28 Feb 2019 16:45:58 +0100
Subject: [R] Randomization Test: SOLVED
In-Reply-To: <BL0PR01MB4132A5179F96C1DC16C640169A750@BL0PR01MB4132.prod.exchangelabs.com>
References: <CAC8ss32K6KNVYUgiJmmw6UParhgjyQJAp9t8q2CdMjKakzHOrg@mail.gmail.com>
 <BL0PR01MB4132FD4418FADCA82CBDC3349A690@BL0PR01MB4132.prod.exchangelabs.com>
 <CAC8ss32HYyCKXXy-U9R8i_eceXxrSVsjSezo=ELF7xchoGiL5w@mail.gmail.com>
 <CAC8ss31pdJ9SisAcHYVHcp+670tKKdSQvj5V-HKaXnLyE_0J9A@mail.gmail.com>
 <BL0PR01MB4132A5179F96C1DC16C640169A750@BL0PR01MB4132.prod.exchangelabs.com>
Message-ID: <CAC8ss33kQ+qiJAD9bSVqa7tCSbxCGOvfYjVXOyqwWS3FXqSpuA@mail.gmail.com>

Dear Ben and Micheal,
Your contributions are quite useful to me!!!

Although my question is quite difficult to articulate, your attempt to
understand what I intend to do and my desperate efforts to get my
points across to you have, interestingly, combined to solve the
problem.

Indeed, the problem is specific (epoch/Chree analysis) and they are
yet a lot of misunderstanding even among scientists using them. I
initially felt quite odd asking you about it and it took me several
days before I decided to  post the question. Though I didn't receive
much of a response, I was happy that I was not trashed.

On the issue of textbook, my reviewer already pointed the example
he/she would like me to follow (a published article). I have but that
option if I want the paper to go soon.

So I spent about 3 days looking at the paper and similar ones. It was
in the course of trying to re-construct my question and re-post
yesterday that I got the idea of how to do it.

As Micheal pointed out, it is Monte Carlo technique that I was after.
If I am to follow the paper recommended, one hundred epoch selected
out of the original epoch will do the job. That will then be used to
judge the statistical significance of the observed decreases.

After posting the question yesterday, the problem became clearer to me
and the next few attempts got me to the destination.

I am thus indebted you.

With the very best wishes
Ogbos
On Thu, Feb 28, 2019 at 3:56 PM Meyners, Michael <meyners.m at pg.com> wrote:
>
> Ogbos,
>
> To share data (in particularly lengthy one as yours), check out ?dput
>
> To replicate sampling, look at ?replicate (will output to a data frame to use further) - that should answer your Q1.
>
> Apart from that (and regarding Q2), the task you are after is getting more and more obscure to me. I don't see what you really want to test - between levels of n (or A in oodf)? If so, you would need to permute levels within each set (are they sets of -5:10, or are all observations completely independent? In the latter case, across all sets). What is the null hypothesis you want to test? And what is the data exactly? I don't understand what the two columns indicate. Then you need to decide on what the test statistic is you want to use. The average? The difference between each pair of averages? Anything like that? Do you want to test all levels simultaneously, or by pairs?
>
> Clearly, your way of sampling is inappropriate for a randomization test. You are rather simulating data that can take any value between min and max with equal probability. That does not seem to be the right null model to me (it might be, though, depending on your null hypothesis). It would not make a randomization test either way, but rather a Monte Carlo simulation under the null hypothesis. Then you would determine your test statistic(s) (whichever they are) and subsequently repeat that often enough and check whether your observed value is higher than 90 or 99% of the simulated ones. Again, that would be a simulation, not a randomization test.
>
> For the latter, you'd need to permute the data in an appropriate way (again depending on the null hypothesis, and on the structure, i.e. are they are coming in blocks of any kind, or all independent), and then recalculate the test statistic every time, and then proceed as before. I'm not sure whether the data was the result of a randomized experiment - if not, you can still use the idea, but fall into something that some refer to as "permutation testing" - the difference being that you need to make much stronger assumptions on independence etc of the data. Many use the terms equivalently, but just so you are aware.
>
> I really think you need to look into a textbook (eg. Edgington & Onghena) or some papers to understand the concept of randomization tests, or consult with a statistician with good background in that field. What you are suggesting is not near it, and unless you have a clear hypothesis and a good understanding of how the data was generated, it is impossible for you (and anyone else) to say how such a test might be designed.
>
> Michael
>
>
> > -----Original Message-----
> > From: Ogbos Okike <giftedlife2014 at gmail.com>
> > Sent: Mittwoch, 27. Februar 2019 22:53
> > To: Meyners, Michael <meyners.m at pg.com>
> > Cc: r-help <r-help at r-project.org>
> > Subject: Re: [R] Randomization Test
> >
> > Dear Kind List,
> >
> > I am still battling with this. I have, however, made some progress with the
> > suggestions of Micheal and others. At least, I have a better picture of what I
> > want to do now as I will attempt a detailed description here.
> >
> > I am aware I should show you just a small part of my code and data.
> > But when I copied out a small portion and run to see what you get when I
> > send that,  I was not satisfied with the signal displayed. The epoch analysis
> > averages data and is quite sensitive to leveraging, especially if a small sample
> > is used.
> >
> > So please permit/exercise patience  me to display the series of epoch that
> > give the averaged valued used. You can just run the code and see the signal
> > of interest. Here is the code and the data:
> >
> > dta <- read.table( text ="n CR
> >  -5 8969
> .
> SNIP...
> .
> > 10 9566
> > ",header=TRUE)
> >
> >  data<-matrix(c(dta$CR),ncol=71)
> > A<-matrix(rep(-5:10,71))
> > B<-matrix(data)
> >
> >  oodf<-data.frame(A,B)
> >  a<--5:10
> > oodf<-data.frame(A,B)
> > library(plotrix)
> > std.error<-function(x) return(sd(x)/(sum(!is.na(x))))
> > oomean<-as.vector(by(oodf$B,oodf$A,mean))
> > oose<-as.vector(by(oodf$B,oodf$A,std.error))
> > plot(-5:10,oomean,type="l",ylim=c(8890,9100),
> >  )
> > A<-oomean-1.96*oose
> >  B<-oomean+1.96*oose
> > lines(a,A,col="red")
> >  lines(a,B,col="red")
> >
> >  My Question:
> > I wish to conduct a randomization test of significance (90 and 99
> > percentile) of the reductions/decreases as displayed by the signal.
> >
> > I am attempting using:
> > x<-sample(8890:9500,1136,replace=T )
> >
> > to generate the random numbers, where 8890, 9500 and 1136 are the
> > minimum  and maximum of the signal and 1136 the length of sample data.
> > Q1: Please how do I generate many samples as x above, say up to 5000 or
> > 10,000? I manually generated and stored as x1,x2, x3 up to x100.
> >
> > Q2: Please how do I use this randomly generated numbers to test the
> > statistical significance level of the signal generated by plot(-
> > 5:10,oomean,type="l",ylim=c(8890,9100),  )?
> >
> > I wish to test for 90% and 99% percentile.
> >
> > I am sorry that this is too long.
> >
> > Many thanks for your kind contributions
> >
> > Best
> > Ogbos
> >
> >
> >
> >
> >
> >
> >
> > On Sun, Feb 10, 2019 at 3:55 PM Ogbos Okike <giftedlife2014 at gmail.com>
> > wrote:
> > >
> > > Dear Michael,
> > > This is great! Thank you.
> > >
> > > I have not really got any response other than yours.
> > >
> > > I have long before now included what I have in a paper submitted to a
> > journal.
> > >
> > > I am awaiting the feedback of the reviewer. I will compare the
> > > comments with your input here and determine the corrections to make
> > > and probably return to the list for additional help.
> > >
> > > Best wishes
> > > Ogbos
> > >
> > > On Fri, Feb 8, 2019 at 4:31 PM Meyners, Michael <meyners.m at pg.com>
> > wrote:
> > > >
> > > > Ogbos,
> > > >
> > > > You do not seem to have received a reply over the list yet, which might
> > be due to the fact that this seems rather a stats than an R question. Neither
> > got your attachment (Figure) through - see posting guide.
> > > >
> > > > I'm not familiar with epoch analysis, so not sure what exactly you are
> > doing / trying to achieve, but some general thoughts:
> > > >
> > > > * You do NOT want to restrict your re-randomizations in a way that "none
> > of the dates corresponds with the ones in the real event" - actually, as a
> > general principle, the true data must be an admissible re-randomization as
> > well. You seem to have excluded that (and a lot of other randomizations at
> > the same time which might have occurred, i.e. dates 1 and 2 reversed but all
> > others the same), thereby rendering the test invalid. Any restrictions you
> > have on your re-randomizations must've applied to the original
> > randomization as well.
> > > > * If you have rather observational data (which I suspect, but not sure),
> > Edgington & Onghena (2007) would rather refer to this as a permutation test
> > - the difference being that you have to make strong assumptions (similar to
> > parametric tests) on the nature of the data, which are designed-in to be true
> > for randomization tests. It might be a merely linguistic discrimination, but it is
> > important to note which assumptions have to be (implicitly) made.
> > > > * I'm not sure what you mean by "mean differences" of the events - is
> > that two groups you are comparing? If so, that seems reasonable, but just
> > make sure the test statistic you use is reasonable and sensitive against the
> > alternatives you are mostly interested in. The randomization/permutation
> > test will never proof that, e.g., means are significantly different, but only that
> > there is SOME difference. By selecting the appropriate test statistic, you can
> > influence what will pop up more easily and what not, but you can never be
> > sure (unless you make strong assumptions about everything else, like in
> > many parametric tests).
> > > > * For any test statistic, you would then determine the proportion of its
> > values among the 5000 samples where it is as large or larger than the one
> > observed (or as small or smaller, or either, depending on the nature of the
> > test statistic and whether you aim for a one- or a two-sided test). That is your
> > p value. If small enough, conclude significance. At least conceptually
> > important: The observed test statistic is always part of the re-randomization
> > (i.e. your 5000) - so you truly only do 4999 plus the one you observed.
> > Otherwise the test may be more or less liberal. Your p value is hence no
> > smaller than 1/n, where n is the total number of samples you looked at
> > (including the observed one), a p value of 0 is not possible in randomization
> > tests (nor in other tests, of course).
> > > >
> > > > I hope this is helpful, but you will need to go through these and refer to
> > your own setup to check whether you adhered to the principles or not,
> > which is impossible for me to judge based on the information provided (and I
> > won't be able to look at excessive code to check either).
> > > >
> > > > Michael
> > > >
> > > > > -----Original Message-----
> > > > > From: R-help <r-help-bounces at r-project.org> On Behalf Of Ogbos
> > > > > Okike
> > > > > Sent: Montag, 28. Januar 2019 19:42
> > > > > To: r-help <r-help at r-project.org>
> > > > > Subject: [R] Randomization Test
> > > > >
> > > > > Dear Contributors,
> > > > >
> > > > > I conducting epoch analysis. I tried to test the significance of
> > > > > my result using randomization test.
> > > > >
> > > > > Since I have 71 events, I randomly selected another 71 events,
> > > > > making sure that none of the dates in the random events
> > > > > corresponds with the ones in the real event.
> > > > >
> > > > > Following the code I found here
> > > > >
> > (https://www.uvm.edu/~dhowell/StatPages/R/RandomizationTestsWithR/
> > > > > R andom2Sample/TwoIndependentSamplesR.html),
> > > > > I combined these two data set and used them to generate another
> > > > > 5000 events. I then plotted the graph of the mean differences for
> > > > > the 5000 randomly generated events. On the graph, I indicated the
> > > > > region of the mean difference between the real 71 epoch and the
> > > > > randomly selected 71 epoch.
> > > > >
> > > > > Since the two tail test shows that the mean difference falls at
> > > > > the extreme of the randomly selected events, I concluded that my
> > > > > result is statistically significant.
> > > > >
> > > > >
> > > > >
> > > > > I am attaching the graph to assistance you in you suggestions.
> > > > >
> > > > > I can attach both my code and the real and randomly generated
> > > > > events if you ask for it.
> > > > >
> > > > > My request is that you help me to understand if I am on the right track
> > or no.
> > > > > This is the first time I am doing this and except the experts
> > > > > decide, I am not quite sure whether I am right or not.
> > > > >
> > > > > Many thanks for your kind concern.
> > > > >
> > > > > Best
> > > > > Ogbos
> > > > > ______________________________________________
> > > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide http://www.R-project.org/posting-
> > > > > guide.html and provide commented, minimal, self-contained,
> > > > > reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Feb 28 16:56:19 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 28 Feb 2019 07:56:19 -0800
Subject: [R] add points to lattice cloud plot (3D scatter)
In-Reply-To: <CAMk+s2T=6kcbJ9VCfR3dQfHHMF0qVVjjTt3UcYXXdnPvUqQaoA@mail.gmail.com>
References: <CAMk+s2Te3OtxiXH4uszCcsSiTZiqTBbBvLMm4iFxtYgpv6Q3Bw@mail.gmail.com>
 <b771fd81-b24c-298c-d66d-9adada547cf9@gmail.com>
 <CAMk+s2T=6kcbJ9VCfR3dQfHHMF0qVVjjTt3UcYXXdnPvUqQaoA@mail.gmail.com>
Message-ID: <A987E94E-EB0C-4425-9358-98F341908DD6@dcn.davis.ca.us>

You are missing the point... lattice assembles the entire data set at once so it can adjust and synchronize all of the scales and then it generates an object that can be printed to a device. This approach is entirely incompatible with the base graphics approach of keeping global variables around that help successive functions cooperate to cumulatively build up an image.

You need to get all your points into the lattice call initially. which may involve changing how you structure the data before you create the plot. Read the vignette and some tutorials.

On February 28, 2019 7:38:52 AM PST, Luigi Marongiu <marongiu.luigi at gmail.com> wrote:
>I see. I have been thinking of superimposing two plots with
>par(new=TRUE), but how could I remove all the graphic parameters
>(axes, background etc) keeping only the actual points in lattice? (if
>possible).
>Tx
>
>On Thu, Feb 28, 2019 at 3:53 PM Duncan Murdoch
><murdoch.duncan at gmail.com> wrote:
>>
>> On 28/02/2019 5:39 a.m., Luigi Marongiu wrote:
>> > Dear all,
>> > is it possible to add points to a lattice cloud plot (3D scatter)?
>I
>> > can plot the main data, but what if I wanted to add another point.
>In
>> > R there is the high level plotting function plot(), then the low
>level
>> > points() or lines() etc. What is the equivalent for lattice?
>>
>> I don't know for sure, but I don't think you can do that in lattice.
>> The scatterplot3d::scatterplot3d function returns enough information
>to
>> do this, but I don't think lattice::cloud does.  But even
>> scatterplot3d::scatterplot3d won't necessarily get it right if points
>> hide others that are behind them.  It uses the "painter's algorithm",
>> and that needs everything to be drawn in just the right order, which
>you
>> probably won't get if you draw things in several calls.
>>
>> You can draw things in arbitrary order using rgl::plot3d or related
>> functions, but you'll need to do more work yourself to get an array
>of
>> plots like lattice gives.
>>
>> Duncan Murdoch
>>
>>
>> >
>> > Thank you
>> >
>> >
>> >>>>
>> >
>> > df = data.frame(Name = c("A", "B", "C", "D", "E"),
>> >                x_axis = c(-0.591, 0.384, -0.384, -0.032, 0.754),
>> >                y_axis = c(-1.302, 1.652, -1.652, 0.326, 0.652),
>> >                z_axis = c(1.33, 1.33, 2.213, 0.032, -0.754),
>> >                stringsAsFactors = FALSE)
>> >
>> > cloud(z_axis ~ x_axis * y_axis, data = df,
>> >        xlab = "X", ylab = "Y", zlab = "Z",
>> >        pch = 16, col = "red", type = "b", cex = 1.5,
>> >        ltext(x=df$x_axis, y=df$y_axis, z=df$z_axis,
>> >              labels=df$Names, pos=1, offset=1, cex=0.8)
>> > )
>> >
>> > df2 = data.frame(Name = "F",
>> >                  x_axis = 0.891,
>> >                  y_axis = 2.302
>> >                  z_axis = -1.83,
>> >                  stringsAsFactors = FALSE)
>> >
>>

-- 
Sent from my phone. Please excuse my brevity.


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Thu Feb 28 18:35:16 2019
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Thu, 28 Feb 2019 18:35:16 +0100
Subject: [R] add points to lattice cloud plot (3D scatter)
In-Reply-To: <A987E94E-EB0C-4425-9358-98F341908DD6@dcn.davis.ca.us>
References: <CAMk+s2Te3OtxiXH4uszCcsSiTZiqTBbBvLMm4iFxtYgpv6Q3Bw@mail.gmail.com>
 <b771fd81-b24c-298c-d66d-9adada547cf9@gmail.com>
 <CAMk+s2T=6kcbJ9VCfR3dQfHHMF0qVVjjTt3UcYXXdnPvUqQaoA@mail.gmail.com>
 <A987E94E-EB0C-4425-9358-98F341908DD6@dcn.davis.ca.us>
Message-ID: <CAMk+s2SZ1kCGpq3U9c3Ehj3KbacWNbyc9sa+o-kQu6Ek6DbEdg@mail.gmail.com>

Fair enough, thank you.

On Thu, Feb 28, 2019 at 4:56 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>
> You are missing the point... lattice assembles the entire data set at once so it can adjust and synchronize all of the scales and then it generates an object that can be printed to a device. This approach is entirely incompatible with the base graphics approach of keeping global variables around that help successive functions cooperate to cumulatively build up an image.
>
> You need to get all your points into the lattice call initially. which may involve changing how you structure the data before you create the plot. Read the vignette and some tutorials.
>
> On February 28, 2019 7:38:52 AM PST, Luigi Marongiu <marongiu.luigi at gmail.com> wrote:
> >I see. I have been thinking of superimposing two plots with
> >par(new=TRUE), but how could I remove all the graphic parameters
> >(axes, background etc) keeping only the actual points in lattice? (if
> >possible).
> >Tx
> >
> >On Thu, Feb 28, 2019 at 3:53 PM Duncan Murdoch
> ><murdoch.duncan at gmail.com> wrote:
> >>
> >> On 28/02/2019 5:39 a.m., Luigi Marongiu wrote:
> >> > Dear all,
> >> > is it possible to add points to a lattice cloud plot (3D scatter)?
> >I
> >> > can plot the main data, but what if I wanted to add another point.
> >In
> >> > R there is the high level plotting function plot(), then the low
> >level
> >> > points() or lines() etc. What is the equivalent for lattice?
> >>
> >> I don't know for sure, but I don't think you can do that in lattice.
> >> The scatterplot3d::scatterplot3d function returns enough information
> >to
> >> do this, but I don't think lattice::cloud does.  But even
> >> scatterplot3d::scatterplot3d won't necessarily get it right if points
> >> hide others that are behind them.  It uses the "painter's algorithm",
> >> and that needs everything to be drawn in just the right order, which
> >you
> >> probably won't get if you draw things in several calls.
> >>
> >> You can draw things in arbitrary order using rgl::plot3d or related
> >> functions, but you'll need to do more work yourself to get an array
> >of
> >> plots like lattice gives.
> >>
> >> Duncan Murdoch
> >>
> >>
> >> >
> >> > Thank you
> >> >
> >> >
> >> >>>>
> >> >
> >> > df = data.frame(Name = c("A", "B", "C", "D", "E"),
> >> >                x_axis = c(-0.591, 0.384, -0.384, -0.032, 0.754),
> >> >                y_axis = c(-1.302, 1.652, -1.652, 0.326, 0.652),
> >> >                z_axis = c(1.33, 1.33, 2.213, 0.032, -0.754),
> >> >                stringsAsFactors = FALSE)
> >> >
> >> > cloud(z_axis ~ x_axis * y_axis, data = df,
> >> >        xlab = "X", ylab = "Y", zlab = "Z",
> >> >        pch = 16, col = "red", type = "b", cex = 1.5,
> >> >        ltext(x=df$x_axis, y=df$y_axis, z=df$z_axis,
> >> >              labels=df$Names, pos=1, offset=1, cex=0.8)
> >> > )
> >> >
> >> > df2 = data.frame(Name = "F",
> >> >                  x_axis = 0.891,
> >> >                  y_axis = 2.302
> >> >                  z_axis = -1.83,
> >> >                  stringsAsFactors = FALSE)
> >> >
> >>
>
> --
> Sent from my phone. Please excuse my brevity.



-- 
Best regards,
Luigi


From bobby@kn|ght @end|ng |rom gm@||@com  Wed Feb 27 22:39:46 2019
From: bobby@kn|ght @end|ng |rom gm@||@com (Robert Knight)
Date: Wed, 27 Feb 2019 15:39:46 -0600
Subject: [R] Error trapping in R
In-Reply-To: <FD7AB93F-9088-4DA8-A49D-2AB370D27715@comcast.net>
References: <FD7AB93F-9088-4DA8-A49D-2AB370D27715@comcast.net>
Message-ID: <A86AAB89-31E9-4EBA-8B0A-7A973CCC3A57@gmail.com>

Some use try blocks, like found in other languages.  Put the code you want to try inside the block.

https://www.robertknight.io/blog/try-blocks-in-r-for-error-handling/ contains a quick example.  The example doesn?t raise exceptions or anything, it just contains it for you so the script keeps going.  I like handling errors with if statements inside of try blocks.

Robert



> On Feb 27, 2019, at 2:55 PM, Bernard Comcast <mcgarvey.bernard at comcast.net> wrote:
> 
> What is the recommended way to trap errors in R? My main need is to be able to trap an error and then skip a section of code if an error has occurred. In VB for Excel I used the ?On Error goto  .....? construct to do this.
> 
> Bernard
> Sent from my iPhone so please excuse the spelling!"
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From |@ke|v|n @end|ng |rom ucd@v|@@edu  Thu Feb 28 20:13:47 2019
From: |@ke|v|n @end|ng |rom ucd@v|@@edu (Lee Steven Kelvin)
Date: Thu, 28 Feb 2019 19:13:47 +0000
Subject: [R] R cairo_pdf function does not respect plotting boundaries
Message-ID: <CAO0anGmxEwS2VtAqT-i6701QrkFCL90cOneB+c_y3uJ_40aG9w@mail.gmail.com>

Hello all,

When producing a plot in R using the cairo_pdf device, the resultant plot does not respect the plotting boundaries. Lines and shaded regions will spill over the lower x-axis and the right-side y-axis (sides 1 and 4). I would like to know if it is possible to fix this behaviour when using 'cairo_pdf' in R?

As an example, see the image at this web link: https://i.stack.imgur.com/0lfZd.png

This image is a screenshot of a PDF file constructed using the following minimum working example code:

cairo_pdf(file="test.pdf", width=0.5, height=0.5)
par("mar"=c(0.25,0.25,0.25,0.25))
plot(NA, xlim=c(0,1), ylim=c(0,1), axes=FALSE)
polygon(x=c(-1,-1,2,2), y=c(-1,2,2,-1), density=5, col="green3", lwd=10)
abline(h=0.25, col="red", lwd=5)
abline(h=0.75, col="hotpink", lwd=5, lend=1)
abline(v=0.25, col="blue", lwd=5)
abline(v=0.75, col="cyan", lwd=5, lend=1)
box()
dev.off()

Here I'm plotting a shaded region in green using 'polygon', with boundaries that lie outside the plot. I'm also drawing two sets of horizontal/vertical lines using 'abline'. The first in each pair uses standard rounded line caps, whilst the second in each pair uses butt line caps.

As you can see, the shading lines and the default rounded-end ablines all extend beyond the plotting region along the lower and right-hand side axes. Only when using 'lend=1' am I able to contain the ablines to the plotting region. I know of no such fix for the shading lines however.

I would naively expect the R plotting region to be respected, and for it to be impossible to plot outside of this region unless explicitly specified by the user.

I have tested this on the other cairo devices (SVG and PS), and also reproduce the same behaviour, indicating that this is an issue with the cairo graphics API, or its implementation within R.

This behaviour does not occur when using the standard R 'pdf' graphics device. I would switch to 'pdf' in general, however, 'cairo_pdf' has several advantages over 'pdf', notably, reduced output file sizes on occasion and support for a larger array of UTF-8 characters, so ideally I would prefer to use cairo_pdf.

I should note that I have also posted this message on StackOverflow at this web link: https://stackoverflow.com/questions/54892809/r-cairo-pdf-function-does-not-respect-plotting-boundaries

Thank you in advance for any insights into this issue.

Sincerely,
Lee Kelvin



--
Dr Lee Kelvin
Department of Physics
UC Davis
One Shields Avenue
Davis, CA 95616
USA

Ph: +1 (530) 752-1500
Fax: +1 (530) 752-4717


	[[alternative HTML version deleted]]


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Thu Feb 28 21:00:06 2019
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard McGarvey)
Date: Thu, 28 Feb 2019 15:00:06 -0500 (EST)
Subject: [R] Error trapping in R
In-Reply-To: <A86AAB89-31E9-4EBA-8B0A-7A973CCC3A57@gmail.com>
References: <FD7AB93F-9088-4DA8-A49D-2AB370D27715@comcast.net>
 <A86AAB89-31E9-4EBA-8B0A-7A973CCC3A57@gmail.com>
Message-ID: <889666658.143852.1551384007271@connect.xfinity.com>

Thanks - the try() approach is exactly what I need.



Lion Bernard McGarvey

Director, Fort Myers Beach Lions Foundation, Inc.

Retired (Lilly Engineering Fellow).



> On February 27, 2019 at 4:39 PM Robert Knight <bobby.knight at gmail.com> wrote:
> 
>     Some use try blocks, like found in other languages.  Put the code you want to try inside the block.
> 
>     https://www.robertknight.io/blog/try-blocks-in-r-for-error-handling/ contains a quick example.  The example doesn?t raise exceptions or anything, it just contains it for you so the script keeps going.  I like handling errors with if statements inside of try blocks.
> 
>     Robert
> 
> 
> 
>     On Feb 27, 2019, at 2:55 PM, Bernard Comcast < mcgarvey.bernard at comcast.net mailto:mcgarvey.bernard at comcast.net > wrote:
> 
> 
>         > >         What is the recommended way to trap errors in R? My main need is to be able to trap an error and then skip a section of code if an error has occurred. In VB for Excel I used the ?On Error goto  .....? construct to do this.
> > 
> >         Bernard
> >         Sent from my iPhone so please excuse the spelling!"
> >         ______________________________________________
> >         R-help at r-project.org mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >         https://stat.ethz.ch/mailman/listinfo/r-help
> >         PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >         and provide commented, minimal, self-contained, reproducible code.
> > 
> >     > 


 

	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Thu Feb 28 22:09:59 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Thu, 28 Feb 2019 13:09:59 -0800
Subject: [R] add points to lattice cloud plot (3D scatter)
In-Reply-To: <A987E94E-EB0C-4425-9358-98F341908DD6@dcn.davis.ca.us>
References: <CAMk+s2Te3OtxiXH4uszCcsSiTZiqTBbBvLMm4iFxtYgpv6Q3Bw@mail.gmail.com>
 <b771fd81-b24c-298c-d66d-9adada547cf9@gmail.com>
 <CAMk+s2T=6kcbJ9VCfR3dQfHHMF0qVVjjTt3UcYXXdnPvUqQaoA@mail.gmail.com>
 <A987E94E-EB0C-4425-9358-98F341908DD6@dcn.davis.ca.us>
Message-ID: <0204d136-0420-17a2-0328-e05f1de4db81@comcast.net>


On 2/28/19 7:56 AM, Jeff Newmiller wrote:
> You are missing the point... lattice assembles the entire data set at once so it can adjust and synchronize all of the scales and then it generates an object that can be printed to a device. This approach is entirely incompatible with the base graphics approach of keeping global variables around that help successive functions cooperate to cumulatively build up an image.
>
> You need to get all your points into the lattice call initially. which may involve changing how you structure the data before you create the plot. Read the vignette and some tutorials.


There is an panel.identify.cloud function which will let one interact 
with a plot as one can do with identify or panel.identify. It calls a 
function that you can view with:


getAnywhere(panel.3didentify)


That said, this is only discussing the possibility of adding to an 
existing 3d plot in theory. I've never done it. I have used some of the 
other "interaction" plotting functions to add lines and curves to 
existing plot objects. You can see the options that have been described in:

?panel.identify.cloud
 ??llines

I tried search for prior question on Rhelp but could only find examples 
where annotation of existing points was attempted, not any examples of 
adding points. I wondered if one could do it by simply editing the 
x,y,z? components of an existing lattice object:


 From object created from the iris dataset:

$ panel.args.common:List of 20
 ? ..$ x?????????? : num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
 ? ..$ y?????????? : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
 ? ..$ z?????????? : num [1:150] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...


I tried working with the original example in this thread but originally 
got an error that on traceback stated that there was a problem with 
arguments to latticeParseFormula. I didn't see why that would be, but 
taking out the `ltext` call allowed completion. I then only succeeded in 
modifying the plot when I prepended new data to the existing vectors, 
but not when I appended points. Perhaps there is a counter in the 
lattice object that I have not yet seen.


Hope this helps;

David.

>
> On February 28, 2019 7:38:52 AM PST, Luigi Marongiu <marongiu.luigi at gmail.com> wrote:
>> I see. I have been thinking of superimposing two plots with
>> par(new=TRUE), but how could I remove all the graphic parameters
>> (axes, background etc) keeping only the actual points in lattice? (if
>> possible).
>> Tx
>>
>> On Thu, Feb 28, 2019 at 3:53 PM Duncan Murdoch
>> <murdoch.duncan at gmail.com> wrote:
>>> On 28/02/2019 5:39 a.m., Luigi Marongiu wrote:
>>>> Dear all,
>>>> is it possible to add points to a lattice cloud plot (3D scatter)?
>> I
>>>> can plot the main data, but what if I wanted to add another point.
>> In
>>>> R there is the high level plotting function plot(), then the low
>> level
>>>> points() or lines() etc. What is the equivalent for lattice?
>>> I don't know for sure, but I don't think you can do that in lattice.
>>> The scatterplot3d::scatterplot3d function returns enough information
>> to
>>> do this, but I don't think lattice::cloud does.  But even
>>> scatterplot3d::scatterplot3d won't necessarily get it right if points
>>> hide others that are behind them.  It uses the "painter's algorithm",
>>> and that needs everything to be drawn in just the right order, which
>> you
>>> probably won't get if you draw things in several calls.
>>>
>>> You can draw things in arbitrary order using rgl::plot3d or related
>>> functions, but you'll need to do more work yourself to get an array
>> of
>>> plots like lattice gives.
>>>
>>> Duncan Murdoch
>>>
>>>
>>>> Thank you
>>>>
>>>>
>>>> df = data.frame(Name = c("A", "B", "C", "D", "E"),
>>>>                 x_axis = c(-0.591, 0.384, -0.384, -0.032, 0.754),
>>>>                 y_axis = c(-1.302, 1.652, -1.652, 0.326, 0.652),
>>>>                 z_axis = c(1.33, 1.33, 2.213, 0.032, -0.754),
>>>>                 stringsAsFactors = FALSE)
>>>>
>>>> cloud(z_axis ~ x_axis * y_axis, data = df,
>>>>         xlab = "X", ylab = "Y", zlab = "Z",
>>>>         pch = 16, col = "red", type = "b", cex = 1.5,
>>>>         ltext(x=df$x_axis, y=df$y_axis, z=df$z_axis,
>>>>               labels=df$Names, pos=1, offset=1, cex=0.8)
>>>> )
>>>>
>>>> df2 = data.frame(Name = "F",
>>>>                   x_axis = 0.891,
>>>>                   y_axis = 2.302
>>>>                   z_axis = -1.83,
>>>>                   stringsAsFactors = FALSE)
>>>>


