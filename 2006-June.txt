From Greg.Snow at intermountainmail.org  Thu Jun  1 00:31:26 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Wed, 31 May 2006 16:31:26 -0600
Subject: [R] Nesting in Cox proportional hazards survivorship analysis
Message-ID: <07E228A5BE53C24CAD490193A7381BBB3E46DF@LP-EXCHVS07.CO.IHC.COM>

The site/station syntax is mainly useful for situations where you have
the same station id's withing different sites (e.g. there is a station
#1 in site #1 and also a different station still labeled #1 within site
#2).  It is unclear whether you really need the /station part or not (it
generally does not hurt if you include it and it is not needed).  To
take into account the heterogenity you can add +cluster(site) or
+cluster(station) to the model to tell it that there is correlation
within sites or stations (see the help on cluster and possibly on
frailty).

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jessica M Pearce
Sent: Wednesday, May 31, 2006 9:40 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Nesting in Cox proportional hazards survivorship analysis

Hello,
 
My advisor and I have been working on some survivorship analyses in R
and we are hoping to get some feedback on a particular issue involving
nesting.
 
We are interested in patterns of food discovery by ant species. Our
observations consist of time to discovery by an ant for three different
food types, each of two different sizes. These data were collected at 6
plots located in each of two states. Every plot is divided into 25
stations, at which the observations were made. In a repeated measures
style design, all stations received all levels of food type and size
over the course of 6 sampling periods. So multiple measurements are
drawn from each station and site; however, each individual bait item is
only discovered once. We also have vapor pressure deficit measurements
(a measure that combines temperature and relative humidity) for each
discovery time. Each state is being analyzed separately and we are using
the Cox proportional hazards approach.
 
It is clear from preliminary analysis that there is a strong influence
of spatial heterogenity as evidenced by significant contributions of
stations and plots to discovery. However, we are not necessarily
interested in the details of this heterogenity and simply wish to
control for it in examining the other factors of the model. Thus we
employed what we think to be the appropriate nesting syntax in the model
we are running (as gleaned from Venables and Ripley 1999, 3rd edition),
with stations being nested within sites.
 
To provide an example of the syntax, the full model with which we began
is:
 
TXa <- coxph(Surv(dt, status)~site/station+foodtype*foodsize+vpd,
data=TXbait)
 
This obviously generates a large number of terms, even as we work down
to the reduced model.
 
Is this syntax testing what we think it is testing, i.e. are we
controlling for station effects in our results? Are there potential
problems with our approach to the analysis of which we should be aware
in our interpretation? We have looked at many sources of survivorship
analysis literature and haven't seen this nesting issue discussed,
besides briefly in Venables and Ripley. We recognize that this is an
unusual use of survivorship analysis and would appreciate any insight
provided.
 
Jessica Pearce
Biology Department
University of Utah
Salt Lake City, UT

	[[alternative HTML version deleted]]


From Greg.Snow at intermountainmail.org  Thu Jun  1 00:40:20 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Wed, 31 May 2006 16:40:20 -0600
Subject: [R] follow-up
Message-ID: <07E228A5BE53C24CAD490193A7381BBB3E46E0@LP-EXCHVS07.CO.IHC.COM>

To get the averages, look at ?tapply or ?aggregate

To exclude parts of the data look at ?subset

To remove the spaces in the lines around points look at ?matplot and
?plot specifically look for type='o'

To add a legend look at ?legend.

You may also want to look at the lattice package, using xyplot with
panel.superpose may work better for you (or may not). 

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rob Tirrell
Sent: Wednesday, May 31, 2006 2:35 PM
To: r-help at stat.math.ethz.ch
Subject: [R] follow-up

Hi,
I've been able to get the data _of every individual_ into a graph (there
are
16 lines), but I have a few more questions...
How do I neatly get the average weights of each group (PC3/CO, PC3/FO,
FADU/CO and FADU/FO) and make a graph of four lines.
How do I exclude the first column (which is each animal's internal
description) - there is a spot on the x axis labeled 1 - that is blank
and presumably for that.  I think that explains why I get 17 "NAs
introduced by coercion" in warnings().
How do I add an argument to matplot() telling it to include a legend?
How do I instruct R to produce a graph that doesn't leave a space to
either side of the data point before the line is drawn?
I'll continue trying to work these out.
Thanks,
Rob

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From ehlers at math.ucalgary.ca  Thu Jun  1 00:49:18 2006
From: ehlers at math.ucalgary.ca (Peter Ehlers)
Date: Wed, 31 May 2006 16:49:18 -0600
Subject: [R] a problem 'cor' function
In-Reply-To: <BAY109-W19B17CC72E0F1B9A571EAC7930@phx.gbl>
References: <BAY109-W19B17CC72E0F1B9A571EAC7930@phx.gbl>
Message-ID: <447E1D6E.80903@math.ucalgary.ca>

Looks like another case of the most F of all FAQs: FAQ 7.31.

See if the following makes sense to you:

  pl <- iris[101:150, 3]
  all.equal(cor(pl,pl), 1)
  [1] TRUE
  cor(pl,pl) == 1
  [1] FALSE
  sprintf("%1.22g", cor(pl, pl))
  [1] "0.99999999999999989"
  sprintf("%1.22g", pl)
  [1] "6"                  "5.0999999999999996" "5.9000000000000004"
etc

Peter Ehlers

Tao Shi wrote:

> Hi list,
> 
> One of my co-workers found this problem with 'cor' in his code and I confirm it too (see below).  He's using R 2.2.1 under Win 2K and I'm using R 2.3.0 under Win XP.
> 
> ===========================================
> 
>>R.Version()
> 
> $platform
> [1] "i386-pc-mingw32"
> 
> $arch
> [1] "i386"
> 
> $os
> [1] "mingw32"
> 
> $system
> [1] "i386, mingw32"
> 
> $status
> [1] ""
> 
> $major
> [1] "2"
> 
> $minor
> [1] "3.0"
> 
> $year
> [1] "2006"
> 
> $month
> [1] "04"
> 
> $day
> [1] "24"
> 
> $`svn rev`
> [1] "37909"
> 
> $language
> [1] "R"
> 
> $version.string
> [1] "Version 2.3.0 (2006-04-24)"
> 
>>data(iris)
>>cor(iris[1:4])
> 
>              Sepal.Length Sepal.Width Petal.Length Petal.Width
> Sepal.Length       1.0000     -0.1176       0.8718      0.8179
> Sepal.Width       -0.1176      1.0000      -0.4284     -0.3661
> Petal.Length       0.8718     -0.4284       1.0000      0.9629
> Petal.Width        0.8179     -0.3661       0.9629      1.0000
> 
>>cor(iris[1:4])==1
> 
>              Sepal.Length Sepal.Width Petal.Length Petal.Width
> Sepal.Length         TRUE       FALSE        FALSE       FALSE
> Sepal.Width         FALSE        TRUE        FALSE       FALSE
> Petal.Length        FALSE       FALSE         TRUE       FALSE
> Petal.Width         FALSE       FALSE        FALSE        TRUE
> 
>>cor(iris[1:4], iris[1:4])
> 
>              Sepal.Length Sepal.Width Petal.Length Petal.Width
> Sepal.Length       1.0000     -0.1176       0.8718      0.8179
> Sepal.Width       -0.1176      1.0000      -0.4284     -0.3661
> Petal.Length       0.8718     -0.4284       1.0000      0.9629
> Petal.Width        0.8179     -0.3661       0.9629      1.0000
> 
>>cor(iris[1:4], iris[1:4])==1
> 
>              Sepal.Length Sepal.Width Petal.Length Petal.Width
> Sepal.Length         TRUE       FALSE        FALSE       FALSE
> Sepal.Width         FALSE        TRUE        FALSE       FALSE
> Petal.Length        FALSE       FALSE        FALSE       FALSE
> Petal.Width         FALSE       FALSE        FALSE        TRUE
> ===========================================
> 
> The two ways of calculating correlation seem to generate the 'same' results, but the second one doesn't appear to be numerically stable (see the 3rd diagonal element of the last matrix).
> 
> thanks,
> 
> ...Tao 
> 
> _________________________________________________________________
> Join the next generation of Hotmail and you could win the adventure of a lifetime
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Peter Ehlers
Chair, Division of Statistics and Actuarial Science
Department of Mathematics and Statistics
University of Calgary, 2500 University Dr. NW       ph: 403-220-3936
Calgary, Alberta  T2N 1N4, CANADA                  fax: 403-282-5150
email: ehlers at math.ucalgary.ca


From rdpeng at gmail.com  Thu Jun  1 01:07:34 2006
From: rdpeng at gmail.com (Roger D. Peng)
Date: Wed, 31 May 2006 19:07:34 -0400
Subject: [R] timeSeq and TimeDate analog in R ?
In-Reply-To: <31568740.505381149111616835.JavaMail.root@vms069.mailsrvcs.net>
References: <31568740.505381149111616835.JavaMail.root@vms069.mailsrvcs.net>
Message-ID: <447E21B6.5080508@gmail.com>

I think the 'fCalendar' package might be of help.

-roger

markleeds at verizon.net wrote:
> Hi All : I am attempting tomove a large amount of code from Splus to R and I was hoping that there was an equivalent in R of the Splus functions timeSeq and timeDate ?
> 
> I did an RSiteSearch but nothing came up ? 
> If the equivalent functions are part of some package, that's fine.
> Thanks a lot.
> 
>                                        Mark
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/


From steve.miller at jhu.edu  Thu Jun  1 01:28:22 2006
From: steve.miller at jhu.edu (Steve Miller)
Date: Wed, 31 May 2006 18:28:22 -0500
Subject: [R] Vertical x axis labels with Trellis dot plots
In-Reply-To: <OF7CAFA998.93D32536-ON8525717F.0065A187-8525717F.00699A97@jpmchase.com>
Message-ID: <200605312328.k4VNSRlP020220@hypatia.math.ethz.ch>


Using R 2.2.1 under Windows, I have developed a very busy Trellis dot plot
with 2 columns of 7 panels and dates on the x axes. Is there a way to print
the date labels vertically instead of horizontally so I can include more on
the graph? Also, can I take some control over which dates are printed (e.g.
1,4,7,10,13)? Nothing stood out to me on trellis.par.get().

Thanks for the help.

Steve Miller


From ccleland at optonline.net  Thu Jun  1 01:37:31 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 31 May 2006 19:37:31 -0400
Subject: [R] Vertical x axis labels with Trellis dot plots
In-Reply-To: <200605312328.k4VNSRlP020220@hypatia.math.ethz.ch>
References: <200605312328.k4VNSRlP020220@hypatia.math.ethz.ch>
Message-ID: <447E28BB.7000802@optonline.net>

See the scales section in the help page for xyplot.  This gives you 
control over the number, location, and labeling of ticks as well as the 
rotation of the labels.

Steve Miller wrote:
> Using R 2.2.1 under Windows, I have developed a very busy Trellis dot plot
> with 2 columns of 7 panels and dates on the x axes. Is there a way to print
> the date labels vertically instead of horizontally so I can include more on
> the graph? Also, can I take some control over which dates are printed (e.g.
> 1,4,7,10,13)? Nothing stood out to me on trellis.par.get().
> 
> Thanks for the help.
> 
> Steve Miller
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From p.murrell at auckland.ac.nz  Thu Jun  1 04:44:30 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Thu, 01 Jun 2006 14:44:30 +1200
Subject: [R] R News, volume 6, issue 2 is now available
Message-ID: <447E548E.2090202@stat.auckland.ac.nz>

Hi

The May 2006 issue of R News is now available on CRAN under the
Documentation/Newsletter link.

Paul
(on behalf of the R News EditorialBoard)
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From macq at llnl.gov  Thu Jun  1 05:12:18 2006
From: macq at llnl.gov (Don MacQueen)
Date: Wed, 31 May 2006 20:12:18 -0700
Subject: [R] timeSeq and TimeDate analog in R ?
In-Reply-To: <31568740.505381149111616835.JavaMail.root@vms069.mailsrvcs.net>
References: <31568740.505381149111616835.JavaMail.root@vms069.mailsrvcs.net>
Message-ID: <p06230900c0a40b44d739@[192.168.1.101]>

If you look at the help for seq() in R (?seq), you will find a 
reference to seq.POSIXt()
-Don

At 4:40 PM -0500 5/31/06, <markleeds at verizon.net> wrote:
>Hi All : I am attempting tomove a large amount of code from Splus to 
>R and I was hoping that there was an equivalent in R of the Splus 
>functions timeSeq and timeDate ?
>
>I did an RSiteSearch but nothing came up ?
>If the equivalent functions are part of some package, that's fine.
>Thanks a lot.
>
>                                        Mark
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
---------------------------------
Don MacQueen
Lawrence Livermore National Laboratory
Livermore, CA, USA


From shitao at hotmail.com  Thu Jun  1 06:35:59 2006
From: shitao at hotmail.com (Tao Shi)
Date: Thu, 1 Jun 2006 04:35:59 +0000
Subject: [R] a problem 'cor' function
Message-ID: <BAY109-W60E6EF52B50ED75E23DFBC7900@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060601/8f2d325f/attachment.pl 

From kubovy at virginia.edu  Thu Jun  1 07:19:14 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Thu, 1 Jun 2006 01:19:14 -0400
Subject: [R] Some non-fatal startup issues
Message-ID: <8032FAA9-C9F3-4952-8D97-D0E9CFD75391@virginia.edu>

Dear r-helpers,

I'm running Version 2.3.0 (2006-04-24) under OS X 10.4.6.

(1) When I start R from the terminal, it has no complaints.

(2) When I start R.app using R for Mac OS X Aqua GUI version 1.15  
(3106) I get (I have increased the quote level of what is different  
from the output of the terminal):

	...
Loading required package: grDevices
Loading required package: mvtnorm
> 2006-06-01 01:06:58.007 R[28814] CFLog (21): Error loading /Users/ 
> mk/Library/Application Enhancers/FruitMenu.ape/Contents/MacOS/ 
> FruitMenu:  error code 4, error number 0 (Symbol not found: ___eprintf
>   Referenced from: /System/Library/Frameworks/QuickTime.framework/ 
> Versions/A/QuickTime
>   Expected in: /Library/Frameworks/R.framework/Resources/lib/libstdc 
> ++.6.dylib
> )

	The following object(s) are masked _by_ .GlobalEnv :

	 .Traceback


	The following object(s) are masked from package:grid :

	 grid.xaxis grid.yaxis

> 2006-06-01 01:07:07.801 R[28814] CFLog (21): Error loading /System/ 
> Library/ScriptingAdditions/StandardAdditions.osax/Contents/MacOS/ 
> StandardAdditions:  error code 4, error number 0 (Symbol not found:  
> ___eprintf
>   Referenced from: /System/Library/Frameworks/ 
> AppleShareClientCore.framework/Versions/A/AppleShareClientCore
>   Expected in: /Library/Frameworks/R.framework/Resources/lib/libstdc 
> ++.6.dylib
> )


(3) When I start with the most recent JGR (1.4-2 is what ?JGR tells  
me), I get (here too I increased the quote level of what's different):

	...
	 grid.xaxis grid.yaxis

> Error in switch(.Platform$OS.type, windows = { :
> 	could not find function "localeToCharset"


Thought on what I may have done wrong?
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From Lorenz.Gygax at fat.admin.ch  Thu Jun  1 07:50:26 2006
From: Lorenz.Gygax at fat.admin.ch (Lorenz.Gygax at fat.admin.ch)
Date: Thu, 1 Jun 2006 07:50:26 +0200
Subject: [R] Help on glmmPQL
In-Reply-To: <20060531145422.41907.qmail@web60717.mail.yahoo.com>
Message-ID: <145C63777EF3ED41A5A99035845F7DD95C8BA2@EVD-C8001.bk.evdad.admin.ch>

See help file of glmmPQL (the first thing you should be doing):

glmmPQL(fixed, random, family, data, correlation, weights,
        control, niter = 10, verbose = TRUE, ...)
                 ^^^^^^^^^^
Arguments
...
niter maximum number of iterations.  
...

Thus, you can increase the value of niter to say 25. In my experience it is likely that the model fit is problematic if 10 iterations are not enough. Perhaps if you are lucky you were close and only some few more iterations are needed. If that is not the case, you may have too few occurrences of your response or too many explanatory variables.

Regards, Lorenz


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of 
> Francisco Redelico
> Sent: Wednesday, May 31, 2006 4:54 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Help on glmmPQL
> 
> 
> Dear R-Users,
> 
> I am trying to fit a generalized linear mixed models
> using glmmPQL, 
> 
> mymodel.glmm = glmmPQL(efect~ time+ cov1+ cov2,
>                     random = ~ fact1+ fact2+ fact3+
> fact1*time + fact2*time+ fact3*- cov1 - cov2- time|
> subject, family=poisson)
> 
> I get this error message: 
> 
> Error in lme.formula(fixed = zz ~ time + cov1 + cov2 ,
> random = ~fact1 +  : 
> iteration limit reached without convergence (9)
> 
> I used
> 
> meControl( maxIter = 10000, msMaxIter=10000,
> tolerance=1e-4, nlmStepMax=10000, msVerbose = TRUE)
> 
> But it does not work.
> 
> Could you tell me what is the error about? What does
> the number in brackets mean?
> 
> Thanks in advance
> 
> Francisco


From comtech.usa at gmail.com  Thu Jun  1 09:43:00 2006
From: comtech.usa at gmail.com (Michael)
Date: Thu, 1 Jun 2006 00:43:00 -0700
Subject: [R] why does arima returns "NAN" standard error?
Message-ID: <b1f16d9d0606010043m117f5288sb42eff00a51ee2a1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060601/dc02b21e/attachment.pl 

From lobry at biomserv.univ-lyon1.fr  Thu Jun  1 11:02:28 2006
From: lobry at biomserv.univ-lyon1.fr (Jean lobry)
Date: Thu, 1 Jun 2006 11:02:28 +0200
Subject: [R] problem when calling help.search() a second time
In-Reply-To: <mailman.13.1148637603.15698.r-help@stat.math.ethz.ch>
References: <mailman.13.1148637603.15698.r-help@stat.math.ethz.ch>
Message-ID: <p06002024c0a45b8b1c7a@[192.168.1.10]>

Dear list,

I would like to make a Sweave document that, for a given package,
automatically inserts the graphical outputs obtained when runing
the example code of high level plot functions. That is, for the
"graphics" base package, something like this:
http://pbil.univ-lyon1.fr/R/fichestd/tdr79.pdf

For this, I need the list of high level plot functions of a
given package. I have encountered a problem when calling twice the
help.search() function which is illustrated bellow:

[rufus:~] lobry% R --vanilla --quiet
>  sessionInfo()
Version 2.3.0 (2006-04-24)
powerpc-apple-darwin8.6.0

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"
>  help.search(package = "graphics", keyword = "hplot")

#this one is OK, but not the next one:

>  help.search(package = "stats", keyword = "hplot")
Erreur dans help.search(package = "stats", keyword = "hplot") :
         could not find package 'stats'
>  q()

#Now, if I switch the call order:

[rufus:~] lobry% R --vanilla --quiet
>  help.search(package = "stats", keyword = "hplot")

#this one is OK, but not the next one:

>  help.search(package = "graphics", keyword = "hplot")
Erreur dans help.search(package = "graphics", keyword = "hplot") :
         could not find package 'graphics'
>  q()
[rufus:~] lobry%


Is there something obvious I'm missing?

Thanks for any hint,

Jean.
-- 
Jean R. Lobry            (lobry at biomserv.univ-lyon1.fr)
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - LYON I,
43 Bd 11/11/1918, F-69622 VILLEURBANNE CEDEX, FRANCE
allo  : +33 472 43 12 87     fax    : +33 472 43 13 88
http://pbil.univ-lyon1.fr/members/lobry/


From p.dalgaard at biostat.ku.dk  Thu Jun  1 11:04:12 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Jun 2006 11:04:12 +0200
Subject: [R] R 2.3.1 is released
Message-ID: <x2ejy9i6wj.fsf@viggo.kubism.ku.dk>


I've rolled up R-2.3.1.tar.gz a short while ago. This version contains
several minor fixups and removes a couple of bad bugs. See the full list
of changes below.

You can get it (in a short while) from

http://cran.r-project.org/src/base/R-2/R-2.3.1.tar.gz

or wait for it to be mirrored at a CRAN site nearer to you. Binaries
for various platforms will appear in due course.
 
There is also a version split for floppies. (This will be the last
time we build those.)

        For the R Core Team

        Peter Dalgaard

These are the md5sums for the freshly created files, in case you wish
to check that they are uncorrupted:

c663533d4d6c86f2a192c2b86f5bac12  AUTHORS
eb723b61539feef013de476e68b5c50a  COPYING
a6f89e2100d9b6cdffcea4f398e37343  COPYING.LIB
7eac2a82dfd523caf7e73eb08ab530a4  FAQ
70447ae7f2c35233d3065b004aa4f331  INSTALL
a592ca1b1582f94d3f468dd7ccb38c18  NEWS
88bbd6781faedc788a1cbd434194480c  ONEWS
4f004de59e24a52d0f500063b4603bcb  OONEWS
a1d6d8081af4c71006a6e5a3ed54e4da  R-2.3.1.tar.gz
0ae4891a63b3d7ebf82b1ff70c4f2dcb  R-2.3.1.tar.gz-split.aa
142e3ecbde7d47b85dcad45782ee3b52  R-2.3.1.tar.gz-split.ab
f6fafac0e6c03dd1523d09960e56c7ca  R-2.3.1.tar.gz-split.ac
8a076dd800893b56184819570b8b2863  R-2.3.1.tar.gz-split.ad
5ed399fa2181575ba67c11c81e33dba5  R-2.3.1.tar.gz-split.ae
6074ecc86fad61d679eea80515ab9944  R-2.3.1.tar.gz-split.af
7ba58cf3d6d55044738439518432d846  R-2.3.1.tar.gz-split.ag
e32d05cb40e79b49489ba09cf8fa654c  R-2.3.1.tar.gz-split.ah
8e9f7675844031803a2076a73271b2c1  R-2.3.1.tar.gz-split.ai
12a494e22ddd220c4cefdb60cc532308  R-2.3.1.tar.gz-split.aj
a1d6d8081af4c71006a6e5a3ed54e4da  R-latest.tar.gz
433182754c05c2cf7a04ad0da474a1d0  README
020479f381d5f9038dcb18708997f5da  RESOURCES
4eaf8a3e428694523edc16feb0140206  THANKS

Here is the relevant bit of the NEWS file:


		CHANGES IN R VERSION 2.3.1


NEW FEATURES

    o	In lm() and glm(), offsets are allowed to be length 1 (and
	if so are replicated to the number of cases).

    o	The \uxxxx notation for Unicode characters in input strings
	can now be used on any platform which supports MBCS, even if
	the current locale is not MBCS (provided that the Unicode
	character is valid in the current character set).

    o	The quasibinomial() family now allows the "cauchit" link. (PR#8851)

    o	edit.data.frame() no longer (silently) coerces character columns
	to factor.


C-LEVEL FACILITIES

    o	The variables controlling stack checking are made available via
        Rinterface.h to front-ends embedding R: see 'Writing R Extensions'

    o	R_SignalHandlers (defined in Rinterface.h) can be set to 0 to
        suppress the R signal handlers in front-ends embedding R.


INSTALLATION CHANGES

    o	There have been a number of changes to help installation on
	platforms that no one had beta-tested.

	- Changes related to older header files, e.g. on Redhat 8.0/9.

	- Problems with 'make install' on older (<3) versions of bash
          on Solaris and elsewhere.

	- AIX 5.2/gcc issues with needing -lm when making modules X11
          and vfonts.

	- Some versions of Solaris and AIX had a fcntl.h that
	  redefined 'open' to be 'open64' and thereby broke
	  compilation of src/main/connections.c and elsewhere.

    o   'make uninstall' works better on a build using a named
	subarchitecture.


BUG FIXES

    o	min(), max(), sum() and prod() gave nonsensical answers with
	an empty list or raw argument.

    o	sum() on a data frame did not allow multiple arguments.	 (PR#8385)

    o	charmatch() and pmatch() did not specify they applied only to
	character vectors.  Now they do, and attempt to coerce 'x' and
	'target' to character before attempting matching.

    o	The Summary() methods for data.frame, Date, POSIXct, POSIXlt
	and difftime all required an argument which can match 'x',
	although the generics did not.

    o	regexpr() now accepts 0-length 'text' inputs.

    o	help.search() no longer errors out on a wrongly installed package
	(with no "hsearch.rds" file).

    o	The LaTeX version of the package reference manual was omitting
	some topics, and was not sorting the foo-package topic first.

    o	Serializing (e.g. via save()) is better protected against C
	stack overflow, which will now abort the conversion but no
	longer crashes the R process on some platforms.

    o	rbind()ing dataframes with a single row could lead to a
	corrupt data frame (a problem with the fix to PR#8506).

    o	plot(lm(y ~ 1)) now works also for 'which = 5'.

    o	dbeta(0, 1, a, 0) now correctly gives 'a' (limit) instead of 0,
	and dbeta(0, a, b, ncp) now returns Inf instead of NaN.

    o	demo(Hershey) was failing on the Cyrillic octal codes in locales
	(e.g. UTF-8) in which these are invalid.

    o	mean() on an integer (or logical) vector was treating NAs as
	actual values (unless na.rm = TRUE).

    o	mean() on a complex vector was calculated incorrectly in
        code to improve precision (PR#8842, John Peters).

    o	Graphical parameters bg, cex, col, lty, and lwd were being
	checked as being of length one even by functions such as
	title() that ignored them.  (Functions such as lines() and
	points() allow them to be of length > 1, so they might be
	passed through ... to other high-level graphical functions
	which then used to reject them.)

    o	str() now is fast again for large character vectors.

    o	edit() would default the environment of a function to .BaseEnv,
	instead of to .GlobalEnv.

    o	lm() and glm() coerce their 'weights' and 'offset' values to
	vector to avoid problems with specifying them as 1D or n x 1
	arrays.

    o	image() with one or both axes on log scales would give a
	spurious warning; contour() would give an error.

    o	legend() with log axes would place the title in the wrong place.

    o	edit.data.frame() was not returning factors edited with
	factor.mode="numeric" to factors.

    o	edit.matrix() tried to set rownames and colnames from the
	original matrix even if the sizes had been altered, and
	ignored changes made to the column names.  edit.row.names has
	a more sensible default (if the rownames are non-NULL).

    o	bindingIsLocked() was returning invalid values of a logical
	vector on some platforms.

    o	merge.data.frame() did not make the column names unique (by
	appending elements of 'suffixes') when performing a Cartesian
	product.  (PR#8676)

    o	rbind.data.frame() matches up the names of columns (which was
	undocumented), but failed to do so when checking if it was
	dealing with a factor column.  (PR#8868)

	If rbind() was used on data frames with duplicated names it
	produced a corrupt data frame.

    o	dt(x, df, ncp= not.0) now longer gives erratic values for
	|x| < ~1e-12.  (PR#8874)

    o	\code{\linkS4class{.}} now works.

    o	ccf() aligns time series by ts.intersect() rather than
	ts.union() and so is less likely to need a non-default
	na.action.  (PR#8893)

    o	optim(method="CG") could return a value that did not
	correspond to $par for very badly behaved functions on which
	the second phase of the line search failed.  (PR#8786)

    o	print.ts() could fail on a corrupt time series: it now warns and
	does the best it can.


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From ripley at stats.ox.ac.uk  Thu Jun  1 11:54:56 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 1 Jun 2006 10:54:56 +0100 (BST)
Subject: [R] problem when calling help.search() a second time
In-Reply-To: <p06002024c0a45b8b1c7a@[192.168.1.10]>
References: <mailman.13.1148637603.15698.r-help@stat.math.ethz.ch>
	<p06002024c0a45b8b1c7a@[192.168.1.10]>
Message-ID: <Pine.LNX.4.64.0606011052510.1042@gannet.stats.ox.ac.uk>

On Thu, 1 Jun 2006, Jean lobry wrote:

> Dear list,
>
> I would like to make a Sweave document that, for a given package,
> automatically inserts the graphical outputs obtained when runing
> the example code of high level plot functions. That is, for the
> "graphics" base package, something like this:
> http://pbil.univ-lyon1.fr/R/fichestd/tdr79.pdf
>
> For this, I need the list of high level plot functions of a
> given package. I have encountered a problem when calling twice the
> help.search() function which is illustrated bellow:
>
> [rufus:~] lobry% R --vanilla --quiet
>>  sessionInfo()
> Version 2.3.0 (2006-04-24)
> powerpc-apple-darwin8.6.0
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
>>  help.search(package = "graphics", keyword = "hplot")
>
> #this one is OK, but not the next one:
>
>>  help.search(package = "stats", keyword = "hplot")
> Erreur dans help.search(package = "stats", keyword = "hplot") :
>         could not find package 'stats'
>>  q()
>
> #Now, if I switch the call order:
>
> [rufus:~] lobry% R --vanilla --quiet
>>  help.search(package = "stats", keyword = "hplot")
>
> #this one is OK, but not the next one:
>
>>  help.search(package = "graphics", keyword = "hplot")
> Erreur dans help.search(package = "graphics", keyword = "hplot") :
>         could not find package 'graphics'
>>  q()
> [rufus:~] lobry%
>
>
> Is there something obvious I'm missing?

Argument 'rebuild'.  The error message will be better in R-devel.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From assampryseley at yahoo.com  Thu Jun  1 12:25:22 2006
From: assampryseley at yahoo.com (Pryseley Assam)
Date: Thu, 1 Jun 2006 03:25:22 -0700 (PDT)
Subject: [R] Help: lme
Message-ID: <20060601102522.40346.qmail@web37110.mail.mud.yahoo.com>

Good day R-Users,
   
  I have a problem accessing some values in the output from the summary of an lme fit.
   
  
The structure of my data is as shown below (I have attached a copy of the full data).
  
 
  id trials endp Z.sas       ST
  1      1   -1         -1        42.42884
  1      1    1         -1         48.12007
  2      1   -1         -1        43.42878
  2      1    1         -1         46.82817
  3      1   -1         -1        45.56736
  













.
  598  10  -1        1          49.59715
  598  10   1        1          55.46324
  599  10   -1       1          46.80873
  599  10    1       1          53.52223
  600  10   -1       1          48.58257
  600  10    1       1          56.78674
  
 I used the following code to fit my model of interest:
   
  ggg <- lme (ST~ -1 + as.factor(endp):Z.sas + as.factor(endp), data=dat4a,
  random=~-1 + as.factor(endp) + as.factor(endp):Z.sas|as.factor(trials),
  correlation = corSymm(form=~1|as.factor(trials)/as.factor(id)), weights=varIdent(form=~1|endp))
   
  hh <- summary(ggg)
  hh
   
  Below is the following part of the output of interest I wish to access:
   
  Correlation Structure: General
 Formula: ~1 | as.factor(trials)/as.factor(id) 
 Parameter estimate(s):
 Correlation: 
  1    
2 0.785
Variance function:
 Structure: Different standard deviations per stratum
 Formula: ~1 | endp 
 Parameter estimates:
       -1         1 
1.0000000 0.9692405 
   
  I wish to access the value of the correlation (0.785) and the vector of the variance function estimates (1, 0.969). I know these can be done through the intervals function, but sometimes when the estimated Hessian matrix is not positive definite or something like that (i am not quite sure), the intervals function delivers an error message. 
   
  Thus, i will like to ask if there is another way to access these values. I tried using the following code:
   
  hh$modelStruct$corStruct[1]
  hh$modelStruct$varStruct[1]
   
  Rather the output was:
   
  > hh$modelStruct$corStruct[1]
[1] -1.308580
> hh$modelStruct$varStruct[1]
[1] -0.03124255
   
  I presume there is a way to calculate the correlation and variance function coefficients using these values. 
   
  Could you tell me how to access those values (without using the intervals function) or better still how to calculate the values from the last output values. 
   
  Kind regards
  Pryseley
   
   

__________________________________________________


-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: dat4a.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060601/49b07c35/attachment.txt 

From rita.sousa at ine.pt  Thu Jun  1 12:40:38 2006
From: rita.sousa at ine.pt (Rita Sousa)
Date: Thu, 1 Jun 2006 11:40:38 +0100 
Subject: [R] FW: How to create a new package?
Message-ID: <2F01C2D1A2A44B41BD01DDE5F72E1CAB8E37E5@rngpew02.drn.ine.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060601/b010e86d/attachment.pl 

From rita.sousa at ine.pt  Thu Jun  1 12:40:38 2006
From: rita.sousa at ine.pt (Rita Sousa)
Date: Thu, 1 Jun 2006 11:40:38 +0100 
Subject: [R] FW: How to create a new package?
Message-ID: <2F01C2D1A2A44B41BD01DDE5F72E1CAB8E37E5@rngpew02.drn.ine.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060601/b010e86d/attachment-0001.pl 

From ggrothendieck at gmail.com  Thu Jun  1 13:20:22 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 1 Jun 2006 07:20:22 -0400
Subject: [R] FW: How to create a new package?
In-Reply-To: <2F01C2D1A2A44B41BD01DDE5F72E1CAB8E37E5@rngpew02.drn.ine.pt>
References: <2F01C2D1A2A44B41BD01DDE5F72E1CAB8E37E5@rngpew02.drn.ine.pt>
Message-ID: <971536df0606010420g39d4de7bqdd3eb7f95757dd8d@mail.gmail.com>

The minimum is to create a DESCRIPTION file, plus R and man
directories containing R code and .Rd files respectively.
It might help to run  Rcmd CHECK mypkg  before installation
and fix any problems it finds.

Googling for   creating R package   will locate some tutorials.


On 6/1/06, Rita Sousa <rita.sousa at ine.pt> wrote:
> Hi,
>
>
>
> I'm a group of functions and I would like to create a package for load in R.
> I have created a directory named INE and a directory below that named R, for
> the files of R functions. A have created the files DESCRIPTION and INDEX in
> the INE directory. The installation from local zip files, in the R 2.3.0,
> results but to load the package I get an error like:
>
>
>
> 'INE' is not a valid package -- installed < 2.0.0?
>
>
>
> I think that is necessary create a Meta directory with package.rds file, but
> I don't know make it! I have read the manual 'Writing R Extensions - 1.
> Creating R packages' but I don't understand the procedure...
>
> Can I create it automatically?
>
>
>
> Could you help me with this?
>
>
>
> Thanks,
>
> ---------------------------------------------------
> Rita Sousa
> DME - ME: Departamento de Metodologia Estat?stica - M?todos Estat?sticos
> INE - DRP: Instituto Nacional de Estat?stica - Delega??o Regional do Porto
> Tel.: 22 6072016 (Extens?o: 4116)
> ---------------------------------------------------
>
>
>
>
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>


From michael.watson at bbsrc.ac.uk  Thu Jun  1 13:23:16 2006
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 1 Jun 2006 12:23:16 +0100
Subject: [R] FW: How to create a new package?
Message-ID: <8975119BCD0AC5419D61A9CF1A923E9503008E29@iahce2ksrv1.iah.bbsrc.ac.uk>

?package.skeleton 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
Sent: 01 June 2006 12:20
To: Rita Sousa
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] FW: How to create a new package?

The minimum is to create a DESCRIPTION file, plus R and man directories containing R code and .Rd files respectively.
It might help to run  Rcmd CHECK mypkg  before installation and fix any problems it finds.

Googling for   creating R package   will locate some tutorials.


On 6/1/06, Rita Sousa <rita.sousa at ine.pt> wrote:
> Hi,
>
>
>
> I'm a group of functions and I would like to create a package for load in R.
> I have created a directory named INE and a directory below that named 
> R, for the files of R functions. A have created the files DESCRIPTION 
> and INDEX in the INE directory. The installation from local zip files, 
> in the R 2.3.0, results but to load the package I get an error like:
>
>
>
> 'INE' is not a valid package -- installed < 2.0.0?
>
>
>
> I think that is necessary create a Meta directory with package.rds 
> file, but I don't know make it! I have read the manual 'Writing R Extensions - 1.
> Creating R packages' but I don't understand the procedure...
>
> Can I create it automatically?
>
>
>
> Could you help me with this?
>
>
>
> Thanks,
>
> ---------------------------------------------------
> Rita Sousa
> DME - ME: Departamento de Metodologia Estat?stica - M?todos 
> Estat?sticos INE - DRP: Instituto Nacional de Estat?stica - Delega??o 
> Regional do Porto
> Tel.: 22 6072016 (Extens?o: 4116)
> ---------------------------------------------------
>
>
>
>
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From HDoran at air.org  Thu Jun  1 13:37:36 2006
From: HDoran at air.org (Doran, Harold)
Date: Thu, 1 Jun 2006 07:37:36 -0400
Subject: [R] Help: lme
Message-ID: <2323A6D37908A847A7C32F1E3662C80E02A390@dc1ex01.air.org>

Pryseley:

You asked this question last week and received answers to that question
along with other suggestions from Doug Bates and myself. Is there some
reason the post by Andrew Robinson is not working?

Harold

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pryseley Assam
> Sent: Thursday, June 01, 2006 6:25 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Help: lme
> 
> Good day R-Users,
>    
>   I have a problem accessing some values in the output from 
> the summary of an lme fit.
>    
>   
> The structure of my data is as shown below (I have attached a 
> copy of the full data).
>   
>  
>   id trials endp Z.sas       ST
>   1      1   -1         -1        42.42884
>   1      1    1         -1         48.12007
>   2      1   -1         -1        43.42878
>   2      1    1         -1         46.82817
>   3      1   -1         -1        45.56736
>   ...........................................
>   598  10  -1        1          49.59715
>   598  10   1        1          55.46324
>   599  10   -1       1          46.80873
>   599  10    1       1          53.52223
>   600  10   -1       1          48.58257
>   600  10    1       1          56.78674
>   
>  I used the following code to fit my model of interest:
>    
>   ggg <- lme (ST~ -1 + as.factor(endp):Z.sas + 
> as.factor(endp), data=dat4a,
>   random=~-1 + as.factor(endp) + 
> as.factor(endp):Z.sas|as.factor(trials),
>   correlation = 
> corSymm(form=~1|as.factor(trials)/as.factor(id)), 
> weights=varIdent(form=~1|endp))
>    
>   hh <- summary(ggg)
>   hh
>    
>   Below is the following part of the output of interest I 
> wish to access:
>    
>   Correlation Structure: General
>  Formula: ~1 | as.factor(trials)/as.factor(id)  Parameter estimate(s):
>  Correlation: 
>   1    
> 2 0.785
> Variance function:
>  Structure: Different standard deviations per stratum
>  Formula: ~1 | endp
>  Parameter estimates:
>        -1         1 
> 1.0000000 0.9692405 
>    
>   I wish to access the value of the correlation (0.785) and 
> the vector of the variance function estimates (1, 0.969). I 
> know these can be done through the intervals function, but 
> sometimes when the estimated Hessian matrix is not positive 
> definite or something like that (i am not quite sure), the 
> intervals function delivers an error message. 
>    
>   Thus, i will like to ask if there is another way to access 
> these values. I tried using the following code:
>    
>   hh$modelStruct$corStruct[1]
>   hh$modelStruct$varStruct[1]
>    
>   Rather the output was:
>    
>   > hh$modelStruct$corStruct[1]
> [1] -1.308580
> > hh$modelStruct$varStruct[1]
> [1] -0.03124255
>    
>   I presume there is a way to calculate the correlation and 
> variance function coefficients using these values. 
>    
>   Could you tell me how to access those values (without using 
> the intervals function) or better still how to calculate the 
> values from the last output values. 
>    
>   Kind regards
>   Pryseley
>    
>    
> 
> __________________________________________________
> 
> 
>


From ligges at statistik.uni-dortmund.de  Thu Jun  1 13:43:12 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Jun 2006 13:43:12 +0200
Subject: [R] FW: How to create a new package?
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E9503008E29@iahce2ksrv1.iah.bbsrc.ac.uk>
References: <8975119BCD0AC5419D61A9CF1A923E9503008E29@iahce2ksrv1.iah.bbsrc.ac.uk>
Message-ID: <447ED2D0.4090107@statistik.uni-dortmund.de>

michael watson (IAH-C) wrote:

> ?package.skeleton 


Folks, please!


Rita Sousa told you she already has a DESCRIPTION file.

Obviously, Rita forgot to build and *install* the package using
R CMD build
and
R CMD INSTALL

(the Meta directory is creating during the installation procedure)

Please note that using the Windows GUI, you can only install binary 
packages, but you have got a source package so far. Hence you need to 
install from the Windows command shell using R CMD INSTALL.

Please see the R Installation and Administration manual on how to 
install packages. For some examples of what is mentioned in that manual 
(how to set stuff up in Windows), you might additionally want to take a 
look into the article "R Help Desk: Make `R CMD' Work under Windows - an 
Example" in R News 5 (2), 27-28.

Best,
Uwe Ligges



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
> Sent: 01 June 2006 12:20
> To: Rita Sousa
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] FW: How to create a new package?
> 
> The minimum is to create a DESCRIPTION file, plus R and man directories containing R code and .Rd files respectively.
> It might help to run  Rcmd CHECK mypkg  before installation and fix any problems it finds.
> 
> Googling for   creating R package   will locate some tutorials.
> 
> 
> On 6/1/06, Rita Sousa <rita.sousa at ine.pt> wrote:
> 
>>Hi,
>>
>>
>>
>>I'm a group of functions and I would like to create a package for load in R.
>>I have created a directory named INE and a directory below that named 
>>R, for the files of R functions. A have created the files DESCRIPTION 
>>and INDEX in the INE directory. The installation from local zip files, 
>>in the R 2.3.0, results but to load the package I get an error like:
>>
>>
>>
>>'INE' is not a valid package -- installed < 2.0.0?
>>
>>
>>
>>I think that is necessary create a Meta directory with package.rds 
>>file, but I don't know make it! I have read the manual 'Writing R Extensions - 1.
>>Creating R packages' but I don't understand the procedure...
>>
>>Can I create it automatically?
>>
>>
>>
>>Could you help me with this?
>>
>>
>>
>>Thanks,
>>
>>---------------------------------------------------
>>Rita Sousa
>>DME - ME: Departamento de Metodologia Estat?stica - M?todos 
>>Estat?sticos INE - DRP: Instituto Nacional de Estat?stica - Delega??o 
>>Regional do Porto
>>Tel.: 22 6072016 (Extens?o: 4116)
>>---------------------------------------------------
>>
>>
>>
>>
>>       [[alternative HTML version deleted]]
>>
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From michael.watson at bbsrc.ac.uk  Thu Jun  1 14:33:56 2006
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 1 Jun 2006 13:33:56 +0100
Subject: [R] FW: How to create a new package?
Message-ID: <8975119BCD0AC5419D61A9CF1A923E9503008E2B@iahce2ksrv1.iah.bbsrc.ac.uk>

In that case, I have found the following useful:

http://www.biostat.jhsph.edu/~kbroman/Rintro/Rwinpack.html 

-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
Sent: 01 June 2006 12:43
To: michael watson (IAH-C)
Cc: Gabor Grothendieck; Rita Sousa; r-help at stat.math.ethz.ch
Subject: Re: [R] FW: How to create a new package?

michael watson (IAH-C) wrote:

> ?package.skeleton


Folks, please!


Rita Sousa told you she already has a DESCRIPTION file.

Obviously, Rita forgot to build and *install* the package using
R CMD build
and
R CMD INSTALL

(the Meta directory is creating during the installation procedure)

Please note that using the Windows GUI, you can only install binary 
packages, but you have got a source package so far. Hence you need to 
install from the Windows command shell using R CMD INSTALL.

Please see the R Installation and Administration manual on how to 
install packages. For some examples of what is mentioned in that manual 
(how to set stuff up in Windows), you might additionally want to take a 
look into the article "R Help Desk: Make `R CMD' Work under Windows - an 
Example" in R News 5 (2), 27-28.

Best,
Uwe Ligges



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
> Sent: 01 June 2006 12:20
> To: Rita Sousa
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] FW: How to create a new package?
> 
> The minimum is to create a DESCRIPTION file, plus R and man directories containing R code and .Rd files respectively.
> It might help to run  Rcmd CHECK mypkg  before installation and fix any problems it finds.
> 
> Googling for   creating R package   will locate some tutorials.
> 
> 
> On 6/1/06, Rita Sousa <rita.sousa at ine.pt> wrote:
> 
>>Hi,
>>
>>
>>
>>I'm a group of functions and I would like to create a package for load in R.
>>I have created a directory named INE and a directory below that named 
>>R, for the files of R functions. A have created the files DESCRIPTION 
>>and INDEX in the INE directory. The installation from local zip files, 
>>in the R 2.3.0, results but to load the package I get an error like:
>>
>>
>>
>>'INE' is not a valid package -- installed < 2.0.0?
>>
>>
>>
>>I think that is necessary create a Meta directory with package.rds 
>>file, but I don't know make it! I have read the manual 'Writing R Extensions - 1.
>>Creating R packages' but I don't understand the procedure...
>>
>>Can I create it automatically?
>>
>>
>>
>>Could you help me with this?
>>
>>
>>
>>Thanks,
>>
>>---------------------------------------------------
>>Rita Sousa
>>DME - ME: Departamento de Metodologia Estat?stica - M?todos 
>>Estat?sticos INE - DRP: Instituto Nacional de Estat?stica - Delega??o 
>>Regional do Porto
>>Tel.: 22 6072016 (Extens?o: 4116)
>>---------------------------------------------------
>>
>>
>>
>>
>>       [[alternative HTML version deleted]]
>>
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From rdiaz at cnio.es  Thu Jun  1 14:43:25 2006
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Thu, 1 Jun 2006 14:43:25 +0200
Subject: [R] FW: How to create a new package?
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E9503008E29@iahce2ksrv1.iah.bbsrc.ac.uk>
References: <8975119BCD0AC5419D61A9CF1A923E9503008E29@iahce2ksrv1.iah.bbsrc.ac.uk>
Message-ID: <200606011443.25532.rdiaz@cnio.es>

Dear Rita,

Do you want a package just for yourself, or something useful for others, with 
docs, etc? I think the rest of the answers in this thread will help you 
create a "full fledged" package. See also the detailed explanation 
in "Writing R extensions".

If you just want something quick and dirty that allows you to use a bunch of 
functions without using "source" (and thus cluttering your global workspace), 
is easy to move around, etc, you just need a directory structure such as:

SignS2/
SignS2/R/
SignS2/R/SignS2.R
SignS2/DESCRIPTION
SignS2/Changes

(Change SignS2 for the name of your package).

This has no documentation whatsoever. You can get rid of the "changes" file, 
but I put it there to keep track of changes.

Run R CMD check against the directory (of coruse, you'll get warnings about 
missing documentation), and then R CMD build.

Best,

R.


On Thursday 01 June 2006 13:23, michael watson (IAH-C) wrote:
> ?package.skeleton
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
> Sent: 01 June 2006 12:20
> To: Rita Sousa
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] FW: How to create a new package?
>
> The minimum is to create a DESCRIPTION file, plus R and man directories
> containing R code and .Rd files respectively. It might help to run  Rcmd
> CHECK mypkg  before installation and fix any problems it finds.
>
> Googling for   creating R package   will locate some tutorials.
>
> On 6/1/06, Rita Sousa <rita.sousa at ine.pt> wrote:
> > Hi,
> >
> >
> >
> > I'm a group of functions and I would like to create a package for load in
> > R. I have created a directory named INE and a directory below that named
> > R, for the files of R functions. A have created the files DESCRIPTION and
> > INDEX in the INE directory. The installation from local zip files, in the
> > R 2.3.0, results but to load the package I get an error like:
> >
> >
> >
> > 'INE' is not a valid package -- installed < 2.0.0?
> >
> >
> >
> > I think that is necessary create a Meta directory with package.rds
> > file, but I don't know make it! I have read the manual 'Writing R
> > Extensions - 1. Creating R packages' but I don't understand the
> > procedure...
> >
> > Can I create it automatically?
> >
> >
> >
> > Could you help me with this?
> >
> >
> >
> > Thanks,
> >
> > ---------------------------------------------------
> > Rita Sousa
> > DME - ME: Departamento de Metodologia Estat?stica - M?todos
> > Estat?sticos INE - DRP: Instituto Nacional de Estat?stica - Delega??o
> > Regional do Porto
> > Tel.: 22 6072016 (Extens?o: 4116)
> > ---------------------------------------------------
> >
> >
> >
> >
> >        [[alternative HTML version deleted]]
> >
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Ram?n D?az-Uriarte
Bioinformatics 
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)



**NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en s...{{dropped}}


From ligges at statistik.uni-dortmund.de  Thu Jun  1 14:48:05 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Jun 2006 14:48:05 +0200
Subject: [R] FW: How to create a new package?
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E9503008E2B@iahce2ksrv1.iah.bbsrc.ac.uk>
References: <8975119BCD0AC5419D61A9CF1A923E9503008E2B@iahce2ksrv1.iah.bbsrc.ac.uk>
Message-ID: <447EE205.7020507@statistik.uni-dortmund.de>

michael watson (IAH-C) wrote:
> In that case, I have found the following useful:
> 
> http://www.biostat.jhsph.edu/~kbroman/Rintro/Rwinpack.html 


This page is really outdated and not comprehensive, since some relevant 
information is missing.
The manual contains even more *relevant* information, and it is up to 
date! What's so difficult with the manual, please?

Uwe Ligges


> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
> Sent: 01 June 2006 12:43
> To: michael watson (IAH-C)
> Cc: Gabor Grothendieck; Rita Sousa; r-help at stat.math.ethz.ch
> Subject: Re: [R] FW: How to create a new package?
> 
> michael watson (IAH-C) wrote:
> 
> 
>>?package.skeleton
> 
> 
> 
> Folks, please!
> 
> 
> Rita Sousa told you she already has a DESCRIPTION file.
> 
> Obviously, Rita forgot to build and *install* the package using
> R CMD build
> and
> R CMD INSTALL
> 
> (the Meta directory is creating during the installation procedure)
> 
> Please note that using the Windows GUI, you can only install binary 
> packages, but you have got a source package so far. Hence you need to 
> install from the Windows command shell using R CMD INSTALL.
> 
> Please see the R Installation and Administration manual on how to 
> install packages. For some examples of what is mentioned in that manual 
> (how to set stuff up in Windows), you might additionally want to take a 
> look into the article "R Help Desk: Make `R CMD' Work under Windows - an 
> Example" in R News 5 (2), 27-28.
> 
> Best,
> Uwe Ligges
> 
> 
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
>>Sent: 01 June 2006 12:20
>>To: Rita Sousa
>>Cc: r-help at stat.math.ethz.ch
>>Subject: Re: [R] FW: How to create a new package?
>>
>>The minimum is to create a DESCRIPTION file, plus R and man directories containing R code and .Rd files respectively.
>>It might help to run  Rcmd CHECK mypkg  before installation and fix any problems it finds.
>>
>>Googling for   creating R package   will locate some tutorials.
>>
>>
>>On 6/1/06, Rita Sousa <rita.sousa at ine.pt> wrote:
>>
>>
>>>Hi,
>>>
>>>
>>>
>>>I'm a group of functions and I would like to create a package for load in R.
>>>I have created a directory named INE and a directory below that named 
>>>R, for the files of R functions. A have created the files DESCRIPTION 
>>>and INDEX in the INE directory. The installation from local zip files, 
>>>in the R 2.3.0, results but to load the package I get an error like:
>>>
>>>
>>>
>>>'INE' is not a valid package -- installed < 2.0.0?
>>>
>>>
>>>
>>>I think that is necessary create a Meta directory with package.rds 
>>>file, but I don't know make it! I have read the manual 'Writing R Extensions - 1.
>>>Creating R packages' but I don't understand the procedure...
>>>
>>>Can I create it automatically?
>>>
>>>
>>>
>>>Could you help me with this?
>>>
>>>
>>>
>>>Thanks,
>>>
>>>---------------------------------------------------
>>>Rita Sousa
>>>DME - ME: Departamento de Metodologia Estat?stica - M?todos 
>>>Estat?sticos INE - DRP: Instituto Nacional de Estat?stica - Delega??o 
>>>Regional do Porto
>>>Tel.: 22 6072016 (Extens?o: 4116)
>>>---------------------------------------------------
>>>
>>>
>>>
>>>
>>>      [[alternative HTML version deleted]]
>>>
>>>
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>>>
>>>
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>


From ashgene at yahoo.co.in  Thu Jun  1 14:59:38 2006
From: ashgene at yahoo.co.in (Ahamarshan jn)
Date: Thu, 1 Jun 2006 05:59:38 -0700 (PDT)
Subject: [R] merge function in R
Message-ID: <20060601125938.18673.qmail@web7613.mail.in.yahoo.com>

hi list,

This question must be very basic but I am just 3 days
old to R. I am trying to find a function to merge two
tables of data in two different files as one.

 Does merge function only fills in colums between two
table where data is missing or is there a way that
merge can be used to merge data between two matrixes
with common dimensions.

say i have

   v1 v2 v3 v4   
h1
h2
h3
h4
h5

and and another table with 

   x1 x2 x3 x4
h1
h2
h3
h4
h5


can i merge the data as

   v1 x1 v2 x2 v3 x3 v4 x4
h1
h2
h3
h4
h5

Thanks


From pramod.r.anugu at jsums.edu  Thu Jun  1 15:08:38 2006
From: pramod.r.anugu at jsums.edu (Pramod Anugu)
Date: Thu, 1 Jun 2006 08:08:38 -0500
Subject: [R] HELP Running R
Message-ID: <000e01c6857c$7fc91000$1321848f@DHP5LZ61>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060601/232485d6/attachment.pl 

From ashgene at yahoo.co.in  Thu Jun  1 15:12:16 2006
From: ashgene at yahoo.co.in (Ahamarshan jn)
Date: Thu, 1 Jun 2006 06:12:16 -0700 (PDT)
Subject: [R] Merging files function
Message-ID: <20060601131216.61414.qmail@web7602.mail.in.yahoo.com>

hi list,

This question must be very basic but I am just 3 days
old to R, so I think i can ask. I am trying to find a
function to merge two
tables of data in two different files as one.

 Does merge function only fills in colums between two
table where data is missing or is there a way that
merge can be used to merge data between two matrixes
with common dimensions.

say i have

   v1 v2 v3 v4   
h1
h2
h3
h4
h5

and and another table with 

   x1 x2 x3 x4
h1
h2
h3
h4
h5


can i merge the data as

   v1 x1 v2 x2 v3 x3 v4 x4
h1
h2
h3
h4
h5

Thanks


From ccleland at optonline.net  Thu Jun  1 15:16:48 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 01 Jun 2006 09:16:48 -0400
Subject: [R] Merging files function
In-Reply-To: <20060601131216.61414.qmail@web7602.mail.in.yahoo.com>
References: <20060601131216.61414.qmail@web7602.mail.in.yahoo.com>
Message-ID: <447EE8C0.9010503@optonline.net>

Ahamarshan jn wrote:
> hi list,
> 
> This question must be very basic but I am just 3 days
> old to R, so I think i can ask. I am trying to find a
> function to merge two
> tables of data in two different files as one.
> 
>  Does merge function only fills in colums between two
> table where data is missing or is there a way that
> merge can be used to merge data between two matrixes
> with common dimensions.
> 
> say i have
> 
>    v1 v2 v3 v4   
> h1
> h2
> h3
> h4
> h5
> 
> and and another table with 
> 
>    x1 x2 x3 x4
> h1
> h2
> h3
> h4
> h5
> 
> 
> can i merge the data as
> 
>    v1 x1 v2 x2 v3 x3 v4 x4
> h1
> h2
> h3
> h4
> h5

See ?merge.

df1 <- as.data.frame(matrix(rnorm(20), ncol=4))
df2 <- as.data.frame(matrix(rnorm(20), ncol=4))
names(df1) <- paste("v", 1:4, sep="")
names(df2) <- paste("x", 1:4, sep="")
row.names(df1) <- paste("h", 1:5, sep="")
row.names(df2) <- paste("h", 1:5, sep="")

newdf <- merge(df1, df2, by="row.names")

> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From Han-Lin.Lai at noaa.gov  Thu Jun  1 15:33:31 2006
From: Han-Lin.Lai at noaa.gov (Han-Lin Lai)
Date: Thu, 01 Jun 2006 09:33:31 -0400
Subject: [R] Help on the association between two events
Message-ID: <447EECAB.6020806@noaa.gov>

Dear R-users,

Could any give me a guidence to solve the following problem?

I have three balls A, B and C.  I can pick A, B or C at the first 
selection.  At the second and third selections, I can either (i) pick 
one of the remaining balls, or (ii) stop to pick any balls.  Therefore, 
I have a classification table:

1st    2nd   3rd    n
A       --      --    9
B       --      --    3
C       --      --    5
A        B     --     7
A        C     --    1
B        A     --     9
B        C     --    2
C        A     --    0
C        B     --    3
A        B      C    5
A        C      B    6
B        A      C     8
B        C      A     9
C        A      B     3
C        B      A     2

My interest is on the estimation and test for the association between A 
and B.  In a special case, what is the assocaition measure between A and 
B if the first pick is either A or B?  Does anyone have done this kind 
of analysis?

Thanks in advance.
Han-Lin Lai
      

From assampryseley at yahoo.com  Thu Jun  1 15:38:21 2006
From: assampryseley at yahoo.com (Pryseley Assam)
Date: Thu, 1 Jun 2006 06:38:21 -0700 (PDT)
Subject: [R] Help: lme
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E02A390@dc1ex01.air.org>
Message-ID: <20060601133821.26734.qmail@web37107.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060601/9e4999fe/attachment.pl 

From gregd at stats.uct.ac.za  Thu Jun  1 15:47:14 2006
From: gregd at stats.uct.ac.za (Greg Distiller)
Date: Thu, 1 Jun 2006 15:47:14 +0200
Subject: [R] plotting grouped data with nlme over several pages
Message-ID: <010301c68581$e3e74de0$6f179e89@UCTPCGREGD>

Hi
I am struggling to split up the plots of the grouped objects in nlme in a 
usable way. The standard plot command generates plots for each group on a 
single page. When there are many groups however this does not look so good. 
I have discovered the layout option which allows one to split up these plots 
over a certain number of pages but the problem is it very quickly scrolls 
through all the pages only leaving the final page in the viewer.

My question is how does one get to view all these pages? or even better is 
there an option where the pages change only when prompted by the user?

Thanks

Greg


From HDoran at air.org  Thu Jun  1 15:48:20 2006
From: HDoran at air.org (Doran, Harold)
Date: Thu, 1 Jun 2006 09:48:20 -0400
Subject: [R] Help: lme
Message-ID: <2323A6D37908A847A7C32F1E3662C80E02A3A3@dc1ex01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060601/142477aa/attachment.pl 

From gregd at stats.uct.ac.za  Thu Jun  1 15:57:48 2006
From: gregd at stats.uct.ac.za (Greg Distiller)
Date: Thu, 1 Jun 2006 15:57:48 +0200
Subject: [R] understanding the verbose output in nlme
Message-ID: <011301c68583$5e2daee0$6f179e89@UCTPCGREGD>

Hi
I have found some postings referring to the fact that one can try and 
understand why a particular model is failing to solve/converge from the 
verbose output one can generate when fitting a nonlinear mixed model. I am 
trying to understand this output and have not been able to find out much:

**Iteration 1
LME step: Loglik: -237.4517 , nlm iterations: 22
reStruct  parameters:
  subjectno1   subjectno2   subjectno3   subjectno4   subjectno5 
subjectno6
 -0.87239181   2.75772772  -0.72892919 -10.36636391   0.55290322 
0.09878685

PNLS step: RSS =  60.50164
 fixed effects:2.59129  0.00741764  0.57155
 iterations: 7

Convergence:
   fixed reStruct
5.740688 2.159285

I know that the Loglik must refer to the value of the log likelihood 
function, that the values after "fixed effects" are the parameter estimates, 
and that the bit after Convergence obviously has something to so with the 
convergence criteria for the fixed effects and the random effects structure. 
I did manage to find a posting where somebody said that the restruct 
parameter is the log of the relative precision of the random effects? The 
one thing that is a bit confusing to me is that it appears as if the fixed 
effects convergence must be zero (or close to it) as one would expect but in 
one of my converged models the output showed a restruct value of 0.72 ?



Then I have no idea what the numbers under subjectno1-6 are, especially as I 
have 103 subjects in the data!



Can anyone help shed some light on this output and how it can be used to 
diagnose issues with a model?



Many thanks



Greg


From andy_liaw at merck.com  Thu Jun  1 16:25:05 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 1 Jun 2006 10:25:05 -0400
Subject: [R] HELP Running R  [Broadcast]
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02585C71@usctmx1106.merck.com>

Could you tell us what version (and variant) of Linux you're using, and the
GCC version (see gcc -v)?  Are you logged in as root when you built R, or as
regular user?  What does the summary output of configure look like (i.e.,
the last 25 lines or so of output from running configure)?

Andy

 

From: Pramod Anugu
> 
> Under the below section it talks about X86_64 architecture for Linux.
> 1. tar -zxvf R-2.3.0.tar.gz
> 2. changed the directory to the newly created directory 
> R-2.3.0 3. Typed ./configure 'r_arch=name' 
> 4. Typed make
> 5. Make check
> 6. make check-all
> 7. Typed make install
> 8. Typed R
> I still get the same error message. 
> Fatal error: unable to open the base package
> 2.5 Sub-architectures
> Some platforms can support closely related builds of R which 
> can share all but the executables and dynamic objects. 
> Examples include builds under Solaris for different chips (in 
> particular, 32- and 64-bit builds), 64- and
> 32- bit builds on `x86_64' Linux and different CPUs (`ppc', 
> `ppc64' and
> `i386') under MacOS 10.4. 
> R supports the idea of architecture-specific builds, 
> specified by adding `r_arch=name' to the configure line. Here 
> name can be anything non-empty, and is used to name 
> subdirectories of lib, etc, include and libs. Example names 
> from other systems are the use of sparcv9 on Solaris and 32 
> by gcc on `x86_64' Linux. 
> If you have two or more such builds you can install them over 
> each other (and one build can be done without `r_arch'). The 
> space savings can be
> considerable: on `x86_64' Linux a basic install (without 
> debugging symbols) took 63Mb, and adding a 32-bit build added 
> 6Mb. If you have installed multiple build you can select 
> which build to run by 
>      R --arch=name
> and running `R' will run the last build that was installed. 
> R CMD INSTALL will detect if more that one build is installed 
> and try to install packages with the appropriate library 
> objects for each. This will not be done if the package has an 
> executable configure script or a src/Makefile file. In such 
> cases you can install for extra builds by 
>      R --arch=name CMD INSTALL --libs-only pkg(s)
> -----------------------------------------------------
> My Server Archeitecute is X86_64
> -bash-2.05b# rpm -q --qf '%{ARCH}\n' zlib
> x86_64
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From gavin.simpson at ucl.ac.uk  Thu Jun  1 16:39:18 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 01 Jun 2006 15:39:18 +0100
Subject: [R] Merging files function
In-Reply-To: <447EE8C0.9010503@optonline.net>
References: <20060601131216.61414.qmail@web7602.mail.in.yahoo.com>
	<447EE8C0.9010503@optonline.net>
Message-ID: <1149172758.25858.22.camel@gsimpson.geog.ucl.ac.uk>

On Thu, 2006-06-01 at 09:16 -0400, Chuck Cleland wrote:
> Ahamarshan jn wrote:
> > hi list,
> > 
> > This question must be very basic but I am just 3 days
> > old to R, so I think i can ask. I am trying to find a
> > function to merge two
> > tables of data in two different files as one.
> > 
> >  Does merge function only fills in colums between two
> > table where data is missing or is there a way that
> > merge can be used to merge data between two matrixes
> > with common dimensions.
<snip />

Hi Ahamarshan,

I asked a similar question recently. Chuck's email provides a solution,
to which I'll add a comment and a link to the discussions I had with
Marc Schwartz and Sundar Dorai-Raj.

If your real world use is more complicated than your example, then
you'll need a slightly different strategy. If you have matrices with
different rows, such as, 

# alter Chuck's example to have one df with 5 rows the other with 4
df1 <- as.data.frame(matrix(rnorm(20), ncol=4))
df2 <- as.data.frame(matrix(rnorm(20), ncol=5))
names(df1) <- paste("v", 1:4, sep="")
names(df2) <- paste("x", 1:5, sep="")
row.names(df1) <- paste("h", 1:5, sep="")
row.names(df2) <- paste("h", 1:4, sep="")

Now if you merge this, merge() gives you a result with only 4 rows.

merge(df1, df2, by="row.names")
## lost a row, now with all rows:
merge(df1, df2, by="row.names", all = TRUE)

So use all = TRUE if row sizes differ.

For more complicated merges, you might check out the replies I got from
Marc and Sundar in the following thread:

http://thread.gmane.org/gmane.comp.lang.r.general/63031/focus=63042

Finally, why did you post your message to the list twice, with different
subject lines?

HTH,

G

> 
> See ?merge.
> 
> df1 <- as.data.frame(matrix(rnorm(20), ncol=4))
> df2 <- as.data.frame(matrix(rnorm(20), ncol=4))
> names(df1) <- paste("v", 1:4, sep="")
> names(df2) <- paste("x", 1:4, sep="")
> row.names(df1) <- paste("h", 1:5, sep="")
> row.names(df2) <- paste("h", 1:5, sep="")
> 
> newdf <- merge(df1, df2, by="row.names")

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
*  Note new Address, Telephone & Fax numbers from 6th April 2006  *
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson
ECRC & ENSIS                  [t] +44 (0)20 7679 0522
UCL Department of Geography   [f] +44 (0)20 7679 0565
Pearson Building              [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street                  [w] http://www.ucl.ac.uk/~ucfagls/cv/
London, UK.                   [w] http://www.ucl.ac.uk/~ucfagls/
WC1E 6BT.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From muster at gmail.com  Thu Jun  1 17:06:10 2006
From: muster at gmail.com (Mu Tian)
Date: Thu, 1 Jun 2006 11:06:10 -0400
Subject: [R] simple index question
Message-ID: <b68812e70606010806r1bbbd608x8eb417c97d71a3b4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060601/22b318d3/attachment.pl 

From mschwartz at mn.rr.com  Thu Jun  1 17:10:32 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 01 Jun 2006 10:10:32 -0500
Subject: [R] simple index question
In-Reply-To: <b68812e70606010806r1bbbd608x8eb417c97d71a3b4@mail.gmail.com>
References: <b68812e70606010806r1bbbd608x8eb417c97d71a3b4@mail.gmail.com>
Message-ID: <1149174633.6931.2.camel@localhost.localdomain>

On Thu, 2006-06-01 at 11:06 -0400, Mu Tian wrote:
> I want to get index number of 0s, like
> 
> x <- c(1, 2, 0, 3, 0, 4)
> 
> I want an output of
> 
> 3, 5
> 
> Thank you.

See ?which

> which(x == 0)
[1] 3 5


HTH,

Marc Schwartz


From bill.shipley at usherbrooke.ca  Thu Jun  1 17:12:17 2006
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Thu, 1 Jun 2006 11:12:17 -0400
Subject: [R] help with syntax of nlme function
Message-ID: <002a01c6858d$c605a720$b61ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060601/d96a13c1/attachment.pl 

From dimitris.rizopoulos at med.kuleuven.be  Thu Jun  1 17:35:33 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 1 Jun 2006 17:35:33 +0200
Subject: [R] simple index question
References: <b68812e70606010806r1bbbd608x8eb417c97d71a3b4@mail.gmail.com>
Message-ID: <016901c68591$05867d90$0540210a@www.domain>

try:

which(x == 0)

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Mu Tian" <muster at gmail.com>
To: "R-Help" <r-help at stat.math.ethz.ch>
Sent: Thursday, June 01, 2006 5:06 PM
Subject: [R] simple index question


>I want to get index number of 0s, like
>
> x <- c(1, 2, 0, 3, 0, 4)
>
> I want an output of
>
> 3, 5
>
> Thank you.
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From markleeds at verizon.net  Thu Jun  1 18:26:48 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Thu, 01 Jun 2006 11:26:48 -0500 (CDT)
Subject: [R] date sequencing using the Fcalendar package
Message-ID: <5423441.1528431149179208316.JavaMail.root@vms068.mailsrvcs.net>

I am using the following command from the Fcalendar Package :

x = timeSequence("1992-12-31","1994-12-31")

and then y = as.character(x) is a vector of character strings

"[1] 1992-12-31" "1993-01-31" "1993-03-03"  "1993-03-31" "1993-05-01" etc

This is very close to what I need and thank you very much
to whomever wrote Fcalendar.
The problem is that I'm not sure what the algorithm is for creating these speicific dates ? At first I thought it might be 31 days between each but that's not the case for March ( see above). Regardless of what the algorithm is, does anyone know of an easy way to make sure the months I get back are US business days ? In other words, push any date that is not a US business day to the next US business day ? I  can do it by hand but I have 10 years of data so it would be easier if there was some R way to do it ? I've looked through the fCalendar package but without success.

                                                 Thanks a lot.


From jgarcia at ija.csic.es  Thu Jun  1 18:28:01 2006
From: jgarcia at ija.csic.es (javier garcia-pintado)
Date: Thu, 01 Jun 2006 18:28:01 +0200
Subject: [R] interacting with X11() device
Message-ID: <447F1591.4000501@ija.csic.es>

Hello;

I'm preparing a simple function that plots sequentially a number of time
series for visual inspection, after some previous transformations.

To allow the user to have time to see the series I'm using
Sys.sleep(1.5) between calls to line(). But is it possible to add some
mechanism that would allow the user, for example with one click on the
plotting device, to pause the sequential plotting for a more detailed
inspection of a specific series and with another click to go on with the
sequential display?

Without the details, it is something like this:

results.plot <- function(i){
 plot(...)
  for (j in i){
    lines(j)
    Sys.time(1.5)
 }
}

And I'm calling the function for all series in a matrix: results.plot(1:100)

Thanks and best regards,

Javier
-- 

Javier Garc?a-Pintado
Institute of Earth Sciences Jaume Almera (CSIC)
Lluis Sole Sabaris s/n, 08028 Barcelona
Phone: +34 934095410
Fax:   +34 934110012
e-mail:jgarcia at ija.csic.es 


From karin.lagesen at medisin.uio.no  Thu Jun  1 19:23:56 2006
From: karin.lagesen at medisin.uio.no (Karin Lagesen)
Date: Thu, 01 Jun 2006 19:23:56 +0200
Subject: [R] variation on vioplot?
In-Reply-To: <447B5F92.9080309@cebitec.uni-bielefeld.de> (Michael Dondrup's
	message of "Mon, 29 May 2006 22:54:42 +0200")
References: <ypx67j45gogs.fsf@uracil.uio.no>
	<447B5F92.9080309@cebitec.uni-bielefeld.de>
Message-ID: <ypx664jkhjrn.fsf@uracil.uio.no>

Michael Dondrup <michael.dondrup at cebitec.uni-bielefeld.de> writes:

> Hi Karin,
> I would like to help with this, but it's not completely clear to me
> what your vioplots look or should look like. Could you post a little
> reproducible code example to the list, and then specify what should be
> different?

OK.

Adapted from the vioplot example:

source("vioplot.R")
uniform<-runif(2000,-4,4)
normal<-rnorm(2000,0,3)
vioplot(normal,uniform,horizontal=TRUE)

Now these two are positioned one on top of the other. I would like to
have them next to each other, if possible. I would then like the scale
underneath to be duplicated so that there is one scale underneath each
of them.

Karin
-- 
Karin Lagesen, PhD student
karin.lagesen at medisin.uio.no
http://www.cmbn.no/rognes/


From Regina_Rendas-Baum at brown.edu  Thu Jun  1 19:27:01 2006
From: Regina_Rendas-Baum at brown.edu (Rendas-Baum, Regina)
Date: Thu, 1 Jun 2006 13:27:01 -0400
Subject: [R] setting the random-effects covariance matrix in lme
Message-ID: <91C1AD5E37A96E42B8A200E334370A9C0495A19C@MAIL2.AD.Brown.Edu>

Dear R-users,
 
I have longitudinal data and would like to fit a model where both the variance-covariance matrix of the random effects and the residual variance are conditional on a (binary) grouping variable.
I guess the model would have the following form (in hierarchical notation)
Yi|bi,k ~ N(XiB+Zibi, sigmak*Ident)
bi|k ~ N(0, Dk)
K~Bernoulli(p)
 
I can obtain different sigmas (sigma0 and sigma1 based on the factor 'dx') using the weights option in the call to lme:
lme(fixed = height ~ -1 + bage + mat_ht + pat_ht + dx + time:dx + time2:dx , 

                 weights=varIdent(form=~1|dx),

                  random = ~ time + time2 |subject, data = pilot, na.action=na.omit)

 but I cannot seem to be able to get two different matrices for the random effects.  

I'm particularly interested in obtaining,
 
Y|k ~ N(XB, Z(Dk)Z+sigmak),
 
so I need to extract D0 and D1 form the lme object.  I've looked in Pinheiro and Bates but was unable to identify how to fit this type of covariance matrix from the examples provided in the book - perhaps it is there expressed in a different form?  I looked in section 4.2.2 (pdMat) of teh book but I'm still not sure how to pass it on in the call.
 
Any help would be greatly appreciated,
 
Regina
 
Regina Rendas-Baum
Graduate Student
Brown University
Biostatistics


From talcon at iastate.edu  Thu Jun  1 20:03:45 2006
From: talcon at iastate.edu (Tim Alcon)
Date: Thu, 01 Jun 2006 13:03:45 -0500
Subject: [R] progressive slowdown during script execution?
Message-ID: <447F2C01.6060205@iastate.edu>

I'm an R novice, so I hope my question is a valid one.  I'm trying to 
run the following script in the current version of R.

for (i in 1:1640){for (j in (i+1):1641){
if (i == 1 && j == 2){x <- cor(sage[i,],sage[j,],method="spearman"); y 
<- cor(frie[i,],frie[j,],method="spearman")}
if (i != 1 || j != 2){x <- 
c(x,cor(sage[i,],sage[j,],method="spearman")); y <- 
c(y,cor(frie[i,],frie[j,],method="spearman"))}}}

It basically just finds all pairwise correlations of the rows in a 
matrix for each of two matrices and stores the results for each matrix 
in a vector.  The problem I seem to be running into is that it seems to 
slow way down during execution somehow.  When I first tried running it I 
stopped execution to see how fast it was running, before trying to 
compute the whole job (the two matrices each have 1641 rows).  Based on 
what I saw, I figured it would easily finish overnight.  Instead, it was 
still running almost 24 hours later.  To quantify this a little better I 
checked it after running for 5 minutes, at which point it had added 
79120 correlations to each of the x and y vectors.  Since there should 
be a total of (1641*1640)/2 = 1345620 pairwise correlations in each 
vector when it finishes running, I worked out that it should take 
(1345620/79120)*5 = 85 minutes to run the whole job.  However, when I 
checked it after running for 2 hours, it had added only 341870 
correlations to each vector.

Any ideas what I'm doing wrong, or why it would run more slowly the 
longer it runs?  Thanks for any help or advice.

Tim


From andy_liaw at merck.com  Thu Jun  1 20:09:20 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 1 Jun 2006 14:09:20 -0400
Subject: [R] progressive slowdown during script execution? [Broadcast ]
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02585CCE@usctmx1106.merck.com>

You should try to figure out how large x and y need to be, and allocate them
before the computation.  "Growing" objects like that in R is very, very,
very bad, as you've found out.

Andy

From: Tim Alcon
> 
> I'm an R novice, so I hope my question is a valid one.  I'm trying to 
> run the following script in the current version of R.
> 
> for (i in 1:1640){for (j in (i+1):1641){
> if (i == 1 && j == 2){x <- 
> cor(sage[i,],sage[j,],method="spearman"); y 
> <- cor(frie[i,],frie[j,],method="spearman")}
> if (i != 1 || j != 2){x <- 
> c(x,cor(sage[i,],sage[j,],method="spearman")); y <- 
> c(y,cor(frie[i,],frie[j,],method="spearman"))}}}
> 
> It basically just finds all pairwise correlations of the rows in a 
> matrix for each of two matrices and stores the results for 
> each matrix 
> in a vector.  The problem I seem to be running into is that 
> it seems to 
> slow way down during execution somehow.  When I first tried 
> running it I 
> stopped execution to see how fast it was running, before trying to 
> compute the whole job (the two matrices each have 1641 rows). 
>  Based on 
> what I saw, I figured it would easily finish overnight.  
> Instead, it was 
> still running almost 24 hours later.  To quantify this a 
> little better I 
> checked it after running for 5 minutes, at which point it had added 
> 79120 correlations to each of the x and y vectors.  Since 
> there should 
> be a total of (1641*1640)/2 = 1345620 pairwise correlations in each 
> vector when it finishes running, I worked out that it should take 
> (1345620/79120)*5 = 85 minutes to run the whole job.  However, when I 
> checked it after running for 2 hours, it had added only 341870 
> correlations to each vector.
> 
> Any ideas what I'm doing wrong, or why it would run more slowly the 
> longer it runs?  Thanks for any help or advice.
> 
> Tim
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From br44114 at gmail.com  Thu Jun  1 20:29:39 2006
From: br44114 at gmail.com (bogdan romocea)
Date: Thu, 1 Jun 2006 14:29:39 -0400
Subject: [R] progressive slowdown during script execution?
Message-ID: <8d5a36350606011129x3732dbafn6fb8a90220c20abb@mail.gmail.com>

Compare
  system.time({
  v <- vector()
  for (i in 1:10^5) v <- c(v,1)
  })
with
  system.time({
  v <- vector(length=10^5)
  for (i in 1:10^5) v[i] <- 1
  })
If you don't know exactly how long v will be, use a value that's large
enough, then throw away what's extra.


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tim Alcon
> Sent: Thursday, June 01, 2006 2:04 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] progressive slowdown during script execution?
>
> I'm an R novice, so I hope my question is a valid one.  I'm trying to
> run the following script in the current version of R.
>
> for (i in 1:1640){for (j in (i+1):1641){
> if (i == 1 && j == 2){x <-
> cor(sage[i,],sage[j,],method="spearman"); y
> <- cor(frie[i,],frie[j,],method="spearman")}
> if (i != 1 || j != 2){x <-
> c(x,cor(sage[i,],sage[j,],method="spearman")); y <-
> c(y,cor(frie[i,],frie[j,],method="spearman"))}}}
>
> It basically just finds all pairwise correlations of the rows in a
> matrix for each of two matrices and stores the results for
> each matrix
> in a vector.  The problem I seem to be running into is that
> it seems to
> slow way down during execution somehow.  When I first tried
> running it I
> stopped execution to see how fast it was running, before trying to
> compute the whole job (the two matrices each have 1641 rows).
>  Based on
> what I saw, I figured it would easily finish overnight.
> Instead, it was
> still running almost 24 hours later.  To quantify this a
> little better I
> checked it after running for 5 minutes, at which point it had added
> 79120 correlations to each of the x and y vectors.  Since
> there should
> be a total of (1641*1640)/2 = 1345620 pairwise correlations in each
> vector when it finishes running, I worked out that it should take
> (1345620/79120)*5 = 85 minutes to run the whole job.  However, when I
> checked it after running for 2 hours, it had added only 341870
> correlations to each vector.
>
> Any ideas what I'm doing wrong, or why it would run more slowly the
> longer it runs?  Thanks for any help or advice.
>
> Tim
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


From michael.dondrup at cebitec.uni-bielefeld.de  Thu Jun  1 20:27:43 2006
From: michael.dondrup at cebitec.uni-bielefeld.de (Michael Dondrup)
Date: Thu, 1 Jun 2006 20:27:43 +0200
Subject: [R] variation on vioplot?
In-Reply-To: <ypx664jkhjrn.fsf@uracil.uio.no>
References: <ypx67j45gogs.fsf@uracil.uio.no>
	<447B5F92.9080309@cebitec.uni-bielefeld.de>
	<ypx664jkhjrn.fsf@uracil.uio.no>
Message-ID: <200606012027.44191.michael.dondrup@cebitec.uni-bielefeld.de>

Hi Karin,
I think it's just setting  horizontal to FALSE.
> vioplot(normal,uniform,horizontal=FALSE)
For the axis: 
How about this:
> vioplot(normal, uniform)
> axis(2, pos=1.5)
sometimes the second axis might overlap, so try this one too:
> ylim <- range(normal, uniform)
> par(mfrow=c(1,2), bty='n')
> vioplot(normal, ylim=ylim,names=c('normal'))
> vioplot(uniform, ylim=ylim, names=c('uniform'))

btw: library(vioplot) works for me

Hope this helps
Michael




> Michael Dondrup <michael.dondrup at cebitec.uni-bielefeld.de> writes:
> > Hi Karin,
> > I would like to help with this, but it's not completely clear to me
> > what your vioplots look or should look like. Could you post a little
> > reproducible code example to the list, and then specify what should be
> > different?
>
> OK.
>
> Adapted from the vioplot example:
>
> source("vioplot.R")
> uniform<-runif(2000,-4,4)
> normal<-rnorm(2000,0,3)
> vioplot(normal,uniform,horizontal=TRUE)
>
> Now these two are positioned one on top of the other. I would like to
> have them next to each other, if possible. I would then like the scale
> underneath to be duplicated so that there is one scale underneath each
> of them.
>
> Karin


From markleeds at verizon.net  Thu Jun  1 21:01:01 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Thu, 01 Jun 2006 14:01:01 -0500 (CDT)
Subject: [R] strange behavior when adding a day to dates
Message-ID: <3986466.999851149188461448.JavaMail.root@vms170.mailsrvcs.net>

I have the following problem which seems really strange but it's more likely something that I don't understand ?

The code below works in that it adds 1 day to all of the dates.
( NB : the Fcalendar package needs to be loaded for this code to run )

startDate="1992-12-31"
endDate="1994-12-31"

begMonthDates=timeSequencestartDate,endDate,by="months,FinCenter="GMT") 

print(begMonthDates)
begMonthDates = begMonthDates + 24*3600
print(begMOnthDates)

------------------------------------------------------------------

Th really strange thing is that, in the code below, the
dates don't change ? The code is essentially the same
as above except in this case I only want to add one day to days
that are not business days ? If anyone could help me with this,
it would be MUCH appreciated. I've been playing with this for
most of the day. ( badindices is not NULL )


---------------------------------------------------------------------

badindices=which(isBizday(begMonthDates,holidays=holiday.NYSE()) == F)

print(begMonthDates[badindices])
begMonthDates[badindices]=begMonthDates[badindices] + 24*3600
print(begMonthDates[badindices])


From carl at mcs.st-and.ac.uk  Thu Jun  1 21:13:19 2006
From: carl at mcs.st-and.ac.uk (Carl)
Date: Thu, 01 Jun 2006 20:13:19 +0100
Subject: [R] Random number generation
Message-ID: <447F3C4F.6070905@mcs.st-andrews.ac.uk>

Hi All.
(This is probably an incredibly stupid question :))

I have just noticed that every time I start an R2.3 session and request 
a random number (say with rnorm) that I receive the same number. AFAIK I 
have not set a seed at any point. Returning to an older version of 
R(v1.8) I get the behaviour that I have come to expect, where a new seed 
is automatically generated for each new session (actually it was a 
student running bootstraps on multiple machines that has noticed this).

I am running R2.3 for windows on winXP pro, AMD Athlon 2400+.

Thanks in advance and be gentle.
C

<...retreats and waits for imminent admonishment...>

-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Dr. Carl Donovan
Research Fellow in Statistical Computing
Ph +44 1334 461802
The Observatory
Buchanan Gardens
University of St Andrews
St Andrews
Fife
KY16 9LZ
Scotland


From Max.Kuhn at pfizer.com  Thu Jun  1 21:32:44 2006
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Thu, 1 Jun 2006 15:32:44 -0400
Subject: [R] Key titles in Lattice
Message-ID: <71257D09F114DA4A8E134DEAC70F25D3053D5C2F@groamrexm03.amer.pfizer.com>

Hi,

I'm creating some lattice plots that have a key and I'd like to put a
label on the key. The problem is that the text label for the key
prevents the values of the group variables from being shown (see example
below). I don't think that this is a feature, but I might be abusing the
title arg for key.

I'm using R Version 2.3.0 (2006-04-24) on Windows XP and lattice V0.13-8

Thanks,

Max


library(lattice)
testData <- expand.grid(
   A = letters[1:2], 
   B = letters[3:4], 
   C = letters[5:6])
testData$y <- rnorm(dim(testData)[1])

stripplot(
   y ~ A|B,  data = testData,
   groups = C,
   panel = function(x, y, groups, subscripts)
   {
      group.values <- sort(unique(groups))
      for (i in seq(along=group.values)) 
      {
         id <- (groups[subscripts] == group.values[i])
         current.val <- group.values[i]
         panel.stripplot(x[id], y[id],
            jitter.data = FALSE, horizontal = FALSE,
            col = trellis.par.get()$superpose.symbol$col[i], 
            pch = trellis.par.get()$superpose.symbol$pch[i])
         panel.linejoin(
            x[id], y[id], horizontal=F,
            col = trellis.par.get()$superpose.symbol$col[i], 
            lty = trellis.par.get()$superpose.line$lty[i], 
            lwd = trellis.par.get()$superpose.line$lwd[i])
      }
   }, 
   main = "Some Text", 
   key = list(
      columns = 2,
      text=list(
# comment the next line out to see 
# the values of the grouping variable
         title = "More Text",  
         lab = letters[5:6]),
      lines=list(
         col = trellis.par.get()$superpose.symbol$col[1:2],
         lwd = trellis.par.get()$superpose.line$lwd[1:2],
         lty = trellis.par.get()$superpose.line$lty[1:2]),
      points = list(
         col = trellis.par.get()$superpose.symbol$col[1:2],
         pch = trellis.par.get()$superpose.symbol$pch[1:2]))) 

----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From murdoch at stats.uwo.ca  Thu Jun  1 21:52:42 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 01 Jun 2006 15:52:42 -0400
Subject: [R] Random number generation
In-Reply-To: <447F3C4F.6070905@mcs.st-andrews.ac.uk>
References: <447F3C4F.6070905@mcs.st-andrews.ac.uk>
Message-ID: <447F458A.20707@stats.uwo.ca>

On 6/1/2006 3:13 PM, Carl wrote:
> Hi All.
> (This is probably an incredibly stupid question :))
> 
> I have just noticed that every time I start an R2.3 session and request 
> a random number (say with rnorm) that I receive the same number. AFAIK I 
> have not set a seed at any point. Returning to an older version of 
> R(v1.8) I get the behaviour that I have come to expect, where a new seed 
> is automatically generated for each new session (actually it was a 
> student running bootstraps on multiple machines that has noticed this).
> 
> I am running R2.3 for windows on winXP pro, AMD Athlon 2400+.

R saves the random seed in a variable called .Random.seed.  (Because of 
the initial dot, this is not normally shown by ls().  Use ls(all=T) to 
see it.

If you save this to an .RData file, it will be reloaded next time you 
run, and your RNG will continue on from before.

I'm guessing you saved a copy once, but are *not* saving your workspace 
each time you restart:  hence you get the same seed each time.

You can avoid this by deleting the saved workspace, or just deleting 
.Random.seed and saving it (but it will be resurrected next time you do 
a save), or by always saving your workspace (so you save a different 
seed every time).

My advice would be not to save the workspace; I save individual objects 
sometimes, but I like a nice clean workspace each time I start.

Duncan Murdoch


From Dimitris.Rizopoulos at med.kuleuven.be  Thu Jun  1 22:01:45 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Thu,  1 Jun 2006 22:01:45 +0200
Subject: [R] Random number generation
In-Reply-To: <447F3C4F.6070905@mcs.st-andrews.ac.uk>
References: <447F3C4F.6070905@mcs.st-andrews.ac.uk>
Message-ID: <1149192105.447f47a9418b6@webmail2.kuleuven.be>

according to ?.Random.seed, `Note' section, different R sessions give 
differen simulation results; maybe each time you start R you load a 
previously saved workspace, with an existing .Random.seed, i.e., check 
if the .Random.seed value is the same each time you start R.

I hope it helps.

Best,
Dimitris

---- 
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting Carl <carl at mcs.st-and.ac.uk>:

> Hi All.
> (This is probably an incredibly stupid question :))
> 
> I have just noticed that every time I start an R2.3 session and
> request 
> a random number (say with rnorm) that I receive the same number.
> AFAIK I 
> have not set a seed at any point. Returning to an older version of 
> R(v1.8) I get the behaviour that I have come to expect, where a new
> seed 
> is automatically generated for each new session (actually it was a 
> student running bootstraps on multiple machines that has noticed
> this).
> 
> I am running R2.3 for windows on winXP pro, AMD Athlon 2400+.
> 
> Thanks in advance and be gentle.
> C
> 
> <...retreats and waits for imminent admonishment...>
> 
> -- 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Dr. Carl Donovan
> Research Fellow in Statistical Computing
> Ph +44 1334 461802
> The Observatory
> Buchanan Gardens
> University of St Andrews
> St Andrews
> Fife
> KY16 9LZ
> Scotland
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From plummer at iarc.fr  Thu Jun  1 22:02:38 2006
From: plummer at iarc.fr (plummer at iarc.fr)
Date: Thu, 01 Jun 2006 22:02:38 +0200
Subject: [R] Random number generation
In-Reply-To: <447F3C4F.6070905@mcs.st-andrews.ac.uk>
References: <447F3C4F.6070905@mcs.st-andrews.ac.uk>
Message-ID: <1149192158.447f47de74122@webmail.iarc.fr>

That certainly sounds like the behaviour you would get if you
had a .Random.seed in your work space.  If you do not save your
workspace at the end of the session then the random seed will
be in exactly the same state every time you start a new R session,
and you will get identical simulations from the same sequence
of function calls. So you either need to save your work space
or launch R from the command line with --no-restore or --vanilla
and get a fresh starting seed in each session based on the
time stamp.

Have you tried checking for the existence of .Random.seed before
you call any random number functions (e.g. with
objects(all.names=TRUE))?

I don't know why you would get different behaviour from R 1.8.0
since the RNG hasn't changed since 1.7.0.

Martyn

Quoting Carl <carl at mcs.st-and.ac.uk>:
> Hi All.
> (This is probably an incredibly stupid question :))
>
> I have just noticed that every time I start an R2.3 session and request
> a random number (say with rnorm) that I receive the same number. AFAIK I
> have not set a seed at any point. Returning to an older version of
> R(v1.8) I get the behaviour that I have come to expect, where a new seed
> is automatically generated for each new session (actually it was a
> student running bootstraps on multiple machines that has noticed this).
>
> I am running R2.3 for windows on winXP pro, AMD Athlon 2400+.
>
> Thanks in advance and be gentle.
> C
>
> <...retreats and waits for imminent admonishment...>
>
> --
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Dr. Carl Donovan
> Research Fellow in Statistical Computing
> Ph +44 1334 461802
> The Observatory
> Buchanan Gardens
> University of St Andrews
> St Andrews
> Fife
> KY16 9LZ
> Scotland
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



-----------------------------------------------------------------------
This message and its attachments are strictly confidential. ...{{dropped}}


From sundar.dorai-raj at pdf.com  Thu Jun  1 22:10:01 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 01 Jun 2006 15:10:01 -0500
Subject: [R] Key titles in Lattice
In-Reply-To: <71257D09F114DA4A8E134DEAC70F25D3053D5C2F@groamrexm03.amer.pfizer.com>
References: <71257D09F114DA4A8E134DEAC70F25D3053D5C2F@groamrexm03.amer.pfizer.com>
Message-ID: <447F4999.6010206@pdf.com>



Kuhn, Max wrote:
> Hi,
> 
> I'm creating some lattice plots that have a key and I'd like to put a
> label on the key. The problem is that the text label for the key
> prevents the values of the group variables from being shown (see example
> below). I don't think that this is a feature, but I might be abusing the
> title arg for key.
> 
> I'm using R Version 2.3.0 (2006-04-24) on Windows XP and lattice V0.13-8
> 
> Thanks,
> 
> Max
> 
> 
> library(lattice)
> testData <- expand.grid(
>    A = letters[1:2], 
>    B = letters[3:4], 
>    C = letters[5:6])
> testData$y <- rnorm(dim(testData)[1])
> 
> stripplot(
>    y ~ A|B,  data = testData,
>    groups = C,
>    panel = function(x, y, groups, subscripts)
>    {
>       group.values <- sort(unique(groups))
>       for (i in seq(along=group.values)) 
>       {
>          id <- (groups[subscripts] == group.values[i])
>          current.val <- group.values[i]
>          panel.stripplot(x[id], y[id],
>             jitter.data = FALSE, horizontal = FALSE,
>             col = trellis.par.get()$superpose.symbol$col[i], 
>             pch = trellis.par.get()$superpose.symbol$pch[i])
>          panel.linejoin(
>             x[id], y[id], horizontal=F,
>             col = trellis.par.get()$superpose.symbol$col[i], 
>             lty = trellis.par.get()$superpose.line$lty[i], 
>             lwd = trellis.par.get()$superpose.line$lwd[i])
>       }
>    }, 
>    main = "Some Text", 
>    key = list(
>       columns = 2,
>       text=list(
> # comment the next line out to see 
> # the values of the grouping variable
>          title = "More Text",  
>          lab = letters[5:6]),
>       lines=list(
>          col = trellis.par.get()$superpose.symbol$col[1:2],
>          lwd = trellis.par.get()$superpose.line$lwd[1:2],
>          lty = trellis.par.get()$superpose.line$lty[1:2]),
>       points = list(
>          col = trellis.par.get()$superpose.symbol$col[1:2],
>          pch = trellis.par.get()$superpose.symbol$pch[1:2]))) 
> 
> ----------------------------------------------------------------------
> LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}
> 


Hi, Max,

I think you missed this in ?xyplot under the "key" arguement:

           The '"text"' component has to have a character or expression
           vector as its first component, and the length of this vector
           determines the number of rows.

I don't see any documentation on "text" allowing a "title" element.  If 
you want to do what I think you want, try this key instead:

    key = list(
       columns = 2,
       text=list(
          c(paste("More Text", letters[5], sep = "    "), letters[6])),
       lines=list(
          col = trellis.par.get()$superpose.symbol$col[1:2],
          lwd = trellis.par.get()$superpose.line$lwd[1:2],
          lty = trellis.par.get()$superpose.line$lty[1:2]),
       points = list(
          col = trellis.par.get()$superpose.symbol$col[1:2],
          pch = trellis.par.get()$superpose.symbol$pch[1:2]))

It's not pretty, but you get idea. Perhaps others might have better 
solutions.

--sundar


From ehlers at math.ucalgary.ca  Thu Jun  1 22:14:09 2006
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Thu, 01 Jun 2006 14:14:09 -0600
Subject: [R] Key titles in Lattice
In-Reply-To: <71257D09F114DA4A8E134DEAC70F25D3053D5C2F@groamrexm03.amer.pfizer.com>
References: <71257D09F114DA4A8E134DEAC70F25D3053D5C2F@groamrexm03.amer.pfizer.com>
Message-ID: <447F4A91.90809@math.ucalgary.ca>

Hi Max,

You have 'title' as a component of the 'text' component of key.
According to the help page for xyplot, 'title' is a component
of key. AFAICS, 'text' does not have a 'title' component.

Peter Ehlers

Kuhn, Max wrote:
> Hi,
> 
> I'm creating some lattice plots that have a key and I'd like to put a
> label on the key. The problem is that the text label for the key
> prevents the values of the group variables from being shown (see example
> below). I don't think that this is a feature, but I might be abusing the
> title arg for key.
> 
> I'm using R Version 2.3.0 (2006-04-24) on Windows XP and lattice V0.13-8
> 
> Thanks,
> 
> Max
> 
> 
> library(lattice)
> testData <- expand.grid(
>    A = letters[1:2], 
>    B = letters[3:4], 
>    C = letters[5:6])
> testData$y <- rnorm(dim(testData)[1])
> 
> stripplot(
>    y ~ A|B,  data = testData,
>    groups = C,
>    panel = function(x, y, groups, subscripts)
>    {
>       group.values <- sort(unique(groups))
>       for (i in seq(along=group.values)) 
>       {
>          id <- (groups[subscripts] == group.values[i])
>          current.val <- group.values[i]
>          panel.stripplot(x[id], y[id],
>             jitter.data = FALSE, horizontal = FALSE,
>             col = trellis.par.get()$superpose.symbol$col[i], 
>             pch = trellis.par.get()$superpose.symbol$pch[i])
>          panel.linejoin(
>             x[id], y[id], horizontal=F,
>             col = trellis.par.get()$superpose.symbol$col[i], 
>             lty = trellis.par.get()$superpose.line$lty[i], 
>             lwd = trellis.par.get()$superpose.line$lwd[i])
>       }
>    }, 
>    main = "Some Text", 
>    key = list(
>       columns = 2,
>       text=list(
> # comment the next line out to see 
> # the values of the grouping variable
>          title = "More Text",  
>          lab = letters[5:6]),
>       lines=list(
>          col = trellis.par.get()$superpose.symbol$col[1:2],
>          lwd = trellis.par.get()$superpose.line$lwd[1:2],
>          lty = trellis.par.get()$superpose.line$lty[1:2]),
>       points = list(
>          col = trellis.par.get()$superpose.symbol$col[1:2],
>          pch = trellis.par.get()$superpose.symbol$pch[1:2]))) 
> 
> ----------------------------------------------------------------------
> LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From gunter.berton at gene.com  Thu Jun  1 22:15:20 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 1 Jun 2006 13:15:20 -0700
Subject: [R] FW: How to create a new package?
In-Reply-To: <200606011443.25532.rdiaz@cnio.es>
Message-ID: <005001c685b8$1c94d370$9f7f10ac@gne.windows.gene.com>

Quicker and dirtier is to simply save a workspace  with your desired
functions and attach() it automatically (e.g. via .First or otherwise) at
startup (?Startup for details and options). Of course none of the tools and
benefits of package management are available, but then I did say quicker and
dirtier.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA

 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ramon 
> Diaz-Uriarte
> Sent: Thursday, June 01, 2006 5:43 AM
> To: r-help at stat.math.ethz.ch
> Cc: Rita Sousa
> Subject: Re: [R] FW: How to create a new package?
> 
> Dear Rita,
> 
> Do you want a package just for yourself, or something useful 
> for others, with 
> docs, etc? I think the rest of the answers in this thread 
> will help you 
> create a "full fledged" package. See also the detailed explanation 
> in "Writing R extensions".
> 
> If you just want something quick and dirty that allows you to 
> use a bunch of 
> functions without using "source" (and thus cluttering your 
> global workspace), 
> is easy to move around, etc, you just need a directory 
> structure such as:
> 
> SignS2/
> SignS2/R/
> SignS2/R/SignS2.R
> SignS2/DESCRIPTION
> SignS2/Changes
> 
> (Change SignS2 for the name of your package).
> 
> This has no documentation whatsoever. You can get rid of the 
> "changes" file, 
> but I put it there to keep track of changes.
> 
> Run R CMD check against the directory (of coruse, you'll get 
> warnings about 
> missing documentation), and then R CMD build.
> 
> Best,
> 
> R.
> 
> 
> On Thursday 01 June 2006 13:23, michael watson (IAH-C) wrote:
> > ?package.skeleton
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Gabor Grothendieck
> > Sent: 01 June 2006 12:20
> > To: Rita Sousa
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] FW: How to create a new package?
> >
> > The minimum is to create a DESCRIPTION file, plus R and man 
> directories
> > containing R code and .Rd files respectively. It might help 
> to run  Rcmd
> > CHECK mypkg  before installation and fix any problems it finds.
> >
> > Googling for   creating R package   will locate some tutorials.
> >
> > On 6/1/06, Rita Sousa <rita.sousa at ine.pt> wrote:
> > > Hi,
> > >
> > >
> > >
> > > I'm a group of functions and I would like to create a 
> package for load in
> > > R. I have created a directory named INE and a directory 
> below that named
> > > R, for the files of R functions. A have created the files 
> DESCRIPTION and
> > > INDEX in the INE directory. The installation from local 
> zip files, in the
> > > R 2.3.0, results but to load the package I get an error like:
> > >
> > >
> > >
> > > 'INE' is not a valid package -- installed < 2.0.0?
> > >
> > >
> > >
> > > I think that is necessary create a Meta directory with package.rds
> > > file, but I don't know make it! I have read the manual 'Writing R
> > > Extensions - 1. Creating R packages' but I don't understand the
> > > procedure...
> > >
> > > Can I create it automatically?
> > >
> > >
> > >
> > > Could you help me with this?
> > >
> > >
> > >
> > > Thanks,
> > >
> > > ---------------------------------------------------
> > > Rita Sousa
> > > DME - ME: Departamento de Metodologia Estat?stica - M?todos
> > > Estat?sticos INE - DRP: Instituto Nacional de Estat?stica 
> - Delega??o
> > > Regional do Porto
> > > Tel.: 22 6072016 (Extens?o: 4116)
> > > ---------------------------------------------------
> > >
> > >
> > >
> > >
> > >        [[alternative HTML version deleted]]
> > >
> > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> -- 
> Ram?n D?az-Uriarte
> Bioinformatics 
> Centro Nacional de Investigaciones Oncol?gicas (CNIO)
> (Spanish National Cancer Center)
> Melchor Fern?ndez Almagro, 3
> 28029 Madrid (Spain)
> Fax: +-34-91-224-6972
> Phone: +-34-91-224-6900
> 
> http://ligarto.org/rdiaz
> PGP KeyID: 0xE89B3462
> (http://ligarto.org/rdiaz/0xE89B3462.asc)
> 
> 
> 
> **NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en 
> s...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From Max.Kuhn at pfizer.com  Thu Jun  1 22:20:53 2006
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Thu, 1 Jun 2006 16:20:53 -0400
Subject: [R] Key titles in Lattice
Message-ID: <71257D09F114DA4A8E134DEAC70F25D3053D5CB1@groamrexm03.amer.pfizer.com>

Peter and Sundar,

Thanks, I did miss that. I used:

  key = list(
    title = "More Text",
    cex.title = 1,
    columns = 2,
    text=list(

to get what I wanted.

Max


-----Original Message-----
From: P Ehlers [mailto:ehlers at math.ucalgary.ca] 
Sent: Thursday, June 01, 2006 4:14 PM
To: Kuhn, Max
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Key titles in Lattice

Hi Max,

You have 'title' as a component of the 'text' component of key.
According to the help page for xyplot, 'title' is a component
of key. AFAICS, 'text' does not have a 'title' component.

Peter Ehlers

Kuhn, Max wrote:
> Hi,
> 
> I'm creating some lattice plots that have a key and I'd like to put a
> label on the key. The problem is that the text label for the key
> prevents the values of the group variables from being shown (see
example
> below). I don't think that this is a feature, but I might be abusing
the
> title arg for key.
> 
> I'm using R Version 2.3.0 (2006-04-24) on Windows XP and lattice
V0.13-8
> 
> Thanks,
> 
> Max
> 
> 
> library(lattice)
> testData <- expand.grid(
>    A = letters[1:2], 
>    B = letters[3:4], 
>    C = letters[5:6])
> testData$y <- rnorm(dim(testData)[1])
> 
> stripplot(
>    y ~ A|B,  data = testData,
>    groups = C,
>    panel = function(x, y, groups, subscripts)
>    {
>       group.values <- sort(unique(groups))
>       for (i in seq(along=group.values)) 
>       {
>          id <- (groups[subscripts] == group.values[i])
>          current.val <- group.values[i]
>          panel.stripplot(x[id], y[id],
>             jitter.data = FALSE, horizontal = FALSE,
>             col = trellis.par.get()$superpose.symbol$col[i], 
>             pch = trellis.par.get()$superpose.symbol$pch[i])
>          panel.linejoin(
>             x[id], y[id], horizontal=F,
>             col = trellis.par.get()$superpose.symbol$col[i], 
>             lty = trellis.par.get()$superpose.line$lty[i], 
>             lwd = trellis.par.get()$superpose.line$lwd[i])
>       }
>    }, 
>    main = "Some Text", 
>    key = list(
>       columns = 2,
>       text=list(
> # comment the next line out to see 
> # the values of the grouping variable
>          title = "More Text",  
>          lab = letters[5:6]),
>       lines=list(
>          col = trellis.par.get()$superpose.symbol$col[1:2],
>          lwd = trellis.par.get()$superpose.line$lwd[1:2],
>          lty = trellis.par.get()$superpose.line$lty[1:2]),
>       points = list(
>          col = trellis.par.get()$superpose.symbol$col[1:2],
>          pch = trellis.par.get()$superpose.symbol$pch[1:2]))) 
> 
> ----------------------------------------------------------------------
> LEGAL NOTICE\ Unless expressly stated otherwise, this
messag...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From carl at mcs.st-and.ac.uk  Thu Jun  1 22:23:47 2006
From: carl at mcs.st-and.ac.uk (Carl)
Date: Thu, 01 Jun 2006 21:23:47 +0100
Subject: [R] Re random number seeds
Message-ID: <447F4CD3.8060004@mcs.st-andrews.ac.uk>

Hi All.
(Just to stem the growing tide of helpful replies pointing out my 
stupidity).

It as as everyone suspects - I have at some point saved my workspace so 
there is a .Random.seed lurking. This is something I don't normally do, 
hence haven't encountered this before... it also explains why running my 
older R versions doesn't show this behaviour.

Thanks for the prompt responses, virtual beers to all.
C

-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Dr. Carl Donovan
Research Fellow in Statistical Computing
Ph +44 1334 461802
The Observatory
Buchanan Gardens
University of St Andrews
St Andrews
Fife
KY16 9LZ
Scotland


From m_nica at hotmail.com  Thu Jun  1 22:57:00 2006
From: m_nica at hotmail.com (Mihai Nica)
Date: Thu, 01 Jun 2006 15:57:00 -0500
Subject: [R] nls model singular gradient matrix parametrization
Message-ID: <BAY105-F13A75A93C1F6198EE76F37F8900@phx.gbl>

Greetings,
I am having a very hard time with a nonlinear regression. The last chance is 
that maybe somebody can spot something wrong? The data and the model are 
described below:

number of observations = 3030
y = [0,?,~16]
D1969 = [.16,?,~70,000]

>mod=nls(log(D1969)~d-log(1+d1*exp(-gt+g1*y)), start=list(d=11, d1=750000, 
>gt=14, g1=.9), trace=TRUE, data=pidg)
Error in nlsModel(formula, mf, start) : singular gradient matrix at initial 
parameter estimates

I ran several variants, changing the start values. However the graph with 
these starting values is almost identical with what is obtained with the 
real data (although it is rather nonlinear)? I am missing something, but 
can?t figure out what. If anybody has a little time and patience, any advice 
would be really really appreciated.
Thanks,


Mihai Nica, ABD
Jackson State University
ITT Tech Instructor
170 East Griffith Street G5
Jackson, MS 39201
601 914 0361

The least of learning is done in the classrooms.
  - Thomas Merton


From comtech.usa at gmail.com  Thu Jun  1 23:09:33 2006
From: comtech.usa at gmail.com (Michael)
Date: Thu, 1 Jun 2006 14:09:33 -0700
Subject: [R] "predict" function does not provide SE estimates for
	multivariate timeseries VAR models?
Message-ID: <b1f16d9d0606011409u42abe042vf9d5a6f7667d1528@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060601/e9a555b8/attachment.pl 

From michael.conklin at markettools.com  Fri Jun  2 00:08:37 2006
From: michael.conklin at markettools.com (Michael Conklin)
Date: Thu, 1 Jun 2006 17:08:37 -0500
Subject: [R]  Help with evaluation of expressions
Message-ID: <E51A91AB9FA86C44A929E431E0E2EAB2029CC42A@mnmail01.markettools.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060601/97d646fa/attachment.pl 

From xprt.wannabe at gmail.com  Fri Jun  2 01:22:25 2006
From: xprt.wannabe at gmail.com (xpRt.wannabe)
Date: Thu, 1 Jun 2006 18:22:25 -0500
Subject: [R] A coding question
Message-ID: <a4fecdd70606011622o2712b2edm9f57ea7ba364457@mail.gmail.com>

Dear List:

I have the follow code:

y <- replicate(10,replicate(8,sum(rnorm(rpois(1,5)))))

Now I need to apply the following condition to _every_ randomly generated
Normal number in the code above:

x - max(0,x-15) + max(0,x-90), where x represents the individual Normal
numbers.

In other words, the said condition needs to be applied before
replicate(...(replicate(...(sum(...))) takes place.

Any help would be greatly appreciated.


platform i386-pc-mingw32
arch i386
os mingw32
system i386, mingw32
status
major 2
minor 2.1
year 2005
month 12
day 20
svn rev 36812
language R


From hpbenton at scripps.edu  Fri Jun  2 01:57:47 2006
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Thu, 1 Jun 2006 16:57:47 -0700
Subject: [R] a matrix in a seprate window?
Message-ID: <200606012358.k51Nw5v05578@dns2.scripps.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060601/2001b4a5/attachment.pl 

From p.murrell at auckland.ac.nz  Fri Jun  2 00:11:01 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 02 Jun 2006 10:11:01 +1200
Subject: [R] DSC 2007 info
Message-ID: <447F65F5.4010300@stat.auckland.ac.nz>

Hi

This is a second announcement and call for abstracts.
Please forward and circulate to other interested parties.

DSC 2007, a conference on systems and environments for statistical
computing, will take place in Auckland, New Zealand on February 15 & 16,
2007.

We invite abstracts on the development of software systems and computing
environments for interactive statistics.  The workshop will focus on,
but is not limited to, open source statistical computing.

The deadline for submitting abstracts is 2006-10-15 (October 15th).
Please send abstracts (one page) to dsc2007 at stat.auckland.ac.nz

The workshop web page (including on-line registration) is now available
at http://www.stat.auckland.ac.nz/dsc-2007/

Paul
(on behalf of the Organising Committee)
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From montez at bu.edu  Fri Jun  2 01:09:01 2006
From: montez at bu.edu (Maria Montez)
Date: Thu, 1 Jun 2006 19:09:01 -0400
Subject: [R] how to add point and label to boxplot using bwplot
Message-ID: <001501c685d0$5f4fb9b0$8d01a8c0@HSRDCHQOER.VHA01.MED.VA.GOV>

Hi.

My data contains information for 10 hospitals for 12 different measures.
Let's call it x1-x12. I need to create a boxplot for each one of this
measures and put them into one page. Each plot also needs to be independent,
i.e. cannot use the group feature because of different scales for each
measure. I was successful using the following code:

x1 <- c(1.317604376, 0.978038233, 0.396823901, 0.601130135, 1.039993474,
0.434282156, 0.941604514, 0.84796521, 1.614475852, 2.723760844)
c1 <- as.integer(c(9, 9, 2, 7, 10, 1, 11, 49, 26, 5))

plot1 <- bwplot(x1,cex=.5,scales=list(cex=0.5), xlab=list("psi03",cex=.5))
print(plot1,split=c(1,1,1,12),more=TRUE)

#plot2<-bwplot(x2,cex=.5,scales=list(cex=0.5), xlab=list("psi04",cex=.5))
#print(plot2,split=c(1,2,1,12),more=TRUE)

... (code for other 10 plots)

The main purpose is to create one of this pages with information relative to
each hospital. So, on each boxplot I need to add a point indicating where
that hospital falls with a label that indicates the number of cases
regarding that variable. Then I create 10 different pages.

I first tried this without using lattice plots by doing:

boxplot(x1, horizontal=TRUE)
points(x1[4],1,pch=19) #plot point for hospital 2
text(x1[4],0.95,c1[4]) # next to that point indicate number of cases

How can I do the same in a lattice plot? I've tried the following, but it
doesn't work (I think I might not be passing the right arguments):

bwplot(x1,cex=.5,scales=list(cex=0.5),tck=0.5, xlab=list("psi03",cex=.5),
			panel=function(x,c1){
					panel.bwplot(x)
					panel.points(x[2],0.5,pch=19)
					panel.text(x[4],0.95,c1[4])
				}
	)

Many thanks, Maria



**********************************************************
Maria E Montez
Programmer / Data Analyst
Center for Health Quality, Outcomes and Economic Research
VA Medical Center
Bedford, MA


From berwin at maths.uwa.edu.au  Fri Jun  2 07:52:59 2006
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Fri, 2 Jun 2006 13:52:59 +0800
Subject: [R] "predict" function does not provide SE estimates
	for	multivariate timeseries VAR models?
In-Reply-To: <b1f16d9d0606011409u42abe042vf9d5a6f7667d1528@mail.gmail.com>
References: <b1f16d9d0606011409u42abe042vf9d5a6f7667d1528@mail.gmail.com>
Message-ID: <17535.53819.101085.194676@bossiaea.maths.uwa.edu.au>

>>>>> "Michael" == Michael  <comtech.usa at gmail.com> writes:

    Michael> What can I do?
Implement this feature and send it to the person responsible for the
code for inclusion?

    Michael> Thanks a lot!
Indeed, you contribution would be very much appreciated.

Cheers,

        Berwin

========================== Full address ============================
Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)   
School of Mathematics and Statistics        +61 (8) 6488 3383 (self)      
The University of Western Australia   FAX : +61 (8) 6488 1028
35 Stirling Highway                   
Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
Australia                        http://www.maths.uwa.edu.au/~berwin


From ripley at stats.ox.ac.uk  Fri Jun  2 08:03:02 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 2 Jun 2006 07:03:02 +0100 (BST)
Subject: [R] nls model singular gradient matrix parametrization
In-Reply-To: <BAY105-F13A75A93C1F6198EE76F37F8900@phx.gbl>
References: <BAY105-F13A75A93C1F6198EE76F37F8900@phx.gbl>
Message-ID: <Pine.LNX.4.64.0606020657090.1935@gannet.stats.ox.ac.uk>

Your model is over-parametrized: d1*exp(-gt) gives two parameters for one 
constant.  As a result, the least-square surface is flat in one direction, 
and the gradient matrix is singular.

If this is the model you intended, you can simplify it by dropping d1.
It is also partially linear (d) so it should be possible to get 
method="plinear" to work.

On Thu, 1 Jun 2006, Mihai Nica wrote:

> Greetings,
> I am having a very hard time with a nonlinear regression. The last chance is 
> that maybe somebody can spot something wrong
 The data and the model are 
> described below:
>
> number of observations = 3030
> y = [0,
,~16]
> D1969 = [.16,
,~70,000]
>
>> mod=nls(log(D1969)~d-log(1+d1*exp(-gt+g1*y)), start=list(d=11, d1=750000, 
>> gt=14, g1=.9), trace=TRUE, data=pidg)
> Error in nlsModel(formula, mf, start) : singular gradient matrix at initial 
> parameter estimates
>
> I ran several variants, changing the start values. However the graph with 
> these starting values is almost identical with what is obtained with the real 
> data (although it is rather nonlinear)
 I am missing something, but can?t 
> figure out what. If anybody has a little time and patience, any advice would 
> be really really appreciated.
> Thanks,
>
>
> Mihai Nica, ABD
> Jackson State University
> ITT Tech Instructor
> 170 East Griffith Street G5
> Jackson, MS 39201
> 601 914 0361
>
> The least of learning is done in the classrooms.
> - Thomas Merton
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From lobry at biomserv.univ-lyon1.fr  Fri Jun  2 08:08:59 2006
From: lobry at biomserv.univ-lyon1.fr (Jean lobry)
Date: Fri, 2 Jun 2006 08:08:59 +0200
Subject: [R] problem when calling help.search() a second time
In-Reply-To: <Pine.LNX.4.64.0606011052510.1042@gannet.stats.ox.ac.uk>
References: <mailman.13.1148637603.15698.r-help@stat.math.ethz.ch>
	<p06002024c0a45b8b1c7a@[192.168.1.10]>
	<Pine.LNX.4.64.0606011052510.1042@gannet.stats.ox.ac.uk>
Message-ID: <p06002029c0a58450efc6@[192.168.1.10]>

>
>Argument 'rebuild'.  The error message will be better in R-devel.
>

Thank you very much, setting argument 'rebuild' to TRUE in
help.search() fixed the problem. My Sweave document can include
now all the high level plot examples from all base packages
intead just from the 'graphics' one. This is fantastic, thanks
again.

I will try to post the error message in R-devel.

-- 
Jean R. Lobry            (lobry at biomserv.univ-lyon1.fr)
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - LYON I,
43 Bd 11/11/1918, F-69622 VILLEURBANNE CEDEX, FRANCE
allo  : +33 472 43 12 87     fax    : +33 472 43 13 88
http://pbil.univ-lyon1.fr/members/lobry/


From ripley at stats.ox.ac.uk  Fri Jun  2 08:12:43 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 2 Jun 2006 07:12:43 +0100 (BST)
Subject: [R] a matrix in a seprate window?
In-Reply-To: <200606012358.k51Nw5v05578@dns2.scripps.edu>
References: <200606012358.k51Nw5v05578@dns2.scripps.edu>
Message-ID: <Pine.LNX.4.64.0606020706091.1935@gannet.stats.ox.ac.uk>

On Thu, 1 Jun 2006, H. Paul Benton wrote:

> A very simple one but I cannot figure it out. I have a matrix and I want to
> display it in a seprate window. I would really like it if it worked on both
> linux and windows.

edit.matrix will provide you with a spreadsheet display in a separate 
window.  Unfortunately it is modal: rewriting it to provide an 
asynchronous data viewer is on my TODO list.

Alternatively, you can write out the matrix to a file and show the file.
For example

library(MASS)
f <- tempfile()
write.table(as.matrix(hills), file=f, sep="\t")
file.show(f)

On linux you would need to use a pager that displayed in a separate 
window: there are many possible choices (I tend to use emacsclient).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From talcon at iastate.edu  Fri Jun  2 10:14:14 2006
From: talcon at iastate.edu (Tim Alcon)
Date: Fri, 02 Jun 2006 03:14:14 -0500
Subject: [R] basic array question
Message-ID: <447FF356.8010207@iastate.edu>

I have a large array and would like to extract from it the row and 
column indices just of values for which a particular boolean condition 
is true.  I assume there's a simple way to do this, but I haven't 
figured it out yet.  Any help would be appreciated.

Tim


From ligges at statistik.uni-dortmund.de  Fri Jun  2 10:26:44 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 02 Jun 2006 10:26:44 +0200
Subject: [R] basic array question
In-Reply-To: <447FF356.8010207@iastate.edu>
References: <447FF356.8010207@iastate.edu>
Message-ID: <447FF644.8020108@statistik.uni-dortmund.de>

Tim Alcon wrote:
> I have a large array and would like to extract from it the row and 
> column indices just of values for which a particular boolean condition 

Do you mean a matrix, i.e. exactly 2 dimensions?

> is true.  I assume there's a simple way to do this, but I haven't 
> figured it out yet.  Any help would be appreciated.

Example:


  X <- matrix(1:9, 3)
  col(X)[X == 4]
  row(X)[X == 4]

Uwe Ligges

> Tim
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ligges at statistik.uni-dortmund.de  Fri Jun  2 10:37:58 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 02 Jun 2006 10:37:58 +0200
Subject: [R] A coding question
In-Reply-To: <a4fecdd70606011622o2712b2edm9f57ea7ba364457@mail.gmail.com>
References: <a4fecdd70606011622o2712b2edm9f57ea7ba364457@mail.gmail.com>
Message-ID: <447FF8E6.6080209@statistik.uni-dortmund.de>

xpRt.wannabe wrote:

> Dear List:
> 
> I have the follow code:
> 
> y <- replicate(10,replicate(8,sum(rnorm(rpois(1,5)))))
> 
> Now I need to apply the following condition to _every_ randomly generated
> Normal number in the code above:
> 
> x - max(0,x-15) + max(0,x-90), where x represents the individual Normal
> numbers.
> 
> In other words, the said condition needs to be applied before
> replicate(...(replicate(...(sum(...))) takes place.
> 
> Any help would be greatly appreciated.


y <- replicate(10, {
         rp <- rpois(8, 5)
         mysum <- sapply(rp, function(x) {
             x <- rnorm(x)
             x <- x - max(0, x-15) + max(0, x-90)
             sum(x)
         })
      })

> 
> platform i386-pc-mingw32
> arch i386
> os mingw32
> system i386, mingw32
> status
> major 2
> minor 2.1
> year 2005
> month 12
> day 20
> svn rev 36812
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From petr.pikal at precheza.cz  Fri Jun  2 10:51:20 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 02 Jun 2006 10:51:20 +0200
Subject: [R] basic array question
In-Reply-To: <447FF644.8020108@statistik.uni-dortmund.de>
References: <447FF356.8010207@iastate.edu>
Message-ID: <44801828.25149.15D347@localhost>

Hi

On 2 Jun 2006 at 10:26, Uwe Ligges wrote:

Date sent:      	Fri, 02 Jun 2006 10:26:44 +0200
From:           	Uwe Ligges <ligges at statistik.uni-dortmund.de>
Organization:   	Fachbereich Statistik, Universitaet Dortmund
To:             	Tim Alcon <talcon at iastate.edu>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] basic array question

> Tim Alcon wrote:
> > I have a large array and would like to extract from it the row and
> > column indices just of values for which a particular boolean
> > condition 
> 
> Do you mean a matrix, i.e. exactly 2 dimensions?
> 
> > is true.  I assume there's a simple way to do this, but I haven't
> > figured it out yet.  Any help would be appreciated.
> 
> Example:
> 
> 
>   X <- matrix(1:9, 3)
>   col(X)[X == 4]
>   row(X)[X == 4]
>

or

> which(X==4, arr.ind=T)
     row col
[1,]   1   2
>
which works on higher dim arrays too

> X <- array(1:12, c(3,2,2))

> X
, , 1

     [,1] [,2]
[1,]    1    4
[2,]    2    5
[3,]    3    6

, , 2

     [,1] [,2]
[1,]    7   10
[2,]    8   11
[3,]    9   12

> which(X==6, arr.ind=T)
     dim1 dim2 dim3
[1,]    3    2    1
>

HTH
Petr


 
> Uwe Ligges
> 
> > Tim
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From konrad.banachewicz at gmail.com  Fri Jun  2 11:22:11 2006
From: konrad.banachewicz at gmail.com (Konrad Banachewicz)
Date: Fri, 2 Jun 2006 11:22:11 +0200
Subject: [R] Multivariate skew-t cdf
Message-ID: <204e4c50606020222t1f46e4a2oebd19fda111fa1c1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060602/1531eae6/attachment.pl 

From m_nica at hotmail.com  Fri Jun  2 12:25:33 2006
From: m_nica at hotmail.com (Mihai Nica)
Date: Fri, 02 Jun 2006 05:25:33 -0500
Subject: [R] nls model singular gradient matrix parametrization
In-Reply-To: <Pine.LNX.4.64.0606020657090.1935@gannet.stats.ox.ac.uk>
Message-ID: <BAY105-F31D1A41D9C03D89B07A3AFF8910@phx.gbl>

Yes! Now I can see it, it's so evident... Thank you so much,
m

Mihai Nica, ABD
Jackson State University
ITT Tech Instructor
170 East Griffith Street G5
Jackson, MS 39201
601 914 0361

The least of learning is done in the classrooms.
  - Thomas Merton



----Original Message Follows----
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
To: Mihai Nica <m_nica at hotmail.com>
CC: r-help at stat.math.ethz.ch
Subject: Re: [R] nls model singular gradient matrix parametrization
Date: Fri, 2 Jun 2006 07:03:02 +0100 (BST)

Your model is over-parametrized: d1*exp(-gt) gives two parameters for one 
constant.  As a result, the least-square surface is flat in one direction, 
and the gradient matrix is singular.

If this is the model you intended, you can simplify it by dropping d1.
It is also partially linear (d) so it should be possible to get 
method="plinear" to work.

On Thu, 1 Jun 2006, Mihai Nica wrote:

>Greetings,
>I am having a very hard time with a nonlinear regression. The last chance 
>is that maybe somebody can spot something wrong? The data and the model are 
>described below:
>
>number of observations = 3030
>y = [0,?,~16]
>D1969 = [.16,?,~70,000]
>
>>mod=nls(log(D1969)~d-log(1+d1*exp(-gt+g1*y)), start=list(d=11, d1=750000, 
>>gt=14, g1=.9), trace=TRUE, data=pidg)
>Error in nlsModel(formula, mf, start) : singular gradient matrix at initial 
>parameter estimates
>
>I ran several variants, changing the start values. However the graph with 
>these starting values is almost identical with what is obtained with the 
>real data (although it is rather nonlinear)? I am missing something, but 
>can?t figure out what. If anybody has a little time and patience, any 
>advice would be really really appreciated.
>Thanks,
>
>
>Mihai Nica, ABD
>Jackson State University
>ITT Tech Instructor
>170 East Griffith Street G5
>Jackson, MS 39201
>601 914 0361
>
>The least of learning is done in the classrooms.
>- Thomas Merton
>
>

--
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jzhang1982 at gmail.com  Fri Jun  2 14:24:03 2006
From: jzhang1982 at gmail.com (zhang jian)
Date: Fri, 2 Jun 2006 08:24:03 -0400
Subject: [R] a question about a simply figure
Message-ID: <3f2938d50606020524u1c927ffej8b7939fb4c972be3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060602/4850acbe/attachment.pl 

From seanpor at acm.org  Fri Jun  2 14:36:57 2006
From: seanpor at acm.org (Sean O'Riordain)
Date: Fri, 2 Jun 2006 13:36:57 +0100
Subject: [R] a question about a simply figure
In-Reply-To: <3f2938d50606020524u1c927ffej8b7939fb4c972be3@mail.gmail.com>
References: <3f2938d50606020524u1c927ffej8b7939fb4c972be3@mail.gmail.com>
Message-ID: <8ed68eed0606020536j755a8d04y6e53a350e9610618@mail.gmail.com>

Hi Zhang Jian,

If I say plot(t$no,t$leiji), then the lower bound is "neatly" set at
about 25... (I'm not sure how I can measure the bounds on the current
plot - but I'm sure it can be found!)

You can set the bounds on the y-axis to be between 0 and 100 by saying
plot(fre$no,fre$leiji,ylim=c(0,100))

cheers,
Sean


On 02/06/06, zhang jian <jzhang1982 at gmail.com> wrote:
> I think there is a question in R. I donot know the reason.
> This is my data about comulative percentage figure. The result is not right.
>
> The first point (no=1,leiji=26.94350) in the plot figure was showen in a
> lower location. Why?
> Thanks !
>
> > fre
>    no     leiji
> 1   1  26.94350
> 2   2  46.86401
> 3   3  60.59032
> 4   4  72.17355
> 5   5  77.85521
> 6   6  82.05853
> 7   7  85.56495
> 8   8  87.64378
> 9   9  89.42997
> 10 10  91.01150
> 11 11  92.32409
> 12 12  93.48106
> 13 13  94.62618
> 14 14  95.47530
> 15 15  96.25000
> 16 16  96.71516
> 17 17  97.14648
> 18 18  97.51522
> 19 19  97.86367
> 20 20  98.10217
> 21 21  98.32544
> 22 22  98.51658
> 23 23  98.69756
> 24 24  98.85995
> 25 25  99.00372
> 26 26  99.11874
> 27 27  99.22192
> 28 28  99.32510
> 29 29  99.42659
> 30 30  99.49932
> 31 31  99.56360
> 32 32  99.61265
> 33 33  99.66001
> 34 34  99.70737
> 35 35  99.75304
> 36 36  99.79702
> 37 37  99.83424
> 38 38  99.86130
> 39 39  99.88836
> 40 40  99.91373
> 41 41  99.93572
> 42 42  99.95264
> 43 43  99.96448
> 44 44  99.97294
> 45 45  99.97970
> 46 46  99.98647
> 47 47  99.99154
> 48 48  99.99493
> 49 49  99.99662
> 50 50  99.99831
> 51 51 100.00000
> >plot(fre$no,fre$leiji)
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From bernarduse1 at yahoo.fr  Fri Jun  2 15:01:54 2006
From: bernarduse1 at yahoo.fr (Marc Bernard)
Date: Fri, 2 Jun 2006 15:01:54 +0200 (CEST)
Subject: [R] nlme: extract covariance matrix of the random effects
Message-ID: <20060602130154.21102.qmail@web25801.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060602/8326e20b/attachment.pl 

From dieter.menne at menne-biomed.de  Fri Jun  2 15:20:05 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 2 Jun 2006 13:20:05 +0000 (UTC)
Subject: [R] nlme: extract covariance matrix of the random effects
References: <20060602130154.21102.qmail@web25801.mail.ukl.yahoo.com>
Message-ID: <loom.20060602T151945-91@post.gmane.org>

Marc Bernard <bernarduse1 <at> yahoo.fr> writes:

>   I am looking for a function to extract, from an nlme object,  the estimated
 variance-covariance matrix of the random effects.
 
VarCorr

D.Menne


From bolker at ufl.edu  Fri Jun  2 15:26:57 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Fri, 2 Jun 2006 13:26:57 +0000 (UTC)
Subject: [R] A coding question
References: <a4fecdd70606011622o2712b2edm9f57ea7ba364457@mail.gmail.com>
	<447FF8E6.6080209@statistik.uni-dortmund.de>
Message-ID: <loom.20060602T152407-422@post.gmane.org>

Uwe Ligges <ligges <at> statistik.uni-dortmund.de> writes:

> 
> xpRt.wannabe wrote:
> 
> > y <- replicate(10,replicate(8,sum(rnorm(rpois(1,5)))))
> > 
> > x - max(0,x-15) + max(0,x-90), where x represents the individual Normal
> > numbers.

> 
> y <- replicate(10, {
>          rp <- rpois(8, 5)
>          mysum <- sapply(rp, function(x) {
>              x <- rnorm(x)
>              x <- x - max(0, x-15) + max(0, x-90)
>              sum(x)
>          })
>       })
> 

  I think this can be boiled down a bit further.

x-max(0,x-15)+max(0,x-90)

if x>90:        x-(x-15)+(x-90) = x+15-90=x-75
if 15<x<90      x-(x-15)+0      = 15
if x<15         x-0+0           = x

therefore

tmpf <- function() {
  x <- rnorm(rpois(1,5))
  sum(ifelse(x>90,x-75,pmin(x,15)))
}

replicate(10,replicate(8,tmpf()))

(you could also generate 80 values and then break them
 up into segments of 8)

  Ben Bolker


From dieter.menne at menne-biomed.de  Fri Jun  2 16:09:29 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 2 Jun 2006 14:09:29 +0000 (UTC)
Subject: [R] Help with evaluation of expressions
References: <E51A91AB9FA86C44A929E431E0E2EAB2029CC42A@mnmail01.markettools.com>
Message-ID: <loom.20060602T160539-705@post.gmane.org>

Michael Conklin <michael.conklin <at> markettools.com> writes:

> 
> I have searched through the help files and I have been unsuccessful in
> solving this problem.
> 
> I am trying to create a small wrapper function that will go around a
> call to a plot function and create a windows metafile in the directory
> and also write the name of the file to a text file. The purpose is to
> efficiently bring a large number of plots into powerpoint.  I am using
> platform      

Try this variant. Note that I am using emf instead of wmf, it's a bit more
stable. And I had to prepare an undecorated filename since savePlot has special
ideas about appending appendixes. 

Please, if you post, provide a full working demo example next time, not only the
function. There are only few genius who can see the right way without firing up
the gui.

Dieter
------

plotSlide<-function(pltexpr){
  expr<-as.expression(pltexpr)
  numslide<-length(dir(pattern="emf"))+1
  fname0<-paste("SG",numslide,sep="")
  fname<-paste(fname0,".emf",sep="")
  dirlist<-dir(pattern="SlideList.txt")
  if (length(dirlist)==1){
    fs<-file("SlideList.txt",open="at")
    write(paste(fname,"Graph",sep=","),file=fs)
    close(fs)} else {
    write(paste(fname,"Graph",sep=","),file="SlideList.txt")
    }
  p=eval(pltexpr)
  if (is.null(p)) {
    savePlot(fname0,type="emf")
  } else {
    trellis.device(device = win.metafile, theme = "col.whitebg", new =
TRUE, retain = FALSE, file=fname,width=6,height=4)
    print(p)
  }
  dev.off()
  fname
}

plotSlide(
  plot(rnorm(100))
)

library(lattice)
plotSlide(
  xyplot(Sepal.Length  ~ Petal.Length, data = iris)
)


From edownie at mc2k.com  Fri Jun  2 15:28:44 2006
From: edownie at mc2k.com (Edward Downie)
Date: Fri, 2 Jun 2006 08:28:44 -0500 (CDT)
Subject: [R] Running Demos
Message-ID: <Pine.LNX.4.10.10606020825200.530-100000@mc2k.com>

I see the output of running, for instance, demo(lm.glm)
just fine, but how do I view the set of commands and the
data that served as the input?

Thanks for the help.

			Edward Downie


From jrkrideau at yahoo.ca  Fri Jun  2 16:33:05 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 2 Jun 2006 10:33:05 -0400 (EDT)
Subject: [R] variation on vioplot?
In-Reply-To: <ypx664jkhjrn.fsf@uracil.uio.no>
Message-ID: <20060602143305.46398.qmail@web33810.mail.mud.yahoo.com>

I don't think this is exactly what you want but try:

par(mfrow=c(1,2))
vioplot(normal, names="Normal", horizontal=TRUE,
ylim=c(-12,12))
vioplot(uniform, names ="Uniform", horizontal=TRUE,
ylim=c(-12,12))



--- Karin Lagesen <karin.lagesen at medisin.uio.no>
wrote:

> Michael Dondrup
> <michael.dondrup at cebitec.uni-bielefeld.de> writes:
> 
> > Hi Karin,
> > I would like to help with this, but it's not
> completely clear to me
> > what your vioplots look or should look like. Could
> you post a little
> > reproducible code example to the list, and then
> specify what should be
> > different?
> 
> OK.
> 
> Adapted from the vioplot example:
> 
> source("vioplot.R")
> uniform<-runif(2000,-4,4)
> normal<-rnorm(2000,0,3)
> vioplot(normal,uniform,horizontal=TRUE)
> 
> Now these two are positioned one on top of the
> other. I would like to
> have them next to each other, if possible. I would
> then like the scale
> underneath to be duplicated so that there is one
> scale underneath each
> of them.
> 
> Karin
> -- 
> Karin Lagesen, PhD student
> karin.lagesen at medisin.uio.no
> http://www.cmbn.no/rognes/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Fri Jun  2 16:38:22 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 2 Jun 2006 10:38:22 -0400
Subject: [R] Running Demos
In-Reply-To: <Pine.LNX.4.10.10606020825200.530-100000@mc2k.com>
References: <Pine.LNX.4.10.10606020825200.530-100000@mc2k.com>
Message-ID: <971536df0606020738l63d4f265o418a40925ca19383@mail.gmail.com>

Try this:

   file.show(system.file("demo/lm.glm.R", package = "stats"))


On 6/2/06, Edward Downie <edownie at mc2k.com> wrote:
> I see the output of running, for instance, demo(lm.glm)
> just fine, but how do I view the set of commands and the
> data that served as the input?
>
> Thanks for the help.
>
>                        Edward Downie


From paulojus at est.ufpr.br  Fri Jun  2 16:41:26 2006
From: paulojus at est.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Fri, 2 Jun 2006 11:41:26 -0300 (BRT)
Subject: [R] Running Demos
In-Reply-To: <Pine.LNX.4.10.10606020825200.530-100000@mc2k.com>
References: <Pine.LNX.4.10.10606020825200.530-100000@mc2k.com>
Message-ID: <Pine.LNX.4.63.0606021140060.30322@est.ufpr.br>


The source R files comes with the package

for instance, in this case, in a Linux machine you can locate it with:

paulojus at pj:~$ locate lm.glm
/usr/lib/R/library/stats/demo/lm.glm.R

In other OS this will be similar, look for the "stats" package

best
P.J.



On Fri, 2 Jun 2006, Edward Downie wrote:

> Date: Fri, 2 Jun 2006 08:28:44 -0500 (CDT)
> From: Edward Downie <edownie at mc2k.com>
> To: r-help at stat.math.ethz.ch
> Subject: [R] Running Demos
> 
> I see the output of running, for instance, demo(lm.glm)
> just fine, but how do I view the set of commands and the
> data that served as the input?
>
> Thanks for the help.
>
> 			Edward Downie
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

Paulo Justiniano Ribeiro Jr
LEG (Laborat?rio de Estat?stica e Geoinforma??o)
Departamento de Estat?stica
Universidade Federal do Paran?
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 3361 3573
Fax: (+55) 41 3361 3141
e-mail: paulojus at est.ufpr.br
http://www.est.ufpr.br/~paulojus

From ivowel at gmail.com  Fri Jun  2 16:41:37 2006
From: ivowel at gmail.com (ivo welch)
Date: Fri, 2 Jun 2006 10:41:37 -0400
Subject: [R] speed?
Message-ID: <50d1c22d0606020741s2a410747r433e569abfcf177@mail.gmail.com>

dear R wizards:

while extolling the virtues of R, one of my young econometrics
colleagues told me that he still wants to run ox because [a] his code
is written in it (good reason); [b] because ox seems to be faster than
R in most benchmarks (huh?).

this got me to wonder.  language speed can't matter much, so it must
be mostly the underlying matrix algebra by now.  I presume that
nowadays most of the plain matrix operation speed depends primarily on
which hardware features the library accesses.  (The basic algorithms
aren't so different, so even though the algorithm may have mattered a
long time ago, they are probably pretty similar nowadays. hmmm...maybe
matrix inversion still is different, but multiplication and adding
should not be.)

On x86 architecture, I believe there is a hierarchy in terms of raw
processing power:  FPU < SSE* < GPU.

is it even possible to use the GPU now for math processing (linux or
windows), and specifically in R?

assuming I compile everything with the proper SSE flags and atlas, is
SSE* fully taken advantage of?

regards,

/ivo


From miltinho_astronauta at yahoo.com.br  Fri Jun  2 16:44:21 2006
From: miltinho_astronauta at yahoo.com.br (Milton Cezar)
Date: Fri, 2 Jun 2006 14:44:21 +0000 (GMT)
Subject: [R] Table collumn to single var in lowcase
Message-ID: <20060602144421.69541.qmail@web53415.mail.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060602/01a2ae2a/attachment.pl 

From linux at comjet.com  Fri Jun  2 16:52:43 2006
From: linux at comjet.com (Larry Howe)
Date: Fri, 2 Jun 2006 10:52:43 -0400
Subject: [R] Intercept of confidence interval with a constant
Message-ID: <200606021052.43908.linux@comjet.com>

Hello,

Is there a way for R to determine the point where a confidence interval equals 
a specified value? For example:

x = seq(1:5)
y = c(5, 5, 4, 4, 3)
lm = lm(y ~ x)
p = predict.lm(lm, interval="confidence")
matplot(p, type="b")
abline(h = 3)

I want to answer the question: "What is the value of x when the y-value of the 
lower confidence interval is equal to 3.0"? Visually, it is the place on the 
example where the abline intersects the lower confidence interval, or about 
4.2. Can R calculate this number for me?

I know that predict.lm will calculate a y-value from an x-value, but what I 
want is the opposite. I know the y-value, and I want to calculate the 
x-value.

Larry Howe


From RMK at krugs.de  Fri Jun  2 17:02:05 2006
From: RMK at krugs.de (Rainer M KRug)
Date: Fri, 02 Jun 2006 17:02:05 +0200
Subject: [R] Problem with mle
Message-ID: <448052ED.4070209@krugs.de>

R 2.3.0
Linux, SuSE 10.0

Hi

I have two problems with mle - probably I am using it the wrong way so
please let me know.

I want to fit different distributions to an observed count of seeds and
in the next step use AIC or BIC to identify the best distribution.

But when I run the script below (which is part of my original script), I
get one error message for the first call of mle:

Error in solve.default(oout$hessian) : Lapack routine dgesv: system is
exactly singular
In addition: Warning messages:
1: NaNs produced in: dweibull(x, shape, scale, log)
2: NaNs produced in: dweibull(x, shape, scale, log)
3: NaNs produced in: dweibull(x, shape, scale, log)
4: NaNs produced in: dweibull(x, shape, scale, log)

What can I do to avoid this problem?

The second one is following the second call of mle:
summary(fit.NE) gives me the summary of the fit, but

profile(fit.NE) gives me the following error message:
Error in onestep(step) : profiling has found a better solution, so
original fit had not converged
In addition: Warning messages:
1: NaNs produced in: dexp(x, 1/rate, log)
2: NaNs produced in: dexp(x, 1/rate, log)
3: NaNs produced in: dexp(x, 1/rate, log)
4: NaNs produced in: dexp(x, 1/rate, log)
5: NaNs produced in: dexp(x, 1/rate, log)
6: NaNs produced in: dexp(x, 1/rate, log)

What can I do in this case - which parameters do I have to change that
it converges?

Rainer


######################################
library(stats4)

SumSeeds <- c(762, 406, 288, 260, 153, 116,  57,  33,  40,  44,  36,
24,  35,  23,  36,  25,  35,  30)
X <- c(1.74924, 3.49848, 5.24772, 6.99696, 8.74620, 17.49240, 26.23860,
34.98480, 43.73100, 52.47720, 61.22340, 69.96960, 71.71880, 73.46810,
75.21730, 76.96650, 78.71580, 80.46500 )

#Sum of log of values in vector
SumLog <- function (x)
{
	sum(log(x))	
}

#-ln(L) with Poisson Likelihood estimator
NeglogPoisL <- function (obs, est)
{
	sel <- est != 0
	SumLogObs <- SumLog(obs[sel])
	-sum( obs[sel] * log(est[sel]) - est[sel]- SumLogObs )
}


#-ln(L) for Negative Exponential
NENeglogPoisL <- function(a=0.2, rate=0.04)
{
	NeglogPoisL( SumSeeds, a * dexp( X, rate) )									
}

#-ln(L) for Weibull
WBNeglogPoisL <- function(a=1000000, shape=0.12, scale=1e+37)
{
	NeglogPoisL( SumSeeds, a * dweibull(X, shape, scale) )
}



fit.WB <- mle(	
				WBNeglogPoisL
				, start=list(a=1, shape=0.1, scale=1)
		 		, fixed=list()
				, control=list(maxit=10000)
			 )

fit.NE <- mle(	
				NENeglogPoisL
				, start=list(a=1, rate=1)
	 			, fixed=list()
				, control=list(maxit=10000)
			 )
######################################


From ccleland at optonline.net  Fri Jun  2 16:59:51 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 02 Jun 2006 10:59:51 -0400
Subject: [R] Table collumn to single var in lowcase
In-Reply-To: <20060602144421.69541.qmail@web53415.mail.yahoo.com>
References: <20060602144421.69541.qmail@web53415.mail.yahoo.com>
Message-ID: <44805267.5010508@optonline.net>

names(DataTABLE) <- tolower(names(DataTABLE))

?casefold
?names

Milton Cezar wrote:
> Dear All,
>    
>   I have read a table using
>       DataTABLE <- read.table("mytable.txt, header=T) 
>    
>   And get the following data structure
>        Var1  VAR2   VaR3  Var4 ...
>    
>   How can I list all collumn names (in lowcase) and create variables from table collumns. By hand I do 
>       var1 <- DataTABLE$Var1
>       var2 <- DataTABLE$VAR2
>       var3 <- DataTABLE$VaR3
>       var4 <- DataTABLE$Var4
>    
>   Unfortunatelly these data come as an output from other program and the data file have about 80 collumns.
>    
>   Thanks in advance!
>    
>   miltinho
> 
>  __________________________________________________
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From andy_liaw at merck.com  Fri Jun  2 17:00:38 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 2 Jun 2006 11:00:38 -0400
Subject: [R] speed?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02585DE7@usctmx1106.merck.com>

You (and your colleague) might want to have a look at
http://www.sciviews.org/benchmark/.  It's a bit dated,
but still may be a good starting point.

Some months ago some one asked about working on getting
R to use the GPU for computation on the R-devel list.
Don't know if anything came of it.

Cheers,
Andy 

From: ivo welch
> 
> dear R wizards:
> 
> while extolling the virtues of R, one of my young 
> econometrics colleagues told me that he still wants to run ox 
> because [a] his code is written in it (good reason); [b] 
> because ox seems to be faster than R in most benchmarks (huh?).
> 
> this got me to wonder.  language speed can't matter much, so 
> it must be mostly the underlying matrix algebra by now.  I 
> presume that nowadays most of the plain matrix operation 
> speed depends primarily on which hardware features the 
> library accesses.  (The basic algorithms aren't so different, 
> so even though the algorithm may have mattered a long time 
> ago, they are probably pretty similar nowadays. hmmm...maybe 
> matrix inversion still is different, but multiplication and 
> adding should not be.)
> 
> On x86 architecture, I believe there is a hierarchy in terms 
> of raw processing power:  FPU < SSE* < GPU.
> 
> is it even possible to use the GPU now for math processing 
> (linux or windows), and specifically in R?
> 
> assuming I compile everything with the proper SSE flags and atlas, is
> SSE* fully taken advantage of?
> 
> regards,
> 
> /ivo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From ehlers at math.ucalgary.ca  Fri Jun  2 17:06:58 2006
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Fri, 02 Jun 2006 09:06:58 -0600
Subject: [R] Table collumn to single var in lowcase
In-Reply-To: <20060602144421.69541.qmail@web53415.mail.yahoo.com>
References: <20060602144421.69541.qmail@web53415.mail.yahoo.com>
Message-ID: <44805412.30102@math.ucalgary.ca>

tolower(names(DataTABLE))

Peter Ehlers


Milton Cezar wrote:
> Dear All,
>    
>   I have read a table using
>       DataTABLE <- read.table("mytable.txt, header=T) 
>    
>   And get the following data structure
>        Var1  VAR2   VaR3  Var4 ...
>    
>   How can I list all collumn names (in lowcase) and create variables from table collumns. By hand I do 
>       var1 <- DataTABLE$Var1
>       var2 <- DataTABLE$VAR2
>       var3 <- DataTABLE$VaR3
>       var4 <- DataTABLE$Var4
>    
>   Unfortunatelly these data come as an output from other program and the data file have about 80 collumns.
>    
>   Thanks in advance!
>    
>   miltinho
> 
>  __________________________________________________
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ehlers at math.ucalgary.ca  Fri Jun  2 17:16:11 2006
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Fri, 02 Jun 2006 09:16:11 -0600
Subject: [R] a question about a simply figure
In-Reply-To: <3f2938d50606020524u1c927ffej8b7939fb4c972be3@mail.gmail.com>
References: <3f2938d50606020524u1c927ffej8b7939fb4c972be3@mail.gmail.com>
Message-ID: <4480563B.1080909@math.ucalgary.ca>

I don't understand the question. What's wrong with showing 26.94350 as
being less than all the other leiji values?

Peter Ehlers


zhang jian wrote:

> I think there is a question in R. I donot know the reason.
> This is my data about comulative percentage figure. The result is not right.
> 
> The first point (no=1,leiji=26.94350) in the plot figure was showen in a
> lower location. Why?
> Thanks !
> 
> 
>>fre
> 
>    no     leiji
> 1   1  26.94350
> 2   2  46.86401
> 3   3  60.59032
> 4   4  72.17355
> 5   5  77.85521
> 6   6  82.05853
> 7   7  85.56495
> 8   8  87.64378
> 9   9  89.42997
> 10 10  91.01150
> 11 11  92.32409
> 12 12  93.48106
> 13 13  94.62618
> 14 14  95.47530
> 15 15  96.25000
> 16 16  96.71516
> 17 17  97.14648
> 18 18  97.51522
> 19 19  97.86367
> 20 20  98.10217
> 21 21  98.32544
> 22 22  98.51658
> 23 23  98.69756
> 24 24  98.85995
> 25 25  99.00372
> 26 26  99.11874
> 27 27  99.22192
> 28 28  99.32510
> 29 29  99.42659
> 30 30  99.49932
> 31 31  99.56360
> 32 32  99.61265
> 33 33  99.66001
> 34 34  99.70737
> 35 35  99.75304
> 36 36  99.79702
> 37 37  99.83424
> 38 38  99.86130
> 39 39  99.88836
> 40 40  99.91373
> 41 41  99.93572
> 42 42  99.95264
> 43 43  99.96448
> 44 44  99.97294
> 45 45  99.97970
> 46 46  99.98647
> 47 47  99.99154
> 48 48  99.99493
> 49 49  99.99662
> 50 50  99.99831
> 51 51 100.00000
> 
>>plot(fre$no,fre$leiji)
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ggrothendieck at gmail.com  Fri Jun  2 17:20:40 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 2 Jun 2006 11:20:40 -0400
Subject: [R] Table collumn to single var in lowcase
In-Reply-To: <20060602144421.69541.qmail@web53415.mail.yahoo.com>
References: <20060602144421.69541.qmail@web53415.mail.yahoo.com>
Message-ID: <971536df0606020820i1c70393ekd0dbe2b96fb41385@mail.gmail.com>

Note that creating separate variables out of your data frame is
not really a good idea.  However, to answer your question:

# this puts it on the path
iris.lc <- iris
names(iris.lc) <- tolower(names(iris.lc))
attach(iris.lc)
search() # shows where its located
print(head(sepal.length))
detach() # get rid of it
search() # now its not there


# this puts them right in the global environment
for(nm in names(iris))
	assign(tolower(nm), iris[[nm]], .GlobalEnv)
ls()
print(head(sepal.length))
rm(list = tolower(names(iris))) # remove them

Also note ?with which is probably preferred:

iris.lc <- iris
names(iris.lc) <- tolower(names(iris.lc))
with(iris.lc, {
  print(head(sepal.length))
  print(head(sepal.length+1))
})



On 6/2/06, Milton Cezar <miltinho_astronauta at yahoo.com.br> wrote:
> Dear All,
>
>  I have read a table using
>      DataTABLE <- read.table("mytable.txt, header=T)
>
>  And get the following data structure
>       Var1  VAR2   VaR3  Var4 ...
>
>  How can I list all collumn names (in lowcase) and create variables from table collumns. By hand I do
>      var1 <- DataTABLE$Var1
>      var2 <- DataTABLE$VAR2
>      var3 <- DataTABLE$VaR3
>      var4 <- DataTABLE$Var4
>
>  Unfortunatelly these data come as an output from other program and the data file have about 80 collumns.
>
>  Thanks in advance!
>
>  miltinho
>
>  __________________________________________________
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Fri Jun  2 17:23:04 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 2 Jun 2006 11:23:04 -0400
Subject: [R] Intercept of confidence interval with a constant
In-Reply-To: <200606021052.43908.linux@comjet.com>
References: <200606021052.43908.linux@comjet.com>
Message-ID: <971536df0606020823k27e46cd9h62f0cadd429e4e8d@mail.gmail.com>

Try this:

approx(p[,"lwr"], 1:5, 3)

On 6/2/06, Larry Howe <linux at comjet.com> wrote:
> Hello,
>
> Is there a way for R to determine the point where a confidence interval equals
> a specified value? For example:
>
> x = seq(1:5)
> y = c(5, 5, 4, 4, 3)
> lm = lm(y ~ x)
> p = predict.lm(lm, interval="confidence")
> matplot(p, type="b")
> abline(h = 3)
>
> I want to answer the question: "What is the value of x when the y-value of the
> lower confidence interval is equal to 3.0"? Visually, it is the place on the
> example where the abline intersects the lower confidence interval, or about
> 4.2. Can R calculate this number for me?
>
> I know that predict.lm will calculate a y-value from an x-value, but what I
> want is the opposite. I know the y-value, and I want to calculate the
> x-value.
>
> Larry Howe
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ligges at statistik.uni-dortmund.de  Fri Jun  2 17:35:34 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 02 Jun 2006 17:35:34 +0200
Subject: [R] Intercept of confidence interval with a constant
In-Reply-To: <200606021052.43908.linux@comjet.com>
References: <200606021052.43908.linux@comjet.com>
Message-ID: <44805AC6.3020700@statistik.uni-dortmund.de>

Larry Howe wrote:

> Hello,
> 
> Is there a way for R to determine the point where a confidence interval equals 
> a specified value? For example:
> 
> x = seq(1:5)
> y = c(5, 5, 4, 4, 3)
> lm = lm(y ~ x)
> p = predict.lm(lm, interval="confidence")
> matplot(p, type="b")
> abline(h = 3)


You can do it this way:

  optimize(function(x)
            (predict(lm, newdata = data.frame(x=x),
                     interval = "confidence")[,2] - 3)^2,
           interval=c(1, 5))$minimum

but maybe you are going to solve a "calibration" problem, in fact and a 
completely different kind of confidence interval (namely for the x-axis)?


Uwe Ligges


> I want to answer the question: "What is the value of x when the y-value of the 
> lower confidence interval is equal to 3.0"? Visually, it is the place on the 
> example where the abline intersects the lower confidence interval, or about 
> 4.2. Can R calculate this number for me?
> 
> I know that predict.lm will calculate a y-value from an x-value, but what I 
> want is the opposite. I know the y-value, and I want to calculate the 
> x-value.
> 
> Larry Howe
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From mb.atelier at web.de  Fri Jun  2 17:41:43 2006
From: mb.atelier at web.de (Matthias Braeunig)
Date: Fri, 02 Jun 2006 17:41:43 +0200
Subject: [R] function environment
Message-ID: <44805C37.3090409@web.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi,

how can I automatically access the functions that I loaded into a
separate environment?

> save(A,B,file="myfun.r")
> load("myfun.r",envir=(ENV<-new.env()))
> ls(ENV)
[1] "A" "B"

?"[" turned up that I can access the functions via

> ENV$A
function ()
{
}
> ENV$A()
NULL

Now, how can they be included in the search() path??
attach() as for data.frames does not work...

Furthermore, if I change a functions environment to ENV, why is it not
listed with ls(ENV)?? Instead it still lives in .GlobalEnv

> C<-function(){}
> environment(C)<-ENV
> ls(ENV)
[1] "A" "B"
> C
function(){}
<environment: 0x9cbbb58>
> ENV
<environment: 0x9cbbb58>

Thanks folks!
I enjoy reading and learning from r-help list!

M
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.2.2 (GNU/Linux)

iD8DBQFEgFw2XjamRUP82DkRAooRAJ9sxwERwfXF3l7pssZ081sMC1+nigCgqAPM
OkA1tNJg6MN3l0PQFrwBlIE=
=tGFq
-----END PGP SIGNATURE-----


From ggrothendieck at gmail.com  Fri Jun  2 18:08:38 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 2 Jun 2006 12:08:38 -0400
Subject: [R] function environment
In-Reply-To: <44805C37.3090409@web.de>
References: <44805C37.3090409@web.de>
Message-ID: <971536df0606020908j1ef98c59n54fe8872ea0e6428@mail.gmail.com>

Try this:

attach(as.list(ENV))
search()  # shows it on the path

A function's environment refers to where variables in the function
are looked for if they can't find the variable in the function itself.
The environment in which a function itself is located is entirely
different.

e.g.

a <- 1
e <- new.env()
e$a <- 2
e$fun <- function(x) x*x+a
environment(fun)
e$fun(10) # 101
fun2 <- function(x) x*x + a
environment(fun2) <- e
fun2(10)  # 102

fun is located in environment e but the environment of fun, i.e.
environment(fun), is the .GlobalEnv.

fun2 is located in the .GlobalEnv but the environment of fun2,
i.e. environment(fun2) is e.



On 6/2/06, Matthias Braeunig <mb.atelier at web.de> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Hi,
>
> how can I automatically access the functions that I loaded into a
> separate environment?
>
> > save(A,B,file="myfun.r")
> > load("myfun.r",envir=(ENV<-new.env()))
> > ls(ENV)
> [1] "A" "B"
>
> ?"[" turned up that I can access the functions via
>
> > ENV$A
> function ()
> {
> }
> > ENV$A()
> NULL
>
> Now, how can they be included in the search() path??
> attach() as for data.frames does not work...
>
> Furthermore, if I change a functions environment to ENV, why is it not
> listed with ls(ENV)?? Instead it still lives in .GlobalEnv
>
> > C<-function(){}
> > environment(C)<-ENV
> > ls(ENV)
> [1] "A" "B"
> > C
> function(){}
> <environment: 0x9cbbb58>
> > ENV
> <environment: 0x9cbbb58>
>
> Thanks folks!
> I enjoy reading and learning from r-help list!
>
> M
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.2.2 (GNU/Linux)
>
> iD8DBQFEgFw2XjamRUP82DkRAooRAJ9sxwERwfXF3l7pssZ081sMC1+nigCgqAPM
> OkA1tNJg6MN3l0PQFrwBlIE=
> =tGFq
> -----END PGP SIGNATURE-----
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From gmane0506242.z.cudgel at neverbox.com  Fri Jun  2 18:38:07 2006
From: gmane0506242.z.cudgel at neverbox.com (Frank Samuelson)
Date: Fri, 02 Jun 2006 12:38:07 -0400
Subject: [R] executable file with R
In-Reply-To: <447C6CD0.4030805@ese.u-psud.fr>
References: <447C6CD0.4030805@ese.u-psud.fr>
Message-ID: <e5pphl$vq6$1@sea.gmane.org>

Romain Lorrilliere wrote:
> Hi,
> 
> I made an R function, and I want make an executable applet with it. Do 
> you know how it is possible?
> 


More details about what you want would be helpful.  Here
is what I do and it may be useful to you.
Under *nix,OSX, etc use bash's "here document" feature.
Create a script like the one below and make it executable.

#!/bin/bash
  R --vanilla << "EOF"      #  Pipe all subsequent lines into R.
  ################ Put all your R code here  ###############
  X11(); plot(1:3)
  require(tcltk)
  tkmessageBox(message="hello")
  ###########################end of R code #########################
EOF


Under the M$ XP operating system I attach my code to the end
of the following script and put it in a batch file, like "runthis.bat"
Then just double click (or whatever) on the "runthis.bat" file.


@echo off

:: A batch script for running R scripts in M$ XP
:: Just attach your R script to the end of this batch file (below the
:: last ":::" comment line) and run this script.

:: A portion of this file was taken from Gabor Grothendieck's
:: batchfiles http://cran.r-project.org/contrib/extra/batchfiles/
:: Copyright by Gabor Grothendieck 2005, GPL v2

setlocal
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
:: I could not get this to work on M$ 2k
ver | findstr XP >NUL
if errorlevel 1 echo Warning: This script only works on Windows XP.

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
:: Find R.
:: use environment variable R_HOME if defined
:: else current folder if bin\rcmd.exe exists
:: else most current R as determined by registry entry
:: else error, not found.
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
if not defined R_HOME if exist bin\rcmd.exe set R_HOME=%CD%
if not defined R_HOME for /f "tokens=2*" %%a in (
  'reg query hkcu\software\r-core\r /v InstallPath 2^>NUL ^| findstr InstallPath'
  ) do set R_HOME=%%~b
if not defined R_HOME for /f "tokens=2*" %%a in (
  'reg query hklm\software\r-core\r /v InstallPath 2^>NUL ^| findstr InstallPath'
   ) do set R_HOME=%%~b
if not defined R_HOME echo Error: R not found. Please install R (www.r-project.org). & pause & exit /b

set cmdpath=%R_HOME%\bin\R.exe
set thisfile=%~f0

:: Run R.  Make it parse the commands at the end of this file
echo x=readLines(Sys.getenv('thisfile'));eval(parse(text=x[-(1:grep('Put R code below',x)[2])]))  | "%cmdpath%" --vanilla

endlocal
exit /b

:::::::::::::::::::  Put R code below this line :::::::::::::::::::::

library(tcltk)   # My example R code
windows(); plot(1:3)
tkmessageBox(message="hello")
q()


From John.Kerpel at infores.com  Fri Jun  2 18:46:58 2006
From: John.Kerpel at infores.com (Kerpel, John)
Date: Fri, 2 Jun 2006 11:46:58 -0500
Subject: [R] speed?
Message-ID: <44A8B25381923D4F93B74B2676A50F6D0279C0DA@MAIL1.infores.com>

The benchmark report is good stuff - I've been wondering about these
speed issues recently myself.

Has anyone tried something similar on 64-bit Linux or other OS?  I'm
contemplating switching to 64-bit Linux if I'll get some dramatic cycle
time improvements.  Anyone have any experience with this?

Best,

John

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
Sent: Friday, June 02, 2006 10:01 AM
To: ivo welch; r-help at stat.math.ethz.ch
Subject: Re: [R] speed?

You (and your colleague) might want to have a look at
http://www.sciviews.org/benchmark/.  It's a bit dated,
but still may be a good starting point.

Some months ago some one asked about working on getting
R to use the GPU for computation on the R-devel list.
Don't know if anything came of it.

Cheers,
Andy 

From: ivo welch
> 
> dear R wizards:
> 
> while extolling the virtues of R, one of my young 
> econometrics colleagues told me that he still wants to run ox 
> because [a] his code is written in it (good reason); [b] 
> because ox seems to be faster than R in most benchmarks (huh?).
> 
> this got me to wonder.  language speed can't matter much, so 
> it must be mostly the underlying matrix algebra by now.  I 
> presume that nowadays most of the plain matrix operation 
> speed depends primarily on which hardware features the 
> library accesses.  (The basic algorithms aren't so different, 
> so even though the algorithm may have mattered a long time 
> ago, they are probably pretty similar nowadays. hmmm...maybe 
> matrix inversion still is different, but multiplication and 
> adding should not be.)
> 
> On x86 architecture, I believe there is a hierarchy in terms 
> of raw processing power:  FPU < SSE* < GPU.
> 
> is it even possible to use the GPU now for math processing 
> (linux or windows), and specifically in R?
> 
> assuming I compile everything with the proper SSE flags and atlas, is
> SSE* fully taken advantage of?
> 
> regards,
> 
> /ivo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From ripley at stats.ox.ac.uk  Fri Jun  2 18:48:40 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 2 Jun 2006 17:48:40 +0100 (BST)
Subject: [R] function environment
In-Reply-To: <44805C37.3090409@web.de>
References: <44805C37.3090409@web.de>
Message-ID: <Pine.LNX.4.64.0606021655380.28868@gannet.stats.ox.ac.uk>

On Fri, 2 Jun 2006, Matthias Braeunig wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Hi,
>
> how can I automatically access the functions that I loaded into a
> separate environment?

This is a topic that is probably best suited to R-devel: the explanations 
are going to get technical, and I am not being comprehensive here.

>> save(A,B,file="myfun.r")
>> load("myfun.r",envir=(ENV<-new.env()))
>> ls(ENV)
> [1] "A" "B"
>
> ?"[" turned up that I can access the functions via
>
>> ENV$A
> function ()
> {
> }
>> ENV$A()
> NULL
>
> Now, how can they be included in the search() path??
> attach() as for data.frames does not work...

Actually, it does.  attach() for a list creates an environment and copies 
the elements of a list to it. Try

attach("myfun.r", pos=2)

or more explicitly

env <- attach(NULL pos=2, name="myenv")
load("myfun.r", envir=env)

which both create an environment and unserialize the objects/bindings into 
it.

We have discussed allowing an environment to be given as an argument of 
attach, but the semantics are not clear: should the environment itself be 
attached or a copy (as for a list).  It seems that a copy would be most 
natural.

Since loading a package clearly does load objects into an environment on 
the search path, there are other sneaky games you can play if you know 
what you are doing.


> Furthermore, if I change a functions environment to ENV, why is it not
> listed with ls(ENV)?? Instead it still lives in .GlobalEnv

Because that sets the enclosing environment, not the frame in which the 
symbol is resolved.  There are at least three possible sources of 
confusion here:

1) `environment' gets used in two senses in R documentation, one as a 
frame and one as an environment tree, that is as a frame and its enclosure 
(and so on recursively).

2) Note that objects do not really `live' in environments:  symbols in 
frames have values and an object can be the value of several symbols. 
(People talk about bindings here: there are symbols are bound to objects, 
but objects can be bound to multiple symbols.)

3) Closures (most functions, including all user-written functions) have 
formals, a body, and an environment tree `attached to' or `associated 
with' the function.  This is called the 'environment' of the function, but 
the 'enclosure' might be less confusing (since it acts as the enclosure of 
the frame created for the body of the function when it is evaluated).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From rdpeng at gmail.com  Fri Jun  2 19:03:12 2006
From: rdpeng at gmail.com (Roger D. Peng)
Date: Fri, 02 Jun 2006 13:03:12 -0400
Subject: [R] function environment
In-Reply-To: <44805C37.3090409@web.de>
References: <44805C37.3090409@web.de>
Message-ID: <44806F50.6020407@gmail.com>

Try

save(A, B, file = "myfun.r")
attach("myfun.r")

Your functions will be on the search list.

-roger

Matthias Braeunig wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> 
> Hi,
> 
> how can I automatically access the functions that I loaded into a
> separate environment?
> 
>> save(A,B,file="myfun.r")
>> load("myfun.r",envir=(ENV<-new.env()))
>> ls(ENV)
> [1] "A" "B"
> 
> ?"[" turned up that I can access the functions via
> 
>> ENV$A
> function ()
> {
> }
>> ENV$A()
> NULL
> 
> Now, how can they be included in the search() path??
> attach() as for data.frames does not work...
> 
> Furthermore, if I change a functions environment to ENV, why is it not
> listed with ls(ENV)?? Instead it still lives in .GlobalEnv
> 
>> C<-function(){}
>> environment(C)<-ENV
>> ls(ENV)
> [1] "A" "B"
>> C
> function(){}
> <environment: 0x9cbbb58>
>> ENV
> <environment: 0x9cbbb58>
> 
> Thanks folks!
> I enjoy reading and learning from r-help list!
> 
> M
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.2.2 (GNU/Linux)
> 
> iD8DBQFEgFw2XjamRUP82DkRAooRAJ9sxwERwfXF3l7pssZ081sMC1+nigCgqAPM
> OkA1tNJg6MN3l0PQFrwBlIE=
> =tGFq
> -----END PGP SIGNATURE-----
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/


From rdpeng at gmail.com  Fri Jun  2 19:06:46 2006
From: rdpeng at gmail.com (Roger D. Peng)
Date: Fri, 02 Jun 2006 13:06:46 -0400
Subject: [R] speed?
In-Reply-To: <44A8B25381923D4F93B74B2676A50F6D0279C0DA@MAIL1.infores.com>
References: <44A8B25381923D4F93B74B2676A50F6D0279C0DA@MAIL1.infores.com>
Message-ID: <44807026.2000209@gmail.com>

Running on 64-bit per se does not make things faster.  In fact, from my 
experience it sometimes makes things slower.  The advantage with 64-bit is the 
extra address space for storing things in memory.

Of course, today's 64-bit chips are all faster than recent 32-bit chips, so you 
will get a speed improvement, but only because you're getting a better chip.

-roger

Kerpel, John wrote:
> The benchmark report is good stuff - I've been wondering about these
> speed issues recently myself.
> 
> Has anyone tried something similar on 64-bit Linux or other OS?  I'm
> contemplating switching to 64-bit Linux if I'll get some dramatic cycle
> time improvements.  Anyone have any experience with this?
> 
> Best,
> 
> John
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
> Sent: Friday, June 02, 2006 10:01 AM
> To: ivo welch; r-help at stat.math.ethz.ch
> Subject: Re: [R] speed?
> 
> You (and your colleague) might want to have a look at
> http://www.sciviews.org/benchmark/.  It's a bit dated,
> but still may be a good starting point.
> 
> Some months ago some one asked about working on getting
> R to use the GPU for computation on the R-devel list.
> Don't know if anything came of it.
> 
> Cheers,
> Andy 
> 
> From: ivo welch
>> dear R wizards:
>>
>> while extolling the virtues of R, one of my young 
>> econometrics colleagues told me that he still wants to run ox 
>> because [a] his code is written in it (good reason); [b] 
>> because ox seems to be faster than R in most benchmarks (huh?).
>>
>> this got me to wonder.  language speed can't matter much, so 
>> it must be mostly the underlying matrix algebra by now.  I 
>> presume that nowadays most of the plain matrix operation 
>> speed depends primarily on which hardware features the 
>> library accesses.  (The basic algorithms aren't so different, 
>> so even though the algorithm may have mattered a long time 
>> ago, they are probably pretty similar nowadays. hmmm...maybe 
>> matrix inversion still is different, but multiplication and 
>> adding should not be.)
>>
>> On x86 architecture, I believe there is a hierarchy in terms 
>> of raw processing power:  FPU < SSE* < GPU.
>>
>> is it even possible to use the GPU now for math processing 
>> (linux or windows), and specifically in R?
>>
>> assuming I compile everything with the proper SSE flags and atlas, is
>> SSE* fully taken advantage of?
>>
>> regards,
>>
>> /ivo
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/


From ripley at stats.ox.ac.uk  Fri Jun  2 19:22:33 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 2 Jun 2006 18:22:33 +0100 (BST)
Subject: [R] speed?
In-Reply-To: <44807026.2000209@gmail.com>
References: <44A8B25381923D4F93B74B2676A50F6D0279C0DA@MAIL1.infores.com>
	<44807026.2000209@gmail.com>
Message-ID: <Pine.LNX.4.64.0606021817430.29732@gannet.stats.ox.ac.uk>

On Fri, 2 Jun 2006, Roger D. Peng wrote:

> Running on 64-bit per se does not make things faster.  In fact, from my 
> experience it sometimes makes things slower.  The advantage with 64-bit 
> is the extra address space for storing things in memory.

See the R-admin manual for some additional comments.  Certainly until you 
use more than say 300Mb the extra overhead of 64-bit pointers is pure 
overhead. (At some point nearer the address space limit you do GC a lot 
more often on a 32-bit platform.)

> Of course, today's 64-bit chips are all faster than recent 32-bit chips, 
> so you will get a speed improvement, but only because you're getting a 
> better chip.

Not entirely true right now: some of the Pentium Core/Duo Core 32-bit 
chips are competitive.  (Pentium M chips have often been very fast for 
their nomimal clock speeds.)

> -roger
>
> Kerpel, John wrote:
>> The benchmark report is good stuff - I've been wondering about these
>> speed issues recently myself.
>>
>> Has anyone tried something similar on 64-bit Linux or other OS?  I'm
>> contemplating switching to 64-bit Linux if I'll get some dramatic cycle
>> time improvements.  Anyone have any experience with this?
>>
>> Best,
>>
>> John
>>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
>> Sent: Friday, June 02, 2006 10:01 AM
>> To: ivo welch; r-help at stat.math.ethz.ch
>> Subject: Re: [R] speed?
>>
>> You (and your colleague) might want to have a look at
>> http://www.sciviews.org/benchmark/.  It's a bit dated,
>> but still may be a good starting point.
>>
>> Some months ago some one asked about working on getting
>> R to use the GPU for computation on the R-devel list.
>> Don't know if anything came of it.
>>
>> Cheers,
>> Andy
>>
>> From: ivo welch
>>> dear R wizards:
>>>
>>> while extolling the virtues of R, one of my young
>>> econometrics colleagues told me that he still wants to run ox
>>> because [a] his code is written in it (good reason); [b]
>>> because ox seems to be faster than R in most benchmarks (huh?).
>>>
>>> this got me to wonder.  language speed can't matter much, so
>>> it must be mostly the underlying matrix algebra by now.  I
>>> presume that nowadays most of the plain matrix operation
>>> speed depends primarily on which hardware features the
>>> library accesses.  (The basic algorithms aren't so different,
>>> so even though the algorithm may have mattered a long time
>>> ago, they are probably pretty similar nowadays. hmmm...maybe
>>> matrix inversion still is different, but multiplication and
>>> adding should not be.)
>>>
>>> On x86 architecture, I believe there is a hierarchy in terms
>>> of raw processing power:  FPU < SSE* < GPU.
>>>
>>> is it even possible to use the GPU now for math processing
>>> (linux or windows), and specifically in R?
>>>
>>> assuming I compile everything with the proper SSE flags and atlas, is
>>> SSE* fully taken advantage of?
>>>
>>> regards,
>>>
>>> /ivo
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>>
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From deepayan.sarkar at gmail.com  Fri Jun  2 19:34:29 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 2 Jun 2006 12:34:29 -0500
Subject: [R] how to add point and label to boxplot using bwplot
In-Reply-To: <001501c685d0$5f4fb9b0$8d01a8c0@HSRDCHQOER.VHA01.MED.VA.GOV>
References: <001501c685d0$5f4fb9b0$8d01a8c0@HSRDCHQOER.VHA01.MED.VA.GOV>
Message-ID: <eb555e660606021034x7dd5339akdbee97416dbb7f92@mail.gmail.com>

On 6/1/06, Maria Montez <montez at bu.edu> wrote:
> Hi.
>
> My data contains information for 10 hospitals for 12 different measures.
> Let's call it x1-x12. I need to create a boxplot for each one of this
> measures and put them into one page. Each plot also needs to be independent,
> i.e. cannot use the group feature because of different scales for each
> measure. I was successful using the following code:
>
> x1 <- c(1.317604376, 0.978038233, 0.396823901, 0.601130135, 1.039993474,
> 0.434282156, 0.941604514, 0.84796521, 1.614475852, 2.723760844)
> c1 <- as.integer(c(9, 9, 2, 7, 10, 1, 11, 49, 26, 5))
>
> plot1 <- bwplot(x1,cex=.5,scales=list(cex=0.5), xlab=list("psi03",cex=.5))
> print(plot1,split=c(1,1,1,12),more=TRUE)
>
> #plot2<-bwplot(x2,cex=.5,scales=list(cex=0.5), xlab=list("psi04",cex=.5))
> #print(plot2,split=c(1,2,1,12),more=TRUE)
>
> ... (code for other 10 plots)
>
> The main purpose is to create one of this pages with information relative to
> each hospital. So, on each boxplot I need to add a point indicating where
> that hospital falls with a label that indicates the number of cases
> regarding that variable. Then I create 10 different pages.
>
> I first tried this without using lattice plots by doing:
>
> boxplot(x1, horizontal=TRUE)
> points(x1[4],1,pch=19) #plot point for hospital 2
> text(x1[4],0.95,c1[4]) # next to that point indicate number of cases
>
> How can I do the same in a lattice plot? I've tried the following, but it
> doesn't work (I think I might not be passing the right arguments):
>
> bwplot(x1,cex=.5,scales=list(cex=0.5),tck=0.5, xlab=list("psi03",cex=.5),
> 			panel=function(x,c1){
> 					panel.bwplot(x)
> 					panel.points(x[2],0.5,pch=19)
> 					panel.text(x[4],0.95,c1[4])
> 				}
> 	)

A literal translation would be

bwplot(x1,cex=.5,scales=list(cex=0.5),
       tck=0.5, xlab=list("psi03",cex=.5),
       panel=function(x, y, ...){
           panel.bwplot(x, y, ...)
           panel.points(x1[4], 1, pch=19)
           panel.text(x1[4], 0.95, c1[4])
       })

-Deepayan


From bolker at ufl.edu  Fri Jun  2 19:42:21 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Fri, 2 Jun 2006 17:42:21 +0000 (UTC)
Subject: [R] Problem with mle
References: <448052ED.4070209@krugs.de>
Message-ID: <loom.20060602T193818-751@post.gmane.org>

Rainer M KRug <RMK <at> krugs.de> writes:

> 
> R 2.3.0
> Linux, SuSE 10.0
> 
> Hi
> 
> I have two problems with mle - probably I am using it the wrong way so
> please let me know.
> 
> I want to fit different distributions to an observed count of seeds and
> in the next step use AIC or BIC to identify the best distribution.

library(stats4)
SumSeeds <- c(762, 406, 288, 260, 153, 116,  57,  33,  40,  44,  36,
              24,  35,  23,  36,  25,  35,  30)
X <- c(1.74924, 3.49848, 5.24772, 6.99696, 8.74620, 17.49240, 26.23860,
       34.98480, 43.73100, 52.47720, 61.22340, 69.96960, 71.71880, 73.46810,
       75.21730, 76.96650, 78.71580, 80.46500 )

plot(X,SumSeeds)
curve(7000*dexp(x,0.1),add=TRUE,from=0)
## exponential function (a*exp(-rate*X)) makes more sense
##  than using dexp, which is normalized to 1 (thus makes
## it harder to interpret a as an intercept -- I don't
## know what X is?  (You could easily reverse this decision
## though.)
negexplik <- function(a=7000,rate=0.1) {
  -sum(dpois(SumSeeds,lambda=a*dexp(X,rate),log=TRUE))
}

## start from eyeball-estimated parameters
m1 = mle(minuslogl=negexplik,start=list(a=7000,rate=0.1))
## add estimated curve
with(as.list(coef(m1)),curve(a*dexp(x,rate),col="red",add=TRUE,from=0))

## conclusion so far: the exponential fit is really bad, because
## the numbers don't fall to zero as fast as expected

## changed parameterization of Weibull slightly to agree with
## the parameterization of the exponential above
weiblik <- function(a=7000,shape=1,rate=0.1) {
  -sum(dpois(SumSeeds,lambda=a*dweibull(X,scale=1/rate,shape=shape),
        log=TRUE))
}

weiblik(a=7222,shape=1,rate=0.05) ## check
## start from exponential a/rate estimates, plus shape=1 which reduces
## to exponential
m2 = mle(minuslogl=weiblik,start=list(a=7222,rate=0.05,shape=1))
with(as.list(coef(m2)),curve(a*dweibull(x,scale=1/rate,shape=shape),
     col="blue",add=TRUE,from=0))


as.numeric(pchisq(-2*(logLik(m1)-logLik(m2)),df=1,
      lower.tail=FALSE,log.p=TRUE))
## insanely significant

  I still had some problems profiling.  Various things that may help:
(1) set parscale= option
(2) do fits on the log-parameters (which all have to be positive) OR
  (3) use L-BFGS-B and set lower=0
(4) summary(m1) will give you approximate (Fisher information) confidence
limits even if profiling doesn't work.

  cheers
    Ben Bolker


From rogeriorosas at gmail.com  Fri Jun  2 20:52:14 2006
From: rogeriorosas at gmail.com (=?ISO-8859-1?Q?Rog=E9rio_Rosa_da_Silva?=)
Date: Fri, 2 Jun 2006 15:52:14 -0300
Subject: [R] doubt with integrate ()
Message-ID: <bf159f7c0606021152t2538e14g3f2e9886c117ca09@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060602/f33d2758/attachment.pl 

From mmfuller at tiem.utk.edu  Fri Jun  2 20:54:46 2006
From: mmfuller at tiem.utk.edu (Michael Fuller)
Date: Fri, 2 Jun 2006 14:54:46 -0400
Subject: [R] geoR, plot of variog4 lines incomplete
Message-ID: <14A460EB-9F61-4089-8E3F-5D8DDF420DB5@tiem.utk.edu>

I'm using R for Mac OSX version 1.14 (2129) and the geoR package  
version 1.6-5 (the current version in the R repository). I'm running  
R in OS 10.4.6 on a Mac G4 iBook (933MHz, 640 MB DDR SDRAM). I  
searched the R archive and did not find a posting on this issue.

I want to use the variog and variog4 functions of geoR to  
characterize the pattern of spatial autocorrelation of tree density  
on a forest plot. But I get incomplete line plots when I plot the  
output of variog4, compared to the plot for the output of variog. I'm  
using the same geodata for each plot.

I've attached 2 graphs to illustrate. The first is a plot of a 90- 
degree directional variogram created using variog:
 > trees90.vgm<-variog(trees.geo,max.dist=100,direction=pi/2)
 > plot(trees90.vgm)

The second is a plot of a multidirectional variogram created using  
variog4:
 > trees.vgm4<-variog4(trees.geo,max.dist=100)
 > plot(trees.vgm4)

The graphs are also available at:
www.tiem.utk.edu/~mmfuller/R_plots.pdf

You can see that the 90 degree line in the multi-line plot from  
variog4 (dotted green line) does not include all the data shown in  
the single 90 degree variogram (circles) plotted using variog. The  
lines for other directions in the variog4 plot are also incompletely  
drawn.

Any suggestions as to what's causing this plotting problem?

The raw data is a file containing 3 columns x,y,z. The x,y values are  
the coordinates of quadrats placed end to end in a grid on a forest  
plot. The z values are the density of trees in each quadrat. Each  
quadrat is 15m square and the plot dimensions are 220x780m. The data  
are arranged in columns (coordinates 20 to 240) and rows (coordiantes  
765 to 150) representing the data posting of each quadrat.

I create geodata objects using:
trees.geo<-read.geodata("tree.dat",header=TRUE)

Here is a data sample:
x	y	z
20	765	0.1556
35	765	0.1556
50	765	0.0356
65	765	0.0622
80	765	0.0933
95	765	0.0400
110	765	0.1111
125	765	0.1244
140	765	0.0800
155	765	0.0667
170	765	0.0711
185	765	0.0889
200	765	0.0667
215	765	0.0844
20	750	0.1067
35	750	0.0800
50	750	0.0578
65	750	0.0622
80	750	0.0578
95	750	0.0933
110	750	0.0844
125	750	0.1200
140	750	0.0711
155	750	0.0711
170	750	0.0711
185	750	0.0578
200	750	0.0489
215	750	0.0933
etc

I have repeated the analysis using different quadrat sizes and the  
problem of incomplete drawing of lines is unaffected.

Any suggestions are appreciated!
Mike
-------------- next part --------------
A non-text attachment was scrubbed...
Name: trees.vgm4.pdf
Type: application/pdf
Size: 4894 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060602/62607e55/attachment.pdf 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: trees90.vgm.pdf
Type: application/pdf
Size: 3863 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060602/62607e55/attachment-0001.pdf 
-------------- next part --------------



From arrayprofile at yahoo.com  Fri Jun  2 21:10:04 2006
From: arrayprofile at yahoo.com (array chip)
Date: Fri, 2 Jun 2006 12:10:04 -0700 (PDT)
Subject: [R] plot with different color
Message-ID: <20060602191004.56787.qmail@web35712.mail.mud.yahoo.com>

Hi how can I plot a series of number as a line, but
with lines above a threshould as one color, and with
lines below the threshold as another color. for
example, a numeric vector: rnorm(1:100), and plot
these numbers in the original order, but lines above
the horizontal line at 0 are in red, and lines below
in blue?

Thanks


From guillaume.blanchet.1 at umontreal.ca  Fri Jun  2 21:16:31 2006
From: guillaume.blanchet.1 at umontreal.ca (Guillaume Blanchet)
Date: Fri, 2 Jun 2006 15:16:31 -0400
Subject: [R] presetting R environment
Message-ID: <a06230906c0a63e1919a5@[192.168.0.12]>

Hi,

Is there a way of presetting my R environment so that, for example 
everything between -1e-15 and -1e-15 be equal to 0.

Thanks in advance

Guilaume Blanchet


From xitingy at yahoo.com  Fri Jun  2 21:29:28 2006
From: xitingy at yahoo.com (Cindy Yang)
Date: Fri, 2 Jun 2006 12:29:28 -0700 (PDT)
Subject: [R] ANCOVA in S-plus/R?
Message-ID: <20060602192928.15191.qmail@web30506.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060602/8f6bd6b4/attachment.pl 

From miltinho_astronauta at yahoo.com.br  Fri Jun  2 21:33:31 2006
From: miltinho_astronauta at yahoo.com.br (Milton Cezar)
Date: Fri, 2 Jun 2006 19:33:31 +0000 (GMT)
Subject: [R] Clumpy in image & Landscape Ecology metrics
Message-ID: <20060602193331.5622.qmail@web53415.mail.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060602/e5aa3813/attachment.pl 

From rmh at temple.edu  Fri Jun  2 21:59:30 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Fri,  2 Jun 2006 15:59:30 -0400 (EDT)
Subject: [R] ANCOVA in S-plus/R?
Message-ID: <20060602155930.BBZ90239@po-d.temple.edu>

To get the ANCOVA table you need to look at the anova() function.

The variables T and L must be factors to get the multi-degree-of-freedom
anova tables you are looking for.

Order matters.  You get the same residual, but the sequential sums of 
squares differ.

bt.aov <- aov(E ~ B + T)
anova(bt.aov)
gives you the (t-1)df test of common intercept vs variable intercept.
This is equivalent to comparing the adjusted means.


tb.aov <- aov(E ~ T + B)
anova(tb.aov)
gives you the 1df test of 0 slope (ordinary ANOVA) vs non-zero common slope.

lm() and aov() are essentially the same program.  The default output
is different.


When you bring in the blocking effect of hospital, you will most
likely want that sequentially first.

lbt.aov <- aov(E ~ L + B + T)
anova(lbt.aov)

If you want to test for interaction with the hospital effect, you can use

l.bt.aov <- aov(E ~ L * (B + T))
anova(l.bt.aov)


From Greg.Snow at intermountainmail.org  Fri Jun  2 22:01:30 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 2 Jun 2006 14:01:30 -0600
Subject: [R] plot with different color
Message-ID: <07E228A5BE53C24CAD490193A7381BBB3E48C2@LP-EXCHVS07.CO.IHC.COM>

Here is one approach.  It uses the function clipplot which is shown
below (someday I will add this to my TeachingDemos package, the
TeachingDemos package is required).

An example of usage:

> x <- rnorm(1:100)
> plot(x, type='l',col='red')
> clipplot( lines(x, col='blue'), ylim=c(-4,0) )

Hope this helps,

The function definition is: 

clipplot <- function(fun, xlim=par('usr')[1:2], 
	ylim=par('usr')[3:4] ){
  old.par <- par(c('plt','xpd'))


  if( length(xlim) < 2 ) stop('xlim must be a vector with at least 2
elements')
  if( length(ylim) < 2 ) stop('ylim must be a vector with at least 2
elements')

  if( !require(TeachingDemos) ) stop('TeachingDemos package needed')

  xl <- range(xlim)
  yl <- range(ylim)

  pc <- cnvrt.coords(xl,yl)$fig

  par(plt=c(pc$x,pc$y),xpd=FALSE)

  fun

  par(old.par)
  box() # need to plot something to reset

}  
  


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of array chip
Sent: Friday, June 02, 2006 1:10 PM
To: r-help at stat.math.ethz.ch
Subject: [R] plot with different color

Hi how can I plot a series of number as a line, but with lines above a
threshould as one color, and with lines below the threshold as another
color. for example, a numeric vector: rnorm(1:100), and plot these
numbers in the original order, but lines above the horizontal line at 0
are in red, and lines below in blue?

Thanks

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From Mike.Prager at noaa.gov  Fri Jun  2 22:27:26 2006
From: Mike.Prager at noaa.gov (Michael H. Prager)
Date: Fri, 02 Jun 2006 16:27:26 -0400
Subject: [R] FW: How to create a new package?
In-Reply-To: <005001c685b8$1c94d370$9f7f10ac@gne.windows.gene.com>
References: <005001c685b8$1c94d370$9f7f10ac@gne.windows.gene.com>
Message-ID: <44809F2E.9060203@noaa.gov>

To follow up on Bert Gunter's remark about a "quicker and dirtier" way, 
here is the R script I use to source every *.r file in a directory and 
save the results into a workspace.  It excludes any files beginning with 
"00" (for example, itself) from the operations.  This simplifies 
maintaining a collection of user functions that are kept in a single 
directory and frequently updated.

###############################################################
## 00make.r     M.H.Prager      November 2005
## This clears the current workspace, sources all the commands,
## and then saves the workspace for use by all R sessions

# Clear all existing objects in workspace:
rm(list=ls())

# Make a list of all R source files in this directory:
flist = list.files(path=".", pattern=".+\.r")

# Remove from the list all files containing the string "00":
# Such files should be used for temporary funcs:
flist2 = flist[-grep("00",flist)]

# Source the files:
for (i in 1:length(flist2)) {
   cat("Sourcing", flist2[i],"\n")
   source(flist2[i])
   }
# Remove temporary objects:
rm(i,flist,flist2)
# Save workspace:
save.image()
# Write message to user:
cat("\nNOTE: The workspace has been saved with all functions.\n",
    "     When exiting R, please do NOT save again.\n")
ls()
##################################################################

This can be run quickly from a Windows console with

rterm.exe --no-restore --no-save < 00make.r > 00make.txt

...Mike Prager


on 6/1/2006 4:15 PM Berton Gunter said the following:
> Quicker and dirtier is to simply save a workspace  with your desired
> functions and attach() it automatically (e.g. via .First or otherwise) at
> startup (?Startup for details and options). Of course none of the tools and
> benefits of package management are available, but then I did say quicker and
> dirtier.
>
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>   

-- 
Michael Prager, Ph.D.
Southeast Fisheries Science Center
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
** Opinions expressed are personal, not official.  No
** official endorsement of any product is made or implied.


From linux at comjet.com  Fri Jun  2 22:46:56 2006
From: linux at comjet.com (Larry Howe)
Date: Fri, 2 Jun 2006 16:46:56 -0400
Subject: [R] Intercept of confidence interval with a constant
In-Reply-To: <971536df0606020823k27e46cd9h62f0cadd429e4e8d@mail.gmail.com>
References: <200606021052.43908.linux@comjet.com>
	<971536df0606020823k27e46cd9h62f0cadd429e4e8d@mail.gmail.com>
Message-ID: <200606021646.56398.linux@comjet.com>

Ah, so I switch y with x, give it x, and ask it for y (which is actually x). 
Clever.

I continue to be impressed with R.

Thanks, Larry

On Friday June 2 2006 11:23, Gabor Grothendieck wrote:
> Try this:
>
> approx(p[,"lwr"], 1:5, 3)
>
> On 6/2/06, Larry Howe <linux at comjet.com> wrote:
> > Hello,
> >
> > Is there a way for R to determine the point where a confidence interval
> > equals a specified value? For example:
> >
> > x = seq(1:5)
> > y = c(5, 5, 4, 4, 3)
> > lm = lm(y ~ x)
> > p = predict.lm(lm, interval="confidence")
> > matplot(p, type="b")
> > abline(h = 3)
> >
> > I want to answer the question: "What is the value of x when the y-value
> > of the lower confidence interval is equal to 3.0"? Visually, it is the
> > place on the example where the abline intersects the lower confidence
> > interval, or about 4.2. Can R calculate this number for me?
> >
> > I know that predict.lm will calculate a y-value from an x-value, but what
> > I want is the opposite. I know the y-value, and I want to calculate the
> > x-value.
> >
> > Larry Howe
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html


From ritwik.sinha at gmail.com  Fri Jun  2 23:17:01 2006
From: ritwik.sinha at gmail.com (Ritwik Sinha)
Date: Fri, 2 Jun 2006 17:17:01 -0400
Subject: [R] lm() variance covariance matrix of coefficients.
Message-ID: <42bc98300606021417s4561f337p5837a3c137033741@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060602/f3520b2d/attachment.pl 

From rolf at erdos.math.unb.ca  Fri Jun  2 23:24:57 2006
From: rolf at erdos.math.unb.ca (Rolf Turner)
Date: Fri, 2 Jun 2006 18:24:57 -0300 (ADT)
Subject: [R] lm() variance covariance matrix of coefficients.
Message-ID: <200606022124.k52LOvIh003769@erdos.math.unb.ca>


summary(object)$cov.unscaled


From p.dalgaard at biostat.ku.dk  Fri Jun  2 23:33:52 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Jun 2006 23:33:52 +0200
Subject: [R] lm() variance covariance matrix of coefficients.
In-Reply-To: <200606022124.k52LOvIh003769@erdos.math.unb.ca>
References: <200606022124.k52LOvIh003769@erdos.math.unb.ca>
Message-ID: <x28xof2qf3.fsf@turmalin.kubism.ku.dk>

Rolf Turner <rolf at erdos.math.unb.ca> writes:

> summary(object)$cov.unscaled

You need to multiply that with sigma. However, vcov(object) is easier. 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From Achim.Zeileis at wu-wien.ac.at  Fri Jun  2 23:34:08 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 2 Jun 2006 23:34:08 +0200
Subject: [R] lm() variance covariance matrix of coefficients.
In-Reply-To: <200606022124.k52LOvIh003769@erdos.math.unb.ca>
References: <200606022124.k52LOvIh003769@erdos.math.unb.ca>
Message-ID: <20060602233408.1b3f9adf.Achim.Zeileis@wu-wien.ac.at>

On Fri, 2 Jun 2006 18:24:57 -0300 (ADT) Rolf Turner wrote:

> summary(object)$cov.unscaled

As the name suggests: this is the unscaled covariance matrix!

Use the vcov() extractor method, i.e.,
  vcov(object)
which has methods not only for "lm" but also many other fitted model
objects.
Z


From gunter.berton at gene.com  Fri Jun  2 23:41:11 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 2 Jun 2006 14:41:11 -0700
Subject: [R] lm() variance covariance matrix of coefficients.
In-Reply-To: <200606022124.k52LOvIh003769@erdos.math.unb.ca>
Message-ID: <005101c6868d$43fcf5c0$9f7f10ac@gne.windows.gene.com>

or more simply and better,

vcov(lm.object)

?vcov

Note R's philosophy:use available extractors to get the key features of the
objects, rather then indexing. This is safer, as it does not depend on the
particular structure/implementation, which can change. This is the
difference between "private" and "public" views of an object in oo lingo.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rolf Turner
> Sent: Friday, June 02, 2006 2:25 PM
> To: r-help at stat.math.ethz.ch; ritwik.sinha at gmail.com
> Subject: Re: [R] lm() variance covariance matrix of coefficients.
> 
> 
> summary(object)$cov.unscaled
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From rolf at erdos.math.unb.ca  Fri Jun  2 23:48:06 2006
From: rolf at erdos.math.unb.ca (Rolf Turner)
Date: Fri, 2 Jun 2006 18:48:06 -0300 (ADT)
Subject: [R] lm() variance covariance matrix of coefficients.
Message-ID: <200606022148.k52Lm6OV006757@erdos.math.unb.ca>

Peter Dalgaard wrote:

> Rolf Turner <rolf at erdos.math.unb.ca> writes:
> 
> > summary(object)$cov.unscaled
> 
> You need to multiply that with sigma. However, vcov(object) is easier. 

	Well, I thought unscaled meant unscaled --- the plain
	unvarnished covariance matrix!  I figure that multiplying
	the *covariance* matrix by something would be scaling
	it.  Silly me.

	Also:

	(a) Shouldn't that be ``multiply by sigma^2'' rather
	than by sigma?

	(b) Wouldn't it be helpful to have a pointer (``see also'')
	to vcov() in the help on summary.lm()?

				cheers,

					Rolf Turner
					rolf at math.unb.ca


From paulojus at est.ufpr.br  Sat Jun  3 00:27:03 2006
From: paulojus at est.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Fri, 2 Jun 2006 19:27:03 -0300 (BRT)
Subject: [R] geoR, plot of variog4 lines incomplete
In-Reply-To: <14A460EB-9F61-4089-8E3F-5D8DDF420DB5@tiem.utk.edu>
References: <14A460EB-9F61-4089-8E3F-5D8DDF420DB5@tiem.utk.edu>
Message-ID: <Pine.LNX.4.63.0606021925050.20331@est.ufpr.br>

Mike

The source of the problem is not obvious to me
from what you've reported
and I could ot reproduce in a few tests with other data.

Would it be possoble to send me the data-set?

Cheers
P.J.


On Fri, 2 Jun 2006, Michael Fuller wrote:

> Date: Fri, 2 Jun 2006 14:54:46 -0400
> From: Michael Fuller <mmfuller at tiem.utk.edu>
> To: r-help at stat.math.ethz.ch
> Subject: [R] geoR, plot of variog4 lines incomplete
> 
> I'm using R for Mac OSX version 1.14 (2129) and the geoR package version 
> 1.6-5 (the current version in the R repository). I'm running R in OS 10.4.6 
> on a Mac G4 iBook (933MHz, 640 MB DDR SDRAM). I searched the R archive and 
> did not find a posting on this issue.
>
> I want to use the variog and variog4 functions of geoR to characterize the 
> pattern of spatial autocorrelation of tree density on a forest plot. But I 
> get incomplete line plots when I plot the output of variog4, compared to the 
> plot for the output of variog. I'm using the same geodata for each plot.
>
> I've attached 2 graphs to illustrate. The first is a plot of a 90-degree 
> directional variogram created using variog:
>> trees90.vgm<-variog(trees.geo,max.dist=100,direction=pi/2)
>> plot(trees90.vgm)
>
> The second is a plot of a multidirectional variogram created using variog4:
>> trees.vgm4<-variog4(trees.geo,max.dist=100)
>> plot(trees.vgm4)
>
> The graphs are also available at:
> www.tiem.utk.edu/~mmfuller/R_plots.pdf
>
> You can see that the 90 degree line in the multi-line plot from variog4 
> (dotted green line) does not include all the data shown in the single 90 
> degree variogram (circles) plotted using variog. The lines for other 
> directions in the variog4 plot are also incompletely drawn.
>
> Any suggestions as to what's causing this plotting problem?
>
> The raw data is a file containing 3 columns x,y,z. The x,y values are the 
> coordinates of quadrats placed end to end in a grid on a forest plot. The z 
> values are the density of trees in each quadrat. Each quadrat is 15m square 
> and the plot dimensions are 220x780m. The data are arranged in columns 
> (coordinates 20 to 240) and rows (coordiantes 765 to 150) representing the 
> data posting of each quadrat.
>
> I create geodata objects using:
> trees.geo<-read.geodata("tree.dat",header=TRUE)
>
> Here is a data sample:
> x	y	z
> 20	765	0.1556
> 35	765	0.1556
> 50	765	0.0356
> 65	765	0.0622
> 80	765	0.0933
> 95	765	0.0400
> 110	765	0.1111
> 125	765	0.1244
> 140	765	0.0800
> 155	765	0.0667
> 170	765	0.0711
> 185	765	0.0889
> 200	765	0.0667
> 215	765	0.0844
> 20	750	0.1067
> 35	750	0.0800
> 50	750	0.0578
> 65	750	0.0622
> 80	750	0.0578
> 95	750	0.0933
> 110	750	0.0844
> 125	750	0.1200
> 140	750	0.0711
> 155	750	0.0711
> 170	750	0.0711
> 185	750	0.0578
> 200	750	0.0489
> 215	750	0.0933
> etc
>
> I have repeated the analysis using different quadrat sizes and the problem of 
> incomplete drawing of lines is unaffected.
>
> Any suggestions are appreciated!
> Mike

Paulo Justiniano Ribeiro Jr
LEG (Laborat?rio de Estat?stica e Geoinforma??o)
Departamento de Estat?stica
Universidade Federal do Paran?
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 3361 3573
Fax: (+55) 41 3361 3141
e-mail: paulojus at est.ufpr.br
http://www.est.ufpr.br/~paulojus

From narendra at att.com  Sat Jun  3 01:04:48 2006
From: narendra at att.com (Ravi, Narendra, INFOT)
Date: Fri, 2 Jun 2006 18:04:48 -0500
Subject: [R] "make check" errors when checking R1.9.1 build (was RE:
	Building V2.3.0 on Tru64 V5.1B)
In-Reply-To: <28F05913385EAC43AF019413F674A0170F7D8582@OCCLUST04EVS1.ugd.att.com>
Message-ID: <28F05913385EAC43AF019413F674A0170F7D85C0@OCCLUST04EVS1.ugd.att.com>

 Hi,
	I am attempting to build R on Alpha servers running Tru64 UNIX.
I am having problems building R 2.3.0 so am attempting to build older
releases.

	I have been able to build R 1.9.1 without problem. However "make
check" fails for the following tests:

1. arith.Rout
The difference in the output is as follows:

# diff arith.Rout.save ../../tests/arith.Rout.save
43,46c43,46
<   -4 0.000-0.500i 1+0i 0.000+2.000i -4+0i -0.000-8.000i  16+-0i
<   -3 0.000-0.577i 1+0i 0.000+1.732i -3+0i -0.000-5.196i   9+-0i
<   -2 0.000-0.707i 1+0i 0.000+1.414i -2+0i -0.000-2.828i   4+-0i
<   -1 0.000-1.000i 1+0i 0.000+1.000i -1+0i -0.000-1.000i   1+-0i
---
>   -4 0.000-0.500i 1+0i 0.000+2.000i -4+0i  0.000-8.000i  16+0i
>   -3 0.000-0.577i 1+0i 0.000+1.732i -3+0i  0.000-5.196i   9+0i
>   -2 0.000-0.707i 1+0i 0.000+1.414i -2+0i  0.000-2.828i   4+0i
>   -1 0.000-1.000i 1+0i 0.000+1.000i -1+0i  0.000-1.000i   1+0i
66c66
< [1] -0.0720852+0.6383267i -0.6058296+-0.0000000i -0.0720852-0.6383267i
---
> [1] -0.0720852+0.6383267i -0.6058296+0.0000000i -0.0720852-0.6383267i
135c135
< n= 30 : 465+0i -15+142.7155i -15+70.56945i -15+46.16525i -15+33.69055i
-15+25.98076i -15+20.64573i -15+16.65919i -15+13.50606i -15+10.89814i
-15+8.660254i -15+6.67843i -15+4.873795i -15+3.188348i -15+1.576564i
-15+-0i -15-1.576564i -15-3.188348i -15-4.873795i -15-6.67843i
-15-8.660254i -15-10.89814i -15-13.50606i -15-16.65919i -15-20.64573i
-15-25.98076i -15-33.69055i -15-46.16525i -15-70.56945i -15-142.7155i
---
> n= 30 : 465+0i -15+142.7155i -15+70.56945i -15+46.16525i -15+33.69055i
-15+25.98076i -15+20.64573i -15+16.65919i -15+13.50606i -15+10.89814i
-15+8.660254i -15+6.67843i -15+4.873795i -15+3.188348i -15+1.576564i
-15+0i -15-1.576564i -15-3.188348i -15-4.873795i -15-6.67843i
-15-8.660254i -15-10.89814i -15-13.50606i -15-16.65919i -15-20.64573i
-15-25.98076i -15-33.69055i -15-46.16525i -15-70.56945i -15-142.7155i

2. d-p-q-r-tests.Rout

# diff d-p-q-r-tests.Rout.save ../../tests/d-p-q-r-tests.Rout.save
405c405
< [20,] 40  0.000000e+00   -0.000000e+00           -Inf      -804.608442
---
> [20,] 40  0.000000e+00    0.000000e+00           -Inf      -804.608442


3. Regression Tests:

running regression tests
make[3]: Entering directory
`/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-1.9.1/build/tests'
running code in '../../tests/reg-tests-1.R'
...228814:/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-1.9.1/bui
ld/bin/R.bin: /sbin/loader: Fatal Error: call to unresolved symbol from
/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-1.9.1/build/modules
/lapack.so (pc=0x3ffbfde3528)
make[3]: *** [reg-tests-1.Rout] Error 1


I am a neophyte when it comes to using R and would appreciate any
insight into what may be the problem.

Narendra Ravi

-----Original Message-----
From: Ravi, Narendra, INFOT 
Sent: Friday, May 26, 2006 7:50 PM
To: 'r-help at stat.math.ethz.ch'
Subject: Building V2.3.0 on Tru64 V5.1B

Hi,
	I am going to be attempting a build of R for the alphas (our
main need is to collate and present graphically, performance metrics we
gather on our applications).

We have:

CC=cc, CXX=cxx, Fortran V5.51 (f77, f95), GNU make 3.80, Perl 5.8.4

I am hesitant to rebuild GCC on this platform, and would like to know if
others have success in building R with native C, C++ and Fortran. I plan
on using the following configure string:

CPPFLAGS="-I/opt/gnu/include -I/opt/libjpeg/include
-I/opt/libpng/include -I/opt/ncurses/include" CFLAGS="-std1"
LDFLAGS="-L/opt/gnu/lib -L/opt/libjpeg/lib -L/opt/libpng/lib
-L/opt/ncurses/lib -lncurses -ljpeg -lpng -rpath
"/opt/libpng/lib:/opt/libjpeg/lib:/opt/ncurses/lib:/opt/gnu/lib:/usr/shl
ib" R_PAPERSIZE=letter CC=cc CXX=cxx ./configure prefix=/opt/R
--without-readline --without-blas

I expect to update the config.site file with the above values. If
someone has built R-2.3.0 on Tru64 V5.1B, I would appreciate hearing
how.

Thank you,

Narendra Ravi


From p.dalgaard at biostat.ku.dk  Sat Jun  3 01:34:48 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Jun 2006 01:34:48 +0200
Subject: [R] lm() variance covariance matrix of coefficients.
In-Reply-To: <200606022148.k52Lm6OV006757@erdos.math.unb.ca>
References: <200606022148.k52Lm6OV006757@erdos.math.unb.ca>
Message-ID: <x24pz32ktj.fsf@turmalin.kubism.ku.dk>

Rolf Turner <rolf at erdos.math.unb.ca> writes:

> Peter Dalgaard wrote:
> 
> > Rolf Turner <rolf at erdos.math.unb.ca> writes:
> > 
> > > summary(object)$cov.unscaled
> > 
> > You need to multiply that with sigma. However, vcov(object) is easier. 
> 
> 	Well, I thought unscaled meant unscaled --- the plain
> 	unvarnished covariance matrix!  I figure that multiplying
> 	the *covariance* matrix by something would be scaling
> 	it.  Silly me.

Think (quasi-)binomial glm() and things become clearer. Unscaled
corresponds to a scale factor of 1.
 
> 	Also:
> 
> 	(a) Shouldn't that be ``multiply by sigma^2'' rather
> 	than by sigma?

Yup

> 	(b) Wouldn't it be helpful to have a pointer (``see also'')
> 	to vcov() in the help on summary.lm()?

Well, it *is* in ?lm ...

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From juan_sr at uol.com.br  Sat Jun  3 01:49:22 2006
From: juan_sr at uol.com.br (Juan Santiago Ramseyer)
Date: Fri, 02 Jun 2006 20:49:22 -0300
Subject: [R] merge function in R
In-Reply-To: <20060601125938.18673.qmail@web7613.mail.in.yahoo.com>
References: <20060601125938.18673.qmail@web7613.mail.in.yahoo.com>
Message-ID: <1149292162.3999.7.camel@localhost.localdomain>

Em Qui, 2006-06-01 ?s 05:59 -0700, Ahamarshan jn escreveu:
> hi list,
> 
> This question must be very basic but I am just 3 days
> old to R. I am trying to find a function to merge two
> tables of data in two different files as one.
> 
>  Does merge function only fills in colums between two
> table where data is missing or is there a way that
> merge can be used to merge data between two matrixes
> with common dimensions.
> 
> say i have
> 
>    v1 v2 v3 v4   
> h1
> h2
> h3
> h4
> h5
> 
> and and another table with 
> 
>    x1 x2 x3 x4
> h1
> h2
> h3
> h4
> h5
> 
> 
> can i merge the data as
> 
>    v1 x1 v2 x2 v3 x3 v4 x4
> h1
> h2
> h3
> h4
> h5
> 
> Thanks
> 
hi

testing the following sequence

# matrix examples
x <- seq(from=-1,to=-20,step =-1)
v <- seq(1:20)
dim(x) <- c(5,4)
dim(v) <- c(5,4)

vx <- rep(NA,times=40)
dim(vx) <- c(5,8)

# answer init
for (i in 1:4) {
  vx[,2*i-1] <- v[,i]
  vx[,2*i] <- x[,i]
}
# answer end

Juan Santiago Ramseyer
Eng. Recursos H?dricos


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ehlers at math.ucalgary.ca  Sat Jun  3 02:18:28 2006
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Fri, 02 Jun 2006 18:18:28 -0600
Subject: [R] plot with different color
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB3E48C2@LP-EXCHVS07.CO.IHC.COM>
References: <07E228A5BE53C24CAD490193A7381BBB3E48C2@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <4480D554.1010103@math.ucalgary.ca>

Greg,

It might be better to use ylim = c(min(x), 0)
in place of ylim = c(-4, 0). I don't think plt can take
negative values.

Peter Ehlers

Greg Snow wrote:

> Here is one approach.  It uses the function clipplot which is shown
> below (someday I will add this to my TeachingDemos package, the
> TeachingDemos package is required).
> 
> An example of usage:
> 
> 
>>x <- rnorm(1:100)
>>plot(x, type='l',col='red')
>>clipplot( lines(x, col='blue'), ylim=c(-4,0) )
> 
> 
> Hope this helps,
> 
> The function definition is: 
> 
> clipplot <- function(fun, xlim=par('usr')[1:2], 
> 	ylim=par('usr')[3:4] ){
>   old.par <- par(c('plt','xpd'))
> 
> 
>   if( length(xlim) < 2 ) stop('xlim must be a vector with at least 2
> elements')
>   if( length(ylim) < 2 ) stop('ylim must be a vector with at least 2
> elements')
> 
>   if( !require(TeachingDemos) ) stop('TeachingDemos package needed')
> 
>   xl <- range(xlim)
>   yl <- range(ylim)
> 
>   pc <- cnvrt.coords(xl,yl)$fig
> 
>   par(plt=c(pc$x,pc$y),xpd=FALSE)
> 
>   fun
> 
>   par(old.par)
>   box() # need to plot something to reset
> 
> }  
>   
> 
>


From xprt.wannabe at gmail.com  Sat Jun  3 04:17:59 2006
From: xprt.wannabe at gmail.com (xpRt.wannabe)
Date: Fri, 2 Jun 2006 21:17:59 -0500
Subject: [R] A coding question
In-Reply-To: <447FF8E6.6080209@statistik.uni-dortmund.de>
References: <a4fecdd70606011622o2712b2edm9f57ea7ba364457@mail.gmail.com>
	<447FF8E6.6080209@statistik.uni-dortmund.de>
Message-ID: <a4fecdd70606021917m4ac5c100k3cf887b2a12e6c0@mail.gmail.com>

Uwe and Ben,

Thank you both for your help.

To me, both sets of code seem to do the job and should produce the
same results.  However, as a test I inserted set.seed( ) as follows.
Unless I put set.seed( ) in the wrong lines, the results produced by
both sets of code turn out to be different.  I am baffled.

y <- replicate(10, {
        set.seed(123)
        rp <- rpois(8, 5)
        mysum <- sapply(rp, function(x) {
            set.seed(123)
            x <- rnorm(x)
            x <- x - max(0, x-15) + max(0, x-90)
            sum(x)
        })
     })

##

tmpf <- function() {
set.seed(123)
x <- rnorm(rpois(1,5))
sum(ifelse(x>90,x-75,pmin(x,15)))
}
replicate(10,replicate(8,tmpf()))

On 6/2/06, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> xpRt.wannabe wrote:
>
> > Dear List:
> >
> > I have the follow code:
> >
> > y <- replicate(10,replicate(8,sum(rnorm(rpois(1,5)))))
> >
> > Now I need to apply the following condition to _every_ randomly generated
> > Normal number in the code above:
> >
> > x - max(0,x-15) + max(0,x-90), where x represents the individual Normal
> > numbers.
> >
> > In other words, the said condition needs to be applied before
> > replicate(...(replicate(...(sum(...))) takes place.
> >
> > Any help would be greatly appreciated.
>
>
> y <- replicate(10, {
>         rp <- rpois(8, 5)
>         mysum <- sapply(rp, function(x) {
>             x <- rnorm(x)
>             x <- x - max(0, x-15) + max(0, x-90)
>             sum(x)
>         })
>      })


From helprhelp at gmail.com  Sat Jun  3 05:44:40 2006
From: helprhelp at gmail.com (Weiwei Shi)
Date: Fri, 2 Jun 2006 22:44:40 -0500
Subject: [R] time series clustering
Message-ID: <cdf817830606022044s766247d9o5da6e1442daffecd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060602/b0c6f129/attachment.pl 

From roger.bos at gmail.com  Sat Jun  3 06:11:05 2006
From: roger.bos at gmail.com (roger bos)
Date: Sat, 3 Jun 2006 00:11:05 -0400
Subject: [R] HTML masked (due to Rpad?)
Message-ID: <1db726800606022111n8e0aa1bo4b61ebb245e57095@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060603/220b3ae4/attachment.pl 

From vishal at MIT.EDU  Sat Jun  3 08:24:48 2006
From: vishal at MIT.EDU (Vishal Saxena)
Date: Sat, 03 Jun 2006 02:24:48 -0400
Subject: [R] strucchange package for windows
Message-ID: <6.2.3.4.2.20060603022242.03fd94d0@hesiod>

Hi Achim,

I'd like to try to run the strucchange package on R. However, it 
seems that the package can only run on systems running Debian and not 
Windows XP. Is this true? Or is there a windows version? I downloaded 
the strucchange file but they seem to have the dot deb extension and 
seem to be designed to run with Debian.

If there is a windows version, could you please let me know how to 
obtain and install it? Thanks very much.

Sincerely,
Vishal


From ripley at stats.ox.ac.uk  Sat Jun  3 09:09:08 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 3 Jun 2006 08:09:08 +0100 (BST)
Subject: [R] strucchange package for windows
In-Reply-To: <6.2.3.4.2.20060603022242.03fd94d0@hesiod>
References: <6.2.3.4.2.20060603022242.03fd94d0@hesiod>
Message-ID: <Pine.LNX.4.64.0606030806070.4457@gannet.stats.ox.ac.uk>

On Sat, 3 Jun 2006, Vishal Saxena wrote:

> Hi Achim,
>
> I'd like to try to run the strucchange package on R. However, it
> seems that the package can only run on systems running Debian and not
> Windows XP. Is this true? Or is there a windows version? I downloaded
> the strucchange file but they seem to have the dot deb extension and
> seem to be designed to run with Debian.

We have no idea at all where you are looking, but you are not following 
the instructions in the `R Installation and Administration Manual'.
There are both source and binary Windows versions on CRAN, thanks to the 
kind helpers who prepare the latter for you.

> If there is a windows version, could you please let me know how to
> obtain and install it? Thanks very much.

Use the menus, as described in the FAQs and manuals.  Go to the Packages 
menu. select Install packages ..., select a CRAN mirror and then the 
package.  That's all.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From tjsund at suvitek.no  Sat Jun  3 11:54:55 2006
From: tjsund at suvitek.no (=?iso-8859-1?Q?Tor_J=F8rund_Sund?=)
Date: Sat, 3 Jun 2006 11:54:55 +0200
Subject: [R] Help on plotting a 3 dimensional surface
Message-ID: <200606030954.k539suXq003608@mail47.fg.online.no>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060603/2c81a668/attachment.pl 

From maechler at stat.math.ethz.ch  Sat Jun  3 12:16:51 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 3 Jun 2006 12:16:51 +0200
Subject: [R] presetting R environment
In-Reply-To: <a06230906c0a63e1919a5@[192.168.0.12]>
References: <a06230906c0a63e1919a5@[192.168.0.12]>
Message-ID: <17537.24979.445149.551120@stat.math.ethz.ch>

>>>>> "Guillaume" == Guillaume Blanchet <guillaume.blanchet.1 at umontreal.ca>
>>>>>     on Fri, 2 Jun 2006 15:16:31 -0400 writes:

    Guillaume> Hi, Is there a way of presetting my R environment
    Guillaume> so that, for example everything between -1e-15
    Guillaume> and -1e-15 be equal to 0.

No.  But that would not be a good idea.

I presume that using  zapsmall()  {or even just round()} may
help solve the problem you are having.

    Guillaume> Thanks in advance

    Guillaume> Guilaume Blanchet


From ligges at statistik.uni-dortmund.de  Sat Jun  3 12:23:10 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 03 Jun 2006 12:23:10 +0200
Subject: [R] A coding question
In-Reply-To: <a4fecdd70606021917m4ac5c100k3cf887b2a12e6c0@mail.gmail.com>
References: <a4fecdd70606011622o2712b2edm9f57ea7ba364457@mail.gmail.com>	
	<447FF8E6.6080209@statistik.uni-dortmund.de>
	<a4fecdd70606021917m4ac5c100k3cf887b2a12e6c0@mail.gmail.com>
Message-ID: <4481630E.2@statistik.uni-dortmund.de>

xpRt.wannabe wrote:
> Uwe and Ben,
> 
> Thank you both for your help.
> 
> To me, both sets of code seem to do the job and should produce the
> same results.  However, as a test I inserted set.seed( ) as follows.
> Unless I put set.seed( ) in the wrong lines, the results produced by
> both sets of code turn out to be different.  I am baffled.
> y <- replicate(10, {
>        set.seed(123)

So you reset the RNG 10 times, makes no sense, move set.seed before the 
first replicate().


>        rp <- rpois(8, 5)

You get 8 (!) different values here (in contrast to the code below).

>        mysum <- sapply(rp, function(x) {
>            set.seed(123)

And again. Makes no sense either.

>            x <- rnorm(x)
>            x <- x - max(0, x-15) + max(0, x-90)
>            sum(x)
>        })
>     })
> 
> ##
> 
> tmpf <- function() {
> set.seed(123)

Here, you reset  80 times, each time you get the same value for rpois().

> x <- rnorm(rpois(1,5))
> sum(ifelse(x>90,x-75,pmin(x,15)))
> }
> replicate(10,replicate(8,tmpf()))


You cannot compare the results of both code fragments with set.seed() 
due to the different stucture when which random number is generated.
(even permutation of generation of normal and poisson distributed random 
numbers).

Uwe Ligges


> On 6/2/06, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> 
>> xpRt.wannabe wrote:
>>
>> > Dear List:
>> >
>> > I have the follow code:
>> >
>> > y <- replicate(10,replicate(8,sum(rnorm(rpois(1,5)))))
>> >
>> > Now I need to apply the following condition to _every_ randomly 
>> generated
>> > Normal number in the code above:
>> >
>> > x - max(0,x-15) + max(0,x-90), where x represents the individual Normal
>> > numbers.
>> >
>> > In other words, the said condition needs to be applied before
>> > replicate(...(replicate(...(sum(...))) takes place.
>> >
>> > Any help would be greatly appreciated.
>>
>>
>> y <- replicate(10, {
>>         rp <- rpois(8, 5)
>>         mysum <- sapply(rp, function(x) {
>>             x <- rnorm(x)
>>             x <- x - max(0, x-15) + max(0, x-90)
>>             sum(x)
>>         })
>>      })


From maechler at stat.math.ethz.ch  Sat Jun  3 12:27:49 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 3 Jun 2006 12:27:49 +0200
Subject: [R] function environment
In-Reply-To: <44806F50.6020407@gmail.com>
References: <44805C37.3090409@web.de>
	<44806F50.6020407@gmail.com>
Message-ID: <17537.25637.878251.900900@stat.math.ethz.ch>

>>>>> "Roger" == Roger D Peng <rdpeng at gmail.com>
>>>>>     on Fri, 02 Jun 2006 13:03:12 -0400 writes:

    Roger> Try

    Roger>   save(A, B, file = "myfun.r")
    Roger>   attach("myfun.r")

    Roger> Your functions will be on the search list.

yes.   But do yourself (and your readers/users/..) a favor by
using a different file extension: ".r" is used in place of ".R"
(mainly by people who work on (non-)operating systems that do not
 properly differentiate between lower and upper case) for  R
source files.
Files resulting from save() typically get an extension ".rda"
(or also ".Rdata").

Martin
    Roger> -roger

    Roger> Matthias Braeunig wrote:
    >> -----BEGIN PGP SIGNED MESSAGE-----
    >> Hash: SHA1
    >> 
    >> Hi,
    >> 
    >> how can I automatically access the functions that I loaded into a
    >> separate environment?
    >> 
    >>> save(A,B,file="myfun.r")
    >>> load("myfun.r",envir=(ENV<-new.env()))
    >>> ls(ENV)
    >> [1] "A" "B"
    >> 
    >> ?"[" turned up that I can access the functions via
    >> 
    >>> ENV$A
    >> function ()
    >> {
    >> }
    >>> ENV$A()
    >> NULL
    >> 
    >> Now, how can they be included in the search() path??
    >> attach() as for data.frames does not work...
    >> 
    >> Furthermore, if I change a functions environment to ENV, why is it not
    >> listed with ls(ENV)?? Instead it still lives in .GlobalEnv
    >> 
    >>> C<-function(){}
    >>> environment(C)<-ENV
    >>> ls(ENV)
    >> [1] "A" "B"
    >>> C
    >> function(){}
    >> <environment: 0x9cbbb58>
    >>> ENV
    >> <environment: 0x9cbbb58>
    >> 
    >> Thanks folks!
    >> I enjoy reading and learning from r-help list!
    >> 
    >> M
    >> -----BEGIN PGP SIGNATURE-----
    >> Version: GnuPG v1.4.2.2 (GNU/Linux)
    >> 
    >> iD8DBQFEgFw2XjamRUP82DkRAooRAJ9sxwERwfXF3l7pssZ081sMC1+nigCgqAPM
    >> OkA1tNJg6MN3l0PQFrwBlIE=
    >> =tGFq
    >> -----END PGP SIGNATURE-----
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
    >> 

    Roger> -- 
    Roger> Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/

    Roger> ______________________________________________
    Roger> R-help at stat.math.ethz.ch mailing list
    Roger> https://stat.ethz.ch/mailman/listinfo/r-help
    Roger> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From murdoch at stats.uwo.ca  Sat Jun  3 13:04:23 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 03 Jun 2006 07:04:23 -0400
Subject: [R] HTML masked (due to Rpad?)
In-Reply-To: <1db726800606022111n8e0aa1bo4b61ebb245e57095@mail.gmail.com>
References: <1db726800606022111n8e0aa1bo4b61ebb245e57095@mail.gmail.com>
Message-ID: <44816CB7.7030002@stats.uwo.ca>

On 6/3/2006 12:11 AM, roger bos wrote:
> With an older verion of R (I think 2.2.0) and an older version of Rpad I
> used to use HTML(go, collapse=false) where go is list of objects returned by
> a function and this worked great.  Now that I have done some upgrading (to R
> 2.3.1 and Rpad 1.1.0) its not working right.  I also get a warning when I
> start R that HTML is masked.  And when I do ?HTML R tells me that HTML shows
> up in to places, Rpad and R2HTML.  Is anything wrong with that?  I
> wouldn't think so because R2HTML is a required package for Rpad.  Does
> anyone have any suggestions for me?  TIA, Roger

I'd contact the maintainers of the respective packages.  What it looks 
like is that they have not coordinated their changes, i.e. most likely 
R2HTML added a function named HTML in a recent release.

The maintainers are available via "library(help=pkgname)" (or sometimes 
"package?pkgname" or just "?pkgname").

Duncan Murdoch

> 
> 
> Loading required package: R2HTML
> 
> Attaching package: 'Rpad'
> 
> 
>         The following object(s) are masked from package:R2HTML :
> 
>          HTML
> 
> 
> 
>> ?HTML
> Help on topic 'HTML' was found in the following packages:
> 
>   Package               Library
>   Rpad                  C:/PROGRA~1/R/R-23~1.1/library
>   R2HTML                C:/PROGRA~1/R/R-23~1.1/library
> 
> Using the first match ...
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From murdoch at stats.uwo.ca  Sat Jun  3 13:09:23 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 03 Jun 2006 07:09:23 -0400
Subject: [R] Help on plotting a 3 dimensional surface
In-Reply-To: <200606030954.k539suXq003608@mail47.fg.online.no>
References: <200606030954.k539suXq003608@mail47.fg.online.no>
Message-ID: <44816DE3.30208@stats.uwo.ca>

On 6/3/2006 5:54 AM, Tor J?rund Sund wrote:
> Hi
> 
>  
> 
> Im working on my master thesis, I need to get a plot currents distributed
> over a surface. 
> 
> I have the data listed in a dataframe with the x coordinates, y coordinates
> and the value.
> 
> Im using the persp function but I?ve not found out how to put the data into
> the z1 variable
> 
>  
> 
> es <-read.table("RDatalog_pass_dev_1_wafers_1_8V.txt",header=T,sep="\t")
> 
> x1 <- seq(0,40, length=40)
> 
> y1 <- x1
> 
> z1 <- ? (es[,5]is the column with the data, es[,3] is the x coocdinate, and
> es[,2] is the y coocrdinate]
> 
> persp(x1,y1,z1)
> 
>  
> 
> I will appreciate all help because this will be a nice thing to present in
> my report.

In order to plot a surface, the z values need to be arranged in a 
matrix, with the x and y values labelling rows and columns.  If you have 
a single z value for each (x,y) pair, you can do a 3d scatterplot using 
the scatterplot3d package, or the cloud function in the lattice package. 
  You can also convert them to a surface using interp from the akima 
package, and then they'll work with persp.

Duncan Murdoch


From kuehnik_0505 at gmx-topmail.de  Sat Jun  3 13:24:11 2006
From: kuehnik_0505 at gmx-topmail.de (Niklaus Kuehnis)
Date: Sat, 03 Jun 2006 13:24:11 +0200
Subject: [R] multiple crosstabulation
Message-ID: <4481715B.10508@gmx-topmail.de>

Hi,

Two questions:

1) I have a data.frame of binary factors (x, y, z, ...) and would 
like to do tables like "table(a,b)" for any combination of factors, 
2 by 2. I.e. I'm looking for the equivalent of

table(x,y)
table(x,z)
table(y,z)
...

Is there a simple way to do this?

2) Similar to the above: I have the same data.frame of binary 
factors and would like to do tables like "table(a,b)" for the 
combinations of one factor(x) with all the others, i.e. the 
equivalent of

table(x,y)
table(x,z)
table(x, ...)
table(x, ...)
...

---------------
table(data.frame) gives too many tables, i.e. it gives 2-by-2 tables 
for all the combinations of all the factors. Most of these tables 
contain only zeros. The factor levels are named differently for each 
factor, so apply(data.frame, 2, table) doesn't make sense.

Any help appreciated,
Niklaus


From patrick.giraudoux at univ-fcomte.fr  Sat Jun  3 14:00:44 2006
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sat, 03 Jun 2006 14:00:44 +0200
Subject: [R] default value for cutoff in gstat variogram()
Message-ID: <448179EC.8040601@univ-fcomte.fr>

I wonder what is the default value for the argument 'cutoff' when not 
specified in the variogram.formula function of gstat. Computing 
variogram envelops within gstat, I am comparing the results obtained 
with variog in geoR and variogram in gstat, and it took me a while 
before understanding that the cutoff default value is not the maximum 
distance.

Can Edzer tell us about it?

All the best,

Patrick


From maechler at stat.math.ethz.ch  Sat Jun  3 14:12:12 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 3 Jun 2006 14:12:12 +0200
Subject: [R] How to call a value labels attribute?
In-Reply-To: <3.0.6.32.20060523011721.00ace7c0@pop.gmx.net>
References: <3.0.6.32.20060523011721.00ace7c0@pop.gmx.net>
Message-ID: <17537.31900.786431.148010@stat.math.ethz.ch>

>>>>> "Heinz" == Heinz Tuechler <tuechler at gmx.at>
>>>>>     on Tue, 23 May 2006 01:17:21 +0100 writes:

    Heinz> Dear All, after searching on CRAN I got the
    Heinz> impression that there is no standard way in R to
    Heinz> label values of a numerical variable.  

Hmm, there's  names(.)  and  "names(.) <- .."
Why are those not sufficient?

x <- 1:3
names(x) <- c("apple", "banana", NA)


    Heinz> Since this
    Heinz> would be useful for me I intend to create such an
    Heinz> attribute, at the moment for my personal use.  Still
    Heinz> I would like to choose a name which does not conflict
    Heinz> with names of commonly used attributes.

    Heinz> Would value.labels or vallabs create conflicts?

    Heinz> The attribute should be structured as data.frame with
    Heinz> two columns, levels (numeric) and labels
    Heinz> (character). These could then also be used to
    Heinz> transform from numeric to factor. If the attribute is
    Heinz> copied to the factor variable it could also serve to
    Heinz> retransform the factor to the original numerical
    Heinz> variable.

    Heinz> Comments? Ideas?

    Heinz> Thanks

    Heinz> Heinz T?chler

    Heinz> ______________________________________________
    Heinz> R-help at stat.math.ethz.ch mailing list
    Heinz> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE
    Heinz> do read the posting guide!
    Heinz> http://www.R-project.org/posting-guide.html


From maechler at stat.math.ethz.ch  Sat Jun  3 14:19:39 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 3 Jun 2006 14:19:39 +0200
Subject: [R] standardization of values before call to pam() or clara()
In-Reply-To: <200605221733.48051.dylan.beaudette@gmail.com>
References: <200605221733.48051.dylan.beaudette@gmail.com>
Message-ID: <17537.32347.658748.915587@stat.math.ethz.ch>

>>>>> "Dylan" == Dylan Beaudette <dylan.beaudette at gmail.com>
>>>>>     on Mon, 22 May 2006 17:33:47 -0700 writes:

    Dylan> Greetings, Experimenting with the cluster package,
    Dylan> and am starting to scratch my head in regards to the
    Dylan> *best* way to standardize my data. Both functions can
    Dylan> pre-standardize columns in a dataframe. according to
    Dylan> the manual:

    Dylan> Measurements are standardized for each variable
    Dylan> (column), by subtracting the variable's mean value
    Dylan> and dividing by the variable's mean absolute
    Dylan> deviation.

    Dylan> This works well when input variables are all in the
    Dylan> same units. When I include new variables with a
    Dylan> different intrinsic range, the ones with the largest
    Dylan> relative values tend to be _weighted_ . this is
    Dylan> certainly not surprising, but complicates things.

    Dylan> Does there exist a robust technique to effectively
    Dylan> re-scale each of the variables, regardless of their
    Dylan> intrinsic range to some set range, say from {0,1} ?

    Dylan> I have tried dividing a variable by the maximum value
    Dylan> of that variable, but I am not sure if this is
    Dylan> statistically correct.

A more usual scaling standardization is accomplished by the
function -- guess what? -- scale()

It defaults to standardize to mean 0 and std. 1.
But you can use it as well to do a [0,1] scaling.

Note that you are very wise to think about the importance of
variable scaling / weighting for cluster analysis.
But people have been "here" before, and invented the much more
general notion of a distance/dissimilarity between observational
units.
--> function  daisy() {in "cluster"} or  dist() {from "stats"}
provide such dissimilarity objects.
These can be used as input for  pam() or clara() as well,
and in constructing them you are much more flexible than trying
to find a proper scaling of your x-matrix.

Note that daisy() in particular has been designed for computing
sensible dissimilarities for the case when X-matrix has a
collection of continuous {eg "interval scaled"} and of
categorical (e.g binary) variables.

I recommend you get a textbook on clustering, to read up more on
the subject.

Regards, 
Martin Maechler, ETH Zurich


    Dylan> Any ideas, thoughts would be greatly appreciated.

    Dylan> Cheers,

    Dylan> -- Dylan Beaudette Soils and Biogeochemistry Graduate
    Dylan> Group University of California at Davis 530.754.7341


From rogeriorosas at gmail.com  Sat Jun  3 16:28:30 2006
From: rogeriorosas at gmail.com (=?ISO-8859-1?Q?Rog=E9rio_Rosa_da_Silva?=)
Date: Sat, 3 Jun 2006 11:28:30 -0300
Subject: [R] doubt with integrate ()
Message-ID: <bf159f7c0606030728u493aba0ft156a2eddaffbc8ae@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060603/b856ce2a/attachment.pl 

From Camarda at demogr.mpg.de  Sat Jun  3 17:19:23 2006
From: Camarda at demogr.mpg.de (Camarda, Carlo Giovanni)
Date: Sat, 3 Jun 2006 17:19:23 +0200
Subject: [R] Help on plotting a 3 dimensional surface
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E6016FFDAE@HERMES.demogr.mpg.de>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060603/b765c88a/attachment.pl 

From markleeds at verizon.net  Sat Jun  3 20:44:13 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Sat, 03 Jun 2006 13:44:13 -0500 (CDT)
Subject: [R] warnings messages from R when returning nothing
Message-ID: <1589784.2302991149360253619.JavaMail.root@vms069.mailsrvcs.net>


Hi : I have old Splus code that I am trying to turn into R
( I am using windows Xp and R 2.20 ) and I am getting a warning 
from one of my statements because the behavior of R is different from Splus.

below, tempdata is a matrix of numbers and 
I have the following command which basically runs through the columns and returns the minimum index of the column
where an NA occurs.

badindices<-sapply(tempdata,function(x)min(bdind<-seq(along=x)[is.na(x)])))

The problems seems to be that, if the column doesn't have any NAs,
then I get the following warning : "no finite arguments to min :
returning inf". So, consequently, I get a lot of warnings
anfd -infs.

I dont have Splus right now but from the code I have under
the above command it looks like Splus returned NA rather than -inf and didn't give any warnings.  This would be the the way
I would prefer it to be handled. So, I was
hoping that there is there a way to avoid getting these 
warning and get the behavior that Splus gave me by modifying
the code or setting something in the environment ?
Thanks a lot.


                                           Mark


From murdoch at stats.uwo.ca  Sat Jun  3 21:01:27 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 03 Jun 2006 15:01:27 -0400
Subject: [R] warnings messages from R when returning nothing
In-Reply-To: <1589784.2302991149360253619.JavaMail.root@vms069.mailsrvcs.net>
References: <1589784.2302991149360253619.JavaMail.root@vms069.mailsrvcs.net>
Message-ID: <4481DC87.7050501@stats.uwo.ca>

On 6/3/2006 2:44 PM, markleeds at verizon.net wrote:
> Hi : I have old Splus code that I am trying to turn into R
> ( I am using windows Xp and R 2.20 ) and I am getting a warning 
> from one of my statements because the behavior of R is different from Splus.
> 
> below, tempdata is a matrix of numbers and 
> I have the following command which basically runs through the columns and returns the minimum index of the column
> where an NA occurs.
> 
> badindices<-sapply(tempdata,function(x)min(bdind<-seq(along=x)[is.na(x)])))
> 
> The problems seems to be that, if the column doesn't have any NAs,
> then I get the following warning : "no finite arguments to min :
> returning inf". So, consequently, I get a lot of warnings
> anfd -infs.
> 
> I dont have Splus right now but from the code I have under
> the above command it looks like Splus returned NA rather than -inf and didn't give any warnings.  This would be the the way
> I would prefer it to be handled. So, I was
> hoping that there is there a way to avoid getting these 
> warning and get the behavior that Splus gave me by modifying
> the code or setting something in the environment ?

There are no options to get Splus behaviour here, but
I think you'll get what you want with this code:

badindices<-sapply(tempdata,function(x)(bdind<-which(is.na(x)))[1])

I'm not sure why you're creating the bdind variable; you could get the 
same thing in badindices without that side effect with the simpler

badindices<-sapply(tempdata,function(x) which(is.na(x))[1])

Duncan Murdoch


From JoernBuse at gmx.de  Sat Jun  3 22:17:17 2006
From: JoernBuse at gmx.de (=?iso-8859-1?Q?J=F6rn_Buse?=)
Date: Sat, 03 Jun 2006 22:17:17 +0200
Subject: [R] nested design
Message-ID: <20060603201717.265430@gmx.net>

Hello,

there is a problem to calculate the following model:

model<-aov(Biomass~Beech+Age+Error(Age/Stand))

Warning message:
Error() model is singular in: aov(Biomass ~ Beech + Age + Error(Age/Stand))

The summary output is:


Error: Age
      Df Sum Sq Mean Sq
Beech  1 142671  142671

Error: Age:Stand
          Df Sum Sq Mean Sq F value Pr(>F)
Beech      1   4042    4042  0.1123 0.7416
Residuals 17 611719   35983               

Error: Within
           Df Sum Sq Mean Sq F value Pr(>F)
Beech       1   6244    6244  2.4194 0.1221
Residuals 139 358724    2581   

I'm interested in the Age-effect. What is wrong with this model?
Additional I've tried to compute an lme with 

 model<-lme(Biomass~Age,random=~1|Age/Stand)

 anova(model)
            numDF denDF  F-value p-value
(Intercept)     1   140 61.17626  <.0001
Age             1     0  2.70482     NaN
Warning message:
NaNs produced in: pf(q, df1, df2, lower.tail, log.p) 

This can't be the right way, or? What is wrong?

thanks for your ideas.

J?rn Buse
Lueneburg, Germany

-- 


Der GMX SmartSurfer hilft bis zu 70% Ihrer Onlinekosten zu sparen!
      Ideal f?r Modem und ISDN: http://www.gmx.net/de/go/smartsurfer


From rogeriorosas at gmail.com  Sat Jun  3 22:34:23 2006
From: rogeriorosas at gmail.com (=?ISO-8859-1?Q?Rog=E9rio_Rosa_da_Silva?=)
Date: Sat, 03 Jun 2006 17:34:23 -0300
Subject: [R] apologies (was: doubt with integrate ())
In-Reply-To: <bf159f7c0606030728u493aba0ft156a2eddaffbc8ae@mail.gmail.com>
References: <bf159f7c0606030728u493aba0ft156a2eddaffbc8ae@mail.gmail.com>
Message-ID: <4481F24F.3090306@gmail.com>

Dear members,

I had to post twice the same message. I really apologize for the
inconvenience.

I had trouble with my Thunderbird and needed to post via web mail. Owing
to the same reason, I didn't see in my inbox the email I sent. Only after
sending it again (so I could go on with my issue) I saw in the r help
archive,
by thread, that the message was first posted in 02 Jun 2006.

I'm also sorry for posting HTLM emails and the incomplete code, but I
think that potential helpers wouldn't have patience or time with the
whole thing.

Thank you for your comments,

Rog?rio


From rooff at merkur.math.uni-magdeburg.de  Sat Jun  3 22:43:55 2006
From: rooff at merkur.math.uni-magdeburg.de (Robert Offinger)
Date: Sat, 03 Jun 2006 22:43:55 +0200
Subject: [R] Problem with pointsizes in Graphics
Message-ID: <4481F48B.6090308@imst.math.uni-magdeburg.de>

Hello,

I have a problem with the sizes of points on the screen (Code tested for
some R versions on a Solaris system and also for Linux). Maybe it is
best to start with an example:

If I do a very simple plot using the character '.' for the points, e.g.,

n<-100; plot(0:n,0:n,pch='.',cex=1+(0:n)/20)

then not all for all the points the horizontal and vertical dimensions
are the same: Some points are larger in the vertical dimension, some are
larger in the horizontal dimension. This behaviour doesn't depend on the
pointsize itself but on the specific coordinate of the point, e.g.,

plot(runif(30),runif(30),pch='.',cex=2)

and some points are thicker or higher than others.
So it seems to me that it is not a good idea using pch='.' with
character expansions greater than 1. But other characters have their
problems too, e.g.,

plot(runif(30),runif(30),pch=19,cex=0.1)

and there aren't any points on my screen (for cex=0.2 it depends on the
computer display I am using).

So I would like to have points which are larger than the pixel-point I
get when I use pch='.' with cex=1. So let's say I want point of the size
2x2 pixels (most of the points  with '.' and cex=2 have these
dimensions), but I do not like the effect that some points are larger
than others. On the other hand I am afraid that using pch=19 and a small
cex results in no points.

Any suggestions?

Thanks,
Robert Offinger
OvG-Universit?t Magdeburg


From ritwik.sinha at gmail.com  Sun Jun  4 01:03:52 2006
From: ritwik.sinha at gmail.com (Ritwik Sinha)
Date: Sat, 3 Jun 2006 19:03:52 -0400
Subject: [R] lm() variance covariance matrix of coefficients.
In-Reply-To: <x24pz32ktj.fsf@turmalin.kubism.ku.dk>
References: <200606022148.k52Lm6OV006757@erdos.math.unb.ca>
	<x24pz32ktj.fsf@turmalin.kubism.ku.dk>
Message-ID: <42bc98300606031603g7ea36500obb60c70b93eb9814@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060603/6be500a4/attachment.pl 

From rmh at temple.edu  Sun Jun  4 01:47:56 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Sat,  3 Jun 2006 19:47:56 -0400 (EDT)
Subject: [R] nested design
Message-ID: <20060603194756.BCB06374@po-d.temple.edu>

Your model
model <- aov(Biomass ~ Beech + Age + Error(Age/Stand))

has a redundancy that might be causing the problem.  I can't tell 
without the data.

Try
tree.aov <- aov(Biomass ~ Beech + Age + Error(Stand %in% Age))


A second potential problem is the class of the variables.
>From the degrees of freedom, it looks like Stand is a factor.
Since there is only one df for each of Beech and Age, they might
be incorrectly interpreted as continuous variables instead of factors.
My guess is that they should be multi-degree-of-freedom factors and aren't.

Rich


From markleeds at verizon.net  Sun Jun  4 02:06:02 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Sat, 03 Jun 2006 19:06:02 -0500 (CDT)
Subject: [R] difference in behavior between batch and source
Message-ID: <19626469.2240261149379563175.JavaMail.root@vms073.mailsrvcs.net>

Hi : I am using R 2.20 on windows XP and I have a REALLY
long read.table statement because the col.names argument
has 440 character strings in it. ( I use python to write R code ).
When I run the read.table statement inside an R program
( the R program only consists of the read.table statement )
using the source command in an interactive R session,
everything works fine. But, if I take the same program
and try to run it using R CMD BATCH program name , 
the .Rout file shows me that the execution gets halted at
one of the col.names arguments but there is no error message.
it just says "execution halted".

So, I am thinking that this may have to do with some limit
in R for how long the col.names argument can be ( or
any command for that matter ) ? But, the same program works using the source command and I would think that the source command
still has to read theline in somewhere  ?

What makes me think even more that the problem has something to
do with the length of the command is that, as a test,
I took the tread.table command and copied and pasted it into
an interactive session to see if it would run that way.
What was interesting was that the whole command did not get pasted and the pasting stopped at exactly the same place that the .Rout file showed that it stopped during BATCH mode.

I can't send the command right now because I don't have email at
the location where I am working but I may be able
to send it later ? Any thoughts, suggestions or solutions
would be really appreciated because I need to be able
to get this program to work using BATCH mode.

                                      Thanks a lot.


From ggrothendieck at gmail.com  Sun Jun  4 02:15:04 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 3 Jun 2006 20:15:04 -0400
Subject: [R] difference in behavior between batch and source
In-Reply-To: <19626469.2240261149379563175.JavaMail.root@vms073.mailsrvcs.net>
References: <19626469.2240261149379563175.JavaMail.root@vms073.mailsrvcs.net>
Message-ID: <971536df0606031715w425a4e28of1d3dc5abe646c59@mail.gmail.com>

Maybe:

nm <- c("a", "b", "c")
nm <- c(nm, "d", "e", "f")
...
read.table("myfile.dat", col.names = nm)

or paste the column names into the first line of the data file
using a text editor and use:

read.table("myfile.dat", header = TRUE)


On 6/3/06, markleeds at verizon.net <markleeds at verizon.net> wrote:
> Hi : I am using R 2.20 on windows XP and I have a REALLY
> long read.table statement because the col.names argument
> has 440 character strings in it. ( I use python to write R code ).
> When I run the read.table statement inside an R program
> ( the R program only consists of the read.table statement )
> using the source command in an interactive R session,
> everything works fine. But, if I take the same program
> and try to run it using R CMD BATCH program name ,
> the .Rout file shows me that the execution gets halted at
> one of the col.names arguments but there is no error message.
> it just says "execution halted".
>
> So, I am thinking that this may have to do with some limit
> in R for how long the col.names argument can be ( or
> any command for that matter ) ? But, the same program works using the source command and I would think that the source command
> still has to read theline in somewhere  ?
>
> What makes me think even more that the problem has something to
> do with the length of the command is that, as a test,
> I took the tread.table command and copied and pasted it into
> an interactive session to see if it would run that way.
> What was interesting was that the whole command did not get pasted and the pasting stopped at exactly the same place that the .Rout file showed that it stopped during BATCH mode.
>
> I can't send the command right now because I don't have email at
> the location where I am working but I may be able
> to send it later ? Any thoughts, suggestions or solutions
> would be really appreciated because I need to be able
> to get this program to work using BATCH mode.
>
>                                      Thanks a lot.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ivowel at gmail.com  Sun Jun  4 03:21:02 2006
From: ivowel at gmail.com (ivo welch)
Date: Sat, 3 Jun 2006 21:21:02 -0400
Subject: [R] surprising dates
Message-ID: <50d1c22d0606031821m34f1d4cbk3778c0fc2bf5b7ee@mail.gmail.com>

I wonder if this is an intentional feature or an oversight.  in some
column summaries or in ifelse operations, apparently I am losing the
date property of my vector.

> a <- c(198012, 198101, 198102)
> b <- a*100+31
> c <- as.Date( as.character(b), "%Y%m%d" )

> summary(c)
        Min.      1st Qu.       Median         Mean      3rd Qu.         Max.
"1980-12-31" "1981-01-07" "1981-01-15" "1981-01-15" "1981-01-23" "1981-01-31"
> summary( cbind(1:3, c) )
       V1            c
 Min.   :1.0   Min.   :4017
 1st Qu.:1.5   1st Qu.:4025
 Median :2.0   Median :4032
 Mean   :2.0   Mean   :4032
 3rd Qu.:2.5   3rd Qu.:4040
 Max.   :3.0   Max.   :4048
               NA's   :   1


> d <- a*100+28
> e <- as.Date( as.character(d), "%Y%m%d" )
> e
[1] "1980-12-28" "1981-01-28" "1981-02-28"
> c
[1] "1980-12-31" "1981-01-31" NA
> ifelse( is.na(c), e, c )
[1] 4017 4048 4076   # date property is lost

PS: this time I do not need help.  I can write my code around this.

regards,

/ivo


From luizrodrigotozzi at gmail.com  Sun Jun  4 03:28:46 2006
From: luizrodrigotozzi at gmail.com (Luiz Rodrigo Tozzi)
Date: Sat, 3 Jun 2006 22:28:46 -0300
Subject: [R] Problems using lwd in GDD
Message-ID: <22175d30606031828ua605b4coe8b00f327d03b35b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060603/04f0424b/attachment.pl 

From ggrothendieck at gmail.com  Sun Jun  4 03:34:48 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 3 Jun 2006 21:34:48 -0400
Subject: [R] surprising dates
In-Reply-To: <50d1c22d0606031821m34f1d4cbk3778c0fc2bf5b7ee@mail.gmail.com>
References: <50d1c22d0606031821m34f1d4cbk3778c0fc2bf5b7ee@mail.gmail.com>
Message-ID: <971536df0606031834i4173e125s51b27d1c921bf009@mail.gmail.com>

On 6/3/06, ivo welch <ivowel at gmail.com> wrote:
> I wonder if this is an intentional feature or an oversight.  in some
> column summaries or in ifelse operations, apparently I am losing the
> date property of my vector.
>
> > a <- c(198012, 198101, 198102)
> > b <- a*100+31
> > c <- as.Date( as.character(b), "%Y%m%d" )
>
> > summary(c)
>        Min.      1st Qu.       Median         Mean      3rd Qu.         Max.
> "1980-12-31" "1981-01-07" "1981-01-15" "1981-01-15" "1981-01-23" "1981-01-31"
> > summary( cbind(1:3, c) )
>       V1            c
>  Min.   :1.0   Min.   :4017
>  1st Qu.:1.5   1st Qu.:4025
>  Median :2.0   Median :4032
>  Mean   :2.0   Mean   :4032
>  3rd Qu.:2.5   3rd Qu.:4040
>  Max.   :3.0   Max.   :4048
>               NA's   :   1
>
>
> > d <- a*100+28
> > e <- as.Date( as.character(d), "%Y%m%d" )
> > e
> [1] "1980-12-28" "1981-01-28" "1981-02-28"
> > c
> [1] "1980-12-31" "1981-01-31" NA
> > ifelse( is.na(c), e, c )
> [1] 4017 4048 4076   # date property is lost
>

Note that ifelse says:

     A vector of the same length and attributes (including class) as
     'test'


From ivowel at gmail.com  Sun Jun  4 04:12:23 2006
From: ivowel at gmail.com (ivo welch)
Date: Sat, 3 Jun 2006 22:12:23 -0400
Subject: [R] text bubble (rectangle)?
Message-ID: <50d1c22d0606031912m9e8d7a7i5e9e11ea1554ace5@mail.gmail.com>

Dear R wizards:  sorry to bug everyone twice in one day.

I would like to annotate my graph by putting text strings into
rectangle boxes with a little cartoon-like bubble with a lid pointing
to a specific location.  I can draw some sort of bubble-with-lid using
the R primitives.  (has anyone done something like this already?)

the problem where I am stuck is that the width of the rectangle must
depend on the length and font of my string.  is there a way to obtain
the lower left and upper right location of a text string that R drew?
(this would be good return values for the text() function, wouldn't
it?)

  bubble.annotate <- function( x, y, xlid, ylid, textstring ) { HELP }
  bubble.annotate( 19871010, 20, 19871019, 13, "Black Monday" );

[and thanks, gabor, for explaining the ifelse tricks with dates to me.]

regards,

/iaw


From zwang at scharp.org  Sun Jun  4 07:37:27 2006
From: zwang at scharp.org (Zhu Wang)
Date: Sat,  3 Jun 2006 22:37:27 -0700
Subject: [R] dependencies ERROR during R CMD check
Message-ID: <1149399447.4482719773145@mail.scharp.org>

Dear all,

I was trying to build my own R package cts on LINUX but got the following
"checking package dependencies error" and a warning message. What were wrong?

Thanks,

Zhu  

# R CMD check cts
* checking for working latex ... OK
* using log directory '/home/zwang/R/pkg/cts.Rcheck'
* using Version 2.3.0 (2006-04-24)
* checking for file 'cts/DESCRIPTION' ... OK
* this is package 'cts' version '0.1-3'
* checking package dependencies ... ERROR
Warning message:
no package 'the_name_of_the_package' was found in: packageDescription(p, lib =
lib, fields = pkgFlds, encoding = NA)

The DESCRIPTION file looks like the following:

Package: cts
Title: Not shown
Version: 0.1-3
Author: Name not shown
Maintainer: Zhu
Depends: R (>= 2.3.0)
Description: Not shown 
License: GPL version 2 or newer (see file COPYING)
Packaged: Mon Jun 3 11:24:42 2006


From ripley at stats.ox.ac.uk  Sun Jun  4 08:28:30 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 4 Jun 2006 07:28:30 +0100 (BST)
Subject: [R] documented (was surprising) dates
In-Reply-To: <50d1c22d0606031821m34f1d4cbk3778c0fc2bf5b7ee@mail.gmail.com>
References: <50d1c22d0606031821m34f1d4cbk3778c0fc2bf5b7ee@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0606040718460.21803@gannet.stats.ox.ac.uk>

On Sat, 3 Jun 2006, ivo welch wrote:

> I wonder if this is an intentional feature or an oversight.

These are documented properties of the functions you are using.

> in some column summaries or in ifelse operations, apparently I am losing 
> the date property of my vector.
>
>> a <- c(198012, 198101, 198102)
>> b <- a*100+31
>> c <- as.Date( as.character(b), "%Y%m%d" )
>> summary(c)
>        Min.      1st Qu.       Median         Mean      3rd Qu.         Max.
> "1980-12-31" "1981-01-07" "1981-01-15" "1981-01-15" "1981-01-23" "1981-01-31"
>> summary( cbind(1:3, c) )
>       V1            c
> Min.   :1.0   Min.   :4017
> 1st Qu.:1.5   1st Qu.:4025
> Median :2.0   Median :4032
> Mean   :2.0   Mean   :4032
> 3rd Qu.:2.5   3rd Qu.:4040
> Max.   :3.0   Max.   :4048
>               NA's   :   1

You are creating a matrix from an integer and a date column.  Matrices can
only have one mode, here numeric.  If you meant to create a data frame, 
use data.frame, not cbind.

>> d <- a*100+28
>> e <- as.Date( as.character(d), "%Y%m%d" )
>> e
> [1] "1980-12-28" "1981-01-28" "1981-02-28"
>> c
> [1] "1980-12-31" "1981-01-31" NA
>> ifelse( is.na(c), e, c )
> [1] 4017 4048 4076   # date property is lost

As documented. From ?ifelse:

Value:

      A vector of the same length and attributes (including class) as
      'test' and data values from the values of 'yes' or 'no'.  The mode
      of the answer will be coerced from logical to accommodate first
      any values taken from 'yes' and then any values taken from 'no'.

Note that the class is taken from 'test'.

> PS: this time I do not need help.  I can write my code around this.

Help in pointing you to the posting guide and its recommended reading of 
the help page might still be helpful.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Sun Jun  4 08:39:12 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 4 Jun 2006 07:39:12 +0100 (BST)
Subject: [R] difference in behavior between batch and source
In-Reply-To: <19626469.2240261149379563175.JavaMail.root@vms073.mailsrvcs.net>
References: <19626469.2240261149379563175.JavaMail.root@vms073.mailsrvcs.net>
Message-ID: <Pine.LNX.4.64.0606040729040.21803@gannet.stats.ox.ac.uk>

There is a documented limit of 1000 bytes on R input lines.  You are 
probably hitting it.  Try teaching your Python code generator to wrap 
lines.

NB: input means input from the console or stdin, not to the parser.  The 
front-end is asked for a line of up to 1000 bytes.  Some front-ends may 
supply a longer line, but Rterm.exe will not.

On Sat, 3 Jun 2006, markleeds at verizon.net wrote:

> Hi : I am using R 2.20 on windows XP and I have a REALLY
> long read.table statement because the col.names argument
> has 440 character strings in it. ( I use python to write R code ).
> When I run the read.table statement inside an R program
> ( the R program only consists of the read.table statement )
> using the source command in an interactive R session,
> everything works fine. But, if I take the same program
> and try to run it using R CMD BATCH program name ,
> the .Rout file shows me that the execution gets halted at
> one of the col.names arguments but there is no error message.
> it just says "execution halted".

Syntax errors in batch can do that for you.

> So, I am thinking that this may have to do with some limit
> in R for how long the col.names argument can be ( or
> any command for that matter ) ? But, the same program works using the source command and I would think that the source command
> still has to read theline in somewhere  ?
>
> What makes me think even more that the problem has something to
> do with the length of the command is that, as a test,
> I took the tread.table command and copied and pasted it into
> an interactive session to see if it would run that way.
> What was interesting was that the whole command did not get pasted and the pasting stopped at exactly the same place that the .Rout file showed that it stopped during BATCH mode.
>
> I can't send the command right now because I don't have email at
> the location where I am working but I may be able
> to send it later ? Any thoughts, suggestions or solutions
> would be really appreciated because I need to be able
> to get this program to work using BATCH mode.
>
>                                      Thanks a lot.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From p.dalgaard at biostat.ku.dk  Sun Jun  4 09:43:05 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Jun 2006 09:43:05 +0200
Subject: [R] text bubble (rectangle)?
In-Reply-To: <50d1c22d0606031912m9e8d7a7i5e9e11ea1554ace5@mail.gmail.com>
References: <50d1c22d0606031912m9e8d7a7i5e9e11ea1554ace5@mail.gmail.com>
Message-ID: <x2y7wdcqnq.fsf@turmalin.kubism.ku.dk>

"ivo welch" <ivowel at gmail.com> writes:

> Dear R wizards:  sorry to bug everyone twice in one day.
> 
> I would like to annotate my graph by putting text strings into
> rectangle boxes with a little cartoon-like bubble with a lid pointing
> to a specific location.  I can draw some sort of bubble-with-lid using
> the R primitives.  (has anyone done something like this already?)
> 
> the problem where I am stuck is that the width of the rectangle must
> depend on the length and font of my string.  is there a way to obtain
> the lower left and upper right location of a text string that R drew?
> (this would be good return values for the text() function, wouldn't
> it?)
> 
>   bubble.annotate <- function( x, y, xlid, ylid, textstring ) { HELP }
>   bubble.annotate( 19871010, 20, 19871019, 13, "Black Monday" );
> 
> [and thanks, gabor, for explaining the ifelse tricks with dates to me.]

Have a look at strwidth/strheight.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From bacaro at unisi.it  Sun Jun  4 11:39:12 2006
From: bacaro at unisi.it (Giovanni Bacaro)
Date: Sun, 04 Jun 2006 11:39:12 +0200 (CEST)
Subject: [R] How to use lmer function and multicomp package?
Message-ID: <1121.62.11.9.114.1149413952.squirrel@webmail.unisi.it>

Dear list members,
First of all thank you for your helpful advices.
After your answeres to my firt mail I studied a lot (R-News n?5) and I
tried to perform my analysis:

First, to fit a GLM with a nested design I decided to use the function
"lmer" in package "lme4"
as suggested by Spencer Graves and Filippo Piro.
I remember to you that my data were:

land use classes, 3 levels (fixed factor) = cla (R variable)
plot number, 98 levels each with 4 replicates (random factor within "cla")
= plotti (R variable)

number of species, totally 392 counts, response variable = sp (R variable)

Now, I started my analysis as follow (after the creation of the data.frame
"bacaro"):


mod1<-lmer(sp~cla+(1|cla:plotti), data=bacaro, family=poisson(link=log))

> summary(mod1) #sunto del modello

Generalized linear mixed model fit using PQL
Formula: sp ~ cla + (1 | cla:plotti)
          Data: bacaro
 Family: poisson(log link)

      AIC      BIC    logLik deviance
 451.2908 467.1759 -221.6454 443.2908

Random effects:
 Groups     Name        Variance Std.Dev.
 cla:plotti (Intercept) 0.60496  0.77779
number of obs: 392, groups: cla:plotti, 98

Estimated scale (compare to 1)  0.6709309

Fixed effects:
            Estimate Std. Error z value  Pr(>|z|)
(Intercept)  2.06406    0.14606 14.1316 < 2.2e-16 ***
cla2        -0.59173    0.17695 -3.3440 0.0008257 ***
cla3        -0.74230    0.83244 -0.8917 0.3725454
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
     (Intr) cla2
cla2 -0.825
cla3 -0.175  0.145


> anova(mod1,test="Chsqr")
Analysis of Variance Table
    Df Sum Sq Mean Sq
cla  2 11.352   5.676

Now, my questions are:
1) is the mod1 well specified? Have I said to R that "plotti" is my random
factor and that "plotti" is nested inside "cla" ("cla" as grouping
factor)? (can be  "lmer(sp~cla+(plotti|cla:plotti), data=bacaro,
family=poisson(link=log)" an alternative solution?)
2) Why if I try "lmer(sp~cla+(1|cla:plotti),..." or
"lmer(sp~cla+(1|plotti:cla),...." I obtain the same results?
3) why the anova summary don't say if differences in classes are
significance (or not significance)?
4) I'd like to perform a post-hoc test with the package "multicomp" but
the lmer function give me a lmer object (and this kind of object is not
read by the "multicomp" package). How could I perform my analysis in a
different way?

Thank you a lot for your help!
Giovanni



>I'd like to perform a glm analysis with a hierarchically nested design.
In particular,
>I have one fixed factor ("Land Use Classes") with three levels and a
random factor ("quadrat") nested within Land Use Classes with different
levels per classes (class artificial = 1 quadrat; class crops = 67
quadrats; and class seminatural = 30 quadrats).
>I have four replicates per each quadrats (response variable = species
richness per plot)

>Here some question about:
>1) could I analize these data using the class "artificial" (i.e. I have
only 1 level)?

>2) using R I'd like perfor a glm analysis considering my response
variable (count of species) with a Poisson distribution. How can I
develop my model considering the nested nature of my design? I'm sorry
but I don't know the right package to use.

>3) for the Anova analysis I'd like use a post-hoc comparison between
pairwise classes. What is the right procedure to do this? Is this
analysis performed in R?


-- 
Dr. Giovanni Bacaro
Universit? degli Studi di Siena
Dipartimento di Scienze Ambientali "G. Sarfatti"
Via P.A. Mattioli 4 53100 Siena
tel. 0577 235408
email: bacaro a unisi.it


From bgreen at dyson.brisnet.org.au  Sun Jun  4 12:39:29 2006
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Sun, 04 Jun 2006 20:39:29 +1000
Subject: [R] logistic regression enquiry
Message-ID: <5.1.0.14.0.20060604195510.00c01880@pop3.brisnet.org.au>

I am hoping for some advie regarding the following scenario.

I have data from 26 studies that I wanted to analyse using logistic 
regression. Given the data was from studies and not individuals I was 
unsure how I would perform this in R.  When analysed in SPSS, weighting was 
used so that each study was included twice. Where "use" occurred,  a value 
of 1 was assigned and was weighted by the value for the variable 
"positive". In  the second instance, where use did not occur a value of 0 
was assigned and was weighted by the variable "negative". Taking the first 
study below, use = 1 would be weighted by 28 and use =0, it would be 
weighted by 117 (the total study N =145). If this data is scrambled in 
transmission I can send a csv version.

My questions:

1. is change required to the data format or is modification of the usual 
code required?
2. what is the best R package for performing logistic regression?


Any assistance is much appreciated

regards

Bob Green




In SPSS, each study was included  I weighted the , though am unsure how I 
would format the data


medyear	where	who	dxbroad	firstep	standard	age	sex	positive	negative
89	3	2	1	0	0	31.5	71	28	117.00
98	2	2	1	0	1	48.0	62	15	72.00
98	4	1	1	0	0	45.2	61	42	57.00
89	3	0	1	0	1	28.7	63	19	48.00
99	2	2	1	0	1	34.7	73	27	73.00
88	3	0	1	0	1	30.6	58	26	57.00
94	1	1	1	0	1	36.3	81	70	124.00
96	3	1	1	0	1	40.0	57	27	40.00
96	2	2	1	0	1	33.1	64	9	41.00
88	2	0	1	1	0	29.5	47	30	202.00
98	1	2	0	0	1	39.3	60	246	734.00
97	4	0	0	0	1	38.4	67	17	85.00
92	3	0	1	0	1	34.3	67	15	127.00
88	2	0	1	0	1		46	9	90.00
85	3	0	1	0	1	30.3	64	58	87.00
94	3	0	1	0	1	38.8	47	47	126.00
88	3	0	1	0	1	33.8	54	25	134.00
92	3	0	1	1	1			67	157.00
90	3	0	1	1	1	26.0	52	17	101.00
90	3	0	1	0	0			39	32.00
90	2	0	1	0	1	36.1	38	10	173.00
90	2	0	1	0	1	38.9	53	64	383.00
97	2	0	1	0	1	31.5	61	12	52.00
99	1	1	1	0	1			25	56.00
100	4	1	1	0	1	45.0	62	46	270.00
101	2	0	1	0	1	32.4	100	33	92.00


From jfox at mcmaster.ca  Sun Jun  4 12:59:33 2006
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 04 Jun 2006 06:59:33 -0400
Subject: [R] logistic regression enquiry
In-Reply-To: <5.1.0.14.0.20060604195510.00c01880@pop3.brisnet.org.au>
Message-ID: <web-128264996@cgpsrv2.cis.mcmaster.ca>

Dear Bob,

If I follow this properly, this is just a binomial logistic regression,
where, for each of your 26 observations, you have a certain number of
"successes" (positive instances) and "failures" (negative ones). 

Fitting a binomial logit model is easily accommodated by the glm()
function. Probably the simplest approach is to prepare one observation
for each study which has the number of successes and failures for that
study, and put these together as the columns of a two-column matrix on
the left-hand side of the model; on the right-hand-side you'd have the
usual specification for a model formula. Thus, something like

model <- glm(cbind(successes, failures) ~ ..., family=binomial,
data=Dataset)

(where the ... represents the RHS of your model) should do the trick.

I hope this helps,
 John

On Sun, 04 Jun 2006 20:39:29 +1000
 Bob Green <bgreen at dyson.brisnet.org.au> wrote:
> I am hoping for some advie regarding the following scenario.
> 
> I have data from 26 studies that I wanted to analyse using logistic 
> regression. Given the data was from studies and not individuals I was
> 
> unsure how I would perform this in R.  When analysed in SPSS,
> weighting was 
> used so that each study was included twice. Where "use" occurred,  a
> value 
> of 1 was assigned and was weighted by the value for the variable 
> "positive". In  the second instance, where use did not occur a value
> of 0 
> was assigned and was weighted by the variable "negative". Taking the
> first 
> study below, use = 1 would be weighted by 28 and use =0, it would be 
> weighted by 117 (the total study N =145). If this data is scrambled
> in 
> transmission I can send a csv version.
> 
> My questions:
> 
> 1. is change required to the data format or is modification of the
> usual 
> code required?
> 2. what is the best R package for performing logistic regression?
> 
> 
> Any assistance is much appreciated
> 
> regards
> 
> Bob Green
> 
> 
> 
> 
> In SPSS, each study was included  I weighted the , though am unsure
> how I 
> would format the data
> 
> 
> medyear	where	who	dxbroad	firstep	standard	age	sex	positive	negative
> 89	3	2	1	0	0	31.5	71	28	117.00
> 98	2	2	1	0	1	48.0	62	15	72.00
> 98	4	1	1	0	0	45.2	61	42	57.00
> 89	3	0	1	0	1	28.7	63	19	48.00
> 99	2	2	1	0	1	34.7	73	27	73.00
> 88	3	0	1	0	1	30.6	58	26	57.00
> 94	1	1	1	0	1	36.3	81	70	124.00
> 96	3	1	1	0	1	40.0	57	27	40.00
> 96	2	2	1	0	1	33.1	64	9	41.00
> 88	2	0	1	1	0	29.5	47	30	202.00
> 98	1	2	0	0	1	39.3	60	246	734.00
> 97	4	0	0	0	1	38.4	67	17	85.00
> 92	3	0	1	0	1	34.3	67	15	127.00
> 88	2	0	1	0	1		46	9	90.00
> 85	3	0	1	0	1	30.3	64	58	87.00
> 94	3	0	1	0	1	38.8	47	47	126.00
> 88	3	0	1	0	1	33.8	54	25	134.00
> 92	3	0	1	1	1			67	157.00
> 90	3	0	1	1	1	26.0	52	17	101.00
> 90	3	0	1	0	0			39	32.00
> 90	2	0	1	0	1	36.1	38	10	173.00
> 90	2	0	1	0	1	38.9	53	64	383.00
> 97	2	0	1	0	1	31.5	61	12	52.00
> 99	1	1	1	0	1			25	56.00
> 100	4	1	1	0	1	45.0	62	46	270.00
> 101	2	0	1	0	1	32.4	100	33	92.00
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/


From tuechler at gmx.at  Sun Jun  4 14:07:22 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Sun, 04 Jun 2006 13:07:22 +0100
Subject: [R] How to call a value labels attribute?
In-Reply-To: <17537.31900.786431.148010@stat.math.ethz.ch>
References: <3.0.6.32.20060523011721.00ace7c0@pop.gmx.net>
	<3.0.6.32.20060523011721.00ace7c0@pop.gmx.net>
Message-ID: <3.0.6.32.20060604130722.00a61e20@pop.gmx.net>

At 14:12 03.06.2006 +0200, Martin Maechler wrote:
>>>>>> "Heinz" == Heinz Tuechler <tuechler at gmx.at>
>>>>>>     on Tue, 23 May 2006 01:17:21 +0100 writes:
>
>    Heinz> Dear All, after searching on CRAN I got the
>    Heinz> impression that there is no standard way in R to
>    Heinz> label values of a numerical variable.  
>
>Hmm, there's  names(.)  and  "names(.) <- .."
>Why are those not sufficient?
>
>x <- 1:3
>names(x) <- c("apple", "banana", NA)

Martin,

I will considere this. For now I am using an attribute value.labels and a
corresponding class to preserve this and other attributes after inclusion
in a data.frame and indexing/subsetting, but using names should do as well.
My idea was more like defining a set of value labels for a variable and
apply it to all the variable, as e.g. in the following _pseudocode_:

### not run
### pseudocode
x <- c(1, 2, 3, 3, 2, 3, 1)
value.labels(x) <- c(apple=1, banana=2, NA=3)
x
### desired result
apple banana  NA  NA banana NA apple
    1      2   3   3      2  3     1

value.labels(x) <- c(Apfel=1, Banane=2, Birne=3) # redefine labels
x
### desired result
Apfel Banane Birne Birne Banane Birne Apfel
    1      2     3     3      2     3     1

value.labels(x) # inspect labels
### desired result
Apfel Banane Birne
    1      2     3

These value.labels should persist even after inclusion in a data.frame and
after indexing/subsetting.
I did not yet try your idea concerning these aspects, but I will do it. My
final goal is to do all the data handling on numerically coded variables
and to transform to factors "on the fly" when needed for statistical
procedures. Given the presence of value.labels a factor function could use
them for the conversion.

I described my motivation for all this in a previous post, titled:
How to represent a metric categorical variable?
There was no response at all and I wonder, if this is such a rare problem.

Thanks,
Heinz

>
>
>    Heinz> Since this
>    Heinz> would be useful for me I intend to create such an
>    Heinz> attribute, at the moment for my personal use.  Still
>    Heinz> I would like to choose a name which does not conflict
>    Heinz> with names of commonly used attributes.
>
>    Heinz> Would value.labels or vallabs create conflicts?
>
>    Heinz> The attribute should be structured as data.frame with
>    Heinz> two columns, levels (numeric) and labels
>    Heinz> (character). These could then also be used to
>    Heinz> transform from numeric to factor. If the attribute is
>    Heinz> copied to the factor variable it could also serve to
>    Heinz> retransform the factor to the original numerical
>    Heinz> variable.
>
>    Heinz> Comments? Ideas?
>
>    Heinz> Thanks
>
>    Heinz> Heinz T?chler
>
>    Heinz> ______________________________________________
>    Heinz> R-help at stat.math.ethz.ch mailing list
>    Heinz> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE
>    Heinz> do read the posting guide!
>    Heinz> http://www.R-project.org/posting-guide.html
>
>


From HDoran at air.org  Sun Jun  4 13:38:35 2006
From: HDoran at air.org (Doran, Harold)
Date: Sun, 4 Jun 2006 07:38:35 -0400
Subject: [R] How to use lmer function and multicomp package?
Message-ID: <2323A6D37908A847A7C32F1E3662C80E055E88@dc1ex01.air.org>

 
Comments below:

> mod1<-lmer(sp~cla+(1|cla:plotti), data=bacaro, 
> family=poisson(link=log))
> 
> > summary(mod1) #sunto del modello
> 
> Generalized linear mixed model fit using PQL
> Formula: sp ~ cla + (1 | cla:plotti)
>           Data: bacaro
>  Family: poisson(log link)
> 
>       AIC      BIC    logLik deviance
>  451.2908 467.1759 -221.6454 443.2908
> 
> Random effects:
>  Groups     Name        Variance Std.Dev.
>  cla:plotti (Intercept) 0.60496  0.77779 number of obs: 392, 
> groups: cla:plotti, 98
> 
> Estimated scale (compare to 1)  0.6709309
> 
> Fixed effects:
>             Estimate Std. Error z value  Pr(>|z|)
> (Intercept)  2.06406    0.14606 14.1316 < 2.2e-16 ***
> cla2        -0.59173    0.17695 -3.3440 0.0008257 ***
> cla3        -0.74230    0.83244 -0.8917 0.3725454
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> Correlation of Fixed Effects:
>      (Intr) cla2
> cla2 -0.825
> cla3 -0.175  0.145
> 
> 
> > anova(mod1,test="Chsqr")
> Analysis of Variance Table
>     Df Sum Sq Mean Sq
> cla  2 11.352   5.676
> 
> Now, my questions are:
> 1) is the mod1 well specified? Have I said to R that "plotti" 
> is my random factor and that "plotti" is nested inside "cla" 
> ("cla" as grouping factor)? 

Yes, you have plotti as a random factor nested in cla

(can be  
> "lmer(sp~cla+(plotti|cla:plotti), data=bacaro, 
> family=poisson(link=log)" an alternative solution?)
> 2) Why if I try "lmer(sp~cla+(1|cla:plotti),..." or 
> "lmer(sp~cla+(1|plotti:cla),...." I obtain the same results?

Because this call is communative, so either way does not matter

> 3) why the anova summary don't say if differences in classes 
> are significance (or not significance)?

See a recent post by Doug Bates at http://finzi.psych.upenn.edu/R/Rhelp02a/archive/76742.html

> 4) I'd like to perform a post-hoc test with the package 
> "multicomp" but the lmer function give me a lmer object (and 
> this kind of object is not read by the "multicomp" package). 
> How could I perform my analysis in a different way?
> 
> Thank you a lot for your help!
> Giovanni
> 
> 
> 
> >I'd like to perform a glm analysis with a hierarchically 
> nested design.
> In particular,
> >I have one fixed factor ("Land Use Classes") with three levels and a
> random factor ("quadrat") nested within Land Use Classes with 
> different levels per classes (class artificial = 1 quadrat; 
> class crops = 67 quadrats; and class seminatural = 30 quadrats).
> >I have four replicates per each quadrats (response variable = species
> richness per plot)
> 
> >Here some question about:
> >1) could I analize these data using the class "artificial" 
> (i.e. I have
> only 1 level)?
> 
> >2) using R I'd like perfor a glm analysis considering my response
> variable (count of species) with a Poisson distribution. How 
> can I develop my model considering the nested nature of my 
> design? I'm sorry but I don't know the right package to use.
> 
> >3) for the Anova analysis I'd like use a post-hoc comparison between
> pairwise classes. What is the right procedure to do this? Is 
> this analysis performed in R?
> 
> 
> --
> Dr. Giovanni Bacaro
> Universit? degli Studi di Siena
> Dipartimento di Scienze Ambientali "G. Sarfatti"
> Via P.A. Mattioli 4 53100 Siena
> tel. 0577 235408
> email: bacaro at unisi.it
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From Dimitris.Rizopoulos at med.kuleuven.be  Sun Jun  4 17:01:28 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Sun, 04 Jun 2006 17:01:28 +0200
Subject: [R] How to call a value labels attribute?
In-Reply-To: <3.0.6.32.20060604130722.00a61e20@pop.gmx.net>
References: <3.0.6.32.20060523011721.00ace7c0@pop.gmx.net>
	<3.0.6.32.20060523011721.00ace7c0@pop.gmx.net>
	<3.0.6.32.20060604130722.00a61e20@pop.gmx.net>
Message-ID: <1149433288.4482f5c808fb2@webmail1.kuleuven.be>

maybe you could consider something like the following:

varlabs <- function(x){
    if (is.null(names(x))) NULL else x[!duplicated(x)]
}
"varlabs<-" <- function(x, value){
    names(x) <- names(value[x])
    x
}
###############
x <- c(1, 2, 3, 3, 2, 3, 1)
x
varlabs(x)
varlabs(x) <- c(apple=1, banana=2, "NA"=3)
x
varlabs(x)
varlabs(x) <- c(Apfel=1, Banane=2, Birne=3)
x
varlabs(x)


I hope it helps.

Best,
Dimitris

---- 
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting Heinz Tuechler <tuechler at gmx.at>:

> At 14:12 03.06.2006 +0200, Martin Maechler wrote:
> >>>>>> "Heinz" == Heinz Tuechler <tuechler at gmx.at>
> >>>>>>     on Tue, 23 May 2006 01:17:21 +0100 writes:
> >
> >    Heinz> Dear All, after searching on CRAN I got the
> >    Heinz> impression that there is no standard way in R to
> >    Heinz> label values of a numerical variable.  
> >
> >Hmm, there's  names(.)  and  "names(.) <- .."
> >Why are those not sufficient?
> >
> >x <- 1:3
> >names(x) <- c("apple", "banana", NA)
> 
> Martin,
> 
> I will considere this. For now I am using an attribute value.labels
> and a
> corresponding class to preserve this and other attributes after
> inclusion
> in a data.frame and indexing/subsetting, but using names should do as
> well.
> My idea was more like defining a set of value labels for a variable
> and
> apply it to all the variable, as e.g. in the following _pseudocode_:
> 
> ### not run
> ### pseudocode
> x <- c(1, 2, 3, 3, 2, 3, 1)
> value.labels(x) <- c(apple=1, banana=2, NA=3)
> x
> ### desired result
> apple banana  NA  NA banana NA apple
>     1      2   3   3      2  3     1
> 
> value.labels(x) <- c(Apfel=1, Banane=2, Birne=3) # redefine labels
> x
> ### desired result
> Apfel Banane Birne Birne Banane Birne Apfel
>     1      2     3     3      2     3     1
> 
> value.labels(x) # inspect labels
> ### desired result
> Apfel Banane Birne
>     1      2     3
> 
> These value.labels should persist even after inclusion in a
> data.frame and
> after indexing/subsetting.
> I did not yet try your idea concerning these aspects, but I will do
> it. My
> final goal is to do all the data handling on numerically coded
> variables
> and to transform to factors "on the fly" when needed for statistical
> procedures. Given the presence of value.labels a factor function
> could use
> them for the conversion.
> 
> I described my motivation for all this in a previous post, titled:
> How to represent a metric categorical variable?
> There was no response at all and I wonder, if this is such a rare
> problem.
> 
> Thanks,
> Heinz
> 
> >
> >
> >    Heinz> Since this
> >    Heinz> would be useful for me I intend to create such an
> >    Heinz> attribute, at the moment for my personal use.  Still
> >    Heinz> I would like to choose a name which does not conflict
> >    Heinz> with names of commonly used attributes.
> >
> >    Heinz> Would value.labels or vallabs create conflicts?
> >
> >    Heinz> The attribute should be structured as data.frame with
> >    Heinz> two columns, levels (numeric) and labels
> >    Heinz> (character). These could then also be used to
> >    Heinz> transform from numeric to factor. If the attribute is
> >    Heinz> copied to the factor variable it could also serve to
> >    Heinz> retransform the factor to the original numerical
> >    Heinz> variable.
> >
> >    Heinz> Comments? Ideas?
> >
> >    Heinz> Thanks
> >
> >    Heinz> Heinz T?chler
> >
> >    Heinz> ______________________________________________
> >    Heinz> R-help at stat.math.ethz.ch mailing list
> >    Heinz> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE
> >    Heinz> do read the posting guide!
> >    Heinz> http://www.R-project.org/posting-guide.html
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From rooff at merkur.math.uni-magdeburg.de  Sun Jun  4 17:23:53 2006
From: rooff at merkur.math.uni-magdeburg.de (Robert Offinger)
Date: Sun, 04 Jun 2006 17:23:53 +0200
Subject: [R] slanted ends of horizontal lines for certain line widths
Message-ID: <4482FB09.3080804@imst.math.uni-magdeburg.de>

Hello,

if I plot a horizontal line, e.g.,

  plot(c(1,2),c(1,1),xlim=c(0,3),lwd=2,type="l")

or

  plot(c(1,2),c(1,1),xlim=c(0,3),lwd=4,type="l")

then the left end (1st example) or both ends (2nd example) of the lines
are not rectangular but slanted on the graphical display (screen).
That behavour first occurred when I was trying to plot a stepfun, e.g.,

y <- round(rnorm(12),1)
Fn12 <- ecdf(y)
plot(Fn12,verticals= FALSE, do.p = FALSE,lwd=2,col.h="red")
plot(Fn12,verticals= FALSE, do.p = FALSE,lwd=4,col.h="red")

(lwd=3 seems okay)

Bug or feature?
Suggestions? (I don't like the slanted ends but lwd=2 would be better
than lwd=3)

Robert Offinger
OvG-Universit?t Magdeburg

----
platform       sparc-sun-solaris2.9
arch           sparc
os             solaris2.9
system         sparc, solaris2.9
status
major          2
minor          3.1
year           2006
month          06
day            01
svn rev        38247
language       R
version.string Version 2.3.1 (2006-06-01)


From luizrodrigotozzi at gmail.com  Sun Jun  4 17:35:43 2006
From: luizrodrigotozzi at gmail.com (Luiz Rodrigo Tozzi)
Date: Sun, 4 Jun 2006 12:35:43 -0300
Subject: [R] Problems using lwd in GDD
In-Reply-To: <22175d30606031828ua605b4coe8b00f327d03b35b@mail.gmail.com>
References: <22175d30606031828ua605b4coe8b00f327d03b35b@mail.gmail.com>
Message-ID: <22175d30606040835v6eb43a9cqa6d283e7bdca1ccd@mail.gmail.com>

hi

I'm using the GDD package (in a 64bits fedora machine using R 2.3.0)
to save in a png file some plots and i noticed that changing the lwd
parameter does not change my line width

I tried the same script in a Windows based R ( 2.2.1 r36812), using no
GDD, and it worked.

Does anybody has a clue?

My testing script is something dumb like this:

library(GDD)
GDD("pres24.png", type="png8", width = 700, height = 500)
par(lty=3,lwd=7,cex.axis=0.65,lab=c(12,12,0),mar=c(2.5, 2.5, 2.5, 2.5))
plot(rnorm(100),type="line",lty=3,lwd=7)
mtext("Rodada ",side=3,padj=-0.33,cex=1,font=c(4))
dev.off()

in windows i take off the initial two lines, of course...

thanks in advanced

-- 
><> ><> ><> ><> ><> ><> ><> ><> ><>
     Luiz Rodrigo Lins Tozzi
   luizrodrigotozzi em gmail.com

><> ><> ><> ><> ><> ><> ><> ><> ><>
    http://luizrodrigotozzi.multiply.com/
    http://www.flogao.com.br/luizrodrigotozzi


From backer at psych.uib.no  Sun Jun  4 17:40:06 2006
From: backer at psych.uib.no (Tom Backer Johnsen)
Date: Sun, 04 Jun 2006 17:40:06 +0200
Subject: [R] Significance test, Cronbach's Alpha
In-Reply-To: <4481F48B.6090308@imst.math.uni-magdeburg.de>
References: <4481F48B.6090308@imst.math.uni-magdeburg.de>
Message-ID: <4482FED6.1030209@psych.uib.no>

Hello:

I am reading a paper at the moment where the author reports on a 
significance test of Cronbach's Alpha.  I did not know that it is 
possible.  Is it?  If so, how?

Tom
+----------------------------------------------------------------+
| Tom Backer Johnsen, Psychometrics Unit,  Faculty of Psychology |
| University of Bergen, Christies gt. 12, N-5015 Bergen,  NORWAY |
| Tel : +47-5558-9185                        Fax : +47-5558-9879 |
| Email : backer at psych.uib.no    URL : http://www.galton.uib.no/ |
+----------------------------------------------------------------+


From Charles.Annis at StatisticalEngineering.com  Sun Jun  4 17:43:23 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Sun, 4 Jun 2006 11:43:23 -0400
Subject: [R] slanted ends of horizontal lines for certain line widths
In-Reply-To: <4482FB09.3080804@imst.math.uni-magdeburg.de>
Message-ID: <009001c687ed$9d5a7d70$6600a8c0@DD4XFW31>

I think you want to change par()$lend

Type 

par()  <enter>

to see the defaults.

In your situation you might want to begin with something like

dev.off()
par(mar=c(4,6,4,5)+0.1, lend=2)
... then your plotting logic ...

Where the "mar" argument adjusts the margins.  (You can omit this if you're
happy with the defaults.)  And the line end, lend=2, makes the ends square.

Best wishes.

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robert Offinger
Sent: Sunday, June 04, 2006 11:24 AM
To: r-help at stat.math.ethz.ch
Subject: [R] slanted ends of horizontal lines for certain line widths

Hello,

if I plot a horizontal line, e.g.,

  plot(c(1,2),c(1,1),xlim=c(0,3),lwd=2,type="l")

or

  plot(c(1,2),c(1,1),xlim=c(0,3),lwd=4,type="l")

then the left end (1st example) or both ends (2nd example) of the lines
are not rectangular but slanted on the graphical display (screen).
That behavour first occurred when I was trying to plot a stepfun, e.g.,

y <- round(rnorm(12),1)
Fn12 <- ecdf(y)
plot(Fn12,verticals= FALSE, do.p = FALSE,lwd=2,col.h="red")
plot(Fn12,verticals= FALSE, do.p = FALSE,lwd=4,col.h="red")

(lwd=3 seems okay)

Bug or feature?
Suggestions? (I don't like the slanted ends but lwd=2 would be better
than lwd=3)

Robert Offinger
OvG-Universit?t Magdeburg

----
platform       sparc-sun-solaris2.9
arch           sparc
os             solaris2.9
system         sparc, solaris2.9
status
major          2
minor          3.1
year           2006
month          06
day            01
svn rev        38247
language       R
version.string Version 2.3.1 (2006-06-01)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From Dimitris.Rizopoulos at med.kuleuven.be  Sun Jun  4 17:53:14 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Sun, 04 Jun 2006 17:53:14 +0200
Subject: [R] slanted ends of horizontal lines for certain line widths
In-Reply-To: <4482FB09.3080804@imst.math.uni-magdeburg.de>
References: <4482FB09.3080804@imst.math.uni-magdeburg.de>
Message-ID: <1149436394.448301eaac096@webmail1.kuleuven.be>

probably you want to look at the `lend' argument of ?par, e.g.,

op <- par(lend = 2)
plot(c(1,2), c(1,1), xlim = c(0,3), lwd = 20, type = "l")
par(op)

I hope it helps.

Best,
Dimitris

---- 
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting Robert Offinger <rooff at merkur.math.uni-magdeburg.de>:

> Hello,
> 
> if I plot a horizontal line, e.g.,
> 
>   plot(c(1,2),c(1,1),xlim=c(0,3),lwd=2,type="l")
> 
> or
> 
>   plot(c(1,2),c(1,1),xlim=c(0,3),lwd=4,type="l")
> 
> then the left end (1st example) or both ends (2nd example) of the
> lines
> are not rectangular but slanted on the graphical display (screen).
> That behavour first occurred when I was trying to plot a stepfun,
> e.g.,
> 
> y <- round(rnorm(12),1)
> Fn12 <- ecdf(y)
> plot(Fn12,verticals= FALSE, do.p = FALSE,lwd=2,col.h="red")
> plot(Fn12,verticals= FALSE, do.p = FALSE,lwd=4,col.h="red")
> 
> (lwd=3 seems okay)
> 
> Bug or feature?
> Suggestions? (I don't like the slanted ends but lwd=2 would be
> better
> than lwd=3)
> 
> Robert Offinger
> OvG-Universit?t Magdeburg
> 
> ----
> platform       sparc-sun-solaris2.9
> arch           sparc
> os             solaris2.9
> system         sparc, solaris2.9
> status
> major          2
> minor          3.1
> year           2006
> month          06
> day            01
> svn rev        38247
> language       R
> version.string Version 2.3.1 (2006-06-01)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From rmh at temple.edu  Sun Jun  4 18:12:54 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Sun,  4 Jun 2006 12:12:54 -0400 (EDT)
Subject: [R] Fwd: Re:  How to call a value labels attribute?
Message-ID: <20060604121254.BCB62686@po-d.temple.edu>

How is what you are doing any different from factors?


> x <- factor(c(1, 2, 3, 3, 2, 3, 1), labels=c("apple", "banana", "other"))
> x
[1] apple  banana other  other  banana other  apple 
Levels: apple banana other
> as.numeric(x)
[1] 1 2 3 3 2 3 1
> levels(x)[3] <- "birne"
> x
[1] apple  banana birne  birne  banana birne  apple 
Levels: apple banana birne
> 


---- Original message ----
>### not run
>### pseudocode
>x <- c(1, 2, 3, 3, 2, 3, 1)
>value.labels(x) <- c(apple=1, banana=2, NA=3)
>x
>### desired result
>apple banana  NA  NA banana NA apple
>    1      2   3   3      2  3     1
>
>value.labels(x) <- c(Apfel=1, Banane=2, Birne=3) # redefine labels
>x
>### desired result
>Apfel Banane Birne Birne Banane Birne Apfel
>    1      2     3     3      2     3     1
>
>value.labels(x) # inspect labels
>### desired result
>Apfel Banane Birne
>    1      2     3


From antonio.fabio at gmail.com  Sun Jun  4 18:42:35 2006
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Sun, 4 Jun 2006 18:42:35 +0200
Subject: [R] strange (to me) ncolumns default value in 'write'
Message-ID: <b0808fdc0606040942l3001fc41v44234e04ab5d3fb7@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060604/a8854663/attachment.pl 

From rooff at merkur.math.uni-magdeburg.de  Sun Jun  4 19:10:15 2006
From: rooff at merkur.math.uni-magdeburg.de (Robert Offinger)
Date: Sun, 04 Jun 2006 19:10:15 +0200
Subject: [R] slanted ends of horizontal lines for certain line widths
In-Reply-To: <1149436394.448301eaac096@webmail1.kuleuven.be>
References: <4482FB09.3080804@imst.math.uni-magdeburg.de>
	<1149436394.448301eaac096@webmail1.kuleuven.be>
Message-ID: <448313F7.1040108@imst.math.uni-magdeburg.de>

Thanks a lot!

That was exactly what I was looking for.
By the way, I see that since R version 2.3.0 that can be specified
inline, i.e.,

plot(c(1,2), c(1,1), xlim=c(0,3), lwd=10, type="l", lend=2)

So no need for par()...

Dimitrios Rizopoulos wrote:
> probably you want to look at the `lend' argument of ?par, e.g.,
> 
> op <- par(lend = 2)
> plot(c(1,2), c(1,1), xlim = c(0,3), lwd = 20, type = "l")
> par(op)
> 
> I hope it helps.
> 
> Best,
> Dimitris
> 
> ---- 
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://med.kuleuven.be/biostat/
>      http://www.student.kuleuven.be/~m0390867/dimitris.htm
> 
> 
> Quoting Robert Offinger <rooff at merkur.math.uni-magdeburg.de>:
> 
>> Hello,
>>
>> if I plot a horizontal line, e.g.,
>>
>>   plot(c(1,2),c(1,1),xlim=c(0,3),lwd=2,type="l")
>>
>> or
>>
>>   plot(c(1,2),c(1,1),xlim=c(0,3),lwd=4,type="l")
>>
>> then the left end (1st example) or both ends (2nd example) of the
>> lines
>> are not rectangular but slanted on the graphical display (screen).
>> That behavour first occurred when I was trying to plot a stepfun,
>> e.g.,
>>
>> y <- round(rnorm(12),1)
>> Fn12 <- ecdf(y)
>> plot(Fn12,verticals= FALSE, do.p = FALSE,lwd=2,col.h="red")
>> plot(Fn12,verticals= FALSE, do.p = FALSE,lwd=4,col.h="red")
>>
>> (lwd=3 seems okay)
>>
>> Bug or feature?
>> Suggestions? (I don't like the slanted ends but lwd=2 would be
>> better
>> than lwd=3)
>>
>> Robert Offinger
>> OvG-Universit?t Magdeburg
>>
>> ----
>> platform       sparc-sun-solaris2.9
>> arch           sparc
>> os             solaris2.9
>> system         sparc, solaris2.9
>> status
>> major          2
>> minor          3.1
>> year           2006
>> month          06
>> day            01
>> svn rev        38247
>> language       R
>> version.string Version 2.3.1 (2006-06-01)
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>


From spencer.graves at pdf.com  Sun Jun  4 19:35:06 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 04 Jun 2006 10:35:06 -0700
Subject: [R] why does arima returns "NAN" standard error?
In-Reply-To: <b1f16d9d0606010043m117f5288sb42eff00a51ee2a1@mail.gmail.com>
References: <b1f16d9d0606010043m117f5288sb42eff00a51ee2a1@mail.gmail.com>
Message-ID: <448319CA.6030809@pdf.com>

	  Your question does not include a simple, self-contained example (as 
requested in the posting guide! "www.R-project.org/posting-guide.html"). 
  Without that, I don't even know where to start.  However, the model 
appears to be overparameterized and NOT invertible.  Have you made "acf" 
and "pacf" plots?

	  Beyond that, have you studied the time series chapter in Venables and 
Ripley (2002) Modern Applied Statistics with S (Springer)?  If no, I 
suggest you do that before you do much else.  After doing that, if you 
still have a question for this list, please submit another post, 
preferably including a simple, self-contained example.

	  Hope this helps,  	
	  Spencer Graves	

Michael wrote:
> Hi everyone,
> 
> 
> -----------------------------
> 
> Coefficients:
> 
>          ar1      ar2      ma1     ma2    sar1  intercept   drift
> 
>       1.5283  -0.7189  -1.9971  0.9999  0.3982     0.0288  -9e-04
> 
> s.e.  0.0869   0.0835   0.0627  0.0627  0.1305        NaN     NaN
> 
> 
> 
> sigma^2 estimated as 0.04383:  log likelihood = 4.34,  aic = 7.32
> 
> Warning message:
> 
> NaNs produced in: sqrt(diag(object$var.coef))
> 
>  -------------------------------------------
> 
> 
> What does this mean?
> 
> 
> Thanks a lot!
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From renaud.lancelot at gmail.com  Sun Jun  4 19:53:54 2006
From: renaud.lancelot at gmail.com (Renaud Lancelot)
Date: Sun, 4 Jun 2006 19:53:54 +0200
Subject: [R] evaluation of the alternative expression in ifelse
Message-ID: <c2ee56800606041053s455e3ee2k1414fbbf91657591@mail.gmail.com>

Dear all,

I am trying to avoid the warnings produced by:

> x <- -2:2
> log(x)
[1]       NaN       NaN      -Inf 0.0000000 0.6931472
Warning message:
production de NaN in: log(x)

I thought that using ifelse would be a solution, but it is not the case:

> ifelse(test = x < 0, yes = NaN, no = log(x))
[1]       NaN       NaN      -Inf 0.0000000 0.6931472
Warning message:
production de NaN in: log(x)

I am aware of the section "Warning" of the help page for ifelse:
Sometimes it is better to use a construction such as (tmp <- yes;
tmp[!test] <- no[!test]; tmp), possibly extended to handle missing
values in test.

However, is there a way to avoid the evaluation of the alternative
expression in ifelse when the argument test is false ?

Best regards,

Renaud
-- 
Renaud LANCELOT
D?partement Elevage et M?decine V?t?rinaire (EMVT) du CIRAD
Directeur adjoint charg? des affaires scientifiques

CIRAD, Animal Production and Veterinary Medicine Department
Deputy director for scientific affairs

Campus international de Baillarguet
TA 30 / B (B?t. B, Bur. 214)
34398 Montpellier Cedex 5 - France
T?l   +33 (0)4 67 59 37 17
Secr. +33 (0)4 67 59 39 04
Fax   +33 (0)4 67 59 37 95


From Dimitris.Rizopoulos at med.kuleuven.be  Sun Jun  4 20:15:37 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Sun, 04 Jun 2006 20:15:37 +0200
Subject: [R] evaluation of the alternative expression in ifelse
In-Reply-To: <c2ee56800606041053s455e3ee2k1414fbbf91657591@mail.gmail.com>
References: <c2ee56800606041053s455e3ee2k1414fbbf91657591@mail.gmail.com>
Message-ID: <1149444937.44832349de810@webmail1.kuleuven.be>

Quoting Renaud Lancelot <renaud.lancelot at gmail.com>:

> Dear all,
> 
> I am trying to avoid the warnings produced by:
> 
> > x <- -2:2
> > log(x)
> [1]       NaN       NaN      -Inf 0.0000000 0.6931472
> Warning message:
> production de NaN in: log(x)
> 
> I thought that using ifelse would be a solution, but it is not the
> case:
> 
> > ifelse(test = x < 0, yes = NaN, no = log(x))
> [1]       NaN       NaN      -Inf 0.0000000 0.6931472
> Warning message:
> production de NaN in: log(x)
> 
> I am aware of the section "Warning" of the help page for ifelse:
> Sometimes it is better to use a construction such as (tmp <- yes;
> tmp[!test] <- no[!test]; tmp), possibly extended to handle missing
> values in test.
> 
> However, is there a way to avoid the evaluation of the alternative
> expression in ifelse when the argument test is false ?

I think the answer is no; the Details section of ifelse() reads: 

"... 'yes' will be evaluated if and only if *any* element of test is 
true, and analogously for 'no'." 

check also the code behind ifelse(). If you really want to use ifelse
(), then you could consider something like:

x <- -2:2
opt <- options(warn = -1)
ifelse(test = x < 0, yes = NaN, no = log(x))
options(opt)


I hope it helps.

Best,
Dimitris


> Best regards,
> 
> Renaud
> -- 
> Renaud LANCELOT
> D?partement Elevage et M?decine V?t?rinaire (EMVT) du CIRAD
> Directeur adjoint charg? des affaires scientifiques
> 
> CIRAD, Animal Production and Veterinary Medicine Department
> Deputy director for scientific affairs
> 
> Campus international de Baillarguet
> TA 30 / B (B?t. B, Bur. 214)
> 34398 Montpellier Cedex 5 - France
> T?l   +33 (0)4 67 59 37 17
> Secr. +33 (0)4 67 59 39 04
> Fax   +33 (0)4 67 59 37 95
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ggrothendieck at gmail.com  Sun Jun  4 20:15:40 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 4 Jun 2006 14:15:40 -0400
Subject: [R] evaluation of the alternative expression in ifelse
In-Reply-To: <c2ee56800606041053s455e3ee2k1414fbbf91657591@mail.gmail.com>
References: <c2ee56800606041053s455e3ee2k1414fbbf91657591@mail.gmail.com>
Message-ID: <971536df0606041115h4cd5bc60h2e4b06981d435aa@mail.gmail.com>

Try:

log(ifelse(x < 0, NaN, x))

or

suppressWarnings(ifelse(x < 0, NaN, log(x)))

or

ifelse(x < 0, NaN, log(pmax(0, x)))



On 6/4/06, Renaud Lancelot <renaud.lancelot at gmail.com> wrote:
> Dear all,
>
> I am trying to avoid the warnings produced by:
>
> > x <- -2:2
> > log(x)
> [1]       NaN       NaN      -Inf 0.0000000 0.6931472
> Warning message:
> production de NaN in: log(x)
>
> I thought that using ifelse would be a solution, but it is not the case:
>
> > ifelse(test = x < 0, yes = NaN, no = log(x))
> [1]       NaN       NaN      -Inf 0.0000000 0.6931472
> Warning message:
> production de NaN in: log(x)
>
> I am aware of the section "Warning" of the help page for ifelse:
> Sometimes it is better to use a construction such as (tmp <- yes;
> tmp[!test] <- no[!test]; tmp), possibly extended to handle missing
> values in test.
>
> However, is there a way to avoid the evaluation of the alternative
> expression in ifelse when the argument test is false ?
>
> Best regards,
>
> Renaud
> --
> Renaud LANCELOT
> D?partement Elevage et M?decine V?t?rinaire (EMVT) du CIRAD
> Directeur adjoint charg? des affaires scientifiques
>
> CIRAD, Animal Production and Veterinary Medicine Department
> Deputy director for scientific affairs
>
> Campus international de Baillarguet
> TA 30 / B (B?t. B, Bur. 214)
> 34398 Montpellier Cedex 5 - France
> T?l   +33 (0)4 67 59 37 17
> Secr. +33 (0)4 67 59 39 04
> Fax   +33 (0)4 67 59 37 95
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From spencer.graves at pdf.com  Sun Jun  4 20:49:48 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 04 Jun 2006 11:49:48 -0700
Subject: [R] understanding the verbose output in nlme
In-Reply-To: <011301c68583$5e2daee0$6f179e89@UCTPCGREGD>
References: <011301c68583$5e2daee0$6f179e89@UCTPCGREGD>
Message-ID: <44832B4C.1090904@pdf.com>

	  I don't know, but if it were my question, I think I could find
out by making local copies of the functions involved and stepping
through the algorithm line by line using "debug" (see, e.g.,
"http://finzi.psych.upenn.edu/R/Rhelp02a/archive/68215.html").

	  Have you read Pinheiro and Bates (2000) Mixed-Effects Models
in S and S-Plus?  If no, I encourage you to do so.  Over the past 4
years or so, I've probably spent more time with this book and referred 
more people to it than any other.  Doug Bates is a leading original 
contributor in this area, and I believe you will find this book well 
worth your money and  your time.

	  Regarding "the numbers under subjectno1-6", I'm guessing that
these may be the current estimates of the random effects for the first 6 
of the 103 subjects.  The purpose of "verbose" is NOT to dump everything
but only enough to help you evaluate whether the algorithm seems to be
converging.

	  hope this helps.
	  Spencer Graves

Greg Distiller wrote:
> Hi
> I have found some postings referring to the fact that one can try and 
> understand why a particular model is failing to solve/converge from the 
> verbose output one can generate when fitting a nonlinear mixed model. I am 
> trying to understand this output and have not been able to find out much:
> 
> **Iteration 1
> LME step: Loglik: -237.4517 , nlm iterations: 22
> reStruct  parameters:
>   subjectno1   subjectno2   subjectno3   subjectno4   subjectno5 
> subjectno6
>  -0.87239181   2.75772772  -0.72892919 -10.36636391   0.55290322 
> 0.09878685
> 
> PNLS step: RSS =  60.50164
>  fixed effects:2.59129  0.00741764  0.57155
>  iterations: 7
> 
> Convergence:
>    fixed reStruct
> 5.740688 2.159285
> 
> I know that the Loglik must refer to the value of the log likelihood 
> function, that the values after "fixed effects" are the parameter estimates, 
> and that the bit after Convergence obviously has something to so with the 
> convergence criteria for the fixed effects and the random effects structure. 
> I did manage to find a posting where somebody said that the restruct 
> parameter is the log of the relative precision of the random effects? The 
> one thing that is a bit confusing to me is that it appears as if the fixed 
> effects convergence must be zero (or close to it) as one would expect but in 
> one of my converged models the output showed a restruct value of 0.72 ?
> 
> 
> 
> Then I have no idea what the numbers under subjectno1-6 are, especially as I 
> have 103 subjects in the data!
> 
> 
> 
> Can anyone help shed some light on this output and how it can be used to 
> diagnose issues with a model?
> 
> 
> 
> Many thanks
> 
> 
> 
> Greg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From spencer.graves at pdf.com  Sun Jun  4 22:02:26 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 04 Jun 2006 13:02:26 -0700
Subject: [R] help with syntax of nlme function
In-Reply-To: <002a01c6858d$c605a720$b61ad284@BIO041>
References: <002a01c6858d$c605a720$b61ad284@BIO041>
Message-ID: <44833C52.7030701@pdf.com>

	  I haven't seen a reply to this, so I will offer a few comments.  I'm 
not sufficiently familiar with nlme to answer your question directly, 
but I hope my comments might help.

	  First, have you studied Pinheiro and Bates (2000) Mixed-Effects 
Models in S and S-Plus (Springer)?  If no, I highly recommend this book 
for you.  It has three chapters with many examples on nonlinear 
mixed-effects models.  The nlme package is the product of Doug Bates and 
many of his graduate students, including Jose Pinheiro.  For over a 
quarter century, Doug Bates has been a leading, original contributor to 
the theory and practice of nonlinear and mixed-effects modeling.  I was 
unable to get much out of lme (let alone nlme) until I read this book. 
If you are not familiar with this book, I believe it will introduce you 
to many "helper" functions that will make it easier for you to produce 
plots as well as p-values, etc., for your model.  This in turn should 
help you get more information from your data and be able to describe 
your results more persuasively and interestingly to your audience.

	  Also, can you check the directory on your computer where R is 
installed?  It should have a subdirectory "library" that includes a 
further subfolder "nlme".  The "script" folder in "nlme" contains script 
files containing R commands to produce most of the computer output in 
the book.  Ch. 7 is a lot of math that you may wish to skip.  However, 
if you work through the script files while reading chapters 6 and 8, you 
will hopefully find examples that are sufficiently close to your current 
application that you will be able to find and fix the problem.

	  If that fails, try to find a simple, self-contained reproducible 
example that also displays your problem.  With luck, you should be able 
to generate such by modifying one of the examples in the book, by 
reducing the size of the data set used or misspelling a name or 
something.  Then submit another post containing this example.

	  I'm sorry I can't give you a quicker answer.

	  Hope this helps,
	  Spencer Graves

Bill Shipley wrote:
> I am having difficulty understanding the syntax of the nlme() function for
> nonlinear mixed models.  The data frame is called Marouane.chlorophyll.  The
> model involves a dependent variable (Absorb) and an independent variable
> (Ch.surf), which are both numeric variables in the data frame.  The data are
> hierarchically grouped as Espece, Plante nested within Espece, and Feuille
> nested within Plante (i.e. leaves within plants within species).  Espece is
> defined as a factor and Plante and Feuille are defined as numeric in the
> data frame.  There are two parameters (k1 and b) and I want to fit a model
> in which these two parameters vary randomly across Espece and Plante (with
> Feuille being the residual level).  Here is how I have specified the call to
> nlme:
>  
>> fit<-nlme(model=Absorb~ k1 - (1/(b*Ch.surf)),
> + data=Marouane.chlorophyll,fixed=k1+b~1,
> + groups=~Espece/Plante,
> + start=list(k1=97.8, b=4.68))
>  
> However, there is something wrong with this syntax because I get the
> following error:
>  
> Problem in names<-: Invalid length for names attribute: structu
> re(.Data = list(structure(.Data = numeric(0), class .... 
>  
> Can someone explain what I am doing wrong?
> 
>  
> 
> Bill Shipley
> 
> North American Editor, Annals of Botany
> 
> Editor, "Population and Community Biology" series, Springer Publishing
> 
> D?partement de biologie, Universit? de Sherbrooke,
> 
> Sherbrooke (Qu?bec) J1K 2R1 CANADA
> 
> Bill.Shipley at USherbrooke.ca
> 
> http://callisto.si.usherb.ca:8080/bshipley/
> 
>  
> 
>  
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From Stephan.Kolassa at gmx.de  Sun Jun  4 22:10:00 2006
From: Stephan.Kolassa at gmx.de (Stephan Kolassa)
Date: Sun, 04 Jun 2006 22:10:00 +0200
Subject: [R] Nested and repeated effects together?
Message-ID: <44833E18.9040407@gmx.de>

Dear R people,

I am having a problem with modeling the following SAS code in R:

Class ID Gr Hemi Region Gender
Model Y = Gr Region Hemi Gender Gr*Hemi Gr*Region Hemi*Region 
Gender*Region Gender*Hemi Gr*Hemi*Region Gender*Hemi*Region 
Gr*Gender*Hemi*Region
Random Intercept Region Hemi /Subject = ID (Gr Gender)

I.e., ID is a random effect nested in Gr and Gender, leading to 
ID-specific Intercept, Region and Hemi means. We have repeated 
measurements within each ID (one measurement each for the different 
combinations of Hemi and Region, i.e. 2 levels in Hemi: L vs. R; 4 
levels in Region: F vs. C vs. T vs. PO).

I have been trying things like

aov(y~Gr+Region+Hemi+Gender+Gr:Hemi+Gr:Region+Hemi:Region+Gender:Region+Gender:Hemi+Gr:Hemi:Region+Gender:Hemi:Region+Gr:Gender:Hemi:Region+
    Error(ID/(Gr+Gender))

and do get results, but I am very unsure whether this implements the 
right model. An Error term like

    Error((1+Region+Hemi)/(ID_Vp/(Gender+Gr))), data=daten))

in the above model looks intuitively better to me, but (1) again: I'm 
unsure about this, (2) this crashes my R.

Of course, I have been googling for all permutations of "Nested 
effects", "repeated effects", "random effects" and digging through the 
R-help archives, but I can't seem to locate a similar question having 
been answered before.

Thank you all for your time!

Best regards,
Stephan


From spencer.graves at pdf.com  Sun Jun  4 22:21:23 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 04 Jun 2006 13:21:23 -0700
Subject: [R] date sequencing using the Fcalendar package
In-Reply-To: <5423441.1528431149179208316.JavaMail.root@vms068.mailsrvcs.net>
References: <5423441.1528431149179208316.JavaMail.root@vms068.mailsrvcs.net>
Message-ID: <448340C3.8050902@pdf.com>

	  Have you reviewed "White Paper" on "timeDate / timeSeries" 
downloadable from "www.rmetrics.org"?  I believe this should answer your 
question.  It is my understanding that "fCalendar" was designed to help 
people coordinate trading times and dates between different financial 
markets all around the world, adjusting for varying national holidays as 
well as time zone and weekend effects.  If you would like further help 
from this listserve, please submit another post.

	  Hope this helps,
	  Spencer Graves

markleeds at verizon.net wrote:
> I am using the following command from the Fcalendar Package :
> 
> x = timeSequence("1992-12-31","1994-12-31")
> 
> and then y = as.character(x) is a vector of character strings
> 
> "[1] 1992-12-31" "1993-01-31" "1993-03-03"  "1993-03-31" "1993-05-01" etc
> 
> This is very close to what I need and thank you very much
> to whomever wrote Fcalendar.
> The problem is that I'm not sure what the algorithm is for 
creating these speicific dates ? At first I thought it might
be 31 days between each but that's not the case for March
( see above). Regardless of what the algorithm is, does anyone
know of an easy way to make sure the months I get back are US
business days ? In other words, push any date that is not a US
business day to the next US business day ? I  can do it by hand
but I have 10 years of data so it would be easier if there was
some R way to do it ? I've looked through the fCalendar package
but without success.
> 
>                                                  Thanks a lot.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From spencer.graves at pdf.com  Sun Jun  4 22:49:08 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 04 Jun 2006 13:49:08 -0700
Subject: [R] setting the random-effects covariance matrix in lme
In-Reply-To: <91C1AD5E37A96E42B8A200E334370A9C0495A19C@MAIL2.AD.Brown.Edu>
References: <91C1AD5E37A96E42B8A200E334370A9C0495A19C@MAIL2.AD.Brown.Edu>
Message-ID: <44834744.1020404@pdf.com>

	  I haven't seen a reply to this post, so I will offer a few comments. 
  I don't know off the top of my head how to do what you want, but I 
believe it can be done.

	  Are you aware that the standard R distribution includes script files 
containing nearly all the R commands required to produce the analyses 
described in Pinheiro and Bates?  Find where R is installed on your 
computer, and then look for "~R/library/nlme/scripts".  This should make 
it vastly easier to work through the book line by line.  [It does for 
me.  For example, I keep forgetting that a "b^2" term in the book must 
usually be written "I(b^2)".  Also, I've spent hours trying to 
understand why my results differed so much from the book only to find a 
stupid typographical error.  These problems disappear when using the 
script file.]

	  If this fails, please provide a simple, self-contained example, e.g., 
modifying an example in the book to show something you tried, and 
explain why it didn't work for you.

	  Hope this helps.
	  Spencer Graves
p.s.  You mentioned you looked at Section 4.2.2.  Have you considered 
Section 5.2  "Variance Functions for Modeling Heteroscedasticity".  What 
you describe sounds like "heterscedasticity" to me.

Rendas-Baum, Regina wrote:
> Dear R-users,
>  
> I have longitudinal data and would like to fit a model where both 
the variance-covariance matrix of the random effects and the residual
variance are conditional on a (binary) grouping variable.
> I guess the model would have the following form (in hierarchical 
notation)
> Yi|bi,k ~ N(XiB+Zibi, sigmak*Ident)
> bi|k ~ N(0, Dk)
> K~Bernoulli(p)
>  
> I can obtain different sigmas (sigma0 and sigma1 based on the factor 
'dx') using the weights option in the call to lme:
> lme(fixed = height ~ -1 + bage + mat_ht + pat_ht + dx + time:dx + time2:dx , 
> 
>    weights=varIdent(form=~1|dx),
> 
>    random = ~ time + time2 |subject, data = pilot, na.action=na.omit)
> 
>  but I cannot seem to be able to get two different matrices for the 
random effects.
> 
> I'm particularly interested in obtaining,
>  
> Y|k ~ N(XB, Z(Dk)Z+sigmak),
>  
> so I need to extract D0 and D1 form the lme object.  I've looked in 
Pinheiro and Bates but was unable to identify how to fit this type of
covariance matrix from the examples provided in the book - perhaps it
is there expressed in a different form?  I looked in section 4.2.2
(pdMat) of teh book but I'm still not sure how to pass it on in the call.
>  
> Any help would be greatly appreciated,
>  
> Regina
>  
> Regina Rendas-Baum
> Graduate Student
> Brown University
> Biostatistics
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From spencer.graves at pdf.com  Mon Jun  5 01:20:42 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 04 Jun 2006 16:20:42 -0700
Subject: [R] "predict" function does not provide SE
 estimates	for	multivariate timeseries VAR models?
In-Reply-To: <17535.53819.101085.194676@bossiaea.maths.uwa.edu.au>
References: <b1f16d9d0606011409u42abe042vf9d5a6f7667d1528@mail.gmail.com>
	<17535.53819.101085.194676@bossiaea.maths.uwa.edu.au>
Message-ID: <44836ACA.9030309@pdf.com>

	  The "dse2" package contains functions "forecast" and "forecastCov". 
Have you tried them?

	  There may be functions to estimate vector autoregressive models in 
more than one package in R.  If you'd like more help from this 
listserve, I would encourage you to submit another post after first 
reading the posting guide! "www.R-project.org/posting-guide.html". 
Please include a simple, self-contained example of something you've 
tried that was as close as you can find to what you want.

	  I know this doesn't answer your question, but I hope it helps.

	  Spencer Graves

Berwin A Turlach wrote:
>>>>>> "Michael" == Michael  <comtech.usa at gmail.com> writes:
> 
>     Michael> What can I do?
> Implement this feature and send it to the person responsible for the
> code for inclusion?
> 
>     Michael> Thanks a lot!
> Indeed, you contribution would be very much appreciated.
> 
> Cheers,
> 
>         Berwin
> 
> ========================== Full address ============================
> Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)   
> School of Mathematics and Statistics        +61 (8) 6488 3383 (self)      
> The University of Western Australia   FAX : +61 (8) 6488 1028
> 35 Stirling Highway                   
> Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
> Australia                        http://www.maths.uwa.edu.au/~berwin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From maj at waikato.ac.nz  Mon Jun  5 06:29:48 2006
From: maj at waikato.ac.nz (Murray Jorgensen)
Date: Mon, 05 Jun 2006 16:29:48 +1200
Subject: [R] Extracting Variance components
Message-ID: <4483B33C.8080403@waikato.ac.nz>

I can ask my question using and example from Chapter 1 of Pinheiro & Bates.

 > # 1.4 An Analysis of Covariance Model
 >
 > OrthoFem <- Orthodont[ Orthodont$Sex == "Female", ]
 > fm1OrthF <-
+   lme( distance ~ age, data = OrthoFem, random = ~ 1 | Subject )
 > summary( fm1OrthF )
Linear mixed-effects model fit by REML
  Data: OrthoFem
        AIC     BIC    logLik
   149.2183 156.169 -70.60916

Random effects:
  Formula: ~1 | Subject
         (Intercept)  Residual
StdDev:     2.06847 0.7800331
[...etc...]

I can extract the estimate of the variance component \sigma (0.7800331) via

sigma <- fm1OrthF$sigma

How do I extract the other component \sigma_b (2.06847) ?

Cheers,  Murray Jorgensen
-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862


From crimson.turquoise at gmail.com  Mon Jun  5 06:55:20 2006
From: crimson.turquoise at gmail.com (Crimson Turquoise)
Date: Sun, 4 Jun 2006 21:55:20 -0700
Subject: [R] Tcltk Default Background Color Question
Message-ID: <780242430606042155y373009a0ub588db9d78e9b92f@mail.gmail.com>

Hello,

I am new at tcltk and would like to identify the default background
color for widgets.  I have found things like systemBackground or
tk_setPalette, but am not able to get them to work in R.  Is there a
name for this color such as "lightgray", etc?

Thank you for your time.


From A.Robinson at ms.unimelb.edu.au  Mon Jun  5 07:22:31 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Mon, 5 Jun 2006 15:22:31 +1000
Subject: [R] Extracting Variance components
In-Reply-To: <4483B33C.8080403@waikato.ac.nz>
References: <4483B33C.8080403@waikato.ac.nz>
Message-ID: <20060605052231.GJ18949@ms.unimelb.edu.au>

Murray,

you'll find it in 

VarCorr(fm1OrthF)

Cheers

Andrew

On Mon, Jun 05, 2006 at 04:29:48PM +1200, Murray Jorgensen wrote:
> I can ask my question using and example from Chapter 1 of Pinheiro & Bates.
> 
>  > # 1.4 An Analysis of Covariance Model
>  >
>  > OrthoFem <- Orthodont[ Orthodont$Sex == "Female", ]
>  > fm1OrthF <-
> +   lme( distance ~ age, data = OrthoFem, random = ~ 1 | Subject )
>  > summary( fm1OrthF )
> Linear mixed-effects model fit by REML
>   Data: OrthoFem
>         AIC     BIC    logLik
>    149.2183 156.169 -70.60916
> 
> Random effects:
>   Formula: ~1 | Subject
>          (Intercept)  Residual
> StdDev:     2.06847 0.7800331
> [...etc...]
> 
> I can extract the estimate of the variance component \sigma (0.7800331) via
> 
> sigma <- fm1OrthF$sigma
> 
> How do I extract the other component \sigma_b (2.06847) ?
> 
> Cheers,  Murray Jorgensen
> -- 
> Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
> Department of Statistics, University of Waikato, Hamilton, New Zealand
> Email: maj at waikato.ac.nz                                Fax 7 838 4155
> Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From joe at gnacademy.org  Mon Jun  5 07:35:57 2006
From: joe at gnacademy.org (Joseph Wang)
Date: Mon, 5 Jun 2006 00:35:57 -0500
Subject: [R] Functions starting with underscores
Message-ID: <200606050035.57596.joe@gnacademy.org>

I'm having problems with functions starting with underscores

'_foo' <- function(x) {1}

seems to work

but I can't assign an attribute to this function

attr('_foo', 'bar') <- 'pow'

Any way of doing this?  This is for a C++ -> R wrapping system so I'd like to 
keep the C++ names which start with underscores.

(please cc: responses to me personally)


From backer at psych.uib.no  Mon Jun  5 08:05:04 2006
From: backer at psych.uib.no (Tom Backer Johnsen)
Date: Mon, 05 Jun 2006 08:05:04 +0200
Subject: [R] Significance test, Cronbach's Alpha
In-Reply-To: <4481F48B.6090308@imst.math.uni-magdeburg.de>
References: <4481F48B.6090308@imst.math.uni-magdeburg.de>
Message-ID: <4483C990.5030706@psych.uib.no>

Hello:

I am reviewing a paper at the moment where the author reports a 
Cronbach's Alpha and adds a significance test of the value.  I was not 
aware that such a test exists.  Does it?  If so, how is it done in R?

Tom
+----------------------------------------------------------------+
| Tom Backer Johnsen, Psychometrics Unit,  Faculty of Psychology |
| University of Bergen, Christies gt. 12, N-5015 Bergen,  NORWAY |
| Tel : +47-5558-9185                        Fax : +47-5558-9879 |
| Email : backer at psych.uib.no    URL : http://www.galton.uib.no/ |
+----------------------------------------------------------------+


From ggrothendieck at gmail.com  Mon Jun  5 08:06:47 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 5 Jun 2006 02:06:47 -0400
Subject: [R] Functions starting with underscores
In-Reply-To: <200606050035.57596.joe@gnacademy.org>
References: <200606050035.57596.joe@gnacademy.org>
Message-ID: <971536df0606042306w43a70055k685a96787eee5eb2@mail.gmail.com>

Use backticks:

attr(`_foo`, "bar") <- "pow"

On 6/5/06, Joseph Wang <joe at gnacademy.org> wrote:
> I'm having problems with functions starting with underscores
>
> '_foo' <- function(x) {1}
>
> seems to work
>
> but I can't assign an attribute to this function
>
> attr('_foo', 'bar') <- 'pow'
>
> Any way of doing this?  This is for a C++ -> R wrapping system so I'd like to
> keep the C++ names which start with underscores.
>
> (please cc: responses to me personally)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From phgrosjean at sciviews.org  Mon Jun  5 08:10:55 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Mon, 05 Jun 2006 08:10:55 +0200
Subject: [R] Functions starting with underscores
In-Reply-To: <200606050035.57596.joe@gnacademy.org>
References: <200606050035.57596.joe@gnacademy.org>
Message-ID: <4483CAEF.9090200@sciviews.org>

It is not allowed to start a variable name with an underscore. So, you 
must use "`" to call this non-conventional name:

 > '_foo' <- function(x) 1
 > attr('_foo', 'bar') <- 'pow'
Error: target of assignment expands to non-language object
 > attr(`_foo`, 'bar') <- 'pow'
 > '_foo'
[1] "_foo"
 > `_foo`
function(x) 1
attr(,"bar")
[1] "pow"
 >

Best,

Philippe Grosjean


Joseph Wang wrote:
> I'm having problems with functions starting with underscores
> 
> '_foo' <- function(x) {1}
> 
> seems to work
> 
> but I can't assign an attribute to this function
> 
> attr('_foo', 'bar') <- 'pow'
> 
> Any way of doing this?  This is for a C++ -> R wrapping system so I'd like to 
> keep the C++ names which start with underscores.
> 
> (please cc: responses to me personally)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>


From gregd at stats.uct.ac.za  Mon Jun  5 09:20:48 2006
From: gregd at stats.uct.ac.za (Greg Distiller)
Date: Mon, 5 Jun 2006 09:20:48 +0200
Subject: [R] understanding the verbose output in nlme
References: <011301c68583$5e2daee0$6f179e89@UCTPCGREGD>
	<44832B4C.1090904@pdf.com>
Message-ID: <005301c68870$91b01ab0$6f179e89@UCTPCGREGD>

Thanks for the reply...will give some thought to your suggestion about 
stepping through the function.
I have read the Pinheiro and Bates book, in fact its my primary reference 
for getting into the nonlinear mixed models with R.
Lastly wrt the bit under subjects 1-6, I had thought about it being an 
estimated random effect but in this model there are 2 random effects so not 
sure if that holds...
thanks again...

----- Original Message ----- 
From: "Spencer Graves" <spencer.graves at pdf.com>
To: "Greg Distiller" <gregd at stats.uct.ac.za>
Cc: <r-help at stat.math.ethz.ch>
Sent: Sunday, June 04, 2006 8:49 PM
Subject: Re: [R] understanding the verbose output in nlme


>   I don't know, but if it were my question, I think I could find
> out by making local copies of the functions involved and stepping
> through the algorithm line by line using "debug" (see, e.g.,
> "http://finzi.psych.upenn.edu/R/Rhelp02a/archive/68215.html").
>
>   Have you read Pinheiro and Bates (2000) Mixed-Effects Models
> in S and S-Plus?  If no, I encourage you to do so.  Over the past 4
> years or so, I've probably spent more time with this book and referred 
> more people to it than any other.  Doug Bates is a leading original 
> contributor in this area, and I believe you will find this book well worth 
> your money and  your time.
>
>   Regarding "the numbers under subjectno1-6", I'm guessing that
> these may be the current estimates of the random effects for the first 6 
> of the 103 subjects.  The purpose of "verbose" is NOT to dump everything
> but only enough to help you evaluate whether the algorithm seems to be
> converging.
>
>   hope this helps.
>   Spencer Graves
>
> Greg Distiller wrote:
>> Hi
>> I have found some postings referring to the fact that one can try and 
>> understand why a particular model is failing to solve/converge from the 
>> verbose output one can generate when fitting a nonlinear mixed model. I 
>> am trying to understand this output and have not been able to find out 
>> much:
>>
>> **Iteration 1
>> LME step: Loglik: -237.4517 , nlm iterations: 22
>> reStruct  parameters:
>>   subjectno1   subjectno2   subjectno3   subjectno4   subjectno5 
>> subjectno6
>>  -0.87239181   2.75772772  -0.72892919 -10.36636391   0.55290322 
>> 0.09878685
>>
>> PNLS step: RSS =  60.50164
>>  fixed effects:2.59129  0.00741764  0.57155
>>  iterations: 7
>>
>> Convergence:
>>    fixed reStruct
>> 5.740688 2.159285
>>
>> I know that the Loglik must refer to the value of the log likelihood 
>> function, that the values after "fixed effects" are the parameter 
>> estimates, and that the bit after Convergence obviously has something to 
>> so with the convergence criteria for the fixed effects and the random 
>> effects structure. I did manage to find a posting where somebody said 
>> that the restruct parameter is the log of the relative precision of the 
>> random effects? The one thing that is a bit confusing to me is that it 
>> appears as if the fixed effects convergence must be zero (or close to it) 
>> as one would expect but in one of my converged models the output showed a 
>> restruct value of 0.72 ?
>>
>>
>>
>> Then I have no idea what the numbers under subjectno1-6 are, especially 
>> as I have 103 subjects in the data!
>>
>>
>>
>> Can anyone help shed some light on this output and how it can be used to 
>> diagnose issues with a model?
>>
>>
>>
>> Many thanks
>>
>>
>>
>> Greg
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>


From falissard_b at wanadoo.fr  Mon Jun  5 09:43:25 2006
From: falissard_b at wanadoo.fr (falissard)
Date: Mon, 5 Jun 2006 09:43:25 +0200
Subject: [R] Significance test, Cronbach's Alpha
In-Reply-To: <4483C990.5030706@psych.uib.no>
Message-ID: <20060605074324.62578240014F@mwinf1004.orange.fr>

This test can be meaningfull, but only in rare circumstancies (are the items
(or raters) more consistent than what could be expected by chance?).
I would do it with R using a bootstrap procedure (Efron & Tibshirani, An
introduction to the bootstrap, p.156).
Best regards,
Bruno

PS : I am not really a specialist...

----------------------------------------------------------------------------
Bruno Falissard
INSERM U669, PSIGIAM
"Paris Sud Innovation Group in Adolescent Mental Health"
Maison de Solenn
97 Boulevard de Port Royal
75679 Paris cedex 14, France
tel : (+33) 6 81 82 70 76
fax : (+33) 1 45 59 34 18
web site : http://perso.wanadoo.fr/bruno.falissard/
----------------------------------------------------------------------------
 

-----Message d'origine-----
De?: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] De la part de Tom Backer Johnsen
Envoy??: lundi 5 juin 2006 08:05
??: r-help at stat.math.ethz.ch
Objet?: [R] Significance test, Cronbach's Alpha

Hello:

I am reviewing a paper at the moment where the author reports a 
Cronbach's Alpha and adds a significance test of the value.  I was not 
aware that such a test exists.  Does it?  If so, how is it done in R?

Tom
+----------------------------------------------------------------+
| Tom Backer Johnsen, Psychometrics Unit,  Faculty of Psychology |
| University of Bergen, Christies gt. 12, N-5015 Bergen,  NORWAY |
| Tel : +47-5558-9185                        Fax : +47-5558-9879 |
| Email : backer at psych.uib.no    URL : http://www.galton.uib.no/ |
+----------------------------------------------------------------+

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From rkrug at sun.ac.za  Mon Jun  5 10:16:07 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Mon, 05 Jun 2006 10:16:07 +0200
Subject: [R] Problem with mle
In-Reply-To: <loom.20060602T193818-751@post.gmane.org>
References: <448052ED.4070209@krugs.de>
	<loom.20060602T193818-751@post.gmane.org>
Message-ID: <4483E847.4010407@sun.ac.za>

Hi Ben

Thanks a lot for your help - it solved my problem and I understand a
little bit more.

Just one more question along this line:
If I want to use another likelihood estimator, e.g. negative binominal,
I guess I have to use
-sum(dpois(SumSeeds,size=???, prob=???,log=TRUE))
instead of
-sum(dnbinom(SumSeeds,lambda=est,log=TRUE))

But what do I use for size and prob?

Rainer

-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
      	Rainer at krugs.de


From RMK at krugs.de  Mon Jun  5 10:22:02 2006
From: RMK at krugs.de (Rainer M KRug)
Date: Mon, 05 Jun 2006 10:22:02 +0200
Subject: [R] Calculation of AIC BIC from mle
Message-ID: <4483E9AA.3060700@krugs.de>

R 2.3.0, all packages up to date
Linux, SuSE 10.0

Hi

I want to calculate AIC or BIC from several results from mle calculation.

I found the AIC function, but it does not seem to work with objects of
class mle -
If I execute the following:
ml1 <- mle(...)
AIC(ml1)

I get the following error messale:
Error in logLik(object) : no applicable method for "logLik"

Therefore I am using the following to calculate the AIC:

#AICmle calculate AIC from mle object
AICmle <- function( m, k=2)
{
	lL <- logLik(m)
	edf <- attr(lL, "df")
	LL <- lL[1]
	- 2 * LL + k * edf
}

1) Why is AIC not working with objects of class mle - am I doing
something wrong, is it a bug or by design?

2) Just for confirmation - is my calculation of AIC correct?

Thanks

Rainer


From maechler at stat.math.ethz.ch  Mon Jun  5 10:35:07 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 5 Jun 2006 10:35:07 +0200
Subject: [R] How to call a value labels attribute?
In-Reply-To: <3.0.6.32.20060604130722.00a61e20@pop.gmx.net>
References: <3.0.6.32.20060523011721.00ace7c0@pop.gmx.net>
	<3.0.6.32.20060604130722.00a61e20@pop.gmx.net>
Message-ID: <17539.60603.813022.903176@stat.math.ethz.ch>

>>>>> "HeinzT" == Heinz Tuechler <tuechler at gmx.at>
>>>>>     on Sun, 04 Jun 2006 13:07:22 +0100 writes:

    HeinzT> At 14:12 03.06.2006 +0200, Martin Maechler wrote:
    >>>>>>> "Heinz" == Heinz Tuechler <tuechler at gmx.at>
    >>>>>>> on Tue, 23 May 2006 01:17:21 +0100 writes:
    >> 
    Heinz> Dear All, after searching on CRAN I got the
    Heinz> impression that there is no standard way in R to
    Heinz> label values of a numerical variable.  
    >> 
    >> Hmm, there's  names(.)  and  "names(.) <- .."
    >> Why are those not sufficient?
    >> 
    >> x <- 1:3
    >> names(x) <- c("apple", "banana", NA)

    HeinzT> Martin,

    HeinzT> I will considere this. For now I am using an
    HeinzT> attribute value.labels and a corresponding class to
    HeinzT> preserve this and other attributes after inclusion
    HeinzT> in a data.frame and indexing/subsetting, but using
    HeinzT> names should do as well.  My idea was more like
    HeinzT> defining a set of value labels for a variable and
    HeinzT> apply it to all the variable, as e.g. in the
    HeinzT> following _pseudocode_:

    HeinzT> ### not run
    HeinzT> ### pseudocode
    HeinzT> x <- c(1, 2, 3, 3, 2, 3, 1)
    HeinzT> value.labels(x) <- c(apple=1, banana=2, NA=3)
    HeinzT> x
    HeinzT> ### desired result
    HeinzT> apple banana  NA  NA banana NA apple
    HeinzT> 1      2   3   3      2  3     1

    HeinzT> value.labels(x) <- c(Apfel=1, Banane=2, Birne=3) # redefine labels
    HeinzT> x
    HeinzT> ### desired result
    HeinzT> Apfel Banane Birne Birne Banane Birne Apfel
    HeinzT> 1      2     3     3      2     3     1

    HeinzT> value.labels(x) # inspect labels
    HeinzT> ### desired result
    HeinzT> Apfel Banane Birne
    HeinzT> 1      2     3

    HeinzT> These value.labels should persist even after
    HeinzT> inclusion in a data.frame and after
    HeinzT> indexing/subsetting.

As far as I can see,  factor()s and their levels/labels provide
all that.

    HeinzT> I did not yet try your idea concerning these
    HeinzT> aspects, but I will do it. My final goal is to do
    HeinzT> all the data handling on numerically coded variables
    HeinzT> and to transform to factors "on the fly" when needed
    HeinzT> for statistical procedures. Given the presence of
    HeinzT> value.labels a factor function could use them for
    HeinzT> the conversion.

    HeinzT> I described my motivation for all this in a previous post, titled:
    HeinzT> How to represent a metric categorical variable?
    HeinzT> There was no response at all and I wonder, if this is such a rare problem.

Probably.  I don't see why this can't be done by (ordered)
factors.  But I'm really not particularly expert nor interested
here; I just wanted to make sure you didn't overlook the
"obvious".

Martin



    HeinzT> Thanks,
    HeinzT> Heinz

    >> 
    >> 
    Heinz> Since this
    Heinz> would be useful for me I intend to create such an
    Heinz> attribute, at the moment for my personal use.  Still
    Heinz> I would like to choose a name which does not conflict
    Heinz> with names of commonly used attributes.
    >> 
    Heinz> Would value.labels or vallabs create conflicts?
    >> 
    Heinz> The attribute should be structured as data.frame with
    Heinz> two columns, levels (numeric) and labels
    Heinz> (character). These could then also be used to
    Heinz> transform from numeric to factor. If the attribute is
    Heinz> copied to the factor variable it could also serve to
    Heinz> retransform the factor to the original numerical
    Heinz> variable.
    >> 
    Heinz> Comments? Ideas?
    >> 
    Heinz> Thanks
    >> 
    Heinz> Heinz T?chler
    >> 
    Heinz> ______________________________________________
    Heinz> R-help at stat.math.ethz.ch mailing list
    Heinz> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE
    Heinz> do read the posting guide!
    Heinz> http://www.R-project.org/posting-guide.html
    >> 
    >> 

>>>>> "HeinzT" == Heinz Tuechler <tuechler at gmx.at>
>>>>>     on Sun, 04 Jun 2006 13:07:22 +0100 writes:

    HeinzT> At 14:12 03.06.2006 +0200, Martin Maechler wrote:
    >>>>>>> "Heinz" == Heinz Tuechler <tuechler at gmx.at> on Tue,
    >>>>>>> 23 May 2006 01:17:21 +0100 writes:
    >>
    Heinz> Dear All, after searching on CRAN I got the
    Heinz> impression that there is no standard way in R to
    Heinz> label values of a numerical variable.
    >>  Hmm, there's names(.)  and "names(.) <- .."  Why are
    >> those not sufficient?
    >> 
    >> x <- 1:3 names(x) <- c("apple", "banana", NA)

    HeinzT> Martin,

    HeinzT> I will considere this. For now I am using an
    HeinzT> attribute value.labels and a corresponding class to
    HeinzT> preserve this and other attributes after inclusion
    HeinzT> in a data.frame and indexing/subsetting, but using
    HeinzT> names should do as well.  My idea was more like
    HeinzT> defining a set of value labels for a variable and
    HeinzT> apply it to all the variable, as e.g. in the
    HeinzT> following _pseudocode_:

    HeinzT> ### not run ### pseudocode x <- c(1, 2, 3, 3, 2, 3,
    HeinzT> 1) value.labels(x) <- c(apple=1, banana=2, NA=3) x
    HeinzT> ### desired result apple banana NA NA banana NA
    HeinzT> apple 1 2 3 3 2 3 1

    HeinzT> value.labels(x) <- c(Apfel=1, Banane=2, Birne=3) #
    HeinzT> redefine labels x ### desired result Apfel Banane
    HeinzT> Birne Birne Banane Birne Apfel 1 2 3 3 2 3 1

    HeinzT> value.labels(x) # inspect labels ### desired result
    HeinzT> Apfel Banane Birne 1 2 3

    HeinzT> These value.labels should persist even after
    HeinzT> inclusion in a data.frame and after
    HeinzT> indexing/subsetting.  I did not yet try your idea
    HeinzT> concerning these aspects, but I will do it. My final
    HeinzT> goal is to do all the data handling on numerically
    HeinzT> coded variables and to transform to factors "on the
    HeinzT> fly" when needed for statistical procedures. Given
    HeinzT> the presence of value.labels a factor function could
    HeinzT> use them for the conversion.

    HeinzT> I described my motivation for all this in a previous
    HeinzT> post, titled: How to represent a metric categorical
    HeinzT> variable?  There was no response at all and I
    HeinzT> wonder, if this is such a rare problem.

    HeinzT> Thanks, Heinz

    >>
    Heinz> Since this would be useful for me I intend to create
    Heinz> such an attribute, at the moment for my personal use.
    Heinz> Still I would like to choose a name which does not
    Heinz> conflict with names of commonly used attributes.
    >>
    Heinz> Would value.labels or vallabs create conflicts?
    >>
    Heinz> The attribute should be structured as data.frame with
    Heinz> two columns, levels (numeric) and labels
    Heinz> (character). These could then also be used to
    Heinz> transform from numeric to factor. If the attribute is
    Heinz> copied to the factor variable it could also serve to
    Heinz> retransform the factor to the original numerical
    Heinz> variable.
    >>
    Heinz> Comments? Ideas?
    >>
    Heinz> Thanks
    >>
    Heinz> Heinz T?chler
    >>
    Heinz> ______________________________________________
    Heinz> R-help at stat.math.ethz.ch mailing list
    Heinz> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE
    Heinz> do read the posting guide!
    Heinz> http://www.R-project.org/posting-guide.html
    >>


From adi at roda.ro  Sun Jun  4 13:10:22 2006
From: adi at roda.ro (Adrian DUSA)
Date: Sun, 4 Jun 2006 14:10:22 +0300
Subject: [R] [R-pkgs] new package QCAGUI
Message-ID: <200606041410.22352.adi@roda.ro>


Dear list members,

I'm pleased to let you know there's a new package on CRAN called QCAGUI, a 
graphical user interface for the QCA package.
This is a stripped down version of John Fox's Rcmdr package, plus a couple of 
menus for QCA.

My thanks to John Fox for his encouragement and advice.

Regards,
Adrian

-- 
Adrian DUSA
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From jim at bitwrit.com.au  Tue Jun  6 01:19:37 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Mon, 05 Jun 2006 19:19:37 -0400
Subject: [R] Significance test, Cronbach's Alpha
In-Reply-To: <4483C990.5030706@psych.uib.no>
References: <4481F48B.6090308@imst.math.uni-magdeburg.de>
	<4483C990.5030706@psych.uib.no>
Message-ID: <4484BC09.8010400@bitwrit.com.au>

Tom Backer Johnsen wrote:
> Hello:
> 
> I am reviewing a paper at the moment where the author reports a 
> Cronbach's Alpha and adds a significance test of the value.  I was not 
> aware that such a test exists.  Does it?  If so, how is it done in R?
> 
Hi Tom,

This may be due to the fact that some interpret Cronbach's alpha as a 
correlation between items, thus encouraging the unwary to assume that 
the probability of a numerically equivalent correlation can be used as a 
test of significance.

Jim


From tuechler at gmx.at  Mon Jun  5 11:57:14 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Mon, 05 Jun 2006 10:57:14 +0100
Subject: [R] How to call a value labels attribute?
In-Reply-To: <1149433288.4482f5c808fb2@webmail1.kuleuven.be>
References: <3.0.6.32.20060604130722.00a61e20@pop.gmx.net>
	<3.0.6.32.20060523011721.00ace7c0@pop.gmx.net>
	<3.0.6.32.20060523011721.00ace7c0@pop.gmx.net>
	<3.0.6.32.20060604130722.00a61e20@pop.gmx.net>
Message-ID: <3.0.6.32.20060605105714.00accc98@pop.gmx.net>

Dimitris,

thank you. I have to considere all the responses to this question and then
your functions may prove to be useful.

Heinz

At 17:01 04.06.2006 +0200, Dimitrios Rizopoulos wrote:
>maybe you could consider something like the following:
>
>varlabs <- function(x){
>    if (is.null(names(x))) NULL else x[!duplicated(x)]
>}
>"varlabs<-" <- function(x, value){
>    names(x) <- names(value[x])
>    x
>}
>###############
>x <- c(1, 2, 3, 3, 2, 3, 1)
>x
>varlabs(x)
>varlabs(x) <- c(apple=1, banana=2, "NA"=3)
>x
>varlabs(x)
>varlabs(x) <- c(Apfel=1, Banane=2, Birne=3)
>x
>varlabs(x)
>
>
>I hope it helps.
>
>Best,
>Dimitris
>
>---- 
>Dimitris Rizopoulos
>Ph.D. Student
>Biostatistical Centre
>School of Public Health
>Catholic University of Leuven
>
>Address: Kapucijnenvoer 35, Leuven, Belgium
>Tel: +32/(0)16/336899
>Fax: +32/(0)16/337015
>Web: http://med.kuleuven.be/biostat/
>     http://www.student.kuleuven.be/~m0390867/dimitris.htm
>
>
>Quoting Heinz Tuechler <tuechler at gmx.at>:
>
>> At 14:12 03.06.2006 +0200, Martin Maechler wrote:
>> >>>>>> "Heinz" == Heinz Tuechler <tuechler at gmx.at>
>> >>>>>>     on Tue, 23 May 2006 01:17:21 +0100 writes:
>> >
>> >    Heinz> Dear All, after searching on CRAN I got the
>> >    Heinz> impression that there is no standard way in R to
>> >    Heinz> label values of a numerical variable.  
>> >
>> >Hmm, there's  names(.)  and  "names(.) <- .."
>> >Why are those not sufficient?
>> >
>> >x <- 1:3
>> >names(x) <- c("apple", "banana", NA)
>> 
>> Martin,
>> 
>> I will considere this. For now I am using an attribute value.labels
>> and a
>> corresponding class to preserve this and other attributes after
>> inclusion
>> in a data.frame and indexing/subsetting, but using names should do as
>> well.
>> My idea was more like defining a set of value labels for a variable
>> and
>> apply it to all the variable, as e.g. in the following _pseudocode_:
>> 
>> ### not run
>> ### pseudocode
>> x <- c(1, 2, 3, 3, 2, 3, 1)
>> value.labels(x) <- c(apple=1, banana=2, NA=3)
>> x
>> ### desired result
>> apple banana  NA  NA banana NA apple
>>     1      2   3   3      2  3     1
>> 
>> value.labels(x) <- c(Apfel=1, Banane=2, Birne=3) # redefine labels
>> x
>> ### desired result
>> Apfel Banane Birne Birne Banane Birne Apfel
>>     1      2     3     3      2     3     1
>> 
>> value.labels(x) # inspect labels
>> ### desired result
>> Apfel Banane Birne
>>     1      2     3
>> 
>> These value.labels should persist even after inclusion in a
>> data.frame and
>> after indexing/subsetting.
>> I did not yet try your idea concerning these aspects, but I will do
>> it. My
>> final goal is to do all the data handling on numerically coded
>> variables
>> and to transform to factors "on the fly" when needed for statistical
>> procedures. Given the presence of value.labels a factor function
>> could use
>> them for the conversion.
>> 
>> I described my motivation for all this in a previous post, titled:
>> How to represent a metric categorical variable?
>> There was no response at all and I wonder, if this is such a rare
>> problem.
>> 
>> Thanks,
>> Heinz
>> 
>> >
>> >
>> >    Heinz> Since this
>> >    Heinz> would be useful for me I intend to create such an
>> >    Heinz> attribute, at the moment for my personal use.  Still
>> >    Heinz> I would like to choose a name which does not conflict
>> >    Heinz> with names of commonly used attributes.
>> >
>> >    Heinz> Would value.labels or vallabs create conflicts?
>> >
>> >    Heinz> The attribute should be structured as data.frame with
>> >    Heinz> two columns, levels (numeric) and labels
>> >    Heinz> (character). These could then also be used to
>> >    Heinz> transform from numeric to factor. If the attribute is
>> >    Heinz> copied to the factor variable it could also serve to
>> >    Heinz> retransform the factor to the original numerical
>> >    Heinz> variable.
>> >
>> >    Heinz> Comments? Ideas?
>> >
>> >    Heinz> Thanks
>> >
>> >    Heinz> Heinz T?chler
>> >
>> >    Heinz> ______________________________________________
>> >    Heinz> R-help at stat.math.ethz.ch mailing list
>> >    Heinz> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE
>> >    Heinz> do read the posting guide!
>> >    Heinz> http://www.R-project.org/posting-guide.html
>> >
>> >
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>> 
>> 
>
>
>Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
>


From tuechler at gmx.at  Mon Jun  5 12:20:08 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Mon, 05 Jun 2006 11:20:08 +0100
Subject: [R] How to call a value labels attribute?
In-Reply-To: <20060604121254.BCB62686@po-d.temple.edu>
Message-ID: <3.0.6.32.20060605112008.00acad28@pop.gmx.net>

Richard, Martin,

the example is not ideal, I see. I was strating from the question, how to
represent a metric categorical variable.
By "metric categorical variable" I intend a variable, which has only few
distinct values and an inherent metric. An example would be a risk score,
which classifies patients in several groups like "low, intermediate, high,
extreme" with corresponding risk estimates of "0, 1, 2, 5.5".
Other examples could be items in a questionnaire. These items often have
numerical values that may range from -5 to 5.
In some cases, like tables and box-plots these scores/items should be
treated like a factor (with labelled values), in other cases like
cox-regression or when forming an overall score they should be treated like
numeric variables.
I was asking for a convenient way to represent a variable like this, but
there was no response.
The crucial point is that the variables should retain their numerical
values and their value labels.
Without value labels they could be defined as factor and used or directly
or by as.numeric(), because the levels still represent the numerical
values, but as soon as labels are used, the original numerical values get
lost.


Thanks,

Heinz T?chler

At 12:12 04.06.2006 -0400, Richard M. Heiberger wrote:
>How is what you are doing any different from factors?
>
>
>> x <- factor(c(1, 2, 3, 3, 2, 3, 1), labels=c("apple", "banana", "other"))
>> x
>[1] apple  banana other  other  banana other  apple 
>Levels: apple banana other
>> as.numeric(x)
>[1] 1 2 3 3 2 3 1
>> levels(x)[3] <- "birne"
>> x
>[1] apple  banana birne  birne  banana birne  apple 
>Levels: apple banana birne
>> 
>
>
>---- Original message ----
>>### not run
>>### pseudocode
>>x <- c(1, 2, 3, 3, 2, 3, 1)
>>value.labels(x) <- c(apple=1, banana=2, NA=3)
>>x
>>### desired result
>>apple banana  NA  NA banana NA apple
>>    1      2   3   3      2  3     1
>>
>>value.labels(x) <- c(Apfel=1, Banane=2, Birne=3) # redefine labels
>>x
>>### desired result
>>Apfel Banane Birne Birne Banane Birne Apfel
>>    1      2     3     3      2     3     1
>>
>>value.labels(x) # inspect labels
>>### desired result
>>Apfel Banane Birne
>>    1      2     3
>
>


From mark_difford at yahoo.co.uk  Mon Jun  5 11:32:29 2006
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Mon, 5 Jun 2006 09:32:29 +0000 (GMT)
Subject: [R] nested design
Message-ID: <20060605093229.8007.qmail@web27308.mail.ukl.yahoo.com>

Jorn,

For your model,

model<-lme(Biomass~Age,random=~1|Age/Stand)

think about nesting age in stand (doesn't that makes more sense, anyway ?).  If you're lucky the NaN will zip.  So, do

model <- lme( Biomass ~ Age, random = ~ 1 | Stand/Age

I've had a similar problem with unbalanced data when I've tried to nest Season%in%Site (which is what I wanted), with Site as a fixed effect as well.  Then, like you, I've got NaN's for my Site(s).  Changing the nesting to Site%in%Season has fixed the problem.  However, random effects are now being specified somewhat differently.  Hopefully a statistician will come in and tell you and me quite what the difference is, because I get lost in the logic of it.

Note that lmer() copes with the problematic Season%in%Site nesting without problems, using the same dataset.


Regards,
Mark

------------------------------------------------------------ 
Mark DiffordPh.D. candidate, Botany Department,
Nelson Mandela Metropolitan University,
Port Elizabeth, SA.


From maechler at stat.math.ethz.ch  Mon Jun  5 11:42:19 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 5 Jun 2006 11:42:19 +0200
Subject: [R] strange (to me) ncolumns default value in 'write'
In-Reply-To: <b0808fdc0606040942l3001fc41v44234e04ab5d3fb7@mail.gmail.com>
References: <b0808fdc0606040942l3001fc41v44234e04ab5d3fb7@mail.gmail.com>
Message-ID: <17539.64635.154933.309075@stat.math.ethz.ch>

>>>>> "AntonioF" == Antonio, Fabio Di Narzo <antonio.fabio at gmail.com>
>>>>>     on Sun, 4 Jun 2006 18:42:35 +0200 writes:

    AntonioF> Hi all.
    AntonioF> In 'write' documentation, one can read:

    AntonioF> write(x, file = "data",
    AntonioF>       ncolumns = if(is.character(x)) 1 else 5,
    AntonioF>       append = FALSE, sep = " ")

    AntonioF> Now my question is: why the default value of 1column for character vectors
    AntonioF> and the special value '5' for non character vectors?

well, most of us have five fingers per hand, so many consider
'5' to be a 'nice' number :-)
For R, the main reason of using this default, was that S already
had it.  And S has had it not only for the above reason, but I
guess also because nicely, 5 * 16 = 80, and 80 used to be the
customary terminal width in the 70s and 80s, i.e., the good old
times S was created.

    AntonioF> Nice sunday to all,

and Pentecoste Monday, for those with an extra free day 
like me.

Martin Maechler, ETH Zurich

    AntonioF> Antonio, Fabio Di Narzo.

    AntonioF> Note: this is just curiosity. Don't read as: "WHY? WHY!?!?", but just as
    AntonioF> "why?"


From maechler at stat.math.ethz.ch  Mon Jun  5 11:43:41 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 5 Jun 2006 11:43:41 +0200
Subject: [R] slanted ends of horizontal lines for certain line widths
In-Reply-To: <009001c687ed$9d5a7d70$6600a8c0@DD4XFW31>
References: <4482FB09.3080804@imst.math.uni-magdeburg.de>
	<009001c687ed$9d5a7d70$6600a8c0@DD4XFW31>
Message-ID: <17539.64717.510959.622775@stat.math.ethz.ch>

>>>>> "Charles" == Charles Annis, P E <Charles.Annis at statisticalengineering.com>
>>>>>     on Sun, 4 Jun 2006 11:43:23 -0400 writes:

    Charles> I think you want to change par()$lend
    Charles> Type 

    Charles> par()  <enter>

    Charles> to see the defaults.

Actually, that's one place, where using str(.) is particularly
beneficial:  Use

   str(par())

    Charles> [............]


From maechler at stat.math.ethz.ch  Mon Jun  5 12:04:32 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 5 Jun 2006 12:04:32 +0200
Subject: [R] Calculation of AIC BIC from mle
In-Reply-To: <4483E9AA.3060700@krugs.de>
References: <4483E9AA.3060700@krugs.de>
Message-ID: <17540.432.4431.457029@stat.math.ethz.ch>

>>>>> "Rainer" == Rainer M KRug <RMK at krugs.de>
>>>>>     on Mon, 05 Jun 2006 10:22:02 +0200 writes:

    Rainer> R 2.3.0, all packages up to date
    Rainer> Linux, SuSE 10.0

    Rainer> Hi

    Rainer> I want to calculate AIC or BIC from several results from mle calculation.

    Rainer> I found the AIC function, but it does not seem to work with objects of
    Rainer> class mle -
    Rainer> If I execute the following:
    Rainer> ml1 <- mle(...)
    Rainer> AIC(ml1)

    Rainer> I get the following error messale:
    Rainer> Error in logLik(object) : no applicable method for "logLik"

which is really not helpful as error message, since it is *wrong*:
There is a logLik(.) method for "mle" objects, and it even
works.

There's some embarassing bug here, since if you quickly look at
the 'stats4' package source, it's very clear that it was designed
to have AIC(), BIC(), logLik() all working, but only the last
one does.


    Rainer> Therefore I am using the following to calculate the AIC:

    Rainer> #AICmle calculate AIC from mle object
    Rainer> AICmle <- function( m, k=2)
    Rainer> {
    Rainer> lL <- logLik(m)
    Rainer> edf <- attr(lL, "df")
    Rainer> LL <- lL[1]
    Rainer> - 2 * LL + k * edf
    Rainer> }

    Rainer> 1) Why is AIC not working with objects of class mle - am I doing
    Rainer> something wrong, is it a bug or by design?

a bug.

    Rainer> 2) Just for confirmation - is my calculation of AIC correct?

it looks so, but I didn't check.
Martin


From Tibi.Codilean at ges.gla.ac.uk  Mon Jun  5 12:09:50 2006
From: Tibi.Codilean at ges.gla.ac.uk (Tibi Codilean)
Date: Mon, 5 Jun 2006 11:09:50 +0100
Subject: [R] random sample from a file
In-Reply-To: <4484BC09.8010400@bitwrit.com.au>
Message-ID: <3240639.1149502190446.JavaMail.root@gesmail.physics.gla.ac.uk>

Dear All,


Thanks to all that responded to my earlier query.


I have a file with two columns: an ID field and a Data field. I wish to extract multiple random samples of 20 numbers from this file in such a way that the ID field is extracted with the data as well. At the moment I am using the 'sample' function but this only extracts the data without the IDs. The file is a comma separated text file and I read it in using read.csv


Could you please tell me if there is a way of doing this such that every random sample contains both the ID and the data.


Thanks


From backer at psych.uib.no  Mon Jun  5 12:34:34 2006
From: backer at psych.uib.no (Tom Backer Johnsen)
Date: Mon, 05 Jun 2006 12:34:34 +0200
Subject: [R] Significance test, Cronbach's Alpha
In-Reply-To: <4484BC09.8010400@bitwrit.com.au>
References: <4481F48B.6090308@imst.math.uni-magdeburg.de>
	<4483C990.5030706@psych.uib.no> <4484BC09.8010400@bitwrit.com.au>
Message-ID: <448408BA.4030700@psych.uib.no>

Jim Lemon wrote:
> Tom Backer Johnsen wrote:
>> Hello:
>>
>> I am reviewing a paper at the moment where the author reports a 
>> Cronbach's Alpha and adds a significance test of the value.  I was not 
>> aware that such a test exists.  Does it?  If so, how is it done in R?
>>
> Hi Tom,
> 
> This may be due to the fact that some interpret Cronbach's alpha as a 
> correlation between items, thus encouraging the unwary to assume that 
> the probability of a numerically equivalent correlation can be used as a 
> test of significance.

That is exactly what I suspect.  Actually, it is not too far off, but 
definitely not the same.

Tom

-- 
+----------------------------------------------------------------+
| Tom Backer Johnsen, Psychometrics Unit,  Faculty of Psychology |
| University of Bergen, Christies gt. 12, N-5015 Bergen,  NORWAY |
| Tel : +47-5558-9185                        Fax : +47-5558-9879 |
| Email : backer at psych.uib.no    URL : http://www.galton.uib.no/ |
+----------------------------------------------------------------+


From ripley at stats.ox.ac.uk  Mon Jun  5 12:38:28 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 5 Jun 2006 11:38:28 +0100 (BST)
Subject: [R] Calculation of AIC BIC from mle
In-Reply-To: <4483E9AA.3060700@krugs.de>
References: <4483E9AA.3060700@krugs.de>
Message-ID: <Pine.LNX.4.64.0606051121120.15751@gannet.stats.ox.ac.uk>

You are mixing S3 and S4 classes and generics here.  AIC(logLik(ml1)) will 
work.

This a namespace issue: stats4 contains a version of logLik, but the 
default method for AIC is from stats, and so dispatches on stats::logLik 
and not stats4::logLik.  There is something fundamentally unsatisfactory 
about converting S3 generics to S4 generics where namespaces are involved.

I have put a workaround in R-devel.

On Mon, 5 Jun 2006, Rainer M KRug wrote:

> R 2.3.0, all packages up to date
> Linux, SuSE 10.0
>
> Hi
>
> I want to calculate AIC or BIC from several results from mle calculation.
>
> I found the AIC function, but it does not seem to work with objects of
> class mle -
> If I execute the following:
> ml1 <- mle(...)
> AIC(ml1)
>
> I get the following error messale:
> Error in logLik(object) : no applicable method for "logLik"
>
> Therefore I am using the following to calculate the AIC:
>
> #AICmle calculate AIC from mle object
> AICmle <- function( m, k=2)
> {
> 	lL <- logLik(m)
> 	edf <- attr(lL, "df")
> 	LL <- lL[1]
> 	- 2 * LL + k * edf
> }
>
> 1) Why is AIC not working with objects of class mle - am I doing
> something wrong, is it a bug or by design?
>
> 2) Just for confirmation - is my calculation of AIC correct?

Not quite.  The correct function is

> stats:::AIC.logLik
function (object, ..., k = 2)
-2 * c(object) + k * attr(object, "df")
<environment: namespace:stats>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From maechler at stat.math.ethz.ch  Mon Jun  5 12:39:13 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 5 Jun 2006 12:39:13 +0200
Subject: [R] Calculation of AIC BIC from mle
In-Reply-To: <17540.432.4431.457029@stat.math.ethz.ch>
References: <4483E9AA.3060700@krugs.de>
	<17540.432.4431.457029@stat.math.ethz.ch>
Message-ID: <17540.2513.176764.278705@stat.math.ethz.ch>

>>>>> "MM" == Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Mon, 5 Jun 2006 12:04:32 +0200 writes:

>>>>> "Rainer" == Rainer M KRug <RMK at krugs.de>
>>>>>     on Mon, 05 Jun 2006 10:22:02 +0200 writes:

    Rainer> R 2.3.0, all packages up to date
    Rainer> Linux, SuSE 10.0

    Rainer> Hi

    Rainer> I want to calculate AIC or BIC from several results from mle calculation.

    Rainer> I found the AIC function, but it does not seem to work with objects of
    Rainer> class mle -
    Rainer> If I execute the following:
    Rainer> ml1 <- mle(...)
    Rainer> AIC(ml1)

    Rainer> I get the following error messale:
    Rainer> Error in logLik(object) : no applicable method for "logLik"

    MM> which is really not helpful as error message, since it is *wrong*:
    MM> There is a logLik(.) method for "mle" objects, and it even
    MM> works.

Hence -- as I forgot to mention in my last e-mail --
for now, you can just use

    AIC(logLik(ml1)) ## AIC
or  AIC(logLik(ml1), k = log(<nobs>) ## BIC

where you have to fill in  <nobs>  yourself.
Martin

    MM> There's some embarassing bug here, since if you quickly look at
    MM> the 'stats4' package source, it's very clear that it was designed
    MM> to have AIC(), BIC(), logLik() all working, but only the last
    MM> one does.


    Rainer> Therefore I am using the following to calculate the AIC:

    Rainer> #AICmle calculate AIC from mle object
    Rainer> AICmle <- function( m, k=2)
    Rainer> {
    Rainer> lL <- logLik(m)
    Rainer> edf <- attr(lL, "df")
    Rainer> LL <- lL[1]
    Rainer> - 2 * LL + k * edf
    Rainer> }

    Rainer> 1) Why is AIC not working with objects of class mle - am I doing
    Rainer> something wrong, is it a bug or by design?

    MM> a bug.

    Rainer> 2) Just for confirmation - is my calculation of AIC correct?

    MM> it looks so, but I didn't check.
    MM> Martin

    MM> ______________________________________________
    MM> R-help at stat.math.ethz.ch mailing list
    MM> https://stat.ethz.ch/mailman/listinfo/r-help
    MM> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From backer at psych.uib.no  Mon Jun  5 12:39:52 2006
From: backer at psych.uib.no (Tom Backer Johnsen)
Date: Mon, 05 Jun 2006 12:39:52 +0200
Subject: [R] significance test and cronbach
In-Reply-To: <200606050627.k556RlS3027050@gator.dt.uh.edu>
References: <200606050627.k556RlS3027050@gator.dt.uh.edu>
Message-ID: <448409F8.8060305@psych.uib.no>

Erin Hodgess wrote:
> Date: Mon, 05 Jun 2006 08:05:04 +0200
> From: Tom Backer Johnsen <backer at psych.uib.no>
> User-Agent: Thunderbird 1.5.0.4 (Windows/20060516)
> MIME-Version: 1.0
> To: r-help at stat.math.ethz.ch
> References: <4481F48B.6090308 at imst.math.uni-magdeburg.de>
> In-Reply-To: <4481F48B.6090308 at imst.math.uni-magdeburg.de>
> X-checked-clean: by exiscan on noralf
> X-Scanner: adae8ccb25ab4e5e1861f38a682801f6 http://tjinfo.uib.no/virus.html
> X-UiB-SpamFlag: NO UIB: -20.4 hits, 8.0 required
> X-UiB-SpamReport: spamassassin found; -15 From is listed in 'whitelist_SA'
> 	-9.0 Message received from UIB
> 	-0.4 Did not pass through any untrusted hosts
> 	4.0 BODY: Probably more lottery
> X-Virus-Scanned: by amavisd-new at stat.math.ethz.ch
> Subject: [R] Significance test, Cronbach's Alpha
> X-BeenThere: r-help at stat.math.ethz.ch
> X-Mailman-Version: 2.1.8
> Precedence: list
> Hi Tom!
> 
> There is a package in R called "psy"
> 
> One of the functions is "cronbach"

Yes, and that function computes Cronbach's Alpha.
> 
> I don't know about psychology, so I don't know really
> how it is all constructed.

That is a standard procedure, and rather easy to write in R.  My 
problem is different and has to do with a signigicance test of the value.

Tom

+----------------------------------------------------------------+
| Tom Backer Johnsen, Psychometrics Unit,  Faculty of Psychology |
| University of Bergen, Christies gt. 12, N-5015 Bergen,  NORWAY |
| Tel : +47-5558-9185                        Fax : +47-5558-9879 |
| Email : backer at psych.uib.no    URL : http://www.galton.uib.no/ |
+----------------------------------------------------------------+


From backer at psych.uib.no  Mon Jun  5 12:47:38 2006
From: backer at psych.uib.no (Tom Backer Johnsen)
Date: Mon, 05 Jun 2006 12:47:38 +0200
Subject: [R] Significance test, Cronbach's Alpha
In-Reply-To: <20060605074324.62578240014F@mwinf1004.orange.fr>
References: <20060605074324.62578240014F@mwinf1004.orange.fr>
Message-ID: <44840BCA.5080509@psych.uib.no>

falissard wrote:
> This test can be meaningfull, but only in rare circumstancies (are the items
> (or raters) more consistent than what could be expected by chance?).

Do you have a reference to such a test?

> I would do it with R using a bootstrap procedure (Efron & Tibshirani, An
> introduction to the bootstrap, p.156).

I agree.  That should be possible, actually quite simple.  Run through 
the data a large number of times, randomize the sequence of the 
observations for each case and compute alpha for each run.

Tom
+----------------------------------------------------------------+
| Tom Backer Johnsen, Psychometrics Unit,  Faculty of Psychology |
| University of Bergen, Christies gt. 12, N-5015 Bergen,  NORWAY |
| Tel : +47-5558-9185                        Fax : +47-5558-9879 |
| Email : backer at psych.uib.no    URL : http://www.galton.uib.no/ |
+----------------------------------------------------------------+


From jim at bitwrit.com.au  Tue Jun  6 02:25:33 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Mon, 05 Jun 2006 20:25:33 -0400
Subject: [R] plot with different color
In-Reply-To: <20060602191004.56787.qmail@web35712.mail.mud.yahoo.com>
References: <20060602191004.56787.qmail@web35712.mail.mud.yahoo.com>
Message-ID: <4484CB7D.8040208@bitwrit.com.au>

array chip wrote:
> Hi how can I plot a series of number as a line, but
> with lines above a threshould as one color, and with
> lines below the threshold as another color. for
> example, a numeric vector: rnorm(1:100), and plot
> these numbers in the original order, but lines above
> the horizontal line at 0 are in red, and lines below
> in blue?
> 
Hi array (or is it, Hi chip?),

Is this what you want?

x<-rnorm(100)
x11(width=7,height=7)
oldpar<-par(no.readonly=TRUE)
xmai<-par("mai")
par(yaxs="i")
plot(x,type="n",ylim=c(-3,3),xlab="Observation",ylab="Value")
par(mai=c(3.5,xmai[2:4]),yaxs="i",new=TRUE)
plot(x,type="l",col="blue",ylim=c(0,3),axes=FALSE,xlab="",ylab="")
par(mai=c(xmai[1:2],3.5,xmai[4]),new=TRUE)
plot(x,type="l",col="red",ylim=c(-3,0),axes=FALSE,xlab="",ylab="")
par(oldpar)

This _is_ an ugly kludge.

Jim


From rita.sousa at ine.pt  Mon Jun  5 12:26:22 2006
From: rita.sousa at ine.pt (Rita Sousa)
Date: Mon, 5 Jun 2006 11:26:22 +0100 
Subject: [R] FW: How to create a new package?
Message-ID: <2F01C2D1A2A44B41BD01DDE5F72E1CAB8E37F7@rngpew02.drn.ine.pt>

Hi,
Thanks very much for your help. I have already created a test package with
the package.skeleton() function and the procedures in the article "R Help
Desk: Make R CMD work under Windows - R News 5 (2), 27-28".
Now, my question is:
How can I install this package in others PC's without install there the Perl
and the remaining components?
Thanks, 
---------------------------------------------------
Rita Sousa
DME - ME: Departamento de Metodologia Estat?stica - M?todos Estat?sticos
INE - DRP: Instituto Nacional de Estat?stica - Delega??o Regional do Porto 
Tel.: 22 6072016 (Extens?o: 4116)
--------------------------------------------------- 

-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
Sent: quinta-feira, 1 de Junho de 2006 12:43
To: michael watson (IAH-C)
Cc: Gabor Grothendieck; Rita Sousa; r-help at stat.math.ethz.ch
Subject: Re: [R] FW: How to create a new package?

michael watson (IAH-C) wrote:

> ?package.skeleton 


Folks, please!


Rita Sousa told you she already has a DESCRIPTION file.

Obviously, Rita forgot to build and *install* the package using
R CMD build
and
R CMD INSTALL

(the Meta directory is creating during the installation procedure)

Please note that using the Windows GUI, you can only install binary 
packages, but you have got a source package so far. Hence you need to 
install from the Windows command shell using R CMD INSTALL.

Please see the R Installation and Administration manual on how to 
install packages. For some examples of what is mentioned in that manual 
(how to set stuff up in Windows), you might additionally want to take a 
look into the article "R Help Desk: Make `R CMD' Work under Windows - an 
Example" in R News 5 (2), 27-28.

Best,
Uwe Ligges



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
> Sent: 01 June 2006 12:20
> To: Rita Sousa
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] FW: How to create a new package?
> 
> The minimum is to create a DESCRIPTION file, plus R and man directories
containing R code and .Rd files respectively.
> It might help to run  Rcmd CHECK mypkg  before installation and fix any
problems it finds.
> 
> Googling for   creating R package   will locate some tutorials.
> 
> 
> On 6/1/06, Rita Sousa <rita.sousa at ine.pt> wrote:
> 
>>Hi,
>>
>>
>>
>>I'm a group of functions and I would like to create a package for load in
R.
>>I have created a directory named INE and a directory below that named 
>>R, for the files of R functions. A have created the files DESCRIPTION 
>>and INDEX in the INE directory. The installation from local zip files, 
>>in the R 2.3.0, results but to load the package I get an error like:
>>
>>
>>
>>'INE' is not a valid package -- installed < 2.0.0?
>>
>>
>>
>>I think that is necessary create a Meta directory with package.rds 
>>file, but I don't know make it! I have read the manual 'Writing R
Extensions - 1.
>>Creating R packages' but I don't understand the procedure...
>>
>>Can I create it automatically?
>>
>>
>>
>>Could you help me with this?
>>
>>
>>
>>Thanks,
>>
>>---------------------------------------------------
>>Rita Sousa
>>DME - ME: Departamento de Metodologia Estat?stica - M?todos 
>>Estat?sticos INE - DRP: Instituto Nacional de Estat?stica - Delega??o 
>>Regional do Porto
>>Tel.: 22 6072016 (Extens?o: 4116)
>>---------------------------------------------------
>>
>>
>>
>>
>>       [[alternative HTML version deleted]]
>>
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From baron at psych.upenn.edu  Mon Jun  5 13:05:54 2006
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Mon, 5 Jun 2006 07:05:54 -0400
Subject: [R] Significance test, Cronbach's Alpha
In-Reply-To: <4484BC09.8010400@bitwrit.com.au>
References: <4481F48B.6090308@imst.math.uni-magdeburg.de>
	<4483C990.5030706@psych.uib.no> <4484BC09.8010400@bitwrit.com.au>
Message-ID: <20060605110554.GA24978@psych.upenn.edu>

On 06/05/06 19:19, Jim Lemon wrote:
> Tom Backer Johnsen wrote:
> > Hello:
> >
> > I am reviewing a paper at the moment where the author reports a
> > Cronbach's Alpha and adds a significance test of the value.  I was not
> > aware that such a test exists.  Does it?  If so, how is it done in R?
> >
> Hi Tom,
> 
> This may be due to the fact that some interpret Cronbach's alpha as a
> correlation between items, thus encouraging the unwary to assume that
> the probability of a numerically equivalent correlation can be used as a
> test of significance.

SPSS has a test for alpha.  I don't know what it does.

It also seems to me that, if the assumptions of alpha are met,
then the assumptions of analysis of variance are also met.  In
particular, alpha equals the reliability if the measurements
(items) are "parallel" (Lord and Novick, "Statistical theories of
mental test scores," 1968, pp. 47 and 90 in particular).  That
means (among other things) that they have equal true variances.
If this can be assumed, then you can do an ANOVA using items and
subjects, and look for a significant effect of subjects.

If the measures are not parallel, then alpha is a lower bound on
the reliability of the test, so an ANOVA might be conservative,
but I have not thought this through.

It is rare to see anyone report a test for alpha because it is
usually used descriptively.  If it isn't .7 or higher, people get 
upset, yet even .5 would be wildly significant in most cases.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
Editor: Judgment and Decision Making (http://journal.sjdm.org)


From ripley at stats.ox.ac.uk  Mon Jun  5 13:22:42 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 5 Jun 2006 12:22:42 +0100 (BST)
Subject: [R] FW: How to create a new package?
In-Reply-To: <2F01C2D1A2A44B41BD01DDE5F72E1CAB8E37F7@rngpew02.drn.ine.pt>
References: <2F01C2D1A2A44B41BD01DDE5F72E1CAB8E37F7@rngpew02.drn.ine.pt>
Message-ID: <Pine.LNX.4.64.0606051219070.29623@gannet.stats.ox.ac.uk>

R CMD INSTALL --build  produces a binary package in a zip file.
See the `R Installation and Administration manual'.

On Mon, 5 Jun 2006, Rita Sousa wrote:

> Hi,
> Thanks very much for your help. I have already created a test package with
> the package.skeleton() function and the procedures in the article "R Help
> Desk: Make R CMD work under Windows - R News 5 (2), 27-28".
> Now, my question is:
> How can I install this package in others PC's without install there the Perl
> and the remaining components?
> Thanks,
> ---------------------------------------------------
> Rita Sousa
> DME - ME: Departamento de Metodologia Estat?stica - M?todos Estat?sticos
> INE - DRP: Instituto Nacional de Estat?stica - Delega??o Regional do Porto
> Tel.: 22 6072016 (Extens?o: 4116)
> ---------------------------------------------------
>
> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
> Sent: quinta-feira, 1 de Junho de 2006 12:43
> To: michael watson (IAH-C)
> Cc: Gabor Grothendieck; Rita Sousa; r-help at stat.math.ethz.ch
> Subject: Re: [R] FW: How to create a new package?
>
> michael watson (IAH-C) wrote:
>
>> ?package.skeleton
>
>
> Folks, please!
>
>
> Rita Sousa told you she already has a DESCRIPTION file.
>
> Obviously, Rita forgot to build and *install* the package using
> R CMD build
> and
> R CMD INSTALL
>
> (the Meta directory is creating during the installation procedure)
>
> Please note that using the Windows GUI, you can only install binary
> packages, but you have got a source package so far. Hence you need to
> install from the Windows command shell using R CMD INSTALL.
>
> Please see the R Installation and Administration manual on how to
> install packages. For some examples of what is mentioned in that manual
> (how to set stuff up in Windows), you might additionally want to take a
> look into the article "R Help Desk: Make `R CMD' Work under Windows - an
> Example" in R News 5 (2), 27-28.
>
> Best,
> Uwe Ligges
>
>
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
>> Sent: 01 June 2006 12:20
>> To: Rita Sousa
>> Cc: r-help at stat.math.ethz.ch
>> Subject: Re: [R] FW: How to create a new package?
>>
>> The minimum is to create a DESCRIPTION file, plus R and man directories
> containing R code and .Rd files respectively.
>> It might help to run  Rcmd CHECK mypkg  before installation and fix any
> problems it finds.
>>
>> Googling for   creating R package   will locate some tutorials.
>>
>>
>> On 6/1/06, Rita Sousa <rita.sousa at ine.pt> wrote:
>>
>>> Hi,
>>>
>>>
>>>
>>> I'm a group of functions and I would like to create a package for load in
> R.
>>> I have created a directory named INE and a directory below that named
>>> R, for the files of R functions. A have created the files DESCRIPTION
>>> and INDEX in the INE directory. The installation from local zip files,
>>> in the R 2.3.0, results but to load the package I get an error like:
>>>
>>>
>>>
>>> 'INE' is not a valid package -- installed < 2.0.0?
>>>
>>>
>>>
>>> I think that is necessary create a Meta directory with package.rds
>>> file, but I don't know make it! I have read the manual 'Writing R
> Extensions - 1.
>>> Creating R packages' but I don't understand the procedure...
>>>
>>> Can I create it automatically?
>>>
>>>
>>>
>>> Could you help me with this?
>>>
>>>
>>>
>>> Thanks,
>>>
>>> ---------------------------------------------------
>>> Rita Sousa
>>> DME - ME: Departamento de Metodologia Estat?stica - M?todos
>>> Estat?sticos INE - DRP: Instituto Nacional de Estat?stica - Delega??o
>>> Regional do Porto
>>> Tel.: 22 6072016 (Extens?o: 4116)
>>> ---------------------------------------------------
>>>
>>>
>>>
>>>
>>>       [[alternative HTML version deleted]]
>>>
>>>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>>
>>>
>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From vincent at 7d4.com  Mon Jun  5 13:25:17 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Mon, 05 Jun 2006 13:25:17 +0200
Subject: [R] making matrix monotonous
Message-ID: <4484149D.3050209@7d4.com>

Dear all,

I'm currently working on correlation matrix.
The function image() is quite useful for visualization.

Before using image(), I'd like to sort the matrix rows/columns,
in order to make the matrix the more monotonous/smooth possible.

Before reinventing the wheel, is there somebody here aware if
such a function already exists ?

Thanks.


From petr.pikal at precheza.cz  Mon Jun  5 13:29:30 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 05 Jun 2006 13:29:30 +0200
Subject: [R] random sample from a file
In-Reply-To: <3240639.1149502190446.JavaMail.root@gesmail.physics.gla.ac.uk>
References: <4484BC09.8010400@bitwrit.com.au>
Message-ID: <448431BA.23256.D1AEA6@localhost>

Hi

sample a vector from 1 to the number of rows in of your file and 
subset a file with sampled values

> iris[sample(1:nrow(iris), 20),]
    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species
99           5.1         2.5          3.0         1.1 versicolor
53           6.9         3.1          4.9         1.5 versicolor
40           5.1         3.4          1.5         0.2     setosa
146          6.7         3.0          5.2         2.3  virginica
93           5.8         2.6          4.0         1.2 versicolor
....

HTH
Petr

On 5 Jun 2006 at 11:09, Tibi Codilean wrote:

Date sent:      	Mon, 5 Jun 2006 11:09:50 +0100
From:           	"Tibi Codilean" <Tibi.Codilean at ges.gla.ac.uk>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] random sample from a file

> Dear All,
> 
> 
> Thanks to all that responded to my earlier query.
> 
> 
> I have a file with two columns: an ID field and a Data field. I wish
> to extract multiple random samples of 20 numbers from this file in
> such a way that the ID field is extracted with the data as well. At
> the moment I am using the 'sample' function but this only extracts the
> data without the IDs. The file is a comma separated text file and I
> read it in using read.csv
> 
> 
> Could you please tell me if there is a way of doing this such that
> every random sample contains both the ID and the data.
> 
> 
> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From rkrug at sun.ac.za  Mon Jun  5 13:37:19 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Mon, 05 Jun 2006 13:37:19 +0200
Subject: [R] Calculation of AIC BIC from mle
In-Reply-To: <Pine.LNX.4.64.0606051121120.15751@gannet.stats.ox.ac.uk>
References: <4483E9AA.3060700@krugs.de>
	<Pine.LNX.4.64.0606051121120.15751@gannet.stats.ox.ac.uk>
Message-ID: <4484176F.5040700@sun.ac.za>

Thanks a lot for your and Martin's help - I understand now why it is not
working and am using AIC(logLik(ml1)).

Thanks

Rainer


Prof Brian Ripley wrote:
> You are mixing S3 and S4 classes and generics here.  AIC(logLik(ml1))
> will work.
> 
> This a namespace issue: stats4 contains a version of logLik, but the
> default method for AIC is from stats, and so dispatches on stats::logLik
> and not stats4::logLik.  There is something fundamentally unsatisfactory
> about converting S3 generics to S4 generics where namespaces are involved.
> 
> I have put a workaround in R-devel.
> 
> On Mon, 5 Jun 2006, Rainer M KRug wrote:
> 
>> R 2.3.0, all packages up to date
>> Linux, SuSE 10.0
>>
>> Hi
>>
>> I want to calculate AIC or BIC from several results from mle calculation.
>>
>> I found the AIC function, but it does not seem to work with objects of
>> class mle -
>> If I execute the following:
>> ml1 <- mle(...)
>> AIC(ml1)
>>
>> I get the following error messale:
>> Error in logLik(object) : no applicable method for "logLik"
>>
>> Therefore I am using the following to calculate the AIC:
>>
>> #AICmle calculate AIC from mle object
>> AICmle <- function( m, k=2)
>> {
>>     lL <- logLik(m)
>>     edf <- attr(lL, "df")
>>     LL <- lL[1]
>>     - 2 * LL + k * edf
>> }
>>
>> 1) Why is AIC not working with objects of class mle - am I doing
>> something wrong, is it a bug or by design?
>>
>> 2) Just for confirmation - is my calculation of AIC correct?
> 
> Not quite.  The correct function is
> 
>> stats:::AIC.logLik
> function (object, ..., k = 2)
> -2 * c(object) + k * attr(object, "df")
> <environment: namespace:stats>
> 


-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
      	Rainer at krugs.de


From ericr at mcs.st-and.ac.uk  Mon Jun  5 13:41:37 2006
From: ericr at mcs.st-and.ac.uk (Eric Rexstad)
Date: Mon, 05 Jun 2006 12:41:37 +0100
Subject: [R] Status of "data.table" package
Message-ID: <44841871.70509@mcs.st-and.ac.uk>

List:

This package was described in R News of May 2006.  However, I cannot 
find it on CRAN mirrors, and use of RSiteSearch is unsatisfying.  I am 
likewise unable to find an email address for the maintainer of the 
package.  Thank you in advance for assistance.

-- 
Eric Rexstad
Research Unit for Wildlife Population Assessment
Centre for Research into Ecological and Environmental Modelling
University of St. Andrews
St. Andrews Scotland KY16 9LZ
+44 (0)1334 461833


From edd at debian.org  Mon Jun  5 13:50:02 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 5 Jun 2006 06:50:02 -0500
Subject: [R] Tcltk Default Background Color Question
In-Reply-To: <780242430606042155y373009a0ub588db9d78e9b92f@mail.gmail.com>
References: <780242430606042155y373009a0ub588db9d78e9b92f@mail.gmail.com>
Message-ID: <20060605115001.GA24581@eddelbuettel.com>

On Sun, Jun 04, 2006 at 09:55:20PM -0700, Crimson Turquoise wrote:
> Hello,
> 
> I am new at tcltk and would like to identify the default background
> color for widgets.  I have found things like systemBackground or
> tk_setPalette, but am not able to get them to work in R.  Is there a
> name for this color such as "lightgray", etc?

This works for me:

   tkcmd("tk_setPalette","gray93")     # set am overall background color

Dirk


-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From wtell67 at hotmail.com  Mon Jun  5 13:56:46 2006
From: wtell67 at hotmail.com (Werner Tell)
Date: Mon, 05 Jun 2006 11:56:46 +0000
Subject: [R] evir: generalized pareto dist
Message-ID: <BAY22-F182C3042BF7F596F66C888A8940@phx.gbl>

Hi,

I'm fitting a generalized Pareto distribution to POT exceedances of a data 
set. The practical stuff works ok, but I have a question regarding theory.
Is there an equation relating parameters of a gpd tail to its (first) 
moments? According to theory for certain parameters either the first moment 
does not exist or the distribution has an upper bound, but I haven't found 
the mentioned relation about moments in the literatur.

Thanks a lot.
wt

_________________________________________________________________
Die Vielfalt der Optionen l?sst Sie im Internet erfolgreich recherchieren.


From wtell67 at hotmail.com  Mon Jun  5 14:05:11 2006
From: wtell67 at hotmail.com (Werner Tell)
Date: Mon, 05 Jun 2006 12:05:11 +0000
Subject: [R] negative binomial: expected number of events?
Message-ID: <BAY22-F8AA3A1F2D7698FC5224CDA8940@phx.gbl>

Hi

I'm fitting poisson and negative binomial distributions to event data. I'm 
interested in the expected number of events occuring in a time period. For 
the poisson this is determined by the parameter lambda only.
For the neg. binomial, is the expected number of events determined by the 
parameter "mu" only or does parameter "size" influence the first moment as 
well?

thank you,
wt

_________________________________________________________________
Die MSN Homepage liefert Ihnen alle Infos zu Ihren Lieblingsthemen.


From andy_liaw at merck.com  Mon Jun  5 14:45:52 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 5 Jun 2006 08:45:52 -0400
Subject: [R] Status of "data.table" package
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02585EF4@usctmx1106.merck.com>

It was announced on the R-devel list:
http://tolstoy.newcastle.edu.au/R/devel/06/04/4923.html

I don't see it on CRAN, either, nor could I find mention of it in the R News
you cited.

Andy

From: Eric Rexstad
> 
> List:
> 
> This package was described in R News of May 2006.  However, I 
> cannot find it on CRAN mirrors, and use of RSiteSearch is 
> unsatisfying.  I am likewise unable to find an email address 
> for the maintainer of the package.  Thank you in advance for 
> assistance.
> 
> --
> Eric Rexstad
> Research Unit for Wildlife Population Assessment Centre for 
> Research into Ecological and Environmental Modelling 
> University of St. Andrews St. Andrews Scotland KY16 9LZ
> +44 (0)1334 461833
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From JeeBee at troefpunt.nl  Mon Jun  5 15:16:34 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Mon, 05 Jun 2006 15:16:34 +0200
Subject: [R] case insensitive regular expression
Message-ID: <pan.2006.06.05.13.16.33.235116@troefpunt.nl>

Can anybody fix the following, to match a string
disregarding case?

I tried: 
with(my.data.frame, regexpr(pattern = '(?i)J\. Test', 
  text=TARGET_COLUMN), perl=T) != -1)

R yields: invalid regular expression

with(my.data.frame, regexpr(pattern = 'J\. Test', 
  text=TARGET_COLUMN), perl=T) != -1)
works, but it does not match "J. TEST".
It does match "J. Test" correctly.

I have the feeling I'm not reading the documentation
good enough, but cannot seem to figure this out, sorry.

Thanks in advance,
JeeBee.


From JeeBee at troefpunt.nl  Mon Jun  5 15:30:00 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Mon, 05 Jun 2006 15:30:00 +0200
Subject: [R] case insensitive regular expression
References: <pan.2006.06.05.13.16.33.235116@troefpunt.nl>
Message-ID: <pan.2006.06.05.13.29.57.617241@troefpunt.nl>

Found the problem already.
Without the 'with' it works, so I know where to look for the problem now.

Thanks anyways,
JeeBee.

On Mon, 05 Jun 2006 15:16:34 +0200, JeeBee wrote:

> Can anybody fix the following, to match a string
> disregarding case?
> 
> I tried: 
> with(my.data.frame, regexpr(pattern = '(?i)J\. Test', 
>   text=TARGET_COLUMN), perl=T) != -1)
> 
> R yields: invalid regular expression
> 
> with(my.data.frame, regexpr(pattern = 'J\. Test', 
>   text=TARGET_COLUMN), perl=T) != -1)
> works, but it does not match "J. TEST".
> It does match "J. Test" correctly.
> 
> I have the feeling I'm not reading the documentation
> good enough, but cannot seem to figure this out, sorry.
> 
> Thanks in advance,
> JeeBee.


From Roger.Bivand at nhh.no  Mon Jun  5 15:40:32 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 5 Jun 2006 15:40:32 +0200 (CEST)
Subject: [R] Status of "data.table" package
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02585EF4@usctmx1106.merck.com>
Message-ID: <Pine.LNX.4.44.0606051538080.26672-100000@reclus.nhh.no>

On Mon, 5 Jun 2006, Liaw, Andy wrote:

> It was announced on the R-devel list:
> http://tolstoy.newcastle.edu.au/R/devel/06/04/4923.html
> 
> I don't see it on CRAN, either, nor could I find mention of it in the R News
> you cited.

In the archive:

http://cran.r-project.org/src/contrib/Archive/D/data.table_1.0.tar.gz

Roger

> 
> Andy
> 
> From: Eric Rexstad
> > 
> > List:
> > 
> > This package was described in R News of May 2006.  However, I 
> > cannot find it on CRAN mirrors, and use of RSiteSearch is 
> > unsatisfying.  I am likewise unable to find an email address 
> > for the maintainer of the package.  Thank you in advance for 
> > assistance.
> > 
> > --
> > Eric Rexstad
> > Research Unit for Wildlife Population Assessment Centre for 
> > Research into Ecological and Environmental Modelling 
> > University of St. Andrews St. Andrews Scotland KY16 9LZ
> > +44 (0)1334 461833
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From ritwik.sinha at gmail.com  Mon Jun  5 15:48:39 2006
From: ritwik.sinha at gmail.com (Ritwik Sinha)
Date: Mon, 5 Jun 2006 09:48:39 -0400
Subject: [R] negative binomial: expected number of events?
In-Reply-To: <BAY22-F8AA3A1F2D7698FC5224CDA8940@phx.gbl>
References: <BAY22-F8AA3A1F2D7698FC5224CDA8940@phx.gbl>
Message-ID: <42bc98300606050648q2fefb970s4c78c1adca51194b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060605/a22e5297/attachment.pl 

From bhs2 at mevik.net  Mon Jun  5 15:52:23 2006
From: bhs2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Mon, 05 Jun 2006 15:52:23 +0200
Subject: [R] Status of "data.table" package
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02585EF4@usctmx1106.merck.com>
	(Andy Liaw's message of "Mon, 5 Jun 2006 08:45:52 -0400")
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02585EF4@usctmx1106.merck.com>
Message-ID: <m0fyij1zhk.fsf@bar.nemo-project.org>

Liaw, Andy wrote:

> I don't see it on CRAN, either, nor could I find mention of it in the R News
> you cited.

p. 66

-- 
Bj?rn-Helge Mevik


From ritwik.sinha at gmail.com  Mon Jun  5 15:55:45 2006
From: ritwik.sinha at gmail.com (Ritwik Sinha)
Date: Mon, 5 Jun 2006 09:55:45 -0400
Subject: [R] negative binomial: expected number of events?
In-Reply-To: <42bc98300606050648q2fefb970s4c78c1adca51194b@mail.gmail.com>
References: <BAY22-F8AA3A1F2D7698FC5224CDA8940@phx.gbl>
	<42bc98300606050648q2fefb970s4c78c1adca51194b@mail.gmail.com>
Message-ID: <42bc98300606050655n79678170ifcb5c9705a3f6701@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060605/7ce60e7e/attachment.pl 

From rmh at temple.edu  Mon Jun  5 16:01:34 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Mon,  5 Jun 2006 10:01:34 -0400 (EDT)
Subject: [R] How to call a value labels attribute?
Message-ID: <20060605100134.BCC58015@po-d.temple.edu>

Aha!  Thank you for the more detailed example.

My solution for that situation is an attribute "position" and function
as.position().  I use this in my book

                Statistical Analysis and Data Display
                Richard M. Heiberger and Burt Holland

The online files for the book are available at
                http://springeronline.com/0-387-40270-5



For this example, you need the function as.position() included in this
email.


### example ########
x <- ordered(c(1,2,3,2,4,3,1,2,4,3,2,1,3),
             labels=c("small", "medium", "large", "very.large"))
x
attr(x, "position") <- c(1,2,4,8)
x
as.position(x)

y <- rnorm(length(x))
y

xyplot(y ~ x)
source("~/h2/library/code/as.position.s")
xyplot(y ~ as.position(x))
xyplot(y ~ as.position(x),
       scale=list(x=list(at=attr(x,"position"), labels=levels(x))))
xyplot(y ~ as.position(x),
       scale=list(x=list(at=attr(x,"position"), labels=levels(x))),
       xlab="x")
### end example ########



### as.position.s #########
as.position <- function(x) {
  if (is.numeric(x))
    x
  else {
    if (!is.factor(x)) stop("x must be either numeric or factor.")

    if (!is.null(attr(x, "position")))
      x <- attr(x, "position")[x]
    else {
      lev.x <- levels(x)
      if (inherits(x, "ordered")) {
        on.exit(options(old.warn))
        old.warn <- options(warn=-1)
        if (!any(is.na(as.numeric(lev.x))))
          x <- as.numeric(lev.x)[x]
        else
          x <- as.numeric(ordered(lev.x, lev.x))[x]
      }
      else
        x <- as.numeric(x)
    }
  }
  x
}


## tmp <- ordered(c("c","b","f","f","c","b"), c("c","b","f"))
## as.numeric(tmp)
## as.position(tmp)
## 
## tmp <- factor(c("c","b","f","f","c","b"))
## as.numeric(tmp)
## as.position(tmp)
## 
## tmp <- factor(c(1,3,5,3,5,1))
## as.numeric(tmp)
## as.position(tmp)
## 
## tmp <- ordered(c(1,3,5,3,5,1))
## as.numeric(tmp)
## as.position(tmp)
## 
## tmp <- c(1,3,5,3,5,1)
## as.numeric(tmp)
## as.position(tmp)

### end as.position.s #########


From justin_bem at yahoo.fr  Mon Jun  5 16:10:49 2006
From: justin_bem at yahoo.fr (justin bem)
Date: Mon, 5 Jun 2006 14:10:49 +0000 (GMT)
Subject: [R] How to compute Commutation Matrix With R
Message-ID: <20060605141049.49214.qmail@web25711.mail.ukl.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060605/1a6052d8/attachment.pl 

From dhajage at gmail.com  Mon Jun  5 16:22:13 2006
From: dhajage at gmail.com (David Hajage)
Date: Mon, 5 Jun 2006 16:22:13 +0200
Subject: [R] Problem with Sweave
Message-ID: <a725cda30606050722k1c24c690qaab5f6e5b3f5c24b@mail.gmail.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060605/68ee88fe/attachment.pl 

From macq at llnl.gov  Mon Jun  5 16:27:29 2006
From: macq at llnl.gov (Don MacQueen)
Date: Mon, 5 Jun 2006 07:27:29 -0700
Subject: [R] merge function in R
In-Reply-To: <1149292162.3999.7.camel@localhost.localdomain>
References: <20060601125938.18673.qmail@web7613.mail.in.yahoo.com>
	<1149292162.3999.7.camel@localhost.localdomain>
Message-ID: <p06230900c0a9ef0d0777@[128.115.153.6]>

(See insertions below)

At 8:49 PM -0300 6/2/06, Juan Santiago Ramseyer wrote:
>Em Qui, 2006-06-01 ?s 05:59 -0700, Ahamarshan jn escreveu:
>>  hi list,
>>
>>  This question must be very basic but I am just 3 days
>>  old to R. I am trying to find a function to merge two
>>  tables of data in two different files as one.
>>
>>   Does merge function only fills in colums between two
>>  table where data is missing or is there a way that
>>  merge can be used to merge data between two matrixes
>>  with common dimensions.
>>
>>  say i have
>>
>>     v1 v2 v3 v4  
>>  h1
>>  h2
>>  h3
>>  h4
>>  h5
>>
>>  and and another table with
>>
>>     x1 x2 x3 x4
>>  h1
>>  h2
>>  h3
>>  h4
>>  h5
>>
>>
>>  can i merge the data as
>>
>>     v1 x1 v2 x2 v3 x3 v4 x4
>>  h1
>>  h2
>>  h3
>>  h4
>>  h5
>>
>>  Thanks
>>
>hi
>
>testing the following sequence
>
># matrix examples
>x <- seq(from=-1,to=-20,step =-1)
>v <- seq(1:20)
>dim(x) <- c(5,4)
>dim(v) <- c(5,4)
>
>vx <- rep(NA,times=40)
>dim(vx) <- c(5,8)
>
># answer init
>for (i in 1:4) {
>   vx[,2*i-1] <- v[,i]
>   vx[,2*i] <- x[,i]
>}
># answer end

The loop is more complicated than necessary.

vx2 <- matrix(numeric(40),ncol=8)
vx2[,c(1,3,5,7)] <- v
vx2[,c(2,4,6,8)] <- x

## or more generally
vx2[ ,seq(1,by=2,len=4)] <- v
vx2[ ,seq(2,by=2,len=4)] <- x

## if the order of columns did not matter then the simplest is
vx3 <- cbind(v,x)

-Don

>Juan Santiago Ramseyer
>Eng. Recursos H?dricos
>
>
>>  ______________________________________________
>>  R-help at stat.math.ethz.ch mailing list
>>  https://stat.ethz.ch/mailman/listinfo/r-help
>>  PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA


From kevinvol2002 at yahoo.com  Mon Jun  5 18:03:34 2006
From: kevinvol2002 at yahoo.com (Hai Lin)
Date: Mon, 5 Jun 2006 09:03:34 -0700 (PDT)
Subject: [R] anova for the objects of list
Message-ID: <20060605160334.435.qmail@web32407.mail.mud.yahoo.com>

Dear R experts,

I have two list objects from runing lme model for each
subjects. And my.x is the full model with all terms I
am interested in and my.x1 is the null model with only
intercept.

I am trying to compare two models at each subject
level. (Doing anova(my.x[[1]], my.x1[[1]]) is to
produce stats for the first subject). My goal is to
obtain stats. for an entire set with hundreds of
subjects. I did something like this, lapply(my.Tem,
function(x)anova(x)), but my.Tem actually is not a
list after combining. It did not work the way as I
expected.

I am very thankful if anyone here could point me out.

Kevin

####
5 subjects extracted as my pilot data.
> class(my.x)
[1] "list"
> length(my.x)
[1] 5
> class(my.x1)
[1] "list"
> length(my.x1)
[1] 5 

####
my.Tem <- cbind(my.x, my.x1)
> class(my.Tem)
[1] "matrix"
> my.Tem
   my.x    my.x1  
s1 List,15 List,15
s2 List,15 List,15
s3 List,15 List,15
s4 List,15 List,15
s5 List,15 List,15


From e.rehak at t-online.de  Mon Jun  5 18:12:18 2006
From: e.rehak at t-online.de (Mark Hempelmann)
Date: Mon, 05 Jun 2006 18:12:18 +0200
Subject: [R] Survey - twophase
In-Reply-To: <mailman.5.1149501603.17588.r-help@stat.math.ethz.ch>
References: <mailman.5.1149501603.17588.r-help@stat.math.ethz.ch>
Message-ID: <448457E2.7060300@t-online.de>

Dear WizaRds,

    I am struggling with the use of twophase in package survey. My goal 
is to compute a simple example in two phase sampling:

phase 1: I sample n1=1000 circuit boards and find 80 non functional
phase 2: Given the n1=1000 sample I sample n2=100 and find 15 non 
functional. Let's say, phase 2 shows this result together with phase 1:
...................phase1........
...................ok defunct....
phase2 ok..........85....0.....85
.......defunct......5...10.....15
sum................90...10....100

That is in R:
fail <- data.frame(id=1:1000 , x=c(rep(0,920), rep(1,80)), 
y=c(rep(0,985), rep(1,15)), n1=rep(1000,1000), n2=rep(100,1000), 
N=rep(5000,1000))

des.fail    <- twophase(id=list(~id,~id), data=fail, subset=~I(x==1)) 
#    fpc=list(~n1,~n2)
svymean(~y, des.fail)

gives mean y 0.1875, SE 0.0196, but theoretically,
we have x.bar1 (phase1)=0.08 and y.bar2 (phase2)=0.15 defect boards.

Two phase sampling assumes some relation between the easily/ fast 
received x-information and the elaborate/ time-consuming y-information, 
say a ratio r=sum y (phase2)/ sum x (phase2)=15/10=1.5 (out of the above 
table)
Ergo, the y.ratio estimator = r*x.bar(phase1) = 1.5*0.08 = 0.12 with 
variance = (n1-n2)/n1 * s_regression^2/n2 + s_y^2/n1 = 900/1000 * 
0.0765/100 + 0.129/1000 = .00081 SE .02846
with s_regression^2 =
yk=c(rep(0,85), rep(1,15)); xk=c(rep(0,90), rep(1,10))
1/98*sum((yk-1.5*xk)^2)
and
s_yk^2 =
1/99 * sum( (yk-.15)^2)=0.1287879


I am sorry to bother you with my false calculations, but I just don't 
know how to receive the correct results. Please help. My example is 
taken from Kauermann/ Kuechenhoff 2006, p. 111f.

thank you so much
yours always

mark


From tlumley at u.washington.edu  Mon Jun  5 18:42:19 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 5 Jun 2006 09:42:19 -0700 (PDT)
Subject: [R] Survey - twophase
In-Reply-To: <448457E2.7060300@t-online.de>
References: <mailman.5.1149501603.17588.r-help@stat.math.ethz.ch>
	<448457E2.7060300@t-online.de>
Message-ID: <Pine.LNX.4.64.0606050928090.29721@homer22.u.washington.edu>

On Mon, 5 Jun 2006, Mark Hempelmann wrote:

> Dear WizaRds,
>
>    I am struggling with the use of twophase in package survey. My goal
> is to compute a simple example in two phase sampling:
>
> phase 1: I sample n1=1000 circuit boards and find 80 non functional
> phase 2: Given the n1=1000 sample I sample n2=100 and find 15 non
> functional. Let's say, phase 2 shows this result together with phase 1:
> ...................phase1........
> ...................ok defunct....
> phase2 ok..........85....0.....85
> .......defunct......5...10.....15
> sum................90...10....100
>
> That is in R:
> fail <- data.frame(id=1:1000 , x=c(rep(0,920), rep(1,80)),
> y=c(rep(0,985), rep(1,15)), n1=rep(1000,1000), n2=rep(100,1000),
> N=rep(5000,1000))
>
> des.fail    <- twophase(id=list(~id,~id), data=fail, subset=~I(x==1))
> #    fpc=list(~n1,~n2)

The second-phase sample is described by subset=~I(x==1), so you have 
sampled only 80 in phase two, not 100.

> svymean(~y, des.fail)
>
> gives mean y 0.1875, SE 0.0196, but theoretically,
> we have x.bar1 (phase1)=0.08 and y.bar2 (phase2)=0.15 defect boards.

15/80=0.1875

> Two phase sampling assumes some relation between the easily/ fast
> received x-information and the elaborate/ time-consuming y-information,
> say a ratio r=sum y (phase2)/ sum x (phase2)=15/10=1.5 (out of the above
> table)

Not quite. Two-phase sampling is *useful* only where there is a 
relationship. No relationship is *assumed*.

There are two ways you can take advantage of a relationship. The first is 
to stratify the phase-two sampling based on phase one information.  In 
this case you need a strata= argument to twophase().

The second way to use a relationship is to calibrate phase two to phase 
one, using the calibrate() function.  This is analogous to the regression 
estimator you describe.

A good example to look at is in vignette("epi").  This describes a 
two-phase sample where about 4000 people are in the first stage (a cancer 
clinical trial) and then the second phase is sampled based on relapse and 
on disease type ("histology") determined at the local hospital.
  Disease type is determined more accurately at a central lab for everyone 
who relapses, everyone whose locally-determined disease type is bad, and 
20% of the rest.

There is also an example of calibration, post-stratifying the second phase 
to the first phase on disease stage, for the same data.

Finally, note that twophase() does not use the unbiased estimator of 
variance. It uses a modification that is easier to compute for cluster 
samples, as described in vignette("phase1").  There is no difference if 
the first phase is sampled from an infinite population (or with 
replacement), which is the case in vignette("epi").


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From spencer.graves at pdf.com  Mon Jun  5 18:43:24 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 05 Jun 2006 09:43:24 -0700
Subject: [R] Multivariate skew-t cdf
In-Reply-To: <204e4c50606020222t1f46e4a2oebd19fda111fa1c1@mail.gmail.com>
References: <204e4c50606020222t1f46e4a2oebd19fda111fa1c1@mail.gmail.com>
Message-ID: <44845F2C.5040708@pdf.com>

	  You want to evaluate skewed t probabilities in how many dimensions? 
If 27 is your maximum, the problem won't be as difficult as if 27 is 
your minimum.  Also, do you want to compute the multivariate cumulative 
probability function for arbitrary points, location, covariance, shape 
and degrees of freedom?  Or are you really only interested in certain 
special case(s)?  If you have a simpler special case, it might be easier 
to get a solution.

	  I was able to replicate your result, even when I reduced the 
dimensionality down to 20;  with 19 dimensions, the function seemed to 
return a sensible answer.  If it were my problem, I might first make a 
local copy of the pmst function and modify it to use the mvtnorm package 
in place of mnormt.  That might get you answers with somewhat higher 
dimensionality, though it might not be adequate -- and I wouldn't trust 
the numbers I got without serious independent testing.  I'd try to think 
how I could modify the skewness so I could check the accuracy that way.

	  Have you studied the reference mentioned in the "dmst" help page, and 
reviewed some of their sources?  Computing multivariate probabilities 
like this is still a research project, I believe.  In this regard, I 
found the following two books helpful:

	  * Evans and Schwarz (2000) Approximating Integrals via Monte Carlo 
and Deterministic Methods (Oxford)

	  * Kythe and Schaeferkotter (2005) Handbook of Computational Methods 
for Integration (Chapman and Hall).

	  Also, have you asked about this directly to the maintainers of the 
"sn", "mnormt" and "mvtnorm" packages?  They might have other suggestions.

	  Hope this helps.
	  Spencer Graves
p.s.  Thanks for the self-contained example.  There seems to be a typo 
in your example:  Omega = diag(0, 27) is the matrix of all zeros, which 
produces a point mass at the center of the distribution.  I got your 
answers after changing it to diag(1, 27).

	  Making the dimension a variable, I found a sharp transition between k 
= 19 and 20:

 > k <- 19
 > xi <- alpha <- x <- rep(0,k)
 > Omega <- diag(1,k)
 > (p19 <- pmst(x, xi, Omega, alpha, df = 5))
[1] 1.907349e-06
attr(,"error")
[1] 2.413537e-20
attr(,"status")
[1] "normal completion"
 > k <- 20
 > xi <- alpha <- x <- rep(0,k)
 > Omega <- diag(1,k)
 > (p20 <- pmst(x, xi, Omega, alpha, df = 5))
[1] 0
attr(,"error")
[1] 1
attr(,"status")
[1] "oversize"

Konrad Banachewicz wrote:
> Dear All,
> I am using the pmst function from the sn package (version 0.4-0). After
> inserting the example from the help page, I get non-trivial answers, so
> everything is fine. However, when I try to extend it to higher dimension:
> xi <- alpha <- x <- rep(0,27)
> Omega <- diag(0,27)
> p1 <- pmst(x, xi, Omega, alpha, df = 5)
> 
> I get the following result:
> 
>> p1
> [1] 0
> attr(,"error")
> [1] 1
> attr(,"status")
> [1] "oversize"
> 
> So it seems like the dimension is a problem here (and not the syntax or type
> mismatch, as I inititally thought - the function is evaluated) - although I
> found no warning about it in the help page.
> 
> Can anyone give me a hint as to how to work around this problem and evaluate
> the skew-t cdf in a large-dimensional space? It's pretty crucial to my
> current research. Thanks in advance,
> 
> Konrad
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From aa at tango.stat.unipd.it  Mon Jun  5 19:36:01 2006
From: aa at tango.stat.unipd.it (Adelchi Azzalini)
Date: Mon, 5 Jun 2006 19:36:01 +0200
Subject: [R] Multivariate skew-t cdf
In-Reply-To: <44845F2C.5040708@pdf.com>
References: <204e4c50606020222t1f46e4a2oebd19fda111fa1c1@mail.gmail.com>
	<44845F2C.5040708@pdf.com>
Message-ID: <20060605173601.GA18582@tango.stat.unipd.it>

On Mon, Jun 05, 2006 at 09:43:24AM -0700, Spencer Graves wrote:


Thanks to Spencer Graves for provinding this clarification about pmst.
As the author of pmst, I was really the one expected to answer the
query, but  I was on travel in the last two weeks and did not read 
this query.

As a complement to what already explained, the reason of the sharp change
from k=19 to k=20 is as follows: pmst (package sn) calls pmt (package
mnormt) with k increased by 1, and pmt calls a Fortran routine (written by
Alan Genz), which issues an error if k>20 or k<1. Hence, the effective
maximum number of dimensions allowed by pmst is 19. 

best wishes,

Adelchi Azzalini



> 	  You want to evaluate skewed t probabilities in how many dimensions? 
> If 27 is your maximum, the problem won't be as difficult as if 27 is 
> your minimum.  Also, do you want to compute the multivariate cumulative 
> probability function for arbitrary points, location, covariance, shape 
> and degrees of freedom?  Or are you really only interested in certain 
> special case(s)?  If you have a simpler special case, it might be easier 
> to get a solution.
> 
> 	  I was able to replicate your result, even when I reduced the 
> dimensionality down to 20;  with 19 dimensions, the function seemed to 
> return a sensible answer.  If it were my problem, I might first make a 
> local copy of the pmst function and modify it to use the mvtnorm package 
> in place of mnormt.  That might get you answers with somewhat higher 
> dimensionality, though it might not be adequate -- and I wouldn't trust 
> the numbers I got without serious independent testing.  I'd try to think 
> how I could modify the skewness so I could check the accuracy that way.
> 
> 	  Have you studied the reference mentioned in the "dmst" help page, and 
> reviewed some of their sources?  Computing multivariate probabilities 
> like this is still a research project, I believe.  In this regard, I 
> found the following two books helpful:
> 
> 	  * Evans and Schwarz (2000) Approximating Integrals via Monte Carlo 
> and Deterministic Methods (Oxford)
> 
> 	  * Kythe and Schaeferkotter (2005) Handbook of Computational Methods 
> for Integration (Chapman and Hall).
> 
> 	  Also, have you asked about this directly to the maintainers of the 
> "sn", "mnormt" and "mvtnorm" packages?  They might have other suggestions.
> 
> 	  Hope this helps.
> 	  Spencer Graves
> p.s.  Thanks for the self-contained example.  There seems to be a typo 
> in your example:  Omega = diag(0, 27) is the matrix of all zeros, which 
> produces a point mass at the center of the distribution.  I got your 
> answers after changing it to diag(1, 27).
> 
> 	  Making the dimension a variable, I found a sharp transition between k 
> = 19 and 20:

> 
>  > k <- 19
>  > xi <- alpha <- x <- rep(0,k)
>  > Omega <- diag(1,k)
>  > (p19 <- pmst(x, xi, Omega, alpha, df = 5))
> [1] 1.907349e-06
> attr(,"error")
> [1] 2.413537e-20
> attr(,"status")
> [1] "normal completion"
>  > k <- 20
>  > xi <- alpha <- x <- rep(0,k)
>  > Omega <- diag(1,k)
>  > (p20 <- pmst(x, xi, Omega, alpha, df = 5))
> [1] 0
> attr(,"error")
> [1] 1
> attr(,"status")
> [1] "oversize"
> 
> Konrad Banachewicz wrote:
> > Dear All,
> > I am using the pmst function from the sn package (version 0.4-0). After
> > inserting the example from the help page, I get non-trivial answers, so
> > everything is fine. However, when I try to extend it to higher dimension:
> > xi <- alpha <- x <- rep(0,27)
> > Omega <- diag(0,27)
> > p1 <- pmst(x, xi, Omega, alpha, df = 5)
> > 
> > I get the following result:
> > 
> >> p1
> > [1] 0
> > attr(,"error")
> > [1] 1
> > attr(,"status")
> > [1] "oversize"
> > 
> > So it seems like the dimension is a problem here (and not the syntax or type
> > mismatch, as I inititally thought - the function is evaluated) - although I
> > found no warning about it in the help page.
> > 
> > Can anyone give me a hint as to how to work around this problem and evaluate
> > the skew-t cdf in a large-dimensional space? It's pretty crucial to my
> > current research. Thanks in advance,
> > 
> > Konrad
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit? di Padova, Italia
tel. +39 049 8274147,  http://azzalini.stat.unipd.it/


From kevinvol2002 at yahoo.com  Mon Jun  5 20:35:14 2006
From: kevinvol2002 at yahoo.com (Hai Lin)
Date: Mon, 5 Jun 2006 11:35:14 -0700 (PDT)
Subject: [R] anova for the objects of list
In-Reply-To: <20060605160334.435.qmail@web32407.mail.mud.yahoo.com>
Message-ID: <20060605183514.66037.qmail@web32407.mail.mud.yahoo.com>

Dear R experts,

Sorry for the previous email. I just figured it out
myself. I would welcome other solutions.

Thanks.
Kevin

my.anova <- sapply(names(my.x),
                               function(x)
                               {
                               anova(my.x[[x]],
my.x1[[x]])
                               }
                                 )

--- Hai Lin <kevinvol2002 at yahoo.com> wrote:

> Dear R experts,
> 
> I have two list objects from runing lme model for
> each
> subjects. And my.x is the full model with all terms
> I
> am interested in and my.x1 is the null model with
> only
> intercept.
> 
> I am trying to compare two models at each subject
> level. (Doing anova(my.x[[1]], my.x1[[1]]) is to
> produce stats for the first subject). My goal is to
> obtain stats. for an entire set with hundreds of
> subjects. I did something like this, lapply(my.Tem,
> function(x)anova(x)), but my.Tem actually is not a
> list after combining. It did not work the way as I
> expected.
> 
> I am very thankful if anyone here could point me
> out.
> 
> Kevin
> 
> ####
> 5 subjects extracted as my pilot data.
> > class(my.x)
> [1] "list"
> > length(my.x)
> [1] 5
> > class(my.x1)
> [1] "list"
> > length(my.x1)
> [1] 5 
> 
> ####
> my.Tem <- cbind(my.x, my.x1)
> > class(my.Tem)
> [1] "matrix"
> > my.Tem
>    my.x    my.x1  
> s1 List,15 List,15
> s2 List,15 List,15
> s3 List,15 List,15
> s4 List,15 List,15
> s5 List,15 List,15
> 
> 
> __________________________________________________
> Do You Yahoo!?

> protection around 
> http://mail.yahoo.com 
>


From chris at psyctc.org  Mon Jun  5 20:40:23 2006
From: chris at psyctc.org (Chris Evans)
Date: Mon, 05 Jun 2006 19:40:23 +0100
Subject: [R] Significance test, Cronbach's Alpha
In-Reply-To: <20060605110554.GA24978@psych.upenn.edu>
References: <4481F48B.6090308@imst.math.uni-magdeburg.de>	<4483C990.5030706@psych.uib.no>
	<4484BC09.8010400@bitwrit.com.au>
	<20060605110554.GA24978@psych.upenn.edu>
Message-ID: <44847A97.1030601@psyctc.org>

Jonathan Baron sent the following  at 05/06/2006 12:05:

... some snipped ...

> It is rare to see anyone report a test for alpha because it is
> usually used descriptively.  If it isn't .7 or higher, people get 
> upset, yet even .5 would be wildly significant in most cases.
> 
> Jon

Feldt did a lot of good work on ANOVA models of alpha, well summarised
in his paper with Salih.  Here are some functions.  Please don't
ridicule my R style, I'm a psychotherapist first, researcher second, and
R enthusiast third.  Amused advice on how to write better code warmly
received.

I'm sure that jackknifing or bootstrapping would give much more
distributionally robust CIs and p values but, as Jon's point above makes
so simply, the real problem is that people don't think through what
they're looking for from an alpha.  I find there are situations in which
I'm genuinely interested in the null: is there some evidence of
covariance here?; and other situations where I want a high alpha because
I'm postulating that we have a useful measure, in the latter case, all
these totemistic values that are "acceptable", "excellent" etc. are
often misleading and a CI around the observed alpha and some exploration
of the factor structure, EFA or CFA, or IRT model explorations, will be
far more important than exactly what alpha you got.

Oops </FLAME>  not quite sure where I should have put the starter on that!

I'll go back to enjoying the fact that I think this is the first time
I've posted something that might be useful to someone!

Very best all:



Chris

feldt1.return <- function(obs.a, n, k, ci = 0.95, null.a = 0)
{
	if(obs.a > null.a)
		f <- (1 - obs.a)/(1 - null.a)
	else f <- (1 - null.a)/(1 - obs.a)	
              # allows for testing against a higher null
	n.den <- (n - 1) * (k - 1)
	n.num <- n - 1
	null.p <- pf(f, n.num, n.den)	
              # set the upper and lower p values for the desired C.I.
	p1 <- (1 - ci)/2
	p2 <- ci + p1	# corresponding F values
	f1 <- qf(p1, n.num, n.den)
	f2 <- qf(p2, n.num, n.den)	# confidence interval
	lwr <- 1 - (1 - obs.a) * f2
	upr <- 1 - (1 - obs.a) * f1
	cat(round(lwr,2), "to",round(upr,2),"\n")
	interval <- list(lwr,upr)
        return(interval)
}

feldt1.lwr <- function(obs.a, n, k, ci = 0.95, null.a = 0)
{
	if(obs.a > null.a)
		f <- (1 - obs.a)/(1 - null.a)
	else f <- (1 - null.a)/(1 - obs.a)	
             # allows for testing against a higher null
	n.den <- (n - 1) * (k - 1)
	n.num <- n - 1
	null.p <- pf(f, n.num, n.den)	
             # set the upper and lower p values for the desired C.I.
	p1 <- (1 - ci)/2
	p2 <- ci + p1	# corresponding F values
	f1 <- qf(p1, n.num, n.den)
	f2 <- qf(p2, n.num, n.den)	# confidence interval
	lwr <- 1 - (1 - obs.a) * f2
        return(lwr)
}

feldt1.upr <- function(obs.a, n, k, ci = 0.95, null.a = 0)
{
	if(obs.a > null.a)
		f <- (1 - obs.a)/(1 - null.a)
	else f <- (1 - null.a)/(1 - obs.a)	
              # allows for testing against a higher null
	n.den <- (n - 1) * (k - 1)
	n.num <- n - 1
	null.p <- pf(f, n.num, n.den)	
              # set the upper and lower p values for the desired C.I.
	p1 <- (1 - ci)/2
	p2 <- ci + p1	# corresponding F values
	f1 <- qf(p1, n.num, n.den)
	f2 <- qf(p2, n.num, n.den)	# confidence interval
	upr <- 1 - (1 - obs.a) * f1
        return(upr)
}



-- 
Chris Evans <chris at psyctc.org>
Hon. Professor of Psychotherapy, Nottingham University;
Consultant Psychiatrist in Psychotherapy, Rampton Hospital;
Research Programmes Director, Nottinghamshire NHS Trust;
Hon. SL Institute of Psychiatry, Hon. Con., Tavistock & Portman Trust
**If I am writing from one of those roles, it will be clear. Otherwise**

**my views are my own and not representative of those institutions    **


From rvalliant at survey.umd.edu  Mon Jun  5 21:19:29 2006
From: rvalliant at survey.umd.edu (Richard Valliant)
Date: Mon, 05 Jun 2006 15:19:29 -0400
Subject: [R] Editor/environment problem
Message-ID: <s4844b99.062@SURVEYGWIA.UMD.EDU>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060605/340a9ddd/attachment.pl 

From apluzhni at bsd.uchicago.edu  Mon Jun  5 21:27:53 2006
From: apluzhni at bsd.uchicago.edu (Anna Pluzhnikov)
Date: Mon,  5 Jun 2006 14:27:53 -0500
Subject: [R] Fastest way to do HWE.exact test on 100K SNP data?
Message-ID: <1149535673.448485b924e4e@netmail.bsd.uchicago.edu>

Hi everyone,

I'm using the function 'HWE.exact' of 'genetics' package to compute p-values of
the HWE test. My data set consists of ~600 subjects (cases and controls) typed
at ~ 10K SNP markers; the test is applied separately to cases and controls. The
genotypes are stored in a list of 'genotype' objects, all.geno, and p-values are
calculated inside the loop over all SNP markers. 

I wish to repeat this procedure multiple times (~1000) permuting the cases and
controls (affection status). It seems straightforward to implement it like this:

#############################################

for (iter in 1:1000) {
  set.seed(iter)
# get the permuted affection status
  permut <- sample(affSt)
  for (j in 1:nSNPs) {
    test <- tapply(all.geno[[j]], permut, HWE.exact)
    pvalControls[j] <- test$"1"$p.value
    pvalCases[j] <- test$"2"$p.value
  }
}

##############################################

The problem is that it takes ~1 min/iteration (on AMD Opteron 252 processor
running Linux). 

Is there a faster/more efficient way to do this? 

Thanks,
-- 
Anna Pluzhnikov, PhD
Section of Genetic Medicine
Department of Medicine
The University of Chicago


-------------------------------------------------
This email is intended only for the use of the individual or...{{dropped}}


From murdoch at stats.uwo.ca  Mon Jun  5 21:35:01 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 05 Jun 2006 15:35:01 -0400
Subject: [R] Editor/environment problem
In-Reply-To: <s4844b99.062@SURVEYGWIA.UMD.EDU>
References: <s4844b99.062@SURVEYGWIA.UMD.EDU>
Message-ID: <44848765.40903@stats.uwo.ca>

This has been fixed in 2.3.1.  See

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/76657.html

or

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/76953.html

Duncan Murdoch



On 6/5/2006 3:19 PM, Richard Valliant wrote:
> I have encountered an odd problem in editing a function. I found no
> mention of this in the archives but may have missed something. I'm using
> Version 2.3.0 (2006-04-24). The problem is that after creating a
> function with fix, making some typos and correcting them, my function
> loses contact with everything in the search path except (apparently)
> package:base. Here is an example: > fix(tmp)> tmpfunction (x) {       
> var(x)}> tmp(1:5)[1] 2.5> fix(tmp) # Typos (,/ at end of var(x) ) were
> intentionally introduced# line is now: var(x),/ Error in edit(name,
> file, title, editor) :         an error occurred on line 3 use a command
> like x <- edit() to recover> tmp <- edit() # Typos corrected but new
> ones introduced# Line is now var(x)**** Error in edit(name, file, title,
> editor) :         an error occurred on line 3 use a command like x <-
> edit() to recover> tmp <- edit() # Typos fixed.# List the function. Note
> the <environment: base> at end. Not there before. > tmpfunction (x) {   
>     var(x)}<environment: base>> tmp(1:5)Error in tmp(1:5) : could not
> find function "var" TIARichard 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From dbogod at gmail.com  Mon Jun  5 22:01:03 2006
From: dbogod at gmail.com (Daniel Bogod)
Date: Mon, 5 Jun 2006 16:01:03 -0400
Subject: [R] Constrained regression
Message-ID: <a16e975c0606051301v575a59fcn7063229447754da8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060605/afa7ce1e/attachment.pl 

From ligges at statistik.uni-dortmund.de  Mon Jun  5 15:52:33 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 05 Jun 2006 15:52:33 +0200
Subject: [R] Status of "data.table" package
In-Reply-To: <Pine.LNX.4.44.0606051538080.26672-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0606051538080.26672-100000@reclus.nhh.no>
Message-ID: <44843721.9000508@statistik.uni-dortmund.de>

Roger Bivand wrote:
> On Mon, 5 Jun 2006, Liaw, Andy wrote:
> 
>> It was announced on the R-devel list:
>> http://tolstoy.newcastle.edu.au/R/devel/06/04/4923.html
>>
>> I don't see it on CRAN, either, nor could I find mention of it in the R News
>> you cited.
> 
> In the archive:
> 
> http://cran.r-project.org/src/contrib/Archive/D/data.table_1.0.tar.gz

AFAIR, the author (I am CCing) decided to remove it from CRAN's main 
repository because R has improved issues the package tries to address. 
Hence the package is no longer that beneficial in newer versions of R.

Uwe Ligges


> Roger
> 
>> Andy
>>
>> From: Eric Rexstad
>>> List:
>>>
>>> This package was described in R News of May 2006.  However, I 
>>> cannot find it on CRAN mirrors, and use of RSiteSearch is 
>>> unsatisfying.  I am likewise unable to find an email address 
>>> for the maintainer of the package.  Thank you in advance for 
>>> assistance.
>>>
>>> --
>>> Eric Rexstad
>>> Research Unit for Wildlife Population Assessment Centre for 
>>> Research into Ecological and Environmental Modelling 
>>> University of St. Andrews St. Andrews Scotland KY16 9LZ
>>> +44 (0)1334 461833
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>>
>>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>


From uofiowa at gmail.com  Mon Jun  5 22:24:00 2006
From: uofiowa at gmail.com (Omar Lakkis)
Date: Mon, 5 Jun 2006 16:24:00 -0400
Subject: [R] simpel POSIXlt question
Message-ID: <3f87cc6d0606051323q18cb8f86kedf30d72102f61d@mail.gmail.com>

How can I access, either to get or set, the  "sec", "min", "hour",
.... fields of a POSIXlt value as in this example?

> t= as.POSIXlt("2004-11-01")
> t
[1] "2004-11-01"
> unclass(t)
$sec
[1] 0

$min
[1] 0

$hour
[1] 0

$mday
[1] 1

$mon
[1] 10

$year
[1] 104

$wday
[1] 1

$yday
[1] 305

$isdst
[1] -1


From gunter.berton at gene.com  Mon Jun  5 22:55:43 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 5 Jun 2006 13:55:43 -0700
Subject: [R] Constrained regression
In-Reply-To: <a16e975c0606051301v575a59fcn7063229447754da8@mail.gmail.com>
Message-ID: <001f01c688e2$69548110$295afea9@gne.windows.gene.com>

If you haven't already done so, please make use of R's search capabilities
before posting.

help.search('constrained regression')
RSiteSearch('constrained regression') ## also available through CRAN's
search functionality.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Daniel Bogod
> Sent: Monday, June 05, 2006 1:01 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Constrained regression
> 
> Hi,
> I would like to run a constrained OLS, with the following constraints:
> 1. sum of coefficients must be 1
> 2. all the coefficients have to be positive.
> 
> Is there an eas way to specify that in R
> 
> Thank you,
>  Daniel Bogod
> dbogod at gmail.com
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From ripley at stats.ox.ac.uk  Mon Jun  5 23:09:25 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 5 Jun 2006 22:09:25 +0100 (BST)
Subject: [R] simpel POSIXlt question
In-Reply-To: <3f87cc6d0606051323q18cb8f86kedf30d72102f61d@mail.gmail.com>
References: <3f87cc6d0606051323q18cb8f86kedf30d72102f61d@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0606052207570.29576@gannet.stats.ox.ac.uk>

On Mon, 5 Jun 2006, Omar Lakkis wrote:

> How can I access, either to get or set, the  "sec", "min", "hour",
> .... fields of a POSIXlt value as in this example?

Just as for any other list:

> t <- as.POSIXlt("2004-11-01")
> t$sec
[1] 0
> t$sec <- 30
> t
[1] "2004-11-01 00:00:30"

and via [["sec"]] if you prefer.

[ ... ]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From vhawkes at lgl.com  Mon Jun  5 23:58:24 2006
From: vhawkes at lgl.com (Virgil Hawkes)
Date: Mon, 05 Jun 2006 14:58:24 -0700
Subject: [R] CCA Plot
Message-ID: <7.0.0.16.0.20060605145657.01f9e698@lgl.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060605/15098120/attachment.pl 

From pierce.gregory at gmail.com  Tue Jun  6 00:36:34 2006
From: pierce.gregory at gmail.com (Gregory Pierce)
Date: Mon, 05 Jun 2006 18:36:34 -0400
Subject: [R] Selective Survival Statistics with R
Message-ID: <1149546994.7761.34.camel@ibm-laptop>

Hello friends and fellow R users,

I have a problem to which I have been unable to find a solution: I am
gathering survival data on patients undergoing treatment with a new kind
of stent. I want to generate survival data and plot survival curves of
these patients based (among other things) on the treating physician. My
data set has been tabulated in the following manner:

Date	(the date the stent was implanted)   
Description (diameter of the stent)
Patient (name)
MRN (ID number)
Age (age in years of the patient)
Physician (last name of the implanting physician)
status (0 or 1)
days (days alive since the stenting procedure)
Cr 
INR
BR
MELD

The data set has over ten physicians. Following the examples in
"Introductory Statistics with R" I have been able to draw cumulative
survival curves and even the survival curves by physician but there are
so many curves on the graph when I do, it is uninterpretable.

I would just like to plot the survival curves of say the top two or
three. That would be much more useful. I have been able to create a
"Surv" object out of my own survival data and that of a colleague in the
following way using indexing:

>Surv(days[viatorr[6]=="Pierce"|
viatorr[6]=="Ed"],status[viatorr[6]=="Pierce"|viatorr[6]=="Ed"]==1)
[1] 558+ 474+ 472+ 446+ 443+ 429+ 401  395+ 390  390+ 362+ 354  344+ 342
326+
[16] 300+ 284+ 280+ 246+ 237+ 233+ 233  230+ 230+ 225+ 215  199+ 191+
191  184+
[31] 161+ 153  150  129+  69+  52+  38+

I can get even further:

surv.ee.gp<-Surv(days[viatorr[6]=="Pierce"|
viatorr[6]=="Ed"],status[viatorr[6]=="Pierce"|viatorr[6]=="Ed"]==1)
> survfit(surv.ee.gp)
Call: survfit(formula = surv.ee.gp)

      n  events  median 0.95LCL 0.95UCL
     37       9     Inf     390     Inf

But now if I want to plot the data using the following command

>plot(survfit(surv.ee.gp)~Physician)

I receive an error:

Error in model.frame(formula, rownames, variables, varnames, extras,
extranames,  :
        invalid variable type

I have tried indexing Physician, but that fails as well:

> plot(survfit(surv.ee.gp)~Physician[viatorr[6]=="Pierce"|
viatorr[6]=="Ed"]==1)
Error in model.frame(formula, rownames, variables, varnames, extras,
extranames,  :
        invalid variable type

I apologize for this long-winded explanation but I am new to this
program and even newer to Statistics and I wanted to make sure the
context and problem were clear (hopefully).

I would appreciate any guidance in a way forward. 

Thank you,
Greg


From pierce.gregory at gmail.com  Tue Jun  6 01:51:51 2006
From: pierce.gregory at gmail.com (Gregory Pierce)
Date: Mon, 05 Jun 2006 19:51:51 -0400
Subject: [R] Selective Survival Statistics with R
In-Reply-To: <56484410CF26D747A645BE746C57CE6DE9D426@SCIUSFREXS3.na.jnj.com>
References: <56484410CF26D747A645BE746C57CE6DE9D426@SCIUSFREXS3.na.jnj.com>
Message-ID: <1149551512.7761.46.camel@ibm-laptop>

On Mon, 2006-06-05 at 18:54 -0400, Barker, Chris [SCIUS] wrote:
> Its probably easiest/fastest for you either to subset your dataset
> first, or else simply use the subset option in survfit()
> 
>  e.g. 
> 
> survfit( ) has a subset option,  
> survfit( Surv( , ) ~ physician , subset=='Jones")
> 
Chris,

Thank you very much for your kind reply. Using subset worked. I had to
modify the syntax a little from what you posted:

survfit(Surv(days,status==1)~Physician,subset(viatorr,viatorr[6]=="Pierce")

where viatorr is the name of my data set.

Applying plot to the function above generated a survival curve for my
patients. I was also able to plot survival curves for other physicians
as well. This is great!


From pierce.gregory at gmail.com  Tue Jun  6 02:02:57 2006
From: pierce.gregory at gmail.com (Gregory Pierce)
Date: Mon, 05 Jun 2006 20:02:57 -0400
Subject: [R] Survival Statistics
Message-ID: <1149552177.7761.49.camel@ibm-laptop>

Hello friends and fellow R users,

I have a problem to which I have been unable to find a solution: I am
gathering survival data on patients undergoing treatment with a new kind
of stent. I want to generate survival data and plot survival curves of
these patients based (among other things) on the treating physician. My
data set has been tabulated in the following manner:

Date    (the date the stent was implanted)   
Description (diameter of the stent)
Patient (name)
MRN (ID number)
Age (age in years of the patient)
Physician (last name of the implanting physician)
status (0 or 1)
days (days alive since the stenting procedure)
Cr 
INR
BR
MELD

The data set has over ten physicians. Following the examples in
"Introductory Statistics with R" I have been able to draw cumulative
survival curves and even the survival curves by physician but there are
so many curves on the graph when I do, it is uninterpretable.

I would just like to plot the survival curves of say the top two or
three. That would be much more useful. I have been able to create a
"Surv" object out of my own survival data and that of a colleague in the
following way using indexing:

>Surv(days[viatorr[6]=="Pierce"|
viatorr[6]=="Ed"],status[viatorr[6]=="Pierce"|viatorr[6]=="Ed"]==1)
[1] 558+ 474+ 472+ 446+ 443+ 429+ 401  395+ 390  390+ 362+ 354  344+ 342
326+
[16] 300+ 284+ 280+ 246+ 237+ 233+ 233  230+ 230+ 225+ 215  199+ 191+
191  184+
[31] 161+ 153  150  129+  69+  52+  38+

I can get even further:

surv.ee.gp<-Surv(days[viatorr[6]=="Pierce"|
viatorr[6]=="Ed"],status[viatorr[6]=="Pierce"|viatorr[6]=="Ed"]==1)
> survfit(surv.ee.gp)
Call: survfit(formula = surv.ee.gp)

      n  events  median 0.95LCL 0.95UCL
     37       9     Inf     390     Inf

But now if I want to plot the data using the following command

>plot(survfit(surv.ee.gp)~Physician)

I receive an error:

Error in model.frame(formula, rownames, variables, varnames, extras,
extranames,  :
        invalid variable type

I have tried indexing Physician, but that fails as well:

> plot(survfit(surv.ee.gp)~Physician[viatorr[6]=="Pierce"|
viatorr[6]=="Ed"]==1)
Error in model.frame(formula, rownames, variables, varnames, extras,
extranames,  :
        invalid variable type

I apologize for this long-winded explanation but I am new to this
program and even newer to Statistics and I wanted to make sure the
context and problem were clear (hopefully).

I would appreciate any guidance in a way forward. 

Thank you,
Greg


From tlumley at u.washington.edu  Tue Jun  6 02:04:47 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 5 Jun 2006 17:04:47 -0700 (PDT)
Subject: [R] Selective Survival Statistics with R
In-Reply-To: <1149551512.7761.46.camel@ibm-laptop>
References: <56484410CF26D747A645BE746C57CE6DE9D426@SCIUSFREXS3.na.jnj.com>
	<1149551512.7761.46.camel@ibm-laptop>
Message-ID: <Pine.LNX.4.64.0606051703210.29721@homer22.u.washington.edu>

On Mon, 5 Jun 2006, Gregory Pierce wrote:
> On Mon, 2006-06-05 at 18:54 -0400, Barker, Chris [SCIUS] wrote:
>> Its probably easiest/fastest for you either to subset your dataset
>> first, or else simply use the subset option in survfit()
>>
>>  e.g.
>>
>> survfit( ) has a subset option,
>> survfit( Surv( , ) ~ physician , subset=='Jones")
>>
> Chris,
>
> Thank you very much for your kind reply. Using subset worked. I had to
> modify the syntax a little from what you posted:
>
> survfit(Surv(days,status==1)~Physician,subset(viatorr,viatorr[6]=="Pierce")
>
> where viatorr is the name of my data set.

An example of another approach is given in the example on the help page 
for survfit

You could do
   curves <- survfit(Surv(days,status==1)~Physician, data=viatorr)
   plot(curves[1])
   plot(curves[2])
etc.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From pierce.gregory at gmail.com  Tue Jun  6 02:23:38 2006
From: pierce.gregory at gmail.com (Gregory Pierce)
Date: Mon, 05 Jun 2006 20:23:38 -0400
Subject: [R] Selective Survival Statistics with R
In-Reply-To: <Pine.LNX.4.64.0606051703210.29721@homer22.u.washington.edu>
References: <56484410CF26D747A645BE746C57CE6DE9D426@SCIUSFREXS3.na.jnj.com>
	<1149551512.7761.46.camel@ibm-laptop>
	<Pine.LNX.4.64.0606051703210.29721@homer22.u.washington.edu>
Message-ID: <1149553418.7761.60.camel@ibm-laptop>

On Mon, 2006-06-05 at 17:04 -0700, Thomas Lumley wrote:
> On Mon, 5 Jun 2006, Gregory Pierce wrote:
> > On Mon, 2006-06-05 at 18:54 -0400, Barker, Chris [SCIUS] wrote:
> >> Its probably easiest/fastest for you either to subset your dataset
> >> first, or else simply use the subset option in survfit()
> >>
> >>  e.g.
> >>
> >> survfit( ) has a subset option,
> >> survfit( Surv( , ) ~ physician , subset=='Jones")
> >>
> > Chris,
> >
> > Thank you very much for your kind reply. Using subset worked. I had to
> > modify the syntax a little from what you posted:
> >
> > survfit(Surv(days,status==1)~Physician,subset(viatorr,viatorr[6]=="Pierce")
> >
> > where viatorr is the name of my data set.
> 
> An example of another approach is given in the example on the help page 
> for survfit
> 
> You could do
>    curves <- survfit(Surv(days,status==1)~Physician, data=viatorr)
>    plot(curves[1])
>    plot(curves[2])
> etc.
> 
>  	-thomas

That also worked great! Thanks. This makes me very,very happy!!!!!!! I
can now go to our statistician without looking like a complete idiot and
I can get to work collecting survival data on our patients who had the
old-fashioned bare-metal Wallstent. Looking back in this way provides a
very interesting and humbling perspective.

Greg


From spencer.graves at pdf.com  Tue Jun  6 02:51:21 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 05 Jun 2006 17:51:21 -0700
Subject: [R] time series clustering
In-Reply-To: <cdf817830606022044s766247d9o5da6e1442daffecd@mail.gmail.com>
References: <cdf817830606022044s766247d9o5da6e1442daffecd@mail.gmail.com>
Message-ID: <4484D189.6040303@pdf.com>

	  I know of no software for time series clustering in R.  Google 
produced some interesting hits for "time series clustering".  If you 
find an algorithm you like, the author might have software. 
Alternatively, the algorithm might be a modification of something 
already available in R.  If that's the case, you wouldn't need to start 
from scratch to program something since R is open source.

	  hope this helps.
	  Spencer Graves

Weiwei Shi wrote:
> Dear Listers:
> 
> I happened to have a problem requiring time-series clustering since the
> clusters will change with time (too old data need to be removed from data
> while new data comes in). I am wondering if there is some paper or reference
> on this topic and there is some kind of implementation in R?
> 
> Thanks,
> 
> Weiwei
>


From Mike.Lawrence at Dal.Ca  Tue Jun  6 05:20:08 2006
From: Mike.Lawrence at Dal.Ca (Mike Lawrence)
Date: Tue,  6 Jun 2006 00:20:08 -0300
Subject: [R] error bars in lattice xyplot *with groups*
Message-ID: <20060606002008.2dfnlqlmpw2ss4wc@my2.dal.ca>

Hi all,

I'm trying to plot error bars in a lattice plot generated with xyplot. Deepayan
Sarkar has provided a very useful solution for simple circumstances
(https://stat.ethz.ch/pipermail/r-help/2005-October/081571.html), yet I am
having trouble getting it to work when the "groups" setting is enabled in
xyplot (i.e. multiple lines). To illustrate this, consider the singer data
generated by the above linked solution previously submitted:

#####################
library(lattice)
singer.split <-
    with(singer,
         split(height, voice.part))

singer.ucl <-
    sapply(singer.split,
           function(x) {
               st <- boxplot.stats(x)
               c(st$stats[3], st$conf)
           })

singer.ucl <- as.data.frame(t(singer.ucl))
names(singer.ucl) <- c("median", "lower", "upper")
singer.ucl$voice.part <-
    factor(rownames(singer.ucl),
           levels = rownames(singer.ucl))

#now let's split up the voice.part factor into two factors,
singer.ucl$voice=factor(rep(c(1,2),4))
singer.ucl$range=factor(rep(c("Bass","Tenor","Alto","Soprano"),each=2))

#here's Deepayan's previous solution, slightly modified to depict
#  the dependent variable (median) and the error bars on the y-axis
#  and the independent variable (voice.part) on the x-axis
prepanel.ci <- function(x, y, ly, uy, subscripts, ...)
{
    x <- as.numeric(x)
    ly <- as.numeric(ly[subscripts])
    uy <- as.numeric(uy[subscripts])
    list(ylim = range(y, uy, ly, finite = TRUE))
}
panel.ci <- function(x, y, ly, uy, subscripts, pch = 16, ...)
{
    x <- as.numeric(x)
    y <- as.numeric(y)
    ly <- as.numeric(ly[subscripts])
    uy <- as.numeric(uy[subscripts])
    panel.arrows(x, ly, x, uy, col = "black",
                 length = 0.25, unit = "native",
                 angle = 90, code = 3)
    panel.xyplot(x, y, pch = pch, ...)
}


#this graph works
xyplot(median ~ voice.part,
	data=singer.ucl,
	ly = singer.ucl$lower,
	uy = singer.ucl$upper,
	prepanel = prepanel.ci,
	panel = panel.ci,
	type="b"
)

#this one does not (it will plot, but will not seperate the groups)
xyplot(median ~ voice,
	groups=range,
	data=singer.ucl,
	ly = singer.ucl$lower,
	uy = singer.ucl$upper,
	prepanel = prepanel.ci,
	panel = panel.ci,
	type="b"
)

####################################

Any suggestions?

-- 

Mike Lawrence

Mike.Lawrence at Dal.Ca

"The road to Wisdom? Well, it's plain and simple to express:
Err and err and err again, but less and less and less."
- Piet Hein


From mtmorgan at fhcrc.org  Tue Jun  6 05:34:18 2006
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Mon, 05 Jun 2006 20:34:18 -0700
Subject: [R] Fastest way to do HWE.exact test on 100K SNP data?
In-Reply-To: <1149535673.448485b924e4e@netmail.bsd.uchicago.edu> (Anna
	Pluzhnikov's message of "Mon,  5 Jun 2006 14:27:53 -0500")
References: <1149535673.448485b924e4e@netmail.bsd.uchicago.edu>
Message-ID: <6phverfvtxh.fsf@gopher1.fhcrc.org>

Anna --

This might be a little faster ...

all.geno.df <- data.frame(all.geno)     # exploit shared structure
set.seed(123)                           # once only
for (iter in 1:1000) {
    permut <- sample(affSt)
    control <- all.geno.df[permut==1,]
    case <- all.geno.df[permut==2,]
    pvalControls <- unlist(sapply(control, HWE.exact)["p.value",])
    pvalCases <- unlist(sapply(case, HWE.exact)["p.value",])
    # presmuably do something with these, before next iteration
}

but probably most of the time is being spent in HWE.exact so these
cosmetic changes won't help much.

Looking at the code for HWE.exact, it looks like it generates a table
based on numbers of alleles, and then uses this table to determine the
probability. Any time two SNPs have the same number of alleles, the
same table gets generated. It seems like this will happen alot. So one
strategy is to only calculate the table once, 'remember' it, and then
the next time a value is needed from that table look it up rather than
calculate it.

This is a bad strategy if most SNPs have different numbers of alleles
(because then the tables get calculated, and a lot of space and some
time is used to store results that are never accessed again).

Here is the code (not tested extensively, so please use with care!):

HWE.exact.lookup <- function() {
    HWE.lookup.tbl <- new.env(hash=TRUE)
    dhwe2 <- function(n11, n12, n22) {
        ## from body of HWE.exact
        f <- function(x) lgamma(x + 1)
        n <- n11 + n12 + n22
        n1 <- 2 * n11 + n12
        n2 <- 2 * n22 + n12
        exp(log(2) * (n12) + f(n) - f(n11) - f(n12) - f(n22) - 
            f(2 * n) + f(n1) + f(n2))
    }
    function (x) {
        ## adopted from HWE.exact
        if (!is.genotype(x)) 
          stop("x must be of class 'genotype' or 'haplotype'")
        nallele <- length(na.omit(allele.names(x)))
        if (nallele != 2) 
          stop("Exact HWE test can only be computed for 2 markers with 2 alleles")
        allele.tab <- table(factor(allele(x, 1), levels = allele.names(x)), 
                            factor(allele(x, 2), levels = allele.names(x)))
        n11 <- allele.tab[1, 1]
        n12 <- allele.tab[1, 2] + allele.tab[2, 1]
        n22 <- allele.tab[2, 2]
        n1 <- 2 * n11 + n12
        n2 <- 2 * n22 + n12

        nm <- paste(n1,n2,sep=".")
        if (!exists(nm, envir=HWE.lookup.tbl)) { # 'remember'
            x12 <- seq(n1%%2, min(n1, n2), 2)
            x11 <- (n1 - x12)/2
            x22 <- (n2 - x12)/2
            dist <- data.frame(n11 = x11, n12 = x12, n22 = x22,
                               density = dhwe2(x11, x12, x22))
            dist <- dist[order(dist$density),]
            dist$density <- cumsum(dist$density)
            assign(nm,dist, envir=HWE.lookup.tbl)
        }

        dist <- get(nm, HWE.lookup.tbl)
        dist$density[dist$n11 == n11 & dist$n12 == n12 & dist$n22 == n22]
    }
}

calcHWE <- HWE.exact.lookup()           # create a new lookup table & function
all.geno.df <- data.frame(all.geno)     # exploit shared structure
set.seed(123)                           # once only
for (iter in 1:1000) {
    permut <- sample(affSt)
    control <- all.geno.df[permut==1,]
    case <- all.geno.df[permut==2,]
    pvalControls <- sapply(control, calcHWE)
    pvalCases <- sapply(case, calcHWE)
}

Let me know how it goes if you adopt this approach!

Martin

Anna Pluzhnikov <apluzhni at bsd.uchicago.edu> writes:

> Hi everyone,
>
> I'm using the function 'HWE.exact' of 'genetics' package to compute p-values of
> the HWE test. My data set consists of ~600 subjects (cases and controls) typed
> at ~ 10K SNP markers; the test is applied separately to cases and controls. The
> genotypes are stored in a list of 'genotype' objects, all.geno, and p-values are
> calculated inside the loop over all SNP markers. 
>
> I wish to repeat this procedure multiple times (~1000) permuting the cases and
> controls (affection status). It seems straightforward to implement it like this:
>
> #############################################
>
> for (iter in 1:1000) {
>   set.seed(iter)
> # get the permuted affection status
>   permut <- sample(affSt)
>   for (j in 1:nSNPs) {
>     test <- tapply(all.geno[[j]], permut, HWE.exact)
>     pvalControls[j] <- test$"1"$p.value
>     pvalCases[j] <- test$"2"$p.value
>   }
> }
>
> ##############################################
>
> The problem is that it takes ~1 min/iteration (on AMD Opteron 252 processor
> running Linux). 
>
> Is there a faster/more efficient way to do this? 
>
> Thanks,
> -- 
> Anna Pluzhnikov, PhD
> Section of Genetic Medicine
> Department of Medicine
> The University of Chicago
>
>
> -------------------------------------------------
> This email is intended only for the use of the individual or...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ritwik.sinha at gmail.com  Tue Jun  6 07:01:23 2006
From: ritwik.sinha at gmail.com (Ritwik Sinha)
Date: Tue, 6 Jun 2006 01:01:23 -0400
Subject: [R] Fastest way to do HWE.exact test on 100K SNP data?
In-Reply-To: <6phverfvtxh.fsf@gopher1.fhcrc.org>
References: <1149535673.448485b924e4e@netmail.bsd.uchicago.edu>
	<6phverfvtxh.fsf@gopher1.fhcrc.org>
Message-ID: <42bc98300606052201xd774a46pc06f3d3163e3afd5@mail.gmail.com>

Hi Anna,

I am not really answering your question, but, here goes my (unsolicited)
suggestion. Some time back I did some simulations to see how the HWE tests
performed. In particular I compared the exact test and the chi squared test.
Look at the attached figure Rplots.ps. I saw that for null simulations with
40 individuals, the HWE chi squared test was reasonably close to the
(expected) uniform distribution. However this was obtained using the
function HWE.chisq(X, simulate.p.value=F), the default seemed to have some
issues. Hence my suggestion would be, if you feel comfortable, to replace
the exact test with a chi sq test, at least at the screening level. Once you
identify a set of SNPs with "small" p-values, you could follow them up with
the exact test.

-- 
Ritwik Sinha
Graduate Student
Epidemiology and Biostatistics
Case Western Reserve University

http://darwin.cwru.edu/~rsinha <http://darwin.cwru.edu/%7Ersinha>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Rplots.ps
Type: application/postscript
Size: 7129 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060606/d1bf16a8/attachment.ps 

From patrick.giraudoux at univ-fcomte.fr  Tue Jun  6 07:19:25 2006
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Tue, 06 Jun 2006 07:19:25 +0200
Subject: [R] default value for cutoff in gstat variogram()
In-Reply-To: <44848636.6020004@geo.uu.nl>
References: <448179EC.8040601@univ-fcomte.fr> <44848636.6020004@geo.uu.nl>
Message-ID: <4485105D.2050903@univ-fcomte.fr>



Edzer J Pebesma a ?crit :
> Patrick Giraudoux wrote:
>
>> I wonder what is the default value for the argument 'cutoff' when not 
>> specified in the variogram.formula function of gstat. Computing 
>> variogram envelops within gstat, I am comparing the results obtained 
>> with variog in geoR and variogram in gstat, and it took me a while 
>> before understanding that the cutoff default value is not the maximum 
>> distance.
>>
>> Can Edzer tell us about it?
>
> Yes, of course :
>
> the default value is computed in the c code. Without checking
> (meaning: from >10 years memory) I do recall that gstat uses
> one third of the diagional of the rectangular (or block for 3D)
> that spans the data locations.
>
> Why? In time series you compute ACF's up to one half
> of the length of the series; after this things start to oscillate
> because you lack independent replication at large distance;
> look at what is meant by ergodicity for further reading.
> Variograms are basically flipped & unscaled acf's for higher
> dimensions.
>
> Some books (Journel & Huijbregts?) gave suggestions
> that half the max. distance in the data set is a good guideline,
> back in 1978. I used  one third of the diagonal because
> I thought finding the maximum distance between any
> too point pairs may be expensive to find for large data
> sets. The parameter "one third" can be overridden by
> those who don't like it.
>
> Please keep us updated about your milage comparing
> gstat and geoR; I once spent an afternoon on this, trying
> to reproduce sample variogram across the packages and
> found this hard (but not impossible). I had the feeling
> it had to do with using < or <= to decide whether a
> point pairs falls in a distance interval or not, but didn't 100%
> assure myself.
> -- 
> Edzer
>


From thomas.hoffmann at uni-bonn.de  Tue Jun  6 09:17:11 2006
From: thomas.hoffmann at uni-bonn.de (Thomas Hoffmann)
Date: Tue, 06 Jun 2006 09:17:11 +0200
Subject: [R] 10 superscript x lables in log-plot
Message-ID: <44852BF7.1080902@uni-bonn.de>

Hi ListMembers,

I  created a log plot with

nums = c(1:10, seq(20,100, 10), seq(200,1000, 100),
         seq(2000,10000, 1000), seq(20000,100000, 10000),200000)
labl = rep("", length(nums))
labl[1] = "1"; labl[10] = "10"; labl[19] = "100";
labl[28] = "1.000"; labl[37] = "10.000"; labl[46] = "100.000"

plot(thickness_cm~area_km2, data=dat,
    log="x",
    xlab=c("catchment area [km?]"), xaxt="n", xlim=c(4,150000),
    ylab=c("thickness [cm]"),ylim=c(0,1000))
axis(1, at = nums, label = labl, las = 2)

however I would like to use lables of the form 10 superscript 5. 
Therefore I tried for example

labl[37]=expression(10^5)

which results in some error messages. Does anybody knows how to solve 
the problem?

Thanks in advance
Thomas


From deepayan.sarkar at gmail.com  Tue Jun  6 09:31:00 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 6 Jun 2006 02:31:00 -0500
Subject: [R] error bars in lattice xyplot *with groups*
In-Reply-To: <20060606002008.2dfnlqlmpw2ss4wc@my2.dal.ca>
References: <20060606002008.2dfnlqlmpw2ss4wc@my2.dal.ca>
Message-ID: <eb555e660606060031v6af23156lb675517291b2ce2a@mail.gmail.com>

On 6/5/06, Mike Lawrence <Mike.Lawrence at dal.ca> wrote:
> Hi all,
>
> I'm trying to plot error bars in a lattice plot generated with xyplot. Deepayan
> Sarkar has provided a very useful solution for simple circumstances
> (https://stat.ethz.ch/pipermail/r-help/2005-October/081571.html), yet I am
> having trouble getting it to work when the "groups" setting is enabled in
> xyplot (i.e. multiple lines). To illustrate this, consider the singer data
> generated by the above linked solution previously submitted:
>
> #####################
> library(lattice)
> singer.split <-
>     with(singer,
>          split(height, voice.part))
>
> singer.ucl <-
>     sapply(singer.split,
>            function(x) {
>                st <- boxplot.stats(x)
>                c(st$stats[3], st$conf)
>            })
>
> singer.ucl <- as.data.frame(t(singer.ucl))
> names(singer.ucl) <- c("median", "lower", "upper")
> singer.ucl$voice.part <-
>     factor(rownames(singer.ucl),
>            levels = rownames(singer.ucl))
>
> #now let's split up the voice.part factor into two factors,
> singer.ucl$voice=factor(rep(c(1,2),4))
> singer.ucl$range=factor(rep(c("Bass","Tenor","Alto","Soprano"),each=2))
>
> #here's Deepayan's previous solution, slightly modified to depict
> #  the dependent variable (median) and the error bars on the y-axis
> #  and the independent variable (voice.part) on the x-axis
> prepanel.ci <- function(x, y, ly, uy, subscripts, ...)
> {
>     x <- as.numeric(x)
>     ly <- as.numeric(ly[subscripts])
>     uy <- as.numeric(uy[subscripts])
>     list(ylim = range(y, uy, ly, finite = TRUE))
> }
> panel.ci <- function(x, y, ly, uy, subscripts, pch = 16, ...)
> {
>     x <- as.numeric(x)
>     y <- as.numeric(y)
>     ly <- as.numeric(ly[subscripts])
>     uy <- as.numeric(uy[subscripts])
>     panel.arrows(x, ly, x, uy, col = "black",
>                  length = 0.25, unit = "native",
>                  angle = 90, code = 3)
>     panel.xyplot(x, y, pch = pch, ...)
> }
>
>
> #this graph works
> xyplot(median ~ voice.part,
>         data=singer.ucl,
>         ly = singer.ucl$lower,
>         uy = singer.ucl$upper,
>         prepanel = prepanel.ci,
>         panel = panel.ci,
>         type="b"
> )
>
> #this one does not (it will plot, but will not seperate the groups)
> xyplot(median ~ voice,
>         groups=range,
>         data=singer.ucl,
>         ly = singer.ucl$lower,
>         uy = singer.ucl$upper,
>         prepanel = prepanel.ci,
>         panel = panel.ci,
>         type="b"
> )
>
> ####################################
>
> Any suggestions?

Use a panel function that knows about groups, e.g.

xyplot(median ~ voice,
        groups=range,
        data=singer.ucl,
        ly = singer.ucl$lower,
        uy = singer.ucl$upper,
        prepanel = prepanel.ci,
        panel = panel.superpose,
        panel.groups = panel.ci,
        type="b")

If you prefer colored bars, change panel.ci to


panel.ci <- function(x, y, ly, uy, subscripts, pch = 16, col.line =
'black', ...)
{
    x <- as.numeric(x)
    y <- as.numeric(y)
    ly <- as.numeric(ly[subscripts])
    uy <- as.numeric(uy[subscripts])
    panel.arrows(x, ly, x, uy, col = col.line,
                 length = 0.25, unit = "native",
                 angle = 90, code = 3)
    panel.xyplot(x, y, pch = pch, col.line = col.line, ...)
}

Deepayan


From dimitris.rizopoulos at med.kuleuven.be  Tue Jun  6 09:42:31 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 6 Jun 2006 09:42:31 +0200
Subject: [R] 10 superscript x lables in log-plot
References: <44852BF7.1080902@uni-bonn.de>
Message-ID: <00c901c6893c$c4bd6f30$0540210a@www.domain>

maybe the first example of ?plotmath could be of help.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Thomas Hoffmann" <thomas.hoffmann at uni-bonn.de>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, June 06, 2006 9:17 AM
Subject: [R] 10 superscript x lables in log-plot


Hi ListMembers,

I  created a log plot with

nums = c(1:10, seq(20,100, 10), seq(200,1000, 100),
         seq(2000,10000, 1000), seq(20000,100000, 10000),200000)
labl = rep("", length(nums))
labl[1] = "1"; labl[10] = "10"; labl[19] = "100";
labl[28] = "1.000"; labl[37] = "10.000"; labl[46] = "100.000"

plot(thickness_cm~area_km2, data=dat,
    log="x",
    xlab=c("catchment area [km?]"), xaxt="n", xlim=c(4,150000),
    ylab=c("thickness [cm]"),ylim=c(0,1000))
axis(1, at = nums, label = labl, las = 2)

however I would like to use lables of the form 10 superscript 5.
Therefore I tried for example

labl[37]=expression(10^5)

which results in some error messages. Does anybody knows how to solve
the problem?

Thanks in advance
Thomas

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From xmeng at capitalbio.com  Tue Jun  6 10:40:15 2006
From: xmeng at capitalbio.com (XinMeng)
Date: Tue, 06 Jun 2006 16:40:15 +0800
Subject: [R] about string
Message-ID: <349583215.23119@capitalbio.com>

Hello sir:
There are 2 questions about string.
1 How to calculate the width of a string? e.g string "abc"'s width is 3;
2 How can I get the "substring" in such kind of condition:
  "f:\\JPCS_signal.txt"  "f:\\PC1_signal.txt" "f:\\PC2_signal.txt"
 What I wanna get is "JPCS"  "PC1"  "PC2".How can I achieve them by R cammand?

Thanks a lot!

My best




------------------------------
*******************************************
Xin Meng 
Capitalbio Corporation
National Engineering Research Center 
for Beijing Biochip Technology 
BioPharma-informatics & Software Dept. 
Research Engineer
Tel: +86-10-80715888/80726868-6438
Fax: +86-10-80726790
Email??xmeng at capitalbio.com 
Address:18 Life Science Parkway, 
Changping District, Beijing 102206, China


From dimitris.rizopoulos at med.kuleuven.be  Tue Jun  6 10:24:41 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 6 Jun 2006 10:24:41 +0200
Subject: [R] about string
References: <349583215.23119@capitalbio.com>
Message-ID: <00fb01c68942$a86e3700$0540210a@www.domain>

try the following:

strg <- "abc"
nchar(strg)
############
strg <- c("f:\\JPCS_signal.txt", "f:\\PC1_signal.txt",
          "f:\\PC2_signal.txt")
strg. <- sapply(strsplit(strg, "\\\\"), "[", 2)
sapply(strsplit(strg., "_"), "[", 1)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "XinMeng" <xmeng at capitalbio.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, June 06, 2006 10:40 AM
Subject: [R] about string


> Hello sir:
> There are 2 questions about string.
> 1 How to calculate the width of a string? e.g string "abc"'s width 
> is 3;
> 2 How can I get the "substring" in such kind of condition:
>  "f:\\JPCS_signal.txt"  "f:\\PC1_signal.txt" "f:\\PC2_signal.txt"
> What I wanna get is "JPCS"  "PC1"  "PC2".How can I achieve them by R 
> cammand?
>
> Thanks a lot!
>
> My best
>
>
>
>
> ------------------------------
> *******************************************
> Xin Meng
> Capitalbio Corporation
> National Engineering Research Center
> for Beijing Biochip Technology
> BioPharma-informatics & Software Dept.
> Research Engineer
> Tel: +86-10-80715888/80726868-6438
> Fax: +86-10-80726790
> Email??xmeng at capitalbio.com
> Address:18 Life Science Parkway,
> Changping District, Beijing 102206, China
>
>


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From andrej.kastrin at siol.net  Tue Jun  6 10:29:23 2006
From: andrej.kastrin at siol.net (Andrej Kastrin)
Date: Tue, 06 Jun 2006 10:29:23 +0200
Subject: [R] time series clustering
In-Reply-To: <4484D189.6040303@pdf.com>
References: <cdf817830606022044s766247d9o5da6e1442daffecd@mail.gmail.com>
	<4484D189.6040303@pdf.com>
Message-ID: <44853CE3.3080007@siol.net>

Spencer Graves wrote:
> 	  I know of no software for time series clustering in R.  Google 
> produced some interesting hits for "time series clustering".  If you 
> find an algorithm you like, the author might have software. 
> Alternatively, the algorithm might be a modification of something 
> already available in R.  If that's the case, you wouldn't need to start 
> from scratch to program something since R is open source.
>
> 	  hope this helps.
> 	  Spencer Graves
>
> Weiwei Shi wrote:
>   
>> Dear Listers:
>>
>> I happened to have a problem requiring time-series clustering since the
>> clusters will change with time (too old data need to be removed from data
>> while new data comes in). I am wondering if there is some paper or reference
>> on this topic and there is some kind of implementation in R?
>>
>> Thanks,
>>
>> Weiwei
>>
>>     
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>   
Try to find the following article: Qualitative Clustering of Short 
Time-Series: A Case Study of Enterprise Reputation Data. It's a 
conference paper, but I have no electronic version of it.

HTH, Andrej


From jacques.veslot at good.ibl.fr  Tue Jun  6 10:27:17 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Tue, 06 Jun 2006 10:27:17 +0200
Subject: [R] about string
In-Reply-To: <349583215.23119@capitalbio.com>
References: <349583215.23119@capitalbio.com>
Message-ID: <44853C65.9030105@good.ibl.fr>

?nchar
sapply(strsplit("f:\\JPCS_signal.txt", "_"), function(x) substring(x[1], 4))
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------

XinMeng a ?crit :
> Hello sir:
> There are 2 questions about string.
> 1 How to calculate the width of a string? e.g string "abc"'s width is 3;
> 2 How can I get the "substring" in such kind of condition:
>   "f:\\JPCS_signal.txt"  "f:\\PC1_signal.txt" "f:\\PC2_signal.txt"
>  What I wanna get is "JPCS"  "PC1"  "PC2".How can I achieve them by R cammand?
> 
> Thanks a lot!
> 
> My best
> 
> 
> 
> 
> ------------------------------
> *******************************************
> Xin Meng 
> Capitalbio Corporation
> National Engineering Research Center 
> for Beijing Biochip Technology 
> BioPharma-informatics & Software Dept. 
> Research Engineer
> Tel: +86-10-80715888/80726868-6438
> Fax: +86-10-80726790
> Email??xmeng at capitalbio.com 
> Address:18 Life Science Parkway, 
> Changping District, Beijing 102206, China
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From tuechler at gmx.at  Tue Jun  6 11:41:56 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Tue, 06 Jun 2006 10:41:56 +0100
Subject: [R] How to call a value labels attribute?
In-Reply-To: <20060605100134.BCC58015@po-d.temple.edu>
Message-ID: <3.0.6.32.20060606104156.00ad01d0@pop.gmx.net>

Thank you, Richard. As soon as I find time I will carefully look at your
solution and your book.

Heinz

At 10:01 05.06.2006 -0400, Richard M. Heiberger wrote:
>Aha!  Thank you for the more detailed example.
>
>My solution for that situation is an attribute "position" and function
>as.position().  I use this in my book
>
>                Statistical Analysis and Data Display
>                Richard M. Heiberger and Burt Holland
>
>The online files for the book are available at
>                http://springeronline.com/0-387-40270-5
>
>
>
>For this example, you need the function as.position() included in this
>email.
>
>
>### example ########
>x <- ordered(c(1,2,3,2,4,3,1,2,4,3,2,1,3),
>             labels=c("small", "medium", "large", "very.large"))
>x
>attr(x, "position") <- c(1,2,4,8)
>x
>as.position(x)
>
>y <- rnorm(length(x))
>y
>
>xyplot(y ~ x)
>source("~/h2/library/code/as.position.s")
>xyplot(y ~ as.position(x))
>xyplot(y ~ as.position(x),
>       scale=list(x=list(at=attr(x,"position"), labels=levels(x))))
>xyplot(y ~ as.position(x),
>       scale=list(x=list(at=attr(x,"position"), labels=levels(x))),
>       xlab="x")
>### end example ########
>
>
>
>### as.position.s #########
>as.position <- function(x) {
>  if (is.numeric(x))
>    x
>  else {
>    if (!is.factor(x)) stop("x must be either numeric or factor.")
>
>    if (!is.null(attr(x, "position")))
>      x <- attr(x, "position")[x]
>    else {
>      lev.x <- levels(x)
>      if (inherits(x, "ordered")) {
>        on.exit(options(old.warn))
>        old.warn <- options(warn=-1)
>        if (!any(is.na(as.numeric(lev.x))))
>          x <- as.numeric(lev.x)[x]
>        else
>          x <- as.numeric(ordered(lev.x, lev.x))[x]
>      }
>      else
>        x <- as.numeric(x)
>    }
>  }
>  x
>}
>
>
>## tmp <- ordered(c("c","b","f","f","c","b"), c("c","b","f"))
>## as.numeric(tmp)
>## as.position(tmp)
>## 
>## tmp <- factor(c("c","b","f","f","c","b"))
>## as.numeric(tmp)
>## as.position(tmp)
>## 
>## tmp <- factor(c(1,3,5,3,5,1))
>## as.numeric(tmp)
>## as.position(tmp)
>## 
>## tmp <- ordered(c(1,3,5,3,5,1))
>## as.numeric(tmp)
>## as.position(tmp)
>## 
>## tmp <- c(1,3,5,3,5,1)
>## as.numeric(tmp)
>## as.position(tmp)
>
>### end as.position.s #########
>
>


From news.ftr at free.fr  Tue Jun  6 11:06:00 2006
From: news.ftr at free.fr (Frank Thomas)
Date: Tue, 06 Jun 2006 11:06:00 +0200
Subject: [R] SPSS variable lables import
Message-ID: <44854578.7050507@free.fr>

Hi,
I try to get the variable labels of a SPSS data file into R but don't 
find this mentioned in the help file for foreign. Is there another way 
to get them ?
BTW: An SPSS variable name is like: VAR001, whereas the variable label 
might be 'Identification no.'

Thanks in advance,
F. Thomas

-- 
..........................................
Dr. Frank Thomas
FTR Internet Research
93110 Rosny-sous-Bois
France



--


From jacques.veslot at good.ibl.fr  Tue Jun  6 11:26:14 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Tue, 06 Jun 2006 11:26:14 +0200
Subject: [R] SPSS variable lables import
In-Reply-To: <44854578.7050507@free.fr>
References: <44854578.7050507@free.fr>
Message-ID: <44854A36.6020600@good.ibl.fr>

data1 <- read.spss("file1.sav", F, T)   # works !
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


Frank Thomas a ?crit :
> Hi,
> I try to get the variable labels of a SPSS data file into R but don't 
> find this mentioned in the help file for foreign. Is there another way 
> to get them ?
> BTW: An SPSS variable name is like: VAR001, whereas the variable label 
> might be 'Identification no.'
> 
> Thanks in advance,
> F. Thomas
>


From ripley at stats.ox.ac.uk  Tue Jun  6 11:30:22 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 6 Jun 2006 10:30:22 +0100 (BST)
Subject: [R] about string
In-Reply-To: <00fb01c68942$a86e3700$0540210a@www.domain>
References: <349583215.23119@capitalbio.com>
	<00fb01c68942$a86e3700$0540210a@www.domain>
Message-ID: <Pine.LNX.4.64.0606061028120.19832@gannet.stats.ox.ac.uk>

On Tue, 6 Jun 2006, Dimitris Rizopoulos wrote:

> try the following:
>
> strg <- "abc"
> nchar(strg)
> ############
> strg <- c("f:\\JPCS_signal.txt", "f:\\PC1_signal.txt",
>          "f:\\PC2_signal.txt")
> strg. <- sapply(strsplit(strg, "\\\\"), "[", 2)
> sapply(strsplit(strg., "_"), "[", 1)

or (a bit simpler)

> gsub("f:\\\\([^_]*).*", "\\1", strg)
[1] "JPCS" "PC1"  "PC2"


>
>
> I hope it helps.
>
> Best,
> Dimitris
>
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
>
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://med.kuleuven.be/biostat/
>     http://www.student.kuleuven.be/~m0390867/dimitris.htm
>
>
> ----- Original Message -----
> From: "XinMeng" <xmeng at capitalbio.com>
> To: <r-help at stat.math.ethz.ch>
> Sent: Tuesday, June 06, 2006 10:40 AM
> Subject: [R] about string
>
>
>> Hello sir:
>> There are 2 questions about string.
>> 1 How to calculate the width of a string? e.g string "abc"'s width
>> is 3;
>> 2 How can I get the "substring" in such kind of condition:
>>  "f:\\JPCS_signal.txt"  "f:\\PC1_signal.txt" "f:\\PC2_signal.txt"
>> What I wanna get is "JPCS"  "PC1"  "PC2".How can I achieve them by R
>> cammand?
>>
>> Thanks a lot!
>>
>> My best
>>
>>
>>
>>
>> ------------------------------
>> *******************************************
>> Xin Meng
>> Capitalbio Corporation
>> National Engineering Research Center
>> for Beijing Biochip Technology
>> BioPharma-informatics & Software Dept.
>> Research Engineer
>> Tel: +86-10-80715888/80726868-6438
>> Fax: +86-10-80726790
>> Email??xmeng at capitalbio.com
>> Address:18 Life Science Parkway,
>> Changping District, Beijing 102206, China
>>
>>
>
>
> --------------------------------------------------------------------------------
>
>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From Fabian.Barth at web.de  Tue Jun  6 12:13:22 2006
From: Fabian.Barth at web.de (Fabian Barth)
Date: Tue, 06 Jun 2006 12:13:22 +0200
Subject: [R] Problems using quadprog for solving quadratic programming
	problem
Message-ID: <723110647@web.de>

Hi,

I'm using the package quadprog to solve the following quadratic programming problem.

I want to minimize the function 
(b_1-b_2)^2+(b_3-b_4)^2
by the following constraints b_i, i=1,...,4:


b_1+b_3=1
b_2+b_4=1

0.1<=b_1<=0.2
0.2<=b_2<=0.4
0.8<=b_3<=0.9
0.6<=b_4<=0.8

In my opinion the solution should be b_1=b_2=0.2 und b_3=b_4=0.8.

Unfortunately R doesn't find this solution and what's surprising to me, evaluation the solution of solve.QP with my function doesn't lead the minimal "value" calculated by solve.QP

I would be very happy, if anyone could help and tell me, where's my mistake. Thank you very much. Fabian

My R-code also containing the sampel of quadprog starts here:

#sample from quadprog package
library(quadprog)

Dmat       <- matrix(0,3,3)
diag(Dmat) <- 1
dvec       <- c(0,5,0)
Amat       <- matrix(c(-4,-3,0,2,1,0,0,-2,1),3,3)
bvec       <- c(-8,2,0)
erg<-solve.QP(Dmat,dvec,Amat,bvec=bvec)

print(erg)


erg<-erg$solution
-dvec%*%erg+.5*erg^T%*%Dmat%*%erg


# my "non working" sample

n<-2
k<-2
Dmatj<-diag((n-1),n)
Dmatj[lower.tri(Dmatj)]<--2
Dmat<-matrix(0,nrow=n*k,ncol=n*k)
for(j in 1:k){
  Dmat[((j-1)*n+1):(j*n),((j-1)*n+1):(j*n)]<-Dmatj
}
print(Dmat)
Amat<-matrix(ncol=n,nrow=k*n,0)
ind<-seq(1,((k-1)*n+1),n)
for(i in 1:n){
  Amat[(ind+(i-1)),i]<-1
}
Amat<-cbind(Amat,diag(n*k),-diag(n*k))
print(Amat)
bvec<-c(rep(1,n),c(0.1,0.2,0.8,0.6,-0.2,-0.4,-0.9,-0.8))
print(bvec)
dvec=rep(0,(n*k))
  
erg<-solve.QP(Dmat,dvec,Amat,bvec,meq=n)

print(erg)


erg<-erg$solution
-dvec%*%erg+.5*erg^T%*%Dmat%*%erg


From berwin at maths.uwa.edu.au  Tue Jun  6 13:05:29 2006
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Tue, 6 Jun 2006 19:05:29 +0800
Subject: [R] Problems using quadprog for solving quadratic
	programming	problem
In-Reply-To: <723110647@web.de>
References: <723110647@web.de>
Message-ID: <17541.24953.924277.308037@bossiaea.maths.uwa.edu.au>

G'day Fabian,

>>>>> "FB" == Fabian Barth <Fabian.Barth at web.de> writes:

    FB> I'm using the package quadprog to solve the following
    FB> quadratic programming problem.

    FB> I want to minimize the function 
    FB> (b_1-b_2)^2+(b_3-b_4)^2
    FB> by the following constraints b_i, i=1,...,4:


    FB> b_1+b_3=1
    FB> b_2+b_4=1

    FB> 0.1<=b_1<=0.2
    FB> 0.2<=b_2<=0.4
    FB> 0.8<=b_3<=0.9
    FB> 0.6<=b_4<=0.8

    FB> In my opinion the solution should be b_1=b_2=0.2 und b_3=b_4=0.8.
This could well be the correct solution.  For these values for the b_i
the function evaluates to zero and you definitely won't get anything
smaller than that. :)

    FB> Unfortunately R doesn't find this solution and what's
    FB> surprising to me, evaluation the solution of solve.QP with my
    FB> function doesn't lead the minimal "value" calculated by
    FB> solve.QP

    FB> I would be very happy, if anyone could help and tell me,
    FB> where's my mistake.
Mmh, the first code that you are sending seems to involve only three
unknowns.  Did you try to eliminate one by hand?  If so, which one.
Also, in that part of the code you do not tell solve.QP that there are
equality constraints.  So I am a bit confused what problem the first
part is supposed to solve. :-)

The second code, labelled `my "non working" sample' seems to implement
the above problem directly.  The problem with that code is that the
matrix Dmat that you construct is not symmetric, there is the
(apparently undocumented and unchecked) assumption that Dmat is
symmetric.  (Note if the matrix D in the quadratic form is not
summetric, then you can always replace it by (D+D^T)/2 without
changing the objective function.  Thus, without loss of generality,
the matrix D in the objective function is always assumed to be
symmetric).  So adding code like
          Dmat <- (Dmat+t(Dmat))/2
in front of your `print(Dmat)' statement would be necessary.

Two minor observations.  First, note that the help page for solve.QP
states that the function to be minimised is
        -d^T b + 1/2 b^T D b
where D is passed in Dmat and d is passed in dvec.  In your case it
might not matter, but strictly speaking, you should multiply your Dmat
by 2 to take care of the 1/2 factor in the definition of the objective
function.  Secondly, the FORTRAN code that is used by solve.QP only
uses the upper triangular part of the matrix Dmat that is passed to
solve.QP.  Thus, essentially you were minimizing
        min 1/2 (b_1^2 + b_2^2 + b_3^2 + b_4^2)
under all the conditions stated.  And, for that problem solve.QP gave
the correct answer.

Once you correct the problem, and a faster way of coding your problem
is probabably the following:

> Dmat1 <- matrix(c(2, -2, -2, 2), 2, 2)
> Dmat2 <- matrix(0, 2, 2)
> Dmat <- rbind(cbind(Dmat1, Dmat2), cbind(Dmat2, Dmat1))
> dvec <- rep(0,4)
> Amat <- cbind(c(1,0,1,0), c(0,1,0,1), diag(4), -diag(4))
> bvec <- c(1, 1, 0.1, 0.2, 0.8, 0.6, -0.2, -0.4, -0.9, -0.8)

you will see that there is still a problem:

> solve.QP(Dmat,dvec,Amat,bvec=bvec, meq=2)
Error in solve.QP(Dmat, dvec, Amat, bvec = bvec, meq = 2) : 
	matrix D in quadratic function is not positive definite!

The Goldfarb-Idnani algorithm starts off by calculating the
unconstrained solution.  Thus, it requires that the matrix D in the
objective function is positive definite.  In your problem the matrix
is indefinite.

Hope this helps.

Cheers,

        Berwin

========================== Full address ============================
Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)   
School of Mathematics and Statistics        +61 (8) 6488 3383 (self)      
The University of Western Australia   FAX : +61 (8) 6488 1028
35 Stirling Highway                   
Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
Australia                        http://www.maths.uwa.edu.au/~berwin


From bgreen at dyson.brisnet.org.au  Tue Jun  6 13:52:11 2006
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Tue, 06 Jun 2006 21:52:11 +1000
Subject: [R] Error in inherits(x, "data.frame") : object "Dataset" not found
In-Reply-To: <mailman.9.1149588003.27900.r-help@stat.math.ethz.ch>
Message-ID: <5.1.0.14.0.20060606214857.02717dc8@pop3.brisnet.org.au>

I have been trying to run a logistic regression using a number of studies. 
Below is the syntax, error message & data.

Any advice regarding what I am doing wrong or solutions are appreciated,

regards

Bob Green


 > logreg <- read.csv("c:\\logregtest.csv",header=T)
 > attach(logreg)
 > names(logreg)
  [1] "medyear"   "where"     "who"       "dxbroad"   "firstep"   "standard"
  [7] "age"       "sex"       "successes" "failures"
 > model <- glm(cbind(successes, failures) ~medyear + age + sex +  where + 
who + dxbroad + firstep + standard, family=binomial, data=Dataset)
Error in inherits(x, "data.frame") : object "Dataset" not found


medyear	where	who	dxbroad	firstep	standard	age	sex	successes	failures
89	3	2	1	0	0	31.5	71	28	117
98	2	2	1	0	1	48	62	15	72
98	4	1	1	0	0	45.2	61	42	57
89	3	0	1	0	1	28.7	63	19	48
99	2	2	1	0	1	34.7	73	27	73
88	3	0	1	0	1	30.6	58	26	57
94	1	1	1	0	1	36.3	81	70	124
96	3	1	1	0	1	40	57	27	40
96	2	2	1	0	1	33.1	64	9	41
88	2	0	1	1	0	29.5	47	30	202
98	1	2	0	0	1	39.3	60	246	734
97	4	0	0	0	1	38.4	67	17	85
92	3	0	1	0	1	34.3	67	15	127
88	2	0	1	0	1	NA	46	9	90
85	3	0	1	0	1	30.3	64	58	87
94	3	0	1	0	1	38.8	47	47	126
88	3	0	1	0	1	33.8	54	25	134
92	3	0	1	1	1	NA	NA	67	157
90	3	0	1	1	1	26	52	17	101
90	3	0	1	0	0	NA	NA	39	32
90	2	0	1	0	1	36.1	38	10	173
90	2	0	1	0	1	38.9	53	64	383
97	2	0	1	0	1	31.5	61	12	52
99	1	1	1	0	1	NA	NA	25	56
100	4	1	1	0	1	45	62	46	270
101	2	0	1	0	1	32.4	100	33	92


From dimitris.rizopoulos at med.kuleuven.be  Tue Jun  6 14:11:24 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 6 Jun 2006 14:11:24 +0200
Subject: [R] Error in inherits(x,
	"data.frame") : object "Dataset" not found
References: <5.1.0.14.0.20060606214857.02717dc8@pop3.brisnet.org.au>
Message-ID: <002401c68962$54aecce0$0540210a@www.domain>

you probably want to use:

model <- glm(cbind(successes, failures) ~ medyear + age + sex + where 
+ who + dxbroad + firstep + standard, family = binomial, data = 
logreg)

since you store the data you imported in the data.frame 'logreg' not 
'Dataset'.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Bob Green" <bgreen at dyson.brisnet.org.au>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, June 06, 2006 1:52 PM
Subject: [R] Error in inherits(x, "data.frame") : object "Dataset" not 
found


>I have been trying to run a logistic regression using a number of 
>studies.
> Below is the syntax, error message & data.
>
> Any advice regarding what I am doing wrong or solutions are 
> appreciated,
>
> regards
>
> Bob Green
>
>
> > logreg <- read.csv("c:\\logregtest.csv",header=T)
> > attach(logreg)
> > names(logreg)
>  [1] "medyear"   "where"     "who"       "dxbroad"   "firstep" 
> "standard"
>  [7] "age"       "sex"       "successes" "failures"
> > model <- glm(cbind(successes, failures) ~medyear + age + sex + 
> > where +
> who + dxbroad + firstep + standard, family=binomial, data=Dataset)
> Error in inherits(x, "data.frame") : object "Dataset" not found
>
>
> medyear where who dxbroad firstep standard age sex successes 
> failures
> 89 3 2 1 0 0 31.5 71 28 117
> 98 2 2 1 0 1 48 62 15 72
> 98 4 1 1 0 0 45.2 61 42 57
> 89 3 0 1 0 1 28.7 63 19 48
> 99 2 2 1 0 1 34.7 73 27 73
> 88 3 0 1 0 1 30.6 58 26 57
> 94 1 1 1 0 1 36.3 81 70 124
> 96 3 1 1 0 1 40 57 27 40
> 96 2 2 1 0 1 33.1 64 9 41
> 88 2 0 1 1 0 29.5 47 30 202
> 98 1 2 0 0 1 39.3 60 246 734
> 97 4 0 0 0 1 38.4 67 17 85
> 92 3 0 1 0 1 34.3 67 15 127
> 88 2 0 1 0 1 NA 46 9 90
> 85 3 0 1 0 1 30.3 64 58 87
> 94 3 0 1 0 1 38.8 47 47 126
> 88 3 0 1 0 1 33.8 54 25 134
> 92 3 0 1 1 1 NA NA 67 157
> 90 3 0 1 1 1 26 52 17 101
> 90 3 0 1 0 0 NA NA 39 32
> 90 2 0 1 0 1 36.1 38 10 173
> 90 2 0 1 0 1 38.9 53 64 383
> 97 2 0 1 0 1 31.5 61 12 52
> 99 1 1 1 0 1 NA NA 25 56
> 100 4 1 1 0 1 45 62 46 270
> 101 2 0 1 0 1 32.4 100 33 92
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ripley at stats.ox.ac.uk  Tue Jun  6 14:18:12 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 6 Jun 2006 13:18:12 +0100 (BST)
Subject: [R] Error in inherits(x,
 "data.frame") : object "Dataset" not found
In-Reply-To: <5.1.0.14.0.20060606214857.02717dc8@pop3.brisnet.org.au>
References: <5.1.0.14.0.20060606214857.02717dc8@pop3.brisnet.org.au>
Message-ID: <Pine.LNX.4.64.0606061315280.20225@gannet.stats.ox.ac.uk>

On Tue, 6 Jun 2006, Bob Green wrote:

> I have been trying to run a logistic regression using a number of studies.
> Below is the syntax, error message & data.
>
> Any advice regarding what I am doing wrong or solutions are appreciated,

I suspect you wanted

logreg <- read.csv("c:\\logregtest.csv", header=TRUE)
model <- glm(cbind(successes, failures) ~ ., family=binomial, data=logreg)

>
> regards
>
> Bob Green
>
>
> > logreg <- read.csv("c:\\logregtest.csv",header=T)
> > attach(logreg)
> > names(logreg)
>  [1] "medyear"   "where"     "who"       "dxbroad"   "firstep"   "standard"
>  [7] "age"       "sex"       "successes" "failures"
> > model <- glm(cbind(successes, failures) ~medyear + age + sex +  where +
> who + dxbroad + firstep + standard, family=binomial, data=Dataset)
> Error in inherits(x, "data.frame") : object "Dataset" not found

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From sawn05 at sigact.cs.unlv.edu  Tue Jun  6 16:12:03 2006
From: sawn05 at sigact.cs.unlv.edu (SAWN_2005)
Date: Tue, 6 Jun 2006 07:12:03 -0700
Subject: [R] "Returned mail: see transcript for details"
Message-ID: <200606061412.k56EC3x13291@sigact.cs.unlv.edu>

Dear Colleague,

SAWN 2005 submission is now closed.  For further details on the 
workshop program please see http://dna.engr.uconn.edu/SAWN2005/

SAWN 2005 Organizers


From konrad.banachewicz at gmail.com  Tue Jun  6 15:52:20 2006
From: konrad.banachewicz at gmail.com (Konrad Banachewicz)
Date: Tue, 6 Jun 2006 15:52:20 +0200
Subject: [R] Multivariate skew-t cdf
In-Reply-To: <20060605173601.GA18582@tango.stat.unipd.it>
References: <204e4c50606020222t1f46e4a2oebd19fda111fa1c1@mail.gmail.com>
	<44845F2C.5040708@pdf.com>
	<20060605173601.GA18582@tango.stat.unipd.it>
Message-ID: <204e4c50606060652k7318298avd90d9b490f665b28@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060606/37cd97e0/attachment.pl 

From carsten.steinhoff at gmx.de  Tue Jun  6 16:09:17 2006
From: carsten.steinhoff at gmx.de (Carsten Steinhoff)
Date: Tue, 6 Jun 2006 16:09:17 +0200
Subject: [R] parameters in fitdistr
Message-ID: <200606061409.k56E998U023201@hypatia.math.ethz.ch>

Hello,

today I've updated from Version 2.1.1 to 2.3.1.

The function you can find below for MLE with double tuncated Lognormal
Distr. is working very well with 2.1.1 but now with the same function I get
the error: object "u_right" not found ... although I've declared
standard-values. I also tryed it with the call:
my.lnvfit(rlnorm(1000,5,2),u_left=100,u_right=10000)!

Is it due to changes in the environment or in the function "fitdistr" (MASS)
? 
For both I could not find anything regarding this problem in the
documentation.

Who can help?

Thanks a lot,
Carsten


============

lognormal_tr=function (loss, u_left=0, u_right=Inf, mu=NA, sigma=NA, ...)

{
library(MASS)
verluste=which(loss>=u_left & loss <=u_right)
loss=loss[verluste]
if (is.na(mu))
mu = mean(log(loss))
if (is.na(sigma)) 
sigma = sd (log(loss))

# Function to optimize

lnv2=function(x,mu,sigma,left,right)
{
return(dlnorm(x,mu,sigma)/(plnorm(right,mu,sigma)-plnorm(left,mu,sigma)))
}

pars=fitdistr(loss,lnv2,list(mu=mu,sigma=sigma),left=u_left,right=u_right,
...)
return(mu=as.vector(pars$estimate[1]), sigma=as.vector(pars$estimate[2]))
}


From BBanerjee at ETS.ORG  Tue Jun  6 16:10:23 2006
From: BBanerjee at ETS.ORG (Banerjee, Bhramori)
Date: Tue, 6 Jun 2006 10:10:23 -0400
Subject: [R] (no subject)
Message-ID: <3B1DEF5FA42FBD4A9117ACBD2CABC84D047FF5EE@rosnt108.etslan.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060606/509fc29d/attachment.pl 

From tlumley at u.washington.edu  Tue Jun  6 16:14:01 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 6 Jun 2006 07:14:01 -0700 (PDT)
Subject: [R] SPSS variable lables import
In-Reply-To: <44854578.7050507@free.fr>
References: <44854578.7050507@free.fr>
Message-ID: <Pine.LNX.4.64.0606060713210.15377@homer22.u.washington.edu>

On Tue, 6 Jun 2006, Frank Thomas wrote:

> Hi,
> I try to get the variable labels of a SPSS data file into R but don't
> find this mentioned in the help file for foreign. Is there another way
> to get them ?
> BTW: An SPSS variable name is like: VAR001, whereas the variable label
> might be 'Identification no.'

mydata <- read.spss("mydata.sav")
attr(mydata, "variable.labels")

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From rkrug at sun.ac.za  Tue Jun  6 16:38:33 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Tue, 06 Jun 2006 16:38:33 +0200
Subject: [R] How to create list of objects?
Message-ID: <44859369.7060507@sun.ac.za>

Hi

I am doing several mle and want to store them in a list (or whatever is
the right construct) to be able to analyse them later.

at the moment I am doing:

f <- list()
f$IP <- mle(...)
f$NE <- mle(...)

but when I say:
> summary(f)
I get:

     Length Class Mode
IP   0      mle   list
NE   0      mle   list

I don't get the output I would have, i.e. the one from
> summary(f$IP)
summary(f$IP)
Maximum likelihood estimation

Call:
mle(minuslogl = IPNeglogPoisL, method = "L-BFGS-B", fixed = list(),
    control = list(maxit = 1e+08, factr = 1e-20))

Coefficients:
      Estimate  Std. Error
a 1242.0185506 44.92341097
b    0.8802538  0.01685811

-2 log L: 145.3509


What I want to do is something like:

AICs <- AIC(logLik(f))

and then have all the AICs in the vector AICs.

It must be possible or is this again a namespace issue?

Rainer

-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa


From jacques.veslot at good.ibl.fr  Tue Jun  6 16:36:52 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Tue, 06 Jun 2006 16:36:52 +0200
Subject: [R] How to create list of objects?
In-Reply-To: <44859369.7060507@sun.ac.za>
References: <44859369.7060507@sun.ac.za>
Message-ID: <44859304.6000108@good.ibl.fr>

lapply(f, summary)
sapply(f, AIC)
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


Rainer M Krug a ?crit :
> Hi
> 
> I am doing several mle and want to store them in a list (or whatever is
> the right construct) to be able to analyse them later.
> 
> at the moment I am doing:
> 
> f <- list()
> f$IP <- mle(...)
> f$NE <- mle(...)
> 
> but when I say:
> 
>>summary(f)
> 
> I get:
> 
>      Length Class Mode
> IP   0      mle   list
> NE   0      mle   list
> 
> I don't get the output I would have, i.e. the one from
> 
>>summary(f$IP)
> 
> summary(f$IP)
> Maximum likelihood estimation
> 
> Call:
> mle(minuslogl = IPNeglogPoisL, method = "L-BFGS-B", fixed = list(),
>     control = list(maxit = 1e+08, factr = 1e-20))
> 
> Coefficients:
>       Estimate  Std. Error
> a 1242.0185506 44.92341097
> b    0.8802538  0.01685811
> 
> -2 log L: 145.3509
> 
> 
> What I want to do is something like:
> 
> AICs <- AIC(logLik(f))
> 
> and then have all the AICs in the vector AICs.
> 
> It must be possible or is this again a namespace issue?
> 
> Rainer
>


From dimitris.rizopoulos at med.kuleuven.be  Tue Jun  6 16:44:57 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 6 Jun 2006 16:44:57 +0200
Subject: [R] How to create list of objects?
References: <44859369.7060507@sun.ac.za>
Message-ID: <00e801c68977$c7f73ba0$0540210a@www.domain>

try something like:

lapply(f, summary)
sapply(f, function(x) AIC(logLik(x)))


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Rainer M Krug" <rkrug at sun.ac.za>
To: "R help list" <r-help at stat.math.ethz.ch>
Sent: Tuesday, June 06, 2006 4:38 PM
Subject: [R] How to create list of objects?


> Hi
>
> I am doing several mle and want to store them in a list (or whatever 
> is
> the right construct) to be able to analyse them later.
>
> at the moment I am doing:
>
> f <- list()
> f$IP <- mle(...)
> f$NE <- mle(...)
>
> but when I say:
>> summary(f)
> I get:
>
>     Length Class Mode
> IP   0      mle   list
> NE   0      mle   list
>
> I don't get the output I would have, i.e. the one from
>> summary(f$IP)
> summary(f$IP)
> Maximum likelihood estimation
>
> Call:
> mle(minuslogl = IPNeglogPoisL, method = "L-BFGS-B", fixed = list(),
>    control = list(maxit = 1e+08, factr = 1e-20))
>
> Coefficients:
>      Estimate  Std. Error
> a 1242.0185506 44.92341097
> b    0.8802538  0.01685811
>
> -2 log L: 145.3509
>
>
> What I want to do is something like:
>
> AICs <- AIC(logLik(f))
>
> and then have all the AICs in the vector AICs.
>
> It must be possible or is this again a namespace issue?
>
> Rainer
>
> -- 
> Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
> Biology (UCT)
>
> Department of Conservation Ecology and Entomology
> University of Stellenbosch
> Matieland 7602
> South Africa
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From deb37 at columbia.edu  Tue Jun  6 17:10:34 2006
From: deb37 at columbia.edu (Daniel E. Bunker)
Date: Tue, 06 Jun 2006 11:10:34 -0400
Subject: [R] xYplot, lmline and abline
Message-ID: <44859AEA.4060003@columbia.edu>

Dear All,

I need to produce a multi-panel plot where:

1) groups within panels receive distinct symbols;
2) a linear regression is fit to the entire panel, not the individual 
groups;
3) a common abline is plotted in each panel.

Essentially I would like to merge the two plots below, but I can't seem 
to figure out how to call both lmline and abline, and also retain the 
grouping symbols. 

In addition, I have not yet figured out how to place the 'Key()' in the 
empty grid cell.

Please see the example code below.  Note that I am running R2.2.1 on 
windows.

Any help would be greatly appreciated.

Thanks for your time,

Dan


library(Hmisc)
windows(record=TRUE)

panelgrp=rep(LETTERS[1:3], 20)
sites=c(rep("d", 15), rep("e", 15), rep("f", 15), rep("g", 15))

data1=data.frame(panelgrp=factor(panelgrp), sites=factor(sites))
data1$pred=runif(60)
data1$resp=runif(60)

lmcoef=lm(resp~pred, data=data1[data1$panelgrp=="A",])

xYplot(resp~pred|panelgrp, groups=sites, data=data1, pch=c(1,2,3,4),
        abline=list(a=lmcoef$coefficients[1], b=lmcoef$coefficients[2], 
lty=2))
    Key()

xYplot(resp ~ pred|panelgrp, groups=sites, pch=c(1,2,3,4),
    data=data1,
       panel = function(x, y) {
        panel.xyplot(x,y)
           panel.lmline(x,y, span=1)
       }
    )





-- 
Daniel E. Bunker
BioMERGE Associate Director
Post-Doctoral Research Scientist
Columbia University
Department of Ecology, Evolution and Environmental Biology
1200 Amsterdam Avenue
New York, NY 10027-5557

deb37ATcolumbiaDOTedu
212-851-1888 phone
212-854-8188 fax


From luke at stat.uiowa.edu  Tue Jun  6 17:40:03 2006
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Tue, 6 Jun 2006 10:40:03 -0500 (CDT)
Subject: [R] [R-pkgs] misc3d_0.4-0 now available on CRAN
Message-ID: <Pine.LNX.4.64.0606061038340.8514@nokomis.stat.uiowa.edu>

Release 0.4-0 of package misc3d is now available from CRAN.  This
package provides a small collection of functions for 3D plotting,
including 'contour3d' for computing and rendering 3D contours or
isosurfaces and 'parametric3d' for rendering parameterized surfaces.

A major change in this release is that rendering in standard and grid
graphics is now supported in addition to rgl.  To support rendering in
standard and grid graphics the package includes some functions for
managing and rendering triangle mesh representations of surfaces.
Rendering uses simple lighting and shading algorithms to represent 3D
structure.  The examples in

    example(contour3d)
    example(parametric3d)
    demo(lighting)
    demo(teapot)

illustrate some of the features.

Dai Feng and Luke Tierney



-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From armin at approximity.com  Tue Jun  6 00:54:44 2006
From: armin at approximity.com (Armin Roehrl)
Date: Tue, 06 Jun 2006 00:54:44 +0200
Subject: [R] use of R in big companies (references) & R-support esp in
	Germany
Message-ID: <4484B634.4000709@approximity.com>

Dear R users,

    sorry for this general email and I am sure it has been asked
way too many times.

IT departements in big companies only want to support the big
standards. Whatever big standards means apart from being expensive.

We are in the process of trying to get a risk management project
for a big conservative company in Germany. As part of the project
we would use R to run simulations, but the company is afraid of R.

1) If anybody has any reference projects using R I can quote, please
drop me an email. Best would be companies like Siemens, Allianz,
Munich Re, Daimler Chrysler, Credit Suisse etc.

2) Are there any software companies around with R know-how and are
interested in paid R-projects? The bigger the company, the better
as this client seems to be scared of software companies with less
than 200 developers.


Thanks,
  -Armin

-- 
Dr. Armin Roehrl, Approximity GmbH

http://www.approximity.com
Agile blog: http://agile.approximity.com
Risk Management blog http://risk.approximity.com


From chrysopa at gmail.com  Tue Jun  6 18:31:56 2006
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Tue, 6 Jun 2006 13:31:56 -0300
Subject: [R] [OFF] The "best" tool for a space-temporal analyses?
Message-ID: <200606061331.56525.chrysopa@gmail.com>

Hi,

I try to make an analyses to discover what is the time that an area begin to 
have spacial autocorrelation. And after, what is the number of individuals 
responsible for this autocorrelation.

The main idea is to discover if exist a contamination of a quadrat from others 
quadrats and how is the population needed to make this contamination.

This is very common to use automata to simulate this situation. But I try to 
make a more statistical approach. I'm studing about, but I dont know the tool 
for testing examples.

I make an example just for tests:

Geodata <- data.frame(X=rep(rep(c(1:10),
(rep(10,10))),5),Y=rep(c(1:10),50),Abund=c(1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 
0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 
0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 3, 0, 0, 2, 0, 0, 1, 2, 0, 0, 
2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 2, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 2, 0, 0, 
0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 5, 
0, 2, 0, 0, 0, 0, 3, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 2, 3, 0, 0, 3, 0, 0, 3, 
0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 0, 0, 4, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 2, 10, 0, 0, 3, 0, 0, 0, 0, 3, 
3, 4, 2, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 0, 3, 0, 
0, 0, 0, 4, 0, 2, 1, 0, 0, 3, 0, 0, 0, 2, 0, 1, 4, 0, 0, 4, 0, 0, 4, 0, 0, 0, 
0, 0, 4, 0, 0, 0, 0, 0, 3, 0, 0, 5, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 4, 10, 15, 0, 0, 4, 0, 0, 0, 0, 8, 11, 9, 0, 
0, 0, 0, 0, 0, 0, 1, 5, 3, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 
5, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 1, 0, 5, 0, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 0, 0, 4, 0, 0, 6, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 0, 3, 10, 15, 20, 0, 0, 0, 0, 0, 0, 4, 13, 16, 13, 0, 0, 0, 
0, 0, 0, 5, 8, 8, 10, 0, 0, 0, 0, 0, 0, 1, 2, 3, 5, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0),Time=rep(c(1:5),rep(100,5)))

X and Y are coordinates, Abund is the number of individuals and Time is the 
date of observation. In this example the population grows from an vertice, 
and after 10 individuals it contaminates your neighbors. I need ideas about 
the best approach and R's tools for this problem.

I'm studing this question in these books:

W.N. Venables, B.D. Ripley. 2003.  Modern Applied Statistics with S. Springer; 
4 edition (September 2, 2003). 512 pages.

Crawley, M. J. 2002. Statistical Computing: An Introduction to Data Analysis 
using S-Plus. John Wiley & Sons; 1st edition (May 15, 2002). 772 pages.

Diggle, Peter J. 2003. Statistical Analysis of Spatial Point Patterns (2nd 
ed.), Arnold, London.

Ripley, B.D. Spatial Statistics

Spatial Ecology

Thanks for all

-- 
I haven't been married in over six years, but we had sexual counseling
every day from Oral Roberts!!
--
|>   // | \\   [***********************************]
|   ( ?   ? )  [Prof. Ronaldo Reis J?nior          ]
|>      V      [UNIMONTES/Depto. Biologia Geral    ]
|    /     \   [36570-000 Vi?osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa em insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Testing

-- 
	Se pelo menos pudessemos viver duas vezes: a primeira vez, 
	para cometer todos os inevitaveis erros; a segunda para 
	lucrar com eles.
		-- D. H. Lawrence 
--
|>   // | \\   [***********************************]
|   ( ?   ? )  [Prof. Ronaldo Reis J?nior          ]
|>      V      [UNIMONTES/Depto. Biologia Geral    ]
|    /     \   [36570-000 Vi?osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa em insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Testing


From spamfilter at conexant.com  Tue Jun  6 18:31:23 2006
From: spamfilter at conexant.com (spamfilter at conexant.com)
Date: Tue, 6 Jun 2006 09:31:23 -0700 (PDT)
Subject: [R] Blocked File Type
Message-ID: <200606061631.k56GVP1D030345@hypatia.math.ethz.ch>

Your message to r-help at stat.math.ethz.ch
was blocked because it contained a file type prohibited by our server.


From dylan.beaudette at gmail.com  Tue Jun  6 18:52:37 2006
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Tue, 6 Jun 2006 09:52:37 -0700
Subject: [R] coloring leaves in a hclust or dendrogram plot [solved]
In-Reply-To: <4412EED2.8000407@free.fr>
References: <200603101336.55545.dylan.beaudette@gmail.com>
	<971536df0603110656x55ab3687p13b322dc1f919913@mail.gmail.com>
	<4412EED2.8000407@free.fr>
Message-ID: <200606060952.37673.dylan.beaudette@gmail.com>

On Saturday 11 March 2006 07:37, Romain Francois wrote:
> Le 11.03.2006 15:56, Gabor Grothendieck a ?crit :
> > Where does one find A2Rplot that is called in that example?
> >
> > A2R does not appear to be on CRAN.  I did find this:
> >
> >   http://addictedtor.free.fr/Download/A2R.zip
> >
> > which is a zip file containing some R code but it is not a package,
> > which the line library(A2R) seems to need, and it does not include
> > A2Rplot in any case.
>
> (...)
>
> Hi Gabor and all,
>
> the package A2R is really highly experimental now and i don't have much
> time to improve it to such an extent that it would be possible to commit
> it to CRAN. For now on, the source of the package is there :
> http://addictedtor.free.fr/packages/A2R/
> For people that can't build it, it is possible to source the files from
> here :
> http://addictedtor.free.fr/packages/A2R/lastVersion/R/
>
> Plus, i am (slowly) preparing an rgg package, so the content of A2R will
> probably go there
>
> Romain

Thanks for the update Romain,

I have successfully installed the A2R package, and I nearly have a working 
graph from A2Rplot. sample output here:

http://169.237.35.250/~dylan/temp/a2rplot.pdf

however, the labels at the bottom of the dendrogram are truncated. Is there 
any way to prevent this - some parameter that I can pass to A2Rplot() ?

Also, is it possible to add more factors to the bottom of the plot?

Thanks!

Dylan




-- 
Dylan Beaudette
Soils and Biogeochemistry Graduate Group
University of California at Davis
530.754.7341


From darakjia at ohsu.edu  Tue Jun  6 18:40:20 2006
From: darakjia at ohsu.edu (Priscila Darakjian)
Date: Tue, 06 Jun 2006 09:40:20 -0700
Subject: [R] memory.limit function not found
Message-ID: <s4854d95.041@ohsu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060606/b04316a2/attachment.pl 

From ripley at stats.ox.ac.uk  Tue Jun  6 18:50:29 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 6 Jun 2006 17:50:29 +0100 (BST)
Subject: [R] memory.limit function not found
In-Reply-To: <s4854d95.041@ohsu.edu>
References: <s4854d95.041@ohsu.edu>
Message-ID: <Pine.LNX.4.64.0606061746210.23107@gannet.stats.ox.ac.uk>

On Tue, 6 Jun 2006, Priscila Darakjian wrote:

> I have installed R 2.2.1 in Solaris 10 and am trying to increase the 
> memory capacity (the system has 16G RAM) to 3 or 4G, but I keep getting:
>
>> memory.limit(size=3000)
> Error: couldn't find function "memory.limit"
>
> Am I missing anything? I do that all the time under Windows.

Yes, you are not using Windows and that is part of R's memory manager on 
Windows.  On Solaris, R uses the system memory manager, and you set limits 
in your shell (via limit or ulimit) before launching R.

If you have a 32-bit build of R, the limit on Solaris is probably 3GB. 
But you should be able to use a 64-bit build with a much larger limit.

One might ask why you installed R 2.2.1 when 2.3.1 is current?

>
> Any help would be appreciated.
> Thanks
>
> Priscila

> 	[[alternative HTML version deleted]]

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ligges at statistik.uni-dortmund.de  Tue Jun  6 18:52:44 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 06 Jun 2006 18:52:44 +0200
Subject: [R] memory.limit function not found
In-Reply-To: <s4854d95.041@ohsu.edu>
References: <s4854d95.041@ohsu.edu>
Message-ID: <4485B2DC.3040605@statistik.uni-dortmund.de>

Priscila Darakjian wrote:
> I have installed R 2.2.1 in Solaris 10 and am trying to increase the
> memory capacity (the system has 16G RAM) to 3 or 4G, but I keep
> getting:
> 
>> memory.limit(size=3000)
> Error: couldn't find function "memory.limit"

The function is only required on Windows. No need to use it on Solaris: 
You will get as much as available for your process automatically.

Uwe Ligges



> Am I missing anything? I do that all the time under Windows.
> 
> Any help would be appreciated. Thanks
> 
> Priscila
> 
> 
> [[alternative HTML version deleted]]
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide! http://www.R-project.org/posting-guide.html


From f.harrell at vanderbilt.edu  Tue Jun  6 17:55:50 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 06 Jun 2006 10:55:50 -0500
Subject: [R] SPSS variable lables import
In-Reply-To: <Pine.LNX.4.64.0606060713210.15377@homer22.u.washington.edu>
References: <44854578.7050507@free.fr>
	<Pine.LNX.4.64.0606060713210.15377@homer22.u.washington.edu>
Message-ID: <4485A586.80505@vanderbilt.edu>

Thomas Lumley wrote:
> On Tue, 6 Jun 2006, Frank Thomas wrote:
> 
> 
>>Hi,
>>I try to get the variable labels of a SPSS data file into R but don't
>>find this mentioned in the help file for foreign. Is there another way
>>to get them ?
>>BTW: An SPSS variable name is like: VAR001, whereas the variable label
>>might be 'Identification no.'
> 
> 
> mydata <- read.spss("mydata.sav")
> attr(mydata, "variable.labels")
> 
>  	-thomas

Or:

library(Hmisc)
mydata <- spss.get('mydata.sav')
describe(mydata)
contents(mydata)
xYplot( )
etc. use the variable labels attached to individual variables.  You can 
retrieve them also this way:

with(mydata, plot(x, y, xlab=label(x), ylab=label(y)))

Frank Harrell


> 
> Thomas Lumley			Assoc. Professor, Biostatistics
> tlumley at u.washington.edu	University of Washington, Seattle
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From darakjia at ohsu.edu  Tue Jun  6 18:56:37 2006
From: darakjia at ohsu.edu (Priscila Darakjian)
Date: Tue, 06 Jun 2006 09:56:37 -0700
Subject: [R] memory.limit function not found
Message-ID: <s4855162.067@OHSU.EDU>

Thanks, Prof Ripley. Now I understand. And , regarding the version, R was installed a while ago in this machine, when v. 2.3.1 was not available.
Thanks again.

>>> "Prof Brian Ripley" <ripley at stats.ox.ac.uk> 6/6/2006 9:50 AM >>>
On Tue, 6 Jun 2006, Priscila Darakjian wrote:

> I have installed R 2.2.1 in Solaris 10 and am trying to increase the 
> memory capacity (the system has 16G RAM) to 3 or 4G, but I keep getting:
>
>> memory.limit(size=3000)
> Error: couldn't find function "memory.limit"
>
> Am I missing anything? I do that all the time under Windows.

Yes, you are not using Windows and that is part of R's memory manager on 
Windows.  On Solaris, R uses the system memory manager, and you set limits 
in your shell (via limit or ulimit) before launching R.

If you have a 32-bit build of R, the limit on Solaris is probably 3GB. 
But you should be able to use a 64-bit build with a much larger limit.

One might ask why you installed R 2.2.1 when 2.3.1 is current?

>
> Any help would be appreciated.
> Thanks
>
> Priscila

> 	[[alternative HTML version deleted]]

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html 


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk 
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/ 
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From patrick.guevel at uk.bnpparibas.com  Tue Jun  6 19:10:48 2006
From: patrick.guevel at uk.bnpparibas.com (patrick.guevel at uk.bnpparibas.com)
Date: Tue, 6 Jun 2006 18:10:48 +0100
Subject: [R] Suspicious behaviour of sort on POSIXct vectors in R-2.3.0 and
	R-2.3.1
Message-ID: <OF572FD044.0131BC92-ON80257185.005D8210-80257185.005E5FD4@bnpparibas.com>

Hi ,

When I sort a vector of POSIXct values in R-2.3.0 and R-2.3.1, I get a
vector of numeric values and this gets some of my code to crash (class
object creation). Is that a R bug?

In the mean time. I'll try to override the sort function for the POSIXct
objects.

Thanks,

Patrick Gu?vel
Head of quantitative research for electronic trading


This message and any attachments (the "message") is\ intende...{{dropped}}


From miltinho_astronauta at yahoo.com.br  Tue Jun  6 19:27:31 2006
From: miltinho_astronauta at yahoo.com.br (Milton Cezar)
Date: Tue, 6 Jun 2006 14:27:31 -0300 (ART)
Subject: [R] bootstrap data from groups
Message-ID: <20060606172731.49313.qmail@web53402.mail.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060606/fb2f73cb/attachment.pl 

From dyang at NRCan.gc.ca  Tue Jun  6 20:03:22 2006
From: dyang at NRCan.gc.ca (Yang, Richard)
Date: Tue, 6 Jun 2006 14:03:22 -0400
Subject: [R] Accessing lme source code
Message-ID: <F0E0B899CB43D5118D220002A55113CF04FE58F0@s2-edm-r1.nrn.nrcan.gc.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060606/0bb78bd9/attachment.pl 

From gunter.berton at gene.com  Tue Jun  6 20:19:01 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 6 Jun 2006 11:19:01 -0700
Subject: [R] Accessing lme source code
In-Reply-To: <F0E0B899CB43D5118D220002A55113CF04FE58F0@s2-edm-r1.nrn.nrcan.gc.ca>
Message-ID: <003e01c68995$b02d2980$295afea9@gne.windows.gene.com>

1. You need to learn about S3 methods. ?UseMethod will tell you what you
need to know.

2. methods("lme") will tell you the available methods.

3. nlme:::lme.formula will give you the code. 

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Yang, Richard
> Sent: Tuesday, June 06, 2006 11:03 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Accessing lme source code
> 
> Dear all;
> 
> 	This an FAQ. I tried to access lme source script so I can step
> into it to debug the problems resulting from a lme() call. I used
> getAnywhere("lme") or nlme:::lme, both produced only the function
> definition and "UseMethod("lme"). 
> 
> 	Any idea how to list the source code?
> 
> 	TIA,
> 
> Richard Yang
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Tue Jun  6 20:20:04 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 6 Jun 2006 14:20:04 -0400
Subject: [R] Suspicious behaviour of sort on POSIXct vectors in R-2.3.0
	and R-2.3.1
In-Reply-To: <OF572FD044.0131BC92-ON80257185.005D8210-80257185.005E5FD4@bnpparibas.com>
References: <OF572FD044.0131BC92-ON80257185.005D8210-80257185.005E5FD4@bnpparibas.com>
Message-ID: <971536df0606061120h773a84c4g8a1644bd8d9b681@mail.gmail.com>

Use order instead:

x <- as.POSIXct(Sys.Date() + 10:1)
x[order(x)]


On 6/6/06, patrick.guevel at uk.bnpparibas.com
<patrick.guevel at uk.bnpparibas.com> wrote:
> Hi ,
>
> When I sort a vector of POSIXct values in R-2.3.0 and R-2.3.1, I get a
> vector of numeric values and this gets some of my code to crash (class
> object creation). Is that a R bug?
>
> In the mean time. I'll try to override the sort function for the POSIXct
> objects.
>
> Thanks,
>
> Patrick Gu?vel
> Head of quantitative research for electronic trading
>
>
> This message and any attachments (the "message") is\ intende...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From dhinds at sonic.net  Tue Jun  6 20:21:46 2006
From: dhinds at sonic.net (dhinds at sonic.net)
Date: Tue, 6 Jun 2006 18:21:46 +0000 (UTC)
Subject: [R] Fastest way to do HWE.exact test on 100K SNP data?
References: <1149535673.448485b924e4e@netmail.bsd.uchicago.edu>
Message-ID: <e64h3q$a8j$1@sea.gmane.org>

Anna Pluzhnikov <apluzhni at bsd.uchicago.edu> wrote:
> Hi everyone,


> I'm using the function 'HWE.exact' of 'genetics' package to compute
> p-values of the HWE test. My data set consists of ~600 subjects
> (cases and controls) typed at ~ 10K SNP markers; the test is applied
> separately to cases and controls. The genotypes are stored in a list
> of 'genotype' objects, all.geno, and p-values are calculated inside
> the loop over all SNP markers.

Just to concur with the previous two posters: when I've needed to
calculate lots of HWE values, I've done one of two things:

1.  Use a faster test: either the chisq test or a likelihood ratio
    test.  Optionally, you could use the exact test when one of the
    allele counts is very small, and use an asymtotic test in other
    cases.  I often just use the fast test on everything.  Especially
    since you are doing a permutation analysis on the test values, the
    exact test may not be buying you anything.

2.  Caching the HWE values works great if you can get some reuse of
    previously calculated values.  If you're calculating for ~300
    cases and ~300 controls, there would be 600*601/4 or only ~90K
    possible sets of AA/AB/BB allele counts assuming complete data
    (you can flip the counts around so that the "AA" count is always
    for the more frequent allele); with missing data there can be many
    more, but most of those possibilities will never be observed under
    the null hypothesis of HWE.  And since you are computing roughly
    10 million HWE values, you'll have a lot of reuse of previously
    calculated values.

-- Dave


From Mitchell.Wachtel at ttuhsc.edu  Tue Jun  6 21:57:02 2006
From: Mitchell.Wachtel at ttuhsc.edu (Wachtel, Mitchell)
Date: Tue, 6 Jun 2006 14:57:02 -0500
Subject: [R] Score test to evalutate the proportional odds assumption.
Message-ID: <1A2BCA4266504B4CA543403718A81FD2787799@TRAVIS.ttuhsc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060606/57f17f59/attachment.pl 

From ripley at stats.ox.ac.uk  Tue Jun  6 22:14:01 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 6 Jun 2006 21:14:01 +0100 (BST)
Subject: [R] Suspicious behaviour of sort on POSIXct vectors in R-2.3.0
 and R-2.3.1
In-Reply-To: <OF572FD044.0131BC92-ON80257185.005D8210-80257185.005E5FD4@bnpparibas.com>
References: <OF572FD044.0131BC92-ON80257185.005D8210-80257185.005E5FD4@bnpparibas.com>
Message-ID: <Pine.LNX.4.64.0606062109200.4685@gannet.stats.ox.ac.uk>

On Tue, 6 Jun 2006, patrick.guevel at uk.bnpparibas.com wrote:

> Hi ,
>
> When I sort a vector of POSIXct values in R-2.3.0 and R-2.3.1, I get a
> vector of numeric values and this gets some of my code to crash (class
> object creation). Is that a R bug?

No, it is as documented: see ?sort

      As from R 2.3.0, all attributes are removed from the return value
      except names, which are sorted.  (If 'partial' is specified even
      the names are removed.)

Note, the class is an attribute.  For many classes sorting destroys the 
appropriateness of the class.

> In the mean time. I'll try to override the sort function for the POSIXct
> objects.

You can restore the class if appropriate.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From dvdkerc at yahoo.com  Tue Jun  6 22:44:28 2006
From: dvdkerc at yahoo.com (Dirk Vandekerckhove)
Date: Tue, 6 Jun 2006 13:44:28 -0700 (PDT)
Subject: [R] Strange behaviour of cbind
Message-ID: <20060606204428.85696.qmail@web51607.mail.yahoo.com>

Hi,

Is this intended behaviour of cbind?

> a<-c(0,1,2,3)
> a
[1] 0 1 2 3
> a<-as.ordered(a)
> a
[1] 0 1 2 3
Levels: 0 < 1 < 2 < 3
> a<-a[a!=0] #remove the zero from a
> a
[1] 1 2 3
Levels: 0 < 1 < 2 < 3
> cbind(a) 
     a
[1,] 2
[2,] 3
[3,] 4

#cbind adds +1 to each element

> a<-as.ordered(as.vector(a))
> a
[1] 1 2 3
Levels: 1 < 2 < 3
> cbind(a)
     a
[1,] 1
[2,] 2
[3,] 3

#now it works...

I am running R 2.3.0 on a windows system.



Regards,

Dirk Vandekerckhove


From lai at lindaspaces.com  Tue Jun  6 22:44:55 2006
From: lai at lindaspaces.com (Jennifer Lai)
Date: Tue, 06 Jun 2006 16:44:55 -0400
Subject: [R] build R with Visual Studio
Message-ID: <4485E947.3000905@lindaspaces.com>

Hi,
    Has anyone had success in building R source with Visual Studio?  I 
followed the instructions in README.packages, but failed on the very 
first step, where it's looking for R.dll. I looked through R source and 
couldn't find the file. Can someone point me to where this file is 
located or generated? Thanks!


Sincerely,
Jennifer


From Steven.Roels at mpi.com  Tue Jun  6 22:52:58 2006
From: Steven.Roels at mpi.com (Roels, Steven)
Date: Tue, 6 Jun 2006 16:52:58 -0400
Subject: [R] vague errors on R CMD check for very minimal S4-style package
Message-ID: <C26BCE7602F7214FA9B98B050027AE17A456E3@US-BE4.corp.mpi.com>


Hello,

I have a very minimal package "simplepkg" (DESCRIPTION, NAMESPACE, and
R) with S4 classes/methods (defines a class "foo" and a show method for
that class" - both the class and show method are exported).  I can
seemingly install the package, then load and use it:

> sessionInfo()
Version 2.3.1 (2006-06-01) 
sparc-sun-solaris2.10 

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"
"datasets"  "base"     

> library(simplepkg,lib.loc="/home/roels/R/lib")
> 
> getClass("foo")

Slots:
                          
Name:       name       age
Class: character   numeric
> 
> myfoo <- new("foo",name="abc",age=10)
> myfoo
I am  abc 
>

But when I do R CMD check, it fails (or perhaps the "errors" are really
"warnings"??):
---------------------------------------

sun890% R CMD check simplepkg 
* checking for working latex ... OK
* using log directory '/home/roels/R/dev/simplepkg/simplepkg.Rcheck'
* using Version 2.3.1 (2006-06-01)
* checking for file 'simplepkg/DESCRIPTION' ... OK
* checking extension type ... Package
* this is package 'simplepkg' version '0.1-1'
* checking package dependencies ... OK
* checking if this is a source package ... OK
* checking whether package 'simplepkg' can be installed ... OK
* checking package directory ... OK
* checking for portable file names ... OK
* checking for sufficient/correct file permissions ... OK
* checking DESCRIPTION meta-information ... OK
* checking top-level files ... OK
* checking index information ... OK
* checking package subdirectories ... OK
* checking R files for syntax errors ... OK
* checking R files for library.dynam ... OK
* checking S3 generic/method consistency ... WARNING
Error: package/namespace load failed for 'simplepkg'
Call sequence:
2: stop(gettextf("package/namespace load failed for '%s'",
libraryPkgName(package)), 
       call. = FALSE, domain = NA)
1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
FALSE)
Execution halted
See section 'Generic functions and methods' of the 'Writing R
Extensions'
manual.
* checking replacement functions ... WARNING
Error: package/namespace load failed for 'simplepkg'
Call sequence:
2: stop(gettextf("package/namespace load failed for '%s'",
libraryPkgName(package)), 
       call. = FALSE, domain = NA)
1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
FALSE)
Execution halted
In R, the argument of a replacement function which corresponds to the
right
hand side must be named 'value'.
* checking foreign function calls ... WARNING
Error: package/namespace load failed for 'simplepkg'
Call sequence:
2: stop(gettextf("package/namespace load failed for '%s'",
libraryPkgName(package)), 
       call. = FALSE, domain = NA)
1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
FALSE)
Execution halted
See section 'System and foreign language interfaces' of the 'Writing R
Extensions' manual.
* checking for missing documentation entries ... ERROR
Error: package/namespace load failed for 'simplepkg'


Here are the file contents:
-------------------------------

sun890% cat DESCRIPTION
Package: simplepkg
Type: Package
Title: Does stuff
Version: 0.1-1
Date: 2006-06-06
Author: Me
Maintainer: Also Me <me at here.com>
Description: Does interesting stuff
License: GPL

sun890% cat NAMESPACE 
exportClasses("foo")
exportMethods("show")

### cat simplepkg.R  #class def and show method
 
setClass("foo",representation(name="character",age="numeric"))

setMethod("show", "foo", function(object) cat("I am
",object at name,"\n",sep=" "))

### cat zzz.R        # required .onLoad  for use of S4-style
classes/methods
.onLoad <- function(lib,pkg) require(methods)





As I said, the package "seems" to work, but I wanted to be sure I wasn't
missing something that was going to come back to bite me later.

Thanks,

-Steve



*****************************************************************
Steve Roels, Ph.D.                       
Senior Scientist I
Computational Biology               
Millennium Pharmaceuticals, Inc
640 Memorial Drive                 
Cambridge, MA 02139-4815
*****************************************************************





This e-mail, including any attachments, is a confidential business communication, and may contain information that is confidential, proprietary and/or privileged.  This e-mail is intended only for the individual(s) to whom it is addressed, and may not be saved, copied, printed, disclosed or used by anyone else.  If you are not the(an) intended recipient, please immediately delete this e-mail from your computer system and notify the sender.  Thank you.


From ecoinformatics at gmail.com  Tue Jun  6 22:59:47 2006
From: ecoinformatics at gmail.com (Xiaohua Dai)
Date: Tue, 6 Jun 2006 22:59:47 +0200
Subject: [R] build R with Visual Studio
In-Reply-To: <4485E947.3000905@lindaspaces.com>
References: <4485E947.3000905@lindaspaces.com>
Message-ID: <15f8e67d0606061359i7dd40cf5r16f4fe428ad43567@mail.gmail.com>

Hi,

R.ll is in my C:\Program Files\R\R-2.3.0\bin. My OS is Win XP.


On 6/6/06, Jennifer Lai <lai at lindaspaces.com> wrote:
> Hi,
>    Has anyone had success in building R source with Visual Studio?  I
> followed the instructions in README.packages, but failed on the very
> first step, where it's looking for R.dll. I looked through R source and
> couldn't find the file. Can someone point me to where this file is
> located or generated? Thanks!
>
>
> Sincerely,
> Jennifer
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From mschwartz at mn.rr.com  Tue Jun  6 22:59:55 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 06 Jun 2006 15:59:55 -0500
Subject: [R] Strange behaviour of cbind
In-Reply-To: <20060606204428.85696.qmail@web51607.mail.yahoo.com>
References: <20060606204428.85696.qmail@web51607.mail.yahoo.com>
Message-ID: <1149627595.4495.31.camel@localhost.localdomain>

On Tue, 2006-06-06 at 13:44 -0700, Dirk Vandekerckhove wrote:
> Hi,
> 
> Is this intended behaviour of cbind?
> 
> > a<-c(0,1,2,3)
> > a
> [1] 0 1 2 3
> > a<-as.ordered(a)
> > a
> [1] 0 1 2 3
> Levels: 0 < 1 < 2 < 3
> > a<-a[a!=0] #remove the zero from a
> > a
> [1] 1 2 3
> Levels: 0 < 1 < 2 < 3
> > cbind(a) 
>      a
> [1,] 2
> [2,] 3
> [3,] 4
> 
> #cbind adds +1 to each element

Does this help?

> a
[1] 1 2 3
Levels: 0 < 1 < 2 < 3

> as.integer(a)
[1] 2 3 4

Note in ?cbind, the Details section indicates:

"In the default method, all the vectors/matrices must be atomic (see
vector) or lists (e.g., not expressions)."

For a factor, the atomic data type is the underlying integer vector.
You eliminated '0' from the original ordered factor, which had an
integer value of 1 (not 0!):

> a
[1] 0 1 2 3
Levels: 0 < 1 < 2 < 3

> as.integer(a)
[1] 1 2 3 4

Unless you re-level the factor (as you do below) the other elements
retain the original integer values.

> > a<-as.ordered(as.vector(a))
> > a
> [1] 1 2 3
> Levels: 1 < 2 < 3
> > cbind(a)
>      a
> [1,] 1
> [2,] 2
> [3,] 3
> 
> #now it works...

Yep, you re-leveled 'a', so the integer values now correspond to the
levels:

> a<-as.ordered(as.vector(a))

> a
[1] 1 2 3
Levels: 1 < 2 < 3

> as.integer(a)
[1] 1 2 3


HTH,

Marc Schwartz


From sarah.goslee at gmail.com  Tue Jun  6 23:01:24 2006
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 6 Jun 2006 17:01:24 -0400
Subject: [R] Strange behaviour of cbind
In-Reply-To: <20060606204428.85696.qmail@web51607.mail.yahoo.com>
References: <20060606204428.85696.qmail@web51607.mail.yahoo.com>
Message-ID: <efb536d50606061401k58251e79g28d1837cc4d0f308@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060606/1d27b3a4/attachment.pl 

From HDoran at air.org  Tue Jun  6 23:07:57 2006
From: HDoran at air.org (Doran, Harold)
Date: Tue, 6 Jun 2006 17:07:57 -0400
Subject: [R] Subset data in long format
Message-ID: <2323A6D37908A847A7C32F1E3662C80E05601C@dc1ex01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060606/ff450975/attachment.pl 

From HDoran at air.org  Tue Jun  6 23:15:12 2006
From: HDoran at air.org (Doran, Harold)
Date: Tue, 6 Jun 2006 17:15:12 -0400
Subject: [R] Subset data in long format
Message-ID: <2323A6D37908A847A7C32F1E3662C80E056021@dc1ex01.air.org>

Apologies, but there were some word wrap issues in the prior email it
seems. So, here is code for the sample data to avoid confusion 


tmp <- data.frame(id = 1:3, matrix(rnorm(30), ncol=10) )

long <- reshape(tmp, idvar='id', varying=list(names(tmp)[2:11]),
v.names=('item'),timevar='position' , direction='long')

long <- long[order(long$id) , ]

long <- long[c(-2,-13),]

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Doran, Harold
> Sent: Tuesday, June 06, 2006 5:08 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Subset data in long format
> 
> I have data in a "long" format where each row is a student 
> and each student occupies multiple rows with multiple 
> observations. I need to subset these data based on a 
> condition which I am having difficulty defining. 
> 
> The dataset I am working with is large, but here is a simple 
> data structure to illustrate the issue
> 
> tmp <- data.frame(id = 1:3, matrix(rnorm(30), ncol=10) ) long 
> <- reshape(tmp, idvar='id', varying=list(names(tmp)[2:11]), 
> v.names=('item'),timevar='position' , direction='long') long 
> <- long[order(long$id) , ] long <- long[c(-2,-13),]
> 
> What I need to do is subset these data so I have the first 6 
> rows for each unique ID. The problem is that the data are 
> unbalanced in that each ID has a different number of 
> observations (which I why I removed obs 2 and 13).
> 
> If the data were balanced, the subset would be trivial and I 
> could just do
> 
> long <- subset(long, position < 7)
> 
> However, the data are not balanced. Consequently, if I were 
> to do this for the unbalanced data I would not have the first 
> 6 obs for the first ID. I would only have the first 5. 
> Theoretically, what I want for id1(and for each unique id) is this
> 
> ID1 <- subset(long, id==1)
> ID1[1:6,]
> 
> However, the goal is to subset the entire dataframe at once 
> such that the subset returns a new dataframe with the first 6 
> rows for each unique id. Is there a feasible method for doing 
> this subset that anyone can suggest? My actual dataset has 
> more than 24,000 unique ids, so I am hoping to avoid looping 
> through this if possible.
> 
> Thanks,
> Harold
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From lai at lindaspaces.com  Tue Jun  6 23:20:10 2006
From: lai at lindaspaces.com (Jennifer Lai)
Date: Tue, 06 Jun 2006 17:20:10 -0400
Subject: [R] build R with Visual Studio
In-Reply-To: <15f8e67d0606061359i7dd40cf5r16f4fe428ad43567@mail.gmail.com>
References: <4485E947.3000905@lindaspaces.com>
	<15f8e67d0606061359i7dd40cf5r16f4fe428ad43567@mail.gmail.com>
Message-ID: <4485F18A.3080600@lindaspaces.com>

actually what I really want is to build the entire R source using Visual 
Studio. I know the current supported way is to use MinGW to build R on 
Windows.
But since there is no 64-bit MinGW, I thought I'll experimenting with 
Visual Studio on 64-bit Windows.
If anyone has success in building R with Visual Studio on 32-bit 
platform, please shed some light on this. Thanks!

Sincerely,
Jennifer

Xiaohua Dai wrote:

> Hi,
>
> R.ll is in my C:\Program Files\R\R-2.3.0\bin. My OS is Win XP.
>
>
> On 6/6/06, Jennifer Lai <lai at lindaspaces.com> wrote:
>
>> Hi,
>>    Has anyone had success in building R source with Visual Studio?  I
>> followed the instructions in README.packages, but failed on the very
>> first step, where it's looking for R.dll. I looked through R source and
>> couldn't find the file. Can someone point me to where this file is
>> located or generated? Thanks!
>>
>>
>> Sincerely,
>> Jennifer
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>


From sfalcon at fhcrc.org  Tue Jun  6 23:21:35 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 06 Jun 2006 14:21:35 -0700
Subject: [R] vague errors on R CMD check for very minimal S4-style
	package
In-Reply-To: <C26BCE7602F7214FA9B98B050027AE17A456E3@US-BE4.corp.mpi.com>
	(Steven Roels's message of "Tue, 6 Jun 2006 16:52:58 -0400")
References: <C26BCE7602F7214FA9B98B050027AE17A456E3@US-BE4.corp.mpi.com>
Message-ID: <m2irnevv34.fsf@ziti.local>

"Roels, Steven" <Steven.Roels at mpi.com> writes:

> Hello,
>
> I have a very minimal package "simplepkg" (DESCRIPTION, NAMESPACE, and
> R) with S4 classes/methods (defines a class "foo" and a show method for
> that class" - both the class and show method are exported).  I can
> seemingly install the package, then load and use it:
>
> Error: package/namespace load failed for 'simplepkg'
> Call sequence:
> 2: stop(gettextf("package/namespace load failed for '%s'",
> libraryPkgName(package)), 
>        call. = FALSE, domain = NA)
> 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
> FALSE)
> Execution halted

>
> Here are the file contents:
> -------------------------------
>
> sun890% cat DESCRIPTION
> Package: simplepkg
> Type: Package
> Title: Does stuff
> Version: 0.1-1
> Date: 2006-06-06
> Author: Me
> Maintainer: Also Me <me at here.com>
> Description: Does interesting stuff
> License: GPL

Try adding LazyLoad: yes to DESCRIPTION (or SaveImage: yes).


From Achim.Zeileis at R-project.org  Tue Jun  6 23:31:46 2006
From: Achim.Zeileis at R-project.org (Achim Zeileis)
Date: Tue, 6 Jun 2006 23:31:46 +0200
Subject: [R] [R-pkgs] zoo: new version 1.1-0
Message-ID: <20060606233146.6bfd068f.Achim.Zeileis@R-project.org>

Dear useRs,

I've just uploaded the new version 1.1-0 of the zoo package for dealing
with regular and irregular time series data to the main CRAN site.
Source and binary packages should be available from the mirrors in the
next days.

There are two changes that are nut fully backwards compatible:
  - The package now has a NAMESPACE and most S3 methods are not exported
    explicitely anymore. If some function needs to be exported for
    useRs/developeRs of other packages, please let us know.
  - By default read.zoo() does not try to produce regular series
    anymore. The previous version could result in misleading
    computations for daily financial data, affecting especially also the
    zoo-quickref vignette. (Thanks to Ajay Shah for pointing this out.)
    The old behaviour can be restored by setting `regular = TRUE'.

For general introductions to the package see:
  vignette("zoo", package = "zoo")
  vignette("zoo-quickref", package = "zoo")

Best wishes,
Z

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From ggrothendieck at gmail.com  Tue Jun  6 23:37:48 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 6 Jun 2006 17:37:48 -0400
Subject: [R] Subset data in long format
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E05601C@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E05601C@dc1ex01.air.org>
Message-ID: <971536df0606061437pab4bab5i40e72aa0736f9c81@mail.gmail.com>

Try this:

subset(long, seq(id) - match(id,id) < 6)

On 6/6/06, Doran, Harold <HDoran at air.org> wrote:
> I have data in a "long" format where each row is a student and each
> student occupies multiple rows with multiple observations. I need to
> subset these data based on a condition which I am having difficulty
> defining.
>
> The dataset I am working with is large, but here is a simple data
> structure to illustrate the issue
>
> tmp <- data.frame(id = 1:3, matrix(rnorm(30), ncol=10) )
> long <- reshape(tmp, idvar='id', varying=list(names(tmp)[2:11]),
> v.names=('item'),timevar='position' , direction='long')
> long <- long[order(long$id) , ]
> long <- long[c(-2,-13),]
>
> What I need to do is subset these data so I have the first 6 rows for
> each unique ID. The problem is that the data are unbalanced in that each
> ID has a different number of observations (which I why I removed obs 2
> and 13).
>
> If the data were balanced, the subset would be trivial and I could just
> do
>
> long <- subset(long, position < 7)
>
> However, the data are not balanced. Consequently, if I were to do this
> for the unbalanced data I would not have the first 6 obs for the first
> ID. I would only have the first 5. Theoretically, what I want for
> id1(and for each unique id) is this
>
> ID1 <- subset(long, id==1)
> ID1[1:6,]
>
> However, the goal is to subset the entire dataframe at once such that
> the subset returns a new dataframe with the first 6 rows for each unique
> id. Is there a feasible method for doing this subset that anyone can
> suggest? My actual dataset has more than 24,000 unique ids, so I am
> hoping to avoid looping through this if possible.
>
> Thanks,
> Harold
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From highstat at highstat.com  Wed Jun  7 00:26:54 2006
From: highstat at highstat.com (Highland Statistics Ltd.)
Date: Tue, 06 Jun 2006 23:26:54 +0100
Subject: [R] gamm error message
Message-ID: <7.0.0.16.0.20060606232236.01bc9470@highstat.com>

Hello,

Why would I get an error message  with the following code for gamm? I 
want to fit the a gam with different variances per stratum.


library(mgcv)
library(nlme)

Y<-rnorm(100)
X<-rnorm(100,sd=2)
Z<-rep(c(T,F),each=50)

test<-gamm(Y~s(X),weights=varIdent(form=~1|Z))

summary(test$lme)   #ok

summary(test$gam)

Gives an error message:
Error in inherits(x, "data.frame") : dim<- : dims [product 100] do 
not match the length of object [0]


All the other output seems to be ok:


 > summary(test$lme)
Linear mixed-effects model fit by maximum likelihood
  Data: strip.offset(mf)
        AIC      BIC    logLik
   299.9468 312.9727 -144.9734

Random effects:
  Formula: ~Xr.1 - 1 | g.1
  Structure: pdIdnot
                Xr.11        Xr.12        Xr.13        Xr.14        Xr.15
StdDev: 0.0001073404 0.0001073404 0.0001073404 0.0001073404 0.0001073404
                Xr.16        Xr.17        Xr.18 Residual
StdDev: 0.0001073404 0.0001073404 0.0001073404 1.045916

Variance function:
  Structure: Different standard deviations per stratum
  Formula: ~1 | Z
  Parameter estimates:
      TRUE     FALSE
1.0000000 0.9721987
Fixed effects: y ~ X.0 - 1
                      Value Std.Error DF    t-value p-value
X.0(Intercept)  0.04426102 0.1041540 98  0.4249573  0.6718
X.0s(X)Fx1     -0.01053900 0.1039558 98 -0.1013796  0.9195
  Correlation:
            X.0(I)
X.0s(X)Fx1 0.002

Standardized Within-Group Residuals:
         Min          Q1         Med          Q3         Max
-1.96955692 -0.81101014  0.07004034  0.72528662  2.44244302

Number of Observations: 100
Number of Groups: 1





Kind regards,

Alain


From lbrooks at scu.edu.au  Wed Jun  7 01:35:02 2006
From: lbrooks at scu.edu.au (Lyndon Brooks)
Date: Wed, 07 Jun 2006 09:35:02 +1000
Subject: [R]  spatial corStruct in lme
Message-ID: <6.2.0.14.0.20060607092358.03ba6008@popstaff.scu.edu.au>

Hi,

I'm fitting a relatively simple growth model to some forest plot data. Two 
species of trees were planted in different mixtures in 10 (nearly-adjacent) 
plots and measured on four occasions over 10 years. The model is 
constructed in terms of the diameter increments (per year; DI) in the 3 
intervals, in which DI is modelled as a function of mid-interval D and DSQ. 
The details of the fixed part of the model are not so important here, but 
four pertinent variables are distance-dependent competition indices: 
C_spp1fromspp1, C_12, C_21, C_22.

The random structure is: random = ~ 1 | PLOT/TREE.

More complex random structures aren't required, there's no obvious 
heterogeneity, and the serial correlations (corSymm) are trivial.

I've been trying to fit a spatial correlation structure using the X, Y 
coordinates of each tree. I've obviously missed a point or so here.

It seems to me that such a structure could be fit for all trees (both 
species) on their average growth over the 3 intervals, by interval, or by 
species by interval.

For now, it would be step forward to get any of these working.

Apart from me learning how to do it, the import of this is that, if the 
CI's are doing what I hope they might, there should be little residual 
spatial correlation.

Thanks,

Lyndon.


From A.Robinson at ms.unimelb.edu.au  Wed Jun  7 02:12:13 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Wed, 7 Jun 2006 10:12:13 +1000
Subject: [R] spatial corStruct in lme
In-Reply-To: <6.2.0.14.0.20060607092358.03ba6008@popstaff.scu.edu.au>
References: <6.2.0.14.0.20060607092358.03ba6008@popstaff.scu.edu.au>
Message-ID: <20060607001213.GA18949@ms.unimelb.edu.au>

Hello Lyndon,

a couple of things you might try - for starters, the use of spatial
correlation structures is documented in Pinheiro and Bates (2002)
"Mixed Effects Models in S and S-Plus" from Springer-Verlag.  So, you
should try that.  Secondly, it's difficult to provide useful advice in
the absence of a small example.  What's best is, for example, if you
send some code snippets that generate sample data, or read it from an
existing package, and then showcase what you've tried.  If you can
then explain the difference between what you get and what you want,
it's easier to provide constructive advice.  You might be able to work
with the Wheat2 data in the nlme package.

Cheers,

Andrew

On Wed, Jun 07, 2006 at 09:35:02AM +1000, Lyndon Brooks wrote:
> Hi,
> 
> I'm fitting a relatively simple growth model to some forest plot data. Two 
> species of trees were planted in different mixtures in 10 (nearly-adjacent) 
> plots and measured on four occasions over 10 years. The model is 
> constructed in terms of the diameter increments (per year; DI) in the 3 
> intervals, in which DI is modelled as a function of mid-interval D and DSQ. 
> The details of the fixed part of the model are not so important here, but 
> four pertinent variables are distance-dependent competition indices: 
> C_spp1fromspp1, C_12, C_21, C_22.
> 
> The random structure is: random = ~ 1 | PLOT/TREE.
> 
> More complex random structures aren't required, there's no obvious 
> heterogeneity, and the serial correlations (corSymm) are trivial.
> 
> I've been trying to fit a spatial correlation structure using the X, Y 
> coordinates of each tree. I've obviously missed a point or so here.
> 
> It seems to me that such a structure could be fit for all trees (both 
> species) on their average growth over the 3 intervals, by interval, or by 
> species by interval.
> 
> For now, it would be step forward to get any of these working.
> 
> Apart from me learning how to do it, the import of this is that, if the 
> CI's are doing what I hope they might, there should be little residual 
> spatial correlation.
> 
> Thanks,
> 
> Lyndon.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From tim_smith_666 at yahoo.com  Wed Jun  7 02:29:45 2006
From: tim_smith_666 at yahoo.com (Tim Smith)
Date: Tue, 6 Jun 2006 17:29:45 -0700 (PDT)
Subject: [R] knn - 10 fold cross validation
Message-ID: <20060607002945.53764.qmail@web35002.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060606/67db30c6/attachment.pl 

From spencer.graves at pdf.com  Wed Jun  7 03:01:05 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 06 Jun 2006 18:01:05 -0700
Subject: [R] Nested and repeated effects together?
In-Reply-To: <44833E18.9040407@gmx.de>
References: <44833E18.9040407@gmx.de>
Message-ID: <44862551.1070202@pdf.com>

	  1.  Am I correct that you are asking for a 4-factor interaction 
without a full set of 2- and 3-factor interactions?   SAS may let you do 
that, but I'm not certain what that means.  The standard treatment of a 
factor with k levels is to code it as (k-1) linearly independent 
"contrasts" that are also linearly independent of the constant term and 
then use ordinary least squares.  As long as standard rules of hierarchy 
are observed, the resulting analyses of variance are independent of the 
specific set of contrasts chosen.  However, when rules of hierarchy are 
broken, the answers may change with different choices for contrasts, and 
it's far from obvious what one is even testing.

	  2.  For these kinds of problems, I usually use "lme" in the "nlme" 
package.  The function 'aov' is older, and its answers may not be as 
good if the design is not balanced.  I've seen only one case where 'aov' 
produced an answer that I couldn't get out of "lme", and that was a 
saturated model in a perfectly balanced experiment where the noise was 
estimated from higher order interactions.  For your case, I would 
consider variations on the following:

	  lme(y~(Gr+Hemi+Region+Gender)^4, random=~1|ID)

	  This assumes that each level of ID is unique.  If not, I suggest you 
make it unique by pasting it together with Region and Gender.

	  The construct (Gr+Hemi+Region+Gender)^4 indicates all main effects 
and interactions up to the fourth order.  If you want, say, all main 
effects plus the three factor interaction Gr*Hemi*Region, I believe you 
could get that from "Gr*Hemi*Region+Gender":  the term 'Gr*Hemi*Region' 
will force all the main effects an subordinate two-factor interactions 
into the model.

	  If you don't already have Pinheiro and Bates (2000) Mixed-Effects 
Models in S and S-Plus (Springer), I suggest you get a copy.  Bates is 
one of the leading contributors in nonlinear estimation and mixed 
models.  The book contains numerous examples.  Also, R script files are 
available for virtually everything discussed in the book.  This will 
allow you to work the examples yourself one line at a time as you read 
the accompanying discussion in the book.  To access them, find where R 
is installed on your hard drive, then find "~\library\nlme\scripts".

	  Hope this helps,
	  Spencer Graves

Stephan Kolassa wrote:
> Dear R people,
> 
> I am having a problem with modeling the following SAS code in R:
> 
> Class ID Gr Hemi Region Gender
> Model Y = Gr Region Hemi Gender Gr*Hemi Gr*Region Hemi*Region 
> Gender*Region Gender*Hemi Gr*Hemi*Region Gender*Hemi*Region 
> Gr*Gender*Hemi*Region
> Random Intercept Region Hemi /Subject = ID (Gr Gender)
> 
> I.e., ID is a random effect nested in Gr and Gender, leading to 
> ID-specific Intercept, Region and Hemi means. We have repeated 
> measurements within each ID (one measurement each for the different 
> combinations of Hemi and Region, i.e. 2 levels in Hemi: L vs. R; 4 
> levels in Region: F vs. C vs. T vs. PO).
> 
> I have been trying things like
> 
> aov(y~Gr+Region+Hemi+Gender+
Gr:Hemi+Gr:Region+Hemi:Region+Gender:Region+Gender:Hemi+
Gr:Hemi:Region+Gender:Hemi:Region+Gr:Gender:Hemi:Region+
>     Error(ID/(Gr+Gender))
> 
> and do get results, but I am very unsure whether this implements the 
> right model. An Error term like
> 
>     Error((1+Region+Hemi)/(ID_Vp/(Gender+Gr))), data=daten))
> 
> in the above model looks intuitively better to me, but (1) again: I'm 
> unsure about this, (2) this crashes my R.
> 
> Of course, I have been googling for all permutations of "Nested 
> effects", "repeated effects", "random effects" and digging through the 
> R-help archives, but I can't seem to locate a similar question having 
> been answered before.
> 
> Thank you all for your time!
> 
> Best regards,
> Stephan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From joe at gnacademy.org  Wed Jun  7 03:51:57 2006
From: joe at gnacademy.org (Joseph Wang)
Date: Tue, 6 Jun 2006 20:51:57 -0500
Subject: [R] S4 Methods with large numbers of args using memory
Message-ID: <200606062052.01714.joe@gnacademy.org>

Thanks all to the people on the r-help list for the backtick help.

I've run into another problem.  With my SWIG C++ wrapper for R, I'm finding 
that S4 generic functions with large numbers of argument (such as 10 or 11) 
seem to be taking up a lot of memory especially when a lot of the arguments 
are set to missing.

This happens because I'm generating S4 methods to wrapper C++ functions with 
optional arguments.  Each wrapper can take several meg of heap causing my R 
program to quickly run out of heap memory.

Any ideas as to what the problem is and any possible way of working around it?


From andy_liaw at merck.com  Wed Jun  7 03:52:18 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 6 Jun 2006 21:52:18 -0400
Subject: [R] knn - 10 fold cross validation
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA01BD0DF2@usctmx1106.merck.com>

You might want to check out the function tune.knn() in the e1071 package.
 
Andy

  _____  

From: r-help-bounces at stat.math.ethz.ch on behalf of Tim Smith
Sent: Tue 6/6/2006 8:29 PM
To: r-help at stat.math.ethz.ch
Subject: [R] knn - 10 fold cross validation [Broadcast]



Hi, 
   
  I was trying to get the optimal 'k' for the knn. To do this I was using
the following function : 
   
  
knn.cvk <- function(datmat, cl, k = 2:9) { 
    datmatT <- (datmat) 
  cv.err <- cl.pred <- c() 
  
  for (i in k) { 
    newpre <- as.vector(knn.cv(datmatT, cl, k = i)) 
    cl.pred <- cbind(cl.pred, newpre) 
    cv.err <- c(cv.err, sum(cl != newpre)) 
    
  } 
  k0 <- k[which.min(cv.err)] 
  print(k0) 
  return(k0) 
} 

   
  However, the knn.cv function does a 'leave one out' cross validation. I
checked the documentation to see if I could change this, but it appears that
I cannot. Since I have large datasets, I would like to do 10 fold cross
validation, instead of the 'leave one out'.

   
  Is there some other function that I can use that will give me a 10 fold
cross validation for KNN ? 
   
  many thanks. 

 __________________________________________________ 



        [[alternative HTML version deleted]] 

______________________________________________ 
R-help at stat.math.ethz.ch mailing list 
https://stat.ethz.ch/mailman/listinfo/r-help
<https://stat.ethz.ch/mailman/listinfo/r-help>  
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
<http://www.R-project.org/posting-guide.html>


From spencer.graves at pdf.com  Wed Jun  7 05:52:47 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 06 Jun 2006 20:52:47 -0700
Subject: [R] making matrix monotonous
In-Reply-To: <4484149D.3050209@7d4.com>
References: <4484149D.3050209@7d4.com>
Message-ID: <44864D8F.10108@pdf.com>

	  I agree it would be great to sort the variables in a correlation 
matrix to make it easier to read and see patterns.  I don't know any 
functions for doing that.  If it were my problem, I might "order" the 
variables by their first principal component.  There may also be some 
cluster analysis way to do that, but I don't know it well enough to say.

	  Hope this helps.
	  Spencer Graves

vincent at 7d4.com wrote:
> Dear all,
> 
> I'm currently working on correlation matrix.
> The function image() is quite useful for visualization.
> 
> Before using image(), I'd like to sort the matrix rows/columns,
> in order to make the matrix the more monotonous/smooth possible.
> 
> Before reinventing the wheel, is there somebody here aware if
> such a function already exists ?
> 
> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From zelickr at pdx.edu  Wed Jun  7 05:55:28 2006
From: zelickr at pdx.edu (Randy Zelick)
Date: Tue, 6 Jun 2006 20:55:28 -0700 (Pacific Daylight Time)
Subject: [R] x y averaging
In-Reply-To: <efb536d50606061401k58251e79g28d1837cc4d0f308@mail.gmail.com>
References: <20060606204428.85696.qmail@web51607.mail.yahoo.com>
	<efb536d50606061401k58251e79g28d1837cc4d0f308@mail.gmail.com>
Message-ID: <Pine.WNT.4.64.0606062012170.3680@BIO-SB2-329-RZN.PSU.DS.PDX.EDU>

Hello,

I am trying to average a number of data sets where the x vector contains 
times and the Y data are instrument readings. The vectors all have 
different numbers of values, but many X time values match. For example:

A fragment of the first data set:

x vect    value
14:56:10  0.325
14:62:11  0.111
14:68:11  0.214
.
.
.
this can go on for 100's of values, spaced by 6 minutes, but there can be 
gaps too, like:

16:05:18  0.245
16:11:09  0.266
16:17:05  0.271
16:33:00  0.304
16:39:05  0.300


A fragment of the second data set:

15:59:08  0.255
16:05:44  0.281
16:11:25  0.249
16:17:39  0.238
16:23:51  0.288


...and the result I am looking for is a new vector that looks like this:

14:56:10  0.325
14:62:11  0.111
14:68:11  0.214
.
.
.
15:59:08  0.255
16:05:18  0.2630  * matches, so average
16:11:09  0.2575  * matches, so average
16:17:05  0.2545  * matches, so average
16:33:00  0.304
16:39:05  0.300


The times values in the new X vector are properly interleaved taking data 
across the datasets, but when there is a match the Y values are averaged. 
Note that the seconds don't matter.

I am using R Version 2.2.1 on a PC

Thanks for thinking about it,

=Randy=

R. Zelick				email: zelickr at pdx.edu
Department of Biology			voice: 503-725-3086
Portland State University		fax:   503-725-3888

mailing:
P.O. Box 751
Portland, OR 97207

shipping:
1719 SW 10th Ave, Room 246
Portland, OR 97201


From ggrothendieck at gmail.com  Wed Jun  7 06:44:31 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 7 Jun 2006 00:44:31 -0400
Subject: [R] x y averaging
In-Reply-To: <Pine.WNT.4.64.0606062012170.3680@BIO-SB2-329-RZN.PSU.DS.PDX.EDU>
References: <20060606204428.85696.qmail@web51607.mail.yahoo.com>
	<efb536d50606061401k58251e79g28d1837cc4d0f308@mail.gmail.com>
	<Pine.WNT.4.64.0606062012170.3680@BIO-SB2-329-RZN.PSU.DS.PDX.EDU>
Message-ID: <971536df0606062144n23b52654p8944727b77c2f902@mail.gmail.com>

Modifying the invalid times 14:62:11 and 14:68:11 to other values
try the following:


Lines1 <- "x y
14:56:10  0.325
14:57:11  0.111
14:58:11  0.214
16:05:18  0.245
16:11:09  0.266
16:17:05  0.271
16:33:00  0.304
16:39:05  0.300"

Lines2 <- "x y
15:59:08  0.255
16:05:44  0.281
16:11:25  0.249
16:17:39  0.238
16:23:51  0.288"

library(zoo)
library(chron)
z1 <- read.zoo(textConnection(Lines1), header = TRUE, FUN = times)
z2 <- read.zoo(textConnection(Lines2), header = TRUE, FUN = times)
z <- merge(z1, z2)  # use merge(z1, z2, ..., zn) if you have n of these
zoo(rowMeans(z, na.rm = TRUE), time(z))


On 6/6/06, Randy Zelick <zelickr at pdx.edu> wrote:
> Hello,
>
> I am trying to average a number of data sets where the x vector contains
> times and the Y data are instrument readings. The vectors all have
> different numbers of values, but many X time values match. For example:
>
> A fragment of the first data set:
>
> x vect    value
> 14:56:10  0.325
> 14:62:11  0.111
> 14:68:11  0.214
> .
> .
> .
> this can go on for 100's of values, spaced by 6 minutes, but there can be
> gaps too, like:
>
> 16:05:18  0.245
> 16:11:09  0.266
> 16:17:05  0.271
> 16:33:00  0.304
> 16:39:05  0.300
>
>
> A fragment of the second data set:
>
> 15:59:08  0.255
> 16:05:44  0.281
> 16:11:25  0.249
> 16:17:39  0.238
> 16:23:51  0.288
>
>
> ...and the result I am looking for is a new vector that looks like this:
>
> 14:56:10  0.325
> 14:62:11  0.111
> 14:68:11  0.214
> .
> .
> .
> 15:59:08  0.255
> 16:05:18  0.2630  * matches, so average
> 16:11:09  0.2575  * matches, so average
> 16:17:05  0.2545  * matches, so average
> 16:33:00  0.304
> 16:39:05  0.300
>
>
> The times values in the new X vector are properly interleaved taking data
> across the datasets, but when there is a match the Y values are averaged.
> Note that the seconds don't matter.
>
> I am using R Version 2.2.1 on a PC
>
> Thanks for thinking about it,
>
> =Randy=
>
> R. Zelick                               email: zelickr at pdx.edu
> Department of Biology                   voice: 503-725-3086
> Portland State University               fax:   503-725-3888
>
> mailing:
> P.O. Box 751
> Portland, OR 97207
>
> shipping:
> 1719 SW 10th Ave, Room 246
> Portland, OR 97201
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From dlr32 at cornell.edu  Wed Jun  7 07:14:23 2006
From: dlr32 at cornell.edu (Dan Rabosky)
Date: Wed, 07 Jun 2006 01:14:23 -0400
Subject: [R] Building packages in R - 'private' functions
Message-ID: <5.2.1.1.2.20060607004234.032b7cd0@postoffice9.mail.cornell.edu>


Hello.

I am creating an R package that I'd like to submit to CRAN (OS Windows 
XP).  How do I distinguish among 'public' functions, e.g., those that are 
intended to be called by users of the package and for which I am providing 
documentation & examples, and 'private' functions, which are used 
internally by the 'public' functions, but for which I do not wish to 
provide documentation?  The private functions are all coded in R (nothing 
in C or Fortran) and are essential to the operation of several public 
functions.

I have been unable to find any documentation on this in the 'writing r 
extensions' manual', on previous posts to R-help, or through any other 
source.  One possibility is to include the source code for the 'private' 
functions within the public functions.  However, since multiple public 
functions utilize the same core set of 'private' functions, this seems 
unwieldy and redundant at best.

If I simply include the source for the 'private' functions in the "R" 
directory (without corresponding *.Rd and *.html documentation in /man), 
then check the package with "R CMD check', it does appear to process the 
private functions (and successfully builds with R CMD build).  However, I 
do receive a warning for including undocumented code objects.  Is this the 
recommended approach and/or is there a better way to do this?  One 
potential problem with this approach is that - should an error occur within 
a private function, it may be very difficult for the user to decipher the 
nature of the problem.

Any suggestions will be greatly appreciated.
~Dan Rabosky



Dan Rabosky
Department of Ecology and Evolutionary Biology
237 Corson Hall
Cornell University
Ithaca, NY14853-2701 USA
DLR32 at cornell.edu
web: http://www.birds.cornell.edu/evb/Graduates_Dan.htm


From gavin.simpson at ucl.ac.uk  Wed Jun  7 08:03:09 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 07 Jun 2006 07:03:09 +0100
Subject: [R] Building packages in R - 'private' functions
In-Reply-To: <5.2.1.1.2.20060607004234.032b7cd0@postoffice9.mail.cornell.edu>
References: <5.2.1.1.2.20060607004234.032b7cd0@postoffice9.mail.cornell.edu>
Message-ID: <1149660190.2396.9.camel@localhost.localdomain>

On Wed, 2006-06-07 at 01:14 -0400, Dan Rabosky wrote:
> Hello.
> 
> I am creating an R package that I'd like to submit to CRAN (OS Windows 
> XP).  How do I distinguish among 'public' functions, e.g., those that are 
> intended to be called by users of the package and for which I am providing 
> documentation & examples, and 'private' functions, which are used 
> internally by the 'public' functions, but for which I do not wish to 
> provide documentation?  The private functions are all coded in R (nothing 
> in C or Fortran) and are essential to the operation of several public 
> functions.

Hi Dan,

The answer is in the Writing R Extensions manual.

You could do either:

        1) Put the code for your "private" functions in a file names
        internal.R. You then provide a simple file named
        <package-name>-internal.Rd which lists in individual \alias{}
        markup the names of the private functions, eg;
        
        \name{mypackage-internal}
        \alias{foo1}
        \alias{foo2}
        \alias{foo3}
        \alias{foo4}
        \title{Internal mypackage Functions}
        \description{
          Internal mypackage functions
        }
        \details{
          These are not to be called by the user.
        }
        \keyword{ internal }
        
        But even here, you aren't documenting the internal functions,
        just working round the package checks.
        
        2) Place your package in a namespace, which is documented fully
        in Writing R Extensions.

Not sure what the best advice is - I'd guess that for all but the
simplest packages, namespaces are the preferred way, but the internal.R
way works just fine also.

By the way, in future, questions of this nature are best asked on the
R-Devel list, not here.

HTH,

Gavin


From ripley at stats.ox.ac.uk  Wed Jun  7 08:28:21 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 7 Jun 2006 07:28:21 +0100 (BST)
Subject: [R] knn - 10 fold cross validation
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA01BD0DF2@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA01BD0DF2@usctmx1106.merck.com>
Message-ID: <Pine.LNX.4.64.0606070725170.11529@gannet.stats.ox.ac.uk>

10-fold cross-validation is easily done at R level: there is generic code 
in MASS, the book knn was written to support.

knn and lda have options for leave-one-out cross-validation just because 
there are compuiationally efficient algorithms for those cases.

On Tue, 6 Jun 2006, Liaw, Andy wrote:

> You might want to check out the function tune.knn() in the e1071 package.
>
> Andy
>
>  _____
>
> From: r-help-bounces at stat.math.ethz.ch on behalf of Tim Smith
> Sent: Tue 6/6/2006 8:29 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] knn - 10 fold cross validation [Broadcast]
>
>
>
> Hi,
>
>  I was trying to get the optimal 'k' for the knn. To do this I was using
> the following function :
>
>
> knn.cvk <- function(datmat, cl, k = 2:9) {
>    datmatT <- (datmat)
>  cv.err <- cl.pred <- c()
>
>  for (i in k) {
>    newpre <- as.vector(knn.cv(datmatT, cl, k = i))
>    cl.pred <- cbind(cl.pred, newpre)
>    cv.err <- c(cv.err, sum(cl != newpre))
>
>  }
>  k0 <- k[which.min(cv.err)]
>  print(k0)
>  return(k0)
> }
>
>
>  However, the knn.cv function does a 'leave one out' cross validation. I
> checked the documentation to see if I could change this, but it appears that
> I cannot. Since I have large datasets, I would like to do 10 fold cross
> validation, instead of the 'leave one out'.
>
>
>  Is there some other function that I can use that will give me a 10 fold
> cross validation for KNN ?
>
>  many thanks.
>
> __________________________________________________
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> <https://stat.ethz.ch/mailman/listinfo/r-help>
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> <http://www.R-project.org/posting-guide.html>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Wed Jun  7 08:38:17 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 7 Jun 2006 07:38:17 +0100 (BST)
Subject: [R] build R with Visual Studio
In-Reply-To: <4485E947.3000905@lindaspaces.com>
References: <4485E947.3000905@lindaspaces.com>
Message-ID: <Pine.LNX.4.64.0606070733450.11529@gannet.stats.ox.ac.uk>

On Tue, 6 Jun 2006, Jennifer Lai wrote:

> Hi,
>    Has anyone had success in building R source with Visual Studio?  I
> followed the instructions in README.packages, but failed on the very
> first step, where it's looking for R.dll. I looked through R source and
> couldn't find the file. Can someone point me to where this file is
> located or generated? Thanks!

R.dll is the main file generated, and the first step is to build Rpwd.exe.
Do you really mean the R source?

People have built R for Windows with Visual Studio (using their own 
projects/makefiles and other tools to generate .def files) but it did not 
work correctly.  It seems that the IEC60559 (aka IEEE754) compliance of 
VC++ was not adequate -- as I recall it thought -Inf > 3.

This isn't the list for such programming questions: R-devel would be more 
appropriate.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Wed Jun  7 08:51:59 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 7 Jun 2006 07:51:59 +0100 (BST)
Subject: [R] vague errors on R CMD check for very minimal S4-style
	package
In-Reply-To: <m2irnevv34.fsf@ziti.local>
References: <C26BCE7602F7214FA9B98B050027AE17A456E3@US-BE4.corp.mpi.com>
	<m2irnevv34.fsf@ziti.local>
Message-ID: <Pine.LNX.4.64.0606070739390.11529@gannet.stats.ox.ac.uk>

This is an S4-using package but missing a dependence on 'methods'.

To run R CMD check the package has to load under minimal conditions, e.g.

R_DEFAULT_PACKAGES=NULL R
...
> library(foo)

should work.  'methods' is quite expensive to load, and so is only used in 
checking if explicitly requested.

On Tue, 6 Jun 2006, Seth Falcon wrote:

> "Roels, Steven" <Steven.Roels at mpi.com> writes:
>
>> Hello,
>>
>> I have a very minimal package "simplepkg" (DESCRIPTION, NAMESPACE, and
>> R) with S4 classes/methods (defines a class "foo" and a show method for
>> that class" - both the class and show method are exported).  I can
>> seemingly install the package, then load and use it:
>>
>> Error: package/namespace load failed for 'simplepkg'
>> Call sequence:
>> 2: stop(gettextf("package/namespace load failed for '%s'",
>> libraryPkgName(package)),
>>        call. = FALSE, domain = NA)
>> 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
>> FALSE)
>> Execution halted
>
>>
>> Here are the file contents:
>> -------------------------------
>>
>> sun890% cat DESCRIPTION
>> Package: simplepkg
>> Type: Package
>> Title: Does stuff
>> Version: 0.1-1
>> Date: 2006-06-06
>> Author: Me
>> Maintainer: Also Me <me at here.com>
>> Description: Does interesting stuff
>> License: GPL
>
> Try adding LazyLoad: yes to DESCRIPTION (or SaveImage: yes).
>
And

Depends: methods

Package stats4 is provided in part as an example of a small S4-using 
package for people to copy.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Wed Jun  7 09:14:18 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 7 Jun 2006 08:14:18 +0100 (BST)
Subject: [R] x y averaging
In-Reply-To: <Pine.WNT.4.64.0606062012170.3680@BIO-SB2-329-RZN.PSU.DS.PDX.EDU>
References: <20060606204428.85696.qmail@web51607.mail.yahoo.com>
	<efb536d50606061401k58251e79g28d1837cc4d0f308@mail.gmail.com>
	<Pine.WNT.4.64.0606062012170.3680@BIO-SB2-329-RZN.PSU.DS.PDX.EDU>
Message-ID: <Pine.LNX.4.64.0606070754540.11529@gannet.stats.ox.ac.uk>

It's confusing to have time values that are different but `match', so 
first you want to reduce your times to hh:mm  (e.g. with substr).

Then you can cbind() the data frames, and use tapply() to do the 
averaging. E.g.

d1 <- read.table("foo1")
d2 <- read.table("foo2")
d <- cbind(d1, d2)
d$time <- factor(substr(as.character(d$x), 1, 5))
tapply(d$y, d$time, mean)
  14:56  14:62  14:68  15:59  16:05  16:11  16:17  16:23  16:33  16:39
0.3250 0.1110 0.2140 0.2575 0.2545 0.3040 0.2550 0.2630 0.2630 0.3000

There are other ways, e.g.

aggregate(d["y"], list(d$time), mean)

or via ave().


On Tue, 6 Jun 2006, Randy Zelick wrote:

> Hello,
>
> I am trying to average a number of data sets where the x vector contains
> times and the Y data are instrument readings. The vectors all have
> different numbers of values, but many X time values match. For example:
>
> A fragment of the first data set:
>
> x vect    value
> 14:56:10  0.325
> 14:62:11  0.111
> 14:68:11  0.214
> .
> .
> .
> this can go on for 100's of values, spaced by 6 minutes, but there can be
> gaps too, like:
>
> 16:05:18  0.245
> 16:11:09  0.266
> 16:17:05  0.271
> 16:33:00  0.304
> 16:39:05  0.300
>
>
> A fragment of the second data set:
>
> 15:59:08  0.255
> 16:05:44  0.281
> 16:11:25  0.249
> 16:17:39  0.238
> 16:23:51  0.288
>
>
> ...and the result I am looking for is a new vector that looks like this:
>
> 14:56:10  0.325
> 14:62:11  0.111
> 14:68:11  0.214
> .
> .
> .
> 15:59:08  0.255
> 16:05:18  0.2630  * matches, so average
> 16:11:09  0.2575  * matches, so average
> 16:17:05  0.2545  * matches, so average
> 16:33:00  0.304
> 16:39:05  0.300
>
>
> The times values in the new X vector are properly interleaved taking data
> across the datasets, but when there is a match the Y values are averaged.
> Note that the seconds don't matter.
>
> I am using R Version 2.2.1 on a PC
>
> Thanks for thinking about it,
>
> =Randy=
>
> R. Zelick				email: zelickr at pdx.edu
> Department of Biology			voice: 503-725-3086
> Portland State University		fax:   503-725-3888
>
> mailing:
> P.O. Box 751
> Portland, OR 97207
>
> shipping:
> 1719 SW 10th Ave, Room 246
> Portland, OR 97201
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From j.van_den_hoff at fz-rossendorf.de  Wed Jun  7 09:49:54 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Wed, 07 Jun 2006 09:49:54 +0200
Subject: [R] Building packages in R - 'private' functions
In-Reply-To: <5.2.1.1.2.20060607004234.032b7cd0@postoffice9.mail.cornell.edu>
References: <5.2.1.1.2.20060607004234.032b7cd0@postoffice9.mail.cornell.edu>
Message-ID: <44868522.5060509@fz-rossendorf.de>

Dan Rabosky wrote:
> Hello.
> 
> I am creating an R package that I'd like to submit to CRAN (OS Windows 
> XP).  How do I distinguish among 'public' functions, e.g., those that are 
> intended to be called by users of the package and for which I am providing 
> documentation & examples, and 'private' functions, which are used 
> internally by the 'public' functions, but for which I do not wish to 
> provide documentation?  The private functions are all coded in R (nothing 
> in C or Fortran) and are essential to the operation of several public 
> functions.
> 
> I have been unable to find any documentation on this in the 'writing r 
> extensions' manual', on previous posts to R-help, or through any other 
> source.  One possibility is to include the source code for the 'private' 
> functions within the public functions.  However, since multiple public 
> functions utilize the same core set of 'private' functions, this seems 
> unwieldy and redundant at best.
> 
> If I simply include the source for the 'private' functions in the "R" 
> directory (without corresponding *.Rd and *.html documentation in /man), 
> then check the package with "R CMD check', it does appear to process the 
> private functions (and successfully builds with R CMD build).  However, I 
> do receive a warning for including undocumented code objects.  Is this the 
> recommended approach and/or is there a better way to do this?  One 
> potential problem with this approach is that - should an error occur within 
> a private function, it may be very difficult for the user to decipher the 
> nature of the problem.
> 
> Any suggestions will be greatly appreciated.
> ~Dan Rabosky
> 
> 
> 
> Dan Rabosky
> Department of Ecology and Evolutionary Biology
> 237 Corson Hall
> Cornell University
> Ithaca, NY14853-2701 USA
> DLR32 at cornell.edu
> web: http://www.birds.cornell.edu/evb/Graduates_Dan.htm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

it's in the 'extensions' manual, including an example I believe:

1.
create a file `NAMESPACE' in the package top level dir (beside `R' and 
`man') containing the single line

exportPattern("^[^\\.]")

2.
name all private functions with a leading `.' (more precisely: all 
functions starting with a `.' are private in this setting).


of course, you can modify the pattern to suit another naming convention.

joerg


From antonio.fabio at gmail.com  Wed Jun  7 10:10:18 2006
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Wed, 7 Jun 2006 10:10:18 +0200
Subject: [R] Building packages in R - 'private' functions
In-Reply-To: <5.2.1.1.2.20060607004234.032b7cd0@postoffice9.mail.cornell.edu>
References: <5.2.1.1.2.20060607004234.032b7cd0@postoffice9.mail.cornell.edu>
Message-ID: <b0808fdc0606070110u5da2aed0h995a879e31973f64@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060607/d8b28391/attachment.pl 

From rkrug at sun.ac.za  Wed Jun  7 10:26:11 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Wed, 07 Jun 2006 10:26:11 +0200
Subject: [R] How to create list of objects?
In-Reply-To: <00e801c68977$c7f73ba0$0540210a@www.domain>
References: <44859369.7060507@sun.ac.za>
	<00e801c68977$c7f73ba0$0540210a@www.domain>
Message-ID: <44868DA3.9020800@sun.ac.za>

Thanks everybody - it's working

Rainer

Dimitris Rizopoulos wrote:
> try something like:
> 
> lapply(f, summary)
> sapply(f, function(x) AIC(logLik(x)))
> 
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://med.kuleuven.be/biostat/
>      http://www.student.kuleuven.be/~m0390867/dimitris.htm
> 
> 
> ----- Original Message ----- 
> From: "Rainer M Krug" <rkrug at sun.ac.za>
> To: "R help list" <r-help at stat.math.ethz.ch>
> Sent: Tuesday, June 06, 2006 4:38 PM
> Subject: [R] How to create list of objects?
> 
> 
>> Hi
>>
>> I am doing several mle and want to store them in a list (or whatever 
>> is
>> the right construct) to be able to analyse them later.
>>
>> at the moment I am doing:
>>
>> f <- list()
>> f$IP <- mle(...)
>> f$NE <- mle(...)
>>
>> but when I say:
>>> summary(f)
>> I get:
>>
>>     Length Class Mode
>> IP   0      mle   list
>> NE   0      mle   list
>>
>> I don't get the output I would have, i.e. the one from
>>> summary(f$IP)
>> summary(f$IP)
>> Maximum likelihood estimation
>>
>> Call:
>> mle(minuslogl = IPNeglogPoisL, method = "L-BFGS-B", fixed = list(),
>>    control = list(maxit = 1e+08, factr = 1e-20))
>>
>> Coefficients:
>>      Estimate  Std. Error
>> a 1242.0185506 44.92341097
>> b    0.8802538  0.01685811
>>
>> -2 log L: 145.3509
>>
>>
>> What I want to do is something like:
>>
>> AICs <- AIC(logLik(f))
>>
>> and then have all the AICs in the vector AICs.
>>
>> It must be possible or is this again a namespace issue?
>>
>> Rainer
>>
>> -- 
>> Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
>> Biology (UCT)
>>
>> Department of Conservation Ecology and Entomology
>> University of Stellenbosch
>> Matieland 7602
>> South Africa
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
      	Rainer at krugs.de


From vincent at 7d4.com  Wed Jun  7 11:20:11 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Wed, 07 Jun 2006 11:20:11 +0200
Subject: [R] making matrix monotonous
In-Reply-To: <44864D8F.10108@pdf.com>
References: <4484149D.3050209@7d4.com> <44864D8F.10108@pdf.com>
Message-ID: <44869A4B.3010905@7d4.com>

Spencer Graves a ?crit :

>       I agree it would be great to sort the variables in a correlation 
> matrix to make it easier to read and see patterns.  I don't know any 
> functions for doing that.  If it were my problem, I might "order" the 
> variables by their first principal component.  There may also be some 
> cluster analysis way to do that, but I don't know it well enough to say.
>       Hope this helps.
>       Spencer Graves

Thanks for your answer Spencer.

Here is a first result of a very simple and naive approach.
http://7d4.com/r/

Of course, there is no assumption the sorting is "optimal",
but on this little example it helps the matrix being
more readable.

Vincent


From f.calboli at imperial.ac.uk  Wed Jun  7 12:40:20 2006
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Wed, 07 Jun 2006 11:40:20 +0100
Subject: [R] smoothing plot(x, type ='l')
Message-ID: <4486AD14.7020507@imperial.ac.uk>

Hi All,

I am using plot(x, type = 'l') for some plotting, but I would like rounded edges 
rather than jagged edges in the plot (purely for aestetic reasons).

How could I achieve that?

Cheers,

Federico

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com


From dimitris.rizopoulos at med.kuleuven.be  Wed Jun  7 12:57:31 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 7 Jun 2006 12:57:31 +0200
Subject: [R] smoothing plot(x, type ='l')
References: <4486AD14.7020507@imperial.ac.uk>
Message-ID: <000601c68a21$2cd26e60$0540210a@www.domain>

probably you want to use the `lend' argument of ?par(); I hope it 
helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Federico Calboli" <f.calboli at imperial.ac.uk>
To: "r-help" <r-help at stat.math.ethz.ch>
Sent: Wednesday, June 07, 2006 12:40 PM
Subject: [R] smoothing plot(x, type ='l')


> Hi All,
>
> I am using plot(x, type = 'l') for some plotting, but I would like 
> rounded edges
> rather than jagged edges in the plot (purely for aestetic reasons).
>
> How could I achieve that?
>
> Cheers,
>
> Federico
>
> -- 
> Federico C. F. Calboli
> Department of Epidemiology and Public Health
> Imperial College, St Mary's Campus
> Norfolk Place, London W2 1PG
>
> Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193
>
> f.calboli [.a.t] imperial.ac.uk
> f.calboli [.a.t] gmail.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From francoisromain at free.fr  Wed Jun  7 12:54:25 2006
From: francoisromain at free.fr (Romain Francois)
Date: Wed, 07 Jun 2006 12:54:25 +0200
Subject: [R] making matrix monotonous
In-Reply-To: <44869A4B.3010905@7d4.com>
References: <4484149D.3050209@7d4.com> <44864D8F.10108@pdf.com>
	<44869A4B.3010905@7d4.com>
Message-ID: <4486B061.5070108@free.fr>

Le 07.06.2006 11:20, vincent at 7d4.com a ?crit :
> Spencer Graves a ?crit :
>
>   
>>       I agree it would be great to sort the variables in a correlation 
>> matrix to make it easier to read and see patterns.  I don't know any 
>> functions for doing that.  If it were my problem, I might "order" the 
>> variables by their first principal component.  There may also be some 
>> cluster analysis way to do that, but I don't know it well enough to say.
>>       Hope this helps.
>>       Spencer Graves
>>     
>
> Thanks for your answer Spencer.
>
> Here is a first result of a very simple and naive approach.
> http://7d4.com/r/
>
> Of course, there is no assumption the sorting is "optimal",
> but on this little example it helps the matrix being
> more readable.
>
> Vincent
>   
Hello Vincent,

Ahhh, the double for loop, the semicolon, the return call. you still 
believe in R code looking like C don't you.
Try this one :

matrix.sort2 <- function(M, fun = function(m) colSums(abs(m)) ){
  M[or <- order(fun(M) , decreasing=T), or]
}

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
mixmod 1.7 is released : http://www-math.univ-fcomte.fr/mixmod/index.php
+---------------------------------------------------------------+
| Romain FRANCOIS - http://francoisromain.free.fr               |
| Doctorant INRIA Futurs / EDF                                  |
+---------------------------------------------------------------+


From f.calboli at imperial.ac.uk  Wed Jun  7 12:56:43 2006
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Wed, 07 Jun 2006 11:56:43 +0100
Subject: [R] smoothing plot(x, type ='l')
In-Reply-To: <000601c68a21$2cd26e60$0540210a@www.domain>
References: <4486AD14.7020507@imperial.ac.uk>
	<000601c68a21$2cd26e60$0540210a@www.domain>
Message-ID: <4486B0EB.9010301@imperial.ac.uk>

Dimitris Rizopoulos wrote:
> probably you want to use the `lend' argument of ?par(); I hope it helps.

Does not seem to work in my case.

F

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com


From ripley at stats.ox.ac.uk  Wed Jun  7 13:00:06 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 7 Jun 2006 12:00:06 +0100 (BST)
Subject: [R] smoothing plot(x, type ='l')
In-Reply-To: <4486AD14.7020507@imperial.ac.uk>
References: <4486AD14.7020507@imperial.ac.uk>
Message-ID: <Pine.LNX.4.64.0606071157170.21891@gannet.stats.ox.ac.uk>

On Wed, 7 Jun 2006, Federico Calboli wrote:

> Hi All,
>
> I am using plot(x, type = 'l') for some plotting, but I would like 
> rounded edges rather than jagged edges in the plot (purely for aestetic 
> reasons).
>
> How could I achieve that?

It I understand you aright, that is done by par(lend) but the default is 
"round".  So maybe your graphics device (unstated) on your OS (unstated) 
does not support this.

We need more details, and preferably a simple reproducible example.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ggrothendieck at gmail.com  Wed Jun  7 13:13:12 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 7 Jun 2006 07:13:12 -0400
Subject: [R] making matrix monotonous
In-Reply-To: <4486B061.5070108@free.fr>
References: <4484149D.3050209@7d4.com> <44864D8F.10108@pdf.com>
	<44869A4B.3010905@7d4.com> <4486B061.5070108@free.fr>
Message-ID: <971536df0606070413w3d0e2da2jc43532be4bcedf3f@mail.gmail.com>

On 6/7/06, Romain Francois <francoisromain at free.fr> wrote:
> Le 07.06.2006 11:20, vincent at 7d4.com a ?crit :
> > Spencer Graves a ?crit :
> >
> >
> >>       I agree it would be great to sort the variables in a correlation
> >> matrix to make it easier to read and see patterns.  I don't know any
> >> functions for doing that.  If it were my problem, I might "order" the
> >> variables by their first principal component.  There may also be some
> >> cluster analysis way to do that, but I don't know it well enough to say.
> >>       Hope this helps.
> >>       Spencer Graves
> >>
> >
> > Thanks for your answer Spencer.
> >
> > Here is a first result of a very simple and naive approach.
> > http://7d4.com/r/
> >
> > Of course, there is no assumption the sorting is "optimal",
> > but on this little example it helps the matrix being
> > more readable.
> >
> > Vincent
> >
> Hello Vincent,
>
> Ahhh, the double for loop, the semicolon, the return call. you still
> believe in R code looking like C don't you.
> Try this one :
>
> matrix.sort2 <- function(M, fun = function(m) colSums(abs(m)) ){
>  M[or <- order(fun(M) , decreasing=T), or]
> }

Even if this works I don't think its guaranteed since one cannot
be sure the first argument, or<-..., is evaluated before the second, or.
Also use TRUE in case there is a T variable in workspace:

matrix.sort3 <- function(M, fun = function(m) colSums(abs(m)) ) {
  or <- order(fun(M), decreasing = TRUE)
  M[or, or]
}


From f.calboli at imperial.ac.uk  Wed Jun  7 13:13:12 2006
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Wed, 07 Jun 2006 12:13:12 +0100
Subject: [R] smoothing plot(x, type ='l')
In-Reply-To: <Pine.LNX.4.64.0606071157170.21891@gannet.stats.ox.ac.uk>
References: <4486AD14.7020507@imperial.ac.uk>
	<Pine.LNX.4.64.0606071157170.21891@gannet.stats.ox.ac.uk>
Message-ID: <4486B4C8.40202@imperial.ac.uk>

Prof Brian Ripley wrote:
  > It I understand you aright, that is done by par(lend) but the default is
> "round".  So maybe your graphics device (unstated) on your OS (unstated) 
> does not support this.

graphics device = X11 (xserver-xorg)
OS = Debian GNU/Linux, Kernel 2.4.27-2-686-smp

> 
> We need more details, and preferably a simple reproducible example.

s.off.dist
   [1] 27 26 15  7 32 50 31 19  1 11  8  4  4  5 11 28  4 32  5 39 15 32  3  3  4
  [26]  1  2  2 22  1 23  4  2  8 40 14 42  3  1  4  3  4  4  6  2 29  4  8  5  9
  [51] 37  2  1 13 13 35  6  9  2  5 31 10  7  1  4 22  3 23  4 10  8 57  1  6  1
  [76]  1  4 10 16  3 12  3 10  8 10 11 16 19  5  5  9  9  3  4  1  8  3 12  3 65
[101]  1  7 21  7  2  4 35 15  6  2  6  4  1 14  4 10 24  3  4  3  2  4 11  4  7
[126] 13  7  1  6  2  4 16  3 13 66 10  4  7  2 17  6  4  3  5  6  8  3  3 10 19
[151]  7  3 17  6  6  6  2  9  5  4  4 18  2  3 17 43 22 12  1  3  1  9  3  5  1
[176]  2  5 36 12 23  1  8 10  6  7 19  5 13  2  5  3  9  3  1 12  4  5  3  6  3
[201]  5  1  1 16  6 12  1  5  4  2  2  4  9  9  3 11  7  4  8 14  5 17  3  3 15
[226]  2  4  2 11 13  1 19  7  4  3 20  2  8  5  2  3  4  2  5  5 10  1  9 10  8
[251]  4  4  2  1  3  5  3  1  4  5 13 12  6  5  4  3 10  5  4  1

plot(hist(s.off.dist, breaks = 'fd')$counts ~ hist(s.off.dist, breaks = 'fd')$m, 
type = 'l')

I want the edges to look round, if at all possible.

Cheers,

Federico



-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com


From jim at bitwrit.com.au  Thu Jun  8 03:23:26 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Wed, 07 Jun 2006 21:23:26 -0400
Subject: [R] smoothing plot(x, type ='l')
In-Reply-To: <4486AD14.7020507@imperial.ac.uk>
References: <4486AD14.7020507@imperial.ac.uk>
Message-ID: <44877C0E.6010904@bitwrit.com.au>

Federico Calboli wrote:
> Hi All,
> 
> I am using plot(x, type = 'l') for some plotting, but I would like rounded edges 
> rather than jagged edges in the plot (purely for aestetic reasons).
> 
> How could I achieve that?
> 
Perhaps you want something like:
x<-rnorm(50)
plot(spline(1:50,x),type="l")

Jim


From ivan.kalafatic at gmail.com  Wed Jun  7 14:32:06 2006
From: ivan.kalafatic at gmail.com (Ivan Kalafatic)
Date: Wed, 7 Jun 2006 13:32:06 +0100
Subject: [R] Help with selecting data from irregular time series {its}
	objects
Message-ID: <6dbf89a50606070532s312d2e27hccdde811f0542f1d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060607/f8051cf0/attachment.pl 

From stat700004 at yahoo.co.in  Wed Jun  7 14:35:54 2006
From: stat700004 at yahoo.co.in (stat stat)
Date: Wed, 7 Jun 2006 13:35:54 +0100 (BST)
Subject: [R] Edit function
Message-ID: <20060607123554.15027.qmail@web8403.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060607/c3fd3d99/attachment.pl 

From friendly at yorku.ca  Wed Jun  7 14:54:41 2006
From: friendly at yorku.ca (Michael Friendly)
Date: Wed, 07 Jun 2006 08:54:41 -0400
Subject: [R] making matrix monotonous
In-Reply-To: <44869A4B.3010905@7d4.com>
References: <4484149D.3050209@7d4.com> <44864D8F.10108@pdf.com>
	<44869A4B.3010905@7d4.com>
Message-ID: <4486CC91.8010301@yorku.ca>

[Presuming you mean monotone; many matrices are already monotonous.]

General solutions to this problem are discussed in

@ARTICLE{Friendly:02:corrgram,
   author = {M. Friendly},
   title = {Corrgrams: Exploratory displays for correlation matrices},
   journal = {The American Statistician},
   year = {2002},
   volume = {56},
   pages = {316--324},
   number = {4},
   url = {http://www.math.yorku.ca/SCS/Papers/corrgram.pdf},
}
and implemented (in SAS)
http://www.math.yorku.ca/SCS/sasmac/corrgram.html

Rather than just the first principal component, it is usually better
to order the variables by the angles between the first 2 PC, 
corresponding to their order around a 2D biplot, using
sort(atan(V2/V1))

-Michael



vincent at 7d4.com wrote:

> Spencer Graves a ?crit :
> 
> 
>>      I agree it would be great to sort the variables in a correlation 
>>matrix to make it easier to read and see patterns.  I don't know any 
>>functions for doing that.  If it were my problem, I might "order" the 
>>variables by their first principal component.  There may also be some 
>>cluster analysis way to do that, but I don't know it well enough to say.
>>      Hope this helps.
>>      Spencer Graves
> 
> 
> Thanks for your answer Spencer.
> 
> Here is a first result of a very simple and naive approach.
> http://7d4.com/r/
> 
> Of course, there is no assumption the sorting is "optimal",
> but on this little example it helps the matrix being
> more readable.
> 
> Vincent
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Michael Friendly     Email: friendly AT yorku DOT ca
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA


From ggrothendieck at gmail.com  Wed Jun  7 15:03:54 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 7 Jun 2006 09:03:54 -0400
Subject: [R] Help with selecting data from irregular time series {its}
	objects
In-Reply-To: <6dbf89a50606070532s312d2e27hccdde811f0542f1d@mail.gmail.com>
References: <6dbf89a50606070532s312d2e27hccdde811f0542f1d@mail.gmail.com>
Message-ID: <971536df0606070603q48e1df76jfc7eea1779f60b19@mail.gmail.com>

See: ?itsSubset

On 6/7/06, Ivan Kalafatic <ivan.kalafatic at gmail.com> wrote:
> If I understood correctly in irregular time series (its) objects, values are
> indexed by time stamps in POSIX format.
> But if I try to select the value of my time series corresponding to specific
> time stamp in the following way:
> x - its object
>
> i <- as.POSIXct("2006-05-19 15:30:00")
> x[i,] or x[i] or x[i,1] I get the error message: subscript out of bounds.
>
> If I use integers: x[1,1] it is ok I get the first element of time series.
>
> Is there a way to select elements by their corresponding dates?
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ripley at stats.ox.ac.uk  Wed Jun  7 15:34:28 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 7 Jun 2006 14:34:28 +0100 (BST)
Subject: [R] Edit function
In-Reply-To: <20060607123554.15027.qmail@web8403.mail.in.yahoo.com>
References: <20060607123554.15027.qmail@web8403.mail.in.yahoo.com>
Message-ID: <Pine.LNX.4.64.0606071433550.4363@gannet.stats.ox.ac.uk>

On Wed, 7 Jun 2006, stat stat wrote:

> Dear all R users,
>
>  I have a query on "Edit" function. Suppose I have a data frame named 
> "data". I can use EDIT function to see the materials contained in data, 
> by using the command:
>
>  > edit(data)
>
>  But when I close the window then again the materials contained in data 
> is displayed in the command window. But I do not want to see these 
> materials again. Can anyone give me any idea on how to do this?

?invisible

>
>  Thanks and regards,
>  stat

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From toby_marks at americancentury.com  Wed Jun  7 15:34:51 2006
From: toby_marks at americancentury.com (toby_marks at americancentury.com)
Date: Wed, 7 Jun 2006 08:34:51 -0500
Subject: [R] Help needed using lattice for area plots lpolygon, xyplot.
Message-ID: <OF1AAA235C.2E6EA9C8-ON86257185.0079E2C6-86257186.004A9A8B@americancentury.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060607/3e0904f5/attachment.pl 

From nicolas.degallier at ird.fr  Wed Jun  7 15:37:55 2006
From: nicolas.degallier at ird.fr (Nicolas Degallier)
Date: Wed, 7 Jun 2006 15:37:55 +0200
Subject: [R] how to read hdf files under R?
Message-ID: <B632E8C7-5CE9-43CD-9D38-F4B88E683C23@ird.fr>

Hi!

I am trying to install in my R environment the rhdf5 package and  
library but it seems to have vanished from either the CRAN or  
BioConductors sites.

Can you tell me where it would be possible to find it or any R  
library (or function) able to read hdf files?

Sincerely,

Nicolas Degallier

UMR 7159 / IRD UR182
Laboratoire d'Oc?anographie et du Climat, Exp?rimentation et  
Approches Num?riques (LOCEAN)
Tour 45-55, 4e ?t., case 100, 4 place Jussieu
75252  Paris Cedex 5  France

t?l: (33) 01 44 27 51 57
fax: (33) 01 44 27 38 05
E-mail: <Nicolas.Degallier at ird.fr>

Publications (anonymous ftp):

ftp://ftp.lodyc.jussieu.fr/LOCEAN/ndelod


From zelickr at pdx.edu  Wed Jun  7 15:48:40 2006
From: zelickr at pdx.edu (Randy Zelick)
Date: Wed, 7 Jun 2006 06:48:40 -0700 (Pacific Daylight Time)
Subject: [R] x y averaging thanks
In-Reply-To: <Pine.LNX.4.64.0606070754540.11529@gannet.stats.ox.ac.uk>
References: <20060606204428.85696.qmail@web51607.mail.yahoo.com>
	<efb536d50606061401k58251e79g28d1837cc4d0f308@mail.gmail.com>
	<Pine.WNT.4.64.0606062012170.3680@BIO-SB2-329-RZN.PSU.DS.PDX.EDU>
	<Pine.LNX.4.64.0606070754540.11529@gannet.stats.ox.ac.uk>
Message-ID: <Pine.WNT.4.64.0606070646550.2484@BIO-SB2-329-RZN.PSU.DS.PDX.EDU>


I have three completely different(!) but workable solutions. Thanks much 
for your help.

=Randy=

R. Zelick				email: zelickr at pdx.edu
Department of Biology			voice: 503-725-3086
Portland State University		fax:   503-725-3888

mailing:
P.O. Box 751
Portland, OR 97207

shipping:
1719 SW 10th Ave, Room 246
Portland, OR 97201


From ripley at stats.ox.ac.uk  Wed Jun  7 15:56:09 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 7 Jun 2006 14:56:09 +0100 (BST)
Subject: [R] how to read hdf files under R?
In-Reply-To: <B632E8C7-5CE9-43CD-9D38-F4B88E683C23@ird.fr>
References: <B632E8C7-5CE9-43CD-9D38-F4B88E683C23@ird.fr>
Message-ID: <Pine.LNX.4.64.0606071455180.17503@gannet.stats.ox.ac.uk>

On Wed, 7 Jun 2006, Nicolas Degallier wrote:

> I am trying to install in my R environment the rhdf5 package and
> library but it seems to have vanished from either the CRAN or
> BioConductors sites.
>
> Can you tell me where it would be possible to find it or any R
> library (or function) able to read hdf files?

How about the hdf5 package on CRAN?  Works for me.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From toby_marks at americancentury.com  Wed Jun  7 16:11:26 2006
From: toby_marks at americancentury.com (toby_marks at americancentury.com)
Date: Wed, 7 Jun 2006 09:11:26 -0500
Subject: [R] Fw: Help needed using lattice for area plots lpolygon, xyplot.
Message-ID: <OF6F850416.61367157-ON86257186.004DEC56-86257186.004DF3FA@americancentury.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060607/866351e2/attachment.pl 

From uhkeller at web.de  Wed Jun  7 16:14:10 2006
From: uhkeller at web.de (Ulrich Keller)
Date: Wed, 07 Jun 2006 16:14:10 +0200
Subject: [R] Edit function
In-Reply-To: <20060607123554.15027.qmail@web8403.mail.in.yahoo.com>
References: <20060607123554.15027.qmail@web8403.mail.in.yahoo.com>
Message-ID: <4486DF32.3030709@web.de>

fix(data)

will invoke edit(data) and store changes you make in data without 
displaying anything.

stat stat schrieb:
> Dear all R users,
>    
>   I have a query on "Edit" function. Suppose I have a data frame named "data". I can use EDIT function to see the materials contained in data, by using the command: 
>    
>   > edit(data)
>    
>   But when I close the window then again the materials contained in data is displayed in the command window. But I do not want to see these materials again. Can anyone give me any idea on how to do this?
>    
>   Thanks and regards,
>   stat
>
>  Send instant messages to your online friends http://in.messenger.yahoo.com 
>
>  Stay connected with your friends even when away from PC.  Link: http://in.mobile.yahoo.com/new/messenger/  
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>


From sfalcon at fhcrc.org  Wed Jun  7 16:29:47 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Wed, 07 Jun 2006 07:29:47 -0700
Subject: [R] Building packages in R - 'private' functions
In-Reply-To: <b0808fdc0606070110u5da2aed0h995a879e31973f64@mail.gmail.com>
	(Fabio Di Narzo Antonio's message of "Wed,
	7 Jun 2006 10:10:18 +0200")
References: <5.2.1.1.2.20060607004234.032b7cd0@postoffice9.mail.cornell.edu>
	<b0808fdc0606070110u5da2aed0h995a879e31973f64@mail.gmail.com>
Message-ID: <m28xo9ujhg.fsf@ziti.local>

"Antonio, Fabio Di Narzo" <antonio.fabio at gmail.com> writes:

> 1. If you have time to change internal functions naming, you can rename
> internal functions by putting a leading '.'.
> Even without namespace, I have noticed there is no check for corresponding
> docs for such functions.
>
> 2. If you don't want to rename all internal functions, the best way is
> writing an 'internals.Rd' file with an alias for each internal function
> (documented in 'writing R extensions').
>
> 3.Finally, you can add a NAMESPACE (see writing R extensions). However, if
> you use S3/S4 classes, this can be much more tedious to do.
>
> I think the no. 2 to be the fastest/safer way.

I think adding a NAMESPACE file is the best solution and I don't think
that the process needs to be particularly tedious.

Having a naming convention for private functions is fine and you can
still do that with a NAMESPACE.  Non-exported functions do not get
checked for documentation, so there is no need for an internals.Rd (of
course, it doesn't hurt to give yourself some documentation for when
you return to the project 3 months later :-)

Besides hiding your private functions, a NAMESPACE protects you from
users or other packages redefining functions that you rely on.  As an
extreme example, if a user redefined length(), many packages without
namespaces would break.

+ seth


From ronggui.huang at gmail.com  Wed Jun  7 16:39:04 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Wed, 7 Jun 2006 22:39:04 +0800
Subject: [R] Edit function
In-Reply-To: <4486DF32.3030709@web.de>
References: <20060607123554.15027.qmail@web8403.mail.in.yahoo.com>
	<4486DF32.3030709@web.de>
Message-ID: <38b9f0350606070739x24dbd923pbd89ee6d05e4d248@mail.gmail.com>

If you want to change the data,_fix_ will be a good option.But if you
just want to browse the data ,then invisible(edit(data)) is better
one.

I remember this question showed up some time ago.


2006/6/7, Ulrich Keller <uhkeller at web.de>:
> fix(data)
>
> will invoke edit(data) and store changes you make in data without
> displaying anything.
>
> stat stat schrieb:
> > Dear all R users,
> >
> >   I have a query on "Edit" function. Suppose I have a data frame named "data". I can use EDIT function to see the materials contained in data, by using the command:
> >
> >   > edit(data)
> >
> >   But when I close the window then again the materials contained in data is displayed in the command window. But I do not want to see these materials again. Can anyone give me any idea on how to do this?
> >
> >   Thanks and regards,
> >   stat
> >
> >  Send instant messages to your online friends http://in.messenger.yahoo.com
> >
> >  Stay connected with your friends even when away from PC.  Link: http://in.mobile.yahoo.com/new/messenger/
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
ronggui huang
Deparment of Sociology
Fudan University


From maustin at hsph.harvard.edu  Wed Jun  7 15:57:10 2006
From: maustin at hsph.harvard.edu (Matthew Austin)
Date: Wed,  7 Jun 2006 09:57:10 -0400
Subject: [R] Help with sample function
Message-ID: <20060607095710.7km62ib05c0w4cog@webmail.sph.harvard.edu>

I have generated some some survival times and censoring indicators. 
Thus I have an ordered pair for each observation. How do I sample these 
ordered paris? I only know how to sample from a vector? I would 
appreciate any help I could get.

Thanks
Matt


From dimitris.rizopoulos at med.kuleuven.be  Wed Jun  7 17:12:00 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 7 Jun 2006 17:12:00 +0200
Subject: [R] Help with sample function
References: <20060607095710.7km62ib05c0w4cog@webmail.sph.harvard.edu>
Message-ID: <008b01c68a44$ba084070$0540210a@www.domain>

try something like:

surv.data <- data.frame(times = rexp(100, 1/10), events = rbinom(100, 
1, 0.7))
surv.data[sample(nrow(surv.data), replace = TRUE), ]


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Matthew Austin" <maustin at hsph.harvard.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, June 07, 2006 3:57 PM
Subject: [R] Help with sample function


>I have generated some some survival times and censoring indicators.
> Thus I have an ordered pair for each observation. How do I sample 
> these
> ordered paris? I only know how to sample from a vector? I would
> appreciate any help I could get.
>
> Thanks
> Matt
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From maustin at amgen.com  Wed Jun  7 17:11:55 2006
From: maustin at amgen.com (Austin, Matt)
Date: Wed, 7 Jun 2006 08:11:55 -0700 
Subject: [R] Help with sample function
Message-ID: <E7D5AB4811D20B489622AABA9C53859109DADD9F@teal-exch.amgen.com>

I couldn't help but respond to this one, it's not often I see my own name.

Using data from the survival library:

library(survival)
lung[1:10, c('time', 'status')]
Surv(lung$time, lung$status)[1:10]

--Matt

Matt Austin
Statistician
Amgen, Inc
800 9AMGEN9 x77431
805-447-7431




-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Matthew Austin
Sent: Wednesday, June 07, 2006 6:57 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Help with sample function


I have generated some some survival times and censoring indicators. 
Thus I have an ordered pair for each observation. How do I sample these 
ordered paris? I only know how to sample from a vector? I would 
appreciate any help I could get.

Thanks
Matt

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From Manuel.A.Morales at williams.edu  Wed Jun  7 17:12:26 2006
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Wed, 07 Jun 2006 11:12:26 -0400
Subject: [R] error bars in lattice xyplot *with groups*
In-Reply-To: <20060606002008.2dfnlqlmpw2ss4wc@my2.dal.ca>
References: <20060606002008.2dfnlqlmpw2ss4wc@my2.dal.ca>
Message-ID: <1149693147.10326.8.camel@solidago.localdomain>

Hi Mike,

If you're not committed to using a panel function, another option is to
use the function lineplot.CI, available in the package sciplot at
http://mutualism.williams.edu/sciplot

E.g.
# Define and generate variables in "long" format
range <- vector()
voice <- vector()

string <- strsplit(as.character(singer$voice.part)," ")
for(i in 1:dim(singer)[1]) {
  range[i] <- string[[i]][1] 
  voice[i] <- string[[i]][2]
}

# Define function for CI
conf.int <- function(x) {
              st <- boxplot.stats(x)
              c((st$conf[2]-st$conf[1])/2)
              }

# Plot
library(sciplot)
lineplot.CI(response=height, x.factor=voice, trace.factor=range,
            data=singer, fun=median, ci.fun=conf.int)

lineplot.CI(response=height, x.factor=voice.part, data=singer,
            fun=median, ci.fun=conf.int)


Manuel


On Tue, 2006-06-06 at 00:20 -0300, Mike Lawrence wrote:
> Hi all,
> 
> I'm trying to plot error bars in a lattice plot generated with xyplot. Deepayan
> Sarkar has provided a very useful solution for simple circumstances
> (https://stat.ethz.ch/pipermail/r-help/2005-October/081571.html), yet I am
> having trouble getting it to work when the "groups" setting is enabled in
> xyplot (i.e. multiple lines). To illustrate this, consider the singer data
> generated by the above linked solution previously submitted:
> 
> #####################
> library(lattice)
> singer.split <-
>     with(singer,
>          split(height, voice.part))
> 
> singer.ucl <-
>     sapply(singer.split,
>            function(x) {
>                st <- boxplot.stats(x)
>                c(st$stats[3], st$conf)
>            })
> 
> singer.ucl <- as.data.frame(t(singer.ucl))
> names(singer.ucl) <- c("median", "lower", "upper")
> singer.ucl$voice.part <-
>     factor(rownames(singer.ucl),
>            levels = rownames(singer.ucl))
> 
> #now let's split up the voice.part factor into two factors,
> singer.ucl$voice=factor(rep(c(1,2),4))
> singer.ucl$range=factor(rep(c("Bass","Tenor","Alto","Soprano"),each=2))
> 
> #here's Deepayan's previous solution, slightly modified to depict
> #  the dependent variable (median) and the error bars on the y-axis
> #  and the independent variable (voice.part) on the x-axis
> prepanel.ci <- function(x, y, ly, uy, subscripts, ...)
> {
>     x <- as.numeric(x)
>     ly <- as.numeric(ly[subscripts])
>     uy <- as.numeric(uy[subscripts])
>     list(ylim = range(y, uy, ly, finite = TRUE))
> }
> panel.ci <- function(x, y, ly, uy, subscripts, pch = 16, ...)
> {
>     x <- as.numeric(x)
>     y <- as.numeric(y)
>     ly <- as.numeric(ly[subscripts])
>     uy <- as.numeric(uy[subscripts])
>     panel.arrows(x, ly, x, uy, col = "black",
>                  length = 0.25, unit = "native",
>                  angle = 90, code = 3)
>     panel.xyplot(x, y, pch = pch, ...)
> }
> 
> 
> #this graph works
> xyplot(median ~ voice.part,
> 	data=singer.ucl,
> 	ly = singer.ucl$lower,
> 	uy = singer.ucl$upper,
> 	prepanel = prepanel.ci,
> 	panel = panel.ci,
> 	type="b"
> )
> 
> #this one does not (it will plot, but will not seperate the groups)
> xyplot(median ~ voice,
> 	groups=range,
> 	data=singer.ucl,
> 	ly = singer.ucl$lower,
> 	uy = singer.ucl$upper,
> 	prepanel = prepanel.ci,
> 	panel = panel.ci,
> 	type="b"
> )
> 
> ####################################
> 
> Any suggestions?
>


From Mark.L.Sessing at noaa.gov  Wed Jun  7 17:30:59 2006
From: Mark.L.Sessing at noaa.gov (Mark L Sessing)
Date: Wed, 07 Jun 2006 10:30:59 -0500
Subject: [R] multiple data sets on one plot
Message-ID: <4486F133.7030203@noaa.gov>

Hello,

I am learning how to use R, and I cannot figure out how to plot more 
than one data set on a single plot.  Can you help me out?

Cheers,
Mark

-- 
Mark Sessing
CIMMS Research Fellow Meteorologist
NWS Warning Decision Training Branch
3200 Marshall Ave Ste. 202
Norman, OK 73072
Phone: 405-573-3332
Fax: 405-573-3462
Mark.L.Sessing at noaa.gov


From spencer.graves at pdf.com  Wed Jun  7 17:34:59 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 07 Jun 2006 08:34:59 -0700
Subject: [R] use of R in big companies (references) & R-support esp in
 Germany
In-Reply-To: <4484B634.4000709@approximity.com>
References: <4484B634.4000709@approximity.com>
Message-ID: <4486F223.2000206@pdf.com>

	  The best rebuttal I've heard recently to arguments like that is Linux 
(www.linux.org):  It's distributed under the same general public license 
(GNU) license as R.

	  A perspective that I don't recall having seen on this list is that 
the cost of producing and distributing software has become too cheap to 
meter, unless you want to charge for it.  Open source projects like 
Linux and R (and Mozilla, Subversion, and others) were much more 
difficult and less common before the Internet, just because the costs of 
coordinating development plus producing and distributing the product 
made such efforts much more difficult.  For a discussion of these 
phenomena by two Economics professors at UC-Berkeley, see Shapiro and 
Varian (1998) Information Rules (Harvard Business School Pr.).  A newer, 
similar title by these same authors is "The Economics of Information 
Technology";  I haven't read this newer book, but it looks like it could 
be relevant also.

	  I mention this, because I suspect some of the opposition to open 
source, "free" software is ideology:  Rabid capitalists refuse to 
believe that anything free can be any good.  (We could talk about air 
and water, but that might be a digression.)  Books like this backed by 
solid research might help counter such opposition.

	  Hope this helps.
	  Spencer Graves

Armin Roehrl wrote:
> Dear R users,
> 
>     sorry for this general email and I am sure it has been asked
> way too many times.
> 
> IT departements in big companies only want to support the big
> standards. Whatever big standards means apart from being expensive.
> 
> We are in the process of trying to get a risk management project
> for a big conservative company in Germany. As part of the project
> we would use R to run simulations, but the company is afraid of R.
> 
> 1) If anybody has any reference projects using R I can quote, please
> drop me an email. Best would be companies like Siemens, Allianz,
> Munich Re, Daimler Chrysler, Credit Suisse etc.
> 
> 2) Are there any software companies around with R know-how and are
> interested in paid R-projects? The bigger the company, the better
> as this client seems to be scared of software companies with less
> than 200 developers.
> 
> 
> Thanks,
>   -Armin
>


From sarah.goslee at gmail.com  Wed Jun  7 17:48:01 2006
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Wed, 7 Jun 2006 11:48:01 -0400
Subject: [R] multiple data sets on one plot
In-Reply-To: <4486F133.7030203@noaa.gov>
References: <4486F133.7030203@noaa.gov>
Message-ID: <efb536d50606070848hafbcbe8q9e1c2772d3f1ac62@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060607/aa057b1f/attachment.pl 

From mschwartz at mn.rr.com  Wed Jun  7 17:49:52 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 07 Jun 2006 10:49:52 -0500
Subject: [R] multiple data sets on one plot
In-Reply-To: <4486F133.7030203@noaa.gov>
References: <4486F133.7030203@noaa.gov>
Message-ID: <1149695393.4698.25.camel@localhost.localdomain>

On Wed, 2006-06-07 at 10:30 -0500, Mark L Sessing wrote:
> Hello,
> 
> I am learning how to use R, and I cannot figure out how to plot more 
> than one data set on a single plot.  Can you help me out?
> 
> Cheers,
> Mark

It depends upon the type of plot (scatter, lines, bar, etc.) and whether
or not you are using R's base graphics or lattice graphics.

With more information, we can offer specific guidance.

For example, with base graphics, you can add additional plot components
with:

?lines
?points
?segments
?matpoints (Note also matplot() on the same page)
?curve
?arrows

A good starting place would be Chapter 12 "Graphical Procedures" in An
Introduction to R, which is available within your R installation (on
Windows from the GUI menus) or from the Documentation links on the R
home page.

An additional resource is the R Graph Gallery:

  http://addictedtor.free.fr/graphiques/index.php

and Chapter 3 "From Data to Graphics" by Vincent Zoonekynd here:

  http://zoonek2.free.fr/UNIX/48_R/all.html

HTH,

Marc Schwartz


From BPikouni at CNTUS.JNJ.COM  Wed Jun  7 17:53:32 2006
From: BPikouni at CNTUS.JNJ.COM (Pikounis, Bill [CNTUS])
Date: Wed, 7 Jun 2006 11:53:32 -0400 
Subject: [R] Edit function
Message-ID: <A89517C7FD248040BB71CA3C04C1ACBB04C9EF80@CNTUSMAEXS4.na.jnj.com>

I use a simple-minded, dirty wrapper on edit:

view <- function(x) {
  warnopt <- options()$warn
  options(warn=-1)
  on.exit({sink(); options(warn=warnopt)})
  edit(x)
  invisible()
}

I say "dirty", because sometimes the output is re-directed from stdout
somewhere else as an unintended side effect, which is why I have the first
sink() statement in there. (Note the help file on sink() advises in numerous
places on using sink()-related calls with care.) I have not been ambitious
enough to make the above version robust and well-understood, as it works
well for me nearly every time, and when it does not, the side effect and
cleanup is neither damaging nor annoying enough.

Also, I have only really used this under Windows R GUI.

Hope that helps,
Bill

-------------------------------
Bill Pikounis, PhD
Nonclinical Statistics
Centocor, Inc.


>   > edit(data)
>    
>   But when I close the window then again the materials 
> contained in data is displayed in the command window. But I 
> do not want to see these materials again. Can anyone give me 
> any idea on how to do this?


Hope that helps,
Bill


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of stat stat
> Sent: Wednesday, June 07, 2006 8:36 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Edit function
> 
> 
> Dear all R users,
>    
>   I have a query on "Edit" function. Suppose I have a data 
> frame named "data". I can use EDIT function to see the 
> materials contained in data, by using the command: 
>    
>   > edit(data)
>    
>   But when I close the window then again the materials 
> contained in data is displayed in the command window. But I 
> do not want to see these materials again. Can anyone give me 
> any idea on how to do this?
>    
>   Thanks and regards,
>   stat
> 
>  Send instant messages to your online friends 
http://in.messenger.yahoo.com 

 Stay connected with your friends even when away from PC.  Link:
http://in.mobile.yahoo.com/new/messenger/  
	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From rusers.sh at gmail.com  Wed Jun  7 18:10:02 2006
From: rusers.sh at gmail.com (zhijie zhang)
Date: Thu, 8 Jun 2006 00:10:02 +0800
Subject: [R] how to do multiple comparison in the nonparametric statistical
	analysis?
Message-ID: <a835c81e0606070910k4b9b5ebaue4e3e7adc28cc2ea@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060608/a45875bb/attachment.pl 

From deepayan.sarkar at gmail.com  Wed Jun  7 18:19:20 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 7 Jun 2006 11:19:20 -0500
Subject: [R] Fw: Help needed using lattice for area plots lpolygon,
	xyplot.
In-Reply-To: <OF6F850416.61367157-ON86257186.004DEC56-86257186.004DF3FA@americancentury.com>
References: <OF6F850416.61367157-ON86257186.004DEC56-86257186.004DF3FA@americancentury.com>
Message-ID: <eb555e660606070919m399f6408h6a2be636dec1ceeb@mail.gmail.com>

On 6/7/06, toby_marks at americancentury.com
<toby_marks at americancentury.com> wrote:
> I am trying to learn how to use the graphics from the lattice package (
> and am very new to R).
>
> I am trying to replicate the example plot referenced below, by using the
> lattice xyplot & lpolygon to create panels.  I get what appears to be the
> correct shape of the filled region, but cannot get the position to overlay
> properly.  I have attempted with various settings of position.  ( i.e.
> position = c(0,0,1,1)  etc settings as well.   I am not understanding
> something about the positioning panels.  I am missing some subtle
> difference between polygon & lpolygon, or am missing something about panel
> overlays &/or panel postions.
>
> #http://addictedtor.free.fr/graphiques/graphcode.php?graph=7.
> par(bg="white")
> n <- 100
> set.seed(43214) #just so we have the same exact graph
> x <- c(0,cumsum(rnorm(n)))
> y <- c(0,cumsum(rnorm(n)))
> xx <- c(0:n, n:0)
> yy <- c(x, rev(y))
> plot(xx, yy, type="n", xlab="Time", ylab="Distance")
> polygon(xx, yy, col="gray")
> title("Distance Between Brownian Motions")
>
>
>
> # using lattice.
> p1 <- xyplot( yy~xx,type='l');
> p2 <- lpolygon(xx,yy,col='blue');
> print(p1,position=c(0,0,1,1), more=TRUE);
> print(p2,position=c(0,0,1,1));

You are missing a fundamental concept in lattice, namely that of panel
functions. A literal translation of that example would be

xyplot(yy ~ xx, panel = lpolygon, col = "gray")

which is more or less equivalent to

xyplot(yy ~ xx,
       panel = function(x, y, ...) {
           # panel.xyplot(x, y, ...) # unnecessary
           lpolygon(x, y, col = "gray")
       })

The line commented out is the equivalent of plot(...type='n'), but is
unnecessary here.

Deepayan


From toby_marks at americancentury.com  Wed Jun  7 18:27:02 2006
From: toby_marks at americancentury.com (toby_marks at americancentury.com)
Date: Wed, 7 Jun 2006 11:27:02 -0500
Subject: [R] Fw: Help needed using lattice for area plots lpolygon,
	xyplot.
In-Reply-To: <eb555e660606070919m399f6408h6a2be636dec1ceeb@mail.gmail.com>
Message-ID: <OF7FD5CC6E.A1FF017C-ON86257186.005A1DE1-86257186.005A5DFF@americancentury.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060607/550e7cec/attachment.pl 

From mschwartz at mn.rr.com  Wed Jun  7 18:33:25 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 07 Jun 2006 11:33:25 -0500
Subject: [R] how to do multiple comparison in the
	nonparametric	statistical analysis?
In-Reply-To: <a835c81e0606070910k4b9b5ebaue4e3e7adc28cc2ea@mail.gmail.com>
References: <a835c81e0606070910k4b9b5ebaue4e3e7adc28cc2ea@mail.gmail.com>
Message-ID: <1149698005.4698.34.camel@localhost.localdomain>

On Thu, 2006-06-08 at 00:10 +0800, zhijie zhang wrote:
> Dear Rusers,
>  As we all know , there are many methods to do multiple comparison in the
> parametric statistical analysis, But i can't find some in nonparametric
> statistical analysis.
>  Could anybody give some suggestions?

Have you looked at the "npmc" package on CRAN?

As a text reference, there is also:

Multiple Comparisons: Theory and methods
by Jason C. Hsu
Chapman & Hall 1996

More information here:

  http://www.stat.ohio-state.edu/~jch/mc.html

Amazon.com link:

  http://www.amazon.com/gp/product/0412982811

HTH,

Marc Schwartz


From cbarker1 at scius.jnj.com  Wed Jun  7 18:38:11 2006
From: cbarker1 at scius.jnj.com (Barker, Chris [SCIUS])
Date: Wed, 7 Jun 2006 12:38:11 -0400 
Subject: [R] how to do multiple comparison in the nonparametric
	statis	tical analysis?
Message-ID: <56484410CF26D747A645BE746C57CE6D021A8C54@SCIUSFREXS3.na.jnj.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060607/fc6b351b/attachment.pl 

From pramirez379 at hotmail.com  Wed Jun  7 19:00:08 2006
From: pramirez379 at hotmail.com (Pedro Ramirez)
Date: Wed, 07 Jun 2006 19:00:08 +0200
Subject: [R] Density Estimation
Message-ID: <BAY119-F5D0F372365D7823ED6C47F88A0@phx.gbl>

Dear R-list,

I have made a simple kernel density estimation by

x <- c(2,1,3,2,3,0,4,5,10,11,12,11,10)
kde <- density(x,n=100)

Now I would like to know the estimated probability that a
new observation falls into the interval 0<x<3.

How can I integrate over the corresponding interval?
In several R-packages for kernel density estimation I did
not found a corresponding function. I could apply
Simpson's Rule for integrating, but perhaps somebody
knows a better solution.

Thanks a lot for help!

Pedro

_________


From Greg.Snow at intermountainmail.org  Wed Jun  7 19:21:51 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Wed, 7 Jun 2006 11:21:51 -0600
Subject: [R] Density Estimation
Message-ID: <07E228A5BE53C24CAD490193A7381BBB3E4BFF@LP-EXCHVS07.CO.IHC.COM>

Not a direct answer to your question, but if you use a logspline density
estimate rather than a kernal density estimate then the logspline
package will help you and it has built in functions for dlogspline,
qlogspline, and plogspline that do the integrals for you.

If you want to stick with the KDE, then you could find the area under
each of the kernals for the range you are interested in (need to work
out the standard deviation used from the bandwidth, then use pnorm for
the default gaussian kernal), then just sum the individual areas. 

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pedro Ramirez
Sent: Wednesday, June 07, 2006 11:00 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Density Estimation

Dear R-list,

I have made a simple kernel density estimation by

x <- c(2,1,3,2,3,0,4,5,10,11,12,11,10)
kde <- density(x,n=100)

Now I would like to know the estimated probability that a new
observation falls into the interval 0<x<3.

How can I integrate over the corresponding interval?
In several R-packages for kernel density estimation I did not found a
corresponding function. I could apply Simpson's Rule for integrating,
but perhaps somebody knows a better solution.

Thanks a lot for help!

Pedro

_________

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From Thomas.Adams at noaa.gov  Wed Jun  7 19:47:11 2006
From: Thomas.Adams at noaa.gov (Thomas Adams)
Date: Wed, 07 Jun 2006 13:47:11 -0400
Subject: [R] use of R in big companies (references) & R-support esp in
 Germany
In-Reply-To: <4486F223.2000206@pdf.com>
References: <4484B634.4000709@approximity.com> <4486F223.2000206@pdf.com>
Message-ID: <4487111F.6050802@noaa.gov>

Spencer,

I agree that there exist biases within large organizations toward 
commercial software packages, but I humbly disagree that this can be 
reduced to geopolitical differences. I work for a US Government Agency 
and while there is some truth to the feeling "[they] refuse to believe 
that anything free can be any good", it has little-to-nothing to do with 
Capitalism vs whatever? I think the feeling has to do more with the 
*perceived* issues related to support and what is "tried & true" ? it's 
a very conservative approach, one that I face on nearly a daily basis. 
Apart from my use of R, I am also a heavy user of the open source 
Geographic Information System (GIS) called GRASS, in which the 
*dominant* GIS company internationally is ESRI and their very expensive 
software called ArcGIS. There is very little that ArcGIS can do that 
GRASS can not do. What is especially troublesome is that ArcGIS is 
restricted to the MS-Windows platform, whereas GRASS runs on Linux, 
MacOS X, UNIX, and MS-Windows with Cygwin.

Thank you for your references as I hope to use them.

Regards,
Tom



Spencer Graves wrote:
> 	  The best rebuttal I've heard recently to arguments like that is Linux 
> (www.linux.org):  It's distributed under the same general public license 
> (GNU) license as R.
>
> 	  A perspective that I don't recall having seen on this list is that 
> the cost of producing and distributing software has become too cheap to 
> meter, unless you want to charge for it.  Open source projects like 
> Linux and R (and Mozilla, Subversion, and others) were much more 
> difficult and less common before the Internet, just because the costs of 
> coordinating development plus producing and distributing the product 
> made such efforts much more difficult.  For a discussion of these 
> phenomena by two Economics professors at UC-Berkeley, see Shapiro and 
> Varian (1998) Information Rules (Harvard Business School Pr.).  A newer, 
> similar title by these same authors is "The Economics of Information 
> Technology";  I haven't read this newer book, but it looks like it could 
> be relevant also.
>
> 	  I mention this, because I suspect some of the opposition to open 
> source, "free" software is ideology:  Rabid capitalists refuse to 
> believe that anything free can be any good.  (We could talk about air 
> and water, but that might be a digression.)  Books like this backed by 
> solid research might help counter such opposition.
>
> 	  Hope this helps.
> 	  Spencer Graves
>
> Armin Roehrl wrote:
>   
>> Dear R users,
>>
>>     sorry for this general email and I am sure it has been asked
>> way too many times.
>>
>> IT departements in big companies only want to support the big
>> standards. Whatever big standards means apart from being expensive.
>>
>> We are in the process of trying to get a risk management project
>> for a big conservative company in Germany. As part of the project
>> we would use R to run simulations, but the company is afraid of R.
>>
>> 1) If anybody has any reference projects using R I can quote, please
>> drop me an email. Best would be companies like Siemens, Allianz,
>> Munich Re, Daimler Chrysler, Credit Suisse etc.
>>
>> 2) Are there any software companies around with R know-how and are
>> interested in paid R-projects? The bigger the company, the better
>> as this client seems to be scared of software companies with less
>> than 200 developers.
>>
>>
>> Thanks,
>>   -Armin
>>
>>     
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>   


-- 
Thomas E Adams
National Weather Service
Ohio River Forecast Center
1901 South State Route 134
Wilmington, OH 45177

EMAIL:	thomas.adams at noaa.gov

VOICE:	937-383-0528
FAX:	937-383-0033


From rolf at erdos.math.unb.ca  Wed Jun  7 19:48:06 2006
From: rolf at erdos.math.unb.ca (Rolf Turner)
Date: Wed, 7 Jun 2006 14:48:06 -0300 (ADT)
Subject: [R] Density Estimation
Message-ID: <200606071748.k57Hm64h017279@erdos.math.unb.ca>


Pedro wrote:

> I have made a simple kernel density estimation by
> 
> x <- c(2,1,3,2,3,0,4,5,10,11,12,11,10)
> kde <- density(x,n=100)
> 
> Now I would like to know the estimated probability that a
> new observation falls into the interval 0<x<3.
> 
> How can I integrate over the corresponding interval?
> In several R-packages for kernel density estimation I did
> not found a corresponding function. I could apply
> Simpson's Rule for integrating, but perhaps somebody
> knows a better solution.

	One possibility is to use splinefun():

	> spiffy <- splinefun(kde$x,kde$y)
	> integrate(spiffy,0,3)
	0.2353400 with absolute error < 2e-09

		cheers,

			Rolf Turner
			rolf at math.unb.ca


From pramirez379 at hotmail.com  Wed Jun  7 19:54:32 2006
From: pramirez379 at hotmail.com (Pedro Ramirez)
Date: Wed, 07 Jun 2006 19:54:32 +0200
Subject: [R] Density Estimation
Message-ID: <BAY119-F13F2C8B631B76E6AA46B66F88A0@phx.gbl>

>Not a direct answer to your question, but if you use a logspline density
>estimate rather than a kernal density estimate then the logspline
>package will help you and it has built in functions for dlogspline,
>qlogspline, and plogspline that do the integrals for you.
>
>If you want to stick with the KDE, then you could find the area under
>each of the kernals for the range you are interested in (need to work
>out the standard deviation used from the bandwidth, then use pnorm for
>the default gaussian kernal), then just sum the individual areas.
>
>Hope this helps,

Thanks a lot for your quick help! I think I will follow your first 
suggestion (logspline
density estimation) instead of summing over the kernel areas because at the
boundaries of the range truncated kernel areas can occur, so I think it is
easier to do it with logsplines. Thanks again for your help!!

Pedro



>
>--
>Gregory (Greg) L. Snow Ph.D.
>Statistical Data Center
>Intermountain Healthcare
>greg.snow at intermountainmail.org
>(801) 408-8111
>
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pedro Ramirez
>Sent: Wednesday, June 07, 2006 11:00 AM
>To: r-help at stat.math.ethz.ch
>Subject: [R] Density Estimation
>
>Dear R-list,
>
>I have made a simple kernel density estimation by
>
>x <- c(2,1,3,2,3,0,4,5,10,11,12,11,10)
>kde <- density(x,n=100)
>
>Now I would like to know the estimated probability that a new
>observation falls into the interval 0<x<3.
>
>How can I integrate over the corresponding interval?
>In several R-packages for kernel density estimation I did not found a
>corresponding function. I could apply Simpson's Rule for integrating,
>but perhaps somebody knows a better solution.
>
>Thanks a lot for help!
>
>Pedro
>
>_________
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>


From srini_iyyer_bio at yahoo.com  Wed Jun  7 20:04:18 2006
From: srini_iyyer_bio at yahoo.com (Srinivas Iyyer)
Date: Wed, 7 Jun 2006 11:04:18 -0700 (PDT)
Subject: [R] help with combination problem
Message-ID: <20060607180418.27833.qmail@web38104.mail.mud.yahoo.com>

hello:

I have 3 data.frame objects.

First df object:
Of dim (149,31). Columns 2:31 are marked as T1..T14 
and N1..N16.

Name     T1    T2    N1   T3   N2  N3  N4  T4
mu1      10    10    9    10   9   9   8   10
mu2      11    11    9    11   9   9   9   11
...
muN      12    12    9    11   9   9   8   12




Second df object:
of Dim (50000,31). Columns 2:31 are maked as T1...T14
and N1..N16.

Name     T1    T2    N1   T3   N2  N3  N4  T4
J1       2     3     20   2    22  21  29   3
J2       4     1     20   3    20  21  22   4
J3       3     1     33   1    31  31  33   3
...
JX       3     2     20   2    21  22  24   2

The column samples are identical in both first and
second data frames. 

Third df object:
of Dim (200,2).  

V1         V2
mu1:J1     -11
mu1:J100   -10.4
mu2:J31     11.3
mu2:J2      10.4
.....       .....
muN:JX     34.5



I want to create a combination of Ts and Ns. Where I
want to subtract value of T-N in all combinations(225
combinations). Such as
T1-N1,T1-N2,T1-N3,T1-N4,T1-N5.......T14-N16

The rows should be the row pairs from 3rd dataframe. 


The final resultant matrix should look like the
following:


      T1-N1  T1-N2  T1-N3  T1-N4  T1-N5.......T14-N16
mu1   1(10-9) 1(10-9)   1    2      1            1
J100  -18(2-20) -20   -19  -27    -20          -29

mu2     1     3        2     2      1            1 
J2     -19   -21     -39    -31    -31         -28



I am a beginner level in R.  I apologise for asking
such a big question and subsequent help. I am unable
to go forward as I have no idea as to how to do this.
Could any one please help me. 
Thanks
sri


From antonio.raju at gmail.com  Wed Jun  7 20:11:32 2006
From: antonio.raju at gmail.com (antonio rodriguez)
Date: Wed, 07 Jun 2006 20:11:32 +0200
Subject: [R] how to read hdf files under R?
In-Reply-To: <B632E8C7-5CE9-43CD-9D38-F4B88E683C23@ird.fr>
References: <B632E8C7-5CE9-43CD-9D38-F4B88E683C23@ird.fr>
Message-ID: <448716D4.6020101@gmail.com>

Nicolas Degallier wrote:
> Hi!
> 
> I am trying to install in my R environment the rhdf5 package and  
> library but it seems to have vanished from either the CRAN or  
> BioConductors sites.
> 
> Can you tell me where it would be possible to find it or any R  
> library (or function) able to read hdf files?

hf5 it is supposed to do this, but I haven't any success trying to open 
an .hdf (V5) file (i.e: pathfinderv5 sst data) I always get some data 
format problem message. So I use to search for a netcdf format for the 
data I want and input into R with the netCDF library (old but useful)

BR,

arv


> 
> Sincerely,
> 
> Nicolas Degallier
> 
> UMR 7159 / IRD UR182
> Laboratoire d'Oc?anographie et du Climat, Exp?rimentation et  
> Approches Num?riques (LOCEAN)
> Tour 45-55, 4e ?t., case 100, 4 place Jussieu
> 75252  Paris Cedex 5  France
> 
> t?l: (33) 01 44 27 51 57
> fax: (33) 01 44 27 38 05
> E-mail: <Nicolas.Degallier at ird.fr>
> 
> Publications (anonymous ftp):
> 
> ftp://ftp.lodyc.jussieu.fr/LOCEAN/ndelod
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
=====
Por favor, si me mandas correos con copia a varias personas,
pon mi direcci?n de correo en copia oculta (CCO), para evitar
que acabe en montones de sitios, eliminando mi privacidad,
favoreciendo la propagaci?n de virus y la proliferaci?n del SPAM. Gracias.
-----
If you send me e-mail which has also been sent to several other people,
kindly mark my address as blind-carbon-copy (or BCC), to avoid its
distribution, which affects my privacy, increases the likelihood of
spreading viruses, and leads to more SPAM. Thanks.
=====


From pinard at iro.umontreal.ca  Wed Jun  7 20:44:06 2006
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Wed, 7 Jun 2006 14:44:06 -0400
Subject: [R] Edit function
In-Reply-To: <A89517C7FD248040BB71CA3C04C1ACBB04C9EF80@CNTUSMAEXS4.na.jnj.com>
References: <A89517C7FD248040BB71CA3C04C1ACBB04C9EF80@CNTUSMAEXS4.na.jnj.com>
Message-ID: <20060607184406.GA23737@alcyon.progiciels-bpi.ca>

[Pikounis, Bill [CNTUS]]

> view <- function(x) {
>   warnopt <- options()$warn
>   options(warn=-1)
>   on.exit({sink(); options(warn=warnopt)})
>   edit(x)
>   invisible()
> }

I'm surprised by the necessity of "sink()".  Presuming it is necessary 
indeed, the above could be simplified a bit like this (untested) code:

  view <- function(x) {
    on.exit(sink())
    invisible(suppressWarnings(edit(x)))
  }

The documentation for "suppressWarnings" is not overly clear about if 
the "warn" option is restored or not in case of error.  It says:

     'suppressWarnings' evaluates its expression in a context that
     ignores all warnings.

My exegesis :-) for that sentence would be that the context does not 
survive the error, and so, the "warn" option is not changed.

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca


From rva_ec at yahoo.com  Wed Jun  7 20:47:14 2006
From: rva_ec at yahoo.com (Vijay A Raghavan)
Date: Wed, 7 Jun 2006 11:47:14 -0700 (PDT)
Subject: [R] decideTests extraction of p-values
Message-ID: <20060607184714.46750.qmail@web53508.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060607/5c37755e/attachment.pl 

From uhkeller at web.de  Wed Jun  7 20:55:11 2006
From: uhkeller at web.de (Ulrich Keller)
Date: Wed, 07 Jun 2006 20:55:11 +0200
Subject: [R] bootstrap data from groups
In-Reply-To: <20060606172731.49313.qmail@web53402.mail.yahoo.com>
References: <20060606172731.49313.qmail@web53402.mail.yahoo.com>
Message-ID: <4487210F.3080904@web.de>

I am not sure I understand what you want to do, but maybe some of this 
will be helpful. I first generate some data that should resemble yours:

dat<-expand.grid(Region=1:3, Species=1:4, Sex=c("M","F"))
dat<-do.call("rbind",lapply(1:10,function(x) dat))
dat$Bodysize<-rnorm(nrow(dat),10,2)

Now what the following piece of code does is this: it samples 4 of the 
10 individuals in each of the 24 subsets (region*species*sex) and 
creates a new data frame with 96 cases. It then computes the mean of 
bodysize in each of the subsets. The whole thing is done 100 times, the 
results are put in a data frame. We end up with 100 bootstrapped means 
for the 24 subsets.

groupmeans<-sapply(1:100, function(z) {
  dat.rs<-do.call("rbind",
    lapply(split(dat,list(dat$Region,dat$Species,dat$Sex)),
      function(x) x[sample(10, 4, replace=TRUE),]))
  aggregate(dat.rs$Bodysize,
    list(dat.rs$Region,dat.rs$Species,dat.rs$Sex),
    mean)$x
  }
)
tmp<-aggregate(dat$Bodysize,
  list(dat$Region,dat$Species,dat$Sex),mean)
rownames(groupmeans)<-apply(tmp[,1:3],1,paste,collapse="")

Now we can compute the mean and sd of the means by group:

 > apply(groupmeans,1,mean)
      11M       21M       31M       12M       22M       32M       
13M       23M
 9.353095  9.267570  9.907933 10.992796  9.575841 10.412816  9.646964  
9.433724
      33M       14M       24M       34M       11F       21F       
31F       12F
10.750797  9.083630 10.573421  9.615743 10.267587 10.231126  9.329375 
10.799071
      22F       32F       13F       23F       33F       14F       
24F       34F
 9.355510 10.555705  9.919161 10.277103  9.335649  9.339544 10.023688  
9.755115
 > apply(groupmeans,1,sd)
      11M       21M       31M       12M       22M       32M       
13M       23M
0.7720758 1.5301540 1.0973516 0.8970237 1.0492995 0.9460970 0.5362957 
1.1106675
      33M       14M       24M       34M       11F       21F       
31F       12F
0.5333081 0.9259341 0.8198624 0.8061832 0.8466780 0.7052473 0.9857680 
1.1057607
      22F       32F       13F       23F       33F       14F       
24F       34F
0.8272433 1.2614559 1.2377154 1.0958545 0.9213648 0.9985215 1.1131870 
1.0572494

Milton Cezar schrieb:
> Hi R-friends.
>    
>   I have a mammal?s dataset looking like:
>    
>        Region   Species Sex  Bodysize
>          1           Sp1      M      10.2
>          1           Sp1      M      12.1
>          1           Sp1      M       9.1
>         ...
>    
>   I have three regions, four species and the body size of 10 individual. I?d like to do a bootstrap resample (100 resamples) of 4 of 10 individuals for each Region, Species and Sex and compute de means and S.D. for the combinations Regions-Species-Sex.
>    
>   How can I do that?
>    
>   Thanks a lot,
>    
>   Miltinho
>
>  __________________________________________________
>
>
> 	[[alternative HTML version deleted]]
>
>   
> ------------------------------------------------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From jansc at gmx.net  Wed Jun  7 21:24:37 2006
From: jansc at gmx.net (Jan Schwanbeck)
Date: Wed, 07 Jun 2006 21:24:37 +0200
Subject: [R] simplier way to import txt time-series data
Message-ID: <448727F5.4030304@gmx.net>

Hello much more experienced R-users,

I have got a txt - file which contains data formated like this:

YYYY MM DD HH  data1  data2 ... data31
2002 12 01 01 0.002 0.003 ... 312.0

The single columns are divided by at least one space.

Is their an easy and fast way to make R understand that the first 4 
columns are year month day and hour  to recognize it as time series data.

The time series are not complete. Single hours could be missing.

Thanks a lot for answering my question. I just cannot believe that it is 
a big problem for R to handle this clean format while it even imports 
data from excel-files.

Greatings

Jan


From muster at gmail.com  Wed Jun  7 21:32:01 2006
From: muster at gmail.com (Mu Tian)
Date: Wed, 7 Jun 2006 15:32:01 -0400
Subject: [R] R crashes on quantreg
Message-ID: <b68812e70606071232g53d4193fw75a60867f3df3d26@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060607/9e830134/attachment.pl 

From muster at gmail.com  Wed Jun  7 21:43:44 2006
From: muster at gmail.com (Mu Tian)
Date: Wed, 7 Jun 2006 15:43:44 -0400
Subject: [R] R crashes on quantreg
In-Reply-To: <b68812e70606071232g53d4193fw75a60867f3df3d26@mail.gmail.com>
References: <b68812e70606071232g53d4193fw75a60867f3df3d26@mail.gmail.com>
Message-ID: <b68812e70606071243m34b2ad01w56b0c9f1c90069ef@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060607/e04d2ece/attachment.pl 

From rkoenker at uiuc.edu  Wed Jun  7 21:48:27 2006
From: rkoenker at uiuc.edu (roger koenker)
Date: Wed, 7 Jun 2006 14:48:27 -0500
Subject: [R] R crashes on quantreg
In-Reply-To: <b68812e70606071232g53d4193fw75a60867f3df3d26@mail.gmail.com>
References: <b68812e70606071232g53d4193fw75a60867f3df3d26@mail.gmail.com>
Message-ID: <04D8A320-A674-4601-995C-FCE9CC26276A@uiuc.edu>

Since the "crash" occurs plotting the lm object it is unclear what
this has to do with quantreg, but maybe you could explain

	1.  what you mean by crash,
	2.  something about x,y,
	
This is best addressed to the maintainer of the package rather than to
R-help, provided, of course, that it is really a question about  
quantreg.

url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820


On Jun 7, 2006, at 2:32 PM, Mu Tian wrote:

> I was trying "quantreg" package,
>
> lm1 <- lm(y~x)
> rq1 <- rq(y~x)
> plot(summary(rq1)) #then got a warning says singular value, etc.  
> but this
> line can be omited
> plot(lm1) #crash here
>
> It happened every time on my PC, Windows XP Pro Serv. Pack 1,  
> Pentium(4)
> 3.00G.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html


From ripley at stats.ox.ac.uk  Wed Jun  7 21:50:47 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 7 Jun 2006 20:50:47 +0100 (BST)
Subject: [R] R crashes on quantreg
In-Reply-To: <b68812e70606071243m34b2ad01w56b0c9f1c90069ef@mail.gmail.com>
References: <b68812e70606071232g53d4193fw75a60867f3df3d26@mail.gmail.com>
	<b68812e70606071243m34b2ad01w56b0c9f1c90069ef@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0606072049000.7332@gannet.stats.ox.ac.uk>

Without y and x we cannot reproduce this.

PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

On Wed, 7 Jun 2006, Mu Tian wrote:

> I forgot to mention my R version is 2.3.1 and quantreg is the most updated
> too.

It has a version number, which the posting guide tells you how to find.

> On 6/7/06, Mu Tian <muster at gmail.com> wrote:
>>
>>  I was trying "quantreg" package,
>>
>> lm1 <- lm(y~x)
>> rq1 <- rq(y~x)
>> plot(summary(rq1)) #then got a warning says singular value, etc. but this
>> line can be omited
>> plot(lm1) #crash here
>>
>> It happened every time on my PC, Windows XP Pro Serv. Pack 1, Pentium(4)
>> 3.00G.
>>
>
> 	[[alternative HTML version deleted]]



-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From muster at gmail.com  Wed Jun  7 22:05:19 2006
From: muster at gmail.com (Mu Tian)
Date: Wed, 7 Jun 2006 16:05:19 -0400
Subject: [R] R crashes on quantreg
In-Reply-To: <Pine.LNX.4.64.0606072049000.7332@gannet.stats.ox.ac.uk>
References: <b68812e70606071232g53d4193fw75a60867f3df3d26@mail.gmail.com>
	<b68812e70606071243m34b2ad01w56b0c9f1c90069ef@mail.gmail.com>
	<Pine.LNX.4.64.0606072049000.7332@gannet.stats.ox.ac.uk>
Message-ID: <b68812e70606071305t141c043erac53699def331780@mail.gmail.com>

I attached the data file here. I restarted the PC but it still happens. It
says a memory address could not be written. I am not sure it is a problem of
R or quantreg but I plot without problems before I load quantreg.

Thank you.

Tian

On 6/7/06, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>
> Without y and x we cannot reproduce this.
>
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> On Wed, 7 Jun 2006, Mu Tian wrote:
>
> > I forgot to mention my R version is 2.3.1 and quantreg is the most
> updated
> > too.
>
> It has a version number, which the posting guide tells you how to find.
>
> > On 6/7/06, Mu Tian <muster at gmail.com> wrote:
> >>
> >>  I was trying "quantreg" package,
> >>
> >> lm1 <- lm(y~x)
> >> rq1 <- rq(y~x)
> >> plot(summary(rq1)) #then got a warning says singular value, etc. but
> this
> >> line can be omited
> >> plot(lm1) #crash here
> >>
> >> It happened every time on my PC, Windows XP Pro Serv. Pack 1,
> Pentium(4)
> >> 3.00G.
> >>
> >
> >       [[alternative HTML version deleted]]
>
>
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>

From Manuel.A.Morales at williams.edu  Wed Jun  7 22:01:36 2006
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Wed, 07 Jun 2006 16:01:36 -0400
Subject: [R] Using data=x or subset=y in user-defined functions
Message-ID: <1149710497.14236.11.camel@solidago.localdomain>

Dear list members,

In some of my functions, I attach the data internally to allow subset
commands or to specify a data frame. This works well except for cases
where there is a "masking" conflict (which returns a warning). I see
some alternative listed in ?attach, but I'm not sure which of them do
what I'd like. Any suggestions?

Below is how I've been setting up my functions:

eg.function <- function(x, data=NULL, subset=NULL, ...) {

# Set up environment
on.exit(detach(data))
attach(data)
if(!is.null(subset)) {
    data<-subset(data,subset)
detach(data)
attach(data)
}
subset = NULL
 
# Function body here
output <- x   
return(output)
}

Thanks!

Manuel


From ripley at stats.ox.ac.uk  Wed Jun  7 22:15:56 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 7 Jun 2006 21:15:56 +0100 (BST)
Subject: [R] R crashes on quantreg
In-Reply-To: <04D8A320-A674-4601-995C-FCE9CC26276A@uiuc.edu>
References: <b68812e70606071232g53d4193fw75a60867f3df3d26@mail.gmail.com>
	<04D8A320-A674-4601-995C-FCE9CC26276A@uiuc.edu>
Message-ID: <Pine.LNX.4.64.0606072111370.7913@gannet.stats.ox.ac.uk>

On Wed, 7 Jun 2006, roger koenker wrote:

> Since the "crash" occurs plotting the lm object it is unclear what
> this has to do with quantreg, but maybe you could explain
>
> 	1.  what you mean by crash,
> 	2.  something about x,y,
>
> This is best addressed to the maintainer of the package rather than to
> R-help, provided, of course, that it is really a question about
> quantreg.

Agreed, but just so R-help knows how to solve such problems, I ran this 
under valgrind (see `Writing R Extensions') and got

> rq1 <- rq(y~x)
==7870== Invalid write of size 8
==7870==    at 0x9ED8BDE: rqbr_ (rqbr.f:309)
...
==7870==  Address 0x92162A0 is 12,248 bytes inside a block of size 12,288 free'd
==7870==    at 0x49055DD: free (vg_replace_malloc.c:235)
==7870==    by 0x53D64A: build_trtable (regex.c:9618)
==7870==    by 0x53AEB4: transit_state (regex.c:8393)
==7870==    by 0x538977: check_matching (regex.c:7318)
==7870==    by 0x537E89: re_search_internal (regex.c:7007)
==7870==    by 0x53EF3D: Rregexec (regex.c:10440)
==7870==    by 0x438AAF: do_gsub (character.c:1127)

and it seems object lm1 has been trashed and you soon get a segfault.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Wed Jun  7 22:36:50 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 7 Jun 2006 21:36:50 +0100 (BST)
Subject: [R] Using data=x or subset=y in user-defined functions
In-Reply-To: <1149710497.14236.11.camel@solidago.localdomain>
References: <1149710497.14236.11.camel@solidago.localdomain>
Message-ID: <Pine.LNX.4.64.0606072135350.8690@gannet.stats.ox.ac.uk>

I suggest you investigate with().

On Wed, 7 Jun 2006, Manuel Morales wrote:

> Dear list members,
>
> In some of my functions, I attach the data internally to allow subset
> commands or to specify a data frame. This works well except for cases
> where there is a "masking" conflict (which returns a warning). I see
> some alternative listed in ?attach, but I'm not sure which of them do
> what I'd like. Any suggestions?
>
> Below is how I've been setting up my functions:
>
> eg.function <- function(x, data=NULL, subset=NULL, ...) {
>
> # Set up environment
> on.exit(detach(data))
> attach(data)
> if(!is.null(subset)) {
>    data<-subset(data,subset)
> detach(data)
> attach(data)
> }
> subset = NULL
>
> # Function body here
> output <- x
> return(output)
> }
>
> Thanks!
>
> Manuel
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From recepaykaya at gmail.com  Wed Jun  7 22:49:01 2006
From: recepaykaya at gmail.com (Recep Aykaya)
Date: Wed, 7 Jun 2006 23:49:01 +0300
Subject: [R] bootstrapping
Message-ID: <dee2a4c20606071349w3ed4aa5bw30a58eaa09c79e7a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060607/d5662f51/attachment.pl 

From rkoenker at uiuc.edu  Wed Jun  7 23:07:25 2006
From: rkoenker at uiuc.edu (roger koenker)
Date: Wed, 7 Jun 2006 16:07:25 -0500
Subject: [R] R crashes on quantreg
In-Reply-To: <b68812e70606071305t141c043erac53699def331780@mail.gmail.com>
References: <b68812e70606071232g53d4193fw75a60867f3df3d26@mail.gmail.com>
	<b68812e70606071243m34b2ad01w56b0c9f1c90069ef@mail.gmail.com>
	<Pine.LNX.4.64.0606072049000.7332@gannet.stats.ox.ac.uk>
	<b68812e70606071305t141c043erac53699def331780@mail.gmail.com>
Message-ID: <9D44F726-67A9-4B09-9F3E-B6E6721C0655@uiuc.edu>

R-help doesn't  foward attached data files like this, but Brian
kindly forwarded it to me.

You need to restrict X so that it is full rank,  it now has
rank 19 and column dimension 29 (with intercept).  See
for example svd(cbind(1,x)).

I'll add some better checking for this, but it will basically amount
to setting singular.ok = FALSE in lm() and forcings users to do
the rank reduction themselves.


url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820


On Jun 7, 2006, at 3:05 PM, Mu Tian wrote:

> I attached the data file here. I restarted the PC but it still  
> happens. It
> says a memory address could not be written. I am not sure it is a  
> problem of
> R or quantreg but I plot without problems before I load quantreg.
>
> Thank you.
>
> Tian
>
> On 6/7/06, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>>
>> Without y and x we cannot reproduce this.
>>
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>> On Wed, 7 Jun 2006, Mu Tian wrote:
>>
>> > I forgot to mention my R version is 2.3.1 and quantreg is the most
>> updated
>> > too.
>>
>> It has a version number, which the posting guide tells you how to  
>> find.
>>
>> > On 6/7/06, Mu Tian <muster at gmail.com> wrote:
>> >>
>> >>  I was trying "quantreg" package,
>> >>
>> >> lm1 <- lm(y~x)
>> >> rq1 <- rq(y~x)
>> >> plot(summary(rq1)) #then got a warning says singular value,  
>> etc. but
>> this
>> >> line can be omited
>> >> plot(lm1) #crash here
>> >>
>> >> It happened every time on my PC, Windows XP Pro Serv. Pack 1,
>> Pentium(4)
>> >> 3.00G.
>> >>
>> >
>> >       [[alternative HTML version deleted]]
>>
>>
>>
>> --
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html


From Clement.Viel at polytech-lille.net  Wed Jun  7 23:43:56 2006
From: Clement.Viel at polytech-lille.net (=?ISO-8859-1?Q?Cl=E9ment_Viel?=)
Date: Wed, 7 Jun 2006 22:43:56 +0100
Subject: [R] bootstrapping
In-Reply-To: <dee2a4c20606071349w3ed4aa5bw30a58eaa09c79e7a@mail.gmail.com>
References: <dee2a4c20606071349w3ed4aa5bw30a58eaa09c79e7a@mail.gmail.com>
Message-ID: <67a23b050606071443p62f7244dp971bdf4b5554585c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060607/d6763289/attachment.pl 

From ggrothendieck at gmail.com  Thu Jun  8 00:09:37 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 7 Jun 2006 18:09:37 -0400
Subject: [R] simplier way to import txt time-series data
In-Reply-To: <448727F5.4030304@gmx.net>
References: <448727F5.4030304@gmx.net>
Message-ID: <971536df0606071509qf5a5a54s24aa829d83aa1521@mail.gmail.com>

Try this which reads in the data as a data frame and appends a
chron variable datetime:

Lines <- "YYYY MM DD HH  data1  data2
2002 12 01 01 0.002 0.003"

DF <- read.table(textConnection(Lines), header = TRUE)
library(chron)
DF$datetime <- with(DF, chron(paste(MM, DD, YYYY, sep = "/")) + HH/24)
DF

On 6/7/06, Jan Schwanbeck <jansc at gmx.net> wrote:
> Hello much more experienced R-users,
>
> I have got a txt - file which contains data formated like this:
>
> YYYY MM DD HH  data1  data2 ... data31
> 2002 12 01 01 0.002 0.003 ... 312.0
>
> The single columns are divided by at least one space.
>
> Is their an easy and fast way to make R understand that the first 4
> columns are year month day and hour  to recognize it as time series data.
>
> The time series are not complete. Single hours could be missing.
>
> Thanks a lot for answering my question. I just cannot believe that it is
> a big problem for R to handle this clean format while it even imports
> data from excel-files.
>
> Greatings
>
> Jan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From jianing at math.bu.edu  Thu Jun  8 05:41:17 2006
From: jianing at math.bu.edu (Jianing Di)
Date: Wed, 7 Jun 2006 23:41:17 -0400 (EDT)
Subject: [R] How to do this simple integration?
Message-ID: <3231.66.75.237.140.1149738077.squirrel@math.bu.edu>

Hello,

I have a simple function in the form as follows:

> f<-function(x){sum(v^x)}

where v is a vector. I was trying to integrate f using the command

> I<-integrate(f,0,1)

However, this will not work and seems that the reason is to use
"integrate", the f must be a function that with input and output of same
length. Anyone can point out which command should I use in order to
compute this type of integration(such as a function involve sum(), prod(),
etc.)?

Thank you.

Jianing


-- 
There are three kinds of lies: lies, damned lies, and Statistics.

                                        -------Benjamin Disraeli


From spencer.graves at pdf.com  Thu Jun  8 06:01:08 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 07 Jun 2006 21:01:08 -0700
Subject: [R] [OFF] The "best" tool for a space-temporal analyses?
In-Reply-To: <200606061331.56525.chrysopa@gmail.com>
References: <200606061331.56525.chrysopa@gmail.com>
Message-ID: <4487A104.9040206@pdf.com>

	  Have you tried RSiteSearch("spatial ecology")?  I just got 47 hits 
from that.  Some of them might be relevant to your question.

	  If that fails, you might consider providing this group with the math 
behind the automata models you are considering.  I might expect them to 
be expressed in terms of Markov chain (or Markov random field) 
probability models with parameters to be estimated.  The standard 
statistical approach is to consider a sequence of different models, with 
at least some of them nested, with increasing numbers of parameters and 
levels of complexity.  We then estimate the parameters to maximize the 
likelihood (= probability of what was observed given the data).  Testing 
typically assumes that 2*log(likelihood ratio) is approximately 
chi-square, with additional precision given by simulation if desired.

	  Hope this helps.
	  Spencer Graves

Ronaldo Reis-Jr. wrote:
> Hi,
> 
> I try to make an analyses to discover what is the time that an area begin to 
> have spacial autocorrelation. And after, what is the number of individuals 
> responsible for this autocorrelation.
> 
> The main idea is to discover if exist a contamination of a quadrat from others 
> quadrats and how is the population needed to make this contamination.
> 
> This is very common to use automata to simulate this situation. But I try to 
> make a more statistical approach. I'm studing about, but I dont know the tool 
> for testing examples.
> 
> I make an example just for tests:
> 
> Geodata <- data.frame(X=rep(rep(c(1:10),
> (rep(10,10))),5),Y=rep(c(1:10),50),Abund=c(1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 
> 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 
> 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 3, 0, 0, 2, 0, 0, 1, 2, 0, 0, 
> 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 2, 0, 0, 0, 
> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 2, 0, 0, 
> 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 5, 
> 0, 2, 0, 0, 0, 0, 3, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 2, 3, 0, 0, 3, 0, 0, 3, 
> 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 0, 0, 4, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 
> 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 2, 10, 0, 0, 3, 0, 0, 0, 0, 3, 
> 3, 4, 2, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 0, 3, 0, 
> 0, 0, 0, 4, 0, 2, 1, 0, 0, 3, 0, 0, 0, 2, 0, 1, 4, 0, 0, 4, 0, 0, 4, 0, 0, 0, 
> 0, 0, 4, 0, 0, 0, 0, 0, 3, 0, 0, 5, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
> 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 4, 10, 15, 0, 0, 4, 0, 0, 0, 0, 8, 11, 9, 0, 
> 0, 0, 0, 0, 0, 0, 1, 5, 3, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 
> 5, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 1, 0, 5, 0, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 0, 
> 0, 0, 0, 0, 0, 4, 0, 0, 6, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
> 0, 0, 0, 0, 0, 0, 0, 3, 10, 15, 20, 0, 0, 0, 0, 0, 0, 4, 13, 16, 13, 0, 0, 0, 
> 0, 0, 0, 5, 8, 8, 10, 0, 0, 0, 0, 0, 0, 1, 2, 3, 5, 0, 0, 0, 0, 0, 0, 0, 0, 
> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
> 0),Time=rep(c(1:5),rep(100,5)))
> 
> X and Y are coordinates, Abund is the number of individuals and Time is the 
> date of observation. In this example the population grows from an vertice, 
> and after 10 individuals it contaminates your neighbors. I need ideas about 
> the best approach and R's tools for this problem.
> 
> I'm studing this question in these books:
> 
> W.N. Venables, B.D. Ripley. 2003.  Modern Applied Statistics with S. Springer; 
> 4 edition (September 2, 2003). 512 pages.
> 
> Crawley, M. J. 2002. Statistical Computing: An Introduction to Data Analysis 
> using S-Plus. John Wiley & Sons; 1st edition (May 15, 2002). 772 pages.
> 
> Diggle, Peter J. 2003. Statistical Analysis of Spatial Point Patterns (2nd 
> ed.), Arnold, London.
> 
> Ripley, B.D. Spatial Statistics
> 
> Spatial Ecology
> 
> Thanks for all
>


From epistat at gmail.com  Thu Jun  8 06:10:49 2006
From: epistat at gmail.com (zhijie zhang)
Date: Thu, 8 Jun 2006 12:10:49 +0800
Subject: [R] how to analyze the following data?--anxious for the result
Message-ID: <2fc17e30606072110r1512061dt94fd599ff11f21e8@mail.gmail.com>

Dear friends,
 I have a dataset: response var--y, class var-group, and the third variable-x.
 I want to test whether there is statistical significance bewteen
group for y with the controlled x. First, i want to use analysis of
covariance in SAS, but i found that y isn't noramal and can't become
normal through transformation.
 Under that condition, what should i do using R / SAS?
 Any suggestions are great appreciate!
-- 
Kind Regards,Zhi Jie,Zhang ,PHDDepartment of EpidemiologySchool of
Public HealthFudan UniversityTel:86-21-54237149


From ripley at stats.ox.ac.uk  Thu Jun  8 08:07:27 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 8 Jun 2006 07:07:27 +0100 (BST)
Subject: [R] bootstrapping
In-Reply-To: <67a23b050606071443p62f7244dp971bdf4b5554585c@mail.gmail.com>
References: <dee2a4c20606071349w3ed4aa5bw30a58eaa09c79e7a@mail.gmail.com>
	<67a23b050606071443p62f7244dp971bdf4b5554585c@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0606080657530.14460@gannet.stats.ox.ac.uk>

In non-parametric bootstrapping, you do draw a sample of the same size as 
the original.  So

> D <- as.vector(WorldPhones[,1])
> sample(D, replace=TRUE)
[1] 45939 64721 60423 79831 71799 45939 76036
> sample(D, replace=TRUE)
[1] 45939 68484 79831 68484 76036 71799 79831

are two different bootstrap resamples.  The problem in the original code 
is that it is trying to replace 1 value by a sample.  Removing extraneous 
bracketing, it was

> boot = numeric(200)
> for (i in 1:200) boot [i] = sample(D,replace=T)
There were 50 or more warnings (use warnings() to see the first 50)

and those warnings are

> warnings()
Warning messages:
1: number of items to replace is not a multiple of replacement length

*and* the values returned are not all the same (as claimed).

To return a set of bootstrap resamples you need something like

boot <- matrix(0,200,length(D))
for (i in 1:200) boot [i,] <- sample(D, replace=TRUE)

or you might intend to save the resampled statistic and not the sample 
itself.


On Wed, 7 Jun 2006, Cl?ment Viel wrote:

> 2006/6/7, Recep Aykaya <recepaykaya at gmail.com>:
>>
>> hi.
>> i'm a statistics student and studying bootstrap in R.
>>
>> i'm trying to draw bootstrap samples from a sample, using the following R
>> code:
>>
>>> *boot = numeric(200)*
>> *> {for (i in 1:200)*
>>
>> *  boot [i] = (sample(data,replace=T))}*
>>
>>
>>
>> i obtain 200 samples but all of them are the same.
>>
>> i want to obtain different samples. what should i do? can you please help
>> me
>> if possible.
>>
>>
>>
>> thank you.
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>
> Hi,
> I think you have forgotten an parameter of the sample function. Indeed the
> help page give these information:
>
> sample(x, size, replace = FALSE, prob = NULL)
>
> size: non-negative integer giving the number of items to choose
>
> By default 'size' is equal to 'length(x)' so that 'sample(x)'
> generates a random permutation of the elements of 'x' (or '1:x').
>
> Therefore try to add the number of observed values with this:
>
> nb=100  #the number of  values for each sample
> boot = c()
> for (i in 1:200)
>   boot [i] = mean(sample(x=data,size=nb,replace=TRUE)) # if you try to
> estimate the mean
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Thu Jun  8 08:26:47 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 8 Jun 2006 07:26:47 +0100 (BST)
Subject: [R] how to analyze the following data?--anxious for the result
In-Reply-To: <2fc17e30606072110r1512061dt94fd599ff11f21e8@mail.gmail.com>
References: <2fc17e30606072110r1512061dt94fd599ff11f21e8@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0606080708000.14460@gannet.stats.ox.ac.uk>

Asserting `y isn't noramal' only eliminates one possibility.  Is it 
binary, a count, long-tailed, short-tailed, ...?

This is really a statistical consulting problem (and apparently about 
SAS): see the R posting guide for where to seek advice on such problems.
As the guide says, had we had sufficient information we might have been 
able to help, but as you need to ask again, please do so in a more 
appropriate place.

On Thu, 8 Jun 2006, zhijie zhang wrote:

> Dear friends,
> I have a dataset: response var--y, class var-group, and the third variable-x.
> I want to test whether there is statistical significance bewteen
> group for y with the controlled x. First, i want to use analysis of
> covariance in SAS, but i found that y isn't noramal and can't become
> normal through transformation.
> Under that condition, what should i do using R / SAS?
> Any suggestions are great appreciate!
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From davidhughjones at gmail.com  Thu Jun  8 08:50:19 2006
From: davidhughjones at gmail.com (David Hugh-Jones)
Date: Thu, 8 Jun 2006 07:50:19 +0100
Subject: [R] help with combination problem
In-Reply-To: <20060607180418.27833.qmail@web38104.mail.mud.yahoo.com>
References: <20060607180418.27833.qmail@web38104.mail.mud.yahoo.com>
Message-ID: <f5d848060606072350x4c65ca46m9e547564c796ff8@mail.gmail.com>

hi Srinivas

I'm not sure I understand: your example result has 10-9 for mu1 but 1
for mu2, which is not equal to 11-9. I will assume you just got the
numbers wrong. (Also J100 should be J1, no?)

I am also wondering why you want what you say. It doesn't seem very
sensible to mix the results from two data frames like this.

The simplest thing is probably to do the two data frames separately,
then intermix the rows. You can do this with for loops:


for (col1 in paste("T", 1:14, sep="")) {
  for (col2 in paste("N", 1:16, sep="")) result[[paste(col1, "-",
col2, sep="")]] = df1[[col1]] - df1[[col2]]
}

Now repeat for data frame 2 with another data frame, e.g. result2
Then intermix the rows from result and result2. I'll leave you to
figure that bit out.

David



On 07/06/06, Srinivas Iyyer <srini_iyyer_bio at yahoo.com> wrote:
> hello:
>
> I have 3 data.frame objects.
>
> First df object:
> Of dim (149,31). Columns 2:31 are marked as T1..T14
> and N1..N16.
>
> Name     T1    T2    N1   T3   N2  N3  N4  T4
> mu1      10    10    9    10   9   9   8   10
> mu2      11    11    9    11   9   9   9   11
> ...
> muN      12    12    9    11   9   9   8   12
>
>
>
>
> Second df object:
> of Dim (50000,31). Columns 2:31 are maked as T1...T14
> and N1..N16.
>
> Name     T1    T2    N1   T3   N2  N3  N4  T4
> J1       2     3     20   2    22  21  29   3
> J2       4     1     20   3    20  21  22   4
> J3       3     1     33   1    31  31  33   3
> ...
> JX       3     2     20   2    21  22  24   2
>
> The column samples are identical in both first and
> second data frames.
>
> Third df object:
> of Dim (200,2).
>
> V1         V2
> mu1:J1     -11
> mu1:J100   -10.4
> mu2:J31     11.3
> mu2:J2      10.4
> .....       .....
> muN:JX     34.5
>
>
>
> I want to create a combination of Ts and Ns. Where I
> want to subtract value of T-N in all combinations(225
> combinations). Such as
> T1-N1,T1-N2,T1-N3,T1-N4,T1-N5.......T14-N16
>
> The rows should be the row pairs from 3rd dataframe.
>
>
> The final resultant matrix should look like the
> following:
>
>
>      T1-N1  T1-N2  T1-N3  T1-N4  T1-N5.......T14-N16
> mu1   1(10-9) 1(10-9)   1    2      1            1
> J100  -18(2-20) -20   -19  -27    -20          -29
>
> mu2     1     3        2     2      1            1
> J2     -19   -21     -39    -31    -31         -28
>
>
>
> I am a beginner level in R.  I apologise for asking
> such a big question and subsequent help. I am unable
> to go forward as I have no idea as to how to do this.
> Could any one please help me.
> Thanks
> sri
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From petr.pikal at precheza.cz  Thu Jun  8 09:21:43 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 08 Jun 2006 09:21:43 +0200
Subject: [R] help with combination problem
In-Reply-To: <f5d848060606072350x4c65ca46m9e547564c796ff8@mail.gmail.com>
References: <20060607180418.27833.qmail@web38104.mail.mud.yahoo.com>
Message-ID: <4487EC27.10336.51284E@localhost>

Hi

or you could try an approach of splitting original data frames and 
using for cycle

> df<-matrix(sample(1:15), 3,5)
> df<-data.frame(df)
> names(df)
> names(df)[c(1,3,4)]<-c("Y1","Y2","Y3")
> df
  Y1 X2 Y2 Y3 X5
1 10 15  8  3  7
2 13  6 12  2  1
3 11  5  9  4 14
> df1<-df[,grep("Y",names(df))]
> df1
  Y1 Y2 Y3
1 10  8  3
2 13 12  2
3 11  9  4
> df2<-df[,-grep("Y",names(df))]
> df2
  X2 X5
1 15  7
2  6  1
3  5 14
> for(i in 1:3) print(df1[,i]-df2)
  X2 X5
1 -5  3
2  7 12
3  6 -3
  X2 X5
1 -7  1
2  6 11
3  4 -5
   X2  X5
1 -12  -4
2  -4   1
3  -1 -10

and merge resulting data frames together preferably in the cycle.

HTH
Petr

On 8 Jun 2006 at 7:50, David Hugh-Jones wrote:

Date sent:      	Thu, 8 Jun 2006 07:50:19 +0100
From:           	"David Hugh-Jones" <davidhughjones at gmail.com>
To:             	"Srinivas Iyyer" <srini_iyyer_bio at yahoo.com>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] help with combination problem

> hi Srinivas
> 
> I'm not sure I understand: your example result has 10-9 for mu1 but 1
> for mu2, which is not equal to 11-9. I will assume you just got the
> numbers wrong. (Also J100 should be J1, no?)
> 
> I am also wondering why you want what you say. It doesn't seem very
> sensible to mix the results from two data frames like this.
> 
> The simplest thing is probably to do the two data frames separately,
> then intermix the rows. You can do this with for loops:
> 
> 
> for (col1 in paste("T", 1:14, sep="")) {
>   for (col2 in paste("N", 1:16, sep="")) result[[paste(col1, "-",
> col2, sep="")]] = df1[[col1]] - df1[[col2]] }
> 
> Now repeat for data frame 2 with another data frame, e.g. result2 Then
> intermix the rows from result and result2. I'll leave you to figure
> that bit out.
> 
> David
> 
> 
> 
> On 07/06/06, Srinivas Iyyer <srini_iyyer_bio at yahoo.com> wrote:
> > hello:
> >
> > I have 3 data.frame objects.
> >
> > First df object:
> > Of dim (149,31). Columns 2:31 are marked as T1..T14
> > and N1..N16.
> >
> > Name     T1    T2    N1   T3   N2  N3  N4  T4
> > mu1      10    10    9    10   9   9   8   10
> > mu2      11    11    9    11   9   9   9   11
> > ...
> > muN      12    12    9    11   9   9   8   12
> >
> >
> >
> >
> > Second df object:
> > of Dim (50000,31). Columns 2:31 are maked as T1...T14
> > and N1..N16.
> >
> > Name     T1    T2    N1   T3   N2  N3  N4  T4
> > J1       2     3     20   2    22  21  29   3
> > J2       4     1     20   3    20  21  22   4
> > J3       3     1     33   1    31  31  33   3
> > ...
> > JX       3     2     20   2    21  22  24   2
> >
> > The column samples are identical in both first and
> > second data frames.
> >
> > Third df object:
> > of Dim (200,2).
> >
> > V1         V2
> > mu1:J1     -11
> > mu1:J100   -10.4
> > mu2:J31     11.3
> > mu2:J2      10.4
> > .....       .....
> > muN:JX     34.5
> >
> >
> >
> > I want to create a combination of Ts and Ns. Where I
> > want to subtract value of T-N in all combinations(225
> > combinations). Such as
> > T1-N1,T1-N2,T1-N3,T1-N4,T1-N5.......T14-N16
> >
> > The rows should be the row pairs from 3rd dataframe.
> >
> >
> > The final resultant matrix should look like the
> > following:
> >
> >
> >      T1-N1  T1-N2  T1-N3  T1-N4  T1-N5.......T14-N16
> > mu1   1(10-9) 1(10-9)   1    2      1            1
> > J100  -18(2-20) -20   -19  -27    -20          -29
> >
> > mu2     1     3        2     2      1            1
> > J2     -19   -21     -39    -31    -31         -28
> >
> >
> >
> > I am a beginner level in R.  I apologise for asking
> > such a big question and subsequent help. I am unable
> > to go forward as I have no idea as to how to do this.
> > Could any one please help me.
> > Thanks
> > sri
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From msubianto at gmail.com  Thu Jun  8 11:13:24 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Thu, 8 Jun 2006 11:13:24 +0200
Subject: [R] expand only one of variable
Message-ID: <c7c17cef0606080213n6ce3e518ic2cb52bbe52b102a@mail.gmail.com>

Dear all,
I want to expand only one of variable in data frame and the others
variable will be following with the expand variable. Here my toy example:

toy.df <- data.frame(size=c(3,1,2,0,3,5,1,0), group=LETTERS[1:8],

country=c("Germany","England","Argentina","Mexico","Italy","Brazil","France","Spain"),
                     w=rep(0,8), d=rep(0,8), l=rep(0,8))
toy.df

The size variable is the number of expand and signed by positive but size
with 0 (zero) must be signed by negative.
The result something like:

  size group   country w d l      sign
     3     A   Germany 0 0 0  positive
     3     A   Germany 0 0 0  positive
     3     A   Germany 0 0 0  positive
     1     B   England 0 0 0  positive
     2     C Argentina 0 0 0  positive
     2     C Argentina 0 0 0  positive
     0     D    Mexico 0 0 0  negative
     3     E     Italy 0 0 0  positive
     3     E     Italy 0 0 0  positive
     3     E     Italy 0 0 0  positive
     5     F    Brazil 0 0 0  positive
     5     F    Brazil 0 0 0  positive
     5     F    Brazil 0 0 0  positive
     5     F    Brazil 0 0 0  positive
     5     F    Brazil 0 0 0  positive
     1     G    France 0 0 0  positive
     0     H     Spain 0 0 0  negative

I  would be very happy if anyone could help me.
Thank you very much in advance.

Kindly regards, Muhammad Subianto


From jacques.veslot at good.ibl.fr  Thu Jun  8 11:35:37 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Thu, 08 Jun 2006 11:35:37 +0200
Subject: [R] expand only one of variable
In-Reply-To: <c7c17cef0606080213n6ce3e518ic2cb52bbe52b102a@mail.gmail.com>
References: <c7c17cef0606080213n6ce3e518ic2cb52bbe52b102a@mail.gmail.com>
Message-ID: <4487EF69.5040406@good.ibl.fr>

 > toy.df$sign <- ifelse(toy.df$size == 0, "negative", "positive")
 > toy.df[rep(1:nrow(toy.df), ifelse(toy.df$size==0, 1, toy.df$size)),]

-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


Muhammad Subianto a ?crit :
> Dear all,
> I want to expand only one of variable in data frame and the others
> variable will be following with the expand variable. Here my toy example:
> 
> toy.df <- data.frame(size=c(3,1,2,0,3,5,1,0), group=LETTERS[1:8],
> 
> country=c("Germany","England","Argentina","Mexico","Italy","Brazil","France","Spain"),
>                      w=rep(0,8), d=rep(0,8), l=rep(0,8))
> toy.df
> 
> The size variable is the number of expand and signed by positive but size
> with 0 (zero) must be signed by negative.
> The result something like:
> 
>   size group   country w d l      sign
>      3     A   Germany 0 0 0  positive
>      3     A   Germany 0 0 0  positive
>      3     A   Germany 0 0 0  positive
>      1     B   England 0 0 0  positive
>      2     C Argentina 0 0 0  positive
>      2     C Argentina 0 0 0  positive
>      0     D    Mexico 0 0 0  negative
>      3     E     Italy 0 0 0  positive
>      3     E     Italy 0 0 0  positive
>      3     E     Italy 0 0 0  positive
>      5     F    Brazil 0 0 0  positive
>      5     F    Brazil 0 0 0  positive
>      5     F    Brazil 0 0 0  positive
>      5     F    Brazil 0 0 0  positive
>      5     F    Brazil 0 0 0  positive
>      1     G    France 0 0 0  positive
>      0     H     Spain 0 0 0  negative
> 
> I  would be very happy if anyone could help me.
> Thank you very much in advance.
> 
> Kindly regards, Muhammad Subianto
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From bernarduse1 at yahoo.fr  Thu Jun  8 11:43:17 2006
From: bernarduse1 at yahoo.fr (Marc Bernard)
Date: Thu, 8 Jun 2006 11:43:17 +0200 (CEST)
Subject: [R] panel.abline and xyplot
Message-ID: <20060608094317.62210.qmail@web25803.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060608/e58a5333/attachment.pl 

From ja at space.mit.edu  Thu Jun  8 11:46:19 2006
From: ja at space.mit.edu (Justin Ashmall)
Date: Thu, 8 Jun 2006 10:46:19 +0100 (BST)
Subject: [R] Re-binning histogram data
Message-ID: <Pine.LNX.4.62.0606081026370.2883@knib.net>

Hi,

Short Version:
Is there a function to re-bin a histogram to new, broader bins?

Long version: I'm trying to create a histogram, however my input-data is 
itself in the form of a fine-grained histogram, i.e. numbers of counts 
in regular one-second bins. I want to produce a histogram of, say, 
10-minute bins (though possibly irregular bins also).

I suppose I could re-create a data set as expected by the hist() function 
(i.e. if time t=3600 has 6 counts, add six entries of 3600 to a list) 
however this seems neither elegant nor efficient (though I'd be pleased to 
be mistaken!). I could then re-create a histogram as normal.

I guessing there's a better solution however! Apologies if this is a basic 
question - I'm rather new to R and trying to get up to speed.

Regards,

Justin


From petr.pikal at precheza.cz  Thu Jun  8 11:56:23 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 08 Jun 2006 11:56:23 +0200
Subject: [R] Re-binning histogram data
In-Reply-To: <Pine.LNX.4.62.0606081026370.2883@knib.net>
Message-ID: <44881067.15719.73882C@localhost>

Hi

try truehist from MASS package and look for argument breaks or h.

HTH
Petr




On 8 Jun 2006 at 10:46, Justin Ashmall wrote:

Date sent:      	Thu, 8 Jun 2006 10:46:19 +0100 (BST)
From:           	Justin Ashmall <ja at space.mit.edu>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Re-binning histogram data

> Hi,
> 
> Short Version:
> Is there a function to re-bin a histogram to new, broader bins?
> 
> Long version: I'm trying to create a histogram, however my input-data
> is itself in the form of a fine-grained histogram, i.e. numbers of
> counts in regular one-second bins. I want to produce a histogram of,
> say, 10-minute bins (though possibly irregular bins also).
> 
> I suppose I could re-create a data set as expected by the hist()
> function (i.e. if time t=3600 has 6 counts, add six entries of 3600 to
> a list) however this seems neither elegant nor efficient (though I'd
> be pleased to be mistaken!). I could then re-create a histogram as
> normal.
> 
> I guessing there's a better solution however! Apologies if this is a
> basic question - I'm rather new to R and trying to get up to speed.
> 
> Regards,
> 
> Justin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From hans.gardfjell at emg.umu.se  Thu Jun  8 12:00:27 2006
From: hans.gardfjell at emg.umu.se (Hans Gardfjell)
Date: Thu, 08 Jun 2006 12:00:27 +0200
Subject: [R] Plotting female and male signs
Message-ID: <4487F53B.9070609@emg.umu.se>

Dear R-users,

Just like other users (as seen from previous posts on the list), I would 
like to use female and male signs in plots. I found B. Ripley's post 
about using Unicode characters. However, it doesn't works for me.

 > text(locator(1),"\u2640")  produces the following error:
Error: invalid \uxxxx sequence

But I can specify other Unicode characters as long I don't exceed 00FF, 
so this works

 > text(locator(1),"\u00FF") or
 > text(locator(1),"\u00E6") and also without preceeding 00
 > text(locator(1),"\uE6")

Can someone give me a hint?
I'm using a Swedish locale on WindowsXP and my R version is 2.3.1 
Patched (2006-06-04 r38279)

Thanks,

Hans Gardfjell

-- 

*********************************
Hans Gardfjell
Ecology and Environmental Science
Ume? University
90187 Ume?, Sweden
email: hans.gardfjell at emg.umu.se
phone:  +46 907865267
mobile: +46 705984464


From ja at space.mit.edu  Thu Jun  8 12:35:46 2006
From: ja at space.mit.edu (Justin Ashmall)
Date: Thu, 8 Jun 2006 11:35:46 +0100 (BST)
Subject: [R] Re-binning histogram data
In-Reply-To: <44881067.15719.73882C@localhost>
References: <44881067.15719.73882C@localhost>
Message-ID: <Pine.LNX.4.62.0606081130560.2883@knib.net>


Thanks for the reply Petr,

It looks to me that truehist() needs a vector of data just like hist()? 
Whereas I have histogram-style input data? Am I missing something?

Cheers,

Justin



On Thu, 8 Jun 2006, Petr Pikal wrote:

> Hi
>
> try truehist from MASS package and look for argument breaks or h.
>
> HTH
> Petr
>
>
>
>
> On 8 Jun 2006 at 10:46, Justin Ashmall wrote:
>
> Date sent:      	Thu, 8 Jun 2006 10:46:19 +0100 (BST)
> From:           	Justin Ashmall <ja at space.mit.edu>
> To:             	r-help at stat.math.ethz.ch
> Subject:        	[R] Re-binning histogram data
>
>> Hi,
>>
>> Short Version:
>> Is there a function to re-bin a histogram to new, broader bins?
>>
>> Long version: I'm trying to create a histogram, however my input-data
>> is itself in the form of a fine-grained histogram, i.e. numbers of
>> counts in regular one-second bins. I want to produce a histogram of,
>> say, 10-minute bins (though possibly irregular bins also).
>>
>> I suppose I could re-create a data set as expected by the hist()
>> function (i.e. if time t=3600 has 6 counts, add six entries of 3600 to
>> a list) however this seems neither elegant nor efficient (though I'd
>> be pleased to be mistaken!). I could then re-create a histogram as
>> normal.
>>
>> I guessing there's a better solution however! Apologies if this is a
>> basic question - I'm rather new to R and trying to get up to speed.
>>
>> Regards,
>>
>> Justin
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
> Petr Pikal
> petr.pikal at precheza.cz
>
>


From Torsten.Hothorn at rzmail.uni-erlangen.de  Thu Jun  8 12:44:39 2006
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Thu, 8 Jun 2006 12:44:39 +0200 (CEST)
Subject: [R] Plotting female and male signs
In-Reply-To: <4487F53B.9070609@emg.umu.se>
References: <4487F53B.9070609@emg.umu.se>
Message-ID: <Pine.LNX.4.64.0606081240160.5623@imbe153.imbe.med.uni-erlangen.de>



On Thu, 8 Jun 2006, Hans Gardfjell wrote:

> Dear R-users,
>
> Just like other users (as seen from previous posts on the list), I would
> like to use female and male signs in plots. I found B. Ripley's post
> about using Unicode characters. However, it doesn't works for me.
>
> > text(locator(1),"\u2640")  produces the following error:
> Error: invalid \uxxxx sequence
>
> But I can specify other Unicode characters as long I don't exceed 00FF,
> so this works
>
> > text(locator(1),"\u00FF") or
> > text(locator(1),"\u00E6") and also without preceeding 00
> > text(locator(1),"\uE6")
>
> Can someone give me a hint?
> I'm using a Swedish locale on WindowsXP and my R version is 2.3.1
> Patched (2006-06-04 r38279)

Hans,

R> edit(vignette("Ch_logistic_regression_glm", package = "HSAUR"))

(you need to install package `HSAUR') has an example, specifically the 
line

text(womensrole$education, y, ifelse(f, "\\VE", "\\MA"), vfont =
      c("serif", "plain"), cex = 1.25)

uses Hershey fonts, see ?Hershey

Torsten

>
> Thanks,
>
> Hans Gardfjell
>
> -- 
>
> *********************************
> Hans Gardfjell
> Ecology and Environmental Science
> Ume? University
> 90187 Ume?, Sweden
> email: hans.gardfjell at emg.umu.se
> phone:  +46 907865267
> mobile: +46 705984464
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

From dieter.menne at menne-biomed.de  Thu Jun  8 12:52:56 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 8 Jun 2006 10:52:56 +0000 (UTC)
Subject: [R] panel.abline and xyplot
References: <20060608094317.62210.qmail@web25803.mail.ukl.yahoo.com>
Message-ID: <loom.20060608T125039-441@post.gmane.org>

Marc Bernard <bernarduse1 <at> yahoo.fr> writes:

> 
>   I am wondering on how to use the abline.xyplot with xyplot such that I will
have different vertical lines for
> each panel. More sepcifically, suppose that the xyplot generates 4 panels
defined by the combination of
> two binary variables: X_1 and X_2. i.e. 
> 
>   xyplot(Y ~ Z | X_1*X_2, data = df) 
>   abline(v = 5)  if X_1=0 and X_2 = 0 
>   abline(v = 10)  if X_1=0 and X_2 = 1
>   abline(v = 15) if X_1=1 and X_2 = 0
>   abline(v = 31) if X_1=1 and X_2 = 1
> 
>   when I used : xyplot(Y ~ Z | X_1*X_2, data=df, panel = function(x,y)
{panel.xyplot(x,y)
> panel.abline(v=70)}) it produces the same vertical line in the four panels.
> 

I use subscripts to display separately stored array data, but I am sure there
must be a more elegant of knowing what the current group is than by accessing
its first element.

Dieter


library(lattice)
quakes$Depth = as.factor(floor(quakes$depth/100))
quakes$Mag = as.factor(floor(quakes$mag))

whereline = expand.grid(
  Mag=levels(quakes$Mag),
  Depth=levels(quakes$Depth))
whereline$line = rnorm(nrow(whereline),-30,10)

xyplot(lat ~ long | Depth*Mag, data = quakes,
  panel = function(x,y,subscripts,...) {   
    mg = quakes[subscripts[1],] 
    with (whereline, 
      panel.abline(h= whereline[Mag==mg$Mag & Depth==mg$Depth,"line"]))
    panel.xyplot(x,y,...)
  }
  )


From ripley at stats.ox.ac.uk  Thu Jun  8 13:48:28 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 8 Jun 2006 12:48:28 +0100 (BST)
Subject: [R] Plotting female and male signs
In-Reply-To: <4487F53B.9070609@emg.umu.se>
References: <4487F53B.9070609@emg.umu.se>
Message-ID: <Pine.LNX.4.64.0606081245030.18106@gannet.stats.ox.ac.uk>

On Thu, 8 Jun 2006, Hans Gardfjell wrote:

> Dear R-users,
>
> Just like other users (as seen from previous posts on the list), I would
> like to use female and male signs in plots. I found B. Ripley's post
> about using Unicode characters. However, it doesn't works for me.
>
> > text(locator(1),"\u2640")  produces the following error:
> Error: invalid \uxxxx sequence
>
> But I can specify other Unicode characters as long I don't exceed 00FF,
> so this works
>
> > text(locator(1),"\u00FF") or
> > text(locator(1),"\u00E6") and also without preceeding 00
> > text(locator(1),"\uE6")
>
> Can someone give me a hint?
> I'm using a Swedish locale on WindowsXP and my R version is 2.3.1
> Patched (2006-06-04 r38279)

Unicode characters only work if they are valid in the locale: for these, 
only in UTF-8 locales (hence not on Windows).

If you were referring to

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/63615.html

it did say

   You don't tell us your platform and the answer depends on the platform.

   Alternatively, look at the Hershey fonts, which have these and many
   other symbols (not necessarily of publication-quality though).


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From narendra at att.com  Thu Jun  8 13:59:28 2006
From: narendra at att.com (Ravi, Narendra, INFOT)
Date: Thu, 8 Jun 2006 06:59:28 -0500
Subject: [R] Problems Building R-2.3.1 on Alpha server ES40 running Tru64
	V5.1B PK#5
Message-ID: <28F05913385EAC43AF019413F674A0170F7D85EB@OCCLUST04EVS1.ugd.att.com>

R Listers,

One of the list contributors suggested I abandon attempts to build
R-1.9.1 and focus on building R-2.3.1. I believe the main set of
packages have been built properly (with some changes to the code) -
however, when building the Recommended packages, the build fails.



CODE CHANGES:
=============

1. My C compiler requires a ";" on line 589 of
..../src/main/printutils.c. Here is the diff:
# diff src/main/printutils.c.orig src/main/printutils.c
589c589
<  if (con_num>0) error("Internal error: this platform does not support
split output")
---
>  if (con_num>0) error("Internal error: this platform does not support
split output");


Build Results:
==============

The problem seems to be in building the Recommended packages. For MASS,
The actual build seems to happen in /tmp, however, the next package
expects the build somewhere else. Do I have to build the Recommended
packages? If not, how do I avoid it? I suspect the unresolved symbols
are OK, as they will get pulled in from shared libraries.


make[2]: Entering directory
`/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/src/li
brary/Recommended'
begin installing recommended package VR
* Installing *source* package 'MASS' ...
** libs
make[3]: Entering directory
`/cluster/members/member0/tmp/R.INSTALL.OVYGvZ/VR/MASS/src'
cc
-I/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/inclu
de -I/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/buil
d/include  -I/opt/gnu/include -I/opt/libjpeg/include
-I/opt/libpng/include -I/opt/zlib/include -I/opt/ncurses/include
-ieee_with_inexact    -st
d1 -c MASS.c -o MASS.o
cc
-I/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/inclu
de -I/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/buil
d/include  -I/opt/gnu/include -I/opt/libjpeg/include
-I/opt/libpng/include -I/opt/zlib/include -I/opt/ncurses/include
-ieee_with_inexact    -st
d1 -c lqs.c -o lqs.o
cc -shared -L/opt/gnu/lib -L/opt/libjpeg/lib -L/opt/libpng/lib
-L/opt/ncurses/lib -L/opt/zlib/lib -lncurses -ljpeg -lpng -lz -rpath
/opt/libpng
/lib:/opt/libjpeg/lib:/opt/ncurses/lib:/opt/gnu/lib:/usr/shlib:/opt/zlib
/lib:/opt/R/lib -o MASS.so MASS.o lqs.o
ld:
Warning: Unresolved:
exp
fabs
pow
sqrt
isnan
Rf_error
R_alloc
Rprintf
R_chk_calloc
R_chk_free
vmmin
R_registerRoutines
R_useDynamicSymbols
log
GetRNGstate
PutRNGstate
unif_rand
R_rsort
Rf_rPsort
R_CheckUserInterrupt
dqrsl_
dqrdc2_
make[3]: Leaving directory
`/cluster/members/member0/tmp/R.INSTALL.OVYGvZ/VR/MASS/src'
chmod: cannot access
`/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/librar
y/MASS/libs//*': No such file or directory
** R
** data
chmod: cannot access
`/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/librar
y/MASS/data/*': No such file or directory
**  moving datasets to lazyload DB

 *** caught segfault ***
address 100000008, cause 'memory not mapped'

Traceback:
 1: print.default(f0)
 2: print(f0)
 3: tools:::data2LazyLoadDB("MASS",
"/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/librar
y")
aborting ...
/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/bin/INS
TALL: 367631 Memory fault - core dumped
ERROR: lazydata failed for package 'MASS'
** Removing
'/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/librar
y/MASS'
** Removing
'/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/librar
y/class'
** Removing
'/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/librar
y/nnet'
** Removing
'/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/librar
y/spatial'
make[2]: *** [VR.ts] Error 1
make[2]: Leaving directory
`/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/src/li
brary/Recommended'
make[1]: *** [recommended-packages] Error 2
make[1]: Leaving directory
`/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/src/li
brary/Recommended'
make: *** [stamp-recommended] Error 2

Narendra Ravi


From anand.gavai at wur.nl  Thu Jun  8 15:07:59 2006
From: anand.gavai at wur.nl (Anand Gavai)
Date: Thu, 8 Jun 2006 15:07:59 +0200
Subject: [R] installation problems  Rgraphviz
Message-ID: <000001c68afc$93d0e390$0434e089@laboratop4nr5s>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060608/cd6dc2ba/attachment.pl 

From aarppe at ling.helsinki.fi  Thu Jun  8 15:10:08 2006
From: aarppe at ling.helsinki.fi (Antti Arppe)
Date: Thu, 8 Jun 2006 16:10:08 +0300 (EEST)
Subject: [R] Reading in a table with ISO-latin1 encoding in MacOS-X (Intel)
Message-ID: <Pine.LNX.4.62.0606081515310.31426@venus.ling.helsinki.fi>

Dear colleages in R,

I have earlier been working with R in Linux, where reading in a table 
containing Scandinavian letters ("?", "?", and "?") in the header as 
part of variable names has not caused any problem whatsoever.

However, when trying to do the same in R running on new MacOS-X (with 
an Intel processor) with the same original text table does not seem to 
work whichever way I try. Following the recommendations on the R site 
and using the 'file' function to set the encoding breaks down at the 
first encounter with a Scandinavian character:

THINK <- read.table(file("R_data/hs+sfnet.T.060505.tbl4", 
encoding="latin1"),header=TRUE)
Warning messages:
1: invalid input found on input connection 
'R_data/hs+sfnet.T.060505.tbl4'
2: incomplete final line found by readTableHeader on 
'R_data/hs+sfnet.T.060505.tbl4'

A sample exemplifying such characters as variable labels is below 
(for which the behavior of R in Mac is the same as for the larger file 
referred to above):.

    ajatella mietti? pohtia
1     FALSE   FALSE   TRUE
2     FALSE   FALSE  FALSE
3     FALSE    TRUE  FALSE
4     FALSE    TRUE  FALSE
5      TRUE   FALSE  FALSE
6      TRUE   FALSE  FALSE
7     FALSE   FALSE  FALSE
8     FALSE    TRUE  FALSE
9     FALSE    TRUE  FALSE
10    FALSE   FALSE  FALSE

Converting the the file from ISO-latin-1 to UTF8 (with Mac's TextEdit 
application)allows the file to be read in in its entirety, but still 
the Scandinavian character in the heading is coerced to a period '.', 
or two, in fact (i.e. 'mietti?' -> 'miett..')

Have I possibly misunderstood how the 'file' function should be used 
in conjunction with 'read.table', or might the problem with 
latin1-to-utf conversion be somewhere else?

Appreciating any help on this matter,

-- 
======================================================================
Antti Arppe - Master of Science (Engineering)
Researcher & doctoral student (Linguistics)
E-mail: antti.arppe at helsinki.fi
WWW: http://www.ling.helsinki.fi/~aarppe

From ripley at stats.ox.ac.uk  Thu Jun  8 15:13:37 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 8 Jun 2006 14:13:37 +0100 (BST)
Subject: [R] Problems Building R-2.3.1 on Alpha server ES40 running
 Tru64 V5.1B PK#5
In-Reply-To: <28F05913385EAC43AF019413F674A0170F7D85EB@OCCLUST04EVS1.ugd.att.com>
References: <28F05913385EAC43AF019413F674A0170F7D85EB@OCCLUST04EVS1.ugd.att.com>
Message-ID: <Pine.LNX.4.64.0606081402160.18912@gannet.stats.ox.ac.uk>

On Thu, 8 Jun 2006, Ravi, Narendra, INFOT wrote:

> R Listers,
>
> One of the list contributors suggested I abandon attempts to build
> R-1.9.1 and focus on building R-2.3.1. I believe the main set of
> packages have been built properly (with some changes to the code) -
> however, when building the Recommended packages, the build fails.
>
>
>
> CODE CHANGES:
> =============
>
> 1. My C compiler requires a ";" on line 589 of
> ..../src/main/printutils.c. Here is the diff:
> # diff src/main/printutils.c.orig src/main/printutils.c
> 589c589
> <  if (con_num>0) error("Internal error: this platform does not support
> split output")
> ---
>>  if (con_num>0) error("Internal error: this platform does not support
> split output");

That is already changed in R-devel and the most recent R-patched.  (Some 
platforms which do not have va_copy required more extensive changes.)

> Build Results:
> ==============
>
> The problem seems to be in building the Recommended packages. For MASS,
> The actual build seems to happen in /tmp, however, the next package
> expects the build somewhere else. Do I have to build the Recommended
> packages? If not, how do I avoid it? I suspect the unresolved symbols
> are OK, as they will get pulled in from shared libraries.

It looks like *warnings* are causing your linker to fail.  If so, you need 
to change the linker flags to allow this to complete.

You can configure and build without recommended packages, but likely the 
problem will arise when you try most additional packages.

>
> make[2]: Entering directory
> `/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/src/li
> brary/Recommended'
> begin installing recommended package VR
> * Installing *source* package 'MASS' ...
> ** libs
> make[3]: Entering directory
> `/cluster/members/member0/tmp/R.INSTALL.OVYGvZ/VR/MASS/src'
> cc
> -I/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/inclu
> de -I/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/buil
> d/include  -I/opt/gnu/include -I/opt/libjpeg/include
> -I/opt/libpng/include -I/opt/zlib/include -I/opt/ncurses/include
> -ieee_with_inexact    -st
> d1 -c MASS.c -o MASS.o
> cc
> -I/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/inclu
> de -I/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/buil
> d/include  -I/opt/gnu/include -I/opt/libjpeg/include
> -I/opt/libpng/include -I/opt/zlib/include -I/opt/ncurses/include
> -ieee_with_inexact    -st
> d1 -c lqs.c -o lqs.o
> cc -shared -L/opt/gnu/lib -L/opt/libjpeg/lib -L/opt/libpng/lib
> -L/opt/ncurses/lib -L/opt/zlib/lib -lncurses -ljpeg -lpng -lz -rpath
> /opt/libpng
> /lib:/opt/libjpeg/lib:/opt/ncurses/lib:/opt/gnu/lib:/usr/shlib:/opt/zlib
> /lib:/opt/R/lib -o MASS.so MASS.o lqs.o
> ld:
> Warning: Unresolved:
> exp
> fabs
> pow
> sqrt
> isnan
> Rf_error
> R_alloc
> Rprintf
> R_chk_calloc
> R_chk_free
> vmmin
> R_registerRoutines
> R_useDynamicSymbols
> log
> GetRNGstate
> PutRNGstate
> unif_rand
> R_rsort
> Rf_rPsort
> R_CheckUserInterrupt
> dqrsl_
> dqrdc2_
> make[3]: Leaving directory
> `/cluster/members/member0/tmp/R.INSTALL.OVYGvZ/VR/MASS/src'
> chmod: cannot access
> `/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/librar
> y/MASS/libs//*': No such file or directory
> ** R
> ** data
> chmod: cannot access
> `/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/librar
> y/MASS/data/*': No such file or directory
> **  moving datasets to lazyload DB
>
> *** caught segfault ***
> address 100000008, cause 'memory not mapped'
>
> Traceback:
> 1: print.default(f0)
> 2: print(f0)
> 3: tools:::data2LazyLoadDB("MASS",
> "/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/librar
> y")
> aborting ...
> /usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/bin/INS
> TALL: 367631 Memory fault - core dumped
> ERROR: lazydata failed for package 'MASS'
> ** Removing
> '/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/librar
> y/MASS'
> ** Removing
> '/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/librar
> y/class'
> ** Removing
> '/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/librar
> y/nnet'
> ** Removing
> '/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/librar
> y/spatial'
> make[2]: *** [VR.ts] Error 1
> make[2]: Leaving directory
> `/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/src/li
> brary/Recommended'
> make[1]: *** [recommended-packages] Error 2
> make[1]: Leaving directory
> `/usr/ruby-deploy/swkits/RUBY/SOFTWARE/3rdPartySW/R/R-2.3.1/build/src/li
> brary/Recommended'
> make: *** [stamp-recommended] Error 2
>
> Narendra Ravi
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From alex_restrepo at hotmail.com  Thu Jun  8 15:14:29 2006
From: alex_restrepo at hotmail.com (Alex Restrepo)
Date: Thu, 08 Jun 2006 08:14:29 -0500
Subject: [R] JGR :: Can't Run Graphics Demo
In-Reply-To: <Pine.LNX.4.64.0606081245030.18106@gannet.stats.ox.ac.uk>
Message-ID: <BAY106-F2027291092E462780DAE3A988B0@phx.gbl>

Hello:

I am using JGR 1.4, the Java GUI for R and I am trying to run the demo for 
graphics using the command:

    demo(graphics)

I keep on getting the following errror:

    Error in get(getOption("device"))() : unable to start device JavaGD


FYI, the version of Java I am using is:

     1.5.0_07-b03


Do I need to use a different version of Java?  Does anyone have any 
recommendations/suggestions?


Many Thanks In Advance:

Alex


From petr.pikal at precheza.cz  Thu Jun  8 15:17:29 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 08 Jun 2006 15:17:29 +0200
Subject: [R] Re-binning histogram data
In-Reply-To: <Pine.LNX.4.62.0606081130560.2883@knib.net>
References: <44881067.15719.73882C@localhost>
Message-ID: <44883F89.11852.12BB1FC@localhost>



On 8 Jun 2006 at 11:35, Justin Ashmall wrote:

Date sent:      	Thu, 8 Jun 2006 11:35:46 +0100 (BST)
From:           	Justin Ashmall <ja at space.mit.edu>
To:             	Petr Pikal <petr.pikal at precheza.cz>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] Re-binning histogram data

> 
> Thanks for the reply Petr,
> 
> It looks to me that truehist() needs a vector of data just like
> hist()? Whereas I have histogram-style input data? Am I missing
> something?

Well, maybe you could use barplot. Or as you suggested recreate the 
original vector and call hist or truehist with other bins.

> hhh<-hist(rnorm(1000))
> barplot(tapply(hhh$counts, c(rep(1:7,each=2),7), sum))
> tapply(hhh$mids, c(rep(1:7,each=2),7), mean)
    1     2     3     4     5     6     7 
-3.00 -2.00 -1.00  0.00  1.00  2.00  3.25 
> hhh1<-rep(hhh$mids,hhh$counts)
> plot(hhh, freq=F)
> lines(density(hhh1))
>

HTH
Petr






> 
> Cheers,
> 
> Justin
> 
> 
> 
> On Thu, 8 Jun 2006, Petr Pikal wrote:
> 
> > Hi
> >
> > try truehist from MASS package and look for argument breaks or h.
> >
> > HTH
> > Petr
> >
> >
> >
> >
> > On 8 Jun 2006 at 10:46, Justin Ashmall wrote:
> >
> > Date sent:      	Thu, 8 Jun 2006 10:46:19 +0100 (BST)
> > From:           	Justin Ashmall <ja at space.mit.edu>
> > To:             	r-help at stat.math.ethz.ch
> > Subject:        	[R] Re-binning histogram data
> >
> >> Hi,
> >>
> >> Short Version:
> >> Is there a function to re-bin a histogram to new, broader bins?
> >>
> >> Long version: I'm trying to create a histogram, however my
> >> input-data is itself in the form of a fine-grained histogram, i.e.
> >> numbers of counts in regular one-second bins. I want to produce a
> >> histogram of, say, 10-minute bins (though possibly irregular bins
> >> also).
> >>
> >> I suppose I could re-create a data set as expected by the hist()
> >> function (i.e. if time t=3600 has 6 counts, add six entries of 3600
> >> to a list) however this seems neither elegant nor efficient (though
> >> I'd be pleased to be mistaken!). I could then re-create a histogram
> >> as normal.
> >>
> >> I guessing there's a better solution however! Apologies if this is
> >> a basic question - I'm rather new to R and trying to get up to
> >> speed.
> >>
> >> Regards,
> >>
> >> Justin
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >
> > Petr Pikal
> > petr.pikal at precheza.cz
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From charles-r-nospam at plessy.org  Thu Jun  8 15:31:48 2006
From: charles-r-nospam at plessy.org (Charles Plessy)
Date: Thu, 8 Jun 2006 22:31:48 +0900
Subject: [R] Reading in a table with ISO-latin1 encoding in MacOS-X
	(Intel)
In-Reply-To: <Pine.LNX.4.62.0606081515310.31426@venus.ling.helsinki.fi>
References: <Pine.LNX.4.62.0606081515310.31426@venus.ling.helsinki.fi>
Message-ID: <20060608133148.GB21327@kunpuu.plessy.org>

Le Thu, Jun 08, 2006 at 04:10:08PM +0300, Antti Arppe a ?crit :
> 
> Converting the the file from ISO-latin-1 to UTF8 (with Mac's TextEdit 
> application)allows the file to be read in in its entirety, but still 
> the Scandinavian character in the heading is coerced to a period '.', 
> or two, in fact (i.e. 'mietti?' -> 'miett..')

Dear Antti,

I may be wrong, but the unicode accented latin letters are not encoded
the same on linux and macintosh. On linux, ? is ?, but on Macintosh, it
is "+a (please read the quotes as if there were an umlaut). Did you try
to just retype the headers with a macintosh text editor?

Good luck!

-- 
Charles Plessy
Wako, Saitama, Japan


From ronggui.huang at gmail.com  Thu Jun  8 15:47:07 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Thu, 8 Jun 2006 21:47:07 +0800
Subject: [R] JGR :: Can't Run Graphics Demo
In-Reply-To: <BAY106-F2027291092E462780DAE3A988B0@phx.gbl>
References: <Pine.LNX.4.64.0606081245030.18106@gannet.stats.ox.ac.uk>
	<BAY106-F2027291092E462780DAE3A988B0@phx.gbl>
Message-ID: <38b9f0350606080647i71c18cf6ub45af00f8e7fa7af@mail.gmail.com>

Have you installed the following packages?They are javaGD,rJava,iplots,iWidgets?

It seems that you have installed the javaGD package.

2006/6/8, Alex Restrepo <alex_restrepo at hotmail.com>:
> Hello:
>
> I am using JGR 1.4, the Java GUI for R and I am trying to run the demo for
> graphics using the command:
>
>     demo(graphics)
>
> I keep on getting the following errror:
>
>     Error in get(getOption("device"))() : unable to start device JavaGD
>
>
> FYI, the version of Java I am using is:
>
>      1.5.0_07-b03
>
>
> Do I need to use a different version of Java?  Does anyone have any
> recommendations/suggestions?
>
>
> Many Thanks In Advance:
>
> Alex
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
??????
Deparment of Sociology
Fudan University


From franz.quehenberger at meduni-graz.at  Thu Jun  8 15:51:11 2006
From: franz.quehenberger at meduni-graz.at (Franz Quehenberger)
Date: Thu, 08 Jun 2006 15:51:11 +0200
Subject: [R] Rgraphviz overlap of nodes and edges
Message-ID: <44882B4F.9030707@meduni-graz.at>

Hi,
I plot graphs on small  diagrams. With colored  or thick edge lines then 
the nodes and the edges overlap. The edge line overwrites the nodes 
which looks ugly.

What can be done about that? I already tried to change the sep attribute 
with no effect.

Thanks,
Franz Quehenberger

V <- letters[1:10]
M <- 1:4
g1 <- randomGraph(V, M, .2)
z <- getDefaultAttrs()
z$edge$color="red"
z$edge$weight=5
plot(g1,"neato",attrs=z)


From p.dalgaard at biostat.ku.dk  Thu Jun  8 16:06:38 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Jun 2006 16:06:38 +0200
Subject: [R] Reading in a table with ISO-latin1 encoding in MacOS-X
	(Intel)
In-Reply-To: <Pine.LNX.4.62.0606081515310.31426@venus.ling.helsinki.fi>
References: <Pine.LNX.4.62.0606081515310.31426@venus.ling.helsinki.fi>
Message-ID: <x2ac8nwxld.fsf@turmalin.kubism.ku.dk>

Antti Arppe <aarppe at ling.helsinki.fi> writes:

> Converting the the file from ISO-latin-1 to UTF8 (with Mac's TextEdit
> application)allows the file to be read in in its entirety, but still
> the Scandinavian character in the heading is coerced to a period '.',
> or two, in fact (i.e. 'mietti?' -> 'miett..')

I think you probably need check.names=FALSE. (Presumably, you cannot
have Finnish characters in variable names either on the Mac?)
 
> Have I possibly misunderstood how the 'file' function should be used
> in conjunction with 'read.table', or might the problem with
> latin1-to-utf conversion be somewhere else?

Not really, text encodings are just a pain. The blame for this fact
can be shifted in various directions, but it doesn't really help...
(My personal angle is that ISO-8859 was terribly shortsighted, and
stuck in a "Western Europe" mindset. As soon as the iron curtain
disappeared and we started to deal with people from Slavic countries,
the weakness was revealed.)

The basic structure looks OK, and works for me on Linux:

> read.table(file("xx.data",encoding="latin1"),header=TRUE)
  ?h b?h
1  1   2

so one can only guess that you have a local or Mac-specific setup
issue. 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ripley at stats.ox.ac.uk  Thu Jun  8 16:17:23 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 8 Jun 2006 15:17:23 +0100 (BST)
Subject: [R] Reading in a table with ISO-latin1 encoding in MacOS-X
 (Intel)
In-Reply-To: <Pine.LNX.4.62.0606081515310.31426@venus.ling.helsinki.fi>
References: <Pine.LNX.4.62.0606081515310.31426@venus.ling.helsinki.fi>
Message-ID: <Pine.LNX.4.64.0606081500360.1362@gannet.stats.ox.ac.uk>

You are using this as intended, although your email message came in latin9 
not latin1, which does not affect your examples.  Have you actually 
checked (e.g. via a hex dump) that the file is in latin1?

I assume that if you converted the file to UTF-8 you then used

read.table(R_data/hs+sfnet.T.060505.tbl4", header=TRUE)

If so, you need to investigate the locale in use, as which letters are 
valid depends on the locale: on Linux UTF-8 locales all letters in all 
languages are valid in R names, but that is not necessarily the MacOS 
interpretation.  (Invalid characters in names will be converted to ., and 
if the locale is wrong so may be the interpretation of bytes as 
characters.)

You might find more informed help on the r-sig-mac list.


On Thu, 8 Jun 2006, Antti Arppe wrote:

> Dear colleages in R,
>
> I have earlier been working with R in Linux, where reading in a table 
> containing Scandinavian letters ("?", "?", and "?") in the header as part of 
> variable names has not caused any problem whatsoever.
>
> However, when trying to do the same in R running on new MacOS-X (with an 
> Intel processor) with the same original text table does not seem to work 
> whichever way I try. Following the recommendations on the R site and using 
> the 'file' function to set the encoding breaks down at the first encounter 
> with a Scandinavian character:
>
> THINK <- read.table(file("R_data/hs+sfnet.T.060505.tbl4", 
> encoding="latin1"),header=TRUE)
> Warning messages:
> 1: invalid input found on input connection 'R_data/hs+sfnet.T.060505.tbl4'
> 2: incomplete final line found by readTableHeader on 
> 'R_data/hs+sfnet.T.060505.tbl4'
>
> A sample exemplifying such characters as variable labels is below (for which 
> the behavior of R in Mac is the same as for the larger file referred to 
> above):.
>
>   ajatella mietti? pohtia
> 1     FALSE   FALSE   TRUE
> 2     FALSE   FALSE  FALSE
> 3     FALSE    TRUE  FALSE
> 4     FALSE    TRUE  FALSE
> 5      TRUE   FALSE  FALSE
> 6      TRUE   FALSE  FALSE
> 7     FALSE   FALSE  FALSE
> 8     FALSE    TRUE  FALSE
> 9     FALSE    TRUE  FALSE
> 10    FALSE   FALSE  FALSE
>
> Converting the the file from ISO-latin-1 to UTF8 (with Mac's TextEdit 
> application)allows the file to be read in in its entirety, but still the 
> Scandinavian character in the heading is coerced to a period '.', or two, in 
> fact (i.e. 'mietti?' -> 'miett..')
>
> Have I possibly misunderstood how the 'file' function should be used in 
> conjunction with 'read.table', or might the problem with latin1-to-utf 
> conversion be somewhere else?
>
> Appreciating any help on this matter,
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From d.d.sutcliffe at durham.ac.uk  Thu Jun  8 16:44:00 2006
From: d.d.sutcliffe at durham.ac.uk (d d sutcliffe@durham ac uk)
Date: Thu, 08 Jun 2006 15:44:00 +0100
Subject: [R] Rotate numbers on the  Y axis on a multiple Boxplot chart
Message-ID: <448837B0.30200@durham.ac.uk>

Hello All,

I am  trying to format a box plot chart for a report so it matches other 
charts I have created in other software programs. My problem is that I 
need to rotate the values on the Y axis by 45 degrees, I have tried to 
use the  srt parameter but with no luck?

Does anybody have any suggestions?

Regards,

Daniel

My syntax is as follows:

boxplot(int~sortf,notch=TRUE,outline=FALSE,col="DarkSeaGreen4", ylab = 
"Length of stay in days",ylim=c(0, 100),xlab="Trust (see key)")


From mschwartz at mn.rr.com  Thu Jun  8 17:10:04 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 08 Jun 2006 10:10:04 -0500
Subject: [R] Rotate numbers on the  Y axis on a multiple Boxplot chart
In-Reply-To: <448837B0.30200@durham.ac.uk>
References: <448837B0.30200@durham.ac.uk>
Message-ID: <1149779404.7593.2.camel@localhost.localdomain>

On Thu, 2006-06-08 at 15:44 +0100, d d sutcliffe at durham ac uk wrote:
> Hello All,
> 
> I am  trying to format a box plot chart for a report so it matches other 
> charts I have created in other software programs. My problem is that I 
> need to rotate the values on the Y axis by 45 degrees, I have tried to 
> use the  srt parameter but with no luck?
> 
> Does anybody have any suggestions?
> 
> Regards,
> 
> Daniel
> 
> My syntax is as follows:
> 
> boxplot(int~sortf,notch=TRUE,outline=FALSE,col="DarkSeaGreen4", ylab = 
> "Length of stay in days",ylim=c(0, 100),xlab="Trust (see key)")

See R FAQ 7.27 How can I create rotated axis labels?

The example there is for the x axis, but the same concept applies for y.

HTH,

Marc Schwartz


From vdemart1 at tin.it  Thu Jun  8 17:03:13 2006
From: vdemart1 at tin.it (Vittorio)
Date: Thu, 8 Jun 2006 16:03:13 +0100 (GMT+01:00)
Subject: [R] Example(chron) doesn't work
Message-ID: <10bb42b233b.vdemart1@tin.it>

Under R 2.3.1 for Windows (win xp) issuing

> example(chron)

gives:

chron> dts <- dates(c("02/27/92", "02/27/92", "01/14/92", 
    
"02/28/92", "02/01/92"))
Error in dates(c("02/27/92", "02/27/92", 
"01/14/92", "02/28/92", "02/01/92")) : 
        no direct or inherited 
method for function 'dates' for this call
> dts<-dates(c("02/27/92", 
"02/27/92", "01/14/92","02/28/92", "02/01/92"),format="%m/%d/%y")
Errore in dates(c("02/27/92", "02/27/92", "01/14/92", "02/28/92", 
"02/01/92"),  : 
        no direct or inherited method for function 
'dates' for this call
>

What should I do?
Thanks
Vittorio


From ggrothendieck at gmail.com  Thu Jun  8 17:19:25 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 8 Jun 2006 11:19:25 -0400
Subject: [R] Example(chron) doesn't work
In-Reply-To: <10bb42b233b.vdemart1@tin.it>
References: <10bb42b233b.vdemart1@tin.it>
Message-ID: <971536df0606080819p47b95c38x3eeded83227f4abd@mail.gmail.com>

I can't reproduce that on 2.3.1 patched nor on 2.2.1.
For me it works.

On 6/8/06, Vittorio <vdemart1 at tin.it> wrote:
> Under R 2.3.1 for Windows (win xp) issuing
>
> > example(chron)
>
> gives:
>
> chron> dts <- dates(c("02/27/92", "02/27/92", "01/14/92",
>
> "02/28/92", "02/01/92"))
> Error in dates(c("02/27/92", "02/27/92",
> "01/14/92", "02/28/92", "02/01/92")) :
>        no direct or inherited
> method for function 'dates' for this call
> > dts<-dates(c("02/27/92",
> "02/27/92", "01/14/92","02/28/92", "02/01/92"),format="%m/%d/%y")
> Errore in dates(c("02/27/92", "02/27/92", "01/14/92", "02/28/92",
> "02/01/92"),  :
>        no direct or inherited method for function
> 'dates' for this call
> >
>
> What should I do?
> Thanks
> Vittorio
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Thu Jun  8 17:20:04 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 8 Jun 2006 11:20:04 -0400
Subject: [R] Example(chron) doesn't work
In-Reply-To: <971536df0606080819p47b95c38x3eeded83227f4abd@mail.gmail.com>
References: <10bb42b233b.vdemart1@tin.it>
	<971536df0606080819p47b95c38x3eeded83227f4abd@mail.gmail.com>
Message-ID: <971536df0606080820h5ebcdde5k7fa31527b5ab0e95@mail.gmail.com>

Sorry that was XP.

On 6/8/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> I can't reproduce that on 2.3.1 patched nor on 2.2.1.
> For me it works.
>
> On 6/8/06, Vittorio <vdemart1 at tin.it> wrote:
> > Under R 2.3.1 for Windows (win xp) issuing
> >
> > > example(chron)
> >
> > gives:
> >
> > chron> dts <- dates(c("02/27/92", "02/27/92", "01/14/92",
> >
> > "02/28/92", "02/01/92"))
> > Error in dates(c("02/27/92", "02/27/92",
> > "01/14/92", "02/28/92", "02/01/92")) :
> >        no direct or inherited
> > method for function 'dates' for this call
> > > dts<-dates(c("02/27/92",
> > "02/27/92", "01/14/92","02/28/92", "02/01/92"),format="%m/%d/%y")
> > Errore in dates(c("02/27/92", "02/27/92", "01/14/92", "02/28/92",
> > "02/01/92"),  :
> >        no direct or inherited method for function
> > 'dates' for this call
> > >
> >
> > What should I do?
> > Thanks
> > Vittorio
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>


From rafael.najmanovich at ebi.ac.uk  Thu Jun  8 17:30:14 2006
From: rafael.najmanovich at ebi.ac.uk (Rafael Najmanovich)
Date: Thu, 8 Jun 2006 16:30:14 +0100
Subject: [R] "reversed" dendogram
Message-ID: <1E04BABA-E95A-4933-A926-413962026021@ebi.ac.uk>

	Dear All,

	I am trying to find a way to plot a dendogram in reverse, that is,  
if the terminal leaves are labelled 1-10 bottom to top (or left to  
right), I would like to be able to plot it in a way such that if  
would display 10-1 bottom to top or left to right. Any idea how to  
achieve this?

	Thanks in advance,
	
	r.


Dr. Rafael Najmanovich
European Bioinformatics Institute
Wellcome Trust Genome Campus
Cambridge CB10 1SD
United Kingdom

rafael.najmanovich at ebi.ac.uk - www.ebi.ac.uk/~rafi
+44-1223-492599 (voice) +44-7786-968257(mobile) +44-1223-494468 (fax)


From azzalini at stat.unipd.it  Thu Jun  8 17:35:10 2006
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Thu, 8 Jun 2006 17:35:10 +0200
Subject: [R] Density Estimation
In-Reply-To: <BAY119-F13F2C8B631B76E6AA46B66F88A0@phx.gbl>
References: <BAY119-F13F2C8B631B76E6AA46B66F88A0@phx.gbl>
Message-ID: <20060608173510.3d1542a3.azzalini@stat.unipd.it>

On Wed, 07 Jun 2006 19:54:32 +0200, Pedro Ramirez wrote:

PR> >Not a direct answer to your question, but if you use a logspline
PR> >density estimate rather than a kernal density estimate then the
PR> >logspline package will help you and it has built in functions for
PR> >dlogspline, qlogspline, and plogspline that do the integrals for
PR> >you.
PR> >
PR> >If you want to stick with the KDE, then you could find the area
PR> >under each of the kernals for the range you are interested in
PR> >(need to work out the standard deviation used from the bandwidth,
PR> >then use pnorm for the default gaussian kernal), then just sum
PR> >the individual areas.
PR> >
PR> >Hope this helps,
PR> 
PR> Thanks a lot for your quick help! I think I will follow your first
PR> 
PR> suggestion (logspline
PR> density estimation) instead of summing over the kernel areas
PR> because at the boundaries of the range truncated kernel areas can
PR> occur, so I think it is easier to do it with logsplines. Thanks
PR> again for your help!!
PR> 
PR> Pedro
PR> 
PR> 

Besides the computational aspect, there is a statistical one:
the optimal choice of bandwidth for estimating the density function 
is not optimal (and possibly not even jsut sensible) for estimating
the distribution function, and the stated problem is equivalent to
estimation of the distribution function. 

In mathematical terms the optimal bandwith for density estimation
decreases at rate n^{-1/5}, while the one for distribution function 
decreases at rate n^{-1/3}, if n is the sample size. In practical terms, 
one must choose an appreciably smaller bandwidth in the second case 
than in the first one.

best wishes,

Adelchi 


PR> 
PR> >
PR> >--
PR> >Gregory (Greg) L. Snow Ph.D.
PR> >Statistical Data Center
PR> >Intermountain Healthcare
PR> >greg.snow a intermountainmail.org
PR> >(801) 408-8111
PR> >
PR> >
PR> >-----Original Message-----
PR> >From: r-help-bounces a stat.math.ethz.ch
PR> >[mailto:r-help-bounces a stat.math.ethz.ch] On Behalf Of Pedro
PR> >Ramirez Sent: Wednesday, June 07, 2006 11:00 AM
PR> >To: r-help a stat.math.ethz.ch
PR> >Subject: [R] Density Estimation
PR> >
PR> >Dear R-list,
PR> >
PR> >I have made a simple kernel density estimation by
PR> >
PR> >x <- c(2,1,3,2,3,0,4,5,10,11,12,11,10)
PR> >kde <- density(x,n=100)
PR> >
PR> >Now I would like to know the estimated probability that a new
PR> >observation falls into the interval 0<x<3.
PR> >
PR> >How can I integrate over the corresponding interval?
PR> >In several R-packages for kernel density estimation I did not
PR> >found a corresponding function. I could apply Simpson's Rule for
PR> >integrating, but perhaps somebody knows a better solution.
PR> >
PR> >Thanks a lot for help!
PR> >
PR> >Pedro
PR> >
PR> >_________
PR> >
PR> >______________________________________________
PR> >R-help a stat.math.ethz.ch mailing list
PR> >https://stat.ethz.ch/mailman/listinfo/r-help
PR> >PLEASE do read the posting guide!
PR> >http://www.R-project.org/posting-guide.html
PR> >
PR> 
PR> ______________________________________________
PR> R-help a stat.math.ethz.ch mailing list
PR> https://stat.ethz.ch/mailman/listinfo/r-help
PR> PLEASE do read the posting guide!
PR> http://www.R-project.org/posting-guide.html
PR>


From gunter.berton at gene.com  Thu Jun  8 17:51:35 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 8 Jun 2006 08:51:35 -0700
Subject: [R] Re-binning histogram data
In-Reply-To: <44883F89.11852.12BB1FC@localhost>
Message-ID: <001c01c68b13$6c787170$295afea9@gne.windows.gene.com>

I would argue that histograms are outdated relics and that density plots
(whatever your favorite flavor is) should **always** be used instead these
days.

In this vein, I would appreciate critical rejoinders (public or private) to
the following proposition: Given modern computer power and software like R
on multi ghz machines, statistical and graphical relics of the pre-computer
era (like histograms, low resolution printer-type plots, and perhaps even
method of moments EMS calculations) should be abandoned in favor of superior
but perhaps computation-intensive alternatives (like density plots, high
resolution plots, and likelihood or resampling or Bayes based methods). 

NB: Please -- no pleadings that new methods would be mystifying to the
non-cogniscenti. Following that to its logical conclusion would mean that
we'd all have to give up our TV remotes and cell phones, and what kind of
world would that be?! :-)

-- Bert Gunter

  

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Petr Pikal
> Sent: Thursday, June 08, 2006 6:17 AM
> To: Justin Ashmall; r-help at stat.math.ethz.ch
> Subject: Re: [R] Re-binning histogram data
> 
> 
> 
> On 8 Jun 2006 at 11:35, Justin Ashmall wrote:
> 
> Date sent:      	Thu, 8 Jun 2006 11:35:46 +0100 (BST)
> From:           	Justin Ashmall <ja at space.mit.edu>
> To:             	Petr Pikal <petr.pikal at precheza.cz>
> Copies to:      	r-help at stat.math.ethz.ch
> Subject:        	Re: [R] Re-binning histogram data
> 
> > 
> > Thanks for the reply Petr,
> > 
> > It looks to me that truehist() needs a vector of data just like
> > hist()? Whereas I have histogram-style input data? Am I missing
> > something?
> 
> Well, maybe you could use barplot. Or as you suggested recreate the 
> original vector and call hist or truehist with other bins.
> 
> > hhh<-hist(rnorm(1000))
> > barplot(tapply(hhh$counts, c(rep(1:7,each=2),7), sum))
> > tapply(hhh$mids, c(rep(1:7,each=2),7), mean)
>     1     2     3     4     5     6     7 
> -3.00 -2.00 -1.00  0.00  1.00  2.00  3.25 
> > hhh1<-rep(hhh$mids,hhh$counts)
> > plot(hhh, freq=F)
> > lines(density(hhh1))
> >
> 
> HTH
> Petr
> 
> 
> 
> 
> 
> 
> > 
> > Cheers,
> > 
> > Justin
> > 
> > 
> > 
> > On Thu, 8 Jun 2006, Petr Pikal wrote:
> > 
> > > Hi
> > >
> > > try truehist from MASS package and look for argument breaks or h.
> > >
> > > HTH
> > > Petr
> > >
> > >
> > >
> > >
> > > On 8 Jun 2006 at 10:46, Justin Ashmall wrote:
> > >
> > > Date sent:      	Thu, 8 Jun 2006 10:46:19 +0100 (BST)
> > > From:           	Justin Ashmall <ja at space.mit.edu>
> > > To:             	r-help at stat.math.ethz.ch
> > > Subject:        	[R] Re-binning histogram data
> > >
> > >> Hi,
> > >>
> > >> Short Version:
> > >> Is there a function to re-bin a histogram to new, broader bins?
> > >>
> > >> Long version: I'm trying to create a histogram, however my
> > >> input-data is itself in the form of a fine-grained 
> histogram, i.e.
> > >> numbers of counts in regular one-second bins. I want to produce a
> > >> histogram of, say, 10-minute bins (though possibly irregular bins
> > >> also).
> > >>
> > >> I suppose I could re-create a data set as expected by the hist()
> > >> function (i.e. if time t=3600 has 6 counts, add six 
> entries of 3600
> > >> to a list) however this seems neither elegant nor 
> efficient (though
> > >> I'd be pleased to be mistaken!). I could then re-create 
> a histogram
> > >> as normal.
> > >>
> > >> I guessing there's a better solution however! Apologies 
> if this is
> > >> a basic question - I'm rather new to R and trying to get up to
> > >> speed.
> > >>
> > >> Regards,
> > >>
> > >> Justin
> > >>
> > >> ______________________________________________
> > >> R-help at stat.math.ethz.ch mailing list
> > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> PLEASE do read the posting guide!
> > >> http://www.R-project.org/posting-guide.html
> > >
> > > Petr Pikal
> > > petr.pikal at precheza.cz
> > >
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From rafael.najmanovich at ebi.ac.uk  Thu Jun  8 17:58:28 2006
From: rafael.najmanovich at ebi.ac.uk (Rafael Najmanovich)
Date: Thu, 8 Jun 2006 16:58:28 +0100
Subject: [R] "reversed" dendogram
In-Reply-To: <1E04BABA-E95A-4933-A926-413962026021@ebi.ac.uk>
References: <1E04BABA-E95A-4933-A926-413962026021@ebi.ac.uk>
Message-ID: <525C47C5-5722-4FA2-AC24-07CB4AA7A0D0@ebi.ac.uk>

	In order to plot a reversed a dendrogram, simply use rev().
	r.
	
On 8 Jun 2006, at 16:30, Rafael Najmanovich wrote:

> 	Dear All,
>
> 	I am trying to find a way to plot a dendogram in reverse, that is,


Dr. Rafael Najmanovich
European Bioinformatics Institute
Wellcome Trust Genome Campus
Cambridge CB10 1SD
United Kingdom

rafael.najmanovich at ebi.ac.uk - www.ebi.ac.uk/~rafi
+44-1223-492599 (voice) +44-7786-968257(mobile) +44-1223-494468 (fax)


From Manuel.A.Morales at williams.edu  Thu Jun  8 17:54:57 2006
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Thu, 08 Jun 2006 11:54:57 -0400
Subject: [R] Using data=x or subset=y in user-defined functions
In-Reply-To: <Pine.LNX.4.64.0606072135350.8690@gannet.stats.ox.ac.uk>
References: <1149710497.14236.11.camel@solidago.localdomain>
	<Pine.LNX.4.64.0606072135350.8690@gannet.stats.ox.ac.uk>
Message-ID: <1149782097.6491.6.camel@solidago.localdomain>

On Wed, 2006-06-07 at 21:36 +0100, Prof Brian Ripley wrote:
> I suggest you investigate with().

Thanks! Just to be sure, the following will do what I want?

eg.function <- function(x, data=NULL, subset=NULL, ...) {
    with(if(is.null(subset)) data else subset(data,subset), {
 
    ...

    })
}

> On Wed, 7 Jun 2006, Manuel Morales wrote:
> 
> > Dear list members,
> >
> > In some of my functions, I attach the data internally to allow subset
> > commands or to specify a data frame. This works well except for cases
> > where there is a "masking" conflict (which returns a warning). I see
> > some alternative listed in ?attach, but I'm not sure which of them do
> > what I'd like. Any suggestions?
> >
> > Below is how I've been setting up my functions:
> >
> > 
> >
> > # Set up environment
> > on.exit(detach(data))
> > attach(data)
> > if(!is.null(subset)) {
> >    data<-subset(data,subset)
> > detach(data)
> > attach(data)
> > }
> > subset = NULL
> >
> > # Function body here
> > output <- x
> > return(output)
> > }
> >
> > Thanks!
> >
> > Manuel
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>


From markleeds at verizon.net  Thu Jun  8 18:03:34 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Thu, 08 Jun 2006 11:03:34 -0500 (CDT)
Subject: [R] [Fwd: R 2.20 Windows XP anaolgue of Splus unix() command ?]
Message-ID: <15477170.361141149782614484.JavaMail.root@vms070.mailsrvcs.net>

>Hi Everyone : As I mentioned earlier, I am taking a lot
>of Splus code and turning into R and I've run into
>another stumbling block that I have not been
>able to figure out.
>
>I did plotting in a loop when I was using Splus on unix
>and the way I made the plots stop so I could
>lookat them as they got plotted ( there are hundreds
>if not thousands getting plotted sequentially ) 
>on the screen was by using the unix() command.
>
>Basically, I wrote a function called wait()
>
>
>wait<-function()
>{
>cat("press return to continue")
>unix("read stuff")
>}
>
>and this worked nicely because I then
>did source("program name") at the Splus prompt and
>a plot was created on the screen  and then
>the wait() function was right under the plotting code
>in the program so that you had to hit the return key to go to the next plot.
>
>I am trying to do the equivalent on R 2.20/windows XP
>I did a ?unix in R and it came back with system() and
>said unix was deprecated so I replaced unix("read stuff") with system("read stuff") but all i get is a warning "read not found" and
>it flies through the successive plots and i can't see them.
>
>Thanks for any help on this. It's much appreciated.
>
>                                        Mark


From Ted.Harding at nessie.mcc.ac.uk  Thu Jun  8 18:16:57 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 08 Jun 2006 17:16:57 +0100 (BST)
Subject: [R] Re-binning histogram data
In-Reply-To: <001c01c68b13$6c787170$295afea9@gne.windows.gene.com>
Message-ID: <XFMail.060608171657.Ted.Harding@nessie.mcc.ac.uk>

On 08-Jun-06 Berton Gunter wrote:
> I would argue that histograms are outdated relics and that density
> plots (whatever your favorite flavor is) should **always** be used
> instead these days.
> 
> In this vein, I would appreciate critical rejoinders (public or
> private) to the following proposition: Given modern computer power
> and software like R on multi ghz machines, statistical and graphical
> relics of the pre-computer era (like histograms, low resolution
> printer-type plots, and perhaps even method of moments EMS
> calculations) should be abandoned in favor of superior but perhaps
> computation-intensive alternatives (like density plots, high
> resolution plots, and likelihood or resampling or Bayes based methods).

While your head is above the parapet, Bert ...

Your general question could go in many directions, but there's a
lot to be said for that point of view (as well as some against).

However, my short answer is that it's a matter of horses for courses.

In particular, where the histogram is concerned, it has a straightforward
property that it exactly represents the information about the counts
within the bin-ranges. While usually the bars are not labelled with
count values, you can (and I quite often have, when it was the only
way) recover the counts using a ruler graduated in millimetres. And
the same time it usually (if judiciously constructed) presents a
good blockwise representation of the implied underlying continuous
distribution.

A continuous density estimation may be a better and smoother (or
at least more appealing) representation of the distribution (though
you would need to be careful about local humps), but to recover the
data from it would take a combination of optical scanning, image
analysis software, and (if you don't know what smoothing method
was used) heuristic algorithm-inference software. Well within
your technological utopia, of course, but ...

> NB: Please -- no pleadings that new methods would be mystifying
> to the non-cogniscenti. Following that to its logical conclusion
> would mean that we'd all have to give up our TV remotes and cell
> phones, and what kind of world would that be?! :-)

One day, let me show you how to use my wooden plough-share.

Best wishes,
Ted.

PS Please bring your own horse.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 08-Jun-06                                       Time: 17:16:53
------------------------------ XFMail ------------------------------


From kalisch at stat.math.ethz.ch  Thu Jun  8 17:55:04 2006
From: kalisch at stat.math.ethz.ch (Markus Kalisch)
Date: Thu, 8 Jun 2006 17:55:04 +0200
Subject: [R] [R-pkgs] New R package "pcalg"
Message-ID: <17544.18520.122208.384189@stat.math.ethz.ch>

The R package "pcalg" is released.

The main purpose of this package is to estimate the skeleton of
a DAG given data sampled from a distribution corresponding to
this DAG.

More intuitively, you can search for "strong dependencies" in
potentially high dimensional data sets. The result will be an
undirected graph where each edge represents some dependence
statement. Optionally, the line width can be varied in order to indicate the
reliability of the dependencies found.

For more details have a look at the vignette.

I'd be happy to receive comments and suggestions for improvement.

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From markleeds at verizon.net  Thu Jun  8 18:47:24 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Thu, 08 Jun 2006 11:47:24 -0500 (CDT)
Subject: [R] apologies if you aready received this ?
Message-ID: <7847663.375401149785244948.JavaMail.root@vms070.mailsrvcs.net>

I am accessing my email account remotely so it
seems to be acting strangely so I am not sure
if this R question was received. I apologize if it was
and thanks for any help you can provide.

-----------------------------------------------------------------


Hi Everyone : As I mentioned earlier, I am taking a lot
of Splus code and turning into R and I've run into
another stumbling block that I have not been
able to figure out.

I did plotting in a loop when I was using Splus on unix
and the way I made the plots stop so I could
lookat them as they got plotted ( there are hundreds
if not thousands getting plotted sequentially ) 
on the screen was by using the unix() command.

Basically, I wrote a function called wait()


wait<-function()
{
cat("press return to continue")
unix("read stuff")
}

and this worked nicely because I then
did source("program name") at the Splus prompt and
a plot was created on the screen  and then
the wait() function was right under the plotting code
in the program so that you had to hit the return key to go to the next plot.

I am trying to do the equivalent on R 2.20/windows XP
I did a ?unix in R and it came back with system() and
said unix was deprecated so I replaced unix("read stuff") with system("read stuff") but all i get is a warning "read not found" and
it flies through the successive plots and i can't see them.

Thanks for any help on this. It's much appreciated.


From kwong at nymex.com  Thu Jun  8 18:57:00 2006
From: kwong at nymex.com (Wong, Kim)
Date: Thu, 8 Jun 2006 12:57:00 -0400
Subject: [R] convert type list to character
Message-ID: <93BDB4436D8C1347BCE07BDBB05835FC0237B499@CORPMAIL02.prod.nymex.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060608/4ca76aa6/attachment.pl 

From sfalcon at fhcrc.org  Thu Jun  8 18:59:30 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Thu, 08 Jun 2006 09:59:30 -0700
Subject: [R] BioC2006 Conference, Aug 3-4 in Seattle
Message-ID: <m2bqt3mvm5.fsf@ziti.local>

Early registration ends July 1

BioC2006    August 3-4 in Seattle, WA, USA

http://bioconductor.org/BioC2006/


About BioC2006:

This conference highlights developments within and beyond Bioconductor
and provides a forum to discuss the use and design of software for
analyzing data arising in biology.

Format: 
  * Scientific talks on both days from 8:30-12:00 
  * Hands-on lab sessions on both afternoons 2:00-5:00

Find out who is speaking and more by visiting the website:
http://bioconductor.org/BioC2006/ 

(You will be redirected to our
secure server: https://cobra.fhcrc.org/BioC2006/)


Best Wishes,

+ seth


PS: There will also be a developer focused meeting the day before the
conference, August 2nd.  For details, please see:
http://wiki.fhcrc.org/bioc/BioC2006/DeveloperDay


From sfalcon at fhcrc.org  Thu Jun  8 19:01:09 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Thu, 08 Jun 2006 10:01:09 -0700
Subject: [R] apologies if you aready received this ?
In-Reply-To: <7847663.375401149785244948.JavaMail.root@vms070.mailsrvcs.net>
	(markleeds@verizon.net's message of "Thu,
	08 Jun 2006 11:47:24 -0500 (CDT)")
References: <7847663.375401149785244948.JavaMail.root@vms070.mailsrvcs.net>
Message-ID: <m27j3rmvje.fsf@ziti.local>

<markleeds at verizon.net> writes:
> Basically, I wrote a function called wait()
>
>
> wait<-function()
> {
> cat("press return to continue")
> unix("read stuff")
> }

Is readline what you want?  See help(readline).


From gunter.berton at gene.com  Thu Jun  8 19:03:28 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 8 Jun 2006 10:03:28 -0700
Subject: [R] apologies if you aready received this ?
In-Reply-To: <7847663.375401149785244948.JavaMail.root@vms070.mailsrvcs.net>
Message-ID: <003a01c68b1d$76cf5710$295afea9@gne.windows.gene.com>

?readline

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA

 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> markleeds at verizon.net
> Sent: Thursday, June 08, 2006 9:47 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] apologies if you aready received this ?
> 
> I am accessing my email account remotely so it
> seems to be acting strangely so I am not sure
> if this R question was received. I apologize if it was
> and thanks for any help you can provide.
> 
> -----------------------------------------------------------------
> 
> 
> Hi Everyone : As I mentioned earlier, I am taking a lot
> of Splus code and turning into R and I've run into
> another stumbling block that I have not been
> able to figure out.
> 
> I did plotting in a loop when I was using Splus on unix
> and the way I made the plots stop so I could
> lookat them as they got plotted ( there are hundreds
> if not thousands getting plotted sequentially ) 
> on the screen was by using the unix() command.
> 
> Basically, I wrote a function called wait()
> 
> 
> wait<-function()
> {
> cat("press return to continue")
> unix("read stuff")
> }
> 
> and this worked nicely because I then
> did source("program name") at the Splus prompt and
> a plot was created on the screen  and then
> the wait() function was right under the plotting code
> in the program so that you had to hit the return key to go to 
> the next plot.
> 
> I am trying to do the equivalent on R 2.20/windows XP
> I did a ?unix in R and it came back with system() and
> said unix was deprecated so I replaced unix("read stuff") 
> with system("read stuff") but all i get is a warning "read 
> not found" and
> it flies through the successive plots and i can't see them.
> 
> Thanks for any help on this. It's much appreciated.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From ja at space.mit.edu  Thu Jun  8 19:19:58 2006
From: ja at space.mit.edu (Justin Ashmall)
Date: Thu, 8 Jun 2006 18:19:58 +0100 (BST)
Subject: [R] Re-binning histogram data
In-Reply-To: <001c01c68b13$6c787170$295afea9@gne.windows.gene.com>
References: <001c01c68b13$6c787170$295afea9@gne.windows.gene.com>
Message-ID: <Pine.LNX.4.62.0606081752230.2883@knib.net>

> histograms [...] should be abandoned in favor of [...] density plots.

I take your point Bert, but I think there is value in data that is simple 
and can be intuitively understood.

For my application, 10-minutes is a good charactersitic chunk of time, and 
I have an intuitive feeling of how many events I would expect to see in a 
10-minute period. By looking at a histogram with 10-minute bins I can 
tell immediately if something looks amiss. I could not do this simply with 
a pdf. Similarly histograms have the nice feature of compartmentalising 
bad data. Perhaps this is practical-use vs mathematical-idealism?

Also it's a case of simple tools for simple jobs. If the handle is loose 
on my kitchen cabient, I tighten the screw with a screwdriver or even the 
tip of a knife from the drawer. I don't need my power-drill with 
torque-controlled screwdriver attachement, much as I love it.

Justin


On Thu, 8 Jun 2006, Berton Gunter wrote:

> I would argue that histograms are outdated relics and that density plots
> (whatever your favorite flavor is) should **always** be used instead these
> days.
>
> In this vein, I would appreciate critical rejoinders (public or private) to
> the following proposition: Given modern computer power and software like R
> on multi ghz machines, statistical and graphical relics of the pre-computer
> era (like histograms, low resolution printer-type plots, and perhaps even
> method of moments EMS calculations) should be abandoned in favor of superior
> but perhaps computation-intensive alternatives (like density plots, high
> resolution plots, and likelihood or resampling or Bayes based methods).
>
> NB: Please -- no pleadings that new methods would be mystifying to the
> non-cogniscenti. Following that to its logical conclusion would mean that
> we'd all have to give up our TV remotes and cell phones, and what kind of
> world would that be?! :-)
>
> -- Bert Gunter
>
>
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Petr Pikal
>> Sent: Thursday, June 08, 2006 6:17 AM
>> To: Justin Ashmall; r-help at stat.math.ethz.ch
>> Subject: Re: [R] Re-binning histogram data
>>
>>
>>
>> On 8 Jun 2006 at 11:35, Justin Ashmall wrote:
>>
>> Date sent:      	Thu, 8 Jun 2006 11:35:46 +0100 (BST)
>> From:           	Justin Ashmall <ja at space.mit.edu>
>> To:             	Petr Pikal <petr.pikal at precheza.cz>
>> Copies to:      	r-help at stat.math.ethz.ch
>> Subject:        	Re: [R] Re-binning histogram data
>>
>>>
>>> Thanks for the reply Petr,
>>>
>>> It looks to me that truehist() needs a vector of data just like
>>> hist()? Whereas I have histogram-style input data? Am I missing
>>> something?
>>
>> Well, maybe you could use barplot. Or as you suggested recreate the
>> original vector and call hist or truehist with other bins.
>>
>>> hhh<-hist(rnorm(1000))
>>> barplot(tapply(hhh$counts, c(rep(1:7,each=2),7), sum))
>>> tapply(hhh$mids, c(rep(1:7,each=2),7), mean)
>>     1     2     3     4     5     6     7
>> -3.00 -2.00 -1.00  0.00  1.00  2.00  3.25
>>> hhh1<-rep(hhh$mids,hhh$counts)
>>> plot(hhh, freq=F)
>>> lines(density(hhh1))
>>>
>>
>> HTH
>> Petr
>>
>>
>>
>>
>>
>>
>>>
>>> Cheers,
>>>
>>> Justin
>>>
>>>
>>>
>>> On Thu, 8 Jun 2006, Petr Pikal wrote:
>>>
>>>> Hi
>>>>
>>>> try truehist from MASS package and look for argument breaks or h.
>>>>
>>>> HTH
>>>> Petr
>>>>
>>>>
>>>>
>>>>
>>>> On 8 Jun 2006 at 10:46, Justin Ashmall wrote:
>>>>
>>>> Date sent:      	Thu, 8 Jun 2006 10:46:19 +0100 (BST)
>>>> From:           	Justin Ashmall <ja at space.mit.edu>
>>>> To:             	r-help at stat.math.ethz.ch
>>>> Subject:        	[R] Re-binning histogram data
>>>>
>>>>> Hi,
>>>>>
>>>>> Short Version:
>>>>> Is there a function to re-bin a histogram to new, broader bins?
>>>>>
>>>>> Long version: I'm trying to create a histogram, however my
>>>>> input-data is itself in the form of a fine-grained
>> histogram, i.e.
>>>>> numbers of counts in regular one-second bins. I want to produce a
>>>>> histogram of, say, 10-minute bins (though possibly irregular bins
>>>>> also).
>>>>>
>>>>> I suppose I could re-create a data set as expected by the hist()
>>>>> function (i.e. if time t=3600 has 6 counts, add six
>> entries of 3600
>>>>> to a list) however this seems neither elegant nor
>> efficient (though
>>>>> I'd be pleased to be mistaken!). I could then re-create
>> a histogram
>>>>> as normal.
>>>>>
>>>>> I guessing there's a better solution however! Apologies
>> if this is
>>>>> a basic question - I'm rather new to R and trying to get up to
>>>>> speed.
>>>>>
>>>>> Regards,
>>>>>
>>>>> Justin
>>>>>
>>>>> ______________________________________________
>>>>> R-help at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide!
>>>>> http://www.R-project.org/posting-guide.html
>>>>
>>>> Petr Pikal
>>>> petr.pikal at precheza.cz
>>>>
>>>>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>
>> Petr Pikal
>> petr.pikal at precheza.cz
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>
>


From spencer.graves at pdf.com  Thu Jun  8 19:25:00 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 08 Jun 2006 10:25:00 -0700
Subject: [R] Re-binning histogram data
In-Reply-To: <XFMail.060608171657.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.060608171657.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <44885D6C.8090301@pdf.com>

Hi, Bert, Ted, et al.:

	  Do you use normal probability plots?  They are the best tool I know 
for identifying all kinds of nonnormality, including normal mixtures 
with either outliers or multimodality as well as skewness.  I've 
experimented with PP and nonnormal QQ plots, and not found them that 
useful.  I prefer to transform the data to apparent normality, because 
that seems to produce about the right amount of visual separation in the 
tails:  A PP plot provides very poor resolution of tail behavior, and 
the image in a QQ plot with longer than normal tails becomes for me so 
overwhelmed by random tail behavior that I'm unable to make sense of it.

	  Also, could someone explain the rationale behind the "datax=FALSE" 
default?  I presume this default was established before research showed 
that humans have better judgment about vertical and horizontal lines 
than lines at other angles, and that 45 degree lines are more easily 
judged than lines at other angles.  This research led to "the 45 degree 
banking rule (see _Visualizing Data_ by William S. Cleveland for 
details)", mentioned on the help page for xyplot{lattice}.

	  In my experience, most normal probability plots come closer to 
meeting this "45 degree banking rule" when datax=TRUE than FALSE.  With 
a typical aspect ratio, normally distributed data will appear with an 
angle less than 45 degrees.  An outlier with the default datax=FALSE 
will reduce that 45 degrees, making it harder to process visually.  By 
contrast, with datax=TRUE, an outlier increases the banking, moving it 
closer to (or even beyond) the 45 degree line that seems to facilitate 
the best human visual processing.

	  Beyond this, what do you think about combining a normal plot with 
either a histogram or a density estimate on the bottom?  With multiple 
lines on the normal probability plot, I've seen stacked-bar histograms 
on the bottom that seemed intelligible.  Would you suggest replacing 
stacked-bar histograms with overlapping plots of density estimates?  And 
how many observations would you need in each group for that to make sense?

	  What do you think?
	  Best Wishes,
	  Spencer Graves


(Ted Harding) wrote:
> On 08-Jun-06 Berton Gunter wrote:
>> I would argue that histograms are outdated relics and that density
>> plots (whatever your favorite flavor is) should **always** be used
>> instead these days.
>>
>> In this vein, I would appreciate critical rejoinders (public or
>> private) to the following proposition: Given modern computer power
>> and software like R on multi ghz machines, statistical and graphical
>> relics of the pre-computer era (like histograms, low resolution
>> printer-type plots, and perhaps even method of moments EMS
>> calculations) should be abandoned in favor of superior but perhaps
>> computation-intensive alternatives (like density plots, high
>> resolution plots, and likelihood or resampling or Bayes based methods).
> 
> While your head is above the parapet, Bert ...
> 
> Your general question could go in many directions, but there's a
> lot to be said for that point of view (as well as some against).
> 
> However, my short answer is that it's a matter of horses for courses.
> 
> In particular, where the histogram is concerned, it has a straightforward
> property that it exactly represents the information about the counts
> within the bin-ranges. While usually the bars are not labelled with
> count values, you can (and I quite often have, when it was the only
> way) recover the counts using a ruler graduated in millimetres. And
> the same time it usually (if judiciously constructed) presents a
> good blockwise representation of the implied underlying continuous
> distribution.
> 
> A continuous density estimation may be a better and smoother (or
> at least more appealing) representation of the distribution (though
> you would need to be careful about local humps), but to recover the
> data from it would take a combination of optical scanning, image
> analysis software, and (if you don't know what smoothing method
> was used) heuristic algorithm-inference software. Well within
> your technological utopia, of course, but ...
> 
>> NB: Please -- no pleadings that new methods would be mystifying
>> to the non-cogniscenti. Following that to its logical conclusion
>> would mean that we'd all have to give up our TV remotes and cell
>> phones, and what kind of world would that be?! :-)
> 
> One day, let me show you how to use my wooden plough-share.
> 
> Best wishes,
> Ted.
> 
> PS Please bring your own horse.
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 08-Jun-06                                       Time: 17:16:53
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From murdoch at stats.uwo.ca  Thu Jun  8 19:35:40 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 08 Jun 2006 13:35:40 -0400
Subject: [R] apologies if you aready received this ?
In-Reply-To: <7847663.375401149785244948.JavaMail.root@vms070.mailsrvcs.net>
References: <7847663.375401149785244948.JavaMail.root@vms070.mailsrvcs.net>
Message-ID: <44885FEC.3060504@stats.uwo.ca>

On 6/8/2006 12:47 PM, markleeds at verizon.net wrote:
> I am accessing my email account remotely so it
> seems to be acting strangely so I am not sure
> if this R question was received. I apologize if it was
> and thanks for any help you can provide.
> 
> -----------------------------------------------------------------
> 
> 
> Hi Everyone : As I mentioned earlier, I am taking a lot
> of Splus code and turning into R and I've run into
> another stumbling block that I have not been
> able to figure out.
> 
> I did plotting in a loop when I was using Splus on unix
> and the way I made the plots stop so I could
> lookat them as they got plotted ( there are hundreds
> if not thousands getting plotted sequentially ) 
> on the screen was by using the unix() command.
> 
> Basically, I wrote a function called wait()
> 
> 
> wait<-function()
> {
> cat("press return to continue")
> unix("read stuff")
> }

You can also use par(ask=TRUE) to get R to pause the script whenever it 
is about to erase the graphics window.

Duncan Murdoch

> 
> and this worked nicely because I then
> did source("program name") at the Splus prompt and
> a plot was created on the screen  and then
> the wait() function was right under the plotting code
> in the program so that you had to hit the return key to go to the next plot.
> 
> I am trying to do the equivalent on R 2.20/windows XP
> I did a ?unix in R and it came back with system() and
> said unix was deprecated so I replaced unix("read stuff") with system("read stuff") but all i get is a warning "read not found" and
> it flies through the successive plots and i can't see them.
> 
> Thanks for any help on this. It's much appreciated.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From pramirez379 at hotmail.com  Thu Jun  8 20:31:26 2006
From: pramirez379 at hotmail.com (Pedro Ramirez)
Date: Thu, 08 Jun 2006 20:31:26 +0200
Subject: [R] Density Estimation
Message-ID: <BAY119-F8489EECD077D4718026CDF88B0@phx.gbl>

>In mathematical terms the optimal bandwith for density estimation
>decreases at rate n^{-1/5}, while the one for distribution function
>decreases at rate n^{-1/3}, if n is the sample size. In practical terms,
>one must choose an appreciably smaller bandwidth in the second case
>than in the first one.

Thanks a lot for your remark! I was not aware of the fact that the
optimal bandwidths for density and distribution do not decrease
at the same rate.

>Besides the computational aspect, there is a statistical one:
>the optimal choice of bandwidth for estimating the density function
>is not optimal (and possibly not even jsut sensible) for estimating
>the distribution function, and the stated problem is equivalent to
>estimation of the distribution function.

The given interval "0<x<3" was only an example, in fact I would
like to estimate the probability for intervals such as

"0<=x<1" , "1<=x<2" , "2<=x<3" , "3<=x<4" , ....

and compare it with the estimates of a corresponding histogram.
In this case the stated problem is not anymore equivalent to the
estimation of the distribution function. What do you think, can
I go a ahead in this case with the optimal bandwidth for the
density? Thanks a lot for your help!

Best wishes
Pedro




>best wishes,
>
>Adelchi
>
>
>PR>
>PR> >
>PR> >--
>PR> >Gregory (Greg) L. Snow Ph.D.
>PR> >Statistical Data Center
>PR> >Intermountain Healthcare
>PR> >greg.snow at intermountainmail.org
>PR> >(801) 408-8111
>PR> >
>PR> >
>PR> >-----Original Message-----
>PR> >From: r-help-bounces at stat.math.ethz.ch
>PR> >[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pedro
>PR> >Ramirez Sent: Wednesday, June 07, 2006 11:00 AM
>PR> >To: r-help at stat.math.ethz.ch
>PR> >Subject: [R] Density Estimation
>PR> >
>PR> >Dear R-list,
>PR> >
>PR> >I have made a simple kernel density estimation by
>PR> >
>PR> >x <- c(2,1,3,2,3,0,4,5,10,11,12,11,10)
>PR> >kde <- density(x,n=100)
>PR> >
>PR> >Now I would like to know the estimated probability that a new
>PR> >observation falls into the interval 0<x<3.
>PR> >
>PR> >How can I integrate over the corresponding interval?
>PR> >In several R-packages for kernel density estimation I did not
>PR> >found a corresponding function. I could apply Simpson's Rule for
>PR> >integrating, but perhaps somebody knows a better solution.
>PR> >
>PR> >Thanks a lot for help!
>PR> >
>PR> >Pedro
>PR> >
>PR> >_________
>PR> >
>PR> >______________________________________________
>PR> >R-help at stat.math.ethz.ch mailing list
>PR> >https://stat.ethz.ch/mailman/listinfo/r-help
>PR> >PLEASE do read the posting guide!
>PR> >http://www.R-project.org/posting-guide.html
>PR> >
>PR>
>PR> ______________________________________________
>PR> R-help at stat.math.ethz.ch mailing list
>PR> https://stat.ethz.ch/mailman/listinfo/r-help
>PR> PLEASE do read the posting guide!
>PR> http://www.R-project.org/posting-guide.html
>PR>


From p.dalgaard at biostat.ku.dk  Thu Jun  8 20:45:52 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Jun 2006 20:45:52 +0200
Subject: [R] apologies if you aready received this ?
In-Reply-To: <m27j3rmvje.fsf@ziti.local>
References: <7847663.375401149785244948.JavaMail.root@vms070.mailsrvcs.net>
	<m27j3rmvje.fsf@ziti.local>
Message-ID: <x2pshja3kv.fsf@turmalin.kubism.ku.dk>

Seth Falcon <sfalcon at fhcrc.org> writes:

> <markleeds at verizon.net> writes:
> > Basically, I wrote a function called wait()
> >
> >
> > wait<-function()
> > {
> > cat("press return to continue")
> > unix("read stuff")
> > }
> 
> Is readline what you want?  See help(readline).

...and the generic issue here is that system() calls the system shell,
and if that doesn't know of a "read" command, you're up the proverbial
roof without a paddle.

If you really insist of unix() functionality, you need a Unix-like
subsystem. You could start by installing bash.exe and do things like
system("bash -c read stuff") but you'll soon realize that you also
need all of the programs that are traditionally available on Unix.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From Manuel.A.Morales at williams.edu  Thu Jun  8 20:56:34 2006
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Thu, 08 Jun 2006 14:56:34 -0400
Subject: [R] Using data=x or subset=y in user-defined functions
In-Reply-To: <1149782097.6491.6.camel@solidago.localdomain>
References: <1149710497.14236.11.camel@solidago.localdomain>
	<Pine.LNX.4.64.0606072135350.8690@gannet.stats.ox.ac.uk>
	<1149782097.6491.6.camel@solidago.localdomain>
Message-ID: <1149792994.6491.17.camel@solidago.localdomain>

On Thu, 2006-06-08 at 11:54 -0400, Manuel Morales wrote:
> On Wed, 2006-06-07 at 21:36 +0100, Prof Brian Ripley wrote:
> > I suggest you investigate with().

Thanks for the suggestion! Unfortunately, it's not clear to me how to
call with() from a function. The example below fails with the error
message: 'Error in mean(x) : object "a" not found'

data.test <- data.frame(a=c(1:10),b=rnorm(10))

eg.function <- function(x, data)
  with(data, return(mean(x)))

eg.function(a,data.test)

Manuel

> Thanks! Just to be sure, the following will do what I want?
> 
> eg.function <- function(x, data=NULL, subset=NULL, ...) {
>     with(if(is.null(subset)) data else subset(data,subset), {
>  
>     ...
> 
>     })
> }
> 
> > On Wed, 7 Jun 2006, Manuel Morales wrote:
> > 
> > > Dear list members,
> > >
> > > In some of my functions, I attach the data internally to allow subset
> > > commands or to specify a data frame. This works well except for cases
> > > where there is a "masking" conflict (which returns a warning). I see
> > > some alternative listed in ?attach, but I'm not sure which of them do
> > > what I'd like. Any suggestions?
> > >
> > > Below is how I've been setting up my functions:
> > >
> > > 
> > >
> > > # Set up environment
> > > on.exit(detach(data))
> > > attach(data)
> > > if(!is.null(subset)) {
> > >    data<-subset(data,subset)
> > > detach(data)
> > > attach(data)
> > > }
> > > subset = NULL
> > >
> > > # Function body here
> > > output <- x
> > > return(output)
> > > }
> > >
> > > Thanks!
> > >
> > > Manuel
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ericpante at hotmail.com  Thu Jun  8 21:09:52 2006
From: ericpante at hotmail.com (Eric Pante)
Date: Thu, 8 Jun 2006 15:09:52 -0400
Subject: [R] nested mixed-effect model: variance components
Message-ID: <BAY106-DAV244471184652FC1DD27A0BBC8B0@phx.gbl>

Dear listers,

I am trying to assess variance components for a nested, mixed-effects 
model. I think I got an answer that make sense from R, but I have a 
warning message and I wanted to check that what I am looking at is 
actually what I need:

my data are organized as transects within stations, stations within 
habitats, habitats within lagoons.
lagoons: random, habitats: fixed
the question is: how much variation is due to lagoons? habitats? 
lagoons*habitat? transects?

Here is my code:

res <- aov(COVER ~ HABITAT + Error(HABITAT+LAGOON+LAGOON/HABITAT), 
data=cov)
summary(res)

and I get Sum Sq for each to calculate variance components:

Error: STRATE
        Df Sum Sq Mean Sq
STRATE  5 4493.1   898.6

Error: ATOLL
           Df Sum Sq Mean Sq F value Pr(>F)
Residuals  5 3340.5   668.1

Error: STRATE:ATOLL
           Df  Sum Sq Mean Sq F value Pr(>F)
Residuals 18 2442.71  135.71

Error: Within
            Df Sum Sq Mean Sq F value Pr(>F)
Residuals 145 6422.0    44.3

My error message seems to come from the LAGOON/HABITAT, the Error is 
computed.
Warning message: Error() model is singular in: aov(COVER ~ HABITAT + 
Error(HABITAT+LAGOON+LAGOON/HABITAT), data=cov),

THANKS !!!
eric



Eric Pante
----------------------------------------------------------------
College of Charleston, Grice Marine Laboratory
205 Fort Johnson Road, Charleston SC 29412
Phone: 843-953-9190 (lab)  -9200 (main office)
----------------------------------------------------------------

	"On ne force pas la curiosite, on l'eveille ..."
	Daniel Pennac


From murdoch at stats.uwo.ca  Thu Jun  8 21:18:04 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 08 Jun 2006 15:18:04 -0400
Subject: [R] Using data=x or subset=y in user-defined functions
In-Reply-To: <1149792994.6491.17.camel@solidago.localdomain>
References: <1149710497.14236.11.camel@solidago.localdomain>	<Pine.LNX.4.64.0606072135350.8690@gannet.stats.ox.ac.uk>	<1149782097.6491.6.camel@solidago.localdomain>
	<1149792994.6491.17.camel@solidago.localdomain>
Message-ID: <448877EC.5030002@stats.uwo.ca>

On 6/8/2006 2:56 PM, Manuel Morales wrote:
> On Thu, 2006-06-08 at 11:54 -0400, Manuel Morales wrote:
>> On Wed, 2006-06-07 at 21:36 +0100, Prof Brian Ripley wrote:
>> > I suggest you investigate with().
> 
> Thanks for the suggestion! Unfortunately, it's not clear to me how to
> call with() from a function. The example below fails with the error
> message: 'Error in mean(x) : object "a" not found'
> 
> data.test <- data.frame(a=c(1:10),b=rnorm(10))
> 
> eg.function <- function(x, data)
>   with(data, return(mean(x)))
> 
> eg.function(a,data.test)

When you call a function in R, you normally need to have the variables 
defined in the local environment.  You are trying to pass a variable 
named a to your function (where a copy will be made in the local 
variable x), but there is no variable a to pass.

I think what you want to do is to pass an expression "a" to your 
function, and have the function evaluate that expression within your 
dataframe, i.e. you don't want the standard evaluation method to be 
used. To do that you'd do something like this:

 > eg.function <- function(expr, data) {
+   x <- eval(substitute(expr), envir=data)
+   return(mean(x))
+ }
 > eg.function(a, data.test)
[1] 5.5

The "substitute" says to give back the unevaluated expression used for 
the argument.

Duncan Murdoch

> 
> Manuel
> 
>> Thanks! Just to be sure, the following will do what I want?
>> 
>> eg.function <- function(x, data=NULL, subset=NULL, ...) {
>>     with(if(is.null(subset)) data else subset(data,subset), {
>>  
>>     ...
>> 
>>     })
>> }
>> 
>> > On Wed, 7 Jun 2006, Manuel Morales wrote:
>> > 
>> > > Dear list members,
>> > >
>> > > In some of my functions, I attach the data internally to allow subset
>> > > commands or to specify a data frame. This works well except for cases
>> > > where there is a "masking" conflict (which returns a warning). I see
>> > > some alternative listed in ?attach, but I'm not sure which of them do
>> > > what I'd like. Any suggestions?
>> > >
>> > > Below is how I've been setting up my functions:
>> > >
>> > > 
>> > >
>> > > # Set up environment
>> > > on.exit(detach(data))
>> > > attach(data)
>> > > if(!is.null(subset)) {
>> > >    data<-subset(data,subset)
>> > > detach(data)
>> > > attach(data)
>> > > }
>> > > subset = NULL
>> > >
>> > > # Function body here
>> > > output <- x
>> > > return(output)
>> > > }
>> > >
>> > > Thanks!
>> > >
>> > > Manuel
>> > >
>> > > ______________________________________________
>> > > R-help at stat.math.ethz.ch mailing list
>> > > https://stat.ethz.ch/mailman/listinfo/r-help
>> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>> > >
>> >
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From jzhang1982 at gmail.com  Thu Jun  8 21:43:57 2006
From: jzhang1982 at gmail.com (zhang jian)
Date: Thu, 8 Jun 2006 15:43:57 -0400
Subject: [R] Error in plot.new() : plot region too large ?
Message-ID: <3f2938d50606081243t7fd1e9b2q8cb4641300f03e63@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060608/62d65a32/attachment.pl 

From alex_restrepo at hotmail.com  Thu Jun  8 22:48:07 2006
From: alex_restrepo at hotmail.com (Alex Restrepo)
Date: Thu, 08 Jun 2006 15:48:07 -0500
Subject: [R] JGR :: Can't Run Graphics Demo
In-Reply-To: <38b9f0350606080647i71c18cf6ub45af00f8e7fa7af@mail.gmail.com>
Message-ID: <BAY106-F31B817CFC09537FE5F2F80988B0@phx.gbl>

Hi:

I have verified that javaGD, rJava, iplots, and iwidgets are installed.    I 
am still not able to run any of the graphics demos.

I still get the following error:

       if (dev.cur() <= 1) get(getOption("device"))()
       Error in get(getOption("device"))() : unable to start device JavaGD

Any help would be appreciated,

Thanks:

Alex



>From: ronggui <ronggui.huang at gmail.com>
>To: "Alex Restrepo" <alex_restrepo at hotmail.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] JGR :: Can't Run Graphics Demo
>Date: Thu, 8 Jun 2006 21:47:07 +0800
>
>Have you installed the following packages?They are 
>javaGD,rJava,iplots,iWidgets?
>
>It seems that you have installed the javaGD package.
>
>2006/6/8, Alex Restrepo <alex_restrepo at hotmail.com>:
>>Hello:
>>
>>I am using JGR 1.4, the Java GUI for R and I am trying to run the demo for
>>graphics using the command:
>>
>>     demo(graphics)
>>
>>I keep on getting the following errror:
>>
>>     Error in get(getOption("device"))() : unable to start device JavaGD
>>
>>
>>FYI, the version of Java I am using is:
>>
>>      1.5.0_07-b03
>>
>>
>>Do I need to use a different version of Java?  Does anyone have any
>>recommendations/suggestions?
>>
>>
>>Many Thanks In Advance:
>>
>>Alex
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>
>
>--
>??????
>Deparment of Sociology
>Fudan University


From alex_restrepo at hotmail.com  Thu Jun  8 23:13:41 2006
From: alex_restrepo at hotmail.com (Alex Restrepo)
Date: Thu, 08 Jun 2006 16:13:41 -0500
Subject: [R] JGR :: Can't Run Graphics Demo
In-Reply-To: <BAY106-F31B817CFC09537FE5F2F80988B0@phx.gbl>
Message-ID: <BAY106-F7EB93F269AE37226D1652988B0@phx.gbl>

I tried to create a pie chart in JGR, via the command:

      pie(rep(1, 24), col = rainbow(24), radius = 0.9)


and I still get the following error:

    Error in JavaGD() : unable to start device JavaGD





>From: "Alex Restrepo" <alex_restrepo at hotmail.com>
>To: ronggui.huang at gmail.com
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] JGR :: Can't Run Graphics Demo
>Date: Thu, 08 Jun 2006 15:48:07 -0500
>
>Hi:
>
>I have verified that javaGD, rJava, iplots, and iwidgets are installed.    
>I am still not able to run any of the graphics demos.
>
>I still get the following error:
>
>       if (dev.cur() <= 1) get(getOption("device"))()
>       Error in get(getOption("device"))() : unable to start device JavaGD
>
>Any help would be appreciated,
>
>Thanks:
>
>Alex
>
>
>
>>From: ronggui <ronggui.huang at gmail.com>
>>To: "Alex Restrepo" <alex_restrepo at hotmail.com>
>>CC: r-help at stat.math.ethz.ch
>>Subject: Re: [R] JGR :: Can't Run Graphics Demo
>>Date: Thu, 8 Jun 2006 21:47:07 +0800
>>
>>Have you installed the following packages?They are 
>>javaGD,rJava,iplots,iWidgets?
>>
>>It seems that you have installed the javaGD package.
>>
>>2006/6/8, Alex Restrepo <alex_restrepo at hotmail.com>:
>>>Hello:
>>>
>>>I am using JGR 1.4, the Java GUI for R and I am trying to run the demo 
>>>for
>>>graphics using the command:
>>>
>>>     demo(graphics)
>>>
>>>I keep on getting the following errror:
>>>
>>>     Error in get(getOption("device"))() : unable to start device JavaGD
>>>
>>>
>>>FYI, the version of Java I am using is:
>>>
>>>      1.5.0_07-b03
>>>
>>>
>>>Do I need to use a different version of Java?  Does anyone have any
>>>recommendations/suggestions?
>>>
>>>
>>>Many Thanks In Advance:
>>>
>>>Alex
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>>>
>>
>>
>>--
>>??????
>>Deparment of Sociology
>>Fudan University
>


>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html


From mmfuller at tiem.utk.edu  Thu Jun  8 23:19:53 2006
From: mmfuller at tiem.utk.edu (Michael Fuller)
Date: Thu, 8 Jun 2006 17:19:53 -0400
Subject: [R] Dimension Err using marks in ppp.object
Message-ID: <EE558028-3738-4A13-8156-FAE806AB03B8@tiem.utk.edu>

I want to generate a ppp object (spastat R package) using the command  
'ppp' and 3 vectors containing coordinates and numeric mark data, but  
I get an error I don't understand. The command is:

index10.ppp  <- ppp(x, y, c(20,240), c(140,780), marks=z)

where:
x is a numeric vector of integers that represents the x-coordinate in  
the plane
y is a numeric vector of integers that represents the y-coordinate in  
the plane
z is a numeric vector of doubles (float) that represents the values  
of an index measured at each x-y point
The values given for owin are the corners of the experimental plot

Here is the error message:
"Attempted to create point pattern with 1 columns of mark data;  
multidimensional marks are not yet implemented"

How can 1 column of data be multidimensional? Does this error refer  
to my use of integer coordinates and double marks? Is there a work  
around?

I'm using R for Mac OS X. The version of R is: 1.15, framework R  
2.3.0. The version of spastat is: 1.9-1.

Any advice is appreciated.
MF


From fisher at plessthan.com  Thu Jun  8 23:42:00 2006
From: fisher at plessthan.com (Dennis Fisher)
Date: Thu, 8 Jun 2006 14:42:00 -0700
Subject: [R] Formatting numbers for printing
In-Reply-To: <mailman.11.1149760807.7251.r-help@stat.math.ethz.ch>
References: <mailman.11.1149760807.7251.r-help@stat.math.ethz.ch>
Message-ID: <3D286041-FC44-4A8E-B06E-632C1B8B19B1@plessthan.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060608/bc2e2ccc/attachment.pl 

From murdoch at stats.uwo.ca  Thu Jun  8 23:54:07 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 08 Jun 2006 17:54:07 -0400
Subject: [R] Formatting numbers for printing
In-Reply-To: <3D286041-FC44-4A8E-B06E-632C1B8B19B1@plessthan.com>
References: <mailman.11.1149760807.7251.r-help@stat.math.ethz.ch>
	<3D286041-FC44-4A8E-B06E-632C1B8B19B1@plessthan.com>
Message-ID: <44889C7F.7010307@stats.uwo.ca>

On 6/8/2006 5:42 PM, Dennis Fisher wrote:
> Colleagues
> 
> I have numbers like "12012" that I will be printing in a graphic.  I  
> would like to format them as follows: 012-012, i.e., the first two  
> digits padded to the left by a zero, followed by a dash, followed by  
> the final three digits, also padded to the left by zeros.
> 
> I can do this with brute force:
> 
> FirstPart			<- sprintf("%03d", floor(Number / 1000))
> SecondPart		<- sprintf("%03d", WhichID %% 1000)
> Combined		<- paste(FirstPart, "-", SecondPart, sep="")
> 
> But, I suspect that there is some more clever means to accomplish  
> this using formatC or sprintf.  Unfortunately, the help pages don't  
> provide sufficient insight.  I would appreciate any tricks to  
> accomplish this.
> 

I think you need to do the separation into two parts as you did (or 
using %/%), but it can all be one sprintf:

sprintf("%03d-%03d", Number %/% 1000, Number %% 1000)

Duncan Murdoch


From luo_weijun at yahoo.com  Fri Jun  9 00:45:49 2006
From: luo_weijun at yahoo.com (Luo Weijun)
Date: Thu, 8 Jun 2006 15:45:49 -0700 (PDT)
Subject: [R] make check errors for  R-2.3.1
In-Reply-To: <mailman.11.1149760807.7251.r-help@stat.math.ethz.ch>
Message-ID: <20060608224549.1451.qmail@web35502.mail.mud.yahoo.com>

Hello all,
I tried to build R-2.3.1 from source under Mac OS X
10.4.6. (I am doing so because only this way I can get
the 64-bit version of R)

The configure and make steps look fine. But I got
errors when I did make check-all, here is the message:

running code in 'base-Ex.R' ...make[4]: ***
[base-Ex.Rout] Error 1
make[3]: *** [test-Examples-Base] Error 2
make[2]: *** [test-Examples] Error 2
make[1]: *** [test-all-basics] Error 1
make: *** [check-all] Error 2

I am not sure what does this mean. 
One potentially related issue is that, I downloaded
and installed a gfortran compiler: GNU Fortran 95
(GCC) 4.2.0 20060512 (experimental), because we don??t
have one originally. But I didn??t use the latest
cctools, and our version is: Apple Computer, Inc.
version cctools-590.23.2.obj~17. Not sure whether this
matters. Our gcc compiler is:
powerpc-apple-darwin8-gcc-4.0.1 (GCC) 4.0.1 (Apple
Computer, Inc. build 5250).
Please let me know if you have any idea about it.
Thank you.
Weijun


From pinard at iro.umontreal.ca  Fri Jun  9 01:53:53 2006
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Thu, 8 Jun 2006 19:53:53 -0400
Subject: [R] Re-binning histogram data
In-Reply-To: <001c01c68b13$6c787170$295afea9@gne.windows.gene.com>
References: <44883F89.11852.12BB1FC@localhost>
	<001c01c68b13$6c787170$295afea9@gne.windows.gene.com>
Message-ID: <20060608235353.GB23457@phenix.progiciels-bpi.ca>

[Berton Gunter]

> I would argue that histograms are outdated relics and that density  
> plots (whatever your favorite flavor is) should **always** be used 
> instead these days.

When a now retired researcher paid us a visit, I showed him a density 
plot produced by R over some data he did work a lot, before he left.
I, too, find them rather sexy, and I wanted to impress him with some of 
the pleasures of R, especially knowing he has been a dedicated user of 
SAS in his times.  Yet, this old and wise man _immediately_ caught that 
the density curve was leaking a tiny bit through the extrema.

Not a big deal of course -- and he did like what he saw.  Nevertheless, 
this reminded me that we should be careful at not dismissing too lightly 
years of accumulated knowledge, experience and know-how, merely because 
we give in joyful enthusiasm for more recent things.

Let me make a comparison, looking at the R mailing lists themselves.  
Some would much like sending HTML email in here: they would get colours,
use various fonts, offer links, and have indentation which dynamically 
adapts on the receiving end to the window size of the reading guy.  But 
the collective wisdom is to stick to non-HTML email, which is quite 
proven and still very functional, after all.  Some impatient people or 
dubious tools use other things than fixed-width fonts while presenting
text/plain email, or merely ignore the usual 79-column limit and other 
oldish etiquette issues while sending it: in last analysis, they kibitz 
the community more than they help it, and deep down, are a bit selfish.  
There is a long way to go before HTML email is really ubiquitous and 
correctly supported.  Consider the long time MIME took to establish 
itself: even now, email readers correctly supporting MIME are hard to 
find -- most are fond on gadgets much more than they know standards.

Another comparison which pops to my mind is how some people fanatically 
try to impose UTF-8 all around, saying that ASCII or ISO-8859-1 (and 
many others) are part of the prehistory of computers.  When mere users,
they can always talk without making too much damage.  But I've seen 
a few maintainers going overboard on such matters, consciously breaking 
software to force their convictions forward: "Crois ou meurs!" as we say 
in French (approximately: "Believe or perish!").  Here, just like for 
HTML mail or nicer bitmapped R graphics, Unicode does have technical 
merit; the truth is that we are _far_ from mastering everything about 
it, and there are lots of open issues that are not strictly technical.

Many proponent of these various things are tempted to say that they want 
to clean out the planet of outdated relics (I liked your expression!)
and have the honest feeling they do trigger overall progress.  Moreover, 
new good things do not necessarily make older things wrong.  In a word, 
we should rather wait for progress with calm, and with respectful care 
of what already exists.  Progress will impose itself slowly over time, 
and is not so much in need of forceful evangelists. :-)

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca


From Charles.Annis at StatisticalEngineering.com  Fri Jun  9 04:17:15 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Thu, 8 Jun 2006 22:17:15 -0400
Subject: [R] Re-binning histogram data
In-Reply-To: <Pine.LNX.4.62.0606081026370.2883@knib.net>
Message-ID: <03b901c68b6a$d3f179d0$6600a8c0@DD4XFW31>

Concerning the several comments on your note relating to histograms, an
informative and entertaining illustration, using Java, of how your
subjective assessment of the data can change with different histograms
constructed from the same data, is provided by R. Webster West, recently
with the Department of Statistics at the University of South Carolina, but
as of May 2006 with the Department of Statistics at Texas A & M University,
http://www.stat.sc.edu/~west/javahtml/Histogram.html  and
http://www.stat.tamu.edu/~west/ 


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Justin Ashmall
Sent: Thursday, June 08, 2006 5:46 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Re-binning histogram data

Hi,

Short Version:
Is there a function to re-bin a histogram to new, broader bins?

Long version: I'm trying to create a histogram, however my input-data is 
itself in the form of a fine-grained histogram, i.e. numbers of counts 
in regular one-second bins. I want to produce a histogram of, say, 
10-minute bins (though possibly irregular bins also).

I suppose I could re-create a data set as expected by the hist() function 
(i.e. if time t=3600 has 6 counts, add six entries of 3600 to a list) 
however this seems neither elegant nor efficient (though I'd be pleased to 
be mistaken!). I could then re-create a histogram as normal.

I guessing there's a better solution however! Apologies if this is a basic 
question - I'm rather new to R and trying to get up to speed.

Regards,

Justin

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From andy_liaw at merck.com  Fri Jun  9 04:27:23 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 8 Jun 2006 22:27:23 -0400
Subject: [R] Re-binning histogram data
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA01BD0DFD@usctmx1106.merck.com>

I'm not sure why you would consider density plots "modern".  Kernel density
estimators were proposed in 1956 and 1962, though they were not used
commonly until the computational powers caught up.
 
Also, I think the advantages of density plots over histograms far out-weight
their possible shortcomings.  If you are unwilling to go that far, at least
consider replacing histograms with ASH plots.
 
[I wouldn't go quite as far as Bert.  Occasionally I still look at
histograms, but _very_ rarely.]
 
Andy

  _____  

From: r-help-bounces at stat.math.ethz.ch on behalf of Fran?ois Pinard
Sent: Thu 6/8/2006 7:53 PM
To: Berton Gunter
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Re-binning histogram data [Broadcast]



[Berton Gunter] 

> I would argue that histograms are outdated relics and that density  
> plots (whatever your favorite flavor is) should **always** be used 
> instead these days. 

When a now retired researcher paid us a visit, I showed him a density 
plot produced by R over some data he did work a lot, before he left. 
I, too, find them rather sexy, and I wanted to impress him with some of 
the pleasures of R, especially knowing he has been a dedicated user of 
SAS in his times.  Yet, this old and wise man _immediately_ caught that 
the density curve was leaking a tiny bit through the extrema. 

Not a big deal of course -- and he did like what he saw.  Nevertheless, 
this reminded me that we should be careful at not dismissing too lightly 
years of accumulated knowledge, experience and know-how, merely because 
we give in joyful enthusiasm for more recent things. 

Let me make a comparison, looking at the R mailing lists themselves.  
Some would much like sending HTML email in here: they would get colours, 
use various fonts, offer links, and have indentation which dynamically 
adapts on the receiving end to the window size of the reading guy.  But 
the collective wisdom is to stick to non-HTML email, which is quite 
proven and still very functional, after all.  Some impatient people or 
dubious tools use other things than fixed-width fonts while presenting 
text/plain email, or merely ignore the usual 79-column limit and other 
oldish etiquette issues while sending it: in last analysis, they kibitz 
the community more than they help it, and deep down, are a bit selfish.  
There is a long way to go before HTML email is really ubiquitous and 
correctly supported.  Consider the long time MIME took to establish 
itself: even now, email readers correctly supporting MIME are hard to 
find -- most are fond on gadgets much more than they know standards. 

Another comparison which pops to my mind is how some people fanatically 
try to impose UTF-8 all around, saying that ASCII or ISO-8859-1 (and 
many others) are part of the prehistory of computers.  When mere users, 
they can always talk without making too much damage.  But I've seen 
a few maintainers going overboard on such matters, consciously breaking 
software to force their convictions forward: "Crois ou meurs!" as we say 
in French (approximately: "Believe or perish!").  Here, just like for 
HTML mail or nicer bitmapped R graphics, Unicode does have technical 
merit; the truth is that we are _far_ from mastering everything about 
it, and there are lots of open issues that are not strictly technical. 

Many proponent of these various things are tempted to say that they want 
to clean out the planet of outdated relics (I liked your expression!) 
and have the honest feeling they do trigger overall progress.  Moreover, 
new good things do not necessarily make older things wrong.  In a word, 
we should rather wait for progress with calm, and with respectful care 
of what already exists.  Progress will impose itself slowly over time, 
and is not so much in need of forceful evangelists. :-) 

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca
<http://pinard.progiciels-bpi.ca/>  

______________________________________________ 
R-help at stat.math.ethz.ch mailing list 
https://stat.ethz.ch/mailman/listinfo/r-help
<https://stat.ethz.ch/mailman/listinfo/r-help>  
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
<http://www.R-project.org/posting-guide.html>


From spencer.graves at pdf.com  Fri Jun  9 05:26:49 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 08 Jun 2006 20:26:49 -0700
Subject: [R] How to do this simple integration?
In-Reply-To: <3231.66.75.237.140.1149738077.squirrel@math.bu.edu>
References: <3231.66.75.237.140.1149738077.squirrel@math.bu.edu>
Message-ID: <4488EA79.1020309@pdf.com>

	  I can't parse your metacode, so I really don't understand your 
question.  Have you worked the examples in the "?integrate" help page? 
Also, do you understand the "..." argument to the "integrate" (and 
other) functions"?  If the answer to the latter is "no", I suggest you 
also read Venables and Ripley, Modern Applied Statistics with S.

	  If that is not adequate and you'd like more help from this listserve, 
please submit a simple, self-contained example that seems to come the 
closest to something you want but can't quite get, as suggested in the 
posting guide! "www.R-project.org/posting-guide.html".

	  I know this didn't answer your question, but I hope it helps you.

	  Best Wishes,
	  Spencer Graves

Jianing Di wrote:
> Hello,
> 
> I have a simple function in the form as follows:
> 
>> f<-function(x){sum(v^x)}
> 
> where v is a vector. I was trying to integrate f using the command
> 
>> I<-integrate(f,0,1)
> 
> However, this will not work and seems that the reason is to use
> "integrate", the f must be a function that with input and output of same
> length. Anyone can point out which command should I use in order to
> compute this type of integration(such as a function involve sum(), prod(),
> etc.)?
> 
> Thank you.
> 
> Jianing
> 
>


From ririzarr at jhsph.edu  Fri Jun  9 05:43:56 2006
From: ririzarr at jhsph.edu (Rafael A. Irizarry)
Date: Thu, 8 Jun 2006 23:43:56 -0400 (EDT)
Subject: [R] X'W in Matrix
Message-ID: <Pine.LNX.4.61.0606082220260.7314@enigma.local>

Hi!

I have used the Matrix package (Version: 0.995-10) successfully 
to obtain the OLS solution for a problem where the design matrix X is 
44000x6000. X is very sparse (about 80000 non-zeros elements).

Now I want to do WLS: (X'WX)^-1X'Wy

I tried W=Diagonal(length(w),w) and
wX=solve(X,W)

but after various minutes R gives a not enough
memory error (Im using a 64bit machine with 16Gigs of RAM).

I ended up doing this:
wX=Matrix(as.matrix(X)*sqrt(w),sparse=TRUE)
coefs1=as.vector(solve(crossprod(wX),crossprod(X,w*y)))

which takes about 1-2 minutes, but it seems a better way, using the 
diagonal matrix, should exist. If there is I'd appreciate hearing it. If 
not, Im happy waiting 1-2 minute x #of iters.


Thanks,
Rafael


From vincent at 7d4.com  Fri Jun  9 07:47:01 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Fri, 09 Jun 2006 07:47:01 +0200
Subject: [R] making matrix monotonous
In-Reply-To: <4484149D.3050209@7d4.com>
References: <4484149D.3050209@7d4.com>
Message-ID: <44890B55.8030906@7d4.com>

Thanks to everybody for your help.
Thanks for the coding improvements
and thanks for the very interesting paper.


From comtech.usa at gmail.com  Fri Jun  9 08:17:40 2006
From: comtech.usa at gmail.com (Michael)
Date: Thu, 8 Jun 2006 23:17:40 -0700
Subject: [R] How to request AIC information from "lm" object?
Message-ID: <b1f16d9d0606082317qf22fcd5h41a57dddc100a89f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060608/0364fdbf/attachment.pl 

From ritz at kvl.dk  Fri Jun  9 08:30:10 2006
From: ritz at kvl.dk (Christian Ritz)
Date: Fri, 09 Jun 2006 08:30:10 +0200
Subject: [R] How to request AIC information from "lm" object?
In-Reply-To: <b1f16d9d0606082317qf22fcd5h41a57dddc100a89f@mail.gmail.com>
References: <b1f16d9d0606082317qf22fcd5h41a57dddc100a89f@mail.gmail.com>
Message-ID: <44891572.4040202@kvl.dk>

Hi Michael,

use: extractAIC to get AIC from an lm object:

y <- rnorm(10)
extractAIC(lm(y~1))



Christian


From ronggui.huang at gmail.com  Fri Jun  9 08:41:18 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Fri, 9 Jun 2006 14:41:18 +0800
Subject: [R] How to request AIC information from "lm" object?
In-Reply-To: <b1f16d9d0606082317qf22fcd5h41a57dddc100a89f@mail.gmail.com>
References: <b1f16d9d0606082317qf22fcd5h41a57dddc100a89f@mail.gmail.com>
Message-ID: <38b9f0350606082341t2583036fxf7e9145fce9249af@mail.gmail.com>

You can use AIC to get what you want.

#example from lm help page
>  ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
>      trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
>      group <- gl(2,10,20, labels=c("Ctl","Trt"))
>      weight <- c(ctl, trt)
>      anova(lm.D9 <- lm(weight ~ group))
Analysis of Variance Table

Response: weight
          Df Sum Sq Mean Sq F value Pr(>F)
group      1 0.6882  0.6882  1.4191  0.249
Residuals 18 8.7293  0.4850
#get the AIC of lm model.
> AIC(lm.D9)
[1] 46.17648



2006/6/9, Michael <comtech.usa at gmail.com>:
> Can "lm" return AIC information?
>
> Thanks a lot!
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
??????
Deparment of Sociology
Fudan University


From ronggui.huang at gmail.com  Fri Jun  9 08:58:15 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Fri, 9 Jun 2006 14:58:15 +0800
Subject: [R] [Fwd: R 2.20 Windows XP anaolgue of Splus unix() command ?]
In-Reply-To: <15477170.361141149782614484.JavaMail.root@vms070.mailsrvcs.net>
References: <15477170.361141149782614484.JavaMail.root@vms070.mailsrvcs.net>
Message-ID: <38b9f0350606082358r10c0a8edx2d46a6eb22df4f37@mail.gmail.com>

In my windows XP there is no "read" command as well,so the _
unix("read stuff")_  will not wor as what _system_ function does is to
pass the 'read stuff' command argument to the system command.

I guess the "read" command to specific to some Unix OS.

Hope this helps.

2006/6/9, markleeds at verizon.net <markleeds at verizon.net>:
> >Hi Everyone : As I mentioned earlier, I am taking a lot
> >of Splus code and turning into R and I've run into
> >another stumbling block that I have not been
> >able to figure out.
> >
> >I did plotting in a loop when I was using Splus on unix
> >and the way I made the plots stop so I could
> >lookat them as they got plotted ( there are hundreds
> >if not thousands getting plotted sequentially )
> >on the screen was by using the unix() command.
> >
> >Basically, I wrote a function called wait()
> >
> >
> >wait<-function()
> >{
> >cat("press return to continue")
> >unix("read stuff")
> >}
> >
> >and this worked nicely because I then
> >did source("program name") at the Splus prompt and
> >a plot was created on the screen  and then
> >the wait() function was right under the plotting code
> >in the program so that you had to hit the return key to go to the next plot.
> >
> >I am trying to do the equivalent on R 2.20/windows XP
> >I did a ?unix in R and it came back with system() and
> >said unix was deprecated so I replaced unix("read stuff") with system("read stuff") but all i get is a warning "read not found" and
> >it flies through the successive plots and i can't see them.
> >
> >Thanks for any help on this. It's much appreciated.
> >
> >                                        Mark
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
??????
Deparment of Sociology
Fudan University


From bates at stat.wisc.edu  Fri Jun  9 09:41:57 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 9 Jun 2006 09:41:57 +0200
Subject: [R] X'W in Matrix
In-Reply-To: <Pine.LNX.4.61.0606082220260.7314@enigma.local>
References: <Pine.LNX.4.61.0606082220260.7314@enigma.local>
Message-ID: <40e66e0b0606090041h27569da9l1809718ed52f903f@mail.gmail.com>

On 6/9/06, Rafael A. Irizarry <ririzarr at jhsph.edu> wrote:
> Hi!
>
> I have used the Matrix package (Version: 0.995-10) successfully
> to obtain the OLS solution for a problem where the design matrix X is
> 44000x6000. X is very sparse (about 80000 non-zeros elements).
>
> Now I want to do WLS: (X'WX)^-1X'Wy
>
> I tried W=Diagonal(length(w),w) and
> wX=solve(X,W)
>
> but after various minutes R gives a not enough
> memory error (Im using a 64bit machine with 16Gigs of RAM).

That's an interesting question, Rafael, and very timely.  I happen to
be visiting Martin Maechler this week and he mentioned to me just a
few minutes ago that we should add the capability for sparse least
squares calculations to the Matrix package.

Just for clarification, did you really mean wX = solve(X, W) above or
were you thinking of wX = crossprod(X, W)?

I would suggest storing the square root of the diagonal matrix W as a
sparse matrix

> w <- abs(rnorm(88000))
> ind <- seq(along = w) - 1:1
> W <- as(new("dgTMatrix", i = ind, j = ind, x = sqrt(w), Dim = c(length(w), length(w))), "dgCMatrix")
> str(W)
Formal class 'dgCMatrix' [package "Matrix"] with 6 slots
  ..@ i       : int [1:88000] 0 1 2 3 4 5 6 7 8 9 ...
  ..@ p       : int [1:88001] 0 1 2 3 4 5 6 7 8 9 ...
  ..@ Dim     : int [1:2] 88000 88000
  ..@ Dimnames:List of 2
  .. ..$ : NULL
  .. ..$ : NULL
  ..@ x       : num [1:88000] 0.712 0.989 1.032 0.348 0.573 ...
  ..@ factors : list()

(The 1:1 expression in the calculation of ind is a cheap trick to get
an integer value of 1 so that ind stays integer).  Now you should be
able to create wX <- W %*% X and wy <- W %*% y very quickly.



> I ended up doing this:
> wX=Matrix(as.matrix(X)*sqrt(w),sparse=TRUE)
> coefs1=as.vector(solve(crossprod(wX),crossprod(X,w*y)))
>
> which takes about 1-2 minutes, but it seems a better way, using the
> diagonal matrix, should exist. If there is I'd appreciate hearing it. If
> not, Im happy waiting 1-2 minute x #of iters.
>
>
> Thanks,
> Rafael
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From wemano at gmail.com  Fri Jun  9 09:59:05 2006
From: wemano at gmail.com (Wagner Costa)
Date: Fri, 9 Jun 2006 17:59:05 +1000
Subject: [R] script for multi linear regression
Message-ID: <70289c540606090059j43b8747bs43f5e2ab8c1a437f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060609/6227b884/attachment.pl 

From chris at psyctc.org  Fri Jun  9 09:59:35 2006
From: chris at psyctc.org (Chris Evans)
Date: Fri, 09 Jun 2006 08:59:35 +0100
Subject: [R] Re-binning histogram data
In-Reply-To: <20060608235353.GB23457@phenix.progiciels-bpi.ca>
References: <44883F89.11852.12BB1FC@localhost>	<001c01c68b13$6c787170$295afea9@gne.windows.gene.com>
	<20060608235353.GB23457@phenix.progiciels-bpi.ca>
Message-ID: <44892A67.7090600@psyctc.org>

Fran?ois Pinard sent the following  at 09/06/2006 00:53:
> [Berton Gunter]
> 
>> I would argue that histograms are outdated relics and that density  
>> plots (whatever your favorite flavor is) should **always** be used 
>> instead these days.
> 
> When a now retired researcher paid us a visit, I showed him a density 
> plot produced by R over some data he did work a lot, before he left.
> I, too, find them rather sexy, and I wanted to impress him with some of 
> the pleasures of R, especially knowing he has been a dedicated user of 
> SAS in his times.  Yet, this old and wise man _immediately_ caught that 
> the density curve was leaking a tiny bit through the extrema.
> 
> Not a big deal of course -- and he did like what he saw.  Nevertheless, 

... rest snipped ...

I did like Francois's post very much and confess I'm not very familiar
with density plots and use histograms a lot still.  However, I'm not a
statistician, though like to think I'm not a complete Luddite.

Rather naive question: doesn't this depend a bit on whether you see
yourself as describing the sample or describing the (inferred)
population.  It's intrigued me, much though I think the developing
graphical methods of data exploration are wonderful, that I think that
distinction between sample and population is not made as clearly for
graphical methods as perhaps it would be if the presentation were
textual.  Perhaps that's because it's often implicitly pretty clear, for
example, boxplots and histograms, with inevitable problems, describing
samples, some density plots at least, implicitly describing populations.

I know there's an argument that only the inferences (and their CIs)
about the population are statistics and the rest is accountancy but I am
not happy with that idea!

I'd be interested to hear others' views even if we are rather OTT (Off
The Topic, not Over The Top) here.  Perhaps I'm completely wrong?

Thanks to all for their posts, as ever, I'm learning much.

Chris

-- 
Chris Evans <chris at psyctc.org>
Hon. Professor of Psychotherapy, Nottingham University;
Consultant Psychiatrist in Psychotherapy, Rampton Hospital;
Research Programmes Director, Nottinghamshire NHS Trust;
Hon. SL Institute of Psychiatry, Hon. Con., Tavistock & Portman Trust
**If I am writing from one of those roles, it will be clear. Otherwise**

**my views are my own and not representative of those institutions    **


From roebuck at mdanderson.org  Fri Jun  9 10:18:00 2006
From: roebuck at mdanderson.org (Paul Roebuck)
Date: Fri, 9 Jun 2006 03:18:00 -0500 (CDT)
Subject: [R] make check errors for  R-2.3.1
In-Reply-To: <20060608224549.1451.qmail@web35502.mail.mud.yahoo.com>
References: <20060608224549.1451.qmail@web35502.mail.mud.yahoo.com>
Message-ID: <Pine.OSF.4.58.0606090310300.320044@wotan.mdacc.tmc.edu>

On Thu, 8 Jun 2006, Luo Weijun wrote:

> I tried to build R-2.3.1 from source under Mac OS X
> 10.4.6. (I am doing so because only this way I can get
> the 64-bit version of R)
>
> The configure and make steps look fine. But I got
> errors when I did make check-all, here is the message:
>
> running code in 'base-Ex.R' ...make[4]: ***
> [base-Ex.Rout] Error 1
> make[3]: *** [test-Examples-Base] Error 2
> make[2]: *** [test-Examples] Error 2
> make[1]: *** [test-all-basics] Error 1
> make: *** [check-all] Error 2
>
> I am not sure what does this mean.
> One potentially related issue is that, I downloaded
> and installed a gfortran compiler: GNU Fortran 95
> (GCC) 4.2.0 20060512 (experimental), because we don??t
> have one originally. But I didn??t use the latest
> cctools, and our version is: Apple Computer, Inc.
> version cctools-590.23.2.obj~17. Not sure whether this
> matters. Our gcc compiler is:
> powerpc-apple-darwin8-gcc-4.0.1 (GCC) 4.0.1 (Apple
> Computer, Inc. build 5250).

See "Tools" section of <http://r.research.att.com/>
for GCC. R-GUI installer contains needed Fortran compiler.
See also <http://wiki.urbanek.info/> for additional
Mac OS X build/compilation issues.

This should properly be discussed on R SIG Mac rather
than R Help.

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)


From jholtman at gmail.com  Fri Jun  9 11:22:49 2006
From: jholtman at gmail.com (jim holtman)
Date: Fri, 9 Jun 2006 05:22:49 -0400
Subject: [R] script for multi linear regression
In-Reply-To: <70289c540606090059j43b8747bs43f5e2ab8c1a437f@mail.gmail.com>
References: <70289c540606090059j43b8747bs43f5e2ab8c1a437f@mail.gmail.com>
Message-ID: <644e1f320606090222i420af0d0t988cc1f757c287bd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060609/2536f147/attachment.pl 

From john.seers at bbsrc.ac.uk  Fri Jun  9 11:58:16 2006
From: john.seers at bbsrc.ac.uk (john seers (IFR))
Date: Fri, 9 Jun 2006 10:58:16 +0100
Subject: [R] HTML nsmall vector format problem
Message-ID: <1CF0B26CECD746438AE02DBF7DDE1C7B03055E5E@ifre2ksrv1.ifrxp.bbsrc.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060609/54076d91/attachment.pl 

From avilella at gmail.com  Fri Jun  9 12:26:26 2006
From: avilella at gmail.com (Albert Vilella)
Date: Fri, 09 Jun 2006 11:26:26 +0100
Subject: [R] barplot dataframes w/ varying dimensions
Message-ID: <1149848786.5812.0.camel@localhost>

Hi all,

I would like to do a barplot of a dataframe like this one:

       alfa   beta gamma delta
 qwert   56.5  58.5 56.5  58.5
 asdfg   73.0  73.0 43.0  73.0
 zxcvb   63.0  63.0 43.0  63.0
 yuiop   63.0  63.0 43.0  63.0

with the labels of the rows and columns.

I would like to have something that works for dataframes with varying
dimensions, and so far I haven't found any way to do it.

What would be the best way to do that?

Thanks in advance,

Albert.


From tshort at eprisolutions.com  Fri Jun  9 12:53:37 2006
From: tshort at eprisolutions.com (tshort)
Date: Fri, 9 Jun 2006 03:53:37 -0700 (PDT)
Subject: [R] HTML nsmall vector format problem
In-Reply-To: <1CF0B26CECD746438AE02DBF7DDE1C7B03055E5E@ifre2ksrv1.ifrxp.bbsrc.ac.uk>
References: <1CF0B26CECD746438AE02DBF7DDE1C7B03055E5E@ifre2ksrv1.ifrxp.bbsrc.ac.uk>
Message-ID: <4791061.post@talk.nabble.com>


John,

I don't think nsmall uses a vector. Try the following with format (which
HTML.data.frame uses):

> format(iris[1:2,1:2],nsmall=c(3,1))
  Sepal.Length Sepal.Width
1        5.100       3.500
2        4.900       3.000

It looks like you'll have to do a format column by column with a loop.

- Tom


john seers (IFR) wrote:
> 
>  
> Hello All
>  
> I am having a bit of trouble formatting my HTML with the desired number
> of digits after the decimal place. Am I doing something
> wrong/misunderstanding or is it a bug?
>  
> Looking at the example supplied with ?HTML.data.frame:
>  
>  HTML(iris[1:2,1:2],nsmall=c(3,1),file="")
> 
> Gives html output that includes the lines:
>  
>  </tr> <tr><td class=firstcolumn>1</td><td
> class=cellinside>5.100</td><td class=cellinside>3.500</td></tr>
>  <tr><td class=firstcolumn>2</td><td class=cellinside>4.900</td><td
> class=cellinside>3.000</td></tr>
> 
> My understanding of how nsmall works, as a vector, the output should be
> something like:
>  
> </tr> <tr><td class=firstcolumn>1</td><td class=cellinside>5.100</td><td
> class=cellinside>3.5</td></tr>
>  <tr><td class=firstcolumn>2</td><td class=cellinside>4.900</td><td
> class=cellinside>3.0</td></tr>
> 
> i.e. first column with 3 digits after the decimal place and the second
> column with 1 digit after the decimal place.
>  
> It appears to only use the first value in the vector.
>  
> Has anybody got any suggestions?
>  
> Thanks for any help.
>  
> John Seers
>  
>  
>  
> ---
> 
> John Seers
> Institute of Food Research
> Norwich Research Park
> Colney
> Norwich
> NR4 7UA
>  
> 
> tel +44 (0)1603 251497
> fax +44 (0)1603 507723
> e-mail john.seers at bbsrc.ac.uk                         
> e-disclaimer at http://www.ifr.ac.uk/edisclaimer/
> <http://www.ifr.ac.uk/edisclaimer/>  
>  
> Web sites:
> 
> www.ifr.ac.uk <http://www.ifr.ac.uk/>    
> www.foodandhealthnetwork.com <http://www.foodandhealthnetwork.com/> 
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
--
View this message in context: http://www.nabble.com/HTML-nsmall-vector-format-problem-t1760896.html#a4791061
Sent from the R help forum at Nabble.com.


From antonio.raju at gmail.com  Fri Jun  9 12:53:57 2006
From: antonio.raju at gmail.com (antonio rodriguez)
Date: Fri, 09 Jun 2006 12:53:57 +0200
Subject: [R] appending arrays
Message-ID: <44895345.5030005@ono.com>

Hi All,

I have 2 arrays:

dim(a1)
[1] 3 23 23
dim(a2)
[1] 3 23 23

And I want a new array, to say, a3, where:

dim(a3)
[1] 6 23 23

where the first dimension is supposed to be (months) so the resultating 
array would start in jan and finish in june.

Best regards,

Antonio


From MSchwartz at mn.rr.com  Fri Jun  9 13:04:42 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 09 Jun 2006 06:04:42 -0500
Subject: [R] barplot dataframes w/ varying dimensions
In-Reply-To: <1149848786.5812.0.camel@localhost>
References: <1149848786.5812.0.camel@localhost>
Message-ID: <1149851082.5374.11.camel@localhost.localdomain>

On Fri, 2006-06-09 at 11:26 +0100, Albert Vilella wrote:
> Hi all,
> 
> I would like to do a barplot of a dataframe like this one:
> 
>        alfa   beta gamma delta
>  qwert   56.5  58.5 56.5  58.5
>  asdfg   73.0  73.0 43.0  73.0
>  zxcvb   63.0  63.0 43.0  63.0
>  yuiop   63.0  63.0 43.0  63.0
> 
> with the labels of the rows and columns.
> 
> I would like to have something that works for dataframes with varying
> dimensions, and so far I haven't found any way to do it.
> 
> What would be the best way to do that?
> 
> Thanks in advance,
> 
> Albert.

barplot() requires the 'height' argument to be a vector or matrix, so
you need to coerce the data frame:

  barplot(as.matrix(DF))

or

  barplot(as.matrix(DF), beside = TRUE)

depending upon the format you prefer.

HTH,

Marc Schwartz


From MSchwartz at mn.rr.com  Fri Jun  9 13:14:34 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 09 Jun 2006 06:14:34 -0500
Subject: [R] barplot dataframes w/ varying dimensions
In-Reply-To: <1149851082.5374.11.camel@localhost.localdomain>
References: <1149848786.5812.0.camel@localhost>
	<1149851082.5374.11.camel@localhost.localdomain>
Message-ID: <1149851675.5374.18.camel@localhost.localdomain>

On Fri, 2006-06-09 at 06:05 -0500, Marc Schwartz wrote:
> On Fri, 2006-06-09 at 11:26 +0100, Albert Vilella wrote:
> > Hi all,
> > 
> > I would like to do a barplot of a dataframe like this one:
> > 
> >        alfa   beta gamma delta
> >  qwert   56.5  58.5 56.5  58.5
> >  asdfg   73.0  73.0 43.0  73.0
> >  zxcvb   63.0  63.0 43.0  63.0
> >  yuiop   63.0  63.0 43.0  63.0
> > 
> > with the labels of the rows and columns.
> > 
> > I would like to have something that works for dataframes with varying
> > dimensions, and so far I haven't found any way to do it.
> > 
> > What would be the best way to do that?
> > 
> > Thanks in advance,
> > 
> > Albert.
> 
> barplot() requires the 'height' argument to be a vector or matrix, so
> you need to coerce the data frame:
> 
>   barplot(as.matrix(DF))
> 
> or
> 
>   barplot(as.matrix(DF), beside = TRUE)
> 
> depending upon the format you prefer.

I forgot to note that you will get the colnames(DF) to label the
groupings by default, but to get the rownames(DF) as well, you might do
that with a legend:

  barplot(as.matrix(DF), legend.text = rownames(DF), 
          ylim = c(0, max(colSums(DF)) * 1.4))

or

  barplot(as.matrix(DF), beside = TRUE, legend.text = rownames(DF), 
          ylim = c(0, max(DF) * 1.4))


Note that I have adjusted the range of the y axis in each case to make
room for the legend in the upper right hand corner.

You would have more flexibility in legend formatting by using legend()
separately. See ?legend for more information.

HTH,

Marc


From antonio.raju at gmail.com  Fri Jun  9 13:16:33 2006
From: antonio.raju at gmail.com (antonio rodriguez)
Date: Fri, 09 Jun 2006 13:16:33 +0200
Subject: [R] appending arrays
Message-ID: <44895891.80501@gmail.com>

Hi All,

I have 2 arrays:

dim(a1)
[1] 3 23 23
dim(a2)
[1] 3 23 23

And I want a new array, to say, a3, where:

dim(a3)
[1] 6 23 23

where the first dimension is supposed to be (months) so the resultating
array would start in jan and finish in june.

Best regards,

Antonio


-- 
=====
Por favor, si me mandas correos con copia a varias personas,
pon mi direcci?n de correo en copia oculta (CCO), para evitar
que acabe en montones de sitios, eliminando mi privacidad,
favoreciendo la propagaci?n de virus y la proliferaci?n del SPAM. Gracias.
-----
If you send me e-mail which has also been sent to several other people,
kindly mark my address as blind-carbon-copy (or BCC), to avoid its
distribution, which affects my privacy, increases the likelihood of
spreading viruses, and leads to more SPAM. Thanks.
=====


From john.seers at bbsrc.ac.uk  Fri Jun  9 13:18:42 2006
From: john.seers at bbsrc.ac.uk (john seers (IFR))
Date: Fri, 9 Jun 2006 12:18:42 +0100
Subject: [R] HTML nsmall vector format problem
Message-ID: <1CF0B26CECD746438AE02DBF7DDE1C7B03055E5F@ifre2ksrv1.ifrxp.bbsrc.ac.uk>



Hi Tom

Thanks for the reply.

I see what you are saying - that format does not format using an nsmall
vector, though the documentation (of HTML.data.frame) and the example
suggest nsmall uses a vector.

Even if I went through and changed the columns with a loop the
HTML.data.frame would reformat them according to its formatting so I
would still get all the columns with one value of nsmall. 

Specifically I want 0 for my first three columns and 4 for the remaining
columns in my data frame. (I would also like to control the widths but I
guess I may run into the same problem).

I could reprocess the HTML output but that makes generating the HTML a
bit redundant!


Regards

John










 
---

John Seers
Institute of Food Research
Norwich Research Park
Colney
Norwich
NR4 7UA
 

tel +44 (0)1603 251497
fax +44 (0)1603 507723
e-mail john.seers at bbsrc.ac.uk                         
e-disclaimer at http://www.ifr.ac.uk/edisclaimer/ 
 
Web sites:

www.ifr.ac.uk   
www.foodandhealthnetwork.com


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of tshort
Sent: 09 June 2006 11:54
To: r-help at stat.math.ethz.ch
Subject: Re: [R] HTML nsmall vector format problem



John,

I don't think nsmall uses a vector. Try the following with format (which
HTML.data.frame uses):

> format(iris[1:2,1:2],nsmall=c(3,1))
  Sepal.Length Sepal.Width
1        5.100       3.500
2        4.900       3.000

It looks like you'll have to do a format column by column with a loop.

- Tom


john seers (IFR) wrote:
> 
>  
> Hello All
>  
> I am having a bit of trouble formatting my HTML with the desired
number
> of digits after the decimal place. Am I doing something
> wrong/misunderstanding or is it a bug?
>  
> Looking at the example supplied with ?HTML.data.frame:
>  
>  HTML(iris[1:2,1:2],nsmall=c(3,1),file="")
> 
> Gives html output that includes the lines:
>  
>  </tr> <tr><td class=firstcolumn>1</td><td
> class=cellinside>5.100</td><td class=cellinside>3.500</td></tr>
>  <tr><td class=firstcolumn>2</td><td class=cellinside>4.900</td><td
> class=cellinside>3.000</td></tr>
> 
> My understanding of how nsmall works, as a vector, the output should
be
> something like:
>  
> </tr> <tr><td class=firstcolumn>1</td><td
class=cellinside>5.100</td><td
> class=cellinside>3.5</td></tr>
>  <tr><td class=firstcolumn>2</td><td class=cellinside>4.900</td><td
> class=cellinside>3.0</td></tr>
> 
> i.e. first column with 3 digits after the decimal place and the second
> column with 1 digit after the decimal place.
>  
> It appears to only use the first value in the vector.
>  
> Has anybody got any suggestions?
>  
> Thanks for any help.
>  
> John Seers
>  
>  
>  
> ---
> 
> John Seers
> Institute of Food Research
> Norwich Research Park
> Colney
> Norwich
> NR4 7UA
>  
> 
> tel +44 (0)1603 251497
> fax +44 (0)1603 507723
> e-mail john.seers at bbsrc.ac.uk                         
> e-disclaimer at http://www.ifr.ac.uk/edisclaimer/
> <http://www.ifr.ac.uk/edisclaimer/>  
>  
> Web sites:
> 
> www.ifr.ac.uk <http://www.ifr.ac.uk/>    
> www.foodandhealthnetwork.com <http://www.foodandhealthnetwork.com/> 
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
--
View this message in context:
http://www.nabble.com/HTML-nsmall-vector-format-problem-t1760896.html#a4
791061
Sent from the R help forum at Nabble.com.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From petr.pikal at precheza.cz  Fri Jun  9 13:24:43 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 09 Jun 2006 13:24:43 +0200
Subject: [R] barplot dataframes w/ varying dimensions
In-Reply-To: <1149848786.5812.0.camel@localhost>
Message-ID: <4489769B.30831.12BDE4B@localhost>

Hi

something like
> tab
      alfa beta gamma delta
qwert 56.5 58.5  56.5  58.5
asdfg 73.0 73.0  43.0  73.0
zxcvb 63.0 63.0  43.0  63.0
yuiop 63.0 63.0  43.0  63.0


barplot(as.matrix(tab), beside=T, legend.text=T)

HTH
Petr

On 9 Jun 2006 at 11:26, Albert Vilella wrote:

From:           	Albert Vilella <avilella at gmail.com>
To:             	r-help at stat.math.ethz.ch
Date sent:      	Fri, 09 Jun 2006 11:26:26 +0100
Subject:        	[R] barplot dataframes w/ varying dimensions
Send reply to:  	avilella at gmail.com
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>

> Hi all,
> 
> I would like to do a barplot of a dataframe like this one:
> 
>        alfa   beta gamma delta
>  qwert   56.5  58.5 56.5  58.5
>  asdfg   73.0  73.0 43.0  73.0
>  zxcvb   63.0  63.0 43.0  63.0
>  yuiop   63.0  63.0 43.0  63.0
> 
> with the labels of the rows and columns.
> 
> I would like to have something that works for dataframes with varying
> dimensions, and so far I haven't found any way to do it.
> 
> What would be the best way to do that?
> 
> Thanks in advance,
> 
> Albert.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From TShort at eprisolutions.com  Fri Jun  9 13:36:25 2006
From: TShort at eprisolutions.com (Short, Tom)
Date: Fri, 9 Jun 2006 07:36:25 -0400
Subject: [R] HTML nsmall vector format problem
Message-ID: <53BD3325330AEE4886960959AB8926FF013AD90A@esiexknx01.esi.local>

John, you can use format ahead of time (this converts to character
columns, so HTML won't reformat), and then use HTML:

> z=format(iris[1:2,1:2],nsmall=3) 
> z[,2]=format(iris[1:2,2],nsmall=1) 
> HTML(z,file="")

- Tom
 
-----Original Message-----
From: john seers (IFR) [mailto:john.seers at bbsrc.ac.uk] 
Sent: Friday, June 09, 2006 7:19 AM
To: Short, Tom; r-help at stat.math.ethz.ch
Subject: RE: [R] HTML nsmall vector format problem



Hi Tom

Thanks for the reply.

I see what you are saying - that format does not format using an nsmall
vector, though the documentation (of HTML.data.frame) and the example
suggest nsmall uses a vector.

Even if I went through and changed the columns with a loop the
HTML.data.frame would reformat them according to its formatting so I
would still get all the columns with one value of nsmall. 

Specifically I want 0 for my first three columns and 4 for the remaining
columns in my data frame. (I would also like to control the widths but I
guess I may run into the same problem).

I could reprocess the HTML output but that makes generating the HTML a
bit redundant!


Regards

John










 
---

John Seers
Institute of Food Research
Norwich Research Park
Colney
Norwich
NR4 7UA
 

tel +44 (0)1603 251497
fax +44 (0)1603 507723
e-mail john.seers at bbsrc.ac.uk                         
e-disclaimer at http://www.ifr.ac.uk/edisclaimer/ 
 
Web sites:

www.ifr.ac.uk   
www.foodandhealthnetwork.com


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of tshort
Sent: 09 June 2006 11:54
To: r-help at stat.math.ethz.ch
Subject: Re: [R] HTML nsmall vector format problem



John,

I don't think nsmall uses a vector. Try the following with format (which
HTML.data.frame uses):

> format(iris[1:2,1:2],nsmall=c(3,1))
  Sepal.Length Sepal.Width
1        5.100       3.500
2        4.900       3.000

It looks like you'll have to do a format column by column with a loop.

- Tom


john seers (IFR) wrote:
> 
>  
> Hello All
>  
> I am having a bit of trouble formatting my HTML with the desired
number
> of digits after the decimal place. Am I doing something 
> wrong/misunderstanding or is it a bug?
>  
> Looking at the example supplied with ?HTML.data.frame:
>  
>  HTML(iris[1:2,1:2],nsmall=c(3,1),file="")
> 
> Gives html output that includes the lines:
>  
>  </tr> <tr><td class=firstcolumn>1</td><td 
> class=cellinside>5.100</td><td class=cellinside>3.500</td></tr>  
> <tr><td class=firstcolumn>2</td><td class=cellinside>4.900</td><td 
> class=cellinside>3.000</td></tr>
> 
> My understanding of how nsmall works, as a vector, the output should
be
> something like:
>  
> </tr> <tr><td class=firstcolumn>1</td><td
class=cellinside>5.100</td><td
> class=cellinside>3.5</td></tr>
>  <tr><td class=firstcolumn>2</td><td class=cellinside>4.900</td><td 
> class=cellinside>3.0</td></tr>
> 
> i.e. first column with 3 digits after the decimal place and the second

> column with 1 digit after the decimal place.
>  
> It appears to only use the first value in the vector.
>  
> Has anybody got any suggestions?
>  
> Thanks for any help.
>  
> John Seers
>  
>  
>  
> ---
> 
> John Seers
> Institute of Food Research
> Norwich Research Park
> Colney
> Norwich
> NR4 7UA
>  
> 
> tel +44 (0)1603 251497
> fax +44 (0)1603 507723
> e-mail john.seers at bbsrc.ac.uk                         
> e-disclaimer at http://www.ifr.ac.uk/edisclaimer/ 
> <http://www.ifr.ac.uk/edisclaimer/>
>  
> Web sites:
> 
> www.ifr.ac.uk <http://www.ifr.ac.uk/>    
> www.foodandhealthnetwork.com <http://www.foodandhealthnetwork.com/>
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
--
View this message in context:
http://www.nabble.com/HTML-nsmall-vector-format-problem-t1760896.html#a4
791061
Sent from the R help forum at Nabble.com.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From bolker at ufl.edu  Fri Jun  9 13:44:28 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Fri, 9 Jun 2006 11:44:28 +0000 (UTC)
Subject: [R] appending arrays
References: <44895345.5030005@ono.com>
Message-ID: <loom.20060609T134255-292@post.gmane.org>

antonio rodriguez <antonio.raju <at> gmail.com> writes:

> I have 2 arrays:
> 
> dim(a1)
> [1] 3 23 23
> dim(a2)
> [1] 3 23 23
> 
> And I want a new array, to say, a3, where:
> 
> dim(a3)
> [1] 6 23 23

   You can (1) figure out how to transpose the
arrays (?aperm), put them together with c(),
and re-array() them, *or* check out the
abind package:

install.packages("abind")
library(abind)
?abind

  Ben


From pkhomski at wiwi.uni-bielefeld.de  Fri Jun  9 14:24:11 2006
From: pkhomski at wiwi.uni-bielefeld.de (Pavel Khomski)
Date: Fri, 09 Jun 2006 14:24:11 +0200
Subject: [R] deal with R.package panel
Message-ID: <4489686B.6070508@wiwi.uni-bielefeld.de>

hello!

my question conserns with use of "panel" package (written by R.C.Gentlman)
(unfortunately the manual and help sites are very short)

1. is it possible to do analysis  just without a(ny) covariate? i 
suggest do it by introducing a covariate with level=0 in all 
obervations, this because of Q(z)=Q_o exp(beta*z),  but it seemingly 
doesn't work

2. in the option gamma in the call of panel function: do you mean an 
initial value for parameter vector gamma?
say if i have 3 theta-parameters, so i have to initialize 
gamma=c(xxx,xxx,xxx), correct?

3. are the (first ) observed times =0 allowed (in $time vectors) or 
schould in such a case begin with =1, if there are any?


thanks for your response

From murdoch at stats.uwo.ca  Fri Jun  9 14:38:49 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 09 Jun 2006 08:38:49 -0400
Subject: [R] Re-binning histogram data
In-Reply-To: <001c01c68b13$6c787170$295afea9@gne.windows.gene.com>
References: <001c01c68b13$6c787170$295afea9@gne.windows.gene.com>
Message-ID: <44896BD9.1070301@stats.uwo.ca>

On 6/8/2006 11:51 AM, Berton Gunter wrote:
> I would argue that histograms are outdated relics and that density plots
> (whatever your favorite flavor is) should **always** be used instead these
> days.

But my favourite density plot is a histogram!

I agree that computational complexity should weigh much less in the 
decision to do something than it used to.  But I'd say a histogram (with 
more bins than the R default) is a good input to my mental density 
estimator.   Adding a rug of points below it is helpful in small 
datasets.  It is very easy to see how much smoothing has been done; 
that's often hard to see in presentations of density plots produced in 
other ways.  It's also easier to recognize discrete atoms in the 
distribution:  they'll show up as isolated bars a lot higher than the usual.

For example, compare these two plots:

  set.seed(123)
  par(mfrow=c(2,1))
  x <- c(rnorm(1000), rbinom(100, 3, 0.5))
  hist(x, breaks=60)
  plot(density(x))

This isn't a fair comparison, since I used the default bandwidth on the 
smoother but not on the histogram (it would be fairer to compare to
plot(density(x,bw=0.05)) ), but I think it still illustrates my point: 
in the latter density plot where the atoms are clearly visible, I still 
need to read the text at the bottom to know the sample size and 
bandwidth, whereas I can see those at a glance in the histogram.  And an 
untrained user could get a lot of information out of the histogram, 
whereas they'd have a lot of trouble getting anything out of the density 
plots.

> 
> In this vein, I would appreciate critical rejoinders (public or private) to
> the following proposition: Given modern computer power and software like R
> on multi ghz machines, statistical and graphical relics of the pre-computer
> era (like histograms, low resolution printer-type plots, and perhaps even
> method of moments EMS calculations) should be abandoned in favor of superior
> but perhaps computation-intensive alternatives (like density plots, high
> resolution plots, and likelihood or resampling or Bayes based methods). 
> 
> NB: Please -- no pleadings that new methods would be mystifying to the
> non-cogniscenti. Following that to its logical conclusion would mean that
> we'd all have to give up our TV remotes and cell phones, and what kind of
> world would that be?! :-)

Now, if you were to suggest that the stem() function is a bizarre 
simulation of a stone-age tool on a modern computer, I might agree.

Duncan Murdoch

> 
> -- Bert Gunter
> 
>   
> 
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch 
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Petr Pikal
>> Sent: Thursday, June 08, 2006 6:17 AM
>> To: Justin Ashmall; r-help at stat.math.ethz.ch
>> Subject: Re: [R] Re-binning histogram data
>> 
>> 
>> 
>> On 8 Jun 2006 at 11:35, Justin Ashmall wrote:
>> 
>> Date sent:      	Thu, 8 Jun 2006 11:35:46 +0100 (BST)
>> From:           	Justin Ashmall <ja at space.mit.edu>
>> To:             	Petr Pikal <petr.pikal at precheza.cz>
>> Copies to:      	r-help at stat.math.ethz.ch
>> Subject:        	Re: [R] Re-binning histogram data
>> 
>> > 
>> > Thanks for the reply Petr,
>> > 
>> > It looks to me that truehist() needs a vector of data just like
>> > hist()? Whereas I have histogram-style input data? Am I missing
>> > something?
>> 
>> Well, maybe you could use barplot. Or as you suggested recreate the 
>> original vector and call hist or truehist with other bins.
>> 
>> > hhh<-hist(rnorm(1000))
>> > barplot(tapply(hhh$counts, c(rep(1:7,each=2),7), sum))
>> > tapply(hhh$mids, c(rep(1:7,each=2),7), mean)
>>     1     2     3     4     5     6     7 
>> -3.00 -2.00 -1.00  0.00  1.00  2.00  3.25 
>> > hhh1<-rep(hhh$mids,hhh$counts)
>> > plot(hhh, freq=F)
>> > lines(density(hhh1))
>> >
>> 
>> HTH
>> Petr
>> 
>> 
>> 
>> 
>> 
>> 
>> > 
>> > Cheers,
>> > 
>> > Justin
>> > 
>> > 
>> > 
>> > On Thu, 8 Jun 2006, Petr Pikal wrote:
>> > 
>> > > Hi
>> > >
>> > > try truehist from MASS package and look for argument breaks or h.
>> > >
>> > > HTH
>> > > Petr
>> > >
>> > >
>> > >
>> > >
>> > > On 8 Jun 2006 at 10:46, Justin Ashmall wrote:
>> > >
>> > > Date sent:      	Thu, 8 Jun 2006 10:46:19 +0100 (BST)
>> > > From:           	Justin Ashmall <ja at space.mit.edu>
>> > > To:             	r-help at stat.math.ethz.ch
>> > > Subject:        	[R] Re-binning histogram data
>> > >
>> > >> Hi,
>> > >>
>> > >> Short Version:
>> > >> Is there a function to re-bin a histogram to new, broader bins?
>> > >>
>> > >> Long version: I'm trying to create a histogram, however my
>> > >> input-data is itself in the form of a fine-grained 
>> histogram, i.e.
>> > >> numbers of counts in regular one-second bins. I want to produce a
>> > >> histogram of, say, 10-minute bins (though possibly irregular bins
>> > >> also).
>> > >>
>> > >> I suppose I could re-create a data set as expected by the hist()
>> > >> function (i.e. if time t=3600 has 6 counts, add six 
>> entries of 3600
>> > >> to a list) however this seems neither elegant nor 
>> efficient (though
>> > >> I'd be pleased to be mistaken!). I could then re-create 
>> a histogram
>> > >> as normal.
>> > >>
>> > >> I guessing there's a better solution however! Apologies 
>> if this is
>> > >> a basic question - I'm rather new to R and trying to get up to
>> > >> speed.
>> > >>
>> > >> Regards,
>> > >>
>> > >> Justin
>> > >>
>> > >> ______________________________________________
>> > >> R-help at stat.math.ethz.ch mailing list
>> > >> https://stat.ethz.ch/mailman/listinfo/r-help
>> > >> PLEASE do read the posting guide!
>> > >> http://www.R-project.org/posting-guide.html
>> > >
>> > > Petr Pikal
>> > > petr.pikal at precheza.cz
>> > >
>> > >
>> > 
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide!
>> > http://www.R-project.org/posting-guide.html
>> 
>> Petr Pikal
>> petr.pikal at precheza.cz
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From Pierre.Lapointe at nbf.ca  Fri Jun  9 14:51:47 2006
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Fri, 9 Jun 2006 08:51:47 -0400 
Subject: [R] sqlSave() and rownames=TRUE makes my Rgui crash
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D0DF35FCB@lbmsg002.fbn-nbf.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060609/b1ff48a5/attachment.pl 

From rkoenker at uiuc.edu  Fri Jun  9 14:57:46 2006
From: rkoenker at uiuc.edu (roger koenker)
Date: Fri, 9 Jun 2006 07:57:46 -0500
Subject: [R] Re-binning histogram data
In-Reply-To: <44896BD9.1070301@stats.uwo.ca>
References: <001c01c68b13$6c787170$295afea9@gne.windows.gene.com>
	<44896BD9.1070301@stats.uwo.ca>
Message-ID: <99FD2C45-41CE-4F72-9BA9-21A8689C857D@uiuc.edu>

On Jun 9, 2006, at 7:38 AM, Duncan Murdoch wrote:
>
> Now, if you were to suggest that the stem() function is a bizarre
> simulation of a stone-age tool on a modern computer, I might agree.
>

But as a stone-age (blackboard)  tool it is unsurpassed.  It is the only
bright spot in the usually depressing ritual  of returning exam
results.  Full disclosure of the distribution in a very concise  
encoding.

url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820


From john.seers at bbsrc.ac.uk  Fri Jun  9 14:58:32 2006
From: john.seers at bbsrc.ac.uk (john seers (IFR))
Date: Fri, 9 Jun 2006 13:58:32 +0100
Subject: [R] HTML nsmall vector format problem
Message-ID: <1CF0B26CECD746438AE02DBF7DDE1C7B03055E60@ifre2ksrv1.ifrxp.bbsrc.ac.uk>


Hi Tom

Excellent! Exactly what I need.

I see what you mean now. 

Thanks very much for your help.

John





 
---

John Seers
Institute of Food Research
Norwich Research Park
Colney
Norwich
NR4 7UA
 

tel +44 (0)1603 251497
fax +44 (0)1603 507723
e-mail john.seers at bbsrc.ac.uk                         
e-disclaimer at http://www.ifr.ac.uk/edisclaimer/ 
 
Web sites:

www.ifr.ac.uk   
www.foodandhealthnetwork.com


-----Original Message-----
From: Short, Tom [mailto:TShort at eprisolutions.com] 
Sent: 09 June 2006 12:36
To: john seers (IFR); r-help at stat.math.ethz.ch
Subject: RE: [R] HTML nsmall vector format problem


John, you can use format ahead of time (this converts to character
columns, so HTML won't reformat), and then use HTML:

> z=format(iris[1:2,1:2],nsmall=3) 
> z[,2]=format(iris[1:2,2],nsmall=1) 
> HTML(z,file="")

- Tom
 
-----Original Message-----
From: john seers (IFR) [mailto:john.seers at bbsrc.ac.uk] 
Sent: Friday, June 09, 2006 7:19 AM
To: Short, Tom; r-help at stat.math.ethz.ch
Subject: RE: [R] HTML nsmall vector format problem



Hi Tom

Thanks for the reply.

I see what you are saying - that format does not format using an nsmall
vector, though the documentation (of HTML.data.frame) and the example
suggest nsmall uses a vector.

Even if I went through and changed the columns with a loop the
HTML.data.frame would reformat them according to its formatting so I
would still get all the columns with one value of nsmall. 

Specifically I want 0 for my first three columns and 4 for the remaining
columns in my data frame. (I would also like to control the widths but I
guess I may run into the same problem).

I could reprocess the HTML output but that makes generating the HTML a
bit redundant!


Regards

John










 
---

John Seers
Institute of Food Research
Norwich Research Park
Colney
Norwich
NR4 7UA
 

tel +44 (0)1603 251497
fax +44 (0)1603 507723
e-mail john.seers at bbsrc.ac.uk                         
e-disclaimer at http://www.ifr.ac.uk/edisclaimer/ 
 
Web sites:

www.ifr.ac.uk   
www.foodandhealthnetwork.com


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of tshort
Sent: 09 June 2006 11:54
To: r-help at stat.math.ethz.ch
Subject: Re: [R] HTML nsmall vector format problem



John,

I don't think nsmall uses a vector. Try the following with format (which
HTML.data.frame uses):

> format(iris[1:2,1:2],nsmall=c(3,1))
  Sepal.Length Sepal.Width
1        5.100       3.500
2        4.900       3.000

It looks like you'll have to do a format column by column with a loop.

- Tom


john seers (IFR) wrote:
> 
>  
> Hello All
>  
> I am having a bit of trouble formatting my HTML with the desired
number
> of digits after the decimal place. Am I doing something 
> wrong/misunderstanding or is it a bug?
>  
> Looking at the example supplied with ?HTML.data.frame:
>  
>  HTML(iris[1:2,1:2],nsmall=c(3,1),file="")
> 
> Gives html output that includes the lines:
>  
>  </tr> <tr><td class=firstcolumn>1</td><td 
> class=cellinside>5.100</td><td class=cellinside>3.500</td></tr>  
> <tr><td class=firstcolumn>2</td><td class=cellinside>4.900</td><td 
> class=cellinside>3.000</td></tr>
> 
> My understanding of how nsmall works, as a vector, the output should
be
> something like:
>  
> </tr> <tr><td class=firstcolumn>1</td><td
class=cellinside>5.100</td><td
> class=cellinside>3.5</td></tr>
>  <tr><td class=firstcolumn>2</td><td class=cellinside>4.900</td><td 
> class=cellinside>3.0</td></tr>
> 
> i.e. first column with 3 digits after the decimal place and the second

> column with 1 digit after the decimal place.
>  
> It appears to only use the first value in the vector.
>  
> Has anybody got any suggestions?
>  
> Thanks for any help.
>  
> John Seers
>  
>  
>  
> ---
> 
> John Seers
> Institute of Food Research
> Norwich Research Park
> Colney
> Norwich
> NR4 7UA
>  
> 
> tel +44 (0)1603 251497
> fax +44 (0)1603 507723
> e-mail john.seers at bbsrc.ac.uk                         
> e-disclaimer at http://www.ifr.ac.uk/edisclaimer/ 
> <http://www.ifr.ac.uk/edisclaimer/>
>  
> Web sites:
> 
> www.ifr.ac.uk <http://www.ifr.ac.uk/>    
> www.foodandhealthnetwork.com <http://www.foodandhealthnetwork.com/>
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
--
View this message in context:
http://www.nabble.com/HTML-nsmall-vector-format-problem-t1760896.html#a4
791061
Sent from the R help forum at Nabble.com.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From stat700004 at yahoo.co.in  Fri Jun  9 15:11:54 2006
From: stat700004 at yahoo.co.in (stat stat)
Date: Fri, 9 Jun 2006 14:11:54 +0100 (BST)
Subject: [R] Date calculation
Message-ID: <20060609131154.12671.qmail@web8415.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060609/94775d4e/attachment.pl 

From d.d.sutcliffe at durham.ac.uk  Fri Jun  9 15:13:29 2006
From: d.d.sutcliffe at durham.ac.uk (d d sutcliffe@durham ac uk)
Date: Fri, 09 Jun 2006 14:13:29 +0100
Subject: [R] Rotate values on Y axis
Message-ID: <448973F9.2040305@durham.ac.uk>

Hi,

I have been working through one of the examples on the FAQ about 
rotating  the labels on the x axis, I need to do the same but for the y 
axis. I have managed to change some of the code, but I am still not 
getting there, there is still something wrong. My syntax is as follows:


 > par(mar = c(5, 7, 4, 2) + 0.1)
 > plot(1 : 8, yaxt = "n",  ylab = "")
 > axis(2, labels = FALSE)
 > labels <- paste("Label", 1:8, sep = " ")
 > text(1:8, par("usr")[3] - 0.25, srt = 45, adj = 1,
+  labels = labels, xpd = TRUE)

If anybody knows what is wrong then I would appreciate your help...been 
working on this for far too long already!

Regards,

Dan


From hugues_sj at yahoo.fr  Fri Jun  9 15:16:31 2006
From: hugues_sj at yahoo.fr (hugues)
Date: Fri, 9 Jun 2006 15:16:31 +0200
Subject: [R] generate random data
Message-ID: <200606091316.k59DGX2e016445@hypatia.math.ethz.ch>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060609/fb6720a7/attachment.pl 

From murdoch at stats.uwo.ca  Fri Jun  9 15:19:40 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 09 Jun 2006 09:19:40 -0400
Subject: [R] sqlSave() and rownames=TRUE makes my Rgui crash
In-Reply-To: <834204C0D7C6D611A3BB000255FC6E9D0DF35FCB@lbmsg002.fbn-nbf.local>
References: <834204C0D7C6D611A3BB000255FC6E9D0DF35FCB@lbmsg002.fbn-nbf.local>
Message-ID: <4489756C.90000@stats.uwo.ca>

On 6/9/2006 8:51 AM, Lapointe, Pierre wrote:
> Hello,
> 
> I created a table in MySQL with this command
> 
> CREATE TABLE example (pk INT NOT NULL AUTO_INCREMENT,PRIMARY KEY(pk),
>  id VARCHAR(30),col1 VARCHAR(30),col2 VARCHAR(30))
> 
> ### In R, I can connect to this table:
> 
> library(DBI)
> library(RODBC)
> chan <- odbcConnect("MySQL51", uid="root", pwd="xxx") 
> first <- sqlQuery(chan, "select * from example")
> close(chan)
> First
> #[1] pk   id   col1 col2
> #<0 rows> (or 0-length row.names)
> 
> ### This is the table I'm trying to save:
> dframe <-data.frame(matrix(1:6,2,3))
> colnames(dframe)=c("id","col1","col2")
> dframe
> #  id col1 col2
> #1  1    3    5
> #2  2    4    6
> 
> ### But this makes Rgui crash and close
> chan <- odbcConnect("MySQL51", uid="root", pwd="xxx")  
> sqlSave(chan, dframe, tablename="example", rownames = FALSE, append=T)
> close(chan)
> 
> ### With rownames = T and safer=F, it works, but I loose the
> autoincrementing PK in MySQL
> chan <- odbcConnect("MySQL51", uid="root", pwd="momie")  #default
> database=fbn
> sqlSave(chan, dframe, tablename="example", rownames = T,
> addPK=T,append=T,safer=F)
> close(chan)
> 
> Any idea?
> 
> I'm on win2K, MySQL version 5.0.21-community-nt

I don't know why you're using DBI; perhaps it interferes with RODBC somehow.

If that's not it, then you might want to try lower level methods than 
sqlSave:  perhaps use sqlQuery to send an INSERT command to the 
database.  Build up from there.

You might also want to look at the thread "Fast update of a lot of 
records in a database?" from around May 20, though it was talking about 
updates rather than insertions.

Duncan Murdoch


From MSchwartz at mn.rr.com  Fri Jun  9 15:27:37 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 09 Jun 2006 08:27:37 -0500
Subject: [R] Rotate values on Y axis
In-Reply-To: <448973F9.2040305@durham.ac.uk>
References: <448973F9.2040305@durham.ac.uk>
Message-ID: <1149859657.5374.28.camel@localhost.localdomain>

On Fri, 2006-06-09 at 14:13 +0100, d d sutcliffe at durham ac uk wrote:
> Hi,
> 
> I have been working through one of the examples on the FAQ about 
> rotating  the labels on the x axis, I need to do the same but for the y 
> axis. I have managed to change some of the code, but I am still not 
> getting there, there is still something wrong. My syntax is as follows:
> 
> 
>  > par(mar = c(5, 7, 4, 2) + 0.1)
>  > plot(1 : 8, yaxt = "n",  ylab = "")
>  > axis(2, labels = FALSE)
>  > labels <- paste("Label", 1:8, sep = " ")
>  > text(1:8, par("usr")[3] - 0.25, srt = 45, adj = 1,
> +  labels = labels, xpd = TRUE)
> 
> If anybody knows what is wrong then I would appreciate your help...been 
> working on this for far too long already!
> 
> Regards,
> 
> Dan

Dan,

You need to adjust two things:

1. par("usr")[3] is the minimum value of the y axis. You would use this
(as in the FAQ example) when you want to move the text vertically below
the x axis. See ?par for more information.

In your case, you want to move the text to the left of the y axis. Thus
you need to use par("usr")[1] instead, which is the minimum value of the
x axis.


2. In the text call, you are setting the 'x' coordinate to 1:8, which is
why the labels are appearing along the x axis, rather than along the y
axis. So you want to set the y coordinate to 1:8 instead.


So, the adjusted sequence would be:

 par(mar = c(5, 7, 4, 2) + 0.1)
 plot(1 : 8, yaxt = "n",  ylab = "")
 axis(2, labels = FALSE)
 labels <- paste("Label", 1:8, sep = " ")

 text(par("usr")[1] - 0.25, 1:8, srt = 45, adj = 1,
      labels = labels, xpd = TRUE)

HTH,

Marc Schwartz


From Pierre.Lapointe at nbf.ca  Fri Jun  9 15:42:57 2006
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Fri, 9 Jun 2006 09:42:57 -0400 
Subject: [R] sqlSave() and rownames=TRUE makes my Rgui crash
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D0DF35FCC@lbmsg002.fbn-nbf.local>


On 6/9/2006 8:51 AM, Lapointe, Pierre wrote:
> Hello,
> 
> I created a table in MySQL with this command
> 
> CREATE TABLE example (pk INT NOT NULL AUTO_INCREMENT,PRIMARY KEY(pk),  
> id VARCHAR(30),col1 VARCHAR(30),col2 VARCHAR(30))
> 
> ### In R, I can connect to this table:
> 
> library(DBI)
> library(RODBC)
> chan <- odbcConnect("MySQL51", uid="root", pwd="xxx")
> first <- sqlQuery(chan, "select * from example")
> close(chan)
> First
> #[1] pk   id   col1 col2
> #<0 rows> (or 0-length row.names)
> 
> ### This is the table I'm trying to save:
> dframe <-data.frame(matrix(1:6,2,3))
> colnames(dframe)=c("id","col1","col2")
> dframe
> #  id col1 col2
> #1  1    3    5
> #2  2    4    6
> 
> ### But this makes Rgui crash and close
> chan <- odbcConnect("MySQL51", uid="root", pwd="xxx")
> sqlSave(chan, dframe, tablename="example", rownames = FALSE, append=T)
> close(chan)
> 
> ### With rownames = T and safer=F, it works, but I loose the 
> autoincrementing PK in MySQL chan <- odbcConnect("MySQL51", 
> uid="root", pwd="momie")  #default database=fbn
> sqlSave(chan, dframe, tablename="example", rownames = T,
> addPK=T,append=T,safer=F)
> close(chan)
> 
> Any idea?
> 
> I'm on win2K, MySQL version 5.0.21-community-nt

>>I don't know why you're using DBI; perhaps it interferes with RODBC
somehow.

**It still crashes without DBI

>>If that's not it, then you might want to try lower level methods than 
>>sqlSave:  perhaps use sqlQuery to send an INSERT command to the 
>>database.  Build up from there.

**Good suggestion, however, I'm not sure how to pass a table through an sql
statement. From this archived doc,
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/10073.html I tried this
using a dataframe instead of a single number.

But I get this error:

#test
chan <- odbcConnect("MySQL51", uid="root", pwd="momie")  #default
database=fbn
query <- paste("INSERT INTO example VALUES ('",dframe,"')",sep="") 
sqlQuery(chan,query) 
close(chan)

[1] "[RODBC] ERROR: Could not SQLExecDirect"

[2] "S1T00 1136 [MySQL][ODBC 3.51 Driver][mysqld-5.0.21-community-nt]Column
count doesn't match value count at row 1"

>>You might also want to look at the thread "Fast update of a lot of 
>>records in a database?" from around May 20, though it was talking about 
>>updates rather than insertions.

Duncan Murdoch

**************************************************
AVIS DE NON-RESPONSABILITE: Ce document transmis par courrie...{{dropped}}


From jacques.veslot at good.ibl.fr  Fri Jun  9 15:45:17 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Fri, 09 Jun 2006 15:45:17 +0200
Subject: [R] Date calculation
In-Reply-To: <20060609131154.12671.qmail@web8415.mail.in.yahoo.com>
References: <20060609131154.12671.qmail@web8415.mail.in.yahoo.com>
Message-ID: <44897B6D.4070002@good.ibl.fr>

 > test
          V1    V2
1  5/2/2006 36560
2  5/3/2006 36538
3  5/4/2006 36452
4  5/5/2006 36510
5  5/8/2006 36485
6  5/9/2006 36502
7 5/10/2006 36584
8 5/11/2006 36571

 > dates <- strptime(test$V1, "%d/%m/%Y")

-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


stat stat a ?crit :
> Dear all R users,
>    
>   Suppose I have a data frame "data" like this:
>    
>   5/2/2006      36560
>  5/3/2006     36538
>  5/4/2006     36452
>  5/5/2006     36510
>  5/8/2006     36485
>  5/9/2006     36502
>  5/10/2006     36584
>  5/11/2006    36571
>    
>   Now I want to create a for loop like this:
>    
>   date = "5/10/2006"
> for (i in 1: 8)
>    {
>     if (data[i,1] > date) break
>    }
>    
>   But I get error while executing this. Can anyone tell me the right way to do this?
>    
>   Sincerely yours
>   stat
> 
>  Send instant messages to your online friends http://in.messenger.yahoo.com 
> 
>  Stay connected with your friends even when away from PC.  Link: http://in.mobile.yahoo.com/new/messenger/  
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Fri Jun  9 15:47:12 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 9 Jun 2006 09:47:12 -0400
Subject: [R] Date calculation
In-Reply-To: <20060609131154.12671.qmail@web8415.mail.in.yahoo.com>
References: <20060609131154.12671.qmail@web8415.mail.in.yahoo.com>
Message-ID: <971536df0606090647r58c21a85mb5d750c9b397af5f@mail.gmail.com>

Try this:

Lines <- "5/2/2006      36560
 5/3/2006     36538
 5/4/2006     36452
 5/5/2006     36510
 5/8/2006     36485
 5/9/2006     36502
 5/10/2006     36584
 5/11/2006    36571"

DF <- read.table(textConnection(Lines), as.is = TRUE)
fmt <- "%m/%d/%Y"
DF[,1] <- as.Date(DF[,1], fmt)
date <- as.Date("5/10/2006", fmt)
which.max(DF[,1] > date)


On 6/9/06, stat stat <stat700004 at yahoo.co.in> wrote:
> Dear all R users,
>
>  Suppose I have a data frame "data" like this:
>
>  5/2/2006      36560
>  5/3/2006     36538
>  5/4/2006     36452
>  5/5/2006     36510
>  5/8/2006     36485
>  5/9/2006     36502
>  5/10/2006     36584
>  5/11/2006    36571
>
>  Now I want to create a for loop like this:
>
>  date = "5/10/2006"
> for (i in 1: 8)
>   {
>    if (data[i,1] > date) break
>   }
>
>  But I get error while executing this. Can anyone tell me the right way to do this?
>
>  Sincerely yours
>  stat
>
>  Send instant messages to your online friends http://in.messenger.yahoo.com
>
>  Stay connected with your friends even when away from PC.  Link: http://in.mobile.yahoo.com/new/messenger/
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From murdoch at stats.uwo.ca  Fri Jun  9 15:53:49 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 09 Jun 2006 09:53:49 -0400
Subject: [R] sqlSave() and rownames=TRUE makes my Rgui crash
In-Reply-To: <834204C0D7C6D611A3BB000255FC6E9D0DF35FCC@lbmsg002.fbn-nbf.local>
References: <834204C0D7C6D611A3BB000255FC6E9D0DF35FCC@lbmsg002.fbn-nbf.local>
Message-ID: <44897D6D.2080708@stats.uwo.ca>

On 6/9/2006 9:42 AM, Lapointe, Pierre wrote:
> On 6/9/2006 8:51 AM, Lapointe, Pierre wrote:
>> Hello,
>> 
>> I created a table in MySQL with this command
>> 
>> CREATE TABLE example (pk INT NOT NULL AUTO_INCREMENT,PRIMARY KEY(pk),  
>> id VARCHAR(30),col1 VARCHAR(30),col2 VARCHAR(30))
>> 
>> ### In R, I can connect to this table:
>> 
>> library(DBI)
>> library(RODBC)
>> chan <- odbcConnect("MySQL51", uid="root", pwd="xxx")
>> first <- sqlQuery(chan, "select * from example")
>> close(chan)
>> First
>> #[1] pk   id   col1 col2
>> #<0 rows> (or 0-length row.names)
>> 
>> ### This is the table I'm trying to save:
>> dframe <-data.frame(matrix(1:6,2,3))
>> colnames(dframe)=c("id","col1","col2")
>> dframe
>> #  id col1 col2
>> #1  1    3    5
>> #2  2    4    6
>> 
>> ### But this makes Rgui crash and close
>> chan <- odbcConnect("MySQL51", uid="root", pwd="xxx")
>> sqlSave(chan, dframe, tablename="example", rownames = FALSE, append=T)
>> close(chan)
>> 
>> ### With rownames = T and safer=F, it works, but I loose the 
>> autoincrementing PK in MySQL chan <- odbcConnect("MySQL51", 
>> uid="root", pwd="momie")  #default database=fbn
>> sqlSave(chan, dframe, tablename="example", rownames = T,
>> addPK=T,append=T,safer=F)
>> close(chan)
>> 
>> Any idea?
>> 
>> I'm on win2K, MySQL version 5.0.21-community-nt
> 
>>>I don't know why you're using DBI; perhaps it interferes with RODBC
> somehow.
> 
> **It still crashes without DBI
> 
>>>If that's not it, then you might want to try lower level methods than 
>>>sqlSave:  perhaps use sqlQuery to send an INSERT command to the 
>>>database.  Build up from there.
> 
> **Good suggestion, however, I'm not sure how to pass a table through an sql
> statement. From this archived doc,
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/10073.html I tried this
> using a dataframe instead of a single number.

You can't.  You can only insert one record at a time this way in 
general, but MySQL allows multiple inserts on one line.  So it's a lot 
of work, but you might figure out what's causing your crash.


> 
> But I get this error:
> 
> #test
> chan <- odbcConnect("MySQL51", uid="root", pwd="momie")  #default
> database=fbn
> query <- paste("INSERT INTO example VALUES ('",dframe,"')",sep="") 
> sqlQuery(chan,query) 
> close(chan)

You'd want something like

inserts <- with(dframe, paste("('", id, "','", col1, "','", col2, "')", 
sep="", collapse=",")
query <- paste("INSERT INTO example(id, col1, col2) VALUES", inserts)
sqlQuery(chan, query)

(This isn't even tested to see if I got the syntax right, and it's 
probably not legal syntax on other databases.  For those you could put
together multiple  INSERT statements.)

Duncan Murdoch

> [1] "[RODBC] ERROR: Could not SQLExecDirect"
> 
> [2] "S1T00 1136 [MySQL][ODBC 3.51 Driver][mysqld-5.0.21-community-nt]Column
> count doesn't match value count at row 1"
> 
>>>You might also want to look at the thread "Fast update of a lot of 
>>>records in a database?" from around May 20, though it was talking about 
>>>updates rather than insertions.
> 
> Duncan Murdoch
> 
> **************************************************
> AVIS DE NON-RESPONSABILITE: Ce document transmis par courrier electronique est destine uniquement a la personne ou a l'entite a qui il est adresse et peut contenir des renseignements confidentiels et assujettis au secret professionnel. La confidentialite et le secret professionnel demeurent malgre l'envoi de ce document a la mauvaise adresse electronique. Si vous n'etes pas le destinataire vise ou la personne chargee de remettre ce document a son destinataire, veuillez nous en informer sans delai et detruire ce document ainsi que toute copie qui en aurait ete faite. Toute distribution, reproduction ou autre utilisation de ce document est strictement interdite. De plus, le Groupe Financiere Banque Nationale et ses filiales ne peuvent pas etre tenus responsables des dommages pouvant etre causes par des virus ou des erreurs de transmission.
> 
> DISCLAIMER: This documentation transmitted by electronic mail is intended for the use of the individual to whom or the entity to which it is addressed and may contain information which is confidential and privileged. Confidentiality and privilege are not lost by this documentation having been sent to the wrong electronic mail address. If you are not the intended recipient or the person responsible for delivering it to the intended recipient please notify the sender immediately and destroy this document as well as any copies of it. Any distribution, reproduction or other use of this document is strictly prohibited. National Bank Financial Group and its affiliates cannot be held liable for any damage that may be caused by viruses or transmission errors.
> **************************************************
>


From hstevens at muohio.edu  Fri Jun  9 15:55:43 2006
From: hstevens at muohio.edu (Martin Henry H. Stevens)
Date: Fri, 9 Jun 2006 09:55:43 -0400
Subject: [R] binomial lmer and fixed effects
Message-ID: <7EE200DD-99F4-4F51-AF78-788DEBA0BDE4@muohio.edu>

Hi Folks,

I think I have searched exhaustively, including, of course R-help (D.  
Bates, S. Graves, and others) and but I remain uncertain about  
testing fixed effects with lmer(..., family=binomial).

I gather that mcmcsamp does not work with Do we rely exclusively on z  
values of model parameters, or could we use anova() with likelihood  
ratios, AIC and BIC, with (or without) method="ML" (with didn't seem  
right to me)?

I also received an error using adaptive Gaussian quadrature (but not  
Laplacian approximations):

 > mod2 <- lmer(yb ~ reg*nutrient*amd +
+             (1|rack) + (1|status) +
+             (1|popu) + (1|popu:amd) +
+             (1|gen) + (1|gen:nutrient) + (1|gen:amd) +
+              (1|gen:nutrient:amd),
+             data=datnm, family=binomial, method="AGQ")
Error in lmer(yb ~ reg * nutrient * amd + (1 | rack) + (1 | status) +  :
	method = "AGQ" not yet implemented for supernodal representation

I would really appreciate any and all thoughts or leads.

Cheers,
Hank Stevens

 > version
                _
platform       powerpc-apple-darwin8.6.0
arch           powerpc
os             darwin8.6.0
system         powerpc, darwin8.6.0
status
major          2
minor          3.1
year           2006
month          06
day            01
svn rev        38247
language       R
version.string Version 2.3.1 (2006-06-01)
 >



Dr. M. Hank H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"


From Pierre.Lapointe at nbf.ca  Fri Jun  9 16:08:53 2006
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Fri, 9 Jun 2006 10:08:53 -0400 
Subject: [R] sqlSave() and rownames=TRUE makes my Rgui crash
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D0DF35FCF@lbmsg002.fbn-nbf.local>


On 6/9/2006 9:42 AM, Lapointe, Pierre wrote:
> On 6/9/2006 8:51 AM, Lapointe, Pierre wrote:
>> Hello,
>> 
>> I created a table in MySQL with this command
>> 
>> CREATE TABLE example (pk INT NOT NULL AUTO_INCREMENT,PRIMARY KEY(pk),
>> id VARCHAR(30),col1 VARCHAR(30),col2 VARCHAR(30))
>> 
>> ### In R, I can connect to this table:
>> 
>> library(DBI)
>> library(RODBC)
>> chan <- odbcConnect("MySQL51", uid="root", pwd="xxx")
>> first <- sqlQuery(chan, "select * from example")
>> close(chan)
>> First
>> #[1] pk   id   col1 col2
>> #<0 rows> (or 0-length row.names)
>> 
>> ### This is the table I'm trying to save:
>> dframe <-data.frame(matrix(1:6,2,3))
>> colnames(dframe)=c("id","col1","col2")
>> dframe
>> #  id col1 col2
>> #1  1    3    5
>> #2  2    4    6
>> 
>> ### But this makes Rgui crash and close
>> chan <- odbcConnect("MySQL51", uid="root", pwd="xxx") sqlSave(chan, 
>> dframe, tablename="example", rownames = FALSE, append=T)
>> close(chan)
>> 
>> ### With rownames = T and safer=F, it works, but I loose the
>> autoincrementing PK in MySQL chan <- odbcConnect("MySQL51", 
>> uid="root", pwd="momie")  #default database=fbn
>> sqlSave(chan, dframe, tablename="example", rownames = T,
>> addPK=T,append=T,safer=F)
>> close(chan)
>> 
>> Any idea?
>> 
>> I'm on win2K, MySQL version 5.0.21-community-nt
> 
>>>I don't know why you're using DBI; perhaps it interferes with RODBC
> somehow.
> 
> **It still crashes without DBI
> 
>>>If that's not it, then you might want to try lower level methods than
>>>sqlSave:  perhaps use sqlQuery to send an INSERT command to the 
>>>database.  Build up from there.
> 
> **Good suggestion, however, I'm not sure how to pass a table through 
> an sql statement. From this archived doc, 
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/10073.html I tried 
> this using a dataframe instead of a single number.

You can't.  You can only insert one record at a time this way in 
general, but MySQL allows multiple inserts on one line.  So it's a lot 
of work, but you might figure out what's causing your crash.


> 
> But I get this error:
> 
> #test
> chan <- odbcConnect("MySQL51", uid="root", pwd="momie")  #default 
> database=fbn query <- paste("INSERT INTO example VALUES 
> ('",dframe,"')",sep="")
> sqlQuery(chan,query)
> close(chan)

You'd want something like

inserts <- with(dframe, paste("('", id, "','", col1, "','", col2, "')", 
sep="", collapse=",")
query <- paste("INSERT INTO example(id, col1, col2) VALUES", inserts)
sqlQuery(chan, query)

(This isn't even tested to see if I got the syntax right, and it's 
probably not legal syntax on other databases.  For those you could put
together multiple  INSERT statements.)

Nice workaround, but I'd be reluctant to use it for the same reason I'd
prefer not to use RMySQL: I'd like my R code to be easily adaptable in case
I port my DB to let's say PostgreSQL.  Using RODBC, I would probably only
have to change the DSN to make it work.

Pierre Lapointe

**************************************************
AVIS DE NON-RESPONSABILITE: Ce document transmis par courrie...{{dropped}}


From davearmstrong.ps at gmail.com  Fri Jun  9 16:23:19 2006
From: davearmstrong.ps at gmail.com (Dave Armstrong)
Date: Fri, 9 Jun 2006 10:23:19 -0400
Subject: [R] [Fwd: R 2.20 Windows XP anaolgue of Splus unix() command ?]
In-Reply-To: <38b9f0350606082358r10c0a8edx2d46a6eb22df4f37@mail.gmail.com>
References: <15477170.361141149782614484.JavaMail.root@vms070.mailsrvcs.net>
	<38b9f0350606082358r10c0a8edx2d46a6eb22df4f37@mail.gmail.com>
Message-ID: <f36efd3b0606090723r4136d959y98cc9a403c91e083@mail.gmail.com>

Mark,

I don't know how many of these files you have, so this may not be a
viable solution, but what if you changed the function "wait" to be
something like the following:

wait <- function() par(ask=T)
for(i in 1:100){
plot(rnorm(100) ~ rnorm(100))
wait()
}

So, in general, this is not a solution to the unix() question, but it
seems to produce what you want in this case.

Hope this helps,
Dave.

Dave Armstrong
University of Maryland
Dept of Government and Politics
3140 Tydings Hall
College Park, MD 20742
Office: 2103L Cole Field House
Phone: 301-405-9735
e-mail: darmstrong at gvpt.umd.edu
web: www.davearmstrong-ps.com

Facts are meaningless.  You can use facts to prove anything that's
even remotely true. - Homer Simpson

On 6/9/06, ronggui <ronggui.huang at gmail.com> wrote:
> In my windows XP there is no "read" command as well,so the _
> unix("read stuff")_  will not wor as what _system_ function does is to
> pass the 'read stuff' command argument to the system command.
>
> I guess the "read" command to specific to some Unix OS.
>
> Hope this helps.
>
> 2006/6/9, markleeds at verizon.net <markleeds at verizon.net>:
> > >Hi Everyone : As I mentioned earlier, I am taking a lot
> > >of Splus code and turning into R and I've run into
> > >another stumbling block that I have not been
> > >able to figure out.
> > >
> > >I did plotting in a loop when I was using Splus on unix
> > >and the way I made the plots stop so I could
> > >lookat them as they got plotted ( there are hundreds
> > >if not thousands getting plotted sequentially )
> > >on the screen was by using the unix() command.
> > >
> > >Basically, I wrote a function called wait()
> > >
> > >
> > >wait<-function()
> > >{
> > >cat("press return to continue")
> > >unix("read stuff")
> > >}
> > >
> > >and this worked nicely because I then
> > >did source("program name") at the Splus prompt and
> > >a plot was created on the screen  and then
> > >the wait() function was right under the plotting code
> > >in the program so that you had to hit the return key to go to the next plot.
> > >
> > >I am trying to do the equivalent on R 2.20/windows XP
> > >I did a ?unix in R and it came back with system() and
> > >said unix was deprecated so I replaced unix("read stuff") with system("read stuff") but all i get is a warning "read not found" and
> > >it flies through the successive plots and i can't see them.
> > >
> > >Thanks for any help on this. It's much appreciated.
> > >
> > >                                        Mark
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
>
> --
> ??????
> Deparment of Sociology
> Fudan University
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>


From dimitris.rizopoulos at med.kuleuven.be  Fri Jun  9 16:37:42 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 9 Jun 2006 16:37:42 +0200
Subject: [R] binomial lmer and fixed effects
References: <7EE200DD-99F4-4F51-AF78-788DEBA0BDE4@muohio.edu>
Message-ID: <014601c68bd2$43d9ad50$0540210a@www.domain>

AFAIK likelihood ratio tests are preferable, especially when you're 
interested in testing several fixed-effects simultaneously. However, 
since in GLMMs the likelihood cannot be calculated explicitly one 
could wonder how does this affect LRTs.

Adaptive Gaussian quadrature is know to provide the best 
approximation; however, taking into account the random-effects 
structure in your model I doubt if it'd ever convergence in this year. 
Thus, I think Laplace is the best you can have in a reasonable 
computing time.

Regarding `method = "ML"' I think this refers to the linear mixed 
model case where you also have the option for REML (the default).

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Martin Henry H. Stevens" <hstevens at muohio.edu>
To: "R-Help" <r-help at stat.math.ethz.ch>
Sent: Friday, June 09, 2006 3:55 PM
Subject: [R] binomial lmer and fixed effects


> Hi Folks,
>
> I think I have searched exhaustively, including, of course R-help 
> (D.
> Bates, S. Graves, and others) and but I remain uncertain about
> testing fixed effects with lmer(..., family=binomial).
>
> I gather that mcmcsamp does not work with Do we rely exclusively on 
> z
> values of model parameters, or could we use anova() with likelihood
> ratios, AIC and BIC, with (or without) method="ML" (with didn't seem
> right to me)?
>
> I also received an error using adaptive Gaussian quadrature (but not
> Laplacian approximations):
>
> > mod2 <- lmer(yb ~ reg*nutrient*amd +
> +             (1|rack) + (1|status) +
> +             (1|popu) + (1|popu:amd) +
> +             (1|gen) + (1|gen:nutrient) + (1|gen:amd) +
> +              (1|gen:nutrient:amd),
> +             data=datnm, family=binomial, method="AGQ")
> Error in lmer(yb ~ reg * nutrient * amd + (1 | rack) + (1 | status) 
> +  :
> method = "AGQ" not yet implemented for supernodal representation
>
> I would really appreciate any and all thoughts or leads.
>
> Cheers,
> Hank Stevens
>
> > version
>                _
> platform       powerpc-apple-darwin8.6.0
> arch           powerpc
> os             darwin8.6.0
> system         powerpc, darwin8.6.0
> status
> major          2
> minor          3.1
> year           2006
> month          06
> day            01
> svn rev        38247
> language       R
> version.string Version 2.3.1 (2006-06-01)
> >
>
>
>
> Dr. M. Hank H. Stevens, Assistant Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
>
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/~stevenmh/
> http://www.muohio.edu/ecology/
> http://www.muohio.edu/botany/
> "E Pluribus Unum"
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From John.Kerpel at infores.com  Fri Jun  9 16:40:58 2006
From: John.Kerpel at infores.com (Kerpel, John)
Date: Fri, 9 Jun 2006 09:40:58 -0500
Subject: [R] Mod function?
Message-ID: <44A8B25381923D4F93B74B2676A50F6D028ACC3A@MAIL1.infores.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060609/de3d9cf6/attachment.pl 

From fdoespin at gmail.com  Fri Jun  9 13:01:17 2006
From: fdoespin at gmail.com (fernando espindola)
Date: Fri, 09 Jun 2006 11:01:17 +0000
Subject: [R] How add degree character in axis?
Message-ID: <448954FD.4020809@gmail.com>

Hi user R,

I am try to put degree character in axis x, but don't make this.  I have 
the next code:

plot(mot[,5], 
time1,xlim=c(-45,-10),type="l",yaxt="n",ylab="",col=1,lwd=2,xlab="",xaxt="n")

The range of value in axis-x  is  -45 to -10, this values represents the 
longitudes positions in space. I try to put 45?S for real value (-45) in 
the axis-x, and for all elements . Anybody can give an advance in this 
problems.....

Thank for all


-- 
******************************************
Fernando Espindola Rebolledo
Departamento de Evaluacion de Pesquerias
Division de Investigacion Pesquera 
Instituto de Fomento Pesquero
Blanco 839
Valparaiso - Chile
tel: (+56)-32-322442
http://fespindola.mi-pagina.cl/index.htm


From maheller at gmail.com  Fri Jun  9 17:00:10 2006
From: maheller at gmail.com (Martin Heller)
Date: Fri, 9 Jun 2006 11:00:10 -0400
Subject: [R] Saving dns2.plot as a jpg image
Message-ID: <5249c7af0606090800l5088c39bna4a967b5e46daf93@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060609/6f434056/attachment.pl 

From tlumley at u.washington.edu  Fri Jun  9 17:04:35 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 9 Jun 2006 08:04:35 -0700 (PDT)
Subject: [R] apologies if you aready received this ?
In-Reply-To: <7847663.375401149785244948.JavaMail.root@vms070.mailsrvcs.net>
References: <7847663.375401149785244948.JavaMail.root@vms070.mailsrvcs.net>
Message-ID: <Pine.LNX.4.64.0606090803130.25744@homer21.u.washington.edu>

On Thu, 8 Jun 2006, markleeds at verizon.net wrote:
>
> Hi Everyone : As I mentioned earlier, I am taking a lot
> of Splus code and turning into R and I've run into
> another stumbling block that I have not been
> able to figure out.
>
> I did plotting in a loop when I was using Splus on unix
> and the way I made the plots stop so I could
> lookat them as they got plotted ( there are hundreds
> if not thousands getting plotted sequentially )
> on the screen was by using the unix() command.
>

People have suggested ways to read stuff from the keyboard already. A 
simpler solution to the underlying problem may be par(ask=TRUE). However, 
when I have this problem I usually just send all the plots to a PDF file, 
so I can page through them at my leisure.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From dhajage at gmail.com  Fri Jun  9 17:07:04 2006
From: dhajage at gmail.com (David Hajage)
Date: Fri, 9 Jun 2006 17:07:04 +0200
Subject: [R] reverse variables
Message-ID: <a725cda30606090807ibf35fdbq4b032777d163ed5e@mail.gmail.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060609/84108484/attachment.pl 

From ggrothendieck at gmail.com  Fri Jun  9 17:15:10 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 9 Jun 2006 11:15:10 -0400
Subject: [R] reverse variables
In-Reply-To: <a725cda30606090807ibf35fdbq4b032777d163ed5e@mail.gmail.com>
References: <a725cda30606090807ibf35fdbq4b032777d163ed5e@mail.gmail.com>
Message-ID: <971536df0606090815t79de50e1mc20cc7d872582db3@mail.gmail.com>

There is a function here:

  http://tolstoy.newcastle.edu.au/R/help/04/06/1430.html

that will facilitate this sort of construct:

a <- 1; b <- 2
list[b,a] <- list(a,b)
a; b # 2 1

On 6/9/06, David Hajage <dhajage at gmail.com> wrote:
> Hello useRs,
>
> Is there a way to reverse values of 2 variables (like with the language
> Python) ? :
>
> a <- 1
> b <- 2
>
> a, b <- b, a
>
> More specifically, I have a data frame :
>
>   famnum generation germain1 germain2  fa  mo ptpn1 ptpn2 drb11 drb12
>       1          2      200      201 101 102     1     1     1     1
>       2          2      200      201 101 102     0     1     1     1
>       3          2      200      201 101 102     1     0     1     1
> ...
>
> I don't want "0" in the seventh column, so I would like to have :
>
>   famnum generation germain1 germain2  fa  mo ptpn1 ptpn2 drb11 drb12
>       1          2      200      201 101 102     1     1     1     1
>       2          2      201      200 101 102     1     0     1     1
>       3          2      200      201 101 102     1     0     1     1
> ...
>
> I thought that a code like this would work :
>
> if (ptpn1 == 0 & ptpn2 == 1)
>  {
>    germain1, germain2 <- germain2, germain1
>    ptpn1, ptpn2 <- ptpn2, ptpn1
>    drb11, drb12 <- drb12n, drb11
>  }
>
> But R is not Python... Is there a way to do it easyly ?
>
> Thank you.
> --
> David
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From jel03r at ecs.soton.ac.uk  Fri Jun  9 17:16:56 2006
From: jel03r at ecs.soton.ac.uk (Jamie Lawrence)
Date: Fri, 09 Jun 2006 16:16:56 +0100
Subject: [R] Postscript output encoding on Windows
Message-ID: <448990E8.5010607@ecs.soton.ac.uk>

With R 2.3.1 on Windows, I've noticed that it defaults to the 
"WinAsci.enc" encoding, which throws up an error when viewed with the 
default install of GhostScript/GSView (versions 8.51/4.7 respectively).  
I didn't have this problem prior to upgrading to 2.3.1 as I can view all 
my old PS graphs and they don't have the WinAsci encoding.  I've got 
around this problem by adding encoding="CP1251.enc" to the postscript 
call but it seems a little bizarre that the default WinAsci wasn't 
working. 

Any ideas what the problem might be?  A problem with Ghostscript, a bug 
in R, or something strange about my installation (there shouldn't be)??  
Should WinAsci be the default?

Thanks,

    Jamie


From ym at climpact.com  Fri Jun  9 17:19:34 2006
From: ym at climpact.com (Yves Magliulo)
Date: 09 Jun 2006 17:19:34 +0200
Subject: [R]  How can i add a color bar with base package
Message-ID: <1149866374.31082.32.camel@new-york.climpact.net>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060609/ebee1cb2/attachment.pl 

From rgentlem at fhcrc.org  Fri Jun  9 17:31:30 2006
From: rgentlem at fhcrc.org (Robert Gentleman)
Date: Fri, 09 Jun 2006 08:31:30 -0700
Subject: [R] deal with R.package panel
In-Reply-To: <4489686B.6070508@wiwi.uni-bielefeld.de>
References: <4489686B.6070508@wiwi.uni-bielefeld.de>
Message-ID: <44899452.9090101@fhcrc.org>

Hi,
  There should be a 20 page pdf document in the doc directory, 
distributed with the package. That answers most of these questions.

  Also, please read the posting guide so that you give enough 
information to actually have some chance of answering your questions,

  best wishes
   Robert

Pavel Khomski wrote:
> hello!
> 
> my question conserns with use of "panel" package (written by R.C.Gentlman)
> (unfortunately the manual and help sites are very short)
> 
> 1. is it possible to do analysis  just without a(ny) covariate? i 
> suggest do it by introducing a covariate with level=0 in all 
> obervations, this because of Q(z)=Q_o exp(beta*z),  but it seemingly 
> doesn't work

  meaning what?
> 
> 2. in the option gamma in the call of panel function: do you mean an 
> initial value for parameter vector gamma?
> say if i have 3 theta-parameters, so i have to initialize 
> gamma=c(xxx,xxx,xxx), correct?

yes
> 
> 3. are the (first ) observed times =0 allowed (in $time vectors) or 
> schould in such a case begin with =1, if there are any?

  I suspect 0, but since you have said nothing about what you are 
actually trying to do, it is entirely a guess on my part

  best wishes
    Robert

> 
> 
> thanks for your response
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Robert Gentleman, PhD
Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M2-B876
PO Box 19024
Seattle, Washington 98109-1024
206-667-7700
rgentlem at fhcrc.org


From gunter.berton at gene.com  Fri Jun  9 17:38:14 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 9 Jun 2006 08:38:14 -0700
Subject: [R] Re-binning histogram data
In-Reply-To: <03b901c68b6a$d3f179d0$6600a8c0@DD4XFW31>
Message-ID: <001301c68bda$b8db9ed0$295afea9@gne.windows.gene.com>

Charles: 

To be fair ... both histograms and densityplots are nonparametric density
estimators whose appearance and effectiveness are dependent on various
parameters. Neither are immune from misleading due to a poor choice of the
parameters. For histograms they are the bin boundaries; for kde's and
friends it is some version of bandwidth.

-- Bert
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Charles Annis, P.E.
> Sent: Thursday, June 08, 2006 7:17 PM
> To: 'Justin Ashmall'; r-help at stat.math.ethz.ch
> Subject: Re: [R] Re-binning histogram data
> 
> Concerning the several comments on your note relating to 
> histograms, an
> informative and entertaining illustration, using Java, of how your
> subjective assessment of the data can change with different histograms
> constructed from the same data, is provided by R. Webster 
> West, recently
> with the Department of Statistics at the University of South 
> Carolina, but
> as of May 2006 with the Department of Statistics at Texas A & 
> M University,
> http://www.stat.sc.edu/~west/javahtml/Histogram.html  and
> http://www.stat.tamu.edu/~west/ 
> 
> 
> Charles Annis, P.E.
> 
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax:  614-455-3265
> http://www.StatisticalEngineering.com
>  
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Justin Ashmall
> Sent: Thursday, June 08, 2006 5:46 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Re-binning histogram data
> 
> Hi,
> 
> Short Version:
> Is there a function to re-bin a histogram to new, broader bins?
> 
> Long version: I'm trying to create a histogram, however my 
> input-data is 
> itself in the form of a fine-grained histogram, i.e. numbers 
> of counts 
> in regular one-second bins. I want to produce a histogram of, say, 
> 10-minute bins (though possibly irregular bins also).
> 
> I suppose I could re-create a data set as expected by the 
> hist() function 
> (i.e. if time t=3600 has 6 counts, add six entries of 3600 to a list) 
> however this seems neither elegant nor efficient (though I'd 
> be pleased to 
> be mistaken!). I could then re-create a histogram as normal.
> 
> I guessing there's a better solution however! Apologies if 
> this is a basic 
> question - I'm rather new to R and trying to get up to speed.
> 
> Regards,
> 
> Justin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From larry.howe at comjet.com  Fri Jun  9 17:38:54 2006
From: larry.howe at comjet.com (Larry Howe)
Date: Fri, 9 Jun 2006 11:38:54 -0400
Subject: [R] Saving dns2.plot as a jpg image
In-Reply-To: <5249c7af0606090800l5088c39bna4a967b5e46daf93@mail.gmail.com>
References: <5249c7af0606090800l5088c39bna4a967b5e46daf93@mail.gmail.com>
Message-ID: <200606091138.54701.larry.howe@comjet.com>

You don't say what OS you are using, or whether you are getting any messages 
or errors.

http://www.R-project.org/posting-guide.html

Larry Howe

On Friday June 9 2006 11:00, Martin Heller wrote:
> Hello,
> I am using R 2.2.1 with sn 0.4.0 and am trying to save a dns2.plot as a jpg
> file.
>
> Here is the latest version of the code I have tried:
>
> x <- seq(0,200,length=200)
> y <- seq(0, 35, length=200)
>
> jpeg(filename = "C:/fig1.jpg", width = 5, height = 4,pointsize = 12,
> quality = 100, bg = "white", res = NA)
> par(mfrow=c(1,2),plt=c(.15,.99,.15,.95),mgp=c(1.5,1,0),tcl=-.25,cex.axis=.8
>) dsn2.plot
> (x,y,xi=c(29,6),Omega=matrix(c(2580,458,458,84),2),alpha=c(5482940,-2120750
>)) dst2.plot(x,y, xi=c(29,6),Omega=matrix(c(2065,380,380,72),2),alpha
> =c(48,-13),df=10)
> dev.off()
>
> I've also tried using graphics.off().
> Any help would be greatly appreciated?
> Thanks,
> Martin Heller
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html


From wheresmybacon at gmail.com  Fri Jun  9 17:54:06 2006
From: wheresmybacon at gmail.com (Elizabeth Rainwater)
Date: Fri, 9 Jun 2006 10:54:06 -0500
Subject: [R] glm with negative binomial family
Message-ID: <eb08b0fc0606090854td38d06aj7d45b23278fc7569@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060609/7cb71d8c/attachment.pl 

From rgentlem at fhcrc.org  Fri Jun  9 18:25:12 2006
From: rgentlem at fhcrc.org (Robert Gentleman)
Date: Fri, 09 Jun 2006 09:25:12 -0700
Subject: [R] deal with R.package panel
In-Reply-To: <44899452.9090101@fhcrc.org>
References: <4489686B.6070508@wiwi.uni-bielefeld.de>
	<44899452.9090101@fhcrc.org>
Message-ID: <4489A0E8.8080402@fhcrc.org>


I just realized that the pdf on CRAN is corrupt, a new package has been 
uploaded and will percolate through over the next few days, one supposes.

Robert Gentleman wrote:
> Hi,
>   There should be a 20 page pdf document in the doc directory, 
> distributed with the package. That answers most of these questions.
> 
>   Also, please read the posting guide so that you give enough 
> information to actually have some chance of answering your questions,
> 
>   best wishes
>    Robert
> 
> Pavel Khomski wrote:
>> hello!
>>
>> my question conserns with use of "panel" package (written by R.C.Gentlman)
>> (unfortunately the manual and help sites are very short)
>>
>> 1. is it possible to do analysis  just without a(ny) covariate? i 
>> suggest do it by introducing a covariate with level=0 in all 
>> obervations, this because of Q(z)=Q_o exp(beta*z),  but it seemingly 
>> doesn't work
> 
>   meaning what?
>> 2. in the option gamma in the call of panel function: do you mean an 
>> initial value for parameter vector gamma?
>> say if i have 3 theta-parameters, so i have to initialize 
>> gamma=c(xxx,xxx,xxx), correct?
> 
> yes
>> 3. are the (first ) observed times =0 allowed (in $time vectors) or 
>> schould in such a case begin with =1, if there are any?
> 
>   I suspect 0, but since you have said nothing about what you are 
> actually trying to do, it is entirely a guess on my part
> 
>   best wishes
>     Robert
> 
>>
>> thanks for your response
>>
>>
>> ------------------------------------------------------------------------
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Robert Gentleman, PhD
Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M2-B876
PO Box 19024
Seattle, Washington 98109-1024
206-667-7700
rgentlem at fhcrc.org


From ripley at stats.ox.ac.uk  Fri Jun  9 18:28:19 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 9 Jun 2006 17:28:19 +0100 (BST)
Subject: [R] Postscript output encoding on Windows
In-Reply-To: <448990E8.5010607@ecs.soton.ac.uk>
References: <448990E8.5010607@ecs.soton.ac.uk>
Message-ID: <Pine.LNX.4.64.0606091721240.14034@gannet.stats.ox.ac.uk>

It is WinAnsi, not WinAsci, and that may well be your problem. Vanilla R 
2.3.1 uses WinAnsi (I have just re-checked), so there is `something 
strange about my installation'.

On Fri, 9 Jun 2006, Jamie Lawrence wrote:

> With R 2.3.1 on Windows, I've noticed that it defaults to the
> "WinAsci.enc" encoding, which throws up an error when viewed with the
> default install of GhostScript/GSView (versions 8.51/4.7 respectively).
> I didn't have this problem prior to upgrading to 2.3.1 as I can view all
> my old PS graphs and they don't have the WinAsci encoding.  I've got
> around this problem by adding encoding="CP1251.enc" to the postscript
> call but it seems a little bizarre that the default WinAsci wasn't
> working.
>
> Any ideas what the problem might be?  A problem with Ghostscript, a bug
> in R, or something strange about my installation (there shouldn't be)??
> Should WinAsci be the default?
>
> Thanks,
>
>    Jamie
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Jun  9 18:35:05 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 9 Jun 2006 17:35:05 +0100 (BST)
Subject: [R] glm with negative binomial family
In-Reply-To: <eb08b0fc0606090854td38d06aj7d45b23278fc7569@mail.gmail.com>
References: <eb08b0fc0606090854td38d06aj7d45b23278fc7569@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0606091730160.14034@gannet.stats.ox.ac.uk>

Did you consult the help page for neg.bin?

Details:

      These are not intended to be called by the user. Some are for
      compatibility with earlier versions of MASS (the book).

The book discusses the family negative.binomial() on p.206, and it is that 
which glm.nb uses.


On Fri, 9 Jun 2006, Elizabeth Rainwater wrote:

> I am analysing parasite egg count data and am having trouble with glm with a
> negative binomial family.
>
> In my first data set, 55% of the 3000 cases have a zero count, and the
> non-zero counts range from 94 to 145,781.
> Eventually, I want to run bic.glm, so I need to be able to use glm(family=
> neg.bin(theta)). But first I ran glm.nb to get an estimate of theta:
>
>> hook.nb<- glm.nb(fh, data=hook)
>
> This works fine, with no errors, and summary( hook.nb) produces the
> following:
>
> Call:
> glm.nb(formula = fh, data = hook, init.theta = 0.0938126159640384,
>    link = log)
>
> Deviance Residuals:
>     Min        1Q    Median        3Q       Max
> -1.45830  -1.16385  -1.01820   0.00535   2.86513
>
> <snip>
>
>              Theta:  0.09381
>          Std. Err.:  0.00299
>
> 2 x log-likelihood:  -23750.45300
>
> Then I tried to use this estimate of theta to specify a glm of the negative
> binomial family but got the following error:
>
>> hook.fam<-glm(fh, data=hook, family=neg.bin(0.09381))
> Error: NA/NaN/Inf in foreign function call (arg 1)
> In addition: Warning message:
> step size truncated due to divergence
>
> When I change theta to be 1 or more, the glm converges (as does bic.glm), so
> I thought maybe the estimate of theta was wrong. But when I ran a negative
> binomial regression in Stata, I got the same theta. (theta = 1/alpha =
> 1/10.65954 = .09381268)
>
>
> In my second set of data, 75% of the cases have zero counts, and the
> non-zero cases range from 94 - 16,688. In this case, I get errors when I run
> the glm.nb:
>
>> asc.nb<-glm.nb (fa, data=asc)
> There were 26 warnings (use warnings() to see them)
>> warnings()
> Warning messages:
> 1: algorithm did not converge in: glm.fitter(x = X, y = Y, w = w, etastart =
> eta, offset = offset,   ...
>    <exactly the same message in 2-24>
> 25: algorithm did not converge in: glm.fitter(x = X, y = Y, w = w, etastart
> = eta, offset = offset,   ...
> 26: alternation limit reached in: glm.nb(fa, data = asc)
>
> Despite these errors, I do get output:
>
> Call:
> glm.nb(formula = fa, data = asc, init.theta = 0.030379484707051,
>    link = log)
>
> Deviance Residuals:
>   Min      1Q  Median      3Q     Max
> - 0.949   -0.787  -0.745  -0.645   2.576
>
> <snip>
>              Theta:  0.03038
>          Std. Err.:  0.00125
> Warning while fitting theta: alternation limit reached
>
> 2 x log-likelihood:  -15743.70600
>
> Again, this estimate of theta agrees with pretty well with Stata's estimate
> (theta = 1/alpha = 1/32.45225 = .03081451). But the glm with negative
> binomial family specification gave the same error as above:
>
>> asc.fam<-glm(fa, data=asc, family=neg.bin(0.0304))
> Error: NA/NaN/Inf in foreign function call (arg 1)
> In addition: Warning message:
> step size truncated due to divergence
>
> Everything runs smoothly when theta is 1 or more, so I don't think anything
> is wrong with my data (which has no missings and all real numbers). I think
> the problem must be with theta or with my specification of it. I have the
> Venables and Ripley book, and am able to run glm(family=negative binomial(
> 0.03)) and bic.glm(glm.family=negative.binomial(0.03)) on the quine data
> that comes with MASS. I have looked in the R-help archives and googled, but
> have not found much besides a few old bug reports (which have been fixed) to
> help me figure out why one of the glm.nb algorithms did not converge and why
> both of the glm(family=neg.bin()) calls throw errors. Any ideas?
>
> Thanks,
> Elizabeth
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From wouter at paramo.be  Fri Jun  9 20:04:11 2006
From: wouter at paramo.be (Wouter)
Date: Fri, 9 Jun 2006 18:04:11 +0000 (GMT)
Subject: [R] quantile() with weights
In-Reply-To: <Pine.LNX.4.64.0606091730160.14034@gannet.stats.ox.ac.uk>
References: <eb08b0fc0606090854td38d06aj7d45b23278fc7569@mail.gmail.com>
	<Pine.LNX.4.64.0606091730160.14034@gannet.stats.ox.ac.uk>
Message-ID: <Pine.LNX.4.64.0606091751060.8150@localhost>


Hi list,

I'm looking for a way to calculate quantiles (such as in quantile()), but 
with the ability to assign a different weight to each member of the sample 
vector.

It is possible to write it out completely, but maybe there is a dedicated 
function out somewhere...

cheers

Wouter


From ririzarr at jhsph.edu  Fri Jun  9 19:52:04 2006
From: ririzarr at jhsph.edu (Rafael A Irizarry)
Date: Fri, 09 Jun 2006 13:52:04 -0400
Subject: [R] X'W in Matrix
In-Reply-To: <40e66e0b0606090041h27569da9l1809718ed52f903f@mail.gmail.com>
References: <Pine.LNX.4.61.0606082220260.7314@enigma.local>
	<40e66e0b0606090041h27569da9l1809718ed52f903f@mail.gmail.com>
Message-ID: <4489B544.1060501@jhsph.edu>

Doug,

I did mean wX = crossprod(X, W). sorry about that.

Thanks for the suggestions and for the *very* useful package. Im still 
smiling about inverting a 6000x6000 matrix in one second with on line of 
R code.

Best wishes,
-r


Douglas Bates wrote:

> On 6/9/06, Rafael A. Irizarry <ririzarr at jhsph.edu> wrote:
>
>> Hi!
>>
>> I have used the Matrix package (Version: 0.995-10) successfully
>> to obtain the OLS solution for a problem where the design matrix X is
>> 44000x6000. X is very sparse (about 80000 non-zeros elements).
>>
>> Now I want to do WLS: (X'WX)^-1X'Wy
>>
>> I tried W=Diagonal(length(w),w) and
>> wX=solve(X,W)
>>
>> but after various minutes R gives a not enough
>> memory error (Im using a 64bit machine with 16Gigs of RAM).
>
>
> That's an interesting question, Rafael, and very timely.  I happen to
> be visiting Martin Maechler this week and he mentioned to me just a
> few minutes ago that we should add the capability for sparse least
> squares calculations to the Matrix package.
>
> Just for clarification, did you really mean wX = solve(X, W) above or
> were you thinking of wX = crossprod(X, W)?
>
> I would suggest storing the square root of the diagonal matrix W as a
> sparse matrix
>
>> w <- abs(rnorm(88000))
>> ind <- seq(along = w) - 1:1
>> W <- as(new("dgTMatrix", i = ind, j = ind, x = sqrt(w), Dim = 
>> c(length(w), length(w))), "dgCMatrix")
>> str(W)
>
> Formal class 'dgCMatrix' [package "Matrix"] with 6 slots
>  ..@ i       : int [1:88000] 0 1 2 3 4 5 6 7 8 9 ...
>  ..@ p       : int [1:88001] 0 1 2 3 4 5 6 7 8 9 ...
>  ..@ Dim     : int [1:2] 88000 88000
>  ..@ Dimnames:List of 2
>  .. ..$ : NULL
>  .. ..$ : NULL
>  ..@ x       : num [1:88000] 0.712 0.989 1.032 0.348 0.573 ...
>  ..@ factors : list()
>
> (The 1:1 expression in the calculation of ind is a cheap trick to get
> an integer value of 1 so that ind stays integer).  Now you should be
> able to create wX <- W %*% X and wy <- W %*% y very quickly.
>
>
>
>> I ended up doing this:
>> wX=Matrix(as.matrix(X)*sqrt(w),sparse=TRUE)
>> coefs1=as.vector(solve(crossprod(wX),crossprod(X,w*y)))
>>
>> which takes about 1-2 minutes, but it seems a better way, using the
>> diagonal matrix, should exist. If there is I'd appreciate hearing it. If
>> not, Im happy waiting 1-2 minute x #of iters.
>>
>>
>> Thanks,
>> Rafael
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>


From rdpeng at gmail.com  Fri Jun  9 20:11:04 2006
From: rdpeng at gmail.com (Roger D. Peng)
Date: Fri, 09 Jun 2006 14:11:04 -0400
Subject: [R] How add degree character in axis?
In-Reply-To: <448954FD.4020809@gmail.com>
References: <448954FD.4020809@gmail.com>
Message-ID: <4489B9B8.30507@gmail.com>

Take a look at ?plotmath.

-roger

fernando espindola wrote:
> Hi user R,
> 
> I am try to put degree character in axis x, but don't make this.  I have 
> the next code:
> 
> plot(mot[,5], 
> time1,xlim=c(-45,-10),type="l",yaxt="n",ylab="",col=1,lwd=2,xlab="",xaxt="n")
> 
> The range of value in axis-x  is  -45 to -10, this values represents the 
> longitudes positions in space. I try to put 45?S for real value (-45) in 
> the axis-x, and for all elements . Anybody can give an advance in this 
> problems.....
> 
> Thank for all
> 
> 

-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/


From brianscholl at yahoo.com  Fri Jun  9 21:13:17 2006
From: brianscholl at yahoo.com (Brian Scholl)
Date: Fri, 9 Jun 2006 12:13:17 -0700 (PDT)
Subject: [R] date.mdy in date package
Message-ID: <20060609191317.17789.qmail@web34307.mail.mud.yahoo.com>

I'm having a problem with output from date.mdy in the date package. 
   
  Goal: to take a long vector of dates of the form "01/22/99" and extract values month="01", day="22", year="1999".  
   
  I am providing the vector of class dates in the attached file to date.mdy: 
   
  > mdy_dates<-date.mdy(trimmed_dates)
   
  The first few obs of the attached vector of dates are: 
              1  1/2/2003    2  1/3/2003    3  1/6/2003    4  1/7/2003    5  1/8/2003    6  1/9/2003    7  1/10/2003    8  1/13/2003    9  1/14/2003
   
  I am expecting to get a list with vectors for the day, month, and year.  The function appears to spit out the format I expect (though I would like months/days with leading zeros) but incorrect values, But what I get in the first few lines is something like: 
   
  > mdy_dates
$month
  [1]  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  2  2
 [26]  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  3  3  3  3  3  3  3  3
 [51]  3  3  3  3  3  3  3  3  3  3  3  3  3  3  4  4  4  4  4  4  4  4  4  4  4
$day
  [1]  1  2  5  6  7  8  9 12 13 14 15 16 19 20 21 22 23 26 27 28 29 30  2  3  4
 [26]  5  6  9 10 11 12 13 16 17 18 19 20 23 24 25 26 27  2  3  4  5  6  9 10 11
 [51] 12 13 16 17 18 19 20 23 24 25 26 27 30 31  1  2  3  6  7  8  9 10 13 14 15
$year
  [1] 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993
 [16] 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993
 [31] 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993

  So: The first day of my list of dates is Jan 2, 2003, but in the list output the first date is Jan 1, 1993.  Have I incorrectly formatted my input, or is there some other problem? 
   
  Thanks in advance. 

 __________________________________________________



From sundar.dorai-raj at pdf.com  Fri Jun  9 21:36:36 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 09 Jun 2006 14:36:36 -0500
Subject: [R] quantile() with weights
In-Reply-To: <Pine.LNX.4.64.0606091751060.8150@localhost>
References: <eb08b0fc0606090854td38d06aj7d45b23278fc7569@mail.gmail.com>	<Pine.LNX.4.64.0606091730160.14034@gannet.stats.ox.ac.uk>
	<Pine.LNX.4.64.0606091751060.8150@localhost>
Message-ID: <4489CDC4.2010901@pdf.com>



Wouter wrote:
> Hi list,
> 
> I'm looking for a way to calculate quantiles (such as in quantile()), but 
> with the ability to assign a different weight to each member of the sample 
> vector.
> 
> It is possible to write it out completely, but maybe there is a dedicated 
> function out somewhere...
> 
> cheers
> 
> Wouter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

See ?wtd.quantile in package Hmisc.

--sundar


From sundar.dorai-raj at pdf.com  Fri Jun  9 21:36:45 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 09 Jun 2006 14:36:45 -0500
Subject: [R] quantile() with weights
In-Reply-To: <Pine.LNX.4.64.0606091751060.8150@localhost>
References: <eb08b0fc0606090854td38d06aj7d45b23278fc7569@mail.gmail.com>	<Pine.LNX.4.64.0606091730160.14034@gannet.stats.ox.ac.uk>
	<Pine.LNX.4.64.0606091751060.8150@localhost>
Message-ID: <4489CDCD.3010908@pdf.com>



Wouter wrote:
> Hi list,
> 
> I'm looking for a way to calculate quantiles (such as in quantile()), but 
> with the ability to assign a different weight to each member of the sample 
> vector.
> 
> It is possible to write it out completely, but maybe there is a dedicated 
> function out somewhere...
> 
> cheers
> 
> Wouter

See ?wtd.quantile in package Hmisc.

--sundar


From linux at comjet.com  Fri Jun  9 21:56:38 2006
From: linux at comjet.com (Larry Howe)
Date: Fri, 9 Jun 2006 15:56:38 -0400
Subject: [R] function parameters - testing
Message-ID: <200606091556.38249.linux@comjet.com>

Hello,

I am trying to test a function argument to see if it is or is not a useful 
number. However I cannot seem to find a test that works. For example

> f = function(x) {
+  print(exists("x"))
+  print(is.null(x))
+ }
>rm(x)
> f(z)
[1] TRUE
Error in print(is.null(x)) : Object "z" not found

exists gives TRUE, but then any other kind of test I try to run gives me an 
error. I need a test for existence that will work inside a function.

Larry Howe


From mschwartz at mn.rr.com  Fri Jun  9 22:00:17 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 09 Jun 2006 15:00:17 -0500
Subject: [R] How add degree character in axis?
In-Reply-To: <448954FD.4020809@gmail.com>
References: <448954FD.4020809@gmail.com>
Message-ID: <1149883219.5032.65.camel@localhost.localdomain>

On Fri, 2006-06-09 at 11:01 +0000, fernando espindola wrote:
> Hi user R,
> 
> I am try to put degree character in axis x, but don't make this.  I have 
> the next code:
> 
> plot(mot[,5], 
> time1,xlim=c(-45,-10),type="l",yaxt="n",ylab="",col=1,lwd=2,xlab="",xaxt="n")
> 
> The range of value in axis-x  is  -45 to -10, this values represents the 
> longitudes positions in space. I try to put 45?S for real value (-45) in 
> the axis-x, and for all elements . Anybody can give an advance in this 
> problems.....
> 
> Thank for all

In general, as Roger noted, see ?plotmath for adding mathematical
annotation to R plots.

To do the degree symbol is relatively straightforward, but adding the
"S" takes a bit of a trick:

# Create vector of x values
at <- seq(-45, -10, 5)

# Create a mock plot
plot(at, 1:8, xlim = range(at), xaxt = "n")

# Now create a set of plotmath expressions, one for 
# each value of 'at' using paste() and parse()
# The general format for the degrees symbol is x*degree
# However, to add the "S" we use the "~" to create 
# a right and left hand side for the expression and 
# place the "S" after it. The "~" will not print.
L <- parse(text = paste(at, "*degree ~ S", sep = ""))

# Now do the x axis
axis(1, at = at, labels = L)


HTH,

Marc Schwartz


From sundar.dorai-raj at pdf.com  Fri Jun  9 22:06:07 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 09 Jun 2006 15:06:07 -0500
Subject: [R] function parameters - testing
In-Reply-To: <200606091556.38249.linux@comjet.com>
References: <200606091556.38249.linux@comjet.com>
Message-ID: <4489D4AF.1090101@pdf.com>



Larry Howe wrote:
> Hello,
> 
> I am trying to test a function argument to see if it is or is not a useful 
> number. However I cannot seem to find a test that works. For example
> 
> 
>>f = function(x) {
> 
> +  print(exists("x"))
> +  print(is.null(x))
> + }
> 
>>rm(x)
>>f(z)
> 
> [1] TRUE
> Error in print(is.null(x)) : Object "z" not found
> 
> exists gives TRUE, but then any other kind of test I try to run gives me an 
> error. I need a test for existence that will work inside a function.
> 
> Larry Howe
> 


This works because "x" exists in the function "f". Perhaps you want this 
instead?

f <- function(x) {
   print(deparse(substitute(x)))
   print(exists(deparse(substitute(x))))
   print(is.null(x))
   invisible()
}

z <- 2
f(z)
rm(z)
f(z)

Also, see the "where" argument in ?exists.

HTH,

--sundar


From wheresmybacon at gmail.com  Fri Jun  9 22:08:59 2006
From: wheresmybacon at gmail.com (Elizabeth Rainwater)
Date: Fri, 9 Jun 2006 15:08:59 -0500
Subject: [R] glm with negative binomial family
In-Reply-To: <Pine.LNX.4.64.0606091730160.14034@gannet.stats.ox.ac.uk>
References: <eb08b0fc0606090854td38d06aj7d45b23278fc7569@mail.gmail.com>
	<Pine.LNX.4.64.0606091730160.14034@gannet.stats.ox.ac.uk>
Message-ID: <eb08b0fc0606091308t20f728b7i1ad140988e1a66bc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060609/9342b111/attachment.pl 

From ggrothendieck at gmail.com  Fri Jun  9 22:10:35 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 9 Jun 2006 16:10:35 -0400
Subject: [R] function parameters - testing
In-Reply-To: <200606091556.38249.linux@comjet.com>
References: <200606091556.38249.linux@comjet.com>
Message-ID: <971536df0606091310x3a3c0e8y16f2b63f528f1668@mail.gmail.com>

x exists within the function f so the first print is TRUE;
however, exists does not try to evaluate it.  is.null
does try to evaluate it and so at that point it discovers
the error.

Perhaps you want this:

f <- function(x) if (exists(deparse(substitute(x)))) print(is.null(x))
else print("none")
if (exists("z")) rm(z)
f(z) # "none"
z <- 3
f(z) # FALSE
z <- NULL
f(z) # TRUE

On 6/9/06, Larry Howe <linux at comjet.com> wrote:
> Hello,
>
> I am trying to test a function argument to see if it is or is not a useful
> number. However I cannot seem to find a test that works. For example
>
> > f = function(x) {
> +  print(exists("x"))
> +  print(is.null(x))
> + }
> >rm(x)
> > f(z)
> [1] TRUE
> Error in print(is.null(x)) : Object "z" not found
>
> exists gives TRUE, but then any other kind of test I try to run gives me an
> error. I need a test for existence that will work inside a function.
>
> Larry Howe
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From janderson_net at yahoo.com  Fri Jun  9 22:20:05 2006
From: janderson_net at yahoo.com (James Anderson)
Date: Fri, 9 Jun 2006 13:20:05 -0700 (PDT)
Subject: [R] question about using ancova in R to do variable selection
Message-ID: <20060609202006.76502.qmail@web34003.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060609/f5ec2835/attachment.pl 

From markleeds at verizon.net  Fri Jun  9 22:36:06 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Fri, 09 Jun 2006 15:36:06 -0500 (CDT)
Subject: [R] seeing the code that is called inside a function
Message-ID: <6137178.752591149885366438.JavaMail.root@vms073.mailsrvcs.net>

this is probably a bad question but if
you type a function name( the function is from a package ) at
an R prompt and you see that the main work of the function is done by a fortran call (.Fortran ), is it possible to see the fortran code ? i am using windows xp and R-2.20 but i doubt that matters.

                                          thanks


From ggrothendieck at gmail.com  Fri Jun  9 22:41:14 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 9 Jun 2006 16:41:14 -0400
Subject: [R] seeing the code that is called inside a function
In-Reply-To: <6137178.752591149885366438.JavaMail.root@vms073.mailsrvcs.net>
References: <6137178.752591149885366438.JavaMail.root@vms073.mailsrvcs.net>
Message-ID: <971536df0606091341y352b2f81oab03dce582d26e2d@mail.gmail.com>

1. Google for:  CRAN mypkg
where the package is called mypkg and then download and

2. unpack the .tar.gz from there: tar xfvz mypkg_0.1-1.tar.gz
(replace with correct filename)


On 6/9/06, markleeds at verizon.net <markleeds at verizon.net> wrote:
> this is probably a bad question but if
> you type a function name( the function is from a package ) at
> an R prompt and you see that the main work of the function is done by a fortran call (.Fortran ), is it possible to see the fortran code ? i am using windows xp and R-2.20 but i doubt that matters.
>
>                                          thanks
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From jsw9c at uic.edu  Fri Jun  9 22:54:28 2006
From: jsw9c at uic.edu (John S. Walker)
Date: Fri, 9 Jun 2006 15:54:28 -0500
Subject: [R] interaction terms in regression analysis
Message-ID: <a718c3aafcf826310eb8633d13c38221@uic.edu>

G'day,

My problem is I'm not sure how to extract effect sizes from a nonlinear 
regression model with a significant interaction term.

My data sets are multiple measurements of force response to an agonist 
with two superimposed treatments each having two levels.
This is very similar to the Ludbrook example in Venables and Ripley.

The experiment is that a muscle is exposed to an agonist and the force 
response is measured. The resulting data is fit to a logistic fit (a 
three parameter rather than the four parameter used by Ludbrook) . This 
is done for each combination of two factors (treatmentA and Treatment 
B) each having two levels (- and +). Each set of measurements is 
obtained on a muscle from a different animal (i.e. each dose response 
curve represents an independent experiment).

The data are stored as follows:

expt	treatA treatB dose force

I use a groupedData object mydata=groupedData(force ~ dose | expt)

I used an nlme obect to model the data as follows (pseudocode):

myfit <- nlme(force ~ ssThreeParLogistic(dose, upper, ed50,slope), 
fixed=list(ed50~factor(treatmentA)*factor(treatmentC)))


The ThreeParLogistic is a properly debugged and fully functional 
selfstarting object that I wrote- no problem here. I also included 
terms for the other terms; upper and slope, but my main focus is on the 
ed50 so that's all I've included here

Running an anova on the resulting object I found theA -/B- (control) to 
be significantly different from zero, treatment A had no significant 
effect, treatment B was significantly different and there was a 
significant interaction between treatment A and treatment B.

  The interaction term is likely to be real. The treatments are on 
sequential steps in a pathway and treatment A may be blocking the 
effect of treatment B, i.e. treatment A alone has no effect because it 
blocks a pathway that is not active, treatment B reduces force via this 
pathway and treament A therefore blocks the effect of treatment B when 
used together.

So back to my question
How do I extract estimates of the parameters from my model object for a 
specific combination of factors including the interaction term.
  i.e. what is the ed50 (and std err) for A-/B-, A+/B-, A-/B+, A+/B+ ?


Regards



John S. Walker, PhD
Department of Physiology & Biophysics
University of Illinois at Chicago
835 Sth Wolcott Ave MC 901
Chicago IL 60612
USA

email: jsw9c at uic.edu
phone: 1 312 355 0150
fax: 1 312 355 0261

Please avoid sending me Word or PowerPoint attachments.
See http://www.fsf.org/philosophy/no-word-attachments.html


From linux at comjet.com  Fri Jun  9 23:43:32 2006
From: linux at comjet.com (Larry Howe)
Date: Fri, 9 Jun 2006 17:43:32 -0400
Subject: [R] function parameters - testing
In-Reply-To: <4489D4AF.1090101@pdf.com>
References: <200606091556.38249.linux@comjet.com> <4489D4AF.1090101@pdf.com>
Message-ID: <200606091743.32916.linux@comjet.com>

On Friday June 9 2006 16:06, Sundar Dorai-Raj wrote:
> Larry Howe wrote:

>
> This works because "x" exists in the function "f". Perhaps you want this
> instead?
>
> f <- function(x) {
>    print(deparse(substitute(x)))
>    print(exists(deparse(substitute(x))))
>    print(is.null(x))
>    invisible()
> }

Your example code works, but when I try to apply it to my function, it does 
not behave as expected. My variable seems to exist before I pass it into the 
function, but once inside the function it does not exist.

I guess what I need is an in-depth guide to functions: scoping, arguments, 
parameters, etc. I am also becoming curious about how to pass values out of a 
function. Can someone point me in the right direction?

Larry


From kishor_1974 at hotmail.com  Sat Jun 10 00:45:00 2006
From: kishor_1974 at hotmail.com (Neil KM)
Date: Fri, 09 Jun 2006 18:45:00 -0400
Subject: [R] Quantile Regressions/Multi-stage complex survey design
Message-ID: <BAY19-F5705D3100C4D6DB02886B9F880@phx.gbl>

Hello,
I am utilizing linear quantile regression models to analyze health survey 
data. The survey (NHANES) is a multi-stage complex survey and I want to 
incorporate survey sampling weights in generating my quantile estimates. To 
my knowledge, this is currently not possible in SAS or STATA. Is there any 
way to do this in R? If so, how?
Thanks!
-Kish


From sourceforge at metrak.com  Sat Jun 10 02:34:04 2006
From: sourceforge at metrak.com (paul sorenson)
Date: Sat, 10 Jun 2006 10:34:04 +1000
Subject: [R] multiple return values and optimization
Message-ID: <448A137C.3080907@metrak.com>

I have a function (masheff) which returns a value which I can optimize 
no problem, eg:

optimize(masheff, c(15,30), maximum=TRUE, m_gd=5.13, v_tot=41, e_c=1.0)

I would like masheff() to return multiple values say as a list with 
named elements like so:

v = masheff(...)
v$eff
v$extract
etc

Is there a simple way to do this in an optimize context or would I need 
to set some global variables and inspect them afterwards?


From ggrothendieck at gmail.com  Sat Jun 10 03:19:00 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 9 Jun 2006 21:19:00 -0400
Subject: [R] multiple return values and optimization
In-Reply-To: <448A137C.3080907@metrak.com>
References: <448A137C.3080907@metrak.com>
Message-ID: <971536df0606091819m21893f79t632b0c14061e162b@mail.gmail.com>

optimize will preserve attributes so try this:

f <- function(x) structure(x^2, cubed = x^3)
optimize(f, c(-1, 1))

On 6/9/06, paul sorenson <sourceforge at metrak.com> wrote:
> I have a function (masheff) which returns a value which I can optimize
> no problem, eg:
>
> optimize(masheff, c(15,30), maximum=TRUE, m_gd=5.13, v_tot=41, e_c=1.0)
>
> I would like masheff() to return multiple values say as a list with
> named elements like so:
>
> v = masheff(...)
> v$eff
> v$extract
> etc
>
> Is there a simple way to do this in an optimize context or would I need
> to set some global variables and inspect them afterwards?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From p_connolly at ihug.co.nz  Sat Jun 10 04:46:41 2006
From: p_connolly at ihug.co.nz (Patrick Connolly)
Date: Sat, 10 Jun 2006 14:46:41 +1200
Subject: [R] Regex engine types
Message-ID: <20060610024641.GG4970@ihug.co.nz>

> version
         _                       
platform x86_64-unknown-linux-gnu
arch     x86_64                  
os       linux-gnu               
system   x86_64, linux-gnu       
status                           
major    2                       
minor    2.1                     
year     2005                    
month    12                      
day      20                      
svn rev  36812                   
language R                       
> 

> grep("[W-Z]", LETTERS, value = TRUE)
[1] "W" "X" "Y" "Z"

That's what I'd have expected.

> grep("[W-Z]", letters, value = TRUE)
[1] "x" "y" "z"

Not what I'd have thought.  However,

> grep("[B-D]", letters, value = TRUE, perl = TRUE)
character(0)

So what is it that standard regular expressions use that's different
from Perl-type ones?

The help file for grep refers to POSIX 1003.2 which looked a bit
daunting to delve into.  From my limited reading, it seems there are
different gegex "Engine Types" which seems to be getting somewhat
tangential to what I was working on.  I could probably avoid problems
if I always set perl=TRUE, but it would be good to know what basic and
extended regular expressions do that's different.  If someone has a
quick line or two describing it, I'd be interested to know.

Thanks

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From calstats05 at yahoo.com  Sat Jun 10 07:30:37 2006
From: calstats05 at yahoo.com (Cal Stats)
Date: Fri, 9 Jun 2006 22:30:37 -0700 (PDT)
Subject: [R] Math symbols for labels in Perspective plots
Message-ID: <20060610053037.82219.qmail@web34012.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060609/06dad793/attachment.pl 

From ggrothendieck at gmail.com  Sat Jun 10 07:40:24 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 10 Jun 2006 01:40:24 -0400
Subject: [R] Regex engine types
In-Reply-To: <20060610024641.GG4970@ihug.co.nz>
References: <20060610024641.GG4970@ihug.co.nz>
Message-ID: <971536df0606092240p70c11ebfxc69ab6eb60019ee6@mail.gmail.com>

I get the same thing on "Version 2.3.1 Patched (2006-06-04 r38279)"
but on "R version 2.2.1, 2005-12-20" it gives character(0), as
expected, so there is some change between versions of R.  I am
on Windows XP.

On 6/9/06, Patrick Connolly <p_connolly at ihug.co.nz> wrote:
> > version
>         _
> platform x86_64-unknown-linux-gnu
> arch     x86_64
> os       linux-gnu
> system   x86_64, linux-gnu
> status
> major    2
> minor    2.1
> year     2005
> month    12
> day      20
> svn rev  36812
> language R
> >
>
> > grep("[W-Z]", LETTERS, value = TRUE)
> [1] "W" "X" "Y" "Z"
>
> That's what I'd have expected.
>
> > grep("[W-Z]", letters, value = TRUE)
> [1] "x" "y" "z"
>
> Not what I'd have thought.  However,
>
> > grep("[B-D]", letters, value = TRUE, perl = TRUE)
> character(0)
>
> So what is it that standard regular expressions use that's different
> from Perl-type ones?
>
> The help file for grep refers to POSIX 1003.2 which looked a bit
> daunting to delve into.  From my limited reading, it seems there are
> different gegex "Engine Types" which seems to be getting somewhat
> tangential to what I was working on.  I could probably avoid problems
> if I always set perl=TRUE, but it would be good to know what basic and
> extended regular expressions do that's different.  If someone has a
> quick line or two describing it, I'd be interested to know.
>
> Thanks
>
> --
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>   ___    Patrick Connolly
>  {~._.~}                         Great minds discuss ideas
>  _( Y )_                        Middle minds discuss events
> (:_~*~_:)                        Small minds discuss people
>  (_)-(_)                                   ..... Anon
>
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From hodgess at gator.dt.uh.edu  Sat Jun 10 08:23:57 2006
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Sat, 10 Jun 2006 01:23:57 -0500
Subject: [R]  Math symbols for labels in Perspective plots
Message-ID: <200606100623.k5A6NvZx008714@gator.dt.uh.edu>

Dear Harsh:

I stole these lines from the Persp examples and 
the plotmath examples. 

>      x <- seq(-10, 10, length= 30)
>      y <- x
>      f <- function(x,y) { r <- sqrt(x^2+y^2); 10 * sin(r)/r }
>      z <- outer(x, y, f)
>      z[is.na(z)] <- 1
>      op <- par(bg = "white")
>      persp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = "lightblue")
>      persp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = "lightblue",
+            ltheta = 120, shade = 0.75, ticktype = "detailed",
+            xlab = "X", ylab = "Y", zlab = "Sinc( r )"
+      ) -> res
> 
> ?plotmath
> title( expression(paste(plain(sin) * phi, "  and  ",
+                                      plain(cos) * phi)))
> 


Hope this helps!

By the way, this is from Windows.

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu


Subject: [R] Math symbols for labels in Perspective plots


Hi ..
  
     I would like to have math symbols in perspective plots
  
  i tried :  persp(x,y,z,xlab=expression(phi))
  
  but it plots it as phi.
  
  Thanks.
  
  Harsh
  
 __________________________________________________


From ripley at stats.ox.ac.uk  Sat Jun 10 08:47:07 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 10 Jun 2006 07:47:07 +0100 (BST)
Subject: [R] Regex engine types
In-Reply-To: <20060610024641.GG4970@ihug.co.nz>
References: <20060610024641.GG4970@ihug.co.nz>
Message-ID: <Pine.LNX.4.64.0606100738470.22963@gannet.stats.ox.ac.uk>

?regex does describe this:

      A range of characters may be specified by giving the first and last
      characters, separated by a hyphen.  (Character ranges are
      interpreted in the collation order of the current locale.)

You did not tell us your locale, but based on questions from you in the 
past I would guess en_NZ.utf8.  In that locale the collation order is 
wWxXyYzZ, so your surprise is explained.  (It seems the PCRE code is not 
using the same ordering in that locale.)

You may find it useful to set LC_COLLATE to C as I do:

> strsplit(Sys.getlocale(), ";")
[[1]]
  [1] "LC_CTYPE=en_GB"       "LC_NUMERIC=C"         "LC_TIME=en_GB"
  [4] "LC_COLLATE=C"         "LC_MONETARY=en_GB"    "LC_MESSAGES=en_GB"
  [7] "LC_PAPER=en_GB"       "LC_NAME=C"            "LC_ADDRESS=C"
[10] "LC_TELEPHONE=C"       "LC_MEASUREMENT=en_GB" "LC_IDENTIFICATION=C"


On Sat, 10 Jun 2006, Patrick Connolly wrote:

>> version
>         _
> platform x86_64-unknown-linux-gnu
> arch     x86_64
> os       linux-gnu
> system   x86_64, linux-gnu
> status
> major    2
> minor    2.1
> year     2005
> month    12
> day      20
> svn rev  36812
> language R
>>
>
>> grep("[W-Z]", LETTERS, value = TRUE)
> [1] "W" "X" "Y" "Z"
>
> That's what I'd have expected.
>
>> grep("[W-Z]", letters, value = TRUE)
> [1] "x" "y" "z"
>
> Not what I'd have thought.  However,
>
>> grep("[B-D]", letters, value = TRUE, perl = TRUE)
> character(0)
>
> So what is it that standard regular expressions use that's different
> from Perl-type ones?
>
> The help file for grep refers to POSIX 1003.2 which looked a bit
> daunting to delve into.  From my limited reading, it seems there are
> different gegex "Engine Types" which seems to be getting somewhat
> tangential to what I was working on.  I could probably avoid problems
> if I always set perl=TRUE, but it would be good to know what basic and
> extended regular expressions do that's different.  If someone has a
> quick line or two describing it, I'd be interested to know.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From bartjoosen at hotmail.com  Sat Jun 10 10:06:35 2006
From: bartjoosen at hotmail.com (Bart Joosen)
Date: Sat, 10 Jun 2006 10:06:35 +0200
Subject: [R] Maximum likelihood estimation of Regression parameters
Message-ID: <BAY111-DAV7F1A9A3A9C25103A68281D8890@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060610/c970de32/attachment.pl 

From xthua111 at zju.edu.cn  Sat Jun 10 11:11:27 2006
From: xthua111 at zju.edu.cn (Xiaoting Hua)
Date: Sat, 10 Jun 2006 17:11:27 +0800
Subject: [R] Maximum likelihood estimation of Regression parameters
In-Reply-To: <BAY111-DAV7F1A9A3A9C25103A68281D8890@phx.gbl>
References: <BAY111-DAV7F1A9A3A9C25103A68281D8890@phx.gbl>
Message-ID: <53ff794a0606100211w57885bd7t4d10bfaaf82d7744@mail.gmail.com>

mle(stats4)                Maximum Likelihood Estimation

is it list above what you want?

On 6/10/06, Bart Joosen <bartjoosen at hotmail.com> wrote:
> Hi,
>
> I want to use Maximum likelihood to estimate the parameters from my regression line.
> I have purchased the book "Applied linear statistical models" from Neter, Kutner, nachtsheim & Wasserman, and in one of the first chapters, they use maximum likelihood to estimate the parameters.
> Now I want to tried it for my self, but couldn't find the right function.
> In the book, they give a fixed variance to work with, but I couldn't find a function where I can estimate the predictor and where I have to give the variance.
> Or isn't this neccesairy?
> Also they calculate likelihood values for the different values, used to estimate the parameters (like a normal probability curve), is it possible to do this with R?
>
> Kind regards
>
> Bart
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
????????
??????
?????????? Kenneth Hua
??????????????
??????????
310029


From ggrothendieck at gmail.com  Sat Jun 10 14:55:28 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 10 Jun 2006 08:55:28 -0400
Subject: [R] Regex engine types
In-Reply-To: <Pine.LNX.4.64.0606100738470.22963@gannet.stats.ox.ac.uk>
References: <20060610024641.GG4970@ihug.co.nz>
	<Pine.LNX.4.64.0606100738470.22963@gannet.stats.ox.ac.uk>
Message-ID: <971536df0606100555m1539ec20tde81a368a0570d5e@mail.gmail.com>

I get the same result in a US collate ordering:

> strsplit(Sys.getlocale(), ";")
[[1]]
[1] "LC_COLLATE=English_United States.1252"
[2] "LC_CTYPE=English_United States.1252"
[3] "LC_MONETARY=English_United States.1252"
[4] "LC_NUMERIC=C"
[5] "LC_TIME=English_United States.1252"

> grep("[W-Z]", letters, value = TRUE)
[1] "x" "y" "z"
> R.version.string # Windows XP
[1] "Version 2.3.1 Patched (2006-06-04 r38279)"

On 6/10/06, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> ?regex does describe this:
>
>      A range of characters may be specified by giving the first and last
>      characters, separated by a hyphen.  (Character ranges are
>      interpreted in the collation order of the current locale.)
>
> You did not tell us your locale, but based on questions from you in the
> past I would guess en_NZ.utf8.  In that locale the collation order is
> wWxXyYzZ, so your surprise is explained.  (It seems the PCRE code is not
> using the same ordering in that locale.)
>
> You may find it useful to set LC_COLLATE to C as I do:
>
> > strsplit(Sys.getlocale(), ";")
> [[1]]
>  [1] "LC_CTYPE=en_GB"       "LC_NUMERIC=C"         "LC_TIME=en_GB"
>  [4] "LC_COLLATE=C"         "LC_MONETARY=en_GB"    "LC_MESSAGES=en_GB"
>  [7] "LC_PAPER=en_GB"       "LC_NAME=C"            "LC_ADDRESS=C"
> [10] "LC_TELEPHONE=C"       "LC_MEASUREMENT=en_GB" "LC_IDENTIFICATION=C"
>
>
> On Sat, 10 Jun 2006, Patrick Connolly wrote:
>
> >> version
> >         _
> > platform x86_64-unknown-linux-gnu
> > arch     x86_64
> > os       linux-gnu
> > system   x86_64, linux-gnu
> > status
> > major    2
> > minor    2.1
> > year     2005
> > month    12
> > day      20
> > svn rev  36812
> > language R
> >>
> >
> >> grep("[W-Z]", LETTERS, value = TRUE)
> > [1] "W" "X" "Y" "Z"
> >
> > That's what I'd have expected.
> >
> >> grep("[W-Z]", letters, value = TRUE)
> > [1] "x" "y" "z"
> >
> > Not what I'd have thought.  However,
> >
> >> grep("[B-D]", letters, value = TRUE, perl = TRUE)
> > character(0)
> >
> > So what is it that standard regular expressions use that's different
> > from Perl-type ones?
> >
> > The help file for grep refers to POSIX 1003.2 which looked a bit
> > daunting to delve into.  From my limited reading, it seems there are
> > different gegex "Engine Types" which seems to be getting somewhat
> > tangential to what I was working on.  I could probably avoid problems
> > if I always set perl=TRUE, but it would be good to know what basic and
> > extended regular expressions do that's different.  If someone has a
> > quick line or two describing it, I'd be interested to know.
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From cberry at tajo.ucsd.edu  Sat Jun 10 17:23:51 2006
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Sat, 10 Jun 2006 08:23:51 -0700
Subject: [R] Mod function?
Message-ID: <Pine.LNX.4.64.0606100816250.11592@tajo.ucsd.edu>


John,

The advice in the posting guide:

--
Do your homework before posting: [...]

     * Do help.search("keyword") with different keywords (type this at the R prompt) 
--

seems to work.

Using

 	help.search("mod")

follow the first hit:

 	?"+"

Chuck

On Fri, 9 Jun 2006, Kerpel, John wrote:

> Hi Folks!
>
>
>
> I need to execute a piece of R code inside a loop, say, every fourth
> time my counter increases (it increases by 1 unit each time.)
>
>
>
> Is there any sort of mod function in R to do this?  Or is this done some
> other way?  Many thanks!
>
>
>
> Best,
>
>
>
> john
>
>
> 	[[alternative HTML version deleted]]
>
>
>
>
>    [ Part 3.30: "Included Message" ]
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0717


From hugues_sj at yahoo.fr  Fri Jun  9 14:59:57 2006
From: hugues_sj at yahoo.fr (hugues)
Date: Fri, 9 Jun 2006 14:59:57 +0200
Subject: [R] random generation for a quasi distribution
Message-ID: <200606091300.k59D00xW010824@hypatia.math.ethz.ch>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060609/ab46b028/attachment.pl 

From aa at tango.stat.unipd.it  Thu Jun  8 21:30:31 2006
From: aa at tango.stat.unipd.it (Adelchi Azzalini)
Date: Thu, 8 Jun 2006 21:30:31 +0200
Subject: [R] Density Estimation
In-Reply-To: <BAY119-F8489EECD077D4718026CDF88B0@phx.gbl>
References: <BAY119-F8489EECD077D4718026CDF88B0@phx.gbl>
Message-ID: <20060608193031.GA7512@tango.stat.unipd.it>

On Thu, Jun 08, 2006 at 08:31:26PM +0200, Pedro Ramirez wrote:
> >In mathematical terms the optimal bandwith for density estimation
> >decreases at rate n^{-1/5}, while the one for distribution function
> >decreases at rate n^{-1/3}, if n is the sample size. In practical terms,
> >one must choose an appreciably smaller bandwidth in the second case
> >than in the first one.
> 
> Thanks a lot for your remark! I was not aware of the fact that the
> optimal bandwidths for density and distribution do not decrease
> at the same rate.
> 
> >Besides the computational aspect, there is a statistical one:
> >the optimal choice of bandwidth for estimating the density function
> >is not optimal (and possibly not even jsut sensible) for estimating
> >the distribution function, and the stated problem is equivalent to
> >estimation of the distribution function.
> 
> The given interval "0<x<3" was only an example, in fact I would
> like to estimate the probability for intervals such as
> 
> "0<=x<1" , "1<=x<2" , "2<=x<3" , "3<=x<4" , ....
> 
> and compare it with the estimates of a corresponding histogram.
> In this case the stated problem is not anymore equivalent to the
> estimation of the distribution function. What do you think, can

why not? the probabilities you are interested in are of the form

F(1)-F(0), F(2)-F(1), and so on

where F(.) if the cumulative distribution function (and it must
be continuous, since its derivative exists).

> I go a ahead in this case with the optimal bandwidth for the
> density? Thanks a lot for your help!

no

best wishes,

Adelchi

> Best wishes
> Pedro
> 
> 
> 
> 
> >best wishes,
> >
> >Adelchi
> >
> >
> >PR>
> >PR> >
> >PR> >--
> >PR> >Gregory (Greg) L. Snow Ph.D.
> >PR> >Statistical Data Center
> >PR> >Intermountain Healthcare
> >PR> >greg.snow at intermountainmail.org
> >PR> >(801) 408-8111
> >PR> >
> >PR> >
> >PR> >-----Original Message-----
> >PR> >From: r-help-bounces at stat.math.ethz.ch
> >PR> >[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pedro
> >PR> >Ramirez Sent: Wednesday, June 07, 2006 11:00 AM
> >PR> >To: r-help at stat.math.ethz.ch
> >PR> >Subject: [R] Density Estimation
> >PR> >
> >PR> >Dear R-list,
> >PR> >
> >PR> >I have made a simple kernel density estimation by
> >PR> >
> >PR> >x <- c(2,1,3,2,3,0,4,5,10,11,12,11,10)
> >PR> >kde <- density(x,n=100)
> >PR> >
> >PR> >Now I would like to know the estimated probability that a new
> >PR> >observation falls into the interval 0<x<3.
> >PR> >
> >PR> >How can I integrate over the corresponding interval?
> >PR> >In several R-packages for kernel density estimation I did not
> >PR> >found a corresponding function. I could apply Simpson's Rule for
> >PR> >integrating, but perhaps somebody knows a better solution.
> >PR> >
> >PR> >Thanks a lot for help!
> >PR> >
> >PR> >Pedro
> >PR> >
> >PR> >_________
> >PR> >
> >PR> >______________________________________________
> >PR> >R-help at stat.math.ethz.ch mailing list
> >PR> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PR> >PLEASE do read the posting guide!
> >PR> >http://www.R-project.org/posting-guide.html
> >PR> >
> >PR>
> >PR> ______________________________________________
> >PR> R-help at stat.math.ethz.ch mailing list
> >PR> https://stat.ethz.ch/mailman/listinfo/r-help
> >PR> PLEASE do read the posting guide!
> >PR> http://www.R-project.org/posting-guide.html
> >PR>
> 
> _________________________________________________________________
> Don't just search. Find. Check out the new MSN Search! 
> http://search.msn.com/

-- 
Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit? di Padova, Italia
tel. +39 049 8274147,  http://azzalini.stat.unipd.it/


From spencer.graves at pdf.com  Sat Jun 10 18:16:54 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 10 Jun 2006 09:16:54 -0700
Subject: [R] nested mixed-effect model: variance components
In-Reply-To: <BAY106-DAV244471184652FC1DD27A0BBC8B0@phx.gbl>
References: <BAY106-DAV244471184652FC1DD27A0BBC8B0@phx.gbl>
Message-ID: <448AF076.8040105@pdf.com>

	  I have seen no reply to this, so I will offer a couple of comments in 
spite of the fact that I know very little about "aov" other than it is 
old and has largely been superceded by "lme" in the "nlme" package. 
I've replied to many posts on random and mixed effects over the past few 
years, and I only remember one case where 'aov' returned an answer that 
could not have been obtained easily from 'lme'.  That one application 
involved estimating a saturated model from perfectly balanced 
experimental data.  'lme' refused to return an answer, at least as I 
specified the model, and 'aov' returned numbers from which anyone 
familiar with the theory for that special case could compute the desired 
F ratios and p-values.  In all other cases that I remember, 'aov' would 
either return the same answer, or much more commonly, a substantively 
inferior answer -- if it were feasible to use 'aov' at all.

	  I mention this, because the simplest way I can think of to check your 
answer is to suggest you try to work it using 'lme'.  To learn how to do 
this, I strongly encourage you to consult Pinheiro and Bates (2000) 
Mixed-Effects Models in S and S-PLUS (Springer).  Bates has been for 
many years one of the leading contributors in this area and is the 
primary author of the 'nlme', 'lme4' and 'Matrix" packages to support 
working with these kinds of models.  This book discusses both how to fit 
these kinds of models as well as how to produce plots of various kinds 
that can be very helpful for explaining the experimental results to 
others as well as diagnosing potential problem.  Moreover, the R scripts 
discussed in the book are available in files called "ch01.R", "ch02.R", 
..., "ch06.R", "ch08.R" in a subfolder '~library\nlme\scripts' of the 
directory in which R is installed on your hard drive.  This makes it 
much easier to work through the examples in the book one line at a time, 
experimenting at with modifications.  In addition, there is one point I 
know where the R syntax differs from that in the book:  S-Plus will 
accept x^2 in a formula as meaning the square of a numeric variable;  R 
will not.  To specify a square in R, you need something like I(x^2). 
When I copied the commands out of the book, I had serious trouble 
getting answers like those in the book until I identified and corrected 
this difference in syntax.  If you use the script files, they provide 
the R syntax.

	  I'm not certain, but I believe the following should estimate the 
model you wrote below:

	  fit <- lme(fixed=COVER ~ HABITAT,
		    random = ~1|LAGOON/HABITAT,
		    data=cov).

	  For comparison, I refer you to Pinheiro and Bates, p. 47, where I 
find the following:

fm1Oats <- lme( yield ~ ordered(nitro) * Variety, data = Oats,
   random = ~ 1 | Block/Variety )

	  There are 3 Varieties and 6 Blocks in this Oats data.frame.  The 
fixed effect of Variety has therefore 2 degrees of freedom.  However, 
the random effect of Variety occurs in 18 levels, being all the 6*3 
Block:Variety combinations.  You can see this by examining 'str(fm1Oats)'.

	  If you want to know "how much variation is due to lagoons? habitats? 
lagoons*habitat? transects?", this model will NOT tell you that, and I 
don't know how to answer that question using 'lme'.  I was able to 
answer a question like that using 'lmer' associated with the 'lme4' and 
'Matrix' packages.  Unfortunately, these packages have some names 
conflicts with 'nlme', and the simplest way I know to change from one to 
the other is to "q()" and restart R  Before I did, however, I made a 
local copy of the "Oats" data.frame.  After I did that, I seemed to get 
sensible numbers from the following:

library(lme4)

fm1Oats4 <- lmer(yield~ ordered(nitro) * Variety
         +(1|Block)+(1|Variety)+(1|Block:Variety),
                  data=Oats)

	  For both "lme" and "lmer", the default "method" is "REML" (restricted 
maximum likelihood).  This is what you want for estimation.  For testing 
random effects, you still want "REML", but you should adjust the degrees 
of freedom of the reference "F" distribution as discussed in section 2.4 
of Pinheiro and Bates;  this also applies to confidence intervals for 
the random effects.  For testing fixed effects, you should use "method = 
'ML'".

	  "lme4" is newer than "nlme" and does not currently have available the 
complete set of helper functions for plotting, etc.  Thus, you will 
likely want to use both.  For documentation on 'lmer', you should still 
start with Pinheiro and Bates for the general theory, then refer to the 
vignettes associated with the "mlmRev" and "lme4" packages;  if you 
don't know about vignettes RSiteSearch("graves vignette") will lead you 
to "http://finzi.psych.upenn.edu/R/Rhelp02a/archive/67006.html" and 
other replies to r-help where I've described how to use them.  You will 
also want the relevant article by Doug Bates in R News.  To find that, 
go "www.r-project.org" -> Documentation:  Newsletter -> Download: 
"Table of Contents", then search for Bates until you find "Douglas 
Bates. Fitting linear mixed models in R. R News, 5(1):27-30, May 2005."
That says you want vol. 5(1).

	  Hope this helps,
	  Spencer Graves

Eric Pante wrote:
> Dear listers,
> 
> I am trying to assess variance components for a nested, mixed-effects 
> model. I think I got an answer that make sense from R, but I have a 
> warning message and I wanted to check that what I am looking at is 
> actually what I need:
> 
> my data are organized as transects within stations, stations within 
> habitats, habitats within lagoons.
> lagoons: random, habitats: fixed
> the question is: how much variation is due to lagoons? habitats? 
> lagoons*habitat? transects?
> 
> Here is my code:
> 
> res <- aov(COVER ~ HABITAT + Error(HABITAT+LAGOON+LAGOON/HABITAT), 
> data=cov)
> summary(res)
> 
> and I get Sum Sq for each to calculate variance components:
> 
> Error: STRATE
>         Df Sum Sq Mean Sq
> STRATE  5 4493.1   898.6
> 
> Error: ATOLL
>            Df Sum Sq Mean Sq F value Pr(>F)
> Residuals  5 3340.5   668.1
> 
> Error: STRATE:ATOLL
>            Df  Sum Sq Mean Sq F value Pr(>F)
> Residuals 18 2442.71  135.71
> 
> Error: Within
>             Df Sum Sq Mean Sq F value Pr(>F)
> Residuals 145 6422.0    44.3
> 
> My error message seems to come from the LAGOON/HABITAT, the Error is 
> computed.
> Warning message: Error() model is singular in: aov(COVER ~ HABITAT + 
> Error(HABITAT+LAGOON+LAGOON/HABITAT), data=cov),
> 
> THANKS !!!
> eric
> 
> 
> 
> Eric Pante
> ----------------------------------------------------------------
> College of Charleston, Grice Marine Laboratory
> 205 Fort Johnson Road, Charleston SC 29412
> Phone: 843-953-9190 (lab)  -9200 (main office)
> ----------------------------------------------------------------
> 
> 	"On ne force pas la curiosite, on l'eveille ..."
> 	Daniel Pennac
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From glarowe at indiana.edu  Sat Jun 10 19:53:29 2006
From: glarowe at indiana.edu (g l)
Date: Sat, 10 Jun 2006 13:53:29 -0400
Subject: [R] sparse matrix, rnorm, malloc
Message-ID: <3D149021-18ED-48EE-AAC0-5B7FB2DC004E@indiana.edu>

Hi,

I'm Sorry for any cross-posting. I've reviewed the archives and could  
not find an exact answer to my question below.

I'm trying to generate very large sparse matrices (< 1% non-zero  
entries per row). I have a sparse matrix function below which works  
well until the row/col count exceeds 10,000. This is being run on a  
machine with 32G memory:

sparse_matrix <- function(dims,rnd,p) {
         ptm <- proc.time()
         x <- round(rnorm(dims*dims),rnd)
         x[((abs(x) - p) < 0)] <- 0
         y <- matrix(x,nrow=dims,ncol=dims)
         proc.time() - ptm
}

When trying to generate the matrix around 20,000 rows/cols on a  
machine with 32G of memory, the error message I receive is:

R(335) malloc: *** vm_allocate(size=3200004096) failed (error code=3)
R(335) malloc: *** error: can't allocate region
R(335) malloc: *** set a breakpoint in szone_error to debug
R(335) malloc: *** vm_allocate(size=3200004096) failed (error code=3)
R(335) malloc: *** error: can't allocate region
R(335) malloc: *** set a breakpoint in szone_error to debug
Error: cannot allocate vector of size 3125000 Kb
Error in round(rnorm(dims * dims), rnd) : unable to find the argument  
'x' in selecting a method for function 'round'

* Last error line is obvious. Question:  on machine w/32G memory, why  
can't it allocate a vector of size 3125000 Kb?

When trying to generate the matrix around 30,000 rows/cols, the error  
message I receive is:

Error in rnorm(dims * dims) : cannot allocate vector of length 900000000
Error in round(rnorm(dims * dims), rnd) : unable to find the argument  
'x' in selecting a method for function 'round'

* Last error line is obvious. Question: is this 900000000 bytes?  
kilobytes? This error seems to be specific now to rnorm, but it  
doesn't indicate the length metric (b/Kb/Mb) as it did for 20,000  
rows/cols. Even if this Mb, why can't this be allocated on a machine  
with 32G free memory?

When trying to generate the matrix with over 50,000 rows/cols, the  
error message I receive is:

Error in rnorm(n, mean, sd) : invalid arguments
In addition: Warning message:
NAs introduced by coercion
Error in round(rnorm(dims * dims), rnd) : unable to find the argument  
'x' in selecting a method for function 'round'

* Same.

Why would it generate different errors in each case? Code fixes? Any  
simple ways to generate sparse matrices which would avoid above  
problems?

Thanks in advance,

Gavin


From roger at ysidro.econ.uiuc.edu  Sat Jun 10 20:16:13 2006
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Sat, 10 Jun 2006 13:16:13 -0500
Subject: [R] sparse matrix, rnorm, malloc
In-Reply-To: <3D149021-18ED-48EE-AAC0-5B7FB2DC004E@indiana.edu>
References: <3D149021-18ED-48EE-AAC0-5B7FB2DC004E@indiana.edu>
Message-ID: <ECA74E1C-6DAE-4C7B-A11E-E85C07E83E89@ysidro.econ.uiuc.edu>

You need to look at the packages specifically designed  for
sparse matrices:  SparseM and Matrix.


url:    www.econ.uiuc.edu/~roger                Roger Koenker
email   rkoenker at uiuc.edu                       Department of Economics
vox:    217-333-4558                            University of Illinois
fax:    217-244-6678                            Champaign, IL 61820


On Jun 10, 2006, at 12:53 PM, g l wrote:

> Hi,
>
> I'm Sorry for any cross-posting. I've reviewed the archives and could
> not find an exact answer to my question below.
>
> I'm trying to generate very large sparse matrices (< 1% non-zero
> entries per row). I have a sparse matrix function below which works
> well until the row/col count exceeds 10,000. This is being run on a
> machine with 32G memory:
>
> sparse_matrix <- function(dims,rnd,p) {
>          ptm <- proc.time()
>          x <- round(rnorm(dims*dims),rnd)
>          x[((abs(x) - p) < 0)] <- 0
>          y <- matrix(x,nrow=dims,ncol=dims)
>          proc.time() - ptm
> }
>
> When trying to generate the matrix around 20,000 rows/cols on a
> machine with 32G of memory, the error message I receive is:
>
> R(335) malloc: *** vm_allocate(size=3200004096) failed (error code=3)
> R(335) malloc: *** error: can't allocate region
> R(335) malloc: *** set a breakpoint in szone_error to debug
> R(335) malloc: *** vm_allocate(size=3200004096) failed (error code=3)
> R(335) malloc: *** error: can't allocate region
> R(335) malloc: *** set a breakpoint in szone_error to debug
> Error: cannot allocate vector of size 3125000 Kb
> Error in round(rnorm(dims * dims), rnd) : unable to find the argument
> 'x' in selecting a method for function 'round'
>
> * Last error line is obvious. Question:  on machine w/32G memory, why
> can't it allocate a vector of size 3125000 Kb?
>
> When trying to generate the matrix around 30,000 rows/cols, the error
> message I receive is:
>
> Error in rnorm(dims * dims) : cannot allocate vector of length  
> 900000000
> Error in round(rnorm(dims * dims), rnd) : unable to find the argument
> 'x' in selecting a method for function 'round'
>
> * Last error line is obvious. Question: is this 900000000 bytes?
> kilobytes? This error seems to be specific now to rnorm, but it
> doesn't indicate the length metric (b/Kb/Mb) as it did for 20,000
> rows/cols. Even if this Mb, why can't this be allocated on a machine
> with 32G free memory?
>
> When trying to generate the matrix with over 50,000 rows/cols, the
> error message I receive is:
>
> Error in rnorm(n, mean, sd) : invalid arguments
> In addition: Warning message:
> NAs introduced by coercion
> Error in round(rnorm(dims * dims), rnd) : unable to find the argument
> 'x' in selecting a method for function 'round'
>
> * Same.
>
> Why would it generate different errors in each case? Code fixes? Any
> simple ways to generate sparse matrices which would avoid above
> problems?
>
> Thanks in advance,
>
> Gavin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html


From spencer.graves at pdf.com  Sat Jun 10 20:24:10 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 10 Jun 2006 11:24:10 -0700
Subject: [R] nested mixed-effect model: variance components
In-Reply-To: <448AF076.8040105@pdf.com>
References: <BAY106-DAV244471184652FC1DD27A0BBC8B0@phx.gbl>
	<448AF076.8040105@pdf.com>
Message-ID: <448B0E4A.9070508@pdf.com>

<see inline>  	

Eric Pante wrote:
 > Hi Spencer,
 >
 > First, thank you very much for taking the time to write this detailed
 > reply !
 > I did try exactly the formula you suggested:
 >
 >       fit <- lme(fixed=COVER ~ HABITAT, random = ~1|LAGOON/HABITAT,
 > data=cov)
 >
 > before writing my post, and indeed, neither summary() or anova()
 > returned the sums of squares to assess variance components.

	  Do you want sums of squares or estimates of variance components?  I 
believe that for over half a century, there is been a substantial 
consensus among leading experts in statistical methods that maximum 
likelihood (or Bayesian posterior where an adequate prior exists) is 
theoretically the best method for most statistical questions.   With 
mixed effects, this is interpreted as requiring the maximization of the 
marginal likelihood (often after integrating out fixed effects to obtain 
the "restricted likelihood" maximized with method = "REML").

	  However, until the last couple of decades, ML or REML was not 
computationally feasible for many cases lacking balance.  This led to an 
extensive literature on alternative methods like MINQUE.  These methods 
are now substantially obsolete as far as I know except in cases with 
appropriate balance where they produce exactly the same answers as ML or 
REML.

	  If you want estimates of the variance components, then use 'lme': 
With appropriately balanced data sets, these would be exactly what you 
would get by writing out the expected mean squares and solving for the 
variance components.  Where the balance is lacking, the other methods 
will generally produce less efficient estimates of what you really want. 
  If you want sums of squares for testing, do your testing as described 
in Pinheiro and Bates.  Except in the saturated case I mentioned, you 
should get identical or superior estimates from lme and lmer than from 
aov.

	  If you want sums of squares for something other than an intermediate 
step for estimation or testing, please explain.

	  Be careful what you ask for, because you might get it -- and it might 
not be what you want.

 > how do you specify in the formula that you want a nested approach (I
 > will check the Pinheiro and Bates book) ? For these two reasons, I had
 > the feeling that aov might be the way to go ...

	  I understood you to say that you thought habitat should be nested 
within lagoon, but you also want to know "how much variation is due to 
lagoons? habitats? lagoons*habitat?".  That sounds to me like you want 
to know how to clean the spark plugs on your bicycle.

	  Have you studied the "Oats" example in my post?  (See also 
"http://finzi.psych.upenn.edu/R/Rhelp02a/archive/63788.html".)

	  Bon Chance!
	  J?esp?re que ceci vous aide.
	  Spencer Graves
 >
 > I will try your suggestions concerning the lme4 package.
 >
 > Thanks again !
 > eric pante
 >
 > Eric Pante

Spencer Graves wrote:
>       I have seen no reply to this, so I will offer a couple of comments 
> in spite of the fact that I know very little about "aov" other than it 
> is old and has largely been superceded by "lme" in the "nlme" package. 
> I've replied to many posts on random and mixed effects over the past few 
> years, and I only remember one case where 'aov' returned an answer that 
> could not have been obtained easily from 'lme'.  That one application 
> involved estimating a saturated model from perfectly balanced 
> experimental data.  'lme' refused to return an answer, at least as I 
> specified the model, and 'aov' returned numbers from which anyone 
> familiar with the theory for that special case could compute the desired 
> F ratios and p-values.  In all other cases that I remember, 'aov' would 
> either return the same answer, or much more commonly, a substantively 
> inferior answer -- if it were feasible to use 'aov' at all.
> 
>       I mention this, because the simplest way I can think of to check 
> your answer is to suggest you try to work it using 'lme'.  To learn how 
> to do this, I strongly encourage you to consult Pinheiro and Bates 
> (2000) Mixed-Effects Models in S and S-PLUS (Springer).  Bates has been 
> for many years one of the leading contributors in this area and is the 
> primary author of the 'nlme', 'lme4' and 'Matrix" packages to support 
> working with these kinds of models.  This book discusses both how to fit 
> these kinds of models as well as how to produce plots of various kinds 
> that can be very helpful for explaining the experimental results to 
> others as well as diagnosing potential problem.  Moreover, the R scripts 
> discussed in the book are available in files called "ch01.R", "ch02.R", 
> ..., "ch06.R", "ch08.R" in a subfolder '~library\nlme\scripts' of the 
> directory in which R is installed on your hard drive.  This makes it 
> much easier to work through the examples in the book one line at a time, 
> experimenting at with modifications.  In addition, there is one point I 
> know where the R syntax differs from that in the book:  S-Plus will 
> accept x^2 in a formula as meaning the square of a numeric variable;  R 
> will not.  To specify a square in R, you need something like I(x^2). 
> When I copied the commands out of the book, I had serious trouble 
> getting answers like those in the book until I identified and corrected 
> this difference in syntax.  If you use the script files, they provide 
> the R syntax.
> 
>       I'm not certain, but I believe the following should estimate the 
> model you wrote below:
> 
>       fit <- lme(fixed=COVER ~ HABITAT,
>             random = ~1|LAGOON/HABITAT,
>             data=cov).
> 
>       For comparison, I refer you to Pinheiro and Bates, p. 47, where I 
> find the following:
> 
> fm1Oats <- lme( yield ~ ordered(nitro) * Variety, data = Oats,
>   random = ~ 1 | Block/Variety )
> 
>       There are 3 Varieties and 6 Blocks in this Oats data.frame.  The 
> fixed effect of Variety has therefore 2 degrees of freedom.  However, 
> the random effect of Variety occurs in 18 levels, being all the 6*3 
> Block:Variety combinations.  You can see this by examining 'str(fm1Oats)'.
> 
>       If you want to know "how much variation is due to lagoons? 
> habitats? lagoons*habitat? transects?", this model will NOT tell you 
> that, and I don't know how to answer that question using 'lme'.  I was 
> able to answer a question like that using 'lmer' associated with the 
> 'lme4' and 'Matrix' packages.  Unfortunately, these packages have some 
> names conflicts with 'nlme', and the simplest way I know to change from 
> one to the other is to "q()" and restart R  Before I did, however, I 
> made a local copy of the "Oats" data.frame.  After I did that, I seemed 
> to get sensible numbers from the following:
> 
> library(lme4)
> 
> fm1Oats4 <- lmer(yield~ ordered(nitro) * Variety
>         +(1|Block)+(1|Variety)+(1|Block:Variety),
>                  data=Oats)
> 
>       For both "lme" and "lmer", the default "method" is "REML" 
> (restricted maximum likelihood).  This is what you want for estimation.  
> For testing random effects, you still want "REML", but you should adjust 
> the degrees of freedom of the reference "F" distribution as discussed in 
> section 2.4 of Pinheiro and Bates;  this also applies to confidence 
> intervals for the random effects.  For testing fixed effects, you should 
> use "method = 'ML'".
> 
>       "lme4" is newer than "nlme" and does not currently have available 
> the complete set of helper functions for plotting, etc.  Thus, you will 
> likely want to use both.  For documentation on 'lmer', you should still 
> start with Pinheiro and Bates for the general theory, then refer to the 
> vignettes associated with the "mlmRev" and "lme4" packages;  if you 
> don't know about vignettes RSiteSearch("graves vignette") will lead you 
> to "http://finzi.psych.upenn.edu/R/Rhelp02a/archive/67006.html" and 
> other replies to r-help where I've described how to use them.  You will 
> also want the relevant article by Doug Bates in R News.  To find that, 
> go "www.r-project.org" -> Documentation:  Newsletter -> Download: "Table 
> of Contents", then search for Bates until you find "Douglas Bates. 
> Fitting linear mixed models in R. R News, 5(1):27-30, May 2005."
> That says you want vol. 5(1).
> 
>       Hope this helps,
>       Spencer Graves
> 
> Eric Pante wrote:
>> Dear listers,
>>
>> I am trying to assess variance components for a nested, mixed-effects 
>> model. I think I got an answer that make sense from R, but I have a 
>> warning message and I wanted to check that what I am looking at is 
>> actually what I need:
>>
>> my data are organized as transects within stations, stations within 
>> habitats, habitats within lagoons.
>> lagoons: random, habitats: fixed
>> the question is: how much variation is due to lagoons? habitats? 
>> lagoons*habitat? transects?
>>
>> Here is my code:
>>
>> res <- aov(COVER ~ HABITAT + Error(HABITAT+LAGOON+LAGOON/HABITAT), 
>> data=cov)
>> summary(res)
>>
>> and I get Sum Sq for each to calculate variance components:
>>
>> Error: STRATE
>>         Df Sum Sq Mean Sq
>> STRATE  5 4493.1   898.6
>>
>> Error: ATOLL
>>            Df Sum Sq Mean Sq F value Pr(>F)
>> Residuals  5 3340.5   668.1
>>
>> Error: STRATE:ATOLL
>>            Df  Sum Sq Mean Sq F value Pr(>F)
>> Residuals 18 2442.71  135.71
>>
>> Error: Within
>>             Df Sum Sq Mean Sq F value Pr(>F)
>> Residuals 145 6422.0    44.3
>>
>> My error message seems to come from the LAGOON/HABITAT, the Error is 
>> computed.
>> Warning message: Error() model is singular in: aov(COVER ~ HABITAT + 
>> Error(HABITAT+LAGOON+LAGOON/HABITAT), data=cov),
>>
>> THANKS !!!
>> eric
>>
>>
>>
>> Eric Pante
>> ----------------------------------------------------------------
>> College of Charleston, Grice Marine Laboratory
>> 205 Fort Johnson Road, Charleston SC 29412
>> Phone: 843-953-9190 (lab)  -9200 (main office)
>> ----------------------------------------------------------------
>>
>>     "On ne force pas la curiosite, on l'eveille ..."
>>     Daniel Pennac
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>


From gabidiaz at gmail.com  Sat Jun 10 20:31:41 2006
From: gabidiaz at gmail.com (Gabriel Diaz)
Date: Sat, 10 Jun 2006 20:31:41 +0200
Subject: [R] R usage for log analysis
Message-ID: <82c890d00606101131u11818d90mf80ec489f005686c@mail.gmail.com>

Hello,

Is there any software project that uses R to do log file analisys?

thanks

gabi


From pierce.gregory at gmail.com  Sat Jun 10 20:52:41 2006
From: pierce.gregory at gmail.com (Gregory Pierce)
Date: Sat, 10 Jun 2006 14:52:41 -0400
Subject: [R] Calculating survival for set time intervals
Message-ID: <1149965561.10012.10.camel@ibm-laptop>

Hello friends and fellow R users,

I have successfully tabulated and entered my survival data into R and
have generated survival curves. But I would like to be able to determine
what the survival rates are now at one month, three months, six months
and one year.

I have a data set, via.wall, which I have entered into R, and which
generates the following Surv object:

Surv(Days,Status==1)
  [1] 648+   3  109  241   99    7  849+ 105    3+ 539+ 121   42  490
21  870+
 [16] 175   20  434  289  826+ 831+ 664  698+   5   24  655+  18    7+
85+  65+
 [31] 547+   8    1   55+  69  499+ 448+   0  158+  31  246+ 230+  19
118+  54
 [46]  48+  45+  21+ 670+ 585  558+ 544+ 494  481+ 474+ 472+ 461  447
446+ 443+
 [61] 429+ 423+ 401  395+ 390  390+ 389+ 383+ 383+ 373+ 362+ 354  344+
342  336+
 [76] 335+ 326+ 306  300+ 292  284+ 280+ 271  246+ 237+ 234  233+ 233
230+ 230+
 [91] 226+ 225+ 218+ 215  211+ 199+ 191+ 191  190+ 184+ 169+ 163+ 161+
153  150
[106] 129+ 110+ 107+ 100+  84+  77+  69+  52+  38+  11+
> names(wall.via)
 [1] "Description" "Patient"     "Physician"   "MRN"         "Age"
 [6] "Status"      "Days"        "Cr"          "INR"         "BR"
[11] "MELD"        "type"

I can guess pretty accurately by looking at the graph what the survival
rates are at each interval, but I would like to understand how to
instruct R to calculate it. Hope I have made this clear. I am just a
beginner, so forgive me if this is trivial. It just isn't clear to me.

Thanks,
Greg


From spencer.graves at pdf.com  Sat Jun 10 22:29:04 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 10 Jun 2006 13:29:04 -0700
Subject: [R] random generation for a quasi distribution
In-Reply-To: <200606091300.k59D00xW010824@hypatia.math.ethz.ch>
References: <200606091300.k59D00xW010824@hypatia.math.ethz.ch>
Message-ID: <448B2B90.4030508@pdf.com>

	  Any "quasi distribution" is not well defined, which is why it is 
called "qausi".  Users decide that its likelihood "looks like" some 
other distribution, e.g. a Poisson, but it is "overdispersed".

	  If you want to simulate a "quasi binomial", you have to select a 
plausible "overdispersion" mechanism.  You could for example decide that 
the Poisson parameter follows a lognormal or a gamma distribution.  Then 
simulate the lognormal or gamma random variables and use those to 
simulate the poisson.

	  Example:

 > set.seed(5)
 > (Lam <- rlnorm(2, sdlog=9))
[1] 5.168803e-04 2.576182e+05
 > rpois(2, Lam)
[1]      0 257885

	  hope this helps
	  Spencer Graves

hugues wrote:
> Dear R-Help,
>  
> As with the rpois() function to generate random data for a poisson
> distribution, I need to generate random data for a quasi distribution with
> var=mu^2.
> Does anyone known how to do this? 
>  
> Thanks in advance,
>  
> Hugues SANTIN-JANIN.
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ripley at stats.ox.ac.uk  Sat Jun 10 22:42:54 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 10 Jun 2006 21:42:54 +0100 (BST)
Subject: [R] sparse matrix, rnorm, malloc
In-Reply-To: <3D149021-18ED-48EE-AAC0-5B7FB2DC004E@indiana.edu>
References: <3D149021-18ED-48EE-AAC0-5B7FB2DC004E@indiana.edu>
Message-ID: <Pine.LNX.4.64.0606102126330.31098@gannet.stats.ox.ac.uk>

On Sat, 10 Jun 2006, g l wrote:

> Hi,
>
> I'm Sorry for any cross-posting. I've reviewed the archives and could
> not find an exact answer to my question below.
>
> I'm trying to generate very large sparse matrices (< 1% non-zero
> entries per row). I have a sparse matrix function below which works
> well until the row/col count exceeds 10,000. This is being run on a
> machine with 32G memory:
>
> sparse_matrix <- function(dims,rnd,p) {
>         ptm <- proc.time()
>         x <- round(rnorm(dims*dims),rnd)
>         x[((abs(x) - p) < 0)] <- 0
>         y <- matrix(x,nrow=dims,ncol=dims)
>         proc.time() - ptm
> }
>
> When trying to generate the matrix around 20,000 rows/cols on a
> machine with 32G of memory, the error message I receive is:
>
> R(335) malloc: *** vm_allocate(size=3200004096) failed (error code=3)
> R(335) malloc: *** error: can't allocate region
> R(335) malloc: *** set a breakpoint in szone_error to debug
> R(335) malloc: *** vm_allocate(size=3200004096) failed (error code=3)
> R(335) malloc: *** error: can't allocate region
> R(335) malloc: *** set a breakpoint in szone_error to debug
> Error: cannot allocate vector of size 3125000 Kb
> Error in round(rnorm(dims * dims), rnd) : unable to find the argument
> 'x' in selecting a method for function 'round'
>
> * Last error line is obvious. Question:  on machine w/32G memory, why
> can't it allocate a vector of size 3125000 Kb?
>
> When trying to generate the matrix around 30,000 rows/cols, the error
> message I receive is:
>
> Error in rnorm(dims * dims) : cannot allocate vector of length 900000000
> Error in round(rnorm(dims * dims), rnd) : unable to find the argument
> 'x' in selecting a method for function 'round'
>
> * Last error line is obvious. Question: is this 900000000 bytes?
> kilobytes? This error seems to be specific now to rnorm, but it
> doesn't indicate the length metric (b/Kb/Mb) as it did for 20,000
> rows/cols. Even if this Mb, why can't this be allocated on a machine
> with 32G free memory?

This is a length of 900000000, as it says.  Please read ?"Memory-limits"
for the limits in force.  (A numeric vector of that length would be over 
2^32 bytes and so exceed the address space of a 32-bit executable.)

You have not told us your platform or other basic facts:

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

and had you heeded that request we would have had a lot more to go on.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From roger at ysidro.econ.uiuc.edu  Sun Jun 11 01:13:31 2006
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Sat, 10 Jun 2006 18:13:31 -0500
Subject: [R] sparse matrix, rnorm, malloc
In-Reply-To: <3D149021-18ED-48EE-AAC0-5B7FB2DC004E@indiana.edu>
References: <3D149021-18ED-48EE-AAC0-5B7FB2DC004E@indiana.edu>
Message-ID: <BA9319C6-871F-492E-AC11-2C446FB60D8B@ysidro.econ.uiuc.edu>


As an example of how one might do this sort of thing in SparseM
ignoring the rounding aspect...

require(SparseM)
require(msm) #for rtnorm
sm <- function(dim,rnd,q){
         n <- rbinom(1, dim * dim, 2 * pnorm(q) - 1)
         ia <- sample(dim,n,replace = TRUE)
         ja <- sample(dim,n,replace = TRUE)
         ra <- rtnorm(n,lower = -q, upper = q)
         A <- new("matrix.coo", ia = as.integer(ia), ja = as.integer 
(ja), ra = ra, dimension = as.integer(c(dim,dim)))
         A <- as.matrix.csr(A)
         }

For dim = 5000 and q = .03 which exceeds Gavin's suggested  1 percent
density, this takes about 30 seconds on my imac and according to Rprof
about 95 percent of that (total) time is spent generating the  
truncated normals.
Word of warning:  pushing this too much further  gets tedious  since the
number of random numbers grows like dim^2.  For example, dim = 20,000
and q = .02 takes 432 seconds with again 93% of the total time spent in
rnorm and rtnorm...


url:    www.econ.uiuc.edu/~roger                Roger Koenker
email   rkoenker at uiuc.edu                       Department of Economics
vox:    217-333-4558                            University of Illinois
fax:    217-244-6678                            Champaign, IL 61820


On Jun 10, 2006, at 12:53 PM, g l wrote:

> Hi,
>
> I'm Sorry for any cross-posting. I've reviewed the archives and could
> not find an exact answer to my question below.
>
> I'm trying to generate very large sparse matrices (< 1% non-zero
> entries per row). I have a sparse matrix function below which works
> well until the row/col count exceeds 10,000. This is being run on a
> machine with 32G memory:
>
> sparse_matrix <- function(dims,rnd,p) {
>          ptm <- proc.time()
>          x <- round(rnorm(dims*dims),rnd)
>          x[((abs(x) - p) < 0)] <- 0
>          y <- matrix(x,nrow=dims,ncol=dims)
>          proc.time() - ptm
> }
>
> When trying to generate the matrix around 20,000 rows/cols on a
> machine with 32G of memory, the error message I receive is:
>
> R(335) malloc: *** vm_allocate(size=3200004096) failed (error code=3)
> R(335) malloc: *** error: can't allocate region
> R(335) malloc: *** set a breakpoint in szone_error to debug
> R(335) malloc: *** vm_allocate(size=3200004096) failed (error code=3)
> R(335) malloc: *** error: can't allocate region
> R(335) malloc: *** set a breakpoint in szone_error to debug
> Error: cannot allocate vector of size 3125000 Kb
> Error in round(rnorm(dims * dims), rnd) : unable to find the argument
> 'x' in selecting a method for function 'round'
>
> * Last error line is obvious. Question:  on machine w/32G memory, why
> can't it allocate a vector of size 3125000 Kb?
>
> When trying to generate the matrix around 30,000 rows/cols, the error
> message I receive is:
>
> Error in rnorm(dims * dims) : cannot allocate vector of length  
> 900000000
> Error in round(rnorm(dims * dims), rnd) : unable to find the argument
> 'x' in selecting a method for function 'round'
>
> * Last error line is obvious. Question: is this 900000000 bytes?
> kilobytes? This error seems to be specific now to rnorm, but it
> doesn't indicate the length metric (b/Kb/Mb) as it did for 20,000
> rows/cols. Even if this Mb, why can't this be allocated on a machine
> with 32G free memory?
>
> When trying to generate the matrix with over 50,000 rows/cols, the
> error message I receive is:
>
> Error in rnorm(n, mean, sd) : invalid arguments
> In addition: Warning message:
> NAs introduced by coercion
> Error in round(rnorm(dims * dims), rnd) : unable to find the argument
> 'x' in selecting a method for function 'round'
>
> * Same.
>
> Why would it generate different errors in each case? Code fixes? Any
> simple ways to generate sparse matrices which would avoid above
> problems?
>
> Thanks in advance,
>
> Gavin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html


From spencer.graves at pdf.com  Sun Jun 11 02:18:00 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 10 Jun 2006 17:18:00 -0700
Subject: [R] date.mdy in date package
In-Reply-To: <20060609191317.17789.qmail@web34307.mail.mud.yahoo.com>
References: <20060609191317.17789.qmail@web34307.mail.mud.yahoo.com>
Message-ID: <448B6138.4010104@pdf.com>

	  Evidently, your 'trimmed_dates' was NOT a "a Julian date value, as 
returned by 'mdy.date()', number of days since 1/1/1960."
	
	  My standard references for this kind of thing are the "zoo" vignette 
and the R News article, Gabor Grothendieck and Thomas Petzoldt. R help 
desk: Date and time classes in R. R News, 4(1):29-32, June 2004 
(www.r-project.org -> Documentation:  Newsletter).  For more on 
vignettes and "zoo" in particular, see 
"http://finzi.psych.upenn.edu/R/Rhelp02a/archive/67006.html".

	  With my limited knowledge of date-time formates, I might approach 
your problem as follows:


 > Lines <-
+ "1  1/2/2003    2  1/3/2003    3  1/6/2003    4  1/7/2003    5 
1/8/2003    6  1/9/2003    7  1/10/2003    8  1/13/2003    9  1/14/2003"
 > (DF <- read.table(textConnection(Lines)))
   V1       V2 V3       V4 V5       V6 V7       V8 V9      V10 V11 
V12 V13
1  1 1/2/2003  2 1/3/2003  3 1/6/2003  4 1/7/2003  5 1/8/2003   6 
1/9/2003   7
         V14 V15       V16 V17       V18
1 1/10/2003   8 1/13/2003   9 1/14/2003
 > (Dates <- sapply(DF[2*(1:9)], as.character))
          V2          V4          V6          V8         V10         V12
  "1/2/2003"  "1/3/2003"  "1/6/2003"  "1/7/2003"  "1/8/2003"  "1/9/2003"
         V14         V16         V18
"1/10/2003" "1/13/2003" "1/14/2003"
 > (Dates. <- strptime(Dates, "%m/%d/%Y"))
[1] "2003-01-02" "2003-01-03" "2003-01-06" "2003-01-07" "2003-01-08"
[6] "2003-01-09" "2003-01-10" "2003-01-13" "2003-01-14"
 > class(Dates.)
[1] "POSIXt"  "POSIXlt"
 > names(Dates.)
[1] "sec"   "min"   "hour"  "mday"  "mon"   "year"  "wday"  "yday"  "isdst"
 >
	  Thus, the 'POSIXlt' format is similar to the output of 'date.mdy', 
except that it has other attributes, which you could ignore.  Getting 
"month / days with leading zeros" is a formatting issue.  If you still 
can't figure that out after studying the Grothendieck and Petzoldt 
article and the zoo vignette, please submit another post.

	  hope this helps.
	  Spencer Graves

Brian Scholl wrote:
> I'm having a problem with output from date.mdy in the date package. 
>    
>   Goal: to take a long vector of dates of the form "01/22/99" and extract values month="01", day="22", year="1999".  
>    
>   I am providing the vector of class dates in the attached file to date.mdy: 
>    
>   > mdy_dates<-date.mdy(trimmed_dates)
>    
>   The first few obs of the attached vector of dates are: 
>               1  1/2/2003    2  1/3/2003    3  1/6/2003    4  1/7/2003    5  1/8/2003    6  1/9/2003    7  1/10/2003    8  1/13/2003    9  1/14/2003
>    
>   I am expecting to get a list with vectors for the day, month, and year.  The function appears to spit out the format I expect (though I would like months/days with leading zeros) but incorrect values, But what I get in the first few lines is something like: 
>    
>   > mdy_dates
> $month
>   [1]  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  2  2
>  [26]  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  3  3  3  3  3  3  3  3
>  [51]  3  3  3  3  3  3  3  3  3  3  3  3  3  3  4  4  4  4  4  4  4  4  4  4  4
> $day
>   [1]  1  2  5  6  7  8  9 12 13 14 15 16 19 20 21 22 23 26 27 28 29 30  2  3  4
>  [26]  5  6  9 10 11 12 13 16 17 18 19 20 23 24 25 26 27  2  3  4  5  6  9 10 11
>  [51] 12 13 16 17 18 19 20 23 24 25 26 27 30 31  1  2  3  6  7  8  9 10 13 14 15
> $year
>   [1] 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993
>  [16] 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993
>  [31] 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 1993
> 
>   So: The first day of my list of dates is Jan 2, 2003, but in the list output the first date is Jan 1, 1993.  Have I incorrectly formatted my input, or is there some other problem? 
>    
>   Thanks in advance. 
> 
>  __________________________________________________
> 
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From spencer.graves at pdf.com  Sun Jun 11 03:02:53 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 10 Jun 2006 18:02:53 -0700
Subject: [R] question about using ancova in R to do variable selection
In-Reply-To: <20060609202006.76502.qmail@web34003.mail.mud.yahoo.com>
References: <20060609202006.76502.qmail@web34003.mail.mud.yahoo.com>
Message-ID: <448B6BBD.1020203@pdf.com>

	  Have you tried Bioconductor (www.bioconductor.org)?  I haven't seen 
any replies to your question, and I know next to nothing about 
microarray data.  I understand that Bioconductor specializes in 
microarray and related data, has a listserve, etc.

	  hope this helps,
	  Spencer Graves

James Anderson wrote:
> Hi,
>   I have a microarray data which has about 10000 genes and 15 measurements. The 15 measurements contain 6 control, 3 low dose, 3 medium dose, 3 high dose. If I treat the dose level as a continuous variable, how can I use ANCOVA to do gene selection? (I mean to select those genes that response to different level of doses differently?). Thank you very much!
> 
>   James
> 
>  __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From miltinho_astronauta at yahoo.com.br  Sun Jun 11 03:26:35 2006
From: miltinho_astronauta at yahoo.com.br (Milton Cezar)
Date: Sat, 10 Jun 2006 22:26:35 -0300 (ART)
Subject: [R] using several columns from a Table in a procedure
Message-ID: <20060611012635.67892.qmail@web53407.mail.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060610/1a6cf357/attachment.pl 

From gatony at gmail.com  Sun Jun 11 04:44:12 2006
From: gatony at gmail.com (Tony Chu)
Date: Sat, 10 Jun 2006 22:44:12 -0400
Subject: [R] assign sequence numbers by group of value
Message-ID: <f8c6ef510606101944t2ace6cbdh62a5e9827a578f6e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060610/d22cb48a/attachment.pl 

From jholtman at gmail.com  Sun Jun 11 04:52:31 2006
From: jholtman at gmail.com (jim holtman)
Date: Sat, 10 Jun 2006 22:52:31 -0400
Subject: [R] assign sequence numbers by group of value
In-Reply-To: <f8c6ef510606101944t2ace6cbdh62a5e9827a578f6e@mail.gmail.com>
References: <f8c6ef510606101944t2ace6cbdh62a5e9827a578f6e@mail.gmail.com>
Message-ID: <644e1f320606101952x24832858j4159069a02a0bce7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060610/fe1ab9cb/attachment.pl 

From xthua111 at zju.edu.cn  Sun Jun 11 06:52:04 2006
From: xthua111 at zju.edu.cn (Xiaoting Hua)
Date: Sun, 11 Jun 2006 12:52:04 +0800
Subject: [R] R usage for log analysis
In-Reply-To: <82c890d00606101131u11818d90mf80ec489f005686c@mail.gmail.com>
References: <82c890d00606101131u11818d90mf80ec489f005686c@mail.gmail.com>
Message-ID: <53ff794a0606102152h2918faf9wbe310b7c4b812f83@mail.gmail.com>

I heared some project use Perl to analysis log file. And I don't think
it's suit to analysis log file for R.

On 6/11/06, Gabriel Diaz <gabidiaz at gmail.com> wrote:
> Hello,
>
> Is there any software project that uses R to do log file analisys?
>
> thanks
>
> gabi
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
????????
??????
?????????? Kenneth Hua
??????????????
??????????
310029


From gabidiaz at gmail.com  Sun Jun 11 11:33:19 2006
From: gabidiaz at gmail.com (Gabriel Diaz)
Date: Sun, 11 Jun 2006 11:33:19 +0200
Subject: [R] R usage for log analysis
In-Reply-To: <53ff794a0606102152h2918faf9wbe310b7c4b812f83@mail.gmail.com>
References: <82c890d00606101131u11818d90mf80ec489f005686c@mail.gmail.com>
	<53ff794a0606102152h2918faf9wbe310b7c4b812f83@mail.gmail.com>
Message-ID: <82c890d00606110233x7c93655cm225ba9fba359ac41@mail.gmail.com>

hello

why not? i know nothing about R but that is a statistical tool, so if
I want statistics about the events in a log file, is still a bad
choice to use R?

thanks

gabi

On 6/11/06, Xiaoting Hua <xthua111 en zju.edu.cn> wrote:
> I heared some project use Perl to analysis log file. And I don't think
> it's suit to analysis log file for R.
>
> On 6/11/06, Gabriel Diaz <gabidiaz en gmail.com> wrote:
> > Hello,
> >
> > Is there any software project that uses R to do log file analisys?
> >
> > thanks
> >
> > gabi
> >
> > ______________________________________________
> > R-help en stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
>
> --
> ????
> ???
> ??? Kenneth Hua
> ???????
> ?????
> 310029
>


From carsten.steinhoff at gmx.de  Sun Jun 11 12:24:58 2006
From: carsten.steinhoff at gmx.de (Carsten Steinhoff)
Date: Sun, 11 Jun 2006 12:24:58 +0200
Subject: [R] MS-Access and R
Message-ID: <200606111024.k5BAOrJi021184@hypatia.math.ethz.ch>

Ein eingebundener Text mit undefiniertem Zeichensatz wurde abgetrennt.
Name: nicht verf?gbar
URL: https://stat.ethz.ch/pipermail/r-help/attachments/20060611/58164419/attachment.pl 

From vincent at 7d4.com  Sun Jun 11 12:46:01 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Sun, 11 Jun 2006 12:46:01 +0200
Subject: [R] R usage for log analysis
In-Reply-To: <82c890d00606110233x7c93655cm225ba9fba359ac41@mail.gmail.com>
References: <82c890d00606101131u11818d90mf80ec489f005686c@mail.gmail.com>	<53ff794a0606102152h2918faf9wbe310b7c4b812f83@mail.gmail.com>
	<82c890d00606110233x7c93655cm225ba9fba359ac41@mail.gmail.com>
Message-ID: <448BF469.9080904@7d4.com>

Gabriel Diaz a ?crit :

> why not? i know nothing about R but that is a statistical tool, so if
> I want statistics about the events in a log file, is still a bad
> choice to use R?

Before knowing R, I wrote some gawk soft to realize log file analysis
(internet web pages visit statistics).
If I should redo it today, I think I'll wrote it in R, without problems.


From gabidiaz at gmail.com  Sun Jun 11 12:48:33 2006
From: gabidiaz at gmail.com (Gabriel Diaz)
Date: Sun, 11 Jun 2006 12:48:33 +0200
Subject: [R] R usage for log analysis
In-Reply-To: <448BF469.9080904@7d4.com>
References: <82c890d00606101131u11818d90mf80ec489f005686c@mail.gmail.com>
	<53ff794a0606102152h2918faf9wbe310b7c4b812f83@mail.gmail.com>
	<82c890d00606110233x7c93655cm225ba9fba359ac41@mail.gmail.com>
	<448BF469.9080904@7d4.com>
Message-ID: <82c890d00606110348i7583cd56kdcdb028a9e95e4a9@mail.gmail.com>

hello

and what is the correct path to do it?

I mean, put logs files in a mysql or somehting like that, and then
make R use that data, using the data from the files directly?
pre-parse the log files to accomodate them to R?

I need faqs, manuals, books, whatever to learn about this, can anyone
give some advice?

thanks.

gabi

On 6/11/06, vincent en 7d4.com <vincent en 7d4.com> wrote:
> Gabriel Diaz a ?crit :
>
> > why not? i know nothing about R but that is a statistical tool, so if
> > I want statistics about the events in a log file, is still a bad
> > choice to use R?
>
> Before knowing R, I wrote some gawk soft to realize log file analysis
> (internet web pages visit statistics).
> If I should redo it today, I think I'll wrote it in R, without problems.
>
> ______________________________________________
> R-help en stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From MSchwartz at mn.rr.com  Sun Jun 11 13:15:32 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sun, 11 Jun 2006 06:15:32 -0500
Subject: [R] assign sequence numbers by group of value
In-Reply-To: <644e1f320606101952x24832858j4159069a02a0bce7@mail.gmail.com>
References: <f8c6ef510606101944t2ace6cbdh62a5e9827a578f6e@mail.gmail.com>
	<644e1f320606101952x24832858j4159069a02a0bce7@mail.gmail.com>
Message-ID: <1150024532.6811.91.camel@localhost.localdomain>

I might offer an additional possibility, in the case where your actual
sequence might contain subsequent runs of the same values. This uses the
rle() function to detect runs of a value in a vector.

For example:

> id2 <- c(id, id)
> id2
 [1] "a" "a" "a" "b" "c" "c" "a" "a" "a" "b" "c" "c"

> unlist(sapply(rle(id2)$lengths, seq))
 [1] 1 2 3 1 1 2 1 2 3 1 1 2


It also works for the case in your initial example:

> unlist(sapply(rle(id)$lengths, seq))
[1] 1 2 3 1 1 2

See ?rle for more information.

HTH,

Marc Schwartz


On Sat, 2006-06-10 at 22:52 -0400, jim holtman wrote:
> > id
> [1] "a" "a" "a" "b" "c" "c"
> > x <- split(id, id)  # separate by unique values
> > x <- lapply(x, seq)  # generate the sequence numbers
> > x
> $a
> [1] 1 2 3
> 
> $b
> [1] 1
> 
> $c
> [1] 1 2
> 
> > x <- unsplit(x, id)  # make back into a vector
> > x
> [1] 1 2 3 1 1 2
> >
> >
> 
> 
> On 6/10/06, Tony Chu <gatony at gmail.com> wrote:
> >
> > Dear R users:
> >
> > I would like to assign sequence numbers based on group of value but
> > cannot find an easy way.
> > Here is a simple instance:
> >
> > > id = c('a','a','a','b','c','c')
> > > id
> > [1] "a" "a" "a" "b" "c" "c"
> >
> > I hope to create a corresponding vector as --
> >
> > [1]  1   2   3   1   1   2
> >
> > That is, in group "a"  the number begins with 1 and then continues to plus
> > 1
> > until
> > it happens to the next group.  For "b" It goes back to 1.  Because there
> > is
> > only one
> > b, it begins to be 1 again for the first c following, etc.  Could someone
> > advise me
> > how to do this in programming rather than manually?  Thanks a lot.
> >
> > Tony C.
> >


From vincent at 7d4.com  Sun Jun 11 13:35:06 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Sun, 11 Jun 2006 13:35:06 +0200
Subject: [R] R usage for log analysis
In-Reply-To: <82c890d00606110348i7583cd56kdcdb028a9e95e4a9@mail.gmail.com>
References: <82c890d00606101131u11818d90mf80ec489f005686c@mail.gmail.com>	
	<53ff794a0606102152h2918faf9wbe310b7c4b812f83@mail.gmail.com>	
	<82c890d00606110233x7c93655cm225ba9fba359ac41@mail.gmail.com>	
	<448BF469.9080904@7d4.com>
	<82c890d00606110348i7583cd56kdcdb028a9e95e4a9@mail.gmail.com>
Message-ID: <448BFFEA.4070607@7d4.com>

Gabriel Diaz a ?crit :

> hello
> 
> and what is the correct path to do it?
> 
> I mean, put logs files in a mysql or somehting like that, and then
> make R use that data, using the data from the files directly?
> pre-parse the log files to accomodate them to R?
> 
> I need faqs, manuals, books, whatever to learn about this, can anyone
> give some advice?

It's not very fresh in my mind,
but my log files were small (< 1Mo)
so I just put the whole stuff in *.txt files.

A 1st pass just consists in reducing the file,
and after that, it's just string manipulation, finding tags,
counting, and do some sortings.
All things easily doable with R.

As I remember, I stop it when my website host provided us Urchin,
which is quite great for this kind of job.


From ggrothendieck at gmail.com  Sun Jun 11 14:40:33 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 11 Jun 2006 08:40:33 -0400
Subject: [R] assign sequence numbers by group of value
In-Reply-To: <f8c6ef510606101944t2ace6cbdh62a5e9827a578f6e@mail.gmail.com>
References: <f8c6ef510606101944t2ace6cbdh62a5e9827a578f6e@mail.gmail.com>
Message-ID: <971536df0606110540n190e2995p423f9d443e587001@mail.gmail.com>

This was recently discussed in:
 http://tolstoy.newcastle.edu.au/R/help/06/06/28664.html
Note that it depends on the groups elements being contiguous:

seq(id) - match(id, id) + 1

On 6/10/06, Tony Chu <gatony at gmail.com> wrote:
>  Dear R users:
>
>  I would like to assign sequence numbers based on group of value but
> cannot find an easy way.
> Here is a simple instance:
>
> > id = c('a','a','a','b','c','c')
> > id
> [1] "a" "a" "a" "b" "c" "c"
>
> I hope to create a corresponding vector as --
>
> [1]  1   2   3   1   1   2
>
> That is, in group "a"  the number begins with 1 and then continues to plus 1
> until
> it happens to the next group.  For "b" It goes back to 1.  Because there is
> only one
> b, it begins to be 1 again for the first c following, etc.  Could someone
> advise me
> how to do this in programming rather than manually?  Thanks a lot.
>
>  Tony C.
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From u08adh at hotmail.com  Sun Jun 11 16:26:42 2006
From: u08adh at hotmail.com (Andreas Hary)
Date: Sun, 11 Jun 2006 15:26:42 +0100
Subject: [R] MS-Access and R
References: <200606111024.k5BAOrJi021184@hypatia.math.ethz.ch>
Message-ID: <BAY114-DAV137A1126E262B4C31D587DF8E0@phx.gbl>

Have a look at odbcConnectAccess in the RODBC package.
Cheers,

Andreas


----- Original Message ----- 
From: "Carsten Steinhoff" <carsten.steinhoff at gmx.de>
To: <r-help at stat.math.ethz.ch>
Sent: Sunday, June 11, 2006 11:24 AM
Subject: [R] MS-Access and R


> Hello,
>
> is there a possibility to connect MS-Access and R, maybe via VBA-Code?
> I have an Access database connected to R (via odbc). Some calculations are
> done and the results are transfered back to Access where the user will 
> work
> further.
> To make the whole thing more user-friendly I'd like to do everything in
> Access.
> I know that there is a module for EXCEL in R-DCOM but my experience in
> programming is not good enough to make it ready for Access.
>
> Is there any ACCESS - example available for such a task? Maybe somebody
> already has done something similar and could send it to me.
>
> Thanks a lot!
>
> Carsten
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From spencer.graves at pdf.com  Sun Jun 11 17:20:42 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 11 Jun 2006 08:20:42 -0700
Subject: [R] Maximum likelihood estimation of Regression parameters
In-Reply-To: <53ff794a0606100211w57885bd7t4d10bfaaf82d7744@mail.gmail.com>
References: <BAY111-DAV7F1A9A3A9C25103A68281D8890@phx.gbl>
	<53ff794a0606100211w57885bd7t4d10bfaaf82d7744@mail.gmail.com>
Message-ID: <448C34CA.3080402@pdf.com>

	  Have you looked at "lm"?  I think that's what you want.

	  Also, have you reviewed the "Documentation" list at 
"www.r-project.org"?  Neter, Kutner, nachtsheim & Wasserman has had a 
long and successful run having first appeared in 1974 and having gone 
through several editions since then.  However, it apparently has not 
kept up with the R revolution, and I would not recommend it today.

	  Beyond this, if you don't have Venables and Ripley (2002) Modern 
Applied Statistics with S (Springer), I recommend you look at it first. 
  It has numerous index entries on "regression" and is all around my 
favorite book on R generally.

	  Also, have you checked the "Documentation" briefly outlined at 
'www.r-project.org', including the "Contributed Documentation" on CRAN, 
and "Practical Regression and Anova using R" by Faraway in particular?

	  hope this helps.
	  Spencer Graves

Xiaoting Hua wrote:
> mle(stats4)                Maximum Likelihood Estimation
> 
> is it list above what you want?
> 
> On 6/10/06, Bart Joosen <bartjoosen at hotmail.com> wrote:
>> Hi,
>>
>> I want to use Maximum likelihood to estimate the parameters from my 
>> regression line.
>> I have purchased the book "Applied linear statistical models" from 
>> Neter, Kutner, nachtsheim & Wasserman, and in one of the first 
>> chapters, they use maximum likelihood to estimate the parameters.
>> Now I want to tried it for my self, but couldn't find the right function.
>> In the book, they give a fixed variance to work with, but I couldn't 
>> find a function where I can estimate the predictor and where I have to 
>> give the variance.
>> Or isn't this neccesairy?
>> Also they calculate likelihood values for the different values, used 
>> to estimate the parameters (like a normal probability curve), is it 
>> possible to do this with R?
>>
>> Kind regards
>>
>> Bart
>>        [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From lord.tyranus.96 at gmail.com  Sun Jun 11 17:24:47 2006
From: lord.tyranus.96 at gmail.com (Lord Tyranus)
Date: Sun, 11 Jun 2006 09:24:47 -0600
Subject: [R] [OT] Information about genetic algorithms
Message-ID: <4f31b0bd0606110824s23388e94w761023988efe5fe4@mail.gmail.com>

Hi wizards, I need information about genetic algorithms for
classification , somebody knows some code in R about it.

Thanks in advance for any help.


From markleeds at verizon.net  Sun Jun 11 21:25:53 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Sun, 11 Jun 2006 14:25:53 -0500 (CDT)
Subject: [R] regular expression question
Message-ID: <16849099.776251150053953542.JavaMail.root@vms170.mailsrvcs.net>

I am very much a novice with regular expressions and I have spent
a decent amount of time trying to do the following and I can't
get it.

i have variables that are of type character but
they have number characters at the end. for example :

"AAL123"
"XELB245"
"A247"

I want a command that gives me just gives me the letter characters
for each one. 
the letter characters always start first and then the number characters come second and it never flips back to letter characters
once the number characters start. i am using R-2.20 on
windows Xp. Thanks. substring doesn't work because the
length of the letter characters can vary.


always occur first with the numbers starting


From edd at debian.org  Sun Jun 11 21:47:47 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 11 Jun 2006 14:47:47 -0500
Subject: [R] regular expression question
In-Reply-To: <16849099.776251150053953542.JavaMail.root@vms170.mailsrvcs.net>
References: <16849099.776251150053953542.JavaMail.root@vms170.mailsrvcs.net>
Message-ID: <17548.29539.453099.878523@basebud.nulle.part>


On 11 June 2006 at 14:25, markleeds at verizon.net wrote:
| i have variables that are of type character but
| they have number characters at the end. for example :
| 
| "AAL123"
| "XELB245"
| "A247"
| 
| I want a command that gives me just gives me the letter characters
| for each one. 
| the letter characters always start first and then the number characters come second and it never flips back to letter characters
| once the number characters start. i am using R-2.20 on
| windows Xp. Thanks. substring doesn't work because the
| length of the letter characters can vary.


> gsub("(\\d*)$","",c("AAL123", "XELB245", "A247", "FOO123BAR"), perl=TRUE)
[1] "AAL"       "XELB"      "A"         "FOO123BAR"
> 

gsub finds what is described by the first regexp [ here (\\d\*)$ --- any
sequence of digits before the end-of-line ] and applies the second regexp 
[ here an empty string as we simply delete ] to the third argument.

Note 
 - how the $ symbol $ \b prevents it from eating the non-final digits
   in the counter example FOO123BAR
 - how the \d for digits needs escaped backslashes \\d
 - how the * char denotes '1 or more of the preceding thingie'

Hth, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From markleeds at verizon.net  Sun Jun 11 23:24:33 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Sun, 11 Jun 2006 16:24:33 -0500 (CDT)
Subject: [R] seemingly simple read.table question
Message-ID: <33207856.786441150061073746.JavaMail.root@vms170.mailsrvcs.net>

I have a file that I thought would be fairly simple to read in using read.table but I am having problems ( as usual ).

each line of the file is of the form ( just 20 lines or so )

financials XXX, YYY, ZZZ
automobiles RTR, ABC, TGH

so the first field in the line is the industry and the other fields
( seperated by commas ) in the line are stock identifiers of stocks
in that industry. note that there is no comma between the industry
and the first stock identifier in the group which i guess might
complicate things ?

my goal is to make the row names the industries and the stock
identifiers the column data but i don't
have a header so , i am unclear ( reading the help on
read.table ) how to tell R that the first field in each line
should be used as the row name ? Thanks for any help
or for telling me tht this is not possible. This will be my last bother of the day to the help group.

I am using R-2.20 on windows xp and i've tried various settings
of read.table without success. thanks. 
  
                                                mark


From jdnewmil at dcn.davis.ca.us  Sun Jun 11 23:35:53 2006
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sun, 11 Jun 2006 14:35:53 -0700
Subject: [R] regular expression question
In-Reply-To: <17548.29539.453099.878523@basebud.nulle.part>
References: <16849099.776251150053953542.JavaMail.root@vms170.mailsrvcs.net>
	<17548.29539.453099.878523@basebud.nulle.part>
Message-ID: <448C8CB9.2000209@dcn.davis.ca.us>

Dirk Eddelbuettel wrote:
> On 11 June 2006 at 14:25, markleeds at verizon.net wrote:
> | i have variables that are of type character but
> | they have number characters at the end. for example :
> | 
> | "AAL123"
> | "XELB245"
> | "A247"
> | 
> | I want a command that gives me just gives me the letter characters
> | for each one. 
> | the letter characters always start first and then the number characters come second and it never flips back to letter characters
> | once the number characters start. i am using R-2.20 on
> | windows Xp. Thanks. substring doesn't work because the
> | length of the letter characters can vary.
> 
> 
> 
>>gsub("(\\d*)$","",c("AAL123", "XELB245", "A247", "FOO123BAR"), perl=TRUE)
> 
> [1] "AAL"       "XELB"      "A"         "FOO123BAR"
> 
> 
> gsub finds what is described by the first regexp [ here (\\d\*)$ --- any
> sequence of digits before the end-of-line ] and applies the second regexp 
> [ here an empty string as we simply delete ] to the third argument.
> 
> Note 
>  - how the $ symbol $ \b prevents it from eating the non-final digits
>    in the counter example FOO123BAR
>  - how the \d for digits needs escaped backslashes \\d
>  - how the * char denotes '1 or more of the preceding thingie'

* normally means "zero or more of the preceding thingie"
+ is the "1 or more or the preceding thingie"

The difference would be apparent if the string being inserted was not
empty.

 > gsub("(\\d*)$","new",c("AAL123", "XELB245", "A247", "FOO123BAR"), perl=TRUE)
[1] "AALnew"       "XELBnew"      "Anew"         "FOO123BARnew"

 > gsub("(\\d+)$","new",c("AAL123", "XELB245", "A247", "FOO123BAR"), perl=TRUE)
[1] "AALnew"    "XELBnew"   "Anew"      "FOO123BAR"


> Hth, Dirk
> 


-- 
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From edd at debian.org  Sun Jun 11 23:57:25 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 11 Jun 2006 16:57:25 -0500
Subject: [R] seemingly simple read.table question
In-Reply-To: <33207856.786441150061073746.JavaMail.root@vms170.mailsrvcs.net>
References: <33207856.786441150061073746.JavaMail.root@vms170.mailsrvcs.net>
Message-ID: <17548.37317.506198.537239@basebud.nulle.part>


On 11 June 2006 at 16:24, markleeds at verizon.net wrote:
| I have a file that I thought would be fairly simple to read in using read.table but I am having problems ( as usual ).
| 
| each line of the file is of the form ( just 20 lines or so )
| 
| financials XXX, YYY, ZZZ
| automobiles RTR, ABC, TGH
| 
| so the first field in the line is the industry and the other fields
| ( seperated by commas ) in the line are stock identifiers of stocks
| in that industry. note that there is no comma between the industry
| and the first stock identifier in the group which i guess might
| complicate things ?

Yup, because that makes it such that the comma is no longer a unique
seperator between _all_ column.  But if the file really looks the way you
typed it here, you should be fine by postprocessing the data afterwards and
just removing the comma. See below for a hack-ish solution.

| my goal is to make the row names the industries and the stock
| identifiers the column data but i don't
| have a header so , i am unclear ( reading the help on
| read.table ) how to tell R that the first field in each line
| should be used as the row name ? Thanks for any help
| or for telling me tht this is not possible. This will be my last bother of the day to the help group.

This is a little clumsy, using an apply to sweep a regexp transformation 
[ hey, you get to use what we taught you earlier :) ] through.  

> rawData <- read.table("/tmp/leeds.txt", row.names=1)
> data <- apply(rawData, 2, function(X)gsub(",$", "", X))
> rownames(data) <- rownames(rawData)
> data
            V2    V3    V4   
financials  "XXX" "YYY" "ZZZ"
automobiles "RTR" "ABC" "TGH"
> 

I'm sure someone named Gabor will soon post something doing the same in half
the lines ...

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From edd at debian.org  Sun Jun 11 23:58:36 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 11 Jun 2006 16:58:36 -0500
Subject: [R] regular expression question
In-Reply-To: <448C8CB9.2000209@dcn.davis.ca.us>
References: <16849099.776251150053953542.JavaMail.root@vms170.mailsrvcs.net>
	<17548.29539.453099.878523@basebud.nulle.part>
	<448C8CB9.2000209@dcn.davis.ca.us>
Message-ID: <17548.37388.647445.445265@basebud.nulle.part>


On 11 June 2006 at 14:35, Jeff Newmiller wrote:
| >>gsub("(\\d*)$","",c("AAL123", "XELB245", "A247", "FOO123BAR"), perl=TRUE)
| > 
| > [1] "AAL"       "XELB"      "A"         "FOO123BAR"
| > 
| > 
| > gsub finds what is described by the first regexp [ here (\\d\*)$ --- any
| > sequence of digits before the end-of-line ] and applies the second regexp 
| > [ here an empty string as we simply delete ] to the third argument.
| > 
| > Note 
| >  - how the $ symbol $ \b prevents it from eating the non-final digits
| >    in the counter example FOO123BAR
| >  - how the \d for digits needs escaped backslashes \\d
| >  - how the * char denotes '1 or more of the preceding thingie'
| 
| * normally means "zero or more of the preceding thingie"
| + is the "1 or more or the preceding thingie"
| 
| The difference would be apparent if the string being inserted was not
| empty.
| 
|  > gsub("(\\d*)$","new",c("AAL123", "XELB245", "A247", "FOO123BAR"), perl=TRUE)
| [1] "AALnew"       "XELBnew"      "Anew"         "FOO123BARnew"
| 
|  > gsub("(\\d+)$","new",c("AAL123", "XELB245", "A247", "FOO123BAR"), perl=TRUE)
| [1] "AALnew"    "XELBnew"   "Anew"      "FOO123BAR"

Thanks for catching, and correcting, that.

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From marsh at uri.edu  Mon Jun 12 04:14:02 2006
From: marsh at uri.edu (Marshall Feldman)
Date: Sun, 11 Jun 2006 22:14:02 -0400
Subject: [R] Chapters
Message-ID: <000001c68dc5$dfb4f550$0300a8c0@Zippy>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060611/c6589c22/attachment.pl 

From ggrothendieck at gmail.com  Mon Jun 12 04:56:00 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 11 Jun 2006 22:56:00 -0400
Subject: [R] seemingly simple read.table question
In-Reply-To: <33207856.786441150061073746.JavaMail.root@vms170.mailsrvcs.net>
References: <33207856.786441150061073746.JavaMail.root@vms170.mailsrvcs.net>
Message-ID: <971536df0606111956o2abe7fb4pafd44e4265663ae9@mail.gmail.com>

Try this (which reads in the lines, replaces commas with spaces
and then rereads it as a data frame using column 1 as the rows:

read.table(textConnection(chartr(",", " ", readLines("myfile.dat"))),
row = 1, as.is = TRUE)

I assume you want character data, not factors, but if you want factors
remove the as.is = TRUE.

On 6/11/06, markleeds at verizon.net <markleeds at verizon.net> wrote:
> I have a file that I thought would be fairly simple to read in using read.table but I am having problems ( as usual ).
>
> each line of the file is of the form ( just 20 lines or so )
>
> financials XXX, YYY, ZZZ
> automobiles RTR, ABC, TGH
>
> so the first field in the line is the industry and the other fields
> ( seperated by commas ) in the line are stock identifiers of stocks
> in that industry. note that there is no comma between the industry
> and the first stock identifier in the group which i guess might
> complicate things ?
>
> my goal is to make the row names the industries and the stock
> identifiers the column data but i don't
> have a header so , i am unclear ( reading the help on
> read.table ) how to tell R that the first field in each line
> should be used as the row name ? Thanks for any help
> or for telling me tht this is not possible. This will be my last bother of the day to the help group.
>
> I am using R-2.20 on windows xp and i've tried various settings
> of read.table without success. thanks.
>
>                                                mark
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Mon Jun 12 05:01:23 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 11 Jun 2006 23:01:23 -0400
Subject: [R] regular expression question
In-Reply-To: <16849099.776251150053953542.JavaMail.root@vms170.mailsrvcs.net>
References: <16849099.776251150053953542.JavaMail.root@vms170.mailsrvcs.net>
Message-ID: <971536df0606112001t5e33cd7ard12aa87c5ddb9714@mail.gmail.com>

This will delete numbers at the end:

sub("[0-9]+$", "", c("AAL123", "XELB245", "A247"))

If you want remove all numbers its just:

gsub("[0-9]", "", c("AAL123", "XELB245", "A247"))

On 6/11/06, markleeds at verizon.net <markleeds at verizon.net> wrote:
> I am very much a novice with regular expressions and I have spent
> a decent amount of time trying to do the following and I can't
> get it.
>
> i have variables that are of type character but
> they have number characters at the end. for example :
>
> "AAL123"
> "XELB245"
> "A247"
>
> I want a command that gives me just gives me the letter characters
> for each one.
> the letter characters always start first and then the number characters come second and it never flips back to letter characters
> once the number characters start. i am using R-2.20 on
> windows Xp. Thanks. substring doesn't work because the
> length of the letter characters can vary.
>
>
> always occur first with the numbers starting
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Mon Jun 12 05:03:43 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 11 Jun 2006 23:03:43 -0400
Subject: [R] Chapters
In-Reply-To: <000001c68dc5$dfb4f550$0300a8c0@Zippy>
References: <000001c68dc5$dfb4f550$0300a8c0@Zippy>
Message-ID: <971536df0606112003l5b7c7f9wb80885b322de909f@mail.gmail.com>

R does not have chapters.  I believe that is related to the difference in
which S stores objects in files whereas R stores them in memory.

On 6/11/06, Marshall Feldman <marsh at uri.edu> wrote:
> I'm surprised this isn't a FAQ, but I searched all over and could not find a
> reference to it.
>
>
>
> Chambers (1998) makes repeated references to "Chapters" in S (e.g., p. 6),
> but I can find no reference to "Chapters" in R. Since Chapters were not used
> in earlier versions of S, I'm wondering if R uses them or not. If it does,
> how does one get them to work? I've been unable to do so.
>
>
>
>            Marsh Feldman
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From spluque at gmail.com  Mon Jun 12 05:54:48 2006
From: spluque at gmail.com (Sebastian Luque)
Date: Sun, 11 Jun 2006 22:54:48 -0500
Subject: [R] cumulative time durations of specified periods (chron)
Message-ID: <87ejxv9gfr.fsf@arctocephalus.homelinux.org>

Hi,

Say we have two chron vectors representing start and end date/times of an
event, respectively:


R> (xfrom <- chron(seq(1.25, 11, 3.25)))
[1] (01/02/70 06:00:00) (01/05/70 12:00:00) (01/08/70 18:00:00)
[4] (01/12/70 00:00:00)
R> (xto <- chron(as.numeric(xfrom) + seq(1.5, 2.25, 0.25)))
[1] (01/03/70 18:00:00) (01/07/70 06:00:00) (01/10/70 18:00:00)
[4] (01/14/70 06:00:00)


and we would like to know how much time is included in a number of
intervals within each event.  We can define the intervals with two chron
vectors:


R> (xt0 <- times(c(0.50, 0)))
[1] 12:00:00 00:00:00
R> (xt1 <- times(c(1 - (1 / 86400), 0.25)))
[1] 23:59:59 06:00:00


So for the first event, 01/02/70 06:00:00 to 01/03/70 18:00:00, the
interest is to find how much time corresponds to periods 12:00:00 -
23:59:59 and 00:00:00 - 06:00:00.

I began writing a function to accomplish this task, but am at an impasse.
The archives may have something on this, but I haven't found a good search
query for it, so I'd appreciate some pointers.  Thanks in advance.


Cheers,

-- 
Seb


From asr at ufl.edu  Mon Jun 12 06:44:51 2006
From: asr at ufl.edu (Allen S. Rout)
Date: 12 Jun 2006 00:44:51 -0400
Subject: [R] R usage for log analysis
References: <82c890d00606101131u11818d90mf80ec489f005686c@mail.gmail.com>
	<53ff794a0606102152h2918faf9wbe310b7c4b812f83@mail.gmail.com>
	<82c890d00606110233x7c93655cm225ba9fba359ac41@mail.gmail.com>
	<448BF469.9080904@7d4.com>
	<82c890d00606110348i7583cd56kdcdb028a9e95e4a9@mail.gmail.com>
Message-ID: <86hd2ryocc.fsf@ufl.edu>

"Gabriel Diaz" <gabidiaz at gmail.com> writes:

> and what is the correct path to do it?
> 
> I mean, put logs files in a mysql or somehting like that, and then
> make R use that data, using the data from the files directly?

I haven't stuck anything in a DB yet.  I'm not sure how much of the DB
clue is used under the covers. 

> pre-parse the log files to accomodate them to R?
 
Probably not; a little familiarity with the reading functions will
obviate most needs to pre-parse.


> I need faqs, manuals, books, whatever to learn about this, can anyone
> give some advice?

[...]


Don't expect a warm welcome.  This community is like all open-source
communities, sharply focused on its' own concerns and expertise.  And,
in an unusual experience for computer types, our core competencies
hold little or no sway here; they don't even give us much of a leg up.
Just wait 'till you want to do something nutso like produce a business
graphic. :)

I'm working on understanding enough of R packaging and documentation
to begin a 'task view' focused on systems administration, for humble
submission. That might end up being mostly "log analysis"; the term
can describe much of what we do, if it's stretched a bit.  I'm hoping
the task view will attract the teeming masses of sysadmins trapped in
the mire of Gnuplot and friends.


For starters, become familliar with read.table(); with a few
variations it will take care of all the 

while (<>) { @blah = split(/,/); etc. etc. etc. } 

you've been accustomed to doing.  

Name columns;  this makes it easier to think about your data.  

names(my_data)<-c("column","names","can","be","assigned","to")

Start thinking of your data in generic sets, as opposed to specific
rows.  Situations which required iteration over specific rows in
PERL-land fall neatly to precise assignment in R.  For example, if
you've got records with dates and times and you want to work with time
structures:

in PERL you'd 

foreach (...) 
{$foo->{pdate} = parsedate($foo->{date}." ".$foo->{time})}

or some such.  In R-land, the iteration is implicit.  Here's a snippet
from something I'm using 

a$pdate<-as.POSIXct(paste(format(a$dte,"%Y/%m/%d"),a$time)) 

You're really acting on logical columns all at once here.  This is
fantastically more efficient in terms of your thought processes.  



- Allen S. Rout


From hodgess at gator.dt.uh.edu  Mon Jun 12 07:05:54 2006
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Mon, 12 Jun 2006 00:05:54 -0500
Subject: [R]  lowesp program in simpleLoess
Message-ID: <200606120505.k5C55sTj014304@gator.dt.uh.edu>

Dear R People:

I use the binary version of R for Windows.

However, I would like to look at the Fortran code
for the simpleLoess function, please.

How could I see the underlying Fortran code, please?

Thanks in advance!

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu


From jeffmiller at adsam.com  Mon Jun 12 07:19:09 2006
From: jeffmiller at adsam.com (Jeff Miller)
Date: Mon, 12 Jun 2006 01:19:09 -0400
Subject: [R] variance specification using glm and quasi
Message-ID: <000001c68ddf$bca17880$6501a8c0@AdSAMJeff>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060612/fce83cdc/attachment.pl 

From ripley at stats.ox.ac.uk  Mon Jun 12 08:07:08 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 12 Jun 2006 07:07:08 +0100 (BST)
Subject: [R] Chapters
In-Reply-To: <000001c68dc5$dfb4f550$0300a8c0@Zippy>
References: <000001c68dc5$dfb4f550$0300a8c0@Zippy>
Message-ID: <Pine.LNX.4.64.0606120701390.16796@gannet.stats.ox.ac.uk>

On Sun, 11 Jun 2006, Marshall Feldman wrote:

> I'm surprised this isn't a FAQ, but I searched all over and could not find a
> reference to it.

I have never seen in mentioned before.  R is not an implementation of S4 
as described in Chambers (1998), so I do not know where you got the idea 
that was relevant.  (The methods package in R is an implementation of 
something close to the class system described in that book, but R can be 
used without the methods package.)

> Chambers (1998) makes repeated references to "Chapters" in S (e.g., p. 6),
> but I can find no reference to "Chapters" in R. Since Chapters were not used
> in earlier versions of S, I'm wondering if R uses them or not. If it does,
> how does one get them to work? I've been unable to do so.

The closest analogues are

1) packages
2) using different working directories.

Since R does not store objects on file and chapters are directories with 
collections of stored objects (and other things, including DLLs), the 
analogues cannot be very close.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Mon Jun 12 08:12:38 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 12 Jun 2006 07:12:38 +0100 (BST)
Subject: [R] lowesp program in simpleLoess
In-Reply-To: <200606120505.k5C55sTj014304@gator.dt.uh.edu>
References: <200606120505.k5C55sTj014304@gator.dt.uh.edu>
Message-ID: <Pine.LNX.4.64.0606120708110.16796@gannet.stats.ox.ac.uk>

On Mon, 12 Jun 2006, Erin Hodgess wrote:

> Dear R People:
>
> I use the binary version of R for Windows.
>
> However, I would like to look at the Fortran code
> for the simpleLoess function, please.
>
> How could I see the underlying Fortran code, please?

In the R sources, so download the R source tarball, e.g.
http://cran.r-project.org/src/base/R-2/R-2.3.1.tar.gz

On Windows you will probably need the source package tools in
http://www.murdoch-sutherland.com/Rtools/tools.zip
to unpack it (it contains symbolic links so other tools fail).

BTW, it is both C and Fortran: stats:::simpleLoess has both .C and 
.Fortran calls.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From alain.yamakana at rogers.com  Sun Jun 11 21:59:20 2006
From: alain.yamakana at rogers.com (Alain Yamakana)
Date: Sun, 11 Jun 2006 15:59:20 -0400 (EDT)
Subject: [R] Error in "[.default"(mf[, 1], ,
	1) : incorrect number of dimensions
Message-ID: <20060611195920.72975.qmail@web88108.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060611/9fc38f2f/attachment.pl 

From petr.pikal at precheza.cz  Mon Jun 12 09:25:29 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 12 Jun 2006 09:25:29 +0200
Subject: [R] using several columns from a Table in a procedure
In-Reply-To: <20060611012635.67892.qmail@web53407.mail.yahoo.com>
Message-ID: <448D3309.2339.29E99D@localhost>

Hi

not sure what you want but if you just would  like to filter numeric 
columns

file[,sapply(file, is.numeric)]

HTH
Petr


On 10 Jun 2006 at 22:26, Milton Cezar wrote:

Date sent:      	Sat, 10 Jun 2006 22:26:35 -0300 (ART)
From:           	Milton Cezar <miltinho_astronauta at yahoo.com.br>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] using several columns from a Table in a procedure

> Dear R-friends
> 
>   I have a table with more than 50 columns (variables). Many of them
>   are numeric and others are of type char. I would like repeat a group
>   of command using only a set of the numeric variables, excluding
>   others (for example V8, V12 etc) and not using the char ones. As a
>   sample I want:
> 
>   ----
>       x11() 
>       plot (X, v2)
>       model_v2<-glm (v2~X)
>       lines (X, predict(model_v2))
> 
>       x11()
>       plot (X, v3)
>       model_v3<-glm (v3~X)
>       lines (X, predict(model_v3))
> 
>       x11()
>       plot (X, v4)
>       model_v4<-glm (v4~X)
>       lines (X, predict(model_v4))
>       ....
>   -----
> 
>   Look that I even use de X variable agains the other ones. How can I
>   do that? Another question is how can I change the name of a column
>   in a table?
> 
>   Thanks A Lot
> 
>   Miltinho
> 
> 
> 
>  __________________________________________________
> 
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From ripley at stats.ox.ac.uk  Mon Jun 12 09:52:58 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 12 Jun 2006 08:52:58 +0100 (BST)
Subject: [R] Regex engine types
In-Reply-To: <Pine.LNX.4.64.0606100738470.22963@gannet.stats.ox.ac.uk>
References: <20060610024641.GG4970@ihug.co.nz>
	<Pine.LNX.4.64.0606100738470.22963@gannet.stats.ox.ac.uk>
Message-ID: <Pine.LNX.4.64.0606120829290.18091@gannet.stats.ox.ac.uk>

On Sat, 10 Jun 2006, Prof Brian Ripley wrote:

> ?regex does describe this:
>
>     A range of characters may be specified by giving the first and last
>     characters, separated by a hyphen.  (Character ranges are
>     interpreted in the collation order of the current locale.)
>
> You did not tell us your locale, but based on questions from you in the past 
> I would guess en_NZ.utf8.  In that locale the collation order is wWxXyYzZ, so 
> your surprise is explained.  (It seems the PCRE code is not using the same 
> ordering in that locale.)

Some digging shows that Perl does not say explicitly what order it uses 
(at least in the man pages on my system), but that PCRE uses (see man 
pcrepattern)

- numerical order of the bytes in a single-byte locale
- numerical order of Unicode points in a UTF-8 locale.

whereas the basic/extended code uses the order set by the locale category 
LC_COLLATE and interpreted by the C function wcscoll (and byte order 
if that is not available).


Gabor Grothendieck <ggrothendieck at gmail.com> worte:

> I get the same thing on "Version 2.3.1 Patched (2006-06-04 r38279)"
> but on "R version 2.2.1, 2005-12-20" it gives character(0), as
> expected, so there is some change between versions of R.  I am
> on Windows XP.

And a helpful person would have studied the CHANGES file before 
commenting!  It says:

   Internationalization
   --------------------

   There is no longer a separate 'East Asian' version of R.dll.

In R 2.2.1 the fully internationalized version behaved as 2.3.1 did, but 
the 8-bit-only version for Windows always used byte-order collation. The 
difference is most likely that GG was using the 8-bit-only version, a 
Windows-specific issue.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From petr.pikal at precheza.cz  Mon Jun 12 09:57:00 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 12 Jun 2006 09:57:00 +0200
Subject: [R] How can i add a color bar with base package
In-Reply-To: <1149866374.31082.32.camel@new-york.climpact.net>
Message-ID: <448D3A6C.26153.46C484@localhost>

Hi

see ?rect
e.g.
> image(1:12, 1:12, outer(1:12, 1:12,"+"))
> for (i in 1:12) rect(10+(i/12-1/12), 8, 10+i/12, 
10,col=heat.colors(12)[i])
> 
HTH
Petr


On 9 Jun 2006 at 17:19, Yves Magliulo wrote:

From:           	Yves Magliulo <ym at climpact.com>
To:             	r-help at stat.math.ethz.ch
Organization:   	Climpact
Date sent:      	09 Jun 2006 17:19:34 +0200
Subject:        	[R]  How can i add a color bar with base package
Send reply to:  	ym at climpact.com
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>

> Hi,
> 
> I'm trying to find this for 3 hours now so i come here to find any
> help. How can I add a color bar to show the color scales to what is
> generated by image(), similar to the one in figures generated by
> filled.contour() using only the base package although i know there is
> solution with contributed package.
> 
> it's sounds very easy but i can't get it! :-(
> 
> thanks by advance.
> 
> ---------------------------------------
> Yves Magliulo
> R&D Engineer
> CLIMPACT : www.climpact.com
> tel : 01 44 27 34 31
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From c.declercq at orsnpdc.org  Mon Jun 12 10:38:23 2006
From: c.declercq at orsnpdc.org (Christophe DECLERCQ)
Date: Mon, 12 Jun 2006 10:38:23 +0200
Subject: [R] quantile() with weights
Message-ID: <892792C268EB89458F1FADC109779A67B612@ors-npdc.orsnpdc.local>

 > De : r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] De la part de Wouter
> Envoy? : vendredi 9 juin 2006 19:15
> Hi list,
> 
> I'm looking for a way to calculate quantiles (such as in 
> quantile()), but with the ability to assign a different 
> weight to each member of the sample vector.
> [...]
 
See 'wtd.quantile' in the 'Hmisc' package. 

Christophe
--
Christophe Declercq, MD
Observatoire r?gional de la sant? Nord-Pas-de-Calais
235, avenue de la recherche
BP 86 F-59373 LOOS CEDEX
Phone +33 3 20 15 49 24
Fax + 33 3 20 15 10 46
E-mail c.declercq at orsnpdc.org


From arun.kumar.saha at gmail.com  Mon Jun 12 11:14:14 2006
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Mon, 12 Jun 2006 14:44:14 +0530
Subject: [R] Combinig two data frames
Message-ID: <d4c57560606120214w63bf5524l34032cfa5174824c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060612/ef89ceaf/attachment.pl 

From MSchwartz at mn.rr.com  Mon Jun 12 11:28:56 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Mon, 12 Jun 2006 04:28:56 -0500
Subject: [R] Combinig two data frames
In-Reply-To: <d4c57560606120214w63bf5524l34032cfa5174824c@mail.gmail.com>
References: <d4c57560606120214w63bf5524l34032cfa5174824c@mail.gmail.com>
Message-ID: <1150104536.7249.29.camel@localhost.localdomain>

On Mon, 2006-06-12 at 14:44 +0530, Arun Kumar Saha wrote:
> Dear all r-users,
> 
> Suppose I have two data frame:
> 
> A
> 1
> 3
> 4
> 5
> 2
> 
> and
> 
> B
> 5
> 6
> 3
> 5
> 
> Now I want combine this two data frames without losing any value from either
> data frame. More precisely I want to see
> 
>  A    B
> 1   5
> 3   6
> 4   3
> 5   5
> 2   NA
> 
> I tried with cbind function but failed, as it only works when two data
> frames have equal number of rows. Can anyone suggest me any code that can be
> used for any data set?

You can use merge() to do this.  merge() will perform SQL-like joins on
the two data frames.

In this case, we are going to merge A and B, telling merge() to use the
rownames in the two data frames as the basis for matching rows ('by'
argument). 

We also tell it to include all rows ('all' argument) in case some (or
all) of the rownames do not match between the two.

Finally, we take the result of the operation, which by default will
include an additional first column of the rownames themselves and we
delete that column.

> merge(A, B, all = TRUE, by = "row.names")[, -1]
  A  B
1 1  5
2 3  6
3 4  3
4 5  5
5 2 NA

See ?merge for more information.  There are also many variations on its
use that you will find in the list archives.

HTH,

Marc Schwartz


From zhanwu.dai at bordeaux.inra.fr  Mon Jun 12 11:45:39 2006
From: zhanwu.dai at bordeaux.inra.fr (ZhanWu Dai)
Date: Mon, 12 Jun 2006 11:45:39 +0200
Subject: [R] solving first-order differential equation
Message-ID: <MMEALBEGJGIEEHCDNAPOGECOCAAA.zhanwu.dai@bordeaux.inra.fr>


I am an initial user of R. Could you give me some explanations or examples on how to solve the first order differential equations by the first-order Runge-Kutta method? 

Thank you very much

Kind regards


From petr.pikal at precheza.cz  Mon Jun 12 11:51:48 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 12 Jun 2006 11:51:48 +0200
Subject: [R] Combinig two data frames
In-Reply-To: <d4c57560606120214w63bf5524l34032cfa5174824c@mail.gmail.com>
Message-ID: <448D5554.31489.AFEB95@localhost>

Hi

I would recommend to use list instead

how do you know that the result shall not be
  A    B
 1   NA
 3   5
 4   6
 5   3
 2   5

> A<-1:5
> B<-4:7
> L<-list(A,B)
> L
[[1]]
[1] 1 2 3 4 5

[[2]]
[1] 4 5 6 7

If you insist on data frame you has to tell the program which cells 
are to be empty or if you have common column(s) you can use merge

> CC
  state    psu weight
1 A. P.  Urban      0
2  Mah.  Rural      0
3  W.B.  Rural      0
4  Ass.  Rural      0
5 M. P.  Urban      0
6 A. P.  Urban      0
> CC1
  state    psu aaa
1 A. P.  Urban 1.3
2 A. P.  Rural 1.2
3 M. P.  Urban 0.8
>
 merge(CC,CC1, all=T)
  state    psu weight aaa
1 A. P.  Rural     NA 1.2
2 A. P.  Urban      0 1.3
3 A. P.  Urban      0 1.3
4  Ass.  Rural      0  NA
5 M. P.  Urban      0 0.8
6  Mah.  Rural      0  NA
7  W.B.  Rural      0  NA

HTH
Petr





On 12 Jun 2006 at 14:44, Arun Kumar Saha wrote:

Date sent:      	Mon, 12 Jun 2006 14:44:14 +0530
From:           	"Arun Kumar Saha" <arun.kumar.saha at gmail.com>
To:             	"r-help at stat.math.ethz.ch" <R-help at stat.math.ethz.ch>
Subject:        	[R] Combinig two data frames

> Dear all r-users,
> 
> Suppose I have two data frame:
> 
> A
> 1
> 3
> 4
> 5
> 2
> 
> and
> 
> B
> 5
> 6
> 3
> 5
> 
> Now I want combine this two data frames without losing any value from
> either data frame. More precisely I want to see
> 
>  A    B
> 1   5
> 3   6
> 4   3
> 5   5
> 2   NA
> 
> I tried with cbind function but failed, as it only works when two data
> frames have equal number of rows. Can anyone suggest me any code that
> can be used for any data set?
> 
> Thanks and regards
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From jfontain at free.fr  Mon Jun 12 12:11:00 2006
From: jfontain at free.fr (Jean-Luc Fontaine)
Date: Mon, 12 Jun 2006 12:11:00 +0200
Subject: [R] R usage for log analysis
In-Reply-To: <86hd2ryocc.fsf@ufl.edu>
References: <82c890d00606101131u11818d90mf80ec489f005686c@mail.gmail.com>	<53ff794a0606102152h2918faf9wbe310b7c4b812f83@mail.gmail.com>	<82c890d00606110233x7c93655cm225ba9fba359ac41@mail.gmail.com>	<448BF469.9080904@7d4.com>	<82c890d00606110348i7583cd56kdcdb028a9e95e4a9@mail.gmail.com>
	<86hd2ryocc.fsf@ufl.edu>
Message-ID: <448D3DB4.4020402@free.fr>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Allen S. Rout wrote:
>
>
> Don't expect a warm welcome.  This community is like all open-source
> communities, sharply focused on its' own concerns and expertise.  And,
> in an unusual experience for computer types, our core competencies
> hold little or no sway here; they don't even give us much of a leg up.
> Just wait 'till you want to do something nutso like produce a business
> graphic. :)
>
> I'm working on understanding enough of R packaging and documentation
> to begin a 'task view' focused on systems administration, for humble
> submission. That might end up being mostly "log analysis"; the term
> can describe much of what we do, if it's stretched a bit.  I'm hoping
> the task view will attract the teeming masses of sysadmins trapped in
> the mire of Gnuplot and friends.
Although not specifically solving the problem at hand, you might want
to take a look at moodss and moomps (http://moodss.sourceforge.net/),
modular monitoring applications, which uses R
(http://jfontain.free.fr/statistics.htm) and its log module
(http://jfontain.free.fr/log/log.htm).

- --
Jean-Luc Fontaine  http://jfontain.free.fr/
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.3 (GNU/Linux)
Comment: Using GnuPG with Fedora - http://enigmail.mozdev.org

iD8DBQFEjT2ykG/MMvcT1qQRAuF6AJ9nf5phV/GMmCHPuc5bVyA+SoXqGACgnLuZ
u1tZpFOTCHNKOfFLZOC9uXI=
=V8yo
-----END PGP SIGNATURE-----


From rafael.najmanovich at ebi.ac.uk  Mon Jun 12 12:26:18 2006
From: rafael.najmanovich at ebi.ac.uk (Rafael Najmanovich)
Date: Mon, 12 Jun 2006 11:26:18 +0100
Subject: [R] strange behaviour with rotated viewports in grid
Message-ID: <BFC0D2E7-9ED9-482D-B669-0A58FBE4F248@ebi.ac.uk>

	Dear all,

	I am having a problem using grid when rotating a viewport. It seems  
to plot everything on a grey background colour which I am not able to  
get rid of. Even book examples such as that that plot figure 5.10 in  
P. Murrell's R Graphics book show the same behaviour.
	The following example illustrates this issue. I would appreciate if  
anyone has a way to solve this.
	Best regards,

	r.

pushViewport(viewport(width=0.9,height=0.9,name="vp1"))
grid.rect()
pushViewport(viewport(width=0.3,height=0.9,angle=0,name="vp2"))
grid.rect()
upViewport()
pushViewport(viewport(width=0.3,height=0.9,angle=10,name="vp3"))
grid.rect()


Dr. Rafael Najmanovich
European Bioinformatics Institute
Wellcome Trust Genome Campus
Cambridge CB10 1SD
United Kingdom

rafael.najmanovich at ebi.ac.uk - www.ebi.ac.uk/~rafi
+44-1223-492599 (voice) +44-7786-968257(mobile) +44-1223-494468 (fax)


From jranke at uni-bremen.de  Mon Jun 12 12:40:40 2006
From: jranke at uni-bremen.de (Johannes Ranke)
Date: Mon, 12 Jun 2006 12:40:40 +0200
Subject: [R] solving first-order differential equation
In-Reply-To: <MMEALBEGJGIEEHCDNAPOGECOCAAA.zhanwu.dai@bordeaux.inra.fr>
References: <MMEALBEGJGIEEHCDNAPOGECOCAAA.zhanwu.dai@bordeaux.inra.fr>
Message-ID: <20060612104040.GA24340@mail.uft.uni-bremen.de>

The fourth-order Runge-Kutta method is in the odesolve package on CRAN, 
but they manual says, only for didactial purposes. First order
differential equations are generally conveniently solved by the 
lsoda function in this package. I hope this helps, even if it does not
directly answer your question.

Best regards,

Johannes Ranke

* ZhanWu Dai <zhanwu.dai at bordeaux.inra.fr> [060612 12:00]:
> 
> I am an initial user of R. Could you give me some explanations or examples on how to solve the first order differential equations by the first-order Runge-Kutta method? 
> 
> Thank you very much
> 
> Kind regards
> 

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Dr. Johannes Ranke                 jranke at uni-bremen.de
UFT Bremen, Leobenerstr. 1         +49 421 218 8971 
D-28359 Bremen                     http://www.uft.uni-bremen.de/chemie/ranke


From lorenzo.isella at gmail.com  Mon Jun 12 12:51:40 2006
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Mon, 12 Jun 2006 12:51:40 +0200
Subject: [R] Fitting Distributions Directly From a Histogram
Message-ID: <a2b3004b0606120351yefb42bbjf1d1f05f541c29@mail.gmail.com>

Dear All,

A simple question: packages like fitdistr should be ideal to analyze
samples of data taken from a univariate distribution, but what if
rather than the raw data of the observations you are given directly
and only a histogram?
I was thinking about generating artificially a set of data
corresponding to the counts binned in the histogram, but this sounds
too cumbersome.
Another question is the following: fitdistr provides the value of the
log-likely hood function, but what if I want e.g. a chi square test to
get some insight on the goodness of the fitting?
I am sure there must be a way to get it straightforwardly without
coding it myself.
Many thanks

Lorenzo


From ripley at stats.ox.ac.uk  Mon Jun 12 13:55:53 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 12 Jun 2006 12:55:53 +0100 (BST)
Subject: [R] Fitting Distributions Directly From a Histogram
In-Reply-To: <a2b3004b0606120351yefb42bbjf1d1f05f541c29@mail.gmail.com>
References: <a2b3004b0606120351yefb42bbjf1d1f05f541c29@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0606121250100.22422@gannet.stats.ox.ac.uk>

On Mon, 12 Jun 2006, Lorenzo Isella wrote:

> Dear All,
>
> A simple question: packages like fitdistr should be ideal to analyze
> samples of data taken from a univariate distribution, but what if
> rather than the raw data of the observations you are given directly
> and only a histogram?

(It is unusual for the actual data not to be available in real problems.)

> I was thinking about generating artificially a set of data
> corresponding to the counts binned in the histogram, but this sounds
> too cumbersome.

And that is not in any case a correct approach, since the mle from 
binned data is not the same as the mle from any dataset corresponding to 
the binned counts.

You could use mle (package stats4) to estimate parameters from the 
likelihood of the grouped data.

> Another question is the following: fitdistr provides the value of the
> log-likely hood function, but what if I want e.g. a chi square test to
> get some insight on the goodness of the fitting?
> I am sure there must be a way to get it straightforwardly without
> coding it myself.

It is not well-defined, as you need to choose a binning to do a chisq 
test.  Once you have (e.g. via cut and table), chisq.test() will produce 
the test statistic and refer it to a reference distribution (which will 
not take any parameter estimation into account).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From j.van_den_hoff at fz-rossendorf.de  Mon Jun 12 13:59:20 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Mon, 12 Jun 2006 13:59:20 +0200
Subject: [R] solving first-order differential equation
In-Reply-To: <MMEALBEGJGIEEHCDNAPOGECOCAAA.zhanwu.dai@bordeaux.inra.fr>
References: <MMEALBEGJGIEEHCDNAPOGECOCAAA.zhanwu.dai@bordeaux.inra.fr>
Message-ID: <448D5718.6060305@fz-rossendorf.de>

ZhanWu Dai wrote:
> I am an initial user of R. Could you give me some explanations or examples on how to solve the first order differential equations by the first-order Runge-Kutta method? 
> 
> Thank you very much
> 
> Kind regards
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

not really an answer, but a remark:

if your ODE is of the form

dy
---  - k y = A f(x)
dx

(k, A const.) it might be a better idea to use the 'analytic' solution
instead of runge-kutta (faster, probably more accurate).
for instance, if the initial condition is

y(x=0) = 0

and you're looking only at x>0 the solution simply is


y(x) = A (x) {*} exp(-kx)


where {*} means the finite (continous) convolution extending from 0 to x:

y(x) = A  integral from z=0 to z=x {f(z) exp(-k(x-z)) dz}


(which, of course, still has to be computed numerically in general.)
this closed-form solution can then
be used, for instance, to determine the unknown parameters (k, A) from a
least squares fit to measured f(x), y(x)


From ripley at stats.ox.ac.uk  Mon Jun 12 14:06:32 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 12 Jun 2006 13:06:32 +0100 (BST)
Subject: [R] variance specification using glm and quasi
In-Reply-To: <000001c68ddf$bca17880$6501a8c0@AdSAMJeff>
References: <000001c68ddf$bca17880$6501a8c0@AdSAMJeff>
Message-ID: <Pine.LNX.4.64.0606120713050.16796@gannet.stats.ox.ac.uk>

On Mon, 12 Jun 2006, Jeff Miller wrote:

> Cameron and Trivedi in their 1998 Regression Analysis of Count Data refer to
> NB1 and NB2
>
> NB1 is the negative binomial model with variance = mu + (alpha * mu^1)
> yielding (1+alpha)*mu
>
> NB2 sets the power to 2; hence, variance = mu + (alpha*mu^2)
>
> I think that NB2 can be requested via
>
> negbin2<-glm(hhm~sex+age,family=quasi(var="mu^2",link="log"))
>
> Is that right?

No.  That is variance = phi*mu^2, not mu + (alpha*mu^2).

> If so, how I can get NB1? The quasi family appears to be very
> limited in variance specification options.

[Not so in the R-devel version of R: you can supply any variance 
function.]

You can use your own family in any version of R.  Package MASS has for 
many years supplied negative.binomial, which has mu + mu^2/theta, a 
parametrization of `NB2'.  It even provides glm.nb to estimate theta.

Note that (and this is explicitly in your reference) (1+alpha)*mu = phi*mu 
so that NB1 can be fitted as a quasipoisson GLM, although the 
quasilikelihood used is the *not* the likelihood of the model (which is 
not a GLM).  You could easily fit this model by maximum likelihood by 
direct maximization: p.445 of MASS provides a suitable template.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From stecalza at tiscali.it  Mon Jun 12 14:38:41 2006
From: stecalza at tiscali.it (Stefano Calza)
Date: Mon, 12 Jun 2006 14:38:41 +0200
Subject: [R] weird behaviour of summary.default
Message-ID: <20060612123841.GE6112@med.unibs.it>

Hi all.

I may missing something here, but if I do summary.default(1:9999), I get:

   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
      1    2500    5000    5000    7500    9999 


but if I do summary.default(1:10001) I get:

   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
      1    2501    5001    5001    7501   10000 

i.e. Max is rounded to 10000.

What's wrong?

My system:

               _                         
platform       i486-pc-linux-gnu         
arch           i486                      
os             linux-gnu                 
system         i486, linux-gnu           
status                                   
major          2                         
minor          3.1                       
year           2006                      
month          06                        
day            01                        
svn rev        38247                     
language       R                         
version.string Version 2.3.1 (2006-06-01)


Regards,
Stefano


From ggrothendieck at gmail.com  Mon Jun 12 14:43:49 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 12 Jun 2006 08:43:49 -0400
Subject: [R] weird behaviour of summary.default
In-Reply-To: <20060612123841.GE6112@med.unibs.it>
References: <20060612123841.GE6112@med.unibs.it>
Message-ID: <971536df0606120543i79f42347re41c821c85acc2e1@mail.gmail.com>

Try

summary(1:10001, digits = 5)

or

old <- options(digits = 8)
summary(1:10001)


Note default for digits= in:

args(summary.default)



On 6/12/06, Stefano Calza <stecalza at tiscali.it> wrote:
> Hi all.
>
> I may missing something here, but if I do summary.default(1:9999), I get:
>
>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>      1    2500    5000    5000    7500    9999
>
>
> but if I do summary.default(1:10001) I get:
>
>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>      1    2501    5001    5001    7501   10000
>
> i.e. Max is rounded to 10000.
>
> What's wrong?
>
> My system:
>
>               _
> platform       i486-pc-linux-gnu
> arch           i486
> os             linux-gnu
> system         i486, linux-gnu
> status
> major          2
> minor          3.1
> year           2006
> month          06
> day            01
> svn rev        38247
> language       R
> version.string Version 2.3.1 (2006-06-01)
>
>
> Regards,
> Stefano
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From arun.kumar.saha at gmail.com  Mon Jun 12 14:48:33 2006
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Mon, 12 Jun 2006 18:18:33 +0530
Subject: [R] NA values
Message-ID: <d4c57560606120548q6a3e58e8hc5515199fb6f6357@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060612/25c7402b/attachment.pl 

From ripley at stats.ox.ac.uk  Mon Jun 12 14:58:54 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 12 Jun 2006 13:58:54 +0100 (BST)
Subject: [R] weird behaviour of summary.default
In-Reply-To: <20060612123841.GE6112@med.unibs.it>
References: <20060612123841.GE6112@med.unibs.it>
Message-ID: <Pine.LNX.4.64.0606121356170.30167@gannet.stats.ox.ac.uk>

On Mon, 12 Jun 2006, Stefano Calza wrote:

> Hi all.
>
> I may missing something here, but if I do summary.default(1:9999), I get:
>
>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>      1    2500    5000    5000    7500    9999
>
>
> but if I do summary.default(1:10001) I get:
>
>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>      1    2501    5001    5001    7501   10000
>
> i.e. Max is rounded to 10000.
>
> What's wrong?

Nothing: you asked for it to be rounded.  Try reading the help page:

   digits: integer, used for number formatting with 'signif()' (for
           'summary.default') or 'format()' (for 'summary.data.frame').

and

> summary(1:10001, digits=7)
    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
       1    2501    5001    5001    7501   10001


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From petr.pikal at precheza.cz  Mon Jun 12 15:04:57 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 12 Jun 2006 15:04:57 +0200
Subject: [R] NA values
In-Reply-To: <d4c57560606120548q6a3e58e8hc5515199fb6f6357@mail.gmail.com>
Message-ID: <448D8299.9845.160C0FE@localhost>

Hi

yes, but I ****do not**** recommend you to do it

> df<-data.frame(a=1:10, b=11:20)
> df[5,2]<-NA
> df
    a  b
1   1 11
2   2 12
3   3 13
4   4 14
5   5 NA
6   6 16
7   7 17
8   8 18
9   9 19
10 10 20
> df[is.na(df)]<-1000
> df
    a    b
1   1   11
2   2   12
3   3   13
4   4   14
5   5 1000
6   6   16
7   7   17
8   8   18
9   9   19
10 10   20
> 

but in that case you can not simply compute means, sums and other 
values just by e.g.

colSums(df, na.rm=T)

HTH
Petr





On 12 Jun 2006 at 18:18, Arun Kumar Saha wrote:

Date sent:      	Mon, 12 Jun 2006 18:18:33 +0530
From:           	"Arun Kumar Saha" <arun.kumar.saha at gmail.com>
To:             	"r-help at stat.math.ethz.ch" <R-help at stat.math.ethz.ch>
Subject:        	[R] NA values

> Dear all R users,
> 
> I am wondering whether there is any way to replace all "NA" values in
> a data frame by some numerical value, suppose 1000?
> 
> Thanks and Regards
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From josep.vidal at ideg.es  Mon Jun 12 15:06:17 2006
From: josep.vidal at ideg.es (Vidal, Josep)
Date: Mon, 12 Jun 2006 15:06:17 +0200
Subject: [R] Limit extension with geodata
Message-ID: <B969CBEE67C1D04180AEB30B750D9B16542424@guane.local.ig>

Se ha borrado un texto insertado con un juego de caracteres sin especificar...
Nombre: no disponible
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060612/689fb8f2/attachment.pl 

From murdoch at stats.uwo.ca  Mon Jun 12 15:20:50 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 12 Jun 2006 09:20:50 -0400
Subject: [R] NA values
In-Reply-To: <d4c57560606120548q6a3e58e8hc5515199fb6f6357@mail.gmail.com>
References: <d4c57560606120548q6a3e58e8hc5515199fb6f6357@mail.gmail.com>
Message-ID: <448D6A32.8090201@stats.uwo.ca>

On 6/12/2006 8:48 AM, Arun Kumar Saha wrote:
> Dear all R users,
> 
> I am wondering whether there is any way to replace all "NA" values in a data
> frame by some numerical value, suppose 1000?

In a vector it is easy, e.g. x[is.na(x)] <- 1000.  A dataframe is a list 
of vectors, so you could iterate through the list, using one of the 
apply functions (or even a for loop):

apply(x, 2, function(col) {col[is.na(col)] <- 1000; col} )

which is essentially a short form for

for (i in 1:ncol(x)) {
   col <- x[,i]
   col[is.na(col)] <- 1000
   x[,i] <- col
}

Duncan Murdoch

> 
> Thanks and Regards
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From gavin.simpson at ucl.ac.uk  Mon Jun 12 15:24:58 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 12 Jun 2006 14:24:58 +0100
Subject: [R] NA values
In-Reply-To: <d4c57560606120548q6a3e58e8hc5515199fb6f6357@mail.gmail.com>
References: <d4c57560606120548q6a3e58e8hc5515199fb6f6357@mail.gmail.com>
Message-ID: <1150118698.1496.4.camel@gsimpson.geog.ucl.ac.uk>

On Mon, 2006-06-12 at 18:18 +0530, Arun Kumar Saha wrote:
> Dear all R users,
> 
> I am wondering whether there is any way to replace all "NA" values in a data
> frame by some numerical value, suppose 1000?
> 
> Thanks and Regards

Hi, Arun,

sapply(dat, function(x) {x[is.na(x)] <- 1000; x})

e.g.:

## some dummy data
dat <- as.data.frame(matrix(rnorm(100), ncol = 10))
## add some NAs
samp <- sample(1:10, 4)
dat[samp, -samp] <- NA
dat
## replace NAs with 1000
dat <- sapply(dat, function(x) {x[is.na(x)] <- 1000; x})
dat

HTH

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
*  Note new Address, Telephone & Fax numbers from 6th April 2006  *
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson
ECRC & ENSIS                  [t] +44 (0)20 7679 0522
UCL Department of Geography   [f] +44 (0)20 7679 0565
Pearson Building              [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street                  [w] http://www.ucl.ac.uk/~ucfagls/cv/
London, UK.                   [w] http://www.ucl.ac.uk/~ucfagls/
WC1E 6BT.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From gabidiaz at gmail.com  Mon Jun 12 15:52:28 2006
From: gabidiaz at gmail.com (Gabriel Diaz)
Date: Mon, 12 Jun 2006 15:52:28 +0200
Subject: [R] R usage for log analysis
In-Reply-To: <448D3DB4.4020402@free.fr>
References: <82c890d00606101131u11818d90mf80ec489f005686c@mail.gmail.com>
	<53ff794a0606102152h2918faf9wbe310b7c4b812f83@mail.gmail.com>
	<82c890d00606110233x7c93655cm225ba9fba359ac41@mail.gmail.com>
	<448BF469.9080904@7d4.com>
	<82c890d00606110348i7583cd56kdcdb028a9e95e4a9@mail.gmail.com>
	<86hd2ryocc.fsf@ufl.edu> <448D3DB4.4020402@free.fr>
Message-ID: <82c890d00606120652g3cb5035cg82a9c2d57923a5a1@mail.gmail.com>

Hello

Thanks all for the answers.

I'm taking an overview to the project documentation, and seems the
database is the way to go to handle log files of GB order (normally
between 2 and 4 GB each 15 day dump).

In this document http://cran.r-project.org/doc/manuals/R-data.html,
says R will load all data into memory to process it when using
read.table and such. Using a database will do the same? Well,
currently i have no machine with > 2 GB of memory.

The moodss thing looks nice, thanks for the link. But what i have to
do now is an offline analysis of big log files :-). I will try to go
with the mysql -> R way.

gabi



On 6/12/06, Jean-Luc Fontaine <jfontain en free.fr> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Allen S. Rout wrote:
> >
> >
> > Don't expect a warm welcome.  This community is like all open-source
> > communities, sharply focused on its' own concerns and expertise.  And,
> > in an unusual experience for computer types, our core competencies
> > hold little or no sway here; they don't even give us much of a leg up.
> > Just wait 'till you want to do something nutso like produce a business
> > graphic. :)
> >
> > I'm working on understanding enough of R packaging and documentation
> > to begin a 'task view' focused on systems administration, for humble
> > submission. That might end up being mostly "log analysis"; the term
> > can describe much of what we do, if it's stretched a bit.  I'm hoping
> > the task view will attract the teeming masses of sysadmins trapped in
> > the mire of Gnuplot and friends.
> Although not specifically solving the problem at hand, you might want
> to take a look at moodss and moomps (http://moodss.sourceforge.net/),
> modular monitoring applications, which uses R
> (http://jfontain.free.fr/statistics.htm) and its log module
> (http://jfontain.free.fr/log/log.htm).
>
> - --
> Jean-Luc Fontaine  http://jfontain.free.fr/
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.3 (GNU/Linux)
> Comment: Using GnuPG with Fedora - http://enigmail.mozdev.org
>
> iD8DBQFEjT2ykG/MMvcT1qQRAuF6AJ9nf5phV/GMmCHPuc5bVyA+SoXqGACgnLuZ
> u1tZpFOTCHNKOfFLZOC9uXI=
> =V8yo
> -----END PGP SIGNATURE-----
>
> ______________________________________________
> R-help en stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From rafael.najmanovich at ebi.ac.uk  Mon Jun 12 16:04:34 2006
From: rafael.najmanovich at ebi.ac.uk (Rafael Najmanovich)
Date: Mon, 12 Jun 2006 15:04:34 +0100
Subject: [R] strange behaviour with rotated viewports in grid
In-Reply-To: <BFC0D2E7-9ED9-482D-B669-0A58FBE4F248@ebi.ac.uk>
References: <BFC0D2E7-9ED9-482D-B669-0A58FBE4F248@ebi.ac.uk>
Message-ID: <C907AAE7-9D6E-431B-A748-3777168A9743@ebi.ac.uk>

	Dear All,

	The issue I reported earlier seems to be restricted at least to Mac  
OSX GUI using 2.3.1. It does not occur in the Terminal in Mac OSX (R  
version 2.3.1) or in Linux (R version 2.2.1). I will separately send  
this to r-sig-mac.
	Best,

On 12 Jun 2006, at 11:26, Rafael Najmanovich wrote:

> 	Dear all,
>
> 	I am having a problem using grid when rotating a viewport. It  
> seems to plot everything on a grey background colour which I am not  
> able to get rid of. Even book examples such as that that plot  
> figure 5.10 in P. Murrell's R Graphics book show the same behaviour.
> 	The following example illustrates this issue. I would appreciate  
> if anyone has a way to solve this.
> 	Best regards,
>
> 	r.
>
> pushViewport(viewport(width=0.9,height=0.9,name="vp1"))
> grid.rect()
> pushViewport(viewport(width=0.3,height=0.9,angle=0,name="vp2"))
> grid.rect()
> upViewport()
> pushViewport(viewport(width=0.3,height=0.9,angle=10,name="vp3"))
> grid.rect()
>
>
> Dr. Rafael Najmanovich
> European Bioinformatics Institute
> Wellcome Trust Genome Campus
> Cambridge CB10 1SD
> United Kingdom
>
> rafael.najmanovich at ebi.ac.uk - www.ebi.ac.uk/~rafi
> +44-1223-492599 (voice) +44-7786-968257(mobile) +44-1223-494468 (fax)
>
>

Dr. Rafael Najmanovich
European Bioinformatics Institute
Wellcome Trust Genome Campus
Cambridge CB10 1SD
United Kingdom

rafael.najmanovich at ebi.ac.uk - www.ebi.ac.uk/~rafi
+44-1223-492599 (voice) +44-7786-968257(mobile) +44-1223-494468 (fax)


From marsh at uri.edu  Mon Jun 12 16:22:32 2006
From: marsh at uri.edu (Marshall Feldman)
Date: Mon, 12 Jun 2006 10:22:32 -0400
Subject: [R] Chapters
In-Reply-To: <Pine.LNX.4.64.0606120701390.16796@gannet.stats.ox.ac.uk>
Message-ID: <005701c68e2b$aab1ba30$52db8083@MFELDMAN>

Thanks.

Reading the discussion of differences between S and R, I had almost come to
this conclusion but wanted to be sure. Since R keeps its working data in
RAM, the use of chapters seemed unlikely, but I wanted to be sure.

I am, however, surprised this has not come up before. The online
documentation says R is an "implementation" of S, but since there are no
standards defining S versus variations on it, exactly what an
"implementation of S" means is not clear. Besides, other than the online
documentation on R, people using R must use the various books using S. I had
expected others to have asked for clarification on the differences between R
and S, including the various incarnations of the latter.

Thanks again.

	Marsh Feldman

Dr. Marshall Feldman
Acting Director of Research and Academic Affairs
Center for Urban Studies and Research 
The University of Rhode Island
80 Washington Street
Providence, RI 02903-1819
email: marsh @ uri.edu (remove spaces) 
telephone: (401) 277-5218
fax: (401) 277-5099


-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Monday, June 12, 2006 2:07 AM
To: Marshall Feldman
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Chapters

On Sun, 11 Jun 2006, Marshall Feldman wrote:

> I'm surprised this isn't a FAQ, but I searched all over and could not find
a
> reference to it.

I have never seen in mentioned before.  R is not an implementation of S4 
as described in Chambers (1998), so I do not know where you got the idea 
that was relevant.  (The methods package in R is an implementation of 
something close to the class system described in that book, but R can be 
used without the methods package.)

> Chambers (1998) makes repeated references to "Chapters" in S (e.g., p. 6),
> but I can find no reference to "Chapters" in R. Since Chapters were not
used
> in earlier versions of S, I'm wondering if R uses them or not. If it does,
> how does one get them to work? I've been unable to do so.

The closest analogues are

1) packages
2) using different working directories.

Since R does not store objects on file and chapters are directories with 
collections of stored objects (and other things, including DLLs), the 
analogues cannot be very close.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From br44114 at gmail.com  Mon Jun 12 16:38:04 2006
From: br44114 at gmail.com (bogdan romocea)
Date: Mon, 12 Jun 2006 10:38:04 -0400
Subject: [R] R usage for log analysis
Message-ID: <8d5a36350606120738x6deb7722lb1f01a402686a834@mail.gmail.com>

I wouldn't use a DBMS at all -- it is not necessary and I don't see
what you would get in return. Instead I would split very large log
files into a number of pieces so that each piece fits in memory (see
below for an example), then process them in a loop. See the list and
the documentation if you have questions about how to read text files,
count strings etc.

#---split big files in two---
for F in `ls *log`
do
  fn=`echo $F | awk -F\. '{print $1}'`
  ln=`wc -l $F | awk '{print $1}'`  #number of lines in the file
  forsplit=`expr $ln / 2 + 50`  #no. of lines in each chunk, tweak as needed
  echo Splitting $F into pieces of $forsplit lines each........
  split -l $forsplit $F $fn
done


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabriel Diaz
> Sent: Monday, June 12, 2006 9:52 AM
> To: Jean-Luc Fontaine
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] R usage for log analysis
>
> Hello
>
> Thanks all for the answers.
>
> I'm taking an overview to the project documentation, and seems the
> database is the way to go to handle log files of GB order (normally
> between 2 and 4 GB each 15 day dump).
>
> In this document http://cran.r-project.org/doc/manuals/R-data.html,
> says R will load all data into memory to process it when using
> read.table and such. Using a database will do the same? Well,
> currently i have no machine with > 2 GB of memory.
>
> The moodss thing looks nice, thanks for the link. But what i have to
> do now is an offline analysis of big log files :-). I will try to go
> with the mysql -> R way.
>
> gabi
>
>
>
> On 6/12/06, Jean-Luc Fontaine <jfontain at free.fr> wrote:
> > -----BEGIN PGP SIGNED MESSAGE-----
> > Hash: SHA1
> >
> > Allen S. Rout wrote:
> > >
> > >
> > > Don't expect a warm welcome.  This community is like all
> open-source
> > > communities, sharply focused on its' own concerns and
> expertise.  And,
> > > in an unusual experience for computer types, our core competencies
> > > hold little or no sway here; they don't even give us much
> of a leg up.
> > > Just wait 'till you want to do something nutso like
> produce a business
> > > graphic. :)
> > >
> > > I'm working on understanding enough of R packaging and
> documentation
> > > to begin a 'task view' focused on systems administration,
> for humble
> > > submission. That might end up being mostly "log
> analysis"; the term
> > > can describe much of what we do, if it's stretched a bit.
>  I'm hoping
> > > the task view will attract the teeming masses of
> sysadmins trapped in
> > > the mire of Gnuplot and friends.
> > Although not specifically solving the problem at hand, you
> might want
> > to take a look at moodss and moomps
> (http://moodss.sourceforge.net/),
> > modular monitoring applications, which uses R
> > (http://jfontain.free.fr/statistics.htm) and its log module
> > (http://jfontain.free.fr/log/log.htm).
> >
> > - --
> > Jean-Luc Fontaine  http://jfontain.free.fr/
> > -----BEGIN PGP SIGNATURE-----
> > Version: GnuPG v1.4.3 (GNU/Linux)
> > Comment: Using GnuPG with Fedora - http://enigmail.mozdev.org
> >
> > iD8DBQFEjT2ykG/MMvcT1qQRAuF6AJ9nf5phV/GMmCHPuc5bVyA+SoXqGACgnLuZ
> > u1tZpFOTCHNKOfFLZOC9uXI=
> > =V8yo
> > -----END PGP SIGNATURE-----
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


From bartjoosen at hotmail.com  Mon Jun 12 16:45:41 2006
From: bartjoosen at hotmail.com (Bart Joosen)
Date: Mon, 12 Jun 2006 16:45:41 +0200
Subject: [R] Maximum likelihood estimation of Regression parameters
References: <BAY111-DAV7F1A9A3A9C25103A68281D8890@phx.gbl>
	<53ff794a0606100211w57885bd7t4d10bfaaf82d7744@mail.gmail.com>
	<448C34CA.3080402@pdf.com>
Message-ID: <BAY111-DAV12B2F46FEE97FA33263713D88F0@phx.gbl>

I'm working with lm for some time, so I know the function.
The problem was that I purchased the book to learn more about linear models
and couldn't find the equivalent for the maximum likelihood in R, I used lm,
mle, and a few others, but it never allow you to set a variance parameter.
But after reading some further, I should be aware of posting to quick, sorry
for that.

I have read the Practical regression and anova in R by Faraway some month
ago, and I know that the Venable and Ripley book should be the one to buy,
but I think I can't find it for about 10$ as I did with the "Applied linear
statistical models".

As I'm not in the position to spend a lot of money an buying books, I should
do my work with the contributed documents, and the "wrong" book.

Thanks for your help and time both of you

Bart

----- Original Message -----
From: "Spencer Graves" <spencer.graves at pdf.com>
To: <xthua111 at zju.edu.cn>
Cc: "Bart Joosen" <bartjoosen at hotmail.com>; <r-help at stat.math.ethz.ch>
Sent: Sunday, June 11, 2006 5:20 PM
Subject: Re: [R] Maximum likelihood estimation of Regression parameters


>   Have you looked at "lm"?  I think that's what you want.
>
>   Also, have you reviewed the "Documentation" list at
> "www.r-project.org"?  Neter, Kutner, nachtsheim & Wasserman has had a
> long and successful run having first appeared in 1974 and having gone
> through several editions since then.  However, it apparently has not
> kept up with the R revolution, and I would not recommend it today.
>
>   Beyond this, if you don't have Venables and Ripley (2002) Modern
> Applied Statistics with S (Springer), I recommend you look at it first.
>   It has numerous index entries on "regression" and is all around my
> favorite book on R generally.
>
>   Also, have you checked the "Documentation" briefly outlined at
> 'www.r-project.org', including the "Contributed Documentation" on CRAN,
> and "Practical Regression and Anova using R" by Faraway in particular?
>
>   hope this helps.
>   Spencer Graves
>
> Xiaoting Hua wrote:
> > mle(stats4)                Maximum Likelihood Estimation
> >
> > is it list above what you want?
> >
> > On 6/10/06, Bart Joosen <bartjoosen at hotmail.com> wrote:
> >> Hi,
> >>
> >> I want to use Maximum likelihood to estimate the parameters from my
> >> regression line.
> >> I have purchased the book "Applied linear statistical models" from
> >> Neter, Kutner, nachtsheim & Wasserman, and in one of the first
> >> chapters, they use maximum likelihood to estimate the parameters.
> >> Now I want to tried it for my self, but couldn't find the right
function.
> >> In the book, they give a fixed variance to work with, but I couldn't
> >> find a function where I can estimate the predictor and where I have to
> >> give the variance.
> >> Or isn't this neccesairy?
> >> Also they calculate likelihood values for the different values, used
> >> to estimate the parameters (like a normal probability curve), is it
> >> possible to do this with R?
> >>
> >> Kind regards
> >>
> >> Bart
> >>        [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >>
> >
> >
> >
> > ------------------------------------------------------------------------
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>


From tlumley at u.washington.edu  Mon Jun 12 17:13:49 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 12 Jun 2006 08:13:49 -0700 (PDT)
Subject: [R] Calculating survival for set time intervals
In-Reply-To: <1149965561.10012.10.camel@ibm-laptop>
References: <1149965561.10012.10.camel@ibm-laptop>
Message-ID: <Pine.LNX.4.64.0606120812190.19806@homer21.u.washington.edu>

On Sat, 10 Jun 2006, Gregory Pierce wrote:

> Hello friends and fellow R users,
>
> I have successfully tabulated and entered my survival data into R and
> have generated survival curves. But I would like to be able to determine
> what the survival rates are now at one month, three months, six months
> and one year.
>
> I have a data set, via.wall, which I have entered into R, and which
> generates the following Surv object:
>
> Surv(Days,Status==1)
>  [1] 648+   3  109  241   99    7  849+ 105    3+ 539+ 121   42  490
> 21  870+
> [16] 175   20  434  289  826+ 831+ 664  698+   5   24  655+  18    7+
> 85+  65+
> [31] 547+   8    1   55+  69  499+ 448+   0  158+  31  246+ 230+  19
> 118+  54
> [46]  48+  45+  21+ 670+ 585  558+ 544+ 494  481+ 474+ 472+ 461  447
> 446+ 443+
> [61] 429+ 423+ 401  395+ 390  390+ 389+ 383+ 383+ 373+ 362+ 354  344+
> 342  336+
> [76] 335+ 326+ 306  300+ 292  284+ 280+ 271  246+ 237+ 234  233+ 233
> 230+ 230+
> [91] 226+ 225+ 218+ 215  211+ 199+ 191+ 191  190+ 184+ 169+ 163+ 161+
> 153  150
> [106] 129+ 110+ 107+ 100+  84+  77+  69+  52+  38+  11+
>> names(wall.via)
> [1] "Description" "Patient"     "Physician"   "MRN"         "Age"
> [6] "Status"      "Days"        "Cr"          "INR"         "BR"
> [11] "MELD"        "type"
>
> I can guess pretty accurately by looking at the graph what the survival
> rates are at each interval, but I would like to understand how to
> instruct R to calculate it. Hope I have made this clear. I am just a
> beginner, so forgive me if this is trivial. It just isn't clear to me.

You will have generated survival curves with survfit().  Give them to 
summary() and specify the times you want. For example, using one of the 
built-in data sets:

> fit <- survfit(Surv(time, status) ~ x, data = aml)
> summary(fit,times=c(10,20,30,40))
Call: survfit(formula = Surv(time, status) ~ x, data = aml)

                 x=Maintained
  time n.risk n.event survival std.err lower 95% CI upper 95% CI
    10     10       1    0.909  0.0867        0.754        1.000
    20      7       2    0.716  0.1397        0.488        1.000
    30      5       1    0.614  0.1526        0.377        0.999
    40      3       2    0.368  0.1627        0.155        0.875

                 x=Nonmaintained
  time n.risk n.event survival std.err lower 95% CI upper 95% CI
    10      8       4    0.667   0.136       0.4468        0.995
    20      6       1    0.583   0.142       0.3616        0.941
    30      4       3    0.292   0.139       0.1148        0.741
    40      2       1    0.194   0.122       0.0569        0.664


 	-thomas


Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From buser at stat.math.ethz.ch  Mon Jun 12 17:37:03 2006
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Mon, 12 Jun 2006 17:37:03 +0200
Subject: [R] nested mixed-effect model: variance components
In-Reply-To: <BAY106-DAV244471184652FC1DD27A0BBC8B0@phx.gbl>
References: <BAY106-DAV244471184652FC1DD27A0BBC8B0@phx.gbl>
	<8b6f5b5d0778bbf210466186a494624d@hotmail.com>
Message-ID: <17549.35359.437056.347681@stat.math.ethz.ch>

Dear Eric

Do you really have habitats nested within lagoons or are they
partially crossed (meaning that you have the same habitats in
different lagoons)?

If you have them perfectly nested, I think that you cannot
calculate both a fixed effect for habitats and a random effect
for lagoon (see the example below, lme and aov).

You can compare e.g. two lagoons by defining a contrast of the
habitats of one lagoon against the habitats of the other (if you
think that this is a meaningful test to interpret), but you
cannot estimate a random effect lagoon in presence of a nested
FIXED effect habitat. 

aov() will not return you the test and warn you about the
singular model. 

lme() will estimate a variance component for lagoon, but does
not provide you a test for the fixed factor.

Regards,

Christoph Buser



set.seed(1)
dat <- data.frame(y = rnorm(100), lagoon = factor(rep(1:4,each = 25)),
                  habitat = factor(rep(1:20, each = 5)))

summary(aov(y~habitat + Error(lagoon/habitat), data = dat))

library(nlme)
summary(lme(y~habitat, random = ~1|lagoon/habitat, data = dat))


--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH Zurich	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------



Eric Pante writes:
 > Dear listers,
 > 
 > I am trying to assess variance components for a nested, mixed-effects 
 > model. I think I got an answer that make sense from R, but I have a 
 > warning message and I wanted to check that what I am looking at is 
 > actually what I need:
 > 
 > my data are organized as transects within stations, stations within 
 > habitats, habitats within lagoons.
 > lagoons: random, habitats: fixed
 > the question is: how much variation is due to lagoons? habitats? 
 > lagoons*habitat? transects?
 > 
 > Here is my code:
 > 
 > res <- aov(COVER ~ HABITAT + Error(HABITAT+LAGOON+LAGOON/HABITAT), 
 > data=cov)
 > summary(res)
 > 
 > and I get Sum Sq for each to calculate variance components:
 > 
 > Error: STRATE
 >         Df Sum Sq Mean Sq
 > STRATE  5 4493.1   898.6
 > 
 > Error: ATOLL
 >            Df Sum Sq Mean Sq F value Pr(>F)
 > Residuals  5 3340.5   668.1
 > 
 > Error: STRATE:ATOLL
 >            Df  Sum Sq Mean Sq F value Pr(>F)
 > Residuals 18 2442.71  135.71
 > 
 > Error: Within
 >             Df Sum Sq Mean Sq F value Pr(>F)
 > Residuals 145 6422.0    44.3
 > 
 > My error message seems to come from the LAGOON/HABITAT, the Error is 
 > computed.
 > Warning message: Error() model is singular in: aov(COVER ~ HABITAT + 
 > Error(HABITAT+LAGOON+LAGOON/HABITAT), data=cov),
 > 
 > THANKS !!!
 > eric
 > 
 > 
 > 
 > Eric Pante
 > ----------------------------------------------------------------
 > College of Charleston, Grice Marine Laboratory
 > 205 Fort Johnson Road, Charleston SC 29412
 > Phone: 843-953-9190 (lab)  -9200 (main office)
 > ----------------------------------------------------------------
 > 
 > 	"On ne force pas la curiosite, on l'eveille ..."
 > 	Daniel Pennac
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From asr at ufl.edu  Mon Jun 12 18:06:36 2006
From: asr at ufl.edu (Allen S. Rout)
Date: 12 Jun 2006 12:06:36 -0400
Subject: [R] R usage for log analysis
References: <82c890d00606101131u11818d90mf80ec489f005686c@mail.gmail.com>
	<53ff794a0606102152h2918faf9wbe310b7c4b812f83@mail.gmail.com>
	<82c890d00606110233x7c93655cm225ba9fba359ac41@mail.gmail.com>
	<448BF469.9080904@7d4.com>
	<82c890d00606110348i7583cd56kdcdb028a9e95e4a9@mail.gmail.com>
	<86hd2ryocc.fsf@ufl.edu> <448D3DB4.4020402@free.fr>
	<82c890d00606120652g3cb5035cg82a9c2d57923a5a1@mail.gmail.com>
Message-ID: <868xo2z7cj.fsf@ufl.edu>

"Gabriel Diaz" <gabidiaz at gmail.com> writes:

> I'm taking an overview to the project documentation, and seems the
> database is the way to go to handle log files of GB order (normally
> between 2 and 4 GB each 15 day dump).

> In this document http://cran.r-project.org/doc/manuals/R-data.html,
> says R will load all data into memory to process it when using
> read.table and such. Using a database will do the same? Well,
> currently i have no machine with > 2 GB of memory.

Remember, swap too.  This means you're using more time, not running
into a hard limit.

If you're concerned about gross size, then preprocessing could be
useful; but consider: RAM is cheap.  Calibrate RAM purchases
w.r.t. hours of your coding time, -before- you start the project.
Then you can at least mutter to yourself when you waste more than the
cost of core trying to make the problem small. :)

It's entirely reasonable to do all your development work on a smaller
set, and then dump the real data into it and go home.  Unless you've
got something O(N^2) or so, you should be fine.


- Allen S. Rout


From duncan at wald.ucdavis.edu  Mon Jun 12 18:34:12 2006
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Mon, 12 Jun 2006 09:34:12 -0700
Subject: [R] R usage for log analysis
In-Reply-To: <53ff794a0606102152h2918faf9wbe310b7c4b812f83@mail.gmail.com>
References: <82c890d00606101131u11818d90mf80ec489f005686c@mail.gmail.com>
	<53ff794a0606102152h2918faf9wbe310b7c4b812f83@mail.gmail.com>
Message-ID: <448D9784.1000605@wald.ucdavis.edu>


Using Perl or R shouldn't be an either/or issue.
We can call Perl code from R (via RSPerl at least)
and then we get the benefit of well tested, fast code
that might exist in Perl to read the log files
and the ability to do interactive, exploratory analysis in R.
And reading the file in Perl does give a significant reduction
in compute time and memory.

When I do this I put the logs into a database, but I use a
one-line Perl script to get them into the appropriate format.

Xiaoting Hua wrote:
> I heared some project use Perl to analysis log file. And I don't think
> it's suit to analysis log file for R.
> 
> On 6/11/06, Gabriel Diaz <gabidiaz at gmail.com> wrote:
>> Hello,
>>
>> Is there any software project that uses R to do log file analisys?
>>
>> thanks
>>
>> gabi
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From jcadima at math.isa.utl.pt  Mon Jun 12 18:38:48 2006
From: jcadima at math.isa.utl.pt (Jorge Cadima)
Date: Mon, 12 Jun 2006 17:38:48 +0100
Subject: [R] [R-pkgs] New version of Subselect package
Message-ID: <E1FppRA-0006Uv-00@DMjcadima>


Version 0.9-99 of package "subselect" is now on CRAN. 

The subselect package has functions that search for k-variable subsets
of a p-variable (p>k) data set that are optimal under some
criterion. Search algorithms include a full search algorithm "leaps",
based on Furnival and Wilson's leaps and bounds algorithm, and three
random search algorithms: a genetic algorithm ("genetic"), a simulated
annealing algorithm ("anneal") and a modified local search algorithm
("improve"). Previous versions of subselect had three criteria for
optimality ("rm", "rv" and "gcd"), based on exploratory data analysis
considerations. This new version adds four new criteria ("tau2",
"zeta2", "xi2" and "ccr12"), associated with the multivariate linear
hypothesis and the well-known test statistics for that hypothesis
(Wilks' lambda, Roy's largest root, Pillai's trace and the
Hotelling-Lawley statistic). Subselect 0.9-99 also includes two
functions that prepare the arguments for the search functions
associated with the new criteria in the most frequent contexts of
application of the multivariate linear hypothesis - linear regression
(function "lmHmat") and linear discriminant analysis ("ldaHmat") - as
well as another function ("glhHmat") that prepares these arguments for
an analysis associated with a hypothesis specified by the user. The
package documentation includes numerous examples.  

Feedback is appreciated.

Best regards,
Jorge Cadima


-- 
Jorge Cadima
Departamento de Matem?tica
Instituto Superior de Agronomia
Universidade T?cnica de Lisboa
Tapada da Ajuda
1349-017 Lisboa
Portugal

e-mail: jcadima at isa.utl.pt
phone: +351 21 365 3317
fax: +351 21 363 07 23

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From abishara at indiana.edu  Mon Jun 12 19:18:53 2006
From: abishara at indiana.edu (Anthony Bishara)
Date: Mon, 12 Jun 2006 13:18:53 -0400
Subject: [R] r's optim vs. matlab's fminsearch
Message-ID: <000001c68e44$479ba1e0$aec34f81@ads.iu.edu>

Hi,
I'm having a problem converting a Matlab program into R.  The R code works
almost all the time, but about 4% of the time R's optim function gets stuck
on a local minimum whereas matlab's fminsearch function does not (or at
least fminsearch finds a better minimum than optim).  My understanding is
that both functions default to Nelder-Mead optimization, but what's
different about the two functions?  Below, I've pasted the relevant default
options I could find. Are there other options I should to consider?  Does
Matlab have default settings for reflection, contraction, and expansion, and
if so what are they?  Are there other reasons optim and fminsearch might
work differently?
Thanks.

***Matlab's fminsearch defaults***
MaxFunEvals: '200*numberofvariables'
MaxIter: '200*numberofvariables'
TolFun: 1.0000e-004		#Termination tolerance on the function
value.
TolX: 1.0000e-004		#Termination tolerance on x.

***R's optim defaults (for Nelder-Mead)***
maxit=500
reltol=1e-8
alpha=1.0			#Reflection
beta=.5			#Contraction
gamma=2.0			#Expansion	


Anthony J. Bishara
Post-Doctoral Fellow
Department of Psychological & Brain Sciences
Indiana University
1101 E. Tenth St.
Bloomington, IN 47405
(812)856-4678


From gatony at gmail.com  Mon Jun 12 19:34:33 2006
From: gatony at gmail.com (Tony Chu)
Date: Mon, 12 Jun 2006 13:34:33 -0400
Subject: [R] select the last row by id group
Message-ID: <f8c6ef510606121034l478a1c77kc298cdef1336dd5a@mail.gmail.com>

  Dear R users:

  I have a small test dataframe as the follows :

math = c(80,75,70,65,65,70)
reading = c(65,70,88,NA,90,NA)
id = c('001','001','001','002','003','003')
score = data.frame(id, reading, math)

> score
   id reading math
1 001      65   80
2 001      70   75
3 001      88   70
4 002      NA   65
5 003      90    65
6 003      NA   70

  Could someone advise me tips about how to select the last row from
each id group of 001, 002, & 003?

In other words, the rows I need are :

   id reading math
3 001      88   70
4 002      NA   65
6 003      NA   70

 I tried function sebset but could not go very far. Thanks !


From ripley at stats.ox.ac.uk  Mon Jun 12 19:39:34 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 12 Jun 2006 18:39:34 +0100 (BST)
Subject: [R] r's optim vs. matlab's fminsearch
In-Reply-To: <000001c68e44$479ba1e0$aec34f81@ads.iu.edu>
References: <000001c68e44$479ba1e0$aec34f81@ads.iu.edu>
Message-ID: <Pine.LNX.4.64.0606121838160.1622@gannet.stats.ox.ac.uk>

Unless you know the function to be non-smooth, I suggest you use 
method="BFGS" in R.

BTW, all such algorithms are only designed to find local minima, and so 
the choice of starting point may be crucial.

On Mon, 12 Jun 2006, Anthony Bishara wrote:

> Hi,
> I'm having a problem converting a Matlab program into R.  The R code works
> almost all the time, but about 4% of the time R's optim function gets stuck
> on a local minimum whereas matlab's fminsearch function does not (or at
> least fminsearch finds a better minimum than optim).  My understanding is
> that both functions default to Nelder-Mead optimization, but what's
> different about the two functions?  Below, I've pasted the relevant default
> options I could find. Are there other options I should to consider?  Does
> Matlab have default settings for reflection, contraction, and expansion, and
> if so what are they?  Are there other reasons optim and fminsearch might
> work differently?
> Thanks.
>
> ***Matlab's fminsearch defaults***
> MaxFunEvals: '200*numberofvariables'
> MaxIter: '200*numberofvariables'
> TolFun: 1.0000e-004		#Termination tolerance on the function
> value.
> TolX: 1.0000e-004		#Termination tolerance on x.
>
> ***R's optim defaults (for Nelder-Mead)***
> maxit=500
> reltol=1e-8
> alpha=1.0			#Reflection
> beta=.5			#Contraction
> gamma=2.0			#Expansion
>
>
> Anthony J. Bishara
> Post-Doctoral Fellow
> Department of Psychological & Brain Sciences
> Indiana University
> 1101 E. Tenth St.
> Bloomington, IN 47405
> (812)856-4678
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From spencer.graves at pdf.com  Mon Jun 12 20:29:01 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 12 Jun 2006 11:29:01 -0700
Subject: [R] r's optim vs. matlab's fminsearch
In-Reply-To: <Pine.LNX.4.64.0606121838160.1622@gannet.stats.ox.ac.uk>
References: <000001c68e44$479ba1e0$aec34f81@ads.iu.edu>
	<Pine.LNX.4.64.0606121838160.1622@gannet.stats.ox.ac.uk>
Message-ID: <448DB26D.1040304@pdf.com>

	  Have you also tried 'nlminb'?  A year or so ago, Doug Bates switched 
from optim to nlminb for mixed-effects estimation.  I'm not certain I 
know why, but I think 'nlminb' may automatically adjust the 'scale' 
parameter by default, while "optim" does not automatically adjust the 
comparable 'control$parscale'.  If Matlab does reasonable auto-scaling, 
that might explain the difference.  Others (e.g., Professors Ripley and 
Bates) might be able to add more.

	  There might be more information on this in 'RSiteSearch', but I think 
it's temporarily off line right now.  And, of course, you can always 
read the R code.  Both 'nlminb' and 'optim' call compiled code, but the 
source should be available.

	  Hope this helps,
	  Spencer Graves	

Prof Brian Ripley wrote:
> Unless you know the function to be non-smooth, I suggest you use 
> method="BFGS" in R.
> 
> BTW, all such algorithms are only designed to find local minima, and so 
> the choice of starting point may be crucial.
> 
> On Mon, 12 Jun 2006, Anthony Bishara wrote:
> 
>> Hi,
>> I'm having a problem converting a Matlab program into R.  The R code works
>> almost all the time, but about 4% of the time R's optim function gets stuck
>> on a local minimum whereas matlab's fminsearch function does not (or at
>> least fminsearch finds a better minimum than optim).  My understanding is
>> that both functions default to Nelder-Mead optimization, but what's
>> different about the two functions?  Below, I've pasted the relevant default
>> options I could find. Are there other options I should to consider?  Does
>> Matlab have default settings for reflection, contraction, and expansion, and
>> if so what are they?  Are there other reasons optim and fminsearch might
>> work differently?
>> Thanks.
>>
>> ***Matlab's fminsearch defaults***
>> MaxFunEvals: '200*numberofvariables'
>> MaxIter: '200*numberofvariables'
>> TolFun: 1.0000e-004		#Termination tolerance on the function
>> value.
>> TolX: 1.0000e-004		#Termination tolerance on x.
>>
>> ***R's optim defaults (for Nelder-Mead)***
>> maxit=500
>> reltol=1e-8
>> alpha=1.0			#Reflection
>> beta=.5			#Contraction
>> gamma=2.0			#Expansion
>>
>>
>> Anthony J. Bishara
>> Post-Doctoral Fellow
>> Department of Psychological & Brain Sciences
>> Indiana University
>> 1101 E. Tenth St.
>> Bloomington, IN 47405
>> (812)856-4678
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>


From Dimitris.Rizopoulos at med.kuleuven.be  Mon Jun 12 20:34:57 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Mon, 12 Jun 2006 20:34:57 +0200
Subject: [R] select the last row by id group
In-Reply-To: <f8c6ef510606121034l478a1c77kc298cdef1336dd5a@mail.gmail.com>
References: <f8c6ef510606121034l478a1c77kc298cdef1336dd5a@mail.gmail.com>
Message-ID: <1150137297.448db3d18459d@webmail2.kuleuven.be>

one approach is the following:

math <- c(80, 75, 70, 65, 65, 70)
reading <- c(65, 70, 88, NA, 90, NA)
id <- c('001', '001', '001', '002', '003', '003')
score <- data.frame(id, reading, math)
#############
ind <- cumsum(tapply(score$id, score$id, length))
score[ind, ]


I hope it helps.

Best,
Dimitris

---- 
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting Tony Chu <gatony at gmail.com>:

>   Dear R users:
> 
>   I have a small test dataframe as the follows :
> 
> math = c(80,75,70,65,65,70)
> reading = c(65,70,88,NA,90,NA)
> id = c('001','001','001','002','003','003')
> score = data.frame(id, reading, math)
> 
> > score
>    id reading math
> 1 001      65   80
> 2 001      70   75
> 3 001      88   70
> 4 002      NA   65
> 5 003      90    65
> 6 003      NA   70
> 
>   Could someone advise me tips about how to select the last row from
> each id group of 001, 002, & 003?
> 
> In other words, the rows I need are :
> 
>    id reading math
> 3 001      88   70
> 4 002      NA   65
> 6 003      NA   70
> 
>  I tried function sebset but could not go very far. Thanks !
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ggrothendieck at gmail.com  Mon Jun 12 20:43:23 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 12 Jun 2006 14:43:23 -0400
Subject: [R] select the last row by id group
In-Reply-To: <f8c6ef510606121034l478a1c77kc298cdef1336dd5a@mail.gmail.com>
References: <f8c6ef510606121034l478a1c77kc298cdef1336dd5a@mail.gmail.com>
Message-ID: <971536df0606121143p28bfa2e6w362c58b60aa708b1@mail.gmail.com>

Try this:

score[tapply(rownames(score), score$id, tail, 1),]


On 6/12/06, Tony Chu <gatony at gmail.com> wrote:
>  Dear R users:
>
>  I have a small test dataframe as the follows :
>
> math = c(80,75,70,65,65,70)
> reading = c(65,70,88,NA,90,NA)
> id = c('001','001','001','002','003','003')
> score = data.frame(id, reading, math)
>
> > score
>   id reading math
> 1 001      65   80
> 2 001      70   75
> 3 001      88   70
> 4 002      NA   65
> 5 003      90    65
> 6 003      NA   70
>
>  Could someone advise me tips about how to select the last row from
> each id group of 001, 002, & 003?
>
> In other words, the rows I need are :
>
>   id reading math
> 3 001      88   70
> 4 002      NA   65
> 6 003      NA   70
>
>  I tried function sebset but could not go very far. Thanks !
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From abishara at indiana.edu  Mon Jun 12 21:00:24 2006
From: abishara at indiana.edu (Anthony Bishara)
Date: Mon, 12 Jun 2006 15:00:24 -0400
Subject: [R] r's optim vs. matlab's fminsearch
In-Reply-To: <Pine.LNX.4.64.0606121838160.1622@gannet.stats.ox.ac.uk>
Message-ID: <000601c68e52$767e9ef0$aec34f81@ads.iu.edu>

	Thanks for the feedback.  I should've mentioned before that the
function is non-smooth.  Also, it has a 3-element free parameter vector, and
I've been using a grid of 27 vectors of starting parameters.  

Anthony

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Monday, June 12, 2006 1:40 PM
To: Anthony Bishara
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] r's optim vs. matlab's fminsearch

Unless you know the function to be non-smooth, I suggest you use 
method="BFGS" in R.

BTW, all such algorithms are only designed to find local minima, and so 
the choice of starting point may be crucial.

On Mon, 12 Jun 2006, Anthony Bishara wrote:

> Hi,
> I'm having a problem converting a Matlab program into R.  The R code works
> almost all the time, but about 4% of the time R's optim function gets
stuck
> on a local minimum whereas matlab's fminsearch function does not (or at
> least fminsearch finds a better minimum than optim).  My understanding is
> that both functions default to Nelder-Mead optimization, but what's
> different about the two functions?  Below, I've pasted the relevant
default
> options I could find. Are there other options I should to consider?  Does
> Matlab have default settings for reflection, contraction, and expansion,
and
> if so what are they?  Are there other reasons optim and fminsearch might
> work differently?
> Thanks.
>
> ***Matlab's fminsearch defaults***
> MaxFunEvals: '200*numberofvariables'
> MaxIter: '200*numberofvariables'
> TolFun: 1.0000e-004		#Termination tolerance on the function
> value.
> TolX: 1.0000e-004		#Termination tolerance on x.
>
> ***R's optim defaults (for Nelder-Mead)***
> maxit=500
> reltol=1e-8
> alpha=1.0			#Reflection
> beta=.5			#Contraction
> gamma=2.0			#Expansion
>
>
> Anthony J. Bishara
> Post-Doctoral Fellow
> Department of Psychological & Brain Sciences
> Indiana University
> 1101 E. Tenth St.
> Bloomington, IN 47405
> (812)856-4678
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Mike.Prager at noaa.gov  Mon Jun 12 21:39:32 2006
From: Mike.Prager at noaa.gov (Michael H. Prager)
Date: Mon, 12 Jun 2006 15:39:32 -0400
Subject: [R] r's optim vs. matlab's fminsearch
In-Reply-To: <000601c68e52$767e9ef0$aec34f81@ads.iu.edu>
References: <000601c68e52$767e9ef0$aec34f81@ads.iu.edu>
Message-ID: <448DC2F4.20001@noaa.gov>

In using Nelder-Mead outside R, I find it critical to restart the 
algorithm (repeatedly) after it thinks it's found a solution, to see if 
it can do better.  I can't say whether the R and Matlab implementations 
do this automatically or not.


on 6/12/2006 3:00 PM Anthony Bishara said the following:
> 	Thanks for the feedback.  I should've mentioned before that the
> function is non-smooth.  Also, it has a 3-element free parameter vector, and
> I've been using a grid of 27 vectors of starting parameters.  
>
> Anthony
>
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Sent: Monday, June 12, 2006 1:40 PM
> To: Anthony Bishara
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] r's optim vs. matlab's fminsearch
>
> Unless you know the function to be non-smooth, I suggest you use 
> method="BFGS" in R.
>
> BTW, all such algorithms are only designed to find local minima, and so 
> the choice of starting point may be crucial.
>
> On Mon, 12 Jun 2006, Anthony Bishara wrote:
>
>   
>> Hi,
>> I'm having a problem converting a Matlab program into R.  The R code works
>> almost all the time, but about 4% of the time R's optim function gets
>>     
> stuck
>   
>> on a local minimum whereas matlab's fminsearch function does not (or at
>> least fminsearch finds a better minimum than optim).  My understanding is
>> that both functions default to Nelder-Mead optimization, but what's
>> different about the two functions?  Below, I've pasted the relevant
>>     
> default
>   
>> options I could find. Are there other options I should to consider?  Does
>> Matlab have default settings for reflection, contraction, and expansion,
>>     
> and
>   
>> if so what are they?  Are there other reasons optim and fminsearch might
>> work differently?
>> Thanks.
>>
>> ***Matlab's fminsearch defaults***
>> MaxFunEvals: '200*numberofvariables'
>> MaxIter: '200*numberofvariables'
>> TolFun: 1.0000e-004		#Termination tolerance on the function
>> value.
>> TolX: 1.0000e-004		#Termination tolerance on x.
>>
>> ***R's optim defaults (for Nelder-Mead)***
>> maxit=500
>> reltol=1e-8
>> alpha=1.0			#Reflection
>> beta=.5			#Contraction
>> gamma=2.0			#Expansion
>>
>>
>> Anthony J. Bishara
>> Post-Doctoral Fellow
>> Department of Psychological & Brain Sciences
>> Indiana University
>> 1101 E. Tenth St.
>> Bloomington, IN 47405
>> (812)856-4678
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>>     
> http://www.R-project.org/posting-guide.html
>   
>
>   

-- 
Michael Prager, Ph.D.
Southeast Fisheries Science Center
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
** Opinions expressed are personal, not official.  No
** official endorsement of any product is made or implied.


From pptakauskas at yahoo.de  Mon Jun 12 22:22:52 2006
From: pptakauskas at yahoo.de (Paulius Ptakauskas)
Date: Mon, 12 Jun 2006 13:22:52 -0700 (PDT)
Subject: [R] bootstrap cauchy
Message-ID: <20060612202252.4840.qmail@web30812.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060612/af2ab19a/attachment.pl 

From smckinney at bccrc.ca  Mon Jun 12 22:45:02 2006
From: smckinney at bccrc.ca (Steven McKinney)
Date: Mon, 12 Jun 2006 13:45:02 -0700
Subject: [R] scripts to process array CGH data files from NimbleGen
Message-ID: <0BE438149FF2254DB4199E2682C8DFEB01D553F9@crcmail1.BCCRC.CA>



Before I reinvent the wheel, has anyone set up scripts to read in
array CGH data files from NimbleGen, for use in the aCGH package or
a similar package?

I have normalized NimbleGen files
([array_id]_[identifier]_normalized.txt in the NimbleGen naming scheme)
several to a directory, to read in and use to create an aCGH object.

Any info appreciated.

Best

Steve McKinney



Steven McKinney

Statistician
Molecular Oncology and Breast Cancer Program
British Columbia Cancer Research Centre

smckinney at bccrc.ca


From aoganyan at niss.org  Mon Jun 12 23:14:46 2006
From: aoganyan at niss.org (Anna Oganyan)
Date: Mon, 12 Jun 2006 17:14:46 -0400
Subject: [R] multivariate quantile estimation
Message-ID: <448DD946.4050003@niss.org>

Hello,
is there any function/package in R for multivariate quantile estimation?
(I couldn't find any for the moment)
Thank you!
Anna


From klingd at reed.edu  Mon Jun 12 23:19:39 2006
From: klingd at reed.edu (David Kling)
Date: Mon, 12 Jun 2006 14:19:39 -0700
Subject: [R] Group averages
Message-ID: <448DDA6B.5010809@reed.edu>

Hello:

I hope none of you will mind helping a newbie.  I'm a student research 
assistant working with a large data set in which observations are 
categorized according to two factors. I'm trying to calculate the group 
mean and variance of a variable (called 'hsgpa' in the example data 
presented below) to each observation  , excluding that observation.  For 
example, if there are 20 observations with the same value of the two 
factors, for each of the 20 I'd like to generate the mean and variance 
of the 'hsgpa' values of the other 19 group members.  This must be done 
for every observation in the data set.

I've searched the R mail archives, read the manuals, and read 
documentation for tapply() andby() as well as summaryBy() in the 'doBy' 
package and with() from 'Hmisc.'  It may be that since I'm new to 
writing functions and R is the first language I've ever worked with I'm 
less able to come up with a solution than some other new R users.  None 
of the functions I have tried have been succesful, and it doesn't seem 
worth it to reproduce and explain my best effort.  I hope someone has 
some ideas!  Looking at what an experienced user would try should help 
me with my present task as well as future problems.

Below I've included some lines that will generate a sample data set 
similar to the one I'm working with:

#
#Example data:
#
case <- sample(seq(1,10000,1),5000,replace=FALSE)
hsgpa <- rbeta(5000,7,1.5)*4.25
yr <- sample(seq(1993,2005,1),5000,replace=TRUE)
conf <- sample(letters[1:5],5000,replace=TRUE)
data <- data.frame(case=case,hsgpa=hsgpa,yr=yr,conf=conf)
data$conf <- as.character(data$conf)
s1 <- sample(seq(1,5000,1),500,replace=FALSE)
k <- data$hsgpa
k[row.names(data) %in% s1] <- NA
data$hsgpa <- k
s2 <- sample(seq(1,5000,1),100,replace=FALSE)
k <- data$yr
k[row.names(data) %in% s2] <- NA
data$yr <- k
k <- data$conf
k[row.names(data) %in% s2] <- NA
data$conf <- k
remove(case,hsgpa,yr,conf,s1,s2,k)
#


From dimitrijoe at ipea.gov.br  Tue Jun 13 00:00:03 2006
From: dimitrijoe at ipea.gov.br (Dimitri Szerman)
Date: Mon, 12 Jun 2006 19:00:03 -0300
Subject: [R] non parametric estimates of the hazard with right censored data
Message-ID: <b6150c70606121500m21b190ccid9f62bc4a1a0f4de@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060612/af1f7c2b/attachment.pl 

From nospam at akroeger.com  Tue Jun 13 00:21:34 2006
From: nospam at akroeger.com (AKA)
Date: Mon, 12 Jun 2006 18:21:34 -0400
Subject: [R] Anybody ever notice...
Message-ID: <000601c68e6e$91bc2090$0500a8c0@akadev.com>

It could be a little bug or a fault in my setup but I notice that after I
end an Rcmdr session I can't restart it without restarting Rgui (Windows)
Do I need to add something to my environment variables? Is it an issue with
Tcl/Tk or it's settings? Or what?
It is not a big deal but if someone has a fix for this I would give it a
shot I usually start Rcmd from the console library(Rcmdr) that have anything
to do with it?


From jholtman at gmail.com  Tue Jun 13 00:25:01 2006
From: jholtman at gmail.com (jim holtman)
Date: Mon, 12 Jun 2006 18:25:01 -0400
Subject: [R] Group averages
In-Reply-To: <448DDA6B.5010809@reed.edu>
References: <448DDA6B.5010809@reed.edu>
Message-ID: <644e1f320606121525r4dcf95c0vb3a1aaf62d520f4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060612/38e817a7/attachment.pl 

From ggrothendieck at gmail.com  Tue Jun 13 00:25:12 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 12 Jun 2006 18:25:12 -0400
Subject: [R] Group averages
In-Reply-To: <448DDA6B.5010809@reed.edu>
References: <448DDA6B.5010809@reed.edu>
Message-ID: <971536df0606121525v5128b6apd35781a9cd8c5473@mail.gmail.com>

Assuming that yr and conf are the two factors referred to in the
description, create a function f which calculates the ith row
of the output and use sapply like this:

attach(data)
f <- function(i) {
	hsgpa <- na.omit(hsgpa[-i][conf[-i] == conf[i] & yr[-i] == yr[i]])
	if (length(hsgpa)) c(mean = mean(hsgpa), var = var(hsgpa))
	else c(mean = NA, var = NA)
}
out <- t(sapply(1:nrow(data), f))

On 6/12/06, David Kling <klingd at reed.edu> wrote:
> Hello:
>
> I hope none of you will mind helping a newbie.  I'm a student research
> assistant working with a large data set in which observations are
> categorized according to two factors. I'm trying to calculate the group
> mean and variance of a variable (called 'hsgpa' in the example data
> presented below) to each observation  , excluding that observation.  For
> example, if there are 20 observations with the same value of the two
> factors, for each of the 20 I'd like to generate the mean and variance
> of the 'hsgpa' values of the other 19 group members.  This must be done
> for every observation in the data set.
>
> I've searched the R mail archives, read the manuals, and read
> documentation for tapply() andby() as well as summaryBy() in the 'doBy'
> package and with() from 'Hmisc.'  It may be that since I'm new to
> writing functions and R is the first language I've ever worked with I'm
> less able to come up with a solution than some other new R users.  None
> of the functions I have tried have been succesful, and it doesn't seem
> worth it to reproduce and explain my best effort.  I hope someone has
> some ideas!  Looking at what an experienced user would try should help
> me with my present task as well as future problems.
>
> Below I've included some lines that will generate a sample data set
> similar to the one I'm working with:
>
> #
> #Example data:
> #
> case <- sample(seq(1,10000,1),5000,replace=FALSE)
> hsgpa <- rbeta(5000,7,1.5)*4.25
> yr <- sample(seq(1993,2005,1),5000,replace=TRUE)
> conf <- sample(letters[1:5],5000,replace=TRUE)
> data <- data.frame(case=case,hsgpa=hsgpa,yr=yr,conf=conf)
> data$conf <- as.character(data$conf)
> s1 <- sample(seq(1,5000,1),500,replace=FALSE)
> k <- data$hsgpa
> k[row.names(data) %in% s1] <- NA
> data$hsgpa <- k
> s2 <- sample(seq(1,5000,1),100,replace=FALSE)
> k <- data$yr
> k[row.names(data) %in% s2] <- NA
> data$yr <- k
> k <- data$conf
> k[row.names(data) %in% s2] <- NA
> data$conf <- k
> remove(case,hsgpa,yr,conf,s1,s2,k)
> #
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From apjaworski at mmm.com  Tue Jun 13 00:29:58 2006
From: apjaworski at mmm.com (apjaworski at mmm.com)
Date: Mon, 12 Jun 2006 17:29:58 -0500
Subject: [R] Anybody ever notice...
In-Reply-To: <000601c68e6e$91bc2090$0500a8c0@akadev.com>
Message-ID: <OF4F9ED68E.9EFEC498-ON8625718B.007B530A-8625718B.007B983E@mmm.com>

I am guessing you are starting Rcmdr using

> library(Rcmdr).

If this is the case, repeating the library command does nothing, since
Rcmdr package is already attached.  Try

> Commander()

instead.  Make sure you use capital C.

Cheers,

Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


                                                                           
             "AKA"                                                         
             <nospam at akroeger.                                             
             com>                                                       To 
             Sent by:                  <r-help at stat.math.ethz.ch>          
             r-help-bounces at st                                          cc 
             at.math.ethz.ch                                               
                                                                   Subject 
                                       [R] Anybody ever notice...          
             06/12/2006 05:21                                              
             PM                                                            
                                                                           
                                                                           
                                                                           
                                                                           




It could be a little bug or a fault in my setup but I notice that after I
end an Rcmdr session I can't restart it without restarting Rgui (Windows)
Do I need to add something to my environment variables? Is it an issue with
Tcl/Tk or it's settings? Or what?
It is not a big deal but if someone has a fix for this I would give it a
shot I usually start Rcmd from the console library(Rcmdr) that have
anything
to do with it?

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From drakegis at dacafe.com  Tue Jun 13 01:13:19 2006
From: drakegis at dacafe.com (DrakeGis)
Date: Mon, 12 Jun 2006 19:13:19 -0400 (EDT)
Subject: [R] Local Ripley's  K and L
Message-ID: <63717.69.137.217.247.1150153999.squirrel@cafemail.giscafe.com>

Hi List,
  Does any of you know about an implementation of the Local Ripley's K and
L functions for R? The function has been described in "Second-Order
Neigborhood Analysis of Mapped Point Patterns (Getis & Franklin, 1987,
Ecology 68:3)

  Thanks.
  D.



-----------------------------------------
Stay ahead of the information curve.
Receive GIS news and jobs on your desktop daily.
Subscribe today to the GIS CafeNews newsletter.
[ http://www10.giscafe.com/nl/newsletter_subscribe.php ]
It's informative and essential.


From klingd at reed.edu  Tue Jun 13 01:48:39 2006
From: klingd at reed.edu (David Kling)
Date: Mon, 12 Jun 2006 16:48:39 -0700
Subject: [R] Group averages
In-Reply-To: <644e1f320606121525r4dcf95c0vb3a1aaf62d520f4@mail.gmail.com>
References: <448DDA6B.5010809@reed.edu>
	<644e1f320606121525r4dcf95c0vb3a1aaf62d520f4@mail.gmail.com>
Message-ID: <448DFD57.2040107@reed.edu>

Thanks!  Both responders understood what I was after despite my poor 
explanation and came up with very helpful responses.  If anyone else has 
an idea, please share!

David Kling


From lrleond at unal.edu.co  Tue Jun 13 01:51:58 2006
From: lrleond at unal.edu.co (Leidy Rocio Leon Davila)
Date: Mon, 12 Jun 2006 18:51:58 -0500
Subject: [R] binomial dat set
Message-ID: <fa65df5abc26.448db7ce@unal.edu.co>

Dear list members,
I need a dat set with follow features:
-Binomial response in longitudinal form
-overdispersion

thanks 
Leidy Roc?o


From list.eric at gmail.com  Tue Jun 13 02:28:56 2006
From: list.eric at gmail.com (Eric Hu)
Date: Mon, 12 Jun 2006 17:28:56 -0700
Subject: [R] plot two graphs with different length of data
Message-ID: <c3e96b080606121728s27d055c1g1a5ee788b9bd4c88@mail.gmail.com>

Hi I am trying to plot two data set in the same picture window without
overlapping with each other. I am using the format plot(x1,y1,x2,y2)
but get the following error message:

>plot(as.numeric(r0[,4]),as.numeric(r0[,7]),as.numeric(r0[,4]),as.numeric(r0[,7][ind[,1]]))
Error in plot.window(xlim, ylim, log, asp, ...) :
        invalid 'ylim' value

Can anyone tell me what went wrong? Thanks.

Eric


From jim at bitwrit.com.au  Tue Jun 13 16:46:18 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Tue, 13 Jun 2006 10:46:18 -0400
Subject: [R] plot two graphs with different length of data
In-Reply-To: <c3e96b080606121728s27d055c1g1a5ee788b9bd4c88@mail.gmail.com>
References: <c3e96b080606121728s27d055c1g1a5ee788b9bd4c88@mail.gmail.com>
Message-ID: <448ECFBA.7080800@bitwrit.com.au>

Eric Hu wrote:
> Hi I am trying to plot two data set in the same picture window without
> overlapping with each other. I am using the format plot(x1,y1,x2,y2)
> but get the following error message:
> 
> 
>>plot(as.numeric(r0[,4]),as.numeric(r0[,7]),as.numeric(r0[,4]),as.numeric(r0[,7][ind[,1]]))
> 
> Error in plot.window(xlim, ylim, log, asp, ...) :
>         invalid 'ylim' value
> 
> Can anyone tell me what went wrong? Thanks.

plot is probably interpreting one of your data vectors as the ylim 
argument. If you want to plot multiple data series of different lengths, 
it is probably simplest to use points() (or lines()) after plotting the 
first one. For an example of this, see:

http://cran.r-project.org/doc/contrib/Lemon-kickstart/kr_addat.html

Jim


From list.eric at gmail.com  Tue Jun 13 03:20:03 2006
From: list.eric at gmail.com (Eric Hu)
Date: Mon, 12 Jun 2006 18:20:03 -0700
Subject: [R] plot two graphs with different length of data
In-Reply-To: <448ECFBA.7080800@bitwrit.com.au>
References: <c3e96b080606121728s27d055c1g1a5ee788b9bd4c88@mail.gmail.com>
	<448ECFBA.7080800@bitwrit.com.au>
Message-ID: <c3e96b080606121820s3f52fd22wdb9dd878673be2d9@mail.gmail.com>

Thanks Jim. This seems to work for my case already.

Eric

On 6/13/06, Jim Lemon <jim at bitwrit.com.au> wrote:
> Eric Hu wrote:
> > Hi I am trying to plot two data set in the same picture window without
> > overlapping with each other. I am using the format plot(x1,y1,x2,y2)
> > but get the following error message:
> >
> >
> >>plot(as.numeric(r0[,4]),as.numeric(r0[,7]),as.numeric(r0[,4]),as.numeric(r0[,7][ind[,1]]))
> >
> > Error in plot.window(xlim, ylim, log, asp, ...) :
> >         invalid 'ylim' value
> >
> > Can anyone tell me what went wrong? Thanks.
>
> plot is probably interpreting one of your data vectors as the ylim
> argument. If you want to plot multiple data series of different lengths,
> it is probably simplest to use points() (or lines()) after plotting the
> first one. For an example of this, see:
>
> http://cran.r-project.org/doc/contrib/Lemon-kickstart/kr_addat.html
>
> Jim
>


From dushoff at eno.princeton.edu  Tue Jun 13 03:27:02 2006
From: dushoff at eno.princeton.edu (Jonathan Dushoff)
Date: Mon, 12 Jun 2006 21:27:02 -0400 (EDT)
Subject: [R] Where is package gridBase?
Message-ID: <Pine.LNX.4.61.0606122119070.3074@tahawus.Princeton.EDU>

Trying to follow the instructions on the page
http://tolstoy.newcastle.edu.au/R/help/05/12/17153.html (to make a
figure with an inset), I typed

> install.packages("gridBase", repos="http://cran.r-project.org")

and was rejected.

 	no package 'gridBase' at the repositories in: download.packages(pkgs,
 	destdir = tmpd, available = available,

I went to the FAQ at
http://cran.r-project.org/doc/FAQ/R-FAQ.html#Add_002don-packages-from-CRAN,
and was informed that gridBase was an add-on package available from
CRAN.

I also tried installing other packages using exactly the same command.

> install.packages("TeachingDemos", repos="http://cran.r-project.org")

These packages were apparently found:
 	trying URL
 	'http://cran.r-project.org/src/contrib/TeachingDemos_1.2.tar.gz'
 	Content type 'application/x-tar' length 66472 bytes
 	opened URL

 	(and so on)

I would appreciate any insight that anyone might have.

Jonathan


From yong.li at unimelb.edu.au  Tue Jun 13 03:35:17 2006
From: yong.li at unimelb.edu.au (Yong Li)
Date: Tue, 13 Jun 2006 11:35:17 +1000
Subject: [R] help on remote sensing
Message-ID: <6.2.5.6.2.20060613113328.031de190@unimelb.edu.au>


Dear all,

Is any body know the R packages for processing remote sensing images 
(not the Rimages) like ERDAS Imagine does? Cheers

Yong


From g.abraham at ms.unimelb.edu.au  Tue Jun 13 04:16:26 2006
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Tue, 13 Jun 2006 12:16:26 +1000
Subject: [R] Multiple lag.plots per page
Message-ID: <448E1FFA.6070601@ms.unimelb.edu.au>

Hi,

I'm trying to plot several lag.plots on a page, however the second plot 
replaces the first one (although it only takes up the upper half as it 
should):

par(mfrow=c(2,1))
a<-sin(1:100)
b<-cos(1:100)
lag.plot(a)
lag.plot(b)

What's the trick to this?

I'm using R 2.2.1 (2005-12-20 r36812) on Ubuntu Linux.

Thanks,
Gad


-- 
Gad Abraham
Department of Mathematics and Statistics
University of Melbourne
Victoria 3010, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From nono.231 at gmail.com  Tue Jun 13 05:43:41 2006
From: nono.231 at gmail.com (No No)
Date: Tue, 13 Jun 2006 06:43:41 +0300
Subject: [R] Multiple lag.plots per page
In-Reply-To: <448E1FFA.6070601@ms.unimelb.edu.au>
References: <448E1FFA.6070601@ms.unimelb.edu.au>
Message-ID: <3ff92a550606122043i1439329eg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060613/0ca381f8/attachment.pl 

From nono.231 at gmail.com  Tue Jun 13 06:21:18 2006
From: nono.231 at gmail.com (No No)
Date: Tue, 13 Jun 2006 07:21:18 +0300
Subject: [R] Multiple lag.plots per page
In-Reply-To: <448E1FFA.6070601@ms.unimelb.edu.au>
References: <448E1FFA.6070601@ms.unimelb.edu.au>
Message-ID: <3ff92a550606122121r54464c84n@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060613/dbd89cad/attachment.pl 

From dushoff at eno.princeton.edu  Tue Jun 13 07:31:40 2006
From: dushoff at eno.princeton.edu (Jonathan Dushoff)
Date: Tue, 13 Jun 2006 01:31:40 -0400 (EDT)
Subject: [R] Updating R on an old Linux installation (was: Where is package
 gridBase?)
Message-ID: <Pine.LNX.4.61.0606130118230.3074@tahawus.Princeton.EDU>


I am running R 2.2.1 on a University-supported linux installation based
on Redhat EL3.  I am sorry that it did not occur to me to mention this
before; I updated R very recently, with the most recent version
available for EL3 at http://cran.cnr.berkeley.edu/bin/linux/redhat/el3/.

Looking at the gridBase documentation, I find that 2.2.1 is not in fact
the most recent version.  I have now spent hours trying to install 2.3.
No available binary rpms seem to work.  If I try the source code,
configure crashes with a complaint that I do not have X, which is false
(see below).

Do I need a new version of Linux?

Any help is appreciated.

Jonathan

----------------------------------------------------------------------
> ./configure

(lots of positive stuff)

checking for X... no
configure: error: --with-x=yes (default) and X11 headers/libs are not
available


---------- Forwarded message ----------
Date: Mon, 12 Jun 2006 21:27:02 -0400 (EDT)
From: Jonathan Dushoff <dushoff at eno.princeton.edu>
To: R-help at stat.math.ethz.ch
Subject: Where is package gridBase?

Trying to follow the instructions on the page
http://tolstoy.newcastle.edu.au/R/help/05/12/17153.html (to make a
figure with an inset), I typed

> install.packages("gridBase", repos="http://cran.r-project.org")

and was rejected.

 	no package 'gridBase' at the repositories in: download.packages(pkgs,
 	destdir = tmpd, available = available,

I went to the FAQ at
http://cran.r-project.org/doc/FAQ/R-FAQ.html#Add_002don-packages-from-CRAN,
and was informed that gridBase was an add-on package available from
CRAN.

I also tried installing other packages using exactly the same command.

> install.packages("TeachingDemos", repos="http://cran.r-project.org")

These packages were apparently found:
 	trying URL
 	'http://cran.r-project.org/src/contrib/TeachingDemos_1.2.tar.gz'
 	Content type 'application/x-tar' length 66472 bytes
 	opened URL

 	(and so on)

I would appreciate any insight that anyone might have.

Jonathan


From g.abraham at ms.unimelb.edu.au  Tue Jun 13 08:14:59 2006
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Tue, 13 Jun 2006 16:14:59 +1000
Subject: [R] Updating R on an old Linux installation (was: Where is
 package gridBase?)
In-Reply-To: <Pine.LNX.4.61.0606130118230.3074@tahawus.Princeton.EDU>
References: <Pine.LNX.4.61.0606130118230.3074@tahawus.Princeton.EDU>
Message-ID: <448E57E3.4080004@ms.unimelb.edu.au>

Jonathan Dushoff wrote:
> I am running R 2.2.1 on a University-supported linux installation based
> on Redhat EL3.  I am sorry that it did not occur to me to mention this
> before; I updated R very recently, with the most recent version
> available for EL3 at http://cran.cnr.berkeley.edu/bin/linux/redhat/el3/.
> 
> Looking at the gridBase documentation, I find that 2.2.1 is not in fact
> the most recent version.  I have now spent hours trying to install 2.3.
> No available binary rpms seem to work.  If I try the source code,
> configure crashes with a complaint that I do not have X, which is false
> (see below).
> 
> Do I need a new version of Linux?
> 
> Any help is appreciated.
> 
> Jonathan
> 
> ----------------------------------------------------------------------
>> ./configure
> 
> (lots of positive stuff)
> 
> checking for X... no
> configure: error: --with-x=yes (default) and X11 headers/libs are not
> available

You might have X but probably not the X development libraries/headers, 
which are in a separate package(s).

If RH EL3 still uses XFree86 and not X.org, then I think the main 
package is called XFree86-devel, and you'll probably need a few others, 
going by 
http://rpmfind.net//linux/RPM/redhat/8.0/i386/XFree86-devel-4.2.0-72.i386.html.

Cheers,
Gad

-- 
Gad Abraham
Department of Mathematics and Statistics
University of Melbourne
Victoria 3010, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From ripley at stats.ox.ac.uk  Tue Jun 13 08:34:02 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 13 Jun 2006 07:34:02 +0100 (BST)
Subject: [R] Updating R on an old Linux installation (was: Where is
 package gridBase?)
In-Reply-To: <Pine.LNX.4.61.0606130118230.3074@tahawus.Princeton.EDU>
References: <Pine.LNX.4.61.0606130118230.3074@tahawus.Princeton.EDU>
Message-ID: <Pine.LNX.4.64.0606130730030.24883@gannet.stats.ox.ac.uk>

There are older versions of gridBase suitable for 2.2.1 at CRAN, e.g.
src/contrib/Archive/gridBase_0.4-2.tar.gz.  Why not just install that?

On Tue, 13 Jun 2006, Jonathan Dushoff wrote:

>
> I am running R 2.2.1 on a University-supported linux installation based
> on Redhat EL3.  I am sorry that it did not occur to me to mention this
> before; I updated R very recently, with the most recent version
> available for EL3 at http://cran.cnr.berkeley.edu/bin/linux/redhat/el3/.
>
> Looking at the gridBase documentation, I find that 2.2.1 is not in fact
> the most recent version.  I have now spent hours trying to install 2.3.
> No available binary rpms seem to work.  If I try the source code,
> configure crashes with a complaint that I do not have X, which is false
> (see below).

That is not what it says, and very likely it is the X11 headers that are 
not installed.

>
> Do I need a new version of Linux?
>
> Any help is appreciated.
>
> Jonathan
>
> ----------------------------------------------------------------------
>> ./configure
>
> (lots of positive stuff)
>
> checking for X... no
> configure: error: --with-x=yes (default) and X11 headers/libs are not
> available
>
>
> ---------- Forwarded message ----------
> Date: Mon, 12 Jun 2006 21:27:02 -0400 (EDT)
> From: Jonathan Dushoff <dushoff at eno.princeton.edu>
> To: R-help at stat.math.ethz.ch
> Subject: Where is package gridBase?
>
> Trying to follow the instructions on the page
> http://tolstoy.newcastle.edu.au/R/help/05/12/17153.html (to make a
> figure with an inset), I typed
>
>> install.packages("gridBase", repos="http://cran.r-project.org")
>
> and was rejected.
>
> 	no package 'gridBase' at the repositories in: download.packages(pkgs,
> 	destdir = tmpd, available = available,
>
> I went to the FAQ at
> http://cran.r-project.org/doc/FAQ/R-FAQ.html#Add_002don-packages-from-CRAN,
> and was informed that gridBase was an add-on package available from
> CRAN.
>
> I also tried installing other packages using exactly the same command.
>
>> install.packages("TeachingDemos", repos="http://cran.r-project.org")
>
> These packages were apparently found:
> 	trying URL
> 	'http://cran.r-project.org/src/contrib/TeachingDemos_1.2.tar.gz'
> 	Content type 'application/x-tar' length 66472 bytes
> 	opened URL
>
> 	(and so on)
>
> I would appreciate any insight that anyone might have.
>
> Jonathan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From hpbenton at scripps.edu  Tue Jun 13 08:42:07 2006
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Mon, 12 Jun 2006 23:42:07 -0700
Subject: [R] plotting gaussian data
Message-ID: <20060613064212.LWKH18458.fed1rmmtao10.cox.net@hpb>

Ok I guess it's time to ask. 
So I want to plot my data. It's my data from a frequency table, "temp". My
formula is just a Gaussian eq. I have done the nls function to get my
parameters and now I want to do the whole plot (...) and then lines(..)
This is what I have done. 

> temp
    bin   x
1  -4.0   0
2  -3.9   0
3  -3.8   0
4  -3.7   0
5  -3.6   0
6  -3.5   0 .... and so on
> fo
x ~ (A/(sig * sqrt(2 * pi))) * exp(-1 * ((bin - mu)^2/(2 * sig^2)))
> fo.v
x ~ (335.48/(0.174 * sqrt(2 * pi))) * exp(-1 * ((bin - (-0.0786))^2/(2 * 
    0.174^2)))
> coef
$A
[1] 335.4812

$mu
[1] -0.07863746

$sig
[1] 0.1746473

plot(fo, temp, coef)
Error in eval(expr, envir, enclos) : object "sig" not found
> plot(fo, coef, temp)
Error in eval(expr, envir, enclos) : object "x" not found
> plot(fo, temp, list=coef)
>

If someone could point me in the right direction I would be very grateful. 

Cheers,


From g.abraham at ms.unimelb.edu.au  Tue Jun 13 08:54:39 2006
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Tue, 13 Jun 2006 16:54:39 +1000
Subject: [R] Multiple lag.plots per page
In-Reply-To: <3ff92a550606122121r54464c84n@mail.gmail.com>
References: <448E1FFA.6070601@ms.unimelb.edu.au>
	<3ff92a550606122121r54464c84n@mail.gmail.com>
Message-ID: <448E612F.5030004@ms.unimelb.edu.au>

No No wrote:
> Does this help?
>  > pdf(file="lag.pdf")
>  > lag.plot(a)
>  > lag.plot(b)
>  > dev.off()
> After that you can open each page of the "lag.pdf" file with GIMP for 
> further manipulation. It gives each plot on a different page, but no 
> plot is replaced.

That would be a last resort, but obviously it's not very practical if 
you have more than a handful of plots.

Thanks,
Gad

> 
> 2006/6/13, Gad Abraham <g.abraham at ms.unimelb.edu.au 
> <mailto:g.abraham at ms.unimelb.edu.au>>:
> 
>     Hi,
> 
>     I'm trying to plot several lag.plots on a page, however the second plot
>     replaces the first one (although it only takes up the upper half as it
>     should):
> 
>     par(mfrow=c(2,1))
>     a<-sin(1:100)
>     b<-cos(1:100)
>     lag.plot(a)
>     lag.plot(b)
> 
>     What's the trick to this?
> 
>     I'm using R 2.2.1 (2005-12-20 r36812) on Ubuntu Linux.
> 


-- 
Gad Abraham
Department of Mathematics and Statistics
University of Melbourne
Victoria 3010, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From ripley at stats.ox.ac.uk  Tue Jun 13 08:41:31 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 13 Jun 2006 07:41:31 +0100 (BST)
Subject: [R] Multiple lag.plots per page
In-Reply-To: <448E1FFA.6070601@ms.unimelb.edu.au>
References: <448E1FFA.6070601@ms.unimelb.edu.au>
Message-ID: <Pine.LNX.4.64.0606130736440.24883@gannet.stats.ox.ac.uk>

On Tue, 13 Jun 2006, Gad Abraham wrote:

> Hi,
>
> I'm trying to plot several lag.plots on a page, however the second plot
> replaces the first one (although it only takes up the upper half as it
> should):
>
> par(mfrow=c(2,1))
> a<-sin(1:100)
> b<-cos(1:100)
> lag.plot(a)
> lag.plot(b)
>
> What's the trick to this?

lag.plot itself calls par(mfrow).  The trick is to get one call to do the 
plots you want:

lag.plot(cbind(a,b))


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From g.abraham at ms.unimelb.edu.au  Tue Jun 13 09:15:52 2006
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Tue, 13 Jun 2006 17:15:52 +1000
Subject: [R] Multiple lag.plots per page
In-Reply-To: <Pine.LNX.4.64.0606130736440.24883@gannet.stats.ox.ac.uk>
References: <448E1FFA.6070601@ms.unimelb.edu.au>
	<Pine.LNX.4.64.0606130736440.24883@gannet.stats.ox.ac.uk>
Message-ID: <448E6628.5060907@ms.unimelb.edu.au>

Prof Brian Ripley wrote:
> On Tue, 13 Jun 2006, Gad Abraham wrote:
> 
>> Hi,
>>
>> I'm trying to plot several lag.plots on a page, however the second plot
>> replaces the first one (although it only takes up the upper half as it
>> should):
>>
>> par(mfrow=c(2,1))
>> a<-sin(1:100)
>> b<-cos(1:100)
>> lag.plot(a)
>> lag.plot(b)
>>
>> What's the trick to this?
> 
> lag.plot itself calls par(mfrow).  The trick is to get one call to do 
> the plots you want:
> 
> lag.plot(cbind(a,b))
> 
> 

Thanks, that works great for multiple lag.plots. Is it possible to have 
a lag.plot and another type of plot on the same page? The second plot() 
always replaces the lag.plot for me.

Cheers,
Gad

-- 
Gad Abraham
Department of Mathematics and Statistics
University of Melbourne
Victoria 3010, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From ripley at stats.ox.ac.uk  Tue Jun 13 09:50:48 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 13 Jun 2006 08:50:48 +0100 (BST)
Subject: [R] Multiple lag.plots per page
In-Reply-To: <448E6628.5060907@ms.unimelb.edu.au>
References: <448E1FFA.6070601@ms.unimelb.edu.au>
	<Pine.LNX.4.64.0606130736440.24883@gannet.stats.ox.ac.uk>
	<448E6628.5060907@ms.unimelb.edu.au>
Message-ID: <Pine.LNX.4.64.0606130847530.19239@gannet.stats.ox.ac.uk>

On Tue, 13 Jun 2006, Gad Abraham wrote:

> Prof Brian Ripley wrote:
>> On Tue, 13 Jun 2006, Gad Abraham wrote:
>> 
>>> Hi,
>>> 
>>> I'm trying to plot several lag.plots on a page, however the second plot
>>> replaces the first one (although it only takes up the upper half as it
>>> should):
>>> 
>>> par(mfrow=c(2,1))
>>> a<-sin(1:100)
>>> b<-cos(1:100)
>>> lag.plot(a)
>>> lag.plot(b)
>>> 
>>> What's the trick to this?
>> 
>> lag.plot itself calls par(mfrow).  The trick is to get one call to do the 
>> plots you want:
>> 
>> lag.plot(cbind(a,b))
>> 
>> 
>
> Thanks, that works great for multiple lag.plots. Is it possible to have a 
> lag.plot and another type of plot on the same page? The second plot() always 
> replaces the lag.plot for me.

Yes, if the other plot is second, e.g

par(mfrow=c(2,1))
a<-sin(1:100)
lag.plot(a)
par(mfg=c(2,1)) # move to second plot
plot(1:10)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From gabidiaz at gmail.com  Tue Jun 13 10:17:40 2006
From: gabidiaz at gmail.com (Gabriel Diaz)
Date: Tue, 13 Jun 2006 10:17:40 +0200
Subject: [R] R usage for log analysis
In-Reply-To: <8d5a36350606120738x6deb7722lb1f01a402686a834@mail.gmail.com>
References: <8d5a36350606120738x6deb7722lb1f01a402686a834@mail.gmail.com>
Message-ID: <82c890d00606130117icea7115s25a35ca97f2630fc@mail.gmail.com>

Hello

thanks for the point, probably i saw DBMS as a need due to my ignorance about R.

If i can process all files like you said, i would not use DBMS as i
prefer to keep it simple and easy to manage and run, less software
dependencies the better.

thanks

gabi


On 6/12/06, bogdan romocea <br44114 en gmail.com> wrote:
> I wouldn't use a DBMS at all -- it is not necessary and I don't see
> what you would get in return. Instead I would split very large log
> files into a number of pieces so that each piece fits in memory (see
> below for an example), then process them in a loop. See the list and
> the documentation if you have questions about how to read text files,
> count strings etc.
>
> #---split big files in two---
> for F in `ls *log`
> do
>   fn=`echo $F | awk -F\. '{print $1}'`
>   ln=`wc -l $F | awk '{print $1}'`  #number of lines in the file
>   forsplit=`expr $ln / 2 + 50`  #no. of lines in each chunk, tweak as needed
>   echo Splitting $F into pieces of $forsplit lines each........
>   split -l $forsplit $F $fn
> done
>
>
> > -----Original Message-----
> > From: r-help-bounces en stat.math.ethz.ch
> > [mailto:r-help-bounces en stat.math.ethz.ch] On Behalf Of Gabriel Diaz
> > Sent: Monday, June 12, 2006 9:52 AM
> > To: Jean-Luc Fontaine
> > Cc: r-help en stat.math.ethz.ch
> > Subject: Re: [R] R usage for log analysis
> >
> > Hello
> >
> > Thanks all for the answers.
> >
> > I'm taking an overview to the project documentation, and seems the
> > database is the way to go to handle log files of GB order (normally
> > between 2 and 4 GB each 15 day dump).
> >
> > In this document http://cran.r-project.org/doc/manuals/R-data.html,
> > says R will load all data into memory to process it when using
> > read.table and such. Using a database will do the same? Well,
> > currently i have no machine with > 2 GB of memory.
> >
> > The moodss thing looks nice, thanks for the link. But what i have to
> > do now is an offline analysis of big log files :-). I will try to go
> > with the mysql -> R way.
> >
> > gabi
> >
> >
> >
> > On 6/12/06, Jean-Luc Fontaine <jfontain en free.fr> wrote:
> > > -----BEGIN PGP SIGNED MESSAGE-----
> > > Hash: SHA1
> > >
> > > Allen S. Rout wrote:
> > > >
> > > >
> > > > Don't expect a warm welcome.  This community is like all
> > open-source
> > > > communities, sharply focused on its' own concerns and
> > expertise.  And,
> > > > in an unusual experience for computer types, our core competencies
> > > > hold little or no sway here; they don't even give us much
> > of a leg up.
> > > > Just wait 'till you want to do something nutso like
> > produce a business
> > > > graphic. :)
> > > >
> > > > I'm working on understanding enough of R packaging and
> > documentation
> > > > to begin a 'task view' focused on systems administration,
> > for humble
> > > > submission. That might end up being mostly "log
> > analysis"; the term
> > > > can describe much of what we do, if it's stretched a bit.
> >  I'm hoping
> > > > the task view will attract the teeming masses of
> > sysadmins trapped in
> > > > the mire of Gnuplot and friends.
> > > Although not specifically solving the problem at hand, you
> > might want
> > > to take a look at moodss and moomps
> > (http://moodss.sourceforge.net/),
> > > modular monitoring applications, which uses R
> > > (http://jfontain.free.fr/statistics.htm) and its log module
> > > (http://jfontain.free.fr/log/log.htm).
> > >
> > > - --
> > > Jean-Luc Fontaine  http://jfontain.free.fr/
> > > -----BEGIN PGP SIGNATURE-----
> > > Version: GnuPG v1.4.3 (GNU/Linux)
> > > Comment: Using GnuPG with Fedora - http://enigmail.mozdev.org
> > >
> > > iD8DBQFEjT2ykG/MMvcT1qQRAuF6AJ9nf5phV/GMmCHPuc5bVyA+SoXqGACgnLuZ
> > > u1tZpFOTCHNKOfFLZOC9uXI=
> > > =V8yo
> > > -----END PGP SIGNATURE-----
> > >
> > > ______________________________________________
> > > R-help en stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > >
> >
> > ______________________________________________
> > R-help en stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>


From j.van_den_hoff at fz-rossendorf.de  Tue Jun 13 09:29:56 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Tue, 13 Jun 2006 09:29:56 +0200
Subject: [R] plot two graphs with different length of data
In-Reply-To: <c3e96b080606121728s27d055c1g1a5ee788b9bd4c88@mail.gmail.com>
References: <c3e96b080606121728s27d055c1g1a5ee788b9bd4c88@mail.gmail.com>
Message-ID: <448E6974.4020706@fz-rossendorf.de>

Eric Hu wrote:
> Hi I am trying to plot two data set in the same picture window without
> overlapping with each other. I am using the format plot(x1,y1,x2,y2)
> but get the following error message:
> 
>> plot(as.numeric(r0[,4]),as.numeric(r0[,7]),as.numeric(r0[,4]),as.numeric(r0[,7][ind[,1]]))
> Error in plot.window(xlim, ylim, log, asp, ...) :
>         invalid 'ylim' value
> 
> Can anyone tell me what went wrong? Thanks.
> 
> Eric
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


matplot(cbind(x1,x2), cbind(y1,y2)) might be what you want.

(if x1, x2 and y1,y2 are of equal length. otherwise pad the short  ones 
with NAs or use  `matplot' with type =n (to get the scaling of the plot 
right), followed by `plot(x1,y1), lines(x2,y2)')


From celine_jouanin at yahoo.fr  Tue Jun 13 10:37:31 2006
From: celine_jouanin at yahoo.fr (Jouanin Celine)
Date: Tue, 13 Jun 2006 10:37:31 +0200 (CEST)
Subject: [R] Predict with loess
Message-ID: <20060613083731.62978.qmail@web27712.mail.ukl.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060613/98f7694a/attachment.pl 

From jacques.veslot at good.ibl.fr  Tue Jun 13 10:41:44 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Tue, 13 Jun 2006 10:41:44 +0200
Subject: [R] plotting gaussian data
In-Reply-To: <20060613064212.LWKH18458.fed1rmmtao10.cox.net@hpb>
References: <20060613064212.LWKH18458.fed1rmmtao10.cox.net@hpb>
Message-ID: <448E7A48.5080101@good.ibl.fr>

curve(coef$A * dnorm(x, coef$mu, coef$sig), -4, 4)

-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


H. Paul Benton a ?crit :
> Ok I guess it's time to ask. 
> So I want to plot my data. It's my data from a frequency table, "temp". My
> formula is just a Gaussian eq. I have done the nls function to get my
> parameters and now I want to do the whole plot (...) and then lines(..)
> This is what I have done. 
> 
> 
>>temp
> 
>     bin   x
> 1  -4.0   0
> 2  -3.9   0
> 3  -3.8   0
> 4  -3.7   0
> 5  -3.6   0
> 6  -3.5   0 .... and so on
> 
>>fo
> 
> x ~ (A/(sig * sqrt(2 * pi))) * exp(-1 * ((bin - mu)^2/(2 * sig^2)))
> 
>>fo.v
> 
> x ~ (335.48/(0.174 * sqrt(2 * pi))) * exp(-1 * ((bin - (-0.0786))^2/(2 * 
>     0.174^2)))
> 
>>coef
> 
> $A
> [1] 335.4812
> 
> $mu
> [1] -0.07863746
> 
> $sig
> [1] 0.1746473
> 
> plot(fo, temp, coef)
> Error in eval(expr, envir, enclos) : object "sig" not found
> 
>>plot(fo, coef, temp)
> 
> Error in eval(expr, envir, enclos) : object "x" not found
> 
>>plot(fo, temp, list=coef)
>>
> 
> 
> If someone could point me in the right direction I would be very grateful. 
> 
> Cheers,
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ripley at stats.ox.ac.uk  Tue Jun 13 10:58:44 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 13 Jun 2006 09:58:44 +0100 (BST)
Subject: [R] Predict with loess
In-Reply-To: <20060613083731.62978.qmail@web27712.mail.ukl.yahoo.com>
References: <20060613083731.62978.qmail@web27712.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.64.0606130947520.20684@gannet.stats.ox.ac.uk>

On Tue, 13 Jun 2006, Jouanin Celine wrote:

>    I want to do a  nonparametric regression. I?m using the function loess.
>      The variable are  the year from 1968 to 1977 and the dependant variable is  a proportion P. The dependant variable have  missing value (NA).
>      The script is :
>
>      year <- 1969:2002
>      length(year)
>      [1] 34
>
>      P <- c(NA,0.1,0.56,NA,NA,0.5,0.4,0.75,0.9,
>      0.98,0.2,0.56,0.7,0.89,0.3,0.1,0.45,0.46,0.49,0.78,
>      0.25,0.79,0.23,0.26,0.46,0.12,0.56,0.8,0.55,0.41,
>      0.36,0.9,0.22,0.1)
>      length(P)
>      [1] 34
>
>      lo1 <- loess(P~year,span=0.3,degree=1)
>      summary(lo1)
>      yearCo <- 1969:2002
>      year_lo <- data.frame(year = yearCo )
>      length(year_lo)
>      [1] 34

I get 1 here, and so should you.

>      mlo <- predict(loess(P~year,span=0.3,degree=1),new.data=year_lo,se=T)

It should be newdata, not new.data

>      mlo$fit
>      mlo$se.fit

Notice that these are of length 31, not 34

You are trying to predict at the values used for fitting (possibly not 
what you intended), so you don't actually need this.  Try

lo1 <- loess(P~year,span=0.3,degree=1, na.action=na.exclude)
fitted(lo1)
plot(year,P,type='o')
lines(year, fitted(lo1))

Or if you want to try interpolation

lines(year, predict(lo1, newdata=year_lo))

This will not extrapolate to 1969, and as far as I recall the version of 
loess in R does not allow extrapolation.

>      plot(year,P,type='o')
>      lines(year,predict(loess(P~year,span=0.15,degree=1),new.data=year_lo,
>      se=T,na.action=na.omit)$fit,col='blue',type='l')
>
>      The message error  indicates that x and y don?t have the same length.

>      In fact in m$fit and m$se.fit there are 3 values who don?t have a 
> fitted value.

Correct, and that's because you used na.action=na.omit and did not specify 
newdata.

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From arun.kumar.saha at gmail.com  Tue Jun 13 12:54:54 2006
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Tue, 13 Jun 2006 16:24:54 +0530
Subject: [R] Garch Warning
Message-ID: <d4c57560606130354h4d42bfc5x691457ae9d3536fd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060613/9c107679/attachment.pl 

From avilella at gmail.com  Tue Jun 13 13:11:13 2006
From: avilella at gmail.com (Albert Vilella)
Date: Tue, 13 Jun 2006 12:11:13 +0100
Subject: [R] bubbleplot for matrix
Message-ID: <1150197073.5830.13.camel@localhost>

Hi all,

I would like to ask if it is possible to use bubbleplot for a 20x20
matrix, instead of a dataframe with factors in columns.

The idea would be to get a tabular representation with bubbles like in
Rnews_2006_2 article, which look very nice.

Thanks in advance,

    Albert.


From h.y.wong at leeds.ac.uk  Tue Jun 13 13:27:48 2006
From: h.y.wong at leeds.ac.uk (Yan Wong)
Date: Tue, 13 Jun 2006 12:27:48 +0100
Subject: [R] Slight fault in error messages
Message-ID: <D216D825-4A9C-4B3A-9172-10A45D97E8B6@leeds.ac.uk>

Just a quick point which may be easy to correct. Whilst typing the  
wrong thing into R 2.2.1, I noticed the following error messages,  
which seem to have some stray quotation marks and commas in the list  
of available families. Perhaps they have been corrected in the latest  
version (sorry, I don't want to upgrade yet, but it should be easy to  
check)?

 > glm(1 ~ 2, family=quasibinomial(link=foo))
Error in quasibinomial(link = foo) : ?foo? link not available for  
quasibinomial family, available links are "logit", ", ""probit" and  
"cloglog"

 > glm(1 ~ 2, family=binomial(link=foo))
Error in binomial(link = foo) : link "foo" not available for binomial  
family, available links are "logit", ""probit", "cloglog", "cauchit"  
and "log"

I hope this is helpful,

Yan


From avilella at gmail.com  Tue Jun 13 13:45:06 2006
From: avilella at gmail.com (Albert Vilella)
Date: Tue, 13 Jun 2006 12:45:06 +0100
Subject: [R] sorting matrix elements by given rownames and colnames
Message-ID: <1150199106.5830.16.camel@localhost>

Hi all,

I would like to know if it is possible to sort the columns and rows in
a matrix given a specified order of their colnames and rownames. For
example:

I would like the original matrix:
  
     RNO  FRU   ANG  CEL  DAR  PTR
RNO 3.45 1.35  2.16 2.25 1.43 1.20
FRU 1.31  Inf  2.22 2.36 1.34 1.40
ANG 1.30 1.30 11.61 1.37 1.33 1.35
CEL 1.34 1.35  1.37  Inf 1.37 1.40
DAR 1.40 1.29  2.29 2.39 9.62 1.48
PTR 1.08 1.30  2.06 2.17 1.39 8.17

To be sorted in a top/down and left/right way in this order
instead: "FRU","ANG","CEL","PTR","RNO","DAR".

Thanks in advance,

    Albert.


From dlalountas at yahoo.com  Tue Jun 13 13:46:22 2006
From: dlalountas at yahoo.com (denis lalountas)
Date: Tue, 13 Jun 2006 04:46:22 -0700 (PDT)
Subject: [R] COX FRAILTY
Message-ID: <20060613114622.27636.qmail@web54709.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060613/4643cd54/attachment.pl 

From ccleland at optonline.net  Tue Jun 13 13:54:27 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 13 Jun 2006 07:54:27 -0400
Subject: [R] sorting matrix elements by given rownames and colnames
In-Reply-To: <1150199106.5830.16.camel@localhost>
References: <1150199106.5830.16.camel@localhost>
Message-ID: <448EA773.5080207@optonline.net>

Albert Vilella wrote:
> Hi all,
> 
> I would like to know if it is possible to sort the columns and rows in
> a matrix given a specified order of their colnames and rownames. For
> example:
> 
> I would like the original matrix:
>   
>      RNO  FRU   ANG  CEL  DAR  PTR
> RNO 3.45 1.35  2.16 2.25 1.43 1.20
> FRU 1.31  Inf  2.22 2.36 1.34 1.40
> ANG 1.30 1.30 11.61 1.37 1.33 1.35
> CEL 1.34 1.35  1.37  Inf 1.37 1.40
> DAR 1.40 1.29  2.29 2.39 9.62 1.48
> PTR 1.08 1.30  2.06 2.17 1.39 8.17
> 
> To be sorted in a top/down and left/right way in this order
> instead: "FRU","ANG","CEL","PTR","RNO","DAR".

X <- round(matrix(rnorm(36), ncol=6), 2)

colnames(X) <- c("RNO", "FRU", "ANG", "CEL", "DAR", "PTR")

rownames(X) <- c("RNO", "FRU", "ANG", "CEL", "DAR", "PTR")

myord <- c("FRU", "ANG", "CEL", "PTR", "RNO", "DAR")

X
       RNO   FRU   ANG   CEL   DAR   PTR
RNO -0.99  0.03 -0.78 -0.55  0.01 -0.64
FRU -0.50 -0.67  2.05  0.28  0.86  0.20
ANG -0.87  0.48 -0.26  0.00 -0.39 -0.18
CEL -0.71  0.55  1.32  0.36 -0.53 -0.53
DAR  0.92 -1.28  0.92 -0.19  0.56 -0.86
PTR -0.60 -0.95 -0.17 -1.23  0.41 -0.36

X[myord,myord]
       FRU   ANG   CEL   PTR   RNO   DAR
FRU -0.67  2.05  0.28  0.20 -0.50  0.86
ANG  0.48 -0.26  0.00 -0.18 -0.87 -0.39
CEL  0.55  1.32  0.36 -0.53 -0.71 -0.53
PTR -0.95 -0.17 -1.23 -0.36 -0.60  0.41
RNO  0.03 -0.78 -0.55 -0.64 -0.99  0.01
DAR -1.28  0.92 -0.19 -0.86  0.92  0.56

> Thanks in advance,
> 
>     Albert.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From nvj at fys.ku.dk  Tue Jun 13 14:04:56 2006
From: nvj at fys.ku.dk (Niels Vestergaard Jensen)
Date: Tue, 13 Jun 2006 14:04:56 +0200 (CEST)
Subject: [R] levelplot and source() problems
Message-ID: <Pine.LNX.4.44.0606131343001.5160-100000@scharff.fys.ku.dk>

I have been using levelplot but have had trouble calling it inside
functions - something seems to go wrong when it's not called directly from
the R command prompt. Simplest reproducible example:

$ R --vanilla
> library(lattice)
> levelplot(matrix(1:4,2,2))

- This gives a nice plot in soothing pastel colors.

Now, with a file lptest.r containing 2 lines:

library(lattice)
levelplot(matrix(11:14,2,2))

> source("lptest.r")

Gives nothing.

I've tried closing all devices and setting it with X11() and pdf().

Am I missing something here or should I file a bug report? I'm running R
Version 2.2.1 (2005-12-20 r36812) on Red Hat, lattice library v. 0.12-11

With the (more complex) example where I ran into this barrier I also
debugged the function calling levelplot. I could call levelplot from the
browser with nice results, but when the function executed the exact same
command in the next moment nothing happened.

I've pasted the output of debug(levelplot) when sourcing lptest.r below,
it's very much like what went on in the more complex example.

best

	Niels


Debugging levelplot:


> library(lattice)
> debug(levelplot)
> source("lptest.r")
[1] "test"
debugging in: levelplot(matrix(11:14, 2, 2))
debug: {
    ocall <- match.call()
    formula <- ocall$formula
    if (!is.null(formula)) {
        warning("The 'formula' argument has been renamed to 'x'. See
?xyplot")
        ocall$formula <- NULL
        if (!("x" %in% names(ocall)))
            ocall$x <- formula
        else warning("'formula' overridden by 'x'")
        eval(ocall, parent.frame())
    }
    else UseMethod("levelplot")
}
Browse[1]>
debug: ocall <- match.call()
Browse[1]>
debug: formula <- ocall$formula
Browse[1]>
debug: if (!is.null(formula)) {
    warning("The 'formula' argument has been renamed to 'x'. See ?xyplot")
    ocall$formula <- NULL
    if (!("x" %in% names(ocall)))
        ocall$x <- formula
    else warning("'formula' overridden by 'x'")
    eval(ocall, parent.frame())
} else UseMethod("levelplot")
Browse[1]>
debugging in: levelplot(form, data, aspect = aspect, ...)
debug: {
    ocall <- match.call()
    formula <- ocall$formula
    if (!is.null(formula)) {
        warning("The 'formula' argument has been renamed to 'x'. See
?xyplot")
        ocall$formula <- NULL
        if (!("x" %in% names(ocall)))
            ocall$x <- formula
        else warning("'formula' overridden by 'x'")
        eval(ocall, parent.frame())
    }
    else UseMethod("levelplot")
}
Browse[1]>
debug: ocall <- match.call()
Browse[1]>
debug: formula <- ocall$formula
Browse[1]>
debug: if (!is.null(formula)) {
    warning("The 'formula' argument has been renamed to 'x'. See ?xyplot")
    ocall$formula <- NULL
    if (!("x" %in% names(ocall)))
        ocall$x <- formula
    else warning("'formula' overridden by 'x'")
    eval(ocall, parent.frame())
} else UseMethod("levelplot")
Browse[1]>
exiting from: levelplot(form, data, aspect = aspect, ...)
exiting from: levelplot(matrix(11:14, 2, 2))
>


From ggrothendieck at gmail.com  Tue Jun 13 14:19:12 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 13 Jun 2006 08:19:12 -0400
Subject: [R] levelplot and source() problems
In-Reply-To: <Pine.LNX.4.44.0606131343001.5160-100000@scharff.fys.ku.dk>
References: <Pine.LNX.4.44.0606131343001.5160-100000@scharff.fys.ku.dk>
Message-ID: <971536df0606130519x56cf61ccr6d0b4824bfc36471@mail.gmail.com>

This is 7.22 of the R FAQ:

http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-do-lattice_002ftrellis-graphics-not-work_003f

On 6/13/06, Niels Vestergaard Jensen <nvj at fys.ku.dk> wrote:
> I have been using levelplot but have had trouble calling it inside
> functions - something seems to go wrong when it's not called directly from
> the R command prompt. Simplest reproducible example:
>
> $ R --vanilla
> > library(lattice)
> > levelplot(matrix(1:4,2,2))
>
> - This gives a nice plot in soothing pastel colors.
>
> Now, with a file lptest.r containing 2 lines:
>
> library(lattice)
> levelplot(matrix(11:14,2,2))
>
> > source("lptest.r")
>
> Gives nothing.
>
> I've tried closing all devices and setting it with X11() and pdf().
>
> Am I missing something here or should I file a bug report? I'm running R
> Version 2.2.1 (2005-12-20 r36812) on Red Hat, lattice library v. 0.12-11
>
> With the (more complex) example where I ran into this barrier I also
> debugged the function calling levelplot. I could call levelplot from the
> browser with nice results, but when the function executed the exact same
> command in the next moment nothing happened.
>
> I've pasted the output of debug(levelplot) when sourcing lptest.r below,
> it's very much like what went on in the more complex example.
>
> best
>
>        Niels
>
>
> Debugging levelplot:
>
>
> > library(lattice)
> > debug(levelplot)
> > source("lptest.r")
> [1] "test"
> debugging in: levelplot(matrix(11:14, 2, 2))
> debug: {
>    ocall <- match.call()
>    formula <- ocall$formula
>    if (!is.null(formula)) {
>        warning("The 'formula' argument has been renamed to 'x'. See
> ?xyplot")
>        ocall$formula <- NULL
>        if (!("x" %in% names(ocall)))
>            ocall$x <- formula
>        else warning("'formula' overridden by 'x'")
>        eval(ocall, parent.frame())
>    }
>    else UseMethod("levelplot")
> }
> Browse[1]>
> debug: ocall <- match.call()
> Browse[1]>
> debug: formula <- ocall$formula
> Browse[1]>
> debug: if (!is.null(formula)) {
>    warning("The 'formula' argument has been renamed to 'x'. See ?xyplot")
>    ocall$formula <- NULL
>    if (!("x" %in% names(ocall)))
>        ocall$x <- formula
>    else warning("'formula' overridden by 'x'")
>    eval(ocall, parent.frame())
> } else UseMethod("levelplot")
> Browse[1]>
> debugging in: levelplot(form, data, aspect = aspect, ...)
> debug: {
>    ocall <- match.call()
>    formula <- ocall$formula
>    if (!is.null(formula)) {
>        warning("The 'formula' argument has been renamed to 'x'. See
> ?xyplot")
>        ocall$formula <- NULL
>        if (!("x" %in% names(ocall)))
>            ocall$x <- formula
>        else warning("'formula' overridden by 'x'")
>        eval(ocall, parent.frame())
>    }
>    else UseMethod("levelplot")
> }
> Browse[1]>
> debug: ocall <- match.call()
> Browse[1]>
> debug: formula <- ocall$formula
> Browse[1]>
> debug: if (!is.null(formula)) {
>    warning("The 'formula' argument has been renamed to 'x'. See ?xyplot")
>    ocall$formula <- NULL
>    if (!("x" %in% names(ocall)))
>        ocall$x <- formula
>    else warning("'formula' overridden by 'x'")
>    eval(ocall, parent.frame())
> } else UseMethod("levelplot")
> Browse[1]>
> exiting from: levelplot(form, data, aspect = aspect, ...)
> exiting from: levelplot(matrix(11:14, 2, 2))
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ripley at stats.ox.ac.uk  Tue Jun 13 14:45:30 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 13 Jun 2006 13:45:30 +0100 (BST)
Subject: [R] levelplot and source() problems
In-Reply-To: <Pine.LNX.4.44.0606131343001.5160-100000@scharff.fys.ku.dk>
References: <Pine.LNX.4.44.0606131343001.5160-100000@scharff.fys.ku.dk>
Message-ID: <Pine.LNX.4.64.0606131343440.30395@gannet.stats.ox.ac.uk>

On Tue, 13 Jun 2006, Niels Vestergaard Jensen wrote:

> I have been using levelplot but have had trouble calling it inside
> functions - something seems to go wrong when it's not called directly from
> the R command prompt. Simplest reproducible example:
>
> $ R --vanilla
>> library(lattice)
>> levelplot(matrix(1:4,2,2))
>
> - This gives a nice plot in soothing pastel colors.
>
> Now, with a file lptest.r containing 2 lines:
>
> library(lattice)
> levelplot(matrix(11:14,2,2))
>
>> source("lptest.r")
>
> Gives nothing.
>
> I've tried closing all devices and setting it with X11() and pdf().
>
> Am I missing something here or should I file a bug report? I'm running R
> Version 2.2.1 (2005-12-20 r36812) on Red Hat, lattice library v. 0.12-11

See R FAQ Q7.22.  (Please don't file a bug report on an FAQ using an old 
version of R: the posting guide did instruct you to upgrade before 
posting.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From dhajage at gmail.com  Tue Jun 13 15:08:04 2006
From: dhajage at gmail.com (David Hajage)
Date: Tue, 13 Jun 2006 15:08:04 +0200
Subject: [R] problem with write.table
Message-ID: <a725cda30606130608q652f913i523805c571f20b9e@mail.gmail.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060613/2a3328d6/attachment.pl 

From kwright68 at gmail.com  Tue Jun 13 15:37:14 2006
From: kwright68 at gmail.com (Kevin Wright)
Date: Tue, 13 Jun 2006 08:37:14 -0500
Subject: [R] DSC 2005 proceedings?
Message-ID: <adf71a630606130637x140a04f7t99e16ee659ac309b@mail.gmail.com>

This page:
http://depts.washington.edu/dsc2005/
says the proceedings for DSC 2005 will be published online and that
the papers were due in final form by September 2005.

Anyone know if the proceedings have been published yet or when (if?)
they will be published?

Thanks.

Kevin Wright


From njohnson at ebi.ac.uk  Tue Jun 13 16:57:02 2006
From: njohnson at ebi.ac.uk (Nathan Johnson)
Date: Tue, 13 Jun 2006 15:57:02 +0100
Subject: [R] Numerical print format and loading with RMySQL
Message-ID: <D220A369-BF7C-44E4-B839-D96FC06CE801@ebi.ac.uk>

Hi

I'm new to R and have been resisting posting to the list thus far,  
but I think I've exhausted my work around options.

I'm having a few problems with RMySQL and/or it's underlying packages.

I'm doing a very simple vsn transformation on large datasets, using  
RMySQL to retrieve the data for a DB.

My first problem was that I could not get the RMySQL package to write  
back to my DB using the dbWriteTable method.

I constructed a data frame with columns corresponding to the table  
fields, and tried with and without a null column for the auto- 
increment internal id column.  I also tried renaming all the column  
names to match those of the table fields.  All of my attempts  
resulted in a long wait and "TRUE" being printed to the terminal,  
which I though was quite promising, however, on inspection of the DB,  
I found nothing had been written.  So my first question are:

Is it necessary to provide field names of the table, or will correct  
column order suffice?
For tables with an internal auto-increment DBID field, is it  
necessary to provide a null column of the correct length?

To get around this I decided to use the write.table function and then  
import via my controlling perl script.  This works fine apart from  
one problem, numbers >-100000 get printed as exponents, which then  
causes subsequent imports to miss these records. So my second and  
rather simple question is:

Is there a setting(options?) which will force full numerical printing?

Thanks in advance

Nath


From ripley at stats.ox.ac.uk  Tue Jun 13 18:02:05 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 13 Jun 2006 17:02:05 +0100 (BST)
Subject: [R] Numerical print format and loading with RMySQL
In-Reply-To: <D220A369-BF7C-44E4-B839-D96FC06CE801@ebi.ac.uk>
References: <D220A369-BF7C-44E4-B839-D96FC06CE801@ebi.ac.uk>
Message-ID: <Pine.LNX.4.64.0606131659480.6799@gannet.stats.ox.ac.uk>

On Tue, 13 Jun 2006, Nathan Johnson wrote:

> I'm new to R and have been resisting posting to the list thus far,
> but I think I've exhausted my work around options.

Have you asked the package maintainer: the posting guide suggests you do 
so first?  He is by far the most likely source of help.

> I'm having a few problems with RMySQL and/or it's underlying packages.

Which is DBI by the same author.

> I'm doing a very simple vsn transformation on large datasets, using
> RMySQL to retrieve the data for a DB.
>
> My first problem was that I could not get the RMySQL package to write
> back to my DB using the dbWriteTable method.
>
> I constructed a data frame with columns corresponding to the table
> fields, and tried with and without a null column for the auto-
> increment internal id column.  I also tried renaming all the column
> names to match those of the table fields.  All of my attempts
> resulted in a long wait and "TRUE" being printed to the terminal,
> which I though was quite promising, however, on inspection of the DB,
> I found nothing had been written.  So my first question are:
>
> Is it necessary to provide field names of the table, or will correct
> column order suffice?
> For tables with an internal auto-increment DBID field, is it
> necessary to provide a null column of the correct length?
>
> To get around this I decided to use the write.table function and then
> import via my controlling perl script.  This works fine apart from
> one problem, numbers >-100000 get printed as exponents, which then
> causes subsequent imports to miss these records. So my second and
> rather simple question is:
>
> Is there a setting(options?) which will force full numerical printing?

Look for options scipen.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From JMIKHALEVSKY-PERE at ADI-GESTION.COM  Tue Jun 13 18:41:23 2006
From: JMIKHALEVSKY-PERE at ADI-GESTION.COM (Julie MIKHALEVSKY-PERE)
Date: Tue, 13 Jun 2006 18:41:23 +0200
Subject: [R] rqss.object
Message-ID: <OF8283454D.0AE43B2A-ONC125718C.005B37FD-C125718C.005BC259@adi-gestion.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060613/a95b1d93/attachment.pl 

From elvis at xlsolutions-corp.com  Tue Jun 13 19:51:04 2006
From: elvis at xlsolutions-corp.com (elvis at xlsolutions-corp.com)
Date: Tue, 13 Jun 2006 10:51:04 -0700
Subject: [R] 2 Courses Near You - (1) Introduction to R/S+ programming:
	Microarrays Analysis and Bioconductor,
	(2) R/Splus Fundamentals and Programming Techniques
Message-ID: <20060613105104.9f08cc34deb45d78e54b3b5664e21546.5c0fde06e6.wbe@email.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to announce:

(1) Introduction to R/S+ programming: Microarrays Analysis and Bioconductor 
 
                  *** San Francisco / July 17-18, 2006 ***
		  *** Chicago       / July 24-25, 2006 ***
                  *** Baltimore     / July 27-28, 2006 *** 
                  *** Raleigh       / July 17-18, 2006 ***
                  *** Boston        / July 27-28, 2006 ***
              http://www.xlsolutions-corp.com/RSmicro

(2) R/Splus Fundamentals and Programming Techniques

                  *** San Francisco / July 10-11, 2006 ***
                  *** Houston       / July 13-14, 2006 ***
                  *** San Diego     / July 17-18, 2006 ***
                  *** Chicago       / July 20-21, 2006 ***
                  *** New York City / July 24-25, 2006 ***
                  *** Boston        / July 27-28, 2006 ***
 
              http://www.xlsolutions-corp.com/Rfund.htm  

Ask for group discount and reserve your seat Now - Earlybird Rates.
Payment due after the class! Email Sue Turner:  sue at xlsolutions-corp.com
Interested in our Advanced Programming class? 

(1) Introduction to R/S+ programming: Microarrays Analysis and Bioconductor   
 
Course Outline:

- R/S System: Overview; Installation and Demonstration 
- Data Manipulation and Management 
- Graphics; Enhancing Plots, Trellis 
- Writing Functions 
- Connecting to External Software 
- R/S Packages and Libraries (e.g. BioConductor) 
- BioConductor: Overview; Installation and Demonstration 
- Array Quality Inspection 
- Correction and Normalization; Affymetrix and cDNA arrays 
- Identification of Differentially Expressed Genes 
- Visualization of Genomic Information 
- Clustering Methods in R/Splus 
- Gene Ontology (GO) and Pathway Analysis 
- Inference, Strategies for Large Data 



(2) R/Splus Fundamentals and Programming Techniques
                       
Course outline.

- An Overview of R and S
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)
- Connecting; ODBC, Rweb, Orca via sockets and via Rjava

Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this
class to take advantage of group discount. Register now to secure your
seat!

Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-

2 Courses - (1) Introduction to R/S+ programming: Microarrays Analysis and Bioconductor 
            (2) R/Splus Fundamentals and Programming Techniques
             Interest in our R/Splus Advanced Programming?  Email us for upcoming courses.


From hpbenton at scripps.edu  Tue Jun 13 20:18:00 2006
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Tue, 13 Jun 2006 11:18:00 -0700
Subject: [R] if syntax error :(
Message-ID: <200606131818.k5DII5s29965@dns1.scripps.edu>

Umm sorry to bother everyone again but I'm having trouble with my if
statement. I come from a perl background so that's probably my problem! :)
So here is my code:

if (any(lgAB>4) | any(lgAB<-4)){
	freq_AB<-hist(lgAB, type="o", plot=F)
	else
	freq_AB<-hist(lgAB, breaks=br,type ="o", plot=F)
}

And I get 
> source("E:/R/GMDA-1.1.R")
Error in parse(file, n = -1, NULL, "?") : syntax error at
11:             freq_AB<-hist(lgAB, type="o", plot=F)
12:             else
>

Thanks again,


From sarah.goslee at gmail.com  Tue Jun 13 20:39:46 2006
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 13 Jun 2006 14:39:46 -0400
Subject: [R] if syntax error :(
In-Reply-To: <200606131818.k5DII5s29965@dns1.scripps.edu>
References: <200606131818.k5DII5s29965@dns1.scripps.edu>
Message-ID: <efb536d50606131139h702db647hafed059d4dd3f723@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060613/61af1528/attachment.pl 

From csardi at rmki.kfki.hu  Tue Jun 13 20:39:59 2006
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Tue, 13 Jun 2006 14:39:59 -0400
Subject: [R] if syntax error :(
In-Reply-To: <200606131818.k5DII5s29965@dns1.scripps.edu>
References: <200606131818.k5DII5s29965@dns1.scripps.edu>
Message-ID: <20060613183959.GF2000@localdomain>

replace 'else' with '} else {'

Gabor

On Tue, Jun 13, 2006 at 11:18:00AM -0700, H. Paul Benton wrote:
> Umm sorry to bother everyone again but I'm having trouble with my if
> statement. I come from a perl background so that's probably my problem! :)
> So here is my code:
> 
> if (any(lgAB>4) | any(lgAB<-4)){
> 	freq_AB<-hist(lgAB, type="o", plot=F)
> 	else
> 	freq_AB<-hist(lgAB, breaks=br,type ="o", plot=F)
> }
> 
> And I get 
> > source("E:/R/GMDA-1.1.R")
> Error in parse(file, n = -1, NULL, "?") : syntax error at
> 11:             freq_AB<-hist(lgAB, type="o", plot=F)
> 12:             else
> >
> 
> Thanks again,
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From ggrothendieck at gmail.com  Tue Jun 13 20:42:35 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 13 Jun 2006 14:42:35 -0400
Subject: [R] if syntax error :(
In-Reply-To: <efb536d50606131139h702db647hafed059d4dd3f723@mail.gmail.com>
References: <200606131818.k5DII5s29965@dns1.scripps.edu>
	<efb536d50606131139h702db647hafed059d4dd3f723@mail.gmail.com>
Message-ID: <971536df0606131142k5117125yaf21a750143080ee@mail.gmail.com>

On 6/13/06, Sarah Goslee <sarah.goslee at gmail.com> wrote:
> You need more brackets:
>
> if(blah) {
>  do something
> } else {
>  do something different
> }
>
> Sarah
>
> PS Using underscores in variable names is not encouraged, and
> can cause you problems in certain contexts.

The only context I can think of is very old versions of R that used
underscore as a synonym for <- so in any sufficiently recent
version of R I don't think it can cause problems.  Whether its
good style or not is another question.

>
> On 6/13/06, H. Paul Benton <hpbenton at scripps.edu> wrote:
> >
> > Umm sorry to bother everyone again but I'm having trouble with my if
> > statement. I come from a perl background so that's probably my problem! :)
> > So here is my code:
> >
> > if (any(lgAB>4) | any(lgAB<-4)){
> >         freq_AB<-hist(lgAB, type="o", plot=F)
> >         else
> >         freq_AB<-hist(lgAB, breaks=br,type ="o", plot=F)
> > }
> >
>
>
> --
> Sarah Goslee
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From sundar.dorai-raj at pdf.com  Tue Jun 13 20:51:33 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 13 Jun 2006 13:51:33 -0500
Subject: [R] if syntax error :(
In-Reply-To: <efb536d50606131139h702db647hafed059d4dd3f723@mail.gmail.com>
References: <200606131818.k5DII5s29965@dns1.scripps.edu>
	<efb536d50606131139h702db647hafed059d4dd3f723@mail.gmail.com>
Message-ID: <448F0935.2010409@pdf.com>



Sarah Goslee wrote:
> You need more brackets:
> 
> if(blah) {
>   do something
> } else {
>   do something different
> }
> 
> Sarah
> 
> PS Using underscores in variable names is not encouraged, and
> can cause you problems in certain contexts.
> 

Hi, Sarah,

Why do you say this? And in what situations will R have problems? The 
only problem I can see is using current code (with underscores in 
variable names) on extremely old versions of R. I've been using R for 6 
years now and applauded the day R-core decided to do away with 
underscore as assignment (a relic of S) and allow it in variable names. 
The one minor difficulty of using underscores of which I am aware is 
having to press `_' twice in ESS.

Thanks,

--sundar

> On 6/13/06, H. Paul Benton <hpbenton at scripps.edu> wrote:
> 
>>Umm sorry to bother everyone again but I'm having trouble with my if
>>statement. I come from a perl background so that's probably my problem! :)
>>So here is my code:
>>
>>if (any(lgAB>4) | any(lgAB<-4)){
>>        freq_AB<-hist(lgAB, type="o", plot=F)
>>        else
>>        freq_AB<-hist(lgAB, breaks=br,type ="o", plot=F)
>>}
>>
> 
> 
>


From sarah.goslee at gmail.com  Tue Jun 13 21:03:25 2006
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 13 Jun 2006 15:03:25 -0400
Subject: [R] underscores
Message-ID: <efb536d50606131203x2f659f3v3beb454e11ce33d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060613/d664580a/attachment.pl 

From peterdlauren at yahoo.com  Tue Jun 13 21:19:42 2006
From: peterdlauren at yahoo.com (Peter Lauren)
Date: Tue, 13 Jun 2006 12:19:42 -0700 (PDT)
Subject: [R] missing value where TRUE/FALSE needed
Message-ID: <20060613191942.12611.qmail@web30302.mail.mud.yahoo.com>

I am using R 2.3.0 and trying to run the following
code.

	BinCnt=round(ShorterVectorLength/10)
	BinMatrix=matrix(nrow=BinCnt, ncol=2)
	Increment=(VectorRange[2]-VectorRange[1])/BinCnt
	BinMatrix[1,1]=VectorRange[1]
	BinMatrix[1,2]=VectorRange[1]+Increment
	for (i in 2:BinCnt)
	{
		BinMatrix[i,1]=BinMatrix[i-1,2]
		BinMatrix[i,2]=BinMatrix[i,2]+Increment
	}
	
	# Count entries in each bin
	Histogram=vector(length=BinCnt, mode="numeric")
	for (i in 1:BinCnt) Histogram[i]=0
	VectorLength=length(FormerVector)
	for (i in 1:VectorLength)
	{
		Value=FormerVector[i]
		for (j in 1:BinCnt) if (Value>=BinMatrix[j,1] &&
Value<=BinMatrix[j,2])
		{
			Histogram[j]=Histogram[j]+1
			break;
		}
	}

Unfotunately, at 
if (Value>=BinMatrix[j,1] && Value<=BinMatrix[j,2])
I get the following error message
Error in if (Value >= BinMatrix[j, 1] && Value <=
BinMatrix[j, 2]) { : 
        missing value where TRUE/FALSE needed
I inserted browser() just before the call and 
(Value>=BinMatrix[j,1] && Value<=BinMatrix[j,2])
returned FALSE.

Is there a bug in my code or a bug in my version of R?

Many thanks in advance,
Peter.


From rpowell at horizonenv.com  Tue Jun 13 21:25:55 2006
From: rpowell at horizonenv.com (Robert Powell)
Date: Tue, 13 Jun 2006 15:25:55 -0400
Subject: [R] Cramer-von Mises normality test
Message-ID: <885B6BC6-39C0-4CEC-8734-D5E2F56C07F7@horizonenv.com>

Hi, this is my first help request so please bear with me.

I've been running some normality tests using the nortest package. For  
some of my datasets the Cramer-von Mises normality test generates an  
extremely high probability (e.g., 1.637e+31) and indicates normality  
when the other tests do not. Is there something I'm misunderstanding  
or potentially a bug in the code?

Below are the results of the tests and then, below that, I've  
provided that column of the datafile.

Thank you in advance for your help.

Bob Powell

 > shapiro.test(Mn.Total..ug.kg)

	Shapiro-Wilk normality test

data:  Mn.Total..ug.kg
W = 0.5269, p-value < 2.2e-16

 > shapiro.test(ln.Mn.Total..ug/kg)
Error in inherits(x, "factor") : object "ln.Mn.Total..ug" not found
 > shapiro.test(ln.Mn.Total..ug.kg)

	Shapiro-Wilk normality test

data:  ln.Mn.Total..ug.kg
W = 0.943, p-value = 1.301e-06

 > sf.test(Mn.Total..ug.kg)

	Shapiro-Francia normality test

data:  Mn.Total..ug.kg
W = 0.5173, p-value < 2.2e-16

 > sf.test(ln.Mn.Total..ug.kg)

	Shapiro-Francia normality test

data:  ln.Mn.Total..ug.kg
W = 0.9403, p-value = 3.461e-06

 > cvm.test(Mn.Total..ug.kg)

	Cramer-von Mises normality test

data:  Mn.Total..ug.kg
W = 4.024, p-value = 1.637e+31

 > cvm.test(ln.Mn.Total..ug.kg)

	Cramer-von Mises normality test

data:  ln.Mn.Total..ug.kg
W = 0.356, p-value = 7.647e-05

 > lillie.test(Mn.Total..ug.kg)

	Lilliefors (Kolmogorov-Smirnov) normality test

data:  Mn.Total..ug.kg
D = 0.2325, p-value < 2.2e-16

 > lillie.test(ln.Mn.Total..ug.kg)

	Lilliefors (Kolmogorov-Smirnov) normality test

data:  ln.Mn.Total..ug.kg
D = 0.0869, p-value = 0.002011


OK, next the data column:

Mn-Total, ug/kg
460000
400000
510000
600000
770000
180000
210000
1000000
2600000
490000
260000
760000
840000
400000
430000
230000
660000
1200000
370000
230000
290000
320000
400000
660000
620000
1100000
350000
450000
880000
960000
570000
870000
820000
970000
1100000
450000
730000
390000
640000
380000
340000
1400000
870000
260000
430000
290000
290000
550000
540000
240000
470000
650000
390000
380000
410000
220000
400000
380000
500000
1000000
520000
690000
500000
520000
260000
630000
630000
410000
290000
1300000
860000
600000
450000
330000
390000
580000
270000
460000
360000
300000
3600000
540000
370000
460000
150000
560000
630000
810000
1300000
470000
620000
520000
540000
440000
1900000
760000
750000
440000
550000
3300000
780000
3600000
1400000
840000
450000
970000
610000
490000
280000
960000
350000
1100000
770000
510000
460000
590000
760000
610000
550000
460000
470000
650000
370000
170000
630000
530000
720000
850000
570000
860000
6000000
1700000
1400000
470000
1200000
360000
370000
1100000
310000
430000
480000
670000
350000
530000
380000
490000
490000
380000
630000
370000
710000
1500000
380000
550000
560000
560000
350000
340000
250000
390000
670000
720000
440000
380000
240000
340000
350000
310000
250000
320000
360000
360000
490000
490000
490000
710000
670000
1200000
530000
500000
490000


From ligges at statistik.uni-dortmund.de  Tue Jun 13 21:34:25 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 13 Jun 2006 21:34:25 +0200
Subject: [R] 2 Courses Near You - (1) Introduction to R/S+ programming:
 Microarrays Analysis and Bioconductor,
 (2) R/Splus Fundamentals and Programming Techniques
In-Reply-To: <20060613105104.9f08cc34deb45d78e54b3b5664e21546.5c0fde06e6.wbe@email.secureserver.net>
References: <20060613105104.9f08cc34deb45d78e54b3b5664e21546.5c0fde06e6.wbe@email.secureserver.net>
Message-ID: <448F1341.1050602@statistik.uni-dortmund.de>

... and again I wonder which courses are "near". This leads at once to 
the question: "which metric is in use?". Probably some football related 
metric: FIFA WM takes place in Dortmund and commercials say something 
like "the world is our guest" ...
Now, let's escape from football to Austria and Vienna's useR!2006 
conference!

Uwe Ligges



elvis at xlsolutions-corp.com wrote:
> XLSolutions Corporation (www.xlsolutions-corp.com) is proud to announce:
> 
> (1) Introduction to R/S+ programming: Microarrays Analysis and Bioconductor 
>  
>                   *** San Francisco / July 17-18, 2006 ***
> 		  *** Chicago       / July 24-25, 2006 ***
>                   *** Baltimore     / July 27-28, 2006 *** 
>                   *** Raleigh       / July 17-18, 2006 ***
>                   *** Boston        / July 27-28, 2006 ***
>               http://www.xlsolutions-corp.com/RSmicro
> 
> (2) R/Splus Fundamentals and Programming Techniques
> 
>                   *** San Francisco / July 10-11, 2006 ***
>                   *** Houston       / July 13-14, 2006 ***
>                   *** San Diego     / July 17-18, 2006 ***
>                   *** Chicago       / July 20-21, 2006 ***
>                   *** New York City / July 24-25, 2006 ***
>                   *** Boston        / July 27-28, 2006 ***
>  
>               http://www.xlsolutions-corp.com/Rfund.htm  
> 
> Ask for group discount and reserve your seat Now - Earlybird Rates.
> Payment due after the class! Email Sue Turner:  sue at xlsolutions-corp.com
> Interested in our Advanced Programming class? 
> 
> (1) Introduction to R/S+ programming: Microarrays Analysis and Bioconductor   
>  
> Course Outline:
> 
> - R/S System: Overview; Installation and Demonstration 
> - Data Manipulation and Management 
> - Graphics; Enhancing Plots, Trellis 
> - Writing Functions 
> - Connecting to External Software 
> - R/S Packages and Libraries (e.g. BioConductor) 
> - BioConductor: Overview; Installation and Demonstration 
> - Array Quality Inspection 
> - Correction and Normalization; Affymetrix and cDNA arrays 
> - Identification of Differentially Expressed Genes 
> - Visualization of Genomic Information 
> - Clustering Methods in R/Splus 
> - Gene Ontology (GO) and Pathway Analysis 
> - Inference, Strategies for Large Data 
> 
> 
> 
> (2) R/Splus Fundamentals and Programming Techniques
>                        
> Course outline.
> 
> - An Overview of R and S
> - Data Manipulation and Graphics
> - Using Lattice Graphics
> - A Comparison of R and S-Plus
> - How can R Complement SAS?
> - Writing Functions
> - Avoiding Loops
> - Vectorization
> - Statistical Modeling
> - Project Management
> - Techniques for Effective use of R and S
> - Enhancing Plots
> - Using High-level Plotting Functions
> - Building and Distributing Packages (libraries)
> - Connecting; ODBC, Rweb, Orca via sockets and via Rjava
> 
> Email us for group discounts.
> Email Sue Turner: sue at xlsolutions-corp.com
> Phone: 206-686-1578
> Visit us: www.xlsolutions-corp.com/training.htm
> Please let us know if you and your colleagues are interested in this
> class to take advantage of group discount. Register now to secure your
> seat!
> 
> Cheers,
> Elvis Miller, PhD
> Manager Training.
> XLSolutions Corporation
> 206 686 1578
> www.xlsolutions-corp.com
> elvis at xlsolutions-
> 
> 2 Courses - (1) Introduction to R/S+ programming: Microarrays Analysis and Bioconductor 
>             (2) R/Splus Fundamentals and Programming Techniques
>              Interest in our R/Splus Advanced Programming?  Email us for upcoming courses.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From mw-u2 at gmx.de  Tue Jun 13 21:34:40 2006
From: mw-u2 at gmx.de (mw-u2 at gmx.de)
Date: Tue, 13 Jun 2006 21:34:40 +0200
Subject: [R] automated data processing
Message-ID: <20060613193440.185720@gmx.net>

I have many files (0.4.dat, 0.5.dat, ...) of which I would like to calculate mean value and variance and save the output in a new file where each line shouldlook like: "0.4 mean(0.4.dat) var(0.4.dat)" and so on. Right now I got a a simple script that makes me unhappy:

1. I run it by "R --no-save < script.r > out.dat" unfortunately out.dat has all the original commands in it and a "[1]" infront of every output

2. I would love to have a variable running through 0.4, 0.5, ... naming the
datafile to process and the first column in the output.

My script looks like:

data <- read.table("0.4.dat"); E <- data$V1[1000:length(data$V1)];
c(0.4, mean(E), var(E));
data <- read.table("0.5.dat"); E <- data$V1[1000:length(data$V1)];
c(0.5, mean(E), var(E));

And that would be its output:
#[1]     0.400 -1134.402  5700.966
#> data <- read.table("0.5.dat"); E <- data$V1[1000:length(data$V1)];
#> c(0.5, mean(E), var(E));
#[1]     0.500 -1787.232  2973.692

Thanks
-- 


Der GMX SmartSurfer hilft bis zu 70% Ihrer Onlinekosten zu sparen!
Ideal f?r Modem und ISDN: http://www.gmx.net/de/go/smartsurfer


From hodgess at gator.dt.uh.edu  Tue Jun 13 22:00:37 2006
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Tue, 13 Jun 2006 15:00:37 -0500
Subject: [R]  R kicking us out
Message-ID: <200606132000.k5DK0bNd002710@gator.dt.uh.edu>

Dear R People:

I am using R for teaching purposes in a large classroom.

Each computer has its own copy of R.

However, every once in a while, about half of us will get 
thrown out of R, for no apparent reason.

By the way, it has happened in other classrooms as well.


Has anyone else run into this, please?

If so, how have you solved this problem, please?

Thanks in advance!

R for Windows

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu


From Scott.Waichler at pnl.gov  Tue Jun 13 22:05:31 2006
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Tue, 13 Jun 2006 13:05:31 -0700
Subject: [R] Failed library(ncdf)
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A0488D569@pnlmse35.pnl.gov>


I am trying to use the ncdf package on a Mac OS X machine (10.4.6), but
I get the following error message:

> library(ncdf)
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library
'/Library/Frameworks/R.framework/Resources/library/ncdf/libs/ppc/ncdf.so
':
 
dlopen(/Library/Frameworks/R.framework/Resources/library/ncdf/libs/ppc/n
cdf.so, 6): Symbol not found: _nc_inq
  Referenced from:
/Library/Frameworks/R.framework/Resources/library/ncdf/libs/ppc/ncdf.so
  Expected in: flat namespace
Error in library(ncdf) : .First.lib failed for 'ncdf'

In the past I have received similar messages in Linux and was able to
fix the problem by copying netCDF library and header files to
directories under /usr/local/.  But in the Mac I'm stumped and can't
figure out what to do to fix this.  I have downloaded source from netCDF
and installed it without problems, but I guess I haven't been able to
get R to see it.

Thanks for your help,  

Scott Waichler
Pacific Northwest National Laboratory
scott.waichler _at_ pnl.gov


From rolf at erdos.math.unb.ca  Tue Jun 13 22:14:02 2006
From: rolf at erdos.math.unb.ca (Rolf Turner)
Date: Tue, 13 Jun 2006 17:14:02 -0300 (ADT)
Subject: [R] if syntax error :(
Message-ID: <200606132014.k5DKE29e004296@erdos.math.unb.ca>

H. Paul Benton wrote:

> Umm sorry to bother everyone again but I'm having trouble with my if
> statement. I come from a perl background so that's probably my problem! :)
> So here is my code:
> 
> if (any(lgAB>4) | any(lgAB<-4)){
> 	freq_AB<-hist(lgAB, type="o", plot=F)
> 	else
> 	freq_AB<-hist(lgAB, breaks=br,type ="o", plot=F)
> }
> 
> And I get 
> > source("E:/R/GMDA-1.1.R")
> Error in parse(file, n = -1, NULL, "?") : syntax error at
> 11:             freq_AB<-hist(lgAB, type="o", plot=F)
> 12:             else
> >

No-one yet has pointed out the following problem, which, while not be
a syntax error as such, will cause you headaches:

	if (any(lgAB>4) | any(lgAB<-4)){
                              ^^^^^^^

This assigns the value 4 to lgAB (which is presumably NOT what
you want to do).  You want ``any(lgAB < -4)''.

General rule:  Put in spaces around operators --- it makes the
code (much) more readable and avoids unintended consequences.

Another infelicity in your code: ``plot=F''.  Use ``plot=FALSE''.
(Note that the symbols ``F'' and ``T'' are assignable, *unlike*
``TRUE'' and ``FALSE''.)

				cheers,

					Rolf Turner
					rolf at math.unb.ca


From ggrothendieck at gmail.com  Tue Jun 13 22:29:09 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 13 Jun 2006 16:29:09 -0400
Subject: [R] if syntax error :(
In-Reply-To: <200606132014.k5DKE29e004296@erdos.math.unb.ca>
References: <200606132014.k5DKE29e004296@erdos.math.unb.ca>
Message-ID: <971536df0606131329v52980d2bh6bbe4fd0fe3d4a88@mail.gmail.com>

On 6/13/06, Rolf Turner <rolf at erdos.math.unb.ca> wrote:
> H. Paul Benton wrote:
>
> > Umm sorry to bother everyone again but I'm having trouble with my if
> > statement. I come from a perl background so that's probably my problem! :)
> > So here is my code:
> >
> > if (any(lgAB>4) | any(lgAB<-4)){
> >       freq_AB<-hist(lgAB, type="o", plot=F)
> >       else
> >       freq_AB<-hist(lgAB, breaks=br,type ="o", plot=F)
> > }
> >
> > And I get
> > > source("E:/R/GMDA-1.1.R")
> > Error in parse(file, n = -1, NULL, "?") : syntax error at
> > 11:             freq_AB<-hist(lgAB, type="o", plot=F)
> > 12:             else
> > >
>
> No-one yet has pointed out the following problem, which, while not be
> a syntax error as such, will cause you headaches:
>
>        if (any(lgAB>4) | any(lgAB<-4)){
>                              ^^^^^^^
>
> This assigns the value 4 to lgAB (which is presumably NOT what
> you want to do).  You want ``any(lgAB < -4)''.
>
> General rule:  Put in spaces around operators --- it makes the
> code (much) more readable and avoids unintended consequences.
>
> Another infelicity in your code: ``plot=F''.  Use ``plot=FALSE''.
> (Note that the symbols ``F'' and ``T'' are assignable, *unlike*
> ``TRUE'' and ``FALSE''.)
>

In fact if we are going to improve the code we could write it more
compactly and in a way which shows more clearly that the if
is only involved in setting breaks like this:

freq_AB <-  hist(lgAB,  type = "o", plot = FALSE,
  breaks = if (any(abs(lgAB) > 4)) "Sturges" else br)


From Ted.Harding at nessie.mcc.ac.uk  Tue Jun 13 22:30:05 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 13 Jun 2006 21:30:05 +0100 (BST)
Subject: [R] R kicking us out
In-Reply-To: <200606132000.k5DK0bNd002710@gator.dt.uh.edu>
Message-ID: <XFMail.060613213005.Ted.Harding@nessie.mcc.ac.uk>

On 13-Jun-06 Erin Hodgess wrote:
> Dear R People:
> 
> I am using R for teaching purposes in a large classroom.
> 
> Each computer has its own copy of R.
> 
> However, every once in a while, about half of us will get 
> thrown out of R, for no apparent reason.
> 
> By the way, it has happened in other classrooms as well.
> 
> 
> Has anyone else run into this, please?
> 
> If so, how have you solved this problem, please?
> 
> Thanks in advance!
> 
> R for Windows

Hmmm ... you're asking statisticans here!

When it happens, is it at about the same moment for all
the machines it happens on?

Is the set of machines it happens on different every time,
or do some seem to be systematically vulnerable?

(As you can tell, I've not experienced this, but am groping
for a mechanism. I find "once in a while, about half of us
will get thrown out of R" puzzling as a phenomenon -- strongly
suggestive of non-independence of machines, despite each
machine having its own copy of R).

Presumably they're networked. Might it be possible that (e.g.)
a central server is re-profiling some of the machines from
time to time (or similar)?

If it's a group phenomenon, I'd be tempted to look either in
the hardware direction (e.g. power fluctuation to which some
machines are sensitive and get e.g. corrupt bytes in RAM
leading to segfault or the like), or the software direction
(e.g. interference from external server as above, or maybe
one machine making a request to others over the net).

Though each machine "has its own copy of R", does each one
have its own copy of the full set of packages that you use?
Or is there some sort of central repository for packages
from which machines can pull down extras as they need them?

Is there a central "monitoring" machine in the classrom for
the instructor to use to collaborate with [groups of] students?

Just random thoughts, really! But hoping that they may help.

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Jun-06                                       Time: 21:29:59
------------------------------ XFMail ------------------------------


From rpowell at horizonenv.com  Tue Jun 13 22:47:06 2006
From: rpowell at horizonenv.com (Robert Powell)
Date: Tue, 13 Jun 2006 16:47:06 -0400
Subject: [R] Rosner's test
Message-ID: <536F1035-96C7-46CD-9574-384956F54121@horizonenv.com>

My second request of the day, sorry to be such a bother.

Can you tell me whether Rosner's test for outliers is available in  
any of the R packages and, if so, which one? I've tried but I can't  
find it.

Thank you very much,

Bob Powell


From jholtman at gmail.com  Tue Jun 13 23:11:31 2006
From: jholtman at gmail.com (jim holtman)
Date: Tue, 13 Jun 2006 17:11:31 -0400
Subject: [R] automated data processing
In-Reply-To: <20060613193440.185720@gmx.net>
References: <20060613193440.185720@gmx.net>
Message-ID: <644e1f320606131411p337e16ach32821d744ecd8260@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060613/753f7990/attachment.pl 

From neuro3000 at hotmail.com  Tue Jun 13 23:34:24 2006
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Tue, 13 Jun 2006 17:34:24 -0400
Subject: [R] automated data processing
In-Reply-To: <20060613193440.185720@gmx.net>
Message-ID: <BAY112-F328CD3FB19E07001F0C8ABAF8C0@phx.gbl>

Hi,
Put all you .dat files in one directory ('c:/data' for example) and do this:

finaltable <-NULL

folder <-dir('c:/data')

for (i in folder){
table1 <-read.table(i)
mean1 <-mean(table1$V1)
var1 <-var(table1$V1)
rowname <-paste(i)
finaltable <-rbind(finaltable,c(rowname,mean1,var1))
}

Neuro

>From: mw-u2 at gmx.de
>To: r-help at stat.math.ethz.ch
>Subject: [R] automated data processing
>Date: Tue, 13 Jun 2006 21:34:40 +0200
>
>I have many files (0.4.dat, 0.5.dat, ...) of which I would like to 
>calculate mean value and variance and save the output in a new file where 
>each line shouldlook like: "0.4 mean(0.4.dat) var(0.4.dat)" and so on. 
>Right now I got a a simple script that makes me unhappy:
>
>1. I run it by "R --no-save < script.r > out.dat" unfortunately out.dat has 
>all the original commands in it and a "[1]" infront of every output
>
>2. I would love to have a variable running through 0.4, 0.5, ... naming the
>datafile to process and the first column in the output.
>
>My script looks like:
>
>data <- read.table("0.4.dat"); E <- data$V1[1000:length(data$V1)];
>c(0.4, mean(E), var(E));
>data <- read.table("0.5.dat"); E <- data$V1[1000:length(data$V1)];
>c(0.5, mean(E), var(E));
>
>And that would be its output:
>#[1]     0.400 -1134.402  5700.966
>#> data <- read.table("0.5.dat"); E <- data$V1[1000:length(data$V1)];
>#> c(0.5, mean(E), var(E));
>#[1]     0.500 -1787.232  2973.692
>
>Thanks
>--
>
>
>Der GMX SmartSurfer hilft bis zu 70% Ihrer Onlinekosten zu sparen!
>Ideal f?r Modem und ISDN: http://www.gmx.net/de/go/smartsurfer
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html


From gunter.berton at gene.com  Tue Jun 13 23:34:48 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 13 Jun 2006 14:34:48 -0700
Subject: [R] Rosner's test
In-Reply-To: <536F1035-96C7-46CD-9574-384956F54121@horizonenv.com>
Message-ID: <007401c68f31$32546900$295afea9@gne.windows.gene.com>

RSiteSearch('Rosner')

?RSiteSearch  
 or search directly from CRAN.

Incidentally, I'll repeat what I've said before. Don't do outlier tests.
They're dangerous. Use robust methods instead.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robert Powell
> Sent: Tuesday, June 13, 2006 1:47 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Rosner's test
> 
> My second request of the day, sorry to be such a bother.
> 
> Can you tell me whether Rosner's test for outliers is available in  
> any of the R packages and, if so, which one? I've tried but I can't  
> find it.
> 
> Thank you very much,
> 
> Bob Powell
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From hpbenton at scripps.edu  Tue Jun 13 23:51:44 2006
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Tue, 13 Jun 2006 14:51:44 -0700
Subject: [R] if syntax error :(
In-Reply-To: <200606132014.k5DKE29e004296@erdos.math.unb.ca>
Message-ID: <200606132151.k5DLpos03968@dns1.scripps.edu>

Thank you,
That was the problem. I know about using the extra brackets but in the R
help manual it doesn't use brackets so I thought maybe R is different. 
However, yes indeed, it was assigning the 'lgAB to 4'. I'm just starting to
really learn R. 
Cheers,

Paul

PS thanks to the Patrick Burns who gave me the link to S poetry.
PPS What is the "sturges" doing in the code ? is it like a goto?
<
freq_AB <-  hist(lgAB,  type = "o", plot = FALSE,
breaks = if (any(abs(lgAB) > 4)) "Sturges" else br)
>
Research Techinician
Mass Spectrometry
  o The
 /
o Scripps
 \
  o Research
 /
o Institute


-----Original Message-----
From: Rolf Turner [mailto:rolf at erdos.math.unb.ca] 
Sent: Tuesday, June 13, 2006 1:14 PM
To: hpbenton at scripps.edu; r-help at stat.math.ethz.ch
Subject: Re: [R] if syntax error :(

H. Paul Benton wrote:

> Umm sorry to bother everyone again but I'm having trouble with my if
> statement. I come from a perl background so that's probably my problem! :)
> So here is my code:
> 
> if (any(lgAB>4) | any(lgAB<-4)){
> 	freq_AB<-hist(lgAB, type="o", plot=F)
> 	else
> 	freq_AB<-hist(lgAB, breaks=br,type ="o", plot=F)
> }
> 
> And I get 
> > source("E:/R/GMDA-1.1.R")
> Error in parse(file, n = -1, NULL, "?") : syntax error at
> 11:             freq_AB<-hist(lgAB, type="o", plot=F)
> 12:             else
> >

No-one yet has pointed out the following problem, which, while not be
a syntax error as such, will cause you headaches:

	if (any(lgAB>4) | any(lgAB<-4)){
                              ^^^^^^^

This assigns the value 4 to lgAB (which is presumably NOT what
you want to do).  You want ``any(lgAB < -4)''.

General rule:  Put in spaces around operators --- it makes the
code (much) more readable and avoids unintended consequences.

Another infelicity in your code: ``plot=F''.  Use ``plot=FALSE''.
(Note that the symbols ``F'' and ``T'' are assignable, *unlike*
``TRUE'' and ``FALSE''.)

				cheers,

					Rolf Turner
					rolf at math.unb.ca


From ggrothendieck at gmail.com  Wed Jun 14 00:45:04 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 13 Jun 2006 18:45:04 -0400
Subject: [R] if syntax error :(
In-Reply-To: <200606132151.k5DLpos03968@dns1.scripps.edu>
References: <200606132014.k5DKE29e004296@erdos.math.unb.ca>
	<200606132151.k5DLpos03968@dns1.scripps.edu>
Message-ID: <971536df0606131545y6d619a0ekc2f021b5987cfa6a@mail.gmail.com>

On 6/13/06, H. Paul Benton <hpbenton at scripps.edu> wrote:
> Thank you,
> That was the problem. I know about using the extra brackets but in the R
> help manual it doesn't use brackets so I thought maybe R is different.

The problem is not that you must use brackets -- you don't.  The problem is that
you put the else INSIDE the brackets.

> However, yes indeed, it was assigning the 'lgAB to 4'. I'm just starting to
> really learn R.
> Cheers,
>
> Paul
>
> PS thanks to the Patrick Burns who gave me the link to S poetry.
> PPS What is the "sturges" doing in the code ? is it like a goto?

Look at ?hist.default to see that "Sturges" is the default value for breaks.


> <
> freq_AB <-  hist(lgAB,  type = "o", plot = FALSE,
> breaks = if (any(abs(lgAB) > 4)) "Sturges" else br)
> >
> Research Techinician
> Mass Spectrometry
>  o The
>  /
> o Scripps
>  \
>  o Research
>  /
> o Institute
>
>
> -----Original Message-----
> From: Rolf Turner [mailto:rolf at erdos.math.unb.ca]
> Sent: Tuesday, June 13, 2006 1:14 PM
> To: hpbenton at scripps.edu; r-help at stat.math.ethz.ch
> Subject: Re: [R] if syntax error :(
>
> H. Paul Benton wrote:
>
> > Umm sorry to bother everyone again but I'm having trouble with my if
> > statement. I come from a perl background so that's probably my problem! :)
> > So here is my code:
> >
> > if (any(lgAB>4) | any(lgAB<-4)){
> >       freq_AB<-hist(lgAB, type="o", plot=F)
> >       else
> >       freq_AB<-hist(lgAB, breaks=br,type ="o", plot=F)
> > }
> >
> > And I get
> > > source("E:/R/GMDA-1.1.R")
> > Error in parse(file, n = -1, NULL, "?") : syntax error at
> > 11:             freq_AB<-hist(lgAB, type="o", plot=F)
> > 12:             else
> > >
>
> No-one yet has pointed out the following problem, which, while not be
> a syntax error as such, will cause you headaches:
>
>        if (any(lgAB>4) | any(lgAB<-4)){
>                              ^^^^^^^
>
> This assigns the value 4 to lgAB (which is presumably NOT what
> you want to do).  You want ``any(lgAB < -4)''.
>
> General rule:  Put in spaces around operators --- it makes the
> code (much) more readable and avoids unintended consequences.
>
> Another infelicity in your code: ``plot=F''.  Use ``plot=FALSE''.
> (Note that the symbols ``F'' and ``T'' are assignable, *unlike*
> ``TRUE'' and ``FALSE''.)
>
>                                cheers,
>
>                                        Rolf Turner
>                                        rolf at math.unb.ca
>
>
>
>


From dhinds at sonic.net  Wed Jun 14 01:23:13 2006
From: dhinds at sonic.net (dhinds at sonic.net)
Date: Tue, 13 Jun 2006 23:23:13 +0000 (UTC)
Subject: [R] Numerical print format and loading with RMySQL
References: <D220A369-BF7C-44E4-B839-D96FC06CE801@ebi.ac.uk>
Message-ID: <e6nhd1$t5l$1@sea.gmane.org>

Nathan Johnson <njohnson at ebi.ac.uk> wrote:

> I constructed a data frame with columns corresponding to the table  
> fields, and tried with and without a null column for the auto- 
> increment internal id column.  I also tried renaming all the column  
> names to match those of the table fields.  All of my attempts  
> resulted in a long wait and "TRUE" being printed to the terminal,  
> which I though was quite promising, however, on inspection of the DB,  
> I found nothing had been written.

Is it possible you needed a call to dbCommit()?

-- Dave


From g.abraham at ms.unimelb.edu.au  Wed Jun 14 01:41:39 2006
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Wed, 14 Jun 2006 09:41:39 +1000
Subject: [R] Multiple lag.plots per page
In-Reply-To: <Pine.LNX.4.64.0606130847530.19239@gannet.stats.ox.ac.uk>
References: <448E1FFA.6070601@ms.unimelb.edu.au>
	<Pine.LNX.4.64.0606130736440.24883@gannet.stats.ox.ac.uk>
	<448E6628.5060907@ms.unimelb.edu.au>
	<Pine.LNX.4.64.0606130847530.19239@gannet.stats.ox.ac.uk>
Message-ID: <448F4D33.7040600@ms.unimelb.edu.au>

Prof Brian Ripley wrote:
> On Tue, 13 Jun 2006, Gad Abraham wrote:
> 
>> Prof Brian Ripley wrote:
>>> On Tue, 13 Jun 2006, Gad Abraham wrote:
>>>
>>>> Hi,
>>>>
>>>> I'm trying to plot several lag.plots on a page, however the second plot
>>>> replaces the first one (although it only takes up the upper half as it
>>>> should):
>>>>
>>>> par(mfrow=c(2,1))
>>>> a<-sin(1:100)
>>>> b<-cos(1:100)
>>>> lag.plot(a)
>>>> lag.plot(b)
>>>>
>>>> What's the trick to this?
>>>
>>> lag.plot itself calls par(mfrow).  The trick is to get one call to do 
>>> the plots you want:
>>>
>>> lag.plot(cbind(a,b))
>>>
>>>
>>
>> Thanks, that works great for multiple lag.plots. Is it possible to 
>> have a lag.plot and another type of plot on the same page? The second 
>> plot() always replaces the lag.plot for me.
> 
> Yes, if the other plot is second, e.g
> 
> par(mfrow=c(2,1))
> a<-sin(1:100)
> lag.plot(a)
> par(mfg=c(2,1)) # move to second plot
> plot(1:10)
> 
> 

Thanks, works great.

Cheers,
Gad

-- 
Gad Abraham
Department of Mathematics and Statistics
University of Melbourne
Victoria 3010, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From spencer.graves at pdf.com  Wed Jun 14 03:49:09 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 13 Jun 2006 18:49:09 -0700
Subject: [R] cumulative time durations of specified periods (chron)
In-Reply-To: <87ejxv9gfr.fsf@arctocephalus.homelinux.org>
References: <87ejxv9gfr.fsf@arctocephalus.homelinux.org>
Message-ID: <448F6B15.3060204@pdf.com>

	  Did you try the following:

 > xto-xfrom
Time in days:
[1] 1.50 1.75 2.00 2.25

	  Also, have you seen Gabor Grothendieck and Thomas Petzoldt. "R help 
desk: Date and time classes in R". R News, 4(1):29-32, June 2004., 
downloadable from www.r-project.org -> "Documentation:  Newsletter" as 
well as the "zoo" vignette (see 
"http://finzi.psych.upenn.edu/R/Rhelp02a/archive/67006.html")?

	  hope this helps.
	  Spencer Graves

Sebastian Luque wrote:
> Hi,
> 
> Say we have two chron vectors representing start and end date/times of an
> event, respectively:
> 
> 
> R> (xfrom <- chron(seq(1.25, 11, 3.25)))
> [1] (01/02/70 06:00:00) (01/05/70 12:00:00) (01/08/70 18:00:00)
> [4] (01/12/70 00:00:00)
> R> (xto <- chron(as.numeric(xfrom) + seq(1.5, 2.25, 0.25)))
> [1] (01/03/70 18:00:00) (01/07/70 06:00:00) (01/10/70 18:00:00)
> [4] (01/14/70 06:00:00)
> 
> 
> and we would like to know how much time is included in a number of
> intervals within each event.  We can define the intervals with two chron
> vectors:
> 
> 
> R> (xt0 <- times(c(0.50, 0)))
> [1] 12:00:00 00:00:00
> R> (xt1 <- times(c(1 - (1 / 86400), 0.25)))
> [1] 23:59:59 06:00:00
> 
> 
> So for the first event, 01/02/70 06:00:00 to 01/03/70 18:00:00, the
> interest is to find how much time corresponds to periods 12:00:00 -
> 23:59:59 and 00:00:00 - 06:00:00.
> 
> I began writing a function to accomplish this task, but am at an impasse.
> The archives may have something on this, but I haven't found a good search
> query for it, so I'd appreciate some pointers.  Thanks in advance.
> 
> 
> Cheers,
>


From spluque at gmail.com  Wed Jun 14 04:06:17 2006
From: spluque at gmail.com (Sebastian Luque)
Date: Tue, 13 Jun 2006 21:06:17 -0500
Subject: [R] cumulative time durations of specified periods (chron)
In-Reply-To: <448F6B15.3060204@pdf.com>
References: <87ejxv9gfr.fsf@arctocephalus.homelinux.org>
	<448F6B15.3060204@pdf.com>
Message-ID: <873be87ap2.fsf@arctocephalus.homelinux.org>

On Tue, 13 Jun 2006 18:49:09 -0700,
Spencer Graves <spencer.graves at pdf.com> wrote:

	  > Did you try the following:
>
>> xto-xfrom
> Time in days:
> [1] 1.50 1.75 2.00 2.25
>
	  Also, have you seen Gabor Grothendieck and Thomas Petzoldt. "R
help desk: Date and time classes in R". R News, 4(1):29-32, June 2004.,
downloadable from www.r-project.org -> "Documentation:  Newsletter" as
well as the "zoo" vignette (see
"http://finzi.psych.upenn.edu/R/Rhelp02a/archive/67006.html")?
>
> 	  hope this helps.
> 	  Spencer Graves
>
> Sebastian Luque wrote:
>> Hi,
>> Say we have two chron vectors representing start and end date/times of
>> an
>> event, respectively:
>> R> (xfrom <- chron(seq(1.25, 11, 3.25)))
>> [1] (01/02/70 06:00:00) (01/05/70 12:00:00) (01/08/70 18:00:00)
>> [4] (01/12/70 00:00:00)
>> R> (xto <- chron(as.numeric(xfrom) + seq(1.5, 2.25, 0.25)))
>> [1] (01/03/70 18:00:00) (01/07/70 06:00:00) (01/10/70 18:00:00)
>> [4] (01/14/70 06:00:00)
>> and we would like to know how much time is included in a number of
>> intervals within each event.  We can define the intervals with two chron
>> vectors:
>> R> (xt0 <- times(c(0.50, 0)))
>> [1] 12:00:00 00:00:00
>> R> (xt1 <- times(c(1 - (1 / 86400), 0.25)))
>> [1] 23:59:59 06:00:00
>> So for the first event, 01/02/70 06:00:00 to 01/03/70 18:00:00, the
>> interest is to find how much time corresponds to periods 12:00:00 -
>> 23:59:59 and 00:00:00 - 06:00:00.
>> I began writing a function to accomplish this task, but am at an
>> impasse.
>> The archives may have something on this, but I haven't found a good search
>> query for it, so I'd appreciate some pointers.  Thanks in advance.
>> Cheers,
>>

-- 
Sebastian P. Luque                    Current address:
Department of Biology                 Fisheries and Oceans Canada
Memorial University of Newfoundland   501 University Crescent
sluque at mun.ca                         Winnipeg, MB R3T 2N6, Canada
http://sebmags.homelinux.org/~sluque  Tel +1 204 586-8170, Fax ... 984-2403


From spluque at gmail.com  Wed Jun 14 05:16:38 2006
From: spluque at gmail.com (Sebastian Luque)
Date: Tue, 13 Jun 2006 22:16:38 -0500
Subject: [R] cumulative time durations of specified periods (chron)
References: <87ejxv9gfr.fsf@arctocephalus.homelinux.org>
	<448F6B15.3060204@pdf.com>
Message-ID: <87k67k5svd.fsf@arctocephalus.homelinux.org>

[sorry for my previous empty follow-up -- my fingers got messed up.]


Spencer Graves <spencer.graves at pdf.com> wrote:

> Did you try the following:
>
>> xto-xfrom
> Time in days:
> [1] 1.50 1.75 2.00 2.25

I wanted to find the total amount of time between a series of date/time's
corresponding to particular times of the day, not how much time there is
between the two series.  I ended my coding it as:


"subTime" <- function(from, to, t0, t1)
{
    stopifnot(length(from) == length(to), length(t0) == length(t1))
    startFrom <- chron(sapply(t0, "+", as.numeric(dates(from))))
    endFrom <- chron(sapply(t1, "+", as.numeric(dates(from))))
    startTo <- chron(sapply(t0, "+", as.numeric(dates(to))))
    endTo <- chron(sapply(t1, "+", as.numeric(dates(to))))
    fromSeq <- mapply(seq, startFrom, startTo)
    toSeq <- mapply(seq, endFrom, endTo)
    diffs <- mapply(function(x0, x1, y0, y1) {
        bad <- x1 < y0 | x0 > y1
        full <- x0 >= y0 & x1 <= y1
        fullDiff <- sum(x1[full] - x0[full], na.rm=TRUE)
        r <- x0 >= y0 & x1 > y1 & !bad
        x1[r] <- y1
        truncR <- sum(pmax(x1[r], x0[r]) - x0[r], na.rm=TRUE)
        l <- x0 < y0 & x1 <= y1 & !bad
        x0[l] <- y0
        truncL <- sum(x1[l] - pmax(x0[l], x1[l]), na.rm=TRUE)
        sum(fullDiff, truncR, truncL)
    }, fromSeq, toSeq, from, to)
    diffDims <- list(phases=paste(from, to, sep="-"),
                     periods=paste(t0, t1, sep="-"))
    matrix(diffs, ncol=length(t0), dimnames=diffDims)
}


The logicals seemed to be needed to account for truncated periods.  It's
working as I needed it to, but it looks too convoluted so if somebody
finds an easier way to do this, I'd be happy to learn about it.  The
function returns a matrix:


R> xfrom <- chron(seq(1.25, 11, 3.25))
R> xto <- chron(as.numeric(xfrom) + seq(1.5, 2.25, 0.25))
R> xt0 <- times(c("04:00:00", "11:00:00"))
R> xt1 <- times(c("10:00:00", "16:00:00"))
R> subTime(xfrom, xto, xt0, xt1)
                                         periods
phases                                    04:00:00-10:00:00 11:00:00-16:00:00
  (01/02/70 06:00:00)-(01/03/70 18:00:00)            0.2500            0.4167
  (01/05/70 12:00:00)-(01/07/70 06:00:00)            0.3333            0.2083
  (01/08/70 18:00:00)-(01/10/70 18:00:00)            0.5000            0.4167
  (01/12/70 00:00:00)-(01/14/70 06:00:00)            0.5833            0.4167



> Also, have you seen Gabor Grothendieck and Thomas Petzoldt. "R help
> desk: Date and time classes in R". R News, 4(1):29-32, June 2004.,
> downloadable from www.r-project.org -> "Documentation: Newsletter" as
> well as the "zoo" vignette (see
> "http://finzi.psych.upenn.edu/R/Rhelp02a/archive/67006.html")?

I haven't read the "zoo" vignette yet, but the R News article is one I
come back to often.


Thank you,

-- 
Seb


From Carl.Hauser at nwea.org  Wed Jun 14 05:21:48 2006
From: Carl.Hauser at nwea.org (Carl Hauser)
Date: Tue, 13 Jun 2006 20:21:48 -0700
Subject: [R] data set size question
Message-ID: <1D47D77A70708D49A3D4A816CE6C69F6019A87DE@exchbe1.Americas.nwea.pvt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060613/e1df80fd/attachment.pl 

From ggrothendieck at gmail.com  Wed Jun 14 06:01:18 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 14 Jun 2006 00:01:18 -0400
Subject: [R] data set size question
In-Reply-To: <1D47D77A70708D49A3D4A816CE6C69F6019A87DE@exchbe1.Americas.nwea.pvt>
References: <1D47D77A70708D49A3D4A816CE6C69F6019A87DE@exchbe1.Americas.nwea.pvt>
Message-ID: <971536df0606132101qf8ce23apd2ff4100aec1cd75@mail.gmail.com>

The restriction is that objects are kept in memory
so if you have sufficient memory and your OS lets you
access it then you should be ok.  S-Plus is a commercial
package similar to R but stores its objects in files and can handle
larger data sets if you run into trouble.

Given that R is free and once downloaded can be
installed on Windows in a minute or so (I assume its
just as easy on other OSes) just install it and generate
some test data and see if you have any problems,
e.g.  I had no trouble running the following on my PC:

n <- 100000
p <- 20
x <- matrix(rnorm(n * p), n)
colnames(x) <- letters[1:p]
# regress column a against the rest
x.lm <- lm(a ~., as.data.frame(x))
plot(x.lm)  # click mouse to advance to successive plots
summary(x.lm)

On 6/13/06, Carl Hauser <Carl.Hauser at nwea.org> wrote:
> Hi there,
>
> I'm very new to R and am only in the beginning stages of investigating
> it for possible use. A document by John Maindonald at the r-project
> website entitled "Using R for Data Analysis and Graphics: Introduction,
> Code and Commentary" contains the following paragraph, "The R system may
> struggle to handle very large data sets. Depending on available computer
> memory, the processing of a data set containing one hundred thousand
> observations and perhaps twenty variables may press the limits of what R
> can easily handle". This document was written in 2004.
>
> My questions are:
>
> Is this still the case? If so, has anyone come up with creative
> solutions to mitigate these limitations? If you work with large data
> sets in R, what have your experiences been?
>
> >From what I've seen so far, R seems to have enormous potential and
> capabilities. I routinely work with data sets of several hundred
> thousand to several million. It would be unfortunate if such potential
> and capabilities were not realized because of (effective) data set size
> limitations.
>
> Please tell me it ain't so.
>
> Thanks for any help or suggestions.
>
> Carl
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From g.abraham at ms.unimelb.edu.au  Wed Jun 14 06:33:12 2006
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Wed, 14 Jun 2006 14:33:12 +1000
Subject: [R] Multiple lag.plots per page
In-Reply-To: <Pine.LNX.4.64.0606130847530.19239@gannet.stats.ox.ac.uk>
References: <448E1FFA.6070601@ms.unimelb.edu.au>
	<Pine.LNX.4.64.0606130736440.24883@gannet.stats.ox.ac.uk>
	<448E6628.5060907@ms.unimelb.edu.au>
	<Pine.LNX.4.64.0606130847530.19239@gannet.stats.ox.ac.uk>
Message-ID: <448F9188.2090102@ms.unimelb.edu.au>

Prof Brian Ripley wrote:
> On Tue, 13 Jun 2006, Gad Abraham wrote:
> 
>> Prof Brian Ripley wrote:
>>> On Tue, 13 Jun 2006, Gad Abraham wrote:
>>>
>>>> Hi,
>>>>
>>>> I'm trying to plot several lag.plots on a page, however the second plot
>>>> replaces the first one (although it only takes up the upper half as it
>>>> should):
>>>>
>>>> par(mfrow=c(2,1))
>>>> a<-sin(1:100)
>>>> b<-cos(1:100)
>>>> lag.plot(a)
>>>> lag.plot(b)
>>>>
>>>> What's the trick to this?
>>>
>>> lag.plot itself calls par(mfrow).  The trick is to get one call to do 
>>> the plots you want:
>>>
>>> lag.plot(cbind(a,b))
>>>
>>>
>>
>> Thanks, that works great for multiple lag.plots. Is it possible to 
>> have a lag.plot and another type of plot on the same page? The second 
>> plot() always replaces the lag.plot for me.
> 
> Yes, if the other plot is second, e.g
> 
> par(mfrow=c(2,1))
> a<-sin(1:100)
> lag.plot(a)
> par(mfg=c(2,1)) # move to second plot
> plot(1:10)
> 
> 

Following from my previous questions, lag.plot doesn't recognise some of 
the standard plot variables, e.g. xaxt="n" doesn't remove the x-axis, 
and setting xlab causes an error:

 > lag.plot(sin(1:100), xlab="foo")
Error in plotts(x = x, y = y, plot.type = plot.type, xy.labels = 
xy.labels,  :
         formal argument "xlab" matched by multiple actual arguments

Is this a bug or a feature?

Also, how can I make lag.plot behave nicely when plotted with other 
plots on the same page? it takes up more room than it's allocated by 
par(mfrow).


Thanks for your help,
Gad

-- 
Gad Abraham
Department of Mathematics and Statistics
University of Melbourne
Victoria 3010, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From ripley at stats.ox.ac.uk  Wed Jun 14 06:46:56 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 14 Jun 2006 05:46:56 +0100 (BST)
Subject: [R] Multiple lag.plots per page
In-Reply-To: <448F9188.2090102@ms.unimelb.edu.au>
References: <448E1FFA.6070601@ms.unimelb.edu.au>
	<Pine.LNX.4.64.0606130736440.24883@gannet.stats.ox.ac.uk>
	<448E6628.5060907@ms.unimelb.edu.au>
	<Pine.LNX.4.64.0606130847530.19239@gannet.stats.ox.ac.uk>
	<448F9188.2090102@ms.unimelb.edu.au>
Message-ID: <Pine.LNX.4.64.0606140539490.2567@gannet.stats.ox.ac.uk>

On Wed, 14 Jun 2006, Gad Abraham wrote:

> Prof Brian Ripley wrote:
>> On Tue, 13 Jun 2006, Gad Abraham wrote:
>> 
>>> Prof Brian Ripley wrote:
>>>> On Tue, 13 Jun 2006, Gad Abraham wrote:
>>>> 
>>>>> Hi,
>>>>> 
>>>>> I'm trying to plot several lag.plots on a page, however the second plot
>>>>> replaces the first one (although it only takes up the upper half as it
>>>>> should):
>>>>> 
>>>>> par(mfrow=c(2,1))
>>>>> a<-sin(1:100)
>>>>> b<-cos(1:100)
>>>>> lag.plot(a)
>>>>> lag.plot(b)
>>>>> 
>>>>> What's the trick to this?
>>>> 
>>>> lag.plot itself calls par(mfrow).  The trick is to get one call to do the 
>>>> plots you want:
>>>> 
>>>> lag.plot(cbind(a,b))
>>>> 
>>>> 
>>> 
>>> Thanks, that works great for multiple lag.plots. Is it possible to have a 
>>> lag.plot and another type of plot on the same page? The second plot() 
>>> always replaces the lag.plot for me.
>> 
>> Yes, if the other plot is second, e.g
>> 
>> par(mfrow=c(2,1))
>> a<-sin(1:100)
>> lag.plot(a)
>> par(mfg=c(2,1)) # move to second plot
>> plot(1:10)
>> 
>> 
>
> Following from my previous questions, lag.plot doesn't recognise some of the 
> standard plot variables, e.g. xaxt="n" doesn't remove the x-axis, and setting 
> xlab causes an error:
>
>> lag.plot(sin(1:100), xlab="foo")
> Error in plotts(x = x, y = y, plot.type = plot.type, xy.labels = xy.labels, 
> :
>        formal argument "xlab" matched by multiple actual arguments
>
> Is this a bug or a feature?

feature.  Note that the help page says

      ...: Further arguments to 'plot.ts'.

and not `graphical parameters'.


> Also, how can I make lag.plot behave nicely when plotted with other plots on 
> the same page? it takes up more room than it's allocated by par(mfrow).

Really you are not using it for its intended purpose, multiple plots at 
different lags.  (Notice the plurals in the title and the description 
on the help page.)  Why not use plot.ts directly?

If you want to pursue lag.plot, try the version in R-devel which works 
better for single-plot displays.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jwd at surewest.net  Wed Jun 14 08:14:51 2006
From: jwd at surewest.net (J Dougherty)
Date: Tue, 13 Jun 2006 23:14:51 -0700
Subject: [R] R kicking us out
In-Reply-To: <200606132000.k5DK0bNd002710@gator.dt.uh.edu>
References: <200606132000.k5DK0bNd002710@gator.dt.uh.edu>
Message-ID: <200606132314.51332.jwd@surewest.net>

On Tuesday 13 June 2006 13:00, Erin Hodgess wrote:
> Dear R People:
>
> I am using R for teaching purposes in a large classroom.
>
> Each computer has its own copy of R.
>
> However, every once in a while, about half of us will get
> thrown out of R, for no apparent reason.
>
> By the way, it has happened in other classrooms as well.
>
>
> Has anyone else run into this, please?
>
> If so, how have you solved this problem, please?
>
> Thanks in advance!
>
> R for Windows
>
You will need to provide a good deal more information before a useful answer 
is likely.  For instance, what operating system do these computers use?  If 
the computers are all running networked in a lab, is there some automatic 
maintenance being run?  What hardware specifications (CPU, memory, harddisk 
storage, etc.)?  What size data sets are being used?  And, more importantly, 
what does "kicking you out" actually mean in terms of computer behaviour?  
Does R simply crash?  When you "about half of us"  is it always the same 
half?  Does it occur at the same time of day?  There are far too many 
unknowns for a diagnosis.

JD


From anil_rohilla at rediffmail.com  Wed Jun 14 08:17:19 2006
From: anil_rohilla at rediffmail.com (anil kumar rohilla)
Date: 14 Jun 2006 06:17:19 -0000
Subject: [R] Decimal series how to make.........
Message-ID: <20060614061719.13910.qmail@webmail26.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060614/4430550e/attachment.pl 

From nvj at fys.ku.dk  Wed Jun 14 08:36:17 2006
From: nvj at fys.ku.dk (Niels Vestergaard Jensen)
Date: Wed, 14 Jun 2006 08:36:17 +0200 (CEST)
Subject: [R] Decimal series how to make.........
In-Reply-To: <20060614061719.13910.qmail@webmail26.rediffmail.com>
Message-ID: <Pine.LNX.4.44.0606140832200.27956-100000@scharff.fys.ku.dk>

On 14 Jun 2006, anil kumar rohilla wrote:

>   Hi List,
>       I am new to this Rsoftware, i want to make a sereis for example which is having values like this, s<- 0,0.1,0.2,0.3,0.4........,1
>
> i tryed this statement
> s<-0:0.1:1
> but this giving an error megssage.
> but by default increment 1 it is taking ,,,,,so what to do ,,

You want the seq() function. Look it up by typing ?seq

About the ":" - watch out for the difference between 1:4-1 and 1:(4-1)
(Go ahead and try)

best

	Niels








>
> i want to use this varible in for loop..
>
> like for(j in s)
>
> thanks in advance
>
>
> ANIL KUMAR( METEOROLOGIST)
> LRF SECTION
> NATIONAL CLIMATE CENTER
> ADGM(RESEARCH)
> INDIA METEOROLOGICAL DEPARTMENT
> SHIVIJI NAGAR
> PUNE-411005 INDIA
> MOBILE +919422023277
> anilkumar at imdpune.gov.in
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From blomsp at ozemail.com.au  Wed Jun 14 08:36:24 2006
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Wed, 14 Jun 2006 16:36:24 +1000
Subject: [R] Decimal series how to make.........
In-Reply-To: <20060614061719.13910.qmail@webmail26.rediffmail.com>
References: <20060614061719.13910.qmail@webmail26.rediffmail.com>
Message-ID: <448FAE68.4050106@ozemail.com.au>

?seq

anil kumar rohilla wrote:
>   Hi List,
>       I am new to this Rsoftware, i want to make a sereis for example which is having values like this, s<- 0,0.1,0.2,0.3,0.4........,1 
>
> i tryed this statement
> s<-0:0.1:1
> but this giving an error megssage.
> but by default increment 1 it is taking ,,,,,so what to do ,,
>
> i want to use this varible in for loop..
>
> like for(j in s)
>
> thanks in advance
>
>
> ANIL KUMAR( METEOROLOGIST)
> LRF SECTION 
> NATIONAL CLIMATE CENTER 
> ADGM(RESEARCH)
> INDIA METEOROLOGICAL DEPARTMENT
> SHIVIJI NAGAR
> PUNE-411005 INDIA
> MOBILE +919422023277
> anilkumar at imdpune.gov.in
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>   


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C


From vokey at uleth.ca  Wed Jun 14 08:53:58 2006
From: vokey at uleth.ca (John Vokey)
Date: Wed, 14 Jun 2006 00:53:58 -0600
Subject: [R] A whine and a request
Message-ID: <BD70FD47-1DF2-4FD6-9D99-F726E95C17F7@uleth.ca>

guRus and useRs,

As I instruct my students: "With R what is difficult in anything  
else, is easy, usually one line of code; but, what is easy in  
anything else, is *&^%$%#$... (translation: next to impossible) in R."

I just ran into such a case that has, to put it mildly, driven me to  
use spreadsheets (if that is not a sign of complete desperation, I  
don't know what is).  It has nothing to do with particular versions  
of R or what OS I am using, but something more fundamental.  Here it  
is.  First, I have a very large data-set (the size, though, is not  
the issue) that at base has a single binary response (score 0/1) for  
each line of the data.  Different, large blocks of the data  
correspond to the responses of single participants to stimuli  
differing in feature x, where the distribution of particular  
differences in feature x is different for each participant. Table  
(and ftable) commands are great for reducing these data to isolate  
the Freqs with which particular x values occur within each  
participant, which is what I want, including how the frequency with  
which the participant responded 1 (or 0) to that level of feature x.   
So, assiduous use of table() gives me more or less what I want:  
within participant, the values of x, the associated frequencies of 0  
responses to those values of x, and the associated frequencies of 1  
responses to those values of x.  Now, what I WANT is the simple  
regression of the proportion of 1 responses (out of the sum of 0 and  
1 responses) to each value of x on the values of x.  What table()  
gives me is close, but useless, as I can't access the columns.  But,  
applying data.frame() to the results from table, gives me access to  
the columns as variables, but returns the (re)expanded data, which  
does not allow me to compute the simple proportions I want.  After  
too many hours of trying way too many desperate commands, I gave up.   
I printed the table(), copied it to the clipboard, pasted it into a  
spreadsheet, computed the proportions, fixed the header, saved it as  
a text file, and the read it back into R to do the regressions.   
There has got to be an easier way.
--
Please avoid sending me Word or PowerPoint attachments.
See <http://www.gnu.org/philosophy/no-word-attachments.html>

-Dr. John R. Vokey


From petr.pikal at precheza.cz  Wed Jun 14 08:56:26 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 14 Jun 2006 08:56:26 +0200
Subject: [R] missing value where TRUE/FALSE needed
In-Reply-To: <20060613191942.12611.qmail@web30302.mail.mud.yahoo.com>
Message-ID: <448FCF3A.12595.43C425@localhost>

Hi

as nobody has any of your variables and can not reproduce your code 
you has to track your problems yourself. R is kindly provided you 
with hint for it.

a<-1
b<-2
value<-2
if(value>=a&&value<=b) print("right") else print("wrong")
[1] "right"
b<-NA
if(value>=a&&value<=b) print("right") else print("wrong")
Error in if (value >= a && value <= b) print("right") else 
print("wrong") : 
        missing value where TRUE/FALSE needed

So one of your variables is NA when evaluating if statement and 
logical statement evaluates to NA but if expects true/false value.

Besides I wonder if you cannor get rid of for cycle.


>

On 13 Jun 2006 at 12:19, Peter Lauren wrote:

Date sent:      	Tue, 13 Jun 2006 12:19:42 -0700 (PDT)
From:           	Peter Lauren <peterdlauren at yahoo.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] missing value where TRUE/FALSE needed

> I am using R 2.3.0 and trying to run the following
> code.
> 
>  BinCnt=round(ShorterVectorLength/10)
>  BinMatrix=matrix(nrow=BinCnt, ncol=2)
>  Increment=(VectorRange[2]-VectorRange[1])/BinCnt
>  BinMatrix[1,1]=VectorRange[1]
>  BinMatrix[1,2]=VectorRange[1]+Increment
>  for (i in 2:BinCnt)
>  {
>   BinMatrix[i,1]=BinMatrix[i-1,2]
>   BinMatrix[i,2]=BinMatrix[i,2]+Increment
>  }
> 
>  # Count entries in each bin
>  Histogram=vector(length=BinCnt, mode="numeric")
>  for (i in 1:BinCnt) Histogram[i]=0
>  VectorLength=length(FormerVector)
>  for (i in 1:VectorLength)
>  {
>   Value=FormerVector[i]
>   for (j in 1:BinCnt) if (Value>=BinMatrix[j,1] &&
> Value<=BinMatrix[j,2])
>   {
>    Histogram[j]=Histogram[j]+1
>    break;
>   }
>  }
> 
> Unfotunately, at 
> if (Value>=BinMatrix[j,1] && Value<=BinMatrix[j,2])
> I get the following error message
> Error in if (Value >= BinMatrix[j, 1] && Value <=
> BinMatrix[j, 2]) { : 
>         missing value where TRUE/FALSE needed
> I inserted browser() just before the call and 
> (Value>=BinMatrix[j,1] && Value<=BinMatrix[j,2])
> returned FALSE.
> 
> Is there a bug in my code or a bug in my version of R?

Neither. The code does not suit your data.

HTH
Petr


> 
> Many thanks in advance,
> Peter.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From jarioksa at sun3.oulu.fi  Wed Jun 14 09:17:43 2006
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Wed, 14 Jun 2006 10:17:43 +0300
Subject: [R] 2 Courses Near You - (1) Introduction to R/S+
	programming:	Microarrays Analysis and Bioconductor,
	(2) R/Splus Fundamentals and	Programming Techniques
In-Reply-To: <448F1341.1050602@statistik.uni-dortmund.de>
References: <20060613105104.9f08cc34deb45d78e54b3b5664e21546.5c0fde06e6.wbe@email.secureserver.net>
	<448F1341.1050602@statistik.uni-dortmund.de>
Message-ID: <1150269463.17239.5.camel@biol102145.oulu.fi>

On Tue, 2006-06-13 at 21:34 +0200, Uwe Ligges wrote:
> ... and again I wonder which courses are "near". This leads at once to 
> the question: "which metric is in use?". 
Possibly this:

### Great Circle distances
### Use different sign to N and S, and to E and W
### (does not matter which sign)
### Lat and long must be in degrees + decimals (sorry)
"globedis" <-
    function(lat0, lon0, lat1, lon1, km = TRUE)
{
    phi0 <- pi/180*lat0
    phi1 <- pi/180*lat1
    lambda <- pi/180*(lon0 - lon1)
    delta <- sin(phi0) * sin(phi1) + cos(phi0) * cos(phi1) * cos(lambda)
    delta <- acos(delta)
    dist <- 60*180/pi*delta
    dist <- dist %% 10800
    if (km)
        dist <- 1.852 * dist
    dist
}

Which says that Boston is nearest to my office (6100km). The other
alternatives are Baltimore 6720km, Chicago 6800km, Raleigh 7050km and
San Francisco 8240km.

In more practical metric of flight time, Baltimore is closest (OUL - BWI
12h55min), but Boston and Chicago are not much further away (OUL - BOS
14h00min, OUL-CHI 14h15min).

cheers, jari oksanen

> Probably some football related 
> metric: FIFA WM takes place in Dortmund and commercials say something 
> like "the world is our guest" ...
> Now, let's escape from football to Austria and Vienna's useR!2006 
> conference!
> 
> Uwe Ligges
> 
> 
> 
> elvis at xlsolutions-corp.com wrote:
> > XLSolutions Corporation (www.xlsolutions-corp.com) is proud to announce:
> > 
> > (1) Introduction to R/S+ programming: Microarrays Analysis and Bioconductor 
> >  
> >                   *** San Francisco / July 17-18, 2006 ***
> > 		  *** Chicago       / July 24-25, 2006 ***
> >                   *** Baltimore     / July 27-28, 2006 *** 
> >                   *** Raleigh       / July 17-18, 2006 ***
> >                   *** Boston        / July 27-28, 2006 ***
> >               http://www.xlsolutions-corp.com/RSmicro
> > 
> > (2) R/Splus Fundamentals and Programming Techniques
> > 
> >                   *** San Francisco / July 10-11, 2006 ***
> >                   *** Houston       / July 13-14, 2006 ***
> >                   *** San Diego     / July 17-18, 2006 ***
> >                   *** Chicago       / July 20-21, 2006 ***
> >                   *** New York City / July 24-25, 2006 ***
> >                   *** Boston        / July 27-28, 2006 ***
> >  
> >               http://www.xlsolutions-corp.com/Rfund.htm  
> > 
> > Ask for group discount and reserve your seat Now - Earlybird Rates.
> > Payment due after the class! Email Sue Turner:  sue at xlsolutions-corp.com
> > Interested in our Advanced Programming class? 
> > 
> > (1) Introduction to R/S+ programming: Microarrays Analysis and Bioconductor   
> >  
> > Course Outline:
> > 
> > - R/S System: Overview; Installation and Demonstration 
> > - Data Manipulation and Management 
> > - Graphics; Enhancing Plots, Trellis 
> > - Writing Functions 
> > - Connecting to External Software 
> > - R/S Packages and Libraries (e.g. BioConductor) 
> > - BioConductor: Overview; Installation and Demonstration 
> > - Array Quality Inspection 
> > - Correction and Normalization; Affymetrix and cDNA arrays 
> > - Identification of Differentially Expressed Genes 
> > - Visualization of Genomic Information 
> > - Clustering Methods in R/Splus 
> > - Gene Ontology (GO) and Pathway Analysis 
> > - Inference, Strategies for Large Data 
> > 
> > 
> > 
> > (2) R/Splus Fundamentals and Programming Techniques
> >                        
> > Course outline.
> > 
> > - An Overview of R and S
> > - Data Manipulation and Graphics
> > - Using Lattice Graphics
> > - A Comparison of R and S-Plus
> > - How can R Complement SAS?
> > - Writing Functions
> > - Avoiding Loops
> > - Vectorization
> > - Statistical Modeling
> > - Project Management
> > - Techniques for Effective use of R and S
> > - Enhancing Plots
> > - Using High-level Plotting Functions
> > - Building and Distributing Packages (libraries)
> > - Connecting; ODBC, Rweb, Orca via sockets and via Rjava
> > 
> > Email us for group discounts.
> > Email Sue Turner: sue at xlsolutions-corp.com
> > Phone: 206-686-1578
> > Visit us: www.xlsolutions-corp.com/training.htm
> > Please let us know if you and your colleagues are interested in this
> > class to take advantage of group discount. Register now to secure your
> > seat!
> > 
> > Cheers,
> > Elvis Miller, PhD
> > Manager Training.
> > XLSolutions Corporation
> > 206 686 1578
> > www.xlsolutions-corp.com
> > elvis at xlsolutions-
> > 
> > 2 Courses - (1) Introduction to R/S+ programming: Microarrays Analysis and Bioconductor 
> >             (2) R/Splus Fundamentals and Programming Techniques
> >              Interest in our R/Splus Advanced Programming?  Email us for upcoming courses.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
Jari Oksanen -- Dept Biology, Univ Oulu, 90014 Oulu, Finland
email jari.oksanen at oulu.fi, homepage http://cc.oulu.fi/~jarioksa/


From vokey at uleth.ca  Wed Jun 14 09:24:49 2006
From: vokey at uleth.ca (John Vokey)
Date: Wed, 14 Jun 2006 01:24:49 -0600
Subject: [R] R ``literal'' comand
In-Reply-To: <mailman.13.1144922404.13122.r-help@stat.math.ethz.ch>
References: <mailman.13.1144922404.13122.r-help@stat.math.ethz.ch>
Message-ID: <215873C8-1E77-43A0-BA43-521370C30946@uleth.ca>

I know I am luddite when it comes to list-based languages, such as  
R.  But, even these beasts must occasionally want to access objects  
at a literal level.  For example, to a naive twit like me, x<--print 
(y) should deposit in x a literal print out of y; that is, x should  
contain the *results* of applying the print method to y (e.g.,  
something like a tab (or space)-delimited flat sheet).  Sadly, no.   
It contains a copy of y.  That is, x<--print(y) == x<--y.  I spend  
more time fighting with all the list crapola (technical term for, uh,  
crapola) in each object than I ever manage successfully to use the  
object.  To cut through the, uh, technical stuff, every now and again  
I would just like to get a simple flat object back with NO list  
baggage.  Something like: z=literal(x), where literal() returns a  
flat spreadsheet-like structure with named columns and rows.   
Data.frame() comes close, but it always *interprets* the object  
first, which means it is usually quite different from what one saw  
with the simple print(y).

Yeah, I know, I will now get a flood of emails telling me to get with  
the program and to learn to love y[[1]] and S4 methods or some such,  
or, worse: why not just write the program myself?  I agree.  I am an  
ignorant twit.  But, aside from saving (or copying from the screen:  
my current solution) the data to a text file to remove all the list,  
uh, stuff (and ignoring the no doubt great pleasure to be had by  
chiming in to agree with my exalted twittedness), what does one do?

--
Please avoid sending me Word or PowerPoint attachments.
See <http://www.gnu.org/philosophy/no-word-attachments.html>

-Dr. John R. Vokey


From xwyefelix at gmail.com  Wed Jun 14 10:01:11 2006
From: xwyefelix at gmail.com (xingwang ye)
Date: Wed, 14 Jun 2006 16:01:11 +0800
Subject: [R] could someone tell me how to implement a multiple comparison
	test for proportions in a 2xc crosstabulation
In-Reply-To: <1bff52c20606140033j3ac8e9fdxa85d4edded2ad94a@mail.gmail.com>
References: <1bff52c20606140033j3ac8e9fdxa85d4edded2ad94a@mail.gmail.com>
Message-ID: <1bff52c20606140101r25188de1g6573c52f3cb5d152@mail.gmail.com>

Dear all,
I wanna to do multiple comparison test for proportions (multiple chi
squre ?), could someone tell me how in R, thank you!


From Brandon.J.Whitcher at gsk.com  Wed Jun 14 10:26:23 2006
From: Brandon.J.Whitcher at gsk.com (Brandon.J.Whitcher at gsk.com)
Date: Wed, 14 Jun 2006 09:26:23 +0100
Subject: [R] matrix log
Message-ID: <OFAFE8AA11.2066EFAB-ON8025718D.002D6985-8025718D.002E3B72@gsk.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060614/2693fe84/attachment.pl 

From bhs2 at mevik.net  Wed Jun 14 10:26:09 2006
From: bhs2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Wed, 14 Jun 2006 10:26:09 +0200
Subject: [R] R ``literal'' comand
In-Reply-To: <215873C8-1E77-43A0-BA43-521370C30946@uleth.ca> (John Vokey's
	message of "Wed, 14 Jun 2006 01:24:49 -0600")
References: <mailman.13.1144922404.13122.r-help@stat.math.ethz.ch>
	<215873C8-1E77-43A0-BA43-521370C30946@uleth.ca>
Message-ID: <m0slm8m9cu.fsf@bar.nemo-project.org>

capture.output(...)

If you want a single string, with newlines: 
paste(capture.output(...), collapse = "\n")

-- 
Bj?rn-Helge Mevik


From ecoinformatics at gmail.com  Wed Jun 14 10:27:29 2006
From: ecoinformatics at gmail.com (Xiaohua Dai)
Date: Wed, 14 Jun 2006 10:27:29 +0200
Subject: [R] set.seed
Message-ID: <15f8e67d0606140127r5b8f4e64qa8eda1ec9e734264@mail.gmail.com>

Hi R users,

Sorry for a simple question:
I found different people use different i in set.seed(i), are there any
rules to choose an i or one can choose as he likes?

Thanks
Xiaohua


From Ted.Harding at nessie.mcc.ac.uk  Wed Jun 14 11:06:07 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 14 Jun 2006 10:06:07 +0100 (BST)
Subject: [R] Decimal series how to make.........
In-Reply-To: <20060614061719.13910.qmail@webmail26.rediffmail.com>
Message-ID: <XFMail.060614100607.Ted.Harding@nessie.mcc.ac.uk>

On 14-Jun-06 anil kumar rohilla wrote:
>   Hi List,
>       I am new to this Rsoftware, i want to make a sereis for example
> which is having values like this, s<- 0,0.1,0.2,0.3,0.4........,1 
> 
> i tryed this statement
> s<-0:0.1:1
> but this giving an error megssage.
> but by default increment 1 it is taking ,,,,,so what to do ,,
> 
> i want to use this varible in for loop..
> 
> like for(j in s)

As well as the use of 'seq' which other have suggested,
a nice clean technique is

  s <- 0.1*(0:10)

which also is easier to type than

  s <- seq(0,1,by=0.1)

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 14-Jun-06                                       Time: 10:05:56
------------------------------ XFMail ------------------------------


From andy_liaw at merck.com  Wed Jun 14 11:13:32 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 14 Jun 2006 05:13:32 -0400
Subject: [R] A whine and a request
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA01BD0E0E@usctmx1106.merck.com>

I'm not sure if I understand your problem exactly, so I'm taking a wild
guess:  When you coerce a 2-d table object to data frame, what you get are
three columns: the two class variables and the frequencies.  If so, this
might help:
 
> x <- sample(1:2, 20, replace=TRUE)
> y <- sample(1:3, 20, replace=TRUE)
> tab <- table(x, y)
> as.data.frame(tab)
  x y Freq
1 1 1    2
2 2 1    2
3 1 2    7
4 2 2    2
5 1 3    3
6 2 3    4
> as.data.frame(unclass(tab))
  1 2 3
1 2 7 3
2 2 2 4

Andy

  _____  

From: r-help-bounces at stat.math.ethz.ch on behalf of John Vokey
Sent: Wed 6/14/2006 2:53 AM
To: r-help at stat.math.ethz.ch
Subject: [R] A whine and a request [Broadcast]



guRus and useRs, 

As I instruct my students: "With R what is difficult in anything  
else, is easy, usually one line of code; but, what is easy in  
anything else, is *&^%$%#$... (translation: next to impossible) in R." 

I just ran into such a case that has, to put it mildly, driven me to  
use spreadsheets (if that is not a sign of complete desperation, I  
don't know what is).  It has nothing to do with particular versions  
of R or what OS I am using, but something more fundamental.  Here it  
is.  First, I have a very large data-set (the size, though, is not  
the issue) that at base has a single binary response (score 0/1) for  
each line of the data.  Different, large blocks of the data  
correspond to the responses of single participants to stimuli  
differing in feature x, where the distribution of particular  
differences in feature x is different for each participant. Table  
(and ftable) commands are great for reducing these data to isolate  
the Freqs with which particular x values occur within each  
participant, which is what I want, including how the frequency with  
which the participant responded 1 (or 0) to that level of feature x.   
So, assiduous use of table() gives me more or less what I want:  
within participant, the values of x, the associated frequencies of 0  
responses to those values of x, and the associated frequencies of 1  
responses to those values of x.  Now, what I WANT is the simple  
regression of the proportion of 1 responses (out of the sum of 0 and  
1 responses) to each value of x on the values of x.  What table()  
gives me is close, but useless, as I can't access the columns.  But,  
applying data.frame() to the results from table, gives me access to  
the columns as variables, but returns the (re)expanded data, which  
does not allow me to compute the simple proportions I want.  After  
too many hours of trying way too many desperate commands, I gave up.   
I printed the table(), copied it to the clipboard, pasted it into a  
spreadsheet, computed the proportions, fixed the header, saved it as  
a text file, and the read it back into R to do the regressions.   
There has got to be an easier way. 
-- 
Please avoid sending me Word or PowerPoint attachments. 
See <http://www.gnu.org/philosophy/no-word-attachments.html
<http://www.gnu.org/philosophy/no-word-attachments.html> > 

-Dr. John R. Vokey 

______________________________________________ 
R-help at stat.math.ethz.ch mailing list 
https://stat.ethz.ch/mailman/listinfo/r-help
<https://stat.ethz.ch/mailman/listinfo/r-help>  
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
<http://www.R-project.org/posting-guide.html>


From ronggui.huang at gmail.com  Wed Jun 14 11:23:02 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Wed, 14 Jun 2006 17:23:02 +0800
Subject: [R] could someone tell me how to implement a multiple
	comparison test for proportions in a 2xc crosstabulation
In-Reply-To: <1bff52c20606140101r25188de1g6573c52f3cb5d152@mail.gmail.com>
References: <1bff52c20606140033j3ac8e9fdxa85d4edded2ad94a@mail.gmail.com>
	<1bff52c20606140101r25188de1g6573c52f3cb5d152@mail.gmail.com>
Message-ID: <38b9f0350606140223g507112dau3478586790daad3d@mail.gmail.com>

?p.adjust
It's the function used to Adjust P-values for Multiple Comparisons.


2006/6/14, xingwang ye <xwyefelix at gmail.com>:
> Dear all,
> I wanna to do multiple comparison test for proportions (multiple chi
> squre ?), could someone tell me how in R, thank you!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
??????
Deparment of Sociology
Fudan University


From rwan at kuicr.kyoto-u.ac.jp  Wed Jun 14 11:28:33 2006
From: rwan at kuicr.kyoto-u.ac.jp (Raymond Wan)
Date: Wed, 14 Jun 2006 18:28:33 +0900 (JST)
Subject: [R] Cramer-von Mises normality test
In-Reply-To: <885B6BC6-39C0-4CEC-8734-D5E2F56C07F7@horizonenv.com>
References: <885B6BC6-39C0-4CEC-8734-D5E2F56C07F7@horizonenv.com>
Message-ID: <Pine.LNX.4.64.0606141818290.25546@vancouver.kuicr.kyoto-u.ac.jp>


Hi Robert,

On Tue, 13 Jun 2006, Robert Powell wrote:
> I've been running some normality tests using the nortest package. For
> some of my datasets the Cramer-von Mises normality test generates an
> extremely high probability (e.g., 1.637e+31) and indicates normality
> when the other tests do not. Is there something I'm misunderstanding
> or potentially a bug in the code?
>
> Below are the results of the tests and then, below that, I've
> provided that column of the datafile.

 	Well, I can't answer all of your questions, but I did try your 
sample and at least your installation does the same thing as mine.  That 
is, for your sample data, I got the same results.

 	Each test is slightly different and looks for different 
characteristics.  So, it is not unusual for one test to indicate 
normality, but another one doesn't.  Granted, the difference in p-value is 
quite large...

 	I plotted the histogram of your data and to me, it doesn't "look" 
very normal...not very scientific, I know...

 	If you can locate it, I suggest finding the book "Testing for 
Normality" which the Nortest package cites.  It is quite good and might 
explain something to you...unfortunately, neither it nor any source I've 
found gives a simple table that says:  Use test X if you want to see if 
the Y property of the distribution is normal.  (Various tests seems to 
place different emphasis on different things...)

Ray


From ronggui.huang at gmail.com  Wed Jun 14 11:38:02 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Wed, 14 Jun 2006 17:38:02 +0800
Subject: [R] set.seed
In-Reply-To: <15f8e67d0606140127r5b8f4e64qa8eda1ec9e734264@mail.gmail.com>
References: <15f8e67d0606140127r5b8f4e64qa8eda1ec9e734264@mail.gmail.com>
Message-ID: <38b9f0350606140238v7933592ej2c2d6ca53a423186@mail.gmail.com>

set.seed is used to set the random number seed.
When we use functions ,say runif, to generate random number ,we almost
get different set of random number.
> runif(5)
[1] 0.2096388 0.3427873 0.5455948 0.7694844 0.4287647
> runif(5)
[1] 0.6864617 0.5218690 0.7965364 0.9030520 0.4324572

But in some cases,we want the results reproducible.so we use set.seed
before generate the number.
>set.seed(100)
> runif(5)
[1] 0.30776611 0.25767250 0.55232243 0.05638315 0.46854928
> set.seed(100)
> runif(5)
[1] 0.30776611 0.25767250 0.55232243 0.05638315 0.46854928

So if you set the same random number seed before you generate the
number,you get the same result.(That's why we call it pesudo-random
number.)

As for what the i in set.seed(i) should be,I don't think it is a serious matter.



2006/6/14, Xiaohua Dai <ecoinformatics at gmail.com>:
> Hi R users,
>
> Sorry for a simple question:
> I found different people use different i in set.seed(i), are there any
> rules to choose an i or one can choose as he likes?
>
> Thanks
> Xiaohua
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
??????
Deparment of Sociology
Fudan University


From xthua111 at zju.edu.cn  Wed Jun 14 11:54:02 2006
From: xthua111 at zju.edu.cn (Xiaoting Hua)
Date: Wed, 14 Jun 2006 17:54:02 +0800
Subject: [R] could someone tell me how to implement a multiple
	comparison test for proportions in a 2xc crosstabulation
In-Reply-To: <1bff52c20606140101r25188de1g6573c52f3cb5d152@mail.gmail.com>
References: <1bff52c20606140033j3ac8e9fdxa85d4edded2ad94a@mail.gmail.com>
	<1bff52c20606140101r25188de1g6573c52f3cb5d152@mail.gmail.com>
Message-ID: <53ff794a0606140254l10f52a1eue11fb6bc29118c1f@mail.gmail.com>

Maybe you want the package "multcomp".

On 6/14/06, xingwang ye <xwyefelix at gmail.com> wrote:
> Dear all,
> I wanna to do multiple comparison test for proportions (multiple chi
> squre ?), could someone tell me how in R, thank you!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
????????
??????
??????????
??????????????
??????????
310029


From B.Rowlingson at lancaster.ac.uk  Wed Jun 14 11:57:20 2006
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 14 Jun 2006 10:57:20 +0100
Subject: [R] set.seed
In-Reply-To: <38b9f0350606140238v7933592ej2c2d6ca53a423186@mail.gmail.com>
References: <15f8e67d0606140127r5b8f4e64qa8eda1ec9e734264@mail.gmail.com>
	<38b9f0350606140238v7933592ej2c2d6ca53a423186@mail.gmail.com>
Message-ID: <448FDD80.90707@lancaster.ac.uk>

ronggui wrote:
> set.seed is used to set the random number seed.
> When we use functions ,say runif, to generate random number ,we almost
> get different set of random number.

> As for what the i in set.seed(i) should be,I don't think it is a serious 
> matter.


The help for set.seed tells you all you need to know. 'i' must be a 
single value "interpreted as an integer".

  You can give it a decimal number, but it makes it an integer:

 > set.seed(pi)
 > runif(2)
[1] 0.1680415 0.8075164
 > set.seed(3)
 > runif(2)
[1] 0.1680415 0.8075164

  But not too big an 'integer':

  > set.seed(1e100)
  Error in set.seed(1e+100) : supplied seed is not a valid integer
  In addition: Warning message:
  NAs introduced by coercion

  because 1e100 isn't represented as an integer internally (in C/Fortran 
code, its a 'float' or'double precision' type of thing.

  For me it takes signed 32 bit integers, so the limits are +/- 2147483647:

  > set.seed(2147483647)
  > set.seed(-2147483647)
  > set.seed(-2147483648)
  Error in set.seed(-2147483648) : supplied seed is not a valid integer
  In addition: Warning message:
  NAs introduced by coercion
  > set.seed(2147483648)
  Error in set.seed(2147483648) : supplied seed is not a valid integer
  In addition: Warning message:
  NAs introduced by coercion

A 32 bit integer gives you over 4 billion possible random sequences. Is 
that enough?

Barry


From anil_rohilla at rediffmail.com  Tue Jun 13 07:24:26 2006
From: anil_rohilla at rediffmail.com (anil kumar rohilla)
Date: 13 Jun 2006 05:24:26 -0000
Subject: [R] how to make a series with decimal increment
Message-ID: <20060613052426.26372.qmail@webmail62.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060613/8b8a91cc/attachment.pl 

From Sebastian.Leuzinger at unibas.ch  Wed Jun 14 12:41:34 2006
From: Sebastian.Leuzinger at unibas.ch (Sebastian Leuzinger)
Date: Wed, 14 Jun 2006 12:41:34 +0200
Subject: [R] write data from function into external table
Message-ID: <200606141241.34984.Sebastian.Leuzinger@unibas.ch>

Dear list,
My apologies if a solution / explanation to this already exists on the list, 
but it is difficult to assign it to a certain keyword.

test<-c(1:3)
testfct <- function(x) {test[1]<-100}
 test
[1] 1 2 3
 testfct(1)
[1] 1 2 3

Basically, I would like to write data into an external table that the function 
does not know. Why is this not working / what alternatives exist?

Thanks, Sebastian 

------------------------------------------------
Sebastian Leuzinger
University of Basel, Department of Environmental Science
Institute of Botany
Sch?nbeinstr. 6 CH-4056 Basel
ph    0041 (0) 61 2673511
fax   0041 (0) 61 2673504
email Sebastian.Leuzinger at unibas.ch 
web   http://pages.unibas.ch/botschoen/leuzinger


From raspberry at webmail.co.za  Wed Jun 14 12:41:14 2006
From: raspberry at webmail.co.za (toby marthews)
Date: Wed, 14 Jun 2006 12:41:14 +0200
Subject: [R] R internal clock time discrepancy
Message-ID: <web-30039113@cgp7.sentechsa.net>

Dear R:

I'm trying to map some time series data on to dates and
because I'm using R's "strptime' facility I get an hour's
break in my time series: my readings for between 1am and
2am on 27th March 2005 won't map onto anything (I'm using a
data logger to record temperature and other things in a
forest).

As far as I know, the clock's didn't change on 27th March
2005 so why is this happening?

> start=strptime("2005-03-27 00:00","%Y-%m-%d %H:%M")
> end=strptime("2005-03-27 02:30","%Y-%m-%d %H:%M")
> seq(from=start,to=end,by=30*60)
[1] "2005-03-27 00:00:00 GMT Standard Time" "2005-03-27
00:30:00 GMT Standard Time" "2005-03-27 02:00:00 GMT
Standard Time"
[4] "2005-03-27 02:30:00 GMT Standard Time"
> as.numeric(seq(from=start,to=end,by=30*60))
[1] 1111881600 1111883400 1111885200 1111887000
> as.numeric(seq(from=start,to=end,by=30*60))/(30*60)
[1] 617712 617713 617714 617715

Any help much appreciated!

Toby Marthews
___________________________________________________________________
For super low premiums, click here http://www.webmail.co.za/dd.pwm

http://www.webmail.co.za the South African FREE email service


From dimitris.rizopoulos at med.kuleuven.be  Wed Jun 14 12:59:39 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 14 Jun 2006 12:59:39 +0200
Subject: [R] write data from function into external table
References: <200606141241.34984.Sebastian.Leuzinger@unibas.ch>
Message-ID: <001701c68fa1$a1a29050$0540210a@www.domain>

maybe you're looking for something like,

test <- 1:3
testfct <- function(x){
    x[1] <- 100
    x
}
############
test
testfct(1)
testfct(test)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Sebastian Leuzinger" <Sebastian.Leuzinger at unibas.ch>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, June 14, 2006 12:41 PM
Subject: [R] write data from function into external table


Dear list,
My apologies if a solution / explanation to this already exists on the 
list,
but it is difficult to assign it to a certain keyword.

test<-c(1:3)
testfct <- function(x) {test[1]<-100}
 test
[1] 1 2 3
 testfct(1)
[1] 1 2 3

Basically, I would like to write data into an external table that the 
function
does not know. Why is this not working / what alternatives exist?

Thanks, Sebastian

------------------------------------------------
Sebastian Leuzinger
University of Basel, Department of Environmental Science
Institute of Botany
Sch?nbeinstr. 6 CH-4056 Basel
ph    0041 (0) 61 2673511
fax   0041 (0) 61 2673504
email Sebastian.Leuzinger at unibas.ch
web   http://pages.unibas.ch/botschoen/leuzinger

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ramasamy at cancer.org.uk  Wed Jun 14 13:01:27 2006
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 14 Jun 2006 11:01:27 +0000
Subject: [R] write data from function into external table
In-Reply-To: <200606141241.34984.Sebastian.Leuzinger@unibas.ch>
References: <200606141241.34984.Sebastian.Leuzinger@unibas.ch>
Message-ID: <1150282887.3501.113.camel@dhcp-82.wolf.ox.ac.uk>

What is your desired output ? This will clarify the problem greatly.

Perhaps, this might be of some use :

 f <- function(v, pos, val=100){  v[pos] <- val; return(v)  }

 test <- 1:3
 test <- f(test, 1)
 test
 [1] 100  2  3

Regards, ADai



On Wed, 2006-06-14 at 12:41 +0200, Sebastian Leuzinger wrote:
> Dear list,
> My apologies if a solution / explanation to this already exists on the list, 
> but it is difficult to assign it to a certain keyword.
> 
> test<-c(1:3)
> testfct <- function(x) {test[1]<-100}
>  test
> [1] 1 2 3
>  testfct(1)
> [1] 1 2 3
> 
> Basically, I would like to write data into an external table that the function 
> does not know. Why is this not working / what alternatives exist?
> 
> Thanks, Sebastian 
> 
> ------------------------------------------------
> Sebastian Leuzinger
> University of Basel, Department of Environmental Science
> Institute of Botany
> Sch?nbeinstr. 6 CH-4056 Basel
> ph    0041 (0) 61 2673511
> fax   0041 (0) 61 2673504
> email Sebastian.Leuzinger at unibas.ch 
> web   http://pages.unibas.ch/botschoen/leuzinger
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From Christoph.Ort at eawag.ch  Wed Jun 14 13:24:11 2006
From: Christoph.Ort at eawag.ch (Ort Christoph)
Date: Wed, 14 Jun 2006 13:24:11 +0200
Subject: [R] Estimate region of highest probabilty density
Message-ID: <F357093629D891468190B597607CB6641808BD@EA-MAIL.eawag.wroot.emp-eaw.ch>

Estimate region of highest probabilty density

Dear R-community

I have data consisting of x and y. To each pair (x,y) a z value (weight) is assigned. With kde2d I can estimate the densities on a regular grid and based on this make a contour plot (not considering the z-values). According to an earlier post in the list I adjusted the kde2d to kde2d.weighted (see code below) to estimate the densities of x and y considering their weights z. There's also a piece of code with artificial data.

Now my question: Does a function exist which calculates the value of the level corresponding to a certain percentage of the volume (i.e. above this level e.g. 95% of the total volume lie, the (parameter) region of highest probability density)? 

And secondly: How is it in the case of n parameters, when I don't want to plot it anymore but estimate quantiles for each parameter considering also the weight z (to each set of parameters c(v,w,x,y) a weight z is assigned)?

Many thanks in advance, I am very grateful for any hint
Chris




# MASS::kde2d copied and modified
# ===============================

library(MASS)

kde2d.weighted <- function (x, y, w, h, n = n, lims = c(range(x), range(y))) {
  nx <- length(x)
  if (length(y) != nx) 
      stop("data vectors must be the same length")
  gx <- seq(lims[1], lims[2], length = n) # gridpoints x
  gy <- seq(lims[3], lims[4], length = n) # gridpoints y
  if (missing(h)) 
    h <- c(bandwidth.nrd(x), bandwidth.nrd(y));
  if (missing(w)) 
    w <- numeric(nx)+1;
  h <- h/4
  ax <- outer(gx, x, "-")/h[1] # distance of each point to each grid point in x-direction
  ay <- outer(gy, y, "-")/h[2] # distance of each point to each grid point in y-direction
  z <- (matrix(rep(w,n), nrow=n, ncol=nx, byrow=TRUE)*matrix(dnorm(ax), n, nx)) %*% t(matrix(dnorm(ay), n, nx))/(sum(w) * h[1] * h[2]) # z is the density
  return(list(x = gx, y = gy, z = z))
}


# Generate artificial data
# ========================

x <- runif(20000,-5,5)
y <- runif(20000,-5,5)
z <- dnorm(x, mean=0, sd=1)*dnorm(y, mean=0, sd=1)

# plot data
# =========

library(Rcmdr)
scatter3d(x=x,z=y,y=z,surface=FALSE,xlab="x",ylab="z",zlab="y",bg.col="black")

temp0 <- kde2d(x=x, y=y, n = 100, lims=c(range(x),range(y))) 
contour(x=temp0$x, y=temp0$y, z=temp0$z, xlab="x", ylab="y", main="z")

temp <- kde2d.weighted(x=x, y=y, w=z, n = 100, lims=c(range(x),range(y))) 
contour(x=temp$x, y=temp$y, z=temp$z, xlab="x", ylab="y", main="z", col="red", add=TRUE)





???
Christoph Ort
Eawag - aquatic research
Environmental Engineering
?berlandstrasse 133
CH-8600 D?bendorf

phone: +41-44-823-5041          
fax:   +41-44-823-5389
cell:  +41-79-218-9242
 
mailto:christoph.ort at eawag.ch

http://www.eawag.ch/~ortchris/

http://www.eawag.ch 




???
Christoph Ort
Eawag - aquatic research
Environmental Engineering
?berlandstrasse 133
CH-8600 D?bendorf

phone: +41-44-823-5041          
fax:   +41-44-823-5389
cell:  +41-79-218-9242
 
mailto:christoph.ort at eawag.ch

http://www.eawag.ch/~ortchris/

http://www.eawag.ch


From ggrothendieck at gmail.com  Wed Jun 14 13:55:38 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 14 Jun 2006 07:55:38 -0400
Subject: [R] Decimal series how to make.........
In-Reply-To: <XFMail.060614100607.Ted.Harding@nessie.mcc.ac.uk>
References: <20060614061719.13910.qmail@webmail26.rediffmail.com>
	<XFMail.060614100607.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <971536df0606140455l1480eec4h15dda5e7b1f691a@mail.gmail.com>

On 6/14/06, Ted Harding <Ted.Harding at nessie.mcc.ac.uk> wrote:
> On 14-Jun-06 anil kumar rohilla wrote:
> >   Hi List,
> >       I am new to this Rsoftware, i want to make a sereis for example
> > which is having values like this, s<- 0,0.1,0.2,0.3,0.4........,1
> >
> > i tryed this statement
> > s<-0:0.1:1
> > but this giving an error megssage.
> > but by default increment 1 it is taking ,,,,,so what to do ,,
> >
> > i want to use this varible in for loop..
> >
> > like for(j in s)
>
> As well as the use of 'seq' which other have suggested,
> a nice clean technique is
>
>  s <- 0.1*(0:10)
>
> which also is easier to type than
>
>  s <- seq(0,1,by=0.1)
>

or 0:10/10 which is even easier to type although perhaps not as clear.


From ggrothendieck at gmail.com  Wed Jun 14 14:22:24 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 14 Jun 2006 08:22:24 -0400
Subject: [R] write data from function into external table
In-Reply-To: <200606141241.34984.Sebastian.Leuzinger@unibas.ch>
References: <200606141241.34984.Sebastian.Leuzinger@unibas.ch>
Message-ID: <971536df0606140522xb563f55ue5cf4f2e528ae146@mail.gmail.com>

Here are two alternatives.  See ?"<<-"

testfct1 <- function() test[1] <<- 100

# following one can be written more compactly as
#   testfct2 <- function(test) replace(test, 1, 100)
testfct2 <- function(test) { test[1] <- 100; test }

# test
test <- 1:3; testfct1(); test
test <- 1:3; test <- testfct2(test); test

On 6/14/06, Sebastian Leuzinger <Sebastian.Leuzinger at unibas.ch> wrote:
> Dear list,
> My apologies if a solution / explanation to this already exists on the list,
> but it is difficult to assign it to a certain keyword.
>
> test<-c(1:3)
> testfct <- function(x) {test[1]<-100}
>  test
> [1] 1 2 3
>  testfct(1)
> [1] 1 2 3
>
> Basically, I would like to write data into an external table that the function
> does not know. Why is this not working / what alternatives exist?
>
> Thanks, Sebastian
>
> ------------------------------------------------
> Sebastian Leuzinger
> University of Basel, Department of Environmental Science
> Institute of Botany
> Sch?nbeinstr. 6 CH-4056 Basel
> ph    0041 (0) 61 2673511
> fax   0041 (0) 61 2673504
> email Sebastian.Leuzinger at unibas.ch
> web   http://pages.unibas.ch/botschoen/leuzinger
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From Sebastian.Leuzinger at unibas.ch  Wed Jun 14 14:36:06 2006
From: Sebastian.Leuzinger at unibas.ch (Sebastian Leuzinger)
Date: Wed, 14 Jun 2006 14:36:06 +0200
Subject: [R] write data from function into external table
In-Reply-To: <971536df0606140522xb563f55ue5cf4f2e528ae146@mail.gmail.com>
References: <200606141241.34984.Sebastian.Leuzinger@unibas.ch>
	<971536df0606140522xb563f55ue5cf4f2e528ae146@mail.gmail.com>
Message-ID: <200606141436.06478.Sebastian.Leuzinger@unibas.ch>

Thanks for the hints, they are useful to me. However, I still do not 
understand why my approach fails. In a user defined function, R does not seem 
to want to write into an object defined outside this function. (see my first 
example below). I guess there is some logic behind this.

On Wednesday 14 June 2006 14:22, you wrote:
> Here are two alternatives.  See ?"<<-"
>
> testfct1 <- function() test[1] <<- 100
>
> # following one can be written more compactly as
> #   testfct2 <- function(test) replace(test, 1, 100)
> testfct2 <- function(test) { test[1] <- 100; test }
>
> # test
> test <- 1:3; testfct1(); test
> test <- 1:3; test <- testfct2(test); test
>
> On 6/14/06, Sebastian Leuzinger <Sebastian.Leuzinger at unibas.ch> wrote:
> > Dear list,
> > My apologies if a solution / explanation to this already exists on the
> > list, but it is difficult to assign it to a certain keyword.
> >
> > test<-c(1:3)
> > testfct <- function(x) {test[1]<-100}
> >  test
> > [1] 1 2 3
> >  testfct(1)
> > [1] 1 2 3
> >
> > Basically, I would like to write data into an external table that the
> > function does not know. Why is this not working / what alternatives
> > exist?
> >
> > Thanks, Sebastian
> >
> > ------------------------------------------------
> > Sebastian Leuzinger
> > University of Basel, Department of Environmental Science
> > Institute of Botany
> > Sch?nbeinstr. 6 CH-4056 Basel
> > ph    0041 (0) 61 2673511
> > fax   0041 (0) 61 2673504
> > email Sebastian.Leuzinger at unibas.ch
> > web   http://pages.unibas.ch/botschoen/leuzinger
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html

-- 
------------------------------------------------
Sebastian Leuzinger
University of Basel, Department of Environmental Science
Institute of Botany
Sch?nbeinstr. 6 CH-4056 Basel
ph    0041 (0) 61 2673511
fax   0041 (0) 61 2673504
email Sebastian.Leuzinger at unibas.ch 
web   http://pages.unibas.ch/botschoen/leuzinger


From hstevens at muohio.edu  Wed Jun 14 14:38:29 2006
From: hstevens at muohio.edu (Martin Henry H. Stevens)
Date: Wed, 14 Jun 2006 08:38:29 -0400
Subject: [R] lmer binomial model overestimating data?
Message-ID: <0E50C39C-FFD9-43E9-BA06-243DD1DD2A8E@muohio.edu>

Hi folks,
Warning: I don't know if the result I am getting makes sense, so this  
may be a statistics question.

The fitted values from my binomial lmer mixed model seem to  
consistently overestimate the cell means, and I don't know why. I  
assume I am doing something stupid.

Below I include code, and a binary image of the data is available at  
this link:
http://www.cas.muohio.edu/~stevenmh/tf.RdataBin

This was done with `Matrix' version 0.995-10 and `lme4' version  
0.995-2. and R v. 2.3.1 on a Mac, OS 10.4.6.

The binomial model below ("mod") was reduced from a more complex one  
by first using AIC, BIC and LRT for "random" effects, and then  
relying on Helmert contrasts and AIC, BIC, and LRT to simplify fixed  
effects. Maybe this was wrong?

 > load("tf.RdataBin")
 > library(lme4)

 > options(contrasts=c("contr.helmert", "contr.poly"))
 > mod <- lmer(tfb ~ reg+nutrient+amd +reg:nutrient+
+     (1|rack) + (1|popu) +  (1|gen), data=dat.tb2, family=binomial,  
method="Laplace")
 > summary(mod)
Generalized linear mixed model fit using Laplace
Formula: tfb ~ reg + nutrient + amd + reg:nutrient + (1 | rack) + (1  
|      popu) + (1 | gen)
	  Data: dat.tb2
Family: binomial(logit link)
     AIC    BIC  logLik deviance
402.53 446.64 -191.26   382.53
Random effects:
Groups Name        Variance Std.Dev.
gen    (Intercept) 0.385    0.621
popu   (Intercept) 0.548    0.741
rack   (Intercept) 0.401    0.633
number of obs: 609, groups: gen, 24; popu, 9; rack, 2

Estimated scale (compare to 1)  0.80656

Fixed effects:
                Estimate Std. Error z value Pr(>|z|)
(Intercept)       2.391      0.574    4.17  3.1e-05
reg1              0.842      0.452    1.86  0.06252
reg2              0.800      0.241    3.32  0.00091
nutrient1         0.788      0.197    4.00  6.3e-05
amd1             -0.540      0.139   -3.88  0.00010
reg1:nutrient1    0.500      0.227    2.21  0.02734
reg2:nutrient1   -0.176      0.146   -1.21  0.22794

Correlation of Fixed Effects:
             (Intr) reg1   reg2   ntrnt1 amd1   rg1:n1
reg1         0.169
reg2        -0.066 -0.191
nutrient1    0.178  0.231 -0.034
amd1        -0.074 -0.044 -0.052 -0.078
reg1:ntrnt1  0.157  0.307 -0.180  0.562 -0.002
reg2:ntrnt1 -0.028 -0.154  0.236  0.141  0.033 -0.378
 > X <- mod @ X
 > fitted <- X %*% fixef(mod)
 >
 > unlogitH <- function(x) {( 1 + exp(-x) )^-1}
 > (result <- data.frame(Raw.Data=with(dat.tb2,
+                          tapply(tfb, list(reg:nutrient:amd),
+                          mean ) ),
+         Fitted.Estimates=with(dat.tb2,
+                          tapply(fitted, list(reg:nutrient:amd),
+                          function(x) unlogitH(mean(x))  ) )  ))
                Raw.Data Fitted.Estimates
SW:1:unclipped  0.50877          0.69520
SW:1:clipped    0.41304          0.43669
SW:8:unclipped  0.67273          0.85231
SW:8:clipped    0.52830          0.66233
NL:1:unclipped  0.88889          0.81887
NL:1:clipped    0.53571          0.60578
NL:8:unclipped  0.96552          0.98830
NL:8:clipped    0.96154          0.96635
SP:1:unclipped  0.98649          0.98361
SP:1:clipped    0.92537          0.95328
SP:8:unclipped  1.00000          0.99308
SP:8:clipped    0.95890          0.97992
 > ### Perhaps the cell SP:8:clipped = 1.0 is messing up the fit?
 > pdf("RawAndFitted.pdf")
 > par(mar=c(8,3,2,2), las=2)
 > barplot(t(result), beside=TRUE )
 > box(); title("Fractions of Plants Producing Fruits")
 > legend("topleft", c("Raw Data", "Fitted Values"),
+        fill=gray.colors(2), bty="n" )
 > dev.off()
quartz
      2
 >

                _
platform       powerpc-apple-darwin8.6.0
arch           powerpc
os             darwin8.6.0
system         powerpc, darwin8.6.0
status
major          2
minor          3.1
year           2006
month          06
day            01
svn rev        38247
language       R
version.string Version 2.3.1 (2006-06-01)
 >

Dr. M. Hank H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"


From nvj at fys.ku.dk  Wed Jun 14 14:46:51 2006
From: nvj at fys.ku.dk (Niels Vestergaard Jensen)
Date: Wed, 14 Jun 2006 14:46:51 +0200 (CEST)
Subject: [R] write data from function into external table
In-Reply-To: <200606141436.06478.Sebastian.Leuzinger@unibas.ch>
Message-ID: <Pine.LNX.4.44.0606141443460.4153-100000@scharff.fys.ku.dk>

On Wed, 14 Jun 2006, Sebastian Leuzinger wrote:

> Thanks for the hints, they are useful to me. However, I still do not
> understand why my approach fails. In a user defined function, R does not seem
> to want to write into an object defined outside this function. (see my first
> example below). I guess there is some logic behind this.

I think the general programming concept you are looking for is "scope",
see sec. 10.7 in the R intro:

http://cran.r-project.org/doc/manuals/R-intro.html#Scope

There's more (and more technical) about it in the R Language Definition.

best

	Niels


> On Wednesday 14 June 2006 14:22, you wrote:
> > Here are two alternatives.  See ?"<<-"
> >
> > testfct1 <- function() test[1] <<- 100
> >
> > # following one can be written more compactly as
> > #   testfct2 <- function(test) replace(test, 1, 100)
> > testfct2 <- function(test) { test[1] <- 100; test }
> >
> > # test
> > test <- 1:3; testfct1(); test
> > test <- 1:3; test <- testfct2(test); test
> >
> > On 6/14/06, Sebastian Leuzinger <Sebastian.Leuzinger at unibas.ch> wrote:
> > > Dear list,
> > > My apologies if a solution / explanation to this already exists on the
> > > list, but it is difficult to assign it to a certain keyword.
> > >
> > > test<-c(1:3)
> > > testfct <- function(x) {test[1]<-100}
> > >  test
> > > [1] 1 2 3
> > >  testfct(1)
> > > [1] 1 2 3
> > >
> > > Basically, I would like to write data into an external table that the
> > > function does not know. Why is this not working / what alternatives
> > > exist?
> > >
> > > Thanks, Sebastian
> > >
> > > ------------------------------------------------
> > > Sebastian Leuzinger
> > > University of Basel, Department of Environmental Science
> > > Institute of Botany
> > > Sch?nbeinstr. 6 CH-4056 Basel
> > > ph    0041 (0) 61 2673511
> > > fax   0041 (0) 61 2673504
> > > email Sebastian.Leuzinger at unibas.ch
> > > web   http://pages.unibas.ch/botschoen/leuzinger
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
>
> --
> ------------------------------------------------
> Sebastian Leuzinger
> University of Basel, Department of Environmental Science
> Institute of Botany
> Sch?nbeinstr. 6 CH-4056 Basel
> ph    0041 (0) 61 2673511
> fax   0041 (0) 61 2673504
> email Sebastian.Leuzinger at unibas.ch
> web   http://pages.unibas.ch/botschoen/leuzinger
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Wed Jun 14 15:13:45 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 14 Jun 2006 09:13:45 -0400
Subject: [R] write data from function into external table
In-Reply-To: <200606141436.06478.Sebastian.Leuzinger@unibas.ch>
References: <200606141241.34984.Sebastian.Leuzinger@unibas.ch>
	<971536df0606140522xb563f55ue5cf4f2e528ae146@mail.gmail.com>
	<200606141436.06478.Sebastian.Leuzinger@unibas.ch>
Message-ID: <971536df0606140613hf59ed2ehe646f845b8e314df@mail.gmail.com>

If functions don't modify their environment then its generally
easier to debug programs so it encourarges better programming.

On 6/14/06, Sebastian Leuzinger <Sebastian.Leuzinger at unibas.ch> wrote:
> Thanks for the hints, they are useful to me. However, I still do not
> understand why my approach fails. In a user defined function, R does not seem
> to want to write into an object defined outside this function. (see my first
> example below). I guess there is some logic behind this.
>
> On Wednesday 14 June 2006 14:22, you wrote:
> > Here are two alternatives.  See ?"<<-"
> >
> > testfct1 <- function() test[1] <<- 100
> >
> > # following one can be written more compactly as
> > #   testfct2 <- function(test) replace(test, 1, 100)
> > testfct2 <- function(test) { test[1] <- 100; test }
> >
> > # test
> > test <- 1:3; testfct1(); test
> > test <- 1:3; test <- testfct2(test); test
> >
> > On 6/14/06, Sebastian Leuzinger <Sebastian.Leuzinger at unibas.ch> wrote:
> > > Dear list,
> > > My apologies if a solution / explanation to this already exists on the
> > > list, but it is difficult to assign it to a certain keyword.
> > >
> > > test<-c(1:3)
> > > testfct <- function(x) {test[1]<-100}
> > >  test
> > > [1] 1 2 3
> > >  testfct(1)
> > > [1] 1 2 3
> > >
> > > Basically, I would like to write data into an external table that the
> > > function does not know. Why is this not working / what alternatives
> > > exist?
> > >
> > > Thanks, Sebastian
> > >
> > > ------------------------------------------------
> > > Sebastian Leuzinger
> > > University of Basel, Department of Environmental Science
> > > Institute of Botany
> > > Sch?nbeinstr. 6 CH-4056 Basel
> > > ph    0041 (0) 61 2673511
> > > fax   0041 (0) 61 2673504
> > > email Sebastian.Leuzinger at unibas.ch
> > > web   http://pages.unibas.ch/botschoen/leuzinger
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
>
> --
> ------------------------------------------------
> Sebastian Leuzinger
> University of Basel, Department of Environmental Science
> Institute of Botany
> Sch?nbeinstr. 6 CH-4056 Basel
> ph    0041 (0) 61 2673511
> fax   0041 (0) 61 2673504
> email Sebastian.Leuzinger at unibas.ch
> web   http://pages.unibas.ch/botschoen/leuzinger
>


From leaflovesun at yahoo.ca  Wed Jun 14 15:35:47 2006
From: leaflovesun at yahoo.ca (Leaf Sun)
Date: Wed, 14 Jun 2006 07:35:47 -0600
Subject: [R] number of iteration s exceeded maximum of 50
References: <1bff52c20606140033j3ac8e9fdxa85d4edded2ad94a@mail.gmail.com>
	<1bff52c20606140101r25188de1g6573c52f3cb5d152@mail.gmail.com>
	<38b9f0350606140223g507112dau3478586790daad3d@mail.gmail.com>
Message-ID: <200606140735459773486@yahoo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060614/d8999b3e/attachment.pl 

From Ted.Harding at nessie.mcc.ac.uk  Wed Jun 14 15:46:28 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 14 Jun 2006 14:46:28 +0100 (BST)
Subject: [R] Decimal series how to make.........
In-Reply-To: <971536df0606140455l1480eec4h15dda5e7b1f691a@mail.gmail.com>
Message-ID: <XFMail.060614144628.Ted.Harding@nessie.mcc.ac.uk>

On 14-Jun-06 Gabor Grothendieck wrote:
> On 6/14/06, Ted Harding <Ted.Harding at nessie.mcc.ac.uk> wrote:
>> On 14-Jun-06 anil kumar rohilla wrote:
>> >   Hi List,
>> >       I am new to this Rsoftware, i want to make a sereis for
>> >       example
>> > which is having values like this, s<- 0,0.1,0.2,0.3,0.4........,1
>> >
>> > i tryed this statement
>> > s<-0:0.1:1
>> > but this giving an error megssage.
>> > but by default increment 1 it is taking ,,,,,so what to do ,,
>> >
>> > i want to use this varible in for loop..
>> >
>> > like for(j in s)
>>
>> As well as the use of 'seq' which other have suggested,
>> a nice clean technique is
>>
>>  s <- 0.1*(0:10)
>>
>> which also is easier to type than
>>
>>  s <- seq(0,1,by=0.1)
>>
> 
> or 0:10/10 which is even easier to type although perhaps not as clear.

On those lines, I'd settle for (0:10)/10 -- I always make a point of
putting ranges like "0:10" in (...) since there are too many traps
for the unwary.
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 14-Jun-06                                       Time: 14:46:24
------------------------------ XFMail ------------------------------


From bail at fas.harvard.edu  Wed Jun 14 15:59:38 2006
From: bail at fas.harvard.edu (Christopher Bail)
Date: Wed, 14 Jun 2006 09:59:38 -0400
Subject: [R] fclustIndex(package e1071) error: LaPack Routine dgesv
Message-ID: <2596970B-CBD2-4C74-A4EE-EA6969AD1943@fas.harvard.edu>

Dear colleagues:

Despite my best efforts, I have not been able to understand/overcome  
an error message I received while running the fclustIndex command in  
package e1071, which produces validity measures for the fuzzy c-means  
clustering algorithm.  I am relatively new to R-- and still depend  
heavily on Rcmdr-- and so I apologize in advance if there is an  
obvious answer to my question:

My code is as follows:

library(e1071)
Dataset <- read.dta("/Users/chris/Desktop/Fuzzy Cluster.dta",  
convert.dates=TRUE, convert.factors=TRUE, missing.type=TRUE,  
convert.underscore=TRUE, warn.missing.labels=TRUE)
row.names(Dataset) <- as.character(Dataset$cntry)
Dataset$cntry <- NULL
cl <- cmeans(Dataset, 4, 20, verbose=TRUE, method="cmeans")
resultindexes <- fclustIndex(cl, Dataset, index= "all")
resultindexes

I am receiving the following error message:

ERROR:  Lapack routine dgesv

I have tried running the example for fclustIndex and it works fine.   
The code above is based on the example from the manual (below).  I  
also checked that I have the latest version installed.  I have also  
had no trouble running similar commands from other packages which  
leads me to believe that everything is installed/compiled properly--  
where have I gone wrong?

The description of fclustIndex in the e1071 .pdf is available via the  
following link:

http://rweb.stat.umn.edu/R/library/e1071/html/fclustIndex.html

I thank you in advance for your time and help.

Sincerely,
Christopher Bail


From rab45+ at pitt.edu  Wed Jun 14 15:59:34 2006
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Wed, 14 Jun 2006 09:59:34 -0400
Subject: [R] lmer and mixed effects logistic regression
Message-ID: <1150293574.3416.11.camel@localhost.localdomain>

I'm using FC4 and R 2.3.1 to fit a mixed effects logistic regression.
The response is 0/1 and both the response and the age are the same for
each pair of observations for each subject (some observations are not
paired). For example:

id response age
1    0      30
1    0      30

2    1      55
2    1      55

3    0      37

4    1      52

5    0      39
5    0      39

etc.

I get the following error:

> (lmer(response~(1|id)+age,data=gdx,family=binomial))
Error in deviance(.Call("mer_coefGets", x, pars, 2, PACKAGE =
"Matrix")) :
        Leading minor of order 2 in downdated X'X is not positive
definite

Similar problem if I use quasibinomial.

If I use glm, of course it thinks I have roughly twice the number of
subjects so the standard errors would be smaller than they should be.

I used SAS's NLMIXED and it converged without problems giving me
parameter estimates close to what glm gives, but with larger standard
errors. glmmPQL(MASS) gives very different parameter estimates.

Is it reasonable to fit a mixed effects model in this situation?

Is there some way to give starting values for lmer and/or glmmPQL?

Rick B.


From peterdlauren at yahoo.com  Wed Jun 14 16:53:27 2006
From: peterdlauren at yahoo.com (Peter Lauren)
Date: Wed, 14 Jun 2006 07:53:27 -0700 (PDT)
Subject: [R] missing value where TRUE/FALSE needed
In-Reply-To: <448FCF3A.12595.43C425@localhost>
Message-ID: <20060614145327.84534.qmail@web30304.mail.mud.yahoo.com>

Hi Petr,

You are correct.  There was an NA in BinMatrix.  I
made the mistake of only looking at the earlier
entries.  My problem was in this loop

for (i in 2:BinCnt)
{
    BinMatrix[i,1]=BinMatrix[i-1,2]
    BinMatrix[i,2]=BinMatrix[i,2]+Increment
}

It should have been
for (i in 2:BinCnt)
{
    BinMatrix[i,1]=BinMatrix[i-1,2]
    BinMatrix[i,2]=BinMatrix[i,1]+Increment
}

Thanks very much,
Peter.

--- Petr Pikal <petr.pikal at precheza.cz> wrote:

> Hi
> 
> as nobody has any of your variables and can not
> reproduce your code 
> you has to track your problems yourself. R is kindly
> provided you 
> with hint for it.
> 
> a<-1
> b<-2
> value<-2
> if(value>=a&&value<=b) print("right") else
> print("wrong")
> [1] "right"
> b<-NA
> if(value>=a&&value<=b) print("right") else
> print("wrong")
> Error in if (value >= a && value <= b)
> print("right") else 
> print("wrong") : 
>         missing value where TRUE/FALSE needed
> 
> So one of your variables is NA when evaluating if
> statement and 
> logical statement evaluates to NA but if expects
> true/false value.
> 
> Besides I wonder if you cannor get rid of for cycle.
> 
> 
> >
> 
> On 13 Jun 2006 at 12:19, Peter Lauren wrote:
> 
> Date sent:      	Tue, 13 Jun 2006 12:19:42 -0700
> (PDT)
> From:           	Peter Lauren
> <peterdlauren at yahoo.com>
> To:             	r-help at stat.math.ethz.ch
> Subject:        	[R] missing value where TRUE/FALSE
> needed
> 
> > I am using R 2.3.0 and trying to run the following
> > code.
> > 
> >  BinCnt=round(ShorterVectorLength/10)
> >  BinMatrix=matrix(nrow=BinCnt, ncol=2)
> >  Increment=(VectorRange[2]-VectorRange[1])/BinCnt
> >  BinMatrix[1,1]=VectorRange[1]
> >  BinMatrix[1,2]=VectorRange[1]+Increment
> >  for (i in 2:BinCnt)
> >  {
> >   BinMatrix[i,1]=BinMatrix[i-1,2]
> >   BinMatrix[i,2]=BinMatrix[i,2]+Increment
> >  }
> > 
> >  # Count entries in each bin
> >  Histogram=vector(length=BinCnt, mode="numeric")
> >  for (i in 1:BinCnt) Histogram[i]=0
> >  VectorLength=length(FormerVector)
> >  for (i in 1:VectorLength)
> >  {
> >   Value=FormerVector[i]
> >   for (j in 1:BinCnt) if (Value>=BinMatrix[j,1] &&
> > Value<=BinMatrix[j,2])
> >   {
> >    Histogram[j]=Histogram[j]+1
> >    break;
> >   }
> >  }
> > 
> > Unfotunately, at 
> > if (Value>=BinMatrix[j,1] &&
> Value<=BinMatrix[j,2])
> > I get the following error message
> > Error in if (Value >= BinMatrix[j, 1] && Value <=
> > BinMatrix[j, 2]) { : 
> >         missing value where TRUE/FALSE needed
> > I inserted browser() just before the call and 
> > (Value>=BinMatrix[j,1] && Value<=BinMatrix[j,2])
> > returned FALSE.
> > 
> > Is there a bug in my code or a bug in my version
> of R?
> 
> Neither. The code does not suit your data.
> 
> HTH
> Petr
> 
> 
> > 
> > Many thanks in advance,
> > Peter.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
>


From wolfram at fischer-zim.ch  Wed Jun 14 16:55:07 2006
From: wolfram at fischer-zim.ch (Wolfram Fischer)
Date: Wed, 14 Jun 2006 16:55:07 +0200
Subject: [R] merge dataframes with conditions formulated as logical
	expressions
Message-ID: <20060614145507.GA4029@s1x.local>

I have a data.frame df containing two variables:
    GRP: Factor
    VAL: num

I have a data.frame dp containing:
    GRP: Factor
    MIN.VAL: num
    MAX.VAL: num
    VAL2: num
with several rows per "GRP"
where dp[i-1, "MAX.VAL"] < dp[i, "MIN.VAL"]
within the same "GRP".

I want to create df[i, "VAL2"] <- dpp[z, "VAL2"] 
with    i along df 
and     dpp <- subset( dp, GRP = df[i, "GRP"] )
so that it is true for each i:
        df[i, "VAL"] > dpp[z, "MIN.VAL"]
   and  df[i, "VAL"] <= dpp[z, "MAX.VAL"]

Is there an easy/efficient way to do that?

Example:
df <- data.frame( GRP=c( "A", "A", "B" ), VAL=c( 10, 100, 200 ) )
dp <- data.frame( GRP=c( "A", "A", "B", "B" ),
    MIN.VAL=c( 1, 50, 1, 70 ), MAX.VAL=c( 49, 999, 59, 999 ), 
    VAL2=c( 1.1, 2.2, 3.3, 4.4 ) )

The result should be:
    df$VAL2 <- c( 1.1, 2.2, 4.4 )

Thanks - Wolfram


From Greg.Snow at intermountainmail.org  Wed Jun 14 16:56:20 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Wed, 14 Jun 2006 08:56:20 -0600
Subject: [R] data set size question
Message-ID: <07E228A5BE53C24CAD490193A7381BBB45D99C@LP-EXCHVS07.CO.IHC.COM>

If you need to analyze something bigger than memory can hold, one option
is the biglm package which will fit linear regression models (and a lot
of different analyses can be restructured as linear regression models)
on blocks of data so that the entire dataset is not in memory all at the
same time.

I tested it out with a database with over 23 million rows and it worked
great.  It computed the exact same answers (to about 7 decimal places, I
didn't bother to look beyond that) as a couple of other methods used for
the same values.



Hope this helps, 


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Carl Hauser
Sent: Tuesday, June 13, 2006 9:22 PM
To: r-help at stat.math.ethz.ch
Subject: [R] data set size question

Hi there,

I'm very new to R and am only in the beginning stages of investigating
it for possible use. A document by John Maindonald at the r-project
website entitled "Using R for Data Analysis and Graphics: Introduction,
Code and Commentary" contains the following paragraph, "The R system may
struggle to handle very large data sets. Depending on available computer
memory, the processing of a data set containing one hundred thousand
observations and perhaps twenty variables may press the limits of what R
can easily handle". This document was written in 2004.

My questions are:

Is this still the case? If so, has anyone come up with creative
solutions to mitigate these limitations? If you work with large data
sets in R, what have your experiences been?

>From what I've seen so far, R seems to have enormous potential and
capabilities. I routinely work with data sets of several hundred
thousand to several million. It would be unfortunate if such potential
and capabilities were not realized because of (effective) data set size
limitations.

Please tell me it ain't so.

Thanks for any help or suggestions.

Carl

 

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From btyner at gmail.com  Wed Jun 14 17:23:04 2006
From: btyner at gmail.com (Benjamin Tyner)
Date: Wed, 14 Jun 2006 11:23:04 -0400
Subject: [R] positioning of separate y-axis labels in xyplot
Message-ID: <449029D8.7030203@stat.purdue.edu>

I like the functionality provided by outer=TRUE, but when it comes time 
to place separate xlabs or ylabs, I always end up 'eyeballing' it on a 
case-by-case basis. For example,

##begin example
require(lattice)

cars.lo <- loess(dist ~ speed, cars)

print(xyplot(cars.lo$residuals+cars.lo$fitted~cars.lo$x,
             strip=FALSE,
             outer=TRUE,
             layout=c(1,2),
             ylab="",
             scales=list(y=list(relation="free",rot=0)),
             panel=function(x,y,panel.number,...){
                   if(panel.number==1){
                      panel.xyplot(x,y)
                      panel.abline(h=0)
                   }else{
                      panel.xyplot(x,y=cars.lo$y)
                      panel.xyplot(x,y,type="l")
                   }
             }))

require(grid)
trellis.focus("panel", 1, 1, clip.off=TRUE, highlight=FALSE)
grid.text("residuals", x=unit(0, "npc") + unit(-2, "lines"),rot=90)
trellis.focus("panel", 1, 2, clip.off=TRUE, highlight=FALSE)
grid.text("fitted", x=unit(0, "npc") + unit(-2, "lines"),rot=90)
## end example

In this case, a distance of -2 lines happens to be enough, but one has 
to make the plot to know this. I'm interested in learning how one can 
place the ylabs without fear of overlapping the tick labels; i.e., how 
to use the exact space allocated by ylab="". I'm thinking it must 
involve viewports?

Ben


From ggrothendieck at gmail.com  Wed Jun 14 17:26:52 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 14 Jun 2006 11:26:52 -0400
Subject: [R] Decimal series how to make.........
In-Reply-To: <XFMail.060614144628.Ted.Harding@nessie.mcc.ac.uk>
References: <971536df0606140455l1480eec4h15dda5e7b1f691a@mail.gmail.com>
	<XFMail.060614144628.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <971536df0606140826i63d21becm2ae9c1715d17db83@mail.gmail.com>

I agree although if you use this construct a lot then you will
likely want to adopt the easier-to-write 0:10/10 style as an
idiom.  I find this is particularly the case with rq from the
quantreg package where its convenient to use it in
specifying the tau= arg:

library(quantreg)
data(engel)
rq(y ~ x, data = engel, tau = 1:9/10)


On 6/14/06, Ted Harding <Ted.Harding at nessie.mcc.ac.uk> wrote:
> On 14-Jun-06 Gabor Grothendieck wrote:
> > On 6/14/06, Ted Harding <Ted.Harding at nessie.mcc.ac.uk> wrote:
> >> On 14-Jun-06 anil kumar rohilla wrote:
> >> >   Hi List,
> >> >       I am new to this Rsoftware, i want to make a sereis for
> >> >       example
> >> > which is having values like this, s<- 0,0.1,0.2,0.3,0.4........,1
> >> >
> >> > i tryed this statement
> >> > s<-0:0.1:1
> >> > but this giving an error megssage.
> >> > but by default increment 1 it is taking ,,,,,so what to do ,,
> >> >
> >> > i want to use this varible in for loop..
> >> >
> >> > like for(j in s)
> >>
> >> As well as the use of 'seq' which other have suggested,
> >> a nice clean technique is
> >>
> >>  s <- 0.1*(0:10)
> >>
> >> which also is easier to type than
> >>
> >>  s <- seq(0,1,by=0.1)
> >>
> >
> > or 0:10/10 which is even easier to type although perhaps not as clear.
>
> On those lines, I'd settle for (0:10)/10 -- I always make a point of
> putting ranges like "0:10" in (...) since there are too many traps
> for the unwary.
> Ted.
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 14-Jun-06                                       Time: 14:46:24
> ------------------------------ XFMail ------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From jeffmiller at adsam.com  Wed Jun 14 17:33:47 2006
From: jeffmiller at adsam.com (Jeff Miller)
Date: Wed, 14 Jun 2006 11:33:47 -0400
Subject: [R] storing/retrieving simulation data
Message-ID: <000501c68fc7$ee4a7a00$6501a8c0@AdSAMJeff>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060614/9bf3d587/attachment.pl 

From hannesalazar at gmail.com  Wed Jun 14 17:35:21 2006
From: hannesalazar at gmail.com (yohannes alazar)
Date: Wed, 14 Jun 2006 16:35:21 +0100
Subject: [R] data managment
Message-ID: <ae94396d0606140835r94bad75jd5ab1cb2c96cc70@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060614/4d1623b9/attachment.pl 

From deepayan.sarkar at gmail.com  Wed Jun 14 18:07:03 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 14 Jun 2006 11:07:03 -0500
Subject: [R] positioning of separate y-axis labels in xyplot
In-Reply-To: <449029D8.7030203@stat.purdue.edu>
References: <449029D8.7030203@stat.purdue.edu>
Message-ID: <eb555e660606140907oa529863w2ee918149701b1fd@mail.gmail.com>

On 6/14/06, Benjamin Tyner <btyner at gmail.com> wrote:
> I like the functionality provided by outer=TRUE, but when it comes time
> to place separate xlabs or ylabs, I always end up 'eyeballing' it on a
> case-by-case basis. For example,
>
> ##begin example
> require(lattice)
>
> cars.lo <- loess(dist ~ speed, cars)
>
> print(xyplot(cars.lo$residuals+cars.lo$fitted~cars.lo$x,
>              strip=FALSE,
>              outer=TRUE,
>              layout=c(1,2),
>              ylab="",
>              scales=list(y=list(relation="free",rot=0)),
>              panel=function(x,y,panel.number,...){
>                    if(panel.number==1){
>                       panel.xyplot(x,y)
>                       panel.abline(h=0)
>                    }else{
>                       panel.xyplot(x,y=cars.lo$y)
>                       panel.xyplot(x,y,type="l")
>                    }
>              }))
>
> require(grid)
> trellis.focus("panel", 1, 1, clip.off=TRUE, highlight=FALSE)
> grid.text("residuals", x=unit(0, "npc") + unit(-2, "lines"),rot=90)
> trellis.focus("panel", 1, 2, clip.off=TRUE, highlight=FALSE)
> grid.text("fitted", x=unit(0, "npc") + unit(-2, "lines"),rot=90)
> ## end example

For the record, one alternative is something like


with(cars.lo,
     xyplot(residuals + fitted ~ x,
            strip=FALSE, strip.left = TRUE,
            outer=TRUE,
            layout=c(1,2),
            ylab="",
            scales=list(y=list(relation="free",rot=0)),
            panel=function(x,y,panel.number,...){
               if (panel.number==1){
                   panel.xyplot(x,y)
                      panel.abline(h=0)
                   }else{
                      panel.xyplot(x,y=cars.lo$y)
                      panel.xyplot(x,y,type="l")
                   }
            }))


> In this case, a distance of -2 lines happens to be enough, but one has
> to make the plot to know this. I'm interested in learning how one can
> place the ylabs without fear of overlapping the tick labels; i.e., how
> to use the exact space allocated by ylab="". I'm thinking it must
> involve viewports?

There is a single viewort for ylab, which you can get to with

downViewport(trellis.vpname("ylab"))

or

trellis.focus("ylab")

(the latter currently doesn't work because of a bug). This viewport
spans all the panels, but you could place your labels suitably if you
know how many rows of panels you have. BTW, ylab = "" may not actually
create this viewport, but ylab = " " should be safe.

Deepayan


From ramasamy at cancer.org.uk  Wed Jun 14 18:17:45 2006
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 14 Jun 2006 16:17:45 +0000
Subject: [R] merge dataframes with conditions formulated as
	logical	expressions
In-Reply-To: <20060614145507.GA4029@s1x.local>
References: <20060614145507.GA4029@s1x.local>
Message-ID: <1150301865.3501.174.camel@dhcp-82.wolf.ox.ac.uk>

You have discontinuity between your MIN.VAL and MAX.VAL for a given
group. If this is true in practise, then you may want to check and
report when VAL is in the discontinuous region.

Here is my solution that ignores that (and only uses MIN.VAL and
completely disrespecting MAX.VAL). Not very elegant but should do the
trick.


 df <- data.frame( GRP=c( "A", "A", "B" ), VAL=c( 10, 100, 200 ) )
 dp <- data.frame( GRP=c( "A", "A", "B", "B" ), MIN.VAL=c( 1, 50, 1,
70 ), MAX.VAL=c( 49, 999, 59, 999 ),  VAL2=c( 1.1, 2.2, 3.3, 4.4 ) )

 x <- split(df, df$GRP)
 y <- split(dp, dp$GRP)

 out <- NULL
 for(g in names(x)){

   xx <- x[[g]]
   yy <- y[[g]]

   w   <- cut(xx$VAL, breaks=c(yy$MIN.VAL, Inf), labels=F)
   tmp <- cbind(xx, yy[w, "VAL2"])
   colnames(tmp) <- c("GRP", "VAL", "VAL2")
   out <- rbind(out, tmp)
 } 
 out

Regards, Adai



On Wed, 2006-06-14 at 16:55 +0200, Wolfram Fischer wrote:
> I have a data.frame df containing two variables:
>     GRP: Factor
>     VAL: num
> 
> I have a data.frame dp containing:
>     GRP: Factor
>     MIN.VAL: num
>     MAX.VAL: num
>     VAL2: num
> with several rows per "GRP"
> where dp[i-1, "MAX.VAL"] < dp[i, "MIN.VAL"]
> within the same "GRP".
> 
> I want to create df[i, "VAL2"] <- dpp[z, "VAL2"] 
> with    i along df 
> and     dpp <- subset( dp, GRP = df[i, "GRP"] )
> so that it is true for each i:
>         df[i, "VAL"] > dpp[z, "MIN.VAL"]
>    and  df[i, "VAL"] <= dpp[z, "MAX.VAL"]
> 
> Is there an easy/efficient way to do that?
> 
> Example:
> df <- data.frame( GRP=c( "A", "A", "B" ), VAL=c( 10, 100, 200 ) )
> dp <- data.frame( GRP=c( "A", "A", "B", "B" ),
>     MIN.VAL=c( 1, 50, 1, 70 ), MAX.VAL=c( 49, 999, 59, 999 ), 
>     VAL2=c( 1.1, 2.2, 3.3, 4.4 ) )
> 
> The result should be:
>     df$VAL2 <- c( 1.1, 2.2, 4.4 )
> 
> Thanks - Wolfram
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From afshart at exchange.sba.miami.edu  Wed Jun 14 18:31:36 2006
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Wed, 14 Jun 2006 12:31:36 -0400
Subject: [R] appending
Message-ID: <6BCB4D493A447546A8126F24332056E803A149C6@school1.business.edu>

 All,

In the function below I have 24 individuals and 6 calculations per
individual.
The 6 calculations are collected each time in a 1:24 loop when
calculating "delta".

I'd like to collect all 144 = 24*6 calculations in one vector
("delta.patient.comb").
The function works as is via indexing, but is there an easier way to
collect the measurements via appendinng the 6 measurements each time to
the current set?  I couldn't find anything in Intro to R on appending.

cheers,
Dave
ps - please respond directly to afshar at miami.edu



creatine.function.new = function(delta.0.Y.0, gamma, comp.LIS.frm,
comp.CAND.frm) { ## function to calcuate the delta.i, i.e. the percent
## leftover ## gamma = rate of Cr going into bucket, e.g., mg/hr ##
delta.0.Y.0 = product of delta.0 and Y.0 at baseline ##
Y.1 = delta.0.Y.0 + gamma
delta = numeric(6)
delta.patient = numeric(24)
delta.patient.comb = numeric(144)
##
	for (k in 1:24) {		## each patient
		patient.k.CAND = which(comp.CAND.frm$Patient_no == k)
		Ucr.CAND.patient.k = comp.CAND.frm$Ucr[patient.k.CAND]
		C = Ucr.CAND.patient.k  ## 6 observed creatanine values
for each patient
		delta[1] = (Y.1 - C[1])/Y.1  
		Y.i = Y.1
		delta.i = delta[1]
		for (i in 1:5) {		## six measurments per
patient
			Y.i.plus.1 = delta.i * Y.i + gamma
			delta.i.plus.1 = (Y.i.plus.1 -
C[i+1])/Y.i.plus.1
			delta[i+1] = delta.i.plus.1
			delta.i = delta[i+1]
			Y.i = Y.i.plus.1
		}
		delta.patient[k] = list(delta)	
		delta.patient.comb[(6*(k-1)+1):(6*(k-1)+ 6)] = delta
	}
list(delta.patient, delta.patient.comb)
}


From gavin.simpson at ucl.ac.uk  Wed Jun 14 18:44:00 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 14 Jun 2006 17:44:00 +0100
Subject: [R] how to make a series with decimal increment
In-Reply-To: <20060613052426.26372.qmail@webmail62.rediffmail.com>
References: <20060613052426.26372.qmail@webmail62.rediffmail.com>
Message-ID: <1150303440.6781.33.camel@gsimpson.geog.ucl.ac.uk>

On Tue, 2006-06-13 at 05:24 +0000, anil kumar rohilla wrote:
>   Hi List, 
>        I am new to this Rsoftware, i want to make a sereis for example
> which is having values like this, s<- 0,0.1,0.2,0.3,0.4........,1  
> 
> i tryed this statement 
> s<-0:0.1:1
> but this giving an error megssage.
>  but by default increment 1 it is taking ,,,,,so what to do ,,
> 
> i want to use this varible in for loop..
> 
> like for(j in s)
> 
> thanks in advance
> 
> with regards
> anil kumar

Hi Anil,

see ?seq for the solution, which is:

s <- seq(0, 1, by = 0.1)
for(i in s) {
  print(i)
}

if you need the sequence for something else, if not:

for(i in seq(0, 1, by = 0.1)) {
  print(i)
}

Note the braces "{" & "}" are superfluous here but you'll need them if
you are doing anything more complicated in your loop.

HTH

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
*  Note new Address, Telephone & Fax numbers from 6th April 2006  *
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson
ECRC & ENSIS                  [t] +44 (0)20 7679 0522
UCL Department of Geography   [f] +44 (0)20 7679 0565
Pearson Building              [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street                  [w] http://www.ucl.ac.uk/~ucfagls/cv/
London, UK.                   [w] http://www.ucl.ac.uk/~ucfagls/
WC1E 6BT.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From tkevans at tkevans.com  Wed Jun 14 19:08:37 2006
From: tkevans at tkevans.com (Tim Evans)
Date: Wed, 14 Jun 2006 13:08:37 -0400
Subject: [R] FireFox Upgrade Breaks R(?)
Message-ID: <20060614170837.M94672@tkevans.com>

Just upgraded Firefox to the current (1.5.0.4) release on a RedHat 7.3 system.
 R now whines that /usr/lib/i386/libjavaplugin_nscp.so doesn't exist. 

Well, no, the referenced i386 subdir (and plugin) exists in the JRE directory
tree, not at the top level of /usr/lib.

And, if I restore the previous Firefox, R is all better.

Never touched R itself during the process.

Thanks.
--
Tim Evans, TKEvans.com, Inc.    |    5 Chestnut Court
tkevans at tkevans.com             |    Owings Mills, MD 21117
http://www.tkevans.com/         |    443-394-3864
http://www.come-here.com/News/  |


From HStevens at MUOhio.edu  Wed Jun 14 19:23:06 2006
From: HStevens at MUOhio.edu (Martin Henry H. Stevens)
Date: Wed, 14 Jun 2006 13:23:06 -0400
Subject: [R] appending
In-Reply-To: <6BCB4D493A447546A8126F24332056E803A149C6@school1.business.edu>
References: <6BCB4D493A447546A8126F24332056E803A149C6@school1.business.edu>
Message-ID: <A61403EB-8693-4A5D-97DB-D7C0958D4886@MUOhio.edu>

Hi David,
It would be helpful if you supply a little data, upon which this  
would operate.
Hank
On Jun 14, 2006, at 12:31 PM, Afshartous, David wrote:

>  All,
>
> In the function below I have 24 individuals and 6 calculations per
> individual.
> The 6 calculations are collected each time in a 1:24 loop when
> calculating "delta".
>
> I'd like to collect all 144 = 24*6 calculations in one vector
> ("delta.patient.comb").
> The function works as is via indexing, but is there an easier way to
> collect the measurements via appendinng the 6 measurements each  
> time to
> the current set?  I couldn't find anything in Intro to R on appending.
>
> cheers,
> Dave
> ps - please respond directly to afshar at miami.edu
>
>
>
> creatine.function.new = function(delta.0.Y.0, gamma, comp.LIS.frm,
> comp.CAND.frm) { ## function to calcuate the delta.i, i.e. the percent
> ## leftover ## gamma = rate of Cr going into bucket, e.g., mg/hr ##
> delta.0.Y.0 = product of delta.0 and Y.0 at baseline ##
> Y.1 = delta.0.Y.0 + gamma
> delta = numeric(6)
> delta.patient = numeric(24)
> delta.patient.comb = numeric(144)
> ##
> 	for (k in 1:24) {		## each patient
> 		patient.k.CAND = which(comp.CAND.frm$Patient_no == k)
> 		Ucr.CAND.patient.k = comp.CAND.frm$Ucr[patient.k.CAND]
> 		C = Ucr.CAND.patient.k  ## 6 observed creatanine values
> for each patient
> 		delta[1] = (Y.1 - C[1])/Y.1
> 		Y.i = Y.1
> 		delta.i = delta[1]
> 		for (i in 1:5) {		## six measurments per
> patient
> 			Y.i.plus.1 = delta.i * Y.i + gamma
> 			delta.i.plus.1 = (Y.i.plus.1 -
> C[i+1])/Y.i.plus.1
> 			delta[i+1] = delta.i.plus.1
> 			delta.i = delta[i+1]
> 			Y.i = Y.i.plus.1
> 		}
> 		delta.patient[k] = list(delta)	
> 		delta.patient.comb[(6*(k-1)+1):(6*(k-1)+ 6)] = delta
> 	}
> list(delta.patient, delta.patient.comb)
> }
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html

Dr. M. Hank H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"


From a_elhabti at yahoo.fr  Wed Jun 14 19:23:53 2006
From: a_elhabti at yahoo.fr (Ahmed Elhabti)
Date: Wed, 14 Jun 2006 19:23:53 +0200 (CEST)
Subject: [R] graph
Message-ID: <20060614172353.40550.qmail@web27811.mail.ukl.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060614/f2996904/attachment.pl 

From csardi at rmki.kfki.hu  Wed Jun 14 19:35:12 2006
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Wed, 14 Jun 2006 13:35:12 -0400
Subject: [R] graph
In-Reply-To: <20060614172353.40550.qmail@web27811.mail.ukl.yahoo.com>
References: <20060614172353.40550.qmail@web27811.mail.ukl.yahoo.com>
Message-ID: <20060614173512.GG11974@localdomain>

On Wed, Jun 14, 2006 at 07:23:53PM +0200, Ahmed Elhabti wrote:
[...]
>   Hi,
>   I want to know how I can have  with R two graphs in only one graph?
>   Example
>    
>   x<-seq(0,4,0.1)
>   plot(x,dnorm(x),type="l")     
>   plot(x,dgamma(x,2,0.5),typ="p")

in the last line, use 'points' instead of 'plot':

x<-seq(0,4,0.1)
plot(x,dnorm(x),type="l")     
points(x,dgamma(x,2,0.5),type="p")

Gabor

>   Thanks
[...]

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From LI at nsabp.pitt.edu  Wed Jun 14 20:14:26 2006
From: LI at nsabp.pitt.edu (Li, Jia)
Date: Wed, 14 Jun 2006 14:14:26 -0400
Subject: [R] A question about stepwise procedures: step function
Message-ID: <D70CBC108DFBD446862A6E1F6F0B4A152ACAD3@nsabpmail.nsabp.pitt.edu>

Dear all,

I tried to use "step"  function to do model selection, but I got an error massage.  What I don't understand is that data as data.frame worked well for my other programs, how come I cannot make it run this time. Could you please tell me how I can fix it?

***************************************************************************************************

>all<-data.frame(z1,z2,z3)

>fit.model.all<- coxph(Surv(t,cen) ~z1+z2+z3,data=all)

> reg.model.all<-step(fit.model.all)
Start:  AIC= 689.1 
 Surv(t, cen) ~ z1 + z2 + z3 
Error in as.data.frame.default(data) : cannot coerce class "function" into a data.frame
***************************************************************************************************
Thanks a lot!
 
Jia


From afshart at exchange.sba.miami.edu  Wed Jun 14 20:35:26 2006
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Wed, 14 Jun 2006 14:35:26 -0400
Subject: [R] appending
Message-ID: <6BCB4D493A447546A8126F24332056E803A14A0E@school1.business.edu>

Hank,
Attached is the dataframe that can be supplied for the argument
comp.CAND.frm.  The argument for comp.LIS.frm can be deleted since
it currently isn't used.   The other arguments can be set as:
delta.0.Y.0 = 50
gamma = 40.
cheers,
dave


 

-----Original Message-----
From: Martin Henry H. Stevens [mailto:HStevens at MUOhio.edu] 
Sent: Wednesday, June 14, 2006 1:23 PM
To: Afshartous, David
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] appending

Hi David,
It would be helpful if you supply a little data, upon which this would
operate.
Hank
On Jun 14, 2006, at 12:31 PM, Afshartous, David wrote:

>  All,
>
> In the function below I have 24 individuals and 6 calculations per 
> individual.
> The 6 calculations are collected each time in a 1:24 loop when 
> calculating "delta".
>
> I'd like to collect all 144 = 24*6 calculations in one vector 
> ("delta.patient.comb").
> The function works as is via indexing, but is there an easier way to 
> collect the measurements via appendinng the 6 measurements each time 
> to the current set?  I couldn't find anything in Intro to R on 
> appending.
>
> cheers,
> Dave
> ps - please respond directly to afshar at miami.edu
>
>
>
> creatine.function.new = function(delta.0.Y.0, gamma, comp.LIS.frm,
> comp.CAND.frm) { ## function to calcuate the delta.i, i.e. the percent

> ## leftover ## gamma = rate of Cr going into bucket, e.g., mg/hr ## 
> delta.0.Y.0 = product of delta.0 and Y.0 at baseline ##
> Y.1 = delta.0.Y.0 + gamma
> delta = numeric(6)
> delta.patient = numeric(24)
> delta.patient.comb = numeric(144)
> ##
> 	for (k in 1:24) {		## each patient
> 		patient.k.CAND = which(comp.CAND.frm$Patient_no == k)
> 		Ucr.CAND.patient.k = comp.CAND.frm$Ucr[patient.k.CAND]
> 		C = Ucr.CAND.patient.k  ## 6 observed creatanine values
for each 
> patient
> 		delta[1] = (Y.1 - C[1])/Y.1
> 		Y.i = Y.1
> 		delta.i = delta[1]
> 		for (i in 1:5) {		## six measurments per
> patient
> 			Y.i.plus.1 = delta.i * Y.i + gamma
> 			delta.i.plus.1 = (Y.i.plus.1 -
> C[i+1])/Y.i.plus.1
> 			delta[i+1] = delta.i.plus.1
> 			delta.i = delta[i+1]
> 			Y.i = Y.i.plus.1
> 		}
> 		delta.patient[k] = list(delta)	
> 		delta.patient.comb[(6*(k-1)+1):(6*(k-1)+ 6)] = delta
> 	}
> list(delta.patient, delta.patient.comb) }
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html

Dr. M. Hank H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"





From jholtman at gmail.com  Wed Jun 14 21:12:08 2006
From: jholtman at gmail.com (jim holtman)
Date: Wed, 14 Jun 2006 15:12:08 -0400
Subject: [R] data managment
In-Reply-To: <ae94396d0606140835r94bad75jd5ab1cb2c96cc70@mail.gmail.com>
References: <ae94396d0606140835r94bad75jd5ab1cb2c96cc70@mail.gmail.com>
Message-ID: <644e1f320606141212y2635eadq4c493133fba5c158@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060614/31899e5e/attachment.pl 

From Mike.Prager at noaa.gov  Wed Jun 14 21:23:31 2006
From: Mike.Prager at noaa.gov (Michael Prager)
Date: Wed, 14 Jun 2006 15:23:31 -0400
Subject: [R] how to make a series with decimal increment
In-Reply-To: <1150303440.6781.33.camel@gsimpson.geog.ucl.ac.uk>
References: <20060613052426.26372.qmail@webmail62.rediffmail.com>
	<1150303440.6781.33.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <44906233.2020302@noaa.gov>

Gavin Simpson wrote on 6/14/2006 12:44 PM:
> On Tue, 2006-06-13 at 05:24 +0000, anil kumar rohilla wrote:
>   
>>   Hi List, 
>>        I am new to this Rsoftware, i want to make a sereis for example
>> which is having values like this, s<- 0,0.1,0.2,0.3,0.4........,1  
>>
>> [...]
> Hi Anil,
>
> see ?seq for the solution, which is:
>
> s <- seq(0, 1, by = 0.1)
> for(i in s) {
>   print(i)
> }
>   

I don't know if R avoids this somehow, but in many languages, generated 
noninteger sequences are sometimes one item shorter than expected, 
because of rounding errors.  For that reason, I would prefer to generate 
an integer sequence and then form a noninteger one from it.  For example:

 s  <- (1:10) / 10

Another option in R would be using the argument "length.out", as in

s = seq(from = 0.1, to = 1, length.out = 10)

...Mike


-- 
Michael H. Prager, Ph.D.
Population Dynamics Team
NOAA Center for Coastal Habitat and Fisheries Research
NMFS Southeast Fisheries Science Center
Beaufort, North Carolina  28516  USA
http://shrimp.ccfhrb.noaa.gov/~mprager/


From br44114 at gmail.com  Wed Jun 14 22:47:32 2006
From: br44114 at gmail.com (bogdan romocea)
Date: Wed, 14 Jun 2006 16:47:32 -0400
Subject: [R] bubbleplot for matrix
Message-ID: <8d5a36350606141347l5519591bh3d1515d6bc6d2574@mail.gmail.com>

Here's an example. By the way, I find that it's more convenient (where
applicable) to keep the data in 3 vectors/factors rather than one
matrix/data frame.

a <- matrix(sample(1:5,100,replace=TRUE),nrow=10,dimnames=list(1:10,5*1:10))
x <- y <- z <- vector()
for (i in 1:nrow(a)) {
  x <- c(x,rep(rownames(a)[i],ncol(a)))
  y <- c(y,colnames(a))
  z <- c(z,a[i,])
}
symbols(as.numeric(x),as.numeric(y),z,inches=0.2,bg="khaki")
text(as.numeric(x),as.numeric(y),labels=z)


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Albert Vilella
> Sent: Tuesday, June 13, 2006 7:11 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] bubbleplot for matrix
>
> Hi all,
>
> I would like to ask if it is possible to use bubbleplot for a 20x20
> matrix, instead of a dataframe with factors in columns.
>
> The idea would be to get a tabular representation with bubbles like in
> Rnews_2006_2 article, which look very nice.
>
> Thanks in advance,
>
>     Albert.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


From ilka at dpi.inpe.br  Thu Jun 15 00:47:27 2006
From: ilka at dpi.inpe.br (ilka at dpi.inpe.br)
Date: Wed, 14 Jun 2006 19:47:27 -0300
Subject: [R] Saving R graphics with version 2.3.1
Message-ID: <20060614194727.ywkprua39pck8gw0@agenda.dpi.inpe.br>

HI !

I?ve installed the latest R version (2.3.1) and I?ve had problems to 
save my graphics as JPEG, PNG and the others available formats.
R saves the file with no extension. Then, I have to open this file 
using an image visualizer and save it as PNG, for instance.

Previous R versions work without problems.

Has everyone had a similar problem?
Is it a bug of R 2.3.1?

Thanks in advance.


Ilka Afonso Reis


From g.abraham at ms.unimelb.edu.au  Thu Jun 15 01:32:45 2006
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Thu, 15 Jun 2006 09:32:45 +1000
Subject: [R] Multiple lag.plots per page
In-Reply-To: <Pine.LNX.4.64.0606140539490.2567@gannet.stats.ox.ac.uk>
References: <448E1FFA.6070601@ms.unimelb.edu.au>
	<Pine.LNX.4.64.0606130736440.24883@gannet.stats.ox.ac.uk>
	<448E6628.5060907@ms.unimelb.edu.au>
	<Pine.LNX.4.64.0606130847530.19239@gannet.stats.ox.ac.uk>
	<448F9188.2090102@ms.unimelb.edu.au>
	<Pine.LNX.4.64.0606140539490.2567@gannet.stats.ox.ac.uk>
Message-ID: <44909C9D.5030401@ms.unimelb.edu.au>

Prof Brian Ripley wrote:
> On Wed, 14 Jun 2006, Gad Abraham wrote:
> 
>> Prof Brian Ripley wrote:
>>> On Tue, 13 Jun 2006, Gad Abraham wrote:
>>>
>>>> Prof Brian Ripley wrote:
>>>>> On Tue, 13 Jun 2006, Gad Abraham wrote:
>>>>>
>>>>>> Hi,
>>>>>>
>>>>>> I'm trying to plot several lag.plots on a page, however the second 
>>>>>> plot
>>>>>> replaces the first one (although it only takes up the upper half 
>>>>>> as it
>>>>>> should):
>>>>>>
>>>>>> par(mfrow=c(2,1))
>>>>>> a<-sin(1:100)
>>>>>> b<-cos(1:100)
>>>>>> lag.plot(a)
>>>>>> lag.plot(b)
>>>>>>
>>>>>> What's the trick to this?
>>>>>
>>>>> lag.plot itself calls par(mfrow).  The trick is to get one call to 
>>>>> do the plots you want:
>>>>>
>>>>> lag.plot(cbind(a,b))
>>>>>
>>>>>
>>>>
>>>> Thanks, that works great for multiple lag.plots. Is it possible to 
>>>> have a lag.plot and another type of plot on the same page? The 
>>>> second plot() always replaces the lag.plot for me.
>>>
>>> Yes, if the other plot is second, e.g
>>>
>>> par(mfrow=c(2,1))
>>> a<-sin(1:100)
>>> lag.plot(a)
>>> par(mfg=c(2,1)) # move to second plot
>>> plot(1:10)
>>>
>>>
>>
>> Following from my previous questions, lag.plot doesn't recognise some 
>> of the standard plot variables, e.g. xaxt="n" doesn't remove the 
>> x-axis, and setting xlab causes an error:
>>
>>> lag.plot(sin(1:100), xlab="foo")
>> Error in plotts(x = x, y = y, plot.type = plot.type, xy.labels = 
>> xy.labels, :
>>        formal argument "xlab" matched by multiple actual arguments
>>
>> Is this a bug or a feature?
> 
> feature.  Note that the help page says
> 
>      ...: Further arguments to 'plot.ts'.
> 
> and not `graphical parameters'.
> 
> 
>> Also, how can I make lag.plot behave nicely when plotted with other 
>> plots on the same page? it takes up more room than it's allocated by 
>> par(mfrow).
> 
> Really you are not using it for its intended purpose, multiple plots at 
> different lags.  (Notice the plurals in the title and the description on 
> the help page.)  Why not use plot.ts directly?
> 
> If you want to pursue lag.plot, try the version in R-devel which works 
> better for single-plot displays.
> 

OK, I've experimented with plot.ts and it does what I need it to.

Thanks for your help,
Gad

-- 
Gad Abraham
Department of Mathematics and Statistics
University of Melbourne
Victoria 3010, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From attenka at utu.fi  Thu Jun 15 02:36:25 2006
From: attenka at utu.fi (kone)
Date: Thu, 15 Jun 2006 03:36:25 +0300
Subject: [R] How to change the margin widths in png-plots?
Message-ID: <f69be5c30019e51d76eb1c8055c54c5b@local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060615/aab93188/attachment.pl 

From attenka at utu.fi  Thu Jun 15 02:39:21 2006
From: attenka at utu.fi (Atte Tenkanen)
Date: Thu, 15 Jun 2006 03:39:21 +0300
Subject: [R] How to change the margin widths in png-plots?
Message-ID: <fb7ed21b27709.4490d669@utu.fi>

Hello,

I have tried to change the margin widths so that mtext (here "sd of  
consecutive pc intervals", look at the picture) and  
plot(...,xlab="bar") fits to the picture.

Here is an example:
http://users.utu.fi/attenka/margins.png

This doesn't help:
par(mar=c(5.1, 7.1, 4.1, 2.1))

And here are the commands:

png(filename="/Users/kone/Vaitostutkimus/Pictures/ 
BachBWV60_sd_cons_pc_int.png", width = 2800, height = 1200,pointsize =  
12, bg = "white");
plot(Compo_SD_succ_int_array_vector,ylim=c(0.40,1.9),col="white",xlab="b 
ar",ylab="", cex.lab=3, cex.axis=2);
mtext("sd of consecutive pc intervals", side=2, line=0,  
padj=-1.8,at=1.2, cex=3)
lines(Compo_SD_succ_int_array_vector,col=1,lty=1,lwd=2);
text(2,0.93,labels="*",cex=3) # "an asterisk..."
dev.off();

What do I do next?

Atte Tenkanen, Turku Finland


From spencer.graves at pdf.com  Thu Jun 15 04:38:01 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 14 Jun 2006 19:38:01 -0700
Subject: [R] Garch Warning
In-Reply-To: <d4c57560606130354h4d42bfc5x691457ae9d3536fd@mail.gmail.com>
References: <d4c57560606130354h4d42bfc5x691457ae9d3536fd@mail.gmail.com>
Message-ID: <4490C809.6010204@pdf.com>

	  Your example is not reproducible.  Have you tried walking
through the code line by line after "debug(garch)"?  After you've tried
that, if you'd still like help from this listserve, please submit a
simple, self-contained / reproducible example, as suggested the posting
guide! "www.R-project.org/posting-guide.html".

	  hope this helps.
	  spencer graves

Arun Kumar Saha wrote:
> Dear all R-users,
> 
> I wanted to fit a Garch(1,1) model to a dataset by:
> 
>> garch1 = garch(na.omit(dat))
> 
> But I got a warning message while executing, which is:
> 
>> Warning message:
>> NaNs produced in: sqrt(pred$e)
> 
> The garch parameters that I got are:
> 
> 
>> garch1
> 
> Call:
> garch(x = na.omit(dat))
> 
> Coefficient(s):
>        a0         a1         b1
> 1.212e-04  1.001e+00  1.111e-14
> 
> Can any one please tell me that why got this message? What is the remedy?
> 
> Thanks and Regards
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From arun.kumar.saha at gmail.com  Thu Jun 15 05:28:22 2006
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Thu, 15 Jun 2006 08:58:22 +0530
Subject: [R] Garch Warning
In-Reply-To: <4490C809.6010204@pdf.com>
References: <d4c57560606130354h4d42bfc5x691457ae9d3536fd@mail.gmail.com>
	<4490C809.6010204@pdf.com>
Message-ID: <d4c57560606142028j16c7dcb4r587b31ba0c4c03b7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060615/9386ae64/attachment.pl 

From mobygeek at yahoo.com  Thu Jun 15 05:42:41 2006
From: mobygeek at yahoo.com (context grey)
Date: Wed, 14 Jun 2006 20:42:41 -0700 (PDT)
Subject: [R] MDS with missing data?
Message-ID: <20060615034241.66187.qmail@web51410.mail.yahoo.com>

Hello

I will be applying MDS (actually Isomap) to make a
psychological
"concept map" of the similarities between N concepts.

I would like to scale to a large number of concepts,
however, the
resulting N*(N-1) pairwise similarities is prohibitive
for a user survey.
I'm thinking of giving people random subsets of the
pairwise
similarities.  

Does anyone have recommendations for this situation?

My current thoughts are to either

1) use nonmetric/gradient descent MDS which seems to
allow missing data, or

2) devise some scheme whereby the data that are ranked
in common
   by several people is used to derive a scaling
factor for each
   person's ratings.

Thanks for any advice,


From jari.oksanen at oulu.fi  Thu Jun 15 06:13:27 2006
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Thu, 15 Jun 2006 07:13:27 +0300
Subject: [R] MDS with missing data?
In-Reply-To: <20060615034241.66187.qmail@web51410.mail.yahoo.com>
References: <20060615034241.66187.qmail@web51410.mail.yahoo.com>
Message-ID: <b88aef9f28ea5ac825854e8400052b02@oulu.fi>

Dear Context Grey,

On 15 Jun 2006, at 6:42, context grey wrote:

>
> I will be applying MDS (actually Isomap) to make a
> psychological
> "concept map" of the similarities between N concepts.
>
So actually, how do you do isomap? RSiteSearch gave me one hit of 
"isomap". I only ask, because I've implemented a working version of 
isomap (not ready for prime time yet, but a proof that it works). If 
isomap already is available in R, I won't do anything more with the 
function. I don't understand the rest of the question, but isomap 
really may be able to work with NA dissimilarities: just replace them 
with shortest path distances via non-missing dissimilarities. In fact, 
you don't need but some ('k') non-missing dissimilarities per item, 
since that is how isomap works. Your dissimilarity structure may become 
disconnected, of course, but that's common in isomap.

If you mean that your raw data has NA, then you may select a 
dissimilarity function that can handle NA input and produce finite 
dissimilarities (I think daisy in the cluster package does this).

Somehow I feel I answered to quite a different question than you asked. 
Sorry.

> I would like to scale to a large number of concepts,
> however, the
> resulting N*(N-1) pairwise similarities is prohibitive
> for a user survey.
> I'm thinking of giving people random subsets of the
> pairwise
> similarities.
>
> Does anyone have recommendations for this situation?
>
> My current thoughts are to either
>
> 1) use nonmetric/gradient descent MDS which seems to
> allow missing data, or
>
Not the isoMDS function in MASS. if N(N-1) is a problem, then nonmetric 
MDS may not be the solution.

> 2) devise some scheme whereby the data that are ranked
> in common
>    by several people is used to derive a scaling
> factor for each
>    person's ratings.
>
> Thanks for any advice,
>
> _

Cheers, Green Power
--
Green Power, Oulu, Finland


From ritwik.sinha at gmail.com  Thu Jun 15 06:16:24 2006
From: ritwik.sinha at gmail.com (Ritwik Sinha)
Date: Thu, 15 Jun 2006 00:16:24 -0400
Subject: [R] Saving R graphics with version 2.3.1
In-Reply-To: <20060614194727.ywkprua39pck8gw0@agenda.dpi.inpe.br>
References: <20060614194727.ywkprua39pck8gw0@agenda.dpi.inpe.br>
Message-ID: <42bc98300606142116s4206549fsbdf8178e30c39c99@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060615/03860f81/attachment.pl 

From wolfram at fischer-zim.ch  Thu Jun 15 07:06:09 2006
From: wolfram at fischer-zim.ch (Wolfram Fischer)
Date: Thu, 15 Jun 2006 07:06:09 +0200
Subject: [R] merge dataframes with conditions formulated as logical
	expressions
In-Reply-To: <1150301865.3501.174.camel@dhcp-82.wolf.ox.ac.uk>
References: <20060614145507.GA4029@s1x.local>
	<1150301865.3501.174.camel@dhcp-82.wolf.ox.ac.uk>
Message-ID: <20060615050609.GB2164@s1x.local>

--- Reply to: ---
>Date:    14.06.06 16:17 (+0000)
>From:    Adaikalavan Ramasamy <ramasamy at cancer.org.uk>
>Subject: Re: [R] merge dataframes with conditions formulated as logical expressions
>
> You have discontinuity between your MIN.VAL and MAX.VAL for a given
> group. If this is true in practise, then you may want to check and
> report when VAL is in the discontinuous region.

Your solution without concerning discontinuity is better
because it is more general.

> Here is my solution that ignores that (and only uses MIN.VAL and
> completely disrespecting MAX.VAL). Not very elegant but should do
> the trick.
> 
> 
>  df <- data.frame( GRP=c( "A", "A", "B" ), VAL=c( 10, 100, 200 ) )
>  dp <- data.frame( GRP=c( "A", "A", "B", "B" ), MIN.VAL=c( 1, 50, 1,
> 70 ), MAX.VAL=c( 49, 999, 59, 999 ),  VAL2=c( 1.1, 2.2, 3.3, 4.4 ) )
> 
>  x <- split(df, df$GRP)
>  y <- split(dp, dp$GRP)
> 
>  out <- NULL
>  for(g in names(x)){
> 
>    xx <- x[[g]]
>    yy <- y[[g]]
> 
>    w   <- cut(xx$VAL, breaks=c(yy$MIN.VAL, Inf), labels=F)
>    tmp <- cbind(xx, yy[w, "VAL2"])
>    colnames(tmp) <- c("GRP", "VAL", "VAL2")
>    out <- rbind(out, tmp)
>  } 
>  out
> 
> Regards, Adai

Thanks for this solution.

I did not yet try to program a conventional solution
because I thought there would be a nice shortcut in R
to solve the problem comparably elegantly as in SQL:
	select df.*, dp.VAL2
	from df, dp
	where df.GRP = dp.GRP
	  and df.VAL > dp.MIN_VAL
	  and df.VAL <= dp.MAX_VAL

Wolfram

> On Wed, 2006-06-14 at 16:55 +0200, Wolfram Fischer wrote:
> > I have a data.frame df containing two variables:
> >     GRP: Factor
> >     VAL: num
> > 
> > I have a data.frame dp containing:
> >     GRP: Factor
> >     MIN.VAL: num
> >     MAX.VAL: num
> >     VAL2: num
> > with several rows per "GRP"
> > where dp[i-1, "MAX.VAL"] < dp[i, "MIN.VAL"]
> > within the same "GRP".
> > 
> > I want to create df[i, "VAL2"] <- dpp[z, "VAL2"] 
> > with    i along df 
> > and     dpp <- subset( dp, GRP = df[i, "GRP"] )
> > so that it is true for each i:
> >         df[i, "VAL"] > dpp[z, "MIN.VAL"]
> >    and  df[i, "VAL"] <= dpp[z, "MAX.VAL"]
> > 
> > Is there an easy/efficient way to do that?
> > 
> > Example:
> > df <- data.frame( GRP=c( "A", "A", "B" ), VAL=c( 10, 100, 200 ) )
> > dp <- data.frame( GRP=c( "A", "A", "B", "B" ),
> >     MIN.VAL=c( 1, 50, 1, 70 ), MAX.VAL=c( 49, 999, 59, 999 ), 
> >     VAL2=c( 1.1, 2.2, 3.3, 4.4 ) )
> > 
> > The result should be:
> >     df$VAL2 <- c( 1.1, 2.2, 4.4 )
> > 
> > Thanks - Wolfram
> > 
> > ______________________________________________


From arun.kumar.saha at gmail.com  Thu Jun 15 08:10:10 2006
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Thu, 15 Jun 2006 11:40:10 +0530
Subject: [R] Problem on Matrix multiplication
Message-ID: <d4c57560606142310w5b214eabq38fcdfad3f8ba74e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060615/cb1a980e/attachment.pl 

From res90sx5 at verizon.net  Thu Jun 15 08:58:32 2006
From: res90sx5 at verizon.net (Daniel Nordlund)
Date: Wed, 14 Jun 2006 23:58:32 -0700
Subject: [R] Problem on Matrix multiplication
In-Reply-To: <d4c57560606142310w5b214eabq38fcdfad3f8ba74e@mail.gmail.com>
Message-ID: <002601c69049$1dc7bf00$6401a8c0@main>

It looks like weight and s are data frames, not vectors/matrices as required (and as the error message tells you).

Dan

Daniel Nordlund
Bothell, WA

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch]
> On Behalf Of Arun Kumar Saha
> Sent: Wednesday, June 14, 2006 11:10 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Problem on Matrix multiplication
> 
> Dear all r-users,
> 
> I am getting a big problem with matrix multiplication
> 
> suppose I have,
> 
> > weight
>   Weight
> 1       1067640
> 2       8871500
> 3      42948778
> 4     127583735
> 5      22000000
> 6      44000000
> 7      56850000
> 8      23805662
> 
> and,
> > s
>             a          b          c          d          e
> f          g          h
> a   402493.18 -133931.62   461483.3  -94042.86   674493.8 -1493713.2  -
> 714081.5 -1320551.6
> b  -133931.62  192766.63  -414674.4  -36700.74  -276595.3   798034.6
> 382661.6   883478.4
> c   461483.25 -414674.43  1618660.4  660949.18   988526.5 -2795048.7 -
> 1483334.1 -1761234.9
> d   -94042.86  -36700.74   660949.2 1053310.43  -300291.4   -98814.9  -
> 263716.7  1130451.3
> e   674493.80 -276595.33   988526.5 -300291.35  3090869.8 -3017747.3 -
> 1766040.6 -2295605.0
> f -1493713.19  798034.60 -2795048.7  -98814.90 -3017747.3 10558734.1
> 4415620.8  6200897.7
> g  -714081.51  382661.62 -1483334.1 -263716.67 -1766040.6  4415620.8
> 2593947.5  2341455.0
> h -1320551.58  883478.39 -1761234.9 1130451.26 -2295605.0  6200897.7
> 2341455.0  7756557.3
> 
> 
> But when I want to get [transpose(weight)][s][weight] using following
> syntax,
> 
> t(weight)%*%s%*%weight
> 
> 
> I got error:
> 
> Error in t(weight) %*% s %*% weight : requires numeric matrix/vector
> arguments
> 
> Can anyone please tell me that where my error is?
> 
> Thanks and regards,
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From lorenzo.isella at gmail.com  Thu Jun 15 10:18:25 2006
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Thu, 15 Jun 2006 10:18:25 +0200
Subject: [R] Latex-Style Characters in R
Message-ID: <a2b3004b0606150118q556f6c2fg99341d17250abc03@mail.gmail.com>

Dear All,
I am starting to use R excellent graphical facilities to produce
good-looking plots.
However, I do not know yet how for use  pedices/apices (e.g. when you
write cm^3) and Greek letters (e.g. \sigma) which I really need now.
Is there a special package to load? Could anyone post a simple example
of such a plot so that I can re-use it?
Cheers

Lorenzo


From stecalza at tiscali.it  Thu Jun 15 10:39:14 2006
From: stecalza at tiscali.it (Stefano Calza)
Date: Thu, 15 Jun 2006 10:39:14 +0200
Subject: [R] Latex-Style Characters in R
In-Reply-To: <a2b3004b0606150118q556f6c2fg99341d17250abc03@mail.gmail.com>
References: <a2b3004b0606150118q556f6c2fg99341d17250abc03@mail.gmail.com>
Message-ID: <20060615083914.GA4102@med.unibs.it>

Try ?plotmath

Stefano

On Thu, Jun 15, 2006 at 10:18:25AM +0200, Lorenzo Isella wrote:
<Lorenzo>Dear All,
<Lorenzo>I am starting to use R excellent graphical facilities to produce
<Lorenzo>good-looking plots.
<Lorenzo>However, I do not know yet how for use  pedices/apices (e.g. when you
<Lorenzo>write cm^3) and Greek letters (e.g. \sigma) which I really need now.
<Lorenzo>Is there a special package to load? Could anyone post a simple example
<Lorenzo>of such a plot so that I can re-use it?
<Lorenzo>Cheers
<Lorenzo>
<Lorenzo>Lorenzo
<Lorenzo>
<Lorenzo>______________________________________________
<Lorenzo>R-help a stat.math.ethz.ch mailing list
<Lorenzo>https://stat.ethz.ch/mailman/listinfo/r-help
<Lorenzo>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From detlef.steuer at hsu-hamburg.de  Thu Jun 15 10:39:03 2006
From: detlef.steuer at hsu-hamburg.de (Detlef Steuer)
Date: Thu, 15 Jun 2006 10:39:03 +0200
Subject: [R] Latex-Style Characters in R
In-Reply-To: <a2b3004b0606150118q556f6c2fg99341d17250abc03@mail.gmail.com>
References: <a2b3004b0606150118q556f6c2fg99341d17250abc03@mail.gmail.com>
Message-ID: <20060615103903.c19132ec.detlef.steuer@hsu-hamburg.de>

Hi,

just take a look at demo(plotmath) and example(plotmath).

Detlef

On Thu, 15 Jun 2006 10:18:25 +0200
"Lorenzo Isella" <lorenzo.isella at gmail.com> wrote:

> Dear All,
> I am starting to use R excellent graphical facilities to produce
> good-looking plots.
> However, I do not know yet how for use  pedices/apices (e.g. when you
> write cm^3) and Greek letters (e.g. \sigma) which I really need now.
> Is there a special package to load? Could anyone post a simple example
> of such a plot so that I can re-use it?
> Cheers
> 
> Lorenzo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From jarioksa at sun3.oulu.fi  Thu Jun 15 11:28:47 2006
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Thu, 15 Jun 2006 12:28:47 +0300
Subject: [R] MDS with missing data?
In-Reply-To: <b88aef9f28ea5ac825854e8400052b02@oulu.fi>
References: <20060615034241.66187.qmail@web51410.mail.yahoo.com>
	<b88aef9f28ea5ac825854e8400052b02@oulu.fi>
Message-ID: <1150363727.9428.45.camel@biol102145.oulu.fi>

On Thu, 2006-06-15 at 07:13 +0300, Jari Oksanen wrote:


> >
> > 1) use nonmetric/gradient descent MDS which seems to
> > allow missing data, or
> >
> Not the isoMDS function in MASS. if N(N-1) is a problem, then nonmetric 
> MDS may not be the solution.

Sorry for the wrong information: isoMDS does handle NA. I remembered old
times when I looked at the issue, but isoMDS changed since. Fine work!

cheers, jari oksanen
-- 
Jari Oksanen -- Dept Biology, Univ Oulu, 90014 Oulu, Finland
email jari.oksanen at oulu.fi, homepage http://cc.oulu.fi/~jarioksa/


From L.Alberts at student.unimaas.nl  Thu Jun 15 11:56:24 2006
From: L.Alberts at student.unimaas.nl (Alberts Laurens (Stud. SIT))
Date: Thu, 15 Jun 2006 11:56:24 +0200
Subject: [R] survival probabilities with cph (counting process)
Message-ID: <918329C84699E5438220CC277CFA7353744510@um-mail0137.unimaas.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060615/987234d8/attachment.pl 

From m.j.bojanowski at fss.uu.nl  Thu Jun 15 12:34:15 2006
From: m.j.bojanowski at fss.uu.nl (Michal Bojanowski)
Date: Thu, 15 Jun 2006 12:34:15 +0200
Subject: [R] Problem on Matrix multiplication
In-Reply-To: <d4c57560606142310w5b214eabq38fcdfad3f8ba74e@mail.gmail.com>
References: <d4c57560606142310w5b214eabq38fcdfad3f8ba74e@mail.gmail.com>
Message-ID: <91686670.20060615123415@fss.uu.nl>

Hi,


AKS> But when I want to get [transpose(weight)][s][weight] using following
AKS> syntax,

AKS> t(weight)%*%s%*%weight

I guess 'weight' and 's' are dataframes and you need matrices. Like
here:

x <- rnorm(8)
s <- matrix(rnorm(64), ncol=8)

t(x) %*% s %*% x   # this is OK

d <- as.data.frame(x)
t(d) %*% s %*% d  # this is your error


You can use data.matrix() to convert dataframes to matrices:

t( data.matrix(d) ) %*% s %*% data.matrix(d)   # again OK




HTH,

Michal

~,~`~,~`~,~`~,~`~,~`~,~`~,~`~,~`~,~

Michal Bojanowski
ICS/Utrecht
Utrecht University
Heidelberglaan 2; 3584 CS Utrecht
Room 1428
m.j.bojanowski at fss.uu.nl


From ramasamy at cancer.org.uk  Thu Jun 15 12:46:20 2006
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 15 Jun 2006 10:46:20 +0000
Subject: [R] data managment
In-Reply-To: <ae94396d0606140835r94bad75jd5ab1cb2c96cc70@mail.gmail.com>
References: <ae94396d0606140835r94bad75jd5ab1cb2c96cc70@mail.gmail.com>
Message-ID: <1150368381.3501.197.camel@dhcp-82.wolf.ox.ac.uk>

If your df contains your data, try

 tmp <- cbind( paste(df[ ,1], df[ ,2], sep=":"), 
               paste(df[ ,3], df[ ,4], sep=":") )
 tmp <- t( apply(tmp, 1, sort) )

 out <- data.frame( do.call(rbind, strsplit( tmp[,1], split=":" )), 
                    do.call(rbind, strsplit( tmp[,2], split=":" )) )
 colnames(out) <- colnames(df)
 out

Regards, Adai



On Wed, 2006-06-14 at 16:35 +0100, yohannes alazar wrote:
> First I would really like to thank the mailing list for help I got in the
> past, as a new to R I am really needing some support on hoe to code the
> following problem.
> 
> 
> 
> I am trying to sort some data I have in a big file. The file has 4 columns
> and 19000 rows. An example of it looks like this:-
> 
> 
> 
> G         0.892   A         0.108
> 
> G         0.883   T          0.117
> 
> T          0.5       C         0.5
> 
> A         0.617   G         0.383
> 
> G         0.925   A         0.075
> 
> A         0.967   G         0.033
> 
> C         0.883   T          0.117
> 
> C         0.633   T          0.367
> 
> G         0.95     A         0.05
> 
> C         0.742   G         0.258
> 
> G         0.875   T          0.125
> 
> T          0.167   C         0.833
> 
> C         0.792   A         0.208
> 
> 
> 
> Columns one and three are alphabets while three and four are their
> corresponding values.
> 
> I wanted to sort this data so that my first and third columns are in
> alphabetic order. For example in the first row the order is "G" then "A".
> This is not in alphabetic order therefore we swap them along with their
> values and it becomes:
> 
>  A        0.108   G         0.892
> 
> Row two looks fine but row three needs the same rearrangement as row one.
> And the final out put looks like:
> 
> A         0.108   G         0.892
> 
> G         0.883   T          0.117
> 
> C         0.5       T          0.5
> 
> A         0.617   G         0.383
> 
> A         0.075   G         0.925
> 
> A         0.967   G         0.033
> 
> C         0.883   T          0.117
> 
> C         0.633   T          0.367
> 
> A         0.05     G         0.95
> 
> C         0.742   G         0.258
> 
> G         0.875   T          0.125
> 
> C         0.833   T          0.167
> 
> A         0.208   C         0.792
> 
> Please some help with the relevant command names or a technique to code this
> task.
> 
> Thank you in advance
> 
> Regards Hannes
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From HStevens at muohio.edu  Thu Jun 15 12:59:48 2006
From: HStevens at muohio.edu (Martin Henry H. Stevens)
Date: Thu, 15 Jun 2006 06:59:48 -0400
Subject: [R] appending
In-Reply-To: <6BCB4D493A447546A8126F24332056E803A14A0E@school1.business.edu>
References: <6BCB4D493A447546A8126F24332056E803A14A0E@school1.business.edu>
Message-ID: <D7AEB2B5-7058-4EC2-98E9-2EC91066B107@muohio.edu>

Hi Dave,
Does this do what you want?
Hank

comp.CAND.frm <- read.table("comp.CAND.frm.frm", header=TRUE)
names(comp.CAND.frm) <- make.names(gsub("_", ".", names 
(comp.CAND.frm)), unique=TRUE)

creatine.function.new = function(delta.0.Y.0=50, gamma=40,
   dat=comp.CAND.frm) {
   ## function to calcuate the delta.i, i.e. the percent
   ## leftover ## gamma = rate of Cr going into bucket, e.g., mg/hr
   ## delta.0.Y.0 = product of delta.0 and Y.0 at baseline ##

##
unlist(
        tapply(dat$Ucr, dat$Patient.no, function(Cs)  {
   n.obs <- length(Cs)
   Y=numeric(n.obs+1)
   Y[1] = delta.0.Y.0 + gamma
   delta = numeric(n.obs)
		for (i in 1:n.obs) delta[i] <- {
		d <- (Y[i] - Cs[i])/Y[i]
                 Y[i+1] = delta[i] * Y[i] + gamma
                 d
		}
              delta}
             )
        )
}
dim(comp.CAND.frm)
6*24
creatine.function.new()
### The names are the concatenation of the patient and the observation
## e.g. patient 24 observation 6 is labeled 246.

On Jun 14, 2006, at 2:35 PM, Afshartous, David wrote:

> Hank,
> Attached is the dataframe that can be supplied for the argument
> comp.CAND.frm.  The argument for comp.LIS.frm can be deleted since
> it currently isn't used.   The other arguments can be set as:
> delta.0.Y.0 = 50
> gamma = 40.
> cheers,
> dave
>
>
>
>
> -----Original Message-----
> From: Martin Henry H. Stevens [mailto:HStevens at MUOhio.edu]
> Sent: Wednesday, June 14, 2006 1:23 PM
> To: Afshartous, David
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] appending
>
> Hi David,
> It would be helpful if you supply a little data, upon which this would
> operate.
> Hank
> On Jun 14, 2006, at 12:31 PM, Afshartous, David wrote:
>
>>  All,
>>
>> In the function below I have 24 individuals and 6 calculations per
>> individual.
>> The 6 calculations are collected each time in a 1:24 loop when
>> calculating "delta".
>>
>> I'd like to collect all 144 = 24*6 calculations in one vector
>> ("delta.patient.comb").
>> The function works as is via indexing, but is there an easier way to
>> collect the measurements via appendinng the 6 measurements each time
>> to the current set?  I couldn't find anything in Intro to R on
>> appending.
>>
>> cheers,
>> Dave
>> ps - please respond directly to afshar at miami.edu
>>
>>
>>
>> creatine.function.new = function(delta.0.Y.0, gamma, comp.LIS.frm,
>> comp.CAND.frm) { ## function to calcuate the delta.i, i.e. the  
>> percent
>
>> ## leftover ## gamma = rate of Cr going into bucket, e.g., mg/hr ##
>> delta.0.Y.0 = product of delta.0 and Y.0 at baseline ##
>> Y.1 = delta.0.Y.0 + gamma
>> delta = numeric(6)
>> delta.patient = numeric(24)
>> delta.patient.comb = numeric(144)
>> ##
>> 	for (k in 1:24) {		## each patient
>> 		patient.k.CAND = which(comp.CAND.frm$Patient_no == k)
>> 		Ucr.CAND.patient.k = comp.CAND.frm$Ucr[patient.k.CAND]
>> 		C = Ucr.CAND.patient.k  ## 6 observed creatanine values
> for each
>> patient
>> 		delta[1] = (Y.1 - C[1])/Y.1
>> 		Y.i = Y.1
>> 		delta.i = delta[1]
>> 		for (i in 1:5) {		## six measurments per
>> patient
>> 			Y.i.plus.1 = delta.i * Y.i + gamma
>> 			delta.i.plus.1 = (Y.i.plus.1 -
>> C[i+1])/Y.i.plus.1
>> 			delta[i+1] = delta.i.plus.1
>> 			delta.i = delta[i+1]
>> 			Y.i = Y.i.plus.1
>> 		}
>> 		delta.patient[k] = list(delta)	
>> 		delta.patient.comb[(6*(k-1)+1):(6*(k-1)+ 6)] = delta
>> 	}
>> list(delta.patient, delta.patient.comb) }
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-
>> guide.html
>
> Dr. M. Hank H. Stevens, Assistant Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
>
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/~stevenmh/
> http://www.muohio.edu/ecology/
> http://www.muohio.edu/botany/
> "E Pluribus Unum"
>
>
>
>
> <comp.CAND.frm.frm>

Dr. M. Hank H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"


From jim at bitwrit.com.au  Fri Jun 16 03:12:26 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Thu, 15 Jun 2006 21:12:26 -0400
Subject: [R] A whine and a request
In-Reply-To: <63541341-13D4-418C-81B1-C77861F4DC0A@uleth.ca>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA01BD0E0E@usctmx1106.merck.com>
	<63541341-13D4-418C-81B1-C77861F4DC0A@uleth.ca>
Message-ID: <4492057A.8050001@bitwrit.com.au>

John Vokey wrote:
> Thanks.  But, as is common with e-mail communication, nobody seems to  
> have understood the issue I was whining about.

Indeed, I was totally wrong.

> So, here is an actual  
> case...
> 
> Now, what I want is to regress the proportions of column(1)/(column(0) 
> +column(1)) on x.  I can accomplish the proportions as follows:  
> props=tab[,2]/(tab[,1]+tab[,2]).  So, I want to regress props on the  
> *row labels* of tab.  But I can't seem to get them to be anything  *but* 
> row labels.  For example,
>  > tab[,0]
>... 
> extracts the row labels, *but still as labels*, not data. 

as.numeric(rownames(tab))

Jim


From DevredE at mar.dfo-mpo.gc.ca  Thu Jun 15 13:50:44 2006
From: DevredE at mar.dfo-mpo.gc.ca (Devred, Emmanuel)
Date: Thu, 15 Jun 2006 08:50:44 -0300
Subject: [R] Problem with Julian function
Message-ID: <1A4AC4BAB9C50A42854582B69B08C0340A53A663@MSGMARBIO05>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060615/a4f7c39a/attachment.pl 

From murdoch at stats.uwo.ca  Thu Jun 15 15:25:14 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 15 Jun 2006 09:25:14 -0400
Subject: [R] Saving R graphics with version 2.3.1
In-Reply-To: <20060614194727.ywkprua39pck8gw0@agenda.dpi.inpe.br>
References: <20060614194727.ywkprua39pck8gw0@agenda.dpi.inpe.br>
Message-ID: <44915FBA.2090803@stats.uwo.ca>

ilka at dpi.inpe.br wrote:
> HI !
>
> I?ve installed the latest R version (2.3.1) and I?ve had problems to 
> save my graphics as JPEG, PNG and the others available formats.
> R saves the file with no extension. Then, I have to open this file 
> using an image visualizer and save it as PNG, for instance.
>   
You can type the extension when you save it, if you're typing the 
filename.  The default
names generally include the extension.
> Previous R versions work without problems.
>   

Could you give a detailed description to reproduce the problem?

Duncan Murdoch
> Has everyone had a similar problem?
> Is it a bug of R 2.3.1?
>
> Thanks in advance.
>
>
> Ilka Afonso Reis
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From Jan.Kleinn at partnerre.com  Thu Jun 15 15:26:04 2006
From: Jan.Kleinn at partnerre.com (Jan.Kleinn at partnerre.com)
Date: Thu, 15 Jun 2006 15:26:04 +0200
Subject: [R] select.list
Message-ID: <OFB52F600E.9D6288FE-ONC125718E.0048D026-C125718E.0049E2EA@partnerre.com>


Dear all,

I don't know, whether this is a bug or whether I', doing something wrong,
but since I installed the latest versions of R and ESS, R hangs within ESS,
when 'select.list' is the first command trying to open a window. Basically,
this hangs:
> x <- c('a', 'b', 'c', 'd')
> y <- select.list(x)
and this works fine:
> winDialog('ok', 'Hello')
> x <- c('a', 'b', 'c', 'd')
> y <- select.list(x)

Here are my specs:
> version
               _
platform       i386-pc-mingw32
arch           i386
os             mingw32
system         i386, mingw32
status
major          2
minor          3.1
year           2006
month          06
day            01
svn rev        38247
language       R
version.string Version 2.3.1 (2006-06-01)
> win.version()
[1] "Windows XP Professional (build 2600) Service Pack 2.0"

Furthermore, this is within GNU Emacs 21.3.1 (i386-mingw-nt5.1.2600) with
the latest ESS 5.3.1.

Any help is highly welcome.

Many thanks in advance and best regards,
Jan


From kwong at nymex.com  Thu Jun 15 15:38:29 2006
From: kwong at nymex.com (Wong, Kim)
Date: Thu, 15 Jun 2006 09:38:29 -0400
Subject: [R] help with table partition
Message-ID: <93BDB4436D8C1347BCE07BDBB05835FC0237B4A3@CORPMAIL02.prod.nymex.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060615/6e5ed2f7/attachment.pl 

From vincent at 7d4.com  Thu Jun 15 15:41:14 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Thu, 15 Jun 2006 15:41:14 +0200
Subject: [R] matrix selection return types
Message-ID: <4491637A.4030705@7d4.com>

Dear Rusers,

I would like some comments about the following results
(under R-2.2.0)

 > m = matrix(1:6 , 2 , 3)
 > m
      [,1] [,2] [,3]
[1,]    1    3    5
[2,]    2    4    6

 > z1 = m[(m[,1]==2),]
 > z1
[1] 2 4 6
 > is.matrix(z1)
[1] FALSE

 > z2 = m[(m[,1]==0),]
 > z2
      [,1] [,2] [,3]
 > is.matrix(z2)
[1] TRUE

Considered together, I'm a bit surprised about the
returned types from z1 and z2.
I would not have been surprised if z1 would still
have been a matrix, or z2=NULL.

There is certainly a logic behind this choice
but it's not very clear for me,
so any help/comment appreciated.

Thanks
Vincent


From LI at nsabp.pitt.edu  Thu Jun 15 15:52:01 2006
From: LI at nsabp.pitt.edu (Li, Jia)
Date: Thu, 15 Jun 2006 09:52:01 -0400
Subject: [R] A question about stepwise procedures: step function
Message-ID: <D70CBC108DFBD446862A6E1F6F0B4A15EB1270@nsabpmail.nsabp.pitt.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060615/5cfc182a/attachment.pl 

From gsxej2 at cam.ac.uk  Thu Jun 15 14:27:05 2006
From: gsxej2 at cam.ac.uk (Gregory Jefferis)
Date: Thu, 15 Jun 2006 13:27:05 +0100
Subject: [R] Access and assign list sub-elements using a string such as
	"l$a$b"
Message-ID: <200606151354.k5FDsU9P009391@hypatia.math.ethz.ch>

If I have a list I can set a sub-element as follows on the command line:

people=list()
people$tom$hair="brown"
people

But what if I have a string containing the name of the sub-element that I
want to access?

subel= "people$tom$hair"

get(subel) # returns error
assign(subel,"red") # silent but doesn't change list
people

The attempts above using assign/get won't do what I am trying to do [nor
according to the help should they].  I would be very grateful for any
suggestions.  Many thanks,

Greg.

-- 
Gregory Jefferis, PhD                                   and:
Research Fellow    
Department of Zoology                                   St John's College
University of Cambridge                                 Cambridge
Downing Street                                          CB2 1TP
Cambridge, CB2 3EJ 
United Kingdom

Lab Tel: +44 (0)1223 336683                     Office: +44 (0)1223 339899
Lab Fax: +44 (0)1223 336676

http://www.zoo.cam.ac.uk/zoostaff/jefferis.html           gsxej2 at cam.ac.uk


From rkrug at sun.ac.za  Thu Jun 15 15:54:43 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Thu, 15 Jun 2006 15:54:43 +0200
Subject: [R] Question concerning mle
Message-ID: <449166A3.40209@sun.ac.za>

Hi

I hope this is the right forum - if not, point me please to a better one.

I am using R 2.3.0 on Linux, SuSE 10.


I have a question concerning mle (method="BFGS").

I have a few models which I am fitting to existing data points. I
realised, that the likelihood is quite sensitive to the start values for
one parameter.

I am wondering: what is the best approach to identify the right initial
values? Do I have to do it recursively, and if yes, how can I automate
it? Or do I have to play with the system?

I am quite confident that the resulting parameters are the optimal for
my problem - but can I verify it?

Thanks,

Rainer


-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa


From ggrothendieck at gmail.com  Thu Jun 15 16:04:08 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 15 Jun 2006 10:04:08 -0400
Subject: [R] Problem with Julian function
In-Reply-To: <1A4AC4BAB9C50A42854582B69B08C0340A53A663@MSGMARBIO05>
References: <1A4AC4BAB9C50A42854582B69B08C0340A53A663@MSGMARBIO05>
Message-ID: <971536df0606150704h48a89b80vaf7fa2ea2b90a5b2@mail.gmail.com>

This looks like a bug to me too although its one off rather than two off since
julian uses origin 0, not 1, e.g.

> julian(1,1,2000, origin = c(1,1,2000))
[1] 0

Here is a workaround that seems to work ok and uses origin 1:

> as.numeric(format(as.Date(ymd), "%j")) # ymd from your post
 [1] 139 139 139 140 140 140 140 141 141 141



On 6/15/06, Devred, Emmanuel <DevredE at mar.dfo-mpo.gc.ca> wrote:
> Dear all,
>
> I have a problem with the function Julian, may be a bug in the function ?
> Here is a vector of character, which represents dates (May 18 to May 20
> 2000):
>
>  > amj <- c("2000-05-18","2000-05-18","2000-05-18","2000-05-19","2000-05-19"
>  > ,"2000-05-19", "2000-05-19", "2000-05-20", "2000-05-20", "2000-05-20")
>
> I load the date and chron libraries, I define the vector of character as a
> date variable using the dates function:
>
> > ymd <- dates(amj,format=c(dates="y-m-d"),origin=c(month=1,
> day=1,year=2000))
>
> Then when I apply the Julian function:
>
> > julian(months(ymd),days(ymd),years(ymd),origin=c(month = 1, day = 1, year
> = as.numeric(years(ymd[1]))))
>
> I get the following result:
>
> [1] 137 137 137 138 138 138 138 139 139 139
>
> However, when I check on the calendar, the dates I have correspond to the
> 139th, 140th and 141st  days of the year (there is a two days shift because
> it is a leap year, for other years I have a one day shift).
> I tried the default origin when I use the dates function but I get the same
> results.
>
> Did anyone have this problem before ? I would appreciate some help,
>
>  Thank you very much,
>
>       Emmanuel
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From jacques.veslot at good.ibl.fr  Thu Jun 15 16:15:11 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Thu, 15 Jun 2006 16:15:11 +0200
Subject: [R] help with table partition
In-Reply-To: <93BDB4436D8C1347BCE07BDBB05835FC0237B4A3@CORPMAIL02.prod.nymex.com>
References: <93BDB4436D8C1347BCE07BDBB05835FC0237B4A3@CORPMAIL02.prod.nymex.com>
Message-ID: <44916B6F.8090009@good.ibl.fr>

do.call(cbind, split(as.data.frame(test_table), rep(1:170,each=366)))
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


Wong, Kim a ?crit :
> Hi,
> 
>  
> 
> I have a test_table where the dim is 62220 by 73 (row by col)
> 
>  
> 
> I would like to partition the rows into 170 equal parts (170 tables
> where each is of dim 366 by 73), and rearrange them horizontally. The
> source codes I have:
> 
>  
> 
> for (i in 1:170) {
> 
>             c = cbind(c,test_table[(367*i+1):(367*(i+1)),2:73]);
> 
>       }
> 
>  
> 
> Unfortunately, using for loop and cbind for a table of this size causes
> long running time.  What is the most efficient way to get the table that
> I want? 
> 
>  
> 
> Thanks for any help.
> 
> K.
> 
> 
> 
> 
> -----------------------------------------
> CONFIDENTIALITY NOTICE: This message and any attachments rel...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From petr.pikal at precheza.cz  Thu Jun 15 16:34:39 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 15 Jun 2006 16:34:39 +0200
Subject: [R] help with table partition
In-Reply-To: <93BDB4436D8C1347BCE07BDBB05835FC0237B4A3@CORPMAIL02.prod.nymex.com>
Message-ID: <44918C1F.17569.7468F6@localhost>

Hi

maybe ?split and ?t is what you want

mat<-matrix(rnorm(1000), 100,10)

mat.s<-split(data.frame(mat), rep(1:5, each=20)) 
#splits mat to list with 5 eqal submatrices

lapply(mat.s,t)
# transpose matrices in list

gives you a list of transposed tables, which is probably better than 
separate tables. Just change rep(1:5,each=20) to rep (1:170, 
each=366).

or
a quicker one without data frame

mat <- matrix(rnorm(62220*73), 62220,73)

dim(mat) <- c(366,73,170)
mat.i <- array(0,dim=c(73,366,170))
for (i in 1:170) mat[ , , i] <- t(mat[ , , i])

HTH
Petr





On 15 Jun 2006 at 9:38, Wong, Kim wrote:

Date sent:      	Thu, 15 Jun 2006 09:38:29 -0400
From:           	"Wong, Kim" <kwong at nymex.com>
To:             	<r-help at stat.math.ethz.ch>
Subject:        	[R] help with table partition

> Hi,
> 
> 
> 
> I have a test_table where the dim is 62220 by 73 (row by col)
> 
> 
> 
> I would like to partition the rows into 170 equal parts (170 tables
> where each is of dim 366 by 73), and rearrange them horizontally. The
> source codes I have:
> 
> 
> 
> for (i in 1:170) {
> 
>             c = cbind(c,test_table[(367*i+1):(367*(i+1)),2:73]);
> 
>       }
> 
> 
> 
> Unfortunately, using for loop and cbind for a table of this size
> causes long running time.  What is the most efficient way to get the
> table that I want? 
> 
> 
> 
> Thanks for any help.
> 
> K.
> 
> 
> 
> 
> -----------------------------------------
> CONFIDENTIALITY NOTICE: This message and any attachments
> rel...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From stat700004 at yahoo.co.in  Thu Jun 15 16:34:29 2006
From: stat700004 at yahoo.co.in (stat stat)
Date: Thu, 15 Jun 2006 15:34:29 +0100 (BST)
Subject: [R] Array
Message-ID: <20060615143429.29217.qmail@web8402.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060615/7ca16f11/attachment.pl 

From petr.pikal at precheza.cz  Thu Jun 15 16:46:50 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 15 Jun 2006 16:46:50 +0200
Subject: [R] matrix selection return types
In-Reply-To: <4491637A.4030705@7d4.com>
Message-ID: <44918EFA.15102.7F90EB@localhost>

Hi

On 15 Jun 2006 at 15:41, vincent at 7d4.com wrote:

Date sent:      	Thu, 15 Jun 2006 15:41:14 +0200
From:           	vincent at 7d4.com
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] matrix selection return types

> Dear Rusers,
> 
> I would like some comments about the following results
> (under R-2.2.0)
> 
>  > m = matrix(1:6 , 2 , 3)
>  > m
>       [,1] [,2] [,3]
> [1,]    1    3    5
> [2,]    2    4    6
> 
>  > z1 = m[(m[,1]==2),]
>  > z1
> [1] 2 4 6
>  > is.matrix(z1)
> [1] FALSE
> 
>  > z2 = m[(m[,1]==0),]
>  > z2
>       [,1] [,2] [,3]
>  > is.matrix(z2)
> [1] TRUE
> 
> Considered together, I'm a bit surprised about the
> returned types from z1 and z2.
> I would not have been surprised if z1 would still
> have been a matrix, or z2=NULL.

If you want z1 to be matrix see argument drop in ?"["

z1 = m[(m[,1]==2),,drop=F]

however why matrix is retained in second case i am not sure. Probably 
only if the result has exactly one dimension it is stripped from dim 
attribute by default.

HTH
Petr


> 
> There is certainly a logic behind this choice
> but it's not very clear for me,
> so any help/comment appreciated.
> 
> Thanks
> Vincent
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Thu Jun 15 16:48:52 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 15 Jun 2006 16:48:52 +0200
Subject: [R] Array
In-Reply-To: <20060615143429.29217.qmail@web8402.mail.in.yahoo.com>
Message-ID: <44918F74.15315.816D7B@localhost>

Hi

see ?array if you are interested

array(0, c(5,5,5,5,5))

gives you 5 dimensional array.

HTH
Petr


On 15 Jun 2006 at 15:34, stat stat wrote:

Date sent:      	Thu, 15 Jun 2006 15:34:29 +0100 (BST)
From:           	stat stat <stat700004 at yahoo.co.in>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Array

> Dear all R users,
> 
>   I am wondering if there is any way to define a 3 dimentional or 4
>   dimentional array in R.
> 
>   Sincerely yours,
> 
> 
> 
> thanks in advance
>  Send instant messages to your online friends
>  http://in.messenger.yahoo.com 
> 
>  Stay connected with your friends even when away from PC.  Link:
>  http://in.mobile.yahoo.com/new/messenger/  [[alternative HTML version
>  deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From davidr at rhotrading.com  Thu Jun 15 16:49:04 2006
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Thu, 15 Jun 2006 09:49:04 -0500
Subject: [R] Standard Deviation Distribution
Message-ID: <F9F2A641C593D7408925574C05A7BE7704851A@rhopost.rhotrading.com>

I'm having trouble with the standard deviation distribution
as shown on http://mathworld.wolfram.com/StandardDeviationDistribution.html .
(Eric Weisstein references Kenney and Keeping 1951, which I can't check.)

I believe the graphs they show, but when I code the function in R, according to the listed formula,
I get very different graphs.

Would someone please point out my error or tell me where it's already implemented in R?

Here is my version:
> sddist
function(s,n) {
sig2 <- n*s*s/(n-1)
2*(n/(2*sig2))^((n-1)/2) / gamma((n-1)/2) * exp(-n*s*s/(2*sig2)) * s^(n-2)
}

Version 2.3.1 (2006-06-01) on Windows XP SP2

Thanks for any help.

David L. Reiner
Rho Trading Securities, LLC
Chicago? IL? 60605
312-362-4963


From petr.pikal at precheza.cz  Thu Jun 15 16:56:59 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 15 Jun 2006 16:56:59 +0200
Subject: [R] Access and assign list sub-elements using a string such
	as	"l$a$b"
In-Reply-To: <200606151354.k5FDsU9P009391@hypatia.math.ethz.ch>
Message-ID: <4491915B.12423.88DC50@localhost>

Hi
very, very close


On 15 Jun 2006 at 13:27, Gregory Jefferis wrote:

Date sent:      	Thu, 15 Jun 2006 13:27:05 +0100
From:           	Gregory Jefferis <gsxej2 at cam.ac.uk>
To:             	"r-help-request at stat.math.ethz.ch" <r-help-request at stat.math.ethz.ch>
Forwarded to:   	<r-help at stat.math.ethz.ch>
Forwarded by:   	Gregory Jefferis <gsxej2 at cam.ac.uk>
Date forwarded: 	Thu, 15 Jun 2006 14:54:13 +0100
Subject:        	[R] Access and assign list sub-elements using a string such as
	"l$a$b"

> If I have a list I can set a sub-element as follows on the command
> line:
> 
> people=list()
> people$tom$hair="brown"
> people
> 
> But what if I have a string containing the name of the sub-element
> that I want to access?
> 
> subel= "people$tom$hair"
> 
> get(subel) # returns error
> assign(subel,"red") # silent but doesn't change list
> people

See what happens when

people<-assign(subel, "red")

HTH
Petr


> 
> The attempts above using assign/get won't do what I am trying to do
> [nor according to the help should they].  I would be very grateful for
> any suggestions.  Many thanks,
> 
> Greg.
> 
> -- 
> Gregory Jefferis, PhD                                   and:
> Research Fellow    
> Department of Zoology                                   St John's
> College University of Cambridge                                
> Cambridge Downing Street                                   CB2 1TP
> Cambridge, CB2 3EJ United Kingdom
> 
> Lab Tel: +44 (0)1223 336683                     Office: +44 (0)1223
> 339899 Lab Fax: +44 (0)1223 336676
> 
> http://www.zoo.cam.ac.uk/zoostaff/jefferis.html          
> gsxej2 at cam.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From dennis.mcleod at uchospitals.edu  Thu Jun 15 17:05:19 2006
From: dennis.mcleod at uchospitals.edu (dennis.mcleod at uchospitals.edu)
Date: Thu, 15 Jun 2006 10:05:19 -0500
Subject: [R] Install R 2.3.1 on SUSE Linux
Message-ID: <9AF5B542C1E2874DAB4E85EB850D69634DF348@UCHMBX01-DRN03S.UCHAD.uchospitals.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060615/7273ef5b/attachment.pl 

From bernarduse1 at yahoo.fr  Thu Jun 15 17:14:45 2006
From: bernarduse1 at yahoo.fr (Marc Bernard)
Date: Thu, 15 Jun 2006 17:14:45 +0200 (CEST)
Subject: [R] xyplot problem
Message-ID: <20060615151445.43551.qmail@web25814.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060615/64fa968e/attachment.pl 

From dimitris.rizopoulos at med.kuleuven.be  Thu Jun 15 17:21:26 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 15 Jun 2006 17:21:26 +0200
Subject: [R] Access and assign list sub-elements using a string
	suchas	"l$a$b"
References: <4491915B.12423.88DC50@localhost>
Message-ID: <014f01c6908f$5e515770$0540210a@www.domain>


----- Original Message ----- 
From: "Petr Pikal" <petr.pikal at precheza.cz>
To: "Gregory Jefferis" <gsxej2 at cam.ac.uk>
Cc: <r-help at stat.math.ethz.ch>
Sent: Thursday, June 15, 2006 4:56 PM
Subject: Re: [R] Access and assign list sub-elements using a string 
suchas "l$a$b"


> Hi
> very, very close
>
>
> On 15 Jun 2006 at 13:27, Gregory Jefferis wrote:
>
> Date sent:      Thu, 15 Jun 2006 13:27:05 +0100
> From:           Gregory Jefferis <gsxej2 at cam.ac.uk>
> To:             "r-help-request at stat.math.ethz.ch" 
> <r-help-request at stat.math.ethz.ch>
> Forwarded to:   <r-help at stat.math.ethz.ch>
> Forwarded by:   Gregory Jefferis <gsxej2 at cam.ac.uk>
> Date forwarded: Thu, 15 Jun 2006 14:54:13 +0100
> Subject:        [R] Access and assign list sub-elements using a 
> string such as
> "l$a$b"
>
>> If I have a list I can set a sub-element as follows on the command
>> line:
>>
>> people=list()
>> people$tom$hair="brown"
>> people
>>
>> But what if I have a string containing the name of the sub-element
>> that I want to access?
>>
>> subel= "people$tom$hair"
>>
>> get(subel) # returns error
>> assign(subel,"red") # silent but doesn't change list
>> people
>
> See what happens when
>
> people<-assign(subel, "red")

but I think this is not what Greg wanted; the above just assigns "red" 
to object 'people' (i.e., check `str(assign(subel, "red"))'). If I 
understood correctly, the following could be of help:

people <- list()
people$tom$hair <- "brown"
people
#################
subel <- "people$tom$hair"
eval(parse(text = subel))
eval(parse(text = paste(subel, "<- 'red'")))
people


Best,
Dimitris


----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


> HTH
> Petr
>
>
>>
>> The attempts above using assign/get won't do what I am trying to do
>> [nor according to the help should they].  I would be very grateful 
>> for
>> any suggestions.  Many thanks,
>>
>> Greg.
>>
>> -- 
>> Gregory Jefferis, PhD                                   and:
>> Research Fellow
>> Department of Zoology                                   St John's
>> College University of Cambridge
>> Cambridge Downing Street                                   CB2 1TP
>> Cambridge, CB2 3EJ United Kingdom
>>
>> Lab Tel: +44 (0)1223 336683                     Office: +44 (0)1223
>> 339899 Lab Fax: +44 (0)1223 336676
>>
>> http://www.zoo.cam.ac.uk/zoostaff/jefferis.html
>> gsxej2 at cam.ac.uk
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
> Petr Pikal
> petr.pikal at precheza.cz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From avilella at gmail.com  Thu Jun 15 17:18:50 2006
From: avilella at gmail.com (Albert Vilella)
Date: Thu, 15 Jun 2006 16:18:50 +0100
Subject: [R] bubbleplot for matrix
In-Reply-To: <8d5a36350606141347l5519591bh3d1515d6bc6d2574@mail.gmail.com>
References: <8d5a36350606141347l5519591bh3d1515d6bc6d2574@mail.gmail.com>
Message-ID: <1150384730.12773.3.camel@localhost>

Thanks Bogdan for the reply,

I almost got it working, but in my case, the rownames and colnames are
strings, not numbers, and I guess that this is a problem when using your
snippet:

a <-
matrix(sample(1:5,100,replace=TRUE),nrow=10,dimnames=list(1:10,5*1:10))
rownames(a) =
c("aed","fde","fda","yxj","ijk","ddd","gcd","sbe","adc","asd")
colnames(a) =
c("aed","fde","fda","yxj","ijk","ddd","gcd","sbe","adc","asd")
x <- y <- z <- vector()
for (i in 1:nrow(a)) {
  x <- c(x,rep(rownames(a)[i],ncol(a)))
  y <- c(y,colnames(a))
  z <- c(z,a[i,])
}
symbols(as.numeric(x),as.numeric(y),z,inches=0.2,bg="khaki")
text(as.numeric(x),as.numeric(y),labels=z)

> symbols(as.numeric(x),as.numeric(y),z,inches=0.2,bg="khaki")
Error in plot.window(xlim, ylim, log, asp, ...) : 
	need finite 'xlim' values
In addition: Warning messages:
1: NAs introduced by coercion 
2: NAs introduced by coercion 
3: no finite arguments to min; returning Inf 
4: no finite arguments to max; returning -Inf 
5: no finite arguments to min; returning Inf 
6: no finite arguments to max; returning -Inf

Any guess?

Thanks in advance,

    Albert.

On Wed, 2006-06-14 at 16:47 -0400, bogdan romocea wrote:
> Here's an example. By the way, I find that it's more convenient (where
> applicable) to keep the data in 3 vectors/factors rather than one
> matrix/data frame.
> 
> a <- matrix(sample(1:5,100,replace=TRUE),nrow=10,dimnames=list(1:10,5*1:10))
> x <- y <- z <- vector()
> for (i in 1:nrow(a)) {
>   x <- c(x,rep(rownames(a)[i],ncol(a)))
>   y <- c(y,colnames(a))
>   z <- c(z,a[i,])
> }
> symbols(as.numeric(x),as.numeric(y),z,inches=0.2,bg="khaki")
> text(as.numeric(x),as.numeric(y),labels=z)
> 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Albert Vilella
> > Sent: Tuesday, June 13, 2006 7:11 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] bubbleplot for matrix
> >
> > Hi all,
> >
> > I would like to ask if it is possible to use bubbleplot for a 20x20
> > matrix, instead of a dataframe with factors in columns.
> >
> > The idea would be to get a tabular representation with bubbles like in
> > Rnews_2006_2 article, which look very nice.
> >
> > Thanks in advance,
> >
> >     Albert.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >


From ivowel at gmail.com  Thu Jun 15 17:27:19 2006
From: ivowel at gmail.com (ivo welch)
Date: Thu, 15 Jun 2006 11:27:19 -0400
Subject: [R] y-axis label location relative to tick label width?
Message-ID: <50d1c22d0606150827k61431890r1fc02e31e517cd1b@mail.gmail.com>

Dear R experts:  I have an odd question:  would it be better to
calculate the location of the axis label relative to the furthest
protruding tick label?  the background is that I have my y-tick labels
oriented horizontally rather than vertically.  if R chooses to set a
tick mark at "2", the y label should be relatively close to the axis.
if R chooses to put 1.998, it should sit farther (because otherwise
the "1" may touch the y label).  possible?  regards,  /ivo


From mikewolfgang at gmail.com  Thu Jun 15 17:29:23 2006
From: mikewolfgang at gmail.com (Mike Wolfgang)
Date: Thu, 15 Jun 2006 11:29:23 -0400
Subject: [R] how to set up parallel environment?
Message-ID: <e668df8c0606150829q6dacb629vf1c341e0f53c619e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060615/f2cdca1b/attachment.pl 

From jsw9c at uic.edu  Thu Jun 15 17:36:39 2006
From: jsw9c at uic.edu (John S. Walker)
Date: Thu, 15 Jun 2006 10:36:39 -0500
Subject: [R] Repost: Estimation when interaction is present: How do I get
	get the parameters from nlme?
Message-ID: <da277754916b5979b7ed69356a477fc4@uic.edu>

Gday,

This is a repost since I only had one direct reply and I remain 
mystified- This
may be stupidity on my part but it may not be so simple.




In brief, my problem is I'm not sure how to extract parameter 
values/effect sizes from a nonlinear
regression model with a significant interaction term.

My data sets are dose response curves (force and dose) for muscle that 
also have two treatments applied
Treatment A (A- or A+) and Treatment B (B-/B+). A single muscle was 
used for each experiment - a full dose response curve and one treatment 
from the matrix A*B (A-/B-, A+/B-, A-/B+ and A+,B+). There are 8 
replicates for each combination of treatments
We fit a dose response curve to each experiment with parameters upper, 
ed50 and slope; we expect treatment A to change upper and ed50. We want 
to know if treatment B blocks the effect of treatment A and if so to 
what degree.
This is similar to the Ludbrook example in Venables and Ripley, however 
they only had one treatment and I have two.

my approach

The dataframe is structured like this:
expt 	treatA 	treatB 	dose 	force.
1	-		-		0.1		20
1	-		-		0.2		40
...
4	+		+		0.1		20
4	+

I used a groupedData object:  mydata=groupedData(force ~ dose | expt)

I used an nlme obect to model the data as follows (pseudocode):

myfit.nlme <- nlme(force ~ ss_tpl(dose, upper, ed50,slope), 
fixed=list(ed50~factor(treatA)*factor(treatB)))


The function ss_tpl  is a properly debugged and fully functional
selfstarting three parameter logistic function that I wrote- no problem 
here. In my analysis
I also included fixed terms for the other fit parameters; upper and 
slope, but my main problem  is with  the
ed50 so that's all I've included here.

Running an anova on the resulting object (anova(myfit.nlme) I found the 
A -/B- (control) to
be significantly different from zero, treatment A was significantly 
different, treatment B had no significant
effect  and there was a significant interaction between treatment A and 
treatment B.

The interaction term is likely to be real. The treatments are on
sequential steps in a pathway and treatment B may be blocking the
effect of treatment A, i.e. treatment B alone has no effect because it
blocks a pathway that is not active, treatment A reduces force via this
pathway and treament B therefore blocks the effect of treatment A when
used together.

 From what I understand, please correct me if I'm wrong, the parameter 
estimates from summary(model.nlme) are not correct for main effects if 
a significant interaction is present. For example in my data treatment 
B alone has no signifcant effect in the anova but the interaction term 
A:B is significant. I believe The summary estimate for B is the 
estimate across all levels of A. What I want to do is pull out the 
estimate for B when A is not present. I suppose I can do it manually 
from the list of coefficients from nls or fit a oneway model with 
treatment levels A, B, AB. But I was kind of hoping there was some 
extractor function.

The reason I need this is that the co-authors want to include a table 
of  parameter values  with std errs or confidence intervals ala:

Treat	upper	ed50	slope
A-/B-		x		x		x		<- shows value for comparison to control studies
A+/B-	x		x		x		<-Shows A is working0
A-/B+	x		x		x		<- Shows B has no effect alone
A+/B	+	x		x		x 		<-shows B blocks A (not necessarily total)
	
So back to my question,How do I extract estimates of the parameters 
from my model object for a
specific combination of factors including the interaction term.
   i.e. what is the ed50 (and std err) for A-/B-, A+/B-, A-/B+, A+/B+ ?


I think this is a fair question and one that many biomedical scientists 
would need.


From kevin.thorpe at utoronto.ca  Thu Jun 15 17:39:24 2006
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Thu, 15 Jun 2006 11:39:24 -0400
Subject: [R] xyplot problem
In-Reply-To: <20060615151445.43551.qmail@web25814.mail.ukl.yahoo.com>
References: <20060615151445.43551.qmail@web25814.mail.ukl.yahoo.com>
Message-ID: <44917F2C.9050207@utoronto.ca>

Marc Bernard wrote:
> Dear all,
>    
>   I have created  the following data (that you can run) in order to  explain my problem:
>    
>   y <- rep(c(1,2), 8)
>   id <- rep(1:8,each=2)
> x1 <- rep(c("-a","+a"), each = 8)
> x2 <- rep(c("-b","+b"), each = 2, times = 4)
> x3 <- rep(c("-c", "+c"), each = 4,2)
> df <- data.frame(cbind(id,y,x1,x2,x3))
>    
>   If I do:
>    
>   xyplot(y~ x3|x1*x2,data=df,groups=id,type = "b")
>    
>   then my id's  are joined by lines which is what I want. However when I wanted to add an horizontal line for each panel by using "panel.abline" the lines disapears, i.e.
>    
>   xyplot(y~ x3|x1*x2,data=df,groups=id,type = "b",panel=function(x,y)
> {
> panel.xyplot(x,y)
> panel.abline(h=1.5)
> }
> ) 
>    
>    
>   I would be grateful if someone can tell me how can I correct the second statement in order to have horizontal lines for each panel and each id are joined  by lines.
>    
>   Thank you,
>    
>   Bernard,

This appears to work for me.

xyplot(y~ x3|x1*x2,data=df,groups=id,type = "b",
   panel=function(x,y,groups,...) {
     panel.superpose(x,y,groups,...)
     panel.abline(h=1.5)})


-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Department of Public Health Sciences
Faculty of Medicine, University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.946.3297


From afshart at exchange.sba.miami.edu  Thu Jun 15 17:44:34 2006
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Thu, 15 Jun 2006 11:44:34 -0400
Subject: [R] appending
Message-ID: <6BCB4D493A447546A8126F24332056E803A9EF23@school1.business.edu>

Hi Hank,
Thanks so much for looking at this. 
Yes, it does what I want and is much more efficient than my code.
I have a couple questions, please see my inserted comments after your
tapply statement and within your for loop.  Also, how do you accomplish
what you mentioned RE the concatation in the naming? 
cheers,
Dave
Ps - just noticed that a quick fix to my code would be to use unlist
towards my original
result delta.patient, then the creative indexing I used would not have
been needed (but your code still more efficient). 



comp.CAND.frm <- read.table("comp.CAND.frm.frm", header=TRUE)
names(comp.CAND.frm) <- make.names(gsub("_", ".", names
(comp.CAND.frm)), unique=TRUE)
#

creatine.function.new.R.help = function(delta.0.Y.0=50, gamma=40,
   dat=comp.CAND.frm) 
{
   ## function to calcuate the delta.i, i.e. the percent
   ## leftover ## gamma = rate of Cr going into bucket, e.g., mg/hr
   ## delta.0.Y.0 = product of delta.0 and Y.0 at baseline ##
unlist(
        tapply(dat$Ucr, dat$Patient.no, function(Cs)  
	{			## Cs is the argument to the function
defined 
				## for this tapply statement
				## Cs will equal Ucr?
   	n.obs <- length(Cs)	
   	Y=numeric(n.obs+1)
   	Y[1] = delta.0.Y.0 + gamma
   	delta = numeric(n.obs)
		for (i in 1:n.obs) delta[i] <-  ## why delta[i] outside
the {}?
		{
		d <- (Y[i] - Cs[i])/Y[i]	## why not just call
this d[i]?
                Y[i+1] = delta[i] * Y[i] + gamma
                d	## the output of each iteration; set equal to
d[i] each time?
		}
        delta	# result of function(Cs) used in tapply
	}
	     )
       )
}
dim(comp.CAND.frm)
6*24
creatine.function.new()
### The names are the concatenation of the patient and the observation
## e.g. patient 24 observation 6 is labeled 246.




-----Original Message-----
From: Martin Henry H. Stevens [mailto:HStevens at muohio.edu] 
Sent: Thursday, June 15, 2006 7:00 AM
To: Afshartous, David
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] appending

Hi Dave,
Does this do what you want?
Hank

comp.CAND.frm <- read.table("comp.CAND.frm.frm", header=TRUE)
names(comp.CAND.frm) <- make.names(gsub("_", ".", names
(comp.CAND.frm)), unique=TRUE)

creatine.function.new = function(delta.0.Y.0=50, gamma=40,
   dat=comp.CAND.frm) {
   ## function to calcuate the delta.i, i.e. the percent
   ## leftover ## gamma = rate of Cr going into bucket, e.g., mg/hr
   ## delta.0.Y.0 = product of delta.0 and Y.0 at baseline ##

##
unlist(
        tapply(dat$Ucr, dat$Patient.no, function(Cs)  {
   n.obs <- length(Cs)
   Y=numeric(n.obs+1)
   Y[1] = delta.0.Y.0 + gamma
   delta = numeric(n.obs)
		for (i in 1:n.obs) delta[i] <- {
		d <- (Y[i] - Cs[i])/Y[i]
                 Y[i+1] = delta[i] * Y[i] + gamma
                 d
		}
              delta}
             )
        )
}
dim(comp.CAND.frm)
6*24
creatine.function.new()
### The names are the concatenation of the patient and the observation
## e.g. patient 24 observation 6 is labeled 246.

On Jun 14, 2006, at 2:35 PM, Afshartous, David wrote:

> Hank,
> Attached is the dataframe that can be supplied for the argument 
> comp.CAND.frm.  The argument for comp.LIS.frm can be deleted since
> it currently isn't used.   The other arguments can be set as:
> delta.0.Y.0 = 50
> gamma = 40.
> cheers,
> dave
>
>
>
>
> -----Original Message-----
> From: Martin Henry H. Stevens [mailto:HStevens at MUOhio.edu]
> Sent: Wednesday, June 14, 2006 1:23 PM
> To: Afshartous, David
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] appending
>
> Hi David,
> It would be helpful if you supply a little data, upon which this would

> operate.
> Hank
> On Jun 14, 2006, at 12:31 PM, Afshartous, David wrote:
>
>>  All,
>>
>> In the function below I have 24 individuals and 6 calculations per 
>> individual.
>> The 6 calculations are collected each time in a 1:24 loop when 
>> calculating "delta".
>>
>> I'd like to collect all 144 = 24*6 calculations in one vector 
>> ("delta.patient.comb").
>> The function works as is via indexing, but is there an easier way to 
>> collect the measurements via appendinng the 6 measurements each time 
>> to the current set?  I couldn't find anything in Intro to R on 
>> appending.
>>
>> cheers,
>> Dave
>> ps - please respond directly to afshar at miami.edu
>>
>>
>>
>> creatine.function.new = function(delta.0.Y.0, gamma, comp.LIS.frm,
>> comp.CAND.frm) { ## function to calcuate the delta.i, i.e. the 
>> percent
>
>> ## leftover ## gamma = rate of Cr going into bucket, e.g., mg/hr ## 
>> delta.0.Y.0 = product of delta.0 and Y.0 at baseline ##
>> Y.1 = delta.0.Y.0 + gamma
>> delta = numeric(6)
>> delta.patient = numeric(24)
>> delta.patient.comb = numeric(144)
>> ##
>> 	for (k in 1:24) {		## each patient
>> 		patient.k.CAND = which(comp.CAND.frm$Patient_no == k)
>> 		Ucr.CAND.patient.k = comp.CAND.frm$Ucr[patient.k.CAND]
>> 		C = Ucr.CAND.patient.k  ## 6 observed creatanine values
> for each
>> patient
>> 		delta[1] = (Y.1 - C[1])/Y.1
>> 		Y.i = Y.1
>> 		delta.i = delta[1]
>> 		for (i in 1:5) {		## six measurments per
>> patient
>> 			Y.i.plus.1 = delta.i * Y.i + gamma
>> 			delta.i.plus.1 = (Y.i.plus.1 -
>> C[i+1])/Y.i.plus.1
>> 			delta[i+1] = delta.i.plus.1
>> 			delta.i = delta[i+1]
>> 			Y.i = Y.i.plus.1
>> 		}
>> 		delta.patient[k] = list(delta)	
>> 		delta.patient.comb[(6*(k-1)+1):(6*(k-1)+ 6)] = delta
>> 	}
>> list(delta.patient, delta.patient.comb) }
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting- 
>> guide.html
>
> Dr. M. Hank H. Stevens, Assistant Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
>
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/~stevenmh/
> http://www.muohio.edu/ecology/
> http://www.muohio.edu/botany/
> "E Pluribus Unum"
>
>
>
>
> <comp.CAND.frm.frm>

Dr. M. Hank H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"


From LI at nsabp.pitt.edu  Thu Jun 15 17:44:50 2006
From: LI at nsabp.pitt.edu (Li, Jia)
Date: Thu, 15 Jun 2006 11:44:50 -0400
Subject: [R] A question about stepwise procedures: step function
Message-ID: <D70CBC108DFBD446862A6E1F6F0B4A15EB1286@nsabpmail.nsabp.pitt.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060615/5f767c85/attachment.pl 

From jholtman at gmail.com  Thu Jun 15 17:58:12 2006
From: jholtman at gmail.com (jim holtman)
Date: Thu, 15 Jun 2006 11:58:12 -0400
Subject: [R] matrix selection return types
In-Reply-To: <4491637A.4030705@7d4.com>
References: <4491637A.4030705@7d4.com>
Message-ID: <644e1f320606150858hf9725adm7c801ba61277c733@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060615/0605308c/attachment.pl 

From petr.pikal at precheza.cz  Thu Jun 15 17:59:58 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 15 Jun 2006 17:59:58 +0200
Subject: [R] Access and assign list sub-elements using a string
	suchas	"l$a$b"
In-Reply-To: <014f01c6908f$5e515770$0540210a@www.domain>
Message-ID: <4491A01E.31235.C29412@localhost>

Hi

yes you are correct, I remembered there is something with eval from 
older posts but did not find a connection to parse from eval help 
page. Shouldn't there be a link? Or even an example?

Best regards
Petr


On 15 Jun 2006 at 17:21, Dimitris Rizopoulos wrote:

From:           	"Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.be>
To:             	"Petr Pikal" <petr.pikal at precheza.cz>, <gsxej2 at cam.ac.uk>
Copies to:      	<r-help at stat.math.ethz.ch>
Subject:        	Re: [R] Access and assign list sub-elements using a string suchas	"l$a$b"
Date sent:      	Thu, 15 Jun 2006 17:21:26 +0200

> 
> ----- Original Message ----- 
> From: "Petr Pikal" <petr.pikal at precheza.cz>
> To: "Gregory Jefferis" <gsxej2 at cam.ac.uk>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Thursday, June 15, 2006 4:56 PM
> Subject: Re: [R] Access and assign list sub-elements using a string
> suchas "l$a$b"
> 
> 
> > Hi
> > very, very close
> >
> >
> > On 15 Jun 2006 at 13:27, Gregory Jefferis wrote:
> >
> > Date sent:      Thu, 15 Jun 2006 13:27:05 +0100
> > From:           Gregory Jefferis <gsxej2 at cam.ac.uk>
> > To:             "r-help-request at stat.math.ethz.ch" 
> > <r-help-request at stat.math.ethz.ch>
> > Forwarded to:   <r-help at stat.math.ethz.ch>
> > Forwarded by:   Gregory Jefferis <gsxej2 at cam.ac.uk>
> > Date forwarded: Thu, 15 Jun 2006 14:54:13 +0100
> > Subject:        [R] Access and assign list sub-elements using a
> > string such as "l$a$b"
> >
> >> If I have a list I can set a sub-element as follows on the command
> >> line:
> >>
> >> people=list()
> >> people$tom$hair="brown"
> >> people
> >>
> >> But what if I have a string containing the name of the sub-element
> >> that I want to access?
> >>
> >> subel= "people$tom$hair"
> >>
> >> get(subel) # returns error
> >> assign(subel,"red") # silent but doesn't change list
> >> people
> >
> > See what happens when
> >
> > people<-assign(subel, "red")
> 
> but I think this is not what Greg wanted; the above just assigns "red"
> to object 'people' (i.e., check `str(assign(subel, "red"))'). If I
> understood correctly, the following could be of help:
> 
> people <- list()
> people$tom$hair <- "brown"
> people
> #################
> subel <- "people$tom$hair"
> eval(parse(text = subel))
> eval(parse(text = paste(subel, "<- 'red'")))
> people
> 
> 
> Best,
> Dimitris
> 
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://med.kuleuven.be/biostat/
>      http://www.student.kuleuven.be/~m0390867/dimitris.htm
> 
> 
> > HTH
> > Petr
> >
> >
> >>
> >> The attempts above using assign/get won't do what I am trying to do
> >> [nor according to the help should they].  I would be very grateful
> >> for any suggestions.  Many thanks,
> >>
> >> Greg.
> >>
> >> -- 
> >> Gregory Jefferis, PhD                                   and:
> >> Research Fellow
> >> Department of Zoology                                   St John's
> >> College University of Cambridge Cambridge Downing Street           
> >>                        CB2 1TP Cambridge, CB2 3EJ United Kingdom
> >>
> >> Lab Tel: +44 (0)1223 336683                     Office: +44 (0)1223
> >> 339899 Lab Fax: +44 (0)1223 336676
> >>
> >> http://www.zoo.cam.ac.uk/zoostaff/jefferis.html
> >> gsxej2 at cam.ac.uk
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >
> > Petr Pikal
> > petr.pikal at precheza.cz
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> 
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> 

Petr Pikal
petr.pikal at precheza.cz


From pinard at iro.umontreal.ca  Thu Jun 15 18:00:19 2006
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Thu, 15 Jun 2006 12:00:19 -0400
Subject: [R] Install R 2.3.1 on SUSE Linux
In-Reply-To: <9AF5B542C1E2874DAB4E85EB850D69634DF348@UCHMBX01-DRN03S.UCHAD.uchospitals.edu>
References: <9AF5B542C1E2874DAB4E85EB850D69634DF348@UCHMBX01-DRN03S.UCHAD.uchospitals.edu>
Message-ID: <20060615160019.GA1176@alcyon.progiciels-bpi.ca>

[dennis.mcleod at uchospitals.edu]

>I am new to Linux, and I am trying to install R 2.3.1 on SUSE Linux
>10.0. The RPM installer, YAST, states that I need libgfortran.so.0.

There is a SuSE 10.0 machine somewhere.  Yes: I installed R on it,
and it works well there:

   $ rpm -qf /usr/lib/libgfortran.so
   gcc-fortran-4.0.2_20050901-3

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca


From HStevens at MUOhio.edu  Thu Jun 15 18:05:40 2006
From: HStevens at MUOhio.edu (Martin Henry H. Stevens)
Date: Thu, 15 Jun 2006 12:05:40 -0400
Subject: [R] Repost: Estimation when interaction is present: How do I
	get get the parameters from nlme?
In-Reply-To: <da277754916b5979b7ed69356a477fc4@uic.edu>
References: <da277754916b5979b7ed69356a477fc4@uic.edu>
Message-ID: <2E8BEF25-2548-4477-AA21-7D1B58F40DAE@MUOhio.edu>

Hi John,
I think a solution is to
1. recode A and B as a single factor, AB, with four levels,
2. define each fixed effect as a function of  AB minus the intercept  
(e.g.  ed50 ~ as.factor(AB)-1).
3. extract the tTable as a data.frame with  summary(model)$tTable.

I will be interested to see what other folks suggest.
Cheers,
Hank

  and then run the Probably not the best, nor the worst solution,  
might be to recode A and B
On Jun 15, 2006, at 11:36 AM, John S. Walker wrote:

> Gday,
>
> This is a repost since I only had one direct reply and I remain
> mystified- This
> may be stupidity on my part but it may not be so simple.
>
>
>
>
> In brief, my problem is I'm not sure how to extract parameter
> values/effect sizes from a nonlinear
> regression model with a significant interaction term.
>
> My data sets are dose response curves (force and dose) for muscle that
> also have two treatments applied
> Treatment A (A- or A+) and Treatment B (B-/B+). A single muscle was
> used for each experiment - a full dose response curve and one  
> treatment
> from the matrix A*B (A-/B-, A+/B-, A-/B+ and A+,B+). There are 8
> replicates for each combination of treatments
> We fit a dose response curve to each experiment with parameters upper,
> ed50 and slope; we expect treatment A to change upper and ed50. We  
> want
> to know if treatment B blocks the effect of treatment A and if so to
> what degree.
> This is similar to the Ludbrook example in Venables and Ripley,  
> however
> they only had one treatment and I have two.
>
> my approach
>
> The dataframe is structured like this:
> expt 	treatA 	treatB 	dose 	force.
> 1	-		-		0.1		20
> 1	-		-		0.2		40
> ...
> 4	+		+		0.1		20
> 4	+
>
> I used a groupedData object:  mydata=groupedData(force ~ dose | expt)
>
> I used an nlme obect to model the data as follows (pseudocode):
>
> myfit.nlme <- nlme(force ~ ss_tpl(dose, upper, ed50,slope),
> fixed=list(ed50~factor(treatA)*factor(treatB)))
>
>
> The function ss_tpl  is a properly debugged and fully functional
> selfstarting three parameter logistic function that I wrote- no  
> problem
> here. In my analysis
> I also included fixed terms for the other fit parameters; upper and
> slope, but my main problem  is with  the
> ed50 so that's all I've included here.
>
> Running an anova on the resulting object (anova(myfit.nlme) I found  
> the
> A -/B- (control) to
> be significantly different from zero, treatment A was significantly
> different, treatment B had no significant
> effect  and there was a significant interaction between treatment A  
> and
> treatment B.
>
> The interaction term is likely to be real. The treatments are on
> sequential steps in a pathway and treatment B may be blocking the
> effect of treatment A, i.e. treatment B alone has no effect because it
> blocks a pathway that is not active, treatment A reduces force via  
> this
> pathway and treament B therefore blocks the effect of treatment A when
> used together.
>
>  From what I understand, please correct me if I'm wrong, the parameter
> estimates from summary(model.nlme) are not correct for main effects if
> a significant interaction is present. For example in my data treatment
> B alone has no signifcant effect in the anova but the interaction term
> A:B is significant. I believe The summary estimate for B is the
> estimate across all levels of A. What I want to do is pull out the
> estimate for B when A is not present. I suppose I can do it manually
> from the list of coefficients from nls or fit a oneway model with
> treatment levels A, B, AB. But I was kind of hoping there was some
> extractor function.
>
> The reason I need this is that the co-authors want to include a table
> of  parameter values  with std errs or confidence intervals ala:
>
> Treat	upper	ed50	slope
> A-/B-		x		x		x		<- shows value for comparison to control studies
> A+/B-	x		x		x		<-Shows A is working0
> A-/B+	x		x		x		<- Shows B has no effect alone
> A+/B	+	x		x		x 		<-shows B blocks A (not necessarily total)
> 	
> So back to my question,How do I extract estimates of the parameters
> from my model object for a
> specific combination of factors including the interaction term.
>    i.e. what is the ed50 (and std err) for A-/B-, A+/B-, A-/B+, A+/B 
> + ?
>
>
> I think this is a fair question and one that many biomedical  
> scientists
> would need.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html

Dr. M. Hank H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"


From spencer.graves at pdf.com  Thu Jun 15 18:13:02 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 15 Jun 2006 09:13:02 -0700
Subject: [R] Garch Warning
In-Reply-To: <d4c57560606142028j16c7dcb4r587b31ba0c4c03b7@mail.gmail.com>
References: <d4c57560606130354h4d42bfc5x691457ae9d3536fd@mail.gmail.com>	
	<4490C809.6010204@pdf.com>
	<d4c57560606142028j16c7dcb4r587b31ba0c4c03b7@mail.gmail.com>
Message-ID: <4491870E.3060201@pdf.com>

<see inline>

Arun Kumar Saha wrote:
> Dear Spencer,
>  
> Can you be more clear on your suggestion?
> 
>  
> On 6/15/06, *Spencer Graves* <spencer.graves at pdf.com 
> <mailto:spencer.graves at pdf.com>> wrote:
> 
>              Your example is not reproducible.  

	  Can you find a simple example, preferably obtained either from a 
simple simulation or from a minor modification of an example included in 
some of the standard documentation.  This listserve sometimes refuses to 
accept attachments, so I would not suggest you attach a data set unless 
it was quite small.  You could, for example, try reducing the size of 
your data set, e.g., using a binary search to find the smallest number 
of observations that would still generate the same error message.

	  Alternatively, the posting guide 
(www.R-project.org/posting-guide.html) includes the following:

'When providing examples, it is best to give an R command that 
constructs the data, as in the matrix() expression above. For more 
complicated data structures, dump("x", file=stdout()) will print an 
expression that will recreate the object x.'

Have you tried walking
>     through the code line by line after "debug(garch)"?  

	  I know I've discussed this issue on this listserve before. 
Therefore, I tried RSiteSearch("graves debug").  This produced 77 hits, 
many of which were not relevant.  The following seemed to be: 
"http://finzi.psych.upenn.edu/R/Rhelp02a/archive/68215.html".

After you've tried
>     that, if you'd still like help from this listserve, please submit a
>     simple, self-contained / reproducible example, as suggested the posting
>     guide! "www.R-project.org/posting-guide.html
>     <http://www.R-project.org/posting-guide.html>".
> 
>              hope this helps.
>              spencer graves
> 
>     Arun Kumar Saha wrote:
>      > Dear all R-users,
>      >
>      > I wanted to fit a Garch(1,1) model to a dataset by:
>      >
>      >> garch1 = garch( na.omit(dat))
>      >
>      > But I got a warning message while executing, which is:
>      >
>      >> Warning message:
>      >> NaNs produced in: sqrt(pred$e)
>      >
>      > The garch parameters that I got are:
>      >
>      >
>      >> garch1
>      >
>      > Call:
>      > garch(x = na.omit(dat))
>      >
>      > Coefficient(s):
>      >        a0         a1         b1
>      > 1.212e-04  1.001e+00  1.111e-14
>      >
>      > Can any one please tell me that why got this message? What is the
>     remedy?
>      >
>      > Thanks and Regards
>      >
>      >       [[alternative HTML version deleted]]
>      >
>      > ______________________________________________
>      > R-help at stat.math.ethz.ch <mailto:R-help at stat.math.ethz.ch>
>     mailing list
>      > https://stat.ethz.ch/mailman/listinfo/r-help
>      > PLEASE do read the posting guide!
>     http://www.R-project.org/posting-guide.html
> 
> 
> 
>


From kwong at nymex.com  Thu Jun 15 18:29:32 2006
From: kwong at nymex.com (Wong, Kim)
Date: Thu, 15 Jun 2006 12:29:32 -0400
Subject: [R] help with table partition
Message-ID: <93BDB4436D8C1347BCE07BDBB05835FC0237B4A4@CORPMAIL02.prod.nymex.com>


Hi, thank you all for the help.

The split function works very well.

I have an additional question.  If I have a matrix of prices (row = 30,
col = 2) in matrix P

P:

30 40
31.5 42
....
....
....

32 43

What is the quickest way to get a new matrix, where each entry is the
ln(Pt/Pt-1)?

I have no prob doing this using a loop, but that might not be most
efficient if my table is huge.  Moreover, I've read the apply/lapply
functions, but I could not get the right parameters to use.

Thank you all for help.
K.


-----Original Message-----
From: Petr Pikal [mailto:petr.pikal at precheza.cz] 
Sent: Thursday, June 15, 2006 10:35 AM
To: Wong, Kim
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] help with table partition

Hi

maybe ?split and ?t is what you want

mat<-matrix(rnorm(1000), 100,10)

mat.s<-split(data.frame(mat), rep(1:5, each=20)) 
#splits mat to list with 5 eqal submatrices

lapply(mat.s,t)
# transpose matrices in list

gives you a list of transposed tables, which is probably better than 
separate tables. Just change rep(1:5,each=20) to rep (1:170, 
each=366).

or
a quicker one without data frame

mat <- matrix(rnorm(62220*73), 62220,73)

dim(mat) <- c(366,73,170)
mat.i <- array(0,dim=c(73,366,170))
for (i in 1:170) mat[ , , i] <- t(mat[ , , i])

HTH
Petr





On 15 Jun 2006 at 9:38, Wong, Kim wrote:

Date sent:      	Thu, 15 Jun 2006 09:38:29 -0400
From:           	"Wong, Kim" <kwong at nymex.com>
To:             	<r-help at stat.math.ethz.ch>
Subject:        	[R] help with table partition

> Hi,
> 
> 
> 
> I have a test_table where the dim is 62220 by 73 (row by col)
> 
> 
> 
> I would like to partition the rows into 170 equal parts (170 tables
> where each is of dim 366 by 73), and rearrange them horizontally. The
> source codes I have:
> 
> 
> 
> for (i in 1:170) {
> 
>             c = cbind(c,test_table[(367*i+1):(367*(i+1)),2:73]);
> 
>       }
> 
> 
> 
> Unfortunately, using for loop and cbind for a table of this size
> causes long running time.  What is the most efficient way to get the
> table that I want? 
> 
> 
> 
> Thanks for any help.
> 
> K.
> 
> 
> 
> 
> -----------------------------------------
> CONFIDENTIALITY NOTICE: This message and any attachments
> rel...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


-----------------------------------------
CONFIDENTIALITY NOTICE: This message and any attachments rel...{{dropped}}


From Mike.Prager at noaa.gov  Thu Jun 15 18:32:31 2006
From: Mike.Prager at noaa.gov (Michael H. Prager)
Date: Thu, 15 Jun 2006 12:32:31 -0400
Subject: [R] Saving R graphics with version 2.3.1
In-Reply-To: <44915FBA.2090803@stats.uwo.ca>
References: <20060614194727.ywkprua39pck8gw0@agenda.dpi.inpe.br>
	<44915FBA.2090803@stats.uwo.ca>
Message-ID: <44918B9F.1080003@noaa.gov>

In response to the OP's query, we use numerous R scripts that save 
graphics as PNG and EPS through savePlot() commands.  We have not 
experienced any problems with R 2.3.1.

MHP


on 6/15/2006 9:25 AM Duncan Murdoch said the following:
> ilka at dpi.inpe.br wrote:
>   
>> HI !
>>
>> I?ve installed the latest R version (2.3.1) and I?ve had problems to 
>> save my graphics as JPEG, PNG and the others available formats.
>> R saves the file with no extension. Then, I have to open this file 
>> using an image visualizer and save it as PNG, for instance.
>>   
>>     

-- 
Michael Prager, Ph.D.
Southeast Fisheries Science Center
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
** Opinions expressed are personal, not official.  No
** official endorsement of any product is made or implied.


From dhajage at gmail.com  Thu Jun 15 18:40:41 2006
From: dhajage at gmail.com (David Hajage)
Date: Thu, 15 Jun 2006 18:40:41 +0200
Subject: [R] Name of a column
Message-ID: <a725cda30606150940t51136163ke1f2a8a2433c046b@mail.gmail.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060615/8bdef5dc/attachment.pl 

From alessandro at idsia.ch  Thu Jun 15 18:55:15 2006
From: alessandro at idsia.ch (Alessandro Antonucci)
Date: Thu, 15 Jun 2006 18:55:15 +0200
Subject: [R] Compact sums in functions definitions
Message-ID: <20060615165515.GA6104@idsia.ch>

I'm trying to make more compact the definition
of a function as for example:

f <- function(x) 2/x+3/x

by simply defining the array of coefficients

arr = c(2,3)

and setting:

g <- function(x) sum((arr/x))

Everything seems to work fine because the values returned 
by f and g result coincident for different values of their
argument, but when I try to plot the function g using:

x = seq(-1,1,.01)
plot(x,g(x))

I receive the errors/warnings:

>Error in xy.coords(x, y, xlabel, ylabel, log) : 
>        'x' and 'y' lengths differ
>In addition: Warning message:
>longer object length
>        is not a multiple of shorter object length in: b/t 
>Execution halted

Any idea about that?

Thanks.

Alessandro

-- 
============================================================
Alessandro Antonucci
Dalle Molle Institute for Artificial Intelligence (IDSIA)
at Idsia			e-mail: alessandro at idsia.ch
Galleria 2			web:   idsia.ch/~alessandro
Via Cantonale			mobile:   +39 339-567-23-28
CH-6928				tel:       +41 58-666-66-69
Manno - Lugano			fax:       +41 58-666-66-61
Switzerland			skype: alessandro.antonucci


From j.van_den_hoff at fz-rossendorf.de  Thu Jun 15 18:55:46 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Thu, 15 Jun 2006 18:55:46 +0200
Subject: [R] Access and assign list sub-elements using a string	suchas
 "l$a$b"
In-Reply-To: <4491A01E.31235.C29412@localhost>
References: <4491A01E.31235.C29412@localhost>
Message-ID: <44919112.8070405@fz-rossendorf.de>

Petr Pikal wrote:
> Hi
> 
> yes you are correct, I remembered there is something with eval from 
> older posts but did not find a connection to parse from eval help 
> page. Shouldn't there be a link? Or even an example?

would be a good thing to do (there only is a link from parse to eval). 
after all eval(parse(text = some_string)) is what many users (which 
might come from matlab/octave where this _would_ suffice) want when they 
try out eval(some_string)). and a standard eval(parse(text=...) example 
on the eval manpage (where most people probably would look, would be 
very good.
> 
> Best regards
> Petr
> 
> 
> On 15 Jun 2006 at 17:21, Dimitris Rizopoulos wrote:
> 
> From:           	"Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.be>
> To:             	"Petr Pikal" <petr.pikal at precheza.cz>, <gsxej2 at cam.ac.uk>
> Copies to:      	<r-help at stat.math.ethz.ch>
> Subject:        	Re: [R] Access and assign list sub-elements using a string suchas	"l$a$b"
> Date sent:      	Thu, 15 Jun 2006 17:21:26 +0200
> 
>> ----- Original Message ----- 
>> From: "Petr Pikal" <petr.pikal at precheza.cz>
>> To: "Gregory Jefferis" <gsxej2 at cam.ac.uk>
>> Cc: <r-help at stat.math.ethz.ch>
>> Sent: Thursday, June 15, 2006 4:56 PM
>> Subject: Re: [R] Access and assign list sub-elements using a string
>> suchas "l$a$b"
>>
>>
>>> Hi
>>> very, very close
>>>
>>>
>>> On 15 Jun 2006 at 13:27, Gregory Jefferis wrote:
>>>
>>> Date sent:      Thu, 15 Jun 2006 13:27:05 +0100
>>> From:           Gregory Jefferis <gsxej2 at cam.ac.uk>
>>> To:             "r-help-request at stat.math.ethz.ch" 
>>> <r-help-request at stat.math.ethz.ch>
>>> Forwarded to:   <r-help at stat.math.ethz.ch>
>>> Forwarded by:   Gregory Jefferis <gsxej2 at cam.ac.uk>
>>> Date forwarded: Thu, 15 Jun 2006 14:54:13 +0100
>>> Subject:        [R] Access and assign list sub-elements using a
>>> string such as "l$a$b"
>>>
>>>> If I have a list I can set a sub-element as follows on the command
>>>> line:
>>>>
>>>> people=list()
>>>> people$tom$hair="brown"
>>>> people
>>>>
>>>> But what if I have a string containing the name of the sub-element
>>>> that I want to access?
>>>>
>>>> subel= "people$tom$hair"
>>>>
>>>> get(subel) # returns error
>>>> assign(subel,"red") # silent but doesn't change list
>>>> people
>>> See what happens when
>>>
>>> people<-assign(subel, "red")
>> but I think this is not what Greg wanted; the above just assigns "red"
>> to object 'people' (i.e., check `str(assign(subel, "red"))'). If I
>> understood correctly, the following could be of help:
>>
>> people <- list()
>> people$tom$hair <- "brown"
>> people
>> #################
>> subel <- "people$tom$hair"
>> eval(parse(text = subel))
>> eval(parse(text = paste(subel, "<- 'red'")))
>> people
>>
>>
>> Best,
>> Dimitris
>>
>>
>> ----
>> Dimitris Rizopoulos
>> Ph.D. Student
>> Biostatistical Centre
>> School of Public Health
>> Catholic University of Leuven
>>
>> Address: Kapucijnenvoer 35, Leuven, Belgium
>> Tel: +32/(0)16/336899
>> Fax: +32/(0)16/337015
>> Web: http://med.kuleuven.be/biostat/
>>      http://www.student.kuleuven.be/~m0390867/dimitris.htm
>>
>>
>>> HTH
>>> Petr
>>>
>>>
>>>> The attempts above using assign/get won't do what I am trying to do
>>>> [nor according to the help should they].  I would be very grateful
>>>> for any suggestions.  Many thanks,
>>>>
>>>> Greg.
>>>>
>>>> -- 
>>>> Gregory Jefferis, PhD                                   and:
>>>> Research Fellow
>>>> Department of Zoology                                   St John's
>>>> College University of Cambridge Cambridge Downing Street           
>>>>                        CB2 1TP Cambridge, CB2 3EJ United Kingdom
>>>>
>>>> Lab Tel: +44 (0)1223 336683                     Office: +44 (0)1223
>>>> 339899 Lab Fax: +44 (0)1223 336676
>>>>
>>>> http://www.zoo.cam.ac.uk/zoostaff/jefferis.html
>>>> gsxej2 at cam.ac.uk
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide!
>>>> http://www.R-project.org/posting-guide.html
>>> Petr Pikal
>>> petr.pikal at precheza.cz
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>>
>>
>> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>>
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From a_elhabti at yahoo.fr  Thu Jun 15 18:58:00 2006
From: a_elhabti at yahoo.fr (Ahmed Elhabti)
Date: Thu, 15 Jun 2006 18:58:00 +0200 (CEST)
Subject: [R] Simulation / bayesian model
Message-ID: <20060615165800.78330.qmail@web27804.mail.ukl.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060615/6ede6cea/attachment.pl 

From JeeBee at troefpunt.nl  Thu Jun 15 19:00:09 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Thu, 15 Jun 2006 19:00:09 +0200
Subject: [R] R with tcl/tk 8.5
Message-ID: <pan.2006.06.15.17.00.08.505141@troefpunt.nl>

Hi List,

Is it possible to tell R to use tcl/tk 8.5?
My R package seems to depend on libtcl8.4.so.
However, in Windows it seems to be possible to set
TCL_LIBRARY and MY_TCLTK.
Is there something similar possible in Linux?

I have installed:

$ rpm -q R
R-2.3.1-1.fc5

$ locate libtcl8
/usr/lib/libtcl8.4.so
/usr/local/lib/libtcl8.5.so


From brown at biology.utah.edu  Thu Jun 15 19:02:47 2006
From: brown at biology.utah.edu (Tim Brown)
Date: Thu, 15 Jun 2006 11:02:47 -0600
Subject: [R] flipping a plot vertically?
Message-ID: <6.2.5.6.0.20060615105516.03787d60@dandelion.org>

Hi,

This seems like an obvious question but I can't find the answer in 
the "par" help document --- I'd like to make a plot where the 0,0 
point is in the top left of the screen rather than bottom left... . 
This is just a regular plot that is flipped vertically -- i.e.  the 
x-values will run along the top of the plot and the y-values will go 
from the top left (0,0) to the bottom left (0,ymax) --  I assume 
there is a simple way to do this but I can't find it.
Any suggestions?

Thanks!

Tim


From ccleland at optonline.net  Thu Jun 15 19:04:32 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 15 Jun 2006 13:04:32 -0400
Subject: [R] Name of a column
In-Reply-To: <a725cda30606150940t51136163ke1f2a8a2433c046b@mail.gmail.com>
References: <a725cda30606150940t51136163ke1f2a8a2433c046b@mail.gmail.com>
Message-ID: <44919320.5070107@optonline.net>

X <- as.logical(rep(c(0,1), each=13))

names(X) <- LETTERS

which(X)
  N  O  P  Q  R  S  T  U  V  W  X  Y  Z
14 15 16 17 18 19 20 21 22 23 24 25 26

names(which(X))
  [1] "N" "O" "P" "Q" "R" "S" "T" "U" "V" "W" "X" "Y" "Z"

?which

David Hajage wrote:
> Hello,
> 
> My problem is quite simply, but I didn't find any solution...
> 
> I have a vector :
> 
>> truc
>    longueur30 longueur40 longueur50 longueur60 longueur70 longueur80
> longueur90
> 34      FALSE      FALSE      FALSE      FALSE       TRUE      FALSE
> FALSE
> 
> I would like to have the name of the column where there is "TRUE".
> 
>> colnames(truc)
> [1] "longueur30" "longueur40" "longueur50" "longueur60" "longueur70"
> [6] "longueur80" "longueur90"
> 
>> truc[truc == T]
> [1] TRUE
> 
>> colnames(truc[truc == T])
> NULL
> 
> How can I do it ?
> 
> Thank you for your help.
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From tristan.rouyer at ifremer.fr  Thu Jun 15 19:13:25 2006
From: tristan.rouyer at ifremer.fr (Rouyer Tristan)
Date: Thu, 15 Jun 2006 19:13:25 +0200
Subject: [R] dpss.taper for spectral estimation
Message-ID: <200606151913.25235.tristan.rouyer@ifremer.fr>

Hello R users,

I'm running R version 2.2.1 with a fedora core 4 linux os.

I want to use dpss (discrete prolate spheroidal sequences) tapers for 
consistent spectral estimation. Such tapers can be computed using the 
dpss.taper() function from the waveslim package (Brandon Whitcher 2004).
The function computes the tapers, but I didn't manage to get the eigenvalues 
which are associated with.
The documentation indicates 6 values for the output of this function and I 
only get one.
How can I get these eigenvalues ?

Thanks in advance for your help,

Tristan


From br44114 at gmail.com  Thu Jun 15 19:14:06 2006
From: br44114 at gmail.com (bogdan romocea)
Date: Thu, 15 Jun 2006 13:14:06 -0400
Subject: [R] bubbleplot for matrix
In-Reply-To: <1150384730.12773.3.camel@localhost>
References: <8d5a36350606141347l5519591bh3d1515d6bc6d2574@mail.gmail.com>
	<1150384730.12773.3.camel@localhost>
Message-ID: <8d5a36350606151014p11453b8cwc67846db631a6667@mail.gmail.com>

This works, though I'm not sure why symbols() complains about
axes=FALSE while fulfilling the request.

a <- matrix(sample(1:5,100,replace=TRUE),nrow=10)
rownames(a) <- c("aed","fde","fda","yxj","ijk","ddd","gcd","sbe","adc","asd")
colnames(a) <- c("aed","fde","fda","yxj","ijk","ddd","gcd","sbe","adc","asd")
x <- y <- z <- vector()
for (i in 1:nrow(a)) {
  x <- c(x,rep(rownames(a)[i],ncol(a)))
  y <- c(y,colnames(a))
  z <- c(z,a[i,])
}

xp <- as.numeric(as.factor(x))
yp <- as.numeric(as.factor(y))
symbols(xp,yp,z,inches=0.2,bg="khaki",axes=FALSE)
axis(1,at=1:length(unique(x)),labels=sort(unique(x)))
axis(2,at=1:length(unique(y)),labels=sort(unique(y)))
box()
text(xp,yp,labels=z)



On 6/15/06, Albert Vilella <avilella at gmail.com> wrote:
> Thanks Bogdan for the reply,
>
> I almost got it working, but in my case, the rownames and colnames are
> strings, not numbers, and I guess that this is a problem when using your
> snippet:
>
> a <-
> matrix(sample(1:5,100,replace=TRUE),nrow=10,dimnames=list(1:10,5*1:10))
> rownames(a) =
> c("aed","fde","fda","yxj","ijk","ddd","gcd","sbe","adc","asd")
> colnames(a) =
> c("aed","fde","fda","yxj","ijk","ddd","gcd","sbe","adc","asd")
> x <- y <- z <- vector()
> for (i in 1:nrow(a)) {
>  x <- c(x,rep(rownames(a)[i],ncol(a)))
>  y <- c(y,colnames(a))
>  z <- c(z,a[i,])
> }
> symbols(as.numeric(x),as.numeric(y),z,inches=0.2,bg="khaki")
> text(as.numeric(x),as.numeric(y),labels=z)
>
> > symbols(as.numeric(x),as.numeric(y),z,inches=0.2,bg="khaki")
> Error in plot.window(xlim, ylim, log, asp, ...) :
>        need finite 'xlim' values
> In addition: Warning messages:
> 1: NAs introduced by coercion
> 2: NAs introduced by coercion
> 3: no finite arguments to min; returning Inf
> 4: no finite arguments to max; returning -Inf
> 5: no finite arguments to min; returning Inf
> 6: no finite arguments to max; returning -Inf
>
> Any guess?
>
> Thanks in advance,
>
>    Albert.
>
> On Wed, 2006-06-14 at 16:47 -0400, bogdan romocea wrote:
> > Here's an example. By the way, I find that it's more convenient (where
> > applicable) to keep the data in 3 vectors/factors rather than one
> > matrix/data frame.
> >
> > a <- matrix(sample(1:5,100,replace=TRUE),nrow=10,dimnames=list(1:10,5*1:10))
> > x <- y <- z <- vector()
> > for (i in 1:nrow(a)) {
> >   x <- c(x,rep(rownames(a)[i],ncol(a)))
> >   y <- c(y,colnames(a))
> >   z <- c(z,a[i,])
> > }
> > symbols(as.numeric(x),as.numeric(y),z,inches=0.2,bg="khaki")
> > text(as.numeric(x),as.numeric(y),labels=z)
> >
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Albert Vilella
> > > Sent: Tuesday, June 13, 2006 7:11 AM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] bubbleplot for matrix
> > >
> > > Hi all,
> > >
> > > I would like to ask if it is possible to use bubbleplot for a 20x20
> > > matrix, instead of a dataframe with factors in columns.
> > >
> > > The idea would be to get a tabular representation with bubbles like in
> > > Rnews_2006_2 article, which look very nice.
> > >
> > > Thanks in advance,
> > >
> > >     Albert.
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
>
>


From ccleland at optonline.net  Thu Jun 15 19:16:23 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 15 Jun 2006 13:16:23 -0400
Subject: [R] Simulation / bayesian model
In-Reply-To: <20060615165800.78330.qmail@web27804.mail.ukl.yahoo.com>
References: <20060615165800.78330.qmail@web27804.mail.ukl.yahoo.com>
Message-ID: <449195E7.7070101@optonline.net>

Ahmed Elhabti wrote:
> Hi,
>    
>   (In the bayesian model, this program makes estimate the value of theta (tn)
>   with the empirical method.)
>    
>   I want to know how one can repeat this program 100 times, before to have 100 estimated values of tn.
>    
>   This is the program:
>    
>   a<-1 
> theta<-rexp(50,a)
> x<-rpois(length(theta),theta)
>   t<-rexp(a)
>   xn<-rpois(1,t)
>   tn<-(1+xn)*mean(x)/(1+mean(x))
>   tn

  my.tn <- function(){
    a <- 1
    theta <- rexp(50,a)
    x <- rpois(length(theta),theta)
    t <- rexp(a)
    xn <- rpois(1,t)
    tn <- (1+xn)*mean(x)/(1+mean(x))
    return(tn)
  }

  replicate(100, my.tn())
   [1] 0.5575221 0.5192308 1.0099010 1.2954545
   [5] 0.4949495 1.0099010 0.5238095 0.4791667
   [9] 1.8494624 1.4693878 0.5098039 0.8235294
  [13] 0.5495495 0.4565217 0.5726496 1.4042553
  [17] 0.5798319 2.2826087 1.0909091 1.0000000
  [21] 0.5327103 0.4623656 1.0291262 0.7804878
  [25] 1.4536082 0.9690722 0.9795918 0.8888889
  [29] 0.5049505 0.5762712 0.4680851 0.4623656
  [33] 0.5327103 1.3516484 0.9247312 0.4505495
  [37] 0.8888889 0.5238095 1.7394958 0.5614035
  [41] 0.5412844 0.5454545 1.0099010 0.4382022
  [45] 0.4444444 0.5000000 0.9898990 0.4444444
  [49] 0.9795918 1.7777778 0.5192308 1.0196078
  [53] 0.4505495 0.5454545 1.4693878 2.2300885
  [57] 0.5238095 0.5098039 1.8021978 1.9591837
  [61] 0.4047619 1.0654206 0.5283019 0.4680851
  [65] 0.5652174 2.5490196 1.1735537 1.0566038
  [69] 2.1308411 2.5728155 0.4186047 1.0099010
  [73] 0.4897959 1.3695652 0.4897959 1.0825688
  [77] 0.5145631 0.4382022 1.0476190 0.5145631
  [81] 0.5535714 0.4565217 0.5238095 0.4680851
  [85] 0.4736842 0.5098039 0.5327103 1.9381443
  [89] 0.5454545 0.5238095 0.5412844 0.4117647
  [93] 0.4444444 1.1379310 0.5327103 1.0990991
  [97] 0.4382022 0.4444444 1.1803279 0.5967742

?replicate

>   Thank you very much.
> 
>  __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From JeeBee at troefpunt.nl  Thu Jun 15 19:41:12 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Thu, 15 Jun 2006 19:41:12 +0200
Subject: [R] Compact sums in functions definitions
References: <20060615165515.GA6104@idsia.ch>
Message-ID: <pan.2006.06.15.17.41.08.821123@troefpunt.nl>


Hi Alessandro,

The problem is that arr/x isn't quite doing what you
thought it was.
arr / x is something like
c(arr[1] / x[1], arr[2] / x[2], arr[1] / x[3], ...)
What I mean is, it divides arr[i] by x[i] and tries to lengthen
arr by repeating itself (because it is shorter than x). It warns
in case length(x) is not a multiple of length(arr).
(or the other way around if arr was longer than x)

I made a nice one for you:
g <- function(x) rowSums( t(t(1/x)) %*% arr )

It is likely someone else can do it much nicer (shorter),
my R knowledge still has to be increased ...

JeeBee.


On Thu, 15 Jun 2006 18:55:15 +0200, Alessandro Antonucci wrote:

> I'm trying to make more compact the definition
> of a function as for example:
> 
> f <- function(x) 2/x+3/x
> 
> by simply defining the array of coefficients
> 
> arr = c(2,3)
> 
> and setting:
> 
> g <- function(x) sum((arr/x))
> 
> Everything seems to work fine because the values returned 
> by f and g result coincident for different values of their
> argument, but when I try to plot the function g using:
> 
> x = seq(-1,1,.01)
> plot(x,g(x))
> 
> I receive the errors/warnings:
> 
>>Error in xy.coords(x, y, xlabel, ylabel, log) : 
>>        'x' and 'y' lengths differ
>>In addition: Warning message:
>>longer object length
>>        is not a multiple of shorter object length in: b/t 
>>Execution halted
> 
> Any idea about that?
> 
> Thanks.
> 
> Alessandro


From pinard at iro.umontreal.ca  Thu Jun 15 19:46:07 2006
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Thu, 15 Jun 2006 13:46:07 -0400
Subject: [R] flipping a plot vertically?
In-Reply-To: <6.2.5.6.0.20060615105516.03787d60@dandelion.org>
References: <6.2.5.6.0.20060615105516.03787d60@dandelion.org>
Message-ID: <20060615174607.GA3126@alcyon.progiciels-bpi.ca>

[Tim Brown]

>This seems like an obvious question but I can't find the answer in the 
>"par" help document --- I'd like to make a plot where the 0,0 point is 
>in the top left of the screen rather than bottom left... .  [...] Any 
>suggestions?

You might retry your plot, adding an ylim=c(HIGHEST, LOWEST) argument,
that is, listing the maximum before the minimum.  For example:

   plot(1:10, ylim=c(10, 1))

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca


From davidr at rhotrading.com  Thu Jun 15 20:06:48 2006
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Thu, 15 Jun 2006 13:06:48 -0500
Subject: [R] help with table partition
Message-ID: <F9F2A641C593D7408925574C05A7BE7704854D@rhopost.rhotrading.com>

apply(log(P), 2, diff)

David L. Reiner
Rho Trading Securities, LLC
Chicago  IL  60605
312-362-4963

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Wong, Kim
Sent: Thursday, June 15, 2006 11:30 AM
To: Petr Pikal; Jacques VESLOT
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] help with table partition


Hi, thank you all for the help.

The split function works very well.

I have an additional question.  If I have a matrix of prices (row = 30,
col = 2) in matrix P

P:

30 40
31.5 42
....
....
....

32 43

What is the quickest way to get a new matrix, where each entry is the
ln(Pt/Pt-1)?

I have no prob doing this using a loop, but that might not be most
efficient if my table is huge.  Moreover, I've read the apply/lapply
functions, but I could not get the right parameters to use.

Thank you all for help.
K.


-----Original Message-----
From: Petr Pikal [mailto:petr.pikal at precheza.cz] 
Sent: Thursday, June 15, 2006 10:35 AM
To: Wong, Kim
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] help with table partition

Hi

maybe ?split and ?t is what you want

mat<-matrix(rnorm(1000), 100,10)

mat.s<-split(data.frame(mat), rep(1:5, each=20)) 
#splits mat to list with 5 eqal submatrices

lapply(mat.s,t)
# transpose matrices in list

gives you a list of transposed tables, which is probably better than 
separate tables. Just change rep(1:5,each=20) to rep (1:170, 
each=366).

or
a quicker one without data frame

mat <- matrix(rnorm(62220*73), 62220,73)

dim(mat) <- c(366,73,170)
mat.i <- array(0,dim=c(73,366,170))
for (i in 1:170) mat[ , , i] <- t(mat[ , , i])

HTH
Petr





On 15 Jun 2006 at 9:38, Wong, Kim wrote:

Date sent:      	Thu, 15 Jun 2006 09:38:29 -0400
From:           	"Wong, Kim" <kwong at nymex.com>
To:             	<r-help at stat.math.ethz.ch>
Subject:        	[R] help with table partition

> Hi,
> 
> 
> 
> I have a test_table where the dim is 62220 by 73 (row by col)
> 
> 
> 
> I would like to partition the rows into 170 equal parts (170 tables
> where each is of dim 366 by 73), and rearrange them horizontally. The
> source codes I have:
> 
> 
> 
> for (i in 1:170) {
> 
>             c = cbind(c,test_table[(367*i+1):(367*(i+1)),2:73]);
> 
>       }
> 
> 
> 
> Unfortunately, using for loop and cbind for a table of this size
> causes long running time.  What is the most efficient way to get the
> table that I want? 
> 
> 
> 
> Thanks for any help.
> 
> K.
> 
> 
> 
> 
> -----------------------------------------
> CONFIDENTIALITY NOTICE: This message and any attachments
> rel...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


-----------------------------------------
CONFIDENTIALITY NOTICE: This message and any attachments\ re...{{dropped}}


From JeeBee at troefpunt.nl  Thu Jun 15 20:13:34 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Thu, 15 Jun 2006 20:13:34 +0200
Subject: [R] Compact sums in functions definitions
References: <20060615165515.GA6104@idsia.ch>
	<pan.2006.06.15.17.41.08.821123@troefpunt.nl>
Message-ID: <pan.2006.06.15.18.13.32.529879@troefpunt.nl>



Slightly better:

h <- function(x) rowSums( as.matrix(1/x) %*% arr )


On Thu, 15 Jun 2006 19:41:12 +0200, JeeBee wrote:

> 
> Hi Alessandro,
> 
> The problem is that arr/x isn't quite doing what you
> thought it was.
> arr / x is something like
> c(arr[1] / x[1], arr[2] / x[2], arr[1] / x[3], ...)
> What I mean is, it divides arr[i] by x[i] and tries to lengthen
> arr by repeating itself (because it is shorter than x). It warns
> in case length(x) is not a multiple of length(arr).
> (or the other way around if arr was longer than x)
> 
> I made a nice one for you:
> g <- function(x) rowSums( t(t(1/x)) %*% arr )
> 
> It is likely someone else can do it much nicer (shorter),
> my R knowledge still has to be increased ...
> 
> JeeBee.
> 
> 
> On Thu, 15 Jun 2006 18:55:15 +0200, Alessandro Antonucci wrote:
> 
>> I'm trying to make more compact the definition
>> of a function as for example:
>> 
>> f <- function(x) 2/x+3/x
>> 
>> by simply defining the array of coefficients
>> 
>> arr = c(2,3)
>> 
>> and setting:
>> 
>> g <- function(x) sum((arr/x))
>> 
>> Everything seems to work fine because the values returned 
>> by f and g result coincident for different values of their
>> argument, but when I try to plot the function g using:
>> 
>> x = seq(-1,1,.01)
>> plot(x,g(x))
>> 
>> I receive the errors/warnings:
>> 
>>>Error in xy.coords(x, y, xlabel, ylabel, log) : 
>>>        'x' and 'y' lengths differ
>>>In addition: Warning message:
>>>longer object length
>>>        is not a multiple of shorter object length in: b/t 
>>>Execution halted
>> 
>> Any idea about that?
>> 
>> Thanks.
>> 
>> Alessandro
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From brown at biology.utah.edu  Thu Jun 15 20:18:28 2006
From: brown at biology.utah.edu (Tim Brown)
Date: Thu, 15 Jun 2006 12:18:28 -0600
Subject: [R] flipping a plot vertically?
In-Reply-To: <20060615174607.GA3126@alcyon.progiciels-bpi.ca>
References: <6.2.5.6.0.20060615105516.03787d60@dandelion.org>
	<20060615174607.GA3126@alcyon.progiciels-bpi.ca>
Message-ID: <6.2.5.6.0.20060615121649.03778e20@dandelion.org>

That works great.  Thanks.

Any idea how to get the x axis numbers to go 
along the top instead of the bottom?

tim

At 11:46 AM 6/15/2006, you wrote:
>[Tim Brown]
>
>>This seems like an obvious question but I can't 
>>find the answer in the "par" help document --- 
>>I'd like to make a plot where the 0,0 point is 
>>in the top left of the screen rather than 
>>bottom left... .  [...] Any suggestions?
>
>You might retry your plot, adding an ylim=c(HIGHEST, LOWEST) argument,
>that is, listing the maximum before the minimum.  For example:
>
>   plot(1:10, ylim=c(10, 1))
>
>--
>Fran?ois Pinard   http://pinard.progiciels-bpi.ca


From ggrothendieck at gmail.com  Thu Jun 15 20:20:55 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 15 Jun 2006 14:20:55 -0400
Subject: [R] log returns (was: Re:  help with table partition)
Message-ID: <971536df0606151120l2044795bm58d5bc1445eb3e0b@mail.gmail.com>

Please use a descriptive subject and not tag onto a prior thread
for new topics.

Assuming the first row is time 1 and the second row is time 2 and
so on try:

diff(log(P))


On 6/15/06, Wong, Kim <kwong at nymex.com> wrote:
>
> Hi, thank you all for the help.
>
> The split function works very well.
>
> I have an additional question.  If I have a matrix of prices (row = 30,
> col = 2) in matrix P
>
> P:
>
> 30 40
> 31.5 42
> ....
> ....
> ....
>
> 32 43
>
> What is the quickest way to get a new matrix, where each entry is the
> ln(Pt/Pt-1)?
>
> I have no prob doing this using a loop, but that might not be most
> efficient if my table is huge.  Moreover, I've read the apply/lapply
> functions, but I could not get the right parameters to use.
>
> Thank you all for help.
> K.
>
>
> -----Original Message-----
> From: Petr Pikal [mailto:petr.pikal at precheza.cz]
> Sent: Thursday, June 15, 2006 10:35 AM
> To: Wong, Kim
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] help with table partition
>
> Hi
>
> maybe ?split and ?t is what you want
>
> mat<-matrix(rnorm(1000), 100,10)
>
> mat.s<-split(data.frame(mat), rep(1:5, each=20))
> #splits mat to list with 5 eqal submatrices
>
> lapply(mat.s,t)
> # transpose matrices in list
>
> gives you a list of transposed tables, which is probably better than
> separate tables. Just change rep(1:5,each=20) to rep (1:170,
> each=366).
>
> or
> a quicker one without data frame
>
> mat <- matrix(rnorm(62220*73), 62220,73)
>
> dim(mat) <- c(366,73,170)
> mat.i <- array(0,dim=c(73,366,170))
> for (i in 1:170) mat[ , , i] <- t(mat[ , , i])
>
> HTH
> Petr
>
>
>
>
>
> On 15 Jun 2006 at 9:38, Wong, Kim wrote:
>
> Date sent:              Thu, 15 Jun 2006 09:38:29 -0400
> From:                   "Wong, Kim" <kwong at nymex.com>
> To:                     <r-help at stat.math.ethz.ch>
> Subject:                [R] help with table partition
>
> > Hi,
> >
> >
> >
> > I have a test_table where the dim is 62220 by 73 (row by col)
> >
> >
> >
> > I would like to partition the rows into 170 equal parts (170 tables
> > where each is of dim 366 by 73), and rearrange them horizontally. The
> > source codes I have:
> >
> >
> >
> > for (i in 1:170) {
> >
> >             c = cbind(c,test_table[(367*i+1):(367*(i+1)),2:73]);
> >
> >       }
> >
> >
> >
> > Unfortunately, using for loop and cbind for a table of this size
> > causes long running time.  What is the most efficient way to get the
> > table that I want?
> >
> >
> >
> > Thanks for any help.
> >
> > K.
> >
> >
> >
> >
> > -----------------------------------------
> > CONFIDENTIALITY NOTICE: This message and any attachments
> > rel...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
>
> Petr Pikal
> petr.pikal at precheza.cz
>
>
> -----------------------------------------
> CONFIDENTIALITY NOTICE: This message and any attachments rel...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From hannesalazar at gmail.com  Thu Jun 15 20:29:04 2006
From: hannesalazar at gmail.com (yohannes alazar)
Date: Thu, 15 Jun 2006 19:29:04 +0100
Subject: [R] data managment
In-Reply-To: <1150368381.3501.197.camel@dhcp-82.wolf.ox.ac.uk>
References: <ae94396d0606140835r94bad75jd5ab1cb2c96cc70@mail.gmail.com>
	<1150368381.3501.197.camel@dhcp-82.wolf.ox.ac.uk>
Message-ID: <ae94396d0606151129i16115edgea4f1d3f080045b9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060615/cfd6d36e/attachment.pl 

From ccleland at optonline.net  Thu Jun 15 20:32:25 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 15 Jun 2006 14:32:25 -0400
Subject: [R] flipping a plot vertically?
In-Reply-To: <6.2.5.6.0.20060615121649.03778e20@dandelion.org>
References: <6.2.5.6.0.20060615105516.03787d60@dandelion.org>
	<20060615174607.GA3126@alcyon.progiciels-bpi.ca>
	<6.2.5.6.0.20060615121649.03778e20@dandelion.org>
Message-ID: <4491A7B9.8030000@optonline.net>

plot(runif(20), xaxt="n", ylim=c(1,0))
axis(side = 3)

Tim Brown wrote:
> That works great.  Thanks.
> 
> Any idea how to get the x axis numbers to go 
> along the top instead of the bottom?
> 
> tim
> 
> At 11:46 AM 6/15/2006, you wrote:
>> [Tim Brown]
>>
>>> This seems like an obvious question but I can't 
>>> find the answer in the "par" help document --- 
>>> I'd like to make a plot where the 0,0 point is 
>>> in the top left of the screen rather than 
>>> bottom left... .  [...] Any suggestions?
>> You might retry your plot, adding an ylim=c(HIGHEST, LOWEST) argument,
>> that is, listing the maximum before the minimum.  For example:
>>
>>   plot(1:10, ylim=c(10, 1))
>>
>> --
>> Fran?ois Pinard   http://pinard.progiciels-bpi.ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From gunter.berton at gene.com  Thu Jun 15 20:35:00 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 15 Jun 2006 11:35:00 -0700
Subject: [R] flipping a plot vertically?
In-Reply-To: <6.2.5.6.0.20060615121649.03778e20@dandelion.org>
Message-ID: <004b01c690aa$691b8890$295afea9@gne.windows.gene.com>

> 
> Any idea how to get the x axis numbers to go 
> along the top instead of the bottom?
> 

Use xaxt = 'n' in your plot call (?par for details) to suppress plotting of
the axis and then add the axis via a call to axis().

If you do a lot of plotting, you may wish to purchase a copy of Murrell's R
GRAPHICS or V&R's MASS. At the very least, do read the relevant sections of
an Introduction to R and the Reference Manual, as I believe this sort of
thing is covered there.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tim Brown
> Sent: Thursday, June 15, 2006 11:18 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] flipping a plot vertically?
> 
> That works great.  Thanks.
> 
> Any idea how to get the x axis numbers to go 
> along the top instead of the bottom?
> 
> tim
> 
> At 11:46 AM 6/15/2006, you wrote:
> >[Tim Brown]
> >
> >>This seems like an obvious question but I can't 
> >>find the answer in the "par" help document --- 
> >>I'd like to make a plot where the 0,0 point is 
> >>in the top left of the screen rather than 
> >>bottom left... .  [...] Any suggestions?
> >
> >You might retry your plot, adding an ylim=c(HIGHEST, LOWEST) 
> argument,
> >that is, listing the maximum before the minimum.  For example:
> >
> >   plot(1:10, ylim=c(10, 1))
> >
> >--
> >Fran?ois Pinard   http://pinard.progiciels-bpi.ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From michela.cameletti at unibg.it  Thu Jun 15 17:06:48 2006
From: michela.cameletti at unibg.it (michela.cameletti at unibg.it)
Date: Thu, 15 Jun 2006 17:06:48 +0200 (CEST)
Subject: [R] SSPIR problem
Message-ID: <2384.193.204.253.13.1150384008.squirrel@mailserver.unibg.it>

Dear R-Users,
I'm using SSPIR package for a spatio-temporal application.
Is it possible to modify the structure of the involved matrixes (Fmat,
Gmat, Vmat,Wmat)?
I want to create a model like this

#y(t)=k*theta(t)+epsilon(t)
#theta(t)=h*theta(t-1)+eta(t)
#epsilon(t) N(0,V)         V=sigma2*I
#eta(t)     N(0,W)        W=sigma2_eta

where the state variable theta has dimension 1(p=1) and at each time time
the observed variable y has dimension equal to d=3. Moreover I want to use
white noise errors.
But with the command
SS(y) where y is my nXd data matrix (d=observations at each time,
n=numbers of time points)
I obtain a different model.

Can you help me please?
Thanks in advance
Michela


From davidr at rhotrading.com  Thu Jun 15 20:55:45 2006
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Thu, 15 Jun 2006 13:55:45 -0500
Subject: [R] Standard Deviation Distribution - solved
Message-ID: <F9F2A641C593D7408925574C05A7BE77048553@rhopost.rhotrading.com>

It turns out that I was confused by Weisstein's double use of the variable s.
sigma^2 = N*(observed s)/(N-1), hence constant in the function. So

> sddist <- function(s,s0,n) {
  sig2 <- n*s0*s0/(n-1)
  2*(n/(2*sig2))^((n-1)/2) / gamma((n-1)/2) * exp(-n*s*s/(2*sig2)) * s^(n-2)
}

gives the plots on the web page I cited.

Thanks to Ed Pegg Jr. at MathWorld for clarifying.

David L. Reiner
Rho Trading Securities, LLC
Chicago? IL? 60605
312-362-4963


From michela.cameletti at unibg.it  Thu Jun 15 17:33:39 2006
From: michela.cameletti at unibg.it (michela.cameletti at unibg.it)
Date: Thu, 15 Jun 2006 17:33:39 +0200 (CEST)
Subject: [R] SSPIR problem
Message-ID: <3723.193.204.253.13.1150385619.squirrel@mailserver.unibg.it>

Dear R-Users,
I'm using SSPIR package for a spatio-temporal application.
Is it possible to modify the structure of the involved matrixes (Fmat,
Gmat, Vmat,Wmat)?
I want to create a model like this

#y(t)=k*theta(t)+epsilon(t)
#theta(t)=h*theta(t-1)+eta(t)
#epsilon(t) N(0,V)         V=sigma2*I
#eta(t)     N(0,W)        W=sigma2_eta

where the state variable theta has dimension 1(p=1) and at each time time
the observed variable y has dimension equal to d=3. Moreover I want to use
white noise errors.
But with the command
SS(y) where y is my nXd data matrix (d=observations at each time,
n=numbers of time points)
I obtain a different model.

Can you help me please?
Thanks in advance
Michela


From zwang at scharp.org  Thu Jun 15 21:19:06 2006
From: zwang at scharp.org (Zhu Wang)
Date: Thu, 15 Jun 2006 12:19:06 -0700
Subject: [R] Problem with Fortran 95 code in R
Message-ID: <4491B2AA.1030404@scharp.org>

Dear helpers:

I am trying to call some Fortran 95 code with R-2.3.0 but had problem as 
below. Any pointers would be appreciated. Thanks, Zhu Wang

On Linux,
R CMD SHLIB rfh.f
R
> dyn.load("rfh.so")
Error in dyn.load(x, as.logical(local), as.logical(now)) :
       unable to load shared library '/home/zwang/rf/rfh.so':
 /home/zwang/rf/rfh.so: undefined symbol: _gfortran_allocate

On Unix,
R CMD SHLIB rfh.f
R
> dyn.load("rfh.so")
Error in dyn.load(x, as.logical(local), as.logical(now)) :
       unable to load shared library '/home/zwang/rf/rfh.so':
 ld.so.1: /usr/local/R-2.3.0/lib/R/bin/exec/R: fatal: relocation error: 
file /home/zwang/rf/rfh.so: symbol __f90_allocate2: referenced symbol 
not found


From harshal at yahoo-inc.com  Thu Jun 15 21:35:30 2006
From: harshal at yahoo-inc.com (Harshal D Dedhia)
Date: Thu, 15 Jun 2006 12:35:30 -0700
Subject: [R] Histbackback and adding color
Message-ID: <959DC0987E39DB43B963EA5ADC9E1A2946E078@SNV-XCHMAIL2.xch.corp.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060615/b044889d/attachment.pl 

From brown at biology.utah.edu  Thu Jun 15 21:42:26 2006
From: brown at biology.utah.edu (Tim Brown)
Date: Thu, 15 Jun 2006 13:42:26 -0600
Subject: [R] flipping a plot vertically?
In-Reply-To: <4491A7B9.8030000@optonline.net>
References: <6.2.5.6.0.20060615105516.03787d60@dandelion.org>
	<20060615174607.GA3126@alcyon.progiciels-bpi.ca>
	<6.2.5.6.0.20060615121649.03778e20@dandelion.org>
	<4491A7B9.8030000@optonline.net>
Message-ID: <6.2.5.6.0.20060615133732.0377d620@dandelion.org>


From hsavin at gmail.com  Thu Jun 15 22:00:40 2006
From: hsavin at gmail.com (Heidi Savin)
Date: Thu, 15 Jun 2006 16:00:40 -0400
Subject: [R] more function in R?
Message-ID: <619993560606151300h63c60be8ob3bedc100eed82c0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060615/0a85c565/attachment.pl 

From mobygeek at yahoo.com  Thu Jun 15 22:05:40 2006
From: mobygeek at yahoo.com (context grey)
Date: Thu, 15 Jun 2006 13:05:40 -0700 (PDT)
Subject: [R] MDS with missing data?  (change topic -> isomap)
In-Reply-To: <1150363727.9428.45.camel@biol102145.oulu.fi>
Message-ID: <20060615200540.57375.qmail@web51408.mail.yahoo.com>

Thanks for the isoMDS pointer.   I found one
implementation
of isomap at  
  http://user.cs.tu-berlin.de/~astro/kmethods
though the web page suggests the code may be immature.


--- Jari Oksanen <jarioksa at sun3.oulu.fi> wrote:

> On Thu, 2006-06-15 at 07:13 +0300, Jari Oksanen
> wrote:
> 
> 
> > >
> > > 1) use nonmetric/gradient descent MDS which
> seems to
> > > allow missing data, or
> > >
> > Not the isoMDS function in MASS. if N(N-1) is a
> problem, then nonmetric 
> > MDS may not be the solution.
> 
> Sorry for the wrong information: isoMDS does handle
> NA. I remembered old
> times when I looked at the issue, but isoMDS changed
> since. Fine work!
> 
> cheers, jari oksanen
> -- 
> Jari Oksanen -- Dept Biology, Univ Oulu, 90014 Oulu,
> Finland
> email jari.oksanen at oulu.fi, homepage
> http://cc.oulu.fi/~jarioksa/
> 
>


From csardi at rmki.kfki.hu  Thu Jun 15 22:07:47 2006
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Thu, 15 Jun 2006 16:07:47 -0400
Subject: [R] more function in R?
In-Reply-To: <619993560606151300h63c60be8ob3bedc100eed82c0@mail.gmail.com>
References: <619993560606151300h63c60be8ob3bedc100eed82c0@mail.gmail.com>
Message-ID: <20060615200747.GE4227@localdomain>

a <- matrix(0, nr=10000, nc=5)
page(a, method="print")

This works on Linux at least. Also, there might be a better way...
Gabor

On Thu, Jun 15, 2006 at 04:00:40PM -0400, Heidi Savin wrote:
> I'm an R newbie and I just have a simple question.
> 
> I'm using interactive R on a linux box and I'm trying to find out if there's
> a more or a less function. The data I want to look at is larger then my
> screen size and all I end up seeing is the last fews line of data. Is there
> a way that I can paginate through the data and the results of my summary
> functions?
> 
> thanks,
> Heidi
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From Iyue.Sung at lm.mmc.com  Thu Jun 15 22:12:38 2006
From: Iyue.Sung at lm.mmc.com (Sung, Iyue)
Date: Thu, 15 Jun 2006 16:12:38 -0400
Subject: [R]  Open Position (Off-topic)
Message-ID: <B01A505EA0E3124998DF4661D313B20601C637A1@mmci-bos03fs01.mmci.ad.root>

Position: Junior Specialist, Customer Science
Company: Mercer Management Consulting
Location: Boston, MA 

Job Description: 

Mercer Management Consulting is seeking candidates with a strong
business and quantitative background for a Junior Specialist position in
its Customer Science Practice. The Customer Science Practice is a key
capability of Mercer, providing expertise in customer research and
analytic techniques with dedicated professionals proficient in advanced
statistical modeling, research design and project management. This
position will require the candidate to work closely with a broader
project team to design and perform analytical research necessary to
address client's strategic issues. Specific responsibilities include
design and implementation of data collection efforts, analysis of large
databases and survey data, and synthesis of results.

Qualifications: 

* Masters degree in Statistics/Econometrics/Marketing/Operations
Research or other related quantitative fields
* Excellent problem-solving skills and understanding of business issues
* Excellent knowledge of statistics/econometrics
* Conversant with the application of varieties of statistical techniques
(e.g. multivariate analysis, segmentation, CART, CHAID, logit modeling,
predictive modeling), preferably in marketing
* Knowledge of statistical packages such as SAS, SPSS, S+, EXCEL and
other analytical packages
* Strong oral and written communication skills
* Willingness to travel
* Relevant prior work experience in marketing positions a plus 

Excellent compensation and benefits package is available. 

Qualified individuals should submit a resume to: 

Iyue Sung, PhD
iyue.sung at mercermc.com  
---------------------------------------------------------------------------- 
This e-mail and any attachments may be confidential or legal...{{dropped}}


From hsavin at gmail.com  Thu Jun 15 22:13:06 2006
From: hsavin at gmail.com (Heidi Savin)
Date: Thu, 15 Jun 2006 16:13:06 -0400
Subject: [R] more function in R?
In-Reply-To: <20060615200747.GE4227@localdomain>
References: <619993560606151300h63c60be8ob3bedc100eed82c0@mail.gmail.com>
	<20060615200747.GE4227@localdomain>
Message-ID: <619993560606151313r32132805nc65e14530467b7a6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060615/001063dc/attachment.pl 

From mobygeek at yahoo.com  Thu Jun 15 22:15:53 2006
From: mobygeek at yahoo.com (context grey)
Date: Thu, 15 Jun 2006 13:15:53 -0700 (PDT)
Subject: [R] individual scales in random subset of pairwise distance survey
Message-ID: <20060615201553.10421.qmail@web51404.mail.yahoo.com>


Hello,

I'm curious if anyone has encounted a version of this
problem
(and it's solution) involving finding a consistent set
of scales
for subsets of survey data.

The goal is to obtain peoples' rankings of pairwise
similarity of a large
number of items, on a 1..5 scale for example, and
average these
across people to use as input to MDS:
  How similar is object A to B    on a 1..5 scale ___
  How similar is object A to C    on a 1..5 scale ___
etc.
  
Because there are many items, there are N(N-1)/2
pairs, so it is not
practical to show every pair to everyone.   Showing
people the
pairs corresponding to random subsets of the objects
seems desirable.

THe problem is that, a particular random subset might
by chance
contain objects that would all be rated "5" if one
were to see
the entire dataset.  When ranking pairs from this
subset, the scale
of 1..5 is different.  

If we ensure that each pair of people must see some
data in common,
then one can think about obtaining a set of scales,
one for each
person, that causes the data that is commonly ranked
to have
as similar scores as possible, summed across all pairs
of people.


Please let me know if you know of a standard procedure
for
this or any similar problems.    

Thank you.


From ggrothendieck at gmail.com  Thu Jun 15 23:08:28 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 15 Jun 2006 17:08:28 -0400
Subject: [R] individual scales in random subset of pairwise distance
	survey
In-Reply-To: <20060615201553.10421.qmail@web51404.mail.yahoo.com>
References: <20060615201553.10421.qmail@web51404.mail.yahoo.com>
Message-ID: <971536df0606151408p55737751m1c65df6ba06c317a@mail.gmail.com>

Perhaps you could try clustering the objects.

# generate test data
set.seed(1)
n <- 25 # number of items
mat <- matrix(0, n, n)
# use different labelling scheme if > 26 items
rownames(mat) <- colnames(mat) <- letters[1:n]
mat[lower.tri(mat)] <- sample(5, n * (n-1)/2, TRUE)
mat <- mat + t(mat) + diag(1, n)

# cluster and plot
plot(hclust(as.dist(mat)))


On 6/15/06, context grey <mobygeek at yahoo.com> wrote:
>
> Hello,
>
> I'm curious if anyone has encounted a version of this
> problem
> (and it's solution) involving finding a consistent set
> of scales
> for subsets of survey data.
>
> The goal is to obtain peoples' rankings of pairwise
> similarity of a large
> number of items, on a 1..5 scale for example, and
> average these
> across people to use as input to MDS:
>  How similar is object A to B    on a 1..5 scale ___
>  How similar is object A to C    on a 1..5 scale ___
> etc.
>
> Because there are many items, there are N(N-1)/2
> pairs, so it is not
> practical to show every pair to everyone.   Showing
> people the
> pairs corresponding to random subsets of the objects
> seems desirable.
>
> THe problem is that, a particular random subset might
> by chance
> contain objects that would all be rated "5" if one
> were to see
> the entire dataset.  When ranking pairs from this
> subset, the scale
> of 1..5 is different.
>
> If we ensure that each pair of people must see some
> data in common,
> then one can think about obtaining a set of scales,
> one for each
> person, that causes the data that is commonly ranked
> to have
> as similar scores as possible, summed across all pairs
> of people.
>
>
> Please let me know if you know of a standard procedure
> for
> this or any similar problems.
>
> Thank you.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Thu Jun 15 23:11:26 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 15 Jun 2006 17:11:26 -0400
Subject: [R] individual scales in random subset of pairwise distance
	survey
In-Reply-To: <971536df0606151408p55737751m1c65df6ba06c317a@mail.gmail.com>
References: <20060615201553.10421.qmail@web51404.mail.yahoo.com>
	<971536df0606151408p55737751m1c65df6ba06c317a@mail.gmail.com>
Message-ID: <971536df0606151411l63b6caf9hcbe3c1f2a7c08dcc@mail.gmail.com>

The diag(1,n) was not needed:


set.seed(1)
n <- 25
mat <- matrix(0, n, n)
rownames(mat) <- colnames(mat) <- letters[1:n]
mat[lower.tri(mat)] <- sample(5, n * (n-1)/2, TRUE)
mat <- mat + t(mat)
plot(hclust(as.dist(mat)))


On 6/15/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Perhaps you could try clustering the objects.
>
> # generate test data
> set.seed(1)
> n <- 25 # number of items
> mat <- matrix(0, n, n)
> # use different labelling scheme if > 26 items
> rownames(mat) <- colnames(mat) <- letters[1:n]
> mat[lower.tri(mat)] <- sample(5, n * (n-1)/2, TRUE)
> mat <- mat + t(mat) + diag(1, n)
>
> # cluster and plot
> plot(hclust(as.dist(mat)))
>
>
> On 6/15/06, context grey <mobygeek at yahoo.com> wrote:
> >
> > Hello,
> >
> > I'm curious if anyone has encounted a version of this
> > problem
> > (and it's solution) involving finding a consistent set
> > of scales
> > for subsets of survey data.
> >
> > The goal is to obtain peoples' rankings of pairwise
> > similarity of a large
> > number of items, on a 1..5 scale for example, and
> > average these
> > across people to use as input to MDS:
> >  How similar is object A to B    on a 1..5 scale ___
> >  How similar is object A to C    on a 1..5 scale ___
> > etc.
> >
> > Because there are many items, there are N(N-1)/2
> > pairs, so it is not
> > practical to show every pair to everyone.   Showing
> > people the
> > pairs corresponding to random subsets of the objects
> > seems desirable.
> >
> > THe problem is that, a particular random subset might
> > by chance
> > contain objects that would all be rated "5" if one
> > were to see
> > the entire dataset.  When ranking pairs from this
> > subset, the scale
> > of 1..5 is different.
> >
> > If we ensure that each pair of people must see some
> > data in common,
> > then one can think about obtaining a set of scales,
> > one for each
> > person, that causes the data that is commonly ranked
> > to have
> > as similar scores as possible, summed across all pairs
> > of people.
> >
> >
> > Please let me know if you know of a standard procedure
> > for
> > this or any similar problems.
> >
> > Thank you.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>


From tolga at coubros.com  Thu Jun 15 23:11:53 2006
From: tolga at coubros.com (Tolga Uzuner)
Date: Fri, 16 Jun 2006 00:11:53 +0300
Subject: [R] Touch options
Message-ID: <20060616001153.753k0r5sreo0oc44@mail.coubros.com>

Hi,

Does anyone know a package which prices simple one-touch options, say  
using Rmetrics ? I want something which simply has a discontinuous  
payoff if an asset touches a single level. No knock-in or knock-out  
features.

Thanks,
Tolga


From list.eric at gmail.com  Fri Jun 16 00:33:31 2006
From: list.eric at gmail.com (Eric Hu)
Date: Thu, 15 Jun 2006 15:33:31 -0700
Subject: [R] plot two graphs with different length of data
In-Reply-To: <448E6974.4020706@fz-rossendorf.de>
References: <c3e96b080606121728s27d055c1g1a5ee788b9bd4c88@mail.gmail.com>
	<448E6974.4020706@fz-rossendorf.de>
Message-ID: <c3e96b080606151533h22be3c2fwd54e30dc10c80ab@mail.gmail.com>

thank you, Joerg. I am able to use points() to add the new data in the
current plot.

Eric

On 6/13/06, Joerg van den Hoff <j.van_den_hoff at fz-rossendorf.de> wrote:
> Eric Hu wrote:
> > Hi I am trying to plot two data set in the same picture window without
> > overlapping with each other. I am using the format plot(x1,y1,x2,y2)
> > but get the following error message:
> >
> >> plot(as.numeric(r0[,4]),as.numeric(r0[,7]),as.numeric(r0[,4]),as.numeric(r0[,7][ind[,1]]))
> > Error in plot.window(xlim, ylim, log, asp, ...) :
> >         invalid 'ylim' value
> >
> > Can anyone tell me what went wrong? Thanks.
> >
> > Eric
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
> matplot(cbind(x1,x2), cbind(y1,y2)) might be what you want.
>
> (if x1, x2 and y1,y2 are of equal length. otherwise pad the short  ones
> with NAs or use  `matplot' with type =n (to get the scaling of the plot
> right), followed by `plot(x1,y1), lines(x2,y2)')
>


From list.eric at gmail.com  Fri Jun 16 01:05:11 2006
From: list.eric at gmail.com (Eric Hu)
Date: Thu, 15 Jun 2006 16:05:11 -0700
Subject: [R] running R in batch with stdin input
Message-ID: <c3e96b080606151605s18305df8wd53e8abc535fe18f@mail.gmail.com>

Hi I have a R script that needs to run a few times for different
systems. I use R --no-save < r.script for one system. I am trying with
no luck to use R CMD BATCH to introduce an stdin input variable for
the script. I wonder if anyone can provide the correct usage to put
the variable in the command like R CMD BATCH r.script name_variable.

Thanks.

-Eric

In the r.script I have

name <- readline("/dev/stdin")
r0 <- read.table("/usr/local/surface/$name/$name_c_r")
...

I want to get at the end:

name <- "1BRS"
r0 <- read.table("/usr/local/surface/1BRS/1BRS_c_r")
...


From jeff.hammerbacher at gmail.com  Fri Jun 16 01:43:52 2006
From: jeff.hammerbacher at gmail.com (Jeff Hammerbacher)
Date: Thu, 15 Jun 2006 16:43:52 -0700
Subject: [R] Touch options
In-Reply-To: <20060616001153.753k0r5sreo0oc44@mail.coubros.com>
References: <20060616001153.753k0r5sreo0oc44@mail.coubros.com>
Message-ID: <aa24041b0606151643r2a963952xd164a3d8f70ee579@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060615/b934c8bf/attachment.pl 

From ramasamy at cancer.org.uk  Fri Jun 16 02:45:42 2006
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 16 Jun 2006 00:45:42 +0000
Subject: [R] running R in batch with stdin input
In-Reply-To: <c3e96b080606151605s18305df8wd53e8abc535fe18f@mail.gmail.com>
References: <c3e96b080606151605s18305df8wd53e8abc535fe18f@mail.gmail.com>
Message-ID: <1150418742.3412.18.camel@dhcp-82.wolf.ox.ac.uk>

?commandArgs



On Thu, 2006-06-15 at 16:05 -0700, Eric Hu wrote:
> Hi I have a R script that needs to run a few times for different
> systems. I use R --no-save < r.script for one system. I am trying with
> no luck to use R CMD BATCH to introduce an stdin input variable for
> the script. I wonder if anyone can provide the correct usage to put
> the variable in the command like R CMD BATCH r.script name_variable.
> 
> Thanks.
> 
> -Eric
> 
> In the r.script I have
> 
> name <- readline("/dev/stdin")
> r0 <- read.table("/usr/local/surface/$name/$name_c_r")
> ...
> 
> I want to get at the end:
> 
> name <- "1BRS"
> r0 <- read.table("/usr/local/surface/1BRS/1BRS_c_r")
> ...
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From jcmartinez at banxico.org.mx  Fri Jun 16 03:16:34 2006
From: jcmartinez at banxico.org.mx (=?iso-8859-1?Q?Mart=EDnez_Ovando_Juan_Carlos?=)
Date: Thu, 15 Jun 2006 20:16:34 -0500
Subject: [R] Change the range of a 'ternaryplot'
Message-ID: <AC172321027ADC49BB891A7D7D5DF69B5D5089@BMCORREOBE1.banxico.org.mx>

Hi all

Does someone know how can I alter the range of some variables in a 'ternaryplot' of the 'vcd' package, with the aim of make a zoom in some region of the plot?

Thank you in advance

Sincerely,

	Juan Carlos


=================================== 
Juan Carlos Mart?nez Ovando 
Banco de M?xico 
Direcci?n General de Investigaci?n Econ?mica
Cinco de Mayo No. 18, 4? Piso A 
Col. Centro, Del. Cuauht?moc
06059, M?xico D.F., MEXICO 
Tel.: +52 (55) 52.37.25.94 
Fax: +52 (55) 52.37.27.03 
e-mail: jcmartinez en banxico.org.mx 
=================================== 

"2006, A?o del Bicentenario del natalicio del Benem?rito de las Am?ricas, Don Benito Ju?rez Garc?a".


From FredeA.Togersen at agrsci.dk  Fri Jun 16 08:34:41 2006
From: FredeA.Togersen at agrsci.dk (=?iso-8859-1?Q?Frede_Aakmann_T=F8gersen?=)
Date: Fri, 16 Jun 2006 08:34:41 +0200
Subject: [R] A question about stepwise procedures: step function
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC047E40ED@DJFPOST01.djf.agrsci.dk>

Well Jia, you use 'all' as a name for your dataframe, but this is also a function, see ?all. If I try it with

mydata <- data.frame(z1,z2,z3) 

all goes well.


Med venlig hilsen
Frede Aakmann T?gersen
 

 

> -----Oprindelig meddelelse-----
> Fra: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] P? vegne af Li, Jia
> Sendt: 15. juni 2006 17:45
> Til: Hong Ooi; r-help at stat.math.ethz.ch
> Emne: Re: [R] A question about stepwise procedures: step function
> 
> Sorry, I still cannot find what's wrong with it. And it seems 
> that nothing is wrong with t.
>  
> Jia
>  
> > z1<-rnorm(N,0,1)
> > z2<-rnorm(N,3,5)
> > z3<-rbinom(N,1,0.6)
> > 
> > prop.cens<-0.45                   
> > cen<-rbinom(N,1,1-prop.cens)     #<-- censor indicator:45% censor in
> the data.
> > t<- rexp(N)
> 
> > all<-data.frame(z1,z2,z3)
> > fit.model.all<- coxph(Surv(t,cen) ~z1+z2+z3,data=all) fit.model.all
> Call:
> coxph(formula = Surv(t, cen) ~ z1 + z2 + z3, data = all)
>  
> 
>         coef exp(coef) se(coef)       z    p
> z1  0.057466     1.059   0.1377  0.4173 0.68
> z2  0.000907     1.001   0.0332  0.0273 0.98
> z3 -0.349273     0.705   0.2867 -1.2184 0.22
>  
> Likelihood ratio test=1.81  on 3 df, p=0.613  n= 100 
> > reg.model.all<-step(fit.model.all)
> Start:  AIC= 376.39
>  Surv(t, cen) ~ z1 + z2 + z3 
>  
> Error in as.data.frame.default(data) : cannot coerce class "function"
> into a data.frame
> 
> 
>   _____  
> 
> From: Hong Ooi [mailto:Hong.Ooi at iag.com.au]
> Sent: Thursday, June 15, 2006 10:53 AM
> To: Li, Jia
> Subject: RE: [R] A question about stepwise procedures: step function
> 
> 
> 
> Note: This e-mail is subject to the disclaimer contained at the bottom
> of this message.
> 
>   _____  
> 
> t is the name of a function. If you have a variable called t in your
> dataset, try renaming it.
>  
> 
>   _____  
> 
> From: r-help-bounces at stat.math.ethz.ch on behalf of Li, Jia
> Sent: Thu 15/06/2006 11:52 PM
> To: r-help at stat.math.ethz.ch; ritwik.sinha at gmail.com
> Subject: Re: [R] A question about stepwise procedures: step function
> 
> 
> 
> Hi,
> 
> Step works for a Cox model. And I got the same error massage using
> stepAIC.
> 
> Jia
> 
>   _____ 
> 
> From: Ritwik Sinha [mailto:ritwik.sinha at gmail.com]
> Sent: Thursday, June 15, 2006 12:12 AM
> To: Li, Jia
> Subject: Re: [R] A question about stepwise procedures: step function
> 
> 
> Hi,
> 
> The step documentation says
> 
> "object: an object representing a model of an appropriate class
>           (mainly '"lm"' and '"glm"'). This is used as the initial
>           model in the stepwise search.
> "
> I wonder if it will work for a cox proportional hazard model. 
> You could
> try stepAIC in MASS.
> 
> 
> 
> 
> On 6/14/06, Li, Jia < LI at nsabp.pitt.edu <mailto:LI at nsabp.pitt.edu> >
> wrote:
> 
>         Dear all,
>        
>         I tried to use "step"  function to do model 
> selection, but I got
> an error massage.  What I don't understand is that data as data.frame
> worked well for my other programs, how come I cannot make it run this
> time. Could you please tell me how I can fix it?
>        
>        
> **************************************************************
> **********
> ***************************
>        
>         >all<-data.frame(z1,z2,z3)
>        
>         >fit.model.all<- coxph(Surv(t,cen) ~z1+z2+z3,data=all)
>        
>         > reg.model.all<-step(fit.model.all)
>         Start:  AIC= 689.1
>         Surv(t, cen) ~ z1 + z2 + z3
>         Error in as.data.frame.default(data) : cannot coerce class
> "function" into a data.frame
>        
> **************************************************************
> **********
> ***************************
>         Thanks a lot!
>        
>         Jia
>        
>         ______________________________________________
>         R-help at stat.math.ethz.ch mailing list
>         https://stat.ethz.ch/mailman/listinfo/r-help
>         PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>        
> 
> 
> 
> 
> --
> Ritwik Sinha
> Graduate Student
> Epidemiology and Biostatistics
> Case Western Reserve University
> 
> http://darwin.cwru.edu/~rsinha
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
>   _____  
> 
> The information transmitted in this message and its 
> attachme...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From priti.desai at kalyptorisk.com  Fri Jun 16 09:26:33 2006
From: priti.desai at kalyptorisk.com (priti desai)
Date: Fri, 16 Jun 2006 12:56:33 +0530
Subject: [R] The qurey about kolmogorov-smirnov test & adding the trendline
	to graph
Message-ID: <2AB7346A3227A74BB97F9A0D79E3E65A2B5D@mailserver.kalyptorisk.com>

I am hereby forwarding the data & method use to calculate the
Kolmogorov-Smirnov goodness of fit test made manually by me in R
launguage which deffers with the actual inbuilt formula as  shown below.
Further I have plot the graph in R. In that graph how to add trendline
(i.e. straight line passing through maximum points in plot) to a Plot.


R script is as follows please run this script to see the output in R.

========================= start ========================= 

# The data is as follows

data <- c( 0.01,  0.02, 0.04, 0.13,  0.17 , 0.19 , 0.21 , 0.27 , 0.27 ,
0.28,  0.29,  0.37,
           0.41,  0.49,  0.51,  0.52,  0.54,  0.57,  0.62,  0.63,  0.68,
0.73,  0.74, 0.79,
           0.81,  0.81,  0.82,  0.86,  0.94,  0.96,  1.02,  1.10,  1.10,
1.20,  1.29,  1.36,
           1.40,  1.41,  1.44,  1.45,  1.62,  1.67,  1.69,  1.78,  1.82,
2.11,  2.13,  2.14,
           2.24,  2.29,  2.34, 2.40,  2.46,  2.70,  2.83,  2.98,  3.00,
3.30,  3.53,  3.70,
           3.86,  3.90,  3.91,  3.98,  5.01,  5.23,  6.05,  6.12, 10.41,
10.73)



# K-S goodness of fit test (actual calculated)
print('K-S goodness of fit test')

# Ho:sample come from postulated prob distribution  vs   H1: Not Ho
# Reject Ho if o > 0.162312045 o.w. don't reject Ho at 5% los.
# Reject Ho if o > 0.194583 o.w. don't reject Ho at 1% los.
# In this case, we don't Reject Ho at 5% &1% los.

average       <- mean(data)  
print('average:')
# estimate the parameter
print('Estimation of parameter')

lambda        <- (1/average)
print(lambda)


e             <- c(1:70)
k             <- c((e-1)/70)
Fx            <- c(1 - exp(-lambda*data))
g             <- sort(Fx)
l             <- c(g-k)
m             <- c(e/70)
n             <- c(m-g)                        
o             <- max(l,n)
print('k-s stat:')
print(o)

#  K-S goodness of fit test (R inbuilt formula)


 ks.test(x= data,"pexp", alternative = c("g"),exact = NULL)



# P-P plot

 
e             <- c(1:70)
f             <- c((e-.5)/70)
Fx            <- c(1 - exp(-lambda*data))
g             <- sort(Fx)
plot(f,g)

========================= end ========================= 

The results are as follows:
  The K-S test calculated manualy giving the result as follows,
 
Ho:sample come from postulated prob distribution  vs   H1: Not Ho
Ks statistic = D = 0.04391726

 Reject Ho if o > 0.162312045 o.w. don't reject Ho at 5% los.
 Reject Ho if o > 0.194583 o.w. don't reject Ho at 1% los.
 In this case, we don't Reject Ho at 5% &1% los.

 (by inbuilt formula the output is as follows,)

 One-sample Kolmogorov-Smirnov test

data:  data 
D^+ = 0.0088, p-value = 0.9893
alternative hypothesis: greater 

Warning message:
cannot compute correct p-values with ties in: ks.test(x = data, "pexp",
alternative = c("g"), exact = NULL) 




It is requested to clarify & confirm formula derirved so as to enable 
me to cross check my calculations made manually. Further please convey 
as how to interprete the results. 
Awaiting your positive reply.

 
Regards,
Priti


From detlef.steuer at hsu-hamburg.de  Fri Jun 16 09:32:59 2006
From: detlef.steuer at hsu-hamburg.de (Detlef Steuer)
Date: Fri, 16 Jun 2006 09:32:59 +0200
Subject: [R] Install R 2.3.1 on SUSE Linux
In-Reply-To: <9AF5B542C1E2874DAB4E85EB850D69634DF348@UCHMBX01-DRN03S.UCHAD.uchospitals.edu>
References: <9AF5B542C1E2874DAB4E85EB850D69634DF348@UCHMBX01-DRN03S.UCHAD.uchospitals.edu>
Message-ID: <20060616093259.d9aa863e.detlef.steuer@hsu-hamburg.de>

Hi,

you have to add an online repository for 10.0 to your installation sources.

This is covered in the file called http://cran.r-project.org/bin/linux/suse/ReadMe.html which was added to be read. ;-)

Detlef

On Thu, 15 Jun 2006 10:05:19 -0500
<dennis.mcleod at uchospitals.edu> wrote:

> Hello:
> 
> I am new to Linux, and I am trying to install R 2.3.1 on SUSE Linux
> 10.0. The RPM installer, YAST, states that I need libgfortran.so.0.
> I have loaded Intel FORTRAN and GCC FORTRAN; but I still do not have the
> libgfortran.so.0 module YAST is asking for. 
> 
> Is there somewhere I can get this module for SUSE Linux 10.0. There is a
> post on the R-Help for this same problem; but I cannot seem to find the
> resolution.
> 
> I would appreciate any help you could provide.
> 
> Thanks
> Dennis
> 
> 
> 
> 
> ********************************************************************************
> This e-mail is intended only for the use of the individual or entity to which
> it is addressed and may contain information that is privileged and confidential.
> If the reader of this e-mail message is not the intended recipient, you are 
> hereby notified that any dissemination, distribution or copying of this
> communication is prohibited. If you have received this e-mail in error, please 
> notify the sender and destroy all copies of the transmittal. 
> 
> Thank you
> University of Chicago Hospitals
> ********************************************************************************
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From r_econometrics at yahoo.co.in  Fri Jun 16 09:52:22 2006
From: r_econometrics at yahoo.co.in (SUMANTA BASAK)
Date: Fri, 16 Jun 2006 08:52:22 +0100 (BST)
Subject: [R] Yahoo data download problem
Message-ID: <20060616075222.26546.qmail@web7606.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060616/697abf8a/attachment.pl 

From anil_rohilla at rediffmail.com  Fri Jun 16 11:13:54 2006
From: anil_rohilla at rediffmail.com (anil kumar rohilla)
Date: 16 Jun 2006 09:13:54 -0000
Subject: [R] Assignemt problem ,,,,,,,,,,,,,,,
Message-ID: <20060616091354.3455.qmail@webmail34.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060616/24f048f8/attachment.pl 

From davidhughjones at gmail.com  Fri Jun 16 11:23:57 2006
From: davidhughjones at gmail.com (David Hugh-Jones)
Date: Fri, 16 Jun 2006 10:23:57 +0100
Subject: [R] inplace assignment
Message-ID: <f5d848060606160223k5c81c70o58019de6db03557@mail.gmail.com>

I get tired of writing, e.g.


data.frame[some.condition & another.condition, big.list.of.columns] <-
paste(data.frame[some.condition & another.condition,
big.list.of.columns], "foobar")


I would a function like:

inplace(paste(data.frame[some.condition & another.condition,
big.list.of.columns], "foobar"))

which would take the first argument of the inner function and assign
the function's result to it.

Has anyone done something like this? Are there simple alternative
solutions that I'm missing?

Cheers
David


From stecalza at tiscali.it  Fri Jun 16 11:32:31 2006
From: stecalza at tiscali.it (Stefano Calza)
Date: Fri, 16 Jun 2006 11:32:31 +0200
Subject: [R] Assignemt problem ,,,,,,,,,,,,,,,
In-Reply-To: <20060616091354.3455.qmail@webmail34.rediffmail.com>
References: <20060616091354.3455.qmail@webmail34.rediffmail.com>
Message-ID: <20060616093231.GA4188@med.unibs.it>

Hi,

your matix has 5 rows and 6 columns. So, either you do 

ind <- 1:5
res[,1] <- ind

or 

dim(res) <- c(6,5)

HIH,
Stefano


On Fri, Jun 16, 2006 at 09:13:54AM -0000, anil kumar rohilla wrote:
<anil> ?
<anil>Hello list,
<anil>     i have a very simple question about matrix assignment.
<anil>
<anil>i did like this.
<anil>res<-1:30
<anil>dim(res)<-c(5,6)
<anil>
<anil>ind<-1:6
<anil>
<anil>now i want to assign the value of this variable ind to first coloumn in matrix res.
<anil>
<anil>like 
<anil>res[,1]<-ind
<anil>
<anil>but this code is giving error , Actualy i have a for loop and value of ind variable is changing every time ,,total 6 times,,i want to assign every vlaue of ind (5 times) to each coloumn of res,,please help me in this regards
<anil>
<anil>anil
<anil>
<anil>
<anil>ANIL KUMAR( METEOROLOGIST)
<anil>LRF SECTION 
<anil>NATIONAL CLIMATE CENTER 
<anil>ADGM(RESEARCH)
<anil>INDIA METEOROLOGICAL DEPARTMENT
<anil>SHIVIJI NAGAR
<anil>PUNE-411005 INDIA
<anil>MOBILE +919422023277
<anil>anilkumar a imdpune.gov.in
<anil>
<anil>	[[alternative HTML version deleted]]
<anil>

<anil>______________________________________________
<anil>R-help a stat.math.ethz.ch mailing list
<anil>https://stat.ethz.ch/mailman/listinfo/r-help
<anil>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From petr.pikal at precheza.cz  Fri Jun 16 12:10:53 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 16 Jun 2006 12:10:53 +0200
Subject: [R] Yahoo data download problem
In-Reply-To: <20060616075222.26546.qmail@web7606.mail.in.yahoo.com>
Message-ID: <44929FCD.26976.F5A06E@localhost>

Hi


On 16 Jun 2006 at 8:52, SUMANTA BASAK wrote:

Date sent:      	Fri, 16 Jun 2006 08:52:22 +0100 (BST)
From:           	SUMANTA BASAK <r_econometrics at yahoo.co.in>
To:             	R HELP <r-help at stat.math.ethz.ch>,
	r-sig-finance-request at stat.math.ethz.ch
Subject:        	[R] Yahoo data download problem

> Hi all R-Experts,
> 
> I'm facing one problem in yahoo data downloading. I'm suing Windows
> XP, R 2.2.0, and i'm using yahoo.get.hist.quote function to download
> data. I need 500 companies of S&P index daily 'closing price' data for
> last ten years. My questions are:
> 
> 1) I have all the ticker names of S&P 500 companies in a .csv format.
> I'm reading those names in R and they are coming as data.frame object.
> How can i change this to a vector?

df<-data.frame(x=sample(letters,10)) 

 as.vector(df$x)
 [1] "g" "b" "p" "u" "r" "q" "j" "h" "o" "k"
>

HTH
Petr


> 
> 2) How can i get all companies data downloaded using a simple "for"
> loop?
> 
> I'm using the following function for a single stock data.
> 
> 
> library(gdata)
> s<-yahoo.get.hist.quote(instrument = "mo", destfile = paste("mo",
> ".csv", sep = ""), start="1996-01-01",
>                      end="2006-06-16", quote = c("Close"), adjusted =
>                      TRUE, download = TRUE, origin = "1970-01-01",
>                      compression = "d")
> 
> 
> Thanks,
> Sumanta Basak.
> 
> 
> ---------------------------------
> 
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From p.dalgaard at biostat.ku.dk  Fri Jun 16 12:15:35 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Jun 2006 12:15:35 +0200
Subject: [R] inplace assignment
In-Reply-To: <f5d848060606160223k5c81c70o58019de6db03557@mail.gmail.com>
References: <f5d848060606160223k5c81c70o58019de6db03557@mail.gmail.com>
Message-ID: <x27j3hl83c.fsf@turmalin.kubism.ku.dk>

"David Hugh-Jones" <davidhughjones at gmail.com> writes:

> I get tired of writing, e.g.
> 
> 
> data.frame[some.condition & another.condition, big.list.of.columns] <-
> paste(data.frame[some.condition & another.condition,
> big.list.of.columns], "foobar")
> 
> 
> I would a function like:
> 
> inplace(paste(data.frame[some.condition & another.condition,
> big.list.of.columns], "foobar"))
> 
> which would take the first argument of the inner function and assign
> the function's result to it.
> 
> Has anyone done something like this? Are there simple alternative
> solutions that I'm missing?

Well, I'd consider

cc <- some.condition & another.condition
l <- big.list.of.columns
mydf[cc,l] <- paste(mydf[cc,l], "foobar)

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ripley at stats.ox.ac.uk  Fri Jun 16 12:16:27 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 16 Jun 2006 11:16:27 +0100 (BST)
Subject: [R] R with tcl/tk 8.5
In-Reply-To: <pan.2006.06.15.17.00.08.505141@troefpunt.nl>
References: <pan.2006.06.15.17.00.08.505141@troefpunt.nl>
Message-ID: <Pine.LNX.4.64.0606161113580.19116@gannet.stats.ox.ac.uk>

When Tcl/Tk 8.5 exists please ask again. (It is currently in *alpha*: do 
you expect to use alpha software with a stable package?)

On Thu, 15 Jun 2006, JeeBee wrote:

> Hi List,
>
> Is it possible to tell R to use tcl/tk 8.5?
> My R package seems to depend on libtcl8.4.so.

It may work if you compile from the sources.

> However, in Windows it seems to be possible to set
> TCL_LIBRARY and MY_TCLTK.

Only to a binary installation of Tcl/Tk 8.4, or at least, not of 8.3.

> Is there something similar possible in Linux?
>
> I have installed:
>
> $ rpm -q R
> R-2.3.1-1.fc5
>
> $ locate libtcl8
> /usr/lib/libtcl8.4.so
> /usr/local/lib/libtcl8.5.so
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ligges at statistik.uni-dortmund.de  Fri Jun 16 12:19:41 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 16 Jun 2006 12:19:41 +0200
Subject: [R] number of iteration s exceeded maximum of 50
In-Reply-To: <200606140735459773486@yahoo.ca>
References: <1bff52c20606140033j3ac8e9fdxa85d4edded2ad94a@mail.gmail.com>	<1bff52c20606140101r25188de1g6573c52f3cb5d152@mail.gmail.com>	<38b9f0350606140223g507112dau3478586790daad3d@mail.gmail.com>
	<200606140735459773486@yahoo.ca>
Message-ID: <449285BD.3020101@statistik.uni-dortmund.de>

Leaf Sun wrote:

> Hi all,
> 
> I found r-site-research not work for me these days.
> 
> When I was doing nls( ) , there was an error "number of iterations exceeded maximum of 50". I set number in nls.control which is supposed to control the number of iterations but it didn't work well. Could anybody with this experience tell me how to fix it? Thanks in advance!

We cannot make suggestions unless you tell us what you tried yourself.
Id possible, please gib?ve a reproducible examle.

Uwe Ligges

> Leaf
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ramasamy at cancer.org.uk  Fri Jun 16 12:31:41 2006
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 16 Jun 2006 10:31:41 +0000
Subject: [R] inplace assignment
In-Reply-To: <f5d848060606160223k5c81c70o58019de6db03557@mail.gmail.com>
References: <f5d848060606160223k5c81c70o58019de6db03557@mail.gmail.com>
Message-ID: <1150453901.3505.17.camel@dhcp-82.wolf.ox.ac.uk>

I do not fully understand your question but how about :

 inplace <- function( df, cond1, cond2, cols, suffix ){
 
  w  <- which( cond1 & cond2 )
  df <- df[ w, cols ]
  paste(df, suffix)
  return(df)
 }


BTW, did you mean "colnames(df) <- paste(colnames(df), suffix)" instead
of "paste(df, suffix)" ?

Regards, Adai



On Fri, 2006-06-16 at 10:23 +0100, David Hugh-Jones wrote:
> I get tired of writing, e.g.
> 
> 
> data.frame[some.condition & another.condition, big.list.of.columns] <-
> paste(data.frame[some.condition & another.condition,
> big.list.of.columns], "foobar")
> 
> 
> I would a function like:
> 
> inplace(paste(data.frame[some.condition & another.condition,
> big.list.of.columns], "foobar"))
> 
> which would take the first argument of the inner function and assign
> the function's result to it.
> 
> Has anyone done something like this? Are there simple alternative
> solutions that I'm missing?
> 
> Cheers
> David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From davidhughjones at gmail.com  Fri Jun 16 13:01:33 2006
From: davidhughjones at gmail.com (David Hugh-Jones)
Date: Fri, 16 Jun 2006 12:01:33 +0100
Subject: [R] inplace assignment
In-Reply-To: <1150453901.3505.17.camel@dhcp-82.wolf.ox.ac.uk>
References: <f5d848060606160223k5c81c70o58019de6db03557@mail.gmail.com>
	<1150453901.3505.17.camel@dhcp-82.wolf.ox.ac.uk>
Message-ID: <f5d848060606160401w77b8ac31va280bc31899bc015@mail.gmail.com>

It's more a general point about having to write things out twice when
you do assignments. I could also have written:

data.frame[some.condition & another.condition, big.list.of.columns] <-
data.frame[some.condition & another.condition,  big.list.of.columns] * 2 + 55

or anything else. Equally, there could be any method of subsetting, or
any expression that can be an assignment target, on the left hand
side:

data.frame[[some.complex.expression.for.columnames]]
<-data.frame[[some.complex.expression.for.columnames]] * 333 + foo *
56

rownames(matrix)[45:53] <- paste(rownames(matrix)[45:53], "blah")


David

On 16/06/06, Adaikalavan Ramasamy <ramasamy at cancer.org.uk> wrote:
> I do not fully understand your question but how about :
>
>  inplace <- function( df, cond1, cond2, cols, suffix ){
>
>   w  <- which( cond1 & cond2 )
>   df <- df[ w, cols ]
>   paste(df, suffix)
>   return(df)
>  }
>
>
> BTW, did you mean "colnames(df) <- paste(colnames(df), suffix)" instead
> of "paste(df, suffix)" ?
>
> Regards, Adai
>
>
>
> On Fri, 2006-06-16 at 10:23 +0100, David Hugh-Jones wrote:
> > I get tired of writing, e.g.
> >
> >
> > data.frame[some.condition & another.condition, big.list.of.columns] <-
> > paste(data.frame[some.condition & another.condition,
> > big.list.of.columns], "foobar")
> >
> >
> > I would a function like:
> >
> > inplace(paste(data.frame[some.condition & another.condition,
> > big.list.of.columns], "foobar"))
> >
> > which would take the first argument of the inner function and assign
> > the function's result to it.
> >
> > Has anyone done something like this? Are there simple alternative
> > solutions that I'm missing?
> >
> > Cheers
> > David
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
>


From r_econometrics at yahoo.co.in  Fri Jun 16 13:10:31 2006
From: r_econometrics at yahoo.co.in (SUMANTA BASAK)
Date: Fri, 16 Jun 2006 12:10:31 +0100 (BST)
Subject: [R] Yahoo data download problem
In-Reply-To: <44929FCD.26976.F5A06E@localhost>
Message-ID: <20060616111031.75276.qmail@web7608.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060616/21a5f117/attachment.pl 

From davidhughjones at gmail.com  Fri Jun 16 13:14:08 2006
From: davidhughjones at gmail.com (David Hugh-Jones)
Date: Fri, 16 Jun 2006 12:14:08 +0100
Subject: [R] Name of a column
In-Reply-To: <f5d848060606160413j29513213m3c5ef8c2c21c6f12@mail.gmail.com>
References: <a725cda30606150940t51136163ke1f2a8a2433c046b@mail.gmail.com>
	<f5d848060606160413j29513213m3c5ef8c2c21c6f12@mail.gmail.com>
Message-ID: <f5d848060606160414t14302b3cr6517654b4497bc11@mail.gmail.com>

1. You don't need to say  truc==T. truc is already a logical vector.

2. colnames are just another vector, so do

colnames(truc)[truc]

Dave

On 15/06/06, David Hajage <dhajage at gmail.com> wrote:
> Hello,
>
> My problem is quite simply, but I didn't find any solution...
>
> I have a vector :
>
> > truc
>    longueur30 longueur40 longueur50 longueur60 longueur70 longueur80
> longueur90
> 34      FALSE      FALSE      FALSE      FALSE       TRUE      FALSE
> FALSE
>
> I would like to have the name of the column where there is "TRUE".
>
> > colnames(truc)
> [1] "longueur30" "longueur40" "longueur50" "longueur60" "longueur70"
> [6] "longueur80" "longueur90"
>
> > truc[truc == T]
> [1] TRUE
>
> > colnames(truc[truc == T])
> NULL
>
> How can I do it ?
>
> Thank you for your help.
>
> --
> David
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From jcmartinez at banxico.org.mx  Fri Jun 16 14:27:18 2006
From: jcmartinez at banxico.org.mx (=?iso-8859-1?Q?Mart=EDnez_Ovando_Juan_Carlos?=)
Date: Fri, 16 Jun 2006 07:27:18 -0500
Subject: [R] A doubt concerning a closed visualization of a ternaryplot
Message-ID: <AC172321027ADC49BB891A7D7D5DF69B5D5277@BMCORREOBE1.banxico.org.mx>

Hi all

Does someone know how can I alter the range of some variables in a 'ternaryplot' of the 'vcd' package, with the aim of make a closed visualization of some of its regions?

Thank you in advance

Sincerely,

	Juan Carlos

"2006, A?o del Bicentenario del natalicio del Benem?rito de las Am?ricas, Don Benito Ju?rez Garc?a".


From vincent at 7d4.com  Fri Jun 16 15:07:07 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Fri, 16 Jun 2006 15:07:07 +0200
Subject: [R] matrix selection return types
In-Reply-To: <644e1f320606150858hf9725adm7c801ba61277c733@mail.gmail.com>
References: <4491637A.4030705@7d4.com>
	<644e1f320606150858hf9725adm7c801ba61277c733@mail.gmail.com>
Message-ID: <4492ACFB.2010607@7d4.com>

jim holtman a ?crit :

>  > z1 <- m[m[,1] == 2,, drop=FALSE]
> What is the problem you are trying to solve?

It was not really a big problem.
Just that without the "drop" argument,
The changing returned type complicated
the next computations.
drop=TRUE makes it homogeneous, which is what I needed.

Thanks for your answer.


From Charles.Annis at StatisticalEngineering.com  Fri Jun 16 15:23:02 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Fri, 16 Jun 2006 09:23:02 -0400
Subject: [R] Yahoo data download problem
In-Reply-To: <20060616111031.75276.qmail@web7608.mail.in.yahoo.com>
Message-ID: <019101c69147$fe874020$6600a8c0@DD4XFW31>

Why create an enormous matrix?  Why not read each company's info and
immediately write it to the file using write.csv( ... append = TRUE ...)?
                                                      ^^^^^^^^^^^^^

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of SUMANTA BASAK
Sent: Friday, June 16, 2006 7:11 AM
To: Petr Pikal; R HELP
Subject: Re: [R] Yahoo data download problem

Hi Petr,

Thanks for the solution. But my problem is still there. I have 500 company
names in a single column in an excel sheet of S&P 500 index. I need to call
all those compnies in a 'for' loop and write that whole dataset into a text
file. I've developed the following one, but the problem is getting all those
company names in a vector.

h<-c("GE","MMM")
s<-matrix(0,3818,2)
for  (i in 1:2)
{
s[,i]<-yahoo.get.hist.quote(instrument = h[i], destfile = paste(h[i],
".csv", sep = ""), start="1996-01-01",
                     end="2006-06-16", quote = c("Close"), adjusted = TRUE,
download = TRUE, origin = "1970-01-01",
                     compression = "d")
write.csv(s,file="Z:/yahoo.out.csv")
}

Here i have taken only two compnies. But i want to fetch all the company
names and put them in a vector so that i can call them in a 'for' loop.
Please guide.

Thanks,
Sumanta Basak.

Petr Pikal <petr.pikal at precheza.cz> wrote: Hi


On 16 Jun 2006 at 8:52, SUMANTA BASAK wrote:

Date sent:       Fri, 16 Jun 2006 08:52:22 +0100 (BST)
From:            SUMANTA BASAK 
To:              R HELP ,
 r-sig-finance-request at stat.math.ethz.ch
Subject:         [R] Yahoo data download problem

> Hi all R-Experts,
> 
> I'm facing one problem in yahoo data downloading. I'm suing Windows
> XP, R 2.2.0, and i'm using yahoo.get.hist.quote function to download
> data. I need 500 companies of S&P index daily 'closing price' data for
> last ten years. My questions are:
> 
> 1) I have all the ticker names of S&P 500 companies in a .csv format.
> I'm reading those names in R and they are coming as data.frame object.
> How can i change this to a vector?

df<-data.frame(x=sample(letters,10)) 

 as.vector(df$x)
 [1] "g" "b" "p" "u" "r" "q" "j" "h" "o" "k"
>

HTH
Petr


> 
> 2) How can i get all companies data downloaded using a simple "for"
> loop?
> 
> I'm using the following function for a single stock data.
> 
> 
> library(gdata)
> s<-yahoo.get.hist.quote(instrument = "mo", destfile = paste("mo",
> ".csv", sep = ""), start="1996-01-01",
>                      end="2006-06-16", quote = c("Close"), adjusted =
>                      TRUE, download = TRUE, origin = "1970-01-01",
>                      compression = "d")
> 
> 
> Thanks,
> Sumanta Basak.
> 
> 
> ---------------------------------
> 
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



 				
---------------------------------


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From jcmartinez at banxico.org.mx  Fri Jun 16 15:30:18 2006
From: jcmartinez at banxico.org.mx (=?iso-8859-1?Q?Mart=EDnez_Ovando_Juan_Carlos?=)
Date: Fri, 16 Jun 2006 08:30:18 -0500
Subject: [R]   doubt concerning a closed visualization of a ternaryplot
Message-ID: <AC172321027ADC49BB891A7D7D5DF69B5F4667@BMCORREOBE1.banxico.org.mx>

Hi all

Does someone know how can I alter the range of some variables in a 'ternaryplot' of the 'vcd' package, with the aim of make a closed visualization of some of its regions?

Thank you in advance

Sincerely,

	Juan Carlos

	"2006, A?o del Bicentenario del natalicio del Benem?rito de las Am?ricas, Don Benito Ju?rez Garc?a".


From dennis.mcleod at uchospitals.edu  Fri Jun 16 15:39:48 2006
From: dennis.mcleod at uchospitals.edu (dennis.mcleod at uchospitals.edu)
Date: Fri, 16 Jun 2006 08:39:48 -0500
Subject: [R] Install R 2.3.1 on SUSE Linux
Message-ID: <9AF5B542C1E2874DAB4E85EB850D69634DF351@UCHMBX01-DRN03S.UCHAD.uchospitals.edu>

Detlef:

Thank You !

Learning Linux is a challenge, sometimes frustrating; but I believe it
is well worth it. 

Thanks again for your help
Dennis

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Detlef Steuer
Sent: Friday, June 16, 2006 2:33 AM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] Install R 2.3.1 on SUSE Linux

Hi,

you have to add an online repository for 10.0 to your installation
sources.

This is covered in the file called
http://cran.r-project.org/bin/linux/suse/ReadMe.html which was added to
be read. ;-)

Detlef

On Thu, 15 Jun 2006 10:05:19 -0500
<dennis.mcleod at uchospitals.edu> wrote:

> Hello:
> 
> I am new to Linux, and I am trying to install R 2.3.1 on SUSE Linux 
> 10.0. The RPM installer, YAST, states that I need libgfortran.so.0.
> I have loaded Intel FORTRAN and GCC FORTRAN; but I still do not have 
> the libgfortran.so.0 module YAST is asking for.
> 
> Is there somewhere I can get this module for SUSE Linux 10.0. There is

> a post on the R-Help for this same problem; but I cannot seem to find 
> the resolution.
> 
> I would appreciate any help you could provide.
> 
> Thanks
> Dennis
> 
> 
> 
> 
> **********************************************************************
> ********** This e-mail is intended only for the use of the individual 
> or entity to which it is addressed and may contain information that is

> privileged and confidential.
> If the reader of this e-mail message is not the intended recipient, 
> you are hereby notified that any dissemination, distribution or 
> copying of this communication is prohibited. If you have received this

> e-mail in error, please notify the sender and destroy all copies of
the transmittal.
> 
> Thank you
> University of Chicago Hospitals
>
************************************************************************
********
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

********************************************************************************
This e-mail is intended only for the use of the individual or entity to which
it is addressed and may contain information that is privileged and confidential.
If the reader of this e-mail message is not the intended recipient, you are 
hereby notified that any dissemination, distribution or copying of this
communication is prohibited. If you have received this e-mail in error, please 
notify the sender and destroy all copies of the transmittal. 

Thank you
University of Chicago Hospitals


From neuro3000 at hotmail.com  Fri Jun 16 16:03:18 2006
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Fri, 16 Jun 2006 10:03:18 -0400
Subject: [R] Yahoo data download problem
In-Reply-To: <20060616111031.75276.qmail@web7608.mail.in.yahoo.com>
Message-ID: <BAY112-F1319E013D9A533B9D8D192AF830@phx.gbl>

Hi, this is what you asked for.  You should consider transforming your ts 
data into something else as you get no column name or date in the basic 
form.

#Initialize empty table
closing <-NULL

#Downalod consituents since I don't have it on my comp
download.file("http://www2.standardandpoors.com/spf/csv/index/sp500.csv",
"c:/constituents.csv", "internal", quiet = FALSE, mode = "wb",cacheOK = 
TRUE)
constituents <-read.csv("c:/constituents.csv",header=TRUE,skip=2,as.is=TRUE)


for (i in constituents$Ticker) {
s<-yahoo.get.hist.quote(instrument = sub("\\.","\\-",i),
destfile = paste(i, ".csv", sep = ""), start="1996-01-01",
end="2006-06-16", quote = c("Close"), adjusted =
TRUE, download = TRUE, origin = "1970-01-01", compression = "d")
closing  <-cbind.ts(closing ,s)
}

write.csv(closing,"c:/yahoooutput.csv")

Neuro

>From: SUMANTA BASAK <r_econometrics at yahoo.co.in>
>To: Petr Pikal <petr.pikal at precheza.cz>, R HELP <r-help at stat.math.ethz.ch>
>Subject: Re: [R] Yahoo data download problem
>Date: Fri, 16 Jun 2006 12:10:31 +0100 (BST)
>
>Hi Petr,
>
>Thanks for the solution. But my problem is still there. I have 500 company 
>names in a single column in an excel sheet of S&P 500 index. I need to call 
>all those compnies in a 'for' loop and write that whole dataset into a text 
>file. I've developed the following one, but the problem is getting all 
>those company names in a vector.
>
>h<-c("GE","MMM")
>s<-matrix(0,3818,2)
>for  (i in 1:2)
>{
>s[,i]<-yahoo.get.hist.quote(instrument = h[i], destfile = paste(h[i], 
>".csv", sep = ""), start="1996-01-01",
>                      end="2006-06-16", quote = c("Close"), adjusted = 
>TRUE, download = TRUE, origin = "1970-01-01",
>                      compression = "d")
>write.csv(s,file="Z:/yahoo.out.csv")
>}
>
>Here i have taken only two compnies. But i want to fetch all the company 
>names and put them in a vector so that i can call them in a 'for' loop. 
>Please guide.
>
>Thanks,
>Sumanta Basak.
>
>Petr Pikal <petr.pikal at precheza.cz> wrote: Hi
>
>
>On 16 Jun 2006 at 8:52, SUMANTA BASAK wrote:
>
>Date sent:       Fri, 16 Jun 2006 08:52:22 +0100 (BST)
>From:            SUMANTA BASAK
>To:              R HELP ,
>  r-sig-finance-request at stat.math.ethz.ch
>Subject:         [R] Yahoo data download problem
>
> > Hi all R-Experts,
> >
> > I'm facing one problem in yahoo data downloading. I'm suing Windows
> > XP, R 2.2.0, and i'm using yahoo.get.hist.quote function to download
> > data. I need 500 companies of S&P index daily 'closing price' data for
> > last ten years. My questions are:
> >
> > 1) I have all the ticker names of S&P 500 companies in a .csv format.
> > I'm reading those names in R and they are coming as data.frame object.
> > How can i change this to a vector?
>
>df<-data.frame(x=sample(letters,10))
>
>  as.vector(df$x)
>  [1] "g" "b" "p" "u" "r" "q" "j" "h" "o" "k"
> >
>
>HTH
>Petr
>
>
> >
> > 2) How can i get all companies data downloaded using a simple "for"
> > loop?
> >
> > I'm using the following function for a single stock data.
> >
> >
> > library(gdata)
> > s<-yahoo.get.hist.quote(instrument = "mo", destfile = paste("mo",
> > ".csv", sep = ""), start="1996-01-01",
> >                      end="2006-06-16", quote = c("Close"), adjusted =
> >                      TRUE, download = TRUE, origin = "1970-01-01",
> >                      compression = "d")
> >
> >
> > Thanks,
> > Sumanta Basak.
> >
> >
> > ---------------------------------
> >
> >
> >  [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
>
>Petr Pikal
>petr.pikal at precheza.cz
>
>
>
>
>---------------------------------
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html


From mikewolfgang at gmail.com  Fri Jun 16 16:31:17 2006
From: mikewolfgang at gmail.com (Mike Wolfgang)
Date: Fri, 16 Jun 2006 10:31:17 -0400
Subject: [R] how to setup parallel environment?
Message-ID: <e668df8c0606160731y4dc65fbak8c103d569f219aaa@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060616/5db5897d/attachment.pl 

From jsandblom at gmail.com  Fri Jun 16 16:50:53 2006
From: jsandblom at gmail.com (Johan Sandblom)
Date: Fri, 16 Jun 2006 16:50:53 +0200
Subject: [R] R in ConTeXt
Message-ID: <97a06f070606160750t7b249156xba478211c1f53db5@mail.gmail.com>

In case there are users of the TeX macro package ConTeXt on this list,
they may be interested to know that it is now possible to include R
code in a ConTeXt document and have the code evaluated while
compiling. The inverse of Sweave, as it were. The advantage is that
there is then only one file to keep track of (.tex vs .rnw and .tex),
while a drawback is speed, since each code snippet is evaluated in its
own R session with associated startup time. Below is a small example
of usage

\usemodule[r]

\starttext

\title{Example usage of R module}

\startRhidden
rm(list=ls())
x <- rnorm(100)
y <- runif(100)
\stopRhidden

\type{x} and \type{y} are randomly generated.

\startR
summary(lm(y~x))
pdf("xy.pdf")
plot(y~x)
dev.off()
\stopR

\placefigure{Y vs X}{\externalfigure[xy][width=.4\textwidth]}

\stoptext

Regards, Johan

-- 
Johan Sandblom  N8, MRC, Karolinska sjh
t +46851776108  17176 Stockholm
m +46735521477  Sweden
"What is wanted is not the will to believe, but the
will to find out, which is the exact opposite"
- Bertrand Russell


From arun.kumar.saha at gmail.com  Fri Jun 16 16:54:21 2006
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Fri, 16 Jun 2006 20:24:21 +0530
Subject: [R] Garch warning
Message-ID: <d4c57560606160754g10d19acbw8275c76fdf246c23@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060616/abe0d38b/attachment.pl 

From spencer.graves at pdf.com  Fri Jun 16 17:07:34 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 16 Jun 2006 08:07:34 -0700
Subject: [R] matrix log
In-Reply-To: <OFAFE8AA11.2066EFAB-ON8025718D.002D6985-8025718D.002E3B72@gsk.com>
References: <OFAFE8AA11.2066EFAB-ON8025718D.002D6985-8025718D.002E3B72@gsk.com>
Message-ID: <4492C936.3070909@pdf.com>

	  I'm not aware of any R implementation of a logarithm of a matrix. 
I've seen discussions on this list of the difficulties involved in 
computing functions of matrices, but I know of nothing.  (The 'nlme' 
package has 'pdLogChol', which may be related, but I think it's 
different.)

	  I wish you luck, and I hope you will find the time to package it for 
the rest of us.

	  Best Wishes,
	  Spencer Graves	

Brandon.J.Whitcher at gsk.com wrote:
> Dear R users,
> 
> Has anyone implemented a "matrix log" function in R similar to the 
> function logm() in Matlab?  I did a quick R site search and browsed the 
> contributed packages to no avail.
> 
> The octave function is far too simplistic and fails for the Matlab test 
> matrix.  Ideally, the code of Cheng, Higham, and Laub (2001) or something 
> similar could be utilized.  Just checking before I spend time porting 
> code.
> 
> cheers...
> 
> Brandon
> 
> Translational Medicine & Genetics
> GlaxoSmithKline
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From f.calboli at imperial.ac.uk  Fri Jun 16 17:07:29 2006
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Fri, 16 Jun 2006 16:07:29 +0100
Subject: [R] diagonal matrix: rows rearrangement
Message-ID: <4492C931.50804@imperial.ac.uk>

Hi All,

I have a non-square matrix of 0s/1s. The rows are individuals, and the colums 
factors. The code is 1 if a factor is present, 0 if not.

I would like to rearrange the order of the rows to find if there are any 
diagonal blocks. Is there a fucntion that would do that already in R?

Cheers,

Federico

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com


From v.tron at ed.ac.uk  Fri Jun 16 17:46:56 2006
From: v.tron at ed.ac.uk (Viktor Tron)
Date: Fri, 16 Jun 2006 16:46:56 +0100
Subject: [R] interfacing ocaml with R
Message-ID: <op.ta8wsip6z5aw2e@pc2a1vtron.inf.ed.ac.uk>

Dear R-ers,

Does anyone know if one can interface R from other languages, notably  
OCaml I'd be
interested in?
Omegahat gives python and perl for which there are bidirectional interfaces
but I wonder if there are general robust ways for other languages.

I can open R as a pipe process and can successfully commmunicate from  
within OCaml
but I just wonder if there are standard ways of transferring data across.
Writing and reading from files seems obvious, simple and robust but
may not be the most efficient.

Thanks for any suggestions
V


From Francois.Bastardie at ifremer.fr  Fri Jun 16 17:50:18 2006
From: Francois.Bastardie at ifremer.fr (Francois.Bastardie at ifremer.fr)
Date: Fri, 16 Jun 2006 17:50:18 +0200
Subject: [R] a method to trim composite objects?
Message-ID: <4492D33A.8010605@ifremer.fr>

dear R users,

please, consider the following code you may run:

setClass("my.class1",

      representation(
          my.list        = "list",
          my.array1      = "array"),
      prototype=prototype(
          my.list        =list(),
          my.array1      =array(0,c(3,3,3))
                          ))



#------------------

setClass("my.class2",
      representation(
          my.array2        = "array",
          my.array3        = "array"),
      prototype=prototype(
          my.array2        =array(0,c(3,3,3)),
          my.array3        =array(0,c(3,3,3))
                           ))



#------------------

x <- new("my.class1")
x at my.list[[1]] <- new("my.class2")
x at my.list[[2]] <- new("my.class2")


Is there any method in R to trim dimensions of all the component arrays 
(my.array1, my.array2, my.array3, etc.) in one time inside all the 
hierarchical level of the composite object I named 'x'?

in other words, if I want to keep only the 2nd dimension, I have to do:
x at my.array1[,2,]
x at my.list[[1]]@my.array2[,2,]
x at my.list[[1]]@my.array3[,2,]
x at my.list[[2]]@my.array2[,2,]
x at my.list[[2]]@my.array3[,2,]
etc.

thank you


-- 
--------------------------------------------------------
Fran?ois Bastardie

French Research Institute for the Development of the Sea (IFREMER)
Ecologie et Mod?les pour l'Halieutique
Rue de l'Ile d'Yeu
BP 21105
44311 NANTES Cedex 03 - France
T?l : 02 40 37 41 64
Fax : 02 40 37 40 75
E-mail : francois.bastardie at ifremer.fr


From leaflovesun at yahoo.ca  Fri Jun 16 18:41:09 2006
From: leaflovesun at yahoo.ca (Leaf Sun)
Date: Fri, 16 Jun 2006 10:41:09 -0600
Subject: [R] number of iteration s exceeded maximum of 50
References: <1bff52c20606140033j3ac8e9fdxa85d4edded2ad94a@mail.gmail.com>	<1bff52c20606140101r25188de1g6573c52f3cb5d152@mail.gmail.com>	<38b9f0350606140223g507112dau3478586790daad3d@mail.gmail.com>
	<200606140735459773486@yahoo.ca>
	<449285BD.3020101@statistik.uni-dortmund.de>
Message-ID: <200606161041072848729@yahoo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060616/d7a6d808/attachment.pl 

From gunter.berton at gene.com  Fri Jun 16 19:20:58 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 16 Jun 2006 10:20:58 -0700
Subject: [R] number of iteration s exceeded maximum of 50
In-Reply-To: <200606161041072848729@yahoo.ca>
Message-ID: <003701c69169$3c128c80$f8c2fea9@gne.windows.gene.com>

My goodness! This is **NOT** a reproducible example. You need to give us the
exact data you fitted to reproduce your results/diagnose your problem.


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Leaf Sun
> Sent: Friday, June 16, 2006 9:41 AM
> To: Uwe Ligges
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] number of iteration s exceeded maximum of 50
> 
> Sorry, I thought it was a straightforward question inside 
> which I was stuck .
> 
> I used nls( ) to estimate a and b in this function. 
> 
> nls(y~ a*x^b,start=list(a=a1,b=b1) 
> 
> seems the start list I gave was not able to reach convergence 
> and it gave notes: number of iteration s exceeded maximum of 
> 50. Then I put  nls.control(maxiter = 50, tol = 1e-05, 
> minFactor = 1/1024) in nls(.. ), and modified the argument of 
> maxiter = 500. But it worked out as the same way and noted : 
> number of iteration s exceeded maximum of 50. I have totally 
> no idea how to set this parameter MAXITER.
> 
> Thanks for any information!
> 
> Leaf
> 
> 
> >  Hi  all,
> >  
> >  I  found  r-site-research  not  work  for  me  these  days.
> >  
> >  When  I  was  doing  nls(  )  ,  there  was  an  error  
> "number  of  iterations  exceeded  maximum  of  50".  I  set  
> number  in  nls.control  which  is  supposed  to  control  
> the  number  of  iterations  but  it  didn't  work  well.  
> Could  anybody  with  this  experience  tell  me  how  to  
> fix  it?  Thanks  in  advance!
> 
> We  cannot  make  suggestions  unless  you  tell  us  what  
> you  tried  yourself.
> Id  possible,  please  gib4ve  a  reproducible  examle.
> 
> Uwe  Ligges
> 
> >  Leaf
> >  
> >   [[alternative  HTML  version  deleted]]
> >  
> >  ______________________________________________
> >  R-help at stat.math.ethz.ch  mailing  list
> >  https://stat.ethz.ch/mailman/listinfo/r-help
> >  PLEASE  do  read  the  posting  guide!  
> http://www.R-project.org/posting-guide.html
> 
> 	[[alternative HTML version deleted]]
> 
>


From CodyH at BaylorHealth.edu  Fri Jun 16 19:31:38 2006
From: CodyH at BaylorHealth.edu (Hamilton, Cody)
Date: Fri, 16 Jun 2006 12:31:38 -0500
Subject: [R] modeling logit(y/n) using lrm
Message-ID: <E52E20F6B4A2F548B16BB259DD5168CF02FB6793@BHDAEXCH11.bhcs.pvt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060616/6836eddb/attachment.pl 

From kevin.thorpe at utoronto.ca  Fri Jun 16 19:39:15 2006
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Fri, 16 Jun 2006 13:39:15 -0400
Subject: [R] modeling logit(y/n) using lrm
In-Reply-To: <E52E20F6B4A2F548B16BB259DD5168CF02FB6793@BHDAEXCH11.bhcs.pvt>
References: <E52E20F6B4A2F548B16BB259DD5168CF02FB6793@BHDAEXCH11.bhcs.pvt>
Message-ID: <4492ECC3.9090005@utoronto.ca>

Hamilton, Cody wrote:
> I have a dataset at a hospital level (as opposed to the patient level)
> that contains number of patients experiencing events (call this number
> y), and the number of patients eligible for such events (call this
> number n).  I am trying to model logit(y/n) = XBeta.  In SAS this can be
> done in PROC LOGISTIC or GENMOD with a model statement such as: model
> y/n = <predictors>;.  Can this be done using lrm from the Hmisc library
> without restructuring the dataset so that for each hospital there is one
> row with y = 1 and one row with y = 0 and then using the weight option
> in lrm to weight these two responses by the number of 'successes' and
> 'failures' for that hospital, respectively?  I would like to avoid the
> restructuring, and I understand that the use of the weight function is
> not compatible with a lot of the validation functions available in Hmisc
> (validate, bootcov, etc.).

I don't know about lrm, but for glm you can do
glm(cbind(y,m)~ ...) where y is number of successes and
m is the number of failures.

So, you might try that.

> Cody Hamilton, Ph.D
> 
> Institute for Health Care Research and Improvement
> 
> Baylor Health Care System
> 
> (214) 265-3618
> 


-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Department of Public Health Sciences
Faculty of Medicine, University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.946.3297


From rolf at erdos.math.unb.ca  Fri Jun 16 19:41:07 2006
From: rolf at erdos.math.unb.ca (Rolf Turner)
Date: Fri, 16 Jun 2006 14:41:07 -0300 (ADT)
Subject: [R] modeling logit(y/n) using lrm
Message-ID: <200606161741.k5GHf7RV012929@erdos.math.unb.ca>


Cody Hamilton, Ph.D, wrote:

> I have a dataset at a hospital level (as opposed to the patient
> level) that contains number of patients experiencing events (call
> this number y), and the number of patients eligible for such events
> (call this number n).  I am trying to model logit(y/n) = XBeta.  In
> SAS this can be done in PROC LOGISTIC or GENMOD with a model
> statement such as: model y/n = <predictors>;.  Can this be done using
> lrm from the Hmisc library without restructuring the dataset so that
> for each hospital there is one row with y = 1 and one row with y = 0
> and then using the weight option in lrm to weight these two responses
> by the number of 'successes' and 'failures' for that hospital,
> respectively?  I would like to avoid the restructuring, and I
> understand that the use of the weight function is not compatible with
> a lot of the validation functions available in Hmisc (validate,
> bootcov, etc.).

	Why do you need lrm()?  Is there something I'm missing?

	As far as I can tell you can simply do

	glm(cbind(y,n-y) ~ <predictors>,family=binomial,data=<data>)

	where ``<data>'' has columns named ``y'' ``n'' and whatever
	the predictors are called.

				cheers,

					Rolf Turner
					rolf at math.unb.ca


From CodyH at BaylorHealth.edu  Fri Jun 16 20:05:29 2006
From: CodyH at BaylorHealth.edu (Hamilton, Cody)
Date: Fri, 16 Jun 2006 13:05:29 -0500
Subject: [R] modeling logit(y/n) using lrm
Message-ID: <E52E20F6B4A2F548B16BB259DD5168CF02FB6794@BHDAEXCH11.bhcs.pvt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060616/2f5ba17e/attachment.pl 

From br44114 at gmail.com  Fri Jun 16 20:39:00 2006
From: br44114 at gmail.com (bogdan romocea)
Date: Fri, 16 Jun 2006 14:39:00 -0400
Subject: [R] modeling logit(y/n) using lrm
Message-ID: <8d5a36350606161139q6fb788d6geb0164a71083b967@mail.gmail.com>

Not sure about your data set, but if you have some kind of
(weighted/stratified) sample of hospitals you need to pay special
attention. Survey data violates the assumptions of the classical
linear models (infinite population, identically distributed errors
etc) and needs to be analyzed differently. In SAS, it's wrong to throw
such data into a PROC LOGISTIC / REG; PROC SURVEYLOGISTIC / SURVEYREG
should be used instead. In R, take a look at the survey package. For
details check
http://www2.sas.com/proceedings/sugi31/193-31.pdf



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Hamilton, Cody
> Sent: Friday, June 16, 2006 1:32 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] modeling logit(y/n) using lrm
>
>
> I have a dataset at a hospital level (as opposed to the patient level)
> that contains number of patients experiencing events (call this number
> y), and the number of patients eligible for such events (call this
> number n).  I am trying to model logit(y/n) = XBeta.  In SAS
> this can be
> done in PROC LOGISTIC or GENMOD with a model statement such as: model
> y/n = <predictors>;.  Can this be done using lrm from the
> Hmisc library
> without restructuring the dataset so that for each hospital
> there is one
> row with y = 1 and one row with y = 0 and then using the weight option
> in lrm to weight these two responses by the number of 'successes' and
> 'failures' for that hospital, respectively?  I would like to avoid the
> restructuring, and I understand that the use of the weight function is
> not compatible with a lot of the validation functions
> available in Hmisc
> (validate, bootcov, etc.).
>
>
>
> Cody Hamilton, Ph.D
>
> Institute for Health Care Research and Improvement
>
> Baylor Health Care System
>
> (214) 265-3618
>
>
>
>
>
> This e-mail, facsimile, or letter and any files or
> attachments transmitted with it contains information that is
> confidential and privileged. This information is intended
> only for the use of the individual(s) and entity(ies) to whom
> it is addressed. If you are the intended recipient, further
> disclosures are prohibited without proper authorization. If
> you are not the intended recipient, any disclosure, copying,
> printing, or use of this information is strictly prohibited
> and possibly a violation of federal or state law and
> regulations. If you have received this information in error,
> please notify Baylor Health Care System immediately at
> 1-866-402-1661 or via e-mail at privacy at baylorhealth.edu.
> Baylor Health Care System, its subsidiaries, and affiliates
> hereby claim all applicable privileges related to this information.
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


From gavin.simpson at ucl.ac.uk  Fri Jun 16 20:42:14 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 16 Jun 2006 19:42:14 +0100
Subject: [R] problem with legend on other graphics devices
Message-ID: <1150483334.23506.67.camel@gsimpson.geog.ucl.ac.uk>

Hi,

I have a bit of a problem with a legend in png, eps and pdf plots
produced from a custom plotting function. I was writing a little
function to produce some stock plots for a routine analysis conducted in
our lab. I have a wrapper function figures() (see below) that produces
eps, png and pdf versions of the lab output. When I draw the plot on the
X11() device the correct amount of space is made available for the
legend, but when I draw the plot directly on the png, postscript or pdf
devices, the legend is placed too close to the edge of the plot and is
clipped.

This example uses the actual test data I'm using to write the functions.
figures() calls the "dating" plot method I wrote to do the plotting, and
dating() is a wrapper for read.csv() that creates a "dating" object with
the lab analysis results and some extra meta data. All the R code is in
dating.R at the URL below.

felb5 <- dating("http://ecrc3.geog.ucl.ac.uk/download/dating/felb5.csv",
                cs = 1963, csError = 5,
                csDepth = 22.5, csDepthError = 2)
source("http://ecrc3.geog.ucl.ac.uk/download/dating/dating.R")
plot(felb5, as.Age = TRUE)
figures(felb5, "felbrig5", as.Age = TRUE)

As you can see, in the generated png, eps and pdf files the legend is
clipped, but it displays fine on the screen. From the list archives, I
seem to be doing things in the recommended way (not using dev.copy()
etc.)

What can I do to force sufficient space for the legend in the pdf,
postscript and png plots?

Many thanks,

Gav


figures <- function(object, filename, eps = TRUE, png = TRUE, pdf = TRUE,
                    inch.width = 6, inch.height = 6, horizontal = TRUE,
                    pixel.width = 600, pixel.height = 600,
                    paper = "special", onefile = FALSE, pointsize = 12, ...)
  {
    if(eps) {
      postscript(paste(filename, ".eps"), width = inch.width,
                 height = inch.height, onefile = onefile, paper = paper,
                 pointsize = pointsize, horizontal = horizontal)
      plot(object, ...)
      invisible(dev.off())
    }
    
    if(png) {
      png(paste(filename, ".png"), width = pixel.width, height = pixel.height,
          pointsize = pointsize)
      plot(object, ...)
      invisible(dev.off())
    }
    
    if(pdf) {
      pdf(paste(filename, ".pdf"), width = inch.width,
          height = inch.height, onefile = onefile, paper = paper,
          pointsize = pointsize)
      plot(object, ...)
      invisible(dev.off())
    }
  }

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
*  Note new Address, Telephone & Fax numbers from 6th April 2006  *
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson
ECRC & ENSIS                  [t] +44 (0)20 7679 0522
UCL Department of Geography   [f] +44 (0)20 7679 0565
Pearson Building              [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street                  [w] http://www.ucl.ac.uk/~ucfagls/cv/
London, UK.                   [w] http://www.ucl.ac.uk/~ucfagls/
WC1E 6BT.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From CodyH at BaylorHealth.edu  Fri Jun 16 20:49:45 2006
From: CodyH at BaylorHealth.edu (Hamilton, Cody)
Date: Fri, 16 Jun 2006 13:49:45 -0500
Subject: [R] modeling logit(y/n) using lrm
Message-ID: <E52E20F6B4A2F548B16BB259DD5168CF02FB6796@BHDAEXCH11.bhcs.pvt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060616/ba2f0b02/attachment.pl 

From kerryrekky at yahoo.com  Fri Jun 16 21:16:19 2006
From: kerryrekky at yahoo.com (Cunningham Kerry)
Date: Fri, 16 Jun 2006 12:16:19 -0700 (PDT)
Subject: [R] any function for monotone nonparametric regression?
Message-ID: <20060616191619.89372.qmail@web51804.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060616/baabc79b/attachment.pl 

From gunter.berton at gene.com  Fri Jun 16 22:05:35 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 16 Jun 2006 13:05:35 -0700
Subject: [R] any function for monotone nonparametric regression?
In-Reply-To: <20060616191619.89372.qmail@web51804.mail.yahoo.com>
Message-ID: <004701c69180$3b23ef50$f8c2fea9@gne.windows.gene.com>

Why wonder? Why not use R's search facilities to find out for yourself?

RSiteSearch('monotonic regression')

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Cunningham Kerry
> Sent: Friday, June 16, 2006 12:16 PM
> To: R-help at stat.math.ethz.ch
> Subject: [R] any function for monotone nonparametric regression?
> 
> I am wondering if there is any package in R that can fit a 
> nonparametric regression model with monotone constraints on 
> the fitted results.
> 
>  		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From JBDavis at txfb-ins.com  Fri Jun 16 22:46:07 2006
From: JBDavis at txfb-ins.com (Davis, Jacob B. )
Date: Fri, 16 Jun 2006 15:46:07 -0500
Subject: [R] Vector Manipulation
Message-ID: <CC7973C6B927754794A205FEE463F7810A9329@TXS9312221.txfb-ins.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060616/04174cd5/attachment.pl 

From HDoran at air.org  Fri Jun 16 22:48:07 2006
From: HDoran at air.org (Doran, Harold)
Date: Fri, 16 Jun 2006 16:48:07 -0400
Subject: [R] Vector Manipulation
Message-ID: <2323A6D37908A847A7C32F1E3662C80E0B5C1F@dc1ex01.air.org>

I think you want

ifelse(x=='N',0,1) 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Davis, 
> Jacob B. 
> Sent: Friday, June 16, 2006 4:46 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Vector Manipulation
> 
> I have a vector that has 1,974 elements and each element is 
> one of the following (B, F, N, Y).  How do I recreate that 
> vector accept in the place of N put 0 and in the place of B, 
> F or Y put a 1?
> 
>  
> 
> Thanks,
> 
> Jacob
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From Dimitris.Rizopoulos at med.kuleuven.be  Fri Jun 16 22:57:48 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Fri, 16 Jun 2006 22:57:48 +0200
Subject: [R] Vector Manipulation
In-Reply-To: <CC7973C6B927754794A205FEE463F7810A9329@TXS9312221.txfb-ins.com>
References: <CC7973C6B927754794A205FEE463F7810A9329@TXS9312221.txfb-ins.com>
Message-ID: <1150491468.44931b4c11d98@webmail1.kuleuven.be>

try the following:

x <- c("B", "F", "N", "Y")
################
y <- numeric(length(x))
y[x != "N"] <- 1
y


I hope it helps.

Best,
Dimitris

---- 
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting "Davis, Jacob B. " <JBDavis at txfb-ins.com>:

> I have a vector that has 1,974 elements and each element is one of
> the
> following (B, F, N, Y).  How do I recreate that vector accept in the
> place of N put 0 and in the place of B, F or Y put a 1?
> 
>  
> 
> Thanks,
> 
> Jacob
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From fdoespin at gmail.com  Fri Jun 16 23:16:23 2006
From: fdoespin at gmail.com (fernando espindola)
Date: Fri, 16 Jun 2006 17:16:23 -0400
Subject: [R] Object polygon data frame
Message-ID: <c197abc70606161416r3aa91d9bqb0ee6d99564482bb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060616/0174074e/attachment.pl 

From markleeds at verizon.net  Fri Jun 16 23:59:26 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Fri, 16 Jun 2006 16:59:26 -0500 (CDT)
Subject: [R] scatterplot but a little tricky
Message-ID: <6512181.129501150495166820.JavaMail.root@vms069.mailsrvcs.net>

i have two vectors of numbers x and y and of course i
can do the standard scatterplot plot(x,y)
and it looks fine. But, I was hoping there was a way to
do the scatterplot so that each point plotted is a number
where the number represents the index in the dataset.

so, if it was x[3] and y[3], then the point would be a 3
or if it was x[4] and y[4], then the point would be a 4 etc.

i doubt this is possible but i figured i would ask anyway.
thanks.

                                         mark


From sarah.goslee at gmail.com  Sat Jun 17 00:09:51 2006
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Fri, 16 Jun 2006 18:09:51 -0400
Subject: [R] scatterplot but a little tricky
In-Reply-To: <6512181.129501150495166820.JavaMail.root@vms069.mailsrvcs.net>
References: <6512181.129501150495166820.JavaMail.root@vms069.mailsrvcs.net>
Message-ID: <efb536d50606161509w616452a9y178b0325421c4bd1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060616/07578cd2/attachment.pl 

From jeff.hammerbacher at gmail.com  Sat Jun 17 00:12:02 2006
From: jeff.hammerbacher at gmail.com (Jeff Hammerbacher)
Date: Fri, 16 Jun 2006 15:12:02 -0700
Subject: [R] scatterplot but a little tricky
In-Reply-To: <6512181.129501150495166820.JavaMail.root@vms069.mailsrvcs.net>
References: <6512181.129501150495166820.JavaMail.root@vms069.mailsrvcs.net>
Message-ID: <aa24041b0606161512o44c1dc0ma5641a2025b28fe5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060616/c004f081/attachment.pl 

From jc.martinez.ovando at gmail.com  Sat Jun 17 00:43:42 2006
From: jc.martinez.ovando at gmail.com (=?ISO-8859-1?Q?Juan_Carlos_Mart=EDnez_Ovando?=)
Date: Fri, 16 Jun 2006 17:43:42 -0500
Subject: [R]  doubt concerning a closed visualization of a ternaryplot
Message-ID: <90d260590606161543w50a9ac16k33a811933704c24c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060616/a9b50aba/attachment.pl 

From bruno.giordano at music.mcgill.ca  Sat Jun 17 01:32:53 2006
From: bruno.giordano at music.mcgill.ca (Bruno L. Giordano)
Date: Fri, 16 Jun 2006 19:32:53 -0400
Subject: [R]  Effect size in mixed models
Message-ID: <002c01c6919d$39416d80$6400a8c0@brungio>

Hello,
Is there a way to compare the relative relevance of fixed and random effects 
in mixed models? I have in mind measures of effect size in ANOVAs, and would 
like to obtain similar information with mixed models.

Are there information criteria that allow to compare the relevance of each 
of the effects in a mixed model to the overall fit?

Thank you,
    Bruno


From aolinto_r at bignet.com.br  Sat Jun 17 01:48:13 2006
From: aolinto_r at bignet.com.br (Antonio Olinto)
Date: Fri, 16 Jun 2006 20:48:13 -0300
Subject: [R] prob in sample function
Message-ID: <1150501693.4493433def296@webmail.bignet.com.br>

Hi,

I have a data set with values (L) varying from 1 to 700, with repeated numbers,
and I want to sample them according to the probabilities given by a logistic
curve P=1/(1+e(-r*(L-Lc))) where r gives "S" the inclination
and Lc the "rotation point". P ranges from 0 to 1.

As I could see I cannot use this P as prob in sample function. I'd need the
probabilities as given by dnorm function, isn't it?

Well, how can I ?translate? the P given in the logistic function?

Thanks for any help. Best regards.

Antonio Olinto
Sao Paulo Fisheries Institute
Brazil
-------------------------------------------------
WebMail Bignet - O seu provedor do litoral
www.bignet.com.br


From pauljohn32 at gmail.com  Sat Jun 17 04:39:46 2006
From: pauljohn32 at gmail.com (Paul Johnson)
Date: Fri, 16 Jun 2006 20:39:46 -0600
Subject: [R] R in ConTeXt
In-Reply-To: <97a06f070606160750t7b249156xba478211c1f53db5@mail.gmail.com>
References: <97a06f070606160750t7b249156xba478211c1f53db5@mail.gmail.com>
Message-ID: <13e802630606161939o33c20f03y632eef7037d1bda2@mail.gmail.com>

I will test this out, but I'd like to point out that there is probably
an easier way.

Using LyX configured with Rweave, my students and I have had very good
luck generating documents that incorporate R graphics.  Like you
propose, it is a one-file approach.  I have example lyx and pdf output
files online in http://pj.freefaculty.org/stat/Distributions and there
is a guide to configure LyX so this works in a seemingly apparent way
on the LyX wiki:
http://wiki.lyx.org/LyX/LyxWithRThroughSweave.  We worked out a
Windows scheme to do the same, but I did not document it.

pj

On 6/16/06, Johan Sandblom <jsandblom at gmail.com> wrote:
> In case there are users of the TeX macro package ConTeXt on this list,
> they may be interested to know that it is now possible to include R
> code in a ConTeXt document and have the code evaluated while
> compiling. The inverse of Sweave, as it were. The advantage is that
> there is then only one file to keep track of (.tex vs .rnw and .tex),
> while a drawback is speed, since each code snippet is evaluated in its
> own R session with associated startup time. Below is a small example
> of usage
>
> \usemodule[r]
>
> \starttext
>
> \title{Example usage of R module}
>
> \startRhidden
> rm(list=ls())
> x <- rnorm(100)
> y <- runif(100)
> \stopRhidden
>
> \type{x} and \type{y} are randomly generated.
>
> \startR
> summary(lm(y~x))
> pdf("xy.pdf")
> plot(y~x)
> dev.off()
> \stopR
>
> \placefigure{Y vs X}{\externalfigure[xy][width=.4\textwidth]}
>
> \stoptext
>
> Regards, Johan
>
> --
> Johan Sandblom  N8, MRC, Karolinska sjh
> t +46851776108  17176 Stockholm
> m +46735521477  Sweden
> "What is wanted is not the will to believe, but the
> will to find out, which is the exact opposite"
> - Bertrand Russell
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
Paul E. Johnson
Professor, Political Science
1541 Lilac Lane, Room 504
University of Kansas


From spencer.graves at pdf.com  Sat Jun 17 05:01:26 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 16 Jun 2006 20:01:26 -0700
Subject: [R] Estimate region of highest probabilty density
In-Reply-To: <F357093629D891468190B597607CB6641808BD@EA-MAIL.eawag.wroot.emp-eaw.ch>
References: <F357093629D891468190B597607CB6641808BD@EA-MAIL.eawag.wroot.emp-eaw.ch>
Message-ID: <44937086.4070206@pdf.com>

	  I have more sympathy than ideas for you, but I'll attempt to offer 
the latter as I haven't seen other replies to your post.

	  First, if you've got a grid for "contour", you can do various kinds 
of numerical integration.  If I just wanted a rough answer and I didn't 
have to sell it to anyone, I might compute as fine a grid as I could 
reasonably do, then cut the grid in two above and below my desired 
threshold, and do some crude trapezoidal integration in each, use the 
sum of the two to normalize and be done with it.

	  However, if I needed something that would stand closer scrutiny, I'd 
read what the MASS book says about this.  I'd also try "RSiteSearch", 
which led me to 
"http://finzi.psych.upenn.edu/R/Rhelp02a/archive/26898.html", among 
other things.

	  If you can figure out how to get the coefficients of the spline fit, 
then you can do the desired integration analytically.

	  I know this doesn't solve your problem, but I hope it helps.

	  Spencer Graves

Christoph wrote:
> Estimate region of highest probabilty density
> 
> Dear R-community
> 
> I have data consisting of x and y. To each pair (x,y) a z 
value (weight) is assigned. With kde2d I can estimate the
densities on a regular grid and based on this make a contour
plot (not considering the z-values). According to an earlier
post in the list I adjusted the kde2d to kde2d.weighted (see
code below) to estimate the densities of x and y considering
their weights z. There's also a piece of code with artificial
data.
> 
> Now my question: Does a function exist which calculates the 
value of the level corresponding to a certain percentage of
the volume (i.e. above this level e.g. 95% of the total volume
lie, the (parameter) region of highest probability density)?
> 
> And secondly: How is it in the case of n parameters, when I 
don't want to plot it anymore but estimate quantiles for each
parameter considering also the weight z (to each set of
parameters c(v,w,x,y) a weight z is assigned)?
> 
> Many thanks in advance, I am very grateful for any hint
> Chris
> 
> 
> 
> 
> # MASS::kde2d copied and modified
> # ===============================
> 
> library(MASS)
> 
> kde2d.weighted <- function (x, y, w, h, n = n, lims = c(range(x), range(y))) {
>   nx <- length(x)
>   if (length(y) != nx) 
>       stop("data vectors must be the same length")
>   gx <- seq(lims[1], lims[2], length = n) # gridpoints x
>   gy <- seq(lims[3], lims[4], length = n) # gridpoints y
>   if (missing(h)) 
>     h <- c(bandwidth.nrd(x), bandwidth.nrd(y));
>   if (missing(w)) 
>     w <- numeric(nx)+1;
>   h <- h/4
>   ax <- outer(gx, x, "-")/h[1] # distance of each point to each grid point in x-direction
>   ay <- outer(gy, y, "-")/h[2] # distance of each point to each grid point in y-direction
>   z <- (matrix(rep(w,n), nrow=n, ncol=nx, byrow=TRUE)*matrix(dnorm(ax), n, nx)) %*% t(matrix(dnorm(ay), n, nx))/(sum(w) * h[1] * h[2]) # z is the density
>   return(list(x = gx, y = gy, z = z))
> }
> 
> 
> # Generate artificial data
> # ========================
> 
> x <- runif(20000,-5,5)
> y <- runif(20000,-5,5)
> z <- dnorm(x, mean=0, sd=1)*dnorm(y, mean=0, sd=1)
> 
> # plot data
> # =========
> 
> library(Rcmdr)
> scatter3d(x=x,z=y,y=z,surface=FALSE,xlab="x",ylab="z",zlab="y",bg.col="black")
> 
> temp0 <- kde2d(x=x, y=y, n = 100, lims=c(range(x),range(y))) 
> contour(x=temp0$x, y=temp0$y, z=temp0$z, xlab="x", ylab="y", main="z")
> 
> temp <- kde2d.weighted(x=x, y=y, w=z, n = 100, lims=c(range(x),range(y))) 
> contour(x=temp$x, y=temp$y, z=temp$z, xlab="x", ylab="y", main="z", col="red", add=TRUE)
> 
> 
> 
> 
> 
> ???
> Christoph Ort
> Eawag - aquatic research
> Environmental Engineering
> ?berlandstrasse 133
> CH-8600 D?bendorf
> 
> phone: +41-44-823-5041          
> fax:   +41-44-823-5389
> cell:  +41-79-218-9242
>  
> mailto:christoph.ort at eawag.ch
> 
> http://www.eawag.ch/~ortchris/
> 
> http://www.eawag.ch 
> 
> 
> 
> 
> ???
> Christoph Ort
> Eawag - aquatic research
> Environmental Engineering
> ?berlandstrasse 133
> CH-8600 D?bendorf
> 
> phone: +41-44-823-5041          
> fax:   +41-44-823-5389
> cell:  +41-79-218-9242
>  
> mailto:christoph.ort at eawag.ch
> 
> http://www.eawag.ch/~ortchris/
> 
> http://www.eawag.ch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From p.dalgaard at biostat.ku.dk  Sat Jun 17 08:44:29 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Jun 2006 08:44:29 +0200
Subject: [R] modeling logit(y/n) using lrm
In-Reply-To: <4492ECC3.9090005@utoronto.ca>
References: <E52E20F6B4A2F548B16BB259DD5168CF02FB6793@BHDAEXCH11.bhcs.pvt>
	<4492ECC3.9090005@utoronto.ca>
Message-ID: <x2hd2kz3g2.fsf@turmalin.kubism.ku.dk>

"Kevin E. Thorpe" <kevin.thorpe at utoronto.ca> writes:

> Hamilton, Cody wrote:
> > I have a dataset at a hospital level (as opposed to the patient level)
> > that contains number of patients experiencing events (call this number
> > y), and the number of patients eligible for such events (call this
> > number n).  I am trying to model logit(y/n) = XBeta.  In SAS this can be
> > done in PROC LOGISTIC or GENMOD with a model statement such as: model
> > y/n = <predictors>;.  Can this be done using lrm from the Hmisc library
> > without restructuring the dataset so that for each hospital there is one
> > row with y = 1 and one row with y = 0 and then using the weight option
> > in lrm to weight these two responses by the number of 'successes' and
> > 'failures' for that hospital, respectively?  I would like to avoid the
> > restructuring, and I understand that the use of the weight function is
> > not compatible with a lot of the validation functions available in Hmisc
> > (validate, bootcov, etc.).
> 
> I don't know about lrm, but for glm you can do
> glm(cbind(y,m)~ ...) where y is number of successes and
> m is the number of failures.

  glm(y/n~..., binomial, weight=n....)

should work as well. I suspect that this passes trough to lrm, too.

> So, you might try that.
> 
> > Cody Hamilton, Ph.D
> > 
> > Institute for Health Care Research and Improvement
> > 
> > Baylor Health Care System
> > 
> > (214) 265-3618
> > 
> 
> 
> -- 
> Kevin E. Thorpe
> Biostatistician/Trialist, Knowledge Translation Program
> Assistant Professor, Department of Public Health Sciences
> Faculty of Medicine, University of Toronto
> email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.946.3297
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From f.harrell at vanderbilt.edu  Sat Jun 17 09:44:59 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 17 Jun 2006 02:44:59 -0500
Subject: [R] modeling logit(y/n) using lrm
In-Reply-To: <E52E20F6B4A2F548B16BB259DD5168CF02FB6796@BHDAEXCH11.bhcs.pvt>
References: <E52E20F6B4A2F548B16BB259DD5168CF02FB6796@BHDAEXCH11.bhcs.pvt>
Message-ID: <4493B2FB.2040403@vanderbilt.edu>

Hamilton, Cody wrote:
> After a little digging, it turns out that fit.mult.impute will allow
> fitter = glm, so previous suggestions regarding modeling cbind(y,n) as
> an outcome will work fine.  Thanks!

Also lrm can easily handle your setup using the weights argument.

Frank

> 
> 
> 
> Cody Hamilton, Ph.D
> 
> Institute for Health Care Research and Improvement
> 
> Baylor Health Care System
> 
> (214) 265-3618
> 
> 
> 
> 
> 
> This e-mail, facsimile, or letter and any files or attachments transmitted with it contains information that is confidential and privileged. This information is intended only for the use of the individual(s) and entity(ies) to whom it is addressed. If you are the intended recipient, further disclosures are prohibited without proper authorization. If you are not the intended recipient, any disclosure, copying, printing, or use of this information is strictly prohibited and possibly a violation of federal or state law and regulations. If you have received this information in error, please notify Baylor Health Care System immediately at 1-866-402-1661 or via e-mail at privacy at baylorhealth.edu. Baylor Health Care System, its subsidiaries, and affiliates hereby claim all applicable privileges related to this information.
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From juliacel at techunix.technion.ac.il  Sat Jun 17 11:58:44 2006
From: juliacel at techunix.technion.ac.il (juliacel at techunix.technion.ac.il)
Date: Sat, 17 Jun 2006 12:58:44 +0300
Subject: [R] Getting forcasting equation from nnet results
Message-ID: <1150538324.4493d254a3283@webmail.technion.ac.il>


I'm trying to build forecasting equation from weights of 2-2-1 neural net.
Running the nnet function gives me a vecto of 9 weights, but I don't know how
to build the equation form these values.
Can anyone advice? Or at least tell me where the nnet output is described in
details (the manual only gives a brief description).

Thanks.


From anil_rohilla at rediffmail.com  Sat Jun 17 13:19:10 2006
From: anil_rohilla at rediffmail.com (anil kumar rohilla)
Date: 17 Jun 2006 11:19:10 -0000
Subject: [R] Projection pursuit regression method
Message-ID: <20060617111910.26158.qmail@webmail62.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060617/bdbb0d1d/attachment.pl 

From hannesalazar at gmail.com  Sat Jun 17 15:40:47 2006
From: hannesalazar at gmail.com (yohannes alazar)
Date: Sat, 17 Jun 2006 14:40:47 +0100
Subject: [R] managing data
Message-ID: <ae94396d0606170640h41c0d8b3m948175e0ca749ba@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060617/e7382bd6/attachment.pl 

From vdemart1 at tin.it  Sat Jun 17 19:06:55 2006
From: vdemart1 at tin.it (vittorio)
Date: Sat, 17 Jun 2006 17:06:55 +0000
Subject: [R] Having a go at Rpad
Message-ID: <200606171706.56056.vdemart1@tin.it>

Context: FreeBSD 6.1on a pentium 4; apache 2 server up & running; mod_perl 
installed; java & php 4 working.

Willing to use Rpad on an office intranet, I installed the Rpad package 
(R2HTML was still there) and carefully read the instructions to set up a 
server.

Now,  on the server I can start the http://localhost/Rpad homepage, I can 
click on and browse the three examples ("General Example", "Data Input 
Example" & "GUI example"), BUT ....
whenever I click on the "calculate" button in those example pages the 
underlying perl programs are browsed, listed  and nothing seems to happen.

Please help to straigthen my Rpad installation up

Ciao
Vittorio

P.S. By the way those perl progs seems to point to a /var/www/Rpad directory. 
That's why I installed the ../basehtml dir under that directory and symlinked 
it to unix usual web dir :/usr/local/www/Rpad


From miltinho_astronauta at yahoo.com.br  Sat Jun 17 17:31:32 2006
From: miltinho_astronauta at yahoo.com.br (Milton Cezar)
Date: Sat, 17 Jun 2006 15:31:32 +0000 (GMT)
Subject: [R] substr or split help needed
Message-ID: <20060617153132.25807.qmail@web53412.mail.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060617/d28b0fb4/attachment.pl 

From philipp.pagel.lists at t-online.de  Sat Jun 17 17:49:52 2006
From: philipp.pagel.lists at t-online.de (Philipp Pagel)
Date: Sat, 17 Jun 2006 17:49:52 +0200
Subject: [R] managing data
In-Reply-To: <ae94396d0606170640h41c0d8b3m948175e0ca749ba@mail.gmail.com>
References: <ae94396d0606170640h41c0d8b3m948175e0ca749ba@mail.gmail.com>
Message-ID: <20060617154952.GA18262@gsf.de>

On Sat, Jun 17, 2006 at 02:40:47PM +0100, yohannes alazar wrote:
> Dear mailing list, may some one be kind to help me solve following problem.
> 
> I am trying to write a code that will combine two tables "x" and "y". The
> first columns of both tables are unique identification for the rows. The
> first column of table "X" is a sub set of the first column of "Y". I need to
> find the matching rows in both tables by looking on their unique
> identification at the first columns.

?merge

cu
	Philipp

-- 
Dr. Philipp Pagel                            Tel.  +49-8161-71 2131
Dept. of Genome Oriented Bioinformatics      Fax.  +49-8161-71 2186
Technical University of Munich
Science Center Weihenstephan
85350 Freising, Germany

 and

Institute for Bioinformatics / MIPS          Tel.  +49-89-3187 3675
GSF - National Research Center               Fax.  +49-89-3187 3585
      for Environment and Health
Ingolst?dter Landstrasse 1
85764 Neuherberg, Germany
http://mips.gsf.de/staff/pagel


From neuro3000 at hotmail.com  Sat Jun 17 17:56:37 2006
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Sat, 17 Jun 2006 11:56:37 -0400
Subject: [R] managing data
In-Reply-To: <ae94396d0606170640h41c0d8b3m948175e0ca749ba@mail.gmail.com>
Message-ID: <BAY112-F26BFA9197D3257943BB43EAF800@phx.gbl>

Hi

Change V1 for the name of the column to be matched on X and Y

cbind(X,Y[match(X$V1,Y$V1),])

Neurorox


>From: "yohannes alazar" <hannesalazar at gmail.com>
>To: r-help at stat.math.ethz.ch
>Subject: [R] managing data
>Date: Sat, 17 Jun 2006 14:40:47 +0100
>
>Dear mailing list, may some one be kind to help me solve following problem.
>
>I am trying to write a code that will combine two tables "x" and "y". The
>first columns of both tables are unique identification for the rows. The
>first column of table "X" is a sub set of the first column of "Y". I need 
>to
>find the matching rows in both tables by looking on their unique
>identification at the first columns. Table is "X" is (19010 rw, 6 col) and
>"Y" is (19850 rw, 9 col). Therefore my out put need to be "Z" is 
>(19010rw,15
>col). A small example of my input and out put is here below.
>
>
>Thank you in advance!
>
>
>input
>Table"X""
>
>A,1,2,2
>B,2,1,1
>C,1,2,2
>E,2,2,1
>F,2,1,1
>H,1,2,2
>J,2,1,1
>K,2,1,1
>
>Table"Y"
>
>A,GG,CG,
>B,CC,TC,
>C,CA,AA,
>D,AT,TT,
>E,AA,AA,
>F,CG,CG,
>G,CC,CC,
>H,GG,GT,
>I,GT,TT,
>J,AA,AT,
>K,AC,CC,
>
>output
>Table"Z""
>
>A,1,2,2,A,GG,CG,
>B,2,1,1,B,CC,TC,
>C,1,2,2,C,CA,AA,
>E,2,2,1,E,AA,AA,
>F,2,1,1,F,CG,CG,
>H,1,2,2,H,GG,GT,
>J,2,1,1,J,AA,AT,
>K,2,1,1,K,AC,CC,
>
>  Regards Hannes
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html


From spencer.graves at pdf.com  Sat Jun 17 18:46:41 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 17 Jun 2006 09:46:41 -0700
Subject: [R] lmer and mixed effects logistic regression
In-Reply-To: <1150293574.3416.11.camel@localhost.localdomain>
References: <1150293574.3416.11.camel@localhost.localdomain>
Message-ID: <449431F1.6000906@pdf.com>



'lmer' RETURNS AN ERROR WHEN SAS NLMIXED RETURNS AN ANSWER

	  Like you, I would expect lmer to return an answer when SAS
NLMIXED does, and I'm concerned that it returns an error message instead.

	  Your example is not self contained, and I've been unable to
get the result you got.  This could occur for several reasons.  First,
what do you get for "sessionInfo()"?  I got the following:

sessionInfo()
Version 2.3.1 (2006-06-01)
i386-pc-mingw32

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
       lme4    lattice     Matrix
  "0.995-2"   "0.13-8" "0.995-10"

	  If you are using different versions of lmer, lattice and / or
Matrix, please update.packages and try again.  If that does not fix the 
problem, could you please try to find a self-contained example that is 
as simple as possible and still generates the same error message?  Then 
send that to this list.

	  When I ran your example limited only to the observations you
listed, I got a different result:

Warning messages:
1: fitted probabilities numerically 0 or 1 occurred in: glm.fit(X, Y,
weights = weights, offset = offset, family = family,
2: IRLS iterations for PQL did not converge

	  We get this these "Warning messages" because all the values
of "response" are constant within each level of "id".  With data like 
this, the likelihood will be maximized by sending Std.Dev.(id) to Inf; 
when 'lmer' quit, Std.Dev.(id) was already at 58779, which 
computationally is moderately close to Inf for the current "lmer" 
algorithm.

	  How many different patterns of "response" by "id" do you have?
With all 0's, the random adjustment to "(Intercept)" for that "id", 
ignoring the Bayesian shrinkage, is "-Inf".  With all 1's, this random 
adjustment would be "+Inf".  With two observations for a subject, if 
they are different, this random adjustment would exactly cancel the 
estimate of the fixed-effect for "(Intercept").  Do you have only these 
three patterns, all 0's, all 1's, and half 0's, half 1's?  If yes, that 
might explain why Std.Dev.(id) wants to go to Inf.

	  I don't know exactly how many different patterns you need, but
you should be able to have some levels of "id" with all 0's or all 1's 
as long as those did not dominate, as they do in this cut down example.

	  If this is the problem, then SAS NLMIXED could be achieving false 
convergence and either not telling you about it or reporting it so
quietly you have not reported it here.


CREATING A REPRODUCIBLE EXAMPLE

	  I'm not eager to experiment with a large complicated example.
You could help us help you by trying to produce a simpler, 
self-contained example.  For example, do you get the same answer when 
you delete 'age' from the model:

	  lmer(response~(1|id),data=gdx,family=binomial)

	  If yes, then you could easily just count the number of levels of "id" 
that have all 0's, all 1's, (0, 1) = (1, 0), etc.  Then write one or two 
lines that would generate that special case and submit that to this 
listserve.  Before you do, however, I encourage you to experiment to 
find small numbers of replicates that reproduce the error you get.  Then 
submit that to this list.

STARTING VALUES?

	  The help page for "lmer" describes the following argument:

    start: a list of relative precision matrices for the random effects.
            This has the same form as the slot '"Omega"' in a fitted
           model.  Only the upper triangle of these symmetric matrices
           should be stored.

	  To understand this, it might help to experiment with one of
the examples on this help page:

> fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
> fm1 at Omega
$Subject
2 x 2 Matrix of class "dpoMatrix"
             (Intercept)       Days
(Intercept)   1.0746247 -0.2942832
Days         -0.2942832 18.7549595

	  Just taking a guess, I tried the following:

fm1. <- lmer(Reaction ~ Days+(Days|Subject), sleepstudy,
              start=list(Subject=diag(1:2)))

fm1.c <- lmer(Reaction ~ Days+(Days|Subject), sleepstudy,
              start=list(Subject=array(1, .1, .1, 2),
                      dim=c(2,2)))

	   Both of these returned the same answer.  Starting values are
only required for the random effects, because these models are all 
"plinear" in the sense described in the "nls" documentation.

	  For "glmmPQL", I was able to get the "update" function to work. 
Thus, if you can get an answer with one model, you can use that as an 
input to "update".  I would expect "update" to try to use the parameter 
estimates from one model fit as starting values for the modification, 
where it can find a way to do that.

	  Hope this helps.
	  Spencer Graves

Rick Bilonick wrote:
> I'm using FC4 and R 2.3.1 to fit a mixed effects logistic regression.
> The response is 0/1 and both the response and the age are the same for
> each pair of observations for each subject (some observations are not
> paired). For example:
> 
> id response age
> 1    0      30
> 1    0      30
> 
> 2    1      55
> 2    1      55
> 
> 3    0      37
> 
> 4    1      52
> 
> 5    0      39
> 5    0      39
> 
> etc.
> 
> I get the following error:
> 
>> (lmer(response~(1|id)+age,data=gdx,family=binomial))
> Error in deviance(.Call("mer_coefGets", x, pars, 2, PACKAGE =
> "Matrix")) :
>         Leading minor of order 2 in downdated X'X is not positive
> definite
> 
> Similar problem if I use quasibinomial.
> 
> If I use glm, of course it thinks I have roughly twice the number of
> subjects so the standard errors would be smaller than they should be.
> 
> I used SAS's NLMIXED and it converged without problems giving me
> parameter estimates close to what glm gives, but with larger standard
> errors. glmmPQL(MASS) gives very different parameter estimates.
> 
> Is it reasonable to fit a mixed effects model in this situation?
> 
> Is there some way to give starting values for lmer and/or glmmPQL?
> 
> Rick B.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From spencer.graves at pdf.com  Sat Jun 17 19:09:04 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 17 Jun 2006 10:09:04 -0700
Subject: [R] lmer binomial model overestimating data?
In-Reply-To: <0E50C39C-FFD9-43E9-BA06-243DD1DD2A8E@muohio.edu>
References: <0E50C39C-FFD9-43E9-BA06-243DD1DD2A8E@muohio.edu>
Message-ID: <44943730.7090007@pdf.com>

<see inline>	

Martin Henry H. Stevens wrote:
> Hi folks,
> Warning: I don't know if the result I am getting makes sense, so this  
> may be a statistics question.
> 
> The fitted values from my binomial lmer mixed model seem to  
> consistently overestimate the cell means, and I don't know why. I  
> assume I am doing something stupid.

	  I copied your "Raw.Data" and "Fitted.Estimates" into two columns of a 
data.frame OE and then did a t test as follows:
 > with(OE, t.test(Raw.Data-Fitted.Estimates))

	One Sample t-test

data:  Raw.Data - Fitted.Estimates
t = -2.1662, df = 11, p-value = 0.05313
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
  -0.0991997752  0.0007897752
sample estimates:
mean of x
-0.049205

	  I also tried the same thing with a modification of an example in the 
"lmer" help file.  In that case, lmer seemed to consistently 
UNDERestimate the cell means.  However, the difference again was not 
statistically significant.

	  I suggest you consider this a disguised Rorschach ink blot test and 
worry about other things.

	  Hope this helps.
	  Spencer Graves	

> 
> Below I include code, and a binary image of the data is available at  
> this link:
> http://www.cas.muohio.edu/~stevenmh/tf.RdataBin
> 
> This was done with `Matrix' version 0.995-10 and `lme4' version  
> 0.995-2. and R v. 2.3.1 on a Mac, OS 10.4.6.
> 
> The binomial model below ("mod") was reduced from a more complex one  
> by first using AIC, BIC and LRT for "random" effects, and then  
> relying on Helmert contrasts and AIC, BIC, and LRT to simplify fixed  
> effects. Maybe this was wrong?
> 
>  > load("tf.RdataBin")
>  > library(lme4)
> 
>  > options(contrasts=c("contr.helmert", "contr.poly"))
>  > mod <- lmer(tfb ~ reg+nutrient+amd +reg:nutrient+
> +     (1|rack) + (1|popu) +  (1|gen), data=dat.tb2, family=binomial,  
> method="Laplace")
>  > summary(mod)
> Generalized linear mixed model fit using Laplace
> Formula: tfb ~ reg + nutrient + amd + reg:nutrient + (1 | rack) + (1  
> |      popu) + (1 | gen)
> 	  Data: dat.tb2
> Family: binomial(logit link)
>      AIC    BIC  logLik deviance
> 402.53 446.64 -191.26   382.53
> Random effects:
> Groups Name        Variance Std.Dev.
> gen    (Intercept) 0.385    0.621
> popu   (Intercept) 0.548    0.741
> rack   (Intercept) 0.401    0.633
> number of obs: 609, groups: gen, 24; popu, 9; rack, 2
> 
> Estimated scale (compare to 1)  0.80656
> 
> Fixed effects:
>                 Estimate Std. Error z value Pr(>|z|)
> (Intercept)       2.391      0.574    4.17  3.1e-05
> reg1              0.842      0.452    1.86  0.06252
> reg2              0.800      0.241    3.32  0.00091
> nutrient1         0.788      0.197    4.00  6.3e-05
> amd1             -0.540      0.139   -3.88  0.00010
> reg1:nutrient1    0.500      0.227    2.21  0.02734
> reg2:nutrient1   -0.176      0.146   -1.21  0.22794
> 
> Correlation of Fixed Effects:
>              (Intr) reg1   reg2   ntrnt1 amd1   rg1:n1
> reg1         0.169
> reg2        -0.066 -0.191
> nutrient1    0.178  0.231 -0.034
> amd1        -0.074 -0.044 -0.052 -0.078
> reg1:ntrnt1  0.157  0.307 -0.180  0.562 -0.002
> reg2:ntrnt1 -0.028 -0.154  0.236  0.141  0.033 -0.378
>  > X <- mod @ X
>  > fitted <- X %*% fixef(mod)
>  >
>  > unlogitH <- function(x) {( 1 + exp(-x) )^-1}
>  > (result <- data.frame(Raw.Data=with(dat.tb2,
> +                          tapply(tfb, list(reg:nutrient:amd),
> +                          mean ) ),
> +         Fitted.Estimates=with(dat.tb2,
> +                          tapply(fitted, list(reg:nutrient:amd),
> +                          function(x) unlogitH(mean(x))  ) )  ))
>                 Raw.Data Fitted.Estimates
> SW:1:unclipped  0.50877          0.69520
> SW:1:clipped    0.41304          0.43669
> SW:8:unclipped  0.67273          0.85231
> SW:8:clipped    0.52830          0.66233
> NL:1:unclipped  0.88889          0.81887
> NL:1:clipped    0.53571          0.60578
> NL:8:unclipped  0.96552          0.98830
> NL:8:clipped    0.96154          0.96635
> SP:1:unclipped  0.98649          0.98361
> SP:1:clipped    0.92537          0.95328
> SP:8:unclipped  1.00000          0.99308
> SP:8:clipped    0.95890          0.97992
>  > ### Perhaps the cell SP:8:clipped = 1.0 is messing up the fit?
>  > pdf("RawAndFitted.pdf")
>  > par(mar=c(8,3,2,2), las=2)
>  > barplot(t(result), beside=TRUE )
>  > box(); title("Fractions of Plants Producing Fruits")
>  > legend("topleft", c("Raw Data", "Fitted Values"),
> +        fill=gray.colors(2), bty="n" )
>  > dev.off()
> quartz
>       2
>  >
> 
>                 _
> platform       powerpc-apple-darwin8.6.0
> arch           powerpc
> os             darwin8.6.0
> system         powerpc, darwin8.6.0
> status
> major          2
> minor          3.1
> year           2006
> month          06
> day            01
> svn rev        38247
> language       R
> version.string Version 2.3.1 (2006-06-01)
>  >
> 
> Dr. M. Hank H. Stevens, Assistant Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
> 
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/~stevenmh/
> http://www.muohio.edu/ecology/
> http://www.muohio.edu/botany/
> "E Pluribus Unum"
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From jholtman at gmail.com  Sat Jun 17 20:24:16 2006
From: jholtman at gmail.com (jim holtman)
Date: Sat, 17 Jun 2006 14:24:16 -0400
Subject: [R] substr or split help needed
In-Reply-To: <20060617153132.25807.qmail@web53412.mail.yahoo.com>
References: <20060617153132.25807.qmail@web53412.mail.yahoo.com>
Message-ID: <644e1f320606171124r722d6a81ie853522de003ad49@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060617/1312bf75/attachment.pl 

From dnealy at rand.org  Sat Jun 17 20:47:40 2006
From: dnealy at rand.org (Nealy, David)
Date: Sat, 17 Jun 2006 11:47:40 -0700
Subject: [R] Panel Bar plots
Message-ID: <5C9D80739A5FCC4995C426E74D3573C4E914D9@smmail4.rand.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060617/34da202e/attachment.pl 

From ggrothendieck at gmail.com  Sat Jun 17 21:44:51 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 17 Jun 2006 15:44:51 -0400
Subject: [R] substr or split help needed
In-Reply-To: <20060617153132.25807.qmail@web53412.mail.yahoo.com>
References: <20060617153132.25807.qmail@web53412.mail.yahoo.com>
Message-ID: <971536df0606171244r5e61da79v792eddf2c4a4e1d6@mail.gmail.com>

Try this:

# read in lines and replace all occurences of Simula, all occurrences
# of .txt, all commas and all underscores with spaces

Lines <- gsub(".*Simula|.txt|[,_]", " ", readLines("/myfile.dat"))

# reread what is left skipping over column headings and setting it ourself
# If you want factors instead of character data omit as.is=TRUE.

dat <- read.table(textConnection(Lines), as.is = TRUE, skip = 1,
	col.names = c("P", "H", "R", "TYPE", "PLAND"))


On 6/17/06, Milton Cezar <miltinho_astronauta at yahoo.com.br> wrote:
> Dear R-friends
>
>  I have several data files with about 1,900 lines (records) each. I?m using read.table command to read the files. The files looks like
>  LID                                                     ,   TYPE ,        PLAND
>  D:\Bijou-MC\Simula_P005_H100_R001.txt , Forest ,       NA
>  D:\Bijou-MC\Simula_P005_H100_R001.txt , Forest ,       10.2
>  D:\Bijou-MC\Simula_P010_H100_R001.txt , Forest ,        9.2
>  ---
>
>  My first problem is that some command (like hist(data$PLAND)) say that the data isn?t a numeric one. May be because the first PLAND value are NA? When I done read.table command I used something link:
>  data<-read.table (file="xxx.dat", head=T, sep="\,", na.strings="NA").
>
>  Another problem is that I need parse the LID column. When I do "print (head(data$LID) I receive the following result (look that the slash was lost on the read):
>       D:Bijou-MCSimula_P005_H100_R001.txt
>       D:Bijou-MCSimula_P005_H100_R001.txt
>       D:Bijou-MCSimula_P010_H100_R001.txt
>  Its ok to me, but now I need create the P, H and R columns into the "data" table as a parse of LID column. When I try use the command "p<-substr(data$LID, 19,3)" I got an error message saying that the variable is not char one.
>
>  Finally, I?d like drop the LID column and insert the P, H and R into the table.
>
>  Thanks for your help!
>
>  Kind regards, miltinho
>
>
>
>
>
>
>  __________________________________________________
>
>
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>


From tshort at eprisolutions.com  Sat Jun 17 22:00:03 2006
From: tshort at eprisolutions.com (tshort)
Date: Sat, 17 Jun 2006 13:00:03 -0700 (PDT)
Subject: [R] Having a go at Rpad
In-Reply-To: <200606171706.56056.vdemart1@tin.it>
References: <200606171706.56056.vdemart1@tin.it>
Message-ID: <4917783.post@talk.nabble.com>


That means that Apache doesn't know to execute the perl files. Something's
wrong with your Apache configuration or directory permissions. Some things
to check:

* Make sure the *.pl files in ../Rpad/server/ are set to enable execution.

* Check your apache http.conf. You need something like the following to tell
perl that those *.pl files are executable (with the directory renamed as
appropriate):

<Directory /var/www/Rpad/server*>  
  <IfModule mod_perl.c>
    <Files *.pl> # requires mod_perl
      SetHandler perl-script
      PerlHandler Apache::Registry
      Options +ExecCGI
      PerlSendHeader ON
    </Files>
  </IfModule>
  Options +ExecCGI
  AddHandler cgi-script .pl
  <IfModule mod_expires.c>
    ExpiresActive on
    ExpiresDefault "now plus 0 seconds"
  </IfModule>
</Directory>

* Check your apache error logs; they might give you some insight. 

If none of those fixes it, contact me offline with more details.

- Tom


vittorio wrote:
> 
> Context: FreeBSD 6.1on a pentium 4; apache 2 server up & running; mod_perl 
> installed; java & php 4 working.
> 
> Willing to use Rpad on an office intranet, I installed the Rpad package 
> (R2HTML was still there) and carefully read the instructions to set up a 
> server.
> 
> Now,  on the server I can start the http://localhost/Rpad homepage, I can 
> click on and browse the three examples ("General Example", "Data Input 
> Example" & "GUI example"), BUT ....
> whenever I click on the "calculate" button in those example pages the 
> underlying perl programs are browsed, listed  and nothing seems to happen.
> 
> Please help to straigthen my Rpad installation up
> 
> Ciao
> Vittorio
> 
> P.S. By the way those perl progs seems to point to a /var/www/Rpad
> directory. 
> That's why I installed the ../basehtml dir under that directory and
> symlinked 
> it to unix usual web dir :/usr/local/www/Rpad
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
--
View this message in context: http://www.nabble.com/Having-a-go-at-Rpad-t1803552.html#a4917783
Sent from the R help forum at Nabble.com.


From gavin.simpson at ucl.ac.uk  Sat Jun 17 22:59:49 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Sat, 17 Jun 2006 21:59:49 +0100
Subject: [R] problem with legend on other graphics devices
In-Reply-To: <1150483334.23506.67.camel@gsimpson.geog.ucl.ac.uk>
References: <1150483334.23506.67.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <1150577989.2824.12.camel@dhcppc2.my.nat.localnet>

On Fri, 2006-06-16 at 19:42 +0100, Gavin Simpson wrote:
> Hi,
> 
> I have a bit of a problem with a legend in png, eps and pdf plots
> produced from a custom plotting function. I was writing a little
> function to produce some stock plots for a routine analysis conducted in
> our lab. I have a wrapper function figures() (see below) that produces
> eps, png and pdf versions of the lab output. When I draw the plot on the
> X11() device the correct amount of space is made available for the
> legend, but when I draw the plot directly on the png, postscript or pdf
> devices, the legend is placed too close to the edge of the plot and is
> clipped.

Hi,

To follow up, the posted code does work and the legend is not clipped on
my home machine but that was a slightly less-up-to-date version of R
2.3.1 patched than the machine at work. So I compiled the latest R2.3.1
patched (svn revision 38350) and it works fine there also. So must have
been something peculiar to my set-up at work or R session at the time.

For the record:
> version
               _
platform       i686-pc-linux-gnu
arch           i686
os             linux-gnu
system         i686, linux-gnu
status         Patched
major          2
minor          3.1
year           2006
month          06
day            16
svn rev        38350
language       R
version.string Version 2.3.1 Patched (2006-06-16 r38350)

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 *Note new Address and Fax and Telephone numbers from 10th April 2006*
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/cv/
WC1E 6BT                          [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From spencer.graves at pdf.com  Sat Jun 17 23:58:39 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 17 Jun 2006 14:58:39 -0700
Subject: [R] Question concerning mle
In-Reply-To: <449166A3.40209@sun.ac.za>
References: <449166A3.40209@sun.ac.za>
Message-ID: <44947B0F.50305@pdf.com>

	  It's difficult to say much at this level of generality, but I have 
four suggestions:

	  1.  Have you tried creating a reasonable grid of starting values 
using "expand.grid" and then plotting the resulting likelihood surface? 
  If you have more than 2 parameters, you may want to use 'lattice' 
graphics.  This should tell you if the functions seems unimodal, convex, 
etc., in the region you covered and at the resolution of your grid.

	  2.  Have you tried method="SANN" = simulated annealing?  I might try 
one pass with SANN, then refine the solution found by SANN using BFGS.

	  3.  After you have a solution, you can then try profile likelihod. 
Unfortunately, my experience with profile.mle has been mixed.  I 
actually made local copies of mle and profile.mle and found and fixed 
some of the deficiencies of each.  I didn't test them enough to offer 
the results to the R Core Team, however.

	  4.  Have you looked at Venables and Ripley (2002) Modern Applied 
Statistics with S, 4th ed. (Springer)?  It's a great book for many 
things, including the use of expand.grid and 'optim'.

	  hope this helps.
	  Spencer Graves

Rainer M Krug wrote:
> Hi
> 
> I hope this is the right forum - if not, point me please to a better one.
> 
> I am using R 2.3.0 on Linux, SuSE 10.
> 
> 
> I have a question concerning mle (method="BFGS").
> 
> I have a few models which I am fitting to existing data points. I
> realised, that the likelihood is quite sensitive to the start values for
> one parameter.
> 
> I am wondering: what is the best approach to identify the right initial
> values? Do I have to do it recursively, and if yes, how can I automate
> it? Or do I have to play with the system?
> 
> I am quite confident that the resulting parameters are the optimal for
> my problem - but can I verify it?
> 
> Thanks,
> 
> Rainer
> 
>


From spencer.graves at pdf.com  Sun Jun 18 01:51:04 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 17 Jun 2006 16:51:04 -0700
Subject: [R] SSPIR problem
In-Reply-To: <3723.193.204.253.13.1150385619.squirrel@mailserver.unibg.it>
References: <3723.193.204.253.13.1150385619.squirrel@mailserver.unibg.it>
Message-ID: <44949568.3060402@pdf.com>

	  Did you try the 'examples' in the SS help file?  If no, please copy 
them from the help file into a script file and work through them line by 
line, trying different modifications.  If you've tried this, have you 
similarly worked through the 'sspir' 'demo' files?  (These can be found 
in the '~\library\sspir\demo' subdirectory of the directory where R is 
installed.)

	  If you've tried these and would like further help from this 
listserve, please submit another post, preferably after reading read the 
posting guide! "www.R-project.org/posting-guide.html".  In particular, 
please include in your post a self-contained example that is as simple 
as you can make it that will still illustrate our problem.

	  I've never used 'sspir', but the examples in the 'SS' help look 
fairly straightforward to me.  For example, it says that Fmat returns a 
matrix of dim(p, d).  From this I infer that the "k" in your model is 
t(Fmat(...)).  Similarly, your "h" is "Gmat", and "Vmat" and "Wmat" 
match your "V" and "W".

	  hope this helps.
	  Spencer Graves

michela.cameletti at unibg.it wrote:
> Dear R-Users,
> I'm using SSPIR package for a spatio-temporal application.
> Is it possible to modify the structure of the involved matrixes (Fmat,
> Gmat, Vmat,Wmat)?
> I want to create a model like this
> 
> #y(t)=k*theta(t)+epsilon(t)
> #theta(t)=h*theta(t-1)+eta(t)
> #epsilon(t) N(0,V)         V=sigma2*I
> #eta(t)     N(0,W)        W=sigma2_eta
> 
> where the state variable theta has dimension 1(p=1) and at each time time
> the observed variable y has dimension equal to d=3. Moreover I want to use
> white noise errors.
> But with the command
> SS(y) where y is my nXd data matrix (d=observations at each time,
> n=numbers of time points)
> I obtain a different model.
> 
> Can you help me please?
> Thanks in advance
> Michela
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From murdoch at stats.uwo.ca  Sun Jun 18 10:14:44 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 18 Jun 2006 04:14:44 -0400
Subject: [R] Standard Deviation Distribution
In-Reply-To: <F9F2A641C593D7408925574C05A7BE7704851A@rhopost.rhotrading.com>
References: <F9F2A641C593D7408925574C05A7BE7704851A@rhopost.rhotrading.com>
Message-ID: <44950B74.4020407@stats.uwo.ca>

davidr at rhotrading.com wrote:
> I'm having trouble with the standard deviation distribution
> as shown on http://mathworld.wolfram.com/StandardDeviationDistribution.html .
> (Eric Weisstein references Kenney and Keeping 1951, which I can't check.)
>
> I believe the graphs they show, but when I code the function in R, according to the listed formula,
> I get very different graphs.
>
> Would someone please point out my error or tell me where it's already implemented in R?
>
> Here is my version:
>   
>> sddist
>>     
> function(s,n) {
> sig2 <- n*s*s/(n-1)
> 2*(n/(2*sig2))^((n-1)/2) / gamma((n-1)/2) * exp(-n*s*s/(2*sig2)) * s^(n-2)
> }
>
>   
Your initial factor looks a lot different from theirs.  I think you 
believed your variable name (i.e. sig2 is sigma^2), but didn't
define it that way.

Duncan Murdoch
> Version 2.3.1 (2006-06-01) on Windows XP SP2
>
> Thanks for any help.
>
> David L. Reiner
> Rho Trading Securities, LLC
> Chicago  IL  60605
> 312-362-4963
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ucecgxu at ucl.ac.uk  Sun Jun 18 12:04:57 2006
From: ucecgxu at ucl.ac.uk (ucecgxu at ucl.ac.uk)
Date: Sun, 18 Jun 2006 11:04:57 +0100
Subject: [R] analyze amino acid sequence (composition)of proteins
Message-ID: <20060618110457.j2oodu8bkgwk0kck@www.webmail.ucl.ac.uk>

Dear R-helpers:

thank your for your attention.

i am a newer to R and i am doing some protein category classification based on
the amino acid sequence.while i have some questions urgently.

1. any packages for analysis amino acid sequence

2. given two sequences "AAA" and "BBB",how can i combine them into "AAABBB"

3. based on "AAABBB",how can i get some statistics of this string such as how
many letters,how many "A"s in the string.


Thank you very much and i am looking forward to hearing from you soon

have a nice day!

Best Regards

Marshall


From bgreen at dyson.brisnet.org.au  Sun Jun 18 12:16:33 2006
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Sun, 18 Jun 2006 20:16:33 +1000
Subject: [R] how to successfully remove missing values for a repeated
 measures analysis
Message-ID: <5.1.0.14.0.20060618184815.00bec300@pop3.brisnet.org.au>

Hello ,

I am hoping for some advice. I want to run a repeated measures ANOVA. The 
primary problem is that my attempt to remove missing values created a 
dataset of missing values.

The data set consists of 92 rows (1 row per participant) x 186 variables.

The steps of the analysis undertaken are outlined below (#). Any assistance 
is appreciated in relation to how to remove the missing values so the 
analysis is run. Feedback regarding the prior steps is also welcomed .

Bob Green



#Step 1

study1dat <- read.csv("c:\\study1.csv",header=T)
attach (study1dat)

outcome <- c(t1frq, t2frq,t3frq,t4frq)
grp <- factor( rep(group, 2,length=368) )
time <- gl(4,92,length=368)
subject <- gl(92,1,length=368)
data.frame(subject, grp, time, outcome)

# there are 3 missing values in $outcome

#Step 2 - create a new data frame removing missing values from variable 
$outcome

  d2<-study1dat[!is.na(outcome),]

#the previous step generates NA values.

#Step 3 detach original data set & attach dataset without missing values
detach(study1dat)

attach(d2)

The following object(s) are masked _by_ .GlobalEnv :         time
         The following object(s) are masked from package:datasets 
:         sleep

#Step 4 run analysis

library(nlme)
anova(lme(outcome ~ grp * time, random = ~ 1 | subject))

#The data is the format below

     subject grp time outcome
1         1   0    1       4
2         2   0    1       3
3         3   0    1       7
4         4   0    1       0
5         5   0    1       1
6         6   0    1       7
7         7   0    1       7
8         8   0    1       7
9         9   0    1       7
10       10 0    1      5


From ucecgxu at ucl.ac.uk  Sun Jun 18 08:45:44 2006
From: ucecgxu at ucl.ac.uk (ucecgxu at ucl.ac.uk)
Date: Sun, 18 Jun 2006 07:45:44 +0100
Subject: [R] about the analysis of strings, thanks
Message-ID: <20060618074544.usb5r3uhskcwo0oo@www.webmail.ucl.ac.uk>

Dear R-helpers:

thank your for your attention.

i am a newer to R and i am doing some protein category classification based on
the amino acid sequence.while i have some question urgent.

1. any packages for analysis amino acid sequence
2. given two sequences "AAA" and "BBB",how can i combine them into "AAABBB"

3. based on "AAABBB",how can i get some statistics of this string such as how
many letters,how many "A"s in the string.


Thank you very much and i am looking forward to hearing from you soon

have a nice day!

Best Regards

Marshall


From Dimitris.Rizopoulos at med.kuleuven.be  Sun Jun 18 13:02:00 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Sun, 18 Jun 2006 13:02:00 +0200
Subject: [R] about the analysis of strings, thanks
In-Reply-To: <20060618074544.usb5r3uhskcwo0oo@www.webmail.ucl.ac.uk>
References: <20060618074544.usb5r3uhskcwo0oo@www.webmail.ucl.ac.uk>
Message-ID: <1150628520.449532a83af2c@webmail2.kuleuven.be>

I'm not aware of package for amino acid sequence analysis, but it'd 
better to search yourself using, e.g., RSiteSearch() and help.search().

Regarding your question consider the following:

strg1 <- c("AAA", "AA", "AAAA")
strg2 <- c("BBB", "BBBB", "BBB")

strg <- paste(strg1, strg2, sep = "")
nA <- nchar(gsub("B", "", strg, fixed = TRUE))
nB <- nchar(gsub("A", "", strg, fixed = TRUE))

strg
nA # freq of A's
nB # freq of B's


I hope it helps.

Best,
Dimitris

---- 
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting ucecgxu at ucl.ac.uk:

> Dear R-helpers:
> 
> thank your for your attention.
> 
> i am a newer to R and i am doing some protein category classification
> based on
> the amino acid sequence.while i have some question urgent.
> 
> 1. any packages for analysis amino acid sequence
> 2. given two sequences "AAA" and "BBB",how can i combine them into
> "AAABBB"
> 
> 3. based on "AAABBB",how can i get some statistics of this string
> such as how
> many letters,how many "A"s in the string.
> 
> 
> Thank you very much and i am looking forward to hearing from you
> soon
> 
> have a nice day!
> 
> Best Regards
> 
> Marshall
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From erich.neuwirth at univie.ac.at  Sun Jun 18 13:30:08 2006
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Sun, 18 Jun 2006 13:30:08 +0200
Subject: [R] Standard Deviation Distribution
In-Reply-To: <44952484.4010200@univie.ac.at>
References: <F9F2A641C593D7408925574C05A7BE7704851A@rhopost.rhotrading.com>
	<44950B74.4020407@stats.uwo.ca> <44952484.4010200@univie.ac.at>
Message-ID: <44953940.4020300@univie.ac.at>

davidr at rhotrading.com wrote:
> > sddist
>>> > >>
> > function(s,n) {
> > sig2 <- n*s*s/(n-1)
> > 2*(n/(2*sig2))^((n-1)/2) / gamma((n-1)/2) * exp(-n*s*s/(2*sig2)) *
s^(n-2)
> > }


There is another way using more statistical knowledge:
Building on functions already available in R
one may note that the easiest way of defining
sddist is

sddist<-function(s,n) dchisq(n*s^2,n-1)*2*n*s

Plotting this function for n in seq(2,12,2)
reproduces the graph in Mathworld.

The idea is the following:
given a random variable x with a chisquare distribution with
df n-1, s can be defined by s=sqrt(x/n) and therefore
x=n*s^2

If F and G are the cumulative distribution functions of x and s
and f and g are the density functions of x and s, then we have
G(s)=F(n*s^2) and therefore
g(s)=f(n*s^2)*2*n*s
-- 
Erich Neuwirth, University of Vienna
Faculty of Computer Science
Computer Supported Didactics Working Group
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-39464 Fax: +43-1-4277-39459


From pburns at pburns.seanet.com  Sun Jun 18 13:48:11 2006
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Sun, 18 Jun 2006 12:48:11 +0100
Subject: [R] R Community mailing list
Message-ID: <44953D7B.6010608@pburns.seanet.com>

Several weeks ago the idea of an 'R Community'
mailing list was discussed.  I was impatiently awaiting
the announcement for it, so asked Martin Maechler
about it yesterday.  I misinterpreted his message on
the subject, so perhaps others did as well.

Martin is seeking a couple of volunteers to whom he
can hand the keys of the mailing list.

Besides doing Rolf's homework for him, a couple other
ideas for the list were expressed.

The first was that it could be used as a meta-mailinglist
for dealing with such issues as what to do about people
who are annoying on the lists (at this point in the conversation
glances were cast in my direction -- I'm not sure what that
means).

The second idea was that it could decentralize tasks.  This
is otherwise known as getting things done without bugging
the hell out of R Core.  For example the birth of the R Wiki,
successful as it has been, would have been smoother with
such a list.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")


From bates at stat.wisc.edu  Sun Jun 18 13:58:51 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 18 Jun 2006 13:58:51 +0200
Subject: [R] lmer and mixed effects logistic regression
In-Reply-To: <449431F1.6000906@pdf.com>
References: <1150293574.3416.11.camel@localhost.localdomain>
	<449431F1.6000906@pdf.com>
Message-ID: <40e66e0b0606180458p177fa824j6a181fd52c3d1277@mail.gmail.com>

If I understand correctly Rick it trying to fit a model with random
effects on a binary response when there are either 1 or 2 observations
per group.  I think that is very optimistic because there is so little
information available per random effect (exactly 1 or 2 bits of
information per random effect). I'm not surprised that there are
difficulties in fitting such a model.

Rick should try to monitor the iterations to see what is happening to
the parameter estimates and perhaps should try the Laplace
approximation to the log-likelihood.  This is done by adding method =
"Laplace" and control = list(msVerbose = TRUE) to the call to lmer.
This causes the PQL iterations to be followed by direct optimization
of the Laplace approximation.  Because the problem is probably in the
PQL iterations Rick may want to turn those off and do direct
optimization of the Laplace approximation only.  In that case the
control argument should be list(usePQL=FALSE, msVerbose = TRUE)


On 6/17/06, Spencer Graves <spencer.graves at pdf.com> wrote:
>
>
> 'lmer' RETURNS AN ERROR WHEN SAS NLMIXED RETURNS AN ANSWER
>
>           Like you, I would expect lmer to return an answer when SAS
> NLMIXED does, and I'm concerned that it returns an error message instead.
>
>           Your example is not self contained, and I've been unable to
> get the result you got.  This could occur for several reasons.  First,
> what do you get for "sessionInfo()"?  I got the following:
>
> sessionInfo()
> Version 2.3.1 (2006-06-01)
> i386-pc-mingw32
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
>
> other attached packages:
>        lme4    lattice     Matrix
>   "0.995-2"   "0.13-8" "0.995-10"
>
>           If you are using different versions of lmer, lattice and / or
> Matrix, please update.packages and try again.  If that does not fix the
> problem, could you please try to find a self-contained example that is
> as simple as possible and still generates the same error message?  Then
> send that to this list.
>
>           When I ran your example limited only to the observations you
> listed, I got a different result:
>
> Warning messages:
> 1: fitted probabilities numerically 0 or 1 occurred in: glm.fit(X, Y,
> weights = weights, offset = offset, family = family,
> 2: IRLS iterations for PQL did not converge
>
>           We get this these "Warning messages" because all the values
> of "response" are constant within each level of "id".  With data like
> this, the likelihood will be maximized by sending Std.Dev.(id) to Inf;
> when 'lmer' quit, Std.Dev.(id) was already at 58779, which
> computationally is moderately close to Inf for the current "lmer"
> algorithm.
>
>           How many different patterns of "response" by "id" do you have?
> With all 0's, the random adjustment to "(Intercept)" for that "id",
> ignoring the Bayesian shrinkage, is "-Inf".  With all 1's, this random
> adjustment would be "+Inf".  With two observations for a subject, if
> they are different, this random adjustment would exactly cancel the
> estimate of the fixed-effect for "(Intercept").  Do you have only these
> three patterns, all 0's, all 1's, and half 0's, half 1's?  If yes, that
> might explain why Std.Dev.(id) wants to go to Inf.
>
>           I don't know exactly how many different patterns you need, but
> you should be able to have some levels of "id" with all 0's or all 1's
> as long as those did not dominate, as they do in this cut down example.
>
>           If this is the problem, then SAS NLMIXED could be achieving false
> convergence and either not telling you about it or reporting it so
> quietly you have not reported it here.
>
>
> CREATING A REPRODUCIBLE EXAMPLE
>
>           I'm not eager to experiment with a large complicated example.
> You could help us help you by trying to produce a simpler,
> self-contained example.  For example, do you get the same answer when
> you delete 'age' from the model:
>
>           lmer(response~(1|id),data=gdx,family=binomial)
>
>           If yes, then you could easily just count the number of levels of "id"
> that have all 0's, all 1's, (0, 1) = (1, 0), etc.  Then write one or two
> lines that would generate that special case and submit that to this
> listserve.  Before you do, however, I encourage you to experiment to
> find small numbers of replicates that reproduce the error you get.  Then
> submit that to this list.
>
> STARTING VALUES?
>
>           The help page for "lmer" describes the following argument:
>
>     start: a list of relative precision matrices for the random effects.
>             This has the same form as the slot '"Omega"' in a fitted
>            model.  Only the upper triangle of these symmetric matrices
>            should be stored.
>
>           To understand this, it might help to experiment with one of
> the examples on this help page:
>
> > fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
> > fm1 at Omega
> $Subject
> 2 x 2 Matrix of class "dpoMatrix"
>              (Intercept)       Days
> (Intercept)   1.0746247 -0.2942832
> Days         -0.2942832 18.7549595
>
>           Just taking a guess, I tried the following:
>
> fm1. <- lmer(Reaction ~ Days+(Days|Subject), sleepstudy,
>               start=list(Subject=diag(1:2)))
>
> fm1.c <- lmer(Reaction ~ Days+(Days|Subject), sleepstudy,
>               start=list(Subject=array(1, .1, .1, 2),
>                       dim=c(2,2)))
>
>            Both of these returned the same answer.  Starting values are
> only required for the random effects, because these models are all
> "plinear" in the sense described in the "nls" documentation.
>
>           For "glmmPQL", I was able to get the "update" function to work.
> Thus, if you can get an answer with one model, you can use that as an
> input to "update".  I would expect "update" to try to use the parameter
> estimates from one model fit as starting values for the modification,
> where it can find a way to do that.
>
>           Hope this helps.
>           Spencer Graves
>
> Rick Bilonick wrote:
> > I'm using FC4 and R 2.3.1 to fit a mixed effects logistic regression.
> > The response is 0/1 and both the response and the age are the same for
> > each pair of observations for each subject (some observations are not
> > paired). For example:
> >
> > id response age
> > 1    0      30
> > 1    0      30
> >
> > 2    1      55
> > 2    1      55
> >
> > 3    0      37
> >
> > 4    1      52
> >
> > 5    0      39
> > 5    0      39
> >
> > etc.
> >
> > I get the following error:
> >
> >> (lmer(response~(1|id)+age,data=gdx,family=binomial))
> > Error in deviance(.Call("mer_coefGets", x, pars, 2, PACKAGE =
> > "Matrix")) :
> >         Leading minor of order 2 in downdated X'X is not positive
> > definite
> >
> > Similar problem if I use quasibinomial.
> >
> > If I use glm, of course it thinks I have roughly twice the number of
> > subjects so the standard errors would be smaller than they should be.
> >
> > I used SAS's NLMIXED and it converged without problems giving me
> > parameter estimates close to what glm gives, but with larger standard
> > errors. glmmPQL(MASS) gives very different parameter estimates.
> >
> > Is it reasonable to fit a mixed effects model in this situation?
> >
> > Is there some way to give starting values for lmer and/or glmmPQL?
> >
> > Rick B.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>


From bates at stat.wisc.edu  Sun Jun 18 14:15:08 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 18 Jun 2006 14:15:08 +0200
Subject: [R] number of iteration s exceeded maximum of 50
In-Reply-To: <200606161041072848729@yahoo.ca>
References: <1bff52c20606140033j3ac8e9fdxa85d4edded2ad94a@mail.gmail.com>
	<1bff52c20606140101r25188de1g6573c52f3cb5d152@mail.gmail.com>
	<38b9f0350606140223g507112dau3478586790daad3d@mail.gmail.com>
	<200606140735459773486@yahoo.ca>
	<449285BD.3020101@statistik.uni-dortmund.de>
	<200606161041072848729@yahoo.ca>
Message-ID: <40e66e0b0606180515t759f1400od071251942d8dac6@mail.gmail.com>

On 6/16/06, Leaf Sun <leaflovesun at yahoo.ca> wrote:
> Sorry, I thought it was a straightforward question inside which I was stuck .
>
> I used nls( ) to estimate a and b in this function.
>
> nls(y~ a*x^b,start=list(a=a1,b=b1)
>
> seems the start list I gave was not able to reach convergence and it gave notes: number of iteration s exceeded maximum of 50. Then I put  nls.control(maxiter = 50, tol = 1e-05, minFactor = 1/1024) in nls(.. ), and modified the argument of maxiter = 500. But it worked out as the same way and noted : number of iteration s exceeded maximum of 50. I have totally no idea how to set this parameter MAXITER.
>
> Thanks for any information!

I think you are assuming that values passed to nls.control are
persistent and will apply to further calls to nls.  They don't.  If
you want to increase the maximum number of iterations you do it as

nls(y ~ a*x^b, start = list(a = a1, b = b1), control = list(maxiter = 500))

but I would suggest that you also use trace = TRUE in the call to nls
so you can see where the iterations are going.  Merely increasing the
number of iterations for an optimization that has gone into
never-never land isn't going to help it converge.

Two other things to consider: this is a partially linear model in the
the parameter `a' appears linearly in the model expression.  You may
be able to stabilize the iterations using

 nls(y ~ x^b, start = list(b = b1), control = list(maxiter = 500),
trace = TRUE, alg = 'plinear')

Finally, and most important, please plot the data before trying to fit
a nonlinear model to it so you can see if it has the characteristics
that you would expect from data generate by such a model.  As Brian
Joiner said, "Regression without plots is truly a regression".
>
> Leaf
>
>
> >  Hi  all,
> >
> >  I  found  r-site-research  not  work  for  me  these  days.
> >
> >  When  I  was  doing  nls(  )  ,  there  was  an  error  "number  of  iterations  exceeded  maximum  of  50".  I  set  number  in  nls.control  which  is  supposed  to  control  the  number  of  iterations  but  it  didn't  work  well.  Could  anybody  with  this  experience  tell  me  how  to  fix  it?  Thanks  in  advance!
>
> We  cannot  make  suggestions  unless  you  tell  us  what  you  tried  yourself.
> Id  possible,  please  gib?ve  a  reproducible  examle.
>
> Uwe  Ligges
>
> >  Leaf
> >
> >   [[alternative  HTML  version  deleted]]
> >
> >  ______________________________________________
> >  R-help at stat.math.ethz.ch  mailing  list
> >  https://stat.ethz.ch/mailman/listinfo/r-help
> >  PLEASE  do  read  the  posting  guide!  http://www.R-project.org/posting-guide.html
>
>         [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>


From phgrosjean at sciviews.org  Sun Jun 18 15:03:17 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Sun, 18 Jun 2006 15:03:17 +0200
Subject: [R] R Wiki - official launch!
Message-ID: <44954F15.4010500@sciviews.org>

Hello all,

We are pleased to announce the official launch of the R Wiki at 
http://wiki.r-project.org. Although there are already many sources of R 
documentation, this R Wiki is a complementary tool, in the sense that 
"users become R documentation authors", a little bit like "users become 
developers" for R code.

=====================================================================
The main sections are:

- Getting Started: dedicated to very R beginners,

- Guides: a section for books, tutorials and demos in Wiki format,

- Tips: this is a large section with many small tips&tricks, initially 
based on the excellent Paul Johnson's Rtips 
(http://pj.freefaculty.org/R/Rtips.html),

- Links: users can contribute links about R here,

- R Graph Gallery: an addition to discuss graphs in the Romain Fran?ois' 
R Graph Gallery site (http://addictedtor.free.fr/graphiques/index.php),

- R packages: packages authors/maintainers are free to add related 
material; R package users can also contribute package-specific material 
here.

- R documentation: the wiki version of all R documentation ("wikified" 
Rd files), with the possibility for everybody to comment, add examples 
or anything else useful. Note that there are still some little 
formatting and navigation problems here, but they will be solved soon 
(do not report bugs yet, please!)

- Users: sections where R users can create their own public page. It is 
also the place where everybody can comment on R events, like useR!2006 
(see http://wiki.r-project.org/rwiki/doku.php?id=users:user-2006)

- Wiki and playground: sections to learn how to write wiki pages and to 
exercise.

========================================================================
As a R documentation reader, you could be interested by the additional 
information in the R Wiki. To be informed of new items added to the R 
Wiki, you have RSS feed available (see 
http://wiki.r-project.org/rwiki/doku.php?id=wiki:usage)

If you have something interesting to share with other R users, this is 
the place to publish it. Here, you will receive the support of other 
people to enhance and keep your document up-to-date with the rapidly 
evolving R. Here are a couple of suggested uses:

1) If you feel there is an interesting thread on this R-Help mailing 
list, summarize and illustrate it in the right subsection of 'Tips'. 
That way, one could answer "Please, visit Wiki page XXXX" for further 
similar questions on the mailing list. The rich-formatted presentation 
of a R Wiki page is much easier to read that a mailing list thread in 
the archives,

2) Contribute to the official R documentation by adding material at the 
end of the wikified man pages in the "R documentation" section. Most 
valuable addition will eventually flow into to official documentation in 
subsequent R versions.

3) As a package author/maintainer, allow users to contribute various 
material (examples, comments, tutorials, ...) by creating a Wiki page 
dedicated to your package, and by adding its link in the 'URL' section 
of your description file.

4) Add a link to your own R-related web site in the 'Links' section.

5) Consider to publish your R tutorial / demo / course / book in the 
"Guides" section. The main advantage to publish it on the Wiki is the 
possibility to get help from your readers to keep this material updated 
(most of such contributed documents, including those on CRAN, are *not* 
regularly updated, if published elsewhere and if you do not revise them 
yourself regularly, that is, every six months!)

6) Start a page in the 'Users' section to share material under development.

7) As a R beginner, share your experience with other beginners in the 
'Getting started' section.

8) For a little bit more fun, participate, or propose a new challenge. 
A challenge consists in solving a problem with optimized R code 
(optimization for speed is easy to quantify, but do not forgot to 
improve readability and style of your code!). We would like to advertise 
R Wiki challenges from time to time on this mailing list. No prize 
here,... just the satisfaction to have written the better code to solve 
a given problem.

There are numerous other ways to use the R Wiki. Browse it to discover 
how useful it can be for you.

There is also a mailing list dedicated to R Wiki developments: 
R-SIG-WIKI (https://stat.ethz.ch/mailman/listinfo/r-sig-wiki).

On behalf of the R Wiki creators,

Philippe Grosjean
-- 
..............................................<?}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Mons, Belgium
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................


From bren at juanantonio.info  Sun Jun 18 15:09:34 2006
From: bren at juanantonio.info (bren at juanantonio.info)
Date: Sun, 18 Jun 2006 15:09:34 +0200
Subject: [R] Yahoo data download problem
In-Reply-To: <mailman.9.1150538404.26844.r-help@stat.math.ethz.ch>
References: <mailman.9.1150538404.26844.r-help@stat.math.ethz.ch>
Message-ID: <20060618130934.13402.qmail@correo6.acens.net>

I have some function to solve your problem. 


#Plot stock quote
PSQ <- function(INSTRUMENT,FROM,TO,START){
   #Procedure to download Stock Quote Information
   DATA <-yahoo.get.hist.quote(
       instrument = INSTRUMENT,
       destfile = paste(INSTRUMENT, ".csv", sep = ""),
       start=FROM,
       end=TO,
       quote = c("Close"),
       adjusted = TRUE,
       download = TRUE,
       origin = "1970-01-01",
       compression = "d"); 

   #Conversion procedure to Time Series R Objetct;
   DATA.TS = ts(DATA,frequency = 365, start = c(START, 1))
   plot(DATA.TS);
   return(DATA.TS);
} 

#To compare
COMPARE_QUOTES <- function(QUOTES,FROM,TO,START){
   L = length(COMPANIES);
   par(mfrow=c(L,1));
   for  (i in 1:L){
       PSQ(QUOTES[i],FROM,TO,START);
   }
   par(mfrow=c(1,1));
} 

COMPANIES <-c("GE","MMM","IBM"); 

COMPARE_QUOTES(COMPANIES,"1990-01-01","2006-06-16",1990);
IBM <- PSQ("IBM","1990-01-01","2006-06-16",1990); 

> 
> Message: 1
> Date: Fri, 16 Jun 2006 12:10:53 +0200
> From: "Petr Pikal" <petr.pikal en precheza.cz>
> Subject: Re: [R] Yahoo data download problem
> To: SUMANTA BASAK <r_econometrics en yahoo.co.in>,	R HELP
> 	<r-help en stat.math.ethz.ch>
> Message-ID: <44929FCD.26976.F5A06E en localhost>
> Content-Type: text/plain; charset=US-ASCII 
> 
> Hi 
> 
> 
> On 16 Jun 2006 at 8:52, SUMANTA BASAK wrote: 
> 
> Date sent:      	Fri, 16 Jun 2006 08:52:22 +0100 (BST)
> From:           	SUMANTA BASAK <r_econometrics en yahoo.co.in>
> To:             	R HELP <r-help en stat.math.ethz.ch>,
> 	r-sig-finance-request en stat.math.ethz.ch
> Subject:        	[R] Yahoo data download problem 
> 
>> Hi all R-Experts, 
>> 
>> I'm facing one problem in yahoo data downloading. I'm suing Windows
>> XP, R 2.2.0, and i'm using yahoo.get.hist.quote function to download
>> data. I need 500 companies of S&P index daily 'closing price' data for
>> last ten years. My questions are: 
>> 
>> 1) I have all the ticker names of S&P 500 companies in a .csv format.
>> I'm reading those names in R and they are coming as data.frame object.
>> How can i change this to a vector?
> 
> df<-data.frame(x=sample(letters,10))  
> 
>  as.vector(df$x)
>  [1] "g" "b" "p" "u" "r" "q" "j" "h" "o" "k"
>>
> 
> HTH
> Petr 
> 
> 
>> 
>> 2) How can i get all companies data downloaded using a simple "for"
>> loop? 
>> 
>> I'm using the following function for a single stock data. 
>> 
>> 
>> library(gdata)
>> s<-yahoo.get.hist.quote(instrument = "mo", destfile = paste("mo",
>> ".csv", sep = ""), start="1996-01-01",
>>                      end="2006-06-16", quote = c("Close"), adjusted =
>>                      TRUE, download = TRUE, origin = "1970-01-01",
>>                      compression = "d") 
>> 
>> 
>> Thanks,
>> Sumanta Basak. 
>> 
>> 
>> --------------------------------- 
>> 
>> 
>>  [[alternative HTML version deleted]] 
>> 
>> ______________________________________________
>> R-help en stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
> 
> Petr Pikal
> petr.pikal en precheza.cz 
> 
 

Juan Antonio Bre?a Moral.
Advanced Marketing Ph.D. , URJC, Spain (Now)
Industrial Organisation Engineering, ICAI, Spain.
Technical Computer Programming Engineering, ICAI, Spain
Web: http://www.juanantonio.info
Mobile: +34 655970320


From scarrizo at cs.usyd.edu.au  Sun Jun 18 15:12:05 2006
From: scarrizo at cs.usyd.edu.au (scarrizo at cs.usyd.edu.au)
Date: Sun, 18 Jun 2006 23:12:05 +1000
Subject: [R] GAM selection error msgs (mgcv & gam packages)
Message-ID: <1150636325.449551252b44f@www-mail.usyd.edu.au>

Hi all,

My question concerns 2 error messages; one in the gam package and one in
the mgcv package (see below). I have read help files and Chambers and
Hastie book but am failing to understand how I can solve this problem.
Could you please tell me what I must adjust so that the command does not
generate error message?

I am trying to achieve model selection for a GAM which is required for
prediction purposes, thus my focus is on AIC. My data set has 3038 records
and 116 predictor variables and a binary response variable [0 or 1]. There
is no current understanding of the predictors' relationship to response so
I am relying on GAM for selection of appropriate predictors.

Thanks
Savrina

*mgcv package 1.3-12:

# I start with specifying the full model with 116 predictors including
isotropic smooth of 3D location variables (when I specify only the first
14 predictors I get no error message)
>
m0<-gam(label~s(x,y,z,k=50),s+(feature4)+s(feature5)+s(feature6)+...+s(feature116),data=k.data,
family=binomial)

Error in smooth.construct.tp.smooth.spec(object, data, knots):
     A term has fewer unique covariate combinations than specified maximum
degrees of freedom

# I was going to follow this with backwards selection by hypothesis testing
(remove highest p-val term one at a time) and also AIC comparison of all
the models

>From help file entitled 'Generalised additive models with integrated
smoothness estimation' I calculated the following where do I go from here?
A) "k is the basis dimension of a given term...if k is not specified
k=10*3^(d-1) where 'd' is the number of covariates for this term"
My calculations: for all my terms but the first d=1 thus k=10*3^0=10.
B) "You must have more unique combinations of covariates than the model has
total parameters"
My calculations: total parameters = sum of basis dimensions(50+10*113) +
sum of non-spline terms(0) - number of spline terms(114) = 1066

*gam package:
I think stepwise selection provided by gam package would be useful in
finding the best predictive model. I follow example on pg 283 from
'Statistical models in S' Chambers and Hastie 1993.
# I start with a full model where all predictors enter linearly
> k.start<-gam(label~., data=k.data, family=binomial)

# set up scope list with possibilities for each term eg .~1 + x + s(x)
# ignore the first column of the data set
> k.scope<-gam.scope(k.data[,-1])

# start step wise selection
> k.step<-step(k.start,k.scope)
#condensed output
Start: AIC=1549.48
 label~s+y+z+feature4+feature5+...+feature116
                Df     Deviance       AIC
<none>                 1319.5         1549.5
- feature54     -1     1319.2         1551.2
- feature26     -1     1319.2         1551.2
...
-feature12      -1     1357.4         1589.4
There were 50 or more warnings (use warnings() to see the first 50)

# all 50 warnings are the same
> warnings()
Warning messages:
1: fitted probabilities numerically 0 or 1 occurred in: glm.fit(x[, jj,
drop = FALSE], y, wt, offset = object$offset,   ...

# it seems to not get passed the orginal linear model. It should show all
the steps taken to the final model
> k.step$anova
  Step Df Deviance Resid. Df Resid. Dev      AIC
1      NA       NA      2922   1317.599 1549.599


From ronggui.huang at gmail.com  Sun Jun 18 16:22:51 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Sun, 18 Jun 2006 22:22:51 +0800
Subject: [R] Method for selection bias with multinomial treatment
Message-ID: <38b9f0350606180722s73d5a11fre705f66861656b20@mail.gmail.com>

I have to the treatment effect base on the observational data.And the
treatment variable is multinomial rather than  binary.Because the
treatment assignment is not random,so the selection-bias exists.Under
this condition,what's the best way to estimate the treatment effect?

I know that if the treatment is binary,I can use propensity score
matching using MatchIt package.But what about multinomial?

Maybe one way is the method  proposed by Imbens,2000,The role of the
propensity score in estimating dose-response
functions,Biometrika,87:706-710.But I can't find any R function to
do the task.So I hope the lister can give me some suggestions.

Thank you in advance!

-- 
??????
Department of Sociology
Fudan University


From phiberoptic_br at yahoo.com.br  Sun Jun 18 17:27:43 2006
From: phiberoptic_br at yahoo.com.br (Anderson de Rezende Rocha)
Date: Sun, 18 Jun 2006 12:27:43 -0300 (ART)
Subject: [R] Books
Message-ID: <20060618152743.49195.qmail@web60420.mail.yahoo.com>

Dears, 

I saw in the R-project site some R-books. However, I'm new in this
community and I didn't figure out what are the best books. 

Can you suggest me some "reference" books? My intentions with R is
concerned to Artificial Intelligence simulations, Classification and
general Statistics (e.g., regression, multivariate regression, etc). 

Thanks for any help, Anderson


From ripley at stats.ox.ac.uk  Sun Jun 18 17:33:01 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 18 Jun 2006 16:33:01 +0100 (BST)
Subject: [R] A question about stepwise procedures: step function
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC047E40ED@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC047E40ED@DJFPOST01.djf.agrsci.dk>
Message-ID: <Pine.LNX.4.64.0606181628420.8963@gannet.stats.ox.ac.uk>

On Fri, 16 Jun 2006, Frede Aakmann T?gersen wrote:

> Well Jia, you use 'all' as a name for your dataframe, but this is also a function, see ?all. If I try it with
>
> mydata <- data.frame(z1,z2,z3)
>
> all goes well.

Exactly: 'step' is in the stats namespace and so object 'all' in the base 
namespace is preferred to the object you have in the user workspace.

You are advised not to use the names of base R objects for other purposes.
Use conflicts() to find out if you have done so.

(Here R could have figured out that a function was not appropriate, and it 
is smarter about that in pre-2.4.0.)


>
>
> Med venlig hilsen
> Frede Aakmann T?gersen
>
>
>
>
>> -----Oprindelig meddelelse-----
>> Fra: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] P? vegne af Li, Jia
>> Sendt: 15. juni 2006 17:45
>> Til: Hong Ooi; r-help at stat.math.ethz.ch
>> Emne: Re: [R] A question about stepwise procedures: step function
>>
>> Sorry, I still cannot find what's wrong with it. And it seems
>> that nothing is wrong with t.
>>
>> Jia
>>
>>> z1<-rnorm(N,0,1)
>>> z2<-rnorm(N,3,5)
>>> z3<-rbinom(N,1,0.6)
>>>
>>> prop.cens<-0.45
>>> cen<-rbinom(N,1,1-prop.cens)     #<-- censor indicator:45% censor in
>> the data.
>>> t<- rexp(N)
>>
>>> all<-data.frame(z1,z2,z3)
>>> fit.model.all<- coxph(Surv(t,cen) ~z1+z2+z3,data=all) fit.model.all
>> Call:
>> coxph(formula = Surv(t, cen) ~ z1 + z2 + z3, data = all)
>>
>>
>>         coef exp(coef) se(coef)       z    p
>> z1  0.057466     1.059   0.1377  0.4173 0.68
>> z2  0.000907     1.001   0.0332  0.0273 0.98
>> z3 -0.349273     0.705   0.2867 -1.2184 0.22
>>
>> Likelihood ratio test=1.81  on 3 df, p=0.613  n= 100
>>> reg.model.all<-step(fit.model.all)
>> Start:  AIC= 376.39
>>  Surv(t, cen) ~ z1 + z2 + z3
>>
>> Error in as.data.frame.default(data) : cannot coerce class "function"
>> into a data.frame
>>
>>
>>   _____
>>
>> From: Hong Ooi [mailto:Hong.Ooi at iag.com.au]
>> Sent: Thursday, June 15, 2006 10:53 AM
>> To: Li, Jia
>> Subject: RE: [R] A question about stepwise procedures: step function
>>
>>
>>
>> Note: This e-mail is subject to the disclaimer contained at the bottom
>> of this message.
>>
>>   _____
>>
>> t is the name of a function. If you have a variable called t in your
>> dataset, try renaming it.
>>
>>
>>   _____
>>
>> From: r-help-bounces at stat.math.ethz.ch on behalf of Li, Jia
>> Sent: Thu 15/06/2006 11:52 PM
>> To: r-help at stat.math.ethz.ch; ritwik.sinha at gmail.com
>> Subject: Re: [R] A question about stepwise procedures: step function
>>
>>
>>
>> Hi,
>>
>> Step works for a Cox model. And I got the same error massage using
>> stepAIC.
>>
>> Jia
>>
>>   _____
>>
>> From: Ritwik Sinha [mailto:ritwik.sinha at gmail.com]
>> Sent: Thursday, June 15, 2006 12:12 AM
>> To: Li, Jia
>> Subject: Re: [R] A question about stepwise procedures: step function
>>
>>
>> Hi,
>>
>> The step documentation says
>>
>> "object: an object representing a model of an appropriate class
>>           (mainly '"lm"' and '"glm"'). This is used as the initial
>>           model in the stepwise search.
>> "
>> I wonder if it will work for a cox proportional hazard model.
>> You could
>> try stepAIC in MASS.
>>
>>
>>
>>
>> On 6/14/06, Li, Jia < LI at nsabp.pitt.edu <mailto:LI at nsabp.pitt.edu> >
>> wrote:
>>
>>         Dear all,
>>
>>         I tried to use "step"  function to do model
>> selection, but I got
>> an error massage.  What I don't understand is that data as data.frame
>> worked well for my other programs, how come I cannot make it run this
>> time. Could you please tell me how I can fix it?
>>
>>
>> **************************************************************
>> **********
>> ***************************
>>
>>        >all<-data.frame(z1,z2,z3)
>>
>>        >fit.model.all<- coxph(Surv(t,cen) ~z1+z2+z3,data=all)
>>
>>        > reg.model.all<-step(fit.model.all)
>>         Start:  AIC= 689.1
>>         Surv(t, cen) ~ z1 + z2 + z3
>>         Error in as.data.frame.default(data) : cannot coerce class
>> "function" into a data.frame
>>
>> **************************************************************
>> **********
>> ***************************
>>         Thanks a lot!
>>
>>         Jia
>>
>>         ______________________________________________
>>         R-help at stat.math.ethz.ch mailing list
>>         https://stat.ethz.ch/mailman/listinfo/r-help
>>         PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>>
>>
>> --
>> Ritwik Sinha
>> Graduate Student
>> Epidemiology and Biostatistics
>> Case Western Reserve University
>>
>> http://darwin.cwru.edu/~rsinha
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>>   _____
>>
>> The information transmitted in this message and its
>> attachme...{{dropped}}
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Sun Jun 18 18:06:42 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 18 Jun 2006 17:06:42 +0100 (BST)
Subject: [R] R internal clock time discrepancy
Message-ID: <Pine.LNX.4.64.0606181702280.9740@gannet.stats.ox.ac.uk>

On Wed, 14 Jun 2006, toby marthews wrote:

> I'm trying to map some time series data on to dates and
> because I'm using R's "strptime' facility I get an hour's
> break in my time series: my readings for between 1am and
> 2am on 27th March 2005 won't map onto anything (I'm using a
> data logger to record temperature and other things in a
> forest).
> 
> As far as I know, the clock's didn't change on 27th March
> 2005 so why is this happening?

Because R is better informed: UK time changed to BST on 2005-03-27 01:00.
Here is what I get

[1] "2005-03-27 00:00:00 GMT" "2005-03-27 00:30:00 GMT"
[3] "2005-03-27 02:00:00 BST" "2005-03-27 02:30:00 BST"

You seems to be using a broken version of Windows.  Mine gives

> seq(from=start,to=end,by=30*60)
[1] "2005-03-27 00:00:00 GMT Standard Time"
[2] "2005-03-27 00:30:00 GMT Standard Time"
[3] "2005-03-27 02:00:00 GMT Daylight Time"
[4] "2005-03-27 02:30:00 GMT Daylight Time"
[5] "2005-03-27 03:00:00 GMT Daylight Time"
[6] "2005-03-27 03:30:00 GMT Daylight Time"

and the difference is in how an illegal time (end) was interpreted.

>> start=strptime("2005-03-27 00:00","%Y-%m-%d %H:%M")
>> end=strptime("2005-03-27 02:30","%Y-%m-%d %H:%M")
>> seq(from=start,to=end,by=30*60)
> [1] "2005-03-27 00:00:00 GMT Standard Time" "2005-03-27
> 00:30:00 GMT Standard Time" "2005-03-27 02:00:00 GMT
> Standard Time"
> [4] "2005-03-27 02:30:00 GMT Standard Time"

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From spencer.graves at pdf.com  Sun Jun 18 18:34:17 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 18 Jun 2006 09:34:17 -0700
Subject: [R] Garch warning
In-Reply-To: <d4c57560606160754g10d19acbw8275c76fdf246c23@mail.gmail.com>
References: <d4c57560606160754g10d19acbw8275c76fdf246c23@mail.gmail.com>
Message-ID: <44958089.4030002@pdf.com>

KURT:  In case you have time to look briefly at the 'garch' code, you 
might be interested in this example.  This email includes a 
self-contained example that generates a warning that seems to me 
unnecessary:  "Warning in sqrt(pred$e) : NaNs produced".  This occurs 
because pred$e[1] = -0.121.  This NaN is immediately replaced by NA in 
the line after this 'sqrt(pred$e)'.  If the user should be concerned 
about this, I suggest you provide a more informative warning. 
Otherwise, I suggest you modify the code to eliminate this warning;  see 
below.

ARUN KUMAR SAHA:

	  Thanks for the reproducible example.  In addition to the warning you 
mention, I also got "FALSE CONVERGENCE".  To explore this issue, I 
looked at the trace output;  the following shows the first and the last 
two rows:

IT  NF      F        RELDF   PRELDF   RELDX  STPPAR  D*STEP   NPRELDF

  0   1 -0.284E+04
  1   7 -0.284E+04  0.16E-02 0.23E-02 0.3E-03 0.9E+10 0.3E-04  0.98E+07
...
54 131 -0.293E+04  0.39E-14 0.40E-14 0.5E-13 0.2E+01 0.1E-12 -0.97E-02
55 133 -0.293E+04 -0.34E+07 0.81E-15 0.1E-13 0.2E+01 0.2E-13 -0.97E-02

  ***** FALSE CONVERGENCE *****

	  The huge negative final value for "RELDF" caught my eye.  I don't 
know what it means, but it seems strange to me, and could relate in some 
way to the "FALSE CONVERGENCE".

	  To investigate this, I tried, "str(garch1)".  This told me that 
'garch1' is a list with 10 components, which I also saw on the "garch" 
help page.  I didn't see anything unusual in summary(garch1) and 
plot(garch1).  Moreover, garch1$residuals has an NA for the first 
observation and no other NAs, which I confirmed using 
sum(is.na(garch1$residuals)).  However, I am NOT an expert on 'garch'; 
others might see things that I missed.

	  To explore this further, I set 'options(warn=2)';  this converts 
warnings to errors.  I got all the same printed output as before, except 
that 'garch' stopped on the 'warning' and failed to return anything. 
This told me that the warning is generated in the post-processing of the 
fit, preparing the list of 10 components that it normally returns.

	  Then I listed "garch", set "debug(garch)" and repeated the call to 
call to "garch" [after resetting options(warn=1)].  The .C calls to 
"fit_garch", "pred_garch", and "ophess_garch" all seemed to work 
normally.  I then found the following line:

     rank <- qr(com.hess$hess, ...)$rank

	  The proclaimed rank was 3, as it should be.  However, to explore this 
more, I tried the following:

eigen(com.hess$hess, symmetric=TRUE)
$values
[1] 4.804906e+11 9.671882e+03 5.837768e+02

$vectors
              [,1]          [,2]          [,3]
[1,] 1.000000e+00  0.0001342715 -4.869677e-06
[2,] 6.262745e-06 -0.0827853262 -9.965674e-01
[3,] 1.342137e-04 -0.9965673945  8.278533e-02

	  The ratio of the first to the last eigenevalues is almost 1e9. 
Conclusion:  This matrix is ill conditioned, consistent with the "FALSE 
CONVERGENCE" report.  When we round the near-singular eigenvector to 2 
significant digits, we get (0, -1, 0.08).  This suggests to me that the 
second parameter is poorly estimated.  However, when I look at 
summary(garch1), I see that the third, not the second parameter seems 
unnecessary:

     Estimate  Std. Error  t value Pr(>|t|)
a0 1.196e-04   1.996e-06    59.88   <2e-16 ***
a1 1.001e+00   4.125e-02    24.26   <2e-16 ***
b1 1.113e-14   1.070e-02 1.04e-12        1

	  The next step generated the warning:

 > sigt <- sqrt(pred$e)
Warning in sqrt(pred$e) : NaNs produced

	  This seems to be more or less anticipated, as the next line
replaces the NaN produced by NA:

     sigt[1:max(order[1], order[2])] <- rep(NA, max(order[1],
         order[2]))

	  I'm suggesting to the maintainer that he consider replacing these two 
steps by the following:

     e. <- pred$e
     e.[1:max(order[1], order[2])] <- rep(NA, max(order[1],
         order[2]))
     sigt <- sqrt(e.)
	
	  This looks to me like it does the same thing without generating this 
strange warning message.  If the warning is needed for some other 
purpose, then the code should be modified to more explicitly advice the 
user of the concern.

	  Next, I tried to refit the model without the third, allegedly 
insignificant parameter:

garch10 <- garch(dat, 1:0)
<snip>
Warning: singular information

	  Maybe the eigenvector analysis was correct, and I misunderstood the 
"summary".  This led me to try the following:

garch01 <- garch(dat, 0:1)

	  This did NOT produce a warning.

	  Next, I tried 'anova':

  anova(garch1, garch01)
Error in anova(garch1, garch01) : no applicable method for "anova"

	  This didn't work, but perhaps I can get the log(likelihoods):

 > logLik(garch1)
'log Lik.' 2222.459 (df=3)
 > logLik(garch01)
'log Lik.' 2222.481 (df=2)

	  Note that logLik for the second model is larger than the first;  this 
is a reflection of the "singular convergence".

	  Hope this helps.
	  Spencer Graves

Arun Kumar Saha wrote:
> Dear all R-users,
> 
> Previously I posted the same query in R-Help. But I haven't got the
> solution. So I am posting the same query again. It is my strong hope that I
> will get my solution this time.
> 
> I wanted to fit a Garch(1,1) model to the following data set by:
> 
>> garch1 = garch(dat)
> But I got a warning message while executing, which is:
> 
>> Warning message:
>> NaNs produced in: sqrt(pred$e)
> 
> dat
>    0.018078 -0.0017 0.019082 0.000448 0.006055 -0.02908 -0.0171 0.136383 -
> 0.01098 0.014523 -0.02504 -0.01261 -0.01301 -0.01937 0.031302 0.001924
> 0.016204 -0.01743 -0.00676 0.016391 0.037661 0.009682 -0.00311 0.034842
> 0.007326 0.004407 0.025833 0.013588 0.014807 0.030997 -0.02917 -0.01277 -
> 0.02489 0.003631 0.020853 0.001613 0.016834 0.144082 -0.01198 0.009475 -
> 0.00944 -0.00731 0.00854 -0.0211 -0.00037 -0.01354 -0.009 -0.00294 0.03639
> 0.005408 0.001024 -0.01789 -0.00682 -0.02709 0.015037 0.014098 -0.02083 -
> 0.00259 0.003773 -0.01239 0.002841 0.006787 -0.0069 -0.00729 0.003035
> 0.001797 0.000785 0.001065 0 -0.00062 -0.00022 0.000673 0.001064 0.000168 -
> 0.0019 0.000224 0.000224 0 -0.00022 0.000616 0 0.000448 0.000224 -0.00106
> 0.137144 0.00453 -0.00234 0.009654 0.017495 0.043445 0.036829 0.12604 -
> 0.06833 0.075942 -0.0157 0.012691 0.0006 0.011262 -0.01698 0.001169 0.035891
> 0.026974 -0.01448 -0.01285 0.010269 -0.00944 -0.00627 -0.02959 -0.00226 -
> 0.00658 -0.01566 0.002465 -0.00595 -0.00071 -0.02837 0.011752 0.000191 -
> 0.01585 0.003368 0.017396 -0.01288 -0.01559 -0.00301 0.009209 -0.00362 -
> 0.00477 -0.00588 0.011478 0.002748 0.01776 0.025986 -0.01573 -0.00452 -
> 0.01353 -0.01166 -0.01138 -0.00419 0.020279 -0.00736 -0.0178 0.017386 -
> 0.04783 -0.00973 -0.01211 -0.01552 0.020297 -0.00268 0.020165 -0.03628
> 0.004313 0.002682 0.009511 -0.0543 0.016064 0.020505 0.005516 0.02107
> 0.00809 0.021399 0.104238 -0.10466 0.025576 -0.00577 -0.01866 -0.00736 -
> 0.00443 0.004892 0.012944 0.015257 -0.00633 -0.01304 -0.0065 -0.00791 -
> 0.00226 0.033313 0.011476 -0.0111 -0.00365 0.033186 0.02298 -0.0037 0.002046
> -0.01238 0.008637 -0.00032 -0.00685 -0.01352 -0.00169 0.011631 0.00227
> 0.003654 0.007346 0.001533 0.005212 0.009414 0.0202 0.011882 -0.00218 -
> 0.00316 7.54E-05 -0.02038 0.011471 -0.00396 -0.00658 0.006229 0.002735 -
> 0.00633 0.001307 -0.00802 -0.0007 -0.01103 0.008764 -0.00781 0.01373 -
> 0.00125 -0.01414 0.008101 0.001142 -0.011 0.010643 0.003889 0.006915 -
> 0.00304 -0.00221 0.003028 -0.00171 -0.00144 -0.00396 0.001579 -0.00854 -
> 0.00202 0.00621 0.01185 -0.01605 -0.0063 0.003573 0.001958 0.004974 -0.00326
> 0.005464 0.003653 -0.00027 0.00781 -0.00609 0.007569 -0.00209 0.001707
> 0.007578 0.008708 -0.00078 0.070744 -0.08202 -0.00271 0.018286 0.003212
> 0.00232 0.000183 0.034699 0.011807 -0.00204 0.000616 3.42E-05 0.014975 -
> 0.00183 0.010891 0.010186 -0.00403 0.01614 0.002654 -0.0057 0.005801
> 0.034555 0.00405 0.010423 -0.00521 -0.00558 -0.00762 -0.00405 0.001195
> 0.002731 -0.01309 -0.00283 0.004433 -0.0118 -0.01089 0.004808 -0.00773
> 0.006685 -0.00064 0.004273 0.038674 -0.02693 -0.02044 0.021999 0.007472
> 0.011074 -0.01286 0.00217 -0.00481 0.000436 0.000871 0.003755 0.003 0.00697
> 0.006982 -0.00576 -0.00119 -0.00242 -0.00257 -0.00242 0 -0.00808 0.001454
> 0.000958 0.000124 -0.01118 0.000406 -0.00976 -0.00117 -0.01069 -0.0011 -
> 0.00156 0.004814 -0.00563 0.004265 0.009255 -0.01358 -0.00346 0.003791 -
> 0.00395 -0.00068 0.005023 -0.00586 -0.00207 0.00035 0.0073 0.00158 0.006326
> 0.004164 0.011432 -0.0017 -0.00139 -0.00372 0.01356 0.014993 -0.00602
> 0.011983 -0.00898 -0.00183 0.008998 -0.00252 -0.00448 0.006338 0.002094 -
> 0.00744 0.000873 0.029343 -0.00155 0.005344 0.007062 0.004139 2.85E-05
> 0.00325 -0.00445 0.005597 0.004748 0.003125 0.002022 0.003193 0.002039 -
> 0.00017 0.005511 -0.00264 0.001279 8.34E-05 -0.00636 0 -2.83E-05 0.002344
> 0.005121 0.009747 0.001056 -0.0049 -0.00137 -0.00029 -0.00249 0.002091
> 0.010335 -0.00105 0.001502 -0.00051 -0.00113 0 -0.00023 -0.00098 0 0.008371
> -0.01234 0.00058 0.002855 0.000115 0.007058 -0.00513 0.002124 -0.00169 -
> 0.00242 0.0021 -0.00164 -0.00154 -0.00889 -0.0047 -0.00306 -0.00979 -0.0031
> 0.001016 -0.00467 -0.00952 -0.00147 -0.00335 -0.00361 -0.02078 -0.00466
> 0.007358 0.012572 0.011018 -0.00369 0.004359 0.002006 -0.00955 0.002938 -
> 0.00314 0.001294 -0.00912 0.000123 -0.01086 9.36E-05 0.006065 -0.00311
> 0.004562 -0.0004 -0.00276 -0.01364 -0.0005 0.007126 0.004463 0.005373
> 0.000836 0.005802 0.001599 -0.01348 -0.01842 -0.0056 -0.00329 -0.00526
> 0.001253 -0.00813 0.004265 0.00694 0.005237 -0.00771 9.63E-05 -0.00019
> 0.005506 -0.00026 -0.00195 0.003003 0.001944 0.000699 0.000508 0.00073
> 0.000222 0.005727 0.001041 -0.00404 -0.01018 -0.00307 -0.00865 0.001065 -
> 0.00686 0.002142 0.000907 0.000162 0.003807 -0.00177 -0.00398 -0.00399 -
> 0.00117 0.002893 -0.00081 -0.00068 0.002409 -0.00639 -0.0062 -0.00617
> 0.005078 -0.00292 0.00226 -0.04239 0.003596 -0.00117 -0.00131 0.003765
> 0.000793 0.006592 -0.00123 0.001917 0.003005 0.001876 -0.0016 -0.00229 -
> 0.00347 0.002996 -0.00203 -0.00273 0.002313 -0.00431 0.000187 0.002663 -
> 0.0008 -0.00227 -0.00227 0.002352 -0.00614 -0.00791 0.003025 -0.00174
> 0.004621 0.004621 0.001327 -0.00105 -0.00344 0.003162 -0.00364 0.001658 -
> 0.00161 -0.00086 -0.00179 -0.00061 0.00453 -0.0006 -0.00529 -0.00458 -0.0019
> 0.001411 -0.00148 -0.00888 -0.00045 0.001676 -0.00194 0.002029 0.001394
> 0.001812 0.002699 -0.00207 0.005783 -0.00469 -0.00156 0.008546 -0.00837 -
> 0.01456 -0.02116 0.004523 -0.00673 -0.00626 -0.0145 -0.00313 -0.03585 -
> 0.02317 0.01289 -0.00122 -0.01519 -0.03454 0.008239 -0.00756 -0.00419
> 0.00629 -0.00236 -0.00498 0.004631 0.008615 0.01143 0.016818 -0.00412 -
> 0.00034 0.001046 -0.00774 -0.00239 -0.01782 -0.00428 0.005898 0.004978 -
> 0.00457 -0.00544 0.00923 -0.00402 -0.0076 0 0.003472 -0.00344 0.006295
> 0.006864 -0.00096 0.001192 -0.00498 0.001576 0.006863 0.004808 0.000976
> 0.00086 0.002033 0.002714 0.004112 0.008735 -0.00377 -0.0002 -0.01089 -
> 0.00402 0.003514 0.00171 -0.00351 -0.05464 0.003282 -0.00508 -0.00342 -
> 0.00253 0.006963 -0.00322 -0.00157 -0.00454 -0.00054 0.002806 -0.00057
> 0.002048 -0.00731 -0.0055 0.000986 -0.00087 -0.04326 0.04659 0.005094
> 0.004819 0.002076 -0.00465 -0.00352 -0.00294 -0.00307 0.00022 0.008688
> 0.001182 0.00087 0.00065 -0.00989 0.005141 -0.00655 -0.00632 -0.00965 -
> 0.00497 0.00045 0.004568 0.004253 0.016834 -0.01762 -0.00521 0.000328 -
> 0.00801 -0.00676 -0.00363 -0.00152 -0.00411 0.002059 -0.00764 -0.00746
> 0.001618 0.001951 0.004091 -0.00159 -0.00164 -0.0014 0.0044 -0.00114
> 0.002384 -0.00119 -0.00602 0.031675 0.006157 -0.00487 0.00327 0.006087
> 0.004132 0.001138 0.000126 0.013054 0.000779 -0.00075 -0.00842 -0.00612 -
> 0.0018 0.002125 -0.00157 0.003201 0.001785 -0.00291 -0.00028 -0.00676
> 0.000185 0.003607 0.001109 0.005279 0.000459 -0.0022 -0.00249 0.003486 -
> 0.00841 -0.00529 0.00237 0.003607 0.001737 0.004438 0.001153 -0.00184 -
> 0.00338 0.001567 -9.39E-05 0.002127 0.007596 -0.00028 -6.20E-05 -0.00165 -
> 0.00495 -0.00012 -0.00471 -0.00498 -0.00713 0.008507 0.001599 -9.40E-05
> 0.003224 0.002466 0.012423 -0.00262 0.001542 -0.00126 0.001264 -0.00361 -
> 0.00509 0.003538 -0.00419 -0.00022 0.002021 0.005543 -0.00649 0.004877 -
> 0.00034 0.000155 -0.00292 -0.00022 -0.00134 0.000249 -0.00193 0.000125 -
> 0.00225 -0.00718 -0.00949 0.01128 0.002425 -0.00035 -0.00397 -0.00662
> 0.003302 -0.00096 -0.00447 0.009146 0.0044
> Can anyone tell me where the problem is?
> 
> Thanks and regards
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From paul_h_artes at yahoo.co.uk  Sun Jun 18 21:25:03 2006
From: paul_h_artes at yahoo.co.uk (Paul Artes)
Date: Sun, 18 Jun 2006 12:25:03 -0700 (PDT)
Subject: [R] Books
In-Reply-To: <20060618152743.49195.qmail@web60420.mail.yahoo.com>
References: <20060618152743.49195.qmail@web60420.mail.yahoo.com>
Message-ID: <4926486.post@talk.nabble.com>


Dear Anderson,

I'd highly recommend "Modern Applied Statistics with S" (4th Ed., by WN
Venables & BD Ripley). This has lots and lots of material on the topics of
your interest. 

Best wishes

Paul
--
View this message in context: http://www.nabble.com/Books-t1807065.html#a4926486
Sent from the R help forum at Nabble.com.


From cbarker1 at scius.jnj.com  Sun Jun 18 23:22:25 2006
From: cbarker1 at scius.jnj.com (Barker, Chris [SCIUS])
Date: Sun, 18 Jun 2006 17:22:25 -0400
Subject: [R] Method for selection bias with multinomial treatment
Message-ID: <56484410CF26D747A645BE746C57CE6D021A8D24@SCIUSFREXS3.na.jnj.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060618/e98c8dd4/attachment.pl 

From e.rehak at t-online.de  Sun Jun 18 22:06:20 2006
From: e.rehak at t-online.de (Mark Hempelmann)
Date: Sun, 18 Jun 2006 22:06:20 +0200
Subject: [R] Post Stratification
Message-ID: <4495B23C.2080304@t-online.de>

Dear WizaRds,

	having met some of you in person in Vienna,  I think even more fondly 
of this community and hope to continue on this route. It was great 
talking with you and learning from you. Thank you. I am trying to work 
through an artificial example in post stratification. This is my dataset:

library(survey)
age <- data.frame(id=1:8, stratum=rep( c("S1","S2"),c(5,3)), 
weight=rep(c(3,4),c(5,3)), nh=rep(c(5,3),c(5,3)), 
Nh=rep(c(15,12),c(5,3)), y=c(23,25,27,21,22, 77,72,74) )

pop.types <- table(stratum=age$stratum)
age.post <- svydesign(ids=~1, strata=NULL, data=age, fpc=~Nh) ## no 
clusters, no strata

post <- postStratify(design=age.post, strata=~stratum, population=pop.types)

svymean  (~y, post)
svytotal (~y, post)

gives
     mean     SE
y 42.625 0.5467
   total     SE
y   341 4.3737

So, is it correct to define pop.types as the number of elements sampled 
per stratum (nh) or rather the total of elements per stratum (Nh)? If so:

pop.types <- data.frame(stratum = c("S1","S2"), Freq = c(15, 12))
The help says: The 'population' totals can be specified as a table with 
the strata variables in the margins, or as a data frame where one 
column lists frequencies and the other columns list the unique 
combinations of strata variables. ??

However, I compute:
Nh=c(15,12); nh=c(5,3); sh=by(age$y, age$stratum, var); N=sum(Nh)
# Mean estimator
y.bar=by(age$y, age$stratum, mean) ## 23.6; 74.33
estimator=1/N*sum(Nh*y.bar) ## 46.14815
# Variance estimator
vari=1/N^2*sum(Nh*(Nh-nh)*sh/nh)
sqrt(vari) ##	.7425903

and with Taylor expansion .7750118

Please help me correct my mistakes. Thank you so much.
Yours
mark


From henric.nilsson at statisticon.se  Sun Jun 18 23:47:45 2006
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Sun, 18 Jun 2006 23:47:45 +0200
Subject: [R] Getting forcasting equation from nnet results
In-Reply-To: <1150538324.4493d254a3283@webmail.technion.ac.il>
References: <1150538324.4493d254a3283@webmail.technion.ac.il>
Message-ID: <4495CA01.5020903@statisticon.se>

juliacel at techunix.technion.ac.il wrote:
> I'm trying to build forecasting equation from weights of 2-2-1 neural net.
> Running the nnet function gives me a vecto of 9 weights, but I don't know how
> to build the equation form these values.
> Can anyone advice? Or at least tell me where the nnet output is described in
> details (the manual only gives a brief description).

Please take a look at the help page of `nnet'. The two books listed 
under the `References:' section does an excellent job of describing 
`nnet' (and lots more).


HTH,
Henric



> 
> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From vincent.goulet at act.ulaval.ca  Mon Jun 19 00:20:35 2006
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Sun, 18 Jun 2006 18:20:35 -0400
Subject: [R] Fitting Distributions Directly From a Histogram
In-Reply-To: <a2b3004b0606120351yefb42bbjf1d1f05f541c29@mail.gmail.com>
References: <a2b3004b0606120351yefb42bbjf1d1f05f541c29@mail.gmail.com>
Message-ID: <200606181820.35874.vincent.goulet@act.ulaval.ca>

Le Lundi 12 Juin 2006 06:51, Lorenzo Isella a ?crit?:
> Dear All,
>
> A simple question: packages like fitdistr should be ideal to analyze
> samples of data taken from a univariate distribution, but what if
> rather than the raw data of the observations you are given directly
> and only a histogram?

Let's assume that you have not only the histogram itself, but also the breaks 
and the counts per bin. Then you have what grouped data --- at least that's 
how we call those in Actuarial Science. Maximum likelyhood estimation is 
feasible for such data, but it is slightly more complicated. "Loss Models" by 
Klugman, Panjer & Willmot (Wiley) covers this.

I'm now thinking of adding this to my actuarial science package "actuar"...

> I was thinking about generating artificially a set of data
> corresponding to the counts binned in the histogram, but this sounds
> too cumbersome.
> Another question is the following: fitdistr provides the value of the
> log-likely hood function, but what if I want e.g. a chi square test to
> get some insight on the goodness of the fitting?

Goodness of fit tests for grouped data are also covered in Loss Models.

> I am sure there must be a way to get it straightforwardly without
> coding it myself.

Once you have the theory, I'm afraid for now you will have to code the 
estimation procedure yourself.

Cheers.

> Many thanks
>
> Lorenzo
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
  Vincent Goulet, Professeur agr?g?
  ?cole d'actuariat
  Universit? Laval, Qu?bec 
  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca


From markleeds at verizon.net  Mon Jun 19 00:47:39 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Sun, 18 Jun 2006 17:47:39 -0500 (CDT)
Subject: [R] more  of a statistics/display question
Message-ID: <24551016.1285231150670859681.JavaMail.root@vms070.mailsrvcs.net>

this is not really an R question but I imagine I would
use R to do it.

I have a loop that has thousands of iterations and
after each iteration the results are a

box leung pvalue
a regression t-stat
and a profit number 

so, i have 1000's  of these sets of the 3 numbers above.
obviously, the p-value varies from zero to 1.
the t-statistic goes from about -3  to 3
and the profit numbers goes from around -30 to +30.

i would like to see how the profit number varies with
the box-leung palue and the regression t-stat but
as a combination rather than separately because
a high t-stat isn't necessarily good if i have
a low box leung p-value or vice versa.

does any know of a good way to visualize this and
an R-tool that would help me. Definitely, if someone
tells me the tool, I would read about but this
sounds like I need a 2 dimensional histogram sort of ?

it's not a problem for me to save these numbers in a 
matrix or asci file or whatever but any idea or tools or book
on how to group them into different groupings would be helpful. thanks                              

                                     mark


From markleeds at verizon.net  Mon Jun 19 00:56:35 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Sun, 18 Jun 2006 17:56:35 -0500 (CDT)
Subject: [R] =?iso-8859-1?q?more_=A0of_a_statistics/display_question?=
Message-ID: <22791288.1286601150671395616.JavaMail.root@vms070.mailsrvcs.net>

>From: markleeds at verizon.net
>Date: Sun Jun 18 17:47:39 CDT 2006
>To: r-help at stat.math.ethz.ch
>Subject: [R] more  of a statistics/display question

one more piece of info in regard to below :
it's okay if i have turn the profit number
into a 0/1 indicator variable with 0 indicating
negative and 1 indicating positive, if
that makes things easier. thanks.





>this is not really an R question but I imagine I would
>use R to do it.
>
>I have a loop that has thousands of iterations and
>after each iteration the results are a
>
>box leung pvalue
>a regression t-stat
>and a profit number 
>
>so, i have 1000's  of these sets of the 3 numbers above.
>obviously, the p-value varies from zero to 1.
>the t-statistic goes from about -3  to 3
>and the profit numbers goes from around -30 to +30.
>
>i would like to see how the profit number varies with
>the box-leung palue and the regression t-stat but
>as a combination rather than separately because
>a high t-stat isn't necessarily good if i have
>a low box leung p-value or vice versa.
>
>does any know of a good way to visualize this and
>an R-tool that would help me. Definitely, if someone
>tells me the tool, I would read about but this
>sounds like I need a 2 dimensional histogram sort of ?
>
>it's not a problem for me to save these numbers in a 
>matrix or asci file or whatever but any idea or tools or book
>on how to group them into different groupings would be helpful. thanks                              
>
>                                     mark
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From jwd at surewest.net  Mon Jun 19 01:17:30 2006
From: jwd at surewest.net (J Dougherty)
Date: Sun, 18 Jun 2006 16:17:30 -0700
Subject: [R] Books
In-Reply-To: <20060618152743.49195.qmail@web60420.mail.yahoo.com>
References: <20060618152743.49195.qmail@web60420.mail.yahoo.com>
Message-ID: <200606181617.30657.jwd@surewest.net>

On Sunday 18 June 2006 08:27, Anderson de Rezende Rocha wrote:
> Dears,
>
> I saw in the R-project site some R-books. However, I'm new in this
> community and I didn't figure out what are the best books.
>
> Can you suggest me some "reference" books? My intentions with R is
> concerned to Artificial Intelligence simulations, Classification and
> general Statistics (e.g., regression, multivariate regression, etc).
>

Four books:

- Introductory Statistics with R by Peter Daalgard
- Modern Applied Statistics with S by Venables and Ripley
- Data Analysis and Graphics Using R by Maindonald and Braun
another very useful introductory book is
- Statistics, an Introduction using R by Crawley

You may also want to track down a hard copy of the R manuals.  It comes free 
along with R, but the pdf is almost 1400 pages long, so it can be worthwhile 
having someone else do the printing and binding.

JDougherty


From spencer.graves at pdf.com  Mon Jun 19 02:45:56 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 18 Jun 2006 17:45:56 -0700
Subject: [R] Fitting Distributions Directly From a Histogram
In-Reply-To: <200606181820.35874.vincent.goulet@act.ulaval.ca>
References: <a2b3004b0606120351yefb42bbjf1d1f05f541c29@mail.gmail.com>
	<200606181820.35874.vincent.goulet@act.ulaval.ca>
Message-ID: <4495F3C4.7090608@pdf.com>

	  Won't 'interval censoring using 'survreg' in the 'survival' package 
handle this? 
(http://finzi.psych.upenn.edu/R/library/survival/html/survreg.html)

	  Hope this helps.
	  Spencer Graves

Vincent Goulet wrote:
> Le Lundi 12 Juin 2006 06:51, Lorenzo Isella a ?crit :
>> Dear All,
>>
>> A simple question: packages like fitdistr should be ideal to analyze
>> samples of data taken from a univariate distribution, but what if
>> rather than the raw data of the observations you are given directly
>> and only a histogram?
> 
> Let's assume that you have not only the histogram itself, but also the breaks 
> and the counts per bin. Then you have what grouped data --- at least that's 
> how we call those in Actuarial Science. Maximum likelyhood estimation is 
> feasible for such data, but it is slightly more complicated. "Loss Models" by 
> Klugman, Panjer & Willmot (Wiley) covers this.
> 
> I'm now thinking of adding this to my actuarial science package "actuar"...
> 
>> I was thinking about generating artificially a set of data
>> corresponding to the counts binned in the histogram, but this sounds
>> too cumbersome.
>> Another question is the following: fitdistr provides the value of the
>> log-likely hood function, but what if I want e.g. a chi square test to
>> get some insight on the goodness of the fitting?
> 
> Goodness of fit tests for grouped data are also covered in Loss Models.
> 
>> I am sure there must be a way to get it straightforwardly without
>> coding it myself.
> 
> Once you have the theory, I'm afraid for now you will have to code the 
> estimation procedure yourself.
> 
> Cheers.
> 
>> Many thanks
>>
>> Lorenzo
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>


From anil_rohilla at rediffmail.com  Mon Jun 19 06:57:10 2006
From: anil_rohilla at rediffmail.com (anil kumar rohilla)
Date: 19 Jun 2006 04:57:10 -0000
Subject: [R] Problem in Projection pursuit regression .
Message-ID: <20060619045710.1142.qmail@webmail71.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060619/7082b35b/attachment.pl 

From alex_restrepo at hotmail.com  Mon Jun 19 07:03:48 2006
From: alex_restrepo at hotmail.com (Alex Restrepo)
Date: Mon, 19 Jun 2006 00:03:48 -0500
Subject: [R] Custom Command to Generate SQL
Message-ID: <BAY106-F137642F2259FC24DA43F5098860@phx.gbl>

Hi:

I would like to create a custom command in R which generates SQL, which is 
then processed via RODBC.

For example, the user would type:

        retrieve firstName('JOHN') middlleName('WILLIAMS') lastName('FORD')

This would generate the following SQL which would then be processed by 
RODBC:

Select *
  from people
  where firstName = 'JOHN' and
            middleName = 'WILLIAMS' and
            lastName     = 'FORD'

Does anyone have a recommendation?  Any ideas would be greatly appreciated.

Alex


From palacios at math.ucalgary.ca  Mon Jun 19 07:12:25 2006
From: palacios at math.ucalgary.ca (Luz Maria Palacios Derflingher)
Date: Sun, 18 Jun 2006 23:12:25 -0600 (MDT)
Subject: [R] help on nlm (gradient)
Message-ID: <Pine.GSO.4.61.0606182310540.21746@ms1.math.ucalgary.ca>

Hello.

I am having some trouble using nlm in R for windows version 2.2.1. I have 
to minimise a function which includes in its insides the evaluation of 
integrals through the "adapt" function.
In order to make R work less, I calculated the gradient and coded it. I 
made sure that the code and function for the gradient worked.
So I know that the main function (the one to minimise,let's call it 
"func1") yields values when evaluated.
I know that the gradient I coded also works and yields values when 
evaluated (on its own); when I put it all together and use nlm I get the 
following message:

Error in nlm(func1, starting values,  :
         probable coding error in analytic gradient

Any light that can be shed on this would be highly appreciated.
Many thanks

Luz Palacios
PhD Candidate University of Calgary


From ggrothendieck at gmail.com  Mon Jun 19 07:21:53 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 19 Jun 2006 01:21:53 -0400
Subject: [R] Custom Command to Generate SQL
In-Reply-To: <BAY106-F137642F2259FC24DA43F5098860@phx.gbl>
References: <BAY106-F137642F2259FC24DA43F5098860@phx.gbl>
Message-ID: <971536df0606182221r3166a098i50d0cf5550d5f60d@mail.gmail.com>

This generates the sql statement so just pass that to your database:

retrieve <- function(...) {
   args <- list(...)
   sub("and", "select * from people where",
      paste(rbind("and", names(args), "=", dQuote(args)), collapse = " "))
}

# test
retrieve(firstname = "JOHN", middlename = "WILLIANS", lastname = "FORD")

On 6/19/06, Alex Restrepo <alex_restrepo at hotmail.com> wrote:
> Hi:
>
> I would like to create a custom command in R which generates SQL, which is
> then processed via RODBC.
>
> For example, the user would type:
>
>        retrieve firstName('JOHN') middlleName('WILLIAMS') lastName('FORD')
>
> This would generate the following SQL which would then be processed by
> RODBC:
>
> Select *
>  from people
>  where firstName = 'JOHN' and
>            middleName = 'WILLIAMS' and
>            lastName     = 'FORD'
>
> Does anyone have a recommendation?  Any ideas would be greatly appreciated.
>
> Alex
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From petr.pikal at precheza.cz  Mon Jun 19 08:57:56 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 19 Jun 2006 08:57:56 +0200
Subject: [R] Vector Manipulation
In-Reply-To: <1150491468.44931b4c11d98@webmail1.kuleuven.be>
References: <CC7973C6B927754794A205FEE463F7810A9329@TXS9312221.txfb-ins.com>
Message-ID: <44966714.6944.176040@localhost>

Hi

or slightly quicker

y<-(x != "N")*1

HTH
Petr

On 16 Jun 2006 at 22:57, Dimitrios Rizopoulos wrote:

Date sent:      	Fri, 16 Jun 2006 22:57:48 +0200
From:           	Dimitrios Rizopoulos <Dimitris.Rizopoulos at med.kuleuven.be>
To:             	"Davis, Jacob B. " <JBDavis at txfb-ins.com>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] Vector Manipulation

> try the following:
> 
> x <- c("B", "F", "N", "Y")
> ################
> y <- numeric(length(x))
> y[x != "N"] <- 1
> y
> 
> 
> I hope it helps.
> 
> Best,
> Dimitris
> 
> ---- 
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://med.kuleuven.be/biostat/
>      http://www.student.kuleuven.be/~m0390867/dimitris.htm
> 
> 
> Quoting "Davis, Jacob B. " <JBDavis at txfb-ins.com>:
> 
> > I have a vector that has 1,974 elements and each element is one of
> > the following (B, F, N, Y).  How do I recreate that vector accept in
> > the place of N put 0 and in the place of B, F or Y put a 1?
> > 
> >  
> > 
> > Thanks,
> > 
> > Jacob
> > 
> >  
> > 
> >  
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Mon Jun 19 09:09:07 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 19 Jun 2006 09:09:07 +0200
Subject: [R] =?iso-8859-2?q?more_=A0of_a_statistics/display_question?=
In-Reply-To: <22791288.1286601150671395616.JavaMail.root@vms070.mailsrvcs.net>
Message-ID: <449669B3.16909.219D49@localhost>

Hi

you could probably use image/contour/persp plots if your data are not 
completely chaotic.

HTH
Petr




On 18 Jun 2006 at 17:56, markleeds at verizon.net wrote:

Date sent:      	Sun, 18 Jun 2006 17:56:35 -0500 (CDT)
From:           	<markleeds at verizon.net>
To:             	markleeds at verizon.net, r-help at stat.math.ethz.ch
Subject:        	Re: [R] more ?of a statistics/display question

> >From: markleeds at verizon.net
> >Date: Sun Jun 18 17:47:39 CDT 2006
> >To: r-help at stat.math.ethz.ch
> >Subject: [R] more  of a statistics/display question
> 
> one more piece of info in regard to below :
> it's okay if i have turn the profit number
> into a 0/1 indicator variable with 0 indicating
> negative and 1 indicating positive, if
> that makes things easier. thanks.
> 
> 
> 
> 
> 
> >this is not really an R question but I imagine I would
> >use R to do it.
> >
> >I have a loop that has thousands of iterations and
> >after each iteration the results are a
> >
> >box leung pvalue
> >a regression t-stat
> >and a profit number 
> >
> >so, i have 1000's  of these sets of the 3 numbers above.
> >obviously, the p-value varies from zero to 1.
> >the t-statistic goes from about -3  to 3
> >and the profit numbers goes from around -30 to +30.
> >
> >i would like to see how the profit number varies with
> >the box-leung palue and the regression t-stat but
> >as a combination rather than separately because
> >a high t-stat isn't necessarily good if i have
> >a low box leung p-value or vice versa.
> >
> >does any know of a good way to visualize this and
> >an R-tool that would help me. Definitely, if someone
> >tells me the tool, I would read about but this
> >sounds like I need a 2 dimensional histogram sort of ?
> >
> >it's not a problem for me to save these numbers in a 
> >matrix or asci file or whatever but any idea or tools or book
> >on how to group them into different groupings would be helpful.
> >thanks                              
> >
> >                                  mark
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
> >http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From attenka at utu.fi  Mon Jun 19 09:15:28 2006
From: attenka at utu.fi (Atte Tenkanen)
Date: Mon, 19 Jun 2006 10:15:28 +0300
Subject: [R] How to change the margin widths in png-plots?
In-Reply-To: <fb7ed21b27709.4490d669@utu.fi>
References: <fb7ed21b27709.4490d669@utu.fi>
Message-ID: <f6d6b655261db.44967940@utu.fi>

I found one solution. It may be unofficial but it works. I put the 
par(mar=c(5.1, 7.1, 4.1, 2.1)) command between png() and plot() -commands.

Atte

> Hello,
> 
> I have tried to change the margin widths so that mtext (here "sd 
> of  
> consecutive pc intervals", look at the picture) and  
> plot(...,xlab="bar") fits to the picture.
> 
> Here is an example:
> http://users.utu.fi/attenka/margins.png
> 
> This doesn't help:
> par(mar=c(5.1, 7.1, 4.1, 2.1))
> 
> And here are the commands:
> 
> png(filename="/Users/kone/Vaitostutkimus/Pictures/ 
> BachBWV60_sd_cons_pc_int.png", width = 2800, height = 
> 1200,pointsize =  
> 12, bg = "white");
> plot(Compo_SD_succ_int_array_vector,ylim=c(0.40,1.9),col="white",xlab="b 
> ar",ylab="", cex.lab=3, cex.axis=2);
> mtext("sd of consecutive pc intervals", side=2, line=0,  
> padj=-1.8,at=1.2, cex=3)
> lines(Compo_SD_succ_int_array_vector,col=1,lty=1,lwd=2);
> text(2,0.93,labels="*",cex=3) # "an asterisk..."
> dev.off();
> 
> What do I do next?
> 
> Atte Tenkanen, Turku Finland
>


From attenka at utu.fi  Mon Jun 19 09:22:03 2006
From: attenka at utu.fi (Atte Tenkanen)
Date: Mon, 19 Jun 2006 10:22:03 +0300
Subject: [R] Border line width?
Message-ID: <fb7ecc9125956.44967acb@utu.fi>

Is there some way to change the line widths of plot borders?
I couldn't find any parameters for that purpose. 

Atte Tenkanen
University of Turku, Finland


From ripley at stats.ox.ac.uk  Mon Jun 19 09:33:35 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 19 Jun 2006 08:33:35 +0100 (BST)
Subject: [R] Border line width?
In-Reply-To: <fb7ecc9125956.44967acb@utu.fi>
References: <fb7ecc9125956.44967acb@utu.fi>
Message-ID: <Pine.LNX.4.64.0606190831510.5220@gannet.stats.ox.ac.uk>

On Mon, 19 Jun 2006, Atte Tenkanen wrote:

> Is there some way to change the line widths of plot borders?
> I couldn't find any parameters for that purpose.

Do you mean the (optional) box?  Use bty="n" and then box(lwd=).

plot(1:10, bty="n")
box(lwd=3)

works for me.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From r.hankin at noc.soton.ac.uk  Mon Jun 19 09:33:08 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 19 Jun 2006 08:33:08 +0100
Subject: [R] combining tables
Message-ID: <B469259B-333B-41BF-958C-D95DAB37B303@soc.soton.ac.uk>

Hi

Suppose I have two tables of counts of different animals and I
wish to pool them so the total of the sum is the sum of the total.

Toy example follows (the real ones are ~10^3 species and ~10^6  
individuals).
x and y are zoos that have merged and I need a combined inventory
including animals that are temporarily absent.


 > (x <- as.table(c(cats=3,squid=7,pigs=2,dogs=1,slugs=0)))
cats squid  pigs  dogs slugs
     3     7     2     1     0
 > (y <- as.table(c(cats=4,dogs=5,slugs=3,crabs=0)))
cats  dogs slugs crabs
     4     5     3     0
 > (desired.output <- as.table(c 
(cats=7,squid=7,pigs=2,dogs=6,slugs=3,crabs=0)))
cats squid  pigs  dogs slugs crabs
     7     7     2     6     3     0
 >




Note that we have 7 cats altogether, and the crabs correctly show
as zero counts.


How to do this nicely in R?




--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From petr.pikal at precheza.cz  Mon Jun 19 09:34:30 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 19 Jun 2006 09:34:30 +0200
Subject: [R] How to change the margin widths in png-plots?
In-Reply-To: <f6d6b655261db.44967940@utu.fi>
References: <fb7ed21b27709.4490d669@utu.fi>
Message-ID: <44966FA6.15012.38DAFE@localhost>

Hi

Perfectly official solution.

First open graphic device (ps, png, jpeg, ...) and then change 
parameters according to your wish.

HTH
Petr



On 19 Jun 2006 at 10:15, Atte Tenkanen wrote:

Date sent:      	Mon, 19 Jun 2006 10:15:28 +0300
From:           	Atte Tenkanen <attenka at utu.fi>
To:             	r-help at stat.math.ethz.ch
Priority:       	normal
Subject:        	Re: [R] How to change the margin widths in png-plots?

> I found one solution. It may be unofficial but it works. I put the
> par(mar=c(5.1, 7.1, 4.1, 2.1)) command between png() and plot()
> -commands.
> 
> Atte
> 
> > Hello,
> > 
> > I have tried to change the margin widths so that mtext (here "sd of 
> > consecutive pc intervals", look at the picture) and 
> > plot(...,xlab="bar") fits to the picture.
> > 
> > Here is an example:
> > http://users.utu.fi/attenka/margins.png
> > 
> > This doesn't help:
> > par(mar=c(5.1, 7.1, 4.1, 2.1))
> > 
> > And here are the commands:
> > 
> > png(filename="/Users/kone/Vaitostutkimus/Pictures/ 
> > BachBWV60_sd_cons_pc_int.png", width = 2800, height = 
> > 1200,pointsize =  
> > 12, bg = "white");
> > plot(Compo_SD_succ_int_array_vector,ylim=c(0.40,1.9),col="white",xla
> > b="b ar",ylab="", cex.lab=3, cex.axis=2); mtext("sd of consecutive
> > pc intervals", side=2, line=0,  padj=-1.8,at=1.2, cex=3)
> > lines(Compo_SD_succ_int_array_vector,col=1,lty=1,lwd=2);
> > text(2,0.93,labels="*",cex=3) # "an asterisk..." dev.off();
> > 
> > What do I do next?
> > 
> > Atte Tenkanen, Turku Finland
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From dimitris.rizopoulos at med.kuleuven.be  Mon Jun 19 09:38:46 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 19 Jun 2006 09:38:46 +0200
Subject: [R] Border line width?
References: <fb7ecc9125956.44967acb@utu.fi>
Message-ID: <01c801c69373$65b87f70$0540210a@www.domain>

maybe you're looking for:

plot(0:1, 0:1)
box(lwd = 3)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Atte Tenkanen" <attenka at utu.fi>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, June 19, 2006 9:22 AM
Subject: [R] Border line width?


> Is there some way to change the line widths of plot borders?
> I couldn't find any parameters for that purpose.
>
> Atte Tenkanen
> University of Turku, Finland
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From petr.pikal at precheza.cz  Mon Jun 19 09:36:32 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 19 Jun 2006 09:36:32 +0200
Subject: [R] Border line width?
In-Reply-To: <fb7ecc9125956.44967acb@utu.fi>
Message-ID: <44967020.15224.3AB77E@localhost>

Hi

plot(1,1)
box(, lwd=5)

HTH
Petr


On 19 Jun 2006 at 10:22, Atte Tenkanen wrote:

Date sent:      	Mon, 19 Jun 2006 10:22:03 +0300
From:           	Atte Tenkanen <attenka at utu.fi>
To:             	r-help at stat.math.ethz.ch
Priority:       	normal
Subject:        	[R] Border line width?

> Is there some way to change the line widths of plot borders?
> I couldn't find any parameters for that purpose. 
> 
> Atte Tenkanen
> University of Turku, Finland
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From attenka at utu.fi  Mon Jun 19 09:41:00 2006
From: attenka at utu.fi (Atte Tenkanen)
Date: Mon, 19 Jun 2006 10:41:00 +0300
Subject: [R] Border line width?
In-Reply-To: <Pine.LNX.4.64.0606190831510.5220@gannet.stats.ox.ac.uk>
References: <fb7ecc9125956.44967acb@utu.fi>
	<Pine.LNX.4.64.0606190831510.5220@gannet.stats.ox.ac.uk>
Message-ID: <f6d8d8a8253cb.44967f3c@utu.fi>

TNX Brian!

Atte

----- Original Message -----
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Date: Monday, June 19, 2006 10:33 am
Subject: Re: [R] Border line width?

> On Mon, 19 Jun 2006, Atte Tenkanen wrote:
> 
> > Is there some way to change the line widths of plot borders?
> > I couldn't find any parameters for that purpose.
> 
> Do you mean the (optional) box?  Use bty="n" and then box(lwd=).
> 
> plot(1:10, bty="n")
> box(lwd=3)
> 
> works for me.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From ripley at stats.ox.ac.uk  Mon Jun 19 09:40:48 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 19 Jun 2006 08:40:48 +0100 (BST)
Subject: [R] Fitting Distributions Directly From a Histogram
In-Reply-To: <4495F3C4.7090608@pdf.com>
References: <a2b3004b0606120351yefb42bbjf1d1f05f541c29@mail.gmail.com>
	<200606181820.35874.vincent.goulet@act.ulaval.ca>
	<4495F3C4.7090608@pdf.com>
Message-ID: <Pine.LNX.4.64.0606190833410.5220@gannet.stats.ox.ac.uk>

On Sun, 18 Jun 2006, Spencer Graves wrote:

> 	  Won't 'interval censoring using 'survreg' in the 'survival' package
> handle this?
> (http://finzi.psych.upenn.edu/R/library/survival/html/survreg.html)

Yes, but only for distributions it supports (and I think even with a 
user-supplied distribution only for positive values).

Coding up a way to fit mles to grouped data would be a fairly simple 
exercise based on fitdistr, but finding suitable starting values for 
optimization would be more difficult.

It is quite rare these days for only the grouped data to be available, now 
most data collection is automated.  I did something like this for 
analytical chemists in the mid 1980s (pre-R and indeed pre-S for me), but 
their need had vanished by the time I started to port things to S.

>
> 	  Hope this helps.
> 	  Spencer Graves
>
> Vincent Goulet wrote:
>> Le Lundi 12 Juin 2006 06:51, Lorenzo Isella a ?crit :
>>> Dear All,
>>>
>>> A simple question: packages like fitdistr should be ideal to analyze
>>> samples of data taken from a univariate distribution, but what if
>>> rather than the raw data of the observations you are given directly
>>> and only a histogram?
>>
>> Let's assume that you have not only the histogram itself, but also the breaks
>> and the counts per bin. Then you have what grouped data --- at least that's
>> how we call those in Actuarial Science. Maximum likelyhood estimation is
>> feasible for such data, but it is slightly more complicated. "Loss Models" by
>> Klugman, Panjer & Willmot (Wiley) covers this.
>>
>> I'm now thinking of adding this to my actuarial science package "actuar"...
>>
>>> I was thinking about generating artificially a set of data
>>> corresponding to the counts binned in the histogram, but this sounds
>>> too cumbersome.
>>> Another question is the following: fitdistr provides the value of the
>>> log-likely hood function, but what if I want e.g. a chi square test to
>>> get some insight on the goodness of the fitting?
>>
>> Goodness of fit tests for grouped data are also covered in Loss Models.
>>
>>> I am sure there must be a way to get it straightforwardly without
>>> coding it myself.
>>
>> Once you have the theory, I'm afraid for now you will have to code the
>> estimation procedure yourself.
>>
>> Cheers.
>>
>>> Many thanks
>>>
>>> Lorenzo
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From dimitris.rizopoulos at med.kuleuven.be  Mon Jun 19 09:54:36 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 19 Jun 2006 09:54:36 +0200
Subject: [R] combining tables
References: <B469259B-333B-41BF-958C-D95DAB37B303@soc.soton.ac.uk>
Message-ID: <01d301c69375$9c58a670$0540210a@www.domain>

one approach is the following:

x <- as.table(c(cats=3,squid=7,pigs=2,dogs=1,slugs=0))
y <- as.table(c(cats=4,dogs=5,slugs=3,crabs=0))
########################
out <- merge(as.data.frame(x), as.data.frame(y), by = "Var1", all = 
TRUE)
desired.output <- rowSums(out[c("Freq.x", "Freq.y")], na.rm = TRUE)
names(desired.output) <- out$Var1
as.table(desired.output)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Robin Hankin" <r.hankin at noc.soton.ac.uk>
To: "RHelp" <r-help at stat.math.ethz.ch>
Sent: Monday, June 19, 2006 9:33 AM
Subject: [R] combining tables


> Hi
>
> Suppose I have two tables of counts of different animals and I
> wish to pool them so the total of the sum is the sum of the total.
>
> Toy example follows (the real ones are ~10^3 species and ~10^6
> individuals).
> x and y are zoos that have merged and I need a combined inventory
> including animals that are temporarily absent.
>
>
> > (x <- as.table(c(cats=3,squid=7,pigs=2,dogs=1,slugs=0)))
> cats squid  pigs  dogs slugs
>     3     7     2     1     0
> > (y <- as.table(c(cats=4,dogs=5,slugs=3,crabs=0)))
> cats  dogs slugs crabs
>     4     5     3     0
> > (desired.output <- as.table(c
> (cats=7,squid=7,pigs=2,dogs=6,slugs=3,crabs=0)))
> cats squid  pigs  dogs slugs crabs
>     7     7     2     6     3     0
> >
>
>
>
>
> Note that we have 7 cats altogether, and the crabs correctly show
> as zero counts.
>
>
> How to do this nicely in R?
>
>
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From rkrug at sun.ac.za  Mon Jun 19 09:53:53 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Mon, 19 Jun 2006 09:53:53 +0200
Subject: [R] Question concerning mle
In-Reply-To: <44947B0F.50305@pdf.com>
References: <449166A3.40209@sun.ac.za> <44947B0F.50305@pdf.com>
Message-ID: <44965811.4000709@sun.ac.za>

Thanks a lot Spencer for your tips - I'll look into all of them.

Rainer

Spencer Graves wrote:
>       It's difficult to say much at this level of generality, but I have
> four suggestions:
> 
>       1.  Have you tried creating a reasonable grid of starting values
> using "expand.grid" and then plotting the resulting likelihood surface?
>  If you have more than 2 parameters, you may want to use 'lattice'
> graphics.  This should tell you if the functions seems unimodal, convex,
> etc., in the region you covered and at the resolution of your grid.
> 
>       2.  Have you tried method="SANN" = simulated annealing?  I might
> try one pass with SANN, then refine the solution found by SANN using BFGS.
> 
>       3.  After you have a solution, you can then try profile likelihod.
> Unfortunately, my experience with profile.mle has been mixed.  I
> actually made local copies of mle and profile.mle and found and fixed
> some of the deficiencies of each.  I didn't test them enough to offer
> the results to the R Core Team, however.
> 
>       4.  Have you looked at Venables and Ripley (2002) Modern Applied
> Statistics with S, 4th ed. (Springer)?  It's a great book for many
> things, including the use of expand.grid and 'optim'.
> 
>       hope this helps.
>       Spencer Graves
> 
> Rainer M Krug wrote:
>> Hi
>>
>> I hope this is the right forum - if not, point me please to a better one.
>>
>> I am using R 2.3.0 on Linux, SuSE 10.
>>
>>
>> I have a question concerning mle (method="BFGS").
>>
>> I have a few models which I am fitting to existing data points. I
>> realised, that the likelihood is quite sensitive to the start values for
>> one parameter.
>>
>> I am wondering: what is the best approach to identify the right initial
>> values? Do I have to do it recursively, and if yes, how can I automate
>> it? Or do I have to play with the system?
>>
>> I am quite confident that the resulting parameters are the optimal for
>> my problem - but can I verify it?
>>
>> Thanks,
>>
>> Rainer
>>
>>


-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa


From M.v.Jole at student.tue.nl  Mon Jun 19 09:59:08 2006
From: M.v.Jole at student.tue.nl (Jole, M. van)
Date: Mon, 19 Jun 2006 09:59:08 +0200
Subject: [R] Generating Random Numbers after Momentfitting
Message-ID: <EA06B2F9265B954BBCF8F88B55D32EEF0DCE05@studentEX8.campus.tue.nl>

L.S.
 
After moment fitting, which I programmed as follows:
#Function which, given the first two moment, computates the density, 
#according to momentfitting
momentfit<-function(ES,c){
    res<-(1:5)
    if(c<=1){
        k<-floor(1/c^2)
        p<-((1)/(1+c^2))*((k+1)*c^(2)-sqrt((k+1)*(1+c^(2))-(k+1)^(2)*c^(2)))
        mu<-(k+1-p)/ES
        res[1]<-0
        res[2]<-k
        res[3]<-p
        res[4]<-mu
        res[5]<-0
     
    }
    
    if(c>1){
        p1<-((1)/(2))*(1+sqrt((c^2-1)/(c^2+1)))
        p2<-1-p1
        mu1<-2*p1/ES
        mu2<-2*p2/ES
        res[1]<-1
        res[2]<-p1
        res[3]<-p2
        res[4]<-mu1
        res[5]<-mu2     
    }
    return(res)
}
I have the either one of the following distributionfunctions:
 
ES<-a
c<-b
para<-momenfit(a,c)
if(para[1]==0){
       fun1<-function(x){(para[3]*dgamma(x,para[2],para[4])+(1-para[3])*dgamma(x,para[2]+1,para[4]))}
}
if(para[1]==1){
       fun2<-function(x){(para[2]*dgamma(x,1,para[4])+para[3]*dgamma(x,1,para[5]))}
}
 
I now want to generate a random number according to the above distrubutions. Can somebody help me? Thanks.
 
 
Martin


From carsten.steinhoff at gmx.de  Sun Jun 18 21:53:35 2006
From: carsten.steinhoff at gmx.de (Carsten Steinhoff)
Date: Sun, 18 Jun 2006 21:53:35 +0200
Subject: [R] Bayesian Networks with deal
Message-ID: <200606181953.k5IJrZh2007239@hypatia.math.ethz.ch>

Ein eingebundener Text mit undefiniertem Zeichensatz wurde abgetrennt.
Name: nicht verf?gbar
URL: https://stat.ethz.ch/pipermail/r-help/attachments/20060618/a60e3fff/attachment.pl 

From deleeuw at stat.ucla.edu  Mon Jun 19 11:34:17 2006
From: deleeuw at stat.ucla.edu (Jan de Leeuw)
Date: Mon, 19 Jun 2006 02:34:17 -0700 (PDT)
Subject: [R] Foometrics in R
Message-ID: <50514.212.123.203.118.1150709657.squirrel@212.123.203.118>


One of the outcomes of useR! 2006 is that JSS is planning to publish
a series of special volumes. They will be guest edited (I have guest
editors already, although somewhat tentative in some cases). Each volume
will have 5-10 issues (articles) of the usual JSS format.

Psychometrics in R
Political Methodology in R
Econometrics in R
Social Science Methodology in R
Spectroscopy/Chemometrics in R
Ecology in R

If you have suggestions, comments, possible contributions, ideas for
additional volumes and so on, send them to me and I'll forward them to the
appropriate authorities.

-- 
Jan de Leeuw; Distinguished Professor and Chair, UCLA Department of
Statistics;
Editor: Journal of Multivariate Analysis, Journal of Statistical Software
US mail: 8130 Math Sciences Bldg, Box 951554, Los Angeles, CA 90095-1554
phone (310)-825-9550;  fax (310)-206-5658;  email: deleeuw at stat.ucla.edu
homepage: http://gifi.stat.ucla.edu
 ------------------------------------------------------
          No matter where you go, there you are. --- Buckaroo Banzai
                   http://gifi.stat.ucla.edu/sounds/nomatter.au
 ------------------------------------------------------


From rob at robertcampbell.co.uk  Mon Jun 19 11:36:25 2006
From: rob at robertcampbell.co.uk (Rob Campbell)
Date: Mon, 19 Jun 2006 10:36:25 +0100
Subject: [R] can I call user-created functions without source() ?
Message-ID: <44967019.50009@robertcampbell.co.uk>

Hi,

I have to R fairly recently from Matlab, where I have been used to
organising my own custom functions into directories which are adding to
the Matlab search path. This enables me to call them as I would one of
Matlab's built-in functions. I have not found a way of doing the same
thing in R. I have resorted to using source() on symlinks located in the
current directory. Not very satisfactory.

I looked at the R homepage and considered making a "package" of my
commonly used functions so that I can call them in one go:
library(myFuncions, lib.loc="/path/to/library") Perhaps this is the only
solution but the docs on the web make the process seem rather
complicated--I only have a few script files I want to call! Surely
there's a straightforward solution?

How have other people solved this problem? Perhaps someone has a simple
"package skeleton" into which I can drop my scripts?


Thanks,

Rob



-- 
  Rob Campbell   -   Research Student
 Autistic Bacteriophage Research Group
 www.autisticBacteriophage.notlong.com

               Oxford

      www.robertcampbell.co.uk


From vincent at 7d4.com  Mon Jun 19 11:53:37 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Mon, 19 Jun 2006 11:53:37 +0200
Subject: [R] function call
Message-ID: <44967421.3040007@7d4.com>

Dear R-users,

When a function f1() is called, is there a way to know,
from inside the function f1(), if f1() is directly called from the
R console, or indirectly from another function f2() ?

Of course, I may add an argument to f1(..., callbyf2=FALSE) only used
by f2() giving explicitely this info, but I would like to know
if there may be a more generic way ?

Thanks for any info or pointer


From p.dalgaard at biostat.ku.dk  Mon Jun 19 12:04:58 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Jun 2006 12:04:58 +0200
Subject: [R] function call
In-Reply-To: <44967421.3040007@7d4.com>
References: <44967421.3040007@7d4.com>
Message-ID: <x2y7vtwjed.fsf@viggo.kubism.ku.dk>

vincent at 7d4.com writes:

> Dear R-users,
> 
> When a function f1() is called, is there a way to know,
> from inside the function f1(), if f1() is directly called from the
> R console, or indirectly from another function f2() ?
> 
> Of course, I may add an argument to f1(..., callbyf2=FALSE) only used
> by f2() giving explicitely this info, but I would like to know
> if there may be a more generic way ?
> 
> Thanks for any info or pointer

You can always query via sys.status/sys.call/sys.function etc. Or, if
the issue is whether the call is from the command line: 

  identical(parent.frame(), globalenv())

However, it does look a bit like unsound programming practice. Why do
you perceive a need to do this?

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From guangxing at ict.ac.cn  Mon Jun 19 12:17:20 2006
From: guangxing at ict.ac.cn (GuangXing)
Date: Mon, 19 Jun 2006 18:17:20 +0800
Subject: [R] how to do this sum?
Message-ID: <200606191817204286535@ict.ac.cn>

Hi, Everybody!
I have a big table which named table_x, and all the elements in the table is very large!
Now I want to do the summay on the talbe_x[,3].
Unfornately, I can't get the right result!
And the R give the warning messages as follow:
> sum(table_x[,3])
>[1] NA
>Warning message:
>interger overflow in sum(.);please use sum(as.numeric(.)) 
(the original upper message is Chinese,and I have translated those to English.) 	

Any one tell me how to use the as.numeric(.)?
And I want to know does R support the 64bits interger?How to do?
Pls forgive my ignorance.

Thank you in advance!			
--------------
GuangXing
2006-06-19


From Ted.Harding at nessie.mcc.ac.uk  Mon Jun 19 12:25:19 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 19 Jun 2006 11:25:19 +0100 (BST)
Subject: [R] can I call user-created functions without source() ?
In-Reply-To: <44967019.50009@robertcampbell.co.uk>
Message-ID: <XFMail.060619112519.Ted.Harding@nessie.mcc.ac.uk>

On 19-Jun-06 Rob Campbell wrote:
> Hi,
> 
> I have to R fairly recently from Matlab, where I have been used to
> organising my own custom functions into directories which are adding to
> the Matlab search path. This enables me to call them as I would one of
> Matlab's built-in functions. I have not found a way of doing the same
> thing in R. I have resorted to using source() on symlinks located in
> the
> current directory. Not very satisfactory.
> 
> I looked at the R homepage and considered making a "package" of my
> commonly used functions so that I can call them in one go:
> library(myFuncions, lib.loc="/path/to/library") Perhaps this is the
> only
> solution but the docs on the web make the process seem rather
> complicated--I only have a few script files I want to call! Surely
> there's a straightforward solution?
> 
> How have other people solved this problem? Perhaps someone has a simple
> "package skeleton" into which I can drop my scripts?
> 
> 
> Thanks,
> 
> Rob

There are pros and cons to this, but on the whole I sympathise
with you (having pre-R been a heavy matlab/octave user myself).

Unfortunately (from this perspective) R does not seem to have
an automatic "load-on-demand" facility similar to what happens
in matlab (i.e. you call a function by name, and R would search
for it in whatever the current search-path is, and load its
definition plus what else it depends on).

I have a few definitions which I want in every R session, so
I have put these in my ".Rprofile". But this is loaded from
my home directory, not from the directory where I was when I
started R, so it is the same every time. Again, one of the
conveniences of the matlab/octave approach is that you can
have a different sub-directory for each project, so if you
start work in a particular one then you have access to any
special definitions for that project, and not to others.

I'm no expert on this aspect of R, but I suspect that the way
start-up is organised in R does not fit well with the other
kind of approach. I stand to be corrected, of course ...

And others may well have formulated their own neat work-rounds,
so we wait eagerly to hear about these!

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 19-Jun-06                                       Time: 11:25:15
------------------------------ XFMail ------------------------------


From vincent at 7d4.com  Mon Jun 19 12:37:43 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Mon, 19 Jun 2006 12:37:43 +0200
Subject: [R] function call
In-Reply-To: <x2y7vtwjed.fsf@viggo.kubism.ku.dk>
References: <44967421.3040007@7d4.com> <x2y7vtwjed.fsf@viggo.kubism.ku.dk>
Message-ID: <44967E77.8030508@7d4.com>

Peter Dalgaard a ?crit :

> vincent at 7d4.com writes:
> 
>>Dear R-users,
>>
>>When a function f1() is called, is there a way to know,
>>from inside the function f1(), if f1() is directly called from the
>>R console, or indirectly from another function f2() ?
>>
>>Of course, I may add an argument to f1(..., callbyf2=FALSE) only used
>>by f2() giving explicitely this info, but I would like to know
>>if there may be a more generic way ?
>>
>>Thanks for any info or pointer
> 
> 
> You can always query via sys.status/sys.call/sys.function etc. Or, if
> the issue is whether the call is from the command line: 
> 
>   identical(parent.frame(), globalenv())
> 
> However, it does look a bit like unsound programming practice. Why do
> you perceive a need to do this?

I have some functions which may :
1/ either be called directly by the user thru the R console,
to obtain only an information concerning a unique argument,
f(arg1).
In this case, I sometimes like to get *detailed* information
about the results : date(), file info, etc
2/ either be called from inside a for loop in a function f2()
passing in review a long list of arguments,
f(arg1),f(arg2),..., f(argn)
and summarizing the results
In this case, I don't want to see the details 100 times.

But my question was also simply for a better R knowledge.

interactive() seems nice.
Thanks for the info.


From guangxing at ict.ac.cn  Mon Jun 19 13:05:51 2006
From: guangxing at ict.ac.cn (GuangXing)
Date: Mon, 19 Jun 2006 19:05:51 +0800
Subject: [R] how to do this sum?
References: <200606191817204286535@ict.ac.cn>
	<000d01c6938a$2c0d0ae0$104efea9@yourgk68c57jh8>
Message-ID: <200606191905512641048@ict.ac.cn>

I am sorry for my ignorance.
table_x is defined as follow:
>table_x<-read.table("stat.txt")
in the stat.txt,there are 5  cols,there are so many big intergers in the every cols.
Now, I want to to that:
>sum(table_x[,3])
But it does not work.
How can I?
Pls help me!
------------------				 
GuangXing
2006-06-19

-------------------------------------------------------------
????????Dr L. Y Hin
??????????2006-06-19 17:16:04
????????GuangXing
??????
??????Re: [R] how to do this sum?

Of course.
table_x means assign x to the function called table.
if you call the table by the name table.x, it will
probably do the job.

----- Original Message ----- 
From: "GuangXing" <guangxing at ict.ac.cn>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, June 19, 2006 6:17 PM
Subject: [R] how to do this sum?


> Hi, Everybody!
> I have a big table which named table_x, and all the elements in the table 
> is very large!
> Now I want to do the summay on the talbe_x[,3].
> Unfornately, I can't get the right result!
> And the R give the warning messages as follow:
>> sum(table_x[,3])
>>[1] NA
>>Warning message:
>>interger overflow in sum(.);please use sum(as.numeric(.))
> (the original upper message is Chinese,and I have translated those to 
> English.)
>
> Any one tell me how to use the as.numeric(.)?
> And I want to know does R support the 64bits interger?How to do?
> Pls forgive my ignorance.
>
> Thank you in advance!
> --------------
> GuangXing
> 2006-06-19
>
>


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


From f.harrell at vanderbilt.edu  Mon Jun 19 12:14:05 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 19 Jun 2006 05:14:05 -0500
Subject: [R] useR! Thanks
Message-ID: <449678ED.1080809@vanderbilt.edu>

After attending my first useR! conference I want to thank the organizers 
for doing a wonderful job and the presenters for their high quality 
presentations and stimulating ideas.   The conference venue was 
excellent and of course Vienna is one of the greatest cities in the 
world to visit.  useR! is one of the most fun conferences I've attended.

Thanks again!
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From f.calboli at imperial.ac.uk  Mon Jun 19 13:14:43 2006
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Mon, 19 Jun 2006 12:14:43 +0100
Subject: [R] MLE maximum number of parameters
Message-ID: <B2B70BDD-81EC-4043-A769-020A2AC16923@imperial.ac.uk>

Hi All,

I would like to know, is there a *ballpark* figure for how many  
parameters the minimisation routines can cope with?

I'm asking because I was asked if I knew.

Cheers,

Federico

--
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St. Mary's Campus
Norfolk Place, London W2 1PG

Tel +44 (0)20 75941602   Fax +44 (0)20 75943193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com


From murdoch at stats.uwo.ca  Mon Jun 19 13:22:48 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 19 Jun 2006 07:22:48 -0400
Subject: [R] can I call user-created functions without source() ?
In-Reply-To: <44967019.50009@robertcampbell.co.uk>
References: <44967019.50009@robertcampbell.co.uk>
Message-ID: <44968908.4000006@stats.uwo.ca>

On 6/19/2006 5:36 AM, Rob Campbell wrote:
> Hi,
> 
> I have to R fairly recently from Matlab, where I have been used to
> organising my own custom functions into directories which are adding to
> the Matlab search path. This enables me to call them as I would one of
> Matlab's built-in functions. I have not found a way of doing the same
> thing in R. I have resorted to using source() on symlinks located in the
> current directory. Not very satisfactory.
> 
> I looked at the R homepage and considered making a "package" of my
> commonly used functions so that I can call them in one go:
> library(myFuncions, lib.loc="/path/to/library") Perhaps this is the only
> solution but the docs on the web make the process seem rather
> complicated--I only have a few script files I want to call! Surely
> there's a straightforward solution?
> 
> How have other people solved this problem? Perhaps someone has a simple
> "package skeleton" into which I can drop my scripts?

You could try the "package.skeleton" function.  It's not completely 
painless (you are still expected to edit some of the files, and to 
install the package), but it's not really so bad.  There are plans 
(already partially implemented in R-devel) to make it easier for the 
next release.  (So far I think the change is to require less editing on 
the default generated files.)

To answer the more general question: I generally use two styles of 
development.  For things where have long term usefulness, I go through 
the work above and create a package.  For more ephemeral things, I put 
them in a file, and in the file where I'm working on today's script, I 
put in source("blahblah.R") at the beginning.

You can also use a file named .Rprofile to contain commands to run; it 
is searched for in the current directory, then the user's home 
directory.  So you could put your source("blahblah.R") into .Rprofile if 
you want these functions to always be available.

Duncan Murdoch


From murdoch at stats.uwo.ca  Mon Jun 19 13:24:02 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 19 Jun 2006 07:24:02 -0400
Subject: [R] can I call user-created functions without source() ?
In-Reply-To: <XFMail.060619112519.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.060619112519.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <44968952.40008@stats.uwo.ca>

On 6/19/2006 6:25 AM, (Ted Harding) wrote:
> On 19-Jun-06 Rob Campbell wrote:
>> Hi,
>>
>> I have to R fairly recently from Matlab, where I have been used to
>> organising my own custom functions into directories which are adding to
>> the Matlab search path. This enables me to call them as I would one of
>> Matlab's built-in functions. I have not found a way of doing the same
>> thing in R. I have resorted to using source() on symlinks located in
>> the
>> current directory. Not very satisfactory.
>>
>> I looked at the R homepage and considered making a "package" of my
>> commonly used functions so that I can call them in one go:
>> library(myFuncions, lib.loc="/path/to/library") Perhaps this is the
>> only
>> solution but the docs on the web make the process seem rather
>> complicated--I only have a few script files I want to call! Surely
>> there's a straightforward solution?
>>
>> How have other people solved this problem? Perhaps someone has a simple
>> "package skeleton" into which I can drop my scripts?
>>
>>
>> Thanks,
>>
>> Rob
> 
> There are pros and cons to this, but on the whole I sympathise
> with you (having pre-R been a heavy matlab/octave user myself).
> 
> Unfortunately (from this perspective) R does not seem to have
> an automatic "load-on-demand" facility similar to what happens
> in matlab (i.e. you call a function by name, and R would search
> for it in whatever the current search-path is, and load its
> definition plus what else it depends on).
> 
> I have a few definitions which I want in every R session, so
> I have put these in my ".Rprofile". But this is loaded from
> my home directory, not from the directory where I was when I
> started R, so it is the same every time. 

Which version of R are you using?  This is not the current documented 
behaviour.  It looks in the current directory first, and only in your 
home directory if that fails.

Duncan Murdoch


Again, one of the
> conveniences of the matlab/octave approach is that you can
> have a different sub-directory for each project, so if you
> start work in a particular one then you have access to any
> special definitions for that project, and not to others.
> 
> I'm no expert on this aspect of R, but I suspect that the way
> start-up is organised in R does not fit well with the other
> kind of approach. I stand to be corrected, of course ...
> 
> And others may well have formulated their own neat work-rounds,
> so we wait eagerly to hear about these!
> 
> Best wishes,
> Ted.
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 19-Jun-06                                       Time: 11:25:15
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From priti.desai at kalyptorisk.com  Mon Jun 19 12:52:24 2006
From: priti.desai at kalyptorisk.com (priti desai)
Date: Mon, 19 Jun 2006 16:22:24 +0530
Subject: [R] Qurey : How to add trendline( st. line)  in Graph
Message-ID: <2AB7346A3227A74BB97F9A0D79E3E65A2BA0@mailserver.kalyptorisk.com>

How to add trendline (i.e. straight line passing through maximum points)
in graph.
I have worked on the data given below.
Please tell me how to add trendline in the graph.

The script is as follows

=================================== start
====================================================

# The data is as follows

data <- c( 0.01,  0.02, 0.04, 0.13,  0.17 , 0.19 , 0.21 , 0.27 , 0.27 ,
0.28,  0.29,  0.37,
           0.41,  0.49,  0.51,  0.52,  0.54,  0.57,  0.62,  0.63,  0.68,
0.73,  0.74, 0.79,
           0.81,  0.81,  0.82,  0.86,  0.94,  0.96,  1.02,  1.10,  1.10,
1.20,  1.29,  1.36,
           1.40,  1.41,  1.44,  1.45,  1.62,  1.67,  1.69,  1.78,  1.82,
2.11,  2.13,  2.14,
           2.24,  2.29,  2.34, 2.40,  2.46,  2.70,  2.83,  2.98,  3.00,
3.30,  3.53,  3.70,
           3.86,  3.90,  3.91,  3.98,  5.01,  5.23,  6.05,  6.12, 10.41,
10.73)

# P-P plot
average       <- mean(data)  
lambda        <- (1/average)
e             <- c(1:70)
f             <- c((e-.5)/70)
Fx            <- c(1 - exp(-lambda*data))
g             <- sort(Fx)
plot(f,g)

===================================== end
====================================================

Awaiting your positive reply.

Regards.
Priti.


From petr.pikal at precheza.cz  Mon Jun 19 13:36:49 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 19 Jun 2006 13:36:49 +0200
Subject: [R] Qurey : How to add trendline( st. line)  in Graph
In-Reply-To: <2AB7346A3227A74BB97F9A0D79E3E65A2BA0@mailserver.kalyptorisk.com>
Message-ID: <4496A871.25750.116C0A8@localhost>

Hi

maybe
abline(0,1)

HTH
Petr

On 19 Jun 2006 at 16:22, priti desai wrote:

Date sent:      	Mon, 19 Jun 2006 16:22:24 +0530
From:           	"priti desai" <priti.desai at kalyptorisk.com>
To:             	<r-help at stat.math.ethz.ch>
Subject:        	[R] Qurey : How to add trendline( st. line)  in Graph

> How to add trendline (i.e. straight line passing through maximum
> points) in graph. I have worked on the data given below. Please tell
> me how to add trendline in the graph.
> 
> The script is as follows
> 
> =================================== start
> ====================================================
> 
> # The data is as follows
> 
> data <- c( 0.01,  0.02, 0.04, 0.13,  0.17 , 0.19 , 0.21 , 0.27 , 0.27
> , 0.28,  0.29,  0.37,
>            0.41,  0.49,  0.51,  0.52,  0.54,  0.57,  0.62,  0.63, 
>            0.68,
> 0.73,  0.74, 0.79,
>            0.81,  0.81,  0.82,  0.86,  0.94,  0.96,  1.02,  1.10, 
>            1.10,
> 1.20,  1.29,  1.36,
>            1.40,  1.41,  1.44,  1.45,  1.62,  1.67,  1.69,  1.78, 
>            1.82,
> 2.11,  2.13,  2.14,
>            2.24,  2.29,  2.34, 2.40,  2.46,  2.70,  2.83,  2.98, 
>            3.00,
> 3.30,  3.53,  3.70,
>            3.86,  3.90,  3.91,  3.98,  5.01,  5.23,  6.05,  6.12,
>            10.41,
> 10.73)
> 
> # P-P plot
> average       <- mean(data)  
> lambda        <- (1/average)
> e             <- c(1:70)
> f             <- c((e-.5)/70)
> Fx            <- c(1 - exp(-lambda*data))
> g             <- sort(Fx)
> plot(f,g)
> 
> ===================================== end
> ====================================================
> 
> Awaiting your positive reply.
> 
> Regards.
> Priti.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Mon Jun 19 13:43:56 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 19 Jun 2006 13:43:56 +0200
Subject: [R] how to do this sum?
In-Reply-To: <200606191905512641048@ict.ac.cn>
Message-ID: <4496AA1C.29426.11D4490@localhost>

Hi

sum(as.numeric(table_x[,3]), na.rm=T)

shall help (maybe)

HTH
Petr




On 19 Jun 2006 at 19:05, GuangXing wrote:

Date sent:      	Mon, 19 Jun 2006 19:05:51 +0800
From:           	"GuangXing" <guangxing at ict.ac.cn>
To:             	"Dr L. Y Hin" <lyhin at netvigator.com>
Copies to:      	"r-help at stat.math.ethz.ch" <r-help at stat.math.ethz.ch>
Subject:        	Re: [R] how to do this sum?

> I am sorry for my ignorance.
> table_x is defined as follow:
> >table_x<-read.table("stat.txt")
> in the stat.txt,there are 5  cols,there are so many big intergers in
> the every cols. Now, I want to to that: >sum(table_x[,3]) But it does
> not work. How can I? Pls help me! ------------------				 GuangXing
> 2006-06-19
> 
> -------------------------------------------------------------
> ????????Dr L. Y Hin
> ??????????2006-06-19 17:16:04
> ????????GuangXing
> ??????
> ??????Re: [R] how to do this sum?
> 
> Of course.
> table_x means assign x to the function called table.
> if you call the table by the name table.x, it will
> probably do the job.
> 
> ----- Original Message ----- 
> From: "GuangXing" <guangxing at ict.ac.cn>
> To: <r-help at stat.math.ethz.ch>
> Sent: Monday, June 19, 2006 6:17 PM
> Subject: [R] how to do this sum?
> 
> 
> > Hi, Everybody!
> > I have a big table which named table_x, and all the elements in the
> > table is very large! Now I want to do the summay on the talbe_x[,3].
> > Unfornately, I can't get the right result! And the R give the
> > warning messages as follow:
> >> sum(table_x[,3])
> >>[1] NA
> >>Warning message:
> >>interger overflow in sum(.);please use sum(as.numeric(.))
> > (the original upper message is Chinese,and I have translated those
> > to English.)
> >
> > Any one tell me how to use the as.numeric(.)?
> > And I want to know does R support the 64bits interger?How to do? Pls
> > forgive my ignorance.
> >
> > Thank you in advance!
> > --------------
> > GuangXing
> > 2006-06-19
> >
> >
> 
> 
> ----------------------------------------------------------------------
> ----------
> 
> 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> 

Petr Pikal
petr.pikal at precheza.cz


From ggrothendieck at gmail.com  Mon Jun 19 13:45:33 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 19 Jun 2006 07:45:33 -0400
Subject: [R] combining tables
In-Reply-To: <B469259B-333B-41BF-958C-D95DAB37B303@soc.soton.ac.uk>
References: <B469259B-333B-41BF-958C-D95DAB37B303@soc.soton.ac.uk>
Message-ID: <971536df0606190445he5a6ccyd57c1b6fbed4dafa@mail.gmail.com>

Try this:

both <- c(x,y)
as.table(tapply(both, names(both), sum))

On 6/19/06, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
> Hi
>
> Suppose I have two tables of counts of different animals and I
> wish to pool them so the total of the sum is the sum of the total.
>
> Toy example follows (the real ones are ~10^3 species and ~10^6
> individuals).
> x and y are zoos that have merged and I need a combined inventory
> including animals that are temporarily absent.
>
>
>  > (x <- as.table(c(cats=3,squid=7,pigs=2,dogs=1,slugs=0)))
> cats squid  pigs  dogs slugs
>     3     7     2     1     0
>  > (y <- as.table(c(cats=4,dogs=5,slugs=3,crabs=0)))
> cats  dogs slugs crabs
>     4     5     3     0
>  > (desired.output <- as.table(c
> (cats=7,squid=7,pigs=2,dogs=6,slugs=3,crabs=0)))
> cats squid  pigs  dogs slugs crabs
>     7     7     2     6     3     0
>  >
>
>
>
>
> Note that we have 7 cats altogether, and the crabs correctly show
> as zero counts.
>
>
> How to do this nicely in R?
>
>
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Mon Jun 19 13:54:14 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 19 Jun 2006 07:54:14 -0400
Subject: [R] function call
In-Reply-To: <44967421.3040007@7d4.com>
References: <44967421.3040007@7d4.com>
Message-ID: <971536df0606190454l260311e6wef1c4e95d27a87ef@mail.gmail.com>

Perhaps you what you want to do is to return an object
that has a print method like this:

f1 <- function(x) structure(c(x, x^2), class = "f1")
print.f1 <- function(x) cat("x is", x[1], "x squared is", x[2], "\n")

Now you could do this:

f1(3)  # gives fancy output
y <- f1(3) # returns structure
unclass(y) # y, not as an "f1" class object

The point is that print.f1 could return something which is different
than the output of f1.


On 6/19/06, vincent at 7d4.com <vincent at 7d4.com> wrote:
> Dear R-users,
>
> When a function f1() is called, is there a way to know,
> from inside the function f1(), if f1() is directly called from the
> R console, or indirectly from another function f2() ?
>
> Of course, I may add an argument to f1(..., callbyf2=FALSE) only used
> by f2() giving explicitely this info, but I would like to know
> if there may be a more generic way ?
>
> Thanks for any info or pointer
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Mon Jun 19 13:58:44 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 19 Jun 2006 07:58:44 -0400
Subject: [R] Qurey : How to add trendline( st. line) in Graph
In-Reply-To: <2AB7346A3227A74BB97F9A0D79E3E65A2BA0@mailserver.kalyptorisk.com>
References: <2AB7346A3227A74BB97F9A0D79E3E65A2BA0@mailserver.kalyptorisk.com>
Message-ID: <971536df0606190458t7343a370oc711bc88c905120c@mail.gmail.com>

Try:

library(quantreg)
abline(rq(g ~ f, tau = .999))

On 6/19/06, priti desai <priti.desai at kalyptorisk.com> wrote:
> How to add trendline (i.e. straight line passing through maximum points)
> in graph.
> I have worked on the data given below.
> Please tell me how to add trendline in the graph.
>
> The script is as follows
>
> =================================== start
> ====================================================
>
> # The data is as follows
>
> data <- c( 0.01,  0.02, 0.04, 0.13,  0.17 , 0.19 , 0.21 , 0.27 , 0.27 ,
> 0.28,  0.29,  0.37,
>           0.41,  0.49,  0.51,  0.52,  0.54,  0.57,  0.62,  0.63,  0.68,
> 0.73,  0.74, 0.79,
>           0.81,  0.81,  0.82,  0.86,  0.94,  0.96,  1.02,  1.10,  1.10,
> 1.20,  1.29,  1.36,
>           1.40,  1.41,  1.44,  1.45,  1.62,  1.67,  1.69,  1.78,  1.82,
> 2.11,  2.13,  2.14,
>           2.24,  2.29,  2.34, 2.40,  2.46,  2.70,  2.83,  2.98,  3.00,
> 3.30,  3.53,  3.70,
>           3.86,  3.90,  3.91,  3.98,  5.01,  5.23,  6.05,  6.12, 10.41,
> 10.73)
>
> # P-P plot
> average       <- mean(data)
> lambda        <- (1/average)
> e             <- c(1:70)
> f             <- c((e-.5)/70)
> Fx            <- c(1 - exp(-lambda*data))
> g             <- sort(Fx)
> plot(f,g)
>
> ===================================== end
> ====================================================
>
> Awaiting your positive reply.
>
> Regards.
> Priti.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From seanpor at acm.org  Mon Jun 19 14:10:09 2006
From: seanpor at acm.org (Sean O'Riordain)
Date: Mon, 19 Jun 2006 13:10:09 +0100
Subject: [R] Qurey : How to add trendline( st. line) in Graph
In-Reply-To: <2AB7346A3227A74BB97F9A0D79E3E65A2BA0@mailserver.kalyptorisk.com>
References: <2AB7346A3227A74BB97F9A0D79E3E65A2BA0@mailserver.kalyptorisk.com>
Message-ID: <8ed68eed0606190510m32d6381n8d200d97970e7de@mail.gmail.com>

lines(predict(lm(g~f))~f,lty=2)
or if its a curvy line you're looking for...
lines(predict(loess(g~f))~f,lty=3)


On 19/06/06, priti desai <priti.desai at kalyptorisk.com> wrote:
> How to add trendline (i.e. straight line passing through maximum points)
> in graph.
> I have worked on the data given below.
> Please tell me how to add trendline in the graph.
>
> The script is as follows
>
> =================================== start
> ====================================================
>
> # The data is as follows
>
> data <- c( 0.01,  0.02, 0.04, 0.13,  0.17 , 0.19 , 0.21 , 0.27 , 0.27 ,
> 0.28,  0.29,  0.37,
>            0.41,  0.49,  0.51,  0.52,  0.54,  0.57,  0.62,  0.63,  0.68,
> 0.73,  0.74, 0.79,
>            0.81,  0.81,  0.82,  0.86,  0.94,  0.96,  1.02,  1.10,  1.10,
> 1.20,  1.29,  1.36,
>            1.40,  1.41,  1.44,  1.45,  1.62,  1.67,  1.69,  1.78,  1.82,
> 2.11,  2.13,  2.14,
>            2.24,  2.29,  2.34, 2.40,  2.46,  2.70,  2.83,  2.98,  3.00,
> 3.30,  3.53,  3.70,
>            3.86,  3.90,  3.91,  3.98,  5.01,  5.23,  6.05,  6.12, 10.41,
> 10.73)
>
> # P-P plot
> average       <- mean(data)
> lambda        <- (1/average)
> e             <- c(1:70)
> f             <- c((e-.5)/70)
> Fx            <- c(1 - exp(-lambda*data))
> g             <- sort(Fx)
> plot(f,g)
>
> ===================================== end
> ====================================================
>
> Awaiting your positive reply.
>
> Regards.
> Priti.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From bgreen at dyson.brisnet.org.au  Mon Jun 19 14:11:57 2006
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Mon, 19 Jun 2006 22:11:57 +1000
Subject: [R] saving rounded numbers as a new variable in a dataframe
In-Reply-To: <mailman.9.1150711204.3935.r-help@stat.math.ethz.ch>
Message-ID: <5.1.0.14.0.20060619220254.00bdf038@pop3.brisnet.org.au>



A basic question, but one that eludes me. I have created a new variable 
$numurder, which I have rounded off. I want to save the rounded off version 
of this variable to an existing datafile called 'ngri.csv' .

  numurder   <-c((murder*no.of.cases)/100)
[[1]]
  [1]  48.952 112.073 182.160 974.610 122.140 663.432 150.856  18.988 
137.925 198.045  68.930 203.148  30.056 100.955


round(numurder, 0)
  [1]  49 112 182 975 122 663 151  19 138 198  69 203  30 101


Advice regarding how to do this is appreciated,

Bob


From j.van_den_hoff at fz-rossendorf.de  Mon Jun 19 14:19:36 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Mon, 19 Jun 2006 14:19:36 +0200
Subject: [R] can I call user-created functions without source() ?
In-Reply-To: <44968952.40008@stats.uwo.ca>
References: <XFMail.060619112519.Ted.Harding@nessie.mcc.ac.uk>
	<44968952.40008@stats.uwo.ca>
Message-ID: <44969658.8050101@fz-rossendorf.de>

Duncan Murdoch wrote:
> On 6/19/2006 6:25 AM, (Ted Harding) wrote:
>> On 19-Jun-06 Rob Campbell wrote:
>>> Hi,
>>>
>>> I have to R fairly recently from Matlab, where I have been used to
>>> organising my own custom functions into directories which are adding to
>>> the Matlab search path. This enables me to call them as I would one of
>>> Matlab's built-in functions. I have not found a way of doing the same
>>> thing in R. I have resorted to using source() on symlinks located in
>>> the
>>> current directory. Not very satisfactory.
>>>
>>> I looked at the R homepage and considered making a "package" of my
>>> commonly used functions so that I can call them in one go:
>>> library(myFuncions, lib.loc="/path/to/library") Perhaps this is the
>>> only
>>> solution but the docs on the web make the process seem rather
>>> complicated--I only have a few script files I want to call! Surely
>>> there's a straightforward solution?
>>>
>>> How have other people solved this problem? Perhaps someone has a simple
>>> "package skeleton" into which I can drop my scripts?
>>>
>>>
>>> Thanks,
>>>
>>> Rob
>> There are pros and cons to this, but on the whole I sympathise
>> with you (having pre-R been a heavy matlab/octave user myself).
>>
>> Unfortunately (from this perspective) R does not seem to have
>> an automatic "load-on-demand" facility similar to what happens
>> in matlab (i.e. you call a function by name, and R would search
>> for it in whatever the current search-path is, and load its
>> definition plus what else it depends on).
>>
>> I have a few definitions which I want in every R session, so
>> I have put these in my ".Rprofile". But this is loaded from
>> my home directory, not from the directory where I was when I
>> started R, so it is the same every time. 
> 
> Which version of R are you using?  This is not the current documented 
> behaviour.  It looks in the current directory first, and only in your 
> home directory if that fails.
> 
> Duncan Murdoch
> 
> 
> Again, one of the
>> conveniences of the matlab/octave approach is that you can
>> have a different sub-directory for each project, so if you
>> start work in a particular one then you have access to any
>> special definitions for that project, and not to others.
>>
>> I'm no expert on this aspect of R, but I suspect that the way
>> start-up is organised in R does not fit well with the other
>> kind of approach. I stand to be corrected, of course ...
>>
>> And others may well have formulated their own neat work-rounds,
>> so we wait eagerly to hear about these!
>>
>> Best wishes,
>> Ted.
>>

using `package.skeleton' (as already mentioned) for the package dir 
layout and `prompt' for generating templates of the needed manpages is 
not so bad (once you get used to it), if the things you have in mind are 
at least of some long(er) term value (for you): at least you are forced 
(sort of) to document your software...

for short term usage of some specialized functions I have added some 
lines to the `.Rprofile' in my home(!) directory as follows (probably 
there are smarter solutions, but at least it works):

#source some temporary useful functions:
fl <- dir(path='~/rfiles/current',patt='.*\\.R$',full.names=TRUE)
for (i in fl) {cat(paste('source("',i,'")\n',sep="")); source(i)}
rm(i,fl)

here, I have put all the temporary stuff in a single dedicated dir 
`~/rfiles/current', but of course you can use several dirs in this way. 
all files in this dir with names ending in `.R' are sourced on startup 
of R. this roughly works like one of the directories on MATLAB's search 
path: every function definition in this directory is  'understood' by R 
(but everything is loaded into the workspace on startup, no matter, 
whether you really need it in the end: no real `load on demand'). one 
important difference, though: this is only sensible for function 
definitions, not scripts ('executable programms' (which would directly 
be executed on R startup, otherwise).
and, contrary to matlab/octave, this is not dynamic: everything is read 
in at startup, later modifications to the directories are not recognized 
without explicitely sourcing the files again.

if you in addition you want to load definitions from the startup 
directory where you launch R (your project dir), the above could be 
modified to:

#source some temporary useful functions from startup dir:
fl <- dir(path=getwd(),patt='.*\\.R$',full.names=TRUE)
for (i in fl) {cat(paste('source("',i,'")\n',sep="")); source(i)}
rm(i,fl)

in this way you at least don't need a separate `.Rprofile' in each 
project dir.



joerg


From neuro3000 at hotmail.com  Mon Jun 19 14:20:16 2006
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Mon, 19 Jun 2006 08:20:16 -0400
Subject: [R] saving rounded numbers as a new variable in a dataframe
In-Reply-To: <5.1.0.14.0.20060619220254.00bdf038@pop3.brisnet.org.au>
Message-ID: <BAY112-F2329C5DD1652A0D346AD6FAF860@phx.gbl>

Hi

1-Please read this: R Data Import/Export 
cran.r-project.org/doc/manuals/R-data.pdf
2-This is the code:

rounded <-round(numurder, 0)
write.csv(rounded,"c:/ngri.csv")

Neuro


>From: Bob Green <bgreen at dyson.brisnet.org.au>
>To: r-help at stat.math.ethz.ch
>Subject: [R] saving rounded numbers as a new variable in a dataframe
>Date: Mon, 19 Jun 2006 22:11:57 +1000
>
>
>
>A basic question, but one that eludes me. I have created a new variable
>$numurder, which I have rounded off. I want to save the rounded off version
>of this variable to an existing datafile called 'ngri.csv' .
>
>   numurder   <-c((murder*no.of.cases)/100)
>[[1]]
>   [1]  48.952 112.073 182.160 974.610 122.140 663.432 150.856  18.988
>137.925 198.045  68.930 203.148  30.056 100.955
>
>
>round(numurder, 0)
>   [1]  49 112 182 975 122 663 151  19 138 198  69 203  30 101
>
>
>Advice regarding how to do this is appreciated,
>
>Bob
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html


From ggrothendieck at gmail.com  Mon Jun 19 14:33:28 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 19 Jun 2006 08:33:28 -0400
Subject: [R] Qurey : How to add trendline( st. line) in Graph
In-Reply-To: <971536df0606190458t7343a370oc711bc88c905120c@mail.gmail.com>
References: <2AB7346A3227A74BB97F9A0D79E3E65A2BA0@mailserver.kalyptorisk.com>
	<971536df0606190458t7343a370oc711bc88c905120c@mail.gmail.com>
Message-ID: <971536df0606190533j5291025cw72532981a4c0c0cd@mail.gmail.com>

One other item.  You had asked for the trendline passing through the
maximum points, which is what my prior answer gives, but
others interpreted your question as just as ordinary trendline so if that
is what you are looking for try this:

abline(lm(g ~ f))


On 6/19/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Try:
>
> library(quantreg)
> abline(rq(g ~ f, tau = .999))
>
> On 6/19/06, priti desai <priti.desai at kalyptorisk.com> wrote:
> > How to add trendline (i.e. straight line passing through maximum points)
> > in graph.
> > I have worked on the data given below.
> > Please tell me how to add trendline in the graph.
> >
> > The script is as follows
> >
> > =================================== start
> > ====================================================
> >
> > # The data is as follows
> >
> > data <- c( 0.01,  0.02, 0.04, 0.13,  0.17 , 0.19 , 0.21 , 0.27 , 0.27 ,
> > 0.28,  0.29,  0.37,
> >           0.41,  0.49,  0.51,  0.52,  0.54,  0.57,  0.62,  0.63,  0.68,
> > 0.73,  0.74, 0.79,
> >           0.81,  0.81,  0.82,  0.86,  0.94,  0.96,  1.02,  1.10,  1.10,
> > 1.20,  1.29,  1.36,
> >           1.40,  1.41,  1.44,  1.45,  1.62,  1.67,  1.69,  1.78,  1.82,
> > 2.11,  2.13,  2.14,
> >           2.24,  2.29,  2.34, 2.40,  2.46,  2.70,  2.83,  2.98,  3.00,
> > 3.30,  3.53,  3.70,
> >           3.86,  3.90,  3.91,  3.98,  5.01,  5.23,  6.05,  6.12, 10.41,
> > 10.73)
> >
> > # P-P plot
> > average       <- mean(data)
> > lambda        <- (1/average)
> > e             <- c(1:70)
> > f             <- c((e-.5)/70)
> > Fx            <- c(1 - exp(-lambda*data))
> > g             <- sort(Fx)
> > plot(f,g)
> >
> > ===================================== end
> > ====================================================
> >
> > Awaiting your positive reply.
> >
> > Regards.
> > Priti.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>


From bolker at ufl.edu  Mon Jun 19 14:38:58 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Mon, 19 Jun 2006 12:38:58 +0000 (UTC)
Subject: [R] MLE maximum number of parameters
References: <B2B70BDD-81EC-4043-A769-020A2AC16923@imperial.ac.uk>
Message-ID: <loom.20060619T143623-482@post.gmane.org>

Federico Calboli <f.calboli <at> imperial.ac.uk> writes:

> 
> Hi All,
> 
> I would like to know, is there a *ballpark* figure for how many  
> parameters the minimisation routines can cope with?
> 

  I think I would make a distinction between theoretical
and practical limits.  A lot depends on how fast your
objective function is, but I would say in general that
if your objective function has more than a few tens
of parameters you should probably be looking for some
more specialized optimization code (e.g. AD Model Builder,
finding a way to shoehorn it into existing routines
such as nlme/lmer, etc.).
   I would be happy to be corrected though.

  Ben


From murdoch at stats.uwo.ca  Mon Jun 19 14:39:48 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 19 Jun 2006 08:39:48 -0400
Subject: [R] can I call user-created functions without source() ?
In-Reply-To: <44969658.8050101@fz-rossendorf.de>
References: <XFMail.060619112519.Ted.Harding@nessie.mcc.ac.uk>	<44968952.40008@stats.uwo.ca>
	<44969658.8050101@fz-rossendorf.de>
Message-ID: <44969B14.8070208@stats.uwo.ca>

Just a few comments below on alternative ways to do the same things:

On 6/19/2006 8:19 AM, Joerg van den Hoff wrote:

> for short term usage of some specialized functions I have added some 
> lines to the `.Rprofile' in my home(!) directory as follows (probably 
> there are smarter solutions, but at least it works):
> 
> #source some temporary useful functions:
> fl <- dir(path='~/rfiles/current',patt='.*\\.R$',full.names=TRUE)
> for (i in fl) {cat(paste('source("',i,'")\n',sep="")); source(i)}
> rm(i,fl)

Another way to do this without worrying about overwriting some existing 
variables is

local({
fl <- ...
for (i in fl) ...
})

No need to remove fl and i at the end; they were created in a temporary 
environment, which was deleted at the end.

> 
> here, I have put all the temporary stuff in a single dedicated dir 
> `~/rfiles/current', but of course you can use several dirs in this way. 
> all files in this dir with names ending in `.R' are sourced on startup 
> of R. this roughly works like one of the directories on MATLAB's search 
> path: every function definition in this directory is  'understood' by R 
> (but everything is loaded into the workspace on startup, no matter, 
> whether you really need it in the end: no real `load on demand'). 

It's possible to have load on demand in R, and this is used in packages. 
  It's probably not worth the trouble to use it unless you're using a 
package.


one
> important difference, though: this is only sensible for function 
> definitions, not scripts ('executable programms' (which would directly 
> be executed on R startup, otherwise).
> and, contrary to matlab/octave, this is not dynamic: everything is read 
> in at startup, later modifications to the directories are not recognized 
> without explicitely sourcing the files again.

There isn't really any reasonable way around this.  I suppose some hook 
could be created to automatically read the file if the time stamp 
changes, but that's not really the R way of doing things:  generally in 
R active things are in the workspace, not on disk.  A good way to work 
is prepare things on disk, then when they are ready, explicitly import 
them into R.

> 
> if you in addition you want to load definitions from the startup 
> directory where you launch R (your project dir), the above could be 
> modified to:
> 
> #source some temporary useful functions from startup dir:
> fl <- dir(path=getwd(),patt='.*\\.R$',full.names=TRUE)
> for (i in fl) {cat(paste('source("',i,'")\n',sep="")); source(i)}
> rm(i,fl)
> 
> in this way you at least don't need a separate `.Rprofile' in each 
> project dir.

Another alternative if you want something special in the project is to 
create a .Rprofile file there, and put source("~/.Rprofile") into it, so 
both the local changes and the general ones get loaded.

Duncan Murdoch
> 
> 
> 
> joerg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From rab45+ at pitt.edu  Mon Jun 19 07:36:03 2006
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Mon, 19 Jun 2006 01:36:03 -0400
Subject: [R] lmer and mixed effects logistic regression
In-Reply-To: <449431F1.6000906@pdf.com>
References: <1150293574.3416.11.camel@localhost.localdomain>
	<449431F1.6000906@pdf.com>
Message-ID: <1150695364.3340.8.camel@localhost.localdomain>

On Sat, 2006-06-17 at 09:46 -0700, Spencer Graves wrote:
> 
> 'lmer' RETURNS AN ERROR WHEN SAS NLMIXED RETURNS AN ANSWER
> 
> 	  Like you, I would expect lmer to return an answer when SAS
> NLMIXED does, and I'm concerned that it returns an error message instead.
> 
> 	  Your example is not self contained, and I've been unable to
> get the result you got.  This could occur for several reasons.  First,
> what do you get for "sessionInfo()"?  I got the following:
> 
> sessionInfo()
> Version 2.3.1 (2006-06-01)
> i386-pc-mingw32
> 

I will try to create an example after updating the packages. I can't
send the actual data - the data I sent was made up.

Rick B.


From petr.pikal at precheza.cz  Mon Jun 19 14:49:57 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 19 Jun 2006 14:49:57 +0200
Subject: [R] saving rounded numbers as a new variable in a dataframe
In-Reply-To: <BAY112-F2329C5DD1652A0D346AD6FAF860@phx.gbl>
References: <5.1.0.14.0.20060619220254.00bdf038@pop3.brisnet.org.au>
Message-ID: <4496B995.3253.159B334@localhost>

Hi



On 19 Jun 2006 at 8:20, Neuro LeSuperH?ros wrote:

From:           	Neuro LeSuperH?ros <neuro3000 at hotmail.com>
To:             	bgreen at dyson.brisnet.org.au, r-help at stat.math.ethz.ch
Date sent:      	Mon, 19 Jun 2006 08:20:16 -0400
Subject:        	Re: [R] saving rounded numbers as a new variable in a dataframe

> Hi
> 
> 1-Please read this: R Data Import/Export 
> cran.r-project.org/doc/manuals/R-data.pdf
> 2-This is the code:
> 
> rounded <-round(numurder, 0)
> write.csv(rounded,"c:/ngri.csv")

this will erase former ngri.csv and write a new ngri.csv containing 
only rounded variable!!!

Let ngri is data frame from which you have murder and other variables

then

write.csv(data.frame(nmurder=rounded, ngri),"c:/ngri.csv")

shall be better choice.

HTH
Petr



> 
> Neuro
> 
> 
> >From: Bob Green <bgreen at dyson.brisnet.org.au>
> >To: r-help at stat.math.ethz.ch
> >Subject: [R] saving rounded numbers as a new variable in a dataframe
> >Date: Mon, 19 Jun 2006 22:11:57 +1000
> >
> >
> >
> >A basic question, but one that eludes me. I have created a new
> >variable $numurder, which I have rounded off. I want to save the
> >rounded off version of this variable to an existing datafile called
> >'ngri.csv' .
> >
> >   numurder   <-c((murder*no.of.cases)/100)
> >[[1]]
> >   [1]  48.952 112.073 182.160 974.610 122.140 663.432 150.856 
> >   18.988
> >137.925 198.045  68.930 203.148  30.056 100.955
> >
> >
> >round(numurder, 0)
> >   [1]  49 112 182 975 122 663 151  19 138 198  69 203  30 101
> >
> >
> >Advice regarding how to do this is appreciated,
> >
> >Bob
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> >http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From h.wickham at gmail.com  Mon Jun 19 14:51:07 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 19 Jun 2006 14:51:07 +0200
Subject: [R] Function hints
Message-ID: <f8e6ff050606190551o24a4573ak4454473b5dbf8199@mail.gmail.com>

One of the recurring themes in the recent UserR conference was that
many people find it difficult to find the functions they need for a
particular task.  Sandy Weisberg suggested a small idea he would like
to see: a hints function that given an object, lists likely
operations.  I've done my best to implement this function using the
tools currently available in R, and my code is included at the bottom
of this email (I hope that I haven't just duplicated something already
present in R).  I think Sandy's idea is genuinely useful, even in the
limited form provided by my implementation, and I have already
discovered a few useful functions that I was unaware of.

While developing and testing this function, I ran into a few problems
which, I think, represent underlying problems with the current
documentation system.  These are typified by the results of running
hints on a object produced by glm (having class c("glm", "lm")).  I
have outlined (very tersely) some possible solutions.  Please note
that while these solutions are largely technological, the problem is
at heart sociological: writing documentation is no easier (and perhaps
much harder) than writing a scientific publication, but the rewards
are fewer.

Problems:

 * Many functions share the same description (eg. head, tail).
Solution: each rdoc file should only describe one method. Problem:
Writing rdoc files is tedious, there is a lot of information
duplicated between the code and the documenation (eg. the usage
statement) and some functions share a lot of similar information.
Solution: make it easier to write documentation (eg. documentation
inline with code), and easier to include certain common descriptions
in multiple methods (eg. new include command)

 * It is difficult to tell which functions are commonly
used/important. Solution: break down by keywords. Problem: keywords
are not useful at the moment.  Solution:  make better list of keywords
available and encourage people to use it.  Problem: people won't
unless there is a strong incentive, plus good keywording requires
considerable expertise (especially in bulding up list).  This is
probably insoluable unless one person systematically keywords all of
the base packages.

 * Some functions aren't documented (eg. simulate.lm, formula.glm) -
typically, these are methods where the documentation is in the
generic.  Solution: these methods should all be aliased to the generic
(by default?), and R CMD check should be amended to check for this
situation.  You could also argue that this is a deficiency with my
function, and easily fixed by automatically referring to the generic
if the specific isn't documented.

 * It can't supply suggestions when there isn't an explicit method
(ie. .default is used), this makes it pretty useless for basic
vectors.  This may not really be a problem, as all possible operations
are probably too numerous to list.

 * Provides full name for function, when best practice is to use
generic part only when calling function.  However, getting precise
documentation may requires that full name.  I do the best I can
(returning the generic if specific is alias to a documentation file
with the same method name), but this reflects a deeper problem that
the name you should use when calling a function may be different to
the name you use to get documentation.

 * Can only display methods from currently loaded packages.  This is a
shortcoming of the methods function, but I suspect it is difficult to
find S3 methods without loading a package.

Relatively trivial problems:

 * Needs wide display to be effective.  Could be dealt with by
breaking description in a sensible manner (there may already by R code
to do this.  Please let me know if you know of any)

 * Doesn't currently include S4 methods.  Solution: add some more code
to wrap showMethods

 * Personally, I think sentence case is more aesthetically pleasing
(and more flexible) than title case.


Hadley


hints <- function(x) {
	db <- eval(utils:::.hsearch_db())
	if (is.null(db)) {
		help.search("abcd!", rebuild=TRUE, agrep=FALSE)
		db <- eval(utils:::.hsearch_db())
	}

	base <- db$Base
	alias <- db$Aliases
	key <- db$Keywords

	m <- all.methods(class=class(x))
	m_id <- alias[match(m, alias[,1]), 2]
	keywords <- lapply(m_id, function(id) key[key[,2] %in% id, 1])

	f.names <- cbind(m, base[match(m_id, base[,3]), 4])
	f.names <- unlist(lapply(1:nrow(f.names), function(i) {
		if (is.na(f.names[i, 2])) return(f.names[i, 1])
		a <- methodsplit(f.names[i, 1])
		b <- methodsplit(f.names[i, 2])
		
		if (a[1] == b[1]) f.names[i, 2] else f.names[i, 1]		
	}))
	
	hints <- cbind(f.names, base[match(m_id, base[,3]), 5])
	hints <- hints[order(tolower(hints[,1])),]
	hints <- rbind(    c("--------", "---------------"), hints)
	rownames(hints) <- rep("", nrow(hints))
	colnames(hints) <- c("Function", "Task")
	hints[is.na(hints)] <- "(Unknown)"
	
	class(hints) <- "hints"
	hints
}

print.hints <- function(x, ...) print(unclass(x), quote=FALSE)

all.methods <- function(classes) {
	methods <- do.call(rbind,lapply(classes, function(x) {
		m <- methods(class=x)
		t(sapply(as.vector(m), methodsplit)) #m[attr(m, "info")$visible]
	}))
	rownames(methods[!duplicated(methods[,1]),])
}

methodsplit <- function(m) {
	parts <- strsplit(m, "\\.")[[1]]
	if (length(parts) == 1) {
		c(name=m, class="")
	} else{
		c(name=paste(parts[-length(parts)], collapse="."), class=parts[length(parts)])
	}	
}


From rdpeng at gmail.com  Mon Jun 19 15:03:09 2006
From: rdpeng at gmail.com (Roger D. Peng)
Date: Mon, 19 Jun 2006 09:03:09 -0400
Subject: [R] MLE maximum number of parameters
In-Reply-To: <B2B70BDD-81EC-4043-A769-020A2AC16923@imperial.ac.uk>
References: <B2B70BDD-81EC-4043-A769-020A2AC16923@imperial.ac.uk>
Message-ID: <4496A08D.3090904@gmail.com>

It really depends on how well-behaved your objective function is, but I've been 
able to fit a few models with 10--15 parameters.  But I felt like I was 
stretching the limit there.

-roger

Federico Calboli wrote:
> Hi All,
> 
> I would like to know, is there a *ballpark* figure for how many  
> parameters the minimisation routines can cope with?
> 
> I'm asking because I was asked if I knew.
> 
> Cheers,
> 
> Federico
> 
> --
> Federico C. F. Calboli
> Department of Epidemiology and Public Health
> Imperial College, St. Mary's Campus
> Norfolk Place, London W2 1PG
> 
> Tel +44 (0)20 75941602   Fax +44 (0)20 75943193
> 
> f.calboli [.a.t] imperial.ac.uk
> f.calboli [.a.t] gmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/


From elatine at gmail.com  Mon Jun 19 16:17:40 2006
From: elatine at gmail.com (=?ISO-8859-1?Q?B=E1lint_Cz=FAcz?=)
Date: Mon, 19 Jun 2006 16:17:40 +0200
Subject: [R] multivariate splits
Message-ID: <fab4bcf70606190717r45a2030dg@mail.gmail.com>

Dear R users!

Does someone know about any algorithms / packages in R, that perform
classification / regression / decision trees using multivariate
splits?

I have done some research, but I found nothing. Packages "tree" and
"rpart" seem only to be able to do CART with univariate splits.

Thank you for your help!

B?lint

-- 
Cz?cz B?lint
PhD hallgat?
BCE KTK Talajtan ?s V?zgazd?lkod?s Tansz?k
1118 Budapest, Vill?nyi ?t 29-43.


From j.van_den_hoff at fz-rossendorf.de  Mon Jun 19 16:19:51 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Mon, 19 Jun 2006 16:19:51 +0200
Subject: [R] can I call user-created functions without source() ?
In-Reply-To: <44969B14.8070208@stats.uwo.ca>
References: <XFMail.060619112519.Ted.Harding@nessie.mcc.ac.uk>	<44968952.40008@stats.uwo.ca>
	<44969658.8050101@fz-rossendorf.de> <44969B14.8070208@stats.uwo.ca>
Message-ID: <4496B287.3070408@fz-rossendorf.de>

Duncan Murdoch wrote:
> Just a few comments below on alternative ways to do the same things:
> 
> On 6/19/2006 8:19 AM, Joerg van den Hoff wrote:
> 
>> for short term usage of some specialized functions I have added some 
>> lines to the `.Rprofile' in my home(!) directory as follows (probably 
>> there are smarter solutions, but at least it works):
>>
>> #source some temporary useful functions:
>> fl <- dir(path='~/rfiles/current',patt='.*\\.R$',full.names=TRUE)
>> for (i in fl) {cat(paste('source("',i,'")\n',sep="")); source(i)}
>> rm(i,fl)
> 
> Another way to do this without worrying about overwriting some existing 
> variables is
> 
> local({
> fl <- ...
> for (i in fl) ...
> })

> 
> No need to remove fl and i at the end; they were created in a temporary 
> environment, which was deleted at the end.
> 
sure, that's better (just one more case, where I did'nt know of the 
existence of a certain function). but what is the difference (with 
regards to scope) of `i' or `fl' and the functions defined via sourcing? 
are'nt both objects defined within `local'? why _are_ the functions 
visible in the workspace? probably I again don't understand the 
`eval'/environment intricacies.
>>
>> here, I have put all the temporary stuff in a single dedicated dir 
>> `~/rfiles/current', but of course you can use several dirs in this 
>> way. all files in this dir with names ending in `.R' are sourced on 
>> startup of R. this roughly works like one of the directories on 
>> MATLAB's search path: every function definition in this directory is  
>> 'understood' by R (but everything is loaded into the workspace on 
>> startup, no matter, whether you really need it in the end: no real 
>> `load on demand'). 
> 
> It's possible to have load on demand in R, and this is used in packages. 
>  It's probably not worth the trouble to use it unless you're using a 
> package.
> 
> 
> one
>> important difference, though: this is only sensible for function 
>> definitions, not scripts ('executable programms' (which would directly 
>> be executed on R startup, otherwise).
>> and, contrary to matlab/octave, this is not dynamic: everything is 
>> read in at startup, later modifications to the directories are not 
>> recognized without explicitely sourcing the files again.
> 
> There isn't really any reasonable way around this.  I suppose some hook 
> could be created to automatically read the file if the time stamp 
> changes, but that's not really the R way of doing things:  generally in 
> R active things are in the workspace, not on disk.  A good way to work 
> is prepare things on disk, then when they are ready, explicitly import 
> them into R.
> 
>>
>> if you in addition you want to load definitions from the startup 
>> directory where you launch R (your project dir), the above could be 
>> modified to:
>>
>> #source some temporary useful functions from startup dir:
>> fl <- dir(path=getwd(),patt='.*\\.R$',full.names=TRUE)
>> for (i in fl) {cat(paste('source("',i,'")\n',sep="")); source(i)}
>> rm(i,fl)
>>
>> in this way you at least don't need a separate `.Rprofile' in each 
>> project dir.
> 
> Another alternative if you want something special in the project is to 
> create a .Rprofile file there, and put source("~/.Rprofile") into it, so 
> both the local changes and the general ones get loaded.
> 
> Duncan Murdoch
>>
>>
>>
>> joerg
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>


From rab45+ at pitt.edu  Mon Jun 19 16:14:14 2006
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Mon, 19 Jun 2006 10:14:14 -0400
Subject: [R] lmer and mixed effects logistic regression
In-Reply-To: <40e66e0b0606180458p177fa824j6a181fd52c3d1277@mail.gmail.com>
References: <1150293574.3416.11.camel@localhost.localdomain>
	<449431F1.6000906@pdf.com>
	<40e66e0b0606180458p177fa824j6a181fd52c3d1277@mail.gmail.com>
Message-ID: <1150726454.3200.20.camel@localhost.localdomain>

On Sun, 2006-06-18 at 13:58 +0200, Douglas Bates wrote:
> If I understand correctly Rick it trying to fit a model with random
> effects on a binary response when there are either 1 or 2 observations
> per group.  I think that is very optimistic because there is so little
> information available per random effect (exactly 1 or 2 bits of
> information per random effect). I'm not surprised that there are
> difficulties in fitting such a model.
> 
> Rick should try to monitor the iterations to see what is happening to
> the parameter estimates and perhaps should try the Laplace
> approximation to the log-likelihood.  This is done by adding method =
> "Laplace" and control = list(msVerbose = TRUE) to the call to lmer.
> This causes the PQL iterations to be followed by direct optimization
> of the Laplace approximation.  Because the problem is probably in the
> PQL iterations Rick may want to turn those off and do direct
> optimization of the Laplace approximation only.  In that case the
> control argument should be list(usePQL=FALSE, msVerbose = TRUE)
> 
> 
> On 6/17/06, Spencer Graves <spencer.graves at pdf.com> wrote:
> >
> >
> > 'lmer' RETURNS AN ERROR WHEN SAS NLMIXED RETURNS AN ANSWER
> >
> >           Like you, I would expect lmer to return an answer when SAS
> > NLMIXED does, and I'm concerned that it returns an error message instead.
> >
> >           Your example is not self contained, and I've been unable to
> > get the result you got.  This could occur for several reasons.  First,
> > what do you get for "sessionInfo()"?  I got the following:
> >
> > sessionInfo()
> > Version 2.3.1 (2006-06-01)
> > i386-pc-mingw32
> >
> > attached base packages:
> > [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> > [7] "base"
> >
> > other attached packages:
> >        lme4    lattice     Matrix
> >   "0.995-2"   "0.13-8" "0.995-10"
> >

I made an example data set that exhibits the error. There is a dump of
the data frame at the end.

First, I updated all my packages:

> sessionInfo()
Version 2.3.1 (2006-06-01)
i686-redhat-linux-gnu

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"
"datasets"
[7] "base"

other attached packages:
     chron       lme4     Matrix    lattice
   "2.3-3"  "0.995-2" "0.995-11"   "0.13-8"

But I still get the error.

For comparison, here is what glm gives:


> summary(glm(y~x,data=example.df,family=binomial))

Call:
glm(formula = y ~ x, family = binomial, data = example.df)

Deviance Residuals:
    Min       1Q   Median       3Q      Max
-1.6747  -0.9087  -0.6125   1.1447   2.0017

Coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  -0.4786     0.1227  -3.901 9.59e-05 ***
x             0.7951     0.1311   6.067 1.31e-09 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 436.63  on 324  degrees of freedom
Residual deviance: 394.15  on 323  degrees of freedom
AIC: 398.15

Number of Fisher Scoring iterations: 4


Running lmer without any tweaks:

> (lmer(y~(1|id)+x,data=example.df,family=binomial))
Error in lmer(y ~ (1 | id) + x, data = example.df, family = binomial) :
        Leading minor of order 2 in downdated X'X is not positive
definite
In addition: Warning message:
nlminb returned message singular convergence (7)
 in: LMEopt(x = mer, value = cv)

Running lmer with list(msVerbose=TRUE):

> (lmer(y~(1|
id)+x,data=example.df,family=binomial,control=list(msVerbose=TRUE)))
  0     -545.002:  44801.6
  1     -545.002:  44801.6
  2     -545.002:  44801.6
  3     -545.003:  44801.9
  4     -545.014:  44805.2
  5     -545.123:  44838.3
  6     -546.208:  45168.3
  7     -556.572:  48444.8
  8     -628.932:  78993.4
  9     -699.716:  127441.
 10     -771.102:  206437.
 11     -842.258:  333880.
 12     -913.501:  540319.
 13     -984.712:  874202.
 14     -1055.93: 1.41452e+06
 15     -1127.15: 2.28873e+06
 16     -1198.37: 3.70326e+06
 17     -1269.59: 5.99199e+06
 18     -1340.81: 9.69524e+06
 19     -1412.03: 1.56872e+07
 20     -1483.25: 2.53825e+07
 21     -1554.47: 4.10697e+07
 22     -1625.69: 6.64522e+07
 23     -1696.91: 1.07522e+08
 24     -1768.13: 1.73974e+08
 25     -1839.35: 2.81496e+08
 26     -1910.57: 4.55470e+08
 27     -1981.78: 7.36966e+08
 28     -2053.00: 1.19244e+09
 29     -2124.22: 1.92940e+09
 30     -2195.44: 3.12184e+09
 31     -2266.66: 5.05124e+09
 32     -2337.88: 8.17308e+09
 33     -2409.10: 1.32243e+10
 34     -2480.32: 2.13974e+10
 35     -2551.54: 3.46217e+10
 36     -2622.76: 5.60190e+10
 37     -2693.98: 9.06405e+10
 38     -2765.20: 1.46659e+11
 39     -2836.42: 2.37299e+11
 40     -2907.64: 3.83962e+11
 41     -2978.85: 6.21253e+11
 42     -3050.07: 1.00521e+12
 43     -3121.28: 1.62645e+12
 44     -3192.47: 2.63147e+12
 45     -3263.70: 4.25757e+12
 46     -3334.89: 6.88953e+12
 47     -3406.11: 1.11441e+13
 48     -3477.22: 1.80392e+13
 49     -3548.36: 2.91492e+13
 50     -3619.76: 4.72269e+13
 51     -3690.52: 7.63668e+13
 52     -3761.36: 1.23295e+14
 53     -3832.63: 1.99577e+14
 54     -3900.88: 3.22856e+14
 55     -3968.08: 4.97009e+14
  0     -4067.06: 1.67844e+15
  1     -4067.06: 1.67844e+15
  0     -4265.60: 5.77607e+15
  1     -4265.60: 5.77607e+15
  0     -4474.52: 1.96098e+16
  1     -4474.52: 1.96098e+16
  0     -4723.57: 6.68597e+16
  1     -4723.57: 6.68597e+16
  0     -4985.37: 2.20089e+17
  1     -4985.37: 2.20089e+17
  0     -5268.68: 7.69417e+17
  1     -5268.68: 7.69417e+17
  0     -5536.64: 2.48775e+18
  1     -5536.64: 2.48775e+18
  0     -5853.10: 8.45248e+18
  1     -5853.10: 8.45248e+18
  0     -6197.46: 3.00106e+19
  1     -6197.46: 3.00106e+19
  0     -6400.09: 8.72855e+19
  1     -6400.09: 8.72855e+19
  0     -6769.87: 3.19354e+20
  1     -6769.87: 3.19354e+20
  0     -7085.60: 1.14993e+21
  1     -7085.60: 1.14993e+21
  0     -7414.58: 4.43964e+21
  1     -7414.58: 4.43964e+21
  0     -7665.61: 1.61085e+22
  1     -7665.61: 1.61085e+22
Error in lmer(y ~ (1 | id) + x, data = example.df, family = binomial,  :
        Leading minor of order 2 in downdated X'X is not positive
definite
In addition: Warning message:
nlminb returned message singular convergence (7)
 in: LMEopt(x = mer, value = cv)


Running lmer with method="Laplace" and
control=list(usePQL=FALSE,msVerbose=TRUE):

> (lmer(y~(1|id)+x,data=example.df,family=binomial,method="Laplace",
+   control=list(usePQL=FALSE,msVerbose=TRUE)))
  0      347.321: -0.478643 0.795145  1.45231
  1      334.637: -0.775380  1.49795  2.09885
  2      326.045: -0.631955 0.917513  2.90042
  3      307.930: -0.627581  1.85085  4.66928
  4      304.717: -1.06671  1.40101  5.11069
  5      299.588: -1.05336  1.85102  5.73305
  6      297.157: -0.682292  1.60623  6.35949
  7      282.629: -1.33421  1.86152  10.2167
  8      270.279: -1.44945  2.72297  14.8450
  9      263.248: -1.61188  3.21518  19.5257
 10      254.336: -1.89092  4.01520  29.0932
 11      248.253: -2.13096  4.72573  39.9024
 12      243.359: -2.39747  5.49392  53.8331
 13      239.255: -2.66754  6.31763  71.9027
 14      235.865: -2.91894  7.17523  94.3541
 15      232.831: -3.14279  8.11396  123.501
 16      230.229: -3.32800  9.12440  159.978
 17      227.957: -3.45824  10.1876  205.312
 18      225.987: -3.50977  11.2006  258.137
 19      223.822: -3.42383  12.2016  327.929
 20      222.281: -3.29714  12.9668  393.939
 21      218.687: -2.35417  15.1107  657.987
 22      217.978: -2.00284  15.3087  724.381
 23      216.828: -1.03243  15.3436  883.159
 24      216.641: -0.727910  15.0860  924.584
 25      216.561: -0.634457  14.8052  935.901
 26      216.477: -0.670831  14.4966  934.259
 27      216.335: -0.882568  14.1066  925.552
 28      216.153: -1.24388  13.9061  926.647
 29      215.914: -1.70066  14.0769  966.092
 30      215.643: -2.07605  14.7379  1073.14
 31      215.365: -2.25220  15.8379  1261.63
 32      215.169: -2.20650  16.9633  1485.79
 33      215.065: -2.05998  17.7714  1685.40
 34      214.993: -1.85386  18.2239  1859.43
 35      214.948: -1.69235  18.3198  1985.48
 36      214.933: -1.65586  18.2629  2051.34
 37      214.933: -1.65578  18.2629  2051.34
 38      214.933: -1.65579  18.2629  2051.34
 39      214.933: -1.65586  18.2629  2051.34
 40      214.933: -1.65654  18.2625  2051.34
 41      214.932: -1.66423  18.2585  2051.33
 42      214.931: -1.70783  18.2351  2051.33
 43      214.931: -1.73215  18.2201  2051.43
 44      214.931: -1.74205  18.2078  2051.65
 45      214.930: -1.73708  18.2686  2076.43
 46      214.929: -1.73209  18.3805  2120.39
 47      214.929: -1.73283  18.3612  2112.76
 48      214.929: -1.73334  18.3600  2112.79
 49      214.929: -1.73332  18.3600  2112.79
 50      214.929: -1.73332  18.3600  2112.79
 51      214.929: -1.73332  18.3600  2112.79
 52      214.929: -1.73332  18.3600  2112.79
 53      214.929: -1.73332  18.3600  2112.79
 54      214.929: -1.73332  18.3600  2112.79
Generalized linear mixed model fit using Laplace
Formula: y ~ (1 | id) + x
          Data: example.df
 Family: binomial(logit link)
      AIC      BIC    logLik deviance
 220.9293 232.2807 -107.4646 214.9293
Random effects:
 Groups Name        Variance Std.Dev.
 id     (Intercept) 2112.8   45.965
number of obs: 325, groups: id, 177

Estimated scale (compare to 1)  0.06664838

Fixed effects:
            Estimate Std. Error  z value Pr(>|z|)
(Intercept)  -1.7333     5.7142 -0.30333  0.76164
x            18.3600     7.3318  2.50416  0.01227 *
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
  (Intr)
x -0.382

Note that the results for x don't agree at all with what glm outputs.
The estimated scale is very small and the sd for id appears to be very
large.


Now changing method="Laplace" to method="ML":

> (lmer(y~(1|id)+x,data=example.df,family=binomial,method="ML",
+   control=list(usePQL=FALSE,msVerbose=TRUE)))
Generalized linear mixed model fit using PQL
Formula: y ~ (1 | id) + x
          Data: example.df
 Family: binomial(logit link)
      AIC      BIC    logLik deviance
 353.3209 364.6724 -173.6604 347.3209
Random effects:
 Groups Name        Variance Std.Dev.
 id     (Intercept) 1.4523   1.2051
number of obs: 325, groups: id, 177

Estimated scale (compare to 1)  0.2372670

Fixed effects:
            Estimate Std. Error z value  Pr(>|z|)
(Intercept) -0.47864    0.16114 -2.9703  0.002975 **
x            0.79514    0.16872  4.7128 2.444e-06 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
  (Intr)
x -0.129

The estimated coefficients are the same as glm to 4 decimal places. The
se's are about 30% larger than for glm. The sd for id is much smaller
and the scale is larger.

If I try to turn PQL back on I get the error message.


I used ML and PQL off on the original data set and the results are
ROUGHLY similar to what SAS NLMIXED gives but the coefficient for x is
about 20% lower than NLMIXED. I haven't had a chance to run NLMIXED on
the example data frame yet.

Finally, besides my thanks for the help and apologies for the length of
this post, here is the dump of the data frame:


example.df <-
structure(list(id = structure(as.integer(c(1, 1, 2, 2, 3, 3,
4, 4, 5, 5, 6, 6, 7, 7, 8, 9, 9, 10, 10, 11, 11, 12, 12, 13,
14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20, 21, 21,
22, 22, 23, 23, 24, 24, 25, 25, 26, 26, 27, 28, 29, 29, 30, 30,
31, 31, 32, 32, 33, 33, 34, 34, 35, 35, 36, 36, 37, 37, 38, 38,
39, 39, 40, 40, 41, 42, 42, 43, 43, 44, 45, 45, 46, 46, 47, 47,
48, 48, 49, 49, 50, 50, 51, 51, 52, 52, 53, 53, 54, 54, 55, 55,
56, 56, 57, 57, 58, 58, 59, 59, 60, 61, 61, 62, 62, 63, 63, 64,
64, 65, 65, 66, 66, 67, 67, 68, 69, 69, 70, 70, 71, 71, 72, 72,
73, 73, 74, 75, 75, 76, 76, 77, 77, 78, 78, 79, 79, 80, 81, 81,
82, 82, 83, 83, 84, 85, 85, 86, 86, 87, 87, 88, 88, 89, 89, 90,
90, 91, 91, 92, 92, 93, 94, 95, 95, 96, 97, 97, 98, 98, 99, 99,
100, 101, 101, 102, 102, 103, 103, 104, 104, 105, 105, 106, 106,
107, 107, 108, 108, 109, 109, 110, 111, 111, 112, 112, 113, 113,
114, 114, 115, 116, 116, 117, 118, 118, 119, 120, 120, 121, 121,
122, 123, 123, 124, 124, 125, 125, 126, 126, 127, 127, 128, 128,
129, 129, 130, 131, 131, 132, 133, 133, 134, 134, 135, 136, 136,
137, 138, 138, 139, 139, 140, 140, 141, 141, 142, 142, 143, 143,
144, 144, 145, 145, 146, 146, 147, 148, 148, 149, 149, 150, 150,
151, 151, 152, 152, 153, 153, 154, 154, 155, 155, 156, 157, 157,
158, 159, 160, 161, 161, 162, 162, 163, 163, 164, 164, 165, 165,
166, 166, 167, 167, 168, 168, 169, 169, 170, 170, 171, 171, 172,
172, 173, 173, 174, 174, 175, 175, 176, 176, 177, 177)), .Label = c("1",
"2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13",
"14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
"25", "26", "27", "28", "29", "30", "31", "32", "33", "34", "35",
"36", "37", "38", "39", "40", "41", "42", "43", "44", "45", "46",
"47", "48", "49", "50", "51", "52", "53", "54", "55", "56", "57",
"58", "59", "60", "61", "62", "63", "64", "65", "66", "67", "68",
"69", "70", "71", "72", "73", "74", "75", "76", "77", "78", "79",
"80", "81", "82", "83", "84", "85", "86", "87", "88", "89", "90",
"91", "92", "93", "94", "95", "96", "97", "98", "99", "100",
"101", "102", "103", "104", "105", "106", "107", "108", "109",
"110", "111", "112", "113", "114", "115", "116", "117", "118",
"119", "120", "121", "122", "123", "124", "125", "126", "127",
"128", "129", "130", "131", "132", "133", "134", "135", "136",
"137", "138", "139", "140", "141", "142", "143", "144", "145",
"146", "147", "148", "149", "150", "151", "152", "153", "154",
"155", "156", "157", "158", "159", "160", "161", "162", "163",
"164", "165", "166", "167", "168", "169", "170", "171", "172",
"173", "174", "175", "176", "177"), class = "factor"), y =
structure(as.integer(c(1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2,
2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1,
1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2,
2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1,
1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,
1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1,
1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2,
2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2,
2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2,
2, 1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2,
2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1,
1, 1, 1, 2, 2, 1, 1, 1, 1)), .Label = c("0", "1"), class = "factor"),
    x = c(0.896492660264945, 0.896492660264945, 1.59446707642661,
    1.59446707642661, -1.05008338359102, -1.05008338359102,
1.09348658068790,
    1.09348658068790, 1.12507994528403, 1.12507994528403,
0.276572438987850,
    0.276572438987850, 0.434273771509725, 0.434273771509725,
    2.09093423436586, -0.622643744937437, -0.622643744937437,
    -0.58706802345943, -0.58706802345943, 0.124446406100572,
    0.124446406100572, 0.126570329770903, 0.126570329770903,
    0.181261364281855, 1.64039692579746, 1.64039692579746,
-0.555474658863302,
    -0.555474658863302, 0.47542479262234, 0.47542479262234,
-0.258656325934905,
    -0.258656325934905, 1.64995458231394, 1.64995458231394,
0.696047363877697,
    0.696047363877697, -1.46716889435175, -1.46716889435175,
    1.03375122745992, 1.03375122745992, 0.790827457666109,
0.790827457666109,
    -1.53194856629677, -1.53194856629677, -1.69389774615931,
    -1.69389774615931, -0.811141970679076, -0.811141970679076,
    -0.582289195201196, -0.582289195201196, 0.00789609469130197,
    0.573390771916238, -1.45628378554133, -1.45628378554133,
    -1.16079290490689, -1.16079290490689, -0.832646697841153,
    -0.832646697841153, -0.241930427031072, -0.241930427031072,
    -1.29353813430241, -1.29353813430241, -0.663794766050042,
    -0.663794766050042, -0.961940551272396, -0.961940551272396,
    1.59499805734419, 1.59499805734419, 0.47144243574048,
0.47144243574048,
    0.952245656611064, 0.952245656611064, -0.304586175305754,
    -0.304586175305754, 0.71463169599307, 0.71463169599307,
-1.32141463247548,
    -0.0983000888251169, -0.0983000888251169, 0.440114561603134,
    0.440114561603134, -1.75761545626916, -0.409985887445808,
    -0.409985887445808, -0.847514163533447, -0.847514163533447,
    -1.09229636653880, -1.09229636653880, -1.00415353422017,
    -1.00415353422017, -1.63681729751924, -1.63681729751924,
    -1.74354446195324, -1.74354446195324, -1.65460515825824,
    -1.65460515825824, -1.30760912861834, -1.30760912861834,
    -1.38300841891499, -1.38300841891499, -0.628750025489628,
    -0.628750025489628, 0.323564250193864, 0.323564250193864,
    -0.524412275184748, -0.524412275184748, -0.486181649118838,
    -0.486181649118838, 0.142234266839580, 0.142234266839580,
    -1.74965074250543, -1.74965074250543, -0.299010875671146,
    2.01049062535218, 2.01049062535218, -1.18229763206896,
-1.18229763206896,
    0.83304044061388, 0.83304044061388, -1.44539867673089,
-1.44539867673089,
    -0.391136064871645, -0.391136064871645, -0.118477363693237,
    -0.118477363693237, -1.73531425773072, -1.73531425773072,
    0.748083493800747, -1.70717226909887, -1.70717226909887,
    -0.210602552893726, -0.210602552893726, 0.681976369561778,
    0.681976369561778, -0.0138741229295563, -0.0138741229295563,
    -0.532111498489687, -0.532111498489687, -0.585740571165474,
    -0.202106858212414, -0.202106858212414, -0.121663249198723,
    -0.121663249198723, -0.328214826138161, -0.328214826138161,
    -0.94468367145097, -0.94468367145097, -1.4807089077501,
-1.4807089077501,
    1.09083167609999, 1.15215997208072, 1.15215997208072,
1.55411252669037,
    1.55411252669037, 0.0299318027709619, 0.0299318027709619,
    -0.0913973368965427, -1.40716805066498, -1.40716805066498,
    -0.246178274371723, -0.246178274371723, 0.473035378493218,
    0.473035378493218, 0.221084933100514, 0.221084933100514,
    -0.297152442459607, -0.297152442459607, 0.487106372809148,
    0.487106372809148, -0.434676500113372, -0.434676500113372,
    -1.18760744124478, -1.18760744124478, 0.937643681377551,
    1.05737987829232, -0.0879459609322652, -0.0879459609322652,
    0.310289727254308, -0.9587546657669, -0.9587546657669,
1.61889219863538,
    1.61889219863538, 0.983573530748409, 0.983573530748409,
0.229580627781825,
    -0.394587440835922, -0.394587440835922, 1.27163067853669,
    1.27163067853669, 1.40649983160254, 1.40649983160254,
-0.275116734379947,
    -0.275116734379947, 1.86526734439348, 1.86526734439348,
1.72668132490455,
    1.72668132490455, 0.929147986696238, 0.929147986696238,
-0.738397584970334,
    -0.738397584970334, 1.38260569031136, 1.38260569031136,
0.869412633468261,
    0.426574548204786, 0.426574548204786, 0.906846788157797,
    0.906846788157797, 0.697109325712863, 0.697109325712863,
    1.11578777922635, 1.11578777922635, 1.36242841544324,
1.20101021649827,
    1.20101021649827, 1.37676490021795, 0.76480939270458,
0.76480939270458,
    1.86393989209952, 0.543124859614057, 0.543124859614057,
-0.379985465602419,
    -0.379985465602419, 1.04224692214123, 1.85411674512425,
1.85411674512425,
    -0.251753574006341, -0.251753574006341, 0.813394146663342,
    0.813394146663342, 0.405335311501501, 0.405335311501501,
    0.590913142196445, 0.590913142196445, -0.263435154193149,
    -0.263435154193149, -1.73690720048346, -1.73690720048346,
    1.55092664118487, 1.10649561316866, 1.10649561316866,
0.454716536836637,
    -0.675741836695642, -0.675741836695642, 0.91959033017976,
    0.91959033017976, -0.256532402264574, -0.383967822484279,
    -0.383967822484279, -0.7036183348687, -1.07955282451682,
    -1.07955282451682, -0.640431605676436, -0.640431605676436,
    -1.48389479325559, -1.48389479325559, 1.72747779628092,
1.72747779628092,
    -0.959816627602066, -0.959816627602066, 0.562771153564595,
    0.562771153564595, 0.830651026484758, 0.830651026484758,
    0.126039348853320, 0.126039348853320, -0.753265050662627,
    -0.753265050662627, 0.570735867328324, 1.56101527861893,
    1.56101527861893, -0.701228920739579, -0.701228920739579,
    0.272059101188397, 0.272059101188397, -0.570607615014388,
    -0.570607615014388, -1.05539319276684, -1.05539319276684,
    -1.09442029020913, -1.09442029020913, -1.68035773276096,
    -1.68035773276096, -0.523350313349583, -0.523350313349583,
    -0.142106014525635, -1.24256396621453, -1.24256396621453,
    0.440380052061925, -0.138389148102566, 0.354626633872418,
    0.294094809268057, 0.294094809268057, 1.84349712677261,
1.84349712677261,
    -1.02857865642895, -1.02857865642895, -0.0266176649515298,
    -0.0266176649515298, 0.699233249383193, 0.699233249383193,
    0.950387223399524, 0.950387223399524, 0.350113296072966,
    0.350113296072966, 0.440114561603134, 0.440114561603134,
    -0.487774591871586, -0.487774591871586, 1.14074388235270,
    1.14074388235270, 0.797199228677091, 0.797199228677091,
-0.831053755088405,
    -0.831053755088405, 0.477283225833879, 0.477283225833879,
    1.13384113042414, 1.13384113042414, 0.607108060182704,
0.607108060182704,
    0.191084511257124, 0.191084511257124, -1.54814348428303,
    -1.54814348428303)), .Names = c("id", "y", "x"), row.names = c("4",
"101", "5", "102", "6", "103", "7", "104", "1", "98", "8", "105",
"9", "106", "198", "199", "263", "10", "107", "11", "108", "200",
"264", "197", "12", "109", "201", "265", "202", "266", "203",
"267", "204", "268", "205", "269", "2", "99", "3", "100", "206",
"270", "16", "113", "17", "114", "18", "115", "19", "116", "117",
"20", "21", "118", "22", "119", "23", "120", "24", "121", "25",
"122", "26", "123", "27", "124", "28", "125", "29", "126", "30",
"127", "31", "128", "207", "271", "32", "33", "129", "208", "272",
"130", "34", "131", "35", "132", "36", "133", "37", "134", "38",
"135", "39", "136", "13", "110", "40", "137", "41", "138", "42",
"139", "209", "273", "43", "140", "44", "141", "210", "274",
"45", "142", "211", "46", "143", "14", "111", "212", "275", "47",
"144", "48", "145", "213", "276", "214", "277", "215", "49",
"146", "50", "147", "216", "278", "217", "279", "218", "280",
"51", "52", "148", "219", "281", "220", "282", "221", "283",
"53", "149", "222", "223", "284", "54", "150", "55", "151", "152",
"56", "153", "57", "154", "58", "155", "224", "285", "225", "286",
"226", "287", "15", "112", "59", "156", "289", "290", "228",
"291", "229", "60", "157", "230", "292", "231", "293", "294",
"232", "295", "233", "296", "61", "158", "234", "297", "235",
"298", "62", "159", "236", "299", "63", "160", "64", "161", "300",
"237", "301", "65", "162", "66", "163", "238", "302", "303",
"67", "164", "304", "68", "165", "239", "240", "305", "241",
"306", "307", "242", "308", "69", "166", "70", "167", "227",
"288", "243", "309", "73", "170", "74", "171", "247", "248",
"312", "249", "75", "172", "250", "313", "244", "76", "173",
"174", "77", "175", "251", "314", "78", "176", "252", "315",
"79", "177", "253", "316", "254", "317", "80", "178", "255",
"318", "319", "81", "179", "82", "180", "83", "181", "84", "182",
"85", "183", "86", "184", "71", "168", "256", "320", "185", "87",
"186", "257", "258", "321", "88", "187", "259", "322", "260",
"323", "245", "310", "261", "324", "89", "188", "90", "189",
"91", "190", "92", "191", "246", "311", "93", "192", "94", "193",
"95", "194", "96", "195", "262", "325", "97", "196", "72", "169"
), class = "data.frame")


Rick B.


From palacios at math.ucalgary.ca  Mon Jun 19 16:28:16 2006
From: palacios at math.ucalgary.ca (Luz Maria Palacios Derflingher)
Date: Mon, 19 Jun 2006 08:28:16 -0600 (MDT)
Subject: [R] help on nlm (gradient) (fwd)
Message-ID: <Pine.GSO.4.61.0606190827030.22720@ms1.math.ucalgary.ca>

Hello

No worries anymore. Figured it out.

Thanks for everything.

Luz

---------- Forwarded message ----------
Date: Sun, 18 Jun 2006 23:12:25 -0600 (MDT)
From: Luz Maria Palacios Derflingher <palacios at math.ucalgary.ca>
To: r-help at lists.R-project.org
Cc: r-help at stat.math.ethz.ch.
Subject: help on nlm (gradient)

Hello.

I am having some trouble using nlm in R for windows version 2.2.1. I have to 
minimise a function which includes in its insides the evaluation of 
integrals through the "adapt" function.
In order to make R work less, I calculated the gradient and coded it. I made 
sure that the code and function for the gradient worked.
So I know that the main function (the one to minimise,let's call it "func1") 
yields values when evaluated.
I know that the gradient I coded also works and yields values when evaluated 
(on its own); when I put it all together and use nlm I get the following 
message:

Error in nlm(func1, starting values,  :
         probable coding error in analytic gradient

Any light that can be shed on this would be highly appreciated.
Many thanks

Luz Palacios
PhD Candidate University of Calgary


From vincent at 7d4.com  Mon Jun 19 16:30:54 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Mon, 19 Jun 2006 16:30:54 +0200
Subject: [R] function call
In-Reply-To: <971536df0606190454l260311e6wef1c4e95d27a87ef@mail.gmail.com>
References: <44967421.3040007@7d4.com>
	<971536df0606190454l260311e6wef1c4e95d27a87ef@mail.gmail.com>
Message-ID: <4496B51E.2020208@7d4.com>

Gabor Grothendieck a ?crit :

> Perhaps you what you want to do is to return an object
> that has a print method like this:
> f1 <- function(x) structure(c(x, x^2), class = "f1")
> print.f1 <- function(x) cat("x is", x[1], "x squared is", x[2], "\n")

Thank you Gabor for this idea.
For the moment interactive() seems enough for my need,
but I'll keep also this way in mind.


From michelcias at gmail.com  Mon Jun 19 16:32:23 2006
From: michelcias at gmail.com (Michel Helcias)
Date: Mon, 19 Jun 2006 11:32:23 -0300
Subject: [R] eacf
Message-ID: <5e19a4710606190732o6d105b96hc38fa6e8aef7953b@mail.gmail.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060619/360ffa0f/attachment.pl 

From MyersDB at missouri.edu  Mon Jun 19 16:41:15 2006
From: MyersDB at missouri.edu (Myers, Brent)
Date: Mon, 19 Jun 2006 09:41:15 -0500
Subject: [R] Fitting Distributions Directly From a Histogram
In-Reply-To: <Pine.LNX.4.64.0606190833410.5220@gannet.stats.ox.ac.uk>
Message-ID: <BE4711F3229E3941AE4A2D115F20A3482AFABE@UM-XMAIL05.um.umsystem.edu>

I'm desperately uninformed about the methods you have been discussing, but for a non-parametric solution, how about splines?

Carl DeBoor (1978) outlines a method of area preserving quadratic splines to fit histograms. All you need to know is the value each bar represents and its interval. I am currently implementing this in S-Plus. This method is included in the spline toolbox for Matlab, which DeBoor authored.


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof Brian Ripley
Sent: Monday, June 19, 2006 2:41 AM
To: Spencer Graves
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Fitting Distributions Directly From a Histogram

On Sun, 18 Jun 2006, Spencer Graves wrote:

> 	  Won't 'interval censoring using 'survreg' in the 'survival' package
> handle this?
> (http://finzi.psych.upenn.edu/R/library/survival/html/survreg.html)

Yes, but only for distributions it supports (and I think even with a 
user-supplied distribution only for positive values).

Coding up a way to fit mles to grouped data would be a fairly simple 
exercise based on fitdistr, but finding suitable starting values for 
optimization would be more difficult.

It is quite rare these days for only the grouped data to be available, now 
most data collection is automated.  I did something like this for 
analytical chemists in the mid 1980s (pre-R and indeed pre-S for me), but 
their need had vanished by the time I started to port things to S.

>
> 	  Hope this helps.
> 	  Spencer Graves
>
> Vincent Goulet wrote:
>> Le Lundi 12 Juin 2006 06:51, Lorenzo Isella a ?crit :
>>> Dear All,
>>>
>>> A simple question: packages like fitdistr should be ideal to analyze
>>> samples of data taken from a univariate distribution, but what if
>>> rather than the raw data of the observations you are given directly
>>> and only a histogram?
>>
>> Let's assume that you have not only the histogram itself, but also the breaks
>> and the counts per bin. Then you have what grouped data --- at least that's
>> how we call those in Actuarial Science. Maximum likelyhood estimation is
>> feasible for such data, but it is slightly more complicated. "Loss Models" by
>> Klugman, Panjer & Willmot (Wiley) covers this.
>>
>> I'm now thinking of adding this to my actuarial science package "actuar"...
>>
>>> I was thinking about generating artificially a set of data
>>> corresponding to the counts binned in the histogram, but this sounds
>>> too cumbersome.
>>> Another question is the following: fitdistr provides the value of the
>>> log-likely hood function, but what if I want e.g. a chi square test to
>>> get some insight on the goodness of the fitting?
>>
>> Goodness of fit tests for grouped data are also covered in Loss Models.
>>
>>> I am sure there must be a way to get it straightforwardly without
>>> coding it myself.
>>
>> Once you have the theory, I'm afraid for now you will have to code the
>> estimation procedure yourself.
>>
>> Cheers.
>>
>>> Many thanks
>>>
>>> Lorenzo
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From murdoch at stats.uwo.ca  Mon Jun 19 16:41:17 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 19 Jun 2006 10:41:17 -0400
Subject: [R] can I call user-created functions without source() ?
In-Reply-To: <4496B287.3070408@fz-rossendorf.de>
References: <XFMail.060619112519.Ted.Harding@nessie.mcc.ac.uk>	<44968952.40008@stats.uwo.ca>	<44969658.8050101@fz-rossendorf.de>
	<44969B14.8070208@stats.uwo.ca> <4496B287.3070408@fz-rossendorf.de>
Message-ID: <4496B78D.5070107@stats.uwo.ca>

On 6/19/2006 10:19 AM, Joerg van den Hoff wrote:
> Duncan Murdoch wrote:
>> Just a few comments below on alternative ways to do the same things:
>> 
>> On 6/19/2006 8:19 AM, Joerg van den Hoff wrote:
>> 
>>> for short term usage of some specialized functions I have added some 
>>> lines to the `.Rprofile' in my home(!) directory as follows (probably 
>>> there are smarter solutions, but at least it works):
>>>
>>> #source some temporary useful functions:
>>> fl <- dir(path='~/rfiles/current',patt='.*\\.R$',full.names=TRUE)
>>> for (i in fl) {cat(paste('source("',i,'")\n',sep="")); source(i)}
>>> rm(i,fl)
>> 
>> Another way to do this without worrying about overwriting some existing 
>> variables is
>> 
>> local({
>> fl <- ...
>> for (i in fl) ...
>> })
> 
>> 
>> No need to remove fl and i at the end; they were created in a temporary 
>> environment, which was deleted at the end.
>> 
> sure, that's better (just one more case, where I did'nt know of the 
> existence of a certain function). but what is the difference (with 
> regards to scope) of `i' or `fl' and the functions defined via sourcing? 
> are'nt both objects defined within `local'? why _are_ the functions 
> visible in the workspace? probably I again don't understand the 
> `eval'/environment intricacies.

Whoops, sorry.  Yes, you'd need to be careful to assign them into 
globalenv().  Those ...'s weren't so simple.

Duncan Murdoch


From Torsten.Hothorn at rzmail.uni-erlangen.de  Mon Jun 19 17:04:40 2006
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Mon, 19 Jun 2006 17:04:40 +0200 (CEST)
Subject: [R] multivariate splits
In-Reply-To: <fab4bcf70606190717r45a2030dg@mail.gmail.com>
References: <fab4bcf70606190717r45a2030dg@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0606191703350.502@imbe153.imbe.med.uni-erlangen.de>


On Mon, 19 Jun 2006, B?lint Cz?cz wrote:

> Dear R users!
>
> Does someone know about any algorithms / packages in R, that perform
> classification / regression / decision trees using multivariate
> splits?
>
> I have done some research, but I found nothing. Packages "tree" and
> "rpart" seem only to be able to do CART with univariate splits.
>

have a look at the machine learning task view on CRAN, which will point 
you to package `RWeka'.

HTH,

Torsten

> Thank you for your help!
>
> B?lint
>
> -- 
> Cz?cz B?lint
> PhD hallgat?
> BCE KTK Talajtan ?s V?zgazd?lkod?s Tansz?k
> 1118 Budapest, Vill?nyi ?t 29-43.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

From a_elhabti at yahoo.fr  Mon Jun 19 17:29:03 2006
From: a_elhabti at yahoo.fr (Ahmed Elhabti)
Date: Mon, 19 Jun 2006 17:29:03 +0200 (CEST)
Subject: [R] Graph and function
Message-ID: <20060619152903.72708.qmail@web27803.mail.ukl.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060619/b2cb9782/attachment.pl 

From j.van_den_hoff at fz-rossendorf.de  Mon Jun 19 17:30:27 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Mon, 19 Jun 2006 17:30:27 +0200
Subject: [R] can I call user-created functions without source() ?
In-Reply-To: <4496B78D.5070107@stats.uwo.ca>
References: <XFMail.060619112519.Ted.Harding@nessie.mcc.ac.uk>	<44968952.40008@stats.uwo.ca>	<44969658.8050101@fz-rossendorf.de>
	<44969B14.8070208@stats.uwo.ca> <4496B287.3070408@fz-rossendorf.de>
	<4496B78D.5070107@stats.uwo.ca>
Message-ID: <4496C313.9080503@fz-rossendorf.de>

Duncan Murdoch wrote:
> On 6/19/2006 10:19 AM, Joerg van den Hoff wrote:
>> Duncan Murdoch wrote:
>>> Just a few comments below on alternative ways to do the same things:
>>>
>>> On 6/19/2006 8:19 AM, Joerg van den Hoff wrote:
>>>
>>>> for short term usage of some specialized functions I have added some 
>>>> lines to the `.Rprofile' in my home(!) directory as follows 
>>>> (probably there are smarter solutions, but at least it works):
>>>>
>>>> #source some temporary useful functions:
>>>> fl <- dir(path='~/rfiles/current',patt='.*\\.R$',full.names=TRUE)
>>>> for (i in fl) {cat(paste('source("',i,'")\n',sep="")); source(i)}
>>>> rm(i,fl)
>>>
>>> Another way to do this without worrying about overwriting some 
>>> existing variables is
>>>
>>> local({
>>> fl <- ...
>>> for (i in fl) ...
>>> })
>>
>>>
>>> No need to remove fl and i at the end; they were created in a 
>>> temporary environment, which was deleted at the end.
>>>
>> sure, that's better (just one more case, where I did'nt know of the 
>> existence of a certain function). but what is the difference (with 
>> regards to scope) of `i' or `fl' and the functions defined via 
>> sourcing? are'nt both objects defined within `local'? why _are_ the 
>> functions visible in the workspace? probably I again don't understand 
>> the `eval'/environment intricacies.
> 
> Whoops, sorry.  Yes, you'd need to be careful to assign them into 
> globalenv().  Those ...'s weren't so simple.
> 
no, no, you _were_ right (I tested it), but I did'nt understand why it 
works. I found the answer only now in the `source' manpage: there is an 
argument `local = FALSE' which by default enforces assignment into 
globalenv(). so the `...' remain unaltered :-). sorry for the confusion.

joerg

PS: and as a follow up on the recent 'setwd vs. cd' thread: as I noted 
in passing `source' has a further argument `chdir' to change the 
directory prior to sourcing, which is neither the usual name (using 
`cd') nor the R-way (using `setwd'), but rather identical to the 
corresponding native matlab/octave command (both of these packages have 
for quite some time changed to accepting `cd' as well). of course, the 
`source' arguments have nothing to do with the commands but consistency 
would'nt harm either :-). not important, but a bit funny.

> Duncan Murdoch


From buser at stat.math.ethz.ch  Mon Jun 19 17:33:21 2006
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Mon, 19 Jun 2006 17:33:21 +0200
Subject: [R] nested mixed-effect model: variance components
In-Reply-To: <17549.35359.437056.347681@stat.math.ethz.ch>
References: <BAY106-DAV244471184652FC1DD27A0BBC8B0@phx.gbl>
	<8b6f5b5d0778bbf210466186a494624d@hotmail.com>
	<17549.35359.437056.347681@stat.math.ethz.ch>
Message-ID: <17558.50113.94684.791297@stat.math.ethz.ch>

Dear John

I've put your mail back to the R list since I have no
explanation for the "lmer" result. Maybe someone else has an
idea. I adapted it to show some else that I do not understand. 

## Creation of the data (habitat is nested in lagoon):
set.seed(1)
dat <- data.frame(y = rnorm(100), lagoon = factor(rep(1:4,each = 25)),
                  habitat = factor(rep(1:20, each = 5)))

## I do not understand how the random intercepts for lagoon and
## lagoon:habitat can both be estimated. It seems a little bit
## strange to me that they are identical (0.46565).
library(lme4)
summary(reg3 <- lmer(y~habitat+(1|lagoon)+(1|lagoon:habitat), data = dat))

## Furthermore I do not understand why the standard errors for
## the fixed effect of habitat are 1.131 for some habitats
## and 1.487 for the others???


## If one removes (1|lagoon), the variance component
## (1|lagoon:habitat) does not change its value (still 0.46565)???
summary(reg3a <- lmer(y~habitat+(1|lagoon:habitat), data = dat))

## Now all standard errors for the fixed factor habitat are 1.131.

## Altogether it seems a little bit strange to me and with the
## warnings and errors of the lme and aov call, I'd be carefull
## by using the output of lmer in that case. In addition I do
## not understand the interpretation of the random effect lagoon
## in top of the nested FIXED factor habitat.

summary(aov(y~habitat + Error(lagoon/habitat), data = dat))

detach(package:Matrix)
detach(package:lme4)
library(nlme)
summary(reg2 <- lme(y~habitat, random = ~1|lagoon/habitat, data = dat))
anova(reg2)


Best regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH Zurich	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------



>> Christoph,
>> 
>> I am sending this off list bacause I tried 'lmer'and
>> it seems to work with your contrived data,but I don't know why.
>> Can you explain it ?
>> 
>> 
>> John
>> 
>> 
>> 
>> 
>> > detach(package:nlme)
>> > library(Matrix)
>> 
>> summary(lmer(y~habitat+(1|lagoon)+(1|lagoon:habitat), data = dat))
>> 
>> Linear mixed-effects model fit by REML
>> Formula: y ~ habitat + (1 | lagoon) + (1 | lagoon:habitat)
>>           Data: dat
>>       AIC      BIC    logLik MLdeviance REMLdeviance
>>  292.3627 349.6764 -124.1814   280.0245     248.3627
>> Random effects:
>>  Groups         Name        Variance Std.Dev.
>>  lagoon:habitat (Intercept) 0.46565  0.68239
>>  lagoon         (Intercept) 0.46565  0.68239
>>  Residual                   0.87310  0.93440
>> number of obs: 100, groups: lagoon:habitat, 20; lagoon, 4
>> 
>> Fixed effects:
>>               Estimate Std. Error  t value
>> (Intercept)  0.1292699  1.0516317  0.12292
>> habitat2     0.0058658  1.1316138  0.00518
>> habitat3    -0.0911469  1.1316138 -0.08055
>> habitat4     0.3302971  1.1316138  0.29188
>> habitat5    -0.0480394  1.1316138 -0.04245
>> habitat6    -0.4778469  1.4872319 -0.32130
>> habitat7    -0.0867301  1.4872319 -0.05832
>> habitat8     0.0696507  1.4872319  0.04683
>> habitat9    -0.0998728  1.4872319 -0.06715
>> habitat10    0.1096064  1.4872319  0.07370
>> habitat11   -0.0430979  1.4872319 -0.02898
>> habitat12    0.0714719  1.4872319  0.04806
>> habitat13    0.3380993  1.4872319  0.22733
>> habitat14    0.3057808  1.4872319  0.20560
>> habitat15   -0.4915582  1.4872319 -0.33052
>> habitat16   -0.2624539  1.4872319 -0.17647
>> habitat17   -0.2203461  1.4872319 -0.14816
>> habitat18    0.2165269  1.4872319  0.14559
>> habitat19    0.6932896  1.4872319  0.46616
>> habitat20   -0.7271468  1.4872319 -0.48893
>>  anova(summary(lmer(y~habitat+(1|lagoon)+(1|lagoon:habitat), data = dat)))
>> 
>> Analysis of Variance Table
>>         Df  Sum Sq Mean Sq
>> habitat 19 2.65706 0.13985
>> 
>> > VarCorr(summary(summary(lmer(y~habitat+(1|lagoon)+(1|lagoon:habitat), data
>> = dat)))
>> + )
>> $`lagoon:habitat`
>> 1 x 1 Matrix of class "dpoMatrix"
>>             (Intercept)
>> (Intercept)   0.4656545
>> 
>> $lagoon
>> 1 x 1 Matrix of class "dpoMatrix"
>>             (Intercept)
>> (Intercept)   0.4656545
>> 
>> attr(,"sc")
>> [1] 0.9343993
>> 
>> Christoph Buser wrote ---
>> 
>> Dear Eric
>> 
>> Do you really have habitats nested within lagoons or are they
>> partially crossed (meaning that you have the same habitats in
>> different lagoons)?
>> 
>> If you have them perfectly nested, I think that you cannot
>> calculate both a fixed effect for habitats and a random effect
>> for lagoon (see the example below, lme and aov).
>> 
>> You can compare e.g. two lagoons by defining a contrast of the
>> habitats of one lagoon against the habitats of the other (if you
>> think that this is a meaningful test to interpret), but you
>> cannot estimate a random effect lagoon in presence of a nested
>> FIXED effect habitat.
>> 
>> aov() will not return you the test and warn you about the
>> singular model.
>> 
>> lme() will estimate a variance component for lagoon, but does
>> not provide you a test for the fixed factor.
>> 
>> Regards,
>> 
>> Christoph Buser
>> 
>> 
>> 
>> set.seed(1)
>> dat <- data.frame(y = rnorm(100), lagoon = factor(rep(1:4,each = 25)),
>>                   habitat = factor(rep(1:20, each = 5)))
>> 
>> summary(aov(y~habitat + Error(lagoon/habitat), data = dat))
>> 
>> library(nlme)
>> summary(lme(y~habitat, random = ~1|lagoon/habitat, data = dat))
>> 
>> 
>>


From Mike.Prager at noaa.gov  Mon Jun 19 17:38:38 2006
From: Mike.Prager at noaa.gov (Michael H. Prager)
Date: Mon, 19 Jun 2006 11:38:38 -0400
Subject: [R] can I call user-created functions without source() ?
In-Reply-To: <44967019.50009@robertcampbell.co.uk>
References: <44967019.50009@robertcampbell.co.uk>
Message-ID: <4496C4FE.4060507@noaa.gov>

Rob,

I accomplish what you ask by putting my functions into a dedicated 
directory, starting R there and sourcing them, and loading the resulting 
workspace in .Rprofile with this:

attach("d:/R/MHP/MHPmisc/.RData")

I find that procedure simpler than learning the package mechanism.  It 
is easy to add new functions periodically.

Not long ago, I posted the R code I used to automate the process.  As 
the archive seems unreachable right now (from here, anyway) and the code 
is relatively short, I'll post it again:

##########################################################
## 00make.r       MHP      Dec 2005
## This R script clears the current workspace, sources all R scripts
## found in the working directory, and then saves the workspace for
## use in other R sessions.

# Clear all existing objects in workspace:
rm(list=ls())

# Make a list of all R source files in this directory:
flist = list.files(path=".", pattern=".+\.r")

# Remove from the list all files containing the string "00":
# Such files should be used for temporary funcs:
flist2 = flist[-grep("00",flist)]

# Source the files:
for (i in 1:length(flist2)) {
   cat("Sourcing", flist2[i],"\n")
   source(flist2[i])
   }
# Remove temporary objects:
rm(i,flist,flist2)
# Save workspace:
save.image()
# Write message to user:
cat("\nNOTE: The workspace has been saved with all functions.\n",
    "     When exiting R, please do NOT save again.\n")
ls()
##########################################################

I run that script straight from the (Windows) command line with the 
following shell script:

rterm.exe --no-restore --no-save < 00make.r > 00make.txt

You will probably need to modify that slightly to work under Unix/Linux.

Hope that helps.

...Mike



on 6/19/2006 5:36 AM Rob Campbell said the following:
> Hi,
>
> I have to R fairly recently from Matlab, where I have been used to
> organising my own custom functions into directories which are adding to
> the Matlab search path. This enables me to call them as I would one of
> Matlab's built-in functions. I have not found a way of doing the same
> thing in R. I have resorted to using source() on symlinks located in the
> current directory. Not very satisfactory.
>
> I looked at the R homepage and considered making a "package" of my
> commonly used functions so that I can call them in one go:
> library(myFuncions, lib.loc="/path/to/library") Perhaps this is the only
> solution but the docs on the web make the process seem rather
> complicated--I only have a few script files I want to call! Surely
> there's a straightforward solution?
>
> How have other people solved this problem? Perhaps someone has a simple
> "package skeleton" into which I can drop my scripts?
>
>
> Thanks,
>
> Rob
>   

-- 
Michael Prager, Ph.D.
Southeast Fisheries Science Center
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
** Opinions expressed are personal, not official.  No
** official endorsement of any product is made or implied.


From maechler at stat.math.ethz.ch  Mon Jun 19 17:51:13 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 19 Jun 2006 17:51:13 +0200
Subject: [R] function call
In-Reply-To: <4496B51E.2020208@7d4.com>
References: <44967421.3040007@7d4.com>
	<971536df0606190454l260311e6wef1c4e95d27a87ef@mail.gmail.com>
	<4496B51E.2020208@7d4.com>
Message-ID: <17558.51185.342802.644781@stat.math.ethz.ch>

>>>>> "vincent" == vincent  <vincent at 7d4.com>
>>>>>     on Mon, 19 Jun 2006 16:30:54 +0200 writes:

    vincent> Gabor Grothendieck a ?crit :
    >> Perhaps you what you want to do is to return an object
    >> that has a print method like this: 

    >> f1 <- function(x) structure(c(x, x^2), class = "f1") 
    >> print.f1 <- function(x) cat("x is", x[1], "x squared is", x[2], "\n")

    vincent> Thank you Gabor for this idea.  

Unfortunately, it's not a good idea inspite of coming from Gabor who
has donated many very good ideas to R-help:

help(print)  tells you

  >>   'print' prints its argument and returns it _invisibly_ (via
  >>   'invisible(x)').  It is a generic function which means that new
  >>   printing methods can be easily added for new 'class'es.

I.e., as a good Ritizen, Gabor's function above should **REALLY** be

   print.f1 <- function(x) { 
	  cat("x is", x[1], "x squared is", x[2], "\n")
	  invisible(x)
   }


Martin Maechler,  ETH Zurich


From spencer.graves at pdf.com  Mon Jun 19 17:58:47 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 19 Jun 2006 08:58:47 -0700
Subject: [R] MLE maximum number of parameters
In-Reply-To: <4496A08D.3090904@gmail.com>
References: <B2B70BDD-81EC-4043-A769-020A2AC16923@imperial.ac.uk>
	<4496A08D.3090904@gmail.com>
Message-ID: <4496C9B7.5030206@pdf.com>

	  Applications with lots of parameters also tend to have parameters in 
a relatively small number of families, and each of these few families 
could be considered to have a distribution.  Splines, for example, have 
lots of parameters -- sometimes more parameters than observations (as do 
neural nets and other data mining techniques).  Spline estimation 
virtually always incorporates some kind of smoothness penalty.  The 
smoothness penalty is in essence an attempt to "eat the Bayesian omelet 
without breaking the Bayesian egg."(1)

	  In such situations, I believe it's wiser to embrace Bayes at least to 
the extent of using something like "lme", "nlme" or "lmer".  This 
redefines the problem in terms of estimating a small number of 
"hyperparameters" (using Frequentist methods with the 'nlme' and 'lme4' 
packages) and getting individual estimates of the larger number of 
"random" parameters conditional on the estimates of the hyperparameters.

	  Hope this helps.
	  Spencer Graves

(1) I don't know where I heard the phrase "eating the Bayesian omelet 
without breaking the Bayesian egg", but Google found it in the 
following: 
"http://philosophy.elte.hu/colloquium/2004/May-June/classicalstats.pdf".

Roger D. Peng wrote:
> It really depends on how well-behaved your objective function is, but I've been 
> able to fit a few models with 10--15 parameters.  But I felt like I was 
> stretching the limit there.
> 
> -roger
> 
> Federico Calboli wrote:
>> Hi All,
>>
>> I would like to know, is there a *ballpark* figure for how many  
>> parameters the minimisation routines can cope with?
>>
>> I'm asking because I was asked if I knew.
>>
>> Cheers,
>>
>> Federico
>>
>> --
>> Federico C. F. Calboli
>> Department of Epidemiology and Public Health
>> Imperial College, St. Mary's Campus
>> Norfolk Place, London W2 1PG
>>
>> Tel +44 (0)20 75941602   Fax +44 (0)20 75943193
>>
>> f.calboli [.a.t] imperial.ac.uk
>> f.calboli [.a.t] gmail.com
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>


From petr.pikal at precheza.cz  Mon Jun 19 18:07:08 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 19 Jun 2006 18:07:08 +0200
Subject: [R] Graph and function
In-Reply-To: <20060619152903.72708.qmail@web27803.mail.ukl.yahoo.com>
Message-ID: <4496E7CC.8151.20E3C7D@localhost>

Hi

well I am a bit puzzled

> a<-0.2 
>   t<-rexp(1,a)
>    xn<-rpois(1,t)
> B<-(xn+1)/(1+a)   

so B has nothing to do with n 

> b<-mean(x)
> BE<-(1+xn)*b/(1+b)

so as Be
you can use
n <- 1:100
and n as an input vector for some vectorized function but I wonder 
where to put such vector in your pieces of code?

Petr



On 19 Jun 2006 at 17:29, Ahmed Elhabti wrote:

Date sent:      	Mon, 19 Jun 2006 17:29:03 +0200 (CEST)
From:           	Ahmed Elhabti <a_elhabti at yahoo.fr>
To:             	R-help at stat.math.ethz.ch
Subject:        	[R] Graph and function

> 
>   I have a program which make calculate them of 4 estimator, B, BE, wB
>   and wBE.
> 
>   Now I want to varied n example: "n<-seq(1,100,1)", and to make a
>   graph of "B" and "Be" according to "n", to make a comparison.
> 
>   Tank you very much.
> 
> 
> n<-50
> 
> a<-0.2 
> 
>  theta<-rexp(n,a)
> 
>  x<-rpois(length(theta),theta)
> 
>   t<-rexp(1,a)
> 
>    xn<-rpois(1,t)
> 
> b<-mean(x)
> 
> 
> 
> B<-(xn+1)/(1+a)   
> 
> BE<-(1+xn)*b/(1+b)
> 
> r<-b/(1+b)
> 
> wB<-1/(a*(1+a))
> 
> wBE<-wB+(2/(a^2)+5/a+4)*(b/(1+b)-1/(1+a))^2
> 
> 
> 
> 
> 
> B
> 
> BE
> 
> wB
> 
> wBE
> 
> 
> ---------------------------------
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From j.van_den_hoff at fz-rossendorf.de  Mon Jun 19 18:14:10 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Mon, 19 Jun 2006 18:14:10 +0200
Subject: [R] Function hints
In-Reply-To: <f8e6ff050606190551o24a4573ak4454473b5dbf8199@mail.gmail.com>
References: <f8e6ff050606190551o24a4573ak4454473b5dbf8199@mail.gmail.com>
Message-ID: <4496CD52.7060905@fz-rossendorf.de>

hadley wickham wrote:
> One of the recurring themes in the recent UserR conference was that
> many people find it difficult to find the functions they need for a
> particular task.  Sandy Weisberg suggested a small idea he would like
> to see: a hints function that given an object, lists likely
> operations.  I've done my best to implement this function using the
> tools currently available in R, and my code is included at the bottom
> of this email (I hope that I haven't just duplicated something already
> present in R).  I think Sandy's idea is genuinely useful, even in the
> limited form provided by my implementation, and I have already
> discovered a few useful functions that I was unaware of.
> 
> While developing and testing this function, I ran into a few problems
> which, I think, represent underlying problems with the current
> documentation system.  These are typified by the results of running
> hints on a object produced by glm (having class c("glm", "lm")).  I
> have outlined (very tersely) some possible solutions.  Please note
> that while these solutions are largely technological, the problem is
> at heart sociological: writing documentation is no easier (and perhaps
> much harder) than writing a scientific publication, but the rewards
> are fewer.
> 
> Problems:
> 
>  * Many functions share the same description (eg. head, tail).
> Solution: each rdoc file should only describe one method. Problem:
> Writing rdoc files is tedious, there is a lot of information
> duplicated between the code and the documenation (eg. the usage
> statement) and some functions share a lot of similar information.
> Solution: make it easier to write documentation (eg. documentation
> inline with code), and easier to include certain common descriptions
> in multiple methods (eg. new include command)
> 
>  * It is difficult to tell which functions are commonly
> used/important. Solution: break down by keywords. Problem: keywords
> are not useful at the moment.  Solution:  make better list of keywords
> available and encourage people to use it.  Problem: people won't
> unless there is a strong incentive, plus good keywording requires
> considerable expertise (especially in bulding up list).  This is
> probably insoluable unless one person systematically keywords all of
> the base packages.
> 
>  * Some functions aren't documented (eg. simulate.lm, formula.glm) -
> typically, these are methods where the documentation is in the
> generic.  Solution: these methods should all be aliased to the generic
> (by default?), and R CMD check should be amended to check for this
> situation.  You could also argue that this is a deficiency with my
> function, and easily fixed by automatically referring to the generic
> if the specific isn't documented.
> 
>  * It can't supply suggestions when there isn't an explicit method
> (ie. .default is used), this makes it pretty useless for basic
> vectors.  This may not really be a problem, as all possible operations
> are probably too numerous to list.
> 
>  * Provides full name for function, when best practice is to use
> generic part only when calling function.  However, getting precise
> documentation may requires that full name.  I do the best I can
> (returning the generic if specific is alias to a documentation file
> with the same method name), but this reflects a deeper problem that
> the name you should use when calling a function may be different to
> the name you use to get documentation.
> 
>  * Can only display methods from currently loaded packages.  This is a
> shortcoming of the methods function, but I suspect it is difficult to
> find S3 methods without loading a package.
> 
> Relatively trivial problems:
> 
>  * Needs wide display to be effective.  Could be dealt with by
> breaking description in a sensible manner (there may already by R code
> to do this.  Please let me know if you know of any)
> 
>  * Doesn't currently include S4 methods.  Solution: add some more code
> to wrap showMethods
> 
>  * Personally, I think sentence case is more aesthetically pleasing
> (and more flexible) than title case.
> 
> 
> Hadley
> 
> 
> hints <- function(x) {
> 	db <- eval(utils:::.hsearch_db())
> 	if (is.null(db)) {
> 		help.search("abcd!", rebuild=TRUE, agrep=FALSE)
> 		db <- eval(utils:::.hsearch_db())
> 	}
> 
> 	base <- db$Base
> 	alias <- db$Aliases
> 	key <- db$Keywords
> 
> 	m <- all.methods(class=class(x))
> 	m_id <- alias[match(m, alias[,1]), 2]
> 	keywords <- lapply(m_id, function(id) key[key[,2] %in% id, 1])
> 
> 	f.names <- cbind(m, base[match(m_id, base[,3]), 4])
> 	f.names <- unlist(lapply(1:nrow(f.names), function(i) {
> 		if (is.na(f.names[i, 2])) return(f.names[i, 1])
> 		a <- methodsplit(f.names[i, 1])
> 		b <- methodsplit(f.names[i, 2])
> 		
> 		if (a[1] == b[1]) f.names[i, 2] else f.names[i, 1]		
> 	}))
> 	
> 	hints <- cbind(f.names, base[match(m_id, base[,3]), 5])
> 	hints <- hints[order(tolower(hints[,1])),]
> 	hints <- rbind(    c("--------", "---------------"), hints)
> 	rownames(hints) <- rep("", nrow(hints))
> 	colnames(hints) <- c("Function", "Task")
> 	hints[is.na(hints)] <- "(Unknown)"
> 	
> 	class(hints) <- "hints"
> 	hints
> }
> 
> print.hints <- function(x, ...) print(unclass(x), quote=FALSE)
> 
> all.methods <- function(classes) {
> 	methods <- do.call(rbind,lapply(classes, function(x) {
> 		m <- methods(class=x)
> 		t(sapply(as.vector(m), methodsplit)) #m[attr(m, "info")$visible]
> 	}))
> 	rownames(methods[!duplicated(methods[,1]),])
> }
> 
> methodsplit <- function(m) {
> 	parts <- strsplit(m, "\\.")[[1]]
> 	if (length(parts) == 1) {
> 		c(name=m, class="")
> 	} else{
> 		c(name=paste(parts[-length(parts)], collapse="."), class=parts[length(parts)])
> 	}	
> }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


just a feedback: that's a useful function, thank you.

but the problem is probably more general: frequently I do not really 
want to know what I generally can do with a data frame, for instance, 
but rather I would like to use `help.search' as I would use, say, Google 
(and with the same rate of success...).
but the actual `keywords' in the manpages seem insufficient and 
`help.search' does not allow full text search in the manpages (I can 
imagine why (1000 hits...), but without such a thing google, for 
instance, would probably not be half as useful as it is, right?) and 
there is no "sorting by relevance" in the `help.search' output, I think. 
how this sorting could be achieved is a different question, of course.


From andy_liaw at merck.com  Mon Jun 19 18:30:47 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 19 Jun 2006 12:30:47 -0400
Subject: [R] multivariate splits
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA026FFDA3@usctmx1106.merck.com>

If by "multivariate split" the OP meant splitting on combinations of
covariates (instead of multi-way split on a single covariate at a time),
there aren't that many methods published (AFAIK).  All the ones I know about
are in the cheminformatics area:  RP-SA and generalizations/extensions of
it.  No R package I know of can do it.

Best,
Andy

From: Torsten Hothorn
> 
> On Mon, 19 Jun 2006, B?lint Cz?cz wrote:
> 
> > Dear R users!
> >
> > Does someone know about any algorithms / packages in R, 
> that perform 
> > classification / regression / decision trees using multivariate 
> > splits?
> >
> > I have done some research, but I found nothing. Packages "tree" and 
> > "rpart" seem only to be able to do CART with univariate splits.
> >
> 
> have a look at the machine learning task view on CRAN, which 
> will point you to package `RWeka'.
> 
> HTH,
> 
> Torsten
> 
> > Thank you for your help!
> >
> > B?lint
> >
> > --
> > Cz?cz B?lint
> > PhD hallgat?
> > BCE KTK Talajtan ?s V?zgazd?lkod?s Tansz?k
> > 1118 Budapest, Vill?nyi ?t 29-43.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> >
>


From andy_liaw at merck.com  Mon Jun 19 18:33:45 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 19 Jun 2006 12:33:45 -0400
Subject: [R] Problem in Projection pursuit regression . [Broadcast]
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA026FFDA6@usctmx1106.merck.com>

ppr() estimates the ridge functions by either the Friedman super smoother or
smoothing splines.  (See ?supsmu and ?smooth.spline.spline.)

Andy 

From: anil kumar rohilla
>   
> Hi list, 
>          Is any body has implemented the projection pursuit 
> regression (ppr) method in MASS library.Running the ppr 
> method is not a problem , But problem is how we are getting 
> the model parameters using least square method ,becoz the 
> function is looking like a continous function and how a 
> continous function can be found out by least square 
> method....THat function name is ridge function (activation 
> function ) If any body is having idia please ,reply its very 
> urgent,,,,,,,,becoz of this problem i am not able to proceed 
> for my paper ,.
> ANy help is much appriciated
> 
> anil
> 
> 
> ANIL KUMAR( METEOROLOGIST)
> LRF SECTION
> NATIONAL CLIMATE CENTER
> ADGM(RESEARCH)
> INDIA METEOROLOGICAL DEPARTMENT
> SHIVIJI NAGAR
> PUNE-411005 INDIA
> MOBILE +919422023277
> anilkumar at imdpune.gov.in
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From stubben at lanl.gov  Mon Jun 19 18:48:29 2006
From: stubben at lanl.gov (stubben)
Date: Mon, 19 Jun 2006 10:48:29 -0600
Subject: [R]  analyze amino acid sequence (composition)of proteins
Message-ID: <60781de86e1668782a1fb34671ada8f9@lanl.gov>

Have you checked the package seqinR at CRAN?  It may help.

Also, if you split a string into single characters using strsplit, then 
use table to count characters.


seq<-"ATGAAC"


table(strsplit(seq, ""))

A C G T
3 1 1 1




 > 3. based on "AAABBB",how can i get some statistics of this string 
such as how
 > many letters,how many "A"s in the string.
-- 

-----------------
Chris Stubben

Los Alamos National Lab
BioScience Division
MS M888
Los Alamos, NM 87545


From theguz at lanl.gov  Mon Jun 19 18:52:33 2006
From: theguz at lanl.gov (Brian Phillip Weaver)
Date: Mon, 19 Jun 2006 10:52:33 -0600 (MDT)
Subject: [R] Display Conditional Probabilities on y-label
Message-ID: <3108.128.165.238.74.1150735953.squirrel@webmail.lanl.gov>

Hello,

I am trying to display a conditional probability for the y-label on a beta
distribution plot. Here is the code I have been using:

z = expression("f(x|")
g = paste(z,expression(alpha),",",expression(beta),")")
plot(x,y,ylim=c(0,3),xlab="x",ylab=g,lwd=3,type="l",cex.lab=1.3)

The output on the plot is as follows:
f(x|alpha,beta)

This is what I want except have the greek letters for alpha and beta
instead of the words.  I have consulted plotmath and other options with no
luck.  I appreciate any help and thank you ahead of time.

best,
Brian


From hpbenton at scripps.edu  Mon Jun 19 18:54:16 2006
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Mon, 19 Jun 2006 09:54:16 -0700
Subject: [R] objects
Message-ID: <000d01c693c1$013c2d50$0202a8c0@hpb>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060619/0a5a26a2/attachment.pl 

From javlacalle at yahoo.es  Mon Jun 19 18:59:17 2006
From: javlacalle at yahoo.es (=?iso-8859-1?q?Javier=20L=F3pez=20de=20Lacalle=20Beltr=E1n=20de=20Heredia?=)
Date: Mon, 19 Jun 2006 18:59:17 +0200 (CEST)
Subject: [R] Compute in C a loop with the lm() function and back to R
Message-ID: <20060619165917.44816.qmail@web26803.mail.ukl.yahoo.com>

I want to do the computations from a loop by means of
a C routine that will be called from R. The lm()
function is entailed in that loop.

As a guidance, I have taken the following example from
http://www.math.mcgill.ca/steele/ (As far as my
question is concerned, I get similar information from
"writing R extensions" and the book "Econometrics in
R" by G.V Farnsworth):

Let convolve.rfun an R function

convolve.rfun <- function(a,b)
{
  ab <- rep(0, length(a)+length(b)-1)
  for(i in seq(along=a)){
    for(j in seq(along=b))
      ab[i+j-1] <- ab[i+j-1] + a[i]*b[j]
  }
  ab
}

then the following C code do the computations of
convolve.rfun,

void convolve(double *a, int *na, double *b, int *nb,
double *ab)
{
 int i, j, nab = *na + *nb - 1;

 for(i = 0; i < nab; i = i+1){
   ab[i] = 0.0;
 }

 for(i = 0; i < *na; i = i + 1){
  for(j = 0; j < *nb; j = j + 1){
    ab[i + j] = ab[i+j] +  a[i] * b[j];
   }
 }
}

which after being compiled (R CMD SHLIB convolve.c)
can be called from R:

dyn.load("convolve.so")
conv <- function(a, b){
  .C("convolve", as.double(a), as.integer(length(a)),
as.double(b),
    as.integer(length(b)),
ab=double(length(a)+length(b)-1))$ab
}

My question is: How should I write the corresponding C
code if the lm() R-function is entailed in the
computations of the loop in convolve.rfun() . How
could I get the OLS estimates in C?. Is there any
routine that I should include in the header of the C
file? Where to find it?

Thank you.


From murdoch at stats.uwo.ca  Mon Jun 19 19:01:22 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 19 Jun 2006 13:01:22 -0400
Subject: [R] can I call user-created functions without source() ?
In-Reply-To: <4496C4FE.4060507@noaa.gov>
References: <44967019.50009@robertcampbell.co.uk> <4496C4FE.4060507@noaa.gov>
Message-ID: <4496D862.5070408@stats.uwo.ca>

On 6/19/2006 11:38 AM, Michael H. Prager wrote:
> Rob,
> 
> I accomplish what you ask by putting my functions into a dedicated 
> directory, starting R there and sourcing them, and loading the resulting 
> workspace in .Rprofile with this:
> 
> attach("d:/R/MHP/MHPmisc/.RData")

This seems like a nice approach.  Some comments:

1.  attach or load?

You could also use

  load("d:/R/MHP/MHPmisc/.RData")

One difference is that this would load the functions into the user's 
workspace, overwriting any existing functions of the same name, whereas 
attach() puts them on the search list.  If the user made changes to the 
functions, they would replace the loaded ones (for that session, not in 
the .RData file).

Another difference is that attach() is cumulative.  I.e. if you forget 
and load() again, the functions will just revert to their saved 
definitions, but if you forget and attach() again, you'll get another 
copy of the functions in memory.  Probably harmless, but potentially 
confusing if you think detach() is going to remove them.

I don't know which of attach() or load() you'd find to be preferable.

2.  If you use the package system, you will be encouraged to create man 
pages for your functions.  This is work, but I think it pays off in the 
end.  I often find that when I document a function I realize an error in 
the design, and I end up improving it.  It's also useful to have 
documentation for functions that you don't use every day, or when using 
functions written by someone else.

Duncan Murdoch

> 
> I find that procedure simpler than learning the package mechanism.  It 
> is easy to add new functions periodically.
> 
> Not long ago, I posted the R code I used to automate the process.  As 
> the archive seems unreachable right now (from here, anyway) and the code 
> is relatively short, I'll post it again:
> 
> ##########################################################
> ## 00make.r       MHP      Dec 2005
> ## This R script clears the current workspace, sources all R scripts
> ## found in the working directory, and then saves the workspace for
> ## use in other R sessions.
> 
> # Clear all existing objects in workspace:
> rm(list=ls())
> 
> # Make a list of all R source files in this directory:
> flist = list.files(path=".", pattern=".+\.r")
> 
> # Remove from the list all files containing the string "00":
> # Such files should be used for temporary funcs:
> flist2 = flist[-grep("00",flist)]
> 
> # Source the files:
> for (i in 1:length(flist2)) {
>    cat("Sourcing", flist2[i],"\n")
>    source(flist2[i])
>    }
> # Remove temporary objects:
> rm(i,flist,flist2)
> # Save workspace:
> save.image()
> # Write message to user:
> cat("\nNOTE: The workspace has been saved with all functions.\n",
>     "     When exiting R, please do NOT save again.\n")
> ls()
> ##########################################################
> 
> I run that script straight from the (Windows) command line with the 
> following shell script:
> 
> rterm.exe --no-restore --no-save < 00make.r > 00make.txt
> 
> You will probably need to modify that slightly to work under Unix/Linux.
> 
> Hope that helps.
> 
> ...Mike
> 
> 
> 
> on 6/19/2006 5:36 AM Rob Campbell said the following:
>> Hi,
>>
>> I have to R fairly recently from Matlab, where I have been used to
>> organising my own custom functions into directories which are adding to
>> the Matlab search path. This enables me to call them as I would one of
>> Matlab's built-in functions. I have not found a way of doing the same
>> thing in R. I have resorted to using source() on symlinks located in the
>> current directory. Not very satisfactory.
>>
>> I looked at the R homepage and considered making a "package" of my
>> commonly used functions so that I can call them in one go:
>> library(myFuncions, lib.loc="/path/to/library") Perhaps this is the only
>> solution but the docs on the web make the process seem rather
>> complicated--I only have a few script files I want to call! Surely
>> there's a straightforward solution?
>>
>> How have other people solved this problem? Perhaps someone has a simple
>> "package skeleton" into which I can drop my scripts?
>>
>>
>> Thanks,
>>
>> Rob
>>   
>


From calstats05 at yahoo.com  Mon Jun 19 19:10:36 2006
From: calstats05 at yahoo.com (Cal Stats)
Date: Mon, 19 Jun 2006 10:10:36 -0700 (PDT)
Subject: [R] Plotting Upper triangle of Matrix with diagonal as the Base
Message-ID: <20060619171036.24982.qmail@web34010.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060619/10219c6c/attachment.pl 

From murdoch at stats.uwo.ca  Mon Jun 19 19:13:01 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 19 Jun 2006 13:13:01 -0400
Subject: [R] Function hints
In-Reply-To: <4496CD52.7060905@fz-rossendorf.de>
References: <f8e6ff050606190551o24a4573ak4454473b5dbf8199@mail.gmail.com>
	<4496CD52.7060905@fz-rossendorf.de>
Message-ID: <4496DB1D.4010904@stats.uwo.ca>

On 6/19/2006 12:14 PM, Joerg van den Hoff wrote:

> just a feedback: that's a useful function, thank you.
> 
> but the problem is probably more general: frequently I do not really 
> want to know what I generally can do with a data frame, for instance, 
> but rather I would like to use `help.search' as I would use, say, Google 
> (and with the same rate of success...).
> but the actual `keywords' in the manpages seem insufficient and 
> `help.search' does not allow full text search in the manpages (I can 
> imagine why (1000 hits...), but without such a thing google, for 
> instance, would probably not be half as useful as it is, right?) and 
> there is no "sorting by relevance" in the `help.search' output, I think. 
> how this sorting could be achieved is a different question, of course.

You probably want RSiteSearch("keyword", restrict="functions") (or even 
without the "restrict" part).

Duncan Murdoch


From Charles.Annis at StatisticalEngineering.com  Mon Jun 19 19:17:02 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Mon, 19 Jun 2006 13:17:02 -0400
Subject: [R] Display Conditional Probabilities on y-label
In-Reply-To: <3108.128.165.238.74.1150735953.squirrel@webmail.lanl.gov>
Message-ID: <036c01c693c4$2eca22b0$6600a8c0@DD4XFW31>

The trick is not to paste expressions but to make an expression of the
pastes:


composite.expression <- expression(paste("f(x | ", alpha, ", ", beta, " )"))

dev.off()
par(mar=c(5,5,1,1)+0.1)

plot(NA,xlim=c(0,1),
ylim=c(0,3),xlab="x",ylab=composite.expression,lwd=3,type="l",cex.lab=1.3)


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Brian Phillip Weaver
Sent: Monday, June 19, 2006 12:53 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Display Conditional Probabilities on y-label

Hello,

I am trying to display a conditional probability for the y-label on a beta
distribution plot. Here is the code I have been using:

z = expression("f(x|")
g = paste(z,expression(alpha),",",expression(beta),")")
plot(x,y,ylim=c(0,3),xlab="x",ylab=g,lwd=3,type="l",cex.lab=1.3)

The output on the plot is as follows:
f(x|alpha,beta)

This is what I want except have the greek letters for alpha and beta
instead of the words.  I have consulted plotmath and other options with no
luck.  I appreciate any help and thank you ahead of time.

best,
Brian

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From rxg218 at psu.edu  Mon Jun 19 19:25:15 2006
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Mon, 19 Jun 2006 13:25:15 -0400
Subject: [R] frechet distance
Message-ID: <1150737915.8534.34.camel@blue.chem.psu.edu>

Hi, is there any package (or source code snippet) that will evaluate the
Frechet distance for curves represented as sets of points?

Searching around only threw up references to a Frechet distribution.

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------

If you believe in telekinesis, raise my hand.


From spencer.graves at pdf.com  Mon Jun 19 19:32:23 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 19 Jun 2006 10:32:23 -0700
Subject: [R] can I call user-created functions without source() ?
In-Reply-To: <4496D862.5070408@stats.uwo.ca>
References: <44967019.50009@robertcampbell.co.uk> <4496C4FE.4060507@noaa.gov>
	<4496D862.5070408@stats.uwo.ca>
Message-ID: <4496DFA7.6040402@pdf.com>

	  I'd like to echo Duncan's comment about creating 'man' pages for 
functions.  I find myself writing man pages for functions I may never 
include in an R package, just because it helps me think through what I'm 
trying to do.  I think I get better code faster.  It's like preparing a 
map before starting on a journey through unknown territory:  I may think 
I know the destination and more or less every step of the way.  However, 
if it's very far, it helps to make a plan first;  then when I encounter 
something I hadn't anticipated, it makes it much easier to figure out 
what to do without sabotaging something else.

	  Hope this helps.
	  Spencer Graves

Duncan Murdoch wrote:
> On 6/19/2006 11:38 AM, Michael H. Prager wrote:
>> Rob,
>>
>> I accomplish what you ask by putting my functions into a dedicated 
>> directory, starting R there and sourcing them, and loading the resulting 
>> workspace in .Rprofile with this:
>>
>> attach("d:/R/MHP/MHPmisc/.RData")
> 
> This seems like a nice approach.  Some comments:
> 
> 1.  attach or load?
> 
> You could also use
> 
>   load("d:/R/MHP/MHPmisc/.RData")
> 
> One difference is that this would load the functions into the user's 
> workspace, overwriting any existing functions of the same name, whereas 
> attach() puts them on the search list.  If the user made changes to the 
> functions, they would replace the loaded ones (for that session, not in 
> the .RData file).
> 
> Another difference is that attach() is cumulative.  I.e. if you forget 
> and load() again, the functions will just revert to their saved 
> definitions, but if you forget and attach() again, you'll get another 
> copy of the functions in memory.  Probably harmless, but potentially 
> confusing if you think detach() is going to remove them.
> 
> I don't know which of attach() or load() you'd find to be preferable.
> 
> 2.  If you use the package system, you will be encouraged to create man 
> pages for your functions.  This is work, but I think it pays off in the 
> end.  I often find that when I document a function I realize an error in 
> the design, and I end up improving it.  It's also useful to have 
> documentation for functions that you don't use every day, or when using 
> functions written by someone else.
> 
> Duncan Murdoch
> 
>> I find that procedure simpler than learning the package mechanism.  It 
>> is easy to add new functions periodically.
>>
>> Not long ago, I posted the R code I used to automate the process.  As 
>> the archive seems unreachable right now (from here, anyway) and the code 
>> is relatively short, I'll post it again:
>>
>> ##########################################################
>> ## 00make.r       MHP      Dec 2005
>> ## This R script clears the current workspace, sources all R scripts
>> ## found in the working directory, and then saves the workspace for
>> ## use in other R sessions.
>>
>> # Clear all existing objects in workspace:
>> rm(list=ls())
>>
>> # Make a list of all R source files in this directory:
>> flist = list.files(path=".", pattern=".+\.r")
>>
>> # Remove from the list all files containing the string "00":
>> # Such files should be used for temporary funcs:
>> flist2 = flist[-grep("00",flist)]
>>
>> # Source the files:
>> for (i in 1:length(flist2)) {
>>    cat("Sourcing", flist2[i],"\n")
>>    source(flist2[i])
>>    }
>> # Remove temporary objects:
>> rm(i,flist,flist2)
>> # Save workspace:
>> save.image()
>> # Write message to user:
>> cat("\nNOTE: The workspace has been saved with all functions.\n",
>>     "     When exiting R, please do NOT save again.\n")
>> ls()
>> ##########################################################
>>
>> I run that script straight from the (Windows) command line with the 
>> following shell script:
>>
>> rterm.exe --no-restore --no-save < 00make.r > 00make.txt
>>
>> You will probably need to modify that slightly to work under Unix/Linux.
>>
>> Hope that helps.
>>
>> ...Mike
>>
>>
>>
>> on 6/19/2006 5:36 AM Rob Campbell said the following:
>>> Hi,
>>>
>>> I have to R fairly recently from Matlab, where I have been used to
>>> organising my own custom functions into directories which are adding to
>>> the Matlab search path. This enables me to call them as I would one of
>>> Matlab's built-in functions. I have not found a way of doing the same
>>> thing in R. I have resorted to using source() on symlinks located in the
>>> current directory. Not very satisfactory.
>>>
>>> I looked at the R homepage and considered making a "package" of my
>>> commonly used functions so that I can call them in one go:
>>> library(myFuncions, lib.loc="/path/to/library") Perhaps this is the only
>>> solution but the docs on the web make the process seem rather
>>> complicated--I only have a few script files I want to call! Surely
>>> there's a straightforward solution?
>>>
>>> How have other people solved this problem? Perhaps someone has a simple
>>> "package skeleton" into which I can drop my scripts?
>>>
>>>
>>> Thanks,
>>>
>>> Rob
>>>   
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From baron at psych.upenn.edu  Mon Jun 19 19:47:36 2006
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Mon, 19 Jun 2006 13:47:36 -0400
Subject: [R] Function hints
In-Reply-To: <4496DB1D.4010904@stats.uwo.ca>
References: <f8e6ff050606190551o24a4573ak4454473b5dbf8199@mail.gmail.com>
	<4496CD52.7060905@fz-rossendorf.de> <4496DB1D.4010904@stats.uwo.ca>
Message-ID: <20060619174736.GA10478@psych.upenn.edu>

On 06/19/06 13:13, Duncan Murdoch wrote:
> > `help.search' does not allow full text search in the manpages (I can
> > imagine why (1000 hits...), but without such a thing google, for
> > instance, would probably not be half as useful as it is, right?) and
> > there is no "sorting by relevance" in the `help.search' output, I think.
> > how this sorting could be achieved is a different question, of course.
> 
> You probably want RSiteSearch("keyword", restrict="functions") (or even
> without the "restrict" part).

Yes.  The restrict part will speed things up quite a bit, if you
want to restrict to functions.

Or, alternatively, you could use Namazu (which I use to generate
what RSiteSearch provides) to generate an index specific to your
own installed functions and packages.  The trick is to cd to the
directory /usr/lib/R/library, or the equivalent, and then say

mknmz -q */html

which will pick up the html version of all the man pages
(assuming you have generated them, and I have no idea whether
this can be done on Windows).  To update, say

mknmz --update=. -q */html

Then make a bookmark for the Namazu search page in your browser,
as a local file.  (I haven't given all the details.  You have to
install Namazu and follow the instructions.)

Or, if you have a web server, you could let Google do it for
you.  But, I warn you, Google will fill up your web logs pretty
fast if you don't exclude it with robots.txt.  I don't let it
search my R stuff.

I think that Macs and various Linux versions also have other
alternative built-in search capabilities, but I haven't tried
them.  Beagle is the new Linux search tool, but I don't know what
it does.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
Editor: Judgment and Decision Making (http://journal.sjdm.org)


From jones at reed.edu  Mon Jun 19 19:50:17 2006
From: jones at reed.edu (Albyn Jones)
Date: Mon, 19 Jun 2006 10:50:17 -0700
Subject: [R] Foometrics in R
In-Reply-To: <50514.212.123.203.118.1150709657.squirrel@212.123.203.118>
References: <50514.212.123.203.118.1150709657.squirrel@212.123.203.118>
Message-ID: <20060619175017.GE12635@laplace.reed.edu>

On Mon, Jun 19, 2006 at 02:34:17AM -0700, Jan de Leeuw wrote:
> 
> One of the outcomes of useR! 2006 is that JSS is planning to publish
> a series of special volumes. They will be guest edited (I have guest
> editors already, although somewhat tentative in some cases). Each volume
> will have 5-10 issues (articles) of the usual JSS format.
> 
> Psychometrics in R
> Political Methodology in R
> Econometrics in R
> Social Science Methodology in R
> Spectroscopy/Chemometrics in R
> Ecology in R
> 
> If you have suggestions, comments, possible contributions, ideas for
> additional volumes and so on, send them to me and I'll forward them to the
> appropriate authorities.
> 


Great idea!

albyn


From john.gavin at ubs.com  Mon Jun 19 19:50:14 2006
From: john.gavin at ubs.com (john.gavin at ubs.com)
Date: Mon, 19 Jun 2006 18:50:14 +0100
Subject: [R] lattice xyplot - aligning date labels so that they align with
	the grid lines in panel.grid
Message-ID: <182544D7A3144B42994EEA5662C54E0102D692F1@NLDNC105PEX1.ubsw.net>

Hi,

I have a basic question about aligning date labels for the x-axis
in an xyplot so that they align with the grid lines
from the panel.grid argument.

For example, with

x <- data.frame(
  date = seq(as.Date("2005/01/01"), as.Date("2006/06/01"), 
  length.out = 20), value = runif(20))
xyplot(value ~ date, data = x,
  panel = function(x, y, subscripts, ...)
  { panel.grid(h = -1, v = -1, col = "grey", lwd = 1, lty = 1)
    panel.xyplot(x, y, ...)
  }) 

How can I get the labels on the x-axis to align 
with the vertical grid lines?
i.e. I have 6 vertical grid lines by default (v = -1)
in this example so I would like 6 labels along the x-axis
at the same points, whereas I see only 2 (2005 and 2006).

As an alternative I would be happy to just specify the number of labels
as long as panel.grid's vertical lines aligned with the labels.

> R.version.string
[1] "Version 2.3.1 (2006-06-01)"

on Windows NT4.

Regards,

John.

John Gavin <john.gavin at ubs.com>,
Commodities, FIRC,
UBS Investment Bank, 2nd floor, 
100 Liverpool St., London EC2M 2RH, UK.
Phone +44 (0) 207 567 4289
This communication is issued by UBS AG or an affiliate ("UBS...{{dropped}}


From pburns at pburns.seanet.com  Mon Jun 19 19:53:00 2006
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Mon, 19 Jun 2006 18:53:00 +0100
Subject: [R] MLE maximum number of parameters
In-Reply-To: <B2B70BDD-81EC-4043-A769-020A2AC16923@imperial.ac.uk>
References: <B2B70BDD-81EC-4043-A769-020A2AC16923@imperial.ac.uk>
Message-ID: <4496E47C.5000303@pburns.seanet.com>

Seagulls have a very different perspective to ballparks
than ants.  Nonetheless, there is something that can be
said.

There are several variables in addition to the number of
parameters that are important.  These include:

* The complexity of the likelihood

* The number of observations in the dataset

* How close to the optimum is close enough

* Your patience

The latter is undoubtedly the most important of all.  It
matters a lot whether you think a minute is a long time
or only periods measured in weeks.

The optimization strategy can also have a big effect.  If
you are using a derivative-based optimizer, then the number
of parameters can have a big impact.  Typically one iteration
in such algorithms requires p+1 function calls, where p is the
number of parameters.  Since more iterations are generally
required with more parameters, the speed can decrease
rapidly as the number of parameters increases.

One strategy to deal with a large number of parameters is to
start with something like a genetic algorithm.  Once the genetic
algorithm has a pretty good solution, then switch to a derivative-
based algorithm to finish.  The amount to run the initial
algorithm before switching depends on the problem, the quality
of the two optimizers, and probably other things.

With this switching strategy and at least a modicum of patience,
problems with thousands of parameters may be feasible to solve.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Federico Calboli wrote:

>Hi All,
>
>I would like to know, is there a *ballpark* figure for how many  
>parameters the minimisation routines can cope with?
>
>I'm asking because I was asked if I knew.
>
>Cheers,
>
>Federico
>
>--
>Federico C. F. Calboli
>Department of Epidemiology and Public Health
>Imperial College, St. Mary's Campus
>Norfolk Place, London W2 1PG
>
>Tel +44 (0)20 75941602   Fax +44 (0)20 75943193
>
>f.calboli [.a.t] imperial.ac.uk
>f.calboli [.a.t] gmail.com
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


From gchappi at gmail.com  Mon Jun 19 20:00:45 2006
From: gchappi at gmail.com (Hans-Peter)
Date: Mon, 19 Jun 2006 20:00:45 +0200
Subject: [R] strange digit switching
Message-ID: <47fce0650606191100m357ed319m53b7a7cc8874525a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060619/2ec2895b/attachment.pl 

From seanpor at acm.org  Mon Jun 19 20:12:28 2006
From: seanpor at acm.org (Sean O'Riordain)
Date: Mon, 19 Jun 2006 18:12:28 +0000
Subject: [R] can I call user-created functions without source() ?
In-Reply-To: <4496D862.5070408@stats.uwo.ca>
References: <44967019.50009@robertcampbell.co.uk> <4496C4FE.4060507@noaa.gov>
	<4496D862.5070408@stats.uwo.ca>
Message-ID: <8ed68eed0606191112g6cde8379h9fe82c1ec6ae4e6e@mail.gmail.com>

Indeed, some folk say that the documentation should be written before
the code...

Sean


On 19/06/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:

> 2.  If you use the package system, you will be encouraged to create man
> pages for your functions.  This is work, but I think it pays off in the
> end.  I often find that when I document a function I realize an error in
> the design, and I end up improving it.  It's also useful to have
> documentation for functions that you don't use every day, or when using
> functions written by someone else.
>
> Duncan Murdoch


From p.dalgaard at biostat.ku.dk  Mon Jun 19 20:17:03 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Jun 2006 20:17:03 +0200
Subject: [R] strange digit switching
In-Reply-To: <47fce0650606191100m357ed319m53b7a7cc8874525a@mail.gmail.com>
References: <47fce0650606191100m357ed319m53b7a7cc8874525a@mail.gmail.com>
Message-ID: <x2u06hxb6o.fsf@turmalin.kubism.ku.dk>

Hans-Peter <gchappi at gmail.com> writes:

> Hi,
> 
> I execute the following code snippet.What I don't understand is, that in the
> first two cases the numbers shown don't correnspondant to the input (though
> I have set the options to show 22 digits). When I assign the third number
> which has one place, the display of the numbers is suddenly correct.
> 
> Can anybody explain what happens here?
> (this is just a sample, my real problem lies in numbers that I import from a
> datafile and I don't know how to control them when this switching occurs)

This looks like it is windows specific: digits=22 is interpreted as 17
or so. On Linux (FC5) I see a few more digits, but I still get an
extra decimal in the 3rd case. 

This is natural since 0.19... needs one more decimal place to be
represented to N digit accuracy than does 6.94... and 8.34...
 
> ###########################
> options( digits=22 )
> x <- matrix(NA, 3,1)
> x[1] <- 6.94788635771657950
> x      # 6.947886357716580     # DIFFERENT ???
>         # NA
>         # NA
> x[1] == 6.94788635771657950   # but is the same
> 
> x[2] <- 8.34284937565404050
> x      # 6.947886357716580     # DIFFERENT ???
>         # 8.342849375654041     # DIFFERENT ???
>         # NA
> x[1] == 6.94788635771657950   # but is the same
> 
> x[3] <- 0.19043842368566377
> x      # 6.94788635771657950   # NOW OK!!!
>         # 8.34284937565404050   # "   "
>         # 0.19043842368566377   # "   "
> x[1] == 6.94788635771657950   # but is the same
> 
> 
> ###
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          3.1
> year           2006
> month          06
> day            01
> svn rev        38247
> language       R
> version.string Version 2.3.1 (2006-06-01)
> 
> 
> -- 
> Regards,
> Hans-Peter
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From tlumley at u.washington.edu  Mon Jun 19 20:21:29 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 19 Jun 2006 11:21:29 -0700 (PDT)
Subject: [R] strange digit switching
In-Reply-To: <47fce0650606191100m357ed319m53b7a7cc8874525a@mail.gmail.com>
References: <47fce0650606191100m357ed319m53b7a7cc8874525a@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0606191115560.9084@homer23.u.washington.edu>

On Mon, 19 Jun 2006, Hans-Peter wrote:

> Hi,
>
> I execute the following code snippet.What I don't understand is, that in the
> first two cases the numbers shown don't correnspondant to the input (though
> I have set the options to show 22 digits). When I assign the third number
> which has one place, the display of the numbers is suddenly correct.
>
> Can anybody explain what happens here?

The problem is that you have set the options to show 22 digits, but the 
numbers are stored to fewer than 16 digits
> .Machine$double.eps
[1] 2.220446049250313080847e-16

Everything displayed after the 16th digit is effectively random (the 
actual values probably depend on exactly when the numbers are loaded into 
the 80-bit floating point registers on the CPU).

The actual results will vary from system to system (on my Linux system the 
output of x is the same at all stages, and is different from what you 
report), but nothing beyond the sixteenth digit is at all meaningful.


 	-thomas


> (this is just a sample, my real problem lies in numbers that I import from a
> datafile and I don't know how to control them when this switching occurs)
>
> ###########################
> options( digits=22 )
> x <- matrix(NA, 3,1)
> x[1] <- 6.94788635771657950
> x      # 6.947886357716580     # DIFFERENT ???
>        # NA
>        # NA
> x[1] == 6.94788635771657950   # but is the same
>
> x[2] <- 8.34284937565404050
> x      # 6.947886357716580     # DIFFERENT ???
>        # 8.342849375654041     # DIFFERENT ???
>        # NA
> x[1] == 6.94788635771657950   # but is the same
>
> x[3] <- 0.19043842368566377
> x      # 6.94788635771657950   # NOW OK!!!
>        # 8.34284937565404050   # "   "
>        # 0.19043842368566377   # "   "
> x[1] == 6.94788635771657950   # but is the same
>
>
> ###
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          3.1
> year           2006
> month          06
> day            01
> svn rev        38247
> language       R
> version.string Version 2.3.1 (2006-06-01)
>
>
> -- 
> Regards,
> Hans-Peter
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From tlumley at u.washington.edu  Mon Jun 19 20:33:26 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 19 Jun 2006 11:33:26 -0700 (PDT)
Subject: [R] lmer binomial model overestimating data?
In-Reply-To: <0E50C39C-FFD9-43E9-BA06-243DD1DD2A8E@muohio.edu>
References: <0E50C39C-FFD9-43E9-BA06-243DD1DD2A8E@muohio.edu>
Message-ID: <Pine.LNX.4.64.0606191123010.9084@homer23.u.washington.edu>

On Wed, 14 Jun 2006, Martin Henry H. Stevens wrote:

> Hi folks,
> Warning: I don't know if the result I am getting makes sense, so this
> may be a statistics question.
>
> The fitted values from my binomial lmer mixed model seem to
> consistently overestimate the cell means, and I don't know why. I
> assume I am doing something stupid.

Not really, there is something subtle going on.  The model says that

  logit E[Y|x, random effects] = x*beta+random effects

Now, when you compute the observed values you are averaging over the 
random effects to get

    E[E[Y|x, random effects]]= E[ invlogit(x*beta +random effects)]

where invlogit is the inverse of logit.

When you compute the fitted values you are also averaging, but on the 
linear predictor scale to get
    E[logit(E[Y|x, random effects])]= invlogit(x*beta)

The logit/unlogit operation is not linear, so these are not the same. In 
fact, invlogit(x*beta) is always further from 1/2 than E[Y|X].


With linear regression it is useful and fairly standard to think of the 
random effects part of a mixed model as giving a model for the covariance 
of Y, seperate from the fixed-effects model for the mean of Y.  With 
generalized linear models these can no longer be separated: adding random 
effects changes the values and the meaning of the fixed effects 
parameters.


 	-thomas


From jones at reed.edu  Mon Jun 19 20:47:57 2006
From: jones at reed.edu (Albyn Jones)
Date: Mon, 19 Jun 2006 11:47:57 -0700
Subject: [R] MLE maximum number of parameters
In-Reply-To: <4496E47C.5000303@pburns.seanet.com>
References: <B2B70BDD-81EC-4043-A769-020A2AC16923@imperial.ac.uk>
	<4496E47C.5000303@pburns.seanet.com>
Message-ID: <20060619184757.GG12635@laplace.reed.edu>

I regularly optimize functions of over 1000 parameters for posterior
mode computations using a variant of newton-raphson.  I have some
favorable conditions: the prior is pretty good, the posterior is
smooth, and I can compute the gradient and hessian.

albyn


On Mon, Jun 19, 2006 at 06:53:00PM +0100, Patrick Burns wrote:
> Seagulls have a very different perspective to ballparks
> than ants.  Nonetheless, there is something that can be
> said.
> 
> There are several variables in addition to the number of
> parameters that are important.  These include:
> 
> * The complexity of the likelihood
> 
> * The number of observations in the dataset
> 
> * How close to the optimum is close enough
> 
> * Your patience
> 
> The latter is undoubtedly the most important of all.  It
> matters a lot whether you think a minute is a long time
> or only periods measured in weeks.
> 
> The optimization strategy can also have a big effect.  If
> you are using a derivative-based optimizer, then the number
> of parameters can have a big impact.  Typically one iteration
> in such algorithms requires p+1 function calls, where p is the
> number of parameters.  Since more iterations are generally
> required with more parameters, the speed can decrease
> rapidly as the number of parameters increases.
> 
> One strategy to deal with a large number of parameters is to
> start with something like a genetic algorithm.  Once the genetic
> algorithm has a pretty good solution, then switch to a derivative-
> based algorithm to finish.  The amount to run the initial
> algorithm before switching depends on the problem, the quality
> of the two optimizers, and probably other things.
> 
> With this switching strategy and at least a modicum of patience,
> problems with thousands of parameters may be feasible to solve.
> 
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> Federico Calboli wrote:
> 
> >Hi All,
> >
> >I would like to know, is there a *ballpark* figure for how many  
> >parameters the minimisation routines can cope with?
> >
> >I'm asking because I was asked if I knew.
> >
> >Cheers,
> >
> >Federico
> >
> >--
> >Federico C. F. Calboli
> >Department of Epidemiology and Public Health
> >Imperial College, St. Mary's Campus
> >Norfolk Place, London W2 1PG
> >
> >Tel +44 (0)20 75941602   Fax +44 (0)20 75943193
> >
> >f.calboli [.a.t] imperial.ac.uk
> >f.calboli [.a.t] gmail.com
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From leaflovesun at yahoo.ca  Mon Jun 19 20:51:24 2006
From: leaflovesun at yahoo.ca (Leaf Sun)
Date: Mon, 19 Jun 2006 12:51:24 -0600
Subject: [R] number of iteration s exceeded maximum of 50
References: <1bff52c20606140033j3ac8e9fdxa85d4edded2ad94a@mail.gmail.com>
	<1bff52c20606140101r25188de1g6573c52f3cb5d152@mail.gmail.com>
	<38b9f0350606140223g507112dau3478586790daad3d@mail.gmail.com>
	<200606140735459773486@yahoo.ca>
	<449285BD.3020101@statistik.uni-dortmund.de>
	<200606161041072848729@yahoo.ca>
	<40e66e0b0606180515t759f1400od071251942d8dac6@mail.gmail.com>
Message-ID: <200606191251227312041@yahoo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060619/813470b7/attachment.pl 

From lobry at biomserv.univ-lyon1.fr  Mon Jun 19 20:53:33 2006
From: lobry at biomserv.univ-lyon1.fr (Jean lobry)
Date: Mon, 19 Jun 2006 20:53:33 +0200
Subject: [R] analyze amino acid sequence (composition)of proteins
In-Reply-To: <mailman.9.1150711204.3935.r-help@stat.math.ethz.ch>
References: <mailman.9.1150711204.3935.r-help@stat.math.ethz.ch>
Message-ID: <p0600200fc0bc9ddde9dd@[192.168.1.10]>

>i am a newer to R and i am doing some protein category classification based on
>the amino acid sequence.while i have some questions urgently.

I'm not sure to understand exactly what you are trying to do, but if
this is a kind of clustering from the amino-acid composition of
proteins beware that the genomic G+C content is a major confounding
factor.

>1. any packages for analysis amino acid sequence

There are some utilities in the seqinR package to retrieve protein
sequences from various protein databases such as SwissProt. The
list of available databases (including nucleic databases) is given by:

   library(seqinr)
   choosebank()
  [1] "genbank"      "embl"         "emblwgs"      "swissprot"    "ensembl"
  [6] "ensemblnew"   "emglib"       "nrsub"        "nbrf"         "hobacnucl"
[11] "hobacprot"    "hovernucl"    "hoverprot"    "hogennucl"    "hogenprot"
[16] "hoverclnu"    "hoverclpr"    "homolensprot" "homolensnucl" "HAMAPnucl"
[21] "HAMAPprot"    "hoppsigen"    "nurebnucl"    "nurebprot"    "taxobacgen"
[26] "greview"      "hogendnucl"   "hogendprot"   "refseq"

More info with the infobank argument set to TRUE:

   choosebank(infobank = TRUE)[1:5,]
        bank status 
info
1   genbank     on             GenBank Rel. 154 (15 June 2006) Last 
Updated: Jun 18, 2006
2      embl     on                                   EMBL Library 
Release 86 (March 2006)
3   emblwgs     on           EMBL Whole Genome Shotgun sequences 
Release 86  (March 2006)
4 swissprot     on UniProt Rel. 7 (SWISS-PROT 49 + TrEMBL 32): Last 
Updated: Jun  6, 2006
5   ensembl     on 
Ensembl databases rel 24

There are also some tools to compute protein indices such as the
Kyte and Doolittle hydrophaty index, or the PI (Isoelectric point
of a protein). See the examples in this document:
http://pbil.univ-lyon1.fr/software/SeqinR/seqinr_1_0-4.pdf

Most protein indices are linear forms on amino acid
relative frequencies whose coefficients are available in the
AAindex (http://www.genome.jp/aaindex/).

>2. given two sequences "AAA" and "BBB",how can i combine them into "AAABBB"

paste() is your friend:

   paste("AAA", "BBB", sep = "")
[1] "AAABBB"

Or define your own infix binary operator for convenience:

   "%+%" <- function(...) paste(..., sep = "")
   "AAA" %+% "BBB" %+% "CCC"
[1] "AAABBBCCC"


>3. based on "AAABBB",how can i get some statistics of this string such as how
>many letters,how many "A"s in the string.

  table(s2c("AAABBB"))

A B
3 3

With a real protein sequence:

  prot <- read.fasta(File = system.file("sequences/seqAA.fasta", 
package = "seqinr"), seqtype = "AA")
  table(prot)
prot
  *  A  C  D  E  F  G  H  I  K  L  M  N  P  Q  R  S  T  V  W  Y
  1  8  6  6 18  6  8  1  9 14 29  5  7 10  9 13 16  7  6  3  1

If you want the three letter code instead:

   table(prot) -> tmp
   names(tmp) <- aaa(names(tmp))
   tmp
Stp Ala Cys Asp Glu Phe Gly His Ile Lys Leu Met Asn Pro Gln Arg Ser 
Thr Val Trp Tyr
   1   8   6   6  18   6   8   1   9  14  29   5   7  10   9  13  16 
7   6   3   1

Try also:

AAstat(prot[[1]])

Hope this helps,

-- 
Jean R. Lobry            (lobry at biomserv.univ-lyon1.fr)
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - LYON I,
43 Bd 11/11/1918, F-69622 VILLEURBANNE CEDEX, FRANCE
allo  : +33 472 43 12 87     fax    : +33 472 43 13 88
http://pbil.univ-lyon1.fr/members/lobry/


From deepayan.sarkar at gmail.com  Mon Jun 19 20:57:45 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 19 Jun 2006 13:57:45 -0500
Subject: [R] lattice xyplot - aligning date labels so that they align
	with the grid lines in panel.grid
In-Reply-To: <182544D7A3144B42994EEA5662C54E0102D692F1@NLDNC105PEX1.ubsw.net>
References: <182544D7A3144B42994EEA5662C54E0102D692F1@NLDNC105PEX1.ubsw.net>
Message-ID: <eb555e660606191157h44bed04ci46b0334b5a86e258@mail.gmail.com>

On 6/19/06, john.gavin at ubs.com <john.gavin at ubs.com> wrote:
> Hi,
>
> I have a basic question about aligning date labels for the x-axis
> in an xyplot so that they align with the grid lines
> from the panel.grid argument.
>
> For example, with
>
> x <- data.frame(
>   date = seq(as.Date("2005/01/01"), as.Date("2006/06/01"),
>   length.out = 20), value = runif(20))
> xyplot(value ~ date, data = x,
>   panel = function(x, y, subscripts, ...)
>   { panel.grid(h = -1, v = -1, col = "grey", lwd = 1, lty = 1)
>     panel.xyplot(x, y, ...)
>   })
>
> How can I get the labels on the x-axis to align
> with the vertical grid lines?
> i.e. I have 6 vertical grid lines by default (v = -1)
> in this example so I would like 6 labels along the x-axis
> at the same points, whereas I see only 2 (2005 and 2006).

You will need to do it manually, e.g.:

xyplot(value ~ date, data = x,
       panel = function(x, y, subscripts, ...) {
           panel.grid(h = -1, v = 0, col = "grey", lwd = 1, lty = 1)
           panel.abline(v = as.Date(c("2005/01/01", "2006/01/01")),
                        col = "grey", lwd = 1, lty = 1)
           panel.xyplot(x, y, ...)
       })

For more, you can put in more locations in panel.abline, and the same
locations as tick mark positions in scales$y$at.

> As an alternative I would be happy to just specify the number of labels
> as long as panel.grid's vertical lines aligned with the labels.

Not possible currently, unlikely to be possible ever.

> > R.version.string
> [1] "Version 2.3.1 (2006-06-01)"
>
> on Windows NT4.
>
> Regards,
>
> John.
>
> John Gavin <john.gavin at ubs.com>,
> Commodities, FIRC,
> UBS Investment Bank, 2nd floor,
> 100 Liverpool St., London EC2M 2RH, UK.
> Phone +44 (0) 207 567 4289
> This communication is issued by UBS AG or an affiliate ("UBS...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


-- 
http://www.stat.wisc.edu/~deepayan/


From elatine at gmail.com  Mon Jun 19 21:38:34 2006
From: elatine at gmail.com (=?ISO-8859-1?Q?B=E1lint_Cz=FAcz?=)
Date: Mon, 19 Jun 2006 21:38:34 +0200
Subject: [R] multivariate splits
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA026FFDA3@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA026FFDA3@usctmx1106.merck.com>
Message-ID: <fab4bcf70606191238o61818ef8q@mail.gmail.com>

Yes,
I am interested in tree-building algorithms that consider some
(linear) combination of the covariates during the splitting. Sorry if
I have not been clear enough.

There seem to be a large variety of methods (e.g. see
http://www.tigr.org/~salzberg/murthy_thesis/survey/node11.html),
I hoped that some methods might have already been developed as R packages...

Thanks for the suggestions.
B?lint


2006/6/19, Liaw, Andy <andy_liaw at merck.com>:
> If by "multivariate split" the OP meant splitting on combinations of
> covariates (instead of multi-way split on a single covariate at a time),
> there aren't that many methods published (AFAIK).  All the ones I know about
> are in the cheminformatics area:  RP-SA and generalizations/extensions of
> it.  No R package I know of can do it.
>
> Best,
> Andy
>
> From: Torsten Hothorn
> >
> > On Mon, 19 Jun 2006, B?lint Cz?cz wrote:
> >
> > > Dear R users!
> > >
> > > Does someone know about any algorithms / packages in R,
> > that perform
> > > classification / regression / decision trees using multivariate
> > > splits?
> > >
> > > I have done some research, but I found nothing. Packages "tree" and
> > > "rpart" seem only to be able to do CART with univariate splits.
> > >
> >
> > have a look at the machine learning task view on CRAN, which
> > will point you to package `RWeka'.
> >
> > HTH,
> >
> > Torsten
> >
> > > Thank you for your help!
> > >
> > > B?lint
> > >
> > > --
> > > Cz?cz B?lint
> > > PhD hallgat?
> > > BCE KTK Talajtan ?s V?zgazd?lkod?s Tansz?k
> > > 1118 Budapest, Vill?nyi ?t 29-43.
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> > >
> >
>
>
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}


From Mike.Prager at noaa.gov  Mon Jun 19 22:06:32 2006
From: Mike.Prager at noaa.gov (Michael H. Prager)
Date: Mon, 19 Jun 2006 16:06:32 -0400
Subject: [R] can I call user-created functions without source() ?
In-Reply-To: <4496DFA7.6040402@pdf.com>
References: <44967019.50009@robertcampbell.co.uk> <4496C4FE.4060507@noaa.gov>
	<4496D862.5070408@stats.uwo.ca> <4496DFA7.6040402@pdf.com>
Message-ID: <449703C8.9080500@noaa.gov>

on 6/19/2006 1:32 PM Spencer Graves said the following:
>       I'd like to echo Duncan's comment about creating 'man' pages for 
> functions.  I find myself writing man pages for functions I may never 
> include in an R package, just because it helps me think through what 
> I'm trying to do.  I think I get better code faster.  It's like 
> preparing a map before starting on a journey through unknown 
> territory:  I may think I know the destination and more or less every 
> step of the way.  However, if it's very far, it helps to make a plan 
> first;  then when I encounter something I hadn't anticipated, it makes 
> it much easier to figure out what to do without sabotaging something else.

Yes, I agree on the value of planning and documentation.  I hope this 
year to have the time to learn the package-generation system, which on 
first impression seems a bit trickier on Windows than on Unix etc. 

Last but not least, thanks for the tips on load() vs. attach().

...Mike

-- 
Michael Prager, Ph.D.
Southeast Fisheries Science Center
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
** Opinions expressed are personal, not official.  No
** official endorsement of any product is made or implied.


From tguennel at vcu.edu  Mon Jun 19 22:06:37 2006
From: tguennel at vcu.edu (Tobias Guennel)
Date: Mon, 19 Jun 2006 16:06:37 -0400
Subject: [R] Nested variance-covariance matrix in Multilevel model
Message-ID: <449703CD.5030505@vcu.edu>

Dear R community,

I have trouble implementing a nested variance-covariance matrix in the 
lme function.

The model has two fixed effects called End and logpgc, the response 
variable is the logarithm to base 2 of  Intensity ( log2(Intensity) ) 
and the random effects are called Probe and ProbeNo.
The model has the following nesting structure: A Pixel is nested within 
the ProbeNo,the ProbeNo is within the ProbeEnd ( there are two ends for 
every probe), and the ProbeEnd is within the Probe.

Now the problem I have is that the variance-covariance structure of the 
model is quite complex and I can not find the right syntax for fitting 
it in the lme function.

The variance-covariance structure  is a block diagonal matrix of the form,
          V1   0       0
 V=    0      V2    0
          0      0        V3

where V1...V3 are of the structure:
          v11      v12
  V1=                          and so on.
          v21      v22

V1...V3 are assumed to have a compound symmetric variance-covariance 
structure and therefore the submatrices are of the form:
                   Lambda   Delta1       Delta1 ...   Delta1
                   Delta1      Lambda    Delta1 ...   Delta1
v11=v22=    .......

                   Delta1   .....                               Lambda
                  
                   Delta2      Delta2       Delta2 ...   Delta2
                   Delta2      Delta2       Delta2 ...   Delta2
v12=v21=    .......

                   Delta2   .....                               Delta2

The elements of these submatrices depend only upon the three covariance 
parameters: the compound symmetry parameter delta; the variance of 
random effect sigma^2g; and the residual variance sigma^2. I have 
formulas for the submatrices Lambda,Delta1 and Delta2 which I can't 
really paste in here.

The SAS code dealing with this model is the following:

proc mixed data=rnadeg.pnau;
title 'CV structure for PNAU';
class probepos probeno end probe pixelid newprobeid;
model logPM=end logpgc / ddfm=satterth;
random probeno newprobeid / subject=probe type=cs;
lsmeans end / diff cl; run;

Any ideas are appreciated a lot since I am kind of stuck at this point.

Thank you
Tobias Guennel


From tguennel at vcu.edu  Mon Jun 19 22:43:39 2006
From: tguennel at vcu.edu (Tobias Guennel)
Date: Mon, 19 Jun 2006 16:43:39 -0400
Subject: [R]  Nested variance-covariance matrix in Multilevel model
Message-ID: <44970C7B.3050306@vcu.edu>

I completely forgot to supply the R code I tried:

vov1i2<-read.table("VOV1_INHIBITED6-16-2006-13h35min33sec.txt",header=TRUE)
test.lme<-lme(fixed=log2(Intensity)~End+logpgc,random=list(Probe=pdBlocked(list(~1,pdCompSymm(~1 
),End=~1,ProbeNo=pdCompSymm(~1)),data=vov1i2)

It doesn't look right to me and it produces an error too.

It should be something around those lines though.

Tobias Guennel


From Greg.Snow at intermountainmail.org  Mon Jun 19 23:03:00 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 19 Jun 2006 15:03:00 -0600
Subject: [R] Change the range of a 'ternaryplot'
Message-ID: <07E228A5BE53C24CAD490193A7381BBB45DDB2@LP-EXCHVS07.CO.IHC.COM>

Try the function triangle.plot from the ade4 package. 


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mart?nez Ovando Juan Carlos
Sent: Thursday, June 15, 2006 7:17 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Change the range of a 'ternaryplot'

Hi all

Does someone know how can I alter the range of some variables in a 'ternaryplot' of the 'vcd' package, with the aim of make a zoom in some region of the plot?

Thank you in advance

Sincerely,

	Juan Carlos


===================================
Juan Carlos Mart?nez Ovando
Banco de M?xico
Direcci?n General de Investigaci?n Econ?mica Cinco de Mayo No. 18, 4? Piso A Col. Centro, Del. Cuauht?moc 06059, M?xico D.F., MEXICO
Tel.: +52 (55) 52.37.25.94
Fax: +52 (55) 52.37.27.03
e-mail: jcmartinez at banxico.org.mx
=================================== 

"2006, A?o del Bicentenario del natalicio del Benem?rito de las Am?ricas, Don Benito Ju?rez Garc?a".

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From faheem at email.unc.edu  Mon Jun 19 23:04:36 2006
From: faheem at email.unc.edu (Faheem Mitha)
Date: Mon, 19 Jun 2006 17:04:36 -0400 (EDT)
Subject: [R] binary for Rmath standalone library for Visual C++ 8.0?
Message-ID: <Pine.LNX.4.62.0606191701020.22817@Chrestomanci>


Hi,

Does anyone happen to have compiled the Rmath standalone library for 
Visual C++ 8.0, and can offer it for me to download?

I'm not that familiar with Microsoft or Visual C++. I mostly use Linux. 
However, my boss needs to use this library on Windows to compile some code 
I wrote, and I don't have the time or (probably) the expertise to compile 
it myself.

Help would be much appreciated. Thanks.

                                                                Faheem.


From faheem at email.unc.edu  Mon Jun 19 23:16:21 2006
From: faheem at email.unc.edu (Faheem Mitha)
Date: Mon, 19 Jun 2006 17:16:21 -0400 (EDT)
Subject: [R] binary for Rmath standalone library for Visual C++ 8.0?
In-Reply-To: <Pine.LNX.4.62.0606191701020.22817@Chrestomanci>
References: <Pine.LNX.4.62.0606191701020.22817@Chrestomanci>
Message-ID: <Pine.LNX.4.62.0606191714050.22817@Chrestomanci>



On Mon, 19 Jun 2006, Faheem Mitha wrote:

>
> Hi,
>
> Does anyone happen to have compiled the Rmath standalone library for Visual 
> C++ 8.0, and can offer it for me to download?
>
> I'm not that familiar with Microsoft or Visual C++. I mostly use Linux. 
> However, my boss needs to use this library on Windows to compile some code I 
> wrote, and I don't have the time or (probably) the expertise to compile it 
> myself.
>
> Help would be much appreciated. Thanks.

Forgot to say that this is on Windows XP (Professional x64 Edition). 
Sorry.

                                                              Faheem.


From tlumley at u.washington.edu  Mon Jun 19 23:17:49 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 19 Jun 2006 14:17:49 -0700 (PDT)
Subject: [R] Post Stratification
In-Reply-To: <4495B23C.2080304@t-online.de>
References: <4495B23C.2080304@t-online.de>
Message-ID: <Pine.LNX.4.64.0606191308450.9084@homer23.u.washington.edu>

On Sun, 18 Jun 2006, Mark Hempelmann wrote:

> Dear WizaRds,
>
> 	having met some of you in person in Vienna,  I think even more fondly
> of this community and hope to continue on this route. It was great
> talking with you and learning from you. Thank you. I am trying to work
> through an artificial example in post stratification. This is my dataset:
>
> library(survey)
> age <- data.frame(id=1:8, stratum=rep( c("S1","S2"),c(5,3)),
> weight=rep(c(3,4),c(5,3)), nh=rep(c(5,3),c(5,3)),
> Nh=rep(c(15,12),c(5,3)), y=c(23,25,27,21,22, 77,72,74) )
>
> pop.types <- table(stratum=age$stratum)

No, you need population totals here, that's the whole point of 
post-stratification.

> age.post <- svydesign(ids=~1, strata=NULL, data=age, fpc=~Nh)

Since the sampling isn't stratified you can't give stratum-specific fpc. 
This should give an error message. I will add one.

> post <- postStratify(design=age.post, strata=~stratum, population=pop.types)

Yes, but pop.types is defined wrongly

>
> pop.types <- data.frame(stratum = c("S1","S2"), Freq = c(15, 12))

This is the correct definition



> However, I compute:
> Nh=c(15,12); nh=c(5,3); sh=by(age$y, age$stratum, var); N=sum(Nh)
> # Mean estimator
> y.bar=by(age$y, age$stratum, mean) ## 23.6; 74.33
> estimator=1/N*sum(Nh*y.bar) ## 46.14815
> # Variance estimator
> vari=1/N^2*sum(Nh*(Nh-nh)*sh/nh)
> sqrt(vari) ##	.7425903
>
> and with Taylor expansion .7750118
>

This is partly the problem with giving stratum-specific fpc and partly 
because "survey" is using a less elegant but more general estimator (as is 
often the case).  The estimator is the one due to Valliant, described in 
the reference on the help page.

The primary difference from the estimator you given is that R does not try 
to compute finite population corrections for each post-stratum.  Another 
more subtle difference is that R's formula is estimating the conditional 
variance, conditional on the estimated weights, rather than the 
unconditional variance.


So, to do the computations correctly

> library(survey)
> age <- data.frame(id=1:8, stratum=rep( c("S1","S2"),c(5,3)),
+ weight=rep(c(3,4),c(5,3)), nh=rep(c(5,3),c(5,3)),
+ Nh=rep(c(15,12),c(5,3)), y=c(23,25,27,21,22, 77,72,74) )
>
> age$N<-rep(27,8)
>
> pop.types <- data.frame(stratum = c("S1","S2"), Freq = c(15, 12))
>

The sampling is unstratified from a population of size 27=15+12

> age.design <- svydesign(ids=~1, data=age, fpc=~N)
> age.post<-postStratify(age.design, strata=~stratum, population=pop.types)
>
> svymean(~y, age.design)
     mean     SE
y 42.625 7.8163
> svymean(~y, age.post)
     mean     SE
y 46.148 0.6737
>

Now, post-stratification adjusts the weights to sum to 15 and 12 for the 
two post-strata, so the mean estimate is a weighted average of the two 
post-stratum-specific means, weighted by 15/27 and 12/27. This is what you 
found.

The variance is the variance of the mean of the residuals after 
subtracting the post-stratum-specific mean.  That is, we can get the 
post-stratified estimates by

Adjust the weights
> age.design2 <- svydesign(ids=~1, data=age, fpc=~N,weight=~weight)
Create a 'residuals' variable
> age.design2<-update(age.design2, yres=y-ifelse(stratum=="S1", 
23.6,74.33333333))

Post-stratified mean is 46.148
> svymean(~y, age.design2)
     mean     SE
y 46.148 8.2317

Standard error of post-stratified mean is 0.6737
> svymean(~yres, age.design2)
            mean     SE
yres 1.4815e-09 0.6737


The formula for the variance of the estimated total is

> (n/(n-1))*((N-sum(nh))/N)*sum(yr2*(Nh/nh)^2)
[1] 330.915

which is formula (11) of the Valliant reference. The variance of the
mean is the same thing divided by N^2:

> (n/(n-1))*((N-sum(nh))/N)*sum(yr2*(Nh/nh)^2)/(N*N)
[1] 0.45393

where yr2<-by(yres^2,age$stratum,sum).  These agree exactly with R's 
estimates
> vcov(svytotal(~y, design=age.post))
         y
y 330.915
> vcov(svymean(~y, design=age.post))
         y
y 0.45393


 	-thomas



Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From ggrothendieck at gmail.com  Tue Jun 20 02:36:03 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 19 Jun 2006 20:36:03 -0400
Subject: [R] function call
In-Reply-To: <17558.51185.342802.644781@stat.math.ethz.ch>
References: <44967421.3040007@7d4.com>
	<971536df0606190454l260311e6wef1c4e95d27a87ef@mail.gmail.com>
	<4496B51E.2020208@7d4.com>
	<17558.51185.342802.644781@stat.math.ethz.ch>
Message-ID: <971536df0606191736n6edd2853ya54164180a87d6df@mail.gmail.com>

On 6/19/06, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> >>>>> "vincent" == vincent  <vincent at 7d4.com>
> >>>>>     on Mon, 19 Jun 2006 16:30:54 +0200 writes:
>
>    vincent> Gabor Grothendieck a ?crit :
>    >> Perhaps you what you want to do is to return an object
>    >> that has a print method like this:
>
>    >> f1 <- function(x) structure(c(x, x^2), class = "f1")
>    >> print.f1 <- function(x) cat("x is", x[1], "x squared is", x[2], "\n")
>
>    vincent> Thank you Gabor for this idea.
>
> Unfortunately, it's not a good idea inspite of coming from Gabor who
> has donated many very good ideas to R-help:
>
> help(print)  tells you
>
>  >>   'print' prints its argument and returns it _invisibly_ (via
>  >>   'invisible(x)').  It is a generic function which means that new
>  >>   printing methods can be easily added for new 'class'es.
>
> I.e., as a good Ritizen, Gabor's function above should **REALLY** be
>
>   print.f1 <- function(x) {
>          cat("x is", x[1], "x squared is", x[2], "\n")
>          invisible(x)
>   }
>

ok, in that case perhaps it would be better to define a summary
method.

f1 <- function(x) structure(c(x, x^2), class = "f1")
summary.f1 <- function(x) c(x = x[1], xsquared = x[2], sum = sum(x))

out <- f1(3)
out
unclass(out)
summary(out)


From alex_restrepo at hotmail.com  Tue Jun 20 02:43:27 2006
From: alex_restrepo at hotmail.com (Alex Restrepo)
Date: Mon, 19 Jun 2006 19:43:27 -0500
Subject: [R] Custom Command to Generate SQL
In-Reply-To: <971536df0606182221r3166a098i50d0cf5550d5f60d@mail.gmail.com>
Message-ID: <BAY106-F181D8ADFECE3F59DA31E0A98870@phx.gbl>

Hi Gabor:

Thanks for the great example.  I am an R newbie, so please forgive my 
question, but
could you describe what the sub() function is doing in your example? Why is 
there an "and" in the first argument to the sub() function?


Many Thanks:

Alex


>From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
>To: "Alex Restrepo" <alex_restrepo at hotmail.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] Custom Command to Generate SQL
>Date: Mon, 19 Jun 2006 01:21:53 -0400
>
>This generates the sql statement so just pass that to your database:
>
>retrieve <- function(...) {
>   args <- list(...)
>   sub("and", "select * from people where",
>      paste(rbind("and", names(args), "=", dQuote(args)), collapse = " "))
>}
>
># test
>retrieve(firstname = "JOHN", middlename = "WILLIANS", lastname = "FORD")
>
>On 6/19/06, Alex Restrepo <alex_restrepo at hotmail.com> wrote:
>>Hi:
>>
>>I would like to create a custom command in R which generates SQL, which is
>>then processed via RODBC.
>>
>>For example, the user would type:
>>
>>        retrieve firstName('JOHN') middlleName('WILLIAMS') 
>>lastName('FORD')
>>
>>This would generate the following SQL which would then be processed by
>>RODBC:
>>
>>Select *
>>  from people
>>  where firstName = 'JOHN' and
>>            middleName = 'WILLIAMS' and
>>            lastName     = 'FORD'
>>
>>Does anyone have a recommendation?  Any ideas would be greatly 
>>appreciated.
>>
>>Alex
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>


From epistat at gmail.com  Tue Jun 20 03:09:16 2006
From: epistat at gmail.com (zhijie zhang)
Date: Tue, 20 Jun 2006 09:09:16 +0800
Subject: [R] how to put the results of loop into a dataframe
Message-ID: <2fc17e30606191809m35e242e8te67ace883d0ac589@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060620/05725995/attachment.pl 

From epistat at gmail.com  Tue Jun 20 03:11:34 2006
From: epistat at gmail.com (zhijie zhang)
Date: Tue, 20 Jun 2006 09:11:34 +0800
Subject: [R] how to put the results of loop into a dataframe
Message-ID: <2fc17e30606191811l36246181p590eacd10e57e490@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060620/e1d6fb55/attachment.pl 

From ggrothendieck at gmail.com  Tue Jun 20 04:34:02 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 19 Jun 2006 22:34:02 -0400
Subject: [R] Custom Command to Generate SQL
In-Reply-To: <BAY106-F181D8ADFECE3F59DA31E0A98870@phx.gbl>
References: <971536df0606182221r3166a098i50d0cf5550d5f60d@mail.gmail.com>
	<BAY106-F181D8ADFECE3F59DA31E0A98870@phx.gbl>
Message-ID: <971536df0606191934v5f4c9534odb471ff1684d3c63@mail.gmail.com>

Hi, Here is an example of paste:

> paste("and", 1:3, collapse = " ")
[1] "and 1 and 2 and 3"

in which and was prepended to each of the vector entries and then
everything was collapsed together.  In our
situation we don't actually want the first 'and' so we substitute for
it, using sub,
the first portion of the SQL statement.

Suggest you turn on debugging:

debug(retrieve)  # undebug(retrieve) will turn it back off

and run the function and at each try printing out various partial
expressions to see what they do.

Regards.


On 6/19/06, Alex Restrepo <alex_restrepo at hotmail.com> wrote:
> Hi Gabor:
>
> Thanks for the great example.  I am an R newbie, so please forgive my
> question, but
> could you describe what the sub() function is doing in your example? Why is
> there an "and" in the first argument to the sub() function?
>
>
> Many Thanks:
>
> Alex
>
>
> >From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
> >To: "Alex Restrepo" <alex_restrepo at hotmail.com>
> >CC: r-help at stat.math.ethz.ch
> >Subject: Re: [R] Custom Command to Generate SQL
> >Date: Mon, 19 Jun 2006 01:21:53 -0400
> >
> >This generates the sql statement so just pass that to your database:
> >
> >retrieve <- function(...) {
> >   args <- list(...)
> >   sub("and", "select * from people where",
> >      paste(rbind("and", names(args), "=", dQuote(args)), collapse = " "))
> >}
> >
> ># test
> >retrieve(firstname = "JOHN", middlename = "WILLIANS", lastname = "FORD")
> >
> >On 6/19/06, Alex Restrepo <alex_restrepo at hotmail.com> wrote:
> >>Hi:
> >>
> >>I would like to create a custom command in R which generates SQL, which is
> >>then processed via RODBC.
> >>
> >>For example, the user would type:
> >>
> >>        retrieve firstName('JOHN') middlleName('WILLIAMS')
> >>lastName('FORD')
> >>
> >>This would generate the following SQL which would then be processed by
> >>RODBC:
> >>
> >>Select *
> >>  from people
> >>  where firstName = 'JOHN' and
> >>            middleName = 'WILLIAMS' and
> >>            lastName     = 'FORD'
> >>
> >>Does anyone have a recommendation?  Any ideas would be greatly
> >>appreciated.
> >>
> >>Alex
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide!
> >>http://www.R-project.org/posting-guide.html
> >>
>
>
>


From spencer.graves at pdf.com  Tue Jun 20 04:52:24 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 19 Jun 2006 19:52:24 -0700
Subject: [R] Effect size in mixed models
In-Reply-To: <002c01c6919d$39416d80$6400a8c0@brungio>
References: <002c01c6919d$39416d80$6400a8c0@brungio>
Message-ID: <449762E8.8000507@pdf.com>

	  You have asked a great question:  It would indeed be useful to 
compare the relative magnitude of fixed and random effects, e.g. to 
prioritize efforts to better understand and possibly manage processes 
being studied.  I will offer some thoughts on this, and I hope if there 
are errors in my logic or if someone else has a better idea, we will 
both benefit from their comments.

	  The ideal might be an estimate of something like a mean square for a 
particular effect to compare with an estimated variance component.
Such mean squares were a mandatory component of any analysis of variance 
table prior to the (a) popularization of generalized linear models and 
(b) availability of software that made it feasible to compute maximum 
likelihood estimates routinely for unbalanced, mixed-effects models. 
However, anova(lme(...)) such mean squares are for most purposes 
unnecessary cluster in a modern anova table.

	  To estimate a mean square for a fixed effect, consider the following 
log(likelihood) for a mixed-effects model:

	  lglk = (-0.5)*(n*log(2*pi*var.e)-log(det(W)) + 
t(y-X%*%b)%*%W%*%(y-X%*%b)/var.e),

where n = the number of observations,

       b = the fixed-effect parameter variance,

and the covariance matrix of the residuals, after integrating out the 
random effects is var.e*solve(W).  In this formulation, the matrix "W" 
is a function of the variance components.  Since they are not needed to 
compute the desired mean squares, they are suppressed in the notation 
here.

	  Then, the maximum likelihood estimate of

	  var.e = SSR/n,

where SSR = t(y-X%*%b)%*%W%*%(y-X%*%b).

	  Then

	  mle.lglk = (-0.5)*(n*(log(2*pi*SSR/n)-1)-log(det(W))).

	  Now let

	  SSR0 = this generalized sum of squares of residuals (SSR) without 
effect "1",

and

	  SSR1 = this generalized SSR with this effect "1".

	  If I've done my math correctly, then

	  D = deviance = 2*log(likelihood ratio)
	    = (n*log(SSR0/SSR1)+log(det(W1)/det(W0)))

	  For roughly half a century, a major part of "the analysis of 
variance" was the Pythagorean idea that the sum of squares under H0 was 
the sum of squares under H1 plus the sum of squares for effect "1":

	  SSR0 = SS1 + SSR1.

	  Whence,

	  exp((D/n)-log(det(W1)/det(W0))) = 1+SS1/SSR1.

Thus,

	  SS1 = SSR1*(exp((D/n)-log(det(W1)/det(W0)))-1).

	  If the difference between deg(W1) and det(W0) can be ignored, we get:

	  SS1 = SSR1*(exp((D/n)-1).

	  Now compute MS1 = SS1/df1, and compare with the variance components.

	  If there is a flaw in this logic, I hope someone will disabuse me of 
it.

	  If this seems too terse or convoluted to follow, please provide a 
simple, self-contained example, as suggested in the posting guide! 
"www.R-project.org/posting-guide.html".  You asked a theoretical 
question, you got a theoretical answer.  If you want a concrete answer, 
it might help to pose a more concrete question.

	  Hope this helps.
	  Spencer Graves	

Bruno L. Giordano wrote:
> Hello,
> Is there a way to compare the relative relevance of fixed and random effects 
> in mixed models? I have in mind measures of effect size in ANOVAs, and would 
> like to obtain similar information with mixed models.
> 
> Are there information criteria that allow to compare the relevance of each 
> of the effects in a mixed model to the overall fit?
> 
> Thank you,
>     Bruno
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From spencer.graves at pdf.com  Tue Jun 20 05:04:15 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 19 Jun 2006 20:04:15 -0700
Subject: [R] how to successfully remove missing values for a repeated
 measures analysis
In-Reply-To: <5.1.0.14.0.20060618184815.00bec300@pop3.brisnet.org.au>
References: <5.1.0.14.0.20060618184815.00bec300@pop3.brisnet.org.au>
Message-ID: <449765AF.9080008@pdf.com>

	  I don't see enough information for me to diagnose the problem.   If 
it were my problem, I'd print out some assessment of the results of 
every step you've described below.  This should expose the step in which 
things disappear.

	  Hope this helps.
	  Spencer Graves
p.s.  I almost never attach a data.frame.  It can be done, but I just 
find the mechanisms too subtle for my brain.  I use "with" routinely, 
but I know I can't include an assignment inside "with", because "with" 
creates a frame and then discards it after it's done.  Anything I 
created inside "with" gets thereby discarded.

p.p.s.  If you'd like more help from this listserve, PLEASE do read the 
posting guide! "www.R-project.org/posting-guide.html".  People who 
follow that posting guide seem more likely to solve their own problems 
in the process of preparing a question for the listserve.  If that 
fails, they are more likely to get a useful reply quickly.  Clean, crisp 
questions sometimes generate a feeding frenzy of replies.  Questions 
that are harder to understand often get ignored.

Bob Green wrote:
> Hello ,
> 
> I am hoping for some advice. I want to run a repeated measures ANOVA. The 
> primary problem is that my attempt to remove missing values created a 
> dataset of missing values.
> 
> The data set consists of 92 rows (1 row per participant) x 186 variables.
> 
> The steps of the analysis undertaken are outlined below (#). Any assistance 
> is appreciated in relation to how to remove the missing values so the 
> analysis is run. Feedback regarding the prior steps is also welcomed .
> 
> Bob Green
> 
> 
> 
> #Step 1
> 
> study1dat <- read.csv("c:\\study1.csv",header=T)
> attach (study1dat)
> 
> outcome <- c(t1frq, t2frq,t3frq,t4frq)
> grp <- factor( rep(group, 2,length=368) )
> time <- gl(4,92,length=368)
> subject <- gl(92,1,length=368)
> data.frame(subject, grp, time, outcome)
> 
> # there are 3 missing values in $outcome
> 
> #Step 2 - create a new data frame removing missing values from variable 
> $outcome
> 
>   d2<-study1dat[!is.na(outcome),]
> 
> #the previous step generates NA values.
> 
> #Step 3 detach original data set & attach dataset without missing values
> detach(study1dat)
> 
> attach(d2)
> 
> The following object(s) are masked _by_ .GlobalEnv :         time
>          The following object(s) are masked from package:datasets 
> :         sleep
> 
> #Step 4 run analysis
> 
> library(nlme)
> anova(lme(outcome ~ grp * time, random = ~ 1 | subject))
> 
> #The data is the format below
> 
>      subject grp time outcome
> 1         1   0    1       4
> 2         2   0    1       3
> 3         3   0    1       7
> 4         4   0    1       0
> 5         5   0    1       1
> 6         6   0    1       7
> 7         7   0    1       7
> 8         8   0    1       7
> 9         9   0    1       7
> 10       10 0    1      5
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From bruno.giordano at music.mcgill.ca  Tue Jun 20 06:19:34 2006
From: bruno.giordano at music.mcgill.ca (Bruno L. Giordano)
Date: Tue, 20 Jun 2006 00:19:34 -0400
Subject: [R] Effect size in mixed models
References: <002c01c6919d$39416d80$6400a8c0@brungio> <449762E8.8000507@pdf.com>
Message-ID: <002801c69420$c5065a30$6400a8c0@brungio>

OK and thanks!! I will try to apply the approach you suggest to my data.

I found a possibly interesting reference on this topic (if my fast reading 
is error-free):

E.  Demidenko, 2004, Mixed Models: Theory and Applications (John Wiley & 
Sons), Chapter 9 (Diagnostics and influence analysis)

outlines a "perturbation-based" methodology called Infinitesimal influence 
analysis

which analyzes the stability of the solution as a function of perturbations 
in the predictors (or in the isolated observations). In this case, 
predictors whose perturbation leads to greater changes in the solution are 
assumed to be more relevant. In the same chapter the idea is extended to 
mixed models, although it focuses on the influence of clusters of 
observations on the solution, and not on the influence of random effects.

A really rough idea I had, more in line with my current mathematical 
background, is to simply examine the percent increase in the residual 
variance when one (or more) fixed/random effect(s) is(are) removed from the 
model.

With this brute approach I found, for example, that for the outcome I am 
analyzing (4 random + 4 fixed effects with interactions) the entire fixed 
portion is less relevant than any of the random effects in isolation.

    Bruno



----- Original Message ----- 
From: "Spencer Graves" <spencer.graves at pdf.com>
To: "Bruno L. Giordano" <bruno.giordano at music.mcgill.ca>
Cc: <r-help at stat.math.ethz.ch>
Sent: Monday, June 19, 2006 10:52 PM
Subject: Re: [R] Effect size in mixed models


>   You have asked a great question:  It would indeed be useful to compare 
> the relative magnitude of fixed and random effects, e.g. to prioritize 
> efforts to better understand and possibly manage processes being studied. 
> I will offer some thoughts on this, and I hope if there are errors in my 
> logic or if someone else has a better idea, we will both benefit from 
> their comments.
>
>   The ideal might be an estimate of something like a mean square for a 
> particular effect to compare with an estimated variance component.
> Such mean squares were a mandatory component of any analysis of variance 
> table prior to the (a) popularization of generalized linear models and (b) 
> availability of software that made it feasible to compute maximum 
> likelihood estimates routinely for unbalanced, mixed-effects models. 
> However, anova(lme(...)) such mean squares are for most purposes 
> unnecessary cluster in a modern anova table.
>
>   To estimate a mean square for a fixed effect, consider the following 
> log(likelihood) for a mixed-effects model:
>
>   lglk = (-0.5)*(n*log(2*pi*var.e)-log(det(W)) + 
> t(y-X%*%b)%*%W%*%(y-X%*%b)/var.e),
>
> where n = the number of observations,
>
>       b = the fixed-effect parameter variance,
>
> and the covariance matrix of the residuals, after integrating out the 
> random effects is var.e*solve(W).  In this formulation, the matrix "W" is 
> a function of the variance components.  Since they are not needed to 
> compute the desired mean squares, they are suppressed in the notation 
> here.
>
>   Then, the maximum likelihood estimate of
>
>   var.e = SSR/n,
>
> where SSR = t(y-X%*%b)%*%W%*%(y-X%*%b).
>
>   Then
>
>   mle.lglk = (-0.5)*(n*(log(2*pi*SSR/n)-1)-log(det(W))).
>
>   Now let
>
>   SSR0 = this generalized sum of squares of residuals (SSR) without effect 
> "1",
>
> and
>
>   SSR1 = this generalized SSR with this effect "1".
>
>   If I've done my math correctly, then
>
>   D = deviance = 2*log(likelihood ratio)
>     = (n*log(SSR0/SSR1)+log(det(W1)/det(W0)))
>
>   For roughly half a century, a major part of "the analysis of variance" 
> was the Pythagorean idea that the sum of squares under H0 was the sum of 
> squares under H1 plus the sum of squares for effect "1":
>
>   SSR0 = SS1 + SSR1.
>
>   Whence,
>
>   exp((D/n)-log(det(W1)/det(W0))) = 1+SS1/SSR1.
>
> Thus,
>
>   SS1 = SSR1*(exp((D/n)-log(det(W1)/det(W0)))-1).
>
>   If the difference between deg(W1) and det(W0) can be ignored, we get:
>
>   SS1 = SSR1*(exp((D/n)-1).
>
>   Now compute MS1 = SS1/df1, and compare with the variance components.
>
>   If there is a flaw in this logic, I hope someone will disabuse me of it.
>
>   If this seems too terse or convoluted to follow, please provide a 
> simple, self-contained example, as suggested in the posting guide! 
> "www.R-project.org/posting-guide.html".  You asked a theoretical question, 
> you got a theoretical answer.  If you want a concrete answer, it might 
> help to pose a more concrete question.
>
>   Hope this helps.
>   Spencer Graves
> Bruno L. Giordano wrote:
>> Hello,
>> Is there a way to compare the relative relevance of fixed and random 
>> effects in mixed models? I have in mind measures of effect size in 
>> ANOVAs, and would like to obtain similar information with mixed models.
>>
>> Are there information criteria that allow to compare the relevance of 
>> each of the effects in a mixed model to the overall fit?
>>
>> Thank you,
>>     Bruno
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>


From seanpor at acm.org  Tue Jun 20 07:30:39 2006
From: seanpor at acm.org (Sean O'Riordain)
Date: Tue, 20 Jun 2006 05:30:39 +0000
Subject: [R] how to successfully remove missing values for a repeated
	measures analysis
In-Reply-To: <449765AF.9080008@pdf.com>
References: <5.1.0.14.0.20060618184815.00bec300@pop3.brisnet.org.au>
	<449765AF.9080008@pdf.com>
Message-ID: <8ed68eed0606192230h26cd80d7sf51bd1ed8c9df3c0@mail.gmail.com>

Spencer,

A number of times I've been wrecking my head trying to solve a
problem, and when I thought I'd tried everything possible (all the
docs that I can think of, web searches etc...) I start re-reading the
posting guide (I've no idea how many times I've read it now)... and 9
times out of 10, the answer is there for me... :-)

Like yesterday... I had forgotten about the posting-guide (why Sean?
why?) and I was trying to get windows to startup with a default entry
for CRAN so I don't have to select *everytime*... which was starting
to annoy me... a lot... eventually after running out of options I
started down the posting guide route... and it turns out I've been
starting using "--vanilla", so I turned that off and hey presto... it
worked... :-)

Takeaway... now I read the posting guide earlier in the problem
solving process :-)

cheers,
Sean


On 20/06/06, Spencer Graves <spencer.graves at pdf.com> wrote:
>           I don't see enough information for me to diagnose the problem.   If
> it were my problem, I'd print out some assessment of the results of
> every step you've described below.  This should expose the step in which
> things disappear.
>
>           Hope this helps.
>           Spencer Graves
> p.s.  I almost never attach a data.frame.  It can be done, but I just
> find the mechanisms too subtle for my brain.  I use "with" routinely,
> but I know I can't include an assignment inside "with", because "with"
> creates a frame and then discards it after it's done.  Anything I
> created inside "with" gets thereby discarded.
>
> p.p.s.  If you'd like more help from this listserve, PLEASE do read the
> posting guide! "www.R-project.org/posting-guide.html".  People who
> follow that posting guide seem more likely to solve their own problems
> in the process of preparing a question for the listserve.  If that
> fails, they are more likely to get a useful reply quickly.  Clean, crisp
> questions sometimes generate a feeding frenzy of replies.  Questions
> that are harder to understand often get ignored.
>
> Bob Green wrote:
> > Hello ,
> >
> > I am hoping for some advice. I want to run a repeated measures ANOVA. The
> > primary problem is that my attempt to remove missing values created a
> > dataset of missing values.
> >
> > The data set consists of 92 rows (1 row per participant) x 186 variables.
> >
> > The steps of the analysis undertaken are outlined below (#). Any assistance
> > is appreciated in relation to how to remove the missing values so the
> > analysis is run. Feedback regarding the prior steps is also welcomed .
> >
> > Bob Green
> >
> >
> >
> > #Step 1
> >
> > study1dat <- read.csv("c:\\study1.csv",header=T)
> > attach (study1dat)
> >
> > outcome <- c(t1frq, t2frq,t3frq,t4frq)
> > grp <- factor( rep(group, 2,length=368) )
> > time <- gl(4,92,length=368)
> > subject <- gl(92,1,length=368)
> > data.frame(subject, grp, time, outcome)
> >
> > # there are 3 missing values in $outcome
> >
> > #Step 2 - create a new data frame removing missing values from variable
> > $outcome
> >
> >   d2<-study1dat[!is.na(outcome),]
> >
> > #the previous step generates NA values.
> >
> > #Step 3 detach original data set & attach dataset without missing values
> > detach(study1dat)
> >
> > attach(d2)
> >
> > The following object(s) are masked _by_ .GlobalEnv :         time
> >          The following object(s) are masked from package:datasets
> > :         sleep
> >
> > #Step 4 run analysis
> >
> > library(nlme)
> > anova(lme(outcome ~ grp * time, random = ~ 1 | subject))
> >
> > #The data is the format below
> >
> >      subject grp time outcome
> > 1         1   0    1       4
> > 2         2   0    1       3
> > 3         3   0    1       7
> > 4         4   0    1       0
> > 5         5   0    1       1
> > 6         6   0    1       7
> > 7         7   0    1       7
> > 8         8   0    1       7
> > 9         9   0    1       7
> > 10       10 0    1      5
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From jfox at mcmaster.ca  Tue Jun 20 08:08:45 2006
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 20 Jun 2006 02:08:45 -0400
Subject: [R] how to put the results of loop into a dataframe
In-Reply-To: <2fc17e30606191809m35e242e8te67ace883d0ac589@mail.gmail.com>
Message-ID: <web-129762221@cgpsrv2.cis.mcmaster.ca>

Dear Zhi Jie, Zhang,

This problem seems so straightforward that I wonder whether I
misunderstand what you want:

x <- runif(100)
y <- x + 1
z <- x + y
Data <- data.frame(x, y, z)

I hope this helps,
 John

On Tue, 20 Jun 2006 09:09:16 +0800
 "zhijie zhang" <epistat at gmail.com> wrote:
> Dear friends,
>  suppose i want to do the following caulation for 100 times, how to
> put the
> results of x , y and z into the same dataframe/dataset?
> x<-runif(1)
> y<-x+1
> z<-x+y
> 
> thanks in advance!
> -- 
> Kind Regards,
> Zhi Jie,Zhang ,PHD
> Department of Epidemiology
> School of Public Health
> Fudan University
> Tel:86-21-54237149
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/


From gregor.gorjanc at gmail.com  Tue Jun 20 08:59:23 2006
From: gregor.gorjanc at gmail.com (Gregor Gorjanc)
Date: Tue, 20 Jun 2006 08:59:23 +0200
Subject: [R] R galleries
Message-ID: <44979CCB.3020207@bfro.uni-lj.si>

Hello!

I just noticed new link on R wiki on R galleries and wanted to share
this info with YOU!

- R graphical manuals (this is awesome page as there are all help pages
of all packages on CRAN and probably even more and all graphics examples
are displayed! - more than 8000 images!)

http://bg9.imslab.co.jp/Rhelp/

This is a very nice addition to already existing R graph and movies
galleries

- R Graph Gallery
http://addictedtor.free.fr/graphiques/

- R Movies Gallery
http://addictedtor.free.fr/movies/

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From L.Alberts at student.unimaas.nl  Tue Jun 20 09:19:03 2006
From: L.Alberts at student.unimaas.nl (Alberts Laurens (Stud. SIT))
Date: Tue, 20 Jun 2006 09:19:03 +0200
Subject: [R] survest function and counting process style input
Message-ID: <918329C84699E5438220CC277CFA7353744513@um-mail0137.unimaas.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060620/f9ebd127/attachment.pl 

From philipp.pagel.lists at t-online.de  Tue Jun 20 10:24:08 2006
From: philipp.pagel.lists at t-online.de (Philipp Pagel)
Date: Tue, 20 Jun 2006 10:24:08 +0200
Subject: [R] how to put the results of loop into a dataframe
In-Reply-To: <2fc17e30606191809m35e242e8te67ace883d0ac589@mail.gmail.com>
References: <2fc17e30606191809m35e242e8te67ace883d0ac589@mail.gmail.com>
Message-ID: <20060620082408.GA3894@gsf.de>

On Tue, Jun 20, 2006 at 09:09:16AM +0800, zhijie zhang wrote:
>  suppose i want to do the following caulation for 100 times, how to put the
> results of x , y and z into the same dataframe/dataset?
> x<-runif(1)
> y<-x+1
> z<-x+y

Several possibilities:

1) Use rbind

Before loop:

d = NULL

And in the loop:

d = rbind(d, data.frame(x, y, z))


2) Build empty data frame of desired size beforehand and fill by row

E.g. for 10 rows:
d = data.frame( x=rep(0, 10), y=rep(0,10), z=rep(0,10))

And in the loop (index i):

d[i, ] = c(x, y, z)


3) Build vectors first, dataframe after the loop

x = NULL
y = NULL
z = NULL

in the loop:

x = append(x, runif(1))
...

After loop:

d = data.frame(x, y, z)

Solutions 1&3 are slower but you don't need to know the final number of
rows in advance. Solution 2 is cleaner, in my opinion, but requires that
you know the final size.

Of course, for the particular example calculation you don't need a loop
at all:

x = runif(100)
y = x+1
z = x+y
d = data.frame(x,y,z)

cu
	Philipp

-- 
Dr. Philipp Pagel                            Tel.  +49-8161-71 2131
Dept. of Genome Oriented Bioinformatics      Fax.  +49-8161-71 2186
Technical University of Munich
Science Center Weihenstephan
85350 Freising, Germany

 and

Institute for Bioinformatics / MIPS          Tel.  +49-89-3187 3675
GSF - National Research Center               Fax.  +49-89-3187 3585
      for Environment and Health
Ingolst?dter Landstrasse 1
85764 Neuherberg, Germany
http://mips.gsf.de/staff/pagel


From lindners at umich.edu  Tue Jun 20 10:42:56 2006
From: lindners at umich.edu (Stephan Lindner)
Date: Tue, 20 Jun 2006 04:42:56 -0400
Subject: [R] Create variables with common values for each group
Message-ID: <20060620084256.GD4443@umich.edu>

Dear all,

sorry, this is for sure really basic, but I searched a lot in the
internet, and just couldn't find a solution. 

The problem is to create new variables from a data frame which
contains both individual and group variables, such as mean age for an
household. My data frame:



df 

       hhid h.age
1  10010020    23
2  10010020    23
3  10010126    42
4  10010126    60
5  10010142    20
6  10010142    49
7  10010142    52
8  10010150    18
9  10010150    51
10 10010150    28


where hhid is the same number for each household, h.age the age for
each household member. 

I tried tapply, by(), and aggregate. The best I could get was:

by(df, df$hhid, function(subset) rep(mean(subset$h.age,na.rm=T),nrow(subset)))

df$hhid: 10010020
[1] 23 23
------------------------------------------------------------ 
df$hhid: 10010126
[1] 51 51
------------------------------------------------------------ 
df$hhid: 10010142
[1] 40.33333 40.33333 40.33333
------------------------------------------------------------ 
df$hhid: 10010150
[1] 32.33333 32.33333 32.33333


Now I principally only would have to stack up the mean values, and
this is where I'm stucked. The function aggregate works nice, and I
could loop then, but I was wondering whether there is a better way to
do that. 

My end result should look like this (assigning mean.age to the data frame):



       hhid h.age  mean.age
1  10010020    23     23.00
2  10010020    23     23.00
3  10010126    42     51.00
4  10010126    60     51.00
5  10010142    20     40.33
6  10010142    49     40.33
7  10010142    52     40.33
8  10010150    18     32.33
9  10010150    51     32.33
10 10010150    28     32.33



Cheers, and thanks a lot,


Stephan Lindner




-- 
-----------------------
Stephan Lindner, Dipl.Vw.
1512 Gilbert Ct., V-17
Ann Arbor, Michigan 48105
U.S.A.
Tel.: 001-734-272-2437
E-Mail: lindners at umich.edu

"The prevailing ideas of a time were always only the ideas of the
ruling class" -- Karl Marx


From gchappi at gmail.com  Tue Jun 20 10:53:21 2006
From: gchappi at gmail.com (Hans-Peter)
Date: Tue, 20 Jun 2006 10:53:21 +0200
Subject: [R] hello Excel... (native/Package/BETA)
Message-ID: <47fce0650606200153s184199dfsad431bd780bfc207@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060620/5aebf011/attachment.pl 

From john.gavin at ubs.com  Tue Jun 20 10:57:26 2006
From: john.gavin at ubs.com (john.gavin at ubs.com)
Date: Tue, 20 Jun 2006 09:57:26 +0100
Subject: [R] lattice xyplot - aligning date labels so that they align
	with the grid lines in panel.grid
In-Reply-To: <eb555e660606191157h44bed04ci46b0334b5a86e258@mail.gmail.com>
Message-ID: <182544D7A3144B42994EEA5662C54E0102D694D2@NLDNC105PEX1.ubsw.net>

Hi Deepayan,

> You will need to do it manually, e.g.:
> 
> xyplot(value ~ date, data = x,
>        panel = function(x, y, subscripts, ...) {
>            panel.grid(h = -1, v = 0, col = "grey", lwd = 1, lty = 1)
>            panel.abline(v = as.Date(c("2005/01/01", "2006/01/01")),
>                         col = "grey", lwd = 1, lty = 1)
>            panel.xyplot(x, y, ...)
>        })
> 
> For more, you can put in more locations in panel.abline, and the same
> locations as tick mark positions in scales$y$at.

Thanks for the suggestion.
That seems to be what I was looking for.

This solution now allows me to specify the number of labels.
The only cost is that this has be done manually outside 
the xyplot command but I don't mind this.

x <- data.frame(
  date = seq(as.Date("2005/01/01"), as.Date("2006/06/01"), 
  length.out = 20), value = runif(20))

numLbl <- 6 # choose number of labels
xx <- seq(from = min(x$date), to = max(x$date), length.out = numLbl)

xyplot(value ~ date, data = x,
  panel = function(x, y, subscripts, ...)
  { panel.grid(h = -1, v = 0, col = "grey", lwd = 1, lty = 1)
    panel.abline(v = xx, col = "grey", lwd = 1, lty = 1)
    panel.xyplot(x, y, ...)
  },
  scale = list(x = list(at = as.numeric(xx), 
    labels = format(xx, "%b%y"), cex = 0.75, rot = 45)) 
) 

Regards,

John.
 

> -----Original Message-----
> From: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com] 
> Sent: 19 June 2006 19:58
> To: Gavin, John
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: lattice xyplot - aligning date labels so that 
> they align with the grid lines in panel.grid
> 
> On 6/19/06, john.gavin at ubs.com <john.gavin at ubs.com> wrote:
> > Hi,
> >
> > I have a basic question about aligning date labels for the x-axis
> > in an xyplot so that they align with the grid lines
> > from the panel.grid argument.
> >
> > For example, with
> >
> > x <- data.frame(
> >   date = seq(as.Date("2005/01/01"), as.Date("2006/06/01"),
> >   length.out = 20), value = runif(20))
> > xyplot(value ~ date, data = x,
> >   panel = function(x, y, subscripts, ...)
> >   { panel.grid(h = -1, v = -1, col = "grey", lwd = 1, lty = 1)
> >     panel.xyplot(x, y, ...)
> >   })
> >
> > How can I get the labels on the x-axis to align
> > with the vertical grid lines?
> > i.e. I have 6 vertical grid lines by default (v = -1)
> > in this example so I would like 6 labels along the x-axis
> > at the same points, whereas I see only 2 (2005 and 2006).
> 
> You will need to do it manually, e.g.:
> 
> xyplot(value ~ date, data = x,
>        panel = function(x, y, subscripts, ...) {
>            panel.grid(h = -1, v = 0, col = "grey", lwd = 1, lty = 1)
>            panel.abline(v = as.Date(c("2005/01/01", "2006/01/01")),
>                         col = "grey", lwd = 1, lty = 1)
>            panel.xyplot(x, y, ...)
>        })
> 
> For more, you can put in more locations in panel.abline, and the same
> locations as tick mark positions in scales$y$at.
> 
> > As an alternative I would be happy to just specify the 
> number of labels
> > as long as panel.grid's vertical lines aligned with the labels.
> 
> Not possible currently, unlikely to be possible ever.
> 
> > > R.version.string
> > [1] "Version 2.3.1 (2006-06-01)"
> >
> > on Windows NT4.
> >
> > Regards,
> >
> > John.
> >
> > John Gavin <john.gavin at ubs.com>,
> > Commodities, FIRC,
> > UBS Investment Bank, 2nd floor,
> > 100 Liverpool St., London EC2M 2RH, UK.
> > Phone +44 (0) 207 567 4289
> > This communication is issued by UBS AG or an affiliate 
> ("UBS...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> 
> -- 
> http://www.stat.wisc.edu/~deepayan/
> 
This communication is issued by UBS AG or an affiliate ("UBS...{{dropped}}


From dimitris.rizopoulos at med.kuleuven.be  Tue Jun 20 11:03:16 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 20 Jun 2006 11:03:16 +0200
Subject: [R] Create variables with common values for each group
References: <20060620084256.GD4443@umich.edu>
Message-ID: <012401c69448$5e6c9b90$0540210a@www.domain>

you can use something like:

dat <- data.frame(hhid = rep(c(10010020, 10010126, 10010142, 
10010150), c(2, 2, 3, 3)), h.age = sample(18:50, 10, TRUE))
###########
dat$mean.age <- rep(tapply(dat$h.age, dat$hhid, mean), 
tapply(dat$h.age, dat$hhid, length))
dat


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Stephan Lindner" <lindners at umich.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, June 20, 2006 10:42 AM
Subject: [R] Create variables with common values for each group


> Dear all,
>
> sorry, this is for sure really basic, but I searched a lot in the
> internet, and just couldn't find a solution.
>
> The problem is to create new variables from a data frame which
> contains both individual and group variables, such as mean age for 
> an
> household. My data frame:
>
>
>
> df
>
>       hhid h.age
> 1  10010020    23
> 2  10010020    23
> 3  10010126    42
> 4  10010126    60
> 5  10010142    20
> 6  10010142    49
> 7  10010142    52
> 8  10010150    18
> 9  10010150    51
> 10 10010150    28
>
>
> where hhid is the same number for each household, h.age the age for
> each household member.
>
> I tried tapply, by(), and aggregate. The best I could get was:
>
> by(df, df$hhid, function(subset) 
> rep(mean(subset$h.age,na.rm=T),nrow(subset)))
>
> df$hhid: 10010020
> [1] 23 23
> ------------------------------------------------------------ 
> df$hhid: 10010126
> [1] 51 51
> ------------------------------------------------------------ 
> df$hhid: 10010142
> [1] 40.33333 40.33333 40.33333
> ------------------------------------------------------------ 
> df$hhid: 10010150
> [1] 32.33333 32.33333 32.33333
>
>
> Now I principally only would have to stack up the mean values, and
> this is where I'm stucked. The function aggregate works nice, and I
> could loop then, but I was wondering whether there is a better way 
> to
> do that.
>
> My end result should look like this (assigning mean.age to the data 
> frame):
>
>
>
>       hhid h.age  mean.age
> 1  10010020    23     23.00
> 2  10010020    23     23.00
> 3  10010126    42     51.00
> 4  10010126    60     51.00
> 5  10010142    20     40.33
> 6  10010142    49     40.33
> 7  10010142    52     40.33
> 8  10010150    18     32.33
> 9  10010150    51     32.33
> 10 10010150    28     32.33
>
>
>
> Cheers, and thanks a lot,
>
>
> Stephan Lindner
>
>
>
>
> -- 
> -----------------------
> Stephan Lindner, Dipl.Vw.
> 1512 Gilbert Ct., V-17
> Ann Arbor, Michigan 48105
> U.S.A.
> Tel.: 001-734-272-2437
> E-Mail: lindners at umich.edu
>
> "The prevailing ideas of a time were always only the ideas of the
> ruling class" -- Karl Marx
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ccleland at optonline.net  Tue Jun 20 11:02:04 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 20 Jun 2006 05:02:04 -0400
Subject: [R] Create variables with common values for each group
In-Reply-To: <20060620084256.GD4443@umich.edu>
References: <20060620084256.GD4443@umich.edu>
Message-ID: <4497B98C.5000103@optonline.net>

Stephan Lindner wrote:
> Dear all,
> 
> sorry, this is for sure really basic, but I searched a lot in the
> internet, and just couldn't find a solution. 
> 
> The problem is to create new variables from a data frame which
> contains both individual and group variables, such as mean age for an
> household. My data frame:
> 
> 
> 
> df 
> 
>        hhid h.age
> 1  10010020    23
> 2  10010020    23
> 3  10010126    42
> 4  10010126    60
> 5  10010142    20
> 6  10010142    49
> 7  10010142    52
> 8  10010150    18
> 9  10010150    51
> 10 10010150    28
> 
> 
> where hhid is the same number for each household, h.age the age for
> each household member. 
> 
> I tried tapply, by(), and aggregate. The best I could get was:
> 
> by(df, df$hhid, function(subset) rep(mean(subset$h.age,na.rm=T),nrow(subset)))
> 
> df$hhid: 10010020
> [1] 23 23
> ------------------------------------------------------------ 
> df$hhid: 10010126
> [1] 51 51
> ------------------------------------------------------------ 
> df$hhid: 10010142
> [1] 40.33333 40.33333 40.33333
> ------------------------------------------------------------ 
> df$hhid: 10010150
> [1] 32.33333 32.33333 32.33333
> 
> 
> Now I principally only would have to stack up the mean values, and
> this is where I'm stucked. The function aggregate works nice, and I
> could loop then, but I was wondering whether there is a better way to
> do that. 

   You could use aggregate() and then merge() the result with df. 
Something like this:

 > df.agg <- aggregate(df$h.age, list(hhid = df$hhid), mean)
 >
 > names(df.agg)[2] <- "mean.age"
 >
 > merge(df, df.agg)
        hhid h.age mean.age
1  10010020    23 23.00000
2  10010020    23 23.00000
3  10010126    42 51.00000
4  10010126    60 51.00000
5  10010142    20 40.33333
6  10010142    49 40.33333
7  10010142    52 40.33333
8  10010150    18 32.33333
9  10010150    51 32.33333
10 10010150    28 32.33333

> My end result should look like this (assigning mean.age to the data frame):
> 
> 
> 
>        hhid h.age  mean.age
> 1  10010020    23     23.00
> 2  10010020    23     23.00
> 3  10010126    42     51.00
> 4  10010126    60     51.00
> 5  10010142    20     40.33
> 6  10010142    49     40.33
> 7  10010142    52     40.33
> 8  10010150    18     32.33
> 9  10010150    51     32.33
> 10 10010150    28     32.33
> 
> 
> 
> Cheers, and thanks a lot,
> 
> 
> Stephan Lindner
> 
> 
> 
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From dieter.menne at menne-biomed.de  Tue Jun 20 11:03:39 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 20 Jun 2006 09:03:39 +0000 (UTC)
Subject: [R] Create variables with common values for each group
References: <20060620084256.GD4443@umich.edu>
Message-ID: <loom.20060620T110128-235@post.gmane.org>

Stephan Lindner <lindners <at> umich.edu> writes:


> The problem is to create new variables from a data frame which
> contains both individual and group variables, such as mean age for an
> household. My data frame:
> 
> df 
> 
>        hhid h.age
> 1  10010020    23
> 2  10010020    23
...
> where hhid is the same number for each household, h.age the age for
> each household member. 
> 
> I tried tapply, by(), and aggregate. The best I could get was:
> 
> by(df, df$hhid, function(subset) rep(mean(subset$h.age,na.rm=T),nrow(subset)))
> 
> df$hhid: 10010020
> [1] 23 23
> ------------------------------------------------------------ 
> df$hhid: 10010126
> [1] 51 51

try something like 

do.call("rbind",byresult)

As you did not provide a running example, the suggestion is only approximately
correct.

Dieter


From davidhughjones at gmail.com  Tue Jun 20 11:32:37 2006
From: davidhughjones at gmail.com (David Hugh-Jones)
Date: Tue, 20 Jun 2006 10:32:37 +0100
Subject: [R] inplace assignment: solution
Message-ID: <f5d848060606200232y8d86821w8218b6bbcf59e4b8@mail.gmail.com>

I worked this out over the weekend. I appreciate that using temporary
variables would be simpler but I think this makes for quite readable
code:

# in RProfile.site
inplace <- function (f, arg=1)
		eval.parent(call("<-",substitute(f)[[arg+1]], f),2)


# examples in code

inplace(foo[bar,baz] *2)
# or
inplace(paste(foo[bar,baz], 1:10))
# or
inplace(sub("blah", "bleh", foo[bar,baz]), 3)


cheers
Dave


On 16/06/06, David Hugh-Jones <davidhughjones at gmail.com> wrote:
> It's more a general point about having to write things out twice when
> you do assignments. I could also have written:
>
> data.frame[some.condition & another.condition, big.list.of.columns] <-
> data.frame[some.condition & another.condition,  big.list.of.columns] * 2 + 55
>
> or anything else. Equally, there could be any method of subsetting, or
> any expression that can be an assignment target, on the left hand
> side:
>
> data.frame[[some.complex.expression.for.columnames]]
> <-data.frame[[some.complex.expression.for.columnames]] * 333 + foo *
> 56
>
> rownames(matrix)[45:53] <- paste(rownames(matrix)[45:53], "blah")
>
>
> David
>
> On 16/06/06, Adaikalavan Ramasamy <ramasamy at cancer.org.uk> wrote:
> > I do not fully understand your question but how about :
> >
> >  inplace <- function( df, cond1, cond2, cols, suffix ){
> >
> >   w  <- which( cond1 & cond2 )
> >   df <- df[ w, cols ]
> >   paste(df, suffix)
> >   return(df)
> >  }
> >
> >
> > BTW, did you mean "colnames(df) <- paste(colnames(df), suffix)" instead
> > of "paste(df, suffix)" ?
> >
> > Regards, Adai
> >
> >
> >
> > On Fri, 2006-06-16 at 10:23 +0100, David Hugh-Jones wrote:
> > > I get tired of writing, e.g.
> > >
> > >
> > > data.frame[some.condition & another.condition, big.list.of.columns] <-
> > > paste(data.frame[some.condition & another.condition,
> > > big.list.of.columns], "foobar")
> > >
> > >
> > > I would a function like:
> > >
> > > inplace(paste(data.frame[some.condition & another.condition,
> > > big.list.of.columns], "foobar"))
> > >
> > > which would take the first argument of the inner function and assign
> > > the function's result to it.
> > >
> > > Has anyone done something like this? Are there simple alternative
> > > solutions that I'm missing?
> > >
> > > Cheers
> > > David
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > >
> >
> >
>


From kristine.kleivi at vtt.fi  Tue Jun 20 11:38:01 2006
From: kristine.kleivi at vtt.fi (Kristine Kleivi)
Date: Tue, 20 Jun 2006 12:38:01 +0300
Subject: [R] Installing bioconductor
Message-ID: <001601c6944d$38eebb00$9e69bc82@ad.vtt.fi>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060620/1dfb8480/attachment.pl 

From gregd at stats.uct.ac.za  Tue Jun 20 11:41:01 2006
From: gregd at stats.uct.ac.za (Greg Distiller)
Date: Tue, 20 Jun 2006 11:41:01 +0200
Subject: [R] NLME: using the layout option with the plot command
Message-ID: <008901c6944d$a4a95030$6f179e89@UCTPCGREGD>

Hi
This is the 2nd time I am posting this question as I never got a reply the 
1st time round - apologies to anybody who might take offense at this but I 
dont know what else to do.

I am struggling to split up the plots of the grouped objects in nlme in a 
usable way. The standard plot command generates plots for each group on a 
single page. When there are many groups however this does not look so good. 
I have discovered the layout option which allows one to split up these plots 
over a certain number of pages but the problem is it very quickly scrolls 
through all the pages only leaving the final page in the viewer.

My question is how does one get to view all these pages? Or even better is 
there an option where the pages change only when prompted by the user?

Thanks

Greg


From jholtman at gmail.com  Tue Jun 20 11:48:43 2006
From: jholtman at gmail.com (jim holtman)
Date: Tue, 20 Jun 2006 05:48:43 -0400
Subject: [R] Create variables with common values for each group
In-Reply-To: <loom.20060620T110128-235@post.gmane.org>
References: <20060620084256.GD4443@umich.edu>
	<loom.20060620T110128-235@post.gmane.org>
Message-ID: <644e1f320606200248g7f2bd358gba8342ed397feff@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060620/b2891058/attachment.pl 

From p.dalgaard at biostat.ku.dk  Tue Jun 20 12:18:56 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Jun 2006 12:18:56 +0200
Subject: [R] Create variables with common values for each group
In-Reply-To: <644e1f320606200248g7f2bd358gba8342ed397feff@mail.gmail.com>
References: <20060620084256.GD4443@umich.edu>
	<loom.20060620T110128-235@post.gmane.org>
	<644e1f320606200248g7f2bd358gba8342ed397feff@mail.gmail.com>
Message-ID: <x2r71k3zan.fsf@viggo.kubism.ku.dk>

"jim holtman" <jholtman at gmail.com> writes:

> ?ave

Finally, someone remembered it!
 
> > cbind(x, ave(x$h.age, x$hhid))

>        hhid h.age ave(x$h.age, x$hhid)
> 1  10010020    23             23.00000
> 2  10010020    23             23.00000


Or, get rid of ugly colname using 

cbind(x, hhavg=ave(x$h.age, x$hhid))

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From davidhughjones at gmail.com  Tue Jun 20 12:29:44 2006
From: davidhughjones at gmail.com (David Hugh-Jones)
Date: Tue, 20 Jun 2006 11:29:44 +0100
Subject: [R] NLME: using the layout option with the plot command
In-Reply-To: <008901c6944d$a4a95030$6f179e89@UCTPCGREGD>
References: <008901c6944d$a4a95030$6f179e89@UCTPCGREGD>
Message-ID: <f5d848060606200329t453907dbwbfc982a10224fc42@mail.gmail.com>

hi greg

If you are using windows, set up a plot window and click the "Record"
option in the menu. Then run the command. Now you can scroll back
through previous pages by hitting Page Up.

Beware that if you save your workspace without clearing the history,
you may have a lot of bloat from the graphs.

David

On 20/06/06, Greg Distiller <gregd at stats.uct.ac.za> wrote:
> Hi
> This is the 2nd time I am posting this question as I never got a reply the
> 1st time round - apologies to anybody who might take offense at this but I
> dont know what else to do.
>
> I am struggling to split up the plots of the grouped objects in nlme in a
> usable way. The standard plot command generates plots for each group on a
> single page. When there are many groups however this does not look so good.
> I have discovered the layout option which allows one to split up these plots
> over a certain number of pages but the problem is it very quickly scrolls
> through all the pages only leaving the final page in the viewer.
>
> My question is how does one get to view all these pages? Or even better is
> there an option where the pages change only when prompted by the user?
>
> Thanks
>
> Greg
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From g.comte at alliance-ir.net  Tue Jun 20 12:38:11 2006
From: g.comte at alliance-ir.net (COMTE Guillaume)
Date: Tue, 20 Jun 2006 12:38:11 +0200
Subject: [R] timediff
Message-ID: <15C100200A5F4E45AF8CFB45A926EF3415E634@allexch01.alliance.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060620/6f01d5cb/attachment.pl 

From p.dalgaard at biostat.ku.dk  Tue Jun 20 12:56:07 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Jun 2006 12:56:07 +0200
Subject: [R] timediff
In-Reply-To: <15C100200A5F4E45AF8CFB45A926EF3415E634@allexch01.alliance.com>
References: <15C100200A5F4E45AF8CFB45A926EF3415E634@allexch01.alliance.com>
Message-ID: <x2mzc83xko.fsf@viggo.kubism.ku.dk>

"COMTE Guillaume" <g.comte at alliance-ir.net> writes:

> I've got 2 dates like these :
> "2006-02-08 17:12:55"
> "2006-02-08 17:15:26"
> I wish to get the middle of these two date :
> "2006-02-08 17 :14 :10"
> Is there is a function in R to do that ?

Well, 

> x1 <- as.POSIXct("2006-02-08 17:12:55")
> x2 <- as.POSIXct("2006-02-08 17:15:26")
> x2-x1
Time difference of 2.516667 mins
> (x2-x1)/2
Time difference of 1.258333 mins
> x1+60*as.numeric((x2-x1)/2)
[1] "2006-02-08 17:14:10 CET"

However, this goes wrong (although not without warning):
> x1+(x2-x1)/2
[1] "2006-02-08 17:12:56 CET"
Warning message:
Incompatible methods ("+.POSIXt", "Ops.difftime") for "+"

---for reasons that are related to this:
> as.numeric(x2)-as.numeric(x1) # seconds!
[1] 151
> as.numeric(x2-x1) # minutes
[1] 2.516667

(This suggests to me that the methodology for difftime objects is
still not quite complete. Volunteers?)

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From msubianto at gmail.com  Tue Jun 20 13:10:29 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Tue, 20 Jun 2006 13:10:29 +0200
Subject: [R] expanded dataset and random number
Message-ID: <c7c17cef0606200410w7d2a4f07h814f1db16e59baff@mail.gmail.com>

Dear all R-users,
(My apologies if this subject is wrong)
I have dataset:
mydat <- as.data.frame(
         matrix(c(14,0,1,0,1,1,
                  25,1,1,0,1,1,
                  5,0,0,1,1,0,
                  31,1,1,1,1,1,
                  10,0,0,0,0,1),
         nrow=5,ncol=6,byrow=TRUE))
dimnames(mydat)[[2]] <-c("size","A","B","C","D","E")
> mydat
  size A B C D E
1   14 0 1 0 1 1
2   25 1 1 0 1 1
3    5 0 0 1 1 0
4   31 1 1 1 1 1
5   10 0 0 0 0 1
> sum(mydat$size)
[1] 85
>

where size is number of each row that have this combination of variables.
In this dataset I have 85 tuples in expanded dataset.
I want to generate random number between 1 and 85.
Say, if the first random number is 15, so the number 15 is
1 1 0 1 1
then, if the next random number is 7, then
0 1 0 1 1

then if
random number is 79
0 0 0 0 1
random number is 46
1 1 1 1 1
random number is 3
0 1 0 1 1
random number is 28
1 1 0 1 1

So, the result random tuples (order from 6 random number):
0 1 0 1 1
0 1 0 1 1
1 1 0 1 1
1 1 0 1 1
1 1 1 1 1
0 0 0 0 1

I  would be very happy if anyone could help me.
Thank you very much in advance.
Kindly regards,  Muhammad Subianto


From Achim.Zeileis at wu-wien.ac.at  Tue Jun 20 13:13:26 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 20 Jun 2006 13:13:26 +0200
Subject: [R] timediff
In-Reply-To: <15C100200A5F4E45AF8CFB45A926EF3415E634@allexch01.alliance.com>
References: <15C100200A5F4E45AF8CFB45A926EF3415E634@allexch01.alliance.com>
Message-ID: <20060620131326.8126b54e.Achim.Zeileis@wu-wien.ac.at>

On Tue, 20 Jun 2006 12:38:11 +0200 COMTE Guillaume wrote:

>  
> 
> Hello,
> 
>  
> 
> I've got 2 dates like these :
> 
>  
> 
> "2006-02-08 17:12:55"
> 
> "2006-02-08 17:15:26"
> 
>  
> 
>  
> 
> I wish to get the middle of these two date :

There is a mean method for "POSIXct" objects:

  x <- as.POSIXct(c("2006-02-08 17:12:55", "2006-02-08 17:15:26"))
  mean(x)

Best,
Z

>  
> 
> "2006-02-08 17 :14 :10"
> 
>  
> 
> Is there is a function in R to do that ?
> 
>  
> 
> Thks all
> 
> guillaume
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


From j.van_den_hoff at fz-rossendorf.de  Tue Jun 20 13:36:34 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Tue, 20 Jun 2006 13:36:34 +0200
Subject: [R] Function hints
In-Reply-To: <20060619174736.GA10478@psych.upenn.edu>
References: <f8e6ff050606190551o24a4573ak4454473b5dbf8199@mail.gmail.com>
	<4496CD52.7060905@fz-rossendorf.de> <4496DB1D.4010904@stats.uwo.ca>
	<20060619174736.GA10478@psych.upenn.edu>
Message-ID: <4497DDC2.3050104@fz-rossendorf.de>

Jonathan Baron wrote:
> On 06/19/06 13:13, Duncan Murdoch wrote:
>>> `help.search' does not allow full text search in the manpages (I can
>>> imagine why (1000 hits...), but without such a thing google, for
>>> instance, would probably not be half as useful as it is, right?) and
>>> there is no "sorting by relevance" in the `help.search' output, I think.
>>> how this sorting could be achieved is a different question, of course.
>> You probably want RSiteSearch("keyword", restrict="functions") (or even
>> without the "restrict" part).
> 
> Yes.  The restrict part will speed things up quite a bit, if you
> want to restrict to functions.
> 
> Or, alternatively, you could use Namazu (which I use to generate
> what RSiteSearch provides) to generate an index specific to your
> own installed functions and packages.  The trick is to cd to the
> directory /usr/lib/R/library, or the equivalent, and then say
> 
> mknmz -q */html
> 
> which will pick up the html version of all the man pages
> (assuming you have generated them, and I have no idea whether
> this can be done on Windows).  To update, say
> 
> mknmz --update=. -q */html
> 
> Then make a bookmark for the Namazu search page in your browser,
> as a local file.  (I haven't given all the details.  You have to
> install Namazu and follow the instructions.)
> 
> Or, if you have a web server, you could let Google do it for
> you.  But, I warn you, Google will fill up your web logs pretty
> fast if you don't exclude it with robots.txt.  I don't let it
> search my R stuff.
> 
> I think that Macs and various Linux versions also have other
> alternative built-in search capabilities, but I haven't tried
> them.  Beagle is the new Linux search tool, but I don't know what
> it does.
> 
> Jon

thanks for theses tips. I was not aware of the  `RSiteSearch' function 
(I did know of the existence of the web sites, though) and this helps, 
but of course this is depdendent on web access (off-line labtop 
usage...) and does not know of 'local' (non-CRAN) packages (and knows of 
maybe "too many" contributed packages, which I might not want to 
consider for one reason or the other)

thanks also for the hint on `Namazu'. maybe I do as adviced to get a 
index which is aware of my local configuration and private packages. 
(under MacOS there is a very good and fast full text search engine, but 
it cannot be told to only search the R documentation, for instance, so 
one gets lots of other hits as well.)

what I really would love to see would be an improved help.search():
on r-devel I found a reference to the /concept tag in .Rd files and the 
fact that it is rarely used (again: I was not aware of this :-( ...), 
which might serve as keyword container suitable for improving 
help.search() results. what about changing the syntax here to something like
\concept {
    keyword = score,
    keyword = score
    ...
}
where score would be restricted to a small range of values (say, 1-3 or 
1-5). if package maintainer then would choose a handful of sensible 
keywords (and scores) for a package and its functions one could expect 
improved search results. this might be a naive idea, but could a 
sort-by-relevance in the help.search() output profit from this?

to make it short: I'm not happy with the output, for instance, of
help.search("fitting")               #1
vs.
help.search("linear fitting")        #2
vs.
help.search("non-linear fitting")    #3
I somehow feel that `lm' and `nls' should both be found in the first 
search and that they should be near the top of the lists when they are 
found.

but `lm' is found only in #1 (near the bottom of the list) and `nls' not 
at all (which is really bad). this is partly a problem, of course, of 
inconsistent nomenclature in the manpages but also due to the fact that 
help.search() only accepts single phrases as pattern (and maybe the 
absense of "concept" keywords including a score?)


From maciejr_copies at yahoo.com  Tue Jun 20 14:20:38 2006
From: maciejr_copies at yahoo.com (Maciej Radziejewski)
Date: Tue, 20 Jun 2006 05:20:38 -0700 (PDT)
Subject: [R] Accessing date subfields
Message-ID: <20060620122038.42422.qmail@web34506.mail.mud.yahoo.com>

Hello,

I can't figure out a "proper" way to extract the month number from a Date class variable. I can do it like this:

> today <- as.Date(Sys.time())
> as.integer (format.Date (today, "%m"))

but it must be inefficient, since I convert a date to text and back to a number. I tried:

 > months(today)

but the months() function returns the name of the month, not the month number. I need to process some time series of station data using  season-dependent criteria, so I need the months numerically.

Thanks in advance for any help,

Maciej.


From rene.ca at web.de  Tue Jun 20 14:20:50 2006
From: rene.ca at web.de (=?iso-8859-15?Q?Ren=E9_Capell?=)
Date: Tue, 20 Jun 2006 14:20:50 +0200
Subject: [R] nested for loop restriction?
Message-ID: <958796020@web.de>

Hello,

is there a restriction for the number of loops in a nested for-loop in R?
I wrote a small function to replace water gage hights by discharge values, where the outer loop walks through the levels of a gage time series and the inner loop looks up the corresponding dicharge value in a vector. 
It seems to work well, with the (however very annoying) restriction that the inner loop stops after 60 steps.
I attached the function at the end of the mail, where tpframe is a 2-column time series dataframe (date,gage hights) and pqframe is a 2-column dataframe (gage hights, discharge).
Any kind of help appreciated,  as I am no native programmer and also quite new to R also replies like "nice try, but there is a more convenient way in R: ...",

thanks in advance,
Ren?


p.zu.q <- function (tpframe, pqframe) {
	tp <- factor(tpframe[[2]])
	tq <- tp
	p <- pqframe[[1]]
	q <- pqframe[[2]]

	for (i in 1:length(levels(tp))) {
		pegel<-levels(tp)[i]

		for (j in 1:length(brugga.p)) {
			if (p[j] == pegel) 
				{levels(tq)[i] <- q[j]
			break
			}
		else next
		}
	next
	}

	tpq <- cbind(tpframe[[1]],as.data.frame(tp),as.data.frame(tq))
	names(tpq) <- c("date","P mm","Q m3/s")
	tpq
} 
_____________________________________________________________________
Der WEB.DE SmartSurfer hilft bis zu 70% Ihrer Onlinekosten zu sparen!


From epistat at gmail.com  Tue Jun 20 14:39:04 2006
From: epistat at gmail.com (zhijie zhang)
Date: Tue, 20 Jun 2006 20:39:04 +0800
Subject: [R] rescale the data into unit square?
Message-ID: <2fc17e30606200539t1163677ev12a14b58d68983e3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060620/65a845b5/attachment.pl 

From fries at fcav.unesp.br  Tue Jun 20 14:44:06 2006
From: fries at fcav.unesp.br (Luiz Alberto Fries)
Date: Tue, 20 Jun 2006 09:44:06 -0300
Subject: [R] How large a genetic analysis will you be able to do in R...
Message-ID: <6.2.3.4.2.20060620094239.02324720@pop.fcav.unesp.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060620/1fd43f01/attachment.pl 

From p.dalgaard at biostat.ku.dk  Tue Jun 20 14:48:53 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Jun 2006 14:48:53 +0200
Subject: [R] Accessing date subfields
In-Reply-To: <20060620122038.42422.qmail@web34506.mail.mud.yahoo.com>
References: <20060620122038.42422.qmail@web34506.mail.mud.yahoo.com>
Message-ID: <x2ac883scq.fsf@viggo.kubism.ku.dk>

Maciej Radziejewski <maciejr_copies at yahoo.com> writes:

> Hello,
> 
> I can't figure out a "proper" way to extract the month number from a Date class variable. I can do it like this:
> 
> > today <- as.Date(Sys.time())
> > as.integer (format.Date (today, "%m"))
> 
> but it must be inefficient, since I convert a date to text and back to a number. I tried:
> 
>  > months(today)
> 
> but the months() function returns the name of the month, not the month number. I need to process some time series of station data using  season-dependent criteria, so I need the months numerically.
> 
> Thanks in advance for any help,

Wrong class, Date (and POSIXct) objects are basically numeric (time in
days, resp. seconds, since Jan. 1, 1970):

> structure(1,class="Date")
[1] "1970-01-02"
> structure(1,class="POSIXct")
[1] "1970-01-01 01:00:01 CET"

Instead, try:


> as.POSIXlt(Sys.time())$mon
[1] 5

(NB: zero-based...)

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From dcn208 at nyu.edu  Tue Jun 20 14:49:41 2006
From: dcn208 at nyu.edu (Damion Colin Nero)
Date: Tue, 20 Jun 2006 08:49:41 -0400
Subject: [R] rcor.test keeping column names
Message-ID: <d576de094add.4497b6a5@mail.nyu.edu>

I have been using the function rcor.test in the ltm package and have
been trying to use it for p-values of pairwise correlations using the
pearson correlation .  However when I call the p-values from the output
it gives me back a number index instead of the row names (I transposed
the columns and rows to compare genes and not columns) which shows up as

column.number column.number p-value  



Is there anyway to get out an output that looks like

column.name1 correlation p-value column.name2

for each pairwise comparison or at least to get

column.name1 column.name2 p.value



Damion Nero
Plant Molecular Biology Lab
Department of Biology 
New York University


From arun.kumar.saha at gmail.com  Tue Jun 20 14:51:02 2006
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Tue, 20 Jun 2006 18:21:02 +0530
Subject: [R] GARCH
Message-ID: <d4c57560606200551x65d99d1pd1851cf45c7ca91@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060620/303c2bc2/attachment.pl 

From petr.pikal at precheza.cz  Tue Jun 20 14:56:03 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 20 Jun 2006 14:56:03 +0200
Subject: [R] nested for loop restriction?
In-Reply-To: <958796020@web.de>
Message-ID: <44980C83.13517.DB7E3F@localhost>

Hi

If you do not insist on loops you can either use merge

merge(tpframe, pqframe)

or

df.new<-data.frame(date=tpframe$date, 
discharge=pqframe$discharge[match(tpframe$gage heights, pqframe$gage 
heights)])

I hope I did not do typo :-)

HTH
Petr




On 20 Jun 2006 at 14:20, Ren? Capell wrote:

Date sent:      	Tue, 20 Jun 2006 14:20:50 +0200
From:           	Ren? Capell <rene.ca at web.de>
To:             	r-help at stat.math.ethz.ch
Organization:   	http://freemail.web.de/
Subject:        	[R] nested for loop restriction?

> Hello,
> 
> is there a restriction for the number of loops in a nested for-loop in
> R? I wrote a small function to replace water gage hights by discharge
> values, where the outer loop walks through the levels of a gage time
> series and the inner loop looks up the corresponding dicharge value in
> a vector. It seems to work well, with the (however very annoying)
> restriction that the inner loop stops after 60 steps. I attached the
> function at the end of the mail, where tpframe is a 2-column time
> series dataframe (date,gage hights) and pqframe is a 2-column
> dataframe (gage hights, discharge). Any kind of help appreciated,  as
> I am no native programmer and also quite new to R also replies like
> "nice try, but there is a more convenient way in R: ...",
> 
> thanks in advance,
> Ren?
> 
> 
> p.zu.q <- function (tpframe, pqframe) {
>  tp <- factor(tpframe[[2]])
>  tq <- tp
>  p <- pqframe[[1]]
>  q <- pqframe[[2]]
> 
>  for (i in 1:length(levels(tp))) {
>   pegel<-levels(tp)[i]
> 
>   for (j in 1:length(brugga.p)) {
>    if (p[j] == pegel) 
>     {levels(tq)[i] <- q[j]
>    break
>    }
>   else next
>   }
>  next
>  }
> 
>  tpq <- cbind(tpframe[[1]],as.data.frame(tp),as.data.frame(tq))
>  names(tpq) <- c("date","P mm","Q m3/s")
>  tpq
> } 
> _____________________________________________________________________
> Der WEB.DE SmartSurfer hilft bis zu 70% Ihrer Onlinekosten zu sparen!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Tue Jun 20 15:03:10 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 20 Jun 2006 15:03:10 +0200
Subject: [R] Accessing date subfields
In-Reply-To: <20060620122038.42422.qmail@web34506.mail.mud.yahoo.com>
Message-ID: <44980E2E.27093.E2010E@localhost>

Hi

you can also do

as.POSIXlt(today)$mon

but I do not know if it is more efficient

HTH
Petr



On 20 Jun 2006 at 5:20, Maciej Radziejewski wrote:

Date sent:      	Tue, 20 Jun 2006 05:20:38 -0700 (PDT)
From:           	Maciej Radziejewski <maciejr_copies at yahoo.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Accessing date subfields
Send reply to:  	Maciej Radziejewski <maciejr_copies at yahoo.com>
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>

> Hello,
> 
> I can't figure out a "proper" way to extract the month number from a
> Date class variable. I can do it like this:
> 
> > today <- as.Date(Sys.time())
> > as.integer (format.Date (today, "%m"))
> 
> but it must be inefficient, since I convert a date to text and back to
> a number. I tried:
> 
>  > months(today)
> 
> but the months() function returns the name of the month, not the month
> number. I need to process some time series of station data using 
> season-dependent criteria, so I need the months numerically.
> 
> Thanks in advance for any help,
> 
> Maciej.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From maechler at stat.math.ethz.ch  Tue Jun 20 15:03:24 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 20 Jun 2006 15:03:24 +0200
Subject: [R] Slight fault in error messages
In-Reply-To: <D216D825-4A9C-4B3A-9172-10A45D97E8B6@leeds.ac.uk>
References: <D216D825-4A9C-4B3A-9172-10A45D97E8B6@leeds.ac.uk>
Message-ID: <17559.61980.571409.457640@stat.math.ethz.ch>

Thank you, Yan,

>>>>> "Yan" == Yan Wong <h.y.wong at leeds.ac.uk>
>>>>>     on Tue, 13 Jun 2006 12:27:48 +0100 writes:

    Yan> Just a quick point which may be easy to correct. Whilst
    Yan> typing the wrong thing into R 2.2.1, I noticed the
    Yan> following error messages, which seem to have some stray
    Yan> quotation marks and commas in the list of available
    Yan> families. Perhaps they have been corrected in the
    Yan> latest version (sorry, I don't want to upgrade yet, but
    Yan> it should be easy to check)?

    >> glm(1 ~ 2, family=quasibinomial(link=foo))
    Yan> Error in quasibinomial(link = foo) : ?foo? link not
    Yan> available for quasibinomial family, available links are
    Yan> "logit", ", ""probit" and "cloglog"

    >> glm(1 ~ 2, family=binomial(link=foo))
    Yan> Error in binomial(link = foo) : link "foo" not
    Yan> available for binomial family, available links are
    Yan> "logit", ""probit", "cloglog", "cauchit" and "log"

    Yan> I hope this is helpful,

yes, indeed, thank you.
It will be fixed in the next versions of R.

Martin


From dimitris.rizopoulos at med.kuleuven.be  Tue Jun 20 15:26:15 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 20 Jun 2006 15:26:15 +0200
Subject: [R] rcor.test keeping column names
References: <d576de094add.4497b6a5@mail.nyu.edu>
Message-ID: <00d401c6946d$1b439d80$0540210a@www.domain>

you can try something like the following:

mat <- matrix(rnorm(1000), 100, 10, dimnames = list(NULL, 
LETTERS[1:10]))
rcts <- rcor.test(mat)
################
rcts

pvals <- as.data.frame(rcts$p.values)
pvals$corr <- rcts$cor.mat[lower.tri(rcts$cor.mat)]
pvals[1:2] <- sapply(pvals[1:2], factor, levels = 1:ncol(mat), labels 
= colnames(mat), simplify = FALSE)
pvals[c(1,2,4,3)]


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Damion Colin Nero" <dcn208 at nyu.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, June 20, 2006 2:49 PM
Subject: [R] rcor.test keeping column names


>I have been using the function rcor.test in the ltm package and have
> been trying to use it for p-values of pairwise correlations using 
> the
> pearson correlation .  However when I call the p-values from the 
> output
> it gives me back a number index instead of the row names (I 
> transposed
> the columns and rows to compare genes and not columns) which shows 
> up as
>
> column.number column.number p-value
>
>
>
> Is there anyway to get out an output that looks like
>
> column.name1 correlation p-value column.name2
>
> for each pairwise comparison or at least to get
>
> column.name1 column.name2 p.value
>
>
>
> Damion Nero
> Plant Molecular Biology Lab
> Department of Biology
> New York University
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From thpe at hhbio.wasser.tu-dresden.de  Tue Jun 20 15:23:32 2006
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Tue, 20 Jun 2006 15:23:32 +0200
Subject: [R] list of interdependent functions
Message-ID: <4497F6D4.5080505@hhbio.wasser.tu-dresden.de>

Hello,

I discussed the following problem on the great useR conference with
several people and wonder if someone of you knows a more elegant (or
more common ?) solution than the one below.

The problem:
============

I have several sets of interrelated functions which should be compared.
The functions themselves have different structure, application-specific
names (for readability) and they should be exchangeable. I want to avoid
to construct a generic for every new function, but the functions should
be aggregated together in a common data structure (e.g. list "eq" or an
S4 instance) *and* it should be able for them to "see" and call each
other with too many $ or @. These functions are used in another function
(called "solver" here) which may be used to prepare something before the
call to f2.

The example and a possible solution (which uses an environment
manipulating function "putInEnv()") is given below.

Thanks a lot

Thomas


##======================================================
## An example
##======================================================

## a small list of functions
eq <- list(
  f1 = function(x, K) K - x,
  f2 = function(x, r, K) r * x * f1(x, K)
)

## a solver fnction which calls them
solverB <- function(eq) {
  eq <- putInEnv(eq, environment()) # that's the trick
  f1(3,4) + f2(1,2,3)
 }

## and the final call (e.g. from the command line)
solverB(eq)


##======================================================
## One possible solution. Is there a better one???
##======================================================


putInEnv <- function(eq, e) {
  ## clone, very important to avoid "interferences"!!!
  eq <- as.list(unlist(eq))
  lapply(eq, "environment<-", e)
  nn <- names(eq)
  for (i in 1:length(eq)) {
    assign(nn[i], eq[[i]], envir = e)
  }
  eq
 }


From jos.kaefer at gmail.com  Tue Jun 20 15:24:11 2006
From: jos.kaefer at gmail.com (Jos Kaefer)
Date: Tue, 20 Jun 2006 15:24:11 +0200
Subject: [R] npmc package error
Message-ID: <6ae646750606200624p519f25fdo2bed1c3e7198ec5a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060620/b6bd3d06/attachment.pl 

From andy_liaw at merck.com  Tue Jun 20 15:24:04 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 20 Jun 2006 09:24:04 -0400
Subject: [R] rescale the data into unit square?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA026FFE41@usctmx1106.merck.com>

If you can quote an actual instance of where it's mentioned, perhaps that
will make it more clear.  I'd interpret that as simply rescaling all
variables to have min=0 and max=1, one variable at a time.

Andy 

From: zhijie zhang
> 
> Dear Rusers,
>  Recently, i saw the sentence "rescale the data into unit 
> square" for several times. Could anybody tell me what it 
> means,and give an example?
>  Thanks very much!
> 
> --
> Kind Regards,
> Zhi Jie,Zhang ,
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From gelman at stat.columbia.edu  Tue Jun 20 15:34:49 2006
From: gelman at stat.columbia.edu (Andrew Gelman)
Date: Tue, 20 Jun 2006 09:34:49 -0400
Subject: [R] Bayesian logistic regression?
Message-ID: <4497F979.7020108@stat.columbia.edu>

Hi all.
Are there any R functions around that do quick logistic regression with 
a Gaussian prior distribution on the coefficients?  I just want 
posterior mode, not MCMC.  (I'm using it as a step within an iterative 
imputation algorithm.)  This isn't hard to do:  each step of a glm 
iteration simply linearizes the derivative of the log-likelihood, and, 
at this point, essentially no effort is required to augment the data to 
include the prior information.  I think this can be done by going inside 
the glm.fit() function--but if somebody's already done it, that would be 
a relief!
Thanks.
Andrew

-- 
Andrew Gelman
Professor, Department of Statistics
Professor, Department of Political Science
gelman at stat.columbia.edu
www.stat.columbia.edu/~gelman

Statistics department office:
  Social Work Bldg (Amsterdam Ave at 122 St), Room 1016
  212-851-2142
Political Science department office:
  International Affairs Bldg (Amsterdam Ave at 118 St), Room 731
  212-854-7075

Mailing address:
  1255 Amsterdam Ave, Room 1016
  Columbia University
  New York, NY 10027-5904
  212-851-2142
  (fax) 212-851-2164


From mtmorgan at fhcrc.org  Tue Jun 20 15:52:05 2006
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Tue, 20 Jun 2006 06:52:05 -0700
Subject: [R] list of interdependent functions
In-Reply-To: <4497F6D4.5080505@hhbio.wasser.tu-dresden.de> (Thomas
	Petzoldt's message of "Tue, 20 Jun 2006 15:23:32 +0200")
References: <4497F6D4.5080505@hhbio.wasser.tu-dresden.de>
Message-ID: <6ph64iw3pfe.fsf@gopher1.fhcrc.org>

Here's another way:

makeSolver <- function() {
  f1 <- function(x, K) K - x
  f2 <- function(x, r, K) r * x * f1(x, K)
  function() f1(3,4) + f2(1,2,3)
}

solverB <- makeSolver()

solverB()

makeSolver (implicitly) creates an environment, installs f1 and f2
into it, and then returns a function that gets assigned to
solverB. Calling solverB invokes the function in makeSolver, so f1
and f2 are visible to it. The principle is similar to the 'bank
account' example in section 10.7 of 'An introduction to R', available
in the manuals section of cran.

A down side is that the signature of solverB is not obvious; I'm not
sure how the documentation facilities (R CMD check) cope with this.

Nice poster at useR!

Martin

Thomas Petzoldt <thpe at hhbio.wasser.tu-dresden.de> writes:

> Hello,
>
> I discussed the following problem on the great useR conference with
> several people and wonder if someone of you knows a more elegant (or
> more common ?) solution than the one below.
>
> The problem:
> ============
>
> I have several sets of interrelated functions which should be compared.
> The functions themselves have different structure, application-specific
> names (for readability) and they should be exchangeable. I want to avoid
> to construct a generic for every new function, but the functions should
> be aggregated together in a common data structure (e.g. list "eq" or an
> S4 instance) *and* it should be able for them to "see" and call each
> other with too many $ or @. These functions are used in another function
> (called "solver" here) which may be used to prepare something before the
> call to f2.
>
> The example and a possible solution (which uses an environment
> manipulating function "putInEnv()") is given below.
>
> Thanks a lot
>
> Thomas
>
>
> ##======================================================
> ## An example
> ##======================================================
>
> ## a small list of functions
> eq <- list(
>   f1 = function(x, K) K - x,
>   f2 = function(x, r, K) r * x * f1(x, K)
> )
>
> ## a solver fnction which calls them
> solverB <- function(eq) {
>   eq <- putInEnv(eq, environment()) # that's the trick
>   f1(3,4) + f2(1,2,3)
>  }
>
> ## and the final call (e.g. from the command line)
> solverB(eq)
>
>
> ##======================================================
> ## One possible solution. Is there a better one???
> ##======================================================
>
>
> putInEnv <- function(eq, e) {
>   ## clone, very important to avoid "interferences"!!!
>   eq <- as.list(unlist(eq))
>   lapply(eq, "environment<-", e)
>   nn <- names(eq)
>   for (i in 1:length(eq)) {
>     assign(nn[i], eq[[i]], envir = e)
>   }
>   eq
>  }
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From avilella at gmail.com  Tue Jun 20 16:13:02 2006
From: avilella at gmail.com (Albert Vilella)
Date: Tue, 20 Jun 2006 15:13:02 +0100
Subject: [R] write.table decimal digits
Message-ID: <1150812782.5807.15.camel@localhost>

Hi all,

I have a table with values that I rounded with:

mytable = round(mytable, digits=2)

and when I use write.table:

write.table(mytable, file = "/home/user/mytable.txt", sep = " ",
row.names=TRUE, col.names=TRUE, quote=FALSE)

the values are printed like "1" instead of "1.00" (which would make the
table well-arranged vertically).

How can I force write.table to write the values with two-digited
decimals for all the fields?

Thanks in advance,

    Albert.


From binabina at bellsouth.net  Tue Jun 20 16:33:16 2006
From: binabina at bellsouth.net (zubin)
Date: Tue, 20 Jun 2006 10:33:16 -0400
Subject: [R] R Training in the USA
In-Reply-To: <446D1C5F.9060002@bellsouth.net>
References: <446D1C5F.9060002@bellsouth.net>
Message-ID: <4498072C.5010600@bellsouth.net>

Hello - need advice on a company or individual who will offer custom 
training in R in Atlanta Georgia.  Specifically need training on data 
manipulations (equivalent to SAS data steps in R) and R Graphics.  Does 
anyone know folks who train on R or any Firms?  Google searches are 
coming up empty.


From jmacdon at med.umich.edu  Tue Jun 20 17:00:03 2006
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Tue, 20 Jun 2006 11:00:03 -0400
Subject: [R] R Training in the USA
In-Reply-To: <4498072C.5010600@bellsouth.net>
References: <446D1C5F.9060002@bellsouth.net> <4498072C.5010600@bellsouth.net>
Message-ID: <44980D73.3070207@med.umich.edu>

These guys advertize R workshops on the list fairly regularly:

http://www.xlsolutions-corp.com/

Best,

Jim

zubin wrote:
> Hello - need advice on a company or individual who will offer custom 
> training in R in Atlanta Georgia.  Specifically need training on data 
> manipulations (equivalent to SAS data steps in R) and R Graphics.  Does 
> anyone know folks who train on R or any Firms?  Google searches are 
> coming up empty.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
James W. MacDonald, M.S.
Biostatistician
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623


**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.


From thpe at hhbio.wasser.tu-dresden.de  Tue Jun 20 17:04:13 2006
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Tue, 20 Jun 2006 17:04:13 +0200
Subject: [R] list of interdependent functions
In-Reply-To: <6ph64iw3pfe.fsf@gopher1.fhcrc.org>
References: <4497F6D4.5080505@hhbio.wasser.tu-dresden.de>
	<6ph64iw3pfe.fsf@gopher1.fhcrc.org>
Message-ID: <44980E6D.8060700@hhbio.wasser.tu-dresden.de>

Martin Morgan wrote:
> Here's another way:
> 
> makeSolver <- function() {
>   f1 <- function(x, K) K - x
>   f2 <- function(x, r, K) r * x * f1(x, K)
>   function() f1(3,4) + f2(1,2,3)
> }
> 
> solverB <- makeSolver()
> 
> solverB()
> 
> makeSolver (implicitly) creates an environment, installs f1 and f2
> into it, and then returns a function that gets assigned to
> solverB. Calling solverB invokes the function in makeSolver, so f1
> and f2 are visible to it. The principle is similar to the 'bank
> account' example in section 10.7 of 'An introduction to R', available
> in the manuals section of cran.

Thank you, and of course, I know this section, but's in fact a little
bit different and uses a list of functions inside a function.

Your solution is obvious at a first look but more tricky in the details.
I wonder if it is possible to access the functions directly, e.g. to
change the body of f1 from "K-x" to, say "1" (switch it off) or to
introduce new functions afterwards (in a straigtforward way avoiding
things like as.list(body) ...

> A down side is that the signature of solverB is not obvious; I'm not
> sure how the documentation facilities (R CMD check) cope with this.

I don't know either and it may seem a little bit ghostly (to some users
;-) that one "copies" R function makeSolver to solverB but in reality
gets a completely different function solverB that in fact uses
*delegation* to access functions f1 and f2 stored in another object.

> Nice poster at useR!

Thanks!

> Martin

Thank you again.

Thomas


From JBDavis at txfb-ins.com  Tue Jun 20 17:09:31 2006
From: JBDavis at txfb-ins.com (Davis, Jacob B. )
Date: Tue, 20 Jun 2006 10:09:31 -0500
Subject: [R] $
Message-ID: <CC7973C6B927754794A205FEE463F7810A932F@TXS9312221.txfb-ins.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060620/9b77b64c/attachment.pl 

From scottlrollins at yahoo.com  Tue Jun 20 17:12:36 2006
From: scottlrollins at yahoo.com (Scott Rollins)
Date: Tue, 20 Jun 2006 08:12:36 -0700 (PDT)
Subject: [R] multivariate splits
In-Reply-To: <mailman.9.1150797603.3451.r-help@stat.math.ethz.ch>
Message-ID: <20060620151236.33106.qmail@web39206.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060620/830ca3e6/attachment.pl 

From JBDavis at txfb-ins.com  Tue Jun 20 17:18:36 2006
From: JBDavis at txfb-ins.com (Davis, Jacob B. )
Date: Tue, 20 Jun 2006 10:18:36 -0500
Subject: [R] glm beta hypothesis testing
Message-ID: <CC7973C6B927754794A205FEE463F7810A9330@TXS9312221.txfb-ins.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060620/ca786567/attachment.pl 

From sfalcon at fhcrc.org  Tue Jun 20 17:16:45 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 20 Jun 2006 17:16:45 +0200
Subject: [R] Installing bioconductor
In-Reply-To: <001601c6944d$38eebb00$9e69bc82@ad.vtt.fi> (Kristine Kleivi's
	message of "Tue, 20 Jun 2006 12:38:01 +0300")
References: <001601c6944d$38eebb00$9e69bc82@ad.vtt.fi>
Message-ID: <m28xnrj1r6.fsf@fhcrc.org>

Hi Kristine,

"Kristine Kleivi" <kristine.kleivi at vtt.fi> writes:
> I been trying to install bioconducter into R using the script on the
> bioconductor home page. However, I get this error message: >
> source("http://www.bioconductor.org/biocLite.R") Error in file(file,
> "r", encoding = encoding) : unable to open connection In addition:
> Warning message: unable to connect to 'www.bioconductor.org' on port
> 80.

Bioconductor has its own mailing lists and issues with BioC
installation, etc are best directed there.
(http://www.bioconductor.org/docs/mailList.html)

> It is maybe due to security systems at my computer that I cannot go
> through this port? Does anyone know how I can change the port in R, so
> I can install the pakages from the bioconductor home page through
> another port?

Most likely, your access to the internet is via a web proxy.  Try
reading the help page for download.packages, there are details there
on how to configure R to connect to the internet via a web proxy.

+ seth


From h.wickham at gmail.com  Tue Jun 20 15:04:17 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 20 Jun 2006 15:04:17 +0200
Subject: [R] Function hints
In-Reply-To: <4497DDC2.3050104@fz-rossendorf.de>
References: <f8e6ff050606190551o24a4573ak4454473b5dbf8199@mail.gmail.com>
	<4496CD52.7060905@fz-rossendorf.de> <4496DB1D.4010904@stats.uwo.ca>
	<20060619174736.GA10478@psych.upenn.edu>
	<4497DDC2.3050104@fz-rossendorf.de>
Message-ID: <f8e6ff050606200604h4dd4575cya370585e1812705c@mail.gmail.com>

> what I really would love to see would be an improved help.search():
> on r-devel I found a reference to the /concept tag in .Rd files and the
> fact that it is rarely used (again: I was not aware of this :-( ...),
> which might serve as keyword container suitable for improving
> help.search() results. what about changing the syntax here to something like
> \concept {
>     keyword = score,
>     keyword = score
>     ...
> }
> where score would be restricted to a small range of values (say, 1-3 or
> 1-5). if package maintainer then would choose a handful of sensible
> keywords (and scores) for a package and its functions one could expect
> improved search results. this might be a naive idea, but could a
> sort-by-relevance in the help.search() output profit from this?

This is not something I think you can solve automatically.  Good
keywording requries a lot of effort, and needs to be consistent to be
useful.  The only way to achieve consistency is to have only person
keywording (difficult/expensive), or exhaustively document the process
of keywording and then require all package authors to read and use
(impossible).

Hadley


From j.van_den_hoff at fz-rossendorf.de  Tue Jun 20 16:32:18 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Tue, 20 Jun 2006 16:32:18 +0200
Subject: [R] Function hints
In-Reply-To: <f8e6ff050606200604h4dd4575cya370585e1812705c@mail.gmail.com>
References: <f8e6ff050606190551o24a4573ak4454473b5dbf8199@mail.gmail.com>	
	<4496CD52.7060905@fz-rossendorf.de>
	<4496DB1D.4010904@stats.uwo.ca>	
	<20060619174736.GA10478@psych.upenn.edu>	
	<4497DDC2.3050104@fz-rossendorf.de>
	<f8e6ff050606200604h4dd4575cya370585e1812705c@mail.gmail.com>
Message-ID: <449806F2.9000005@fz-rossendorf.de>

hadley wickham wrote:
>> what I really would love to see would be an improved help.search():
>> on r-devel I found a reference to the /concept tag in .Rd files and the
>> fact that it is rarely used (again: I was not aware of this :-( ...),
>> which might serve as keyword container suitable for improving
>> help.search() results. what about changing the syntax here to 
>> something like
>> \concept {
>>     keyword = score,
>>     keyword = score
>>     ...
>> }
>> where score would be restricted to a small range of values (say, 1-3 or
>> 1-5). if package maintainer then would choose a handful of sensible
>> keywords (and scores) for a package and its functions one could expect
>> improved search results. this might be a naive idea, but could a
>> sort-by-relevance in the help.search() output profit from this?
> 
> This is not something I think you can solve automatically.  Good
> keywording requries a lot of effort, and needs to be consistent to be
> useful.  The only way to achieve consistency is to have only person
I was thinking of manual keywording (by the package authors, nobody 
else!) as a means to give the search engine (help.search()) reasonable 
information including a (subjective) relevance score for each keyword.
of course, the problem is the same as with every (especially permuted) 
index: to find the best compromise betweeen indexing next to nothing and 
indexing everything (the best probably meaning to index comprehensively 
but not excessively with reasonable index terms) in the documents at hand.
sure, consistency could not be enforced but it's not consistent right 
now, simply because the real \keyword tag is far to restrictive for 
indexing purposes(only a handful of predefined allowed keywords) and 
otherwise only the name/alias and title in the Rd files seem to be 
searched (and here the author must be really aware that these fields are 
at the moment the ones which should be forced to contain the relevant 
'keywords' if the function is to be found by help.search -- this imposes 
sometimes rather artificial constraints on the wording, especially if 
you try to include some general keyword in the title of a very 
specialized function).

looking at the example I gave
(help.search("fitting") etc.) it's quite clear that `nls' simply is not 
found because 'fitting' does not occur in the title, but I trust, if 
asked to provide, say, three keywords, one of them would contain "fit" 
or "fitting". I mean, every scientific journal asks you to do just this: 
provide some free-text keywords, which you think to be relevant for the 
paper. there are no restrictions/directives, usually, but the purpose 
(to categorize the paper a bit) is served quite well.

and maybe the \concept tag really is meant for something different, I'm 
not sure. what I have in mind really is similar to providing index terms 
(plus scores to guide `help.search' in sorting). to stay with the `nls' 
example:
\concept {
    non-linear fitting = 4
    non-linear least-squares = 5
    non-linear models = 3
    parameter estimimation = 2
    gauss-newton = 1
}
would probably achieve that `nls' usually is correctly found (if this 
syntax were allowed). apart from the scores (which would be nice, I 
think) my main point is that extensive use of \concept (or a new tag 
`\index', for instance, if \concept's purpose is actually different -- 
I'm not sure) should be pushed to get better hits from help.search().

I personally have decided to start using the \concept tag in its present 
form for our local .Rd files extensively to "inject" a sufficient number 
of free-text relevant keywords into help.search()


joerg
> keywording (difficult/expensive), or exhaustively document the process
> of keywording and then require all package authors to read and use
> (impossible).
> 
> Hadley


From j.van_den_hoff at fz-rossendorf.de  Tue Jun 20 18:21:09 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Tue, 20 Jun 2006 18:21:09 +0200
Subject: [R] $
In-Reply-To: <CC7973C6B927754794A205FEE463F7810A932F@TXS9312221.txfb-ins.com>
References: <CC7973C6B927754794A205FEE463F7810A932F@TXS9312221.txfb-ins.com>
Message-ID: <44982075.5050404@fz-rossendorf.de>

Davis, Jacob B. wrote:
> If object is user defined is:
> 
>  
> 
> object$df.residual
> 
>  
> 
> the same thing as
> 
>  
> 
> df.residual(object)
> 
>  
> 
> This is my first time to encounter the $ sign in R, I'm new.  I'm
> reviewing "summary.glm" and in most cases it looks as though the $ is
> used to extract some characteristic/property of the object, but I'm not
> positive.
> 
>  
> 
> Thanks
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 
>  
> 
> Jacob Davis
> 
> Actuarial Analyst
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

`$' is for list component extraction. this is really very basic and, for 
  once (I usually don't like this answer...),  looking at the very first 
pages of the very first manual, would be not so bad an idea:


http://cran.r-project.org/  --> Manuals --> An Introduction to R


From sundar.dorai-raj at pdf.com  Tue Jun 20 18:37:02 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 20 Jun 2006 11:37:02 -0500
Subject: [R] changes to terms.formula in 2.3.x
Message-ID: <4498242E.8080406@pdf.com>

Hi, all,

I just recently noticed a change in terms.formula from 2.2.1 to 2.3.1 
(possibly 2.3.0, but I didn't check). Here's the problem:

## 2.2.1
 >  update(~ x | y, z ~ .)
z ~ x | y

## 2.3.1
 > update(~ x | y, z ~ .)
z ~ (x | y)

and in the NEWS for 2.3.1

     o	terms.formula needed to add parentheses to formulae with
	terms containing '|'.  (PR#8462)

So, there must be a reason for this change. However, in the lattice 
framework I have this causes many problems because now the `|' is 
interpreted as a logical OR. Could someone suggest a workaround for me 
so that I still get the 2.2.1 behavior my code relies on. Here's a very 
simple test function (works in 2.2.1, fails in 2.3.1):

library(lattice)
foo <- function(x, data) {
   x <- update(x, y ~ .)
   xyplot(x, data)
}
z <- expand.grid(x = 1:10, g = letters[1:3])
z$y <- rnorm(nrow(z))
foo(~ x | g, z)

Thanks,

--sundar


From marcv at water.ca.gov  Tue Jun 20 18:40:14 2006
From: marcv at water.ca.gov (=?iso-8859-1?Q?Vayssi=E8res=2C_Marc?=)
Date: Tue, 20 Jun 2006 09:40:14 -0700
Subject: [R] FW:  multivariate splits
Message-ID: <30CA44F956F0E545825A119B6FE2DD0C01759F4D@sacex3.ad.water.ca.gov>

 

-----Original Message-----
From: Vayssi?res, Marc 
Sent: Tuesday, June 20, 2006 9:35 AM
To: 'r-help-bounces at stat.math.ethz.ch'
Subject: RE: [R] multivariate splits

Glen De'ath's package for R is on cran! It is called mvpart, see:
http://cran.cnr.berkeley.edu/doc/packages/mvpart.pdf

Cheers,

Marc Vayssi?res

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Scott Rollins
Sent: Tuesday, June 20, 2006 8:13 AM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] multivariate splits

Glenn De'ath published a paper in 'Ecology' several years ago and included S-Plus functions in the archives. I haven't looked at the files, so I'm not sure what modifications would be necessary for R.

De'ath, G. 2002. Multivariate regression trees: a new technique for modeling species--environment relationships. Ecology 83:1105-1117.

Archives: http://www.esapubs.org/archive/ecol/E083/017/

Scott


> Date: Mon, 19 Jun 2006 16:17:40 +0200
> From: " B?lint Cz?cz " <elatine at gmail.com>
> Subject: [R] multivariate splits
> To: r-help <r-help at stat.math.ethz.ch>
> Message-ID: <fab4bcf70606190717r45a2030dg at mail.gmail.com>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>
> Dear R users!
> 
> Does someone know about any algorithms / packages in R, that perform 
> classification / regression / decision trees using multivariate 
> splits?
> 
> I have done some research, but I found nothing. Packages "tree" and 
> "rpart" seem only to be able to do CART with univariate splits.
> 
> Thank you for your help!
> 
> B?lint
> 
> --
> Cz?cz B?lint
> PhD hallgat?
> BCE KTK Talajtan ?s V?zgazd?lkod?s Tansz?k
> 1118 Budapest, Vill?nyi ?t 29-43.

 		
---------------------------------


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ggrothendieck at gmail.com  Tue Jun 20 18:51:58 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 20 Jun 2006 12:51:58 -0400
Subject: [R] changes to terms.formula in 2.3.x
In-Reply-To: <4498242E.8080406@pdf.com>
References: <4498242E.8080406@pdf.com>
Message-ID: <971536df0606200951j35501b01ib5e8919e92c92449@mail.gmail.com>

Try this:

> fo <- ~ x | y
> fo[[3]] <- fo[[2]]
> fo[[2]] <- as.name("z")
> fo
z ~ x | y

On 6/20/06, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
> Hi, all,
>
> I just recently noticed a change in terms.formula from 2.2.1 to 2.3.1
> (possibly 2.3.0, but I didn't check). Here's the problem:
>
> ## 2.2.1
>  >  update(~ x | y, z ~ .)
> z ~ x | y
>
> ## 2.3.1
>  > update(~ x | y, z ~ .)
> z ~ (x | y)
>
> and in the NEWS for 2.3.1
>
>     o  terms.formula needed to add parentheses to formulae with
>        terms containing '|'.  (PR#8462)
>
> So, there must be a reason for this change. However, in the lattice
> framework I have this causes many problems because now the `|' is
> interpreted as a logical OR. Could someone suggest a workaround for me
> so that I still get the 2.2.1 behavior my code relies on. Here's a very
> simple test function (works in 2.2.1, fails in 2.3.1):
>
> library(lattice)
> foo <- function(x, data) {
>   x <- update(x, y ~ .)
>   xyplot(x, data)
> }
> z <- expand.grid(x = 1:10, g = letters[1:3])
> z$y <- rnorm(nrow(z))
> foo(~ x | g, z)
>
> Thanks,
>
> --sundar
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From JBDavis at txfb-ins.com  Tue Jun 20 19:03:45 2006
From: JBDavis at txfb-ins.com (Davis, Jacob B. )
Date: Tue, 20 Jun 2006 12:03:45 -0500
Subject: [R] $
Message-ID: <CC7973C6B927754794A205FEE463F7810A9331@TXS9312221.txfb-ins.com>

Thanks for humbling yourself to my level and not answering my question.

If object is user defined is:
 object$df.residual 
 the same thing as
 df.residual(object)

You know I have self taught myself: Visual Basic, PL/SQL, C++, Matlab
and I'm not a programmer.  

In these programs I help beginners all the time with the dumb easy to
find questions because the first few months of working with a new
language can be overwhelming even when the answer is staring you in the
face.

I have been in R less than a month and I only have time to spend maybe
20 minutes a day with it.  I have about five manuals sitting on my desk
that I thumb through while surfing the internet trying to figure things
out.

Asking about the $ sign is a basic question, but until I have spent at
least 400 or so hours with the program I'm not going to know how to help
myself.  I go through this every time I learn a new piece of software
and every time comments like yours come up.  For the most part I'm sure
most of the questions that come up in the list-serve can be answered
independently if enough independent digging is done.  The purpose of the
list-serve is to aid and speed up the learning process.

You know in about six months or a year I'm sure I will be up to speed
and making contributions and then I will be helping to answer the $ sign
questions so that they can be up to speed also and contribute.

In the beginning it's helpful just to know where the resources are so
one can begin to get a feel for how to help their self.

Patrick Burns showed me where S Poetry was, being a newbie I had no idea
what are where that was, now I do.



-----Original Message-----
From: Joerg van den Hoff [mailto:j.van_den_hoff at fz-rossendorf.de] 
Sent: Tuesday, June 20, 2006 11:21 AM
To: Davis, Jacob B. 
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] $

Davis, Jacob B. wrote:
> If object is user defined is:
> 
>  
> 
> object$df.residual
> 
>  
> 
> the same thing as
> 
>  
> 
> df.residual(object)
> 
>  
> 
> This is my first time to encounter the $ sign in R, I'm new.  I'm
> reviewing "summary.glm" and in most cases it looks as though the $ is
> used to extract some characteristic/property of the object, but I'm
not
> positive.
> 
>  
> 
> Thanks
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 
>  
> 
> Jacob Davis
> 
> Actuarial Analyst
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

`$' is for list component extraction. this is really very basic and, for

  once (I usually don't like this answer...),  looking at the very first

pages of the very first manual, would be not so bad an idea:


http://cran.r-project.org/  --> Manuals --> An Introduction to R


From pzs6 at CDC.GOV  Tue Jun 20 19:00:51 2006
From: pzs6 at CDC.GOV (Smith, Phil (CDC/NIP/DMD))
Date: Tue, 20 Jun 2006 13:00:51 -0400
Subject: [R] Help with dimnames()
Message-ID: <CE5FFC36D6AADB4C93A4D5D2125D8C6A0A6012@exp-clft3.cdc.gov>

Hi R people:

I'm trying to set the dimnames of a data frame called "ests" and am
having trouble!

First, I check to see if "ests" is a data.frame... 

> is.data.frame( ests )
[1] TRUE

... and it is a data frame!

Next, I try to assign dimnames to that data frame....

> dimnames( ests )[[ 1 ]] <- as.character( ests$stfips )
Error in "row.names<-.data.frame"(`*tmp*`, value = c("1001", "1003",
"1005",  : 
        missing 'row.names' are not allowed

... and I have trouble!!

Finally, I check to see if ests$stfips has the same length as the number
of rows in "ests"...

> nrow( ests ) == length( as.character( ests$stfips ) )
[1] TRUE

... and it does!!

Does anybody have a suggestion to solve this problem?

Thanks!
Phil Smith
pzs6 at cdc.gov
National Immunization Program
Centers for Disease Control and Prevention
Atlanta, GA


From Roger.Bivand at nhh.no  Tue Jun 20 19:29:34 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 20 Jun 2006 19:29:34 +0200 (CEST)
Subject: [R] Help with dimnames()
In-Reply-To: <CE5FFC36D6AADB4C93A4D5D2125D8C6A0A6012@exp-clft3.cdc.gov>
Message-ID: <Pine.LNX.4.44.0606201927180.14818-100000@reclus.nhh.no>

On Tue, 20 Jun 2006, Smith, Phil (CDC/NIP/DMD) wrote:

> Hi R people:
> 
> I'm trying to set the dimnames of a data frame called "ests" and am
> having trouble!
> 
> First, I check to see if "ests" is a data.frame... 
> 
> > is.data.frame( ests )
> [1] TRUE
> 
> ... and it is a data frame!
> 
> Next, I try to assign dimnames to that data frame....
> 
> > dimnames( ests )[[ 1 ]] <- as.character( ests$stfips )
> Error in "row.names<-.data.frame"(`*tmp*`, value = c("1001", "1003",
> "1005",  : 
>         missing 'row.names' are not allowed
> 

ests <- data.frame(x=runif(50), y=runif(50))
stfips <- c(letters[1:25], LETTERS[1:25])
is.na(stfips) <- 50
dimnames( ests )[[ 1 ]] <- as.character(stfips)

reproduces the error message, and:

which(is.na(stfips))

says which stfips is missing. If more than one is missing, the error 
message is different. 


> ... and I have trouble!!
> 
> Finally, I check to see if ests$stfips has the same length as the number
> of rows in "ests"...
> 
> > nrow( ests ) == length( as.character( ests$stfips ) )
> [1] TRUE
> 
> ... and it does!!
> 
> Does anybody have a suggestion to solve this problem?
> 
> Thanks!
> Phil Smith
> pzs6 at cdc.gov
> National Immunization Program
> Centers for Disease Control and Prevention
> Atlanta, GA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From sundar.dorai-raj at pdf.com  Tue Jun 20 19:26:36 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 20 Jun 2006 12:26:36 -0500
Subject: [R] changes to terms.formula in 2.3.x
In-Reply-To: <971536df0606200951j35501b01ib5e8919e92c92449@mail.gmail.com>
References: <4498242E.8080406@pdf.com>
	<971536df0606200951j35501b01ib5e8919e92c92449@mail.gmail.com>
Message-ID: <44982FCC.3020402@pdf.com>

Thanks, Gabor. Works like a charm.

--sundar

Gabor Grothendieck wrote:
> Try this:
> 
>> fo <- ~ x | y
>> fo[[3]] <- fo[[2]]
>> fo[[2]] <- as.name("z")
>> fo
> 
> z ~ x | y
> 
> On 6/20/06, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
> 
>> Hi, all,
>>
>> I just recently noticed a change in terms.formula from 2.2.1 to 2.3.1
>> (possibly 2.3.0, but I didn't check). Here's the problem:
>>
>> ## 2.2.1
>>  >  update(~ x | y, z ~ .)
>> z ~ x | y
>>
>> ## 2.3.1
>>  > update(~ x | y, z ~ .)
>> z ~ (x | y)
>>
>> and in the NEWS for 2.3.1
>>
>>     o  terms.formula needed to add parentheses to formulae with
>>        terms containing '|'.  (PR#8462)
>>
>> So, there must be a reason for this change. However, in the lattice
>> framework I have this causes many problems because now the `|' is
>> interpreted as a logical OR. Could someone suggest a workaround for me
>> so that I still get the 2.2.1 behavior my code relies on. Here's a very
>> simple test function (works in 2.2.1, fails in 2.3.1):
>>
>> library(lattice)
>> foo <- function(x, data) {
>>   x <- update(x, y ~ .)
>>   xyplot(x, data)
>> }
>> z <- expand.grid(x = 1:10, g = letters[1:3])
>> z$y <- rnorm(nrow(z))
>> foo(~ x | g, z)
>>
>> Thanks,
>>
>> --sundar
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>


From bolker at ufl.edu  Tue Jun 20 19:27:15 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Tue, 20 Jun 2006 17:27:15 +0000 (UTC)
Subject: [R] $
References: <CC7973C6B927754794A205FEE463F7810A9331@TXS9312221.txfb-ins.com>
Message-ID: <loom.20060620T191248-841@post.gmane.org>

Davis, Jacob B.  <JBDavis <at> txfb-ins.com> writes:

> 
> Thanks for humbling yourself to my level and not answering my question.
> 
> If object is user defined is:
>  object$df.residual 
>  the same thing as
>  df.residual(object)
> 

  df.residual() is an extractor function.

stats:::df.residual.default gives:

function (object, ...)
object$df.residual
<environment: namespace:stats>

  so yes, in general they are the same thing.
It's better to use the extractor function rather than $
if possible;
while it doesn't appear that anyone has written a
class of objects that have a different method
for extracting the residual degrees of freedom,
they could -- or someone could change the
internal representation of lm and glm objects
(unlikely though that is) to mean that 
object$df.residual was missing, or even wrong.

  Ben Bolker


From gavin.simpson at ucl.ac.uk  Tue Jun 20 19:32:38 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 20 Jun 2006 18:32:38 +0100
Subject: [R] $
In-Reply-To: <CC7973C6B927754794A205FEE463F7810A9331@TXS9312221.txfb-ins.com>
References: <CC7973C6B927754794A205FEE463F7810A9331@TXS9312221.txfb-ins.com>
Message-ID: <1150824758.2512.11.camel@graptoleberis.geog.ucl.ac.uk>

On Tue, 2006-06-20 at 12:03 -0500, Davis, Jacob B. wrote:
> Thanks for humbling yourself to my level and not answering my question.
> 
> If object is user defined is:
>  object$df.residual 
>  the same thing as
>  df.residual(object)

Hi,

In this case, yes they are the same, but don't think that if you had
object$foo that you could do foo(object) and get anything useful. Most
of the time you can't and R will throw an error.

df.residual() is an extractor function and these are generally preferred
over extracting the component of the list directly using the $ notation.
Other extractors include coef(), residuals(), fitted() etc.

"$" _is_ used to extract components from lists, and can be used on data
frames as these are just a special type of list.

Look at this example:

y <- runif(100)
x <- rnorm(100)
mod <- lm(y ~ x)
mod
mod$df.residual
df.residual(mod)

If we look at the structure of the lm object, we see many components:

str(mod)

There is a model component, which you could extract using

mod$model

but if you try

model(mod)

you'll get an error about not being able to find function "model".

Hopefully that explains things a little.

Gav


From gavin.simpson at ucl.ac.uk  Tue Jun 20 19:36:05 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 20 Jun 2006 18:36:05 +0100
Subject: [R] multivariate splits
In-Reply-To: <20060620151236.33106.qmail@web39206.mail.mud.yahoo.com>
References: <20060620151236.33106.qmail@web39206.mail.mud.yahoo.com>
Message-ID: <1150824965.2512.15.camel@graptoleberis.geog.ucl.ac.uk>

On Tue, 2006-06-20 at 08:12 -0700, Scott Rollins wrote:
> Glenn De'ath published a paper in 'Ecology' several years ago and
> included S-Plus functions in the archives. I haven't looked at the
> files, so I'm not sure what modifications would be necessary for R.
> 
> De'ath, G. 2002. Multivariate regression trees: a new technique for
> modeling species--environment relationships. Ecology 83:1105-1117.
> 
> Archives: http://www.esapubs.org/archive/ecol/E083/017/
> 
> Scott

Glenn's code is in package mvpart on CRAN.

Not sure if the OP wanted this or not, but the multivariate nature of
mvpart is in allowing multivariate responses, common in ecological data
analysis. The fitting is still done using rpart (actually a modified
version of rpart to allow for the multivariate response).

Gavin

> 
> 
> > Date: Mon, 19 Jun 2006 16:17:40 +0200
> > From: " B?lint Cz?cz " <elatine at gmail.com>
> > Subject: [R] multivariate splits
> > To: r-help <r-help at stat.math.ethz.ch>
> > Message-ID: <fab4bcf70606190717r45a2030dg at mail.gmail.com>
> > Content-Type: text/plain; charset=ISO-8859-1; format=flowed
> >
> > Dear R users!
> > 
> > Does someone know about any algorithms / packages in R, that perform
> > classification / regression / decision trees using multivariate
> > splits?
> > 
> > I have done some research, but I found nothing. Packages "tree" and
> > "rpart" seem only to be able to do CART with univariate splits.
> > 
> > Thank you for your help!
> > 
> > B?lint
> > 
> > -- 
> > Cz?cz B?lint
> > PhD hallgat?
> > BCE KTK Talajtan ?s V?zgazd?lkod?s Tansz?k
> > 1118 Budapest, Vill?nyi ?t 29-43.
> 
>  		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From nospam at akroeger.com  Tue Jun 20 19:41:40 2006
From: nospam at akroeger.com (AKA)
Date: Tue, 20 Jun 2006 13:41:40 -0400
Subject: [R] $
In-Reply-To: <CC7973C6B927754794A205FEE463F7810A9331@TXS9312221.txfb-ins.com>
Message-ID: <001001c69490$c9bdd2e0$0500a8c0@akadev.com>

 R has many samples included with the distribution just look in the library
sub folder of your distribution. In windows examine the base library
c:\Program Files\R\R-2.3.1\library\base\R-ex you might find they are
archived (rex.zip) extract them and use Tinn-R (or any other editor) to play
around with the various samples. You might want to add a load statement to
some samples as their library may not be auto loaded 'library(RODBC)' to
examine the features/capabilities. That has been a nice bit of help for me
thus far and a good idea to have included samples with an R distribution :)

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Davis, Jacob B. 
Sent: Tuesday, June 20, 2006 1:04 PM
To: Joerg van den Hoff
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] $

Thanks for humbling yourself to my level and not answering my question.

If object is user defined is:
 object$df.residual
 the same thing as
 df.residual(object)

You know I have self taught myself: Visual Basic, PL/SQL, C++, Matlab and
I'm not a programmer.  

In these programs I help beginners all the time with the dumb easy to find
questions because the first few months of working with a new language can be
overwhelming even when the answer is staring you in the face.

I have been in R less than a month and I only have time to spend maybe 20
minutes a day with it.  I have about five manuals sitting on my desk that I
thumb through while surfing the internet trying to figure things out.

Asking about the $ sign is a basic question, but until I have spent at least
400 or so hours with the program I'm not going to know how to help myself.
I go through this every time I learn a new piece of software and every time
comments like yours come up.  For the most part I'm sure most of the
questions that come up in the list-serve can be answered independently if
enough independent digging is done.  The purpose of the list-serve is to aid
and speed up the learning process.

You know in about six months or a year I'm sure I will be up to speed and
making contributions and then I will be helping to answer the $ sign
questions so that they can be up to speed also and contribute.

In the beginning it's helpful just to know where the resources are so one
can begin to get a feel for how to help their self.

Patrick Burns showed me where S Poetry was, being a newbie I had no idea
what are where that was, now I do.



-----Original Message-----
From: Joerg van den Hoff [mailto:j.van_den_hoff at fz-rossendorf.de]
Sent: Tuesday, June 20, 2006 11:21 AM
To: Davis, Jacob B. 
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] $

Davis, Jacob B. wrote:
> If object is user defined is:
> 
>  
> 
> object$df.residual
> 
>  
> 
> the same thing as
> 
>  
> 
> df.residual(object)
> 
>  
> 
> This is my first time to encounter the $ sign in R, I'm new.  I'm 
> reviewing "summary.glm" and in most cases it looks as though the $ is 
> used to extract some characteristic/property of the object, but I'm
not
> positive.
> 
>  
> 
> Thanks
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 
>  
> 
> Jacob Davis
> 
> Actuarial Analyst
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

`$' is for list component extraction. this is really very basic and, for

  once (I usually don't like this answer...),  looking at the very first

pages of the very first manual, would be not so bad an idea:


http://cran.r-project.org/  --> Manuals --> An Introduction to R

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From gchappi at gmail.com  Tue Jun 20 19:41:53 2006
From: gchappi at gmail.com (Hans-Peter)
Date: Tue, 20 Jun 2006 19:41:53 +0200
Subject: [R] hello Excel... (native/Package/BETA)
In-Reply-To: <47fce0650606200153s184199dfsad431bd780bfc207@mail.gmail.com>
References: <47fce0650606200153s184199dfsad431bd780bfc207@mail.gmail.com>
Message-ID: <47fce0650606201041j964d8d2md3b5cb3b613322e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060620/a6ecf7ee/attachment.pl 

From buddha_314 at yahoo.com  Tue Jun 20 19:54:14 2006
From: buddha_314 at yahoo.com (Brian Dolan)
Date: Tue, 20 Jun 2006 10:54:14 -0700
Subject: [R] arima fails when called from command line
Message-ID: <1150826054.22765.3.camel@localhost.localdomain>

I'm sure there is no consistent way to reproduce this, but I'm hoping
someone has some information.

I have a time series we'll call y.  The data gets updated every day, so
I run a cron job that fits and predicts from an arima(0,0,1) X (1,1,1)_7
model.

When I open R and run the script, it processes completely.  If I call
the script via a crontab entry

R --no-save --slave < myscript.R

then the fitting of the arima model fails.  I have checked to make sure
the shell is calling the correct version of R, so it should be no
different than when I am using it interactively ...right?

Thanks for your time on this,
B


From jhallman at frb.gov  Tue Jun 20 19:54:15 2006
From: jhallman at frb.gov (jhallman at frb.gov)
Date: Tue, 20 Jun 2006 13:54:15 -0400
Subject: [R]  Packaging platform-specific functions
Message-ID: <20060620175415.2BA0C53583@mail.rsma.frb.gov>

I have a few functions, such as screenWidth() and screenHeight(), which
I have been able to implement for a Unix/Linux environment, but not for
Windows. (Does anyone know how to find the screen dimensions in
Windows?) 

The Writing R Extensions manual tells me how to include
platform-specific sections in documentation, and even how to have
platform-specific help files.  But it doesn't say anything about when or
how to handle platform-specific code.  In particular, how do package
writers handle functions that make sense for one platform but not for
another?  I'd like to keep a single set of source code files and
generate the Windows and Linux packages from it.

Any suggestions?

Jeff Hallman


From bolker at ufl.edu  Tue Jun 20 19:56:10 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Tue, 20 Jun 2006 17:56:10 +0000 (UTC)
Subject: [R] glm beta hypothesis testing
References: <CC7973C6B927754794A205FEE463F7810A9330@TXS9312221.txfb-ins.com>
Message-ID: <loom.20060620T193415-39@post.gmane.org>

Davis, Jacob B.  <JBDavis <at> txfb-ins.com> writes:

> 
> In summary.glm I'm trying to get a better feel for the z output.  The
> following lines can be found in the function
> 
  [snip]

  digging through the function is good: debugging your way through
the function is sometimes even better.

examples(glm,local=TRUE)  ## run glm examples and get
   ## results left in local workspace)

ls()
debug(summary.glm)
summary(glm.D93)

  shows ...
  p is object rank (~ number of parameters)
  Qr$pivot gives the order in which the parameters
have been rearranged to solve the model,
so Qr$pivot[p1] gives the rearranged order
of the coefficients.  We need this rearranged
order because we're going to extract the
unscaled covariance matrix by solving the
inverse QR matrix, which is in the pivoted
(rearranged) order.

The null hypothesis for any particular contrast
in glm is that the parameter is 0, so the
estimates of the coefficients (object$coefficients)
*are* the distance from the null hypothesis.

   Ben Bolker


From dieter.menne at menne-biomed.de  Tue Jun 20 20:00:41 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 20 Jun 2006 18:00:41 +0000 (UTC)
Subject: [R] Create variables with common values for each group
References: <20060620084256.GD4443@umich.edu>
	<loom.20060620T110128-235@post.gmane.org>
	<644e1f320606200248g7f2bd358gba8342ed397feff@mail.gmail.com>
	<x2r71k3zan.fsf@viggo.kubism.ku.dk>
Message-ID: <loom.20060620T195837-753@post.gmane.org>

Peter Dalgaard <p.dalgaard <at> biostat.ku.dk> writes:

> > ?ave
> 
> Finally, someone remembered it!

I feel ashamed. May I suggest to add a link under "by" and/or "tapply" ?

Dieter


From andy_liaw at merck.com  Tue Jun 20 20:02:51 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 20 Jun 2006 14:02:51 -0400
Subject: [R] multivariate splits
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA026FFEFA@usctmx1106.merck.com>

From: Gavin Simpson
> 
> On Tue, 2006-06-20 at 08:12 -0700, Scott Rollins wrote:
> > Glenn De'ath published a paper in 'Ecology' several years ago and 
> > included S-Plus functions in the archives. I haven't looked at the 
> > files, so I'm not sure what modifications would be necessary for R.
> > 
> > De'ath, G. 2002. Multivariate regression trees: a new technique for 
> > modeling species--environment relationships. Ecology 83:1105-1117.
> > 
> > Archives: http://www.esapubs.org/archive/ecol/E083/017/
> > 
> > Scott
> 
> Glenn's code is in package mvpart on CRAN.
> 
> Not sure if the OP wanted this or not, but the multivariate 
> nature of mvpart is in allowing multivariate responses, 
> common in ecological data analysis. The fitting is still done 
> using rpart (actually a modified version of rpart to allow 
> for the multivariate response).
> 
> Gavin

The OP's reply to my follow-up to Torsten's message seems to indicate that
he has univariate response.  He wants something that can split on linear
combinations of predictors, as described in the CART book, I believe.  What
I thought he wanted was something that finds some optimal subset of
predictors to split each node.

I am not aware of any open source implementations of tree algorithms that
does linear combination splits, perhaps others know better.  I suppose
Torsten's double bagging (in the ipred package) sort of does that, but in an
ensemble rather than a single tree.

Andy

 
> > 
> > 
> > > Date: Mon, 19 Jun 2006 16:17:40 +0200
> > > From: " B?lint Cz?cz " <elatine at gmail.com>
> > > Subject: [R] multivariate splits
> > > To: r-help <r-help at stat.math.ethz.ch>
> > > Message-ID: <fab4bcf70606190717r45a2030dg at mail.gmail.com>
> > > Content-Type: text/plain; charset=ISO-8859-1; format=flowed
> > >
> > > Dear R users!
> > > 
> > > Does someone know about any algorithms / packages in R, 
> that perform 
> > > classification / regression / decision trees using multivariate 
> > > splits?
> > > 
> > > I have done some research, but I found nothing. Packages 
> "tree" and 
> > > "rpart" seem only to be able to do CART with univariate splits.
> > > 
> > > Thank you for your help!
> > > 
> > > B?lint
> > > 
> > > --
> > > Cz?cz B?lint
> > > PhD hallgat?
> > > BCE KTK Talajtan ?s V?zgazd?lkod?s Tansz?k
> > > 1118 Budapest, Vill?nyi ?t 29-43.
> > 
> >  		
> > ---------------------------------
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From m1jjh00 at frb.gov  Tue Jun 20 20:15:54 2006
From: m1jjh00 at frb.gov (Jeffrey J. Hallman)
Date: Tue, 20 Jun 2006 14:15:54 -0400
Subject: [R] Packaging platform-specific functions
In-Reply-To: Message from jhallman@frb.gov of "Tue, 20 Jun 2006 13:54:15 EDT."
	<20060620175415.2BA0C53583@mail.rsma.frb.gov> 
Message-ID: <20060620181554.B134853583@mail.rsma.frb.gov>

Never mind. I RFTM'ed more carefully and found that the 'R' directory
can also have a 'unix' or 'windows' subdirectory.

jhallman at frb.gov wrote:

  jh> I have a few functions, such as screenWidth() and screenHeight(), which
  jh> I have been able to implement for a Unix/Linux environment, but not for
  jh> Windows. (Does anyone know how to find the screen dimensions in
  jh> Windows?) 

  jh> The Writing R Extensions manual tells me how to include
  jh> platform-specific sections in documentation, and even how to have
  jh> platform-specific help files.  But it doesn't say anything about when or
  jh> how to handle platform-specific code.  In particular, how do package
  jh> writers handle functions that make sense for one platform but not for
  jh> another?  I'd like to keep a single set of source code files and
  jh> generate the Windows and Linux packages from it.

  jh> Any suggestions?


From goran.brostrom at gmail.com  Tue Jun 20 20:27:03 2006
From: goran.brostrom at gmail.com (=?UTF-8?Q?G=C3=B6ran_Brostr=C3=B6m?=)
Date: Tue, 20 Jun 2006 20:27:03 +0200
Subject: [R] lmer and mixed effects logistic regression
In-Reply-To: <1150726454.3200.20.camel@localhost.localdomain>
References: <1150293574.3416.11.camel@localhost.localdomain>
	<449431F1.6000906@pdf.com>
	<40e66e0b0606180458p177fa824j6a181fd52c3d1277@mail.gmail.com>
	<1150726454.3200.20.camel@localhost.localdomain>
Message-ID: <148ed8180606201127p730e699emb9e901b09a7ab039@mail.gmail.com>

On 6/19/06, Rick Bilonick <rab45+ at pitt.edu> wrote:
> On Sun, 2006-06-18 at 13:58 +0200, Douglas Bates wrote:
> > If I understand correctly Rick it trying to fit a model with random
> > effects on a binary response when there are either 1 or 2 observations
> > per group.

If you look at Rick's examples, it's worse than that; each group
contains identical observations (by design?).

May I suggest:

> glm(y ~ x, family = binomial, data = unique(example.df))

I think lmer gives a very sensible answer to this problem.

G?ran


From szdobrowski at ucdavis.edu  Tue Jun 20 21:08:05 2006
From: szdobrowski at ucdavis.edu (Solomon Dobrowski)
Date: Tue, 20 Jun 2006 12:08:05 -0700
Subject: [R] Comparing partial response curves from GAM
Message-ID: <200606201908.k5KJ81ft009150@mx1.ucdavis.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060620/c32e1d46/attachment.pl 

From markleeds at verizon.net  Tue Jun 20 21:10:35 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Tue, 20 Jun 2006 14:10:35 -0500 (CDT)
Subject: [R] arima fails when called from command line
Message-ID: <720189.1117921150830636044.JavaMail.root@vms074.mailsrvcs.net>

>From: Brian Dolan <buddha_314 at yahoo.com>
>Date: Tue Jun 20 12:54:14 CDT 2006
>To: r-help at stat.math.ethz.ch
>Subject: [R] arima fails when called from command line

hi : i had a similar problem a few weeks agowhen i was tryin g to use R CMD BATCH ( but when i sourced the copde at the r prompt everything worked fine ). eventually, i fiigured out that it had to do with a read.tale statement having a col.names argument that was too long for the script to handle. if it doesn't read in the data correctly,
this might be causing the arima failed message. that's
just a guess though. someone else may have a better idea.








>I'm sure there is no consistent way to reproduce this, but I'm hoping
>someone has some information.
>
>I have a time series we'll call y.  The data gets updated every day, so
>I run a cron job that fits and predicts from an arima(0,0,1) X (1,1,1)_7
>model.
>
>When I open R and run the script, it processes completely.  If I call
>the script via a crontab entry
>
>R --no-save --slave < myscript.R
>
>then the fitting of the arima model fails.  I have checked to make sure
>the shell is calling the correct version of R, so it should be no
>different than when I am using it interactively ...right?
>
>Thanks for your time on this,
>B
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From rsparapa at mcw.edu  Tue Jun 20 21:52:18 2006
From: rsparapa at mcw.edu (Rodney Sparapani)
Date: Tue, 20 Jun 2006 14:52:18 -0500
Subject: [R] Successfully building 2.3.1 with Sun One Studio 8 on Solaris 9
	(SPARC)
Message-ID: <449851F2.1060203@mcw.edu>


I had a hard time building 1.9.1 with the Sun One Studio
on Solaris (SPARC) almost 2 years ago.  I filed a bug report,
but I can't seem to find it right now.  For 2.3.1, I discovered
that the same problems remain.  Here are the tricks that
resolved them for me if anyone is interested.

#1. /usr/lib/cpp will give you nothing, but headaches.
Fortunately, the Solaris Software Companion CD (SSCC)
has an alternative version and alot of other goodies
that will make your life easier.  I'm assuming that you have
installed all of these tools in the default location:  /opt/sfw

#2. the Sun Performance library is also on the SSCC
which has optimized BLAS and LAPACK routines.  This is an
optional choice, so make sure you pick it.

#3. for some reason, the Foreign package will not build
initially.  So delete src/library/Recommended/foreign.tgz
and create src/library/Recommended/foreign.ts

#4. setup your environment to be as free software-friendly
as possible with /opt/sfw/bin in your PATH:
CONFIG_SHELL=/usr/xpg4/bin/sh
CC=cc
GCC=
CFLAGS=-xlibmil -dalign
CXX=CC
CXXFLAGS=-xlibmil -dalign
F77=f95
F95=f95
FFLAGS=-xlibmil -dalign
CPP=/opt/sfw/bin/cpp
CPPFLAGS=-I/opt/sfw/include
LDFLAGS=-L/opt/sfw/lib
BLAS_LIBS=-xlic_lib=sunperf

#5. configure, make and install
configure --with-blas --with-lapack
gmake
gmake install
cd src/library/Recommended
R CMD INSTALL foreign_0.8-15.tar.gz

#6. I also asked on R-help about how to install a bunch of
packages at once.  There were some interesting tips.  But, the
easiest way that I found was the command line:
for i in *.tar.gz
do
R CMD INSTALL $i
done

That did the trick for me.  Hopefully, I didn't leave anything
out.

Rodney


From asr at ufl.edu  Tue Jun 20 21:52:36 2006
From: asr at ufl.edu (Allen S. Rout)
Date: 20 Jun 2006 15:52:36 -0400
Subject: [R] Sort of data.frame yields a thing which is not a data.frame...
Message-ID: <86irmvr4e3.fsf@ufl.edu>



I've observed something I don't understand, and I was hoping someone
could point me to the right section of docs.

There's a portion in one of my analyses in which I am wont to sort a
data.frame so:

seriesS  <- seriesS[order(as.Date(row.names(seriesS),format="%m/%d/%Y")),] 

So, I've got row.names which are textual representations of dates, I'd
like to retain them as such, but order them datewise.


As long as seriesS has more than one column, this works nicely,
seriesS afterwards is a data.frame with columns similar to those it
had going in.  But if I encounter a case with only one column, the
result is _not_ a data frame, but instead an ? array? list? 

I can solve this with a kluge:

seriesS$stupid <- 0;
seriesS <- seriesS[order(as.Date(row.names(seriesS),format="%m/%d/%Y")),] 
seriesS <- seriesS[,-c(which(names(seriesS)=="stupid")) ]

but this mostly tells me I've failed to understand something about how
the process should work.


Any good references to the Chapter and Verse of the Canon of R I
should hit?



- Allen S. Rout


From perjmi at gmail.com  Tue Jun 20 22:05:10 2006
From: perjmi at gmail.com (Per Jensen)
Date: Tue, 20 Jun 2006 22:05:10 +0200
Subject: [R] Sort of data.frame yields a thing which is not a
	data.frame...
In-Reply-To: <86irmvr4e3.fsf@ufl.edu>
References: <86irmvr4e3.fsf@ufl.edu>
Message-ID: <8c0ddffa0606201305p4b0755e6j3485c6da879b221f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060620/6244a041/attachment.pl 

From markleeds at verizon.net  Tue Jun 20 22:23:14 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Tue, 20 Jun 2006 15:23:14 -0500 (CDT)
Subject: [R] strange use of sapply
Message-ID: <14861566.1143391150834995326.JavaMail.root@vms074.mailsrvcs.net>

I've tried and give up. I have a matrix of say 200 columns and 400 rows.

For each odd ( or even i suppose if i wanted to )column, 
I want to know the number of rows in which the value is greater than zero.

So, I did sapply(tempMatrix,2,function(x) sum( x > 0 ))

this almost works but i don't know how to tell
it to only do the odd columns. my guess is , like everything else
in R, this is possible. Thanks.


From perjmi at gmail.com  Tue Jun 20 22:28:11 2006
From: perjmi at gmail.com (Per Jensen)
Date: Tue, 20 Jun 2006 22:28:11 +0200
Subject: [R] expanded dataset and random number
In-Reply-To: <c7c17cef0606200410w7d2a4f07h814f1db16e59baff@mail.gmail.com>
References: <c7c17cef0606200410w7d2a4f07h814f1db16e59baff@mail.gmail.com>
Message-ID: <8c0ddffa0606201328y488ba3afh79dc76f823da12d0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060620/8fdca237/attachment.pl 

From andy_liaw at merck.com  Tue Jun 20 22:31:16 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 20 Jun 2006 16:31:16 -0400
Subject: [R] strange use of sapply
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA026FFF50@usctmx1106.merck.com>

sapply() is not the right tool.  It operates on a list, and a matrix is not
a list (at least not treated as you'd expected it to be).  It would have
sort of worked if tempMatrix were a data frame instead of a matrix.

Try something like:

colSums(tempMatrix[, seq(1, ncol(tempMatrix), by=2)] > 0)

Andy 

From: markleeds at verizon.net
> 
> I've tried and give up. I have a matrix of say 200 columns 
> and 400 rows.
> 
> For each odd ( or even i suppose if i wanted to )column, I 
> want to know the number of rows in which the value is greater 
> than zero.
> 
> So, I did sapply(tempMatrix,2,function(x) sum( x > 0 ))
> 
> this almost works but i don't know how to tell it to only do 
> the odd columns. my guess is , like everything else in R, 
> this is possible. Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From perjmi at gmail.com  Tue Jun 20 22:39:37 2006
From: perjmi at gmail.com (Per Jensen)
Date: Tue, 20 Jun 2006 22:39:37 +0200
Subject: [R] strange use of sapply
In-Reply-To: <14861566.1143391150834995326.JavaMail.root@vms074.mailsrvcs.net>
References: <14861566.1143391150834995326.JavaMail.root@vms074.mailsrvcs.net>
Message-ID: <8c0ddffa0606201339s7a3a434ajab7044de723e8be3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060620/8b7a6bce/attachment.pl 

From Greg.Snow at intermountainmail.org  Tue Jun 20 22:41:24 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 20 Jun 2006 14:41:24 -0600
Subject: [R] Plotting Upper triangle of Matrix with diagonal as the Base
Message-ID: <07E228A5BE53C24CAD490193A7381BBB45DEDD@LP-EXCHVS07.CO.IHC.COM>

Try these functions (modify to suit your needs:

 tri1 <- function(x){
	n <- dim(x)[2]
	for(i in n:1){
		for( j in 1:(n-i+1) ){
			cat(sprintf('  %5.2f',x[j,j+i-1]))
		}
		cat("\n")
	}
}


tri2 <- function(x){
	n <- dim(x)[2]
	for(i in n:1){
		cat( rep(' ', 3*(i-1)), sep='',collapse='' )
		for( j in 1:(n-i+1) ){
			cat(sprintf(' %5.2f',x[j,j+i-1]))
		}
		cat("\n")
	}
}

tmp <- matrix(1:25,5) # not symmetric, but you get the idea
tri1(tmp)
tri2(tmp)

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Cal Stats
Sent: Monday, June 19, 2006 11:11 AM
To: r help
Subject: [R] Plotting Upper triangle of Matrix with diagonal as the Base

Hi..
  
     I a have a symmetric matrix to plot . I would like to plot  only
the Upper triangle but with the diagonal as the Base of the  rectangle.
Is there an easy way to do it.
  
  Thanks.
  
  Harsh
  
 			
---------------------------------

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From alex_restrepo at hotmail.com  Tue Jun 20 22:48:03 2006
From: alex_restrepo at hotmail.com (Alex Restrepo)
Date: Tue, 20 Jun 2006 15:48:03 -0500
Subject: [R] Determine the data type of a function to an argument
Message-ID: <BAY106-F20628D60B2A6FF1F042D6598870@phx.gbl>

Hello All:

How can I determing the "types" of args passed to an R function?  For 
example, given the following:


    calculate <- function(...)
    {
        args <- list(...)
        argName     = names(args)

        if (arg1 == character)
            cat("arg1 is a character")
        else
             cat("arg1 is numeric")


        if (arg2 == character)
            cat("arg2 is a character")
        else
            cat("arg2 is a numeric")

    }
    value = calculate(arg1='222' arg2=333)

Programatically, how can I determine if arg1 is a character and arg2 is 
numeric?

Many Thanks:

Alex


From lalithaviswanath at yahoo.com  Tue Jun 20 22:57:07 2006
From: lalithaviswanath at yahoo.com (lalitha viswanath)
Date: Tue, 20 Jun 2006 13:57:07 -0700 (PDT)
Subject: [R] Query about getting a table of binned values
Message-ID: <20060620205707.38853.qmail@web34102.mail.mud.yahoo.com>

Hi
I am working with a dataset of age and class of
proteins 
#Age
0
0.03333
0.6777777

#Class
Type A
Type B
.
.
.
Type K

I wish to get a table that reads as follows
         0-0.02   0.02-0.04 0.04-0.06 ..... 0.78-0.8
Type A    15       20           5             8
Type B     8        6 ....
.
.
.
Type K     1        0           7

I would appreciate your input regarding the
appropriate functions to use for this purpose

regards
Lalitha


From jholtman at gmail.com  Tue Jun 20 23:19:12 2006
From: jholtman at gmail.com (jim holtman)
Date: Tue, 20 Jun 2006 17:19:12 -0400
Subject: [R] timediff
In-Reply-To: <x2mzc83xko.fsf@viggo.kubism.ku.dk>
References: <15C100200A5F4E45AF8CFB45A926EF3415E634@allexch01.alliance.com>
	<x2mzc83xko.fsf@viggo.kubism.ku.dk>
Message-ID: <644e1f320606201419n26836602i24fed8544907dac3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060620/cf104513/attachment.pl 

From gerifalte28 at hotmail.com  Tue Jun 20 23:41:59 2006
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Tue, 20 Jun 2006 21:41:59 +0000
Subject: [R] Query about getting a table of binned values
In-Reply-To: <20060620205707.38853.qmail@web34102.mail.mud.yahoo.com>
Message-ID: <BAY103-F150C4AFDECE3D977AAA950A6870@phx.gbl>

Dear Lalitha

Take a look at ?cut and ?table. Cut will create categories of your 
continuous "Age" variable and table will create a contingency table of the 
categorized values against "Type"  i.e.

#Creates practice data frame
dat=data.frame(Type=sample(c("TypeA","TypeB","TypeC"),100,replace=T),Age=runif(100))

#Creates table
tab=table(dat$Type, cut(dat$Age,seq(0,1,.02)))

You can use the labels argument within cut to get a more pretty output

I hope this helps

Francisco

Dr. Francisco J. Zagmutt
College of Veterinary Medicine and Biomedical Sciences
Colorado State University




>From: lalitha viswanath <lalithaviswanath at yahoo.com>
>To: r-help at stat.math.ethz.ch
>Subject: [R] Query about getting a table of binned values
>Date: Tue, 20 Jun 2006 13:57:07 -0700 (PDT)
>
>Hi
>I am working with a dataset of age and class of
>proteins
>#Age
>0
>0.03333
>0.6777777
>
>#Class
>Type A
>Type B
>.
>.
>.
>Type K
>
>I wish to get a table that reads as follows
>          0-0.02   0.02-0.04 0.04-0.06 ..... 0.78-0.8
>Type A    15       20           5             8
>Type B     8        6 ....
>.
>.
>.
>Type K     1        0           7
>
>I would appreciate your input regarding the
>appropriate functions to use for this purpose
>
>regards
>Lalitha
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html


From gerifalte28 at hotmail.com  Tue Jun 20 23:46:06 2006
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Tue, 20 Jun 2006 21:46:06 +0000
Subject: [R] Determine the data type of a function to an argument
In-Reply-To: <BAY106-F20628D60B2A6FF1F042D6598870@phx.gbl>
Message-ID: <BAY103-F42F0E61FEFB00ED5DFD38A6870@phx.gbl>

Look at ?class and perhaps is?.  i.e.

x=c("1","2","3")
class(x)
[1] "character"

x=c(1,2,3)
class(x)
[1] "numeric"

I hope this helps

Francisco

Dr. Francisco J. Zagmutt
College of Veterinary Medicine and Biomedical Sciences
Colorado State University




>From: "Alex Restrepo" <alex_restrepo at hotmail.com>
>To: r-help at stat.math.ethz.ch
>Subject: [R] Determine the data type of a function to an argument
>Date: Tue, 20 Jun 2006 15:48:03 -0500
>
>Hello All:
>
>How can I determing the "types" of args passed to an R function?  For
>example, given the following:
>
>
>     calculate <- function(...)
>     {
>         args <- list(...)
>         argName     = names(args)
>
>         if (arg1 == character)
>             cat("arg1 is a character")
>         else
>              cat("arg1 is numeric")
>
>
>         if (arg2 == character)
>             cat("arg2 is a character")
>         else
>             cat("arg2 is a numeric")
>
>     }
>     value = calculate(arg1='222' arg2=333)
>
>Programatically, how can I determine if arg1 is a character and arg2 is
>numeric?
>
>Many Thanks:
>
>Alex
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html


From markleeds at verizon.net  Tue Jun 20 23:59:59 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Tue, 20 Jun 2006 16:59:59 -0500 (CDT)
Subject: [R] weird application of apply again
Message-ID: <3762405.1917641150840802085.JavaMail.root@vms061.mailsrvcs.net>


uugh : i promise that this will be my last question of the day.
i hate to constantly bother this group but it takes me time to get familar with all of these functions, tricks and and manipulations.
i appreciate everyone's patience. i used too splus a lot
but i've gotten rusty.

i have a matrix of say 200 rows and 600 columns.

i have a function "getprofit" that takes two series and returns a number.

getprofit<-function(series1, series2) {
do some stufff 
return(somenumber)

is there a way to do something clever so that
i call the function, getprofit, on the first two columns 
of the matrix ( where the first column is series1 and the second column is series2 ), then the next two columns of the matrix,
then the next two columns of the matrix and so on and so forth return a 300 by 1 ( oir 1 by 300, it doesn't matter ) vector of "somenumbers".

i ( maybe stupidly ) put the data in a matrix but
now i am thinking that wasn't a good thing to do ?
i can change it to some thing else if i have to.

                                thanks


From gerald.jansen at newpage.ca  Wed Jun 21 00:24:09 2006
From: gerald.jansen at newpage.ca (Gerald Jansen)
Date: Tue, 20 Jun 2006 18:24:09 -0400
Subject: [R] multi-dimension array of raw
Message-ID: <20060620182409.2f2902a4@pav.localdomain>

I would like to store and manipulate large sets of marker genotypes 
compactly using "raw" data arrays. This works fine for vectors or
matrices, but I run into the error shown in the example below as soon
as I try to use 3 dimensional arrays (eg. animal x marker x allele).

> a <- array(as.raw(1:6),c(2,3))
> a
     [,1] [,2] [,3]
[1,]   01   03   05
[2,]   02   04   06
> a[1,] <- raw(3)
> a
     [,1] [,2] [,3]
[1,]   00   00   00
[2,]   02   04   06
> b <- array(as.raw(1:6),c(1,2,3))
> b[1,,]
     [,1] [,2] [,3]
[1,]   01   03   05
[2,]   02   04   06
> b[1,1,] <- raw(3)
Error: incompatible types (from raw to raw) in array subset assignment

I can work around this with computed indices, but I wonder if this is
expected behaviour. 

Gerald Jansen


From jholtman at gmail.com  Wed Jun 21 02:52:08 2006
From: jholtman at gmail.com (jim holtman)
Date: Tue, 20 Jun 2006 20:52:08 -0400
Subject: [R] weird application of apply again
In-Reply-To: <3762405.1917641150840802085.JavaMail.root@vms061.mailsrvcs.net>
References: <3762405.1917641150840802085.JavaMail.root@vms061.mailsrvcs.net>
Message-ID: <644e1f320606201752l5791dad3v50bed6aab779f9f4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060620/07b794c4/attachment.pl 

From murdoch at stats.uwo.ca  Wed Jun 21 03:00:37 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 20 Jun 2006 21:00:37 -0400
Subject: [R] multi-dimension array of raw
In-Reply-To: <20060620182409.2f2902a4@pav.localdomain>
References: <20060620182409.2f2902a4@pav.localdomain>
Message-ID: <44989A35.3020405@stats.uwo.ca>

On 6/20/2006 6:24 PM, Gerald Jansen wrote:
> I would like to store and manipulate large sets of marker genotypes 
> compactly using "raw" data arrays. This works fine for vectors or
> matrices, but I run into the error shown in the example below as soon
> as I try to use 3 dimensional arrays (eg. animal x marker x allele).
> 
>> a <- array(as.raw(1:6),c(2,3))
>> a
>      [,1] [,2] [,3]
> [1,]   01   03   05
> [2,]   02   04   06
>> a[1,] <- raw(3)
>> a
>      [,1] [,2] [,3]
> [1,]   00   00   00
> [2,]   02   04   06
>> b <- array(as.raw(1:6),c(1,2,3))
>> b[1,,]
>      [,1] [,2] [,3]
> [1,]   01   03   05
> [2,]   02   04   06
>> b[1,1,] <- raw(3)
> Error: incompatible types (from raw to raw) in array subset assignment
> 
> I can work around this with computed indices, but I wonder if this is
> expected behaviour. 

I don't think so.  It is just an unimplemented case.

Using raw in this way is a fairly unusual thing to do, and you've come 
across a case nobody thought of implementing.

Indexing is a "primitive" function, so it's hard for you to fix this. 
It needs to be done in the internals of R (function ArrayAssign, in 
src/main/subassign.c, if you're interested).  I'll try to take a look 
and see if it looks reasonable to add.

Duncan Murdoch


From dingjun_cn at yahoo.com  Wed Jun 21 04:25:02 2006
From: dingjun_cn at yahoo.com (Jun Ding)
Date: Tue, 20 Jun 2006 19:25:02 -0700 (PDT)
Subject: [R] Extract information from the summary of 'lm'
Message-ID: <20060621022502.27311.qmail@web81010.mail.mud.yahoo.com>

Hi Everyone, 

I just don't know how to extract the information I
want from the summary of a linear regression model
fitting. 

For example, I fit the following simple linear
regression model: 

results = lm(y_var ~ x_var)

summary(results) gives me:

Call:
lm(formula = y_var ~ x_var)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.9859 -1.5849  0.4574  2.0163  4.6015 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  -1.7782     0.5948  -2.990 0.004879 ** 
x_var         2.1237     0.5073   4.187 0.000162 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.'
0.1 ' ' 1 


I can get the esitmates (i.e. -1.7782 and 2.1237) of
coefficients by calling

results$coefficients

But I don't know how to get the corresponding t values
and P values for those coefficients. I am running a
lot of regression models and I can not run 'summary'
every time to get the t and P values for each model.

Can anybody give me some hints? Thank you very much!

Best,

Jun

----------------------------
Jun Ding, Ph.D. student
Department of Biostatistics
University of Michigan
Ann Arbor, MI, 48105


From ronggui.huang at gmail.com  Wed Jun 21 04:43:40 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Wed, 21 Jun 2006 10:43:40 +0800
Subject: [R] Extract information from the summary of 'lm'
In-Reply-To: <20060621022502.27311.qmail@web81010.mail.mud.yahoo.com>
References: <20060621022502.27311.qmail@web81010.mail.mud.yahoo.com>
Message-ID: <38b9f0350606201943p36741bf3p8da0ecebe96e890e@mail.gmail.com>

>   ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
>      trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
>      group <- gl(2,10,20, labels=c("Ctl","Trt"))
>      weight <- c(ctl, trt)
> lm.D9 <- lm(weight ~ group)
> summary(lm.D9)$coef
            Estimate Std. Error   t value     Pr(>|t|)
(Intercept)    5.032  0.2202177 22.850117 9.547128e-15
groupTrt      -0.371  0.3114349 -1.191260 2.490232e-01

> class(summary(lm.D9)$coef)
[1] "matrix"

 > colnames(summary(lm.D9)$coef)
[1] "Estimate"   "Std. Error" "t value"    "Pr(>|t|)"

So you can get t and p-value  by
 >summary(lm.D9)$coef[,"t value"]
> summary(lm.D9)$coef[,"Pr(>|t|)"]

If you want to get the result as matrix,just use the drop=F argument.
> summary(lm.D9)$coef[,"Pr(>|t|)",drop=F]
                Pr(>|t|)
(Intercept) 9.547128e-15
groupTrt    2.490232e-01



2006/6/21, Jun Ding <dingjun_cn at yahoo.com>:
> Hi Everyone,
>
> I just don't know how to extract the information I
> want from the summary of a linear regression model
> fitting.
>
> For example, I fit the following simple linear
> regression model:
>
> results = lm(y_var ~ x_var)
>
> summary(results) gives me:
>
> Call:
> lm(formula = y_var ~ x_var)
>
> Residuals:
>     Min      1Q  Median      3Q     Max
> -5.9859 -1.5849  0.4574  2.0163  4.6015
>
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept)  -1.7782     0.5948  -2.990 0.004879 **
> x_var         2.1237     0.5073   4.187 0.000162 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.'
> 0.1 ' ' 1
>
>
> I can get the esitmates (i.e. -1.7782 and 2.1237) of
> coefficients by calling
>
> results$coefficients
I think using coef(result) is a better habit.

> But I don't know how to get the corresponding t values
> and P values for those coefficients. I am running a
> lot of regression models and I can not run 'summary'
> every time to get the t and P values for each model.
>
> Can anybody give me some hints? Thank you very much!
>
> Best,
>
> Jun
>
> ----------------------------
> Jun Ding, Ph.D. student
> Department of Biostatistics
> University of Michigan
> Ann Arbor, MI, 48105
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
??????
Department of Sociology
Fudan University


From epistat at gmail.com  Wed Jun 21 04:47:27 2006
From: epistat at gmail.com (zhijie zhang)
Date: Wed, 21 Jun 2006 10:47:27 +0800
Subject: [R] how to finish my task?
Message-ID: <2fc17e30606201947t7cd13af9jf77a0b2e6eae32@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060621/01e99455/attachment.pl 

From btyner at gmail.com  Wed Jun 21 05:02:49 2006
From: btyner at gmail.com (Benjamin Tyner)
Date: Tue, 20 Jun 2006 23:02:49 -0400
Subject: [R] xyplot, type="b"
Message-ID: <4498B6D9.2050004@stat.purdue.edu>

Is there any way to have xyplot produce the "points connected by lines" 
analogous to the effect of type="b" under standard graphics? I seem to 
recall doing this in xyplot at one point.

Thanks,
Ben


From ronggui.huang at gmail.com  Wed Jun 21 05:16:11 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Wed, 21 Jun 2006 11:16:11 +0800
Subject: [R] xyplot, type="b"
In-Reply-To: <4498B6D9.2050004@stat.purdue.edu>
References: <4498B6D9.2050004@stat.purdue.edu>
Message-ID: <38b9f0350606202016y1ded6022ne0e3ecde374780d3@mail.gmail.com>

> apropos("^panel")
will show you what panel function exist.It seems that panel.points
plus panel.lines are what  you want.

> dat<-data.frame(x=1:10,y=1:10,z=sample(letters[1:3],10,T))
>      xyplot(y~x | z, data = dat,pan=function(x,y,...) {panel.points(x,y,...);panel.lines(x,y,...)})





2006/6/21, Benjamin Tyner <btyner at gmail.com>:
> Is there any way to have xyplot produce the "points connected by lines"
> analogous to the effect of type="b" under standard graphics? I seem to
> recall doing this in xyplot at one point.
>
> Thanks,
> Ben
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
??????
Department of Sociology
Fudan University


From btyner at gmail.com  Wed Jun 21 05:21:01 2006
From: btyner at gmail.com (Benjamin Tyner)
Date: Tue, 20 Jun 2006 23:21:01 -0400
Subject: [R] xyplot, type="b"
In-Reply-To: <38b9f0350606202016y1ded6022ne0e3ecde374780d3@mail.gmail.com>
References: <4498B6D9.2050004@stat.purdue.edu>
	<38b9f0350606202016y1ded6022ne0e3ecde374780d3@mail.gmail.com>
Message-ID: <4498BB1D.5000306@stat.purdue.edu>

Thanks, but this proposal has the same effect as type="b" in
panel.xyplot, which as noted in the documentation is the same as
type="o". To clarify, I don't want type="o" at all; I want there to be
gaps between the lines and points. Have a look at

plot(y~x,data=dat,type="b")

to see what I mean.

Ben

ronggui wrote:

>> apropos("^panel")
>
> will show you what panel function exist.It seems that panel.points
> plus panel.lines are what you want.
>
>> dat<-data.frame(x=1:10,y=1:10,z=sample(letters[1:3],10,T))
>> xyplot(y~x | z, data = dat,pan=function(x,y,...)
>> {panel.points(x,y,...);panel.lines(x,y,...)})
>
>
>
>
>
>
> 2006/6/21, Benjamin Tyner <btyner at gmail.com>:
>
>> Is there any way to have xyplot produce the "points connected by lines"
>> analogous to the effect of type="b" under standard graphics? I seem to
>> recall doing this in xyplot at one point.
>>
>> Thanks,
>> Ben
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>
>


From murdoch at stats.uwo.ca  Wed Jun 21 05:23:17 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 20 Jun 2006 23:23:17 -0400
Subject: [R] multi-dimension array of raw
In-Reply-To: <20060620182409.2f2902a4@pav.localdomain>
References: <20060620182409.2f2902a4@pav.localdomain>
Message-ID: <4498BBA5.9040502@stats.uwo.ca>

On 6/20/2006 6:24 PM, Gerald Jansen wrote:
> I would like to store and manipulate large sets of marker genotypes 
> compactly using "raw" data arrays. This works fine for vectors or
> matrices, but I run into the error shown in the example below as soon
> as I try to use 3 dimensional arrays (eg. animal x marker x allele).
> 
>> a <- array(as.raw(1:6),c(2,3))
>> a
>      [,1] [,2] [,3]
> [1,]   01   03   05
> [2,]   02   04   06
>> a[1,] <- raw(3)
>> a
>      [,1] [,2] [,3]
> [1,]   00   00   00
> [2,]   02   04   06
>> b <- array(as.raw(1:6),c(1,2,3))
>> b[1,,]
>      [,1] [,2] [,3]
> [1,]   01   03   05
> [2,]   02   04   06
>> b[1,1,] <- raw(3)
> Error: incompatible types (from raw to raw) in array subset assignment
> 
> I can work around this with computed indices, but I wonder if this is
> expected behaviour. 

This is now fixed in r-devel and r-patched.

Duncan Murdoch


From p.murrell at auckland.ac.nz  Wed Jun 21 05:35:11 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 21 Jun 2006 15:35:11 +1200
Subject: [R] xyplot, type="b"
In-Reply-To: <4498BB1D.5000306@stat.purdue.edu>
References: <4498B6D9.2050004@stat.purdue.edu>	<38b9f0350606202016y1ded6022ne0e3ecde374780d3@mail.gmail.com>
	<4498BB1D.5000306@stat.purdue.edu>
Message-ID: <4498BE6F.8060001@stat.auckland.ac.nz>

Hi


Benjamin Tyner wrote:
> Thanks, but this proposal has the same effect as type="b" in
> panel.xyplot, which as noted in the documentation is the same as
> type="o". To clarify, I don't want type="o" at all; I want there to be
> gaps between the lines and points. Have a look at
> 
> plot(y~x,data=dat,type="b")


Is this what you mean ... ?

dat<-data.frame(x=1:10,y=1:10,z=sample(letters[1:3],10,T))
xyplot(y~x | z, data = dat,
       panel = function(x, y, ...) {
         panel.lines(x, y, ...)
         panel.points(x, y, col=trellis.par.get("background")$col,
                      cex=1.5, pch=16, ...)
         panel.points(x, y, ...)
       })

Paul


> ronggui wrote:
> 
>>> apropos("^panel")
>> will show you what panel function exist.It seems that panel.points
>> plus panel.lines are what you want.
>>
>>> dat<-data.frame(x=1:10,y=1:10,z=sample(letters[1:3],10,T))
>>> xyplot(y~x | z, data = dat,pan=function(x,y,...)
>>> {panel.points(x,y,...);panel.lines(x,y,...)})
>>
>>
>>
>>
>>
>> 2006/6/21, Benjamin Tyner <btyner at gmail.com>:
>>
>>> Is there any way to have xyplot produce the "points connected by lines"
>>> analogous to the effect of type="b" under standard graphics? I seem to
>>> recall doing this in xyplot at one point.
>>>
>>> Thanks,
>>> Ben
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>>
>>
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From btyner at gmail.com  Wed Jun 21 05:59:34 2006
From: btyner at gmail.com (Benjamin Tyner)
Date: Tue, 20 Jun 2006 23:59:34 -0400
Subject: [R] xyplot, type="b"
In-Reply-To: <4498BE6F.8060001@stat.auckland.ac.nz>
References: <4498B6D9.2050004@stat.purdue.edu>	<38b9f0350606202016y1ded6022ne0e3ecde374780d3@mail.gmail.com>
	<4498BB1D.5000306@stat.purdue.edu>
	<4498BE6F.8060001@stat.auckland.ac.nz>
Message-ID: <4498C426.7040804@stat.purdue.edu>

I can imagine the intended effect of this, but for some reason it does
not work as expected--the 'gaps' do not show up (I'm using 2.3.0). Also,
I think it would have to be tweaked to prevent overlapping for points
very close together. Incidentally, the benefit I seek is most pronounced
with pch=".", as then one gets the best of both worlds.

Ben

Paul Murrell wrote:

>Is this what you mean ... ?
>
>dat<-data.frame(x=1:10,y=1:10,z=sample(letters[1:3],10,T))
>xyplot(y~x | z, data = dat,
>       panel = function(x, y, ...) {
>         panel.lines(x, y, ...)
>         panel.points(x, y, col=trellis.par.get("background")$col,
>                      cex=1.5, pch=16, ...)
>         panel.points(x, y, ...)
>       })
>
>Paul
>


From sundar.dorai-raj at pdf.com  Wed Jun 21 06:26:23 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 20 Jun 2006 23:26:23 -0500
Subject: [R] xyplot, type="b"
In-Reply-To: <4498C426.7040804@stat.purdue.edu>
References: <4498B6D9.2050004@stat.purdue.edu>	<38b9f0350606202016y1ded6022ne0e3ecde374780d3@mail.gmail.com>	<4498BB1D.5000306@stat.purdue.edu>	<4498BE6F.8060001@stat.auckland.ac.nz>
	<4498C426.7040804@stat.purdue.edu>
Message-ID: <4498CA6F.6050504@pdf.com>

I think Paul's suggestion works if you use:

  panel.points(x, y, col = "white", cex = 1.5, pch = 16, ...)

instead of the default background color. For me
trellis.par.get("background")$col returns "transparent".

HTH,

--sundar

Benjamin Tyner wrote:
> I can imagine the intended effect of this, but for some reason it does
> not work as expected--the 'gaps' do not show up (I'm using 2.3.0). Also,
> I think it would have to be tweaked to prevent overlapping for points
> very close together. Incidentally, the benefit I seek is most pronounced
> with pch=".", as then one gets the best of both worlds.
> 
> Ben
> 
> Paul Murrell wrote:
> 
> 
>>Is this what you mean ... ?
>>
>>dat<-data.frame(x=1:10,y=1:10,z=sample(letters[1:3],10,T))
>>xyplot(y~x | z, data = dat,
>>      panel = function(x, y, ...) {
>>        panel.lines(x, y, ...)
>>        panel.points(x, y, col=trellis.par.get("background")$col,
>>                     cex=1.5, pch=16, ...)
>>        panel.points(x, y, ...)
>>      })
>>
>>Paul
>>
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From jdnewmil at dcn.davis.ca.us  Wed Jun 21 07:27:20 2006
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Tue, 20 Jun 2006 22:27:20 -0700
Subject: [R] GARCH
In-Reply-To: <d4c57560606200551x65d99d1pd1851cf45c7ca91@mail.gmail.com>
References: <d4c57560606200551x65d99d1pd1851cf45c7ca91@mail.gmail.com>
Message-ID: <4498D8B8.8060808@dcn.davis.ca.us>

Arun Kumar Saha wrote:
> Dear all R-users,
> 
> I have a GARCH related query. Suppose I fit a GARCH(1,1) model on a
> dataframe dat
> 
> 
>>garch1 = garch(dat)
>>summary(garch1)
> 
> Call:
> garch(x = dat)
> 
> Model:
> GARCH(1,1)
> 
> Residuals:
>     Min      1Q  Median      3Q     Max
> -4.7278 -0.3240  0.0000  0.3107 12.3981
> 
> Coefficient(s):
>     Estimate  Std. Error  t value Pr(>|t|)
> a0 1.212e-04   2.053e-06    59.05   <2e-16 ***
> a1 1.001e+00   4.165e-02    24.04   <2e-16 ***
> b1 2.435e-15   1.086e-02 2.24e-13        1
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> Diagnostic Tests:
>         Jarque Bera Test
> 
> data:  Residuals
> X-squared = 54480.76, df = 2, p-value < 2.2e-16
> 
> Now I want to store the value of Pr(>|t|) for coefficient a0, a1, and b1,
> and also values of these coefficients, so that I can use them in future
> separately. I know that I can do it for coefficients by using the command:
> coef(garch1)["a0"] etc, but not for Pr(>|t|). Can anyone please tell me how
> to do this?

This is less a question about GARCH and more a question about how R works
(which I know is true because I didn't know what GARCH was when I read the
question and I still don't but I can provide you a usable answer).

You can use the str() function to see the structure of an object in R:

 > garch1.summary <- summary(garch1)
 > str(garch1.summary)
List of 6
  $ residuals: Time-Series [1:999] from 2 to 1000:  0.206  0.709  0.476 

                                                    -0.291 -1.676 ...
   ..- attr(*, "na.removed")= int 1
  $ coef     : num [1:3, 1:4] 0.0799 0.6287 0.2118 0.0110 0.0755 ...
   ..- attr(*, "dimnames")=List of 2
   .. ..$ : chr [1:3] "a0" "a1" "b1"
   .. ..$ : chr [1:4] " Estimate" " Std. Error" " t value" "Pr(>|t|)"
  $ call     : language garch(x = x, order = c(1, 1))
  $ order    : Named num [1:2] 1 1
   ..- attr(*, "names")= chr [1:2] "p" "q"
  $ j.b.test :List of 5
   ..$ statistic: Named num 0.468
   .. ..- attr(*, "names")= chr "X-squared"
   ..$ parameter: Named num 2
   .. ..- attr(*, "names")= chr "df"
   ..$ p.value  : Named num 0.791
   .. ..- attr(*, "names")= chr "X-squared"
   ..$ method   : chr "Jarque Bera Test"
   ..$ data.name: chr "Residuals"
   ..- attr(*, "class")= chr "htest"
  $ l.b.test :List of 5
   ..$ statistic: Named num 1.01
   .. ..- attr(*, "names")= chr "X-squared"
   ..$ parameter: Named num 1
   .. ..- attr(*, "names")= chr "df"
   ..$ p.value  : num 0.316
   ..$ method   : chr "Box-Ljung test"
   ..$ data.name: chr "Squared.Residuals"
   ..- attr(*, "class")= chr "htest"
  - attr(*, "class")= chr "summary.garch"

 From this you can see that there is a "coef" list member
that contains the information you are after:

 > garch1.summary$coef
      Estimate  Std. Error  t value     Pr(>|t|)
a0 0.07989234  0.01104719 7.231912 4.762857e-13
a1 0.62870916  0.07551333 8.325804 0.000000e+00
b1 0.21184013  0.05384033 3.934599 8.333558e-05

this is apparently a matrix, so try matrix notation:

 > garch1.summary$coef[,4]
           a0           a1           b1
4.762857e-13 0.000000e+00 8.333558e-05

or

 > garch1.summary$coef[1,4]
[1] 4.762857e-13

Having said all that, this solution depends on implementation details
of the innards of the relevant objects, and in general if accessor
functions are available they should be used instead... but in this
case such accessors don't seem to be available.

 > help.search("garch-methods", package="tseries")

-- 
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From manojsw at gmail.com  Wed Jun 21 08:07:41 2006
From: manojsw at gmail.com (ManojW)
Date: Wed, 21 Jun 2006 15:07:41 +0900
Subject: [R] IMSL Wrapper
Message-ID: <01c301c694f9$04eac4a0$6900a8c0@HCJP.COM>

Dear R-help,
    Is there any wrapper avaliable to use IMSL C library functions within R?

Thanks.

Manoj


From epistat at gmail.com  Wed Jun 21 08:58:23 2006
From: epistat at gmail.com (zhijie zhang)
Date: Wed, 21 Jun 2006 14:58:23 +0800
Subject: [R] A question related with resale()
Message-ID: <2fc17e30606202358s6d4d76edk565c138fef458714@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060621/109a826f/attachment.pl 

From rkrug at sun.ac.za  Wed Jun 21 09:02:21 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Wed, 21 Jun 2006 09:02:21 +0200
Subject: [R] hello Excel... (native/Package/BETA)
In-Reply-To: <47fce0650606200153s184199dfsad431bd780bfc207@mail.gmail.com>
References: <47fce0650606200153s184199dfsad431bd780bfc207@mail.gmail.com>
Message-ID: <4498EEFD.4090807@sun.ac.za>

Hans-Peter wrote:
> Dear list members
> 
> I am pleased to annonce that I have just finished a native Excel
> reader/writer. It's wrapped up in two packages: either "xlsReadWrite" (open
> source) or the slightly beefed-up "xlsReadWritePro" (shareware). Working
> with Excel data is now as easy as writing read.xls and write.xls.
> 
> Some more details:
> 
> - Infos and download: http://treetron.googlepages.com
> - for detailed documentation pls. download and see: help files, DESCRIPTION
> and README.
> - I set up a newsgroup for technical questions and feedback:
> http://groups.google.ch/group/supportR
> 
> - while it could be ported to other platforms, currently it is WINDOWS only
> (see technical below)
<SNIP>

If I understand it right, the the flexcel library includes the
components for kylix as well - therefore I would try to compile it for
Linux, I would suggest to use CrossKylix to compile it.


Rainer

<SNIP>



-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
      	Rainer at krugs.de


From ripley at stats.ox.ac.uk  Wed Jun 21 09:50:33 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Jun 2006 08:50:33 +0100 (BST)
Subject: [R] multi-dimension array of raw
In-Reply-To: <44989A35.3020405@stats.uwo.ca>
References: <20060620182409.2f2902a4@pav.localdomain>
	<44989A35.3020405@stats.uwo.ca>
Message-ID: <Pine.LNX.4.64.0606210847430.11993@gannet.stats.ox.ac.uk>

On Tue, 20 Jun 2006, Duncan Murdoch wrote:

> On 6/20/2006 6:24 PM, Gerald Jansen wrote:
>> I would like to store and manipulate large sets of marker genotypes
>> compactly using "raw" data arrays. This works fine for vectors or
>> matrices, but I run into the error shown in the example below as soon
>> as I try to use 3 dimensional arrays (eg. animal x marker x allele).
>>
>>> a <- array(as.raw(1:6),c(2,3))
>>> a
>>      [,1] [,2] [,3]
>> [1,]   01   03   05
>> [2,]   02   04   06
>>> a[1,] <- raw(3)
>>> a
>>      [,1] [,2] [,3]
>> [1,]   00   00   00
>> [2,]   02   04   06
>>> b <- array(as.raw(1:6),c(1,2,3))
>>> b[1,,]
>>      [,1] [,2] [,3]
>> [1,]   01   03   05
>> [2,]   02   04   06
>>> b[1,1,] <- raw(3)
>> Error: incompatible types (from raw to raw) in array subset assignment
>>
>> I can work around this with computed indices, but I wonder if this is
>> expected behaviour.
>
> I don't think so.  It is just an unimplemented case.
>
> Using raw in this way is a fairly unusual thing to do, and you've come
> across a case nobody thought of implementing.

Actually, raw arrays were originally not implemented at all, so this is 
`expected behaviour', and you may well encounter other unimplemented areas 
as no one has attempted to implement them comprehensively. Please report 
them, but on R-devel not here.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Wed Jun 21 09:59:28 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Jun 2006 08:59:28 +0100 (BST)
Subject: [R] GARCH
In-Reply-To: <4498D8B8.8060808@dcn.davis.ca.us>
References: <d4c57560606200551x65d99d1pd1851cf45c7ca91@mail.gmail.com>
	<4498D8B8.8060808@dcn.davis.ca.us>
Message-ID: <Pine.LNX.4.64.0606210854240.11993@gannet.stats.ox.ac.uk>

Why do you think

> help.search("garch-methods", package="tseries")

finds accessor functions?  That is notation for S4 methods, and "garch" is 
an S3 class so there will be none.  Here there _is_ an accessor, coef(), 
and you can find that there is by

> methods(class="garch")
[1] coef.garch*      fitted.garch*    logLik.garch*    plot.garch*
[5] predict.garch*   print.garch*     residuals.garch* summary.garch*

    Non-visible functions are asterisked

Note though that inherited methods might be relevant too (e.g. default 
methods) and indeed it seems that here the default method for coef would 
work just as well.


On Tue, 20 Jun 2006, Jeff Newmiller wrote:

> Arun Kumar Saha wrote:
>> Dear all R-users,
>>
>> I have a GARCH related query. Suppose I fit a GARCH(1,1) model on a
>> dataframe dat
>>
>>
>>> garch1 = garch(dat)
>>> summary(garch1)
>>
>> Call:
>> garch(x = dat)
>>
>> Model:
>> GARCH(1,1)
>>
>> Residuals:
>>     Min      1Q  Median      3Q     Max
>> -4.7278 -0.3240  0.0000  0.3107 12.3981
>>
>> Coefficient(s):
>>     Estimate  Std. Error  t value Pr(>|t|)
>> a0 1.212e-04   2.053e-06    59.05   <2e-16 ***
>> a1 1.001e+00   4.165e-02    24.04   <2e-16 ***
>> b1 2.435e-15   1.086e-02 2.24e-13        1
>> ---
>> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>
>> Diagnostic Tests:
>>         Jarque Bera Test
>>
>> data:  Residuals
>> X-squared = 54480.76, df = 2, p-value < 2.2e-16
>>
>> Now I want to store the value of Pr(>|t|) for coefficient a0, a1, and b1,
>> and also values of these coefficients, so that I can use them in future
>> separately. I know that I can do it for coefficients by using the command:
>> coef(garch1)["a0"] etc, but not for Pr(>|t|). Can anyone please tell me how
>> to do this?
>
> This is less a question about GARCH and more a question about how R works
> (which I know is true because I didn't know what GARCH was when I read the
> question and I still don't but I can provide you a usable answer).
>
> You can use the str() function to see the structure of an object in R:
>
> > garch1.summary <- summary(garch1)
> > str(garch1.summary)
> List of 6
>  $ residuals: Time-Series [1:999] from 2 to 1000:  0.206  0.709  0.476
>
>                                                    -0.291 -1.676 ...
>   ..- attr(*, "na.removed")= int 1
>  $ coef     : num [1:3, 1:4] 0.0799 0.6287 0.2118 0.0110 0.0755 ...
>   ..- attr(*, "dimnames")=List of 2
>   .. ..$ : chr [1:3] "a0" "a1" "b1"
>   .. ..$ : chr [1:4] " Estimate" " Std. Error" " t value" "Pr(>|t|)"
>  $ call     : language garch(x = x, order = c(1, 1))
>  $ order    : Named num [1:2] 1 1
>   ..- attr(*, "names")= chr [1:2] "p" "q"
>  $ j.b.test :List of 5
>   ..$ statistic: Named num 0.468
>   .. ..- attr(*, "names")= chr "X-squared"
>   ..$ parameter: Named num 2
>   .. ..- attr(*, "names")= chr "df"
>   ..$ p.value  : Named num 0.791
>   .. ..- attr(*, "names")= chr "X-squared"
>   ..$ method   : chr "Jarque Bera Test"
>   ..$ data.name: chr "Residuals"
>   ..- attr(*, "class")= chr "htest"
>  $ l.b.test :List of 5
>   ..$ statistic: Named num 1.01
>   .. ..- attr(*, "names")= chr "X-squared"
>   ..$ parameter: Named num 1
>   .. ..- attr(*, "names")= chr "df"
>   ..$ p.value  : num 0.316
>   ..$ method   : chr "Box-Ljung test"
>   ..$ data.name: chr "Squared.Residuals"
>   ..- attr(*, "class")= chr "htest"
>  - attr(*, "class")= chr "summary.garch"
>
> From this you can see that there is a "coef" list member
> that contains the information you are after:
>
> > garch1.summary$coef
>      Estimate  Std. Error  t value     Pr(>|t|)
> a0 0.07989234  0.01104719 7.231912 4.762857e-13
> a1 0.62870916  0.07551333 8.325804 0.000000e+00
> b1 0.21184013  0.05384033 3.934599 8.333558e-05
>
> this is apparently a matrix, so try matrix notation:
>
> > garch1.summary$coef[,4]
>           a0           a1           b1
> 4.762857e-13 0.000000e+00 8.333558e-05
>
> or
>
> > garch1.summary$coef[1,4]
> [1] 4.762857e-13
>
> Having said all that, this solution depends on implementation details
> of the innards of the relevant objects, and in general if accessor
> functions are available they should be used instead... but in this
> case such accessors don't seem to be available.
>
> > help.search("garch-methods", package="tseries")
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From hodgess at gator.dt.uh.edu  Wed Jun 21 11:35:32 2006
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Wed, 21 Jun 2006 04:35:32 -0500
Subject: [R]  latex function with lm
Message-ID: <200606210935.k5L9ZWMc002927@gator.dt.uh.edu>

Dear R People:

I have used the "latex" function from the Hmisc
package and it is just great!

However, I have a new question regarding that function:
is there an option for summary(lm(y~x)), please?  There are
options for different types of objects, but I didn't see one
for that.  Maybe I just missed it.

Thanks in advance!

R for Windows Version 2.3.1

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu


From aroman6 at gmail.com  Wed Jun 21 11:46:27 2006
From: aroman6 at gmail.com (Angel Roman)
Date: Wed, 21 Jun 2006 11:46:27 +0200
Subject: [R] png() or jpeg() in a php script
Message-ID: <f452beeb0606210246n7d484937o3467e4b7030e03c1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060621/78c8f860/attachment.pl 

From ripley at stats.ox.ac.uk  Wed Jun 21 12:00:32 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Jun 2006 11:00:32 +0100 (BST)
Subject: [R] png() or jpeg() in a php script
In-Reply-To: <f452beeb0606210246n7d484937o3467e4b7030e03c1@mail.gmail.com>
References: <f452beeb0606210246n7d484937o3467e4b7030e03c1@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0606211057560.30847@gannet.stats.ox.ac.uk>

This is covered (in at least three places) on the help page for those 
functions.

> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html

and do as we ask (including reporting your system, reading help pages and 
not sending HTML mail).

On Wed, 21 Jun 2006, Angel Roman wrote:

> Hello,
>
> I've got a problem with a PHP script, in which I call the system function
> (to call another processes). The call is :
>
> system("R --slave --vanilla < path/to/source/source.R");
>
> if in this R file, I've got the lines:
>  pdf(file="/path/to/file/file.pdf")
>  commands
>  dev.off()
>
> the pdf file is perfectly created
>
> but if I change the lines to:
>
>  png(file="/path/to/file/file.png");
>  commands
>  dev.off()
>
> (or with the jpeg() function)
>
> the files are not created. any help?
>
> Thanks
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From dieter.menne at menne-biomed.de  Wed Jun 21 12:13:42 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Wed, 21 Jun 2006 10:13:42 +0000 (UTC)
Subject: [R] latex function with lm
References: <200606210935.k5L9ZWMc002927@gator.dt.uh.edu>
Message-ID: <loom.20060621T120944-102@post.gmane.org>

Erin Hodgess <hodgess <at> gator.dt.uh.edu> writes:

> I have used the "latex" function from the Hmisc
> package and it is just great!
> 
> However, I have a new question regarding that function:
> is there an option for summary(lm(y~x)), please?  There are
> options for different types of objects, but I didn't see one

I have written one (also form lme) in a library called Dmisc. It passes CRAN
tests, but as it's a bit my personal taste, I have not submitted it to CRAN.
Main feature is that reduces show number of digits per line by looking at the
standard deviation. Too many people loved Excel-type 12 decimals. You may try it
as starter.

http://www.menne-biomed.de/download

Dieter


From avilella at gmail.com  Wed Jun 21 12:38:17 2006
From: avilella at gmail.com (Albert Vilella)
Date: Wed, 21 Jun 2006 11:38:17 +0100
Subject: [R] sort matrix by sum of columns
Message-ID: <1150886297.5922.1.camel@localhost>

Hi all,

I would like to know how can I sort the cols of a matrix by the sum of
their elements.


a <- matrix(as.integer(rnorm(25,4,2)),10,5)
colnames(a) = c("alfa","bravo","charlie","delta","echo")

I guess I should use colSums, and then rearrange the matrix somehow
according to the result.

My idea is to display a "sorted" barplot:

barplot(a, horiz=TRUE, legend.text=T)

Thanks in advance,

    Albert.


From dimitris.rizopoulos at med.kuleuven.be  Wed Jun 21 13:02:59 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 21 Jun 2006 13:02:59 +0200
Subject: [R] sort matrix by sum of columns
References: <1150886297.5922.1.camel@localhost>
Message-ID: <001d01c69522$42503fe0$0540210a@www.domain>

probably you're looking for ?order(), e.g.,

barplot(a[, order(colSums(a))], horiz = TRUE, legend.text = TRUE)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Albert Vilella" <avilella at gmail.com>
To: "r help" <r-help at stat.math.ethz.ch>
Sent: Wednesday, June 21, 2006 12:38 PM
Subject: [R] sort matrix by sum of columns


> Hi all,
>
> I would like to know how can I sort the cols of a matrix by the sum 
> of
> their elements.
>
>
> a <- matrix(as.integer(rnorm(25,4,2)),10,5)
> colnames(a) = c("alfa","bravo","charlie","delta","echo")
>
> I guess I should use colSums, and then rearrange the matrix somehow
> according to the result.
>
> My idea is to display a "sorted" barplot:
>
> barplot(a, horiz=TRUE, legend.text=T)
>
> Thanks in advance,
>
>    Albert.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From petr.pikal at precheza.cz  Wed Jun 21 13:04:48 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 21 Jun 2006 13:04:48 +0200
Subject: [R] sort matrix by sum of columns
In-Reply-To: <1150886297.5922.1.camel@localhost>
Message-ID: <449943F0.13129.105B85D@localhost>

Hi

order is your friend


On 21 Jun 2006 at 11:38, Albert Vilella wrote:

From:           	Albert Vilella <avilella at gmail.com>
To:             	r help <r-help at stat.math.ethz.ch>
Date sent:      	Wed, 21 Jun 2006 11:38:17 +0100
Subject:        	[R] sort matrix by sum of columns
Send reply to:  	avilella at gmail.com
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>

> Hi all,
> 
> I would like to know how can I sort the cols of a matrix by the sum of
> their elements.
> 
> 
> a <- matrix(as.integer(rnorm(25,4,2)),10,5)
> colnames(a) = c("alfa","bravo","charlie","delta","echo")

 ord<-order(colSums(a))
 barplot(a[,ord], horiz=TRUE, legend.text=T)

HTH
Petr


>
> 
> I guess I should use colSums, and then rearrange the matrix somehow
> according to the result.
> 
> My idea is to display a "sorted" barplot:
> 
> barplot(a, horiz=TRUE, legend.text=T)
> 
> Thanks in advance,
> 
>     Albert.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From manojsw at gmail.com  Wed Jun 21 13:31:22 2006
From: manojsw at gmail.com (Manoj)
Date: Wed, 21 Jun 2006 20:31:22 +0900
Subject: [R] IMSL Wrapper
Message-ID: <829e6c8a0606210431v7118ee2dh706ff610852a2011@mail.gmail.com>

Dear All,
       Is there any Wrapper written around IMSL C libraries that makes
it possible to access the IMSL C functions from within R?

        Any pointers would be greatly appreciated!

Cheers

Manoj


From sundar.dorai-raj at pdf.com  Wed Jun 21 14:25:58 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 21 Jun 2006 07:25:58 -0500
Subject: [R] IMSL Wrapper
In-Reply-To: <829e6c8a0606210431v7118ee2dh706ff610852a2011@mail.gmail.com>
References: <829e6c8a0606210431v7118ee2dh706ff610852a2011@mail.gmail.com>
Message-ID: <44993AD6.8030608@pdf.com>



Manoj wrote:
> Dear All,
>        Is there any Wrapper written around IMSL C libraries that makes
> it possible to access the IMSL C functions from within R?
> 
>         Any pointers would be greatly appreciated!
> 
> Cheers
> 
> Manoj
> 


What functions are you looking for? I'd be surprised if IMSL had some 
functionality that R did not.


--sundar


From burak.karacaoeren at inw.agrl.ethz.ch  Wed Jun 21 14:40:31 2006
From: burak.karacaoeren at inw.agrl.ethz.ch (Burak Karacaoeren)
Date: Wed, 21 Jun 2006 14:40:31 +0200
Subject: [R] submission
Message-ID: <44993E3F.6050504@inw.agrl.ethz.ch>


From f.harrell at vanderbilt.edu  Wed Jun 21 14:46:32 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 21 Jun 2006 12:46:32 +0000
Subject: [R] latex function with lm
In-Reply-To: <200606210935.k5L9ZWMc002927@gator.dt.uh.edu>
References: <200606210935.k5L9ZWMc002927@gator.dt.uh.edu>
Message-ID: <44993FA8.1020807@vanderbilt.edu>

Erin Hodgess wrote:
> Dear R People:
> 
> I have used the "latex" function from the Hmisc
> package and it is just great!
> 
> However, I have a new question regarding that function:
> is there an option for summary(lm(y~x)), please?  There are
> options for different types of objects, but I didn't see one
> for that.  Maybe I just missed it.

There is no latex method for summary(lm); contributions welcomed.  You 
might also look at latex.ols, latex.anova.Design, latex.summary.Design.

Frank

> 
> Thanks in advance!
> 
> R for Windows Version 2.3.1
> 
> Sincerely,
> Erin Hodgess
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgess at gator.uhd.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From rab45+ at pitt.edu  Wed Jun 21 14:59:29 2006
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Wed, 21 Jun 2006 08:59:29 -0400
Subject: [R] lmer and mixed effects logistic regression
In-Reply-To: <148ed8180606201127p730e699emb9e901b09a7ab039@mail.gmail.com>
References: <1150293574.3416.11.camel@localhost.localdomain>
	<449431F1.6000906@pdf.com>
	<40e66e0b0606180458p177fa824j6a181fd52c3d1277@mail.gmail.com>
	<1150726454.3200.20.camel@localhost.localdomain>
	<148ed8180606201127p730e699emb9e901b09a7ab039@mail.gmail.com>
Message-ID: <1150894770.3329.13.camel@localhost.localdomain>

On Tue, 2006-06-20 at 20:27 +0200, G?ran Brostr?m wrote:
> On 6/19/06, Rick Bilonick <rab45+ at pitt.edu> wrote:
> > On Sun, 2006-06-18 at 13:58 +0200, Douglas Bates wrote:
> > > If I understand correctly Rick it trying to fit a model with random
> > > effects on a binary response when there are either 1 or 2 observations
> > > per group.
> 
> If you look at Rick's examples, it's worse than that; each group
> contains identical observations (by design?).
> 
> May I suggest:
> 
> > glm(y ~ x, family = binomial, data = unique(example.df))
> 
> I think lmer gives a very sensible answer to this problem.
> 
> G?ran
> 
The paired responses happen to be always the same in the data set that I
have. My understanding is that they could differ, but rarely do. For the
particular single independent variable, it will always be the same for
each observation for a given subject. So I essentially have 2n
observations where there are n unique results. However, I want to add
additional independent variables where the measurements differ within a
subject (even though the response within the subject is the same).

I ran glm on the n subjects and the 2n for lmer and get similar
estimates and se's but not identical. With just one independent variable
where the observations are identical in each cluster, lmer gives
slightly smaller se's using all 2n. When I include a second independent
variable that varies within each subject, lmer gives larger standard
errors, about 25% larger for the independent variable that doesn't vary
within subjects and just slightly larger for the one that does vary.

I could create a data set where I take all subjects with just one
observation per subject and then randomly select one observation from
each pair for subjects who have both observations. But I'd rather not
have to randomly remove observations.

I would expect that when the responses and independent variable are the
same within each subject for all subjects, the residual error must be
zero after you account for a random effect for subjects.

Rick B.


From dieter.menne at menne-biomed.de  Wed Jun 21 15:06:40 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Wed, 21 Jun 2006 13:06:40 +0000 (UTC)
Subject: [R] png() or jpeg() in a php script
References: <f452beeb0606210246n7d484937o3467e4b7030e03c1@mail.gmail.com>
Message-ID: <loom.20060621T150459-955@post.gmane.org>

Angel Roman <aroman6 <at> gmail.com> writes:

> I've got a problem with a PHP script, in which I call the system function
> (to call another processes). The call is :
> 
> system("R --slave --vanilla < path/to/source/source.R");
...


> but if I change the lines to:
> 
>   png(file="/path/to/file/file.png");
>   commands
>   dev.off()
> 
> (or with the jpeg() function)
> 
> the files are not created. any help?

Try the following in your script to find out if you can create jpeg on your
operation system.

sink("cap.txt")
capabilities()
sink()


Dieter


From andy_liaw at merck.com  Wed Jun 21 15:36:22 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 21 Jun 2006 09:36:22 -0400
Subject: [R] IMSL Wrapper
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA026FFFA8@usctmx1106.merck.com>

From: Sundar Dorai-Raj
> 
> Manoj wrote:
> > Dear All,
> >        Is there any Wrapper written around IMSL C libraries 
> that makes 
> > it possible to access the IMSL C functions from within R?
> > 
> >         Any pointers would be greatly appreciated!
> > 
> > Cheers
> > 
> > Manoj
> > 
> 
> 
> What functions are you looking for? I'd be surprised if IMSL 
> had some functionality that R did not.
> 
> --sundar

Perhaps another alternative is the gsl package, which provides R interface
to the GSL (if what Manoj wants is available there).

Andy


From hb8hb8 at gmail.com  Wed Jun 21 16:12:18 2006
From: hb8hb8 at gmail.com (Greg thomas)
Date: Wed, 21 Jun 2006 07:12:18 -0700
Subject: [R] rJava: java.lang.OutOfMemoryError
Message-ID: <943ea5310606210712y36b323e5hbd307549c7dbc75f@mail.gmail.com>

Hi,
Has anybody encountered the following problem?
TIA, Gregoire

> library(rJava)
> .jinit()
Error occurred during initialization of VM
java.lang.OutOfMemoryError: unable to create new native thread

This would not help:
> .jinit(parameters=c("-Xmx512m","-Xmx128m"))

R and rJava versions are:
> installed.packages()["rJava",]
                Package                 LibPath                 Version
                "rJava"    "/home/user/lib/R"                 "0.4-3"
               Priority                  Bundle                Contains
                     NA                      NA                      NA
                Depends                Suggests                 Imports
"R (>= 2.0.0), methods"                      NA                      NA
                  Built
                "2.3.1"
> R.Version()
$platform
[...]
$system
[1] "i686, linux-gnu"
[...]
$version.string
[1] "Version 2.3.1 (2006-06-01)"


From David.Brahm at geodecapital.com  Wed Jun 21 16:22:46 2006
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Wed, 21 Jun 2006 10:22:46 -0400
Subject: [R] hello Excel... (native/Package/BETA)
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D806867D77@MSGBOSCLF2WIN.DMN1.FMR.COM>

Hans-Peter,

   In my office, files are stored on an EMC shared server which is
used by both our Windows PC's and our Linux machines.  So it is common
for us to process spreadsheet data using R on Linux.  A Linux version
of your package would be very welcome.  Thanks!

-- David Brahm (brahm at alum.mit.edu)
P.S. No, I'm not volunteering to port it :-)


From msubianto at gmail.com  Wed Jun 21 17:22:03 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Wed, 21 Jun 2006 17:22:03 +0200
Subject: [R] expanded dataset and random number
In-Reply-To: <8c0ddffa0606201328y488ba3afh79dc76f823da12d0@mail.gmail.com>
References: <c7c17cef0606200410w7d2a4f07h814f1db16e59baff@mail.gmail.com>
	<8c0ddffa0606201328y488ba3afh79dc76f823da12d0@mail.gmail.com>
Message-ID: <4499641B.5000004@gmail.com>

Dear all,
Per Jensen, thanks for your great help. All methods are very useful.

Best, Muhammad Subianto

On this day 20/06/2006 22:28, Per Jensen wrote:

> A couple of suggestions:
>
> #First solution
> mydatexpanded<-mydat[rep(1:5,mydat[,1]),]
>
> sampledat<-mydatexpanded[sample(1:85,7),-1]
>
> #Second solution
>
> sampledat<-mydat[sample(1:5,size=7,prob=mydat[,1]/85,replace=TRUE),-1]
>
> Regards
> Per Jensen
>
> On 6/20/06, Muhammad Subianto <msubianto at gmail.com> wrote:
>
>>
>> Dear all R-users,
>> (My apologies if this subject is wrong)
>> I have dataset:
>> mydat <- as.data.frame(
>>          matrix(c(14,0,1,0,1,1,
>>                   25,1,1,0,1,1,
>>                   5,0,0,1,1,0,
>>                   31,1,1,1,1,1,
>>                   10,0,0,0,0,1),
>>          nrow=5,ncol=6,byrow=TRUE))
>> dimnames(mydat)[[2]] <-c("size","A","B","C","D","E")
>> > mydat
>>   size A B C D E
>> 1   14 0 1 0 1 1
>> 2   25 1 1 0 1 1
>> 3    5 0 0 1 1 0
>> 4   31 1 1 1 1 1
>> 5   10 0 0 0 0 1
>> > sum(mydat$size)
>> [1] 85
>> >
>>
>> where size is number of each row that have this combination of 
>> variables.
>> In this dataset I have 85 tuples in expanded dataset.
>> I want to generate random number between 1 and 85.
>> Say, if the first random number is 15, so the number 15 is
>> 1 1 0 1 1
>> then, if the next random number is 7, then
>> 0 1 0 1 1
>>
>> then if
>> random number is 79
>> 0 0 0 0 1
>> random number is 46
>> 1 1 1 1 1
>> random number is 3
>> 0 1 0 1 1
>> random number is 28
>> 1 1 0 1 1
>>
>> So, the result random tuples (order from 6 random number):
>> 0 1 0 1 1
>> 0 1 0 1 1
>> 1 1 0 1 1
>> 1 1 0 1 1
>> 1 1 1 1 1
>> 0 0 0 0 1
>>
>> I  would be very happy if anyone could help me.
>> Thank you very much in advance.
>> Kindly regards,  Muhammad Subianto
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>


From spencer.graves at pdf.com  Wed Jun 21 17:35:15 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 21 Jun 2006 08:35:15 -0700
Subject: [R] lmer and mixed effects logistic regression
In-Reply-To: <1150894770.3329.13.camel@localhost.localdomain>
References: <1150293574.3416.11.camel@localhost.localdomain>	<449431F1.6000906@pdf.com>	<40e66e0b0606180458p177fa824j6a181fd52c3d1277@mail.gmail.com>	<1150726454.3200.20.camel@localhost.localdomain>	<148ed8180606201127p730e699emb9e901b09a7ab039@mail.gmail.com>
	<1150894770.3329.13.camel@localhost.localdomain>
Message-ID: <44996733.9050609@pdf.com>

	  You could think of 'lmer(..., family=binomial)' as doing a separate 
"glm" fit for each subject, with some shrinkage provided by the assumed 
distribution of the random effect parameters for each subject.  Since 
your data are constant within subject, the intercept in your model 
without the subject's random effect distribution will be estimated at 
+/-Inf.  Since this occurs for all subjects, the maximum likelihood 
estimate of the subject variance is Inf, which is what I wrote in an 
earlier contribution to this thread.

	  What kind of answer do you get from SAS NLMIXED?  If it does NOT tell 
you that there is something strange about the estimation problem you've 
given it, I would call that a serious infelicity in the code.  If it is 
documented behavior, some might argue that it doesn't deserve the "B" 
word ("Bug").  The warning messages issued by 'lmer' in this case are 
something I think users would want, even if they are cryptic.

	  Hope this helps.
	  Spencer Graves	

Rick Bilonick wrote:
> On Tue, 2006-06-20 at 20:27 +0200, G?ran Brostr?m wrote:
>> On 6/19/06, Rick Bilonick <rab45+ at pitt.edu> wrote:
>>> On Sun, 2006-06-18 at 13:58 +0200, Douglas Bates wrote:
>>>> If I understand correctly Rick it trying to fit a model with random
>>>> effects on a binary response when there are either 1 or 2 observations
>>>> per group.
>> If you look at Rick's examples, it's worse than that; each group
>> contains identical observations (by design?).
>>
>> May I suggest:
>>
>>> glm(y ~ x, family = binomial, data = unique(example.df))
>> I think lmer gives a very sensible answer to this problem.
>>
>> G?ran
>>
> The paired responses happen to be always the same in the data set that I
> have. My understanding is that they could differ, but rarely do. For the
> particular single independent variable, it will always be the same for
> each observation for a given subject. So I essentially have 2n
> observations where there are n unique results. However, I want to add
> additional independent variables where the measurements differ within a
> subject (even though the response within the subject is the same).
> 
> I ran glm on the n subjects and the 2n for lmer and get similar
> estimates and se's but not identical. With just one independent variable
> where the observations are identical in each cluster, lmer gives
> slightly smaller se's using all 2n. When I include a second independent
> variable that varies within each subject, lmer gives larger standard
> errors, about 25% larger for the independent variable that doesn't vary
> within subjects and just slightly larger for the one that does vary.
> 
> I could create a data set where I take all subjects with just one
> observation per subject and then randomly select one observation from
> each pair for subjects who have both observations. But I'd rather not
> have to randomly remove observations.
> 
> I would expect that when the responses and independent variable are the
> same within each subject for all subjects, the residual error must be
> zero after you account for a random effect for subjects.
> 
> Rick B.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From gunter.berton at gene.com  Wed Jun 21 17:42:40 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 21 Jun 2006 08:42:40 -0700
Subject: [R] R Reference Card and other help (especially useful for Newbies)
Message-ID: <002b01c69549$54b66a70$f8c2fea9@gne.windows.gene.com>

 
Hi all:

Happy summer solstice to all northern hemispherics (and winter solstice to
the southerners).
 
Newbies (and others!) may find useful the R Reference Card made available by
Tom Short and Rpad at http://www.rpad.org/Rpad/Rpad-refcard.pdf  or through
the "Contributed" link on CRAN (where some other reference cards are also
linked). It categorizes and organizes a bunch of R's basic, most used
functions so that they can be easily found. For example, paste() is under
the "Strings" heading and expand.grid() is under "Data Creation." For
newbies struggling to find the right R function as well as veterans who
can't quite remember the function name, it's very handy.

Also don't forget R's other Help facilties:

help.search("keyword or phrase") to search the **installed man pages **

RSiteSearch("keyword or phrase") to search the CRAN website via Jonathan
Baron's search engine. This can also be done directly from CRAN by following
the "search" link there.

And, occasionally, find()/apropos() to search **the attached session
packages** for functions using rexexp's.

Though R certainly can be intimidating, please **do** try these measures
first before posting questions to the list. And please **do** read the other
basic R reference materials. Better and faster answers can usually be found
this way.
 
-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From sw283 at maths.bath.ac.uk  Wed Jun 21 18:09:43 2006
From: sw283 at maths.bath.ac.uk (Simon Wood)
Date: Wed, 21 Jun 2006 17:09:43 +0100 (BST)
Subject: [R] GAM selection error msgs (mgcv & gam packages)
In-Reply-To: <1150636325.449551252b44f@www-mail.usyd.edu.au>
References: <1150636325.449551252b44f@www-mail.usyd.edu.au>
Message-ID: <Pine.LNX.4.64.0606211701040.4922@osiris.maths.bath.ac.uk>

>
> My question concerns 2 error messages; one in the gam package and one in
> the mgcv package (see below). I have read help files and Chambers and
> Hastie book but am failing to understand how I can solve this problem.
> Could you please tell me what I must adjust so that the command does not
> generate error message?
>
> I am trying to achieve model selection for a GAM which is required for
> prediction purposes, thus my focus is on AIC. My data set has 3038 records
> and 116 predictor variables and a binary response variable [0 or 1]. There
> is no current understanding of the predictors' relationship to response so
> I am relying on GAM for selection of appropriate predictors.

- I have some worries about using a GAM in this sort of situation - it 
seems like an odd model to start from to me: you don't know the 
relationship to the covariates, but do know that it should be additive? Is 
that really true? If it is then it may still be alot to ask of the model 
selection methods to find a good model. (I'd certainly consider upping 
the `gamma' parameter in mgcv:::gam).

- General uneasiness apart, the specific warning message relates to the 
number of distinct covariate values that you have (or number of distinct 
X,Y,Z triplets). Do any of the covariates for single smooths have fewer 
than 10 distinct values? There are more than 50 distinct x,y,z triplets, I 
suppose? If you have distinct fewer covariate points for a smooth than the 
default k (10), then you need to reduce k to the number of distinct 
points, or fewer.

- Finally, for speed reasons, I'd use the "cr" basis (see ?s) if doing 
this.

best,
Simon

>- Simon Wood, Mathematical Sciences, University of Bath, Bath BA2 7AY 
>-             +44 (0)1225 386603         www.maths.bath.ac.uk/~sw283/


>
> Thanks
> Savrina
>
> *mgcv package 1.3-12:
>
> # I start with specifying the full model with 116 predictors including
> isotropic smooth of 3D location variables (when I specify only the first
> 14 predictors I get no error message)
>>
> m0<-gam(label~s(x,y,z,k=50),s+(feature4)+s(feature5)+s(feature6)+...+s(feature116),data=k.data,
> family=binomial)
>
> Error in smooth.construct.tp.smooth.spec(object, data, knots):
>     A term has fewer unique covariate combinations than specified maximum
> degrees of freedom
>
> # I was going to follow this with backwards selection by hypothesis testing
> (remove highest p-val term one at a time) and also AIC comparison of all
> the models
>
>> From help file entitled 'Generalised additive models with integrated
> smoothness estimation' I calculated the following where do I go from here?
> A) "k is the basis dimension of a given term...if k is not specified
> k=10*3^(d-1) where 'd' is the number of covariates for this term"
> My calculations: for all my terms but the first d=1 thus k=10*3^0=10.
> B) "You must have more unique combinations of covariates than the model has
> total parameters"
> My calculations: total parameters = sum of basis dimensions(50+10*113) +
> sum of non-spline terms(0) - number of spline terms(114) = 1066
>
> *gam package:
> I think stepwise selection provided by gam package would be useful in
> finding the best predictive model. I follow example on pg 283 from
> 'Statistical models in S' Chambers and Hastie 1993.
> # I start with a full model where all predictors enter linearly
>> k.start<-gam(label~., data=k.data, family=binomial)
>
> # set up scope list with possibilities for each term eg .~1 + x + s(x)
> # ignore the first column of the data set
>> k.scope<-gam.scope(k.data[,-1])
>
> # start step wise selection
>> k.step<-step(k.start,k.scope)
> #condensed output
> Start: AIC=1549.48
> label~s+y+z+feature4+feature5+...+feature116
>                Df     Deviance       AIC
> <none>                 1319.5         1549.5
> - feature54     -1     1319.2         1551.2
> - feature26     -1     1319.2         1551.2
> ...
> -feature12      -1     1357.4         1589.4
> There were 50 or more warnings (use warnings() to see the first 50)
>
> # all 50 warnings are the same
>> warnings()
> Warning messages:
> 1: fitted probabilities numerically 0 or 1 occurred in: glm.fit(x[, jj,
> drop = FALSE], y, wt, offset = object$offset,   ...
>
> # it seems to not get passed the orginal linear model. It should show all
> the steps taken to the final model
>> k.step$anova
>  Step Df Deviance Resid. Df Resid. Dev      AIC
> 1      NA       NA      2922   1317.599 1549.599
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From jdnewmil at dcn.davis.ca.us  Wed Jun 21 18:12:22 2006
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Wed, 21 Jun 2006 09:12:22 -0700
Subject: [R] GARCH
In-Reply-To: <Pine.LNX.4.64.0606210854240.11993@gannet.stats.ox.ac.uk>
References: <d4c57560606200551x65d99d1pd1851cf45c7ca91@mail.gmail.com>
	<4498D8B8.8060808@dcn.davis.ca.us>
	<Pine.LNX.4.64.0606210854240.11993@gannet.stats.ox.ac.uk>
Message-ID: <44996FE6.6040909@dcn.davis.ca.us>

Prof Brian Ripley wrote:
> Why do you think
> 
>> help.search("garch-methods", package="tseries")
> 
> finds accessor functions?  That is notation for S4 methods, and "garch" 
> is an S3 class so there will be none.  Here there _is_ an accessor, 
> coef(), and you can find that there is by

Probably because I used it, found a mention of various extraction functions
including coef(), and could not find a way to access "Pr(>|t|)" using
coef(). Nor have I had luck with
  help.search("summary.garch", package="tseries")

Possibly also because I have not yet figured out the difference between
S4 and S3 methods, but since the result of my  help.search call displayed S3
functions I don't see how knowing this difference would have helped.

>> methods(class="garch")
> 
> [1] coef.garch*      fitted.garch*    logLik.garch*    plot.garch*
> [5] predict.garch*   print.garch*     residuals.garch* summary.garch*
> 
>    Non-visible functions are asterisked
> 
> Note though that inherited methods might be relevant too (e.g. default 
> methods) and indeed it seems that here the default method for coef would 
> work just as well.

Given Arun Kumar Saha's question...

 > > Now I want to store the value of Pr(>|t|) for coefficient a0, a1,
 > > and b1, and also values of these coefficients, so that I can use
 > > them in future separately. I know that I can do it for coefficients
 > > by using the command:
 > > coef(garch1)["a0"] etc, but not for Pr(>|t|). Can anyone please
 > > tell me how to do this?

... I don't see how coef() helps because I have yet to figure out how
to use coef() (or any other accessor) to find " Std. Error" of the
coefficient, much less "Pr(>|t|)". summary.garch seems to have only a
print method, with no accessors at all. Can you offer a solution?

-- 
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From karloh at mi.uib.no  Wed Jun 21 18:20:32 2006
From: karloh at mi.uib.no (Karl Ove Hufthammer)
Date: Wed, 21 Jun 2006 18:20:32 +0200
Subject: [R] latex function with lm
References: <200606210935.k5L9ZWMc002927@gator.dt.uh.edu>
Message-ID: <e7brkg$bru$1@sea.gmane.org>

Erin Hodgess skreiv:

> However, I have a new question regarding that function:
> is there an option for summary(lm(y~x)), please???There?are
> options for different types of objects, but I didn't see one
> for that.??Maybe?I?just?missed?it.

FWIW: I find that the following works fairly well for printing
the table part of summary.lm:

library(Hmisc)
library(xtable)
latex(xtable(l), booktabs=TRUE, digits=2)

-- 
Karl Ove Hufthammer


From caobg at email.uc.edu  Wed Jun 21 18:22:16 2006
From: caobg at email.uc.edu (Baoqiang Cao)
Date: Wed, 21 Jun 2006 12:22:16 -0400 (EDT)
Subject: [R] help on ploting various lines
Message-ID: <20060621122216.ACQ99123@mirapoint.uc.edu>

Dear All,

I tried to plot a variety of lines(curves) on same figure. What I did is,
plot(x=x1,y=y1)
lines(x=x2,y=y2)
lines(x=x3,y=y3)
...

In my data, the maximum of y1 is much smaller than those maximums of other y vectors. So, in the figure I got, there are some curves which are not complete, I mean, they were cut off at the maximum of y1 at the y axis. Could anybody point out some right commands I need use? Thanks!

Best,
 Cao


From karloh at mi.uib.no  Wed Jun 21 18:23:13 2006
From: karloh at mi.uib.no (Karl Ove Hufthammer)
Date: Wed, 21 Jun 2006 18:23:13 +0200
Subject: [R] latex function with lm
References: <200606210935.k5L9ZWMc002927@gator.dt.uh.edu>
	<loom.20060621T120944-102@post.gmane.org>
Message-ID: <e7brph$bru$2@sea.gmane.org>

Dieter Menne skreiv:

> I have written one (also form lme) in a library called Dmisc. It passes
> CRAN tests, but as it's a bit my personal taste, I have not submitted it
> to CRAN. Main feature is that reduces show number of digits per line by
> looking at the standard deviation. Too many people loved Excel-type 12
> decimals. You may try it as starter.
> 
> http://www.menne-biomed.de/download

I believe that should be http://www.menne-biomed.de/download/download.html

-- 
Karl Ove Hufthammer


From mschwartz at mn.rr.com  Wed Jun 21 18:55:18 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 21 Jun 2006 11:55:18 -0500
Subject: [R] help on ploting various lines
In-Reply-To: <20060621122216.ACQ99123@mirapoint.uc.edu>
References: <20060621122216.ACQ99123@mirapoint.uc.edu>
Message-ID: <1150908919.7951.30.camel@localhost.localdomain>

On Wed, 2006-06-21 at 12:22 -0400, Baoqiang Cao wrote:
> Dear All,
> 
> I tried to plot a variety of lines(curves) on same figure. What I did
> is,
> plot(x=x1,y=y1)
> lines(x=x2,y=y2)
> lines(x=x3,y=y3)
> ...
> 
> In my data, the maximum of y1 is much smaller than those maximums of
> other y vectors. So, in the figure I got, there are some curves which
> are not complete, I mean, they were cut off at the maximum of y1 at
> the y axis. Could anybody point out some right commands I need use?
> Thanks!
> 
> Best,
>  Cao

You will want to use the 'xlim' and 'ylim' arguments in plot() to set
the initial axis ranges for the scatter plot based upon the ranges of
the combined x* or y* vectors. That way, the plot region ranges are set
to include all of your values.

x.range <- range(x1, x2, x3)
y.range <- range(y1, y2, y3)

plot(x1, y1, xlim = x.range, ylim = y.range)
lines(x2, y2)
lines(x3, y3)

See ?plot.default for more information.

HTH,

Marc Schwartz


From mschwartz at mn.rr.com  Wed Jun 21 18:59:51 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 21 Jun 2006 11:59:51 -0500
Subject: [R] help on ploting various lines
In-Reply-To: <1150908919.7951.30.camel@localhost.localdomain>
References: <20060621122216.ACQ99123@mirapoint.uc.edu>
	<1150908919.7951.30.camel@localhost.localdomain>
Message-ID: <1150909191.7951.36.camel@localhost.localdomain>

On Wed, 2006-06-21 at 11:55 -0500, Marc Schwartz (via MN) wrote:
> On Wed, 2006-06-21 at 12:22 -0400, Baoqiang Cao wrote:
> > Dear All,
> > 
> > I tried to plot a variety of lines(curves) on same figure. What I did
> > is,
> > plot(x=x1,y=y1)
> > lines(x=x2,y=y2)
> > lines(x=x3,y=y3)
> > ...
> > 
> > In my data, the maximum of y1 is much smaller than those maximums of
> > other y vectors. So, in the figure I got, there are some curves which
> > are not complete, I mean, they were cut off at the maximum of y1 at
> > the y axis. Could anybody point out some right commands I need use?
> > Thanks!
> > 
> > Best,
> >  Cao
> 
> You will want to use the 'xlim' and 'ylim' arguments in plot() to set
> the initial axis ranges for the scatter plot based upon the ranges of
> the combined x* or y* vectors. That way, the plot region ranges are set
> to include all of your values.
> 
> x.range <- range(x1, x2, x3)
> y.range <- range(y1, y2, y3)
> 
> plot(x1, y1, xlim = x.range, ylim = y.range)
> lines(x2, y2)
> lines(x3, y3)
> 
> See ?plot.default for more information.

One other note, which is to review:

  ?matplot

which also has help for matpoints() and matlines(), which will do the
common axis range adjustments for you. 

This will be helpful if you are going to be plotting a larger number of
points/lines in a single graphic.

HTH,

Marc


From neuro3000 at hotmail.com  Wed Jun 21 19:03:07 2006
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Wed, 21 Jun 2006 13:03:07 -0400
Subject: [R] help on ploting various lines
In-Reply-To: <20060621122216.ACQ99123@mirapoint.uc.edu>
Message-ID: <BAY112-F34D97CBAE6922864B11599AF840@phx.gbl>

Hi.  set a ylim equal to the max of your data:

#create test data
x1 <- x2 <-x3 <-(1:10)
y1 <-runif(10)/2 #to get a low maximum y1
y2 <-runif(10)
y3 <-runif(10)

#plot as you did
plot(x=x1,y=y1,type="l") #y-axis is not big enough
lines(x=x2,y=y2)
lines(x=x3,y=y3)

#plot with minimum/maximum y limit
plot(x=x1,y=y1,type="l",ylim=c(min(y1,y2,y2),max(y1,y2,y2))) #fixed
lines(x=x2,y=y2)
lines(x=x3,y=y3)

Neuro



>From: Baoqiang Cao <caobg at email.uc.edu>
>Reply-To: caobg at email.uc.edu
>To: R-help at stat.math.ethz.ch
>Subject: [R] help on ploting various lines
>Date: Wed, 21 Jun 2006 12:22:16 -0400 (EDT)
>
>Dear All,
>
>I tried to plot a variety of lines(curves) on same figure. What I did is,
>plot(x=x1,y=y1)
>lines(x=x2,y=y2)
>lines(x=x3,y=y3)
>...
>
>In my data, the maximum of y1 is much smaller than those maximums of other 
>y vectors. So, in the figure I got, there are some curves which are not 
>complete, I mean, they were cut off at the maximum of y1 at the y axis. 
>Could anybody point out some right commands I need use? Thanks!
>
>Best,
>  Cao
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html


From neuro3000 at hotmail.com  Wed Jun 21 19:05:37 2006
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Wed, 21 Jun 2006 13:05:37 -0400
Subject: [R] help on ploting various lines
In-Reply-To: <20060621122216.ACQ99123@mirapoint.uc.edu>
Message-ID: <BAY112-F159ABEA77EACA6D345F271AF840@phx.gbl>

Sorry, had two y2 in the min/max ylim

plot(x=x1,y=y1,type="l",ylim=c(min(y1,y2,y3),max(y1,y2,y3))) #fixed
lines(x=x2,y=y2)
lines(x=x3,y=y3)



>From: Baoqiang Cao <caobg at email.uc.edu>
>Reply-To: caobg at email.uc.edu
>To: R-help at stat.math.ethz.ch
>Subject: [R] help on ploting various lines
>Date: Wed, 21 Jun 2006 12:22:16 -0400 (EDT)
>
>Dear All,
>
>I tried to plot a variety of lines(curves) on same figure. What I did is,
>plot(x=x1,y=y1)
>lines(x=x2,y=y2)
>lines(x=x3,y=y3)
>...
>
>In my data, the maximum of y1 is much smaller than those maximums of other 
>y vectors. So, in the figure I got, there are some curves which are not 
>complete, I mean, they were cut off at the maximum of y1 at the y axis. 
>Could anybody point out some right commands I need use? Thanks!
>
>Best,
>  Cao
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html


From murdoch at stats.uwo.ca  Wed Jun 21 19:43:39 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 21 Jun 2006 13:43:39 -0400
Subject: [R] rank(x,y)?
Message-ID: <4499854B.8020707@stats.uwo.ca>

Suppose I have two columns, x,y.  I can use order(x,y) to calculate a 
permutation that puts them into increasing order of x,
with ties broken by y.

I'd like instead to calculate the rank of each pair under the same 
ordering, but the rank() function doesn't take multiple values
as input.  Is there a simple way to get what I want?

E.g.

 > x <- c(1,2,3,4,1,2,3,4)
 > y <- c(1,2,3,1,2,3,1,2)
 > rank(x+y/10)
[1] 1 3 6 7 2 4 5 8

gives me the answer I want, but only because I know the range of y and 
the size of gaps in the x values.  What do I do in general?

Duncan Murdoch


From jfox at mcmaster.ca  Wed Jun 21 20:01:26 2006
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 21 Jun 2006 14:01:26 -0400
Subject: [R] rank(x,y)?
In-Reply-To: <4499854B.8020707@stats.uwo.ca>
Message-ID: <web-129956658@cgpsrv2.cis.mcmaster.ca>

Dear Duncan,

How about something like the following?

rank2 <- function(x, y){
    x <- (x - min(x))/diff(range(x))
    y <- (y - min(y))/diff(range(y))
    rank(10*(x + 1) + y)
    }

Regards,
 John

On Wed, 21 Jun 2006 13:43:39 -0400
 Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> Suppose I have two columns, x,y.  I can use order(x,y) to calculate a
> 
> permutation that puts them into increasing order of x,
> with ties broken by y.
> 
> I'd like instead to calculate the rank of each pair under the same 
> ordering, but the rank() function doesn't take multiple values
> as input.  Is there a simple way to get what I want?
> 
> E.g.
> 
>  > x <- c(1,2,3,4,1,2,3,4)
>  > y <- c(1,2,3,1,2,3,1,2)
>  > rank(x+y/10)
> [1] 1 3 6 7 2 4 5 8
> 
> gives me the answer I want, but only because I know the range of y
> and 
> the size of gaps in the x values.  What do I do in general?
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/


From andy_liaw at merck.com  Wed Jun 21 20:05:56 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 21 Jun 2006 14:05:56 -0400
Subject: [R] rank(x,y)?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02700021@usctmx1106.merck.com>

This might work for you:

> order(order(x, y))
[1] 1 3 6 7 2 4 5 8

Andy 

From: Duncan Murdoch
> 
> Suppose I have two columns, x,y.  I can use order(x,y) to 
> calculate a permutation that puts them into increasing order 
> of x, with ties broken by y.
> 
> I'd like instead to calculate the rank of each pair under the 
> same ordering, but the rank() function doesn't take multiple 
> values as input.  Is there a simple way to get what I want?
> 
> E.g.
> 
>  > x <- c(1,2,3,4,1,2,3,4)
>  > y <- c(1,2,3,1,2,3,1,2)
>  > rank(x+y/10)
> [1] 1 3 6 7 2 4 5 8
> 
> gives me the answer I want, but only because I know the range 
> of y and the size of gaps in the x values.  What do I do in general?
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From p.dalgaard at biostat.ku.dk  Wed Jun 21 20:07:31 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Jun 2006 20:07:31 +0200
Subject: [R] rank(x,y)?
In-Reply-To: <4499854B.8020707@stats.uwo.ca>
References: <4499854B.8020707@stats.uwo.ca>
Message-ID: <x21wtil6vw.fsf@turmalin.kubism.ku.dk>

Duncan Murdoch <murdoch at stats.uwo.ca> writes:

> Suppose I have two columns, x,y.  I can use order(x,y) to calculate a 
> permutation that puts them into increasing order of x,
> with ties broken by y.
> 
> I'd like instead to calculate the rank of each pair under the same 
> ordering, but the rank() function doesn't take multiple values
> as input.  Is there a simple way to get what I want?
> 
> E.g.
> 
>  > x <- c(1,2,3,4,1,2,3,4)
>  > y <- c(1,2,3,1,2,3,1,2)
>  > rank(x+y/10)
> [1] 1 3 6 7 2 4 5 8
> 
> gives me the answer I want, but only because I know the range of y and 
> the size of gaps in the x values.  What do I do in general?

Still not quite general, but in the absence of ties:

> z[order(x,y)]<-1:8
> z
[1] 1 3 6 7 2 4 5 8


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From caobg at email.uc.edu  Wed Jun 21 20:11:39 2006
From: caobg at email.uc.edu (Baoqiang Cao)
Date: Wed, 21 Jun 2006 14:11:39 -0400 (EDT)
Subject: [R] help on ploting various lines
Message-ID: <20060621141139.ACR15410@mirapoint.uc.edu>

Thank you very much Marc Schwartz and Neuro LeSuperH?ros! At last I got what I expected, of course, with help of your messages. What a great day!

Best,
 Cao


From murdoch at stats.uwo.ca  Wed Jun 21 20:13:38 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 21 Jun 2006 14:13:38 -0400
Subject: [R] rank(x,y)?
In-Reply-To: <x21wtil6vw.fsf@turmalin.kubism.ku.dk>
References: <4499854B.8020707@stats.uwo.ca>
	<x21wtil6vw.fsf@turmalin.kubism.ku.dk>
Message-ID: <44998C52.7040502@stats.uwo.ca>

Peter Dalgaard wrote:
> Duncan Murdoch <murdoch at stats.uwo.ca> writes:
>
>   
>> Suppose I have two columns, x,y.  I can use order(x,y) to calculate a 
>> permutation that puts them into increasing order of x,
>> with ties broken by y.
>>
>> I'd like instead to calculate the rank of each pair under the same 
>> ordering, but the rank() function doesn't take multiple values
>> as input.  Is there a simple way to get what I want?
>>
>> E.g.
>>
>>  > x <- c(1,2,3,4,1,2,3,4)
>>  > y <- c(1,2,3,1,2,3,1,2)
>>  > rank(x+y/10)
>> [1] 1 3 6 7 2 4 5 8
>>
>> gives me the answer I want, but only because I know the range of y and 
>> the size of gaps in the x values.  What do I do in general?
>>     
>
> Still not quite general, but in the absence of ties:
>
>   
>> z[order(x,y)]<-1:8
>> z
>>     
> [1] 1 3 6 7 2 4 5 8
>
>   

Thanks to all who have replied.  Unfortunately for me, ties do exist, 
and I'd like them to get identical ranks.  John Fox's suggestion would 
handle ties properly, but I'm worried about rounding error giving 
spurious ties.

Duncan Murdoch


From elvis at xlsolutions-corp.com  Wed Jun 21 20:23:18 2006
From: elvis at xlsolutions-corp.com (elvis at xlsolutions-corp.com)
Date: Wed, 21 Jun 2006 11:23:18 -0700
Subject: [R] Courses - R/Splus Advanced Programming in San Francisco This
	July ***by the R Development Core Tean Guru
Message-ID: <20060621112318.9f08cc34deb45d78e54b3b5664e21546.f2dc99a508.wbe@email.secureserver.net>


From ggrothendieck at gmail.com  Wed Jun 21 20:34:36 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 21 Jun 2006 14:34:36 -0400
Subject: [R] rank(x,y)?
In-Reply-To: <4499854B.8020707@stats.uwo.ca>
References: <4499854B.8020707@stats.uwo.ca>
Message-ID: <971536df0606211134n125db2c4y89b958ebd281c647@mail.gmail.com>

Try this:

order(order(x,y))


On 6/21/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> Suppose I have two columns, x,y.  I can use order(x,y) to calculate a
> permutation that puts them into increasing order of x,
> with ties broken by y.
>
> I'd like instead to calculate the rank of each pair under the same
> ordering, but the rank() function doesn't take multiple values
> as input.  Is there a simple way to get what I want?
>
> E.g.
>
>  > x <- c(1,2,3,4,1,2,3,4)
>  > y <- c(1,2,3,1,2,3,1,2)
>  > rank(x+y/10)
> [1] 1 3 6 7 2 4 5 8
>
> gives me the answer I want, but only because I know the range of y and
> the size of gaps in the x values.  What do I do in general?
>
> Duncan Murdoch
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From bren at juanantonio.info  Wed Jun 21 20:57:34 2006
From: bren at juanantonio.info (bren at juanantonio.info)
Date: Wed, 21 Jun 2006 20:57:34 +0200
Subject: [R] png() or jpeg() in a php script
In-Reply-To: <mailman.11.1150884004.12069.r-help@stat.math.ethz.ch>
References: <mailman.11.1150884004.12069.r-help@stat.math.ethz.ch>
Message-ID: <20060621185734.6005.qmail@correo4.acens.net>

Hi, I have created a OOP version of R_PHP Project, developed by Steve Chen, 
you can download the php script, here: 
http://www.juanantonio.info/p_research/statistics/r/rphp/RClass.1.0.zip 

<?php
class R{ 

	//Configuration Variables
	var $temp_dir;
	var $R_path;
	var $R_options_1;
	var $R_options_2;
	var $graphic;
	var $bannedCommandConfigFile;
	var $RCODE;
	var $Rerror; 

	//Constructor de clase
	function __construct(){
		$this -> temp_dir = "tmp";
		$this -> R_path = "c:/R-2.3.0/bin/Rterm.exe";
		$this -> R_options_1 = "--quiet --no-restore --no-save  < ";
		$this -> R_options_2 = " > ";
		$this -> graphic = "jpeg";
		$this -> bannedCommandConfigFile = "security.txt";
	} 

	//Metodo para establecer valores en propiedades
	function setProp($PropName, $PropValue){
		$this->$PropName = $PropValue;
	} 

	//Funcion que comprueba el fichero de configuracion
	function checkBCCFile(){
		
		$lines = file($this -> bannedCommandConfigFile);
		//$lines = file("security.txt");
		$total = count($lines); 

		$j = 0;
		for ($i=0;$i < $total;$i++){
 			$line = trim($lines[$i]); 

			if (!strrchr($line,"#")){
   			$j = $j + 1;
				if (strrchr($line,"|")){
     				$terms = explode("|",$line);
     				$bad_0 = trim($terms[0]);
     				$bad_op = trim($terms[1]);
					$bad_cmd[$j]= $bad_0;
					$bad_option[$bad_0] = $bad_op;
				}else{
					$bad_cmd[$j]= $line;
 				}
			}else{
   			continue;
			}
		}
		
		echo("Comprobando el fichero de configuracion<br />");
	}//End Function
	
	//Funcion que comprueba cada linea del codigo R dado, por si emplea un  
commando prohibido.
	function check_bad($text,$j){
		global $bad_cmd,$bad_option;
		$is_bad = 0; 

		foreach($bad_cmd as $bad){
			$bad1 = str_replace(".","\.",$bad); 

			if(ereg($bad1,$text)){
     			if(strrchr($bad,".") && (strlen($bad) > 3)){
       			$is_bad = 1;
     			}else{
			        // get remaining string after targer key word
			        $terms = explode($bad,$text);
			        // get rid of spaces before a possible following "("
			        $term1 = ltrim($terms[1]);
			
			        if($bad_option[$bad] != ""){
						if(eregi($bad_option[$bad],$term1)){
							// if (strstr($term1,$bad_option[$bad]))
			             	$is_bad = 1;
						}
			        }else{
						if(substr($term1,0,1) == "("){
							$is_bad = 1;
						}
			        }
     			} 

				if($is_bad == 1){
					if ($bad_option[$bad] != ""){
						echo "<font color=red>$bad</font> function";
						echo " with <font color=red>".$bad_option[$bad]."</font>";
						echo " option is NOT permitted ";
		         	}else{
						echo "<font color=red>$bad</font> function is NOT permitted ";
						echo "in Line <font color=red>$j</font><BR>";
						echo "<blockquote>$text</blockquote>";
						echo "Program Stopped!<BR>";
						exit;
					}
				}
		 	}
		}
	}//End function 

	//Funcion que genera un nombre aleatorio a ficheros
	function random_str($size){
		$randoms = array(
               0, 1, 2, 3, 4, 5, 6, 7, 8, 9, a, b,
               c, d, e, f, g, h, i, j, k, l, m, n,
               o, p, q, r, s, t, u, v, w, x, y, z
		); 

		srand ((double) microtime() * 1000000); 

		for($i = 1; $i <= $size; $i++){
			$text .= $randoms[(rand(0,35))];
       } 

		return $text;
	}//End function 

	//Funcion que obtiene un nombre de fichero
	function get_file_name($text){
		// Unix   : bitmap(file="something.jpg")
		// Windows: jpeg(file="something.jpg") 

		$temp1 = explode("file=\"",$text);
		$fname = explode("\"",$temp1[1]); 

		return $fname[0];
	}//End function 


	function createRInputFile($r_code){
	
		//Obtiene informacion sobre el codigo a ejecutar y lo parte en lineas
		$old_code = explode(chr(10),$r_code);
		$total = count($old_code);
		$new_code = "";
	
		for($i=0;$i < $total; $i++){
			// replace original graphic file name with a random name
			// Windows system if (ereg("jpeg",$old_code[$i])) 

			// if (ereg("bitmap",$old_code[$i])) 

			$j = $i+1;
			$old = $old_code[$i];
			//echo("Linea ".$i."codigo ".$old."<br />");
			
			//Mejorar esto
			//Comprueba si emplea un codigo incorrecto.
			//$this-> check_bad($old,$j); 

			if(ereg($this -> graphic,$old_code[$i])){
			 	$gfile_name = $this-> get_file_name($old_code[$i]);
			 	$gfile_name = $this-> random_str(4).$gfile_name;
			
			 	//$new_code .= "bitmap(file=\"$temp_dir/$gfile_name\") \n";
				//$new_code .= "#ff;";
				
				$tmp = $this ->temp_dir;
				$new_code .= $this ->graphic."(file=\"tmp/$gfile_name\") \n";
			}else{
				$new_code .= $old_code[$i]."\n";
			}
		}
	
		//echo($new_code);
	
		$this -> r_name = $this-> random_str(10);
		$this -> r_input = $this-> temp_dir."/".$this -> r_name.".R";
		$this -> r_output = $this-> temp_dir."/".$this -> r_name.".Rout";
	
		$fp = fopen($this -> r_input,"w");
		fwrite($fp,$new_code);
		fclose($fp);
		
		echo("Fichero R generado <br />");
} 

function execution(){ 

	try{
		// $rsoft = "/usr/local/lib/R/bin/R";
		$rsoft = $this-> R_path;
	
		// Unix :
		//    R BATCH --slave --no-save $r_input $r_output
		//
		// Windows :
		//    Rterm.exe --quiet --no-restore --no-save < test.R > test.Rout
		
		// $command = "$rsoft BATCH --slave --no-save $r_input $r_output"; 

		$command = $this-> R_path." ".$this -> R_options_1." ".$this -> r_input." 
".$this -> R_options_2." ".$this -> r_output;
		//echo($command);
		
		$result = "";
		$error = "";
	
		$exec_result = exec($command,$result,$error);
		
		echo("R CONSOLE COMMAND: ".$command."<br />");
		
		/*
		if($error){
			$this -> Rerror = true;
		}
		*/
	
	}catch(Exception $e){
		echo "ERROR ". $e -> getCode() .
		"en la linea:" . $e -> getLine() .
		":" . $e-> getMessage();
		
		$this -> Rerror = true;
	}
} 

function showResultsRCode(){
	$lines = file($this -> r_output);
$total = count($lines); 

if ($this -> Rerror){
 echo "<font color=red>Error: Something wrong! Please check output!</font>";
 echo "<HR>Output of R program : <P><HR>"; 

	for ($i=0;$i < $total;$i++){
   	echo $lines[$i]."<BR>";
		exit;
	}
} 

echo "Output of R program<HR>"; 

	$to_do_plot = 0;
	
	for ($i=0;$i < $total;$i++){ 

		$line = $lines[$i];
	
		// Unix   : if (ereg("bitmap",$line))
		// Windows: if (ereg("jpeg",$line))
	
		if (ereg($this-> graphic,$line)){
	    	echo $line."<BR>";
	
			$gfile_name = $this -> get_file_name($line);
			$to_do_plot = 1;
			//echo "<P><IMG SRC=\"$file_name\"><P>";
		}else if (ereg("dev.off",$line)){
			echo $line."<BR>";
	
			if ($to_do_plot == 1){
				echo "<P><IMG SRC=\"$gfile_name\"><P>";
				$to_do_plot = 0;
		    }
		}else if (ereg("null device",$line)){
	    	continue;
		}else if (ereg("          1",$line)){
	    	continue;
		}else{
	    	echo $line."<BR>";
		}
	}
}//End function 

function executeRCode($r_code){
	
	echo("SOURCE CODE: ".$r_code."<br />");
	
	$this -> checkBCCFile();
	$this -> createRInputFile($r_code);
	$this -> execution();
	$this -> showResultsRCode();
}//End function 

}//End Class 


?> 

An example to show how to execute RClass.php: 

<?php 

require("RClass.php"); 

//Instancia de clase;
$rObject = new R();
$rObject -> executeRCode($r_code); 


?> 


I use jpeg in the script. 

> 
> Message: 90
> Date: Wed, 21 Jun 2006 11:46:27 +0200
> From: "Angel Roman" <aroman6 en gmail.com>
> Subject: [R] png() or jpeg() in a php script
> To: r-help en stat.math.ethz.ch
> Message-ID:
> 	<f452beeb0606210246n7d484937o3467e4b7030e03c1 en mail.gmail.com>
> Content-Type: text/plain 
> 
> Hello, 
> 
> I've got a problem with a PHP script, in which I call the system function
> (to call another processes). The call is : 
> 
> system("R --slave --vanilla < path/to/source/source.R"); 
> 
> if in this R file, I've got the lines:
>   pdf(file="/path/to/file/file.pdf")
>   commands
>   dev.off() 
> 
> the pdf file is perfectly created 
> 
> but if I change the lines to: 
> 
>   png(file="/path/to/file/file.png");
>   commands
>   dev.off() 
> 
> (or with the jpeg() function) 
> 
> the files are not created. any help? 
> 
> Thanks 
> 
> 	[[alternative HTML version deleted]] 
> 
>  
> 
> ------------------------------ 
> 
> _______________________________________________
> R-help en stat.math.ethz.ch mailing list  
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE read the posting guide! http://www.R-project.org/posting-guide.html 
> 
> 
> End of R-help Digest, Vol 40, Issue 21
> **************************************
 


Juan Antonio Bre?a Moral.
Advanced Marketing Ph.D. , URJC, Spain (Now)
Industrial Organisation Engineering, ICAI, Spain.
Technical Computer Programming Engineering, ICAI, Spain
Web: http://www.juanantonio.info
Mobile: +34 655970320


From mail at friedrich-schuster.de  Wed Jun 21 21:21:45 2006
From: mail at friedrich-schuster.de (Friedrich Schuster)
Date: Wed, 21 Jun 2006 21:21:45 +0200
Subject: [R] Eclipse plugin for R code submit to Rserve available (Alpha)
Message-ID: <200606212121.45524.mail@friedrich-schuster.de>


Hello R-users and developers,

This is the announce of a new Eclipse plugin for R.

It is in its "early" (alpha) stage. It's GPL, source code is provided with the 
plugin. 
The update site is ready for install with eclipse 3.1 at :
http://www.alysis.de/Rsubmit/update/ 

If you're interested, please read the feature list below or go to 
http://www.alysis.de/Rsubmit/

Thanks, 
Friedrich Schuster (mail_at_friedrich-schuster.de)

----------

Introduction

The Rsubmit plugin executes selected R source code from within an editor 
window. The code is passed to a (local or remote) Rserve session and the 
output is streamed back to eclipse.

Features

    * Submit of selected code segments in opened files or whole text (if 
nothing is selected) from an editor
    * Connects to local Rserve sessions or Rserve on a remote host.
    * Supports different configurations for code submit
    * Keeps workspace elements, supports incremental code submit
    * Is able to displays output from a local Rserve session in console window
    * Is (partially) able to display output from a remote Rserve session in 
console window
    * Output of remote sessions is (or should be) displayed incremental and in 
(near) real-time. Well, at least it worked in the runtime workbench. The 
writing to the eclipse console window (in "normal" eclipse mode) seems to be 
buffered, so there is not always a immediate reacton to new R output.

Requires...

    * eclipse 3.1 (3.0 does / should not work)
    * An R installation. Get it from: R Project for Statistical Computing 
(http://www.r-project.org/)
    * Installed Rserve (http://stats.math.uni-augsburg.de/Rserve/)
    * Please start the Rserve session before connection from the plugin! 
(Currently the connection will not restart after an error).
    * There seems to be an issue with JDK 1.5. It works with 1.4.2. Please try 
and report errors to me.

-- 

Friedrich Schuster
mail_at_friedrich-schuster.de


From btyner at gmail.com  Wed Jun 21 21:20:57 2006
From: btyner at gmail.com (Benjamin Tyner)
Date: Wed, 21 Jun 2006 15:20:57 -0400
Subject: [R] xyplot, type="b"
In-Reply-To: <4498CA6F.6050504@pdf.com>
References: <4498B6D9.2050004@stat.purdue.edu>	<38b9f0350606202016y1ded6022ne0e3ecde374780d3@mail.gmail.com>	<4498BB1D.5000306@stat.purdue.edu>	<4498BE6F.8060001@stat.auckland.ac.nz>
	<4498C426.7040804@stat.purdue.edu> <4498CA6F.6050504@pdf.com>
Message-ID: <44999C19.9090907@stat.purdue.edu>

Yes, this does work as long as the data are spaced far enough apart. If
you peek inside ./main/plot.c, you will find the function 'do_plot_xy'
which provides the functionality I desire for standard graphics. A
down-and-dirty implementation for lattice might be along the lines of

my.panel <- function(x, y, ...){
panel.xyplot(x, y, ...)
d <- 0.5 # for now, should be half the size of 'cex'
xold <- NA
yold <- NA
for(i in 1:length(x)){
xx <- x[i]
yy <- y[i]
f <- d / sqrt(xx * xx + yy * yy)
if(f < 0.5){
# just a kludge for now, would really want to use grid.lines or some such
panel.lines(x = c(xold + f * (xx - xold), xx + f * (xold - xx)),
y = c(yold + f * (yy - yold), yy + f * (yold - yy)))
}
xold <- xx
yold <- yy
}
}

Unfortunately this is not quite right; to do it correctly it seems one
has to address two problems:

1. how to get the size of the default 'cex' to use for 'd' (do_xy_plot uses 'GConvertYUnits' to accomplish this)
2. figure out how to achieve the same effect as what 'GConvert(&xx, &yy, USER, INCHES, dd)' does in do_xy_plot. Otherwise, the gap sizes are not constant.

(1) sounds easy but I don't know the answer offhand. (2) seems more subtle. Any suggestions would be greatly appreciated.

Ben


Sundar Dorai-Raj wrote:

>I think Paul's suggestion works if you use:
>
>  panel.points(x, y, col = "white", cex = 1.5, pch = 16, ...)
>
>instead of the default background color. For me
>trellis.par.get("background")$col returns "transparent".
>
>HTH,
>
>--sundar
>
>  
>


From sw283 at maths.bath.ac.uk  Wed Jun 21 21:39:20 2006
From: sw283 at maths.bath.ac.uk (Simon Wood)
Date: Wed, 21 Jun 2006 20:39:20 +0100 (BST)
Subject: [R] Comparing partial response curves from GAM
In-Reply-To: <200606201908.k5KJ81ft009150@mx1.ucdavis.edu>
References: <200606201908.k5KJ81ft009150@mx1.ucdavis.edu>
Message-ID: <Pine.LNX.4.64.0606212037260.14477@archer.maths.bath.ac.uk>

Can you give a bit more information on exactly what you want to compare? 
(maybe give example models). There are ways of comparing curves, but it is 
a bit context dependent.

>- Simon Wood, Mathematical Sciences, University of Bath, Bath BA2 7AY
>-             +44 (0)1225 386603         www.maths.bath.ac.uk/~sw283/


On Tue, 20 Jun 2006, Solomon Dobrowski wrote:

> Hello all, I was wondering if anyone is aware of formal approaches and tools
> for comparing partial response curves produced in GAM? My interest is in
> determining if two partial response curves are "statistically" different. I
> recognize that point-wise standard error estimates can be produced using the
> GAM package but Im not certain how to translate this into a statistical test
> for the entire curve. Any feedback is much appreciated. Thanks. Solomon
>
> Solomon Dobrowski
> Tahoe Environmental Research Center (TERC)
> John Muir Institute of the Environment
> University of California, Davis
> 530 754 9354
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From sw283 at maths.bath.ac.uk  Wed Jun 21 21:54:56 2006
From: sw283 at maths.bath.ac.uk (Simon Wood)
Date: Wed, 21 Jun 2006 20:54:56 +0100 (BST)
Subject: [R] gamm error message
In-Reply-To: <7.0.0.16.0.20060606232236.01bc9470@highstat.com>
References: <7.0.0.16.0.20060606232236.01bc9470@highstat.com>
Message-ID: <Pine.LNX.4.64.0606212044310.14477@archer.maths.bath.ac.uk>

This is a bug. The `prior.weights' element of the the faked `gam' object 
(i.e. `test$gam$prior.wieghts' below) has been set to the varIdent() 
variance function, rather than the weights that this eventually 
represents. I'll fix this for the next patch release, (as soon as I get 
any time to do research rather than doing dummy runs of describing how 
good the research might be that I would be doing if I was doing it instead 
of practicing writing about it).

In the meantime if you do something like:
   test$gam$prior.weights <- rep(1,length(Z))
then
   summary(test.gam)
will work, but you should ignore the reported r^2.

best,
Simon

>- Simon Wood, Mathematical Sciences, University of Bath, Bath BA2 7AY 
>-             +44 (0)1225 386603         www.maths.bath.ac.uk/~sw283/


On Tue, 6 Jun 2006, Highland Statistics Ltd. wrote:

> Hello,
>
> Why would I get an error message  with the following code for gamm? I
> want to fit the a gam with different variances per stratum.
>
>
> library(mgcv)
> library(nlme)
>
> Y<-rnorm(100)
> X<-rnorm(100,sd=2)
> Z<-rep(c(T,F),each=50)
>
> test<-gamm(Y~s(X),weights=varIdent(form=~1|Z))
>
> summary(test$lme)   #ok
>
> summary(test$gam)
>
> Gives an error message:
> Error in inherits(x, "data.frame") : dim<- : dims [product 100] do
> not match the length of object [0]
>
>
> All the other output seems to be ok:
>
>
> > summary(test$lme)
> Linear mixed-effects model fit by maximum likelihood
>  Data: strip.offset(mf)
>        AIC      BIC    logLik
>   299.9468 312.9727 -144.9734
>
> Random effects:
>  Formula: ~Xr.1 - 1 | g.1
>  Structure: pdIdnot
>                Xr.11        Xr.12        Xr.13        Xr.14        Xr.15
> StdDev: 0.0001073404 0.0001073404 0.0001073404 0.0001073404 0.0001073404
>                Xr.16        Xr.17        Xr.18 Residual
> StdDev: 0.0001073404 0.0001073404 0.0001073404 1.045916
>
> Variance function:
>  Structure: Different standard deviations per stratum
>  Formula: ~1 | Z
>  Parameter estimates:
>      TRUE     FALSE
> 1.0000000 0.9721987
> Fixed effects: y ~ X.0 - 1
>                      Value Std.Error DF    t-value p-value
> X.0(Intercept)  0.04426102 0.1041540 98  0.4249573  0.6718
> X.0s(X)Fx1     -0.01053900 0.1039558 98 -0.1013796  0.9195
>  Correlation:
>            X.0(I)
> X.0s(X)Fx1 0.002
>
> Standardized Within-Group Residuals:
>         Min          Q1         Med          Q3         Max
> -1.96955692 -0.81101014  0.07004034  0.72528662  2.44244302
>
> Number of Observations: 100
> Number of Groups: 1
>
>
>
>
>
> Kind regards,
>
> Alain
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From John.Kerpel at infores.com  Wed Jun 21 21:50:06 2006
From: John.Kerpel at infores.com (Kerpel, John)
Date: Wed, 21 Jun 2006 14:50:06 -0500
Subject: [R] colClasses
Message-ID: <44A8B25381923D4F93B74B2676A50F6D02A5588F@MAIL1.infores.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060621/770eb520/attachment.pl 

From p.dalgaard at biostat.ku.dk  Wed Jun 21 22:04:25 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Jun 2006 22:04:25 +0200
Subject: [R] colClasses
In-Reply-To: <44A8B25381923D4F93B74B2676A50F6D02A5588F@MAIL1.infores.com>
References: <44A8B25381923D4F93B74B2676A50F6D02A5588F@MAIL1.infores.com>
Message-ID: <x2wtbajmwm.fsf@turmalin.kubism.ku.dk>

"Kerpel, John" <John.Kerpel at infores.com> writes:

> Hi Folks!
> 
>  
> 
> I'm reading in some data from a .csv file that has a date column.
> 
>  
> 
> How do I use colClasses to get read.csv to recognize the date column?
> The documentation on this seems to be nil - 
> 
> And yes, I've read help and R Data Import/Export and can't figure out
> what the colClasses syntax is.

1. It may be easier just to read the date field as factor or character
   and convert using as.Date afterwards. If a format is needed, this
   is actually the only way.

2. colClasses is a character vector, possibly named, so

   cls <- rep(NA,17)
   cls[3] <- "Date"
   read.csv(...., colClasses=cls, ....)

   or

   read.csv(...., colClasses=c(mydate="Date"), ....)

   should work (assuming that the variable in question is called
   "mydate" and is in the 3 of 17 columns). This is documented in
   ?read.csv.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From John.Kerpel at infores.com  Wed Jun 21 22:09:56 2006
From: John.Kerpel at infores.com (Kerpel, John)
Date: Wed, 21 Jun 2006 15:09:56 -0500
Subject: [R] colClasses
Message-ID: <44A8B25381923D4F93B74B2676A50F6D02A558E6@MAIL1.infores.com>

Peter:

Thanks - I'll try both approaches.  The examples clear things up a bit.

John

-----Original Message-----
From: pd at pubhealth.ku.dk [mailto:pd at pubhealth.ku.dk] On Behalf Of Peter Dalgaard
Sent: Wednesday, June 21, 2006 3:04 PM
To: Kerpel, John
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] colClasses

"Kerpel, John" <John.Kerpel at infores.com> writes:

> Hi Folks!
> 
>  
> 
> I'm reading in some data from a .csv file that has a date column.
> 
>  
> 
> How do I use colClasses to get read.csv to recognize the date column?
> The documentation on this seems to be nil - 
> 
> And yes, I've read help and R Data Import/Export and can't figure out
> what the colClasses syntax is.

1. It may be easier just to read the date field as factor or character
   and convert using as.Date afterwards. If a format is needed, this
   is actually the only way.

2. colClasses is a character vector, possibly named, so

   cls <- rep(NA,17)
   cls[3] <- "Date"
   read.csv(...., colClasses=cls, ....)

   or

   read.csv(...., colClasses=c(mydate="Date"), ....)

   should work (assuming that the variable in question is called
   "mydate" and is in the 3 of 17 columns). This is documented in
   ?read.csv.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From jholtman at gmail.com  Wed Jun 21 22:12:12 2006
From: jholtman at gmail.com (jim holtman)
Date: Wed, 21 Jun 2006 16:12:12 -0400
Subject: [R] colClasses
In-Reply-To: <44A8B25381923D4F93B74B2676A50F6D02A5588F@MAIL1.infores.com>
References: <44A8B25381923D4F93B74B2676A50F6D02A5588F@MAIL1.infores.com>
Message-ID: <644e1f320606211312o4357f9dch13b0e112846df6b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060621/2f75a315/attachment.pl 

From ggrothendieck at gmail.com  Wed Jun 21 22:14:46 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 21 Jun 2006 16:14:46 -0400
Subject: [R] colClasses
In-Reply-To: <44A8B25381923D4F93B74B2676A50F6D02A5588F@MAIL1.infores.com>
References: <44A8B25381923D4F93B74B2676A50F6D02A5588F@MAIL1.infores.com>
Message-ID: <971536df0606211314k1dfa4c02k25d43722810993d9@mail.gmail.com>

Peter has already directly answered your question but in case
what you really want is a zoo time series object (applicable
if this is a time series) then see the example in ?read.zoo in the
zoo package.  Also,

  library(zoo); vignette("zoo")

gives info on zoo package.

On 6/21/06, Kerpel, John <John.Kerpel at infores.com> wrote:
> Hi Folks!
>
>
>
> I'm reading in some data from a .csv file that has a date column.
>
>
>
> How do I use colClasses to get read.csv to recognize the date column?
> The documentation on this seems to be nil -
>
> And yes, I've read help and R Data Import/Export and can't figure out
> what the colClasses syntax is.
>
>
>
> Thanks,
>
>
>
> john
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From John.Kerpel at infores.com  Wed Jun 21 22:20:19 2006
From: John.Kerpel at infores.com (Kerpel, John)
Date: Wed, 21 Jun 2006 15:20:19 -0500
Subject: [R] colClasses
Message-ID: <44A8B25381923D4F93B74B2676A50F6D02A5591A@MAIL1.infores.com>

Read.zoo did it!  Thanks!

-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Sent: Wednesday, June 21, 2006 3:15 PM
To: Kerpel, John
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] colClasses

Peter has already directly answered your question but in case
what you really want is a zoo time series object (applicable
if this is a time series) then see the example in ?read.zoo in the
zoo package.  Also,

  library(zoo); vignette("zoo")

gives info on zoo package.

On 6/21/06, Kerpel, John <John.Kerpel at infores.com> wrote:
> Hi Folks!
>
>
>
> I'm reading in some data from a .csv file that has a date column.
>
>
>
> How do I use colClasses to get read.csv to recognize the date column?
> The documentation on this seems to be nil -
>
> And yes, I've read help and R Data Import/Export and can't figure out
> what the colClasses syntax is.
>
>
>
> Thanks,
>
>
>
> john
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>


From JeeBee at troefpunt.nl  Wed Jun 21 23:52:52 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Wed, 21 Jun 2006 23:52:52 +0200
Subject: [R] Some R-Tcl/Tk-BWidget newbie questions.
Message-ID: <pan.2006.06.21.21.52.51.870565@troefpunt.nl>


Dear list,

Could somebody who is more experienced with the Tcl/Tk interface from R
please help me clarify the issues I've put below with ### --> tags?

Several things go wrong, and it's probably because of messy code, but I
have a difficult time finding out what is the cause.

Thanks very much,
JeeBee.


require(tcltk) || stop("Package tcltk is not available.")
# Add path to BWidgets
addTclPath(".")
version.BWidget <<- tclvalue(tclRequire("BWidget"))

start.gui <- function() {
  # Toplevel
  tt <- tktoplevel()
  tkwm.title(tt, "MyTitle")

  # Notebook
### --> The following line makes R hang with 100% cpu forever
  #tkpack(tn <- tkwidget(tt, "NoteBook"), fill="both", expand=TRUE)
### --> This works, but not sticky="news",
### --> the notebook does not resize together with the window
  tn <- tkwidget(tt, "NoteBook")
  tkgrid(tn, sticky="news")

### --> I guess there is a simpler way to do this ...
  tbn <- tclvalue(tkinsert(tn, "end", "Algemeen", "-text", "Algemeen"))
  tkpack(tbw.algemeen <- .Tk.newwin(tbn))
  tkpack(tab.algemeen <- tkframe(tbw.algemeen))
  tbn <- tclvalue(tkinsert(tn, "end", "Relaties", "-text", "Relaties"))
  tkpack(tbw.relaties <- .Tk.newwin(tbn))
  tkpack(tab.relaties <- tkframe(tbw.relaties))
  tbn <- tclvalue(tkinsert(tn, "end", "Posten", "-text", "Posten"))
  tkpack(tbw.posten <- .Tk.newwin(tbn))
  tkpack(tab.posten <- tkframe(tbw.posten))
  tbn <- tclvalue(tkinsert(tn, "end", "Instellingen", "-text", "Instellingen"))
  tkpack(tbw.instellingen <- .Tk.newwin(tbn))
  tkpack(tab.instellingen <- tkframe(tbw.instellingen))

  gui.maak.tab(tab.algemeen)
  gui.maak.tab(tab.relaties)
  gui.maak.tab(tab.posten)
  gui.maak.tab(tab.instellingen)

  tkgrid(tkbutton(tt, text="GUI bijwerken", 
    command=function() verwerk.button("GUIBijwerken", tt)))

  # FIXME: add cleanup handler?
  #tkbind(tt, "<Destroy>", function() tkdestroy(tn))

### --> Don't understand this ... if I do the last "raise" only,
### --> then I see all tabs in the first tab, until I go to another
### --> tab and back. With all these "raises", it works
  tcl(tn, "raise", "Instellingen")
  tcl(tn, "raise", "Posten")
  tcl(tn, "raise", "Relaties")
  tcl(tn, "raise", "Algemeen")
}

gui.maak.tab <- function(tt) {
  tkgrid(tklabel(tt, text="Instellingen"), sticky="news")
  test.button <- tclVar("one two") 
  tkgrid(tkcheckbutton(tt,variable=test.button))
}


From m.bridgman at sbcglobal.net  Wed Jun 21 23:01:35 2006
From: m.bridgman at sbcglobal.net (Matthew Bridgman)
Date: Wed, 21 Jun 2006 14:01:35 -0700
Subject: [R] effect size
Message-ID: <71e96256eb94bd24fee8fd0b61f3f5c2@sbcglobal.net>

Does anyone know a simple way of calculating effect sizes?

Thanks
MB


From f.harrell at vanderbilt.edu  Thu Jun 22 00:23:22 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 21 Jun 2006 17:23:22 -0500
Subject: [R] effect size
In-Reply-To: <71e96256eb94bd24fee8fd0b61f3f5c2@sbcglobal.net>
References: <71e96256eb94bd24fee8fd0b61f3f5c2@sbcglobal.net>
Message-ID: <4499C6DA.1010204@vanderbilt.edu>

Matthew Bridgman wrote:
> Does anyone know a simple way of calculating effect sizes?
> 
> Thanks
> MB

Yes - the following formula is simple and fairly universal:

2

:-)
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From bruno.giordano at music.mcgill.ca  Thu Jun 22 00:36:13 2006
From: bruno.giordano at music.mcgill.ca (Bruno L. Giordano)
Date: Wed, 21 Jun 2006 18:36:13 -0400
Subject: [R] effect size
References: <71e96256eb94bd24fee8fd0b61f3f5c2@sbcglobal.net>
Message-ID: <005701c69583$22749d40$6400a8c0@brungio>

For ANOVA one option is the partial eta squared \eta^2_p:

\eta^2_p=SSeffect/(SSeffect+SSerror)

For multiple regression (continuous predictors) you might use the
standardized parameter estimate, the regression coefficient you obtain
standardizing the predictor: the larger the absolute value, the larger the
size of the effect.

You might take a look at:

S. Olejnik and J. Algina, (2003), Generalized eta and omega squared
statistics: measures of effect size for some common research designs,
Psychol Methods. 8(4):434-47.

and, if you have repeated measures:

R. Bakeman (2005), Recommended effect size statistics for repeated measures
designs, Behavior Research Methods, 37 (3), 379-384.

    Bruno


----- Original Message ----- 
From: "Matthew Bridgman" <m.bridgman at sbcglobal.net>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, June 21, 2006 5:01 PM
Subject: [R] effect size


> Does anyone know a simple way of calculating effect sizes?
>
> Thanks
> MB
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


From cbarker1 at scius.jnj.com  Thu Jun 22 00:44:21 2006
From: cbarker1 at scius.jnj.com (Barker, Chris [SCIUS])
Date: Wed, 21 Jun 2006 18:44:21 -0400
Subject: [R] eliminating a do loop
Message-ID: <56484410CF26D747A645BE746C57CE6D021A8D89@SCIUSFREXS3.na.jnj.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060621/92d22b5d/attachment.pl 

From rab45+ at pitt.edu  Thu Jun 22 01:01:50 2006
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Wed, 21 Jun 2006 19:01:50 -0400
Subject: [R] lmer and mixed effects logistic regression
In-Reply-To: <44996733.9050609@pdf.com>
References: <1150293574.3416.11.camel@localhost.localdomain>
	<449431F1.6000906@pdf.com>
	<40e66e0b0606180458p177fa824j6a181fd52c3d1277@mail.gmail.com>
	<1150726454.3200.20.camel@localhost.localdomain>
	<148ed8180606201127p730e699emb9e901b09a7ab039@mail.gmail.com>
	<1150894770.3329.13.camel@localhost.localdomain>
	<44996733.9050609@pdf.com>
Message-ID: <1150930911.3329.42.camel@localhost.localdomain>

On Wed, 2006-06-21 at 08:35 -0700, Spencer Graves wrote:
> 	  You could think of 'lmer(..., family=binomial)' as doing a separate 
> "glm" fit for each subject, with some shrinkage provided by the assumed 
> distribution of the random effect parameters for each subject.  Since 
> your data are constant within subject, the intercept in your model 
> without the subject's random effect distribution will be estimated at 
> +/-Inf.  Since this occurs for all subjects, the maximum likelihood 
> estimate of the subject variance is Inf, which is what I wrote in an 
> earlier contribution to this thread.
> 
> 	  What kind of answer do you get from SAS NLMIXED?  If it does NOT tell 
> you that there is something strange about the estimation problem you've 
> given it, I would call that a serious infelicity in the code.  If it is 
> documented behavior, some might argue that it doesn't deserve the "B" 
> word ("Bug").  The warning messages issued by 'lmer' in this case are 
> something I think users would want, even if they are cryptic.
> 
> 	  Hope this helps.
> 	  Spencer Graves	
> 
I did send in an example with data set that duplicates the problem.
Changing the control parameters allowed lmer to produce what seem like
reasonable estimates. Even for the case with essentially duplicate
pairs, lmer and NLMIXED produce similar estimates (finite intercepts
also) although lmer's coefficient estimates are as far as I can tell the
same as glm but the standard errors are larger.

The problem I really want estimates for is different from this one
explanatory factor example.  The model I estimate will have several
explanatory factors, including factors that differ within each subject
(although the responses within each subject are the same). BTW, as far
as I know, the responses could be different within a subject but it
seems to be very rare.


Possibly the example I thought I sent never made it to the list. The
example is below.

Rick B.

###########################################################################
# Example of lmer error message


I made an example data set that exhibits the error. There is a dump of
the data frame at the end.

First, I updated all my packages:

> sessionInfo()
Version 2.3.1 (2006-06-01)
i686-redhat-linux-gnu

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"
"datasets"
[7] "base"

other attached packages:
     chron       lme4     Matrix    lattice
   "2.3-3"  "0.995-2" "0.995-11"   "0.13-8"

But I still get the error.

For comparison, here is what glm gives:


> summary(glm(y~x,data=example.df,family=binomial))

Call:
glm(formula = y ~ x, family = binomial, data = example.df)

Deviance Residuals:
    Min       1Q   Median       3Q      Max
-1.6747  -0.9087  -0.6125   1.1447   2.0017

Coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  -0.4786     0.1227  -3.901 9.59e-05 ***
x             0.7951     0.1311   6.067 1.31e-09 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 436.63  on 324  degrees of freedom
Residual deviance: 394.15  on 323  degrees of freedom
AIC: 398.15

Number of Fisher Scoring iterations: 4


Running lmer without any tweaks:

> (lmer(y~(1|id)+x,data=example.df,family=binomial))
Error in lmer(y ~ (1 | id) + x, data = example.df, family = binomial) :
        Leading minor of order 2 in downdated X'X is not positive
definite
In addition: Warning message:
nlminb returned message singular convergence (7)
 in: LMEopt(x = mer, value = cv)

Running lmer with list(msVerbose=TRUE):

> (lmer(y~(1|
id)+x,data=example.df,family=binomial,control=list(msVerbose=TRUE)))
  0     -545.002:  44801.6
  1     -545.002:  44801.6
  2     -545.002:  44801.6
  3     -545.003:  44801.9
  4     -545.014:  44805.2
  5     -545.123:  44838.3
  6     -546.208:  45168.3
  7     -556.572:  48444.8
  8     -628.932:  78993.4
  9     -699.716:  127441.
 10     -771.102:  206437.
 11     -842.258:  333880.
 12     -913.501:  540319.
 13     -984.712:  874202.
 14     -1055.93: 1.41452e+06
 15     -1127.15: 2.28873e+06
 16     -1198.37: 3.70326e+06
 17     -1269.59: 5.99199e+06
 18     -1340.81: 9.69524e+06
 19     -1412.03: 1.56872e+07
 20     -1483.25: 2.53825e+07
 21     -1554.47: 4.10697e+07
 22     -1625.69: 6.64522e+07
 23     -1696.91: 1.07522e+08
 24     -1768.13: 1.73974e+08
 25     -1839.35: 2.81496e+08
 26     -1910.57: 4.55470e+08
 27     -1981.78: 7.36966e+08
 28     -2053.00: 1.19244e+09
 29     -2124.22: 1.92940e+09
 30     -2195.44: 3.12184e+09
 31     -2266.66: 5.05124e+09
 32     -2337.88: 8.17308e+09
 33     -2409.10: 1.32243e+10
 34     -2480.32: 2.13974e+10
 35     -2551.54: 3.46217e+10
 36     -2622.76: 5.60190e+10
 37     -2693.98: 9.06405e+10
 38     -2765.20: 1.46659e+11
 39     -2836.42: 2.37299e+11
 40     -2907.64: 3.83962e+11
 41     -2978.85: 6.21253e+11
 42     -3050.07: 1.00521e+12
 43     -3121.28: 1.62645e+12
 44     -3192.47: 2.63147e+12
 45     -3263.70: 4.25757e+12
 46     -3334.89: 6.88953e+12
 47     -3406.11: 1.11441e+13
 48     -3477.22: 1.80392e+13
 49     -3548.36: 2.91492e+13
 50     -3619.76: 4.72269e+13
 51     -3690.52: 7.63668e+13
 52     -3761.36: 1.23295e+14
 53     -3832.63: 1.99577e+14
 54     -3900.88: 3.22856e+14
 55     -3968.08: 4.97009e+14
  0     -4067.06: 1.67844e+15
  1     -4067.06: 1.67844e+15
  0     -4265.60: 5.77607e+15
  1     -4265.60: 5.77607e+15
  0     -4474.52: 1.96098e+16
  1     -4474.52: 1.96098e+16
  0     -4723.57: 6.68597e+16
  1     -4723.57: 6.68597e+16
  0     -4985.37: 2.20089e+17
  1     -4985.37: 2.20089e+17
  0     -5268.68: 7.69417e+17
  1     -5268.68: 7.69417e+17
  0     -5536.64: 2.48775e+18
  1     -5536.64: 2.48775e+18
  0     -5853.10: 8.45248e+18
  1     -5853.10: 8.45248e+18
  0     -6197.46: 3.00106e+19
  1     -6197.46: 3.00106e+19
  0     -6400.09: 8.72855e+19
  1     -6400.09: 8.72855e+19
  0     -6769.87: 3.19354e+20
  1     -6769.87: 3.19354e+20
  0     -7085.60: 1.14993e+21
  1     -7085.60: 1.14993e+21
  0     -7414.58: 4.43964e+21
  1     -7414.58: 4.43964e+21
  0     -7665.61: 1.61085e+22
  1     -7665.61: 1.61085e+22
Error in lmer(y ~ (1 | id) + x, data = example.df, family = binomial,  :
        Leading minor of order 2 in downdated X'X is not positive
definite
In addition: Warning message:
nlminb returned message singular convergence (7)
 in: LMEopt(x = mer, value = cv)


Running lmer with method="Laplace" and
control=list(usePQL=FALSE,msVerbose=TRUE):

> (lmer(y~(1|id)+x,data=example.df,family=binomial,method="Laplace",
+   control=list(usePQL=FALSE,msVerbose=TRUE)))
  0      347.321: -0.478643 0.795145  1.45231
  1      334.637: -0.775380  1.49795  2.09885
  2      326.045: -0.631955 0.917513  2.90042
  3      307.930: -0.627581  1.85085  4.66928
  4      304.717: -1.06671  1.40101  5.11069
  5      299.588: -1.05336  1.85102  5.73305
  6      297.157: -0.682292  1.60623  6.35949
  7      282.629: -1.33421  1.86152  10.2167
  8      270.279: -1.44945  2.72297  14.8450
  9      263.248: -1.61188  3.21518  19.5257
 10      254.336: -1.89092  4.01520  29.0932
 11      248.253: -2.13096  4.72573  39.9024
 12      243.359: -2.39747  5.49392  53.8331
 13      239.255: -2.66754  6.31763  71.9027
 14      235.865: -2.91894  7.17523  94.3541
 15      232.831: -3.14279  8.11396  123.501
 16      230.229: -3.32800  9.12440  159.978
 17      227.957: -3.45824  10.1876  205.312
 18      225.987: -3.50977  11.2006  258.137
 19      223.822: -3.42383  12.2016  327.929
 20      222.281: -3.29714  12.9668  393.939
 21      218.687: -2.35417  15.1107  657.987
 22      217.978: -2.00284  15.3087  724.381
 23      216.828: -1.03243  15.3436  883.159
 24      216.641: -0.727910  15.0860  924.584
 25      216.561: -0.634457  14.8052  935.901
 26      216.477: -0.670831  14.4966  934.259
 27      216.335: -0.882568  14.1066  925.552
 28      216.153: -1.24388  13.9061  926.647
 29      215.914: -1.70066  14.0769  966.092
 30      215.643: -2.07605  14.7379  1073.14
 31      215.365: -2.25220  15.8379  1261.63
 32      215.169: -2.20650  16.9633  1485.79
 33      215.065: -2.05998  17.7714  1685.40
 34      214.993: -1.85386  18.2239  1859.43
 35      214.948: -1.69235  18.3198  1985.48
 36      214.933: -1.65586  18.2629  2051.34
 37      214.933: -1.65578  18.2629  2051.34
 38      214.933: -1.65579  18.2629  2051.34
 39      214.933: -1.65586  18.2629  2051.34
 40      214.933: -1.65654  18.2625  2051.34
 41      214.932: -1.66423  18.2585  2051.33
 42      214.931: -1.70783  18.2351  2051.33
 43      214.931: -1.73215  18.2201  2051.43
 44      214.931: -1.74205  18.2078  2051.65
 45      214.930: -1.73708  18.2686  2076.43
 46      214.929: -1.73209  18.3805  2120.39
 47      214.929: -1.73283  18.3612  2112.76
 48      214.929: -1.73334  18.3600  2112.79
 49      214.929: -1.73332  18.3600  2112.79
 50      214.929: -1.73332  18.3600  2112.79
 51      214.929: -1.73332  18.3600  2112.79
 52      214.929: -1.73332  18.3600  2112.79
 53      214.929: -1.73332  18.3600  2112.79
 54      214.929: -1.73332  18.3600  2112.79
Generalized linear mixed model fit using Laplace
Formula: y ~ (1 | id) + x
          Data: example.df
 Family: binomial(logit link)
      AIC      BIC    logLik deviance
 220.9293 232.2807 -107.4646 214.9293
Random effects:
 Groups Name        Variance Std.Dev.
 id     (Intercept) 2112.8   45.965
number of obs: 325, groups: id, 177

Estimated scale (compare to 1)  0.06664838

Fixed effects:
            Estimate Std. Error  z value Pr(>|z|)
(Intercept)  -1.7333     5.7142 -0.30333  0.76164
x            18.3600     7.3318  2.50416  0.01227 *
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
  (Intr)
x -0.382

Note that the results for x don't agree at all with what glm outputs.
The estimated scale is very small and the sd for id appears to be very
large.


Now changing method="Laplace" to method="ML":

> (lmer(y~(1|id)+x,data=example.df,family=binomial,method="ML",
+   control=list(usePQL=FALSE,msVerbose=TRUE)))
Generalized linear mixed model fit using PQL
Formula: y ~ (1 | id) + x
          Data: example.df
 Family: binomial(logit link)
      AIC      BIC    logLik deviance
 353.3209 364.6724 -173.6604 347.3209
Random effects:
 Groups Name        Variance Std.Dev.
 id     (Intercept) 1.4523   1.2051
number of obs: 325, groups: id, 177

Estimated scale (compare to 1)  0.2372670

Fixed effects:
            Estimate Std. Error z value  Pr(>|z|)
(Intercept) -0.47864    0.16114 -2.9703  0.002975 **
x            0.79514    0.16872  4.7128 2.444e-06 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
  (Intr)
x -0.129

The estimated coefficients are the same as glm to 4 decimal places. The
se's are about 30% larger than for glm. The sd for id is much smaller
and the scale is larger.

If I try to turn PQL back on I get the error message.


I used ML and PQL off on the original data set and the results are
ROUGHLY similar to what SAS NLMIXED gives but the coefficient for x is
about 20% lower than NLMIXED. I haven't had a chance to run NLMIXED on
the example data frame yet.

Finally, besides my thanks for the help and apologies for the length of
this post, here is the dump of the data frame:


example.df <-
structure(list(id = structure(as.integer(c(1, 1, 2, 2, 3, 3,
4, 4, 5, 5, 6, 6, 7, 7, 8, 9, 9, 10, 10, 11, 11, 12, 12, 13,
14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20, 21, 21,
22, 22, 23, 23, 24, 24, 25, 25, 26, 26, 27, 28, 29, 29, 30, 30,
31, 31, 32, 32, 33, 33, 34, 34, 35, 35, 36, 36, 37, 37, 38, 38,
39, 39, 40, 40, 41, 42, 42, 43, 43, 44, 45, 45, 46, 46, 47, 47,
48, 48, 49, 49, 50, 50, 51, 51, 52, 52, 53, 53, 54, 54, 55, 55,
56, 56, 57, 57, 58, 58, 59, 59, 60, 61, 61, 62, 62, 63, 63, 64,
64, 65, 65, 66, 66, 67, 67, 68, 69, 69, 70, 70, 71, 71, 72, 72,
73, 73, 74, 75, 75, 76, 76, 77, 77, 78, 78, 79, 79, 80, 81, 81,
82, 82, 83, 83, 84, 85, 85, 86, 86, 87, 87, 88, 88, 89, 89, 90,
90, 91, 91, 92, 92, 93, 94, 95, 95, 96, 97, 97, 98, 98, 99, 99,
100, 101, 101, 102, 102, 103, 103, 104, 104, 105, 105, 106, 106,
107, 107, 108, 108, 109, 109, 110, 111, 111, 112, 112, 113, 113,
114, 114, 115, 116, 116, 117, 118, 118, 119, 120, 120, 121, 121,
122, 123, 123, 124, 124, 125, 125, 126, 126, 127, 127, 128, 128,
129, 129, 130, 131, 131, 132, 133, 133, 134, 134, 135, 136, 136,
137, 138, 138, 139, 139, 140, 140, 141, 141, 142, 142, 143, 143,
144, 144, 145, 145, 146, 146, 147, 148, 148, 149, 149, 150, 150,
151, 151, 152, 152, 153, 153, 154, 154, 155, 155, 156, 157, 157,
158, 159, 160, 161, 161, 162, 162, 163, 163, 164, 164, 165, 165,
166, 166, 167, 167, 168, 168, 169, 169, 170, 170, 171, 171, 172,
172, 173, 173, 174, 174, 175, 175, 176, 176, 177, 177)), .Label = c("1",
"2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13",
"14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
"25", "26", "27", "28", "29", "30", "31", "32", "33", "34", "35",
"36", "37", "38", "39", "40", "41", "42", "43", "44", "45", "46",
"47", "48", "49", "50", "51", "52", "53", "54", "55", "56", "57",
"58", "59", "60", "61", "62", "63", "64", "65", "66", "67", "68",
"69", "70", "71", "72", "73", "74", "75", "76", "77", "78", "79",
"80", "81", "82", "83", "84", "85", "86", "87", "88", "89", "90",
"91", "92", "93", "94", "95", "96", "97", "98", "99", "100",
"101", "102", "103", "104", "105", "106", "107", "108", "109",
"110", "111", "112", "113", "114", "115", "116", "117", "118",
"119", "120", "121", "122", "123", "124", "125", "126", "127",
"128", "129", "130", "131", "132", "133", "134", "135", "136",
"137", "138", "139", "140", "141", "142", "143", "144", "145",
"146", "147", "148", "149", "150", "151", "152", "153", "154",
"155", "156", "157", "158", "159", "160", "161", "162", "163",
"164", "165", "166", "167", "168", "169", "170", "171", "172",
"173", "174", "175", "176", "177"), class = "factor"), y =
structure(as.integer(c(1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2,
2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1,
1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2,
2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1,
1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,
1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1,
1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2,
2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2,
2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2,
2, 1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2,
2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1,
1, 1, 1, 2, 2, 1, 1, 1, 1)), .Label = c("0", "1"), class = "factor"),
    x = c(0.896492660264945, 0.896492660264945, 1.59446707642661,
    1.59446707642661, -1.05008338359102, -1.05008338359102,
1.09348658068790,
    1.09348658068790, 1.12507994528403, 1.12507994528403,
0.276572438987850,
    0.276572438987850, 0.434273771509725, 0.434273771509725,
    2.09093423436586, -0.622643744937437, -0.622643744937437,
    -0.58706802345943, -0.58706802345943, 0.124446406100572,
    0.124446406100572, 0.126570329770903, 0.126570329770903,
    0.181261364281855, 1.64039692579746, 1.64039692579746,
-0.555474658863302,
    -0.555474658863302, 0.47542479262234, 0.47542479262234,
-0.258656325934905,
    -0.258656325934905, 1.64995458231394, 1.64995458231394,
0.696047363877697,
    0.696047363877697, -1.46716889435175, -1.46716889435175,
    1.03375122745992, 1.03375122745992, 0.790827457666109,
0.790827457666109,
    -1.53194856629677, -1.53194856629677, -1.69389774615931,
    -1.69389774615931, -0.811141970679076, -0.811141970679076,
    -0.582289195201196, -0.582289195201196, 0.00789609469130197,
    0.573390771916238, -1.45628378554133, -1.45628378554133,
    -1.16079290490689, -1.16079290490689, -0.832646697841153,
    -0.832646697841153, -0.241930427031072, -0.241930427031072,
    -1.29353813430241, -1.29353813430241, -0.663794766050042,
    -0.663794766050042, -0.961940551272396, -0.961940551272396,
    1.59499805734419, 1.59499805734419, 0.47144243574048,
0.47144243574048,
    0.952245656611064, 0.952245656611064, -0.304586175305754,
    -0.304586175305754, 0.71463169599307, 0.71463169599307,
-1.32141463247548,
    -0.0983000888251169, -0.0983000888251169, 0.440114561603134,
    0.440114561603134, -1.75761545626916, -0.409985887445808,
    -0.409985887445808, -0.847514163533447, -0.847514163533447,
    -1.09229636653880, -1.09229636653880, -1.00415353422017,
    -1.00415353422017, -1.63681729751924, -1.63681729751924,
    -1.74354446195324, -1.74354446195324, -1.65460515825824,
    -1.65460515825824, -1.30760912861834, -1.30760912861834,
    -1.38300841891499, -1.38300841891499, -0.628750025489628,
    -0.628750025489628, 0.323564250193864, 0.323564250193864,
    -0.524412275184748, -0.524412275184748, -0.486181649118838,
    -0.486181649118838, 0.142234266839580, 0.142234266839580,
    -1.74965074250543, -1.74965074250543, -0.299010875671146,
    2.01049062535218, 2.01049062535218, -1.18229763206896,
-1.18229763206896,
    0.83304044061388, 0.83304044061388, -1.44539867673089,
-1.44539867673089,
    -0.391136064871645, -0.391136064871645, -0.118477363693237,
    -0.118477363693237, -1.73531425773072, -1.73531425773072,
    0.748083493800747, -1.70717226909887, -1.70717226909887,
    -0.210602552893726, -0.210602552893726, 0.681976369561778,
    0.681976369561778, -0.0138741229295563, -0.0138741229295563,
    -0.532111498489687, -0.532111498489687, -0.585740571165474,
    -0.202106858212414, -0.202106858212414, -0.121663249198723,
    -0.121663249198723, -0.328214826138161, -0.328214826138161,
    -0.94468367145097, -0.94468367145097, -1.4807089077501,
-1.4807089077501,
    1.09083167609999, 1.15215997208072, 1.15215997208072,
1.55411252669037,
    1.55411252669037, 0.0299318027709619, 0.0299318027709619,
    -0.0913973368965427, -1.40716805066498, -1.40716805066498,
    -0.246178274371723, -0.246178274371723, 0.473035378493218,
    0.473035378493218, 0.221084933100514, 0.221084933100514,
    -0.297152442459607, -0.297152442459607, 0.487106372809148,
    0.487106372809148, -0.434676500113372, -0.434676500113372,
    -1.18760744124478, -1.18760744124478, 0.937643681377551,
    1.05737987829232, -0.0879459609322652, -0.0879459609322652,
    0.310289727254308, -0.9587546657669, -0.9587546657669,
1.61889219863538,
    1.61889219863538, 0.983573530748409, 0.983573530748409,
0.229580627781825,
    -0.394587440835922, -0.394587440835922, 1.27163067853669,
    1.27163067853669, 1.40649983160254, 1.40649983160254,
-0.275116734379947,
    -0.275116734379947, 1.86526734439348, 1.86526734439348,
1.72668132490455,
    1.72668132490455, 0.929147986696238, 0.929147986696238,
-0.738397584970334,
    -0.738397584970334, 1.38260569031136, 1.38260569031136,
0.869412633468261,
    0.426574548204786, 0.426574548204786, 0.906846788157797,
    0.906846788157797, 0.697109325712863, 0.697109325712863,
    1.11578777922635, 1.11578777922635, 1.36242841544324,
1.20101021649827,
    1.20101021649827, 1.37676490021795, 0.76480939270458,
0.76480939270458,
    1.86393989209952, 0.543124859614057, 0.543124859614057,
-0.379985465602419,
    -0.379985465602419, 1.04224692214123, 1.85411674512425,
1.85411674512425,
    -0.251753574006341, -0.251753574006341, 0.813394146663342,
    0.813394146663342, 0.405335311501501, 0.405335311501501,
    0.590913142196445, 0.590913142196445, -0.263435154193149,
    -0.263435154193149, -1.73690720048346, -1.73690720048346,
    1.55092664118487, 1.10649561316866, 1.10649561316866,
0.454716536836637,
    -0.675741836695642, -0.675741836695642, 0.91959033017976,
    0.91959033017976, -0.256532402264574, -0.383967822484279,
    -0.383967822484279, -0.7036183348687, -1.07955282451682,
    -1.07955282451682, -0.640431605676436, -0.640431605676436,
    -1.48389479325559, -1.48389479325559, 1.72747779628092,
1.72747779628092,
    -0.959816627602066, -0.959816627602066, 0.562771153564595,
    0.562771153564595, 0.830651026484758, 0.830651026484758,
    0.126039348853320, 0.126039348853320, -0.753265050662627,
    -0.753265050662627, 0.570735867328324, 1.56101527861893,
    1.56101527861893, -0.701228920739579, -0.701228920739579,
    0.272059101188397, 0.272059101188397, -0.570607615014388,
    -0.570607615014388, -1.05539319276684, -1.05539319276684,
    -1.09442029020913, -1.09442029020913, -1.68035773276096,
    -1.68035773276096, -0.523350313349583, -0.523350313349583,
    -0.142106014525635, -1.24256396621453, -1.24256396621453,
    0.440380052061925, -0.138389148102566, 0.354626633872418,
    0.294094809268057, 0.294094809268057, 1.84349712677261,
1.84349712677261,
    -1.02857865642895, -1.02857865642895, -0.0266176649515298,
    -0.0266176649515298, 0.699233249383193, 0.699233249383193,
    0.950387223399524, 0.950387223399524, 0.350113296072966,
    0.350113296072966, 0.440114561603134, 0.440114561603134,
    -0.487774591871586, -0.487774591871586, 1.14074388235270,
    1.14074388235270, 0.797199228677091, 0.797199228677091,
-0.831053755088405,
    -0.831053755088405, 0.477283225833879, 0.477283225833879,
    1.13384113042414, 1.13384113042414, 0.607108060182704,
0.607108060182704,
    0.191084511257124, 0.191084511257124, -1.54814348428303,
    -1.54814348428303)), .Names = c("id", "y", "x"), row.names = c("4",
"101", "5", "102", "6", "103", "7", "104", "1", "98", "8", "105",
"9", "106", "198", "199", "263", "10", "107", "11", "108", "200",
"264", "197", "12", "109", "201", "265", "202", "266", "203",
"267", "204", "268", "205", "269", "2", "99", "3", "100", "206",
"270", "16", "113", "17", "114", "18", "115", "19", "116", "117",
"20", "21", "118", "22", "119", "23", "120", "24", "121", "25",
"122", "26", "123", "27", "124", "28", "125", "29", "126", "30",
"127", "31", "128", "207", "271", "32", "33", "129", "208", "272",
"130", "34", "131", "35", "132", "36", "133", "37", "134", "38",
"135", "39", "136", "13", "110", "40", "137", "41", "138", "42",
"139", "209", "273", "43", "140", "44", "141", "210", "274",
"45", "142", "211", "46", "143", "14", "111", "212", "275", "47",
"144", "48", "145", "213", "276", "214", "277", "215", "49",
"146", "50", "147", "216", "278", "217", "279", "218", "280",
"51", "52", "148", "219", "281", "220", "282", "221", "283",
"53", "149", "222", "223", "284", "54", "150", "55", "151", "152",
"56", "153", "57", "154", "58", "155", "224", "285", "225", "286",
"226", "287", "15", "112", "59", "156", "289", "290", "228",
"291", "229", "60", "157", "230", "292", "231", "293", "294",
"232", "295", "233", "296", "61", "158", "234", "297", "235",
"298", "62", "159", "236", "299", "63", "160", "64", "161", "300",
"237", "301", "65", "162", "66", "163", "238", "302", "303",
"67", "164", "304", "68", "165", "239", "240", "305", "241",
"306", "307", "242", "308", "69", "166", "70", "167", "227",
"288", "243", "309", "73", "170", "74", "171", "247", "248",
"312", "249", "75", "172", "250", "313", "244", "76", "173",
"174", "77", "175", "251", "314", "78", "176", "252", "315",
"79", "177", "253", "316", "254", "317", "80", "178", "255",
"318", "319", "81", "179", "82", "180", "83", "181", "84", "182",
"85", "183", "86", "184", "71", "168", "256", "320", "185", "87",
"186", "257", "258", "321", "88", "187", "259", "322", "260",
"323", "245", "310", "261", "324", "89", "188", "90", "189",
"91", "190", "92", "191", "246", "311", "93", "192", "94", "193",
"95", "194", "96", "195", "262", "325", "97", "196", "72", "169"
), class = "data.frame")


From edd at debian.org  Thu Jun 22 01:44:57 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 21 Jun 2006 18:44:57 -0500
Subject: [R] IMSL Wrapper
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA026FFFA8@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA026FFFA8@usctmx1106.merck.com>
Message-ID: <17561.55801.331562.748565@basebud.nulle.part>


On 21 June 2006 at 09:36, Liaw, Andy wrote:
| From: Sundar Dorai-Raj
| > Manoj wrote:
| > > Dear All,
| > >        Is there any Wrapper written around IMSL C libraries 
| > that makes 
| > > it possible to access the IMSL C functions from within R?
| > > 
| > >         Any pointers would be greatly appreciated!
| > 
| > What functions are you looking for? I'd be surprised if IMSL 
| > had some functionality that R did not.
| > 
| > --sundar
| 
| Perhaps another alternative is the gsl package, which provides R interface
| to the GSL (if what Manoj wants is available there).

But AFAIK the gsl package only wraps the GSL special functions.

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From blomsp at ozemail.com.au  Thu Jun 22 02:07:58 2006
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Thu, 22 Jun 2006 10:07:58 +1000
Subject: [R] Suggestion for ?split
In-Reply-To: <44979CCB.3020207@bfro.uni-lj.si>
References: <44979CCB.3020207@bfro.uni-lj.si>
Message-ID: <4499DF5E.5000703@ozemail.com.au>

Hi all,

I noticed an undocumented feature for split. It sorts the resulting list 
according to the grouping factor. An example:

test <- data.frame(x=rnorm(48), f=letters[sample(1:8)])
split(test, test$f)

I wasn't expecting this behaviour, although I was pleasantly surprised. 
I suggest that the help page for split be amended to include this 
feature. I know it's a small thing, but someone else may also find it 
useful to know.

Cheers,

Simon.

-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C


From spencer.graves at pdf.com  Thu Jun 22 02:15:40 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 21 Jun 2006 17:15:40 -0700
Subject: [R] IMSL Wrapper
In-Reply-To: <17561.55801.331562.748565@basebud.nulle.part>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA026FFFA8@usctmx1106.merck.com>
	<17561.55801.331562.748565@basebud.nulle.part>
Message-ID: <4499E12C.7010200@pdf.com>

	  I'd be extremely interested to know what Manoj wants from the IMSL 
library and how it compares with open source software available in R and 
elsewhere.

	   Best Wishes,
	   Spencer Graves

Dirk Eddelbuettel wrote:
> On 21 June 2006 at 09:36, Liaw, Andy wrote:
> | From: Sundar Dorai-Raj
> | > Manoj wrote:
> | > > Dear All,
> | > >        Is there any Wrapper written around IMSL C libraries 
> | > that makes 
> | > > it possible to access the IMSL C functions from within R?
> | > > 
> | > >         Any pointers would be greatly appreciated!
> | > 
> | > What functions are you looking for? I'd be surprised if IMSL 
> | > had some functionality that R did not.
> | > 
> | > --sundar
> | 
> | Perhaps another alternative is the gsl package, which provides R interface
> | to the GSL (if what Manoj wants is available there).
> 
> But AFAIK the gsl package only wraps the GSL special functions.
> 
> Dirk
>


From ronggui.huang at gmail.com  Thu Jun 22 02:40:02 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Thu, 22 Jun 2006 08:40:02 +0800
Subject: [R] eliminating a do loop
In-Reply-To: <56484410CF26D747A645BE746C57CE6D021A8D89@SCIUSFREXS3.na.jnj.com>
References: <56484410CF26D747A645BE746C57CE6D021A8D89@SCIUSFREXS3.na.jnj.com>
Message-ID: <38b9f0350606211740v9142963pc576316a77c2c512@mail.gmail.com>

Have you tried these:
>png()
> lapply(simint.by.fit,plot)
>dev.off()


2006/6/22, Barker, Chris [SCIUS] <cbarker1 at scius.jnj.com>:
>
> Using a "by() statement, I am preparing ANOVA's for multiple experiments,
> and using simint() to generate confidence intervals.
> This works fine.
>
>
> simint.by.fit <- by(analytes.dfr, list(Assay = analytes.dfr$analyte ),
> function(data) (simint(value ~ tx, data = data,type='Tukey' ) ) )
>
>
> I can separately prepare plots of the confidence intervals, and I can
> prepare separate plots or use for/do loop e.g.
>
> plot( simint.by.fit$"Assay 1"  )
> plot( simint.by.fit$"Assay 2")
>
> Is there a way to generate and save multiple -separate- plots without using
> a do loop, e.g. using lapply?.
>
> I tried a couple variations using lapply,
>
> lapply(simint.by.fit,plot)
>
> not surprising, I only get the last plot, the other plots seem to get
> overwritten.
>
> Thanks in advance
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
??????
Department of Sociology
Fudan University


From NordlDJ at dshs.wa.gov  Thu Jun 22 02:40:24 2006
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS))
Date: Wed, 21 Jun 2006 17:40:24 -0700
Subject: [R] rank(x,y)?
Message-ID: <592E8923DB6EA348BE8E33FCAADEFFFC13EED8F3@dshs-exch2.dshs.wa.lcl>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Duncan Murdoch
> Sent: Wednesday, June 21, 2006 11:14 AM
> To: Peter Dalgaard
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] rank(x,y)?
> 
> Peter Dalgaard wrote:
> > Duncan Murdoch <murdoch at stats.uwo.ca> writes:
> >
> >
> >> Suppose I have two columns, x,y.  I can use order(x,y) to calculate a
> >> permutation that puts them into increasing order of x,
> >> with ties broken by y.
> >>
> >> I'd like instead to calculate the rank of each pair under the same
> >> ordering, but the rank() function doesn't take multiple values
> >> as input.  Is there a simple way to get what I want?
> >>
> >> E.g.
> >>
> >>  > x <- c(1,2,3,4,1,2,3,4)
> >>  > y <- c(1,2,3,1,2,3,1,2)
> >>  > rank(x+y/10)
> >> [1] 1 3 6 7 2 4 5 8
> >>
> >> gives me the answer I want, but only because I know the range of y and
> >> the size of gaps in the x values.  What do I do in general?
> >>
> >
> > Still not quite general, but in the absence of ties:
> >
> >
> >> z[order(x,y)]<-1:8
> >> z
> >>
> > [1] 1 3 6 7 2 4 5 8
> >
> >
> 
> Thanks to all who have replied.  Unfortunately for me, ties do exist,
> and I'd like them to get identical ranks.  John Fox's suggestion would
> handle ties properly, but I'm worried about rounding error giving
> spurious ties.
> 
> Duncan Murdoch
> 

Duncan,

Similar to John's approach, how about something like

rankPair<-function(a,b){
  n<-ceiling(log(length(a)))+1
  rank(10^n*rank(a)+rank(b))
  }

This should avoid rounding problems and should be reasonably general (as
long as the number of pairs to be ranked doesn't become too large.

Dan

Daniel J. Nordlund
Research and Data Analysis
Washington State Department of Social and Health Services
Olympia, WA  98504-5204


From manojsw at gmail.com  Thu Jun 22 02:41:26 2006
From: manojsw at gmail.com (Manoj)
Date: Thu, 22 Jun 2006 09:41:26 +0900
Subject: [R] IMSL Wrapper
In-Reply-To: <4499E12C.7010200@pdf.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA026FFFA8@usctmx1106.merck.com>
	<17561.55801.331562.748565@basebud.nulle.part>
	<4499E12C.7010200@pdf.com>
Message-ID: <829e6c8a0606211741t74a742fbpf6280a057d794b2@mail.gmail.com>

Dear All,
    Thanks for your replies and interest so far.

    I guess R pretty much covers most of what imsl offers but the area
that I really wanted to test imsl out was in the optimization
functions. Though the imsl functions are not as rich as NUOPT (which
is quite expensive) , imsl c v6 claims that it's much faster and
solves a broader range of problems hence the need for a wrapper.

Cheers

Manoj


On 6/22/06, Spencer Graves <spencer.graves at pdf.com> wrote:
>          I'd be extremely interested to know what Manoj wants from the IMSL
> library and how it compares with open source software available in R and
> elsewhere.
>
>           Best Wishes,
>           Spencer Graves
>
> Dirk Eddelbuettel wrote:
> > On 21 June 2006 at 09:36, Liaw, Andy wrote:
> > | From: Sundar Dorai-Raj
> > | > Manoj wrote:
> > | > > Dear All,
> > | > >        Is there any Wrapper written around IMSL C libraries
> > | > that makes
> > | > > it possible to access the IMSL C functions from within R?
> > | > >
> > | > >         Any pointers would be greatly appreciated!
> > | >
> > | > What functions are you looking for? I'd be surprised if IMSL
> > | > had some functionality that R did not.
> > | >
> > | > --sundar
> > |
> > | Perhaps another alternative is the gsl package, which provides R interface
> > | to the GSL (if what Manoj wants is available there).
> >
> > But AFAIK the gsl package only wraps the GSL special functions.
> >
> > Dirk
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From klebyn at yahoo.com.br  Thu Jun 22 03:21:20 2006
From: klebyn at yahoo.com.br (Cleber N.Borges)
Date: Wed, 21 Jun 2006 22:21:20 -0300 (ART)
Subject: [R] How to plot a image with restrictions?
Message-ID: <20060622012120.11944.qmail@web30611.mail.mud.yahoo.com>



Hello All!


How to plot a image ( image function )
with restrictions, like a polygon??

Thanks in advance!

Cleber N. Borges


##### problem example

x <- y <- seq(-4*pi, 4*pi, len=27)
r <- sqrt(outer(x^2, y^2, "+"))
image( z )
contour(z, add = TRUE, drawlabels = FALSE)

m=scan()
0.2  0.2
0.8  0.2
0.5  0.8

m = matrix(m,nr=3,byrow=T)
polygon( m, lwd=5 )


From ggrothendieck at gmail.com  Thu Jun 22 04:02:33 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 21 Jun 2006 22:02:33 -0400
Subject: [R] rank(x,y)?
In-Reply-To: <44998C52.7040502@stats.uwo.ca>
References: <4499854B.8020707@stats.uwo.ca>
	<x21wtil6vw.fsf@turmalin.kubism.ku.dk> <44998C52.7040502@stats.uwo.ca>
Message-ID: <971536df0606211902i6f7242d7qe28ae580a1290b3d@mail.gmail.com>

On 6/21/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> Peter Dalgaard wrote:
> > Duncan Murdoch <murdoch at stats.uwo.ca> writes:
> >
> >
> >> Suppose I have two columns, x,y.  I can use order(x,y) to calculate a
> >> permutation that puts them into increasing order of x,
> >> with ties broken by y.
> >>
> >> I'd like instead to calculate the rank of each pair under the same
> >> ordering, but the rank() function doesn't take multiple values
> >> as input.  Is there a simple way to get what I want?
> >>
> >> E.g.
> >>
> >>  > x <- c(1,2,3,4,1,2,3,4)
> >>  > y <- c(1,2,3,1,2,3,1,2)
> >>  > rank(x+y/10)
> >> [1] 1 3 6 7 2 4 5 8
> >>
> >> gives me the answer I want, but only because I know the range of y and
> >> the size of gaps in the x values.  What do I do in general?
> >>
> >
> > Still not quite general, but in the absence of ties:
> >
> >
> >> z[order(x,y)]<-1:8
> >> z
> >>
> > [1] 1 3 6 7 2 4 5 8
> >
> >
>
> Thanks to all who have replied.  Unfortunately for me, ties do exist,
> and I'd like them to get identical ranks.  John Fox's suggestion would
> handle ties properly, but I'm worried about rounding error giving
> spurious ties.
>

Try this variant of my prior solution:

(order(order(x,y)) + rev(order(order(rev(x), rev(y)))))/2

Note that no arithmetic is done on the original data, only on
the output of order, so there should not be any worry about
rounding -- in fact its sufficiently general that the data
do not have to be numeric, e.g.

> x <- c("a", "a", "b", "a", "c", "d")
> y <- c("b", "a", "b", "b", "a", "a")
> (order(order(x,y)) + rev(order(order(rev(x), rev(y)))))/2
[1] 2.5 1.0 4.0 2.5 5.0 6.0


From ripley at stats.ox.ac.uk  Thu Jun 22 08:51:50 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 22 Jun 2006 07:51:50 +0100 (BST)
Subject: [R] Suggestion for ?split
In-Reply-To: <4499DF5E.5000703@ozemail.com.au>
References: <44979CCB.3020207@bfro.uni-lj.si> <4499DF5E.5000703@ozemail.com.au>
Message-ID: <Pine.LNX.4.64.0606220750180.31797@gannet.stats.ox.ac.uk>

On Thu, 22 Jun 2006, Simon Blomberg wrote:

> Hi all,
>
> I noticed an undocumented feature for split. It sorts the resulting list
> according to the grouping factor. An example:
>
> test <- data.frame(x=rnorm(48), f=letters[sample(1:8)])
> split(test, test$f)
>
> I wasn't expecting this behaviour, although I was pleasantly surprised.
> I suggest that the help page for split be amended to include this
> feature. I know it's a small thing, but someone else may also find it
> useful to know.

It is not really true.  The help page says

      The value returned from 'split' is a list of vectors containing
      the values for the groups.  The components of the list are named
      by the _used_ factor levels given by 'f'.

They are in the same order as the _used_ factor levels (as the statement 
implies), but those are in no sense sorted.  Indeed, the factor may be 
created by as.factor or interaction, and working out the order of the 
factor levels can be tricky, which is why they are named.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mark_difford at yahoo.co.uk  Tue Jun 20 14:24:50 2006
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Tue, 20 Jun 2006 12:24:50 +0000 (GMT)
Subject: [R] NLME: using the layout option with the plot command
Message-ID: <20060620122450.98275.qmail@web27306.mail.ukl.yahoo.com>

Hi Greg,

Since you haven't yet had a response, you could distill this.  It uses the pixel dataset from nlme() as an example.

## To get separate files, do this
postscript("c:\MyGraph%03.ps", onefile=F)
plot(Pixel, display = "Dog", inner = ~Side, layout=c(4,1))
dev.off()

## To get your layout into one file, as separate pages, do this
postscript("c:\MyGraph.ps", onefile=T)
plot(Pixel, display = "Dog", inner = ~Side, layout=c(4,1))
dev.off()

If you prefer pdf, then use : pdf(filename, onefile=F), &c.  At the R prompt do : ?postscript (and ?pdf), and go on reading!  Also have a look at setps() in package Hmisc.

Regards,
Mark.

 
Mark DiffordPh.D. candidate, Botany Department,
Nelson Mandela Metropolitan University,
Port Elizabeth, SA.


From r_econometrics at yahoo.co.in  Thu Jun 22 07:54:53 2006
From: r_econometrics at yahoo.co.in (Sumanta Basak)
Date: Thu, 22 Jun 2006 06:54:53 +0100 (BST)
Subject: [R] Basic NA handling problem
Message-ID: <20060622055453.32786.qmail@web7606.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060622/e80d93ab/attachment.pl 

From JeeBee at troefpunt.nl  Thu Jun 22 09:50:47 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Thu, 22 Jun 2006 09:50:47 +0200
Subject: [R] Basic NA handling problem
References: <20060622055453.32786.qmail@web7606.mail.in.yahoo.com>
Message-ID: <pan.2006.06.22.07.50.44.250440@troefpunt.nl>


Try this:
help.search("interpolate")

Then find this:

> library(zoo)
> na.approx(x)
 [1]  1.0  4.0  5.0  8.0  6.0  4.0  4.5  5.0  5.0  1.0  2.0  7.0  8.0  9.0 10.5
[16] 12.0 13.5 15.0  6.0  8.0

Hth,
JeeBee.

On Thu, 22 Jun 2006 06:54:53 +0100, Sumanta Basak wrote:

> Hi All,
> 
> I need your help in NA handling.
> 
> I've following data series.
> 
> x<-c(1,4,5,8,NA,4,NA,5,5,1,2,7,8,9,NA,NA,NA,15,6,8)
> 
> Now i want to interpolate where NA value persists. Like, between 9 and 5 there are three NA's. So, that should be interpolated like,
> 
> 1st NA-> (15-9)/4
> 2nd NA-> 1st NA value + (15-9)/4
> 
> Can i get help on this using a 'for' loop. Actually i have huge daily time series with lots of NA values where i need to make these values.
> 
> Please help.
> 
> Thanks,
> Sumanta Basak.
> 
>  				
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From Roger.Bivand at nhh.no  Thu Jun 22 09:56:28 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 22 Jun 2006 09:56:28 +0200 (CEST)
Subject: [R] How to plot a image with restrictions?
In-Reply-To: <20060622012120.11944.qmail@web30611.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.44.0606220950490.17495-100000@reclus.nhh.no>

On Wed, 21 Jun 2006, Cleber N.Borges wrote:

> 
> 
> Hello All!
> 
> 
> How to plot a image ( image function )
> with restrictions, like a polygon??

The suggestions given in:

http://finzi.psych.upenn.edu/R/Rhelp02a.new/archive/15030.html

by Denis White let you overplot the image with a masking polygon - that
may be the most suitable for you. Alternatively, you can set the image
values to be omitted to NA, but how you would do this depends rather on
whether the polygon is irregular or not. If the polygon is regular,
unrolling the image into a vector and setting conditions on expand.grid()
of the row and column coordinates will work with is.na(). If irregular,
you'll need to do point-in-polygon checking of the image grid coordinates.
If your data are geographical (not assumed here), you may like to follow
this up on the R-sig-geo list.

Roger

> 
> Thanks in advance!
> 
> Cleber N. Borges
> 
> 
> ##### problem example
> 
> x <- y <- seq(-4*pi, 4*pi, len=27)
> r <- sqrt(outer(x^2, y^2, "+"))
> image( z )
> contour(z, add = TRUE, drawlabels = FALSE)
> 
> m=scan()
> 0.2  0.2
> 0.8  0.2
> 0.5  0.8
> 
> m = matrix(m,nr=3,byrow=T)
> polygon( m, lwd=5 )
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From maechler at stat.math.ethz.ch  Thu Jun 22 11:03:54 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 22 Jun 2006 11:03:54 +0200
Subject: [R] Rosner's test
In-Reply-To: <007401c68f31$32546900$295afea9@gne.windows.gene.com>
References: <536F1035-96C7-46CD-9574-384956F54121@horizonenv.com>
	<007401c68f31$32546900$295afea9@gne.windows.gene.com>
Message-ID: <17562.23802.793188.600782@stat.math.ethz.ch>

>>>>> "BertG" == Berton Gunter <gunter.berton at gene.com>
>>>>>     on Tue, 13 Jun 2006 14:34:48 -0700 writes:

    BertG> RSiteSearch('Rosner') ?RSiteSearch or search directly
    BertG> from CRAN.

    BertG> Incidentally, I'll repeat what I've said
    BertG> before. Don't do outlier tests.  They're
    BertG> dangerous. Use robust methods instead.

Yes, yes, yes!!!

Note that  rlm() or cov.rob()  from recommended package MASS
will most probably be sufficient for your needs.

For slightly newer methodology, look at package 'robustbase', or
also 'rrcov'.

Martin Maechler, ETH Zurich

    BertG> -- Bert Gunter Genentech Non-Clinical Statistics
    BertG> South San Francisco, CA
 
    BertG> "The business of the statistician is to catalyze the
    BertG> scientific learning process."  - George E. P. Box


From elatine at gmail.com  Thu Jun 22 11:17:38 2006
From: elatine at gmail.com (=?ISO-8859-1?Q?B=E1lint_Cz=FAcz?=)
Date: Thu, 22 Jun 2006 11:17:38 +0200
Subject: [R]  multivariate splits
In-Reply-To: <fab4bcf70606220215l78736bb2x@mail.gmail.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA026FFEFA@usctmx1106.merck.com>
	<fab4bcf70606220215l78736bb2x@mail.gmail.com>
Message-ID: <fab4bcf70606220217l16d608dcg@mail.gmail.com>

Thanks to everyone for the replies to my question.

There are several good algorithms to make decision trees with
multivariate splits (sometimes called oblique trees), some are even
freeware (QUEST, CRUISE...), but most are not open-sourced.

Unfortunately there seems to be no good choice for oblique trees in R
now. Mvpart seems to be really fancy, although it cannot handle splits
on the linear combination of the covariates. Rweka might be useful,
but it still takes some time for me to get used to its "language".
I'll see where I get.

Thanks again and bye:
B?lint


-- 
Cz?cz B?lint
PhD hallgat?
BCE KTK Talajtan ?s V?zgazd?lkod?s Tansz?k
1118 Budapest, Vill?nyi ?t 29-43.


From john.seers at bbsrc.ac.uk  Thu Jun 22 11:48:46 2006
From: john.seers at bbsrc.ac.uk (john seers (IFR))
Date: Thu, 22 Jun 2006 10:48:46 +0100
Subject: [R] sort matrix by sum of columns
Message-ID: <1CF0B26CECD746438AE02DBF7DDE1C7B03055E7C@ifre2ksrv1.ifrxp.bbsrc.ac.uk>


Albert

Is this what you want?:

 a[,order(colSums(a))]


John S

 
---


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Albert Vilella
Sent: 21 June 2006 11:38
To: r help
Subject: [R] sort matrix by sum of columns


Hi all,

I would like to know how can I sort the cols of a matrix by the sum of
their elements.


a <- matrix(as.integer(rnorm(25,4,2)),10,5)
colnames(a) = c("alfa","bravo","charlie","delta","echo")

I guess I should use colSums, and then rearrange the matrix somehow
according to the result.

My idea is to display a "sorted" barplot:

barplot(a, horiz=TRUE, legend.text=T)

Thanks in advance,

    Albert.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From dargosch at gmail.com  Thu Jun 22 13:41:29 2006
From: dargosch at gmail.com (Fredrik Karlsson)
Date: Thu, 22 Jun 2006 13:41:29 +0200
Subject: [R] Two sets of axis in a lattice plot?
Message-ID: <376e97ec0606220441v6990da27ub0d539c2250064a5@mail.gmail.com>

Dear list,

Is it possible to have lattice provide two different ordinate axis,
one on the right side and one of the left side of the plot?

I would like to make a xyplot with x both in a linear scale and in a
log-approximate scale.

Thank you in advance.

/Fredrik


From rkrug at sun.ac.za  Thu Jun 22 13:47:11 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Thu, 22 Jun 2006 13:47:11 +0200
Subject: [R] problem with integrate()
Message-ID: <449A833F.7080306@sun.ac.za>

Hi

System:
Linux, SuSE 10
R 2.3.0

I try to do the following

> ip <- function(x) 1240*dip(x, 0.88)
> iip <- function(x) integrate(ip, x - 0.045, x + 0.045)$value
> f <- integrate(iip, 10, 100)
Error in integrate(iip, 10, 100) : evaluation of function gave a result
of wrong length

How can I solve this?

Thanks

Rainer



-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
      	Rainer at krugs.de


From rkrug at sun.ac.za  Thu Jun 22 14:03:10 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Thu, 22 Jun 2006 14:03:10 +0200
Subject: [R] problem with integrate() - correction
In-Reply-To: <449A833F.7080306@sun.ac.za>
References: <449A833F.7080306@sun.ac.za>
Message-ID: <449A86FE.5020005@sun.ac.za>

Sorry

should read:

ip <- function(x) 1240 * x^(-0.88)


Rainer M Krug wrote:
> Hi
> 
> System:
> Linux, SuSE 10
> R 2.3.0
> 
> I try to do the following
> 
>> ip <- function(x) 1240*dip(x, 0.88)
>> iip <- function(x) integrate(ip, x - 0.045, x + 0.045)$value
>> f <- integrate(iip, 10, 100)
> Error in integrate(iip, 10, 100) : evaluation of function gave a result
> of wrong length
> 
> How can I solve this?
> 
> Thanks
> 
> Rainer
> 
> 
> 


-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
      	Rainer at krugs.de


From FredeA.Togersen at agrsci.dk  Thu Jun 22 14:59:47 2006
From: FredeA.Togersen at agrsci.dk (=?iso-8859-1?Q?Frede_Aakmann_T=F8gersen?=)
Date: Thu, 22 Jun 2006 14:59:47 +0200
Subject: [R] problem with integrate() - correction
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC047E4216@DJFPOST01.djf.agrsci.dk>

Well why not do it symbolically, e.g.

int(a*x^b,x) = \frac{a}{b+1}x^{b+1}

and integrating this again gives

\frac{a}{(b+1)(b+2)}x^{b+2},

of course for resonable values of a and b such that things are well defined.

But if you follow the link

https://stat.ethz.ch/pipermail/r-help/2005-January/064026.html

you may come to this solution:


ip <- function(x) 1240 * x^(-0.88)

iip <- function(x){
  res <- rep(NA,length(x))
  for (i in seq(along=x))        
    res[i] <- integrate(ip, x[i] - 0.045, x[i] + 0.045)$value
  return(res)
}

f <- integrate(iip, 10, 100)

which may not be an effective way to do it.


Med venlig hilsen
Frede Aakmann T?gersen
 

 

> -----Oprindelig meddelelse-----
> Fra: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] P? vegne af Rainer M Krug
> Sendt: 22. juni 2006 14:03
> Til: rkrug at sun.ac.za
> Cc: R help list
> Emne: [R] problem with integrate() - correction
> 
> Sorry
> 
> should read:
> 
> ip <- function(x) 1240 * x^(-0.88)
> 
> 
> Rainer M Krug wrote:
> > Hi
> > 
> > System:
> > Linux, SuSE 10
> > R 2.3.0
> > 
> > I try to do the following
> > 
> >> ip <- function(x) 1240*dip(x, 0.88)
> >> iip <- function(x) integrate(ip, x - 0.045, x + 0.045)$value f <- 
> >> integrate(iip, 10, 100)
> > Error in integrate(iip, 10, 100) : evaluation of function gave a 
> > result of wrong length
> > 
> > How can I solve this?
> > 
> > Thanks
> > 
> > Rainer
> > 
> > 
> > 
> 
> 
> --
> Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation Biology (UCT)
> 
> Department of Conservation Ecology and Entomology University 
> of Stellenbosch Matieland 7602 South Africa
> 
> Tel:		+27 - (0)72 808 2975 (w)
> Fax:		+27 - (0)21 808 3304
> Cell:		+27 - (0)83 9479 042
> 
> email:	RKrug at sun.ac.za
>       	Rainer at krugs.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From rkrug at sun.ac.za  Thu Jun 22 15:07:28 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Thu, 22 Jun 2006 15:07:28 +0200
Subject: [R] problem with integrate() - correction
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC047E4216@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC047E4216@DJFPOST01.djf.agrsci.dk>
Message-ID: <449A9610.2030908@sun.ac.za>

Thanks a lot - the symbolic solution will work and I understand what is
wrong with my function

Rainer


Frede Aakmann T?gersen wrote:
> Well why not do it symbolically, e.g.
> 
> int(a*x^b,x) = \frac{a}{b+1}x^{b+1}
> 
> and integrating this again gives
> 
> \frac{a}{(b+1)(b+2)}x^{b+2},
> 
> of course for resonable values of a and b such that things are well defined.
> 
> But if you follow the link
> 
> https://stat.ethz.ch/pipermail/r-help/2005-January/064026.html
> 
> you may come to this solution:
> 
> 
> ip <- function(x) 1240 * x^(-0.88)
> 
> iip <- function(x){
>   res <- rep(NA,length(x))
>   for (i in seq(along=x))        
>     res[i] <- integrate(ip, x[i] - 0.045, x[i] + 0.045)$value
>   return(res)
> }
> 
> f <- integrate(iip, 10, 100)
> 
> which may not be an effective way to do it.
> 
> 
> Med venlig hilsen
> Frede Aakmann T?gersen
>  
> 
>  
> 
>> -----Oprindelig meddelelse-----
>> Fra: r-help-bounces at stat.math.ethz.ch 
>> [mailto:r-help-bounces at stat.math.ethz.ch] P? vegne af Rainer M Krug
>> Sendt: 22. juni 2006 14:03
>> Til: rkrug at sun.ac.za
>> Cc: R help list
>> Emne: [R] problem with integrate() - correction
>>
>> Sorry
>>
>> should read:
>>
>> ip <- function(x) 1240 * x^(-0.88)
>>
>>
>> Rainer M Krug wrote:
>>> Hi
>>>
>>> System:
>>> Linux, SuSE 10
>>> R 2.3.0
>>>
>>> I try to do the following
>>>
>>>> ip <- function(x) 1240*dip(x, 0.88)
>>>> iip <- function(x) integrate(ip, x - 0.045, x + 0.045)$value f <- 
>>>> integrate(iip, 10, 100)
>>> Error in integrate(iip, 10, 100) : evaluation of function gave a 
>>> result of wrong length
>>>
>>> How can I solve this?
>>>
>>> Thanks
>>>
>>> Rainer
>>>
>>>
>>>
>>
>> --
>> Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation Biology (UCT)
>>
>> Department of Conservation Ecology and Entomology University 
>> of Stellenbosch Matieland 7602 South Africa
>>
>> Tel:		+27 - (0)72 808 2975 (w)
>> Fax:		+27 - (0)21 808 3304
>> Cell:		+27 - (0)83 9479 042
>>
>> email:	RKrug at sun.ac.za
>>       	Rainer at krugs.de
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>


-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
      	Rainer at krugs.de


From deepayan.sarkar at gmail.com  Thu Jun 22 15:19:54 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 22 Jun 2006 08:19:54 -0500
Subject: [R] Two sets of axis in a lattice plot?
In-Reply-To: <376e97ec0606220441v6990da27ub0d539c2250064a5@mail.gmail.com>
References: <376e97ec0606220441v6990da27ub0d539c2250064a5@mail.gmail.com>
Message-ID: <eb555e660606220619r1df7d958le551cf0a3e2b808@mail.gmail.com>

On 6/22/06, Fredrik Karlsson <dargosch at gmail.com> wrote:
> Dear list,
>
> Is it possible to have lattice provide two different ordinate axis,
> one on the right side and one of the left side of the plot?

Not easily. I hope to add some support for this sort of thing in the
future, but I'm not sure when. Currently, you can add a second axis as
part of the panel function (using e.g. panel.axis), but you will have
to adjust the space for it manually. You will also need to turn off
clipping.

-Deepayan

> I would like to make a xyplot with x both in a linear scale and in a
> log-approximate scale.
>
> Thank you in advance.
>
> /Fredrik
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
http://www.stat.wisc.edu/~deepayan/


From justin.rapp at gmail.com  Thu Jun 22 16:11:50 2006
From: justin.rapp at gmail.com (Justin Rapp)
Date: Thu, 22 Jun 2006 10:11:50 -0400
Subject: [R] As.Factor with Logistic Regression
Message-ID: <e11df81d0606220711u1db3d2fep1e5fc708e4bc1a12@mail.gmail.com>

I am modeling the probability of player succeeding in the NFL with a
binomial logistic regression with 1 signifying success and 0
signifying no success.   I performed the regression of the binomial
variable against overall draft position using the college conference
for which each player played as a factor using the
as.factor(Conference) command.

My question is:

How do I plot specific factors against the curve for the entire set.
There are only a few factors that have significant coefficients and I
would like to plot those against the logistic curve for the entire
set.

Any help would be greatly appreciated.

-- 
Justin Rapp
409 S. 22nd St.
Apt. 1
Philadelphia, PA 19146
Cell:(267)252.0297
justin.rapp at gmail.com


From JBDavis at txfb-ins.com  Thu Jun 22 17:05:07 2006
From: JBDavis at txfb-ins.com (Davis, Jacob B. )
Date: Thu, 22 Jun 2006 10:05:07 -0500
Subject: [R] glm beta hypothesis testing
Message-ID: <CC7973C6B927754794A205FEE463F7810A933F@TXS9312221.txfb-ins.com>

Thanks for the insight on the debugger.  It has taken me a day or so to
get use to it, but it is very helpful.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ben Bolker
Sent: Tuesday, June 20, 2006 12:56 PM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] glm beta hypothesis testing

Davis, Jacob B.  <JBDavis <at> txfb-ins.com> writes:

> 
> In summary.glm I'm trying to get a better feel for the z output.  The
> following lines can be found in the function
> 
  [snip]

  digging through the function is good: debugging your way through
the function is sometimes even better.

examples(glm,local=TRUE)  ## run glm examples and get
   ## results left in local workspace)

ls()
debug(summary.glm)
summary(glm.D93)

  shows ...
  p is object rank (~ number of parameters)
  Qr$pivot gives the order in which the parameters
have been rearranged to solve the model,
so Qr$pivot[p1] gives the rearranged order
of the coefficients.  We need this rearranged
order because we're going to extract the
unscaled covariance matrix by solving the
inverse QR matrix, which is in the pivoted
(rearranged) order.

The null hypothesis for any particular contrast
in glm is that the parameter is 0, so the
estimates of the coefficients (object$coefficients)
*are* the distance from the null hypothesis.

   Ben Bolker

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From f.harrell at vanderbilt.edu  Thu Jun 22 17:05:05 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 22 Jun 2006 10:05:05 -0500
Subject: [R] As.Factor with Logistic Regression
In-Reply-To: <e11df81d0606220711u1db3d2fep1e5fc708e4bc1a12@mail.gmail.com>
References: <e11df81d0606220711u1db3d2fep1e5fc708e4bc1a12@mail.gmail.com>
Message-ID: <449AB1A1.2090906@vanderbilt.edu>

Justin Rapp wrote:
> I am modeling the probability of player succeeding in the NFL with a
> binomial logistic regression with 1 signifying success and 0
> signifying no success.   I performed the regression of the binomial
> variable against overall draft position using the college conference
> for which each player played as a factor using the
> as.factor(Conference) command.
> 
> My question is:
> 
> How do I plot specific factors against the curve for the entire set.
> There are only a few factors that have significant coefficients and I
> would like to plot those against the logistic curve for the entire
> set.
> 
> Any help would be greatly appreciated.
> 

It will bias the analysis to omit insignificant factors.

To get the plots you want:

library(Hmisc); library(Design)
dd <- datadist(mydata); options(datadist='dd')
f <- lrm(y ~ x1+x2+....)
plot(f, x1=NA, fun=plogis)
plot(f, x2=NA, fun=plogis)
plot(f, fun=plogis) # plot partial effects of all predictors, 
probability scale

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From JBDavis at txfb-ins.com  Thu Jun 22 17:27:33 2006
From: JBDavis at txfb-ins.com (Davis, Jacob B. )
Date: Thu, 22 Jun 2006 10:27:33 -0500
Subject: [R] GLM plotting estimated responses
Message-ID: <CC7973C6B927754794A205FEE463F7810A9340@TXS9312221.txfb-ins.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060622/324eaf76/attachment.pl 

From ccleland at optonline.net  Thu Jun 22 17:30:26 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 22 Jun 2006 11:30:26 -0400
Subject: [R] GLM plotting estimated responses
In-Reply-To: <CC7973C6B927754794A205FEE463F7810A9340@TXS9312221.txfb-ins.com>
References: <CC7973C6B927754794A205FEE463F7810A9340@TXS9312221.txfb-ins.com>
Message-ID: <449AB792.30608@optonline.net>

Davis, Jacob B. wrote:
> I have recently fit a binomial link = logit model and I want to visually
> see the estimated response across a specific covariate.
> 
>  
> 
> I can accomplish this by creating a function that extracts the betas
> from the model then forms a linear combination with the classes I'm
> interested in next pass the value through the inverse logit function, do
> this for each class in a covariate, then plot the results.
> 
>  
> 
> My question is:  Is there a built in function that will do all of this
> for me, where all I have to do is supply the classes for each covariate
> I'm interested in and it plots for me?
> 

Have a look at the following:

?predict
?all.effects in library(effects)
?effect in library(effects)

> On a similar note is there a quick way to plot the study size across a
> covariate from model results, without creating my own functions?
> 
>  
> 
> Thanks in advance,
> 
>  
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 
>  
> 
> Jacob Davis
> 
> Actuarial Analyst
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From frederic.jean at univ-brest.fr  Thu Jun 22 17:58:06 2006
From: frederic.jean at univ-brest.fr (Fred JEAN)
Date: Thu, 22 Jun 2006 17:58:06 +0200
Subject: [R] programming advice
Message-ID: <449ABE0E.8010506@univ-brest.fr>

Dear R users

I want to compute Kendall's Tau between two vectors x and y.
But x and y  may have zeros in the same position(s) and I wrote the
following function to be sure to drop out those "double zeros"

"cor.kendall" <- function(x,y) {
  nox <- c()
  noy <- c()
  #
  for (i in 1:length(x)) if (x[i]!= 0 | y[i] != 0)
      nox[length(nox)+1]<- x[i]
  for (i in 1:length(y)) if (x[i]!= 0 | y[i] != 0)
      noy[length(noy)+1]<- y[i]
  #
  res.kendall <- cor.test(nox,noy,method = "kendall",exact=F)
  return(list(x=nox,y=noy,res.kendall,length(nox)))
}

Do you know a more elegant way to achieve the same goal ?
(I'm sure you do : it's a newbie's program actually)

-- 
Fred


From jayemerson at gmail.com  Thu Jun 22 18:08:13 2006
From: jayemerson at gmail.com (Jay Emerson)
Date: Thu, 22 Jun 2006 12:08:13 -0400
Subject: [R] Basic package structure question
Message-ID: <d4588dec0606220908w415cc012y8fa678148cef44e3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060622/13bc7e28/attachment.pl 

From jasonshi510 at hotmail.com  Thu Jun 22 18:09:31 2006
From: jasonshi510 at hotmail.com (Xin)
Date: Thu, 22 Jun 2006 17:09:31 +0100
Subject: [R] Why different results with different initial values for MLE
	(optim)!
Message-ID: <BAY107-DAV6C38E8CD62F7504EB210DF0850@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060622/371bdb36/attachment.pl 

From spencer.graves at pdf.com  Thu Jun 22 18:13:01 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 22 Jun 2006 09:13:01 -0700
Subject: [R] Bayesian Networks with deal
In-Reply-To: <200606181953.k5IJrZh2007239@hypatia.math.ethz.ch>
References: <200606181953.k5IJrZh2007239@hypatia.math.ethz.ch>
Message-ID: <449AC18D.8090006@pdf.com>

	  Since I haven't seen a reply to this, I will offer a couple of 
comments.  I've never used "deal", but it sounded interesting, so I 
thought I'd look at it.

	  Have you looked at Susanne G. B?ttcher and Claus Dethlefsen. deal: A 
Package for Learning Bayesian Networks. Journal of Statistical Software, 
8(20), 2003, and the deal reference manual downloadable under 
"documentation" from "www.math.aau.dk/~dethlef/novo/deal"?  If yes and 
you still would like more help from this listserve, please submit 
another post including a simple, self-contained example explaining 
something you've tried and why it doesn't seem to answer your question? 
  (This is suggested in the posting guide! 
'www.R-project.org/posting-guide.html'.)

	  This documentation might answer your questions.  Even though I've not 
read them, I will guess potential answers to your two questions, hoping 
some other reader may disabuse us both of our ignorance:

	  From what I saw in the examples, I would guess that "deal" supports 
two types of distributions:  normal and finite (discrete).  If so, it 
does NOT support a Poisson.  If it were my problem and I still held that 
view after reviewing this documentation, I might write to the maintainer 
[listed with help(package="deal")] and ask him for suggestions.  Then if 
it were sufficiently important, I might think about how I would modify 
the code to allow for a Poisson.

	  Regarding simulations, have you looked at "rnetwork", which provides 
"simulation of data sets with a given dependency structure"?

	  Hope this helps,
	  Spencer Graves

Carsten Steinhoff wrote:
> Hello,
>  
> I want to use R to model a bayesian belief network of frequencies for system
> failiures in various departments of a company.
>  
> For the nodes I want to use a poisson-distribution parameterized with expert
> knowledge (e.g. with a gamma prior).
> Then I want to fill in learning-data to improve the initial estimates and
> get some information about possible connections.
> Later I want to simulate dependend random variables from the network
>  
> I tryed to use the package "deal" for that task, which is as far as I know
> the best (and only?) R-package for that task.
> But a few questions rose that I could not solve with the documentation:
>  
> (1) Is it possible to parameterize the prior distribution (for example
> (dpois(x,lambda=60) directly and non gaussian ?
>  
> (2) If I've chosen a structure, can I simulate dependend states that are non
> gausian distributed?
>  
> Thank you for any idea!
>  
> Regards, Carsten
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ronggui.huang at gmail.com  Thu Jun 22 18:13:19 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Fri, 23 Jun 2006 00:13:19 +0800
Subject: [R] programming advice
In-Reply-To: <449ABE0E.8010506@univ-brest.fr>
References: <449ABE0E.8010506@univ-brest.fr>
Message-ID: <38b9f0350606220913x2f6fad07kecd06c3f0606173f@mail.gmail.com>

Will the following code work for you?
> index<-!(x==0 & y==0)
> x<-x[index]
> y<-y[index]


2006/6/22, Fred JEAN <frederic.jean at univ-brest.fr>:
> Dear R users
>
> I want to compute Kendall's Tau between two vectors x and y.
> But x and y  may have zeros in the same position(s) and I wrote the
> following function to be sure to drop out those "double zeros"
>
> "cor.kendall" <- function(x,y) {
>   nox <- c()
>   noy <- c()
>   #
>   for (i in 1:length(x)) if (x[i]!= 0 | y[i] != 0)
>       nox[length(nox)+1]<- x[i]
>   for (i in 1:length(y)) if (x[i]!= 0 | y[i] != 0)
>       noy[length(noy)+1]<- y[i]
>   #
>   res.kendall <- cor.test(nox,noy,method = "kendall",exact=F)
>   return(list(x=nox,y=noy,res.kendall,length(nox)))
> }
>
> Do you know a more elegant way to achieve the same goal ?
> (I'm sure you do : it's a newbie's program actually)
>
> --
> Fred
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
??????
Department of Sociology
Fudan University


From john.emerson at yale.edu  Thu Jun 22 18:16:52 2006
From: john.emerson at yale.edu (John W Emerson)
Date: Thu, 22 Jun 2006 12:16:52 -0400 (EDT)
Subject: [R] Package struction question (second try)
Message-ID: <Pine.LNX.4.63.0606221214060.953@ares.its.yale.edu>


Sorry, gmail seemed to have made an attachment out of my first attempted 
post.  Trying again:

------------------------------

At the encouragement of many at UseR, I'm trying to build my first real
package. I have no C/Fortran code, just plain old R code, so it should be
rocket science.  On a Linux box, I used package.skeleton() to create a 
basic package containing just one "hello world" type of function.  I 
edited the DESCRIPTION file, changin the package name appropriately.  I 
edited the hello.Rd file.  Upon running R CMD check hello, the only 
warning had to do with the fact that src/ was empty (obviously I had no 
source in such a simple package).  I doubt this is a problem.

I was able to install and use the package successfully on the Linux system
from the .tar.gz file, so far so good!  Next, on to Windows, where the
problem arose:

I created a zip file from inside the package directory:

zip -r ../hello.zip ./*

When I moved this to my Windows machine and tried to install the package 
using the GUI, I received the following error:

> utils:::menuInstallLocal()
Error in unpackPkg(pkgs[i], pkgnames[i], lib, installWithVers) :
         malformed bundle DESCRIPTION file, no Contains field

I only found one mention of this in my Google search, with no reply to the
thread.  The Contains field appears to be used for bundles, but I'm trying
to create a package, not a bundle.  This leads me to believe that a simple
zipping of the package directory structure is not the correct format for
Windows.

Needless to say, there appears to be wide agreement that making packages
requires precision, but fundamentally a package should (as described in 
the
documentation) just be a collection of files and folders organized a 
certain
way.  If someone could point me to documentation I may have missed that
explains this, I would be grateful.

Regards,

Jay

-- 
John W. Emerson (Jay)
Assistant Professor of Statistics
Yale University
http://www.stat.yale.edu/~jay

 	[[alternative HTML version deleted]]


From aarppe at ling.helsinki.fi  Thu Jun 22 18:27:39 2006
From: aarppe at ling.helsinki.fi (Antti Arppe)
Date: Thu, 22 Jun 2006 19:27:39 +0300 (EEST)
Subject: [R] Reading in a table with ISO-latin1 encoding in MacOS-X
 (Intel)
In-Reply-To: <x2ac8nwxld.fsf@turmalin.kubism.ku.dk>
References: <Pine.LNX.4.62.0606081515310.31426@venus.ling.helsinki.fi>
	<x2ac8nwxld.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.62.0606221916430.8497@venus.ling.helsinki.fi>

Dear colleagues,

With the help of a colleague of mine here in Helsinki (Seppo Nyrkk?) 
who looked at innards of the R source code for Mac it turned out that 
this was indeed an issue concerning the Mac locale and its settings 
and not R.

Though we had tried this earlier by changing the LANG variable to 
'fi_FI', we hadn't looked hard enough in the available encodings (with 
locale -a) to select the exactly correct value, being:

LANG=fi_FI.IS08859-1;
export LANG;

With this configuration R was able to happily read in my original 
table with the Scandinavian characters in the header, without no 
fuss.

Thanks for your advice, and wishing all a good Midsummer,

 	-Antti Arppe

On Thu, 8 Jun 2006, Peter Dalgaard wrote:
> so one can only guess that you have a local or Mac-specific setup
> issue.
On Thu, 8 Jun 2006, Prof Brian Ripley wrote:
> If so, you need to investigate the locale in use, as which letters are valid
> depends on the locale: on Linux UTF-8 locales all letters in all languages
> are valid in R names, but that is not necessarily the MacOS interpretation.
> (Invalid characters in names will be converted to ., and if the locale is
> wrong so may be the interpretation of bytes as characters.)
> You might find more informed help on the r-sig-mac list.


From tron at mokk.bme.hu  Thu Jun 22 18:32:54 2006
From: tron at mokk.bme.hu (Viktor Tron)
Date: Thu, 22 Jun 2006 17:32:54 +0100
Subject: [R] legend title in effects plot
Message-ID: <op.tbj2w4rgl59tfy@viktor-trons-powerbook58.local>

Could someone tell how I can change/remove the legend title in a
(multiline) effects plot?
Thanks
V

This is virtually a resubmission of
https://stat.ethz.ch/pipermail/r-help/2006-May/106340.html


-- 
Using Opera's revolutionary e-mail client: http://www.opera.com/mail/


From rvaradhan at jhmi.edu  Thu Jun 22 18:44:39 2006
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Thu, 22 Jun 2006 12:44:39 -0400
Subject: [R] Why different results with different initial values for
	MLE(optim)!
In-Reply-To: <BAY107-DAV6C38E8CD62F7504EB210DF0850@phx.gbl>
Message-ID: <000501c6961b$27848f00$7c94100a@win.ad.jhu.edu>

It is difficult to say much since you haven't described your likelihood
function.  However, there are some general things that you can do: (1) make
sure that you are "maximizing", by defining your objective function as the
negative of likelihood, (2) use log-likelihood rather than the likelihood
function, (3) look at the value of the objective function at different
converged points to see whether you truly have multiple local maxima or if
the likelihood is very flat near the optimum, (4) look at convergence
criterion from the output to see whether you have true convergence, and (5)
try different optimization techniques in "optim" or try "fitdistr" function
in MASS library.

Ravi.

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
Webpage: http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html 
--------------------------------------------------------------------------
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Xin
> Sent: Thursday, June 22, 2006 12:10 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Why different results with different initial values for
> MLE(optim)!
> 
> Hi, All:
> 
>     I used optim() to minimise likelihood function for fitting the data to
> a partiuclar distribution. The function is converged and the value of log-
> likelihood is different when I change the intial value.
> 
>     Whether it means the program does not work well?
> 
>    Thanks!
> 
>    Xin
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html


From gunter.berton at gene.com  Thu Jun 22 18:44:33 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 22 Jun 2006 09:44:33 -0700
Subject: [R] High breakdown/efficiency statistics -- was RE: Rosner's test
In-Reply-To: <17562.23802.793188.600782@stat.math.ethz.ch>
Message-ID: <001701c6961b$2452fe20$0e7e11ac@gne.windows.gene.com>

Many thanks for this Martin. There now are several packages with what appear
to be overlapping functions (or at least algorithms). Besides those you
mentioned, "robust" and "roblm" are at least two others. Any recommendations
about how or whether to choose among these for us enthusiastic but
non-expert users?


Cheers,
Bert
 

> -----Original Message-----
> From: Martin Maechler [mailto:maechler at stat.math.ethz.ch] 
> Sent: Thursday, June 22, 2006 2:04 AM
> To: Berton Gunter
> Cc: 'Robert Powell'; r-help at stat.math.ethz.ch
> Subject: Re: [R] Rosner's test
> 
> >>>>> "BertG" == Berton Gunter <gunter.berton at gene.com>
> >>>>>     on Tue, 13 Jun 2006 14:34:48 -0700 writes:
> 
>     BertG> RSiteSearch('Rosner') ?RSiteSearch or search directly
>     BertG> from CRAN.
> 
>     BertG> Incidentally, I'll repeat what I've said
>     BertG> before. Don't do outlier tests.  They're
>     BertG> dangerous. Use robust methods instead.
> 
> Yes, yes, yes!!!
> 
> Note that  rlm() or cov.rob()  from recommended package MASS
> will most probably be sufficient for your needs.
> 
> For slightly newer methodology, look at package 'robustbase', or
> also 'rrcov'.
> 
> Martin Maechler, ETH Zurich
> 
>     BertG> -- Bert Gunter Genentech Non-Clinical Statistics
>     BertG> South San Francisco, CA
>  
>     BertG> "The business of the statistician is to catalyze the
>     BertG> scientific learning process."  - George E. P. Box
>  
>  
>


From john.emerson at yale.edu  Thu Jun 22 18:44:56 2006
From: john.emerson at yale.edu (John W Emerson)
Date: Thu, 22 Jun 2006 12:44:56 -0400 (EDT)
Subject: [R] Package structure question (progress!)
Message-ID: <Pine.LNX.4.63.0606221240310.953@ares.its.yale.edu>


Thanks to Jeff Laake for giving me a starting point.
It appears my zipping should have been:

 	zip -r hello.zip hello

essentially creating an outer folder having the package
name inside the zip file.  It still didn't quite work,
with an error as follows:

> utils:::menuInstallLocal()
updating HTML package descriptions
Warning message:
no package 'file678418be' was found in: packageDescription(i, lib.loc = 
lib, field = "Title", encoding = "UTF-8")
>

The warning doesn't mean anything to me, but I suspect that because
I zipped the linux files, it is possible that the unix end-of-line
difference might pose a problem.  I'll try doing the obvious
conversion and will report back.

As I wrote to Jeff, I'll be happy to document this experience once
I understand it.  For those of us interested in spreading R source
with no C/Fortran (e.g. packages in their simplest form), this
packaging should be fairly simple from first principles without
having to rely on special tools.

Regards,

Jay


From Max.Kuhn at pfizer.com  Thu Jun 22 18:50:21 2006
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Thu, 22 Jun 2006 12:50:21 -0400
Subject: [R]  Package struction question (second try)
Message-ID: <71257D09F114DA4A8E134DEAC70F25D3056E95A9@groamrexm03.amer.pfizer.com>

Jay,

You should use "RCMD install --build pkgName" to create the zip file on
Windows. The zip files you see on CRAN are Windows binaries. You could
also used "RCMD build pkgName", but I remember seeing a post a while
back saying that using install instead of build was best (anyone - is
that true?).

See you next week in Groton,

Max

<snip>

Sorry, gmail seemed to have made an attachment out of my first attempted

post.  Trying again:

------------------------------

At the encouragement of many at UseR, I'm trying to build my first real
package. I have no C/Fortran code, just plain old R code, so it should
be
rocket science.  On a Linux box, I used package.skeleton() to create a 
basic package containing just one "hello world" type of function.  I 
edited the DESCRIPTION file, changin the package name appropriately.  I 
edited the hello.Rd file.  Upon running R CMD check hello, the only 
warning had to do with the fact that src/ was empty (obviously I had no 
source in such a simple package).  I doubt this is a problem.

I was able to install and use the package successfully on the Linux
system
from the .tar.gz file, so far so good!  Next, on to Windows, where the
problem arose:

I created a zip file from inside the package directory:

zip -r ../hello.zip ./*

When I moved this to my Windows machine and tried to install the package

using the GUI, I received the following error:

> utils:::menuInstallLocal()
Error in unpackPkg(pkgs[i], pkgnames[i], lib, installWithVers) :
         malformed bundle DESCRIPTION file, no Contains field

I only found one mention of this in my Google search, with no reply to
the
thread.  The Contains field appears to be used for bundles, but I'm
trying
to create a package, not a bundle.  This leads me to believe that a
simple
zipping of the package directory structure is not the correct format for
Windows.

Needless to say, there appears to be wide agreement that making packages
requires precision, but fundamentally a package should (as described in 
the
documentation) just be a collection of files and folders organized a 
certain
way.  If someone could point me to documentation I may have missed that
explains this, I would be grateful.

Regards,

Jay

-- 
John W. Emerson (Jay)
Assistant Professor of Statistics
Yale University
http://www.stat.yale.edu/~jay

 	[[alternative HTML version deleted]]


    * Previous message: [R] Why different results with different initial
values for MLE (optim)!
    * Next message: [R] legend title in effects plot
    * Messages sorted by: [ date ] [ thread ] [ subject ] [ author ]

More information about the R-help mailing list
----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From rvaradhan at jhmi.edu  Thu Jun 22 18:53:23 2006
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Thu, 22 Jun 2006 12:53:23 -0400
Subject: [R] useR! Thanks
In-Reply-To: <449678ED.1080809@vanderbilt.edu>
Message-ID: <000601c6961c$600f6470$7c94100a@win.ad.jhu.edu>

Dear All,

I would like to totally echo Frank Harrell's appreciation of both UseR! and
Vienna.  I had a wonderful time despite being jet-lagged for the whole time!

I was also wondering whether the minutes of the final session on "getting
credit for contributions to computational statistics software" could be made
available to the participants.

Best,
Ravi.

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
Webpage: http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html 
--------------------------------------------------------------------------

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Frank E Harrell Jr
> Sent: Monday, June 19, 2006 6:14 AM
> To: RHELP
> Subject: [R] useR! Thanks
> 
> After attending my first useR! conference I want to thank the organizers
> for doing a wonderful job and the presenters for their high quality
> presentations and stimulating ideas.   The conference venue was
> excellent and of course Vienna is one of the greatest cities in the
> world to visit.  useR! is one of the most fun conferences I've attended.
> 
> Thanks again!
> --
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                       Department of Biostatistics   Vanderbilt University
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html


From stefano.sofia at regione.marche.it  Thu Jun 22 19:27:27 2006
From: stefano.sofia at regione.marche.it (Stefano Sofia)
Date: Thu, 22 Jun 2006 19:27:27 +0200
Subject: [R] QUERY: correlation between two time series
Message-ID: <163CA7BD55F0B84AAD787CE6192698D70C5935@GANDALF.regionemarche.intra>

I am writing on the behalf of a colleague. 
 
She needs to evaluate the correlation between two time series. 
How can this be performed through R? (how many types of correlations can be performed and which are the commands to do it?)
 
thank you for your help and attention
Stefano
 

From murdoch at stats.uwo.ca  Thu Jun 22 19:30:51 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 22 Jun 2006 13:30:51 -0400
Subject: [R] Basic package structure question
In-Reply-To: <d4588dec0606220908w415cc012y8fa678148cef44e3@mail.gmail.com>
References: <d4588dec0606220908w415cc012y8fa678148cef44e3@mail.gmail.com>
Message-ID: <449AD3CB.4050202@stats.uwo.ca>

Jay Emerson wrote:
> At the encouragement of many at UseR, I'm trying to build my first real
> package. I have no C/Fortran code, just plain old R code, so it should be
> rocket science.  On a Linux box, I used package.skeleton() to create a basic
> package containing just one "hello world" type of function.  I edited the
> DESCRIPTION file, changin the package name appropriately.  I edited the
> hello.Rd file.  Upon running R CMD check hello, the only warning had to do
> with the fact that src/ was empty (obviously I had no source in such a
> simple package).  I doubt this is a problem.
>
> I was able to install and use the package successfully on the Linux system
> from the .tar.gz file, so far so good!  Next, on to Windows, where the
> problem arose:
>
> I created a zip file from inside the package directory: zip -r ../hello.zip
> ./*
>
>   
Which package directory, the source or the installed copy?  I think this 
might work in the installed copy, but would not work on the source.  
It's not the documented way to build a binary zip, though.
> When I moved this to my Windows machine and tried to install the package, I
> received the following error:
>
>   
>> utils:::menuInstallLocal()
>>     
> Error in unpackPkg(pkgs[i], pkgnames[i], lib, installWithVers) :
>         malformed bundle DESCRIPTION file, no Contains field
>
> I only found one mention of this in my Google search, with no reply to the
> thread.  The Contains field appears to be used for bundles, but I'm trying
> to create a package, not a bundle.  This leads me to believe that a simple
> zipping of the package directory structure is not the correct format for
> Windows.
>
> Needless to say, there appears to be wide agreement that making packages
> requires precision, but fundamentally a package should (as described in the
> documentation) just be a collection of files and folders organized a certain
> way.  If someone could point me to documentation I may have missed that
> explains this, I would be grateful.
>   

I think the "organized in a certain way" part is actually important.  
Using R CMD install --build is the documented way to achieve this.  It's 
not trivial to do this on Windows, because you need to set up a build 
environment first, but it's not horribly difficult.

Duncan Murdoch
> Regards,
>
> Jay
>
>


From Achim.Zeileis at wu-wien.ac.at  Thu Jun 22 19:30:11 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 22 Jun 2006 19:30:11 +0200
Subject: [R] useR! Thanks
In-Reply-To: <000601c6961c$600f6470$7c94100a@win.ad.jhu.edu>
References: <449678ED.1080809@vanderbilt.edu>
	<000601c6961c$600f6470$7c94100a@win.ad.jhu.edu>
Message-ID: <20060622193011.eb8ef9de.Achim.Zeileis@wu-wien.ac.at>

On Thu, 22 Jun 2006 12:53:23 -0400 Ravi Varadhan wrote:

> Dear All,
> 
> I would like to totally echo Frank Harrell's appreciation of both
> UseR! and Vienna.  I had a wonderful time despite being jet-lagged
> for the whole time!

Thanks for the positive feedback!

> I was also wondering whether the minutes of the final session on
> "getting credit for contributions to computational statistics
> software" could be made available to the participants.

The plan is to provide the original presentation slides of the
panelists and a video of the whole panel disc, and probably some minutes
and/or further information.

We'll send out an info mail, once this and other materials are
available from the Web page.

Best,
Z


From justin.rapp at gmail.com  Thu Jun 22 20:03:14 2006
From: justin.rapp at gmail.com (Justin Rapp)
Date: Thu, 22 Jun 2006 14:03:14 -0400
Subject: [R] Strings to Numbers?
Message-ID: <e11df81d0606221103t3d3da9b7xacb8ecb2270d4fb1@mail.gmail.com>

I am running the following command.

for( i in 1:378){

	for (j in 1:5) {
				*********Year [((i-1)*5)+j,1]<- yearly[i,2];*********
				Year [((i-1)*5)+j,2] <- j;
				Year [((i-1)*5)+j,3] <- yearly[i,(j+2)];
				j <- (j+1);
			}
		i <- (i+1)
		}

The array referenced yearly[i,2] contains conference names i.e. BIG12,
SEC, BIG10.  The loop executes fine except for the fact that instead
of the strings from yearly, Year[,1] contains numbers.

Any suggestions.
-- 
Justin Rapp
409 S. 22nd St.
Apt. 1
Philadelphia, PA 19146
Cell:(267)252.0297


From andy_liaw at merck.com  Thu Jun 22 20:21:17 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 22 Jun 2006 14:21:17 -0400
Subject: [R] High breakdown/efficiency statistics -- was RE: Rosner's
 test [Broadcast]
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02700173@usctmx1106.merck.com>

What would be nice is to have something like a "robust" task view...

Andy 

From: Berton Gunter
> 
> Many thanks for this Martin. There now are several packages 
> with what appear to be overlapping functions (or at least 
> algorithms). Besides those you mentioned, "robust" and 
> "roblm" are at least two others. Any recommendations about 
> how or whether to choose among these for us enthusiastic but 
> non-expert users?
> 
> 
> Cheers,
> Bert
>  
> 
> > -----Original Message-----
> > From: Martin Maechler [mailto:maechler at stat.math.ethz.ch]
> > Sent: Thursday, June 22, 2006 2:04 AM
> > To: Berton Gunter
> > Cc: 'Robert Powell'; r-help at stat.math.ethz.ch
> > Subject: Re: [R] Rosner's test
> > 
> > >>>>> "BertG" == Berton Gunter <gunter.berton at gene.com>
> > >>>>>     on Tue, 13 Jun 2006 14:34:48 -0700 writes:
> > 
> >     BertG> RSiteSearch('Rosner') ?RSiteSearch or search directly
> >     BertG> from CRAN.
> > 
> >     BertG> Incidentally, I'll repeat what I've said
> >     BertG> before. Don't do outlier tests.  They're
> >     BertG> dangerous. Use robust methods instead.
> > 
> > Yes, yes, yes!!!
> > 
> > Note that  rlm() or cov.rob()  from recommended package 
> MASS will most 
> > probably be sufficient for your needs.
> > 
> > For slightly newer methodology, look at package 
> 'robustbase', or also 
> > 'rrcov'.
> > 
> > Martin Maechler, ETH Zurich
> > 
> >     BertG> -- Bert Gunter Genentech Non-Clinical Statistics
> >     BertG> South San Francisco, CA
> >  
> >     BertG> "The business of the statistician is to catalyze the
> >     BertG> scientific learning process."  - George E. P. Box
> >  
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From jfox at mcmaster.ca  Thu Jun 22 20:27:37 2006
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 22 Jun 2006 14:27:37 -0400
Subject: [R] legend title in effects plot
In-Reply-To: <op.tbj2w4rgl59tfy@viktor-trons-powerbook58.local>
Message-ID: <web-130083200@cgpsrv2.cis.mcmaster.ca>

Dear Viktor,

I didn't anticipate that someone would want to remove the legend title,
and so I didn't make allowance for this. More generally, there are so
many different ways of plotting effects that it's hard to anticipate
what people will want to do, and I intended the effect plots that are
produced by plot.effect() to be primarily for data analysis rather than
as presentation graphs.

Two suggestions: (1) You could modify effects:::plot.effect to do what
you want; or (2) you could simply use the information in the effect
object to make a custom plot.

I hope that this helps,
 John

On Thu, 22 Jun 2006 17:32:54 +0100
 "Viktor Tron" <tron at mokk.bme.hu> wrote:
> Could someone tell how I can change/remove the legend title in a
> (multiline) effects plot?
> Thanks
> V
> 
> This is virtually a resubmission of
> https://stat.ethz.ch/pipermail/r-help/2006-May/106340.html
> 
> 
> -- 
> Using Opera's revolutionary e-mail client: http://www.opera.com/mail/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/


From john.emerson at yale.edu  Thu Jun 22 21:19:22 2006
From: john.emerson at yale.edu (John W Emerson)
Date: Thu, 22 Jun 2006 15:19:22 -0400 (EDT)
Subject: [R] Package structure question (resolution)
Message-ID: <Pine.LNX.4.63.0606221505520.953@ares.its.yale.edu>


Thanks to Max Kuhn, Jeff Laake, Duncan Murdoch, and
Dirk Eddelbuettel, for their tips that eventually
helped me get my teeth around this one.

I'm in the process of documenting my experience, in case
others find it useful.  Please email me if you want to add
to this or clarify anything.

However, to close this thread, in a nutshell, working in Linux:

1. package.skeleton().
2. Make obvious changes to files.
3. R CMD check packagename.
4. R CMD build --binary packagename.
5. The result of 4 should be a working Linux package, .tar.gz.
6. Unpack the .tar.gz file by hand, and zip the resulting directory.
7. The result of 6 should be a working Windows package, .zip.

If there is an easier way to get both the Linux and Windows
packages (without installing any extra specialized tools),
I'd be interested.

Problems encountered:

1. The --binary option seems necessary now, but perhaps it
wasn't in earlier versions?  Not sure.  This may have resulted
in my quick success in Linux (where we are running an older
version of R), but problems in Windows (newer version).

2. In the process of making early mistakes, my Windows installation
was corrupted by some odd folders in R/library that generated
the following error message:

> utils:::menuInstallLocal()
updating HTML package descriptions
Warning message:
no package 'file678418be' was found in: packageDescription(i, lib.loc =
lib, field = "Title", encoding = "UTF-8")

I'm hesitant to call this a bug (because there might be a good reason
for it), but the solution was simply to delete the folders named
things like "file678418be" inside R/library.  After doing this I
had no problem, though.  Go figure.

Cheers,

Jay


From JeeBee at troefpunt.nl  Thu Jun 22 21:20:53 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Thu, 22 Jun 2006 21:20:53 +0200
Subject: [R] Strings to Numbers?
References: <e11df81d0606221103t3d3da9b7xacb8ecb2270d4fb1@mail.gmail.com>
Message-ID: <pan.2006.06.22.19.20.51.299311@troefpunt.nl>


- The increment to i and j in your loops are done already, so remove them.
  try this to see what I mean:
  for(i in seq(1,5)) { print(i) }
- Provide us with a small (not with 378 rows) example that works,
  hence give us your yearly.
  Try str(yearly) to see for yourself what's in there exactly.

  My guess is there are factors in yearly (so your conference names are
  not strings as you think they are).
  See ?factor (type "?factor" in R without the quotes).

  If you used something like read.csv to import your yearly from a file,
  try adding as.is=TRUE there, that might also solve your problem.

JeeBee.

> I am running the following command.
> 
> for( i in 1:378){
> 
> 	for (j in 1:5) {
> 				*********Year [((i-1)*5)+j,1]<- yearly[i,2];*********
> 				Year [((i-1)*5)+j,2] <- j;
> 				Year [((i-1)*5)+j,3] <- yearly[i,(j+2)];
> 				j <- (j+1);
> 			}
> 		i <- (i+1)
> 		}
> 
> The array referenced yearly[i,2] contains conference names i.e. BIG12,
> SEC, BIG10.  The loop executes fine except for the fact that instead
> of the strings from yearly, Year[,1] contains numbers.
> 
> Any suggestions.


From jeff.horner at vanderbilt.edu  Thu Jun 22 22:40:34 2006
From: jeff.horner at vanderbilt.edu (Jeffrey Horner)
Date: Thu, 22 Jun 2006 15:40:34 -0500
Subject: [R] useR! Thanks
In-Reply-To: <20060622193011.eb8ef9de.Achim.Zeileis@wu-wien.ac.at>
References: <449678ED.1080809@vanderbilt.edu>	<000601c6961c$600f6470$7c94100a@win.ad.jhu.edu>
	<20060622193011.eb8ef9de.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <449B0042.5060902@vanderbilt.edu>

Achim Zeileis wrote:
[...]
> The plan is to provide the original presentation slides of the
> panelists and a video of the whole panel disc, and probably some minutes
> and/or further information.

Did you happen to video Ivan Mizera's talk as well? I'd really like to 
see that again!

-- 
Jeffrey Horner       Computer Systems Analyst         School of Medicine
615-322-8606         Department of Biostatistics   Vanderbilt University


From mschwartz at mn.rr.com  Thu Jun 22 23:05:37 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 22 Jun 2006 16:05:37 -0500
Subject: [R] useR! Thanks
In-Reply-To: <449B0042.5060902@vanderbilt.edu>
References: <449678ED.1080809@vanderbilt.edu>
	<000601c6961c$600f6470$7c94100a@win.ad.jhu.edu>
	<20060622193011.eb8ef9de.Achim.Zeileis@wu-wien.ac.at>
	<449B0042.5060902@vanderbilt.edu>
Message-ID: <1151010337.6428.43.camel@localhost.localdomain>

On Thu, 2006-06-22 at 15:40 -0500, Jeffrey Horner wrote:
> Achim Zeileis wrote:
> [...]
> > The plan is to provide the original presentation slides of the
> > panelists and a video of the whole panel disc, and probably some minutes
> > and/or further information.
> 
> Did you happen to video Ivan Mizera's talk as well? I'd really like to 
> see that again!

Hear! Hear!

A most enjoyable presentation!

I now suffer from one newly engendered fear:

   Becoming an abuseR!

;-)


Thanks to the Organizers, Hosts and Presenters for another wonderful
meeting. It is great to see the continued healthy growth and diversity
in this community. I look forward to the next one.

Best regards,

Marc Schwartz


From bolker at ufl.edu  Thu Jun 22 23:56:03 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Thu, 22 Jun 2006 21:56:03 +0000 (UTC)
Subject: [R] QUERY: correlation between two time series
References: <163CA7BD55F0B84AAD787CE6192698D70C5935@GANDALF.regionemarche.intra>
Message-ID: <loom.20060622T234503-31@post.gmane.org>

Stefano Sofia <stefano.sofia <at> regione.marche.it> writes:

> 
> I am writing on the behalf of a colleague. 
> 
> She needs to evaluate the correlation between two time series. 
> How can this be performed through R? (how many types of correlations can be
performed and which are the
> commands to do it?)
> 

  This question is too broad, and the first answer is
the usual "please read the
posting guide/documentation/etc." -- as a start, try 

?cor
help.search("correlation")
help.search("time series")

RSiteSearch( etc. )

  taking a wild guess at your friend's statistical issue,
she might needs to estimate cross-correlations between
the series in the presence of autocorrelation in each time
series, which is typically done using a multiple ARMA
model:

http://finzi.psych.upenn.edu/R/Rhelp02a.new/archive/24612.html

suggests that the dse bundle has such capabilities.

  This list is *not* a general-purpose statistics query list, but
if your friend (or you) submitted a much more careful description
of the problem, and showed more evidence of (1) having a good idea
of the general statistical background of the problem (e.g. "I think
model X would be appropriate based on reading the literature in
my area but I can't figure out whether R can do it" or 
"everyone in my field uses model X but it's not appropriate
in this case because ...") and (2) having tried to use the existing
R documentation (RSiteSearch, etc.) to find the answers to your
questions --- then you'd be much more likely to get a useful
response, even if it was not strictly an R question.

  Ben Bolker


From btyner at gmail.com  Fri Jun 23 00:35:18 2006
From: btyner at gmail.com (Benjamin Tyner)
Date: Thu, 22 Jun 2006 18:35:18 -0400
Subject: [R] xyplot, type="b"
In-Reply-To: <44999C19.9090907@stat.purdue.edu>
References: <4498B6D9.2050004@stat.purdue.edu>	<38b9f0350606202016y1ded6022ne0e3ecde374780d3@mail.gmail.com>	<4498BB1D.5000306@stat.purdue.edu>	<4498BE6F.8060001@stat.auckland.ac.nz>
	<4498C426.7040804@stat.purdue.edu> <4498CA6F.6050504@pdf.com>
	<44999C19.9090907@stat.purdue.edu>
Message-ID: <449B1B26.4090906@stat.purdue.edu>

For those who are interested, here is a solution using grid. This
preserves some of the original spirit of do_plot_xy, but is more
satisfactory when x and y are on different scales:

my.panel <- function(x, y, cex=1, ...){
require(grid)
panel.xyplot(x, y, cex = cex, ...)
a <- 0.5 * cex * convertWidth(unit(1, "char"), "native", valueOnly=TRUE)
b <- 0.5 * cex * convertHeight(unit(1, "char"), "native",valueOnly=TRUE)
for(i in 1:length(x)){
dx <- x[i] - x[i - 1]
dy <- y[i] - y[i - 1]
f <- 1 / sqrt((dx/a)^2 + (dy/b)^2)
if((i > 1) & identical(f < 0.5, TRUE)){
panel.lines(x = c(x[i - 1] + f * dx, x[i] - f * dx),
y = c(y[i - 1] + f * dy, y[i] - f * dy))
}
}
}

# example
n <- 50
dat <- data.frame(x = runif(n), y = runif(n))
dat <- dat[order(dat$x), ]
myplot <- xyplot(y~x, data = dat, pch=".", panel = my.panel)
print(myplot)

(It's a little slow due to looping panel.lines.)

Ben

Benjamin Tyner wrote:

>Unfortunately this is not quite right; to do it correctly it seems one
>has to address two problems:
>
>1. how to get the size of the default 'cex' to use for 'd' (do_plot_xy uses 'GConvertYUnits' to accomplish this)
>2. figure out how to achieve the same effect as what 'GConvert(&xx, &yy, USER, INCHES, dd)' does in do_plot_xy. Otherwise, the gap sizes are not constant.
>
>(1) sounds easy but I don't know the answer offhand. (2) seems more subtle. Any suggestions would be greatly appreciated.
>
>Ben
>  
>


From spencer.graves at pdf.com  Fri Jun 23 02:23:52 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 22 Jun 2006 17:23:52 -0700
Subject: [R] Effect size in mixed models
In-Reply-To: <449762E8.8000507@pdf.com>
References: <002c01c6919d$39416d80$6400a8c0@brungio> <449762E8.8000507@pdf.com>
Message-ID: <449B3498.8060901@pdf.com>

	  I just learned that my earlier suggestion was wrong.  It's better to 
compute the variance of the predicted or fitted values and compare those 
with the estimated variance components.

	  To see how to do this, consider the following minor modification of 
an example in the "lme" documentation:

fm1. <- lme(distance ~ age, data = Orthodont, random=~1)
fm2. <- lme(distance ~ age + Sex, data = Orthodont, random = ~ 1)

# str(fm1.) suggested the following:
 > var(fm2.$fitted[, "fixed"]-fm1.$fitted[, "fixed"])
[1] 1.312756
 > VarCorr(fm1.)[, 1]
(Intercept)    Residual
  "4.472056"  "2.049456"
 > VarCorr(fm2.)[, 1]
(Intercept)    Residual
  "3.266784"  "2.049456"

	  In this example, the subject variance without considering "Sex" was 
4.47 but with "Sex" in the model, it dropped to 3.27, while the Residual 
variance remained unchanged at 2.05.  The difference between 
fm2.$fitted[, "fixed"] and fm1.$fitted[, "fixed"] is the change in the 
predictions generated by the addition of "Sex" to the model.  The 
variance of that difference was 1.31.  Note that 3.27 + 1.31 = 4.58, 
which is moderately close to 4.47.

	  In sum, I think we can get a reasonable estimate of the size of an 
effect from the variance of the differences in the "fixed" portion of 
the fitted model.

	  Comments?
	  Hope this helps.
	  Spencer Graves

Spencer Graves wrote:
>       You have asked a great question:  It would indeed be useful to 
> compare the relative magnitude of fixed and random effects, e.g. to 
> prioritize efforts to better understand and possibly manage processes 
> being studied.  I will offer some thoughts on this, and I hope if there 
> are errors in my logic or if someone else has a better idea, we will 
> both benefit from their comments.
> 
>       The ideal might be an estimate of something like a mean square for 
> a particular effect to compare with an estimated variance component.
> Such mean squares were a mandatory component of any analysis of variance 
> table prior to the (a) popularization of generalized linear models and 
> (b) availability of software that made it feasible to compute maximum 
> likelihood estimates routinely for unbalanced, mixed-effects models. 
> However, anova(lme(...)) such mean squares are for most purposes 
> unnecessary cluster in a modern anova table.
> 
>       To estimate a mean square for a fixed effect, consider the 
> following log(likelihood) for a mixed-effects model:
> 
>       lglk = (-0.5)*(n*log(2*pi*var.e)-log(det(W)) + 
> t(y-X%*%b)%*%W%*%(y-X%*%b)/var.e),
> 
> where n = the number of observations,
> 
>       b = the fixed-effect parameter variance,
> 
> and the covariance matrix of the residuals, after integrating out the 
> random effects is var.e*solve(W).  In this formulation, the matrix "W" 
> is a function of the variance components.  Since they are not needed to 
> compute the desired mean squares, they are suppressed in the notation here.
> 
>       Then, the maximum likelihood estimate of
> 
>       var.e = SSR/n,
> 
> where SSR = t(y-X%*%b)%*%W%*%(y-X%*%b).
> 
>       Then
> 
>       mle.lglk = (-0.5)*(n*(log(2*pi*SSR/n)-1)-log(det(W))).
> 
>       Now let
> 
>       SSR0 = this generalized sum of squares of residuals (SSR) without 
> effect "1",
> 
> and
> 
>       SSR1 = this generalized SSR with this effect "1".
> 
>       If I've done my math correctly, then
> 
>       D = deviance = 2*log(likelihood ratio)
>         = (n*log(SSR0/SSR1)+log(det(W1)/det(W0)))
> 
>       For roughly half a century, a major part of "the analysis of 
> variance" was the Pythagorean idea that the sum of squares under H0 was 
> the sum of squares under H1 plus the sum of squares for effect "1":
> 
>       SSR0 = SS1 + SSR1.
> 
>       Whence,
> 
>       exp((D/n)-log(det(W1)/det(W0))) = 1+SS1/SSR1.
> 
> Thus,
> 
>       SS1 = SSR1*(exp((D/n)-log(det(W1)/det(W0)))-1).
> 
>       If the difference between deg(W1) and det(W0) can be ignored, we get:
> 
>       SS1 = SSR1*(exp((D/n)-1).
> 
>       Now compute MS1 = SS1/df1, and compare with the variance components.
> 
>       If there is a flaw in this logic, I hope someone will disabuse me 
> of it.
> 
>       If this seems too terse or convoluted to follow, please provide a 
> simple, self-contained example, as suggested in the posting guide! 
> "www.R-project.org/posting-guide.html".  You asked a theoretical 
> question, you got a theoretical answer.  If you want a concrete answer, 
> it might help to pose a more concrete question.
> 
>       Hope this helps.
>       Spencer Graves   
> 
> Bruno L. Giordano wrote:
>> Hello,
>> Is there a way to compare the relative relevance of fixed and random 
>> effects in mixed models? I have in mind measures of effect size in 
>> ANOVAs, and would like to obtain similar information with mixed models.
>>
>> Are there information criteria that allow to compare the relevance of 
>> each of the effects in a mixed model to the overall fit?
>>
>> Thank you,
>>     Bruno
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>


From murdoch at stats.uwo.ca  Thu Jun 22 23:09:20 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 22 Jun 2006 17:09:20 -0400
Subject: [R] rank(x,y)?
In-Reply-To: <971536df0606211902i6f7242d7qe28ae580a1290b3d@mail.gmail.com>
References: <4499854B.8020707@stats.uwo.ca>	<x21wtil6vw.fsf@turmalin.kubism.ku.dk>
	<44998C52.7040502@stats.uwo.ca>
	<971536df0606211902i6f7242d7qe28ae580a1290b3d@mail.gmail.com>
Message-ID: <449B0700.6010106@stats.uwo.ca>

Gabor Grothendieck wrote:
> On 6/21/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
>   
>> Peter Dalgaard wrote:
>>     
>>> Duncan Murdoch <murdoch at stats.uwo.ca> writes:
>>>
>>>
>>>       
>>>> Suppose I have two columns, x,y.  I can use order(x,y) to calculate a
>>>> permutation that puts them into increasing order of x,
>>>> with ties broken by y.
>>>>
>>>> I'd like instead to calculate the rank of each pair under the same
>>>> ordering, but the rank() function doesn't take multiple values
>>>> as input.  Is there a simple way to get what I want?
>>>>
>>>> E.g.
>>>>
>>>>  > x <- c(1,2,3,4,1,2,3,4)
>>>>  > y <- c(1,2,3,1,2,3,1,2)
>>>>  > rank(x+y/10)
>>>> [1] 1 3 6 7 2 4 5 8
>>>>
>>>> gives me the answer I want, but only because I know the range of y and
>>>> the size of gaps in the x values.  What do I do in general?
>>>>
>>>>         
>>> Still not quite general, but in the absence of ties:
>>>
>>>
>>>       
>>>> z[order(x,y)]<-1:8
>>>> z
>>>>
>>>>         
>>> [1] 1 3 6 7 2 4 5 8
>>>
>>>
>>>       
>> Thanks to all who have replied.  Unfortunately for me, ties do exist,
>> and I'd like them to get identical ranks.  John Fox's suggestion would
>> handle ties properly, but I'm worried about rounding error giving
>> spurious ties.
>>
>>     
>
> Try this variant of my prior solution:
>
> (order(order(x,y)) + rev(order(order(rev(x), rev(y)))))/2
>
> Note that no arithmetic is done on the original data, only on
> the output of order, so there should not be any worry about
> rounding -- in fact its sufficiently general that the data
> do not have to be numeric, e.g.
>
>   
>> x <- c("a", "a", "b", "a", "c", "d")
>> y <- c("b", "a", "b", "b", "a", "a")
>> (order(order(x,y)) + rev(order(order(rev(x), rev(y)))))/2
>>     
> [1] 2.5 1.0 4.0 2.5 5.0 6.0
>   

This is a very nice solution, thanks!

So now we have equivalents to ties="average" and "first"; ties="random" 
would be easy.  I wonder if it's worth working out ties="max" and 
ties="min" and putting in a new function?

Duncan Murdoch
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From jeffmiller at adsam.com  Fri Jun 23 02:31:51 2006
From: jeffmiller at adsam.com (Jeff Miller)
Date: Thu, 22 Jun 2006 20:31:51 -0400
Subject: [R] hurdle and zip model
Message-ID: <000001c6965c$6c6d0700$6501a8c0@AdSAMJeff>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060622/572c215b/attachment.pl 

From ggrothendieck at gmail.com  Fri Jun 23 04:19:19 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 22 Jun 2006 22:19:19 -0400
Subject: [R] Basic package structure question
In-Reply-To: <449AD3CB.4050202@stats.uwo.ca>
References: <d4588dec0606220908w415cc012y8fa678148cef44e3@mail.gmail.com>
	<449AD3CB.4050202@stats.uwo.ca>
Message-ID: <971536df0606221919u5de6a851u370fbd2846f92644@mail.gmail.com>

On 6/22/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> Jay Emerson wrote:
> > At the encouragement of many at UseR, I'm trying to build my first real
> > package. I have no C/Fortran code, just plain old R code, so it should be
> > rocket science.  On a Linux box, I used package.skeleton() to create a basic
> > package containing just one "hello world" type of function.  I edited the
> > DESCRIPTION file, changin the package name appropriately.  I edited the
> > hello.Rd file.  Upon running R CMD check hello, the only warning had to do
> > with the fact that src/ was empty (obviously I had no source in such a
> > simple package).  I doubt this is a problem.
> >
> > I was able to install and use the package successfully on the Linux system
> > from the .tar.gz file, so far so good!  Next, on to Windows, where the
> > problem arose:
> >
> > I created a zip file from inside the package directory: zip -r ../hello.zip
> > ./*
> >
> >
> Which package directory, the source or the installed copy?  I think this
> might work in the installed copy, but would not work on the source.
> It's not the documented way to build a binary zip, though.
> > When I moved this to my Windows machine and tried to install the package, I
> > received the following error:
> >
> >
> >> utils:::menuInstallLocal()
> >>
> > Error in unpackPkg(pkgs[i], pkgnames[i], lib, installWithVers) :
> >         malformed bundle DESCRIPTION file, no Contains field
> >
> > I only found one mention of this in my Google search, with no reply to the
> > thread.  The Contains field appears to be used for bundles, but I'm trying
> > to create a package, not a bundle.  This leads me to believe that a simple
> > zipping of the package directory structure is not the correct format for
> > Windows.
> >
> > Needless to say, there appears to be wide agreement that making packages
> > requires precision, but fundamentally a package should (as described in the
> > documentation) just be a collection of files and folders organized a certain
> > way.  If someone could point me to documentation I may have missed that
> > explains this, I would be grateful.
> >
>
> I think the "organized in a certain way" part is actually important.
> Using R CMD install --build is the documented way to achieve this.  It's
> not trivial to do this on Windows, because you need to set up a build
> environment first, but it's not horribly difficult.
>
> Duncan Murdoch
> > Regards,
> >
> > Jay

One idea that occurred to me in reading this would be to have a server
that one can send a package to and get back a Windows build to
eliminate having to set up a development environment.  Not sure if
this is feasible, particularly security aspects, but if it were it would
open up package building on Windows to a larger audience.


From spencer.graves at pdf.com  Fri Jun 23 04:20:19 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 22 Jun 2006 19:20:19 -0700
Subject: [R] eacf
In-Reply-To: <5e19a4710606190732o6d105b96hc38fa6e8aef7953b@mail.gmail.com>
References: <5e19a4710606190732o6d105b96hc38fa6e8aef7953b@mail.gmail.com>
Message-ID: <449B4FE3.1070308@pdf.com>

	  RSiteSearch("EACF") led me to
"http://finzi.psych.upenn.edu/R/Rhelp02a.new/archive/43927.html".  In
this note from Oct. 2004, Brian Ripley said the EACF did not exist but
'acf' and 'pacf' did.  Moreover, he said that model selection using acf
and pacf is "a rather old-fashioned idea. Just fit all the target models
and compare their performance."

	What are you trying to do?  If model time series, I suggest you follow 
Prof. Ripley's advice.  If you are conducting research and you want to 
compare the EACF with something else, I doubt if it would be too hard to 
program in R (though I could neither recall nor find the formulae 
involved in the few minutes I spent just now looking).  If you try to 
write an 'eacf' function and get stuck, please submit another post 
describing what you've tried and what does not seem to work (as 
suggested in the posting guide! www.R-project.org/posting-guide.html).

	  Hope this helps.
	  Spencer Graves
p.s.  Your English is just fine.

Michel Helcias wrote:
> I am Brazilian and I don't know how to speak English, for that I apologize
> for my writing.
> I'd like of informations about the extended (sample) autocorrelation
> function (*EACF*) for the time series. where to acquire some related
> command.
> thak you.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From p.murrell at auckland.ac.nz  Fri Jun 23 04:33:21 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 23 Jun 2006 14:33:21 +1200
Subject: [R] xyplot, type="b"
In-Reply-To: <449B1B26.4090906@stat.purdue.edu>
References: <4498B6D9.2050004@stat.purdue.edu>	<38b9f0350606202016y1ded6022ne0e3ecde374780d3@mail.gmail.com>	<4498BB1D.5000306@stat.purdue.edu>	<4498BE6F.8060001@stat.auckland.ac.nz>	<4498C426.7040804@stat.purdue.edu>
	<4498CA6F.6050504@pdf.com>	<44999C19.9090907@stat.purdue.edu>
	<449B1B26.4090906@stat.purdue.edu>
Message-ID: <449B52F1.6090309@stat.auckland.ac.nz>

Hi


Benjamin Tyner wrote:
> For those who are interested, here is a solution using grid. This
> preserves some of the original spirit of do_plot_xy, but is more
> satisfactory when x and y are on different scales:
> 
> my.panel <- function(x, y, cex=1, ...){
> require(grid)
> panel.xyplot(x, y, cex = cex, ...)
> a <- 0.5 * cex * convertWidth(unit(1, "char"), "native", valueOnly=TRUE)
> b <- 0.5 * cex * convertHeight(unit(1, "char"), "native",valueOnly=TRUE)
> for(i in 1:length(x)){
> dx <- x[i] - x[i - 1]
> dy <- y[i] - y[i - 1]
> f <- 1 / sqrt((dx/a)^2 + (dy/b)^2)
> if((i > 1) & identical(f < 0.5, TRUE)){
> panel.lines(x = c(x[i - 1] + f * dx, x[i] - f * dx),
> y = c(y[i - 1] + f * dy, y[i] - f * dy))
> }
> }
> }
> 
> # example
> n <- 50
> dat <- data.frame(x = runif(n), y = runif(n))
> dat <- dat[order(dat$x), ]
> myplot <- xyplot(y~x, data = dat, pch=".", panel = my.panel)
> print(myplot)
> 
> (It's a little slow due to looping panel.lines.)


Here's one way to vectorize the lines ...

my.panel2 <- function(x, y, cex=1, ...){
    panel.xyplot(x, y, cex = cex, ...)
    a <- 0.5 * cex *
         convertWidth(unit(1, "char"), "native", valueOnly=TRUE)
    b <- 0.5 * cex *
         convertHeight(unit(1, "char"), "native",valueOnly=TRUE)
    dx <- diff(x)
    dy <- diff(y)
    f <- 1 / sqrt((dx/a)^2 + (dy/b)^2)
    n <- length(x)
    x1 <- x[1:(n-1)] + f*dx
    x2 <- x[2:n] - f*dx
    y1 <- y[1:(n-1)] + f*dy
    y2 <- y[2:n] - f*dy
    i <- f < 0.5
    panel.segments(x1[i], y1[i], x2[i], y2[i])
}

Paul


> Benjamin Tyner wrote:
> 
> 
>>Unfortunately this is not quite right; to do it correctly it seems one
>>has to address two problems:
>>
>>1. how to get the size of the default 'cex' to use for 'd' (do_plot_xy uses 'GConvertYUnits' to accomplish this)
>>2. figure out how to achieve the same effect as what 'GConvert(&xx, &yy, USER, INCHES, dd)' does in do_plot_xy. Otherwise, the gap sizes are not constant.
>>
>>(1) sounds easy but I don't know the answer offhand. (2) seems more subtle. Any suggestions would be greatly appreciated.
>>
>>Ben
>> 
>>
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From baron at psych.upenn.edu  Fri Jun 23 04:45:33 2006
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Thu, 22 Jun 2006 22:45:33 -0400
Subject: [R] "improvements" in RSiteSearch
Message-ID: <20060623024533.GA27288@psych.upenn.edu>

I've made a few changes in the indices in my R search site.
http://finzi.psych.upenn.edu is the general URL, and RSiteSearch
takes you to
http://finzi.psych.upenn.edu/search.html, which is equivalent to
http://finzi.psych.upenn.edu/nmz.html.

The main change is that I have eliminated "Previous message" and
"Next message" from each page.  (I still have "Next in thread,"
if it exists, and replys.)  These no longer get indexed by the
search engine (Namazu).  As a result, search should be a bit less
slow and the lower items in the list should be less irrelevant.

The down side of all this is that specific reference to other
messages has been broken for the last two days.  Please try not
to refer to other message as Rhelp02a.new or tmp/Rhelp02a.  They
should be just Rhelp02a, and they will be.

To fix this, I need to re-build the mailing list index.  I plan
to do this about 01:00 UTC (2 AM in London, 9 PM in Philadelphia)
tomorrow night (June 23/24).  Things may not work for a couple of
hours around that time.  The old message numbers will be
preserved, however, as Rhelp02a.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron


From btyner at gmail.com  Fri Jun 23 04:53:49 2006
From: btyner at gmail.com (Benjamin Tyner)
Date: Thu, 22 Jun 2006 22:53:49 -0400
Subject: [R] xyplot, type="b"
In-Reply-To: <449B52F1.6090309@stat.auckland.ac.nz>
References: <4498B6D9.2050004@stat.purdue.edu>	<38b9f0350606202016y1ded6022ne0e3ecde374780d3@mail.gmail.com>	<4498BB1D.5000306@stat.purdue.edu>	<4498BE6F.8060001@stat.auckland.ac.nz>	<4498C426.7040804@stat.purdue.edu>
	<4498CA6F.6050504@pdf.com>	<44999C19.9090907@stat.purdue.edu>
	<449B1B26.4090906@stat.purdue.edu>
	<449B52F1.6090309@stat.auckland.ac.nz>
Message-ID: <449B57BD.7060308@stat.purdue.edu>

Most clever! I'd completely forgotten about panel.segments.

Paul Murrell wrote:

>Here's one way to vectorize the lines ...
>
>my.panel2 <- function(x, y, cex=1, ...){
>    panel.xyplot(x, y, cex = cex, ...)
>    a <- 0.5 * cex *
>         convertWidth(unit(1, "char"), "native", valueOnly=TRUE)
>    b <- 0.5 * cex *
>         convertHeight(unit(1, "char"), "native",valueOnly=TRUE)
>    dx <- diff(x)
>    dy <- diff(y)
>    f <- 1 / sqrt((dx/a)^2 + (dy/b)^2)
>    n <- length(x)
>    x1 <- x[1:(n-1)] + f*dx
>    x2 <- x[2:n] - f*dx
>    y1 <- y[1:(n-1)] + f*dy
>    y2 <- y[2:n] - f*dy
>    i <- f < 0.5
>    panel.segments(x1[i], y1[i], x2[i], y2[i])
>}
>
>Paul
>


From p.murrell at auckland.ac.nz  Fri Jun 23 04:57:43 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 23 Jun 2006 14:57:43 +1200
Subject: [R] positioning of separate y-axis labels in xyplot
In-Reply-To: <449029D8.7030203@stat.purdue.edu>
References: <449029D8.7030203@stat.purdue.edu>
Message-ID: <449B58A7.8010901@stat.auckland.ac.nz>

Hi


Benjamin Tyner wrote:
> I like the functionality provided by outer=TRUE, but when it comes time 
> to place separate xlabs or ylabs, I always end up 'eyeballing' it on a 
> case-by-case basis. For example,
> 
> ##begin example
> require(lattice)
> 
> cars.lo <- loess(dist ~ speed, cars)
> 
> print(xyplot(cars.lo$residuals+cars.lo$fitted~cars.lo$x,
>              strip=FALSE,
>              outer=TRUE,
>              layout=c(1,2),
>              ylab="",
>              scales=list(y=list(relation="free",rot=0)),
>              panel=function(x,y,panel.number,...){
>                    if(panel.number==1){
>                       panel.xyplot(x,y)
>                       panel.abline(h=0)
>                    }else{
>                       panel.xyplot(x,y=cars.lo$y)
>                       panel.xyplot(x,y,type="l")
>                    }
>              }))
> 
> require(grid)
> trellis.focus("panel", 1, 1, clip.off=TRUE, highlight=FALSE)
> grid.text("residuals", x=unit(0, "npc") + unit(-2, "lines"),rot=90)
> trellis.focus("panel", 1, 2, clip.off=TRUE, highlight=FALSE)
> grid.text("fitted", x=unit(0, "npc") + unit(-2, "lines"),rot=90)
> ## end example
> 
> In this case, a distance of -2 lines happens to be enough, but one has 
> to make the plot to know this. I'm interested in learning how one can 
> place the ylabs without fear of overlapping the tick labels; i.e., how 
> to use the exact space allocated by ylab="". I'm thinking it must 
> involve viewports?


There is a 'ylab' viewport set up by lattice, although it is just one 
big one for all panels.  Here's what you could do for your example (with 
knowledge of how many rows of panels there are) ...

downViewport(trellis.vpname("ylab"))
# If you want to see where the viewport is ...
# grid.rect(gp=gpar(col="red"), name="temp")
grid.text("residuals", y=0.25, rot=90)
grid.text("fitted", y=0.75, rot=90)
# If you want to remove the rect again ...
# grid.remove("temp")
upViewport(0)

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From spencer.graves at pdf.com  Fri Jun 23 04:52:01 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 22 Jun 2006 19:52:01 -0700
Subject: [R] frechet distance
In-Reply-To: <1150737915.8534.34.camel@blue.chem.psu.edu>
References: <1150737915.8534.34.camel@blue.chem.psu.edu>
Message-ID: <449B5751.9030106@pdf.com>

	  RSiteSearch("Frechet distance") returned only one hit for me just 
now, and that was for a Frechet distribution, as you mentioned.  Google 
found "www.cs.concordia.ca/cccg/papers/39.pdf", which suggests that 
computing it may not be easy.

	  If you'd like more help from this listserve, I can offer two 
suggestions:

	  1.  If you absolutely need the Frechet distance and you can describe 
an algorithm for computing it but get stuck writing a function for it, 
please submit another question outlining what you've tried and that 
obstacle you've found.

	  2.  Alternatively, you might describe the more general problem you 
are trying to solve, why you thought the Frechet distance might help and 
invite alternative suggestions.

	  Hope this helps.
	  Spencer Graves	

Rajarshi Guha wrote:
> Hi, is there any package (or source code snippet) that will evaluate the
> Frechet distance for curves represented as sets of points?
> 
> Searching around only threw up references to a Frechet distribution.
> 
> Thanks,
> 
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> 
> If you believe in telekinesis, raise my hand.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From spencer.graves at pdf.com  Fri Jun 23 05:22:03 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 22 Jun 2006 20:22:03 -0700
Subject: [R] Nested variance-covariance matrix in Multilevel model
In-Reply-To: <449703CD.5030505@vcu.edu>
References: <449703CD.5030505@vcu.edu>
Message-ID: <449B5E5B.5030701@pdf.com>

	  What's your error message?  I see a syntax error in

random=list(Probe=pdBlocked(list(~1,pdCompSymm(~1
),End=~1,ProbeNo=pdCompSymm(~1))

	  I count 5 open parens and 3 close parens.  I know that might have 
been part of what you copied into this email and not what you gave R. 
However, it sounds like you have several problems.

	  Have you seen Pinheiro and Bates (2000) Mixed-effects models in S and 
S-PLUS (Springer)?  This is the premier (almost mandatory) reference for 
'lme'.  Moreover, the "~\library\nlme\scripts" subdirectory of the R 
installation directory (at least under Windows) contains files "ch01.R", 
"ch02.R", ..., "ch06.R", "ch08.R" containing essentially all the S 
commands used in the book, organized by chapter.  This can save endless 
hassles with spelling problems (and minor syntax differences between 
S-Plus and R).  Sections 1.5 and 1.6, pp. 40-72, discuss analyses if two 
different nested experiments.

	  If just want a standard nested analysis, this should be adequate.  If 
you have other special things happening that I missed in skimming your 
email, please explain why the analyses described in Pinheiro and Bates 
do you apply.  In doing so, please include a simple, self-contained 
example showing what you tried and why you think it's not adequate. 
Please don't send us a megabyte of data.  Instead, please either (a) 
modify an example from the book or some other R documentation  or (b) 
generate phony data with a very few lines of R code.

	  Hope this helps.
	  Spencer Graves

Tobias Guennel wrote:
 > I completely forgot to supply the R code I tried:
 > 
vov1i2<-read.table("VOV1_INHIBITED6-16-2006-13h35min33sec.txt",header=TRUE)
 > 
test.lme<-lme(fixed=log2(Intensity)~End+logpgc,random=list(Probe=pdBlocked(list(~1,pdCompSymm(~1 

 > ),End=~1,ProbeNo=pdCompSymm(~1)),data=vov1i2)
 >
 > It doesn't look right to me and it produces an error too.
 >
 > It should be something around those lines though.
 >
 > Tobias Guennel

Tobias Guennel wrote:
> Dear R community,
> 
> I have trouble implementing a nested variance-covariance matrix in the 
> lme function.
> 
> The model has two fixed effects called End and logpgc, the response 
> variable is the logarithm to base 2 of  Intensity ( log2(Intensity) ) 
> and the random effects are called Probe and ProbeNo.
> The model has the following nesting structure: A Pixel is nested within 
> the ProbeNo,the ProbeNo is within the ProbeEnd ( there are two ends for 
> every probe), and the ProbeEnd is within the Probe.
> 
> Now the problem I have is that the variance-covariance structure of the 
> model is quite complex and I can not find the right syntax for fitting 
> it in the lme function.
> 
> The variance-covariance structure  is a block diagonal matrix of the form,
>           V1   0       0
>  V=    0      V2    0
>           0      0        V3
> 
> where V1...V3 are of the structure:
>           v11      v12
>   V1=                          and so on.
>           v21      v22
> 
> V1...V3 are assumed to have a compound symmetric variance-covariance 
> structure and therefore the submatrices are of the form:
>                    Lambda   Delta1       Delta1 ...   Delta1
>                    Delta1      Lambda    Delta1 ...   Delta1
> v11=v22=    .......
> 
>                    Delta1   .....                               Lambda
>                   
>                    Delta2      Delta2       Delta2 ...   Delta2
>                    Delta2      Delta2       Delta2 ...   Delta2
> v12=v21=    .......
> 
>                    Delta2   .....                               Delta2
> 
> The elements of these submatrices depend only upon the three covariance 
> parameters: the compound symmetry parameter delta; the variance of 
> random effect sigma^2g; and the residual variance sigma^2. I have 
> formulas for the submatrices Lambda,Delta1 and Delta2 which I can't 
> really paste in here.
> 
> The SAS code dealing with this model is the following:
> 
> proc mixed data=rnadeg.pnau;
> title 'CV structure for PNAU';
> class probepos probeno end probe pixelid newprobeid;
> model logPM=end logpgc / ddfm=satterth;
> random probeno newprobeid / subject=probe type=cs;
> lsmeans end / diff cl; run;
> 
> Any ideas are appreciated a lot since I am kind of stuck at this point.
> 
> Thank you
> Tobias Guennel
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From g.abraham at ms.unimelb.edu.au  Fri Jun 23 05:48:33 2006
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Fri, 23 Jun 2006 13:48:33 +1000
Subject: [R] Time series labeling with Zoo
Message-ID: <449B6491.3010501@ms.unimelb.edu.au>

Hi,

I'm using zoo because it can automatically label the months of a time 
series composed of daily observations.

This works well for certain time series lengths, but not for others, e.g.:

While:

 > library(zoo)
 > plot(zoo(runif(10), as.Date("2005-06-01") + 0:50))

Shows up the months and day of month,

 > plot(zoo(runif(10), as.Date("2005-06-01") + 0:380))

results in a single label "2006" in the middle of the x axis, with no 
months labelled, although there's plenty of room for labels.


How do I get zoo to label the months too, without having to manually 
work out which day was the first day of each month?


I'm using zoo 1.0-3 with R 2.2.1.

Thanks,
Gad

-- 
Gad Abraham
Department of Mathematics and Statistics
University of Melbourne
Parkville 3010, Victoria, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From rajarshi at presidency.com  Fri Jun 23 06:02:32 2006
From: rajarshi at presidency.com (Rajarshi Guha)
Date: Fri, 23 Jun 2006 00:02:32 -0400
Subject: [R] frechet distance
In-Reply-To: <449B5751.9030106@pdf.com>
References: <1150737915.8534.34.camel@blue.chem.psu.edu>
	<449B5751.9030106@pdf.com>
Message-ID: <1151035353.6483.19.camel@localhost>

On Thu, 2006-06-22 at 19:52 -0700, Spencer Graves wrote:
> 	  RSiteSearch("Frechet distance") returned only one hit for me just 
> now, and that was for a Frechet distribution, as you mentioned.  Google 
> found "www.cs.concordia.ca/cccg/papers/39.pdf", which suggests that 
> computing it may not be easy.

In addition, from what I have read it is supposed to be NP-hard. However
my problems are usually small. 

> 1.  If you absolutely need the Frechet distance and you can describe 
> an algorithm for computing it but get stuck writing a function for it, 
> please submit another question outlining what you've tried and that 
> obstacle you've found.
> 
> 	  2.  Alternatively, you might describe the more general problem you 
> are trying to solve, why you thought the Frechet distance might help and 
> invite alternative suggestions.

I have some curves (in the form of points) which are sigmoidal. I also
have some curves which look like 2 sigmoid curves joined head to tail.
Something like


                                     -----
				    /
			           /
			     ------
		            /
			   /
			  /
		   ------

Now these curves can vary: in some cases the initial lower tail might be
truncated.  For the 'stepwise' sigmoidal curves, the middle step might
not be horizontal but inclined to some degree and so on.

My goal is to be given a set of points representing a curve and try and
identify whether it is of the standard sigmoid form or of the 'stepwise'
sigmoid form.

My plan was to generate a 'canonical sigmoid curve' via the logistic
equation and then perform a curve matching operation. Thus for a
supplied curve that is sigmoid in nature it will match the 'canonical
curve' to a better extent than would a curve that is stepwise in nature.
(The matching is performed after applying a Procrustes transformation)

Initially I tried using the Hausdorff distance, but this does not take
into account the ordering of the points in the curve and did not always
give a conclusive answer. A number of references (including the one
above) indicate that the Frechet distance is better suited for curve
matching problems.

As you noted, evaluating the Frechet distance is non-trivial and the
only code that I could find was some code that is dependent on the CGAL
(http://www.cgal.org/) library. As far as I could see, CGAL does not
have any R bindings.

An alternative that I had considered was to to evaluate the distance
matrix of the points making up the curve and then evaluating the root
mean square error of the matrix elements for the canonical curve and the
supplied curve. My initial experiments indicated that this generally
works but I observed some cases where a stepwise curve matched the
canonical sigmoid better (ie lower RMSE) than an actual sigmoid curve.

Another alternative is look at a graph of the first derivative of the
curve. A standard sigmoidal curve will result in a graph with a single
peak, a stepwise curve like above will result in a graph with 2 peaks.
Thus this could be reduced to a peak picking problem. The problem is the
curves I'll get are not smooth and can have small kinks - this leads to
(usually) quite small peaks in the graph of the first derivative - but
most of the code that has been described on this list for peak picking
also picks them up, thus making identification of the curve ambiguous.

To be honest I do not fully understand the algorithm used to evaluate
the Frechet distance hence my request for code. However, I'm not fixated
on the Frechet distance :) If there are simpler approaches I'm open to
them.

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Q: What do you get when you cross a mosquito with a mountain climber?
A: Nothing. You can't cross a vector with a scaler.


From spencer.graves at pdf.com  Fri Jun 23 06:38:56 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 22 Jun 2006 21:38:56 -0700
Subject: [R] frechet distance
In-Reply-To: <1151035353.6483.19.camel@localhost>
References: <1150737915.8534.34.camel@blue.chem.psu.edu>	<449B5751.9030106@pdf.com>
	<1151035353.6483.19.camel@localhost>
Message-ID: <449B7060.9090308@pdf.com>

<see inline>	

Rajarshi Guha wrote:
> On Thu, 2006-06-22 at 19:52 -0700, Spencer Graves wrote:
>> 	  RSiteSearch("Frechet distance") returned only one hit for me just 
>> now, and that was for a Frechet distribution, as you mentioned.  Google 
>> found "www.cs.concordia.ca/cccg/papers/39.pdf", which suggests that 
>> computing it may not be easy.
> 
> In addition, from what I have read it is supposed to be NP-hard. However
> my problems are usually small. 
> 
>> 1.  If you absolutely need the Frechet distance and you can describe 
>> an algorithm for computing it but get stuck writing a function for it, 
>> please submit another question outlining what you've tried and that 
>> obstacle you've found.
>>
>> 	  2.  Alternatively, you might describe the more general problem you 
>> are trying to solve, why you thought the Frechet distance might help and 
>> invite alternative suggestions.
> 
> I have some curves (in the form of points) which are sigmoidal. I also
> have some curves which look like 2 sigmoid curves joined head to tail.
> Something like
> 
> 
>                                      -----
> 				    /
> 			           /
> 			     ------
> 		            /
> 			   /
> 			  /
> 		   ------
> 
	  If all curves are of this form, it suggests they are continuous and 
monotonically increasing.  Is that correct?  If yes, isn't the Frechet 
distance then just the max of the distances between the two curves at 
the end points?

	  I don't see a complete proof yet, but it would seem that the 
difficulties in computing Frechet distances would come from functions 
that are not monotonic and / or discontinuous.

	  This brings me back to my second question:  What's the larger problem 
you want to solve, for which you think the Frechet distance might help?

	  Hope this helps.
	  Spencer Graves

suppose we want

> Now these curves can vary: in some cases the initial lower tail might be
> truncated.  For the 'stepwise' sigmoidal curves, the middle step might
> not be horizontal but inclined to some degree and so on.
> 
> My goal is to be given a set of points representing a curve and try and
> identify whether it is of the standard sigmoid form or of the 'stepwise'
> sigmoid form.
> 
> My plan was to generate a 'canonical sigmoid curve' via the logistic
> equation and then perform a curve matching operation. Thus for a
> supplied curve that is sigmoid in nature it will match the 'canonical
> curve' to a better extent than would a curve that is stepwise in nature.
> (The matching is performed after applying a Procrustes transformation)
> 
> Initially I tried using the Hausdorff distance, but this does not take
> into account the ordering of the points in the curve and did not always
> give a conclusive answer. A number of references (including the one
> above) indicate that the Frechet distance is better suited for curve
> matching problems.
> 
> As you noted, evaluating the Frechet distance is non-trivial and the
> only code that I could find was some code that is dependent on the CGAL
> (http://www.cgal.org/) library. As far as I could see, CGAL does not
> have any R bindings.
> 
> An alternative that I had considered was to to evaluate the distance
> matrix of the points making up the curve and then evaluating the root
> mean square error of the matrix elements for the canonical curve and the
> supplied curve. My initial experiments indicated that this generally
> works but I observed some cases where a stepwise curve matched the
> canonical sigmoid better (ie lower RMSE) than an actual sigmoid curve.
> 
> Another alternative is look at a graph of the first derivative of the
> curve. A standard sigmoidal curve will result in a graph with a single
> peak, a stepwise curve like above will result in a graph with 2 peaks.
> Thus this could be reduced to a peak picking problem. The problem is the
> curves I'll get are not smooth and can have small kinks - this leads to
> (usually) quite small peaks in the graph of the first derivative - but
> most of the code that has been described on this list for peak picking
> also picks them up, thus making identification of the curve ambiguous.
> 
> To be honest I do not fully understand the algorithm used to evaluate
> the Frechet distance hence my request for code. However, I'm not fixated
> on the Frechet distance :) If there are simpler approaches I'm open to
> them.
> 
> Thanks,
> 
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> Q: What do you get when you cross a mosquito with a mountain climber?
> A: Nothing. You can't cross a vector with a scaler.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From maechler at stat.math.ethz.ch  Fri Jun 23 07:52:11 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 23 Jun 2006 07:52:11 +0200
Subject: [R] High breakdown/efficiency statistics -- was RE: Rosner's
	test [Broadcast]
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02700173@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02700173@usctmx1106.merck.com>
Message-ID: <17563.33163.744141.739537@stat.math.ethz.ch>

>>>>> "AndyL" == Liaw, Andy <andy_liaw at merck.com>
>>>>>     on Thu, 22 Jun 2006 14:21:17 -0400 writes:

    AndyL> What would be nice is to have something like a
    AndyL> "robust" task view...  Andy

Indeed.  I have volunteered to do this a while ago.
First wanted to get "robustbase" to a "reasonable" state.

A 'robust' CRAN task view is planned to be done by the end of
the summer, hopefully even earlier.

Martin

    AndyL> From: Berton Gunter
    >>  Many thanks for this Martin. There now are several
    >> packages with what appear to be overlapping functions (or
    >> at least algorithms). Besides those you mentioned,
    >> "robust" and "roblm" are at least two others. Any
    >> recommendations about how or whether to choose among
    >> these for us enthusiastic but non-expert users?
    >> 
    >> 
    >> Cheers, Bert
    >> 
    >> 
    >> > -----Original Message----- > From: Martin Maechler
    >> [mailto:maechler at stat.math.ethz.ch] > Sent: Thursday,
    >> June 22, 2006 2:04 AM > To: Berton Gunter > Cc: 'Robert
    >> Powell'; r-help at stat.math.ethz.ch > Subject: Re: [R]
    >> Rosner's test
    >> > 
    >> > >>>>> "BertG" == Berton Gunter <gunter.berton at gene.com>
    >> > >>>>> on Tue, 13 Jun 2006 14:34:48 -0700 writes:
    >> > 
    >> > BertG> RSiteSearch('Rosner') ?RSiteSearch or search
    >> directly > BertG> from CRAN.
    >> > 
    >> > BertG> Incidentally, I'll repeat what I've said >
    >> BertG> before. Don't do outlier tests.  They're > BertG>
    >> dangerous. Use robust methods instead.
    >> > 
    >> > Yes, yes, yes!!!
    >> > 
    >> > Note that rlm() or cov.rob() from recommended package
    >> MASS will most > probably be sufficient for your needs.
    >> > 
    >> > For slightly newer methodology, look at package
    >> 'robustbase', or also > 'rrcov'.
    >> > 
    >> > Martin Maechler, ETH Zurich
    >> > 
    >> > BertG> -- Bert Gunter Genentech Non-Clinical Statistics
    >> > BertG> South San Francisco, CA
    >> >  
    >> > BertG> "The business of the statistician is to catalyze
    >> the > BertG> scientific learning process."  - George
    >> E. P. Box
    >> >  
    >> >  
    >> >
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
    >> read the posting guide!
    >> http://www.R-project.org/posting-guide.html
    >> 
    >> 


    AndyL> ------------------------------------------------------------------------------
    AndyL> Notice: This e-mail message, together with any
    AndyL> attachments, contains information of Merck & Co.,
    AndyL> Inc. (One Merck Drive, Whitehouse Station, New
    AndyL> Jersey, USA 08889), and/or its affiliates (which may
    AndyL> be known outside the United States as Merck Frosst,
    AndyL> Merck Sharp & Dohme or MSD and in Japan, as Banyu)
    AndyL> that may be confidential, proprietary copyrighted
    AndyL> and/or legally privileged. It is intended solely for
    AndyL> the use of the individual or entity named on this
    AndyL> message.  If you are not the intended recipient, and
    AndyL> have received this message in error, please notify us
    AndyL> immediately by reply e-mail and then delete it from
    AndyL> your system.
    AndyL> ------------------------------------------------------------------------------


From ggrothendieck at gmail.com  Fri Jun 23 07:54:41 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 23 Jun 2006 01:54:41 -0400
Subject: [R] Time series labeling with Zoo
In-Reply-To: <449B6491.3010501@ms.unimelb.edu.au>
References: <449B6491.3010501@ms.unimelb.edu.au>
Message-ID: <971536df0606222254r25838c28w22bbe4bbaf2d2579@mail.gmail.com>

When the axis labelling does not work well you will have to do it yourself
like this.  The plot statement is instructed not to plot the axis and then
we extract into tt all the dates which are day of the month 1.  Then
we manually draw the axis using those.

library(zoo)
set.seed(1)
z <- zoo(runif(10), as.Date("2005-06-01") + 0:380)

plot(z, xaxt = "n")
tt <- time(z)[as.POSIXlt(time(z))$mday == 1]
axis(1, tt, format(tt, "%d%b%y"))

On 6/22/06, Gad Abraham <g.abraham at ms.unimelb.edu.au> wrote:
> Hi,
>
> I'm using zoo because it can automatically label the months of a time
> series composed of daily observations.
>
> This works well for certain time series lengths, but not for others, e.g.:
>
> While:
>
>  > library(zoo)
>  > plot(zoo(runif(10), as.Date("2005-06-01") + 0:50))
>
> Shows up the months and day of month,
>
>  > plot(zoo(runif(10), as.Date("2005-06-01") + 0:380))
>
> results in a single label "2006" in the middle of the x axis, with no
> months labelled, although there's plenty of room for labels.
>
>
> How do I get zoo to label the months too, without having to manually
> work out which day was the first day of each month?
>
>
> I'm using zoo 1.0-3 with R 2.2.1.
>
> Thanks,
> Gad
>
> --
> Gad Abraham
> Department of Mathematics and Statistics
> University of Melbourne
> Parkville 3010, Victoria, Australia
> email: g.abraham at ms.unimelb.edu.au
> web: http://www.ms.unimelb.edu.au/~gabraham
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From g.abraham at ms.unimelb.edu.au  Fri Jun 23 09:09:29 2006
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Fri, 23 Jun 2006 17:09:29 +1000
Subject: [R] Time series labeling with Zoo
In-Reply-To: <971536df0606222254r25838c28w22bbe4bbaf2d2579@mail.gmail.com>
References: <449B6491.3010501@ms.unimelb.edu.au>
	<971536df0606222254r25838c28w22bbe4bbaf2d2579@mail.gmail.com>
Message-ID: <449B93A9.2000609@ms.unimelb.edu.au>

Gabor Grothendieck wrote:
> When the axis labelling does not work well you will have to do it yourself
> like this.  The plot statement is instructed not to plot the axis and then
> we extract into tt all the dates which are day of the month 1.  Then
> we manually draw the axis using those.
> 
> library(zoo)
> set.seed(1)
> z <- zoo(runif(10), as.Date("2005-06-01") + 0:380)
> 
> plot(z, xaxt = "n")
> tt <- time(z)[as.POSIXlt(time(z))$mday == 1]
> axis(1, tt, format(tt, "%d%b%y"))

Thanks Gabor,

That works nicely.

Cheers,
Gad


-- 
Gad Abraham
Department of Mathematics and Statistics
University of Melbourne
Parkville 3010, Victoria, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From j.van_den_hoff at fz-rossendorf.de  Fri Jun 23 10:34:08 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Fri, 23 Jun 2006 10:34:08 +0200
Subject: [R] Basic package structure question
In-Reply-To: <971536df0606221919u5de6a851u370fbd2846f92644@mail.gmail.com>
References: <d4588dec0606220908w415cc012y8fa678148cef44e3@mail.gmail.com>	<449AD3CB.4050202@stats.uwo.ca>
	<971536df0606221919u5de6a851u370fbd2846f92644@mail.gmail.com>
Message-ID: <449BA780.2020206@fz-rossendorf.de>

Gabor Grothendieck wrote:
> On 6/22/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
>> Jay Emerson wrote:
>>> At the encouragement of many at UseR, I'm trying to build my first real
>>> package. I have no C/Fortran code, just plain old R code, so it should be
>>> rocket science.  On a Linux box, I used package.skeleton() to create a basic
>>> package containing just one "hello world" type of function.  I edited the
>>> DESCRIPTION file, changin the package name appropriately.  I edited the
>>> hello.Rd file.  Upon running R CMD check hello, the only warning had to do
>>> with the fact that src/ was empty (obviously I had no source in such a
>>> simple package).  I doubt this is a problem.
>>>
>>> I was able to install and use the package successfully on the Linux system
>>> from the .tar.gz file, so far so good!  Next, on to Windows, where the
>>> problem arose:
>>>
>>> I created a zip file from inside the package directory: zip -r ../hello.zip
>>> ./*
>>>
>>>
>> Which package directory, the source or the installed copy?  I think this
>> might work in the installed copy, but would not work on the source.
>> It's not the documented way to build a binary zip, though.
>>> When I moved this to my Windows machine and tried to install the package, I
>>> received the following error:
>>>
>>>
>>>> utils:::menuInstallLocal()
>>>>
>>> Error in unpackPkg(pkgs[i], pkgnames[i], lib, installWithVers) :
>>>         malformed bundle DESCRIPTION file, no Contains field
>>>
>>> I only found one mention of this in my Google search, with no reply to the
>>> thread.  The Contains field appears to be used for bundles, but I'm trying
>>> to create a package, not a bundle.  This leads me to believe that a simple
>>> zipping of the package directory structure is not the correct format for
>>> Windows.
>>>
>>> Needless to say, there appears to be wide agreement that making packages
>>> requires precision, but fundamentally a package should (as described in the
>>> documentation) just be a collection of files and folders organized a certain
>>> way.  If someone could point me to documentation I may have missed that
>>> explains this, I would be grateful.
>>>
>> I think the "organized in a certain way" part is actually important.
>> Using R CMD install --build is the documented way to achieve this.  It's
>> not trivial to do this on Windows, because you need to set up a build
>> environment first, but it's not horribly difficult.
>>
>> Duncan Murdoch
>>> Regards,
>>>
>>> Jay
> 
> One idea that occurred to me in reading this would be to have a server
> that one can send a package to and get back a Windows build to
> eliminate having to set up a development environment.  Not sure if
> this is feasible, particularly security aspects, but if it were it would
> open up package building on Windows to a larger audience.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

just to confirm duncan murdochs remark:

our Windows machines lack proper development environments (mainly 
missing perl is the problem for pure R-code packages, I believe?) and we 
bypass this (for pure R-code packages only, of course) by

1.) install the package on the unix machine into the desired R library
2.) zip the _installed_ package (not the source tree!) found in the R 
   	    library directory
3.) transfer this archive to the Windows machine
4.) unzip directly into the desired library destination

this procedure up to now always worked including properly installed 
manpages (text + html (and I hope this remains the case in the future...)

joerg van den hoff


From maechler at stat.math.ethz.ch  Fri Jun 23 10:42:58 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 23 Jun 2006 10:42:58 +0200
Subject: [R] High breakdown/efficiency statistics -- was RE: Rosner's
	test
In-Reply-To: <001701c6961b$2452fe20$0e7e11ac@gne.windows.gene.com>
References: <17562.23802.793188.600782@stat.math.ethz.ch>
	<001701c6961b$2452fe20$0e7e11ac@gne.windows.gene.com>
Message-ID: <17563.43410.601845.498856@stat.math.ethz.ch>

I'm CC'ing this to the R-SIG-robust mailing list
 [R Special Interest Group on robust statistics]
so it's properly archived there as well.
Follow up ideally should only go there.

{BTW: Did you know that to *search* mailing list archives of
      such R-SIG-foo mailing lists, you can use google very
      efficiently by prepending the mailing list name and 'site:stat.ethz.ch'?
      e.g., use google search on
      "R-SIG-robust site:stat.ethz.ch lmrob"
}
      
>>>>> "BertG" == Berton Gunter <gunter.berton at gene.com>
>>>>>     on Thu, 22 Jun 2006 09:44:33 -0700 writes:

    BertG> Many thanks for this Martin. There now are several
    BertG> packages with what appear to be overlapping functions
    BertG> (or at least algorithms). Besides those you
    BertG> mentioned, "robust" and "roblm" are at least two others.

actually quite particular ones:

- "roblm" by Matias Salibian-Barreras is really predecessor to
  parts in 'robustbase'. His roblm() function is now  lmrob() in
  robustbase, i.e., robustbase::lmrob(), and lmrob() is a bit
  more efficient and further has a anova() method.

- "robust" : by Kjell Konis -- is planned to become a full port
	   of the S-plus library section "robust" (from
	   Insightful, also mainly by Kjell Konis, built
	   on code of many more, see DESCRIPTION).
   At the moment it comes with a 'Insightful Robust Library License'
   which seems a kind of open source licence, but pretty "peculiar"
   (to me: IANAL (I am not a lawyer)).

   At the moment it only has "robust covariance + location", but
   when it will contain everything from its S-plus counterpart,
   it will be a very nice benchmark; in many parts "first rate".

    BertG> Any recommendations about how or whether to
    BertG> choose among these for us enthusiastic but non-expert
    BertG> users?

As I said (in reply to Andy's suggestion) there will be a CRAN
task view "real soon now" 
in order to give some guidance on the diverse packages with
robustness functionality.

    BertG> Cheers, Bert
 

    >> -----Original Message----- From: Martin Maechler
    >> [mailto:maechler at stat.math.ethz.ch] Sent: Thursday, June
    >> 22, 2006 2:04 AM To: Berton Gunter Cc: 'Robert Powell';
    >> r-help at stat.math.ethz.ch Subject: Re: [R] Rosner's test
    >> 
    >> >>>>> "BertG" == Berton Gunter <gunter.berton at gene.com>
    >> >>>>> on Tue, 13 Jun 2006 14:34:48 -0700 writes:
    >> 
    BertG> RSiteSearch('Rosner') ?RSiteSearch or search directly
    BertG> from CRAN.
    >>
    BertG> Incidentally, I'll repeat what I've said
    BertG> before. Don't do outlier tests.  They're
    BertG> dangerous. Use robust methods instead.
    >>  Yes, yes, yes!!!
    >> 
    >> Note that rlm() or cov.rob() from recommended package
    >> MASS will most probably be sufficient for your needs.
    >> 
    >> For slightly newer methodology, look at package
    >> 'robustbase', or also 'rrcov'.
    >> 
    >> Martin Maechler, ETH Zurich
    >> 
    BertG> -- Bert Gunter Genentech Non-Clinical Statistics
    BertG> South San Francisco, CA
    >>
    BertG> "The business of the statistician is to catalyze the
    BertG> scientific learning process."  - George E. P. Box
    >>


From ripley at stats.ox.ac.uk  Fri Jun 23 10:48:11 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Jun 2006 09:48:11 +0100 (BST)
Subject: [R] Basic package structure question
In-Reply-To: <449BA780.2020206@fz-rossendorf.de>
References: <d4588dec0606220908w415cc012y8fa678148cef44e3@mail.gmail.com>
	<449AD3CB.4050202@stats.uwo.ca>
	<971536df0606221919u5de6a851u370fbd2846f92644@mail.gmail.com>
	<449BA780.2020206@fz-rossendorf.de>
Message-ID: <Pine.LNX.4.64.0606230946340.13624@gannet.stats.ox.ac.uk>

On Fri, 23 Jun 2006, Joerg van den Hoff wrote:

> just to confirm duncan murdochs remark:
>
> our Windows machines lack proper development environments (mainly
> missing perl is the problem for pure R-code packages, I believe?) and we
> bypass this (for pure R-code packages only, of course) by
>
> 1.) install the package on the unix machine into the desired R library
> 2.) zip the _installed_ package (not the source tree!) found in the R
>   	    library directory
> 3.) transfer this archive to the Windows machine
> 4.) unzip directly into the desired library destination
>
> this procedure up to now always worked including properly installed
> manpages (text + html (and I hope this remains the case in the future...)

>From README.packages:

   If your package has no compiled code it is possible that zipping up the
   installed package on Linux will produce an installable package on
   Windows.  (It has always worked for us, but failures have been reported.)

so this is indeed already documented, and does not work for everyone.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From maechler at stat.math.ethz.ch  Fri Jun 23 11:15:29 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 23 Jun 2006 11:15:29 +0200
Subject: [R] programming advice
In-Reply-To: <449ABE0E.8010506@univ-brest.fr>
References: <449ABE0E.8010506@univ-brest.fr>
Message-ID: <17563.45361.717989.320310@stat.math.ethz.ch>

>>>>> "Fred" == Fred JEAN <frederic.jean at univ-brest.fr>
>>>>>     on Thu, 22 Jun 2006 17:58:06 +0200 writes:

    Fred> Dear R users
    Fred> I want to compute Kendall's Tau between two vectors x and y.
    Fred> But x and y  may have zeros in the same position(s) and I wrote the
    Fred> following function to be sure to drop out those "double zeros"

    "cor.kendall" <- function(x,y) {
      nox <- c()
      noy <- c()
      #
      for (i in 1:length(x)) if (x[i]!= 0 | y[i] != 0)
	  nox[length(nox)+1]<- x[i]
      for (i in 1:length(y)) if (x[i]!= 0 | y[i] != 0)
	  noy[length(noy)+1]<- y[i]
      #
      res.kendall <- cor.test(nox,noy,method = "kendall",exact=F)
      return(list(x=nox,y=noy,res.kendall,length(nox)))
    }
    
    Fred> Do you know a more elegant way to achieve the same goal ?
    Fred> (I'm sure you do : it's a newbie's program actually)

"Ronggui" already helped you with your main question.

Just a note:  
Why are you making the detour of calling cor.test(.)
when  
      cor(nox, noy,  method = "kendall")    

is probably all you need?


From j.van_den_hoff at fz-rossendorf.de  Fri Jun 23 11:45:20 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Fri, 23 Jun 2006 11:45:20 +0200
Subject: [R] Basic package structure question
In-Reply-To: <Pine.LNX.4.64.0606230946340.13624@gannet.stats.ox.ac.uk>
References: <d4588dec0606220908w415cc012y8fa678148cef44e3@mail.gmail.com>
	<449AD3CB.4050202@stats.uwo.ca>
	<971536df0606221919u5de6a851u370fbd2846f92644@mail.gmail.com>
	<449BA780.2020206@fz-rossendorf.de>
	<Pine.LNX.4.64.0606230946340.13624@gannet.stats.ox.ac.uk>
Message-ID: <449BB830.7030902@fz-rossendorf.de>

Prof Brian Ripley wrote:
> On Fri, 23 Jun 2006, Joerg van den Hoff wrote:
> 
>> just to confirm duncan murdochs remark:
>>
>> our Windows machines lack proper development environments (mainly
>> missing perl is the problem for pure R-code packages, I believe?) and we
>> bypass this (for pure R-code packages only, of course) by
>>
>> 1.) install the package on the unix machine into the desired R library
>> 2.) zip the _installed_ package (not the source tree!) found in the R
>>           library directory
>> 3.) transfer this archive to the Windows machine
>> 4.) unzip directly into the desired library destination
>>
>> this procedure up to now always worked including properly installed
>> manpages (text + html (and I hope this remains the case in the future...)
> 
>  From README.packages:
> 
>   If your package has no compiled code it is possible that zipping up the
>   installed package on Linux will produce an installable package on
>   Windows.  (It has always worked for us, but failures have been reported.)
> 
> so this is indeed already documented, and does not work for everyone.
> 
albeit sparsely...

AFAIKS it's not in the `R Extensions' manual at all. If(!) this approach 
could be made an 'official' workaround and explained in the manual, that 
would be good.

I'd appreciate if someone could tell me:

are the mentioned failures confirmed or are they "UFOs"?

if so, are (the) reasons for (or circumstances of) failure known (I'm 
always afraid walking on thin ice when using this transfer strategy)?

what does "produce an installable package on Windows" in the README text 
mean? I presume it does not mean that R CMD INSTALL (or the Windows 
equivalent) does work? if it really means "unzip the package on the 
Windows machine into the library directory", should'nt the text be altered?

and I forgot to mention in my first mail: I use the described procedure 
for transfer from a non-Intel apple machine under MacOS (a FreeBSD 
descendant) to Windows (and even (unneccessarily, I know) to 
Sun/Solaris). so the strategy is not restricted to transfer from Linux 
-> Windows.

and it is useful (if it is not 'accidental' that it works at all): in 
this way one can keep very easily in sync several local incarnations of 
a package across platforms (if network access to a common disk is not 
the way to go): I simply `rsync' the affected (local, R-code only) 
library directories.


From frederic.jean at univ-brest.fr  Fri Jun 23 11:57:36 2006
From: frederic.jean at univ-brest.fr (Fred JEAN)
Date: Fri, 23 Jun 2006 11:57:36 +0200
Subject: [R] programming advice
In-Reply-To: <17563.45361.717989.320310@stat.math.ethz.ch>
References: <449ABE0E.8010506@univ-brest.fr>
	<17563.45361.717989.320310@stat.math.ethz.ch>
Message-ID: <449BBB10.2050806@univ-brest.fr>

Martin Maechler wrote:
> [...]
> Just a note:  
> Why are you making the detour of calling cor.test(.)
> when  
>       cor(nox, noy,  method = "kendall")    
> 
> is probably all you need?

I use cor.test() because I want to test the value of Tau.

Thanks to everyone. I'll use the solution proposed by Ronggui.

The Kendall package solution is certainly also valuable but less direct
(in my newbie opinion) : in fact x and y are extracted from a
contingency table and I should use replace() to substitute "double NAs"
to "double zeros"

Thanks again

-- 
Fred


From murdoch at stats.uwo.ca  Fri Jun 23 13:15:54 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 23 Jun 2006 07:15:54 -0400
Subject: [R] Basic package structure question
In-Reply-To: <449BB830.7030902@fz-rossendorf.de>
References: <d4588dec0606220908w415cc012y8fa678148cef44e3@mail.gmail.com>
	<449AD3CB.4050202@stats.uwo.ca>
	<971536df0606221919u5de6a851u370fbd2846f92644@mail.gmail.com>
	<449BA780.2020206@fz-rossendorf.de>
	<Pine.LNX.4.64.0606230946340.13624@gannet.stats.ox.ac.uk>
	<449BB830.7030902@fz-rossendorf.de>
Message-ID: <449BCD6A.8020507@stats.uwo.ca>

On 6/23/2006 5:45 AM, Joerg van den Hoff wrote:
> Prof Brian Ripley wrote:
>> On Fri, 23 Jun 2006, Joerg van den Hoff wrote:
>>
>>> just to confirm duncan murdochs remark:
>>>
>>> our Windows machines lack proper development environments (mainly
>>> missing perl is the problem for pure R-code packages, I believe?) and we
>>> bypass this (for pure R-code packages only, of course) by
>>>
>>> 1.) install the package on the unix machine into the desired R library
>>> 2.) zip the _installed_ package (not the source tree!) found in the R
>>>           library directory
>>> 3.) transfer this archive to the Windows machine
>>> 4.) unzip directly into the desired library destination
>>>
>>> this procedure up to now always worked including properly installed
>>> manpages (text + html (and I hope this remains the case in the future...)
>>  From README.packages:
>>
>>   If your package has no compiled code it is possible that zipping up the
>>   installed package on Linux will produce an installable package on
>>   Windows.  (It has always worked for us, but failures have been reported.)
>>
>> so this is indeed already documented, and does not work for everyone.
>>
> albeit sparsely...
> 
> AFAIKS it's not in the `R Extensions' manual at all. If(!) this approach 
> could be made an 'official' workaround and explained in the manual, that 
> would be good.
> 
> I'd appreciate if someone could tell me:
> 
> are the mentioned failures confirmed or are they "UFOs"?
> 
> if so, are (the) reasons for (or circumstances of) failure known (I'm 
> always afraid walking on thin ice when using this transfer strategy)?
> 
> what does "produce an installable package on Windows" in the README text 
> mean? I presume it does not mean that R CMD INSTALL (or the Windows 
> equivalent) does work? if it really means "unzip the package on the 
> Windows machine into the library directory", should'nt the text be altered?

I do not want to support more than one method of installing packages. 
The R CMD install method works.  If some other method also works, it's 
unsupported.

One obvious limitation of this install method is that it won't produce 
native Windows help files (.chm).  Plans are to make CHM the default 
help system as of the 2.4.0 release, so your packages will not work 
properly unless you give special instructions on how to change the help 
system defaults.

The other obvious limitation is that it won't work if you have any C or 
Fortran code in your package.

> and I forgot to mention in my first mail: I use the described procedure 
> for transfer from a non-Intel apple machine under MacOS (a FreeBSD 
> descendant) to Windows (and even (unneccessarily, I know) to 
> Sun/Solaris). so the strategy is not restricted to transfer from Linux 
> -> Windows.
> 
> and it is useful (if it is not 'accidental' that it works at all): in 
> this way one can keep very easily in sync several local incarnations of 
> a package across platforms (if network access to a common disk is not 
> the way to go): I simply `rsync' the affected (local, R-code only) 
> library directories.

You might be able to achieve this by doing your builds on Windows, 
rather than on Linux or MacOS, but as far as I know it is not possible 
to build .chm files on those OSs.

Duncan Murdoch


From bernarduse1 at yahoo.fr  Fri Jun 23 13:28:07 2006
From: bernarduse1 at yahoo.fr (Marc Bernard)
Date: Fri, 23 Jun 2006 13:28:07 +0200 (CEST)
Subject: [R] PowerPoint
Message-ID: <20060623112807.74518.qmail@web25806.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060623/2f722356/attachment.pl 

From rogeriorosas at gmail.com  Fri Jun 23 14:12:41 2006
From: rogeriorosas at gmail.com (=?ISO-8859-1?Q?Rog=E9rio_Rosa_da_Silva?=)
Date: Fri, 23 Jun 2006 09:12:41 -0300
Subject: [R] integrate
Message-ID: <449BDAB9.9080201@gmail.com>

Dear All,

My doubt about how to integrate a simple kernel density estimation goes on.

I have seen the recent posts on integrate density estimation, which seem
similar to my question. However, I haven't found a solution.

I have made two simple kernel density estimation by:
 
    kde.1 <-density(x, bw=sd(x), kernel="gaussian")$y     # x<- c(2,3,5,12)
    kde.2 <-density(y, bw=sd(y), kernel="gaussian")$y     # y<- c(4,2,4,11)
 
Now I would like to integrate the difference in the estimated density
values, i.e.:

    diff.kde <- abs (kde.1- kde.2)

How can I integrate diff.kde over -Inf to Inf ?

Best,

Rog?rio


From j.van_den_hoff at fz-rossendorf.de  Fri Jun 23 14:44:42 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Fri, 23 Jun 2006 14:44:42 +0200
Subject: [R] problem with "code/documentation mismatch"
Message-ID: <449BE23A.6060609@fz-rossendorf.de>

I have a package with a division method for special objects of class 
"RoidataList", i.e. the function is named `/.RoidataList'.
documentation for this is in the file "Divide.RoidataList".


R CMD CHECK complains with:

=================cut======================
* checking for code/documentation mismatches ... WARNING
Functions/methods with usage in documentation object 
'Divide.RoidataList' but not in code:
   /

* checking Rd \usage sections ... WARNING
Objects in \usage without \alias in documentation object 
'Divide.RoidataList':
   /
=================cut======================

the `usage' section in the Rd file reads

=================cut======================
\usage{
    x/y
}
=================cut======================

which, of course is the desired way to use the function.
what am I doing wrong, i.e. how should I modify the Rd file?
maybe obvious, but not to me.

joerg van den hoff


From HDoran at air.org  Fri Jun 23 14:50:33 2006
From: HDoran at air.org (Doran, Harold)
Date: Fri, 23 Jun 2006 08:50:33 -0400
Subject: [R] PowerPoint
Message-ID: <2323A6D37908A847A7C32F1E3662C80E132136@dc1ex01.air.org>

Use the functions in library(grDevices) for jpeg, bmp, or png formats.
Or, you can use postscript() for an eps file. Of course, I personally
think tex files make for much better looking presentations if you can be
persuaded.

Harold
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marc Bernard
> Sent: Friday, June 23, 2006 7:28 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] PowerPoint
> 
> Dear All,
>    
>   I am looking for the best way to use graphs from R (like 
> xyplot, curve ...)   for a presentation with powerpoint. I 
> used to save my plot as pdf and after to copy them as image 
> in powerpoint but the quality is not optimal by so doing.
>    
>   Another completely independent question is the following: 
> when I use "main"  in the  xyplot, the main title is very 
> close to my plot, i.e. no ligne separate the main and the 
> plot. I would like my title to be well distinguished from the plots.
>    
>   I would be grateful for any improvements...
>    
>   Many thanks,
>    
>   Bernard,
>    
>     
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From andy_liaw at merck.com  Fri Jun 23 15:00:17 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 23 Jun 2006 09:00:17 -0400
Subject: [R] frechet distance
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA027001ED@usctmx1106.merck.com>

 
[Sorry for coming to this so late...  I've been trying to play catch-up with
~1000 unread messages in my R-help folder...]

If the curves are sufficiently smooth (i.e., `kinks' are quite small,
relative to the real sigmoidal features of interest), what I would try is
something like smoothing splines or local polynomials, but over-smooth
(i.e., use "large enough" smoothing parameters) so that only the essential
features of the sigmoidal or double sigmoidal features remains, then look at
the zero crossings of the derivatives.  Martin has functions like D1ss() in
the `sfsmisc' package to do this.

HTH,
Andy


From: Rajarshi Guha
> 
> On Thu, 2006-06-22 at 19:52 -0700, Spencer Graves wrote:
> > 	  RSiteSearch("Frechet distance") returned only one hit 
> for me just 
> > now, and that was for a Frechet distribution, as you 
> mentioned.  Google 
> > found "www.cs.concordia.ca/cccg/papers/39.pdf", which suggests that 
> > computing it may not be easy.
> 
> In addition, from what I have read it is supposed to be 
> NP-hard. However
> my problems are usually small. 
> 
> > 1.  If you absolutely need the Frechet distance and you can 
> describe 
> > an algorithm for computing it but get stuck writing a 
> function for it, 
> > please submit another question outlining what you've tried and that 
> > obstacle you've found.
> > 
> > 	  2.  Alternatively, you might describe the more 
> general problem you 
> > are trying to solve, why you thought the Frechet distance 
> might help and 
> > invite alternative suggestions.
> 
> I have some curves (in the form of points) which are sigmoidal. I also
> have some curves which look like 2 sigmoid curves joined head to tail.
> Something like
> 
> 
>                                      -----
> 				    /
> 			           /
> 			     ------
> 		            /
> 			   /
> 			  /
> 		   ------
> 
> Now these curves can vary: in some cases the initial lower 
> tail might be
> truncated.  For the 'stepwise' sigmoidal curves, the middle step might
> not be horizontal but inclined to some degree and so on.
> 
> My goal is to be given a set of points representing a curve 
> and try and
> identify whether it is of the standard sigmoid form or of the 
> 'stepwise'
> sigmoid form.
> 
> My plan was to generate a 'canonical sigmoid curve' via the logistic
> equation and then perform a curve matching operation. Thus for a
> supplied curve that is sigmoid in nature it will match the 'canonical
> curve' to a better extent than would a curve that is stepwise 
> in nature.
> (The matching is performed after applying a Procrustes transformation)
> 
> Initially I tried using the Hausdorff distance, but this does not take
> into account the ordering of the points in the curve and did 
> not always
> give a conclusive answer. A number of references (including the one
> above) indicate that the Frechet distance is better suited for curve
> matching problems.
> 
> As you noted, evaluating the Frechet distance is non-trivial and the
> only code that I could find was some code that is dependent 
> on the CGAL
> (http://www.cgal.org/) library. As far as I could see, CGAL does not
> have any R bindings.
> 
> An alternative that I had considered was to to evaluate the distance
> matrix of the points making up the curve and then evaluating the root
> mean square error of the matrix elements for the canonical 
> curve and the
> supplied curve. My initial experiments indicated that this generally
> works but I observed some cases where a stepwise curve matched the
> canonical sigmoid better (ie lower RMSE) than an actual sigmoid curve.
> 
> Another alternative is look at a graph of the first derivative of the
> curve. A standard sigmoidal curve will result in a graph with a single
> peak, a stepwise curve like above will result in a graph with 2 peaks.
> Thus this could be reduced to a peak picking problem. The 
> problem is the
> curves I'll get are not smooth and can have small kinks - 
> this leads to
> (usually) quite small peaks in the graph of the first derivative - but
> most of the code that has been described on this list for peak picking
> also picks them up, thus making identification of the curve ambiguous.
> 
> To be honest I do not fully understand the algorithm used to evaluate
> the Frechet distance hence my request for code. However, I'm 
> not fixated
> on the Frechet distance :) If there are simpler approaches I'm open to
> them.
> 
> Thanks,
> 
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> Q: What do you get when you cross a mosquito with a mountain climber?
> A: Nothing. You can't cross a vector with a scaler.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From ggrothendieck at gmail.com  Fri Jun 23 15:21:37 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 23 Jun 2006 09:21:37 -0400
Subject: [R] PowerPoint
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E132136@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E132136@dc1ex01.air.org>
Message-ID: <971536df0606230621t1584d4aer7cd4778978600fd0@mail.gmail.com>

Note that jpg, bmp and png are in less desirable bit mapped formats whereas
eps is in a more desirable vector format (magnification and shrinking does
not involve loss of info) and so would be preferable from a quality
viewpoint.  See:
http://www.stc-saz.org/resources/0203_graphics.pdf

On 6/23/06, Doran, Harold <HDoran at air.org> wrote:
> Use the functions in library(grDevices) for jpeg, bmp, or png formats.
> Or, you can use postscript() for an eps file. Of course, I personally
> think tex files make for much better looking presentations if you can be
> persuaded.
>
> Harold
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marc Bernard
> > Sent: Friday, June 23, 2006 7:28 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] PowerPoint
> >
> > Dear All,
> >
> >   I am looking for the best way to use graphs from R (like
> > xyplot, curve ...)   for a presentation with powerpoint. I
> > used to save my plot as pdf and after to copy them as image
> > in powerpoint but the quality is not optimal by so doing.
> >
> >   Another completely independent question is the following:
> > when I use "main"  in the  xyplot, the main title is very
> > close to my plot, i.e. no ligne separate the main and the
> > plot. I would like my title to be well distinguished from the plots.
> >
> >   I would be grateful for any improvements...
> >
> >   Many thanks,
> >
> >   Bernard,
> >
> >
> >
> >
> > ---------------------------------
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From rkrug at sun.ac.za  Fri Jun 23 15:29:07 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Fri, 23 Jun 2006 15:29:07 +0200
Subject: [R] How to use mle or similar with integrate?
Message-ID: <449BECA3.80406@sun.ac.za>

Hi

I have the following formula (I hope it is clear - if no, I can try to
do better the next time)

h(x, a, b) =
integral(0 to pi/2)
(
  (
    integral(D/sin(alpha) to Inf)
    (
      (
        f(x, a, b)
      )
      dx
    )
  dalpha
)

and I want to do an mle with it.
I know how to use mle() and I also know about integrate(). My problem is
to give the parameter values a and b to the integrate function.

In other words, how can I write

h <- function...

so that I can estimate a and b?

Thanks,

Rainer


-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa


From rdporto1 at terra.com.br  Fri Jun 23 15:34:23 2006
From: rdporto1 at terra.com.br (rdporto1)
Date: Fri, 23 Jun 2006 10:34:23 -0300
Subject: [R] PowerPoint
Message-ID: <J1BF1B$EC68968FD4D5DF25694EB8D5287B8A5A@terra.com.br>

Bernard,

there are some things that affect your graphs in
MS Powerpoint like the formats you save and include
them. You should consider test the various combinations.
You should test the final presentation on the
chosen presentation device 'cause there's some
resolution variation there too.

I use to save a .jpg format since I can control and
test some resolution levels and include as image.

For your main title, it's better you supply the group
with some code in order we can help you more.

HTH,

Rogerio Porto.

---------- Cabe?alho original -----------

De: r-help-bounces at stat.math.ethz.ch
Para: r-help at stat.math.ethz.ch
C?pia: 
Data: Fri, 23 Jun 2006 13:28:07 +0200 (CEST)
Assunto: [R] PowerPoint

> Dear All,
>    
>   I am looking for the best way to use graphs from R (like xyplot, curve ...)   for a presentation with powerpoint. I used to save my plot as pdf and after to copy them as image in powerpoint but the quality is not optimal by so doing.
>    
>   Another completely independent question is the following: when I use "main"  in the  xyplot, the main title is very close to my plot, i.e. no ligne separate the main and the plot. I would like my title to be well distinguished from the plots.
>    
>   I would be grateful for any improvements...
>    
>   Many thanks,
>    
>   Bernard,
>    
>     
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From cooch17 at verizon.net  Fri Jun 23 15:35:25 2006
From: cooch17 at verizon.net (Evan Cooch)
Date: Fri, 23 Jun 2006 09:35:25 -0400
Subject: [R] command line boa problems...
Message-ID: <449BEE1D.1010005@verizon.net>

Greetings -

For a number of reasons, I'm moving from CODA to BOA - and I have one or 
two really basic, boa-newbie questions. While I have the 'menu-driven' 
version of boa working fine (most recent version, running under R 2.3.1 
on a Windows machine), for the life of me I can't seem to get some basic 
boa.xxxx command-line functions to work at all. Even things like 
boa.version() or boa.license() return errors. But, boa.init() and other 
functions seem to work fine.

At this stage, I'd be happy getting the first - fairly key - function to 
work - boa.importASCII. My MCMC sample data are in a flat ASCII file 
(white-space delimited), called practice.txt. There are two columns in 
the file (iteration number, parameter value), both labeled in the first 
row of the file (as per instructions in the boa documentation). I know 
the file is formatted OK, because I can import and play with it 
successfully using the boa.menu() approach. But, for some reason, 
boa.importASCII won't touch it.

Suppose the file is on my desktop (remember, windows machine)

I've tried all the variuous front-slash, back-slash, double-slash etc. 
combinations I can think of...

boa.importASCII(practice,"c:\documents and settings\eg7\desktop")

boa.importASCII(practice,"c:\\documents and settings\\eg7\\desktop")

boa.importASCII(practice,"c:/documents and settings/eg7/desktop")

boa.importASCII(practice,path="c:/documents and settings/eg7/desktop")

boa.importASCII(practice,path="c:\\documents and settings\\eg7\\desktop")

and so on...and so on...in each case, boa.importASCII reports that

Error in paste(prefix, boa.par("ASCIIext"), sep = "") :
       object "practice" not found

or something to that effect - basically, practice.txt isn't being found.

Help! 

Thanks!


From seanpor at acm.org  Fri Jun 23 15:37:53 2006
From: seanpor at acm.org (Sean O'Riordain)
Date: Fri, 23 Jun 2006 14:37:53 +0100
Subject: [R] PowerPoint
In-Reply-To: <971536df0606230621t1584d4aer7cd4778978600fd0@mail.gmail.com>
References: <2323A6D37908A847A7C32F1E3662C80E132136@dc1ex01.air.org>
	<971536df0606230621t1584d4aer7cd4778978600fd0@mail.gmail.com>
Message-ID: <8ed68eed0606230637gc0ef611ye57d0a0d0024c0@mail.gmail.com>

or try win.metafile()

if i'm in a hurry, (in windows) i just right click on the graph and
select "Copy as Metafile" and paste directly into powerpoint...

Sean


On 23/06/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Note that jpg, bmp and png are in less desirable bit mapped formats whereas
> eps is in a more desirable vector format (magnification and shrinking does
> not involve loss of info) and so would be preferable from a quality
> viewpoint.  See:
> http://www.stc-saz.org/resources/0203_graphics.pdf
>
> On 6/23/06, Doran, Harold <HDoran at air.org> wrote:
> > Use the functions in library(grDevices) for jpeg, bmp, or png formats.
> > Or, you can use postscript() for an eps file. Of course, I personally
> > think tex files make for much better looking presentations if you can be
> > persuaded.
> >
> > Harold
> >
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marc Bernard
> > > Sent: Friday, June 23, 2006 7:28 AM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] PowerPoint
> > >
> > > Dear All,
> > >
> > >   I am looking for the best way to use graphs from R (like
> > > xyplot, curve ...)   for a presentation with powerpoint. I
> > > used to save my plot as pdf and after to copy them as image
> > > in powerpoint but the quality is not optimal by so doing.
> > >
> > >   Another completely independent question is the following:
> > > when I use "main"  in the  xyplot, the main title is very
> > > close to my plot, i.e. no ligne separate the main and the
> > > plot. I would like my title to be well distinguished from the plots.
> > >
> > >   I would be grateful for any improvements...
> > >
> > >   Many thanks,
> > >
> > >   Bernard,
> > >
> > >
> > >
> > >
> > > ---------------------------------
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From plummer at iarc.fr  Fri Jun 23 15:46:15 2006
From: plummer at iarc.fr (Martyn Plummer)
Date: Fri, 23 Jun 2006 15:46:15 +0200
Subject: [R] command line boa problems...
In-Reply-To: <449BEE1D.1010005@verizon.net>
References: <449BEE1D.1010005@verizon.net>
Message-ID: <1151070375.7408.5.camel@seurat.iarc.fr>

You need to give the file name in quotes. If you do not, R will look for
an object of that name in your work space. Note that the error message
is "object practice not found", not "file practice.txt not found."

You might also need to give the file extension, if this is not added by
the boa.importASCII function.

What's wrong with coda anyway? Just curious.

Martyn

On Fri, 2006-06-23 at 09:35 -0400, Evan Cooch wrote:
> Greetings -
> 
> For a number of reasons, I'm moving from CODA to BOA - and I have one or 
> two really basic, boa-newbie questions. While I have the 'menu-driven' 
> version of boa working fine (most recent version, running under R 2.3.1 
> on a Windows machine), for the life of me I can't seem to get some basic 
> boa.xxxx command-line functions to work at all. Even things like 
> boa.version() or boa.license() return errors. But, boa.init() and other 
> functions seem to work fine.
> 
> At this stage, I'd be happy getting the first - fairly key - function to 
> work - boa.importASCII. My MCMC sample data are in a flat ASCII file 
> (white-space delimited), called practice.txt. There are two columns in 
> the file (iteration number, parameter value), both labeled in the first 
> row of the file (as per instructions in the boa documentation). I know 
> the file is formatted OK, because I can import and play with it 
> successfully using the boa.menu() approach. But, for some reason, 
> boa.importASCII won't touch it.
> 
> Suppose the file is on my desktop (remember, windows machine)
> 
> I've tried all the variuous front-slash, back-slash, double-slash etc. 
> combinations I can think of...
> 
> boa.importASCII(practice,"c:\documents and settings\eg7\desktop")
> 
> boa.importASCII(practice,"c:\\documents and settings\\eg7\\desktop")
> 
> boa.importASCII(practice,"c:/documents and settings/eg7/desktop")
> 
> boa.importASCII(practice,path="c:/documents and settings/eg7/desktop")
> 
> boa.importASCII(practice,path="c:\\documents and settings\\eg7\\desktop")
> 
> and so on...and so on...in each case, boa.importASCII reports that
> 
> Error in paste(prefix, boa.par("ASCIIext"), sep = "") :
>        object "practice" not found
> 
> or something to that effect - basically, practice.txt isn't being found.
> 
> Help! 
> 
> Thanks!


-----------------------------------------------------------------------
This message and its attachments are strictly confidential. ...{{dropped}}


From jranke at uni-bremen.de  Fri Jun 23 15:58:44 2006
From: jranke at uni-bremen.de (Johannes Ranke)
Date: Fri, 23 Jun 2006 15:58:44 +0200
Subject: [R] PowerPoint
In-Reply-To: <20060623112807.74518.qmail@web25806.mail.ukl.yahoo.com>
References: <20060623112807.74518.qmail@web25806.mail.ukl.yahoo.com>
Message-ID: <20060623135844.GA7608@mail.uft.uni-bremen.de>

Dear Bernard,

if you use MS Powerpoint, it seems likely to me that you are using the
Windows version of R. Are you aware of the fact, that you can just
right-click on any graph and copy it to the clipboard (copy as metafile
or similar).

That way you get a vectorized version of the graph, which you can nicely
paste into Powerpoint and edit.

Johannes

* Marc Bernard <bernarduse1 at yahoo.fr> [060623 13:40]:
> Dear All,
>    
>   I am looking for the best way to use graphs from R (like xyplot, curve ...)   for a presentation with powerpoint. I used to save my plot as pdf and after to copy them as image in powerpoint but the quality is not optimal by so doing.
>    
>   Another completely independent question is the following: when I use "main"  in the  xyplot, the main title is very close to my plot, i.e. no ligne separate the main and the plot. I would like my title to be well distinguished from the plots.
>    
>   I would be grateful for any improvements...
>    
>   Many thanks,
>    
>   Bernard,
>    
>     
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Dr. Johannes Ranke                 jranke at uni-bremen.de
UFT Bremen, Leobenerstr. 1         +49 421 218 8971 
D-28359 Bremen                     http://www.uft.uni-bremen.de/chemie/ranke


From paleologo at gmail.com  Fri Jun 23 16:21:09 2006
From: paleologo at gmail.com (Giuseppe Paleologo)
Date: Fri, 23 Jun 2006 10:21:09 -0400
Subject: [R] problem installing gsl package under Ubuntu Breezy Badger
Message-ID: <87654fa10606230721i6680205bl1ccaf738e9688e94@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060623/54336137/attachment.pl 

From Thomas.Petzoldt at TU-Dresden.de  Fri Jun 23 16:30:46 2006
From: Thomas.Petzoldt at TU-Dresden.de (Thomas Petzoldt)
Date: Fri, 23 Jun 2006 16:30:46 +0200
Subject: [R] assign / environment side effect on R 2.4.0
Message-ID: <449BFB16.6010501@TU-Dresden.de>

Hello,

I got several off-list answers to my question on R-Help:

"[R] list of interdependent functions" from  2006-06-20

and by evaluating this (and also my own example) I found differences in
the behavior of older versions of R (R 2.2.1 and 2.3.1) and the most
recent R 2.4. under development (SVN revision 38399, WinXP SP 2).

The example is a constructed cut-down  example, but this behavior is
observed also in different versions of the full implementation.

While in the older versions the environment of L$test remains
R_Globalenv, a changed environment is returned under R 2.4.0

Ist this side effect a "bug" or a "feature"? The list(unlist(L))
workaround helps to avoid the side effect.

Thomas




envfun <- function(L) {
  # L <- as.list(unlist(L)) # !!! workaround
  p <- parent.frame()
  assign("test", L$test, p)
  environment(p[["test"]]) <- p
}


solver <- function(L) {
  envfun(L)
  # some other stuff
}

L <- list(test = function() 1 + 2)

e1 <- environment(L$test)
solver(L)
e2 <- environment(L$test)

print(e1)
# <environment: R_GlobalEnv>

print(e2)
# <environment: 0x01d0b088>


-- 
Thomas Petzoldt                    Tel. +49-351-463 3 4954
Technische Universitaet Dresden    Fax  +49-351-463 3 7108
Institut fuer Hydrobiologie        thomas.petzoldt at tu-dresden.de
01062 Dresden                      http://tu-dresden.de/hydrobiologie/
GERMANY

Upcoming: German Limnology Conference!    http://tu-dresden.de/dgl2006


From sundar.dorai-raj at pdf.com  Fri Jun 23 16:55:26 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 23 Jun 2006 09:55:26 -0500
Subject: [R] PowerPoint
In-Reply-To: <20060623135844.GA7608@mail.uft.uni-bremen.de>
References: <20060623112807.74518.qmail@web25806.mail.ukl.yahoo.com>
	<20060623135844.GA7608@mail.uft.uni-bremen.de>
Message-ID: <449C00DE.4050005@pdf.com>

Hi, all,

(Sorry to highjack the thread, but I think the OP should also know this)

One of the plots Marc mentions is xyplot. Has anybody else on this list 
had a problem with lattice and win.metafile (or Ctrl-W in the R graphics 
device)? I will sometimes import wmf files (or Ctrl-V) with lattice 
graphics into powerpoint and notice some of the border lines are 
missing. I can re-size the plot to make the lines reappear but have to 
find just the right size to make it look right. This seems to be a 
problem with PPT, XLS, and Word. I never have this problem with 
traditional graphics (e.g. plot.default, etc.).

I'm using Windows XP Pro with R-2.3.1 and lattice-0.13.8, though I've 
also experienced the problem on earlier versions of R and earlier 
versions of lattice.

Thanks,

--sundar

Johannes Ranke wrote:
> Dear Bernard,
> 
> if you use MS Powerpoint, it seems likely to me that you are using the
> Windows version of R. Are you aware of the fact, that you can just
> right-click on any graph and copy it to the clipboard (copy as metafile
> or similar).
> 
> That way you get a vectorized version of the graph, which you can nicely
> paste into Powerpoint and edit.
> 
> Johannes
> 
> * Marc Bernard <bernarduse1 at yahoo.fr> [060623 13:40]:
> 
>>Dear All,
>>   
>>  I am looking for the best way to use graphs from R (like xyplot, curve ...)   for a presentation with powerpoint. I used to save my plot as pdf and after to copy them as image in powerpoint but the quality is not optimal by so doing.
>>   
>>  Another completely independent question is the following: when I use "main"  in the  xyplot, the main title is very close to my plot, i.e. no ligne separate the main and the plot. I would like my title to be well distinguished from the plots.
>>   
>>  I would be grateful for any improvements...
>>   
>>  Many thanks,
>>   
>>  Bernard,
>>   
>>    
>>
>> 		
>>---------------------------------
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>


From I.J.Wilson at ncl.ac.uk  Fri Jun 23 16:57:26 2006
From: I.J.Wilson at ncl.ac.uk (Ian Wilson)
Date: Fri, 23 Jun 2006 15:57:26 +0100 (BST)
Subject: [R] problem installing gsl package under Ubuntu Breezy Badger
In-Reply-To: <87654fa10606230721i6680205bl1ccaf738e9688e94@mail.gmail.com>
References: <87654fa10606230721i6680205bl1ccaf738e9688e94@mail.gmail.com>
Message-ID: <1643.10.64.208.150.1151074646.squirrel@rws2.ncl.ac.uk>

You need the libgsl-dev package.  The gsl-bin package just contains the
example programs.

on breezy

# apt-get install libgsl0-dev


Ian

On Fri, June 23, 2006 3:21 pm, Giuseppe Paleologo wrote:
> I am trying to install the gls package (a wrapper for GNU scientific
> library
> special functions) package under Ubuntu 5.10. I have gls-bin (the debian
> GNU
> Scientific Library binary package). When I try to install the R package, I
> receive the following.
>
>> install.packages("gsl",dependencies=T)
> Warning in install.packages("gsl", dependencies = T) :
>          argument 'lib' is missing: using /usr/local/lib/R/site-library
> trying URL 'http://lib.stat.cmu.edu/R/CRAN/src/contrib/gsl_1.6-6.tar.gz'
> Content type 'application/x-gzip' length 50969 bytes
> opened URL
> ==================================================
> downloaded 49Kb
>
> * Installing *source* package 'gsl' ...
> checking for gcc... gcc
> checking for C compiler default output... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables...
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ANSI C... none needed
> checking for gsl_sf_airy_Ai_e in -lgsl... no
> configure: error: Cannot find Gnu Scientific Library.
> ERROR: configuration failed for package 'gsl'
> The downloaded packages are in
>         /tmp/RtmpbxQ4fl/downloaded_packages
> Warning message:
> installation of package 'gsl' had non-zero exit status in:
> install.packages("gsl",
> dependencies = T)
>
>
> as if the gls-bin package were absent. Any insights? Thanks in advance,
>
>
> - gappy (International Man of Mistery)
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


From edd at debian.org  Fri Jun 23 17:05:17 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 23 Jun 2006 10:05:17 -0500
Subject: [R] problem installing gsl package under Ubuntu Breezy Badger
In-Reply-To: <87654fa10606230721i6680205bl1ccaf738e9688e94@mail.gmail.com>
References: <87654fa10606230721i6680205bl1ccaf738e9688e94@mail.gmail.com>
Message-ID: <20060623150517.GA5647@eddelbuettel.com>

On Fri, Jun 23, 2006 at 10:21:09AM -0400, Giuseppe Paleologo wrote:
> I am trying to install the gls package (a wrapper for GNU scientific library
> special functions) package under Ubuntu 5.10. I have gls-bin (the debian GNU
> Scientific Library binary package). When I try to install the R package, I

You also need the corresponding -dev package to compile. 

Hth, Dirk

> receive the following.
> 
> > install.packages("gsl",dependencies=T)
> Warning in install.packages("gsl", dependencies = T) :
>          argument 'lib' is missing: using /usr/local/lib/R/site-library
> trying URL 'http://lib.stat.cmu.edu/R/CRAN/src/contrib/gsl_1.6-6.tar.gz'
> Content type 'application/x-gzip' length 50969 bytes
> opened URL
> ==================================================
> downloaded 49Kb
> 
> * Installing *source* package 'gsl' ...
> checking for gcc... gcc
> checking for C compiler default output... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables...
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ANSI C... none needed
> checking for gsl_sf_airy_Ai_e in -lgsl... no
> configure: error: Cannot find Gnu Scientific Library.
> ERROR: configuration failed for package 'gsl'
> The downloaded packages are in
>         /tmp/RtmpbxQ4fl/downloaded_packages
> Warning message:
> installation of package 'gsl' had non-zero exit status in:
> install.packages("gsl",
> dependencies = T)
> 
> 
> as if the gls-bin package were absent. Any insights? Thanks in advance,
> 
> 
> - gappy (International Man of Mistery)
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From jranke at uni-bremen.de  Fri Jun 23 17:09:05 2006
From: jranke at uni-bremen.de (Johannes Ranke)
Date: Fri, 23 Jun 2006 17:09:05 +0200
Subject: [R] problem installing gsl package under Ubuntu Breezy Badger
In-Reply-To: <87654fa10606230721i6680205bl1ccaf738e9688e94@mail.gmail.com>
References: <87654fa10606230721i6680205bl1ccaf738e9688e94@mail.gmail.com>
Message-ID: <20060623150905.GA9676@mail.uft.uni-bremen.de>

Hi Giuseppe,

you need the -dev package for it (header files). In Debian, this is
called libgsl0-dev, so chances are good that the name is the same in
Ubuntu. Otherwise you might want to try 
	
	sudo apt-cache search gsl

Best regards,

Johannes

* Giuseppe Paleologo <paleologo at gmail.com> [060623 16:30]:
> I am trying to install the gls package (a wrapper for GNU scientific library
> special functions) package under Ubuntu 5.10. I have gls-bin (the debian GNU
> Scientific Library binary package). When I try to install the R package, I
> receive the following.
> 
> > install.packages("gsl",dependencies=T)
> Warning in install.packages("gsl", dependencies = T) :
>          argument 'lib' is missing: using /usr/local/lib/R/site-library
> trying URL 'http://lib.stat.cmu.edu/R/CRAN/src/contrib/gsl_1.6-6.tar.gz'
> Content type 'application/x-gzip' length 50969 bytes
> opened URL
> ==================================================
> downloaded 49Kb
> 
> * Installing *source* package 'gsl' ...
> checking for gcc... gcc
> checking for C compiler default output... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables...
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ANSI C... none needed
> checking for gsl_sf_airy_Ai_e in -lgsl... no
> configure: error: Cannot find Gnu Scientific Library.
> ERROR: configuration failed for package 'gsl'
> The downloaded packages are in
>         /tmp/RtmpbxQ4fl/downloaded_packages
> Warning message:
> installation of package 'gsl' had non-zero exit status in:
> install.packages("gsl",
> dependencies = T)
> 
> 
> as if the gls-bin package were absent. Any insights? Thanks in advance,
> 
> 
> - gappy (International Man of Mistery)
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Dr. Johannes Ranke                 jranke at uni-bremen.de
UFT Bremen, Leobenerstr. 1         +49 421 218 8971 
D-28359 Bremen                     http://www.uft.uni-bremen.de/chemie/ranke


From gunter.berton at gene.com  Fri Jun 23 17:08:12 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 23 Jun 2006 08:08:12 -0700
Subject: [R] PowerPoint
In-Reply-To: <449C00DE.4050005@pdf.com>
Message-ID: <002e01c696d6$d8791720$f8c2fea9@gne.windows.gene.com>


I've always assumed that this was a rendering problem in the MS application,
as the reappearance of the missing lines on re-sizing shows that that the
necessary information **is** in the imported .wmf file, right?

-- Bert 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sundar 
> Dorai-Raj
> Sent: Friday, June 23, 2006 7:55 AM
> To: Johannes Ranke
> Cc: r-help at stat.math.ethz.ch; Marc Bernard
> Subject: Re: [R] PowerPoint
> 
> Hi, all,
> 
> (Sorry to highjack the thread, but I think the OP should also 
> know this)
> 
> One of the plots Marc mentions is xyplot. Has anybody else on 
> this list 
> had a problem with lattice and win.metafile (or Ctrl-W in the 
> R graphics 
> device)? I will sometimes import wmf files (or Ctrl-V) with lattice 
> graphics into powerpoint and notice some of the border lines are 
> missing. I can re-size the plot to make the lines reappear 
> but have to 
> find just the right size to make it look right. This seems to be a 
> problem with PPT, XLS, and Word. I never have this problem with 
> traditional graphics (e.g. plot.default, etc.).
> 
> I'm using Windows XP Pro with R-2.3.1 and lattice-0.13.8, though I've 
> also experienced the problem on earlier versions of R and earlier 
> versions of lattice.
> 
> Thanks,
> 
> --sundar
> 
> Johannes Ranke wrote:
> > Dear Bernard,
> > 
> > if you use MS Powerpoint, it seems likely to me that you 
> are using the
> > Windows version of R. Are you aware of the fact, that you can just
> > right-click on any graph and copy it to the clipboard (copy 
> as metafile
> > or similar).
> > 
> > That way you get a vectorized version of the graph, which 
> you can nicely
> > paste into Powerpoint and edit.
> > 
> > Johannes
> > 
> > * Marc Bernard <bernarduse1 at yahoo.fr> [060623 13:40]:
> > 
> >>Dear All,
> >>   
> >>  I am looking for the best way to use graphs from R (like 
> xyplot, curve ...)   for a presentation with powerpoint. I 
> used to save my plot as pdf and after to copy them as image 
> in powerpoint but the quality is not optimal by so doing.
> >>   
> >>  Another completely independent question is the following: 
> when I use "main"  in the  xyplot, the main title is very 
> close to my plot, i.e. no ligne separate the main and the 
> plot. I would like my title to be well distinguished from the plots.
> >>   
> >>  I would be grateful for any improvements...
> >>   
> >>  Many thanks,
> >>   
> >>  Bernard,
> >>   
> >>    
> >>
> >> 		
> >>---------------------------------
> >>
> >>	[[alternative HTML version deleted]]
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From tlumley at u.washington.edu  Fri Jun 23 17:13:34 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 23 Jun 2006 08:13:34 -0700 (PDT)
Subject: [R] integrate
In-Reply-To: <449BDAB9.9080201@gmail.com>
References: <449BDAB9.9080201@gmail.com>
Message-ID: <Pine.LNX.4.64.0606230805380.23108@homer24.u.washington.edu>

On Fri, 23 Jun 2006, Rog?rio Rosa da Silva wrote:

> Dear All,
>
> My doubt about how to integrate a simple kernel density estimation goes on.
>
> I have seen the recent posts on integrate density estimation, which seem
> similar to my question. However, I haven't found a solution.
>
> I have made two simple kernel density estimation by:
>
>    kde.1 <-density(x, bw=sd(x), kernel="gaussian")$y     # x<- c(2,3,5,12)
>    kde.2 <-density(y, bw=sd(y), kernel="gaussian")$y     # y<- c(4,2,4,11)
>
> Now I would like to integrate the difference in the estimated density
> values, i.e.:
>
>    diff.kde <- abs (kde.1- kde.2)
>
> How can I integrate diff.kde over -Inf to Inf ?

Well, the answer is zero.

Computationally this is a bit tricky.  You can turn the density estimates 
into functions with approxfun()
x<-rexp(100)
kde<-density(x)
  f<-approxfun(kde$x,kde$y,rule=2)
integrate(f,-1,10)
1.000936 with absolute error < 3.3e-05

But if you want to integrate over -Inf to Inf you need the function to 
specify the values outside the range of the data.  The only value that 
will work over the range -Inf to Inf is zero
> f<-approxfun(kde$x,kde$y,yleft=0,yright=0)
> integrate(f,-1,10)
1.00072 with absolute error < 1.5e-05
> integrate(f,-Inf,Inf)
1.000811 with absolute error < 2.3e-05


 	-thomas

From thpe at hhbio.wasser.tu-dresden.de  Fri Jun 23 17:35:42 2006
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Fri, 23 Jun 2006 17:35:42 +0200
Subject: [R] assign / environment side effect on R 2.4.0
In-Reply-To: <449BFB16.6010501@TU-Dresden.de>
References: <449BFB16.6010501@TU-Dresden.de>
Message-ID: <449C0A4E.9040203@hhbio.wasser.tu-dresden.de>

Sorry,

the posted example had the side effect on all platforms (correctly: R
2.2.1/Windows, 2.3.1/Linux, 2.4.0/Windows), but in the following
corrected example the behavior of 2.4.0 differs from the older versions.

The only difference between the "wrong" and the "new" example is
L[["test"]] vs. L$test in the assign.

Thomas P.


envfun <- function(L) {
#  L <- as.list(unlist(L))
  p <- parent.frame()
  assign("test", L[["test"]], p) ## [["test"]] instead of $test
  environment(p[["test"]]) <- p
}


solver <- function(L) {
  envfun(L)
  # some other stuff
}

L <- list(test = function() 1 + 2)

e1 <- environment(L$test)
solver(L)
e2 <- environment(L$test)

print(e1)
print(e2)


From spencer.graves at pdf.com  Fri Jun 23 17:35:14 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 23 Jun 2006 08:35:14 -0700
Subject: [R] Bayesian logistic regression?
In-Reply-To: <4497F979.7020108@stat.columbia.edu>
References: <4497F979.7020108@stat.columbia.edu>
Message-ID: <449C0A32.40606@pdf.com>

	  I don't know of anything.  A brief search using RSiteSearch("Bayesian 
logistic regression") and RSiteSearch("Bayesian regression") led me to 
the BMA package plus several MCMC solutions (coda, MCMCpack, and 
BayesCslogistic {cslogistic}).  If it were my problem, I might spend a 
few minutes with BMA and then probably write my own.

	  I would like to see a (possibly singular) multivariate normal (or 
normal + inverse gamma) "prior" as an optional argument for lm and glm 
and when present would produce the obvious "posterior" [exact for lm and 
approximate for glm] as an attribute of the output.  A few years ago, 
wrote something to do this that would do ordinary least squares one step 
at a time and get the standard OLS answer (starting from a 
noninformative norma + inverse gamma prior).  From this, it is a short 
step to Kalman filtering:  Just add an appropriate "decay" function to 
increase the uncertainty to convert the posterior at one step into the 
prior for the next.

	  I'm sure this didn't help much other than confirm that your own 
search did not overlook something obvious.

	  Best Wishes,
	  Spencer Graves

Andrew Gelman wrote:
> Hi all.
> Are there any R functions around that do quick logistic regression with 
> a Gaussian prior distribution on the coefficients?  I just want 
> posterior mode, not MCMC.  (I'm using it as a step within an iterative 
> imputation algorithm.)  This isn't hard to do:  each step of a glm 
> iteration simply linearizes the derivative of the log-likelihood, and, 
> at this point, essentially no effort is required to augment the data to 
> include the prior information.  I think this can be done by going inside 
> the glm.fit() function--but if somebody's already done it, that would be 
> a relief!
> Thanks.
> Andrew
>


From sundar.dorai-raj at pdf.com  Fri Jun 23 17:42:23 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 23 Jun 2006 10:42:23 -0500
Subject: [R] PowerPoint
In-Reply-To: <002e01c696d6$d8791720$f8c2fea9@gne.windows.gene.com>
References: <002e01c696d6$d8791720$f8c2fea9@gne.windows.gene.com>
Message-ID: <449C0BDF.7070001@pdf.com>

Hi, Bert,

Yes, this is true. However, it seems only to be a problem with lattice 
graphics and not traditional. So I am confused as to why there is a 
difference.

Thanks,

--sundar

Berton Gunter wrote:
> I've always assumed that this was a rendering problem in the MS application,
> as the reappearance of the missing lines on re-sizing shows that that the
> necessary information **is** in the imported .wmf file, right?
> 
> -- Bert 
>  
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sundar 
>>Dorai-Raj
>>Sent: Friday, June 23, 2006 7:55 AM
>>To: Johannes Ranke
>>Cc: r-help at stat.math.ethz.ch; Marc Bernard
>>Subject: Re: [R] PowerPoint
>>
>>Hi, all,
>>
>>(Sorry to highjack the thread, but I think the OP should also 
>>know this)
>>
>>One of the plots Marc mentions is xyplot. Has anybody else on 
>>this list 
>>had a problem with lattice and win.metafile (or Ctrl-W in the 
>>R graphics 
>>device)? I will sometimes import wmf files (or Ctrl-V) with lattice 
>>graphics into powerpoint and notice some of the border lines are 
>>missing. I can re-size the plot to make the lines reappear 
>>but have to 
>>find just the right size to make it look right. This seems to be a 
>>problem with PPT, XLS, and Word. I never have this problem with 
>>traditional graphics (e.g. plot.default, etc.).
>>
>>I'm using Windows XP Pro with R-2.3.1 and lattice-0.13.8, though I've 
>>also experienced the problem on earlier versions of R and earlier 
>>versions of lattice.
>>
>>Thanks,
>>
>>--sundar
>>
>>Johannes Ranke wrote:
>>
>>>Dear Bernard,
>>>
>>>if you use MS Powerpoint, it seems likely to me that you 
>>
>>are using the
>>
>>>Windows version of R. Are you aware of the fact, that you can just
>>>right-click on any graph and copy it to the clipboard (copy 
>>
>>as metafile
>>
>>>or similar).
>>>
>>>That way you get a vectorized version of the graph, which 
>>
>>you can nicely
>>
>>>paste into Powerpoint and edit.
>>>
>>>Johannes
>>>
>>>* Marc Bernard <bernarduse1 at yahoo.fr> [060623 13:40]:
>>>
>>>
>>>>Dear All,
>>>>  
>>>> I am looking for the best way to use graphs from R (like 
>>
>>xyplot, curve ...)   for a presentation with powerpoint. I 
>>used to save my plot as pdf and after to copy them as image 
>>in powerpoint but the quality is not optimal by so doing.
>>
>>>>  
>>>> Another completely independent question is the following: 
>>
>>when I use "main"  in the  xyplot, the main title is very 
>>close to my plot, i.e. no ligne separate the main and the 
>>plot. I would like my title to be well distinguished from the plots.
>>
>>>>  
>>>> I would be grateful for any improvements...
>>>>  
>>>> Many thanks,
>>>>  
>>>> Bernard,
>>>>  
>>>>   
>>>>
>>>>		
>>>>---------------------------------
>>>>
>>>>	[[alternative HTML version deleted]]
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! 
>>
>>http://www.R-project.org/posting-guide.html
>>
>>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From mschwartz at mn.rr.com  Fri Jun 23 17:52:32 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 23 Jun 2006 10:52:32 -0500
Subject: [R] PowerPoint
In-Reply-To: <002e01c696d6$d8791720$f8c2fea9@gne.windows.gene.com>
References: <002e01c696d6$d8791720$f8c2fea9@gne.windows.gene.com>
Message-ID: <1151077952.3828.13.camel@localhost.localdomain>

Has anyone tried this with OO.org's Impress or Writer on Windows to see
if the same behavior occurs?  

My recollection from prior experience on Windows (it's been a while) is
that a subtle resize takes place when pasting/importing graphics into
the aforementioned apps. You can right click on the graphic in the app
and then select "Original Size" or something worded similarly on the
graphic object formatting dialog window. Not sure if that is enough to
get the lines back or if one has to go slightly larger than the original
size to resolve the issue.

It also seems to me that there was some behavior on the R Windows
graphic device relative to re-sizing the plot region and then doing the
metafile copy and paste, but it has been long enough that my memory may
not be intact (which my wife would suggest anyway....  ;-).

HTH,

Marc Schwartz


On Fri, 2006-06-23 at 08:08 -0700, Berton Gunter wrote:
> I've always assumed that this was a rendering problem in the MS application,
> as the reappearance of the missing lines on re-sizing shows that that the
> necessary information **is** in the imported .wmf file, right?
> 
> -- Bert 
>  
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sundar 
> > Dorai-Raj
> > Sent: Friday, June 23, 2006 7:55 AM
> > To: Johannes Ranke
> > Cc: r-help at stat.math.ethz.ch; Marc Bernard
> > Subject: Re: [R] PowerPoint
> > 
> > Hi, all,
> > 
> > (Sorry to highjack the thread, but I think the OP should also 
> > know this)
> > 
> > One of the plots Marc mentions is xyplot. Has anybody else on 
> > this list 
> > had a problem with lattice and win.metafile (or Ctrl-W in the 
> > R graphics 
> > device)? I will sometimes import wmf files (or Ctrl-V) with lattice 
> > graphics into powerpoint and notice some of the border lines are 
> > missing. I can re-size the plot to make the lines reappear 
> > but have to 
> > find just the right size to make it look right. This seems to be a 
> > problem with PPT, XLS, and Word. I never have this problem with 
> > traditional graphics (e.g. plot.default, etc.).
> > 
> > I'm using Windows XP Pro with R-2.3.1 and lattice-0.13.8, though I've 
> > also experienced the problem on earlier versions of R and earlier 
> > versions of lattice.
> > 
> > Thanks,
> > 
> > --sundar
> > 
> > Johannes Ranke wrote:
> > > Dear Bernard,
> > > 
> > > if you use MS Powerpoint, it seems likely to me that you 
> > are using the
> > > Windows version of R. Are you aware of the fact, that you can just
> > > right-click on any graph and copy it to the clipboard (copy 
> > as metafile
> > > or similar).
> > > 
> > > That way you get a vectorized version of the graph, which 
> > you can nicely
> > > paste into Powerpoint and edit.
> > > 
> > > Johannes
> > > 
> > > * Marc Bernard <bernarduse1 at yahoo.fr> [060623 13:40]:
> > > 
> > >>Dear All,
> > >>   
> > >>  I am looking for the best way to use graphs from R (like 
> > xyplot, curve ...)   for a presentation with powerpoint. I 
> > used to save my plot as pdf and after to copy them as image 
> > in powerpoint but the quality is not optimal by so doing.
> > >>   
> > >>  Another completely independent question is the following: 
> > when I use "main"  in the  xyplot, the main title is very 
> > close to my plot, i.e. no ligne separate the main and the 
> > plot. I would like my title to be well distinguished from the plots.
> > >>   
> > >>  I would be grateful for any improvements...
> > >>   
> > >>  Many thanks,
> > >>   
> > >>  Bernard,


From muster at gmail.com  Fri Jun 23 17:53:21 2006
From: muster at gmail.com (Mu Tian)
Date: Fri, 23 Jun 2006 11:53:21 -0400
Subject: [R] columnwise multiplication?
Message-ID: <b68812e70606230853v1f5caee7hdc64dcc0947532a2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060623/7e72fcb9/attachment.pl 

From ripley at stats.ox.ac.uk  Fri Jun 23 18:04:32 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Jun 2006 17:04:32 +0100 (BST)
Subject: [R] columnwise multiplication?
In-Reply-To: <b68812e70606230853v1f5caee7hdc64dcc0947532a2@mail.gmail.com>
References: <b68812e70606230853v1f5caee7hdc64dcc0947532a2@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0606231703250.17754@gannet.stats.ox.ac.uk>

On Fri, 23 Jun 2006, Mu Tian wrote:

> Hi all,
>
> I'd like to do a multiplication between 2 matrices buy only want resulsts of
> cloumn 1 * column 1, column 2 * column 2 and so on.
>
> Now I do
>
> C <- diag(t(A) %*% B)
>
> Is there a bulit in way to do this?

If * here means vector inner product, colSums(A*B) which is a lot more 
efficient.

>
> Thank you.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mschwartz at mn.rr.com  Fri Jun 23 18:10:13 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 23 Jun 2006 11:10:13 -0500
Subject: [R] columnwise multiplication?
In-Reply-To: <b68812e70606230853v1f5caee7hdc64dcc0947532a2@mail.gmail.com>
References: <b68812e70606230853v1f5caee7hdc64dcc0947532a2@mail.gmail.com>
Message-ID: <1151079014.3828.25.camel@localhost.localdomain>

On Fri, 2006-06-23 at 11:53 -0400, Mu Tian wrote:
> Hi all,
> 
> I'd like to do a multiplication between 2 matrices buy only want resulsts of
> cloumn 1 * column 1, column 2 * column 2 and so on.
> 
> Now I do
> 
> C <- diag(t(A) %*% B)
> 
> Is there a bulit in way to do this?
> 
> Thank you.


You just want:

  A * B

for the initial multiplication. This technically gives you element by
element multiplication. Since matrices are vectors with a dim attribute
and elements stored by default in column order (top to bottom, then left
to right), the result matrix will yield what you require on a column by
column basis.

For example:

> A <- matrix(1:12, ncol = 3)
> B <- matrix(1:12, ncol = 3)

> A
     [,1] [,2] [,3]
[1,]    1    5    9
[2,]    2    6   10
[3,]    3    7   11
[4,]    4    8   12

> B
     [,1] [,2] [,3]
[1,]    1    5    9
[2,]    2    6   10
[3,]    3    7   11
[4,]    4    8   12

> A * B
     [,1] [,2] [,3]
[1,]    1   25   81
[2,]    4   36  100
[3,]    9   49  121
[4,]   16   64  144


To then get cumulative column totals, you can do:

> colSums(A * B)
[1]  30 174 446


HTH,

Marc Schwartz


From f.calboli at imperial.ac.uk  Fri Jun 23 18:10:08 2006
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Fri, 23 Jun 2006 17:10:08 +0100
Subject: [R] rearranging data frame rows
Message-ID: <449C1260.1040603@imperial.ac.uk>

Hi All,

I have two data frames. The first contains data about a number of individuals, 
coded in the first column with a name, in an order I find convenient.

The second contains different data about the same indivduals, in a different 
order. Both data frame have the individual names in the first column.

I need to reorder the second data frame so the rows are rearranged in the same 
manner as the fist. How?

I cannot turn the individual names in a numeric vairable with 
as.numeric(data1[,1]), because the two data frames are subset of different data, 
so the the factor levels are way off between the two. I think I need to actually 
use the names as a index.

Cheers,

Fede

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com


From philipp.pagel.lists at t-online.de  Fri Jun 23 18:16:49 2006
From: philipp.pagel.lists at t-online.de (Philipp Pagel)
Date: Fri, 23 Jun 2006 18:16:49 +0200
Subject: [R] PowerPoint
In-Reply-To: <971536df0606230621t1584d4aer7cd4778978600fd0@mail.gmail.com>
References: <2323A6D37908A847A7C32F1E3662C80E132136@dc1ex01.air.org>
	<971536df0606230621t1584d4aer7cd4778978600fd0@mail.gmail.com>
Message-ID: <20060623161649.GA5532@gsf.de>

On Fri, Jun 23, 2006 at 09:21:37AM -0400, Gabor Grothendieck wrote:
> Note that jpg, bmp and png are in less desirable bit mapped formats whereas
> eps is in a more desirable vector format (magnification and shrinking does
> not involve loss of info) and so would be preferable from a quality
> viewpoint.

In addition to seconding the above statement I'd like to add that in
cases where you are forced to use a bitmap format png tends to produce
much better results than jpg where line drawings (e.g. most plots) are
concerned. JPG format on the other hand is great for anyting which can be
discirbed as photography-like. jpg images of plots tend to suffer from
bad artifacts...

cu
	Philipp

-- 
Dr. Philipp Pagel                            Tel.  +49-8161-71 2131
Dept. of Genome Oriented Bioinformatics      Fax.  +49-8161-71 2186
Technical University of Munich
Science Center Weihenstephan
85350 Freising, Germany

 and

Institute for Bioinformatics / MIPS          Tel.  +49-89-3187 3675
GSF - National Research Center               Fax.  +49-89-3187 3585
      for Environment and Health
Ingolst?dter Landstrasse 1
85764 Neuherberg, Germany
http://mips.gsf.de/staff/pagel


From vdemart1 at tin.it  Fri Jun 23 18:23:47 2006
From: vdemart1 at tin.it (Vittorio)
Date: Fri, 23 Jun 2006 17:23:47 +0100 (GMT+01:00)
Subject: [R] Producing png plot in batch mode
Message-ID: <10c01b446ea.vdemart1@tin.it>

I have set up an R procedure that is launched every three hours by 
crontab in a unix server. Crontab runs at regular intervals the 
following line:
R CMD BATH myprog.R

myprog.R (which by the way uses 
R2HTML) should create an updated png graph  to be referred to and seen 
in an intranet web-page index.html.

The problem is that both:

png
(....) 
plot(...) 
dev.off() 

AND:

plot(...)
HTMLplot(...)

fail when 
launched in a batch manner compalining that they need an X11() instance 
to be used (I understand that they work only in a graphic context and 
intarictively).

How can I obtain that png file? 

Ciao
Vittorio


From vdemart1 at tin.it  Fri Jun 23 18:25:13 2006
From: vdemart1 at tin.it (Vittorio)
Date: Fri, 23 Jun 2006 17:25:13 +0100 (GMT+01:00)
Subject: [R] R:  rearranging data frame rows
Message-ID: <10c01b598cf.vdemart1@tin.it>

Have a look at merge.
Ciao
Vittorio

>----Messaggio originale----
>Da: 
f.calboli at imperial.ac.uk
>Data: 23-giu-2006 18.10
>A: "r-help"<r-
help at stat.math.ethz.ch>
>Ogg: [R] rearranging data frame rows
>
>Hi 
All,
>
>I have two data frames. The first contains data about a number 
of individuals, 
>coded in the first column with a name, in an order I 
find convenient.
>
>The second contains different data about the same 
indivduals, in a different 
>order. Both data frame have the individual 
names in the first column.
>
>I need to reorder the second data frame 
so the rows are rearranged in the same 
>manner as the fist. How?
>
>I 
cannot turn the individual names in a numeric vairable with 
>as.numeric
(data1[,1]), because the two data frames are subset of different data, 
>so the the factor levels are way off between the two. I think I need 
to actually 
>use the names as a index.
>
>Cheers,
>
>Fede
>
>-- 
>Federico C. F. Calboli
>Department of Epidemiology and Public Health
>Imperial College, St Mary's Campus
>Norfolk Place, London W2 1PG
>
>Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193
>
>f.calboli [.a.
t] imperial.ac.uk
>f.calboli [.a.t] gmail.com
>
>______________________________________________
>R-help at stat.math.ethz.
ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE 
do read the posting guide! http://www.R-project.org/posting-guide.html
>


From mschwartz at mn.rr.com  Fri Jun 23 18:27:00 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 23 Jun 2006 11:27:00 -0500
Subject: [R] PowerPoint
In-Reply-To: <20060623161649.GA5532@gsf.de>
References: <2323A6D37908A847A7C32F1E3662C80E132136@dc1ex01.air.org>
	<971536df0606230621t1584d4aer7cd4778978600fd0@mail.gmail.com>
	<20060623161649.GA5532@gsf.de>
Message-ID: <1151080021.3828.42.camel@localhost.localdomain>

On Fri, 2006-06-23 at 18:16 +0200, Philipp Pagel wrote:
> On Fri, Jun 23, 2006 at 09:21:37AM -0400, Gabor Grothendieck wrote:
> > Note that jpg, bmp and png are in less desirable bit mapped formats whereas
> > eps is in a more desirable vector format (magnification and shrinking does
> > not involve loss of info) and so would be preferable from a quality
> > viewpoint.
> 
> In addition to seconding the above statement I'd like to add that in
> cases where you are forced to use a bitmap format png tends to produce
> much better results than jpg where line drawings (e.g. most plots) are
> concerned. JPG format on the other hand is great for anyting which can be
> discirbed as photography-like. jpg images of plots tend to suffer from
> bad artifacts...
> 
> cu
> 	Philipp


That is generally because png files are not compressed, whereas jpg
files are. 

The compression algorithms that are typically used are lossy, which
means that you give up image quality in order to gain the reduction in
file size. The greater the compression you use, the greater the loss in
image quality.

Yet another reason to use EPS for plots.

HTH,

Marc Schwartz


From M.J.Bojanowski at fss.uu.nl  Fri Jun 23 18:27:45 2006
From: M.J.Bojanowski at fss.uu.nl (Bojanowski, M.J.  (Michal))
Date: Fri, 23 Jun 2006 18:27:45 +0200
Subject: [R] problem with hist() for 'times' objects from 'chron' package
Message-ID: <94E133D09AA24D43BF6341B675C01A33113394@uu01msg-exb01.soliscom.uu.nl>

Hello dear useRs and wizaRds,

I encountered the following problem using the hist() method for the
'times' classes
from package 'chron'. You should be able to recreate it using the code:



library(chron)

# pasted from chron help file (?chron)
dts <- dates(c("02/27/92", "02/27/92", "01/14/92", "02/28/92",
"02/01/92"))
class(dts)

hist(dts) # which yields:

# Error in axis(side, at, labels, tick, line, pos, outer, font, lty,
lwd,  : 
#        'label' is supplied and not 'at'
# In addition: Warning messages:
# 1: "histo" is not a graphical parameter in: plot.window(xlim, ylim,
log, asp, ...) 
# 2: "histo" is not a graphical parameter in: title(main, sub, xlab,
ylab, line, outer, ...)



The plot is produced, but there are no axes.

As far as it goes for the warnings I looked in the sources and I think
they are caused
by hist.times() in package 'chron' which is calling the barplot() with
the histo=TRUE,
whereas neither barplot(), plot.window(), title() nor axis() seem to
accept this argument.

Am I doing something wrong or there is something not right with the
hist.times method?


Kind regards,

Michal





PS: I'm using R 2.3.1 on Windows XP and:



> sessionInfo()
Version 2.3.1 (2006-06-01) 
i386-pc-mingw32 

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"
"datasets" 
[7] "base"     

other attached packages:
  chron 
"2.3-3" 
>


library(help="chron")

Package:       chron
Version:       2.3-3
Date:          2006-05-09
Author:        S original by David James <dj at research.bell-labs.com>, R
               port by Kurt Hornik <Kurt.Hornik at R-project.org>.
Maintainer:    Kurt Hornik <Kurt.Hornik at R-project.org>
Description:   Chronological objects which can handle dates and times
Title:         Chronological objects which can handle dates and times
Depends:       R (>= 1.6.0)
License:       GPL
Packaged:      Fri May 12 09:31:49 2006; hornik
Built:         R 2.3.0; i386-pc-mingw32; 2006-05-13 12:21:51; windows 






~,~`~,~`~,~`~,~`~,~`~,~`~,~`~,~`~,~
 
Michal Bojanowski
ICS / Utrecht University
Heidelberglaan 2; 3584 CS Utrecht
Room 1428
m.j.bojanowski at fss.uu.nl


From justin.rapp at gmail.com  Fri Jun 23 18:28:07 2006
From: justin.rapp at gmail.com (Justin Rapp)
Date: Fri, 23 Jun 2006 12:28:07 -0400
Subject: [R] Testing for Significance Between Logistic Regressions
Message-ID: <e11df81d0606230928t1dc272cbpac37cc6f304d1f8a@mail.gmail.com>

This is more of a statistics question with implementation implications.

I have used R to calculate logistic regressions for various
characterstics.  I would now like to verify that the difference
between a particular subset is significantly different from the
logistic regression of the entire set.

Example.
I have a logistic regression containing every running back drafted
between 1980-2000.  I have created an object, logistic.glm.  I also
have an object, sec.glm, that contains only players from the SEC.    I
am curious into determining whether or not the difference between the
two is sigificant.

Subquestion 1:  Can I do this for each of the coefficents, beta0 and
beta1, individually?  i.e there may be a statistically signicant
difference in the intercept but not in the rate of decay as a function
of my independent variable

Subquestion 2:  Can the same method of testing differences in
regressions be applied to linear regressions?

Thanks in advance for everyone's time and help.

jdr

-- 
Justin Rapp
409 S. 22nd St.
Apt. 1
Philadelphia, PA 19146
Cell:(267)252.0297


From mschwartz at mn.rr.com  Fri Jun 23 18:29:14 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 23 Jun 2006 11:29:14 -0500
Subject: [R] Producing png plot in batch mode
In-Reply-To: <10c01b446ea.vdemart1@tin.it>
References: <10c01b446ea.vdemart1@tin.it>
Message-ID: <1151080154.3828.45.camel@localhost.localdomain>

On Fri, 2006-06-23 at 17:23 +0100, Vittorio wrote:
> I have set up an R procedure that is launched every three hours by 
> crontab in a unix server. Crontab runs at regular intervals the 
> following line:
> R CMD BATH myprog.R
> 
> myprog.R (which by the way uses 
> R2HTML) should create an updated png graph  to be referred to and seen 
> in an intranet web-page index.html.
> 
> The problem is that both:
> 
> png
> (....) 
> plot(...) 
> dev.off() 
> 
> AND:
> 
> plot(...)
> HTMLplot(...)
> 
> fail when 
> launched in a batch manner compalining that they need an X11() instance 
> to be used (I understand that they work only in a graphic context and 
> intarictively).
> 
> How can I obtain that png file? 

See R FAQ 7.19 How do I produce PNG graphics in batch mode?

HTH,

Marc Schwartz


From ripley at stats.ox.ac.uk  Fri Jun 23 18:32:27 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Jun 2006 17:32:27 +0100 (BST)
Subject: [R] Producing png plot in batch mode
In-Reply-To: <10c01b446ea.vdemart1@tin.it>
References: <10c01b446ea.vdemart1@tin.it>
Message-ID: <Pine.LNX.4.64.0606231731290.17754@gannet.stats.ox.ac.uk>

?png explains this to you and gives alternatives.

On Fri, 23 Jun 2006, Vittorio wrote:

> I have set up an R procedure that is launched every three hours by
> crontab in a unix server. Crontab runs at regular intervals the
> following line:
> R CMD BATH myprog.R

BATCH?

> myprog.R (which by the way uses
> R2HTML) should create an updated png graph  to be referred to and seen
> in an intranet web-page index.html.
>
> The problem is that both:
>
> png
> (....)
> plot(...)
> dev.off()
>
> AND:
>
> plot(...)
> HTMLplot(...)
>
> fail when
> launched in a batch manner compalining that they need an X11() instance
> to be used (I understand that they work only in a graphic context and
> intarictively).
>
> How can I obtain that png file?
>
> Ciao
> Vittorio
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From M.J.Bojanowski at fss.uu.nl  Fri Jun 23 18:38:24 2006
From: M.J.Bojanowski at fss.uu.nl (Bojanowski, M.J.  (Michal))
Date: Fri, 23 Jun 2006 18:38:24 +0200
Subject: [R]  rearranging data frame rows
Message-ID: <94E133D09AA24D43BF6341B675C01A33113395@uu01msg-exb01.soliscom.uu.nl>

Hi Fede,

How about using merge()? For example:

n <- letters[1:10]
d1 <- data.frame( n=n, x1=rnorm(10) )
d2 <- data.frame( n=sample(n), x2=rnorm(10))
d1
d2
merge(d1,d2)


Is this what you had in mind?

HTH,

Michal




========================================================================
============
Hi All,

I have two data frames. The first contains data about a number of
individuals, 
coded in the first column with a name, in an order I find convenient.

The second contains different data about the same indivduals, in a
different 
order. Both data frame have the individual names in the first column.

I need to reorder the second data frame so the rows are rearranged in
the same 
manner as the fist. How?

I cannot turn the individual names in a numeric vairable with 
as.numeric(data1[,1]), because the two data frames are subset of
different data, 
so the the factor levels are way off between the two. I think I need to
actually 
use the names as a index.

Cheers,

Fede

~,~`~,~`~,~`~,~`~,~`~,~`~,~`~,~`~,~
 
Michal Bojanowski
ICS / Utrecht University
Heidelberglaan 2; 3584 CS Utrecht
Room 1428
m.j.bojanowski at fss.uu.nl


From datkins at fuller.edu  Fri Jun 23 18:41:11 2006
From: datkins at fuller.edu (Dave Atkins)
Date: Fri, 23 Jun 2006 09:41:11 -0700
Subject: [R] Effect size in mixed models
Message-ID: <449C19A7.6050401@fuller.edu>


Spencer & Bruno--

You might to take a look at a paper by Kyle Roberts at:

http://www.hlm-online.com/papers/

Kyle has been working on the issue of effect-sizes in mixed-effects (aka 
multilevel aka HLM) for a couple years.  I haven't had a chance to compare what 
Spencer has suggested with Kyle's approach.  [Though, it would be *lovely* if 
there were an agreed upon method for estimating effect-sizes in mixed-effects 
models; in psychology, reviewers will bludgeon you for an effect-size...]

cheers, Dave
-- 
Dave Atkins, PhD
Assistant Professor in Clinical Psychology
Fuller Graduate School of Psychology
Email: datkins at fuller.edu



Spencer wrote:

   I just learned that my earlier suggestion was wrong.  It's better to
compute the variance of the predicted or fitted values and compare those
with the estimated variance components.

	  To see how to do this, consider the following minor modification of
an example in the "lme" documentation:

fm1. <- lme(distance ~ age, data = Orthodont, random=~1)
fm2. <- lme(distance ~ age + Sex, data = Orthodont, random = ~ 1)

# str(fm1.) suggested the following:
  > var(fm2.$fitted[, "fixed"]-fm1.$fitted[, "fixed"])
[1] 1.312756
  > VarCorr(fm1.)[, 1]
(Intercept)    Residual
   "4.472056"  "2.049456"
  > VarCorr(fm2.)[, 1]
(Intercept)    Residual
   "3.266784"  "2.049456"

	  In this example, the subject variance without considering "Sex" was
4.47 but with "Sex" in the model, it dropped to 3.27, while the Residual
variance remained unchanged at 2.05.  The difference between
fm2.$fitted[, "fixed"] and fm1.$fitted[, "fixed"] is the change in the
predictions generated by the addition of "Sex" to the model.  The
variance of that difference was 1.31.  Note that 3.27 + 1.31 = 4.58,
which is moderately close to 4.47.

	  In sum, I think we can get a reasonable estimate of the size of an
effect from the variance of the differences in the "fixed" portion of
the fitted model.

	  Comments?
	  Hope this helps.
	  Spencer Graves

Spencer Graves wrote:
 >       You have asked a great question:  It would indeed be useful to
 > compare the relative magnitude of fixed and random effects, e.g. to
 > prioritize efforts to better understand and possibly manage processes
 > being studied.  I will offer some thoughts on this, and I hope if there
 > are errors in my logic or if someone else has a better idea, we will
 > both benefit from their comments.
 >
 >       The ideal might be an estimate of something like a mean square for
 > a particular effect to compare with an estimated variance component.
 > Such mean squares were a mandatory component of any analysis of variance
 > table prior to the (a) popularization of generalized linear models and
 > (b) availability of software that made it feasible to compute maximum
 > likelihood estimates routinely for unbalanced, mixed-effects models.
 > However, anova(lme(...)) such mean squares are for most purposes
 > unnecessary cluster in a modern anova table.
 >
 >       To estimate a mean square for a fixed effect, consider the
 > following log(likelihood) for a mixed-effects model:
 >
 >       lglk = (-0.5)*(n*log(2*pi*var.e)-log(det(W)) +
 > t(y-X%*%b)%*%W%*%(y-X%*%b)/var.e),
 >
 > where n = the number of observations,
 >
 >       b = the fixed-effect parameter variance,
 >
 > and the covariance matrix of the residuals, after integrating out the
 > random effects is var.e*solve(W).  In this formulation, the matrix "W"
 > is a function of the variance components.  Since they are not needed to
 > compute the desired mean squares, they are suppressed in the notation here.
 >
 >       Then, the maximum likelihood estimate of
 >
 >       var.e = SSR/n,
 >
 > where SSR = t(y-X%*%b)%*%W%*%(y-X%*%b).
 >
 >       Then
 >
 >       mle.lglk = (-0.5)*(n*(log(2*pi*SSR/n)-1)-log(det(W))).
 >
 >       Now let
 >
 >       SSR0 = this generalized sum of squares of residuals (SSR) without
 > effect "1",
 >
 > and
 >
 >       SSR1 = this generalized SSR with this effect "1".
 >
 >       If I've done my math correctly, then
 >
 >       D = deviance = 2*log(likelihood ratio)
 >         = (n*log(SSR0/SSR1)+log(det(W1)/det(W0)))
 >
 >       For roughly half a century, a major part of "the analysis of
 > variance" was the Pythagorean idea that the sum of squares under H0 was
 > the sum of squares under H1 plus the sum of squares for effect "1":
 >
 >       SSR0 = SS1 + SSR1.
 >
 >       Whence,
 >
 >       exp((D/n)-log(det(W1)/det(W0))) = 1+SS1/SSR1.
 >
 > Thus,
 >
 >       SS1 = SSR1*(exp((D/n)-log(det(W1)/det(W0)))-1).
 >
 >       If the difference between deg(W1) and det(W0) can be ignored, we get:
 >
 >       SS1 = SSR1*(exp((D/n)-1).
 >
 >       Now compute MS1 = SS1/df1, and compare with the variance components.
 >
 >       If there is a flaw in this logic, I hope someone will disabuse me
 > of it.
 >
 >       If this seems too terse or convoluted to follow, please provide a
 > simple, self-contained example, as suggested in the posting guide!
 > "www.R-project.org/posting-guide.html".  You asked a theoretical
 > question, you got a theoretical answer.  If you want a concrete answer,
 > it might help to pose a more concrete question.
 >
 >       Hope this helps.
 >       Spencer Graves
 >
 > Bruno L. Giordano wrote:
 >> Hello,
 >> Is there a way to compare the relative relevance of fixed and random
 >> effects in mixed models? I have in mind measures of effect size in
 >> ANOVAs, and would like to obtain similar information with mixed models.
 >>
 >> Are there information criteria that allow to compare the relevance of
 >> each of the effects in a mixed model to the overall fit?
 >>
 >> Thank you,
 >>     Bruno
 >>
 >> ______________________________________________
 >> R-help at stat.math.ethz.ch mailing list
 >> https://stat.ethz.ch/mailman/listinfo/r-help
 >> PLEASE do read the posting guide!
 >> http://www.R-project.org/posting-guide.html
 >


From ripley at stats.ox.ac.uk  Fri Jun 23 18:46:46 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Jun 2006 17:46:46 +0100 (BST)
Subject: [R] problem with hist() for 'times' objects from 'chron' package
In-Reply-To: <94E133D09AA24D43BF6341B675C01A33113394@uu01msg-exb01.soliscom.uu.nl>
References: <94E133D09AA24D43BF6341B675C01A33113394@uu01msg-exb01.soliscom.uu.nl>
Message-ID: <Pine.LNX.4.64.0606231742530.17754@gannet.stats.ox.ac.uk>

As `Writing R Extensions' points out, using traceback() is often very 
informative.  Here it gives

> traceback()
3: axis(2, adj = adj, cex = cex, font = font, las = las, lab = lab,
        mgp = mgp, tcl = tcl)
2: hist.times(dts)
1: hist(dts)

and the problem is partial matching of 'lab' to 'labels'.

This is discussed on the help page for axis, and in fact passing 'lab' to 
axis has never worked.  Please report this to the package maintainer (as 
advised in the posting guide), and as a fix edit hist.times to remove 
', lab=lab'.

On Fri, 23 Jun 2006, Bojanowski, M.J.  (Michal) wrote:

> Hello dear useRs and wizaRds,
>
> I encountered the following problem using the hist() method for the
> 'times' classes
> from package 'chron'. You should be able to recreate it using the code:
>
>
>
> library(chron)
>
> # pasted from chron help file (?chron)
> dts <- dates(c("02/27/92", "02/27/92", "01/14/92", "02/28/92",
> "02/01/92"))
> class(dts)
>
> hist(dts) # which yields:
>
> # Error in axis(side, at, labels, tick, line, pos, outer, font, lty,
> lwd,  :
> #        'label' is supplied and not 'at'
> # In addition: Warning messages:
> # 1: "histo" is not a graphical parameter in: plot.window(xlim, ylim,
> log, asp, ...)
> # 2: "histo" is not a graphical parameter in: title(main, sub, xlab,
> ylab, line, outer, ...)
>
>
>
> The plot is produced, but there are no axes.
>
> As far as it goes for the warnings I looked in the sources and I think
> they are caused
> by hist.times() in package 'chron' which is calling the barplot() with
> the histo=TRUE,
> whereas neither barplot(), plot.window(), title() nor axis() seem to
> accept this argument.
>
> Am I doing something wrong or there is something not right with the
> hist.times method?
>
>
> Kind regards,
>
> Michal
>
>
>
>
>
> PS: I'm using R 2.3.1 on Windows XP and:
>
>
>
>> sessionInfo()
> Version 2.3.1 (2006-06-01)
> i386-pc-mingw32
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"
> "datasets"
> [7] "base"
>
> other attached packages:
>  chron
> "2.3-3"
>>
>
>
> library(help="chron")
>
> Package:       chron
> Version:       2.3-3
> Date:          2006-05-09
> Author:        S original by David James <dj at research.bell-labs.com>, R
>               port by Kurt Hornik <Kurt.Hornik at R-project.org>.
> Maintainer:    Kurt Hornik <Kurt.Hornik at R-project.org>
> Description:   Chronological objects which can handle dates and times
> Title:         Chronological objects which can handle dates and times
> Depends:       R (>= 1.6.0)
> License:       GPL
> Packaged:      Fri May 12 09:31:49 2006; hornik
> Built:         R 2.3.0; i386-pc-mingw32; 2006-05-13 12:21:51; windows
>
>
>
>
>
>
> ~,~`~,~`~,~`~,~`~,~`~,~`~,~`~,~`~,~
>
> Michal Bojanowski
> ICS / Utrecht University
> Heidelberglaan 2; 3584 CS Utrecht
> Room 1428
> m.j.bojanowski at fss.uu.nl
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From markleeds at verizon.net  Fri Jun 23 18:46:56 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Fri, 23 Jun 2006 11:46:56 -0500 (CDT)
Subject: [R] stat question but maybe related to R
Message-ID: <6638466.1335181151081217131.JavaMail.root@vms073.mailsrvcs.net>

i don't have any stats books with me because i am not at home so
i was hoping someone could direct me to a book or
maybe there is a way to do it in R ?

i do an lm with no intercept and there are 6 coeeficients on
the right hand side and i get back estimates of model which is fine
in the sense tha i check the t-stats and do normality, indepenendence
of residual checks etc.

then, some more data points come in ( say 20 ) and i use the 
coeeficients that i estimated previously to calculate the 
residuals (  the new residuals and the previous residuals ).

what i want are the t-stats on the old regression coefficients
( really just the first one actually )to see if they are still significant, given the new data, but i'm not reestimating. i still want to use the previous coeeficients.

is there a formula for this or a way to do this in R ?

                                       thanks.


From ggrothendieck at gmail.com  Fri Jun 23 18:48:52 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 23 Jun 2006 12:48:52 -0400
Subject: [R] problem with hist() for 'times' objects from 'chron' package
In-Reply-To: <94E133D09AA24D43BF6341B675C01A33113394@uu01msg-exb01.soliscom.uu.nl>
References: <94E133D09AA24D43BF6341B675C01A33113394@uu01msg-exb01.soliscom.uu.nl>
Message-ID: <971536df0606230948h5399f804sec4792e5f0b16079@mail.gmail.com>

Try this:

hist(as.Date(dts), "days") # or "weeks" or "months"

On 6/23/06, Bojanowski, M.J.  (Michal) <M.J.Bojanowski at fss.uu.nl> wrote:
> Hello dear useRs and wizaRds,
>
> I encountered the following problem using the hist() method for the
> 'times' classes
> from package 'chron'. You should be able to recreate it using the code:
>
>
>
> library(chron)
>
> # pasted from chron help file (?chron)
> dts <- dates(c("02/27/92", "02/27/92", "01/14/92", "02/28/92",
> "02/01/92"))
> class(dts)
>
> hist(dts) # which yields:
>
> # Error in axis(side, at, labels, tick, line, pos, outer, font, lty,
> lwd,  :
> #        'label' is supplied and not 'at'
> # In addition: Warning messages:
> # 1: "histo" is not a graphical parameter in: plot.window(xlim, ylim,
> log, asp, ...)
> # 2: "histo" is not a graphical parameter in: title(main, sub, xlab,
> ylab, line, outer, ...)
>
>
>
> The plot is produced, but there are no axes.
>
> As far as it goes for the warnings I looked in the sources and I think
> they are caused
> by hist.times() in package 'chron' which is calling the barplot() with
> the histo=TRUE,
> whereas neither barplot(), plot.window(), title() nor axis() seem to
> accept this argument.
>
> Am I doing something wrong or there is something not right with the
> hist.times method?
>
>
> Kind regards,
>
> Michal
>
>
>
>
>
> PS: I'm using R 2.3.1 on Windows XP and:
>
>
>
> > sessionInfo()
> Version 2.3.1 (2006-06-01)
> i386-pc-mingw32
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"
> "datasets"
> [7] "base"
>
> other attached packages:
>  chron
> "2.3-3"
> >
>
>
> library(help="chron")
>
> Package:       chron
> Version:       2.3-3
> Date:          2006-05-09
> Author:        S original by David James <dj at research.bell-labs.com>, R
>               port by Kurt Hornik <Kurt.Hornik at R-project.org>.
> Maintainer:    Kurt Hornik <Kurt.Hornik at R-project.org>
> Description:   Chronological objects which can handle dates and times
> Title:         Chronological objects which can handle dates and times
> Depends:       R (>= 1.6.0)
> License:       GPL
> Packaged:      Fri May 12 09:31:49 2006; hornik
> Built:         R 2.3.0; i386-pc-mingw32; 2006-05-13 12:21:51; windows
>
>
>
>
>
>
> ~,~`~,~`~,~`~,~`~,~`~,~`~,~`~,~`~,~
>
> Michal Bojanowski
> ICS / Utrecht University
> Heidelberglaan 2; 3584 CS Utrecht
> Room 1428
> m.j.bojanowski at fss.uu.nl
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From paul_h_artes at yahoo.co.uk  Fri Jun 23 19:34:59 2006
From: paul_h_artes at yahoo.co.uk (Paul Artes)
Date: Fri, 23 Jun 2006 10:34:59 -0700 (PDT)
Subject: [R] PowerPoint
In-Reply-To: <20060623112807.74518.qmail@web25806.mail.ukl.yahoo.com>
References: <20060623112807.74518.qmail@web25806.mail.ukl.yahoo.com>
Message-ID: <5016074.post@talk.nabble.com>


Most useful: the Ungroup command in ppt lets you take apart the graph when
you insert it as wmf. 
I often resort to that when I want to change labels / fonts / colours etc.
Very flexible.
--
View this message in context: http://www.nabble.com/PowerPoint-t1835745.html#a5016074
Sent from the R help forum at Nabble.com.


From philipp.pagel.lists at t-online.de  Fri Jun 23 19:39:58 2006
From: philipp.pagel.lists at t-online.de (Philipp Pagel)
Date: Fri, 23 Jun 2006 19:39:58 +0200
Subject: [R] PowerPoint
In-Reply-To: <1151080021.3828.42.camel@localhost.localdomain>
References: <2323A6D37908A847A7C32F1E3662C80E132136@dc1ex01.air.org>
	<971536df0606230621t1584d4aer7cd4778978600fd0@mail.gmail.com>
	<20060623161649.GA5532@gsf.de>
	<1151080021.3828.42.camel@localhost.localdomain>
Message-ID: <20060623173958.GA3518@gsf.de>

On Fri, Jun 23, 2006 at 11:27:00AM -0500, Marc Schwartz (via MN) wrote:
> On Fri, 2006-06-23 at 18:16 +0200, Philipp Pagel wrote:
> > On Fri, Jun 23, 2006 at 09:21:37AM -0400, Gabor Grothendieck wrote:
> > > Note that jpg, bmp and png are in less desirable bit mapped formats whereas
> > > eps is in a more desirable vector format (magnification and shrinking does
> > > not involve loss of info) and so would be preferable from a quality
> > > viewpoint.
> > 
> > In addition to seconding the above statement I'd like to add that in
> > cases where you are forced to use a bitmap format png tends to produce
> > much better results than jpg where line drawings (e.g. most plots) are
> > concerned. JPG format on the other hand is great for anyting which can be
> > discirbed as photography-like. jpg images of plots tend to suffer from
> > bad artifacts...

> That is generally because png files are not compressed, whereas jpg
> files are. 

In fact, png uses a combination of pre-filtering and lossless compression 
(in contrast to lossy compression algortihms use in jpg). 

Of course, lossy compression can achieve much smaller file sizes for
most images. While even a substantial loss of information can go
undetected by the observer in the case of photographic images with no
sharp edges, line drawings suffer badly. 

Line drawings usually contain vast percentages of empty space (i.e.
white) and thus can be compressed quite effectively by the
pre-filteing/lossless compression used by png.

Anyway - I totally agree that eps rules and pixel formats should be
avoided at all cost for illustrations, plots, etc. ...

cu
	Philipp

-- 
Dr. Philipp Pagel                            Tel.  +49-8161-71 2131
Dept. of Genome Oriented Bioinformatics      Fax.  +49-8161-71 2186
Technical University of Munich
Science Center Weihenstephan
85350 Freising, Germany

 and

Institute for Bioinformatics / MIPS          Tel.  +49-89-3187 3675
GSF - National Research Center               Fax.  +49-89-3187 3585
      for Environment and Health
Ingolst?dter Landstrasse 1
85764 Neuherberg, Germany
http://mips.gsf.de/staff/pagel


From jrosenba at rand.org  Fri Jun 23 19:42:12 2006
From: jrosenba at rand.org (Janet Rosenbaum)
Date: Fri, 23 Jun 2006 10:42:12 -0700
Subject: [R] Tetrachoric correlation in R vs. stata
Message-ID: <449C27F4.2010400@rand.org>


I hope someone here knows the answer to this since it will save me from 
delving deep into documentation.

Based on 22 pairs of vectors, I have noticed that tetrachoric 
correlation coefficients in stata are almost uniformly higher than those 
in R, sometimes dramatically so (TCC=.61 in stata, .51 in R;  .51 in 
stata, .39 in R).  Stata's estimate is higher than R's in 20 out of 22 
computations, although the estimates always fall within the 95% CI for 
the TCC calculated by R.

Do stata and R calculate TCC in dramatically different ways?  Is the 
handling of missing data perhaps different?  Any thoughts?

Btw, I am sending this question only to the R-help list.

Thanks,

Janet


--------------------

This email message is for the sole use of the intended recip...{{dropped}}


From spencer.graves at pdf.com  Fri Jun 23 19:47:01 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 23 Jun 2006 10:47:01 -0700
Subject: [R] integrate
In-Reply-To: <Pine.LNX.4.64.0606230805380.23108@homer24.u.washington.edu>
References: <449BDAB9.9080201@gmail.com>
	<Pine.LNX.4.64.0606230805380.23108@homer24.u.washington.edu>
Message-ID: <449C2915.70209@pdf.com>

<comment inline>

Thomas Lumley wrote:
> On Fri, 23 Jun 2006, Rog?rio Rosa da Silva wrote:
> 
>> Dear All,
>>
>> My doubt about how to integrate a simple kernel density estimation 
>> goes on.
>>
>> I have seen the recent posts on integrate density estimation, which seem
>> similar to my question. However, I haven't found a solution.
>>
>> I have made two simple kernel density estimation by:
>>
>>    kde.1 <-density(x, bw=sd(x), kernel="gaussian")$y     # x<- 
>> c(2,3,5,12)
>>    kde.2 <-density(y, bw=sd(y), kernel="gaussian")$y     # y<- 
>> c(4,2,4,11)
>>
>> Now I would like to integrate the difference in the estimated density
>> values, i.e.:
>>
>>    diff.kde <- abs (kde.1- kde.2)
>>
>> How can I integrate diff.kde over -Inf to Inf ?
> 
> Well, the answer is zero.
> 
> Computationally this is a bit tricky.  You can turn the density 
> estimates into functions with approxfun()
> x<-rexp(100)
> kde<-density(x)
>  f<-approxfun(kde$x,kde$y,rule=2)
> integrate(f,-1,10)
> 1.000936 with absolute error < 3.3e-05
> 
> But if you want to integrate over -Inf to Inf you need the function to 
> specify the values outside the range of the data.  The only value that 
> will work over the range -Inf to Inf is zero

	  This may be true for standard splines but not if the segment at the 
end is log-linear.  For example, consider data with range(x) = c(x1, 
x.n) with (n-2) knots at x[i], i = 2:(n-1) with x[i]<x[i+1].  Consider a 
function f(x) modeled with exp(a1+b1*x) over [x[1], x[2]].  Then if b1>0,

	  integral{exp(a1+b1*x) from -Inf to x[2]}
	  = exp(a1+b1*x[2])/b1.

	  Do you know if this kind of thing has been studied?  I am NOT 
familiar with the literature on splines, but it would seem to me that 
this kind of thing could be quite valuable for building fast 
approximations to various mixture distributions that are otherwise quite 
difficult and expensive to compute.  It seems to me that this kind of 
thing could be used in multilevel modeling, with multidimensional 
smoothing splines fit to data obtained from potentially expensive 
evaluations of the unconditional likelihood, and the marginal likelihood 
could then be computed and optimized from the spline.  Then an estimate 
of the uncertainty could then be converted into another set of points at 
which to evaluate the likelihood and the process repeated until 
convergence.  I have problems like this that I can't solve right now, 
and it seems to me that this might lead to fast, accurate solutions.

	  Comments?
	  Spencer Graves

>> f<-approxfun(kde$x,kde$y,yleft=0,yright=0)
>> integrate(f,-1,10)
> 1.00072 with absolute error < 1.5e-05
>> integrate(f,-Inf,Inf)
> 1.000811 with absolute error < 2.3e-05
> 
> 
>     -thomas
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From Mike.Prager at noaa.gov  Fri Jun 23 20:02:49 2006
From: Mike.Prager at noaa.gov (Michael H. Prager)
Date: Fri, 23 Jun 2006 14:02:49 -0400
Subject: [R] PowerPoint - eps not suitable
In-Reply-To: <20060623173958.GA3518@gsf.de>
References: <2323A6D37908A847A7C32F1E3662C80E132136@dc1ex01.air.org>
	<971536df0606230621t1584d4aer7cd4778978600fd0@mail.gmail.com>
	<20060623161649.GA5532@gsf.de>
	<1151080021.3828.42.camel@localhost.localdomain>
	<20060623173958.GA3518@gsf.de>
Message-ID: <449C2CC9.8080002@noaa.gov>

Previous posters have argued for EPS files as a desirable transfer 
format for quality reasons.  This is of course true when the output is 
through a Postscript device.

However, the original poster is making presentations with PowerPoint.  
Those essentially are projected from the screen -- and screens of 
Windows PCs are NOT Postscript devices.  The version of PowerPoint I 
have will display a bitmapped, low-resolution preview when EPS is 
imported, and that is what will be projected.  It is passable, but much 
better can be done!

In this application, I have had best results using cut and paste or the 
Windows metafile format, both of which (as others have said) give 
scalable vector graphics.  When quirks of Windows metafile arise (as 
they can do, especially when fonts differ between PCs), I have had good 
results with PNG for line art and JPG for other art.

Mike

-- 
Michael Prager, Ph.D.
Southeast Fisheries Science Center
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
** Opinions expressed are personal, not official.  No
** official endorsement of any product is made or implied.


From ggrothendieck at gmail.com  Fri Jun 23 20:15:18 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 23 Jun 2006 14:15:18 -0400
Subject: [R] PowerPoint - eps not suitable
In-Reply-To: <449C2CC9.8080002@noaa.gov>
References: <2323A6D37908A847A7C32F1E3662C80E132136@dc1ex01.air.org>
	<971536df0606230621t1584d4aer7cd4778978600fd0@mail.gmail.com>
	<20060623161649.GA5532@gsf.de>
	<1151080021.3828.42.camel@localhost.localdomain>
	<20060623173958.GA3518@gsf.de> <449C2CC9.8080002@noaa.gov>
Message-ID: <971536df0606231115q31d80bdeobecda044a66fe923@mail.gmail.com>

I think I was just comparing the ones that were discussed but
certainly the vector format used on Windows is normally emf or wmf
and that is what I would normally use too.

On 6/23/06, Michael H. Prager <Mike.Prager at noaa.gov> wrote:
> Previous posters have argued for EPS files as a desirable transfer
> format for quality reasons.  This is of course true when the output is
> through a Postscript device.
>
> However, the original poster is making presentations with PowerPoint.
> Those essentially are projected from the screen -- and screens of
> Windows PCs are NOT Postscript devices.  The version of PowerPoint I
> have will display a bitmapped, low-resolution preview when EPS is
> imported, and that is what will be projected.  It is passable, but much
> better can be done!
>
> In this application, I have had best results using cut and paste or the
> Windows metafile format, both of which (as others have said) give
> scalable vector graphics.  When quirks of Windows metafile arise (as
> they can do, especially when fonts differ between PCs), I have had good
> results with PNG for line art and JPG for other art.
>
> Mike
>
> --
> Michael Prager, Ph.D.
> Southeast Fisheries Science Center
> NOAA Center for Coastal Fisheries and Habitat Research
> Beaufort, North Carolina  28516
> ** Opinions expressed are personal, not official.  No
> ** official endorsement of any product is made or implied.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From roebuck at mdanderson.org  Fri Jun 23 20:26:06 2006
From: roebuck at mdanderson.org (Paul Roebuck)
Date: Fri, 23 Jun 2006 13:26:06 -0500 (CDT)
Subject: [R] default help system (was Re: Basic package structure question)
In-Reply-To: <449BCD6A.8020507@stats.uwo.ca>
References: <d4588dec0606220908w415cc012y8fa678148cef44e3@mail.gmail.com>
	<449AD3CB.4050202@stats.uwo.ca>
	<971536df0606221919u5de6a851u370fbd2846f92644@mail.gmail.com>
	<449BA780.2020206@fz-rossendorf.de>
	<Pine.LNX.4.64.0606230946340.13624@gannet.stats.ox.ac.uk>
	<449BB830.7030902@fz-rossendorf.de> <449BCD6A.8020507@stats.uwo.ca>
Message-ID: <Pine.OSF.4.58.0606231312220.120673@wotan.mdacc.tmc.edu>

<<<Many parts of original thread [SNIP]'d>>>

On Fri, 23 Jun 2006, Duncan Murdoch wrote:

> >> On Fri, 23 Jun 2006, Joerg van den Hoff wrote:
> >>
> >>> our Windows machines lack proper development environments (mainly
> >>> missing perl is the problem for pure R-code packages, I believe?) and we
> >>> bypass this (for pure R-code packages only, of course) by
> >>>
> >>> 1.) install the package on the unix machine into the desired R library
> >>> 2.) zip the _installed_ package (not the source tree!) found in the R
> >>>           library directory
> >>> 3.) transfer this archive to the Windows machine
> >>> 4.) unzip directly into the desired library destination
> >>>
> >>> this procedure up to now always worked including properly installed
> >>> manpages (text + html (and I hope this remains the case in the future...)
>
> One obvious limitation of this install method is that it won't produce
> native Windows help files (.chm).  Plans are to make CHM the default
> help system as of the 2.4.0 release, so your packages will not work
> properly unless you give special instructions on how to change the help
> system defaults.

Is the decision to change default help files to CHM set
in stone? What's the payback for this change?

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)


From ibaxter at purdue.edu  Fri Jun 23 20:38:08 2006
From: ibaxter at purdue.edu (Ivan Baxter)
Date: Fri, 23 Jun 2006 14:38:08 -0400
Subject: [R] looping through a data frame
Message-ID: <449C3510.4010906@purdue.edu>

Hi- I am having trouble with the syntax of  looping through  the rows 
and columns of a data frame.

I have a table with 17 observations for 84 lines at n=5-10 per line. So 
the table is ~700x17.

I want to pull out the median and stdev for each line and put it in a 
dataframe with rowname = linename.

So I have tried the following....
#read in the table
input.table <- read.table(file =  "First_run_all.txt", header = T)
#pull out the line names
line.run <- unique(input.table$Line)
#pull out the column names except for Line
el.names <- names(input.table[2:18])


#now I want to calculate the median for each line for each column. The 
code below would work for a matrix
calc.frame.med <- matrix(ncol = length(el.names), nrow = 
length(line.run), dimnames = list(line.run,el.names))
for(i in 1:length(el.names)){
    for(j in 1:length(line.run)){
       calc.frame.med[j,i] <- median(input.table[input.table$Line == 
line.run[j],el.names[i]])
    }
}


#however, it won't allow me to pull stuff out based on the row names 
will it?
batch1.med <- calc.frame.med[rownames(calc.frame.med) == batch1,]
#doesn't work.
#It seems like I want to create the data as a matrix and then be able to 
treat it like a data.frame.

can anyone set me straight on the right way to do this?

Thanks

Ivan



-- 
**************************************************************
Ivan Baxter
Research Scientist
Bindley Bioscience Center
Purdue University
765-543-7288
ibaxter at purdue.edu


From mschwartz at mn.rr.com  Fri Jun 23 20:43:54 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 23 Jun 2006 13:43:54 -0500
Subject: [R] PowerPoint - eps not suitable
In-Reply-To: <449C2CC9.8080002@noaa.gov>
References: <2323A6D37908A847A7C32F1E3662C80E132136@dc1ex01.air.org>
	<971536df0606230621t1584d4aer7cd4778978600fd0@mail.gmail.com>
	<20060623161649.GA5532@gsf.de>
	<1151080021.3828.42.camel@localhost.localdomain>
	<20060623173958.GA3518@gsf.de>  <449C2CC9.8080002@noaa.gov>
Message-ID: <1151088234.3828.97.camel@localhost.localdomain>

On Fri, 2006-06-23 at 14:02 -0400, Michael H. Prager wrote:
> Previous posters have argued for EPS files as a desirable transfer 
> format for quality reasons.  This is of course true when the output is 
> through a Postscript device.
> 
> However, the original poster is making presentations with PowerPoint.  
> Those essentially are projected from the screen -- and screens of 
> Windows PCs are NOT Postscript devices.  The version of PowerPoint I 
> have will display a bitmapped, low-resolution preview when EPS is 
> imported, and that is what will be projected.  It is passable, but much 
> better can be done!
> 
> In this application, I have had best results using cut and paste or the 
> Windows metafile format, both of which (as others have said) give 
> scalable vector graphics.  When quirks of Windows metafile arise (as 
> they can do, especially when fonts differ between PCs), I have had good 
> results with PNG for line art and JPG for other art.
> 
> Mike

Just so that it is covered (though this has been noted in other
threads), even in this situation, one can still use EPS files embedded
in PowerPoint (or Impress) presentations.

The scenario is to print out the PowerPoint presentation to a Postscript
file (using a PS printer driver). If you have Ghostscript installed, you
can then use ps2pdf to convert the PS file to a PDF file.

If you have OO.org, there is a Distiller type of printer driver called
PDF Converter (configured via the printer admin program) available,
which you can use to go directly to a PDF in a single step. This also
uses Ghostscript (-sDEVICE=pdfwrite) as an intermediary (though hidden
from the user) step.

The standard OO.org PDF export mechanism (using the toolbar icon) only
exports the bitmapped preview, not the native EPS image. This is what
you see as the preview image in these "Office" type of apps by default.

Most PDF file viewers (Acrobat, xpdf, Evince, etc.) have a full screen
mode, whereby you can the use the viewer to display the presentation in
a landscape orientation to an audience.

I have done this frequently (under Linux with OO.org) to facilitate
presentations, when for any number of reasons, using LaTeX (ie. Beamer)
was not practical.

Even when using Beamer, the net result is still the same: creating a PDF
file via pdflatex, which is then displayed landscape in a PDF rendering
application full screen. 

This was the typical mode of operation at last week's useR! meeting in
Vienna.

All that being said, the ultimate test is in the eye of the user. So
whatever gives you sufficient quality for your application with minimal
hassle is the way to go.

HTH,

Marc Schwartz


From sixpackadelic at yahoo.com  Fri Jun 23 20:57:21 2006
From: sixpackadelic at yahoo.com (Ray D.)
Date: Fri, 23 Jun 2006 11:57:21 -0700 (PDT)
Subject: [R] R connectivity to database
Message-ID: <20060623185721.61485.qmail@web55507.mail.re4.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060623/4eeee22b/attachment.pl 

From cborges at iqm.unicamp.br  Fri Jun 23 20:59:10 2006
From: cborges at iqm.unicamp.br (Cleber N. Borges)
Date: Fri, 23 Jun 2006 15:59:10 -0300
Subject: [R] PowerPoint
In-Reply-To: <20060623112807.74518.qmail@web25806.mail.ukl.yahoo.com>
References: <20060623112807.74518.qmail@web25806.mail.ukl.yahoo.com>
Message-ID: <449C39FE.5000906@iqm.unicamp.br>


Hello,
IMHO,

for the printer

1 - The best choice of  graphics format is postscript ( PS )  in
Microsoft ( M$ ), since you install the M$ Convert Pack [1]!
Make a preview in PS file with EMF format! use epstool for this

2 - Enhanced MetaFile ( EMF ) in M$ and OpenOffice ( OOo) is not the
same... This can be a problem! See pstoedit page [2]

3 - In OOo, I use the EPS file with the follow procedure:
      - save my graphic in PS
      - make to use de EPSTOOL for to produce EPS ( ps with tiff 
preview
) -> preview is a tiff graphic with low quality

*note:  emf can be also to included for preview


      - I need a PS-printer... in the case of NO-ps-printer, the 
tiff
(low quality) will be printed! { :-(  }, then, I make a PDF final 
report
with ExtendendPDF!

      - this procedure also work in the M$-Word


HTH,
Cleber N. Borges {klebyn}
---------------------------

[1] -
http://www.microsoft.com/downloads/details.aspx?FamilyID=cf196df0-70e5-4595-8a98-370278f40c57&DisplayLang=en

[2] - http://www.pstoedit.net/pstoedit

[3] - http://www.3bview.com/epdf-home.html







Marc Bernard wrote:

>Dear All,
>   
>  I am looking for the best way to use graphs from R (like xyplot, curve ...)   for a presentation with powerpoint. I used to save my plot as pdf and after to copy them as image in powerpoint but the quality is not optimal by so doing.
>   
>  Another completely independent question is the following: when I use "main"  in the  xyplot, the main title is very close to my plot, i.e. no ligne separate the main and the plot. I would like my title to be well distinguished from the plots.
>   
>  I would be grateful for any improvements...
>   
>  Many thanks,
>   
>  Bernard,
>   
>    
>
> 		
>---------------------------------
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help em stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>.
>
>  
>


From liuwensui at gmail.com  Fri Jun 23 21:02:13 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Fri, 23 Jun 2006 15:02:13 -0400
Subject: [R] R connectivity to database
In-Reply-To: <20060623185721.61485.qmail@web55507.mail.re4.yahoo.com>
References: <20060623185721.61485.qmail@web55507.mail.re4.yahoo.com>
Message-ID: <1115a2b00606231202v4aa21ef1h2308e85256583ff5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060623/1e61faaf/attachment.pl 

From jfox at mcmaster.ca  Fri Jun 23 21:17:00 2006
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 23 Jun 2006 15:17:00 -0400
Subject: [R] Tetrachoric correlation in R vs. stata
In-Reply-To: <449C27F4.2010400@rand.org>
Message-ID: <web-130200542@cgpsrv2.cis.mcmaster.ca>

Dear Janet,

Are you using the polychor() function in the polycor package to compute
tetrachoric correlations? If so, two methods are provided: A relatively
quick method (the default) and ML. The methods implemented are
described in the references given in ?polycor.

Missing data simply are eliminated from the contingency table from
which a tetrachoric correlation is computed. If, however, you're using
hetcor() to compute a matrix of tetrachoric correlations, then missing
data are handled according to the use argument, which defaults to
"complete.obs" and is described in ?hetcor.

If you want to know whether polychor() or Stata is right, then one
thing that you might do is try them on data for which you know the
answer. If you do this, you should of course make sure that both are
trying to compute the same thing (e.g., the ML estimate).

I hope this helps,
 John

On Fri, 23 Jun 2006 10:42:12 -0700
 Janet Rosenbaum <jrosenba at rand.org> wrote:
> 
> I hope someone here knows the answer to this since it will save me
> from 
> delving deep into documentation.
> 
> Based on 22 pairs of vectors, I have noticed that tetrachoric 
> correlation coefficients in stata are almost uniformly higher than
> those 
> in R, sometimes dramatically so (TCC=.61 in stata, .51 in R;  .51 in 
> stata, .39 in R).  Stata's estimate is higher than R's in 20 out of
> 22 
> computations, although the estimates always fall within the 95% CI
> for 
> the TCC calculated by R.
> 
> Do stata and R calculate TCC in dramatically different ways?  Is the 
> handling of missing data perhaps different?  Any thoughts?
> 
> Btw, I am sending this question only to the R-help list.
> 
> Thanks,
> 
> Janet
> 
> 
> --------------------
> 
> This email message is for the sole use of the intended\ > ...{{dropped}}


From p.dalgaard at biostat.ku.dk  Fri Jun 23 21:22:32 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Jun 2006 21:22:32 +0200
Subject: [R] Tetrachoric correlation in R vs. stata
In-Reply-To: <449C27F4.2010400@rand.org>
References: <449C27F4.2010400@rand.org>
Message-ID: <x2zmg37k3r.fsf@turmalin.kubism.ku.dk>

Janet Rosenbaum <jrosenba at rand.org> writes:

> I hope someone here knows the answer to this since it will save me from 
> delving deep into documentation.
> 
> Based on 22 pairs of vectors, I have noticed that tetrachoric 
> correlation coefficients in stata are almost uniformly higher than those 
> in R, sometimes dramatically so (TCC=.61 in stata, .51 in R;  .51 in 
> stata, .39 in R).  Stata's estimate is higher than R's in 20 out of 22 
> computations, although the estimates always fall within the 95% CI for 
> the TCC calculated by R.
> 
> Do stata and R calculate TCC in dramatically different ways?  Is the 
> handling of missing data perhaps different?  Any thoughts?
> 
> Btw, I am sending this question only to the R-help list.


A bit more information seems necessary:

- tetrachoric correlations depend on 4 numbers, so you should be able
  to give a direct example

- you're not telling us how you calculate the TCC in R. This is not
  obvious (package polycor?).

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From John.Kerpel at infores.com  Fri Jun 23 21:52:07 2006
From: John.Kerpel at infores.com (Kerpel, John)
Date: Fri, 23 Jun 2006 14:52:07 -0500
Subject: [R] Problems with weekday extraction from zoo objects
Message-ID: <44A8B25381923D4F93B74B2676A50F6D02AF3709@MAIL1.infores.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060623/f7233f9d/attachment.pl 

From justin.rapp at gmail.com  Fri Jun 23 22:07:38 2006
From: justin.rapp at gmail.com (Justin Rapp)
Date: Fri, 23 Jun 2006 16:07:38 -0400
Subject: [R] Interpreting as.factor
Message-ID: <e11df81d0606231307h55427119ieaeb2e311fdca498@mail.gmail.com>

When I run a linear regression and include a variable in the
regression with as.factor  i.e.

lm(y ~x +as.factor(x1)

and i read the output as
as.factor(x1)1....
as.factor(x1)2...
etc.

how do i interpret the estimate for each level?  Is this simply to be
regarded as a shift in the equation predicted by the intercept and
independent variable x?

jdr


-- 
Justin Rapp
409 S. 22nd St.
Apt. 1
Philadelphia, PA 19146
Cell:(267)252.0297


From tehflash at gmail.com  Fri Jun 23 22:20:57 2006
From: tehflash at gmail.com (Robert Robinson)
Date: Fri, 23 Jun 2006 16:20:57 -0400
Subject: [R] Problems creating packages.
Message-ID: <a9f0a0ea0606231320q210b6a5ex84279c2a14220691@mail.gmail.com>

I'm creating my own package for personal and I'm having trouble
getting it to a point where R (v 2.3.1) will recognise it.  I've
followed two different tutorials for how to create the package
structure and the DESCRIPTION file (
http://web.maths.unsw.edu.au/~wand/webcpdg/rpack.html ,
http://www.maths.bris.ac.uk/~maman/computerstuff/Rhelp/Rpackages.html#Lin-Lin
).  I'm still getting errors where when I try to load the library in R
by using library(samp) I get an error:

Error in library(samp) : 'samp' is not a valid package -- installed < 2.0.0?

And when I use the library() call I get this:

samp                    ** No title available (pre-2.0.0 install?) **

I'm not really sure where to go, I've looked throgh the huge document
on how to create an R package on the site but that didn't help.  Just
for reference I'm going to add a sample of everything becasue at this
point I really don't know what it is.  Basic procedure is what they
layout in the tutorials I linked above.  First I made the files,
followed by calling R CMD build samp on the samp directory that holds
all my files, followed by calling sudo R CMD INSTALL samp_0.1-1.tar.gz
 I don't get errors on either of those calls.  When I call R CMD check
samp I get this output:

* checking for working latex ...sh: latex: command not found
 NO
* using log directory '/home/rrobinson/myrpac/samp.Rcheck'
* using Version 2.3.0 (2006-04-24)
* checking for file 'samp/DESCRIPTION' ... OK
* this is package 'samp' version '0.1-1'
* checking package dependencies ... OK
* checking if this is a source package ... ERROR
Only *source* packages can be checked.

Here is what my file structure looks like:

.:
DESCRIPTION  man  R

./man:
bootstrap.rd  hessian.rd         loglikelihood.rd  stdeviation6.rd
freq.rd       ks.rd              score6.rd         stdeviation.rd
hessian6.rd   loglikelihood6.rd  score.rd

./R:
bootsample.r  hessian6.r  loglikelihood6.r  score6.r        stdeviation.r
firstlib.r    hessian.r   loglikelihood.r   score.r
freq.r        ks.r        Prob.r            stdeviation6.r

here is what my DESCRIPTION file looks like:

Package: samp
Title: R custom simulation package
Version: 0.1-1
Date: 2006-06-23
Author: Cheng Peng <*********************>
Maintainer: ************** <tehflash at gmail.com>
Description: Functions for custom simulation, includes a score and
        loglikelihood function.
License: GPL (version 2 or later)
Built: R 2.3.0; i686-pc-linux-gnu; 2006-06-16 11:28:36; unix
Packaged: Fri Jun 16 11:36:02 2006; root

Here is a sample .r file format:

###########################################################################
#
#        Freq function returns the frequencies of numerical vectors
#
###########################################################################

  freq = function(x1,x2,x3){
     nn1=rep(0, length(x))
     nn2=rep(0, length(x))
     nn3=rep(0, length(x))
     for ( i in 1:length(x)){
         nn1[i]=sum(x[i]==x1)
         nn2[i]=sum(x[i]==x2)
         nn3[i]=sum(x[i]==x3)
     }
   mm = as.matrix(rbind(nn1,nn2,nn3))
   mm
  }

Here is a sample man page format:

\name{freq}
\alias{freq}
\title{Frequencies of numerical vectors}
\description{
   Freq function returns the frequencies of numerical vectors
}
\usage{
freq(x1,x2,x3)
}
\arguments{
   \item{x1}{numerical vector}
   \item{x2}{numerical vector}
   \item{x3}{numerical vector}
}
\keyword{custom}


Thanks to anyone who can help me solve this.


From justin_bem at yahoo.fr  Fri Jun 23 22:21:07 2006
From: justin_bem at yahoo.fr (justin bem)
Date: Fri, 23 Jun 2006 20:21:07 +0000 (GMT)
Subject: [R] Re :  Interpreting as.factor
In-Reply-To: <e11df81d0606231307h55427119ieaeb2e311fdca498@mail.gmail.com>
Message-ID: <20060623202107.74533.qmail@web25707.mail.ukl.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060623/dea4a922/attachment.pl 

From ggrothendieck at gmail.com  Fri Jun 23 22:24:18 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 23 Jun 2006 16:24:18 -0400
Subject: [R] Problems with weekday extraction from zoo objects
In-Reply-To: <44A8B25381923D4F93B74B2676A50F6D02AF3709@MAIL1.infores.com>
References: <44A8B25381923D4F93B74B2676A50F6D02AF3709@MAIL1.infores.com>
Message-ID: <971536df0606231324s51d5eb07sc1e49a609fdacdfa@mail.gmail.com>

Please provide a reproducible example (and read the posting
guide at the bottom of each email).

On 6/23/06, Kerpel, John <John.Kerpel at infores.com> wrote:
> Hi Folks!
>
>
>
> I'm struggling with dates - but enough about my personal life.....
>
>
>
> I have two daily time series files.  In one (x) the date format is Y/m/d
> and the other (y) is d/m/y.  I used read.zoo on both and they read into
> R with no problem.
>
>
>
> Then I use: weekdays(as.Date(x$DATE)) and get what I expect - all the
> days of the week in my data set.
>
>
>
> When I use: weekdays(as.Date(y$Date)) I get:
>
>
>
> Error in fromchar(x) : character string is not in a standard unambiguous
> format
>
>
>
> I've tried to set the format= in read.zoo to format="%d/%m/%Y" but this
> doesn't seem to solve the problem.
>
>
>
> What's going on here?  (I'm new to these dates functions, so please be
> patient - I'll get the hang out of soon!)
>
>
>
> Best,
>
>
>
> John
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From Dimitris.Rizopoulos at med.kuleuven.be  Fri Jun 23 22:24:44 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Fri, 23 Jun 2006 22:24:44 +0200
Subject: [R] Interpreting as.factor
In-Reply-To: <e11df81d0606231307h55427119ieaeb2e311fdca498@mail.gmail.com>
References: <e11df81d0606231307h55427119ieaeb2e311fdca498@mail.gmail.com>
Message-ID: <1151094284.449c4e0cc0471@webmail1.kuleuven.be>

probably you want to look at the help page of ?contr.treatment

I hope it helps.

Best,
Dimitris

---- 
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting Justin Rapp <justin.rapp at gmail.com>:

> When I run a linear regression and include a variable in the
> regression with as.factor  i.e.
> 
> lm(y ~x +as.factor(x1)
> 
> and i read the output as
> as.factor(x1)1....
> as.factor(x1)2...
> etc.
> 
> how do i interpret the estimate for each level?  Is this simply to
> be
> regarded as a shift in the equation predicted by the intercept and
> independent variable x?
> 
> jdr
> 
> 
> -- 
> Justin Rapp
> 409 S. 22nd St.
> Apt. 1
> Philadelphia, PA 19146
> Cell:(267)252.0297
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ggrothendieck at gmail.com  Fri Jun 23 22:27:25 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 23 Jun 2006 16:27:25 -0400
Subject: [R] Interpreting as.factor
In-Reply-To: <e11df81d0606231307h55427119ieaeb2e311fdca498@mail.gmail.com>
References: <e11df81d0606231307h55427119ieaeb2e311fdca498@mail.gmail.com>
Message-ID: <971536df0606231327w3c1345dcre00a6c80ac60456e@mail.gmail.com>

Try this:

model.matrix(y ~ x + as.factor(x1))

to see the model matrix that is being used.  Matrix multiplication of
that matrix by the coefficients is the prediction equation.

On 6/23/06, Justin Rapp <justin.rapp at gmail.com> wrote:
> When I run a linear regression and include a variable in the
> regression with as.factor  i.e.
>
> lm(y ~x +as.factor(x1)
>
> and i read the output as
> as.factor(x1)1....
> as.factor(x1)2...
> etc.
>
> how do i interpret the estimate for each level?  Is this simply to be
> regarded as a shift in the equation predicted by the intercept and
> independent variable x?
>
> jdr
>
>
> --
> Justin Rapp
> 409 S. 22nd St.
> Apt. 1
> Philadelphia, PA 19146
> Cell:(267)252.0297
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From jholtman at gmail.com  Fri Jun 23 22:46:24 2006
From: jholtman at gmail.com (jim holtman)
Date: Fri, 23 Jun 2006 16:46:24 -0400
Subject: [R] looping through a data frame
In-Reply-To: <449C3510.4010906@purdue.edu>
References: <449C3510.4010906@purdue.edu>
Message-ID: <644e1f320606231346y561b1505h6b1d9aa0e9b6f206@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060623/03dd8e01/attachment.pl 

From ripley at stats.ox.ac.uk  Fri Jun 23 23:01:12 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Jun 2006 22:01:12 +0100 (BST)
Subject: [R] R connectivity to database
In-Reply-To: <20060623185721.61485.qmail@web55507.mail.re4.yahoo.com>
References: <20060623185721.61485.qmail@web55507.mail.re4.yahoo.com>
Message-ID: <Pine.LNX.4.64.0606232142230.27320@gannet.stats.ox.ac.uk>

You have not told us your platform nor version of OO (nor what
'it' is which 'says that R can connect to MS Access').  R can certainly 
connect to MS Access on Windows via RODBC, which does come with Access 
examples.

I think though that it is unrealistic to claim that Base is 'OOo's version 
of MS Access': it seems to be a lot less than that even in version 2.0.
Base 2.0 comes with an HSQLDB DBMS, but Base is not itself a relational 
DBMS as Access is.  Both R and Base can act as ODBC clients, which may 
help if you use Base to store data in a fully-fledged RDBMS such as MySQL 
or SQLite.


On Fri, 23 Jun 2006, Ray D. wrote:

> Hello, does anyone know how I would go about getting R to connect to 
> OpenOffice's Base program (OOo's version of MS Access) such that I can 
> retrieve data from the database and perform calculations and data 
> analysis?  I'm totally new to R and Base and I've looked at some 
> documentation, but found only examples for R connecting to PostgreSQL 
> and MySQL, but nothing for OOo's Base (there wasn't any examples for MS 
> Access either even though it says that R can connect to MS Access).  Is 
> R even capable of this or am I just out of luck to use R and OOo's Base 
> together?  Thanks in advance.
>
>  -ray
>
>
> ---------------------------------
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From John.Kerpel at infores.com  Fri Jun 23 23:12:27 2006
From: John.Kerpel at infores.com (Kerpel, John)
Date: Fri, 23 Jun 2006 16:12:27 -0500
Subject: [R] Problems with weekday extraction from zoo objects
Message-ID: <44A8B25381923D4F93B74B2676A50F6D02AF383D@MAIL1.infores.com>

Gabor:

In my attempts to provide a reproducible example I now run into the
following problems:

> SP500<-read.zoo("SP500.csv", sep = ",")
Error in read.zoo("SP500.csv", sep = ",") : 
        index contains NAs
> DGS10<-read.zoo("DGS10.csv",sep=",")
Error in read.zoo("DGS10.csv", sep = ",") : 
        index contains NAs
> SP500
Error: object "SP500" not found
> DGS10
Error: object "DGS10" not found
> 


First ten records of SP500.csv:

Date		Open	High	Low	Close	Volume	Adj. Close*
3-Jan-50	16.66	16.66	16.66	16.66	1260000	16.66
4-Jan-50	16.85	16.85	16.85	16.85	1890000	16.85
5-Jan-50	16.93	16.93	16.93	16.93	2550000	16.93
6-Jan-50	16.98	16.98	16.98	16.98	2010000	16.98
9-Jan-50	17.08	17.08	17.08	17.08	2520000	17.08
10-Jan-50	17.03	17.03	17.03	17.03	2160000	17.03
11-Jan-50	17.09	17.09	17.09	17.09	2630000	17.09
12-Jan-50	16.76	16.76	16.76	16.76	2970000	16.76
13-Jan-50	16.67	16.67	16.67	16.67	3330000	16.67

First ten records of DGS10.csv

DATE      	 VALUE
1/2/1962	4.06
1/3/1962	4.03
1/4/1962	3.99
1/5/1962	4.02
1/8/1962	4.03
1/9/1962	4.05
1/10/1962	4.07
1/11/1962	4.08
1/12/1962	4.08

This worked perfectly yesterday; I did get the NA warnings but it read
the entire file(s) correctly.

John


-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Sent: Friday, June 23, 2006 3:24 PM
To: Kerpel, John
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Problems with weekday extraction from zoo objects

Please provide a reproducible example (and read the posting
guide at the bottom of each email).

On 6/23/06, Kerpel, John <John.Kerpel at infores.com> wrote:
> Hi Folks!
>
>
>
> I'm struggling with dates - but enough about my personal life.....
>
>
>
> I have two daily time series files.  In one (x) the date format is
Y/m/d
> and the other (y) is d/m/y.  I used read.zoo on both and they read
into
> R with no problem.
>
>
>
> Then I use: weekdays(as.Date(x$DATE)) and get what I expect - all the
> days of the week in my data set.
>
>
>
> When I use: weekdays(as.Date(y$Date)) I get:
>
>
>
> Error in fromchar(x) : character string is not in a standard
unambiguous
> format
>
>
>
> I've tried to set the format= in read.zoo to format="%d/%m/%Y" but
this
> doesn't seem to solve the problem.
>
>
>
> What's going on here?  (I'm new to these dates functions, so please be
> patient - I'll get the hang out of soon!)
>
>
>
> Best,
>
>
>
> John
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>


From jrosenba at rand.org  Fri Jun 23 23:33:31 2006
From: jrosenba at rand.org (Janet Rosenbaum)
Date: Fri, 23 Jun 2006 14:33:31 -0700
Subject: [R] Tetrachoric correlation in R vs. stata
In-Reply-To: <x2zmg37k3r.fsf@turmalin.kubism.ku.dk>
References: <449C27F4.2010400@rand.org> <x2zmg37k3r.fsf@turmalin.kubism.ku.dk>
Message-ID: <449C5E2B.9060501@rand.org>

Peter --- Thanks for pointing out the omitted information.  The hazards 
of attempting to be brief.

In R, I am using polychor(vec1, vec2, std.err=T) and have used both the 
ML and 2 step estimates, which give virtually identical answers.  I am 
explicitly using only the 632 complete cases in R to make sure missing 
data is handled the same way as in stata.

Here's my data:

522	54
34	22

> polychor(v1, v2, std.err=T, ML=T)

Polychoric Correlation, ML est. = 0.5172 (0.08048)
Test of bivariate normality: Chisquare = 8.063e-06, df = 0, p = NaN

    Row Thresholds
    Threshold Std.Err.
  1     1.349  0.07042


    Column Thresholds
    Threshold Std.Err.
  1     1.174  0.06458
  Warning message:
  NaNs produced in: pchisq(q, df, lower.tail, log.p)

In stata, I get:

. tetrachoric t1_v19a ct1_ix17

Tetrachoric correlations (N=632)

----------------------------------
     Variable |  t1_v19a  ct1_ix17
-------------+--------------------
      t1_v19a |        1
     ct1_ix17 |    .6169         1
----------------------------------

Thanks for your help.

Janet



Peter Dalgaard wrote:
> Janet Rosenbaum <jrosenba at rand.org> writes:
> 
>> I hope someone here knows the answer to this since it will save me from 
>> delving deep into documentation.
>>
>> Based on 22 pairs of vectors, I have noticed that tetrachoric 
>> correlation coefficients in stata are almost uniformly higher than those 
>> in R, sometimes dramatically so (TCC=.61 in stata, .51 in R;  .51 in 
>> stata, .39 in R).  Stata's estimate is higher than R's in 20 out of 22 
>> computations, although the estimates always fall within the 95% CI for 
>> the TCC calculated by R.
>>
>> Do stata and R calculate TCC in dramatically different ways?  Is the 
>> handling of missing data perhaps different?  Any thoughts?
>>
>> Btw, I am sending this question only to the R-help list.
> 
> 
> A bit more information seems necessary:
> 
> - tetrachoric correlations depend on 4 numbers, so you should be able
>   to give a direct example
> 
> - you're not telling us how you calculate the TCC in R. This is not
>   obvious (package polycor?).
> 


--------------------

This email message is for the sole use of the intended recip...{{dropped}}


From lewinger at usc.edu  Sat Jun 24 00:00:16 2006
From: lewinger at usc.edu (Juan Pablo Lewinger)
Date: Fri, 23 Jun 2006 15:00:16 -0700
Subject: [R] numeric variables converted to character when recoding missing
	values
Message-ID: <000001c69710$6d38fcc0$a7e07d80@NIEHS.usc.edu>

Dear R helpers,

I have a data frame where missing values for numeric variables are coded as
999. I want to recode those as NAs. The following only partially succeeds
because numeric variables are converted to character in the process:

df <- data.frame(a=c(999,1,999,2), b=LETTERS[1:4])
is.na(df[2,1]) <- TRUE
df

    a b
1 999 A
2  NA B
3 999 C
4   2 D

is.numeric(df$a)
[1] TRUE


is.na(df[!is.na(df) & df==999]) <- TRUE
df
     a b
1 <NA> A
2    1 B
3 <NA> C
4    2 D

is.character(df$a)
[1] TRUE

My question is how to do the recoding while avoiding this undesirable side
effect. I'm using R 2.2.1 (yes, I know 2.3.1 is available but don't want to
switch mid project). I'd appreciate any help.

Further details:

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    2.1            
year     2005           
month    12             
day      20             
svn rev  36812          
language R              



Juan Pablo Lewinger
Department of Preventive Medicine 
Keck School of Medicine 
University of Southern California


From Achim.Zeileis at wu-wien.ac.at  Sat Jun 24 00:04:51 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Sat, 24 Jun 2006 00:04:51 +0200
Subject: [R] Problems with weekday extraction from zoo objects
In-Reply-To: <44A8B25381923D4F93B74B2676A50F6D02AF383D@MAIL1.infores.com>
References: <44A8B25381923D4F93B74B2676A50F6D02AF383D@MAIL1.infores.com>
Message-ID: <20060624000451.6737bfdd.Achim.Zeileis@wu-wien.ac.at>

On Fri, 23 Jun 2006 16:12:27 -0500 Kerpel, John wrote:

> > SP500<-read.zoo("SP500.csv", sep = ",")
> Error in read.zoo("SP500.csv", sep = ",") : 
>         index contains NAs

Well, there are two problems with this: 1. the CSV is not
comma-separated (despite its siffix) and 2. the date format should be
specified.

> First ten records of SP500.csv:
> 
> Date		Open	High	Low	Close
> Volume	Adj. Close*
                ^^^^^^^^^^^
and I changed this name to "Adj.Close" which gives a syntactically
valid name in R.

Then I successfully employed:
  SP500 <- read.zoo("SP500.csv", format = "%d-%b-%y", header = TRUE)
  DGS10 <- read.zoo("DGS10.csv", format = "%m/%d/%Y", header = TRUE)
and then
  weekdays(time(SP500))
  weekdays(time(DGS10))

Z


From gunter.berton at gene.com  Sat Jun 24 00:15:20 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 23 Jun 2006 15:15:20 -0700
Subject: [R] numeric variables converted to character when recoding
	missingvalues
In-Reply-To: <000001c69710$6d38fcc0$a7e07d80@NIEHS.usc.edu>
Message-ID: <00b601c69712$84597ae0$f8c2fea9@gne.windows.gene.com>

Please read section 2.5 of "An Introduction to R". Numerical missing values
are assigned as NA:

x[x==999]<-NA

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Juan 
> Pablo Lewinger
> Sent: Friday, June 23, 2006 3:00 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] numeric variables converted to character when 
> recoding missingvalues
> 
> Dear R helpers,
> 
> I have a data frame where missing values for numeric 
> variables are coded as
> 999. I want to recode those as NAs. The following only 
> partially succeeds
> because numeric variables are converted to character in the process:
> 
> df <- data.frame(a=c(999,1,999,2), b=LETTERS[1:4])
> is.na(df[2,1]) <- TRUE
> df
> 
>     a b
> 1 999 A
> 2  NA B
> 3 999 C
> 4   2 D
> 
> is.numeric(df$a)
> [1] TRUE
> 
> 
> is.na(df[!is.na(df) & df==999]) <- TRUE
> df
>      a b
> 1 <NA> A
> 2    1 B
> 3 <NA> C
> 4    2 D
> 
> is.character(df$a)
> [1] TRUE
> 
> My question is how to do the recoding while avoiding this 
> undesirable side
> effect. I'm using R 2.2.1 (yes, I know 2.3.1 is available but 
> don't want to
> switch mid project). I'd appreciate any help.
> 
> Further details:
> 
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    2.1            
> year     2005           
> month    12             
> day      20             
> svn rev  36812          
> language R              
> 
> 
> 
> Juan Pablo Lewinger
> Department of Preventive Medicine 
> Keck School of Medicine 
> University of Southern California
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From p.dalgaard at biostat.ku.dk  Sat Jun 24 00:30:03 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 24 Jun 2006 00:30:03 +0200
Subject: [R] Tetrachoric correlation in R vs. stata
In-Reply-To: <449C5E2B.9060501@rand.org>
References: <449C27F4.2010400@rand.org> <x2zmg37k3r.fsf@turmalin.kubism.ku.dk>
	<449C5E2B.9060501@rand.org>
Message-ID: <x2veqr7bf8.fsf@turmalin.kubism.ku.dk>

Janet Rosenbaum <jrosenba at rand.org> writes:

> Peter --- Thanks for pointing out the omitted information.  The
> hazards of attempting to be brief.
> 
> In R, I am using polychor(vec1, vec2, std.err=T) and have used both
> the ML and 2 step estimates, which give virtually identical answers.
> I am explicitly using only the 632 complete cases in R to make sure
> missing data is handled the same way as in stata.
> 
> Here's my data:
> 
> 522	54
> 34	22
> 
> > polychor(v1, v2, std.err=T, ML=T)
> 
> Polychoric Correlation, ML est. = 0.5172 (0.08048)
> Test of bivariate normality: Chisquare = 8.063e-06, df = 0, p = NaN
> 
>     Row Thresholds
>     Threshold Std.Err.
>   1     1.349  0.07042
> 
> 
>     Column Thresholds
>     Threshold Std.Err.
>   1     1.174  0.06458
>   Warning message:
>   NaNs produced in: pchisq(q, df, lower.tail, log.p)
> 
> In stata, I get:
> 
> . tetrachoric t1_v19a ct1_ix17
> 
> Tetrachoric correlations (N=632)
> 
> ----------------------------------
>      Variable |  t1_v19a  ct1_ix17
> -------------+--------------------
>       t1_v19a |        1
>      ct1_ix17 |    .6169         1
> ----------------------------------

Well, 

> pmvnorm(c(1.349,1.174),c(Inf,Inf),
+    sigma=matrix(c(1,.5172,.5172,1),2))*632
[1] 22.00511
attr(,"error")
[1] 1e-15
attr(,"msg")
[1] "Normal Completion"
> pnorm(1.349)*632
[1] 575.9615
> pnorm(1.174)*632
[1] 556.0352

so the estimates from R appear to be consistent with the table. In
contrast, plugging in the .6169 from Stata gives


> pmvnorm(c(1.349,1.174),c(Inf,Inf),
+     sigma=matrix(c(1,.6169,.6169,1),2))*632
[1] 26.34487
...

You might want to follow up on

http://www.ats.ucla.edu/stat/stata/faq/tetrac.htm


> Thanks for your help.
> 
> Janet
> 
> 
> 
> Peter Dalgaard wrote:
> > Janet Rosenbaum <jrosenba at rand.org> writes:
> >
> >> I hope someone here knows the answer to this since it will save me
> >> from delving deep into documentation.
> >>
> >> Based on 22 pairs of vectors, I have noticed that tetrachoric
> >> correlation coefficients in stata are almost uniformly higher than
> >> those in R, sometimes dramatically so (TCC=.61 in stata, .51 in R;
> >> .51 in stata, .39 in R).  Stata's estimate is higher than R's in 20
> >> out of 22 computations, although the estimates always fall within
> >> the 95% CI for the TCC calculated by R.
> >>
> >> Do stata and R calculate TCC in dramatically different ways?  Is
> >> the handling of missing data perhaps different?  Any thoughts?
> >>
> >> Btw, I am sending this question only to the R-help list.
> > A bit more information seems necessary:
> > - tetrachoric correlations depend on 4 numbers, so you should be able
> >   to give a direct example
> > - you're not telling us how you calculate the TCC in R. This is not
> >   obvious (package polycor?).
> >
> 
> 
> --------------------
> 
> This email message is for the sole use of the intended rec...{{dropped}}


From bren at juanantonio.info  Sat Jun 24 01:27:28 2006
From: bren at juanantonio.info (=?UTF-8?Q?Juan_Antonio_Bre=C3=B1a_Moral?=)
Date: Fri, 23 Jun 2006 16:27:28 -0700 (PDT)
Subject: [R] R connectivity to database
In-Reply-To: <20060623185721.61485.qmail@web55507.mail.re4.yahoo.com>
References: <20060623185721.61485.qmail@web55507.mail.re4.yahoo.com>
Message-ID: <5020665.post@talk.nabble.com>


You should download ODBC Driver and use RODBC Package;

Best Regards.

Juan Antonio Bre?a Moral.
Advanced Marketing Ph.D. , URJC, Spain (Now)
Industrial Organisation Engineering, ICAI, Spain.
Technical Computer Programming Engineering, ICAI, Spain
Web: http://www.juanantonio.info
Mobile: +34 655970320 
--
View this message in context: http://www.nabble.com/R-connectivity-to-database-t1837933.html#a5020665
Sent from the R help forum at Nabble.com.


From jmburgos at u.washington.edu  Sat Jun 24 01:54:46 2006
From: jmburgos at u.washington.edu (Julian Burgos)
Date: Fri, 23 Jun 2006 16:54:46 -0700
Subject: [R] Running executable files from R
Message-ID: <449C7F46.5010501@u.washington.edu>

Hello fellow R's,

I apologize if this question was answer elsewhere.  I have an executable 
file that I need to run from R.  Basically I want to use R to create the 
input files this executable requires, then run it, and finally use R 
again to analyze the output files.  Because I have to do this many times 
I'd like to have the R code call the executable file after the input 
files are ready.  Is there an easy way to have R run an .exe file?
Thanks for any help,

Julian

-- 
Julian M. Burgos

Fisheries Acoustics Research Lab
School of Aquatic and Fishery Science
University of Washington

1122 NE Boat Street
Seattle, WA  98105 

Phone: 206-221-6864


From sundar.dorai-raj at pdf.com  Sat Jun 24 03:15:03 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 23 Jun 2006 20:15:03 -0500
Subject: [R] Running executable files from R
In-Reply-To: <449C7F46.5010501@u.washington.edu>
References: <449C7F46.5010501@u.washington.edu>
Message-ID: <449C9217.2050902@pdf.com>



Julian Burgos wrote:
> Hello fellow R's,
> 
> I apologize if this question was answer elsewhere.  I have an executable 
> file that I need to run from R.  Basically I want to use R to create the 
> input files this executable requires, then run it, and finally use R 
> again to analyze the output files.  Because I have to do this many times 
> I'd like to have the R code call the executable file after the input 
> files are ready.  Is there an easy way to have R run an .exe file?
> Thanks for any help,
> 
> Julian
> 

See ?system.

HTH
--sundar


From spencer.graves at pdf.com  Sat Jun 24 06:12:13 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 23 Jun 2006 21:12:13 -0700
Subject: [R] GARCH
In-Reply-To: <44996FE6.6040909@dcn.davis.ca.us>
References: <d4c57560606200551x65d99d1pd1851cf45c7ca91@mail.gmail.com>	<4498D8B8.8060808@dcn.davis.ca.us>	<Pine.LNX.4.64.0606210854240.11993@gannet.stats.ox.ac.uk>
	<44996FE6.6040909@dcn.davis.ca.us>
Message-ID: <449CBB9D.5090808@pdf.com>

	  I'll outline here how you can solve this kind of problem, using the 
first example in the 'garch' help page:

library(tseries)
      n <- 1100
      a <- c(0.1, 0.5, 0.2)  # ARCH(2) coefficients
      e <- rnorm(n)
      x <- double(n)
      x[1:2] <- rnorm(2, sd = sqrt(a[1]/(1.0-a[2]-a[3])))
      for(i in 3:n)  # Generate ARCH(2) process
      {
        x[i] <- e[i]*sqrt(a[1]+a[2]*x[i-1]^2+a[3]*x[i-2]^2)
      }
      x <- ts(x[101:1100])
      x.arch <- garch(x, order = c(0,2))  # Fit ARCH(2)
(sum.arch <- summary(x.arch))
<snip>
     Estimate  Std. Error  t value Pr(>|t|)
a0   0.09887     0.01013    9.764  < 2e-16 ***
a1   0.43104     0.05276    8.170 2.22e-16 ***
a2   0.31261     0.05844    5.350 8.82e-08 ***
<snip>

	  Then I tried 'str(sum.arch)'.  This told me it is a list with 6 
components, and the one I want is named 'coef'.  This led me to examine 
'sum.arch$coef', which includes the desired numbers.  Moreover, 
'class(sum.arch$coef)' told me this is a 'matrix'.  This information 
suggests that the following might be what you requested:

  sum.arch$coef[, "Pr(>|t|)"]
           a0           a1           a2
0.000000e+00 2.220446e-16 8.815239e-08

	  Hope this helps.
	  Spencer Graves

Jeff Newmiller wrote:
> Prof Brian Ripley wrote:
>> Why do you think
>>
>>> help.search("garch-methods", package="tseries")
>> finds accessor functions?  That is notation for S4 methods, and "garch" 
>> is an S3 class so there will be none.  Here there _is_ an accessor, 
>> coef(), and you can find that there is by
> 
> Probably because I used it, found a mention of various extraction functions
> including coef(), and could not find a way to access "Pr(>|t|)" using
> coef(). Nor have I had luck with
>   help.search("summary.garch", package="tseries")
> 
> Possibly also because I have not yet figured out the difference between
> S4 and S3 methods, but since the result of my  help.search call displayed S3
> functions I don't see how knowing this difference would have helped.
> 
>>> methods(class="garch")
>> [1] coef.garch*      fitted.garch*    logLik.garch*    plot.garch*
>> [5] predict.garch*   print.garch*     residuals.garch* summary.garch*
>>
>>    Non-visible functions are asterisked
>>
>> Note though that inherited methods might be relevant too (e.g. default 
>> methods) and indeed it seems that here the default method for coef would 
>> work just as well.
> 
> Given Arun Kumar Saha's question...
> 
>  > > Now I want to store the value of Pr(>|t|) for coefficient a0, a1,
>  > > and b1, and also values of these coefficients, so that I can use
>  > > them in future separately. I know that I can do it for coefficients
>  > > by using the command:
>  > > coef(garch1)["a0"] etc, but not for Pr(>|t|). Can anyone please
>  > > tell me how to do this?
> 
> ... I don't see how coef() helps because I have yet to figure out how
> to use coef() (or any other accessor) to find " Std. Error" of the
> coefficient, much less "Pr(>|t|)". summary.garch seems to have only a
> print method, with no accessors at all. Can you offer a solution?
>


From spencer.graves at pdf.com  Sat Jun 24 06:38:11 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 23 Jun 2006 21:38:11 -0700
Subject: [R] lmer and mixed effects logistic regression
In-Reply-To: <1150930911.3329.42.camel@localhost.localdomain>
References: <1150293574.3416.11.camel@localhost.localdomain>	<449431F1.6000906@pdf.com>	<40e66e0b0606180458p177fa824j6a181fd52c3d1277@mail.gmail.com>	<1150726454.3200.20.camel@localhost.localdomain>	<148ed8180606201127p730e699emb9e901b09a7ab039@mail.gmail.com>	<1150894770.3329.13.camel@localhost.localdomain>	<44996733.9050609@pdf.com>
	<1150930911.3329.42.camel@localhost.localdomain>
Message-ID: <449CC1B3.9040305@pdf.com>

	  Permit me to try to repeat what I said earlier a little more clearly: 
  When the outcomes are constant for each subject, either all 0's or all 
1's, the maximum likelihood estimate of the between-subject variance in 
Inf.  Any software that returns a different answer is wrong. This is NOT 
a criticism of 'lmer' or SAS NLMIXED:  This is a sufficiently rare, 
extreme case that the software does not test for it and doesn't handle 
it well when it occurs.  Adding other explanatory variables to the model 
only makes this problem worse, because anything that will produce 
complete separation for each subject will produce this kind of 
instability.

	 Consider the following:

library(lme4)
DF <- data.frame(y=c(0,0, 0,1, 1,1),
                  Subj=rep(letters[1:3], each=2),
                  x=rep(c(-1, 1), 3))
fit1 <- lmer(y~1+(1|Subj), data=DF, family=binomial)

# 'lmer' works fine here, because the outcomes from
# 1 of the 3 subjects is not constant.

 > fit.x <- lmer(y~x+(1|Subj), data=DF, family=binomial)
Warning message:
IRLS iterations for PQL did not converge

	  The addition of 'x' to the model now allows complete separation for 
each subject.  We see this in the result:

Generalized linear mixed model fit using PQL
<snip>
Random effects:
  Groups Name        Variance   Std.Dev.
  Subj   (Intercept) 3.5357e+20 1.8803e+10
number of obs: 6, groups: Subj, 3

Estimated scale (compare to 1)  9.9414e-09

Fixed effects:
                Estimate  Std. Error    z value Pr(>|z|)
(Intercept) -5.4172e-05  1.0856e+10  -4.99e-15        1
x            8.6474e+01  2.7397e+07 3.1563e-06        1

	  Note that the subject variance is 3.5e20, the estimate for x is 86 
wit a standard error of 2.7e7.  All three of these numbers are reaching 
for Inf;  lmer quit before it got there.

	  Does this make any sense, or are we still misunderstanding one another?

	  Hope this helps.
	  Spencer Graves

Rick Bilonick wrote:
> On Wed, 2006-06-21 at 08:35 -0700, Spencer Graves wrote:
>> 	  You could think of 'lmer(..., family=binomial)' as doing a separate 
>> "glm" fit for each subject, with some shrinkage provided by the assumed 
>> distribution of the random effect parameters for each subject.  Since 
>> your data are constant within subject, the intercept in your model 
>> without the subject's random effect distribution will be estimated at 
>> +/-Inf.  Since this occurs for all subjects, the maximum likelihood 
>> estimate of the subject variance is Inf, which is what I wrote in an 
>> earlier contribution to this thread.
>>
>> 	  What kind of answer do you get from SAS NLMIXED?  If it does NOT tell 
>> you that there is something strange about the estimation problem you've 
>> given it, I would call that a serious infelicity in the code.  If it is 
>> documented behavior, some might argue that it doesn't deserve the "B" 
>> word ("Bug").  The warning messages issued by 'lmer' in this case are 
>> something I think users would want, even if they are cryptic.
>>
>> 	  Hope this helps.
>> 	  Spencer Graves	
>>
> I did send in an example with data set that duplicates the problem.
> Changing the control parameters allowed lmer to produce what seem like
> reasonable estimates. Even for the case with essentially duplicate
> pairs, lmer and NLMIXED produce similar estimates (finite intercepts
> also) although lmer's coefficient estimates are as far as I can tell the
> same as glm but the standard errors are larger.
> 
> The problem I really want estimates for is different from this one
> explanatory factor example.  The model I estimate will have several
> explanatory factors, including factors that differ within each subject
> (although the responses within each subject are the same). BTW, as far
> as I know, the responses could be different within a subject but it
> seems to be very rare.
> 
> 
> Possibly the example I thought I sent never made it to the list. The
> example is below.
> 
> Rick B.
> 
> ###########################################################################
> # Example of lmer error message
> 
> 
> I made an example data set that exhibits the error. There is a dump of
> the data frame at the end.
> 
> First, I updated all my packages:
> 
>> sessionInfo()
> Version 2.3.1 (2006-06-01)
> i686-redhat-linux-gnu
> 
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"
> "datasets"
> [7] "base"
> 
> other attached packages:
>      chron       lme4     Matrix    lattice
>    "2.3-3"  "0.995-2" "0.995-11"   "0.13-8"
> 
> But I still get the error.
> 
> For comparison, here is what glm gives:
> 
> 
>> summary(glm(y~x,data=example.df,family=binomial))
> 
> Call:
> glm(formula = y ~ x, family = binomial, data = example.df)
> 
> Deviance Residuals:
>     Min       1Q   Median       3Q      Max
> -1.6747  -0.9087  -0.6125   1.1447   2.0017
> 
> Coefficients:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept)  -0.4786     0.1227  -3.901 9.59e-05 ***
> x             0.7951     0.1311   6.067 1.31e-09 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> (Dispersion parameter for binomial family taken to be 1)
> 
>     Null deviance: 436.63  on 324  degrees of freedom
> Residual deviance: 394.15  on 323  degrees of freedom
> AIC: 398.15
> 
> Number of Fisher Scoring iterations: 4
> 
> 
> Running lmer without any tweaks:
> 
>> (lmer(y~(1|id)+x,data=example.df,family=binomial))
> Error in lmer(y ~ (1 | id) + x, data = example.df, family = binomial) :
>         Leading minor of order 2 in downdated X'X is not positive
> definite
> In addition: Warning message:
> nlminb returned message singular convergence (7)
>  in: LMEopt(x = mer, value = cv)
> 
> Running lmer with list(msVerbose=TRUE):
> 
>> (lmer(y~(1|
> id)+x,data=example.df,family=binomial,control=list(msVerbose=TRUE)))
>   0     -545.002:  44801.6
>   1     -545.002:  44801.6
>   2     -545.002:  44801.6
>   3     -545.003:  44801.9
>   4     -545.014:  44805.2
>   5     -545.123:  44838.3
>   6     -546.208:  45168.3
>   7     -556.572:  48444.8
>   8     -628.932:  78993.4
>   9     -699.716:  127441.
>  10     -771.102:  206437.
>  11     -842.258:  333880.
>  12     -913.501:  540319.
>  13     -984.712:  874202.
>  14     -1055.93: 1.41452e+06
>  15     -1127.15: 2.28873e+06
>  16     -1198.37: 3.70326e+06
>  17     -1269.59: 5.99199e+06
>  18     -1340.81: 9.69524e+06
>  19     -1412.03: 1.56872e+07
>  20     -1483.25: 2.53825e+07
>  21     -1554.47: 4.10697e+07
>  22     -1625.69: 6.64522e+07
>  23     -1696.91: 1.07522e+08
>  24     -1768.13: 1.73974e+08
>  25     -1839.35: 2.81496e+08
>  26     -1910.57: 4.55470e+08
>  27     -1981.78: 7.36966e+08
>  28     -2053.00: 1.19244e+09
>  29     -2124.22: 1.92940e+09
>  30     -2195.44: 3.12184e+09
>  31     -2266.66: 5.05124e+09
>  32     -2337.88: 8.17308e+09
>  33     -2409.10: 1.32243e+10
>  34     -2480.32: 2.13974e+10
>  35     -2551.54: 3.46217e+10
>  36     -2622.76: 5.60190e+10
>  37     -2693.98: 9.06405e+10
>  38     -2765.20: 1.46659e+11
>  39     -2836.42: 2.37299e+11
>  40     -2907.64: 3.83962e+11
>  41     -2978.85: 6.21253e+11
>  42     -3050.07: 1.00521e+12
>  43     -3121.28: 1.62645e+12
>  44     -3192.47: 2.63147e+12
>  45     -3263.70: 4.25757e+12
>  46     -3334.89: 6.88953e+12
>  47     -3406.11: 1.11441e+13
>  48     -3477.22: 1.80392e+13
>  49     -3548.36: 2.91492e+13
>  50     -3619.76: 4.72269e+13
>  51     -3690.52: 7.63668e+13
>  52     -3761.36: 1.23295e+14
>  53     -3832.63: 1.99577e+14
>  54     -3900.88: 3.22856e+14
>  55     -3968.08: 4.97009e+14
>   0     -4067.06: 1.67844e+15
>   1     -4067.06: 1.67844e+15
>   0     -4265.60: 5.77607e+15
>   1     -4265.60: 5.77607e+15
>   0     -4474.52: 1.96098e+16
>   1     -4474.52: 1.96098e+16
>   0     -4723.57: 6.68597e+16
>   1     -4723.57: 6.68597e+16
>   0     -4985.37: 2.20089e+17
>   1     -4985.37: 2.20089e+17
>   0     -5268.68: 7.69417e+17
>   1     -5268.68: 7.69417e+17
>   0     -5536.64: 2.48775e+18
>   1     -5536.64: 2.48775e+18
>   0     -5853.10: 8.45248e+18
>   1     -5853.10: 8.45248e+18
>   0     -6197.46: 3.00106e+19
>   1     -6197.46: 3.00106e+19
>   0     -6400.09: 8.72855e+19
>   1     -6400.09: 8.72855e+19
>   0     -6769.87: 3.19354e+20
>   1     -6769.87: 3.19354e+20
>   0     -7085.60: 1.14993e+21
>   1     -7085.60: 1.14993e+21
>   0     -7414.58: 4.43964e+21
>   1     -7414.58: 4.43964e+21
>   0     -7665.61: 1.61085e+22
>   1     -7665.61: 1.61085e+22
> Error in lmer(y ~ (1 | id) + x, data = example.df, family = binomial,  :
>         Leading minor of order 2 in downdated X'X is not positive
> definite
> In addition: Warning message:
> nlminb returned message singular convergence (7)
>  in: LMEopt(x = mer, value = cv)
> 
> 
> Running lmer with method="Laplace" and
> control=list(usePQL=FALSE,msVerbose=TRUE):
> 
>> (lmer(y~(1|id)+x,data=example.df,family=binomial,method="Laplace",
> +   control=list(usePQL=FALSE,msVerbose=TRUE)))
>   0      347.321: -0.478643 0.795145  1.45231
>   1      334.637: -0.775380  1.49795  2.09885
>   2      326.045: -0.631955 0.917513  2.90042
>   3      307.930: -0.627581  1.85085  4.66928
>   4      304.717: -1.06671  1.40101  5.11069
>   5      299.588: -1.05336  1.85102  5.73305
>   6      297.157: -0.682292  1.60623  6.35949
>   7      282.629: -1.33421  1.86152  10.2167
>   8      270.279: -1.44945  2.72297  14.8450
>   9      263.248: -1.61188  3.21518  19.5257
>  10      254.336: -1.89092  4.01520  29.0932
>  11      248.253: -2.13096  4.72573  39.9024
>  12      243.359: -2.39747  5.49392  53.8331
>  13      239.255: -2.66754  6.31763  71.9027
>  14      235.865: -2.91894  7.17523  94.3541
>  15      232.831: -3.14279  8.11396  123.501
>  16      230.229: -3.32800  9.12440  159.978
>  17      227.957: -3.45824  10.1876  205.312
>  18      225.987: -3.50977  11.2006  258.137
>  19      223.822: -3.42383  12.2016  327.929
>  20      222.281: -3.29714  12.9668  393.939
>  21      218.687: -2.35417  15.1107  657.987
>  22      217.978: -2.00284  15.3087  724.381
>  23      216.828: -1.03243  15.3436  883.159
>  24      216.641: -0.727910  15.0860  924.584
>  25      216.561: -0.634457  14.8052  935.901
>  26      216.477: -0.670831  14.4966  934.259
>  27      216.335: -0.882568  14.1066  925.552
>  28      216.153: -1.24388  13.9061  926.647
>  29      215.914: -1.70066  14.0769  966.092
>  30      215.643: -2.07605  14.7379  1073.14
>  31      215.365: -2.25220  15.8379  1261.63
>  32      215.169: -2.20650  16.9633  1485.79
>  33      215.065: -2.05998  17.7714  1685.40
>  34      214.993: -1.85386  18.2239  1859.43
>  35      214.948: -1.69235  18.3198  1985.48
>  36      214.933: -1.65586  18.2629  2051.34
>  37      214.933: -1.65578  18.2629  2051.34
>  38      214.933: -1.65579  18.2629  2051.34
>  39      214.933: -1.65586  18.2629  2051.34
>  40      214.933: -1.65654  18.2625  2051.34
>  41      214.932: -1.66423  18.2585  2051.33
>  42      214.931: -1.70783  18.2351  2051.33
>  43      214.931: -1.73215  18.2201  2051.43
>  44      214.931: -1.74205  18.2078  2051.65
>  45      214.930: -1.73708  18.2686  2076.43
>  46      214.929: -1.73209  18.3805  2120.39
>  47      214.929: -1.73283  18.3612  2112.76
>  48      214.929: -1.73334  18.3600  2112.79
>  49      214.929: -1.73332  18.3600  2112.79
>  50      214.929: -1.73332  18.3600  2112.79
>  51      214.929: -1.73332  18.3600  2112.79
>  52      214.929: -1.73332  18.3600  2112.79
>  53      214.929: -1.73332  18.3600  2112.79
>  54      214.929: -1.73332  18.3600  2112.79
> Generalized linear mixed model fit using Laplace
> Formula: y ~ (1 | id) + x
>           Data: example.df
>  Family: binomial(logit link)
>       AIC      BIC    logLik deviance
>  220.9293 232.2807 -107.4646 214.9293
> Random effects:
>  Groups Name        Variance Std.Dev.
>  id     (Intercept) 2112.8   45.965
> number of obs: 325, groups: id, 177
> 
> Estimated scale (compare to 1)  0.06664838
> 
> Fixed effects:
>             Estimate Std. Error  z value Pr(>|z|)
> (Intercept)  -1.7333     5.7142 -0.30333  0.76164
> x            18.3600     7.3318  2.50416  0.01227 *
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> Correlation of Fixed Effects:
>   (Intr)
> x -0.382
> 
> Note that the results for x don't agree at all with what glm outputs.
> The estimated scale is very small and the sd for id appears to be very
> large.
> 
> 
> Now changing method="Laplace" to method="ML":
> 
>> (lmer(y~(1|id)+x,data=example.df,family=binomial,method="ML",
> +   control=list(usePQL=FALSE,msVerbose=TRUE)))
> Generalized linear mixed model fit using PQL
> Formula: y ~ (1 | id) + x
>           Data: example.df
>  Family: binomial(logit link)
>       AIC      BIC    logLik deviance
>  353.3209 364.6724 -173.6604 347.3209
> Random effects:
>  Groups Name        Variance Std.Dev.
>  id     (Intercept) 1.4523   1.2051
> number of obs: 325, groups: id, 177
> 
> Estimated scale (compare to 1)  0.2372670
> 
> Fixed effects:
>             Estimate Std. Error z value  Pr(>|z|)
> (Intercept) -0.47864    0.16114 -2.9703  0.002975 **
> x            0.79514    0.16872  4.7128 2.444e-06 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> Correlation of Fixed Effects:
>   (Intr)
> x -0.129
> 
> The estimated coefficients are the same as glm to 4 decimal places. The
> se's are about 30% larger than for glm. The sd for id is much smaller
> and the scale is larger.
> 
> If I try to turn PQL back on I get the error message.
> 
> 
> I used ML and PQL off on the original data set and the results are
> ROUGHLY similar to what SAS NLMIXED gives but the coefficient for x is
> about 20% lower than NLMIXED. I haven't had a chance to run NLMIXED on
> the example data frame yet.
> 
> Finally, besides my thanks for the help and apologies for the length of
> this post, here is the dump of the data frame:
> 
> 
> example.df <-
> structure(list(id = structure(as.integer(c(1, 1, 2, 2, 3, 3,
> 4, 4, 5, 5, 6, 6, 7, 7, 8, 9, 9, 10, 10, 11, 11, 12, 12, 13,
> 14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20, 21, 21,
> 22, 22, 23, 23, 24, 24, 25, 25, 26, 26, 27, 28, 29, 29, 30, 30,
> 31, 31, 32, 32, 33, 33, 34, 34, 35, 35, 36, 36, 37, 37, 38, 38,
> 39, 39, 40, 40, 41, 42, 42, 43, 43, 44, 45, 45, 46, 46, 47, 47,
> 48, 48, 49, 49, 50, 50, 51, 51, 52, 52, 53, 53, 54, 54, 55, 55,
> 56, 56, 57, 57, 58, 58, 59, 59, 60, 61, 61, 62, 62, 63, 63, 64,
> 64, 65, 65, 66, 66, 67, 67, 68, 69, 69, 70, 70, 71, 71, 72, 72,
> 73, 73, 74, 75, 75, 76, 76, 77, 77, 78, 78, 79, 79, 80, 81, 81,
> 82, 82, 83, 83, 84, 85, 85, 86, 86, 87, 87, 88, 88, 89, 89, 90,
> 90, 91, 91, 92, 92, 93, 94, 95, 95, 96, 97, 97, 98, 98, 99, 99,
> 100, 101, 101, 102, 102, 103, 103, 104, 104, 105, 105, 106, 106,
> 107, 107, 108, 108, 109, 109, 110, 111, 111, 112, 112, 113, 113,
> 114, 114, 115, 116, 116, 117, 118, 118, 119, 120, 120, 121, 121,
> 122, 123, 123, 124, 124, 125, 125, 126, 126, 127, 127, 128, 128,
> 129, 129, 130, 131, 131, 132, 133, 133, 134, 134, 135, 136, 136,
> 137, 138, 138, 139, 139, 140, 140, 141, 141, 142, 142, 143, 143,
> 144, 144, 145, 145, 146, 146, 147, 148, 148, 149, 149, 150, 150,
> 151, 151, 152, 152, 153, 153, 154, 154, 155, 155, 156, 157, 157,
> 158, 159, 160, 161, 161, 162, 162, 163, 163, 164, 164, 165, 165,
> 166, 166, 167, 167, 168, 168, 169, 169, 170, 170, 171, 171, 172,
> 172, 173, 173, 174, 174, 175, 175, 176, 176, 177, 177)), .Label = c("1",
> "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13",
> "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
> "25", "26", "27", "28", "29", "30", "31", "32", "33", "34", "35",
> "36", "37", "38", "39", "40", "41", "42", "43", "44", "45", "46",
> "47", "48", "49", "50", "51", "52", "53", "54", "55", "56", "57",
> "58", "59", "60", "61", "62", "63", "64", "65", "66", "67", "68",
> "69", "70", "71", "72", "73", "74", "75", "76", "77", "78", "79",
> "80", "81", "82", "83", "84", "85", "86", "87", "88", "89", "90",
> "91", "92", "93", "94", "95", "96", "97", "98", "99", "100",
> "101", "102", "103", "104", "105", "106", "107", "108", "109",
> "110", "111", "112", "113", "114", "115", "116", "117", "118",
> "119", "120", "121", "122", "123", "124", "125", "126", "127",
> "128", "129", "130", "131", "132", "133", "134", "135", "136",
> "137", "138", "139", "140", "141", "142", "143", "144", "145",
> "146", "147", "148", "149", "150", "151", "152", "153", "154",
> "155", "156", "157", "158", "159", "160", "161", "162", "163",
> "164", "165", "166", "167", "168", "169", "170", "171", "172",
> "173", "174", "175", "176", "177"), class = "factor"), y =
> structure(as.integer(c(1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2,
> 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1,
> 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2,
> 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1,
> 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,
> 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1,
> 1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2,
> 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2,
> 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2,
> 2, 1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2,
> 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1,
> 1, 1, 1, 2, 2, 1, 1, 1, 1)), .Label = c("0", "1"), class = "factor"),
>     x = c(0.896492660264945, 0.896492660264945, 1.59446707642661,
>     1.59446707642661, -1.05008338359102, -1.05008338359102,
> 1.09348658068790,
>     1.09348658068790, 1.12507994528403, 1.12507994528403,
> 0.276572438987850,
>     0.276572438987850, 0.434273771509725, 0.434273771509725,
>     2.09093423436586, -0.622643744937437, -0.622643744937437,
>     -0.58706802345943, -0.58706802345943, 0.124446406100572,
>     0.124446406100572, 0.126570329770903, 0.126570329770903,
>     0.181261364281855, 1.64039692579746, 1.64039692579746,
> -0.555474658863302,
>     -0.555474658863302, 0.47542479262234, 0.47542479262234,
> -0.258656325934905,
>     -0.258656325934905, 1.64995458231394, 1.64995458231394,
> 0.696047363877697,
>     0.696047363877697, -1.46716889435175, -1.46716889435175,
>     1.03375122745992, 1.03375122745992, 0.790827457666109,
> 0.790827457666109,
>     -1.53194856629677, -1.53194856629677, -1.69389774615931,
>     -1.69389774615931, -0.811141970679076, -0.811141970679076,
>     -0.582289195201196, -0.582289195201196, 0.00789609469130197,
>     0.573390771916238, -1.45628378554133, -1.45628378554133,
>     -1.16079290490689, -1.16079290490689, -0.832646697841153,
>     -0.832646697841153, -0.241930427031072, -0.241930427031072,
>     -1.29353813430241, -1.29353813430241, -0.663794766050042,
>     -0.663794766050042, -0.961940551272396, -0.961940551272396,
>     1.59499805734419, 1.59499805734419, 0.47144243574048,
> 0.47144243574048,
>     0.952245656611064, 0.952245656611064, -0.304586175305754,
>     -0.304586175305754, 0.71463169599307, 0.71463169599307,
> -1.32141463247548,
>     -0.0983000888251169, -0.0983000888251169, 0.440114561603134,
>     0.440114561603134, -1.75761545626916, -0.409985887445808,
>     -0.409985887445808, -0.847514163533447, -0.847514163533447,
>     -1.09229636653880, -1.09229636653880, -1.00415353422017,
>     -1.00415353422017, -1.63681729751924, -1.63681729751924,
>     -1.74354446195324, -1.74354446195324, -1.65460515825824,
>     -1.65460515825824, -1.30760912861834, -1.30760912861834,
>     -1.38300841891499, -1.38300841891499, -0.628750025489628,
>     -0.628750025489628, 0.323564250193864, 0.323564250193864,
>     -0.524412275184748, -0.524412275184748, -0.486181649118838,
>     -0.486181649118838, 0.142234266839580, 0.142234266839580,
>     -1.74965074250543, -1.74965074250543, -0.299010875671146,
>     2.01049062535218, 2.01049062535218, -1.18229763206896,
> -1.18229763206896,
>     0.83304044061388, 0.83304044061388, -1.44539867673089,
> -1.44539867673089,
>     -0.391136064871645, -0.391136064871645, -0.118477363693237,
>     -0.118477363693237, -1.73531425773072, -1.73531425773072,
>     0.748083493800747, -1.70717226909887, -1.70717226909887,
>     -0.210602552893726, -0.210602552893726, 0.681976369561778,
>     0.681976369561778, -0.0138741229295563, -0.0138741229295563,
>     -0.532111498489687, -0.532111498489687, -0.585740571165474,
>     -0.202106858212414, -0.202106858212414, -0.121663249198723,
>     -0.121663249198723, -0.328214826138161, -0.328214826138161,
>     -0.94468367145097, -0.94468367145097, -1.4807089077501,
> -1.4807089077501,
>     1.09083167609999, 1.15215997208072, 1.15215997208072,
> 1.55411252669037,
>     1.55411252669037, 0.0299318027709619, 0.0299318027709619,
>     -0.0913973368965427, -1.40716805066498, -1.40716805066498,
>     -0.246178274371723, -0.246178274371723, 0.473035378493218,
>     0.473035378493218, 0.221084933100514, 0.221084933100514,
>     -0.297152442459607, -0.297152442459607, 0.487106372809148,
>     0.487106372809148, -0.434676500113372, -0.434676500113372,
>     -1.18760744124478, -1.18760744124478, 0.937643681377551,
>     1.05737987829232, -0.0879459609322652, -0.0879459609322652,
>     0.310289727254308, -0.9587546657669, -0.9587546657669,
> 1.61889219863538,
>     1.61889219863538, 0.983573530748409, 0.983573530748409,
> 0.229580627781825,
>     -0.394587440835922, -0.394587440835922, 1.27163067853669,
>     1.27163067853669, 1.40649983160254, 1.40649983160254,
> -0.275116734379947,
>     -0.275116734379947, 1.86526734439348, 1.86526734439348,
> 1.72668132490455,
>     1.72668132490455, 0.929147986696238, 0.929147986696238,
> -0.738397584970334,
>     -0.738397584970334, 1.38260569031136, 1.38260569031136,
> 0.869412633468261,
>     0.426574548204786, 0.426574548204786, 0.906846788157797,
>     0.906846788157797, 0.697109325712863, 0.697109325712863,
>     1.11578777922635, 1.11578777922635, 1.36242841544324,
> 1.20101021649827,
>     1.20101021649827, 1.37676490021795, 0.76480939270458,
> 0.76480939270458,
>     1.86393989209952, 0.543124859614057, 0.543124859614057,
> -0.379985465602419,
>     -0.379985465602419, 1.04224692214123, 1.85411674512425,
> 1.85411674512425,
>     -0.251753574006341, -0.251753574006341, 0.813394146663342,
>     0.813394146663342, 0.405335311501501, 0.405335311501501,
>     0.590913142196445, 0.590913142196445, -0.263435154193149,
>     -0.263435154193149, -1.73690720048346, -1.73690720048346,
>     1.55092664118487, 1.10649561316866, 1.10649561316866,
> 0.454716536836637,
>     -0.675741836695642, -0.675741836695642, 0.91959033017976,
>     0.91959033017976, -0.256532402264574, -0.383967822484279,
>     -0.383967822484279, -0.7036183348687, -1.07955282451682,
>     -1.07955282451682, -0.640431605676436, -0.640431605676436,
>     -1.48389479325559, -1.48389479325559, 1.72747779628092,
> 1.72747779628092,
>     -0.959816627602066, -0.959816627602066, 0.562771153564595,
>     0.562771153564595, 0.830651026484758, 0.830651026484758,
>     0.126039348853320, 0.126039348853320, -0.753265050662627,
>     -0.753265050662627, 0.570735867328324, 1.56101527861893,
>     1.56101527861893, -0.701228920739579, -0.701228920739579,
>     0.272059101188397, 0.272059101188397, -0.570607615014388,
>     -0.570607615014388, -1.05539319276684, -1.05539319276684,
>     -1.09442029020913, -1.09442029020913, -1.68035773276096,
>     -1.68035773276096, -0.523350313349583, -0.523350313349583,
>     -0.142106014525635, -1.24256396621453, -1.24256396621453,
>     0.440380052061925, -0.138389148102566, 0.354626633872418,
>     0.294094809268057, 0.294094809268057, 1.84349712677261,
> 1.84349712677261,
>     -1.02857865642895, -1.02857865642895, -0.0266176649515298,
>     -0.0266176649515298, 0.699233249383193, 0.699233249383193,
>     0.950387223399524, 0.950387223399524, 0.350113296072966,
>     0.350113296072966, 0.440114561603134, 0.440114561603134,
>     -0.487774591871586, -0.487774591871586, 1.14074388235270,
>     1.14074388235270, 0.797199228677091, 0.797199228677091,
> -0.831053755088405,
>     -0.831053755088405, 0.477283225833879, 0.477283225833879,
>     1.13384113042414, 1.13384113042414, 0.607108060182704,
> 0.607108060182704,
>     0.191084511257124, 0.191084511257124, -1.54814348428303,
>     -1.54814348428303)), .Names = c("id", "y", "x"), row.names = c("4",
> "101", "5", "102", "6", "103", "7", "104", "1", "98", "8", "105",
> "9", "106", "198", "199", "263", "10", "107", "11", "108", "200",
> "264", "197", "12", "109", "201", "265", "202", "266", "203",
> "267", "204", "268", "205", "269", "2", "99", "3", "100", "206",
> "270", "16", "113", "17", "114", "18", "115", "19", "116", "117",
> "20", "21", "118", "22", "119", "23", "120", "24", "121", "25",
> "122", "26", "123", "27", "124", "28", "125", "29", "126", "30",
> "127", "31", "128", "207", "271", "32", "33", "129", "208", "272",
> "130", "34", "131", "35", "132", "36", "133", "37", "134", "38",
> "135", "39", "136", "13", "110", "40", "137", "41", "138", "42",
> "139", "209", "273", "43", "140", "44", "141", "210", "274",
> "45", "142", "211", "46", "143", "14", "111", "212", "275", "47",
> "144", "48", "145", "213", "276", "214", "277", "215", "49",
> "146", "50", "147", "216", "278", "217", "279", "218", "280",
> "51", "52", "148", "219", "281", "220", "282", "221", "283",
> "53", "149", "222", "223", "284", "54", "150", "55", "151", "152",
> "56", "153", "57", "154", "58", "155", "224", "285", "225", "286",
> "226", "287", "15", "112", "59", "156", "289", "290", "228",
> "291", "229", "60", "157", "230", "292", "231", "293", "294",
> "232", "295", "233", "296", "61", "158", "234", "297", "235",
> "298", "62", "159", "236", "299", "63", "160", "64", "161", "300",
> "237", "301", "65", "162", "66", "163", "238", "302", "303",
> "67", "164", "304", "68", "165", "239", "240", "305", "241",
> "306", "307", "242", "308", "69", "166", "70", "167", "227",
> "288", "243", "309", "73", "170", "74", "171", "247", "248",
> "312", "249", "75", "172", "250", "313", "244", "76", "173",
> "174", "77", "175", "251", "314", "78", "176", "252", "315",
> "79", "177", "253", "316", "254", "317", "80", "178", "255",
> "318", "319", "81", "179", "82", "180", "83", "181", "84", "182",
> "85", "183", "86", "184", "71", "168", "256", "320", "185", "87",
> "186", "257", "258", "321", "88", "187", "259", "322", "260",
> "323", "245", "310", "261", "324", "89", "188", "90", "189",
> "91", "190", "92", "191", "246", "311", "93", "192", "94", "193",
> "95", "194", "96", "195", "262", "325", "97", "196", "72", "169"
> ), class = "data.frame")
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From lewinger at usc.edu  Sat Jun 24 08:14:12 2006
From: lewinger at usc.edu (Juan Pablo Lewinger)
Date: Fri, 23 Jun 2006 23:14:12 -0700
Subject: [R] numeric variables converted to character when recoding
 missingvalues
In-Reply-To: <00b601c69712$84597ae0$f8c2fea9@gne.windows.gene.com>
Message-ID: <000101c69755$6cfc3480$0400a8c0@NIEHS.usc.edu>

Thanks Bert, that works of course and is much more straightforward than what
I was trying. However, I'm still puzzled as to why x[x==99]<-NA works (i.e.
it replaces the 999s with NAs and keeps the numeric variables numeric) but
is.na(x[x==999])<-TRUE doesn't (it replaces the 999s with NAs but changes
all variables where a replacement was made to character)

PS:  As far as I can tell section 2.5 of "An Introduction to R" -which I had
read- doesn't answer my original question.

Juan Pablo Lewinger
Department of Preventive Medicine 
Keck School of Medicine 
University of Southern California

-----Original Message-----
From: Berton Gunter [mailto:gunter.berton at gene.com] 
Sent: Friday, June 23, 2006 3:15 PM
To: 'Juan Pablo Lewinger'; r-help at stat.math.ethz.ch
Subject: RE: [R] numeric variables converted to character when recoding
missingvalues

Please read section 2.5 of "An Introduction to R". Numerical missing values
are assigned as NA:

x[x==999]<-NA

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Juan 
> Pablo Lewinger
> Sent: Friday, June 23, 2006 3:00 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] numeric variables converted to character when 
> recoding missingvalues
> 
> Dear R helpers,
> 
> I have a data frame where missing values for numeric 
> variables are coded as
> 999. I want to recode those as NAs. The following only 
> partially succeeds
> because numeric variables are converted to character in the process:
> 
> df <- data.frame(a=c(999,1,999,2), b=LETTERS[1:4])
> is.na(df[2,1]) <- TRUE
> df
> 
>     a b
> 1 999 A
> 2  NA B
> 3 999 C
> 4   2 D
> 
> is.numeric(df$a)
> [1] TRUE
> 
> 
> is.na(df[!is.na(df) & df==999]) <- TRUE
> df
>      a b
> 1 <NA> A
> 2    1 B
> 3 <NA> C
> 4    2 D
> 
> is.character(df$a)
> [1] TRUE
> 
> My question is how to do the recoding while avoiding this 
> undesirable side
> effect. I'm using R 2.2.1 (yes, I know 2.3.1 is available but 
> don't want to
> switch mid project). I'd appreciate any help.
> 
> Further details:
> 
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    2.1            
> year     2005           
> month    12             
> day      20             
> svn rev  36812          
> language R              
> 
> 
> 
> Juan Pablo Lewinger
> Department of Preventive Medicine 
> Keck School of Medicine 
> University of Southern California
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From jfox at mcmaster.ca  Sat Jun 24 09:09:47 2006
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 24 Jun 2006 03:09:47 -0400
Subject: [R] Tetrachoric correlation in R vs. stata
In-Reply-To: <x2veqr7bf8.fsf@turmalin.kubism.ku.dk>
Message-ID: <web-130231135@cgpsrv2.cis.mcmaster.ca>

Dear Peter,

Thanks for fielding the followup to this question -- I didn't see it
until Saturday morning.

Janet says that polychor() gives virtually the same ML and 2-step
estimates, but (thinking about it) I believe that these should be
identical within rounding error in the 2 x 2 case, and that appears to
be true.

Janet's example shows that the print method for polycor objects tries
to compute a test of bivariate normality even for tetrachoric
correlations where the df for the test are 0, producing a warning. I've
fixed that and uploaded a new version of the package to CRAN.

Regards,
 John

On 24 Jun 2006 00:30:03 +0200
 Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Janet Rosenbaum <jrosenba at rand.org> writes:
> 
> > Peter --- Thanks for pointing out the omitted information.  The
> > hazards of attempting to be brief.
> > 
> > In R, I am using polychor(vec1, vec2, std.err=T) and have used both
> > the ML and 2 step estimates, which give virtually identical
> answers.
> > I am explicitly using only the 632 complete cases in R to make sure
> > missing data is handled the same way as in stata.
> > 
> > Here's my data:
> > 
> > 522	54
> > 34	22
> > 
> > > polychor(v1, v2, std.err=T, ML=T)
> > 
> > Polychoric Correlation, ML est. = 0.5172 (0.08048)
> > Test of bivariate normality: Chisquare = 8.063e-06, df = 0, p = NaN
> > 
> >     Row Thresholds
> >     Threshold Std.Err.
> >   1     1.349  0.07042
> > 
> > 
> >     Column Thresholds
> >     Threshold Std.Err.
> >   1     1.174  0.06458
> >   Warning message:
> >   NaNs produced in: pchisq(q, df, lower.tail, log.p)
> > 
> > In stata, I get:
> > 
> > . tetrachoric t1_v19a ct1_ix17
> > 
> > Tetrachoric correlations (N=632)
> > 
> > ----------------------------------
> >      Variable |  t1_v19a  ct1_ix17
> > -------------+--------------------
> >       t1_v19a |        1
> >      ct1_ix17 |    .6169         1
> > ----------------------------------
> 
> Well, 
> 
> > pmvnorm(c(1.349,1.174),c(Inf,Inf),
> +    sigma=matrix(c(1,.5172,.5172,1),2))*632
> [1] 22.00511
> attr(,"error")
> [1] 1e-15
> attr(,"msg")
> [1] "Normal Completion"
> > pnorm(1.349)*632
> [1] 575.9615
> > pnorm(1.174)*632
> [1] 556.0352
> 
> so the estimates from R appear to be consistent with the table. In
> contrast, plugging in the .6169 from Stata gives
> 
> 
> > pmvnorm(c(1.349,1.174),c(Inf,Inf),
> +     sigma=matrix(c(1,.6169,.6169,1),2))*632
> [1] 26.34487
> ...
> 
> You might want to follow up on
> 
> http://www.ats.ucla.edu/stat/stata/faq/tetrac.htm
> 
> 
> > Thanks for your help.
> > 
> > Janet
> > 
> > 
> > 
> > Peter Dalgaard wrote:
> > > Janet Rosenbaum <jrosenba at rand.org> writes:
> > >
> > >> I hope someone here knows the answer to this since it will save
> me
> > >> from delving deep into documentation.
> > >>
> > >> Based on 22 pairs of vectors, I have noticed that tetrachoric
> > >> correlation coefficients in stata are almost uniformly higher
> than
> > >> those in R, sometimes dramatically so (TCC=.61 in stata, .51 in
> R;
> > >> .51 in stata, .39 in R).  Stata's estimate is higher than R's in
> 20
> > >> out of 22 computations, although the estimates always fall
> within
> > >> the 95% CI for the TCC calculated by R.
> > >>
> > >> Do stata and R calculate TCC in dramatically different ways?  Is
> > >> the handling of missing data perhaps different?  Any thoughts?
> > >>
> > >> Btw, I am sending this question only to the R-help list.
> > > A bit more information seems necessary:
> > > - tetrachoric correlations depend on 4 numbers, so you should be
> able
> > >   to give a direct example
> > > - you're not telling us how you calculate the TCC in R. This is
> not
> > >   obvious (package polycor?).
> > >
> > 
> > 
> > --------------------
> > 
> > This email message is for the sole use of the intended
> rec...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/


From bartjoosen at hotmail.com  Sat Jun 24 09:22:44 2006
From: bartjoosen at hotmail.com (Bart Joosen)
Date: Sat, 24 Jun 2006 09:22:44 +0200
Subject: [R] data frame search
Message-ID: <BAY111-DAV70800C6D373C882315E5AD87B0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060624/9249d7ed/attachment.pl 

From michael.hopkins at hopkins-research.com  Sat Jun 24 11:47:09 2006
From: michael.hopkins at hopkins-research.com (Michael Hopkins)
Date: Sat, 24 Jun 2006 10:47:09 +0100
Subject: [R] Overlaying 2D kernel density plots on scatterplot matrix
Message-ID: <C0C2C8AD.1AE63%michael.hopkins@hopkins-research.com>



Hi all

We are pretty new to R here and trying to achieve something that we believe
is possible but it?s not easy to work out how to do it.

We are producing scatterplot matrices for e.g. 10 variables.  What we would
like to do is superimpose 2D kernel density estimators on top of each plot
so that we end up with a scatterplot matrix of 2D kernel density (contour)
plots.

The kernel density plots don?t have to be very sophisticated i.e. default
settings and greyscale are fine ? we can work on details later.  What has
stumped us so far is how you ?attach? the kernel density results to the
scatterplot results and then overlay them.

Any ideas, links or code gratefully received.

TIA and please CC answers here if possible

Michael


From ahmad at bpe.agr.hokudai.ac.jp  Thu Jun 22 10:20:54 2006
From: ahmad at bpe.agr.hokudai.ac.jp (ahmad at bpe.agr.hokudai.ac.jp)
Date: Thu, 22 Jun 2006 17:20:54 +0900
Subject: [R] Not able to save the work space
Message-ID: <200606220820.AA00023@gabriel.bpe.agr.hokudai.ac.jp>

Hello,

I am trying to save my work space but I recieve the following message error in save.image(name), how can 
I overcome this problem?

Ahmad A. Al-Mallahi
Laboratory of Bioproduction Engineering
Graduate School of Agriculture
Hokkaido University
Sapporo-Japan


From fiona.sammut at um.edu.mt  Sat Jun 24 13:07:37 2006
From: fiona.sammut at um.edu.mt (Fiona Sammut)
Date: Sat, 24 Jun 2006 13:07:37 +0200
Subject: [R] difference in results from R vs SPSS
Message-ID: <449D1CF9.2060009@um.edu.mt>

Hi all,

1.  I am doing some data analysis using both R and SPSS, to get used to 
both software packages.  I had performed linear regression in R and SPSS 
for a number of times before this last one and the resulting coefficient 
values always matched.  However, this last data set I was analyzing 
using simple linear regression and using the command lm(y~x), gave me 
different readings from R and SPSS:

R:                y= 33.803 + 6.897x

SPSS:             y= 47.589 + 6.263x


I have no doubts regarding the procedure employed in SPSS and I am sure 
I am using the same dataset for both software packages.


2.  Correct me if I am wrong, but from what I've read, it seems that the 
command /anova /in R works out an analysis of variance without including 
the intercept.  Is that so?  Cause again I am working out an anova in R 
and SPSS and I got different resulting sums of squares and different 
degrees of freedom.  I thought this was because one package is working 
with an intercept and the other isn't.

Can anyone enlighten me as to what might have happened in both 1. and 2. 
please?  Are they both due to my poor knowledge of R or could occurences 
as above happen?


3.  I would like a reference to a command which can be used to fit a 
distribution to a disrete distribution.


Thanks in advance for your consideration.

- Fiona Sammut



From ggrothendieck at gmail.com  Sat Jun 24 13:15:04 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 24 Jun 2006 07:15:04 -0400
Subject: [R] data frame search
In-Reply-To: <BAY111-DAV70800C6D373C882315E5AD87B0@phx.gbl>
References: <BAY111-DAV70800C6D373C882315E5AD87B0@phx.gbl>
Message-ID: <971536df0606240415y3ea1dd5do509b4a7d39e7917@mail.gmail.com>

Try this:

which(DF1 == 4, arr.ind = TRUE)


On 6/24/06, Bart Joosen <bartjoosen at hotmail.com> wrote:
> Hi,
>
> I want to make a data frame which contains the positions of some searched values in another data frame.
>
> Like:
> Dataframe 1:
>
> 1  2  3  4  1  2  3  4
> 2  3  4  1  2  3  4  2
> 4  1  2  3  2  3  4  1
>
> Let's say I searched on "4", then Dataframe 2 should contain:
> x  y
> 1  4
> 1  8
> 2  3
> 2  7
> 3  1
> 3  7
>
> I have written a routine, but it seems to me that it isn't that perfect:
> x<- 0
>
> y<- 0
>
> for (j in 1:ncol(df)) {
>
> for (i in 1:nrow(df)) {
>
> if (df[i,j] == 3) {
>
> x <- c(x,i)
>
> y <- c(y,j)
>
> }
>
> }
>
> }
>
> df2 <- data.frame(x,y)
>
> df2 <- df2[-1,]
>
> Can someone come up with an elegant/faster solution, because the ultimate goal of this routine is to analyze an jpg image.
>
> Kind regards
>
> Bart


From ccleland at optonline.net  Sat Jun 24 13:29:09 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Sat, 24 Jun 2006 07:29:09 -0400
Subject: [R] difference in results from R vs SPSS
In-Reply-To: <449D1CF9.2060009@um.edu.mt>
References: <449D1CF9.2060009@um.edu.mt>
Message-ID: <449D2205.60101@optonline.net>

Fiona Sammut wrote:
> Hi all,
> 
> 1.  I am doing some data analysis using both R and SPSS, to get used to 
> both software packages.  I had performed linear regression in R and SPSS 
> for a number of times before this last one and the resulting coefficient 
> values always matched.  However, this last data set I was analyzing 
> using simple linear regression and using the command lm(y~x), gave me 
> different readings from R and SPSS:
> 
> R:                y= 33.803 + 6.897x
> 
> SPSS:             y= 47.589 + 6.263x
> 
> I have no doubts regarding the procedure employed in SPSS and I am sure 
> I am using the same dataset for both software packages.

   My guess is you have missing values defined in SPSS but don't have 
those same values set to NA in R.  Have you requested basic descriptive 
statistics in both programs and found that they match?  Also, perhaps x 
a factor in R.  If so, how are its contrasts set?

> 2.  Correct me if I am wrong, but from what I've read, it seems that the 
> command /anova /in R works out an analysis of variance without including 
> the intercept.  Is that so?  Cause again I am working out an anova in R 
> and SPSS and I got different resulting sums of squares and different 
> degrees of freedom.  I thought this was because one package is working 
> with an intercept and the other isn't.

   I don't think this has anything to do with the discrepancy.

> Can anyone enlighten me as to what might have happened in both 1. and 2. 
> please?  Are they both due to my poor knowledge of R or could occurences 
> as above happen?
> 
> 
> 3.  I would like a reference to a command which can be used to fit a 
> distribution to a disrete distribution.

library(MASS)
?fitdistr

> Thanks in advance for your consideration.
> 
> - Fiona Sammut
> ------------------------------------------------------------------------
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From fiona.sammut at um.edu.mt  Sat Jun 24 13:51:36 2006
From: fiona.sammut at um.edu.mt (Fiona Sammut)
Date: Sat, 24 Jun 2006 13:51:36 +0200
Subject: [R] difference in results from R vs SPSS
In-Reply-To: <449D2205.60101@optonline.net>
References: <449D1CF9.2060009@um.edu.mt> <449D2205.60101@optonline.net>
Message-ID: <449D2748.9000309@um.edu.mt>

Thanks for the reply.  The mistake in 1 resulted to be due to using a 
different number of decimal places in the data. 

As regards 2, will check it out again.

Thanks again.

> Fiona Sammut wrote:
>> Hi all,
>>
>> 1.  I am doing some data analysis using both R and SPSS, to get used 
>> to both software packages.  I had performed linear regression in R 
>> and SPSS for a number of times before this last one and the resulting 
>> coefficient values always matched.  However, this last data set I was 
>> analyzing using simple linear regression and using the command 
>> lm(y~x), gave me different readings from R and SPSS:
>>
>> R:                y= 33.803 + 6.897x
>>
>> SPSS:             y= 47.589 + 6.263x
>>
>> I have no doubts regarding the procedure employed in SPSS and I am 
>> sure I am using the same dataset for both software packages.
>
>   My guess is you have missing values defined in SPSS but don't have 
> those same values set to NA in R.  Have you requested basic 
> descriptive statistics in both programs and found that they match?  
> Also, perhaps x a factor in R.  If so, how are its contrasts set?
>
>> 2.  Correct me if I am wrong, but from what I've read, it seems that 
>> the command /anova /in R works out an analysis of variance without 
>> including the intercept.  Is that so?  Cause again I am working out 
>> an anova in R and SPSS and I got different resulting sums of squares 
>> and different degrees of freedom.  I thought this was because one 
>> package is working with an intercept and the other isn't.
>
>   I don't think this has anything to do with the discrepancy.
>
>> Can anyone enlighten me as to what might have happened in both 1. and 
>> 2. please?  Are they both due to my poor knowledge of R or could 
>> occurences as above happen?
>>
>>
>> 3.  I would like a reference to a command which can be used to fit a 
>> distribution to a disrete distribution.
>
> library(MASS)
> ?fitdistr
>
>> Thanks in advance for your consideration.
>>
>> - Fiona Sammut
>> ------------------------------------------------------------------------
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>


From ripley at stats.ox.ac.uk  Sat Jun 24 14:06:09 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 24 Jun 2006 13:06:09 +0100 (BST)
Subject: [R] Not able to save the work space
In-Reply-To: <200606220820.AA00023@gabriel.bpe.agr.hokudai.ac.jp>
References: <200606220820.AA00023@gabriel.bpe.agr.hokudai.ac.jp>
Message-ID: <Pine.LNX.4.64.0606241303300.21717@gannet.stats.ox.ac.uk>

Unfortunately you have not told us what the message was, nor the R 
platform nor how you were trying to save the `work space'.

The most likely explanation is that you do not have permission to write 
the image file.  But we need a lot more details to be able to help you 
without excessive guessing.

On Thu, 22 Jun 2006, ahmad at bpe.agr.hokudai.ac.jp wrote:

> Hello,
>
> I am trying to save my work space but I recieve the following message error in save.image(name), how can
> I overcome this problem?
>
> Ahmad A. Al-Mallahi
> Laboratory of Bioproduction Engineering
> Graduate School of Agriculture
> Hokkaido University
> Sapporo-Japan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Please do supply the information it asks for.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From vdemart1 at tin.it  Sat Jun 24 17:52:18 2006
From: vdemart1 at tin.it (vittorio)
Date: Sat, 24 Jun 2006 15:52:18 +0000
Subject: [R] Producing png plot in batch mode
In-Reply-To: <1151080154.3828.45.camel@localhost.localdomain>
References: <10c01b446ea.vdemart1@tin.it>
	<1151080154.3828.45.camel@localhost.localdomain>
Message-ID: <200606241552.18782.vdemart1@tin.it>

SorThanks for the reply!!!.ry for my being so lazy and thanks a lot for your 
kind replies.....

 Well, I tried to install GDD  under freebsd (this time I read cafrefully the 
package docs and checked if the required libgd and 
freetype2 were installed and they were) BUT there's 
something wrong with  "Makevars":


NbBSD# R CMD INSTALL GDD_0.1-8.tar.gz 
* Installing *source* package 'GDD' ...
checking for gcc... cc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables... 
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether cc accepts -g... yes
checking for cc option to accept ANSI C... none needed
checking how to run the C preprocessor... cc -E
checking for egrep... grep -E
checking for ANSI C header files... yes
checking for sys/wait.h that is POSIX.1 compatible... yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for unistd.h... yes
checking for string.h... (cached) yes
checking sys/time.h usability... yes
checking sys/time.h presence... yes
checking for sys/time.h... yes
checking for unistd.h... (cached) yes
checking for an ANSI C-conforming const... yes
checking whether time.h and sys/time.h may both be included... yes
checking for stdlib.h... (cached) yes
checking for GNU libc compatible malloc... yes
checking return type of signal handlers... void
checking for memset... yes
checking for mkdir... yes
checking for rmdir... yes
checking for gdlib-config... /usr/local/bin/gdlib-config
libgd-flags according to gdlib-config:
   -I/usr/local/include -L/usr/local/lib -L/usr/local/lib -lgd
if that is not correct, fix your gd installation
checking gd.h usability... yes
checking gd.h presence... yes
checking for gd.h... yes
checking for gdImageCreateFromPng in -lgd... yes
checking usability of FreeType in GD... yes
checking whether GD programs can be compiled... yes
configure: creating ./config.status
config.status: creating src/Makevars
config.status: creating src/gddconfig.h
** libs
"Makevars", line 2: Need an operator
"Makevars", line 4: Need an operator
make: fatal errors encountered -- cannot continue
chmod: /usr/local/lib/R/library/GDD/libs/*: No such file or directory
ERROR: compilation failed for package 'GDD'
** Removing '/usr/local/lib/R/library/GDD'


What should I do?

Vittorio


Alle 16:29, venerd? 23 giugno 2006, Marc Schwartz (via MN) ha scritto:
> On Fri, 2006-06-23 at 17:23 +0100, Vittorio wrote:
> > I have set up an R procedure that is launched every three hours by
> > crontab in a unix server. Crontab runs at regular intervals the
> > following line:
> > R CMD BATH myprog.R
> >
> > myprog.R (which by the way uses
> > R2HTML) should create an updated png graph  to be referred to and seen
> > in an intranet web-page index.html.
> >
> > The problem is that both:
> >
> > png
> > (....)
> > plot(...)
> > dev.off()
> >
> > AND:
> >
> > plot(...)
> > HTMLplot(...)
> >
> > fail when
> > launched in a batch manner compalining that they need an X11() instance
> > to be used (I understand that they work only in a graphic context and
> > intarictively).
> >
> > How can I obtain that png file?
>
> See R FAQ 7.19 How do I produce PNG graphics in batch mode?
>
> HTH,
>
> Marc Schwartz


From jayemerson at gmail.com  Sat Jun 24 15:54:23 2006
From: jayemerson at gmail.com (Jay Emerson)
Date: Sat, 24 Jun 2006 09:54:23 -0400
Subject: [R] Problems creating packages.
Message-ID: <d4588dec0606240654o76a31b8av79c0133f63de83e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060624/884bc9a6/attachment.pl 

From jayemerson at gmail.com  Sat Jun 24 16:05:37 2006
From: jayemerson at gmail.com (Jay Emerson)
Date: Sat, 24 Jun 2006 10:05:37 -0400
Subject: [R] data frame search
Message-ID: <d4588dec0606240705p75d78b36ib06118535049d043@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060624/967318dd/attachment.pl 

From john.emerson at yale.edu  Sat Jun 24 16:22:33 2006
From: john.emerson at yale.edu (John W Emerson)
Date: Sat, 24 Jun 2006 10:22:33 -0400 (EDT)
Subject: [R] my poor post formatting in "data frame search" and "Problems
 creating packages" (a summary)
Message-ID: <Pine.LNX.4.63.0606241012210.2259@ajax.its.yale.edu>


I'm trying to figure out how to get my mail client (gmail)
to interface more  elegantly with the help list for submissions
so they don't appear as attachments.  Sorry about that.  A
quick summary:

- Although I was pleased with my solution to Bart Joosen's
query about data frames, Gabor's solution using which() is
clearly more elegant.

- My post to Robert Robinson's query about problems creating
simple packages will be expanded in more detail as I work on
a 'howto' summarizing my recent experience creating packages.
Thanks, again, to Duncan, Max, Jeff, and Dirk for their guidance.

Jay


From sms13+ at pitt.edu  Sat Jun 24 17:35:16 2006
From: sms13+ at pitt.edu (Steven Shechter)
Date: Sat, 24 Jun 2006 11:35:16 -0400 (EDT)
Subject: [R] smoothing splines and degrees of freedom
Message-ID: <50509.151.201.54.128.1151163316.squirrel@webmail.pitt.edu>

Hi,
If I set df=2 in my smooth.spline function, is that equivalent to running
a linear regression through my data?  It appears that df=# of data points
gives the interpolating spline and that df = 2 gives the linear
regression, but I just want to confirm this.

Thank you,
Steven


From ripley at stats.ox.ac.uk  Sat Jun 24 18:28:28 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 24 Jun 2006 17:28:28 +0100 (BST)
Subject: [R] Producing png plot in batch mode
In-Reply-To: <200606241552.18782.vdemart1@tin.it>
References: <10c01b446ea.vdemart1@tin.it>
	<1151080154.3828.45.camel@localhost.localdomain>
	<200606241552.18782.vdemart1@tin.it>
Message-ID: <Pine.LNX.4.64.0606241725200.28465@gannet.stats.ox.ac.uk>

The advice in the posting guide was to ask the package maintainer (Cc:ed 
here).  Please report what he had to say, as it would be helpful to the 
rest of us.

Note that the src/Makevars.in has

# we need to add JNI specific stuff here
ifdef DEBUG
   PKG_CFLAGS+=-DJGD_DEBUG
endif

which is GNU-make specific, so you could just delete those three lines and 
try again (or use GNU make).

On Sat, 24 Jun 2006, vittorio wrote:

> SorThanks for the reply!!!.ry for my being so lazy and thanks a lot for your
> kind replies.....
>
> Well, I tried to install GDD  under freebsd (this time I read cafrefully the
> package docs and checked if the required libgd and
> freetype2 were installed and they were) BUT there's
> something wrong with  "Makevars":
>
>
> NbBSD# R CMD INSTALL GDD_0.1-8.tar.gz
> * Installing *source* package 'GDD' ...
> checking for gcc... cc
> checking for C compiler default output file name... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables...
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether cc accepts -g... yes
> checking for cc option to accept ANSI C... none needed
> checking how to run the C preprocessor... cc -E
> checking for egrep... grep -E
> checking for ANSI C header files... yes
> checking for sys/wait.h that is POSIX.1 compatible... yes
> checking for sys/types.h... yes
> checking for sys/stat.h... yes
> checking for stdlib.h... yes
> checking for string.h... yes
> checking for memory.h... yes
> checking for strings.h... yes
> checking for inttypes.h... yes
> checking for stdint.h... yes
> checking for unistd.h... yes
> checking for string.h... (cached) yes
> checking sys/time.h usability... yes
> checking sys/time.h presence... yes
> checking for sys/time.h... yes
> checking for unistd.h... (cached) yes
> checking for an ANSI C-conforming const... yes
> checking whether time.h and sys/time.h may both be included... yes
> checking for stdlib.h... (cached) yes
> checking for GNU libc compatible malloc... yes
> checking return type of signal handlers... void
> checking for memset... yes
> checking for mkdir... yes
> checking for rmdir... yes
> checking for gdlib-config... /usr/local/bin/gdlib-config
> libgd-flags according to gdlib-config:
>   -I/usr/local/include -L/usr/local/lib -L/usr/local/lib -lgd
> if that is not correct, fix your gd installation
> checking gd.h usability... yes
> checking gd.h presence... yes
> checking for gd.h... yes
> checking for gdImageCreateFromPng in -lgd... yes
> checking usability of FreeType in GD... yes
> checking whether GD programs can be compiled... yes
> configure: creating ./config.status
> config.status: creating src/Makevars
> config.status: creating src/gddconfig.h
> ** libs
> "Makevars", line 2: Need an operator
> "Makevars", line 4: Need an operator
> make: fatal errors encountered -- cannot continue
> chmod: /usr/local/lib/R/library/GDD/libs/*: No such file or directory
> ERROR: compilation failed for package 'GDD'
> ** Removing '/usr/local/lib/R/library/GDD'
>
>
> What should I do?
>
> Vittorio
>
>
> Alle 16:29, venerd? 23 giugno 2006, Marc Schwartz (via MN) ha scritto:
>> On Fri, 2006-06-23 at 17:23 +0100, Vittorio wrote:
>>> I have set up an R procedure that is launched every three hours by
>>> crontab in a unix server. Crontab runs at regular intervals the
>>> following line:
>>> R CMD BATH myprog.R
>>>
>>> myprog.R (which by the way uses
>>> R2HTML) should create an updated png graph  to be referred to and seen
>>> in an intranet web-page index.html.
>>>
>>> The problem is that both:
>>>
>>> png
>>> (....)
>>> plot(...)
>>> dev.off()
>>>
>>> AND:
>>>
>>> plot(...)
>>> HTMLplot(...)
>>>
>>> fail when
>>> launched in a batch manner compalining that they need an X11() instance
>>> to be used (I understand that they work only in a graphic context and
>>> intarictively).
>>>
>>> How can I obtain that png file?
>>
>> See R FAQ 7.19 How do I produce PNG graphics in batch mode?
>>
>> HTH,
>>
>> Marc Schwartz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From bartjoosen at hotmail.com  Sat Jun 24 20:14:41 2006
From: bartjoosen at hotmail.com (Bart Joosen)
Date: Sat, 24 Jun 2006 20:14:41 +0200
Subject: [R] data frame search
References: <BAY111-DAV70800C6D373C882315E5AD87B0@phx.gbl>
	<971536df0606240415y3ea1dd5do509b4a7d39e7917@mail.gmail.com>
Message-ID: <BAY111-DAV12ECB661C9C37270A3C463D87B0@phx.gbl>

Thanks, couldn't find this function,

Best regards

Bart

----- Original Message ----- 
From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
To: "Bart Joosen" <bartjoosen at hotmail.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Saturday, June 24, 2006 1:15 PM
Subject: Re: [R] data frame search


> Try this:
>
> which(DF1 == 4, arr.ind = TRUE)
>
>
> On 6/24/06, Bart Joosen <bartjoosen at hotmail.com> wrote:
>> Hi,
>>
>> I want to make a data frame which contains the positions of some searched 
>> values in another data frame.
>>
>> Like:
>> Dataframe 1:
>>
>> 1  2  3  4  1  2  3  4
>> 2  3  4  1  2  3  4  2
>> 4  1  2  3  2  3  4  1
>>
>> Let's say I searched on "4", then Dataframe 2 should contain:
>> x  y
>> 1  4
>> 1  8
>> 2  3
>> 2  7
>> 3  1
>> 3  7
>>
>> I have written a routine, but it seems to me that it isn't that perfect:
>> x<- 0
>>
>> y<- 0
>>
>> for (j in 1:ncol(df)) {
>>
>> for (i in 1:nrow(df)) {
>>
>> if (df[i,j] == 3) {
>>
>> x <- c(x,i)
>>
>> y <- c(y,j)
>>
>> }
>>
>> }
>>
>> }
>>
>> df2 <- data.frame(x,y)
>>
>> df2 <- df2[-1,]
>>
>> Can someone come up with an elegant/faster solution, because the ultimate 
>> goal of this routine is to analyze an jpg image.
>>
>> Kind regards
>>
>> Bart
>


From gregory_gentlemen at yahoo.ca  Sat Jun 24 20:41:37 2006
From: gregory_gentlemen at yahoo.ca (Gregory Gentlemen)
Date: Sat, 24 Jun 2006 14:41:37 -0400 (EDT)
Subject: [R] getting the smoother matrix from smooth.spline
Message-ID: <20060624184137.81438.qmail@web31201.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060624/274cc7c5/attachment.pl 

From vdemart1 at tin.it  Sat Jun 24 23:42:42 2006
From: vdemart1 at tin.it (vittorio)
Date: Sat, 24 Jun 2006 21:42:42 +0000
Subject: [R] Producing png plot in batch mode - Problems compiling GDD
In-Reply-To: <Pine.LNX.4.64.0606241725200.28465@gannet.stats.ox.ac.uk>
References: <10c01b446ea.vdemart1@tin.it> <200606241552.18782.vdemart1@tin.it>
	<Pine.LNX.4.64.0606241725200.28465@gannet.stats.ox.ac.uk>
Message-ID: <200606242142.43011.vdemart1@tin.it>

Thanks again Prof Ripley.
Under FreeBSD 6.1 I eliminated the three lines you suggested in Makevars.in 
and the package was compiled like a charm.

Bye
Vittorio 

Alle 16:28, sabato 24 giugno 2006, Prof Brian Ripley ha scritto:
> The advice in the posting guide was to ask the package maintainer (Cc:ed
> here).  Please report what he had to say, as it would be helpful to the
> rest of us.
>
> Note that the src/Makevars.in has
>
> # we need to add JNI specific stuff here
> ifdef DEBUG
>    PKG_CFLAGS+=-DJGD_DEBUG
> endif
>
> which is GNU-make specific, so you could just delete those three lines and
> try again (or use GNU make).
>
> On Sat, 24 Jun 2006, vittorio wrote:
> > SorThanks for the reply!!!.ry for my being so lazy and thanks a lot for
> > your kind replies.....
> >
> > Well, I tried to install GDD  under freebsd (this time I read cafrefully
> > the package docs and checked if the required libgd and
> > freetype2 were installed and they were) BUT there's
> > something wrong with  "Makevars":
> >
> >
> > NbBSD# R CMD INSTALL GDD_0.1-8.tar.gz
> > * Installing *source* package 'GDD' ...
> > checking for gcc... cc
> > checking for C compiler default output file name... a.out
> > checking whether the C compiler works... yes
> > checking whether we are cross compiling... no
> > checking for suffix of executables...
> > checking for suffix of object files... o
> > checking whether we are using the GNU C compiler... yes
> > checking whether cc accepts -g... yes
> > checking for cc option to accept ANSI C... none needed
> > checking how to run the C preprocessor... cc -E
> > checking for egrep... grep -E
> > checking for ANSI C header files... yes
> > checking for sys/wait.h that is POSIX.1 compatible... yes
> > checking for sys/types.h... yes
> > checking for sys/stat.h... yes
> > checking for stdlib.h... yes
> > checking for string.h... yes
> > checking for memory.h... yes
> > checking for strings.h... yes
> > checking for inttypes.h... yes
> > checking for stdint.h... yes
> > checking for unistd.h... yes
> > checking for string.h... (cached) yes
> > checking sys/time.h usability... yes
> > checking sys/time.h presence... yes
> > checking for sys/time.h... yes
> > checking for unistd.h... (cached) yes
> > checking for an ANSI C-conforming const... yes
> > checking whether time.h and sys/time.h may both be included... yes
> > checking for stdlib.h... (cached) yes
> > checking for GNU libc compatible malloc... yes
> > checking return type of signal handlers... void
> > checking for memset... yes
> > checking for mkdir... yes
> > checking for rmdir... yes
> > checking for gdlib-config... /usr/local/bin/gdlib-config
> > libgd-flags according to gdlib-config:
> >   -I/usr/local/include -L/usr/local/lib -L/usr/local/lib -lgd
> > if that is not correct, fix your gd installation
> > checking gd.h usability... yes
> > checking gd.h presence... yes
> > checking for gd.h... yes
> > checking for gdImageCreateFromPng in -lgd... yes
> > checking usability of FreeType in GD... yes
> > checking whether GD programs can be compiled... yes
> > configure: creating ./config.status
> > config.status: creating src/Makevars
> > config.status: creating src/gddconfig.h
> > ** libs
> > "Makevars", line 2: Need an operator
> > "Makevars", line 4: Need an operator
> > make: fatal errors encountered -- cannot continue
> > chmod: /usr/local/lib/R/library/GDD/libs/*: No such file or directory
> > ERROR: compilation failed for package 'GDD'
> > ** Removing '/usr/local/lib/R/library/GDD'
> >
> >
> > What should I do?
> >
> > Vittorio
> >
> > Alle 16:29, venerd? 23 giugno 2006, Marc Schwartz (via MN) ha scritto:
> >> On Fri, 2006-06-23 at 17:23 +0100, Vittorio wrote:
> >>> I have set up an R procedure that is launched every three hours by
> >>> crontab in a unix server. Crontab runs at regular intervals the
> >>> following line:
> >>> R CMD BATH myprog.R
> >>>
> >>> myprog.R (which by the way uses
> >>> R2HTML) should create an updated png graph  to be referred to and seen
> >>> in an intranet web-page index.html.
> >>>
> >>> The problem is that both:
> >>>
> >>> png
> >>> (....)
> >>> plot(...)
> >>> dev.off()
> >>>
> >>> AND:
> >>>
> >>> plot(...)
> >>> HTMLplot(...)
> >>>
> >>> fail when
> >>> launched in a batch manner compalining that they need an X11() instance
> >>> to be used (I understand that they work only in a graphic context and
> >>> intarictively).
> >>>
> >>> How can I obtain that png file?
> >>
> >> See R FAQ 7.19 How do I produce PNG graphics in batch mode?
> >>
> >> HTH,
> >>
> >> Marc Schwartz
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html


From spencer.graves at pdf.com  Sat Jun 24 21:50:20 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 24 Jun 2006 12:50:20 -0700
Subject: [R] How to use mle or similar with integrate?
In-Reply-To: <449BECA3.80406@sun.ac.za>
References: <449BECA3.80406@sun.ac.za>
Message-ID: <449D977C.1020808@pdf.com>

	  Could you use 'optim' directly?  This won't return a nice object of 
class 'mle', but it will give you parameter estimates.

	  If you want an object of class 'mle', have you worked through the 
examples in the help file?

	  Also, are your arguments "a" and "b" are scalars?  If yes, did you 
try passing them to your 'minuslogl' function in the 'fixed' argument?

	  If this won't work for you, you can try the functions 'mle.' and 
'profile.mle' below.  If I'm not mistaken, the standard 'mle' requires 
the 'fixed' argument to be a named list of scalars.  I don't remember 
now for sure, but I think the 'mle.' function below differs from the 
standard 'mle' only in allowing 'fixed' arguments of length greater than 
1 and then dropping them from operations that require scalars.  My 
'profile.mle' is modified from the function in 'stats4' to make error 
termination less likely, at least for the applications I used.

	  Hope this helps.
	  Spencer Graves
p.s.  I thought about sending this to someone, but the "Maintainer" for 
the "stats4" package is the "R Core Team <R-core at r-project.org>", and I 
decided not to bother that whole group.

#######################################
mle. <-
function(minuslogl, start = formals(minuslogl), method = "BFGS",
     fixed = list(), ...)
{
     call <- match.call()
     n <- names(fixed)
     fullcoef <- formals(minuslogl)
     if (any(!n %in% names(fullcoef)))stop(
        "some named arguments in 'fixed' are not arguments to the 
supplied log-likelihood")
     fullcoef[n] <- fixed
     if (!missing(start) && (!is.list(start) || is.null(names(start))))
         stop("'start' must be a named list")
     start[n] <- NULL
     start <- sapply(start, eval.parent)
     nm <- names(start)
     oo <- match(nm, names(fullcoef))
     if (any(is.na(oo)))
         stop("some named arguments in 'start' are not arguments to the 
supplied log-likelihood")
     start <- start[order(oo)]
     f <- function(p) {
         l <- as.list(p)
         names(l) <- nm
         l[n] <- fixed
         do.call("minuslogl", l)
     }
     dots <- list(...)
     oout <- optim(start, f, method = method, hessian = TRUE,
         ...)
     oout$fixed <- fixed
     oout$dots <- list(...)
     coef <- oout$par
     vcov <- if (length(coef))
         solve(oout$hessian)
     else matrix(numeric(0), 0, 0)
     min <- oout$value
     fcoef <- fullcoef
     fcoef[nm] <- coef
     len.coef <- sapply(fcoef, length)
#   Drop "fixed" arguments of length != 1 as they
#   almost certainly relate arguments of "minuslogl"
#   that are NOT parameters to be estimated.
     f.coef <- fcoef[len.coef==1]
     newMLE <- new("mle", call = call, coef = coef, fullcoef = 
unlist(f.coef),
         vcov = vcov, min = min, details = oout, minuslogl = minuslogl,
         method = method)

}

#########################
#getMethods("profile", "package:stats4")

profile.mle <- function (fitted, ...){
     .local <- function (fitted, which = 1:p, maxsteps = 100,
         alpha = 0.01, zmax = sqrt(qchisq(1 - alpha/2, p)), del = zmax/5,
         trace = FALSE, ...)
     {
       onestep <- function(step, call, fixed) {
             bi <- B0[i] + sgn * step * del * std.err[i]
#           Additional fix for profile
#           Get original fix
             {
               if(is.list(fixed) && (length(fixed)>0)){
                 fixed[[pi]] <- NULL
                 k. <- length(fixed)
                 fix2 <- vector(k.+1, mode="list")
                 names(fix2) <- c(names(fixed), pi)
                 fix2[1:k.] <- fixed
                 fix2[k.+1] <- bi
               }
               else{
                 fix2 <- list(bi)
                 names(fix2) <- pi
               }
             }
             call$fixed <- fix2
             pfit <- try(eval.parent(call, 2), silent = TRUE)
             if (inherits(pfit, "try-error"))
                 return(NA)
             else {
                 zz <- 2 * (pfit at min - fitted at min)
                 ri <- pv0
                 ri[, names(pfit at coef)] <- pfit at coef
                 ri[, pi] <- bi
                 if(zz<=0){
                   msg0 <- paste("Profiling has found a better ",
                       "solution (log(LR) = ", zz, " <0), so the ",
                       "original fit had not converged:  ",
                       "This will be ignored for now, but try ",
                       "mle(..., start=list(", sep="")
                   pars <- paste(Pnames, coef(pfit), sep="=")
                   pars. <- paste(pars, collapse=", ")
                   msg1 <- paste(msg0, pars., ")).\n", sep="")
                   warning(msg1)
                 }
                 z0 <- max(zz, 0)
                 z <- sgn * sqrt(z0)
                 pvi <<- rbind(pvi, ri)
                 zi <<- c(zi, z)
               }
             if (trace)
                 cat(bi, z, "\n")
########### end subfunction onestep ##########
             list(z=z, zfit=pfit)
         }
       betterSum <- function(fit){
         if((!inherits(fit, "try-error")) && (class(fit)=="mle")){
           fitMin <- fit at min
           if(!is.numeric(fitMin)){
             warn("'mle' returned 'min' of class ", class(fitMin),
                  ";  must be numeric.")
             return(B1.)
           }
           if((nMin <- length(fitMin))!=1){
             warn("'mle' returned 'min' of length ", nMin,
                  ";  must be 1")
             return(B1.)
           }
           if(fitMin<B1.$minuslogl)
             return(list(coef=fit at fullcoef[Pnames],minuslogl=fit at min,
                nImprovementsFound=B1.$nImprovementsFound+1))
         }
########### end subfunction betterSum ##########
         B1.
       }
         summ <- summary(fitted)
         std.err <- summ at coef[, "Std. Error"]
         Pnames <- names(B0 <- fitted at coef)
         pv0 <- t(as.matrix(B0))
         B0. <- list(coef=B0, minuslogl=fitted at min)
         B1. <- list(coef=B0, minuslogl=fitted at min,
              nImprovementsFound=0)
         p <- length(Pnames)
         prof <- vector("list", length = length(which))
         names(prof) <- Pnames[which]
         call <- fitted at call
         call$minuslogl <- fitted at minuslogl
         call$method <- fitted at method
         ndeps <- eval.parent(call$control$ndeps)
         parscale <- eval.parent(call$control$parscale)
         fixed <- fitted at details$fixed
         dots <- fitted at details$dots
         if(length(dots)>0)
           for(i.. in names(dots))
             call[[i..]] <- dots[[i..]]
         for (i in which) {
             zi <- 0
             pvi <- pv0
             pi <- Pnames[i]
             if (!is.null(ndeps))
                 call$control$ndeps <- ndeps[-i]
             if (!is.null(parscale))
                 call$control$parscale <- parscale[-i]
             for (sgn in c(-1, 1)) {
                 if (trace)
                   cat("\nParameter:", pi, c("down", "up")[(sgn +
                     1)/2 + 1], "\n")
                 step <- 0
                 z <- 0
                 call$start <- as.list(B0)
                 lastz <- 0
                 while ((step <- step + 1) < maxsteps && abs(z) <
                   zmax) {
                   z. <- onestep(step, call, fixed)
                   B1. <- betterSum(z.$zfit)
                   z <- z.$z
                   if (is.na(z))
                     break
                   lastz <- z
                 }
                 if (abs(lastz) < zmax) {
                   for (dstep in c(0.2, 0.4, 0.6, 0.8, 0.9)) {
                     z. <- onestep(step-1+dstep, call, fixed)
                     B1. <- betterSum(z.$zfit)
                     z <- z.$z
                     if (is.na(z) || abs(z) > zmax)
                       break
                   }
                 }
                 else if (length(zi) < 5) {
                   mxstep <- step - 1
                   step <- 0.5
                   while ((step <- step + 1) < mxstep){
                        z. <- onestep(step, call, fixed)
                        B1. <- betterSum(z.$zfit)
#                      z <- z.$z
                      }
                 }
             }
             si <- order(pvi[, i])
             prof[[pi]] <- data.frame(z = zi[si])
             prof[[pi]]$par.vals <- pvi[si, , drop = FALSE]
         }
         new("profile.mle", profile = prof, summary = summ)
     }
     .local(fitted, ...)
}
###################################################
# mle examples:
      x <- 0:10
      y <- c(26, 17, 13, 12, 20, 5, 9, 8, 5, 4, 8)
      ll <- function(ymax=15, xhalf=6)
          -sum(stats::dpois(y, lambda=ymax/(1+x/xhalf), log=TRUE))
(fit <- mle(ll))
      (fit. <- mle.(ll))
      (fit.5 <- mle.(ll, fixed=list(xhalf=6)))

      summary(fit)
      summary(fit.)
      logLik(fit)
      logLik(fit.)
      vcov(fit)
      vcov(fit.)
      plot(profile(fit), absVal=FALSE)
      plot(prof. <- profile.mle(fit.), absVal=FALSE)
      confint(fit)
      confint(prof.)

      ## use bounded optimization
      ## the lower bounds are really > 0, but we use >=0 to stress-test 
profiling
      (fit1 <- mle(ll, method="L-BFGS-B", lower=c(0, 0)))
      (fit1. <- mle.(ll, method="L-BFGS-B", lower=c(0, 0)))
      plot(profile(fit1), absVal=FALSE)
      plot(profile.mle(fit1), absVal=FALSE)

      ## a better parametrization:
      ll2 <- function(lymax=log(15), lxhalf=log(6))
          -sum(stats::dpois(y, lambda=exp(lymax)/(1+x/exp(lxhalf)), 
log=TRUE))
      (fit2 <- mle(ll2))
      (fit2. <- mle.(ll2))
      plot(profile(fit2), absVal=FALSE)
      plot(prof2 <- profile.mle(fit2), absVal=FALSE)
      exp(confint(fit2))
      exp(confint(prof2))

      ll. <- function(ymax=15, xhalf=6, x, y)
          -sum(stats::dpois(y, lambda=ymax/(1+x/xhalf), log=TRUE))

fit <- mle(ll)
fit. <- mle.(ll)
all.equal(fit, fit.)

prof <- profile(fit)
plot(prof)
with(prof at profile$xhalf,
      plot(par.vals[,2], z, type="l"))
with(prof at profile$xhalf,
      plot(par.vals[,2], abs(z), type="l"))
with(prof at profile$xhalf,
      plot(par.vals[,1], abs(z), type="l"))

undebug(profile.mle)
prof. <- profile.mle(fit.)
plot(prof.)

(fit.xy <- mle.(ll., start=list(ymax=20, xhalf=3),
             fixed=list(x=x, y=y)))
(prof.fit <- profile.mle(fit.xy))
plot(prof.fit)

logLik(fit.xy)
logLik(fit)
logLik(fit.)

mle2 <- function(x, y){
   mle.(ll., start=list(ymax=20, xhalf=3),
        fixed=list(x=x, y=y))
}
fit2 <- mle2(x, y)
prof2 <- profile.mle(fit2)
plot(prof2)
confint(prof2)

mle3 <- function(x, y){
   do.call('mle.',
     list(ll., start=list(ymax=20, xhalf=3),
         fixed=list(x=x, y=y)))
}
fit3 <- mle3(x, y)
prof3 <- profile.mle(fit3)
confint(prof3)
plot(prof3)

mle4 <- function(x, y){
   eval(mle.(ll., start=list(ymax=20, xhalf=3),
         fixed=list(x=x, y=y)))
}
fit4 <- mle4(x, y)
prof4 <- profile.mle(fit4)
confint(prof4)

mle5 <- function(x, y){
   mle.call <- paste(
      "mle.(ll.,",
      "start=list(ymax=20, xhalf=3),",
      "fixed=list(x=x, y=y))")
   mle.parsed <- parse(text=mle.call)
   eval(mle.parsed)
}

debug(mle5)
fit5 <- mle5(x, y)
prof5 <- profile.mle(fit5)
confint(prof5)
plot(prof5)

#######################################
Rainer M Krug wrote:
> Hi
> 
> I have the following formula (I hope it is clear - if no, I can try to
> do better the next time)
> 
> h(x, a, b) =
> integral(0 to pi/2)
> (
>   (
>     integral(D/sin(alpha) to Inf)
>     (
>       (
>         f(x, a, b)
>       )
>       dx
>     )
>   dalpha
> )
> 
> and I want to do an mle with it.
> I know how to use mle() and I also know about integrate(). My problem is
> to give the parameter values a and b to the integrate function.
> 
> In other words, how can I write
> 
> h <- function...
> 
> so that I can estimate a and b?
> 
> Thanks,
> 
> Rainer
> 
>


From vdemart1 at tin.it  Sun Jun 25 01:03:20 2006
From: vdemart1 at tin.it (vittorio)
Date: Sat, 24 Jun 2006 23:03:20 +0000
Subject: [R] Producing png plot in batch mode - Problems compiling GDD
In-Reply-To: <200606242142.43011.vdemart1@tin.it>
References: <10c01b446ea.vdemart1@tin.it>
	<Pine.LNX.4.64.0606241725200.28465@gannet.stats.ox.ac.uk>
	<200606242142.43011.vdemart1@tin.it>
Message-ID: <200606242303.21458.vdemart1@tin.it>


Alle 21:42, sabato 24 giugno 2006, vittorio ha scritto:
> Thanks again Prof Ripley.
> Under FreeBSD 6.1 I eliminated the three lines you suggested in Makevars.in
> and the package was compiled like a charm.
>
> Bye
> Vittorio
>
> Alle 16:28, sabato 24 giugno 2006, Prof Brian Ripley ha scritto:
> > The advice in the posting guide was to ask the package maintainer (Cc:ed
> > here).  Please report what he had to say, as it would be helpful to the
> > rest of us.
> >
> > Note that the src/Makevars.in has
> >
> > # we need to add JNI specific stuff here
> > ifdef DEBUG
> >    PKG_CFLAGS+=-DJGD_DEBUG
> > endif
> >
> > which is GNU-make specific, so you could just delete those three lines
> > and try again (or use GNU make).
> >
> > On Sat, 24 Jun 2006, vittorio wrote:
> > > SorThanks for the reply!!!.ry for my being so lazy and thanks a lot for
> > > your kind replies.....
> > >
> > > Well, I tried to install GDD  under freebsd (this time I read
> > > cafrefully the package docs and checked if the required libgd and
> > > freetype2 were installed and they were) BUT there's
> > > something wrong with  "Makevars":
> > >
> > >
> > > NbBSD# R CMD INSTALL GDD_0.1-8.tar.gz
> > > * Installing *source* package 'GDD' ...
> > > checking for gcc... cc
> > > checking for C compiler default output file name... a.out
> > > checking whether the C compiler works... yes
> > > checking whether we are cross compiling... no
> > > checking for suffix of executables...
> > > checking for suffix of object files... o
> > > checking whether we are using the GNU C compiler... yes
> > > checking whether cc accepts -g... yes
> > > checking for cc option to accept ANSI C... none needed
> > > checking how to run the C preprocessor... cc -E
> > > checking for egrep... grep -E
> > > checking for ANSI C header files... yes
> > > checking for sys/wait.h that is POSIX.1 compatible... yes
> > > checking for sys/types.h... yes
> > > checking for sys/stat.h... yes
> > > checking for stdlib.h... yes
> > > checking for string.h... yes
> > > checking for memory.h... yes
> > > checking for strings.h... yes
> > > checking for inttypes.h... yes
> > > checking for stdint.h... yes
> > > checking for unistd.h... yes
> > > checking for string.h... (cached) yes
> > > checking sys/time.h usability... yes
> > > checking sys/time.h presence... yes
> > > checking for sys/time.h... yes
> > > checking for unistd.h... (cached) yes
> > > checking for an ANSI C-conforming const... yes
> > > checking whether time.h and sys/time.h may both be included... yes
> > > checking for stdlib.h... (cached) yes
> > > checking for GNU libc compatible malloc... yes
> > > checking return type of signal handlers... void
> > > checking for memset... yes
> > > checking for mkdir... yes
> > > checking for rmdir... yes
> > > checking for gdlib-config... /usr/local/bin/gdlib-config
> > > libgd-flags according to gdlib-config:
> > >   -I/usr/local/include -L/usr/local/lib -L/usr/local/lib -lgd
> > > if that is not correct, fix your gd installation
> > > checking gd.h usability... yes
> > > checking gd.h presence... yes
> > > checking for gd.h... yes
> > > checking for gdImageCreateFromPng in -lgd... yes
> > > checking usability of FreeType in GD... yes
> > > checking whether GD programs can be compiled... yes
> > > configure: creating ./config.status
> > > config.status: creating src/Makevars
> > > config.status: creating src/gddconfig.h
> > > ** libs
> > > "Makevars", line 2: Need an operator
> > > "Makevars", line 4: Need an operator
> > > make: fatal errors encountered -- cannot continue
> > > chmod: /usr/local/lib/R/library/GDD/libs/*: No such file or directory
> > > ERROR: compilation failed for package 'GDD'
> > > ** Removing '/usr/local/lib/R/library/GDD'
> > >
> > >
> > > What should I do?
> > >
> > > Vittorio
> > >
> > > Alle 16:29, venerd? 23 giugno 2006, Marc Schwartz (via MN) ha scritto:
> > >> On Fri, 2006-06-23 at 17:23 +0100, Vittorio wrote:
> > >>> I have set up an R procedure that is launched every three hours by
> > >>> crontab in a unix server. Crontab runs at regular intervals the
> > >>> following line:
> > >>> R CMD BATH myprog.R
> > >>>
> > >>> myprog.R (which by the way uses
> > >>> R2HTML) should create an updated png graph  to be referred to and
> > >>> seen in an intranet web-page index.html.
> > >>>
> > >>> The problem is that both:
> > >>>
> > >>> png
> > >>> (....)
> > >>> plot(...)
> > >>> dev.off()
> > >>>
> > >>> AND:
> > >>>
> > >>> plot(...)
> > >>> HTMLplot(...)
> > >>>
> > >>> fail when
> > >>> launched in a batch manner compalining that they need an X11()
> > >>> instance to be used (I understand that they work only in a graphic
> > >>> context and intarictively).
> > >>>
> > >>> How can I obtain that png file?
> > >>
> > >> See R FAQ 7.19 How do I produce PNG graphics in batch mode?
> > >>
> > >> HTH,
> > >>
> > >> Marc Schwartz
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html


From vdemart1 at tin.it  Sun Jun 25 01:07:09 2006
From: vdemart1 at tin.it (vittorio)
Date: Sat, 24 Jun 2006 23:07:09 +0000
Subject: [R] Producing png plot in batch mode
In-Reply-To: <Pine.LNX.4.64.0606241725200.28465@gannet.stats.ox.ac.uk>
References: <10c01b446ea.vdemart1@tin.it> <200606241552.18782.vdemart1@tin.it>
	<Pine.LNX.4.64.0606241725200.28465@gannet.stats.ox.ac.uk>
Message-ID: <200606242307.09857.vdemart1@tin.it>

Thanks again Prof Ripley.
Under FreeBSD 6.1 I eliminated the three lines you suggested in Makevars.in 
and the package was compiled like a charm.

Bye
Vittorio 

Alle 16:28, sabato 24 giugno 2006, Prof Brian Ripley ha scritto:
> The advice in the posting guide was to ask the package maintainer (Cc:ed
> here). ?Please report what he had to say, as it would be helpful to the
> rest of us.
>
> Note that the src/Makevars.in has
>
> # we need to add JNI specific stuff here
> ifdef DEBUG
> ? ?PKG_CFLAGS+=-DJGD_DEBUG
> endif
>
> which is GNU-make specific, so you could just delete those three lines and
> try again (or use GNU make).
>
> On Sat, 24 Jun 2006, vittorio wrote:
> > SorThanks for the reply!!!.ry for my being so lazy and thanks a lot for
> > your kind replies.....
> >
> > Well, I tried to install GDD ?under freebsd (this time I read cafrefully
> > the package docs and checked if the required libgd and
> > freetype2 were installed and they were) BUT there's
> > something wrong with ?"Makevars":
> >
> >
> > NbBSD# R CMD INSTALL GDD_0.1-8.tar.gz
> > * Installing *source* package 'GDD' ...
> > checking for gcc... cc
> > checking for C compiler default output file name... a.out
> > checking whether the C compiler works... yes
> > checking whether we are cross compiling... no
> > checking for suffix of executables...
> > checking for suffix of object files... o
> > checking whether we are using the GNU C compiler... yes
> > checking whether cc accepts -g... yes
> > checking for cc option to accept ANSI C... none needed
> > checking how to run the C preprocessor... cc -E
> > checking for egrep... grep -E
> > checking for ANSI C header files... yes
> > checking for sys/wait.h that is POSIX.1 compatible... yes
> > checking for sys/types.h... yes
> > checking for sys/stat.h... yes
> > checking for stdlib.h... yes
> > checking for string.h... yes
> > checking for memory.h... yes
> > checking for strings.h... yes
> > checking for inttypes.h... yes
> > checking for stdint.h... yes
> > checking for unistd.h... yes
> > checking for string.h... (cached) yes
> > checking sys/time.h usability... yes
> > checking sys/time.h presence... yes
> > checking for sys/time.h... yes
> > checking for unistd.h... (cached) yes
> > checking for an ANSI C-conforming const... yes
> > checking whether time.h and sys/time.h may both be included... yes
> > checking for stdlib.h... (cached) yes
> > checking for GNU libc compatible malloc... yes
> > checking return type of signal handlers... void
> > checking for memset... yes
> > checking for mkdir... yes
> > checking for rmdir... yes
> > checking for gdlib-config... /usr/local/bin/gdlib-config
> > libgd-flags according to gdlib-config:
> > ? -I/usr/local/include -L/usr/local/lib -L/usr/local/lib -lgd
> > if that is not correct, fix your gd installation
> > checking gd.h usability... yes
> > checking gd.h presence... yes
> > checking for gd.h... yes
> > checking for gdImageCreateFromPng in -lgd... yes
> > checking usability of FreeType in GD... yes
> > checking whether GD programs can be compiled... yes
> > configure: creating ./config.status
> > config.status: creating src/Makevars
> > config.status: creating src/gddconfig.h
> > ** libs
> > "Makevars", line 2: Need an operator
> > "Makevars", line 4: Need an operator
> > make: fatal errors encountered -- cannot continue
> > chmod: /usr/local/lib/R/library/GDD/libs/*: No such file or directory
> > ERROR: compilation failed for package 'GDD'
> > ** Removing '/usr/local/lib/R/library/GDD'
> >
> >
> > What should I do?
> >
> > Vittorio
> >
> > Alle 16:29, venerd? 23 giugno 2006, Marc Schwartz (via MN) ha scritto:
> >> On Fri, 2006-06-23 at 17:23 +0100, Vittorio wrote:
> >>> I have set up an R procedure that is launched every three hours by
> >>> crontab in a unix server. Crontab runs at regular intervals the
> >>> following line:
> >>> R CMD BATH myprog.R
> >>>
> >>> myprog.R (which by the way uses
> >>> R2HTML) should create an updated png graph ?to be referred to and seen
> >>> in an intranet web-page index.html.
> >>>
> >>> The problem is that both:
> >>>
> >>> png
> >>> (....)
> >>> plot(...)
> >>> dev.off()
> >>>
> >>> AND:
> >>>
> >>> plot(...)
> >>> HTMLplot(...)
> >>>
> >>> fail when
> >>> launched in a batch manner compalining that they need an X11() instance
> >>> to be used (I understand that they work only in a graphic context and
> >>> intarictively).
> >>>
> >>> How can I obtain that png file?
> >>
> >> See R FAQ 7.19 How do I produce PNG graphics in batch mode?
> >>
> >> HTH,
> >>
> >> Marc Schwartz
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From binabina at bellsouth.net  Sat Jun 24 23:09:05 2006
From: binabina at bellsouth.net (zubin)
Date: Sat, 24 Jun 2006 17:09:05 -0400
Subject: [R] R Reporting - PDF/HTML mature presentation quality package?
In-Reply-To: <4498072C.5010600@bellsouth.net>
References: <446D1C5F.9060002@bellsouth.net> <4498072C.5010600@bellsouth.net>
Message-ID: <449DA9F1.90700@bellsouth.net>

Hello, searching for the key packages so i can output, Text, Tables ,and 
Graphics into a HTML or PDF report - have R create these reports in an 
easy and efficient way without LaTeX - I have searched the R pages but 
don't see any mature packages - anyone have any advice on a easy to use 
R package one can use for generating publication quality reports?  
Outputting HTML or PDF.


From penga at ttk.ru  Sun Jun 25 00:00:55 2006
From: penga at ttk.ru (penga at ttk.ru)
Date: Sun, 25 Jun 2006 02:00:55 +0400
Subject: [R] Rebuild private R package.
Message-ID: <449DB617.50201@ttk.ru>

Hello!
Suppose I have my own R-package with long src/ subdirectory. If I edit 
only one file from this directory what I have to do to make R recompile 
only this file and resulting *.so library but not the all other source 
files? It looks to me like the default behaviour has changed in R-2.3.1.
Thank you.
M.Kondrin


From ggrothendieck at gmail.com  Sun Jun 25 01:03:28 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 24 Jun 2006 19:03:28 -0400
Subject: [R] data frame search
In-Reply-To: <BAY111-DAV12ECB661C9C37270A3C463D87B0@phx.gbl>
References: <BAY111-DAV70800C6D373C882315E5AD87B0@phx.gbl>
	<971536df0606240415y3ea1dd5do509b4a7d39e7917@mail.gmail.com>
	<BAY111-DAV12ECB661C9C37270A3C463D87B0@phx.gbl>
Message-ID: <971536df0606241603v29f0f1cer2a8c642c5cf447d8@mail.gmail.com>

which is part of base R so look again.

On 6/24/06, Bart Joosen <bartjoosen at hotmail.com> wrote:
> Thanks, couldn't find this function,
>
> Best regards
>
> Bart
>
> ----- Original Message -----
> From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
> To: "Bart Joosen" <bartjoosen at hotmail.com>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Saturday, June 24, 2006 1:15 PM
> Subject: Re: [R] data frame search
>
>
> > Try this:
> >
> > which(DF1 == 4, arr.ind = TRUE)
> >
> >
> > On 6/24/06, Bart Joosen <bartjoosen at hotmail.com> wrote:
> >> Hi,
> >>
> >> I want to make a data frame which contains the positions of some searched
> >> values in another data frame.
> >>
> >> Like:
> >> Dataframe 1:
> >>
> >> 1  2  3  4  1  2  3  4
> >> 2  3  4  1  2  3  4  2
> >> 4  1  2  3  2  3  4  1
> >>
> >> Let's say I searched on "4", then Dataframe 2 should contain:
> >> x  y
> >> 1  4
> >> 1  8
> >> 2  3
> >> 2  7
> >> 3  1
> >> 3  7
> >>
> >> I have written a routine, but it seems to me that it isn't that perfect:
> >> x<- 0
> >>
> >> y<- 0
> >>
> >> for (j in 1:ncol(df)) {
> >>
> >> for (i in 1:nrow(df)) {
> >>
> >> if (df[i,j] == 3) {
> >>
> >> x <- c(x,i)
> >>
> >> y <- c(y,j)
> >>
> >> }
> >>
> >> }
> >>
> >> }
> >>
> >> df2 <- data.frame(x,y)
> >>
> >> df2 <- df2[-1,]
> >>
> >> Can someone come up with an elegant/faster solution, because the ultimate
> >> goal of this routine is to analyze an jpg image.
> >>
> >> Kind regards
> >>
> >> Bart
> >
>


From f.harrell at vanderbilt.edu  Sun Jun 25 03:04:23 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 24 Jun 2006 20:04:23 -0500
Subject: [R] R Reporting - PDF/HTML mature presentation quality package?
In-Reply-To: <449DA9F1.90700@bellsouth.net>
References: <446D1C5F.9060002@bellsouth.net> <4498072C.5010600@bellsouth.net>
	<449DA9F1.90700@bellsouth.net>
Message-ID: <449DE117.9010904@vanderbilt.edu>

zubin wrote:
> Hello, searching for the key packages so i can output, Text, Tables ,and 
> Graphics into a HTML or PDF report - have R create these reports in an 
> easy and efficient way without LaTeX - I have searched the R pages but 
> don't see any mature packages - anyone have any advice on a easy to use 
> R package one can use for generating publication quality reports?  
> Outputting HTML or PDF.

Doing this without LaTeX is like doing statistical analysis without 
linear models and the Wilcoxon test.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From jfox at mcmaster.ca  Sun Jun 25 08:26:03 2006
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 25 Jun 2006 02:26:03 -0400
Subject: [R] Tetrachoric correlation in R vs. stata
In-Reply-To: <449C5E2B.9060501@rand.org>
Message-ID: <web-130268948@cgpsrv2.cis.mcmaster.ca>

Dear Janet,

A good thing to do when different software gives different answers is
to check each against known results. I'm away from home, and don't have
all of the examples that I used to check polychor(), but I dug up the
following. The polychor() function produces output that agrees with
both of these sources. How does Stata do?

> # example from Drasgow (1988), pp. 69-74 in Kotz and Johnson,
> #  Encyclopedia of statistical sciences. Vol. 7.
> tab
     [,1] [,2] [,3]
[1,]   58   52    1
[2,]   26   58    3
[3,]    8   12    9
 
> polychor(tab, std.err=TRUE)

Polychoric Correlation, 2-step est. = 0.42 (0.07474)
Test of bivariate normality: Chisquare = 11.55, df = 3, p = 0.009078

> polychor(tab, ML=TRUE, std.err=TRUE)

Polychoric Correlation, ML est. = 0.4191 (0.07616)
Test of bivariate normality: Chisquare = 11.54, df = 3, p = 0.009157

  Row Thresholds
  Threshold Std.Err.
1  -0.02988  0.08299
2   1.13300  0.10630


  Column Thresholds
  Threshold Std.Err.
1   -0.2422  0.08361
2    1.5940  0.13720
  
> tab # example from Brown (1977) Applied Statistics, 26:343-351. 
     [,1] [,2]
[1,] 1562   42
[2,]  383   94

> polychor(tab)    
[1] 0.595824
> 

Regards,
 John

On Fri, 23 Jun 2006 14:33:31 -0700
 Janet Rosenbaum <jrosenba at rand.org> wrote:
> Peter --- Thanks for pointing out the omitted information.  The
> hazards 
> of attempting to be brief.
> 
> In R, I am using polychor(vec1, vec2, std.err=T) and have used both
> the 
> ML and 2 step estimates, which give virtually identical answers.  I
> am 
> explicitly using only the 632 complete cases in R to make sure
> missing 
> data is handled the same way as in stata.
> 
> Here's my data:
> 
> 522	54
> 34	22
> 
> > polychor(v1, v2, std.err=T, ML=T)
> 
> Polychoric Correlation, ML est. = 0.5172 (0.08048)
> Test of bivariate normality: Chisquare = 8.063e-06, df = 0, p = NaN
> 
>     Row Thresholds
>     Threshold Std.Err.
>   1     1.349  0.07042
> 
> 
>     Column Thresholds
>     Threshold Std.Err.
>   1     1.174  0.06458
>   Warning message:
>   NaNs produced in: pchisq(q, df, lower.tail, log.p)
> 
> In stata, I get:
> 
> . tetrachoric t1_v19a ct1_ix17
> 
> Tetrachoric correlations (N=632)
> 
> ----------------------------------
>      Variable |  t1_v19a  ct1_ix17
> -------------+--------------------
>       t1_v19a |        1
>      ct1_ix17 |    .6169         1
> ----------------------------------
> 
> Thanks for your help.
> 
> Janet
> 
> 
> 
> Peter Dalgaard wrote:
> > Janet Rosenbaum <jrosenba at rand.org> writes:
> > 
> >> I hope someone here knows the answer to this since it will save me
> from 
> >> delving deep into documentation.
> >>
> >> Based on 22 pairs of vectors, I have noticed that tetrachoric 
> >> correlation coefficients in stata are almost uniformly higher than
> those 
> >> in R, sometimes dramatically so (TCC=.61 in stata, .51 in R;  .51
> in 
> >> stata, .39 in R).  Stata's estimate is higher than R's in 20 out
> of 22 
> >> computations, although the estimates always fall within the 95% CI
> for 
> >> the TCC calculated by R.
> >>
> >> Do stata and R calculate TCC in dramatically different ways?  Is
> the 
> >> handling of missing data perhaps different?  Any thoughts?
> >>
> >> Btw, I am sending this question only to the R-help list.
> > 
> > 
> > A bit more information seems necessary:
> > 
> > - tetrachoric correlations depend on 4 numbers, so you should be
> able
> >   to give a direct example
> > 
> > - you're not telling us how you calculate the TCC in R. This is not
> >   obvious (package polycor?).
> > 
> 
> 
> --------------------
> 
> This email message is for the sole use of the intended\ > ...{{dropped}}


From ripley at stats.ox.ac.uk  Sun Jun 25 09:31:08 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 25 Jun 2006 08:31:08 +0100 (BST)
Subject: [R] Rebuild private R package.
In-Reply-To: <449DB617.50201@ttk.ru>
References: <449DB617.50201@ttk.ru>
Message-ID: <Pine.LNX.4.64.0606250825300.15880@gannet.stats.ox.ac.uk>

On Sun, 25 Jun 2006, penga at ttk.ru wrote:

> Hello!
> Suppose I have my own R-package with long src/ subdirectory. If I edit
> only one file from this directory what I have to do to make R recompile
> only this file and resulting *.so library but not the all other source
> files? It looks to me like the default behaviour has changed in R-2.3.1.

I will presume you are using a Unix-alike, but you failed to say (please 
do study the posting guide and supply the basic information requested).

You have not said how you are doing this: R CMD SHLIB still works as you 
want.  What changed was R CMD INSTALL on Unix-alikes in 2.3.0, to allow 
for multiple sub-architectures.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From calstats05 at yahoo.com  Sun Jun 25 10:14:34 2006
From: calstats05 at yahoo.com (Cal Stats)
Date: Sun, 25 Jun 2006 01:14:34 -0700 (PDT)
Subject: [R] Inverting a large Matrix   (14000 x 14000)
Message-ID: <20060625081434.53896.qmail@web34001.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060625/236b0a16/attachment.pl 

From phgrosjean at sciviews.org  Sun Jun 25 10:36:32 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Sun, 25 Jun 2006 10:36:32 +0200
Subject: [R] R Reporting - PDF/HTML mature presentation quality package?
In-Reply-To: <449DE117.9010904@vanderbilt.edu>
References: <446D1C5F.9060002@bellsouth.net>
	<4498072C.5010600@bellsouth.net>	<449DA9F1.90700@bellsouth.net>
	<449DE117.9010904@vanderbilt.edu>
Message-ID: <449E4B10.9050604@sciviews.org>

Frank E Harrell Jr wrote:
> zubin wrote:
> 
>>Hello, searching for the key packages so i can output, Text, Tables ,and 
>>Graphics into a HTML or PDF report - have R create these reports in an 
>>easy and efficient way without LaTeX - I have searched the R pages but 
>>don't see any mature packages - anyone have any advice on a easy to use 
>>R package one can use for generating publication quality reports?  
>>Outputting HTML or PDF.
> 
> 
> Doing this without LaTeX is like doing statistical analysis without 
> linear models and the Wilcoxon test.
> 

It depends the complexity of formatting and the intended use of the 
report. If required formatting is just title, bold, italic, etc. + 
tables and graphs inserted in the same page to be viewed in a web 
browser, HTML (or xHTML/CSS, to be more precise) can do a pretty good 
job (and it can do even much more). Of course, LaTeX + Sweave => PDF is 
another solution, richer in functionnalities, but more complex to 
install, especially on Windows.

Zubin, you ask for "mature package". You should look at R2HTML + svViews 
(from the SciViews bundle) that work hand in hand to generate chunks of 
xHTML/CSS elements from R objects (the so called, views), or complete 
HTML page collecting together several of these views (reports). 
Depending on the "degree of maturity" that is acceptable for you, you 
should perhaps plan a beta test and expect some code adaptation, as it 
is usual with any R package that is not intensively used.

If you prefer dynamic reports, there are many approaches, but I tend to 
consider Rpad (also on CRAN) as one of the better solution out there. 
Note that it also uses R2HTML for R objects formatting, although Tom 
Short customized some of the R2HTML functions for a better use in Rpad. 
You should probably also customize and/or contribute to the development 
of R2HTML if you want a solution that best fits your own needs in any case.

Best,

Philippe Grosjean


From mkondrin at hppi.troitsk.ru  Sun Jun 25 11:18:55 2006
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Sun, 25 Jun 2006 13:18:55 +0400
Subject: [R] Rebuild private R package.
In-Reply-To: <Pine.LNX.4.64.0606250825300.15880@gannet.stats.ox.ac.uk>
References: <449DB617.50201@ttk.ru>
	<Pine.LNX.4.64.0606250825300.15880@gannet.stats.ox.ac.uk>
Message-ID: <449E54FF.4000401@hppi.troitsk.ru>

Thank you, you have guessed right. I didn't provide details because I 
thought that the only way to build r-package is to do R CMD INSTALL . 
But still I do not understand what is the best practice to install 
R-package with minor changes in the source code? Otherwise your reply 
was very educative, thank you.
M.Kondrin

> I will presume you are using a Unix-alike, but you failed to say 
> (please do study the posting guide and supply the basic information 
> requested).
>
> You have not said how you are doing this: R CMD SHLIB still works as 
> you want.  What changed was R CMD INSTALL on Unix-alikes in 2.3.0, to 
> allow for multiple sub-architectures.
>
>  
>


From collins.gs at gmail.com  Sun Jun 25 11:10:07 2006
From: collins.gs at gmail.com (Gary Collins)
Date: Sun, 25 Jun 2006 10:10:07 +0100
Subject: [R] Tetrachoric correlation in R vs. stata
In-Reply-To: <web-130268948@cgpsrv2.cis.mcmaster.ca>
References: <449C5E2B.9060501@rand.org> <web-130268948@cgpsrv2.cis.mcmaster.ca>
Message-ID: <dd6040c90606250210v4573cf76n5bfbb2efdd48c566@mail.gmail.com>

looking at the help page/code in STATA for tetrachoric, it says it
estimates the tetrachoric correlation via the approximation suggested
by Edwards & Edwards (1984), "Approximating the tetrachoric
correlation", Biometrics, 40(2): 563.

that is,

(alpha (pi/4) - 1) / (alpha^(pi/4)+1), where alpha is ad/bc

i.e.
> alpha=(522 * 22)/(34 * 54)
> (alpha^(pi/4)-1) / (alpha^(pi/4)+1)
[1] 0.6168851

HTH

Gary

On 25/06/06, John Fox <jfox at mcmaster.ca> wrote:
> Dear Janet,
>
> A good thing to do when different software gives different answers is
> to check each against known results. I'm away from home, and don't have
> all of the examples that I used to check polychor(), but I dug up the
> following. The polychor() function produces output that agrees with
> both of these sources. How does Stata do?
>
> > # example from Drasgow (1988), pp. 69-74 in Kotz and Johnson,
> > #  Encyclopedia of statistical sciences. Vol. 7.
> > tab
>      [,1] [,2] [,3]
> [1,]   58   52    1
> [2,]   26   58    3
> [3,]    8   12    9
>
> > polychor(tab, std.err=TRUE)
>
> Polychoric Correlation, 2-step est. = 0.42 (0.07474)
> Test of bivariate normality: Chisquare = 11.55, df = 3, p = 0.009078
>
> > polychor(tab, ML=TRUE, std.err=TRUE)
>
> Polychoric Correlation, ML est. = 0.4191 (0.07616)
> Test of bivariate normality: Chisquare = 11.54, df = 3, p = 0.009157
>
>   Row Thresholds
>   Threshold Std.Err.
> 1  -0.02988  0.08299
> 2   1.13300  0.10630
>
>
>   Column Thresholds
>   Threshold Std.Err.
> 1   -0.2422  0.08361
> 2    1.5940  0.13720
>
> > tab # example from Brown (1977) Applied Statistics, 26:343-351.
>      [,1] [,2]
> [1,] 1562   42
> [2,]  383   94
>
> > polychor(tab)
> [1] 0.595824
> >
>
> Regards,
>  John
>
> On Fri, 23 Jun 2006 14:33:31 -0700
>  Janet Rosenbaum <jrosenba at rand.org> wrote:
> > Peter --- Thanks for pointing out the omitted information.  The
> > hazards
> > of attempting to be brief.
> >
> > In R, I am using polychor(vec1, vec2, std.err=T) and have used both
> > the
> > ML and 2 step estimates, which give virtually identical answers.  I
> > am
> > explicitly using only the 632 complete cases in R to make sure
> > missing
> > data is handled the same way as in stata.
> >
> > Here's my data:
> >
> > 522   54
> > 34    22
> >
> > > polychor(v1, v2, std.err=T, ML=T)
> >
> > Polychoric Correlation, ML est. = 0.5172 (0.08048)
> > Test of bivariate normality: Chisquare = 8.063e-06, df = 0, p = NaN
> >
> >     Row Thresholds
> >     Threshold Std.Err.
> >   1     1.349  0.07042
> >
> >
> >     Column Thresholds
> >     Threshold Std.Err.
> >   1     1.174  0.06458
> >   Warning message:
> >   NaNs produced in: pchisq(q, df, lower.tail, log.p)
> >
> > In stata, I get:
> >
> > . tetrachoric t1_v19a ct1_ix17
> >
> > Tetrachoric correlations (N=632)
> >
> > ----------------------------------
> >      Variable |  t1_v19a  ct1_ix17
> > -------------+--------------------
> >       t1_v19a |        1
> >      ct1_ix17 |    .6169         1
> > ----------------------------------
> >
> > Thanks for your help.
> >
> > Janet
> >
> >
> >
> > Peter Dalgaard wrote:
> > > Janet Rosenbaum <jrosenba at rand.org> writes:
> > >
> > >> I hope someone here knows the answer to this since it will save me
> > from
> > >> delving deep into documentation.
> > >>
> > >> Based on 22 pairs of vectors, I have noticed that tetrachoric
> > >> correlation coefficients in stata are almost uniformly higher than
> > those
> > >> in R, sometimes dramatically so (TCC=.61 in stata, .51 in R;  .51
> > in
> > >> stata, .39 in R).  Stata's estimate is higher than R's in 20 out
> > of 22
> > >> computations, although the estimates always fall within the 95% CI
> > for
> > >> the TCC calculated by R.
> > >>
> > >> Do stata and R calculate TCC in dramatically different ways?  Is
> > the
> > >> handling of missing data perhaps different?  Any thoughts?
> > >>
> > >> Btw, I am sending this question only to the R-help list.
> > >
> > >
> > > A bit more information seems necessary:
> > >
> > > - tetrachoric correlations depend on 4 numbers, so you should be
> > able
> > >   to give a direct example
> > >
> > > - you're not telling us how you calculate the TCC in R. This is not
> > >   obvious (package polycor?).
> > >
> >
> >
> > --------------------
> >
> > This email message is for the sole use of the intended\ > ...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From p.dalgaard at biostat.ku.dk  Sun Jun 25 11:41:52 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Jun 2006 11:41:52 +0200
Subject: [R] Tetrachoric correlation in R vs. stata
In-Reply-To: <dd6040c90606250210v4573cf76n5bfbb2efdd48c566@mail.gmail.com>
References: <449C5E2B.9060501@rand.org> <web-130268948@cgpsrv2.cis.mcmaster.ca>
	<dd6040c90606250210v4573cf76n5bfbb2efdd48c566@mail.gmail.com>
Message-ID: <x2mzc1a7xb.fsf@turmalin.kubism.ku.dk>

"Gary Collins" <collins.gs at gmail.com> writes:

> looking at the help page/code in STATA for tetrachoric, it says it
> estimates the tetrachoric correlation via the approximation suggested
> by Edwards & Edwards (1984), "Approximating the tetrachoric
> correlation", Biometrics, 40(2): 563.
> 
> that is,
> 
> (alpha (pi/4) - 1) / (alpha^(pi/4)+1), where alpha is ad/bc
> 
> i.e.
> > alpha=(522 * 22)/(34 * 54)
> > (alpha^(pi/4)-1) / (alpha^(pi/4)+1)
> [1] 0.6168851


...and the approximation is obviously quite far off the mark in this
case. Presumably (I'm lazy) the approximation holds for the odds ratio
alpha close to 1 (rho close to 0) and/or marginal distributions close
to 50:50.

There's a Stata package "polychoric" which claims to do things more
accurately, referred to at

http://www.ats.ucla.edu/STAT/stata/faq/tetrac.htm 

(I believe I mentioned this before, but possibly in a private mail to
Janet which never reached r-help).
 
> HTH
> 
> Gary
> 
> On 25/06/06, John Fox <jfox at mcmaster.ca> wrote:
> > Dear Janet,
> >
> > A good thing to do when different software gives different answers is
> > to check each against known results. I'm away from home, and don't have
> > all of the examples that I used to check polychor(), but I dug up the
> > following. The polychor() function produces output that agrees with
> > both of these sources. How does Stata do?
> >
> > > # example from Drasgow (1988), pp. 69-74 in Kotz and Johnson,
> > > #  Encyclopedia of statistical sciences. Vol. 7.
> > > tab
> >      [,1] [,2] [,3]
> > [1,]   58   52    1
> > [2,]   26   58    3
> > [3,]    8   12    9
> >
> > > polychor(tab, std.err=TRUE)
> >
> > Polychoric Correlation, 2-step est. = 0.42 (0.07474)
> > Test of bivariate normality: Chisquare = 11.55, df = 3, p = 0.009078
> >
> > > polychor(tab, ML=TRUE, std.err=TRUE)
> >
> > Polychoric Correlation, ML est. = 0.4191 (0.07616)
> > Test of bivariate normality: Chisquare = 11.54, df = 3, p = 0.009157
> >
> >   Row Thresholds
> >   Threshold Std.Err.
> > 1  -0.02988  0.08299
> > 2   1.13300  0.10630
> >
> >
> >   Column Thresholds
> >   Threshold Std.Err.
> > 1   -0.2422  0.08361
> > 2    1.5940  0.13720
> >
> > > tab # example from Brown (1977) Applied Statistics, 26:343-351.
> >      [,1] [,2]
> > [1,] 1562   42
> > [2,]  383   94
> >
> > > polychor(tab)
> > [1] 0.595824
> > >
> >
> > Regards,
> >  John
> >
> > On Fri, 23 Jun 2006 14:33:31 -0700
> >  Janet Rosenbaum <jrosenba at rand.org> wrote:
> > > Peter --- Thanks for pointing out the omitted information.  The
> > > hazards
> > > of attempting to be brief.
> > >
> > > In R, I am using polychor(vec1, vec2, std.err=T) and have used both
> > > the
> > > ML and 2 step estimates, which give virtually identical answers.  I
> > > am
> > > explicitly using only the 632 complete cases in R to make sure
> > > missing
> > > data is handled the same way as in stata.
> > >
> > > Here's my data:
> > >
> > > 522   54
> > > 34    22
> > >
> > > > polychor(v1, v2, std.err=T, ML=T)
> > >
> > > Polychoric Correlation, ML est. = 0.5172 (0.08048)
> > > Test of bivariate normality: Chisquare = 8.063e-06, df = 0, p = NaN
> > >
> > >     Row Thresholds
> > >     Threshold Std.Err.
> > >   1     1.349  0.07042
> > >
> > >
> > >     Column Thresholds
> > >     Threshold Std.Err.
> > >   1     1.174  0.06458
> > >   Warning message:
> > >   NaNs produced in: pchisq(q, df, lower.tail, log.p)
> > >
> > > In stata, I get:
> > >
> > > . tetrachoric t1_v19a ct1_ix17
> > >
> > > Tetrachoric correlations (N=632)
> > >
> > > ----------------------------------
> > >      Variable |  t1_v19a  ct1_ix17
> > > -------------+--------------------
> > >       t1_v19a |        1
> > >      ct1_ix17 |    .6169         1
> > > ----------------------------------
> > >
> > > Thanks for your help.
> > >
> > > Janet
> > >
> > >
> > >
> > > Peter Dalgaard wrote:
> > > > Janet Rosenbaum <jrosenba at rand.org> writes:
> > > >
> > > >> I hope someone here knows the answer to this since it will save me
> > > from
> > > >> delving deep into documentation.
> > > >>
> > > >> Based on 22 pairs of vectors, I have noticed that tetrachoric
> > > >> correlation coefficients in stata are almost uniformly higher than
> > > those
> > > >> in R, sometimes dramatically so (TCC=.61 in stata, .51 in R;  .51
> > > in
> > > >> stata, .39 in R).  Stata's estimate is higher than R's in 20 out
> > > of 22
> > > >> computations, although the estimates always fall within the 95% CI
> > > for
> > > >> the TCC calculated by R.
> > > >>
> > > >> Do stata and R calculate TCC in dramatically different ways?  Is
> > > the
> > > >> handling of missing data perhaps different?  Any thoughts?
> > > >>
> > > >> Btw, I am sending this question only to the R-help list.
> > > >
> > > >
> > > > A bit more information seems necessary:
> > > >
> > > > - tetrachoric correlations depend on 4 numbers, so you should be
> > > able
> > > >   to give a direct example
> > > >
> > > > - you're not telling us how you calculate the TCC in R. This is not
> > > >   obvious (package polycor?).
> > > >
> > >
> > >
> > > --------------------
> > >
> > > This email message is for the sole use of the intended\ > ...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From jim at bitwrit.com.au  Mon Jun 26 01:54:55 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Sun, 25 Jun 2006 19:54:55 -0400
Subject: [R] R Reporting - PDF/HTML mature presentation quality package?
Message-ID: <449F224F.4060109@bitwrit.com.au>

I heartily second Phillipe's response. I just started a new job and the 
first thing required was a neat stats report for a dataset. I thought I 
would give R2HTML a try and about 5 minutes after downloading it, I was 
looking at the first draft of the report. I did have to do a bit of 
hacking on the graphics, but it was easy and I can now present the 
report first thing in the morning. Had I not been able to do this, I 
probably would have been told, "You'll have to use SPSS."

I was so impressed by R2HTML that I began writing a primitive HTML 
generator that will scan an R script and do something like R2HTML. I 
couldn't find anything like this as the svMisc package seems to have 
disappeared. If anyone knows of something like this or is working on it, 
I'd appreciate knowing about it.

Jim


From phgrosjean at sciviews.org  Sun Jun 25 12:07:20 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Sun, 25 Jun 2006 12:07:20 +0200
Subject: [R] R Reporting - PDF/HTML mature presentation quality package?
In-Reply-To: <449F224F.4060109@bitwrit.com.au>
References: <449F224F.4060109@bitwrit.com.au>
Message-ID: <449E6058.6010803@sciviews.org>

Jim Lemon wrote:
> I heartily second Phillipe's response. I just started a new job and the 
> first thing required was a neat stats report for a dataset. I thought I 
> would give R2HTML a try and about 5 minutes after downloading it, I was 
> looking at the first draft of the report. I did have to do a bit of 
> hacking on the graphics, but it was easy and I can now present the 
> report first thing in the morning. Had I not been able to do this, I 
> probably would have been told, "You'll have to use SPSS."
> 
> I was so impressed by R2HTML that I began writing a primitive HTML 
> generator that will scan an R script and do something like R2HTML. I 
> couldn't find anything like this as the svMisc package seems to have 
> disappeared.

No, it is still in the SciViews bundle. Tell me if you have problems 
with it.
Best,

Philippe Grosjean

If anyone knows of something like this or is working on it,
> I'd appreciate knowing about it.
> 
> Jim
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>


From Ted.Harding at nessie.mcc.ac.uk  Sun Jun 25 12:34:38 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 25 Jun 2006 11:34:38 +0100 (BST)
Subject: [R] difference in results from R vs SPSS
In-Reply-To: <449D2748.9000309@um.edu.mt>
Message-ID: <XFMail.060625113438.Ted.Harding@nessie.mcc.ac.uk>

On 24-Jun-06 Fiona Sammut wrote:
> Thanks for the reply.  The mistake in 1 resulted to be due to using a 
> different number of decimal places in the data.

If using a different number of decimal places results in such
a large difference, you need to seriously consider whether a
regression can give meaningful results at all with your data.

What do the SEs of the coefficients look like? In R, do

  summary(lm(y ~ x))

For example, I tried the following (in R):

  x<-0.1*(0:10)
  y<-x + rnorm(11)
  y3<-round(y,3)
  y1<-round(y,1)

so

 y3:
 0.058  0.165  1.329 -0.964 -0.896  0.888
       -1.124  0.630 -0.544  0.702  0.944

 y1:
 0.1    0.2    1.3   -1.0   -0.9    0.9
       -1.1    0.6   -0.5    0.7    0.9

and then:

summary(lm(y3~x))
  Coefficients:
              Estimate Std. Error t value Pr(>|t|)
  (Intercept) -0.07014    0.51113  -0.137    0.894
  x            0.35627    0.86396   0.412    0.690

summary(lm(y1~x))
  Coefficients:
              Estimate Std. Error t value Pr(>|t|)
  (Intercept) -0.05455    0.50562  -0.108    0.916
  x            0.32727    0.85465   0.383    0.711

so here too there are differences between y3~x and y1~x
not unlike yours (23% in intercept compared with your 29%,
8% in x-coefficient compared with your 9%) purely as a
result of rounding. But the SEs of intercept and x-coeff
are large, so there is very little information about what
their values should be: in round figures, a 95%CI for the
intercept would be -0.06 +/- 2.26*0.5 = (-1.19, 1.07), and
for the x-coeff 0.34 +/- 2.26*0.86 = (-1.60, 2.28). Compared
with this, the distinction between the regressions y3~x and
y1~x is at the limit of subtlety -- otherwise put, it wouldn't
matter a lot what the data were!

Best wishes,
Ted.

> As regards 2, will check it out again.
> 
> Thanks again.
> 
>> Fiona Sammut wrote:
>>> Hi all,
>>>
>>> 1.  I am doing some data analysis using both R and SPSS, to get used 
>>> to both software packages.  I had performed linear regression in R 
>>> and SPSS for a number of times before this last one and the resulting
>>> coefficient values always matched.  However, this last data set I was
>>> analyzing using simple linear regression and using the command 
>>> lm(y~x), gave me different readings from R and SPSS:
>>>
>>> R:                y= 33.803 + 6.897x
>>>
>>> SPSS:             y= 47.589 + 6.263x
>>>
>>> I have no doubts regarding the procedure employed in SPSS and I am 
>>> sure I am using the same dataset for both software packages.
>>> [...]


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 25-Jun-06                                       Time: 11:34:30
------------------------------ XFMail ------------------------------


From dmgatti at email.unc.edu  Sun Jun 25 13:27:40 2006
From: dmgatti at email.unc.edu (Daniel Gatti)
Date: Sun, 25 Jun 2006 07:27:40 -0400
Subject: [R] R memory size increases
Message-ID: <449E732C.9050800@email.unc.edu>

O/S : Solaris 9
R version : 2.2.1

I was getting out of memory errors from R when running a large job, so 
I've switched to a larger machine with 40G shared memory.  I issue the 
following command when starting R to increase memory available to R:

R --save --min-vsize=4G --min-nsize=4G

When reading in a file, R responds with "could not allocate vector of 
size 146Kb."  So I'm clearly using the command incorrectly.  Does anyone 
know how to use this command correctly?

Dan


From ripley at stats.ox.ac.uk  Sun Jun 25 15:53:37 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 25 Jun 2006 14:53:37 +0100 (BST)
Subject: [R] R memory size increases
In-Reply-To: <449E732C.9050800@email.unc.edu>
References: <449E732C.9050800@email.unc.edu>
Message-ID: <Pine.LNX.4.64.0606251440590.25021@gannet.stats.ox.ac.uk>

On Sun, 25 Jun 2006, Daniel Gatti wrote:

> O/S : Solaris 9
> R version : 2.2.1
>
> I was getting out of memory errors from R when running a large job, so
> I've switched to a larger machine with 40G shared memory.  I issue the
> following command when starting R to increase memory available to R:
>
> R --save --min-vsize=4G --min-nsize=4G
>
> When reading in a file, R responds with "could not allocate vector of
> size 146Kb."  So I'm clearly using the command incorrectly.  Does anyone
> know how to use this command correctly?

Yes, not at all.  See ?Memory and ?"Memory-limits".  The first says

      R has a variable-sized workspace (from version 1.2.0). There is
      now much less need to set memory options than previously, and most
      users will never need to set these.  They are provided both as a
      way to control the overall memory usage (which can also be done by
      operating-system facilities such as 'limit' on Unix), and since
      setting larger values of the minima will make R slightly more
      efficient on large tasks.

Do you have a 64-bit version of R?  (If not, build one, as you will need 
one to make use of large amounts of memory.)

The units for nsize are not bytes but a number of cells (28 bytes on a 
32-bit system, 56 on a 64-bit system).  You don't need that many cells 
(and you will not get it anyway).  If (as I suspect) you have a 32-bit 
version of R, the limit on vsize will not help either since address-space 
limits will intervene first.

See ?gc and ?gcinfo for ways to track actual memory usage.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From m.bridgman at sbcglobal.net  Sun Jun 25 16:49:03 2006
From: m.bridgman at sbcglobal.net (Matthew Bridgman)
Date: Sun, 25 Jun 2006 07:49:03 -0700
Subject: [R] warning messages
Message-ID: <e19cad5bff20c3da2dcd9bb876596c4c@sbcglobal.net>

I want to put warning messages into some of my functions to remind 
myself to change the output file names each time I run them. Is there a 
way to do this that will allow me to respond to the warning message 
(i.e. "continue function" or "cancel")?

Thanks,
   Matt


From rgazaffi at carpa.ciagri.usp.br  Sun Jun 25 16:48:57 2006
From: rgazaffi at carpa.ciagri.usp.br (Rodrigo Gazaffi)
Date: Sun, 25 Jun 2006 11:48:57 -0300
Subject: [R] help with contrasts.
Message-ID: <200606251449.k5PEnpuI011727@hypatia.math.ethz.ch>

Hello list,

I would like to know how can I use contrats in statistical analysis. I
have a simple model, "one-way analysis of variance" "or two-way..."
(balanced case).

I've already used the command aov (model) to get  the analysis of
variance, but i know that my treatment could be organized in groups,
so i would like to test these groups.

Example: I have 4 treatments t1, t2, t3, t4 and i would like to test
(1, 1, -1, -1) and (1, -1, 0, 0) and (0, 0, 1, -1).

thanks for the attention.

Rodrigo Gazaffi


From ccatj at web.de  Sun Jun 25 17:00:44 2006
From: ccatj at web.de (Christian Jones)
Date: Sun, 25 Jun 2006 17:00:44 +0200
Subject: [R] hier.part function???
Message-ID: <624115685@web.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060625/6283f61d/attachment.pl 

From ggrothendieck at gmail.com  Sun Jun 25 17:06:23 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 25 Jun 2006 11:06:23 -0400
Subject: [R] warning messages
In-Reply-To: <e19cad5bff20c3da2dcd9bb876596c4c@sbcglobal.net>
References: <e19cad5bff20c3da2dcd9bb876596c4c@sbcglobal.net>
Message-ID: <971536df0606250806v26836692qc445a7b7d1820e38@mail.gmail.com>

Try this:

f <- function() {
	cat("c to continue, anything else to quit: ")
	x <- readline()
	if (x != "c") stop("User forced exit")
	# rest of function
}

f()

On 6/25/06, Matthew Bridgman <m.bridgman at sbcglobal.net> wrote:
> I want to put warning messages into some of my functions to remind
> myself to change the output file names each time I run them. Is there a
> way to do this that will allow me to respond to the warning message
> (i.e. "continue function" or "cancel")?
>
> Thanks,
>   Matt
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Sun Jun 25 17:25:25 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 25 Jun 2006 11:25:25 -0400
Subject: [R] warning messages
In-Reply-To: <971536df0606250806v26836692qc445a7b7d1820e38@mail.gmail.com>
References: <e19cad5bff20c3da2dcd9bb876596c4c@sbcglobal.net>
	<971536df0606250806v26836692qc445a7b7d1820e38@mail.gmail.com>
Message-ID: <971536df0606250825u1fb43bc7r6dbad789d6c8862a@mail.gmail.com>

One small improvement, the cat can be eliminated:

f <- function() {
       x <- readline("c to continue, anything else to quit: ")
       if (x != "c") stop("User forced exit")
       # rest of function
}

f()



On 6/25/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Try this:
>
> f <- function() {
>        cat("c to continue, anything else to quit: ")
>        x <- readline()
>        if (x != "c") stop("User forced exit")
>        # rest of function
> }
>
> f()
>
> On 6/25/06, Matthew Bridgman <m.bridgman at sbcglobal.net> wrote:
> > I want to put warning messages into some of my functions to remind
> > myself to change the output file names each time I run them. Is there a
> > way to do this that will allow me to respond to the warning message
> > (i.e. "continue function" or "cancel")?
> >
> > Thanks,
> >   Matt
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>


From thierry.girard at unibas.ch  Sun Jun 25 18:02:04 2006
From: thierry.girard at unibas.ch (Thierry Girard)
Date: Sun, 25 Jun 2006 17:02:04 +0100
Subject: [R] analyze summary data
Message-ID: <E866C4CD-74DC-4F3F-8E40-3E8F14B9F918@unibas.ch>

Hello!

I would like to ask the list a question, as I could not find the  
answer in online material as well as several books.

I do have summary data (mean, standard deviation and sample size n)  
and want to analyze this data.
The summary data is supposed to be from a normal distribution.

I need the following calculations on this summary data (no, I do not  
have the original data):

- one sample t-test against a known mu
- two sample t-test
- analysis of variance between 4 groups.

I would appreciate any help available.

One possible solution could be to simulate the data using rnorm with  
the appropriate n, mu and sd, but I don't know if there would be a  
more accurate solution.

thanks in advance

Thierry Girard
thierry.girard at unibas.ch

using Version 2.2.1  (2005-12-20 r36812) on a Mac OS X 10.4.6


From bolker at ufl.edu  Sun Jun 25 19:13:46 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Sun, 25 Jun 2006 17:13:46 +0000 (UTC)
Subject: [R] analyze summary data
References: <E866C4CD-74DC-4F3F-8E40-3E8F14B9F918@unibas.ch>
Message-ID: <loom.20060625T190408-80@post.gmane.org>

Thierry Girard <thierry.girard <at> unibas.ch> writes:

> I do have summary data (mean, standard deviation and sample size n)  
> and want to analyze this data.
> The summary data is supposed to be from a normal distribution.
> 
> I need the following calculations on this summary data (no, I do not  
> have the original data):
> 
> - one sample t-test against a known mu
> - two sample t-test
> - analysis of variance between 4 groups.
> 
> I would appreciate any help available.
> 
> One possible solution could be to simulate the data using rnorm with  
> the appropriate n, mu and sd, but I don't know if there would be a  
> more accurate solution.


  this is the kind of situation where you need to go back to the basics --
knowing what computations these statistical tests are _actually
doing_ -- which you should be able to find in any basic stats book, 
or by digging
into the guts of the R functions.  The only other thing you need to
know is the R functions for cumulative distribution functions, pt
(for the t distribution) and pf (for the F dist.)

  For example:

   stats:::t.test.default

 has lots of complicated stuff inside but the key lines are
(for a one sample test)

 nx <- length(x)
  df <- nx - 1
  stderr <- sqrt(vx/nx)
  # if you already have the standard deviation then you want
  # sqrt(sd^2/nx)
 tstat <- (mx - mu)/stderr   ## mu is the known mean you're testing against
 pval <- 2 * pt(-abs(tstat), df)

(assuming 2-tailed)

  you will find similar stuff for the two-sample t-test,
depending on your particular choices.

  The 1-way ANOVA might be harder to dig out of the R code;
there you're better off going back and (re)learning from
a basic stats treatment how to
compute the between-group and (pooled) within-group variances.

  Bottom line is that, except for knowing about pt and pf,
this is really a basic statistics question rather than an
R question.

  good luck
    Ben Bolker

PS: it is too bad, but the increasing sophistication of R is
making it harder for beginners to explore the guts --- e.g.
knowing to look for "stats:::t.test.default" in order to find
the code ...


From jfox at mcmaster.ca  Sun Jun 25 19:19:27 2006
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 25 Jun 2006 13:19:27 -0400
Subject: [R] help with contrasts.
In-Reply-To: <200606251449.k5PEnpuI011727@hypatia.math.ethz.ch>
Message-ID: <web-130284472@cgpsrv2.cis.mcmaster.ca>

Dear Rodrigo,

One way to do what you want is by

contrasts(f) <- matrix(
  c(1, 1, -1, -1,   1, -1, 0, 0,   0, 0, 1, -1), 
  ncol=3)

See ?contrasts for details.

I hope this helps,
 John

On Sun, 25 Jun 2006 11:48:57 -0300
 Rodrigo Gazaffi <rgazaffi at carpa.ciagri.usp.br> wrote:
> Hello list,
> 
> I would like to know how can I use contrats in statistical analysis.
> I
> have a simple model, "one-way analysis of variance" "or two-way..."
> (balanced case).
> 
> I've already used the command aov (model) to get  the analysis of
> variance, but i know that my treatment could be organized in groups,
> so i would like to test these groups.
> 
> Example: I have 4 treatments t1, t2, t3, t4 and i would like to test
> (1, 1, -1, -1) and (1, -1, 0, 0) and (0, 0, 1, -1).
> 
> thanks for the attention.
> 
> Rodrigo Gazaffi
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/


From bdavenhall at sbcglobal.net  Sun Jun 25 20:28:46 2006
From: bdavenhall at sbcglobal.net (Brian Davenhall)
Date: Sun, 25 Jun 2006 11:28:46 -0700
Subject: [R] Error in model.frame: invalid variable type
Message-ID: <200606251828.k5PISmr2026541@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060625/723cf412/attachment.pl 

From ggrothendieck at gmail.com  Sun Jun 25 20:37:36 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 25 Jun 2006 14:37:36 -0400
Subject: [R] Error in model.frame: invalid variable type
In-Reply-To: <200606251828.k5PISmr2026541@hypatia.math.ethz.ch>
References: <200606251828.k5PISmr2026541@hypatia.math.ethz.ch>
Message-ID: <971536df0606251137i78c80d5egae74aaf99daca7e4@mail.gmail.com>

The names of your variables are Response and Seed, not response
and seed.  The latter two are the names of your data frames.

On 6/25/06, Brian Davenhall <bdavenhall at sbcglobal.net> wrote:
> I'm getting the following error when trying to execute a glm() procedure:
>
>
>
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  :
>        invalid variable type for 'response'
>
>
>
> The commands I'm using to import the *.csv files, construct the data frame,
> and run the glm procedure are below:
>
>
>
> response <- read.csv("Response.csv", header = TRUE, sep = ",")
>
> seed <- read.csv("Seed.csv", header = TRUE, sep = ",")
>
> data <- data.frame(seed, response)
>
> model <- glm(response~seed, family=binomial(link=logit), data=data)
>
>
>
> My Response.csv files look like this:
>
>
>
> Response
>
> 0
>
> 0
>
> 1
>
> ...
>
>
>
> My Seed.csv files look like this:
>
>
>
> Seed
>
> 0.25
>
> 0.65
>
> 0.47
>
> ....
>
>
>
> Thanks in advance for any help,
>
> Brian
>
>
>
>
>
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ripley at stats.ox.ac.uk  Sun Jun 25 20:58:07 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 25 Jun 2006 19:58:07 +0100 (BST)
Subject: [R] Error in model.frame: invalid variable type
In-Reply-To: <200606251828.k5PISmr2026541@hypatia.math.ethz.ch>
References: <200606251828.k5PISmr2026541@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0606251947520.32546@gannet.stats.ox.ac.uk>

What are the names of 'data?' It looks like you intended the response to 
be data$Response and not the object response found in your workspace.
If so, you need to capitalize your formula.

(And what version of R is this? -- R-devel actually tells you the type in 
this error message, so we know it is not that.)

BTW, 'data' is an R system object and you really should avoid using the 
name for your own data frames.  Normally R works out what you mean, but 
this can be cause of hard-to-find errors.

On Sun, 25 Jun 2006, Brian Davenhall wrote:

> I'm getting the following error when trying to execute a glm() procedure:
>
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  :
>        invalid variable type for 'response'
>
> The commands I'm using to import the *.csv files, construct the data frame,
> and run the glm procedure are below:
>
> response <- read.csv("Response.csv", header = TRUE, sep = ",")
>
> seed <- read.csv("Seed.csv", header = TRUE, sep = ",")
>
> data <- data.frame(seed, response)
>
> model <- glm(response~seed, family=binomial(link=logit), data=data)
>
>
>
> My Response.csv files look like this:
>
>
>
> Response
>
> 0
>
> 0
>
> 1
>
> ...
>
>
>
> My Seed.csv files look like this:
>
>
>
> Seed
>
> 0.25
>
> 0.65
>
> 0.47
>
> ....
>
>
>
> Thanks in advance for any help,
>
> Brian
>
>
>
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Mike.Prager at noaa.gov  Sun Jun 25 21:19:08 2006
From: Mike.Prager at noaa.gov (Michael Prager)
Date: Sun, 25 Jun 2006 15:19:08 -0400
Subject: [R] warning messages
In-Reply-To: <e19cad5bff20c3da2dcd9bb876596c4c@sbcglobal.net>
References: <e19cad5bff20c3da2dcd9bb876596c4c@sbcglobal.net>
Message-ID: <449EE1AC.4000407@noaa.gov>

You might consider passing the output file name to the function as an 
argument.


Matthew Bridgman wrote on 6/25/2006 10:49 AM:
> I want to put warning messages into some of my functions to remind 
> myself to change the output file names each time I run them. Is there a 
> way to do this that will allow me to respond to the warning message 
> (i.e. "continue function" or "cancel")?
>
> Thanks,
>    Matt
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>   

-- 
Michael H. Prager, Ph.D.
Population Dynamics Team
NOAA Center for Coastal Habitat and Fisheries Research
NMFS Southeast Fisheries Science Center
Beaufort, North Carolina  28516  USA
http://shrimp.ccfhrb.noaa.gov/~mprager/


From vdemart1 at tin.it  Sun Jun 25 23:32:42 2006
From: vdemart1 at tin.it (vittorio)
Date: Sun, 25 Jun 2006 21:32:42 +0000
Subject: [R] Fighting with GDD & lwd
Message-ID: <200606252132.43607.vdemart1@tin.it>

I succeeded in compiling and installing the GDD package under my FreeBSD 6.1 
box after eliminating 

ifdef DEBUG
? ?PKG_CFLAGS+=-DJGD_DEBUG
endif

in Makevars.in

Now, with the following code

..............................................
grafico1<- function() {
  plot(1:length(tabella[,1]),tabella[,2],ylim=c(Ymin,Ymax),type="h", 
    lwd=6,col="red",main=paste("TEMPERATURE MEDIE GIORNALIERE\nMESE DI", 
toupper(format(ierid,"%B")),sep=" "),
    col.main="dark blue", xlab="Giorni del mese",ylab="?C", 
col.lab="blue",axes=FALSE)
  lines(1:length(tabella[,1]),tabella[,3],type="l",lwd=2,col="dark green") 
  legend(1,Ymax,c(substring(FineMeseAnnoPrima,1,4),substring(Ieri,1,4)), 
col=c("dark green","red"),lty=c(1,1),lwd=4,bty="n")
  axis(1,at=1:length(tabella[,1]),lab=1:length(tabella[,1]),col="brown4", 
col.axis="brown4")
  axis(2,at=seq(Ymin,Ymax,by=2),lab=seq(Ymin,Ymax,by=2), 
col="brown4",col.axis="brown4",las=2)
  axis(4,at=seq(Ymin,Ymax,by=2),lab=seq(Ymin,Ymax,by=2), 
col="brown4",col.axis="brown4",las=2)
  grid(NA,ny=NULL,col="pink",lty="dotted")
	}
#
#
GDD(file="temperature",type="png",width=600,height=480)
grafico1()
dev.off()

 While under X11 grafico1() produces  *** thick ***  red histograms  due to 
the lwd=6 (and col="red", of course) option in the plot command, under GDD the 
png (but also gif and jpg) results in a graph with very thin histograms as if 
the lwd option doesn't exert any influence at all (I checked also with 
lwd=160 to no avail).

What should I do?

Ciao
Vittorio


From tolga at coubros.com  Sun Jun 25 23:11:18 2006
From: tolga at coubros.com (Tolga Uzuner)
Date: Sun, 25 Jun 2006 22:11:18 +0100
Subject: [R] Generating Correlated Random Variables
Message-ID: <449EFBF6.4050406@coubros.com>

Dear Fellow R Users,

If I have an NxN correlation matrix and want to generate N correlated 
random variables N(0,1), is there a simply package in R I can use ?

Thanks,
Tolga


From Dimitris.Rizopoulos at med.kuleuven.be  Sun Jun 25 23:31:39 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Sun, 25 Jun 2006 23:31:39 +0200
Subject: [R] Generating Correlated Random Variables
In-Reply-To: <449EFBF6.4050406@coubros.com>
References: <449EFBF6.4050406@coubros.com>
Message-ID: <1151271099.449f00bb94aa9@webmail2.kuleuven.be>

look at: 

help("mvrnorm", package = "MASS")
help("rmnorm", package = "mnormt")
help("rmvnorm", package = "mvtnorm")

I hope it helps.

Best,
Dimitris

---- 
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting Tolga Uzuner <tolga at coubros.com>:

> Dear Fellow R Users,
> 
> If I have an NxN correlation matrix and want to generate N correlated
> 
> random variables N(0,1), is there a simply package in R I can use ?
> 
> Thanks,
> Tolga
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From matthew.fulcher at zomojo.com  Mon Jun 26 08:40:34 2006
From: matthew.fulcher at zomojo.com (Matthew Fulcher)
Date: Mon, 26 Jun 2006 16:40:34 +1000
Subject: [R] C stack usage is too close to the limit
Message-ID: <449F8162.9010506@zomojo.com>


I am getting a seg fault and the following error when trying to use an 
embedded version of R in a multithreaded application.

Error: C stack usage is too close to the limit

When I do the same thing in a single threaded app, then there is no problem.
I've tried this with R version 2.3.0, 2.3.1 and even 2.2.1 and the same 
error occurs.

Any tips on how to solve this would be greatly appreciated...


From pzanis at geol.uoa.gr  Mon Jun 26 09:15:08 2006
From: pzanis at geol.uoa.gr (Prodromos Zanis)
Date: Mon, 26 Jun 2006 10:15:08 +0300 (EEST)
Subject: [R] About filled.contour function
Message-ID: <62977.87.203.235.112.1151306108.squirrel@webmail.uoa.gr>

Dear R-projects users

I would like like to ask if there is any way  to produce a multipanel plot
with the filled.contour function. In the help information of filled
contour it is said that this function is restricted to a full page
display.

With kind regards

Prodromos Zanis




-- 
****************************************************
Dr. Prodromos Zanis
Centre for Atmospheric Physics and Climatology
Academy of Athens
3rd September 131, Athens 11251, Greece
Tel. +30 210 8832048
Fax: +30 210 8832048
e-mail: pzanis at geol.uoa.gr
Web address: http://users.auth.gr/~zanis/


From maechler at stat.math.ethz.ch  Mon Jun 26 10:06:21 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 26 Jun 2006 10:06:21 +0200
Subject: [R] Inverting a large Matrix   (14000 x 14000)
In-Reply-To: <20060625081434.53896.qmail@web34001.mail.mud.yahoo.com>
References: <20060625081434.53896.qmail@web34001.mail.mud.yahoo.com>
Message-ID: <17567.38269.167555.376714@stat.math.ethz.ch>

>>>>> "Cal" == Cal Stats <calstats05 at yahoo.com>
>>>>>     on Sun, 25 Jun 2006 01:14:34 -0700 (PDT) writes:

    Cal> Hi..  I have to invert a 15000 x 15000 matrix
    Cal> (generalized inverse). I do run the process on a fairly
    Cal> powerful computer. but still complains indufficient
    Cal> memory.
  
    Cal>   Is there a way one can invert a large matrix in some
    Cal> other efficient manner.

Is the matrix sparse, i.e., has most of its entries == 0 ?
If yes, then there are more efficient ways, notably in packages
'SparseM' and 'Matrix'.
  
    Cal>   Thanks Harsh


From j.van_den_hoff at fz-rossendorf.de  Mon Jun 26 10:27:48 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Mon, 26 Jun 2006 10:27:48 +0200
Subject: [R] About filled.contour function
In-Reply-To: <62977.87.203.235.112.1151306108.squirrel@webmail.uoa.gr>
References: <62977.87.203.235.112.1151306108.squirrel@webmail.uoa.gr>
Message-ID: <449F9A84.1060309@fz-rossendorf.de>

Prodromos Zanis wrote:
> Dear R-projects users
> 
> I would like like to ask if there is any way  to produce a multipanel plot
> with the filled.contour function. In the help information of filled
> contour it is said that this function is restricted to a full page
> display.
> 
> With kind regards
> 
> Prodromos Zanis
> 
> 
> 
> 
`filled.contour' sets up a 2-by-1 grid (for colormap and image) using 
the `layout' function, hence a prior setup of a multipanel layout would 
be (and is) destroyed by the `filled.contour' call.

you might edit a copy of `filled contour' to modify the `layout' call 
(e.g. to set up a 2-by-2 grid where only the upper row is used by the 
plots generated in filled contour). I tried this very shortly before 
answering but it seems that only 'within' the filled.contour copy one 
has access to the other subplots, after return from the function plot 
focus is again in subplot no. one --no time to check this further. in 
any case  along this line one should be able to enforce multiple 
subplots where 2 of them are used by the modified `filled.contour'

joerg


From bhs2 at mevik.net  Mon Jun 26 10:42:37 2006
From: bhs2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Mon, 26 Jun 2006 10:42:37 +0200
Subject: [R] problem with "code/documentation mismatch"
In-Reply-To: <449BE23A.6060609@fz-rossendorf.de> (Joerg van den Hoff's
	message of "Fri, 23 Jun 2006 14:44:42 +0200")
References: <449BE23A.6060609@fz-rossendorf.de>
Message-ID: <m0r71cfgua.fsf@bar.nemo-project.org>

Just an idea:  how about using the \usage for the formal syntax, and
\synopsis for the "user syntax", i.e. x/y ?

Not sure it will work, but it might be worth a try... :-)

-- 
Bj?rn-Helge Mevik


From jtk at cmp.uea.ac.uk  Mon Jun 26 10:56:40 2006
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Mon, 26 Jun 2006 09:56:40 +0100
Subject: [R] PowerPoint - eps not suitable
In-Reply-To: <1151088234.3828.97.camel@localhost.localdomain>
References: <2323A6D37908A847A7C32F1E3662C80E132136@dc1ex01.air.org>
	<971536df0606230621t1584d4aer7cd4778978600fd0@mail.gmail.com>
	<20060623161649.GA5532@gsf.de>
	<1151080021.3828.42.camel@localhost.localdomain>
	<20060623173958.GA3518@gsf.de> <449C2CC9.8080002@noaa.gov>
	<1151088234.3828.97.camel@localhost.localdomain>
Message-ID: <20060626085640.GA2490@jtkpc.cmp.uea.ac.uk>

On Fri, Jun 23, 2006 at 01:43:54PM -0500, Marc Schwartz (via MN) wrote:
> On Fri, 2006-06-23 at 14:02 -0400, Michael H. Prager wrote:
> > Previous posters have argued for EPS files as a desirable transfer 
> > format for quality reasons.  This is of course true when the output is 
> > through a Postscript device.
> > 
> > However, the original poster is making presentations with PowerPoint.  
> > Those essentially are projected from the screen -- and screens of 
> > Windows PCs are NOT Postscript devices.  The version of PowerPoint I 
> > have will display a bitmapped, low-resolution preview when EPS is 
> > imported, and that is what will be projected.  It is passable, but much 
> > better can be done!
> > 
> > In this application, I have had best results using cut and paste or the 
> > Windows metafile format, both of which (as others have said) give 
> > scalable vector graphics.  When quirks of Windows metafile arise (as 
> > they can do, especially when fonts differ between PCs), I have had good 
> > results with PNG for line art and JPG for other art.
> > 
> > Mike
> 
> Just so that it is covered (though this has been noted in other
> threads), even in this situation, one can still use EPS files embedded
> in PowerPoint (or Impress) presentations.

Just to cover yet another possible route, I've in the past used ghostscript
to produce high resolution (or, more generally, whatever resolution was
required) raster images from PostScript by something like

    gs -r150x150 -sDEVICE=bmp256 -sOutputFile=x.bmp -dNOPAUSE myfile.ps -c quit

Apologies if this has been mentioned in this thread already.

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |             email: jtk at cmp.uea.ac.uk                               |
 |             WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*


From maechler at stat.math.ethz.ch  Mon Jun 26 11:34:30 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 26 Jun 2006 11:34:30 +0200
Subject: [R] getting the smoother matrix from smooth.spline
In-Reply-To: <20060624184137.81438.qmail@web31201.mail.mud.yahoo.com>
References: <20060624184137.81438.qmail@web31201.mail.mud.yahoo.com>
Message-ID: <17567.43558.65224.353865@stat.math.ethz.ch>

>>>>> "Gregory" == Gregory Gentlemen <gregory_gentlemen at yahoo.ca>
>>>>>     on Sat, 24 Jun 2006 14:41:37 -0400 (EDT) writes:


    Gregory> Can anyone tell me the trick for obtaining the
    Gregory> smoother matrix from smooth.spline when there are
    Gregory> non-unique values for x. I have the following code
    Gregory> but, of course, it only works when all values of x
    Gregory> are unique.

   >>    ## get the smoother matrix (x having unique values
   >>    smooth.matrix = function(x, df){
   >>      n = length(x);
   >>      A = matrix(0, n, n);
   >>      for(i in 1:n){
   >>        y = rep(0, n); y[i]=1;
   >>        yi = smooth.spline(x, y, df=df)$y;
   >>        A[,i]= yi;
   >>      }
   >>      (A+t(A))/2;
   >>    }

{ All the extraneous ";" at the end of lines make the above
  unncessarily ugly  (and potentially even slightly inefficient).}
   
Package 'sfsmisc' has had a  function  hatMat()  which returns
the hat matrix aka smoother matrix, in a slightly more general
way -- you can use it also for other smoothers, see the examples
in  help(hatMat).  The smoother defaults to smooth.spline(), so
you can directly use it, e.g.,
    hatMat(x, df=5)

Why do you think your code or hatMat()  would not work for
non-unique x-values?


    Gregory> Thanks for any assistance,
    Gregory> Gregory

    Gregory> ---------------------------------

    Gregory> [[alternative HTML version deleted]]
	     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

[please do read the posting guide, and hence get rid of the line above ! ]

    Gregory> ______________________________________________
    Gregory> R-help at stat.math.ethz.ch mailing list
    Gregory> https://stat.ethz.ch/mailman/listinfo/r-help
    Gregory> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ggrothendieck at gmail.com  Mon Jun 26 12:33:27 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 26 Jun 2006 06:33:27 -0400
Subject: [R] PowerPoint - eps not suitable
In-Reply-To: <20060626085640.GA2490@jtkpc.cmp.uea.ac.uk>
References: <2323A6D37908A847A7C32F1E3662C80E132136@dc1ex01.air.org>
	<971536df0606230621t1584d4aer7cd4778978600fd0@mail.gmail.com>
	<20060623161649.GA5532@gsf.de>
	<1151080021.3828.42.camel@localhost.localdomain>
	<20060623173958.GA3518@gsf.de> <449C2CC9.8080002@noaa.gov>
	<1151088234.3828.97.camel@localhost.localdomain>
	<20060626085640.GA2490@jtkpc.cmp.uea.ac.uk>
Message-ID: <971536df0606260333m5df2ead0h59bd146fb184dc86@mail.gmail.com>

On 6/26/06, Jan T. Kim <jtk at cmp.uea.ac.uk> wrote:
> On Fri, Jun 23, 2006 at 01:43:54PM -0500, Marc Schwartz (via MN) wrote:
> > On Fri, 2006-06-23 at 14:02 -0400, Michael H. Prager wrote:
> > > Previous posters have argued for EPS files as a desirable transfer
> > > format for quality reasons.  This is of course true when the output is
> > > through a Postscript device.
> > >
> > > However, the original poster is making presentations with PowerPoint.
> > > Those essentially are projected from the screen -- and screens of
> > > Windows PCs are NOT Postscript devices.  The version of PowerPoint I
> > > have will display a bitmapped, low-resolution preview when EPS is
> > > imported, and that is what will be projected.  It is passable, but much
> > > better can be done!
> > >
> > > In this application, I have had best results using cut and paste or the
> > > Windows metafile format, both of which (as others have said) give
> > > scalable vector graphics.  When quirks of Windows metafile arise (as
> > > they can do, especially when fonts differ between PCs), I have had good
> > > results with PNG for line art and JPG for other art.
> > >
> > > Mike
> >
> > Just so that it is covered (though this has been noted in other
> > threads), even in this situation, one can still use EPS files embedded
> > in PowerPoint (or Impress) presentations.
>
> Just to cover yet another possible route, I've in the past used ghostscript
> to produce high resolution (or, more generally, whatever resolution was
> required) raster images from PostScript by something like
>
>    gs -r150x150 -sDEVICE=bmp256 -sOutputFile=x.bmp -dNOPAUSE myfile.ps -c quit
>
> Apologies if this has been mentioned in this thread already.
>
> Best regards, Jan
> --
>  +- Jan T. Kim -------------------------------------------------------+
>  |             email: jtk at cmp.uea.ac.uk                               |
>  |             WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
>  *-----=<  hierarchical systems are for files, not for humans  >=-----*

Useful in this regard on Windows XP, is
  withgs.bat
in batchfiles.  It  will locate ghostscript on your system (using the
registry), temporarily add it to your path and then run its argument
as a command.
withgs.bat can be placed anywhere in your path.  Google for:
  CRAN batchfiles


From tolga at coubros.com  Mon Jun 26 12:35:51 2006
From: tolga at coubros.com (Tolga Uzuner)
Date: Mon, 26 Jun 2006 11:35:51 +0100
Subject: [R] Function with upper/lower bound, shift,slope,curvature
Message-ID: <449FB887.5050301@coubros.com>

Dear R Users,

I am looking for a function to use within optim s.t.
- f(x,a,b,c,d,e) where I am searching over parameter values a,b,c,d,e
- a is the lower bound (typically 1)
- b is the upper bound (typically 0)
- c,d,e give the function a shift, slope and curvature between a and b

Any good candidates for a non-linear optimisation problem known, either 
already built into R, or otherwise.

A sigmoid sort of gets half there, but I need some more control over the 
curvature and upper/lower bounds of the function.

sigm<-function(x,a,t) 0.5*(1+(1-exp((x-a)/t))/(1+exp((x-a)/t)))

Thanks in advance,
Tolga


From ripley at stats.ox.ac.uk  Mon Jun 26 13:00:39 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 26 Jun 2006 12:00:39 +0100 (BST)
Subject: [R] assign / environment side effect on R 2.4.0
In-Reply-To: <449C0A4E.9040203@hhbio.wasser.tu-dresden.de>
References: <449BFB16.6010501@TU-Dresden.de>
	<449C0A4E.9040203@hhbio.wasser.tu-dresden.de>
Message-ID: <Pine.LNX.4.64.0606261120370.23616@gannet.stats.ox.ac.uk>

Please do study the posting guide, as there is no `R 2.4.0'.

This seems to be related to the NEWS item

     o	[[ on a list does not duplicate the extracted element unless
 	necessary.  (It did not duplicate in other cases, e.g. a
 	pairlist.)

Now in so far as I can follow it, in your example [[ behaves in the same 
way as $ always did (which is what informed people would expect).

R-help is not the place to discuss unreleased versions of R, nor for 
questions about code development.  (Did I mention studying the posting 
guide?)


On Fri, 23 Jun 2006, Thomas Petzoldt wrote:

> Sorry,
>
> the posted example had the side effect on all platforms (correctly: R
> 2.2.1/Windows, 2.3.1/Linux, 2.4.0/Windows), but in the following
> corrected example the behavior of 2.4.0 differs from the older versions.
>
> The only difference between the "wrong" and the "new" example is
> L[["test"]] vs. L$test in the assign.
>
> Thomas P.
>
>
> envfun <- function(L) {
> #  L <- as.list(unlist(L))
>  p <- parent.frame()
>  assign("test", L[["test"]], p) ## [["test"]] instead of $test
>  environment(p[["test"]]) <- p
> }
>
>
> solver <- function(L) {
>  envfun(L)
>  # some other stuff
> }
>
> L <- list(test = function() 1 + 2)
>
> e1 <- environment(L$test)
> solver(L)
> e2 <- environment(L$test)
>
> print(e1)
> print(e2)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From thl at dmu.dk  Mon Jun 26 13:52:54 2006
From: thl at dmu.dk (Larsen, Thomas)
Date: Mon, 26 Jun 2006 13:52:54 +0200
Subject: [R] NLS and fitting of x-values?
Message-ID: <4D0A38D7A770B04BA18311C6ABC0F394014E4394@dmuspost.dmu.dk>

I collected eggs laid by Springtails everyday over 28 days after swich to isotopically enriched diet. The eggs were pooled at day 7, 14, and 28 (+ day 0 = initial value) and analyzed for isotopes. After the diet switch the isotopic values of the adults and eggs change towards those of the new diet.
Here are the d13C values (y) of the eggs:

x      y 
0   -22.2
0   -22.2
0   -22.2
0   -22.0
7   486.9
7   498.6
7   489.6
14  820.9
14  817.4
28  895.6
28  900.7
28  890.6
28  885.8

The y values represent the mean of the sampling period.

The dataset is very small but previous experiments have shown that a exponential asymptotic model can be used for this kind of situations. 

How do I fit a model to these pooled values? The y values can be regarded as the mean of the given sampling period.

My first guess is that the x values should be in the middle of the collection period. I call these x-values xi:
xi=c(rep(0,4),rep(3.5,3),rep(10.5,2),rep(21,4))

If I fit them to a nonlinear regression model via least squares (NLS) I get the parameters:
       Value Std. Error  t value 
a 900.386000  3.7839900 237.9460
b 916.630000 29.3987000  31.1792
c   0.230811  0.0102677  22.4792

How do I procede from here? I should probably use a maximum likelihood estimate to estimate the fitted xi? 
Any help would be greatly appreciated.

-------------------------------------------
Thomas Larsen, PhD student
Department of Terrestrial Ecology
National Environmental Research Institute
Vejls?vej 25, P.O. Box 314, DK-8600 Silkeborg
Phone +45 8920 1572; Fax +45 8920 1414;


From p.dalgaard at biostat.ku.dk  Mon Jun 26 14:11:03 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Jun 2006 14:11:03 +0200
Subject: [R] NLS and fitting of x-values?
In-Reply-To: <4D0A38D7A770B04BA18311C6ABC0F394014E4394@dmuspost.dmu.dk>
References: <4D0A38D7A770B04BA18311C6ABC0F394014E4394@dmuspost.dmu.dk>
Message-ID: <x24py8t8vc.fsf@viggo.kubism.ku.dk>

"Larsen, Thomas" <thl at dmu.dk> writes:

> I collected eggs laid by Springtails everyday over 28 days after swich to isotopically enriched diet. The eggs were pooled at day 7, 14, and 28 (+ day 0 = initial value) and analyzed for isotopes. After the diet switch the isotopic values of the adults and eggs change towards those of the new diet.
> Here are the d13C values (y) of the eggs:
> 
> x      y 
> 0   -22.2
> 0   -22.2
> 0   -22.2
> 0   -22.0
> 7   486.9
> 7   498.6
> 7   489.6
> 14  820.9
> 14  817.4
> 28  895.6
> 28  900.7
> 28  890.6
> 28  885.8
> 
> The y values represent the mean of the sampling period.
> 
> The dataset is very small but previous experiments have shown that a exponential asymptotic model can be used for this kind of situations. 
> 
> How do I fit a model to these pooled values? The y values can be regarded as the mean of the given sampling period.
> 
> My first guess is that the x values should be in the middle of the collection period. I call these x-values xi:
> xi=c(rep(0,4),rep(3.5,3),rep(10.5,2),rep(21,4))
> 
> If I fit them to a nonlinear regression model via least squares (NLS) I get the parameters:
>        Value Std. Error  t value 
> a 900.386000  3.7839900 237.9460
> b 916.630000 29.3987000  31.1792
> c   0.230811  0.0102677  22.4792
> 
> How do I procede from here? I should probably use a maximum likelihood estimate to estimate the fitted xi? 
> Any help would be greatly appreciated.

My take is that you should just have your expected y values (which in
this case are also the eggs-values, but never mind...) modeled as what
they are, namely an integral under the curve between x[i-1] and x[i],
or for practical purposes take the sum over days (say) 15 to 28 and
divide by 14. 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From mb.atelier at web.de  Mon Jun 26 14:23:18 2006
From: mb.atelier at web.de (Matthias Braeunig)
Date: Mon, 26 Jun 2006 14:23:18 +0200
Subject: [R] reshaping data.frame question
Message-ID: <449FD1B6.9030104@web.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Dear R-helpers,

my data.frame is of the form

x <- data.frame( f=gl(4,3), X=rep(0:2,4), p=c(.1,.2,.3))
x
   f X   p
1  1 0 0.1
2  1 1 0.2
3  1 2 0.3
4  2 0 0.1
5  2 1 0.2
6  2 2 0.3
7  3 0 0.1
8  3 1 0.2
9  3 2 0.3
10 4 0 0.1
11 4 1 0.2
12 4 2 0.3

which  tabulates some values p(X) for several factors f.

Now I want to put it in "wide" format, so that factor levels appear as
column heads. Note also that X starts from zero. It would be nice if I
could simply access p_f[X==0] as f[0]. How can I possibly do that?

(The resilting object does not have to be a data.frame. As there are
only numeric values, also a matrix would do.)

I tried the following

y<-unstack(x,form=p~f)
row.names(y) <- 0:2
y
   X1  X2  X3  X4
0 0.1 0.1 0.1 0.1
1 0.2 0.2 0.2 0.2
2 0.3 0.3 0.3 0.3

Now, how to access X3[0], say?

Maybe reshape would be the right tool, but I could not figure it out.

I appreciate your help. Thanks!


-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.2.2 (GNU/Linux)

iD8DBQFEn9G2XjamRUP82DkRAorGAJ9JirG7WtNJLWRQkJvgW0zTFHTYagCgvONw
IC4jgoxE2+CsOmmogv5dzF0=
=24Kj
-----END PGP SIGNATURE-----


From jim at bitwrit.com.au  Tue Jun 27 04:44:31 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Mon, 26 Jun 2006 22:44:31 -0400
Subject: [R] New version of plotrix
Message-ID: <44A09B8F.2060109@bitwrit.com.au>

Hi folks,

I usually don't do version announcements, but several people have 
requested things that are in this version (2.1). Prominent among these 
are that soil.texture has inflated into yet another general triangle 
plot function (triax.plot), the dreaded shadow effect has materialized, 
pie3D has lost a couple of bugs and by a sequence of improbable 
coincidences another wind rose plot is airborne, this one in the 
Australian style. As always, please tell me where I've blown it, for 
your comments have often gotten me on a better tack.

Jim


From rab45+ at pitt.edu  Mon Jun 26 01:37:37 2006
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Sun, 25 Jun 2006 19:37:37 -0400
Subject: [R] lmer and mixed effects logistic regression
In-Reply-To: <449CC1B3.9040305@pdf.com>
References: <1150293574.3416.11.camel@localhost.localdomain>
	<449431F1.6000906@pdf.com>
	<40e66e0b0606180458p177fa824j6a181fd52c3d1277@mail.gmail.com>
	<1150726454.3200.20.camel@localhost.localdomain>
	<148ed8180606201127p730e699emb9e901b09a7ab039@mail.gmail.com>
	<1150894770.3329.13.camel@localhost.localdomain>
	<44996733.9050609@pdf.com>
	<1150930911.3329.42.camel@localhost.localdomain>
	<449CC1B3.9040305@pdf.com>
Message-ID: <1151278662.3366.23.camel@localhost.localdomain>

On Fri, 2006-06-23 at 21:38 -0700, Spencer Graves wrote:
> 	  Permit me to try to repeat what I said earlier a little more clearly: 
>   When the outcomes are constant for each subject, either all 0's or all 
> 1's, the maximum likelihood estimate of the between-subject variance in 
> Inf.  Any software that returns a different answer is wrong. This is NOT 
> a criticism of 'lmer' or SAS NLMIXED:  This is a sufficiently rare, 
> extreme case that the software does not test for it and doesn't handle 
> it well when it occurs.  Adding other explanatory variables to the model 
> only makes this problem worse, because anything that will produce 
> complete separation for each subject will produce this kind of 
> instability.
> 
> 	 Consider the following:
> 
> library(lme4)
> DF <- data.frame(y=c(0,0, 0,1, 1,1),
>                   Subj=rep(letters[1:3], each=2),
>                   x=rep(c(-1, 1), 3))
> fit1 <- lmer(y~1+(1|Subj), data=DF, family=binomial)
> 
> # 'lmer' works fine here, because the outcomes from
> # 1 of the 3 subjects is not constant.
> 
>  > fit.x <- lmer(y~x+(1|Subj), data=DF, family=binomial)
> Warning message:
> IRLS iterations for PQL did not converge
> 
> 	  The addition of 'x' to the model now allows complete separation for 
> each subject.  We see this in the result:
> 
> Generalized linear mixed model fit using PQL
> <snip>
> Random effects:
>   Groups Name        Variance   Std.Dev.
>   Subj   (Intercept) 3.5357e+20 1.8803e+10
> number of obs: 6, groups: Subj, 3
> 
> Estimated scale (compare to 1)  9.9414e-09
> 
> Fixed effects:
>                 Estimate  Std. Error    z value Pr(>|z|)
> (Intercept) -5.4172e-05  1.0856e+10  -4.99e-15        1
> x            8.6474e+01  2.7397e+07 3.1563e-06        1
> 
> 	  Note that the subject variance is 3.5e20, the estimate for x is 86 
> wit a standard error of 2.7e7.  All three of these numbers are reaching 
> for Inf;  lmer quit before it got there.
> 
> 	  Does this make any sense, or are we still misunderstanding one another?
> 
> 	  Hope this helps.
> 	  Spencer Graves
> 
Yes, thanks, it's clear. I had created a new data set that has each
subject with just one observation and randomly sampled one observation
from each subject with two observations (they are right and left eyes).
I'm not sure why lmer gives small estimated variances for the random
effects when it should be infinite. I ran NLMIXED on the original data
set with several explanatory factors and the variance component was in
the thousands.

I guess the moral is before you do any computations you have to make
sure the procedure makes sense for the data.

Rick B.


From jholtman at gmail.com  Mon Jun 26 14:58:58 2006
From: jholtman at gmail.com (jim holtman)
Date: Mon, 26 Jun 2006 08:58:58 -0400
Subject: [R] reshaping data.frame question
In-Reply-To: <449FD1B6.9030104@web.de>
References: <449FD1B6.9030104@web.de>
Message-ID: <644e1f320606260558r43b003fbw875e6c0d0987627e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060626/d3a23046/attachment.pl 

From sachinj.2006 at yahoo.com  Mon Jun 26 15:07:23 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Mon, 26 Jun 2006 06:07:23 -0700 (PDT)
Subject: [R] converting to time series object : ts - package:stats
Message-ID: <20060626130723.80794.qmail@web37614.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060626/047c99cc/attachment.pl 

From balajis at stanford.edu  Mon Jun 26 15:08:44 2006
From: balajis at stanford.edu (Balaji S. Srinivasan)
Date: Mon, 26 Jun 2006 06:08:44 -0700
Subject: [R] Patch for rgl with gcc 4.0 in R 2.3.0 on OS X
Message-ID: <39422c340606260608x673b1685kea250b477d510a60@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060626/3eb2a7f2/attachment.pl 

From murdoch at stats.uwo.ca  Mon Jun 26 15:17:49 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 26 Jun 2006 09:17:49 -0400
Subject: [R] Patch for rgl with gcc 4.0 in R 2.3.0 on OS X
In-Reply-To: <39422c340606260608x673b1685kea250b477d510a60@mail.gmail.com>
References: <39422c340606260608x673b1685kea250b477d510a60@mail.gmail.com>
Message-ID: <449FDE7D.5040605@stats.uwo.ca>

I think this has already been fixed.  The rgl repository had a disk 
crash and is being restored today, but you can see a recent build on my 
web page,

http://www.stats.uwo.ca/faculty/murdoch/software/

We are planning a new release very soon; the disk crash is a bit of a 
nuisance, though.

Duncan Murdoch

On 6/26/2006 9:08 AM, Balaji S. Srinivasan wrote:
> Hi,
> 
> I recently had a problem installing the rgl package on OS X and put together
> a simple patch. The patched package is available here:
> 
> http://jinome.stanford.edu/files/rgl_0.66-patched_for_gcc4.tar.gz
> 
> It can be installed with "R CMD INSTALL rgl_0.66-patched_for_gcc4.tar.gz" as
> normal at the command line.
> 
> Also -- as of right now rgl is not in the repository of version 2.3 packages
> (or at least did not come up when I ran new.packages() at the R prompt). I'm
> not sure if this build error was the problem, but if so this
> version now works perfectly. For those who are interested, I have included
> details on what exactly went wrong and the fix:
> 
> 1) What went wrong -- here is the output of the initial build attempt. The
> install fails because of an invalid integer conversion, which I eventually
> figured out was
> due to gcc-4 flagging it as an error, whereas gcc-3 and earlier would have
> called it a warning.
> 
> [root:/root/downloads/rgl_patch]$R CMD INSTALL rgl_0.66.tar.gz
> * Installing *source* package 'rgl' ...
> checking for libpng-config... yes
> configure: using libpng-config
> configure: using libpng dynamic linkage
> configure: creating ./config.status
> config.status: creating src/Makevars
> ** libs
> ** arch - i386
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c BBoxDeco.cpp -o BBoxDeco.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c Background.cpp -o Background.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c Color.cpp -o Color.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c Disposable.cpp -o Disposable.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c FaceSet.cpp -o FaceSet.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c Light.cpp -o Light.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c LineSet.cpp -o LineSet.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c LineStripSet.cpp -o LineStripSet.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c Material.cpp -o Material.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c PointSet.cpp -o PointSet.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c PrimitiveSet.cpp -o PrimitiveSet.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c QuadSet.cpp -o QuadSet.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c RenderContext.cpp -o RenderContext.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c Shape.cpp -o Shape.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c SphereMesh.cpp -o SphereMesh.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c SphereSet.cpp -o SphereSet.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c SpriteSet.cpp -o SpriteSet.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c String.cpp -o String.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c Surface.cpp -o Surface.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c TextSet.cpp -o TextSet.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c Texture.cpp -o Texture.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c TriangleSet.cpp -o TriangleSet.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c Viewpoint.cpp -o Viewpoint.o
> g++-4.0 -arch i386 -I/Library/Frameworks/R.framework/Resources/include
> -I/Library/Frameworks/R.framework/Resources/include/i386 -DRGL_USE_CARBON
> -I/System/Library/Frameworks/AGL.framework/Headers -DHAVE_PNG_H
> -I/opt/local/include/libpng12 -msse3   -fPIC -fno-common  -g -O2
> -march=pentium-m -mtune=prescott -c api.cpp -o api.o
> api.cpp: In function 'void rgl_user2window(int*, int*, double*, double*,
> double*, double*, int*)':
> api.cpp:608: error: invalid conversion from 'int*' to 'const GLint*'
> api.cpp:608: error:   initializing argument 6 of 'GLint gluProject(GLdouble,
> GLdouble, GLdouble, const GLdouble*, const GLdouble*, const GLint*,
> GLdouble*, GLdouble*, GLdouble*)'
> api.cpp: In function 'void rgl_window2user(int*, int*, double*, double*,
> double*, double*, int*)':
> api.cpp:633: error: invalid conversion from 'int*' to 'const GLint*'
> api.cpp:633: error:   initializing argument 6 of 'GLint
> gluUnProject(GLdouble, GLdouble, GLdouble, const GLdouble*, const GLdouble*,
> const GLint*, GLdouble*, GLdouble*, GLdouble*)'
> make: *** [api.o] Error 1
> chmod: cannot access
> `/Library/Frameworks/R.framework/Versions/2.3/Resources/library/rgl/libs/i386/*':
> No such file or directory
> ERROR: compilation failed for package 'rgl'
> ** Removing
> '/Library/Frameworks/R.framework/Versions/2.3/Resources/library/rgl'
> 
> ---------------------
> 
> 2) The fix is below. It seems that gcc-4.0 does not want to convert an int*
> directly to a const GLint*, maybe because it might be possible to
> fool around with the underlying variable. To deal with this, I defined a
> GLint named viewgl and set it equal to the dereferenced value
> of the int* view. I then passed in the address of viewgl to the gluProject
> routine. This eliminated the error messages and allowed compilation.
> 
> 
> -----RELEVANT SECTION OF rgl/src/api.cpp BEFORE ---
> 
> void rgl_user2window(int* successptr, int* idata, double* point, double*
> pixel, double* model, double* proj, int* view)
> {
>   int success = RGL_FAIL;
>   GLdouble* vertex = pixel;
>   int columns = idata[0];
> 
>   Device* device = deviceManager->getAnyDevice();
> 
>   if ( device ) {
>         for (int i=0; i<columns; i++) {
>                 gluProject(point[0],point[1],point[2],model,proj,view,
>                 vertex,vertex+1,vertex+2);
>                 vertex[0] /= view[2];
>                 vertex[1] /= view[3];
>                 point += 3;
>                 vertex += 3;
>         }
>         success = RGL_SUCCESS;
>   }
> 
>   *successptr = success;
> }
> 
> void rgl_window2user(int* successptr, int* idata, double* point, double*
> pixel, double* model, double* proj, int* view)
> {
>   int success = RGL_FAIL;
>   GLdouble* vertex = point;
>   int columns = idata[0];
> 
>   Device* device = deviceManager->getAnyDevice();
> 
>   if ( device ) {
>         for (int i=0; i<columns; i++) {
>                 pixel[0] *= view[2];
>                 pixel[1] *= view[3];
>                 gluUnProject(pixel[0],pixel[1],pixel[2],model,proj,view,
>                 vertex,vertex+1,vertex+2);
>                 pixel += 3;
>                 vertex += 3;
>         }
>         success = RGL_SUCCESS;
>   }
> 
>   *successptr = success;
> }
> 
> 
> 
> ----RELEVANT SECTION OF rgl/src/api.cpp AFTER ----
> 
> void rgl_user2window(int* successptr, int* idata, double* point, double*
> pixel, double* model, double* proj, int* view)
> {
>   int success = RGL_FAIL;
>   GLdouble* vertex = pixel;
>   int columns = idata[0];
> 
>   //To fix gcc-4.0 error about "invalid conversion of int* to const GLint*,
> initialize
>   //a GLint variable viewgl and set it to the dereferenced value of view.
> This is now safe.
>   GLint viewgl = *view;
> 
>   Device* device = deviceManager->getAnyDevice();
> 
>   if ( device ) {
>         for (int i=0; i<columns; i++) {
>                 gluProject(point[0],point[1],point[2],model,proj,&viewgl,
> //pass address of viewgl
>                 vertex,vertex+1,vertex+2);
>                 vertex[0] /= view[2];
>                 vertex[1] /= view[3];
>                 point += 3;
>                 vertex += 3;
>         }
>         success = RGL_SUCCESS;
>   }
> 
>   *successptr = success;
> }
> 
> void rgl_window2user(int* successptr, int* idata, double* point, double*
> pixel, double* model, double* proj, int* view)
> {
>   int success = RGL_FAIL;
>   GLdouble* vertex = point;
>   int columns = idata[0];
> 
>   //To fix gcc-4.0 error about "invalid conversion of int* to const GLint*,
> initialize
>   //a GLint variable viewgl and set it to the dereferenced value of view.
> This is now safe.
>   GLint viewgl = *view;
> 
>   Device* device = deviceManager->getAnyDevice();
> 
>   if ( device ) {
>         for (int i=0; i<columns; i++) {
>                 pixel[0] *= view[2];
>                 pixel[1] *= view[3];
>                 gluUnProject(pixel[0],pixel[1],pixel[2],model,proj,&viewgl,
> //pass address of viewgl
>                 vertex,vertex+1,vertex+2);
>                 pixel += 3;
>                 vertex += 3;
>         }
>         success = RGL_SUCCESS;
>   }
> 
>   *successptr = success;
> }
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ripley at stats.ox.ac.uk  Mon Jun 26 15:25:41 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 26 Jun 2006 14:25:41 +0100 (BST)
Subject: [R] converting to time series object : ts - package:stats
In-Reply-To: <20060626130723.80794.qmail@web37614.mail.mud.yahoo.com>
References: <20060626130723.80794.qmail@web37614.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0606261421040.9143@gannet.stats.ox.ac.uk>

On Mon, 26 Jun 2006, Sachin J wrote:

>  I am trying to convert a dataset (dataframe) into time series object 
> using ts function in stats package. My dataset is as follows:
>
>  >df
>  [1] 11.08 7.08  7.08  6.08  6.08  6.08  23.08 32.08 8.08  11.08 6.08  13.08 13.83 16.83 19.83 8.83  20.83 17.83
>  [19] 9.83  20.83 10.83 12.83 15.83 11.83

No data frame will print like that, so it seems that your description and 
printout do not match.

>  I converted this into time series object as follows
>
>  >tsdata <-  ts((df),frequency = 12, start = c(1999, 1))

>From the help page for ts:

     data: a numeric vector or matrix of the observed time-series
           values. A data frame will be coerced to a numeric matrix via
           'data.matrix'.

I suspect you have a single-column data frame with a factor column.
Look up what data.matrix does for factors.

>  The resulting time series is as follows:
>
>       Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
> 1999   2  15  15  14  14  14  12  13  16   2  14   5
> 2000   6   8  10  17  11   9  18  11   1   4   7   3
>
>  I am unable to understand why the values of df and tsdata does not 
> match. I looked at ts function and I couldn't find any data 
> transformation. Am I missing something here? Any pointers would be of 
> great help.
>
>  Thanks in advance.
>
>  Sachin

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ajayshah at mayin.org  Sun Jun 25 15:33:37 2006
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Sun, 25 Jun 2006 19:03:37 +0530
Subject: [R] Puzzled with contour()
Message-ID: <20060625133337.GB24241@lubyanka.local>

Folks,

The contour() function wants x and y to be in increasing order. I have
a situation where I have a grid in x and y, and associated z values,
which looks like this:

              x   y     z
      [1,] 0.00  20 1.000
      [2,] 0.00  30 1.000
      [3,] 0.00  40 1.000
      [4,] 0.00  50 1.000
      [5,] 0.00  60 1.000
      [6,] 0.00  70 1.000
      [7,] 0.00  80 0.000
      [8,] 0.00  90 0.000
      [9,] 0.00 100 0.000
     [10,] 0.00 110 0.000
     [11,] 0.00 120 0.000
     [12,] 0.00 130 0.000
     [13,] 0.00 140 0.000
     [14,] 0.00 150 0.000
     [15,] 0.00 160 0.000
     [16,] 0.00 170 0.000
     [17,] 0.00 180 0.000
     [18,] 0.00 190 0.000
     [19,] 0.00 200 0.000
     [20,] 0.05  20 1.000
     [21,] 0.05  30 1.000
     [22,] 0.05  40 1.000
     [23,] 0.05  50 1.000
     [24,] 0.05  60 0.998
     [25,] 0.05  70 0.124
     [26,] 0.05  80 0.000
     [27,] 0.05  90 0.000
     [28,] 0.05 100 0.000
     [29,] 0.05 110 0.000
     [30,] 0.05 120 0.000
     [31,] 0.05 130 0.000
     [32,] 0.05 140 0.000
     [33,] 0.05 150 0.000
     [34,] 0.05 160 0.000
     [35,] 0.05 170 0.000
     [36,] 0.05 180 0.000
     [37,] 0.05 190 0.000
     [38,] 0.05 200 0.000
     [39,] 0.10  20 1.000
     [40,] 0.10  30 1.000

This looks like a nice case where both x and y are in increasing
order. But contour() gets unhappy saying that he wants x and y in
increasing order.

Gnuplot generates pretty 3d pictures from such data, where you are
standing above a surface and looking down at it. How does one do that
in R?

Any help will be most appreciated. A dput() of my data object is :

structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 
0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 
0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 
0.1, 0.1, 0.1, 0.1, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 
0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 
0.15, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 
0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 
0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 
0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 
0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.35, 
0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 
0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.4, 0.4, 0.4, 0.4, 
0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 
0.4, 0.4, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 
0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.5, 
0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 
0.5, 0.5, 0.5, 0.5, 0.5, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 
0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 
0.55, 0.55, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 
0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.65, 0.65, 0.65, 
0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 
0.65, 0.65, 0.65, 0.65, 0.65, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 
0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.75, 
0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 
0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.8, 0.8, 0.8, 0.8, 
0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 
0.8, 0.8, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 
0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.9, 
0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 
0.9, 0.9, 0.9, 0.9, 0.9, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 
0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 
0.95, 0.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 
150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 
100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 
40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 
180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 
130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 
80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 
20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 
160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 
110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 
50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 
190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 
140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 
90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 
30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 
170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 
120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 
60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 
190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 
140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 
90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 
30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 
170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 
120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 
60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 
190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 
140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 
90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 
30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 
170, 180, 190, 200, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 0, 0, 1, 1, 1, 1, 0.998, 0.124, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 1, 1, 1, 0.998, 0.71, 0.068, 0, 0, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0.998, 0.898, 0.396, 0.058, 0.002, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.998, 0.97, 0.726, 0.268, 
0.056, 0.006, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.996, 0.88, 
0.546, 0.208, 0.054, 0.012, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0.998, 0.964, 0.776, 0.418, 0.18, 0.054, 0.014, 0.002, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.998, 0.906, 0.664, 0.342, 
0.166, 0.056, 0.018, 0.006, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 0.986, 0.862, 0.568, 0.29, 0.15, 0.056, 0.022, 0.008, 0.002, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.954, 0.778, 0.494, 0.26, 0.148, 
0.056, 0.024, 0.012, 0.004, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0.906, 0.712, 0.43, 0.242, 0.144, 0.058, 0.028, 0.012, 0.006, 
0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.878, 0.642, 0.38, 0.222, 
0.142, 0.066, 0.034, 0.014, 0.008, 0.004, 0.002, 0, 0, 0, 0, 
0, 0, 0, 0, 0.846, 0.586, 0.348, 0.208, 0.136, 0.068, 0.034, 
0.016, 0.012, 0.006, 0.004, 0.002, 0, 0, 0, 0, 0, 0, 0, 0.8, 
0.538, 0.318, 0.204, 0.136, 0.07, 0.046, 0.024, 0.012, 0.008, 
0.004, 0.002, 0.002, 0, 0, 0, 0, 0, 0, 0.762, 0.496, 0.294, 0.2, 
0.138, 0.072, 0.05, 0.024, 0.014, 0.012, 0.006, 0.004, 0.002, 
0.002, 0, 0, 0, 0, 0, 0.704, 0.472, 0.286, 0.198, 0.138, 0.074, 
0.054, 0.028, 0.016, 0.012, 0.008, 0.006, 0.004, 0.002, 0.002, 
0, 0, 0, 0, 0.668, 0.438, 0.276, 0.196, 0.138, 0.078, 0.054, 
0.032, 0.024, 0.014, 0.012, 0.008, 0.004, 0.004, 0.002, 0.002, 
0, 0, 0, 0.634, 0.412, 0.27, 0.194, 0.14, 0.086, 0.056, 0.032, 
0.024, 0.016, 0.012, 0.01, 0.006, 0.004, 0.004, 0.002, 0.002, 
0, 0, 0.604, 0.388, 0.26, 0.19, 0.144, 0.088, 0.058, 0.048, 0.026, 
0.022, 0.014, 0.012, 0.008, 0.006, 0.004, 0.004, 0.002, 0.002, 
0, 0.586, 0.376, 0.256, 0.19, 0.146, 0.094, 0.062, 0.052, 0.028, 
0.024, 0.014, 0.012, 0.012, 0.008, 0.004, 0.004, 0.004, 0.002, 
0.002, 0.566, 0.364, 0.254, 0.192, 0.148, 0.098, 0.064, 0.054, 
0.032, 0.024, 0.022, 0.014, 0.012, 0.012, 0.008, 0.004, 0.004, 
0.004, 0.002), .Dim = c(399, 3), .Dimnames = list(NULL, c("x", 
"y", "z")))

--
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From sachinj.2006 at yahoo.com  Mon Jun 26 15:43:57 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Mon, 26 Jun 2006 06:43:57 -0700 (PDT)
Subject: [R] converting to time series object : ts - package:stats
In-Reply-To: <Pine.LNX.4.64.0606261421040.9143@gannet.stats.ox.ac.uk>
Message-ID: <20060626134357.74388.qmail@web37612.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060626/12532094/attachment.pl 

From Achim.Zeileis at wu-wien.ac.at  Mon Jun 26 15:31:37 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 26 Jun 2006 15:31:37 +0200 (CEST)
Subject: [R] converting to time series object : ts - package:stats
In-Reply-To: <20060626130723.80794.qmail@web37614.mail.mud.yahoo.com>
References: <20060626130723.80794.qmail@web37614.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.58.0606261525130.20027@thorin.ci.tuwien.ac.at>

On Mon, 26 Jun 2006, Sachin J wrote:

> Hi,
>
>   I am trying to convert a dataset (dataframe) into time series object
> using ts function in stats package. My dataset is as follows:
>
>   >df
>   [1] 11.08 7.08  7.08  6.08  6.08  6.08  23.08 32.08 8.08  11.08 6.08  13.08 13.83 16.83 19.83 8.83  20.83 17.83
>   [19] 9.83  20.83 10.83 12.83 15.83 11.83

Please provide a reproducible example. You just showed us the print output
for an object, claiming that it is an object of class "data.frame" which
is rather unlikely given the print output.

>   I converted this into time series object as follows
>
>   >tsdata <-  ts((df),frequency = 12, start = c(1999, 1))

which produces the right result for me if `df' is a vector or a
data.frame:

df <- c(11.08, 7.08, 7.08, 6.08, 6.08, 6.08, 23.08, 32.08, 8.08, 11.08,
        6.08, 13.08, 13.83, 16.83, 19.83, 8.83, 20.83, 17.83, 9.83, 20.83,
        10.83, 12.83, 15.83, 11.83)
ts(df, frequency = 12, start = c(1999, 1))
ts(as.data.frame(df), frequency = 12, start = c(1999, 1))

>   The resulting time series is as follows:
>
>        Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
> 1999   2  15  15  14  14  14  12  13  16   2  14   5
> 2000   6   8  10  17  11   9  18  11   1   4   7   3
>
>   I am unable to understand why the values of df and tsdata does not match.

So are we because you didn't really tell us enough about df...

Best,
Z

> I looked at ts function and I couldn't find any data transformation. Am
> I missing something here? Any pointers would be of great help.
>
>   Thanks in advance.
>
>   Sachin
>
>
> ---------------------------------
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From sachinj.2006 at yahoo.com  Mon Jun 26 15:55:13 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Mon, 26 Jun 2006 06:55:13 -0700 (PDT)
Subject: [R] converting to time series object : ts - package:stats
In-Reply-To: <Pine.LNX.4.58.0606261525130.20027@thorin.ci.tuwien.ac.at>
Message-ID: <20060626135513.82842.qmail@web37603.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060626/8508a4e8/attachment.pl 

From ggrothendieck at gmail.com  Mon Jun 26 16:01:08 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 26 Jun 2006 10:01:08 -0400
Subject: [R] converting to time series object : ts - package:stats
In-Reply-To: <20060626135513.82842.qmail@web37603.mail.mud.yahoo.com>
References: <Pine.LNX.4.58.0606261525130.20027@thorin.ci.tuwien.ac.at>
	<20060626135513.82842.qmail@web37603.mail.mud.yahoo.com>
Message-ID: <971536df0606260701x4624fe4dqeceea23e80893daf@mail.gmail.com>

We don't have data.csv so its still not ***reproducible*** by anyone
else.  To be reproducible it means that anyone can copy the code
in your post, paste it into R and get the same answer.

Suggest you post the output of
   dput(df)

and then post
dput <- ...the output you got from dput(df)...

Now its reproducible.

On 6/26/06, Sachin J <sachinj.2006 at yahoo.com> wrote:
> Hi Achim,
>
>  I did the following:
>
>  >df <- read.csv("C:/data.csv", header=TRUE,sep=",",na.strings="NA", dec=",",  strip.white=TRUE)
>
>  Note: data.csv has 10 (V1...V10) columns.
>
> >df[1]
>              V1
>  1        11.08
> 2         7.08
> 3         7.08
> 4         6.08
> 5         6.08
> 6         6.08
> 7        23.08
> 8        32.08
> 9         8.08
> 10       11.08
> 11        6.08
> 12       13.08
> 13       13.83
> 14       16.83
> 15       19.83
> 16        8.83
> 17       20.83
> 18       17.83
> 19        9.83
> 20       20.83
> 21       10.83
> 22       12.83
> 23       15.83
> 24       11.83
>
>  >tsdata <-  ts((df[1]),frequency = 12, start = c(2005, 1))
>
>  The resulting time series is different from the df. I don't know why? I think I am doing something silly.
>
>  TIA
>
>  Sachin
>
>
> Achim Zeileis <Achim.Zeileis at wu-wien.ac.at> wrote:
>  On Mon, 26 Jun 2006, Sachin J wrote:
>
> > Hi,
> >
> > I am trying to convert a dataset (dataframe) into time series object
> > using ts function in stats package. My dataset is as follows:
> >
> > >df
> > [1] 11.08 7.08 7.08 6.08 6.08 6.08 23.08 32.08 8.08 11.08 6.08 13.08 13.83 16.83 19.83 8.83 20.83 17.83
> > [19] 9.83 20.83 10.83 12.83 15.83 11.83
>
> Please provide a reproducible example. You just showed us the print output
> for an object, claiming that it is an object of class "data.frame" which
> is rather unlikely given the print output.
>
> > I converted this into time series object as follows
> >
> > >tsdata <- ts((df),frequency = 12, start = c(1999, 1))
>
> which produces the right result for me if `df' is a vector or a
> data.frame:
>
> df <- c(11.08, 7.08, 7.08, 6.08, 6.08, 6.08, 23.08, 32.08, 8.08, 11.08,
> 6.08, 13.08, 13.83, 16.83, 19.83, 8.83, 20.83, 17.83, 9.83, 20.83,
> 10.83, 12.83, 15.83, 11.83)
> ts(df, frequency = 12, start = c(1999, 1))
> ts(as.data.frame(df), frequency = 12, start = c(1999, 1))
>
> > The resulting time series is as follows:
> >
> > Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
> > 1999 2 15 15 14 14 14 12 13 16 2 14 5
> > 2000 6 8 10 17 11 9 18 11 1 4 7 3
> >
> > I am unable to understand why the values of df and tsdata does not match.
>
> So are we because you didn't really tell us enough about df...
>
> Best,
> Z
>
> > I looked at ts function and I couldn't find any data transformation. Am
> > I missing something here? Any pointers would be of great help.
> >
> > Thanks in advance.
> >
> > Sachin
> >
> >
> > ---------------------------------
> >
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
>
>  __________________________________________________
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Mon Jun 26 16:02:23 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 26 Jun 2006 10:02:23 -0400
Subject: [R] converting to time series object : ts - package:stats
In-Reply-To: <971536df0606260701x4624fe4dqeceea23e80893daf@mail.gmail.com>
References: <Pine.LNX.4.58.0606261525130.20027@thorin.ci.tuwien.ac.at>
	<20060626135513.82842.qmail@web37603.mail.mud.yahoo.com>
	<971536df0606260701x4624fe4dqeceea23e80893daf@mail.gmail.com>
Message-ID: <971536df0606260702mf5afd82sd13e59f3311551c3@mail.gmail.com>

Sorry I meant issue dput(df) and
post

df <- ...the output your got from dput(df)...
...rest of your code...

Now its reproducible.


On 6/26/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> We don't have data.csv so its still not ***reproducible*** by anyone
> else.  To be reproducible it means that anyone can copy the code
> in your post, paste it into R and get the same answer.
>
> Suggest you post the output of
>   dput(df)
>
> and then post
> dput <- ...the output you got from dput(df)...
>
> Now its reproducible.
>
> On 6/26/06, Sachin J <sachinj.2006 at yahoo.com> wrote:
> > Hi Achim,
> >
> >  I did the following:
> >
> >  >df <- read.csv("C:/data.csv", header=TRUE,sep=",",na.strings="NA", dec=",",  strip.white=TRUE)
> >
> >  Note: data.csv has 10 (V1...V10) columns.
> >
> > >df[1]
> >              V1
> >  1        11.08
> > 2         7.08
> > 3         7.08
> > 4         6.08
> > 5         6.08
> > 6         6.08
> > 7        23.08
> > 8        32.08
> > 9         8.08
> > 10       11.08
> > 11        6.08
> > 12       13.08
> > 13       13.83
> > 14       16.83
> > 15       19.83
> > 16        8.83
> > 17       20.83
> > 18       17.83
> > 19        9.83
> > 20       20.83
> > 21       10.83
> > 22       12.83
> > 23       15.83
> > 24       11.83
> >
> >  >tsdata <-  ts((df[1]),frequency = 12, start = c(2005, 1))
> >
> >  The resulting time series is different from the df. I don't know why? I think I am doing something silly.
> >
> >  TIA
> >
> >  Sachin
> >
> >
> > Achim Zeileis <Achim.Zeileis at wu-wien.ac.at> wrote:
> >  On Mon, 26 Jun 2006, Sachin J wrote:
> >
> > > Hi,
> > >
> > > I am trying to convert a dataset (dataframe) into time series object
> > > using ts function in stats package. My dataset is as follows:
> > >
> > > >df
> > > [1] 11.08 7.08 7.08 6.08 6.08 6.08 23.08 32.08 8.08 11.08 6.08 13.08 13.83 16.83 19.83 8.83 20.83 17.83
> > > [19] 9.83 20.83 10.83 12.83 15.83 11.83
> >
> > Please provide a reproducible example. You just showed us the print output
> > for an object, claiming that it is an object of class "data.frame" which
> > is rather unlikely given the print output.
> >
> > > I converted this into time series object as follows
> > >
> > > >tsdata <- ts((df),frequency = 12, start = c(1999, 1))
> >
> > which produces the right result for me if `df' is a vector or a
> > data.frame:
> >
> > df <- c(11.08, 7.08, 7.08, 6.08, 6.08, 6.08, 23.08, 32.08, 8.08, 11.08,
> > 6.08, 13.08, 13.83, 16.83, 19.83, 8.83, 20.83, 17.83, 9.83, 20.83,
> > 10.83, 12.83, 15.83, 11.83)
> > ts(df, frequency = 12, start = c(1999, 1))
> > ts(as.data.frame(df), frequency = 12, start = c(1999, 1))
> >
> > > The resulting time series is as follows:
> > >
> > > Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
> > > 1999 2 15 15 14 14 14 12 13 16 2 14 5
> > > 2000 6 8 10 17 11 9 18 11 1 4 7 3
> > >
> > > I am unable to understand why the values of df and tsdata does not match.
> >
> > So are we because you didn't really tell us enough about df...
> >
> > Best,
> > Z
> >
> > > I looked at ts function and I couldn't find any data transformation. Am
> > > I missing something here? Any pointers would be of great help.
> > >
> > > Thanks in advance.
> > >
> > > Sachin
> > >
> > >
> > > ---------------------------------
> > >
> > > [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > >
> >
> >
> >  __________________________________________________
> >
> >
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>


From sachinj.2006 at yahoo.com  Mon Jun 26 16:11:47 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Mon, 26 Jun 2006 07:11:47 -0700 (PDT)
Subject: [R] converting to time series object : ts - package:stats
In-Reply-To: <971536df0606260702mf5afd82sd13e59f3311551c3@mail.gmail.com>
Message-ID: <20060626141147.5517.qmail@web37614.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060626/420ff1e1/attachment.pl 

From murdoch at stats.uwo.ca  Mon Jun 26 16:12:16 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 26 Jun 2006 10:12:16 -0400
Subject: [R] Puzzled with contour()
In-Reply-To: <20060625133337.GB24241@lubyanka.local>
References: <20060625133337.GB24241@lubyanka.local>
Message-ID: <449FEB40.6000405@stats.uwo.ca>

On 6/25/2006 9:33 AM, Ajay Narottam Shah wrote:
> Folks,
> 
> The contour() function wants x and y to be in increasing order. I have
> a situation where I have a grid in x and y, and associated z values,
> which looks like this:

contour() wants vectors of x and y values, and a matrix of z values, 
where the x values correspond to the rows of z, and the y values to the 
columns.  You have a collection of points which need to be turned into 
such a grid.

There's an interp function in the akima package that can do this in 
general.  In your case, it's probably sufficient to do something like this:

zmat <- matrix(NA, 3, 19)
zmat[cbind(20*x + 1, y/10 - 1)] <- z
x <- (0:2)/20
y <- (2:20)*10
contour(x,y,zmat)

Duncan Murdoch


> 
>               x   y     z
>       [1,] 0.00  20 1.000
>       [2,] 0.00  30 1.000
>       [3,] 0.00  40 1.000
>       [4,] 0.00  50 1.000
>       [5,] 0.00  60 1.000
>       [6,] 0.00  70 1.000
>       [7,] 0.00  80 0.000
>       [8,] 0.00  90 0.000
>       [9,] 0.00 100 0.000
>      [10,] 0.00 110 0.000
>      [11,] 0.00 120 0.000
>      [12,] 0.00 130 0.000
>      [13,] 0.00 140 0.000
>      [14,] 0.00 150 0.000
>      [15,] 0.00 160 0.000
>      [16,] 0.00 170 0.000
>      [17,] 0.00 180 0.000
>      [18,] 0.00 190 0.000
>      [19,] 0.00 200 0.000
>      [20,] 0.05  20 1.000
>      [21,] 0.05  30 1.000
>      [22,] 0.05  40 1.000
>      [23,] 0.05  50 1.000
>      [24,] 0.05  60 0.998
>      [25,] 0.05  70 0.124
>      [26,] 0.05  80 0.000
>      [27,] 0.05  90 0.000
>      [28,] 0.05 100 0.000
>      [29,] 0.05 110 0.000
>      [30,] 0.05 120 0.000
>      [31,] 0.05 130 0.000
>      [32,] 0.05 140 0.000
>      [33,] 0.05 150 0.000
>      [34,] 0.05 160 0.000
>      [35,] 0.05 170 0.000
>      [36,] 0.05 180 0.000
>      [37,] 0.05 190 0.000
>      [38,] 0.05 200 0.000
>      [39,] 0.10  20 1.000
>      [40,] 0.10  30 1.000
> 
> This looks like a nice case where both x and y are in increasing
> order. But contour() gets unhappy saying that he wants x and y in
> increasing order.
> 
> Gnuplot generates pretty 3d pictures from such data, where you are
> standing above a surface and looking down at it. How does one do that
> in R?
> 
> Any help will be most appreciated. A dput() of my data object is :
> 
> structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
> 0, 0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 
> 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 
> 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 
> 0.1, 0.1, 0.1, 0.1, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 
> 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 
> 0.15, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 
> 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 
> 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 
> 0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 
> 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.35, 
> 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 
> 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.4, 0.4, 0.4, 0.4, 
> 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 
> 0.4, 0.4, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 
> 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.5, 
> 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 
> 0.5, 0.5, 0.5, 0.5, 0.5, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 
> 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 
> 0.55, 0.55, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 
> 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.65, 0.65, 0.65, 
> 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 
> 0.65, 0.65, 0.65, 0.65, 0.65, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 
> 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.75, 
> 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 
> 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.8, 0.8, 0.8, 0.8, 
> 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 
> 0.8, 0.8, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 
> 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.9, 
> 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 
> 0.9, 0.9, 0.9, 0.9, 0.9, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 
> 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 
> 0.95, 0.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
> 1, 1, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 
> 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 
> 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 
> 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 
> 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 
> 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 
> 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 
> 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 
> 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 
> 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 
> 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 
> 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 
> 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 
> 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 
> 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 
> 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 
> 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 
> 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 
> 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 
> 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 
> 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 
> 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 
> 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 
> 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 
> 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 
> 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 
> 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 
> 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 
> 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 
> 170, 180, 190, 200, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 
> 0, 0, 0, 0, 0, 1, 1, 1, 1, 0.998, 0.124, 0, 0, 0, 0, 0, 0, 0, 
> 0, 0, 0, 0, 0, 0, 1, 1, 1, 0.998, 0.71, 0.068, 0, 0, 0, 0, 0, 
> 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0.998, 0.898, 0.396, 0.058, 0.002, 
> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.998, 0.97, 0.726, 0.268, 
> 0.056, 0.006, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.996, 0.88, 
> 0.546, 0.208, 0.054, 0.012, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
> 0, 0, 0.998, 0.964, 0.776, 0.418, 0.18, 0.054, 0.014, 0.002, 
> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.998, 0.906, 0.664, 0.342, 
> 0.166, 0.056, 0.018, 0.006, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
> 0, 0.986, 0.862, 0.568, 0.29, 0.15, 0.056, 0.022, 0.008, 0.002, 
> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.954, 0.778, 0.494, 0.26, 0.148, 
> 0.056, 0.024, 0.012, 0.004, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
> 0.906, 0.712, 0.43, 0.242, 0.144, 0.058, 0.028, 0.012, 0.006, 
> 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.878, 0.642, 0.38, 0.222, 
> 0.142, 0.066, 0.034, 0.014, 0.008, 0.004, 0.002, 0, 0, 0, 0, 
> 0, 0, 0, 0, 0.846, 0.586, 0.348, 0.208, 0.136, 0.068, 0.034, 
> 0.016, 0.012, 0.006, 0.004, 0.002, 0, 0, 0, 0, 0, 0, 0, 0.8, 
> 0.538, 0.318, 0.204, 0.136, 0.07, 0.046, 0.024, 0.012, 0.008, 
> 0.004, 0.002, 0.002, 0, 0, 0, 0, 0, 0, 0.762, 0.496, 0.294, 0.2, 
> 0.138, 0.072, 0.05, 0.024, 0.014, 0.012, 0.006, 0.004, 0.002, 
> 0.002, 0, 0, 0, 0, 0, 0.704, 0.472, 0.286, 0.198, 0.138, 0.074, 
> 0.054, 0.028, 0.016, 0.012, 0.008, 0.006, 0.004, 0.002, 0.002, 
> 0, 0, 0, 0, 0.668, 0.438, 0.276, 0.196, 0.138, 0.078, 0.054, 
> 0.032, 0.024, 0.014, 0.012, 0.008, 0.004, 0.004, 0.002, 0.002, 
> 0, 0, 0, 0.634, 0.412, 0.27, 0.194, 0.14, 0.086, 0.056, 0.032, 
> 0.024, 0.016, 0.012, 0.01, 0.006, 0.004, 0.004, 0.002, 0.002, 
> 0, 0, 0.604, 0.388, 0.26, 0.19, 0.144, 0.088, 0.058, 0.048, 0.026, 
> 0.022, 0.014, 0.012, 0.008, 0.006, 0.004, 0.004, 0.002, 0.002, 
> 0, 0.586, 0.376, 0.256, 0.19, 0.146, 0.094, 0.062, 0.052, 0.028, 
> 0.024, 0.014, 0.012, 0.012, 0.008, 0.004, 0.004, 0.004, 0.002, 
> 0.002, 0.566, 0.364, 0.254, 0.192, 0.148, 0.098, 0.064, 0.054, 
> 0.032, 0.024, 0.022, 0.014, 0.012, 0.012, 0.008, 0.004, 0.004, 
> 0.004, 0.002), .Dim = c(399, 3), .Dimnames = list(NULL, c("x", 
> "y", "z")))
> 
> --
> Ajay Shah                                      http://www.mayin.org/ajayshah  
> ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
> <*(:-? - wizard who doesn't know the answer.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ggrothendieck at gmail.com  Mon Jun 26 16:26:57 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 26 Jun 2006 10:26:57 -0400
Subject: [R] converting to time series object : ts - package:stats
In-Reply-To: <20060626141147.5517.qmail@web37614.mail.mud.yahoo.com>
References: <971536df0606260702mf5afd82sd13e59f3311551c3@mail.gmail.com>
	<20060626141147.5517.qmail@web37614.mail.mud.yahoo.com>
Message-ID: <971536df0606260726k1787e2ecv751fbf7e46249450@mail.gmail.com>

df[] <- sapply(format(df), as.numeric)

will convert it to numeric but I think the real problem is the read.csv
statement.  Do commas represent separators or decimals since
you have specified comma for both?  Assuming it looks like:

A,B,C
1,2,3
4,5,6

just do:

DF <- read.csv("Data.csv")
str(DF)




On 6/26/06, Sachin J <sachinj.2006 at yahoo.com> wrote:
>
>
> It seems I have problem in reading the data as dataframe. It is reading it
> as factors. Here is the df
>
> df <-
> read.csv("C:/Data.csv",header=TRUE,sep=",",na.strings="NA",
> dec=",", strip.white=TRUE)
>
> > dput(df)
>
> > df <- structure(list(V1 = structure(c(2, 15, 15, 14, 14, 14, 12, 13,
> + 16, 2, 14, 5, 6, 8, 10, 17, 11, 9, 18, 11, 1, 4, 7, 3), .Label =
> c("10.83",
> + "11.08", "11.83", "12.83", "13.08", "13.83", "15.83", "16.83",
> + "17.83", "19.83", "20.83", "23.08", "32.08", "6.08", "7.08",
> + "8.08", "8.83", "9.83"), class = "factor"), V2 = structure(c(8,
> + 15, 2, 10, 9, 18, 1, 4, 10, 2, 8, 6, 17, 5, 16, 13, 5, 14, 3,
> + 11, 3, 12, 7, 7), .Label = c("10.73", "11.73", "11.75", "12.73",
> + "15.75", "19.73", "19.75", "21.73", "25.73", "26.73", "26.75",
> + "27.75", "32.75", "33.75", "37.73", "42.75", "61.75", "9.73"), class =
> "factor"),
> +     V3 = structure(c(3, 8, 7, 9, 11, 9, 3, 8, 10, 9, 11, 10,
> +     2, 1, 12, 12, 6, 5, 4, 6, 2, 5, 5, 1), .Label = c("10.33",
> +     "12.33", "13.08", "13.33", "14.33", "15.33", "21.08", "6.08",
> +     "7.08", "8.08", "9.08", "9.33"), class = "factor")), .Names = c("V1",
> + "V2", "V3"), class = "data.frame", row.names = c("1", "2", "3",
> + "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15",
> + "16", "17", "18", "19", "20", "21", "22", "23", "24"))
>
> TIA
>
> Sachin
>
>
>
>
> Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>
> Sorry I meant issue dput(df) and
> post
>
> df <- ...the output your got from dput(df)...
> ...rest of your code...
>
> Now its reproducible.
>
>
> On 6/26/06, Gabor Grothendieck wrote:
> > We don't have data.csv so its still not ***reproducible*** by anyone
> > else. To be reproducible it means that anyone can copy the code
> > in your post, paste it into R and get the same answer.
> >
> > Suggest you post the output of
> > dput(df)
> >
> > and then post
> > dput <- ...the output you got from dput(df)...
> >
> > Now its reproducible.
> >
> > On 6/26/06, Sachin J wrote:
> > > Hi Achim,
> > >
> > > I did the following:
> > >
> > > >df <- read.csv("C:/data.csv", header=TRUE,sep=",",na.strings="NA",
> dec=",", strip.white=TRUE)
> > >
> > > Note: data.csv has 10 (V1...V10) columns.
> > >
> > > >df[1]
> > > V1
> > > 1 11.08
> > > 2 7.08
> > > 3 7.08
> > > 4 6.08
> > > 5 6.08
> > > 6 6.08
> > > 7 23.08
> > > 8 32.08
> > > 9 8.08
> > > 10 11.08
> > > 11 6.08
> > > 12 13.08
> > > 13 13.83
> > > 14 16.83
> > > 15 19.83
> > > 16 8.83
> > > 17 20.83
> > > 18 17.83
> > > 19 9.83
> > > 20 20.83
> > > 21 10.83
> > > 22 12.83
> > > 23 15.83
> > > 24 11.83
> > >
> > > >tsdata <- ts((df[1]),frequency = 12, start = c(2005, 1))
> > >
> > > The resulting time series is different from the df. I don't know why? I
> think I am doing something silly.
> > >
> > > TIA
> > >
> > > Sachin
> > >
> > >
> > > Achim Zeileis wrote:
> > > On Mon, 26 Jun 2006, Sachin J wrote:
> > >
> > > > Hi,
> > > >
> > > > I am trying to convert a dataset (dataframe) into time series object
> > > > using ts function in stats package. My dataset is as follows:
> > > >
> > > > >df
> > > > [1] 11.08 7.08 7.08 6.08 6.08 6.08 23.08 32.08 8.08 11.08 6.08 13.08
> 13.83 16.83 19.83 8.83 20.83 17.83
> > > > [19] 9.83 20.83 10.83 12.83 15.83 11.83
> > >
> > > Please provide a reproducible example. You just showed us the print
> output
> > > for an object, claiming that it is an object of class "data.frame" which
> > > is rather unlikely given the print output.
> > >
> > > > I converted this into time series object as follows
> > > >
> > > > >tsdata <- ts((df),frequency = 12, start = c(1999, 1))
> > >
> > > which produces the right result for me if `df' is a vector or a
> > > data.frame:
> > >
> > > df <- c(11.08, 7.08, 7.08, 6.08, 6.08, 6.08, 23.08, 32.08, 8.08, 11.08,
> > > 6.08, 13.08, 13.83, 16.83, 19.83, 8.83, 20.83, 17.83, 9.83, 20.83,
> > > 10.83, 12.83, 15.83, 11.83)
> > > ts(df, frequency = 12, start = c(1999, 1))
> > > ts(as.data.frame(df), frequency = 12, start = c(1999, 1))
> > >
> > > > The resulting time series is as follows:
> > > >
> > > > Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
> > > > 1999 2 15 15 14 14 14 12 13 16 2 14 5
> > > > 2000 6 8 10 17 11 9 18 11 1 4 7 3
> > > >
> > > > I am unable to understand why the values of df and tsdata does not
> match.
> > >
> > > So are we because you didn't really tell us enough about df...
> > >
> > > Best,
> > > Z
> > >
> > > > I looked at ts function and I couldn't find any data transformation.
> Am
> > > > I missing something here? Any pointers would be of great help.
> > > >
> > > > Thanks in advance.
> > > >
> > > > Sachin
> > > >
> > > >
> > > > ---------------------------------
> > > >
> > > > [[alternative HTML version deleted]]
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> > > >
> > >
> > >
> > > __________________________________________________
> > >
> > >
> > >
> > > [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> > >
> >
>
>
>
>
> ________________________________
> Do you Yahoo!?
> Next-gen email? Have it all with the all-new Yahoo! Mail Beta.
>
>


From ggrothendieck at gmail.com  Mon Jun 26 16:39:20 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 26 Jun 2006 10:39:20 -0400
Subject: [R] Puzzled with contour()
In-Reply-To: <449FEB40.6000405@stats.uwo.ca>
References: <20060625133337.GB24241@lubyanka.local>
	<449FEB40.6000405@stats.uwo.ca>
Message-ID: <971536df0606260739o26a54ad1u389f56986ce460ac@mail.gmail.com>

I think it would be helpful if this were added to the contour help file.

On 6/26/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> On 6/25/2006 9:33 AM, Ajay Narottam Shah wrote:
> > Folks,
> >
> > The contour() function wants x and y to be in increasing order. I have
> > a situation where I have a grid in x and y, and associated z values,
> > which looks like this:
>
> contour() wants vectors of x and y values, and a matrix of z values,
> where the x values correspond to the rows of z, and the y values to the
> columns.  You have a collection of points which need to be turned into
> such a grid.
>
> There's an interp function in the akima package that can do this in
> general.  In your case, it's probably sufficient to do something like this:
>
> zmat <- matrix(NA, 3, 19)
> zmat[cbind(20*x + 1, y/10 - 1)] <- z
> x <- (0:2)/20
> y <- (2:20)*10
> contour(x,y,zmat)
>
> Duncan Murdoch
>
>
> >
> >               x   y     z
> >       [1,] 0.00  20 1.000
> >       [2,] 0.00  30 1.000
> >       [3,] 0.00  40 1.000
> >       [4,] 0.00  50 1.000
> >       [5,] 0.00  60 1.000
> >       [6,] 0.00  70 1.000
> >       [7,] 0.00  80 0.000
> >       [8,] 0.00  90 0.000
> >       [9,] 0.00 100 0.000
> >      [10,] 0.00 110 0.000
> >      [11,] 0.00 120 0.000
> >      [12,] 0.00 130 0.000
> >      [13,] 0.00 140 0.000
> >      [14,] 0.00 150 0.000
> >      [15,] 0.00 160 0.000
> >      [16,] 0.00 170 0.000
> >      [17,] 0.00 180 0.000
> >      [18,] 0.00 190 0.000
> >      [19,] 0.00 200 0.000
> >      [20,] 0.05  20 1.000
> >      [21,] 0.05  30 1.000
> >      [22,] 0.05  40 1.000
> >      [23,] 0.05  50 1.000
> >      [24,] 0.05  60 0.998
> >      [25,] 0.05  70 0.124
> >      [26,] 0.05  80 0.000
> >      [27,] 0.05  90 0.000
> >      [28,] 0.05 100 0.000
> >      [29,] 0.05 110 0.000
> >      [30,] 0.05 120 0.000
> >      [31,] 0.05 130 0.000
> >      [32,] 0.05 140 0.000
> >      [33,] 0.05 150 0.000
> >      [34,] 0.05 160 0.000
> >      [35,] 0.05 170 0.000
> >      [36,] 0.05 180 0.000
> >      [37,] 0.05 190 0.000
> >      [38,] 0.05 200 0.000
> >      [39,] 0.10  20 1.000
> >      [40,] 0.10  30 1.000
> >
> > This looks like a nice case where both x and y are in increasing
> > order. But contour() gets unhappy saying that he wants x and y in
> > increasing order.
> >
> > Gnuplot generates pretty 3d pictures from such data, where you are
> > standing above a surface and looking down at it. How does one do that
> > in R?
> >
> > Any help will be most appreciated. A dput() of my data object is :
> >
> > structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> > 0, 0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,
> > 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1,
> > 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,
> > 0.1, 0.1, 0.1, 0.1, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,
> > 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,
> > 0.15, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
> > 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25,
> > 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,
> > 0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,
> > 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.35,
> > 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35,
> > 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.4, 0.4, 0.4, 0.4,
> > 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,
> > 0.4, 0.4, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,
> > 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.5,
> > 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,
> > 0.5, 0.5, 0.5, 0.5, 0.5, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55,
> > 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55,
> > 0.55, 0.55, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,
> > 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.65, 0.65, 0.65,
> > 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65,
> > 0.65, 0.65, 0.65, 0.65, 0.65, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7,
> > 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.75,
> > 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,
> > 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.8, 0.8, 0.8, 0.8,
> > 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,
> > 0.8, 0.8, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85,
> > 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.9,
> > 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,
> > 0.9, 0.9, 0.9, 0.9, 0.9, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,
> > 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,
> > 0.95, 0.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> > 1, 1, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140,
> > 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90,
> > 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30,
> > 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170,
> > 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120,
> > 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70,
> > 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200,
> > 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150,
> > 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100,
> > 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40,
> > 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180,
> > 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130,
> > 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80,
> > 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20,
> > 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160,
> > 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110,
> > 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50,
> > 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180,
> > 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130,
> > 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80,
> > 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20,
> > 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160,
> > 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110,
> > 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50,
> > 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180,
> > 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130,
> > 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80,
> > 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20,
> > 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160,
> > 170, 180, 190, 200, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
> > 0, 0, 0, 0, 0, 1, 1, 1, 1, 0.998, 0.124, 0, 0, 0, 0, 0, 0, 0,
> > 0, 0, 0, 0, 0, 0, 1, 1, 1, 0.998, 0.71, 0.068, 0, 0, 0, 0, 0,
> > 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0.998, 0.898, 0.396, 0.058, 0.002,
> > 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.998, 0.97, 0.726, 0.268,
> > 0.056, 0.006, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.996, 0.88,
> > 0.546, 0.208, 0.054, 0.012, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> > 0, 0, 0.998, 0.964, 0.776, 0.418, 0.18, 0.054, 0.014, 0.002,
> > 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.998, 0.906, 0.664, 0.342,
> > 0.166, 0.056, 0.018, 0.006, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> > 0, 0.986, 0.862, 0.568, 0.29, 0.15, 0.056, 0.022, 0.008, 0.002,
> > 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.954, 0.778, 0.494, 0.26, 0.148,
> > 0.056, 0.024, 0.012, 0.004, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> > 0.906, 0.712, 0.43, 0.242, 0.144, 0.058, 0.028, 0.012, 0.006,
> > 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.878, 0.642, 0.38, 0.222,
> > 0.142, 0.066, 0.034, 0.014, 0.008, 0.004, 0.002, 0, 0, 0, 0,
> > 0, 0, 0, 0, 0.846, 0.586, 0.348, 0.208, 0.136, 0.068, 0.034,
> > 0.016, 0.012, 0.006, 0.004, 0.002, 0, 0, 0, 0, 0, 0, 0, 0.8,
> > 0.538, 0.318, 0.204, 0.136, 0.07, 0.046, 0.024, 0.012, 0.008,
> > 0.004, 0.002, 0.002, 0, 0, 0, 0, 0, 0, 0.762, 0.496, 0.294, 0.2,
> > 0.138, 0.072, 0.05, 0.024, 0.014, 0.012, 0.006, 0.004, 0.002,
> > 0.002, 0, 0, 0, 0, 0, 0.704, 0.472, 0.286, 0.198, 0.138, 0.074,
> > 0.054, 0.028, 0.016, 0.012, 0.008, 0.006, 0.004, 0.002, 0.002,
> > 0, 0, 0, 0, 0.668, 0.438, 0.276, 0.196, 0.138, 0.078, 0.054,
> > 0.032, 0.024, 0.014, 0.012, 0.008, 0.004, 0.004, 0.002, 0.002,
> > 0, 0, 0, 0.634, 0.412, 0.27, 0.194, 0.14, 0.086, 0.056, 0.032,
> > 0.024, 0.016, 0.012, 0.01, 0.006, 0.004, 0.004, 0.002, 0.002,
> > 0, 0, 0.604, 0.388, 0.26, 0.19, 0.144, 0.088, 0.058, 0.048, 0.026,
> > 0.022, 0.014, 0.012, 0.008, 0.006, 0.004, 0.004, 0.002, 0.002,
> > 0, 0.586, 0.376, 0.256, 0.19, 0.146, 0.094, 0.062, 0.052, 0.028,
> > 0.024, 0.014, 0.012, 0.012, 0.008, 0.004, 0.004, 0.004, 0.002,
> > 0.002, 0.566, 0.364, 0.254, 0.192, 0.148, 0.098, 0.064, 0.054,
> > 0.032, 0.024, 0.022, 0.014, 0.012, 0.012, 0.008, 0.004, 0.004,
> > 0.004, 0.002), .Dim = c(399, 3), .Dimnames = list(NULL, c("x",
> > "y", "z")))
> >
> > --
> > Ajay Shah                                      http://www.mayin.org/ajayshah
> > ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
> > <*(:-? - wizard who doesn't know the answer.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From murdoch at stats.uwo.ca  Mon Jun 26 16:53:06 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 26 Jun 2006 10:53:06 -0400
Subject: [R] Puzzled with contour()
In-Reply-To: <971536df0606260739o26a54ad1u389f56986ce460ac@mail.gmail.com>
References: <20060625133337.GB24241@lubyanka.local>	<449FEB40.6000405@stats.uwo.ca>
	<971536df0606260739o26a54ad1u389f56986ce460ac@mail.gmail.com>
Message-ID: <449FF4D2.1080009@stats.uwo.ca>

On 6/26/2006 10:39 AM, Gabor Grothendieck wrote:
> I think it would be helpful if this were added to the contour help file.

You mean an example of building up the z matrix from points, or just a 
general discussion of the issue?

Duncan Murdoch
> 
> On 6/26/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
>> On 6/25/2006 9:33 AM, Ajay Narottam Shah wrote:
>> > Folks,
>> >
>> > The contour() function wants x and y to be in increasing order. I have
>> > a situation where I have a grid in x and y, and associated z values,
>> > which looks like this:
>>
>> contour() wants vectors of x and y values, and a matrix of z values,
>> where the x values correspond to the rows of z, and the y values to the
>> columns.  You have a collection of points which need to be turned into
>> such a grid.
>>
>> There's an interp function in the akima package that can do this in
>> general.  In your case, it's probably sufficient to do something like this:
>>
>> zmat <- matrix(NA, 3, 19)
>> zmat[cbind(20*x + 1, y/10 - 1)] <- z
>> x <- (0:2)/20
>> y <- (2:20)*10
>> contour(x,y,zmat)
>>
>> Duncan Murdoch
>>
>>
>> >
>> >               x   y     z
>> >       [1,] 0.00  20 1.000
>> >       [2,] 0.00  30 1.000
>> >       [3,] 0.00  40 1.000
>> >       [4,] 0.00  50 1.000
>> >       [5,] 0.00  60 1.000
>> >       [6,] 0.00  70 1.000
>> >       [7,] 0.00  80 0.000
>> >       [8,] 0.00  90 0.000
>> >       [9,] 0.00 100 0.000
>> >      [10,] 0.00 110 0.000
>> >      [11,] 0.00 120 0.000
>> >      [12,] 0.00 130 0.000
>> >      [13,] 0.00 140 0.000
>> >      [14,] 0.00 150 0.000
>> >      [15,] 0.00 160 0.000
>> >      [16,] 0.00 170 0.000
>> >      [17,] 0.00 180 0.000
>> >      [18,] 0.00 190 0.000
>> >      [19,] 0.00 200 0.000
>> >      [20,] 0.05  20 1.000
>> >      [21,] 0.05  30 1.000
>> >      [22,] 0.05  40 1.000
>> >      [23,] 0.05  50 1.000
>> >      [24,] 0.05  60 0.998
>> >      [25,] 0.05  70 0.124
>> >      [26,] 0.05  80 0.000
>> >      [27,] 0.05  90 0.000
>> >      [28,] 0.05 100 0.000
>> >      [29,] 0.05 110 0.000
>> >      [30,] 0.05 120 0.000
>> >      [31,] 0.05 130 0.000
>> >      [32,] 0.05 140 0.000
>> >      [33,] 0.05 150 0.000
>> >      [34,] 0.05 160 0.000
>> >      [35,] 0.05 170 0.000
>> >      [36,] 0.05 180 0.000
>> >      [37,] 0.05 190 0.000
>> >      [38,] 0.05 200 0.000
>> >      [39,] 0.10  20 1.000
>> >      [40,] 0.10  30 1.000
>> >
>> > This looks like a nice case where both x and y are in increasing
>> > order. But contour() gets unhappy saying that he wants x and y in
>> > increasing order.
>> >
>> > Gnuplot generates pretty 3d pictures from such data, where you are
>> > standing above a surface and looking down at it. How does one do that
>> > in R?
>> >
>> > Any help will be most appreciated. A dput() of my data object is :
>> >
>> > structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> > 0, 0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,
>> > 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1,
>> > 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,
>> > 0.1, 0.1, 0.1, 0.1, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,
>> > 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,
>> > 0.15, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
>> > 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25,
>> > 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,
>> > 0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,
>> > 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.35,
>> > 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35,
>> > 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.4, 0.4, 0.4, 0.4,
>> > 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,
>> > 0.4, 0.4, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,
>> > 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.5,
>> > 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,
>> > 0.5, 0.5, 0.5, 0.5, 0.5, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55,
>> > 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55,
>> > 0.55, 0.55, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,
>> > 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.65, 0.65, 0.65,
>> > 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65,
>> > 0.65, 0.65, 0.65, 0.65, 0.65, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7,
>> > 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.75,
>> > 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,
>> > 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.8, 0.8, 0.8, 0.8,
>> > 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,
>> > 0.8, 0.8, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85,
>> > 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.9,
>> > 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,
>> > 0.9, 0.9, 0.9, 0.9, 0.9, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,
>> > 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,
>> > 0.95, 0.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
>> > 1, 1, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140,
>> > 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90,
>> > 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30,
>> > 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170,
>> > 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120,
>> > 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70,
>> > 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200,
>> > 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150,
>> > 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100,
>> > 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40,
>> > 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180,
>> > 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130,
>> > 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80,
>> > 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20,
>> > 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160,
>> > 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110,
>> > 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50,
>> > 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180,
>> > 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130,
>> > 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80,
>> > 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20,
>> > 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160,
>> > 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110,
>> > 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50,
>> > 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180,
>> > 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130,
>> > 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80,
>> > 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20,
>> > 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160,
>> > 170, 180, 190, 200, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
>> > 0, 0, 0, 0, 0, 1, 1, 1, 1, 0.998, 0.124, 0, 0, 0, 0, 0, 0, 0,
>> > 0, 0, 0, 0, 0, 0, 1, 1, 1, 0.998, 0.71, 0.068, 0, 0, 0, 0, 0,
>> > 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0.998, 0.898, 0.396, 0.058, 0.002,
>> > 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.998, 0.97, 0.726, 0.268,
>> > 0.056, 0.006, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.996, 0.88,
>> > 0.546, 0.208, 0.054, 0.012, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> > 0, 0, 0.998, 0.964, 0.776, 0.418, 0.18, 0.054, 0.014, 0.002,
>> > 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.998, 0.906, 0.664, 0.342,
>> > 0.166, 0.056, 0.018, 0.006, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> > 0, 0.986, 0.862, 0.568, 0.29, 0.15, 0.056, 0.022, 0.008, 0.002,
>> > 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.954, 0.778, 0.494, 0.26, 0.148,
>> > 0.056, 0.024, 0.012, 0.004, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> > 0.906, 0.712, 0.43, 0.242, 0.144, 0.058, 0.028, 0.012, 0.006,
>> > 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.878, 0.642, 0.38, 0.222,
>> > 0.142, 0.066, 0.034, 0.014, 0.008, 0.004, 0.002, 0, 0, 0, 0,
>> > 0, 0, 0, 0, 0.846, 0.586, 0.348, 0.208, 0.136, 0.068, 0.034,
>> > 0.016, 0.012, 0.006, 0.004, 0.002, 0, 0, 0, 0, 0, 0, 0, 0.8,
>> > 0.538, 0.318, 0.204, 0.136, 0.07, 0.046, 0.024, 0.012, 0.008,
>> > 0.004, 0.002, 0.002, 0, 0, 0, 0, 0, 0, 0.762, 0.496, 0.294, 0.2,
>> > 0.138, 0.072, 0.05, 0.024, 0.014, 0.012, 0.006, 0.004, 0.002,
>> > 0.002, 0, 0, 0, 0, 0, 0.704, 0.472, 0.286, 0.198, 0.138, 0.074,
>> > 0.054, 0.028, 0.016, 0.012, 0.008, 0.006, 0.004, 0.002, 0.002,
>> > 0, 0, 0, 0, 0.668, 0.438, 0.276, 0.196, 0.138, 0.078, 0.054,
>> > 0.032, 0.024, 0.014, 0.012, 0.008, 0.004, 0.004, 0.002, 0.002,
>> > 0, 0, 0, 0.634, 0.412, 0.27, 0.194, 0.14, 0.086, 0.056, 0.032,
>> > 0.024, 0.016, 0.012, 0.01, 0.006, 0.004, 0.004, 0.002, 0.002,
>> > 0, 0, 0.604, 0.388, 0.26, 0.19, 0.144, 0.088, 0.058, 0.048, 0.026,
>> > 0.022, 0.014, 0.012, 0.008, 0.006, 0.004, 0.004, 0.002, 0.002,
>> > 0, 0.586, 0.376, 0.256, 0.19, 0.146, 0.094, 0.062, 0.052, 0.028,
>> > 0.024, 0.014, 0.012, 0.012, 0.008, 0.004, 0.004, 0.004, 0.002,
>> > 0.002, 0.566, 0.364, 0.254, 0.192, 0.148, 0.098, 0.064, 0.054,
>> > 0.032, 0.024, 0.022, 0.014, 0.012, 0.012, 0.008, 0.004, 0.004,
>> > 0.004, 0.002), .Dim = c(399, 3), .Dimnames = list(NULL, c("x",
>> > "y", "z")))
>> >
>> > --
>> > Ajay Shah                                      http://www.mayin.org/ajayshah
>> > ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
>> > <*(:-? - wizard who doesn't know the answer.
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From sachinj.2006 at yahoo.com  Mon Jun 26 17:03:25 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Mon, 26 Jun 2006 08:03:25 -0700 (PDT)
Subject: [R] converting to time series object : ts - package:stats
In-Reply-To: <971536df0606260726k1787e2ecv751fbf7e46249450@mail.gmail.com>
Message-ID: <20060626150325.62071.qmail@web37613.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060626/3add58d4/attachment.pl 

From ggrothendieck at gmail.com  Mon Jun 26 17:26:07 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 26 Jun 2006 11:26:07 -0400
Subject: [R] converting to time series object : ts - package:stats
In-Reply-To: <20060626150325.62071.qmail@web37613.mail.mud.yahoo.com>
References: <971536df0606260726k1787e2ecv751fbf7e46249450@mail.gmail.com>
	<20060626150325.62071.qmail@web37613.mail.mud.yahoo.com>
Message-ID: <971536df0606260826x643fe428w9e0e2989e480bf33@mail.gmail.com>

I think the problem is that you specified the decimal character to be
a comma so when it saw the dot it figured its not a number.

On 6/26/06, Sachin J <sachinj.2006 at yahoo.com> wrote:
>
> Hi Gabor,
>
> You are correct. The real problem is with read.csv. I am not sure why? My
> data looks
>
> V1,V2,V3
> 11.08,21.73,13.08
> 7.08,37.73,6.08
> 7.08,11.73,21.08
>
> I never had this problem earlier. Anyway I did
>
> >df <- read.csv("Data.csv")
> >tsdata <-  ts((df),frequency = 12, start = c(1999, 1))
>
> it works fine. But still puzzled with read.csv behavior. Any thoughts?
>
> Thanx Gabor, Achim  and Brian for your help.
>
> Sachin
>
> Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>
> df[] <- sapply(format(df), as.numeric)
>
> will convert it to numeric but I think the real problem is the read.csv
> statement. Do commas represent separators or decimals since
> you have specified comma for both? Assuming it looks like:
>
> A,B,C
> 1,2,3
> 4,5,6
>
> just do:
>
> DF <- read.csv("Data.csv")
> str(DF)
>
>
>
>
> On 6/26/06, Sachin J wrote:
> >
> >
> > It seems I have problem in reading the data as dataframe. It is reading it
> > as factors. Here is the df
> >
> > df <-
> >
> read.csv("C:/Data.csv",header=TRUE,sep=",",na.strings="NA",
> > dec=",", strip.white=TRUE)
> >
> > > dput(df)
> >
> > > df <- structure(list(V1 = structure(c(2, 15, 15, 14, 14, 14, 12, 13,
> > + 16, 2, 14, 5, 6, 8, 10, 17, 11, 9, 18, 11, 1, 4, 7, 3), .Label =
> > c("10.83",
> > + "11.08", "11.83", "12.83", "13.08", "13.83", "15.83", "16.83",
> > + "17.83", "19.83", "20.83", "23.08", "32.08", "6.08", "7.08",
> > + "8.08", "8.83", "9.83"), class = "factor"), V2 = structure(c(8,
> > + 15, 2, 10, 9, 18, 1, 4, 10, 2, 8, 6, 17, 5, 16, 13, 5, 14, 3,
> > + 11, 3, 12, 7, 7), .Label = c("10.73", "11.73", "11.75", "12.73",
> > + "15.75", "19.73", "19.75", "21.73", "25.73", "26.73", "26.75",
> > + "27.75", "32.75", "33.75", "37.73", "42.75", "61.75", "9.73"), class =
> > "factor"),
> > + V3 = structure(c(3, 8, 7, 9, 11, 9, 3, 8, 10, 9, 11, 10,
> > + 2, 1, 12, 12, 6, 5, 4, 6, 2, 5, 5, 1), .Label = c("10.33",
> > + "12.33", "13.08", "13.33", "14.33", "15.33", "21.08", "6.08",
> > + "7.08", "8.08", "9.08", "9.33"), class = "factor")), .Names = c("V1",
> > + "V2", "V3"), class = "data.frame", row.names = c("1", "2", "3",
> > + "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15",
> > + "16", "17", "18", "19", "20", "21", "22", "23", "24"))
> >
> > TIA
> >
> > Sachin
> >
> >
> >
> >
> > Gabor Grothendieck wrote:
> >
> > Sorry I meant issue dput(df) and
> > post
> >
> > df <- ...the output your got from dput(df)...
> > ...rest of your code...
> >
> > Now its reproducible.
> >
> >
> > On 6/26/06, Gabor Grothendieck wrote:
> > > We don't have data.csv so its still not ***reproducible*** by anyone
> > > else. To be reproducible it means that anyone can copy the code
> > > in your post, paste it into R and get the same answer.
> > >
> > > Suggest you post the output of
> > > dput(df)
> > >
> > > and then post
> > > dput <- ...the output you got from dput(df)...
> > >
> > > Now its reproducible.
> > >
> > > On 6/26/06, Sachin J wrote:
> > > > Hi Achim,
> > > >
> > > > I did the following:
> > > >
> > > > >df <- read.csv("C:/data.csv", header=TRUE,sep=",",na.strings="NA",
> > dec=",", strip.white=TRUE)
> > > >
> > > > Note: data.csv has 10 (V1...V10) columns.
> > > >
> > > > >df[1]
> > > > V1
> > > > 1 11.08
> > > > 2 7.08
> > > > 3 7.08
> > > > 4 6.08
> > > > 5 6.08
> > > > 6 6.08
> > > > 7 23.08
> > > > 8 32.08
> > > > 9 8.08
> > > > 10 11.08
> > > > 11 6.08
> > > > 12 13.08
> > > > 13 13.83
> > > > 14 16.83
> > > > 15 19.83
> > > > 16 8.83
> > > > 17 20.83
> > > > 18 17.83
> > > > 19 9.83
> > > > 20 20.83
> > > > 21 10.83
> > > > 22 12.83
> > > > 23 15.83
> > > > 24 11.83
> > > >
> > > > >tsdata <- ts((df[1]),frequency = 12, start = c(2005, 1))
> > > >
> > > > The resulting time series is different from the df. I don't know why?
> I
> > think I am doing something silly.
> > > >
> > > > TIA
> > > >
> > > > Sachin
> > > >
> > > >
> > > > Achim Zeileis wrote:
> > > > On Mon, 26 Jun 2006, Sachin J wrote:
> > > >
> > > > > Hi,
> > > > >
> > > > > I am trying to convert a dataset (dataframe) into time series object
> > > > > using ts function in stats package. My dataset is as follows:
> > > > >
> > > > > >df
> > > > > [1] 11.08 7.08 7.08 6.08 6.08 6.08 23.08 32.08 8.08 11.08 6.08 13.08
> > 13.83 16.83 19.83 8.83 20.83 17.83
> > > > > [19] 9.83 20.83 10.83 12.83 15.83 11.83
> > > >
> > > > Please provide a reproducible example. You just showed us the print
> > output
> > > > for an object, claiming that it is an object of class "data.frame"
> which
> > > > is rather unlikely given the print output.
> > > >
> > > > > I converted this into time series object as follows
> > > > >
> > > > > >tsdata <- ts((df),frequency = 12, start = c(1999, 1))
> > > >
> > > > which produces the right result for me if `df' is a vector or a
> > > > data.frame:
> > > >
> > > > df <- c(11.08, 7.08, 7.08, 6.08, 6.08, 6.08, 23.08, 32.08, 8.08,
> 11.08,
> > > > 6.08, 13.08, 13.83, 16.83, 19.83, 8.83, 20.83, 17.83, 9.83, 20.83,
> > > > 10.83, 12.83, 15.83, 11.83)
> > > > ts(df, frequency = 12, start = c(1999, 1))
> > > > ts(as.data.frame(df), frequency = 12, start = c(1999, 1))
> > > >
> > > > > The resulting time series is as follows:
> > > > >
> > > > > Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
> > > > > 1999 2 15 15 14 14 14 12 13 16 2 14 5
> > > > > 2000 6 8 10 17 11 9 18 11 1 4 7 3
> > > > >
> > > > > I am unable to understand why the values of df and tsdata does not
> > match.
> > > >
> > > > So are we because you didn't really tell us enough about df...
> > > >
> > > > Best,
> > > > Z
> > > >
> > > > > I looked at ts function and I couldn't find any data transformation.
> > Am
> > > > > I missing something here? Any pointers would be of great help.
> > > > >
> > > > > Thanks in advance.
> > > > >
> > > > > Sachin
> > > > >
> > > > >
> > > > > ---------------------------------
> > > > >
> > > > > [[alternative HTML version deleted]]
> > > > >
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > > > >
> > > >
> > > >
> > > > __________________________________________________
> > > >
> > > >
> > > >
> > > > [[alternative HTML version deleted]]
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > > >
> > >
> >
> >
> >
> >
> > ________________________________
> > Do you Yahoo!?
> > Next-gen email? Have it all with the all-new Yahoo! Mail Beta.
> >
> >
>
>
>
>
> ________________________________
> Do you Yahoo!?
> Next-gen email? Have it all with the all-new Yahoo! Mail Beta.
>
>


From ggrothendieck at gmail.com  Mon Jun 26 17:28:48 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 26 Jun 2006 11:28:48 -0400
Subject: [R] Puzzled with contour()
In-Reply-To: <449FF4D2.1080009@stats.uwo.ca>
References: <20060625133337.GB24241@lubyanka.local>
	<449FEB40.6000405@stats.uwo.ca>
	<971536df0606260739o26a54ad1u389f56986ce460ac@mail.gmail.com>
	<449FF4D2.1080009@stats.uwo.ca>
Message-ID: <971536df0606260828y221cc4f2ta58668f221862342@mail.gmail.com>

I think its often the case that one has 3 tuples and does not know
how to use contour with that; so, it would be nice if the contour
help page gave advice and an example and a pointer to the relevant
functions if it cannot be done by contour.

On 6/26/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> On 6/26/2006 10:39 AM, Gabor Grothendieck wrote:
> > I think it would be helpful if this were added to the contour help file.
>
> You mean an example of building up the z matrix from points, or just a
> general discussion of the issue?
>
> Duncan Murdoch
> >
> > On 6/26/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> >> On 6/25/2006 9:33 AM, Ajay Narottam Shah wrote:
> >> > Folks,
> >> >
> >> > The contour() function wants x and y to be in increasing order. I have
> >> > a situation where I have a grid in x and y, and associated z values,
> >> > which looks like this:
> >>
> >> contour() wants vectors of x and y values, and a matrix of z values,
> >> where the x values correspond to the rows of z, and the y values to the
> >> columns.  You have a collection of points which need to be turned into
> >> such a grid.
> >>
> >> There's an interp function in the akima package that can do this in
> >> general.  In your case, it's probably sufficient to do something like this:
> >>
> >> zmat <- matrix(NA, 3, 19)
> >> zmat[cbind(20*x + 1, y/10 - 1)] <- z
> >> x <- (0:2)/20
> >> y <- (2:20)*10
> >> contour(x,y,zmat)
> >>
> >> Duncan Murdoch
> >>
> >>
> >> >
> >> >               x   y     z
> >> >       [1,] 0.00  20 1.000
> >> >       [2,] 0.00  30 1.000
> >> >       [3,] 0.00  40 1.000
> >> >       [4,] 0.00  50 1.000
> >> >       [5,] 0.00  60 1.000
> >> >       [6,] 0.00  70 1.000
> >> >       [7,] 0.00  80 0.000
> >> >       [8,] 0.00  90 0.000
> >> >       [9,] 0.00 100 0.000
> >> >      [10,] 0.00 110 0.000
> >> >      [11,] 0.00 120 0.000
> >> >      [12,] 0.00 130 0.000
> >> >      [13,] 0.00 140 0.000
> >> >      [14,] 0.00 150 0.000
> >> >      [15,] 0.00 160 0.000
> >> >      [16,] 0.00 170 0.000
> >> >      [17,] 0.00 180 0.000
> >> >      [18,] 0.00 190 0.000
> >> >      [19,] 0.00 200 0.000
> >> >      [20,] 0.05  20 1.000
> >> >      [21,] 0.05  30 1.000
> >> >      [22,] 0.05  40 1.000
> >> >      [23,] 0.05  50 1.000
> >> >      [24,] 0.05  60 0.998
> >> >      [25,] 0.05  70 0.124
> >> >      [26,] 0.05  80 0.000
> >> >      [27,] 0.05  90 0.000
> >> >      [28,] 0.05 100 0.000
> >> >      [29,] 0.05 110 0.000
> >> >      [30,] 0.05 120 0.000
> >> >      [31,] 0.05 130 0.000
> >> >      [32,] 0.05 140 0.000
> >> >      [33,] 0.05 150 0.000
> >> >      [34,] 0.05 160 0.000
> >> >      [35,] 0.05 170 0.000
> >> >      [36,] 0.05 180 0.000
> >> >      [37,] 0.05 190 0.000
> >> >      [38,] 0.05 200 0.000
> >> >      [39,] 0.10  20 1.000
> >> >      [40,] 0.10  30 1.000
> >> >
> >> > This looks like a nice case where both x and y are in increasing
> >> > order. But contour() gets unhappy saying that he wants x and y in
> >> > increasing order.
> >> >
> >> > Gnuplot generates pretty 3d pictures from such data, where you are
> >> > standing above a surface and looking down at it. How does one do that
> >> > in R?
> >> >
> >> > Any help will be most appreciated. A dput() of my data object is :
> >> >
> >> > structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> >> > 0, 0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,
> >> > 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1,
> >> > 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,
> >> > 0.1, 0.1, 0.1, 0.1, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,
> >> > 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,
> >> > 0.15, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
> >> > 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25,
> >> > 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,
> >> > 0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,
> >> > 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.35,
> >> > 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35,
> >> > 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.4, 0.4, 0.4, 0.4,
> >> > 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,
> >> > 0.4, 0.4, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,
> >> > 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.5,
> >> > 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,
> >> > 0.5, 0.5, 0.5, 0.5, 0.5, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55,
> >> > 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55,
> >> > 0.55, 0.55, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,
> >> > 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.65, 0.65, 0.65,
> >> > 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65,
> >> > 0.65, 0.65, 0.65, 0.65, 0.65, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7,
> >> > 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.75,
> >> > 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,
> >> > 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.8, 0.8, 0.8, 0.8,
> >> > 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,
> >> > 0.8, 0.8, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85,
> >> > 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.9,
> >> > 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,
> >> > 0.9, 0.9, 0.9, 0.9, 0.9, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,
> >> > 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,
> >> > 0.95, 0.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> >> > 1, 1, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140,
> >> > 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90,
> >> > 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30,
> >> > 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170,
> >> > 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120,
> >> > 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70,
> >> > 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200,
> >> > 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150,
> >> > 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100,
> >> > 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40,
> >> > 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180,
> >> > 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130,
> >> > 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80,
> >> > 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20,
> >> > 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160,
> >> > 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110,
> >> > 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50,
> >> > 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180,
> >> > 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130,
> >> > 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80,
> >> > 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20,
> >> > 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160,
> >> > 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110,
> >> > 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50,
> >> > 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180,
> >> > 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130,
> >> > 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80,
> >> > 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20,
> >> > 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160,
> >> > 170, 180, 190, 200, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
> >> > 0, 0, 0, 0, 0, 1, 1, 1, 1, 0.998, 0.124, 0, 0, 0, 0, 0, 0, 0,
> >> > 0, 0, 0, 0, 0, 0, 1, 1, 1, 0.998, 0.71, 0.068, 0, 0, 0, 0, 0,
> >> > 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0.998, 0.898, 0.396, 0.058, 0.002,
> >> > 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.998, 0.97, 0.726, 0.268,
> >> > 0.056, 0.006, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.996, 0.88,
> >> > 0.546, 0.208, 0.054, 0.012, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> >> > 0, 0, 0.998, 0.964, 0.776, 0.418, 0.18, 0.054, 0.014, 0.002,
> >> > 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.998, 0.906, 0.664, 0.342,
> >> > 0.166, 0.056, 0.018, 0.006, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> >> > 0, 0.986, 0.862, 0.568, 0.29, 0.15, 0.056, 0.022, 0.008, 0.002,
> >> > 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.954, 0.778, 0.494, 0.26, 0.148,
> >> > 0.056, 0.024, 0.012, 0.004, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> >> > 0.906, 0.712, 0.43, 0.242, 0.144, 0.058, 0.028, 0.012, 0.006,
> >> > 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.878, 0.642, 0.38, 0.222,
> >> > 0.142, 0.066, 0.034, 0.014, 0.008, 0.004, 0.002, 0, 0, 0, 0,
> >> > 0, 0, 0, 0, 0.846, 0.586, 0.348, 0.208, 0.136, 0.068, 0.034,
> >> > 0.016, 0.012, 0.006, 0.004, 0.002, 0, 0, 0, 0, 0, 0, 0, 0.8,
> >> > 0.538, 0.318, 0.204, 0.136, 0.07, 0.046, 0.024, 0.012, 0.008,
> >> > 0.004, 0.002, 0.002, 0, 0, 0, 0, 0, 0, 0.762, 0.496, 0.294, 0.2,
> >> > 0.138, 0.072, 0.05, 0.024, 0.014, 0.012, 0.006, 0.004, 0.002,
> >> > 0.002, 0, 0, 0, 0, 0, 0.704, 0.472, 0.286, 0.198, 0.138, 0.074,
> >> > 0.054, 0.028, 0.016, 0.012, 0.008, 0.006, 0.004, 0.002, 0.002,
> >> > 0, 0, 0, 0, 0.668, 0.438, 0.276, 0.196, 0.138, 0.078, 0.054,
> >> > 0.032, 0.024, 0.014, 0.012, 0.008, 0.004, 0.004, 0.002, 0.002,
> >> > 0, 0, 0, 0.634, 0.412, 0.27, 0.194, 0.14, 0.086, 0.056, 0.032,
> >> > 0.024, 0.016, 0.012, 0.01, 0.006, 0.004, 0.004, 0.002, 0.002,
> >> > 0, 0, 0.604, 0.388, 0.26, 0.19, 0.144, 0.088, 0.058, 0.048, 0.026,
> >> > 0.022, 0.014, 0.012, 0.008, 0.006, 0.004, 0.004, 0.002, 0.002,
> >> > 0, 0.586, 0.376, 0.256, 0.19, 0.146, 0.094, 0.062, 0.052, 0.028,
> >> > 0.024, 0.014, 0.012, 0.012, 0.008, 0.004, 0.004, 0.004, 0.002,
> >> > 0.002, 0.566, 0.364, 0.254, 0.192, 0.148, 0.098, 0.064, 0.054,
> >> > 0.032, 0.024, 0.022, 0.014, 0.012, 0.012, 0.008, 0.004, 0.004,
> >> > 0.004, 0.002), .Dim = c(399, 3), .Dimnames = list(NULL, c("x",
> >> > "y", "z")))
> >> >
> >> > --
> >> > Ajay Shah                                      http://www.mayin.org/ajayshah
> >> > ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
> >> > <*(:-? - wizard who doesn't know the answer.
> >> >
> >> > ______________________________________________
> >> > R-help at stat.math.ethz.ch mailing list
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>


From p.dalgaard at biostat.ku.dk  Mon Jun 26 17:30:20 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Jun 2006 17:30:20 +0200
Subject: [R] converting to time series object : ts - package:stats
In-Reply-To: <20060626150325.62071.qmail@web37613.mail.mud.yahoo.com>
References: <20060626150325.62071.qmail@web37613.mail.mud.yahoo.com>
Message-ID: <x2d5cwrl2r.fsf@viggo.kubism.ku.dk>

Sachin J <sachinj.2006 at yahoo.com> writes:

> Hi Gabor,
>    
>   You are correct. The real problem is with read.csv. I am not sure why? My data looks 
>    
>   V1,V2,V3
> 11.08,21.73,13.08
> 7.08,37.73,6.08
> 7.08,11.73,21.08
....
> > read.csv("C:/Data.csv",header=TRUE,sep=",",na.strings="NA",
> > dec=",", strip.white=TRUE)

If that's how it looks, then dec="," is wrong. It sets the decimal
separator, so expects 11,08 etc. You want dec=".". In general, having
sep and dec set to the same value is asking for trouble, and you
didn't actually need to depart from the default settings, except for
the strip.white bit.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From spencer.graves at pdf.com  Mon Jun 26 17:51:20 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 26 Jun 2006 08:51:20 -0700
Subject: [R] lmer and mixed effects logistic regression
In-Reply-To: <1151278662.3366.23.camel@localhost.localdomain>
References: <1150293574.3416.11.camel@localhost.localdomain>
	<449431F1.6000906@pdf.com>
	<40e66e0b0606180458p177fa824j6a181fd52c3d1277@mail.gmail.com>
	<1150726454.3200.20.camel@localhost.localdomain>
	<148ed8180606201127p730e699emb9e901b09a7ab039@mail.gmail.com>
	<1150894770.3329.13.camel@localhost.localdomain>
	<44996733.9050609@pdf.com>
	<1150930911.3329.42.camel@localhost.localdomain>
	<449CC1B3.9040305@pdf.com>
	<1151278662.3366.23.camel@localhost.localdomain>
Message-ID: <44A00278.1040403@pdf.com>

<see inline>

Rick Bilonick wrote:
> On Fri, 2006-06-23 at 21:38 -0700, Spencer Graves wrote:
>> 	  Permit me to try to repeat what I said earlier a little more clearly: 
>>   When the outcomes are constant for each subject, either all 0's or all 
>> 1's, the maximum likelihood estimate of the between-subject variance in 
>> Inf.  Any software that returns a different answer is wrong. This is NOT 
>> a criticism of 'lmer' or SAS NLMIXED:  This is a sufficiently rare, 
>> extreme case that the software does not test for it and doesn't handle 
>> it well when it occurs.  Adding other explanatory variables to the model 
>> only makes this problem worse, because anything that will produce 
>> complete separation for each subject will produce this kind of 
>> instability.
>>
>> 	 Consider the following:
>>
>> library(lme4)
>> DF <- data.frame(y=c(0,0, 0,1, 1,1),
>>                   Subj=rep(letters[1:3], each=2),
>>                   x=rep(c(-1, 1), 3))
>> fit1 <- lmer(y~1+(1|Subj), data=DF, family=binomial)
>>
>> # 'lmer' works fine here, because the outcomes from
>> # 1 of the 3 subjects is not constant.
>>
>>  > fit.x <- lmer(y~x+(1|Subj), data=DF, family=binomial)
>> Warning message:
>> IRLS iterations for PQL did not converge
>>
>> 	  The addition of 'x' to the model now allows complete separation for 
>> each subject.  We see this in the result:
>>
>> Generalized linear mixed model fit using PQL
>> <snip>
>> Random effects:
>>   Groups Name        Variance   Std.Dev.
>>   Subj   (Intercept) 3.5357e+20 1.8803e+10
>> number of obs: 6, groups: Subj, 3
>>
>> Estimated scale (compare to 1)  9.9414e-09
>>
>> Fixed effects:
>>                 Estimate  Std. Error    z value Pr(>|z|)
>> (Intercept) -5.4172e-05  1.0856e+10  -4.99e-15        1
>> x            8.6474e+01  2.7397e+07 3.1563e-06        1
>>
>> 	  Note that the subject variance is 3.5e20, the estimate for x is 86 
>> wit a standard error of 2.7e7.  All three of these numbers are reaching 
>> for Inf;  lmer quit before it got there.
>>
>> 	  Does this make any sense, or are we still misunderstanding one another?
>>
>> 	  Hope this helps.
>> 	  Spencer Graves
>>
> Yes, thanks, it's clear. I had created a new data set that has each
> subject with just one observation and randomly sampled one observation
> from each subject with two observations (they are right and left eyes).
> I'm not sure why lmer gives small estimated variances for the random
> effects when it should be infinite. 

SG:  If lmer gave me small estimated variances for the random effects, I 
would check very carefully my model, as I would believe I probably have 
specified something incorrectly.

I ran NLMIXED on the original data
> set with several explanatory factors and the variance component was in
> the thousands.
> 
> I guess the moral is before you do any computations you have to make
> sure the procedure makes sense for the data.
> 
> Rick B.
>


From eribaron at yahoo.com.br  Mon Jun 26 15:59:57 2006
From: eribaron at yahoo.com.br (Erica Baron)
Date: Mon, 26 Jun 2006 10:59:57 -0300 (ART)
Subject: [R] Plylogenetic analysis
Message-ID: <20060626135957.23924.qmail@web61225.mail.yahoo.com>

Dear coleges,
How to use the genotype from microsatelite markers
from many different populations to construct a
tridimensional phylogenetic tree with R? Any
suggestions?
Thank you very much!
Baron, Erica


 
Dra. Erica Baron
Universidade dos A?ores - Departamento de Ci?ncias Agr?rias
Grupo de Biotecnologia - Campus de Angra
Terra-Ch?
9701-851 
A?ores - Portugal
Tel. +351 295 402 235
Fax. +351 295 402 205
eribaron at mail.angra.uac.pt


From gunter.berton at gene.com  Mon Jun 26 18:05:12 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 26 Jun 2006 09:05:12 -0700
Subject: [R] lmer and mixed effects logistic regression
In-Reply-To: <44A00278.1040403@pdf.com>
Message-ID: <001501c6993a$4efc0550$f8c2fea9@gne.windows.gene.com>

> Rick Bilonick wrote:

> > I guess the moral is before you do any computations you have to make
> > sure the procedure makes sense for the data.
> > 

Is this a candidate for the fortunes package? (an oxymoronic profound, but
obvious comment).




-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA


From Greg.Snow at intermountainmail.org  Mon Jun 26 18:10:57 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 26 Jun 2006 10:10:57 -0600
Subject: [R] R Reporting - PDF/HTML mature presentation quality package?
Message-ID: <07E228A5BE53C24CAD490193A7381BBB45E275@LP-EXCHVS07.CO.IHC.COM>

The R2HTML package does have a driver for sweave (see help for
RweaveHTML).  This allows you to write an HTML file and add the R
commands you want (use <<>>=, @ combinations to indicate R commands and
<Sexpr r-code> for inline replacement), then process it with sweave and
have a final HTML file (and possibly additional graphic files for the
links) with the R output included. 

Is this what you were looking for?

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jim Lemon
Sent: Sunday, June 25, 2006 5:55 PM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] R Reporting - PDF/HTML mature presentation quality
package?

I heartily second Phillipe's response. I just started a new job and the
first thing required was a neat stats report for a dataset. I thought I
would give R2HTML a try and about 5 minutes after downloading it, I was
looking at the first draft of the report. I did have to do a bit of
hacking on the graphics, but it was easy and I can now present the
report first thing in the morning. Had I not been able to do this, I
probably would have been told, "You'll have to use SPSS."

I was so impressed by R2HTML that I began writing a primitive HTML
generator that will scan an R script and do something like R2HTML. I
couldn't find anything like this as the svMisc package seems to have
disappeared. If anyone knows of something like this or is working on it,
I'd appreciate knowing about it.

Jim

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From amsa36060 at yahoo.com  Mon Jun 26 18:14:17 2006
From: amsa36060 at yahoo.com (Amir Safari)
Date: Mon, 26 Jun 2006 09:14:17 -0700 (PDT)
Subject: [R] How to generate a figure using par( ) with some densityplot( )'s
Message-ID: <20060626161417.50920.qmail@web60419.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060626/f2ae1f5e/attachment.pl 

From ezhil02 at yahoo.com  Mon Jun 26 18:17:27 2006
From: ezhil02 at yahoo.com (A Ezhil)
Date: Mon, 26 Jun 2006 09:17:27 -0700 (PDT)
Subject: [R] Finding a color code.
Message-ID: <20060626161727.52909.qmail@web32110.mail.mud.yahoo.com>

Hi,

Is it possible to find corresponding color code in R
for the following RGB (R185, G35 & B80)? 

Thanks in advance.

Best regards,
Ezhil


From sundar.dorai-raj at pdf.com  Mon Jun 26 18:20:01 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 26 Jun 2006 11:20:01 -0500
Subject: [R] How to generate a figure using par( ) with some
 densityplot( )'s
In-Reply-To: <20060626161417.50920.qmail@web60419.mail.yahoo.com>
References: <20060626161417.50920.qmail@web60419.mail.yahoo.com>
Message-ID: <44A00931.4000102@pdf.com>



Amir Safari wrote:
>  
>   Hi Dear R users,
>   For a pair plotting, usaully we use par( ) function. Apparently it does not work anywhere. I want to have 3 plots in a single figure, like this:
>   par(mfrow=c(3,1))
>   densityplot( a) 
>   densityplot(b)
>   densityplot(c)
>   But it does not work. How is it possible to have such a figure with densityplot( ) in a single figure?
>   So many thanks for any help.
>   Amir Safari
>    
> 
>  		

Assuming you are talking about densityplot in lattice, then you are 
missing the point of lattice. You should try:

library(lattice)
set.seed(1)
a <- rnorm(100)
b <- rnorm(50)
c <- rnorm(75)
densityplot(~a + b + c, outer = TRUE, layout = c(3, 1))

Set "outer" to FALSE to overlay the densities (this is the default 
behavior). You should remove the "layout" argument in that case, though.

HTH,

--sundar


From sundar.dorai-raj at pdf.com  Mon Jun 26 18:25:09 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 26 Jun 2006 11:25:09 -0500
Subject: [R] Finding a color code.
In-Reply-To: <20060626161727.52909.qmail@web32110.mail.mud.yahoo.com>
References: <20060626161727.52909.qmail@web32110.mail.mud.yahoo.com>
Message-ID: <44A00A65.5060408@pdf.com>



A Ezhil wrote:
> Hi,
> 
> Is it possible to find corresponding color code in R
> for the following RGB (R185, G35 & B80)? 
> 
> Thanks in advance.
> 
> Best regards,
> Ezhil
> 


How about:

x <- c(185, 35, 80)
class(x) <- "hexmode"
paste("#", paste(format(x), collapse = ""), sep = "")
[1] "#b92350"

I found this using

help.search("hex")

which led to ?format.hexmode.

HTH,

--sundar


From sarah.goslee at gmail.com  Mon Jun 26 18:29:15 2006
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 26 Jun 2006 12:29:15 -0400
Subject: [R] Finding a color code.
In-Reply-To: <20060626161727.52909.qmail@web32110.mail.mud.yahoo.com>
References: <20060626161727.52909.qmail@web32110.mail.mud.yahoo.com>
Message-ID: <efb536d50606260929h2133cce2v45818f0c9611df50@mail.gmail.com>

Well, rgb(185, 35, 80, max=255) seems like a good place
to start.

help.search("rgb")
turns up all kinds of color-related functions.

Sarah

On 6/26/06, A Ezhil <ezhil02 at yahoo.com> wrote:
> Hi,
>
> Is it possible to find corresponding color code in R
> for the following RGB (R185, G35 & B80)?
>
> Thanks in advance.
>
> Best regards,
> Ezhil
>

-- 
Sarah Goslee


From bolker at zoo.ufl.edu  Mon Jun 26 18:32:07 2006
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Mon, 26 Jun 2006 12:32:07 -0400
Subject: [R] X11 font troubles (Knoppix/Debian unstable)
Message-ID: <44A00C07.5000202@zoo.ufl.edu>


  I'm getting

Error in strwidth(...) : X11 font at size 15 could not be loaded

 kinds of errors when I try to change cex to something *other*
than 1 (works OK otherwise).

  I can't get through to RSiteSearch() right now, but as I recall
my search of it only revealed people who had problems because they
had failed to install fonts: my Knoppix/tracking Debian unstable
distribution claims that the most recent versions of
xfonts-100dpi and xfonts-75dpi are
both installed. (version 1:1.0.0-2)  I'm using the Xorg version
of X11.


xset -q says:

/usr/X11R6/lib/X11/fonts/misc:unscaled,/usr/X11R6/lib/X11/fonts/misc,/usr/X11R6/lib/X11/fonts/75dpi:unscaled,
/usr/X11R6/lib/X11/fonts/75dpi,/usr/X11R6/lib/X11/fonts/100dpi:unscaled,/usr/X11R6/lib/X11/fonts/100dpi,
/usr/X11R6/lib/X11/fonts/Speedo,/usr/X11R6/lib/X11/fonts/Type1,/usr/share/fonts/ttf/western,
/usr/share/fonts/ttf/decoratives,/usr/share/fonts/truetype/ttf-bitstream-vera,/usr/share/fonts/latex-ttf-fonts,
/usr/share/fonts/X11/misc/,/usr/share/fonts/X11/Type1/,/usr/share/fonts/X11/100dpi/,/usr/share/fonts/X11/75dpi

  I haven't noticed any other applications on my system having
problems finding fonts, not that that's a very strong test.

  Any ideas about where to start looking/diagnosing?

  thanks
    Ben Bolker


From Mike.Prager at noaa.gov  Mon Jun 26 18:38:08 2006
From: Mike.Prager at noaa.gov (Michael H. Prager)
Date: Mon, 26 Jun 2006 12:38:08 -0400
Subject: [R] Finding a color code.
In-Reply-To: <20060626161727.52909.qmail@web32110.mail.mud.yahoo.com>
References: <20060626161727.52909.qmail@web32110.mail.mud.yahoo.com>
Message-ID: <44A00D70.7090707@noaa.gov>

 > ?rgb

You will need to divide your values by the maximum or specify it, as in

 > rgb(185, 35, 80, max=255)


on 6/26/2006 12:17 PM A Ezhil said the following:
> Is it possible to find corresponding color code in R
> for the following RGB (R185, G35 & B80)? 
>
>   

-- 
Michael Prager, Ph.D.
Southeast Fisheries Science Center
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
** Opinions expressed are personal, not official.  No
** official endorsement of any product is made or implied.


From Greg.Snow at intermountainmail.org  Mon Jun 26 18:40:06 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 26 Jun 2006 10:40:06 -0600
Subject: [R] Finding a color code.
Message-ID: <07E228A5BE53C24CAD490193A7381BBB45E27A@LP-EXCHVS07.CO.IHC.COM>

You can use the following:

> rgb(185, 35, 80, max=255)

Which gives "#B92350"

Or if you want a color name, the closest I found is "maroon" which is
red: 176, green: 48, blue: 96

Hope this helps, 


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of A Ezhil
Sent: Monday, June 26, 2006 10:17 AM
To: R-help at stat.math.ethz.ch
Subject: [R] Finding a color code.

Hi,

Is it possible to find corresponding color code in R for the following
RGB (R185, G35 & B80)? 

Thanks in advance.

Best regards,
Ezhil

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From deepayan.sarkar at gmail.com  Mon Jun 26 18:46:00 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 26 Jun 2006 11:46:00 -0500
Subject: [R] How to generate a figure using par( ) with some
	densityplot( )'s
In-Reply-To: <44A00931.4000102@pdf.com>
References: <20060626161417.50920.qmail@web60419.mail.yahoo.com>
	<44A00931.4000102@pdf.com>
Message-ID: <eb555e660606260946k69a0eadeq31985594c9d9aaa7@mail.gmail.com>

On 6/26/06, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
>
>
> Amir Safari wrote:
> >
> >   Hi Dear R users,
> >   For a pair plotting, usaully we use par( ) function. Apparently it does not work anywhere. I want to have 3 plots in a single figure, like this:
> >   par(mfrow=c(3,1))
> >   densityplot( a)
> >   densityplot(b)
> >   densityplot(c)
> >   But it does not work. How is it possible to have such a figure with densityplot( ) in a single figure?
> >   So many thanks for any help.
> >   Amir Safari
> >
> >
> >
>
> Assuming you are talking about densityplot in lattice, then you are
> missing the point of lattice. You should try:
>
> library(lattice)
> set.seed(1)
> a <- rnorm(100)
> b <- rnorm(50)
> c <- rnorm(75)
> densityplot(~a + b + c, outer = TRUE, layout = c(3, 1))

This only works if a, b and c are of the same length. The following
should work though:

densityplot(~data | which,
            data = make.groups(a, b, c))

-Deepayan

> Set "outer" to FALSE to overlay the densities (this is the default
> behavior). You should remove the "layout" argument in that case, though.
>
> HTH,
>
> --sundar


-- 
http://www.stat.wisc.edu/~deepayan/


From ccleland at optonline.net  Mon Jun 26 18:52:43 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 26 Jun 2006 12:52:43 -0400
Subject: [R] Finding a color code.
In-Reply-To: <20060626161727.52909.qmail@web32110.mail.mud.yahoo.com>
References: <20060626161727.52909.qmail@web32110.mail.mud.yahoo.com>
Message-ID: <44A010DB.8080307@optonline.net>

A Ezhil wrote:
> Hi,
> 
> Is it possible to find corresponding color code in R
> for the following RGB (R185, G35 & B80)? 

rgb(185, 35, 80, max=255)
[1] "#B92350"

?rgb

> Thanks in advance.
> 
> Best regards,
> Ezhil
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From ezhil02 at yahoo.com  Mon Jun 26 18:54:55 2006
From: ezhil02 at yahoo.com (A Ezhil)
Date: Mon, 26 Jun 2006 09:54:55 -0700 (PDT)
Subject: [R] Finding a color code.
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB45E27A@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <20060626165455.88932.qmail@web32105.mail.mud.yahoo.com>

Hi Greg,

Thank you very much for your response. It works.
Thanks also for Michael,Sarah & Sundar.

Best regards,
Ezhil  

 
--- Greg Snow <Greg.Snow at intermountainmail.org> wrote:

> You can use the following:
> 
> > rgb(185, 35, 80, max=255)
> 
> Which gives "#B92350"
> 
> Or if you want a color name, the closest I found is
> "maroon" which is
> red: 176, green: 48, blue: 96
> 
> Hope this helps, 
> 
> 
> -- 
> Gregory (Greg) L. Snow Ph.D.
> Statistical Data Center
> Intermountain Healthcare
> greg.snow at intermountainmail.org
> (801) 408-8111
>  
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf
> Of A Ezhil
> Sent: Monday, June 26, 2006 10:17 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Finding a color code.
> 
> Hi,
> 
> Is it possible to find corresponding color code in R
> for the following
> RGB (R185, G35 & B80)? 
> 
> Thanks in advance.
> 
> Best regards,
> Ezhil
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
>


From ligges at statistik.uni-dortmund.de  Mon Jun 26 19:22:55 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 26 Jun 2006 19:22:55 +0200
Subject: [R] Package struction question (second try)
In-Reply-To: <71257D09F114DA4A8E134DEAC70F25D3056E95A9@groamrexm03.amer.pfizer.com>
References: <71257D09F114DA4A8E134DEAC70F25D3056E95A9@groamrexm03.amer.pfizer.com>
Message-ID: <44A017EF.5050800@statistik.uni-dortmund.de>

Kuhn, Max wrote:

> Jay,
> 
> You should use "RCMD install --build pkgName" to create the zip file on
> Windows. The zip files you see on CRAN are Windows binaries. You could
> also used "RCMD build pkgName", but I remember seeing a post a while
> back saying that using install instead of build was best (anyone - is
> that true?).

Yes,

   R CMD INSTALL --build

is preferable to

   R CMD build --binary

but

   R CMD build

(without "--binary") builds a source package rather than a binary package.

Uwe Ligges


> 
> See you next week in Groton,
> 
> Max
> 
> <snip>
> 
> Sorry, gmail seemed to have made an attachment out of my first attempted
> 
> post.  Trying again:
> 
> ------------------------------
> 
> At the encouragement of many at UseR, I'm trying to build my first real
> package. I have no C/Fortran code, just plain old R code, so it should
> be
> rocket science.  On a Linux box, I used package.skeleton() to create a 
> basic package containing just one "hello world" type of function.  I 
> edited the DESCRIPTION file, changin the package name appropriately.  I 
> edited the hello.Rd file.  Upon running R CMD check hello, the only 
> warning had to do with the fact that src/ was empty (obviously I had no 
> source in such a simple package).  I doubt this is a problem.
> 
> I was able to install and use the package successfully on the Linux
> system
> from the .tar.gz file, so far so good!  Next, on to Windows, where the
> problem arose:
> 
> I created a zip file from inside the package directory:
> 
> zip -r ../hello.zip ./*
> 
> When I moved this to my Windows machine and tried to install the package
> 
> using the GUI, I received the following error:
> 
> 
>>utils:::menuInstallLocal()
> 
> Error in unpackPkg(pkgs[i], pkgnames[i], lib, installWithVers) :
>          malformed bundle DESCRIPTION file, no Contains field
> 
> I only found one mention of this in my Google search, with no reply to
> the
> thread.  The Contains field appears to be used for bundles, but I'm
> trying
> to create a package, not a bundle.  This leads me to believe that a
> simple
> zipping of the package directory structure is not the correct format for
> Windows.
> 
> Needless to say, there appears to be wide agreement that making packages
> requires precision, but fundamentally a package should (as described in 
> the
> documentation) just be a collection of files and folders organized a 
> certain
> way.  If someone could point me to documentation I may have missed that
> explains this, I would be grateful.
> 
> Regards,
> 
> Jay
>


From arrayprofile at yahoo.com  Mon Jun 26 19:21:36 2006
From: arrayprofile at yahoo.com (array chip)
Date: Mon, 26 Jun 2006 10:21:36 -0700 (PDT)
Subject: [R] comparing 2 odds ratios
Message-ID: <20060626172136.62356.qmail@web35704.mail.mud.yahoo.com>

Hi there, is there any way to compare 2 odds ratios? I
have two tests that are supposed to detect a disease
presence. So for each test, I can compute an odds
ratio. My problem is how can I compare the 2 tests by
testing whether the 2 odds ratios are the same?

Appreciate


From ff809 at ncf.ca  Mon Jun 26 19:25:18 2006
From: ff809 at ncf.ca (Brian Lunergan)
Date: Mon, 26 Jun 2006 13:25:18 -0400
Subject: [R] Problem running one of the rgl demo scripts...
Message-ID: <44A0187E.4010505@ncf.ca>

Afternoon folks:

I'm getting a program crash when I try to run demo(rgl). The following 
error details result:

RGUI caused a stack fault in module NVOPENGL.DLL at 017f:695280f0.
Registers:
EAX=00000002 CS=017f EIP=695280f0 EFLGS=00010246
EBX=00000001 SS=0187 ESP=00572000 EBP=004d1208
ECX=042f1f01 DS=0187 ESI=004d1208 FS=5d1f
EDX=00442d84 ES=0187 EDI=042f1f3c GS=5d0e
Bytes at CS:EIP:
53 56 8b 5c 24 0c 57 83 bb 9c 39 00 00 02 0f 84
Stack dump:
69541be5 004d1208 004d1208 00000001 69541bef 00000001 004d1208 00000001 
69541bef 00000001 004d1208 00000001 69541bef 00000001 004d1208 00000001

Setup details are:

R version: 2.2.1
OS: win98se
RGL file name: rgl_0.66.zip

It appears to be gagging on an Nvidia opengl driver file, NVIDIA Compatible 
OpenGL ICD, version 4.12.01.0631. The video card is recorded as:

3DForce2 MX Series,NVIDIA GeForce2 MX (Ver 4.12.01.0631 ,9/20/2000)

I also tried this with version 2.3.1 of R with the same results.

Anyone have any thoughts or ideas on the subject? Has it occurred any place 
else? Is there a workaround or solution, or should I perhaps turf the 
package and forgo its abilities since it appears my system as it stands may 
not be able to support it?

-- 
Brian Lunergan
Nepean, Ontario
Canada


---
avast! Antivirus: Outbound message clean.
Virus Database (VPS): 0626-0, 2006-06-26
Tested on: 2006-06-26 13:25:48
avast! is copyright (c) 2000-2006 ALWIL Software.
http://www.avast.com


From sundar.dorai-raj at pdf.com  Mon Jun 26 19:41:21 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 26 Jun 2006 12:41:21 -0500
Subject: [R] How to generate a figure using par( ) with some
 densityplot( )'s
In-Reply-To: <eb555e660606260946k69a0eadeq31985594c9d9aaa7@mail.gmail.com>
References: <20060626161417.50920.qmail@web60419.mail.yahoo.com>	
	<44A00931.4000102@pdf.com>
	<eb555e660606260946k69a0eadeq31985594c9d9aaa7@mail.gmail.com>
Message-ID: <44A01C41.60502@pdf.com>


Deepayan Sarkar wrote:
> On 6/26/06, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
> 
>>
>>
>> Amir Safari wrote:
>> >
>> >   Hi Dear R users,
>> >   For a pair plotting, usaully we use par( ) function. Apparently it 
>> does not work anywhere. I want to have 3 plots in a single figure, 
>> like this:
>> >   par(mfrow=c(3,1))
>> >   densityplot( a)
>> >   densityplot(b)
>> >   densityplot(c)
>> >   But it does not work. How is it possible to have such a figure 
>> with densityplot( ) in a single figure?
>> >   So many thanks for any help.
>> >   Amir Safari
>> >
>> >
>> >
>>
>> Assuming you are talking about densityplot in lattice, then you are
>> missing the point of lattice. You should try:
>>
>> library(lattice)
>> set.seed(1)
>> a <- rnorm(100)
>> b <- rnorm(50)
>> c <- rnorm(75)
>> densityplot(~a + b + c, outer = TRUE, layout = c(3, 1))
> 
> 
> This only works if a, b and c are of the same length. The following
> should work though:
> 
> densityplot(~data | which,
>            data = make.groups(a, b, c))
> 
> -Deepayan
> 

Hi, Deepayan,

My mistake. This is clear in ?densityplot. However, there is no warning 
if the condition is not met and, apparently, recycling rules are applied.

Thanks,

--sundar


From murdoch at stats.uwo.ca  Mon Jun 26 19:41:19 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 26 Jun 2006 13:41:19 -0400
Subject: [R] Problem running one of the rgl demo scripts...
In-Reply-To: <44A0187E.4010505@ncf.ca>
References: <44A0187E.4010505@ncf.ca>
Message-ID: <44A01C3F.20304@stats.uwo.ca>

Brian Lunergan wrote:
> Afternoon folks:
>
> I'm getting a program crash when I try to run demo(rgl). The following 
> error details result:
>
> RGUI caused a stack fault in module NVOPENGL.DLL at 017f:695280f0.
> Registers:
> EAX=00000002 CS=017f EIP=695280f0 EFLGS=00010246
> EBX=00000001 SS=0187 ESP=00572000 EBP=004d1208
> ECX=042f1f01 DS=0187 ESI=004d1208 FS=5d1f
> EDX=00442d84 ES=0187 EDI=042f1f3c GS=5d0e
> Bytes at CS:EIP:
> 53 56 8b 5c 24 0c 57 83 bb 9c 39 00 00 02 0f 84
> Stack dump:
> 69541be5 004d1208 004d1208 00000001 69541bef 00000001 004d1208 00000001 
> 69541bef 00000001 004d1208 00000001 69541bef 00000001 004d1208 00000001
>
> Setup details are:
>
> R version: 2.2.1
> OS: win98se
> RGL file name: rgl_0.66.zip
>
> It appears to be gagging on an Nvidia opengl driver file, NVIDIA Compatible 
> OpenGL ICD, version 4.12.01.0631. The video card is recorded as:
>
> 3DForce2 MX Series,NVIDIA GeForce2 MX (Ver 4.12.01.0631 ,9/20/2000)
>
> I also tried this with version 2.3.1 of R with the same results.
>
> Anyone have any thoughts or ideas on the subject? Has it occurred any place 
> else? Is there a workaround or solution, or should I perhaps turf the 
> package and forgo its abilities since it appears my system as it stands may 
> not be able to support it?
You could try a newer build, available on my web page 
(http://www.stats.uwo.ca/faculty/murdoch/software).  If it dies the same 
way, you could perhaps try to diagnose what is going wrong and send a 
patch if it's an rgl bug.  Given the age of your video driver 
(9/20/2000), you might be able to update it and fix a bug there.

Duncan Murdoch


From markleeds at verizon.net  Mon Jun 26 19:40:19 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Mon, 26 Jun 2006 12:40:19 -0500 (CDT)
Subject: [R] probably need to se sapply but i can't get it
Message-ID: <18053374.1191211151343620329.JavaMail.root@vms071.mailsrvcs.net>

Hi : I think I need to use sapply but I can't figure this out.

Suppose I have two vectors : tempa ( 4, 6,10 ) and  tempb 
(  11,23 ,39 ) 


I want a function that returns 4:11,6:23 and 10:39 as vectors.

I tried :

sapply(1:length(tempa) function (z) seq(tempa[z],tempb[z])

but i got 3 really strange vectors back in the sense that the numbers in them did not make no sense to me. obviously,
i must be doing something wrong.  thanks a lot.

                                       mark


From mschwartz at mn.rr.com  Mon Jun 26 19:54:15 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 26 Jun 2006 12:54:15 -0500
Subject: [R] probably need to se sapply but i can't get it
In-Reply-To: <18053374.1191211151343620329.JavaMail.root@vms071.mailsrvcs.net>
References: <18053374.1191211151343620329.JavaMail.root@vms071.mailsrvcs.net>
Message-ID: <1151344455.8219.53.camel@localhost.localdomain>

On Mon, 2006-06-26 at 12:40 -0500, markleeds at verizon.net wrote:
> Hi : I think I need to use sapply but I can't figure this out.
> 
> Suppose I have two vectors : tempa ( 4, 6,10 ) and  tempb 
> (  11,23 ,39 ) 
> 
> 
> I want a function that returns 4:11,6:23 and 10:39 as vectors.
> 
> I tried :
> 
> sapply(1:length(tempa) function (z) seq(tempa[z],tempb[z])
> 
> but i got 3 really strange vectors back in the sense that the numbers
> in them did not make no sense to me. obviously,
> i must be doing something wrong.  thanks a lot.
> 
>                                        mark


Mark,

Try this using mapply():

 tempa <- c(4, 6, 10)

 tempb <- c(11, 23, 39)


> mapply(seq, from = tempa, to = tempb)
[[1]]
[1]  4  5  6  7  8  9 10 11

[[2]]
 [1]  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23

[[3]]
 [1] 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31
[23] 32 33 34 35 36 37 38 39


You will get a list back in this case and you can then deal with the 3
vectors as you require. Each vector is a different length, so a list is
about the only way to return them here.

See ?mapply for more info.

HTH,

Marc Schwartz


From karloh at mi.uib.no  Mon Jun 26 19:57:37 2006
From: karloh at mi.uib.no (Karl Ove Hufthammer)
Date: Mon, 26 Jun 2006 19:57:37 +0200
Subject: [R] Overlaying 2D kernel density plots on scatterplot matrix
Message-ID: <200606261957.39409.karloh@mi.uib.no>

Michael Hopkins skreiv:

> The kernel density plots don?t have to be very sophisticated i.e.
> default settings and greyscale are fine ? we can work on details
> later.??What?has stumped us so far is how you ?attach? the kernel
> density results to the scatterplot results and then overlay them.
> 
> Any ideas, links or code gratefully received.

Here?s my suggestion?:

library(lattice)
library(MASS)

splom(~iris[1:4], panel=function(x,y)
{
   xy=kde2d(x,y)
   xy.tr=con2tr(xy)
   panel.contourplot(xy.tr$x, xy.tr$y, xy.tr$z,
                     subscripts=seq(nrow(xy.tr)),
                     contour=TRUE, region=FALSE)
   panel.xyplot(x,y)
}
)

? Which is basically Richard M. Heiberger?s solution to a similar
  query I had on this list earlier about a month ago:
  http://tolstoy.newcastle.edu.au/R/help/06/05/27184.html

-- 
Karl Ove Hufthammer


From tlumley at u.washington.edu  Mon Jun 26 19:59:30 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 26 Jun 2006 10:59:30 -0700 (PDT)
Subject: [R] probably need to se sapply but i can't get it
In-Reply-To: <18053374.1191211151343620329.JavaMail.root@vms071.mailsrvcs.net>
References: <18053374.1191211151343620329.JavaMail.root@vms071.mailsrvcs.net>
Message-ID: <Pine.LNX.4.64.0606261056580.13179@homer21.u.washington.edu>

On Mon, 26 Jun 2006, markleeds at verizon.net wrote:

> Hi : I think I need to use sapply but I can't figure this out.
>
> Suppose I have two vectors : tempa ( 4, 6,10 ) and  tempb
> (  11,23 ,39 )
>
>
> I want a function that returns 4:11,6:23 and 10:39 as vectors.
>
> I tried :
>
> sapply(1:length(tempa) function (z) seq(tempa[z],tempb[z])
>
> but i got 3 really strange vectors back in the sense that the numbers in them did not make no sense to me. obviously,
> i must be doing something wrong.  thanks a lot.

An easier way to do this is
   mapply(seq,tempa,tempb)

Your approach should have worked. It's hard to tell why it didn't because 
there are two syntax errors in your example so it clearly isn't actually 
what you did.  Fixing them, I get
> sapply(1:length(tempa), function (z) seq(tempa[z],tempb[z]))
[[1]]
[1]  4  5  6  7  8  9 10 11

[[2]]
  [1]  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23

[[3]]
  [1] 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34
[26] 35 36 37 38 39

as you wanted.

 	-thomas


From deepayan.sarkar at gmail.com  Mon Jun 26 20:17:30 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 26 Jun 2006 13:17:30 -0500
Subject: [R] How to generate a figure using par( ) with some
	densityplot( )'s
In-Reply-To: <44A01C41.60502@pdf.com>
References: <20060626161417.50920.qmail@web60419.mail.yahoo.com>
	<44A00931.4000102@pdf.com>
	<eb555e660606260946k69a0eadeq31985594c9d9aaa7@mail.gmail.com>
	<44A01C41.60502@pdf.com>
Message-ID: <eb555e660606261117y7d6a2494me40d1aa7794254a9@mail.gmail.com>

On 6/26/06, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
>
> Deepayan Sarkar wrote:
> > On 6/26/06, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
> >
> >>
> >>
> >> Amir Safari wrote:
> >> >
> >> >   Hi Dear R users,
> >> >   For a pair plotting, usaully we use par( ) function. Apparently it
> >> does not work anywhere. I want to have 3 plots in a single figure,
> >> like this:
> >> >   par(mfrow=c(3,1))
> >> >   densityplot( a)
> >> >   densityplot(b)
> >> >   densityplot(c)
> >> >   But it does not work. How is it possible to have such a figure
> >> with densityplot( ) in a single figure?
> >> >   So many thanks for any help.
> >> >   Amir Safari
> >> >
> >> >
> >> >
> >>
> >> Assuming you are talking about densityplot in lattice, then you are
> >> missing the point of lattice. You should try:
> >>
> >> library(lattice)
> >> set.seed(1)
> >> a <- rnorm(100)
> >> b <- rnorm(50)
> >> c <- rnorm(75)
> >> densityplot(~a + b + c, outer = TRUE, layout = c(3, 1))
> >
> >
> > This only works if a, b and c are of the same length. The following
> > should work though:
> >
> > densityplot(~data | which,
> >            data = make.groups(a, b, c))
> >
> > -Deepayan
> >
>
> Hi, Deepayan,
>
> My mistake. This is clear in ?densityplot. However, there is no warning
> if the condition is not met and, apparently, recycling rules are applied.

I'm aware of that, but the discrepancy in lengths is not easy to
catch. Patches are welcome of course :-).

Deepayan


From sachinj.2006 at yahoo.com  Mon Jun 26 20:34:36 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Mon, 26 Jun 2006 11:34:36 -0700 (PDT)
Subject: [R] write.table & csv help
Message-ID: <20060626183436.90021.qmail@web37612.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060626/256f0e2c/attachment.pl 

From choid at ohsu.edu  Mon Jun 26 21:14:20 2006
From: choid at ohsu.edu (Dongseok Choi)
Date: Mon, 26 Jun 2006 12:14:20 -0700
Subject: [R] questions on local customized R distribution CD
Message-ID: <s49fcfb2.050@ohsu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060626/231557cd/attachment.pl 

From nathan.dabney at gmail.com  Mon Jun 26 21:26:35 2006
From: nathan.dabney at gmail.com (Nathan Dabney)
Date: Mon, 26 Jun 2006 12:26:35 -0700
Subject: [R] Inverse Error Function
Message-ID: <ca854e880606261226g3618210ag84257b7338d46a5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060626/4320c72c/attachment.pl 

From sundar.dorai-raj at pdf.com  Mon Jun 26 21:44:46 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 26 Jun 2006 14:44:46 -0500
Subject: [R] Inverse Error Function
In-Reply-To: <ca854e880606261226g3618210ag84257b7338d46a5@mail.gmail.com>
References: <ca854e880606261226g3618210ag84257b7338d46a5@mail.gmail.com>
Message-ID: <44A0392E.8070609@pdf.com>



Nathan Dabney wrote:
> Do any of the R libraries have an implementation of the Inverse Error
> Function (Inverse ERF)?
> 
> ref:
> http://mathworld.wolfram.com/InverseErf.html
> http://functions.wolfram.com/GammaBetaErf/InverseErf/
> 
> Thanks,
> Nathan
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


Don't know of a built-in function, but you can try this:

## if you want the so-called 'error function'
## from ?pnorm
erf <- function(x) 2 * pnorm(x * sqrt(2)) - 1
erf.inv <- function(x) qnorm((x + 1)/2)/sqrt(2)

erf.inv(1)
erf.inv(0)
erf.inv(-1)
erf.inv(erf(.25))
erf(erf.inv(.25))
erf.inv(.5)

HTH,

--sundar


From rvaradhan at jhmi.edu  Mon Jun 26 21:46:32 2006
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Mon, 26 Jun 2006 15:46:32 -0400
Subject: [R] Inverse Error Function
In-Reply-To: <ca854e880606261226g3618210ag84257b7338d46a5@mail.gmail.com>
Message-ID: <000901c69959$3996efd0$7c94100a@win.ad.jhu.edu>

Hi,

You can use the following relation between standard normal probability
distribution (\Phi) and error function:

Erf(z) = 2 * \Phi(\sqrt(2) z) - 1

to evaluate invErf(x) in R as follows:

invErf <-  function(x) {
# argument x must lie between -1 and 1
qnorm((1 + x) /2) / sqrt(2)
}

For example,
> invErf(0.5)
[1] 0.4769362762

Hope this helps,
Ravi.

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
Webpage: http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html 
--------------------------------------------------------------------------

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Nathan Dabney
> Sent: Monday, June 26, 2006 3:27 PM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Inverse Error Function
> 
> Do any of the R libraries have an implementation of the Inverse Error
> Function (Inverse ERF)?
> 
> ref:
> http://mathworld.wolfram.com/InverseErf.html
> http://functions.wolfram.com/GammaBetaErf/InverseErf/
> 
> Thanks,
> Nathan
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html


From murdoch at stats.uwo.ca  Mon Jun 26 21:57:12 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 26 Jun 2006 15:57:12 -0400
Subject: [R] questions on local customized R distribution CD
In-Reply-To: <s49fcfb2.050@ohsu.edu>
References: <s49fcfb2.050@ohsu.edu>
Message-ID: <44A03C18.6090404@stats.uwo.ca>

On 6/26/2006 3:14 PM, Dongseok Choi wrote:
> Hello all!
>   
>   I hope this is the right place to post this question.
>  
>   The Oregon Chapter of ASA is working with local high school teachers as one of its outreaching program.
>   We hope to use and test R as teaching tools.
>   So, we think that a menu system (like R commander) with a few packages and a bit simplified installation instruction need to be developed.
>  
>   The main question is:
> 1)
> Is it OK to develop a customized CD-ROM distribution of R  with pre-selected packages for high school? 
> It will be distributed free, of course.
> Also, we plan to make it available from the chap web or deposit it to R-project, if requested.

Generally the answer is yes, but read the GPL for the conditions.  You 
do need to make the source code available.
>  
> 2)
>   If the customized distribution CD is OK, I also hope to get some technical help/advice from the core group members if any one is interested.

See the R Installation and Administration manual first.  It tells how to 
build R installers with non-standard included packages.  Hopefully for 
2.4.0 more customizations will be possible.

Duncan Murdoch


From kubovy at virginia.edu  Mon Jun 26 22:37:22 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Mon, 26 Jun 2006 16:37:22 -0400
Subject: [R] Tcl/Tk failing in JGR, but not in R for Mac OS X GUI
Message-ID: <42C423F4-9914-48B2-857C-0F3EDF2BA5C6@virginia.edu>

Dear r-helpers,

I wonder if you can figure out why the following is working:

********************************************************************
 > sessionInfo()
Version 2.3.1 (2006-06-01)
powerpc-apple-darwin8.6.0

attached base packages:
[1] "tcltk"     "methods"   "stats"     "graphics"  "grDevices"  
"utils"     "datasets"  "base"

other attached packages:
igraph
"0.1.2"

 > g <- graph.ring(10)
 > tkplot(g)
Loading required package: tcltk
Loading Tcl/Tk interface ... done
[1] 1
********************************************************************
An X window appears, and the graph can be interactively manipulated,  
as intended. But in JGR:

********************************************************************
 > sessionInfo()
Version 2.3.1 (2006-06-01)
powerpc-apple-darwin8.6.0

attached base packages:
[1] "graphics"  "grDevices" "stats"     "utils"     "methods"
[6] "base"

other attached packages:
igraph    BHH2     JGR  JavaGD   rJava
"0.1.2" "0.2-1" "1.4-2" "0.3-3" "0.4-3"

 > g <- graph.ring(10)
 > tkplot(g)
Loading required package: tcltk
Loading Tcl/Tk interface ... Error in fun(...) : Can't find a usable  
init.tcl in the following directories:
     @TCL_IN_FRAMEWORK@ @TCL_IN_FRAMEWORK@



This probably means that Tcl wasn't installed properly.
Error: .onLoad failed in 'loadNamespace' for 'tcltk'
Error in tkplot(g) : tcl/tk library not available
********************************************************************

_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From kubovy at virginia.edu  Mon Jun 26 23:13:57 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Mon, 26 Jun 2006 17:13:57 -0400
Subject: [R] Some tcltk-related packages not loading (OS X)
Message-ID: <F4DD0049-A723-43F6-9067-738B37F582B6@virginia.edu>

Dear r helpers,

In my exploration of the tcltk facilities of R I've had some success  
but some failures, and wonder if someone could point me to a  
solution. To begin:

******************************************************
 > sessionInfo()
Version 2.3.1 (2006-06-01)
powerpc-apple-darwin8.6.0

attached base packages:
[1] "tcltk"     "methods"   "stats"     "graphics"  "grDevices"  
"utils"     "datasets"  "base"

other attached packages:
igraph
"0.1.2"
******************************************************
A success:
******************************************************
 > library(tcltk)
 > tt <- tktoplevel()
 > tkpack(txt.w <- tktext(tt))
<Tcl>
 > tkinsert(txt.w, "0.0", "plot(1:10)")
<Tcl>
 >
 > eval.txt <- function()
+    eval(parse(text=tclvalue(tkget(txt.w, "0.0", "end"))))
 > tkpack(but.w <- tkbutton(tt,text="Submit", command=eval.txt))
<Tcl>
 >
 > tkdestroy(tt)
******************************************************
An interactive session ran successfully. One failure:
******************************************************
 > library(gtkDevice)
Error in dyn.load(x, as.logical(local), as.logical(now)) :
	unable to load shared library '/Library/Frameworks/R.framework/ 
Versions/2.3/Resources/library/gtkDevice/libs/ppc/gtkDevice.so':
   dlopen(/Library/Frameworks/R.framework/Versions/2.3/Resources/ 
library/gtkDevice/libs/ppc/gtkDevice.so, 6): Symbol not found:  
_gtk_main_quit
   Referenced from: /Library/Frameworks/R.framework/Versions/2.3/ 
Resources/library/gtkDevice/libs/ppc/gtkDevice.so
   Expected in: flat namespace
Error in library(gtkDevice) : .First.lib failed for 'gtkDevice'
******************************************************
and another:
******************************************************
 > library(tkrplot)
Error in structure(.External("dotTcl", ..., PACKAGE = "tcltk"), class  
= "tclObj") :
	[tcl] image not found
NSCreateObjectFileImageFromFile() error: not a Mach-O MH_BUNDLE file.
Error in library(tkrplot) : .First.lib failed for 'tkrplot'
******************************************************
I ran
 > installed.packages()
the two relevant lines are:
			Package	LibPath															Version	Priority	Bundle
gtkDevice	"gtkDevice"	"/Library/Frameworks/R.framework/Versions/2.3/ 
Resources/library"	"1.9-4"	NA		NA
tkrplot		"tkrplot"		"/Library/Frameworks/R.framework/Versions/2.3/ 
Resources/library"	"0.0-14"	NA		NA
******************************************************
I tried reinstalling gtkDevice from source, and got:
******************************************************
 >
trying URL 'http://lib.stat.cmu.edu/R/CRAN/src/contrib/ 
gtkDevice_1.9-4.tar.gz'
Content type 'application/x-gzip' length 41475 bytes
opened URL
==================================================
downloaded 40Kb

WARNING: ignoring environment value of R_HOME
* Installing *source* package 'gtkDevice' ...
checking for gtk-config... no
checking for gtk12-config... no
ERROR: Cannot find gtk-config.
** Removing '/Library/Frameworks/R.framework/Versions/2.3/Resources/ 
library/gtkDevice'
** Restoring previous '/Library/Frameworks/R.framework/Versions/2.3/ 
Resources/library/gtkDevice'

The downloaded packages are in
	/private/tmp/RtmpjNWS1J/downloaded_packages
ERROR: configuration failed for package 'gtkDevice'
******************************************************
I tried installing tkrplot from source, and that worked!
******************************************************
trying URL 'http://lib.stat.cmu.edu/R/CRAN/src/contrib/ 
tkrplot_0.0-14.tar.gz'
Content type 'application/x-gzip' length 38339 bytes
opened URL
==================================================
downloaded 37Kb

WARNING: ignoring environment value of R_HOME
* Installing *source* package 'tkrplot' ...
configure: creating ./config.status
config.status: creating src/Makevars
** libs
** arch - ppc
gcc-4.0 -arch ppc -I/Library/Frameworks/R.framework/Resources/include  
-I/Library/Frameworks/R.framework/Resources/include/ppc -I/usr/local/ 
include -I/usr/local/include -I/usr/local/include   -fPIC -fno- 
common  -g -O2 -std=gnu99 -c tcltkimg.c -o tcltkimg.o
gcc-4.0 -arch ppc -flat_namespace -bundle -undefined suppress -L/usr/ 
local/lib -o tkrplot.so tcltkimg.o -L/usr/local/lib -ltcl8.4 -L/usr/ 
local/lib -ltk8.4 -L/usr/X11R6/lib -lX11  -L/Library/Frameworks/ 
R.framework/Resources/lib/ppc -lR
** R
** help
 >>> Building/Updating help pages for package 'tkrplot'
      Formats: text html latex example
   TkRplot                           text    html    latex   example
** building package indices ...
* DONE (tkrplot)

The downloaded packages are in
	/private/tmp/RtmpjNWS1J/downloaded_packages
******************************************************
But:
******************************************************
 > library(tkrplot)
Loading required package: tcltk
Loading Tcl/Tk interface ... done
Error in structure(.External("dotTcl", ..., PACKAGE = "tcltk"), class  
= "tclObj") :
	[tcl] image not found
NSCreateObjectFileImageFromFile() error: not a Mach-O MH_BUNDLE file.
Error in library(tkrplot) : .First.lib failed for 'tkrplot'

_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From arrayprofile at yahoo.com  Mon Jun 26 23:25:24 2006
From: arrayprofile at yahoo.com (array chip)
Date: Mon, 26 Jun 2006 14:25:24 -0700 (PDT)
Subject: [R] compare odds ratios
Message-ID: <20060626212524.26432.qmail@web35713.mail.mud.yahoo.com>

Hi there, is there any way to compare 2 odds ratios? I
have two tests that are supposed to detect a disease
presence. So for each test, I can compute an odds
ratio. My problem is how can I compare the 2 tests by
testing whether the 2 odds ratios are the same?

Appreciate


From p_connolly at ihug.co.nz  Mon Jun 26 23:47:39 2006
From: p_connolly at ihug.co.nz (Patrick Connolly)
Date: Tue, 27 Jun 2006 09:47:39 +1200
Subject: [R] princomp and prcomp confusion
Message-ID: <20060626214739.GK4970@ihug.co.nz>

When I look through archives at
https://stat.ethz.ch/pipermail/r-help/2003-October/040525.html

I see this:

Liaw, Andy wrote:

>In the `Detail' section of ?princomp:
>
>princomp only handles so-called Q-mode PCA, that is feature extraction of
>variables. If a data matrix is supplied (possibly via a formula) it is
>required that there are at least as many units as variables. For R-mode PCA
>use prcomp. 
>

It doesn't appear that anyone disputed the accuracy of it.


My current installation (version.string Version 2.3.1 (2006-06-01))
says in the detail of princomp


     'princomp' only handles so-called R-mode PCA, that is feature
     extraction of variables.  If a data matrix is supplied (possibly
     via a formula) it is required that there are at least as many
     units as variables.  For Q-mode PCA use 'prcomp'.

I've not been following principal components and have only recently
had a use for that methodology.  Am I to assume that the later version
is the correct one?  I thought I'd worked out what the distinction
between R-mode and Q-mode was, but now I'm as confused as ever.

best

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From lizzylaws at yahoo.com  Mon Jun 26 23:52:06 2006
From: lizzylaws at yahoo.com (Elizabeth Lawson)
Date: Mon, 26 Jun 2006 14:52:06 -0700 (PDT)
Subject: [R] Griddy-Gibbs sampler
Message-ID: <20060626215206.52785.qmail@web32107.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060626/5d7511d0/attachment.pl 

From p.dalgaard at biostat.ku.dk  Tue Jun 27 00:03:26 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Jun 2006 00:03:26 +0200
Subject: [R] Some tcltk-related packages not loading (OS X)
In-Reply-To: <F4DD0049-A723-43F6-9067-738B37F582B6@virginia.edu>
References: <F4DD0049-A723-43F6-9067-738B37F582B6@virginia.edu>
Message-ID: <x21wtb3781.fsf@turmalin.kubism.ku.dk>

Michael Kubovy <kubovy at virginia.edu> writes:

> Dear r helpers,
> 
> In my exploration of the tcltk facilities of R I've had some success  
> but some failures, and wonder if someone could point me to a  
> solution. To begin:
> 
> ******************************************************
>  > sessionInfo()
> Version 2.3.1 (2006-06-01)
> powerpc-apple-darwin8.6.0
> 
> attached base packages:
> [1] "tcltk"     "methods"   "stats"     "graphics"  "grDevices"  
> "utils"     "datasets"  "base"
> 
> other attached packages:
> igraph
> "0.1.2"
> ******************************************************
> A success:
> ******************************************************
>  > library(tcltk)
>  > tt <- tktoplevel()
>  > tkpack(txt.w <- tktext(tt))
> <Tcl>
>  > tkinsert(txt.w, "0.0", "plot(1:10)")
> <Tcl>
>  >
>  > eval.txt <- function()
> +    eval(parse(text=tclvalue(tkget(txt.w, "0.0", "end"))))
>  > tkpack(but.w <- tkbutton(tt,text="Submit", command=eval.txt))
> <Tcl>
>  >
>  > tkdestroy(tt)
> ******************************************************
> An interactive session ran successfully. One failure:
> ******************************************************
>  > library(gtkDevice)

gtk is not related to tcltk. They are essentially different GUI
programming interfaces.

 ....
> ******************************************************
> trying URL 'http://lib.stat.cmu.edu/R/CRAN/src/contrib/ 
> tkrplot_0.0-14.tar.gz'
> Content type 'application/x-gzip' length 38339 bytes
> opened URL
> ==================================================
> downloaded 37Kb
> 
> WARNING: ignoring environment value of R_HOME
> * Installing *source* package 'tkrplot' ...
> configure: creating ./config.status
> config.status: creating src/Makevars
> ** libs
> ** arch - ppc
> gcc-4.0 -arch ppc -I/Library/Frameworks/R.framework/Resources/include  
> -I/Library/Frameworks/R.framework/Resources/include/ppc -I/usr/local/ 
> include -I/usr/local/include -I/usr/local/include   -fPIC -fno- 
> common  -g -O2 -std=gnu99 -c tcltkimg.c -o tcltkimg.o
> gcc-4.0 -arch ppc -flat_namespace -bundle -undefined suppress -L/usr/ 
> local/lib -o tkrplot.so tcltkimg.o -L/usr/local/lib -ltcl8.4 -L/usr/ 
> local/lib -ltk8.4 -L/usr/X11R6/lib -lX11  -L/Library/Frameworks/ 
> R.framework/Resources/lib/ppc -lR
> ** R
> ** help
>  >>> Building/Updating help pages for package 'tkrplot'
>       Formats: text html latex example
>    TkRplot                           text    html    latex   example
> ** building package indices ...
> * DONE (tkrplot)
> 
> The downloaded packages are in
> 	/private/tmp/RtmpjNWS1J/downloaded_packages
> ******************************************************
> But:
> ******************************************************
>  > library(tkrplot)
> Loading required package: tcltk
> Loading Tcl/Tk interface ... done
> Error in structure(.External("dotTcl", ..., PACKAGE = "tcltk"), class  
> = "tclObj") :
> 	[tcl] image not found
> NSCreateObjectFileImageFromFile() error: not a Mach-O MH_BUNDLE file.
> Error in library(tkrplot) : .First.lib failed for 'tkrplot'

Presumably, this means that the "load" command in Tcl is not happy
loading tkrplot.so as generated by the 2nd gcc-4.0 command above. For
a non-Mac-guru like me this raises a few questions: Is .so really the
right extension? Is a special processing step needed for Mac OSX?
However, better leave this for real Mac gurus...

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From aarti_dahiya at hotmail.com  Tue Jun 27 00:17:09 2006
From: aarti_dahiya at hotmail.com (Aarti Dahiya)
Date: Mon, 26 Jun 2006 18:17:09 -0400
Subject: [R] Passing arguments from a function within another function
Message-ID: <BAY106-F5F56AB8D8F6EC4E6BC6C2E3790@phx.gbl>

Hi all,

I have a function getSomeData() that is called from command line - 
getSomeData(id='1240'). The function getSomeData() calls another function 
getData that needs the exact same argument passed to getSomeData().  I am 
using the function call getData(paste(names(args[1]), "=", 
sQuote(args[[1]]))).  The argument passed is- id='1240'.

The problem is that in getData(), it treats the whole thing "id='1240'" as 
one arguments i.e. for getData args[[1]] is "id='1240'".  Hence, I am unable 
to extract the name id and the value of id separately.  Beacuse of this, I 
am not able to generate the SQL query select * from table where id = '1240'.

Thank you.

Aarti


From gavin.simpson at ucl.ac.uk  Tue Jun 27 01:56:38 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 27 Jun 2006 00:56:38 +0100
Subject: [R] Plylogenetic analysis
In-Reply-To: <20060626135957.23924.qmail@web61225.mail.yahoo.com>
References: <20060626135957.23924.qmail@web61225.mail.yahoo.com>
Message-ID: <1151366198.2463.9.camel@graptoleberis.geog.ucl.ac.uk>

On Mon, 2006-06-26 at 10:59 -0300, Erica Baron wrote:
> Dear coleges,
> How to use the genotype from microsatelite markers
> from many different populations to construct a
> tridimensional phylogenetic tree with R? Any
> suggestions?
> Thank you very much!
> Baron, Erica

Hi Erica,

I know nothing about phylogenetics, but perhaps you could help yourself
by checking out the CRAN Task View on Genetics, e.g.:

http://www.stats.bris.ac.uk/R/src/contrib/Views/Genetics.html

You could also do:

RSiteSearch("phylogenetic tree")

If that doesn't get you anywhere, then you might get further help from
the list if you post some example data and perhaps a link to a image of
the type of plot you are wanting to reproduce. Some kind soul might then
be able to help...

Gav


From sell_mirage_ne at hotmail.com  Tue Jun 27 02:21:01 2006
From: sell_mirage_ne at hotmail.com (Taka Matzmoto)
Date: Mon, 26 Jun 2006 19:21:01 -0500
Subject: [R] looking for a more efficient R code.
Message-ID: <BAY110-F22F5DA4F84FDC3F9690CF5C77E0@phx.gbl>

Dear R-users

I have four simple linear models, which are all in the form of a*X+b
The estimated parameter vectors are

a <- c(1,2,3,4)
b <- c(4,3,2,1)

My goal is to draw a plot where x-axis is X (range from -100 to 100) and 
y-axis is the sum of
all four linear models

X <- seq(-100,100, length=10000)
plot(X, sum of the four linear functions)

I started with a function for summing

summing <- function(a,b)
{temp <-0;
for (i in 1:length(a))
temp <- temp + a[i]*X+b[i]
}

plot(X, summing(a,b))

I am a R beginner.  I am looking for a more efficient R code.

Any help or advice will be appreciated.

Taka,


From A.Robinson at ms.unimelb.edu.au  Tue Jun 27 02:25:27 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Tue, 27 Jun 2006 10:25:27 +1000
Subject: [R] Effect size in mixed models
In-Reply-To: <449B3498.8060901@pdf.com>
References: <002c01c6919d$39416d80$6400a8c0@brungio>
	<449762E8.8000507@pdf.com> <449B3498.8060901@pdf.com>
Message-ID: <20060627002527.GB712@ms.unimelb.edu.au>

Hi Spencer,

what you're describing now seems to me to be similar to the idea of
intra-class correlation.  I have previously used the relative and
absolute amounts of variance represented at different hierarchical
levels of random effects, and they seemed to me to be easy to
interpret and useful, although I used method of moments estimators
instead of ML.  This ease of interpretation and utility is especially
true when the random effects structure matches the physical layout of
a sample, e.g., multiple measurements within trees within plots within
stands within forests etc.
 
Sadly I cannot see how to cleanly generalize the principle to
situations of crossed random effects.  Also, as Goldstein et al point
out in a nice paper in Understanding Statistics (2002): "Partitioning
variation in multilevel models", random covariates also complicate the
picture substantially.

Cheers

Andrew

On Thu, Jun 22, 2006 at 05:23:52PM -0700, Spencer Graves wrote:
> 	  I just learned that my earlier suggestion was wrong.  It's better to 
> compute the variance of the predicted or fitted values and compare those 
> with the estimated variance components.
> 
> 	  To see how to do this, consider the following minor modification of 
> an example in the "lme" documentation:
> 
> fm1. <- lme(distance ~ age, data = Orthodont, random=~1)
> fm2. <- lme(distance ~ age + Sex, data = Orthodont, random = ~ 1)
> 
> # str(fm1.) suggested the following:
>  > var(fm2.$fitted[, "fixed"]-fm1.$fitted[, "fixed"])
> [1] 1.312756
>  > VarCorr(fm1.)[, 1]
> (Intercept)    Residual
>   "4.472056"  "2.049456"
>  > VarCorr(fm2.)[, 1]
> (Intercept)    Residual
>   "3.266784"  "2.049456"
> 
> 	  In this example, the subject variance without considering "Sex" was 
> 4.47 but with "Sex" in the model, it dropped to 3.27, while the Residual 
> variance remained unchanged at 2.05.  The difference between 
> fm2.$fitted[, "fixed"] and fm1.$fitted[, "fixed"] is the change in the 
> predictions generated by the addition of "Sex" to the model.  The 
> variance of that difference was 1.31.  Note that 3.27 + 1.31 = 4.58, 
> which is moderately close to 4.47.
> 
> 	  In sum, I think we can get a reasonable estimate of the size of an 
> effect from the variance of the differences in the "fixed" portion of 
> the fitted model.
> 
> 	  Comments?
> 	  Hope this helps.
> 	  Spencer Graves
> 
> Spencer Graves wrote:
> >       You have asked a great question:  It would indeed be useful to 
> > compare the relative magnitude of fixed and random effects, e.g. to 
> > prioritize efforts to better understand and possibly manage processes 
> > being studied.  I will offer some thoughts on this, and I hope if there 
> > are errors in my logic or if someone else has a better idea, we will 
> > both benefit from their comments.
> > 
> >       The ideal might be an estimate of something like a mean square for 
> > a particular effect to compare with an estimated variance component.
> > Such mean squares were a mandatory component of any analysis of variance 
> > table prior to the (a) popularization of generalized linear models and 
> > (b) availability of software that made it feasible to compute maximum 
> > likelihood estimates routinely for unbalanced, mixed-effects models. 
> > However, anova(lme(...)) such mean squares are for most purposes 
> > unnecessary cluster in a modern anova table.
> > 
> >       To estimate a mean square for a fixed effect, consider the 
> > following log(likelihood) for a mixed-effects model:
> > 
> >       lglk = (-0.5)*(n*log(2*pi*var.e)-log(det(W)) + 
> > t(y-X%*%b)%*%W%*%(y-X%*%b)/var.e),
> > 
> > where n = the number of observations,
> > 
> >       b = the fixed-effect parameter variance,
> > 
> > and the covariance matrix of the residuals, after integrating out the 
> > random effects is var.e*solve(W).  In this formulation, the matrix "W" 
> > is a function of the variance components.  Since they are not needed to 
> > compute the desired mean squares, they are suppressed in the notation here.
> > 
> >       Then, the maximum likelihood estimate of
> > 
> >       var.e = SSR/n,
> > 
> > where SSR = t(y-X%*%b)%*%W%*%(y-X%*%b).
> > 
> >       Then
> > 
> >       mle.lglk = (-0.5)*(n*(log(2*pi*SSR/n)-1)-log(det(W))).
> > 
> >       Now let
> > 
> >       SSR0 = this generalized sum of squares of residuals (SSR) without 
> > effect "1",
> > 
> > and
> > 
> >       SSR1 = this generalized SSR with this effect "1".
> > 
> >       If I've done my math correctly, then
> > 
> >       D = deviance = 2*log(likelihood ratio)
> >         = (n*log(SSR0/SSR1)+log(det(W1)/det(W0)))
> > 
> >       For roughly half a century, a major part of "the analysis of 
> > variance" was the Pythagorean idea that the sum of squares under H0 was 
> > the sum of squares under H1 plus the sum of squares for effect "1":
> > 
> >       SSR0 = SS1 + SSR1.
> > 
> >       Whence,
> > 
> >       exp((D/n)-log(det(W1)/det(W0))) = 1+SS1/SSR1.
> > 
> > Thus,
> > 
> >       SS1 = SSR1*(exp((D/n)-log(det(W1)/det(W0)))-1).
> > 
> >       If the difference between deg(W1) and det(W0) can be ignored, we get:
> > 
> >       SS1 = SSR1*(exp((D/n)-1).
> > 
> >       Now compute MS1 = SS1/df1, and compare with the variance components.
> > 
> >       If there is a flaw in this logic, I hope someone will disabuse me 
> > of it.
> > 
> >       If this seems too terse or convoluted to follow, please provide a 
> > simple, self-contained example, as suggested in the posting guide! 
> > "www.R-project.org/posting-guide.html".  You asked a theoretical 
> > question, you got a theoretical answer.  If you want a concrete answer, 
> > it might help to pose a more concrete question.
> > 
> >       Hope this helps.
> >       Spencer Graves   
> > 
> > Bruno L. Giordano wrote:
> >> Hello,
> >> Is there a way to compare the relative relevance of fixed and random 
> >> effects in mixed models? I have in mind measures of effect size in 
> >> ANOVAs, and would like to obtain similar information with mixed models.
> >>
> >> Are there information criteria that allow to compare the relevance of 
> >> each of the effects in a mixed model to the overall fit?
> >>
> >> Thank you,
> >>     Bruno
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide! 
> >> http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From res90sx5 at verizon.net  Tue Jun 27 02:54:13 2006
From: res90sx5 at verizon.net (Daniel Nordlund)
Date: Mon, 26 Jun 2006 17:54:13 -0700
Subject: [R] looking for a more efficient R code.
In-Reply-To: <BAY110-F22F5DA4F84FDC3F9690CF5C77E0@phx.gbl>
Message-ID: <000e01c69984$378bcf00$6401a8c0@main>



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch]
> On Behalf Of Taka Matzmoto
> Sent: Monday, June 26, 2006 5:21 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] looking for a more efficient R code.
> 
> Dear R-users
> 
> I have four simple linear models, which are all in the form of a*X+b
> The estimated parameter vectors are
> 
> a <- c(1,2,3,4)
> b <- c(4,3,2,1)
> 
> My goal is to draw a plot where x-axis is X (range from -100 to 100) and
> y-axis is the sum of
> all four linear models
> 
> X <- seq(-100,100, length=10000)
> plot(X, sum of the four linear functions)
> 
> I started with a function for summing
> 
> summing <- function(a,b)
> {temp <-0;
> for (i in 1:length(a))
> temp <- temp + a[i]*X+b[i]
> }
> 
> plot(X, summing(a,b))
> 
> I am a R beginner.  I am looking for a more efficient R code.
> 
> Any help or advice will be appreciated.
> 
> Taka,
> 

Taka,

I won't claim more efficient, that is for you to decide.  But you might try something like:

plot(x, colSums(t(x %*% t(b)) + a))

Dan

Daniel Nordlund
Bothell, WA


From jholtman at gmail.com  Tue Jun 27 03:05:15 2006
From: jholtman at gmail.com (jim holtman)
Date: Mon, 26 Jun 2006 21:05:15 -0400
Subject: [R] looking for a more efficient R code.
In-Reply-To: <BAY110-F22F5DA4F84FDC3F9690CF5C77E0@phx.gbl>
References: <BAY110-F22F5DA4F84FDC3F9690CF5C77E0@phx.gbl>
Message-ID: <644e1f320606261805i2aafe55yc731dcca12e5b171@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060626/955e4c57/attachment.pl 

From johannesson1 at llnl.gov  Tue Jun 27 03:12:56 2006
From: johannesson1 at llnl.gov (Gardar Johannesson)
Date: Mon, 26 Jun 2006 18:12:56 -0700
Subject: [R] looking for a more efficient R code.
In-Reply-To: <BAY110-F22F5DA4F84FDC3F9690CF5C77E0@phx.gbl>
References: <BAY110-F22F5DA4F84FDC3F9690CF5C77E0@phx.gbl>
Message-ID: <1151370776.6753.76.camel@johanssen1rh>

Little algebra will convince you that your 'summing' function is just
equal to:
sum(a) + sum(b)*X
So, it is very difficult to get it any faster than:
plot(X, sum(a)+sum(b)*X)

And, by the way, most of the execution time is spent plotting, not
computing the average:
> system.time({y1 <- summing(a,b)})
[1] 0.002 0.000 0.001 0.000 0.000
> system.time({y2 = sum(a)+sum(b)*X})
[1] 0 0 0 0 0
> all.equal(y1,y2)
[1] TRUE
> system.time(plot(X,y2))
[1] 0.019 0.002 0.051 0.000 0.000
>


Gardar



On Mon, 2006-06-26 at 17:21, Taka Matzmoto wrote:
> Dear R-users
> 
> I have four simple linear models, which are all in the form of a*X+b
> The estimated parameter vectors are
> 
> a <- c(1,2,3,4)
> b <- c(4,3,2,1)
> 
> My goal is to draw a plot where x-axis is X (range from -100 to 100) and 
> y-axis is the sum of
> all four linear models
> 
> X <- seq(-100,100, length=10000)
> plot(X, sum of the four linear functions)
> 
> I started with a function for summing
> 
> summing <- function(a,b)
> {temp <-0;
> for (i in 1:length(a))
> temp <- temp + a[i]*X+b[i]
> }
> 
> plot(X, summing(a,b))
> 
> I am a R beginner.  I am looking for a more efficient R code.
> 
> Any help or advice will be appreciated.
> 
> Taka,
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From bruno.giordano at music.mcgill.ca  Tue Jun 27 03:21:25 2006
From: bruno.giordano at music.mcgill.ca (Bruno L. Giordano)
Date: Mon, 26 Jun 2006 21:21:25 -0400
Subject: [R]  Robustness of linear mixed models
References: <BAY110-F22F5DA4F84FDC3F9690CF5C77E0@phx.gbl>
	<644e1f320606261805i2aafe55yc731dcca12e5b171@mail.gmail.com>
Message-ID: <005d01c69988$13d8b8d0$6400a8c0@brungio>

Hello,

with 4 different linear mixed models (continuous dependent) I find that my 
residuals do not follow the normality assumption (significant Shapiro-Wilk 
with values equal/higher than 0.976; sample sizes 750 or 1200). I find, 
instead, that my residuals are really well fitted by a t distribution with 
dofs' ranging, in the different datasets, from 5 to 12.

Should this be considered such a severe violation of the normality 
assumption as to make model-based inferences invalid?

Thanks a lot,
    Bruno


From markleeds at verizon.net  Tue Jun 27 03:46:43 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Mon, 26 Jun 2006 20:46:43 -0500 (CDT)
Subject: [R] related to my previous sapply question]
Message-ID: <22621496.4264641151372805086.JavaMail.root@vms169.mailsrvcs.net>

in my previous post in which i asked about creating sequences
from two vectors of numbers,  all suggestions worked.

tradevectors<-mapply(seq,from=tempa,to=tempb)

or 

tradevectors<-sapply(1:length(tempa),function(x) seq(tempa[x],tempb[x])

>both return a list with 3 components.

the problem is that i want to take these 3 sequences and

use them as the indices of two other vectors, X and Y and mutiply them  ( element by element )


>so, i tried 
>
>temp<-sapply(1:length(tradevectors),function(i)X[tradevectors[[i]]]*Y[tradevectors[[i]]]


basically, the sequence output from the previous command are
indices to two vectors that i want to multiply ( element
by element ).

but, the message i get is that i cannot coerce list object to double. 

i looked up the info on unlist, but that just
makes long vector. i need 3 vectors.

                                   thanks


From ggrothendieck at gmail.com  Tue Jun 27 04:01:49 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 26 Jun 2006 22:01:49 -0400
Subject: [R] Passing arguments from a function within another function
In-Reply-To: <BAY106-F5F56AB8D8F6EC4E6BC6C2E3790@phx.gbl>
References: <BAY106-F5F56AB8D8F6EC4E6BC6C2E3790@phx.gbl>
Message-ID: <971536df0606261901j12278035qdb69664a2d227906@mail.gmail.com>

See:
http://tolstoy.newcastle.edu.au/R/help/06/06/29301.html

On 6/26/06, Aarti Dahiya <aarti_dahiya at hotmail.com> wrote:
> Hi all,
>
> I have a function getSomeData() that is called from command line -
> getSomeData(id='1240'). The function getSomeData() calls another function
> getData that needs the exact same argument passed to getSomeData().  I am
> using the function call getData(paste(names(args[1]), "=",
> sQuote(args[[1]]))).  The argument passed is- id='1240'.
>
> The problem is that in getData(), it treats the whole thing "id='1240'" as
> one arguments i.e. for getData args[[1]] is "id='1240'".  Hence, I am unable
> to extract the name id and the value of id separately.  Beacuse of this, I
> am not able to generate the SQL query select * from table where id = '1240'.
>
> Thank you.
>
> Aarti
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From markleeds at verizon.net  Tue Jun 27 04:11:54 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Mon, 26 Jun 2006 21:11:54 -0500 (CDT)
Subject: [R] almost figured it out but failed
Message-ID: <16365550.4270721151374315626.JavaMail.root@vms169.mailsrvcs.net>

I used RSearchSite and found out about using

matrix(unlist(thelist),nrow=length(thelist),byrow=TRUE)

but then i realized that I can't use this because each
component of the list can be a different length
so the matrix would have to have variable column lengths which
makes no sense. thanks for any suggestions.


                                   mark


From MSchwartz at mn.rr.com  Tue Jun 27 04:44:13 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Mon, 26 Jun 2006 21:44:13 -0500
Subject: [R] related to my previous sapply question]
In-Reply-To: <22621496.4264641151372805086.JavaMail.root@vms169.mailsrvcs.net>
References: <22621496.4264641151372805086.JavaMail.root@vms169.mailsrvcs.net>
Message-ID: <1151376254.13100.29.camel@localhost.localdomain>

On Mon, 2006-06-26 at 20:46 -0500, markleeds at verizon.net wrote:
> in my previous post in which i asked about creating sequences
> from two vectors of numbers,  all suggestions worked.
> 
> tradevectors<-mapply(seq,from=tempa,to=tempb)
> 
> or 
> 
> tradevectors<-sapply(1:length(tempa),function(x) seq(tempa[x],tempb[x])
> 
> >both return a list with 3 components.
> 
> the problem is that i want to take these 3 sequences and
> 
> use them as the indices of two other vectors, X and Y and mutiply them  ( element by element )
> 
> 
> >so, i tried 
> >
> >temp<-sapply(1:length(tradevectors),function(i)X[tradevectors[[i]]]*Y[tradevectors[[i]]]
> 
> 
> basically, the sequence output from the previous command are
> indices to two vectors that i want to multiply ( element
> by element ).
> 
> but, the message i get is that i cannot coerce list object to double. 
> 
> i looked up the info on unlist, but that just
> makes long vector. i need 3 vectors.

How about something like this:

  tempa <- c(4, 6, 10)

  tempb <- c(11, 23, 39)

  tradevectors <- mapply(seq, from = tempa, to = tempb)


> tradevectors
[[1]]
[1]  4  5  6  7  8  9 10 11

[[2]]
 [1]  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23

[[3]]
 [1] 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31
[23] 32 33 34 35 36 37 38 39

  
  X <- seq(2, 80, 2)
  Y <- 1:40

> X
 [1]  2  4  6  8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44
[23] 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80

> Y
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22
[23] 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40



> lapply(tradevectors, function(x) X[x] * Y[x])
[[1]]
[1]  32  50  72  98 128 162 200 242

[[2]]
 [1]   72   98  128  162  200  242  288  338  392  450  512  578  648
[14]  722  800  882  968 1058

[[3]]
 [1]  200  242  288  338  392  450  512  578  648  722  800  882  968
[14] 1058 1152 1250 1352 1458 1568 1682 1800 1922 2048 2178 2312 2450
[27] 2592 2738 2888 3042


For example, the first value in [[1]] is 32, which is:

  X[4] * Y[4]    # The '4' comes from tradevectors[[1]][1]

which is:

  8 * 4


The other list elements are similarly constructed.  You can use lapply()
when you want to apply functions to individual list elements (in this
case vectors) as you traverse the list tree. See ?lapply

HTH,

Marc Schwartz


From mb.atelier at web.de  Tue Jun 27 07:24:37 2006
From: mb.atelier at web.de (Matthias Braeunig)
Date: Tue, 27 Jun 2006 07:24:37 +0200
Subject: [R] reshaping data.frame question
In-Reply-To: <644e1f320606260558r43b003fbw875e6c0d0987627e@mail.gmail.com>
References: <449FD1B6.9030104@web.de>
	<644e1f320606260558r43b003fbw875e6c0d0987627e@mail.gmail.com>
Message-ID: <44A0C115.1060709@web.de>

Thanks, this is not what what I meant. I need to reshape the original
dataframe that I can access p_f[X] for numerical X. Maybe I was not
clear enough.

The problem really is that X starts at 0. Note that in my example
changing the row names to 0:2 does not have the desired effect.



jim holtman wrote:
> You need to specify the row/column name as character:
> 
>> y
>   X1  X2  X3  X4
> 0 0.1 0.1 0.1 0.1
> 1 0.2 0.2 0.2 0.2
> 2 0.3 0.3 0.3 0.3
> 
>> y[,'X3']
> [1] 0.1 0.2 0.3
>> y['0','X3']
> [1] 0.1
> 
> 
> 
> 
> On 6/26/06, Matthias Braeunig <mb.atelier at web.de> wrote:
>>
> Dear R-helpers,
> 
> 
> my data.frame is of the form
> 
> x <- data.frame( f=gl(4,3), X=rep(0:2,4), p=c(.1,.2,.3))
> x
>   f X   p
> 1  1 0 0.1
> 2  1 1 0.2
> 3  1 2 0.3
> 4  2 0 0.1
> 5  2 1 0.2
> 6  2 2 0.3
> 7  3 0 0.1
> 8  3 1 0.2
> 9  3 2 0.3
> 10 4 0 0.1
> 11 4 1 0.2
> 12 4 2 0.3
> 
> which  tabulates some values p(X) for several factors f.
> 
> Now I want to put it in "wide" format, so that factor levels appear as
> column heads. Note also that X starts from zero. It would be nice if I
> could simply access p_f[X==0] as f[0]. How can I possibly do that?
> 
> (The resilting object does not have to be a data.frame. As there are
> only numeric values, also a matrix would do.)
> 
> I tried the following
> 
> y<-unstack(x,form=p~f)
> row.names(y) <- 0:2
> y
>   X1  X2  X3  X4
> 0 0.1 0.1 0.1 0.1
> 1 0.2 0.2 0.2 0.2
> 2 0.3 0.3 0.3 0.3
> 
> Now, how to access X3[0], say?
> 
> Maybe reshape would be the right tool, but I could not figure it out.
> 
> I appreciate your help. Thanks!


From papenfus at gmail.com  Tue Jun 27 07:28:03 2006
From: papenfus at gmail.com (michael papenfus)
Date: Tue, 27 Jun 2006 01:28:03 -0400
Subject: [R] Adding elements of matrices of different dimensions
Message-ID: <e936d1ed0606262228x6c8153a9u8da1fc297382d7b1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060627/17103d1f/attachment.pl 

From dimitris.rizopoulos at med.kuleuven.be  Tue Jun 27 09:00:30 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 27 Jun 2006 09:00:30 +0200
Subject: [R] Adding elements of matrices of different dimensions
References: <e936d1ed0606262228x6c8153a9u8da1fc297382d7b1@mail.gmail.com>
Message-ID: <009701c699b7$60b509e0$0540210a@www.domain>

try the following:

matrix(1:50, ncol = 10) + 1:5
# or
b + drop(a)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "michael papenfus" <papenfus at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, June 27, 2006 7:28 AM
Subject: [R] Adding elements of matrices of different dimensions


> If I have two matrices:
>
>> a<-matrix(1:5,ncol=1)
>
>     [,1]
> [1,]    1
> [2,]    2
> [3,]    3
> [4,]    4
> [5,]    5
>> b<-matrix(1:50,ncol=10)
>
>     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
> [1,]    1    6   11   16   21   26   31   36   41    46
> [2,]    2    7   12   17   22   27   32   37   42    47
> [3,]    3    8   13   18   23   28   33   38   43    48
> [4,]    4    9   14   19   24   29   34   39   44    49
> [5,]    5   10   15   20   25   30   35   40   45    50
>
> How can I add a[1,1] to each column in the first row of b.
> and the then add a[2,1] to each column in second row of b
> and so on.
>
> I know there must be a way to use the apply function or nested for 
> loops but
> I can't seem to get it working in R.
> thanks,
> michael
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ripley at stats.ox.ac.uk  Tue Jun 27 09:00:55 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 27 Jun 2006 08:00:55 +0100 (BST)
Subject: [R] Robustness of linear mixed models
In-Reply-To: <005d01c69988$13d8b8d0$6400a8c0@brungio>
References: <BAY110-F22F5DA4F84FDC3F9690CF5C77E0@phx.gbl>
	<644e1f320606261805i2aafe55yc731dcca12e5b171@mail.gmail.com>
	<005d01c69988$13d8b8d0$6400a8c0@brungio>
Message-ID: <Pine.LNX.4.64.0606270757250.4301@gannet.stats.ox.ac.uk>

On Mon, 26 Jun 2006, Bruno L. Giordano wrote:

> Hello,
>
> with 4 different linear mixed models (continuous dependent) I find that my
> residuals do not follow the normality assumption (significant Shapiro-Wilk
> with values equal/higher than 0.976; sample sizes 750 or 1200). I find,
> instead, that my residuals are really well fitted by a t distribution with
> dofs' ranging, in the different datasets, from 5 to 12.
>
> Should this be considered such a severe violation of the normality
> assumption as to make model-based inferences invalid?

For some aspects, yes.  Given that R provides you with the means to fit 
robust linear models, why not use them and find out if they make a 
difference to the aspects you are interested in?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From maechler at stat.math.ethz.ch  Tue Jun 27 09:19:37 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 27 Jun 2006 09:19:37 +0200
Subject: [R] Puzzled with contour()
In-Reply-To: <971536df0606260828y221cc4f2ta58668f221862342@mail.gmail.com>
References: <20060625133337.GB24241@lubyanka.local>
	<449FEB40.6000405@stats.uwo.ca>
	<971536df0606260739o26a54ad1u389f56986ce460ac@mail.gmail.com>
	<449FF4D2.1080009@stats.uwo.ca>
	<971536df0606260828y221cc4f2ta58668f221862342@mail.gmail.com>
Message-ID: <17568.56329.517392.800784@stat.math.ethz.ch>

>>>>> "Gabor" == Gabor Grothendieck <ggrothendieck at gmail.com>
>>>>>     on Mon, 26 Jun 2006 11:28:48 -0400 writes:

    Gabor> I think its often the case that one has 3 tuples and does not know
    Gabor> how to use contour with that; so, it would be nice if the contour
    Gabor> help page gave advice and an example and a pointer to the relevant
    Gabor> functions if it cannot be done by contour.

Yes, but even more importantly,
what  help(contour) should really have is a \code{\link[lattice]{contourplot}}
since contourplot() from package 'lattice'
can deal well excellently with the situation Ajay has:

After reading his data into matrix 'm3'

> d3 <- data.frame(m3)
> summary(d3)
       x              y             z         
 Min.   :0.00   Min.   : 20   Min.   :0.0000  
 1st Qu.:0.25   1st Qu.: 60   1st Qu.:0.0000  
 Median :0.50   Median :110   Median :0.0040  
 Mean   :0.50   Mean   :110   Mean   :0.1583  
 3rd Qu.:0.75   3rd Qu.:160   3rd Qu.:0.1410  
 Max.   :1.00   Max.   :200   Max.   :1.0000  

contourplot(z ~ x+y, data=d3)
## or nicer
contourplot(z ~ x+y, data=d3, cuts=20, region = TRUE)
## or rather use logit - transformed z values:
contourplot(qlogis(z) ~ x+y, data=d3, cuts=20, region = TRUE)

    Gabor> On 6/26/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
    >> On 6/26/2006 10:39 AM, Gabor Grothendieck wrote:
    >> > I think it would be helpful if this were added to the contour help file.
    >> 
    >> You mean an example of building up the z matrix from points, or just a
    >> general discussion of the issue?
    >> 
    >> Duncan Murdoch
    >> >
    >> > On 6/26/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
    >> >> On 6/25/2006 9:33 AM, Ajay Narottam Shah wrote:
    >> >> > Folks,
    >> >> >
    >> >> > The contour() function wants x and y to be in increasing order. I have
    >> >> > a situation where I have a grid in x and y, and associated z values,
    >> >> > which looks like this:
    >> >>
    >> >> contour() wants vectors of x and y values, and a matrix of z values,
    >> >> where the x values correspond to the rows of z, and the y values to the
    >> >> columns.  You have a collection of points which need to be turned into
    >> >> such a grid.
    >> >>
    >> >> There's an interp function in the akima package that can do this in
    >> >> general.  In your case, it's probably sufficient to do something like this:
    >> >>
    >> >> zmat <- matrix(NA, 3, 19)
    >> >> zmat[cbind(20*x + 1, y/10 - 1)] <- z
    >> >> x <- (0:2)/20
    >> >> y <- (2:20)*10
    >> >> contour(x,y,zmat)
    >> >>
    >> >> Duncan Murdoch
    >> >>
    >> >>
    >> >> >
    >> >> >               x   y     z
    >> >> >       [1,] 0.00  20 1.000
    >> >> >       [2,] 0.00  30 1.000
    >> >> >       [3,] 0.00  40 1.000
    >> >> >       [4,] 0.00  50 1.000
    >> >> >       [5,] 0.00  60 1.000
    >> >> >       [6,] 0.00  70 1.000
    >> >> >       [7,] 0.00  80 0.000
    >> >> >       [8,] 0.00  90 0.000
    >> >> >       [9,] 0.00 100 0.000
    >> >> >      [10,] 0.00 110 0.000
    >> >> >      [11,] 0.00 120 0.000
    >> >> >      [12,] 0.00 130 0.000
    >> >> >      [13,] 0.00 140 0.000
    >> >> >      [14,] 0.00 150 0.000
    >> >> >      [15,] 0.00 160 0.000
    >> >> >      [16,] 0.00 170 0.000
    >> >> >      [17,] 0.00 180 0.000
    >> >> >      [18,] 0.00 190 0.000
    >> >> >      [19,] 0.00 200 0.000
    >> >> >      [20,] 0.05  20 1.000
    >> >> >      [21,] 0.05  30 1.000
    >> >> >      [22,] 0.05  40 1.000
    >> >> >      [23,] 0.05  50 1.000
    >> >> >      [24,] 0.05  60 0.998
    >> >> >      [25,] 0.05  70 0.124
    >> >> >      [26,] 0.05  80 0.000
    >> >> >      [27,] 0.05  90 0.000
    >> >> >      [28,] 0.05 100 0.000
    >> >> >      [29,] 0.05 110 0.000
    >> >> >      [30,] 0.05 120 0.000
    >> >> >      [31,] 0.05 130 0.000
    >> >> >      [32,] 0.05 140 0.000
    >> >> >      [33,] 0.05 150 0.000
    >> >> >      [34,] 0.05 160 0.000
    >> >> >      [35,] 0.05 170 0.000
    >> >> >      [36,] 0.05 180 0.000
    >> >> >      [37,] 0.05 190 0.000
    >> >> >      [38,] 0.05 200 0.000
    >> >> >      [39,] 0.10  20 1.000
    >> >> >      [40,] 0.10  30 1.000
    >> >> >
    >> >> > This looks like a nice case where both x and y are in increasing
    >> >> > order. But contour() gets unhappy saying that he wants x and y in
    >> >> > increasing order.
    >> >> >
    >> >> > Gnuplot generates pretty 3d pictures from such data, where you are
    >> >> > standing above a surface and looking down at it. How does one do that
    >> >> > in R?
    >> >> >
    >> >> > Any help will be most appreciated. A dput() of my data object is :
    >> >> >
    >> >> > structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    >> >> > 0, 0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,
    >> >> > 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1,
    >> >> > 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,
    >> >> > 0.1, 0.1, 0.1, 0.1, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,
    >> >> > 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,
    >> >> > 0.15, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    >> >> > 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25,
    >> >> > 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,
    >> >> > 0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,
    >> >> > 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.35,
    >> >> > 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35,
    >> >> > 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.4, 0.4, 0.4, 0.4,
    >> >> > 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,
    >> >> > 0.4, 0.4, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,
    >> >> > 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.5,
    >> >> > 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,
    >> >> > 0.5, 0.5, 0.5, 0.5, 0.5, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55,
    >> >> > 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55,
    >> >> > 0.55, 0.55, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,
    >> >> > 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.65, 0.65, 0.65,
    >> >> > 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65,
    >> >> > 0.65, 0.65, 0.65, 0.65, 0.65, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7,
    >> >> > 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.75,
    >> >> > 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,
    >> >> > 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.8, 0.8, 0.8, 0.8,
    >> >> > 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,
    >> >> > 0.8, 0.8, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85,
    >> >> > 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.9,
    >> >> > 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,
    >> >> > 0.9, 0.9, 0.9, 0.9, 0.9, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,
    >> >> > 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,
    >> >> > 0.95, 0.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
    >> >> > 1, 1, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140,
    >> >> > 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90,
    >> >> > 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30,
    >> >> > 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170,
    >> >> > 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120,
    >> >> > 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70,
    >> >> > 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200,
    >> >> > 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150,
    >> >> > 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100,
    >> >> > 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40,
    >> >> > 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180,
    >> >> > 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130,
    >> >> > 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80,
    >> >> > 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20,
    >> >> > 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160,
    >> >> > 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110,
    >> >> > 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50,
    >> >> > 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180,
    >> >> > 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130,
    >> >> > 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80,
    >> >> > 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20,
    >> >> > 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160,
    >> >> > 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110,
    >> >> > 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50,
    >> >> > 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180,
    >> >> > 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130,
    >> >> > 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80,
    >> >> > 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20,
    >> >> > 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160,
    >> >> > 170, 180, 190, 200, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
    >> >> > 0, 0, 0, 0, 0, 1, 1, 1, 1, 0.998, 0.124, 0, 0, 0, 0, 0, 0, 0,
    >> >> > 0, 0, 0, 0, 0, 0, 1, 1, 1, 0.998, 0.71, 0.068, 0, 0, 0, 0, 0,
    >> >> > 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0.998, 0.898, 0.396, 0.058, 0.002,
    >> >> > 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.998, 0.97, 0.726, 0.268,
    >> >> > 0.056, 0.006, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.996, 0.88,
    >> >> > 0.546, 0.208, 0.054, 0.012, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    >> >> > 0, 0, 0.998, 0.964, 0.776, 0.418, 0.18, 0.054, 0.014, 0.002,
    >> >> > 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.998, 0.906, 0.664, 0.342,
    >> >> > 0.166, 0.056, 0.018, 0.006, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    >> >> > 0, 0.986, 0.862, 0.568, 0.29, 0.15, 0.056, 0.022, 0.008, 0.002,
    >> >> > 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.954, 0.778, 0.494, 0.26, 0.148,
    >> >> > 0.056, 0.024, 0.012, 0.004, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    >> >> > 0.906, 0.712, 0.43, 0.242, 0.144, 0.058, 0.028, 0.012, 0.006,
    >> >> > 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.878, 0.642, 0.38, 0.222,
    >> >> > 0.142, 0.066, 0.034, 0.014, 0.008, 0.004, 0.002, 0, 0, 0, 0,
    >> >> > 0, 0, 0, 0, 0.846, 0.586, 0.348, 0.208, 0.136, 0.068, 0.034,
    >> >> > 0.016, 0.012, 0.006, 0.004, 0.002, 0, 0, 0, 0, 0, 0, 0, 0.8,
    >> >> > 0.538, 0.318, 0.204, 0.136, 0.07, 0.046, 0.024, 0.012, 0.008,
    >> >> > 0.004, 0.002, 0.002, 0, 0, 0, 0, 0, 0, 0.762, 0.496, 0.294, 0.2,
    >> >> > 0.138, 0.072, 0.05, 0.024, 0.014, 0.012, 0.006, 0.004, 0.002,
    >> >> > 0.002, 0, 0, 0, 0, 0, 0.704, 0.472, 0.286, 0.198, 0.138, 0.074,
    >> >> > 0.054, 0.028, 0.016, 0.012, 0.008, 0.006, 0.004, 0.002, 0.002,
    >> >> > 0, 0, 0, 0, 0.668, 0.438, 0.276, 0.196, 0.138, 0.078, 0.054,
    >> >> > 0.032, 0.024, 0.014, 0.012, 0.008, 0.004, 0.004, 0.002, 0.002,
    >> >> > 0, 0, 0, 0.634, 0.412, 0.27, 0.194, 0.14, 0.086, 0.056, 0.032,
    >> >> > 0.024, 0.016, 0.012, 0.01, 0.006, 0.004, 0.004, 0.002, 0.002,
    >> >> > 0, 0, 0.604, 0.388, 0.26, 0.19, 0.144, 0.088, 0.058, 0.048, 0.026,
    >> >> > 0.022, 0.014, 0.012, 0.008, 0.006, 0.004, 0.004, 0.002, 0.002,
    >> >> > 0, 0.586, 0.376, 0.256, 0.19, 0.146, 0.094, 0.062, 0.052, 0.028,
    >> >> > 0.024, 0.014, 0.012, 0.012, 0.008, 0.004, 0.004, 0.004, 0.002,
    >> >> > 0.002, 0.566, 0.364, 0.254, 0.192, 0.148, 0.098, 0.064, 0.054,
    >> >> > 0.032, 0.024, 0.022, 0.014, 0.012, 0.012, 0.008, 0.004, 0.004,
    >> >> > 0.004, 0.002), .Dim = c(399, 3), .Dimnames = list(NULL, c("x",
    >> >> > "y", "z")))
    >> >> >
    >> >> > --
    >> >> > Ajay Shah                                      http://www.mayin.org/ajayshah
    >> >> > ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
    >> >> > <*(:-? - wizard who doesn't know the answer.
    >> >> >
    >> >> > ______________________________________________
    >> >> > R-help at stat.math.ethz.ch mailing list
    >> >> > https://stat.ethz.ch/mailman/listinfo/r-help
    >> >> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
    >> >>
    >> >> ______________________________________________
    >> >> R-help at stat.math.ethz.ch mailing list
    >> >> https://stat.ethz.ch/mailman/listinfo/r-help
    >> >> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
    >> >>
    >> >
    >> > ______________________________________________
    >> > R-help at stat.math.ethz.ch mailing list
    >> > https://stat.ethz.ch/mailman/listinfo/r-help
    >> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
    >> 
    >> 

    Gabor> ______________________________________________
    Gabor> R-help at stat.math.ethz.ch mailing list
    Gabor> https://stat.ethz.ch/mailman/listinfo/r-help
    Gabor> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From scruveil at genoscope.cns.fr  Tue Jun 27 09:19:29 2006
From: scruveil at genoscope.cns.fr (Stephane CRUVEILLER)
Date: Tue, 27 Jun 2006 09:19:29 +0200
Subject: [R] RMySQL...Can't initialize driver???
Message-ID: <200606270919.29654.scruveil@genoscope.cns.fr>

Dear R users,

I would like to query a MySQL database through R. I have installed
the latest required packages (RMySQL and DBI) in R (v2.3.1). A MySQL
server (v5.0.22) is running on my local machine but I can't initialize MYSQL 
driver: 
------------------------------------------------------------------------------------
> library("RMySQL")
Loading required package: DBI
> MySQL(max.con = 10, fetch.default.rec = 500, force.reload = F)
Error in new("MySQLDriver", Id = drvId) : unused argument(s) (Id ...)
-------------------------------------------------------------------------------------

Could somebody tell me what I have missed?


Thx

St?phane.


-- 
==========================================================
Stephane CRUVEILLER Ph. D.
Genoscope - Centre National de Sequencage
Atelier de Genomique Comparative
2, Rue Gaston Cremieux   CP 5706
91057 Evry Cedex - France
Phone: +33 (0)1 60 87 84 58
Fax: +33 (0)1 60 87 25 14
EMails: scruveil at genoscope.cns.fr ,scruvell at infobiogen.fr


From joe-byers at utulsa.edu  Sun Jun 25 06:05:44 2006
From: joe-byers at utulsa.edu (Joe Byers)
Date: Sat, 24 Jun 2006 23:05:44 -0500
Subject: [R] GARCH message 75
Message-ID: <449E0B98.8040304@utulsa.edu>

All,

You might check out the @ operator on Classes and objects.  I can find 
no documentation on this but if you look at code in fseries or fbasic in 
the method fARMA.summary.  You will find where the object fit in the 
returned object is access using the @ operator.

        object <- x at fit
        ans$coefmat <- cbind(format(object$coef,digits=digits), 
format(object$se.coef,digits=digits),
            format(tval,digits=digits), 
prob=format.pval(prob,digits=digits))

where x is the from x<-armaFit(....).  This I believe would be the same 
for the GARCH results.

Note the format.pval on the prob.  This is important because if you do 
not do this you will get 0 for small pvalues.  I learned this by looking 
at the code of the pretty printing functions so I can save only results 
that I want from multiple models runs.  You also can then to wald test 
and LL ratio tests on the models.

Good luck
Joe


From sara at gmesintra.com  Tue Jun 27 10:56:23 2006
From: sara at gmesintra.com (Sara Mouro)
Date: Tue, 27 Jun 2006 09:56:23 +0100
Subject: [R] R on MAC OS X
References: <CA9F1357-D5DE-481B-86D6-E4F794C4603B@gmesintra.com>
Message-ID: <E44BDD1F-3816-4419-AA16-DDBEACB87B4A@gmesintra.com>



>
> Dear all,
>
> I have been usig R for some time, but now I have a MAC instead of a  
> PC, am I am having problems in reading files...
>
>
> I have tried:
> Data<-read.table("Users/SaraMM/PhD/Analises-LitterBags/Dados- 
> Litter.txt",head=T)
>
> but it said:
> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open file 'Users/SaraMM/PhD/Analises-LitterBags/Dados- 
> Litter.txt', reason 'No such file or directory'
>
>
> I guess that might be simple...
> ... related to the fact that in PCs we do "C:/........"
> .... but in iMAC, the only "path" I found was that one: "Users/ 
> SaraMM(...)"...
>
> Can someone please help me on this?
>
> Wishes,
> Sara


From knoblauch at lyon.inserm.fr  Tue Jun 27 11:04:20 2006
From: knoblauch at lyon.inserm.fr (ken knoblauch)
Date: Tue, 27 Jun 2006 11:04:20 +0200
Subject: [R]  R on MAC OS X
Message-ID: <37cf9c533d748a6121f33e1d9ed3ad3f@lyon.inserm.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060627/bc45fcc3/attachment.pl 

From cinnamon313 at web.de  Tue Jun 27 11:05:21 2006
From: cinnamon313 at web.de (Tanja Seifried)
Date: Tue, 27 Jun 2006 11:05:21 +0200
Subject: [R] Mauchly and Levene
Message-ID: <2015593961@web.de>

Hallo!


I just started working with R to do the statistical analyses for my diploma thesis.
I got two sets of data. 
Both contain repeated measures.
One has only one within-subject factor with four levels.
The other has one within-subject factor with two levels and one between-factor with two levels.
 
I want to compute a Mauchly test for both sets and a Levene test for the second set.
 
I couldn't find the LEvene test in R. Is it there?
 
I found the mauchly.test but I wasn't able to find out how to compute the error of covariance matrix I need as an argument for mauchly.test.
could anybody help me with the correct input I have to give to Mauchly.test so R will give me a test of sphericity out of my data?
 
 Thanks in advance!
 Greetings, Tanja


______________________________________________________________________________
Mit WEB.DE iNews werden Sie ?ber die Ergebnisse der wichtigsten WM-Begegnungen


From j.van_den_hoff at fz-rossendorf.de  Tue Jun 27 11:10:04 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Tue, 27 Jun 2006 11:10:04 +0200
Subject: [R] R on MAC OS X
In-Reply-To: <E44BDD1F-3816-4419-AA16-DDBEACB87B4A@gmesintra.com>
References: <CA9F1357-D5DE-481B-86D6-E4F794C4603B@gmesintra.com>
	<E44BDD1F-3816-4419-AA16-DDBEACB87B4A@gmesintra.com>
Message-ID: <44A0F5EC.9010104@fz-rossendorf.de>

Sara Mouro wrote:
> 
>> Dear all,
>>
>> I have been usig R for some time, but now I have a MAC instead of a  
>> PC, am I am having problems in reading files...
>>
>>
>> I have tried:
>> Data<-read.table("Users/SaraMM/PhD/Analises-LitterBags/Dados- 
>> Litter.txt",head=T)
>>
>> but it said:
>> Error in file(file, "r") : unable to open connection
>> In addition: Warning message:
>> cannot open file 'Users/SaraMM/PhD/Analises-LitterBags/Dados- 
>> Litter.txt', reason 'No such file or directory'
>>
>>
>> I guess that might be simple...
>> ... related to the fact that in PCs we do "C:/........"
>> .... but in iMAC, the only "path" I found was that one: "Users/ 
>> SaraMM(...)"...
>>
>> Can someone please help me on this?
>>
>> Wishes,
>> Sara
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


the (absolute) path to your file has to start with `/', i.e. your file 
should be

"/Users/SaraMM/PhD/Analises-LitterBags/Dados-Litter.txt"
  ^
  |

the leading slash was missing in your read.table call (in the original 
way it is a relative path name and would only work, if your current 
directory within R were `/' itself)

joerg


From Arne.Jol at Unilever.com  Tue Jun 27 11:18:57 2006
From: Arne.Jol at Unilever.com (Jol, Arne)
Date: Tue, 27 Jun 2006 10:18:57 +0100
Subject: [R] weights in multinom
Message-ID: <2D0E2123711DE7478FE1EB933CD3046E20D5A1@BRBSEVS20001.s2.ms.unilever.com>

Best R Help,

I like to estimate a Multinomial Logit Model with 10 Classes. The
problem is that the number of observations differs a lot over the 10
classes:

Class | num. Observations
A | 373
B | 631
C | 171
D | 700
E | 87
F | 249
G | 138
H | 133
I | 162
J | 407
Total: 3051

Where my data looks like:

x1	x2	x3	x4	Class
1	1,02	2	1	A
2	7,2	1	5	B
3	4,2	1	4	H
1	4,1	1	8	F
2	2,4	3	7	D
1	1,2	0	4	J
2	0,9	1	2	G
4	4	3	0	C
.	.	.	.	.

My model looks like:
estmodel <- multinom(choice ~ x1 + x2 + x3 + x4, data = trainset) 

When I estimate the model and use the resulting model for prediction of
'new' observations the model has a bias towards the Classes with a large
number of observations (A,B,D,J), the other classes are never predicted
by the model.

I thougth that the option "weights" of the multinom function could be
usefull but I am not sure how to use this in the above case.

Is there someone with experience regarding such a weigthing approach in
multinom? If someone could help me with suggestions it would be great!

Nice day,
Arne


From p.dalgaard at biostat.ku.dk  Tue Jun 27 11:22:57 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Jun 2006 11:22:57 +0200
Subject: [R] Mauchly and Levene
In-Reply-To: <2015593961@web.de>
References: <2015593961@web.de>
Message-ID: <x2odwfj6ku.fsf@viggo.kubism.ku.dk>

Tanja Seifried <cinnamon313 at web.de> writes:

> Hallo!
> 
> 
> I just started working with R to do the statistical analyses for my diploma thesis.
> I got two sets of data. 
> Both contain repeated measures.
> One has only one within-subject factor with four levels.
> The other has one within-subject factor with two levels and one between-factor with two levels.
>  
> I want to compute a Mauchly test for both sets and a Levene test for the second set.
>  
> I couldn't find the LEvene test in R. Is it there?

There's one in the "car" package.
  
> I found the mauchly.test but I wasn't able to find out how to compute the error of covariance matrix I need as an argument for mauchly.test.
> could anybody help me with the correct input I have to give to Mauchly.test so R will give me a test of sphericity out of my data?

Someone already did. This is the first example on the help page for
mauchly.test. And generating SSD/Variance matrices is on the help page
for SSD.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ligges at statistik.uni-dortmund.de  Tue Jun 27 11:33:24 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 27 Jun 2006 11:33:24 +0200
Subject: [R] write.table & csv help
In-Reply-To: <20060626183436.90021.qmail@web37612.mail.mud.yahoo.com>
References: <20060626183436.90021.qmail@web37612.mail.mud.yahoo.com>
Message-ID: <44A0FB64.2040807@statistik.uni-dortmund.de>

Sachin J wrote:
> Hi,
>    
>   How can I produce the following output in .csv format using write.table function.
>    
>   for(i in seq(1:2))
>
>  df <- rnorm(4, mean=0, sd=1)
>  write.table(df,"C:/output.csv", append = TRUE, quote = FALSE, sep = ",", row.names = FALSE, col.names = TRUE)
> }


You cannot append columns with write.table().

Uwe Ligges






>   Current O/p:
>               x    0.287816    -0.81803    -0.15231    -0.25849    x    2.26831    0.863174    0.269914    0.181486
>   
> Desired output
>               x1  x2    0.287816  2.26831    -0.81803  0.863174    -0.15231  0.269914    -0.25849  0.181486
>    
>   Thanx in advance
>    
>   Sachin
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ligges at statistik.uni-dortmund.de  Tue Jun 27 11:38:54 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 27 Jun 2006 11:38:54 +0200
Subject: [R] analyze summary data
In-Reply-To: <loom.20060625T190408-80@post.gmane.org>
References: <E866C4CD-74DC-4F3F-8E40-3E8F14B9F918@unibas.ch>
	<loom.20060625T190408-80@post.gmane.org>
Message-ID: <44A0FCAE.6020306@statistik.uni-dortmund.de>

Ben Bolker wrote:
> Thierry Girard <thierry.girard <at> unibas.ch> writes:
> 
>> I do have summary data (mean, standard deviation and sample size n)  
>> and want to analyze this data.
>> The summary data is supposed to be from a normal distribution.
>>
>> I need the following calculations on this summary data (no, I do not  
>> have the original data):
>>
>> - one sample t-test against a known mu
>> - two sample t-test
>> - analysis of variance between 4 groups.
>>
>> I would appreciate any help available.
>>
>> One possible solution could be to simulate the data using rnorm with  
>> the appropriate n, mu and sd, but I don't know if there would be a  
>> more accurate solution.
> 
> 
>   this is the kind of situation where you need to go back to the basics --
> knowing what computations these statistical tests are _actually
> doing_ -- which you should be able to find in any basic stats book, 
> or by digging
> into the guts of the R functions.  The only other thing you need to
> know is the R functions for cumulative distribution functions, pt
> (for the t distribution) and pf (for the F dist.)
> 
>   For example:
> 
>    stats:::t.test.default
> 
>  has lots of complicated stuff inside but the key lines are
> (for a one sample test)
> 
>  nx <- length(x)
>   df <- nx - 1
>   stderr <- sqrt(vx/nx)
>   # if you already have the standard deviation then you want
>   # sqrt(sd^2/nx)
>  tstat <- (mx - mu)/stderr   ## mu is the known mean you're testing against
>  pval <- 2 * pt(-abs(tstat), df)
> 
> (assuming 2-tailed)
> 
>   you will find similar stuff for the two-sample t-test,
> depending on your particular choices.
> 
>   The 1-way ANOVA might be harder to dig out of the R code;
> there you're better off going back and (re)learning from
> a basic stats treatment how to
> compute the between-group and (pooled) within-group variances.
> 
>   Bottom line is that, except for knowing about pt and pf,
> this is really a basic statistics question rather than an
> R question.
> 
>   good luck
>     Ben Bolker
> 
> PS: it is too bad, but the increasing sophistication of R is
> making it harder for beginners to explore the guts --- e.g.
> knowing to look for "stats:::t.test.default" in order to find
> the code ...

Thanks for the hint, I already had in mind writing an R Help Desk about 
"Finding the code" meaning both, R source code as described above as 
well as C code corresponding to the .Primitive, .C, .Call and friends' 
entry points.
Maybe for the next R News issue, if nobody is willing to contribute to 
the Help Desk column (hint, hint!!!).

Uwe Ligges


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From petr.pikal at precheza.cz  Tue Jun 27 11:44:27 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 27 Jun 2006 11:44:27 +0200
Subject: [R] write.table & csv help
Message-ID: <44A11A1B.6382.602AD1F@localhost>

Hi

I am not sure about what you want to achieve by sequential writing to 
a file so maybe you could fill an object in a loop and write it only 
once.

df<-data.frame(matrix(nrow=4,ncol=2))
for(i in seq(1:2))
{
 df[,i]  <- rnorm(4, mean=0, sd=1)
}
write.table(df,"output.csv", quote = FALSE, sep = ",", row.names = 
FALSE, col.names = TRUE)

HTH
Petr



From: Sachin J <sachinj.2006_at_yahoo.com> 
Date: Tue 27 Jun 2006 - 04:34:36 EST


Hi,     

  How can I produce the following output in .csv format using 
write.table function.     

  for(i in seq(1:2)) 
{ 
 df <- rnorm(4, mean=0, sd=1) 
 write.table(df,"C:/output.csv", append = TRUE, quote = FALSE, sep = 
",", row.names = FALSE, col.names = TRUE) } 

  Current O/p: 

              x 0.287816 -0.81803 -0.15231 -0.25849 x 2.26831 
0.863174 0.269914 0.181486    

Desired output 

              x1 x2 0.287816 2.26831 -0.81803 0.863174 -0.15231 
0.269914 -0.25849 0.181486     

  Thanx in advance     

  Sachin                   


----------------------------------------------------------------------
----------

        [[alternative HTML version deleted]] 


----------------------------------------------------------------------
----------

R-help at stat.math.ethz.ch mailing list 
https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the 
posting guide! http://www.R-project.org/posting-guide.html
Petr Pikal
petr.pikal at precheza.cz


From francois.michonneau at gmail.com  Tue Jun 27 11:54:22 2006
From: francois.michonneau at gmail.com (=?ISO-8859-1?Q?Fran=E7ois_MICHONNEAU?=)
Date: Tue, 27 Jun 2006 11:54:22 +0200
Subject: [R] change the class of an object within with()
Message-ID: <44A1004E.4000600@gmail.com>

Hello,

Can anyone explain me what is wrong in the following code? and in particular why it is not 
possible to change the class of an object thanks to the function with(). Does an 
alternative exist?

xxx <- data.frame(x = c("a","b","c","d","e"), y = c("1","2","3","4","5"))
str(xxx)
with(xxx, {
	x <- as.character(x)
	y <- as.numeric(y)
})
str(xxx) #no effect on the class of x and y

xxx$x <- as.character(xxx$x)
xxx$y <- as.numeric(xxx$y)
str(xxx)

Thanks

Fran?ois Michonnneau


From plummer at iarc.fr  Tue Jun 27 11:56:14 2006
From: plummer at iarc.fr (Martyn Plummer)
Date: Tue, 27 Jun 2006 11:56:14 +0200
Subject: [R] Griddy-Gibbs sampler
In-Reply-To: <20060626215206.52785.qmail@web32107.mail.mud.yahoo.com>
References: <20060626215206.52785.qmail@web32107.mail.mud.yahoo.com>
Message-ID: <1151402174.2395.4.camel@seurat.iarc.fr>

OpenBUGS has a Griddy Gibbs sampler, written in Component Pascal

http://www.mathstat.helsinki.fi/openbugs/

The source code is not in plain text format. You will need to install
the Black Box Component Builder to read it. I hope this helps.

Martyn

On Mon, 2006-06-26 at 14:52 -0700, Elizabeth Lawson wrote:
> Hey everyone,
>    
>   I have read the paper by Ritter and Tanner(1992) on Griddy-Gibbs
> sampler and I am trying to implement it in R without much luck.  I was
> wondering if anyone had used this or could point me to any example
> code.
>    
>   Thanks,
>    
>   Liz


-----------------------------------------------------------------------
This message and its attachments are strictly confidential. ...{{dropped}}


From raharjo_hendry at yahoo.com.sg  Tue Jun 27 12:21:32 2006
From: raharjo_hendry at yahoo.com.sg (hendry raharjo)
Date: Tue, 27 Jun 2006 18:21:32 +0800 (CST)
Subject: [R] compositional time series
Message-ID: <20060627102132.60524.qmail@web35712.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060627/ea60a2e9/attachment.pl 

From p.dalgaard at biostat.ku.dk  Tue Jun 27 12:31:10 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Jun 2006 12:31:10 +0200
Subject: [R] change the class of an object within with()
In-Reply-To: <44A1004E.4000600@gmail.com>
References: <44A1004E.4000600@gmail.com>
Message-ID: <x2k672khzl.fsf@viggo.kubism.ku.dk>

Fran?ois MICHONNEAU <francois.michonneau at gmail.com> writes:

> Hello,
> 
> Can anyone explain me what is wrong in the following code? and in particular why it is not 
> possible to change the class of an object thanks to the function with(). Does an 
> alternative exist?
> 
> xxx <- data.frame(x = c("a","b","c","d","e"), y = c("1","2","3","4","5"))
> str(xxx)
> with(xxx, {
> 	x <- as.character(x)
> 	y <- as.numeric(y)
> })
> str(xxx) #no effect on the class of x and y
> 
> xxx$x <- as.character(xxx$x)
> xxx$y <- as.numeric(xxx$y)
> str(xxx)

This is due to the way with() (and eval() & friends) deals with data
frames. The problem is that they are converted internally to
environments before the expressions are evaluated. This means
effectively that any assignments or modifications go into a temporary
copy of the data frame and are then lost. 

This is a bit unfortunate, in that it would be really nice for people
to be able to say 

  with(foo, bmi <- weight/height^2)

and have foo extended with a new bmi column. However, there are snags.
In particular, how would you deal with a computation that yielded a
result that was incompatible with the data frame, like a function, an
lm object, or just a vector of the wrong length? Some of us tend to
think that this should be sorted out eventually, but for now it just
doesn't work. 

You've already shown one alternative, another one is to use
transform().   

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ggrothendieck at gmail.com  Tue Jun 27 12:54:02 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 27 Jun 2006 06:54:02 -0400
Subject: [R] change the class of an object within with()
In-Reply-To: <x2k672khzl.fsf@viggo.kubism.ku.dk>
References: <44A1004E.4000600@gmail.com> <x2k672khzl.fsf@viggo.kubism.ku.dk>
Message-ID: <971536df0606270354p674b4a40r98b2fe4725be72d0@mail.gmail.com>

On 27 Jun 2006 12:31:10 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Fran?ois MICHONNEAU <francois.michonneau at gmail.com> writes:
>
> > Hello,
> >
> > Can anyone explain me what is wrong in the following code? and in particular why it is not
> > possible to change the class of an object thanks to the function with(). Does an
> > alternative exist?
> >
> > xxx <- data.frame(x = c("a","b","c","d","e"), y = c("1","2","3","4","5"))
> > str(xxx)
> > with(xxx, {
> >       x <- as.character(x)
> >       y <- as.numeric(y)
> > })
> > str(xxx) #no effect on the class of x and y
> >
> > xxx$x <- as.character(xxx$x)
> > xxx$y <- as.numeric(xxx$y)
> > str(xxx)
>
> This is due to the way with() (and eval() & friends) deals with data
> frames. The problem is that they are converted internally to
> environments before the expressions are evaluated. This means
> effectively that any assignments or modifications go into a temporary
> copy of the data frame and are then lost.
>
> This is a bit unfortunate, in that it would be really nice for people
> to be able to say
>
>  with(foo, bmi <- weight/height^2)
>
> and have foo extended with a new bmi column. However, there are snags.
> In particular, how would you deal with a computation that yielded a
> result that was incompatible with the data frame, like a function, an
> lm object, or just a vector of the wrong length? Some of us tend to
> think that this should be sorted out eventually, but for now it just
> doesn't work.
>
> You've already shown one alternative, another one is to use
> transform().
>

transform is probably the best but if you reallly want to use 'with' :

   xxx[c("x", "y")] <- with(xxx, list(as.character(x), as.numeric(y)))

or the following two if there are no additional columns in xxx:

   xxx[] <- with(xxx, list(as.character(x), as.numeric(y)))

   xxx <- with(xxx, data.frame(x = as.character(x), y = as.numeric(y)))


From bernarduse1 at yahoo.fr  Tue Jun 27 13:05:13 2006
From: bernarduse1 at yahoo.fr (Marc Bernard)
Date: Tue, 27 Jun 2006 13:05:13 +0200 (CEST)
Subject: [R] horizontal yaxis label
Message-ID: <20060627110513.52661.qmail@web25801.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060627/6b45020e/attachment.pl 

From ripley at stats.ox.ac.uk  Tue Jun 27 13:21:24 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 27 Jun 2006 12:21:24 +0100 (BST)
Subject: [R] weights in multinom
In-Reply-To: <2D0E2123711DE7478FE1EB933CD3046E20D5A1@BRBSEVS20001.s2.ms.unilever.com>
References: <2D0E2123711DE7478FE1EB933CD3046E20D5A1@BRBSEVS20001.s2.ms.unilever.com>
Message-ID: <Pine.LNX.4.64.0606271220430.15913@gannet.stats.ox.ac.uk>

See the references for ?multinom and ?nnet: this is covered in my 1996 
book.

On Tue, 27 Jun 2006, Jol, Arne wrote:

> Best R Help,
>
> I like to estimate a Multinomial Logit Model with 10 Classes. The
> problem is that the number of observations differs a lot over the 10
> classes:
>
> Class | num. Observations
> A | 373
> B | 631
> C | 171
> D | 700
> E | 87
> F | 249
> G | 138
> H | 133
> I | 162
> J | 407
> Total: 3051
>
> Where my data looks like:
>
> x1	x2	x3	x4	Class
> 1	1,02	2	1	A
> 2	7,2	1	5	B
> 3	4,2	1	4	H
> 1	4,1	1	8	F
> 2	2,4	3	7	D
> 1	1,2	0	4	J
> 2	0,9	1	2	G
> 4	4	3	0	C
> .	.	.	.	.
>
> My model looks like:
> estmodel <- multinom(choice ~ x1 + x2 + x3 + x4, data = trainset)
>
> When I estimate the model and use the resulting model for prediction of
> 'new' observations the model has a bias towards the Classes with a large
> number of observations (A,B,D,J), the other classes are never predicted
> by the model.
>
> I thougth that the option "weights" of the multinom function could be
> usefull but I am not sure how to use this in the above case.
>
> Is there someone with experience regarding such a weigthing approach in
> multinom? If someone could help me with suggestions it would be great!
>
> Nice day,
> Arne
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ccleland at optonline.net  Tue Jun 27 13:34:18 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 27 Jun 2006 07:34:18 -0400
Subject: [R] horizontal yaxis label
In-Reply-To: <20060627110513.52661.qmail@web25801.mail.ukl.yahoo.com>
References: <20060627110513.52661.qmail@web25801.mail.ukl.yahoo.com>
Message-ID: <44A117BA.3040108@optonline.net>

Marc Bernard wrote:
> Dear all,
>    
>   I wonder how to get  an  horizontal   label for the y axis in the plot function. I have looked at par and many function but didn't find it.
>   more specifically , let:
>    
>   df <- data.frame(cbind(1:10, 11:20))
>   names(df) <- c("y","x")
>    
>   plot(y ~ x, data = df, ylab = "dependent")
>    
>   how to ask R to have the label "independent" in an horizontal form?

   I think you want something like this:

df <- data.frame(cbind(1:10, 11:20))
names(df) <- c("y","x")

par(mar = c(7, 10, 7, 7))
plot(y ~ x, data = df, ylab = "", xlab = "independent")
text(par("usr")[1] - 1, 5.5, adj = 1, labels = "dependent", xpd = TRUE)

   Adapted from the example in section 7.27 of 
http://cran.r-project.org/doc/FAQ/R-FAQ.html

>   I have another question concerning the xyplot:
>    
>   Suppose that I have made an xyplot with 4 panels defined by one factor variable. How may I have a blank space between the four panels?

   See the between argument.  For example:

library(lattice)

df2 <- data.frame(Y = runif(20), X = rnorm(20), G = 
rep(c("A","B","C","D"), each=5))

xyplot(Y ~ X | G, data = df2, layout=c(2,2), between=list(x=1,y=1))

>   Thank you,
>    
>   Bernard,
>    
>    
>    
>    
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From ezhil02 at yahoo.com  Tue Jun 27 13:45:01 2006
From: ezhil02 at yahoo.com (A Ezhil)
Date: Tue, 27 Jun 2006 04:45:01 -0700 (PDT)
Subject: [R] Boxplot questions.
Message-ID: <20060627114501.96501.qmail@web32115.mail.mud.yahoo.com>

Dear all,

I am having a data for 2 different treatments with
different time points. So, I used the following code
to plot the boxplot and also to do anova. 

T11 <- c(280, 336, 249, 277, 429)
T12 <- c(400, 397, 285, 407, 313)
T13 <- c(725, 373, 364, 706, 249)

T21 <- c(589, 257, 466, 248, 913)
T22 <- c(519, 424, 512, 298, 907)
T23 <- c(529, 479, 634, 354, 1015)

obs <- c(T11, T12, T13, T21, T22, T23)
treat <- c(rep("T1",15), rep("T2",15))
time <- c(rep("one",5), rep("two",5), rep("thr",5),  
          rep("one",5), rep("two",5), rep("thr",5)  
         )

table <- data.frame(obs, treat, time)
boxplot(obs ~ treat*time, data=table)

I am able to produce a boxplot for the above data. 
(1) If I want to add a color to T1 and different color
to T2, what are the options I should use?
(2) my prof. asked me to add '*' above the boxplot, if
the comparison is significant. How do I add this '*'
in the boxplot? 

I used: anova(lm(obs ~ treat*time, data=table)) for
finding significance.

Any help is greatly appreciated. Thanks in advance.

Regards,
Ezhil


From m.ballardini at ior-forli.it  Tue Jun 27 13:57:38 2006
From: m.ballardini at ior-forli.it (Michela Ballardini)
Date: Tue, 27 Jun 2006 13:57:38 +0200
Subject: [R] Survival
Message-ID: <000e01c699e0$e373a9d0$0200a8c0@Michela>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060627/84d9bd54/attachment.pl 

From cinnamon313 at web.de  Tue Jun 27 13:58:44 2006
From: cinnamon313 at web.de (Tanja Seifried)
Date: Tue, 27 Jun 2006 13:58:44 +0200
Subject: [R] Mauchly and Levene
Message-ID: <2015835067@web.de>

Hallo

thanks a lot for the answer, I found the Levene test in the car package.

I already tried to compute my Mauchly.test the same way as in the example(SSD)
matrix <- lm(data)
mauchly.test(matrix,X=~1)

I was just unsure about it because with that I get a different result than when I do the Mauchly test with SPSS... hm

thanks again,
Tanja


> Tanja Seifried <cinnamon313 at web.de> writes:
> 
> > Hallo!
> > 
> > 
> > I just started working with R to do the statistical analyses for my diploma thesis.
> > I got two sets of data. 
> > Both contain repeated measures.
> > One has only one within-subject factor with four levels.
> > The other has one within-subject factor with two levels and one between-factor with two levels.
> >  
> > I want to compute a Mauchly test for both sets and a Levene test for the second set.
> >  
> > I couldn't find the LEvene test in R. Is it there?
> 
> There's one in the "car" package.
>   
> > I found the mauchly.test but I wasn't able to find out how to compute the error of covariance matrix I need as an argument for mauchly.test.
> > could anybody help me with the correct input I have to give to Mauchly.test so R will give me a test of sphericity out of my data?
> 
> Someone already did. This is the first example on the help page for
> mauchly.test. And generating SSD/Variance matrices is on the help page
> for SSD.
> 
> -- 
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From andy_liaw at merck.com  Tue Jun 27 14:05:09 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 27 Jun 2006 08:05:09 -0400
Subject: [R] questions on local customized R distribution CD
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA028840FC@usctmx1106.merck.com>

 
From: Duncan Murdoch
> 
> On 6/26/2006 3:14 PM, Dongseok Choi wrote:
> > Hello all!
> >   
> >   I hope this is the right place to post this question.
> >  
> >   The Oregon Chapter of ASA is working with local high 
> school teachers as one of its outreaching program.
> >   We hope to use and test R as teaching tools.
> >   So, we think that a menu system (like R commander) with a 
> few packages and a bit simplified installation instruction 
> need to be developed.
> >  
> >   The main question is:
> > 1)
> > Is it OK to develop a customized CD-ROM distribution of R  
> with pre-selected packages for high school? 
> > It will be distributed free, of course.
> > Also, we plan to make it available from the chap web or 
> deposit it to R-project, if requested.
> 
> Generally the answer is yes, but read the GPL for the 
> conditions.  You do need to make the source code available.

I was under the impression that telling the user how to get the source code
would satisfy the GPL, instead of distributing the source along with the
binary.  Is that right?

> > 2)
> >   If the customized distribution CD is OK, I also hope to 
> get some technical help/advice from the core group members if 
> any one is interested.
> 
> See the R Installation and Administration manual first.  It 
> tells how to build R installers with non-standard included 
> packages.  Hopefully for 2.4.0 more customizations will be possible.

Yes, it's not all that hard.  Follow the directions carefully and literally
and there shouldn't be problem.

Andy

 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From stecalza at tiscali.it  Tue Jun 27 14:22:48 2006
From: stecalza at tiscali.it (Stefano Calza)
Date: Tue, 27 Jun 2006 14:22:48 +0200
Subject: [R] Survival
In-Reply-To: <000e01c699e0$e373a9d0$0200a8c0@Michela>
References: <000e01c699e0$e373a9d0$0200a8c0@Michela>
Message-ID: <20060627122248.GH4676@med.unibs.it>

try using summary(os, censored=T)

Stef

On Tue, Jun 27, 2006 at 01:57:38PM +0200, Michela Ballardini wrote:
<Michela>Dear all,
<Michela>I write to know if it possible to know the number of censor subject in function of time. I run:
<Michela>os<-survfit(Surv(datios$time,datios$status))
<Michela>
<Michela>summary(os)
<Michela>
<Michela>but it give me only the nomber of events.
<Michela>Can you help me?
<Michela>Thanke you
<Michela>Michela
<Michela>
<Michela>
<Michela>**************************************
<Michela>Dr.ssa Michela Ballardini
<Michela>Unit? di Biostatistica e Sperimentazioni Cliniche
<Michela>c/o Osp. Morgagni-Pierantoni - Pad. Valsalva
<Michela>Via Forlanini, 34
<Michela>47100 Forl?
<Michela>Tel 0543-731836
<Michela>Tel/Fax 0543-731612
<Michela>**************************************
<Michela>
<Michela>
<Michela>	[[alternative HTML version deleted]]
<Michela>

<Michela>______________________________________________
<Michela>R-help a stat.math.ethz.ch mailing list
<Michela>https://stat.ethz.ch/mailman/listinfo/r-help
<Michela>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From roger.bos at gmail.com  Tue Jun 27 14:23:43 2006
From: roger.bos at gmail.com (roger bos)
Date: Tue, 27 Jun 2006 08:23:43 -0400
Subject: [R] Memory available to 32-bit R app on 64-bit machine
Message-ID: <1db726800606270523q4235fd03o4ce7dc751e5f313c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060627/ceaf3bab/attachment.pl 

From andy_liaw at merck.com  Tue Jun 27 14:27:34 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 27 Jun 2006 08:27:34 -0400
Subject: [R] reshaping data.frame question
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02884106@usctmx1106.merck.com>

You might want to try the Oarray package on CRAN.

Andy 

From: Matthias Braeunig
> 
> Thanks, this is not what what I meant. I need to reshape the 
> original dataframe that I can access p_f[X] for numerical X. 
> Maybe I was not clear enough.
> 
> The problem really is that X starts at 0. Note that in my 
> example changing the row names to 0:2 does not have the 
> desired effect.
> 
> 
> 
> jim holtman wrote:
> > You need to specify the row/column name as character:
> > 
> >> y
> >   X1  X2  X3  X4
> > 0 0.1 0.1 0.1 0.1
> > 1 0.2 0.2 0.2 0.2
> > 2 0.3 0.3 0.3 0.3
> > 
> >> y[,'X3']
> > [1] 0.1 0.2 0.3
> >> y['0','X3']
> > [1] 0.1
> > 
> > 
> > 
> > 
> > On 6/26/06, Matthias Braeunig <mb.atelier at web.de> wrote:
> >>
> > Dear R-helpers,
> > 
> > 
> > my data.frame is of the form
> > 
> > x <- data.frame( f=gl(4,3), X=rep(0:2,4), p=c(.1,.2,.3)) x
> >   f X   p
> > 1  1 0 0.1
> > 2  1 1 0.2
> > 3  1 2 0.3
> > 4  2 0 0.1
> > 5  2 1 0.2
> > 6  2 2 0.3
> > 7  3 0 0.1
> > 8  3 1 0.2
> > 9  3 2 0.3
> > 10 4 0 0.1
> > 11 4 1 0.2
> > 12 4 2 0.3
> > 
> > which  tabulates some values p(X) for several factors f.
> > 
> > Now I want to put it in "wide" format, so that factor 
> levels appear as 
> > column heads. Note also that X starts from zero. It would 
> be nice if I 
> > could simply access p_f[X==0] as f[0]. How can I possibly do that?
> > 
> > (The resilting object does not have to be a data.frame. As 
> there are 
> > only numeric values, also a matrix would do.)
> > 
> > I tried the following
> > 
> > y<-unstack(x,form=p~f)
> > row.names(y) <- 0:2
> > y
> >   X1  X2  X3  X4
> > 0 0.1 0.1 0.1 0.1
> > 1 0.2 0.2 0.2 0.2
> > 2 0.3 0.3 0.3 0.3
> > 
> > Now, how to access X3[0], say?
> > 
> > Maybe reshape would be the right tool, but I could not 
> figure it out.
> > 
> > I appreciate your help. Thanks!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From petr.pikal at precheza.cz  Tue Jun 27 14:33:06 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 27 Jun 2006 14:33:06 +0200
Subject: [R] Boxplot questions.
In-Reply-To: <20060627114501.96501.qmail@web32115.mail.mud.yahoo.com>
Message-ID: <44A141A2.30639.69D164C@localhost>

Hi

HTH
Petr



On 27 Jun 2006 at 4:45, A Ezhil wrote:

Date sent:      	Tue, 27 Jun 2006 04:45:01 -0700 (PDT)
From:           	A Ezhil <ezhil02 at yahoo.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Boxplot questions.

> Dear all,
> 
> I am having a data for 2 different treatments with
> different time points. So, I used the following code
> to plot the boxplot and also to do anova. 
> 
> T11 <- c(280, 336, 249, 277, 429)
> T12 <- c(400, 397, 285, 407, 313)
> T13 <- c(725, 373, 364, 706, 249)
> 
> T21 <- c(589, 257, 466, 248, 913)
> T22 <- c(519, 424, 512, 298, 907)
> T23 <- c(529, 479, 634, 354, 1015)
> 
> obs <- c(T11, T12, T13, T21, T22, T23)
> treat <- c(rep("T1",15), rep("T2",15))
> time <- c(rep("one",5), rep("two",5), rep("thr",5),  
>           rep("one",5), rep("two",5), rep("thr",5)  
>          )
> 
> table <- data.frame(obs, treat, time)
> boxplot(obs ~ treat*time, data=table)
> 
> I am able to produce a boxplot for the above data. 
> (1) If I want to add a color to T1 and different color
> to T2, what are the options I should use?

 from help page

 border an optional vector of colors for the outlines of the 
boxplots. The values in border are recycled if the length of border 
is less than the number of plots. 

 col if col is non-null it is assumed to contain colors to be used to 
colour the bodies of the box plots. By default they are in the 
background colour. 


 > boxplot(obs ~ treat*time, data=table, border=rep(1:3,each=2))


> (2) my prof. asked me to add '*' above the boxplot, if
> the comparison is significant. How do I add this '*'
> in the boxplot? 

see bxp and try

bbb<- boxplot(.....)

and look at bbb. You can derive positions of boxes from it and put 
any text e.g. asterix according to these positions.

HTH
Petr







> 
> I used: anova(lm(obs ~ treat*time, data=table)) for
> finding significance.
> 
> Any help is greatly appreciated. Thanks in advance.
> 
> Regards,
> Ezhil
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From andy_liaw at merck.com  Tue Jun 27 14:35:52 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 27 Jun 2006 08:35:52 -0400
Subject: [R] princomp and prcomp confusion
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA0288410A@usctmx1106.merck.com>

From: Patrick Connolly
> 
> When I look through archives at
> https://stat.ethz.ch/pipermail/r-help/2003-October/040525.html
> 
> I see this:
> 
> Liaw, Andy wrote:
> 
> >In the `Detail' section of ?princomp:
> >
> >princomp only handles so-called Q-mode PCA, that is feature 
> extraction 
> >of variables. If a data matrix is supplied (possibly via a 
> formula) it 
> >is required that there are at least as many units as variables. For 
> >R-mode PCA use prcomp.
> >
> 
> It doesn't appear that anyone disputed the accuracy of it.
> 
> 
> My current installation (version.string Version 2.3.1 
> (2006-06-01)) says in the detail of princomp
> 
> 
>      'princomp' only handles so-called R-mode PCA, that is feature
>      extraction of variables.  If a data matrix is supplied (possibly
>      via a formula) it is required that there are at least as many
>      units as variables.  For Q-mode PCA use 'prcomp'.
> 
> I've not been following principal components and have only 
> recently had a use for that methodology.  Am I to assume that 
> the later version is the correct one?  I thought I'd worked 
> out what the distinction between R-mode and Q-mode was, but 
> now I'm as confused as ever.

>From what I can gather by googling around about R-mode and Q-mode PCA, 
the current description should be the correct one.  

Andy


> best
> 
> -- 
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
> ~.~.~.~.~.   
>    ___    Patrick Connolly   
>  {~._.~}          		 Great minds discuss ideas    
>  _( Y )_  	  	        Middle minds discuss events 
> (:_~*~_:) 	       		 Small minds discuss people  
>  (_)-(_)  	                           ..... Anon
> 	  
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
> ~.~.~.~.~.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From ripley at stats.ox.ac.uk  Tue Jun 27 14:40:26 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 27 Jun 2006 13:40:26 +0100 (BST)
Subject: [R] questions on local customized R distribution CD
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA028840FC@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA028840FC@usctmx1106.merck.com>
Message-ID: <Pine.LNX.4.64.0606271325590.18372@gannet.stats.ox.ac.uk>

On Tue, 27 Jun 2006, Liaw, Andy wrote:

> From: Duncan Murdoch
>>
>> On 6/26/2006 3:14 PM, Dongseok Choi wrote:
>>> Hello all!
>>>
>>>   I hope this is the right place to post this question.
>>>
>>>   The Oregon Chapter of ASA is working with local high
>> school teachers as one of its outreaching program.
>>>   We hope to use and test R as teaching tools.
>>>   So, we think that a menu system (like R commander) with a
>> few packages and a bit simplified installation instruction
>> need to be developed.
>>>
>>>   The main question is:
>>> 1)
>>> Is it OK to develop a customized CD-ROM distribution of R
>> with pre-selected packages for high school?
>>> It will be distributed free, of course.
>>> Also, we plan to make it available from the chap web or
>> deposit it to R-project, if requested.
>>
>> Generally the answer is yes, but read the GPL for the
>> conditions.  You do need to make the source code available.
>
> I was under the impression that telling the user how to get the source code
> would satisfy the GPL, instead of distributing the source along with the
> binary.  Is that right?

No, the first part is definitely wrong.  (However, you don't have to 
distribute 'the source along with the binary', unless it is on the 
Internet.)

The obligation is on the distributor to make the exact sources available, 
not to rely on anyone else (e.g. CRAN, who might just lose them or not be 
available 2.99 years from now).  The relevant clauses are

     b) Accompany it with a written offer, valid for at least three
     years, to give any third party, for a charge no more than your
     cost of physically performing source distribution, a complete
     machine-readable copy of the corresponding source code, to be
     distributed under the terms of Sections 1 and 2 above on a medium
     customarily used for software interchange; or,

[The following clause c) does not apply if you repackage the 
distribution.]

If distribution of executable or object code is made by offering
access to copy from a designated place, then offering equivalent
access to copy the source code from the same place counts as
distribution of the source code, even though third parties are not
compelled to copy the source along with the object code.

See e.g.

http://www.gnu.org/licenses/gpl-faq.html#DistributeWithSourceOnInternet
http://www.gnu.org/licenses/gpl-faq.html#SourceAndBinaryOnDifferentSites

The easiest way to meet the obligations is to put the sources on the CD, 
especially as the sources concerned are only around 5% of the capacity of 
the CD.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From murdoch at stats.uwo.ca  Tue Jun 27 14:40:32 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 27 Jun 2006 08:40:32 -0400
Subject: [R] questions on local customized R distribution CD
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA028840FC@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA028840FC@usctmx1106.merck.com>
Message-ID: <44A12740.6040909@stats.uwo.ca>

On 6/27/2006 8:05 AM, Liaw, Andy wrote:
>  
> From: Duncan Murdoch
>> 
>> On 6/26/2006 3:14 PM, Dongseok Choi wrote:
>> > Hello all!
>> >   
>> >   I hope this is the right place to post this question.
>> >  
>> >   The Oregon Chapter of ASA is working with local high 
>> school teachers as one of its outreaching program.
>> >   We hope to use and test R as teaching tools.
>> >   So, we think that a menu system (like R commander) with a 
>> few packages and a bit simplified installation instruction 
>> need to be developed.
>> >  
>> >   The main question is:
>> > 1)
>> > Is it OK to develop a customized CD-ROM distribution of R  
>> with pre-selected packages for high school? 
>> > It will be distributed free, of course.
>> > Also, we plan to make it available from the chap web or 
>> deposit it to R-project, if requested.
>> 
>> Generally the answer is yes, but read the GPL for the 
>> conditions.  You do need to make the source code available.
> 
> I was under the impression that telling the user how to get the source code
> would satisfy the GPL, instead of distributing the source along with the
> binary.  Is that right?

Possibly, but not necessarily.  See section 3 of the GPL, distributed in 
the COPYING file with R.

Duncan Murdoch


From ggrothendieck at gmail.com  Tue Jun 27 14:40:56 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 27 Jun 2006 08:40:56 -0400
Subject: [R] Puzzled with contour()
In-Reply-To: <17568.56329.517392.800784@stat.math.ethz.ch>
References: <20060625133337.GB24241@lubyanka.local>
	<449FEB40.6000405@stats.uwo.ca>
	<971536df0606260739o26a54ad1u389f56986ce460ac@mail.gmail.com>
	<449FF4D2.1080009@stats.uwo.ca>
	<971536df0606260828y221cc4f2ta58668f221862342@mail.gmail.com>
	<17568.56329.517392.800784@stat.math.ethz.ch>
Message-ID: <971536df0606270540w48febbe6mf2dc5a7e505e16aa@mail.gmail.com>

On 6/27/06, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> >>>>> "Gabor" == Gabor Grothendieck <ggrothendieck at gmail.com>
> >>>>>     on Mon, 26 Jun 2006 11:28:48 -0400 writes:
>
>    Gabor> I think its often the case that one has 3 tuples and does not know
>    Gabor> how to use contour with that; so, it would be nice if the contour
>    Gabor> help page gave advice and an example and a pointer to the relevant
>    Gabor> functions if it cannot be done by contour.
>
> Yes, but even more importantly,
> what  help(contour) should really have is a \code{\link[lattice]{contourplot}}
> since contourplot() from package 'lattice'
> can deal well excellently with the situation Ajay has:
>
> After reading his data into matrix 'm3'
>
> > d3 <- data.frame(m3)
> > summary(d3)
>       x              y             z
>  Min.   :0.00   Min.   : 20   Min.   :0.0000
>  1st Qu.:0.25   1st Qu.: 60   1st Qu.:0.0000
>  Median :0.50   Median :110   Median :0.0040
>  Mean   :0.50   Mean   :110   Mean   :0.1583
>  3rd Qu.:0.75   3rd Qu.:160   3rd Qu.:0.1410
>  Max.   :1.00   Max.   :200   Max.   :1.0000
>
> contourplot(z ~ x+y, data=d3)
> ## or nicer
> contourplot(z ~ x+y, data=d3, cuts=20, region = TRUE)
> ## or rather use logit - transformed z values:
> contourplot(qlogis(z) ~ x+y, data=d3, cuts=20, region = TRUE)
>
>    Gabor> On 6/26/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
>    >> On 6/26/2006 10:39 AM, Gabor Grothendieck wrote:
>    >> > I think it would be helpful if this were added to the contour help file.
>    >>
>    >> You mean an example of building up the z matrix from points, or just a
>    >> general discussion of the issue?
>    >>
>    >> Duncan Murdoch
>    >> >
>    >> > On 6/26/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
>    >> >> On 6/25/2006 9:33 AM, Ajay Narottam Shah wrote:
>    >> >> > Folks,
>    >> >> >
>    >> >> > The contour() function wants x and y to be in increasing order. I have
>    >> >> > a situation where I have a grid in x and y, and associated z values,
>    >> >> > which looks like this:
>    >> >>
>    >> >> contour() wants vectors of x and y values, and a matrix of z values,
>    >> >> where the x values correspond to the rows of z, and the y values to the
>    >> >> columns.  You have a collection of points which need to be turned into
>    >> >> such a grid.
>    >> >>
>    >> >> There's an interp function in the akima package that can do this in
>    >> >> general.  In your case, it's probably sufficient to do something like this:
>    >> >>
>    >> >> zmat <- matrix(NA, 3, 19)
>    >> >> zmat[cbind(20*x + 1, y/10 - 1)] <- z
>    >> >> x <- (0:2)/20
>    >> >> y <- (2:20)*10
>    >> >> contour(x,y,zmat)
>    >> >>
>    >> >> Duncan Murdoch
>    >> >>
>    >> >>
>    >> >> >
>    >> >> >               x   y     z
>    >> >> >       [1,] 0.00  20 1.000
>    >> >> >       [2,] 0.00  30 1.000
>    >> >> >       [3,] 0.00  40 1.000
>    >> >> >       [4,] 0.00  50 1.000
>    >> >> >       [5,] 0.00  60 1.000
>    >> >> >       [6,] 0.00  70 1.000
>    >> >> >       [7,] 0.00  80 0.000
>    >> >> >       [8,] 0.00  90 0.000
>    >> >> >       [9,] 0.00 100 0.000
>    >> >> >      [10,] 0.00 110 0.000
>    >> >> >      [11,] 0.00 120 0.000
>    >> >> >      [12,] 0.00 130 0.000
>    >> >> >      [13,] 0.00 140 0.000
>    >> >> >      [14,] 0.00 150 0.000
>    >> >> >      [15,] 0.00 160 0.000
>    >> >> >      [16,] 0.00 170 0.000
>    >> >> >      [17,] 0.00 180 0.000
>    >> >> >      [18,] 0.00 190 0.000
>    >> >> >      [19,] 0.00 200 0.000
>    >> >> >      [20,] 0.05  20 1.000
>    >> >> >      [21,] 0.05  30 1.000
>    >> >> >      [22,] 0.05  40 1.000
>    >> >> >      [23,] 0.05  50 1.000
>    >> >> >      [24,] 0.05  60 0.998
>    >> >> >      [25,] 0.05  70 0.124
>    >> >> >      [26,] 0.05  80 0.000
>    >> >> >      [27,] 0.05  90 0.000
>    >> >> >      [28,] 0.05 100 0.000
>    >> >> >      [29,] 0.05 110 0.000
>    >> >> >      [30,] 0.05 120 0.000
>    >> >> >      [31,] 0.05 130 0.000
>    >> >> >      [32,] 0.05 140 0.000
>    >> >> >      [33,] 0.05 150 0.000
>    >> >> >      [34,] 0.05 160 0.000
>    >> >> >      [35,] 0.05 170 0.000
>    >> >> >      [36,] 0.05 180 0.000
>    >> >> >      [37,] 0.05 190 0.000
>    >> >> >      [38,] 0.05 200 0.000
>    >> >> >      [39,] 0.10  20 1.000
>    >> >> >      [40,] 0.10  30 1.000
>    >> >> >
>    >> >> > This looks like a nice case where both x and y are in increasing
>    >> >> > order. But contour() gets unhappy saying that he wants x and y in
>    >> >> > increasing order.
>    >> >> >
>    >> >> > Gnuplot generates pretty 3d pictures from such data, where you are
>    >> >> > standing above a surface and looking down at it. How does one do that
>    >> >> > in R?
>    >> >> >
>    >> >> > Any help will be most appreciated. A dput() of my data object is :
>    >> >> >
>    >> >> > structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>    >> >> > 0, 0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,
>    >> >> > 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1,
>    >> >> > 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,
>    >> >> > 0.1, 0.1, 0.1, 0.1, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,
>    >> >> > 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,
>    >> >> > 0.15, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
>    >> >> > 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25,
>    >> >> > 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,
>    >> >> > 0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,
>    >> >> > 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.35,
>    >> >> > 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35,
>    >> >> > 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.4, 0.4, 0.4, 0.4,
>    >> >> > 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,
>    >> >> > 0.4, 0.4, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,
>    >> >> > 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.5,
>    >> >> > 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,
>    >> >> > 0.5, 0.5, 0.5, 0.5, 0.5, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55,
>    >> >> > 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55,
>    >> >> > 0.55, 0.55, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,
>    >> >> > 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.65, 0.65, 0.65,
>    >> >> > 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65,
>    >> >> > 0.65, 0.65, 0.65, 0.65, 0.65, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7,
>    >> >> > 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.75,
>    >> >> > 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,
>    >> >> > 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.8, 0.8, 0.8, 0.8,
>    >> >> > 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,
>    >> >> > 0.8, 0.8, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85,
>    >> >> > 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.9,
>    >> >> > 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,
>    >> >> > 0.9, 0.9, 0.9, 0.9, 0.9, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,
>    >> >> > 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,
>    >> >> > 0.95, 0.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
>    >> >> > 1, 1, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140,
>    >> >> > 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90,
>    >> >> > 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30,
>    >> >> > 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170,
>    >> >> > 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120,
>    >> >> > 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70,
>    >> >> > 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200,
>    >> >> > 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150,
>    >> >> > 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100,
>    >> >> > 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40,
>    >> >> > 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180,
>    >> >> > 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130,
>    >> >> > 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80,
>    >> >> > 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20,
>    >> >> > 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160,
>    >> >> > 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110,
>    >> >> > 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50,
>    >> >> > 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180,
>    >> >> > 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130,
>    >> >> > 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80,
>    >> >> > 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20,
>    >> >> > 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160,
>    >> >> > 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110,
>    >> >> > 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50,
>    >> >> > 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180,
>    >> >> > 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130,
>    >> >> > 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80,
>    >> >> > 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20,
>    >> >> > 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160,
>    >> >> > 170, 180, 190, 200, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
>    >> >> > 0, 0, 0, 0, 0, 1, 1, 1, 1, 0.998, 0.124, 0, 0, 0, 0, 0, 0, 0,
>    >> >> > 0, 0, 0, 0, 0, 0, 1, 1, 1, 0.998, 0.71, 0.068, 0, 0, 0, 0, 0,
>    >> >> > 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0.998, 0.898, 0.396, 0.058, 0.002,
>    >> >> > 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.998, 0.97, 0.726, 0.268,
>    >> >> > 0.056, 0.006, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.996, 0.88,
>    >> >> > 0.546, 0.208, 0.054, 0.012, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>    >> >> > 0, 0, 0.998, 0.964, 0.776, 0.418, 0.18, 0.054, 0.014, 0.002,
>    >> >> > 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.998, 0.906, 0.664, 0.342,
>    >> >> > 0.166, 0.056, 0.018, 0.006, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>    >> >> > 0, 0.986, 0.862, 0.568, 0.29, 0.15, 0.056, 0.022, 0.008, 0.002,
>    >> >> > 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.954, 0.778, 0.494, 0.26, 0.148,
>    >> >> > 0.056, 0.024, 0.012, 0.004, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>    >> >> > 0.906, 0.712, 0.43, 0.242, 0.144, 0.058, 0.028, 0.012, 0.006,
>    >> >> > 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.878, 0.642, 0.38, 0.222,
>    >> >> > 0.142, 0.066, 0.034, 0.014, 0.008, 0.004, 0.002, 0, 0, 0, 0,
>    >> >> > 0, 0, 0, 0, 0.846, 0.586, 0.348, 0.208, 0.136, 0.068, 0.034,
>    >> >> > 0.016, 0.012, 0.006, 0.004, 0.002, 0, 0, 0, 0, 0, 0, 0, 0.8,
>    >> >> > 0.538, 0.318, 0.204, 0.136, 0.07, 0.046, 0.024, 0.012, 0.008,
>    >> >> > 0.004, 0.002, 0.002, 0, 0, 0, 0, 0, 0, 0.762, 0.496, 0.294, 0.2,
>    >> >> > 0.138, 0.072, 0.05, 0.024, 0.014, 0.012, 0.006, 0.004, 0.002,
>    >> >> > 0.002, 0, 0, 0, 0, 0, 0.704, 0.472, 0.286, 0.198, 0.138, 0.074,
>    >> >> > 0.054, 0.028, 0.016, 0.012, 0.008, 0.006, 0.004, 0.002, 0.002,
>    >> >> > 0, 0, 0, 0, 0.668, 0.438, 0.276, 0.196, 0.138, 0.078, 0.054,
>    >> >> > 0.032, 0.024, 0.014, 0.012, 0.008, 0.004, 0.004, 0.002, 0.002,
>    >> >> > 0, 0, 0, 0.634, 0.412, 0.27, 0.194, 0.14, 0.086, 0.056, 0.032,
>    >> >> > 0.024, 0.016, 0.012, 0.01, 0.006, 0.004, 0.004, 0.002, 0.002,
>    >> >> > 0, 0, 0.604, 0.388, 0.26, 0.19, 0.144, 0.088, 0.058, 0.048, 0.026,
>    >> >> > 0.022, 0.014, 0.012, 0.008, 0.006, 0.004, 0.004, 0.002, 0.002,
>    >> >> > 0, 0.586, 0.376, 0.256, 0.19, 0.146, 0.094, 0.062, 0.052, 0.028,
>    >> >> > 0.024, 0.014, 0.012, 0.012, 0.008, 0.004, 0.004, 0.004, 0.002,
>    >> >> > 0.002, 0.566, 0.364, 0.254, 0.192, 0.148, 0.098, 0.064, 0.054,
>    >> >> > 0.032, 0.024, 0.022, 0.014, 0.012, 0.012, 0.008, 0.004, 0.004,
>    >> >> > 0.004, 0.002), .Dim = c(399, 3), .Dimnames = list(NULL, c("x",
>    >> >> > "y", "z")))
>    >> >> >
>    >> >> > --
>    >> >> > Ajay Shah                                      http://www.mayin.org/ajayshah
>    >> >> > ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
>    >> >> > <*(:-? - wizard who doesn't know the answer.
>    >> >> >
>    >> >> > ______________________________________________
>    >> >> > R-help at stat.math.ethz.ch mailing list
>    >> >> > https://stat.ethz.ch/mailman/listinfo/r-help
>    >> >> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>    >> >>
>    >> >> ______________________________________________
>    >> >> R-help at stat.math.ethz.ch mailing list
>    >> >> https://stat.ethz.ch/mailman/listinfo/r-help
>    >> >> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>    >> >>
>    >> >
>    >> > ______________________________________________
>    >> > R-help at stat.math.ethz.ch mailing list
>    >> > https://stat.ethz.ch/mailman/listinfo/r-help
>    >> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>    >>
>    >>
>
>    Gabor> ______________________________________________
>    Gabor> R-help at stat.math.ethz.ch mailing list
>    Gabor> https://stat.ethz.ch/mailman/listinfo/r-help
>    Gabor> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Or perhaps contour should be extended so that it can handle triples
giving the capability directly in base graphics.  I think that that is what
many people really expect.


From scruveil at genoscope.cns.fr  Tue Jun 27 14:51:04 2006
From: scruveil at genoscope.cns.fr (Stephane Cruveiller)
Date: Tue, 27 Jun 2006 14:51:04 +0200
Subject: [R] RMySQL...Can't initialize driver???
Message-ID: <44A129B8.3060309@genoscope.cns.fr>

Dear R users,

I would like to query a MySQL database through R. I have installed
the latest required packages (RMySQL and DBI) in R (v2.3.1). A MySQL
server (v5.0.22) is running on my local machine but I can't initialize 
MYSQL
driver:
------------------------------------------------------------------------------------
 > library("RMySQL")
Loading required package: DBI
 > MySQL(max.con = 10, fetch.default.rec = 500, force.reload = F)
Error in new("MySQLDriver", Id = drvId) : unused argument(s) (Id ...)
-------------------------------------------------------------------------------------

Could somebody tell me what I have missed?


Thx

St?phane.


From murdoch at stats.uwo.ca  Tue Jun 27 14:58:59 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 27 Jun 2006 08:58:59 -0400
Subject: [R] Puzzled with contour()
In-Reply-To: <971536df0606270540w48febbe6mf2dc5a7e505e16aa@mail.gmail.com>
References: <20060625133337.GB24241@lubyanka.local>	
	<449FEB40.6000405@stats.uwo.ca>	
	<971536df0606260739o26a54ad1u389f56986ce460ac@mail.gmail.com>	
	<449FF4D2.1080009@stats.uwo.ca>	
	<971536df0606260828y221cc4f2ta58668f221862342@mail.gmail.com>	
	<17568.56329.517392.800784@stat.math.ethz.ch>
	<971536df0606270540w48febbe6mf2dc5a7e505e16aa@mail.gmail.com>
Message-ID: <44A12B93.7050103@stats.uwo.ca>

On 6/27/2006 8:40 AM, Gabor Grothendieck wrote:
> On 6/27/06, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
>> >>>>> "Gabor" == Gabor Grothendieck <ggrothendieck at gmail.com>
>> >>>>>     on Mon, 26 Jun 2006 11:28:48 -0400 writes:
>>
>>    Gabor> I think its often the case that one has 3 tuples and does not know
>>    Gabor> how to use contour with that; so, it would be nice if the contour
>>    Gabor> help page gave advice and an example and a pointer to the relevant
>>    Gabor> functions if it cannot be done by contour.
>>
>> Yes, but even more importantly,
>> what  help(contour) should really have is a \code{\link[lattice]{contourplot}}
>> since contourplot() from package 'lattice'
>> can deal well excellently with the situation Ajay has:
>>

[ deletions ]

> Or perhaps contour should be extended so that it can handle triples
> giving the capability directly in base graphics.  I think that that is what
> many people really expect.

The problem here is that most users are happy enough with the akima or 
contourplot implementations, so there isn't much motivation to spend 
several hours doing the necessary work.  But if you want to contribute 
this, please do.

Be careful about where you get the code: the interp implementation in 
akima is not licensed freely enough to be included in R ("no commercial 
use"). The one in lattice is GPL'd, so it should be fine.

Duncan Murdoch


From ripley at stats.ox.ac.uk  Tue Jun 27 15:01:56 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 27 Jun 2006 14:01:56 +0100 (BST)
Subject: [R] Memory available to 32-bit R app on 64-bit machine
In-Reply-To: <1db726800606270523q4235fd03o4ce7dc751e5f313c@mail.gmail.com>
References: <1db726800606270523q4235fd03o4ce7dc751e5f313c@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0606271340570.18372@gannet.stats.ox.ac.uk>

On Tue, 27 Jun 2006, roger bos wrote:

> I want to get a 64-bit machine/OS system so I can put 16Gb of RAM in it.  As
> first I assumed that I would have to use the 64-bit version of R to make use
> of the 16Gb of RAM, which would mean that I would use the Linux version of
> R.  But I have heard many posters say they run the 32-bit version of R on a
> 64-bit machine/OS.

Yes, and the address-space limits on 32-bit applications still apply: it 
is done for speed.

> So my questions, in Windows 64-bit, how much memory
> would be available to the 32-bit R binary?  Is it 4Gb (because its a 32-bit
> application) or 16Gb (because its being run on a 64-bit OS)?

2Gb, as it is a rather limited 32-bit subsystem of a 64-bit OS.  See the 
reference in the rw-FAQ Q2.9 or ?"Memory-limits".

No 32-bit OS gives 4Gb address space to an application: most manage about 
3Gb.  See ?"Memory-limits" (which is tailored to the OS you are running).

> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Oh, PLEASE do and turn off HTML mail as we ask.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From murdoch at stats.uwo.ca  Tue Jun 27 15:04:22 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 27 Jun 2006 09:04:22 -0400
Subject: [R] RMySQL...Can't initialize driver???
In-Reply-To: <44A129B8.3060309@genoscope.cns.fr>
References: <44A129B8.3060309@genoscope.cns.fr>
Message-ID: <44A12CD6.3090008@stats.uwo.ca>

On 6/27/2006 8:51 AM, Stephane Cruveiller wrote:
> Dear R users,
> 
> I would like to query a MySQL database through R. I have installed
> the latest required packages (RMySQL and DBI) in R (v2.3.1). A MySQL
> server (v5.0.22) is running on my local machine but I can't initialize 
> MYSQL
> driver:
> ------------------------------------------------------------------------------------
>  > library("RMySQL")
> Loading required package: DBI
>  > MySQL(max.con = 10, fetch.default.rec = 500, force.reload = F)
> Error in new("MySQLDriver", Id = drvId) : unused argument(s) (Id ...)
> -------------------------------------------------------------------------------------
> 
> Could somebody tell me what I have missed?

I have never used RMySQL, but from the error message, this looks like an 
incompatibility between it and DBI, presumably because one is newer than 
the other.

I have always found it easier to use RODBC.  Because it uses the more 
general ODBC interface, it may be slower than a package that is tuned to 
a particular database, but it works, which is a substantial advantage.

Duncan Murdoch


From scruveil at genoscope.cns.fr  Tue Jun 27 15:09:51 2006
From: scruveil at genoscope.cns.fr (Stephane Cruveiller)
Date: Tue, 27 Jun 2006 15:09:51 +0200
Subject: [R] RMySQL...Can't initialize driver???
In-Reply-To: <44A12CD6.3090008@stats.uwo.ca>
References: <44A129B8.3060309@genoscope.cns.fr> <44A12CD6.3090008@stats.uwo.ca>
Message-ID: <44A12E1F.8020901@genoscope.cns.fr>

Duncan Murdoch wrote:
> On 6/27/2006 8:51 AM, Stephane Cruveiller wrote:
>> Dear R users,
>>
>> I would like to query a MySQL database through R. I have installed
>> the latest required packages (RMySQL and DBI) in R (v2.3.1). A MySQL
>> server (v5.0.22) is running on my local machine but I can't 
>> initialize MYSQL
>> driver:
>> ------------------------------------------------------------------------------------ 
>>
>>  > library("RMySQL")
>> Loading required package: DBI
>>  > MySQL(max.con = 10, fetch.default.rec = 500, force.reload = F)
>> Error in new("MySQLDriver", Id = drvId) : unused argument(s) (Id ...)
>> ------------------------------------------------------------------------------------- 
>>
>>
>> Could somebody tell me what I have missed?
>
> I have never used RMySQL, but from the error message, this looks like 
> an incompatibility between it and DBI, presumably because one is newer 
> than the other.
>
> I have always found it easier to use RODBC.  Because it uses the more 
> general ODBC interface, it may be slower than a package that is tuned 
> to a particular database, but it works, which is a substantial advantage.
thanks for the hint... I  will give it a try very soon.
>
> Duncan Murdoch
>

Stephane.

From rkrug at sun.ac.za  Tue Jun 27 15:25:47 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Tue, 27 Jun 2006 15:25:47 +0200
Subject: [R] RMySQL...Can't initialize driver???
In-Reply-To: <44A12E1F.8020901@genoscope.cns.fr>
References: <44A129B8.3060309@genoscope.cns.fr> <44A12CD6.3090008@stats.uwo.ca>
	<44A12E1F.8020901@genoscope.cns.fr>
Message-ID: <44A131DB.3070600@sun.ac.za>

Stephane Cruveiller wrote:
> Duncan Murdoch wrote:
>> On 6/27/2006 8:51 AM, Stephane Cruveiller wrote:
>>> Dear R users,
>>>
>>> I would like to query a MySQL database through R. I have installed
>>> the latest required packages (RMySQL and DBI) in R (v2.3.1). A MySQL
>>> server (v5.0.22) is running on my local machine but I can't
>>> initialize MYSQL
>>> driver:
>>> ------------------------------------------------------------------------------------
>>>
>>>  > library("RMySQL")
>>> Loading required package: DBI
>>>  > MySQL(max.con = 10, fetch.default.rec = 500, force.reload = F)
>>> Error in new("MySQLDriver", Id = drvId) : unused argument(s) (Id ...)
>>> -------------------------------------------------------------------------------------
>>>
>>>
>>> Could somebody tell me what I have missed?

I am running R 2.3.0 under Linux and connect to a MySQl 4.1 server.

You have to create a file in your home directory which has the
connection infos. It should look like that:

				
[renpatch]
user = UserName
password = PassWord
database = ...
host = ...

[renpatch_renosterbos]
user = UserName
password = PassWord
database = ...
host = ..

and be called .my.conf

Then you do:


library("RMySQL")
m <- dbDriver("MySQL")
con <- dbConnect(m, group = "renpatch")
q <- TheSQLQuery
rs <- dbSendQuery(con, q)
TheResults <- fetch(rs, n = -1)
dbDisconnect(con)
rm(con)


and it should work - at least that is what it is doing for me.

Rainer

-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
      	Rainer at krugs.de


From maechler at stat.math.ethz.ch  Tue Jun 27 15:11:31 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 27 Jun 2006 15:11:31 +0200
Subject: [R] Puzzled with contour()
In-Reply-To: <44A12B93.7050103@stats.uwo.ca>
References: <20060625133337.GB24241@lubyanka.local>
	<449FEB40.6000405@stats.uwo.ca>
	<971536df0606260739o26a54ad1u389f56986ce460ac@mail.gmail.com>
	<449FF4D2.1080009@stats.uwo.ca>
	<971536df0606260828y221cc4f2ta58668f221862342@mail.gmail.com>
	<17568.56329.517392.800784@stat.math.ethz.ch>
	<971536df0606270540w48febbe6mf2dc5a7e505e16aa@mail.gmail.com>
	<44A12B93.7050103@stats.uwo.ca>
Message-ID: <17569.11907.214023.114456@stat.math.ethz.ch>

>>>>> "Duncan" == Duncan Murdoch <murdoch at stats.uwo.ca>
>>>>>     on Tue, 27 Jun 2006 08:58:59 -0400 writes:

    Duncan> On 6/27/2006 8:40 AM, Gabor Grothendieck wrote:
    >> On 6/27/06, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
    >>> >>>>> "Gabor" == Gabor Grothendieck <ggrothendieck at gmail.com>
    >>> >>>>>     on Mon, 26 Jun 2006 11:28:48 -0400 writes:
    >>> 
    Gabor> I think its often the case that one has 3 tuples and does not know
    Gabor> how to use contour with that; so, it would be nice if the contour
    Gabor> help page gave advice and an example and a pointer to the relevant
    Gabor> functions if it cannot be done by contour.
    >>> 
    >>> Yes, but even more importantly,
    >>> what  help(contour) should really have is a \code{\link[lattice]{contourplot}}
    >>> since contourplot() from package 'lattice'
    >>> can deal well excellently with the situation Ajay has:
    >>> 

    Duncan> [ deletions ]

    >> Or perhaps contour should be extended so that it can handle triples
    >> giving the capability directly in base graphics.  I think that that is what
    >> many people really expect.

    Duncan> The problem here is that most users are happy enough
    Duncan> with the akima or contourplot implementations, so
    Duncan> there isn't much motivation to spend several hours
    Duncan> doing the necessary work.  But if you want to
    Duncan> contribute this, please do.

But also consider that --- since the beginning of time or so ---
the triad of
      contour(), image(), and persp()
have had an (almost?) identical way of dealing with  (x,y,z)
{and related arguments}.

So we would want a change using a helper function that
can be used in all three cases {and BTW, that helper function
would use  xyz.coords() - similarly to what happens in
scatterplot3d() {in the pkg of the same name}.

    Duncan> Be careful about where you get the code: the interp
    Duncan> implementation in akima is not licensed freely
    Duncan> enough to be included in R ("no commercial
    Duncan> use"). The one in lattice is GPL'd, so it should be
    Duncan> fine.

and I agree that making that functionality part of ``base R''
(would be package 'grDevices') is something quite desirable.
Reiterating Duncan: Contributions are welcome, particularly if
they are patches against R-devel (and contain updated help files).

    Duncan> Duncan Murdoch

Martin Maechler


From dj at research.bell-labs.com  Tue Jun 27 16:11:43 2006
From: dj at research.bell-labs.com (David James)
Date: Tue, 27 Jun 2006 10:11:43 -0400
Subject: [R] RMySQL...Can't initialize driver???
In-Reply-To: <200606270919.29654.scruveil@genoscope.cns.fr>
References: <200606270919.29654.scruveil@genoscope.cns.fr>
Message-ID: <20060627141142.GB26976@jessie.research.bell-labs.com>

Stephane CRUVEILLER wrote:
> Dear R users,
> 
> I would like to query a MySQL database through R. I have installed
> the latest required packages (RMySQL and DBI) in R (v2.3.1). A MySQL
> server (v5.0.22) is running on my local machine but I can't initialize MYSQL 
> driver: 
> ------------------------------------------------------------------------------------
> > library("RMySQL")
> Loading required package: DBI
> > MySQL(max.con = 10, fetch.default.rec = 500, force.reload = F)
> Error in new("MySQLDriver", Id = drvId) : unused argument(s) (Id ...)
> -------------------------------------------------------------------------------------
> 
> Could somebody tell me what I have missed?

Hmm, I can't reproduce the problem:

   > m2 <- MySQL(max.con = 16, fetch.default.rec = 500, force.reload = F)
   > summary(m2, verbose=TRUE)
   <MySQLDriver:(31672)>
     Driver name:  MySQL
     Max  connections: 16
     Conn. processed: 0
     Default records per fetch: 500
     DBI API version:  0.1-10
     MySQL client version:  4.1.7
     Open connections: 0

   > version
                  _
   platform       i686-pc-linux-gnu
   arch           i686
   os             linux-gnu
   system         i686, linux-gnu
   status
   major          2
   minor          3.1
   year           2006
   month          06
   day            01
   svn rev        38247
   language       R
   version.string Version 2.3.1 (2006-06-01)

--
David

> 
> 
> Thx
> 
> St?phane.
> 
> 
> -- 
> ==========================================================
> Stephane CRUVEILLER Ph. D.
> Genoscope - Centre National de Sequencage
> Atelier de Genomique Comparative
> 2, Rue Gaston Cremieux   CP 5706
> 91057 Evry Cedex - France
> Phone: +33 (0)1 60 87 84 58
> Fax: +33 (0)1 60 87 25 14
> EMails: scruveil at genoscope.cns.fr ,scruvell at infobiogen.fr
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From rdporto1 at terra.com.br  Tue Jun 27 16:28:54 2006
From: rdporto1 at terra.com.br (rdporto1)
Date: Tue, 27 Jun 2006 11:28:54 -0300
Subject: [R] compositional time series
Message-ID: <J1IW86$2C607AFD9290E8A9F3378BA8ABE48858@terra.com.br>

Hendry,

you have to be very very carefull 'cause of
the compositional nature of your data and
because of your short time series.

1.You have to be carefull about compositional
data (see Aitchinson's book: Compositional
Data Analysis).

2. You have a very short time series and it's
difficult to model that (see Chattfield's
book: Statistical Problem Solving - I think!)

Considering 1 and 2, you can analyze your
data using R or any other statistical software
since you have a model. Do you have one?
Tell the group wich one it is and maybe we
can help you more.

Rogerio Porto.


---------- Cabe?alho original -----------

De: r-help-bounces at stat.math.ethz.ch
Para: R-help at stat.math.ethz.ch
C?pia: 
Data: Tue, 27 Jun 2006 18:21:32 +0800 (CST)
Assunto: [R] compositional time series

> Dear R users,
>    
>   i am wondering if anyone has some hints for this problem (i have not found a clear answer after searching the R-mailing list archive, 'help.search' in R, and R-Wiki, and the like...): 
>    
>   let's assume that i have 4 periods compositional time series data:
>    
>   t=1, A=0.1; B=0.5; C=0.4
>   t=2, A=0.2; B=0.4; C=0.4
>   t=3, A=0.5; B=0.3; C=0.2
>   t=4, A=0.4; B=0.3; C=0.3
>   t=5, ???
>    
>   By using R, could anyone suggest how to obtain the forecasted value of the fifth period (t=5) as well as the forecast error? (it seems that 'mixeR' [M.Bren] and 'compositions' [v.d.Boogart] have not provided a 'direct' solution to this problem...)
>    
>   really appreciate your time/help for this,
>    
>   Thank you very much, 
>    
>   Sincerely,
>   hendry raharjo
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From scruveil at genoscope.cns.fr  Tue Jun 27 16:41:22 2006
From: scruveil at genoscope.cns.fr (Stephane Cruveiller)
Date: Tue, 27 Jun 2006 16:41:22 +0200
Subject: [R] RMySQL...Can't initialize driver???
In-Reply-To: <44A131DB.3070600@sun.ac.za>
References: <44A129B8.3060309@genoscope.cns.fr> <44A12CD6.3090008@stats.uwo.ca>
	<44A12E1F.8020901@genoscope.cns.fr> <44A131DB.3070600@sun.ac.za>
Message-ID: <44A14392.6020807@genoscope.cns.fr>

Rainer M Krug wrote:
> Stephane Cruveiller wrote:
>   
>> Duncan Murdoch wrote:
>>     
>>> On 6/27/2006 8:51 AM, Stephane Cruveiller wrote:
>>>       
>>>> Dear R users,
>>>>
>>>> I would like to query a MySQL database through R. I have installed
>>>> the latest required packages (RMySQL and DBI) in R (v2.3.1). A MySQL
>>>> server (v5.0.22) is running on my local machine but I can't
>>>> initialize MYSQL
>>>> driver:
>>>> ------------------------------------------------------------------------------------
>>>>
>>>>  > library("RMySQL")
>>>> Loading required package: DBI
>>>>  > MySQL(max.con = 10, fetch.default.rec = 500, force.reload = F)
>>>> Error in new("MySQLDriver", Id = drvId) : unused argument(s) (Id ...)
>>>> -------------------------------------------------------------------------------------
>>>>
>>>>
>>>> Could somebody tell me what I have missed?
>>>>         
>
> I am running R 2.3.0 under Linux and connect to a MySQl 4.1 server.
>
> You have to create a file in your home directory which has the
> connection infos. It should look like that:
>
> 				
> [renpatch]
> user = UserName
> password = PassWord
> database = ...
> host = ...
>
> [renpatch_renosterbos]
> user = UserName
> password = PassWord
> database = ...
> host = ..
>
> and be called .my.conf
>
>   
I followed your instruction. Here is my .my.cnf:
---------------------------------------------------------------------------
[client]
user=steff
password=XXXXXX
database=justforfun
---------------------------------------------------------------------------
with it, I can connect to my MYSQL server without problem
and then I try to connect through R:

---------------------------------------------------------------------------
 > library(RMySQL)
Loading required package: DBI
 > m <- dbDriver("MySQL")
Error in new("MySQLDriver", Id = drvId) : unused argument(s) (Id ...)
---------------------------------------------------------------------------

It still produces the error. I really  do not  know what is going on here...


> Then you do:
>
>
> library("RMySQL")
> m <- dbDriver("MySQL")
> con <- dbConnect(m, group = "renpatch")
> q <- TheSQLQuery
> rs <- dbSendQuery(con, q)
> TheResults <- fetch(rs, n = -1)
> dbDisconnect(con)
> rm(con)
>
>
> and it should work - at least that is what it is doing for me.
>
> Rainer
>
>   

Stephane.

From pngom at ucad.sn  Tue Jun 27 11:33:10 2006
From: pngom at ucad.sn (pngom at ucad.sn)
Date: Tue, 27 Jun 2006 09:33:10 +0000
Subject: [R] Monte carlo simulations
Message-ID: <20060627093310.lq3z4sxr42yog00g@mail.ucad.sn>

Dear professor,
I want to have some examples of programm (programming under R), not the 
command only, but the code source, for the monte carlo simulations and 
MCMC method.
with king regards.

-- 
Dr. P. NGOM,
Facult? des Sciences et Techniques
D?partement de Math?matiques et Informatique
Universit? Cheikh Anta Diop Dakar - S?n?gal

----------------------------------------------------------------
Universite Cheikh Anta DIOP - DAKAR


From cuauv at yahoo.com  Tue Jun 27 17:06:36 2006
From: cuauv at yahoo.com (Cuau)
Date: Tue, 27 Jun 2006 08:06:36 -0700 (PDT)
Subject: [R] reading a matrix from a file
Message-ID: <20060627150636.74997.qmail@web52314.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060627/cab70768/attachment.pl 

From snpandit at hotmail.com  Tue Jun 27 17:26:31 2006
From: snpandit at hotmail.com (Richard pandit)
Date: Tue, 27 Jun 2006 15:26:31 +0000
Subject: [R] significant test of Cross correlation coeficent
Message-ID: <BAY120-F368B13ED0051691D782523D47E0@phx.gbl>


From gunter.berton at gene.com  Tue Jun 27 17:29:14 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 27 Jun 2006 08:29:14 -0700
Subject: [R] Robustness of linear mixed models
In-Reply-To: <Pine.LNX.4.64.0606270757250.4301@gannet.stats.ox.ac.uk>
Message-ID: <000f01c699fe$7319ee60$f8c2fea9@gne.windows.gene.com>

Below...

> > Hello,
> >
> > with 4 different linear mixed models (continuous dependent) 
> I find that my
> > residuals do not follow the normality assumption 
> (significant Shapiro-Wilk
> > with values equal/higher than 0.976; sample sizes 750 or 
> 1200). I find,
> > instead, that my residuals are really well fitted by a t 
> distribution with
> > dofs' ranging, in the different datasets, from 5 to 12.
> >
> > Should this be considered such a severe violation of the normality
> > assumption as to make model-based inferences invalid?
> 
> For some aspects, yes.  Given that R provides you with the 
> means to fit 
> robust linear models, why not use them and find out if they make a 
> difference to the aspects you are interested in?
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 

Or do your inferences in a way that does not depend on normality, perhaps
via (careful to honor the multilevel sampling assumptions) bootstrapping?

Cautions apply. 

First, linear mixed models is actually a nonlinear modeling technique, as is
robust linear fitting. So the process may be sensitive to initial values  I
believe this was pointed out to me by Professior Ripley, though in a
different context. I would appreciate any more informed comments and
qualifications about this.

Second, both the normal theory inference and bootstrapping are asymptotic
and therefore approximate.  I believe this was the point Prof. Ripley was
making when he said "For **some** aspects..." Comparing results under
various assumptions is always a good idea to check sensitivity to those sets
of assumptions, though it may emphasize the fact that choice of the "right"
analysis may be a complex and application and data specific issue. 

Cheers,

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA


From p.dalgaard at biostat.ku.dk  Tue Jun 27 17:58:52 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Jun 2006 17:58:52 +0200
Subject: [R] Mauchly and Levene
In-Reply-To: <2015835067@web.de>
References: <2015835067@web.de>
Message-ID: <x2irmm3803.fsf@turmalin.kubism.ku.dk>

Tanja Seifried <cinnamon313 at web.de> writes:

> Hallo
> 
> thanks a lot for the answer, I found the Levene test in the car package.
> 
> I already tried to compute my Mauchly.test the same way as in the example(SSD)
> matrix <- lm(data)

That is not how lm is used in example(mlmfit)... Is this really what
you did, and why are you calling the result "matrix"?

> mauchly.test(matrix,X=~1)
> 
> I was just unsure about it because with that I get a different result than when I do the Mauchly test with SPSS... hm

Very different, or just a little? I know that the R version carries a
few more terms of the asymptotic expansion that the one in SAS.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From gerifalte28 at hotmail.com  Tue Jun 27 18:32:36 2006
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Tue, 27 Jun 2006 16:32:36 +0000
Subject: [R] reading a matrix from a file
In-Reply-To: <20060627150636.74997.qmail@web52314.mail.yahoo.com>
Message-ID: <BAY103-F245588B5D2E2113A697DABA67E0@phx.gbl>

Dear Cuau

More details on your posting would have helped. From your example it seems 
that the matrix you are trying to import is just a comma Separated text 
file. If that's the case, you can use read.csv and matrix to import those 
numbers and then make a matrix in R.  Assuming the matrix has 3 rows and 4 
columns you can do the following:

x=read.csv("mymatrix.txt", header=FALSE) #Reads your file
dim(x)  #Tells you the dimensions of object x in rows and columns
[1] 3 4

newMat= as.matrix(x) #Creates matrix from imported data
is.matrix(newMat) #Tests whether the object newMat is of class matrix
[1] TRUE


I hope this helps

Francisco

Dr. Francisco J. Zagmutt
College of Veterinary Medicine and Biomedical Sciences
Colorado State University




>From: Cuau <cuauv at yahoo.com>
>To: r-help at stat.math.ethz.ch
>Subject: [R] reading a matrix from a file
>Date: Tue, 27 Jun 2006 08:06:36 -0700 (PDT)
>
>
>    Hello everyone,
>
>    I'm writting a little script that will read a matrix from a file
>
>   i.e.
>
>   0,.11,.22,.4
>   .11,0,.5,.3
>   .22,.5,0,.7
>   anb so on
>
>   and will then calculate some standard stats for nets (i.e. 
>centralization, degree, etc).
>
>     So far I have opened the file and read the contents, however I' m 
>using readLines(filename)
>    to read the file and it returns it as one big String with no divitions. 
>I tried using
>   strsplit(String)
>   to split it but eventhough is working I'm not able to put the output of 
>the above into a matrix.
>
>    Below is an example of what I have done
>
>
>   > INfile<-file("mTest.txt", "r")
>   > readLines(INfile)->matrix
>   > matrix
>   [1] "1, 2, 3"
>   > strsplit(matrix, ",")->splitLine
>   > splitLine
>   [[1]]
>   [1] "1"  " 2" " 3"
>
>   > netMatrix <-matrix(c(splitLine), nrow=1,ncol=3)
>   > netMatrix
>        [,1]         [,2]         [,3]
>   [1,] Character,3 Character,3 Character,3
>
>
>    Does anyone have an idea how can I read a matrix and store it in the 
>form of a matrix.
>
>   thks
>
>   -Cuau Vital
>
>
>
>---------------------------------
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html


From zhuanshi.he at gmail.com  Tue Jun 27 18:58:25 2006
From: zhuanshi.he at gmail.com (Zhuanshi He)
Date: Wed, 28 Jun 2006 01:58:25 +0900
Subject: [R] reading a matrix from a file
In-Reply-To: <20060627150636.74997.qmail@web52314.mail.yahoo.com>
References: <20060627150636.74997.qmail@web52314.mail.yahoo.com>
Message-ID: <2a8feb1c0606270958t36201799obae24d21ccdabc2f@mail.gmail.com>

Maybe this link is useful

http://www.bic.mni.mcgill.ca/users/jason/cortex/stats-manuals/mni.read.glim.file.html


Also, section 2.3 through http://cran.r-project.org/doc/manuals/R-data.html


2.3 Using scan directly

Both read.table and read.fwf use scan to read the file, and then
process the results of scan. They are very convenient, but sometimes
it is better to use scan directly.

Function scan has many arguments, most of which we have already
covered under read.table. The most crucial argument is what, which
specifies a list of modes of variables to be read from the file. If
the list is named, the names are used for the components of the
returned list. Modes can be numeric, character or complex, and are
usually specified by an example, e.g. 0, "" or 0i. For example
    cat("2 3 5 7", "11 13 17 19", file="ex.dat", sep="\n")
    scan(file="ex.dat", what=list(x=0, y="", z=0), flush=TRUE)

returns a list with three components and discards the fourth column in
the file.

There is a function readLines which will be more convenient if all you
want is to read whole lines into R for further processing.

One common use of scan is to read in a large matrix. Suppose file
matrix.dat just contains the numbers for a 200 x 2000 matrix. Then we
can use
    A <- matrix(scan("matrix.dat", n = 200*2000), 200, 2000, byrow = TRUE)

On one test this took 1 second (under Linux, 3 seconds under Windows
on the same machine) whereas
    A <- as.matrix(read.table("matrix.dat"))

took 10 seconds (and more memory), and
    A <- as.matrix(read.table("matrix.dat", header = FALSE, nrows = 200,
                              comment.char = "", colClasses = "numeric"))

took 7 seconds. The difference is almost entirely due to the overhead
of reading 2000 separate short columns: were they of length 2000, scan
took 9 seconds whereas read.table took 18 if used efficiently (in
particular, specifying colClasses) and 125 if used naively.

Note that timings can depend on the type read and the data. Consider
reading a million distinct integers:
    writeLines(as.character((1+1e6):2e6), "ints.dat")
    xi <- scan("ints.dat", what=integer(0), n=1e6)   # 0.77s
    xn <- scan("ints.dat", what=numeric(0), n=1e6)   # 0.93s
    xc <- scan("ints.dat", what=character(0), n=1e6) # 0.85s
    xf <- as.factor(xc)                              # 2.2s
    DF <- read.table("ints.dat")                     # 4.5s

and a million examples of a small set of codes:
    code <- c("LMH", "SJC", "CHCH", "SPC", "SOM")
    writeLines(sample(code, 1e6, replace=TRUE), "code.dat")
    y <- scan("code.dat", what=character(0), n=1e6)  # 0.44s
    yf <- as.factor(y)                               # 0.21s
    DF <- read.table("code.dat")                     # 4.9s
    DF <- read.table("code.dat", nrows=1e6)          # 3.6s

Note that these timings depend heavily on the operating system (the
basic reads in Windows take at least as twice as long as these Linux
times) and on the precise state of the garbage collector.


Hope this works.

Z. He
On 6/28/06, Cuau <cuauv at yahoo.com> wrote:
>
>
>    Hello everyone,
>
>    I'm writting a little script that will read a matrix from a file
>
>   i.e.
>
>   0,.11,.22,.4
>   .11,0,.5,.3
>   .22,.5,0,.7
>   anb so on
>
>   and will then calculate some standard stats for nets (i.e. centralization, degree, etc).
>
>     So far I have opened the file and read the contents, however I' m using readLines(filename)
>    to read the file and it returns it as one big String with no divitions. I tried using
>   strsplit(String)
>   to split it but eventhough is working I'm not able to put the output of the above into a matrix.
>
>    Below is an example of what I have done
>
>
>   > INfile<-file("mTest.txt", "r")
>   > readLines(INfile)->matrix
>   > matrix
>   [1] "1, 2, 3"
>   > strsplit(matrix, ",")->splitLine
>   > splitLine
>   [[1]]
>   [1] "1"  " 2" " 3"
>
>   > netMatrix <-matrix(c(splitLine), nrow=1,ncol=3)
>   > netMatrix
>        [,1]         [,2]         [,3]
>   [1,] Character,3 Character,3 Character,3
>
>
>    Does anyone have an idea how can I read a matrix and store it in the form of a matrix.
>
>   thks
>
>   -Cuau Vital
>
>
>
> ---------------------------------
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>


-- 
Zhuanshi He / Z. He (PhD)
ADvanced Environmental Monitoring Research Center (ADEMRC)
Gwangju Institute of Science and Technology
1 Oryong-dong, Buk-gu, Gwangju 500-712, Republic of Korea.
Tel. +82-62-970-3406   Fax. +82-62-970-3404
Email: Zhuanshi.He at gmail.com
Web: http://atm1.gist.ac.kr/~hzs/


From zhuanshi.he at gmail.com  Tue Jun 27 19:03:30 2006
From: zhuanshi.he at gmail.com (Zhuanshi He)
Date: Wed, 28 Jun 2006 02:03:30 +0900
Subject: [R] reading a matrix from a file
In-Reply-To: <20060627150636.74997.qmail@web52314.mail.yahoo.com>
References: <20060627150636.74997.qmail@web52314.mail.yahoo.com>
Message-ID: <2a8feb1c0606271003q41c62114g602467cada912f8e@mail.gmail.com>

Maybe this link is useful

http://www.bic.mni.mcgill.ca/users/jason/cortex/stats-manuals/mni.read.glim.file.html


Also, section 2.3 through http://cran.r-project.org/doc/manuals/R-data.html


2.3 Using scan directly

Both read.table and read.fwf use scan to read the file, and then
process the results of scan. They are very convenient, but sometimes
it is better to use scan directly.

Function scan has many arguments, most of which we have already
covered under read.table. The most crucial argument is what, which
specifies a list of modes of variables to be read from the file. If
the list is named, the names are used for the components of the
returned list. Modes can be numeric, character or complex, and are
usually specified by an example, e.g. 0, "" or 0i. For example
    cat("2 3 5 7", "11 13 17 19", file="ex.dat", sep="\n")
    scan(file="ex.dat", what=list(x=0, y="", z=0), flush=TRUE)

returns a list with three components and discards the fourth column in
the file.

There is a function readLines which will be more convenient if all you
want is to read whole lines into R for further processing.

One common use of scan is to read in a large matrix. Suppose file
matrix.dat just contains the numbers for a 200 x 2000 matrix. Then we
can use
    A <- matrix(scan("matrix.dat", n = 200*2000), 200, 2000, byrow = TRUE)

On one test this took 1 second (under Linux, 3 seconds under Windows
on the same machine) whereas
    A <- as.matrix(read.table("matrix.dat"))

took 10 seconds (and more memory), and
    A <- as.matrix(read.table("matrix.dat", header = FALSE, nrows = 200,
                              comment.char = "", colClasses = "numeric"))

took 7 seconds. The difference is almost entirely due to the overhead
of reading 2000 separate short columns: were they of length 2000, scan
took 9 seconds whereas read.table took 18 if used efficiently (in
particular, specifying colClasses) and 125 if used naively.

Note that timings can depend on the type read and the data. Consider
reading a million distinct integers:
    writeLines(as.character((1+1e6):2e6), "ints.dat")
    xi <- scan("ints.dat", what=integer(0), n=1e6)   # 0.77s
    xn <- scan("ints.dat", what=numeric(0), n=1e6)   # 0.93s
    xc <- scan("ints.dat", what=character(0), n=1e6) # 0.85s
    xf <- as.factor(xc)                              # 2.2s
    DF <- read.table("ints.dat")                     # 4.5s

and a million examples of a small set of codes:
    code <- c("LMH", "SJC", "CHCH", "SPC", "SOM")
    writeLines(sample(code, 1e6, replace=TRUE), "code.dat")
    y <- scan("code.dat", what=character(0), n=1e6)  # 0.44s
    yf <- as.factor(y)                               # 0.21s
    DF <- read.table("code.dat")                     # 4.9s
    DF <- read.table("code.dat", nrows=1e6)          # 3.6s

Note that these timings depend heavily on the operating system (the
basic reads in Windows take at least as twice as long as these Linux
times) and on the precise state of the garbage collector.


Hope this works.

Z. He


????????????????????????????????????
On 6/28/06, Cuau <cuauv at yahoo.com> wrote:
>
>
>    Hello everyone,
>
>    I'm writting a little script that will read a matrix from a file
>
>   i.e.
>
>   0,.11,.22,.4
>   .11,0,.5,.3
>   .22,.5,0,.7
>   anb so on
>
>   and will then calculate some standard stats for nets (i.e. centralization, degree, etc).
>
>     So far I have opened the file and read the contents, however I' m using readLines(filename)
>    to read the file and it returns it as one big String with no divitions. I tried using
>   strsplit(String)
>   to split it but eventhough is working I'm not able to put the output of the above into a matrix.
>
>    Below is an example of what I have done
>
>
>   > INfile<-file("mTest.txt", "r")
>   > readLines(INfile)->matrix
>   > matrix
>   [1] "1, 2, 3"
>   > strsplit(matrix, ",")->splitLine
>   > splitLine
>   [[1]]
>   [1] "1"  " 2" " 3"
>
>   > netMatrix <-matrix(c(splitLine), nrow=1,ncol=3)
>   > netMatrix
>        [,1]         [,2]         [,3]
>   [1,] Character,3 Character,3 Character,3
>
>
>    Does anyone have an idea how can I read a matrix and store it in the form of a matrix.
>
>   thks
>
>   -Cuau Vital
>
>
>
> ---------------------------------
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>


-- 
Zhuanshi He / Z. He (PhD)
ADvanced Environmental Monitoring Research Center (ADEMRC)
Gwangju Institute of Science and Technology
1 Oryong-dong, Buk-gu, Gwangju 500-712, Republic of Korea.
Tel. +82-62-970-3406   Fax. +82-62-970-3404
Email: Zhuanshi.He at gmail.com
          Zhuanshi_He at msn.com
          Zhuanshi_He at yahoo.com.cn
Web: http://atm1.gist.ac.kr/~hzs/  BBS: http://atm1.gist.ac.kr/~hzs/phpBB2/


From goran.brostrom at gmail.com  Tue Jun 27 19:17:22 2006
From: goran.brostrom at gmail.com (=?UTF-8?Q?G=C3=B6ran_Brostr=C3=B6m?=)
Date: Tue, 27 Jun 2006 19:17:22 +0200
Subject: [R] Survival
In-Reply-To: <000e01c699e0$e373a9d0$0200a8c0@Michela>
References: <000e01c699e0$e373a9d0$0200a8c0@Michela>
Message-ID: <148ed8180606271017n31582b85q23ba70cb9e230369@mail.gmail.com>

On 6/27/06, Michela Ballardini <m.ballardini at ior-forli.it> wrote:
> Dear all,
> I write to know if it possible to know the number of censor subject in function of time. I run:
> os<-survfit(Surv(datios$time,datios$status))
>
> summary(os)
>
> but it give me only the nomber of events.
> Can you help me?

Well, if you know the number of events, you can find the number of
censored observations by subtracting the number of events from the
number of observations.

If you want complete statistics from all risksets (at event times),
consider the function 'risksets' in the package 'eha'.

G?ran

> Thanke you
> Michela
>
>
> **************************************
> Dr.ssa Michela Ballardini
> Unit? di Biostatistica e Sperimentazioni Cliniche
> c/o Osp. Morgagni-Pierantoni - Pad. Valsalva
> Via Forlanini, 34
> 47100 Forl?
> Tel 0543-731836
> Tel/Fax 0543-731612
> **************************************
>
>
>         [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>


-- 
G?ran Brostr?m


From klebyn at yahoo.com.br  Tue Jun 27 19:25:20 2006
From: klebyn at yahoo.com.br (Cleber N.Borges)
Date: Tue, 27 Jun 2006 14:25:20 -0300 (ART)
Subject: [R] how to rotate a triangle image(ZMAT) ?
Message-ID: <20060627172520.92465.qmail@web30611.mail.mud.yahoo.com>

 Hello R users...

 how to align this Zmat (triangle image)  in X axis?
 I would like that the triangle's base become in the X
axis and
 the triangle's height become in the Y axis 

 Is there some trick for make this?

 Thanks.
 Cleber
 

######################## my test and try
 f <- function(x,y){
 z=1-x-y
 z[ z < (-1e-15) ] <- NA 
 return( -100*x+0*y+100*z )
 }

 x = seq( 1, 0, by = -0.01 )
 y = seq( 1, 0, by = -0.01 )
 zmat =  outer(x,y,f)

 image(zmat,  col=terrain.colors(10))
 contour(zmat, add=T)


From chrish at stats.ucl.ac.uk  Tue Jun 27 19:42:02 2006
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Tue, 27 Jun 2006 18:42:02 +0100 (BST)
Subject: [R] Random numbers negatively correlated?
Message-ID: <Pine.LNX.4.64.0606271827030.2589@egon.stats.ucl.ac.uk>

Dear list,

I did simulations in which I generated 10000
independent Bernoulli(0.5)-sequences of length 100. I estimated
p for each sequence and I also estimated the conditional probability that 
a one is followed by another one (which should be p as well).
However, the second probability is significantly smaller than 0.5 (namely
about 0.494, see below) and of course smaller than the direct estimates of 
p as well, indicating negative correlation between the random numbers.

See below the code and the results.
Did I do something wrong or are the numbers in 
fact negatively correlated? (A type I error is quite unlikely with a
p-value below 2.2e-16.)

Best,
Christian

set.seed(123456)
n <- 100
p <- 0.5
simruns <- 10000
est <- est11 <- numeric(0)
for (i in 1:simruns){
#    if (i/100==round(i/100)) print(i)
     x <- rbinom(n,1,p)
     est[i] <- mean(x)
     x11 <- 3*x[2:n]-x[1:(n-1)]
     est11[i] <- sum(x11==2)/sum(x11==2 | x11==(-1))
     # x11==(-1): 0 follows 1, x11==2: 1 follows 1.
}

> print(mean(est))
[1] 0.499554
> print(sd(est)/sqrt(simruns))
[1] 0.0004958232
# OK

> print(mean(est11))
[1] 0.4935211
> print(sd(est11)/sqrt(simruns))
[1] 0.0007136213
# mean(est11)+2*sd(mean) < 0.495

> print(sum(est>est11))
[1] 5575
> binom.test(5575,10000)

 	Exact binomial test

data:  5575 and 10000
number of successes = 5575, number of trials = 10000, p-value <
2.2e-16


*** --- ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche


From tlumley at u.washington.edu  Tue Jun 27 20:07:52 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 27 Jun 2006 11:07:52 -0700 (PDT)
Subject: [R] Random numbers negatively correlated?
In-Reply-To: <Pine.LNX.4.64.0606271827030.2589@egon.stats.ucl.ac.uk>
References: <Pine.LNX.4.64.0606271827030.2589@egon.stats.ucl.ac.uk>
Message-ID: <Pine.LNX.4.64.0606271055530.6827@homer23.u.washington.edu>

On Tue, 27 Jun 2006, Christian Hennig wrote:

> Dear list,
>
> I did simulations in which I generated 10000
> independent Bernoulli(0.5)-sequences of length 100. I estimated
> p for each sequence and I also estimated the conditional probability that
> a one is followed by another one (which should be p as well).
> However, the second probability is significantly smaller than 0.5 (namely
> about 0.494, see below) and of course smaller than the direct estimates of
> p as well, indicating negative correlation between the random numbers.
>
> See below the code and the results.
> Did I do something wrong or are the numbers in
> fact negatively correlated? (A type I error is quite unlikely with a
> p-value below 2.2e-16.)

I think you did something wrong, and that there is a problem with 
overlapping blocks of two.

If you do
   x<-matrix(rbinom(1e6,1,p),ncol=2)
   tt<-table(x[,1],x[,2])

you get much better looking results. In this case you have 500,000 
independent pairs of numbers that can be 01, 10, 11, 00. A test for 
independence seems fine.

> tt

          0      1
   0 125246 124814
   1 125140 124800
> fisher.test(tt)

         Fisher's Exact Test for Count Data

data:  tt
p-value = 0.8987
alternative hypothesis: true odds ratio is not equal to 1
95 percent confidence interval:
  0.9896745 1.0119211
sample estimates:
odds ratio
   1.000735


In your case the deficit in est11 is suspiciously close to 0.5/n. Changing 
n to 1000 and using the same seed I get
> mean(est11)-0.5
[1] -0.0005534743
10 times smaller, and still close to 0.5/n.

Now, consider what happens in a case where we can see all the 
possibilities, n=3

x    1/0  1/1 
000   -   -
001   -   -
010   1   0
011   0   1
100   1   0
101   1   0
110   1   1
111   2   0

So that if each of these three triplets has the same probability your 
est11 would be 2/8 rather than 4/8, and est11 is not an unbiased estimate 
of the long-run conditional probability. The bias is of order 1/n, so you 
need n to be of larger order than  sqrt(simruns).


 	-thomas


From tlumley at u.washington.edu  Tue Jun 27 20:20:46 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 27 Jun 2006 11:20:46 -0700 (PDT)
Subject: [R] Random numbers negatively correlated?
In-Reply-To: <Pine.LNX.4.64.0606271055530.6827@homer23.u.washington.edu>
References: <Pine.LNX.4.64.0606271827030.2589@egon.stats.ucl.ac.uk>
	<Pine.LNX.4.64.0606271055530.6827@homer23.u.washington.edu>
Message-ID: <Pine.LNX.4.64.0606271114130.6827@homer23.u.washington.edu>

On Tue, 27 Jun 2006, Thomas Lumley wrote:

I got the table wrong, it should read

  x    1/0  1/1  est11
  000   -   -     -
  001   -   -     -
  010   1   0     0
  011   0   1     1
  100   1   0     0
  101   1   0     0
  110   1   1     0.5
  111   0   2     1

So the explanation is slightly more complicated. The problem is that 
although sum(x11==2)/n and sum(x11==2 | x11==(-1))/n are unbiased 
estimators their ratio is not an unbiased estimator, but has mean 5/12 
(which is 0.5-0.5/n).

 	-thomas


From rashmim at sfu.ca  Tue Jun 27 20:22:20 2006
From: rashmim at sfu.ca (Rashmi Mathur)
Date: Tue, 27 Jun 2006 11:22:20 -0700
Subject: [R] rotate text using mtext
Message-ID: <200606271822.k5RIMK8A012956@rm-rstar.sfu.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060627/d44260f2/attachment.pl 

From arrayprofile at yahoo.com  Tue Jun 27 20:22:14 2006
From: arrayprofile at yahoo.com (array chip)
Date: Tue, 27 Jun 2006 11:22:14 -0700 (PDT)
Subject: [R] compare odds ratios
In-Reply-To: <20060626212524.26432.qmail@web35713.mail.mud.yahoo.com>
Message-ID: <20060627182214.88643.qmail@web35709.mail.mud.yahoo.com>

Hi dear all, I haven't heard any suggestions on how to
tackle the problem in my previous email yet. I
searched on google and was not getting any useful
information yet. I did get someone from google groups
suggesting Cockran Mantel Haenszel test with each
subject as the stratum. But as far as I understand,
CMH test is to test whether the common odds ratio
(assuming odds ratios across stratum are equal) is
equal to 1, not really the question I was asking which
was whether the 2 odds ratios were equal (doesn't
matter if they are equal to 1). Also, someone
suggested loglinear regression, but I am not sure
either how to set things up for my problem.

One clarification to my original email: the 2
diagnostic tests were performed on the set set of
patients, the issue here how to test whether the 2
odds ratios for the 2 diagnostic tests are equal.

Here is a hypothetical dataset, for example:

dat<-cbind(disease=sample(c(rep(1,15),rep(0,20))),test1=sample(c(rep(1,11),rep(0,24))),test2=sample(c(rep(1,14),rep(0,21))))

Hope some statistical experts would guide me some
directions. Many thanks


--- array chip <arrayprofile at yahoo.com> wrote:

> Hi there, is there any way to compare 2 odds ratios?
> I
> have two tests that are supposed to detect a disease
> presence. So for each test, I can compute an odds
> ratio. My problem is how can I compare the 2 tests
> by
> testing whether the 2 odds ratios are the same?
> 
> Appreciate
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


From etiennesky at yahoo.com  Tue Jun 27 20:28:59 2006
From: etiennesky at yahoo.com (etienne)
Date: Tue, 27 Jun 2006 11:28:59 -0700 (PDT)
Subject: [R] distribution of daily rainfall values in binned categories
Message-ID: <20060627182859.78214.qmail@web36902.mail.mud.yahoo.com>

Hi,

I'm a newbie in using R and I would like to have a few
clues as to how I could compute and plot a
distribution of daily rainfall intensity in different
categories.  I have daily values (mm/day) for several
years and I need to show the frequency of 0-1, 1-2.5,
2.5-5, 5-10, 10-20, 20+ mm/day.  Can this be done
easily?

Thanks,
Etienne


From srini_iyyer_bio at yahoo.com  Tue Jun 27 20:53:16 2006
From: srini_iyyer_bio at yahoo.com (Srinivas Iyyer)
Date: Tue, 27 Jun 2006 11:53:16 -0700 (PDT)
Subject: [R] Reading a set of values from a numeric matrix
Message-ID: <20060627185316.6866.qmail@web38108.mail.mud.yahoo.com>

Dear group, 
I have a matrix with (190,20000) dimensions. 

The values range from : -973.8149  to 807.4688 

I want to select the values less than -200 and want to
know what are the names of row and columns. 

   m1 m2 m3 m4 ...... mn
k1
k2
k3
k4
.
.
.
kj



>mymat[1:4,1:4]

       1-Dec       1-Mar      1-Sep     10-Sep
m1  -0.4220701 -0.94359666 -421.06729 -1.8525286
m2   0.4408303 -250.706960  0.6422206  0.6344322
m3   0.4005077 -430.076784  1.3653584  0.4677905
m4   5.0358915  6.40544709 -430.67964  3.2017410


I want to select the pairs:

m2 1-Mar   -250.706960
m3 1-Mar   -430.076784
m1 1-Sep   -421.06729
m4 1-Sep   -430.67964


How can I do it.  Could any one help me out. Please. 

Thanks
sri


From Scott.Waichler at pnl.gov  Tue Jun 27 20:55:08 2006
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Tue, 27 Jun 2006 11:55:08 -0700
Subject: [R] Possible to get a definition of a function from a package to
 use without invoking the package?
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A04A5ADA6@pnlmse35.pnl.gov>


Hi,

I often use the mod() and instring() functions that are available in the
clim.pact package.  This package has a lot of dependencies, including
installation of netCDF, and I haven't yet been able to get
library(clim.pact) to work on a Mac OS 10.4.6.  A previous request for
help with the Mac problem yielded no results, so now I wonder if I could
just extract the definitions for the couple of functions that I need and
save them in my own file of R functions.  I'm pretty sure that mod() and
instring() are very basic and don't have any exotic dependencies.  I did
find an alternative mod() in the new matlab package, and that's fine.
Now I just need another way to get the instring() functionality.

Thanks,
Scott Waichler
Pacific Northwest National Laboratory
scott.waichler _at_ pnl.gov


From John.Kerpel at infores.com  Tue Jun 27 21:02:05 2006
From: John.Kerpel at infores.com (Kerpel, John)
Date: Tue, 27 Jun 2006 14:02:05 -0500
Subject: [R] Outliers and robust methods
Message-ID: <44A8B25381923D4F93B74B2676A50F6D02B38E40@MAIL1.infores.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060627/5c039843/attachment.pl 

From ggrothendieck at gmail.com  Tue Jun 27 21:03:41 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 27 Jun 2006 15:03:41 -0400
Subject: [R] Reading a set of values from a numeric matrix
In-Reply-To: <20060627185316.6866.qmail@web38108.mail.mud.yahoo.com>
References: <20060627185316.6866.qmail@web38108.mail.mud.yahoo.com>
Message-ID: <971536df0606271203i6a5f5ee9wa24d97d2c2132d58@mail.gmail.com>

See:

http://thread.gmane.org/gmane.comp.lang.r.general/64473/focus=64487

On 6/27/06, Srinivas Iyyer <srini_iyyer_bio at yahoo.com> wrote:
> Dear group,
> I have a matrix with (190,20000) dimensions.
>
> The values range from : -973.8149  to 807.4688
>
> I want to select the values less than -200 and want to
> know what are the names of row and columns.
>
>   m1 m2 m3 m4 ...... mn
> k1
> k2
> k3
> k4
> .
> .
> .
> kj
>
>
>
> >mymat[1:4,1:4]
>
>       1-Dec       1-Mar      1-Sep     10-Sep
> m1  -0.4220701 -0.94359666 -421.06729 -1.8525286
> m2   0.4408303 -250.706960  0.6422206  0.6344322
> m3   0.4005077 -430.076784  1.3653584  0.4677905
> m4   5.0358915  6.40544709 -430.67964  3.2017410
>
>
> I want to select the pairs:
>
> m2 1-Mar   -250.706960
> m3 1-Mar   -430.076784
> m1 1-Sep   -421.06729
> m4 1-Sep   -430.67964
>
>
> How can I do it.  Could any one help me out. Please.
>
> Thanks
> sri
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From rdpeng at gmail.com  Tue Jun 27 21:12:30 2006
From: rdpeng at gmail.com (Roger D. Peng)
Date: Tue, 27 Jun 2006 15:12:30 -0400
Subject: [R] Possible to get a definition of a function from a package
 to use without invoking the package?
In-Reply-To: <7E4C06F49D6FEB49BE4B60E5FC92ED7A04A5ADA6@pnlmse35.pnl.gov>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A04A5ADA6@pnlmse35.pnl.gov>
Message-ID: <44A1831E.7000505@gmail.com>

You should be able to download the source package from CRAN and copy the code 
for the 'instring()' function into your own source file.

-roger

Waichler, Scott R wrote:
> Hi,
> 
> I often use the mod() and instring() functions that are available in the
> clim.pact package.  This package has a lot of dependencies, including
> installation of netCDF, and I haven't yet been able to get
> library(clim.pact) to work on a Mac OS 10.4.6.  A previous request for
> help with the Mac problem yielded no results, so now I wonder if I could
> just extract the definitions for the couple of functions that I need and
> save them in my own file of R functions.  I'm pretty sure that mod() and
> instring() are very basic and don't have any exotic dependencies.  I did
> find an alternative mod() in the new matlab package, and that's fine.
> Now I just need another way to get the instring() functionality.
> 
> Thanks,
> Scott Waichler
> Pacific Northwest National Laboratory
> scott.waichler _at_ pnl.gov
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/


From ggrothendieck at gmail.com  Tue Jun 27 21:12:18 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 27 Jun 2006 15:12:18 -0400
Subject: [R] Possible to get a definition of a function from a package
	to use without invoking the package?
In-Reply-To: <7E4C06F49D6FEB49BE4B60E5FC92ED7A04A5ADA6@pnlmse35.pnl.gov>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A04A5ADA6@pnlmse35.pnl.gov>
Message-ID: <971536df0606271212p78ac12e0kd7ec18894fee2eda@mail.gmail.com>

You can use

   %%

instead of mod, e.g.

   12 %% 5

see ?"%%"

You can use

   unlist(gregexpr(pat, string, fixed = TRUE))

instead of instring(pat, string).  If you just want the first
occurrence use regexpr instead of gregexpr.

On 6/27/06, Waichler, Scott R <Scott.Waichler at pnl.gov> wrote:
>
> Hi,
>
> I often use the mod() and instring() functions that are available in the
> clim.pact package.  This package has a lot of dependencies, including
> installation of netCDF, and I haven't yet been able to get
> library(clim.pact) to work on a Mac OS 10.4.6.  A previous request for
> help with the Mac problem yielded no results, so now I wonder if I could
> just extract the definitions for the couple of functions that I need and
> save them in my own file of R functions.  I'm pretty sure that mod() and
> instring() are very basic and don't have any exotic dependencies.  I did
> find an alternative mod() in the new matlab package, and that's fine.
> Now I just need another way to get the instring() functionality.
>
> Thanks,
> Scott Waichler
> Pacific Northwest National Laboratory
> scott.waichler _at_ pnl.gov
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From murdoch at stats.uwo.ca  Tue Jun 27 21:22:16 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 27 Jun 2006 15:22:16 -0400
Subject: [R] Possible to get a definition of a function from a package
 to use without invoking the package?
In-Reply-To: <7E4C06F49D6FEB49BE4B60E5FC92ED7A04A5ADA6@pnlmse35.pnl.gov>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A04A5ADA6@pnlmse35.pnl.gov>
Message-ID: <44A18568.3050402@stats.uwo.ca>

On 6/27/2006 2:55 PM, Waichler, Scott R wrote:
> Hi,
> 
> I often use the mod() and instring() functions that are available in the
> clim.pact package.  This package has a lot of dependencies, including
> installation of netCDF, and I haven't yet been able to get
> library(clim.pact) to work on a Mac OS 10.4.6.  A previous request for
> help with the Mac problem yielded no results, so now I wonder if I could
> just extract the definitions for the couple of functions that I need and
> save them in my own file of R functions.  

I'm not sure what you mean by this question.

Technically you can do that.

Legally you can do it because clim.pact is GPL'd.

It's a slightly rude thing to do if you end up redistributing the 
functions, so I'd check with the author first.

It may cause you weird problems in the future if you decide to use both 
your functions and clim.pact at the same time, especially if one of them 
has mutated in the meantime.

I'd suggest the best approach is to work with the clim.pact author to 
get that package working on your platform.

By the way, mod() is available as a basic R operator, namely %%.  It 
provides different results in some cases, e.g.


 > -5 %% 2
[1] 1
 > mod(-5, 2)
[1] -1

so be careful about your definitions.  I don't know a simple substitute 
for instring().

Duncan Murdoch


I'm pretty sure that mod() and
> instring() are very basic and don't have any exotic dependencies.  I did
> find an alternative mod() in the new matlab package, and that's fine.
> Now I just need another way to get the instring() functionality.
> 
> Thanks,
> Scott Waichler
> Pacific Northwest National Laboratory
> scott.waichler _at_ pnl.gov
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ccleland at optonline.net  Tue Jun 27 21:22:28 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 27 Jun 2006 15:22:28 -0400
Subject: [R] rotate text using mtext
In-Reply-To: <200606271822.k5RIMK8A012956@rm-rstar.sfu.ca>
References: <200606271822.k5RIMK8A012956@rm-rstar.sfu.ca>
Message-ID: <44A18574.9020704@optonline.net>

Rashmi Mathur wrote:
> Hello,
> 
> I wish to write a label to the right-hand side of a plot (side=4) using
> mtext, with the text facing inwards - that is, rotated 180 degrees from the
> default orientation.  How might I do this?  (I've tried experimenting with
> las but no luck.)

   Use text() instead.  I'm not sure how you want the label oriented, 
but this should get you started:

df <- data.frame(cbind(1:10, 11:20))
names(df) <- c("y","x")

par(mar = c(7, 7, 7, 10))
plot(y ~ x, data = df, ylab = "", xlab = "independent")
text(par("usr")[2] + 0.25, 5.5, srt=-45, adj = 0, labels = "dependent", 
xpd = TRUE)

> Thanks,
> Rashmi
> 
> 
> Rashmi Mathur
> Master's Candidate
> Fisheries Science and Management Research Group
> School of Resource and Environmental Management (REM)
> Simon Fraser University
> Burnaby, BC	
> rashmim at sfu.ca
> http://www.rem.sfu.ca/fishgrp
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From jporzak at gmail.com  Tue Jun 27 22:01:50 2006
From: jporzak at gmail.com (Jim Porzak)
Date: Tue, 27 Jun 2006 13:01:50 -0700
Subject: [R] distribution of daily rainfall values in binned categories
In-Reply-To: <20060627182859.78214.qmail@web36902.mail.mud.yahoo.com>
References: <20060627182859.78214.qmail@web36902.mail.mud.yahoo.com>
Message-ID: <2a9c000c0606271301r380bd0b2o9cfdeecd26c4da2d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060627/4286afe6/attachment.pl 

From afshart at exchange.sba.miami.edu  Tue Jun 27 22:19:12 2006
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Tue, 27 Jun 2006 16:19:12 -0400
Subject: [R] supplying dynamic main argument to plot?
Message-ID: <6BCB4D493A447546A8126F24332056E803B01562@school1.business.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060627/1c90f302/attachment.pl 

From kwright68 at gmail.com  Tue Jun 27 22:31:02 2006
From: kwright68 at gmail.com (Kevin Wright)
Date: Tue, 27 Jun 2006 15:31:02 -0500
Subject: [R] Mixing grid and base graphics--need help understanding this
	quirk
Message-ID: <adf71a630606271331h17c763b8qe21415c2a74fc25a@mail.gmail.com>

My setup: Windows 2000, R 2.3.1

When I start a brand new session of R and paste the code below into R,
the graphic device shows "Some text" in the lower left corner.  If I
paste the code into the command window again, then "Some text" does
not appear in the lower left corner.  Why is this?

require(grid)
par(mfrow=c(1,2))
plot(1:10)
plot(-10:1)
par(mfrow=c(1,1))
pushViewport(viewport(.04, .04, width=stringWidth("Some text"),
height=unit(2,"lines"),
                      name="pagenum", gp=gpar(fontsize=10)))
grid.text("Some text", gp=gpar(col="gray30"),
          just=c("left","bottom"))
popViewport()


Kevin Wright


From rmh at temple.edu  Tue Jun 27 22:32:16 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Tue, 27 Jun 2006 16:32:16 -0400 (EDT)
Subject: [R] supplying dynamic main argument to plot?
Message-ID: <20060627163216.BDI45280@po-d.temple.edu>

It is easy to control the main title with plot, but you will
get much better looking plots if you use xyplot.

library(lattice)
tmp <- data.frame(x=rnorm(100),
                  y=rnorm(100,4,2),
                  ID=paste("Patient", rep(1:5, rep(20,5))))
xyplot(y ~ x | ID, data=tmp)


Rich


From aarti_dahiya at hotmail.com  Tue Jun 27 22:36:00 2006
From: aarti_dahiya at hotmail.com (Aarti Dahiya)
Date: Tue, 27 Jun 2006 16:36:00 -0400
Subject: [R] Passing argument to a function called within another function
Message-ID: <BAY106-F13B6AB6367433B28000204E37E0@phx.gbl>


#Test at command line
getQuery(name='John')

This should give the result- select * from table_name where name='John'
Instead, it gives- select * from table_name where  = ' name = 'John'

The reason is because in function call queryGenerator$generateQuery(passed) 
from within getQuery(), passed is a character vector so the function 
generateQuery() treats the whole thing as one argument value i.e. as 
args[[1]].

What can I do solve this problem?  Thanks.  I appreciate any help.


File QueryGenerator.R
-------------------------------

#Constructor for QueryGenerator
setConstructorS3("QueryGenerator", function()
{
	extend(Object(), "QueryGenerator")
})

# Called when print(object) or object$print is called
setMethodS3("as.character", "QueryGenerator", function(this)
{
      paste("This is a QueryGenerator object.")
})

# getData function generates the SQL query
setMethodS3("generateQuery", "QueryGenerator", function(this,...)
{
	args <- list(...)

	sql <- "select * from table_name where"

    	for(i in 1:length(args))
    	{
    		if(is.numeric(args[[i]]))
    	 	{
			sql = paste(sql, names(args[i]), "=", args[[i]], collapse = " ")
    	 	}

    	 	else if(is.character(args[[i]]))
    	 	{
	  		sql = paste(sql, names(args[i]), "=", sQuote(args[[i]]), collapse = " 
")
            }

		# If in the current iteration args is not the last argument
		# add "and" keyword
		if(!identical(i, length(args)))
		{
			if(identical(names(args[i]),names(args[i + 1])))
				sql = paste(sql, "or", collapse = " ")
			else
				sql = paste(sql, "and", collapse = " ")
		}

            cat("\nIntermediate Queries\n")
            cat(sql)
     }

     cat("\n\nFinal query\n")
     cat(sql)

     cat("\n")
     return(sql)
})

File getQuery.R
----------------------

getQuery <- function(...)
{

    args <- list(...)

    #Create the argument to be passed to generateQuery
    passed <- sub(",", "", paste(rbind(",", names(args), "=", sQuote(args)), 
collapse = " "))

    #Create the QueryGenerator Object
    queryGenerator <- QueryGenerator()

    #Call getData() of DataRetriever class.
    results <- queryGenerator$generateQuery(passed)

}


From dj at research.bell-labs.com  Tue Jun 27 22:36:24 2006
From: dj at research.bell-labs.com (David James)
Date: Tue, 27 Jun 2006 16:36:24 -0400
Subject: [R] RMySQL...Can't initialize driver???
In-Reply-To: <44A14392.6020807@genoscope.cns.fr>
References: <44A129B8.3060309@genoscope.cns.fr> <44A12CD6.3090008@stats.uwo.ca>
	<44A12E1F.8020901@genoscope.cns.fr> <44A131DB.3070600@sun.ac.za>
	<44A14392.6020807@genoscope.cns.fr>
Message-ID: <20060627203624.GE29613@jessie.research.bell-labs.com>

This looks very suspicious.  The function mysqlInitDriver
is pretty straight forward (see below), but the error you're reporting
apparently has something to do with S4 dispatching:

  > mysqlInitDriver
  function (max.con = 16, fetch.default.rec = 500, force.reload = FALSE)
  {
      if (fetch.default.rec <= 0)
          stop("default num of records per fetch must be positive")
      config.params <- as.integer(c(max.con, fetch.default.rec))
      force <- as.logical(force.reload)
      drvId <- .Call("RS_MySQL_init", config.params, force, PACKAGE = .MySQLPkgName)
      new("MySQLDriver", Id = drvId)
  }

Could you send me the output of 

   R --vanilla
   library(RMySQL)
   search()
   ## ask R to put you in a browser in case of errors
   options(error = recover)   

   con <- MySQL(max.con = 10, fetch.default.rec=500, force.reload = F)

when you encounter the error, you'll be prompted with

     "Enter a frame number, or 0 to exit"
  
type 2 (or the number for the call to mysqlInitDriver(max.con = ...))
and then find out what class of object is "drvId" (it should be "integer").
    
    Browser[1]>  class(drvId)
    

Regards,

--
David

   
  

Stephane Cruveiller wrote:
> Rainer M Krug wrote:
> >Stephane Cruveiller wrote:
> >  
> >>Duncan Murdoch wrote:
> >>    
> >>>On 6/27/2006 8:51 AM, Stephane Cruveiller wrote:
> >>>      
> >>>>Dear R users,
> >>>>
> >>>>I would like to query a MySQL database through R. I have installed
> >>>>the latest required packages (RMySQL and DBI) in R (v2.3.1). A MySQL
> >>>>server (v5.0.22) is running on my local machine but I can't
> >>>>initialize MYSQL
> >>>>driver:
> >>>>------------------------------------------------------------------------------------
> >>>>
> >>>> > library("RMySQL")
> >>>>Loading required package: DBI
> >>>> > MySQL(max.con = 10, fetch.default.rec = 500, force.reload = F)
> >>>>Error in new("MySQLDriver", Id = drvId) : unused argument(s) (Id ...)
> >>>>-------------------------------------------------------------------------------------
> >>>>
> >>>>
> >>>>Could somebody tell me what I have missed?
> >>>>        
> >
> >I am running R 2.3.0 under Linux and connect to a MySQl 4.1 server.
> >
> >You have to create a file in your home directory which has the
> >connection infos. It should look like that:
> >
> >				
> >[renpatch]
> >user = UserName
> >password = PassWord
> >database = ...
> >host = ...
> >
> >[renpatch_renosterbos]
> >user = UserName
> >password = PassWord
> >database = ...
> >host = ..
> >
> >and be called .my.conf
> >
> >  
> I followed your instruction. Here is my .my.cnf:
> ---------------------------------------------------------------------------
> [client]
> user=steff
> password=XXXXXX
> database=justforfun
> ---------------------------------------------------------------------------
> with it, I can connect to my MYSQL server without problem
> and then I try to connect through R:
> 
> ---------------------------------------------------------------------------
> > library(RMySQL)
> Loading required package: DBI
> > m <- dbDriver("MySQL")
> Error in new("MySQLDriver", Id = drvId) : unused argument(s) (Id ...)
> ---------------------------------------------------------------------------
> 
> It still produces the error. I really  do not  know what is going on here...
> 
> 
> >Then you do:
> >
> >
> >library("RMySQL")
> >m <- dbDriver("MySQL")
> >con <- dbConnect(m, group = "renpatch")
> >q <- TheSQLQuery
> >rs <- dbSendQuery(con, q)
> >TheResults <- fetch(rs, n = -1)
> >dbDisconnect(con)
> >rm(con)
> >
> >
> >and it should work - at least that is what it is doing for me.
> >
> >Rainer
> >
> >  
> 
> Stephane.
> 

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From gerifalte28 at hotmail.com  Tue Jun 27 23:01:05 2006
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Tue, 27 Jun 2006 21:01:05 +0000
Subject: [R] supplying dynamic main argument to plot?
In-Reply-To: <6BCB4D493A447546A8126F24332056E803B01562@school1.business.edu>
Message-ID: <BAY103-F248DE4539D2C0FB87D21C1A67E0@phx.gbl>

Hi Dave,

Try using paste within main i.e.

x=rnorm(100)
y=runif(100)
k=sample(1:10,10, replace=T)

for(i in k){
  plot(x,y, xlab="gamma", ylab="r1", main=paste("Patient", i, sep=" "))
  Sys.sleep(1)
  }


In this eaxmple I am just plotting the same x,y values on each figure but 
you can easily plot the observations for each patient by subetting your data 
by the K variable. See ?"[" for more details on subsetting.

I hope this helps

Francisco


Dr. Francisco J. Zagmutt
College of Veterinary Medicine and Biomedical Sciences
Colorado State University




>From: "Afshartous, David" <afshart at exchange.sba.miami.edu>
>To: "Afshartous, David" <afshart at exchange.sba.miami.edu>,        
><r-help at stat.math.ethz.ch>
>Subject: [R] supplying dynamic main argument to plot?
>Date: Tue, 27 Jun 2006 16:19:12 -0400
>
>All,
>
>Simple question but I don't seem to be able to find the answer in the
>documentation:
>When using "plot" within a loop, is there any way to supply the argument
>to "main" dynamically,
>i.e., so that the title is Patient k below as the loop cycles through
>each value of k?
>
>plot(x,y, xlim=c(0,250), ylim=c(0,1000), xlab="gamma", ylab="r1",
>main="Patient k")
>
>
>thanks
>dave
>ps - I'm running windows.
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html


From david.meyer at wu-wien.ac.at  Tue Jun 27 23:04:04 2006
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Tue, 27 Jun 2006 23:04:04 +0200
Subject: [R]  comparing 2 odds ratios
Message-ID: <20060627230404.d4922eb8.david.meyer@wu-wien.ac.at>

Hi,

you can have a look at fourfold() and oddratio() in package vcd.

Best,
David

Hi there, is there any way to compare 2 odds ratios? I
have two tests that are supposed to detect a disease
presence. So for each test, I can compute an odds
ratio. My problem is how can I compare the 2 tests by
testing whether the 2 odds ratios are the same?

-- 
Dr. David Meyer
Department of Information Systems and Operations

Vienna University of Economics and Business Administration
Augasse 2-6, A-1090 Wien, Austria, Europe
Tel: +43-1-313 36 4393
Fax: +43-1-313 36 90 4393 
HP:  http://wi.wu-wien.ac.at/~meyer/


From john.maindonald at anu.edu.au  Tue Jun 27 23:54:50 2006
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Wed, 28 Jun 2006 07:54:50 +1000
Subject: [R]   Robustness of linear mixed models
In-Reply-To: <mailman.4.1151402402.29002.r-help@stat.math.ethz.ch>
References: <mailman.4.1151402402.29002.r-help@stat.math.ethz.ch>
Message-ID: <B96D602F-CFFA-490F-A335-17D1E66578CA@anu.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060628/3bbb3c5c/attachment.pl 

From pinard at iro.umontreal.ca  Tue Jun 27 21:45:33 2006
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Tue, 27 Jun 2006 15:45:33 -0400
Subject: [R] how to rotate a triangle image(ZMAT) ?
In-Reply-To: <20060627172520.92465.qmail@web30611.mail.mud.yahoo.com>
References: <20060627172520.92465.qmail@web30611.mail.mud.yahoo.com>
Message-ID: <20060627194533.GA19165@alcyon.progiciels-bpi.ca>

[Cleber N.Borges]

> how to align this Zmat (triangle image)  in X axis?  I would like that 
> the triangle's base become in the X axis and the triangle's height 
> become in the Y axis.  Is there some trick for make this?

I'm not fully sure of what is the base and the height of the triangle, 
but if I guess correctly, you may peek at "?image", the last paragraph 
of the "Details:" section, and also in the "Examples:" section, where it 
says "Need to transpose and flip matrix horizontally.".  Maybe you'll 
find some explanations or ideas in there.

> f <- function(x, y) {
>   z = 1-x-y
>   z[z < (-1e-15)] <- NA
    return(-100*x+0*y+100*z)
> }

> x = seq(1, 0, by = -0.01)
> y = seq(1, 0, by = -0.01)
> zmat = outer(x, y, f)

> image(zmat, col=terrain.colors(10))
> contour(zmat, add=T)

Another idea is to exchange "x, y" in the "outer" call, and maybe also 
use "rev()" on one of them.

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca


From klebyn at yahoo.com.br  Wed Jun 28 01:22:35 2006
From: klebyn at yahoo.com.br (Cleber N.Borges)
Date: Tue, 27 Jun 2006 20:22:35 -0300 (ART)
Subject: [R] how to rotate a triangle image(ZMAT) ?
Message-ID: <20060627232235.32887.qmail@web30604.mail.mud.yahoo.com>



I thank Fran?ois Pinard for your attention

one solution, but
very dirty
very ugly
and ~ 30% of calculations cut loose!

if somebody can give a tip, I thank...

Cleber

############
 f <- function(x,y){
 z=1-x-y
 z[ z < (-1e-15) ] <- NA 
 return( -100*x + 0*y + 100*z )
 }

 x = y = seq( 1, 0, by = -0.01 )
 z =  outer(x,y,f)
 
 t1 = length(x)
 aux = numeric(0)
 im  = numeric(0)
 for( i in seq( 1, t1, by = 2 ) ){
    idx = seq( i*t1, t1**2, by = t1 ) - (0:(t1 - i))
    im = c(im, aux, z[idx], aux )
    aux = c(aux, NA)
 }
 im = matrix(im,nr=t1)

 image(im,  col=terrain.colors(256))
 contour(im, add=T)


From llei at bccrc.ca  Wed Jun 28 01:34:44 2006
From: llei at bccrc.ca (Linda Lei)
Date: Tue, 27 Jun 2006 16:34:44 -0700
Subject: [R] the plot of parametric model curve
Message-ID: <90B06673D826C64E8ED8EEA6B6FDF8CAE72C11@crcmail1.BCCRC.CA>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060627/0348098d/attachment.pl 

From markleeds at verizon.net  Wed Jun 28 04:12:57 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Tue, 27 Jun 2006 21:12:57 -0500 (CDT)
Subject: [R] hopefully my last question on lapply
Message-ID: <12552741.1665961151460777389.JavaMail.root@vms071.mailsrvcs.net>

Marc and many other people ( whose names escape me ) have been
very helpful in explaining the use of lapply to me.

In his last response, Marc explained that if tradevectors is a list
of vectors of different lengths ( excuse my terminology ) then

lapply(tradevectors,function(x) G[x]*B[x] )

will go through each component of the list as if it was a vector
and apply the element by element multiplication of G and B,
( G and B are vectors ) Then, it again returns a list of vectors.

This is fine and I understand it.

What I find confusing is that sometimes I want
to take the two vectors G abd B and send them totally
into some function with the respective indices from
tradevectors not just the element by element index.
For example, suppose I have a function called
myfunction that takes two vectors as its inputs.
Then, here are my ideas for what could be done.

Attempt 1 : lapply(tradevectors,function(i) myfunction(G[i],B[i])

Attempt 2 : lapply(along=tradevectors),function(i) myfunction(G[[i]],B[[i]]


In attempt1, I am just putting tradevectors and indexing
using the [] which I think won't work.  In attemp t2, I am using along=tradevectors and using [[]].

I think # 2 is correct but could someone confirm this 
because I have quite large vectors and it's not easy at all
for me to check what's going on and I'm not so clear
with lapply usage. There's seem tobe many different ways
of setting up the first parameter in the call to lapply. 

                                    Thanks


From hodgess at gator.dt.uh.edu  Wed Jun 28 05:18:51 2006
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Tue, 27 Jun 2006 22:18:51 -0500
Subject: [R]  installing R on RedHat
Message-ID: <200606280318.k5S3IpiO003090@gator.dt.uh.edu>

Dear R People:

Yet again, I am attempting to install R on RedHat Linux.

Here is my sorry attempt to date:
[hodgess at gator hodgess]$ rpm -vi R.rpm
warning: R.rpm: V3 DSA signature: NOKEY, key ID 97d3544e
error: cannot write to %sourcedir /usr/src/redhat/SOURCES

I only want to write it to my own userid, since I am the only one
who uses it.

Any suggestions would be much appreciated.

Thanks,
Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu


From MSchwartz at mn.rr.com  Wed Jun 28 05:46:49 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Tue, 27 Jun 2006 22:46:49 -0500
Subject: [R] hopefully my last question on lapply
In-Reply-To: <12552741.1665961151460777389.JavaMail.root@vms071.mailsrvcs.net>
References: <12552741.1665961151460777389.JavaMail.root@vms071.mailsrvcs.net>
Message-ID: <1151466409.5722.23.camel@localhost.localdomain>

On Tue, 2006-06-27 at 21:12 -0500, markleeds at verizon.net wrote:
> Marc and many other people ( whose names escape me ) have been
> very helpful in explaining the use of lapply to me.
> 
> In his last response, Marc explained that if tradevectors is a list
> of vectors of different lengths ( excuse my terminology ) then
> 
> lapply(tradevectors,function(x) G[x]*B[x] )
> 
> will go through each component of the list as if it was a vector
> and apply the element by element multiplication of G and B,
> ( G and B are vectors ) Then, it again returns a list of vectors.
> 
> This is fine and I understand it.
> 
> What I find confusing is that sometimes I want
> to take the two vectors G abd B and send them totally
> into some function with the respective indices from
> tradevectors not just the element by element index.
> For example, suppose I have a function called
> myfunction that takes two vectors as its inputs.
> Then, here are my ideas for what could be done.
> 
> Attempt 1 : lapply(tradevectors,function(i) myfunction(G[i],B[i])
> 
> Attempt 2 : lapply(along=tradevectors),function(i) myfunction(G[[i]],B[[i]]
> 
> 
> In attempt1, I am just putting tradevectors and indexing
> using the [] which I think won't work.  In attemp t2, I am using along=tradevectors and using [[]].
> 
> I think # 2 is correct but could someone confirm this 
> because I have quite large vectors and it's not easy at all
> for me to check what's going on and I'm not so clear
> with lapply usage. There's seem tobe many different ways
> of setting up the first parameter in the call to lapply. 

Mark,

My presumption is that you want to pass the subsetted vectors G and B to
myfunction() during each iteration along the list 'tradevectors' and
then perform some more complex operation on them.  

Is that correct?

If so, then I think that you are misunderstanding what is happening in
lapply(), when you use:

  lapply(tradevectors,function(i) myfunction(G[i], B[i]))

In this case, it is not the individual elements of G and B that are
passed to myfunction() one pair at a time, but the entire subsetted G[i]
and B[i] that are passed. The vector subsetting takes place BEFORE the
vectors are passed to myfunction().

Thus, for example, let's use the original data that we have been working
with here:

tempa <- c(4, 6, 10)

tempb <- c(11, 23, 39)

tradevectors <- mapply(seq, from = tempa, to = tempb)

X <- seq(2, 80, 2)
Y <- 1:40

# myfunction() will take x and y, cbind() them,
# print the result and then return a NULL to lapply
# So for each iteration in lapply, the cbind/print will be 
# executed once.
myfunction <- function(x, y) 
{
  print(cbind(x, y))
  NULL
}


> lapply(tradevectors,function(i) myfunction(X[i], Y[i]))
      x  y
[1,]  8  4
[2,] 10  5
[3,] 12  6
[4,] 14  7
[5,] 16  8
[6,] 18  9
[7,] 20 10
[8,] 22 11
       x  y
 [1,] 12  6
 [2,] 14  7
 [3,] 16  8
 [4,] 18  9
 [5,] 20 10
 [6,] 22 11
 [7,] 24 12
 [8,] 26 13
 [9,] 28 14
[10,] 30 15
[11,] 32 16
[12,] 34 17
[13,] 36 18
[14,] 38 19
[15,] 40 20
[16,] 42 21
[17,] 44 22
[18,] 46 23
       x  y
 [1,] 20 10
 [2,] 22 11
 [3,] 24 12
 [4,] 26 13
 [5,] 28 14
 [6,] 30 15
 [7,] 32 16
 [8,] 34 17
 [9,] 36 18
[10,] 38 19
[11,] 40 20
[12,] 42 21
[13,] 44 22
[14,] 46 23
[15,] 48 24
[16,] 50 25
[17,] 52 26
[18,] 54 27
[19,] 56 28
[20,] 58 29
[21,] 60 30
[22,] 62 31
[23,] 64 32
[24,] 66 33
[25,] 68 34
[26,] 70 35
[27,] 72 36
[28,] 74 37
[29,] 76 38
[30,] 78 39
[[1]]
NULL

[[2]]
NULL

[[3]]
NULL


Note that for each of the three iterations through tradevectors in
lapply(), myfunction() prints out a multi-row matrix consisting of the
subsetted vectors X and Y, using the indices provided in tradevectors.

The three final NULLs are then returned once lapply() has finished.


In addition, note that this behavior is different than mapply(), where
each argument in mapply() is passed in an element-by-element fashion to
the function indicated as the first argument. That is the behavior that
we are using above to create the sequences that become tradevectors. In
that case, each element of tempa and tempb, in sequence, are passed one
at a time as a single pair to seq(). That occurs three times.

HTH,

Marc Schwartz


From gerifalte28 at hotmail.com  Wed Jun 28 05:51:31 2006
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Wed, 28 Jun 2006 03:51:31 +0000
Subject: [R] distribution of daily rainfall values in binned categories
In-Reply-To: <20060627182859.78214.qmail@web36902.mail.mud.yahoo.com>
Message-ID: <BAY103-F5B8186CA891667715768DA67F0@phx.gbl>

Hi Etienne,

Somebody asked a somehow related question recently.  
http://tolstoy.newcastle.edu.au/R/help/06/06/29485.html

Take a look at cut? table? and barplot?
i.e.
#Creates fake data from uniform(0,30)

x=runif(50, 0,30)

#Creates categories
rain=cut(x,breaks=c( 0, 1,2.5,5, 10, 20, Inf))

#Creates contingency table of categories
tab=table(rain)

#Plots frequencies of rainfall
barplot(tab)


I hope this helps!

Francisco

Dr. Francisco J. Zagmutt
College of Veterinary Medicine and Biomedical Sciences
Colorado State University




>From: etienne <etiennesky at yahoo.com>
>To: r-help at stat.math.ethz.ch
>Subject: [R] distribution of daily rainfall values in binned categories
>Date: Tue, 27 Jun 2006 11:28:59 -0700 (PDT)
>
>Hi,
>
>I'm a newbie in using R and I would like to have a few
>clues as to how I could compute and plot a
>distribution of daily rainfall intensity in different
>categories.  I have daily values (mm/day) for several
>years and I need to show the frequency of 0-1, 1-2.5,
>2.5-5, 5-10, 10-20, 20+ mm/day.  Can this be done
>easily?
>
>Thanks,
>Etienne
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html


From MSchwartz at mn.rr.com  Wed Jun 28 06:01:06 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Tue, 27 Jun 2006 23:01:06 -0500
Subject: [R] installing R on RedHat
In-Reply-To: <200606280318.k5S3IpiO003090@gator.dt.uh.edu>
References: <200606280318.k5S3IpiO003090@gator.dt.uh.edu>
Message-ID: <1151467266.5722.34.camel@localhost.localdomain>

On Tue, 2006-06-27 at 22:18 -0500, Erin Hodgess wrote:
> Dear R People:
> 
> Yet again, I am attempting to install R on RedHat Linux.
> 
> Here is my sorry attempt to date:
> [hodgess at gator hodgess]$ rpm -vi R.rpm
> warning: R.rpm: V3 DSA signature: NOKEY, key ID 97d3544e
> error: cannot write to %sourcedir /usr/src/redhat/SOURCES
> 
> I only want to write it to my own userid, since I am the only one
> who uses it.
> 
> Any suggestions would be much appreciated.

Erin,

As far as I know, the R RPMS provide either by Martyn Plummer et al on
CRAN, or more recently via Fedora Extras, are generally not
relocatable. 

In other words, they must be installed as root into a pre-defined
location.

I checked the list archive and this had come up last year:

http://finzi.psych.upenn.edu/R/tmp/Rhelp02a/archive/53976.html

and I don't know that this has changed.

If you need to install only as a local user, you will likely need to do
so from source as I referenced in the above thread. See the R-admin
manual for more information on how to configure for this.

More generally, if you have root access on your system, the default on
Linux is to install using system-wide configurations, not per user, even
if you are the only user.

User specific installation is generally only used if you need to install
something and do not have root access.

HTH,

Marc Schwartz


From raharjo_hendry at yahoo.com.sg  Wed Jun 28 06:03:13 2006
From: raharjo_hendry at yahoo.com.sg (hendry raharjo)
Date: Wed, 28 Jun 2006 12:03:13 +0800 (CST)
Subject: [R] compositional time series
In-Reply-To: <J1IW86$2C607AFD9290E8A9F3378BA8ABE48858@terra.com.br>
Message-ID: <20060628040313.63061.qmail@web35711.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060628/787b7b18/attachment.pl 

From shireesh_s at yahoo.com  Wed Jun 28 06:15:40 2006
From: shireesh_s at yahoo.com (Shireesh Srivastava)
Date: Tue, 27 Jun 2006 21:15:40 -0700 (PDT)
Subject: [R] Question regarding topTable function in limma
Message-ID: <20060628041540.81427.qmail@web53204.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060627/79ebe418/attachment.pl 

From chatfield at alumni.rice.edu  Wed Jun 28 06:55:39 2006
From: chatfield at alumni.rice.edu (Robert Chatfield)
Date: Tue, 27 Jun 2006 21:55:39 -0700
Subject: [R] Reporting ppr fits and using them externally.
Message-ID: <A5C6BB17-51AE-408A-B745-683FB91DE9E2@alumni.rice.edu>

The pursuit projection packages ppr is an excellent contribution to  
R.  It is great for one-to-three ridge fits,
often somewhat intuitive, and for multi-ridge fits, where it at least  
describes a lot of variance.

Like many folk, I need to report the fits obtained from ppr to the  
greater, outside, non-R
world.  It is fairly obvious how to use the terms alpha and beta to  
report on directionality
and importance.

It has proven difficult to report on the spline fits generated.  We  
are moving into some "cryptanalysis"
of the uncommented "predict"  code with the "ppr" method in order to  
locate the information,
and can report, if warranted.

The question:
How can one simply recover the spline knots and the spline parameters  
associated with
a particular fit?

Are we missing something obvious, or has there been contributed code  
that we could
make use of?   We have considered making spline fits of the spline  
fit variables, but this
seems a bit obtuse.

In our case, there are usually several thousand rows of the predictor  
variables, so the exact
description of the knots is necessary but not very problem-dependent.

A second (rhetorical) question:
Can more information be associated with the differing projection  
directions chosen
for the fit?  We have made a second analysis with sphered data, and  
run arbitrary
subsets to assess contributions of spline fits along the various  
directions, and computed
correlations with fitted and (fitted+residual) data. Maybe there's a  
more standard approach we're missing.

Thanks for your advice!

Bob Chatfield / NASA Ames Research Center


From petr.pikal at precheza.cz  Wed Jun 28 08:49:14 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 28 Jun 2006 08:49:14 +0200
Subject: [R] hopefully my last question on lapply
In-Reply-To: <1151466409.5722.23.camel@localhost.localdomain>
References: <12552741.1665961151460777389.JavaMail.root@vms071.mailsrvcs.net>
Message-ID: <44A2428A.20002.4FF06A@localhost>

Hi

or you can use
some.list <- vector("list", length(tradevectors))
for (i in length(tradevectors) some.list[[i]] <-
your.function(G[tradevectors[[i]]], B[tradevectors[[i]]])

HTH
Petr



On 27 Jun 2006 at 22:46, Marc Schwartz wrote:

From:           	Marc Schwartz <MSchwartz at mn.rr.com>
To:             	markleeds at verizon.net
Date sent:      	Tue, 27 Jun 2006 22:46:49 -0500
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] hopefully my last question on lapply
Send reply to:  	MSchwartz at mn.rr.com
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>

> On Tue, 2006-06-27 at 21:12 -0500, markleeds at verizon.net wrote:
> > Marc and many other people ( whose names escape me ) have been
> > very helpful in explaining the use of lapply to me.
> > 
> > In his last response, Marc explained that if tradevectors is a list
> > of vectors of different lengths ( excuse my terminology ) then
> > 
> > lapply(tradevectors,function(x) G[x]*B[x] )
> > 
> > will go through each component of the list as if it was a vector and
> > apply the element by element multiplication of G and B, ( G and B
> > are vectors ) Then, it again returns a list of vectors.
> > 
> > This is fine and I understand it.
> > 
> > What I find confusing is that sometimes I want
> > to take the two vectors G abd B and send them totally
> > into some function with the respective indices from
> > tradevectors not just the element by element index.
> > For example, suppose I have a function called
> > myfunction that takes two vectors as its inputs.
> > Then, here are my ideas for what could be done.
> > 
> > Attempt 1 : lapply(tradevectors,function(i) myfunction(G[i],B[i])
> > 
> > Attempt 2 : lapply(along=tradevectors),function(i)
> > myfunction(G[[i]],B[[i]]
> > 
> > 
> > In attempt1, I am just putting tradevectors and indexing
> > using the [] which I think won't work.  In attemp t2, I am using
> > along=tradevectors and using [[]].
> > 
> > I think # 2 is correct but could someone confirm this 
> > because I have quite large vectors and it's not easy at all
> > for me to check what's going on and I'm not so clear
> > with lapply usage. There's seem tobe many different ways
> > of setting up the first parameter in the call to lapply. 
> 
> Mark,
> 
> My presumption is that you want to pass the subsetted vectors G and B
> to myfunction() during each iteration along the list 'tradevectors'
> and then perform some more complex operation on them.  
> 
> Is that correct?
> 
> If so, then I think that you are misunderstanding what is happening in
> lapply(), when you use:
> 
>   lapply(tradevectors,function(i) myfunction(G[i], B[i]))
> 
> In this case, it is not the individual elements of G and B that are
> passed to myfunction() one pair at a time, but the entire subsetted
> G[i] and B[i] that are passed. The vector subsetting takes place
> BEFORE the vectors are passed to myfunction().
> 
> Thus, for example, let's use the original data that we have been
> working with here:
> 
> tempa <- c(4, 6, 10)
> 
> tempb <- c(11, 23, 39)
> 
> tradevectors <- mapply(seq, from = tempa, to = tempb)
> 
> X <- seq(2, 80, 2)
> Y <- 1:40
> 
> # myfunction() will take x and y, cbind() them,
> # print the result and then return a NULL to lapply
> # So for each iteration in lapply, the cbind/print will be 
> # executed once.
> myfunction <- function(x, y) 
> {
>   print(cbind(x, y))
>   NULL
> }
> 
> 
> > lapply(tradevectors,function(i) myfunction(X[i], Y[i]))
>       x  y
> [1,]  8  4
> [2,] 10  5
> [3,] 12  6
> [4,] 14  7
> [5,] 16  8
> [6,] 18  9
> [7,] 20 10
> [8,] 22 11
>        x  y
>  [1,] 12  6
>  [2,] 14  7
>  [3,] 16  8
>  [4,] 18  9
>  [5,] 20 10
>  [6,] 22 11
>  [7,] 24 12
>  [8,] 26 13
>  [9,] 28 14
> [10,] 30 15
> [11,] 32 16
> [12,] 34 17
> [13,] 36 18
> [14,] 38 19
> [15,] 40 20
> [16,] 42 21
> [17,] 44 22
> [18,] 46 23
>        x  y
>  [1,] 20 10
>  [2,] 22 11
>  [3,] 24 12
>  [4,] 26 13
>  [5,] 28 14
>  [6,] 30 15
>  [7,] 32 16
>  [8,] 34 17
>  [9,] 36 18
> [10,] 38 19
> [11,] 40 20
> [12,] 42 21
> [13,] 44 22
> [14,] 46 23
> [15,] 48 24
> [16,] 50 25
> [17,] 52 26
> [18,] 54 27
> [19,] 56 28
> [20,] 58 29
> [21,] 60 30
> [22,] 62 31
> [23,] 64 32
> [24,] 66 33
> [25,] 68 34
> [26,] 70 35
> [27,] 72 36
> [28,] 74 37
> [29,] 76 38
> [30,] 78 39
> [[1]]
> NULL
> 
> [[2]]
> NULL
> 
> [[3]]
> NULL
> 
> 
> Note that for each of the three iterations through tradevectors in
> lapply(), myfunction() prints out a multi-row matrix consisting of the
> subsetted vectors X and Y, using the indices provided in tradevectors.
> 
> The three final NULLs are then returned once lapply() has finished.
> 
> 
> In addition, note that this behavior is different than mapply(), where
> each argument in mapply() is passed in an element-by-element fashion
> to the function indicated as the first argument. That is the behavior
> that we are using above to create the sequences that become
> tradevectors. In that case, each element of tempa and tempb, in
> sequence, are passed one at a time as a single pair to seq(). That
> occurs three times.
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From ripley at stats.ox.ac.uk  Wed Jun 28 08:50:16 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 28 Jun 2006 07:50:16 +0100 (BST)
Subject: [R] Reporting ppr fits and using them externally.
In-Reply-To: <A5C6BB17-51AE-408A-B745-683FB91DE9E2@alumni.rice.edu>
References: <A5C6BB17-51AE-408A-B745-683FB91DE9E2@alumni.rice.edu>
Message-ID: <Pine.LNX.4.64.0606280742100.7109@gannet.stats.ox.ac.uk>

It is normal to report smooth curves via plots of smooth curves. There are 
examples for ppr in MASS4 p.241 (referenced on the help page).  This is 
done by the plot() method.

Please do consult the references in the documentation: those who very 
carefully documented these tools put them there because they do give 
additional information.

On Tue, 27 Jun 2006, Robert Chatfield wrote:

> The pursuit projection packages ppr is an excellent contribution to R. 
> It is great for one-to-three ridge fits, often somewhat intuitive, and 
> for multi-ridge fits, where it at least describes a lot of variance.
>
> Like many folk, I need to report the fits obtained from ppr to the 
> greater, outside, non-R world.  It is fairly obvious how to use the 
> terms alpha and beta to report on directionality and importance.
>
> It has proven difficult to report on the spline fits generated.  We are 
> moving into some "cryptanalysis" of the uncommented "predict"  code with 
> the "ppr" method in order to locate the information, and can report, if 
> warranted.

In what sense do you claim it to be `undocumented'?  Methods should do 
what the generic is documented to do, and that is the case here.

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From harry.wills at gmail.com  Wed Jun 28 09:02:06 2006
From: harry.wills at gmail.com (harry wills)
Date: Wed, 28 Jun 2006 00:02:06 -0700
Subject: [R] Linear Mixed Effects
Message-ID: <8ced90440606280002y7b1c18a9q2c98164bc3f84909@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060628/fb68c6e2/attachment.pl 

From dimitris.rizopoulos at med.kuleuven.be  Wed Jun 28 09:23:56 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 28 Jun 2006 09:23:56 +0200
Subject: [R] Linear Mixed Effects
References: <8ced90440606280002y7b1c18a9q2c98164bc3f84909@mail.gmail.com>
Message-ID: <00ca01c69a83$d1439ba0$0540210a@www.domain>

AFAIK lme(), by default, estimates the covariance between two or more 
random-effects, thus you do not have to specify anything. You're 
probably looking for something like:

fit <- lme(y ~ time * treat, random = ~ time | subject)
fit
summary(fit)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "harry wills" <harry.wills at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, June 28, 2006 9:02 AM
Subject: [R] Linear Mixed Effects


> Hi,
> I have implemented the lme in SAS and specified the random effects
> covariance structure as unstructured using the statement "random / 
> type=UN"
> I want to specify same in R but not able to know how to do it.
> Can anyone please advise me on how to proceed.
> Thanks
>
>
> -- 
> Wills, Harry
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From rkrug at sun.ac.za  Wed Jun 28 09:29:45 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Wed, 28 Jun 2006 09:29:45 +0200
Subject: [R] RMySQL...Can't initialize driver???
In-Reply-To: <44A14392.6020807@genoscope.cns.fr>
References: <44A129B8.3060309@genoscope.cns.fr> <44A12CD6.3090008@stats.uwo.ca>
	<44A12E1F.8020901@genoscope.cns.fr> <44A131DB.3070600@sun.ac.za>
	<44A14392.6020807@genoscope.cns.fr>
Message-ID: <44A22FE9.1020305@sun.ac.za>

Stephane Cruveiller wrote:
> Rainer M Krug wrote:
> I followed your instruction. Here is my .my.cnf:
> ---------------------------------------------------------------------------
> [client]
> user=steff
> password=XXXXXX
> database=justforfun
> ---------------------------------------------------------------------------
> with it, I can connect to my MYSQL server without problem
> and then I try to connect through R:
> 
> ---------------------------------------------------------------------------
>> library(RMySQL)
> Loading required package: DBI
>> m <- dbDriver("MySQL")
> Error in new("MySQLDriver", Id = drvId) : unused argument(s) (Id ...)
> ---------------------------------------------------------------------------
> 
> It still produces the error. I really  do not  know what is going on
> here...

Sorry - no idea. I guess you are using Linux? And all packages are up to
date?

Rainer




> 
> 
>> Then you do:
>>
>>
>> library("RMySQL")
>> m <- dbDriver("MySQL")
>> con <- dbConnect(m, group = "renpatch")
>> q <- TheSQLQuery
>> rs <- dbSendQuery(con, q)
>> TheResults <- fetch(rs, n = -1)
>> dbDisconnect(con)
>> rm(con)
>>
>>
>> and it should work - at least that is what it is doing for me.
>>
>> Rainer
>>
>>   
> 
> Stephane.


-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
      	Rainer at krugs.de


From scruveil at genoscope.cns.fr  Wed Jun 28 09:42:53 2006
From: scruveil at genoscope.cns.fr (Stephane Cruveiller)
Date: Wed, 28 Jun 2006 09:42:53 +0200
Subject: [R] RMySQL...Can't initialize driver??? Solved!!!
In-Reply-To: <44A22FE9.1020305@sun.ac.za>
References: <44A129B8.3060309@genoscope.cns.fr> <44A12CD6.3090008@stats.uwo.ca>
	<44A12E1F.8020901@genoscope.cns.fr> <44A131DB.3070600@sun.ac.za>
	<44A14392.6020807@genoscope.cns.fr> <44A22FE9.1020305@sun.ac.za>
Message-ID: <44A232FD.1090700@genoscope.cns.fr>

Hi,

I have finally made it working. I do not if that was the problem but
the libraries (DBI and RMySQL) were installed in a non-standard
location (of course specified in a .Renviron file). Reinstalling both of
them at R root did the trick...


thanks to you all for your help...

Stephane.

Rainer M Krug wrote:
> Stephane Cruveiller wrote:
>   
>> Rainer M Krug wrote:
>> I followed your instruction. Here is my .my.cnf:
>> ---------------------------------------------------------------------------
>> [client]
>> user=steff
>> password=XXXXXX
>> database=justforfun
>> ---------------------------------------------------------------------------
>> with it, I can connect to my MYSQL server without problem
>> and then I try to connect through R:
>>
>> ---------------------------------------------------------------------------
>>     
>>> library(RMySQL)
>>>       
>> Loading required package: DBI
>>     
>>> m <- dbDriver("MySQL")
>>>       
>> Error in new("MySQLDriver", Id = drvId) : unused argument(s) (Id ...)
>> ---------------------------------------------------------------------------
>>
>> It still produces the error. I really  do not  know what is going on
>> here...
>>     
>
> Sorry - no idea. I guess you are using Linux? And all packages are up to
> date?
>
> Rainer
>
>
>
>
>   
>>     
>>> Then you do:
>>>
>>>
>>> library("RMySQL")
>>> m <- dbDriver("MySQL")
>>> con <- dbConnect(m, group = "renpatch")
>>> q <- TheSQLQuery
>>> rs <- dbSendQuery(con, q)
>>> TheResults <- fetch(rs, n = -1)
>>> dbDisconnect(con)
>>> rm(con)
>>>
>>>
>>> and it should work - at least that is what it is doing for me.
>>>
>>> Rainer
>>>
>>>   
>>>       
>> Stephane.
>>     
>
>
>   


From plummer at iarc.fr  Wed Jun 28 10:26:04 2006
From: plummer at iarc.fr (Martyn Plummer)
Date: Wed, 28 Jun 2006 10:26:04 +0200
Subject: [R] installing R on RedHat
In-Reply-To: <1151467266.5722.34.camel@localhost.localdomain>
References: <200606280318.k5S3IpiO003090@gator.dt.uh.edu>
	<1151467266.5722.34.camel@localhost.localdomain>
Message-ID: <1151483164.2364.14.camel@seurat.iarc.fr>

On Tue, 2006-06-27 at 23:01 -0500, Marc Schwartz wrote:
> On Tue, 2006-06-27 at 22:18 -0500, Erin Hodgess wrote:
> > Dear R People:
> > 
> > Yet again, I am attempting to install R on RedHat Linux.
> > 
> > Here is my sorry attempt to date:
> > [hodgess at gator hodgess]$ rpm -vi R.rpm
> > warning: R.rpm: V3 DSA signature: NOKEY, key ID 97d3544e
> > error: cannot write to %sourcedir /usr/src/redhat/SOURCES

It looks to me like  you are trying to install the *source*
RPM.

> > I only want to write it to my own userid, since I am the only one
> > who uses it.
> > 
> > Any suggestions would be much appreciated.
> 
> Erin,
> 
> As far as I know, the R RPMS provide either by Martyn Plummer et al on
> CRAN, or more recently via Fedora Extras, are generally not
> relocatable. 
> 
> In other words, they must be installed as root into a pre-defined
> location.
> 
> I checked the list archive and this had come up last year:
> 
> http://finzi.psych.upenn.edu/R/tmp/Rhelp02a/archive/53976.html
> 
> and I don't know that this has changed.
> 
> If you need to install only as a local user, you will likely need to do
> so from source as I referenced in the above thread. See the R-admin
> manual for more information on how to configure for this.
> 
> More generally, if you have root access on your system, the default on
> Linux is to install using system-wide configurations, not per user, even
> if you are the only user.
> 
> User specific installation is generally only used if you need to install
> something and do not have root access.
> 
> HTH,
> 
> Marc Schwartz

Just to confirm that the RPMs are not relocatable. At one point they
were, but now all of the install destinations are parameterized in terms
of rpm macros such as %{_libdir} %{_infodir}, ... This allows the same
spec file to be used for multiple Linux distributions, but is
incompatible with making the RPM relocatable.

In your case, I don't see a problem with asking your system
administrator to install R for you.  You do not need write access to the
installation directories. You can install your own R packages without
administrative priviliges by defining the environment variable R_LIBS to
be a sub-directory of your home directory.

Martyn


-----------------------------------------------------------------------
This message and its attachments are strictly confidential. ...{{dropped}}


From gregor.gorjanc at bfro.uni-lj.si  Wed Jun 28 10:39:20 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Wed, 28 Jun 2006 10:39:20 +0200
Subject: [R] Data in R for tag clouds
Message-ID: <44A24038.5060900@bfro.uni-lj.si>

Hello!

I have written function for tag clouds[1]. Currently it is in alpha
state and can be found at[2]. I would like to provide some data for this
in examples, but I would like to consider if there is any relevant data
for such a plot already in R so I can easily load it.

Btw. If anyone has any comments, suggestions, (check also [3]) etc.
please feel free to send them to me!

Thanks!

[1]http://en.wikipedia.org/wiki/Tag_cloud
[2]http://www.bfro.uni-lj.si/MR/ggorjan/software/R/index.html#tagCloud
[3]http://www.bfro.uni-lj.si/MR/ggorjan/software/R/tagCloud/TODO.txt
-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From maechler at stat.math.ethz.ch  Wed Jun 28 10:39:58 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 28 Jun 2006 10:39:58 +0200
Subject: [R] distribution of daily rainfall values in binned categories
In-Reply-To: <BAY103-F5B8186CA891667715768DA67F0@phx.gbl>
References: <20060627182859.78214.qmail@web36902.mail.mud.yahoo.com>
	<BAY103-F5B8186CA891667715768DA67F0@phx.gbl>
Message-ID: <17570.16478.127762.160753@stat.math.ethz.ch>

>>>>> "FJZ" == Francisco J Zagmutt <gerifalte28 at hotmail.com>
>>>>>     on Wed, 28 Jun 2006 03:51:31 +0000 writes:

    FJZ> Hi Etienne,
    FJZ> Somebody asked a somehow related question recently.  
    FJZ> http://tolstoy.newcastle.edu.au/R/help/06/06/29485.html

    FJZ> Take a look at cut? table? and barplot?
    FJZ> i.e.

      # Creates fake data from uniform(0,30)
      set.seed(1) ## <<- added by MM
      x=runif(50, 0,30)

      # Creates categories
      rain=cut(x,breaks=c( 0, 1,2.5,5, 10, 20, Inf))

      # Creates contingency table of categories
      tab=table(rain)

      # Plots frequencies of rainfall
      barplot(tab)


No, no, no!  Do not confuse histograms with bar plots!

-  barplot() is {one possibility} for visualizing discrete
   ("categorical", "factor") data,
-  hist() is for visualizing *continuous* data  (*)

As Jim Porzak replied, do use hist(): the example really is a matter
of visualization of a continuous distribution which should *not*
be done by a barplot.  Instead, e.g.,

  hist(x, breaks = c(0, 1,2.5,5, 10,20, max(pretty(max(x)))),
       freq = TRUE, col = "gray")

will give a graphic similar to the above --- BUT also 
warns you about the hidden deception (aka sillyness) of *both* graphics:
Namely, the above hist() call warns you with

>> Warning message:
>> the AREAS in the plot are wrong -- rather use freq=FALSE in: ....

and finally,

  hist(x, breaks = c(0, 1,2.5,5, 10,20, max(pretty(max(x)))), col="gray")

gives you a more honest graphic --- which -- for the runif()
example -- may finally lead to you to realize that using unequal
break may really not be such a good idea.
Note however that for the OP rainfall data, that may well be different
and if I look at rainfall data, I find I would rather view

   hist(log10( <rainfall> ))
or then
   plot(density( log10( <rainfall> ) ))

Martin Maechler, ETH Zurich

(*) From statistical point of view, histograms just density estimators, 
    and -- as known for a while -- have quite some drawbacks.
    Hence they should nowadays often be replaced by
        plot(density(.), ..)


    >> From: etienne <etiennesky at yahoo.com>
    >> To: r-help at stat.math.ethz.ch
    >> Subject: [R] distribution of daily rainfall values in binned categories
    >> Date: Tue, 27 Jun 2006 11:28:59 -0700 (PDT)
    >> 
    >> Hi,
    >> 
    >> I'm a newbie in using R and I would like to have a few
    >> clues as to how I could compute and plot a
    >> distribution of daily rainfall intensity in different
    >> categories.  I have daily values (mm/day) for several
    >> years and I need to show the frequency of 0-1, 1-2.5,
    >> 2.5-5, 5-10, 10-20, 20+ mm/day.  Can this be done
    >> easily?
    >> 
    >> Thanks,
    >> Etienne
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide! 
    >> http://www.R-project.org/posting-guide.html

    FJZ> ______________________________________________
    FJZ> R-help at stat.math.ethz.ch mailing list
    FJZ> https://stat.ethz.ch/mailman/listinfo/r-help
    FJZ> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From stp02fm at sheffield.ac.uk  Wed Jun 28 13:04:56 2006
From: stp02fm at sheffield.ac.uk (F Monadjemi)
Date: Wed, 28 Jun 2006 12:04:56 +0100
Subject: [R] NLME Fitting
Message-ID: <1151492696.44a2625892b3d@webmail.shef.ac.uk>

Dear Reader,

Is it possible to extract the random part of nlme fitting analysis (non linear
mixed effect model) i.e.sigma(b), in R?

Thank you for respond


Farinaz


From jim at bitwrit.com.au  Thu Jun 29 03:10:21 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Wed, 28 Jun 2006 21:10:21 -0400
Subject: [R] how to rotate a triangle image(ZMAT) ?
In-Reply-To: <20060627172520.92465.qmail@web30611.mail.mud.yahoo.com>
References: <20060627172520.92465.qmail@web30611.mail.mud.yahoo.com>
Message-ID: <44A3287D.1010207@bitwrit.com.au>

Cleber N.Borges wrote:
>  Hello R users...
> 
>  how to align this Zmat (triangle image)  in X axis?
>  I would like that the triangle's base become in the X
> axis and
>  the triangle's height become in the Y axis 
> 
>  Is there some trick for make this?
> 
How about:

image(zmat,col=terrain.colors(10),xlim=c(1,0),ylim=c(1,0))

Jim


From jim at bitwrit.com.au  Thu Jun 29 03:26:51 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Wed, 28 Jun 2006 21:26:51 -0400
Subject: [R] Continuation and parse
Message-ID: <44A32C5B.3030409@bitwrit.com.au>

Hi gurus,

After an unsuccessful scrabble through the documentation and Jon's 
excellent search facility, I am no wiser as to how R recognizes an 
incomplete command line and politely raises its hand for more. The help 
page for parse gives no indication that it does anything more than spit 
the dummy when fed an incomplete command line, but something in there 
must recognize such ellipsis. Any hints?

Jim


From paslopez at hotmail.com  Wed Jun 28 13:26:38 2006
From: paslopez at hotmail.com (Pascual Lopez)
Date: Wed, 28 Jun 2006 13:26:38 +0200
Subject: [R] Help with circular statistics
Message-ID: <BAY104-DAV13361D041A44812D80E8BEC17F0@phx.gbl>

Se ha borrado un texto insertado con un juego de caracteres sin especificar...
Nombre: no disponible
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060628/32226ee4/attachment.pl 

From paul.bliese at us.army.mil  Wed Jun 28 13:31:55 2006
From: paul.bliese at us.army.mil (Bliese, Paul D LTC USAMH)
Date: Wed, 28 Jun 2006 13:31:55 +0200
Subject: [R] Simulate dichotomous correlation matrix
Message-ID: <77C117DE9E87354CB7804A2DE7D566C2B7DF09@amedmlermc132>

Newsgroup members,

Does anyone have a clever way to simulate a correlation matrix such that
each column contains dichotomous variables (0,1) and where each column
has different prevalence rates.

For instance, I would like to simulate the following correlation matrix:

> CORMAT[1:4,1:4]
          PUREPT    PTCUT2    PHQCUT2T  ALCCUTT2
PUREPT   1.0000000 0.5141552 0.1913139 0.1917923
PTCUT2   0.5141552 1.0000000 0.2913552 0.2204097
PHQCUT2T 0.1913139 0.2913552 1.0000000 0.1803987
ALCCUTT2 0.1917923 0.2204097 0.1803987 1.0000000

Where the prevalence for each variable is:

> prevvals=c(0.26,0.10,0.09,0.10)

I can use the mvrnorm function in MASS to create a matrix containing
random normal variables and dichotomize these variables into 0,1;
however, this is a less than ideal solution as my observed correlation
matrix is downwardly biased and the amount of the bias is related to the
prevalence of each variable.

Thanks,


Paul D. Bliese
Heidelberg, Germany
COMM:  +49-6221-172626


From davidek at zla-ryba.cz  Tue Jun 27 18:07:22 2006
From: davidek at zla-ryba.cz (davidek at zla-ryba.cz)
Date: Tue, 27 Jun 2006 18:07:22 +0200
Subject: [R] Very slow read.table on Linux, compared to Win2000
Message-ID: <7DADB02FEDB74A74AC51CDD8B17EEC7.MAI@email.forpsi.com>

Dear all,

I read.table a 17MB tabulator separated table with 483 variables(mostly numeric) and 15000 
observations into R. This takes a few seconds with R 2.3.1 on windows 2000, but it takes 
several minutes on my Linux machine. The linux machine is Ubuntu 6.06, 256 MR RAM,
Athlon 1600 processor. The windows hardware is better (Pentium 4, 512 RAM), but it
shouldn't make such a difference. 

The strange thing is that even doing something with the data(say a histogram of a variable, or
transforming
integers into a factor)  takes really long time on the linux box and the computer seems to work
extensively with the hard disk. 
Could this be caused by swapping ? Can I increase the memory allocated to R somehow ?
I have checked the manual, but the memory options allowed for linux don't seem to
help me (I may be doing it wrong, though ...)

The code I run:

TBO <- read.table(file="TBO.dat",sep="\t",header=TRUE,dec=",");   # this takes forever
TBO$sexe<-factor(TBO$sexe,labels=c("man","vrouw"));   # even this takes like 30 seconds, compared
to nothing on Win2000

I'd be grateful for any suggestions,

Regards,
David Vonka


------------------------------------------------------------------
David Vonka (Netspar, Universiteit van Tilburg, room B-623)
CZ: Ovci Hajek 42, Praha 5, Czech Republic, tel: +420777022926 
NL: Telefoonstraat 1, 5038DL Tilburg, The Netherlands, tel:+31638083064


From MSchwartz at mn.rr.com  Wed Jun 28 14:06:17 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 28 Jun 2006 07:06:17 -0500
Subject: [R] installing R on RedHat
In-Reply-To: <1151483164.2364.14.camel@seurat.iarc.fr>
References: <200606280318.k5S3IpiO003090@gator.dt.uh.edu>
	<1151467266.5722.34.camel@localhost.localdomain>
	<1151483164.2364.14.camel@seurat.iarc.fr>
Message-ID: <1151496378.5722.37.camel@localhost.localdomain>

On Wed, 2006-06-28 at 10:26 +0200, Martyn Plummer wrote:
> On Tue, 2006-06-27 at 23:01 -0500, Marc Schwartz wrote:
> > On Tue, 2006-06-27 at 22:18 -0500, Erin Hodgess wrote:
> > > Dear R People:
> > > 
> > > Yet again, I am attempting to install R on RedHat Linux.
> > > 
> > > Here is my sorry attempt to date:
> > > [hodgess at gator hodgess]$ rpm -vi R.rpm
> > > warning: R.rpm: V3 DSA signature: NOKEY, key ID 97d3544e
> > > error: cannot write to %sourcedir /usr/src/redhat/SOURCES
> 
> It looks to me like  you are trying to install the *source*
> RPM.
> 
> > > I only want to write it to my own userid, since I am the only one
> > > who uses it.
> > > 
> > > Any suggestions would be much appreciated.
> > 
> > Erin,
> > 
> > As far as I know, the R RPMS provide either by Martyn Plummer et al on
> > CRAN, or more recently via Fedora Extras, are generally not
> > relocatable. 
> > 
> > In other words, they must be installed as root into a pre-defined
> > location.
> > 
> > I checked the list archive and this had come up last year:
> > 
> > http://finzi.psych.upenn.edu/R/tmp/Rhelp02a/archive/53976.html
> > 
> > and I don't know that this has changed.
> > 
> > If you need to install only as a local user, you will likely need to do
> > so from source as I referenced in the above thread. See the R-admin
> > manual for more information on how to configure for this.
> > 
> > More generally, if you have root access on your system, the default on
> > Linux is to install using system-wide configurations, not per user, even
> > if you are the only user.
> > 
> > User specific installation is generally only used if you need to install
> > something and do not have root access.
> > 
> > HTH,
> > 
> > Marc Schwartz
> 
> Just to confirm that the RPMs are not relocatable. At one point they
> were, but now all of the install destinations are parameterized in terms
> of rpm macros such as %{_libdir} %{_infodir}, ... This allows the same
> spec file to be used for multiple Linux distributions, but is
> incompatible with making the RPM relocatable.
> 
> In your case, I don't see a problem with asking your system
> administrator to install R for you.  You do not need write access to the
> installation directories. You can install your own R packages without
> administrative priviliges by defining the environment variable R_LIBS to
> be a sub-directory of your home directory.

Thanks for the confirmation Martyn.

Regards,

Marc


From p.dalgaard at biostat.ku.dk  Wed Jun 28 14:21:09 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Jun 2006 14:21:09 +0200
Subject: [R] Simulate dichotomous correlation matrix
In-Reply-To: <77C117DE9E87354CB7804A2DE7D566C2B7DF09@amedmlermc132>
References: <77C117DE9E87354CB7804A2DE7D566C2B7DF09@amedmlermc132>
Message-ID: <x28xnho4i2.fsf@viggo.kubism.ku.dk>

"Bliese, Paul D LTC USAMH" <paul.bliese at us.army.mil> writes:

> Newsgroup members,
> 
> Does anyone have a clever way to simulate a correlation matrix such that
> each column contains dichotomous variables (0,1) and where each column
> has different prevalence rates.
> 
> For instance, I would like to simulate the following correlation matrix:
> 
> > CORMAT[1:4,1:4]
>           PUREPT    PTCUT2    PHQCUT2T  ALCCUTT2
> PUREPT   1.0000000 0.5141552 0.1913139 0.1917923
> PTCUT2   0.5141552 1.0000000 0.2913552 0.2204097
> PHQCUT2T 0.1913139 0.2913552 1.0000000 0.1803987
> ALCCUTT2 0.1917923 0.2204097 0.1803987 1.0000000
> 
> Where the prevalence for each variable is:
> 
> > prevvals=c(0.26,0.10,0.09,0.10)
> 
> I can use the mvrnorm function in MASS to create a matrix containing
> random normal variables and dichotomize these variables into 0,1;
> however, this is a less than ideal solution as my observed correlation
> matrix is downwardly biased and the amount of the bias is related to the
> prevalence of each variable.

This is related to the concept of polychoric correlations: These are
correlations that could be passed to mvrnorm and dichotomized by
thresholds to give data with an observed distribution. The question is
if there is a nice way to go from raw correlations and prevalences to
polychoric corr. and thresholds. The threshold bit is easy, just take
qnorm(), but the other bit might not. You could try looking into the
polycor package and see which pieces of information are used there. 

Alternatively, you could notice that what you really have is the set
of all 2x2 marginals of a 2x2x2x2 table (you can reconstruct sum(X),
sum(Y) and sum(XY) from the information given) and you could fit a
(log-linear) model for all 16 probabilities using the IPS algorithm. 


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From p.dalgaard at biostat.ku.dk  Wed Jun 28 14:28:42 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Jun 2006 14:28:42 +0200
Subject: [R] Very slow read.table on Linux, compared to Win2000
In-Reply-To: <7DADB02FEDB74A74AC51CDD8B17EEC7.MAI@email.forpsi.com>
References: <7DADB02FEDB74A74AC51CDD8B17EEC7.MAI@email.forpsi.com>
Message-ID: <x24py5o45h.fsf@viggo.kubism.ku.dk>

<davidek at zla-ryba.cz> writes:

> Dear all,
> 
> I read.table a 17MB tabulator separated table with 483 variables(mostly numeric) and 15000 
> observations into R. This takes a few seconds with R 2.3.1 on windows 2000, but it takes 
> several minutes on my Linux machine. The linux machine is Ubuntu 6.06, 256 MR RAM,
> Athlon 1600 processor. The windows hardware is better (Pentium 4, 512 RAM), but it
> shouldn't make such a difference. 
> 
> The strange thing is that even doing something with the data(say a histogram of a variable, or
> transforming
> integers into a factor)  takes really long time on the linux box and the computer seems to work
> extensively with the hard disk. 
> Could this be caused by swapping ? Can I increase the memory allocated to R somehow ?
> I have checked the manual, but the memory options allowed for linux don't seem to
> help me (I may be doing it wrong, though ...)
> 
> The code I run:
> 
> TBO <- read.table(file="TBO.dat",sep="\t",header=TRUE,dec=",");   # this takes forever
> TBO$sexe<-factor(TBO$sexe,labels=c("man","vrouw"));   # even this takes like 30 seconds, compared
> to nothing on Win2000
> 
> I'd be grateful for any suggestions,

Almost surely, the fix is to insert more RAM chips. 256 MB leaves you
very little space for actual work these days, and a 17MB file will get
expanded to several times the original size during reading and data
manipulations. Using a lightweight window manager can help, but you
usually regret the switch for other reasons. 


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ripley at stats.ox.ac.uk  Wed Jun 28 14:44:49 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 28 Jun 2006 13:44:49 +0100 (BST)
Subject: [R] Continuation and parse
In-Reply-To: <44A32C5B.3030409@bitwrit.com.au>
References: <44A32C5B.3030409@bitwrit.com.au>
Message-ID: <Pine.LNX.4.64.0606281340230.22096@gannet.stats.ox.ac.uk>

On Wed, 28 Jun 2006, Jim Lemon wrote:

> Hi gurus,
>
> After an unsuccessful scrabble through the documentation and Jon's
> excellent search facility, I am no wiser as to how R recognizes an
> incomplete command line and politely raises its hand for more. The help
> page for parse gives no indication that it does anything more than spit
> the dummy when fed an incomplete command line, but something in there
> must recognize such ellipsis. Any hints?

It's internal.  Look in src/main/main.c, in particular the R_Repl* 
functions.  In short, R_Parse1Buffer can return PARSE_INCOMPLETE.

This is not done by the R-level function parse(): that parses a whole 
buffer rather than a line at a time, and maps all non-OK results to an 
error.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From gb at stat.umu.se  Wed Jun 28 14:42:16 2006
From: gb at stat.umu.se (=?ISO-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Wed, 28 Jun 2006 14:42:16 +0200
Subject: [R] [R-pkgs] New version of eha
Message-ID: <44A27928.7090105@stat.umu.se>

A new version (0.96.3) of the package 'eha' is now on CRAN. Apart from 
some bug fixes, there are some news. The most noteworthy are: (i) 
'plot.Surv' has a few new options, (ii) 'weibreg' now can fit null 
models (i.e., only scale and shape) and covariates are no longer 
automatically centered (although this is still the default), (iii) there 
is a new function, 'toBinary', which transforms a data frame suitable 
for survival analysis to a data frame suitable for ,eg, logistic 
regression as a substitute for a discrete time cox regression. In the 
process, sampling of survivors in risk sets is possible.

G?ran
-- 
G?ran Brostr?m, professor         phone: 46 90 786 5223
Department of Statistics	  fax: 46 90 786 6614
Ume? University                 email: gb at stat.umu.se
SE-90187 Ume?, Sweden            http://www.stat.umu.se/~goran.brostrom

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From gb at stat.umu.se  Wed Jun 28 14:49:18 2006
From: gb at stat.umu.se (=?ISO-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Wed, 28 Jun 2006 14:49:18 +0200
Subject: [R] [R-pkgs] New version of glmmML (p-values!)
Message-ID: <44A27ACE.1050301@stat.umu.se>

A new version of 'glmmML' (0.28-4) is uploaded to CRAN. The most 
important new feature is the possibility to get a p-value for the test 
of the hypothesis that the variance of the random effects is zero, on 
the wishlist of many R users these days! Note two things: (i) glmmML 
only treats random intercepts for binomial and poisson models, (ii) the 
p-value is calculated thru bootstrapping (can be slow with large data sets).

G?ran
-- 
G?ran Brostr?m, professor         phone: 46 90 786 5223
Department of Statistics	  fax: 46 90 786 6614
Ume? University                 email: gb at stat.umu.se
SE-90187 Ume?, Sweden            http://www.stat.umu.se/~goran.brostrom

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From e.rehak at t-online.de  Wed Jun 28 14:55:35 2006
From: e.rehak at t-online.de (Mark Hempelmann)
Date: Wed, 28 Jun 2006 14:55:35 +0200
Subject: [R] persp/ scatterplot3d
In-Reply-To: <mailman.9.1151488803.12054.r-help@stat.math.ethz.ch>
References: <mailman.9.1151488803.12054.r-help@stat.math.ethz.ch>
Message-ID: <44A27C47.7080408@t-online.de>

Dear WizaRds,

    I would like to create a 3d-plot with persp(). I sampled 17 points 
with xyz-coordinates (real-life example!), representing the peaks of the 
whole plane with "zero coordinates" x=3,y=3,z=3. My intention is to show 
which entries are above or below the "zero" level with persp() on a 
nicely created grid. I also tried scatterplot3d(), but, alas, I am 
unable to tell the function that my points represent the peaks of the 
plane and are either above or below "normal" (whatever that means...) 
Please help me:

dat<-matrix(c(1.33,1.00,2.67,4.33,4.00,5.00,0.67,3.33,1.00,3.00,3.00,1.33,1.67,1.67,1.33,1.67,2.33,1.67,0.67,1.00,1.33,3.33,2.67,1.67,1.67,2.67,2.00,0.33,0.67,0.33,2.67,3.33,0.67,0.67,1.33,0.00,4.33,3.33,4.67,3.00,4.00,5.00,4.00,3.67,1.67,3.00,3.67,3.33,1.00,1.33,0.33), 
ncol=3)
colnames(dat)<-c("x","y","z")

x=dat$x; y=dat$y, z=dat$z
persp(x,y,z) #   doesn't work at all, of course, even if I utilize outer()

scatterplot3d(x,y,z) #   returns a 3d scatterplot, but not the way I 
would like to see it fit.


Thank you so much for your help and support!


mark


From andy_liaw at merck.com  Wed Jun 28 14:43:02 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 28 Jun 2006 08:43:02 -0400
Subject: [R] Very slow read.table on Linux,
 compared to Win2000 [Broad cast]
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02884265@usctmx1106.merck.com>

From: Peter Dalgaard
> 
> <davidek at zla-ryba.cz> writes:
> 
> > Dear all,
> > 
> > I read.table a 17MB tabulator separated table with 483 
> > variables(mostly numeric) and 15000 observations into R. 
> This takes a 
> > few seconds with R 2.3.1 on windows 2000, but it takes 
> several minutes 
> > on my Linux machine. The linux machine is Ubuntu 6.06, 256 MR RAM, 
> > Athlon 1600 processor. The windows hardware is better 
> (Pentium 4, 512 RAM), but it shouldn't make such a difference.
> > 
> > The strange thing is that even doing something with the data(say a 
> > histogram of a variable, or transforming integers into a factor)  
> > takes really long time on the linux box and the computer 
> seems to work 
> > extensively with the hard disk.
> > Could this be caused by swapping ? Can I increase the 
> memory allocated to R somehow ?
> > I have checked the manual, but the memory options allowed for linux 
> > don't seem to help me (I may be doing it wrong, though ...)
> > 
> > The code I run:
> > 
> > TBO <- 
> read.table(file="TBO.dat",sep="\t",header=TRUE,dec=",");   # 
> this takes forever
> > TBO$sexe<-factor(TBO$sexe,labels=c("man","vrouw"));   # 
> even this takes like 30 seconds, compared
> > to nothing on Win2000
> > 
> > I'd be grateful for any suggestions,
> 
> Almost surely, the fix is to insert more RAM chips. 256 MB 
> leaves you very little space for actual work these days, and 

Try running Windows on the 256MB box and you'll see why Peter recommended
the above.  Consider yourself lucky that R actually still does something
useful under Unbuntu with so little RAM.  If adding more RAM is not an
option, perhaps not running X altogether would help.

Andy

> a 17MB file will get expanded to several times the original 
> size during reading and data manipulations. Using a 
> lightweight window manager can help, but you usually regret 
> the switch for other reasons. 
> 
> 
> -- 
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From luis.cota at avmltd.com  Wed Jun 28 15:08:17 2006
From: luis.cota at avmltd.com (Luis Cota)
Date: Wed, 28 Jun 2006 09:08:17 -0400
Subject: [R] Transform Normal Dist to ArcTan Dist
Message-ID: <5E8C818A1F91C34A9E80DE55A38C75910B2F8E@avm-ex-s1.avm.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060628/c665ff1c/attachment.pl 

From phgrosjean at sciviews.org  Wed Jun 28 15:24:07 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 28 Jun 2006 15:24:07 +0200
Subject: [R] Continuation and parse
In-Reply-To: <44A32C5B.3030409@bitwrit.com.au>
References: <44A32C5B.3030409@bitwrit.com.au>
Message-ID: <44A282F7.8070901@sciviews.org>

Well, you haven't used the search engines with the right key: the magic 
words are:

 > RSiteSearch("incomplete line")

With the first document being my query (almost two years ago), and the 
second one being Peter Dalgaard answer. You must adapt it to cope with 
internationalization, but basically, you could use something like:

 > grep("\n2:",try(parse(textConnection("ls)")), silent = TRUE))
numeric(0)
 > grep("\n2:",try(parse(textConnection("ls(")), silent = TRUE))
[1] 1

Best,

Philippe Grosjean

..............................................<?}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Belgium
( ( ( ( (
..............................................................

Jim Lemon wrote:
> Hi gurus,
> 
> After an unsuccessful scrabble through the documentation and Jon's 
> excellent search facility, I am no wiser as to how R recognizes an 
> incomplete command line and politely raises its hand for more. The help 
> page for parse gives no indication that it does anything more than spit 
> the dummy when fed an incomplete command line, but something in there 
> must recognize such ellipsis. Any hints?
> 
> Jim
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>


From ecoinformatics at gmail.com  Wed Jun 28 15:32:59 2006
From: ecoinformatics at gmail.com (Xiaohua Dai)
Date: Wed, 28 Jun 2006 15:32:59 +0200
Subject: [R] Help with circular statistics
In-Reply-To: <BAY104-DAV13361D041A44812D80E8BEC17F0@phx.gbl>
References: <BAY104-DAV13361D041A44812D80E8BEC17F0@phx.gbl>
Message-ID: <15f8e67d0606280632r176a2d84u523f3bd851cad6fc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060628/f44379dd/attachment.pl 

From pramod.r.anugu at jsums.edu  Wed Jun 28 15:34:20 2006
From: pramod.r.anugu at jsums.edu (Pramod Anugu)
Date: Wed, 28 Jun 2006 08:34:20 -0500
Subject: [R] Running R
Message-ID: <000301c69ab7$8fe54da0$1321848f@E72517PA>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060628/07bfecce/attachment.pl 

From afshart at exchange.sba.miami.edu  Wed Jun 28 16:06:34 2006
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Wed, 28 Jun 2006 10:06:34 -0400
Subject: [R] sapply question
Message-ID: <6BCB4D493A447546A8126F24332056E803B01617@school1.business.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060628/81748cfe/attachment.pl 

From andy_liaw at merck.com  Wed Jun 28 16:23:02 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 28 Jun 2006 10:23:02 -0400
Subject: [R] sapply question
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA028842A2@usctmx1106.merck.com>

If you attached files, none made it through the mailing list.

Can't you try posting a simplified version of the problem instead?  Try to
make it as easy for others to help you as possible will maximize the chance
you get some useful response.  Asking people to read code you've attached in
files isn't helping, especially if the code is long and make use of data not
accessible to anyone but you.

In general, if you want something like tapply, but need several variables
instead of one, you can use something like:

sapply(split(dataFrame, factor_to_split_by), function(...))

HTH,
Andy

From: Afshartous, David
> 
> sent this to the list yesterday but didn't see it listed in 
> the daily summary ... apologies if you receive it twice ...
> 
> ________________________________
> 
> From: Afshartous, David
> Sent: Tuesday, June 27, 2006 10:02 AM
> To: 'r-help at stat.math.ethz.ch'
> Subject: sapply question
> 
> 
> All:
>  
> I'm trying to use sapply to break up data within another function.
> (tapply doens't seem to work since I want to access several 
> variables of
> a 
> data set, not just break up a single variable according to a factor.)
> I tried the R documentation but that wasn't much help on this.
>  
> The first function in the attached file works, breaking up the data
> according to patient number (6 
> observations per 24 patients) via a for loop i=1,..24. 
>  
> The second function is an attempt to use sapply instead of the first
> "for loop".    
> it doesn't load properly and I seem to be absolutely blind as 
> to why; my
> guess is that
> it's something "obvious".  any help appreciated.  (necessary data
> attached)
>  
> thanks!
> dave  
>  
> ps:
> I'm on windows and,
> r.2 =100  (input to function)
> dat = comp.CAND.frm (attached file)
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From p.dalgaard at biostat.ku.dk  Wed Jun 28 16:29:06 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Jun 2006 16:29:06 +0200
Subject: [R] Running R
In-Reply-To: <000301c69ab7$8fe54da0$1321848f@E72517PA>
References: <000301c69ab7$8fe54da0$1321848f@E72517PA>
Message-ID: <x2veqlmk0d.fsf@viggo.kubism.ku.dk>

"Pramod Anugu" <pramod.r.anugu at jsums.edu> writes:

> 1. tar -zxvf R-2.3.0.tar.gz
> 2. changed the directory to the newly created directory R-2.3.0
> 3. Typed ./configure 

and what happened here??? Presumably an error since no Makefile was
created. 

> 4. Typed make
> make: *** No targets specified and no makefile found.  Stop.
> I cannot run make. Please let me know.
> #gcc -v
> Reading specs from /usr/lib/gcc/x86_64-redhat-linux/3.4.4/specs
> Configured with: ../configure --prefix=/usr --mandir=/usr/share/man
> --infodir=/usr/share/info --enable-shared --enable-threads=posix
> --disable-checking --with-system-zlib --enable-__cxa_atexit
> --disable-libunwind-exceptions --enable-java-awt=gtk
> --host=x86_64-redhat-linux
> Thread model: posix
> gcc version 3.4.4 20050721 (Red Hat 3.4.4-2)
> 
> *5. Make check
> *6. make check-all
> *7. Typed make install
> *8. Typed R
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From shireesh_s at yahoo.com  Wed Jun 28 16:31:06 2006
From: shireesh_s at yahoo.com (Shireesh Srivastava)
Date: Wed, 28 Jun 2006 07:31:06 -0700 (PDT)
Subject: [R] Help with topTable function in limma
Message-ID: <20060628143106.85191.qmail@web53211.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060628/359c10b1/attachment.pl 

From petr.pikal at precheza.cz  Wed Jun 28 16:44:16 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 28 Jun 2006 16:44:16 +0200
Subject: [R] persp/ scatterplot3d
Message-ID: <44A2B1E0.31062.202E25D@localhost>

Hi,

there was some thread recently about image, contour, persp.
from persp help page

x, y locations of grid lines at which the values in z are measured. 
These must be in **ascending order**. By default, equally spaced 
values from 0 to 1 are used. If x is a list, its components x$x and 
x$y are used for x and y, respectively. 
z a **matrix** containing the values to be plotted (NAs are allowed). 
Note that x can be used instead of z for convenience. 

so generally

persp(ascending.vector.with.nrow.length, 
ascending.vector.with.ncol.length, 
matrix.with.nrows.and.ncols)

or

persp(x.coords, y.coords, matrix.of.z.values)

is the necessary input. Something like geographical grid and value 
for each set of coordinates.

Or you can use interp function from akima package.

HTH
Petr

From: Mark Hempelmann <e.rehak_at_t-online.de> 
Date: Wed 28 Jun 2006 - 22:55:35 EST
Dear WizaRds, 

    I would like to create a 3d-plot with persp(). I sampled 17 
points with xyz-coordinates (real-life example!), representing the 
peaks of the whole plane with "zero coordinates" x=3,y=3,z=3. My 
intention is to show which entries are above or below the "zero" 
level with persp() on a nicely created grid. I also tried 
scatterplot3d(), but, alas, I am unable to tell the function that my 
points represent the peaks of the plane and are either above or below 
"normal" (whatever that means...) Please help me: 

dat<-
matrix(c(1.33,1.00,2.67,4.33,4.00,5.00,0.67,3.33,1.00,3.00,3.00,1.33,1
.67,1.67,1.33,1.67,2.33,1.67,0.67,1.00,1.33,3.33,2.67,1.67,1.67,2.67,2
.00,0.33,0.67,0.33,2.67,3.33,0.67,0.67,1.33,0.00,4.33,3.33,4.67,3.00,4
.00,5.00,4.00,3.67,1.67,3.00,3.67,3.33,1.00,1.33,0.33), ncol=3) 
colnames(dat)<-c("x","y","z") 

x=dat$x; y=dat$y, z=dat$z 
persp(x,y,z) # doesn't work at all, of course, even if I utilize 
outer() 

scatterplot3d(x,y,z) # returns a 3d scatterplot, but not the way I 
would like to see it fit. 

Thank you so much for your help and support! 

mark 

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Wed Jun 28 16:49:21 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 28 Jun 2006 16:49:21 +0200
Subject: [R] sapply question
In-Reply-To: <6BCB4D493A447546A8126F24332056E803B01617@school1.business.edu>
Message-ID: <44A2B311.1758.20789B5@localhost>

Hi

from posting guide

Technical details of posting: See General Instructions for more 
details of the following: 

No binary ***attachments*** except for PS, PDF, and some image and 
archive formats (others are automatically stripped off because they 
can contain malicious software). Files in other formats and larger 
ones should rather be put on the web and have only their URLs posted. 
This way a reader has the option to download them or not. 

So your attachement was stripped off. 

Tapply uses only one variable but you can use ?by or ?aggregate.
by gives you a list, aggregate a data frame

by(part.of.your.data.frame[,c(1,5,8:11,15)], your.column, function)

aggregate(part.of.your.data.frame[,c(1,5,8:11,15)], 
list(your.columns,  function)

HTH
Petr


On 28 Jun 2006 at 10:06, Afshartous, David wrote:

Date sent:      	Wed, 28 Jun 2006 10:06:34 -0400
From:           	"Afshartous, David" <afshart at exchange.sba.miami.edu>
To:             	"Afshartous, David" <afshart at exchange.sba.miami.edu>,
	<r-help at stat.math.ethz.ch>
Subject:        	[R] sapply question

> sent this to the list yesterday but didn't see it listed in the daily
> summary ... apologies if you receive it twice ...
> 
> ________________________________
> 
> From: Afshartous, David 
> Sent: Tuesday, June 27, 2006 10:02 AM
> To: 'r-help at stat.math.ethz.ch'
> Subject: sapply question
> 
> 
> All:
> 
> I'm trying to use sapply to break up data within another function.
> (tapply doens't seem to work since I want to access several variables
> of a data set, not just break up a single variable according to a
> factor.) I tried the R documentation but that wasn't much help on
> this.
> 
> The first function in the attached file works, breaking up the data
> according to patient number (6 observations per 24 patients) via a for
> loop i=1,..24. 
> 
> The second function is an attempt to use sapply instead of the first
> "for loop".    it doesn't load properly and I seem to be absolutely
> blind as to why; my guess is that it's something "obvious".  any help
> appreciated.  (necessary data attached)
> 
> thanks!
> dave  
> 
> ps:
> I'm on windows and,
> r.2 =100  (input to function)
> dat = comp.CAND.frm (attached file)
> 
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From harry.wills at gmail.com  Wed Jun 28 17:04:54 2006
From: harry.wills at gmail.com (harry wills)
Date: Wed, 28 Jun 2006 11:04:54 -0400
Subject: [R] lme - Random Effects Struture
Message-ID: <8ced90440606280804m4a97516eya83f625727193c40@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060628/62bf624b/attachment.pl 

From aarti_dahiya at hotmail.com  Wed Jun 28 17:17:11 2006
From: aarti_dahiya at hotmail.com (Aarti Dahiya)
Date: Wed, 28 Jun 2006 11:17:11 -0400
Subject: [R] Error: evaluation nested too deeply: infinite recursion /
	options(expressions=)?
Message-ID: <BAY106-F30F483E542BC233261E798E37F0@phx.gbl>

I am getting this error:- Error: evaluation nested too deeply: infinite 
recursion / options(expressions=)? The reason is probbaly because I am 
calling methods within methods.

I read related posts.  It said the solution is to set  options(expressions = 
1000). I wanted to know where in my code or system to set 
options(expressions = 1000)?

Thanks.

Aarti


From amurta at ipimar.pt  Thu Jun 29 01:43:27 2006
From: amurta at ipimar.pt (Alberto Murta)
Date: Wed, 28 Jun 2006 16:43:27 -0700
Subject: [R] Very slow read.table on Linux,
	compared to Win2000 [Broad cast]
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02884265@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02884265@usctmx1106.merck.com>
Message-ID: <200606281643.29459.amurta@ipimar.pt>

I have a pentium 4 pc with 256 MB of RAM, so I made a text file, tab 
separated, with column names and 15000 x 483 integers:

> system("ls -l teste.txt")
-rw-r--r--  1 amurta  amurta  16998702 Jun 28 16:08 teste.txt

the time it took to import it was around 15 secs:

> system.time(teste <- read.delim("teste.txt"))
[1] 15.349  0.244 16.597  0.000  0.000

so I think lack of RAM must not be the main problem.
Cheers

Alberto


> version
               _
platform       i386-unknown-freebsd6.1
arch           i386
os             freebsd6.1
system         i386, freebsd6.1
status
major          2
minor          3.1
year           2006
month          06
day            01
svn rev        38247
language       R
version.string Version 2.3.1 (2006-06-01)



On Wednesday 28 June 2006 05:43, Liaw, Andy wrote:
> From: Peter Dalgaard
>
> > <davidek at zla-ryba.cz> writes:
> > > Dear all,
> > >
> > > I read.table a 17MB tabulator separated table with 483
> > > variables(mostly numeric) and 15000 observations into R.
> >
> > This takes a
> >
> > > few seconds with R 2.3.1 on windows 2000, but it takes
> >
> > several minutes
> >
> > > on my Linux machine. The linux machine is Ubuntu 6.06, 256 MR RAM,
> > > Athlon 1600 processor. The windows hardware is better
> >
> > (Pentium 4, 512 RAM), but it shouldn't make such a difference.
> >
> > > The strange thing is that even doing something with the data(say a
> > > histogram of a variable, or transforming integers into a factor)
> > > takes really long time on the linux box and the computer
> >
> > seems to work
> >
> > > extensively with the hard disk.
> > > Could this be caused by swapping ? Can I increase the
> >
> > memory allocated to R somehow ?
> >
> > > I have checked the manual, but the memory options allowed for linux
> > > don't seem to help me (I may be doing it wrong, though ...)
> > >
> > > The code I run:
> > >
> > > TBO <-
> >
> > read.table(file="TBO.dat",sep="\t",header=TRUE,dec=",");   #
> > this takes forever
> >
> > > TBO$sexe<-factor(TBO$sexe,labels=c("man","vrouw"));   #
> >
> > even this takes like 30 seconds, compared
> >
> > > to nothing on Win2000
> > >
> > > I'd be grateful for any suggestions,
> >
> > Almost surely, the fix is to insert more RAM chips. 256 MB
> > leaves you very little space for actual work these days, and
>
> Try running Windows on the 256MB box and you'll see why Peter recommended
> the above.  Consider yourself lucky that R actually still does something
> useful under Unbuntu with so little RAM.  If adding more RAM is not an
> option, perhaps not running X altogether would help.
>
> Andy
>
> > a 17MB file will get expanded to several times the original
> > size during reading and data manipulations. Using a
> > lightweight window manager can help, but you usually regret
> > the switch for other reasons.
> >
> >
> > --
> >    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
> >   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
> >  (*) \(*) -- University of Copenhagen   Denmark          Ph:
> > (+45) 35327918
> > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX:
> > (+45) 35327907
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
  Alberto G. Murta
IPIMAR - Institute of Fisheries and Marine Research 
Avenida de Brasilia; 1449-006 Lisboa; Portugal
Tel: +351 213027120; Fax: +351 213015948


From sridharshalini at hotmail.com  Wed Jun 28 17:43:21 2006
From: sridharshalini at hotmail.com (Shalini Sridhar)
Date: Wed, 28 Jun 2006 15:43:21 +0000
Subject: [R] Help on hclust and viewing the results as PDF and JPEG please
Message-ID: <BAY103-F211E60AC9E4DE84CEB9B69C07F0@phx.gbl>


From noah78 at web.de  Wed Jun 28 17:48:00 2006
From: noah78 at web.de (=?iso-8859-15?Q?J=F6rn_Schulz?=)
Date: Wed, 28 Jun 2006 17:48:00 +0200
Subject: [R] read file with readBin (the file was saved  with a C-routine)
Message-ID: <1827873593@web.de>

Hello!

I have problems with using of "readBin" to read files, which was written in C with "fwrite". In the C-File there is the following Code:

fwrite(MyitINI,sizeof(itINItype),1,outfile);

where MyitINI is a structure of the following form

typedef struct{
 int  KernelFileSave;      /* Determined, if Systemmatrix saved or not.*/
 char KernelFileName[200]; /* A-Matrix name                            */
 char StartFileName[200];  /* Startguess (optional)                    */
 int XSamples;             /* No of samples on 1. axis of recon image  */
 int YSamples;             /* No of samples on 2. axis of recon image  */
 float DeltaX;             /* Sampling distance 1. axis of recon image */
 float DeltaY;             /* Sampling distance 2. axis of recon image */
 float Xmin;               /* 1. sample position 1.axis of recon image */
 float Ymin;               /* 1. sample position 2.axis of recon image */
 }  itINItype;

I thought the following use of "readBin" sould it do, but it doesn't

KernelFileSave <- readBin( con, integer(), n=1, size=4 )
KernelFileName <- readBin( con, character(), n=1 )
StartFileName <- readBin( con, character(), n=1 )
XSamples <- readBin( con, integer(), n=1, size=4 )
YSamples <- readBin( con, integer(), n=1, size=4 )
DeltaX <- readBin( con, numeric(), n=1, size=4 )
DeltaY <- readBin( con, numeric(), n=1, size=4 )
Xmin <- readBin( con, numeric(), n=1, size=4 )
Ymin <- readBin( con, numeric(), n=1, size=4 )

I think there is a problem if I read the character. Have you any ideas ???

Thanks for help.
J?rn Schulz.
_____________________________________________________________________
Der WEB.DE SmartSurfer hilft bis zu 70% Ihrer Onlinekosten zu sparen!


From rab45+ at pitt.edu  Wed Jun 28 18:05:48 2006
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Wed, 28 Jun 2006 12:05:48 -0400
Subject: [R] lme - Random Effects Struture
In-Reply-To: <8ced90440606280804m4a97516eya83f625727193c40@mail.gmail.com>
References: <8ced90440606280804m4a97516eya83f625727193c40@mail.gmail.com>
Message-ID: <1151510748.3462.3.camel@localhost.localdomain>

On Wed, 2006-06-28 at 11:04 -0400, harry wills wrote:
> Thanks for the help Dimitris,
> 
> However I still have a question, this time I'll be more specific,
> 
> the following is my SAS code
> 
> 
> 
> proc mixed data=Reg;
>         class ID;
>         model y=Time Time*x1 Time*x2 Time*x3 /S;
>         random intercept Time /S type=UN subject=ID G GCORR V;
>         repeated /subject = ID R RCORR;
> run;    **
> 
> (Type =UN for random effects)
> 
> 
> 
> The eqivalent lme statement I am using is :
> 
> reglme  <- lme(y ~ Time+Time*x1+Time*x2+Time*x3, data=Reg, random = ~ Time |
> ID)
> 
> 
> 
> When I compare the results, the values differ by considerable margin; I
> suppose this is due to the Random effects covariance structure. R output
> tells me that the structure is
> 
> 
> 
> "Structure: General positive-definite, Log-Cholesky parametrization"
> 
> 
> 
> Hence the problem for me is how to control this structure in R. Any help
> would appreciated
> 
> Thanks
> 
> Harry

>From my understanding of SAS, a*b means the interaction of a and b. But
in R, a*b is shorthand for a + b + a:b where a:b is the interaction
term. The way you've written the lme formula, you have time showing up 4
times plus you have additional main effects x1, x2, and x3. Is this what
you want? Maybe I'm wrong but I don't think the SAS code and the R code
represent the same model.

Rick B.


From assampryseley at yahoo.com  Wed Jun 28 18:18:27 2006
From: assampryseley at yahoo.com (Pryseley Assam)
Date: Wed, 28 Jun 2006 09:18:27 -0700 (PDT)
Subject: [R] lme convergence
Message-ID: <20060628161827.24795.qmail@web37111.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060628/ff22e359/attachment.pl 

From beperron at wustl.edu  Wed Jun 28 18:25:29 2006
From: beperron at wustl.edu (Brian Perron)
Date: Wed, 28 Jun 2006 11:25:29 -0500
Subject: [R] lme4 - higher level
Message-ID: <44A2AD79.4050100@wustl.edu>

Hello all,

I just started working with the lme4 package to estimate a multilevel 
logistic regression and am planning to use this package for a 
cross-classification / multiple-membership model.  I haven't found many 
worked examples and am trying to figure out how to add variables to the 
higher-level part of the model.  Consider the following example:

test.1 <- lmer(y ~ b1 + b2 + b3 + (1 | group), org.data)

This model shows a simple two-level nesting pattern -- for example, 
persons nested in groups.  If I have data describing the groups, such as 
the age or accreditation status of the group (or both), how would I 
include those variables in the model? 

I found a very nice description of lme4 by Bates describing the package 
in R News.  Is anybody aware of any other examples or resources that 
provide worked examples preferably with annotated results?

Thanks,
Brian


From gerifalte28 at hotmail.com  Wed Jun 28 18:34:49 2006
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Wed, 28 Jun 2006 16:34:49 +0000
Subject: [R] distribution of daily rainfall values in binned categories
In-Reply-To: <17570.16478.127762.160753@stat.math.ethz.ch>
Message-ID: <BAY103-F12EACC3EA3F885A0262ED8A67F0@phx.gbl>

Hi Martin

I agree with all your previous concerns.  I was just answering her question 
about visualizing frequencies for a continuous variable that is artificially 
categorized.  However, she did mention the word *distribution* (a part that 
I obviously ignored when I posted my answer) so your comments are more than 
appropriate. I am surprised nobody else jumped with the usual discussion 
about violin plots and his friends   ;-)

Cheers

Francisco



Dr. Francisco J. Zagmutt
College of Veterinary Medicine and Biomedical Sciences
Colorado State University




>From: Martin Maechler <maechler at stat.math.ethz.ch>
>Reply-To: Martin Maechler <maechler at stat.math.ethz.ch>
>To: "Francisco J. Zagmutt" <gerifalte28 at hotmail.com>
>CC: etiennesky at yahoo.com, r-help at stat.math.ethz.ch
>Subject: Re: [R] distribution of daily rainfall values in binned categories
>Date: Wed, 28 Jun 2006 10:39:58 +0200
>
> >>>>> "FJZ" == Francisco J Zagmutt <gerifalte28 at hotmail.com>
> >>>>>     on Wed, 28 Jun 2006 03:51:31 +0000 writes:
>
>     FJZ> Hi Etienne,
>     FJZ> Somebody asked a somehow related question recently.
>     FJZ> http://tolstoy.newcastle.edu.au/R/help/06/06/29485.html
>
>     FJZ> Take a look at cut? table? and barplot?
>     FJZ> i.e.
>
>       # Creates fake data from uniform(0,30)
>       set.seed(1) ## <<- added by MM
>       x=runif(50, 0,30)
>
>       # Creates categories
>       rain=cut(x,breaks=c( 0, 1,2.5,5, 10, 20, Inf))
>
>       # Creates contingency table of categories
>       tab=table(rain)
>
>       # Plots frequencies of rainfall
>       barplot(tab)
>
>
>No, no, no!  Do not confuse histograms with bar plots!
>
>-  barplot() is {one possibility} for visualizing discrete
>    ("categorical", "factor") data,
>-  hist() is for visualizing *continuous* data  (*)
>
>As Jim Porzak replied, do use hist(): the example really is a matter
>of visualization of a continuous distribution which should *not*
>be done by a barplot.  Instead, e.g.,
>
>   hist(x, breaks = c(0, 1,2.5,5, 10,20, max(pretty(max(x)))),
>        freq = TRUE, col = "gray")
>
>will give a graphic similar to the above --- BUT also
>warns you about the hidden deception (aka sillyness) of *both* graphics:
>Namely, the above hist() call warns you with
>
> >> Warning message:
> >> the AREAS in the plot are wrong -- rather use freq=FALSE in: ....
>
>and finally,
>
>   hist(x, breaks = c(0, 1,2.5,5, 10,20, max(pretty(max(x)))), col="gray")
>
>gives you a more honest graphic --- which -- for the runif()
>example -- may finally lead to you to realize that using unequal
>break may really not be such a good idea.
>Note however that for the OP rainfall data, that may well be different
>and if I look at rainfall data, I find I would rather view
>
>    hist(log10( <rainfall> ))
>or then
>    plot(density( log10( <rainfall> ) ))
>
>Martin Maechler, ETH Zurich
>
>(*) From statistical point of view, histograms just density estimators,
>     and -- as known for a while -- have quite some drawbacks.
>     Hence they should nowadays often be replaced by
>         plot(density(.), ..)
>
>
>     >> From: etienne <etiennesky at yahoo.com>
>     >> To: r-help at stat.math.ethz.ch
>     >> Subject: [R] distribution of daily rainfall values in binned 
>categories
>     >> Date: Tue, 27 Jun 2006 11:28:59 -0700 (PDT)
>     >>
>     >> Hi,
>     >>
>     >> I'm a newbie in using R and I would like to have a few
>     >> clues as to how I could compute and plot a
>     >> distribution of daily rainfall intensity in different
>     >> categories.  I have daily values (mm/day) for several
>     >> years and I need to show the frequency of 0-1, 1-2.5,
>     >> 2.5-5, 5-10, 10-20, 20+ mm/day.  Can this be done
>     >> easily?
>     >>
>     >> Thanks,
>     >> Etienne
>     >>
>     >> ______________________________________________
>     >> R-help at stat.math.ethz.ch mailing list
>     >> https://stat.ethz.ch/mailman/listinfo/r-help
>     >> PLEASE do read the posting guide!
>     >> http://www.R-project.org/posting-guide.html
>
>     FJZ> ______________________________________________
>     FJZ> R-help at stat.math.ethz.ch mailing list
>     FJZ> https://stat.ethz.ch/mailman/listinfo/r-help
>     FJZ> PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html


From murdoch at stats.uwo.ca  Wed Jun 28 18:37:40 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 28 Jun 2006 12:37:40 -0400
Subject: [R] read file with readBin (the file was saved with a C-routine)
In-Reply-To: <1827873593@web.de>
References: <1827873593@web.de>
Message-ID: <44A2B054.9000703@stats.uwo.ca>

On 6/28/2006 11:48 AM, J?rn Schulz wrote:
> Hello!
> 
> I have problems with using of "readBin" to read files, which was written in C with "fwrite". In the C-File there is the following Code:
> 
> fwrite(MyitINI,sizeof(itINItype),1,outfile);
> 
> where MyitINI is a structure of the following form
> 
> typedef struct{
>  int  KernelFileSave;      /* Determined, if Systemmatrix saved or not.*/
>  char KernelFileName[200]; /* A-Matrix name                            */
>  char StartFileName[200];  /* Startguess (optional)                    */
>  int XSamples;             /* No of samples on 1. axis of recon image  */
>  int YSamples;             /* No of samples on 2. axis of recon image  */
>  float DeltaX;             /* Sampling distance 1. axis of recon image */
>  float DeltaY;             /* Sampling distance 2. axis of recon image */
>  float Xmin;               /* 1. sample position 1.axis of recon image */
>  float Ymin;               /* 1. sample position 2.axis of recon image */
>  }  itINItype;
> 
> I thought the following use of "readBin" sould it do, but it doesn't
> 
> KernelFileSave <- readBin( con, integer(), n=1, size=4 )
> KernelFileName <- readBin( con, character(), n=1 )

You've written out 200 characters, but are only reading up to the first 
null.  I think you want readChar here, or you want to change the way you 
write the struct.

> StartFileName <- readBin( con, character(), n=1 )

Same here.

> XSamples <- readBin( con, integer(), n=1, size=4 )
> YSamples <- readBin( con, integer(), n=1, size=4 )
> DeltaX <- readBin( con, numeric(), n=1, size=4 )
> DeltaY <- readBin( con, numeric(), n=1, size=4 )
> Xmin <- readBin( con, numeric(), n=1, size=4 )
> Ymin <- readBin( con, numeric(), n=1, size=4 )
> 
> I think there is a problem if I read the character. Have you any ideas ???
> 
> Thanks for help.
> J?rn Schulz.

I would also worry about the alignment of the fields within the struct, 
though it is probably okay given that everything is a multiple of 4 
bytes long.

Duncan Murdoch


From ripley at stats.ox.ac.uk  Wed Jun 28 18:43:11 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 28 Jun 2006 17:43:11 +0100 (BST)
Subject: [R] read file with readBin (the file was saved with a C-routine)
In-Reply-To: <1827873593@web.de>
References: <1827873593@web.de>
Message-ID: <Pine.LNX.4.64.0606281741550.28605@gannet.stats.ox.ac.uk>

On Wed, 28 Jun 2006, J?rn Schulz wrote:

> Hello!
>
> I have problems with using of "readBin" to read files, which was written in C with "fwrite". In the C-File there is the following Code:
>
> fwrite(MyitINI,sizeof(itINItype),1,outfile);
>
> where MyitINI is a structure of the following form
>
> typedef struct{
> int  KernelFileSave;      /* Determined, if Systemmatrix saved or not.*/
> char KernelFileName[200]; /* A-Matrix name                            */
> char StartFileName[200];  /* Startguess (optional)                    */
> int XSamples;             /* No of samples on 1. axis of recon image  */
> int YSamples;             /* No of samples on 2. axis of recon image  */
> float DeltaX;             /* Sampling distance 1. axis of recon image */
> float DeltaY;             /* Sampling distance 2. axis of recon image */
> float Xmin;               /* 1. sample position 1.axis of recon image */
> float Ymin;               /* 1. sample position 2.axis of recon image */
> }  itINItype;
>
> I thought the following use of "readBin" sould it do, but it doesn't
>
> KernelFileSave <- readBin( con, integer(), n=1, size=4 )
> KernelFileName <- readBin( con, character(), n=1 )
> StartFileName <- readBin( con, character(), n=1 )
> XSamples <- readBin( con, integer(), n=1, size=4 )
> YSamples <- readBin( con, integer(), n=1, size=4 )
> DeltaX <- readBin( con, numeric(), n=1, size=4 )
> DeltaY <- readBin( con, numeric(), n=1, size=4 )
> Xmin <- readBin( con, numeric(), n=1, size=4 )
> Ymin <- readBin( con, numeric(), n=1, size=4 )
>
> I think there is a problem if I read the character. Have you any ideas ???

The help page points you to readChar, which seems more appropriate: you 
could also use raw().

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From DSmith2 at dhs.ca.gov  Wed Jun 28 18:44:39 2006
From: DSmith2 at dhs.ca.gov (Smith, Daniel (DHS-DEODC-EHIB))
Date: Wed, 28 Jun 2006 09:44:39 -0700
Subject: [R] compare odds ratios
Message-ID: <3144C4F831806648B4FD0BA612193859029237EF@dhsexcmsg05.intra.dhs.ca.gov>

You could try a test for heterogeneity of the odds ratios, usually part
of a Mantel-Haenszel analysis, with each test as a stratum.  Also, the
field of meta-analysis has tests for heterogeneity of estimates.  Both
are covered in Rothman and Greenland's Modern Epidemiology (1998) text,
Chapters 15 and 32 respectively.  

Daniel Smith
Environmental Health Investigations Branch
California Department of Health Services


> -----Original Message-----
> 
> Message: 43
> Date: Tue, 27 Jun 2006 11:22:14 -0700 (PDT)
> From: array chip <arrayprofile at yahoo.com>
> Subject: Re: [R] compare odds ratios
> To: r-help at stat.math.ethz.ch
> Message-ID: <20060627182214.88643.qmail at web35709.mail.mud.yahoo.com>
> Content-Type: text/plain; charset=iso-8859-1
> 
> Hi dear all, I haven't heard any suggestions on how to
> tackle the problem in my previous email yet. I
> searched on google and was not getting any useful
> information yet. I did get someone from google groups
> suggesting Cockran Mantel Haenszel test with each
> subject as the stratum. But as far as I understand,
> CMH test is to test whether the common odds ratio
> (assuming odds ratios across stratum are equal) is
> equal to 1, not really the question I was asking which
> was whether the 2 odds ratios were equal (doesn't
> matter if they are equal to 1). Also, someone
> suggested loglinear regression, but I am not sure
> either how to set things up for my problem.
> 
> One clarification to my original email: the 2
> diagnostic tests were performed on the set set of
> patients, the issue here how to test whether the 2
> odds ratios for the 2 diagnostic tests are equal.
> 
> Here is a hypothetical dataset, for example:
> 
> dat<-
>
cbind(disease=sample(c(rep(1,15),rep(0,20))),test1=sample(c(rep(1,11),re
p(
> 0,24))),test2=sample(c(rep(1,14),rep(0,21))))
> 
> Hope some statistical experts would guide me some
> directions. Many thanks
> 
> **************************************


From rvaradhan at jhmi.edu  Wed Jun 28 18:52:19 2006
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Wed, 28 Jun 2006 12:52:19 -0400
Subject: [R] lme convergence
In-Reply-To: <20060628161827.24795.qmail@web37111.mail.mud.yahoo.com>
Message-ID: <002001c69ad3$38073050$7c94100a@win.ad.jhu.edu>

Use "try" to capture error messages without breaking the loop.
?try

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
Webpage: http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html 
--------------------------------------------------------------------------

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Pryseley Assam
> Sent: Wednesday, June 28, 2006 12:18 PM
> To: R-Users
> Subject: [R] lme convergence
> 
> Dear R-Users,
> 
>   Is it possible to get the covariance matrix from an lme model that did
> not converge ?
> 
>   I am doing a simulation which entails fitting linear mixed models, using
> a "for loop".
>   Within each loop, i generate a new data set and analyze it using a mixed
> model.  The loop stops When the "lme function" does not converge for a
> simulated dataset. I want to inquire if there is a method to suppress the
> error message from the lme function, or better still, a way of going about
> this issue of the loop ending once the lme function does not converge.
> 
>   Thanks in advance,
>   Pryseley
> 
> 
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html


From HDoran at air.org  Wed Jun 28 18:56:56 2006
From: HDoran at air.org (Doran, Harold)
Date: Wed, 28 Jun 2006 12:56:56 -0400
Subject: [R] lme4 - higher level
References: <44A2AD79.4050100@wustl.edu>
Message-ID: <2323A6D37908A847A7C32F1E3662C80E04E56E@dc1ex01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060628/7d97c71b/attachment.pl 

From chrysopa at gmail.com  Wed Jun 28 19:25:42 2006
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Wed, 28 Jun 2006 14:25:42 -0300
Subject: [R] calculating the spacial autocorrelation for poisson data
Message-ID: <181ecf6a0606281025w444a1c8exd5dffce83f0eaecb@mail.gmail.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060628/2ff7714d/attachment.pl 

From afshart at exchange.sba.miami.edu  Wed Jun 28 19:37:33 2006
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Wed, 28 Jun 2006 13:37:33 -0400
Subject: [R] sapply question
Message-ID: <6BCB4D493A447546A8126F24332056E803B01697@school1.business.edu>


Andy,
My apologies for my very poor email.   Below is some simplified code.
I think my confusion has more to do w/ embedding the sapplpy within
another function and 
returning the result, thus this would apply to your "split" suggestion
as well.
However, when I run this the result is still empty.
thanks!
dave


creatine.function.new.3 = function(dat) {
result =  sapply(levels(dat$Patient_no), function(i) 
	{
		subdat <- subset(dat, dat$Patient_no==i)
		U = subdat$UV		 
		r = numeric(6)   
		for (i in 1:6) {		
			r[i] = 2*U[i]
		}
       r  ## results for each patient
       }
       )
result
}
 

-----Original Message-----
From: Liaw, Andy [mailto:andy_liaw at merck.com] 
Sent: Wednesday, June 28, 2006 10:23 AM
To: Afshartous, David; r-help at stat.math.ethz.ch
Subject: RE: [R] sapply question

If you attached files, none made it through the mailing list.

Can't you try posting a simplified version of the problem instead?  Try
to make it as easy for others to help you as possible will maximize the
chance you get some useful response.  Asking people to read code you've
attached in files isn't helping, especially if the code is long and make
use of data not accessible to anyone but you.

In general, if you want something like tapply, but need several
variables instead of one, you can use something like:

sapply(split(dataFrame, factor_to_split_by), function(...))

HTH,
Andy

From: Afshartous, David
> 
> sent this to the list yesterday but didn't see it listed in the daily 
> summary ... apologies if you receive it twice ...
> 
> ________________________________
> 
> From: Afshartous, David
> Sent: Tuesday, June 27, 2006 10:02 AM
> To: 'r-help at stat.math.ethz.ch'
> Subject: sapply question
> 
> 
> All:
>  
> I'm trying to use sapply to break up data within another function.
> (tapply doens't seem to work since I want to access several variables 
> of a data set, not just break up a single variable according to a 
> factor.) I tried the R documentation but that wasn't much help on 
> this.
>  
> The first function in the attached file works, breaking up the data 
> according to patient number (6 observations per 24 patients) via a for

> loop i=1,..24.
>  
> The second function is an attempt to use sapply instead of the first
> "for loop".    
> it doesn't load properly and I seem to be absolutely blind as to why; 
> my guess is that it's something "obvious".  any help appreciated.  
> (necessary data
> attached)
>  
> thanks!
> dave
>  
> ps:
> I'm on windows and,
> r.2 =100  (input to function)
> dat = comp.CAND.frm (attached file)
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------
------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From ivowel at gmail.com  Wed Jun 28 19:49:12 2006
From: ivowel at gmail.com (ivo welch)
Date: Wed, 28 Jun 2006 13:49:12 -0400
Subject: [R] y-axis label location relative to tick label width?
In-Reply-To: <50d1c22d0606150827k61431890r1fc02e31e517cd1b@mail.gmail.com>
References: <50d1c22d0606150827k61431890r1fc02e31e517cd1b@mail.gmail.com>
Message-ID: <50d1c22d0606281049u592fee89lddab5839e0a54b19@mail.gmail.com>

Dear R-Wizards:  does anyone know how to obtain the width of the
widest y- or x-tick label that R has drawn, so that I can leave
appropriate space between it and my overall axis text label
appropriately?

regards, /ivo welch


From murdoch at stats.uwo.ca  Wed Jun 28 19:55:55 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 28 Jun 2006 13:55:55 -0400
Subject: [R] y-axis label location relative to tick label width?
In-Reply-To: <50d1c22d0606281049u592fee89lddab5839e0a54b19@mail.gmail.com>
References: <50d1c22d0606150827k61431890r1fc02e31e517cd1b@mail.gmail.com>
	<50d1c22d0606281049u592fee89lddab5839e0a54b19@mail.gmail.com>
Message-ID: <44A2C2AB.3050304@stats.uwo.ca>

On 6/28/2006 1:49 PM, ivo welch wrote:
> Dear R-Wizards:  does anyone know how to obtain the width of the
> widest y- or x-tick label that R has drawn, so that I can leave
> appropriate space between it and my overall axis text label
> appropriately?

Probably the easiest way is to draw them yourself.  That is, draw the 
plot with axes=FALSE, then draw the axes using the axis() function, 
using pretty() to calculate the tick locations.

Duncan Murdoch


From bill.shipley at usherbrooke.ca  Wed Jun 28 20:09:02 2006
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Wed, 28 Jun 2006 14:09:02 -0400
Subject: [R] superimposing histograms
Message-ID: <004001c69add$f0609830$b61ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060628/f8463b47/attachment.pl 

From andy_liaw at merck.com  Wed Jun 28 20:14:30 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 28 Jun 2006 14:14:30 -0400
Subject: [R] sapply question
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02884322@usctmx1106.merck.com>

Here's an example, using a data frame with two variables plus a grouping
factor:

> (dat <- data.frame(x1=1:10, x2=10:1, g=gl(2, 5)))
   x1 x2 g
1   1 10 1
2   2  9 1
3   3  8 1
4   4  7 1
5   5  6 1
6   6  5 2
7   7  4 2
8   8  3 2
9   9  2 2
10 10  1 2

Now, I want to compute 2 * x1 + x2 for each group in g:

> sapply(split(dat, dat$g), function(d) 2 * d$x1 + d$x2)
      1  2
[1,] 12 17
[2,] 13 18
[3,] 14 19
[4,] 15 20
[5,] 16 21

Does that help?

Andy 

From: Afshartous, David
> 
> Andy,
> My apologies for my very poor email.   Below is some simplified code.
> I think my confusion has more to do w/ embedding the sapplpy 
> within another function and returning the result, thus this 
> would apply to your "split" suggestion as well.
> However, when I run this the result is still empty.
> thanks!
> dave
> 
> 
> creatine.function.new.3 = function(dat) {
> result =  sapply(levels(dat$Patient_no), function(i) 
> 	{
> 		subdat <- subset(dat, dat$Patient_no==i)
> 		U = subdat$UV		 
> 		r = numeric(6)   
> 		for (i in 1:6) {		
> 			r[i] = 2*U[i]
> 		}
>        r  ## results for each patient
>        }
>        )
> result
> }
>  
> 
> -----Original Message-----
> From: Liaw, Andy [mailto:andy_liaw at merck.com] 
> Sent: Wednesday, June 28, 2006 10:23 AM
> To: Afshartous, David; r-help at stat.math.ethz.ch
> Subject: RE: [R] sapply question
> 
> If you attached files, none made it through the mailing list.
> 
> Can't you try posting a simplified version of the problem 
> instead?  Try
> to make it as easy for others to help you as possible will 
> maximize the
> chance you get some useful response.  Asking people to read 
> code you've
> attached in files isn't helping, especially if the code is 
> long and make
> use of data not accessible to anyone but you.
> 
> In general, if you want something like tapply, but need several
> variables instead of one, you can use something like:
> 
> sapply(split(dataFrame, factor_to_split_by), function(...))
> 
> HTH,
> Andy
> 
> From: Afshartous, David
> > 
> > sent this to the list yesterday but didn't see it listed in 
> the daily 
> > summary ... apologies if you receive it twice ...
> > 
> > ________________________________
> > 
> > From: Afshartous, David
> > Sent: Tuesday, June 27, 2006 10:02 AM
> > To: 'r-help at stat.math.ethz.ch'
> > Subject: sapply question
> > 
> > 
> > All:
> >  
> > I'm trying to use sapply to break up data within another function.
> > (tapply doens't seem to work since I want to access several 
> variables 
> > of a data set, not just break up a single variable according to a 
> > factor.) I tried the R documentation but that wasn't much help on 
> > this.
> >  
> > The first function in the attached file works, breaking up the data 
> > according to patient number (6 observations per 24 
> patients) via a for
> 
> > loop i=1,..24.
> >  
> > The second function is an attempt to use sapply instead of the first
> > "for loop".    
> > it doesn't load properly and I seem to be absolutely blind 
> as to why; 
> > my guess is that it's something "obvious".  any help appreciated.  
> > (necessary data
> > attached)
> >  
> > thanks!
> > dave
> >  
> > ps:
> > I'm on windows and,
> > r.2 =100  (input to function)
> > dat = comp.CAND.frm (attached file)
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
> --------------------------------------------------------------
> ----------
> ------
> Notice:  This e-mail message, together with any attachment...{{dropped}}


From ggrothendieck at gmail.com  Wed Jun 28 20:19:29 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 28 Jun 2006 14:19:29 -0400
Subject: [R] superimposing histograms
In-Reply-To: <004001c69add$f0609830$b61ad284@BIO041>
References: <004001c69add$f0609830$b61ad284@BIO041>
Message-ID: <971536df0606281119rfe744dbucc9f115a42229a1@mail.gmail.com>

See:

http://www.medepi.net/data/wnv/index.html

On 6/28/06, Bill Shipley <bill.shipley at usherbrooke.ca> wrote:
> I want to superimpose histograms from three populations onto the same graph,
> changing the shading of the bars for each population.  After consulting the
> help files and the archives I cannot find out how to do this (seemly) simple
> graph.  To be clear, I want
>
> - a single x axis (from -3 to 18)
> - three groups of bars forming the histograms of each population (they will
> not overlap much, but this is a detail)
> - the bars from each histogram having different shadings or other visually
> distinguishing features.
>
> Can anyone explain how to do this?
>
> Thanks.
>
>
> Bill Shipley
>
>
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From paulojus at est.ufpr.br  Wed Jun 28 20:32:21 2006
From: paulojus at est.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Wed, 28 Jun 2006 15:32:21 -0300 (BRT)
Subject: [R] calculating the spacial autocorrelation for poisson data
In-Reply-To: <181ecf6a0606281025w444a1c8exd5dffce83f0eaecb@mail.gmail.com>
References: <181ecf6a0606281025w444a1c8exd5dffce83f0eaecb@mail.gmail.com>
Message-ID: <Pine.LNX.4.63.0606281528240.24247@est.ufpr.br>


Hi Ronaldo

As an exploratory analysis you can fit a glm to the data and
use variog() on the residuals to check if there is an spatial correlation.

The approach in geoRglm is more compreheensive and you can fit a 
Poisson model with spatially correlated random effects using MCMC maximum 
likelihood with likfit.glsm()

There are some examples in the geoRglm web page
http://www.daimi.au.dk/~olefc/geoRglm/

best
P.J.



Paulo Justiniano Ribeiro Jr
LEG (Laborat?rio de Estat?stica e Geoinforma??o)
Departamento de Estat?stica
Universidade Federal do Paran?
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 3361 3573
Fax: (+55) 41 3361 3141
e-mail: paulojus at est.ufpr.br
http://www.est.ufpr.br/~paulojus

On Wed, 28 Jun 2006, Ronaldo Reis-Jr. wrote:

> Hi,
>
> I have some count data. I try to calculate if exist spacial
> auto-correlation. In geoR I make:
>
> variog.p50 <- variog(p50,uvec=c(1:10))
>
> but I think that the correct is using geoRglm
>
> I try:
>
> covariog.p50 <- covariog(p50,uvec=c(1:10))
>
> But it is not the same  calculation.
>
> I new in geostatistic, I'm stunding.
>
> The semivariogram I know how to interpret the output and graphics, but dont
> with covariance.
>
> Any help are welcome
>
> Thanks
> Ronaldo
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

From harry.wills at gmail.com  Wed Jun 28 20:46:07 2006
From: harry.wills at gmail.com (harry wills)
Date: Wed, 28 Jun 2006 14:46:07 -0400
Subject: [R] lme - Random Effects Struture
In-Reply-To: <1151510748.3462.3.camel@localhost.localdomain>
References: <8ced90440606280804m4a97516eya83f625727193c40@mail.gmail.com>
	<1151510748.3462.3.camel@localhost.localdomain>
Message-ID: <8ced90440606281146q208557bah65efc2c2ca373c0c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060628/41bbe9f4/attachment.pl 

From jcurole at usc.edu  Wed Jun 28 21:04:15 2006
From: jcurole at usc.edu (Jason Curole)
Date: Wed, 28 Jun 2006 12:04:15 -0700
Subject: [R] y-axis break with lattice graphics
Message-ID: <BA6A0A1B-E5B5-41FF-8EFB-A9BF62546D7D@usc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060628/6d74eb22/attachment.pl 

From afshart at exchange.sba.miami.edu  Wed Jun 28 21:34:11 2006
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Wed, 28 Jun 2006 15:34:11 -0400
Subject: [R] sapply question
Message-ID: <6BCB4D493A447546A8126F24332056E803B016E1@school1.business.edu>

Andy,

That helps, but in my function below I want to perform 
several calculations before returning a final calculation, hence the 
{}, and I think that's part of the problem.

cheers,

dave
 

-----Original Message-----
From: Liaw, Andy [mailto:andy_liaw at merck.com] 
Sent: Wednesday, June 28, 2006 2:15 PM
To: Afshartous, David; r-help at stat.math.ethz.ch
Subject: RE: [R] sapply question

Here's an example, using a data frame with two variables plus a grouping
factor:

> (dat <- data.frame(x1=1:10, x2=10:1, g=gl(2, 5)))
   x1 x2 g
1   1 10 1
2   2  9 1
3   3  8 1
4   4  7 1
5   5  6 1
6   6  5 2
7   7  4 2
8   8  3 2
9   9  2 2
10 10  1 2

Now, I want to compute 2 * x1 + x2 for each group in g:

> sapply(split(dat, dat$g), function(d) 2 * d$x1 + d$x2)
      1  2
[1,] 12 17
[2,] 13 18
[3,] 14 19
[4,] 15 20
[5,] 16 21

Does that help?

Andy 

From: Afshartous, David
> 
> Andy,
> My apologies for my very poor email.   Below is some simplified code.
> I think my confusion has more to do w/ embedding the sapplpy within 
> another function and returning the result, thus this would apply to 
> your "split" suggestion as well.
> However, when I run this the result is still empty.
> thanks!
> dave
> 
> 
> creatine.function.new.3 = function(dat) { result =  
> sapply(levels(dat$Patient_no), function(i)
> 	{
> 		subdat <- subset(dat, dat$Patient_no==i)
> 		U = subdat$UV		 
> 		r = numeric(6)   
> 		for (i in 1:6) {		
> 			r[i] = 2*U[i]
> 		}
>        r  ## results for each patient
>        }
>        )
> result
> }
>  
> 
> -----Original Message-----
> From: Liaw, Andy [mailto:andy_liaw at merck.com]
> Sent: Wednesday, June 28, 2006 10:23 AM
> To: Afshartous, David; r-help at stat.math.ethz.ch
> Subject: RE: [R] sapply question
> 
> If you attached files, none made it through the mailing list.
> 
> Can't you try posting a simplified version of the problem instead?  
> Try to make it as easy for others to help you as possible will 
> maximize the chance you get some useful response.  Asking people to 
> read code you've attached in files isn't helping, especially if the 
> code is long and make use of data not accessible to anyone but you.
> 
> In general, if you want something like tapply, but need several 
> variables instead of one, you can use something like:
> 
> sapply(split(dataFrame, factor_to_split_by), function(...))
> 
> HTH,
> Andy
> 
> From: Afshartous, David
> > 
> > sent this to the list yesterday but didn't see it listed in
> the daily
> > summary ... apologies if you receive it twice ...
> > 
> > ________________________________
> > 
> > From: Afshartous, David
> > Sent: Tuesday, June 27, 2006 10:02 AM
> > To: 'r-help at stat.math.ethz.ch'
> > Subject: sapply question
> > 
> > 
> > All:
> >  
> > I'm trying to use sapply to break up data within another function.
> > (tapply doens't seem to work since I want to access several
> variables
> > of a data set, not just break up a single variable according to a
> > factor.) I tried the R documentation but that wasn't much help on 
> > this.
> >  
> > The first function in the attached file works, breaking up the data 
> > according to patient number (6 observations per 24
> patients) via a for
> 
> > loop i=1,..24.
> >  
> > The second function is an attempt to use sapply instead of the first
> > "for loop".    
> > it doesn't load properly and I seem to be absolutely blind
> as to why;
> > my guess is that it's something "obvious".  any help appreciated.  
> > (necessary data
> > attached)
> >  
> > thanks!
> > dave
> >  
> > ps:
> > I'm on windows and,
> > r.2 =100  (input to function)
> > dat = comp.CAND.frm (attached file)
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
> --------------------------------------------------------------
> ----------
> ------
> Notice:  This e-mail message, together with any attachments, contains 
> information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station,

> New Jersey, USA 08889), and/or its affiliates (which may be known 
> outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD 
> and in Japan, as Banyu) that may be confidential, proprietary 
> copyrighted and/or legally privileged. It is intended solely for the 
> use of the individual or entity named on this message.  If you are not

> the intended recipient, and have received this message in error, 
> please notify us immediately by reply e-mail and then delete it from 
> your system.
> --------------------------------------------------------------
> ----------
> ------
> 
> 
> 


------------------------------------------------------------------------
------
Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station,
New Jersey, USA 08889), and/or its affiliates (which may be known
outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD
and in Japan, as Banyu) that may be confidential, proprietary
copyrighted and/or legally privileged. It is intended solely for the use
of the individual or entity named on this message.  If you are not the
intended recipient, and have received this message in error, please
notify us immediately by reply e-mail and then delete it from your
system.
------------------------------------------------------------------------
------


From Patrick_Bedard at brown.edu  Wed Jun 28 21:53:55 2006
From: Patrick_Bedard at brown.edu (Patrick Bedard)
Date: Wed, 28 Jun 2006 15:53:55 -0400
Subject: [R] R project question (SEM)
Message-ID: <C0C85693.29C5%Patrick_Bedard@brown.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060628/eb6e140a/attachment.pl 

From bill.shipley at usherbrooke.ca  Wed Jun 28 21:53:35 2006
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Wed, 28 Jun 2006 15:53:35 -0400
Subject: [R] superimposing histograms con't
Message-ID: <004601c69aec$8b869450$b61ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060628/aacb8d5e/attachment.pl 

From Patrick_Bedard at brown.edu  Wed Jun 28 21:56:41 2006
From: Patrick_Bedard at brown.edu (Patrick Bedard)
Date: Wed, 28 Jun 2006 15:56:41 -0400
Subject: [R]  data.frame error using sem package
Message-ID: <C0C85739.29C6%Patrick_Bedard@brown.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060628/1eb10d5f/attachment.pl 

From jfox at mcmaster.ca  Wed Jun 28 22:03:16 2006
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 28 Jun 2006 16:03:16 -0400
Subject: [R] R project question (SEM)
In-Reply-To: <C0C85693.29C5%Patrick_Bedard@brown.edu>
Message-ID: <web-130636332@cgpsrv2.cis.mcmaster.ca>

Dear Patrick,

It's hard to know how to help since you provide very little specific
information. For example, what OS are you using -- Windows, Mac OS,
Linux? 

Packages such as sem have to be installed before they are usable, and
even then ?sem won't provide you with information before the package is
loaded via library(sem).

You might start by reading the Introduction to R manual that comes with
R. Among other things, it explains how to install packages. Under
Windows, for example, the easiest way to proceed if you have an active
Internet connection is via the "Packages -> Install package(s)" menu in
the R console.

I hope this helps,
 John

On Wed, 28 Jun 2006 15:53:55 -0400
 Patrick Bedard <Patrick_Bedard at brown.edu> wrote:
> Hi there, 
> 
> I just saw your question on the help list and tough you could help me
> with
> some start-up info....
> 
> I just downloaded the R project software to try running SEM analysis
> and I
> also downloaded the SEM package. But it just sits there on the
> desktop and R
> does not know where it is...I can?t figure out how to organize the
> files...When I type ?sem in R it says object sem not found...I can?t
> seem to
> find help on the web on how to set-up the packages....
> 
> Well, thanks in advance
> 
> __________________ 
> Patrick Bedard Ph.D.
> Dept. of Neuroscience
> Brown University
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/


From NordlDJ at dshs.wa.gov  Wed Jun 28 22:16:30 2006
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS))
Date: Wed, 28 Jun 2006 13:16:30 -0700
Subject: [R] R project question (SEM)
Message-ID: <592E8923DB6EA348BE8E33FCAADEFFFC13EED8FF@dshs-exch2.dshs.wa.lcl>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Patrick Bedard
> Sent: Wednesday, June 28, 2006 12:54 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] R project question (SEM)
> 
> Hi there,
> 
> I just saw your question on the help list and tough you could help me with
> some start-up info....
> 
> I just downloaded the R project software to try running SEM analysis and I
> also downloaded the SEM package. But it just sits there on the desktop and
> R
> does not know where it is...I can?t figure out how to organize the
> files...When I type ?sem in R it says object sem not found...I can?t seem
> to
> find help on the web on how to set-up the packages....
> 
> Well, thanks in advance
> 
> __________________
> Patrick Bedard Ph.D.
> Dept. of Neuroscience
> Brown University
> 

Patrick,

You didn't specify what platform or OS you are working on, but whatever you
are using you need to install the sem package before you can use it. For me
on either WinXP Pro or Linux (SuSE10.0), the easient way to install a
package is to start up R and then type 

  > install.packages()

This will pop up window to select a package repository.  You will then be
given a list of packages from which to select.  Selected packages will be
download and installed. (You can also install the file you already
downloaded - see the docs)

Then to use the installed package you need to load it with the command

  > library(sem)

You will then be able to use the functions from the sem package.  There is
lots of documentation in the manuals included with the R software.  You
should read them along with the FAQ.  These can all be found with  your R
installation or you can read them on your nearest CRAN mirror.

Hope this is helpful,

Dan


Daniel J. Nordlund
Research and Data Analysis
Washington State Department of Social and Health Services
Olympia, WA  98504-5204


From pramod.r.anugu at jsums.edu  Wed Jun 28 22:18:10 2006
From: pramod.r.anugu at jsums.edu (Pramod Anugu)
Date: Wed, 28 Jun 2006 15:18:10 -0500
Subject: [R] Running R
In-Reply-To: <x2lkrhnq31.fsf@turmalin.kubism.ku.dk>
Message-ID: <000901c69aef$fa4c8d60$1321848f@E72517PA>

[root at genetics R-2.3.0]# ./configure
checking build system type... x86_64-unknown-linux-gnu
checking host system type... x86_64-unknown-linux-gnu
loading site script './config.site'
loading build specific script './config.site'
checking for pwd... /bin/pwd
checking whether builddir is srcdir... yes
checking for working aclocal... found
checking for working autoconf... found
checking for working automake... found
checking for working autoheader... found
checking for working makeinfo... found
checking for gawk... gawk
checking for egrep... grep -E
checking whether ln -s works... yes
checking for ranlib... ranlib
checking for bison... no
checking for byacc... no
checking for ar... ar
checking for a BSD-compatible install... /usr/bin/install -c
checking for sed... /bin/sed
checking for less... /usr/bin/less
checking for perl... /usr/bin/perl
checking whether perl version is at least 5.004... yes
checking for dvips... /usr/bin/dvips
checking for tex... /usr/bin/tex
checking for latex... /usr/bin/latex
checking for makeindex... /usr/bin/makeindex
checking for pdftex... /usr/bin/pdftex
checking for pdflatex... /usr/bin/pdflatex
checking for makeinfo... /usr/bin/makeinfo
checking for unzip... /usr/bin/unzip
checking for zip... /usr/bin/zip
checking for gzip... /bin/gzip
checking for firefox... /usr/bin/firefox
using default browser ... /usr/bin/firefox
checking for acroread... no
checking for acroread4... no
checking for xpdf... no
checking for gv... no
checking for gnome-gv... no
checking for ggv... /usr/bin/ggv
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
checking whether gcc needs -traditional... no
checking how to run the C preprocessor... gcc -E
checking for g77... g77
checking whether we are using the GNU Fortran 77 compiler... yes
checking whether g77 accepts -g... yes
checking for g++... g++
checking whether we are using the GNU C++ compiler... yes
checking whether g++ accepts -g... yes
checking how to run the C++ preprocessor... g++ -E
checking whether __attribute__((visibility())) is supported... yes
checking whether gcc accepts -fvisibility... yes
checking whether g77 accepts -fvisibility... yes
checking for a sed that does not truncate output... /bin/sed
checking for ld used by gcc... /usr/bin/ld
checking if the linker (/usr/bin/ld) is GNU ld... yes
checking for /usr/bin/ld option to reload object files... -r
checking for BSD-compatible nm... /usr/bin/nm -B
checking how to recognise dependent libraries... pass_all
checking for ANSI C header files... yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for unistd.h... yes
checking dlfcn.h usability... yes
checking dlfcn.h presence... yes
checking for dlfcn.h... yes
checking the maximum length of command line arguments... 32768
checking command to parse /usr/bin/nm -B output from gcc object... ok
checking for objdir... .libs
checking for ranlib... (cached) ranlib
checking for strip... strip
checking if gcc static flag  works... yes
checking if gcc supports -fno-rtti -fno-exceptions... no
checking for gcc option to produce PIC... -fPIC
checking if gcc PIC flag -fPIC works... yes
checking if gcc supports -c -o file.o... yes
checking whether the gcc linker (/usr/bin/ld -m elf_x86_64) supports shared
libraries... yes
checking whether -lc should be explicitly linked in... no
checking dynamic linker characteristics... GNU/Linux ld.so
checking how to hardcode library paths into programs... immediate
checking whether stripping libraries is possible... yes
checking if libtool supports shared libraries... yes
checking whether to build shared libraries... yes
checking whether to build static libraries... no
configure: creating libtool
appending configuration tag "CXX" to libtool
checking for ld used by g++... /usr/bin/ld -m elf_x86_64
checking if the linker (/usr/bin/ld -m elf_x86_64) is GNU ld... yes
checking whether the g++ linker (/usr/bin/ld -m elf_x86_64) supports shared
libraries... yes
checking for g++ option to produce PIC... -fPIC
checking if g++ PIC flag -fPIC works... yes
checking if g++ supports -c -o file.o... yes
checking whether the g++ linker (/usr/bin/ld -m elf_x86_64) supports shared
libraries... yes
checking dynamic linker characteristics... GNU/Linux ld.so
checking how to hardcode library paths into programs... immediate
checking whether stripping libraries is possible... yes
appending configuration tag "F77" to libtool
checking if libtool supports shared libraries... yes
checking whether to build shared libraries... yes
checking whether to build static libraries... no
checking for g77 option to produce PIC... -fPIC
checking if g77 PIC flag -fPIC works... yes
checking if g77 supports -c -o file.o... yes
checking whether the g77 linker (/usr/bin/ld -m elf_x86_64) supports shared
libraries... yes
checking dynamic linker characteristics... GNU/Linux ld.so
checking how to hardcode library paths into programs... immediate
checking whether stripping libraries is possible... yes
checking whether makeinfo version is at least 4.7... yes
checking for cos in -lm... yes
checking for sin in -lm... yes
checking for dlopen in -ldl... yes
checking readline/history.h usability... no
checking readline/history.h presence... no
checking for readline/history.h... no
checking readline/readline.h usability... no
checking readline/readline.h presence... no
checking for readline/readline.h... no
checking for rl_callback_read_char in -lreadline... no
checking for main in -lncurses... yes
checking for rl_callback_read_char in -lreadline... no
checking for history_truncate_file... no
configure: error: --with-readline=yes (default) and headers/libs are not
available

--One suggestion was to install readline-devel. But according to the below I
already have readline-devel.

# find . -iname readline-devel\*
 find -iname readline-devel\*
./os/4.1/x86_64/RedHat/RPMS/readline-devel-4.3-13.x86_64.rpm

#rpm -ivh readline*.rpm
Preparing...                ###########################################
[100%]
        package readline-4.3-13 is already installed

Once you've found an RPM, to install it on the frontend, just type
 
# rpm -ivh filename.rpm

-------------

   1. tar -zxvf R-2.3.0.tar.gz
   2. changed the directory to the newly created directory R-2.3.0
   3. Typed ./configure 
  
  
  
   4. Typed make
   make: *** No targets specified and no makefile found.  Stop.
   I cannot run make. Please let me know.
   
   *5. Make check
   *6. make check-all
   *7. Typed make install
   *8. Typed R


From jeffmiller at alphapoint05.net  Wed Jun 28 22:20:39 2006
From: jeffmiller at alphapoint05.net (Jeff Miller)
Date: Wed, 28 Jun 2006 16:20:39 -0400
Subject: [R] superimposing histograms con't
In-Reply-To: <004601c69aec$8b869450$b61ad284@BIO041>
Message-ID: <000f01c69af0$53321ee0$6401a8c0@AdSAMJeff>

I was just thinking about this last night.

I would like to do the same but WITH overlapping.

For example, I graph 2 sets of count data. Say the bars for the 1`s
overlap...I would like to show that with a different shading for the group
that has the higher frequency. For example, it could be black up to a
frequency of 5 followed by diagonal-dashes from 5-7 representing the higher
frequency of a second group.

Thank you,
Jeff Miller



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bill Shipley
Sent: Wednesday, June 28, 2006 3:54 PM
To: R help list
Subject: [R] superimposing histograms con't

Earlier, I posted the following question:
I want to superimpose histograms from three populations onto the same graph,
changing the shading of the bars for each population. After  consulting the
help files and the archives I cannot find out how to do  this (seemly)
simple graph. To be clear, I want
- a single x axis (from -3 to 18)
 - three groups of bars forming the histograms of each population (they
will not overlap much, but this is a detail)
- the bars from each histogram having different shadings or other  visually
distinguishing features.
 
Gabor Grothendieck [ggrothendieck at gmail.com] pointed to some code to to this
but I have found another way that works even easier.
 
hist(x[sel1],xlim=c(a,b),ylim=c(A,B))  - this plots the histogram for the
first group (indexed by sel1) but with an x axis and a y axis that spans the
entire range.
 
par(new=T)  - to keep on the same graph
 
hist(x[sel2],main=Null,xlab=NULL,ylab=NULL,axes=F) -superimposes the second
histogram
 
par(new=T)  - to keep on the same graph
 
hist(x[sel3],main=Null,xlab=NULL,ylab=NULL,axes=F) -superimposes the third
histogram
 
 

Bill Shipley

North American Editor, Annals of Botany

Editor, "Population and Community Biology" series, Springer Publishing

Dipartement de biologie, Universiti de Sherbrooke,

Sherbrooke (Quibec) J1K 2R1 CANADA

Bill.Shipley at USherbrooke.ca

http://callisto.si.usherb.ca:8080/bshipley/

 

 

	[[alternative HTML version deleted]]


From bhatticr at yahoo.com  Wed Jun 28 22:34:04 2006
From: bhatticr at yahoo.com (Chad R. Bhatti)
Date: Wed, 28 Jun 2006 13:34:04 -0700 (PDT)
Subject: [R] Fwd: add1() and anova() with glm with dispersion
Message-ID: <20060628203404.14044.qmail@web52814.mail.yahoo.com>

> Hello,
> 
> I have a question about a discrepancy between the
> reported F statistics using anova() and add1() from
> adding an additional term to form nested models. 
> 
> I found and old posting related to anova() and 
> drop1() regarding a glm with a dispersion parameter.
> 
> The posting is very old (May 2000, R 1.1.0).
> The old posting is located here.
> 
>
https://stat.ethz.ch/pipermail/r-devel/2000-May/020720.html
> 
>  
> I first noticed the discrepancy in the PC version of
> R
> 2.2.1.  I have upgraded to PC R 2.3.1 and the
> discrepancy remains.  
> 
> To be clear I will be as exact as possible and show
> the model output below.
> I am fitting nested models from the quasi-Poisson
> family using glm(). Here, model.0 is contained in
> model.1 where model.1 contains one additional
> covariate not in model.0.
>  
> To be specific I will show you model.0 and model.1.
> > > summary(model.0)
> > 
> > Call:
> > glm(formula = as.formula(formula.0), family =
> > quasipoisson, data = data.1)
> > 
> > Deviance Residuals: 
> >     Min       1Q   Median       3Q      Max  
> > -4.0256  -1.0080  -0.1604   0.7372   4.8545  
> > 
> > Coefficients:
> >               Estimate Std. Error t value Pr(>|t|)
>  
> >  
> > (Intercept)  9.440e-01  8.968e-02  10.526  < 2e-16
> > ***
> > I11         -6.857e-02  3.693e-02  -1.857 0.063468
> .
> >  
> > I12         -7.338e-02  4.142e-02  -1.771 0.076598
> .
> >  
> > I13          3.329e-02  4.130e-02   0.806 0.420208
>  
> >  
> > I14          1.830e-01  3.717e-02   4.924 9.00e-07
> > ***
> > I15          2.015e-01  3.480e-02   5.789 7.93e-09
> > ***
> > trades1      1.775e-01  2.114e-02   8.398  < 2e-16
> > ***
> > trades2      3.325e-02  1.857e-02   1.790 0.073493
> .
> >  
> > trades3      1.050e-01  1.873e-02   5.604 2.31e-08
> > ***
> > trades4      4.615e-02  1.853e-02   2.490 0.012827
> *
> >  
> > vol1         1.932e-04  5.116e-05   3.777 0.000162
> > ***
> > sVol1        1.211e-04  4.536e-05   2.670 0.007643
> > ** 
> > trades5      1.835e-02  1.850e-02   0.992 0.321297
>  
> >  
> > trades6      7.797e-03  1.837e-02   0.425 0.671213
>  
> >  
> > trades7      3.802e-02  1.831e-02   2.077 0.037916
> *
> >  
> > trades8      5.904e-02  1.831e-02   3.224 0.001280
> > ** 
> > trades9      4.581e-02  1.830e-02   2.503 0.012383
> *
> >  
> > trades10     5.521e-02  1.802e-02   3.063 0.002212
> > ** 
> > ---
> > Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05
> '.'
> > 0.1 ' ' 1 
> > 
> > (Dispersion parameter for quasipoisson family
> taken
> > to
> > be 1.832780)
> > 
> >     Null deviance: 7020.8  on 2623  degrees of
> > freedom
> > Residual deviance: 4697.2  on 2606  degrees of
> > freedom
> > AIC: NA
> > 
> > Number of Fisher Scoring iterations: 4
> > 
> > 
> > 
> > 
> > > summary(model.1)
> > 
> > Call:
> > glm(formula = as.formula(formula.1), family =
> > quasipoisson, data = data.1)
> > 
> > Deviance Residuals: 
> >     Min       1Q   Median       3Q      Max  
> > -4.0807  -1.0061  -0.1659   0.7461   4.9228  
> > 
> > Coefficients:
> >               Estimate Std. Error t value Pr(>|t|)
>  
> >  
> > (Intercept)  9.627e-01  9.019e-02  10.675  < 2e-16
> > ***
> > I11         -6.631e-02  3.694e-02  -1.795 0.072713
> .
> >  
> > I12         -7.383e-02  4.141e-02  -1.783 0.074717
> .
> >  
> > I13          3.363e-02  4.129e-02   0.814 0.415467
>  
> >  
> > I14          1.844e-01  3.717e-02   4.960 7.50e-07
> > ***
> > I15          2.017e-01  3.479e-02   5.798 7.51e-09
> > ***
> > trades1      1.840e-01  2.140e-02   8.600  < 2e-16
> > ***
> > trades2      3.585e-02  1.861e-02   1.927 0.054135
> .
> >  
> > trades3      1.050e-01  1.872e-02   5.607 2.27e-08
> > ***
> > trades4      4.647e-02  1.852e-02   2.509 0.012166
> *
> >  
> > vol1         1.957e-04  5.111e-05   3.828 0.000132
> > ***
> > sVol1        1.238e-04  4.535e-05   2.730 0.006374
> > ** 
> > trades5      1.883e-02  1.849e-02   1.019 0.308506
>  
> >  
> > trades6      7.358e-03  1.836e-02   0.401 0.688663
>  
> >  
> > trades7      3.840e-02  1.829e-02   2.099 0.035928
> *
> >  
> > trades8      5.907e-02  1.831e-02   3.226 0.001272
> > ** 
> > trades9      4.527e-02  1.830e-02   2.473 0.013447
> *
> >  
> > trades10     5.582e-02  1.802e-02   3.098 0.001970
> > ** 
> > aveSpread1  -2.386e-01  1.238e-01  -1.928 0.054022
> .
> >  
> > ---
> > Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05
> '.'
> > 0.1 ' ' 1 
> > 
> > (Dispersion parameter for quasipoisson family
> taken
> > to
> > be 1.831032)
> > 
> >     Null deviance: 7020.8  on 2623  degrees of
> > freedom
> > Residual deviance: 4690.4  on 2605  degrees of
> > freedom
> > AIC: NA
> > 
> > Number of Fisher Scoring iterations: 4
>  
> So model.1 is model.0 + aveSpread1.
> 
>  
> The R anova() function is supposed to account for a
> dispersion/scale parameter of a glm object and has
> the
> following output. The anova() function adds the
> terms
> sequentially and computes a test statistic. 
> 
> > > anova(model.1, test="F")
> > Analysis of Deviance Table
> > 
> > Model: quasipoisson, link: log
> > 
> > Response: trades
> > 
> > Terms added sequentially (first to last)
> > 
> > 
> >              Df Deviance Resid. Df Resid. Dev     
>  
> > F
> >    Pr(>F)    
> > NULL                          2623     7020.8     
>  
> >  
> >              
> > I11           1     91.5      2622     6929.3 
> > 49.9706
> > 1.996e-12 ***
> > I12           1    622.3      2621     6307.0
> > 339.8717
> > < 2.2e-16 ***
> > I13           1    614.4      2620     5692.6
> > 335.5561
> > < 2.2e-16 ***
> > I14           1     53.3      2619     5639.4 
> > 29.0903
> > 7.529e-08 ***
> > I15           1     64.6      2618     5574.8 
> > 35.2590
> > 3.270e-09 ***
> > trades1       1    535.2      2617     5039.6
> > 292.3131
> > < 2.2e-16 ***
> > trades2       1     49.0      2616     4990.5 
> > 26.7798
> > 2.454e-07 ***
> > trades3       1    113.6      2615     4876.9 
> > 62.0682
> > 4.830e-15 ***
> > trades4       1     31.3      2614     4845.6 
> > 17.0783
> > 3.700e-05 ***
> > vol1          1     20.8      2613     4824.8 
> > 11.3810
> > 0.0007528 ***
> > sVol1         1     13.9      2612     4810.9  
> > 7.5932
> > 0.0058997 ** 
> > trades5       1     10.6      2611     4800.3  
> > 5.7901
> > 0.0161861 *  
> > trades6       1      8.1      2610     4792.2  
> > 4.4175
> > 0.0356685 *  
> > trades7       1     24.7      2609     4767.5 
> > 13.4770
> > 0.0002464 ***
> > trades8       1     33.5      2608     4734.0 
> > 18.2926
> > 1.963e-05 ***
> > trades9       1     19.5      2607     4714.5 
> > 10.6649
> > 0.0011060 ** 
> > trades10      1     17.3      2606     4697.2  
> > 9.4474
> > 0.0021364 ** 
> > aveSpread1    1      6.8      2605     4690.4  
> > 3.7240
> > 0.0537449 .  
> > ---
>  
> 
> Now consider the output from add1() when adding
> aveSpread1 to model.0.
> 
> > > add1(model.0, ~ . + aveSpread1, test = "F")
> > Single term additions
> > 
> > Model:
> > trades ~ I11 + I12 + I13 + I14 + I15 + trades1 +
> > trades2 + trades3 + 
> >     trades4 + vol1 + sVol1 + trades5 + trades6 +
> > trades7 + trades8 + 
> >     trades9 + trades10
> >            Df Deviance F value   Pr(F)  
> > <none>          4697.2                  
> > aveSpread1  1   4690.4  3.7871 0.05176 .
> > 
> 
> 
> Here is the discrepancy. Compare F statistics
> for aveSpread1 3.7240 from anova() to 3.7871 from 
> add1().  
> 
> 
> I built my own anova-type table that did not account
> for the dispersion parameter and my last term
> (rounded) matches that reported by add1().  
> 
> > source('main.R')
>             [,1]                                    
>  
>                              
> model.table "Variable       Deviance       Resid Dev
>  
>    F              p-value   "
> add.term    "trades1        535.2          5039.6   
>  
>    277.84         0.0000    "
> add.term    "trades2        49.0           4990.5   
>  
>    25.69          0.0000    "
> add.term    "trades3        113.6          4876.9   
>  
>    60.92          0.0000    "
> add.term    "trades4        31.3           4845.6   
>  
>    16.86          0.0000    "
> add.term    "vol1           20.8           4824.8   
>  
>    11.28          0.0008    "
> add.term    "sVol1          13.9           4810.9   
>  
>    7.55           0.0061    "
> add.term    "trades5        10.6           4800.3   
>  
>    5.76           0.0164    "
> add.term    "trades6        8.1            4792.2   
>  
>    4.40           0.0360    "
> add.term    "trades7        24.7           4767.5   
>  
>    13.50          0.0002    "
> add.term    "trades8        33.5           4734.0   
>  
>    18.45          0.0000    "
> add.term    "trades9        19.5           4714.5   
>  
>    10.79          0.0010    "
> add.term    "trades10       17.3           4697.2   
>  
>    9.59           0.0020    "
> add.term    "aveSpread1     6.8            4690.4   
>  
>    3.79           0.0518    "
> 
> 
> McCullagh and Nelder (2nd ed) p207 #4 show an
> adjustment for the dispersion parameter in the
> F-statistic.  It does not seem that add1() is making
> this adjustment though the anova() help page
> suggests
> that anova() is.  The estimated dispersion
> parameters
> are s0 = 1.832780 and s1 = 1.831032.  I have tried
> scaling the F statistic from add1() by s1/s0, but it
> still is not equal to the F statistic from anova().
> 
> I am not experienced with the R source code.  I
> would
> like to know how add1() computes the F statistic
> with
> dispersion and how anova() computes the F statistic
> with dispersion and why they are different.
> 
> I apologize if I have missed something obvious, but
> add1() and anova() do not show the code in the PC
> version.  In the PC version if I type lm, glm,
> glm.fit, I see code and I do not for add1 and anova
> so
> I do not know how to examine these functions.
> 
> Thanks for all of your assistance.
> 
> Chad R. Bhatti
> 
> 
> 
> 
> __________________________________________________
> Do You Yahoo!?

> protection around 
> http://mail.yahoo.com 
>


From matthew_wiener at merck.com  Wed Jun 28 22:51:35 2006
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Wed, 28 Jun 2006 16:51:35 -0400
Subject: [R] superimposing histograms con't  [Broadcast]
Message-ID: <4E9A692D8755DF478B56A2892388EE1FDB3250@usctmx1118.merck.com>

I wrote some code to do this.  It only works with 2 groups (that's all I
needed), but could probably be generalized.  It got my graph made, and I
haven't needed a graph like this one again, so I never went back to really
clean it up.

It works by first plotting both sets of rectangles, then going back over the
ones that had the first bar totally covered by the second.

Hope this helps,

Matt Wiener


"f.back.front.hist" <-
function(breaks, x1, x2, col1 = "gray50", col2 = "white",
           lwd = diff(range(mids))/20,
           xlab = "", ylab = "",
           leg.text = NULL, ...){
    if(length(x1) != length(x2))
      stop("x1 and x2 must have same length")
    if(length(x1) != length(breaks) - 1)
      stop("length of breaks must be 1 more than length of x1 and x2")
    plot.lim <- c(0, max(c(x1, x2)))
    mids <- 0.5 * (breaks[-1] + breaks[-length(breaks)])
    plot(mids, pmax(x1, x2), col = par()$bg, lwd = lwd,
         ylim = plot.lim, xlab = xlab, ylab = ylab,
         cex.axis = 1.5, font.axis = 2, cex.lab = 1.5, font.lab = 2, ...)
    rect(breaks[-length(breaks)], 0, breaks[-1], x1, col = col1)
    rect(breaks[-length(breaks)], 0, breaks[-1], x2, col = col2)
    ind <- x1 < x2
    rect(breaks[-length(breaks)][ind], 0,
         breaks[-1][ind], x1[ind] ,col = col1)
    if(!is.null(leg.text))
      legend(mids[1], plot.lim[2], leg.text,
             fill = c(col1, col2), cex = 1.5)

  } 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jeff Miller
Sent: Wednesday, June 28, 2006 4:21 PM
To: 'Bill Shipley'; 'R help list'
Subject: Re: [R] superimposing histograms con't [Broadcast]

I was just thinking about this last night.

I would like to do the same but WITH overlapping.

For example, I graph 2 sets of count data. Say the bars for the 1`s
overlap...I would like to show that with a different shading for the group
that has the higher frequency. For example, it could be black up to a
frequency of 5 followed by diagonal-dashes from 5-7 representing the higher
frequency of a second group.

Thank you,
Jeff Miller



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bill Shipley
Sent: Wednesday, June 28, 2006 3:54 PM
To: R help list
Subject: [R] superimposing histograms con't

Earlier, I posted the following question:
I want to superimpose histograms from three populations onto the same graph,
changing the shading of the bars for each population. After  consulting the
help files and the archives I cannot find out how to do  this (seemly)
simple graph. To be clear, I want
- a single x axis (from -3 to 18)
 - three groups of bars forming the histograms of each population (they
will not overlap much, but this is a detail)
- the bars from each histogram having different shadings or other  visually
distinguishing features.
 
Gabor Grothendieck [ggrothendieck at gmail.com] pointed to some code to to this
but I have found another way that works even easier.
 
hist(x[sel1],xlim=c(a,b),ylim=c(A,B))  - this plots the histogram for the
first group (indexed by sel1) but with an x axis and a y axis that spans the
entire range.
 
par(new=T)  - to keep on the same graph
 
hist(x[sel2],main=Null,xlab=NULL,ylab=NULL,axes=F) -superimposes the second
histogram
 
par(new=T)  - to keep on the same graph
 
hist(x[sel3],main=Null,xlab=NULL,ylab=NULL,axes=F) -superimposes the third
histogram
 
 

Bill Shipley

North American Editor, Annals of Botany

Editor, "Population and Community Biology" series, Springer Publishing

Dipartement de biologie, Universiti de Sherbrooke,

Sherbrooke (Quibec) J1K 2R1 CANADA

Bill.Shipley at USherbrooke.ca

http://callisto.si.usherb.ca:8080/bshipley/

 

 

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From deepayan.sarkar at gmail.com  Wed Jun 28 23:11:15 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 28 Jun 2006 16:11:15 -0500
Subject: [R] y-axis break with lattice graphics
In-Reply-To: <BA6A0A1B-E5B5-41FF-8EFB-A9BF62546D7D@usc.edu>
References: <BA6A0A1B-E5B5-41FF-8EFB-A9BF62546D7D@usc.edu>
Message-ID: <eb555e660606281411l70ba6ea6jcf58d541b08a1701@mail.gmail.com>

On 6/28/06, Jason Curole <jcurole at usc.edu> wrote:
> Hello all,
>
> I am trying to incorporate a y-axis break (i.e. the lightning bolt
> style) in an xyplot (using panel.superpose).  I have tried to do this
> manually using panel.line, but panel.line will not draw to the left
> of the y-axis (or on the y-axis for that matter).  I would prefer not
> to have to do this by hand, but I feel like I am running out of options.
>
> The figure can be viewed here (http://threeridge.usc.edu/
> Figure1.revised.pdf).  The solid, green, horizontal line above the
> 20000 tick is where I would like to draw the break.  NB, this solid,
> horizontal line should extend beyond the y-axis to the left, but is
> clearly cut off.  I can post the source if necessary.

Try adding

  par.settings = list(clip = list(panel = "off"))

-Deepayan


From gangchen at mail.nih.gov  Wed Jun 28 23:14:10 2006
From: gangchen at mail.nih.gov (Gang Chen)
Date: Wed, 28 Jun 2006 17:14:10 -0400
Subject: [R] Problem with package sem
Message-ID: <D3D1C39D-CD3B-41E9-A503-7ED5F1463B62@mail.nih.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060628/a1e6a16f/attachment.pl 

From maj at stats.waikato.ac.nz  Thu Jun 29 07:15:08 2006
From: maj at stats.waikato.ac.nz (maj at stats.waikato.ac.nz)
Date: Thu, 29 Jun 2006 17:15:08 +1200 (NZST)
Subject: [R] Extracting R plots from MS Word
Message-ID: <62025.58.105.171.47.1151558108.squirrel@webmail.scms.waikato.ac.nz>

Hi,

I am revising a paper that I am a co-author of. The figures are plots
generated from R but at the moment I do not have the R code that generates
them.

As this is time critical I would like to slightly abuse the list by asking
whether anyone knows how to extract from MS Word into a stand-alone
graphics file a plot that was pasted into Word from R (probably as a
Windows Metafile, but possibly as a bitmap).

I would be very grateful for help with this.

Regards,  Murray Jorgensen


From p_connolly at ihug.co.nz  Thu Jun 29 01:13:32 2006
From: p_connolly at ihug.co.nz (Patrick Connolly)
Date: Thu, 29 Jun 2006 11:13:32 +1200
Subject: [R] Address of FactoMineR package author
Message-ID: <20060628231332.GQ4970@ihug.co.nz>

I've tried to contact the author of part of the FactoMineR package
which was given as:  Fran?ois.Husson at agrocampus-rennes.fr

However, the cedilla is not liked by any mail system I have access to,
so that address is a non-starter.  I tried using a simple letter 'c'
which didn't cause a syntax error, but achieved nothing else.  

Could it just be that the address was correct and it's the season in
France for people to be away and so not answering email?  Or is there
something I should know about such characters in email addresses?

TIA

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From ligges at statistik.uni-dortmund.de  Thu Jun 29 08:41:59 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 29 Jun 2006 08:41:59 +0200
Subject: [R] Problem with package sem
In-Reply-To: <D3D1C39D-CD3B-41E9-A503-7ED5F1463B62@mail.nih.gov>
References: <D3D1C39D-CD3B-41E9-A503-7ED5F1463B62@mail.nih.gov>
Message-ID: <44A37637.104@statistik.uni-dortmund.de>

Gang Chen wrote:
> Hi experts,
> 
> I just started to learn R today, and tried to work with an add-on  
> package sem. I have a version of 2.3.1 on MacOS X 10.4.6 with sem put  
> under /Library/Frameworks/R.framework/Versions/2.3/Resources/library
> 
> However when I typed
> 
> library(sem)
> 
> the following error showed up:
> 
> Error in library(sem) : 'sem' is not a valid package -- installed <  
> 2.0.0?


Please read the R Installation and Administration manual on how to 
install packages (e.g. with R CMD INTALL in the OS' console or 
install.packages() in R)

Uwe Ligges


> Why is this?
> 
> Thank you,
> Gang Chen
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ligges at statistik.uni-dortmund.de  Thu Jun 29 08:44:58 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 29 Jun 2006 08:44:58 +0200
Subject: [R] R project question (SEM)
In-Reply-To: <C0C85693.29C5%Patrick_Bedard@brown.edu>
References: <C0C85693.29C5%Patrick_Bedard@brown.edu>
Message-ID: <44A376EA.8060103@statistik.uni-dortmund.de>

Patrick Bedard wrote:
> Hi there, 
> 
> I just saw your question on the help list and tough you could help me with
> some start-up info....
> 
> I just downloaded the R project software to try running SEM analysis and I
> also downloaded the SEM package. But it just sits there on the desktop and R
> does not know where it is...I can?t figure out how to organize the
> files...When I type ?sem in R it says object sem not found...I can?t seem to
> find help on the web on how to set-up the packages....


Same answer:
There is the "R Installation and Administration" manual.

Uwe Ligges

> Well, thanks in advance
> 
> __________________ 
> Patrick Bedard Ph.D.
> Dept. of Neuroscience
> Brown University
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From mikewolfgang at gmail.com  Thu Jun 29 04:28:53 2006
From: mikewolfgang at gmail.com (Mike Wolfgang)
Date: Wed, 28 Jun 2006 22:28:53 -0400
Subject: [R] re-direct to "more" or "less"
Message-ID: <e668df8c0606281928l3ec50963sd630e6f1f55f6e61@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060628/5361c59c/attachment.pl 

From manojsw at gmail.com  Thu Jun 29 04:42:54 2006
From: manojsw at gmail.com (Manoj)
Date: Thu, 29 Jun 2006 11:42:54 +0900
Subject: [R] Question on memory allocation & loop
Message-ID: <829e6c8a0606281942w65f815fbx6709c06e5e26cf96@mail.gmail.com>

Hello All,
      I am trying to work on writing the following piece of (pseudo)
code in an optimal fashion:

----------------------------------------------------
# Two data frames with some data

a = data.frame(somedata)
b = data.frame(somedata)

for(i in 1:nrow(dt) {
  # Merge dates for a given date into a new data frame
   c = merge(a[a$dt==dt[i],),b[b$dt == dt[i],], by=c(some column));
}
----------------------------------------------------


Now, my understanding is that the data frame c in the above code is
malloc'ed in every count of the loop.  Is that assumption correct?

Is the following attempt a better way of doing things?

----------------------------------------------------
a = data.frame(somedata)
b = data.frame(somedata)

# Pre-allocate data frame c

c = data.frame(for some size);

for(i in 1:nrow(dt) {
  # Merge dates for a given date into a new data frame
   # and copy the result into c

  copy(c, merge(a[a$dt==dt[i],),b[b$dt == dt[i],], by=c(some column));

}
----------------------------------------------------

Now the question is, How can I copy the merged data into my
pre-allocated data frame c ? I tried rbind/cbind but they are pretty
fuzzy about having the right names and dimension hence it fails.

Any help would be greatly appreciated!

Thanks.

Manoj


From jfox at mcmaster.ca  Thu Jun 29 08:54:44 2006
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 29 Jun 2006 02:54:44 -0400
Subject: [R] Problem with package sem
In-Reply-To: <D3D1C39D-CD3B-41E9-A503-7ED5F1463B62@mail.nih.gov>
Message-ID: <web-130681973@cgpsrv2.cis.mcmaster.ca>

Dear Gang Chen,

I expect that either you didn't install the package properly (see
?install.packages) or installed a version of the package for an earlier
version of R.

I hope this helps,
 John

On Wed, 28 Jun 2006 17:14:10 -0400
 Gang Chen <gangchen at mail.nih.gov> wrote:
> Hi experts,
> 
> I just started to learn R today, and tried to work with an add-on  
> package sem. I have a version of 2.3.1 on MacOS X 10.4.6 with sem put
>  
> under /Library/Frameworks/R.framework/Versions/2.3/Resources/library
> 
> However when I typed
> 
> library(sem)
> 
> the following error showed up:
> 
> Error in library(sem) : 'sem' is not a valid package -- installed <  
> 2.0.0?
> 
> Why is this?
> 
> Thank you,
> Gang Chen
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/


From dimitris.rizopoulos at med.kuleuven.be  Thu Jun 29 09:12:44 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 29 Jun 2006 09:12:44 +0200
Subject: [R] re-direct to "more" or "less"
References: <e668df8c0606281928l3ec50963sd630e6f1f55f6e61@mail.gmail.com>
Message-ID: <009e01c69b4b$6af03f20$0540210a@www.domain>

maybe ?sink() or ?capture.output() could be useful, in this case.


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Mike Wolfgang" <mikewolfgang at gmail.com>
To: "R-help list" <r-help at stat.math.ethz.ch>
Sent: Thursday, June 29, 2006 4:28 AM
Subject: [R] re-direct to "more" or "less"


> Dear list,
>
> sometimes my function generates too much data and shows them on 
> screen, i
> cannot view first several lines until program ends and I have to 
> scroll my
> mouse up to get them. Is there any re-direction function in R to 
> pipeline
> outputs to "more" or "less" type functions?
> Thanks
>
> mike
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From maechler at stat.math.ethz.ch  Thu Jun 29 09:09:23 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 29 Jun 2006 09:09:23 +0200
Subject: [R] Building R: use the current version! {was " Running R"}
In-Reply-To: <000901c69aef$fa4c8d60$1321848f@E72517PA>
References: <x2lkrhnq31.fsf@turmalin.kubism.ku.dk>
	<000901c69aef$fa4c8d60$1321848f@E72517PA>
Message-ID: <17571.31907.856651.878321@stat.math.ethz.ch>

I'm not really answering your question, but if you install from
the sources -- always a useful experience in finding out if your
computer is well equipped with software -- 
*PLEASE* use the latest aka "current" version of R.
Currently, that's 2.3.1 and not 2.3.0.

A very reasonable alternative is also to work with R-patched
(i.e. 'R 2.3.1 patched' at the moment), since we (aim to) ensure
that R-patched only contains simple well tested bug fixes in
addition to the respective released version.

(Contrary to "R-devel" which always contains the current R
 development version with all potential new features and which
 is hence also designated 'unstable'.)

Martin Maechler, 
ETH Zurich (and R-core)


>>>>> "Pramod" == Pramod Anugu <pramod.r.anugu at jsums.edu>
>>>>>     on Wed, 28 Jun 2006 15:18:10 -0500 writes:

    Pramod> [root at genetics R-2.3.0]# ./configure

  .................
  .................

    Pramod> 1. tar -zxvf R-2.3.0.tar.gz
    Pramod> 2. changed the directory to the newly created directory R-2.3.0

   .................
   .................


From toby_marks at americancentury.com  Thu Jun 29 04:18:51 2006
From: toby_marks at americancentury.com (toby_marks at americancentury.com)
Date: Wed, 28 Jun 2006 21:18:51 -0500
Subject: [R] Help needed understanding eval,quote,expression
Message-ID: <OF7E0000C5.21E7DB82-ON8625719B.00787616-8625719C.000CA1B3@americancentury.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060628/d0ae9a61/attachment.pl 

From ripley at stats.ox.ac.uk  Thu Jun 29 09:21:03 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 29 Jun 2006 08:21:03 +0100 (BST)
Subject: [R] Question on memory allocation & loop
In-Reply-To: <829e6c8a0606281942w65f815fbx6709c06e5e26cf96@mail.gmail.com>
References: <829e6c8a0606281942w65f815fbx6709c06e5e26cf96@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0606290809180.21841@gannet.stats.ox.ac.uk>

On Thu, 29 Jun 2006, Manoj wrote:

> Hello All,
>      I am trying to work on writing the following piece of (pseudo)
> code in an optimal fashion:
>
> ----------------------------------------------------
> # Two data frames with some data
>
> a = data.frame(somedata)
> b = data.frame(somedata)
>
> for(i in 1:nrow(dt) {
>  # Merge dates for a given date into a new data frame
>   c = merge(a[a$dt==dt[i],),b[b$dt == dt[i],], by=c(some column));
> }

Note that only the last iteration of that loop is actually needed.

What are you really trying to do, and why are you worrying about memory? 
E.g. merge() in R-devel is a lot more efficient for some operations, 
including perhaps your example.

> ----------------------------------------------------
>
>
> Now, my understanding is that the data frame c in the above code is
> malloc'ed in every count of the loop.  Is that assumption correct?

No.  Here 'c' is just a symbol, and assignment (please use <- in public 
code, it is easier to read) binds the symbol to the data frame returned by 
merge().  So the allocation (not 'malloc' necessarily) is going on inside 
merge(). Also, 'c' is a system object, so you are confusing people by 
using its name for your own object.

When you assign to 'c' you change the binding to a different already 
allocated object.  Eventually garbage collection will recover (to R) the 
memory allocated to objects which are no longer bound to symbols.

I am not aware of any account which describes in detail how R works at 
this level, and end users do not need to know it.  (It is also the case 
that R maintains a number of illusions and internally may not do what it 
appears to do.)

>
> Is the following attempt a better way of doing things?
>
> ----------------------------------------------------
> a = data.frame(somedata)
> b = data.frame(somedata)
>
> # Pre-allocate data frame c
>
> c = data.frame(for some size);
>
> for(i in 1:nrow(dt) {
>  # Merge dates for a given date into a new data frame
>   # and copy the result into c
>
>  copy(c, merge(a[a$dt==dt[i],),b[b$dt == dt[i],], by=c(some column));
>
> }
> ----------------------------------------------------
>
> Now the question is, How can I copy the merged data into my
> pre-allocated data frame c ? I tried rbind/cbind but they are pretty
> fuzzy about having the right names and dimension hence it fails.
>
> Any help would be greatly appreciated!
>
> Thanks.
>
> Manoj
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Soren.Hojsgaard at agrsci.dk  Thu Jun 29 09:26:09 2006
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Thu, 29 Jun 2006 09:26:09 +0200
Subject: [R] re-direct to "more" or "less"
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC047E42D9@DJFPOST01.djf.agrsci.dk>

Something like

less <- function(a){
  fn <- paste(tempdir(),"\\dataframe.txt",sep='',collapse='')
  write.table(a, quote=F, file=fn)
  system(paste("less ",fn))
}

could perhaps help you (assuming that you have less on your computer). I agree that it would be very nice to have a built-in version...
Regards
S?ren

 

> -----Oprindelig meddelelse-----
> Fra: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] P? vegne af Mike Wolfgang
> Sendt: 29. juni 2006 04:29
> Til: R-help list
> Emne: [R] re-direct to "more" or "less"
> 
> Dear list,
> 
> sometimes my function generates too much data and shows them 
> on screen, i cannot view first several lines until program 
> ends and I have to scroll my mouse up to get them. Is there 
> any re-direction function in R to pipeline outputs to "more" 
> or "less" type functions?
> Thanks
> 
> mike
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From ligges at statistik.uni-dortmund.de  Thu Jun 29 09:45:44 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 29 Jun 2006 09:45:44 +0200
Subject: [R] Error: evaluation nested too deeply: infinite recursion /
 options(expressions=)?
In-Reply-To: <BAY106-F30F483E542BC233261E798E37F0@phx.gbl>
References: <BAY106-F30F483E542BC233261E798E37F0@phx.gbl>
Message-ID: <44A38528.9080503@statistik.uni-dortmund.de>

Aarti Dahiya wrote:
> I am getting this error:- Error: evaluation nested too deeply: infinite 
> recursion / options(expressions=)? The reason is probbaly because I am 
> calling methods within methods.

Maybe at some point you get an infinite recursion because you forgot to 
"unclass" something?


> I read related posts.  It said the solution is to set  options(expressions = 
> 1000). I wanted to know where in my code or system to set 
> options(expressions = 1000)?


Bewore you start the problematic code, but probably you really have 
infinite recursion ....

Uwe Ligges


> Thanks.
> 
> Aarti
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ripley at stats.ox.ac.uk  Thu Jun 29 10:14:02 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 29 Jun 2006 09:14:02 +0100 (BST)
Subject: [R] Help needed understanding eval,quote,expression
In-Reply-To: <OF7E0000C5.21E7DB82-ON8625719B.00787616-8625719C.000CA1B3@americancentury.com>
References: <OF7E0000C5.21E7DB82-ON8625719B.00787616-8625719C.000CA1B3@americancentury.com>
Message-ID: <Pine.LNX.4.64.0606290906130.23502@gannet.stats.ox.ac.uk>

You are missing eval(parse(text=)). E.g.

> x <- list(y=list(y1="hello",y2="world"),z=list(z1="foo",z2="bar"))
(what do you mean by the $ at the start of these lines?)
> eval(parse(text="x$y$y1"))
[1] "hello"

However, bear in mind

> fortune("parse")

If the answer is parse() you should usually rethink the question.
    -- Thomas Lumley
       R-help (February 2005)

In your indicated example you could probably use substitute() as 
effectively.


On Wed, 28 Jun 2006, toby_marks at americancentury.com wrote:

> I am trying to build up a quoted or character expression representing a
> component in a list  in order to reference it indirectly.
> For instance, I have a list that has data I want to pull, and another list
> that has character vectors and/or lists of characters containing the names
> of the components in the first list.
>
>
> It seems that the way to do this is as evaluating expressions, but I seem
> to be missing something.  The concept should be similar to the snippet
> below:
>
>
> For instance:
>
> $x = list(y=list(y1="hello",y2="world"),z=list(z1="foo",z2="bar"))
> $y = quote(x$y$y1)
> $eval(y)
> [1] "hello"
>
>
> but, I'm trying to accomplish this by building up y as a character and
> then evaluating it, and having no success.
>
> $y1=paste("x$y$","y1",sep="")
> $y1
> [1] "x$y$y1"
>
>
> How can I evaluate y1 as I did with y previously?  or can I?
>
>
> Much Thanks !
>
>
>
>
>
>
>
>
> ------------------------------------------------------------
> CONFIDENTIALITY NOTICE: This electronic mail transmission (i...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ff809 at ncf.ca  Thu Jun 29 01:20:00 2006
From: ff809 at ncf.ca (Brian Lunergan)
Date: Wed, 28 Jun 2006 19:20:00 -0400
Subject: [R] Problem running one of the rgl demo scripts...
In-Reply-To: <44A01C3F.20304@stats.uwo.ca>
References: <44A0187E.4010505@ncf.ca> <44A01C3F.20304@stats.uwo.ca>
Message-ID: <44A30EA0.5090903@ncf.ca>

Duncan Murdoch wrote:
> Brian Lunergan wrote:
>> Afternoon folks:
>>
>> I'm getting a program crash when I try to run demo(rgl). The following 
>> error details result:
>>
>> RGUI caused a stack fault in module NVOPENGL.DLL at 017f:695280f0.
>> Registers:
>> EAX=00000002 CS=017f EIP=695280f0 EFLGS=00010246
>> EBX=00000001 SS=0187 ESP=00572000 EBP=004d1208
>> ECX=042f1f01 DS=0187 ESI=004d1208 FS=5d1f
>> EDX=00442d84 ES=0187 EDI=042f1f3c GS=5d0e
>> Bytes at CS:EIP:
>> 53 56 8b 5c 24 0c 57 83 bb 9c 39 00 00 02 0f 84
>> Stack dump:
>> 69541be5 004d1208 004d1208 00000001 69541bef 00000001 004d1208 
>> 00000001 69541bef 00000001 004d1208 00000001 69541bef 00000001 
>> 004d1208 00000001
>>
>> Setup details are:
>>
>> R version: 2.2.1
>> OS: win98se
>> RGL file name: rgl_0.66.zip
>>
>> It appears to be gagging on an Nvidia opengl driver file, NVIDIA 
>> Compatible OpenGL ICD, version 4.12.01.0631. The video card is 
>> recorded as:
>>
>> 3DForce2 MX Series,NVIDIA GeForce2 MX (Ver 4.12.01.0631 ,9/20/2000)
>>
>> I also tried this with version 2.3.1 of R with the same results.
>>
>> Anyone have any thoughts or ideas on the subject? Has it occurred any 
>> place else? Is there a workaround or solution, or should I perhaps 
>> turf the package and forgo its abilities since it appears my system as 
>> it stands may not be able to support it?
> You could try a newer build, available on my web page 
> (http://www.stats.uwo.ca/faculty/murdoch/software).  If it dies the same 
> way, you could perhaps try to diagnose what is going wrong and send a 
> patch if it's an rgl bug.  Given the age of your video driver 
> (9/20/2000), you might be able to update it and fix a bug there.

Just downloaded and installed a new video driver (file the demo originally 
reporting choking on now showing v4.14.10.8198, December 10, 2005 03:06:00) 
and things seemed to have settled down. The demo now runs through fully 
without complaint.

-- 
Brian Lunergan
Nepean, Ontario
Canada


---
avast! Antivirus: Outbound message clean.
Virus Database (VPS): 0626-2, 2006-06-28
Tested on: 2006-06-28 19:20:04
avast! is copyright (c) 2000-2006 ALWIL Software.
http://www.avast.com


From jranke at uni-bremen.de  Thu Jun 29 10:27:27 2006
From: jranke at uni-bremen.de (Johannes Ranke)
Date: Thu, 29 Jun 2006 10:27:27 +0200
Subject: [R] Extracting R plots from MS Word
In-Reply-To: <62025.58.105.171.47.1151558108.squirrel@webmail.scms.waikato.ac.nz>
References: <62025.58.105.171.47.1151558108.squirrel@webmail.scms.waikato.ac.nz>
Message-ID: <20060629082727.GA17206@mail.uft.uni-bremen.de>

Hi,

I would also like to know how to do this with MS products alone.

However, a nice tool to do this is wmf2eps (http://www.wmf2eps.de.vu/).
You can paste windows vector graphics into wmf2eps, and it first saves
it as EMF (enhanced metafile), before it creates EPS, which is what I
use it for.

Maybe your publisher will be even more happy if you supply EPS.

Best regards,

Johannes Ranke


* maj at stats.waikato.ac.nz <maj at stats.waikato.ac.nz> [060629 08:30]:
> Hi,
> 
> I am revising a paper that I am a co-author of. The figures are plots
> generated from R but at the moment I do not have the R code that generates
> them.
> 
> As this is time critical I would like to slightly abuse the list by asking
> whether anyone knows how to extract from MS Word into a stand-alone
> graphics file a plot that was pasted into Word from R (probably as a
> Windows Metafile, but possibly as a bitmap).
> 
> I would be very grateful for help with this.
> 
> Regards,  Murray Jorgensen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Dr. Johannes Ranke                 jranke at uni-bremen.de
UFT Bremen, Leobenerstr. 1         +49 421 218 8971 
D-28359 Bremen                     http://www.uft.uni-bremen.de/chemie/ranke


From p.dalgaard at biostat.ku.dk  Thu Jun 29 10:31:07 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Jun 2006 10:31:07 +0200
Subject: [R] re-direct to "more" or "less"
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC047E42D9@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC047E42D9@DJFPOST01.djf.agrsci.dk>
Message-ID: <x2wtb0z7lg.fsf@turmalin.kubism.ku.dk>

S?ren H?jsgaard <Soren.Hojsgaard at agrsci.dk> writes:

> Something like
> 
> less <- function(a){
>   fn <- paste(tempdir(),"\\dataframe.txt",sep='',collapse='')
>   write.table(a, quote=F, file=fn)
>   system(paste("less ",fn))
> }
> 
> could perhaps help you (assuming that you have less on your computer). I agree that it would be very nice to have a built-in version...


Like page(), you mean... ?

:-)

(This goes via file.show, so Windows GUI users get a separate window,
I suppose.) 

> Regards
> S?ren
> 
>  
> 
> > -----Oprindelig meddelelse-----
> > Fra: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] P? vegne af Mike Wolfgang
> > Sendt: 29. juni 2006 04:29
> > Til: R-help list
> > Emne: [R] re-direct to "more" or "less"
> > 
> > Dear list,
> > 
> > sometimes my function generates too much data and shows them 
> > on screen, i cannot view first several lines until program 
> > ends and I have to scroll my mouse up to get them. Is there 
> > any re-direction function in R to pipeline outputs to "more" 
> > or "less" type functions?
> > Thanks
> > 
> > mike
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From j.van_den_hoff at fz-rossendorf.de  Thu Jun 29 10:48:05 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Thu, 29 Jun 2006 10:48:05 +0200
Subject: [R] Help needed understanding eval,quote,expression
In-Reply-To: <Pine.LNX.4.64.0606290906130.23502@gannet.stats.ox.ac.uk>
References: <OF7E0000C5.21E7DB82-ON8625719B.00787616-8625719C.000CA1B3@americancentury.com>
	<Pine.LNX.4.64.0606290906130.23502@gannet.stats.ox.ac.uk>
Message-ID: <44A393C5.8080805@fz-rossendorf.de>

Prof Brian Ripley wrote:
> You are missing eval(parse(text=)). E.g.
> 
>> x <- list(y=list(y1="hello",y2="world"),z=list(z1="foo",z2="bar"))
> (what do you mean by the $ at the start of these lines?)
>> eval(parse(text="x$y$y1"))
> [1] "hello"
> 
> However, bear in mind
> 
>> fortune("parse")
> 
> If the answer is parse() you should usually rethink the question.
>     -- Thomas Lumley
>        R-help (February 2005)
> 
> In your indicated example you could probably use substitute() as 
> effectively.
> 
> 
> On Wed, 28 Jun 2006, toby_marks at americancentury.com wrote:
> 
>> I am trying to build up a quoted or character expression representing a
>> component in a list  in order to reference it indirectly.
>> For instance, I have a list that has data I want to pull, and another list
>> that has character vectors and/or lists of characters containing the names
>> of the components in the first list.
>>
>>
>> It seems that the way to do this is as evaluating expressions, but I seem
>> to be missing something.  The concept should be similar to the snippet
>> below:
>>
>>
>> For instance:
>>
>> $x = list(y=list(y1="hello",y2="world"),z=list(z1="foo",z2="bar"))
>> $y = quote(x$y$y1)
>> $eval(y)
>> [1] "hello"
>>
>>
>> but, I'm trying to accomplish this by building up y as a character and
>> then evaluating it, and having no success.
>>
>> $y1=paste("x$y$","y1",sep="")
>> $y1
>> [1] "x$y$y1"
>>
>>
>> How can I evaluate y1 as I did with y previously?  or can I?
>>
>>
>> Much Thanks !
>>
>>

if I understand you correctly you can achieve your goal much easier than 
with eval, parse, substitute and the like:

x <- list(y=list(y1="hello",y2="world"),z=list(z1="foo",z2="bar"))

s1 <- 'y'
s2 <- 'y1'

x[[s1]][[s2]]

i.e. using `[[' instead of `$' for list component extraction allows to 
use characters for indexing (in other words: x$y == x[['y']])

joerg


From Soren.Hojsgaard at agrsci.dk  Thu Jun 29 10:49:54 2006
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Thu, 29 Jun 2006 10:49:54 +0200
Subject: [R] re-direct to "more" or "less"
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC047E42DE@DJFPOST01.djf.agrsci.dk>

No - not like page(). Page (on windows) gives

structure(list(Sepal.Length = c(5.1, 4.9, 4.7, 4.6, 5, 5.4, 4.6, 
5, 4.4, 4.9, 5.4, 4.8, 4.8, 4.3, 5.8, 5.7, 5.4, 5.1, 5.7, 5.1, 
5.4, 5.1, 4.6, 5.1, 4.8, 5, 5, 5.2, 5.2, 4.7, 4.8, 5.4, 5.2, .....

while the less() function below gives

Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1 5.1 3.5 1.4 0.2 setosa
2 4.9 3 1.4 0.2 setosa
3 4.7 3.2 1.3 0.2 setosa
4 4.6 3.1 1.5 0.2 setosa
....

Regards
S?ren


 

> -----Oprindelig meddelelse-----
> Fra: pd at pubhealth.ku.dk [mailto:pd at pubhealth.ku.dk] P? vegne 
> af Peter Dalgaard
> Sendt: 29. juni 2006 10:31
> Til: S?ren H?jsgaard
> Cc: Mike Wolfgang; R-help list
> Emne: Re: [R] re-direct to "more" or "less"
> 
> S?ren H?jsgaard <Soren.Hojsgaard at agrsci.dk> writes:
> 
> > Something like
> > 
> > less <- function(a){
> >   fn <- paste(tempdir(),"\\dataframe.txt",sep='',collapse='')
> >   write.table(a, quote=F, file=fn)
> >   system(paste("less ",fn))
> > }
> > 
> > could perhaps help you (assuming that you have less on your 
> computer). I agree that it would be very nice to have a 
> built-in version...
> 
> 
> Like page(), you mean... ?
> 
> :-)
> 
> (This goes via file.show, so Windows GUI users get a separate 
> window, I suppose.) 
> 
> > Regards
> > S?ren
> > 
> >  
> > 
> > > -----Oprindelig meddelelse-----
> > > Fra: r-help-bounces at stat.math.ethz.ch 
> > > [mailto:r-help-bounces at stat.math.ethz.ch] P? vegne af 
> Mike Wolfgang
> > > Sendt: 29. juni 2006 04:29
> > > Til: R-help list
> > > Emne: [R] re-direct to "more" or "less"
> > > 
> > > Dear list,
> > > 
> > > sometimes my function generates too much data and shows them on 
> > > screen, i cannot view first several lines until program 
> ends and I 
> > > have to scroll my mouse up to get them. Is there any re-direction 
> > > function in R to pipeline outputs to "more"
> > > or "less" type functions?
> > > Thanks
> > > 
> > > mike
> > > 
> > > 	[[alternative HTML version deleted]]
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> 
> -- 
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: 
> (+45) 35327907
>


From p.dalgaard at biostat.ku.dk  Thu Jun 29 10:58:00 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Jun 2006 10:58:00 +0200
Subject: [R] re-direct to "more" or "less"
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC047E42DE@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC047E42DE@DJFPOST01.djf.agrsci.dk>
Message-ID: <x2odwcz6cn.fsf@turmalin.kubism.ku.dk>

S?ren H?jsgaard <Soren.Hojsgaard at agrsci.dk> writes:

> No - not like page(). Page (on windows) gives
> 
> structure(list(Sepal.Length = c(5.1, 4.9, 4.7, 4.6, 5, 5.4, 4.6, 
> 5, 4.4, 4.9, 5.4, 4.8, 4.8, 4.3, 5.8, 5.7, 5.4, 5.1, 5.7, 5.1, 
> 5.4, 5.1, 4.6, 5.1, 4.8, 5, 5, 5.2, 5.2, 4.7, 4.8, 5.4, 5.2, .....
> 
> while the less() function below gives
> 
> Sepal.Length Sepal.Width Petal.Length Petal.Width Species
> 1 5.1 3.5 1.4 0.2 setosa
> 2 4.9 3 1.4 0.2 setosa
> 3 4.7 3.2 1.3 0.2 setosa
> 4 4.6 3.1 1.5 0.2 setosa
> ....


If all else fails, read the manual...: page() has a method= argument.
(It might not be a bad idea to switch the default, though).

> > -----Oprindelig meddelelse-----
> > Fra: pd at pubhealth.ku.dk [mailto:pd at pubhealth.ku.dk] P? vegne 
> > af Peter Dalgaard
> > Sendt: 29. juni 2006 10:31
> > Til: S?ren H?jsgaard
> > Cc: Mike Wolfgang; R-help list
> > Emne: Re: [R] re-direct to "more" or "less"
> > 
> > S?ren H?jsgaard <Soren.Hojsgaard at agrsci.dk> writes:
> > 
> > > Something like
> > > 
> > > less <- function(a){
> > >   fn <- paste(tempdir(),"\\dataframe.txt",sep='',collapse='')
> > >   write.table(a, quote=F, file=fn)
> > >   system(paste("less ",fn))
> > > }
> > > 
> > > could perhaps help you (assuming that you have less on your 
> > computer). I agree that it would be very nice to have a 
> > built-in version...
> > 
> > 
> > Like page(), you mean... ?
> > 
> > :-)
> > 
> > (This goes via file.show, so Windows GUI users get a separate 
> > window, I suppose.) 
> > 
> > > Regards
> > > S?ren
> > > 
> > >  
> > > 
> > > > -----Oprindelig meddelelse-----
> > > > Fra: r-help-bounces at stat.math.ethz.ch 
> > > > [mailto:r-help-bounces at stat.math.ethz.ch] P? vegne af 
> > Mike Wolfgang
> > > > Sendt: 29. juni 2006 04:29
> > > > Til: R-help list
> > > > Emne: [R] re-direct to "more" or "less"
> > > > 
> > > > Dear list,
> > > > 
> > > > sometimes my function generates too much data and shows them on 
> > > > screen, i cannot view first several lines until program 
> > ends and I 
> > > > have to scroll my mouse up to get them. Is there any re-direction 
> > > > function in R to pipeline outputs to "more"
> > > > or "less" type functions?
> > > > Thanks
> > > > 
> > > > mike
> > > > 
> > > > 	[[alternative HTML version deleted]]
> > > > 
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list 
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide! 
> > > > http://www.R-project.org/posting-guide.html
> > > >
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > > 
> > 
> > -- 
> >    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
> >   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
> >  (*) \(*) -- University of Copenhagen   Denmark          Ph:  
> > (+45) 35327918
> > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: 
> > (+45) 35327907
> > 
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ripley at stats.ox.ac.uk  Thu Jun 29 11:06:50 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 29 Jun 2006 10:06:50 +0100 (BST)
Subject: [R] Help needed understanding eval,quote,expression
In-Reply-To: <44A393C5.8080805@fz-rossendorf.de>
References: <OF7E0000C5.21E7DB82-ON8625719B.00787616-8625719C.000CA1B3@americancentury.com>
	<Pine.LNX.4.64.0606290906130.23502@gannet.stats.ox.ac.uk>
	<44A393C5.8080805@fz-rossendorf.de>
Message-ID: <Pine.LNX.4.64.0606290953500.25166@gannet.stats.ox.ac.uk>

On Thu, 29 Jun 2006, Joerg van den Hoff wrote:

> Prof Brian Ripley wrote:
>> You are missing eval(parse(text=)). E.g.
>> 
>>> x <- list(y=list(y1="hello",y2="world"),z=list(z1="foo",z2="bar"))
>> (what do you mean by the $ at the start of these lines?)
>>> eval(parse(text="x$y$y1"))
>> [1] "hello"
>> 
>> However, bear in mind
>> 
>>> fortune("parse")
>> 
>> If the answer is parse() you should usually rethink the question.
>>     -- Thomas Lumley
>>        R-help (February 2005)
>> 
>> In your indicated example you could probably use substitute() as 
>> effectively.
>> 
>> 
>> On Wed, 28 Jun 2006, toby_marks at americancentury.com wrote:
>> 
>>> I am trying to build up a quoted or character expression representing a
>>> component in a list  in order to reference it indirectly.
>>> For instance, I have a list that has data I want to pull, and another list
>>> that has character vectors and/or lists of characters containing the names
>>> of the components in the first list.
>>> 
>>> It seems that the way to do this is as evaluating expressions, but I seem
>>> to be missing something.  The concept should be similar to the snippet
>>> below:
>>> 
>>> 
>>> For instance:
>>> 
>>> $x = list(y=list(y1="hello",y2="world"),z=list(z1="foo",z2="bar"))
>>> $y = quote(x$y$y1)
>>> $eval(y)
>>> [1] "hello"
>>> 
>>> 
>>> but, I'm trying to accomplish this by building up y as a character and
>>> then evaluating it, and having no success.
>>> 
>>> $y1=paste("x$y$","y1",sep="")
>>> $y1
>>> [1] "x$y$y1"
>>> 
>>> 
>>> How can I evaluate y1 as I did with y previously?  or can I?
>>> 
>>> 
>>> Much Thanks !
>>> 
>>> 
>
> if I understand you correctly you can achieve your goal much easier than with 
> eval, parse, substitute and the like:
>
> x <- list(y=list(y1="hello",y2="world"),z=list(z1="foo",z2="bar"))
>
> s1 <- 'y'
> s2 <- 'y1'
>
> x[[s1]][[s2]]
>
> i.e. using `[[' instead of `$' for list component extraction allows to use 
> characters for indexing (in other words: x$y == x[['y']])


But what he actually asked for was

>>> I am trying to build up a quoted or character expression representing a
>>> component in a list  in order to reference it indirectly.

You just typed in x[[s1]][[s2]], not 'built [it] up'.  Suppose the 
specification had been

r <- "x"
s <- c("y", "y1")

and s was of variable length?  Then you need to construct a call similar 
to x[["y"]][["y1"]] from r and s.

[There was another reason for sticking with $ rather than using [[: the 
latter makes unnecessary copies in released versions of R.]


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Thu Jun 29 11:15:42 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 29 Jun 2006 10:15:42 +0100 (BST)
Subject: [R] re-direct to "more" or "less"
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC047E42DE@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC047E42DE@DJFPOST01.djf.agrsci.dk>
Message-ID: <Pine.LNX.4.64.0606291012550.25166@gannet.stats.ox.ac.uk>

Please read the help before replying ...

> page(iris, method="print")

does exactly what I understand was asked for.

On Thu, 29 Jun 2006, S?ren H?jsgaard wrote:

> No - not like page(). Page (on windows) gives
>
> structure(list(Sepal.Length = c(5.1, 4.9, 4.7, 4.6, 5, 5.4, 4.6,
> 5, 4.4, 4.9, 5.4, 4.8, 4.8, 4.3, 5.8, 5.7, 5.4, 5.1, 5.7, 5.1,
> 5.4, 5.1, 4.6, 5.1, 4.8, 5, 5, 5.2, 5.2, 4.7, 4.8, 5.4, 5.2, .....
>
> while the less() function below gives
>
> Sepal.Length Sepal.Width Petal.Length Petal.Width Species
> 1 5.1 3.5 1.4 0.2 setosa
> 2 4.9 3 1.4 0.2 setosa
> 3 4.7 3.2 1.3 0.2 setosa
> 4 4.6 3.1 1.5 0.2 setosa
> ....
>
> Regards
> S?ren
>
>
>
>
>> -----Oprindelig meddelelse-----
>> Fra: pd at pubhealth.ku.dk [mailto:pd at pubhealth.ku.dk] P? vegne
>> af Peter Dalgaard
>> Sendt: 29. juni 2006 10:31
>> Til: S?ren H?jsgaard
>> Cc: Mike Wolfgang; R-help list
>> Emne: Re: [R] re-direct to "more" or "less"
>>
>> S?ren H?jsgaard <Soren.Hojsgaard at agrsci.dk> writes:
>>
>>> Something like
>>>
>>> less <- function(a){
>>>   fn <- paste(tempdir(),"\\dataframe.txt",sep='',collapse='')
>>>   write.table(a, quote=F, file=fn)
>>>   system(paste("less ",fn))
>>> }
>>>
>>> could perhaps help you (assuming that you have less on your
>> computer). I agree that it would be very nice to have a
>> built-in version...
>>
>>
>> Like page(), you mean... ?
>>
>> :-)
>>
>> (This goes via file.show, so Windows GUI users get a separate
>> window, I suppose.)
>>
>>> Regards
>>> S?ren
>>>
>>>
>>>
>>>> -----Oprindelig meddelelse-----
>>>> Fra: r-help-bounces at stat.math.ethz.ch
>>>> [mailto:r-help-bounces at stat.math.ethz.ch] P? vegne af
>> Mike Wolfgang
>>>> Sendt: 29. juni 2006 04:29
>>>> Til: R-help list
>>>> Emne: [R] re-direct to "more" or "less"
>>>>
>>>> Dear list,
>>>>
>>>> sometimes my function generates too much data and shows them on
>>>> screen, i cannot view first several lines until program
>> ends and I
>>>> have to scroll my mouse up to get them. Is there any re-direction
>>>> function in R to pipeline outputs to "more"
>>>> or "less" type functions?
>>>> Thanks
>>>>
>>>> mike
>>>>
>>>> 	[[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide!
>>>> http://www.R-project.org/posting-guide.html
>>>>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>>
>>
>> --
>>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>>  (*) \(*) -- University of Copenhagen   Denmark          Ph:
>> (+45) 35327918
>> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX:
>> (+45) 35327907
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From andy_liaw at merck.com  Thu Jun 29 00:17:12 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 28 Jun 2006 18:17:12 -0400
Subject: [R] Running R  [Broadcast]
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA01BD0E18@usctmx1106.merck.com>

You have two choices (AFAIK): 
 
1.  Tell configure exactly where to find the readline header files (by
something like --with-readline=/where/the/files/are, check configure
--help).
 
2.  Compile R without readline; i.e., configure --with-readline=no.
 
Andy

  _____  

From: r-help-bounces at stat.math.ethz.ch on behalf of Pramod Anugu
Sent: Wed 6/28/2006 4:18 PM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] Running R [Broadcast]



[root at genetics R-2.3.0]# ./configure 
checking build system type... x86_64-unknown-linux-gnu 
checking host system type... x86_64-unknown-linux-gnu 
loading site script './config.site' 
loading build specific script './config.site' 
checking for pwd... /bin/pwd 
checking whether builddir is srcdir... yes 
checking for working aclocal... found 
checking for working autoconf... found 
checking for working automake... found 
checking for working autoheader... found 
checking for working makeinfo... found 
checking for gawk... gawk 
checking for egrep... grep -E 
checking whether ln -s works... yes 
checking for ranlib... ranlib 
checking for bison... no 
checking for byacc... no 
checking for ar... ar 
checking for a BSD-compatible install... /usr/bin/install -c 
checking for sed... /bin/sed 
checking for less... /usr/bin/less 
checking for perl... /usr/bin/perl 
checking whether perl version is at least 5.004... yes 
checking for dvips... /usr/bin/dvips 
checking for tex... /usr/bin/tex 
checking for latex... /usr/bin/latex 
checking for makeindex... /usr/bin/makeindex 
checking for pdftex... /usr/bin/pdftex 
checking for pdflatex... /usr/bin/pdflatex 
checking for makeinfo... /usr/bin/makeinfo 
checking for unzip... /usr/bin/unzip 
checking for zip... /usr/bin/zip 
checking for gzip... /bin/gzip 
checking for firefox... /usr/bin/firefox 
using default browser ... /usr/bin/firefox 
checking for acroread... no 
checking for acroread4... no 
checking for xpdf... no 
checking for gv... no 
checking for gnome-gv... no 
checking for ggv... /usr/bin/ggv 
checking for gcc... gcc 
checking for C compiler default output file name... a.out 
checking whether the C compiler works... yes 
checking whether we are cross compiling... no 
checking for suffix of executables... 
checking for suffix of object files... o 
checking whether we are using the GNU C compiler... yes 
checking whether gcc accepts -g... yes 
checking for gcc option to accept ANSI C... none needed 
checking how to run the C preprocessor... gcc -E 
checking whether gcc needs -traditional... no 
checking how to run the C preprocessor... gcc -E 
checking for g77... g77 
checking whether we are using the GNU Fortran 77 compiler... yes 
checking whether g77 accepts -g... yes 
checking for g++... g++ 
checking whether we are using the GNU C++ compiler... yes 
checking whether g++ accepts -g... yes 
checking how to run the C++ preprocessor... g++ -E 
checking whether __attribute__((visibility())) is supported... yes 
checking whether gcc accepts -fvisibility... yes 
checking whether g77 accepts -fvisibility... yes 
checking for a sed that does not truncate output... /bin/sed 
checking for ld used by gcc... /usr/bin/ld 
checking if the linker (/usr/bin/ld) is GNU ld... yes 
checking for /usr/bin/ld option to reload object files... -r 
checking for BSD-compatible nm... /usr/bin/nm -B 
checking how to recognise dependent libraries... pass_all 
checking for ANSI C header files... yes 
checking for sys/types.h... yes 
checking for sys/stat.h... yes 
checking for stdlib.h... yes 
checking for string.h... yes 
checking for memory.h... yes 
checking for strings.h... yes 
checking for inttypes.h... yes 
checking for stdint.h... yes 
checking for unistd.h... yes 
checking dlfcn.h usability... yes 
checking dlfcn.h presence... yes 
checking for dlfcn.h... yes 
checking the maximum length of command line arguments... 32768 
checking command to parse /usr/bin/nm -B output from gcc object... ok 
checking for objdir... .libs 
checking for ranlib... (cached) ranlib 
checking for strip... strip 
checking if gcc static flag  works... yes 
checking if gcc supports -fno-rtti -fno-exceptions... no 
checking for gcc option to produce PIC... -fPIC 
checking if gcc PIC flag -fPIC works... yes 
checking if gcc supports -c -o file.o... yes 
checking whether the gcc linker (/usr/bin/ld -m elf_x86_64) supports shared 
libraries... yes 
checking whether -lc should be explicitly linked in... no 
checking dynamic linker characteristics... GNU/Linux ld.so 
checking how to hardcode library paths into programs... immediate 
checking whether stripping libraries is possible... yes 
checking if libtool supports shared libraries... yes 
checking whether to build shared libraries... yes 
checking whether to build static libraries... no 
configure: creating libtool 
appending configuration tag "CXX" to libtool 
checking for ld used by g++... /usr/bin/ld -m elf_x86_64 
checking if the linker (/usr/bin/ld -m elf_x86_64) is GNU ld... yes 
checking whether the g++ linker (/usr/bin/ld -m elf_x86_64) supports shared 
libraries... yes 
checking for g++ option to produce PIC... -fPIC 
checking if g++ PIC flag -fPIC works... yes 
checking if g++ supports -c -o file.o... yes 
checking whether the g++ linker (/usr/bin/ld -m elf_x86_64) supports shared 
libraries... yes 
checking dynamic linker characteristics... GNU/Linux ld.so 
checking how to hardcode library paths into programs... immediate 
checking whether stripping libraries is possible... yes 
appending configuration tag "F77" to libtool 
checking if libtool supports shared libraries... yes 
checking whether to build shared libraries... yes 
checking whether to build static libraries... no 
checking for g77 option to produce PIC... -fPIC 
checking if g77 PIC flag -fPIC works... yes 
checking if g77 supports -c -o file.o... yes 
checking whether the g77 linker (/usr/bin/ld -m elf_x86_64) supports shared 
libraries... yes 
checking dynamic linker characteristics... GNU/Linux ld.so 
checking how to hardcode library paths into programs... immediate 
checking whether stripping libraries is possible... yes 
checking whether makeinfo version is at least 4.7... yes 
checking for cos in -lm... yes 
checking for sin in -lm... yes 
checking for dlopen in -ldl... yes 
checking readline/history.h usability... no 
checking readline/history.h presence... no 
checking for readline/history.h... no 
checking readline/readline.h usability... no 
checking readline/readline.h presence... no 
checking for readline/readline.h... no 
checking for rl_callback_read_char in -lreadline... no 
checking for main in -lncurses... yes 
checking for rl_callback_read_char in -lreadline... no 
checking for history_truncate_file... no 
configure: error: --with-readline=yes (default) and headers/libs are not 
available 

--One suggestion was to install readline-devel. But according to the below I

already have readline-devel. 

# find . -iname readline-devel\* 
 find -iname readline-devel\* 
./os/4.1/x86_64/RedHat/RPMS/readline-devel-4.3-13.x86_64.rpm 

#rpm -ivh readline*.rpm 
Preparing...                ########################################### 
[100%] 
        package readline-4.3-13 is already installed 

Once you've found an RPM, to install it on the frontend, just type 
  
# rpm -ivh filename.rpm 

------------- 

   1. tar -zxvf R-2.3.0.tar.gz 
   2. changed the directory to the newly created directory R-2.3.0 
   3. Typed ./configure 
  
  
  
   4. Typed make 
   make: *** No targets specified and no makefile found.  Stop. 
   I cannot run make. Please let me know. 
   
   *5. Make check 
   *6. make check-all 
   *7. Typed make install 
   *8. Typed R 

______________________________________________ 
R-help at stat.math.ethz.ch mailing list 
https://stat.ethz.ch/mailman/listinfo/r-help
<https://stat.ethz.ch/mailman/listinfo/r-help>  
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
<http://www.R-project.org/posting-guide.html>


From mendes.richard at gmail.com  Thu Jun 29 11:23:17 2006
From: mendes.richard at gmail.com (richard mendes)
Date: Thu, 29 Jun 2006 11:23:17 +0200
Subject: [R] kmeans clustering
Message-ID: <5c5fa360606290223v5f3f052eoa42abbd24d977d20@mail.gmail.com>

Hello R list members,

I'm a bio informatics student from the Leiden university
(netherlands). We were asked to make a program with different
clustering methods. The problem we are experiencing is the following.

we have a matrix with data like the following

                 research1    research2    research3    enz
sample1      0.5             0.2              0.4

sample2      0.4             0.4              0.3

sample3      0.7             0.2              0.8

enz


now if we use kmeans(matrix,3,20) the clustering method will cluster
only on rows by using the columns values as multiple variables. The
output from this syntax is for example

sample1 = 1
sample2 = 2
sample3 = 3
enz

Is there a method that will use the rows and columns as seperate
values so that every variable in the matrix will be assigned to a
cluster instead of a row.

the output then could be interperted as a heatmap.

thank you in advance for your time,

Richard Mendes


From andrej.kastrin at siol.net  Thu Jun 29 11:40:29 2006
From: andrej.kastrin at siol.net (Andrej Kastrin)
Date: Thu, 29 Jun 2006 11:40:29 +0200
Subject: [R] kmeans clustering
In-Reply-To: <5c5fa360606290223v5f3f052eoa42abbd24d977d20@mail.gmail.com>
References: <5c5fa360606290223v5f3f052eoa42abbd24d977d20@mail.gmail.com>
Message-ID: <44A3A00D.4010600@siol.net>

richard mendes wrote:
> Hello R list members,
>
> I'm a bio informatics student from the Leiden university
> (netherlands). We were asked to make a program with different
> clustering methods. The problem we are experiencing is the following.
>
> we have a matrix with data like the following
>
>                  research1    research2    research3    enz
> sample1      0.5             0.2              0.4
>
> sample2      0.4             0.4              0.3
>
> sample3      0.7             0.2              0.8
>
> enz
>
>
> now if we use kmeans(matrix,3,20) the clustering method will cluster
> only on rows by using the columns values as multiple variables. The
> output from this syntax is for example
>
> sample1 = 1
> sample2 = 2
> sample3 = 3
> enz
>
> Is there a method that will use the rows and columns as seperate
> values so that every variable in the matrix will be assigned to a
> cluster instead of a row.
>
> the output then could be interperted as a heatmap.
>
> thank you in advance for your time,
>
> Richard Mendes
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>   
You mean something like:?
kmeans(dataMatrix,5) ## clustering by rows
kmeans(t(dataMatrix),5) ## clustering by attribute

HTH, Andrej


From p.murrell at auckland.ac.nz  Thu Jun 29 00:46:50 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Thu, 29 Jun 2006 10:46:50 +1200
Subject: [R] Mixing grid and base graphics--need help understanding this
 quirk
In-Reply-To: <adf71a630606271331h17c763b8qe21415c2a74fc25a@mail.gmail.com>
References: <adf71a630606271331h17c763b8qe21415c2a74fc25a@mail.gmail.com>
Message-ID: <44A306DA.3010902@stat.auckland.ac.nz>

Hi


Kevin Wright wrote:
> My setup: Windows 2000, R 2.3.1
> 
> When I start a brand new session of R and paste the code below into R,
> the graphic device shows "Some text" in the lower left corner.  If I
> paste the code into the command window again, then "Some text" does
> not appear in the lower left corner.  Why is this?
> 
> require(grid)
> par(mfrow=c(1,2))
> plot(1:10)
> plot(-10:1)
> par(mfrow=c(1,1))
> pushViewport(viewport(.04, .04, width=stringWidth("Some text"),
> height=unit(2,"lines"),
>                       name="pagenum", gp=gpar(fontsize=10)))
> grid.text("Some text", gp=gpar(col="gray30"),
>           just=c("left","bottom"))
> popViewport()


Good question! :)

Short answer:
Traditional and grid graphics are fighting each other for control of the
clipping region.

Simple fix/workaround:
Explicitly set the clipping region for the grid output, like this ...

require(grid)
par(mfrow=c(1,2))
plot(1:10)
plot(-10:1)
par(mfrow=c(1,1))
# Requires R >= 2.3.0
grid.clip()
pushViewport(viewport(.04, .04, width=stringWidth("Some text"),
height=unit(2,"lines"),
                      name="pagenum", gp=gpar(fontsize=10)))
grid.text("Some text", gp=gpar(col="gray30"),
          just=c("left","bottom"))
popViewport()

Long answer:
The first time you run the code, the pushViewport() call is the first
grid operation on the graphics device, so some initial grid set up
occurs, *including setting the clipping region to the whole device*.
The second time you run the code, the pushViewport() call just inherits
the current clipping region (which is the one set by the last
traditional plot) so the text gets clipped.  You can see this effect,
with the following code (the text does not even appear the first time
because the grid initialisation occurs before the traditional plots) ...

require(grid)
grid.newpage()
par(mfrow=c(1,2))
plot(1:10)
plot(-10:1)
par(mfrow=c(1,1))
pushViewport(viewport(.04, .04, width=stringWidth("Some text"),
height=unit(2,"lines"),
                      name="pagenum", gp=gpar(fontsize=10)))
grid.text("Some text", gp=gpar(col="gray30"),
          just=c("left","bottom"))
popViewport()

... so really the problem is that the text should never appear.

Interestingly, if you run your original code (text is drawn), then run
it again (text is not drawn), then ONLY run the grid part (push, text,
pop) the text DOES get drawn!  This happens because the popViewport()
restores the clipping region of the previous grid viewport (which is the
whole device).

I need to think a bit more about whether this behaviour can be "fixed",
whether it should be fixed, or whether the current "user beware" warning
about mixing traditional and grid graphics is sufficient.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From p_connolly at ihug.co.nz  Thu Jun 29 00:45:37 2006
From: p_connolly at ihug.co.nz (Patrick Connolly)
Date: Thu, 29 Jun 2006 10:45:37 +1200
Subject: [R] Very slow read.table on Linux,
	compared to Win2000 [Broad cast]
In-Reply-To: <200606281643.29459.amurta@ipimar.pt>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02884265@usctmx1106.merck.com>
	<200606281643.29459.amurta@ipimar.pt>
Message-ID: <20060628224537.GO4970@ihug.co.nz>

On Wed, 28-Jun-2006 at 04:43PM -0700, Alberto Murta wrote:

|> I have a pentium 4 pc with 256 MB of RAM, so I made a text file, tab 
|> separated, with column names and 15000 x 483 integers:
|> 
|> > system("ls -l teste.txt")
|> -rw-r--r--  1 amurta  amurta  16998702 Jun 28 16:08 teste.txt
|> 
|> the time it took to import it was around 15 secs:
|> 
|> > system.time(teste <- read.delim("teste.txt"))
|> [1] 15.349  0.244 16.597  0.000  0.000
|> 
|> so I think lack of RAM must not be the main problem.
|> Cheers
|> 
|> Alberto
|> 
|> 
|> > version
|>                _
|> platform       i386-unknown-freebsd6.1
|> arch           i386
|> os             freebsd6.1
|> system         i386, freebsd6.1
|> status
|> major          2
|> minor          3.1
|> year           2006
|> month          06
|> day            01
|> svn rev        38247
|> language       R
|> version.string Version 2.3.1 (2006-06-01)

Doesn't tell us about your window manager.  I'd suspect you're using a
light one.



-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From bxc at steno.dk  Thu Jun 29 11:34:41 2006
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Thu, 29 Jun 2006 11:34:41 +0200
Subject: [R] zero.print in print.table after adding margins
Message-ID: <40D3930AC1C8EA469E39536E5BC80835023FB024@EXDKBA021.corp.novocorp.net>

The function addmargins() adds margins to a table, but returns a matrix.
But even after converted to a table the print.zero="." option of
print.table() does not work:

> x <- sample( 1:7, 20, replace=T )
> y <- sample( 1:7, 20, replace=T )
> tt <- table( x, y )
> tx <- as.table( addmargins( table( x, y ) ) )
> print( tt, zero.print="." )
   y
x   1 2 3 4 5 6 7
  1 1 2 2 . . 1 .
  2 1 . . 1 . . .
  3 . . . . . . 2
  4 1 . . . . 1 .
  5 1 . 1 . . 1 .
  6 . 1 . 1 . . .
  7 . . 1 . 1 1 .
> print( tx, zero.print="." )
     y
x      1  2  3  4  5  6  7 Sum
  1    1  2  2  0  0  1  0   6
  2    1  0  0  1  0  0  0   2
  3    0  0  0  0  0  0  2   2
  4    1  0  0  0  0  1  0   2
  5    1  0  1  0  0  1  0   3
  6    0  1  0  1  0  0  0   2
  7    0  0  1  0  1  1  0   3
  Sum  4  3  4  2  1  4  2  20

Is this a facility of print.table?
The attributes() of tt and tx have identical stucture. 

Best,
Bendix
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc


From p.dalgaard at biostat.ku.dk  Thu Jun 29 11:54:38 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Jun 2006 11:54:38 +0200
Subject: [R] Running R  [Broadcast]
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA01BD0E18@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA01BD0E18@usctmx1106.merck.com>
Message-ID: <x2bqsc9ti9.fsf@viggo.kubism.ku.dk>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> You have two choices (AFAIK): 
>  
> 1.  Tell configure exactly where to find the readline header files (by
> something like --with-readline=/where/the/files/are, check configure
> --help).
>  
> 2.  Compile R without readline; i.e., configure --with-readline=no.
>  
> Andy


But first check sequencing - I'm pretty sure that configure can find
header files on RedHat if they are there. 

He's not too good at interpreting his output:

> --One suggestion was to install readline-devel. But according to the below I
> 
> already have readline-devel. 
> 
> # find . -iname readline-devel\* 
>  find -iname readline-devel\* 
> ./os/4.1/x86_64/RedHat/RPMS/readline-devel-4.3-13.x86_64.rpm 
> 
> #rpm -ivh readline*.rpm 
> Preparing...                ########################################### 
> [100%] 
>         package readline-4.3-13 is already installed 

and as far as I can see, this means that readline-devel got installed
and readline (NOT -devel) was there already. So readline-devel is
there now, but wasn't there before. Now did he rerun configure? If so,
did it fail with a different message?



-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From amurta at ipimar.pt  Thu Jun 29 20:10:42 2006
From: amurta at ipimar.pt (Alberto Murta)
Date: Thu, 29 Jun 2006 11:10:42 -0700
Subject: [R] Very slow read.table on Linux,
	compared to Win2000 [Broad cast]
In-Reply-To: <20060628224537.GO4970@ihug.co.nz>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02884265@usctmx1106.merck.com>
	<200606281643.29459.amurta@ipimar.pt>
	<20060628224537.GO4970@ihug.co.nz>
Message-ID: <200606291110.43499.amurta@ipimar.pt>

On Wednesday 28 June 2006 15:45, Patrick Connolly wrote:
> On Wed, 28-Jun-2006 at 04:43PM -0700, Alberto Murta wrote:
> |> I have a pentium 4 pc with 256 MB of RAM, so I made a text file, tab
> |>
> |> separated, with column names and 15000 x 483 integers:
> |> > system("ls -l teste.txt")
> |>
> |> -rw-r--r--  1 amurta  amurta  16998702 Jun 28 16:08 teste.txt
> |>
> |> the time it took to import it was around 15 secs:
> |> > system.time(teste <- read.delim("teste.txt"))
> |>
> |> [1] 15.349  0.244 16.597  0.000  0.000
> |>
> |> so I think lack of RAM must not be the main problem.
> |> Cheers
> |>
> |> Alberto
> |>
> |> > version
> |>
> |>                _
> |> platform       i386-unknown-freebsd6.1
> |> arch           i386
> |> os             freebsd6.1
> |> system         i386, freebsd6.1
> |> status
> |> major          2
> |> minor          3.1
> |> year           2006
> |> month          06
> |> day            01
> |> svn rev        38247
> |> language       R
> |> version.string Version 2.3.1 (2006-06-01)
>
> Doesn't tell us about your window manager.  I'd suspect you're using a
> light one.

I'm using KDE 3.5.2 (a not so light one) and running R in emacs.

-- 
  Alberto G. Murta
IPIMAR - Institute of Fisheries and Marine Research 
Avenida de Brasilia; 1449-006 Lisboa; Portugal
Tel: +351 213027120; Fax: +351 213015948


From p.dalgaard at biostat.ku.dk  Thu Jun 29 12:18:13 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Jun 2006 12:18:13 +0200
Subject: [R] zero.print in print.table after adding margins
In-Reply-To: <40D3930AC1C8EA469E39536E5BC80835023FB024@EXDKBA021.corp.novocorp.net>
References: <40D3930AC1C8EA469E39536E5BC80835023FB024@EXDKBA021.corp.novocorp.net>
Message-ID: <x264ik9sey.fsf@viggo.kubism.ku.dk>

"BXC (Bendix Carstensen)" <bxc at steno.dk> writes:

> The function addmargins() adds margins to a table, but returns a matrix.
> But even after converted to a table the print.zero="." option of
> print.table() does not work:
> 
> > x <- sample( 1:7, 20, replace=T )
> > y <- sample( 1:7, 20, replace=T )
> > tt <- table( x, y )
> > tx <- as.table( addmargins( table( x, y ) ) )
> > print( tt, zero.print="." )
>    y
> x   1 2 3 4 5 6 7
>   1 1 2 2 . . 1 .
>   2 1 . . 1 . . .
>   3 . . . . . . 2
>   4 1 . . . . 1 .
>   5 1 . 1 . . 1 .
>   6 . 1 . 1 . . .
>   7 . . 1 . 1 1 .
> > print( tx, zero.print="." )
>      y
> x      1  2  3  4  5  6  7 Sum
>   1    1  2  2  0  0  1  0   6
>   2    1  0  0  1  0  0  0   2
>   3    0  0  0  0  0  0  2   2
>   4    1  0  0  0  0  1  0   2
>   5    1  0  1  0  0  1  0   3
>   6    0  1  0  1  0  0  0   2
>   7    0  0  1  0  1  1  0   3
>   Sum  4  3  4  2  1  4  2  20
> 
> Is this a facility of print.table?
> The attributes() of tt and tx have identical stucture. 

It appears to be intentional.

print.table has

    if (is.integer(x) && zero.print != "0" && any(i0 <- !ina &
        x == 0))
        xx[i0] <- sub("0", zero.print, xx[i0])

and of course,

> storage.mode(tx)
[1] "double"
> storage.mode(tt)
[1] "integer"

The reason could be that it is not entirely clear what to expect for
values that are zero up to round-off.

storage.mode(tx) <- "integer" fixes things up.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From dvumani at hotmail.com  Thu Jun 29 12:18:18 2006
From: dvumani at hotmail.com (Vumani Dlamini)
Date: Thu, 29 Jun 2006 10:18:18 +0000
Subject: [R] using "rbinom" in C code gives me erroneous results... random
 variable is not random (always zero)...
Message-ID: <BAY110-W3A486E14516F2A444550EA37C0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060629/92f370e4/attachment.pl 

From ggrothendieck at gmail.com  Thu Jun 29 12:22:09 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 29 Jun 2006 06:22:09 -0400
Subject: [R] Extracting R plots from MS Word
In-Reply-To: <62025.58.105.171.47.1151558108.squirrel@webmail.scms.waikato.ac.nz>
References: <62025.58.105.171.47.1151558108.squirrel@webmail.scms.waikato.ac.nz>
Message-ID: <971536df0606290322p1e1d7e09s88fa6649514b4896@mail.gmail.com>

Click the graphic, press ctrl-C to copy it to the clipboard and then
using ctrl-V paste it into mspaint or Xnview (free, find it via google) or other
graphics program and then save it from there.   Which program will
work will depend on the format of the image.

Another possibility is to save the Word file in HTML format.  In
Word, choose File | SaveAs and choose the Save As Type to be Web Page.
That will create an HTML file plus it will create a folder with
one file per image.

On 6/29/06, maj at stats.waikato.ac.nz <maj at stats.waikato.ac.nz> wrote:
> Hi,
>
> I am revising a paper that I am a co-author of. The figures are plots
> generated from R but at the moment I do not have the R code that generates
> them.
>
> As this is time critical I would like to slightly abuse the list by asking
> whether anyone knows how to extract from MS Word into a stand-alone
> graphics file a plot that was pasted into Word from R (probably as a
> Windows Metafile, but possibly as a bitmap).
>
> I would be very grateful for help with this.
>
> Regards,  Murray Jorgensen
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Thu Jun 29 12:26:58 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 29 Jun 2006 06:26:58 -0400
Subject: [R] re-direct to "more" or "less"
In-Reply-To: <e668df8c0606281928l3ec50963sd630e6f1f55f6e61@mail.gmail.com>
References: <e668df8c0606281928l3ec50963sd630e6f1f55f6e61@mail.gmail.com>
Message-ID: <971536df0606290326q4ea169c3q991ccf66d5d08b1f@mail.gmail.com>

The first few lines or last few lines of an object can often be viewed
like this:

head(iris)
tail(iris)



On 6/28/06, Mike Wolfgang <mikewolfgang at gmail.com> wrote:
> Dear list,
>
> sometimes my function generates too much data and shows them on screen, i
> cannot view first several lines until program ends and I have to scroll my
> mouse up to get them. Is there any re-direction function in R to pipeline
> outputs to "more" or "less" type functions?
> Thanks
>
> mike
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From baron at psych.upenn.edu  Thu Jun 29 12:53:24 2006
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Thu, 29 Jun 2006 06:53:24 -0400
Subject: [R] Extracting R plots from MS Word
In-Reply-To: <20060629082727.GA17206@mail.uft.uni-bremen.de>
References: <62025.58.105.171.47.1151558108.squirrel@webmail.scms.waikato.ac.nz>
	<20060629082727.GA17206@mail.uft.uni-bremen.de>
Message-ID: <20060629105324.GA10660@psych.upenn.edu>

Another way to do this is with OpenOffice (OO).  Read the Word file
into OO Writer.  Cut the figure.  Open OO Draw.  Click Paste, and 
the figure appears in Draw.  Save it as EPS, or whatever you
like.  This works for both EMF and WMF.

> * maj at stats.waikato.ac.nz <maj at stats.waikato.ac.nz> [060629 08:30]:
> > As this is time critical I would like to slightly abuse the list by asking
> > whether anyone knows how to extract from MS Word into a stand-alone
> > graphics file a plot that was pasted into Word from R (probably as a
> > Windows Metafile, but possibly as a bitmap).

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
Editor: Judgment and Decision Making (http://journal.sjdm.org)


From ripley at stats.ox.ac.uk  Thu Jun 29 13:00:34 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 29 Jun 2006 12:00:34 +0100 (BST)
Subject: [R] using "rbinom" in C code gives me erroneous results...
 random variable is not random (always zero)...
In-Reply-To: <BAY110-W3A486E14516F2A444550EA37C0@phx.gbl>
References: <BAY110-W3A486E14516F2A444550EA37C0@phx.gbl>
Message-ID: <Pine.LNX.4.64.0606291155560.28926@gannet.stats.ox.ac.uk>

Your text has lost almost all the carriage returns: please do as we ask 
and not send HTML code.

You appear not to be initializing the random seed in your code: the manual 
says:

   The C code behind @R{}'s @code{r at var{xxx}} functions can be accessed by
   including the header file @file{Rmath.h}; @xref{Distribution
   functions}.  Those calls generate a single variate and should also be
   enclosed in calls to @code{GetRNGstate} and @code{PutRNGstate}.

Why have you not done so?

On Thu, 29 Jun 2006, Vumani Dlamini wrote:

> Dear Listers,
> I am trying to use "rbinom" in my C code, but i always get zeros as output no matter the probability. Am not sure what I am doing wrong because the function has worked before. Attached in an example. Noticed that "rbinom" expects 'n' to be REAL.
> Regards, Vumani
>
> R  2.3.1 (2006-06-01)
> Windows XP
> Gcc
> /* Called this file binom.c and then ran rcmd shlib on it */#include <R.h>#include <Rmath.h>#include <math.h>#include <Rdefines.h>SEXP binomial(SEXP r, SEXP n, SEXP p){ int i; SEXP out; PROTECT(out = allocVector(REALSXP,INTEGER(r)[0]));  for(i = 0; i < INTEGER(r)[0]; ++i){  REAL(out)[i] = rbinom(REAL(n)[0],REAL(p)[0]); } Rprintf("%d   %f   %f\n",INTEGER(r)[0],REAL(n)[0],REAL(p)[0]); UNPROTECT(1); return(out);}
>
> ### used theses line in R
> dyn.load("binom.dll").Call("binomial",as.integer(10),as.double(1),as.double(0.4))dyn.unload("binom.dll")
> _________________________________________________________________
> Try Live.com - your fast, personalized homepage with all the things you care about in one place.
> http://www.live.com/getstarted
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From pingping.zheng at lancaster.ac.uk  Thu Jun 29 13:04:50 2006
From: pingping.zheng at lancaster.ac.uk (Pingping Zheng)
Date: Thu, 29 Jun 2006 12:04:50 +0100
Subject: [R] Package Spatialkernel Annocement
Message-ID: <44A3B3D2.9060106@lancs.ac.uk>

Hi,

A new R package spatialkernel has been accepted by CRAN. Spatialkernel
focuses on implementation of kernel regression methods proposed in
Diggle P.J. et al (2005), J. R. Stat. Soc. C, 54, 3, 645-658
and kernel density estimation of inhomogeneous Poisson point process
with edge-correction proposed by
M. Berman and P.J. Diggle (1989), J. R. Stat. Soc, B, 51, 81?92,
for an arbitrary polygonal area.

The package source and Windows binary are available at
http://www.maths.lancs.ac.uk/~zhengp1/spatialkernel/
Examples in spatialkernel-package shows a quick run of some basic
functionality.

Sorry for the wrong linked URL address at CRAN archives.

Best wishes


-- 
Dr Pingping Zheng
Department of Mathematics and Statistics
Fylde College
Lancaster University
Lancaster LA1 4YF
UK


From przeszczepan at o2.pl  Thu Jun 29 13:23:55 2006
From: przeszczepan at o2.pl (=?UTF-8?Q?przeszczepan?=)
Date: Thu, 29 Jun 2006 13:23:55 +0200
Subject: [R] =?utf-8?q?numerical_integration_problem?=
Message-ID: <6850e508.74dbe543.44a3b84b.b7754@o2.pl>

Hi,

I have got problems integrating the following function using "integrate": 

lambdat<-function(t){
tempT<-T[k,][!is.na(T[k,])]#available values from k-th row of matrix T
tempJ<-J[k,][!is.na(J[k,])]

hg<-length(tempT[tempT<=t & tempJ==0])#counts observations satisfing the conditions
ag<-length(tempT[tempT<=t & tempJ==1])

lambdaXY[hg+1,ag+1]#takes  values from a 10x10 matrix
}

I keep receiving this message:

1: longer object length
        is not a multiple of shorter object length in: tempT <= t 
2: longer object length
        is not a multiple of shorter object length in: tempT <= t & tempJ == 0 

What I suspect is that the "integrate" function submits the whole vector of points at which the integral is to be evaluated at once. For my function to be integrated it would rather have to be evaluated at each point after another in a loop of some kind.

Can you think of a way to solve this problem without me having to write the integrating procedure from scratch (I have no idea about FORTRAN and this is what the "integrate" description refers to)?

Thank you.

Kind Regards,
Lukasz Szczepanski
Student


From sundar.dorai-raj at pdf.com  Thu Jun 29 13:38:55 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 29 Jun 2006 06:38:55 -0500
Subject: [R] numerical integration problem
In-Reply-To: <6850e508.74dbe543.44a3b84b.b7754@o2.pl>
References: <6850e508.74dbe543.44a3b84b.b7754@o2.pl>
Message-ID: <44A3BBCF.3010206@pdf.com>



przeszczepan wrote:
> Hi,
> 
> I have got problems integrating the following function using "integrate": 
> 
> lambdat<-function(t){
> tempT<-T[k,][!is.na(T[k,])]#available values from k-th row of matrix T
> tempJ<-J[k,][!is.na(J[k,])]
> 
> hg<-length(tempT[tempT<=t & tempJ==0])#counts observations satisfing the conditions
> ag<-length(tempT[tempT<=t & tempJ==1])
> 
> lambdaXY[hg+1,ag+1]#takes  values from a 10x10 matrix
> }
> 
> I keep receiving this message:
> 
> 1: longer object length
>         is not a multiple of shorter object length in: tempT <= t 
> 2: longer object length
>         is not a multiple of shorter object length in: tempT <= t & tempJ == 0 
> 
> What I suspect is that the "integrate" function submits the whole vector of points at which the integral is to be evaluated at once. For my function to be integrated it would rather have to be evaluated at each point after another in a loop of some kind.
> 

You suspect correctly. Best to read ?integrate too.

> Can you think of a way to solve this problem without me having to write the integrating procedure from scratch (I have no idea about FORTRAN and this is what the "integrate" description refers to)?
> 

Just put a "for"-loop in your function to iterate over t.

   n <- length(t)
   hg <- ag <- vector("numeric", n)
   for(i in seq(n)) {
     hg[i] <- length(tempT[tempT <= t[i] & tempJ == 0])
     ag[i] <- length(tempT[tempT <= t[i] & tempJ == 1])
   }

I doubt this will work because integrate is expecting a vector of 
n=length(t) from lambdat. The last line of the function returns a nxn 
matrix. Please submit data to run the function plus your call to 
integrate in any subsequent postings.

HTH,

--sundar

> Thank you.
> 
> Kind Regards,
> Lukasz Szczepanski
> Student
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From murdoch at stats.uwo.ca  Thu Jun 29 14:29:57 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 29 Jun 2006 08:29:57 -0400
Subject: [R] numerical integration problem
In-Reply-To: <44A3BBCF.3010206@pdf.com>
References: <6850e508.74dbe543.44a3b84b.b7754@o2.pl> <44A3BBCF.3010206@pdf.com>
Message-ID: <44A3C7C5.9020807@stats.uwo.ca>

On 6/29/2006 7:38 AM, Sundar Dorai-Raj wrote:
> 
> przeszczepan wrote:
>> Hi,
>> 
>> I have got problems integrating the following function using "integrate": 
>> 
>> lambdat<-function(t){
>> tempT<-T[k,][!is.na(T[k,])]#available values from k-th row of matrix T
>> tempJ<-J[k,][!is.na(J[k,])]
>> 
>> hg<-length(tempT[tempT<=t & tempJ==0])#counts observations satisfing the conditions
>> ag<-length(tempT[tempT<=t & tempJ==1])
>> 
>> lambdaXY[hg+1,ag+1]#takes  values from a 10x10 matrix
>> }
>> 
>> I keep receiving this message:
>> 
>> 1: longer object length
>>         is not a multiple of shorter object length in: tempT <= t 
>> 2: longer object length
>>         is not a multiple of shorter object length in: tempT <= t & tempJ == 0 
>> 
>> What I suspect is that the "integrate" function submits the whole vector of points at which the integral is to be evaluated at once. For my function to be integrated it would rather have to be evaluated at each point after another in a loop of some kind.
>> 
> 
> You suspect correctly. Best to read ?integrate too.
> 
>> Can you think of a way to solve this problem without me having to write the integrating procedure from scratch (I have no idea about FORTRAN and this is what the "integrate" description refers to)?
>> 
> 
> Just put a "for"-loop in your function to iterate over t.

Or use Vectorize().

vlambdat <- Vectorize(lambdat)

should give a function that can be passed to integrate(), assuming that 
lambdat works when given a length 1 vector as input.

Duncan Murdoch


> 
>    n <- length(t)
>    hg <- ag <- vector("numeric", n)
>    for(i in seq(n)) {
>      hg[i] <- length(tempT[tempT <= t[i] & tempJ == 0])
>      ag[i] <- length(tempT[tempT <= t[i] & tempJ == 1])
>    }
> 
> I doubt this will work because integrate is expecting a vector of 
> n=length(t) from lambdat. The last line of the function returns a nxn 
> matrix. Please submit data to run the function plus your call to 
> integrate in any subsequent postings.
> 
> HTH,
> 
> --sundar
> 
>> Thank you.
>> 
>> Kind Regards,
>> Lukasz Szczepanski
>> Student
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From pramod.r.anugu at jsums.edu  Thu Jun 29 15:09:20 2006
From: pramod.r.anugu at jsums.edu (Pramod Anugu)
Date: Thu, 29 Jun 2006 08:09:20 -0500
Subject: [R] Running R
In-Reply-To: DF50834EE43A2E4084B98CA2523F220D
Message-ID: <000601c69b7d$3c7621f0$1321848f@E72517PA>

Thank you Andy for your suggestion. I used the  method 2 and was able to go
upto Step 8. But When I type R

# R
Fatal error: unable to open the base package
?
1.? Tell configure exactly where to find the readline header files (by
something like --with-readline=/where/the/files/are, check configure
--help).
?
2.? Compile R without readline; i.e., configure --with-readline=no.
?
Andy



?? 1. gunzip  R-patched.tar.gz
   2. tar -xvf R-patched.tar
?? 3. changed the directory to the newly created directory R-patched
?? 4. Typed ./configure 
? ?5. Typed make 
?? 6. Make check 
?? 7. make check-all 
?? 8. Typed make install 
?? *9. Typed R


From bernarduse1 at yahoo.fr  Thu Jun 29 15:17:13 2006
From: bernarduse1 at yahoo.fr (Marc Bernard)
Date: Thu, 29 Jun 2006 15:17:13 +0200 (CEST)
Subject: [R] specify y axis values
Message-ID: <20060629131713.60534.qmail@web25808.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060629/89835a65/attachment.pl 

From gregd at stats.uct.ac.za  Thu Jun 29 15:39:37 2006
From: gregd at stats.uct.ac.za (Greg Distiller)
Date: Thu, 29 Jun 2006 15:39:37 +0200
Subject: [R] Problems using na.include
Message-ID: <0d5d01c69b81$770a0670$6f179e89@UCTPCGREGD>

Hi
I am having problems using na.action=na.include in a nlme call. Basically I 
am trying to run a particular model without including any covariates but my 
dataset does include some covariates (factors) with missing values. My 
understanding from the Pinheiro and Bates book is that one would use 
na.action=na.include in order to treat the missing values as another factor 
level and then one can also use naPattern to specify which rows must be 
excluded from the calculation of the objective function.

I have tried this but keep getting error messages about being unable to find 
the object or the function "na.include". I have also tried to replicate the 
particular Phenobarbital example verbatim from the book but am getting the 
same error message. Could it be that my version of the nlme package is 
outdated? I also cannot find any help on na.include...

Thanks

Greg


From s-dhar at northwestern.edu  Thu Jun 29 15:40:06 2006
From: s-dhar at northwestern.edu (Sumitrajit Dhar)
Date: Thu, 29 Jun 2006 08:40:06 -0500
Subject: [R] boxplot of subset of factors, in an order of my choice
Message-ID: <65F8CD1B-7F61-4394-9925-FD7D025DEF25@northwestern.edu>

Hi Folks,

Say I am working with the following data set:
testResults:
	stud		test		score
1	1		fTest	43
2	1		rTest	39
3	1		gTest	43
4	2		fTest	23
5	2		rTest	33
6	2		gTest	41
.
.
.
N	n		gTest	47

I would like to generate a boxplot of rTest only. Here is what I have  
done after reading the data in and attaching the object.

test <- factor(test)

Now when I try

boxplot(score ~ test, subset = test=="rTest")

1. I still see the tick marks for the other factors on the x-axis  
although no data are plotted. Can I eliminate these extra markers  
(and the space for these faactors) and just have the tick marker and  
the label for "rTest"?

2. I would also like to be able to rearrange the tests as they appear  
from left to right on the graph. I read the hints and help about  
reorder.factor but it does not seem to be the solution in this case.  
I am wanting to re-order the boxes without any logic (statistical  
anyways). Can this be done?

Thanks in advance,
Sumit


From andy_liaw at merck.com  Thu Jun 29 15:53:55 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 29 Jun 2006 09:53:55 -0400
Subject: [R] boxplot of subset of factors,
 in an order of my choice [B roadcast]
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02884419@usctmx1106.merck.com>

1. Use something like 

      with(testResults, boxplot(score[test == "rTest"]))

2. testResults$test <- factor(as.character(testResults$test),
                              levels=c("rTest", "gTest", "fTest"))

[Substitute any order you like in the last line.]

Andy

From: Sumitrajit Dhar
> 
> Hi Folks,
> 
> Say I am working with the following data set:
> testResults:
> 	stud		test		score
> 1	1		fTest	43
> 2	1		rTest	39
> 3	1		gTest	43
> 4	2		fTest	23
> 5	2		rTest	33
> 6	2		gTest	41
> .
> .
> .
> N	n		gTest	47
> 
> I would like to generate a boxplot of rTest only. Here is 
> what I have done after reading the data in and attaching the object.
> 
> test <- factor(test)
> 
> Now when I try
> 
> boxplot(score ~ test, subset = test=="rTest")
> 
> 1. I still see the tick marks for the other factors on the 
> x-axis although no data are plotted. Can I eliminate these 
> extra markers (and the space for these faactors) and just 
> have the tick marker and the label for "rTest"?
> 
> 2. I would also like to be able to rearrange the tests as 
> they appear from left to right on the graph. I read the hints 
> and help about reorder.factor but it does not seem to be the 
> solution in this case.  
> I am wanting to re-order the boxes without any logic 
> (statistical anyways). Can this be done?
> 
> Thanks in advance,
> Sumit
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From rab45+ at pitt.edu  Thu Jun 29 15:52:46 2006
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Thu, 29 Jun 2006 09:52:46 -0400
Subject: [R] lmer - Is this reasonable output?
Message-ID: <1151589167.4033.29.camel@localhost.localdomain>

I'm estimating two models for data with n = 179 with four clusters (21,
70, 36, and 52) named siteid. I'm estimating a logistic regression model
with random intercept and another version with random intercept and
random slope for one of the independent variables.


fit.1 <- lmer(glaucoma~(1|siteid)+x1
+x2,family=binomial,data=set1,method="ML",
  control=list(usePQL=FALSE,msVerbose=TRUE))

Generalized linear mixed model fit using PQL
Formula: glaucoma ~ (1 | siteid) + x1 + x2
          Data: set1
 Family: binomial(logit link)
      AIC      BIC    logLik deviance
 236.7448 249.4944 -114.3724 228.7448
Random effects:
 Groups Name        Variance Std.Dev.
 siteid (Intercept) 0.05959  0.24411
number of obs: 179, groups: siteid, 4

Estimated scale (compare to 1)  0.464267

Fixed effects:
             Estimate Std. Error z value Pr(>|z|)
(Intercept) -2.213779   0.688158 -3.2170 0.001296 **
x1           0.609028   0.293250  2.0768 0.037818 *
x2           0.025027   0.009683  2.5846 0.009749 **
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
       (Intr)     x1
x1     -0.871
x2     -0.372 -0.024

> ranef(fit.1)
An object of class ?ranef.lmer?
[[1]]
  (Intercept)
1 -0.05053772
2 -0.21592157
3  0.36643051
4 -0.04141520


fit.2 <- lmer(glaucoma~(x1|siteid)+x1
+x2,family=binomial,data=set1,method="ML",
  control=list(usePQL=FALSE,msVerbose=TRUE))

Generalized linear mixed model fit using PQL
Formula: glaucoma ~ (x1 | siteid) + x1 + x2
          Data: set1
 Family: binomial(logit link)
      AIC      BIC    logLik deviance
 239.3785 258.5029 -113.6893 227.3785
Random effects:
 Groups Name        Variance Std.Dev. Corr
 siteid (Intercept) 0.059590 0.24411
        x1          0.013079 0.11436  0.000
number of obs: 179, groups: siteid, 4

Estimated scale (compare to 1)  0.4599856

Fixed effects:
              Estimate Std. Error z value Pr(>|z|)
(Intercept) -2.2137787  0.6911360 -3.2031 0.001360 **
x1           0.6090279  0.2995553  2.0331 0.042042 *
x2           0.0250268  0.0097569  2.5650 0.010316 *
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
       (Intr)     x1
x1     -0.854
x2     -0.372 -0.023

> ranef(fit.2)
An object of class ?ranef.lmer?
[[1]]
  (Intercept)           x1
1 -0.05534800  0.009265667
2 -0.16991678 -0.052723584
3  0.27692026  0.137597965
4  0.01648737 -0.062232012

Note that the fixed coefficient estimates are identical for both models
and they are exactly equal to what glm gives ignoring the sites (but the
standard errors given by lmer are definitely larger - which would seem
reasonable). The se's for the fixed factors differ slightly between the
two models. Note also that the estimated random effect sd for siteid
intercept is identical for both models.

I ran both models using PROC NLMIXED in SAS. It gives be similar
estimates but not identical for the fixed effects and random effects.
The confidence intervals on the random effects for each site are very
large.

Am I getting these results because I don't really need to fit a random
effect for siteid? The estimated random effects for slope seem to say
that siteid matters. When I plot the data for each site with smoothing
splines it indicates reasonably different patterns between sites.

I don't think these models are that complicated and I have a reasonable
amount of data. I fit the fixed and random curves to the separate sites
(along with separate glm estimates for each site). As would be expected,
the random curves lie between the fixed effect curve and the glm curve.
But there seems to be a lot of shrinkage toward the fixed effect curve.
(The fixed effect curve fits the smoothing spline curve for all sites
combined very closely.)

If I specify method="ML" and use PQL, I get similar fixed effect
estimates (but not identical to glm). The random intercept sd about
doubles. I think SAS NLMIXED uses an approximation to the likelihood so
that may explain some of the differences.

One other thing that seems odd to me is the random intercept sd for site
id. It equals 0.24411 in both models. If I change x1 to, say x3 (an
entirely different variable), I still get 0.24411. However, the random
slope sd does change.

I just want to make sure I'm fitting a reasonable model and getting
reasonable estimates.

Rick B.


From yschen at jhu.edu  Thu Jun 29 16:26:29 2006
From: yschen at jhu.edu (YIHSU CHEN)
Date: Thu, 29 Jun 2006 10:26:29 -0400
Subject: [R] function in boot
Message-ID: <5282a9b1ca0.44a3aad5@jhumail.jhu.edu>

Dear R users:

I'm trying to use "boot" function under the package boot to run some bootstrapping.

Basically, I have a df as follows. (The data is simplified for illustrative purpose.) 

V1 V2 V3
1  2  3 
4  5  7
3  5  2

Say that I want bootrap among (V1,V2,V3) for 1000 times and calculate the average of them, ie., v_bar still has lenght of 3. I think my question is really how to write bf function below. 

bf <- function(d,f)
{
    gp1 <- 1:3 #indicate random draw from three series
    dmean <- apply(d[,gp1]*f[gp1],1,mean)      
}
a <- boot(df,bf, R=1000, stype="f")


Any help or suggestions? Many thanks.

Yihsu


From dimitris.rizopoulos at med.kuleuven.be  Thu Jun 29 16:31:13 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 29 Jun 2006 16:31:13 +0200
Subject: [R] lmer - Is this reasonable output?
References: <1151589167.4033.29.camel@localhost.localdomain>
Message-ID: <009701c69b88$ac878d20$0540210a@www.domain>

Hello Rick,

some comments below:

o in lmer(), 'method = "ML"' and its counterpart "REML" refer to the 
case of linear mixed models; for GLMMs the methods currently available 
are PQL and Laplace. The control argument 'usePQL = FALSE' can be used 
when 'method = Laplace', and in fact specifies not to use PQL as a 
refinement to the initial values.

o SAS proc NLMIXED performs (adaptive) Gaussian Quadrature and is 
conceptually closer to 'method = Laplace' in lmer(); for PQL you have 
to use proc GLIMMIX.

o for GLMMs both lmer() and NLMIXED work with an approximation to the 
observed data likelihood, since this likelihood involves an integral 
with no closed-form solution (except from very specific cases).

o with only 4 clusters I think it'd be difficult to estimate variance 
components; If you want to correct for site, you could just put it as 
an ordinary covariate in your logistic regression.

I hope my comments help.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Rick Bilonick" <rab45+ at pitt.edu>
To: "R Help" <r-help at stat.math.ethz.ch>
Sent: Thursday, June 29, 2006 3:52 PM
Subject: [R] lmer - Is this reasonable output?


> I'm estimating two models for data with n = 179 with four clusters 
> (21,
> 70, 36, and 52) named siteid. I'm estimating a logistic regression 
> model
> with random intercept and another version with random intercept and
> random slope for one of the independent variables.
>
>
> fit.1 <- lmer(glaucoma~(1|siteid)+x1
> +x2,family=binomial,data=set1,method="ML",
>  control=list(usePQL=FALSE,msVerbose=TRUE))
>
> Generalized linear mixed model fit using PQL
> Formula: glaucoma ~ (1 | siteid) + x1 + x2
>          Data: set1
> Family: binomial(logit link)
>      AIC      BIC    logLik deviance
> 236.7448 249.4944 -114.3724 228.7448
> Random effects:
> Groups Name        Variance Std.Dev.
> siteid (Intercept) 0.05959  0.24411
> number of obs: 179, groups: siteid, 4
>
> Estimated scale (compare to 1)  0.464267
>
> Fixed effects:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept) -2.213779   0.688158 -3.2170 0.001296 **
> x1           0.609028   0.293250  2.0768 0.037818 *
> x2           0.025027   0.009683  2.5846 0.009749 **
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Correlation of Fixed Effects:
>       (Intr)     x1
> x1     -0.871
> x2     -0.372 -0.024
>
>> ranef(fit.1)
> An object of class ?ranef.lmer?
> [[1]]
>  (Intercept)
> 1 -0.05053772
> 2 -0.21592157
> 3  0.36643051
> 4 -0.04141520
>
>
> fit.2 <- lmer(glaucoma~(x1|siteid)+x1
> +x2,family=binomial,data=set1,method="ML",
>  control=list(usePQL=FALSE,msVerbose=TRUE))
>
> Generalized linear mixed model fit using PQL
> Formula: glaucoma ~ (x1 | siteid) + x1 + x2
>          Data: set1
> Family: binomial(logit link)
>      AIC      BIC    logLik deviance
> 239.3785 258.5029 -113.6893 227.3785
> Random effects:
> Groups Name        Variance Std.Dev. Corr
> siteid (Intercept) 0.059590 0.24411
>        x1          0.013079 0.11436  0.000
> number of obs: 179, groups: siteid, 4
>
> Estimated scale (compare to 1)  0.4599856
>
> Fixed effects:
>              Estimate Std. Error z value Pr(>|z|)
> (Intercept) -2.2137787  0.6911360 -3.2031 0.001360 **
> x1           0.6090279  0.2995553  2.0331 0.042042 *
> x2           0.0250268  0.0097569  2.5650 0.010316 *
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Correlation of Fixed Effects:
>       (Intr)     x1
> x1     -0.854
> x2     -0.372 -0.023
>
>> ranef(fit.2)
> An object of class ?ranef.lmer?
> [[1]]
>  (Intercept)           x1
> 1 -0.05534800  0.009265667
> 2 -0.16991678 -0.052723584
> 3  0.27692026  0.137597965
> 4  0.01648737 -0.062232012
>
> Note that the fixed coefficient estimates are identical for both 
> models
> and they are exactly equal to what glm gives ignoring the sites (but 
> the
> standard errors given by lmer are definitely larger - which would 
> seem
> reasonable). The se's for the fixed factors differ slightly between 
> the
> two models. Note also that the estimated random effect sd for siteid
> intercept is identical for both models.
>
> I ran both models using PROC NLMIXED in SAS. It gives be similar
> estimates but not identical for the fixed effects and random 
> effects.
> The confidence intervals on the random effects for each site are 
> very
> large.
>
> Am I getting these results because I don't really need to fit a 
> random
> effect for siteid? The estimated random effects for slope seem to 
> say
> that siteid matters. When I plot the data for each site with 
> smoothing
> splines it indicates reasonably different patterns between sites.
>
> I don't think these models are that complicated and I have a 
> reasonable
> amount of data. I fit the fixed and random curves to the separate 
> sites
> (along with separate glm estimates for each site). As would be 
> expected,
> the random curves lie between the fixed effect curve and the glm 
> curve.
> But there seems to be a lot of shrinkage toward the fixed effect 
> curve.
> (The fixed effect curve fits the smoothing spline curve for all 
> sites
> combined very closely.)
>
> If I specify method="ML" and use PQL, I get similar fixed effect
> estimates (but not identical to glm). The random intercept sd about
> doubles. I think SAS NLMIXED uses an approximation to the likelihood 
> so
> that may explain some of the differences.
>
> One other thing that seems odd to me is the random intercept sd for 
> site
> id. It equals 0.24411 in both models. If I change x1 to, say x3 (an
> entirely different variable), I still get 0.24411. However, the 
> random
> slope sd does change.
>
> I just want to make sure I'm fitting a reasonable model and getting
> reasonable estimates.
>
> Rick B.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From spencer.graves at pdf.com  Thu Jun 29 16:36:02 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 29 Jun 2006 07:36:02 -0700
Subject: [R] smoothing splines and degrees of freedom
In-Reply-To: <50509.151.201.54.128.1151163316.squirrel@webmail.pitt.edu>
References: <50509.151.201.54.128.1151163316.squirrel@webmail.pitt.edu>
Message-ID: <44A3E552.5020008@pdf.com>

	  The help page for 'smooth.spline' says the argument 'df' is 'the 
desired equivalent number of degrees of freedom (trace of the smoother 
matrix).'  It also explains that the output of 'smooth.spline' includes 
a component 'fit', and two components of 'fit' are 'knot' and 'coef'. 
To learn more, you can run the examples, examine 'str(cars.spl)' and the 
other objects produced by those examples.  You can also read more in the 
references cited there.

	  If you would like further help from this group, please submit another 
question, preferably after first reading the posting guide! 
"www.R-project.org/posting-guide.html".  There is substantial but 
anecdotal evidence to suggest that posts more consistent with that guide 
tend to get better answers quicker.  For example, if the above does NOT 
answer your question, I believe you would have gotten a better reply if 
you had provided a simple, self-contained example, rather than having me 
rely on one from the 'example' section of the 'smooth.spline' help page.

	  Hope this helps.
	  Spencer Graves


Steven Shechter wrote:
> Hi,
> If I set df=2 in my smooth.spline function, is that equivalent to running
> a linear regression through my data?  It appears that df=# of data points
> gives the interpolating spline and that df = 2 gives the linear
> regression, but I just want to confirm this.
> 
> Thank you,
> Steven
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From remigijus.lapinskas at mif.vu.lt  Thu Jun 29 16:37:44 2006
From: remigijus.lapinskas at mif.vu.lt (Remigijus Lapinskas)
Date: Thu, 29 Jun 2006 17:37:44 +0300
Subject: [R] multivariate normality test
Message-ID: <44A3E5B8.6090407@mif.vu.lt>

Hello,

Could someone help me to explain the VERY big difference in applying two
tests on multivariate normality:

library(mvnormtest)
data(EuStockMarkets)
mshapiro.test(t(EuStockMarkets[15:29,1:4]))


         Shapiro-Wilk normality test
data:  Z
W = 0.8161, p-value = 0.005955

and

library(energy)
mvnorm.etest( EuStockMarkets[15:29,1:4] )

         Energy test of multivariate normality: estimated parameters

data:  x, sample size 15, dimension 4, replicates 999
E-statistic = 1.0041, p-value = 0.2482



Many thanks,
Rem


From toby_marks at americancentury.com  Thu Jun 29 16:46:04 2006
From: toby_marks at americancentury.com (toby_marks at americancentury.com)
Date: Thu, 29 Jun 2006 09:46:04 -0500
Subject: [R] Help needed understanding eval,quote,expression
In-Reply-To: <Pine.LNX.4.64.0606290953500.25166@gannet.stats.ox.ac.uk>
Message-ID: <OF564C10AB.E48D598F-ON8625719C.004D7E93-8625719C.00510AEC@americancentury.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060629/3e67af0a/attachment.pl 

From f.harrell at vanderbilt.edu  Thu Jun 29 16:49:57 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 29 Jun 2006 09:49:57 -0500
Subject: [R] Problems using na.include
In-Reply-To: <0d5d01c69b81$770a0670$6f179e89@UCTPCGREGD>
References: <0d5d01c69b81$770a0670$6f179e89@UCTPCGREGD>
Message-ID: <44A3E895.7050309@vanderbilt.edu>

Greg Distiller wrote:
> Hi
> I am having problems using na.action=na.include in a nlme call. Basically I 
> am trying to run a particular model without including any covariates but my 
> dataset does include some covariates (factors) with missing values. My 
> understanding from the Pinheiro and Bates book is that one would use 
> na.action=na.include in order to treat the missing values as another factor 
> level and then one can also use naPattern to specify which rows must be 
> excluded from the calculation of the objective function.

The approach of adding new categories for missing values has been shown 
to be highly problematic, completely changing the interpretation of the 
predictor and causing biases.  See an article by Werner Vach on this in 
the Encyclopedia of Biostatistics.

> 
> I have tried this but keep getting error messages about being unable to find 
> the object or the function "na.include". I have also tried to replicate the 
> particular Phenobarbital example verbatim from the book but am getting the 
> same error message. Could it be that my version of the nlme package is 
> outdated? I also cannot find any help on na.include...
> 
> Thanks
> 
> Greg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From aarti_dahiya at hotmail.com  Thu Jun 29 17:06:24 2006
From: aarti_dahiya at hotmail.com (Aarti Dahiya)
Date: Thu, 29 Jun 2006 11:06:24 -0400
Subject: [R] Problems Creating an R package
Message-ID: <BAY106-F1854DC246F57F46DF07D11E37C0@phx.gbl>

Hi all,

When I check my package using "Rcmd check ..\myPackage\R.mykg" on Windows in 
Command Prompt, this is what get:-

* using log directory 'C:/R/bin/R.getdata.Rcheck'
* using Version 2.3.1 (2006-06-01)
* checking for file 'R.getdata/DESCRIPTION' ... OK
* checking extension type ... Package
* this is package 'R.getdata' version '1.0'
* checking package dependencies ... OK
* checking if this is a source package ... OK
* checking whether package 'R.getdata' can be installed ... OK
* checking package directory ... OK
* checking for portable file names ... OK
* checking DESCRIPTION meta-information ... OK
* checking top-level files ... OK
* checking index information ... OK
* checking package subdirectories ... WARNING
Subdirectory 'src' contains no source files.
* checking R files for syntax errors ... ERROR
Syntax error in file 'R/ConnectionFactory.R'

Q1. What should I put in 'src'.  I did put two of my source .R files, it 
still does not work.

Q2. This is what I get if I source R/ConnectionFactory.R.
Error in parse(file, n = -1, NULL, "?") : syntax error at
5: }
6: , .env = <

These are the contents of R/ConnectionFactory.R.

"ConnectionFactory" <-
structure(function()
{
	extend(Object(), "ConnectionFactory")
}
, .env = <environment>, class = c("Class", "Object"), ...instanciationTime = 
structure(1151521420.604, class = c("POSIXt",
"POSIXct")), formals = c("public", "class"), modifiers = c("public",
"class")

Please note that this file is one that was automatically generated in 
R.mykg/R.  My source file ConnectionFactory.R has the actual R code which 
sources just fine.

I will greatly appreciate any help!  Thank you.

Aarti


From philipp.pagel.lists at t-online.de  Thu Jun 29 13:38:04 2006
From: philipp.pagel.lists at t-online.de (Philipp Pagel)
Date: Thu, 29 Jun 2006 13:38:04 +0200
Subject: [R] Extracting R plots from MS Word
In-Reply-To: <62025.58.105.171.47.1151558108.squirrel@webmail.scms.waikato.ac.nz>
References: <62025.58.105.171.47.1151558108.squirrel@webmail.scms.waikato.ac.nz>
Message-ID: <20060629113804.GA5945@gsf.de>

On Thu, Jun 29, 2006 at 05:15:08PM +1200, maj at stats.waikato.ac.nz wrote:
> I am revising a paper that I am a co-author of. The figures are plots
> generated from R but at the moment I do not have the R code that generates
> them.
> 
> As this is time critical I would like to slightly abuse the list by asking
> whether anyone knows how to extract from MS Word into a stand-alone
> graphics file a plot that was pasted into Word from R (probably as a
> Windows Metafile, but possibly as a bitmap).

Open the doc file in OpenOffice, save in sxw format (openoffice's old 
native format), unpack the file with unzip and find the figure in the
resulting directory.

I'm not sure if and how this works with the new odt format used by
openoffice now, but sxw will work.

cu
	Philipp

-- 
Dr. Philipp Pagel                            Tel.  +49-8161-71 2131
Dept. of Genome Oriented Bioinformatics      Fax.  +49-8161-71 2186
Technical University of Munich
Science Center Weihenstephan
85350 Freising, Germany

 and

Institute for Bioinformatics / MIPS          Tel.  +49-89-3187 3675
GSF - National Research Center               Fax.  +49-89-3187 3585
      for Environment and Health
Ingolst?dter Landstrasse 1
85764 Neuherberg, Germany
http://mips.gsf.de/staff/pagel


From cuauv at yahoo.com  Thu Jun 29 17:33:33 2006
From: cuauv at yahoo.com (Cuau)
Date: Thu, 29 Jun 2006 08:33:33 -0700 (PDT)
Subject: [R] initializing table and filling it out
Message-ID: <20060629153333.53725.qmail@web52309.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060629/8e0fba7f/attachment.pl 

From maechler at stat.math.ethz.ch  Thu Jun 29 17:37:47 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 29 Jun 2006 17:37:47 +0200
Subject: [R] superimposing histograms con't
In-Reply-To: <004601c69aec$8b869450$b61ad284@BIO041>
References: <004601c69aec$8b869450$b61ad284@BIO041>
Message-ID: <17571.62411.486920.111310@stat.math.ethz.ch>

>>>>> "Bill" == Bill Shipley <bill.shipley at usherbrooke.ca>
>>>>>     on Wed, 28 Jun 2006 15:53:35 -0400 writes:

    Bill> Earlier, I posted the following question:

    Bill> I want to superimpose histograms from three
    Bill> populations onto the same graph, changing the shading
    Bill> of the bars for each population. After consulting the
    Bill> help files and the archives I cannot find out how to
    Bill> do this (seemly) simple graph. To be clear, I want

    Bill> - a single x axis (from -3 to 18)
    Bill> - three groups of bars forming the histograms of each population
    Bill>   (they will not overlap much, but this is a detail)
    Bill> - the bars from each histogram having different
    Bill>   shadings or other  visually distinguishing features.
 
    Bill> Gabor Grothendieck [ggrothendieck at gmail.com] pointed
    Bill> to some code to to this but I have found another way
    Bill> that works even easier.
 

    Bill> hist(x[sel1],xlim=c(a,b),ylim=c(A,B))  - this plots the histogram for the
    Bill> first group (indexed by sel1) but with an x axis and a y axis that spans the
    Bill> entire range.
 
    Bill> par(new=T)  - to keep on the same graph
 
    Bill> hist(x[sel2],main=Null,xlab=NULL,ylab=NULL,axes=F) -superimposes the second
    Bill> histogram
 
    Bill> par(new=T)  - to keep on the same graph
 
    Bill> hist(x[sel3],main=Null,xlab=NULL,ylab=NULL,axes=F) -superimposes the third
    Bill> histogram
 
Hmm, the above does not work (because of 'Null' instead of  'NULL')
but even if you fix that, I'm pretty sure you get pretty wrong
plots.

par(new = TRUE) is quite often a bad choice.
In the present case, plot 2 and 3 use different
coordinate systems than plot 1.

The "correct" solution -- apart from using  lattice::histogram()
with the correct 'superpose' panel is really to make use of the
fact that

  1) hist() returns a "histogram" (S3) object which is plotted by
     
  2) plot.histogram() which has a nice help page even though
     the funcion is namespace-hidden.

Here's a reproducible solution --- with quite a bit of extra
code in order to show two ``colorizations'' for overplotting,
in particular one with ``transparent colors'' :


##MM: construct reproducible example data
set.seed(1)
x <- c(rnorm(50), (rnorm(60) +3.5), (rnorm(40) +3.5 + 3))
str(grouping <- factor(c(rep(1,50), rep(2,60), rep(3,40))))
(n.gr <- length(table(grouping))) # 3

xr <- range(x)
## Compute all three histograms objects but without plotting:
histL <- tapply(x, grouping, hist, plot = FALSE)
maxC <- max(print( sapply(lapply(histL, "[[", "counts"), max)) ) # 14 15 15

pdf("3histo.pdf", version = "1.4") # >= 1.4 is needed
## or try the default x11()

if((TC <- transparent.cols <- .Device %in% c("pdf", "png"))) {
    cols <- hcl(h = seq(30, by=360 / n.gr, length = n.gr), l = 65,
                alpha = 0.5) ## << transparency
} else {
    h.den <- c(10, 15, 20)
    h.ang <- c(45, 15, -30)
}

## Plots the histogram for the
## first group (indexed by sel1) but with an x axis and a y axis that spans the
## entire range.
if(TC) {
    plot(histL[[1]], xlim = xr, ylim= c(0, maxC), col = cols[1],
         xlab = "x", main = "3 Histograms of 3 Selections from 'x'")
} else
    plot(histL[[1]], xlim = xr, ylim= c(0, maxC), density = h.den[1], angle = h.ang[1],
         xlab = "x", main = "Histogram of 3 selections from 'x'")
## Using density instead of color make these *add*

if(!transparent.cols) {
    for(j in 2:n.gr)
        plot(histL[[j]], add = TRUE, density = h.den[j], angle = h.ang[j])
} else { ## use semi-translucent colors (available only for PDF >= 1.4 and PNG devices):
    for(j in 2:n.gr)
        plot(histL[[j]], add = TRUE, col = cols[j])
}

if(.Device == "pdf") { ## you have set it above
  dev.off()
  system("gv 3histo.pdf &")## or use 'acroread' ;  'xpdf' does not show transparent colors...
}

1 ##--- Martin Maechler, ETH Zurich


From thpe at hhbio.wasser.tu-dresden.de  Thu Jun 29 17:39:59 2006
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 29 Jun 2006 17:39:59 +0200
Subject: [R] Problems Creating an R package
In-Reply-To: <BAY106-F1854DC246F57F46DF07D11E37C0@phx.gbl>
References: <BAY106-F1854DC246F57F46DF07D11E37C0@phx.gbl>
Message-ID: <44A3F44F.9030609@hhbio.wasser.tu-dresden.de>

Hi Aarti

src/  if you have source files in C or Fortran.
R/    for the R sources.

If you have no C or Fortran files, you should delete the src/ directory.

The syntax error in your R function is simply that the "," is on a new
line so R "thinks" that the line above is complete.

Hope it helps

Thomas P.


Aarti Dahiya wrote:
> Hi all,
> 
> When I check my package using "Rcmd check ..\myPackage\R.mykg" on Windows in 
> Command Prompt, this is what get:-
> 
> * using log directory 'C:/R/bin/R.getdata.Rcheck'
> * using Version 2.3.1 (2006-06-01)
> * checking for file 'R.getdata/DESCRIPTION' ... OK
> * checking extension type ... Package
> * this is package 'R.getdata' version '1.0'
> * checking package dependencies ... OK
> * checking if this is a source package ... OK
> * checking whether package 'R.getdata' can be installed ... OK
> * checking package directory ... OK
> * checking for portable file names ... OK
> * checking DESCRIPTION meta-information ... OK
> * checking top-level files ... OK
> * checking index information ... OK
> * checking package subdirectories ... WARNING
> Subdirectory 'src' contains no source files.
> * checking R files for syntax errors ... ERROR
> Syntax error in file 'R/ConnectionFactory.R'
> 
> Q1. What should I put in 'src'.  I did put two of my source .R files, it 
> still does not work.
> 
> Q2. This is what I get if I source R/ConnectionFactory.R.
> Error in parse(file, n = -1, NULL, "?") : syntax error at
> 5: }
> 6: , .env = <
> 
> These are the contents of R/ConnectionFactory.R.
> 
> "ConnectionFactory" <-
> structure(function()
> {
> 	extend(Object(), "ConnectionFactory")
> }
> , .env = <environment>, class = c("Class", "Object"), ...instanciationTime = 


> structure(1151521420.604, class = c("POSIXt",
> "POSIXct")), formals = c("public", "class"), modifiers = c("public",
> "class")
> 
> Please note that this file is one that was automatically generated in 
> R.mykg/R.  My source file ConnectionFactory.R has the actual R code which 
> sources just fine.
> 
> I will greatly appreciate any help!  Thank you.
> 
> Aarti
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From maechler at stat.math.ethz.ch  Thu Jun 29 17:42:51 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 29 Jun 2006 17:42:51 +0200
Subject: [R] superimposing histograms con't
In-Reply-To: <000f01c69af0$53321ee0$6401a8c0@AdSAMJeff>
References: <004601c69aec$8b869450$b61ad284@BIO041>
	<000f01c69af0$53321ee0$6401a8c0@AdSAMJeff>
Message-ID: <17571.62715.520684.692198@stat.math.ethz.ch>

>>>>> "Jeff" == Jeff Miller <jeffmiller at alphapoint05.net>
>>>>>     on Wed, 28 Jun 2006 16:20:39 -0400 writes:

    Jeff> I was just thinking about this last night.
    Jeff> I would like to do the same but WITH overlapping.

    Jeff> For example, I graph 2 sets of count data. Say the
    Jeff> bars for the 1`s overlap...I would like to show that
    Jeff> with a different shading for the group that has the
    Jeff> higher frequency. For example, it could be black up to
    Jeff> a frequency of 5 followed by diagonal-dashes from 5-7
    Jeff> representing the higher frequency of a second group.

Just 5 minutes ago, I've posted a reply to Bill's message
containing R code which I think also solves the above problem.

Solving overlapping visually either by
  (old style) shading  or by
  (new style) transparent colors -- unfortunately only available
	      on pdf(version >= 1.4), png(), and quartz(), AFAIK.

Martin Maechler


From ggrothendieck at gmail.com  Thu Jun 29 17:54:51 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 29 Jun 2006 11:54:51 -0400
Subject: [R] initializing table and filling it out
In-Reply-To: <20060629153333.53725.qmail@web52309.mail.yahoo.com>
References: <20060629153333.53725.qmail@web52309.mail.yahoo.com>
Message-ID: <971536df0606290854xd85d308x80a1eca4da416b38@mail.gmail.com>

If you use lapply you don't have to set up the storage area.

Here is an example.  I have used textConnection with character
strings to make this reproducible but you can replace conn with
a vector of filenames,

# test data
Lines1 <- "
1 2
3 4
"

Lines2 <- "
11 12
13 14
"

conn <- list(textConnection(Lines1), textConnection(Lines2))

# this is code
f <- function(f) with(read.table(f), c(a = mean(V1), b = sd(V2)))
do.call(rbind, lapply(conn, f))



On 6/29/06, Cuau <cuauv at yahoo.com> wrote:
>
>   Hi everyone,
>
>     I'm writting a script that will open multiple files in a  single folder and then will do some calculations to finally save  everything in a big table.
>   So here  is the pseudo code
>
>  #read all files in the given directory
>  myfiles <-list.files("c:\\myDir")
>
>  #initialize table
>  ???
>
>  #loop through files
>  for(f in myfiles[-1]) {
>    netMat <- read.table( f, sep" " )
>    netMat <-as.matrix(netMat)
>
>   # calculate the needed Stats
>   gden(netMat)->density
>   centralization(netMat, degree) -> Gdegree
>   centralization(netMat, betweenness) -> Gbetweenness
>   centralization(netMat, closeness) -> Gcloseness
>
>  #store into table
>  ????
>
>  }
>
>    So my questions are two
>
>   First how can I initialize the table (it will be a 4 X 1001 table) without filling it out first.
>  Second how can I store each value in the table (i.e. in each round I'll  be adding one full column with density and the 3 different  centralizations)?
>
>  thks
>
>  -Cuau
>
>
>
>
> ---------------------------------
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ripley at stats.ox.ac.uk  Thu Jun 29 18:03:26 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 29 Jun 2006 17:03:26 +0100 (BST)
Subject: [R] Problems Creating an R package
In-Reply-To: <44A3F44F.9030609@hhbio.wasser.tu-dresden.de>
References: <BAY106-F1854DC246F57F46DF07D11E37C0@phx.gbl>
	<44A3F44F.9030609@hhbio.wasser.tu-dresden.de>
Message-ID: <Pine.LNX.4.64.0606291701220.4568@gannet.stats.ox.ac.uk>

On Thu, 29 Jun 2006, Thomas Petzoldt wrote:

> Hi Aarti
>
> src/  if you have source files in C or Fortran.
> R/    for the R sources.
>
> If you have no C or Fortran files, you should delete the src/ directory.
>
> The syntax error in your R function is simply that the "," is on a new
> line so R "thinks" that the line above is complete.

R does know it is waiting for a ')'.  The error is exactly where it says, at
>> 6: , .env = <

and of course

>> , .env = <environment>, class = c("Class", "Object"), ...instanciationTime =
             ^^^^^^^^^^^^^
which is not valid R.

>
> Hope it helps
>
> Thomas P.
>
>
> Aarti Dahiya wrote:
>> Hi all,
>>
>> When I check my package using "Rcmd check ..\myPackage\R.mykg" on Windows in
>> Command Prompt, this is what get:-
>>
>> * using log directory 'C:/R/bin/R.getdata.Rcheck'
>> * using Version 2.3.1 (2006-06-01)
>> * checking for file 'R.getdata/DESCRIPTION' ... OK
>> * checking extension type ... Package
>> * this is package 'R.getdata' version '1.0'
>> * checking package dependencies ... OK
>> * checking if this is a source package ... OK
>> * checking whether package 'R.getdata' can be installed ... OK
>> * checking package directory ... OK
>> * checking for portable file names ... OK
>> * checking DESCRIPTION meta-information ... OK
>> * checking top-level files ... OK
>> * checking index information ... OK
>> * checking package subdirectories ... WARNING
>> Subdirectory 'src' contains no source files.
>> * checking R files for syntax errors ... ERROR
>> Syntax error in file 'R/ConnectionFactory.R'
>>
>> Q1. What should I put in 'src'.  I did put two of my source .R files, it
>> still does not work.
>>
>> Q2. This is what I get if I source R/ConnectionFactory.R.
>> Error in parse(file, n = -1, NULL, "?") : syntax error at
>> 5: }
>> 6: , .env = <
>>
>> These are the contents of R/ConnectionFactory.R.
>>
>> "ConnectionFactory" <-
>> structure(function()
>> {
>> 	extend(Object(), "ConnectionFactory")
>> }
>> , .env = <environment>, class = c("Class", "Object"), ...instanciationTime =
>
>
>> structure(1151521420.604, class = c("POSIXt",
>> "POSIXct")), formals = c("public", "class"), modifiers = c("public",
>> "class")
>>
>> Please note that this file is one that was automatically generated in
>> R.mykg/R.  My source file ConnectionFactory.R has the actual R code which
>> sources just fine.
>>
>> I will greatly appreciate any help!  Thank you.
>>
>> Aarti
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Mike.Li at fda.hhs.gov  Thu Jun 29 18:05:40 2006
From: Mike.Li at fda.hhs.gov (Li, Mike)
Date: Thu, 29 Jun 2006 12:05:40 -0400
Subject: [R] R server
Message-ID: <27CA3827C6B33E40874682C469E774DD71C0DB@FMD3CT001.fda.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060629/67b4c4eb/attachment.pl 

From jholtman at gmail.com  Thu Jun 29 18:10:46 2006
From: jholtman at gmail.com (jim holtman)
Date: Thu, 29 Jun 2006 12:10:46 -0400
Subject: [R] specify y axis values
In-Reply-To: <20060629131713.60534.qmail@web25808.mail.ukl.yahoo.com>
References: <20060629131713.60534.qmail@web25808.mail.ukl.yahoo.com>
Message-ID: <644e1f320606290910s7f54721asd888a4409ba160f7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060629/13cadf78/attachment.pl 

From csardi at rmki.kfki.hu  Thu Jun 29 18:19:03 2006
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Thu, 29 Jun 2006 12:19:03 -0400
Subject: [R] R server
In-Reply-To: <27CA3827C6B33E40874682C469E774DD71C0DB@FMD3CT001.fda.gov>
References: <27CA3827C6B33E40874682C469E774DD71C0DB@FMD3CT001.fda.gov>
Message-ID: <20060629161903.GB8056@localdomain>

One approach would be to install R on a unix machine and let other machines
connect to it via a standard X server.
There might be other approaches though....

Gabor

On Thu, Jun 29, 2006 at 12:05:40PM -0400, Li, Mike wrote:
> Hi, 
> 
> I just installed R in Windows, it seems to me that R is a standalone
> desktop application. Do you know if R has its server version which could
> handle multi-users, have several distinct R sessions and their own
> variables in Unix or Windows? 
> 
> I saw OpenStatServer, RStatServer available, can those handle
> multi-sessions?
> 
> Thanks
> 
> Mike
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From epurdom at stanford.edu  Thu Jun 29 19:27:45 2006
From: epurdom at stanford.edu (Elizabeth Purdom)
Date: Thu, 29 Jun 2006 10:27:45 -0700
Subject: [R] Internet Problem in R
Message-ID: <6.1.2.0.2.20060629101702.04b88c70@epurdom.pobox.stanford.edu>

Hi,

I have a student who's R seems to be unable to connect to the internet. I 
have verified that her computer has internet access that's working fine, 
but that R is not able to establish connections. For example, she cannot 
download packages for R from the URL site, cannot source webpages, or 
read.table from a webpage (all of these give errors of not being able to 
establish a connection). Right now I have shown her how to manually 
download the zip file for the package and install it from the local file, 
but she wants Bioconductor -- the instructions for which seem to require 
the usual internet access. Does anyone have any troubleshooting 
suggestions? (I've never had problems with the internet interface with R, 
so I don't even know where to start to try to fix it or even diagnose the 
problem). Could a firewall have this effect? Wireless?

Her computer is running version R.2.3.1, on a Windows XP.

Thanks,
Elizabeth Purdom


From andy_liaw at merck.com  Thu Jun 29 19:40:02 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 29 Jun 2006 13:40:02 -0400
Subject: [R] Internet Problem in R
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA028844C9@usctmx1106.merck.com>

It could well be a firewall issue.  Try adding --internet2 as argument to
Rgui.exe in the target field of the shortcut for Rgui.

Andy 

From: Elizabeth Purdom
> 
> Hi,
> 
> I have a student who's R seems to be unable to connect to the 
> internet. I have verified that her computer has internet 
> access that's working fine, but that R is not able to 
> establish connections. For example, she cannot download 
> packages for R from the URL site, cannot source webpages, or 
> read.table from a webpage (all of these give errors of not 
> being able to establish a connection). Right now I have shown 
> her how to manually download the zip file for the package and 
> install it from the local file, but she wants Bioconductor -- 
> the instructions for which seem to require the usual internet 
> access. Does anyone have any troubleshooting suggestions? 
> (I've never had problems with the internet interface with R, 
> so I don't even know where to start to try to fix it or even 
> diagnose the problem). Could a firewall have this effect? Wireless?
> 
> Her computer is running version R.2.3.1, on a Windows XP.
> 
> Thanks,
> Elizabeth Purdom
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From Horace.Tso at pgn.com  Thu Jun 29 19:47:16 2006
From: Horace.Tso at pgn.com (Horace Tso)
Date: Thu, 29 Jun 2006 10:47:16 -0700
Subject: [R] Biobass, SAGx, and Jonckheere-Terpstra test
Message-ID: <s4a3afbe.007@pgn.com>

Hi list,

I tried to load the package SAGx and failed because it complains it's
looking for the Biobass which is not there. Then I looked up the package
list and Biobass is not found. 

I'm trying to run the Jonckheere-Terpstra test and from what I see in
the R archive, SAGx is the only place it's been implemented.

> library(SAGx)
Loading required package: multtest
Loading required package: survival
Loading required package: splines
Loading required package: sma
Error in loadNamespace(i, c(lib.loc, .libPaths())) : 
        there is no package called 'Biobase'
Error: package/namespace load failed for 'SAGx'


Horace W. Tso


From thsudler at bluewin.ch  Thu Jun 29 21:10:35 2006
From: thsudler at bluewin.ch (Thomas Sudler)
Date: Thu, 29 Jun 2006 21:10:35 +0200
Subject: [R] RCOM Package
Message-ID: <001b01c69baf$b3bae570$0b00a8c0@heimcomputer>

Hi list,

I just installed the rcom package and tried to read/give out some values 
from/to Excel. Altogether it works great... but nevertheless I don't know 
how the syntax works or in other words: "Which command needs which 
parameters?"

Is there somwhere a manual about this package with good examples? I've read 
the Package description... but there are not really good 
descriptions/examples.

I know the following parameters/commands:

#Connect to the active Workbook
excel <- comGetObject("Excel.Application")

#Set the active sheet
sheet <- comGetProperty(excel,"ActiveSheet")

#Set a specific sheet
sheet <- comGetProperty(excel,"Worksheets","WorksheetXY")

#Create a new Worksheet
comInvoke(comGetProperty(excel,"Worksheets"),"Add")

#Define a range in a defined sheet
range1 <- comGetProperty(sheet,"Range","A1","A20")

#Read out this range from Excel
val1 <- as.double(comGetProperty(range1,"Value"))

#Give back the results to Excel
comSetProperty(range1,"Value",val1)

Some of my questions are:

- How can I rename a worksheet?
- How can I define the name of a new worksheet?
- How can I delete a worksheet?
- How can I connect to a specific Workbook, if more than one is opened?
- ...

Can someone help me?

Thanks,
Thomas


From ggrothendieck at gmail.com  Thu Jun 29 21:17:15 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 29 Jun 2006 15:17:15 -0400
Subject: [R] RCOM Package
In-Reply-To: <001b01c69baf$b3bae570$0b00a8c0@heimcomputer>
References: <001b01c69baf$b3bae570$0b00a8c0@heimcomputer>
Message-ID: <971536df0606291217r3ae2062du449d0e4266cb7691@mail.gmail.com>

Go into Excel, turn on macro recording, perform whatever operations
you want and look at whatever code is generated for you.

On 6/29/06, Thomas Sudler <thsudler at bluewin.ch> wrote:
> Hi list,
>
> I just installed the rcom package and tried to read/give out some values
> from/to Excel. Altogether it works great... but nevertheless I don't know
> how the syntax works or in other words: "Which command needs which
> parameters?"
>
> Is there somwhere a manual about this package with good examples? I've read
> the Package description... but there are not really good
> descriptions/examples.
>
> I know the following parameters/commands:
>
> #Connect to the active Workbook
> excel <- comGetObject("Excel.Application")
>
> #Set the active sheet
> sheet <- comGetProperty(excel,"ActiveSheet")
>
> #Set a specific sheet
> sheet <- comGetProperty(excel,"Worksheets","WorksheetXY")
>
> #Create a new Worksheet
> comInvoke(comGetProperty(excel,"Worksheets"),"Add")
>
> #Define a range in a defined sheet
> range1 <- comGetProperty(sheet,"Range","A1","A20")
>
> #Read out this range from Excel
> val1 <- as.double(comGetProperty(range1,"Value"))
>
> #Give back the results to Excel
> comSetProperty(range1,"Value",val1)
>
> Some of my questions are:
>
> - How can I rename a worksheet?
> - How can I define the name of a new worksheet?
> - How can I delete a worksheet?
> - How can I connect to a specific Workbook, if more than one is opened?
> - ...
>
> Can someone help me?
>
> Thanks,
> Thomas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From dheidemann at versanet.de  Thu Jun 29 21:20:06 2006
From: dheidemann at versanet.de (Dennis Heidemann)
Date: Thu, 29 Jun 2006 21:20:06 +0200
Subject: [R] Cointegration Test in R
Message-ID: <001e01c69bb1$07e54360$624afea9@fa162df94e0b40b>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060629/152953e3/attachment.pl 

From ripley at stats.ox.ac.uk  Thu Jun 29 21:28:59 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 29 Jun 2006 20:28:59 +0100 (BST)
Subject: [R] Biobass, SAGx, and Jonckheere-Terpstra test
In-Reply-To: <s4a3afbe.007@pgn.com>
References: <s4a3afbe.007@pgn.com>
Message-ID: <Pine.LNX.4.64.0606292026200.9404@gannet.stats.ox.ac.uk>

It is *Biobase*.  That is part of BioC (www.bioconductor.org), although I 
agree that it should make clear where to get non-CRAN packages. (Worse: 
SAGx used not to require it, and only the update to the most recent 
version does.)

On Thu, 29 Jun 2006, Horace Tso wrote:

> Hi list,
>
> I tried to load the package SAGx and failed because it complains it's
> looking for the Biobass which is not there. Then I looked up the package
> list and Biobass is not found.
>
> I'm trying to run the Jonckheere-Terpstra test and from what I see in
> the R archive, SAGx is the only place it's been implemented.
>
>> library(SAGx)
> Loading required package: multtest
> Loading required package: survival
> Loading required package: splines
> Loading required package: sma
> Error in loadNamespace(i, c(lib.loc, .libPaths())) :
>        there is no package called 'Biobase'
> Error: package/namespace load failed for 'SAGx'
>
>
> Horace W. Tso
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ecoinformatics at gmail.com  Thu Jun 29 21:44:55 2006
From: ecoinformatics at gmail.com (Xiaohua Dai)
Date: Thu, 29 Jun 2006 21:44:55 +0200
Subject: [R] How to use a for loop to generate two sequences
Message-ID: <15f8e67d0606291244q3db8f4deq44a90ef9e51ae276@mail.gmail.com>

Hi R users,

Hope the question is not too simple:

How to use a for loop to generate two lists as below:

testlist <- list(test1, test2, ..., test1000)
stringlist <- list("test1","test2",...,"test1000")

Thanks
Xiaohua


From tlumley at u.washington.edu  Thu Jun 29 21:51:18 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 29 Jun 2006 12:51:18 -0700 (PDT)
Subject: [R] How to use a for loop to generate two sequences
In-Reply-To: <15f8e67d0606291244q3db8f4deq44a90ef9e51ae276@mail.gmail.com>
References: <15f8e67d0606291244q3db8f4deq44a90ef9e51ae276@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0606291250370.11074@homer22.u.washington.edu>

On Thu, 29 Jun 2006, Xiaohua Dai wrote:

> Hi R users,
>
> Hope the question is not too simple:
>
> How to use a for loop to generate two lists as below:
>
> testlist <- list(test1, test2, ..., test1000)
> stringlist <- list("test1","test2",...,"test1000")

I don't know why you would want to use a for loop:

  stringlist<-as.list(paste("test",1:1000,sep=""))
  testlist<-lapply(stringlist, as.name)


 	-thomas


From noah78 at web.de  Thu Jun 29 22:03:04 2006
From: noah78 at web.de (=?ANSI_X3.4-1968?Q?J=3Frn_Schulz?=)
Date: Thu, 29 Jun 2006 13:03:04 -0700 (PDT)
Subject: [R] cat and positioning of the output
Message-ID: <5109359.post@talk.nabble.com>


Hello R users!

I like to use cat in a loop. I know, loops are not the best way in R ... but
my question: It is possible to overwrite the expression "Reading row: i" in
each iteration of the loop (print out in the below loop on the screen) or
more particulary to overwrite the counter "i".

for(i in 1:header$M){
   cat("Reading row: ", i)
   SparseIndex[[i]] <- readBin( con, integer(), n=MIndexNumber[i], size=4 )
   SparseSignal[[i]] <- readBin( con, numeric(), n=MIndexNumber[i], size=4 )
}

Many thanks
J?rn Schulz.
-- 
View this message in context: http://www.nabble.com/cat-and-positioning-of-the-output-tf1869521.html#a5109359
Sent from the R help forum at Nabble.com.


From ggrothendieck at gmail.com  Thu Jun 29 22:29:15 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 29 Jun 2006 16:29:15 -0400
Subject: [R] cat and positioning of the output
In-Reply-To: <5109359.post@talk.nabble.com>
References: <5109359.post@talk.nabble.com>
Message-ID: <971536df0606291329pc1cc4ecrebd0609ad855b7cf@mail.gmail.com>

On Windows XP this works for me:

for(i in 1:3) { if (i > 1) cat("\b"); cat(i); flush.console();
Sys.sleep(1) }; cat("\n")

It displays 1, then it overwrites it with 2 and then overwrites that with 3.


On 6/29/06, J?rn Schulz <noah78 at web.de> wrote:
>
> Hello R users!
>
> I like to use cat in a loop. I know, loops are not the best way in R ... but
> my question: It is possible to overwrite the expression "Reading row: i" in
> each iteration of the loop (print out in the below loop on the screen) or
> more particulary to overwrite the counter "i".
>
> for(i in 1:header$M){
>   cat("Reading row: ", i)
>   SparseIndex[[i]] <- readBin( con, integer(), n=MIndexNumber[i], size=4 )
>   SparseSignal[[i]] <- readBin( con, numeric(), n=MIndexNumber[i], size=4 )
> }
>
> Many thanks
> J?rn Schulz.
> --
> View this message in context: http://www.nabble.com/cat-and-positioning-of-the-output-tf1869521.html#a5109359
> Sent from the R help forum at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From phgrosjean at sciviews.org  Thu Jun 29 22:30:15 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Thu, 29 Jun 2006 22:30:15 +0200
Subject: [R] cat and positioning of the output
In-Reply-To: <5109359.post@talk.nabble.com>
References: <5109359.post@talk.nabble.com>
Message-ID: <44A43857.6000003@sciviews.org>


=?ANSI_X3.4-1968?Q?J=3Frn_Schulz?= wrote:
> Hello R users!
> 
> I like to use cat in a loop. I know, loops are not the best way in R ... but
> my question: It is possible to overwrite the expression "Reading row: i" in
> each iteration of the loop (print out in the below loop on the screen) or
> more particulary to overwrite the counter "i".

In package svMisc (SciViews bundle):

 > ?progress
 > example(progress)
Best,

Philippe Grosjean


> for(i in 1:header$M){
>    cat("Reading row: ", i)
>    SparseIndex[[i]] <- readBin( con, integer(), n=MIndexNumber[i], size=4 )
>    SparseSignal[[i]] <- readBin( con, numeric(), n=MIndexNumber[i], size=4 )
> }
> 
> Many thanks
> J?rn Schulz.


From dkgunter at lbl.gov  Thu Jun 29 22:33:57 2006
From: dkgunter at lbl.gov (Dan Gunter)
Date: Thu, 29 Jun 2006 13:33:57 -0700
Subject: [R] cat and positioning of the output
In-Reply-To: <5109359.post@talk.nabble.com>
References: <5109359.post@talk.nabble.com>
Message-ID: <44A43935.4020107@lbl.gov>

=?ANSI_X3.4-1968?Q?J=3Frn_Schulz?= wrote:
> Hello R users!
>
> I like to use cat in a loop. I know, loops are not the best way in R ... but
> my question: It is possible to overwrite the expression "Reading row: i" in
> each iteration of the loop (print out in the below loop on the screen) or
> more particulary to overwrite the counter "i".
>
> for(i in 1:header$M){
>    cat("Reading row: ", i)
>    SparseIndex[[i]] <- readBin( con, integer(), n=MIndexNumber[i], size=4 )
>    SparseSignal[[i]] <- readBin( con, numeric(), n=MIndexNumber[i], size=4 )
> }
>
> Many thanks
> J?rn Schulz.
>   
On a UNIX terminal, adding '\r' will cause the cursor to go back to the 
beginning of the same line. I can't speak for Windows.

e.g.

for ( i in 1:10 ) { cat("Number",i,"\r"); Sys.sleep(1) }; cat("\n")

-Dan

-- 
Dan Gunter. voice:510-495-2504 fax:510-486-6363 dsd.lbl.gov/~dang


From phgrosjean at sciviews.org  Thu Jun 29 22:53:44 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Thu, 29 Jun 2006 22:53:44 +0200
Subject: [R] cat and positioning of the output
In-Reply-To: <44A43935.4020107@lbl.gov>
References: <5109359.post@talk.nabble.com> <44A43935.4020107@lbl.gov>
Message-ID: <44A43DD8.3090902@sciviews.org>

Dan Gunter wrote:
> =?ANSI_X3.4-1968?Q?J=3Frn_Schulz?= wrote:
> 
>>Hello R users!
>>
>>I like to use cat in a loop. I know, loops are not the best way in R ... but
>>my question: It is possible to overwrite the expression "Reading row: i" in
>>each iteration of the loop (print out in the below loop on the screen) or
>>more particulary to overwrite the counter "i".
>>
>>for(i in 1:header$M){
>>   cat("Reading row: ", i)
>>   SparseIndex[[i]] <- readBin( con, integer(), n=MIndexNumber[i], size=4 )
>>   SparseSignal[[i]] <- readBin( con, numeric(), n=MIndexNumber[i], size=4 )
>>}
>>
>>Many thanks
>>J?rn Schulz.
>>  
> 
> On a UNIX terminal, adding '\r' will cause the cursor to go back to the 
> beginning of the same line. I can't speak for Windows.
> 
> e.g.
> 
> for ( i in 1:10 ) { cat("Number",i,"\r"); Sys.sleep(1) }; cat("\n")
> 
> -Dan

It works, but I don't like much strange things that can happen with "\r" 
because the caret is located at the beginning of the line. Look at what 
happens if you forgot the latests 'cat("\n")'. Also, note that for RGui 
under windows, you must use flush.console(). So:

 > for (i in 1:10) {
 >    cat("Number",i,"\r")
 >    flush.console()
 >    Sys.sleep(1)
 > }
 > cat("\n")

does the job. The alternative is to use "\b" repeatedly, which is done 
in progress() indeed.
Best,

Philippe Grosjean


From Mike.Prager at noaa.gov  Thu Jun 29 22:56:22 2006
From: Mike.Prager at noaa.gov (Michael H. Prager)
Date: Thu, 29 Jun 2006 16:56:22 -0400
Subject: [R] cat and positioning of the output
In-Reply-To: <44A43935.4020107@lbl.gov>
References: <5109359.post@talk.nabble.com> <44A43935.4020107@lbl.gov>
Message-ID: <44A43E76.7070903@noaa.gov>

It works on Windows (rgui.exe) with this change:

 for ( i in 1:10 ) { cat("Number",i,"\r"); flush.console(); Sys.sleep(1) 
}; cat("\n")

MHP


on 6/29/2006 4:33 PM Dan Gunter said the following:
> =?ANSI_X3.4-1968?Q?J=3Frn_Schulz?= wrote:
>   
>> Hello R users!
>>
>> I like to use cat in a loop. I know, loops are not the best way in R ... but
>> my question: It is possible to overwrite the expression "Reading row: i" in
>> each iteration of the loop (print out in the below loop on the screen) or
>> more particulary to overwrite the counter "i".
>>
>> for(i in 1:header$M){
>>    cat("Reading row: ", i)
>>    SparseIndex[[i]] <- readBin( con, integer(), n=MIndexNumber[i], size=4 )
>>    SparseSignal[[i]] <- readBin( con, numeric(), n=MIndexNumber[i], size=4 )
>> }
>>
>> Many thanks
>> J?rn Schulz.
>>   
>>     
> On a UNIX terminal, adding '\r' will cause the cursor to go back to the 
> beginning of the same line. I can't speak for Windows.
>
> e.g.
>
> for ( i in 1:10 ) { cat("Number",i,"\r"); Sys.sleep(1) }; cat("\n")
>
> -Dan
>
>   

-- 
Michael Prager, Ph.D.
Southeast Fisheries Science Center
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
** Opinions expressed are personal, not official.  No
** official endorsement of any product is made or implied.


From aarti_dahiya at hotmail.com  Thu Jun 29 22:59:00 2006
From: aarti_dahiya at hotmail.com (Aarti Dahiya)
Date: Thu, 29 Jun 2006 16:59:00 -0400
Subject: [R] Passing argument in nested function calls
Message-ID: <BAY106-F31BBE268E4D4E5A419A3EE37C0@phx.gbl>

Please, do not ask why you would do this.  I have simplified my complicated 
code to just the basic problem occuring to be able to make it easier to 
understand the actual problem.

I have two function getArgs() and extractArgs() - code and test results 
provided below.
I test them using getArgs(id = 's1002') from the command line.

getArgs() prints the name of the argument as id and the actual argument as 
s1002, which is correct.

But, when I call extractArgs() from within getArgs() passing value of passed 
(stored in passed, passed is created from paste(names(args),"=", 
sQuote(args))); extractArgs() treats the value of passed as a string "id = 
's1002'" and treats the whole thing as one argument.  So, it prints 
args[[1]] as the whole thing id = 's1002'", but prints blank for 
names(args[[1]]), obviously beacuse it cannot find a name for the argument.

If I call extractArgs(id = 's1002') from the command line, it works just 
fine, prints the name of the argument as id and the actual argument as 
s1002, which is correct.

How do I fix the problem so that when extractArgs() is called within 
getArgs(), it behaves the same way as getArgs() and extracts the argument 
and argument name?  Can we unstring passed or change it into another mode?

I'll appreciate any help or pointers.  Thanks.  Aarti

getArgs <- function(...)
{
    cat("\nEntering getArgs()\n\n")

    args <- list(...)

    cat("In getArgs(), names(args[1]) is ")
    cat(names(args[1]))
    cat(" and args[[1]] is ")
    cat(args[[1]])
    cat("\n")

    passed <- paste(names(args),"=", sQuote(args))

    extractArgs(passed)

    cat("Exiting getArgs()\n\n")
}

extractArgs <- function(...)
{
    cat("\nEntering extractArgs()\n\n")

    args <- list(...)

    cat("In extractArgs(), names(args[1]) is ")
    cat(names(args[1]))
    cat(" and args[[1]] is ")
    cat(args[[1]])
    cat("\n")

    cat("\nExiting extractArgs()\n\n")
}

Test:

>getArgs(id = 's1002')

Entering getArgs()

In getArgs(), names(args[1]) is id and args[[1]] is s1002

Entering extractArgs()

In extractArgs(), names(args[1]) is  and args[[1]] is id = 's1002'

Exiting extractArgs()

Exiting getArgs()

>extractArgs(id = 's1002')

Entering extractArgs()

In extractArgs(), names(args[1]) is id and args[[1]] is s1002

Exiting extractArgs()


From bolker at ufl.edu  Thu Jun 29 23:06:23 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Thu, 29 Jun 2006 21:06:23 +0000 (UTC)
Subject: [R] Transform Normal Dist to ArcTan Dist
References: <5E8C818A1F91C34A9E80DE55A38C75910B2F8E@avm-ex-s1.avm.local>
Message-ID: <loom.20060629T230238-245@post.gmane.org>

Luis Cota <luis.cota <at> avmltd.com> writes:

> 
> I have a set of data that is distributed normally.  How can I transform
> this data to fit an ArcTan distribution?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help <at> stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

  we need more information.  This is a stats question more than an R
question, and there isn't enough context here to know either (1) what
arctangent distribution you're talking about (it may be the Cauchy/
t-distribution with 1 df, or it may be something else) or (2) why
you want to transform the distribution, and whether it's a sensible
thing to do in the first place ... (see what the posting guide has
to say about statistical questions ...)

  Ben Bolker


From ll9f at cms.mail.virginia.edu  Thu Jun 29 23:33:59 2006
From: ll9f at cms.mail.virginia.edu (Lei  Liu)
Date: Thu, 29 Jun 2006 17:33:59 -0400
Subject: [R] help with coxme
Message-ID: <web-212457757@cgatepro-4.mail.virginia.edu>

Hi there,

I have a question on fitting data by coxme. In particular I want to fit a 
random intercept and random slope cox model. Using the rats dataset as an 
example, I generated another covariate x2 and want to specify a random slope 
for x2. Here is my code:

x2=matrix(rep(runif(50), 3), 50, 3)
x2=as.vector(t(x2))

rats2=cbind(rats, x2)

But when I used the coxme function as follows, it gave an error message. 
What is the right way to do it? Thanks a lot!

  coxme(Surv(time, status) ~ rx+x2, data=rats2, random=~ (1+x2)|litter )


Lei Liu
Assistant Professor
Division of Biostatistics and Epidemiology
Dept. of Public Health Sciences
School of Medicine
University of Virginia

3181 Hospital West Complex
Charlottesville, VA 22908-0717

1-434-982-3364 (o)
1-434-806-8086 (c)

liulei at virginia.edu
ll9f at virginia.edu


From ggrothendieck at gmail.com  Fri Jun 30 00:24:29 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 29 Jun 2006 18:24:29 -0400
Subject: [R] Passing argument in nested function calls
In-Reply-To: <BAY106-F31BBE268E4D4E5A419A3EE37C0@phx.gbl>
References: <BAY106-F31BBE268E4D4E5A419A3EE37C0@phx.gbl>
Message-ID: <971536df0606291524y3b2f079fxb52cd97a7b628711@mail.gmail.com>

getArgs should be:

getArgs <- function(...) extractArgs(...)

# test
extractArgs(x = 3)
getArgs(x = 3)


On 6/29/06, Aarti Dahiya <aarti_dahiya at hotmail.com> wrote:
> Please, do not ask why you would do this.  I have simplified my complicated
> code to just the basic problem occuring to be able to make it easier to
> understand the actual problem.
>
> I have two function getArgs() and extractArgs() - code and test results
> provided below.
> I test them using getArgs(id = 's1002') from the command line.
>
> getArgs() prints the name of the argument as id and the actual argument as
> s1002, which is correct.
>
> But, when I call extractArgs() from within getArgs() passing value of passed
> (stored in passed, passed is created from paste(names(args),"=",
> sQuote(args))); extractArgs() treats the value of passed as a string "id =
> 's1002'" and treats the whole thing as one argument.  So, it prints
> args[[1]] as the whole thing id = 's1002'", but prints blank for
> names(args[[1]]), obviously beacuse it cannot find a name for the argument.
>
> If I call extractArgs(id = 's1002') from the command line, it works just
> fine, prints the name of the argument as id and the actual argument as
> s1002, which is correct.
>
> How do I fix the problem so that when extractArgs() is called within
> getArgs(), it behaves the same way as getArgs() and extracts the argument
> and argument name?  Can we unstring passed or change it into another mode?
>
> I'll appreciate any help or pointers.  Thanks.  Aarti
>
> getArgs <- function(...)
> {
>    cat("\nEntering getArgs()\n\n")
>
>    args <- list(...)
>
>    cat("In getArgs(), names(args[1]) is ")
>    cat(names(args[1]))
>    cat(" and args[[1]] is ")
>    cat(args[[1]])
>    cat("\n")
>
>    passed <- paste(names(args),"=", sQuote(args))
>
>    extractArgs(passed)
>
>    cat("Exiting getArgs()\n\n")
> }
>
> extractArgs <- function(...)
> {
>    cat("\nEntering extractArgs()\n\n")
>
>    args <- list(...)
>
>    cat("In extractArgs(), names(args[1]) is ")
>    cat(names(args[1]))
>    cat(" and args[[1]] is ")
>    cat(args[[1]])
>    cat("\n")
>
>    cat("\nExiting extractArgs()\n\n")
> }
>
> Test:
>
> >getArgs(id = 's1002')
>
> Entering getArgs()
>
> In getArgs(), names(args[1]) is id and args[[1]] is s1002
>
> Entering extractArgs()
>
> In extractArgs(), names(args[1]) is  and args[[1]] is id = 's1002'
>
> Exiting extractArgs()
>
> Exiting getArgs()
>
> >extractArgs(id = 's1002')
>
> Entering extractArgs()
>
> In extractArgs(), names(args[1]) is id and args[[1]] is s1002
>
> Exiting extractArgs()
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Fri Jun 30 00:34:13 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 29 Jun 2006 18:34:13 -0400
Subject: [R] How to use a for loop to generate two sequences
In-Reply-To: <15f8e67d0606291244q3db8f4deq44a90ef9e51ae276@mail.gmail.com>
References: <15f8e67d0606291244q3db8f4deq44a90ef9e51ae276@mail.gmail.com>
Message-ID: <971536df0606291534g344c54c5offd9860fa2cf7523@mail.gmail.com>

A vector of character strings:

  nm <- paste("test", 1:1000, sep = "")

A list whose components are the objects: test1, test2, ...

  lapply(nm, get)

or if you do it this way the list will have 'test1', 'test2', ... as the
component names:

  L <- sapply(paste("test", 1:1000, sep = ""), get, simplify = FALSE)

and then

  names(L)

gives the vector of names.


Also note that

  apropos("^test[0-9]+")

will give a vector of object names corresponding to objects
 in the current workspace whose names
are "test" followed by a number.

On 6/29/06, Xiaohua Dai <ecoinformatics at gmail.com> wrote:
> Hi R users,
>
> Hope the question is not too simple:
>
> How to use a for loop to generate two lists as below:
>
> testlist <- list(test1, test2, ..., test1000)
> stringlist <- list("test1","test2",...,"test1000")
>
> Thanks
> Xiaohua
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From chatfield at alumni.rice.edu  Fri Jun 30 00:34:58 2006
From: chatfield at alumni.rice.edu (Robert Chatfield)
Date: Thu, 29 Jun 2006 15:34:58 -0700
Subject: [R] Reporting ppr fits and using them externally.
In-Reply-To: <Pine.LNX.4.64.0606280742100.7109@gannet.stats.ox.ac.uk>
References: <A5C6BB17-51AE-408A-B745-683FB91DE9E2@alumni.rice.edu>
	<Pine.LNX.4.64.0606280742100.7109@gannet.stats.ox.ac.uk>
Message-ID: <D3069D64-5FB5-45F0-9654-72C08B464AF7@alumni.rice.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060629/92f474f4/attachment.pl 

From spencer.graves at pdf.com  Fri Jun 30 01:57:08 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 29 Jun 2006 16:57:08 -0700
Subject: [R] NLME Fitting
In-Reply-To: <1151492696.44a2625892b3d@webmail.shef.ac.uk>
References: <1151492696.44a2625892b3d@webmail.shef.ac.uk>
Message-ID: <44A468D4.4080406@pdf.com>

	  I'm not certain what you want, but it sounds like the answer might be 
in '?nlme' help page.  Toward the end, it describes the 'Value' returned 
as follows:

    an object of class 'nlme' representing the nonlinear mixed-effects
      model fit. Generic functions such as 'print', 'plot' and 'summary'
      have methods to show the results of the fit. See 'nlmeObject' for
      the components of the fit. The functions 'resid', 'coef',
      'fitted', 'fixed.effects', and 'random.effects'  can be used to
      extract some of its components.

	  Also, 'str' might help you find what you want.

	  Spencer Graves
p.s.  If you'd like further help from this listserve, PLEASE do read the 
posting guide! "www.R-project.org/posting-guide.html".  This might help 
you write a question so it is easier for others to understand, thereby 
increasing the chances for a quick and useful reply.

F Monadjemi wrote:
> Dear Reader,
> 
> Is it possible to extract the random part of nlme fitting analysis (non linear
> mixed effect model) i.e.sigma(b), in R?
> 
> Thank you for respond
> 
> 
> Farinaz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ecoinformatics at gmail.com  Fri Jun 30 02:00:06 2006
From: ecoinformatics at gmail.com (Xiaohua Dai)
Date: Fri, 30 Jun 2006 02:00:06 +0200
Subject: [R] How to use a for loop to generate two sequences
In-Reply-To: <971536df0606291534g344c54c5offd9860fa2cf7523@mail.gmail.com>
References: <15f8e67d0606291244q3db8f4deq44a90ef9e51ae276@mail.gmail.com>
	<971536df0606291534g344c54c5offd9860fa2cf7523@mail.gmail.com>
Message-ID: <15f8e67d0606291700pf057abev62678b69550a622b@mail.gmail.com>

I followed your method but there was an error on FUN get:

> nm <- paste("test", 1:1000, sep = "")
> lapply(nm, get)
Error in get(x, envir, mode, inherits) : variable "test1" was not found
> L <- sapply(paste("test", 1:1000, sep = ""), get, simplify = FALSE)
Error in get(x, envir, mode, inherits) : variable "test1" was not found

But lapply(nm, as.name) worked well. Are there any problem with my R env?

> traceback()
2: FUN(X[[1]], ...)
1: lapply(nm, get)

> sessionInfo()
Version 2.3.1 (2006-06-01)
i386-pc-mingw32

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

> win.version()
[1] "Windows XP Professional (build 2600) Service Pack 2.0"


Thanks
Xiaohua


On 6/30/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> A vector of character strings:
>
>  nm <- paste("test", 1:1000, sep = "")
>
> A list whose components are the objects: test1, test2, ...
>
>  lapply(nm, get)
>
> or if you do it this way the list will have 'test1', 'test2', ... as the
> component names:
>
>  L <- sapply(paste("test", 1:1000, sep = ""), get, simplify = FALSE)
>
> and then
>
>  names(L)
>
> gives the vector of names.
>
>
> Also note that
>
>  apropos("^test[0-9]+")
>
> will give a vector of object names corresponding to objects
>  in the current workspace whose names
> are "test" followed by a number.
>
> On 6/29/06, Xiaohua Dai <ecoinformatics at gmail.com> wrote:
> > Hi R users,
> >
> > Hope the question is not too simple:
> >
> > How to use a for loop to generate two lists as below:
> >
> > testlist <- list(test1, test2, ..., test1000)
> > stringlist <- list("test1","test2",...,"test1000")
> >
> > Thanks
> > Xiaohua
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >


From maj at stats.waikato.ac.nz  Fri Jun 30 02:15:32 2006
From: maj at stats.waikato.ac.nz (maj at stats.waikato.ac.nz)
Date: Fri, 30 Jun 2006 12:15:32 +1200 (NZST)
Subject: [R] Extracting R plots from MS Word
In-Reply-To: <971536df0606290322p1e1d7e09s88fa6649514b4896@mail.gmail.com>
References: <62025.58.105.171.47.1151558108.squirrel@webmail.scms.waikato.ac.nz>
	<971536df0606290322p1e1d7e09s88fa6649514b4896@mail.gmail.com>
Message-ID: <61039.58.105.171.47.1151626532.squirrel@webmail.scms.waikato.ac.nz>

Thanks to all those who responded to my request about extracting R plots
from MS Word. I decided to try Gabor Grothendieck's second suggestion and
saved the Word document as html. (I may yet try some of the other
suggestions.)

Saving the .doc as .htm indeed produced a folder with many of the images
in .gif form. However some of my images seemed to be described by files
with .wmz or .emz extensions. I am not quite sure how to change these into
.gif or .eps or anything useful!

Cheers,  Murray Jorgensen

> Click the graphic, press ctrl-C to copy it to the clipboard and then
> using ctrl-V paste it into mspaint or Xnview (free, find it via google) or
> other
> graphics program and then save it from there.   Which program will
> work will depend on the format of the image.
>
> Another possibility is to save the Word file in HTML format.  In
> Word, choose File | SaveAs and choose the Save As Type to be Web Page.
> That will create an HTML file plus it will create a folder with
> one file per image.
>
> On 6/29/06, maj at stats.waikato.ac.nz <maj at stats.waikato.ac.nz> wrote:
>> Hi,
>>
>> I am revising a paper that I am a co-author of. The figures are plots
>> generated from R but at the moment I do not have the R code that
>> generates
>> them.
>>
>> As this is time critical I would like to slightly abuse the list by
>> asking
>> whether anyone knows how to extract from MS Word into a stand-alone
>> graphics file a plot that was pasted into Word from R (probably as a
>> Windows Metafile, but possibly as a bitmap).
>>
>> I would be very grateful for help with this.
>>
>> Regards,  Murray Jorgensen
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>


From ggrothendieck at gmail.com  Fri Jun 30 02:24:47 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 29 Jun 2006 20:24:47 -0400
Subject: [R] Extracting R plots from MS Word
In-Reply-To: <61039.58.105.171.47.1151626532.squirrel@webmail.scms.waikato.ac.nz>
References: <62025.58.105.171.47.1151558108.squirrel@webmail.scms.waikato.ac.nz>
	<971536df0606290322p1e1d7e09s88fa6649514b4896@mail.gmail.com>
	<61039.58.105.171.47.1151626532.squirrel@webmail.scms.waikato.ac.nz>
Message-ID: <971536df0606291724l2f066061v6a3a66e4c00f2d68@mail.gmail.com>

I think these are zipped emf and wmf files.  Google for emz extension.

On 6/29/06, maj at stats.waikato.ac.nz <maj at stats.waikato.ac.nz> wrote:
> Thanks to all those who responded to my request about extracting R plots
> from MS Word. I decided to try Gabor Grothendieck's second suggestion and
> saved the Word document as html. (I may yet try some of the other
> suggestions.)
>
> Saving the .doc as .htm indeed produced a folder with many of the images
> in .gif form. However some of my images seemed to be described by files
> with .wmz or .emz extensions. I am not quite sure how to change these into
> .gif or .eps or anything useful!
>
> Cheers,  Murray Jorgensen
>
> > Click the graphic, press ctrl-C to copy it to the clipboard and then
> > using ctrl-V paste it into mspaint or Xnview (free, find it via google) or
> > other
> > graphics program and then save it from there.   Which program will
> > work will depend on the format of the image.
> >
> > Another possibility is to save the Word file in HTML format.  In
> > Word, choose File | SaveAs and choose the Save As Type to be Web Page.
> > That will create an HTML file plus it will create a folder with
> > one file per image.
> >
> > On 6/29/06, maj at stats.waikato.ac.nz <maj at stats.waikato.ac.nz> wrote:
> >> Hi,
> >>
> >> I am revising a paper that I am a co-author of. The figures are plots
> >> generated from R but at the moment I do not have the R code that
> >> generates
> >> them.
> >>
> >> As this is time critical I would like to slightly abuse the list by
> >> asking
> >> whether anyone knows how to extract from MS Word into a stand-alone
> >> graphics file a plot that was pasted into Word from R (probably as a
> >> Windows Metafile, but possibly as a bitmap).
> >>
> >> I would be very grateful for help with this.
> >>
> >> Regards,  Murray Jorgensen
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >>
> >
>
>
>


From ggrothendieck at gmail.com  Fri Jun 30 02:29:10 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 29 Jun 2006 20:29:10 -0400
Subject: [R] How to use a for loop to generate two sequences
In-Reply-To: <15f8e67d0606291700pf057abev62678b69550a622b@mail.gmail.com>
References: <15f8e67d0606291244q3db8f4deq44a90ef9e51ae276@mail.gmail.com>
	<971536df0606291534g344c54c5offd9860fa2cf7523@mail.gmail.com>
	<15f8e67d0606291700pf057abev62678b69550a622b@mail.gmail.com>
Message-ID: <971536df0606291729g5653bfbbpe14a5bd62f552d83@mail.gmail.com>

On 6/29/06, Xiaohua Dai <ecoinformatics at gmail.com> wrote:
> I followed your method but there was an error on FUN get:
>
> > nm <- paste("test", 1:1000, sep = "")
> > lapply(nm, get)
> Error in get(x, envir, mode, inherits) : variable "test1" was not found
> > L <- sapply(paste("test", 1:1000, sep = ""), get, simplify = FALSE)
> Error in get(x, envir, mode, inherits) : variable "test1" was not found
>
> But lapply(nm, as.name) worked well. Are there any problem with my R env?

These are different things.  Using as.name gives you a list of names
but I am assuming you are looking for a list of the objects themselves
so you need to have those objects defined.  Here is a reproducible
example:

> test1 <- test2 <- 1
> sapply(paste("test", 1:2, sep = ""), get, simplify = FALSE)
$test1
[1] 1

$test2
[1] 1





>
> > traceback()
> 2: FUN(X[[1]], ...)
> 1: lapply(nm, get)
>
> > sessionInfo()
> Version 2.3.1 (2006-06-01)
> i386-pc-mingw32
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
>
> > win.version()
> [1] "Windows XP Professional (build 2600) Service Pack 2.0"
>
>
> Thanks
> Xiaohua
>
>
> On 6/30/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > A vector of character strings:
> >
> >  nm <- paste("test", 1:1000, sep = "")
> >
> > A list whose components are the objects: test1, test2, ...
> >
> >  lapply(nm, get)
> >
> > or if you do it this way the list will have 'test1', 'test2', ... as the
> > component names:
> >
> >  L <- sapply(paste("test", 1:1000, sep = ""), get, simplify = FALSE)
> >
> > and then
> >
> >  names(L)
> >
> > gives the vector of names.
> >
> >
> > Also note that
> >
> >  apropos("^test[0-9]+")
> >
> > will give a vector of object names corresponding to objects
> >  in the current workspace whose names
> > are "test" followed by a number.
> >
> > On 6/29/06, Xiaohua Dai <ecoinformatics at gmail.com> wrote:
> > > Hi R users,
> > >
> > > Hope the question is not too simple:
> > >
> > > How to use a for loop to generate two lists as below:
> > >
> > > testlist <- list(test1, test2, ..., test1000)
> > > stringlist <- list("test1","test2",...,"test1000")
> > >
> > > Thanks
> > > Xiaohua
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > >
>


From mcclatchie.sam at saugov.sa.gov.au  Fri Jun 30 03:41:22 2006
From: mcclatchie.sam at saugov.sa.gov.au (McClatchie, Sam (PIRSA-SARDI))
Date: Fri, 30 Jun 2006 11:11:22 +0930
Subject: [R] Trellis.par.set/ family/ global change font?
Message-ID: <F9F85A0E26512A44A530B8DDD317D09B0182A7DD@sagemsg0028.sagemsmrd01.sa.gov.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060630/539d4c7f/attachment.pl 

From spencer.graves at pdf.com  Fri Jun 30 04:08:16 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 29 Jun 2006 19:08:16 -0700
Subject: [R] lme convergence
In-Reply-To: <002001c69ad3$38073050$7c94100a@win.ad.jhu.edu>
References: <002001c69ad3$38073050$7c94100a@win.ad.jhu.edu>
Message-ID: <44A48790.9030002@pdf.com>

	  Does anyone know how to obtain the 'returnObject' from an 'lme' run 
that fails to converge?  An argument of this name is described on the 
'lmeControl' help page as, "a logical value indicating whether the 
fitted object should be returned when the maximum number of iterations 
is reached without convergence of the algorithm. Default is 'FALSE'."

	  Unfortunately, I've so far been unable to get it to work, as 
witnessed by the following modification of an example from the '?lme' 
help page:

 > library(nlme)
 > fm1 <- lme(distance ~ age, data = Orthodont,
+            control=lmeControl(msMaxIter=1))
Error in lme.formula(distance ~ age, data = Orthodont, control = 
lmeControl(msMaxIter = 1)) :
	iteration limit reached without convergence (9)
 > fm1
Error: object "fm1" not found
 > fm1 <- lme(distance ~ age, data = Orthodont,
+            control=lmeControl(msMaxIter=1,
+              returnObject=TRUE))
Error in lme.formula(distance ~ age, data = Orthodont, control = 
lmeControl(msMaxIter = 1,  :
	iteration limit reached without convergence (9)
 > fm1
Error: object "fm1" not found	

	  I might be able to fix the problem myself, working through the 'lme' 
code line by line, e.g., using 'debug'.  However, I'm not ready to do 
that just now.

	  Best Wishes,
	  Spencer Graves

Ravi Varadhan wrote:
> Use "try" to capture error messages without breaking the loop.
> ?try
> 
> --------------------------------------------------------------------------
> Ravi Varadhan, Ph.D.
> Assistant Professor,  The Center on Aging and Health
> Division of Geriatric Medicine and Gerontology
> Johns Hopkins University
> Ph: (410) 502-2619
> Fax: (410) 614-9625
> Email:  rvaradhan at jhmi.edu
> Webpage: http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html 
> --------------------------------------------------------------------------
> 
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
>> bounces at stat.math.ethz.ch] On Behalf Of Pryseley Assam
>> Sent: Wednesday, June 28, 2006 12:18 PM
>> To: R-Users
>> Subject: [R] lme convergence
>>
>> Dear R-Users,
>>
>>   Is it possible to get the covariance matrix from an lme model that did
>> not converge ?
>>
>>   I am doing a simulation which entails fitting linear mixed models, using
>> a "for loop".
>>   Within each loop, i generate a new data set and analyze it using a mixed
>> model.  The loop stops When the "lme function" does not converge for a
>> simulated dataset. I want to inquire if there is a method to suppress the
>> error message from the lme function, or better still, a way of going about
>> this issue of the loop ending once the lme function does not converge.
>>
>>   Thanks in advance,
>>   Pryseley
>>
>>
>> ---------------------------------
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-
>> guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From wwguocn at gmail.com  Fri Jun 30 04:54:47 2006
From: wwguocn at gmail.com (Guo Wei-Wei)
Date: Fri, 30 Jun 2006 10:54:47 +0800
Subject: [R]  aggregate data.frame by one column
Message-ID: <d3677d7d0606291954o18d0761bne51fb54a7678d2ce@mail.gmail.com>

Hi, everyone,

I have a data.frame named "eva" like this:

IND PARTNO VC1 EO1 EO2 EO3 EO4 EO5
114 114001   2   5   4   4   5   4
114 114001   2   4   4   4   4   4
114 114001   2   4  NA  NA  NA  NA
112 112002   2   3   3   6   2   6
112 112002   2   1   1   3   4   4
112 112003   2   6   6   6   5   6
112 112003   2   5   7   6   6   6
112 112003   2   6   6   6   4   5
114 114004   2   2   3   3   2   4
114 114004   2   5   3   4   4   2
114 114004   2  NA  NA  NA  NA  NA
113 113005   2   5   5   6   6   5
113 113005   2   7   7   4   7   6
111 111006   2   5   7   7   7   7
112 112007   2   7   7   7   2   2
112 112007   2   6   6   6   1   2
112 112007   2   7   6   6   2   2
111 111008   2   4   1   3   1   4
111 111008   2   3   1   5   3   2

This is only a small part of the whole data. "PARTNO" is a digit variable
and I want to use it as a group variable to aggreate other variables.
What I want to get looks like this:

IND PARTNO NUM VC1 EO1 EO2 EO3 EO4 EO5
114 114001   3   2 4.3   4   4 4.5   4
112 112002   2   2   2   2 4.5   3   5
112 112003   3   2 5.7 6.3   6   5 5.7
114 114004   3   2 3.5   3 3.5   3   3
113 113005   2   2   6   6   5 6.5 5.5
111 111006   1   2   5   7   7   7   7
112 112007   3   2 6.7 6.3 6.3 1.7   2
111 111008   2   2 3.5   1   4   2   3

"NUM" is a newly added variable which indicates the case number
of each group grouped by "PARTNO".

I have two questions on this manipulation.

The first is how to get the newly added variable "NUM". I have no idea
on this question.

The second is how to average other variables by group. If there are
"NA", I want
the average operation is done on other cases. For example, the
variable "EO1" has
values of 2, 5, and "NA" on case 114004. What I have done is

> aggregate(eva[,-2], by=eva[,-2], mean)

But it seems because there are "NA"s, the "aggregate" cannot process.
Because the "NA" values are not a small part, I cannot use imputation
methods. I'm not sure whether my operation is right.

Does anyone have any suggestion on the two problems? Thanks in advance!


From deepayan.sarkar at gmail.com  Fri Jun 30 05:05:11 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 29 Jun 2006 22:05:11 -0500
Subject: [R] Trellis.par.set/ family/ global change font?
In-Reply-To: <F9F85A0E26512A44A530B8DDD317D09B0182A7DD@sagemsg0028.sagemsmrd01.sa.gov.au>
References: <F9F85A0E26512A44A530B8DDD317D09B0182A7DD@sagemsg0028.sagemsmrd01.sa.gov.au>
Message-ID: <eb555e660606292005i4126ef2eyeda48d7be3356ab1@mail.gmail.com>

On 6/29/06, McClatchie, Sam (PIRSA-SARDI)
<mcclatchie.sam at saugov.sa.gov.au> wrote:
> Background:
> OS: Linux Ubuntu Dapper 6.06
> release: R 2.3.1
> editor: GNU Emacs 21.4.1
> front-end: ESS 5.2.3
> ---------------------------------
>
> Colleagues
>
> I have a rather complicated trellis plot that a journal editor has requested I edit and change all the fonts to times.
>
> I'd like to change all fonts globally for the plot, as in par(family="serif") for non-trellis plots. Various experiments with trellis.par.set after reading the help page have not solved the problem for me. Even doing the local change
> trellis.par.set(par.xlab.text=list(cex=1.5, family="serif")) does not change the font to times for xlab (I mean my syntax is wrong, not that there is a bug).
>
> So I'm obviously misreading the help page or just missing the meaning. Any suggestions?

Lattice uses 'fontfamily' rather than 'family' (borrowed from grid, I
suppose). I don't think there's a way to set the family globally. You
might try

trellis.par.set(grid.pars = list(fontfamily = "serif"))

but I'm not sure if that will work.

-Deepayan


From A.Robinson at ms.unimelb.edu.au  Fri Jun 30 05:25:04 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Fri, 30 Jun 2006 13:25:04 +1000
Subject: [R] aggregate data.frame by one column
In-Reply-To: <d3677d7d0606291954o18d0761bne51fb54a7678d2ce@mail.gmail.com>
References: <d3677d7d0606291954o18d0761bne51fb54a7678d2ce@mail.gmail.com>
Message-ID: <20060630032504.GN758@ms.unimelb.edu.au>

Hi Wei-Wei,

try this:

eva.agg <- aggregate(x = list(
                       VC1=eva$VC1,
                       EO1=eva$EO1,
                       EO2=eva$EO2,
                       EO3=eva$EO3,
                       EO4=eva$EO4,
                       EO5=eva$EO5
                       ),
                     by = list(PARTNO=eva$PARTNO),
                     FUN = mean, na.rm = TRUE)

eva.agg$NUM <- aggregate(eva$PARTNO, list(eva$PARTNO), length)


Cheers

Andrew


On Fri, Jun 30, 2006 at 10:54:47AM +0800, Guo Wei-Wei wrote:
> Hi, everyone,
> 
> I have a data.frame named "eva" like this:
> 
> IND PARTNO VC1 EO1 EO2 EO3 EO4 EO5
> 114 114001   2   5   4   4   5   4
> 114 114001   2   4   4   4   4   4
> 114 114001   2   4  NA  NA  NA  NA
> 112 112002   2   3   3   6   2   6
> 112 112002   2   1   1   3   4   4
> 112 112003   2   6   6   6   5   6
> 112 112003   2   5   7   6   6   6
> 112 112003   2   6   6   6   4   5
> 114 114004   2   2   3   3   2   4
> 114 114004   2   5   3   4   4   2
> 114 114004   2  NA  NA  NA  NA  NA
> 113 113005   2   5   5   6   6   5
> 113 113005   2   7   7   4   7   6
> 111 111006   2   5   7   7   7   7
> 112 112007   2   7   7   7   2   2
> 112 112007   2   6   6   6   1   2
> 112 112007   2   7   6   6   2   2
> 111 111008   2   4   1   3   1   4
> 111 111008   2   3   1   5   3   2
> 
> This is only a small part of the whole data. "PARTNO" is a digit variable
> and I want to use it as a group variable to aggreate other variables.
> What I want to get looks like this:
> 
> IND PARTNO NUM VC1 EO1 EO2 EO3 EO4 EO5
> 114 114001   3   2 4.3   4   4 4.5   4
> 112 112002   2   2   2   2 4.5   3   5
> 112 112003   3   2 5.7 6.3   6   5 5.7
> 114 114004   3   2 3.5   3 3.5   3   3
> 113 113005   2   2   6   6   5 6.5 5.5
> 111 111006   1   2   5   7   7   7   7
> 112 112007   3   2 6.7 6.3 6.3 1.7   2
> 111 111008   2   2 3.5   1   4   2   3
> 
> "NUM" is a newly added variable which indicates the case number
> of each group grouped by "PARTNO".
> 
> I have two questions on this manipulation.
> 
> The first is how to get the newly added variable "NUM". I have no idea
> on this question.
> 
> The second is how to average other variables by group. If there are
> "NA", I want
> the average operation is done on other cases. For example, the
> variable "EO1" has
> values of 2, 5, and "NA" on case 114004. What I have done is
> 
> > aggregate(eva[,-2], by=eva[,-2], mean)
> 
> But it seems because there are "NA"s, the "aggregate" cannot process.
> Because the "NA" values are not a small part, I cannot use imputation
> methods. I'm not sure whether my operation is right.
> 
> Does anyone have any suggestion on the two problems? Thanks in advance!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From wwguocn at gmail.com  Fri Jun 30 06:23:43 2006
From: wwguocn at gmail.com (Guo Wei-Wei)
Date: Fri, 30 Jun 2006 12:23:43 +0800
Subject: [R] aggregate data.frame by one column
In-Reply-To: <20060630032504.GN758@ms.unimelb.edu.au>
References: <d3677d7d0606291954o18d0761bne51fb54a7678d2ce@mail.gmail.com>
	<20060630032504.GN758@ms.unimelb.edu.au>
Message-ID: <d3677d7d0606292123v2bac14fax1dab175f40229150@mail.gmail.com>

Hi Andrew,

Thank you very much! It works so well than I can expect.

All the best,
Wei-Wei

2006/6/30, Andrew Robinson <A.Robinson at ms.unimelb.edu.au>:
> Hi Wei-Wei,
>
> try this:
>
> eva.agg <- aggregate(x = list(
>                        VC1=eva$VC1,
>                        EO1=eva$EO1,
>                        EO2=eva$EO2,
>                        EO3=eva$EO3,
>                        EO4=eva$EO4,
>                        EO5=eva$EO5
>                        ),
>                      by = list(PARTNO=eva$PARTNO),
>                      FUN = mean, na.rm = TRUE)
>
> eva.agg$NUM <- aggregate(eva$PARTNO, list(eva$PARTNO), length)
>
> Cheers
>
> Andrew
>


From Augusto.Sanabria at ga.gov.au  Fri Jun 30 07:57:17 2006
From: Augusto.Sanabria at ga.gov.au (Augusto.Sanabria at ga.gov.au)
Date: Fri, 30 Jun 2006 15:57:17 +1000
Subject: [R] Empirical CDF
Message-ID: <9707EBA615A57747A0668CECD4638A300136FC5F@mail.agso.gov.au>


Good day everyone,

I want to assess the error when fitting a Gram-Charlier
CDF to some data 'ws', that is, I want to calculate:

     Err = |ecdf(ws) - GCh_ser(ws)|

The problem is, I cannot get the F(x) values from the
ecdf.

'Summary(ecdf())' returns some of the x-axis values,
but how do you get the F(x) values?

Thank you for any help you can provide.

Regards,

Augusto


--------------------------------------------
Augusto Sanabria. MSc, PhD.
Mathematical Modeller
Risk Research Group
Geospatial & Earth Monitoring Division
Geoscience Australia (www.ga.gov.au)
Cnr. Jerrabomberra Av. & Hindmarsh Dr.
Symonston ACT 2609
Ph. (02) 6249-9155


From renaud.lancelot at gmail.com  Fri Jun 30 08:12:47 2006
From: renaud.lancelot at gmail.com (Renaud Lancelot)
Date: Fri, 30 Jun 2006 08:12:47 +0200
Subject: [R] Trellis.par.set/ family/ global change font?
In-Reply-To: <eb555e660606292005i4126ef2eyeda48d7be3356ab1@mail.gmail.com>
References: <F9F85A0E26512A44A530B8DDD317D09B0182A7DD@sagemsg0028.sagemsmrd01.sa.gov.au>
	<eb555e660606292005i4126ef2eyeda48d7be3356ab1@mail.gmail.com>
Message-ID: <c2ee56800606292312n6d1e1ca9k903f13c43195a009@mail.gmail.com>

A radical solution is to edit the file .../etc/Rdevga and to change
the definition of the first set of fonts, e.g.

# TT Arial : plain
# TT Arial : bold
# TT Arial : italic
# TT Arial : bold&italic

TT Times New Roman : plain
TT Times New Roman : bold
TT Times New Roman : italic
TT Times New Roman : bold&italic

Best,

Renaud

2006/6/30, Deepayan Sarkar <deepayan.sarkar at gmail.com>:
> On 6/29/06, McClatchie, Sam (PIRSA-SARDI)
> <mcclatchie.sam at saugov.sa.gov.au> wrote:
> > Background:
> > OS: Linux Ubuntu Dapper 6.06
> > release: R 2.3.1
> > editor: GNU Emacs 21.4.1
> > front-end: ESS 5.2.3
> > ---------------------------------
> >
> > Colleagues
> >
> > I have a rather complicated trellis plot that a journal editor has requested I edit and change all the fonts to times.
> >
> > I'd like to change all fonts globally for the plot, as in par(family="serif") for non-trellis plots. Various experiments with trellis.par.set after reading the help page have not solved the problem for me. Even doing the local change
> > trellis.par.set(par.xlab.text=list(cex=1.5, family="serif")) does not change the font to times for xlab (I mean my syntax is wrong, not that there is a bug).
> >
> > So I'm obviously misreading the help page or just missing the meaning. Any suggestions?
>
> Lattice uses 'fontfamily' rather than 'family' (borrowed from grid, I
> suppose). I don't think there's a way to set the family globally. You
> might try
>
> trellis.par.set(grid.pars = list(fontfamily = "serif"))
>
> but I'm not sure if that will work.
>
> -Deepayan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
Renaud LANCELOT
D?partement Elevage et M?decine V?t?rinaire (EMVT) du CIRAD
Directeur adjoint charg? des affaires scientifiques

CIRAD, Animal Production and Veterinary Medicine Department
Deputy director for scientific affairs

Campus international de Baillarguet
TA 30 / B (B?t. B, Bur. 214)
34398 Montpellier Cedex 5 - France
T?l   +33 (0)4 67 59 37 17
Secr. +33 (0)4 67 59 39 04
Fax   +33 (0)4 67 59 37 95


From ripley at stats.ox.ac.uk  Fri Jun 30 08:35:29 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 30 Jun 2006 07:35:29 +0100 (BST)
Subject: [R] Trellis.par.set/ family/ global change font?
In-Reply-To: <c2ee56800606292312n6d1e1ca9k903f13c43195a009@mail.gmail.com>
References: <F9F85A0E26512A44A530B8DDD317D09B0182A7DD@sagemsg0028.sagemsmrd01.sa.gov.au>
	<eb555e660606292005i4126ef2eyeda48d7be3356ab1@mail.gmail.com>
	<c2ee56800606292312n6d1e1ca9k903f13c43195a009@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0606300729160.25140@gannet.stats.ox.ac.uk>

On Fri, 30 Jun 2006, Renaud Lancelot wrote:

> A radical solution is to edit the file .../etc/Rdevga and to change
> the definition of the first set of fonts, e.g.
>
> # TT Arial : plain
> # TT Arial : bold
> # TT Arial : italic
> # TT Arial : bold&italic
>
> TT Times New Roman : plain
> TT Times New Roman : bold
> TT Times New Roman : italic
> TT Times New Roman : bold&italic

He is on Linux, so that will not work.  We have not been told the graphics 
device, but both postscript() and pdf() allow the device to be set when 
the device is opened, and that works for me:

pdf(family="serif")
example(xyplot)
dev.off()


>
> Best,
>
> Renaud
>
> 2006/6/30, Deepayan Sarkar <deepayan.sarkar at gmail.com>:
>> On 6/29/06, McClatchie, Sam (PIRSA-SARDI)
>> <mcclatchie.sam at saugov.sa.gov.au> wrote:
>>> Background:
>>> OS: Linux Ubuntu Dapper 6.06
>>> release: R 2.3.1
>>> editor: GNU Emacs 21.4.1
>>> front-end: ESS 5.2.3
>>> ---------------------------------
>>>
>>> Colleagues
>>>
>>> I have a rather complicated trellis plot that a journal editor has requested I edit and change all the fonts to times.
>>>
>>> I'd like to change all fonts globally for the plot, as in par(family="serif") for non-trellis plots. Various experiments with trellis.par.set after reading the help page have not solved the problem for me. Even doing the local change
>>> trellis.par.set(par.xlab.text=list(cex=1.5, family="serif")) does not change the font to times for xlab (I mean my syntax is wrong, not that there is a bug).
>>>
>>> So I'm obviously misreading the help page or just missing the meaning. Any suggestions?
>>
>> Lattice uses 'fontfamily' rather than 'family' (borrowed from grid, I
>> suppose). I don't think there's a way to set the family globally. You
>> might try
>>
>> trellis.par.set(grid.pars = list(fontfamily = "serif"))
>>
>> but I'm not sure if that will work.
>>
>> -Deepayan
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From noah78 at web.de  Fri Jun 30 09:11:08 2006
From: noah78 at web.de (=?ANSI_X3.4-1968?Q?J=3Frn_Schulz?=)
Date: Fri, 30 Jun 2006 00:11:08 -0700 (PDT)
Subject: [R] cat and positioning of the output
In-Reply-To: <44A43DD8.3090902@sciviews.org>
References: <5109359.post@talk.nabble.com> <44A43935.4020107@lbl.gov>
	<44A43DD8.3090902@sciviews.org>
Message-ID: <5115737.post@talk.nabble.com>


Thanks at all for your carefully help. I decided to used both version of your
proposals and to combine with a request of the operating system.

All the best - J?rn Schulz.
-- 
View this message in context: http://www.nabble.com/cat-and-positioning-of-the-output-tf1869521.html#a5115737
Sent from the R help forum at Nabble.com.


From p.dalgaard at biostat.ku.dk  Fri Jun 30 09:41:22 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Jun 2006 09:41:22 +0200
Subject: [R] Empirical CDF
In-Reply-To: <9707EBA615A57747A0668CECD4638A300136FC5F@mail.agso.gov.au>
References: <9707EBA615A57747A0668CECD4638A300136FC5F@mail.agso.gov.au>
Message-ID: <x2zmfvt7j1.fsf@turmalin.kubism.ku.dk>

<Augusto.Sanabria at ga.gov.au> writes:

> Good day everyone,
> 
> I want to assess the error when fitting a Gram-Charlier
> CDF to some data 'ws', that is, I want to calculate:
> 
>      Err = |ecdf(ws) - GCh_ser(ws)|
> 
> The problem is, I cannot get the F(x) values from the
> ecdf.
> 
> 'Summary(ecdf())' returns some of the x-axis values,
> but how do you get the F(x) values?

By applying the function returned by ecdf?

Fn <- ecdf(some.data)
Fn(some.other.data)

should work. 
 
> Thank you for any help you can provide.
> 
> Regards,
> 
> Augusto
> 
> 
> --------------------------------------------
> Augusto Sanabria. MSc, PhD.
> Mathematical Modeller
> Risk Research Group
> Geospatial & Earth Monitoring Division
> Geoscience Australia (www.ga.gov.au)
> Cnr. Jerrabomberra Av. & Hindmarsh Dr.
> Symonston ACT 2609
> Ph. (02) 6249-9155
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From Bernhard_Pfaff at fra.invesco.com  Fri Jun 30 11:08:25 2006
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Fri, 30 Jun 2006 10:08:25 +0100
Subject: [R] Cointegration Test in R
In-Reply-To: <001e01c69bb1$07e54360$624afea9@fa162df94e0b40b>
Message-ID: <E4A9111DA23BA048B9A46686BF727CF403915D@DEFRAXMB01.corp.amvescap.net>

Hello Dennis,

have you considered the function bh6lrtest() in the package urca? 
To my knowledge, there is no other package available that offers VECM functionalities. 

Best,
Bernhard

ps: As you migth be interested in VAR and SVAR too: I am currently working on such a package which should be submitted to CRAN after summer. 

Dr. Bernhard Pfaff
Global Structured Products Group
(Europe)

Invesco Asset Management Deutschland GmbH
Bleichstrasse 60-62
D-60313 Frankfurt am Main

Tel: +49(0)69 29807 230
Fax: +49(0)69 29807 178
Email: bernhard_pfaff at fra.invesco.com  

>-----Urspr?ngliche Nachricht-----
>Von: r-help-bounces at stat.math.ethz.ch 
>[mailto:r-help-bounces at stat.math.ethz.ch] Im Auftrag von 
>Dennis Heidemann
>Gesendet: Donnerstag, 29. Juni 2006 21:20
>An: R-help at stat.math.ethz.ch
>Betreff: [R] Cointegration Test in R
>
>Hello!
>
>I'm using the blrtest() function in the urca package
>to test cointegration relationships.
>Unfortunately, the hypothesis (restrictions on beta) 
>specifies the same restriction on all cointegration vectors.
>Is there any possibility to specify different restrictions on
>the cointegration vectors?
>Are there any other packages in R using cointegration tests?
>
>Thanks and best regards.
>Dennis Heidemann
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html
>
*****************************************************************
Confidentiality Note: The information contained in this mess...{{dropped}}


From john.seers at bbsrc.ac.uk  Fri Jun 30 11:35:30 2006
From: john.seers at bbsrc.ac.uk (john seers (IFR))
Date: Fri, 30 Jun 2006 10:35:30 +0100
Subject: [R] R server
Message-ID: <1CF0B26CECD746438AE02DBF7DDE1C7B03055E9A@ifre2ksrv1.ifrxp.bbsrc.ac.uk>


There is the Rserve package which you can look at here:

http://stats.math.uni-augsburg.de/Rserve/down.shtml


JS


 
---


From i.m.s.white at ed.ac.uk  Fri Jun 30 11:48:23 2006
From: i.m.s.white at ed.ac.uk (i.m.s.white)
Date: Fri, 30 Jun 2006 10:48:23 +0100
Subject: [R] smoothing splines and degrees of freedom
In-Reply-To: <50509.151.201.54.128.1151163316.squirrel@webmail.pitt.edu>
References: <50509.151.201.54.128.1151163316.squirrel@webmail.pitt.edu>
Message-ID: <20060630094822.GB20328@trotter.cap.ed.ac.uk>

Steven,

I cannot vouch for the behaviour of the function smooth.spline(), but
the theoretical answer to your question is yes. If g = Sy is the transformation
from data vector y to spline vector g, the equivalent degrees of freedom are
usually defined as EDF = trace(S), where S is the n x n smoothing matrix:

EDF = sum_i(1/(1+theta*lambda_i)),

where lambda_1 to lambda_n are the eigenvalues of S. Two of these are zero, so

EDF = 2 + sum(1/(1+theta*lambda_i))

the sum now over i=3 to n. Here theta is the smoothing parameter. Setting
theta = 0 (no smoothing) gives EDF=n and produces the interpolating spline.
Setting theta = infty gives EDF=2 and a straight line fit. See either

Green and Silverman, Nonparametric regression and generalized linear models,
(p37), or
Hastie and Tibshirani, Generalized additive models, p52.

On Sat, Jun 24, 2006 at 11:35:16AM -0400, Steven Shechter wrote:
> Hi,
> If I set df=2 in my smooth.spline function, is that equivalent to running
> a linear regression through my data?  It appears that df=# of data points
> gives the interpolating spline and that df = 2 gives the linear
> regression, but I just want to confirm this.
> 
> Thank you,
> Steven
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
************************************************
*    I.White                                   *
*    University of Edinburgh                   *
*    Ashworth Laboratories, West Mains Road    *
*    Edinburgh EH9 3JT                         *
*    Fax: 0131 650 6564   Tel: 0131 650 5490   *
*    E-mail: i.m.s.white at ed.ac.uk              *


From roger.bos at gmail.com  Fri Jun 30 13:41:17 2006
From: roger.bos at gmail.com (roger bos)
Date: Fri, 30 Jun 2006 07:41:17 -0400
Subject: [R] R server
In-Reply-To: <1CF0B26CECD746438AE02DBF7DDE1C7B03055E9A@ifre2ksrv1.ifrxp.bbsrc.ac.uk>
References: <1CF0B26CECD746438AE02DBF7DDE1C7B03055E9A@ifre2ksrv1.ifrxp.bbsrc.ac.uk>
Message-ID: <1db726800606300441x5566129ao424229a4296501c1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060630/275ea7df/attachment.pl 

From hannesalazar at gmail.com  Fri Jun 30 13:45:16 2006
From: hannesalazar at gmail.com (yohannes alazar)
Date: Fri, 30 Jun 2006 12:45:16 +0100
Subject: [R] data extraction
Message-ID: <ae94396d0606300445w2fceaer691740b5e1b4b285@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060630/207574ab/attachment.pl 

From andza at osi.lv  Fri Jun 30 13:52:31 2006
From: andza at osi.lv (Andris Jankevics)
Date: Fri, 30 Jun 2006 14:52:31 +0300
Subject: [R] data extraction
In-Reply-To: <ae94396d0606300445w2fceaer691740b5e1b4b285@mail.gmail.com>
References: <ae94396d0606300445w2fceaer691740b5e1b4b285@mail.gmail.com>
Message-ID: <200606301452.32222.andza@osi.lv>

This should work:

data[seq(1,nrow(data),10),]



Andris

On Piektdiena, 30. J??nijs 2006 14:45, yohannes alazar wrote:
> Dear mailing list I have a data that have 20,000 rows and 20 columns. Io
> wonted to extract the 10th row only. Example the 10th, 20th, 30th
> 40th?..20000 th. can you please help me how do I do that.Than kyou.
> Example is below.
> Inpute:
> AG GG GG AG
> CC CC CC CC
> CT CC CT CT
> GG GG GG GG
> CC CC CC CC
> GG GG GG GG
> CC CC CC CC
> GG CG CG GG
> GG GG GG GG
> *CC CC CC CC*
> AA AG AG AA
> AA AA AA AA
> GG AG AG GG
> GG AG AG GG
> GG GG GG GG
> TT TT TT TT
> AA AG AG AA
> CT CC CT TT
> AG AA AG GG
> *AA AA AG AG*
> AA AA AA AA
> CC CC CC CG
> GG GG GG AG
> CT TT CT CT
> AT AA AT AT
> GG GG GG AG
> CG CC CG GG
> GG GG GG AG
> CC CC CC CT
> GG GG GG GG
> *GG GG AG AG
> *CC CC CC CT
> TT TT TT CT
> AG GG AG AG
> GG GG GG GG
> AG AG AA AA
> AG GG AG AA
> CT TT CT CT
> CT CT CC CT
> *AC CC AC AC*
>
> output
> *CC CC CC CC
> AA AA AG AG
> GG GG GG GG
> AC CC AC AC*
>
> thankyou a lot inadvance!
>
> 	[[alternative HTML version deleted]]


From ccleland at optonline.net  Fri Jun 30 13:52:52 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 30 Jun 2006 07:52:52 -0400
Subject: [R] data extraction
In-Reply-To: <ae94396d0606300445w2fceaer691740b5e1b4b285@mail.gmail.com>
References: <ae94396d0606300445w2fceaer691740b5e1b4b285@mail.gmail.com>
Message-ID: <44A51094.2000302@optonline.net>

mydf[seq(10,20000,10),]

yohannes alazar wrote:
> Dear mailing list I have a data that have 20,000 rows and 20 columns. Io
> wonted to extract the 10th row only. Example the 10th, 20th, 30th 40th?..20000
> th. can you please help me how do I do that.Than kyou.
> Example is below.
> Inpute:
> AG GG GG AG
> CC CC CC CC
> CT CC CT CT
> GG GG GG GG
> CC CC CC CC
> GG GG GG GG
> CC CC CC CC
> GG CG CG GG
> GG GG GG GG
> *CC CC CC CC*
> AA AG AG AA
> AA AA AA AA
> GG AG AG GG
> GG AG AG GG
> GG GG GG GG
> TT TT TT TT
> AA AG AG AA
> CT CC CT TT
> AG AA AG GG
> *AA AA AG AG*
> AA AA AA AA
> CC CC CC CG
> GG GG GG AG
> CT TT CT CT
> AT AA AT AT
> GG GG GG AG
> CG CC CG GG
> GG GG GG AG
> CC CC CC CT
> GG GG GG GG
> *GG GG AG AG
> *CC CC CC CT
> TT TT TT CT
> AG GG AG AG
> GG GG GG GG
> AG AG AA AA
> AG GG AG AA
> CT TT CT CT
> CT CT CC CT
> *AC CC AC AC*
> 
> output
> *CC CC CC CC
> AA AA AG AG
> GG GG GG GG
> AC CC AC AC*
> 
> thankyou a lot inadvance!
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From maj at stats.waikato.ac.nz  Fri Jun 30 13:54:00 2006
From: maj at stats.waikato.ac.nz (maj at stats.waikato.ac.nz)
Date: Fri, 30 Jun 2006 23:54:00 +1200 (NZST)
Subject: [R] Extracting R plots from MS Word
In-Reply-To: <971536df0606291724l2f066061v6a3a66e4c00f2d68@mail.gmail.com>
References: <62025.58.105.171.47.1151558108.squirrel@webmail.scms.waikato.ac.nz> 
	<971536df0606290322p1e1d7e09s88fa6649514b4896@mail.gmail.com> 
	<61039.58.105.171.47.1151626532.squirrel@webmail.scms.waikato.ac.nz>
	<971536df0606291724l2f066061v6a3a66e4c00f2d68@mail.gmail.com>
Message-ID: <63797.58.105.171.47.1151668440.squirrel@webmail.scms.waikato.ac.nz>

Just a note to say what I did. I think that the results were OK but I have
yet to hear from the journal.

1. I saved the Word document under another name.
2. I deleted all the contents of the document except the target graphic.
3. I printed to file yielding a .prn file.
4. I changed the extension to .ps.
5. I used Gsview PS to EPS.

Murray Jorgensen


From dimitris.rizopoulos at med.kuleuven.be  Fri Jun 30 14:00:56 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 30 Jun 2006 14:00:56 +0200
Subject: [R] data extraction
References: <ae94396d0606300445w2fceaer691740b5e1b4b285@mail.gmail.com>
Message-ID: <001c01c69c3c$d82d5c20$0540210a@www.domain>

you need something like:

new.data <- data[seq(10, nrow(data), 10), ]


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "yohannes alazar" <hannesalazar at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, June 30, 2006 1:45 PM
Subject: [R] data extraction


Dear mailing list I have a data that have 20,000 rows and 20 columns. 
Io
wonted to extract the 10th row only. Example the 10th, 20th, 30th 
40th...20000
th. can you please help me how do I do that.Than kyou.
Example is below.
Inpute:
AG GG GG AG
CC CC CC CC
CT CC CT CT
GG GG GG GG
CC CC CC CC
GG GG GG GG
CC CC CC CC
GG CG CG GG
GG GG GG GG
*CC CC CC CC*
AA AG AG AA
AA AA AA AA
GG AG AG GG
GG AG AG GG
GG GG GG GG
TT TT TT TT
AA AG AG AA
CT CC CT TT
AG AA AG GG
*AA AA AG AG*
AA AA AA AA
CC CC CC CG
GG GG GG AG
CT TT CT CT
AT AA AT AT
GG GG GG AG
CG CC CG GG
GG GG GG AG
CC CC CC CT
GG GG GG GG
*GG GG AG AG
*CC CC CC CT
TT TT TT CT
AG GG AG AG
GG GG GG GG
AG AG AA AA
AG GG AG AA
CT TT CT CT
CT CT CC CT
*AC CC AC AC*

output
*CC CC CC CC
AA AA AG AG
GG GG GG GG
AC CC AC AC*

thankyou a lot inadvance!

[[alternative HTML version deleted]]




--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From candrews at buffalo.edu  Fri Jun 30 14:21:47 2006
From: candrews at buffalo.edu (Chris Andrews)
Date: Fri, 30 Jun 2006 08:21:47 -0400
Subject: [R]  Biobass, SAGx, and Jonckheere-Terpstra test
Message-ID: <44A5175B.3040701@buffalo.edu>

Horace,
Last year I found another Jonckheere-Terpstra function at 
http://www.biostat.wustl.edu/archives/html/s-news/2000-10/msg00126.html
Using that and the version in SAGx and wanting a few other options, I 
created the version below.

Hope it is useful,
Chris
-- 
Christopher Andrews, PhD
SUNY Buffalo, Department of Biostatistics



#####
#####
#####
j.test <- function(x, y,
   alternative = c("two.sided", "decreasing", "increasing"),
   asymp=TRUE, correct=FALSE, perm=0, na.action=c("omit", "fail"),
   permgraph=FALSE, permreps=FALSE, ...) {

#Jonckheere-Terpstra test
#x = response
#y = group membership
#alternative hypothesis
#asymptotic = asymptotic formula for variance or don't bother
#correct = continuity correction
#perm = number of repetitions for a permutation test
#na.action = what to do with missing data
#permgraph = draw a histogram of the permutations?
#permreps = return the permutations?
#... for graph

   alternative <- match.arg(alternative)
   complete <- complete.cases(x,y)
   nn <- sum(complete)
   cat(nn, "complete cases.\n")
   if (!all(complete) && (na.action=="fail")) {
     cat("option na.action is 'fail' and",
       sum(!complete), "cases are not complete.\n")
     return(NULL)
   }
   response <- x[complete]
   groupvec <- y[complete, drop=TRUE]

   if(!is.ordered(groupvec)) {
     groupvec <- as.ordered(groupvec)
     cat("y was not an ordered factor.  Redefined to be one.\n")
   }

   lev <- levels(groupvec)
   nlev <- length(lev)
   if(nlev <= 2) {
     if (nlev < 2) {
       cat("Not enough groups (k =", nlev, ") for Jonckheere.\n")
       return(NULL)
     } else {
       cat("Two groups.  Could use wilcox.test.\n")
     }
   }

   computestat <- function(response, groupvec, n.groups) {
     pairwise <- function(x, y) {
       length(x)*(length(y) + (1+length(x))/2) - 
sum(rank(c(x,y))[seq(along=x)])
     }
     H<-0
     for (i in seq(n.groups-1))
       for (j in seq(i+1, n.groups))
         H <- H+pairwise(response[groupvec == lev[i]], response[groupvec 
== lev[j]])
     return(H)
   }

   retval <- list(statistic=computestat(response, groupvec, nlev),
     alternative = paste(alternative,
       paste(levels(groupvec),collapse=switch(alternative,
         two.sided = ", ", decreasing= " > ", increasing = " < ")), 
sep=": "),
     method = "Jonckheere",
     data.name = paste(deparse(substitute(x)), "by", 
deparse(substitute(y))))

   if (asymp) {
     ns <- tabulate(as.numeric(groupvec))
     retval$EH <- (nn * nn - sum(ns * ns))/4
     retval$VH <- if (!any(duplicated(response))) {
       (nn * nn * (2 * nn + 3) - sum(ns * ns * (2 * ns + 3)))/72
     } else {
       ds <- as.vector(table(response))
       ((nn*(nn-1)*(2*nn+5) - sum(ns*(ns-1)*(2*ns+5)) - 
sum(ds*(ds-1)*(2*ds+5)))/72 +
       sum(ns*(ns-1)*(ns-2))*sum(ds*(ds-1)*(ds-2))/(36*nn*(nn-1)*(nn-2)) +
       sum(ns*(ns-1))*sum(ds*(ds-1))/(8*nn*(nn - 1)))
     }
     pp <- if (!correct) {
       pnorm(retval$statistic, retval$EH, sqrt(retval$VH))
     } else if (retval$statistic >= retval$EH + 1) {
       pnorm(retval$statistic - 1, retval$EH, sqrt(retval$VH))
     } else if (retval$statistic <= retval$EH - 1) {
       pnorm(retval$statistic + 1, retval$EH, sqrt(retval$VH))
     } else {
       .5
     }
     retval$p.value <- switch(alternative,
       two.sided = 2*min(pp,1-pp), decreasing = pp, increasing = 1-pp)
   }
   if (perm>0) {
     reps <- numeric(perm)
     for (i in seq(perm)) {
       reps[i] <- computestat(response, sample(groupvec), nlev)
     }
     retval$EH.perm <- mean(reps)
     retval$VH.perm <- var(reps)
     ppp <- if (!correct) {
       pnorm(retval$statistic, retval$EH.perm, sqrt(retval$VH.perm))
     } else if (retval$statistic >= retval$EH.perm + 1) {
       pnorm(retval$statistic - 1, retval$EH.perm, sqrt(retval$VH.perm))
     } else if (retval$statistic <= retval$EH.perm - 1) {
       pnorm(retval$statistic + 1, retval$EH.perm, sqrt(retval$VH.perm))
     } else {
       .5
     }
     retval$p.value.perm <- switch(alternative,
       two.sided = 2*min(ppp,1-ppp), decreasing = ppp, increasing = 1-ppp)
     rr <- rank(c(retval$statistic, reps))[1]/(perm+2)
     retval$p.value.rank <- switch(alternative,
       two.sided = 2*min(rr,1-rr), decreasing = rr, increasing = 1-rr)
     if (permreps)
       retval$reps <- reps
     if (permgraph) {
       hist(reps, xlab="Jonckheere Statistic", prob=TRUE,
         main="Histogram of Permutations", sub=paste(perm, "permutations"))
       abline(v=retval$statistic, col="red")
       points((-3:3)*sqrt(retval$VH.perm)+retval$EH.perm, rep(0,7), 
col="blue",
         pch=as.character(c(3:1,0:3)))
     }
   }
   class(retval) <- "htest"
   names(retval$statistic) <- "J"
   return(retval)

#returns list (of class htest) with the following components
#Always:
#statistic = value of J
#alternative = same as input
#method = "Jonckheere"
#data.name = source of data based on call
#Most of the time (i.e., when asymp=TRUE):
#EH = expected value based on sample size
#VH = variance (adjusted for ties if necessary)
#p.value = asymptotic p value
#Sometimes (i.e., when perm>0):
#EH.perm = expected value based on permutations
#VH.perm = variance based on permutations
#p.value.perm = p value based on normal approximation to permutations
#p.value.rank = p value based on rank within permutation
#perms = vector of permutation.
}

j.test(rnorm(54), ordered(rep(letters[1:27],2)), na.action="fail")
j.test(rnorm(54), ordered(rep(letters[1:27],2)), na.action="omit", 
asymp=FALSE, perm=1000)
j.test(ttt <- rnorm(54), ordered(rep(letters[1:2],27)), alt="d")
wilcox.test(ttt ~ordered(rep(letters[1:2],27)), correct=FALSE, 
exact=FALSE, alt="g")

#Higgins p 178 Example
totals <- c(10,12,17,30,9,9,11,35,7,8,12,43)
tlevels <- factor(c("A", "B", "C"),
   levels=c("A", "B", "C"),
   labels=c("A", "B", "C"),
   ordered=TRUE)
Treatment <- rep(
   rep(tlevels, each=4)
   , times = totals)
rlevels <- factor(c("Severe", "Moderate", "Slight", "None"),
   levels=rev(c("Severe", "Moderate", "Slight", "None")),
   labels=rev(c("Severe", "Moderate", "Slight", "None")),
   ordered=TRUE)
Response <- rep(
   rep(rlevels, times=3)
   , times=totals)

result <- j.test(Response, Treatment, alt="t", perm=1000, 
permgraph=TRUE, correct=TRUE)
result
# note that the standard print does not report the permutation pvalues.
# But they are there
result$p.value.perm
result$p.value.rank


From e.rapsomaniki at mail.cryst.bbk.ac.uk  Fri Jun 30 14:26:28 2006
From: e.rapsomaniki at mail.cryst.bbk.ac.uk (e.rapsomaniki at mail.cryst.bbk.ac.uk)
Date: Fri, 30 Jun 2006 13:26:28 +0100
Subject: [R] Customizing breaks for histograms of unequal ranges
Message-ID: <1151670388.44a51874457a5@webmail.cryst.bbk.ac.uk>

Hi, 

I would really appreciate any suggestions on this (rather trivial) problem..

Say I have two vectors:
v1=seq(1:10)
v2=seq(1:15)

For a set of common breaks I need to divide the density of v2 over v1. This
means that I want to avoid having 0 counts for any v1 breakpoint. But
(unsuprisingly) if  I define my common breaks as those returned by calling hist
for v1:

v1.hist=hist(v1, plot=F)
v2.hist=hist(v2, plot=F, breaks=v1.hist$breaks)

I get an error:
Error in hist.default(v1, plot = F, breaks = v2.hist$breaks) : 
        some 'x' not counted; maybe 'breaks' do not span range of 'x'

If I had used the combined vector (c(v1,v2)) to set my breaks I would end up
with 0 for some v1 counts. So my question is: is there a way to define breaks
that cover the whole range of v1 and v2 while avoiding having 0 for the
shortest vector?

Many thanks
Eleni Rapsomaniki
Birkbeck College, UK


From dimitris.rizopoulos at med.kuleuven.be  Fri Jun 30 09:03:45 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 30 Jun 2006 09:03:45 +0200
Subject: [R] lme convergence
References: <002001c69ad3$38073050$7c94100a@win.ad.jhu.edu>
	<44A48790.9030002@pdf.com>
Message-ID: <009901c69c13$54616ae0$0540210a@www.domain>

I think the following part of lme.formula

if (numIter > controlvals$maxIter) {
    stop("Maximum number of iterations reached without convergence.")
}


should be something like


if (numIter > controlvals$maxIter) {
    if (controlvals$returnObject) {
        warning("Maximum number of iterations reached without 
convergence.")
        break
    } else {
        stop("Maximum number of iterations reached without 
convergence.")
    }
}


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Spencer Graves" <spencer.graves at pdf.com>
To: "Ravi Varadhan" <rvaradhan at jhmi.edu>
Cc: "'Pryseley Assam'" <assampryseley at yahoo.com>; "'R-Users'" 
<R-help at stat.math.ethz.ch>; "Douglas Bates" <bates at stat.wisc.edu>
Sent: Friday, June 30, 2006 4:08 AM
Subject: Re: [R] lme convergence


>   Does anyone know how to obtain the 'returnObject' from an 'lme' 
> run
> that fails to converge?  An argument of this name is described on 
> the
> 'lmeControl' help page as, "a logical value indicating whether the
> fitted object should be returned when the maximum number of 
> iterations
> is reached without convergence of the algorithm. Default is 
> 'FALSE'."
>
>   Unfortunately, I've so far been unable to get it to work, as
> witnessed by the following modification of an example from the 
> '?lme'
> help page:
>
> > library(nlme)
> > fm1 <- lme(distance ~ age, data = Orthodont,
> +            control=lmeControl(msMaxIter=1))
> Error in lme.formula(distance ~ age, data = Orthodont, control =
> lmeControl(msMaxIter = 1)) :
> iteration limit reached without convergence (9)
> > fm1
> Error: object "fm1" not found
> > fm1 <- lme(distance ~ age, data = Orthodont,
> +            control=lmeControl(msMaxIter=1,
> +              returnObject=TRUE))
> Error in lme.formula(distance ~ age, data = Orthodont, control =
> lmeControl(msMaxIter = 1,  :
> iteration limit reached without convergence (9)
> > fm1
> Error: object "fm1" not found
>
>   I might be able to fix the problem myself, working through the 
> 'lme'
> code line by line, e.g., using 'debug'.  However, I'm not ready to 
> do
> that just now.
>
>   Best Wishes,
>   Spencer Graves
>
> Ravi Varadhan wrote:
>> Use "try" to capture error messages without breaking the loop.
>> ?try
>>
>> --------------------------------------------------------------------------
>> Ravi Varadhan, Ph.D.
>> Assistant Professor,  The Center on Aging and Health
>> Division of Geriatric Medicine and Gerontology
>> Johns Hopkins University
>> Ph: (410) 502-2619
>> Fax: (410) 614-9625
>> Email:  rvaradhan at jhmi.edu
>> Webpage: 
>> http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html
>> --------------------------------------------------------------------------
>>
>>> -----Original Message-----
>>> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
>>> bounces at stat.math.ethz.ch] On Behalf Of Pryseley Assam
>>> Sent: Wednesday, June 28, 2006 12:18 PM
>>> To: R-Users
>>> Subject: [R] lme convergence
>>>
>>> Dear R-Users,
>>>
>>>   Is it possible to get the covariance matrix from an lme model 
>>> that did
>>> not converge ?
>>>
>>>   I am doing a simulation which entails fitting linear mixed 
>>> models, using
>>> a "for loop".
>>>   Within each loop, i generate a new data set and analyze it using 
>>> a mixed
>>> model.  The loop stops When the "lme function" does not converge 
>>> for a
>>> simulated dataset. I want to inquire if there is a method to 
>>> suppress the
>>> error message from the lme function, or better still, a way of 
>>> going about
>>> this issue of the loop ending once the lme function does not 
>>> converge.
>>>
>>>   Thanks in advance,
>>>   Pryseley
>>>
>>>
>>> ---------------------------------
>>>
>>>
>>> [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-
>>> guide.html
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From spencer.graves at pdf.com  Thu Jun 29 17:58:38 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 29 Jun 2006 08:58:38 -0700
Subject: [R] GARCH message 75
In-Reply-To: <449E0B98.8040304@utulsa.edu>
References: <449E0B98.8040304@utulsa.edu>
Message-ID: <44A3F8AE.3090102@pdf.com>

	  For those who "can find no documention on" the "@" operator, "?@" 
says it is used to "Extract tbe contents of a slot in a object with a 
formal class structure."  "?slot" provides more information, including 
some critical references.  The primary reference is Chambers (1998) 
Programming with Data (Springer).  This describes the "S 4" standard for 
object oriented programming in R.  Additional information can be found 
in a chapter of Venables and Ripley (2000) S Programming (Springer).

	  When I read Chambers a few years ago, I found it very difficult, 
partly because of the inconsistencies between what Chambers described 
and what was actually implemented in versions of S-Plus and to which I 
had access at the time.  That may have improved since then.

	  Hope this helps,
	  Spencer Graves

Joe Byers wrote:
> All,
> 
> You might check out the @ operator on Classes and objects.  I can find 
> no documentation on this but if you look at code in fseries or fbasic in 
> the method fARMA.summary.  You will find where the object fit in the 
> returned object is access using the @ operator.
> 
>        object <- x at fit
>        ans$coefmat <- cbind(format(object$coef,digits=digits), 
> format(object$se.coef,digits=digits),
>            format(tval,digits=digits), 
> prob=format.pval(prob,digits=digits))
> 
> where x is the from x<-armaFit(....).  This I believe would be the same 
> for the GARCH results.
> 
> Note the format.pval on the prob.  This is important because if you do 
> not do this you will get 0 for small pvalues.  I learned this by looking 
> at the code of the pretty printing functions so I can save only results 
> that I want from multiple models runs.  You also can then to wald test 
> and LL ratio tests on the models.
> 
> Good luck
> Joe
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From pramod.r.anugu at jsums.edu  Fri Jun 30 14:56:41 2006
From: pramod.r.anugu at jsums.edu (Pramod Anugu)
Date: Fri, 30 Jun 2006 07:56:41 -0500
Subject: [R] Running R
Message-ID: <000601c69c44$a26fe870$1321848f@E72517PA>

Does any one have solution for R not running? I am getting the error
message. Please Advice.
Thanks

?
 ? 1. gunzip  R-patched.tar.gz
   2. tar -xvf R-patched.tar
?? 3. changed the directory to the newly created directory R-patched
?? 4. Typed ./configure
? ?5. Typed make
?? 6. Make check
?? 7. make check-all
?? 8. Typed make install
?? *9. Typed R
# R
Fatal error: unable to open the base package


From gregr at rand.org  Thu Jun 29 22:50:20 2006
From: gregr at rand.org (Ridgeway, Greg)
Date: Thu, 29 Jun 2006 13:50:20 -0700
Subject: [R] [R-pkgs] twang - Toolkit for Weighting and Analysis
	of	Nonequivalent Groups
Message-ID: <461D139C576A2C49B0C4720EF89EC164016D2D33@smmail4.rand.org>


The Toolkit for Weighting and Analysis of Nonequivalent Groups (twang
1.0) has been released to CRAN. The package collects functions useful
for computing propensity score weights for treatment effect estimation,
developing nonresponse weights, and diagnosing the quality of those
weights. The package includes a vignette containing some basic theory
and walks through two examples. It is available by typing
vignette("twang")
at the R prompt after loading the library.

Background on the methodology with an application to evaluating drug
treatment programs is available in

McCaffrey, D., G. Ridgeway, A. Morral (2004). "Propensity score
estimation with boosted regression for evaluating adolescent substance
abuse treatment," Psychological Methods 9(4):403-425.


--------------------

This email message is for the sole use of the intended recip...{{dropped}}

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From antoine.lucas at laposte.net  Thu Jun 29 23:07:15 2006
From: antoine.lucas at laposte.net (Antoine)
Date: Thu, 29 Jun 2006 23:07:15 +0200
Subject: [R] [R-pkgs] gmp: bigintegers with matrix computation
Message-ID: <20060629230715.60733488@localhost.localdomain>

Dear all,

gmp package is now available on cran with version 0.4

Aim of gmp is to provide a lot a function to manipulate big integers, big rationals and last but not least big integers in Z/nZ (modulo n)

We add in last version support for matrix computation (standards operators, + * - / %*% ...) and inversion matrix (solve):
	in Z/nZ if matrix defined in Z/nZ
	with rationals (absolute precision) if not.

Best Regards,

	Antoine Lucas.

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From sell_mirage_ne at hotmail.com  Fri Jun 30 15:18:40 2006
From: sell_mirage_ne at hotmail.com (Taka Matzmoto)
Date: Fri, 30 Jun 2006 08:18:40 -0500
Subject: [R] incomplete final line found by readLines on ...
Message-ID: <BAY110-F12679687B95B5EFC29AAE8C77D0@phx.gbl>

Dear R-users

I need to read some text files produced by some other software.
I used readLines (with n = -1 ) command and then tried to find some numbers 
I liked to extract or some line numbers I like to know.

The problem is that there is no empty line at the end of the text files.

R gave me this warning below;

In addition: Warning message:
incomplete final line found by readLines on 'output.txt'

I know this doesn't affect anything and just warning messages. Is there any 
way to prevent this warning message.

Thanks

Taka


From priti.desai at kalyptorisk.com  Fri Jun 30 15:22:52 2006
From: priti.desai at kalyptorisk.com (priti desai)
Date: Fri, 30 Jun 2006 18:52:52 +0530
Subject: [R] Query :   Chi Square goodness of fit test
Message-ID: <2AB7346A3227A74BB97F9A0D79E3E65A01BFDB@mailserver.kalyptorisk.com>

I want to calculate chi square test of goodness of fit to test,
Sample coming from Poisson distribution.

please copy this script in R & run the script
The R script is as follows

########################## start
#########################################

No_of_Frouds<-
c(4,1,6,9,9,10,2,4,8,2,3,0,1,2,3,1,3,4,5,4,4,4,9,5,4,3,11,8,12,3,10,0,7)


N <- length(No_of_Frouds)

# Estimation of Parameter


lambda<- sum(No_of_Frouds)/N
lambda 

pmf  <- dpois(i, lambda, log = FALSE)

step_function  <- ppois(i, lambda, lower.tail = TRUE, log.p = FALSE)

# Chi-Squared Goodness of Fit Test

# Ho: The data follow a Poisson distribution Vs H1: Not Ho


Frauds <- c(1:13)

counts<-  c(2,3,3,5,7,2,1,1,2,3,2,1,1,0)  # Observed frequency

Expected
<-c(0.251005528,1.224602726,2.987288468,4.85811559,5.925428863,5.7817821
03,4.701348074,3.276697142,1.998288788,1.083247457,0.528493456,0.2344006
79,0.095299266,0.035764993)

chisq.test(counts, Expected, simulate.p.value =FALSE, correct = FALSE)



######################### end ########################################


The result of R is as follows

  Pearson's Chi-squared test

data:  counts and poisson_fit 
X-squared = 70, df = 65, p-value = 0.3135

Warning message:
Chi-squared approximation may be incorrect in: chisq.test(counts,
poisson_fit, simulate.p.value = FALSE, correct = FALSE)



But I have done calculations in Excel. I am getting different answer.

Observed  = 2,3,3,5,7,2,1,1,2,3,2,1,1,0
Expected=0.251005528,1.224602726,2.987288468,4.85811559,5.925428863,5.78
1782103,4.701348074,3.276697142,1.998288788,1.083247457,0.528493456,0.23
4400679,0.095299266,0.035764993


 Estimated Parameter  =4.878788

Chi square stat =  0.000113


My excel answer tally with the book which I have refer for excel.   
Please tell me the correct calculations in R.
########################################################################
######################

Awaiting your positive reply.

Regards.
Priti.


From pinard at iro.umontreal.ca  Fri Jun 30 15:34:15 2006
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Fri, 30 Jun 2006 09:34:15 -0400
Subject: [R] incomplete final line found by readLines on ...
In-Reply-To: <BAY110-F12679687B95B5EFC29AAE8C77D0@phx.gbl>
References: <BAY110-F12679687B95B5EFC29AAE8C77D0@phx.gbl>
Message-ID: <20060630133415.GA8301@phenix.progiciels-bpi.ca>

[Taka Matzmoto]

>Is there any way to prevent [this] warning message.

Hi, Taka.  The easiest might be using the suppressWarnings wrapper.
See ?suppressWarnings for more information.

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca


From andy_liaw at merck.com  Fri Jun 30 15:33:57 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 30 Jun 2006 09:33:57 -0400
Subject: [R] Running R
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA028845D4@usctmx1106.merck.com>

Have you checked and see if there is any error in steps 5 through 8?  If you
can't start R, I doubt steps 6 and 7 ran fine.

Andy 

From: Pramod Anugu 
> 
> Does any one have solution for R not running? I am getting 
> the error message. Please Advice.
> Thanks
> 
> ?
>  ? 1. gunzip  R-patched.tar.gz
>    2. tar -xvf R-patched.tar
> ?? 3. changed the directory to the newly created directory R-patched
> ?? 4. Typed ./configure
> ? ?5. Typed make
> ?? 6. Make check
> ?? 7. make check-all
> ?? 8. Typed make install
> ?? *9. Typed R
> # R
> Fatal error: unable to open the base package
> 
>


From edd at debian.org  Fri Jun 30 15:34:36 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 30 Jun 2006 08:34:36 -0500
Subject: [R] Running R
In-Reply-To: <000601c69c44$a26fe870$1321848f@E72517PA>
References: <000601c69c44$a26fe870$1321848f@E72517PA>
Message-ID: <20060630133436.GA8113@eddelbuettel.com>

On Fri, Jun 30, 2006 at 07:56:41AM -0500, Pramod Anugu wrote:
> Does any one have solution for R not running? I am getting the error
> message. Please Advice.

Yes, you have emailing the group fairly regularly about your troubles.

And you have been told just about each and every time to *check the output
of the configure step*.  Unless that step concludes with all relevant pieces
found, *there is no point in resuming with steps 5 to 9*.

As you are emailing from an educational institution, consider getting local
help about configuring and compiling software.  Building R is *extensively
documented in a dedicated manual* but the manual may need someone with more
familiarity with the process than you currently have.

Good luck, and consider to stop sending the *same* email to the list every
couple of days.

Dirk


> Thanks
> 
> ?
>  ? 1. gunzip  R-patched.tar.gz
>    2. tar -xvf R-patched.tar
> ?? 3. changed the directory to the newly created directory R-patched
> ?? 4. Typed ./configure
> ? ?5. Typed make
> ?? 6. Make check
> ?? 7. make check-all
> ?? 8. Typed make install
> ?? *9. Typed R
> # R
> Fatal error: unable to open the base package
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From longeregnol at yahoo.com.cn  Fri Jun 30 07:01:01 2006
From: longeregnol at yahoo.com.cn (Long Qu)
Date: Fri, 30 Jun 2006 13:01:01 +0800 (CST)
Subject: [R] Random numbers from noncentral t-distribution
Message-ID: <20060630050101.53331.qmail@web15103.mail.cnb.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060630/a073ab34/attachment.pl 

From jacques.veslot at good.ibl.fr  Fri Jun 30 16:01:32 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Fri, 30 Jun 2006 16:01:32 +0200
Subject: [R] Query :   Chi Square goodness of fit test
In-Reply-To: <2AB7346A3227A74BB97F9A0D79E3E65A01BFDB@mailserver.kalyptorisk.com>
References: <2AB7346A3227A74BB97F9A0D79E3E65A01BFDB@mailserver.kalyptorisk.com>
Message-ID: <44A52EBC.2010606@good.ibl.fr>

 > chisq.test(counts, p=Expected/sum(Expected), simulate.p.value =FALSE, correct = FALSE)

         Chi-squared test for given probabilities

data:  counts
X-squared = 40.5207, df = 13, p-value = 0.0001139

Warning message:
l'approximation du Chi-2 est peut-?tre incorrecte in: chisq.test(counts, p = Expected/sum(Expected), 
simulate.p.value = FALSE,

but the use of Chi2 test is incorrect since some of Expected frequencies are lower than 5.

-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


priti desai a ?crit :
> I want to calculate chi square test of goodness of fit to test,
> Sample coming from Poisson distribution.
> 
> please copy this script in R & run the script
> The R script is as follows
> 
> ########################## start
> #########################################
> 
> No_of_Frouds<-
> c(4,1,6,9,9,10,2,4,8,2,3,0,1,2,3,1,3,4,5,4,4,4,9,5,4,3,11,8,12,3,10,0,7)
> 
> 
> N <- length(No_of_Frouds)
> 
> # Estimation of Parameter
> 
> 
> lambda<- sum(No_of_Frouds)/N
> lambda 
> 
> pmf  <- dpois(i, lambda, log = FALSE)
> 
> step_function  <- ppois(i, lambda, lower.tail = TRUE, log.p = FALSE)
> 
> # Chi-Squared Goodness of Fit Test
> 
> # Ho: The data follow a Poisson distribution Vs H1: Not Ho
> 
> 
> Frauds <- c(1:13)
> 
> counts<-  c(2,3,3,5,7,2,1,1,2,3,2,1,1,0)  # Observed frequency
> 
> Expected
> <-c(0.251005528,1.224602726,2.987288468,4.85811559,5.925428863,5.7817821
> 03,4.701348074,3.276697142,1.998288788,1.083247457,0.528493456,0.2344006
> 79,0.095299266,0.035764993)
> 
> chisq.test(counts, Expected, simulate.p.value =FALSE, correct = FALSE)
> 
> 
> 
> ######################### end ########################################
> 
> 
> The result of R is as follows
> 
>   Pearson's Chi-squared test
> 
> data:  counts and poisson_fit 
> X-squared = 70, df = 65, p-value = 0.3135
> 
> Warning message:
> Chi-squared approximation may be incorrect in: chisq.test(counts,
> poisson_fit, simulate.p.value = FALSE, correct = FALSE)
> 
> 
> 
> But I have done calculations in Excel. I am getting different answer.
> 
> Observed  = 2,3,3,5,7,2,1,1,2,3,2,1,1,0
> Expected=0.251005528,1.224602726,2.987288468,4.85811559,5.925428863,5.78
> 1782103,4.701348074,3.276697142,1.998288788,1.083247457,0.528493456,0.23
> 4400679,0.095299266,0.035764993
> 
> 
>  Estimated Parameter  =4.878788
> 
> Chi square stat =  0.000113
> 
> 
> My excel answer tally with the book which I have refer for excel.   
> Please tell me the correct calculations in R.
> ########################################################################
> ######################
> 
> Awaiting your positive reply.
> 
> Regards.
> Priti.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From wade.wall at gmail.com  Fri Jun 30 16:11:15 2006
From: wade.wall at gmail.com (Wade Wall)
Date: Fri, 30 Jun 2006 10:11:15 -0400
Subject: [R] Way to convert data frame to matrix
Message-ID: <e23082be0606300711w79a45c58r4b09c9c37acf403a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060630/3145bbf3/attachment.pl 

From msears at zoology.siu.edu  Fri Jun 30 11:18:25 2006
From: msears at zoology.siu.edu (Mike Sears)
Date: Fri, 30 Jun 2006 04:18:25 -0500
Subject: [R] Way to convert data frame to matrix
In-Reply-To: <e23082be0606300711w79a45c58r4b09c9c37acf403a@mail.gmail.com>
References: <e23082be0606300711w79a45c58r4b09c9c37acf403a@mail.gmail.com>
Message-ID: <200606300418.26090.msears@zoology.siu.edu>

?as.matrix

On Friday 30 June 2006 09:11, Wade Wall wrote:
>   I have a text file that I have imported into R.  It contains 3 columns
> and 316940 rows.  The first column is vegetation plot ID, the second
> species names and the third is a cover value (numeric).  I imported using
> the read.table function.
>
> My problem is this.  I need to reformat the information as a matrix, with
> the first column becoming the row labels and the second the column labels
> and the cover values as the matrix cell data.  However, since the
> read.tablefunction imported the data as an indexed data frame, I can't
> use the columns
> as vectors.  Is there a way around this, to convert the data frame as 3
> separate vectors?  I have been looking all over for a function, and my
> programming skills are not great.
>
> Thanks in advance
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Michael W. Sears, Ph.D.
Associate Editor, Herpetologica
Assistant Professor
Center for Ecology
Department of Zoology
Southern Illinois University
Carbondale, IL 62901

phone: 618-453-4137
web:	http://equinox.unr.edu/homepage/msears
	http://www.ecology.siu.edu

"Natural selection is a mechanism for generating an exceedingly 
high degree of improbability"  Sir Ronald A. Fisher (1890-1962)


From murdoch at stats.uwo.ca  Fri Jun 30 16:21:56 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 30 Jun 2006 10:21:56 -0400
Subject: [R] Way to convert data frame to matrix
In-Reply-To: <e23082be0606300711w79a45c58r4b09c9c37acf403a@mail.gmail.com>
References: <e23082be0606300711w79a45c58r4b09c9c37acf403a@mail.gmail.com>
Message-ID: <44A53384.20102@stats.uwo.ca>

On 6/30/2006 10:11 AM, Wade Wall wrote:
>   I have a text file that I have imported into R.  It contains 3 columns and
> 316940 rows.  The first column is vegetation plot ID, the second species
> names and the third is a cover value (numeric).  I imported using the
> read.table function.
> 
> My problem is this.  I need to reformat the information as a matrix, with
> the first column becoming the row labels and the second the column labels
> and the cover values as the matrix cell data.  However, since the
> read.tablefunction imported the data as an indexed data frame, I can't
> use the columns
> as vectors.  Is there a way around this, to convert the data frame as 3
> separate vectors?  I have been looking all over for a function, and my
> programming skills are not great.

Internally, dataframes are just lists with a class="dataframe" 
attribute.  This means you can extract the columns as if they were just 
lists.

So if your columns are named A, B, and C, and the dataframe is dataf, 
you get them as vectors using

dataf$A, dataf$B, and dataf$C

Duncan Murdoch


From philipp.pagel.lists at t-online.de  Fri Jun 30 16:32:43 2006
From: philipp.pagel.lists at t-online.de (Philipp Pagel)
Date: Fri, 30 Jun 2006 16:32:43 +0200
Subject: [R] Way to convert data frame to matrix
In-Reply-To: <e23082be0606300711w79a45c58r4b09c9c37acf403a@mail.gmail.com>
References: <e23082be0606300711w79a45c58r4b09c9c37acf403a@mail.gmail.com>
Message-ID: <20060630143243.GA13415@gsf.de>

On Fri, Jun 30, 2006 at 10:11:15AM -0400, Wade Wall wrote:
>   I have a text file that I have imported into R.  It contains 3 columns and
> 316940 rows.  The first column is vegetation plot ID, the second species
> names and the third is a cover value (numeric).  I imported using the
> read.table function.
> 
> My problem is this.  I need to reformat the information as a matrix, with
> the first column becoming the row labels and the second the column labels
> and the cover values as the matrix cell data.

I'm not 100% sure but I think you are looking for reshape().

cu
	Philipp

-- 
Dr. Philipp Pagel                            Tel.  +49-8161-71 2131
Dept. of Genome Oriented Bioinformatics      Fax.  +49-8161-71 2186
Technical University of Munich
Science Center Weihenstephan
85350 Freising, Germany

 and

Institute for Bioinformatics / MIPS          Tel.  +49-89-3187 3675
GSF - National Research Center               Fax.  +49-89-3187 3585
      for Environment and Health
Ingolst?dter Landstrasse 1
85764 Neuherberg, Germany
http://mips.gsf.de/staff/pagel


From sarah.goslee at gmail.com  Fri Jun 30 16:33:21 2006
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Fri, 30 Jun 2006 10:33:21 -0400
Subject: [R] Way to convert data frame to matrix
In-Reply-To: <e23082be0606300711w79a45c58r4b09c9c37acf403a@mail.gmail.com>
References: <e23082be0606300711w79a45c58r4b09c9c37acf403a@mail.gmail.com>
Message-ID: <efb536d50606300733h2a4515e7nba6c29f2d460f469@mail.gmail.com>

Hi Wade,

On 6/30/06, Wade Wall <wade.wall at gmail.com> wrote:
>   I have a text file that I have imported into R.  It contains 3 columns and
> 316940 rows.  The first column is vegetation plot ID, the second species
> names and the third is a cover value (numeric).  I imported using the
> read.table function.
>
> My problem is this.  I need to reformat the information as a matrix, with
> the first column becoming the row labels and the second the column labels
> and the cover values as the matrix cell data.

Try crosstab(mydata$plotID, mydata$species, mydata$cover) from the
ecodist package.

Sarah
-- 
Sarah Goslee


From christian.bieli at unibas.ch  Fri Jun 30 16:49:45 2006
From: christian.bieli at unibas.ch (Christian Bieli)
Date: Fri, 30 Jun 2006 16:49:45 +0200
Subject: [R] Passing arguments to glm()
Message-ID: <44A53A09.7060302@unibas.ch>

Hi there

I want to pass arguments (i.e. the response variable and the subset 
argument) in a self-made function to glm.
Here is one way I can do this:

f.myglm <- function(y,subfact,subval) {
  
glm(d.mydata[,y]~d.mydata[,'x1'],family=binomial,subset=d.mydata[,subfact]==subval)
}

 > str(d.mydata)
`data.frame':    15806 obs. of  3 variables:
 $ y : Factor w/ 2 levels "no","yes": 1 1 1 1 1 1 1 NA 1 1 ...
 $ x1: Factor w/ 2 levels "no","yes": 2 2 1 2 2 2 2 2 2 2 ...
 $ x2: Factor w/ 2 levels "no","yes": 1 1 1 1 1 2 2 1 2 2 ...

 > f.myglm('y','x2','yes')

But is there a way I can pass the arguments and use the data argument of 
glm()?
In a naive way of thinking I'd like to something like this:
f.myglm <- function(y,sub) {
  glm(y~x1,family=binomial,data=d.mydata,subset=sub)
}
 > f.myglm(y=y,sub=x2=='yes')

I know that's not possible, because the objects y and x2 are not defined 
in the user workspace.
So, something like passing the arguments as an expression and evaluate 
it in the glm function should work, but I didn't manage to do it.

I'd appreciate your advice.
Christian

 > R.version
         _             
platform i386-pc-mingw32
arch     i386          
os       mingw32       
system   i386, mingw32 
status                 
major    2             
minor    2.1           
year     2005          
month    12            
day      20            
svn rev  36812         
language R


From sell_mirage_ne at hotmail.com  Fri Jun 30 16:50:51 2006
From: sell_mirage_ne at hotmail.com (Taka Matzmoto)
Date: Fri, 30 Jun 2006 09:50:51 -0500
Subject: [R] apply a function to several lists' components
Message-ID: <BAY110-F399F256B5E00047F153648C77D0@phx.gbl>

Dear R-user
I have 100 lists.
Each list has several components.
For example,

>data1
$a
[1] 1 2

$b
[1] 3 4

$c
[1] 5

There are data1, data2,...., data100. All lists have the same number and the 
same name of components.


Is there any function I can use for applying to only a specific component 
across 100 lists?
(e.g.,  taking mean of $c acorss 100 lists) or do I need to write my own 
function for that?

Thank you.

Taka,


From jacques.veslot at good.ibl.fr  Fri Jun 30 17:07:31 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Fri, 30 Jun 2006 17:07:31 +0200
Subject: [R] Passing arguments to glm()
In-Reply-To: <44A53A09.7060302@unibas.ch>
References: <44A53A09.7060302@unibas.ch>
Message-ID: <44A53E33.4000804@good.ibl.fr>

 > f.myglm <- function(y=y, subset="x2 == 'yes'", data=d.d.mydata) eval(parse(text="glm(", 
deparse(substitute(y)), "~ x1, family=binomial, data=", deparse(substitute(data)), ", subset =, 
subset, ")"))
 > f.myglm()

-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


Christian Bieli a ?crit :
> Hi there
> 
> I want to pass arguments (i.e. the response variable and the subset 
> argument) in a self-made function to glm.
> Here is one way I can do this:
> 
> f.myglm <- function(y,subfact,subval) {
>   
> glm(d.mydata[,y]~d.mydata[,'x1'],family=binomial,subset=d.mydata[,subfact]==subval)
> }
> 
>  > str(d.mydata)
> `data.frame':    15806 obs. of  3 variables:
>  $ y : Factor w/ 2 levels "no","yes": 1 1 1 1 1 1 1 NA 1 1 ...
>  $ x1: Factor w/ 2 levels "no","yes": 2 2 1 2 2 2 2 2 2 2 ...
>  $ x2: Factor w/ 2 levels "no","yes": 1 1 1 1 1 2 2 1 2 2 ...
> 
>  > f.myglm('y','x2','yes')
> 
> But is there a way I can pass the arguments and use the data argument of 
> glm()?
> In a naive way of thinking I'd like to something like this:
> f.myglm <- function(y,sub) {
>   glm(y~x1,family=binomial,data=d.mydata,subset=sub)
> }
>  > f.myglm(y=y,sub=x2=='yes')
> 
> I know that's not possible, because the objects y and x2 are not defined 
> in the user workspace.
> So, something like passing the arguments as an expression and evaluate 
> it in the glm function should work, but I didn't manage to do it.
> 
> I'd appreciate your advice.
> Christian
> 
>  > R.version
>          _             
> platform i386-pc-mingw32
> arch     i386          
> os       mingw32       
> system   i386, mingw32 
> status                 
> major    2             
> minor    2.1           
> year     2005          
> month    12            
> day      20            
> svn rev  36812         
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From jacques.veslot at good.ibl.fr  Fri Jun 30 17:17:52 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Fri, 30 Jun 2006 17:17:52 +0200
Subject: [R] Passing arguments to glm()
In-Reply-To: <44A53E33.4000804@good.ibl.fr>
References: <44A53A09.7060302@unibas.ch> <44A53E33.4000804@good.ibl.fr>
Message-ID: <44A540A0.20009@good.ibl.fr>

i forgot a paste():
f.myglm <- function(y=y, subset="x2 == 'yes'", data=d.d.mydata) eval(parse(text=paste("glm(",
deparse(substitute(y)), "~ x1, family=binomial, data=", deparse(substitute(data)), ", subset =,
subset, ")", sep="")))
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------

Jacques VESLOT a ?crit :
>  > f.myglm <- function(y=y, subset="x2 == 'yes'", data=d.d.mydata) eval(parse(text="glm(", 
> deparse(substitute(y)), "~ x1, family=binomial, data=", deparse(substitute(data)), ", subset =, 
> subset, ")"))
>  > f.myglm()
> 
> -------------------------------------------------------------------
> Jacques VESLOT
> 
> CNRS UMR 8090
> I.B.L (2?me ?tage)
> 1 rue du Professeur Calmette
> B.P. 245
> 59019 Lille Cedex
> 
> Tel : 33 (0)3.20.87.10.44
> Fax : 33 (0)3.20.87.10.31
> 
> http://www-good.ibl.fr
> -------------------------------------------------------------------
> 
> 
> Christian Bieli a ?crit :
> 
>>Hi there
>>
>>I want to pass arguments (i.e. the response variable and the subset 
>>argument) in a self-made function to glm.
>>Here is one way I can do this:
>>
>>f.myglm <- function(y,subfact,subval) {
>>  
>>glm(d.mydata[,y]~d.mydata[,'x1'],family=binomial,subset=d.mydata[,subfact]==subval)
>>}
>>
>> > str(d.mydata)
>>`data.frame':    15806 obs. of  3 variables:
>> $ y : Factor w/ 2 levels "no","yes": 1 1 1 1 1 1 1 NA 1 1 ...
>> $ x1: Factor w/ 2 levels "no","yes": 2 2 1 2 2 2 2 2 2 2 ...
>> $ x2: Factor w/ 2 levels "no","yes": 1 1 1 1 1 2 2 1 2 2 ...
>>
>> > f.myglm('y','x2','yes')
>>
>>But is there a way I can pass the arguments and use the data argument of 
>>glm()?
>>In a naive way of thinking I'd like to something like this:
>>f.myglm <- function(y,sub) {
>>  glm(y~x1,family=binomial,data=d.mydata,subset=sub)
>>}
>> > f.myglm(y=y,sub=x2=='yes')
>>
>>I know that's not possible, because the objects y and x2 are not defined 
>>in the user workspace.
>>So, something like passing the arguments as an expression and evaluate 
>>it in the glm function should work, but I didn't manage to do it.
>>
>>I'd appreciate your advice.
>>Christian
>>
>> > R.version
>>         _             
>>platform i386-pc-mingw32
>>arch     i386          
>>os       mingw32       
>>system   i386, mingw32 
>>status                 
>>major    2             
>>minor    2.1           
>>year     2005          
>>month    12            
>>day      20            
>>svn rev  36812         
>>language R
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From hydinghua at gmail.com  Fri Jun 30 17:30:28 2006
From: hydinghua at gmail.com (Philip He)
Date: Fri, 30 Jun 2006 10:30:28 -0500
Subject: [R] median of gamma distribution
Message-ID: <8727b8b60606300830o7e67e308w161319e7580a518f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060630/64e818b6/attachment.pl 

From p.dalgaard at biostat.ku.dk  Fri Jun 30 17:47:36 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Jun 2006 17:47:36 +0200
Subject: [R] median of gamma distribution
In-Reply-To: <8727b8b60606300830o7e67e308w161319e7580a518f@mail.gmail.com>
References: <8727b8b60606300830o7e67e308w161319e7580a518f@mail.gmail.com>
Message-ID: <x2y7veabmv.fsf@turmalin.kubism.ku.dk>

"Philip He" <hydinghua at gmail.com> writes:

> Doese anyone know a R function to find the median of a gamma distribution?

qgamma(0.5, ....)

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From tlumley at u.washington.edu  Fri Jun 30 17:49:25 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 30 Jun 2006 08:49:25 -0700 (PDT)
Subject: [R] median of gamma distribution
In-Reply-To: <8727b8b60606300830o7e67e308w161319e7580a518f@mail.gmail.com>
References: <8727b8b60606300830o7e67e308w161319e7580a518f@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0606300847540.18063@homer22.u.washington.edu>

On Fri, 30 Jun 2006, Philip He wrote:

> Doese anyone know a R function to find the median of a gamma distribution?
>

It's not clear what you mean.  If you know the parameters of a gamma 
distribution then qgamma() will give you any quantile.

If you have data and want to estimate the median then it's hard to beat 
median(), but you could use mle() to estimate the parameters and then 
qgamma().

 	-thomas


From markleeds at verizon.net  Fri Jun 30 17:51:12 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Fri, 30 Jun 2006 10:51:12 -0500 (CDT)
Subject: [R] median of gamma distribution
Message-ID: <26579627.430441151682672296.JavaMail.root@vms062.mailsrvcs.net>

>From: Philip He <hydinghua at gmail.com>
>Date: Fri Jun 30 10:30:28 CDT 2006
>To: R help list <r-help at stat.math.ethz.ch>
>Subject: [R] median of gamma distribution


someone might know a  more elegant way but one way 
is to use the distribution ( i think it's dnorm for
the normal but i get confused because ther
is also pnorm, rnorm and one other but you can look that up to
figure out which one is the one you need )
functions in R and just put in .50 as the probabiliy
input and itr will kick back the result which is the median.

                                  mark

                                       


 

>Doese anyone know a R function to find the median of a gamma distribution?
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From Ted.Harding at nessie.mcc.ac.uk  Fri Jun 30 17:53:19 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 30 Jun 2006 16:53:19 +0100 (BST)
Subject: [R] median of gamma distribution
In-Reply-To: <8727b8b60606300830o7e67e308w161319e7580a518f@mail.gmail.com>
Message-ID: <XFMail.060630165319.Ted.Harding@nessie.mcc.ac.uk>

On 30-Jun-06 Philip He wrote:
> Doese anyone know a R function to find the median of a gamma
> distribution?

qgamma will do it. Test:

> -log(0.5)
[1] 0.6931472
> qgamma(0.5,1)
[1] 0.6931472

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 30-Jun-06                                       Time: 16:53:16
------------------------------ XFMail ------------------------------


From tlumley at u.washington.edu  Fri Jun 30 18:00:45 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 30 Jun 2006 09:00:45 -0700 (PDT)
Subject: [R] Random numbers from noncentral t-distribution
In-Reply-To: <20060630050101.53331.qmail@web15103.mail.cnb.yahoo.com>
References: <20060630050101.53331.qmail@web15103.mail.cnb.yahoo.com>
Message-ID: <Pine.LNX.4.64.0606300858130.18063@homer22.u.washington.edu>

On Fri, 30 Jun 2006, Long Qu wrote:

> Hi there:
>
>  I'd thought these two versions of noncentral t-distribution are essentially the same:
>  > qqplot(rt(1000,df=20,ncp=3),qt(runif(1000),df=20,ncp=3))
>
>  But, the scales of the x-axis and the y-axis are quite different according to the QQ-plot.
>
>  Did I make any mistakes somewhere?
>

No, I think we did.

We have
> rt
function (n, df, ncp = 0)
{
     if (ncp == 0)
         .Internal(rt(n, df))
     else rnorm(n, ncp)/(rchisq(n, df)/sqrt(df))
}

and the rchisq() in the denominator should be inside the sqrt().


 	-thomas


From sw283 at maths.bath.ac.uk  Fri Jun 30 18:07:40 2006
From: sw283 at maths.bath.ac.uk (Simon Wood)
Date: Fri, 30 Jun 2006 17:07:40 +0100 (BST)
Subject: [R] getting the smoother matrix from smooth.spline
In-Reply-To: <20060624184137.81438.qmail@web31201.mail.mud.yahoo.com>
References: <20060624184137.81438.qmail@web31201.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0606301707010.9510@osiris.maths.bath.ac.uk>

smooth.matrix = function(x, df){
  n = length(x);
  A = matrix(0, n, n);
  for(i in 1:n){
        y = rep(0, n); y[i]=1;
        yi = predict(smooth.spline(x, y, df=df),x)$y;
        A[,i]= yi;
}
  (A+t(A))/2;
}


>- Simon Wood, Mathematical Sciences, University of Bath, Bath BA2 7AY
>-             +44 (0)1225 386603         www.maths.bath.ac.uk/~sw283/


On Sat, 24 Jun 2006, Gregory Gentlemen wrote:

> Can anyone tell me the trick for obtaining the smoother matrix from smooth.spline when there are non-unique values for x. I have the following code but, of course, it only works when all values of x are unique.
>
>  ## get the smoother matrix (x having unique values
> smooth.matrix = function(x, df){
> n = length(x);
> A = matrix(0, n, n);
> for(i in 1:n){
>       y = rep(0, n); y[i]=1;
>       yi = smooth.spline(x, y, df=df)$y;
>       A[,i]= yi;
> }
> (A+t(A))/2;
> }
>
>
>  Thanks for any assistance,
>  Gregory
>
>
> ---------------------------------
>
> ---------------------------------
> Get a sneak peak at messages with a handy reading pane.
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Fri Jun 30 18:14:34 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 30 Jun 2006 12:14:34 -0400
Subject: [R] getting the smoother matrix from smooth.spline
In-Reply-To: <Pine.LNX.4.64.0606301707010.9510@osiris.maths.bath.ac.uk>
References: <20060624184137.81438.qmail@web31201.mail.mud.yahoo.com>
	<Pine.LNX.4.64.0606301707010.9510@osiris.maths.bath.ac.uk>
Message-ID: <971536df0606300914n6510c24cm3df3d38ffe574ced@mail.gmail.com>

Perhaps this could be developed into a spline smooth method
for model.matrix and included in R.

On 6/30/06, Simon Wood <sw283 at maths.bath.ac.uk> wrote:
> smooth.matrix = function(x, df){
>  n = length(x);
>  A = matrix(0, n, n);
>  for(i in 1:n){
>        y = rep(0, n); y[i]=1;
>        yi = predict(smooth.spline(x, y, df=df),x)$y;
>        A[,i]= yi;
> }
>  (A+t(A))/2;
> }
>
>
> >- Simon Wood, Mathematical Sciences, University of Bath, Bath BA2 7AY
> >-             +44 (0)1225 386603         www.maths.bath.ac.uk/~sw283/
>
>
> On Sat, 24 Jun 2006, Gregory Gentlemen wrote:
>
> > Can anyone tell me the trick for obtaining the smoother matrix from smooth.spline when there are non-unique values for x. I have the following code but, of course, it only works when all values of x are unique.
> >
> >  ## get the smoother matrix (x having unique values
> > smooth.matrix = function(x, df){
> > n = length(x);
> > A = matrix(0, n, n);
> > for(i in 1:n){
> >       y = rep(0, n); y[i]=1;
> >       yi = smooth.spline(x, y, df=df)$y;
> >       A[,i]= yi;
> > }
> > (A+t(A))/2;
> > }
> >
> >
> >  Thanks for any assistance,
> >  Gregory
> >
> >
> > ---------------------------------
> >
> > ---------------------------------
> > Get a sneak peak at messages with a handy reading pane.
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Fri Jun 30 18:27:18 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 30 Jun 2006 12:27:18 -0400
Subject: [R] Passing arguments to glm()
In-Reply-To: <44A53A09.7060302@unibas.ch>
References: <44A53A09.7060302@unibas.ch>
Message-ID: <971536df0606300927k413a98edi9f4371b17ff84536@mail.gmail.com>

This is the same as glm except that
- formula may be of class character in which case its
  regarded as the name of the response variable and
  the formula defaults to resp ~ Species for that response
- the data frame defaults to iris
- modify as appropriate for your case

myglm <- function (formula, family = gaussian, data,
    weights, subset, na.action, start = NULL, etastart, mustart, offset,
    control = glm.control(...), model = TRUE, method = "glm.fit",
    x = FALSE, y = TRUE, contrasts = NULL, ...) {
    cl <- match.call()
    cl[[1]] <- as.name("glm")
    if (is.character(formula)) {
       fo <- . ~ Species  ### default formula
       fo[[2]] <- as.name(formula)
       cl$formula <- fo
    }
    if (missing(data)) cl$data <- as.name("iris") # default data frame
    eval(cl, parent.frame())
}
# test
myglm("Sepal.Length", subset = Petal.Length > mean(Petal.Length))
myglm(Sepal.Length ~ Petal.Length)
myglm("Sepal.Length", subset = 1:100)


On 6/30/06, Christian Bieli <christian.bieli at unibas.ch> wrote:
> Hi there
>
> I want to pass arguments (i.e. the response variable and the subset
> argument) in a self-made function to glm.
> Here is one way I can do this:
>
> f.myglm <- function(y,subfact,subval) {
>
> glm(d.mydata[,y]~d.mydata[,'x1'],family=binomial,subset=d.mydata[,subfact]==subval)
> }
>
>  > str(d.mydata)
> `data.frame':    15806 obs. of  3 variables:
>  $ y : Factor w/ 2 levels "no","yes": 1 1 1 1 1 1 1 NA 1 1 ...
>  $ x1: Factor w/ 2 levels "no","yes": 2 2 1 2 2 2 2 2 2 2 ...
>  $ x2: Factor w/ 2 levels "no","yes": 1 1 1 1 1 2 2 1 2 2 ...
>
>  > f.myglm('y','x2','yes')
>
> But is there a way I can pass the arguments and use the data argument of
> glm()?
> In a naive way of thinking I'd like to something like this:
> f.myglm <- function(y,sub) {
>  glm(y~x1,family=binomial,data=d.mydata,subset=sub)
> }
>  > f.myglm(y=y,sub=x2=='yes')
>
> I know that's not possible, because the objects y and x2 are not defined
> in the user workspace.
> So, something like passing the arguments as an expression and evaluate
> it in the glm function should work, but I didn't manage to do it.
>
> I'd appreciate your advice.
> Christian
>
>  > R.version
>         _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    2.1
> year     2005
> month    12
> day      20
> svn rev  36812
> language R
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From jeroschh at ohsu.edu  Fri Jun 30 18:36:41 2006
From: jeroschh at ohsu.edu (Michael Jerosch-Herold)
Date: Fri, 30 Jun 2006 09:36:41 -0700
Subject: [R] lme convergence
Message-ID: <s4a4f0b5.047@ohsu.edu>

It looks like in the call to lme

 fm1 <- lme(distance ~ age, data = Orthodont,
+            control=lmeControl(msMaxIter=1))

you did not specify any random effects. Why not try:

 fm1 <- lme(distance ~ age, random= ~1| groupID, data = Orthodont,
+            control=lmeControl(msMaxIter=1))

where groupID is some factor that can be used to stratify the data.

Also, the "Othodont" data set is used in Pinheiro & Bates book, and you may want to consult
that book to see the models they use in connection with that data set. For the Orthodont data set
the groupID would most likely be the subject ID ("Subject" variable).

So a possible model would be:

> fm1 <- lme(distance ~ age, random= ~1|Subject, data=Orthodont)
> summary(fm1)
Linear mixed-effects model fit by REML
 Data: Orthodont 
       AIC      BIC    logLik
  455.0025 465.6563 -223.5013

Random effects:
 Formula: ~1 | Subject
        (Intercept) Residual
StdDev:    2.114724 1.431592

Fixed effects: distance ~ age 
                Value Std.Error DF  t-value p-value
(Intercept) 16.761111 0.8023952 80 20.88885       0
age          0.660185 0.0616059 80 10.71626       0
 Correlation: 
    (Intr)
age -0.845

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-3.66453932 -0.53507984 -0.01289591  0.48742859  3.72178465 

Number of Observations: 108
Number of Groups: 27 

So this runs fine.

As, I said this data set and its analysis is discussed extensively in Pinheiro and Bates book

Michael Jerosch-Herold


>>> "Spencer Graves" <spencer.graves at pdf.com> 06/29/06 7:08 PM >>>
	  Does anyone know how to obtain the 'returnObject' from an 'lme' run 
that fails to converge?  An argument of this name is described on the 
'lmeControl' help page as, "a logical value indicating whether the 
fitted object should be returned when the maximum number of iterations 
is reached without convergence of the algorithm. Default is 'FALSE'."

	  Unfortunately, I've so far been unable to get it to work, as 
witnessed by the following modification of an example from the '?lme' 
help page:

 > library(nlme)
 > fm1 <- lme(distance ~ age, data = Orthodont,
+            control=lmeControl(msMaxIter=1))
Error in lme.formula(distance ~ age, data = Orthodont, control = 
lmeControl(msMaxIter = 1)) :
	iteration limit reached without convergence (9)
 > fm1
Error: object "fm1" not found
 > fm1 <- lme(distance ~ age, data = Orthodont,
+            control=lmeControl(msMaxIter=1,
+              returnObject=TRUE))
Error in lme.formula(distance ~ age, data = Orthodont, control = 
lmeControl(msMaxIter = 1,  :
	iteration limit reached without convergence (9)
 > fm1
Error: object "fm1" not found	

	  I might be able to fix the problem myself, working through the 'lme' 
code line by line, e.g., using 'debug'.  However, I'm not ready to do 
that just now.

	  Best Wishes,
	  Spencer Graves

Ravi Varadhan wrote:
> Use "try" to capture error messages without breaking the loop.
> ?try
> 
> --------------------------------------------------------------------------
> Ravi Varadhan, Ph.D.
> Assistant Professor,  The Center on Aging and Health
> Division of Geriatric Medicine and Gerontology
> Johns Hopkins University
> Ph: (410) 502-2619
> Fax: (410) 614-9625
> Email:  rvaradhan at jhmi.edu 
> Webpage: http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html 
> --------------------------------------------------------------------------
> 
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
>> bounces at stat.math.ethz.ch] On Behalf Of Pryseley Assam
>> Sent: Wednesday, June 28, 2006 12:18 PM
>> To: R-Users
>> Subject: [R] lme convergence
>>
>> Dear R-Users,
>>
>>   Is it possible to get the covariance matrix from an lme model that did
>> not converge ?
>>
>>   I am doing a simulation which entails fitting linear mixed models, using
>> a "for loop".
>>   Within each loop, i generate a new data set and analyze it using a mixed
>> model.  The loop stops When the "lme function" does not converge for a
>> simulated dataset. I want to inquire if there is a method to suppress the
>> error message from the lme function, or better still, a way of going about
>> this issue of the loop ending once the lme function does not converge.
>>
>>   Thanks in advance,
>>   Pryseley
>>
>>
>> ---------------------------------
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help 
>> PLEASE do read the posting guide! http://www.R-project.org/posting- 
>> guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From longeregnol at yahoo.com.cn  Fri Jun 30 18:46:14 2006
From: longeregnol at yahoo.com.cn (Long Qu)
Date: Sat, 1 Jul 2006 00:46:14 +0800 (CST)
Subject: [R] Random numbers from noncentral t-distribution
In-Reply-To: <Pine.LNX.4.64.0606300858130.18063@homer22.u.washington.edu>
Message-ID: <20060630164614.87454.qmail@web15106.mail.cnb.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060701/aa44a8aa/attachment.pl 

From HDoran at air.org  Fri Jun 30 18:57:57 2006
From: HDoran at air.org (Doran, Harold)
Date: Fri, 30 Jun 2006 12:57:57 -0400
Subject: [R] lme convergence
Message-ID: <2323A6D37908A847A7C32F1E3662C80E132545@dc1ex01.air.org>

In the old version of lme, one could construct a grouped data object and
this would alleviate the need to specify the random portion of the
model. So, Spencer's call is equivalent to 

fm1 <- lme(distance ~ age, random= ~age| Subject, data = Orthodont)

This condition does not hold under lmer, however.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Michael Jerosch-Herold
> Sent: Friday, June 30, 2006 12:37 PM
> To: spencer.graves at pdf.com
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] lme convergence
> 
> It looks like in the call to lme
> 
>  fm1 <- lme(distance ~ age, data = Orthodont,
> +            control=lmeControl(msMaxIter=1))
> 
> you did not specify any random effects. Why not try:
> 
>  fm1 <- lme(distance ~ age, random= ~1| groupID, data = Orthodont,
> +            control=lmeControl(msMaxIter=1))
> 
> where groupID is some factor that can be used to stratify the data.
> 
> Also, the "Othodont" data set is used in Pinheiro & Bates 
> book, and you may want to consult that book to see the models 
> they use in connection with that data set. For the Orthodont 
> data set the groupID would most likely be the subject ID 
> ("Subject" variable).
> 
> So a possible model would be:
> 
> > fm1 <- lme(distance ~ age, random= ~1|Subject, data=Orthodont)
> > summary(fm1)
> Linear mixed-effects model fit by REML
>  Data: Orthodont 
>        AIC      BIC    logLik
>   455.0025 465.6563 -223.5013
> 
> Random effects:
>  Formula: ~1 | Subject
>         (Intercept) Residual
> StdDev:    2.114724 1.431592
> 
> Fixed effects: distance ~ age 
>                 Value Std.Error DF  t-value p-value
> (Intercept) 16.761111 0.8023952 80 20.88885       0
> age          0.660185 0.0616059 80 10.71626       0
>  Correlation: 
>     (Intr)
> age -0.845
> 
> Standardized Within-Group Residuals:
>         Min          Q1         Med          Q3         Max 
> -3.66453932 -0.53507984 -0.01289591  0.48742859  3.72178465 
> 
> Number of Observations: 108
> Number of Groups: 27 
> 
> So this runs fine.
> 
> As, I said this data set and its analysis is discussed 
> extensively in Pinheiro and Bates book
> 
> Michael Jerosch-Herold
> 
> 
> >>> "Spencer Graves" <spencer.graves at pdf.com> 06/29/06 7:08 PM >>>
> 	  Does anyone know how to obtain the 'returnObject' 
> from an 'lme' run that fails to converge?  An argument of 
> this name is described on the 'lmeControl' help page as, "a 
> logical value indicating whether the fitted object should be 
> returned when the maximum number of iterations is reached 
> without convergence of the algorithm. Default is 'FALSE'."
> 
> 	  Unfortunately, I've so far been unable to get it to 
> work, as witnessed by the following modification of an 
> example from the '?lme' 
> help page:
> 
>  > library(nlme)
>  > fm1 <- lme(distance ~ age, data = Orthodont,
> +            control=lmeControl(msMaxIter=1))
> Error in lme.formula(distance ~ age, data = Orthodont, 
> control = lmeControl(msMaxIter = 1)) :
> 	iteration limit reached without convergence (9)  > fm1
> Error: object "fm1" not found
>  > fm1 <- lme(distance ~ age, data = Orthodont,
> +            control=lmeControl(msMaxIter=1,
> +              returnObject=TRUE))
> Error in lme.formula(distance ~ age, data = Orthodont, 
> control = lmeControl(msMaxIter = 1,  :
> 	iteration limit reached without convergence (9)  > fm1
> Error: object "fm1" not found	
> 
> 	  I might be able to fix the problem myself, working 
> through the 'lme' 
> code line by line, e.g., using 'debug'.  However, I'm not 
> ready to do that just now.
> 
> 	  Best Wishes,
> 	  Spencer Graves
> 
> Ravi Varadhan wrote:
> > Use "try" to capture error messages without breaking the loop.
> > ?try
> > 
> > 
> ----------------------------------------------------------------------
> > ----
> > Ravi Varadhan, Ph.D.
> > Assistant Professor,  The Center on Aging and Health Division of 
> > Geriatric Medicine and Gerontology Johns Hopkins University
> > Ph: (410) 502-2619
> > Fax: (410) 614-9625
> > Email:  rvaradhan at jhmi.edu
> > Webpage: 
> > http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html
> > 
> ----------------------------------------------------------------------
> > ----
> > 
> >> -----Original Message-----
> >> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help- 
> >> bounces at stat.math.ethz.ch] On Behalf Of Pryseley Assam
> >> Sent: Wednesday, June 28, 2006 12:18 PM
> >> To: R-Users
> >> Subject: [R] lme convergence
> >>
> >> Dear R-Users,
> >>
> >>   Is it possible to get the covariance matrix from an lme 
> model that 
> >> did not converge ?
> >>
> >>   I am doing a simulation which entails fitting linear 
> mixed models, 
> >> using a "for loop".
> >>   Within each loop, i generate a new data set and analyze 
> it using a 
> >> mixed model.  The loop stops When the "lme function" does not 
> >> converge for a simulated dataset. I want to inquire if there is a 
> >> method to suppress the error message from the lme 
> function, or better 
> >> still, a way of going about this issue of the loop ending 
> once the lme function does not converge.
> >>
> >>   Thanks in advance,
> >>   Pryseley
> >>
> >>
> >> ---------------------------------
> >>
> >>
> >> 	[[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list 
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide! 
> http://www.R-project.org/posting- 
> >> guide.html
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From tlumley at u.washington.edu  Fri Jun 30 19:24:32 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 30 Jun 2006 10:24:32 -0700 (PDT)
Subject: [R] Random numbers from noncentral t-distribution
In-Reply-To: <20060630164614.87454.qmail@web15106.mail.cnb.yahoo.com>
References: <20060630164614.87454.qmail@web15106.mail.cnb.yahoo.com>
Message-ID: <Pine.LNX.4.64.0606301023070.18063@homer22.u.washington.edu>

On Sat, 1 Jul 2006, Long Qu wrote:

> Thank you very much for your kind reply. It solved the problem of rt( ). :D
>
>  But it seems that the qt( ) also have problems:

Yes, there does seem to be a problem near zero. A clearer version is
   curve(qt(x,df=20,ncp=3),from=0,to=0.004)
   curve(qt(10^x,df=20,ncp=3),from=-10,to=-2,n=1000)

The fix is less obvious here. I'll file it as a bug.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From andy_liaw at merck.com  Fri Jun 30 19:27:11 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 30 Jun 2006 13:27:11 -0400
Subject: [R] Running R
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA0288467D@usctmx1106.merck.com>

Just for the record.  (One of the) problem was that configure picked up
ATLAS, but had problem with it at link time for whatever reason.

(This is on some version of Redhat, x86_64.  I should think there are people
who have similar setup and got both ATLAS and readline to work.)

Andy

> From: Pramod Anugu
> 
> IT WORKS!!!
> Thanks you very much for your help. So the below line fixed 
> my installation
> 
> #./configure --with-blas=no --with-readline=no
> 
>


From spencer.graves at pdf.com  Fri Jun 30 19:28:19 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 30 Jun 2006 10:28:19 -0700
Subject: [R] lme convergence
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E132545@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E132545@dc1ex01.air.org>
Message-ID: <44A55F33.4060501@pdf.com>

	  Harold is correct.  The help page for 'Orthodont' includes the 
following example:

 > formula(Orthodont)
distance ~ age | Subject

	  If 'random' is not specified, 'lme' sets random = formula(data).  If 
that's NULL, the 'lme' help page says it "Defaults to a formula 
consisting of the right hand side of 'fixed'."  This will generally 
return an error, indicated by the following example:

 > tstDF <- data.frame(gp=rep(1:2, 2), y=1:4)
 > lme(y~1, tstDF)
Error in getGroups.data.frame(dataMix, groups) :
	Invalid formula for groups
 > lme(y~1, tstDF, random=~1)
Error in getGroups.data.frame(dataMix, groups) :
	Invalid formula for groups
 >
	  Hope this helps.
	  Spencer

Doran, Harold wrote:
> In the old version of lme, one could construct a grouped data object and
> this would alleviate the need to specify the random portion of the
> model. So, Spencer's call is equivalent to 
> 
> fm1 <- lme(distance ~ age, random= ~age| Subject, data = Orthodont)
> 
> This condition does not hold under lmer, however.
> 
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch 
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
>> Michael Jerosch-Herold
>> Sent: Friday, June 30, 2006 12:37 PM
>> To: spencer.graves at pdf.com
>> Cc: r-help at stat.math.ethz.ch
>> Subject: Re: [R] lme convergence
>>
>> It looks like in the call to lme
>>
>>  fm1 <- lme(distance ~ age, data = Orthodont,
>> +            control=lmeControl(msMaxIter=1))
>>
>> you did not specify any random effects. Why not try:
>>
>>  fm1 <- lme(distance ~ age, random= ~1| groupID, data = Orthodont,
>> +            control=lmeControl(msMaxIter=1))
>>
>> where groupID is some factor that can be used to stratify the data.
>>
>> Also, the "Othodont" data set is used in Pinheiro & Bates 
>> book, and you may want to consult that book to see the models 
>> they use in connection with that data set. For the Orthodont 
>> data set the groupID would most likely be the subject ID 
>> ("Subject" variable).
>>
>> So a possible model would be:
>>
>>> fm1 <- lme(distance ~ age, random= ~1|Subject, data=Orthodont)
>>> summary(fm1)
>> Linear mixed-effects model fit by REML
>>  Data: Orthodont 
>>        AIC      BIC    logLik
>>   455.0025 465.6563 -223.5013
>>
>> Random effects:
>>  Formula: ~1 | Subject
>>         (Intercept) Residual
>> StdDev:    2.114724 1.431592
>>
>> Fixed effects: distance ~ age 
>>                 Value Std.Error DF  t-value p-value
>> (Intercept) 16.761111 0.8023952 80 20.88885       0
>> age          0.660185 0.0616059 80 10.71626       0
>>  Correlation: 
>>     (Intr)
>> age -0.845
>>
>> Standardized Within-Group Residuals:
>>         Min          Q1         Med          Q3         Max 
>> -3.66453932 -0.53507984 -0.01289591  0.48742859  3.72178465 
>>
>> Number of Observations: 108
>> Number of Groups: 27 
>>
>> So this runs fine.
>>
>> As, I said this data set and its analysis is discussed 
>> extensively in Pinheiro and Bates book
>>
>> Michael Jerosch-Herold
>>
>>
>>>>> "Spencer Graves" <spencer.graves at pdf.com> 06/29/06 7:08 PM >>>
>> 	  Does anyone know how to obtain the 'returnObject' 
>> from an 'lme' run that fails to converge?  An argument of 
>> this name is described on the 'lmeControl' help page as, "a 
>> logical value indicating whether the fitted object should be 
>> returned when the maximum number of iterations is reached 
>> without convergence of the algorithm. Default is 'FALSE'."
>>
>> 	  Unfortunately, I've so far been unable to get it to 
>> work, as witnessed by the following modification of an 
>> example from the '?lme' 
>> help page:
>>
>>  > library(nlme)
>>  > fm1 <- lme(distance ~ age, data = Orthodont,
>> +            control=lmeControl(msMaxIter=1))
>> Error in lme.formula(distance ~ age, data = Orthodont, 
>> control = lmeControl(msMaxIter = 1)) :
>> 	iteration limit reached without convergence (9)  > fm1
>> Error: object "fm1" not found
>>  > fm1 <- lme(distance ~ age, data = Orthodont,
>> +            control=lmeControl(msMaxIter=1,
>> +              returnObject=TRUE))
>> Error in lme.formula(distance ~ age, data = Orthodont, 
>> control = lmeControl(msMaxIter = 1,  :
>> 	iteration limit reached without convergence (9)  > fm1
>> Error: object "fm1" not found	
>>
>> 	  I might be able to fix the problem myself, working 
>> through the 'lme' 
>> code line by line, e.g., using 'debug'.  However, I'm not 
>> ready to do that just now.
>>
>> 	  Best Wishes,
>> 	  Spencer Graves
>>
>> Ravi Varadhan wrote:
>>> Use "try" to capture error messages without breaking the loop.
>>> ?try
>>>
>>>
>> ----------------------------------------------------------------------
>>> ----
>>> Ravi Varadhan, Ph.D.
>>> Assistant Professor,  The Center on Aging and Health Division of 
>>> Geriatric Medicine and Gerontology Johns Hopkins University
>>> Ph: (410) 502-2619
>>> Fax: (410) 614-9625
>>> Email:  rvaradhan at jhmi.edu
>>> Webpage: 
>>> http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html
>>>
>> ----------------------------------------------------------------------
>>> ----
>>>
>>>> -----Original Message-----
>>>> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help- 
>>>> bounces at stat.math.ethz.ch] On Behalf Of Pryseley Assam
>>>> Sent: Wednesday, June 28, 2006 12:18 PM
>>>> To: R-Users
>>>> Subject: [R] lme convergence
>>>>
>>>> Dear R-Users,
>>>>
>>>>   Is it possible to get the covariance matrix from an lme 
>> model that 
>>>> did not converge ?
>>>>
>>>>   I am doing a simulation which entails fitting linear 
>> mixed models, 
>>>> using a "for loop".
>>>>   Within each loop, i generate a new data set and analyze 
>> it using a 
>>>> mixed model.  The loop stops When the "lme function" does not 
>>>> converge for a simulated dataset. I want to inquire if there is a 
>>>> method to suppress the error message from the lme 
>> function, or better 
>>>> still, a way of going about this issue of the loop ending 
>> once the lme function does not converge.
>>>>   Thanks in advance,
>>>>   Pryseley
>>>>
>>>>
>>>> ---------------------------------
>>>>
>>>>
>>>> 	[[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list 
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting- 
>>>> guide.html
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From JeeBee at troefpunt.nl  Fri Jun 30 19:42:13 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Fri, 30 Jun 2006 19:42:13 +0200
Subject: [R] tkbutton command - how to know which button was clicked?
Message-ID: <pan.2006.06.30.17.42.12.548936@troefpunt.nl>


In the below code fragment, print(arg) always prints the
last element of rekeningen$rekening.
Is this because of lazy evaluation? I.e. arg is evaluated at
the time the button is pressed?
And, if so, how can I avoid this?
I tried function() {force(arg); print(arg)} but that didn't work either.

Thanks,
Jeebee.

  for(rek in seq(1,nrow(rekeningen))) {
    arg <- rekeningen$rekening[rek]

    tkgrid(tkbutton(frame.1,
      text=paste("Saldo historie", arg),
      command=function() print(arg)),
      sticky="news")
  }


From JeeBee at troefpunt.nl  Fri Jun 30 20:01:40 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Fri, 30 Jun 2006 20:01:40 +0200
Subject: [R] apply a function to several lists' components
References: <BAY110-F399F256B5E00047F153648C77D0@phx.gbl>
Message-ID: <pan.2006.06.30.18.01.37.914318@troefpunt.nl>


Maybe this helps

( data1 = list(a=c(1,2), b=c(3,4), c=c(5,6,7)) )
( data2 = list(a=c(10,11), b=c(30,40), c=c(70,80)) )

cc <- NULL
for(data in ls(pattern="^data[0-9]+$")) {
  cc <- c(cc, with(get(data), c))
}

mean(cc)

JeeBee.


On Fri, 30 Jun 2006 09:50:51 -0500, Taka Matzmoto wrote:

> Dear R-user
> I have 100 lists.
> Each list has several components.
> For example,
> 
>>data1
> $a
> [1] 1 2
> 
> $b
> [1] 3 4
> 
> $c
> [1] 5
> 
> There are data1, data2,...., data100. All lists have the same number and the 
> same name of components.
> 
> 
> Is there any function I can use for applying to only a specific component 
> across 100 lists?
> (e.g.,  taking mean of $c acorss 100 lists) or do I need to write my own 
> function for that?
> 
> Thank you.
> 
> Taka,
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From mb.atelier at web.de  Fri Jun 30 21:01:27 2006
From: mb.atelier at web.de (Matthias Braeunig)
Date: Fri, 30 Jun 2006 21:01:27 +0200
Subject: [R] send output to printer
Message-ID: <44A57507.1030901@web.de>

It has to be a simple thing, but I could not figure it out:

How do I send the text output from object x to the printer?
As a shell user I would expect a pipe to the printer... "|kprinter" or
"|lpr -Pmyprinter" somehow. And yes, I'm on Linux.

Thanks!


From kjarcher at vcu.edu  Fri Jun 30 21:08:53 2006
From: kjarcher at vcu.edu (Kellie J. Archer, Ph.D.)
Date: Fri, 30 Jun 2006 15:08:53 -0400
Subject: [R] SAS Proc Mixed and lme
Message-ID: <200606301908.PAA24189@arrakis.vcu.edu>

I am trying to use lme to fit a mixed effects model to get the same
results as when using the following SAS code:

proc mixed;
class refseqid probeid probeno end;
model expression=end logpgc / ddfm=satterth;
random probeno probeid / subject=refseqid type=cs;
lsmeans end / diff cl; run;

There are 3 genes (refseqid) which is the large grouping factor, with
2 probeids nested within each refseqid, and 16 probenos nested within
each of the probeids.

I have specified in the SAS Proc Mixed procedure that the
variance-covariance structure is to be compound symmetric. Therefore,
the variance-covariance matrix is a block diagonal matrix of the form,

V_1  0   0
0   V_2  0
0    0   V3

where each V_i represents a RefSeqID. Moreover, for each V_i the
structure within the block is

v_{11}   v{12}
v_{21}   v{22}

where v_{11} and v_{22} are different probeids nested within the
refseqid, and so are correlated. The structure of
both v_{11} and v_{22} are compound symmetric, and v_{12} and v{21}
contain a constant for all elements of the matrix.

I have tried to reproduce this using lme, but it is unclear from the
documentation (and Pinheiro & Bates text) how the pdBlocked and
compound symmetric structure can be combined.

fit.lme<-lme(expression~End+logpgc,random=list(RefSeqID=pdBlocked(list
(~1,~ProbeID-1),pdClass="pdSymm")),data=dataset,correlation=corCompSym
m(form=~1|RefSeqID/ProbeID/ProbeNo))


The point estimates are essentially the same comparing R and SAS for
the fixed effects, but the 95% confidence intervals are much shorter
using lme(). In order to find the difference in the algorithms used by
SAS and R I tried to extract the variance-covariance matrix to look at
its structure. I used the getVarCov() command, but it tells me that
this function is not available for nested structures. Is there another
way to extract the variance-covariance structure for nested models?
Does anyone know how I could get the var-cov structure above using
lme?


Kellie J. Archer, Ph.D.
Assistant Professor, Department of Biostatistics
Fellow, Center for the Study of Biological Complexity
Virginia Commonwealth University
1101 East Marshall St., B1-066
Richmond, VA 23298-0032
phone: (804) 827-2039
fax: (804) 828-8900
e-mail: kjarcher at vcu.edu
website: www.people.vcu.edu/~kjarcher


From kjarcher at vcu.edu  Fri Jun 30 21:31:51 2006
From: kjarcher at vcu.edu (Kellie J. Archer, Ph.D.)
Date: Fri, 30 Jun 2006 15:31:51 -0400
Subject: [R] lme and SAS Proc mixed
In-Reply-To: <mailman.13346.1151694701.5831.r-help@stat.math.ethz.ch>
Message-ID: <200606301931.PAA24757@arrakis.vcu.edu>

I am trying to use lme to fit a mixed effects model to get the same
results as when using the following SAS code:

proc mixed;
class refseqid probeid probeno end;
model expression=end logpgc / ddfm=satterth;
random probeno probeid / subject=refseqid type=cs;
lsmeans end / diff cl; run;

There are 3 genes (refseqid) which is the large grouping factor, with
2 probeids nested within each refseqid, and 16 probenos nested within
each of the probeids.

I have specified in the SAS Proc Mixed procedure that the
variance-covariance structure is to be compound symmetric. Therefore,
the variance-covariance matrix is a block diagonal matrix of the form,

V_1  0   0
0   V_2  0
0    0   V3

where each V_i represents a RefSeqID. Moreover, for each V_i the
structure within the block is 

v_{11}   v{12}
v_{21}   v{22}

where v_{11} and v_{22} are different probeids nested within the
refseqid, and so are correlated. The structure of
both v_{11} and v_{22} are compound symmetric, and v_{12} and v{21}
contain a constant for all elements of the matrix.

I have tried to reproduce this using lme, but it is unclear from the
documentation (and Pinheiro & Bates text) how the pdBlocked and
compound symmetric structure can be combined.

fit.lme<-lme(expression~End+logpgc,random=list(RefSeqID=pdBlocked(list
(~1,~ProbeID-1),pdClass="pdSymm")),data=dataset,correlation=corCompSym
m(form=~1|RefSeqID/ProbeID/ProbeNo))


The point estimates are essentially the same comparing R and SAS for
the fixed effects, but the 95% confidence intervals are much shorter
using lme(). In order to find the difference in the algorithms used by
SAS and R I tried to extract the variance-covariance matrix to look at
its structure. I used the getVarCov() command, but it tells me that
this function is not available for nested structures. Is there another
way to extract the variance-covariance structure for nested models?
Does anyone know how I could get the var-cov structure above using
lme?


Kellie J. Archer, Ph.D.
Assistant Professor, Department of Biostatistics
Fellow, Center for the Study of Biological Complexity
Virginia Commonwealth University
1101 East Marshall St., B1-066
Richmond, VA 23298-0032
phone: (804) 827-2039
fax: (804) 828-8900
e-mail: kjarcher at vcu.edu
website: www.people.vcu.edu/~kjarcher


From raphael.fraser at gmail.com  Fri Jun 30 21:58:56 2006
From: raphael.fraser at gmail.com (Raphael Fraser)
Date: Fri, 30 Jun 2006 14:58:56 -0500
Subject: [R] Creating Vectors
Message-ID: <509bb6a90606301258t49153c36vb199dc6e2c8a855a@mail.gmail.com>

 type count
   0     20
   1     15
    0     10
   1     35
   0     28

I would like to create two vectors from the data above. For example,
type1=c(15, 35) and type0 = c(20, 10, 28). Can any one help

Raphael


From p.dalgaard at biostat.ku.dk  Fri Jun 30 22:24:48 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Jun 2006 22:24:48 +0200
Subject: [R] tkbutton command - how to know which button was clicked?
In-Reply-To: <pan.2006.06.30.17.42.12.548936@troefpunt.nl>
References: <pan.2006.06.30.17.42.12.548936@troefpunt.nl>
Message-ID: <x264iiz90v.fsf@turmalin.kubism.ku.dk>

JeeBee <JeeBee at troefpunt.nl> writes:

> In the below code fragment, print(arg) always prints the
> last element of rekeningen$rekening.
> Is this because of lazy evaluation? I.e. arg is evaluated at
> the time the button is pressed?

No and yes. Lazy evaluation has nothing to do with it, but the
function passed to command= is not evaluated until the button is
pressed. If you want a different "arg" variable for each
button-function, you have to make sure that they have different
environments or have the arg actually part of the function.

There are various possible variations. One is

mk.button <- function(arg)
     tkgrid(tkbutton(frame.1,
       text=paste("Saldo historie", arg),
       command=function() print(arg)),
       sticky="news")

for .... {
    ....
    mk.button(arg)
}

another is to wrap a local({arg <- arg;.....}) around the tkgrid call
in your code. A 3rd one is

.....command=eval(substitute(function()print(arg)))...

> And, if so, how can I avoid this?
> I tried function() {force(arg); print(arg)} but that didn't work either.
> 
> Thanks,
> Jeebee.
> 
>   for(rek in seq(1,nrow(rekeningen))) {
>     arg <- rekeningen$rekening[rek]
> 
>     tkgrid(tkbutton(frame.1,
>       text=paste("Saldo historie", arg),
>       command=function() print(arg)),
>       sticky="news")
>   }

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From Achim.Zeileis at wu-wien.ac.at  Fri Jun 30 22:17:38 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 30 Jun 2006 22:17:38 +0200
Subject: [R] useR! 2006: presentation slides
Message-ID: <20060630221738.0331a63b.Achim.Zeileis@wu-wien.ac.at>

Dear useRs,

the useR! 2006 conference took place in Vienna two weeks ago: it was an
exciting and interesting meeting with about 400 useRs from all over the
world and more than 150 presentations.

Especially for those of you who could not make it to the conference, we
have made 4up PDF versions of the presentations slides available.

The slides for the keynote lectures are available at
  http://www.R-project.org/useR-2006/Keynotes/
and the user-contributed presentations at
  http://www.R-project.org/useR-2006/Presentations/

Finally, some materials for the panel discussion on "Getting recognition
for excellence in computational statistics" are provided at
  http://www.R-project.org/useR-2006/PanelDisc/
including a summary, the short presentation slides of the panelists and
the full discussion as a video.

A big thank you to everyone who contributed to the conference and...
best wishes from Vienna!

The organizing team
Achim, Torsten, David, Bettina, Fritz and Kurt.

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From alexnerdy at hotmail.com  Fri Jun 30 23:57:35 2006
From: alexnerdy at hotmail.com (Alexander Nervedi)
Date: Fri, 30 Jun 2006 21:57:35 +0000
Subject: [R] Creating Vectors
In-Reply-To: <509bb6a90606301258t49153c36vb199dc6e2c8a855a@mail.gmail.com>
Message-ID: <BAY106-F27A5055DB1C027AE20CCF3BB7D0@phx.gbl>

Hi all.

I have a factor variable distributed over time. I am looking for an elegant 
way to code duration of a state. Suppose,

>rainfall.shocks <- factor(sample(c(1,2,3), size = 15, replace = TRUE, prob 
>= unit.p),
+                  label = c("Drought", "Normal", "High"))
>
>rainfall.shocks
[1] Normal  High    High    Drought Normal  Normal  High    Normal  Drought
[10] Normal  Drought Normal  Normal  Normal  Normal


So capture the duration of say drought, I'd need a variable that is able to 
keep track of rainfall.shocks as well as its past values. I was wondering if 
there is any obvious way to do this. the Drought variable in this case would 
have values

0 0 0 1 0 0 0 0 1 0 1 0 0 0 0

many thanks for the suggestions you are likely to make.

Alexander Nervedi


