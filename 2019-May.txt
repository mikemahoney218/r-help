From dc@r|@on @end|ng |rom t@mu@edu  Wed May  1 00:29:24 2019
From: dc@r|@on @end|ng |rom t@mu@edu (David L Carlson)
Date: Tue, 30 Apr 2019 22:29:24 +0000
Subject: [R] Fwd: Re:  transpose and split dataframe
In-Reply-To: <e4a9e321-b437-eed6-344b-472319e85fec@molbio.mgh.harvard.edu>
References: <e154ccbb-71a6-c2ac-41ca-2171c8a9dc76@molbio.mgh.harvard.edu>
 <e4a9e321-b437-eed6-344b-472319e85fec@molbio.mgh.harvard.edu>
Message-ID: <1d59b3c0584a40c1b322b0efd5de7646@tamu.edu>

If you read the data frame with read.csv() or one of the other read() functions, use the asis=TRUE argument to prevent conversion to factors. If not do the conversion first:

# Convert factors to characters
DataMatrix <- sapply(TF2list, as.character)
# Split the vector of hits
DataList <- sapply(DataMatrix[, 2], strsplit, split=",")
# Use the values in Regulator to name the parts of the list
names(DataList) <- DataMatrix[,"Regulator"]

# Now create a data frame
# How long is the longest list of hits?
mx <- max(sapply(DataList, length))
# Now add NAs to vectors shorter than mx
DataList2 <- lapply(DataList, function(x) c(x, rep(NA, mx-length(x))))
# Finally convert back to a data frame
TF2list2 <- do.call(data.frame, DataList2)

Try this on a portion of the list, say 25 lines and print each object to see what is happening. 

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352





-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Matthew
Sent: Tuesday, April 30, 2019 4:31 PM
To: r-help at r-project.org
Subject: [R] Fwd: Re: transpose and split dataframe

Thanks for your reply. I was trying to simplify it a little, but must 
have got it wrong. Here is the real dataframe, TF2list:

 ?str(TF2list)
'data.frame':??? 152 obs. of? 2 variables:
 ?$ Regulator: Factor w/ 87 levels "AT1G02065","AT1G13960",..: 17 6 6 54 
54 82 82 82 82 82 ...
 ?$ hits???? : Factor w/ 97 levels 
"AT1G05675,AT3G12910,AT1G22810,AT1G14540,AT1G21120,AT1G07160,AT5G22520,AT1G56250,AT2G31345,AT5G22530,AT4G11170,A"| 
__truncated__,..: 65 57 90 57 87 57 56 91 31 17 ...

 ?? And the first few lines resulting from dput(head(TF2list)):

dput(head(TF2list))
structure(list(Regulator = structure(c(17L, 6L, 6L, 54L, 54L,
82L), .Label = c("AT1G02065", "AT1G13960", "AT1G18860", "AT1G23380",
"AT1G29280", "AT1G29860", "AT1G30650", "AT1G55600", "AT1G62300",
"AT1G62990", "AT1G64000", "AT1G66550", "AT1G66560", "AT1G66600",
"AT1G68150", "AT1G69310", "AT1G69490", "AT1G69810", "AT1G70510", ...

This is another way of looking at the first 4 entries (Regulator is 
tab-separated from hits):

Regulator
 ? hits
1
AT1G69490
 ?AT4G31950,AT5G24110,AT1G26380,AT1G05675,AT3G12910,AT5G64905,AT1G22810,AT1G79680,AT3G02840,AT5G25260,AT5G57220,AT2G37430,AT2G26560,AT1G56250,AT3G23230,AT1G16420,AT1G78410,AT4G22030,AT5G05300,AT1G69930,AT4G03460,AT4G11470,AT5G25250,AT5G36925,AT2G30750,AT1G16150,AT1G02930,AT2G19190,AT4G11890,AT1G72520,AT4G31940,AT5G37490,AT5G52760,AT5G66020,AT3G57460,AT4G23220,AT3G15518,AT2G43620,AT2G02010,AT1G35210,AT5G46295,AT1G17147,AT1G11925,AT2G39200,AT1G02920,AT2G40180,AT1G59865,AT4G35180,AT4G15417,AT1G51820,AT1G06135,AT1G36622,AT5G42830
2
AT1G29860
 ?AT4G31950,AT5G24110,AT1G05675,AT3G12910,AT5G64905,AT1G22810,AT1G14540,AT1G79680,AT1G07160,AT3G23250,AT5G25260,AT1G53625,AT5G57220,AT2G37430,AT3G54150,AT1G56250,AT3G23230,AT1G16420,AT1G78410,AT4G22030,AT1G69930,AT4G03460,AT4G11470,AT5G25250,AT5G36925,AT4G14450,AT2G30750,AT1G16150,AT1G02930,AT2G19190,AT4G11890,AT1G72520,AT4G31940,AT5G37490,AT4G08555,AT5G66020,AT5G26920,AT3G57460,AT4G23220,AT3G15518,AT2G43620,AT1G35210,AT5G46295,AT1G17147,AT1G11925,AT2G39200,AT1G02920,AT4G35180,AT4G15417,AT1G51820,AT4G40020,AT1G06135

3
AT1G2986
 ?AT5G64905,AT1G21120,AT1G07160,AT5G25260,AT1G53625,AT1G56250,AT2G31345,AT4G11170,AT1G66090,AT1G26410,AT3G55840,AT1G69930,AT4G03460,AT5G25250,AT5G36925,AT1G26420,AT5G42380,AT1G16150,AT2G22880,AT1G02930,AT4G11890,AT1G72520,AT5G66020,AT2G43620,AT2G44370,AT4G15975,AT1G35210,AT5G46295,AT1G11925,AT2G39200,AT1G02920,AT4G14370,AT4G35180,AT4G15417,AT2G18690,AT5G11140,AT1G06135,AT5G42830

 ?? So, the goal would be to

first: Transpose the existing dataframe so that the factor Regulator 
becomes a column name (column 1 name = AT1G69490, column2 name 
AT1G29860, etc.) and the hits associated with each Regulator become 
rows. Hits is a comma separated 'list' ( I do not not know if 
technically it is an R list.), so it would have to be comma 
'unseparated' with each entry becoming a row (col 1 row 1 = AT4G31950, 
col 1 row 2 - AT5G24410, etc); like this :

AT1G69490
AT4G31950
AT5G24110
AT1G05675
AT5G64905

... I did not include all the rows)

I think it would be best to actually make the first entry a separate 
dataframe ( 1 column with name = AT1G69490 and number of rows depending 
on the number of hits), then make the second column (column name = 
AT1G29860, and number of rows depending on the number of hits) into a 
new dataframe and do a full join of of the two dataframes; continue by 
making the third column (column name = AT1G2986) into a dataframe and 
full join it with the previous; continue for the 152 observations so 
that then end result is a dataframe with 152 columns and number of rows 
depending on the entry with the greatest number of hits. The full joins 
I can do with dplyr, but getting up to that point seems rather difficult.

This would get me what my ultimate goal would be; each Regulator is a 
column name (152 columns) and a given row has either NA or the same hit.

 ?? This seems very difficult to me, but I appreciate any attempt.

Matthew

On 4/30/2019 4:34 PM, David L Carlson wrote:
>          External Email - Use Caution
>
> I think we need more information. Can you give us the structure of the data with str(YourDataFrame). Alternatively you could copy a small piece into your email message by copying and pasting the results of the following code:
>
> dput(head(YourDataFrame))
>
> The data frame you present could not be a data frame since you say "hits" is a factor with a variable number of elements. If each value of "hits" was a single character string, it would only have 2 factor levels not 6 and your efforts to parse the string would make more sense. Transposing to a data frame would only be possible if each column was padded with NAs to make them equal in length. Since your example tries use the name TF2list, it is possible that you do not have a data frame but a list and you have no factor levels, just character vectors.
>
> If you are not familiar with R, it may be helpful to tell us what your overall goal is rather than an intermediate step. Very likely R can easily handle what you want by doing things a different way.
>
> ----------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77843-4352
>
>
>
> -----Original Message-----
> From: R-help<r-help-bounces at r-project.org>  On Behalf Of Matthew
> Sent: Tuesday, April 30, 2019 2:25 PM
> To: r-help (r-help at r-project.org)<r-help at r-project.org>
> Subject: [R] transpose and split dataframe
>
> I have a data frame that is a lot bigger but for simplicity sake we can
> say it looks like this:
>
> Regulator??? hits
> AT1G69490??? AT4G31950,AT5G24110,AT1G26380,AT1G05675
> AT2G55980??? AT2G85403,AT4G89223
>
>   ?? In other words:
>
> data.frame : 2 obs. of 2 variables
> $Regulator: Factor w/ 2 levels
> $hits???????? : Factor w/ 6 levels
>
>   ? I want to transpose it so that Regulator is now the column headings
> and each of the AGI numbers now separated by commas is a row. So,
> AT1G69490 is now the header of the first column and AT4G31950 is row 1
> of column 1, AT5G24110 is row 2 of column 1, etc. AT2G55980 is header of
> column 2 and AT2G85403 is row 1 of column 2, etc.
>
>   ? I have tried playing around with strsplit(TF2list[2:2]) and
> strsplit(as.character(TF2list[2:2]), but I am getting nowhere.
>
> Matthew
>
> ______________________________________________
> R-help at r-project.org  mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guidehttp://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From murdoch@dunc@n @end|ng |rom gm@||@com  Wed May  1 00:32:17 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Tue, 30 Apr 2019 18:32:17 -0400
Subject: [R] Passing formula as parameter to `lm` within `sapply` causes
 error [BUG?]
In-Reply-To: <75abba2b-c528-460e-df92-08f8479ba399@students.unibe.ch>
References: <75abba2b-c528-460e-df92-08f8479ba399@students.unibe.ch>
Message-ID: <9b51f623-98d5-1fa3-2bd5-d7d3af47a788@gmail.com>

On 30/04/2019 11:24 a.m., Jens Heumann wrote:
> Hi,
> 
> `lm` won't take formula as a parameter when it is within a `sapply`; see
> example below. Please, could anyone either point me to a syntax error or
> confirm that this might be a bug?
> 

I haven't looked carefully at your example.  From a quick glance, 
however, I'd suspect that the issue is with the formula.  Formulas have 
attached environments, where they look up variables in them that aren't 
in the data argument to lm().  In your code it's not obvious to me what 
environment would be attached, but I suspect it's the caller of sapply, 
not the environment that sapply creates for a particular value of its 
argument.  I think this because of a rule that is supposed to be 
followed in R:

   Formulas get the environment where they were created attached to 
them.  That would be your global environment.

R is flexible, so functions don't have to follow this rule, but it 
causes lots of confusion when they don't.

Duncan Murdoch



> Best,
> Jens
> 
> [Disclaimer: This is my first post here, following advice of how to
> proceed with possible bugs from here: https://www.r-project.org/bugs.html]
> 
> 
> SUMMARY
> 
> While `lm` alone accepts formula parameter `FO` well, the same within a
> `sapply` causes an error. When putting everything as parameter but
> formula `FO`, it's still working, though. All parameters work fine
> within a similar `for` loop.
> 
> 
> MCVE (see data / R-version at bottom)
> 
>   > summary(lm(y ~ x, df1, df1[["z"]] == 1, df1[["w"]]))$coef[1, ]
>     Estimate Std. Error    t value   Pr(>|t|)
>    1.6269038  0.9042738  1.7991275  0.3229600
>   > summary(lm(FO, data, data[[st]] == st1, data[[ws]]))$coef[1, ]
>     Estimate Std. Error    t value   Pr(>|t|)
>    1.6269038  0.9042738  1.7991275  0.3229600
>   > sapply(unique(df1$z), function(s)
> +   summary(lm(y ~ x, df1, df1[["z"]] == s, df1[[ws]]))$coef[1, ])
>                   [,1]       [,2]         [,3]
> Estimate   1.6269038 -0.1404174 -0.010338774
> Std. Error 0.9042738  0.4577001  1.858138516
> t value    1.7991275 -0.3067890 -0.005564049
> Pr(>|t|)   0.3229600  0.8104951  0.996457853
>   > sapply(unique(data[[st]]), function(s)
> +   summary(lm(FO, data, data[[st]] == s, data[[ws]]))$coef[1, ])  # !!!
> Error in eval(substitute(subset), data, env) : object 's' not found
>   > sapply(unique(data[[st]]), function(s)
> +   summary(lm(y ~ x, data, data[[st]] == s, data[[ws]]))$coef[1, ])
>                   [,1]       [,2]         [,3]
> Estimate   1.6269038 -0.1404174 -0.010338774
> Std. Error 0.9042738  0.4577001  1.858138516
> t value    1.7991275 -0.3067890 -0.005564049
> Pr(>|t|)   0.3229600  0.8104951  0.996457853
>   > m <- matrix(NA, 4, length(unique(data[[st]])))
>   > for (s in unique(data[[st]])) {
> +   m[, s] <- summary(lm(FO, data, data[[st]] == s, data[[ws]]))$coef[1, ]
> + }
>   > m
>             [,1]       [,2]         [,3]
> [1,] 1.6269038 -0.1404174 -0.010338774
> [2,] 0.9042738  0.4577001  1.858138516
> [3,] 1.7991275 -0.3067890 -0.005564049
> [4,] 0.3229600  0.8104951  0.996457853
> 
> # DATA #################################################################
> 
> df1 <- structure(list(x = c(1.37095844714667, -0.564698171396089,
> 0.363128411337339,
> 0.63286260496104, 0.404268323140999, -0.106124516091484, 1.51152199743894,
> -0.0946590384130976, 2.01842371387704), y = c(1.30824434809425,
> 0.740171482827397, 2.64977380403845, -0.755998096151299, 0.125479556323628,
> -0.239445852485142, 2.14747239550901, -0.37891195982917, -0.638031707027734
> ), z = c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L), w = c(0.7, 0.8,
> 1.2, 0.9, 1.3, 1.2, 0.8, 1, 1)), class = "data.frame", row.names = c(NA,
> -9L))
> 
> FO <- y ~ x; data <- df1; st <- "z"; ws <- "w"; st1 <- 1
> 
> ########################################################################
> 
>   > R.version
>                  _
> platform       x86_64-w64-mingw32
> arch           x86_64
> os             mingw32
> system         x86_64, mingw32
> status
> major          3
> minor          6.0
> year           2019
> month          04
> day            26
> svn rev        76424
> language       R
> version.string R version 3.6.0 (2019-04-26)
> nickname       Planting of a Tree
> 
> #########################################################################
> 
> NOTE: Question on SO two days ago
> (https://stackoverflow.com/questions/55893189/passing-formula-as-parameter-to-lm-within-sapply-causes-error-bug-confirmation)
> brought many views but neither answer nor bug confirmation.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @purd|e@@ @end|ng |rom gm@||@com  Wed May  1 03:19:16 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abs Spurdle)
Date: Wed, 1 May 2019 13:19:16 +1200
Subject: [R] 
 Time series (trend over time) for irregular sampling dates and
 multiple sites
In-Reply-To: <CAB8pepxHYbCXQPX5CaUQ868kMAp80z+zSXH7LHak+xDabJOjKg@mail.gmail.com>
References: <CAOQWJbvY+JKy80sksmfC8tu-C+5qq-tzwAd21XbyGvJAyYjQPQ@mail.gmail.com>
 <CAB8pepxHYbCXQPX5CaUQ868kMAp80z+zSXH7LHak+xDabJOjKg@mail.gmail.com>
Message-ID: <CAB8pepxfxCBEUmR=KuJRuVsF9GPYtkqpnBQ0u_Um+RmUSRuDGg@mail.gmail.com>

> > My data has a few problems: (1) I think I will need to fix the effects
of
> > seasonal variation (Monthly) and (2) of possible spatial correlation
> > (probability of finding an item is higher after finding one since they
can
> > come from the same ship). (3) How do I handle the fact that the
> > measurements were not taken at a regular interval?
>
> Can I ask two questions:
> (1) Is the data autocorrelated (or "Seasonal") over time?
> If not then this problem is a lot simpler.
> (2) Can you expand on the following statement?
> "possible spatial correlation (probability of finding an item is higher
after finding one since they can come from the same ship"

I just had a closer look at your example.

You've tried to model nMonth (presumably in {1, 2, ..., 12}) but is there a
long term trend, over Year?
Also, I'm not an expert on mgcv, but I was wondering if you want bs="cp"
rather than bs="ps"?

When you say "measurements were not taken at a regular interval" are you
referring to the variable "DaysIa"?
In which case, my previous question about autocorrelation applies to this
variable.

	[[alternative HTML version deleted]]


From @purd|e@@ @end|ng |rom gm@||@com  Wed May  1 06:06:49 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abs Spurdle)
Date: Wed, 1 May 2019 16:06:49 +1200
Subject: [R] 
 Time series (trend over time) for irregular sampling dates and
 multiple sites
In-Reply-To: <CAOQWJbv_Fh0v3_8bTgNUR-6P2phxU61TuHcB0drez+p7Lkbzzw@mail.gmail.com>
References: <CAOQWJbvY+JKy80sksmfC8tu-C+5qq-tzwAd21XbyGvJAyYjQPQ@mail.gmail.com>
 <CAB8pepxHYbCXQPX5CaUQ868kMAp80z+zSXH7LHak+xDabJOjKg@mail.gmail.com>
 <CAB8pepxfxCBEUmR=KuJRuVsF9GPYtkqpnBQ0u_Um+RmUSRuDGg@mail.gmail.com>
 <CAOQWJbv_Fh0v3_8bTgNUR-6P2phxU61TuHcB0drez+p7Lkbzzw@mail.gmail.com>
Message-ID: <CAB8pepysszCAsD--EDfHjc=cFqOq3yFk1dkUycTt2DJL+fVbNA@mail.gmail.com>

This is possibly off topic now...
However, given that it involves mgcv, I think that it's relevant to R.

> to test if there is a change over the years on the amount of debris in
these locations and more specifically a change after the implementation of
a mitigation strategy

> My debris items per effort (Ieffort) are fishing and shipping related
items that can be due to an intentional discharge or an accidental
discharge. It is very common to find a great amount of these items together
in the beach (from where we collected these data (beach clean-ups),
possibly having origin from the same ship. I was thinking that this can be
a problem but still don't know how to overcome or if it makes sense to
include in the model.

I could be wrong on this.
If your goal is simply to determine whether the MARPOL term in significant
or not (or how strong the effect is), I don't think the above issue is
important.
However, you could do a separate spatial analysis, which could be very
interesting...

> This does not apply along the different years.

Are you sure (there's no long term effect)?
Note that you could combine Year and nMonth into one variable, say t.
However, if I understand your variables correctly, this would be correlated
with DaysIa.
So, if you try to fit a model with both Year and DaysIa, then Year is less
likely to be significant, and you probably don't need both.

Note that another approach, is to regard month as a categorical variable.

Also, note that it may be worthwhile testing for interactions, between
MARPOL and Location or Site.
If you want to be fancy, you could test for interactions between MARPOL and
your time variables.

It's possible that there are higher order interactions, however, these sort
of models are difficult for most people to interpret, so are probably a bad
idea.

	[[alternative HTML version deleted]]


From jen@@heum@nn @end|ng |rom @tudent@@un|be@ch  Wed May  1 07:14:38 2019
From: jen@@heum@nn @end|ng |rom @tudent@@un|be@ch (Jens Heumann)
Date: Wed, 1 May 2019 07:14:38 +0200
Subject: [R] Passing formula as parameter to `lm` within `sapply` causes
 error [BUG?]
In-Reply-To: <924255D4-912E-4C24-8E85-6E313EC50203@comcast.net>
References: <75abba2b-c528-460e-df92-08f8479ba399@students.unibe.ch>
 <924255D4-912E-4C24-8E85-6E313EC50203@comcast.net>
Message-ID: <6c823a04-a2db-dd0f-5614-7c52808709ee@students.unibe.ch>

Thanks a lot for your hint, David. It finally worked doing:

 > sapply(unique(data[[st]]), function(s)
+   summary(do.call("lm", list(FO, data, data[[st]] == s,
+                              data[[ws]])))$coef[1, ])
                 [,1]       [,2]         [,3]
Estimate   1.6269038 -0.1404174 -0.010338774
Std. Error 0.9042738  0.4577001  1.858138516
t value    1.7991275 -0.3067890 -0.005564049
Pr(>|t|)   0.3229600  0.8104951  0.996457853

Best,
Jens

On 30.04.2019 23:03, David Winsemius wrote:
> Try using do.call
> 
> ?
> David
> 
> Sent from my iPhone
> 
>> On Apr 30, 2019, at 9:24 AM, Jens Heumann <jens.heumann at students.unibe.ch> wrote:
>>
>> Hi,
>>
>> `lm` won't take formula as a parameter when it is within a `sapply`; see example below. Please, could anyone either point me to a syntax error or confirm that this might be a bug?
>>
>> Best,
>> Jens
>>
>> [Disclaimer: This is my first post here, following advice of how to proceed with possible bugs from here: https://www.r-project.org/bugs.html]
>>
>>
>> SUMMARY
>>
>> While `lm` alone accepts formula parameter `FO` well, the same within a `sapply` causes an error. When putting everything as parameter but formula `FO`, it's still working, though. All parameters work fine within a similar `for` loop.
>>
>>
>> MCVE (see data / R-version at bottom)
>>
>>> summary(lm(y ~ x, df1, df1[["z"]] == 1, df1[["w"]]))$coef[1, ]
>>   Estimate Std. Error    t value   Pr(>|t|)
>> 1.6269038  0.9042738  1.7991275  0.3229600
>>> summary(lm(FO, data, data[[st]] == st1, data[[ws]]))$coef[1, ]
>>   Estimate Std. Error    t value   Pr(>|t|)
>> 1.6269038  0.9042738  1.7991275  0.3229600
>>> sapply(unique(df1$z), function(s)
>> +   summary(lm(y ~ x, df1, df1[["z"]] == s, df1[[ws]]))$coef[1, ])
>>                 [,1]       [,2]         [,3]
>> Estimate   1.6269038 -0.1404174 -0.010338774
>> Std. Error 0.9042738  0.4577001  1.858138516
>> t value    1.7991275 -0.3067890 -0.005564049
>> Pr(>|t|)   0.3229600  0.8104951  0.996457853
>>> sapply(unique(data[[st]]), function(s)
>> +   summary(lm(FO, data, data[[st]] == s, data[[ws]]))$coef[1, ])  # !!!
>> Error in eval(substitute(subset), data, env) : object 's' not found
>>> sapply(unique(data[[st]]), function(s)
>> +   summary(lm(y ~ x, data, data[[st]] == s, data[[ws]]))$coef[1, ])
>>                 [,1]       [,2]         [,3]
>> Estimate   1.6269038 -0.1404174 -0.010338774
>> Std. Error 0.9042738  0.4577001  1.858138516
>> t value    1.7991275 -0.3067890 -0.005564049
>> Pr(>|t|)   0.3229600  0.8104951  0.996457853
>>> m <- matrix(NA, 4, length(unique(data[[st]])))
>>> for (s in unique(data[[st]])) {
>> +   m[, s] <- summary(lm(FO, data, data[[st]] == s, data[[ws]]))$coef[1, ]
>> + }
>>> m
>>           [,1]       [,2]         [,3]
>> [1,] 1.6269038 -0.1404174 -0.010338774
>> [2,] 0.9042738  0.4577001  1.858138516
>> [3,] 1.7991275 -0.3067890 -0.005564049
>> [4,] 0.3229600  0.8104951  0.996457853
>>
>> # DATA #################################################################
>>
>> df1 <- structure(list(x = c(1.37095844714667, -0.564698171396089, 0.363128411337339,
>> 0.63286260496104, 0.404268323140999, -0.106124516091484, 1.51152199743894,
>> -0.0946590384130976, 2.01842371387704), y = c(1.30824434809425,
>> 0.740171482827397, 2.64977380403845, -0.755998096151299, 0.125479556323628,
>> -0.239445852485142, 2.14747239550901, -0.37891195982917, -0.638031707027734
>> ), z = c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L), w = c(0.7, 0.8,
>> 1.2, 0.9, 1.3, 1.2, 0.8, 1, 1)), class = "data.frame", row.names = c(NA,
>> -9L))
>>
>> FO <- y ~ x; data <- df1; st <- "z"; ws <- "w"; st1 <- 1
>>
>> ########################################################################
>>
>>> R.version
>>                _
>> platform       x86_64-w64-mingw32
>> arch           x86_64
>> os             mingw32
>> system         x86_64, mingw32
>> status
>> major          3
>> minor          6.0
>> year           2019
>> month          04
>> day            26
>> svn rev        76424
>> language       R
>> version.string R version 3.6.0 (2019-04-26)
>> nickname       Planting of a Tree
>>
>> #########################################################################
>>
>> NOTE: Question on SO two days ago (https://stackoverflow.com/questions/55893189/passing-formula-as-parameter-to-lm-within-sapply-causes-error-bug-confirmation) brought many views but neither answer nor bug confirmation.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From pd@|gd @end|ng |rom gm@||@com  Wed May  1 10:47:10 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Wed, 1 May 2019 10:47:10 +0200
Subject: [R] Passing formula as parameter to `lm` within `sapply` causes
 error [BUG?]
In-Reply-To: <6c823a04-a2db-dd0f-5614-7c52808709ee@students.unibe.ch>
References: <75abba2b-c528-460e-df92-08f8479ba399@students.unibe.ch>
 <924255D4-912E-4C24-8E85-6E313EC50203@comcast.net>
 <6c823a04-a2db-dd0f-5614-7c52808709ee@students.unibe.ch>
Message-ID: <00B52F89-6DCF-4E08-A8A4-970C61567940@gmail.com>

Or, following up on the hint by Duncan, this works too

> sapply(unique(data[[st]]), function(s){
+   environment(FO) <- environment()
+   summary(lm(FO, data, data[[st]] == s, data[[ws]]))$coef[1, ]
+   })  # !!!
                [,1]       [,2]         [,3]
Estimate   1.6269038 -0.1404174 -0.010338774
Std. Error 0.9042738  0.4577001  1.858138516
t value    1.7991275 -0.3067890 -0.005564049
Pr(>|t|)   0.3229600  0.8104951  0.996457853

or even :

> sapply(unique(data[[st]]), function(s){
+   environment(FO) <- environment()
+   summary(lm(FO, data, z == s, w))$coef[1, ]
+   })  # !!!
                [,1]       [,2]         [,3]
Estimate   1.6269038 -0.1404174 -0.010338774
Std. Error 0.9042738  0.4577001  1.858138516
t value    1.7991275 -0.3067890 -0.005564049
Pr(>|t|)   0.3229600  0.8104951  0.996457853
>

> On 1 May 2019, at 07:14 , Jens Heumann <jens.heumann at students.unibe.ch> wrote:
> 
> Thanks a lot for your hint, David. It finally worked doing:
> 
> > sapply(unique(data[[st]]), function(s)
> +   summary(do.call("lm", list(FO, data, data[[st]] == s,
> +                              data[[ws]])))$coef[1, ])
>                [,1]       [,2]         [,3]
> Estimate   1.6269038 -0.1404174 -0.010338774
> Std. Error 0.9042738  0.4577001  1.858138516
> t value    1.7991275 -0.3067890 -0.005564049
> Pr(>|t|)   0.3229600  0.8104951  0.996457853
> 
> Best,
> Jens
> 
> On 30.04.2019 23:03, David Winsemius wrote:
>> Try using do.call
>> ?
>> David
>> Sent from my iPhone
>>> On Apr 30, 2019, at 9:24 AM, Jens Heumann <jens.heumann at students.unibe.ch> wrote:
>>> 
>>> Hi,
>>> 
>>> `lm` won't take formula as a parameter when it is within a `sapply`; see example below. Please, could anyone either point me to a syntax error or confirm that this might be a bug?
>>> 
>>> Best,
>>> Jens
>>> 
>>> [Disclaimer: This is my first post here, following advice of how to proceed with possible bugs from here: https://www.r-project.org/bugs.html]
>>> 
>>> 
>>> SUMMARY
>>> 
>>> While `lm` alone accepts formula parameter `FO` well, the same within a `sapply` causes an error. When putting everything as parameter but formula `FO`, it's still working, though. All parameters work fine within a similar `for` loop.
>>> 
>>> 
>>> MCVE (see data / R-version at bottom)
>>> 
>>>> summary(lm(y ~ x, df1, df1[["z"]] == 1, df1[["w"]]))$coef[1, ]
>>>  Estimate Std. Error    t value   Pr(>|t|)
>>> 1.6269038  0.9042738  1.7991275  0.3229600
>>>> summary(lm(FO, data, data[[st]] == st1, data[[ws]]))$coef[1, ]
>>>  Estimate Std. Error    t value   Pr(>|t|)
>>> 1.6269038  0.9042738  1.7991275  0.3229600
>>>> sapply(unique(df1$z), function(s)
>>> +   summary(lm(y ~ x, df1, df1[["z"]] == s, df1[[ws]]))$coef[1, ])
>>>                [,1]       [,2]         [,3]
>>> Estimate   1.6269038 -0.1404174 -0.010338774
>>> Std. Error 0.9042738  0.4577001  1.858138516
>>> t value    1.7991275 -0.3067890 -0.005564049
>>> Pr(>|t|)   0.3229600  0.8104951  0.996457853
>>>> sapply(unique(data[[st]]), function(s)
>>> +   summary(lm(FO, data, data[[st]] == s, data[[ws]]))$coef[1, ])  # !!!
>>> Error in eval(substitute(subset), data, env) : object 's' not found
>>>> sapply(unique(data[[st]]), function(s)
>>> +   summary(lm(y ~ x, data, data[[st]] == s, data[[ws]]))$coef[1, ])
>>>                [,1]       [,2]         [,3]
>>> Estimate   1.6269038 -0.1404174 -0.010338774
>>> Std. Error 0.9042738  0.4577001  1.858138516
>>> t value    1.7991275 -0.3067890 -0.005564049
>>> Pr(>|t|)   0.3229600  0.8104951  0.996457853
>>>> m <- matrix(NA, 4, length(unique(data[[st]])))
>>>> for (s in unique(data[[st]])) {
>>> +   m[, s] <- summary(lm(FO, data, data[[st]] == s, data[[ws]]))$coef[1, ]
>>> + }
>>>> m
>>>          [,1]       [,2]         [,3]
>>> [1,] 1.6269038 -0.1404174 -0.010338774
>>> [2,] 0.9042738  0.4577001  1.858138516
>>> [3,] 1.7991275 -0.3067890 -0.005564049
>>> [4,] 0.3229600  0.8104951  0.996457853
>>> 
>>> # DATA #################################################################
>>> 
>>> df1 <- structure(list(x = c(1.37095844714667, -0.564698171396089, 0.363128411337339,
>>> 0.63286260496104, 0.404268323140999, -0.106124516091484, 1.51152199743894,
>>> -0.0946590384130976, 2.01842371387704), y = c(1.30824434809425,
>>> 0.740171482827397, 2.64977380403845, -0.755998096151299, 0.125479556323628,
>>> -0.239445852485142, 2.14747239550901, -0.37891195982917, -0.638031707027734
>>> ), z = c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L), w = c(0.7, 0.8,
>>> 1.2, 0.9, 1.3, 1.2, 0.8, 1, 1)), class = "data.frame", row.names = c(NA,
>>> -9L))
>>> 
>>> FO <- y ~ x; data <- df1; st <- "z"; ws <- "w"; st1 <- 1
>>> 
>>> ########################################################################
>>> 
>>>> R.version
>>>               _
>>> platform       x86_64-w64-mingw32
>>> arch           x86_64
>>> os             mingw32
>>> system         x86_64, mingw32
>>> status
>>> major          3
>>> minor          6.0
>>> year           2019
>>> month          04
>>> day            26
>>> svn rev        76424
>>> language       R
>>> version.string R version 3.6.0 (2019-04-26)
>>> nickname       Planting of a Tree
>>> 
>>> #########################################################################
>>> 
>>> NOTE: Question on SO two days ago (https://stackoverflow.com/questions/55893189/passing-formula-as-parameter-to-lm-within-sapply-causes-error-bug-confirmation) brought many views but neither answer nor bug confirmation.
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From gr@eme@r@d@v|d@on @end|ng |rom gm@||@com  Wed May  1 17:10:50 2019
From: gr@eme@r@d@v|d@on @end|ng |rom gm@||@com (Graeme Davidson)
Date: Wed, 1 May 2019 16:10:50 +0100
Subject: [R] Referencing multiple lists in api loop call
Message-ID: <BEC27F18-EE2D-4E70-85B2-CA43DF113ADB@gmail.com>

Hi all, 

I am working with a google analytics api, and need to pull out data on multiple views (n = 11, basically different databases with the same data types and formats).  However, in addition to this I need to specify the beginning and end date separately, in to get each months data, and I need to do this over multiple months (n = 6). Longer date ranges are not suitable, as the data aggregates between the specified dates.

I am using the RGA package for this as I need to pull out multichannel data. 

My attempt pulls in the data but only for one month. I am not sure how to specify the lists of the start and end dates for each month. Happy to use a different approach if this is not optimal. 

install.packages(?RGA?)
library(RGA)


#get list of month dates for last 6 months
end_months <- ymd("2018-11-01")+ months(0:6)-days(1)
start_months <- ymd("2018-10-01")+ months(0:6)

#list of view ids required to make api call (pseudonymised)
viewId <- c(26494958, 477448251,
            47843527, 96382507,
            537821552, 67482819)

for (i in viewId) {
  ga_data_temp <-
    get_mcf(i, #=This is a (dynamic) ViewID parameter
            start.date = "2019-01-01", end.date = "2019-01-31",
                     dimensions=c('mcf:conversionGoalNumber', 'mcf:basicChannelGroupingPath'),
                     metrics=c('mcf:totalConversions', 'mcf:totalConversionValue'))
  ga_data_temp$viewId <- i
  ga_data_final <- rbind(ga_data_final, ga_data_temp)
}


Thanks for any support in advance. 

All the best

Graeme

Graeme R Davidson PhD

Data and Insight Analyst 

From mccorm@ck @end|ng |rom mo|b|o@mgh@h@rv@rd@edu  Thu May  2 01:04:08 2019
From: mccorm@ck @end|ng |rom mo|b|o@mgh@h@rv@rd@edu (Matthew)
Date: Wed, 1 May 2019 19:04:08 -0400
Subject: [R] Fwd: Re: transpose and split dataframe
In-Reply-To: <1d59b3c0584a40c1b322b0efd5de7646@tamu.edu>
References: <e154ccbb-71a6-c2ac-41ca-2171c8a9dc76@molbio.mgh.harvard.edu>
 <e4a9e321-b437-eed6-344b-472319e85fec@molbio.mgh.harvard.edu>
 <1d59b3c0584a40c1b322b0efd5de7646@tamu.edu>
Message-ID: <efcf4d33-51cc-050a-cf87-d2c58d31af4c@molbio.mgh.harvard.edu>

Thank you very much, David and Jim for your work and solutions.

I have been working through both of them to better learn R. They both 
proceed through a similar logic except David's starts with a character 
matrix and Jim's with a dataframe, and both end with equivalent 
dataframes (? identical(tmmdf, TF2list2)) returns TRUE? ). They have 
both been very helpful. However, there is one attribute of my intended 
final dataframe that is missing.

Looking at part of the final dataframe:

 ?head(tmmdf)
 ? AT1G69490 AT1G29860 AT1G29860.1 AT4G18170 AT4G18170.1 AT5G46350
1 *AT4G31950* *AT4G31950*?? AT5G64905 *AT4G31950* AT5G64905 *AT4G31950*
2 AT5G24110 AT5G24110?? AT1G21120 AT5G24110?? AT1G14540 AT5G24110
3 AT1G26380 AT1G05675?? AT1G07160 AT1G05675?? AT1G21120 AT1G05675

Row 1 has *AT4G31950* in columns 1,2,4 and 6, but AT4G31950 in columns 3 
and 5. What I was aiming at would be that each row would have a unique 
entry so that AT4G31950 is row 1 columns 1,2,4 and 6, and NA is row 1 
columns 3 and 5. AT4G31950 is row 2 columns 3 and 5 and NA is row 2 
columns 1,2,4 and 6. So, it would look like this:

 ?head(intended_df)
 ? AT1G69490 AT1G29860 AT1G29860.1 AT4G18170 AT4G18170.1 AT5G46350
1 AT4G31950 AT4G31950???? NA ? ? ? ? ? ? ?? AT4G31950? ? ?? NA ? ? ? ? 
AT4G31950

2 ? ?? NA ? ? ? ? ? ? ?? NA ? ? ? ? ? AT4G31950 ? ? ? NA ? ? ? ? ?? 
AT4G31950 ? ?? NA

I have been trying to adjust the code to get my intended result 
basically by trying to build a dataframe one column at a time from each 
entry in the character matrix, but have not got anything near working yet.

Matthew

On 4/30/2019 6:29 PM, David L Carlson wrote
> If you read the data frame with read.csv() or one of the other read() functions, use the asis=TRUE argument to prevent conversion to factors. If not do the conversion first:
>
> # Convert factors to characters
> DataMatrix <- sapply(TF2list, as.character)
> # Split the vector of hits
> DataList <- sapply(DataMatrix[, 2], strsplit, split=",")
> # Use the values in Regulator to name the parts of the list
> names(DataList) <- DataMatrix[,"Regulator"]
>
> # Now create a data frame
> # How long is the longest list of hits?
> mx <- max(sapply(DataList, length))
> # Now add NAs to vectors shorter than mx
> DataList2 <- lapply(DataList, function(x) c(x, rep(NA, mx-length(x))))
> # Finally convert back to a data frame
> TF2list2 <- do.call(data.frame, DataList2)
>
> Try this on a portion of the list, say 25 lines and print each object to see what is happening.
>
> ----------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77843-4352
>
>
>
>
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Matthew
> Sent: Tuesday, April 30, 2019 4:31 PM
> To: r-help at r-project.org
> Subject: [R] Fwd: Re: transpose and split dataframe
>
> Thanks for your reply. I was trying to simplify it a little, but must
> have got it wrong. Here is the real dataframe, TF2list:
>
>   ?str(TF2list)
> 'data.frame':??? 152 obs. of? 2 variables:
>   ?$ Regulator: Factor w/ 87 levels "AT1G02065","AT1G13960",..: 17 6 6 54
> 54 82 82 82 82 82 ...
>   ?$ hits???? : Factor w/ 97 levels
> "AT1G05675,AT3G12910,AT1G22810,AT1G14540,AT1G21120,AT1G07160,AT5G22520,AT1G56250,AT2G31345,AT5G22530,AT4G11170,A"|
> __truncated__,..: 65 57 90 57 87 57 56 91 31 17 ...
>
>   ?? And the first few lines resulting from dput(head(TF2list)):
>
> dput(head(TF2list))
> structure(list(Regulator = structure(c(17L, 6L, 6L, 54L, 54L,
> 82L), .Label = c("AT1G02065", "AT1G13960", "AT1G18860", "AT1G23380",
> "AT1G29280", "AT1G29860", "AT1G30650", "AT1G55600", "AT1G62300",
> "AT1G62990", "AT1G64000", "AT1G66550", "AT1G66560", "AT1G66600",
> "AT1G68150", "AT1G69310", "AT1G69490", "AT1G69810", "AT1G70510", ...
>
> This is another way of looking at the first 4 entries (Regulator is
> tab-separated from hits):
>
> Regulator
>   ? hits
> 1
> AT1G69490
>   ?AT4G31950,AT5G24110,AT1G26380,AT1G05675,AT3G12910,AT5G64905,AT1G22810,AT1G79680,AT3G02840,AT5G25260,AT5G57220,AT2G37430,AT2G26560,AT1G56250,AT3G23230,AT1G16420,AT1G78410,AT4G22030,AT5G05300,AT1G69930,AT4G03460,AT4G11470,AT5G25250,AT5G36925,AT2G30750,AT1G16150,AT1G02930,AT2G19190,AT4G11890,AT1G72520,AT4G31940,AT5G37490,AT5G52760,AT5G66020,AT3G57460,AT4G23220,AT3G15518,AT2G43620,AT2G02010,AT1G35210,AT5G46295,AT1G17147,AT1G11925,AT2G39200,AT1G02920,AT2G40180,AT1G59865,AT4G35180,AT4G15417,AT1G51820,AT1G06135,AT1G36622,AT5G42830
> 2
> AT1G29860
>   ?AT4G31950,AT5G24110,AT1G05675,AT3G12910,AT5G64905,AT1G22810,AT1G14540,AT1G79680,AT1G07160,AT3G23250,AT5G25260,AT1G53625,AT5G57220,AT2G37430,AT3G54150,AT1G56250,AT3G23230,AT1G16420,AT1G78410,AT4G22030,AT1G69930,AT4G03460,AT4G11470,AT5G25250,AT5G36925,AT4G14450,AT2G30750,AT1G16150,AT1G02930,AT2G19190,AT4G11890,AT1G72520,AT4G31940,AT5G37490,AT4G08555,AT5G66020,AT5G26920,AT3G57460,AT4G23220,AT3G15518,AT2G43620,AT1G35210,AT5G46295,AT1G17147,AT1G11925,AT2G39200,AT1G02920,AT4G35180,AT4G15417,AT1G51820,AT4G40020,AT1G06135
>
> 3
> AT1G2986
>   ?AT5G64905,AT1G21120,AT1G07160,AT5G25260,AT1G53625,AT1G56250,AT2G31345,AT4G11170,AT1G66090,AT1G26410,AT3G55840,AT1G69930,AT4G03460,AT5G25250,AT5G36925,AT1G26420,AT5G42380,AT1G16150,AT2G22880,AT1G02930,AT4G11890,AT1G72520,AT5G66020,AT2G43620,AT2G44370,AT4G15975,AT1G35210,AT5G46295,AT1G11925,AT2G39200,AT1G02920,AT4G14370,AT4G35180,AT4G15417,AT2G18690,AT5G11140,AT1G06135,AT5G42830
>
>   ?? So, the goal would be to
>
> first: Transpose the existing dataframe so that the factor Regulator
> becomes a column name (column 1 name = AT1G69490, column2 name
> AT1G29860, etc.) and the hits associated with each Regulator become
> rows. Hits is a comma separated 'list' ( I do not not know if
> technically it is an R list.), so it would have to be comma
> 'unseparated' with each entry becoming a row (col 1 row 1 = AT4G31950,
> col 1 row 2 - AT5G24410, etc); like this :
>
> AT1G69490
> AT4G31950
> AT5G24110
> AT1G05675
> AT5G64905
>
> ... I did not include all the rows)
>
> I think it would be best to actually make the first entry a separate
> dataframe ( 1 column with name = AT1G69490 and number of rows depending
> on the number of hits), then make the second column (column name =
> AT1G29860, and number of rows depending on the number of hits) into a
> new dataframe and do a full join of of the two dataframes; continue by
> making the third column (column name = AT1G2986) into a dataframe and
> full join it with the previous; continue for the 152 observations so
> that then end result is a dataframe with 152 columns and number of rows
> depending on the entry with the greatest number of hits. The full joins
> I can do with dplyr, but getting up to that point seems rather difficult.
>
> This would get me what my ultimate goal would be; each Regulator is a
> column name (152 columns) and a given row has either NA or the same hit.
>
>   ?? This seems very difficult to me, but I appreciate any attempt.
>
> Matthew
>
> On 4/30/2019 4:34 PM, David L Carlson wrote:
>>           External Email - Use Caution
>>
>> I think we need more information. Can you give us the structure of the data with str(YourDataFrame). Alternatively you could copy a small piece into your email message by copying and pasting the results of the following code:
>>
>> dput(head(YourDataFrame))
>>
>> The data frame you present could not be a data frame since you say "hits" is a factor with a variable number of elements. If each value of "hits" was a single character string, it would only have 2 factor levels not 6 and your efforts to parse the string would make more sense. Transposing to a data frame would only be possible if each column was padded with NAs to make them equal in length. Since your example tries use the name TF2list, it is possible that you do not have a data frame but a list and you have no factor levels, just character vectors.
>>
>> If you are not familiar with R, it may be helpful to tell us what your overall goal is rather than an intermediate step. Very likely R can easily handle what you want by doing things a different way.
>>
>> ----------------------------------------
>> David L Carlson
>> Department of Anthropology
>> Texas A&M University
>> College Station, TX 77843-4352
>>
>>
>>
>> -----Original Message-----
>> From: R-help<r-help-bounces at r-project.org>  On Behalf Of Matthew
>> Sent: Tuesday, April 30, 2019 2:25 PM
>> To: r-help (r-help at r-project.org)<r-help at r-project.org>
>> Subject: [R] transpose and split dataframe
>>
>> I have a data frame that is a lot bigger but for simplicity sake we can
>> say it looks like this:
>>
>> Regulator??? hits
>> AT1G69490??? AT4G31950,AT5G24110,AT1G26380,AT1G05675
>> AT2G55980??? AT2G85403,AT4G89223
>>
>>    ?? In other words:
>>
>> data.frame : 2 obs. of 2 variables
>> $Regulator: Factor w/ 2 levels
>> $hits???????? : Factor w/ 6 levels
>>
>>    ? I want to transpose it so that Regulator is now the column headings
>> and each of the AGI numbers now separated by commas is a row. So,
>> AT1G69490 is now the header of the first column and AT4G31950 is row 1
>> of column 1, AT5G24110 is row 2 of column 1, etc. AT2G55980 is header of
>> column 2 and AT2G85403 is row 1 of column 2, etc.
>>
>>    ? I have tried playing around with strsplit(TF2list[2:2]) and
>> strsplit(as.character(TF2list[2:2]), but I am getting nowhere.
>>
>> Matthew
>>
>> ______________________________________________
>> R-help at r-project.org  mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guidehttp://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From @hugu@ng79 @end|ng |rom qq@com  Thu May  2 06:57:37 2019
From: @hugu@ng79 @end|ng |rom qq@com (Shuguang Sun)
Date: Thu, 02 May 2019 12:57:37 +0800
Subject: [R] Can't rsync extsoft/3.6 when compiling R 3.6
Message-ID: <vk5ysgtxv4fy.fsf@qq.com>+1D90A6F9E5364214

Hi all,

When I try to compile R 3.6 in windows 10 in Rtools35, it raised the error message:
--8<---------------cut here---------------start------------->8---
make rsync-extsoft
(mkdir -p ../../extsoft; \
cd ../../extsoft; \
rsync --timeout=60 -rcvp --delete cran.r-project.org::CRAN/bin/windows/extsoft/3.6/ . ) 
receiving incremental file list
rsync: change_dir "/bin/windows/extsoft/3.6" (in CRAN) failed: No such file or directory (2)
--8<---------------cut here---------------end--------------->8---

There is no "windows/extsoft/3.6/" and only up to 3.5.

Best Regards,
Shuguang Sun


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Thu May  2 09:36:39 2019
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Thu, 2 May 2019 09:36:39 +0200
Subject: [R] Can't rsync extsoft/3.6 when compiling R 3.6
In-Reply-To: <vk5ysgtxv4fy.fsf@qq.com>
References: <vk5ysgtxv4fy.fsf@qq.com>
Message-ID: <23754.40455.851085.510425@stat.math.ethz.ch>

>>>>> Shuguang Sun 
>>>>>     on Thu, 2 May 2019 12:57:37 +0800 writes:

    > Hi all,
    > When I try to compile R 3.6 in windows 10 in Rtools35, it raised the error message:
    > --8<---------------cut here---------------start------------->8---
    > make rsync-extsoft
    > (mkdir -p ../../extsoft; \
    > cd ../../extsoft; \
    > rsync --timeout=60 -rcvp --delete cran.r-project.org::CRAN/bin/windows/extsoft/3.6/ . ) 
    > receiving incremental file list
    > rsync: change_dir "/bin/windows/extsoft/3.6" (in CRAN) failed: No such file or directory (2)
    > --8<---------------cut here---------------end--------------->8---

    > There is no "windows/extsoft/3.6/" and only up to 3.5.

    > Best Regards,
    > Shuguang Sun

Indeed.
If you look at this FTP directory in a capabable browser (such
as Emacs), you see

    Dec 22 2014  3.2
    Jun 16 2016  3.3
    Jul  6 2016  3.4 -> 3.3
    May  8 2017  3.5 -> 3.3

So it seems the CRAN team (or the R Core release master?) forgot
to make that symbolic link there.

As workaround get the 3.5 version and "call it" 3.6.

Best,
Martin Maechler
ETH Zurich and R Core team


    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.


From jeroenoom@ @end|ng |rom gm@||@com  Thu May  2 12:10:01 2019
From: jeroenoom@ @end|ng |rom gm@||@com (Jeroen Ooms)
Date: Thu, 2 May 2019 12:10:01 +0200
Subject: [R] Can't rsync extsoft/3.6 when compiling R 3.6
In-Reply-To: <23754.40455.851085.510425@stat.math.ethz.ch>
References: <vk5ysgtxv4fy.fsf@qq.com>
 <23754.40455.851085.510425@stat.math.ethz.ch>
Message-ID: <CABFfbXtYk2MO0LS6Oq=Grr3P1JK4gnbqd-d59G5SjtPp9ZLXHg@mail.gmail.com>

On Thu, May 2, 2019 at 9:36 AM Martin Maechler
<maechler at stat.math.ethz.ch> wrote:
>
> >>>>> Shuguang Sun
> >>>>>     on Thu, 2 May 2019 12:57:37 +0800 writes:
>
>     > Hi all,
>     > When I try to compile R 3.6 in windows 10 in Rtools35, it raised the error message:
>     > --8<---------------cut here---------------start------------->8---
>     > make rsync-extsoft
>     > (mkdir -p ../../extsoft; \
>     > cd ../../extsoft; \
>     > rsync --timeout=60 -rcvp --delete cran.r-project.org::CRAN/bin/windows/extsoft/3.6/ . )
>     > receiving incremental file list
>     > rsync: change_dir "/bin/windows/extsoft/3.6" (in CRAN) failed: No such file or directory (2)
>     > --8<---------------cut here---------------end--------------->8---
>
>     > There is no "windows/extsoft/3.6/" and only up to 3.5.
>
>     > Best Regards,
>     > Shuguang Sun
>
> Indeed.
> If you look at this FTP directory in a capabable browser (such
> as Emacs), you see
>
>     Dec 22 2014  3.2
>     Jun 16 2016  3.3
>     Jul  6 2016  3.4 -> 3.3
>     May  8 2017  3.5 -> 3.3
>
> So it seems the CRAN team (or the R Core release master?) forgot
> to make that symbolic link there.

I think that might be my responsibility (not entirely sure honestly).
I'll add that directory and include the current versions of the libs
we use to build the official r-base on Windows.


From mure|th|h@dd|@on @end|ng |rom gm@||@com  Thu May  2 14:37:26 2019
From: mure|th|h@dd|@on @end|ng |rom gm@||@com (Haddison Mureithi)
Date: Thu, 2 May 2019 15:37:26 +0300
Subject: [R] Survuval Anaysis
In-Reply-To: <mailman.354552.1.1556704801.3302.r-help@r-project.org>
References: <mailman.354552.1.1556704801.3302.r-help@r-project.org>
Message-ID: <CABVwvn65QfJzmmD7+MhgVaCWQuqFirTdC6iPoPKJpUiHxZx2NQ@mail.gmail.com>

Hello guys this problem was never answered and I happened to come across
the same problem , kindly help. This is a simple R program that I have been
trying to run. I keep running into the "singular matrix" error. I end up
with no sensible results. Can anyone suggest any changes or a way around
this?

I am a total rookie when working with R.

Thanks,
Haddison

> library(survival)
Loading required package: splines
> args(coxph)
function (formula, data, weights, subset, na.action, init, control,
    method = c("efron", "breslow", "exact"), singular.ok = TRUE,
    robust = FALSE, model = FALSE, x = FALSE, y = TRUE, tt, ...)
NULL
> test1<-read.table("S:/FISHDO/03_Phase_I_Field_Work/Data_6_28_2011/Working
Folder/R_files/4SondesJuly24.csv", header=T, sep=",")
> sondes<-coxph(Surv(Start, Stop, Depart)~DOLoomis + DOI55 + DODamen,
data=test1)
Warning messages:
1: In fitter(X, Y, strats, offset, init, control, weights = weights,  :
  Loglik converged before variable  1,2 ; beta may be infinite.
2: In coxph(Surv(Start, Stop, Depart) ~ DOLoomis + DOI55 + DODamen,  :
  X matrix deemed to be singular; variable 3
> summary(sondes)
Call:
coxph(formula = Surv(Start, Stop, Depart) ~ DOLoomis + DOI55 +
    DODamen, data = test1)

  n= 1737, number of events= 58
   (1 observation deleted due to missingness)

               coef  exp(coef)   se(coef)  z Pr(>|z|)
DOLoomis -2.152e+00  1.163e-01  1.161e+05  0        1
DOI55     4.560e-01  1.578e+00  3.755e+04  0        1
DODamen          NA         NA  0.000e+00 NA       NA

         exp(coef) exp(-coef) lower .95 upper .95
DOLoomis    0.1163     8.5995         0       Inf
DOI55       1.5777     0.6338         0       Inf
DODamen         NA         NA        NA        NA

Concordance= 0.5  (se = 0 )
Rsquare= 0   (max possible= 0.01 )
Likelihood ratio test= 0  on 2 df,   p=1
Wald test            = 0  on 2 df,   p=1
Score (logrank) test = 0  on 2 df,   p=1

On Wed, 1 May 2019, 1:00 pm , <r-help-request at r-project.org> wrote:

> Send R-help mailing list submissions to
>         r-help at r-project.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         https://stat.ethz.ch/mailman/listinfo/r-help
> or, via email, send a message with subject or body 'help' to
>         r-help-request at r-project.org
>
> You can reach the person managing the list at
>         r-help-owner at r-project.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of R-help digest..."
>
>
> Today's Topics:
>
>    1. Re: Bug in R 3.6.0? (Martin Maechler)
>    2. Re: Bug in R 3.6.0? (ocjt at free.fr)
>    3. Time series (trend over time) for irregular sampling dates
>       and multiple sites (=?UTF-8?Q?Catarina_Serra_Gon=C3=A7alves?=)
>    4. Re:  Time series (trend over time) for irregular sampling
>       dates and multiple sites (Bert Gunter)
>    5. Passing formula as parameter to `lm` within `sapply` causes
>       error [BUG?] (Jens Heumann)
>    6. (no subject) (Haddison Mureithi)
>    7. Help with loop for column means into new column by a subset
>       Factor w/131 levels (Bill Poling)
>    8. Re: Help with loop for column means into new column by a
>       subset Factor w/131 levels (Bill Poling)
>    9. transpose and split dataframe (Matthew)
>   10. Re: transpose and split dataframe (David L Carlson)
>   11. Re: Passing formula as parameter to `lm` within `sapply`
>       causes error [BUG?] (David Winsemius)
>   12. Fwd: Re:  transpose and split dataframe (Matthew)
>   13. Re: transpose and split dataframe (Jim Lemon)
>   14. Re:  Time series (trend over time) for irregular sampling
>       dates and multiple sites (Abs Spurdle)
>   15. Re: Fwd: Re:  transpose and split dataframe (David L Carlson)
>   16. Re: Passing formula as parameter to `lm` within `sapply`
>       causes error [BUG?] (Duncan Murdoch)
>   17. Re:  Time series (trend over time) for irregular sampling
>       dates and multiple sites (Abs Spurdle)
>   18. Re:  Time series (trend over time) for irregular sampling
>       dates and multiple sites (Abs Spurdle)
>   19. Re: Passing formula as parameter to `lm` within `sapply`
>       causes error [BUG?] (Jens Heumann)
>   20. Re: Passing formula as parameter to `lm` within `sapply`
>       causes error [BUG?] (peter dalgaard)
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Tue, 30 Apr 2019 16:54:10 +0200
> From: Martin Maechler <maechler at stat.math.ethz.ch>
> To: Morgan Morgan <morgan.emailbox at gmail.com>
> Cc: <r-help at r-project.org>
> Subject: Re: [R] Bug in R 3.6.0?
> Message-ID: <23752.24978.45927.96764 at stat.math.ethz.ch>
> Content-Type: text/plain; charset="utf-8"
>
> >>>>> Morgan Morgan
> >>>>>     on Mon, 29 Apr 2019 21:42:36 +0100 writes:
>
>     > Hi,
>     > I am using the R 3.6.0 on windows. The issue that I report below
> does not
>     > exist with previous version of R.
>     > In order to reproduce the error you must install a package of your
> choice
>     > from source (tar.gz).
>
>     > -Create a .Rprofile file with the following command in it :
> setwd("D:/")
>     > -Close your R session and re-open it. Your working directory must be
> now set
>     > to D:
>     > -Install a package of your choice from source, example :
>     > install.packages("data.table",type="source")
>
>     > In my case the package fail to install and I get the following error
>     > message:
>
>     > ** R
>     > ** inst
>     > ** byte-compile and prepare package for lazy loading
>     > Error in tools:::.read_description(file) :
>     > file 'DESCRIPTION' does not exist
>     > Calls: suppressPackageStartupMessages ... withCallingHandlers ->
>     > .getRequiredPackages -> <Anonymous> -> <Anonymous>
>     > Execution halted
>     > ERROR: lazy loading failed for package 'data.table'
>     > * removing 'C:/Users/Morgan/Documents/R/win-library/3.6/data.table'
>     > * restoring previous
>     > 'C:/Users/Morgan/Documents/R/win-library/3.6/data.table'
>     > Warning in install.packages :
>     > installation of package ?data.table? had non-zero exit status
>
>     > Now remove the .Rprofile file, restart your R session and try to
> install th
> e
>     > package with the same command.
>     > In that case everything should be installed just fine.
>
>     > FYI the issue happens on macOS as well and I suspect it also does on
> all
>     > linux systems.
>
>     > My question: Is this expected or is it a bug?
>
> It is a bug, thank you very much for reporting it.
>
> I've been told privately by ?mer An (thank you!) who's been
> affected as well, that this problem seems to affect others, and
> that there's a thread about this over at the Rstudio support site
>
>
> https://support.rstudio.com/hc/en-us/community/posts/200704708-Build-tool-does-not-recognize-DESCRIPTION-file
>
> There, users mention that (all?) packages are affected which
> have a multiline 'Description:' field in their DESCRIPTION file.
> Of course, many if not most packages have this property.
>
> Indeed, I can reproduce the problem (e.g. with my 'sfsmisc'
> package) if I ("silly enough to") add a setwd() call to my
> Rprofile file  (the one I set via env.var  R_PROFILE or R_PROFILE_USER).
>
> This is clearly a bug, and indeed a bad one.
>
> It seems all R core (and other R expert users who have tried R
> 3.6.0 alpha, beta, and RC versions) have *not* seen the bug as they
> are intuitively smart not to mess with R's working directory in
> a global R profile file ...
>
> For now you definitively have to work around by not doing what's
> the problem : do *NOT* setwd() in your  ~/.Rprofile or other
> such R init files.
>
> Best,
> Martin Maechler
> ETH Zurich and  R Core Team
>
>
>
>
> ------------------------------
>
> Message: 2
> Date: Tue, 30 Apr 2019 16:15:46 +0200
> From: <ocjt at free.fr>
> To: "'Morgan Morgan'" <morgan.emailbox at gmail.com>,
>         <r-help at r-project.org>
> Subject: Re: [R] Bug in R 3.6.0?
> Message-ID: <002d01d4ff5f$34816be0$9d8443a0$@free.fr>
> Content-Type: text/plain; charset="utf-8"
>
> Hello,
>
> I have exactly the same problem when I install one of my own packages:
>
> Error in tools:::.read_description(file) :
>   file 'DESCRIPTION' does not exist
> Calls: suppressPackageStartupMessages ... withCallingHandlers ->
> .getRequiredPackages -> <Anonymous> -> <Anonymous>
> Ex?cution arr?t?e
> ERROR: lazy loading failed for package 'RRegArch'
>
> Best,
> Ollivier
>
>
> -----Message d'origine-----
> De : R-help <r-help-bounces at r-project.org> De la part de Morgan Morgan
> Envoy? : lundi 29 avril 2019 22:43
> ? : r-help at r-project.org
> Objet : [R] Bug in R 3.6.0?
>
> Hi,
>
> I am using the R 3.6.0 on windows. The issue that I report below does not
> exist with previous version of R.
> In order to reproduce the error you must install a package of your choice
> from source (tar.gz).
>
> -Create a .Rprofile file with the following command in it : setwd("D:/")
> -Close your R session and re-open it. Your working directory must be now
> set to D:
> -Install a package of your choice from source, example :
> install.packages("data.table",type="source")
>
> In my case the package fail to install and I get the following error
> message:
>
> ** R
> ** inst
> ** byte-compile and prepare package for lazy loading Error in
> tools:::.read_description(file) :
>   file 'DESCRIPTION' does not exist
> Calls: suppressPackageStartupMessages ... withCallingHandlers ->
> .getRequiredPackages -> <Anonymous> -> <Anonymous> Execution halted
> ERROR: lazy loading failed for package 'data.table'
> * removing 'C:/Users/Morgan/Documents/R/win-library/3.6/data.table'
> * restoring previous
> 'C:/Users/Morgan/Documents/R/win-library/3.6/data.table'
> Warning in install.packages :
>   installation of package ?data.table? had non-zero exit status
>
> Now remove the .Rprofile file, restart your R session and try to install
> the package with the same command.
> In that case everything should be installed just fine.
>
> FYI the issue happens on macOS as well and I suspect it also does on all
> linux systems.
>
> My question: Is this expected or is it a bug?
>
> Thank you
> Best regards,
> Morgan
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
> ------------------------------
>
> Message: 3
> Date: Wed, 1 May 2019 00:57:43 +1000
> From: =?UTF-8?Q?Catarina_Serra_Gon=C3=A7alves?= <catarinasg at gmail.com>
> To: r-help at r-project.org
> Subject: [R] Time series (trend over time) for irregular sampling
>         dates and multiple sites
> Message-ID:
>         <
> CAOQWJbvY+JKy80sksmfC8tu-C+5qq-tzwAd21XbyGvJAyYjQPQ at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> I have a dataset of marine debris items (number of items standardized per
> effort: Items/(number of volunteers*Hours*Lenght)) taken from 2 main
> locations (WA and Queensland) in Australia (8 Sub Sites in total: 4 in WA
> and 4 in Queensland) at irregular sampling intervals over a period 15
> years.
>
> I want to test if there is a change over the years on the amount of debris
> in these locations and more specifically a change after the implementation
> of a mitigation strategy (in 2013).
> Here?s the head of the data:[image: enter image description here]
> <https://i.stack.imgur.com/VNIpb.png>Description of each one of the
> varables in the dataframe:
>
> *eventid *= each sampling (clean-up) event Location = Queensland and New
> South Wales Sites = all the 9 sampling beaches
>
> *Date *= specific dates for the clean-up events (day-month-year)
>
> *Date1 *= specific dates for the clean-up events (day-month-year) on the
> POSICXT format Year= Year of sampling event (2004 to 2018)
>
> *Month*= Month of the sampling event (jan to dec)
>
> *nMonth*= a number was determined to the respective month of the sampling
> event (1 to 12)
>
> *Day*= Day of sampling (1 to 31) Days = Days since the first date of clean
> up = just another way of using the dates
>
> *MARPOL *= before and after implementation (factor with 2 levels)
>
> *DaysC *= days between sampling events for the same sites = number of days
> since the previous clean-up event
>
> *DaysI *= Days since intervention, all the dates before implementation are
> zero, and after we count the number of days since the implementation date
> (1 jan 2013)
>
> *DaysIa*= same as DayI but instead of zero for before the intervention we
> have negative values (days)
>
> *Items *= number of fishing and shipping items counted in each clean-up
> event
>
> *Hours *= hours spent by all volunteers together at each clean up event
>
> *Lenght *= Lenght of beach sampled by all volunteers together at each clean
> up event volunteers = all volunteers at each clean up event
>
> *HoursVolunteer *= hours spent bt each volunteer at each clean up event
> (Hours/volunteers)
>
> *Ieffort *= the items standarized by the effort (hours, volunteers and
> lenght)
>
> *GrossWeight & **GrossTotal are not relevant *
> ------------------------------
> Problems:
>
> My data has a few problems: (1) I think I will need to fix the effects of
> seasonal variation (Monthly) and (2) of possible spatial correlation
> (probability of finding an item is higher after finding one since they can
> come from the same ship). (3) How do I handle the fact that the
> measurements were not taken at a regular interval?
>
> I was trying to use GAMs to analyse the data and see the trends over time.
> The model I came across is the following:
>
> m4<- gamm(Ieffort ~ s(DaysIa)+MARPOL+ s(nMonth, bs = "ps", k = 12),
> random=list(Site=~1,Location=~1),data = d)
>
> *thank you in advance.*
> -
> *Catarina Serra Gon?alves *
> PhD candidate
>
> Adrift Lab  <https://adriftlab.org>
> University of Tasmania <http://www.utas.edu.au/> | Institute for Marine
> and
> Antarctic Studies  <http://www.imas.utas.edu.au/>
> Launceston, TAS | Australia
>
> Personal website <https://catarinasg.wixsite.com/acserra>
> <https://catarinasg.wixsite.com/acserra>| E-mail  <acserra at utas.edu.au> |
> Twitter <https://twitter.com/CatarinaSerraG>
> Research Gate
> <https://www.researchgate.net/profile/Catarina_Serra_Goncalves> | Google
> Scholar <https://scholar.google.pt/citations?user=8nBrRFwAAAAJ&hl=en>
>
>         [[alternative HTML version deleted]]
>
>
>
>
> ------------------------------
>
> Message: 4
> Date: Tue, 30 Apr 2019 08:28:37 -0700
> From: Bert Gunter <bgunter.4567 at gmail.com>
> To: =?UTF-8?Q?Catarina_Serra_Gon=C3=A7alves?= <catarinasg at gmail.com>
> Cc: R-help <r-help at r-project.org>
> Subject: Re: [R]  Time series (trend over time) for irregular sampling
>         dates and multiple sites
> Message-ID:
>         <CAGxFJbT2YSB1xcs0MajpeqtHbbn4T1ycYoSOBEFvMucFme1t=
> g at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> I have 0 expertise, but I suggest that you check out the SPatioTemporal
> taskview on CRAN (or possibly others, like environmetrics). You might also
> want to move this to the R-Sig-geo list,where you probably are more likely
> to find relevant expertise.
>
> Cheers,
> Bert
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Tue, Apr 30, 2019 at 8:13 AM Catarina Serra Gon?alves <
> catarinasg at gmail.com> wrote:
>
> > I have a dataset of marine debris items (number of items standardized per
> > effort: Items/(number of volunteers*Hours*Lenght)) taken from 2 main
> > locations (WA and Queensland) in Australia (8 Sub Sites in total: 4 in WA
> > and 4 in Queensland) at irregular sampling intervals over a period 15
> > years.
> >
> > I want to test if there is a change over the years on the amount of
> debris
> > in these locations and more specifically a change after the
> implementation
> > of a mitigation strategy (in 2013).
> > Here?s the head of the data:[image: enter image description here]
> > <https://i.stack.imgur.com/VNIpb.png>Description of each one of the
> > varables in the dataframe:
> >
> > *eventid *= each sampling (clean-up) event Location = Queensland and New
> > South Wales Sites = all the 9 sampling beaches
> >
> > *Date *= specific dates for the clean-up events (day-month-year)
> >
> > *Date1 *= specific dates for the clean-up events (day-month-year) on the
> > POSICXT format Year= Year of sampling event (2004 to 2018)
> >
> > *Month*= Month of the sampling event (jan to dec)
> >
> > *nMonth*= a number was determined to the respective month of the sampling
> > event (1 to 12)
> >
> > *Day*= Day of sampling (1 to 31) Days = Days since the first date of
> clean
> > up = just another way of using the dates
> >
> > *MARPOL *= before and after implementation (factor with 2 levels)
> >
> > *DaysC *= days between sampling events for the same sites = number of
> days
> > since the previous clean-up event
> >
> > *DaysI *= Days since intervention, all the dates before implementation
> are
> > zero, and after we count the number of days since the implementation date
> > (1 jan 2013)
> >
> > *DaysIa*= same as DayI but instead of zero for before the intervention we
> > have negative values (days)
> >
> > *Items *= number of fishing and shipping items counted in each clean-up
> > event
> >
> > *Hours *= hours spent by all volunteers together at each clean up event
> >
> > *Lenght *= Lenght of beach sampled by all volunteers together at each
> clean
> > up event volunteers = all volunteers at each clean up event
> >
> > *HoursVolunteer *= hours spent bt each volunteer at each clean up event
> > (Hours/volunteers)
> >
> > *Ieffort *= the items standarized by the effort (hours, volunteers and
> > lenght)
> >
> > *GrossWeight & **GrossTotal are not relevant *
> > ------------------------------
> > Problems:
> >
> > My data has a few problems: (1) I think I will need to fix the effects of
> > seasonal variation (Monthly) and (2) of possible spatial correlation
> > (probability of finding an item is higher after finding one since they
> can
> > come from the same ship). (3) How do I handle the fact that the
> > measurements were not taken at a regular interval?
> >
> > I was trying to use GAMs to analyse the data and see the trends over
> time.
> > The model I came across is the following:
> >
> > m4<- gamm(Ieffort ~ s(DaysIa)+MARPOL+ s(nMonth, bs = "ps", k = 12),
> > random=list(Site=~1,Location=~1),data = d)
> >
> > *thank you in advance.*
> > -
> > *Catarina Serra Gon?alves *
> > PhD candidate
> >
> > Adrift Lab  <https://adriftlab.org>
> > University of Tasmania <http://www.utas.edu.au/> | Institute for Marine
> > and
> > Antarctic Studies  <http://www.imas.utas.edu.au/>
> > Launceston, TAS | Australia
> >
> > Personal website <https://catarinasg.wixsite.com/acserra>
> > <https://catarinasg.wixsite.com/acserra>| E-mail  <acserra at utas.edu.au>
> |
> > Twitter <https://twitter.com/CatarinaSerraG>
> > Research Gate
> > <https://www.researchgate.net/profile/Catarina_Serra_Goncalves> | Google
> > Scholar <https://scholar.google.pt/citations?user=8nBrRFwAAAAJ&hl=en>
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
>
>
>
> ------------------------------
>
> Message: 5
> Date: Tue, 30 Apr 2019 17:24:33 +0200
> From: Jens Heumann <jens.heumann at students.unibe.ch>
> To: <r-help at r-project.org>
> Subject: [R] Passing formula as parameter to `lm` within `sapply`
>         causes error [BUG?]
> Message-ID: <75abba2b-c528-460e-df92-08f8479ba399 at students.unibe.ch>
> Content-Type: text/plain; charset="utf-8"; Format="flowed"
>
> Hi,
>
> `lm` won't take formula as a parameter when it is within a `sapply`; see
> example below. Please, could anyone either point me to a syntax error or
> confirm that this might be a bug?
>
> Best,
> Jens
>
> [Disclaimer: This is my first post here, following advice of how to
> proceed with possible bugs from here: https://www.r-project.org/bugs.html]
>
>
> SUMMARY
>
> While `lm` alone accepts formula parameter `FO` well, the same within a
> `sapply` causes an error. When putting everything as parameter but
> formula `FO`, it's still working, though. All parameters work fine
> within a similar `for` loop.
>
>
> MCVE (see data / R-version at bottom)
>
>  > summary(lm(y ~ x, df1, df1[["z"]] == 1, df1[["w"]]))$coef[1, ]
>    Estimate Std. Error    t value   Pr(>|t|)
>   1.6269038  0.9042738  1.7991275  0.3229600
>  > summary(lm(FO, data, data[[st]] == st1, data[[ws]]))$coef[1, ]
>    Estimate Std. Error    t value   Pr(>|t|)
>   1.6269038  0.9042738  1.7991275  0.3229600
>  > sapply(unique(df1$z), function(s)
> +   summary(lm(y ~ x, df1, df1[["z"]] == s, df1[[ws]]))$coef[1, ])
>                  [,1]       [,2]         [,3]
> Estimate   1.6269038 -0.1404174 -0.010338774
> Std. Error 0.9042738  0.4577001  1.858138516
> t value    1.7991275 -0.3067890 -0.005564049
> Pr(>|t|)   0.3229600  0.8104951  0.996457853
>  > sapply(unique(data[[st]]), function(s)
> +   summary(lm(FO, data, data[[st]] == s, data[[ws]]))$coef[1, ])  # !!!
> Error in eval(substitute(subset), data, env) : object 's' not found
>  > sapply(unique(data[[st]]), function(s)
> +   summary(lm(y ~ x, data, data[[st]] == s, data[[ws]]))$coef[1, ])
>                  [,1]       [,2]         [,3]
> Estimate   1.6269038 -0.1404174 -0.010338774
> Std. Error 0.9042738  0.4577001  1.858138516
> t value    1.7991275 -0.3067890 -0.005564049
> Pr(>|t|)   0.3229600  0.8104951  0.996457853
>  > m <- matrix(NA, 4, length(unique(data[[st]])))
>  > for (s in unique(data[[st]])) {
> +   m[, s] <- summary(lm(FO, data, data[[st]] == s, data[[ws]]))$coef[1, ]
> + }
>  > m
>            [,1]       [,2]         [,3]
> [1,] 1.6269038 -0.1404174 -0.010338774
> [2,] 0.9042738  0.4577001  1.858138516
> [3,] 1.7991275 -0.3067890 -0.005564049
> [4,] 0.3229600  0.8104951  0.996457853
>
> # DATA #################################################################
>
> df1 <- structure(list(x = c(1.37095844714667, -0.564698171396089,
> 0.363128411337339,
> 0.63286260496104, 0.404268323140999, -0.106124516091484, 1.51152199743894,
> -0.0946590384130976, 2.01842371387704), y = c(1.30824434809425,
> 0.740171482827397, 2.64977380403845, -0.755998096151299, 0.125479556323628,
> -0.239445852485142, 2.14747239550901, -0.37891195982917, -0.638031707027734
> ), z = c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L), w = c(0.7, 0.8,
> 1.2, 0.9, 1.3, 1.2, 0.8, 1, 1)), class = "data.frame", row.names = c(NA,
> -9L))
>
> FO <- y ~ x; data <- df1; st <- "z"; ws <- "w"; st1 <- 1
>
> ########################################################################
>
>  > R.version
>                 _
> platform       x86_64-w64-mingw32
> arch           x86_64
> os             mingw32
> system         x86_64, mingw32
> status
> major          3
> minor          6.0
> year           2019
> month          04
> day            26
> svn rev        76424
> language       R
> version.string R version 3.6.0 (2019-04-26)
> nickname       Planting of a Tree
>
> #########################################################################
>
> NOTE: Question on SO two days ago
> (
> https://stackoverflow.com/questions/55893189/passing-formula-as-parameter-to-lm-within-sapply-causes-error-bug-confirmation)
>
> brought many views but neither answer nor bug confirmation.
>
>
>
>
> ------------------------------
>
> Message: 6
> Date: Mon, 29 Apr 2019 21:38:00 +0300
> From: Haddison Mureithi <mureithihaddison at gmail.com>
> To: r-help at r-project.org
> Subject: [R] (no subject)
> Message-ID:
>         <CABVwvn6y_M2M1o41HryKYp=
> LQcbsajdtginyw_RPVf81o4BmqQ at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> Hello guys this problem was never answered and I happened to come across
> the same problem , kindly help. This is a simple R program that I have been
> trying to run. I keep running into the "singular matrix" error. I end up
> with no sensible results. Can anyone suggest any changes or a way around
> this?
>
> I am a total rookie when working with R.
>
> Thanks,
> Rasika
>
> > library(survival)
> Loading required package: splines
> > args(coxph)
> function (formula, data, weights, subset, na.action, init, control,
>     method = c("efron", "breslow", "exact"), singular.ok = TRUE,
>     robust = FALSE, model = FALSE, x = FALSE, y = TRUE, tt, ...)
> NULL
> > test1<-read.table("S:/FISHDO/03_Phase_I_Field_Work/Data_6_28_2011/Working
> Folder/R_files/4SondesJuly24.csv", header=T, sep=",")
> > sondes<-coxph(Surv(Start, Stop, Depart)~DOLoomis + DOI55 + DODamen,
> data=test1)
> Warning messages:
> 1: In fitter(X, Y, strats, offset, init, control, weights = weights,  :
>   Loglik converged before variable  1,2 ; beta may be infinite.
> 2: In coxph(Surv(Start, Stop, Depart) ~ DOLoomis + DOI55 + DODamen,  :
>   X matrix deemed to be singular; variable 3
> > summary(sondes)
> Call:
> coxph(formula = Surv(Start, Stop, Depart) ~ DOLoomis + DOI55 +
>     DODamen, data = test1)
>
>   n= 1737, number of events= 58
>    (1 observation deleted due to missingness)
>
>                coef  exp(coef)   se(coef)  z Pr(>|z|)
> DOLoomis -2.152e+00  1.163e-01  1.161e+05  0        1
> DOI55     4.560e-01  1.578e+00  3.755e+04  0        1
> DODamen          NA         NA  0.000e+00 NA       NA
>
>          exp(coef) exp(-coef) lower .95 upper .95
> DOLoomis    0.1163     8.5995         0       Inf
> DOI55       1.5777     0.6338         0       Inf
> DODamen         NA         NA        NA        NA
>
> Concordance= 0.5  (se = 0 )
> Rsquare= 0   (max possible= 0.01 )
> Likelihood ratio test= 0  on 2 df,   p=1
> Wald test            = 0  on 2 df,   p=1
> Score (logrank) test = 0  on 2 df,   p=1
>
>         [[alternative HTML version deleted]]
>
>
>
>
> ------------------------------
>
> Message: 7
> Date: Tue, 30 Apr 2019 16:50:48 +0000
> From: Bill Poling <Bill.Poling at zelis.com>
> To: "r-help (r-help at r-project.org)" <r-help at r-project.org>
> Subject: [R] Help with loop for column means into new column by a
>         subset Factor w/131 levels
> Message-ID:
>         <
> BN7PR02MB50737455E93F882B58EAA4F4EA3A0 at BN7PR02MB5073.namprd02.prod.outlook.com
> >
>
> Content-Type: text/plain; charset="windows-1252"
>
> Good afternoon.
>
> #RStudio Version 1.1.456
> sessionInfo()
> #R version 3.5.3 (2019-03-11)
> #Platform: x86_64-w64-mingw32/x64 (64-bit)
> #Running under: Windows >= 8 x64 (build 9200)
>
>
>
> #I have a DF of 8 columns and 14025 rows
>
> str(hcd2tmp2)
>
> # 'data.frame':14025 obs. of  8 variables:
> # $ Submitted_Charge: num  21021 15360 40561 29495 7904 ...
> # $ Allowed_Amt     : num  18393 6254 40561 29495 7904 ...
> # $ Submitted_Units : num  60 240 420 45 120 215 215 15 57 2 ...
> # $ Procedure_Code1 : Factor w/ 131 levels "A9606","J0129",..: 43 113 117
> 125 24 85 85 90 86 25 ...
> # $ AllowByLimit    : num  4.268 0.949 7.913 6.124 3.524 ...
> # $ UnitsByDose     : num  600 240 420 450 120 215 215 750 570 500 ...
> # $ LimitByUnits    : num  4310 6591 5126 4816 2243 ...
> # $ HCPCSCodeDose1  : num  10 1 1 10 1 1 1 50 10 250 ...
>
> #I would like to create four additional columns that are the mean of four
> current columns in the DF.
> #Current columns
> #Allowed_Amt
> #LimitByUnits
> #AllowByLimit
> #UnitsByDose
>
> #The goal is to be able to identify rows where (for instance) Allowed_Amt
> is greater than the average (aka outliers).
>
> #The trick Is I want the means of those columns based on a Factor value
> #The Factor is:
> #Procedure_Code1 : Factor w/ 131 levels "A9606","J0129"
>
> #So each of my four new columns will have 131 distinct values based on the
> mean for the specific Procedure_Code1 grouping
>
> #In SQL it would look something like this:
>
> #SELECT *,
> # NewCol1 = mean(Allowed_Amt) OVER (PARTITION BY Procedure_Code1),
> # NewCol2 = mean(LimitByUnits) OVER (PARTITION BY Procedure_Code1),
> # NewCol3 = mean(AllowByLimit) OVER (PARTITION BY Procedure_Code1),
> # NewCol4 = mean(UnitsByDose) OVER (PARTITION BY Procedure_Code1)
> #INTO NewTable
> #FROM Oldtable
>
> #Here are some sample data
>
> head(hcd2tmp2, n=40)
> #      Submitted_Charge Allowed_Amt Submitted_Units Procedure_Code1
> AllowByLimit UnitsByDose LimitByUnits HCPCSCodeDose1
> # 1          21020.70    18393.12              60           J1745
> 4.2679810         600      4309.56             10
> # 2          15360.00     6254.40             240           J9299
> 0.9488785         240      6591.36              1
> # 3          40561.32    40561.32             420           J9306
> 7.9133539         420      5125.68              1
> # 4          29495.25    29495.25              45           J9355
> 6.1244417         450      4815.99             10
> # 5           7904.30     7904.30             120           J0897
> 3.5243000         120      2242.80              1
> # 6          15331.95    10614.31             215           J9034
> 2.0586686         215      5155.91              1
> # 7          15331.95    10614.31             215           J9034
> 2.0586686         215      5155.91              1
> # 8            461.90        0.00              15           J9045
> 0.0000000         750        46.38             50
> # 9          27340.96    15092.21              57           J9035
> 3.2600227         570      4629.48             10
> # 10           768.00      576.00               2           J1190
> 1.3617343         500       422.99            250
> # 11           101.00       38.38               5           J2250
>  59.9687500           5         0.64              1
> # 12         17458.40        0.00             200           J9033
> 0.0000000         200      5990.00              1
> # 13          7885.10     7569.70               1           J1745
> 105.3835445          10        71.83             10
> # 14          2015.00     1155.78               4           J2785
> 5.0051100           0       230.92              0
> # 15           443.72      443.72              12           J9045
>  11.9601078         600        37.10             50
> # 16        113750.00   113750.00             600           J2350
> 3.3025003         600     34443.60              1
> # 17          3582.85     3582.85              10           J2469
>  30.5573561         250       117.25             25
> # 18          5152.65     5152.65              50           J2796
> 1.4362988         500      3587.45             10
> # 19          5152.65     5152.65              50           J2796
> 1.4362988         500      3587.45             10
> # 20         39664.09        0.00              74           J9355
> 0.0000000         740      7919.63             10
> # 21           166.71      102.53               9           J9045
> 3.6841538         450        27.83             50
> # 22         13823.61     9676.53               1           J2505
> 2.0785247           6      4655.48              6
> # 23         90954.00    26436.53             360           J1786
> 1.7443775        3600     15155.28             10
> # 24          4800.00     3494.40             800           J3262
> 0.8861838         800      3943.20              1
> # 25           216.00      105.84               4           J0696
>  42.3360000        1000         2.50            250
> # 26          5300.00     4770.00               1           J0178
> 4.9677151           1       960.20              1
> # 27         35203.00    35203.00             200           J9271
> 3.5772498         200      9840.80              1
> # 28         17589.15    17589.15             300           J3380
> 2.9696855         300      5922.90              1
> # 29         18394.64    17842.79               1           J9355
> 166.7238834          10       107.02             10
> # 30           770.00      731.50              10           J2469
> 6.2388060         250       117.25             25
> # 31           461.90        0.00              15           J9045
> 0.0000000         750        46.38             50
> # 32          8160.00     3342.40              80           J1459
> 1.0260818       40000      3257.44            500
> # 33          1653.48      314.16               6           J9305
> 0.7661505          60       410.05             10
> # 34         13036.50        0.00             194           J9034
> 0.0000000         194      4652.31              1
> # 35         10486.87        0.00             156           J9034
> 0.0000000         156      3741.04              1
> # 36         15360.00     6254.40             240           J9299
> 0.9488785         240      6591.36              1
> # 37          1616.83     1616.83             150           J1453
> 5.2528590         150       307.80              1
> # 38         80685.74    34772.43              96           J9035
> 4.4597077         960      7797.02             10
> # 39         85220.58    35925.13             287           J9299
> 4.5577715         287      7882.17              1
> # 40          3860.17     1627.27              13           J9299
> 4.5577963          13       357.03              1
>
>
> #I hope this is enough inforamtion to warrant your support
> #Thank you
> #WHP
>
>
>
> Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}
>
>
>
>
> ------------------------------
>
> Message: 8
> Date: Tue, 30 Apr 2019 18:45:40 +0000
> From: Bill Poling <Bill.Poling at zelis.com>
> To: "r-help (r-help at r-project.org)" <r-help at r-project.org>
> Subject: Re: [R] Help with loop for column means into new column by a
>         subset Factor w/131 levels
> Message-ID:
>         <
> BN7PR02MB5073D732498AB265872F5750EA3A0 at BN7PR02MB5073.namprd02.prod.outlook.com
> >
>
> Content-Type: text/plain; charset="windows-1252"
>
> I ran this routine but I was thinking there must be a more elegant way of
> doing this.
>
>
> #
> https://community.rstudio.com/t/how-to-average-mean-variables-in-r-based-on-the-level-of-another-variable-and-save-this-as-a-new-variable/8764/8
>
> hcd2tmp2_summmary <- hcd2tmp2 %>%
>   select(.) %>%
>   group_by(Procedure_Code1) %>%
>   summarize(average = mean(Allowed_Amt))
> # A tibble: 131 x 2
> # Procedure_Code1 average
> # <fct>             <dbl>
> # 1 A9606            57785.
> # 2 J0129             5420.
> # 3 J0178             4700.
> # 4 J0180            13392.
> # 5 J0202            56328.
> # 6 J0256            17366.
> # 7 J0257             7563.
> # 8 J0485             2450.
> # 9 J0490             6398.
> # 10 J0585            4492.
> # ... with 121 more rows
>
> hcd2tmp2 <- hcd2tmp %>%
>   group_by(Procedure_Code1) %>%
>   summarise(Avg_Allowed_Amt = mean(Allowed_Amt))
>
> view(hcd2tmp2)
>
>
> hcd2tmp3 <- hcd2tmp %>%
>   group_by(Procedure_Code1) %>%
>   summarise(Avg_AllowByLimit = mean(AllowByLimit))
>
> view(hcd2tmp3)
>
>
> hcd2tmp4 <- hcd2tmp %>%
>   group_by(Procedure_Code1) %>%
>   summarise(Avg_UnitsByDose = mean(UnitsByDose))
>
> view(hcd2tmp4)
>
> hcd2tmp5 <- hcd2tmp %>%
>   group_by(Procedure_Code1) %>%
>   summarise(Avg_LimitByUnits = mean(LimitByUnits))
>
> view(hcd2tmp5)
>
> #Joins----
>
>
> hcd2tmp <- left_join(hcd2tmp2, hcd2tmp, by =
> c("Procedure_Code1"="Procedure_Code1"))
> hcd2tmp <- left_join(hcd2tmp3, hcd2tmp, by =
> c("Procedure_Code1"="Procedure_Code1"))
> hcd2tmp <- left_join(hcd2tmp4, hcd2tmp, by =
> c("Procedure_Code1"="Procedure_Code1"))
> hcd2tmp <- left_join(hcd2tmp5, hcd2tmp, by =
> c("Procedure_Code1"="Procedure_Code1"))
>
> view(hcd2tmp)
>
> hcd2tmp$Avg_LimitByUnits <- round(hcd2tmp$Avg_LimitByUnits, digits = 2)
> hcd2tmp$Avg_Allowed_Amt <- round(hcd2tmp$Avg_Allowed_Amt, digits = 2)
> hcd2tmp$Avg_AllowByLimit <- round(hcd2tmp$Avg_AllowByLimit, digits = 2)
> hcd2tmp$Avg_UnitsByDose <- round(hcd2tmp$Avg_UnitsByDose, digits = 2)
>
> view(hcd2tmp)
>
> #Over under columns----
> hcd2tmp$AllowByLimitFlag <- hcd2tmp$AllowByLimit > hcd2tmp$Avg_AllowByLimit
> hcd2tmp$LimitByUnitsFlag <- hcd2tmp$LimitByUnits > hcd2tmp$Avg_LimitByUnits
> hcd2tmp$Allowed_AmtFlag  <- hcd2tmp$Allowed_Amt  > hcd2tmp$Avg_Allowed_Amt
> hcd2tmp$UnitsByDoseFlag  <- hcd2tmp$UnitsByDose  > hcd2tmp$Avg_UnitsByDose
>
> view(hcd2tmp)
>
>
> -----Original Message-----
> From: Bill Poling
> Sent: Tuesday, April 30, 2019 12:51 PM
> To: r-help (r-help at r-project.org) <r-help at r-project.org>
> Cc: Bill Poling <Bill.Poling at zelis.com>
> Subject: Help with loop for column means into new column by a subset
> Factor w/131 levels
>
> Good afternoon.
>
> #RStudio Version 1.1.456
> sessionInfo()
> #R version 3.5.3 (2019-03-11)
> #Platform: x86_64-w64-mingw32/x64 (64-bit) #Running under: Windows >= 8
> x64 (build 9200)
>
>
>
> #I have a DF of 8 columns and 14025 rows
>
> str(hcd2tmp2)
>
> # 'data.frame':14025 obs. of  8 variables:
> # $ Submitted_Charge: num  21021 15360 40561 29495 7904 ...
> # $ Allowed_Amt     : num  18393 6254 40561 29495 7904 ...
> # $ Submitted_Units : num  60 240 420 45 120 215 215 15 57 2 ...
> # $ Procedure_Code1 : Factor w/ 131 levels "A9606","J0129",..: 43 113 117
> 125 24 85 85 90 86 25 ...
> # $ AllowByLimit    : num  4.268 0.949 7.913 6.124 3.524 ...
> # $ UnitsByDose     : num  600 240 420 450 120 215 215 750 570 500 ...
> # $ LimitByUnits    : num  4310 6591 5126 4816 2243 ...
> # $ HCPCSCodeDose1  : num  10 1 1 10 1 1 1 50 10 250 ...
>
> #I would like to create four additional columns that are the mean of four
> current columns in the DF.
> #Current columns
> #Allowed_Amt
> #LimitByUnits
> #AllowByLimit
> #UnitsByDose
>
> #The goal is to be able to identify rows where (for instance) Allowed_Amt
> is greater than the average (aka outliers).
>
> #The trick Is I want the means of those columns based on a Factor value
> #The Factor is:
> #Procedure_Code1 : Factor w/ 131 levels "A9606","J0129"
>
> #So each of my four new columns will have 131 distinct values based on the
> mean for the specific Procedure_Code1 grouping
>
> #In SQL it would look something like this:
>
> #SELECT *,
> # NewCol1 = mean(Allowed_Amt) OVER (PARTITION BY Procedure_Code1),
> # NewCol2 = mean(LimitByUnits) OVER (PARTITION BY Procedure_Code1),
> # NewCol3 = mean(AllowByLimit) OVER (PARTITION BY Procedure_Code1),
> # NewCol4 = mean(UnitsByDose) OVER (PARTITION BY Procedure_Code1)
> #INTO NewTable
> #FROM Oldtable
>
> #Here are some sample data
>
> head(hcd2tmp2, n=40)
> #      Submitted_Charge Allowed_Amt Submitted_Units Procedure_Code1
> AllowByLimit UnitsByDose LimitByUnits HCPCSCodeDose1
> # 1          21020.70    18393.12              60           J1745
> 4.2679810         600      4309.56             10
> # 2          15360.00     6254.40             240           J9299
> 0.9488785         240      6591.36              1
> # 3          40561.32    40561.32             420           J9306
> 7.9133539         420      5125.68              1
> # 4          29495.25    29495.25              45           J9355
> 6.1244417         450      4815.99             10
> # 5           7904.30     7904.30             120           J0897
> 3.5243000         120      2242.80              1
> # 6          15331.95    10614.31             215           J9034
> 2.0586686         215      5155.91              1
> # 7          15331.95    10614.31             215           J9034
> 2.0586686         215      5155.91              1
> # 8            461.90        0.00              15           J9045
> 0.0000000         750        46.38             50
> # 9          27340.96    15092.21              57           J9035
> 3.2600227         570      4629.48             10
> # 10           768.00      576.00               2           J1190
> 1.3617343         500       422.99            250
> # 11           101.00       38.38               5           J2250
>  59.9687500           5         0.64              1
> # 12         17458.40        0.00             200           J9033
> 0.0000000         200      5990.00              1
> # 13          7885.10     7569.70               1           J1745
> 105.3835445          10        71.83             10
> # 14          2015.00     1155.78               4           J2785
> 5.0051100           0       230.92              0
> # 15           443.72      443.72              12           J9045
>  11.9601078         600        37.10             50
> # 16        113750.00   113750.00             600           J2350
> 3.3025003         600     34443.60              1
> # 17          3582.85     3582.85              10           J2469
>  30.5573561         250       117.25             25
> # 18          5152.65     5152.65              50           J2796
> 1.4362988         500      3587.45             10
> # 19          5152.65     5152.65              50           J2796
> 1.4362988         500      3587.45             10
> # 20         39664.09        0.00              74           J9355
> 0.0000000         740      7919.63             10
> # 21           166.71      102.53               9           J9045
> 3.6841538         450        27.83             50
> # 22         13823.61     9676.53               1           J2505
> 2.0785247           6      4655.48              6
> # 23         90954.00    26436.53             360           J1786
> 1.7443775        3600     15155.28             10
> # 24          4800.00     3494.40             800           J3262
> 0.8861838         800      3943.20              1
> # 25           216.00      105.84               4           J0696
>  42.3360000        1000         2.50            250
> # 26          5300.00     4770.00               1           J0178
> 4.9677151           1       960.20              1
> # 27         35203.00    35203.00             200           J9271
> 3.5772498         200      9840.80              1
> # 28         17589.15    17589.15             300           J3380
> 2.9696855         300      5922.90              1
> # 29         18394.64    17842.79               1           J9355
> 166.7238834          10       107.02             10
> # 30           770.00      731.50              10           J2469
> 6.2388060         250       117.25             25
> # 31           461.90        0.00              15           J9045
> 0.0000000         750        46.38             50
> # 32          8160.00     3342.40              80           J1459
> 1.0260818       40000      3257.44            500
> # 33          1653.48      314.16               6           J9305
> 0.7661505          60       410.05             10
> # 34         13036.50        0.00             194           J9034
> 0.0000000         194      4652.31              1
> # 35         10486.87        0.00             156           J9034
> 0.0000000         156      3741.04              1
> # 36         15360.00     6254.40             240           J9299
> 0.9488785         240      6591.36              1
> # 37          1616.83     1616.83             150           J1453
> 5.2528590         150       307.80              1
> # 38         80685.74    34772.43              96           J9035
> 4.4597077         960      7797.02             10
> # 39         85220.58    35925.13             287           J9299
> 4.5577715         287      7882.17              1
> # 40          3860.17     1627.27              13           J9299
> 4.5577963          13       357.03              1
>
>
> #I hope this is enough inforamtion to warrant your support
> #Thank you
> #WHP
>
>
>
> Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}
>
>
>
>
> ------------------------------
>
> Message: 9
> Date: Tue, 30 Apr 2019 15:24:57 -0400
> From: Matthew <mccormack at molbio.mgh.harvard.edu>
> To: "r-help (r-help at r-project.org)" <r-help at r-project.org>
> Subject: [R] transpose and split dataframe
> Message-ID:
>         <0d6ac524-4291-ab03-6bcb-592b3996cc74 at molbio.mgh.harvard.edu>
> Content-Type: text/plain; charset="utf-8"; Format="flowed"
>
> I have a data frame that is a lot bigger but for simplicity sake we can
> say it looks like this:
>
> Regulator    hits
> AT1G69490    AT4G31950,AT5G24110,AT1G26380,AT1G05675
> AT2G55980    AT2G85403,AT4G89223
>
>     In other words:
>
> data.frame : 2 obs. of 2 variables
> $Regulator: Factor w/ 2 levels
> $hits         : Factor w/ 6 levels
>
>    I want to transpose it so that Regulator is now the column headings
> and each of the AGI numbers now separated by commas is a row. So,
> AT1G69490 is now the header of the first column and AT4G31950 is row 1
> of column 1, AT5G24110 is row 2 of column 1, etc. AT2G55980 is header of
> column 2 and AT2G85403 is row 1 of column 2, etc.
>
>    I have tried playing around with strsplit(TF2list[2:2]) and
> strsplit(as.character(TF2list[2:2]), but I am getting nowhere.
>
> Matthew
>
>
>
>
> ------------------------------
>
> Message: 10
> Date: Tue, 30 Apr 2019 21:04:50 +0000
> From: David L Carlson <dcarlson at tamu.edu>
> To: "r-help at r-project.org" <r-help at r-project.org>, Matthew
>         <mccormack at molbio.mgh.harvard.edu>
> Subject: Re: [R] transpose and split dataframe
> Message-ID: <db8cede89a724defb691cea72a25b092 at tamu.edu>
> Content-Type: text/plain; charset="utf-8"
>
> I neglected to copy this to the list:
>
> I think we need more information. Can you give us the structure of the
> data with str(YourDataFrame). Alternatively you could copy a small piece
> into your email message by copying and pasting the results of the following
> code:
>
> dput(head(YourDataFrame))
>
> The data frame you present could not be a data frame since you say "hits"
> is a factor with a variable number of elements. If each value of "hits" was
> a single character string, it would only have 2 factor levels not 6 and
> your efforts to parse the string would make more sense. Transposing to a
> data frame would only be possible if each column was padded with NAs to
> make them equal in length. Since your example tries use the name TF2list,
> it is possible that you do not have a data frame but a list and you have no
> factor levels, just character vectors.
>
> If you are not familiar with R, it may be helpful to tell us what your
> overall goal is rather than an intermediate step. Very likely R can easily
> handle what you want by doing things a different way.
>
> ----------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77843-4352
>
>
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Matthew
> Sent: Tuesday, April 30, 2019 2:25 PM
> To: r-help (r-help at r-project.org) <r-help at r-project.org>
> Subject: [R] transpose and split dataframe
>
> I have a data frame that is a lot bigger but for simplicity sake we can
> say it looks like this:
>
> Regulator    hits
> AT1G69490    AT4G31950,AT5G24110,AT1G26380,AT1G05675
> AT2G55980    AT2G85403,AT4G89223
>
>     In other words:
>
> data.frame : 2 obs. of 2 variables
> $Regulator: Factor w/ 2 levels
> $hits         : Factor w/ 6 levels
>
>    I want to transpose it so that Regulator is now the column headings
> and each of the AGI numbers now separated by commas is a row. So,
> AT1G69490 is now the header of the first column and AT4G31950 is row 1
> of column 1, AT5G24110 is row 2 of column 1, etc. AT2G55980 is header of
> column 2 and AT2G85403 is row 1 of column 2, etc.
>
>    I have tried playing around with strsplit(TF2list[2:2]) and
> strsplit(as.character(TF2list[2:2]), but I am getting nowhere.
>
> Matthew
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
> ------------------------------
>
> Message: 11
> Date: Tue, 30 Apr 2019 15:03:09 -0600
> From: David Winsemius <dwinsemius at comcast.net>
> To: Jens Heumann <jens.heumann at students.unibe.ch>
> Cc: r-help at r-project.org
> Subject: Re: [R] Passing formula as parameter to `lm` within `sapply`
>         causes error [BUG?]
> Message-ID: <924255D4-912E-4C24-8E85-6E313EC50203 at comcast.net>
> Content-Type: text/plain; charset="utf-8"
>
> Try using do.call
>
> ?
> David
>
> Sent from my iPhone
>
> > On Apr 30, 2019, at 9:24 AM, Jens Heumann <
> jens.heumann at students.unibe.ch> wrote:
> >
> > Hi,
> >
> > `lm` won't take formula as a parameter when it is within a `sapply`; see
> example below. Please, could anyone either point me to a syntax error or
> confirm that this might be a bug?
> >
> > Best,
> > Jens
> >
> > [Disclaimer: This is my first post here, following advice of how to
> proceed with possible bugs from here: https://www.r-project.org/bugs.html]
> >
> >
> > SUMMARY
> >
> > While `lm` alone accepts formula parameter `FO` well, the same within a
> `sapply` causes an error. When putting everything as parameter but formula
> `FO`, it's still working, though. All parameters work fine within a similar
> `for` loop.
> >
> >
> > MCVE (see data / R-version at bottom)
> >
> > > summary(lm(y ~ x, df1, df1[["z"]] == 1, df1[["w"]]))$coef[1, ]
> >  Estimate Std. Error    t value   Pr(>|t|)
> > 1.6269038  0.9042738  1.7991275  0.3229600
> > > summary(lm(FO, data, data[[st]] == st1, data[[ws]]))$coef[1, ]
> >  Estimate Std. Error    t value   Pr(>|t|)
> > 1.6269038  0.9042738  1.7991275  0.3229600
> > > sapply(unique(df1$z), function(s)
> > +   summary(lm(y ~ x, df1, df1[["z"]] == s, df1[[ws]]))$coef[1, ])
> >                [,1]       [,2]         [,3]
> > Estimate   1.6269038 -0.1404174 -0.010338774
> > Std. Error 0.9042738  0.4577001  1.858138516
> > t value    1.7991275 -0.3067890 -0.005564049
> > Pr(>|t|)   0.3229600  0.8104951  0.996457853
> > > sapply(unique(data[[st]]), function(s)
> > +   summary(lm(FO, data, data[[st]] == s, data[[ws]]))$coef[1, ])  # !!!
> > Error in eval(substitute(subset), data, env) : object 's' not found
> > > sapply(unique(data[[st]]), function(s)
> > +   summary(lm(y ~ x, data, data[[st]] == s, data[[ws]]))$coef[1, ])
> >                [,1]       [,2]         [,3]
> > Estimate   1.6269038 -0.1404174 -0.010338774
> > Std. Error 0.9042738  0.4577001  1.858138516
> > t value    1.7991275 -0.3067890 -0.005564049
> > Pr(>|t|)   0.3229600  0.8104951  0.996457853
> > > m <- matrix(NA, 4, length(unique(data[[st]])))
> > > for (s in unique(data[[st]])) {
> > +   m[, s] <- summary(lm(FO, data, data[[st]] == s, data[[ws]]))$coef[1,
> ]
> > + }
> > > m
> >          [,1]       [,2]         [,3]
> > [1,] 1.6269038 -0.1404174 -0.010338774
> > [2,] 0.9042738  0.4577001  1.858138516
> > [3,] 1.7991275 -0.3067890 -0.005564049
> > [4,] 0.3229600  0.8104951  0.996457853
> >
> > # DATA #################################################################
> >
> > df1 <- structure(list(x = c(1.37095844714667, -0.564698171396089,
> 0.363128411337339,
> > 0.63286260496104, 0.404268323140999, -0.106124516091484,
> 1.51152199743894,
> > -0.0946590384130976, 2.01842371387704), y = c(1.30824434809425,
> > 0.740171482827397, 2.64977380403845, -0.755998096151299,
> 0.125479556323628,
> > -0.239445852485142, 2.14747239550901, -0.37891195982917,
> -0.638031707027734
> > ), z = c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L), w = c(0.7, 0.8,
> > 1.2, 0.9, 1.3, 1.2, 0.8, 1, 1)), class = "data.frame", row.names = c(NA,
> > -9L))
> >
> > FO <- y ~ x; data <- df1; st <- "z"; ws <- "w"; st1 <- 1
> >
> > ########################################################################
> >
> > > R.version
> >               _
> > platform       x86_64-w64-mingw32
> > arch           x86_64
> > os             mingw32
> > system         x86_64, mingw32
> > status
> > major          3
> > minor          6.0
> > year           2019
> > month          04
> > day            26
> > svn rev        76424
> > language       R
> > version.string R version 3.6.0 (2019-04-26)
> > nickname       Planting of a Tree
> >
> > #########################################################################
> >
> > NOTE: Question on SO two days ago (
> https://stackoverflow.com/questions/55893189/passing-formula-as-parameter-to-lm-within-sapply-causes-error-bug-confirmation)
> brought many views but neither answer nor bug confirmation.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
> ------------------------------
>
> Message: 12
> Date: Tue, 30 Apr 2019 17:31:28 -0400
> From: Matthew <mccormack at molbio.mgh.harvard.edu>
> To: "r-help at r-project.org" <r-help at r-project.org>
> Subject: [R] Fwd: Re:  transpose and split dataframe
> Message-ID:
>         <e4a9e321-b437-eed6-344b-472319e85fec at molbio.mgh.harvard.edu>
> Content-Type: text/plain; charset="utf-8"
>
> Thanks for your reply. I was trying to simplify it a little, but must
> have got it wrong. Here is the real dataframe, TF2list:
>
>   str(TF2list)
> 'data.frame':    152 obs. of  2 variables:
>   $ Regulator: Factor w/ 87 levels "AT1G02065","AT1G13960",..: 17 6 6 54
> 54 82 82 82 82 82 ...
>   $ hits     : Factor w/ 97 levels
> "AT1G05675,AT3G12910,AT1G22810,AT1G14540,AT1G21120,AT1G07160,AT5G22520,AT1G56250,AT2G31345,AT5G22530,AT4G11170,A"|
>
> __truncated__,..: 65 57 90 57 87 57 56 91 31 17 ...
>
>     And the first few lines resulting from dput(head(TF2list)):
>
> dput(head(TF2list))
> structure(list(Regulator = structure(c(17L, 6L, 6L, 54L, 54L,
> 82L), .Label = c("AT1G02065", "AT1G13960", "AT1G18860", "AT1G23380",
> "AT1G29280", "AT1G29860", "AT1G30650", "AT1G55600", "AT1G62300",
> "AT1G62990", "AT1G64000", "AT1G66550", "AT1G66560", "AT1G66600",
> "AT1G68150", "AT1G69310", "AT1G69490", "AT1G69810", "AT1G70510", ...
>
> This is another way of looking at the first 4 entries (Regulator is
> tab-separated from hits):
>
> Regulator
>    hits
> 1
> AT1G69490
>
>   AT4G31950,AT5G24110,AT1G26380,AT1G05675,AT3G12910,AT5G64905,AT1G22810,AT1G79680,AT3G02840,AT5G25260,AT5G57220,AT2G37430,AT2G26560,AT1G56250,AT3G23230,AT1G16420,AT1G78410,AT4G22030,AT5G05300,AT1G69930,AT4G03460,AT4G11470,AT5G25250,AT5G36925,AT2G30750,AT1G16150,AT1G02930,AT2G19190,AT4G11890,AT1G72520,AT4G31940,AT5G37490,AT5G52760,AT5G66020,AT3G57460,AT4G23220,AT3G15518,AT2G43620,AT2G02010,AT1G35210,AT5G46295,AT1G17147,AT1G11925,AT2G39200,AT1G02920,AT2G40180,AT1G59865,AT4G35180,AT4G15417,AT1G51820,AT1G06135,AT1G36622,AT5G42830
> 2
> AT1G29860
>
>   AT4G31950,AT5G24110,AT1G05675,AT3G12910,AT5G64905,AT1G22810,AT1G14540,AT1G79680,AT1G07160,AT3G23250,AT5G25260,AT1G53625,AT5G57220,AT2G37430,AT3G54150,AT1G56250,AT3G23230,AT1G16420,AT1G78410,AT4G22030,AT1G69930,AT4G03460,AT4G11470,AT5G25250,AT5G36925,AT4G14450,AT2G30750,AT1G16150,AT1G02930,AT2G19190,AT4G11890,AT1G72520,AT4G31940,AT5G37490,AT4G08555,AT5G66020,AT5G26920,AT3G57460,AT4G23220,AT3G15518,AT2G43620,AT1G35210,AT5G46295,AT1G17147,AT1G11925,AT2G39200,AT1G02920,AT4G35180,AT4G15417,AT1G51820,AT4G40020,AT1G06135
>
> 3
> AT1G2986
>
>   AT5G64905,AT1G21120,AT1G07160,AT5G25260,AT1G53625,AT1G56250,AT2G31345,AT4G11170,AT1G66090,AT1G26410,AT3G55840,AT1G69930,AT4G03460,AT5G25250,AT5G36925,AT1G26420,AT5G42380,AT1G16150,AT2G22880,AT1G02930,AT4G11890,AT1G72520,AT5G66020,AT2G43620,AT2G44370,AT4G15975,AT1G35210,AT5G46295,AT1G11925,AT2G39200,AT1G02920,AT4G14370,AT4G35180,AT4G15417,AT2G18690,AT5G11140,AT1G06135,AT5G42830
>
>     So, the goal would be to
>
> first: Transpose the existing dataframe so that the factor Regulator
> becomes a column name (column 1 name = AT1G69490, column2 name
> AT1G29860, etc.) and the hits associated with each Regulator become
> rows. Hits is a comma separated 'list' ( I do not not know if
> technically it is an R list.), so it would have to be comma
> 'unseparated' with each entry becoming a row (col 1 row 1 = AT4G31950,
> col 1 row 2 - AT5G24410, etc); like this :
>
> AT1G69490
> AT4G31950
> AT5G24110
> AT1G05675
> AT5G64905
>
> ... I did not include all the rows)
>
> I think it would be best to actually make the first entry a separate
> dataframe ( 1 column with name = AT1G69490 and number of rows depending
> on the number of hits), then make the second column (column name =
> AT1G29860, and number of rows depending on the number of hits) into a
> new dataframe and do a full join of of the two dataframes; continue by
> making the third column (column name = AT1G2986) into a dataframe and
> full join it with the previous; continue for the 152 observations so
> that then end result is a dataframe with 152 columns and number of rows
> depending on the entry with the greatest number of hits. The full joins
> I can do with dplyr, but getting up to that point seems rather difficult.
>
> This would get me what my ultimate goal would be; each Regulator is a
> column name (152 columns) and a given row has either NA or the same hit.
>
>     This seems very difficult to me, but I appreciate any attempt.
>
> Matthew
>
> On 4/30/2019 4:34 PM, David L Carlson wrote:
> >          External Email - Use Caution
> >
> > I think we need more information. Can you give us the structure of the
> data with str(YourDataFrame). Alternatively you could copy a small piece
> into your email message by copying and pasting the results of the following
> code:
> >
> > dput(head(YourDataFrame))
> >
> > The data frame you present could not be a data frame since you say
> "hits" is a factor with a variable number of elements. If each value of
> "hits" was a single character string, it would only have 2 factor levels
> not 6 and your efforts to parse the string would make more sense.
> Transposing to a data frame would only be possible if each column was
> padded with NAs to make them equal in length. Since your example tries use
> the name TF2list, it is possible that you do not have a data frame but a
> list and you have no factor levels, just character vectors.
> >
> > If you are not familiar with R, it may be helpful to tell us what your
> overall goal is rather than an intermediate step. Very likely R can easily
> handle what you want by doing things a different way.
> >
> > ----------------------------------------
> > David L Carlson
> > Department of Anthropology
> > Texas A&M University
> > College Station, TX 77843-4352
> >
> >
> >
> > -----Original Message-----
> > From: R-help<r-help-bounces at r-project.org>  On Behalf Of Matthew
> > Sent: Tuesday, April 30, 2019 2:25 PM
> > To: r-help (r-help at r-project.org)<r-help at r-project.org>
> > Subject: [R] transpose and split dataframe
> >
> > I have a data frame that is a lot bigger but for simplicity sake we can
> > say it looks like this:
> >
> > Regulator    hits
> > AT1G69490    AT4G31950,AT5G24110,AT1G26380,AT1G05675
> > AT2G55980    AT2G85403,AT4G89223
> >
> >      In other words:
> >
> > data.frame : 2 obs. of 2 variables
> > $Regulator: Factor w/ 2 levels
> > $hits         : Factor w/ 6 levels
> >
> >     I want to transpose it so that Regulator is now the column headings
> > and each of the AGI numbers now separated by commas is a row. So,
> > AT1G69490 is now the header of the first column and AT4G31950 is row 1
> > of column 1, AT5G24110 is row 2 of column 1, etc. AT2G55980 is header of
> > column 2 and AT2G85403 is row 1 of column 2, etc.
> >
> >     I have tried playing around with strsplit(TF2list[2:2]) and
> > strsplit(as.character(TF2list[2:2]), but I am getting nowhere.
> >
> > Matthew
> >
> > ______________________________________________
> > R-help at r-project.org  mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guidehttp://
> www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>         [[alternative HTML version deleted]]
>
>
>
>
> ------------------------------
>
> Message: 13
> Date: Wed, 1 May 2019 07:46:32 +1000
> From: Jim Lemon <drjimlemon at gmail.com>
> To: Matthew <mccormack at molbio.mgh.harvard.edu>
> Cc: "r-help (r-help at r-project.org)" <r-help at r-project.org>
> Subject: Re: [R] transpose and split dataframe
> Message-ID:
>         <CA+8X3fUjv3APb=
> UcsNQAD61pmOSbvoYBFsW3caZW7p11eD7umg at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> Hi Matthew,
> Is this what you are trying to do?
>
> mmdf<-read.table(text="Regulator    hits
> AT1G69490    AT4G31950,AT5G24110,AT1G26380,AT1G05675
> AT2G55980    AT2G85403,AT4G89223",header=TRUE,
> stringsAsFactors=FALSE)
> # split the second column at the commas
> hitsplit<-strsplit(mmdf$hits,",")
> # define a function that will fill with NAs
> NAfill<-function(x,n) return(x[1:n])
> # get the maximum length of hits
> maxlen<-max(unlist(lapply(hitsplit,length)))
> # fill the list with NAs
> hitsplit<-lapply(hitsplit,NAfill,maxlen)
> # change the names of the list
> names(hitsplit)<-mmdf$Regulator
> # convert to a data frame
> tmmdf<-as.data.frame(hitsplit)
>
> Jim
>
> On Wed, May 1, 2019 at 5:25 AM Matthew <mccormack at molbio.mgh.harvard.edu>
> wrote:
> >
> > I have a data frame that is a lot bigger but for simplicity sake we can
> > say it looks like this:
> >
> > Regulator    hits
> > AT1G69490    AT4G31950,AT5G24110,AT1G26380,AT1G05675
> > AT2G55980    AT2G85403,AT4G89223
> >
> >     In other words:
> >
> > data.frame : 2 obs. of 2 variables
> > $Regulator: Factor w/ 2 levels
> > $hits         : Factor w/ 6 levels
> >
> >    I want to transpose it so that Regulator is now the column headings
> > and each of the AGI numbers now separated by commas is a row. So,
> > AT1G69490 is now the header of the first column and AT4G31950 is row 1
> > of column 1, AT5G24110 is row 2 of column 1, etc. AT2G55980 is header of
> > column 2 and AT2G85403 is row 1 of column 2, etc.
> >
> >    I have tried playing around with strsplit(TF2list[2:2]) and
> > strsplit(as.character(TF2list[2:2]), but I am getting nowhere.
> >
> > Matthew
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
> ------------------------------
>
> Message: 14
> Date: Wed, 1 May 2019 09:58:34 +1200
> From: Abs Spurdle <spurdle.a at gmail.com>
> To: =?UTF-8?Q?Catarina_Serra_Gon=C3=A7alves?= <catarinasg at gmail.com>
> Cc: r-help <r-help at r-project.org>
> Subject: Re: [R]  Time series (trend over time) for irregular sampling
>         dates and multiple sites
> Message-ID:
>         <
> CAB8pepxHYbCXQPX5CaUQ868kMAp80z+zSXH7LHak+xDabJOjKg at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> > My data has a few problems: (1) I think I will need to fix the effects of
> > seasonal variation (Monthly) and (2) of possible spatial correlation
> > (probability of finding an item is higher after finding one since they
> can
> > come from the same ship). (3) How do I handle the fact that the
> > measurements were not taken at a regular interval?
>
> Can I ask two questions:
> (1) Is the data autocorrelated (or "Seasonal") over time?
> If not then this problem is a lot simpler.
> (2) Can you expand on the following statement?
> "possible spatial correlation (probability of finding an item is higher
> after finding one since they can come from the same ship"
>
>         [[alternative HTML version deleted]]
>
>
>
>
> ------------------------------
>
> Message: 15
> Date: Tue, 30 Apr 2019 22:29:24 +0000
> From: David L Carlson <dcarlson at tamu.edu>
> To: Matthew <mccormack at molbio.mgh.harvard.edu>, "r-help at r-project.org"
>         <r-help at r-project.org>
> Subject: Re: [R] Fwd: Re:  transpose and split dataframe
> Message-ID: <1d59b3c0584a40c1b322b0efd5de7646 at tamu.edu>
> Content-Type: text/plain; charset="utf-8"
>
> If you read the data frame with read.csv() or one of the other read()
> functions, use the asis=TRUE argument to prevent conversion to factors. If
> not do the conversion first:
>
> # Convert factors to characters
> DataMatrix <- sapply(TF2list, as.character)
> # Split the vector of hits
> DataList <- sapply(DataMatrix[, 2], strsplit, split=",")
> # Use the values in Regulator to name the parts of the list
> names(DataList) <- DataMatrix[,"Regulator"]
>
> # Now create a data frame
> # How long is the longest list of hits?
> mx <- max(sapply(DataList, length))
> # Now add NAs to vectors shorter than mx
> DataList2 <- lapply(DataList, function(x) c(x, rep(NA, mx-length(x))))
> # Finally convert back to a data frame
> TF2list2 <- do.call(data.frame, DataList2)
>
> Try this on a portion of the list, say 25 lines and print each object to
> see what is happening.
>
> ----------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77843-4352
>
>
>
>
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Matthew
> Sent: Tuesday, April 30, 2019 4:31 PM
> To: r-help at r-project.org
> Subject: [R] Fwd: Re: transpose and split dataframe
>
> Thanks for your reply. I was trying to simplify it a little, but must
> have got it wrong. Here is the real dataframe, TF2list:
>
>   str(TF2list)
> 'data.frame':    152 obs. of  2 variables:
>   $ Regulator: Factor w/ 87 levels "AT1G02065","AT1G13960",..: 17 6 6 54
> 54 82 82 82 82 82 ...
>   $ hits     : Factor w/ 97 levels
> "AT1G05675,AT3G12910,AT1G22810,AT1G14540,AT1G21120,AT1G07160,AT5G22520,AT1G56250,AT2G31345,AT5G22530,AT4G11170,A"|
>
> __truncated__,..: 65 57 90 57 87 57 56 91 31 17 ...
>
>     And the first few lines resulting from dput(head(TF2list)):
>
> dput(head(TF2list))
> structure(list(Regulator = structure(c(17L, 6L, 6L, 54L, 54L,
>

	[[alternative HTML version deleted]]


From dc@r|@on @end|ng |rom t@mu@edu  Thu May  2 16:43:02 2019
From: dc@r|@on @end|ng |rom t@mu@edu (David L Carlson)
Date: Thu, 2 May 2019 14:43:02 +0000
Subject: [R] Fwd: Re:  transpose and split dataframe
In-Reply-To: <e4a9e321-b437-eed6-344b-472319e85fec@molbio.mgh.harvard.edu>
References: <e154ccbb-71a6-c2ac-41ca-2171c8a9dc76@molbio.mgh.harvard.edu>
 <e4a9e321-b437-eed6-344b-472319e85fec@molbio.mgh.harvard.edu>
Message-ID: <df8f17304f9541cda96fbc48d969ca0e@tamu.edu>

We still have only the toy version of your data from your first email. The second email used dput() as I suggested, but you truncated the results so it is useless for testing purposes.

Use the following code after creating DataList (up to mx <- ... ) in my earlier answer:

n <- sapply(DataList, length)
hits <- unname(unlist(DataList))
Regulator <- unname(unlist(mapply(rep, names(DataList), times=n)))
DataTable <- table(hits, Regulator)

#            Regulator
# hits        AT1G69490 AT2G55980
#  AT1G05675         1         0
#  AT1G26380         1         0
#  AT2G85403         0         1
#  AT4G31950         1         0
#  AT4G89223         0         1
#  AT5G24110         1         0

Now the Regulators and the hits will be listed in alphabetical order. The table has 0's for Regulators that do not have a particular hit. If you want NAs:

DataTable[DataTable==0] <- NA
print(DataTable, na.print="NA")
#            Regulator
# hits        AT1G69490 AT2G55980
#   AT1G05675         1        NA
#   AT1G26380         1        NA
#   AT2G85403        NA         1
#   AT4G31950         1        NA
#   AT4G89223        NA         1
#   AT5G24110         1        NA

If you need a data frame instead of a table:

as.data.frame.matrix(DataTable)

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Matthew
Sent: Tuesday, April 30, 2019 4:31 PM
To: r-help at r-project.org
Subject: [R] Fwd: Re: transpose and split dataframe

Thanks for your reply. I was trying to simplify it a little, but must 
have got it wrong. Here is the real dataframe, TF2list:

 ?str(TF2list)
'data.frame':??? 152 obs. of? 2 variables:
 ?$ Regulator: Factor w/ 87 levels "AT1G02065","AT1G13960",..: 17 6 6 54 
54 82 82 82 82 82 ...
 ?$ hits???? : Factor w/ 97 levels 
"AT1G05675,AT3G12910,AT1G22810,AT1G14540,AT1G21120,AT1G07160,AT5G22520,AT1G56250,AT2G31345,AT5G22530,AT4G11170,A"| 
__truncated__,..: 65 57 90 57 87 57 56 91 31 17 ...

 ?? And the first few lines resulting from dput(head(TF2list)):

dput(head(TF2list))
structure(list(Regulator = structure(c(17L, 6L, 6L, 54L, 54L,
82L), .Label = c("AT1G02065", "AT1G13960", "AT1G18860", "AT1G23380",
"AT1G29280", "AT1G29860", "AT1G30650", "AT1G55600", "AT1G62300",
"AT1G62990", "AT1G64000", "AT1G66550", "AT1G66560", "AT1G66600",
"AT1G68150", "AT1G69310", "AT1G69490", "AT1G69810", "AT1G70510", ...

This is another way of looking at the first 4 entries (Regulator is 
tab-separated from hits):

Regulator
 ? hits
1
AT1G69490
 ?AT4G31950,AT5G24110,AT1G26380,AT1G05675,AT3G12910,AT5G64905,AT1G22810,AT1G79680,AT3G02840,AT5G25260,AT5G57220,AT2G37430,AT2G26560,AT1G56250,AT3G23230,AT1G16420,AT1G78410,AT4G22030,AT5G05300,AT1G69930,AT4G03460,AT4G11470,AT5G25250,AT5G36925,AT2G30750,AT1G16150,AT1G02930,AT2G19190,AT4G11890,AT1G72520,AT4G31940,AT5G37490,AT5G52760,AT5G66020,AT3G57460,AT4G23220,AT3G15518,AT2G43620,AT2G02010,AT1G35210,AT5G46295,AT1G17147,AT1G11925,AT2G39200,AT1G02920,AT2G40180,AT1G59865,AT4G35180,AT4G15417,AT1G51820,AT1G06135,AT1G36622,AT5G42830
2
AT1G29860
 ?AT4G31950,AT5G24110,AT1G05675,AT3G12910,AT5G64905,AT1G22810,AT1G14540,AT1G79680,AT1G07160,AT3G23250,AT5G25260,AT1G53625,AT5G57220,AT2G37430,AT3G54150,AT1G56250,AT3G23230,AT1G16420,AT1G78410,AT4G22030,AT1G69930,AT4G03460,AT4G11470,AT5G25250,AT5G36925,AT4G14450,AT2G30750,AT1G16150,AT1G02930,AT2G19190,AT4G11890,AT1G72520,AT4G31940,AT5G37490,AT4G08555,AT5G66020,AT5G26920,AT3G57460,AT4G23220,AT3G15518,AT2G43620,AT1G35210,AT5G46295,AT1G17147,AT1G11925,AT2G39200,AT1G02920,AT4G35180,AT4G15417,AT1G51820,AT4G40020,AT1G06135

3
AT1G2986
 ?AT5G64905,AT1G21120,AT1G07160,AT5G25260,AT1G53625,AT1G56250,AT2G31345,AT4G11170,AT1G66090,AT1G26410,AT3G55840,AT1G69930,AT4G03460,AT5G25250,AT5G36925,AT1G26420,AT5G42380,AT1G16150,AT2G22880,AT1G02930,AT4G11890,AT1G72520,AT5G66020,AT2G43620,AT2G44370,AT4G15975,AT1G35210,AT5G46295,AT1G11925,AT2G39200,AT1G02920,AT4G14370,AT4G35180,AT4G15417,AT2G18690,AT5G11140,AT1G06135,AT5G42830

 ?? So, the goal would be to

first: Transpose the existing dataframe so that the factor Regulator 
becomes a column name (column 1 name = AT1G69490, column2 name 
AT1G29860, etc.) and the hits associated with each Regulator become 
rows. Hits is a comma separated 'list' ( I do not not know if 
technically it is an R list.), so it would have to be comma 
'unseparated' with each entry becoming a row (col 1 row 1 = AT4G31950, 
col 1 row 2 - AT5G24410, etc); like this :

AT1G69490
AT4G31950
AT5G24110
AT1G05675
AT5G64905

... I did not include all the rows)

I think it would be best to actually make the first entry a separate 
dataframe ( 1 column with name = AT1G69490 and number of rows depending 
on the number of hits), then make the second column (column name = 
AT1G29860, and number of rows depending on the number of hits) into a 
new dataframe and do a full join of of the two dataframes; continue by 
making the third column (column name = AT1G2986) into a dataframe and 
full join it with the previous; continue for the 152 observations so 
that then end result is a dataframe with 152 columns and number of rows 
depending on the entry with the greatest number of hits. The full joins 
I can do with dplyr, but getting up to that point seems rather difficult.

This would get me what my ultimate goal would be; each Regulator is a 
column name (152 columns) and a given row has either NA or the same hit.

 ?? This seems very difficult to me, but I appreciate any attempt.

Matthew

On 4/30/2019 4:34 PM, David L Carlson wrote:
>          External Email - Use Caution
>
> I think we need more information. Can you give us the structure of the data with str(YourDataFrame). Alternatively you could copy a small piece into your email message by copying and pasting the results of the following code:
>
> dput(head(YourDataFrame))
>
> The data frame you present could not be a data frame since you say "hits" is a factor with a variable number of elements. If each value of "hits" was a single character string, it would only have 2 factor levels not 6 and your efforts to parse the string would make more sense. Transposing to a data frame would only be possible if each column was padded with NAs to make them equal in length. Since your example tries use the name TF2list, it is possible that you do not have a data frame but a list and you have no factor levels, just character vectors.
>
> If you are not familiar with R, it may be helpful to tell us what your overall goal is rather than an intermediate step. Very likely R can easily handle what you want by doing things a different way.
>
> ----------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77843-4352
>
>
>
> -----Original Message-----
> From: R-help<r-help-bounces at r-project.org>  On Behalf Of Matthew
> Sent: Tuesday, April 30, 2019 2:25 PM
> To: r-help (r-help at r-project.org)<r-help at r-project.org>
> Subject: [R] transpose and split dataframe
>
> I have a data frame that is a lot bigger but for simplicity sake we can
> say it looks like this:
>
> Regulator??? hits
> AT1G69490??? AT4G31950,AT5G24110,AT1G26380,AT1G05675
> AT2G55980??? AT2G85403,AT4G89223
>
>   ?? In other words:
>
> data.frame : 2 obs. of 2 variables
> $Regulator: Factor w/ 2 levels
> $hits???????? : Factor w/ 6 levels
>
>   ? I want to transpose it so that Regulator is now the column headings
> and each of the AGI numbers now separated by commas is a row. So,
> AT1G69490 is now the header of the first column and AT4G31950 is row 1
> of column 1, AT5G24110 is row 2 of column 1, etc. AT2G55980 is header of
> column 2 and AT2G85403 is row 1 of column 2, etc.
>
>   ? I have tried playing around with strsplit(TF2list[2:2]) and
> strsplit(as.character(TF2list[2:2]), but I am getting nowhere.
>
> Matthew
>
> ______________________________________________
> R-help at r-project.org  mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guidehttp://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From ||@t@ @end|ng |rom dewey@myzen@co@uk  Thu May  2 17:44:53 2019
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Thu, 2 May 2019 16:44:53 +0100
Subject: [R] Survuval Anaysis
In-Reply-To: <CABVwvn65QfJzmmD7+MhgVaCWQuqFirTdC6iPoPKJpUiHxZx2NQ@mail.gmail.com>
References: <mailman.354552.1.1556704801.3302.r-help@r-project.org>
 <CABVwvn65QfJzmmD7+MhgVaCWQuqFirTdC6iPoPKJpUiHxZx2NQ@mail.gmail.com>
Message-ID: <fce36861-6961-42cf-5a8f-c90220e5fd67@dewey.myzen.co.uk>

Without more details it is hard to answer but it is suspicious that it 
is dropping one of your predictors and the standard errors of the other 
are very large. This suggests you should investigate the joint 
distribution of your predictors and the events.

Michael

On 02/05/2019 13:37, Haddison Mureithi wrote:
> Hello guys this problem was never answered and I happened to come across
> the same problem , kindly help. This is a simple R program that I have been
> trying to run. I keep running into the "singular matrix" error. I end up
> with no sensible results. Can anyone suggest any changes or a way around
> this?
> 
> I am a total rookie when working with R.
> 
> Thanks,
> Haddison
> 
>> library(survival)
> Loading required package: splines
>> args(coxph)
> function (formula, data, weights, subset, na.action, init, control,
>      method = c("efron", "breslow", "exact"), singular.ok = TRUE,
>      robust = FALSE, model = FALSE, x = FALSE, y = TRUE, tt, ...)
> NULL
>> test1<-read.table("S:/FISHDO/03_Phase_I_Field_Work/Data_6_28_2011/Working
> Folder/R_files/4SondesJuly24.csv", header=T, sep=",")
>> sondes<-coxph(Surv(Start, Stop, Depart)~DOLoomis + DOI55 + DODamen,
> data=test1)
> Warning messages:
> 1: In fitter(X, Y, strats, offset, init, control, weights = weights,  :
>    Loglik converged before variable  1,2 ; beta may be infinite.
> 2: In coxph(Surv(Start, Stop, Depart) ~ DOLoomis + DOI55 + DODamen,  :
>    X matrix deemed to be singular; variable 3
>> summary(sondes)
> Call:
> coxph(formula = Surv(Start, Stop, Depart) ~ DOLoomis + DOI55 +
>      DODamen, data = test1)
> 
>    n= 1737, number of events= 58
>     (1 observation deleted due to missingness)
> 
>                 coef  exp(coef)   se(coef)  z Pr(>|z|)
> DOLoomis -2.152e+00  1.163e-01  1.161e+05  0        1
> DOI55     4.560e-01  1.578e+00  3.755e+04  0        1
> DODamen          NA         NA  0.000e+00 NA       NA
> 
>           exp(coef) exp(-coef) lower .95 upper .95
> DOLoomis    0.1163     8.5995         0       Inf
> DOI55       1.5777     0.6338         0       Inf
> DODamen         NA         NA        NA        NA
> 
> Concordance= 0.5  (se = 0 )
> Rsquare= 0   (max possible= 0.01 )
> Likelihood ratio test= 0  on 2 df,   p=1
> Wald test            = 0  on 2 df,   p=1
> Score (logrank) test = 0  on 2 df,   p=1
> 
> On Wed, 1 May 2019, 1:00 pm , <r-help-request at r-project.org> wrote:
> 
>> Send R-help mailing list submissions to
>>          r-help at r-project.org
>>
>> To subscribe or unsubscribe via the World Wide Web, visit
>>          https://stat.ethz.ch/mailman/listinfo/r-help
>> or, via email, send a message with subject or body 'help' to
>>          r-help-request at r-project.org
>>
>> You can reach the person managing the list at
>>          r-help-owner at r-project.org
>>
>> When replying, please edit your Subject line so it is more specific
>> than "Re: Contents of R-help digest..."
>>
>>
>> Today's Topics:
>>
>>     1. Re: Bug in R 3.6.0? (Martin Maechler)
>>     2. Re: Bug in R 3.6.0? (ocjt at free.fr)
>>     3. Time series (trend over time) for irregular sampling dates
>>        and multiple sites (=?UTF-8?Q?Catarina_Serra_Gon=C3=A7alves?=)
>>     4. Re:  Time series (trend over time) for irregular sampling
>>        dates and multiple sites (Bert Gunter)
>>     5. Passing formula as parameter to `lm` within `sapply` causes
>>        error [BUG?] (Jens Heumann)
>>     6. (no subject) (Haddison Mureithi)
>>     7. Help with loop for column means into new column by a subset
>>        Factor w/131 levels (Bill Poling)
>>     8. Re: Help with loop for column means into new column by a
>>        subset Factor w/131 levels (Bill Poling)
>>     9. transpose and split dataframe (Matthew)
>>    10. Re: transpose and split dataframe (David L Carlson)
>>    11. Re: Passing formula as parameter to `lm` within `sapply`
>>        causes error [BUG?] (David Winsemius)
>>    12. Fwd: Re:  transpose and split dataframe (Matthew)
>>    13. Re: transpose and split dataframe (Jim Lemon)
>>    14. Re:  Time series (trend over time) for irregular sampling
>>        dates and multiple sites (Abs Spurdle)
>>    15. Re: Fwd: Re:  transpose and split dataframe (David L Carlson)
>>    16. Re: Passing formula as parameter to `lm` within `sapply`
>>        causes error [BUG?] (Duncan Murdoch)
>>    17. Re:  Time series (trend over time) for irregular sampling
>>        dates and multiple sites (Abs Spurdle)
>>    18. Re:  Time series (trend over time) for irregular sampling
>>        dates and multiple sites (Abs Spurdle)
>>    19. Re: Passing formula as parameter to `lm` within `sapply`
>>        causes error [BUG?] (Jens Heumann)
>>    20. Re: Passing formula as parameter to `lm` within `sapply`
>>        causes error [BUG?] (peter dalgaard)
>>
>> ----------------------------------------------------------------------
>>
>> Message: 1
>> Date: Tue, 30 Apr 2019 16:54:10 +0200
>> From: Martin Maechler <maechler at stat.math.ethz.ch>
>> To: Morgan Morgan <morgan.emailbox at gmail.com>
>> Cc: <r-help at r-project.org>
>> Subject: Re: [R] Bug in R 3.6.0?
>> Message-ID: <23752.24978.45927.96764 at stat.math.ethz.ch>
>> Content-Type: text/plain; charset="utf-8"
>>
>>>>>>> Morgan Morgan
>>>>>>>      on Mon, 29 Apr 2019 21:42:36 +0100 writes:
>>
>>      > Hi,
>>      > I am using the R 3.6.0 on windows. The issue that I report below
>> does not
>>      > exist with previous version of R.
>>      > In order to reproduce the error you must install a package of your
>> choice
>>      > from source (tar.gz).
>>
>>      > -Create a .Rprofile file with the following command in it :
>> setwd("D:/")
>>      > -Close your R session and re-open it. Your working directory must be
>> now set
>>      > to D:
>>      > -Install a package of your choice from source, example :
>>      > install.packages("data.table",type="source")
>>
>>      > In my case the package fail to install and I get the following error
>>      > message:
>>
>>      > ** R
>>      > ** inst
>>      > ** byte-compile and prepare package for lazy loading
>>      > Error in tools:::.read_description(file) :
>>      > file 'DESCRIPTION' does not exist
>>      > Calls: suppressPackageStartupMessages ... withCallingHandlers ->
>>      > .getRequiredPackages -> <Anonymous> -> <Anonymous>
>>      > Execution halted
>>      > ERROR: lazy loading failed for package 'data.table'
>>      > * removing 'C:/Users/Morgan/Documents/R/win-library/3.6/data.table'
>>      > * restoring previous
>>      > 'C:/Users/Morgan/Documents/R/win-library/3.6/data.table'
>>      > Warning in install.packages :
>>      > installation of package ?data.table? had non-zero exit status
>>
>>      > Now remove the .Rprofile file, restart your R session and try to
>> install th
>> e
>>      > package with the same command.
>>      > In that case everything should be installed just fine.
>>
>>      > FYI the issue happens on macOS as well and I suspect it also does on
>> all
>>      > linux systems.
>>
>>      > My question: Is this expected or is it a bug?
>>
>> It is a bug, thank you very much for reporting it.
>>
>> I've been told privately by ?mer An (thank you!) who's been
>> affected as well, that this problem seems to affect others, and
>> that there's a thread about this over at the Rstudio support site
>>
>>
>> https://support.rstudio.com/hc/en-us/community/posts/200704708-Build-tool-does-not-recognize-DESCRIPTION-file
>>
>> There, users mention that (all?) packages are affected which
>> have a multiline 'Description:' field in their DESCRIPTION file.
>> Of course, many if not most packages have this property.
>>
>> Indeed, I can reproduce the problem (e.g. with my 'sfsmisc'
>> package) if I ("silly enough to") add a setwd() call to my
>> Rprofile file  (the one I set via env.var  R_PROFILE or R_PROFILE_USER).
>>
>> This is clearly a bug, and indeed a bad one.
>>
>> It seems all R core (and other R expert users who have tried R
>> 3.6.0 alpha, beta, and RC versions) have *not* seen the bug as they
>> are intuitively smart not to mess with R's working directory in
>> a global R profile file ...
>>
>> For now you definitively have to work around by not doing what's
>> the problem : do *NOT* setwd() in your  ~/.Rprofile or other
>> such R init files.
>>
>> Best,
>> Martin Maechler
>> ETH Zurich and  R Core Team
>>
>>
>>
>>
>> ------------------------------
>>
>> Message: 2
>> Date: Tue, 30 Apr 2019 16:15:46 +0200
>> From: <ocjt at free.fr>
>> To: "'Morgan Morgan'" <morgan.emailbox at gmail.com>,
>>          <r-help at r-project.org>
>> Subject: Re: [R] Bug in R 3.6.0?
>> Message-ID: <002d01d4ff5f$34816be0$9d8443a0$@free.fr>
>> Content-Type: text/plain; charset="utf-8"
>>
>> Hello,
>>
>> I have exactly the same problem when I install one of my own packages:
>>
>> Error in tools:::.read_description(file) :
>>    file 'DESCRIPTION' does not exist
>> Calls: suppressPackageStartupMessages ... withCallingHandlers ->
>> .getRequiredPackages -> <Anonymous> -> <Anonymous>
>> Ex?cution arr?t?e
>> ERROR: lazy loading failed for package 'RRegArch'
>>
>> Best,
>> Ollivier
>>
>>
>> -----Message d'origine-----
>> De : R-help <r-help-bounces at r-project.org> De la part de Morgan Morgan
>> Envoy? : lundi 29 avril 2019 22:43
>> ? : r-help at r-project.org
>> Objet : [R] Bug in R 3.6.0?
>>
>> Hi,
>>
>> I am using the R 3.6.0 on windows. The issue that I report below does not
>> exist with previous version of R.
>> In order to reproduce the error you must install a package of your choice
>> from source (tar.gz).
>>
>> -Create a .Rprofile file with the following command in it : setwd("D:/")
>> -Close your R session and re-open it. Your working directory must be now
>> set to D:
>> -Install a package of your choice from source, example :
>> install.packages("data.table",type="source")
>>
>> In my case the package fail to install and I get the following error
>> message:
>>
>> ** R
>> ** inst
>> ** byte-compile and prepare package for lazy loading Error in
>> tools:::.read_description(file) :
>>    file 'DESCRIPTION' does not exist
>> Calls: suppressPackageStartupMessages ... withCallingHandlers ->
>> .getRequiredPackages -> <Anonymous> -> <Anonymous> Execution halted
>> ERROR: lazy loading failed for package 'data.table'
>> * removing 'C:/Users/Morgan/Documents/R/win-library/3.6/data.table'
>> * restoring previous
>> 'C:/Users/Morgan/Documents/R/win-library/3.6/data.table'
>> Warning in install.packages :
>>    installation of package ?data.table? had non-zero exit status
>>
>> Now remove the .Rprofile file, restart your R session and try to install
>> the package with the same command.
>> In that case everything should be installed just fine.
>>
>> FYI the issue happens on macOS as well and I suspect it also does on all
>> linux systems.
>>
>> My question: Is this expected or is it a bug?
>>
>> Thank you
>> Best regards,
>> Morgan
>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>>
>> ------------------------------
>>
>> Message: 3
>> Date: Wed, 1 May 2019 00:57:43 +1000
>> From: =?UTF-8?Q?Catarina_Serra_Gon=C3=A7alves?= <catarinasg at gmail.com>
>> To: r-help at r-project.org
>> Subject: [R] Time series (trend over time) for irregular sampling
>>          dates and multiple sites
>> Message-ID:
>>          <
>> CAOQWJbvY+JKy80sksmfC8tu-C+5qq-tzwAd21XbyGvJAyYjQPQ at mail.gmail.com>
>> Content-Type: text/plain; charset="utf-8"
>>
>> I have a dataset of marine debris items (number of items standardized per
>> effort: Items/(number of volunteers*Hours*Lenght)) taken from 2 main
>> locations (WA and Queensland) in Australia (8 Sub Sites in total: 4 in WA
>> and 4 in Queensland) at irregular sampling intervals over a period 15
>> years.
>>
>> I want to test if there is a change over the years on the amount of debris
>> in these locations and more specifically a change after the implementation
>> of a mitigation strategy (in 2013).
>> Here?s the head of the data:[image: enter image description here]
>> <https://i.stack.imgur.com/VNIpb.png>Description of each one of the
>> varables in the dataframe:
>>
>> *eventid *= each sampling (clean-up) event Location = Queensland and New
>> South Wales Sites = all the 9 sampling beaches
>>
>> *Date *= specific dates for the clean-up events (day-month-year)
>>
>> *Date1 *= specific dates for the clean-up events (day-month-year) on the
>> POSICXT format Year= Year of sampling event (2004 to 2018)
>>
>> *Month*= Month of the sampling event (jan to dec)
>>
>> *nMonth*= a number was determined to the respective month of the sampling
>> event (1 to 12)
>>
>> *Day*= Day of sampling (1 to 31) Days = Days since the first date of clean
>> up = just another way of using the dates
>>
>> *MARPOL *= before and after implementation (factor with 2 levels)
>>
>> *DaysC *= days between sampling events for the same sites = number of days
>> since the previous clean-up event
>>
>> *DaysI *= Days since intervention, all the dates before implementation are
>> zero, and after we count the number of days since the implementation date
>> (1 jan 2013)
>>
>> *DaysIa*= same as DayI but instead of zero for before the intervention we
>> have negative values (days)
>>
>> *Items *= number of fishing and shipping items counted in each clean-up
>> event
>>
>> *Hours *= hours spent by all volunteers together at each clean up event
>>
>> *Lenght *= Lenght of beach sampled by all volunteers together at each clean
>> up event volunteers = all volunteers at each clean up event
>>
>> *HoursVolunteer *= hours spent bt each volunteer at each clean up event
>> (Hours/volunteers)
>>
>> *Ieffort *= the items standarized by the effort (hours, volunteers and
>> lenght)
>>
>> *GrossWeight & **GrossTotal are not relevant *
>> ------------------------------
>> Problems:
>>
>> My data has a few problems: (1) I think I will need to fix the effects of
>> seasonal variation (Monthly) and (2) of possible spatial correlation
>> (probability of finding an item is higher after finding one since they can
>> come from the same ship). (3) How do I handle the fact that the
>> measurements were not taken at a regular interval?
>>
>> I was trying to use GAMs to analyse the data and see the trends over time.
>> The model I came across is the following:
>>
>> m4<- gamm(Ieffort ~ s(DaysIa)+MARPOL+ s(nMonth, bs = "ps", k = 12),
>> random=list(Site=~1,Location=~1),data = d)
>>
>> *thank you in advance.*
>> -
>> *Catarina Serra Gon?alves *
>> PhD candidate
>>
>> Adrift Lab  <https://adriftlab.org>
>> University of Tasmania <http://www.utas.edu.au/> | Institute for Marine
>> and
>> Antarctic Studies  <http://www.imas.utas.edu.au/>
>> Launceston, TAS | Australia
>>
>> Personal website <https://catarinasg.wixsite.com/acserra>
>> <https://catarinasg.wixsite.com/acserra>| E-mail  <acserra at utas.edu.au> |
>> Twitter <https://twitter.com/CatarinaSerraG>
>> Research Gate
>> <https://www.researchgate.net/profile/Catarina_Serra_Goncalves> | Google
>> Scholar <https://scholar.google.pt/citations?user=8nBrRFwAAAAJ&hl=en>
>>
>>          [[alternative HTML version deleted]]
>>
>>
>>
>>
>> ------------------------------
>>
>> Message: 4
>> Date: Tue, 30 Apr 2019 08:28:37 -0700
>> From: Bert Gunter <bgunter.4567 at gmail.com>
>> To: =?UTF-8?Q?Catarina_Serra_Gon=C3=A7alves?= <catarinasg at gmail.com>
>> Cc: R-help <r-help at r-project.org>
>> Subject: Re: [R]  Time series (trend over time) for irregular sampling
>>          dates and multiple sites
>> Message-ID:
>>          <CAGxFJbT2YSB1xcs0MajpeqtHbbn4T1ycYoSOBEFvMucFme1t=
>> g at mail.gmail.com>
>> Content-Type: text/plain; charset="utf-8"
>>
>> I have 0 expertise, but I suggest that you check out the SPatioTemporal
>> taskview on CRAN (or possibly others, like environmetrics). You might also
>> want to move this to the R-Sig-geo list,where you probably are more likely
>> to find relevant expertise.
>>
>> Cheers,
>> Bert
>>
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming along and
>> sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>> On Tue, Apr 30, 2019 at 8:13 AM Catarina Serra Gon?alves <
>> catarinasg at gmail.com> wrote:
>>
>>> I have a dataset of marine debris items (number of items standardized per
>>> effort: Items/(number of volunteers*Hours*Lenght)) taken from 2 main
>>> locations (WA and Queensland) in Australia (8 Sub Sites in total: 4 in WA
>>> and 4 in Queensland) at irregular sampling intervals over a period 15
>>> years.
>>>
>>> I want to test if there is a change over the years on the amount of
>> debris
>>> in these locations and more specifically a change after the
>> implementation
>>> of a mitigation strategy (in 2013).
>>> Here?s the head of the data:[image: enter image description here]
>>> <https://i.stack.imgur.com/VNIpb.png>Description of each one of the
>>> varables in the dataframe:
>>>
>>> *eventid *= each sampling (clean-up) event Location = Queensland and New
>>> South Wales Sites = all the 9 sampling beaches
>>>
>>> *Date *= specific dates for the clean-up events (day-month-year)
>>>
>>> *Date1 *= specific dates for the clean-up events (day-month-year) on the
>>> POSICXT format Year= Year of sampling event (2004 to 2018)
>>>
>>> *Month*= Month of the sampling event (jan to dec)
>>>
>>> *nMonth*= a number was determined to the respective month of the sampling
>>> event (1 to 12)
>>>
>>> *Day*= Day of sampling (1 to 31) Days = Days since the first date of
>> clean
>>> up = just another way of using the dates
>>>
>>> *MARPOL *= before and after implementation (factor with 2 levels)
>>>
>>> *DaysC *= days between sampling events for the same sites = number of
>> days
>>> since the previous clean-up event
>>>
>>> *DaysI *= Days since intervention, all the dates before implementation
>> are
>>> zero, and after we count the number of days since the implementation date
>>> (1 jan 2013)
>>>
>>> *DaysIa*= same as DayI but instead of zero for before the intervention we
>>> have negative values (days)
>>>
>>> *Items *= number of fishing and shipping items counted in each clean-up
>>> event
>>>
>>> *Hours *= hours spent by all volunteers together at each clean up event
>>>
>>> *Lenght *= Lenght of beach sampled by all volunteers together at each
>> clean
>>> up event volunteers = all volunteers at each clean up event
>>>
>>> *HoursVolunteer *= hours spent bt each volunteer at each clean up event
>>> (Hours/volunteers)
>>>
>>> *Ieffort *= the items standarized by the effort (hours, volunteers and
>>> lenght)
>>>
>>> *GrossWeight & **GrossTotal are not relevant *
>>> ------------------------------
>>> Problems:
>>>
>>> My data has a few problems: (1) I think I will need to fix the effects of
>>> seasonal variation (Monthly) and (2) of possible spatial correlation
>>> (probability of finding an item is higher after finding one since they
>> can
>>> come from the same ship). (3) How do I handle the fact that the
>>> measurements were not taken at a regular interval?
>>>
>>> I was trying to use GAMs to analyse the data and see the trends over
>> time.
>>> The model I came across is the following:
>>>
>>> m4<- gamm(Ieffort ~ s(DaysIa)+MARPOL+ s(nMonth, bs = "ps", k = 12),
>>> random=list(Site=~1,Location=~1),data = d)
>>>
>>> *thank you in advance.*
>>> -
>>> *Catarina Serra Gon?alves *
>>> PhD candidate
>>>
>>> Adrift Lab  <https://adriftlab.org>
>>> University of Tasmania <http://www.utas.edu.au/> | Institute for Marine
>>> and
>>> Antarctic Studies  <http://www.imas.utas.edu.au/>
>>> Launceston, TAS | Australia
>>>
>>> Personal website <https://catarinasg.wixsite.com/acserra>
>>> <https://catarinasg.wixsite.com/acserra>| E-mail  <acserra at utas.edu.au>
>> |
>>> Twitter <https://twitter.com/CatarinaSerraG>
>>> Research Gate
>>> <https://www.researchgate.net/profile/Catarina_Serra_Goncalves> | Google
>>> Scholar <https://scholar.google.pt/citations?user=8nBrRFwAAAAJ&hl=en>
>>>
>>>          [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>          [[alternative HTML version deleted]]
>>
>>
>>
>>
>> ------------------------------
>>
>> Message: 5
>> Date: Tue, 30 Apr 2019 17:24:33 +0200
>> From: Jens Heumann <jens.heumann at students.unibe.ch>
>> To: <r-help at r-project.org>
>> Subject: [R] Passing formula as parameter to `lm` within `sapply`
>>          causes error [BUG?]
>> Message-ID: <75abba2b-c528-460e-df92-08f8479ba399 at students.unibe.ch>
>> Content-Type: text/plain; charset="utf-8"; Format="flowed"
>>
>> Hi,
>>
>> `lm` won't take formula as a parameter when it is within a `sapply`; see
>> example below. Please, could anyone either point me to a syntax error or
>> confirm that this might be a bug?
>>
>> Best,
>> Jens
>>
>> [Disclaimer: This is my first post here, following advice of how to
>> proceed with possible bugs from here: https://www.r-project.org/bugs.html]
>>
>>
>> SUMMARY
>>
>> While `lm` alone accepts formula parameter `FO` well, the same within a
>> `sapply` causes an error. When putting everything as parameter but
>> formula `FO`, it's still working, though. All parameters work fine
>> within a similar `for` loop.
>>
>>
>> MCVE (see data / R-version at bottom)
>>
>>   > summary(lm(y ~ x, df1, df1[["z"]] == 1, df1[["w"]]))$coef[1, ]
>>     Estimate Std. Error    t value   Pr(>|t|)
>>    1.6269038  0.9042738  1.7991275  0.3229600
>>   > summary(lm(FO, data, data[[st]] == st1, data[[ws]]))$coef[1, ]
>>     Estimate Std. Error    t value   Pr(>|t|)
>>    1.6269038  0.9042738  1.7991275  0.3229600
>>   > sapply(unique(df1$z), function(s)
>> +   summary(lm(y ~ x, df1, df1[["z"]] == s, df1[[ws]]))$coef[1, ])
>>                   [,1]       [,2]         [,3]
>> Estimate   1.6269038 -0.1404174 -0.010338774
>> Std. Error 0.9042738  0.4577001  1.858138516
>> t value    1.7991275 -0.3067890 -0.005564049
>> Pr(>|t|)   0.3229600  0.8104951  0.996457853
>>   > sapply(unique(data[[st]]), function(s)
>> +   summary(lm(FO, data, data[[st]] == s, data[[ws]]))$coef[1, ])  # !!!
>> Error in eval(substitute(subset), data, env) : object 's' not found
>>   > sapply(unique(data[[st]]), function(s)
>> +   summary(lm(y ~ x, data, data[[st]] == s, data[[ws]]))$coef[1, ])
>>                   [,1]       [,2]         [,3]
>> Estimate   1.6269038 -0.1404174 -0.010338774
>> Std. Error 0.9042738  0.4577001  1.858138516
>> t value    1.7991275 -0.3067890 -0.005564049
>> Pr(>|t|)   0.3229600  0.8104951  0.996457853
>>   > m <- matrix(NA, 4, length(unique(data[[st]])))
>>   > for (s in unique(data[[st]])) {
>> +   m[, s] <- summary(lm(FO, data, data[[st]] == s, data[[ws]]))$coef[1, ]
>> + }
>>   > m
>>             [,1]       [,2]         [,3]
>> [1,] 1.6269038 -0.1404174 -0.010338774
>> [2,] 0.9042738  0.4577001  1.858138516
>> [3,] 1.7991275 -0.3067890 -0.005564049
>> [4,] 0.3229600  0.8104951  0.996457853
>>
>> # DATA #################################################################
>>
>> df1 <- structure(list(x = c(1.37095844714667, -0.564698171396089,
>> 0.363128411337339,
>> 0.63286260496104, 0.404268323140999, -0.106124516091484, 1.51152199743894,
>> -0.0946590384130976, 2.01842371387704), y = c(1.30824434809425,
>> 0.740171482827397, 2.64977380403845, -0.755998096151299, 0.125479556323628,
>> -0.239445852485142, 2.14747239550901, -0.37891195982917, -0.638031707027734
>> ), z = c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L), w = c(0.7, 0.8,
>> 1.2, 0.9, 1.3, 1.2, 0.8, 1, 1)), class = "data.frame", row.names = c(NA,
>> -9L))
>>
>> FO <- y ~ x; data <- df1; st <- "z"; ws <- "w"; st1 <- 1
>>
>> ########################################################################
>>
>>   > R.version
>>                  _
>> platform       x86_64-w64-mingw32
>> arch           x86_64
>> os             mingw32
>> system         x86_64, mingw32
>> status
>> major          3
>> minor          6.0
>> year           2019
>> month          04
>> day            26
>> svn rev        76424
>> language       R
>> version.string R version 3.6.0 (2019-04-26)
>> nickname       Planting of a Tree
>>
>> #########################################################################
>>
>> NOTE: Question on SO two days ago
>> (
>> https://stackoverflow.com/questions/55893189/passing-formula-as-parameter-to-lm-within-sapply-causes-error-bug-confirmation)
>>
>> brought many views but neither answer nor bug confirmation.
>>
>>
>>
>>
>> ------------------------------
>>
>> Message: 6
>> Date: Mon, 29 Apr 2019 21:38:00 +0300
>> From: Haddison Mureithi <mureithihaddison at gmail.com>
>> To: r-help at r-project.org
>> Subject: [R] (no subject)
>> Message-ID:
>>          <CABVwvn6y_M2M1o41HryKYp=
>> LQcbsajdtginyw_RPVf81o4BmqQ at mail.gmail.com>
>> Content-Type: text/plain; charset="utf-8"
>>
>> Hello guys this problem was never answered and I happened to come across
>> the same problem , kindly help. This is a simple R program that I have been
>> trying to run. I keep running into the "singular matrix" error. I end up
>> with no sensible results. Can anyone suggest any changes or a way around
>> this?
>>
>> I am a total rookie when working with R.
>>
>> Thanks,
>> Rasika
>>
>>> library(survival)
>> Loading required package: splines
>>> args(coxph)
>> function (formula, data, weights, subset, na.action, init, control,
>>      method = c("efron", "breslow", "exact"), singular.ok = TRUE,
>>      robust = FALSE, model = FALSE, x = FALSE, y = TRUE, tt, ...)
>> NULL
>>> test1<-read.table("S:/FISHDO/03_Phase_I_Field_Work/Data_6_28_2011/Working
>> Folder/R_files/4SondesJuly24.csv", header=T, sep=",")
>>> sondes<-coxph(Surv(Start, Stop, Depart)~DOLoomis + DOI55 + DODamen,
>> data=test1)
>> Warning messages:
>> 1: In fitter(X, Y, strats, offset, init, control, weights = weights,  :
>>    Loglik converged before variable  1,2 ; beta may be infinite.
>> 2: In coxph(Surv(Start, Stop, Depart) ~ DOLoomis + DOI55 + DODamen,  :
>>    X matrix deemed to be singular; variable 3
>>> summary(sondes)
>> Call:
>> coxph(formula = Surv(Start, Stop, Depart) ~ DOLoomis + DOI55 +
>>      DODamen, data = test1)
>>
>>    n= 1737, number of events= 58
>>     (1 observation deleted due to missingness)
>>
>>                 coef  exp(coef)   se(coef)  z Pr(>|z|)
>> DOLoomis -2.152e+00  1.163e-01  1.161e+05  0        1
>> DOI55     4.560e-01  1.578e+00  3.755e+04  0        1
>> DODamen          NA         NA  0.000e+00 NA       NA
>>
>>           exp(coef) exp(-coef) lower .95 upper .95
>> DOLoomis    0.1163     8.5995         0       Inf
>> DOI55       1.5777     0.6338         0       Inf
>> DODamen         NA         NA        NA        NA
>>
>> Concordance= 0.5  (se = 0 )
>> Rsquare= 0   (max possible= 0.01 )
>> Likelihood ratio test= 0  on 2 df,   p=1
>> Wald test            = 0  on 2 df,   p=1
>> Score (logrank) test = 0  on 2 df,   p=1
>>
>>          [[alternative HTML version deleted]]
>>
>>
>>
>>
>> ------------------------------
>>
>> Message: 7
>> Date: Tue, 30 Apr 2019 16:50:48 +0000
>> From: Bill Poling <Bill.Poling at zelis.com>
>> To: "r-help (r-help at r-project.org)" <r-help at r-project.org>
>> Subject: [R] Help with loop for column means into new column by a
>>          subset Factor w/131 levels
>> Message-ID:
>>          <
>> BN7PR02MB50737455E93F882B58EAA4F4EA3A0 at BN7PR02MB5073.namprd02.prod.outlook.com
>>>
>>
>> Content-Type: text/plain; charset="windows-1252"
>>
>> Good afternoon.
>>
>> #RStudio Version 1.1.456
>> sessionInfo()
>> #R version 3.5.3 (2019-03-11)
>> #Platform: x86_64-w64-mingw32/x64 (64-bit)
>> #Running under: Windows >= 8 x64 (build 9200)
>>
>>
>>
>> #I have a DF of 8 columns and 14025 rows
>>
>> str(hcd2tmp2)
>>
>> # 'data.frame':14025 obs. of  8 variables:
>> # $ Submitted_Charge: num  21021 15360 40561 29495 7904 ...
>> # $ Allowed_Amt     : num  18393 6254 40561 29495 7904 ...
>> # $ Submitted_Units : num  60 240 420 45 120 215 215 15 57 2 ...
>> # $ Procedure_Code1 : Factor w/ 131 levels "A9606","J0129",..: 43 113 117
>> 125 24 85 85 90 86 25 ...
>> # $ AllowByLimit    : num  4.268 0.949 7.913 6.124 3.524 ...
>> # $ UnitsByDose     : num  600 240 420 450 120 215 215 750 570 500 ...
>> # $ LimitByUnits    : num  4310 6591 5126 4816 2243 ...
>> # $ HCPCSCodeDose1  : num  10 1 1 10 1 1 1 50 10 250 ...
>>
>> #I would like to create four additional columns that are the mean of four
>> current columns in the DF.
>> #Current columns
>> #Allowed_Amt
>> #LimitByUnits
>> #AllowByLimit
>> #UnitsByDose
>>
>> #The goal is to be able to identify rows where (for instance) Allowed_Amt
>> is greater than the average (aka outliers).
>>
>> #The trick Is I want the means of those columns based on a Factor value
>> #The Factor is:
>> #Procedure_Code1 : Factor w/ 131 levels "A9606","J0129"
>>
>> #So each of my four new columns will have 131 distinct values based on the
>> mean for the specific Procedure_Code1 grouping
>>
>> #In SQL it would look something like this:
>>
>> #SELECT *,
>> # NewCol1 = mean(Allowed_Amt) OVER (PARTITION BY Procedure_Code1),
>> # NewCol2 = mean(LimitByUnits) OVER (PARTITION BY Procedure_Code1),
>> # NewCol3 = mean(AllowByLimit) OVER (PARTITION BY Procedure_Code1),
>> # NewCol4 = mean(UnitsByDose) OVER (PARTITION BY Procedure_Code1)
>> #INTO NewTable
>> #FROM Oldtable
>>
>> #Here are some sample data
>>
>> head(hcd2tmp2, n=40)
>> #      Submitted_Charge Allowed_Amt Submitted_Units Procedure_Code1
>> AllowByLimit UnitsByDose LimitByUnits HCPCSCodeDose1
>> # 1          21020.70    18393.12              60           J1745
>> 4.2679810         600      4309.56             10
>> # 2          15360.00     6254.40             240           J9299
>> 0.9488785         240      6591.36              1
>> # 3          40561.32    40561.32             420           J9306
>> 7.9133539         420      5125.68              1
>> # 4          29495.25    29495.25              45           J9355
>> 6.1244417         450      4815.99             10
>> # 5           7904.30     7904.30             120           J0897
>> 3.5243000         120      2242.80              1
>> # 6          15331.95    10614.31             215           J9034
>> 2.0586686         215      5155.91              1
>> # 7          15331.95    10614.31             215           J9034
>> 2.0586686         215      5155.91              1
>> # 8            461.90        0.00              15           J9045
>> 0.0000000         750        46.38             50
>> # 9          27340.96    15092.21              57           J9035
>> 3.2600227         570      4629.48             10
>> # 10           768.00      576.00               2           J1190
>> 1.3617343         500       422.99            250
>> # 11           101.00       38.38               5           J2250
>>   59.9687500           5         0.64              1
>> # 12         17458.40        0.00             200           J9033
>> 0.0000000         200      5990.00              1
>> # 13          7885.10     7569.70               1           J1745
>> 105.3835445          10        71.83             10
>> # 14          2015.00     1155.78               4           J2785
>> 5.0051100           0       230.92              0
>> # 15           443.72      443.72              12           J9045
>>   11.9601078         600        37.10             50
>> # 16        113750.00   113750.00             600           J2350
>> 3.3025003         600     34443.60              1
>> # 17          3582.85     3582.85              10           J2469
>>   30.5573561         250       117.25             25
>> # 18          5152.65     5152.65              50           J2796
>> 1.4362988         500      3587.45             10
>> # 19          5152.65     5152.65              50           J2796
>> 1.4362988         500      3587.45             10
>> # 20         39664.09        0.00              74           J9355
>> 0.0000000         740      7919.63             10
>> # 21           166.71      102.53               9           J9045
>> 3.6841538         450        27.83             50
>> # 22         13823.61     9676.53               1           J2505
>> 2.0785247           6      4655.48              6
>> # 23         90954.00    26436.53             360           J1786
>> 1.7443775        3600     15155.28             10
>> # 24          4800.00     3494.40             800           J3262
>> 0.8861838         800      3943.20              1
>> # 25           216.00      105.84               4           J0696
>>   42.3360000        1000         2.50            250
>> # 26          5300.00     4770.00               1           J0178
>> 4.9677151           1       960.20              1
>> # 27         35203.00    35203.00             200           J9271
>> 3.5772498         200      9840.80              1
>> # 28         17589.15    17589.15             300           J3380
>> 2.9696855         300      5922.90              1
>> # 29         18394.64    17842.79               1           J9355
>> 166.7238834          10       107.02             10
>> # 30           770.00      731.50              10           J2469
>> 6.2388060         250       117.25             25
>> # 31           461.90        0.00              15           J9045
>> 0.0000000         750        46.38             50
>> # 32          8160.00     3342.40              80           J1459
>> 1.0260818       40000      3257.44            500
>> # 33          1653.48      314.16               6           J9305
>> 0.7661505          60       410.05             10
>> # 34         13036.50        0.00             194           J9034
>> 0.0000000         194      4652.31              1
>> # 35         10486.87        0.00             156           J9034
>> 0.0000000         156      3741.04              1
>> # 36         15360.00     6254.40             240           J9299
>> 0.9488785         240      6591.36              1
>> # 37          1616.83     1616.83             150           J1453
>> 5.2528590         150       307.80              1
>> # 38         80685.74    34772.43              96           J9035
>> 4.4597077         960      7797.02             10
>> # 39         85220.58    35925.13             287           J9299
>> 4.5577715         287      7882.17              1
>> # 40          3860.17     1627.27              13           J9299
>> 4.5577963          13       357.03              1
>>
>>
>> #I hope this is enough inforamtion to warrant your support
>> #Thank you
>> #WHP
>>
>>
>>
>> Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}
>>
>>
>>
>>
>> ------------------------------
>>
>> Message: 8
>> Date: Tue, 30 Apr 2019 18:45:40 +0000
>> From: Bill Poling <Bill.Poling at zelis.com>
>> To: "r-help (r-help at r-project.org)" <r-help at r-project.org>
>> Subject: Re: [R] Help with loop for column means into new column by a
>>          subset Factor w/131 levels
>> Message-ID:
>>          <
>> BN7PR02MB5073D732498AB265872F5750EA3A0 at BN7PR02MB5073.namprd02.prod.outlook.com
>>>
>>
>> Content-Type: text/plain; charset="windows-1252"
>>
>> I ran this routine but I was thinking there must be a more elegant way of
>> doing this.
>>
>>
>> #
>> https://community.rstudio.com/t/how-to-average-mean-variables-in-r-based-on-the-level-of-another-variable-and-save-this-as-a-new-variable/8764/8
>>
>> hcd2tmp2_summmary <- hcd2tmp2 %>%
>>    select(.) %>%
>>    group_by(Procedure_Code1) %>%
>>    summarize(average = mean(Allowed_Amt))
>> # A tibble: 131 x 2
>> # Procedure_Code1 average
>> # <fct>             <dbl>
>> # 1 A9606            57785.
>> # 2 J0129             5420.
>> # 3 J0178             4700.
>> # 4 J0180            13392.
>> # 5 J0202            56328.
>> # 6 J0256            17366.
>> # 7 J0257             7563.
>> # 8 J0485             2450.
>> # 9 J0490             6398.
>> # 10 J0585            4492.
>> # ... with 121 more rows
>>
>> hcd2tmp2 <- hcd2tmp %>%
>>    group_by(Procedure_Code1) %>%
>>    summarise(Avg_Allowed_Amt = mean(Allowed_Amt))
>>
>> view(hcd2tmp2)
>>
>>
>> hcd2tmp3 <- hcd2tmp %>%
>>    group_by(Procedure_Code1) %>%
>>    summarise(Avg_AllowByLimit = mean(AllowByLimit))
>>
>> view(hcd2tmp3)
>>
>>
>> hcd2tmp4 <- hcd2tmp %>%
>>    group_by(Procedure_Code1) %>%
>>    summarise(Avg_UnitsByDose = mean(UnitsByDose))
>>
>> view(hcd2tmp4)
>>
>> hcd2tmp5 <- hcd2tmp %>%
>>    group_by(Procedure_Code1) %>%
>>    summarise(Avg_LimitByUnits = mean(LimitByUnits))
>>
>> view(hcd2tmp5)
>>
>> #Joins----
>>
>>
>> hcd2tmp <- left_join(hcd2tmp2, hcd2tmp, by =
>> c("Procedure_Code1"="Procedure_Code1"))
>> hcd2tmp <- left_join(hcd2tmp3, hcd2tmp, by =
>> c("Procedure_Code1"="Procedure_Code1"))
>> hcd2tmp <- left_join(hcd2tmp4, hcd2tmp, by =
>> c("Procedure_Code1"="Procedure_Code1"))
>> hcd2tmp <- left_join(hcd2tmp5, hcd2tmp, by =
>> c("Procedure_Code1"="Procedure_Code1"))
>>
>> view(hcd2tmp)
>>
>> hcd2tmp$Avg_LimitByUnits <- round(hcd2tmp$Avg_LimitByUnits, digits = 2)
>> hcd2tmp$Avg_Allowed_Amt <- round(hcd2tmp$Avg_Allowed_Amt, digits = 2)
>> hcd2tmp$Avg_AllowByLimit <- round(hcd2tmp$Avg_AllowByLimit, digits = 2)
>> hcd2tmp$Avg_UnitsByDose <- round(hcd2tmp$Avg_UnitsByDose, digits = 2)
>>
>> view(hcd2tmp)
>>
>> #Over under columns----
>> hcd2tmp$AllowByLimitFlag <- hcd2tmp$AllowByLimit > hcd2tmp$Avg_AllowByLimit
>> hcd2tmp$LimitByUnitsFlag <- hcd2tmp$LimitByUnits > hcd2tmp$Avg_LimitByUnits
>> hcd2tmp$Allowed_AmtFlag  <- hcd2tmp$Allowed_Amt  > hcd2tmp$Avg_Allowed_Amt
>> hcd2tmp$UnitsByDoseFlag  <- hcd2tmp$UnitsByDose  > hcd2tmp$Avg_UnitsByDose
>>
>> view(hcd2tmp)
>>
>>
>> -----Original Message-----
>> From: Bill Poling
>> Sent: Tuesday, April 30, 2019 12:51 PM
>> To: r-help (r-help at r-project.org) <r-help at r-project.org>
>> Cc: Bill Poling <Bill.Poling at zelis.com>
>> Subject: Help with loop for column means into new column by a subset
>> Factor w/131 levels
>>
>> Good afternoon.
>>
>> #RStudio Version 1.1.456
>> sessionInfo()
>> #R version 3.5.3 (2019-03-11)
>> #Platform: x86_64-w64-mingw32/x64 (64-bit) #Running under: Windows >= 8
>> x64 (build 9200)
>>
>>
>>
>> #I have a DF of 8 columns and 14025 rows
>>
>> str(hcd2tmp2)
>>
>> # 'data.frame':14025 obs. of  8 variables:
>> # $ Submitted_Charge: num  21021 15360 40561 29495 7904 ...
>> # $ Allowed_Amt     : num  18393 6254 40561 29495 7904 ...
>> # $ Submitted_Units : num  60 240 420 45 120 215 215 15 57 2 ...
>> # $ Procedure_Code1 : Factor w/ 131 levels "A9606","J0129",..: 43 113 117
>> 125 24 85 85 90 86 25 ...
>> # $ AllowByLimit    : num  4.268 0.949 7.913 6.124 3.524 ...
>> # $ UnitsByDose     : num  600 240 420 450 120 215 215 750 570 500 ...
>> # $ LimitByUnits    : num  4310 6591 5126 4816 2243 ...
>> # $ HCPCSCodeDose1  : num  10 1 1 10 1 1 1 50 10 250 ...
>>
>> #I would like to create four additional columns that are the mean of four
>> current columns in the DF.
>> #Current columns
>> #Allowed_Amt
>> #LimitByUnits
>> #AllowByLimit
>> #UnitsByDose
>>
>> #The goal is to be able to identify rows where (for instance) Allowed_Amt
>> is greater than the average (aka outliers).
>>
>> #The trick Is I want the means of those columns based on a Factor value
>> #The Factor is:
>> #Procedure_Code1 : Factor w/ 131 levels "A9606","J0129"
>>
>> #So each of my four new columns will have 131 distinct values based on the
>> mean for the specific Procedure_Code1 grouping
>>
>> #In SQL it would look something like this:
>>
>> #SELECT *,
>> # NewCol1 = mean(Allowed_Amt) OVER (PARTITION BY Procedure_Code1),
>> # NewCol2 = mean(LimitByUnits) OVER (PARTITION BY Procedure_Code1),
>> # NewCol3 = mean(AllowByLimit) OVER (PARTITION BY Procedure_Code1),
>> # NewCol4 = mean(UnitsByDose) OVER (PARTITION BY Procedure_Code1)
>> #INTO NewTable
>> #FROM Oldtable
>>
>> #Here are some sample data
>>
>> head(hcd2tmp2, n=40)
>> #      Submitted_Charge Allowed_Amt Submitted_Units Procedure_Code1
>> AllowByLimit UnitsByDose LimitByUnits HCPCSCodeDose1
>> # 1          21020.70    18393.12              60           J1745
>> 4.2679810         600      4309.56             10
>> # 2          15360.00     6254.40             240           J9299
>> 0.9488785         240      6591.36              1
>> # 3          40561.32    40561.32             420           J9306
>> 7.9133539         420      5125.68              1
>> # 4          29495.25    29495.25              45           J9355
>> 6.1244417         450      4815.99             10
>> # 5           7904.30     7904.30             120           J0897
>> 3.5243000         120      2242.80              1
>> # 6          15331.95    10614.31             215           J9034
>> 2.0586686         215      5155.91              1
>> # 7          15331.95    10614.31             215           J9034
>> 2.0586686         215      5155.91              1
>> # 8            461.90        0.00              15           J9045
>> 0.0000000         750        46.38             50
>> # 9          27340.96    15092.21              57           J9035
>> 3.2600227         570      4629.48             10
>> # 10           768.00      576.00               2           J1190
>> 1.3617343         500       422.99            250
>> # 11           101.00       38.38               5           J2250
>>   59.9687500           5         0.64              1
>> # 12         17458.40        0.00             200           J9033
>> 0.0000000         200      5990.00              1
>> # 13          7885.10     7569.70               1           J1745
>> 105.3835445          10        71.83             10
>> # 14          2015.00     1155.78               4           J2785
>> 5.0051100           0       230.92              0
>> # 15           443.72      443.72              12           J9045
>>   11.9601078         600        37.10             50
>> # 16        113750.00   113750.00             600           J2350
>> 3.3025003         600     34443.60              1
>> # 17          3582.85     3582.85              10           J2469
>>   30.5573561         250       117.25             25
>> # 18          5152.65     5152.65              50           J2796
>> 1.4362988         500      3587.45             10
>> # 19          5152.65     5152.65              50           J2796
>> 1.4362988         500      3587.45             10
>> # 20         39664.09        0.00              74           J9355
>> 0.0000000         740      7919.63             10
>> # 21           166.71      102.53               9           J9045
>> 3.6841538         450        27.83             50
>> # 22         13823.61     9676.53               1           J2505
>> 2.0785247           6      4655.48              6
>> # 23         90954.00    26436.53             360           J1786
>> 1.7443775        3600     15155.28             10
>> # 24          4800.00     3494.40             800           J3262
>> 0.8861838         800      3943.20              1
>> # 25           216.00      105.84               4           J0696
>>   42.3360000        1000         2.50            250
>> # 26          5300.00     4770.00               1           J0178
>> 4.9677151           1       960.20              1
>> # 27         35203.00    35203.00             200           J9271
>> 3.5772498         200      9840.80              1
>> # 28         17589.15    17589.15             300           J3380
>> 2.9696855         300      5922.90              1
>> # 29         18394.64    17842.79               1           J9355
>> 166.7238834          10       107.02             10
>> # 30           770.00      731.50              10           J2469
>> 6.2388060         250       117.25             25
>> # 31           461.90        0.00              15           J9045
>> 0.0000000         750        46.38             50
>> # 32          8160.00     3342.40              80           J1459
>> 1.0260818       40000      3257.44            500
>> # 33          1653.48      314.16               6           J9305
>> 0.7661505          60       410.05             10
>> # 34         13036.50        0.00             194           J9034
>> 0.0000000         194      4652.31              1
>> # 35         10486.87        0.00             156           J9034
>> 0.0000000         156      3741.04              1
>> # 36         15360.00     6254.40             240           J9299
>> 0.9488785         240      6591.36              1
>> # 37          1616.83     1616.83             150           J1453
>> 5.2528590         150       307.80              1
>> # 38         80685.74    34772.43              96           J9035
>> 4.4597077         960      7797.02             10
>> # 39         85220.58    35925.13             287           J9299
>> 4.5577715         287      7882.17              1
>> # 40          3860.17     1627.27              13           J9299
>> 4.5577963          13       357.03              1
>>
>>
>> #I hope this is enough inforamtion to warrant your support
>> #Thank you
>> #WHP
>>
>>
>>
>> Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}
>>
>>
>>
>>
>> ------------------------------
>>
>> Message: 9
>> Date: Tue, 30 Apr 2019 15:24:57 -0400
>> From: Matthew <mccormack at molbio.mgh.harvard.edu>
>> To: "r-help (r-help at r-project.org)" <r-help at r-project.org>
>> Subject: [R] transpose and split dataframe
>> Message-ID:
>>          <0d6ac524-4291-ab03-6bcb-592b3996cc74 at molbio.mgh.harvard.edu>
>> Content-Type: text/plain; charset="utf-8"; Format="flowed"
>>
>> I have a data frame that is a lot bigger but for simplicity sake we can
>> say it looks like this:
>>
>> Regulator    hits
>> AT1G69490    AT4G31950,AT5G24110,AT1G26380,AT1G05675
>> AT2G55980    AT2G85403,AT4G89223
>>
>>      In other words:
>>
>> data.frame : 2 obs. of 2 variables
>> $Regulator: Factor w/ 2 levels
>> $hits         : Factor w/ 6 levels
>>
>>     I want to transpose it so that Regulator is now the column headings
>> and each of the AGI numbers now separated by commas is a row. So,
>> AT1G69490 is now the header of the first column and AT4G31950 is row 1
>> of column 1, AT5G24110 is row 2 of column 1, etc. AT2G55980 is header of
>> column 2 and AT2G85403 is row 1 of column 2, etc.
>>
>>     I have tried playing around with strsplit(TF2list[2:2]) and
>> strsplit(as.character(TF2list[2:2]), but I am getting nowhere.
>>
>> Matthew
>>
>>
>>
>>
>> ------------------------------
>>
>> Message: 10
>> Date: Tue, 30 Apr 2019 21:04:50 +0000
>> From: David L Carlson <dcarlson at tamu.edu>
>> To: "r-help at r-project.org" <r-help at r-project.org>, Matthew
>>          <mccormack at molbio.mgh.harvard.edu>
>> Subject: Re: [R] transpose and split dataframe
>> Message-ID: <db8cede89a724defb691cea72a25b092 at tamu.edu>
>> Content-Type: text/plain; charset="utf-8"
>>
>> I neglected to copy this to the list:
>>
>> I think we need more information. Can you give us the structure of the
>> data with str(YourDataFrame). Alternatively you could copy a small piece
>> into your email message by copying and pasting the results of the following
>> code:
>>
>> dput(head(YourDataFrame))
>>
>> The data frame you present could not be a data frame since you say "hits"
>> is a factor with a variable number of elements. If each value of "hits" was
>> a single character string, it would only have 2 factor levels not 6 and
>> your efforts to parse the string would make more sense. Transposing to a
>> data frame would only be possible if each column was padded with NAs to
>> make them equal in length. Since your example tries use the name TF2list,
>> it is possible that you do not have a data frame but a list and you have no
>> factor levels, just character vectors.
>>
>> If you are not familiar with R, it may be helpful to tell us what your
>> overall goal is rather than an intermediate step. Very likely R can easily
>> handle what you want by doing things a different way.
>>
>> ----------------------------------------
>> David L Carlson
>> Department of Anthropology
>> Texas A&M University
>> College Station, TX 77843-4352
>>
>>
>>
>> -----Original Message-----
>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Matthew
>> Sent: Tuesday, April 30, 2019 2:25 PM
>> To: r-help (r-help at r-project.org) <r-help at r-project.org>
>> Subject: [R] transpose and split dataframe
>>
>> I have a data frame that is a lot bigger but for simplicity sake we can
>> say it looks like this:
>>
>> Regulator    hits
>> AT1G69490    AT4G31950,AT5G24110,AT1G26380,AT1G05675
>> AT2G55980    AT2G85403,AT4G89223
>>
>>      In other words:
>>
>> data.frame : 2 obs. of 2 variables
>> $Regulator: Factor w/ 2 levels
>> $hits         : Factor w/ 6 levels
>>
>>     I want to transpose it so that Regulator is now the column headings
>> and each of the AGI numbers now separated by commas is a row. So,
>> AT1G69490 is now the header of the first column and AT4G31950 is row 1
>> of column 1, AT5G24110 is row 2 of column 1, etc. AT2G55980 is header of
>> column 2 and AT2G85403 is row 1 of column 2, etc.
>>
>>     I have tried playing around with strsplit(TF2list[2:2]) and
>> strsplit(as.character(TF2list[2:2]), but I am getting nowhere.
>>
>> Matthew
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>> ------------------------------
>>
>> Message: 11
>> Date: Tue, 30 Apr 2019 15:03:09 -0600
>> From: David Winsemius <dwinsemius at comcast.net>
>> To: Jens Heumann <jens.heumann at students.unibe.ch>
>> Cc: r-help at r-project.org
>> Subject: Re: [R] Passing formula as parameter to `lm` within `sapply`
>>          causes error [BUG?]
>> Message-ID: <924255D4-912E-4C24-8E85-6E313EC50203 at comcast.net>
>> Content-Type: text/plain; charset="utf-8"
>>
>> Try using do.call
>>
>> ?
>> David
>>
>> Sent from my iPhone
>>
>>> On Apr 30, 2019, at 9:24 AM, Jens Heumann <
>> jens.heumann at students.unibe.ch> wrote:
>>>
>>> Hi,
>>>
>>> `lm` won't take formula as a parameter when it is within a `sapply`; see
>> example below. Please, could anyone either point me to a syntax error or
>> confirm that this might be a bug?
>>>
>>> Best,
>>> Jens
>>>
>>> [Disclaimer: This is my first post here, following advice of how to
>> proceed with possible bugs from here: https://www.r-project.org/bugs.html]
>>>
>>>
>>> SUMMARY
>>>
>>> While `lm` alone accepts formula parameter `FO` well, the same within a
>> `sapply` causes an error. When putting everything as parameter but formula
>> `FO`, it's still working, though. All parameters work fine within a similar
>> `for` loop.
>>>
>>>
>>> MCVE (see data / R-version at bottom)
>>>
>>>> summary(lm(y ~ x, df1, df1[["z"]] == 1, df1[["w"]]))$coef[1, ]
>>>   Estimate Std. Error    t value   Pr(>|t|)
>>> 1.6269038  0.9042738  1.7991275  0.3229600
>>>> summary(lm(FO, data, data[[st]] == st1, data[[ws]]))$coef[1, ]
>>>   Estimate Std. Error    t value   Pr(>|t|)
>>> 1.6269038  0.9042738  1.7991275  0.3229600
>>>> sapply(unique(df1$z), function(s)
>>> +   summary(lm(y ~ x, df1, df1[["z"]] == s, df1[[ws]]))$coef[1, ])
>>>                 [,1]       [,2]         [,3]
>>> Estimate   1.6269038 -0.1404174 -0.010338774
>>> Std. Error 0.9042738  0.4577001  1.858138516
>>> t value    1.7991275 -0.3067890 -0.005564049
>>> Pr(>|t|)   0.3229600  0.8104951  0.996457853
>>>> sapply(unique(data[[st]]), function(s)
>>> +   summary(lm(FO, data, data[[st]] == s, data[[ws]]))$coef[1, ])  # !!!
>>> Error in eval(substitute(subset), data, env) : object 's' not found
>>>> sapply(unique(data[[st]]), function(s)
>>> +   summary(lm(y ~ x, data, data[[st]] == s, data[[ws]]))$coef[1, ])
>>>                 [,1]       [,2]         [,3]
>>> Estimate   1.6269038 -0.1404174 -0.010338774
>>> Std. Error 0.9042738  0.4577001  1.858138516
>>> t value    1.7991275 -0.3067890 -0.005564049
>>> Pr(>|t|)   0.3229600  0.8104951  0.996457853
>>>> m <- matrix(NA, 4, length(unique(data[[st]])))
>>>> for (s in unique(data[[st]])) {
>>> +   m[, s] <- summary(lm(FO, data, data[[st]] == s, data[[ws]]))$coef[1,
>> ]
>>> + }
>>>> m
>>>           [,1]       [,2]         [,3]
>>> [1,] 1.6269038 -0.1404174 -0.010338774
>>> [2,] 0.9042738  0.4577001  1.858138516
>>> [3,] 1.7991275 -0.3067890 -0.005564049
>>> [4,] 0.3229600  0.8104951  0.996457853
>>>
>>> # DATA #################################################################
>>>
>>> df1 <- structure(list(x = c(1.37095844714667, -0.564698171396089,
>> 0.363128411337339,
>>> 0.63286260496104, 0.404268323140999, -0.106124516091484,
>> 1.51152199743894,
>>> -0.0946590384130976, 2.01842371387704), y = c(1.30824434809425,
>>> 0.740171482827397, 2.64977380403845, -0.755998096151299,
>> 0.125479556323628,
>>> -0.239445852485142, 2.14747239550901, -0.37891195982917,
>> -0.638031707027734
>>> ), z = c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L), w = c(0.7, 0.8,
>>> 1.2, 0.9, 1.3, 1.2, 0.8, 1, 1)), class = "data.frame", row.names = c(NA,
>>> -9L))
>>>
>>> FO <- y ~ x; data <- df1; st <- "z"; ws <- "w"; st1 <- 1
>>>
>>> ########################################################################
>>>
>>>> R.version
>>>                _
>>> platform       x86_64-w64-mingw32
>>> arch           x86_64
>>> os             mingw32
>>> system         x86_64, mingw32
>>> status
>>> major          3
>>> minor          6.0
>>> year           2019
>>> month          04
>>> day            26
>>> svn rev        76424
>>> language       R
>>> version.string R version 3.6.0 (2019-04-26)
>>> nickname       Planting of a Tree
>>>
>>> #########################################################################
>>>
>>> NOTE: Question on SO two days ago (
>> https://stackoverflow.com/questions/55893189/passing-formula-as-parameter-to-lm-within-sapply-causes-error-bug-confirmation)
>> brought many views but neither answer nor bug confirmation.
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>>
>> ------------------------------
>>
>> Message: 12
>> Date: Tue, 30 Apr 2019 17:31:28 -0400
>> From: Matthew <mccormack at molbio.mgh.harvard.edu>
>> To: "r-help at r-project.org" <r-help at r-project.org>
>> Subject: [R] Fwd: Re:  transpose and split dataframe
>> Message-ID:
>>          <e4a9e321-b437-eed6-344b-472319e85fec at molbio.mgh.harvard.edu>
>> Content-Type: text/plain; charset="utf-8"
>>
>> Thanks for your reply. I was trying to simplify it a little, but must
>> have got it wrong. Here is the real dataframe, TF2list:
>>
>>    str(TF2list)
>> 'data.frame':    152 obs. of  2 variables:
>>    $ Regulator: Factor w/ 87 levels "AT1G02065","AT1G13960",..: 17 6 6 54
>> 54 82 82 82 82 82 ...
>>    $ hits     : Factor w/ 97 levels
>> "AT1G05675,AT3G12910,AT1G22810,AT1G14540,AT1G21120,AT1G07160,AT5G22520,AT1G56250,AT2G31345,AT5G22530,AT4G11170,A"|
>>
>> __truncated__,..: 65 57 90 57 87 57 56 91 31 17 ...
>>
>>      And the first few lines resulting from dput(head(TF2list)):
>>
>> dput(head(TF2list))
>> structure(list(Regulator = structure(c(17L, 6L, 6L, 54L, 54L,
>> 82L), .Label = c("AT1G02065", "AT1G13960", "AT1G18860", "AT1G23380",
>> "AT1G29280", "AT1G29860", "AT1G30650", "AT1G55600", "AT1G62300",
>> "AT1G62990", "AT1G64000", "AT1G66550", "AT1G66560", "AT1G66600",
>> "AT1G68150", "AT1G69310", "AT1G69490", "AT1G69810", "AT1G70510", ...
>>
>> This is another way of looking at the first 4 entries (Regulator is
>> tab-separated from hits):
>>
>> Regulator
>>     hits
>> 1
>> AT1G69490
>>
>>    AT4G31950,AT5G24110,AT1G26380,AT1G05675,AT3G12910,AT5G64905,AT1G22810,AT1G79680,AT3G02840,AT5G25260,AT5G57220,AT2G37430,AT2G26560,AT1G56250,AT3G23230,AT1G16420,AT1G78410,AT4G22030,AT5G05300,AT1G69930,AT4G03460,AT4G11470,AT5G25250,AT5G36925,AT2G30750,AT1G16150,AT1G02930,AT2G19190,AT4G11890,AT1G72520,AT4G31940,AT5G37490,AT5G52760,AT5G66020,AT3G57460,AT4G23220,AT3G15518,AT2G43620,AT2G02010,AT1G35210,AT5G46295,AT1G17147,AT1G11925,AT2G39200,AT1G02920,AT2G40180,AT1G59865,AT4G35180,AT4G15417,AT1G51820,AT1G06135,AT1G36622,AT5G42830
>> 2
>> AT1G29860
>>
>>    AT4G31950,AT5G24110,AT1G05675,AT3G12910,AT5G64905,AT1G22810,AT1G14540,AT1G79680,AT1G07160,AT3G23250,AT5G25260,AT1G53625,AT5G57220,AT2G37430,AT3G54150,AT1G56250,AT3G23230,AT1G16420,AT1G78410,AT4G22030,AT1G69930,AT4G03460,AT4G11470,AT5G25250,AT5G36925,AT4G14450,AT2G30750,AT1G16150,AT1G02930,AT2G19190,AT4G11890,AT1G72520,AT4G31940,AT5G37490,AT4G08555,AT5G66020,AT5G26920,AT3G57460,AT4G23220,AT3G15518,AT2G43620,AT1G35210,AT5G46295,AT1G17147,AT1G11925,AT2G39200,AT1G02920,AT4G35180,AT4G15417,AT1G51820,AT4G40020,AT1G06135
>>
>> 3
>> AT1G2986
>>
>>    AT5G64905,AT1G21120,AT1G07160,AT5G25260,AT1G53625,AT1G56250,AT2G31345,AT4G11170,AT1G66090,AT1G26410,AT3G55840,AT1G69930,AT4G03460,AT5G25250,AT5G36925,AT1G26420,AT5G42380,AT1G16150,AT2G22880,AT1G02930,AT4G11890,AT1G72520,AT5G66020,AT2G43620,AT2G44370,AT4G15975,AT1G35210,AT5G46295,AT1G11925,AT2G39200,AT1G02920,AT4G14370,AT4G35180,AT4G15417,AT2G18690,AT5G11140,AT1G06135,AT5G42830
>>
>>      So, the goal would be to
>>
>> first: Transpose the existing dataframe so that the factor Regulator
>> becomes a column name (column 1 name = AT1G69490, column2 name
>> AT1G29860, etc.) and the hits associated with each Regulator become
>> rows. Hits is a comma separated 'list' ( I do not not know if
>> technically it is an R list.), so it would have to be comma
>> 'unseparated' with each entry becoming a row (col 1 row 1 = AT4G31950,
>> col 1 row 2 - AT5G24410, etc); like this :
>>
>> AT1G69490
>> AT4G31950
>> AT5G24110
>> AT1G05675
>> AT5G64905
>>
>> ... I did not include all the rows)
>>
>> I think it would be best to actually make the first entry a separate
>> dataframe ( 1 column with name = AT1G69490 and number of rows depending
>> on the number of hits), then make the second column (column name =
>> AT1G29860, and number of rows depending on the number of hits) into a
>> new dataframe and do a full join of of the two dataframes; continue by
>> making the third column (column name = AT1G2986) into a dataframe and
>> full join it with the previous; continue for the 152 observations so
>> that then end result is a dataframe with 152 columns and number of rows
>> depending on the entry with the greatest number of hits. The full joins
>> I can do with dplyr, but getting up to that point seems rather difficult.
>>
>> This would get me what my ultimate goal would be; each Regulator is a
>> column name (152 columns) and a given row has either NA or the same hit.
>>
>>      This seems very difficult to me, but I appreciate any attempt.
>>
>> Matthew
>>
>> On 4/30/2019 4:34 PM, David L Carlson wrote:
>>>           External Email - Use Caution
>>>
>>> I think we need more information. Can you give us the structure of the
>> data with str(YourDataFrame). Alternatively you could copy a small piece
>> into your email message by copying and pasting the results of the following
>> code:
>>>
>>> dput(head(YourDataFrame))
>>>
>>> The data frame you present could not be a data frame since you say
>> "hits" is a factor with a variable number of elements. If each value of
>> "hits" was a single character string, it would only have 2 factor levels
>> not 6 and your efforts to parse the string would make more sense.
>> Transposing to a data frame would only be possible if each column was
>> padded with NAs to make them equal in length. Since your example tries use
>> the name TF2list, it is possible that you do not have a data frame but a
>> list and you have no factor levels, just character vectors.
>>>
>>> If you are not familiar with R, it may be helpful to tell us what your
>> overall goal is rather than an intermediate step. Very likely R can easily
>> handle what you want by doing things a different way.
>>>
>>> ----------------------------------------
>>> David L Carlson
>>> Department of Anthropology
>>> Texas A&M University
>>> College Station, TX 77843-4352
>>>
>>>
>>>
>>> -----Original Message-----
>>> From: R-help<r-help-bounces at r-project.org>  On Behalf Of Matthew
>>> Sent: Tuesday, April 30, 2019 2:25 PM
>>> To: r-help (r-help at r-project.org)<r-help at r-project.org>
>>> Subject: [R] transpose and split dataframe
>>>
>>> I have a data frame that is a lot bigger but for simplicity sake we can
>>> say it looks like this:
>>>
>>> Regulator    hits
>>> AT1G69490    AT4G31950,AT5G24110,AT1G26380,AT1G05675
>>> AT2G55980    AT2G85403,AT4G89223
>>>
>>>       In other words:
>>>
>>> data.frame : 2 obs. of 2 variables
>>> $Regulator: Factor w/ 2 levels
>>> $hits         : Factor w/ 6 levels
>>>
>>>      I want to transpose it so that Regulator is now the column headings
>>> and each of the AGI numbers now separated by commas is a row. So,
>>> AT1G69490 is now the header of the first column and AT4G31950 is row 1
>>> of column 1, AT5G24110 is row 2 of column 1, etc. AT2G55980 is header of
>>> column 2 and AT2G85403 is row 1 of column 2, etc.
>>>
>>>      I have tried playing around with strsplit(TF2list[2:2]) and
>>> strsplit(as.character(TF2list[2:2]), but I am getting nowhere.
>>>
>>> Matthew
>>>
>>> ______________________________________________
>>> R-help at r-project.org  mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guidehttp://
>> www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>          [[alternative HTML version deleted]]
>>
>>
>>
>>
>> ------------------------------
>>
>> Message: 13
>> Date: Wed, 1 May 2019 07:46:32 +1000
>> From: Jim Lemon <drjimlemon at gmail.com>
>> To: Matthew <mccormack at molbio.mgh.harvard.edu>
>> Cc: "r-help (r-help at r-project.org)" <r-help at r-project.org>
>> Subject: Re: [R] transpose and split dataframe
>> Message-ID:
>>          <CA+8X3fUjv3APb=
>> UcsNQAD61pmOSbvoYBFsW3caZW7p11eD7umg at mail.gmail.com>
>> Content-Type: text/plain; charset="utf-8"
>>
>> Hi Matthew,
>> Is this what you are trying to do?
>>
>> mmdf<-read.table(text="Regulator    hits
>> AT1G69490    AT4G31950,AT5G24110,AT1G26380,AT1G05675
>> AT2G55980    AT2G85403,AT4G89223",header=TRUE,
>> stringsAsFactors=FALSE)
>> # split the second column at the commas
>> hitsplit<-strsplit(mmdf$hits,",")
>> # define a function that will fill with NAs
>> NAfill<-function(x,n) return(x[1:n])
>> # get the maximum length of hits
>> maxlen<-max(unlist(lapply(hitsplit,length)))
>> # fill the list with NAs
>> hitsplit<-lapply(hitsplit,NAfill,maxlen)
>> # change the names of the list
>> names(hitsplit)<-mmdf$Regulator
>> # convert to a data frame
>> tmmdf<-as.data.frame(hitsplit)
>>
>> Jim
>>
>> On Wed, May 1, 2019 at 5:25 AM Matthew <mccormack at molbio.mgh.harvard.edu>
>> wrote:
>>>
>>> I have a data frame that is a lot bigger but for simplicity sake we can
>>> say it looks like this:
>>>
>>> Regulator    hits
>>> AT1G69490    AT4G31950,AT5G24110,AT1G26380,AT1G05675
>>> AT2G55980    AT2G85403,AT4G89223
>>>
>>>      In other words:
>>>
>>> data.frame : 2 obs. of 2 variables
>>> $Regulator: Factor w/ 2 levels
>>> $hits         : Factor w/ 6 levels
>>>
>>>     I want to transpose it so that Regulator is now the column headings
>>> and each of the AGI numbers now separated by commas is a row. So,
>>> AT1G69490 is now the header of the first column and AT4G31950 is row 1
>>> of column 1, AT5G24110 is row 2 of column 1, etc. AT2G55980 is header of
>>> column 2 and AT2G85403 is row 1 of column 2, etc.
>>>
>>>     I have tried playing around with strsplit(TF2list[2:2]) and
>>> strsplit(as.character(TF2list[2:2]), but I am getting nowhere.
>>>
>>> Matthew
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>>
>> ------------------------------
>>
>> Message: 14
>> Date: Wed, 1 May 2019 09:58:34 +1200
>> From: Abs Spurdle <spurdle.a at gmail.com>
>> To: =?UTF-8?Q?Catarina_Serra_Gon=C3=A7alves?= <catarinasg at gmail.com>
>> Cc: r-help <r-help at r-project.org>
>> Subject: Re: [R]  Time series (trend over time) for irregular sampling
>>          dates and multiple sites
>> Message-ID:
>>          <
>> CAB8pepxHYbCXQPX5CaUQ868kMAp80z+zSXH7LHak+xDabJOjKg at mail.gmail.com>
>> Content-Type: text/plain; charset="utf-8"
>>
>>> My data has a few problems: (1) I think I will need to fix the effects of
>>> seasonal variation (Monthly) and (2) of possible spatial correlation
>>> (probability of finding an item is higher after finding one since they
>> can
>>> come from the same ship). (3) How do I handle the fact that the
>>> measurements were not taken at a regular interval?
>>
>> Can I ask two questions:
>> (1) Is the data autocorrelated (or "Seasonal") over time?
>> If not then this problem is a lot simpler.
>> (2) Can you expand on the following statement?
>> "possible spatial correlation (probability of finding an item is higher
>> after finding one since they can come from the same ship"
>>
>>          [[alternative HTML version deleted]]
>>
>>
>>
>>
>> ------------------------------
>>
>> Message: 15
>> Date: Tue, 30 Apr 2019 22:29:24 +0000
>> From: David L Carlson <dcarlson at tamu.edu>
>> To: Matthew <mccormack at molbio.mgh.harvard.edu>, "r-help at r-project.org"
>>          <r-help at r-project.org>
>> Subject: Re: [R] Fwd: Re:  transpose and split dataframe
>> Message-ID: <1d59b3c0584a40c1b322b0efd5de7646 at tamu.edu>
>> Content-Type: text/plain; charset="utf-8"
>>
>> If you read the data frame with read.csv() or one of the other read()
>> functions, use the asis=TRUE argument to prevent conversion to factors. If
>> not do the conversion first:
>>
>> # Convert factors to characters
>> DataMatrix <- sapply(TF2list, as.character)
>> # Split the vector of hits
>> DataList <- sapply(DataMatrix[, 2], strsplit, split=",")
>> # Use the values in Regulator to name the parts of the list
>> names(DataList) <- DataMatrix[,"Regulator"]
>>
>> # Now create a data frame
>> # How long is the longest list of hits?
>> mx <- max(sapply(DataList, length))
>> # Now add NAs to vectors shorter than mx
>> DataList2 <- lapply(DataList, function(x) c(x, rep(NA, mx-length(x))))
>> # Finally convert back to a data frame
>> TF2list2 <- do.call(data.frame, DataList2)
>>
>> Try this on a portion of the list, say 25 lines and print each object to
>> see what is happening.
>>
>> ----------------------------------------
>> David L Carlson
>> Department of Anthropology
>> Texas A&M University
>> College Station, TX 77843-4352
>>
>>
>>
>>
>>
>> -----Original Message-----
>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Matthew
>> Sent: Tuesday, April 30, 2019 4:31 PM
>> To: r-help at r-project.org
>> Subject: [R] Fwd: Re: transpose and split dataframe
>>
>> Thanks for your reply. I was trying to simplify it a little, but must
>> have got it wrong. Here is the real dataframe, TF2list:
>>
>>    str(TF2list)
>> 'data.frame':    152 obs. of  2 variables:
>>    $ Regulator: Factor w/ 87 levels "AT1G02065","AT1G13960",..: 17 6 6 54
>> 54 82 82 82 82 82 ...
>>    $ hits     : Factor w/ 97 levels
>> "AT1G05675,AT3G12910,AT1G22810,AT1G14540,AT1G21120,AT1G07160,AT5G22520,AT1G56250,AT2G31345,AT5G22530,AT4G11170,A"|
>>
>> __truncated__,..: 65 57 90 57 87 57 56 91 31 17 ...
>>
>>      And the first few lines resulting from dput(head(TF2list)):
>>
>> dput(head(TF2list))
>> structure(list(Regulator = structure(c(17L, 6L, 6L, 54L, 54L,
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ---
> This email has been checked for viruses by AVG.
> https://www.avg.com
> 
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From pd@|gd @end|ng |rom gm@||@com  Fri May  3 00:45:18 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Fri, 3 May 2019 00:45:18 +0200
Subject: [R] Survuval Anaysis
In-Reply-To: <fce36861-6961-42cf-5a8f-c90220e5fd67@dewey.myzen.co.uk>
References: <mailman.354552.1.1556704801.3302.r-help@r-project.org>
 <CABVwvn65QfJzmmD7+MhgVaCWQuqFirTdC6iPoPKJpUiHxZx2NQ@mail.gmail.com>
 <fce36861-6961-42cf-5a8f-c90220e5fd67@dewey.myzen.co.uk>
Message-ID: <8B8D0644-1078-4F3A-89EA-19C6878E2D68@gmail.com>

Also, please do not include every single other message from a digested list!!

And yes, most likely there is a linear dependency between the predictors, or the 3rd one is constant. There could be other possibilities, though.

> On 2 May 2019, at 17:44 , Michael Dewey <lists at dewey.myzen.co.uk> wrote:
> 
> Without more details it is hard to answer but it is suspicious that it is dropping one of your predictors and the standard errors of the other are very large. This suggests you should investigate the joint distribution of your predictors and the events.
> 
> Michael
> 

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From drj|m|emon @end|ng |rom gm@||@com  Fri May  3 02:32:39 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 3 May 2019 10:32:39 +1000
Subject: [R] Fwd: Re: transpose and split dataframe
In-Reply-To: <df8f17304f9541cda96fbc48d969ca0e@tamu.edu>
References: <e154ccbb-71a6-c2ac-41ca-2171c8a9dc76@molbio.mgh.harvard.edu>
 <e4a9e321-b437-eed6-344b-472319e85fec@molbio.mgh.harvard.edu>
 <df8f17304f9541cda96fbc48d969ca0e@tamu.edu>
Message-ID: <CA+8X3fXJ=+iU+6fVGqfu33yy03t_Ap6jNao=Fgk10CTxg=mqiQ@mail.gmail.com>

Hi Matthew,
I'm not sure whether you want something like your initial request or
David's solution. The result of this can be transformed into the
latter:

mmdf<-read.table(text="Regulator hits
AT1G69490 AT4G31950,AT5G24110,AT1G26380,AT1G05675,AT3G12910,AT5G64905,AT1G22810,AT1G79680,AT3G02840,AT5G25260,AT5G57220,AT2G37430,AT2G26560,AT1G56250,AT3G23230,AT1G16420,AT1G78410,AT4G22030,AT5G05300,AT1G69930,AT4G03460,AT4G11470,AT5G25250,AT5G36925,AT2G30750,AT1G16150,AT1G02930,AT2G19190,AT4G11890,AT1G72520,AT4G31940,AT5G37490,AT5G52760,AT5G66020,AT3G57460,AT4G23220,AT3G15518,AT2G43620,AT2G02010,AT1G35210,AT5G46295,AT1G17147,AT1G11925,AT2G39200,AT1G02920,AT2G40180,AT1G59865,AT4G35180,AT4G15417,AT1G51820,AT1G06135,AT1G36622,AT5G42830
AT1G29860 AT4G31950,AT5G24110,AT1G05675,AT3G12910,AT5G64905,AT1G22810,AT1G14540,AT1G79680,AT1G07160,AT3G23250,AT5G25260,AT1G53625,AT5G57220,AT2G37430,AT3G54150,AT1G56250,AT3G23230,AT1G16420,AT1G78410,AT4G22030,AT1G69930,AT4G03460,AT4G11470,AT5G25250,AT5G36925,AT4G14450,AT2G30750,AT1G16150,AT1G02930,AT2G19190,AT4G11890,AT1G72520,AT4G31940,AT5G37490,AT4G08555,AT5G66020,AT5G26920,AT3G57460,AT4G23220,AT3G15518,AT2G43620,AT1G35210,AT5G46295,AT1G17147,AT1G11925,AT2G39200,AT1G02920,AT4G35180,AT4G15417,AT1G51820,AT4G40020,AT1G06135
AT1G2986 AT5G64905,AT1G21120,AT1G07160,AT5G25260,AT1G53625,AT1G56250,AT2G31345,AT4G11170,AT1G66090,AT1G26410,AT3G55840,AT1G69930,AT4G03460,AT5G25250,AT5G36925,AT1G26420,AT5G42380,AT1G16150,AT2G22880,AT1G02930,AT4G11890,AT1G72520,AT5G66020,AT2G43620,AT2G44370,AT4G15975,AT1G35210,AT5G46295,AT1G11925,AT2G39200,AT1G02920,AT4G14370,AT4G35180,AT4G15417,AT2G18690,AT5G11140,AT1G06135,AT5G42830",
header=TRUE,stringsAsFactors=FALSE)
# split the second column at the commas
hitsplit<-strsplit(mmdf$hits,",")
# define a function that will fill with NAs
NAfill<-function(x,n) return(x[1:n])
# get the maximum length of hits
maxlen<-max(unlist(lapply(hitsplit,length)))
# fill the list with NAs
hitsplit<-lapply(hitsplit,NAfill,maxlen)
# get all the sorted hits
allhits<-sort(unique(unlist(hitsplit)))
tmmdf<-as.data.frame(matrix(NA,ncol=length(hitsplit),nrow=length(allhits)))
# change the names of the list
names(tmmdf)<-mmdf$Regulator
# replace all NA values in tmmdf where they appear in hitsplit
for(column in 1:length(hitsplit)) {
 hitmatches<-match(hitsplit[[column]],allhits)
 hitmatches<-hitmatches[!is.na(hitmatches)]
 tmmdf[hitmatches,column]<-allhits[hitmatches]
}

Jim

On Fri, May 3, 2019 at 12:43 AM David L Carlson <dcarlson at tamu.edu> wrote:
>
> We still have only the toy version of your data from your first email. The second email used dput() as I suggested, but you truncated the results so it is useless for testing purposes.
>
> Use the following code after creating DataList (up to mx <- ... ) in my earlier answer:
>
> n <- sapply(DataList, length)
> hits <- unname(unlist(DataList))
> Regulator <- unname(unlist(mapply(rep, names(DataList), times=n)))
> DataTable <- table(hits, Regulator)
>
> #            Regulator
> # hits        AT1G69490 AT2G55980
> #  AT1G05675         1         0
> #  AT1G26380         1         0
> #  AT2G85403         0         1
> #  AT4G31950         1         0
> #  AT4G89223         0         1
> #  AT5G24110         1         0
>
> Now the Regulators and the hits will be listed in alphabetical order. The table has 0's for Regulators that do not have a particular hit. If you want NAs:
>
> DataTable[DataTable==0] <- NA
> print(DataTable, na.print="NA")
> #            Regulator
> # hits        AT1G69490 AT2G55980
> #   AT1G05675         1        NA
> #   AT1G26380         1        NA
> #   AT2G85403        NA         1
> #   AT4G31950         1        NA
> #   AT4G89223        NA         1
> #   AT5G24110         1        NA
>
> If you need a data frame instead of a table:
>
> as.data.frame.matrix(DataTable)
>
> ----------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77843-4352
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Matthew
> Sent: Tuesday, April 30, 2019 4:31 PM
> To: r-help at r-project.org
> Subject: [R] Fwd: Re: transpose and split dataframe
>
> Thanks for your reply. I was trying to simplify it a little, but must
> have got it wrong. Here is the real dataframe, TF2list:
>
>   str(TF2list)
> 'data.frame':    152 obs. of  2 variables:
>   $ Regulator: Factor w/ 87 levels "AT1G02065","AT1G13960",..: 17 6 6 54
> 54 82 82 82 82 82 ...
>   $ hits     : Factor w/ 97 levels
> "AT1G05675,AT3G12910,AT1G22810,AT1G14540,AT1G21120,AT1G07160,AT5G22520,AT1G56250,AT2G31345,AT5G22530,AT4G11170,A"|
> __truncated__,..: 65 57 90 57 87 57 56 91 31 17 ...
>
>     And the first few lines resulting from dput(head(TF2list)):
>
> dput(head(TF2list))
> structure(list(Regulator = structure(c(17L, 6L, 6L, 54L, 54L,
> 82L), .Label = c("AT1G02065", "AT1G13960", "AT1G18860", "AT1G23380",
> "AT1G29280", "AT1G29860", "AT1G30650", "AT1G55600", "AT1G62300",
> "AT1G62990", "AT1G64000", "AT1G66550", "AT1G66560", "AT1G66600",
> "AT1G68150", "AT1G69310", "AT1G69490", "AT1G69810", "AT1G70510", ...
>
> This is another way of looking at the first 4 entries (Regulator is
> tab-separated from hits):
>
> Regulator
>    hits
> 1
> AT1G69490
>   AT4G31950,AT5G24110,AT1G26380,AT1G05675,AT3G12910,AT5G64905,AT1G22810,AT1G79680,AT3G02840,AT5G25260,AT5G57220,AT2G37430,AT2G26560,AT1G56250,AT3G23230,AT1G16420,AT1G78410,AT4G22030,AT5G05300,AT1G69930,AT4G03460,AT4G11470,AT5G25250,AT5G36925,AT2G30750,AT1G16150,AT1G02930,AT2G19190,AT4G11890,AT1G72520,AT4G31940,AT5G37490,AT5G52760,AT5G66020,AT3G57460,AT4G23220,AT3G15518,AT2G43620,AT2G02010,AT1G35210,AT5G46295,AT1G17147,AT1G11925,AT2G39200,AT1G02920,AT2G40180,AT1G59865,AT4G35180,AT4G15417,AT1G51820,AT1G06135,AT1G36622,AT5G42830
> 2
> AT1G29860
>   AT4G31950,AT5G24110,AT1G05675,AT3G12910,AT5G64905,AT1G22810,AT1G14540,AT1G79680,AT1G07160,AT3G23250,AT5G25260,AT1G53625,AT5G57220,AT2G37430,AT3G54150,AT1G56250,AT3G23230,AT1G16420,AT1G78410,AT4G22030,AT1G69930,AT4G03460,AT4G11470,AT5G25250,AT5G36925,AT4G14450,AT2G30750,AT1G16150,AT1G02930,AT2G19190,AT4G11890,AT1G72520,AT4G31940,AT5G37490,AT4G08555,AT5G66020,AT5G26920,AT3G57460,AT4G23220,AT3G15518,AT2G43620,AT1G35210,AT5G46295,AT1G17147,AT1G11925,AT2G39200,AT1G02920,AT4G35180,AT4G15417,AT1G51820,AT4G40020,AT1G06135
>
> 3
> AT1G2986
>   AT5G64905,AT1G21120,AT1G07160,AT5G25260,AT1G53625,AT1G56250,AT2G31345,AT4G11170,AT1G66090,AT1G26410,AT3G55840,AT1G69930,AT4G03460,AT5G25250,AT5G36925,AT1G26420,AT5G42380,AT1G16150,AT2G22880,AT1G02930,AT4G11890,AT1G72520,AT5G66020,AT2G43620,AT2G44370,AT4G15975,AT1G35210,AT5G46295,AT1G11925,AT2G39200,AT1G02920,AT4G14370,AT4G35180,AT4G15417,AT2G18690,AT5G11140,AT1G06135,AT5G42830
>
>     So, the goal would be to
>
> first: Transpose the existing dataframe so that the factor Regulator
> becomes a column name (column 1 name = AT1G69490, column2 name
> AT1G29860, etc.) and the hits associated with each Regulator become
> rows. Hits is a comma separated 'list' ( I do not not know if
> technically it is an R list.), so it would have to be comma
> 'unseparated' with each entry becoming a row (col 1 row 1 = AT4G31950,
> col 1 row 2 - AT5G24410, etc); like this :
>
> AT1G69490
> AT4G31950
> AT5G24110
> AT1G05675
> AT5G64905
>
> ... I did not include all the rows)
>
> I think it would be best to actually make the first entry a separate
> dataframe ( 1 column with name = AT1G69490 and number of rows depending
> on the number of hits), then make the second column (column name =
> AT1G29860, and number of rows depending on the number of hits) into a
> new dataframe and do a full join of of the two dataframes; continue by
> making the third column (column name = AT1G2986) into a dataframe and
> full join it with the previous; continue for the 152 observations so
> that then end result is a dataframe with 152 columns and number of rows
> depending on the entry with the greatest number of hits. The full joins
> I can do with dplyr, but getting up to that point seems rather difficult.
>
> This would get me what my ultimate goal would be; each Regulator is a
> column name (152 columns) and a given row has either NA or the same hit.
>
>     This seems very difficult to me, but I appreciate any attempt.
>
> Matthew
>
> On 4/30/2019 4:34 PM, David L Carlson wrote:
> >          External Email - Use Caution
> >
> > I think we need more information. Can you give us the structure of the data with str(YourDataFrame). Alternatively you could copy a small piece into your email message by copying and pasting the results of the following code:
> >
> > dput(head(YourDataFrame))
> >
> > The data frame you present could not be a data frame since you say "hits" is a factor with a variable number of elements. If each value of "hits" was a single character string, it would only have 2 factor levels not 6 and your efforts to parse the string would make more sense. Transposing to a data frame would only be possible if each column was padded with NAs to make them equal in length. Since your example tries use the name TF2list, it is possible that you do not have a data frame but a list and you have no factor levels, just character vectors.
> >
> > If you are not familiar with R, it may be helpful to tell us what your overall goal is rather than an intermediate step. Very likely R can easily handle what you want by doing things a different way.
> >
> > ----------------------------------------
> > David L Carlson
> > Department of Anthropology
> > Texas A&M University
> > College Station, TX 77843-4352
> >
> >
> >
> > -----Original Message-----
> > From: R-help<r-help-bounces at r-project.org>  On Behalf Of Matthew
> > Sent: Tuesday, April 30, 2019 2:25 PM
> > To: r-help (r-help at r-project.org)<r-help at r-project.org>
> > Subject: [R] transpose and split dataframe
> >
> > I have a data frame that is a lot bigger but for simplicity sake we can
> > say it looks like this:
> >
> > Regulator    hits
> > AT1G69490    AT4G31950,AT5G24110,AT1G26380,AT1G05675
> > AT2G55980    AT2G85403,AT4G89223
> >
> >      In other words:
> >
> > data.frame : 2 obs. of 2 variables
> > $Regulator: Factor w/ 2 levels
> > $hits         : Factor w/ 6 levels
> >
> >     I want to transpose it so that Regulator is now the column headings
> > and each of the AGI numbers now separated by commas is a row. So,
> > AT1G69490 is now the header of the first column and AT4G31950 is row 1
> > of column 1, AT5G24110 is row 2 of column 1, etc. AT2G55980 is header of
> > column 2 and AT2G85403 is row 1 of column 2, etc.
> >
> >     I have tried playing around with strsplit(TF2list[2:2]) and
> > strsplit(as.character(TF2list[2:2]), but I am getting nowhere.
> >
> > Matthew
> >
> > ______________________________________________
> > R-help at r-project.org  mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guidehttp://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drj|m|emon @end|ng |rom gm@||@com  Fri May  3 02:40:12 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 3 May 2019 10:40:12 +1000
Subject: [R] Fwd: Re: transpose and split dataframe
In-Reply-To: <CA+8X3fXJ=+iU+6fVGqfu33yy03t_Ap6jNao=Fgk10CTxg=mqiQ@mail.gmail.com>
References: <e154ccbb-71a6-c2ac-41ca-2171c8a9dc76@molbio.mgh.harvard.edu>
 <e4a9e321-b437-eed6-344b-472319e85fec@molbio.mgh.harvard.edu>
 <df8f17304f9541cda96fbc48d969ca0e@tamu.edu>
 <CA+8X3fXJ=+iU+6fVGqfu33yy03t_Ap6jNao=Fgk10CTxg=mqiQ@mail.gmail.com>
Message-ID: <CA+8X3fX1nS_bD8cfW+KH8iOK5TA3ft2ZwFFDWUh5xMwhRx-bJg@mail.gmail.com>

Hi again,
Just noticed that the NA fill in the original solution is unnecessary, thus:

# split the second column at the commas
hitsplit<-strsplit(mmdf$hits,",")
# get all the sorted hits
allhits<-sort(unique(unlist(hitsplit)))
tmmdf<-as.data.frame(matrix(NA,ncol=length(hitsplit),nrow=length(allhits)))
# change the names of the list
names(tmmdf)<-mmdf$Regulator
for(column in 1:length(hitsplit)) {
 hitmatches<-match(hitsplit[[column]],allhits)
 hitmatches<-hitmatches[!is.na(hitmatches)]
 tmmdf[hitmatches,column]<-allhits[hitmatches]
}

Jim

On Fri, May 3, 2019 at 10:32 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi Matthew,
> I'm not sure whether you want something like your initial request or
> David's solution. The result of this can be transformed into the
> latter:
>
> mmdf<-read.table(text="Regulator hits
> AT1G69490 AT4G31950,AT5G24110,AT1G26380,AT1G05675,AT3G12910,AT5G64905,AT1G22810,AT1G79680,AT3G02840,AT5G25260,AT5G57220,AT2G37430,AT2G26560,AT1G56250,AT3G23230,AT1G16420,AT1G78410,AT4G22030,AT5G05300,AT1G69930,AT4G03460,AT4G11470,AT5G25250,AT5G36925,AT2G30750,AT1G16150,AT1G02930,AT2G19190,AT4G11890,AT1G72520,AT4G31940,AT5G37490,AT5G52760,AT5G66020,AT3G57460,AT4G23220,AT3G15518,AT2G43620,AT2G02010,AT1G35210,AT5G46295,AT1G17147,AT1G11925,AT2G39200,AT1G02920,AT2G40180,AT1G59865,AT4G35180,AT4G15417,AT1G51820,AT1G06135,AT1G36622,AT5G42830
> AT1G29860 AT4G31950,AT5G24110,AT1G05675,AT3G12910,AT5G64905,AT1G22810,AT1G14540,AT1G79680,AT1G07160,AT3G23250,AT5G25260,AT1G53625,AT5G57220,AT2G37430,AT3G54150,AT1G56250,AT3G23230,AT1G16420,AT1G78410,AT4G22030,AT1G69930,AT4G03460,AT4G11470,AT5G25250,AT5G36925,AT4G14450,AT2G30750,AT1G16150,AT1G02930,AT2G19190,AT4G11890,AT1G72520,AT4G31940,AT5G37490,AT4G08555,AT5G66020,AT5G26920,AT3G57460,AT4G23220,AT3G15518,AT2G43620,AT1G35210,AT5G46295,AT1G17147,AT1G11925,AT2G39200,AT1G02920,AT4G35180,AT4G15417,AT1G51820,AT4G40020,AT1G06135
> AT1G2986 AT5G64905,AT1G21120,AT1G07160,AT5G25260,AT1G53625,AT1G56250,AT2G31345,AT4G11170,AT1G66090,AT1G26410,AT3G55840,AT1G69930,AT4G03460,AT5G25250,AT5G36925,AT1G26420,AT5G42380,AT1G16150,AT2G22880,AT1G02930,AT4G11890,AT1G72520,AT5G66020,AT2G43620,AT2G44370,AT4G15975,AT1G35210,AT5G46295,AT1G11925,AT2G39200,AT1G02920,AT4G14370,AT4G35180,AT4G15417,AT2G18690,AT5G11140,AT1G06135,AT5G42830",
> header=TRUE,stringsAsFactors=FALSE)
> # split the second column at the commas
> hitsplit<-strsplit(mmdf$hits,",")
> # define a function that will fill with NAs
> NAfill<-function(x,n) return(x[1:n])
> # get the maximum length of hits
> maxlen<-max(unlist(lapply(hitsplit,length)))
> # fill the list with NAs
> hitsplit<-lapply(hitsplit,NAfill,maxlen)
> # get all the sorted hits
> allhits<-sort(unique(unlist(hitsplit)))
> tmmdf<-as.data.frame(matrix(NA,ncol=length(hitsplit),nrow=length(allhits)))
> # change the names of the list
> names(tmmdf)<-mmdf$Regulator
> # replace all NA values in tmmdf where they appear in hitsplit
> for(column in 1:length(hitsplit)) {
>  hitmatches<-match(hitsplit[[column]],allhits)
>  hitmatches<-hitmatches[!is.na(hitmatches)]
>  tmmdf[hitmatches,column]<-allhits[hitmatches]
> }
>
> Jim
>
> On Fri, May 3, 2019 at 12:43 AM David L Carlson <dcarlson at tamu.edu> wrote:
> >
> > We still have only the toy version of your data from your first email. The second email used dput() as I suggested, but you truncated the results so it is useless for testing purposes.
> >
> > Use the following code after creating DataList (up to mx <- ... ) in my earlier answer:
> >
> > n <- sapply(DataList, length)
> > hits <- unname(unlist(DataList))
> > Regulator <- unname(unlist(mapply(rep, names(DataList), times=n)))
> > DataTable <- table(hits, Regulator)
> >
> > #            Regulator
> > # hits        AT1G69490 AT2G55980
> > #  AT1G05675         1         0
> > #  AT1G26380         1         0
> > #  AT2G85403         0         1
> > #  AT4G31950         1         0
> > #  AT4G89223         0         1
> > #  AT5G24110         1         0
> >
> > Now the Regulators and the hits will be listed in alphabetical order. The table has 0's for Regulators that do not have a particular hit. If you want NAs:
> >
> > DataTable[DataTable==0] <- NA
> > print(DataTable, na.print="NA")
> > #            Regulator
> > # hits        AT1G69490 AT2G55980
> > #   AT1G05675         1        NA
> > #   AT1G26380         1        NA
> > #   AT2G85403        NA         1
> > #   AT4G31950         1        NA
> > #   AT4G89223        NA         1
> > #   AT5G24110         1        NA
> >
> > If you need a data frame instead of a table:
> >
> > as.data.frame.matrix(DataTable)
> >
> > ----------------------------------------
> > David L Carlson
> > Department of Anthropology
> > Texas A&M University
> > College Station, TX 77843-4352
> >
> > -----Original Message-----
> > From: R-help <r-help-bounces at r-project.org> On Behalf Of Matthew
> > Sent: Tuesday, April 30, 2019 4:31 PM
> > To: r-help at r-project.org
> > Subject: [R] Fwd: Re: transpose and split dataframe
> >
> > Thanks for your reply. I was trying to simplify it a little, but must
> > have got it wrong. Here is the real dataframe, TF2list:
> >
> >   str(TF2list)
> > 'data.frame':    152 obs. of  2 variables:
> >   $ Regulator: Factor w/ 87 levels "AT1G02065","AT1G13960",..: 17 6 6 54
> > 54 82 82 82 82 82 ...
> >   $ hits     : Factor w/ 97 levels
> > "AT1G05675,AT3G12910,AT1G22810,AT1G14540,AT1G21120,AT1G07160,AT5G22520,AT1G56250,AT2G31345,AT5G22530,AT4G11170,A"|
> > __truncated__,..: 65 57 90 57 87 57 56 91 31 17 ...
> >
> >     And the first few lines resulting from dput(head(TF2list)):
> >
> > dput(head(TF2list))
> > structure(list(Regulator = structure(c(17L, 6L, 6L, 54L, 54L,
> > 82L), .Label = c("AT1G02065", "AT1G13960", "AT1G18860", "AT1G23380",
> > "AT1G29280", "AT1G29860", "AT1G30650", "AT1G55600", "AT1G62300",
> > "AT1G62990", "AT1G64000", "AT1G66550", "AT1G66560", "AT1G66600",
> > "AT1G68150", "AT1G69310", "AT1G69490", "AT1G69810", "AT1G70510", ...
> >
> > This is another way of looking at the first 4 entries (Regulator is
> > tab-separated from hits):
> >
> > Regulator
> >    hits
> > 1
> > AT1G69490
> >   AT4G31950,AT5G24110,AT1G26380,AT1G05675,AT3G12910,AT5G64905,AT1G22810,AT1G79680,AT3G02840,AT5G25260,AT5G57220,AT2G37430,AT2G26560,AT1G56250,AT3G23230,AT1G16420,AT1G78410,AT4G22030,AT5G05300,AT1G69930,AT4G03460,AT4G11470,AT5G25250,AT5G36925,AT2G30750,AT1G16150,AT1G02930,AT2G19190,AT4G11890,AT1G72520,AT4G31940,AT5G37490,AT5G52760,AT5G66020,AT3G57460,AT4G23220,AT3G15518,AT2G43620,AT2G02010,AT1G35210,AT5G46295,AT1G17147,AT1G11925,AT2G39200,AT1G02920,AT2G40180,AT1G59865,AT4G35180,AT4G15417,AT1G51820,AT1G06135,AT1G36622,AT5G42830
> > 2
> > AT1G29860
> >   AT4G31950,AT5G24110,AT1G05675,AT3G12910,AT5G64905,AT1G22810,AT1G14540,AT1G79680,AT1G07160,AT3G23250,AT5G25260,AT1G53625,AT5G57220,AT2G37430,AT3G54150,AT1G56250,AT3G23230,AT1G16420,AT1G78410,AT4G22030,AT1G69930,AT4G03460,AT4G11470,AT5G25250,AT5G36925,AT4G14450,AT2G30750,AT1G16150,AT1G02930,AT2G19190,AT4G11890,AT1G72520,AT4G31940,AT5G37490,AT4G08555,AT5G66020,AT5G26920,AT3G57460,AT4G23220,AT3G15518,AT2G43620,AT1G35210,AT5G46295,AT1G17147,AT1G11925,AT2G39200,AT1G02920,AT4G35180,AT4G15417,AT1G51820,AT4G40020,AT1G06135
> >
> > 3
> > AT1G2986
> >   AT5G64905,AT1G21120,AT1G07160,AT5G25260,AT1G53625,AT1G56250,AT2G31345,AT4G11170,AT1G66090,AT1G26410,AT3G55840,AT1G69930,AT4G03460,AT5G25250,AT5G36925,AT1G26420,AT5G42380,AT1G16150,AT2G22880,AT1G02930,AT4G11890,AT1G72520,AT5G66020,AT2G43620,AT2G44370,AT4G15975,AT1G35210,AT5G46295,AT1G11925,AT2G39200,AT1G02920,AT4G14370,AT4G35180,AT4G15417,AT2G18690,AT5G11140,AT1G06135,AT5G42830
> >
> >     So, the goal would be to
> >
> > first: Transpose the existing dataframe so that the factor Regulator
> > becomes a column name (column 1 name = AT1G69490, column2 name
> > AT1G29860, etc.) and the hits associated with each Regulator become
> > rows. Hits is a comma separated 'list' ( I do not not know if
> > technically it is an R list.), so it would have to be comma
> > 'unseparated' with each entry becoming a row (col 1 row 1 = AT4G31950,
> > col 1 row 2 - AT5G24410, etc); like this :
> >
> > AT1G69490
> > AT4G31950
> > AT5G24110
> > AT1G05675
> > AT5G64905
> >
> > ... I did not include all the rows)
> >
> > I think it would be best to actually make the first entry a separate
> > dataframe ( 1 column with name = AT1G69490 and number of rows depending
> > on the number of hits), then make the second column (column name =
> > AT1G29860, and number of rows depending on the number of hits) into a
> > new dataframe and do a full join of of the two dataframes; continue by
> > making the third column (column name = AT1G2986) into a dataframe and
> > full join it with the previous; continue for the 152 observations so
> > that then end result is a dataframe with 152 columns and number of rows
> > depending on the entry with the greatest number of hits. The full joins
> > I can do with dplyr, but getting up to that point seems rather difficult.
> >
> > This would get me what my ultimate goal would be; each Regulator is a
> > column name (152 columns) and a given row has either NA or the same hit.
> >
> >     This seems very difficult to me, but I appreciate any attempt.
> >
> > Matthew
> >
> > On 4/30/2019 4:34 PM, David L Carlson wrote:
> > >          External Email - Use Caution
> > >
> > > I think we need more information. Can you give us the structure of the data with str(YourDataFrame). Alternatively you could copy a small piece into your email message by copying and pasting the results of the following code:
> > >
> > > dput(head(YourDataFrame))
> > >
> > > The data frame you present could not be a data frame since you say "hits" is a factor with a variable number of elements. If each value of "hits" was a single character string, it would only have 2 factor levels not 6 and your efforts to parse the string would make more sense. Transposing to a data frame would only be possible if each column was padded with NAs to make them equal in length. Since your example tries use the name TF2list, it is possible that you do not have a data frame but a list and you have no factor levels, just character vectors.
> > >
> > > If you are not familiar with R, it may be helpful to tell us what your overall goal is rather than an intermediate step. Very likely R can easily handle what you want by doing things a different way.
> > >
> > > ----------------------------------------
> > > David L Carlson
> > > Department of Anthropology
> > > Texas A&M University
> > > College Station, TX 77843-4352
> > >
> > >
> > >
> > > -----Original Message-----
> > > From: R-help<r-help-bounces at r-project.org>  On Behalf Of Matthew
> > > Sent: Tuesday, April 30, 2019 2:25 PM
> > > To: r-help (r-help at r-project.org)<r-help at r-project.org>
> > > Subject: [R] transpose and split dataframe
> > >
> > > I have a data frame that is a lot bigger but for simplicity sake we can
> > > say it looks like this:
> > >
> > > Regulator    hits
> > > AT1G69490    AT4G31950,AT5G24110,AT1G26380,AT1G05675
> > > AT2G55980    AT2G85403,AT4G89223
> > >
> > >      In other words:
> > >
> > > data.frame : 2 obs. of 2 variables
> > > $Regulator: Factor w/ 2 levels
> > > $hits         : Factor w/ 6 levels
> > >
> > >     I want to transpose it so that Regulator is now the column headings
> > > and each of the AGI numbers now separated by commas is a row. So,
> > > AT1G69490 is now the header of the first column and AT4G31950 is row 1
> > > of column 1, AT5G24110 is row 2 of column 1, etc. AT2G55980 is header of
> > > column 2 and AT2G85403 is row 1 of column 2, etc.
> > >
> > >     I have tried playing around with strsplit(TF2list[2:2]) and
> > > strsplit(as.character(TF2list[2:2]), but I am getting nowhere.
> > >
> > > Matthew
> > >
> > > ______________________________________________
> > > R-help at r-project.org  mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guidehttp://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From m@rce|om@r|@no@||v@ @end|ng |rom gm@||@com  Fri May  3 15:31:35 2019
From: m@rce|om@r|@no@||v@ @end|ng |rom gm@||@com (Marcelo Mariano Silva)
Date: Fri, 3 May 2019 10:31:35 -0300
Subject: [R] Loading EDF files
In-Reply-To: <CANZDV5Aoi-bNpW80cT9Q+pjrVz-9C-xw4EvBTdYojOGXvrgg-Q@mail.gmail.com>
References: <ce8f0350-f042-94e6-d304-2e475b9e6a3e@ualg.pt>
 <CANZDV5Aoi-bNpW80cT9Q+pjrVz-9C-xw4EvBTdYojOGXvrgg-Q@mail.gmail.com>
Message-ID: <CANZDV5D-S6TvVwYS+1cbScvcdh4BryAJvQDW3dC8ZfZLZkjiTQ@mail.gmail.com>

Dear All,

I have tried to load an EDF file using the code below and I received the
following massage :

Error in signals[[sn]]$signal[nextInCSignal[sn]:lastOne] <- samples[[sn]] :

replacement has length zero

What this massage means?
Is there any problem with the EDF file?

Code used:
hdr <- readEdfHeader("/lasse/neurobit/edf/medidas/g1/G1medida1.edf");


df_id1 <- readEdfSignals(hdr, signals = "All", from = 0, till = Inf,
physical = TRUE, fragments = TRUE, recordStarts = TRUE, mergeASignals =
TRUE, simplify = TRUE);


Any help would be highly appreciated.


Mariano


>
>

	[[alternative HTML version deleted]]


From er|cjberger @end|ng |rom gm@||@com  Fri May  3 18:52:46 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Fri, 3 May 2019 19:52:46 +0300
Subject: [R] Loading EDF files
In-Reply-To: <CANZDV5D-S6TvVwYS+1cbScvcdh4BryAJvQDW3dC8ZfZLZkjiTQ@mail.gmail.com>
References: <ce8f0350-f042-94e6-d304-2e475b9e6a3e@ualg.pt>
 <CANZDV5Aoi-bNpW80cT9Q+pjrVz-9C-xw4EvBTdYojOGXvrgg-Q@mail.gmail.com>
 <CANZDV5D-S6TvVwYS+1cbScvcdh4BryAJvQDW3dC8ZfZLZkjiTQ@mail.gmail.com>
Message-ID: <CAGgJW77AKTcXdoMAL1iSeCCMLaAWKfe==VNp6SX-uZvSEATwjw@mail.gmail.com>

Hi Mariano,
The problem appears to be that samples[[sn]] has zero length (or is NULL).
Consider the following code which gives the same error message.

x <- 1:10
x[1:5] <- as.numeric(NULL)
Error in x[1:5] <- as.numeric(NULL) : replacement has length zero

HTH,
Eric

On Fri, May 3, 2019 at 4:32 PM Marcelo Mariano Silva <
marcelomarianosilva at gmail.com> wrote:

> Dear All,
>
> I have tried to load an EDF file using the code below and I received the
> following massage :
>
> Error in signals[[sn]]$signal[nextInCSignal[sn]:lastOne] <- samples[[sn]] :
>
> replacement has length zero
>
> What this massage means?
> Is there any problem with the EDF file?
>
> Code used:
> hdr <- readEdfHeader("/lasse/neurobit/edf/medidas/g1/G1medida1.edf");
>
>
> df_id1 <- readEdfSignals(hdr, signals = "All", from = 0, till = Inf,
> physical = TRUE, fragments = TRUE, recordStarts = TRUE, mergeASignals =
> TRUE, simplify = TRUE);
>
>
> Any help would be highly appreciated.
>
>
> Mariano
>
>
> >
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Fri May  3 19:44:29 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 3 May 2019 10:44:29 -0700
Subject: [R] Loading EDF files
In-Reply-To: <CAGgJW77AKTcXdoMAL1iSeCCMLaAWKfe==VNp6SX-uZvSEATwjw@mail.gmail.com>
References: <ce8f0350-f042-94e6-d304-2e475b9e6a3e@ualg.pt>
 <CANZDV5Aoi-bNpW80cT9Q+pjrVz-9C-xw4EvBTdYojOGXvrgg-Q@mail.gmail.com>
 <CANZDV5D-S6TvVwYS+1cbScvcdh4BryAJvQDW3dC8ZfZLZkjiTQ@mail.gmail.com>
 <CAGgJW77AKTcXdoMAL1iSeCCMLaAWKfe==VNp6SX-uZvSEATwjw@mail.gmail.com>
Message-ID: <CAGxFJbQ70o0KrH9keL0Cer4VhHJt+08QCnOfJngJNz6mAF0ohA@mail.gmail.com>

Try samples[["sn"]]     ?


On Fri, May 3, 2019, 9:53 AM Eric Berger <ericjberger at gmail.com> wrote:

> Hi Mariano,
> The problem appears to be that samples[[sn]] has zero length (or is NULL).
> Consider the following code which gives the same error message.
>
> x <- 1:10
> x[1:5] <- as.numeric(NULL)
> Error in x[1:5] <- as.numeric(NULL) : replacement has length zero
>
> HTH,
> Eric
>
> On Fri, May 3, 2019 at 4:32 PM Marcelo Mariano Silva <
> marcelomarianosilva at gmail.com> wrote:
>
> > Dear All,
> >
> > I have tried to load an EDF file using the code below and I received the
> > following massage :
> >
> > Error in signals[[sn]]$signal[nextInCSignal[sn]:lastOne] <-
> samples[[sn]] :
> >
> > replacement has length zero
> >
> > What this massage means?
> > Is there any problem with the EDF file?
> >
> > Code used:
> > hdr <- readEdfHeader("/lasse/neurobit/edf/medidas/g1/G1medida1.edf");
> >
> >
> > df_id1 <- readEdfSignals(hdr, signals = "All", from = 0, till = Inf,
> > physical = TRUE, fragments = TRUE, recordStarts = TRUE, mergeASignals =
> > TRUE, simplify = TRUE);
> >
> >
> > Any help would be highly appreciated.
> >
> >
> > Mariano
> >
> >
> > >
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@rn@@w@g|ey @end|ng |rom gm@||@com  Fri May  3 21:24:09 2019
From: m@rn@@w@g|ey @end|ng |rom gm@||@com (Marna Wagley)
Date: Fri, 3 May 2019 12:24:09 -0700
Subject: [R] create a table for time difference (from t3 to t1, so on..)
Message-ID: <CAMwU6B0g8LBr_MiYVoTWksz0PB-DSZ6fft5hAebkuiDjjfXjZQ@mail.gmail.com>

Hi R User.
I have a date set in which I wanted to find the duration (period)  between
released time and detection time for each individual. Is there any simplest
way to create a matrix?
I appreciate your help.

Thanks,
MW
--------------
Here is the example data,

dAT<-structure(list(Id = structure(c(1L, 2L, 3L, 4L, 1L, 2L, 5L, 4L,

1L, 6L, 6L, 3L, 4L, 1L), .Label = c("a", "b", "c", "e", "f",

"m"), class = "factor"), date = structure(c(1L, 1L, 1L, 5L, 4L,

1L, 5L, 5L, 3L, 1L, 2L, 7L, 6L, 8L), .Label = c("1-Jan-18", "1-Jul-18",

"10-Mar-18", "15-Jan-18", "2-Jan-18", "27-Jan-18", "3-Jan-18",

"7-Mar-19"), class = "factor")), .Names = c("Id", "date"), class =
"data.frame", row.names = c(NA,

-14L))


#and the results looks like


Result<-structure(list(Id = structure(1:6, .Label = c("a", "b", "c",

"e", "f", "m"), class = "factor"), intial.released.date = structure(c(1L,

1L, 1L, 2L, 2L, 1L), .Label = c("1-Jan-18", "2-Jan-18"), class = "factor"),

    duration.between.t2.and.t1 = c(14L, 0L, 2L, 0L, NA, 181L),

    duration.between.t3.and.t1 = c(68L, NA, NA, 25L, NA, NA),

    duration.between.t4.and.t1 = c(430L, NA, NA, NA, NA, NA),

    duration.between.t3.and.t2 = c(54L, NA, NA, NA, NA, NA),

    duration.between.t4.and.t2 = c(416L, NA, NA, NA, NA, NA)), .Names = c(
"Id",

"intial.released.date", "duration.between.t2.and.t1",
"duration.between.t3.and.t1",

"duration.between.t4.and.t1", "duration.between.t3.and.t2",
"duration.between.t4.and.t2"

), class = "data.frame", row.names = c(NA, -6L))

	[[alternative HTML version deleted]]


From p@u|bern@|07 @end|ng |rom gm@||@com  Fri May  3 22:49:48 2019
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Fri, 3 May 2019 15:49:48 -0500
Subject: [R] stukel function unavailable for some odd reason
Message-ID: <CAMOcQfM9GjGRSvXnuMTUxxeoqaDOnp=7uaz1cyHUdF0kTkRa8g@mail.gmail.com>

Dear friends,

I have been fitting a logistic regression and wanted to try a couple of
goodness of fit tests on the model.

Doing some research, I came across Chris Dardi's stukel and logiGOF
functions from package LogisticDx v0.1.

I tried installing package LogisticDx in different R versions without any
success.

I first tried installing LogisticDx package in R version 3.5.3 (for windows
64-bit OS) and this is what happened:

> install.packages("LogisticDx")
Installing package into 'C:/Users/PaulBernal/Documents/R/win-library/3.5'
(as 'lib' is unspecified)
--- Please select a CRAN mirror for use in this session ---
also installing the dependencies 'polspline', 'rms', 'speedglm', 'aod'

trying URL '
https://cran.cnr.berkeley.edu/bin/windows/contrib/3.5/polspline_1.1.14.zip'
Content type 'application/zip' length 778099 bytes (759 KB)
downloaded 759 KB

trying URL '
https://cran.cnr.berkeley.edu/bin/windows/contrib/3.5/rms_5.1-3.1.zip'
Content type 'application/zip' length 2044689 bytes (1.9 MB)
downloaded 1.9 MB

trying URL '
https://cran.cnr.berkeley.edu/bin/windows/contrib/3.5/speedglm_0.3-2.zip'
Content type 'application/zip' length 187929 bytes (183 KB)
downloaded 183 KB

trying URL '
https://cran.cnr.berkeley.edu/bin/windows/contrib/3.5/aod_1.3.1.zip'
Content type 'application/zip' length 323652 bytes (316 KB)
downloaded 316 KB

trying URL '
https://cran.cnr.berkeley.edu/bin/windows/contrib/3.5/LogisticDx_0.2.zip'
Content type 'application/zip' length 931345 bytes (909 KB)
downloaded 909 KB

package 'polspline' successfully unpacked and MD5 sums checked
package 'rms' successfully unpacked and MD5 sums checked
package 'speedglm' successfully unpacked and MD5 sums checked
package 'aod' successfully unpacked and MD5 sums checked
package 'LogisticDx' successfully unpacked and MD5 sums checked


> stukel(RegGLM_Mod1)
Error in stukel(RegGLM_Mod1) : could not find function "stukel"

> library(LogisticDx)
Error: package or namespace load failed for 'LogisticDx':
 object 'plotp' not found whilst loading namespace 'rms'
> install.packages("plotp")
Installing package into 'C:/Users/PaulBernal/Documents/R/win-library/3.5'
(as 'lib' is unspecified)
Warning: package 'plotp' is not available (for R version 3.5.3)

After this, I installed R version 3.6.0 and tried again:
> install.packages("LogisticDx")
Installing package into ?C:/Users/PaulBernal/Documents/R/win-library/3.6?
(as ?lib? is unspecified)
--- Please select a CRAN mirror for use in this session ---
trying URL '
https://mirrors.dotsrc.org/cran/bin/windows/contrib/3.6/LogisticDx_0.2.zip'
Content type 'application/zip' length 932914 bytes (911 KB)
downloaded 911 KB

package ?LogisticDx? successfully unpacked and MD5 sums checked

The downloaded binary packages are in

C:\Users\PaulBernal\AppData\Local\Temp\RtmpInhoDn\downloaded_packages
> library(LogisticDx)
Registered S3 methods overwritten by 'ggplot2':
  method         from
  [.quosures     rlang
  c.quosures     rlang
  print.quosures rlang

> stukel(GLM.1)
Error in stukel(GLM.1) : could not find function "stukel"

Anyone can give me some light on what could be happening here? Do I have to
try with another R version? Is it something else?

Thanks in advance,

Paul

	[[alternative HTML version deleted]]


From dr@ke@go@@| @end|ng |rom gm@||@com  Fri May  3 23:02:31 2019
From: dr@ke@go@@| @end|ng |rom gm@||@com (Drake Gossi)
Date: Fri, 3 May 2019 14:02:31 -0700
Subject: [R] randomly sampling and visualizing 100 nodes in a citation
 network with igraph library
Message-ID: <CAPSTy5favwbtgHJVhGR8Nzw6R8BsVhLx_gB22n3Jozj8DtMbaQ@mail.gmail.com>

Hello everyone,

How would I randomly select 100 nodes to look at out of these approx.
25,0000? I'm practicing constructing a citation network.

edgeList below is an edge list.

> head (edgeList)
     to        from
[1,] "4US6"    "3US320"
[2,] "4US6"    "4US1"
[3,] "6US280"  "1US393"
[4,] "6US280"  "1US53"
[5,] "6US280"  "3US133"
[6,] "14US179" "5US321"

> G <- graph.edgelist(edgeList, directed=FALSE)
> G
IGRAPH a473ce7 UN-- 25417 216738 --
+ attr: name (v/c)
+ edges from a473ce7 (vertex names):
 [1] 4US6   --3US320  4US6   --4US1    6US280 --1US393
 [4] 6US280 --1US53   6US280 --3US133  14US179--5US321
 [7] 14US179--9US262  15US45 --3US384  15US45 --4US436
[10] 15US45 --4US441  15US45 --8US421  15US227--13US388
[13] 15US227--2US36   15US290--7US220  9US262 --15US290
[16] 15US369--11US504 15US369--11US577 15US369--5US137
[19] 15US369--9US115  15US396--1US371  15US396--4US450
[22] 15US396--6US358  15US396--7US73   16US1  --11US603
+ ... omitted several edges

Can I use the sample () function? I'm working with the text Humanities
Data in R (Arnold & Tilton). I'm working within the igraph library.

Drake


From murdoch@dunc@n @end|ng |rom gm@||@com  Fri May  3 23:45:57 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Fri, 3 May 2019 17:45:57 -0400
Subject: [R] stukel function unavailable for some odd reason
In-Reply-To: <CAMOcQfM9GjGRSvXnuMTUxxeoqaDOnp=7uaz1cyHUdF0kTkRa8g@mail.gmail.com>
References: <CAMOcQfM9GjGRSvXnuMTUxxeoqaDOnp=7uaz1cyHUdF0kTkRa8g@mail.gmail.com>
Message-ID: <6e23b094-8bd9-dcd4-bb8a-3f2c49d305b3@gmail.com>

On 03/05/2019 4:49 p.m., Paul Bernal wrote:
> Dear friends,
> 
> I have been fitting a logistic regression and wanted to try a couple of
> goodness of fit tests on the model.
> 
> Doing some research, I came across Chris Dardi's stukel and logiGOF
> functions from package LogisticDx v0.1.

The current version of LogisticDx is 0.2 from 2015, and it doesn't 
export those functions.  Source for version 0.1 from 2014 is available, 
and it does contain those functions, but I wouldn't touch them with a 
ten foot pole unless I first heard from the author why he left them out 
of the next release.

Duncan Murdoch


> 
> I tried installing package LogisticDx in different R versions without any
> success.
> 
> I first tried installing LogisticDx package in R version 3.5.3 (for windows
> 64-bit OS) and this is what happened:
> 
>> install.packages("LogisticDx")
> Installing package into 'C:/Users/PaulBernal/Documents/R/win-library/3.5'
> (as 'lib' is unspecified)
> --- Please select a CRAN mirror for use in this session ---
> also installing the dependencies 'polspline', 'rms', 'speedglm', 'aod'
> 
> trying URL '
> https://cran.cnr.berkeley.edu/bin/windows/contrib/3.5/polspline_1.1.14.zip'
> Content type 'application/zip' length 778099 bytes (759 KB)
> downloaded 759 KB
> 
> trying URL '
> https://cran.cnr.berkeley.edu/bin/windows/contrib/3.5/rms_5.1-3.1.zip'
> Content type 'application/zip' length 2044689 bytes (1.9 MB)
> downloaded 1.9 MB
> 
> trying URL '
> https://cran.cnr.berkeley.edu/bin/windows/contrib/3.5/speedglm_0.3-2.zip'
> Content type 'application/zip' length 187929 bytes (183 KB)
> downloaded 183 KB
> 
> trying URL '
> https://cran.cnr.berkeley.edu/bin/windows/contrib/3.5/aod_1.3.1.zip'
> Content type 'application/zip' length 323652 bytes (316 KB)
> downloaded 316 KB
> 
> trying URL '
> https://cran.cnr.berkeley.edu/bin/windows/contrib/3.5/LogisticDx_0.2.zip'
> Content type 'application/zip' length 931345 bytes (909 KB)
> downloaded 909 KB
> 
> package 'polspline' successfully unpacked and MD5 sums checked
> package 'rms' successfully unpacked and MD5 sums checked
> package 'speedglm' successfully unpacked and MD5 sums checked
> package 'aod' successfully unpacked and MD5 sums checked
> package 'LogisticDx' successfully unpacked and MD5 sums checked
> 
> 
>> stukel(RegGLM_Mod1)
> Error in stukel(RegGLM_Mod1) : could not find function "stukel"
> 
>> library(LogisticDx)
> Error: package or namespace load failed for 'LogisticDx':
>   object 'plotp' not found whilst loading namespace 'rms'
>> install.packages("plotp")
> Installing package into 'C:/Users/PaulBernal/Documents/R/win-library/3.5'
> (as 'lib' is unspecified)
> Warning: package 'plotp' is not available (for R version 3.5.3)
> 
> After this, I installed R version 3.6.0 and tried again:
>> install.packages("LogisticDx")
> Installing package into ?C:/Users/PaulBernal/Documents/R/win-library/3.6?
> (as ?lib? is unspecified)
> --- Please select a CRAN mirror for use in this session ---
> trying URL '
> https://mirrors.dotsrc.org/cran/bin/windows/contrib/3.6/LogisticDx_0.2.zip'
> Content type 'application/zip' length 932914 bytes (911 KB)
> downloaded 911 KB
> 
> package ?LogisticDx? successfully unpacked and MD5 sums checked
> 
> The downloaded binary packages are in
> 
> C:\Users\PaulBernal\AppData\Local\Temp\RtmpInhoDn\downloaded_packages
>> library(LogisticDx)
> Registered S3 methods overwritten by 'ggplot2':
>    method         from
>    [.quosures     rlang
>    c.quosures     rlang
>    print.quosures rlang
> 
>> stukel(GLM.1)
> Error in stukel(GLM.1) : could not find function "stukel"
> 
> Anyone can give me some light on what could be happening here? Do I have to
> try with another R version? Is it something else?
> 
> Thanks in advance,
> 
> Paul
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From drj|m|emon @end|ng |rom gm@||@com  Fri May  3 23:57:52 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sat, 4 May 2019 07:57:52 +1000
Subject: [R] create a table for time difference (from t3 to t1, so on..)
In-Reply-To: <CAMwU6B0g8LBr_MiYVoTWksz0PB-DSZ6fft5hAebkuiDjjfXjZQ@mail.gmail.com>
References: <CAMwU6B0g8LBr_MiYVoTWksz0PB-DSZ6fft5hAebkuiDjjfXjZQ@mail.gmail.com>
Message-ID: <CA+8X3fXX_rYJ_vV6R_6rU6GPZH=Lv0_Tf=WaHRbe5wZgP2PoQA@mail.gmail.com>

Hi Marna,
You can get the information you need with this:

dAT$date<-as.Date(dAT$date,"%d-%b-%y")
diffs<-function(x,maxn) return(diff(x)[1:maxn])
initdate<-function(x) return(min(x))
datediffs<-aggregate(dAT$date,list(dAT$Id),diffs,3)

I can't do the manipulation of the resulting values at the moment, but
can work it out later if necessary.

Jim

On Sat, May 4, 2019 at 5:24 AM Marna Wagley <marna.wagley at gmail.com> wrote:
>
> Hi R User.
> I have a date set in which I wanted to find the duration (period)  between
> released time and detection time for each individual. Is there any simplest
> way to create a matrix?
> I appreciate your help.
>
> Thanks,
> MW
> --------------
> Here is the example data,
>
> dAT<-structure(list(Id = structure(c(1L, 2L, 3L, 4L, 1L, 2L, 5L, 4L,
>
> 1L, 6L, 6L, 3L, 4L, 1L), .Label = c("a", "b", "c", "e", "f",
>
> "m"), class = "factor"), date = structure(c(1L, 1L, 1L, 5L, 4L,
>
> 1L, 5L, 5L, 3L, 1L, 2L, 7L, 6L, 8L), .Label = c("1-Jan-18", "1-Jul-18",
>
> "10-Mar-18", "15-Jan-18", "2-Jan-18", "27-Jan-18", "3-Jan-18",
>
> "7-Mar-19"), class = "factor")), .Names = c("Id", "date"), class =
> "data.frame", row.names = c(NA,
>
> -14L))
>
>
> #and the results looks like
>
>
> Result<-structure(list(Id = structure(1:6, .Label = c("a", "b", "c",
>
> "e", "f", "m"), class = "factor"), intial.released.date = structure(c(1L,
>
> 1L, 1L, 2L, 2L, 1L), .Label = c("1-Jan-18", "2-Jan-18"), class = "factor"),
>
>     duration.between.t2.and.t1 = c(14L, 0L, 2L, 0L, NA, 181L),
>
>     duration.between.t3.and.t1 = c(68L, NA, NA, 25L, NA, NA),
>
>     duration.between.t4.and.t1 = c(430L, NA, NA, NA, NA, NA),
>
>     duration.between.t3.and.t2 = c(54L, NA, NA, NA, NA, NA),
>
>     duration.between.t4.and.t2 = c(416L, NA, NA, NA, NA, NA)), .Names = c(
> "Id",
>
> "intial.released.date", "duration.between.t2.and.t1",
> "duration.between.t3.and.t1",
>
> "duration.between.t4.and.t1", "duration.between.t3.and.t2",
> "duration.between.t4.and.t2"
>
> ), class = "data.frame", row.names = c(NA, -6L))
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat May  4 00:27:05 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 3 May 2019 23:27:05 +0100
Subject: [R] randomly sampling and visualizing 100 nodes in a citation
 network with igraph library
In-Reply-To: <CAPSTy5favwbtgHJVhGR8Nzw6R8BsVhLx_gB22n3Jozj8DtMbaQ@mail.gmail.com>
References: <CAPSTy5favwbtgHJVhGR8Nzw6R8BsVhLx_gB22n3Jozj8DtMbaQ@mail.gmail.com>
Message-ID: <449d5889-4037-4459-5148-e008f4ed3ce6@sapo.pt>

Hello,

Maybe you can sample from the vertices

subv <- sample(V(G), 100)

and then use something like [1] (StackOverflow).

[1]https://stackoverflow.com/questions/23682113/creating-subgraph-using-igraph-in-r


Hope this helps,

Rui Barradas

?s 22:02 de 03/05/19, Drake Gossi escreveu:
> Hello everyone,
> 
> How would I randomly select 100 nodes to look at out of these approx.
> 25,0000? I'm practicing constructing a citation network.
> 
> edgeList below is an edge list.
> 
>> head (edgeList)
>       to        from
> [1,] "4US6"    "3US320"
> [2,] "4US6"    "4US1"
> [3,] "6US280"  "1US393"
> [4,] "6US280"  "1US53"
> [5,] "6US280"  "3US133"
> [6,] "14US179" "5US321"
> 
>> G <- graph.edgelist(edgeList, directed=FALSE)
>> G
> IGRAPH a473ce7 UN-- 25417 216738 --
> + attr: name (v/c)
> + edges from a473ce7 (vertex names):
>   [1] 4US6   --3US320  4US6   --4US1    6US280 --1US393
>   [4] 6US280 --1US53   6US280 --3US133  14US179--5US321
>   [7] 14US179--9US262  15US45 --3US384  15US45 --4US436
> [10] 15US45 --4US441  15US45 --8US421  15US227--13US388
> [13] 15US227--2US36   15US290--7US220  9US262 --15US290
> [16] 15US369--11US504 15US369--11US577 15US369--5US137
> [19] 15US369--9US115  15US396--1US371  15US396--4US450
> [22] 15US396--6US358  15US396--7US73   16US1  --11US603
> + ... omitted several edges
> 
> Can I use the sample () function? I'm working with the text Humanities
> Data in R (Arnold & Tilton). I'm working within the igraph library.
> 
> Drake
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @en@k@hk@ @end|ng |rom gm@||@com  Thu May  2 11:00:35 2019
From: @en@k@hk@ @end|ng |rom gm@||@com (Senaka Amarakeerthi)
Date: Thu, 2 May 2019 14:30:35 +0530
Subject: [R] One-way ANOVA with replicates
Message-ID: <CA+si8qm0AE_d2JWZ8k8XZjMWyTBRb2jQ8WWLF1tbRcrfu5h_ng@mail.gmail.com>

Hi All,
I have a dataset (attached) with three groups (treatments) namely Neutral ,
Video, Audio and four output variables.
I am planning to check the ANOVA results to show that is a difference in
observations when treatment is changed.
To generalize the conclusion, I did the same experiment for set of people
(subjects in attached file).
Now I am not sure whether I can use One-Way ANOVA for my dataset. Highly
appreciate if somebody can explain me.
I am using the R package to analyze data.
Thank you.

From m@rce|o@m@r|@no @end|ng |rom un||e@p@br  Fri May  3 15:28:49 2019
From: m@rce|o@m@r|@no @end|ng |rom un||e@p@br (Marcelo Mariano Silva)
Date: Fri, 3 May 2019 10:28:49 -0300
Subject: [R] Loading EDF files
In-Reply-To: <ce8f0350-f042-94e6-d304-2e475b9e6a3e@ualg.pt>
References: <ce8f0350-f042-94e6-d304-2e475b9e6a3e@ualg.pt>
Message-ID: <CANZDV5Aoi-bNpW80cT9Q+pjrVz-9C-xw4EvBTdYojOGXvrgg-Q@mail.gmail.com>

Dear All,

I have tried to load an EDF file using the code below and I received the
following massage :

Error in signals[[sn]]$signal[nextInCSignal[sn]:lastOne] <- samples[[sn]] :

replacement has length zero

What this massage means?
Is there any problem with the EDF file?

Code used:
hdr <- readEdfHeader("/lasse/neurobit/edf/medidas/g1/G1medida1.edf");


df_id1 <- readEdfSignals(hdr, signals = "All", from = 0, till = Inf,
physical = TRUE, fragments = TRUE, recordStarts = TRUE, mergeASignals =
TRUE, simplify = TRUE);


Any help would be highly appreciated.


Mariano


>
>

	[[alternative HTML version deleted]]


From r@|mgren @end|ng |rom qu@nt|t@t|vebroker@@com  Fri May  3 21:45:44 2019
From: r@|mgren @end|ng |rom qu@nt|t@t|vebroker@@com (Robert Almgren)
Date: Fri, 3 May 2019 15:45:44 -0400
Subject: [R] approx with NAs
Message-ID: <55393658-0A87-4708-99AF-C612F896AE01@quantitativebrokers.com>

There is something I do not think is right in the approx() function in base R, with method="constant" and in the presence of NA values. I have 3.6.0, but the behavior seems to be the same in earlier versions.

My suggested fix is to add an "na.rm" argument to approx(), as in mean(). If this argument is FALSE, then NA values should be propagated into the output rather than being removed.

Details:

The documentation says 

"f: for method = "constant" a number between 0 and 1 inclusive, indicating a compromise between left- and right-continuous step functions. If y0 and y1 are the values to the left and right of the point then the value is y0 if f == 0, y1 if f == 1, and y0*(1-f)+y1*f for intermediate values. In this way the result is right-continuous for f == 0 and left-continuous for f == 1, even for non-finite y values."

This suggests to me that if the left value y0 is NA, and if f=0 (the default), then the interpolated value should be NA. (Regardless of the right value y1, see bug 15655 fixed in 2014.)

The documentation further says, below under "Details", that

"The inputs can contain missing values which are deleted."

The question is what is the appropriate behavior if one of the input values y is NA. Currently, approx() seems to interpret NA values as faulty data points, which should be deleted and the previous values carried forward (example below).

But in many applications, especially with "constant" interpolation, an NA value is intended to mean that we really do not know the value in the next interval, or explicitly that there is no value. Therefore the NA should not be removed, but should be propagated forward into the output within the corresponding interval.

The situation is similar with functions like mean(). The presence of an NA value may mean either (a) we want to compute the mean without that value (na.rm=TRUE), or (b) we really are missing important information, we cannot determine the mean, and we should return NA (na.rm=FALSE).

Therefore, I propose that approx() also be given an na.rm argument, indicating whether we wish to delete NA values, or treat them as actual values on the corresponding interval. That option makes even more sense for approx() than for mean(), since the NA values apply only on small regions of the data range.

                                       --Robert Almgren

Example:

: R --vanilla

R version 3.6.0 (2019-04-26) -- "Planting of a Tree"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin15.6.0 (64-bit)
...

> t1 <- 1:5
> x1 <- c( 1, as.numeric(NA), 3, as.numeric(NA), 5 )
> print(data.frame(t1,x1))
  t1 x1
1  1  1
2  2 NA   <-- we do not know the value between t=2 and t=3
3  3  3
4  4 NA   <-- we do not know the value between t=4 and t=5
5  5  5
> X <- approx( t1, x1, (1:4) + 0.5, method='constant', rule=c(1,2) )
> print(data.frame(X))
    x y
1 1.5 1
2 2.5 1   <---- I believe that these two values should be NA
3 3.5 3
4 4.5 3   <---- I believe that these two values should be NA

--
Quantitative Brokers         http://www.quantitativebrokers.com




-- 

















CONFIDENTIALITY NOTICE: This e-mail and any attachments=...{{dropped:23}}


From drj|m|emon @end|ng |rom gm@||@com  Sat May  4 11:48:32 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sat, 4 May 2019 19:48:32 +1000
Subject: [R] create a table for time difference (from t3 to t1, so on..)
In-Reply-To: <CA+8X3fXX_rYJ_vV6R_6rU6GPZH=Lv0_Tf=WaHRbe5wZgP2PoQA@mail.gmail.com>
References: <CAMwU6B0g8LBr_MiYVoTWksz0PB-DSZ6fft5hAebkuiDjjfXjZQ@mail.gmail.com>
 <CA+8X3fXX_rYJ_vV6R_6rU6GPZH=Lv0_Tf=WaHRbe5wZgP2PoQA@mail.gmail.com>
Message-ID: <CA+8X3fUpDsgk548BMF4BvE5hGYvgrwu8N5oOXX1-t3dco-ubzw@mail.gmail.com>

Hi Marna,
I was able to have another look at it. I think this is what you want
except for the column names (I'm lazy):

dAT<-structure(list(Id = structure(c(1L, 2L, 3L, 4L, 1L, 2L, 5L, 4L,
1L, 6L, 6L, 3L, 4L, 1L), .Label = c("a", "b", "c", "e", "f",
"m"), class = "factor"), date = structure(c(1L, 1L, 1L, 5L, 4L,
1L, 5L, 5L, 3L, 1L, 2L, 7L, 6L, 8L), .Label = c("1-Jan-18", "1-Jul-18",
"10-Mar-18", "15-Jan-18", "2-Jan-18", "27-Jan-18", "3-Jan-18",
"7-Mar-19"), class = "factor")), .Names = c("Id", "date"), class =
"data.frame", row.names = c(NA,-14L))
dAT$date<-as.Date(dAT$date,"%d-%b-%y")
diffs<-function(x,maxn) return(diff(x)[1:maxn])
mindate<-function(x) return(min(x))
datediffs<-aggregate(dAT$date,list(dAT$Id),diffs,3)
initdates<-as.character(as.Date(as.vector(by(dAT$date,dAT$Id,min)),
 origin=as.Date("1970-01-01")))
Result<-data.frame(Id=datediffs$Group.1,Release=initdates,
 'T1-T2'=datediffs$x[,1],'T1-T3'=datediffs$x[,1]+datediffs$x[,2],
 'T1-T4'=datediffs$x[,1]+datediffs$x[,2]+datediffs$x[,3],
 'T2-T3'=datediffs$x[,2],'T2-T4'=datediffs$x[,2]+datediffs$x[,3])


Jim

On Sat, May 4, 2019 at 7:57 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi Marna,
> You can get the information you need with this:
>
> dAT$date<-as.Date(dAT$date,"%d-%b-%y")
> diffs<-function(x,maxn) return(diff(x)[1:maxn])
> initdate<-function(x) return(min(x))
> datediffs<-aggregate(dAT$date,list(dAT$Id),diffs,3)
>
> I can't do the manipulation of the resulting values at the moment, but
> can work it out later if necessary.
>
> Jim
>
> On Sat, May 4, 2019 at 5:24 AM Marna Wagley <marna.wagley at gmail.com> wrote:
> >
> > Hi R User.
> > I have a date set in which I wanted to find the duration (period)  between
> > released time and detection time for each individual. Is there any simplest
> > way to create a matrix?
> > I appreciate your help.
> >
> > Thanks,
> > MW
> > --------------
> > Here is the example data,
> >
> > dAT<-structure(list(Id = structure(c(1L, 2L, 3L, 4L, 1L, 2L, 5L, 4L,
> >
> > 1L, 6L, 6L, 3L, 4L, 1L), .Label = c("a", "b", "c", "e", "f",
> >
> > "m"), class = "factor"), date = structure(c(1L, 1L, 1L, 5L, 4L,
> >
> > 1L, 5L, 5L, 3L, 1L, 2L, 7L, 6L, 8L), .Label = c("1-Jan-18", "1-Jul-18",
> >
> > "10-Mar-18", "15-Jan-18", "2-Jan-18", "27-Jan-18", "3-Jan-18",
> >
> > "7-Mar-19"), class = "factor")), .Names = c("Id", "date"), class =
> > "data.frame", row.names = c(NA,
> >
> > -14L))
> >
> >
> > #and the results looks like
> >
> >
> > Result<-structure(list(Id = structure(1:6, .Label = c("a", "b", "c",
> >
> > "e", "f", "m"), class = "factor"), intial.released.date = structure(c(1L,
> >
> > 1L, 1L, 2L, 2L, 1L), .Label = c("1-Jan-18", "2-Jan-18"), class = "factor"),
> >
> >     duration.between.t2.and.t1 = c(14L, 0L, 2L, 0L, NA, 181L),
> >
> >     duration.between.t3.and.t1 = c(68L, NA, NA, 25L, NA, NA),
> >
> >     duration.between.t4.and.t1 = c(430L, NA, NA, NA, NA, NA),
> >
> >     duration.between.t3.and.t2 = c(54L, NA, NA, NA, NA, NA),
> >
> >     duration.between.t4.and.t2 = c(416L, NA, NA, NA, NA, NA)), .Names = c(
> > "Id",
> >
> > "intial.released.date", "duration.between.t2.and.t1",
> > "duration.between.t3.and.t1",
> >
> > "duration.between.t4.and.t1", "duration.between.t3.and.t2",
> > "duration.between.t4.and.t2"
> >
> > ), class = "data.frame", row.names = c(NA, -6L))
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From drj|m|emon @end|ng |rom gm@||@com  Sat May  4 11:53:57 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sat, 4 May 2019 19:53:57 +1000
Subject: [R] One-way ANOVA with replicates
In-Reply-To: <CA+si8qm0AE_d2JWZ8k8XZjMWyTBRb2jQ8WWLF1tbRcrfu5h_ng@mail.gmail.com>
References: <CA+si8qm0AE_d2JWZ8k8XZjMWyTBRb2jQ8WWLF1tbRcrfu5h_ng@mail.gmail.com>
Message-ID: <CA+8X3fUYR-n1Y28BcOhcDX+1=oeqqTKOPoT88Cg4YXtmhQ1cBA@mail.gmail.com>

Hi Senaka,
Your attachments didn't make it through. If the datasets are not
large, perhaps use "dput" to create a text version that can be pasted
into the email. Large data sets may have to be uploaded to a public
URL>

Jim


On Sat, May 4, 2019 at 3:10 PM Senaka Amarakeerthi <senakahks at gmail.com> wrote:
>
> Hi All,
> I have a dataset (attached) with three groups (treatments) namely Neutral ,
> Video, Audio and four output variables.
> I am planning to check the ANOVA results to show that is a difference in
> observations when treatment is changed.
> To generalize the conclusion, I did the same experiment for set of people
> (subjects in attached file).
> Now I am not sure whether I can use One-Way ANOVA for my dataset. Highly
> appreciate if somebody can explain me.
> I am using the R package to analyze data.
> Thank you.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dr@ke@go@@| @end|ng |rom gm@||@com  Sun May  5 02:34:18 2019
From: dr@ke@go@@| @end|ng |rom gm@||@com (Drake Gossi)
Date: Sat, 4 May 2019 17:34:18 -0700
Subject: [R] what would break a citation network? [Error in
 as.igraph.vs(graph, vids) : Invalid vertex names]
Message-ID: <CAPSTy5cAFV7uSi=-K0Pz_7ZwHq540d+ORyS0-J3eosoPHkq6Fw@mail.gmail.com>

Hello everyone,

I'm trying to learn how to put together a citation network, and, in
doing so, I'm playing around with a data set of my own making. I'm
going back and forth between two .csv files. One has two columns and
is simply labeled "to" and "from":

to from
rickert heidegger
rickert nietzsche
rickert parmenides
rickert diogenes
rickert latour
rickert haraway
rickert barad
rickert burke
boyle mackenzie
boyle flusser
boyle kittler
boyle massumi
boyle mattern
boyle rickert
boyle berlant
boyle ulmer
boyle manovich
boyle burke

So, rickert and boyle are academics, and the people to the right are
other scholars they cite. This one .csv file goes on for 75 rows like
this.

But I have another file too, which looks like this. The two columns
are "author" and "association":

author association
latour STS
burke rhetoric
kittler media theory
heidegger philosophy
barthes philosophy
mackenzie media theory
massumi affect theory
nietzsche philosophy
flusser media theory
rickert rhetoric
spinuzzi tech comm
boyle rhetoric
gries rhetoric
jeff rice rhetoric
jenny rice rhetoric
gunn rhetoric
rivers rhetoric
mol STS
stengers STS
barad STS
braidotti posthumanism
wolfe posthumanism
haraway STS

The trouble is, when I add the following (davis, rhetoric) onto the
second list, the citation network breaks.

...
...
braidotti posthumanism
wolfe posthumanism
haraway STS
davis rhetoric

My question is: why would the addition of a single row break the
analysis? I am getting this error message:

Error in as.igraph.vs(graph, vids) : Invalid vertex names

My code is this:

install.packages("igraph")
library(igraph)
myEdgeList <- as.matrix(read.csv("myEdgeList.csv", as.is=TRUE))
G <- graph.edgelist(myEdgeList, directed=FALSE)
myThemes <- read.csv("myThemes.csv", as.is=TRUE)
rowNumberofMatchingCases <- which(myThemes$association %in% c("rhetoric"
                                                              ))
authorsOfMatchingCases <- myThemes$author [rowNumberofMatchingCases]
H <- induced.subgraph (G, authorsOfMatchingCases)
lout <- layout.fruchterman.reingold(H)
plot.igraph(H, layout=lout, vertex.size=5, vertex.label.cex=.5)

My guess is I have broken some fundamental rule of graphing, like one
of the names performs too many functions in the graph, or I have
"rhetoric" too many times in the is the second csv file...

But I really think the better question is: if you were just fooling
around and trying to make a citation network from hand and by scratch,
like I'm doing, what could you not do with the list? and does this
have something to do with transitivity?

Drake


From m@||||@t@ @end|ng |rom pp@|net@||  Sun May  5 07:35:29 2019
From: m@||||@t@ @end|ng |rom pp@|net@|| (K. Elo)
Date: Sun, 05 May 2019 08:35:29 +0300
Subject: [R] what would break a citation network? [Error in
 as.igraph.vs(graph, vids) : Invalid vertex names]
In-Reply-To: <CAPSTy5cAFV7uSi=-K0Pz_7ZwHq540d+ORyS0-J3eosoPHkq6Fw@mail.gmail.com>
References: <CAPSTy5cAFV7uSi=-K0Pz_7ZwHq540d+ORyS0-J3eosoPHkq6Fw@mail.gmail.com>
Message-ID: <8005112ca67ede4df90057e3a127bc2f318eea2e.camel@pp.inet.fi>

Hi Drake,

2019-05-04, 17:34 -0700, Drake Gossi wrote:
> Hello everyone,
> 
> I'm trying to learn how to put together a citation network, and, in
> doing so, I'm playing around with a data set of my own making. I'm
> going back and forth between two .csv files. One has two columns and
> is simply labeled "to" and "from":
> 
> to from
> rickert heidegger
> rickert nietzsche
> rickert parmenides
> rickert diogenes
> rickert latour
> rickert haraway
> rickert barad
> rickert burke
> boyle mackenzie
> boyle flusser
> boyle kittler
> boyle massumi
> boyle mattern
> boyle rickert
> boyle berlant
> boyle ulmer
> boyle manovich
> boyle burke
> 
> So, rickert and boyle are academics, and the people to the right are
> other scholars they cite. This one .csv file goes on for 75 rows like
> this.
> 
> But I have another file too, which looks like this. The two columns
> are "author" and "association":
> 
> author association
> latour STS
> burke rhetoric
> kittler media theory
> heidegger philosophy
> barthes philosophy
> mackenzie media theory
> massumi affect theory
> nietzsche philosophy
> flusser media theory
> rickert rhetoric
> spinuzzi tech comm
> boyle rhetoric
> gries rhetoric
> jeff rice rhetoric
> jenny rice rhetoric
> gunn rhetoric
> rivers rhetoric
> mol STS
> stengers STS
> barad STS
> braidotti posthumanism
> wolfe posthumanism
> haraway STS
> 
> The trouble is, when I add the following (davis, rhetoric) onto the
> second list, the citation network breaks.
> 
> ...
> ...
> braidotti posthumanism
> wolfe posthumanism
> haraway STS
> davis rhetoric
> 
> My question is: why would the addition of a single row break the
> analysis? I am getting this error message:
> 
> Error in as.igraph.vs(graph, vids) : Invalid vertex names
> 
> My code is this:
> 
> install.packages("igraph")
> library(igraph)
> myEdgeList <- as.matrix(read.csv("myEdgeList.csv", as.is=TRUE))
> G <- graph.edgelist(myEdgeList, directed=FALSE)
> myThemes <- read.csv("myThemes.csv", as.is=TRUE)
> rowNumberofMatchingCases <- which(myThemes$association %in%
> c("rhetoric"
>                                                               ))
> authorsOfMatchingCases <- myThemes$author [rowNumberofMatchingCases]
> H <- induced.subgraph (G, authorsOfMatchingCases)
> lout <- layout.fruchterman.reingold(H)
> plot.igraph(H, layout=lout, vertex.size=5, vertex.label.cex=.5)
> 
> My guess is I have broken some fundamental rule of graphing, like one
> of the names performs too many functions in the graph, or I have
> "rhetoric" too many times in the is the second csv file...
> 
> But I really think the better question is: if you were just fooling
> around and trying to make a citation network from hand and by
> scratch,
> like I'm doing, what could you not do with the list? and does this
> have something to do with transitivity?
> 
> Drake

With "davis" you try to refer to a vertex (node) not existing in your
vertex list. This results in an error.

Take a look on this, if I understood your problem correctly it is a
similar issue:


https://stackoverflow.com/questions/30201510/error-with-subgraph-in-igraph-package

HTH,
Kimmo


From dr@ke@go@@| @end|ng |rom gm@||@com  Sun May  5 22:13:16 2019
From: dr@ke@go@@| @end|ng |rom gm@||@com (Drake Gossi)
Date: Sun, 5 May 2019 13:13:16 -0700
Subject: [R] what would break a citation network? [Error in
 as.igraph.vs(graph, vids) : Invalid vertex names]
In-Reply-To: <8005112ca67ede4df90057e3a127bc2f318eea2e.camel@pp.inet.fi>
References: <CAPSTy5cAFV7uSi=-K0Pz_7ZwHq540d+ORyS0-J3eosoPHkq6Fw@mail.gmail.com>
 <8005112ca67ede4df90057e3a127bc2f318eea2e.camel@pp.inet.fi>
Message-ID: <CAPSTy5dV_ky7rzZWYL4VWfPwXzGfQenE9=_oLpxmCd=hDYA8NA@mail.gmail.com>

So, "davis" was on the original edge list. It's just that I didn't
want to reproduce the whole thing, since it was like 75 rows long, and
I didn't want the email to get too cumbersome. In fact, that's partly
why I was so confused, since that name was on both lists.

But, even stranger, I ran the code today again as an experiment, and
it worked just fine. Why would it work one day and not another?

I should say that I'm at a university, too, and that therefore I'm on
a public computer.

Thank you for the link as well.

D


On Sat, May 4, 2019 at 10:35 PM K. Elo <maillists at pp.inet.fi> wrote:
>
> Hi Drake,
>
> 2019-05-04, 17:34 -0700, Drake Gossi wrote:
> > Hello everyone,
> >
> > I'm trying to learn how to put together a citation network, and, in
> > doing so, I'm playing around with a data set of my own making. I'm
> > going back and forth between two .csv files. One has two columns and
> > is simply labeled "to" and "from":
> >
> > to from
> > rickert heidegger
> > rickert nietzsche
> > rickert parmenides
> > rickert diogenes
> > rickert latour
> > rickert haraway
> > rickert barad
> > rickert burke
> > boyle mackenzie
> > boyle flusser
> > boyle kittler
> > boyle massumi
> > boyle mattern
> > boyle rickert
> > boyle berlant
> > boyle ulmer
> > boyle manovich
> > boyle burke
> >
> > So, rickert and boyle are academics, and the people to the right are
> > other scholars they cite. This one .csv file goes on for 75 rows like
> > this.
> >
> > But I have another file too, which looks like this. The two columns
> > are "author" and "association":
> >
> > author association
> > latour STS
> > burke rhetoric
> > kittler media theory
> > heidegger philosophy
> > barthes philosophy
> > mackenzie media theory
> > massumi affect theory
> > nietzsche philosophy
> > flusser media theory
> > rickert rhetoric
> > spinuzzi tech comm
> > boyle rhetoric
> > gries rhetoric
> > jeff rice rhetoric
> > jenny rice rhetoric
> > gunn rhetoric
> > rivers rhetoric
> > mol STS
> > stengers STS
> > barad STS
> > braidotti posthumanism
> > wolfe posthumanism
> > haraway STS
> >
> > The trouble is, when I add the following (davis, rhetoric) onto the
> > second list, the citation network breaks.
> >
> > ...
> > ...
> > braidotti posthumanism
> > wolfe posthumanism
> > haraway STS
> > davis rhetoric
> >
> > My question is: why would the addition of a single row break the
> > analysis? I am getting this error message:
> >
> > Error in as.igraph.vs(graph, vids) : Invalid vertex names
> >
> > My code is this:
> >
> > install.packages("igraph")
> > library(igraph)
> > myEdgeList <- as.matrix(read.csv("myEdgeList.csv", as.is=TRUE))
> > G <- graph.edgelist(myEdgeList, directed=FALSE)
> > myThemes <- read.csv("myThemes.csv", as.is=TRUE)
> > rowNumberofMatchingCases <- which(myThemes$association %in%
> > c("rhetoric"
> >                                                               ))
> > authorsOfMatchingCases <- myThemes$author [rowNumberofMatchingCases]
> > H <- induced.subgraph (G, authorsOfMatchingCases)
> > lout <- layout.fruchterman.reingold(H)
> > plot.igraph(H, layout=lout, vertex.size=5, vertex.label.cex=.5)
> >
> > My guess is I have broken some fundamental rule of graphing, like one
> > of the names performs too many functions in the graph, or I have
> > "rhetoric" too many times in the is the second csv file...
> >
> > But I really think the better question is: if you were just fooling
> > around and trying to make a citation network from hand and by
> > scratch,
> > like I'm doing, what could you not do with the list? and does this
> > have something to do with transitivity?
> >
> > Drake
>
> With "davis" you try to refer to a vertex (node) not existing in your
> vertex list. This results in an error.
>
> Take a look on this, if I understood your problem correctly it is a
> similar issue:
>
>
> https://stackoverflow.com/questions/30201510/error-with-subgraph-in-igraph-package
>
> HTH,
> Kimmo
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @pbr@ckett20 @end|ng |rom @@|ntjo@ephh@@com  Mon May  6 01:09:19 2019
From: @pbr@ckett20 @end|ng |rom @@|ntjo@ephh@@com (Spencer Brackett)
Date: Sun, 5 May 2019 19:09:19 -0400
Subject: [R] Question concerning Loading large Data Sets
Message-ID: <CAPQaxLNsjQY9WR4Wx=zdrzDuHK4awa1AMKqz7K9HzVUrwZGosQ@mail.gmail.com>

Good evening,

I am attempting to reproduce the following lines of code...

library(data.table)
anno = as.data.frame(fread(file = "file name", sep ="\t", header = T))


The following is my attempt...

> library(data.table)
> mapper <- read.delim("~/Vakul's GBM code/mapper.txt")
>   View(mapper)
## fread() was used as datset is quite large ##
> GBM_meth <- fread("~/Vakul's GBM code/mapper.txt")
> anno = as.data.frame(fread(file = "~/Vakul's GBM code/mapper.txt", sep
="\t", header = T))

Via View(mapper), I can view the dataset contained within the "mapper.txt"
file, but the >anno = as.data.frame portion does not trigger any sort of
'response' on R and simply brings my cursor down to the next line, ready
for further implementation. Should I be getting some sort of output from R,
and if so what should I expect?

I apologize in advance for any errors in the summary of the above script

Best,

Spencer Brackett

	[[alternative HTML version deleted]]


From @pbr@ckett20 @end|ng |rom @@|ntjo@ephh@@com  Mon May  6 01:14:53 2019
From: @pbr@ckett20 @end|ng |rom @@|ntjo@ephh@@com (Spencer Brackett)
Date: Sun, 5 May 2019 19:14:53 -0400
Subject: [R] Question concerning Loading large Data Sets
In-Reply-To: <CAPQaxLNsjQY9WR4Wx=zdrzDuHK4awa1AMKqz7K9HzVUrwZGosQ@mail.gmail.com>
References: <CAPQaxLNsjQY9WR4Wx=zdrzDuHK4awa1AMKqz7K9HzVUrwZGosQ@mail.gmail.com>
Message-ID: <CAPQaxLPNq9+RotbOQbcvOGczQT8q0QEGtwwcjra=n6Mif-_ZyA@mail.gmail.com>

Also,

In case there is any confusion... "~/Vakul's GBM code" was set up in the
working directory (via setwd) as the folder in which the file 'mapper.txt'
is contained... thereby making the "~/Vakul's GBM code/mapper.txt" as
shown.

Should I perhaps reset my working directory to the file I'm trying to
access, exclusively?

On Sun, May 5, 2019 at 7:09 PM Spencer Brackett <
spbrackett20 at saintjosephhs.com> wrote:

> Good evening,
>
> I am attempting to reproduce the following lines of code...
>
> library(data.table)
> anno = as.data.frame(fread(file = "file name", sep ="\t", header = T))
>
>
> The following is my attempt...
>
> > library(data.table)
> > mapper <- read.delim("~/Vakul's GBM code/mapper.txt")
> >   View(mapper)
> ## fread() was used as datset is quite large ##
> > GBM_meth <- fread("~/Vakul's GBM code/mapper.txt")
> > anno = as.data.frame(fread(file = "~/Vakul's GBM code/mapper.txt", sep
> ="\t", header = T))
>
> Via View(mapper), I can view the dataset contained within the "mapper.txt"
> file, but the >anno = as.data.frame portion does not trigger any sort of
> 'response' on R and simply brings my cursor down to the next line, ready
> for further implementation. Should I be getting some sort of output from R,
> and if so what should I expect?
>
> I apologize in advance for any errors in the summary of the above script
>
> Best,
>
> Spencer Brackett
>

	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Mon May  6 01:26:10 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Sun, 5 May 2019 17:26:10 -0600
Subject: [R] Question concerning Loading large Data Sets
In-Reply-To: <CAPQaxLPNq9+RotbOQbcvOGczQT8q0QEGtwwcjra=n6Mif-_ZyA@mail.gmail.com>
References: <CAPQaxLNsjQY9WR4Wx=zdrzDuHK4awa1AMKqz7K9HzVUrwZGosQ@mail.gmail.com>
 <CAPQaxLPNq9+RotbOQbcvOGczQT8q0QEGtwwcjra=n6Mif-_ZyA@mail.gmail.com>
Message-ID: <16DEE3E8-8326-4204-A9A5-5B010ECD36FC@comcast.net>

I wouldn?t have expected an message, but it does raise the question: why are you making two different copies of the same text file if you are concerned about size issues?

? 
David

Sent from my iPhone

> On May 5, 2019, at 5:14 PM, Spencer Brackett <spbrackett20 at saintjosephhs.com> wrote:
> 
> Also,
> 
> In case there is any confusion... "~/Vakul's GBM code" was set up in the
> working directory (via setwd) as the folder in which the file 'mapper.txt'
> is contained... thereby making the "~/Vakul's GBM code/mapper.txt" as
> shown.
> 
> Should I perhaps reset my working directory to the file I'm trying to
> access, exclusively?
> 
> On Sun, May 5, 2019 at 7:09 PM Spencer Brackett <
> spbrackett20 at saintjosephhs.com> wrote:
> 
>> Good evening,
>> 
>> I am attempting to reproduce the following lines of code...
>> 
>> library(data.table)
>> anno = as.data.frame(fread(file = "file name", sep ="\t", header = T))
>> 
>> 
>> The following is my attempt...
>> 
>>> library(data.table)
>>> mapper <- read.delim("~/Vakul's GBM code/mapper.txt")
>>>  View(mapper)
>> ## fread() was used as datset is quite large ##
>>> GBM_meth <- fread("~/Vakul's GBM code/mapper.txt")
>>> anno = as.data.frame(fread(file = "~/Vakul's GBM code/mapper.txt", sep
>> ="\t", header = T))
>> 
>> Via View(mapper), I can view the dataset contained within the "mapper.txt"
>> file, but the >anno = as.data.frame portion does not trigger any sort of
>> 'response' on R and simply brings my cursor down to the next line, ready
>> for further implementation. Should I be getting some sort of output from R,
>> and if so what should I expect?
>> 
>> I apologize in advance for any errors in the summary of the above script
>> 
>> Best,
>> 
>> Spencer Brackett
>> 
> 
>    [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m|@ojpm @end|ng |rom gm@||@com  Mon May  6 04:24:22 2019
From: m|@ojpm @end|ng |rom gm@||@com (John)
Date: Mon, 6 May 2019 10:24:22 +0800
Subject: [R] The function sink() --delete everything before printing by
 "sink()"
Message-ID: <CABcx46AsrjXSgkT69DMo_ERTet-vO_5HOmwD=q6O1NsQvubmyA@mail.gmail.com>

I use the functions "sink" and "print" to print the results to a txt file.
May I delete everything in the txt files before it start to print to the
txt file? Thanks!
#######
sink(file_output.txt"))
for(i in c("a", "b"))
{
  for(j in c("c", "d"))
  {
    {
      print(c(i,j))
      print(xyz1[[i,j]])
      print("t-stat")
      print(abc1[[i,j]])
    }
  }
}
sink()
#########

	[[alternative HTML version deleted]]


From @hugu@ng79 @end|ng |rom qq@com  Sat May  4 10:16:36 2019
From: @hugu@ng79 @end|ng |rom qq@com (Shuguang Sun)
Date: Sat, 04 May 2019 16:16:36 +0800
Subject: [R] Can't rsync extsoft/3.6 when compiling R 3.6
In-Reply-To: <CABFfbXtYk2MO0LS6Oq=Grr3P1JK4gnbqd-d59G5SjtPp9ZLXHg@mail.gmail.com>
 (Jeroen Ooms's message of "Thu, 2 May 2019 12:10:01 +0200")
References: <vk5ysgtxv4fy.fsf@qq.com>
 <23754.40455.851085.510425@stat.math.ethz.ch>
 <CABFfbXtYk2MO0LS6Oq=Grr3P1JK4gnbqd-d59G5SjtPp9ZLXHg@mail.gmail.com>
Message-ID: <vk5yef5e1vob.fsf@qq.com>+D013590DB5B65891

Jeroen Ooms <jeroenooms at gmail.com> writes:

Now I can build R 3.6 in my Windows 10 box.
Thanks.

> On Thu, May 2, 2019 at 9:36 AM Martin Maechler
> <maechler at stat.math.ethz.ch> wrote:
>>
>> >>>>> Shuguang Sun
>> >>>>>     on Thu, 2 May 2019 12:57:37 +0800 writes:
>>
>>     > Hi all,
>>     > When I try to compile R 3.6 in windows 10 in Rtools35, it raised the error message:
>>     > --8<---------------cut here---------------start------------->8---
>>     > make rsync-extsoft
>>     > (mkdir -p ../../extsoft; \
>>     > cd ../../extsoft; \
>>     > rsync --timeout=60 -rcvp --delete cran.r-project.org::CRAN/bin/windows/extsoft/3.6/ . )
>>     > receiving incremental file list
>>     > rsync: change_dir "/bin/windows/extsoft/3.6" (in CRAN) failed: No such file or directory (2)
>>     > --8<---------------cut here---------------end--------------->8---
>>
>>     > There is no "windows/extsoft/3.6/" and only up to 3.5.
>>
>>     > Best Regards,
>>     > Shuguang Sun
>>
>> Indeed.
>> If you look at this FTP directory in a capabable browser (such
>> as Emacs), you see
>>
>>     Dec 22 2014  3.2
>>     Jun 16 2016  3.3
>>     Jul  6 2016  3.4 -> 3.3
>>     May  8 2017  3.5 -> 3.3
>>
>> So it seems the CRAN team (or the R Core release master?) forgot
>> to make that symbolic link there.
>
> I think that might be my responsibility (not entirely sure honestly).
> I'll add that directory and include the current versions of the libs
> we use to build the official r-base on Windows.
>

-- 
Best Regards
Shuguang Sun


From rk@h|r712 @end|ng |rom gm@||@com  Sun May  5 06:53:48 2019
From: rk@h|r712 @end|ng |rom gm@||@com (Rajesh Ahir_GJ)
Date: Sun, 5 May 2019 10:23:48 +0530
Subject: [R] Required help
Message-ID: <CAND=j8gEDoMzo5tBNZGsBEyYgqCcnQxY0Yr9m1UYMuWyGcDNSg@mail.gmail.com>

Hello R users,

I am getting an error while running following code.

library(ggplot2)
ggplot(hourly_data1,aes(hour, power))+
geom_boxplot(aes(fill=monthname),outlier.shape=NA) + facet_wrap(~monthname)
ggplot(hourly_data1,aes(hour, power))+
geom_boxplot(aes(fill=dayname),outlier.shape=NA) + facet_wrap(~dayname) +
geom_smooth(aes(group=1))

An error i am getting is:
Error: Must request at least one colour from a hue palette.

Please help me to solve this.

-- 

Rajesh

	[[alternative HTML version deleted]]


From r@v|ndr@@bh|ng@rde @end|ng |rom gm@||@com  Sun May  5 15:41:58 2019
From: r@v|ndr@@bh|ng@rde @end|ng |rom gm@||@com (Ravindra Bhingarde)
Date: Sun, 5 May 2019 19:11:58 +0530
Subject: [R] Memmory issue
Message-ID: <CA+ELMm_d+j8Dut1iBkXvqH6f3LN0gZZy6wDXKU0bpkuptfQwAA@mail.gmail.com>

Dear sir

We are use R software R 3.6.0 version with intel core i7 9th gen processor
16gb memory but r application utilize 3 gb ram .So kindly give solution for
same

Ravindra
9011096015

	[[alternative HTML version deleted]]


From n|co|@@rugg|ero@unt @end|ng |rom gm@||@com  Sun May  5 23:11:17 2019
From: n|co|@@rugg|ero@unt @end|ng |rom gm@||@com (Nicola Ruggiero)
Date: Sun, 5 May 2019 14:11:17 -0700
Subject: [R] R package that translates zip codes into longitude and longitude
Message-ID: <CAEneZ8njZAqH0n0_ZaeUMdux6H948rJ-cO9DDVXT4gKBwvootQ@mail.gmail.com>

Hi there,

I'm looking for a R package that would enable me to add a column to a
data.frame that is itself a translation of zip codes into longitude
and latitude. The column in the data.frame looks like this [begin
quote]:

waltham, Massachusetts 02451
Columbia, SC 29209

Wheat Ridge , Colorado 80033
Charlottesville, Virginia 22902
Fairbanks, AK 99709
Montpelier, VT 05602
Dobbs Ferry, New York 10522

Henderson , Kentucky 42420

[end quote:] The spaces represent absences in the column. Regardless,
I'm told that there is such an R package. I'm just having trouble
locating it on Google.

Warmly,
Nicola


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon May  6 07:33:22 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sun, 05 May 2019 22:33:22 -0700
Subject: [R] Memmory issue
In-Reply-To: <CA+ELMm_d+j8Dut1iBkXvqH6f3LN0gZZy6wDXKU0bpkuptfQwAA@mail.gmail.com>
References: <CA+ELMm_d+j8Dut1iBkXvqH6f3LN0gZZy6wDXKU0bpkuptfQwAA@mail.gmail.com>
Message-ID: <907F39B3-8F73-4D7F-8C2D-114C1AF2A09B@dcn.davis.ca.us>

 I have no idea what the issue is that is bothering you (do you expect more memory should be used or less?), but I suspect that reading the Posting Guide mentioned at the bottom of all messages on this list might help.  Also, posting the output of the sessionInfo() function and the gc function from a fresh R session might help you communicate more clearly what you are concerned about.

On May 5, 2019 6:41:58 AM PDT, Ravindra Bhingarde <ravindra.bhingarde at gmail.com> wrote:
>Dear sir
>
>We are use R software R 3.6.0 version with intel core i7 9th gen
>processor
>16gb memory but r application utilize 3 gb ram .So kindly give solution
>for
>same
>
>Ravindra
>9011096015
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From m@||||@t@ @end|ng |rom pp@|net@||  Mon May  6 08:22:40 2019
From: m@||||@t@ @end|ng |rom pp@|net@|| (K. Elo)
Date: Mon, 06 May 2019 09:22:40 +0300
Subject: [R] Required help
In-Reply-To: <CAND=j8gEDoMzo5tBNZGsBEyYgqCcnQxY0Yr9m1UYMuWyGcDNSg@mail.gmail.com>
References: <CAND=j8gEDoMzo5tBNZGsBEyYgqCcnQxY0Yr9m1UYMuWyGcDNSg@mail.gmail.com>
Message-ID: <073ca25069f9b2925ad6f822af5f86dacba1362b.camel@pp.inet.fi>

Hi Rajesh,

2019-05-05 10:23 +0530, Rajesh Ahir_GJ wrote:
> Hello R users,
> 
> I am getting an error while running following code.
> 
> library(ggplot2)
> ggplot(hourly_data1,aes(hour, power))+
> geom_boxplot(aes(fill=monthname),outlier.shape=NA) +
> facet_wrap(~monthname)
> ggplot(hourly_data1,aes(hour, power))+
> geom_boxplot(aes(fill=dayname),outlier.shape=NA) +
> facet_wrap(~dayname) +
> geom_smooth(aes(group=1))
> 
> An error i am getting is:
> Error: Must request at least one colour from a hue palette.
> 
> Please help me to solve this.

1) A sample data would be nice, it is hard to debug without proper data
:-)

2) One thing you could check: is there any NAs in you data, i.e. in the
variable used for 'fill'? Check this first.

3) You have two 'ggplot' commands in your example. Do you get the error
for both or just for one?

Best,
Kimmo


From er|cjberger @end|ng |rom gm@||@com  Mon May  6 10:02:03 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Mon, 6 May 2019 11:02:03 +0300
Subject: [R] The function sink() --delete everything before printing by
 "sink()"
In-Reply-To: <CABcx46AsrjXSgkT69DMo_ERTet-vO_5HOmwD=q6O1NsQvubmyA@mail.gmail.com>
References: <CABcx46AsrjXSgkT69DMo_ERTet-vO_5HOmwD=q6O1NsQvubmyA@mail.gmail.com>
Message-ID: <CAGgJW76GH1ON3oDMNEx5KfSOStCkDn7PF5dAn+tmimGXCZ7eDg@mail.gmail.com>

?sink
 for the HELP page on that sink() function.
Check the description of the (optional) argument append to get an answer to
your question.

On Mon, May 6, 2019 at 5:24 AM John <miaojpm at gmail.com> wrote:

> I use the functions "sink" and "print" to print the results to a txt file.
> May I delete everything in the txt files before it start to print to the
> txt file? Thanks!
> #######
> sink(file_output.txt"))
> for(i in c("a", "b"))
> {
>   for(j in c("c", "d"))
>   {
>     {
>       print(c(i,j))
>       print(xyz1[[i,j]])
>       print("t-stat")
>       print(abc1[[i,j]])
>     }
>   }
> }
> sink()
> #########
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Mon May  6 12:31:57 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Mon, 6 May 2019 20:31:57 +1000
Subject: [R] R package that translates zip codes into longitude and
 longitude
In-Reply-To: <CAEneZ8njZAqH0n0_ZaeUMdux6H948rJ-cO9DDVXT4gKBwvootQ@mail.gmail.com>
References: <CAEneZ8njZAqH0n0_ZaeUMdux6H948rJ-cO9DDVXT4gKBwvootQ@mail.gmail.com>
Message-ID: <CA+8X3fUU8VBCN3LDJeAMNrb5=vSMEA5cAT8yZkGYKLyzcMsP=Q@mail.gmail.com>

Hi Nicola,
There seems to be a big database of this here:

http://download.geonames.org/export/dump/

Jim

On Mon, May 6, 2019 at 2:47 PM Nicola Ruggiero
<nicola.ruggiero.unt at gmail.com> wrote:
>
> Hi there,
>
> I'm looking for a R package that would enable me to add a column to a
> data.frame that is itself a translation of zip codes into longitude
> and latitude. The column in the data.frame looks like this [begin
> quote]:
>
> waltham, Massachusetts 02451
> Columbia, SC 29209
>
> Wheat Ridge , Colorado 80033
> Charlottesville, Virginia 22902
> Fairbanks, AK 99709
> Montpelier, VT 05602
> Dobbs Ferry, New York 10522
>
> Henderson , Kentucky 42420
>
> [end quote:] The spaces represent absences in the column. Regardless,
> I'm told that there is such an R package. I'm just having trouble
> locating it on Google.
>
> Warmly,
> Nicola
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From j|ox @end|ng |rom mcm@@ter@c@  Mon May  6 15:05:04 2019
From: j|ox @end|ng |rom mcm@@ter@c@ (Fox, John)
Date: Mon, 6 May 2019 13:05:04 +0000
Subject: [R] Memmory issue
In-Reply-To: <11667_1557118073_x464lqfQ010660_CA+ELMm_d+j8Dut1iBkXvqH6f3LN0gZZy6wDXKU0bpkuptfQwAA@mail.gmail.com>
References: <11667_1557118073_x464lqfQ010660_CA+ELMm_d+j8Dut1iBkXvqH6f3LN0gZZy6wDXKU0bpkuptfQwAA@mail.gmail.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC836BA1F78@FHSDB2D11-2.csu.mcmaster.ca>

Dear Ravindra,

My guess is that you're using the 32-bit version of R for Windows rather than the 64-bit version. Read question 2.9 in the R FAQ for Windows  <https://cran.r-project.org/bin/windows/base/rw-FAQ.html#There-seems-to-be-a-limit-on-the-memory-it-uses_0021>.

If this is the case, the solution is to use the 64-bit version of R, which normally would be installed along with the 32-bit version.

I hope that this helps,
 John

-----------------------------------------------------------------
John Fox
Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
Web: https://socialsciences.mcmaster.ca/jfox/



> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ravindra
> Bhingarde
> Sent: Sunday, May 5, 2019 9:42 AM
> To: r-help at r-project.org
> Cc: ps_chougule at rediffmail.com
> Subject: [R] Memmory issue
> 
> Dear sir
> 
> We are use R software R 3.6.0 version with intel core i7 9th gen processor
> 16gb memory but r application utilize 3 gb ram .So kindly give solution for
> same
> 
> Ravindra
> 9011096015
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From p@u|bern@|07 @end|ng |rom gm@||@com  Mon May  6 19:35:36 2019
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Mon, 6 May 2019 12:35:36 -0500
Subject: [R] Errors Generated by Function gof from LogisticDx Package
Message-ID: <CAMOcQfPUKQr5WGUFHA-0a-q8v6RkSzx23TrS1TXO0Yq=sQ4VCg@mail.gmail.com>

Dear Christopher and friends,

Hope you are all doing great. I am currently using R version 3.6.0 and I
have a Windows 8, 64-bit Operating System.

When applying function gof on my glm model, I get the following errors:

> gof(GLM1)
Error in factor(G, labels = dx1[, format(max(P), digits = 3), by = G]$V1) :
  invalid 'labels'; length 11 should be 1 or 10

> gof(glm(TRANSIT ~ Draft + TOTALCOST + BUNKER + CHARTERVALUE,
family=binomial, data=Dataset), plot=FALSE)
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
Error in factor(G, labels = dx1[, format(max(P), digits = 3), by = G]$V1) :
  invalid 'labels'; length 11 should be 1 or 10

I would like to know what I am doing wrong, or if there is any issue with
the gof function that I am not aware of.

I provide de dput() of my dataset as well as other details below.


My code is as follows:

> library(LogisticDx)
Registered S3 methods overwritten by 'ggplot2':
  method         from
  [.quosures     rlang
  c.quosures     rlang
  print.quosures rlang

> GLM1 <- glm(TRANSIT ~ TOTALCOST + Draft + CHARTERVALUE + BUNKER,
+   family=binomial(logit), data=Dataset)

> dput(Dataset)
structure(list(TRANSIT = c(1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 0L,
1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 1L, 0L,
0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 0L,
1L, 1L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L,
1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L,
1L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L,
0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L,
1L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L,
0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L,
0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L,
0L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L,
0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L,
1L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 0L,
1L, 0L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L,
0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 0L,
0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L,
1L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L,
1L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 1L,
0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L,
0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L,
1L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L,
0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 0L,
0L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 1L,
0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 1L,
1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L,
1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L,
0L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L,
0L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L,
1L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L), Draft = c(12L, 12L, 12L, 13L, 12L, 12L,
12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 14L, 14L, 14L, 12L, 13L,
12L, 12L, 14L, 12L, 14L, 13L, 12L, 12L, 12L, 14L, 12L, 12L, 11L,
13L, 13L, 14L, 12L, 12L, 13L, 14L, 12L, 13L, 13L, 12L, 14L, 14L,
13L, 12L, 14L, 14L, 13L, 14L, 14L, 12L, 13L, 12L, 12L, 13L, 12L,
12L, 14L, 14L, 14L, 12L, 12L, 12L, 12L, 14L, 13L, 14L, 12L, 13L,
14L, 13L, 12L, 12L, 12L, 12L, 12L, 12L, 14L, 12L, 14L, 13L, 14L,
14L, 12L, 12L, 12L, 14L, 12L, 12L, 13L, 14L, 13L, 13L, 12L, 14L,
13L, 13L, 14L, 12L, 12L, 14L, 12L, 12L, 14L, 12L, 13L, 13L, 14L,
14L, 14L, 14L, 12L, 12L, 14L, 13L, 12L, 12L, 12L, 13L, 12L, 14L,
12L, 12L, 14L, 14L, 12L, 12L, 12L, 12L, 12L, 12L, 13L, 12L, 12L,
12L, 13L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 14L, 12L, 12L, 12L,
12L, 14L, 14L, 14L, 14L, 10L, 12L, 11L, 12L, 12L, 12L, 12L, 12L,
12L, 12L, 12L, 12L, 12L, 13L, 14L, 14L, 14L, 12L, 12L, 12L, 14L,
12L, 12L, 12L, 14L, 14L, 14L, 14L, 12L, 12L, 11L, 12L, 12L, 12L,
12L, 12L, 12L, 12L, 12L, 12L, 12L, 10L, 12L, 12L, 12L, 11L, 13L,
14L, 14L, 12L, 13L, 14L, 14L, 12L, 13L, 14L, 12L, 12L, 12L, 14L,
13L, 14L, 12L, 12L, 14L, 14L, 12L, 14L, 13L, 12L, 14L, 12L, 14L,
12L, 12L, 12L, 12L, 14L, 13L, 12L, 13L, 12L, 12L, 14L, 12L, 14L,
14L, 14L, 12L, 12L, 12L, 13L, 12L, 14L, 14L, 14L, 12L, 12L, 12L,
12L, 13L, 12L, 12L, 12L, 14L, 14L, 12L, 12L, 14L, 12L, 14L, 14L,
12L, 12L, 12L, 13L, 12L, 12L, 12L, 12L, 12L, 14L, 14L, 13L, 12L,
13L, 14L, 12L, 12L, 12L, 12L, 13L, 14L, 12L, 14L, 13L, 14L, 14L,
14L, 14L, 14L, 13L, 14L, 14L, 13L, 14L, 12L, 12L, 14L, 14L, 14L,
14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 13L, 12L, 14L, 14L,
14L, 12L, 14L, 14L, 14L, 12L, 12L, 14L, 14L, 14L, 14L, 12L, 14L,
14L, 12L, 14L, 14L, 12L, 13L, 12L, 12L, 12L, 14L, 14L, 13L, 14L,
12L, 13L, 13L, 13L, 12L, 12L, 12L, 12L, 12L, 14L, 12L, 14L, 13L,
14L, 12L, 12L, 14L, 13L, 14L, 14L, 14L, 12L, 14L, 14L, 12L, 12L,
14L, 12L, 14L, 12L, 12L, 12L, 14L, 12L, 13L, 14L, 12L, 12L, 14L,
12L, 12L, 12L, 12L, 12L, 14L, 12L, 12L, 12L, 14L, 14L, 12L, 12L,
14L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 14L, 12L, 13L, 13L, 13L,
14L, 14L, 12L, 12L, 12L, 12L, 12L, 13L, 12L, 14L, 13L, 12L, 12L,
12L, 12L, 12L, 13L, 12L, 14L, 13L, 13L, 13L, 11L, 12L, 14L, 14L,
12L, 14L, 12L, 11L, 12L, 12L, 12L, 12L, 14L, 12L, 12L, 12L, 12L,
14L, 14L, 12L, 12L, 12L, 14L, 12L, 12L, 12L, 12L, 14L, 12L, 13L,
14L, 12L, 12L, 14L, 14L, 12L, 12L, 12L, 13L, 12L, 14L, 14L, 14L,
12L, 14L, 13L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 13L, 6L, 12L,
12L, 14L, 14L, 14L, 14L, 12L, 14L, 12L, 12L, 12L, 12L, 14L, 12L,
14L, 12L, 12L, 12L, 14L, 12L, 12L, 13L, 14L, 12L, 13L, 12L, 14L,
12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L,
12L), TOTALCOST = c(194364L, 219364L, 198260L, 237456L, 197159L,
198992L, 194337L, 219337L, 199198L, 196604L, 230607L, 196604L,
196604L, 194496L, 238600L, 236936L, 237476L, 197220L, 236950L,
197300L, 182042L, 237938L, 199221L, 237475L, 239190L, 157406L,
157211L, 182211L, 237475L, 182475L, 181599L, 156599L, 238269L,
238402L, 238069L, 161436L, 225031L, 238180L, 237572L, 189861L,
239005L, 239049L, 163814L, 240064L, 239171L, 238410L, 200878L,
239019L, 239087L, 239350L, 239352L, 240275L, 164844L, 238400L,
225158L, 202495L, 239681L, 201791L, 226863L, 244092L, 244590L,
239171L, 189811L, 219412L, 228480L, 203650L, 237514L, 247451L,
244739L, 211770L, 244308L, 239197L, 238419L, 224977L, 157362L,
162434L, 162434L, 162434L, 162434L, 239681L, 163316L, 237265L,
243920L, 244088L, 244163L, 202256L, 159592L, 201346L, 239187L,
189800L, 191959L, 239476L, 239171L, 238087L, 238052L, 164169L,
245057L, 244215L, 240812L, 239156L, 156879L, 197853L, 245367L,
164710L, 164710L, 244192L, 211110L, 239156L, 244213L, 237504L,
239018L, 241150L, 244447L, 238506L, 210298L, 243482L, 239166L,
159489L, 184600L, 226439L, 239127L, 235243L, 244296L, 159696L,
189046L, 244355L, 244446L, 187595L, 162595L, 162595L, 162595L,
170604L, 188774L, 244103L, 188680L, 163611L, 200551L, 244055L,
170606L, 169154L, 194154L, 170905L, 200551L, 191412L, 166412L,
243969L, 170483L, 210719L, 168554L, 164016L, 245158L, 245131L,
245186L, 239166L, 116360L, 155698L, 155698L, 155698L, 155698L,
223827L, 191968L, 159650L, 189999L, 201193L, 201011L, 226218L,
201218L, 243970L, 244291L, 243993L, 243993L, 236035L, 236035L,
236035L, 244070L, 159692L, 194183L, 169110L, 241994L, 238216L,
238301L, 242948L, 169810L, 189280L, 164662L, 164156L, 189156L,
163989L, 163924L, 159577L, 159650L, 170566L, 170598L, 188975L,
189006L, 99983L, 191595L, 166907L, 228744L, 166621L, 243593L,
244001L, 239035L, 172934L, 238288L, 241665L, 241665L, 193991L,
238361L, 238361L, 164215L, 168867L, 194304L, 241732L, 237745L,
237911L, 195374L, 195374L, 244044L, 244044L, 169118L, 244040L,
244040L, 198518L, 244106L, 236206L, 244136L, 191390L, 164516L,
165137L, 232682L, 244021L, 244101L, 236136L, 244101L, 194181L,
169181L, 244058L, 212313L, 238240L, 242502L, 239175L, 166221L,
184500L, 170027L, 237701L, 211035L, 244050L, 243745L, 242782L,
164482L, 166341L, 189482L, 174552L, 244213L, 190960L, 184494L,
169116L, 239123L, 239121L, 165097L, 206396L, 241738L, 165622L,
242651L, 250331L, 178778L, 169133L, 238280L, 244044L, 193182L,
194156L, 194156L, 169156L, 196240L, 244060L, 244060L, 244060L,
196050L, 243546L, 243546L, 195500L, 195500L, 170389L, 195389L,
243549L, 243503L, 211398L, 243510L, 238436L, 238546L, 243907L,
243654L, 238709L, 238656L, 244171L, 244136L, 243215L, 243957L,
243957L, 164455L, 164455L, 243287L, 238203L, 243738L, 243266L,
243294L, 243548L, 243262L, 243262L, 237628L, 243266L, 243382L,
243927L, 243574L, 168364L, 243598L, 243596L, 243647L, 191094L,
243655L, 244550L, 243907L, 200636L, 210208L, 243632L, 243632L,
243367L, 243048L, 212125L, 244651L, 243357L, 202542L, 243778L,
243502L, 170036L, 237911L, 195234L, 195220L, 170220L, 239391L,
244397L, 244397L, 238631L, 225921L, 244034L, 244051L, 243310L,
189976L, 164976L, 164976L, 164999L, 165154L, 243439L, 211003L,
244034L, 243859L, 243859L, 170008L, 175602L, 238078L, 243484L,
243619L, 243333L, 243289L, 200618L, 243392L, 243376L, 164873L,
235797L, 243930L, 191502L, 243906L, 195351L, 170527L, 195307L,
243551L, 175551L, 244759L, 238122L, 178863L, 170249L, 243701L,
200549L, 236254L, 189982L, 163055L, 203863L, 243561L, 165089L,
164574L, 193750L, 238061L, 240569L, 175435L, 164313L, 243153L,
189825L, 189825L, 164825L, 189825L, 164340L, 203691L, 168483L,
243970L, 193608L, 243054L, 243115L, 243115L, 243043L, 243115L,
201917L, 204065L, 177917L, 178745L, 178735L, 243911L, 200920L,
242726L, 243042L, 204204L, 181109L, 179157L, 200093L, 179164L,
243676L, 235476L, 243862L, 243873L, 243945L, 243927L, 168102L,
168102L, 243734L, 243929L, 179053L, 246381L, 204130L, 200546L,
200301L, 174699L, 199699L, 178309L, 243549L, 204424L, 216428L,
203785L, 204101L, 245074L, 243224L, 163661L, 179036L, 199248L,
243458L, 199190L, 200330L, 200406L, 174754L, 243138L, 195257L,
244796L, 243069L, 179132L, 204171L, 243718L, 243719L, 200616L,
175749L, 179010L, 243037L, 178405L, 243953L, 243923L, 243485L,
200891L, 239635L, 243661L, 204041L, 179002L, 204070L, 206036L,
198896L, 164487L, 166891L, 246375L, 200217L, 179153L, 210112L,
243941L, 243052L, 243724L, 246328L, 164311L, 243736L, 154373L,
192956L, 237690L, 193282L, 244901L, 198985L, 246315L, 179272L,
204007L, 202386L, 246315L, 202386L, 178856L, 243704L, 243750L,
164533L, 246330L, 204082L, 243790L, 189359L, 164359L, 168286L,
168286L, 175262L, 164395L, 189395L, 164299L, 189299L, 189110L,
154953L, 166251L, 175373L, 235883L), BUNKER = c(350L, 405L, 276L,
350L, 373L, 355L, 370L, 343L, 345L, 288L, 313L, 358L, 440L, 292L,
318L, 360L, 318L, 288L, 350L, 349L, 350L, 318L, 345L, 313L, 313L,
378L, 298L, 363L, 315L, 435L, 423L, 440L, 343L, 355L, 313L, 318L,
435L, 313L, 345L, 318L, 349L, 353L, 368L, 362L, 348L, 345L, 296L,
313L, 365L, 355L, 368L, 362L, 378L, 348L, 313L, 418L, 348L, 418L,
345L, 362L, 318L, 350L, 300L, 343L, 348L, 349L, 298L, 313L, 303L,
388L, 370L, 360L, 362L, 338L, 313L, 350L, 313L, 423L, 313L, 343L,
353L, 313L, 318L, 360L, 292L, 423L, 298L, 343L, 313L, 367L, 368L,
303L, 355L, 353L, 370L, 296L, 303L, 355L, 343L, 313L, 353L, 370L,
313L, 303L, 418L, 373L, 353L, 349L, 349L, 363L, 367L, 355L, 365L,
443L, 440L, 350L, 363L, 318L, 423L, 364L, 313L, 422L, 358L, 430L,
358L, 343L, 370L, 298L, 362L, 378L, 419L, 445L, 362L, 313L, 432L,
373L, 355L, 318L, 353L, 283L, 338L, 255L, 276L, 276L, 430L, 313L,
367L, 276L, 300L, 313L, 283L, 350L, 313L, 313L, 362L, 288L, 425L,
313L, 348L, 426L, 345L, 313L, 353L, 355L, 443L, 355L, 423L, 343L,
355L, 348L, 303L, 298L, 318L, 367L, 313L, 435L, 313L, 425L, 355L,
318L, 368L, 370L, 343L, 430L, 348L, 300L, 313L, 423L, 350L, 443L,
338L, 276L, 292L, 358L, 378L, 313L, 443L, 313L, 348L, 338L, 370L,
313L, 318L, 360L, 363L, 358L, 345L, 353L, 318L, 313L, 338L, 345L,
345L, 313L, 355L, 348L, 313L, 422L, 363L, 313L, 276L, 318L, 350L,
363L, 313L, 292L, 350L, 368L, 418L, 298L, 375L, 313L, 315L, 353L,
313L, 288L, 348L, 360L, 413L, 318L, 345L, 365L, 292L, 348L, 318L,
362L, 426L, 313L, 365L, 367L, 315L, 368L, 425L, 276L, 345L, 360L,
350L, 405L, 362L, 313L, 350L, 343L, 360L, 313L, 355L, 303L, 358L,
419L, 350L, 298L, 367L, 313L, 343L, 405L, 419L, 345L, 303L, 367L,
265L, 378L, 345L, 318L, 432L, 350L, 445L, 303L, 364L, 296L, 418L,
365L, 370L, 313L, 362L, 318L, 313L, 353L, 373L, 360L, 345L, 313L,
353L, 422L, 365L, 315L, 365L, 313L, 313L, 360L, 413L, 345L, 318L,
338L, 355L, 313L, 349L, 418L, 360L, 303L, 313L, 355L, 313L, 318L,
367L, 425L, 270L, 318L, 349L, 353L, 318L, 349L, 345L, 368L, 318L,
313L, 362L, 338L, 303L, 296L, 345L, 364L, 283L, 368L, 368L, 343L,
423L, 367L, 368L, 313L, 298L, 355L, 405L, 292L, 368L, 355L, 440L,
313L, 313L, 313L, 438L, 358L, 313L, 292L, 338L, 313L, 313L, 373L,
360L, 345L, 423L, 348L, 370L, 292L, 303L, 345L, 265L, 364L, 315L,
338L, 350L, 368L, 313L, 318L, 370L, 303L, 423L, 388L, 343L, 362L,
355L, 426L, 350L, 365L, 345L, 355L, 343L, 443L, 313L, 270L, 360L,
350L, 435L, 445L, 313L, 348L, 355L, 430L, 362L, 349L, 349L, 298L,
313L, 292L, 375L, 367L, 318L, 315L, 368L, 296L, 300L, 318L, 296L,
425L, 355L, 288L, 353L, 370L, 362L, 355L, 318L, 313L, 435L, 343L,
435L, 292L, 355L, 440L, 338L, 313L, 355L, 288L, 440L, 435L, 303L,
360L, 270L, 435L, 283L, 373L, 353L, 265L, 265L, 425L, 367L, 353L,
367L, 448L, 368L, 283L, 350L, 343L, 353L, 303L, 355L, 368L, 373L,
343L, 375L, 348L, 413L, 362L, 303L, 298L, 313L, 300L, 440L, 349L,
355L, 318L, 355L, 388L, 363L, 440L, 292L, 373L, 349L, 300L, 315L,
338L, 373L, 353L, 348L, 370L, 362L, 338L, 440L, 440L, 350L, 296L,
343L, 368L, 349L, 423L, 364L, 348L, 349L, 423L, 353L, 345L, 370L,
292L, 355L, 349L, 355L, 276L, 440L, 283L, 358L, 375L, 348L, 440L,
355L, 423L, 445L, 368L, 348L, 355L, 367L), CHARTERVALUE = c(14000L,
12825L, 10475L, 11850L, 13250L, 12100L, 11875L, 14500L, 12500L,
10500L, 13375L, 14500L, 13400L, 11000L, 12750L, 11625L, 11875L,
10500L, 11850L, 11900L, 11850L, 12750L, 12500L, 12000L, 12250L,
12750L, 10450L, 12900L, 12425L, 13375L, 12075L, 13400L, 12625L,
11125L, 12000L, 11875L, 13400L, 12000L, 12500L, 12750L, 11900L,
13625L, 12750L, 11800L, 12500L, 12500L, 9850L, 12000L, 12350L,
11125L, 12750L, 11800L, 12750L, 12500L, 12250L, 13125L, 13125L,
13125L, 12500L, 11800L, 11875L, 11850L, 11500L, 12625L, 13125L,
11900L, 10425L, 12250L, 12375L, 12400L, 11875L, 11625L, 11800L,
12400L, 12000L, 14000L, 12000L, 13125L, 12250L, 12625L, 13875L,
12400L, 11875L, 11625L, 11000L, 12075L, 10450L, 12625L, 13375L,
12875L, 13125L, 12375L, 11125L, 13625L, 11875L, 9850L, 12375L,
12100L, 14500L, 12000L, 13875L, 11875L, 12400L, 12375L, 13125L,
13250L, 13875L, 11900L, 11900L, 12900L, 12875L, 12100L, 12350L,
12375L, 13125L, 11850L, 12900L, 12750L, 13125L, 13875L, 13375L,
13025L, 14500L, 13400L, 14500L, 12625L, 11875L, 10450L, 11800L,
12750L, 12625L, 12250L, 11800L, 12250L, 13250L, 13250L, 12100L,
12750L, 13625L, 11125L, 12400L, 10250L, 10475L, 10475L, 13400L,
13375L, 12875L, 10475L, 11500L, 12400L, 11125L, 11850L, 13375L,
12400L, 11800L, 10500L, 13375L, 13375L, 12500L, 12625L, 12500L,
12000L, 13875L, 11125L, 12375L, 11125L, 13125L, 12625L, 12100L,
12500L, 12375L, 10450L, 12750L, 12875L, 12250L, 13400L, 12250L,
13375L, 11125L, 12750L, 12750L, 11875L, 14500L, 13400L, 12500L,
11500L, 13375L, 13125L, 11850L, 12375L, 12400L, 10475L, 11000L,
14500L, 12750L, 12400L, 12375L, 12250L, 12500L, 12400L, 11875L,
12250L, 12750L, 11625L, 12900L, 14500L, 12500L, 13875L, 11875L,
13375L, 12400L, 12500L, 12500L, 13375L, 12100L, 12500L, 12400L,
13025L, 12900L, 12400L, 10475L, 12750L, 11850L, 12900L, 13375L,
11000L, 11850L, 13125L, 13125L, 10450L, 12500L, 12250L, 12425L,
13875L, 12000L, 10500L, 13125L, 11625L, 12975L, 12750L, 12500L,
12350L, 11000L, 13125L, 12750L, 11800L, 12625L, 13375L, 12350L,
12875L, 12425L, 12750L, 12675L, 10475L, 12500L, 11625L, 11850L,
12825L, 11800L, 13375L, 14000L, 12625L, 11625L, 12400L, 11125L,
12375L, 14500L, 12625L, 14000L, 10425L, 12875L, 13375L, 14500L,
12825L, 12625L, 12500L, 12375L, 12875L, 9875L, 12750L, 12500L,
12750L, 13250L, 11850L, 12250L, 12375L, 13875L, 9850L, 13125L,
12350L, 11875L, 12000L, 11800L, 11875L, 12000L, 13875L, 13250L,
11625L, 12500L, 12400L, 13625L, 13025L, 12350L, 12425L, 12350L,
12400L, 12400L, 11625L, 12975L, 12500L, 11875L, 12400L, 11125L,
12000L, 11900L, 13125L, 11625L, 12375L, 12250L, 12100L, 13375L,
12750L, 12875L, 12675L, 10000L, 11875L, 11900L, 13625L, 12750L,
11900L, 12500L, 12750L, 12750L, 12000L, 11800L, 12400L, 12375L,
9850L, 12500L, 13875L, 11125L, 12750L, 12750L, 12625L, 12075L,
12875L, 13125L, 12250L, 10450L, 11125L, 12825L, 11000L, 13125L,
11125L, 13125L, 12000L, 12000L, 13375L, 13375L, 14500L, 12000L,
11000L, 12400L, 12250L, 12000L, 13250L, 11625L, 12500L, 13125L,
13125L, 11875L, 11000L, 12375L, 12500L, 9875L, 13875L, 12425L,
12400L, 14000L, 12750L, 12400L, 11875L, 11875L, 12375L, 12075L,
12400L, 14500L, 11800L, 12100L, 12625L, 14000L, 12350L, 12500L,
12100L, 12625L, 12375L, 12250L, 10000L, 11625L, 14000L, 13375L,
12250L, 13375L, 12500L, 11125L, 13400L, 11800L, 11900L, 11900L,
10425L, 12400L, 11000L, 12500L, 12875L, 12750L, 12425L, 12750L,
9850L, 11500L, 12750L, 9850L, 13375L, 11125L, 10500L, 13875L,
11875L, 11800L, 11125L, 12750L, 12250L, 13375L, 12625L, 13375L,
11000L, 11125L, 13125L, 12400L, 13375L, 12100L, 10500L, 13075L,
13375L, 12375L, 11625L, 10000L, 13400L, 11125L, 13250L, 13875L,
9875L, 9875L, 13375L, 12875L, 13875L, 12875L, 13500L, 12750L,
11125L, 11850L, 12625L, 13875L, 12375L, 12100L, 13125L, 13250L,
12625L, 12500L, 13125L, 12975L, 11800L, 12375L, 10425L, 13375L,
11500L, 13075L, 11900L, 12100L, 11875L, 11125L, 12400L, 12900L,
13400L, 11000L, 13250L, 11900L, 11500L, 12425L, 12400L, 13250L,
13625L, 12500L, 11875L, 11800L, 12400L, 13125L, 13075L, 14000L,
9850L, 14500L, 13125L, 11900L, 13125L, 13875L, 13125L, 11900L,
13125L, 13875L, 12500L, 11875L, 11000L, 11125L, 11900L, 11125L,
10475L, 13075L, 11125L, 14500L, 12500L, 13125L, 13125L, 12100L,
13125L, 12250L, 13125L, 12500L, 11125L, 12875L)), class = "data.frame",
row.names = c(NA,
-527L))


Best regards,

Paul

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Mon May  6 19:48:29 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 6 May 2019 10:48:29 -0700
Subject: [R] Errors Generated by Function gof from LogisticDx Package
In-Reply-To: <CAMOcQfPUKQr5WGUFHA-0a-q8v6RkSzx23TrS1TXO0Yq=sQ4VCg@mail.gmail.com>
References: <CAMOcQfPUKQr5WGUFHA-0a-q8v6RkSzx23TrS1TXO0Yq=sQ4VCg@mail.gmail.com>
Message-ID: <CAGxFJbR2gOp7=QGC73BEbBHQcUtM-qZzkoiZB8g2j7YpvpJcCg@mail.gmail.com>

The warning message should be your hint. You cannot have gof statistics
with perfect separation. See here for some suggestions:

https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, May 6, 2019 at 10:36 AM Paul Bernal <paulbernal07 at gmail.com> wrote:

> Dear Christopher and friends,
>
> Hope you are all doing great. I am currently using R version 3.6.0 and I
> have a Windows 8, 64-bit Operating System.
>
> When applying function gof on my glm model, I get the following errors:
>
> > gof(GLM1)
> Error in factor(G, labels = dx1[, format(max(P), digits = 3), by = G]$V1) :
>   invalid 'labels'; length 11 should be 1 or 10
>
> > gof(glm(TRANSIT ~ Draft + TOTALCOST + BUNKER + CHARTERVALUE,
> family=binomial, data=Dataset), plot=FALSE)
> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
> Error in factor(G, labels = dx1[, format(max(P), digits = 3), by = G]$V1) :
>   invalid 'labels'; length 11 should be 1 or 10
>
> I would like to know what I am doing wrong, or if there is any issue with
> the gof function that I am not aware of.
>
> I provide de dput() of my dataset as well as other details below.
>
>
> My code is as follows:
>
> > library(LogisticDx)
> Registered S3 methods overwritten by 'ggplot2':
>   method         from
>   [.quosures     rlang
>   c.quosures     rlang
>   print.quosures rlang
>
> > GLM1 <- glm(TRANSIT ~ TOTALCOST + Draft + CHARTERVALUE + BUNKER,
> +   family=binomial(logit), data=Dataset)
>
> > dput(Dataset)
> structure(list(TRANSIT = c(1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 0L,
> 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 1L, 0L,
> 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 0L,
> 1L, 1L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L,
> 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L,
> 1L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L,
> 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L,
> 1L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L,
> 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L,
> 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L,
> 0L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L,
> 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L,
> 1L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 0L,
> 1L, 0L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L,
> 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 0L,
> 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L,
> 1L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L,
> 1L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 1L,
> 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L,
> 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L,
> 1L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L,
> 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 0L,
> 0L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 1L,
> 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 1L,
> 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L,
> 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L,
> 0L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L,
> 0L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L,
> 1L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L), Draft = c(12L, 12L, 12L, 13L, 12L, 12L,
> 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 14L, 14L, 14L, 12L, 13L,
> 12L, 12L, 14L, 12L, 14L, 13L, 12L, 12L, 12L, 14L, 12L, 12L, 11L,
> 13L, 13L, 14L, 12L, 12L, 13L, 14L, 12L, 13L, 13L, 12L, 14L, 14L,
> 13L, 12L, 14L, 14L, 13L, 14L, 14L, 12L, 13L, 12L, 12L, 13L, 12L,
> 12L, 14L, 14L, 14L, 12L, 12L, 12L, 12L, 14L, 13L, 14L, 12L, 13L,
> 14L, 13L, 12L, 12L, 12L, 12L, 12L, 12L, 14L, 12L, 14L, 13L, 14L,
> 14L, 12L, 12L, 12L, 14L, 12L, 12L, 13L, 14L, 13L, 13L, 12L, 14L,
> 13L, 13L, 14L, 12L, 12L, 14L, 12L, 12L, 14L, 12L, 13L, 13L, 14L,
> 14L, 14L, 14L, 12L, 12L, 14L, 13L, 12L, 12L, 12L, 13L, 12L, 14L,
> 12L, 12L, 14L, 14L, 12L, 12L, 12L, 12L, 12L, 12L, 13L, 12L, 12L,
> 12L, 13L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 14L, 12L, 12L, 12L,
> 12L, 14L, 14L, 14L, 14L, 10L, 12L, 11L, 12L, 12L, 12L, 12L, 12L,
> 12L, 12L, 12L, 12L, 12L, 13L, 14L, 14L, 14L, 12L, 12L, 12L, 14L,
> 12L, 12L, 12L, 14L, 14L, 14L, 14L, 12L, 12L, 11L, 12L, 12L, 12L,
> 12L, 12L, 12L, 12L, 12L, 12L, 12L, 10L, 12L, 12L, 12L, 11L, 13L,
> 14L, 14L, 12L, 13L, 14L, 14L, 12L, 13L, 14L, 12L, 12L, 12L, 14L,
> 13L, 14L, 12L, 12L, 14L, 14L, 12L, 14L, 13L, 12L, 14L, 12L, 14L,
> 12L, 12L, 12L, 12L, 14L, 13L, 12L, 13L, 12L, 12L, 14L, 12L, 14L,
> 14L, 14L, 12L, 12L, 12L, 13L, 12L, 14L, 14L, 14L, 12L, 12L, 12L,
> 12L, 13L, 12L, 12L, 12L, 14L, 14L, 12L, 12L, 14L, 12L, 14L, 14L,
> 12L, 12L, 12L, 13L, 12L, 12L, 12L, 12L, 12L, 14L, 14L, 13L, 12L,
> 13L, 14L, 12L, 12L, 12L, 12L, 13L, 14L, 12L, 14L, 13L, 14L, 14L,
> 14L, 14L, 14L, 13L, 14L, 14L, 13L, 14L, 12L, 12L, 14L, 14L, 14L,
> 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 13L, 12L, 14L, 14L,
> 14L, 12L, 14L, 14L, 14L, 12L, 12L, 14L, 14L, 14L, 14L, 12L, 14L,
> 14L, 12L, 14L, 14L, 12L, 13L, 12L, 12L, 12L, 14L, 14L, 13L, 14L,
> 12L, 13L, 13L, 13L, 12L, 12L, 12L, 12L, 12L, 14L, 12L, 14L, 13L,
> 14L, 12L, 12L, 14L, 13L, 14L, 14L, 14L, 12L, 14L, 14L, 12L, 12L,
> 14L, 12L, 14L, 12L, 12L, 12L, 14L, 12L, 13L, 14L, 12L, 12L, 14L,
> 12L, 12L, 12L, 12L, 12L, 14L, 12L, 12L, 12L, 14L, 14L, 12L, 12L,
> 14L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 14L, 12L, 13L, 13L, 13L,
> 14L, 14L, 12L, 12L, 12L, 12L, 12L, 13L, 12L, 14L, 13L, 12L, 12L,
> 12L, 12L, 12L, 13L, 12L, 14L, 13L, 13L, 13L, 11L, 12L, 14L, 14L,
> 12L, 14L, 12L, 11L, 12L, 12L, 12L, 12L, 14L, 12L, 12L, 12L, 12L,
> 14L, 14L, 12L, 12L, 12L, 14L, 12L, 12L, 12L, 12L, 14L, 12L, 13L,
> 14L, 12L, 12L, 14L, 14L, 12L, 12L, 12L, 13L, 12L, 14L, 14L, 14L,
> 12L, 14L, 13L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 13L, 6L, 12L,
> 12L, 14L, 14L, 14L, 14L, 12L, 14L, 12L, 12L, 12L, 12L, 14L, 12L,
> 14L, 12L, 12L, 12L, 14L, 12L, 12L, 13L, 14L, 12L, 13L, 12L, 14L,
> 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L,
> 12L), TOTALCOST = c(194364L, 219364L, 198260L, 237456L, 197159L,
> 198992L, 194337L, 219337L, 199198L, 196604L, 230607L, 196604L,
> 196604L, 194496L, 238600L, 236936L, 237476L, 197220L, 236950L,
> 197300L, 182042L, 237938L, 199221L, 237475L, 239190L, 157406L,
> 157211L, 182211L, 237475L, 182475L, 181599L, 156599L, 238269L,
> 238402L, 238069L, 161436L, 225031L, 238180L, 237572L, 189861L,
> 239005L, 239049L, 163814L, 240064L, 239171L, 238410L, 200878L,
> 239019L, 239087L, 239350L, 239352L, 240275L, 164844L, 238400L,
> 225158L, 202495L, 239681L, 201791L, 226863L, 244092L, 244590L,
> 239171L, 189811L, 219412L, 228480L, 203650L, 237514L, 247451L,
> 244739L, 211770L, 244308L, 239197L, 238419L, 224977L, 157362L,
> 162434L, 162434L, 162434L, 162434L, 239681L, 163316L, 237265L,
> 243920L, 244088L, 244163L, 202256L, 159592L, 201346L, 239187L,
> 189800L, 191959L, 239476L, 239171L, 238087L, 238052L, 164169L,
> 245057L, 244215L, 240812L, 239156L, 156879L, 197853L, 245367L,
> 164710L, 164710L, 244192L, 211110L, 239156L, 244213L, 237504L,
> 239018L, 241150L, 244447L, 238506L, 210298L, 243482L, 239166L,
> 159489L, 184600L, 226439L, 239127L, 235243L, 244296L, 159696L,
> 189046L, 244355L, 244446L, 187595L, 162595L, 162595L, 162595L,
> 170604L, 188774L, 244103L, 188680L, 163611L, 200551L, 244055L,
> 170606L, 169154L, 194154L, 170905L, 200551L, 191412L, 166412L,
> 243969L, 170483L, 210719L, 168554L, 164016L, 245158L, 245131L,
> 245186L, 239166L, 116360L, 155698L, 155698L, 155698L, 155698L,
> 223827L, 191968L, 159650L, 189999L, 201193L, 201011L, 226218L,
> 201218L, 243970L, 244291L, 243993L, 243993L, 236035L, 236035L,
> 236035L, 244070L, 159692L, 194183L, 169110L, 241994L, 238216L,
> 238301L, 242948L, 169810L, 189280L, 164662L, 164156L, 189156L,
> 163989L, 163924L, 159577L, 159650L, 170566L, 170598L, 188975L,
> 189006L, 99983L, 191595L, 166907L, 228744L, 166621L, 243593L,
> 244001L, 239035L, 172934L, 238288L, 241665L, 241665L, 193991L,
> 238361L, 238361L, 164215L, 168867L, 194304L, 241732L, 237745L,
> 237911L, 195374L, 195374L, 244044L, 244044L, 169118L, 244040L,
> 244040L, 198518L, 244106L, 236206L, 244136L, 191390L, 164516L,
> 165137L, 232682L, 244021L, 244101L, 236136L, 244101L, 194181L,
> 169181L, 244058L, 212313L, 238240L, 242502L, 239175L, 166221L,
> 184500L, 170027L, 237701L, 211035L, 244050L, 243745L, 242782L,
> 164482L, 166341L, 189482L, 174552L, 244213L, 190960L, 184494L,
> 169116L, 239123L, 239121L, 165097L, 206396L, 241738L, 165622L,
> 242651L, 250331L, 178778L, 169133L, 238280L, 244044L, 193182L,
> 194156L, 194156L, 169156L, 196240L, 244060L, 244060L, 244060L,
> 196050L, 243546L, 243546L, 195500L, 195500L, 170389L, 195389L,
> 243549L, 243503L, 211398L, 243510L, 238436L, 238546L, 243907L,
> 243654L, 238709L, 238656L, 244171L, 244136L, 243215L, 243957L,
> 243957L, 164455L, 164455L, 243287L, 238203L, 243738L, 243266L,
> 243294L, 243548L, 243262L, 243262L, 237628L, 243266L, 243382L,
> 243927L, 243574L, 168364L, 243598L, 243596L, 243647L, 191094L,
> 243655L, 244550L, 243907L, 200636L, 210208L, 243632L, 243632L,
> 243367L, 243048L, 212125L, 244651L, 243357L, 202542L, 243778L,
> 243502L, 170036L, 237911L, 195234L, 195220L, 170220L, 239391L,
> 244397L, 244397L, 238631L, 225921L, 244034L, 244051L, 243310L,
> 189976L, 164976L, 164976L, 164999L, 165154L, 243439L, 211003L,
> 244034L, 243859L, 243859L, 170008L, 175602L, 238078L, 243484L,
> 243619L, 243333L, 243289L, 200618L, 243392L, 243376L, 164873L,
> 235797L, 243930L, 191502L, 243906L, 195351L, 170527L, 195307L,
> 243551L, 175551L, 244759L, 238122L, 178863L, 170249L, 243701L,
> 200549L, 236254L, 189982L, 163055L, 203863L, 243561L, 165089L,
> 164574L, 193750L, 238061L, 240569L, 175435L, 164313L, 243153L,
> 189825L, 189825L, 164825L, 189825L, 164340L, 203691L, 168483L,
> 243970L, 193608L, 243054L, 243115L, 243115L, 243043L, 243115L,
> 201917L, 204065L, 177917L, 178745L, 178735L, 243911L, 200920L,
> 242726L, 243042L, 204204L, 181109L, 179157L, 200093L, 179164L,
> 243676L, 235476L, 243862L, 243873L, 243945L, 243927L, 168102L,
> 168102L, 243734L, 243929L, 179053L, 246381L, 204130L, 200546L,
> 200301L, 174699L, 199699L, 178309L, 243549L, 204424L, 216428L,
> 203785L, 204101L, 245074L, 243224L, 163661L, 179036L, 199248L,
> 243458L, 199190L, 200330L, 200406L, 174754L, 243138L, 195257L,
> 244796L, 243069L, 179132L, 204171L, 243718L, 243719L, 200616L,
> 175749L, 179010L, 243037L, 178405L, 243953L, 243923L, 243485L,
> 200891L, 239635L, 243661L, 204041L, 179002L, 204070L, 206036L,
> 198896L, 164487L, 166891L, 246375L, 200217L, 179153L, 210112L,
> 243941L, 243052L, 243724L, 246328L, 164311L, 243736L, 154373L,
> 192956L, 237690L, 193282L, 244901L, 198985L, 246315L, 179272L,
> 204007L, 202386L, 246315L, 202386L, 178856L, 243704L, 243750L,
> 164533L, 246330L, 204082L, 243790L, 189359L, 164359L, 168286L,
> 168286L, 175262L, 164395L, 189395L, 164299L, 189299L, 189110L,
> 154953L, 166251L, 175373L, 235883L), BUNKER = c(350L, 405L, 276L,
> 350L, 373L, 355L, 370L, 343L, 345L, 288L, 313L, 358L, 440L, 292L,
> 318L, 360L, 318L, 288L, 350L, 349L, 350L, 318L, 345L, 313L, 313L,
> 378L, 298L, 363L, 315L, 435L, 423L, 440L, 343L, 355L, 313L, 318L,
> 435L, 313L, 345L, 318L, 349L, 353L, 368L, 362L, 348L, 345L, 296L,
> 313L, 365L, 355L, 368L, 362L, 378L, 348L, 313L, 418L, 348L, 418L,
> 345L, 362L, 318L, 350L, 300L, 343L, 348L, 349L, 298L, 313L, 303L,
> 388L, 370L, 360L, 362L, 338L, 313L, 350L, 313L, 423L, 313L, 343L,
> 353L, 313L, 318L, 360L, 292L, 423L, 298L, 343L, 313L, 367L, 368L,
> 303L, 355L, 353L, 370L, 296L, 303L, 355L, 343L, 313L, 353L, 370L,
> 313L, 303L, 418L, 373L, 353L, 349L, 349L, 363L, 367L, 355L, 365L,
> 443L, 440L, 350L, 363L, 318L, 423L, 364L, 313L, 422L, 358L, 430L,
> 358L, 343L, 370L, 298L, 362L, 378L, 419L, 445L, 362L, 313L, 432L,
> 373L, 355L, 318L, 353L, 283L, 338L, 255L, 276L, 276L, 430L, 313L,
> 367L, 276L, 300L, 313L, 283L, 350L, 313L, 313L, 362L, 288L, 425L,
> 313L, 348L, 426L, 345L, 313L, 353L, 355L, 443L, 355L, 423L, 343L,
> 355L, 348L, 303L, 298L, 318L, 367L, 313L, 435L, 313L, 425L, 355L,
> 318L, 368L, 370L, 343L, 430L, 348L, 300L, 313L, 423L, 350L, 443L,
> 338L, 276L, 292L, 358L, 378L, 313L, 443L, 313L, 348L, 338L, 370L,
> 313L, 318L, 360L, 363L, 358L, 345L, 353L, 318L, 313L, 338L, 345L,
> 345L, 313L, 355L, 348L, 313L, 422L, 363L, 313L, 276L, 318L, 350L,
> 363L, 313L, 292L, 350L, 368L, 418L, 298L, 375L, 313L, 315L, 353L,
> 313L, 288L, 348L, 360L, 413L, 318L, 345L, 365L, 292L, 348L, 318L,
> 362L, 426L, 313L, 365L, 367L, 315L, 368L, 425L, 276L, 345L, 360L,
> 350L, 405L, 362L, 313L, 350L, 343L, 360L, 313L, 355L, 303L, 358L,
> 419L, 350L, 298L, 367L, 313L, 343L, 405L, 419L, 345L, 303L, 367L,
> 265L, 378L, 345L, 318L, 432L, 350L, 445L, 303L, 364L, 296L, 418L,
> 365L, 370L, 313L, 362L, 318L, 313L, 353L, 373L, 360L, 345L, 313L,
> 353L, 422L, 365L, 315L, 365L, 313L, 313L, 360L, 413L, 345L, 318L,
> 338L, 355L, 313L, 349L, 418L, 360L, 303L, 313L, 355L, 313L, 318L,
> 367L, 425L, 270L, 318L, 349L, 353L, 318L, 349L, 345L, 368L, 318L,
> 313L, 362L, 338L, 303L, 296L, 345L, 364L, 283L, 368L, 368L, 343L,
> 423L, 367L, 368L, 313L, 298L, 355L, 405L, 292L, 368L, 355L, 440L,
> 313L, 313L, 313L, 438L, 358L, 313L, 292L, 338L, 313L, 313L, 373L,
> 360L, 345L, 423L, 348L, 370L, 292L, 303L, 345L, 265L, 364L, 315L,
> 338L, 350L, 368L, 313L, 318L, 370L, 303L, 423L, 388L, 343L, 362L,
> 355L, 426L, 350L, 365L, 345L, 355L, 343L, 443L, 313L, 270L, 360L,
> 350L, 435L, 445L, 313L, 348L, 355L, 430L, 362L, 349L, 349L, 298L,
> 313L, 292L, 375L, 367L, 318L, 315L, 368L, 296L, 300L, 318L, 296L,
> 425L, 355L, 288L, 353L, 370L, 362L, 355L, 318L, 313L, 435L, 343L,
> 435L, 292L, 355L, 440L, 338L, 313L, 355L, 288L, 440L, 435L, 303L,
> 360L, 270L, 435L, 283L, 373L, 353L, 265L, 265L, 425L, 367L, 353L,
> 367L, 448L, 368L, 283L, 350L, 343L, 353L, 303L, 355L, 368L, 373L,
> 343L, 375L, 348L, 413L, 362L, 303L, 298L, 313L, 300L, 440L, 349L,
> 355L, 318L, 355L, 388L, 363L, 440L, 292L, 373L, 349L, 300L, 315L,
> 338L, 373L, 353L, 348L, 370L, 362L, 338L, 440L, 440L, 350L, 296L,
> 343L, 368L, 349L, 423L, 364L, 348L, 349L, 423L, 353L, 345L, 370L,
> 292L, 355L, 349L, 355L, 276L, 440L, 283L, 358L, 375L, 348L, 440L,
> 355L, 423L, 445L, 368L, 348L, 355L, 367L), CHARTERVALUE = c(14000L,
> 12825L, 10475L, 11850L, 13250L, 12100L, 11875L, 14500L, 12500L,
> 10500L, 13375L, 14500L, 13400L, 11000L, 12750L, 11625L, 11875L,
> 10500L, 11850L, 11900L, 11850L, 12750L, 12500L, 12000L, 12250L,
> 12750L, 10450L, 12900L, 12425L, 13375L, 12075L, 13400L, 12625L,
> 11125L, 12000L, 11875L, 13400L, 12000L, 12500L, 12750L, 11900L,
> 13625L, 12750L, 11800L, 12500L, 12500L, 9850L, 12000L, 12350L,
> 11125L, 12750L, 11800L, 12750L, 12500L, 12250L, 13125L, 13125L,
> 13125L, 12500L, 11800L, 11875L, 11850L, 11500L, 12625L, 13125L,
> 11900L, 10425L, 12250L, 12375L, 12400L, 11875L, 11625L, 11800L,
> 12400L, 12000L, 14000L, 12000L, 13125L, 12250L, 12625L, 13875L,
> 12400L, 11875L, 11625L, 11000L, 12075L, 10450L, 12625L, 13375L,
> 12875L, 13125L, 12375L, 11125L, 13625L, 11875L, 9850L, 12375L,
> 12100L, 14500L, 12000L, 13875L, 11875L, 12400L, 12375L, 13125L,
> 13250L, 13875L, 11900L, 11900L, 12900L, 12875L, 12100L, 12350L,
> 12375L, 13125L, 11850L, 12900L, 12750L, 13125L, 13875L, 13375L,
> 13025L, 14500L, 13400L, 14500L, 12625L, 11875L, 10450L, 11800L,
> 12750L, 12625L, 12250L, 11800L, 12250L, 13250L, 13250L, 12100L,
> 12750L, 13625L, 11125L, 12400L, 10250L, 10475L, 10475L, 13400L,
> 13375L, 12875L, 10475L, 11500L, 12400L, 11125L, 11850L, 13375L,
> 12400L, 11800L, 10500L, 13375L, 13375L, 12500L, 12625L, 12500L,
> 12000L, 13875L, 11125L, 12375L, 11125L, 13125L, 12625L, 12100L,
> 12500L, 12375L, 10450L, 12750L, 12875L, 12250L, 13400L, 12250L,
> 13375L, 11125L, 12750L, 12750L, 11875L, 14500L, 13400L, 12500L,
> 11500L, 13375L, 13125L, 11850L, 12375L, 12400L, 10475L, 11000L,
> 14500L, 12750L, 12400L, 12375L, 12250L, 12500L, 12400L, 11875L,
> 12250L, 12750L, 11625L, 12900L, 14500L, 12500L, 13875L, 11875L,
> 13375L, 12400L, 12500L, 12500L, 13375L, 12100L, 12500L, 12400L,
> 13025L, 12900L, 12400L, 10475L, 12750L, 11850L, 12900L, 13375L,
> 11000L, 11850L, 13125L, 13125L, 10450L, 12500L, 12250L, 12425L,
> 13875L, 12000L, 10500L, 13125L, 11625L, 12975L, 12750L, 12500L,
> 12350L, 11000L, 13125L, 12750L, 11800L, 12625L, 13375L, 12350L,
> 12875L, 12425L, 12750L, 12675L, 10475L, 12500L, 11625L, 11850L,
> 12825L, 11800L, 13375L, 14000L, 12625L, 11625L, 12400L, 11125L,
> 12375L, 14500L, 12625L, 14000L, 10425L, 12875L, 13375L, 14500L,
> 12825L, 12625L, 12500L, 12375L, 12875L, 9875L, 12750L, 12500L,
> 12750L, 13250L, 11850L, 12250L, 12375L, 13875L, 9850L, 13125L,
> 12350L, 11875L, 12000L, 11800L, 11875L, 12000L, 13875L, 13250L,
> 11625L, 12500L, 12400L, 13625L, 13025L, 12350L, 12425L, 12350L,
> 12400L, 12400L, 11625L, 12975L, 12500L, 11875L, 12400L, 11125L,
> 12000L, 11900L, 13125L, 11625L, 12375L, 12250L, 12100L, 13375L,
> 12750L, 12875L, 12675L, 10000L, 11875L, 11900L, 13625L, 12750L,
> 11900L, 12500L, 12750L, 12750L, 12000L, 11800L, 12400L, 12375L,
> 9850L, 12500L, 13875L, 11125L, 12750L, 12750L, 12625L, 12075L,
> 12875L, 13125L, 12250L, 10450L, 11125L, 12825L, 11000L, 13125L,
> 11125L, 13125L, 12000L, 12000L, 13375L, 13375L, 14500L, 12000L,
> 11000L, 12400L, 12250L, 12000L, 13250L, 11625L, 12500L, 13125L,
> 13125L, 11875L, 11000L, 12375L, 12500L, 9875L, 13875L, 12425L,
> 12400L, 14000L, 12750L, 12400L, 11875L, 11875L, 12375L, 12075L,
> 12400L, 14500L, 11800L, 12100L, 12625L, 14000L, 12350L, 12500L,
> 12100L, 12625L, 12375L, 12250L, 10000L, 11625L, 14000L, 13375L,
> 12250L, 13375L, 12500L, 11125L, 13400L, 11800L, 11900L, 11900L,
> 10425L, 12400L, 11000L, 12500L, 12875L, 12750L, 12425L, 12750L,
> 9850L, 11500L, 12750L, 9850L, 13375L, 11125L, 10500L, 13875L,
> 11875L, 11800L, 11125L, 12750L, 12250L, 13375L, 12625L, 13375L,
> 11000L, 11125L, 13125L, 12400L, 13375L, 12100L, 10500L, 13075L,
> 13375L, 12375L, 11625L, 10000L, 13400L, 11125L, 13250L, 13875L,
> 9875L, 9875L, 13375L, 12875L, 13875L, 12875L, 13500L, 12750L,
> 11125L, 11850L, 12625L, 13875L, 12375L, 12100L, 13125L, 13250L,
> 12625L, 12500L, 13125L, 12975L, 11800L, 12375L, 10425L, 13375L,
> 11500L, 13075L, 11900L, 12100L, 11875L, 11125L, 12400L, 12900L,
> 13400L, 11000L, 13250L, 11900L, 11500L, 12425L, 12400L, 13250L,
> 13625L, 12500L, 11875L, 11800L, 12400L, 13125L, 13075L, 14000L,
> 9850L, 14500L, 13125L, 11900L, 13125L, 13875L, 13125L, 11900L,
> 13125L, 13875L, 12500L, 11875L, 11000L, 11125L, 11900L, 11125L,
> 10475L, 13075L, 11125L, 14500L, 12500L, 13125L, 13125L, 12100L,
> 13125L, 12250L, 13125L, 12500L, 11125L, 12875L)), class = "data.frame",
> row.names = c(NA,
> -527L))
>
>
> Best regards,
>
> Paul
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From mccorm@ck @end|ng |rom mo|b|o@mgh@h@rv@rd@edu  Mon May  6 22:12:42 2019
From: mccorm@ck @end|ng |rom mo|b|o@mgh@h@rv@rd@edu (Matthew)
Date: Mon, 6 May 2019 16:12:42 -0400
Subject: [R] Fwd: Re: transpose and split dataframe
In-Reply-To: <CA+8X3fX1nS_bD8cfW+KH8iOK5TA3ft2ZwFFDWUh5xMwhRx-bJg@mail.gmail.com>
References: <e154ccbb-71a6-c2ac-41ca-2171c8a9dc76@molbio.mgh.harvard.edu>
 <e4a9e321-b437-eed6-344b-472319e85fec@molbio.mgh.harvard.edu>
 <df8f17304f9541cda96fbc48d969ca0e@tamu.edu>
 <CA+8X3fXJ=+iU+6fVGqfu33yy03t_Ap6jNao=Fgk10CTxg=mqiQ@mail.gmail.com>
 <CA+8X3fX1nS_bD8cfW+KH8iOK5TA3ft2ZwFFDWUh5xMwhRx-bJg@mail.gmail.com>
Message-ID: <e69ffd7f-d3d4-d67d-3c97-868e4a795a6b@molbio.mgh.harvard.edu>

Thank you very much Jim and David for your scripts and accompanying 
explanations.

I was intrigued at the results that came from David's script.? As seen 
below where I have taken a small piece of his DataTable:

AT1G69490 AT1G29860??? AT4G18170 *AT5G46350*
AT1G01560??? 0??? 0??? 0??? 1
*AT1G02920*??? 1??? 2??? 2??? 4
AT1G02930??? 1??? 2??? 2??? 4
AT1G05675??? 1??? 1??? 1??? 2

 ?? There are numbers other than 1 or 0, which was not what I was 
expecting. The data I am working with come from downloading results of 
an analysis done at a particular web site. I looked at Jim's solution, 
and the equivalent of the above would be:

 ??? AT1G69490 _AT1G29860_ _AT1G29860_ AT4G18170??? AT4G18170 
*AT5G46350??? AT5G46350 AT5G46350??? AT5G46350??? AT5G46350*
AT1G01560??? NA??? NA??? NA??? NA??? NA??? NA NA??? NA??? AT1G01560??? NA
*AT1G02920*??? AT1G02920??? AT1G02920 AT1G02920??? AT1G02920??? 
AT1G02920??? AT1G02920??? AT1G02920 AT1G02920??? AT1G02920??? NA
AT1G02930??? AT1G02930??? AT1G02930??? AT1G02930 AT1G02930??? 
AT1G02930??? AT1G02930??? AT1G02930??? AT1G02930 AT1G02930??? NA
AT1G05675??? AT1G05675??? AT1G05675??? NA AT1G05675??? NA??? 
AT1G05675??? AT1G05675??? NA??? NA??? NA

 ? The above is the format that I was desiring, but I was not expecting 
that a single ATG number would be the name of multiple columns. As shown 
above, _AT1G2960_ is the name of two columns and *AT5G46350* is the name 
of 5 columns (You may have to widen the e-mail across the screen to see 
it clearly). When a single ATG number, such as AT5G46350, names multiple 
columns, then the contents of each of those columns may or may not be 
the same. For example, going across a single row looking at *AT1G02920*, 
it occurs in the first column, hence the 1 in David's DataTable. It 
occurs in both AT1G29860 columns, hence the 2 in the DataTable. It again 
occurs in both AT4G18170 columns, so another 2 in the DataTable, and 
finally it occurs in only 4 of the 5 AT5G46350 columns, so the 4 in the 
DataTable.

 ??? When the same ATG number names multiple columns it is because 
different methods were used to determine the content of each column. So, 
if an ATG number such as AT1G05675 occurs in all columns with the same 
name, I then know that it was by multiple methods that this has been 
shown, and if it only occurs in some of the columns, I know that all 
methods did not associate it with the column name ATG.? David's result 
complements Jim's, and both end up being very helpful to me.

 ? Thanks again to both of you for your time and help.

Matthew


On 5/2/2019 8:40 PM, Jim Lemon wrote:
>          External Email - Use Caution
>
> Hi again,
> Just noticed that the NA fill in the original solution is unnecessary, thus:
>
> # split the second column at the commas
> hitsplit<-strsplit(mmdf$hits,",")
> # get all the sorted hits
> allhits<-sort(unique(unlist(hitsplit)))
> tmmdf<-as.data.frame(matrix(NA,ncol=length(hitsplit),nrow=length(allhits)))
> # change the names of the list
> names(tmmdf)<-mmdf$Regulator
> for(column in 1:length(hitsplit)) {
>   hitmatches<-match(hitsplit[[column]],allhits)
>   hitmatches<-hitmatches[!is.na(hitmatches)]
>   tmmdf[hitmatches,column]<-allhits[hitmatches]
> }
>
> Jim
>
> On Fri, May 3, 2019 at 10:32 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>> Hi Matthew,
>> I'm not sure whether you want something like your initial request or
>> David's solution. The result of this can be transformed into the
>> latter:
>>
>> mmdf<-read.table(text="Regulator hits
>> AT1G69490 AT4G31950,AT5G24110,AT1G26380,AT1G05675,AT3G12910,AT5G64905,AT1G22810,AT1G79680,AT3G02840,AT5G25260,AT5G57220,AT2G37430,AT2G26560,AT1G56250,AT3G23230,AT1G16420,AT1G78410,AT4G22030,AT5G05300,AT1G69930,AT4G03460,AT4G11470,AT5G25250,AT5G36925,AT2G30750,AT1G16150,AT1G02930,AT2G19190,AT4G11890,AT1G72520,AT4G31940,AT5G37490,AT5G52760,AT5G66020,AT3G57460,AT4G23220,AT3G15518,AT2G43620,AT2G02010,AT1G35210,AT5G46295,AT1G17147,AT1G11925,AT2G39200,AT1G02920,AT2G40180,AT1G59865,AT4G35180,AT4G15417,AT1G51820,AT1G06135,AT1G36622,AT5G42830
>> AT1G29860 AT4G31950,AT5G24110,AT1G05675,AT3G12910,AT5G64905,AT1G22810,AT1G14540,AT1G79680,AT1G07160,AT3G23250,AT5G25260,AT1G53625,AT5G57220,AT2G37430,AT3G54150,AT1G56250,AT3G23230,AT1G16420,AT1G78410,AT4G22030,AT1G69930,AT4G03460,AT4G11470,AT5G25250,AT5G36925,AT4G14450,AT2G30750,AT1G16150,AT1G02930,AT2G19190,AT4G11890,AT1G72520,AT4G31940,AT5G37490,AT4G08555,AT5G66020,AT5G26920,AT3G57460,AT4G23220,AT3G15518,AT2G43620,AT1G35210,AT5G46295,AT1G17147,AT1G11925,AT2G39200,AT1G02920,AT4G35180,AT4G15417,AT1G51820,AT4G40020,AT1G06135
>> AT1G2986 AT5G64905,AT1G21120,AT1G07160,AT5G25260,AT1G53625,AT1G56250,AT2G31345,AT4G11170,AT1G66090,AT1G26410,AT3G55840,AT1G69930,AT4G03460,AT5G25250,AT5G36925,AT1G26420,AT5G42380,AT1G16150,AT2G22880,AT1G02930,AT4G11890,AT1G72520,AT5G66020,AT2G43620,AT2G44370,AT4G15975,AT1G35210,AT5G46295,AT1G11925,AT2G39200,AT1G02920,AT4G14370,AT4G35180,AT4G15417,AT2G18690,AT5G11140,AT1G06135,AT5G42830",
>> header=TRUE,stringsAsFactors=FALSE)
>> # split the second column at the commas
>> hitsplit<-strsplit(mmdf$hits,",")
>> # define a function that will fill with NAs
>> NAfill<-function(x,n) return(x[1:n])
>> # get the maximum length of hits
>> maxlen<-max(unlist(lapply(hitsplit,length)))
>> # fill the list with NAs
>> hitsplit<-lapply(hitsplit,NAfill,maxlen)
>> # get all the sorted hits
>> allhits<-sort(unique(unlist(hitsplit)))
>> tmmdf<-as.data.frame(matrix(NA,ncol=length(hitsplit),nrow=length(allhits)))
>> # change the names of the list
>> names(tmmdf)<-mmdf$Regulator
>> # replace all NA values in tmmdf where they appear in hitsplit
>> for(column in 1:length(hitsplit)) {
>>   hitmatches<-match(hitsplit[[column]],allhits)
>>   hitmatches<-hitmatches[!is.na(hitmatches)]
>>   tmmdf[hitmatches,column]<-allhits[hitmatches]
>> }
>>
>> Jim
>>
>> On Fri, May 3, 2019 at 12:43 AM David L Carlson <dcarlson at tamu.edu> wrote:
>>> We still have only the toy version of your data from your first email. The second email used dput() as I suggested, but you truncated the results so it is useless for testing purposes.
>>>
>>> Use the following code after creating DataList (up to mx <- ... ) in my earlier answer:
>>>
>>> n <- sapply(DataList, length)
>>> hits <- unname(unlist(DataList))
>>> Regulator <- unname(unlist(mapply(rep, names(DataList), times=n)))
>>> DataTable <- table(hits, Regulator)
>>>
>>> #            Regulator
>>> # hits        AT1G69490 AT2G55980
>>> #  AT1G05675         1         0
>>> #  AT1G26380         1         0
>>> #  AT2G85403         0         1
>>> #  AT4G31950         1         0
>>> #  AT4G89223         0         1
>>> #  AT5G24110         1         0
>>>
>>> Now the Regulators and the hits will be listed in alphabetical order. The table has 0's for Regulators that do not have a particular hit. If you want NAs:
>>>
>>> DataTable[DataTable==0] <- NA
>>> print(DataTable, na.print="NA")
>>> #            Regulator
>>> # hits        AT1G69490 AT2G55980
>>> #   AT1G05675         1        NA
>>> #   AT1G26380         1        NA
>>> #   AT2G85403        NA         1
>>> #   AT4G31950         1        NA
>>> #   AT4G89223        NA         1
>>> #   AT5G24110         1        NA
>>>
>>> If you need a data frame instead of a table:
>>>
>>> as.data.frame.matrix(DataTable)
>>>
>>> ----------------------------------------
>>> David L Carlson
>>> Department of Anthropology
>>> Texas A&M University
>>> College Station, TX 77843-4352
>>>
>>> -----Original Message-----
>>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Matthew
>>> Sent: Tuesday, April 30, 2019 4:31 PM
>>> To: r-help at r-project.org
>>> Subject: [R] Fwd: Re: transpose and split dataframe
>>>
>>> Thanks for your reply. I was trying to simplify it a little, but must
>>> have got it wrong. Here is the real dataframe, TF2list:
>>>
>>>    str(TF2list)
>>> 'data.frame':    152 obs. of  2 variables:
>>>    $ Regulator: Factor w/ 87 levels "AT1G02065","AT1G13960",..: 17 6 6 54
>>> 54 82 82 82 82 82 ...
>>>    $ hits     : Factor w/ 97 levels
>>> "AT1G05675,AT3G12910,AT1G22810,AT1G14540,AT1G21120,AT1G07160,AT5G22520,AT1G56250,AT2G31345,AT5G22530,AT4G11170,A"|
>>> __truncated__,..: 65 57 90 57 87 57 56 91 31 17 ...
>>>
>>>      And the first few lines resulting from dput(head(TF2list)):
>>>
>>> dput(head(TF2list))
>>> structure(list(Regulator = structure(c(17L, 6L, 6L, 54L, 54L,
>>> 82L), .Label = c("AT1G02065", "AT1G13960", "AT1G18860", "AT1G23380",
>>> "AT1G29280", "AT1G29860", "AT1G30650", "AT1G55600", "AT1G62300",
>>> "AT1G62990", "AT1G64000", "AT1G66550", "AT1G66560", "AT1G66600",
>>> "AT1G68150", "AT1G69310", "AT1G69490", "AT1G69810", "AT1G70510", ...
>>>
>>> This is another way of looking at the first 4 entries (Regulator is
>>> tab-separated from hits):
>>>
>>> Regulator
>>>     hits
>>> 1
>>> AT1G69490
>>>    AT4G31950,AT5G24110,AT1G26380,AT1G05675,AT3G12910,AT5G64905,AT1G22810,AT1G79680,AT3G02840,AT5G25260,AT5G57220,AT2G37430,AT2G26560,AT1G56250,AT3G23230,AT1G16420,AT1G78410,AT4G22030,AT5G05300,AT1G69930,AT4G03460,AT4G11470,AT5G25250,AT5G36925,AT2G30750,AT1G16150,AT1G02930,AT2G19190,AT4G11890,AT1G72520,AT4G31940,AT5G37490,AT5G52760,AT5G66020,AT3G57460,AT4G23220,AT3G15518,AT2G43620,AT2G02010,AT1G35210,AT5G46295,AT1G17147,AT1G11925,AT2G39200,AT1G02920,AT2G40180,AT1G59865,AT4G35180,AT4G15417,AT1G51820,AT1G06135,AT1G36622,AT5G42830
>>> 2
>>> AT1G29860
>>>    AT4G31950,AT5G24110,AT1G05675,AT3G12910,AT5G64905,AT1G22810,AT1G14540,AT1G79680,AT1G07160,AT3G23250,AT5G25260,AT1G53625,AT5G57220,AT2G37430,AT3G54150,AT1G56250,AT3G23230,AT1G16420,AT1G78410,AT4G22030,AT1G69930,AT4G03460,AT4G11470,AT5G25250,AT5G36925,AT4G14450,AT2G30750,AT1G16150,AT1G02930,AT2G19190,AT4G11890,AT1G72520,AT4G31940,AT5G37490,AT4G08555,AT5G66020,AT5G26920,AT3G57460,AT4G23220,AT3G15518,AT2G43620,AT1G35210,AT5G46295,AT1G17147,AT1G11925,AT2G39200,AT1G02920,AT4G35180,AT4G15417,AT1G51820,AT4G40020,AT1G06135
>>>
>>> 3
>>> AT1G2986
>>>    AT5G64905,AT1G21120,AT1G07160,AT5G25260,AT1G53625,AT1G56250,AT2G31345,AT4G11170,AT1G66090,AT1G26410,AT3G55840,AT1G69930,AT4G03460,AT5G25250,AT5G36925,AT1G26420,AT5G42380,AT1G16150,AT2G22880,AT1G02930,AT4G11890,AT1G72520,AT5G66020,AT2G43620,AT2G44370,AT4G15975,AT1G35210,AT5G46295,AT1G11925,AT2G39200,AT1G02920,AT4G14370,AT4G35180,AT4G15417,AT2G18690,AT5G11140,AT1G06135,AT5G42830
>>>
>>>      So, the goal would be to
>>>
>>> first: Transpose the existing dataframe so that the factor Regulator
>>> becomes a column name (column 1 name = AT1G69490, column2 name
>>> AT1G29860, etc.) and the hits associated with each Regulator become
>>> rows. Hits is a comma separated 'list' ( I do not not know if
>>> technically it is an R list.), so it would have to be comma
>>> 'unseparated' with each entry becoming a row (col 1 row 1 = AT4G31950,
>>> col 1 row 2 - AT5G24410, etc); like this :
>>>
>>> AT1G69490
>>> AT4G31950
>>> AT5G24110
>>> AT1G05675
>>> AT5G64905
>>>
>>> ... I did not include all the rows)
>>>
>>> I think it would be best to actually make the first entry a separate
>>> dataframe ( 1 column with name = AT1G69490 and number of rows depending
>>> on the number of hits), then make the second column (column name =
>>> AT1G29860, and number of rows depending on the number of hits) into a
>>> new dataframe and do a full join of of the two dataframes; continue by
>>> making the third column (column name = AT1G2986) into a dataframe and
>>> full join it with the previous; continue for the 152 observations so
>>> that then end result is a dataframe with 152 columns and number of rows
>>> depending on the entry with the greatest number of hits. The full joins
>>> I can do with dplyr, but getting up to that point seems rather difficult.
>>>
>>> This would get me what my ultimate goal would be; each Regulator is a
>>> column name (152 columns) and a given row has either NA or the same hit.
>>>
>>>      This seems very difficult to me, but I appreciate any attempt.
>>>
>>> Matthew
>>>
>>> On 4/30/2019 4:34 PM, David L Carlson wrote:
>>>>           External Email - Use Caution
>>>>
>>>> I think we need more information. Can you give us the structure of the data with str(YourDataFrame). Alternatively you could copy a small piece into your email message by copying and pasting the results of the following code:
>>>>
>>>> dput(head(YourDataFrame))
>>>>
>>>> The data frame you present could not be a data frame since you say "hits" is a factor with a variable number of elements. If each value of "hits" was a single character string, it would only have 2 factor levels not 6 and your efforts to parse the string would make more sense. Transposing to a data frame would only be possible if each column was padded with NAs to make them equal in length. Since your example tries use the name TF2list, it is possible that you do not have a data frame but a list and you have no factor levels, just character vectors.
>>>>
>>>> If you are not familiar with R, it may be helpful to tell us what your overall goal is rather than an intermediate step. Very likely R can easily handle what you want by doing things a different way.
>>>>
>>>> ----------------------------------------
>>>> David L Carlson
>>>> Department of Anthropology
>>>> Texas A&M University
>>>> College Station, TX 77843-4352
>>>>
>>>>
>>>>
>>>> -----Original Message-----
>>>> From: R-help<r-help-bounces at r-project.org>  On Behalf Of Matthew
>>>> Sent: Tuesday, April 30, 2019 2:25 PM
>>>> To: r-help (r-help at r-project.org)<r-help at r-project.org>
>>>> Subject: [R] transpose and split dataframe
>>>>
>>>> I have a data frame that is a lot bigger but for simplicity sake we can
>>>> say it looks like this:
>>>>
>>>> Regulator    hits
>>>> AT1G69490    AT4G31950,AT5G24110,AT1G26380,AT1G05675
>>>> AT2G55980    AT2G85403,AT4G89223
>>>>
>>>>       In other words:
>>>>
>>>> data.frame : 2 obs. of 2 variables
>>>> $Regulator: Factor w/ 2 levels
>>>> $hits         : Factor w/ 6 levels
>>>>
>>>>      I want to transpose it so that Regulator is now the column headings
>>>> and each of the AGI numbers now separated by commas is a row. So,
>>>> AT1G69490 is now the header of the first column and AT4G31950 is row 1
>>>> of column 1, AT5G24110 is row 2 of column 1, etc. AT2G55980 is header of
>>>> column 2 and AT2G85403 is row 1 of column 2, etc.
>>>>
>>>>      I have tried playing around with strsplit(TF2list[2:2]) and
>>>> strsplit(as.character(TF2list[2:2]), but I am getting nowhere.
>>>>
>>>> Matthew
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org  mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guidehttp://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>          [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From |@heemj@n93 @end|ng |rom y@hoo@com  Mon May  6 07:22:27 2019
From: |@heemj@n93 @end|ng |rom y@hoo@com (Faheem Jan)
Date: Mon, 6 May 2019 05:22:27 +0000 (UTC)
Subject: [R] fitting functional autoregresive model
References: <737085316.501133.1557120147367.ref@mail.yahoo.com>
Message-ID: <737085316.501133.1557120147367@mail.yahoo.com>

Hi,?i trying to? extend the functional autoregressive model one FAR(1) to fit the functional autoregressive model of order two FAR(2). the coding i do for far(1) is?library(fda)library(far)# CREATE DUMMY VARIABLESfactor2dummy=function(x){? n=length(x)? tab=as.factor(names(table(x)))? p=length(tab)? xdummy=matrix(0,n,p)? for(i in 1:p)? {? ? xdummy[x==tab[i],i]=1? }? colnames(xdummy)=tab? return(xdummy)}
# READ DATA
demnd=read.csv("c:/Users/Khan/Desktop/dem99141.csv",header=TRUE)xdata <- as.matrix(demnd[-1,-1], ncol = 25, nrow =1826, byrow= TRUE)class(xdata)date=strptime(as.character(xdata[,1]),"%Y-%m-%d")weekday=weekdays(date)week=format(date,"%U")xdata=xdata[,-1]xdata# DAILY AVERAGExmean=apply(xdata,1,mean)xmean
# SEASONAL ADJUSTMENT#seasadj=function(x) decompose(ts(x,frequency=7))$rand#xdata=apply(xdata,2,seasadj)#xdata=xdata[!is.na(xdata[,1]),]nrall=nrow(xdata)#wd=factor2dummy(weekday)#wnr=factor2dummy(week)#e=lm(xmean~wd+wnr-1)$residuals#tsdiag(arima(e,c(7,0,0)))#seasadj=function(x) lm(x~wd+wnr-1)$residuals#xdata=apply(xdata,2,seasadj)
# HOLD-OUT-PERIODnout=100nin=nrall-noutxin=xdata[1:nin,]
nr=nrow(xin)nc=ncol(xin)n=nr*ncy=matrix(t(xin),n,1)xfd=as.fdata(y,col=1,p=23,name="Cons") #p=23 is the multiple of length=39698 of data
# ESTIMATE FAR(1) MODELk1=far.cv(xfd,y="Cons",kn=20,ncv=1000)$minL2[1]far1=far(xfd,kn=k1)far1# ESTIMATE AR(1)-MODELSp=14f=function(x) ar(x,aic=FALSE,order.max=p)ar.models=apply(xin,2,f)
# FORECASTerrorsfar=matrix(0,nout,nc)errorsar=matrix(0,nout,nc)errorsnaive=matrix(0,nout,nc)predar=matrix(0,1,nc)prednaive=matrix(0,1,nc)for(i in 1:nout){? for(j in 1:length(ar.models))? {? ? predar[1,j]=predict(ar.models[[j]],newdata=xdata[(nr+i-p):(nr+i-1),j])$pred? }? xnew=as.fdata(t(xdata[nr+i-1,]),col=1,p=23,name="Cons")? pred=predict(far1,newdata=xnew)? prednaive=xdata[nr+i-1,]? obs=xdata[nr+i,]? errorsnaive[i,]=t(obs-prednaive)? errorsar[i,]=t(obs-predar)? errorsfar[i,]=t(obs-pred$Cons)}msefar=apply(errorsfar^2,2,mean)msefarmsear=apply(errorsar^2,2,mean)msenaive=apply(errorsnaive^2,2,mean)mean(msear)mean(msenaive)mean(msefar

i use the consumption data ...
	[[alternative HTML version deleted]]


From r|cc@rdo@porrec@ @end|ng |rom m|r@|-@o|ut|on@@com  Mon May  6 11:42:58 2019
From: r|cc@rdo@porrec@ @end|ng |rom m|r@|-@o|ut|on@@com (Riccardo Porreca)
Date: Mon, 6 May 2019 11:42:58 +0200
Subject: [R] [R-pkgs] rTRNG: Advanced and Parallel Random Number Generation
 via TRNG
Message-ID: <CAHBAGFVJrDQ1yrxjUUbC8UNe_Dh+rFFuwvcH7Q7+Ht7C0Z7dCw@mail.gmail.com>

We are happy to announce the first CRAN release of rTRNG version 4.20-1.

rTRNG is a package for advanced parallel Random Number Generation in
R. It relies on TRNG (Tina?s Random Number Generator,
<https://numbercrunch.de/trng/>), a state-of-the-art C++ pseudo-random
number generator library for sequential and parallel Monte Carlo
simulations. In particular, parallel random number generators provided
by TRNG can be manipulated by jump and split operations. These allow
to jump ahead by an arbitrary number of steps and to split a sequence
into any desired sub-sequence(s), thus enabling techniques suitable to
parallel algorithms, such as block-splitting and leapfrogging.

The package provides the R user with access to the functionality of
the underlying TRNG C++ library. It embeds TRNG sources and headers
and makes them available to other projects combining R with C++.
Beyond this, rTRNG exposes the creation, manipulation and use of
pseudo-random streams to R, via Rcpp and RcppParallel.

rTRNG on CRAN
<https://cran.r-project.org/package=rTRNG>
Public repository
<https://github.com/miraisolutions/rTRNG#readme>

More on the history behind rTRNG and its way to CRAN at
<https://mirai-solutions.ch/news/2019/05/04/rTRNG-on-CRAN/>


-- 
Riccardo Porreca
Senior Solutions Consultant
Mirai Solutions GmbH
riccardo.porreca at mirai-solutions.com

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From n|co|@@rugg|ero@unt @end|ng |rom gm@||@com  Tue May  7 02:40:57 2019
From: n|co|@@rugg|ero@unt @end|ng |rom gm@||@com (Nicola Ruggiero)
Date: Mon, 6 May 2019 17:40:57 -0700
Subject: [R] R package that translates zip codes into longitude and
 longitude
In-Reply-To: <CA+8X3fUU8VBCN3LDJeAMNrb5=vSMEA5cAT8yZkGYKLyzcMsP=Q@mail.gmail.com>
References: <CAEneZ8njZAqH0n0_ZaeUMdux6H948rJ-cO9DDVXT4gKBwvootQ@mail.gmail.com>
 <CA+8X3fUU8VBCN3LDJeAMNrb5=vSMEA5cAT8yZkGYKLyzcMsP=Q@mail.gmail.com>
Message-ID: <CAEneZ8naETM4faB5FZipmpdDbncogx1QsxMpco1kh1nC8Q+NaQ@mail.gmail.com>

I found this package "zipcode", by Jeffry Breen
(http://www.jeffreybreen.com/code/).

Does anyone have experience working with this one? And do you think it
would help with my data, such as I exemplified it in my previous post
to the listserv?

His blog looks good
(https://datamatters.blog/2011/01/05/cran-zipcode/). And this, too,
was written about it
(https://blog.exploratory.io/geocoding-us-zip-code-data-with-dplyr-and-zipcode-package-7f539c3702b0).

Warmly, Nicola


On Mon, May 6, 2019 at 3:32 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi Nicola,
> There seems to be a big database of this here:
>
> http://download.geonames.org/export/dump/
>
> Jim
>
> On Mon, May 6, 2019 at 2:47 PM Nicola Ruggiero
> <nicola.ruggiero.unt at gmail.com> wrote:
> >
> > Hi there,
> >
> > I'm looking for a R package that would enable me to add a column to a
> > data.frame that is itself a translation of zip codes into longitude
> > and latitude. The column in the data.frame looks like this [begin
> > quote]:
> >
> > waltham, Massachusetts 02451
> > Columbia, SC 29209
> >
> > Wheat Ridge , Colorado 80033
> > Charlottesville, Virginia 22902
> > Fairbanks, AK 99709
> > Montpelier, VT 05602
> > Dobbs Ferry, New York 10522
> >
> > Henderson , Kentucky 42420
> >
> > [end quote:] The spaces represent absences in the column. Regardless,
> > I'm told that there is such an R package. I'm just having trouble
> > locating it on Google.
> >
> > Warmly,
> > Nicola
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Tue May  7 03:42:58 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 6 May 2019 18:42:58 -0700
Subject: [R] R package that translates zip codes into longitude and
 longitude
In-Reply-To: <CAEneZ8naETM4faB5FZipmpdDbncogx1QsxMpco1kh1nC8Q+NaQ@mail.gmail.com>
References: <CAEneZ8njZAqH0n0_ZaeUMdux6H948rJ-cO9DDVXT4gKBwvootQ@mail.gmail.com>
 <CA+8X3fUU8VBCN3LDJeAMNrb5=vSMEA5cAT8yZkGYKLyzcMsP=Q@mail.gmail.com>
 <CAEneZ8naETM4faB5FZipmpdDbncogx1QsxMpco1kh1nC8Q+NaQ@mail.gmail.com>
Message-ID: <CAGxFJbQ6-7iv9VR5mZxdR6uk+6B=f=cT=4ymejm8DUkgnViisw@mail.gmail.com>

For these sorts of questions, it might be more useful if you address these
sorts of queries to the package author or maintainer directly.

--Bert

On Mon, May 6, 2019 at 5:41 PM Nicola Ruggiero <
nicola.ruggiero.unt at gmail.com> wrote:

> I found this package "zipcode", by Jeffry Breen
> (http://www.jeffreybreen.com/code/).
>
> Does anyone have experience working with this one? And do you think it
> would help with my data, such as I exemplified it in my previous post
> to the listserv?
>
> His blog looks good
> (https://datamatters.blog/2011/01/05/cran-zipcode/). And this, too,
> was written about it
> (
> https://blog.exploratory.io/geocoding-us-zip-code-data-with-dplyr-and-zipcode-package-7f539c3702b0
> ).
>
> Warmly, Nicola
>
>
> On Mon, May 6, 2019 at 3:32 AM Jim Lemon <drjimlemon at gmail.com> wrote:
> >
> > Hi Nicola,
> > There seems to be a big database of this here:
> >
> > http://download.geonames.org/export/dump/
> >
> > Jim
> >
> > On Mon, May 6, 2019 at 2:47 PM Nicola Ruggiero
> > <nicola.ruggiero.unt at gmail.com> wrote:
> > >
> > > Hi there,
> > >
> > > I'm looking for a R package that would enable me to add a column to a
> > > data.frame that is itself a translation of zip codes into longitude
> > > and latitude. The column in the data.frame looks like this [begin
> > > quote]:
> > >
> > > waltham, Massachusetts 02451
> > > Columbia, SC 29209
> > >
> > > Wheat Ridge , Colorado 80033
> > > Charlottesville, Virginia 22902
> > > Fairbanks, AK 99709
> > > Montpelier, VT 05602
> > > Dobbs Ferry, New York 10522
> > >
> > > Henderson , Kentucky 42420
> > >
> > > [end quote:] The spaces represent absences in the column. Regardless,
> > > I'm told that there is such an R package. I'm just having trouble
> > > locating it on Google.
> > >
> > > Warmly,
> > > Nicola
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Tue May  7 10:32:43 2019
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Tue, 7 May 2019 10:32:43 +0200
Subject: [R] how to choose cost in SVM analysis with kernlab
Message-ID: <CAMk+s2R8hp6=dBeDeHBpGUkDOPkOhAkpRdzxZEWYaiSnjZBiWw@mail.gmail.com>

Dear all,
I have set a model for SVM analysis using laplacedot with the package
kernlab. I checked the classification error with a k-fold approach,
that is I analyzed 1/10 of the data ten times and averaged the error
(FalseNeg + FalsePos) / TOT. I tested different levels of cost C and
the results are:

C         error
0.01    0.106566
0.10    0.070798
0.50    0.000985
1.00    0.000556
2.50    0.000198
5.00    0.000079
7.50    0.000040
8.00    0.000040
8.50    0.000016
9.00    0.000008
9.50    0.000000
10.00  0.000000
10.50  0.000000
100.00 0.000000

Given that the purpose of the optimization is to minimize the error, a
C>=9 is therefore what I am looking for. But, if the model is too
stringent, then I will have problems with the future sets.
So what level should I set? My feeling is that C=1 is enough.
Is there a method within kernlab to maximize C (and gamma perhaps)
automatically?


-- 
Best regards,
Luigi


From er|cjberger @end|ng |rom gm@||@com  Tue May  7 16:35:39 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Tue, 7 May 2019 17:35:39 +0300
Subject: [R] Question concerning Loading large Data Sets
In-Reply-To: <16DEE3E8-8326-4204-A9A5-5B010ECD36FC@comcast.net>
References: <CAPQaxLNsjQY9WR4Wx=zdrzDuHK4awa1AMKqz7K9HzVUrwZGosQ@mail.gmail.com>
 <CAPQaxLPNq9+RotbOQbcvOGczQT8q0QEGtwwcjra=n6Mif-_ZyA@mail.gmail.com>
 <16DEE3E8-8326-4204-A9A5-5B010ECD36FC@comcast.net>
Message-ID: <CAGgJW75C--_z1c=iS6zfFrMgXfKsDP6r7yubhmDgBnK8od2FZQ@mail.gmail.com>

I don't think I understand your question.
What if you enter

class(anno)

Does that give anything?


On Mon, May 6, 2019 at 2:29 AM David Winsemius <dwinsemius at comcast.net>
wrote:

> I wouldn?t have expected an message, but it does raise the question: why
> are you making two different copies of the same text file if you are
> concerned about size issues?
>
> ?
> David
>
> Sent from my iPhone
>
> > On May 5, 2019, at 5:14 PM, Spencer Brackett <
> spbrackett20 at saintjosephhs.com> wrote:
> >
> > Also,
> >
> > In case there is any confusion... "~/Vakul's GBM code" was set up in the
> > working directory (via setwd) as the folder in which the file
> 'mapper.txt'
> > is contained... thereby making the "~/Vakul's GBM code/mapper.txt" as
> > shown.
> >
> > Should I perhaps reset my working directory to the file I'm trying to
> > access, exclusively?
> >
> > On Sun, May 5, 2019 at 7:09 PM Spencer Brackett <
> > spbrackett20 at saintjosephhs.com> wrote:
> >
> >> Good evening,
> >>
> >> I am attempting to reproduce the following lines of code...
> >>
> >> library(data.table)
> >> anno = as.data.frame(fread(file = "file name", sep ="\t", header = T))
> >>
> >>
> >> The following is my attempt...
> >>
> >>> library(data.table)
> >>> mapper <- read.delim("~/Vakul's GBM code/mapper.txt")
> >>>  View(mapper)
> >> ## fread() was used as datset is quite large ##
> >>> GBM_meth <- fread("~/Vakul's GBM code/mapper.txt")
> >>> anno = as.data.frame(fread(file = "~/Vakul's GBM code/mapper.txt", sep
> >> ="\t", header = T))
> >>
> >> Via View(mapper), I can view the dataset contained within the
> "mapper.txt"
> >> file, but the >anno = as.data.frame portion does not trigger any sort of
> >> 'response' on R and simply brings my cursor down to the next line, ready
> >> for further implementation. Should I be getting some sort of output
> from R,
> >> and if so what should I expect?
> >>
> >> I apologize in advance for any errors in the summary of the above script
> >>
> >> Best,
> >>
> >> Spencer Brackett
> >>
> >
> >    [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Wed May  8 10:25:21 2019
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Wed, 8 May 2019 10:25:21 +0200
Subject: [R] Extract hyperplane from kernlab's Support Vector Machine model
Message-ID: <CAMk+s2SNwToRE0233fCucVamLU6VMdDkLdDoUmK_e9YC=_+hSA@mail.gmail.com>

Dear all,
I would like to extract an hyperplance froma SVM model generated with
the kernlab package. The model I have made is non-linear and I am
looking to plot the line that separates the two groups in order to
draw custom plots with more control.
I understand that I should use a contour plot but I do not understand
how to implement the function.
I will show the process I have done with the example reported in here
(https://www.datacamp.com/community/tutorials/support-vector-machines-r);
I had to re-arrange the data to pass from e1071 to kernlab.
Essentially it all works until I reach the contour part. How can I add
a line that represents the boundaries between the red and blue dots?
In the following working example, this boundary (the hyperplane) is
represented roughly by the green line; in the real case, it should be
a convoluted curve.
Also, I reckon countour should have an add=TRUE option in order to add
it to an existing plot.
Thank you

>>>
set.seed(10111)
library(kernlab)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
dat = data.frame(x, y = as.factor(y))
MAXx1 = max(dat$X1)
MAXx2 = max(dat$X2)
MINx1 = min(dat$X1)
MINx2 = min(dat$X2)
svmfit = ksvm(y ~ X1+X2, data = dat,
           type = "C-svc",
           kernel = "vanilladot",
           kpar = "automatic",
           C = 10,
           prob.model = TRUE)
print(svmfit)
plot(svmfit, data = dat)

# make own plot
make.grid = function(x, n = 75) {
  grange = apply(x, 2, range)
  x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
  x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
  expand.grid(X1 = x1, X2 = x2)
}
M = matrix(c(dat$X1, dat$X2), ncol = 2, byrow=FALSE)
xgrid = make.grid(M)
xgrid[1:10,]
ygrid = predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2,
     xlim=c(MINx1, MAXx1), ylim = c(MINx2, MAXx2))
points(dat[dat[, "y"] == -1,], col = "red", pch = 19)
points(dat[dat[, "y"] == 1,], col = "blue", pch = 18, cex = 1.5)
abline(a = 0.9, b = -0.65, col = "green", lwd = 2)

# here is where I am lost
contour(dat[dat[, "y"] == -1,]$X1, dat[dat[, "y"] == 1,]$X2,
        dat[dat[, "y"] == -1,]$y)
# Error in contour.default(dat[dat[, "y"] == -1, ]$X1, dat[dat[, "y"] ==  :
#    increasing 'x' and 'y' values expected
M_ord = M[order(M[,1], M[,2],decreasing=TRUE),]
contour(M[, 1], M[, 2],  dat[dat[, "y"] == -1,]$y)
# Error in contour.default(M[, 1], M[, 2], dat[dat[, "y"] == -1, ]$y) :
#  increasing 'x' and 'y' values expected
M_ord = M[order(M[,1], M[,2],decreasing=FALSE),]
contour(M[, 1], M[, 2],  dat[dat[, "y"] == -1,]$y)
# Error in contour.default(M[, 1], M[, 2], dat[dat[, "y"] == -1, ]$y) :
#  increasing 'x' and 'y' values expected
-- 
Best regards,
Luigi


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Wed May  8 11:26:25 2019
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Wed, 8 May 2019 11:26:25 +0200
Subject: [R] approx with NAs
In-Reply-To: <55393658-0A87-4708-99AF-C612F896AE01@quantitativebrokers.com>
References: <55393658-0A87-4708-99AF-C612F896AE01@quantitativebrokers.com>
Message-ID: <23762.41153.537637.978717@stat.math.ethz.ch>

Dear Robert,

this is really not asking for help about R  but rather wishing
for new features of a (very long) existing R function.
Hence this is a topic for the 'R-devel'  mailing list
(https://stat.ethz.ch/mailman/listinfo/R-devel )
rather than 'R-help'; see also  https://www.r-project.org/mail.html
on what the different lists are aimed at.

==> I will do a long reply to this post but divert it to R-devel
 (and will CC you at least in the first reply).

--> Further follow up to this: Please on 'R-devel'

>>>>> Robert Almgren 
>>>>>     on Fri, 3 May 2019 15:45:44 -0400 writes:

    > There is something I do not think is right in the approx() function in base R, with method="constant" and in the presence of NA values. I have 3.6.0, but the behavior seems to be the same in earlier versions.
    > My suggested fix is to add an "na.rm" argument to approx(), as in mean(). If this argument is FALSE, then NA values should be propagated into the output rather than being removed.

    > Details:

    > The documentation says 

    > "f: for method = "constant" a number between 0 and 1 inclusive, indicating a compromise between left- and right-continuous step functions. If y0 and y1 are the values to the left and right of the point then the value is y0 if f == 0, y1 if f == 1, and y0*(1-f)+y1*f for intermediate values. In this way the result is right-continuous for f == 0 and left-continuous for f == 1, even for non-finite y values."

    > This suggests to me that if the left value y0 is NA, and if f=0 (the default), then the interpolated value should be NA. (Regardless of the right value y1, see bug 15655 fixed in 2014.)

    > The documentation further says, below under "Details", that

    > "The inputs can contain missing values which are deleted."

    > The question is what is the appropriate behavior if one of the input values y is NA. Currently, approx() seems to interpret NA values as faulty data points, which should be deleted and the previous values carried forward (example below).

    > But in many applications, especially with "constant" interpolation, an NA value is intended to mean that we really do not know the value in the next interval, or explicitly that there is no value. Therefore the NA should not be removed, but should be propagated forward into the output within the corresponding interval.

    > The situation is similar with functions like mean(). The presence of an NA value may mean either (a) we want to compute the mean without that value (na.rm=TRUE), or (b) we really are missing important information, we cannot determine the mean, and we should return NA (na.rm=FALSE).

    > Therefore, I propose that approx() also be given an na.rm argument, indicating whether we wish to delete NA values, or treat them as actual values on the corresponding interval. That option makes even more sense for approx() than for mean(), since the NA values apply only on small regions of the data range.

    > --Robert Almgren

    > Example:

    > : R --vanilla

    > R version 3.6.0 (2019-04-26) -- "Planting of a Tree"
    > Copyright (C) 2019 The R Foundation for Statistical Computing
    > Platform: x86_64-apple-darwin15.6.0 (64-bit)
    > ...

    >> t1 <- 1:5
    >> x1 <- c( 1, as.numeric(NA), 3, as.numeric(NA), 5 )
    >> print(data.frame(t1,x1))
    > t1 x1
    > 1  1  1
    > 2  2 NA   <-- we do not know the value between t=2 and t=3
    > 3  3  3
    > 4  4 NA   <-- we do not know the value between t=4 and t=5
    > 5  5  5
    >> X <- approx( t1, x1, (1:4) + 0.5, method='constant', rule=c(1,2) )
    >> print(data.frame(X))
    > x y
    > 1 1.5 1
    > 2 2.5 1   <---- I believe that these two values should be NA
    > 3 3.5 3
    > 4 4.5 3   <---- I believe that these two values should be NA

    > --


From yue||7 @end|ng |rom 126@com  Wed May  8 15:05:29 2019
From: yue||7 @end|ng |rom 126@com (yueli)
Date: Wed, 8 May 2019 21:05:29 +0800 (CST)
Subject: [R] install R
Message-ID: <20a7a9e5.86bb.16a978c81af.Coremail.yueli7@126.com>

Hello,


I am trying to install R.


Thanks in advance for any help!


Yue




li at li-HP-Pavilion-Desktop-590-p0xxx:~/R-3.6.0$ ./configure
checking build system type... x86_64-pc-linux-gnu
checking host system type... x86_64-pc-linux-gnu
loading site script './config.site'
loading build-specific script './config.site'
checking for pwd... /bin/pwd
checking whether builddir is srcdir... yes
checking whether ln -s works... yes
checking for ar... ar
checking for a BSD-compatible install... /usr/bin/install -c
checking for sed... /bin/sed
checking for which... /usr/bin/which
checking for less... /usr/bin/less
checking for gtar... no
checking for gnutar... no
checking for tar... /bin/tar
checking for tex... no
checking for pdftex... no
configure: WARNING: you cannot build PDF versions of the R manuals
checking for pdflatex... no
configure: WARNING: you cannot build PDF versions of vignettes and help pages
checking for makeindex... no
checking for texi2any... no
configure: WARNING: you cannot build info or HTML versions of the R manuals
checking for texi2dvi... no
checking for kpsewhich... no
checking for latex inconsolata package... checking for unzip... /usr/bin/unzip
checking for zip... /usr/bin/zip
checking for gzip... /bin/gzip
checking for bzip2... /bin/bzip2
checking for firefox... /usr/bin/firefox
using default browser ... /usr/bin/firefox
checking for acroread... no
checking for acroread4... no
checking for xdg-open... /usr/bin/xdg-open
checking for working aclocal... missing
checking for working autoconf... missing
checking for working autoheader... missing
checking for bison... no
checking for byacc... no
checking for yacc... no
checking for notangle... false
checking for realpath... /usr/bin/realpath
checking for pkg-config... no
checking for gcc... gcc
checking whether the C compiler works... yes
checking for C compiler default output file name... a.out
checking for suffix of executables...
checking whether we are cross compiling... no
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ISO C89... none needed
checking how to run the C preprocessor... gcc -E
checking for grep that handles long lines and -e... /bin/grep
checking for egrep... /bin/grep -E
checking whether gcc needs -traditional... no
checking for ANSI C header files... yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for unistd.h... yes
checking minix/config.h usability... no
checking minix/config.h presence... no
checking for minix/config.h... no
checking whether it is safe to define __EXTENSIONS__... yes
checking how to run the C preprocessor... gcc -E
looking for a modern Fortran compiler
checking for gfortran... no
checking for g95... no
checking for xlf95... no
checking for f95... no
checking for fort... no
checking for ifort... no
checking for ifc... no
checking for efc... no
checking for pgfortran... no
checking for pgf95... no
checking for lf95... no
checking for ftn... no
checking for nagfor... no
checking for xlf90... no
checking for f90... no
checking for pgf90... no
checking for pghpf... no
checking for epcf90... no
checking for g77... no
checking for xlf... no
checking for f77... no
checking for frt... no
checking for pgf77... no
checking for cf77... no
checking for fort77... no
checking for fl32... no
checking for af77... no
checking whether we are using the GNU Fortran compiler... no
checking whether  accepts -g... no
configure: error: No Fortran compiler found


	[[alternative HTML version deleted]]


From er|cjberger @end|ng |rom gm@||@com  Wed May  8 15:54:01 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Wed, 8 May 2019 16:54:01 +0300
Subject: [R] install R
In-Reply-To: <20a7a9e5.86bb.16a978c81af.Coremail.yueli7@126.com>
References: <20a7a9e5.86bb.16a978c81af.Coremail.yueli7@126.com>
Message-ID: <CAGgJW76bRVxnw_bKec4h7euPXQJhZOUDPje6qJU4KbO=CkxaJg@mail.gmail.com>

Do a web search such as "ubuntu install fortran"  (or replace ubuntu by
your distribution if it's different, e.g. debian or centos or ..)
e.g. the above search gives a result of the following link which provides
the appropriate commands

https://fossnaija.com/how-to-install-fortran-compiler-in-linux/

Good luck

On Wed, May 8, 2019 at 4:37 PM yueli <yueli7 at 126.com> wrote:

> Hello,
>
>
> I am trying to install R.
>
>
> Thanks in advance for any help!
>
>
> Yue
>
>
>
>
> li at li-HP-Pavilion-Desktop-590-p0xxx:~/R-3.6.0$ ./configure
> checking build system type... x86_64-pc-linux-gnu
> checking host system type... x86_64-pc-linux-gnu
> loading site script './config.site'
> loading build-specific script './config.site'
> checking for pwd... /bin/pwd
> checking whether builddir is srcdir... yes
> checking whether ln -s works... yes
> checking for ar... ar
> checking for a BSD-compatible install... /usr/bin/install -c
> checking for sed... /bin/sed
> checking for which... /usr/bin/which
> checking for less... /usr/bin/less
> checking for gtar... no
> checking for gnutar... no
> checking for tar... /bin/tar
> checking for tex... no
> checking for pdftex... no
> configure: WARNING: you cannot build PDF versions of the R manuals
> checking for pdflatex... no
> configure: WARNING: you cannot build PDF versions of vignettes and help
> pages
> checking for makeindex... no
> checking for texi2any... no
> configure: WARNING: you cannot build info or HTML versions of the R manuals
> checking for texi2dvi... no
> checking for kpsewhich... no
> checking for latex inconsolata package... checking for unzip...
> /usr/bin/unzip
> checking for zip... /usr/bin/zip
> checking for gzip... /bin/gzip
> checking for bzip2... /bin/bzip2
> checking for firefox... /usr/bin/firefox
> using default browser ... /usr/bin/firefox
> checking for acroread... no
> checking for acroread4... no
> checking for xdg-open... /usr/bin/xdg-open
> checking for working aclocal... missing
> checking for working autoconf... missing
> checking for working autoheader... missing
> checking for bison... no
> checking for byacc... no
> checking for yacc... no
> checking for notangle... false
> checking for realpath... /usr/bin/realpath
> checking for pkg-config... no
> checking for gcc... gcc
> checking whether the C compiler works... yes
> checking for C compiler default output file name... a.out
> checking for suffix of executables...
> checking whether we are cross compiling... no
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ISO C89... none needed
> checking how to run the C preprocessor... gcc -E
> checking for grep that handles long lines and -e... /bin/grep
> checking for egrep... /bin/grep -E
> checking whether gcc needs -traditional... no
> checking for ANSI C header files... yes
> checking for sys/types.h... yes
> checking for sys/stat.h... yes
> checking for stdlib.h... yes
> checking for string.h... yes
> checking for memory.h... yes
> checking for strings.h... yes
> checking for inttypes.h... yes
> checking for stdint.h... yes
> checking for unistd.h... yes
> checking minix/config.h usability... no
> checking minix/config.h presence... no
> checking for minix/config.h... no
> checking whether it is safe to define __EXTENSIONS__... yes
> checking how to run the C preprocessor... gcc -E
> looking for a modern Fortran compiler
> checking for gfortran... no
> checking for g95... no
> checking for xlf95... no
> checking for f95... no
> checking for fort... no
> checking for ifort... no
> checking for ifc... no
> checking for efc... no
> checking for pgfortran... no
> checking for pgf95... no
> checking for lf95... no
> checking for ftn... no
> checking for nagfor... no
> checking for xlf90... no
> checking for f90... no
> checking for pgf90... no
> checking for pghpf... no
> checking for epcf90... no
> checking for g77... no
> checking for xlf... no
> checking for f77... no
> checking for frt... no
> checking for pgf77... no
> checking for cf77... no
> checking for fort77... no
> checking for fl32... no
> checking for af77... no
> checking whether we are using the GNU Fortran compiler... no
> checking whether  accepts -g... no
> configure: error: No Fortran compiler found
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @@r@h@go@|ee @end|ng |rom gm@||@com  Wed May  8 15:54:23 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Wed, 8 May 2019 09:54:23 -0400
Subject: [R] install R
In-Reply-To: <20a7a9e5.86bb.16a978c81af.Coremail.yueli7@126.com>
References: <20a7a9e5.86bb.16a978c81af.Coremail.yueli7@126.com>
Message-ID: <CAM_vjum8BcsbY2KSzB68dR=BhS=4jbwYavkfQKNft6qJORvMMg@mail.gmail.com>

Hi,

If you don't want to install the binary for your version of linux,
then you must install the prerequisites first.

Notably, you have to have a compiler if you want to compile R from source!

Here's a list: https://cran.r-project.org/doc/manuals/r-release/R-admin.html#Essential-and-useful-other-programs-under-a-Unix_002dalike

But your standard package installation method (like apt or dnf)
probably can install a pre-built binary for you. If you aren't
familiar with compiling software, that's the way to go.

Sarah

On Wed, May 8, 2019 at 9:37 AM yueli <yueli7 at 126.com> wrote:
>
> Hello,
>
>
> I am trying to install R.
>
>
> Thanks in advance for any help!
>
>
> Yue
>
>
>
>
> li at li-HP-Pavilion-Desktop-590-p0xxx:~/R-3.6.0$ ./configure
> checking build system type... x86_64-pc-linux-gnu
> checking host system type... x86_64-pc-linux-gnu
> loading site script './config.site'
> loading build-specific script './config.site'
> checking for pwd... /bin/pwd
> checking whether builddir is srcdir... yes
> checking whether ln -s works... yes
> checking for ar... ar
> checking for a BSD-compatible install... /usr/bin/install -c
> checking for sed... /bin/sed
> checking for which... /usr/bin/which
> checking for less... /usr/bin/less
> checking for gtar... no
> checking for gnutar... no
> checking for tar... /bin/tar
> checking for tex... no
> checking for pdftex... no
> configure: WARNING: you cannot build PDF versions of the R manuals
> checking for pdflatex... no
> configure: WARNING: you cannot build PDF versions of vignettes and help pages
> checking for makeindex... no
> checking for texi2any... no
> configure: WARNING: you cannot build info or HTML versions of the R manuals
> checking for texi2dvi... no
> checking for kpsewhich... no
> checking for latex inconsolata package... checking for unzip... /usr/bin/unzip
> checking for zip... /usr/bin/zip
> checking for gzip... /bin/gzip
> checking for bzip2... /bin/bzip2
> checking for firefox... /usr/bin/firefox
> using default browser ... /usr/bin/firefox
> checking for acroread... no
> checking for acroread4... no
> checking for xdg-open... /usr/bin/xdg-open
> checking for working aclocal... missing
> checking for working autoconf... missing
> checking for working autoheader... missing
> checking for bison... no
> checking for byacc... no
> checking for yacc... no
> checking for notangle... false
> checking for realpath... /usr/bin/realpath
> checking for pkg-config... no
> checking for gcc... gcc
> checking whether the C compiler works... yes
> checking for C compiler default output file name... a.out
> checking for suffix of executables...
> checking whether we are cross compiling... no
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ISO C89... none needed
> checking how to run the C preprocessor... gcc -E
> checking for grep that handles long lines and -e... /bin/grep
> checking for egrep... /bin/grep -E
> checking whether gcc needs -traditional... no
> checking for ANSI C header files... yes
> checking for sys/types.h... yes
> checking for sys/stat.h... yes
> checking for stdlib.h... yes
> checking for string.h... yes
> checking for memory.h... yes
> checking for strings.h... yes
> checking for inttypes.h... yes
> checking for stdint.h... yes
> checking for unistd.h... yes
> checking minix/config.h usability... no
> checking minix/config.h presence... no
> checking for minix/config.h... no
> checking whether it is safe to define __EXTENSIONS__... yes
> checking how to run the C preprocessor... gcc -E
> looking for a modern Fortran compiler
> checking for gfortran... no
> checking for g95... no
> checking for xlf95... no
> checking for f95... no
> checking for fort... no
> checking for ifort... no
> checking for ifc... no
> checking for efc... no
> checking for pgfortran... no
> checking for pgf95... no
> checking for lf95... no
> checking for ftn... no
> checking for nagfor... no
> checking for xlf90... no
> checking for f90... no
> checking for pgf90... no
> checking for pghpf... no
> checking for epcf90... no
> checking for g77... no
> checking for xlf... no
> checking for f77... no
> checking for frt... no
> checking for pgf77... no
> checking for cf77... no
> checking for fort77... no
> checking for fl32... no
> checking for af77... no
> checking whether we are using the GNU Fortran compiler... no
> checking whether  accepts -g... no
> configure: error: No Fortran compiler found
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From reichm@@j m@iii@g oii sbcgiob@i@@et  Wed May  8 17:04:30 2019
From: reichm@@j m@iii@g oii sbcgiob@i@@et (reichm@@j m@iii@g oii sbcgiob@i@@et)
Date: Wed, 8 May 2019 10:04:30 -0500
Subject: [R] Date Time Conversion
Message-ID: <000001d505af$5727a7d0$0576f770$@sbcglobal.net>

r-Help Community

 

I need to convert a date-time field (column)  but I'm losing the time  when
I convert using  ..

 

tsData <- myData[,10, drop=FALSE]

tsData$date_time <- as.Date(tsData$date_time, format="%m/%d/%y %H:%M")

head(tsData)

 


 

 

date_time

<date>

	

1

2013-06-20

	

2

2013-06-20

	

3

2013-06-20

	

4

2013-06-20

	

5

2013-05-30

	

6

2013-06-20

	

 

R is doing what I'm asing it to do so I'm obviously using the wrong command.
How do I convert, retaining the time 

 

Sincerely

 

Jeffery (Jeff) Reichman


	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed May  8 17:30:15 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 8 May 2019 16:30:15 +0100
Subject: [R] Date Time Conversion
In-Reply-To: <000001d505af$5727a7d0$0576f770$@sbcglobal.net>
References: <000001d505af$5727a7d0$0576f770$@sbcglobal.net>
Message-ID: <f52677c9-2d74-2ba7-09ca-460c412ce46d@sapo.pt>

Hello,

as.Date() outputs an object of class "Date", you want an object of class 
c("POSIXt", "POSIXct"). Use as.POSIXct().

Hope this helps,

Rui Barradas

?s 16:04 de 08/05/19, reichmanj at sbcglobal.net escreveu:
> r-Help Community
> 
>   
> 
> I need to convert a date-time field (column)  but I'm losing the time  when
> I convert using  ..
> 
>   
> 
> tsData <- myData[,10, drop=FALSE]
> 
> tsData$date_time <- as.Date(tsData$date_time, format="%m/%d/%y %H:%M")
> 
> head(tsData)
> 
>   
> 
> 
>   
> 
>   
> 
> date_time
> 
> <date>
> 
> 	
> 
> 1
> 
> 2013-06-20
> 
> 	
> 
> 2
> 
> 2013-06-20
> 
> 	
> 
> 3
> 
> 2013-06-20
> 
> 	
> 
> 4
> 
> 2013-06-20
> 
> 	
> 
> 5
> 
> 2013-05-30
> 
> 	
> 
> 6
> 
> 2013-06-20
> 
> 	
> 
>   
> 
> R is doing what I'm asing it to do so I'm obviously using the wrong command.
> How do I convert, retaining the time
> 
>   
> 
> Sincerely
> 
>   
> 
> Jeffery (Jeff) Reichman
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From reichm@@j m@iii@g oii sbcgiob@i@@et  Wed May  8 17:54:52 2019
From: reichm@@j m@iii@g oii sbcgiob@i@@et (reichm@@j m@iii@g oii sbcgiob@i@@et)
Date: Wed, 8 May 2019 10:54:52 -0500
Subject: [R] Date Time Conversion
Message-ID: <000601d505b6$60862b10$21928130$@sbcglobal.net>

r-Help Community

 

Never mine figured it out just use the "as.POSIXct" function

 

Jeff

 

I need to convert a date-time field (column)  but I'm losing the time  when
I convert using  ..

 

tsData <- myData[,10, drop=FALSE]

tsData$date_time <- as.Date(tsData$date_time, format="%m/%d/%y %H:%M")

head(tsData)

 


 

 

date_time

<date>

	

1

2013-06-20

	

2

2013-06-20

	

3

2013-06-20

	

4

2013-06-20

	

5

2013-05-30

	

6

2013-06-20

	

 

R is doing what I'm asing it to do so I'm obviously using the wrong command.
How do I convert, retaining the time 

 

Sincerely

 

Jeffery (Jeff) Reichman


	[[alternative HTML version deleted]]


From jwd @end|ng |rom @urewe@t@net  Thu May  9 00:19:44 2019
From: jwd @end|ng |rom @urewe@t@net (John)
Date: Wed, 8 May 2019 15:19:44 -0700
Subject: [R] install R
In-Reply-To: <20a7a9e5.86bb.16a978c81af.Coremail.yueli7@126.com>
References: <20a7a9e5.86bb.16a978c81af.Coremail.yueli7@126.com>
Message-ID: <20190508151944.7519b6f6@Draco.localdomain>

On Wed, 8 May 2019 21:05:29 +0800 (CST)
yueli <yueli7 at 126.com> wrote:

> Hello,
> 
> 
> I am trying to install R.
> 
> 
> Thanks in advance for any help!
> 
> 
> Yue
> 
> 
It appears that your GCC compiler installation doesn't include all the
compilers that can come with it.  All those "no's" listed in your
output listing should be "yes" to compile R, its help files, and
various compiled material required by add-on packages (or just about
anything else that needs a compiler other than vanilla C). At a
minimum you should have both "gfortran" and "g++" in your /usr/bin
directory.  You can see from your output that the configure routine
looks for several different alternatives for fortran compiler and none
are found. 

As it stands, the only compiler you seem to have enabled appears to
be the basic GNU C compiler. It looks as if you might have a minimal
desktop system installed, but linux is open source, which means that to
fully use it, you will want various compilers for any software that
comes as source you want to compile.  Your alternative with R
would be to install the binary, but the lack of compilers will still be
a problem with many packages you may want to use in R, since they too
often use at least the Fortran compiler, and I'm pretty sure I have
see g++ called as well.

JDougherty


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Thu May  9 03:22:56 2019
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Thu, 9 May 2019 01:22:56 +0000
Subject: [R] Trying to understand the magic of lm
Message-ID: <BN7PR03MB37305C22541D2254354FBAFDE2330@BN7PR03MB3730.namprd03.prod.outlook.com>

Can someone send me something I can read about passing parameters so I can understand how lm manages to have a dataframe passed to it, and use columns from the dataframe to set up a regression. I have looked at the code for lm and don't understand what I am reading. What I want to do is something like the following,


myfunction <- function(y,x,dataframe){

  fit0 <- lm(y~x,data=dataframe)
  print (summary(fit0))
}

# Run the function using dep and ind as dependent and independent variables.
mydata <- data.frame(dep=c(1,2,3,4,5),ind=c(1,2,4,5,7))
myfunction(dep,ind)
# Run the function using outcome and predictor as dependent and independent variables.
newdata <- data.frame(outcome=c(1,2,3,4,5),predictor=c(1,2,4,5,7))
myfunction(outcome,predictor)





John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)


	[[alternative HTML version deleted]]


From btupper @end|ng |rom b|ge|ow@org  Thu May  9 03:35:49 2019
From: btupper @end|ng |rom b|ge|ow@org (Ben Tupper)
Date: Wed, 8 May 2019 21:35:49 -0400
Subject: [R] Trying to understand the magic of lm
In-Reply-To: <BN7PR03MB37305C22541D2254354FBAFDE2330@BN7PR03MB3730.namprd03.prod.outlook.com>
References: <BN7PR03MB37305C22541D2254354FBAFDE2330@BN7PR03MB3730.namprd03.prod.outlook.com>
Message-ID: <233DC1A2-8EA3-4E35-B404-B3C80528214C@bigelow.org>

Hi,

I'm not sure if this is what you are after, but instead of defining arguments for elements of the formula why not simply pass your desired formula to your function?

Cheers,
Ben



myfunction <- function(frmla,dataframe){
 fit0 <- lm(frmla,data=dataframe)
 print (summary(fit0))
}

# Run the function using dep and ind as dependent and independent variables.
mydata <- data.frame(dep=c(1,2,3,4,5),ind=c(1,2,4,5,7))
myfunction(ind ~ dep, mydata)

# Call:
# lm(formula = frmla, data = dataframe)

# Residuals:
   # 1    2    3    4    5 
 # 0.2 -0.3  0.2 -0.3  0.2 

# Coefficients:
            # Estimate Std. Error t value Pr(>|t|)    
# (Intercept)  -0.7000     0.3317  -2.111 0.125298    
# dep           1.5000     0.1000  15.000 0.000643 ***
# ---
# Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

# Residual standard error: 0.3162 on 3 degrees of freedom
# Multiple R-squared:  0.9868,	Adjusted R-squared:  0.9825 
# F-statistic:   225 on 1 and 3 DF,  p-value: 0.0006431


# Run the function using outcome and predictor as dependent and independent variables.
newdata <- data.frame(outcome=c(1,2,3,4,5),predictor=c(1,2,4,5,7))
myfunction(predictor ~ outcome, newdata)

# # Call:
# lm(formula = frmla, data = dataframe)

# Residuals:
   # 1    2    3    4    5 
 # 0.2 -0.3  0.2 -0.3  0.2 

# Coefficients:
            # Estimate Std. Error t value Pr(>|t|)    
# (Intercept)  -0.7000     0.3317  -2.111 0.125298    
# outcome       1.5000     0.1000  15.000 0.000643 ***
# ---
# Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

# Residual standard error: 0.3162 on 3 degrees of freedom
# Multiple R-squared:  0.9868,	Adjusted R-squared:  0.9825 
# F-statistic:   225 on 1 and 3 DF,  p-value: 0.0006431




> On May 8, 2019, at 9:22 PM, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
> 
> Can someone send me something I can read about passing parameters so I can understand how lm manages to have a dataframe passed to it, and use columns from the dataframe to set up a regression. I have looked at the code for lm and don't understand what I am reading. What I want to do is something like the following,
> 
> 
> myfunction <- function(y,x,dataframe){
> 
>  fit0 <- lm(y~x,data=dataframe)
>  print (summary(fit0))
> }
> 
> # Run the function using dep and ind as dependent and independent variables.
> mydata <- data.frame(dep=c(1,2,3,4,5),ind=c(1,2,4,5,7))
> myfunction(dep,ind)
> # Run the function using outcome and predictor as dependent and independent variables.
> newdata <- data.frame(outcome=c(1,2,3,4,5),predictor=c(1,2,4,5,7))
> myfunction(outcome,predictor)
> 
> 
> 
> 
> 
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org

Ecological Forecasting: https://eco.bigelow.org/






	[[alternative HTML version deleted]]


From |@heemj@n93 @end|ng |rom y@hoo@com  Thu May  9 11:28:43 2019
From: |@heemj@n93 @end|ng |rom y@hoo@com (Faheem Jan)
Date: Thu, 9 May 2019 09:28:43 +0000 (UTC)
Subject: [R] functional autoregressive model of order two
References: <515702461.1968112.1557394123238.ref@mail.yahoo.com>
Message-ID: <515702461.1968112.1557394123238@mail.yahoo.com>

HI, i am functional data analysis, using times series data to which i fit the functional autoregressive model of order one using FAR package in R, know i want to extend my model to order two as it is observed that the FAR package can? only do for order one., can any body help me that how to extend my model to order two or suggest me any package which help to solve my problem. I will be thanks full in this regard.?


	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu May  9 11:37:32 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 9 May 2019 10:37:32 +0100
Subject: [R] Trying to understand the magic of lm
In-Reply-To: <BN7PR03MB37305C22541D2254354FBAFDE2330@BN7PR03MB3730.namprd03.prod.outlook.com>
References: <BN7PR03MB37305C22541D2254354FBAFDE2330@BN7PR03MB3730.namprd03.prod.outlook.com>
Message-ID: <2ae96f37-f17b-dcb2-17d4-0aadfd53d8cb@sapo.pt>

Hello,

There is a "standard" deparse/substitute trick that gets the names of 
the variables passed to a function. There are more sophisticated ways 
but maybe that is what you are looking for.


myfunction <- function(y, x, dataframe){
   y <- deparse(substitute(y))
   x <- deparse(substitute(x))
   fmla <- as.formula(paste(y, '~', x))
   fit0 <- lm(fmla, data = dataframe)
   summary(fit0)
}

# Run the function using dep and ind as dependent and independent variables.
mydata <- data.frame(dep = c(1,2,3,4,5),ind=c(1,2,4,5,7))
myfunction(dep, ind, mydata)

# Run the function using outcome and predictor as dependent and 
independent variables.
newdata <- data.frame(outcome=c(1,2,3,4,5),predictor=c(1,2,4,5,7))
myfunction(outcome, predictor, newdata)


Note: your function has an argument 'dataframe' that you didn't use in 
any of the two calls.


Hope this helps,

Rui Barradas

?s 02:22 de 09/05/19, Sorkin, John escreveu:
> Can someone send me something I can read about passing parameters so I can understand how lm manages to have a dataframe passed to it, and use columns from the dataframe to set up a regression. I have looked at the code for lm and don't understand what I am reading. What I want to do is something like the following,
> 
> 
> myfunction <- function(y,x,dataframe){
> 
>    fit0 <- lm(y~x,data=dataframe)
>    print (summary(fit0))
> }
> 
> # Run the function using dep and ind as dependent and independent variables.
> mydata <- data.frame(dep=c(1,2,3,4,5),ind=c(1,2,4,5,7))
> myfunction(dep,ind)
> # Run the function using outcome and predictor as dependent and independent variables.
> newdata <- data.frame(outcome=c(1,2,3,4,5),predictor=c(1,2,4,5,7))
> myfunction(outcome,predictor)
> 
> 
> 
> 
> 
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From p@u|@b|v@nd @end|ng |rom gm@||@com  Thu May  9 14:15:27 2019
From: p@u|@b|v@nd @end|ng |rom gm@||@com (Paul Bivand)
Date: Thu, 9 May 2019 13:15:27 +0100
Subject: [R] install R
In-Reply-To: <20190508151944.7519b6f6@Draco.localdomain>
References: <20a7a9e5.86bb.16a978c81af.Coremail.yueli7@126.com>
 <20190508151944.7519b6f6@Draco.localdomain>
Message-ID: <CAC=KSNjgBQULMC3vzbzY6JB2Uo7zKjKhjYyoLp2_Y78R01Dj1g@mail.gmail.com>

I'd encourage you to install a full build system for your linux. That
way you are able to compile any contributed packages you may want.

Your linux package manager (apt, dnf, urpmi etc) may be able to
provide an R compiled package, which with the exception of
Debian/Ubuntu is likely to be several versions out of date. However,
they will provide a command to install all the things needed to
compile R.

Something like dnf builddep R-cran-base-3.5.0.src.rpm will install all
the build dependencies for R 3.5.0, which should be good for 3.6.0
too. The names of the source file differ between distributions.

Good luck

Paul Bivand

On Wed, 8 May 2019 at 23:20, John via R-help <r-help at r-project.org> wrote:
>
> On Wed, 8 May 2019 21:05:29 +0800 (CST)
> yueli <yueli7 at 126.com> wrote:
>
> > Hello,
> >
> >
> > I am trying to install R.
> >
> >
> > Thanks in advance for any help!
> >
> >
> > Yue
> >
> >
> It appears that your GCC compiler installation doesn't include all the
> compilers that can come with it.  All those "no's" listed in your
> output listing should be "yes" to compile R, its help files, and
> various compiled material required by add-on packages (or just about
> anything else that needs a compiler other than vanilla C). At a
> minimum you should have both "gfortran" and "g++" in your /usr/bin
> directory.  You can see from your output that the configure routine
> looks for several different alternatives for fortran compiler and none
> are found.
>
> As it stands, the only compiler you seem to have enabled appears to
> be the basic GNU C compiler. It looks as if you might have a minimal
> desktop system installed, but linux is open source, which means that to
> fully use it, you will want various compilers for any software that
> comes as source you want to compile.  Your alternative with R
> would be to install the binary, but the lack of compilers will still be
> a problem with many packages you may want to use in R, since they too
> often use at least the Fortran compiler, and I'm pretty sure I have
> see g++ called as well.
>
> JDougherty
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ju@ngomezdu@@o @end|ng |rom gm@||@com  Thu May  9 12:06:15 2019
From: ju@ngomezdu@@o @end|ng |rom gm@||@com (Juan Gomez)
Date: Thu, 9 May 2019 12:06:15 +0200
Subject: [R] error in duplicated() man page
Message-ID: <CAN+qQ0_98V=Myb6UKnJDpBh8sU2-a7BrmF8tbc_SAUeOd-wRZA@mail.gmail.com>

I think there is an error in duplicated() help page when it states that:
"The array method calculates for each element of the sub-array
specified by MARGIN if the remaining dimensions are identical to those
for an earlier (or later, when fromLast = TRUE) element (in row-major
order). "
Instead of:
"... (in column-major order)"

For instance:
duplicated(array(c(1,2,3,2,5,6),c(3,2)), MARGIN=1:2)
      [,1]  [,2]
[1,] FALSE  TRUE
[2,] FALSE FALSE
[3,] FALSE FALSE
>


From bgunter@4567 @end|ng |rom gm@||@com  Thu May  9 17:46:15 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 9 May 2019 08:46:15 -0700
Subject: [R] error in duplicated() man page
In-Reply-To: <CAN+qQ0_98V=Myb6UKnJDpBh8sU2-a7BrmF8tbc_SAUeOd-wRZA@mail.gmail.com>
References: <CAN+qQ0_98V=Myb6UKnJDpBh8sU2-a7BrmF8tbc_SAUeOd-wRZA@mail.gmail.com>
Message-ID: <CAGxFJbQJ6qOeRCZFoDASFB0HqUBud=codcZ4YNZvwHQp2s=JVA@mail.gmail.com>

Juan:
No, I think there may be a bug:

> duplicated(array(c(1,2,3,2,5,6),c(3,2)), MARGIN=1:2)
      [,1]  [,2]
[1,] FALSE  TRUE
[2,] FALSE FALSE
[3,] FALSE FALSE
## This is wrong

## But if we first define the array...
> a <- array(c(1,2,3,4,5,6), c(3,2))
> duplicated(a, MARGIN = 1:2)
      [,1]  [,2]
[1,] FALSE FALSE
[2,] FALSE FALSE
[3,] FALSE FALSE
## This is right

I'll wait a bit before filing a bug report so that any error I may be
making can be pointed out (note that my R version is NOT current, so I need
to update).

> sessionInfo()
R version 3.5.2 (2018-12-20)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS Mojave 10.14.4

Matrix products: default
BLAS:
/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
LAPACK:
/Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] compiler_3.5.2 tools_3.5.2

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, May 9, 2019 at 8:10 AM Juan Gomez <juangomezduaso at gmail.com> wrote:

> I think there is an error in duplicated() help page when it states that:
> "The array method calculates for each element of the sub-array
> specified by MARGIN if the remaining dimensions are identical to those
> for an earlier (or later, when fromLast = TRUE) element (in row-major
> order). "
> Instead of:
> "... (in column-major order)"
>
> For instance:
> duplicated(array(c(1,2,3,2,5,6),c(3,2)), MARGIN=1:2)
>       [,1]  [,2]
> [1,] FALSE  TRUE
> [2,] FALSE FALSE
> [3,] FALSE FALSE
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Thu May  9 17:53:50 2019
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Thu, 9 May 2019 17:53:50 +0200
Subject: [R] error in duplicated() man page
In-Reply-To: <CAGxFJbQJ6qOeRCZFoDASFB0HqUBud=codcZ4YNZvwHQp2s=JVA@mail.gmail.com>
References: <CAN+qQ0_98V=Myb6UKnJDpBh8sU2-a7BrmF8tbc_SAUeOd-wRZA@mail.gmail.com>
 <CAGxFJbQJ6qOeRCZFoDASFB0HqUBud=codcZ4YNZvwHQp2s=JVA@mail.gmail.com>
Message-ID: <23764.19726.176465.757858@stat.math.ethz.ch>

>>>>> Bert Gunter 
>>>>>     on Thu, 9 May 2019 08:46:15 -0700 writes:

    > Juan:
    > No, I think there may be a bug:

    >> duplicated(array(c(1,2,3,2,5,6),c(3,2)), MARGIN=1:2)
    > [,1]  [,2]
    > [1,] FALSE  TRUE
    > [2,] FALSE FALSE
    > [3,] FALSE FALSE
    > ## This is wrong

    > ## But if we first define the array...
    >> a <- array(c(1,2,3,4,5,6), c(3,2))
    >> duplicated(a, MARGIN = 1:2)
    > [,1]  [,2]
    > [1,] FALSE FALSE
    > [2,] FALSE FALSE
    > [3,] FALSE FALSE
    > ## This is right

Well, the two arrays are different:
The first has a '2' instead of a '4'
((and this would not happen if you used 1:6  instead  ..))

    > I'll wait a bit before filing a bug report so that any error I may be
    > making can be pointed out (note that my R version is NOT current, so I need
    > to update).

    >> sessionInfo()
    > R version 3.5.2 (2018-12-20)
    > Platform: x86_64-apple-darwin15.6.0 (64-bit)
    > Running under: macOS Mojave 10.14.4

    > Matrix products: default
    > BLAS:
    > /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
    > LAPACK:
    > /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib

    > locale:
    > [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

    > attached base packages:
    > [1] stats     graphics  grDevices utils     datasets  methods   base

    > loaded via a namespace (and not attached):
    > [1] compiler_3.5.2 tools_3.5.2

    > Bert Gunter

    > "The trouble with having an open mind is that people keep coming along and
    > sticking things into it."
    > -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


    > On Thu, May 9, 2019 at 8:10 AM Juan Gomez <juangomezduaso at gmail.com> wrote:

    >> I think there is an error in duplicated() help page when it states that:
    >> "The array method calculates for each element of the sub-array
    >> specified by MARGIN if the remaining dimensions are identical to those
    >> for an earlier (or later, when fromLast = TRUE) element (in row-major
    >> order). "
    >> Instead of:
    >> "... (in column-major order)"
    >> 
    >> For instance:
    >> duplicated(array(c(1,2,3,2,5,6),c(3,2)), MARGIN=1:2)
    >> [,1]  [,2]
    >> [1,] FALSE  TRUE
    >> [2,] FALSE FALSE
    >> [3,] FALSE FALSE
    >> >
    >> 
    >> ______________________________________________
    >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    >> https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide
    >> http://www.R-project.org/posting-guide.html
    >> and provide commented, minimal, self-contained, reproducible code.
    >> 

    > [[alternative HTML version deleted]]

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.


From wjm1 @end|ng |rom c@@@co|umb|@@edu  Thu May  9 19:01:01 2019
From: wjm1 @end|ng |rom c@@@co|umb|@@edu (William Michels)
Date: Thu, 9 May 2019 10:01:01 -0700
Subject: [R] Trying to understand the magic of lm
In-Reply-To: <BN7PR03MB37305C22541D2254354FBAFDE2330@BN7PR03MB3730.namprd03.prod.outlook.com>
References: <BN7PR03MB37305C22541D2254354FBAFDE2330@BN7PR03MB3730.namprd03.prod.outlook.com>
Message-ID: <CAA99HCxsE2SFczSNreVqHwtx0XUhYkBKDx8Lawt8g9jET8GvTw@mail.gmail.com>

Hello John,

Others have commented on the first half of your question, but the
second half of your question looks very much like R's built-in
predict() functions:

>?predict
>?predict.lm

Best Regards,

Bill.

W. Michels, Ph.D.



On Wed, May 8, 2019 at 6:23 PM Sorkin, John <jsorkin at som.umaryland.edu> wrote:
>
> Can someone send me something I can read about passing parameters so I can understand how lm manages to have a dataframe passed to it, and use columns from the dataframe to set up a regression. I have looked at the code for lm and don't understand what I am reading. What I want to do is something like the following,
>
>
> myfunction <- function(y,x,dataframe){
>
>   fit0 <- lm(y~x,data=dataframe)
>   print (summary(fit0))
> }
>
> # Run the function using dep and ind as dependent and independent variables.
> mydata <- data.frame(dep=c(1,2,3,4,5),ind=c(1,2,4,5,7))
> myfunction(dep,ind)
> # Run the function using outcome and predictor as dependent and independent variables.
> newdata <- data.frame(outcome=c(1,2,3,4,5),predictor=c(1,2,4,5,7))
> myfunction(outcome,predictor)
>
>
>
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Thu May  9 19:15:02 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 9 May 2019 10:15:02 -0700
Subject: [R] error in duplicated() man page
In-Reply-To: <23764.19726.176465.757858@stat.math.ethz.ch>
References: <CAN+qQ0_98V=Myb6UKnJDpBh8sU2-a7BrmF8tbc_SAUeOd-wRZA@mail.gmail.com>
 <CAGxFJbQJ6qOeRCZFoDASFB0HqUBud=codcZ4YNZvwHQp2s=JVA@mail.gmail.com>
 <23764.19726.176465.757858@stat.math.ethz.ch>
Message-ID: <CAGxFJbRb+JdS03u1U9HcZu2-8XLRMxPn7K_taw+iSoZFrqsx0g@mail.gmail.com>

Thanks, Martin. I missed the duplication. My apology -- old age is
asserting it's presence.

Then my response is: I think the documentation is correct as written:

> a <- matrix(rep(1:3,2), nr=3)
> a
     [,1] [,2]
[1,]    1    1
[2,]    2    2
[3,]    3    3

> duplicated(a)
[1] FALSE FALSE FALSE
> ## Note: Row major by default !

> duplicated(a, MAR = 2)
[1] FALSE  TRUE

Again, apologies for my silly error.

Cheers,
Bert



On Thu, May 9, 2019 at 8:56 AM Martin Maechler <maechler at stat.math.ethz.ch>
wrote:

> >>>>> Bert Gunter
> >>>>>     on Thu, 9 May 2019 08:46:15 -0700 writes:
>
>     > Juan:
>     > No, I think there may be a bug:
>
>     >> duplicated(array(c(1,2,3,2,5,6),c(3,2)), MARGIN=1:2)
>     > [,1]  [,2]
>     > [1,] FALSE  TRUE
>     > [2,] FALSE FALSE
>     > [3,] FALSE FALSE
>     > ## This is wrong
>
>     > ## But if we first define the array...
>     >> a <- array(c(1,2,3,4,5,6), c(3,2))
>     >> duplicated(a, MARGIN = 1:2)
>     > [,1]  [,2]
>     > [1,] FALSE FALSE
>     > [2,] FALSE FALSE
>     > [3,] FALSE FALSE
>     > ## This is right
>
> Well, the two arrays are different:
> The first has a '2' instead of a '4'
> ((and this would not happen if you used 1:6  instead  ..))
>
>     > I'll wait a bit before filing a bug report so that any error I may be
>     > making can be pointed out (note that my R version is NOT current, so
> I need
>     > to update).
>
>     >> sessionInfo()
>     > R version 3.5.2 (2018-12-20)
>     > Platform: x86_64-apple-darwin15.6.0 (64-bit)
>     > Running under: macOS Mojave 10.14.4
>
>     > Matrix products: default
>     > BLAS:
>     >
> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
>     > LAPACK:
>     >
> /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
>
>     > locale:
>     > [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>
>     > attached base packages:
>     > [1] stats     graphics  grDevices utils     datasets  methods   base
>
>     > loaded via a namespace (and not attached):
>     > [1] compiler_3.5.2 tools_3.5.2
>
>     > Bert Gunter
>
>     > "The trouble with having an open mind is that people keep coming
> along and
>     > sticking things into it."
>     > -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
>     > On Thu, May 9, 2019 at 8:10 AM Juan Gomez <juangomezduaso at gmail.com>
> wrote:
>
>     >> I think there is an error in duplicated() help page when it states
> that:
>     >> "The array method calculates for each element of the sub-array
>     >> specified by MARGIN if the remaining dimensions are identical to
> those
>     >> for an earlier (or later, when fromLast = TRUE) element (in
> row-major
>     >> order). "
>     >> Instead of:
>     >> "... (in column-major order)"
>     >>
>     >> For instance:
>     >> duplicated(array(c(1,2,3,2,5,6),c(3,2)), MARGIN=1:2)
>     >> [,1]  [,2]
>     >> [1,] FALSE  TRUE
>     >> [2,] FALSE FALSE
>     >> [3,] FALSE FALSE
>     >> >
>     >>
>     >> ______________________________________________
>     >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>     >> https://stat.ethz.ch/mailman/listinfo/r-help
>     >> PLEASE do read the posting guide
>     >> http://www.R-project.org/posting-guide.html
>     >> and provide commented, minimal, self-contained, reproducible code.
>     >>
>
>     > [[alternative HTML version deleted]]
>
>     > ______________________________________________
>     > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>     > https://stat.ethz.ch/mailman/listinfo/r-help
>     > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
>     > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Thu May  9 19:39:54 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 9 May 2019 10:39:54 -0700
Subject: [R] Trying to understand the magic of lm
In-Reply-To: <CAA99HCxsE2SFczSNreVqHwtx0XUhYkBKDx8Lawt8g9jET8GvTw@mail.gmail.com>
References: <BN7PR03MB37305C22541D2254354FBAFDE2330@BN7PR03MB3730.namprd03.prod.outlook.com>
 <CAA99HCxsE2SFczSNreVqHwtx0XUhYkBKDx8Lawt8g9jET8GvTw@mail.gmail.com>
Message-ID: <CAGxFJbSSX+6-27No5_VJYjW6PQX7FHJ02Ohbgtkyax_pkZ2wwg@mail.gmail.com>

I don't think previous responses have addressed the question, which appears
to be: "How does R know to look in the "data" object for the variable names
in the formula?" And, of course, I could be wrong -- in which case ignore
all the following.

My answer to that question is: it's quite complicated. I think you have to
know about calls, function closures, evaluation environments, and the
details of model.frame.lm -- and perhaps more. The following **might** be a
start:

> dat <- data.frame(x = 1:10, y = rnorm(10))
>
> ## substitute() is used to return the unevaluated expression for the call
> mc <- match.call(lm, call = substitute(lm(y~x,data = dat)))
> class(mc)
[1] "call"
> as.list(mc)
[[1]]
lm

$formula
y ~ x

$data
dat

Cheers,
Bert Gunter

On Thu, May 9, 2019 at 10:01 AM William Michels via R-help <
r-help at r-project.org> wrote:

> Hello John,
>
> Others have commented on the first half of your question, but the
> second half of your question looks very much like R's built-in
> predict() functions:
>
> >?predict
> >?predict.lm
>
> Best Regards,
>
> Bill.
>
> W. Michels, Ph.D.
>
>
>
> On Wed, May 8, 2019 at 6:23 PM Sorkin, John <jsorkin at som.umaryland.edu>
> wrote:
> >
> > Can someone send me something I can read about passing parameters so I
> can understand how lm manages to have a dataframe passed to it, and use
> columns from the dataframe to set up a regression. I have looked at the
> code for lm and don't understand what I am reading. What I want to do is
> something like the following,
> >
> >
> > myfunction <- function(y,x,dataframe){
> >
> >   fit0 <- lm(y~x,data=dataframe)
> >   print (summary(fit0))
> > }
> >
> > # Run the function using dep and ind as dependent and independent
> variables.
> > mydata <- data.frame(dep=c(1,2,3,4,5),ind=c(1,2,4,5,7))
> > myfunction(dep,ind)
> > # Run the function using outcome and predictor as dependent and
> independent variables.
> > newdata <- data.frame(outcome=c(1,2,3,4,5),predictor=c(1,2,4,5,7))
> > myfunction(outcome,predictor)
> >
> >
> >
> >
> >
> > John David Sorkin M.D., Ph.D.
> > Professor of Medicine
> > Chief, Biostatistics and Informatics
> > University of Maryland School of Medicine Division of Gerontology and
> Geriatric Medicine
> > Baltimore VA Medical Center
> > 10 North Greene Street
> > GRECC (BT/18/GR)
> > Baltimore, MD 21201-1524
> > (Phone) 410-605-7119
> > (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bog@@o@chr|@to|er @end|ng |rom gm@||@com  Thu May  9 21:36:47 2019
From: bog@@o@chr|@to|er @end|ng |rom gm@||@com (Christofer Bogaso)
Date: Fri, 10 May 2019 01:06:47 +0530
Subject: [R] A question on generating Error message upon Timeout
Message-ID: <CA+dpOJ=ooQ_6OFAVkMM5T48bWi4WRPOYM5-7Jvwi5CWBLq-hWg@mail.gmail.com>

Hi,

I have created a function called myFn() which should be acting as below:

1. If function takes too long, then it will timeout and a specific message
will be displayed
2. This function may generate error on its own, so if it evaluates before
that specific time and fails then another specific message will be displayed
3. Within that specific time, if that function successfully run then the
result will be displayed.

To implement above strategy my function is as follows:


*library(R.utils)*
*myFn = function(n) {*
* n_Try = try({*
* if (n == 1) {Sys.sleep(10); Res = -123}*
* if (n == 2) stop()*
* if (n > 2) Res = 1:5 *
* }, silent = TRUE)*
* if (class(n_Try) == 'try-error') {*
* return("ERROR : Good Error.")*
* } else {*
* return(Res)*
* }*
* }*

Below is for case #1

*aa = *
*tryCatch({*
*  res <- withTimeout({*
*    myFn(1)*
*  }, timeout = 2, onTimeout = 'error')*
*}, error = function(ex) {*
*  ("Timeout Error")*
*}); aa*

I was expecting to get the expression for *aa* as *"Timeout Error"*,
however I am getting *"ERROR : Good Error."*. This is not going with my plan

Also, I dont think this function getting timeout just after 2 sec, appears
like it is evaluating for longer time

However for *myFn(2)*, I am getting right message as "*"ERROR : Good
Error."*"

*aa = *
*tryCatch({*
*  res <- withTimeout({*
*    myFn(2)*
*  }, timeout = 2, onTimeout = 'error')*
*}, error = function(ex) {*
*  ("Timeout Error")*
*}); aa*

So clearly my strategy is failing for *myFn(1)*. Any pointer where I was
wrong?

	[[alternative HTML version deleted]]


From mchowe||2 @end|ng |rom gm@||@com  Thu May  9 21:29:03 2019
From: mchowe||2 @end|ng |rom gm@||@com (Michael Howell)
Date: Thu, 9 May 2019 14:29:03 -0500
Subject: [R] Error message when adding drift for Arima model
Message-ID: <CAFH+Q7wCOKtwZBnc7a9NA+vxHpzUFrYuFQBdB_bGnNSGBGNe_A@mail.gmail.com>

Hello everyone,
So this is my first post to this list, I'm trying to fit an Arima (2,0,0)
model and I think a drift term would help but I'm getting an error term
when I'm trying to include it. Here is my data:

-6.732172338
-2.868884273
-5.371585089
-6.512740463
-4.171062657
-5.738499071
-3.343947176
-1.944879508
-5.464109272
-3.189183392
-3.684700232
-2.168303451
-2.329837082
-0.761979236
-2.189025304
1.094238807
-4.812300745
0.784198777
-1.567075922
0.143963653
1.131119051
2.899746353
-0.498719993
3.121623505 I created a time series object with 24 annual observations. I
didn't include dates because there isn't an observation for every year.

tsdata<-ts(read.csv("...\\Pre2001LaunchDateTraining.csv"), start = c(1,1),
end = c(24,1), frequency = 1) I then created a time series object using the
Arima() function. fitdata <- Arima(tsdata,c(2,0,0),include.drift = "true")
After executing I get this error: Error in (order[2] + seasonal$order[2]) >
1 & include.drift: operations are possible only for numeric, logical or
complex types Traceback: 1. Arima(tsdata, c(2, 0, 0), include.drift = "true")
Any help would be greatly appreciated!

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Thu May  9 23:38:53 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 9 May 2019 14:38:53 -0700
Subject: [R] Error message when adding drift for Arima model
In-Reply-To: <CAFH+Q7wCOKtwZBnc7a9NA+vxHpzUFrYuFQBdB_bGnNSGBGNe_A@mail.gmail.com>
References: <CAFH+Q7wCOKtwZBnc7a9NA+vxHpzUFrYuFQBdB_bGnNSGBGNe_A@mail.gmail.com>
Message-ID: <CAGxFJbTXDv6FKdcBvpsHijj_qQ99B2CjXRHSd3GdfGO6u2Pt5A@mail.gmail.com>

Please start by reading and following the posting guide linked at the
bottom of this email. In particular:

1) Post in **plain text** on this plain text list so we don't get the
mangled html of your post.

2) Tell us what package Arima() is in.

Cheers,
Bert Gunter




On Thu, May 9, 2019 at 2:27 PM Michael Howell <mchowell2 at gmail.com> wrote:

> Hello everyone,
> So this is my first post to this list, I'm trying to fit an Arima (2,0,0)
> model and I think a drift term would help but I'm getting an error term
> when I'm trying to include it. Here is my data:
>
> -6.732172338
> -2.868884273
> -5.371585089
> -6.512740463
> -4.171062657
> -5.738499071
> -3.343947176
> -1.944879508
> -5.464109272
> -3.189183392
> -3.684700232
> -2.168303451
> -2.329837082
> -0.761979236
> -2.189025304
> 1.094238807
> -4.812300745
> 0.784198777
> -1.567075922
> 0.143963653
> 1.131119051
> 2.899746353
> -0.498719993
> 3.121623505 I created a time series object with 24 annual observations. I
> didn't include dates because there isn't an observation for every year.
>
> tsdata<-ts(read.csv("...\\Pre2001LaunchDateTraining.csv"), start = c(1,1),
> end = c(24,1), frequency = 1) I then created a time series object using the
> Arima() function. fitdata <- Arima(tsdata,c(2,0,0),include.drift = "true")
> After executing I get this error: Error in (order[2] + seasonal$order[2]) >
> 1 & include.drift: operations are possible only for numeric, logical or
> complex types Traceback: 1. Arima(tsdata, c(2, 0, 0), include.drift =
> "true")
> Any help would be greatly appreciated!
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Fri May 10 01:26:55 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 9 May 2019 16:26:55 -0700
Subject: [R] Error message when adding drift for Arima model
In-Reply-To: <CAFH+Q7z6QfhMUgOA7_OxKHtqadsJioJQVHD0Dhp+aw8GHaOVtw@mail.gmail.com>
References: <CAFH+Q7wCOKtwZBnc7a9NA+vxHpzUFrYuFQBdB_bGnNSGBGNe_A@mail.gmail.com>
 <CAGxFJbTXDv6FKdcBvpsHijj_qQ99B2CjXRHSd3GdfGO6u2Pt5A@mail.gmail.com>
 <CAFH+Q7z6QfhMUgOA7_OxKHtqadsJioJQVHD0Dhp+aw8GHaOVtw@mail.gmail.com>
Message-ID: <CAGxFJbR0bb2y4jUFEH3Ov85UJKxRMVc+QFRS7UczRuoM-LzNUw@mail.gmail.com>

In future, always cc the list (unless it's personal,which this isn't). I
have done so here. As I am largely ignorant on the subject matter, others
will have to help, which is why you should cc the list.

Cheers,
Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, May 9, 2019 at 3:49 PM Michael Howell <mchowell2 at gmail.com> wrote:

> I apologize for that. The Arima() function that I'm trying to use comes
> from the forecast package. I created a time series object using the above
> 24 observations. The initial model I created doesn't seem to perform so
> well so I thought a drift term might fit the data better. I used the
> following code to create the time series object:
>
> tsdata<- ts(data, start = c(1,1), end = c(24,1), frequency = 1)
>
>
> Where* data* is the dataframe that contains the initial 24 observations.
> I then used the following code to try to create the model:
>
> fitdata  <-  Arima(tsdata,c(2,0,0),include.drift="true")
>>
>
> After doing this I obtained the following error message:
>
> Error in (order[2] + seasonal$order[2]) > 1 & include.drift: operations
>> are possible only for numeric, logical or complex types
>> Traceback:
>>
>> 1. Arima(tsdata, c(2, 0, 0), include.drift = "true")
>
>
>  I hope this is more clear.
>
> On Thu, May 9, 2019 at 4:39 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
>> Please start by reading and following the posting guide linked at the
>> bottom of this email. In particular:
>>
>> 1) Post in **plain text** on this plain text list so we don't get the
>> mangled html of your post.
>>
>> 2) Tell us what package Arima() is in.
>>
>> Cheers,
>> Bert Gunter
>>
>>
>>
>>
>> On Thu, May 9, 2019 at 2:27 PM Michael Howell <mchowell2 at gmail.com>
>> wrote:
>>
>>> Hello everyone,
>>> So this is my first post to this list, I'm trying to fit an Arima (2,0,0)
>>> model and I think a drift term would help but I'm getting an error term
>>> when I'm trying to include it. Here is my data:
>>>
>>> -6.732172338
>>> -2.868884273
>>> -5.371585089
>>> -6.512740463
>>> -4.171062657
>>> -5.738499071
>>> -3.343947176
>>> -1.944879508
>>> -5.464109272
>>> -3.189183392
>>> -3.684700232
>>> -2.168303451
>>> -2.329837082
>>> -0.761979236
>>> -2.189025304
>>> 1.094238807
>>> -4.812300745
>>> 0.784198777
>>> -1.567075922
>>> 0.143963653
>>> 1.131119051
>>> 2.899746353
>>> -0.498719993
>>> 3.121623505 I created a time series object with 24 annual observations. I
>>> didn't include dates because there isn't an observation for every year.
>>>
>>> tsdata<-ts(read.csv("...\\Pre2001LaunchDateTraining.csv"), start =
>>> c(1,1),
>>> end = c(24,1), frequency = 1) I then created a time series object using
>>> the
>>> Arima() function. fitdata <- Arima(tsdata,c(2,0,0),include.drift =
>>> "true")
>>> After executing I get this error: Error in (order[2] +
>>> seasonal$order[2]) >
>>> 1 & include.drift: operations are possible only for numeric, logical or
>>> complex types Traceback: 1. Arima(tsdata, c(2, 0, 0), include.drift =
>>> "true")
>>> Any help would be greatly appreciated!
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri May 10 07:43:16 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 10 May 2019 06:43:16 +0100
Subject: [R] Error message when adding drift for Arima model
In-Reply-To: <CAGxFJbR0bb2y4jUFEH3Ov85UJKxRMVc+QFRS7UczRuoM-LzNUw@mail.gmail.com>
References: <CAFH+Q7wCOKtwZBnc7a9NA+vxHpzUFrYuFQBdB_bGnNSGBGNe_A@mail.gmail.com>
 <CAGxFJbTXDv6FKdcBvpsHijj_qQ99B2CjXRHSd3GdfGO6u2Pt5A@mail.gmail.com>
 <CAFH+Q7z6QfhMUgOA7_OxKHtqadsJioJQVHD0Dhp+aw8GHaOVtw@mail.gmail.com>
 <CAGxFJbR0bb2y4jUFEH3Ov85UJKxRMVc+QFRS7UczRuoM-LzNUw@mail.gmail.com>
Message-ID: <811a30b9-1617-d0a0-9eab-0f29618f8c63@sapo.pt>

Hello,

This is just a typo, in R logical values ("true) are not character 
strings. You must pass FALSE (the default, can be omited) or TRUE.

fitdata  <-  Arima(tsdata, c(2, 0, 0), include.drift = TRUE)


 From the help page ?logical

Details

TRUE and FALSE are reserved words denoting logical constants in the R 
language, whereas T and F are global variables whose initial values set 
to these. All four are logical(1) vectors.

Hope this helps,

Rui Barradas

?s 00:26 de 10/05/19, Bert Gunter escreveu:
> In future, always cc the list (unless it's personal,which this isn't). I
> have done so here. As I am largely ignorant on the subject matter, others
> will have to help, which is why you should cc the list.
> 
> Cheers,
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> 
> On Thu, May 9, 2019 at 3:49 PM Michael Howell <mchowell2 at gmail.com> wrote:
> 
>> I apologize for that. The Arima() function that I'm trying to use comes
>> from the forecast package. I created a time series object using the above
>> 24 observations. The initial model I created doesn't seem to perform so
>> well so I thought a drift term might fit the data better. I used the
>> following code to create the time series object:
>>
>> tsdata<- ts(data, start = c(1,1), end = c(24,1), frequency = 1)
>>
>>
>> Where* data* is the dataframe that contains the initial 24 observations.
>> I then used the following code to try to create the model:
>>
>> fitdata  <-  Arima(tsdata,c(2,0,0),include.drift="true")
>>>
>>
>> After doing this I obtained the following error message:
>>
>> Error in (order[2] + seasonal$order[2]) > 1 & include.drift: operations
>>> are possible only for numeric, logical or complex types
>>> Traceback:
>>>
>>> 1. Arima(tsdata, c(2, 0, 0), include.drift = "true")
>>
>>
>>   I hope this is more clear.
>>
>> On Thu, May 9, 2019 at 4:39 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>
>>> Please start by reading and following the posting guide linked at the
>>> bottom of this email. In particular:
>>>
>>> 1) Post in **plain text** on this plain text list so we don't get the
>>> mangled html of your post.
>>>
>>> 2) Tell us what package Arima() is in.
>>>
>>> Cheers,
>>> Bert Gunter
>>>
>>>
>>>
>>>
>>> On Thu, May 9, 2019 at 2:27 PM Michael Howell <mchowell2 at gmail.com>
>>> wrote:
>>>
>>>> Hello everyone,
>>>> So this is my first post to this list, I'm trying to fit an Arima (2,0,0)
>>>> model and I think a drift term would help but I'm getting an error term
>>>> when I'm trying to include it. Here is my data:
>>>>
>>>> -6.732172338
>>>> -2.868884273
>>>> -5.371585089
>>>> -6.512740463
>>>> -4.171062657
>>>> -5.738499071
>>>> -3.343947176
>>>> -1.944879508
>>>> -5.464109272
>>>> -3.189183392
>>>> -3.684700232
>>>> -2.168303451
>>>> -2.329837082
>>>> -0.761979236
>>>> -2.189025304
>>>> 1.094238807
>>>> -4.812300745
>>>> 0.784198777
>>>> -1.567075922
>>>> 0.143963653
>>>> 1.131119051
>>>> 2.899746353
>>>> -0.498719993
>>>> 3.121623505 I created a time series object with 24 annual observations. I
>>>> didn't include dates because there isn't an observation for every year.
>>>>
>>>> tsdata<-ts(read.csv("...\\Pre2001LaunchDateTraining.csv"), start =
>>>> c(1,1),
>>>> end = c(24,1), frequency = 1) I then created a time series object using
>>>> the
>>>> Arima() function. fitdata <- Arima(tsdata,c(2,0,0),include.drift =
>>>> "true")
>>>> After executing I get this error: Error in (order[2] +
>>>> seasonal$order[2]) >
>>>> 1 & include.drift: operations are possible only for numeric, logical or
>>>> complex types Traceback: 1. Arima(tsdata, c(2, 0, 0), include.drift =
>>>> "true")
>>>> Any help would be greatly appreciated!
>>>>
>>>>          [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri May 10 07:50:58 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 10 May 2019 06:50:58 +0100
Subject: [R] Error message when adding drift for Arima model
In-Reply-To: <811a30b9-1617-d0a0-9eab-0f29618f8c63@sapo.pt>
References: <CAFH+Q7wCOKtwZBnc7a9NA+vxHpzUFrYuFQBdB_bGnNSGBGNe_A@mail.gmail.com>
 <CAGxFJbTXDv6FKdcBvpsHijj_qQ99B2CjXRHSd3GdfGO6u2Pt5A@mail.gmail.com>
 <CAFH+Q7z6QfhMUgOA7_OxKHtqadsJioJQVHD0Dhp+aw8GHaOVtw@mail.gmail.com>
 <CAGxFJbR0bb2y4jUFEH3Ov85UJKxRMVc+QFRS7UczRuoM-LzNUw@mail.gmail.com>
 <811a30b9-1617-d0a0-9eab-0f29618f8c63@sapo.pt>
Message-ID: <5cf42fce-0082-7000-85f7-953de601e310@sapo.pt>

Why not

Arima(tsdata, c(0, 0, 1), include.drift = TRUE)

?

Why do you say it should be an AR(2) model?

Hope this helps,

Rui Barradas

?s 06:43 de 10/05/19, Rui Barradas escreveu:
> Hello,
> 
> This is just a typo, in R logical values ("true) are not character 
> strings. You must pass FALSE (the default, can be omited) or TRUE.
> 
> fitdata? <-? Arima(tsdata, c(2, 0, 0), include.drift = TRUE)
> 
> 
>  From the help page ?logical
> 
> Details
> 
> TRUE and FALSE are reserved words denoting logical constants in the R 
> language, whereas T and F are global variables whose initial values set 
> to these. All four are logical(1) vectors.
> 
> Hope this helps,
> 
> Rui Barradas
> 
> ?s 00:26 de 10/05/19, Bert Gunter escreveu:
>> In future, always cc the list (unless it's personal,which this isn't). I
>> have done so here. As I am largely ignorant on the subject matter, others
>> will have to help, which is why you should cc the list.
>>
>> Cheers,
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming along 
>> and
>> sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>> On Thu, May 9, 2019 at 3:49 PM Michael Howell <mchowell2 at gmail.com> 
>> wrote:
>>
>>> I apologize for that. The Arima() function that I'm trying to use comes
>>> from the forecast package. I created a time series object using the 
>>> above
>>> 24 observations. The initial model I created doesn't seem to perform so
>>> well so I thought a drift term might fit the data better. I used the
>>> following code to create the time series object:
>>>
>>> tsdata<- ts(data, start = c(1,1), end = c(24,1), frequency = 1)
>>>
>>>
>>> Where* data* is the dataframe that contains the initial 24 observations.
>>> I then used the following code to try to create the model:
>>>
>>> fitdata? <-? Arima(tsdata,c(2,0,0),include.drift="true")
>>>>
>>>
>>> After doing this I obtained the following error message:
>>>
>>> Error in (order[2] + seasonal$order[2]) > 1 & include.drift: operations
>>>> are possible only for numeric, logical or complex types
>>>> Traceback:
>>>>
>>>> 1. Arima(tsdata, c(2, 0, 0), include.drift = "true")
>>>
>>>
>>> ? I hope this is more clear.
>>>
>>> On Thu, May 9, 2019 at 4:39 PM Bert Gunter <bgunter.4567 at gmail.com> 
>>> wrote:
>>>
>>>> Please start by reading and following the posting guide linked at the
>>>> bottom of this email. In particular:
>>>>
>>>> 1) Post in **plain text** on this plain text list so we don't get the
>>>> mangled html of your post.
>>>>
>>>> 2) Tell us what package Arima() is in.
>>>>
>>>> Cheers,
>>>> Bert Gunter
>>>>
>>>>
>>>>
>>>>
>>>> On Thu, May 9, 2019 at 2:27 PM Michael Howell <mchowell2 at gmail.com>
>>>> wrote:
>>>>
>>>>> Hello everyone,
>>>>> So this is my first post to this list, I'm trying to fit an Arima 
>>>>> (2,0,0)
>>>>> model and I think a drift term would help but I'm getting an error 
>>>>> term
>>>>> when I'm trying to include it. Here is my data:
>>>>>
>>>>> -6.732172338
>>>>> -2.868884273
>>>>> -5.371585089
>>>>> -6.512740463
>>>>> -4.171062657
>>>>> -5.738499071
>>>>> -3.343947176
>>>>> -1.944879508
>>>>> -5.464109272
>>>>> -3.189183392
>>>>> -3.684700232
>>>>> -2.168303451
>>>>> -2.329837082
>>>>> -0.761979236
>>>>> -2.189025304
>>>>> 1.094238807
>>>>> -4.812300745
>>>>> 0.784198777
>>>>> -1.567075922
>>>>> 0.143963653
>>>>> 1.131119051
>>>>> 2.899746353
>>>>> -0.498719993
>>>>> 3.121623505 I created a time series object with 24 annual 
>>>>> observations. I
>>>>> didn't include dates because there isn't an observation for every 
>>>>> year.
>>>>>
>>>>> tsdata<-ts(read.csv("...\\Pre2001LaunchDateTraining.csv"), start =
>>>>> c(1,1),
>>>>> end = c(24,1), frequency = 1) I then created a time series object 
>>>>> using
>>>>> the
>>>>> Arima() function. fitdata <- Arima(tsdata,c(2,0,0),include.drift =
>>>>> "true")
>>>>> After executing I get this error: Error in (order[2] +
>>>>> seasonal$order[2]) >
>>>>> 1 & include.drift: operations are possible only for numeric, 
>>>>> logical or
>>>>> complex types Traceback: 1. Arima(tsdata, c(2, 0, 0), include.drift =
>>>>> "true")
>>>>> Any help would be greatly appreciated!
>>>>>
>>>>> ???????? [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>
>>
>> ????[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @|gbert @end|ng |rom w|w|@hu-ber||n@de  Fri May 10 09:03:13 2019
From: @|gbert @end|ng |rom w|w|@hu-ber||n@de (Sigbert Klinke)
Date: Fri, 10 May 2019 09:03:13 +0200
Subject: [R] Rd files with duplicated alias 'plot'
Message-ID: <70bc0d2a-74b6-c01c-667d-5ae719548858@wiwi.hu-berlin.de>

Hi,

in package I have for two S3 classes plot routines: plot.ABC and 
plot.DEF. For appearing both under ?plot I added in each file a 
"@aliases plot".

This leads to the above warning message when calling devtools::check(). 
And only one of the routines appears under ?plot.

Any hint how to solve the problem?

Thanks Sigbert

-- 
https://hu.berlin/sk
https://hu.berlin/mmstat3


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri May 10 09:22:44 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 10 May 2019 00:22:44 -0700
Subject: [R] Rd files with duplicated alias 'plot'
In-Reply-To: <70bc0d2a-74b6-c01c-667d-5ae719548858@wiwi.hu-berlin.de>
References: <70bc0d2a-74b6-c01c-667d-5ae719548858@wiwi.hu-berlin.de>
Message-ID: <939E4893-6D7B-4E87-8C21-FFF1815492F1@dcn.davis.ca.us>

a) Wrong list. Read the Posting Guide.

b) Don't use @aliases for this purpose.

c) Just use @export... plot already has a dispatch function.

d) Best for you to be clear (don't assume everyone will know what you are talking about) that this question is about a contributed package called roxygen2. R Core supports Rd files directly.

On May 10, 2019 12:03:13 AM PDT, Sigbert Klinke <sigbert at wiwi.hu-berlin.de> wrote:
>Hi,
>
>in package I have for two S3 classes plot routines: plot.ABC and 
>plot.DEF. For appearing both under ?plot I added in each file a 
>"@aliases plot".
>
>This leads to the above warning message when calling devtools::check().
>
>And only one of the routines appears under ?plot.
>
>Any hint how to solve the problem?
>
>Thanks Sigbert

-- 
Sent from my phone. Please excuse my brevity.


From c@|@ndr@ @end|ng |rom rgzm@de  Fri May 10 13:54:11 2019
From: c@|@ndr@ @end|ng |rom rgzm@de (Ivan Calandra)
Date: Fri, 10 May 2019 13:54:11 +0200 (CEST)
Subject: [R] convert microns to nm in a messy dataset
Message-ID: <7778852.1449.1557489251944.JavaMail.open-xchange@mail.rgzm.de>

Dear useRs,

Below is a sample of my dataset (I have more rows and columns).

As you can see in the 2nd column, there are values, the name of the parameter
('Sq' in that case), some integer ('45' in that case) and the unit ('?m' or
'nm').
I know how to extract the rows of interest (those with values), but they are
expressed in different units. All values following a line with the unit are
expressed in that unit, but the number of lines is not constant (sometimes each
value is expressed in a different unit so there will be a new unit line, but
there are sometimes several values in a row expressed in the same unit so
without unit lines in between). I hope this is clear (it should be with the
example provided).
This messy dataset comes from an external software so I don't have any means to
format the ways the data are collated. I have to find a way to deal with it in
R.

What I would like to do is convert the values in nm to ?m; I just need to
multiply by 1000.

What I don't know is how to identify the values that are expressed in nm (all
values that follow a line with 'nm' until there is a line with '?m').

I don't even know how I should search online because I don't know how this kind
of operation is called.
Any help is appreciated.

Thank you in advance.
Ivan


my.data <- structure(list(V1 = c("2019/05/10", "#", "#", "#", "2019/05/10",
"2019/05/10", "2019/05/10", "#", "#", "#", "2019/05/10", "#", "#", "#",
"2019/05/10", "#", "#", "#", "2019/05/10", "2019/05/10"), V19 =
c("0.2012800083", "45", "Sq", "?m", "0.3634383236", "0.4360454777",
"0.3767733568", "45", "Sq", "nm", "102.013048", "45", "Sq", "?m",
"0.1413840498", "45", "Sq", "nm", "65.4459715", "46.45802917")), row.names =
c(NA, 20L), class = "data.frame")

--
Dr. Ivan Calandra
TraCEr, laboratory for Traceology and Controlled Experiments
MONREPOS Archaeological Research Centre and
Museum for Human Behavioural Evolution
Schloss Monrepos
56567 Neuwied, Germany
+49 (0) 2631 9772-243
https://www.researchgate.net/profile/Ivan_Calandra


From w|nter@@d@v|d@p @end|ng |rom gm@||@com  Fri May 10 04:54:10 2019
From: w|nter@@d@v|d@p @end|ng |rom gm@||@com (David Winters)
Date: Thu, 9 May 2019 19:54:10 -0700
Subject: [R] saving an R script
Message-ID: <CAEDvDqur7z4f8k_xgiFnRkJiQ9weuc4QKZ6JH9uTSunxCEuuVg@mail.gmail.com>

Greetings,

This is a super embarrassing question. But, how do you save an R
script with code? that is, not doing it the cheat way? where you just
hit the "save" button? what function would it be?

I know how to save the environment in code:

save.image(file='myEnvironment.RData')
quit(save='no')
load('myEnvironment.RData')

But how would I do the same thing with a script? rather than the environment?

David


From pd@|gd @end|ng |rom gm@||@com  Fri May 10 15:29:57 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Fri, 10 May 2019 15:29:57 +0200
Subject: [R] convert microns to nm in a messy dataset
In-Reply-To: <7778852.1449.1557489251944.JavaMail.open-xchange@mail.rgzm.de>
References: <7778852.1449.1557489251944.JavaMail.open-xchange@mail.rgzm.de>
Message-ID: <641F34EA-C6FD-46B8-9C79-B10EE1E7F2A8@gmail.com>

From nm to micron, _divide_ by 1000.... (as you likely know)

What are the units of the first value? Looks like micron in your example, but is there a rule?

Basically, it is a "last observation carried forward" type problem, so something like this:


my.data <- structure(list(V1 = c("2019/05/10", "#", "#", "#", "2019/05/10",
"2019/05/10", "2019/05/10", "#", "#", "#", "2019/05/10", "#", "#", "#",
"2019/05/10", "#", "#", "#", "2019/05/10", "2019/05/10"), V19 =
c("0.2012800083", "45", "Sq", "?m", "0.3634383236", "0.4360454777",
"0.3767733568", "45", "Sq", "nm", "102.013048", "45", "Sq", "?m",
"0.1413840498", "45", "Sq", "nm", "65.4459715", "46.45802917")), row.names =
c(NA, 20L), class = "data.frame")

y <- my.data$V19 
u <- ifelse(y=="nm" | y=="?m", y, NA)
num <- my.data$V1 != "#"
uu <- zoo::na.locf(u, na.rm=FALSE)
data.frame(val = as.numeric(y[num]), units = uu[num])

giving 
          val units
1   0.2012800  <NA>
2   0.3634383    ?m
3   0.4360455    ?m
4   0.3767734    ?m
5 102.0130480    nm
6   0.1413840    ?m
7  65.4459715    nm
8  46.4580292    nm

and you can surely take it from there.

-pd


> On 10 May 2019, at 13:54 , Ivan Calandra <calandra at rgzm.de> wrote:
> 
> Dear useRs,
> 
> Below is a sample of my dataset (I have more rows and columns).
> 
> As you can see in the 2nd column, there are values, the name of the parameter
> ('Sq' in that case), some integer ('45' in that case) and the unit ('?m' or
> 'nm').
> I know how to extract the rows of interest (those with values), but they are
> expressed in different units. All values following a line with the unit are
> expressed in that unit, but the number of lines is not constant (sometimes each
> value is expressed in a different unit so there will be a new unit line, but
> there are sometimes several values in a row expressed in the same unit so
> without unit lines in between). I hope this is clear (it should be with the
> example provided).
> This messy dataset comes from an external software so I don't have any means to
> format the ways the data are collated. I have to find a way to deal with it in
> R.
> 
> What I would like to do is convert the values in nm to ?m; I just need to
> multiply by 1000.
> 
> What I don't know is how to identify the values that are expressed in nm (all
> values that follow a line with 'nm' until there is a line with '?m').
> 
> I don't even know how I should search online because I don't know how this kind
> of operation is called.
> Any help is appreciated.
> 
> Thank you in advance.
> Ivan
> 
> 
> my.data <- structure(list(V1 = c("2019/05/10", "#", "#", "#", "2019/05/10",
> "2019/05/10", "2019/05/10", "#", "#", "#", "2019/05/10", "#", "#", "#",
> "2019/05/10", "#", "#", "#", "2019/05/10", "2019/05/10"), V19 =
> c("0.2012800083", "45", "Sq", "?m", "0.3634383236", "0.4360454777",
> "0.3767733568", "45", "Sq", "nm", "102.013048", "45", "Sq", "?m",
> "0.1413840498", "45", "Sq", "nm", "65.4459715", "46.45802917")), row.names =
> c(NA, 20L), class = "data.frame")
> 
> --
> Dr. Ivan Calandra
> TraCEr, laboratory for Traceology and Controlled Experiments
> MONREPOS Archaeological Research Centre and
> Museum for Human Behavioural Evolution
> Schloss Monrepos
> 56567 Neuwied, Germany
> +49 (0) 2631 9772-243
> https://www.researchgate.net/profile/Ivan_Calandra
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From HDor@n @end|ng |rom @|r@org  Fri May 10 15:32:57 2019
From: HDor@n @end|ng |rom @|r@org (Doran, Harold)
Date: Fri, 10 May 2019 13:32:57 +0000
Subject: [R] saving an R script
In-Reply-To: <CAEDvDqur7z4f8k_xgiFnRkJiQ9weuc4QKZ6JH9uTSunxCEuuVg@mail.gmail.com>
References: <CAEDvDqur7z4f8k_xgiFnRkJiQ9weuc4QKZ6JH9uTSunxCEuuVg@mail.gmail.com>
Message-ID: <BN7PR05MB58573D81E12E012AEC8A7DEBCA0C0@BN7PR05MB5857.namprd05.prod.outlook.com>

David

Most of us use an editor of some kind to write our code. I use Notepad++ but there are many options. In this way, you simply write your code in a text file and save it.

Some editors allow for you to execute your code from the script. 

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of David Winters
Sent: Thursday, May 09, 2019 10:54 PM
To: r-help at r-project.org
Subject: [R] saving an R script

Greetings,

This is a super embarrassing question. But, how do you save an R script with code? that is, not doing it the cheat way? where you just hit the "save" button? what function would it be?

I know how to save the environment in code:

save.image(file='myEnvironment.RData')
quit(save='no')
load('myEnvironment.RData')

But how would I do the same thing with a script? rather than the environment?

David

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri May 10 15:42:04 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 10 May 2019 06:42:04 -0700
Subject: [R] saving an R script
In-Reply-To: <CAEDvDqur7z4f8k_xgiFnRkJiQ9weuc4QKZ6JH9uTSunxCEuuVg@mail.gmail.com>
References: <CAEDvDqur7z4f8k_xgiFnRkJiQ9weuc4QKZ6JH9uTSunxCEuuVg@mail.gmail.com>
Message-ID: <27286396-EC39-44C4-9E96-CCDA74865E01@dcn.davis.ca.us>

Don't. Not from within R.

Edit your script in a text editor, and save it using the usual method for that editor. You can copy lines from that file and paste them into the R console to confirm that each part of your script works, or save it regularly and use the source function from within the R console to have R run the whole thing.

One reason why Integrated Development Environments (IDEs) like RStudio and ESS are so popular is that they turn these steps into a hot-key keyboard combination accessible from the text editor. (They also tend to provide typing completion help that helps you enter syntactically correct code more quickly, and display variable contents in a side bar.) But this is the R-help mailing list, where the topic is the language, not the tools, and you should not assume here that anyone else is using the same editor/IDE that you are.


On May 9, 2019 7:54:10 PM PDT, David Winters <winters.david.p at gmail.com> wrote:
>Greetings,
>
>This is a super embarrassing question. But, how do you save an R
>script with code? that is, not doing it the cheat way? where you just
>hit the "save" button? what function would it be?
>
>I know how to save the environment in code:
>
>save.image(file='myEnvironment.RData')
>quit(save='no')
>load('myEnvironment.RData')
>
>But how would I do the same thing with a script? rather than the
>environment?
>
>David
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From rb@er @end|ng |rom @t@u@edu  Fri May 10 15:42:33 2019
From: rb@er @end|ng |rom @t@u@edu (Robert Baer)
Date: Fri, 10 May 2019 08:42:33 -0500
Subject: [R] saving an R script
In-Reply-To: <BN7PR05MB58573D81E12E012AEC8A7DEBCA0C0@BN7PR05MB5857.namprd05.prod.outlook.com>
References: <CAEDvDqur7z4f8k_xgiFnRkJiQ9weuc4QKZ6JH9uTSunxCEuuVg@mail.gmail.com>
 <BN7PR05MB58573D81E12E012AEC8A7DEBCA0C0@BN7PR05MB5857.namprd05.prod.outlook.com>
Message-ID: <891c9ed7-5fc1-c58e-b961-daf4bd717ed1@atsu.edu>

I agree that an editor or a development environment like RStudio is the 
best answer, but perhaps savehistory() function is what you are looking for.

On 5/10/2019 8:32 AM, Doran, Harold wrote:
> David
>
> Most of us use an editor of some kind to write our code. I use Notepad++ but there are many options. In this way, you simply write your code in a text file and save it.
>
> Some editors allow for you to execute your code from the script.
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of David Winters
> Sent: Thursday, May 09, 2019 10:54 PM
> To: r-help at r-project.org
> Subject: [R] saving an R script
>
> Greetings,
>
> This is a super embarrassing question. But, how do you save an R script with code? that is, not doing it the cheat way? where you just hit the "save" button? what function would it be?
>
> I know how to save the environment in code:
>
> save.image(file='myEnvironment.RData')
> quit(save='no')
> load('myEnvironment.RData')
>
> But how would I do the same thing with a script? rather than the environment?
>
> David
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 


--
Robert W. Baer, Ph.D.
Professor of Physiology
Kirksville College of Osteopathic Medicine
A T Still University of Health Sciences
800 W. Jefferson St
Kirksville, MO 63501
660-626-2321 Department
660-626-2965 FAX


From john@@rch|e@mckown @end|ng |rom gm@||@com  Fri May 10 15:47:10 2019
From: john@@rch|e@mckown @end|ng |rom gm@||@com (John McKown)
Date: Fri, 10 May 2019 08:47:10 -0500
Subject: [R] convert microns to nm in a messy dataset
In-Reply-To: <7778852.1449.1557489251944.JavaMail.open-xchange@mail.rgzm.de>
References: <7778852.1449.1557489251944.JavaMail.open-xchange@mail.rgzm.de>
Message-ID: <CAAJSdjiS90wu4adFufvjqSL_zmQjy66rZN5q3OHaUk9keSvLRg@mail.gmail.com>

This is my approach. It is based entirely on what you said (multiply by
1000 to convert from nm to ?m, but I think it is divide by). It assumes
that the starting value is in ?m. If the starting value is in nm, change
the "factor=1" to "factor=1000". That ?m is micro-meters (10^-6) and nm is
nano-meters (10^-9), so divide by would be correct.

 factor=1;
 for (i in 1:length(my.data$V19)) {
 print("Start");print(factor);print(my.data[i,]);
 if (my.data$V19[i] == "nm") { factor=1000;
my.data$V19[i]="?m";print("nm");} else if (my.data$V19[i] == "?m")
{factor=1;};
 if (suppressWarnings(!is.na(as.numeric(my.data$V19[i])))) { my.data$V19[i]
= as.character(as.numeric(my.data$V19[i]) * factor); print("changed"); }
 print(factor);print(my.data[i,]);print("End");



On Fri, May 10, 2019 at 6:54 AM Ivan Calandra <calandra at rgzm.de> wrote:

> Dear useRs,
>
> Below is a sample of my dataset (I have more rows and columns).
>
> As you can see in the 2nd column, there are values, the name of the
> parameter
> ('Sq' in that case), some integer ('45' in that case) and the unit ('?m' or
> 'nm').
> I know how to extract the rows of interest (those with values), but they
> are
> expressed in different units. All values following a line with the unit are
> expressed in that unit, but the number of lines is not constant (sometimes
> each
> value is expressed in a different unit so there will be a new unit line,
> but
> there are sometimes several values in a row expressed in the same unit so
> without unit lines in between). I hope this is clear (it should be with the
> example provided).
> This messy dataset comes from an external software so I don't have any
> means to
> format the ways the data are collated. I have to find a way to deal with
> it in
> R.
>
> What I would like to do is convert the values in nm to ?m; I just need to
> multiply by 1000.
>
> What I don't know is how to identify the values that are expressed in nm
> (all
> values that follow a line with 'nm' until there is a line with '?m').
>
> I don't even know how I should search online because I don't know how this
> kind
> of operation is called.
> Any help is appreciated.
>
> Thank you in advance.
> Ivan
>
>
> my.data <- structure(list(V1 = c("2019/05/10", "#", "#", "#", "2019/05/10",
> "2019/05/10", "2019/05/10", "#", "#", "#", "2019/05/10", "#", "#", "#",
> "2019/05/10", "#", "#", "#", "2019/05/10", "2019/05/10"), V19 =
> c("0.2012800083", "45", "Sq", "?m", "0.3634383236", "0.4360454777",
> "0.3767733568", "45", "Sq", "nm", "102.013048", "45", "Sq", "?m",
> "0.1413840498", "45", "Sq", "nm", "65.4459715", "46.45802917")), row.names
> =
> c(NA, 20L), class = "data.frame")
>
> --
> Dr. Ivan Calandra
> TraCEr, laboratory for Traceology and Controlled Experiments
> MONREPOS Archaeological Research Centre and
> Museum for Human Behavioural Evolution
> Schloss Monrepos
> 56567 Neuwied, Germany
> +49 (0) 2631 9772-243
> https://www.researchgate.net/profile/Ivan_Calandra
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
This is clearly another case of too many mad scientists, and not enough
hunchbacks.


Maranatha! <><
John McKown

	[[alternative HTML version deleted]]


From h@@@n@d|w@n @end|ng |rom gm@||@com  Fri May 10 16:37:01 2019
From: h@@@n@d|w@n @end|ng |rom gm@||@com (Hasan Diwan)
Date: Fri, 10 May 2019 14:37:01 -0000
Subject: [R] R 3.4.4 is released
In-Reply-To: <3e5108a7-81ee-3092-c8dc-7e11aa1c709b@dewey.myzen.co.uk>
References: <817ABAF4-92B2-461B-A361-7CD2350E0C61@email.wm.edu>
 <3e5108a7-81ee-3092-c8dc-7e11aa1c709b@dewey.myzen.co.uk>
Message-ID: <CAP+bYWC4xV1kq_1nsA+bFbji+gtba1Tq3_VuVfRaQjPUrhdzEg-7943@mail.gmail.com>

Congrats to the team! -- H

On Thu, 18 Apr 2019 at 08:35, Michael Dewey <lists at dewey.myzen.co.uk> wrote:

> Dear Stephen
>
> Questions about RStudio ae best asked in their help forums but I would
> definitely install the latest version of R and RStudio and do
> update.packages before asking
>
> Michael
>
> On 18/04/2019 13:19, Stephen Muldoon wrote:
> > Hi,
> >
> > I am new to R studio. If my R studio continually asks to restart for new
> packages to run, should I remove R studio and reinstall this latest version?
> >
> > Thanks,
> > Stephen
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> > ---
> > This email has been checked for viruses by AVG.
> > https://www.avg.com
> >
> >
>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
OpenPGP:
https://sks-keyservers.net/pks/lookup?op=get&search=0xFEBAD7FFD041BBA1
If you wish to request my time, please do so using
*bit.ly/hd1AppointmentRequest
<http://bit.ly/hd1AppointmentRequest>*.
Si vous voudrais faire connnaisance, allez a *bit.ly/hd1AppointmentRequest
<http://bit.ly/hd1AppointmentRequest>*.

<https://sks-keyservers.net/pks/lookup?op=get&search=0xFEBAD7FFD041BBA1>Sent
from my mobile device
Envoye de mon portable

	[[alternative HTML version deleted]]


From bor|@@@te|pe @end|ng |rom utoronto@c@  Fri May 10 17:12:19 2019
From: bor|@@@te|pe @end|ng |rom utoronto@c@ (Boris Steipe)
Date: Fri, 10 May 2019 15:12:19 +0000
Subject: [R] saving an R script
In-Reply-To: <891c9ed7-5fc1-c58e-b961-daf4bd717ed1@atsu.edu>
References: <CAEDvDqur7z4f8k_xgiFnRkJiQ9weuc4QKZ6JH9uTSunxCEuuVg@mail.gmail.com>
 <BN7PR05MB58573D81E12E012AEC8A7DEBCA0C0@BN7PR05MB5857.namprd05.prod.outlook.com>
 <891c9ed7-5fc1-c58e-b961-daf4bd717ed1@atsu.edu>
Message-ID: <F8711827-CE28-4C81-916F-DD6273AD8D03@utoronto.ca>

If the script is originally in a file. I use ...

myScript <- readLines(<filename>)
# [...] modify the myScript vector
writeLines(myScript, con = <filename>)

If this is not what you mean, perhaps you can describe your intended workflow more explicitly.

Cheers,
B.




> On 2019-05-10, at 09:42, Robert Baer <rbaer at atsu.edu> wrote:
> 
> I agree that an editor or a development environment like RStudio is the best answer, but perhaps savehistory() function is what you are looking for.
> 
> On 5/10/2019 8:32 AM, Doran, Harold wrote:
>> David
>> 
>> Most of us use an editor of some kind to write our code. I use Notepad++ but there are many options. In this way, you simply write your code in a text file and save it.
>> 
>> Some editors allow for you to execute your code from the script.
>> 
>> -----Original Message-----
>> From: R-help <r-help-bounces at r-project.org> On Behalf Of David Winters
>> Sent: Thursday, May 09, 2019 10:54 PM
>> To: r-help at r-project.org
>> Subject: [R] saving an R script
>> 
>> Greetings,
>> 
>> This is a super embarrassing question. But, how do you save an R script with code? that is, not doing it the cheat way? where you just hit the "save" button? what function would it be?
>> 
>> I know how to save the environment in code:
>> 
>> save.image(file='myEnvironment.RData')
>> quit(save='no')
>> load('myEnvironment.RData')
>> 
>> But how would I do the same thing with a script? rather than the environment?
>> 
>> David
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> -- 
> 
> 
> --
> Robert W. Baer, Ph.D.
> Professor of Physiology
> Kirksville College of Osteopathic Medicine
> A T Still University of Health Sciences
> 800 W. Jefferson St
> Kirksville, MO 63501
> 660-626-2321 Department
> 660-626-2965 FAX
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Fri May 10 21:53:32 2019
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Fri, 10 May 2019 19:53:32 +0000
Subject: [R] Trying to understand the magic of lm (Still trying)
Message-ID: <BN7PR03MB37305AD07CF68968208E5F41E20C0@BN7PR03MB3730.namprd03.prod.outlook.com>

A number of people have helped me in my mission to understand how lm (and other fucntions) are able to pass a dataframe and then refer to a specific column in the dataframe. I thank everyone who has responded. I now know a bit about deparse(substitute(xx)), but I still don't fully understand how it works. The program below attempts to print a column of a dataframe from a function whose parameters include the dataframe (df) and the column requested (col). The program works fine until the last print statement were I receive an error,  Error in `[.data.frame`(df, , col) : object 'y' not found . I hope someone can explain to me (1) why my code does not work, and (2) what I can do to fix it.


Many thanks to everyone who tries to help lost souls like me!


Thank you,

John


data <- data.frame(x=c(1,2,3,4,5),y=c(5,4,3,2,1))
data

doit <- function(df,col){
  dfx <- deparse(substitute(df))
  colx<- deparse(substitute(col))

  cat("results of deparse substitute")
  print(colx)
  print (dfx)

  cat("I can print the columns using column relative reference\n")
  print(df[,1])
  print(df[,2])

  cat("I can print the entire data frame \n")
  print(df)

  cat("I can print a single columng from the dataframe using a column name\n")
  print(df[,col])
}

doit(data,y)









John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)


	[[alternative HTML version deleted]]


From gor@n@bro@trom @end|ng |rom umu@@e  Fri May 10 22:12:42 2019
From: gor@n@bro@trom @end|ng |rom umu@@e (=?UTF-8?Q?G=c3=b6ran_Brostr=c3=b6m?=)
Date: Fri, 10 May 2019 22:12:42 +0200
Subject: [R] saving an R script
In-Reply-To: <CAEDvDqur7z4f8k_xgiFnRkJiQ9weuc4QKZ6JH9uTSunxCEuuVg@mail.gmail.com>
References: <CAEDvDqur7z4f8k_xgiFnRkJiQ9weuc4QKZ6JH9uTSunxCEuuVg@mail.gmail.com>
Message-ID: <1f2a5c77-2247-2601-2db7-dec5c538d4a1@umu.se>

?dump

Den 2019-05-10 kl. 04:54, skrev David Winters:
> Greetings,
> 
> This is a super embarrassing question. But, how do you save an R
> script with code? that is, not doing it the cheat way? where you just
> hit the "save" button? what function would it be?
> 
> I know how to save the environment in code:
> 
> save.image(file='myEnvironment.RData')
> quit(save='no')
> load('myEnvironment.RData')
> 
> But how would I do the same thing with a script? rather than the environment?
> 
> David
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dw|n@em|u@ @end|ng |rom comc@@t@net  Fri May 10 22:29:58 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Fri, 10 May 2019 13:29:58 -0700
Subject: [R] Trying to understand the magic of lm (Still trying)
In-Reply-To: <BN7PR03MB37305AD07CF68968208E5F41E20C0@BN7PR03MB3730.namprd03.prod.outlook.com>
References: <BN7PR03MB37305AD07CF68968208E5F41E20C0@BN7PR03MB3730.namprd03.prod.outlook.com>
Message-ID: <52b7aeb8-f581-dd09-08c6-1a8d10f7625e@comcast.net>


On 5/10/19 12:53 PM, Sorkin, John wrote:
> A number of people have helped me in my mission to understand how lm (and other fucntions) are able to pass a dataframe and then refer to a specific column in the dataframe. I thank everyone who has responded. I now know a bit about deparse(substitute(xx)), but I still don't fully understand how it works. The program below attempts to print a column of a dataframe from a function whose parameters include the dataframe (df) and the column requested (col). The program works fine until the last print statement were I receive an error,  Error in `[.data.frame`(df, , col) : object 'y' not found . I hope someone can explain to me (1) why my code does not work, and (2) what I can do to fix it.
>
>
> Many thanks to everyone who tries to help lost souls like me!
>
>
> Thank you,
>
> John
>
>
> data <- data.frame(x=c(1,2,3,4,5),y=c(5,4,3,2,1))
> data
>
> doit <- function(df,col){
>    dfx <- deparse(substitute(df))
>    colx<- deparse(substitute(col))
>
>    cat("results of deparse substitute")
>    print(colx)
>    print (dfx)
>
>    cat("I can print the columns using column relative reference\n")
>    print(df[,1])
>    print(df[,2])
>
>    cat("I can print the entire data frame \n")
>    print(df)
>
>    cat("I can print a single columng from the dataframe using a column name\n")
>    #

#Try instead:

 ? ? print( df[ , colx]? # colx will be a character value, ... there is 
no `y`-object

> #print(df[,col])
> }
>
> doit(data,y)
>
>
>
>
>
>
>
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From er|cjberger @end|ng |rom gm@||@com  Sat May 11 12:57:43 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Sat, 11 May 2019 13:57:43 +0300
Subject: [R] A question on generating Error message upon Timeout
In-Reply-To: <CA+dpOJ=ooQ_6OFAVkMM5T48bWi4WRPOYM5-7Jvwi5CWBLq-hWg@mail.gmail.com>
References: <CA+dpOJ=ooQ_6OFAVkMM5T48bWi4WRPOYM5-7Jvwi5CWBLq-hWg@mail.gmail.com>
Message-ID: <CAGgJW77CX-pgx+Cv-2_1Pa_sqfxcY34_U3+OhzD8+hxk9gXF8g@mail.gmail.com>

Hi Christofer,
You have a number of misunderstandings.
The first thing you could have tried in order to figure out what was going
on was to remove the 'silent=TRUE' from the call to try().
This would then give you extra information, specifically that there was a
timeout. The exact message that gets printed on the call myFn(1) is
'Error in Sys.sleep(10) : reached elapsed time limit'
This confirms that your timeout condition is met, but that the error is
caught by the try() within myFn().
The class of the error is "try-error", the same as the class of the error
generated by the stop() statement, so the condition
if ( class(n_Try) == 'try-error' ) does not differentiate between the two
cases. You could modify the definition of myFn() with something like:

 myFn = function(n) {
    n_Try = try({
      if (n == 1) {Sys.sleep(10); Res = -123}
      if (n == 2) stop()
      if (n > 2) Res = 1:5
    })  #, silent = FALSE)
    if (class(n_Try) == "try-error") {
      if ( length(grep("reached elapsed time limit",n_Try)) > 0 )
         return("ERROR: Timeout")
      else
        return("ERROR : Good Error.")
    } else {
      return(Res)
    }
  }

HTH,
Eric


On Thu, May 9, 2019 at 10:37 PM Christofer Bogaso <
bogaso.christofer at gmail.com> wrote:

> Hi,
>
> I have created a function called myFn() which should be acting as below:
>
> 1. If function takes too long, then it will timeout and a specific message
> will be displayed
> 2. This function may generate error on its own, so if it evaluates before
> that specific time and fails then another specific message will be
> displayed
> 3. Within that specific time, if that function successfully run then the
> result will be displayed.
>
> To implement above strategy my function is as follows:
>
>
> *library(R.utils)*
> *myFn = function(n) {*
> * n_Try = try({*
> * if (n == 1) {Sys.sleep(10); Res = -123}*
> * if (n == 2) stop()*
> * if (n > 2) Res = 1:5 *
> * }, silent = TRUE)*
> * if (class(n_Try) == 'try-error') {*
> * return("ERROR : Good Error.")*
> * } else {*
> * return(Res)*
> * }*
> * }*
>
> Below is for case #1
>
> *aa = *
> *tryCatch({*
> *  res <- withTimeout({*
> *    myFn(1)*
> *  }, timeout = 2, onTimeout = 'error')*
> *}, error = function(ex) {*
> *  ("Timeout Error")*
> *}); aa*
>
> I was expecting to get the expression for *aa* as *"Timeout Error"*,
> however I am getting *"ERROR : Good Error."*. This is not going with my
> plan
>
> Also, I dont think this function getting timeout just after 2 sec, appears
> like it is evaluating for longer time
>
> However for *myFn(2)*, I am getting right message as "*"ERROR : Good
> Error."*"
>
> *aa = *
> *tryCatch({*
> *  res <- withTimeout({*
> *    myFn(2)*
> *  }, timeout = 2, onTimeout = 'error')*
> *}, error = function(ex) {*
> *  ("Timeout Error")*
> *}); aa*
>
> So clearly my strategy is failing for *myFn(1)*. Any pointer where I was
> wrong?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat May 11 15:33:20 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sat, 11 May 2019 14:33:20 +0100
Subject: [R] A question on generating Error message upon Timeout
In-Reply-To: <CAGgJW77CX-pgx+Cv-2_1Pa_sqfxcY34_U3+OhzD8+hxk9gXF8g@mail.gmail.com>
References: <CA+dpOJ=ooQ_6OFAVkMM5T48bWi4WRPOYM5-7Jvwi5CWBLq-hWg@mail.gmail.com>
 <CAGgJW77CX-pgx+Cv-2_1Pa_sqfxcY34_U3+OhzD8+hxk9gXF8g@mail.gmail.com>
Message-ID: <001ec0b7-095f-f5bc-f75a-57652e6e7e41@sapo.pt>

Hello,

This solution works but when I've tried it, I realized it depends on the 
locale. In mine, pt_PT.UTF-8, the error message is in Portuguese and it 
failed. So I have two suggestions.

1) grep "Sys.sleep" in attr(n_Try, "condition")
2) not related but instead of length(grep(.)) > 0, logical grepl is simpler.

So the second 'if' in the function would become

if (grepl("Sys.sleep", attr(n_Try, "condition")) )

And now everything is Ok.

Hope this helps,

Rui Barradas

?s 11:57 de 11/05/19, Eric Berger escreveu:
> Hi Christofer,
> You have a number of misunderstandings.
> The first thing you could have tried in order to figure out what was going
> on was to remove the 'silent=TRUE' from the call to try().
> This would then give you extra information, specifically that there was a
> timeout. The exact message that gets printed on the call myFn(1) is
> 'Error in Sys.sleep(10) : reached elapsed time limit'
> This confirms that your timeout condition is met, but that the error is
> caught by the try() within myFn().
> The class of the error is "try-error", the same as the class of the error
> generated by the stop() statement, so the condition
> if ( class(n_Try) == 'try-error' ) does not differentiate between the two
> cases. You could modify the definition of myFn() with something like:
> 
>   myFn = function(n) {
>      n_Try = try({
>        if (n == 1) {Sys.sleep(10); Res = -123}
>        if (n == 2) stop()
>        if (n > 2) Res = 1:5
>      })  #, silent = FALSE)
>      if (class(n_Try) == "try-error") {
>        if ( length(grep("reached elapsed time limit",n_Try)) > 0 )
>           return("ERROR: Timeout")
>        else
>          return("ERROR : Good Error.")
>      } else {
>        return(Res)
>      }
>    }
> 
> HTH,
> Eric
> 
> 
> On Thu, May 9, 2019 at 10:37 PM Christofer Bogaso <
> bogaso.christofer at gmail.com> wrote:
> 
>> Hi,
>>
>> I have created a function called myFn() which should be acting as below:
>>
>> 1. If function takes too long, then it will timeout and a specific message
>> will be displayed
>> 2. This function may generate error on its own, so if it evaluates before
>> that specific time and fails then another specific message will be
>> displayed
>> 3. Within that specific time, if that function successfully run then the
>> result will be displayed.
>>
>> To implement above strategy my function is as follows:
>>
>>
>> *library(R.utils)*
>> *myFn = function(n) {*
>> * n_Try = try({*
>> * if (n == 1) {Sys.sleep(10); Res = -123}*
>> * if (n == 2) stop()*
>> * if (n > 2) Res = 1:5 *
>> * }, silent = TRUE)*
>> * if (class(n_Try) == 'try-error') {*
>> * return("ERROR : Good Error.")*
>> * } else {*
>> * return(Res)*
>> * }*
>> * }*
>>
>> Below is for case #1
>>
>> *aa = *
>> *tryCatch({*
>> *  res <- withTimeout({*
>> *    myFn(1)*
>> *  }, timeout = 2, onTimeout = 'error')*
>> *}, error = function(ex) {*
>> *  ("Timeout Error")*
>> *}); aa*
>>
>> I was expecting to get the expression for *aa* as *"Timeout Error"*,
>> however I am getting *"ERROR : Good Error."*. This is not going with my
>> plan
>>
>> Also, I dont think this function getting timeout just after 2 sec, appears
>> like it is evaluating for longer time
>>
>> However for *myFn(2)*, I am getting right message as "*"ERROR : Good
>> Error."*"
>>
>> *aa = *
>> *tryCatch({*
>> *  res <- withTimeout({*
>> *    myFn(2)*
>> *  }, timeout = 2, onTimeout = 'error')*
>> *}, error = function(ex) {*
>> *  ("Timeout Error")*
>> *}); aa*
>>
>> So clearly my strategy is failing for *myFn(1)*. Any pointer where I was
>> wrong?
>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @eren@de@te|@n| @end|ng |rom gm@||@com  Sun May 12 20:02:14 2019
From: @eren@de@te|@n| @end|ng |rom gm@||@com (Serena De Stefani)
Date: Sun, 12 May 2019 14:02:14 -0400
Subject: [R] Flip heatmap color range in base R?
Message-ID: <CAPk26q9DJ+FDusZ6HG4r_7NE8Qvvi1x+v+PgZ-RjLZXG5G+tTA@mail.gmail.com>

I am building a simple heat map in base R.
This is my matrix:

    stleft = matrix(
         c(0,5,5,2,6,8,4,6,9),
         nrow=3,
         ncol=3)
    colnames(stleft) <- c("Narrow","Wide", "Wider")
    rownames(stleft) <- c("Person", "Object","Bare")
    stleft


The matrix looks like this:

    > stleft
           Narrow Wide Wider
    Person      0    2     4
    Object      5    6     6
    Bare        5    8     9

To build the heat map I simply run:

    heatmap(stleft, Colv = NA, Rowv = NA, scale = "none")

You may see it here: https://i.stack.imgur.com/KCblOm.png

As you can see the "0" (corresponding to the narrow/person cell) appears as
a deep red while the "9" (corresponding to the wider/bare cell) appears as
light yellow.

- How can I "flip" this range so that the "0" appears in a light color and
the "9" in a deep color?

I have also three other questions (less important at the moment):

- How can I have a heat map with the same row/column order as my original
matrix?

- Why is the image truncated? It appears truncated in the RStudio plot
panel and it is saved truncated. I have tried to enlarge the plot panel in
R Studio to no avail.

- How can I insert numbers in the cells?

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sun May 12 21:00:39 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 12 May 2019 12:00:39 -0700
Subject: [R] Flip heatmap color range in base R?
In-Reply-To: <CAPk26q9DJ+FDusZ6HG4r_7NE8Qvvi1x+v+PgZ-RjLZXG5G+tTA@mail.gmail.com>
References: <CAPk26q9DJ+FDusZ6HG4r_7NE8Qvvi1x+v+PgZ-RjLZXG5G+tTA@mail.gmail.com>
Message-ID: <CAGxFJbR+V72_XbZYLXNsn5VmRb9_=jp4itBbiN+tXfB0P+eJcA@mail.gmail.com>

?heatmap     (carefully!)

and note the reference to "col" in the "..." argument (and the need to
consult ?image).

I suspect that your other questions can be similarly answered by a careful
reading of the Help file, but I have not checked. You should.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, May 12, 2019 at 11:02 AM Serena De Stefani <
serenadestefani at gmail.com> wrote:

> I am building a simple heat map in base R.
> This is my matrix:
>
>     stleft = matrix(
>          c(0,5,5,2,6,8,4,6,9),
>          nrow=3,
>          ncol=3)
>     colnames(stleft) <- c("Narrow","Wide", "Wider")
>     rownames(stleft) <- c("Person", "Object","Bare")
>     stleft
>
>
> The matrix looks like this:
>
>     > stleft
>            Narrow Wide Wider
>     Person      0    2     4
>     Object      5    6     6
>     Bare        5    8     9
>
> To build the heat map I simply run:
>
>     heatmap(stleft, Colv = NA, Rowv = NA, scale = "none")
>
> You may see it here: https://i.stack.imgur.com/KCblOm.png
>
> As you can see the "0" (corresponding to the narrow/person cell) appears as
> a deep red while the "9" (corresponding to the wider/bare cell) appears as
> light yellow.
>
> - How can I "flip" this range so that the "0" appears in a light color and
> the "9" in a deep color?
>
> I have also three other questions (less important at the moment):
>
> - How can I have a heat map with the same row/column order as my original
> matrix?
>
> - Why is the image truncated? It appears truncated in the RStudio plot
> panel and it is saved truncated. I have tried to enlarge the plot panel in
> R Studio to no avail.
>
> - How can I insert numbers in the cells?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @ndrew@h@||ord @end|ng |rom gm@||@com  Mon May 13 07:00:25 2019
From: @ndrew@h@||ord @end|ng |rom gm@||@com (Andrew Halford)
Date: Mon, 13 May 2019 16:00:25 +1100
Subject: [R] multiple graphs on one plot
Message-ID: <CAJrFtqJM+1K61AHMSXy__RQq7c1UNaqGFqcJwSpw98krUfX0_w@mail.gmail.com>

Hi Listers

I've been trying to make a single graphic that has frequency histograms for
male and female mud crabs displayed side by side (such as when using the
beside=TRUE command for barplots). I then want to display a normal
distribution on top of the male and female histograms.

I have been using the multhist command in Plotrix to generate the
histograms without too much problem, but I cannot get the normal
distributions to plot up on the same graph.

Histograms plot

mf <-
list(lf_crabs$cw[lf_crabs$sex=='female'],lf_crabs$cw[lf_crabs$sex=='male'])
multhist(mf, xlab="CW", ylab="Frequency", ylim=c(0,100),main="All Measured
Crabs", col=c("dark gray", "light gray"),
         breaks=seq(90,210, by=10),beside=TRUE,space=c(0,0.5))
legend("topright", c("Females", "Males"), fill=c("dark gray", "light gray"))

Then I try to add a normal distribution curve just to the female data but I
cant get the output to plot

points(seq(min(mf[[1]]), max(mf[[1]]), length.out=300),
       dnorm(seq(min(mf[[1]]), max(mf[[1]]), length.out=300),
             mean(mf[[1]]), sd(mf[[1]])),type="l", col="dark gray")

Even trying to add an abline to the plot doesn't work.

What am I missing?

cheers

Andy

-- 
Andrew Halford Ph.D
Senior Coastal Fisheries Scientist
Pacific Community | Communaut? du Pacifique CPS ? B.P. D5 | 98848 Noumea,
New Caledonia | Noum?a, Nouvelle-Cal?donie

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Mon May 13 08:30:34 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Mon, 13 May 2019 16:30:34 +1000
Subject: [R] multiple graphs on one plot
In-Reply-To: <CAJrFtqJM+1K61AHMSXy__RQq7c1UNaqGFqcJwSpw98krUfX0_w@mail.gmail.com>
References: <CAJrFtqJM+1K61AHMSXy__RQq7c1UNaqGFqcJwSpw98krUfX0_w@mail.gmail.com>
Message-ID: <CA+8X3fX=Zt9NK3a6j=ECxRRBZc1FXn=c3jacae8y9F-di7RnRQ@mail.gmail.com>

Hi Andrew,
First, a little mind reading. My crystal ball says that "cw" can be
interpreted as "carapace width". It didn't tell me the parameters of
the distribution, so:

set.seed(1234)
mf<-list(rnorm(400,145,15),rnorm(400,160,15))
library(plotrix)
multhist(mf, xlab="CW", ylab="Frequency", ylim=c(0,100),main="All Measured
Crabs", col=c("dark gray", "light gray"),
         breaks=seq(90,210, by=10),beside=TRUE,space=c(0,0.5))
legend("topright", c("Females", "Males"), fill=c("dark gray", "light gray"))
lines(seq(0,32,length.out=121),rescale(dnorm(90:210,145,15),c(0,100)))

This produces what I think you are after. Note that it may be
misleading as the distribution of carapace width in real mud crabs
doesn't look normal to me.

Jim

On Mon, May 13, 2019 at 3:00 PM Andrew Halford <andrew.halford at gmail.com> wrote:
>
> Hi Listers
>
> I've been trying to make a single graphic that has frequency histograms for
> male and female mud crabs displayed side by side (such as when using the
> beside=TRUE command for barplots). I then want to display a normal
> distribution on top of the male and female histograms.
>
> I have been using the multhist command in Plotrix to generate the
> histograms without too much problem, but I cannot get the normal
> distributions to plot up on the same graph.
>
> Histograms plot
>
> mf <-
> list(lf_crabs$cw[lf_crabs$sex=='female'],lf_crabs$cw[lf_crabs$sex=='male'])
> multhist(mf, xlab="CW", ylab="Frequency", ylim=c(0,100),main="All Measured
> Crabs", col=c("dark gray", "light gray"),
>          breaks=seq(90,210, by=10),beside=TRUE,space=c(0,0.5))
> legend("topright", c("Females", "Males"), fill=c("dark gray", "light gray"))
>
> Then I try to add a normal distribution curve just to the female data but I
> cant get the output to plot
>
> points(seq(min(mf[[1]]), max(mf[[1]]), length.out=300),
>        dnorm(seq(min(mf[[1]]), max(mf[[1]]), length.out=300),
>              mean(mf[[1]]), sd(mf[[1]])),type="l", col="dark gray")
>
> Even trying to add an abline to the plot doesn't work.
>
> What am I missing?
>
> cheers
>
> Andy
>
> --
> Andrew Halford Ph.D
> Senior Coastal Fisheries Scientist
> Pacific Community | Communaut? du Pacifique CPS ? B.P. D5 | 98848 Noumea,
> New Caledonia | Noum?a, Nouvelle-Cal?donie
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From c@|@ndr@ @end|ng |rom rgzm@de  Mon May 13 08:41:24 2019
From: c@|@ndr@ @end|ng |rom rgzm@de (Ivan Calandra)
Date: Mon, 13 May 2019 08:41:24 +0200 (CEST)
Subject: [R] convert microns to nm in a messy dataset
In-Reply-To: <CAAJSdjiS90wu4adFufvjqSL_zmQjy66rZN5q3OHaUk9keSvLRg@mail.gmail.com>
References: <7778852.1449.1557489251944.JavaMail.open-xchange@mail.rgzm.de>
 <CAAJSdjiS90wu4adFufvjqSL_zmQjy66rZN5q3OHaUk9keSvLRg@mail.gmail.com>
Message-ID: <26147193.3218.1557729684302.JavaMail.open-xchange@mail.rgzm.de>

Dear John,

Thank you for your answer.
However, it does not make sense to me, as it works only line by line of the
data.frame, and I need something for "last observation carried forward" as Peter
mentioned. The script does not work as is either, probably due to typos with
semi-colons and "if... else" statements, so I cannot really test it.

Best,
Ivan

--
Dr. Ivan Calandra
TraCEr, laboratory for Traceology and Controlled Experiments
MONREPOS Archaeological Research Centre and
Museum for Human Behavioural Evolution
Schloss Monrepos
56567 Neuwied, Germany
+49 (0) 2631 9772-243
https://www.researchgate.net/profile/Ivan_Calandra

On May 10, 2019 at 3:47 PM John McKown <john.archie.mckown at gmail.com> wrote:

>  This is my approach. It is based entirely on what you said (multiply by 1000
> to convert from nm to  ?m, but I think it is divide by). It assumes that the
> starting value is in  ?m. If the starting value is in nm, change the
> "factor=1" to "factor=1000". That  ?m is micro-meters (10^-6) and nm is
> nano-meters (10^-9), so divide by would be correct.
> 
>   factor=1;
>   for (i in 1:length(my.data$V19)) {
>   print("Start");print(factor);print(my.data[i,]);
>   if (my.data$V19[i] == "nm") { factor=1000; my.data$V19[i]="?m";print("nm");}
> else if (my.data$V19[i] == "?m") {factor=1;};
>   if (suppressWarnings(! is.na <http://is.na> (as.numeric(my.data$V19[i])))) {
> my.data$V19[i] = as.character(as.numeric(my.data$V19[i]) * factor);
> print("changed"); }
>   print(factor);print(my.data[i,]);print("End");
> 
> 
>
>  On Fri, May 10, 2019 at 6:54 AM Ivan Calandra < calandra at rgzm.de
> <mailto:calandra at rgzm.de> > wrote:
>    > > Dear useRs,
> >
> >    Below is a sample of my dataset (I have more rows and columns).
> >
> >    As you can see in the 2nd column, there are values, the name of the
> > parameter
> >    ('Sq' in that case), some integer ('45' in that case) and the unit ('?m'
> > or
> >    'nm').
> >    I know how to extract the rows of interest (those with values), but they
> > are
> >    expressed in different units. All values following a line with the unit
> > are
> >    expressed in that unit, but the number of lines is not constant
> > (sometimes each
> >    value is expressed in a different unit so there will be a new unit line,
> > but
> >    there are sometimes several values in a row expressed in the same unit so
> >    without unit lines in between). I hope this is clear (it should be with
> > the
> >    example provided).
> >    This messy dataset comes from an external software so I don't have any
> > means to
> >    format the ways the data are collated. I have to find a way to deal with
> > it in
> >    R.
> >
> >    What I would like to do is convert the values in nm to ?m; I just need to
> >    multiply by 1000.
> >
> >    What I don't know is how to identify the values that are expressed in nm
> > (all
> >    values that follow a line with 'nm' until there is a line with '?m').
> >
> >    I don't even know how I should search online because I don't know how
> > this kind
> >    of operation is called.
> >    Any help is appreciated.
> >
> >    Thank you in advance.
> >    Ivan
> >
> >
> >    my.data <- structure(list(V1 = c("2019/05/10", "#", "#", "#",
> > "2019/05/10",
> >    "2019/05/10", "2019/05/10", "#", "#", "#", "2019/05/10", "#", "#", "#",
> >    "2019/05/10", "#", "#", "#", "2019/05/10", "2019/05/10"), V19 =
> >    c("0.2012800083", "45", "Sq", "?m", "0.3634383236", "0.4360454777",
> >    "0.3767733568", "45", "Sq", "nm", "102.013048", "45", "Sq", "?m",
> >    "0.1413840498", "45", "Sq", "nm", "65.4459715", "46.45802917")),
> > row.names =
> >    c(NA, 20L), class = "data.frame")
> >
> >    --
> >    Dr. Ivan Calandra
> >    TraCEr, laboratory for Traceology and Controlled Experiments
> >    MONREPOS Archaeological Research Centre and
> >    Museum for Human Behavioural Evolution
> >    Schloss Monrepos
> >    56567 Neuwied, Germany
> >    +49 (0) 2631 9772-243
> >    https://www.researchgate.net/profile/Ivan_Calandra
> > <https://www.researchgate.net/profile/Ivan_Calandra>
> >
> >    ______________________________________________
> >    R-help at r-project.org <mailto:R-help at r-project.org> mailing list -- To
> > UNSUBSCRIBE and more, see
> >    https://stat.ethz.ch/mailman/listinfo/r-help
> > <https://stat.ethz.ch/mailman/listinfo/r-help>
> >    PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > <http://www.R-project.org/posting-guide.html>
> >    and provide commented, minimal, self-contained, reproducible code.
> >  >
> 
>  --
>  This is clearly another case of too many mad scientists, and not enough
> hunchbacks.
> 
> 
>  Maranatha! <><
>  John McKown
>


From c@|@ndr@ @end|ng |rom rgzm@de  Mon May 13 08:48:22 2019
From: c@|@ndr@ @end|ng |rom rgzm@de (Ivan Calandra)
Date: Mon, 13 May 2019 08:48:22 +0200 (CEST)
Subject: [R] convert microns to nm in a messy dataset [solved]
In-Reply-To: <641F34EA-C6FD-46B8-9C79-B10EE1E7F2A8@gmail.com>
References: <7778852.1449.1557489251944.JavaMail.open-xchange@mail.rgzm.de>
 <641F34EA-C6FD-46B8-9C79-B10EE1E7F2A8@gmail.com>
Message-ID: <24780861.3250.1557730102618.JavaMail.open-xchange@mail.rgzm.de>

Dear Peter,

Thank you for your answer, the function na.locf() is exactly what I needed!
I had started processing my dataset so the first lines (used as headers) were
not included in the sample I have sent. But there is also a "unit" line before
the first value.

And yes, of course, divide by 1000.

Best,
Ivan

--
Dr. Ivan Calandra
TraCEr, laboratory for Traceology and Controlled Experiments
MONREPOS Archaeological Research Centre and
Museum for Human Behavioural Evolution
Schloss Monrepos
56567 Neuwied, Germany
+49 (0) 2631 9772-243
https://www.researchgate.net/profile/Ivan_Calandra

On May 10, 2019 at 3:29 PM peter dalgaard <pdalgd at gmail.com> wrote:
> From nm to micron, _divide_ by 1000.... (as you likely know)
>
> What are the units of the first value? Looks like micron in your example, but
> is there a rule?
>
> Basically, it is a "last observation carried forward" type problem, so
> something like this:
>
>
> my.data <- structure(list(V1 = c("2019/05/10", "#", "#", "#", "2019/05/10",
> "2019/05/10", "2019/05/10", "#", "#", "#", "2019/05/10", "#", "#", "#",
> "2019/05/10", "#", "#", "#", "2019/05/10", "2019/05/10"), V19 =
> c("0.2012800083", "45", "Sq", "?m", "0.3634383236", "0.4360454777",
> "0.3767733568", "45", "Sq", "nm", "102.013048", "45", "Sq", "?m",
> "0.1413840498", "45", "Sq", "nm", "65.4459715", "46.45802917")), row.names =
> c(NA, 20L), class = "data.frame")
>
> y <- my.data$V19
> u <- ifelse(y=="nm" | y=="?m", y, NA)
> num <- my.data$V1 != "#"
> uu <- zoo::na.locf(u, na.rm=FALSE)
> data.frame(val = as.numeric(y[num]), units = uu[num])
>
> giving
> val units
> 1 0.2012800 <NA>
> 2 0.3634383 ?m
> 3 0.4360455 ?m
> 4 0.3767734 ?m
> 5 102.0130480 nm
> 6 0.1413840 ?m
> 7 65.4459715 nm
> 8 46.4580292 nm
>
> and you can surely take it from there.
>
> -pd
>
>
> > On 10 May 2019, at 13:54 , Ivan Calandra <calandra at rgzm.de> wrote:
> >
> > Dear useRs,
> >
> > Below is a sample of my dataset (I have more rows and columns).
> >
> > As you can see in the 2nd column, there are values, the name of the
> > parameter
> > ('Sq' in that case), some integer ('45' in that case) and the unit ('?m' or
> > 'nm').
> > I know how to extract the rows of interest (those with values), but they are
> > expressed in different units. All values following a line with the unit are
> > expressed in that unit, but the number of lines is not constant (sometimes
> > each
> > value is expressed in a different unit so there will be a new unit line, but
> > there are sometimes several values in a row expressed in the same unit so
> > without unit lines in between). I hope this is clear (it should be with the
> > example provided).
> > This messy dataset comes from an external software so I don't have any means
> > to
> > format the ways the data are collated. I have to find a way to deal with it
> > in
> > R.
> >
> > What I would like to do is convert the values in nm to ?m; I just need to
> > multiply by 1000.
> >
> > What I don't know is how to identify the values that are expressed in nm
> > (all
> > values that follow a line with 'nm' until there is a line with '?m').
> >
> > I don't even know how I should search online because I don't know how this
> > kind
> > of operation is called.
> > Any help is appreciated.
> >
> > Thank you in advance.
> > Ivan
> >
> >
> > my.data <- structure(list(V1 = c("2019/05/10", "#", "#", "#", "2019/05/10",
> > "2019/05/10", "2019/05/10", "#", "#", "#", "2019/05/10", "#", "#", "#",
> > "2019/05/10", "#", "#", "#", "2019/05/10", "2019/05/10"), V19 =
> > c("0.2012800083", "45", "Sq", "?m", "0.3634383236", "0.4360454777",
> > "0.3767733568", "45", "Sq", "nm", "102.013048", "45", "Sq", "?m",
> > "0.1413840498", "45", "Sq", "nm", "65.4459715", "46.45802917")), row.names =
> > c(NA, 20L), class = "data.frame")
> >
> > --
> > Dr. Ivan Calandra
> > TraCEr, laboratory for Traceology and Controlled Experiments
> > MONREPOS Archaeological Research Centre and
> > Museum for Human Behavioural Evolution
> > Schloss Monrepos
> > 56567 Neuwied, Germany
> > +49 (0) 2631 9772-243
> > https://www.researchgate.net/profile/Ivan_Calandra
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>
	[[alternative HTML version deleted]]


From wewo|@k| @end|ng |rom gm@||@com  Mon May 13 13:31:01 2019
From: wewo|@k| @end|ng |rom gm@||@com (Witold E Wolski)
Date: Mon, 13 May 2019 13:31:01 +0200
Subject: [R] linear model contrast in R
Message-ID: <CAAjnpdhe15mK0xT4JE29=UBtp7nMMhOxjj04xGDtH_89co3Qyg@mail.gmail.com>

I am looking for a function to compute contrasts with a interface
similar to that of

lmerTest::contest
multcomp::glht

i.e. taking the model and a contrast vector or matrix as an argument,
but for linear models, and without the multiple testing adjusted made
by multcomp::glht.

Thank you


-- 
Witold Eryk Wolski


From therne@u @end|ng |rom m@yo@edu  Mon May 13 15:29:13 2019
From: therne@u @end|ng |rom m@yo@edu (Therneau, Terry M., Ph.D.)
Date: Mon, 13 May 2019 08:29:13 -0500
Subject: [R] Trying to understand the magic of lm (Still trying)
In-Reply-To: <mailman.354642.1.1557568801.34439.r-help@r-project.org>
References: <mailman.354642.1.1557568801.34439.r-help@r-project.org>
Message-ID: <9ecfbe$bm3sbf@ironport10.mayo.edu>

John,

 ?The text below is cut out of a "how to write a package" course I gave at the R 
conference in Vanderbilt.?? I need to find a home for the course notes, because it had a 
lot of tidbits that are not well explained in the R documentation.
Terry T.

----

Model frames:
One of the first tasks of any modeling routine is to construct a special data frame 
containing the covariates that will be used, via a call to the model.frame function. The 
code to do this is found in many routines, and can be a little opaque on first view. The 
obvious code would be
\begin{verbatim}
coxph <- function(formula, data, weights, subset, na.action,
 ??????? init, control, ties= c("efron", "breslow", "exact"),
 ??????? singular.ok =TRUE, robust=FALSE,
 ??????? model=FALSE, x=FALSE, y=TRUE,? tt, method=ties, ...) {

 ???? mf <- model.frame(formula, data, subset, weights, na.action)
\end{verbatim}
since those are the coxph arguments that are passed forward to the model.frame routine.? 
However, this simple approach will fail with a ``not found'' error message if any of the 
data, subset, weights, etc. arguments are missing. Programs have to take the slightly more 
complicated approach of constructing a call.
\begin{verbatim}
Call <- match.call()
indx <- match(c("formula", "data", "weights", "subset", "na.action"),
 ????????????????? names(Call), nomatch=0)
if (indx[1] ==0) stop("A formula argument is required")
temp <- Call[c(1,indx)]? # only keep the arguments we wanted
temp[[1]] <- as.name('model.frame')? # change the function called
mf <- eval(temp, parent.frame())

Y <- model.response(mf)
etc.
\end{verbatim}

We start with a copy of the call to the program, which we want to save anyway as 
documentation in the output object. Then subscripting is used to extract only the portions 
of the call that we want, saving the result in a temporary. This is based on the fact that 
a call object can be viewed as a list whose first element is the name of the function to 
call, followed by the arguments to the call. Note the use of \code{nomatch=0}; if any 
arguments on the list are missing they will then be missing in \code{temp}, without 
generating an error message. The \mycode{temp} variable will contain a object of type 
``call'', which is an unevaluated call to a routine.? Finally, the name of the function to 
be called is changed from ``coxph'' to ``model.frame'' and the call is evaluated.? In many 
of the core routines the result is stored in a variable ``m''.? This is a horribly short 
and non-descriptive name. (The above used mf which isn't a much better.)? Many routines 
also use ``m'' for the temporary variable leading to \code{m <- eval(m, parent.frame())}, 
but I think that is unnecessarily confusing.

The list of names in the match call will include all arguments that should be evaluated 
within context of the named dataframe. This can include more than the list above, the 
survfit routine for instance has an optional argument ``id'' that names an identifying 
variable (several rows of the data may represent a single subject), and this is included 
along with ``formula'' etc in the list of choices in the match function.? The order of 
names in the list makes no difference.? The id is later retrieved with 
\code{model.extract(m, 'id')}, which will be NULL if the argument was not supplied. At the 
time that coxph was written I had not caught on to this fact and thought that all 
variables that came from a data frame had to be represented in the formula somehow, thus 
the use of \code{cluster(id)} as part of the formula, in order to denote a grouping variable.

On 5/11/19 5:00 AM, r-help-request at r-project.org wrote:
> A number of people have helped me in my mission to understand how lm (and other fucntions) are able to pass a dataframe and then refer to a specific column in the dataframe. I thank everyone who has responded. I now know a bit about deparse(substitute(xx)), but I still don't fully understand how it works. The program below attempts to print a column of a dataframe from a function whose parameters include the dataframe (df) and the column requested (col). The program works fine until the last print statement were I receive an error,  Error in `[.data.frame`(df, , col) : object 'y' not found . I hope someone can explain to me (1) why my code does not work, and (2) what I can do to fix it.


	[[alternative HTML version deleted]]


From rmh @end|ng |rom temp|e@edu  Mon May 13 18:17:04 2019
From: rmh @end|ng |rom temp|e@edu (Richard M. Heiberger)
Date: Mon, 13 May 2019 12:17:04 -0400
Subject: [R] linear model contrast in R
In-Reply-To: <CAAjnpdhe15mK0xT4JE29=UBtp7nMMhOxjj04xGDtH_89co3Qyg@mail.gmail.com>
References: <CAAjnpdhe15mK0xT4JE29=UBtp7nMMhOxjj04xGDtH_89co3Qyg@mail.gmail.com>
Message-ID: <CAGx1TMDdm+giuY4hyGJzLqE+6VQKVx0=DfVFjmj5ypqw_vPhiQ@mail.gmail.com>

I think you might be looking for
?contrasts
to form the contrast matrix.

Rich

On Mon, May 13, 2019 at 7:31 AM Witold E Wolski <wewolski at gmail.com> wrote:
>
> I am looking for a function to compute contrasts with a interface
> similar to that of
>
> lmerTest::contest
> multcomp::glht
>
> i.e. taking the model and a contrast vector or matrix as an argument,
> but for linear models, and without the multiple testing adjusted made
> by multcomp::glht.
>
> Thank you
>
>
> --
> Witold Eryk Wolski
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Mon May 13 18:33:06 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Mon, 13 May 2019 17:33:06 +0100
Subject: [R] Error message when adding drift for Arima model
In-Reply-To: <CAFH+Q7xtNB3Tpb98QA_8k4EVi6w9Au4nF8Wc25SAn0zp1wy4Hg@mail.gmail.com>
References: <CAFH+Q7wCOKtwZBnc7a9NA+vxHpzUFrYuFQBdB_bGnNSGBGNe_A@mail.gmail.com>
 <CAGxFJbTXDv6FKdcBvpsHijj_qQ99B2CjXRHSd3GdfGO6u2Pt5A@mail.gmail.com>
 <CAFH+Q7z6QfhMUgOA7_OxKHtqadsJioJQVHD0Dhp+aw8GHaOVtw@mail.gmail.com>
 <CAGxFJbR0bb2y4jUFEH3Ov85UJKxRMVc+QFRS7UczRuoM-LzNUw@mail.gmail.com>
 <811a30b9-1617-d0a0-9eab-0f29618f8c63@sapo.pt>
 <5cf42fce-0082-7000-85f7-953de601e310@sapo.pt>
 <CAFH+Q7xtNB3Tpb98QA_8k4EVi6w9Au4nF8Wc25SAn0zp1wy4Hg@mail.gmail.com>
Message-ID: <c9f3d98f-65c1-abe8-a6f4-8b7ce419d0a2@sapo.pt>

Hello,

Sorry for the late reply.
Inline.

?s 17:54 de 10/05/19, Michael Howell escreveu:
> Rui,
> I'm still new to ARIMA forecasting but I examined the PACF and saw 
> significant correlation at lag 2. 

You saw a PACF with a significant correlation at lag 2 but not at lag 1. 
When this happens, it many times means that you shouldn't consider the 
lag 2. In fact, it might mean that the process is nonlinear.
And the ACF shows an insignificant lag 1.

Try

ords <- list(c(1, 0, 0), c(2, 0, 0), c(0, 0, 1))
fit_list <- lapply(ords, function(o)
   Arima(tsdata, order = o, include.drift = TRUE))
sapply(fit_list, AIC)
sapply(fit_list, BIC)


Which gives the minimum AIC? And BIC?
These are not perfect and automated model selection can have problems, 
but it's not unreasonable to compare them.

I believe this is off-topic for R-Help, since it's a question about 
statistics and nonlinear time series is a really, really broad field to 
be discussed here. Try to find local help on this.

Hope this helps,

Rui Barradas

The ACF showed a more gradual decline
> which seemed to indicate it was Autoregressive. That should mean it's a 
> AR(2) process right?
> 
> image.png
> **//___^
> Regards,
> Michael Howell
> 
> 
> On Fri, May 10, 2019 at 12:51 AM Rui Barradas <ruipbarradas at sapo.pt 
> <mailto:ruipbarradas at sapo.pt>> wrote:
> 
>     Why not
> 
>     Arima(tsdata, c(0, 0, 1), include.drift = TRUE)
> 
>     ?
> 
>     Why do you say it should be an AR(2) model?
> 
>     Hope this helps,
> 
>     Rui Barradas
> 
>     ?s 06:43 de 10/05/19, Rui Barradas escreveu:
>      > Hello,
>      >
>      > This is just a typo, in R logical values ("true) are not character
>      > strings. You must pass FALSE (the default, can be omited) or TRUE.
>      >
>      > fitdata? <-? Arima(tsdata, c(2, 0, 0), include.drift = TRUE)
>      >
>      >
>      >? From the help page ?logical
>      >
>      > Details
>      >
>      > TRUE and FALSE are reserved words denoting logical constants in
>     the R
>      > language, whereas T and F are global variables whose initial
>     values set
>      > to these. All four are logical(1) vectors.
>      >
>      > Hope this helps,
>      >
>      > Rui Barradas
>      >
>      > ?s 00:26 de 10/05/19, Bert Gunter escreveu:
>      >> In future, always cc the list (unless it's personal,which this
>     isn't). I
>      >> have done so here. As I am largely ignorant on the subject
>     matter, others
>      >> will have to help, which is why you should cc the list.
>      >>
>      >> Cheers,
>      >> Bert Gunter
>      >>
>      >> "The trouble with having an open mind is that people keep coming
>     along
>      >> and
>      >> sticking things into it."
>      >> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>      >>
>      >>
>      >> On Thu, May 9, 2019 at 3:49 PM Michael Howell
>     <mchowell2 at gmail.com <mailto:mchowell2 at gmail.com>>
>      >> wrote:
>      >>
>      >>> I apologize for that. The Arima() function that I'm trying to
>     use comes
>      >>> from the forecast package. I created a time series object using
>     the
>      >>> above
>      >>> 24 observations. The initial model I created doesn't seem to
>     perform so
>      >>> well so I thought a drift term might fit the data better. I
>     used the
>      >>> following code to create the time series object:
>      >>>
>      >>> tsdata<- ts(data, start = c(1,1), end = c(24,1), frequency = 1)
>      >>>
>      >>>
>      >>> Where* data* is the dataframe that contains the initial 24
>     observations.
>      >>> I then used the following code to try to create the model:
>      >>>
>      >>> fitdata? <-? Arima(tsdata,c(2,0,0),include.drift="true")
>      >>>>
>      >>>
>      >>> After doing this I obtained the following error message:
>      >>>
>      >>> Error in (order[2] + seasonal$order[2]) > 1 & include.drift:
>     operations
>      >>>> are possible only for numeric, logical or complex types
>      >>>> Traceback:
>      >>>>
>      >>>> 1. Arima(tsdata, c(2, 0, 0), include.drift = "true")
>      >>>
>      >>>
>      >>> ? I hope this is more clear.
>      >>>
>      >>> On Thu, May 9, 2019 at 4:39 PM Bert Gunter
>     <bgunter.4567 at gmail.com <mailto:bgunter.4567 at gmail.com>>
>      >>> wrote:
>      >>>
>      >>>> Please start by reading and following the posting guide linked
>     at the
>      >>>> bottom of this email. In particular:
>      >>>>
>      >>>> 1) Post in **plain text** on this plain text list so we don't
>     get the
>      >>>> mangled html of your post.
>      >>>>
>      >>>> 2) Tell us what package Arima() is in.
>      >>>>
>      >>>> Cheers,
>      >>>> Bert Gunter
>      >>>>
>      >>>>
>      >>>>
>      >>>>
>      >>>> On Thu, May 9, 2019 at 2:27 PM Michael Howell
>     <mchowell2 at gmail.com <mailto:mchowell2 at gmail.com>>
>      >>>> wrote:
>      >>>>
>      >>>>> Hello everyone,
>      >>>>> So this is my first post to this list, I'm trying to fit an
>     Arima
>      >>>>> (2,0,0)
>      >>>>> model and I think a drift term would help but I'm getting an
>     error
>      >>>>> term
>      >>>>> when I'm trying to include it. Here is my data:
>      >>>>>
>      >>>>> -6.732172338
>      >>>>> -2.868884273
>      >>>>> -5.371585089
>      >>>>> -6.512740463
>      >>>>> -4.171062657
>      >>>>> -5.738499071
>      >>>>> -3.343947176
>      >>>>> -1.944879508
>      >>>>> -5.464109272
>      >>>>> -3.189183392
>      >>>>> -3.684700232
>      >>>>> -2.168303451
>      >>>>> -2.329837082
>      >>>>> -0.761979236
>      >>>>> -2.189025304
>      >>>>> 1.094238807
>      >>>>> -4.812300745
>      >>>>> 0.784198777
>      >>>>> -1.567075922
>      >>>>> 0.143963653
>      >>>>> 1.131119051
>      >>>>> 2.899746353
>      >>>>> -0.498719993
>      >>>>> 3.121623505 I created a time series object with 24 annual
>      >>>>> observations. I
>      >>>>> didn't include dates because there isn't an observation for
>     every
>      >>>>> year.
>      >>>>>
>      >>>>> tsdata<-ts(read.csv("...\\Pre2001LaunchDateTraining.csv"),
>     start =
>      >>>>> c(1,1),
>      >>>>> end = c(24,1), frequency = 1) I then created a time series
>     object
>      >>>>> using
>      >>>>> the
>      >>>>> Arima() function. fitdata <-
>     Arima(tsdata,c(2,0,0),include.drift =
>      >>>>> "true")
>      >>>>> After executing I get this error: Error in (order[2] +
>      >>>>> seasonal$order[2]) >
>      >>>>> 1 & include.drift: operations are possible only for numeric,
>      >>>>> logical or
>      >>>>> complex types Traceback: 1. Arima(tsdata, c(2, 0, 0),
>     include.drift =
>      >>>>> "true")
>      >>>>> Any help would be greatly appreciated!
>      >>>>>
>      >>>>> ???????? [[alternative HTML version deleted]]
>      >>>>>
>      >>>>> ______________________________________________
>      >>>>> R-help at r-project.org <mailto:R-help at r-project.org> mailing
>     list -- To UNSUBSCRIBE and more, see
>      >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>      >>>>> PLEASE do read the posting guide
>      >>>>> http://www.R-project.org/posting-guide.html
>      >>>>> and provide commented, minimal, self-contained, reproducible
>     code.
>      >>>>>
>      >>>>
>      >>
>      >> ????[[alternative HTML version deleted]]
>      >>
>      >> ______________________________________________
>      >> R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>     -- To UNSUBSCRIBE and more, see
>      >> https://stat.ethz.ch/mailman/listinfo/r-help
>      >> PLEASE do read the posting guide
>      >> http://www.R-project.org/posting-guide.html
>      >> and provide commented, minimal, self-contained, reproducible code.
>      >>
>      >
>      > ______________________________________________
>      > R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>     -- To UNSUBSCRIBE and more, see
>      > https://stat.ethz.ch/mailman/listinfo/r-help
>      > PLEASE do read the posting guide
>      > http://www.R-project.org/posting-guide.html
>      > and provide commented, minimal, self-contained, reproducible code.
>


From mchowe||2 @end|ng |rom gm@||@com  Mon May 13 20:35:46 2019
From: mchowe||2 @end|ng |rom gm@||@com (Michael Howell)
Date: Mon, 13 May 2019 13:35:46 -0500
Subject: [R] Error message when adding drift for Arima model
In-Reply-To: <c9f3d98f-65c1-abe8-a6f4-8b7ce419d0a2@sapo.pt>
References: <CAFH+Q7wCOKtwZBnc7a9NA+vxHpzUFrYuFQBdB_bGnNSGBGNe_A@mail.gmail.com>
 <CAGxFJbTXDv6FKdcBvpsHijj_qQ99B2CjXRHSd3GdfGO6u2Pt5A@mail.gmail.com>
 <CAFH+Q7z6QfhMUgOA7_OxKHtqadsJioJQVHD0Dhp+aw8GHaOVtw@mail.gmail.com>
 <CAGxFJbR0bb2y4jUFEH3Ov85UJKxRMVc+QFRS7UczRuoM-LzNUw@mail.gmail.com>
 <811a30b9-1617-d0a0-9eab-0f29618f8c63@sapo.pt>
 <5cf42fce-0082-7000-85f7-953de601e310@sapo.pt>
 <CAFH+Q7xtNB3Tpb98QA_8k4EVi6w9Au4nF8Wc25SAn0zp1wy4Hg@mail.gmail.com>
 <c9f3d98f-65c1-abe8-a6f4-8b7ce419d0a2@sapo.pt>
Message-ID: <CAFH+Q7w0s9gXqf_Q6+y3wKD4mk20ukR=+5krpYk4f_UAzJg09g@mail.gmail.com>

Yes that is a little off topic but I will look into it more. Thank you very
much for your help.

Michael

On Mon, May 13, 2019 at 11:33 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> Sorry for the late reply.
> Inline.
>
> ?s 17:54 de 10/05/19, Michael Howell escreveu:
> > Rui,
> > I'm still new to ARIMA forecasting but I examined the PACF and saw
> > significant correlation at lag 2.
>
> You saw a PACF with a significant correlation at lag 2 but not at lag 1.
> When this happens, it many times means that you shouldn't consider the
> lag 2. In fact, it might mean that the process is nonlinear.
> And the ACF shows an insignificant lag 1.
>
> Try
>
> ords <- list(c(1, 0, 0), c(2, 0, 0), c(0, 0, 1))
> fit_list <- lapply(ords, function(o)
>    Arima(tsdata, order = o, include.drift = TRUE))
> sapply(fit_list, AIC)
> sapply(fit_list, BIC)
>
>
> Which gives the minimum AIC? And BIC?
> These are not perfect and automated model selection can have problems,
> but it's not unreasonable to compare them.
>
> I believe this is off-topic for R-Help, since it's a question about
> statistics and nonlinear time series is a really, really broad field to
> be discussed here. Try to find local help on this.
>
> Hope this helps,
>
> Rui Barradas
>
> The ACF showed a more gradual decline
> > which seemed to indicate it was Autoregressive. That should mean it's a
> > AR(2) process right?
> >
> > image.png
> > **//___^
> > Regards,
> > Michael Howell
> >
> >
> > On Fri, May 10, 2019 at 12:51 AM Rui Barradas <ruipbarradas at sapo.pt
> > <mailto:ruipbarradas at sapo.pt>> wrote:
> >
> >     Why not
> >
> >     Arima(tsdata, c(0, 0, 1), include.drift = TRUE)
> >
> >     ?
> >
> >     Why do you say it should be an AR(2) model?
> >
> >     Hope this helps,
> >
> >     Rui Barradas
> >
> >     ?s 06:43 de 10/05/19, Rui Barradas escreveu:
> >      > Hello,
> >      >
> >      > This is just a typo, in R logical values ("true) are not character
> >      > strings. You must pass FALSE (the default, can be omited) or TRUE.
> >      >
> >      > fitdata  <-  Arima(tsdata, c(2, 0, 0), include.drift = TRUE)
> >      >
> >      >
> >      >  From the help page ?logical
> >      >
> >      > Details
> >      >
> >      > TRUE and FALSE are reserved words denoting logical constants in
> >     the R
> >      > language, whereas T and F are global variables whose initial
> >     values set
> >      > to these. All four are logical(1) vectors.
> >      >
> >      > Hope this helps,
> >      >
> >      > Rui Barradas
> >      >
> >      > ?s 00:26 de 10/05/19, Bert Gunter escreveu:
> >      >> In future, always cc the list (unless it's personal,which this
> >     isn't). I
> >      >> have done so here. As I am largely ignorant on the subject
> >     matter, others
> >      >> will have to help, which is why you should cc the list.
> >      >>
> >      >> Cheers,
> >      >> Bert Gunter
> >      >>
> >      >> "The trouble with having an open mind is that people keep coming
> >     along
> >      >> and
> >      >> sticking things into it."
> >      >> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip
> )
> >      >>
> >      >>
> >      >> On Thu, May 9, 2019 at 3:49 PM Michael Howell
> >     <mchowell2 at gmail.com <mailto:mchowell2 at gmail.com>>
> >      >> wrote:
> >      >>
> >      >>> I apologize for that. The Arima() function that I'm trying to
> >     use comes
> >      >>> from the forecast package. I created a time series object using
> >     the
> >      >>> above
> >      >>> 24 observations. The initial model I created doesn't seem to
> >     perform so
> >      >>> well so I thought a drift term might fit the data better. I
> >     used the
> >      >>> following code to create the time series object:
> >      >>>
> >      >>> tsdata<- ts(data, start = c(1,1), end = c(24,1), frequency = 1)
> >      >>>
> >      >>>
> >      >>> Where* data* is the dataframe that contains the initial 24
> >     observations.
> >      >>> I then used the following code to try to create the model:
> >      >>>
> >      >>> fitdata  <-  Arima(tsdata,c(2,0,0),include.drift="true")
> >      >>>>
> >      >>>
> >      >>> After doing this I obtained the following error message:
> >      >>>
> >      >>> Error in (order[2] + seasonal$order[2]) > 1 & include.drift:
> >     operations
> >      >>>> are possible only for numeric, logical or complex types
> >      >>>> Traceback:
> >      >>>>
> >      >>>> 1. Arima(tsdata, c(2, 0, 0), include.drift = "true")
> >      >>>
> >      >>>
> >      >>>   I hope this is more clear.
> >      >>>
> >      >>> On Thu, May 9, 2019 at 4:39 PM Bert Gunter
> >     <bgunter.4567 at gmail.com <mailto:bgunter.4567 at gmail.com>>
> >      >>> wrote:
> >      >>>
> >      >>>> Please start by reading and following the posting guide linked
> >     at the
> >      >>>> bottom of this email. In particular:
> >      >>>>
> >      >>>> 1) Post in **plain text** on this plain text list so we don't
> >     get the
> >      >>>> mangled html of your post.
> >      >>>>
> >      >>>> 2) Tell us what package Arima() is in.
> >      >>>>
> >      >>>> Cheers,
> >      >>>> Bert Gunter
> >      >>>>
> >      >>>>
> >      >>>>
> >      >>>>
> >      >>>> On Thu, May 9, 2019 at 2:27 PM Michael Howell
> >     <mchowell2 at gmail.com <mailto:mchowell2 at gmail.com>>
> >      >>>> wrote:
> >      >>>>
> >      >>>>> Hello everyone,
> >      >>>>> So this is my first post to this list, I'm trying to fit an
> >     Arima
> >      >>>>> (2,0,0)
> >      >>>>> model and I think a drift term would help but I'm getting an
> >     error
> >      >>>>> term
> >      >>>>> when I'm trying to include it. Here is my data:
> >      >>>>>
> >      >>>>> -6.732172338
> >      >>>>> -2.868884273
> >      >>>>> -5.371585089
> >      >>>>> -6.512740463
> >      >>>>> -4.171062657
> >      >>>>> -5.738499071
> >      >>>>> -3.343947176
> >      >>>>> -1.944879508
> >      >>>>> -5.464109272
> >      >>>>> -3.189183392
> >      >>>>> -3.684700232
> >      >>>>> -2.168303451
> >      >>>>> -2.329837082
> >      >>>>> -0.761979236
> >      >>>>> -2.189025304
> >      >>>>> 1.094238807
> >      >>>>> -4.812300745
> >      >>>>> 0.784198777
> >      >>>>> -1.567075922
> >      >>>>> 0.143963653
> >      >>>>> 1.131119051
> >      >>>>> 2.899746353
> >      >>>>> -0.498719993
> >      >>>>> 3.121623505 I created a time series object with 24 annual
> >      >>>>> observations. I
> >      >>>>> didn't include dates because there isn't an observation for
> >     every
> >      >>>>> year.
> >      >>>>>
> >      >>>>> tsdata<-ts(read.csv("...\\Pre2001LaunchDateTraining.csv"),
> >     start =
> >      >>>>> c(1,1),
> >      >>>>> end = c(24,1), frequency = 1) I then created a time series
> >     object
> >      >>>>> using
> >      >>>>> the
> >      >>>>> Arima() function. fitdata <-
> >     Arima(tsdata,c(2,0,0),include.drift =
> >      >>>>> "true")
> >      >>>>> After executing I get this error: Error in (order[2] +
> >      >>>>> seasonal$order[2]) >
> >      >>>>> 1 & include.drift: operations are possible only for numeric,
> >      >>>>> logical or
> >      >>>>> complex types Traceback: 1. Arima(tsdata, c(2, 0, 0),
> >     include.drift =
> >      >>>>> "true")
> >      >>>>> Any help would be greatly appreciated!
> >      >>>>>
> >      >>>>>          [[alternative HTML version deleted]]
> >      >>>>>
> >      >>>>> ______________________________________________
> >      >>>>> R-help at r-project.org <mailto:R-help at r-project.org> mailing
> >     list -- To UNSUBSCRIBE and more, see
> >      >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >      >>>>> PLEASE do read the posting guide
> >      >>>>> http://www.R-project.org/posting-guide.html
> >      >>>>> and provide commented, minimal, self-contained, reproducible
> >     code.
> >      >>>>>
> >      >>>>
> >      >>
> >      >>     [[alternative HTML version deleted]]
> >      >>
> >      >> ______________________________________________
> >      >> R-help at r-project.org <mailto:R-help at r-project.org> mailing list
> >     -- To UNSUBSCRIBE and more, see
> >      >> https://stat.ethz.ch/mailman/listinfo/r-help
> >      >> PLEASE do read the posting guide
> >      >> http://www.R-project.org/posting-guide.html
> >      >> and provide commented, minimal, self-contained, reproducible
> code.
> >      >>
> >      >
> >      > ______________________________________________
> >      > R-help at r-project.org <mailto:R-help at r-project.org> mailing list
> >     -- To UNSUBSCRIBE and more, see
> >      > https://stat.ethz.ch/mailman/listinfo/r-help
> >      > PLEASE do read the posting guide
> >      > http://www.R-project.org/posting-guide.html
> >      > and provide commented, minimal, self-contained, reproducible code.
> >
>

	[[alternative HTML version deleted]]


From n|co|@@rugg|ero@unt @end|ng |rom gm@||@com  Mon May 13 21:56:35 2019
From: n|co|@@rugg|ero@unt @end|ng |rom gm@||@com (Nicola Ruggiero)
Date: Mon, 13 May 2019 12:56:35 -0700
Subject: [R] converting zipcodes to latitude/longitude
Message-ID: <CAEneZ8kJjfb4zRu44Xnj677hKkX1GBSyOZfxZRhYM-MxdJ-SnA@mail.gmail.com>

Hello everyone,

I've downloaded Jeffrey Breen's R package "zipcode," which has the
latitude and longitude for all of the US zip codes. So, this is a
data.frame with 43,191 observations. That's one data frame in my
environment.

Then, I have another data.frame with over 100,000 observations that
look like this:

waltham, Massachusetts 02451
Columbia, SC 29209

Wheat Ridge , Colorado 80033
Charlottesville, Virginia 22902
Fairbanks, AK 99709
Montpelier, VT 05602
Dobbs Ferry, New York 10522

Henderson , Kentucky 42420

The spaces represent absences in the column. Regardless,
I need to figure out how to write a code that would, presumably, match
the zipcodes and produce another column to the data frame with the
latitude and longitude. So, for example, the code would recognize
02451 above, and, in the the column next to it, the code would write
42.3765? N, 71.2356? W in the column next to it, since that's the
latitude and longitude for Waltham, Massachusetts.

Any idea of how to begin a code that would perform such an operation?

Again, I have a data.frame with the zipcodes linked to the the
latitudes and longitudes, on the one hand, and another data.frame with
only zipcodes (and some holes). I need to produce the corresponding
latitude/longitudes in the latter data.frame.

Nicola


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Tue May 14 04:01:03 2019
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Tue, 14 May 2019 02:01:03 +0000
Subject: [R] Trying to understand the magic of lm (Still trying)
In-Reply-To: <9ecfbe$bm3sbg@ironport10.mayo.edu>
References: <mailman.354642.1.1557568801.34439.r-help@r-project.org>,
 <9ecfbe$bm3sbg@ironport10.mayo.edu>
Message-ID: <BN7PR03MB3730F14F8E211997219B2F3EE2080@BN7PR03MB3730.namprd03.prod.outlook.com>

Terry ,


Thank you. Many years ago I took a course you taught in which you explained how to conduct survival analyses using S. The course was very useful, as was the email you sent me today. If you find a place where you can store your lecture notes, please send me the URL.


I believe that there is a great need for someone to explain not just how to write a package, but generally how to write a function that checks the parameters passed to it, that uses the parameters in a manner that allows the output of the function to produce output that informs the function users of the call to the function, etc. While these steps are needed when one writes a package, they should be taught as a matter of good coding practice when anyone writes a function that will be used more than once. Many years ago when I was a mainframe system programmer, it was de rigueur that one learned (and used) certain standards about saving registers at the beginning of a function and restoring them at the end of the function. The same should be true for all R functions; certain standardized, well described steps should be considered a part of writing any function. The problem, at least from my perspective, is that there is no commonly recognized document that explains the steps clearly.


Thank you,

John


John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)



________________________________
From: Therneau, Terry M., Ph.D. <therneau at mayo.edu>
Sent: Monday, May 13, 2019 9:29 AM
To: r-help at r-project.org; Sorkin, John
Subject: Re: [R] Trying to understand the magic of lm (Still trying)

John,

 The text below is cut out of a "how to write a package" course I gave at the R conference in Vanderbilt.   I need to find a home for the course notes, because it had a lot of tidbits that are not well explained in the R documentation.
Terry T.

----

Model frames:
One of the first tasks of any modeling routine is to construct a special data frame containing the covariates that will be used, via a call to the model.frame function. The code to do this is found in many routines, and can be a little opaque on first view. The obvious code would be
\begin{verbatim}
coxph <- function(formula, data, weights, subset, na.action,
        init, control, ties= c("efron", "breslow", "exact"),
        singular.ok =TRUE, robust=FALSE,
        model=FALSE, x=FALSE, y=TRUE,  tt, method=ties, ...) {

     mf <- model.frame(formula, data, subset, weights, na.action)
\end{verbatim}
since those are the coxph arguments that are passed forward to the model.frame routine.  However, this simple approach will fail with a ``not found'' error message if any of the data, subset, weights, etc. arguments are missing. Programs have to take the slightly more complicated approach of constructing a call.
\begin{verbatim}
Call <- match.call()
indx <- match(c("formula", "data", "weights", "subset", "na.action"),
                  names(Call), nomatch=0)
if (indx[1] ==0) stop("A formula argument is required")
temp <- Call[c(1,indx)]  # only keep the arguments we wanted
temp[[1]] <- as.name('model.frame')  # change the function called
mf <- eval(temp, parent.frame())

Y <- model.response(mf)
etc.
\end{verbatim}

We start with a copy of the call to the program, which we want to save anyway as documentation in the output object. Then subscripting is used to extract only the portions of the call that we want, saving the result in a temporary. This is based on the fact that a call object can be viewed as a list whose first element is the name of the function to call, followed by the arguments to the call. Note the use of \code{nomatch=0}; if any arguments on the list are missing they will then be missing in \code{temp}, without generating an error message. The \mycode{temp} variable will contain a object of type ``call'', which is an unevaluated call to a routine.  Finally, the name of the function to be called is changed from ``coxph'' to ``model.frame'' and the call is evaluated.  In many of the core routines the result is stored in a variable ``m''.  This is a horribly short and non-descriptive name. (The above used mf which isn't a much better.)  Many routines also use ``m'' for the temporary variable leading to \code{m <- eval(m, parent.frame())}, but I think that is unnecessarily confusing.

The list of names in the match call will include all arguments that should be evaluated within context of the named dataframe. This can include more than the list above, the survfit routine for instance has an optional argument ``id'' that names an identifying variable (several rows of the data may represent a single subject), and this is included along with ``formula'' etc in the list of choices in the match function.  The order of names in the list makes no difference.  The id is later retrieved with \code{model.extract(m, 'id')}, which will be NULL if the argument was not supplied. At the time that coxph was written I had not caught on to this fact and thought that all variables that came from a data frame had to be represented in the formula somehow, thus the use of \code{cluster(id)} as part of the formula, in order to denote a grouping variable.

On 5/11/19 5:00 AM, r-help-request at r-project.org<mailto:r-help-request at r-project.org> wrote:

A number of people have helped me in my mission to understand how lm (and other fucntions) are able to pass a dataframe and then refer to a specific column in the dataframe. I thank everyone who has responded. I now know a bit about deparse(substitute(xx)), but I still don't fully understand how it works. The program below attempts to print a column of a dataframe from a function whose parameters include the dataframe (df) and the column requested (col). The program works fine until the last print statement were I receive an error,  Error in `[.data.frame`(df, , col) : object 'y' not found . I hope someone can explain to me (1) why my code does not work, and (2) what I can do to fix it.



	[[alternative HTML version deleted]]


From yue||7 @end|ng |rom 126@com  Tue May 14 04:31:22 2019
From: yue||7 @end|ng |rom 126@com (yueli)
Date: Tue, 14 May 2019 10:31:22 +0800 (CST)
Subject: [R] install pcre
Message-ID: <59dee18d.1240.16ab42e1ebe.Coremail.yueli7@126.com>

Hello,

I am trying to install R.

Thanks in advance for any help!

Yue


checking for pcre.h... yes
checking pcre/pcre.h usability... no
checking pcre/pcre.h presence... no
checking for pcre/pcre.h... no
checking if PCRE version >= 8.20, < 10.0 and has UTF-8 support... no
checking whether PCRE support suffices... configure: error: pcre >= 8.20 library and headers are required

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue May 14 04:41:58 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 13 May 2019 20:41:58 -0600
Subject: [R] install pcre
In-Reply-To: <59dee18d.1240.16ab42e1ebe.Coremail.yueli7@126.com>
References: <59dee18d.1240.16ab42e1ebe.Coremail.yueli7@126.com>
Message-ID: <894179C1-4254-4BA7-AE47-6AF0E41561D2@dcn.davis.ca.us>

Looks like you don't have all of the dev tools installed on your unnamed OS. I highly recommend Googling for how to install R in your OS.

Please read the Posting Guide... Discussions about compiling R should be in R-devel. Most people install binaries which would be relevant here.

On May 13, 2019 8:31:22 PM MDT, yueli <yueli7 at 126.com> wrote:
>Hello,
>
>I am trying to install R.
>
>Thanks in advance for any help!
>
>Yue
>
>
>checking for pcre.h... yes
>checking pcre/pcre.h usability... no
>checking pcre/pcre.h presence... no
>checking for pcre/pcre.h... no
>checking if PCRE version >= 8.20, < 10.0 and has UTF-8 support... no
>checking whether PCRE support suffices... configure: error: pcre >=
>8.20 library and headers are required
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From drj|m|emon @end|ng |rom gm@||@com  Tue May 14 06:09:03 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Tue, 14 May 2019 14:09:03 +1000
Subject: [R] converting zipcodes to latitude/longitude
In-Reply-To: <CAEneZ8kJjfb4zRu44Xnj677hKkX1GBSyOZfxZRhYM-MxdJ-SnA@mail.gmail.com>
References: <CAEneZ8kJjfb4zRu44Xnj677hKkX1GBSyOZfxZRhYM-MxdJ-SnA@mail.gmail.com>
Message-ID: <CA+8X3fUzS+csW_e2yFSTt0133zKPke5+UPC6p9-GTtxtCk3osg@mail.gmail.com>

Hi Nicola,
Getting the blank rows will be a bit more difficult and I don't see
why they should be in the final data frame, so:

townzip<-read.table(text="waltham, Massachusetts 02451
Columbia, SC 29209

Wheat Ridge , Colorado 80033
Charlottesville, Virginia 22902
Fairbanks, AK 99709
Montpelier, VT 05602
Dobbs Ferry, New York 10522

Henderson , Kentucky 42420",
sep="\t",stringsAsFactors=FALSE)
zip_split<-function(x) {
 commasplit<-unlist(strsplit(x,","))
 state<-trimws(gsub("[[:digit:]]","",commasplit[2]))
 zip<-trimws(gsub("[[:alpha:]]","",commasplit[2]))
 return(c(commasplit[1],state,zip))
}
townzipsplit<-as.data.frame(t(sapply(townzip$V1,zip_split)))
rownames(townzipsplit)<-NULL
names(townzipsplit)<-c("town","state","zip")
townzipsplit$latlon<-NA
# I don't know the name of the zipcode column in the "zipcode" data frame
newzipdf<-merge(townzipsplit,zipcodedf,by.x="zip",by.y="zip")

Jim

On Tue, May 14, 2019 at 5:57 AM Nicola Ruggiero
<nicola.ruggiero.unt at gmail.com> wrote:
>
> Hello everyone,
>
> I've downloaded Jeffrey Breen's R package "zipcode," which has the
> latitude and longitude for all of the US zip codes. So, this is a
> data.frame with 43,191 observations. That's one data frame in my
> environment.
>
> Then, I have another data.frame with over 100,000 observations that
> look like this:
>
> waltham, Massachusetts 02451
> Columbia, SC 29209
>
> Wheat Ridge , Colorado 80033
> Charlottesville, Virginia 22902
> Fairbanks, AK 99709
> Montpelier, VT 05602
> Dobbs Ferry, New York 10522
>
> Henderson , Kentucky 42420
>
> The spaces represent absences in the column. Regardless,
> I need to figure out how to write a code that would, presumably, match
> the zipcodes and produce another column to the data frame with the
> latitude and longitude. So, for example, the code would recognize
> 02451 above, and, in the the column next to it, the code would write
> 42.3765? N, 71.2356? W in the column next to it, since that's the
> latitude and longitude for Waltham, Massachusetts.
>
> Any idea of how to begin a code that would perform such an operation?
>
> Again, I have a data.frame with the zipcodes linked to the the
> latitudes and longitudes, on the one hand, and another data.frame with
> only zipcodes (and some holes). I need to produce the corresponding
> latitude/longitudes in the latter data.frame.
>
> Nicola
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @mun|r1 @end|ng |rom b|ngh@mton@edu  Mon May 13 21:10:09 2019
From: @mun|r1 @end|ng |rom b|ngh@mton@edu (Syed Rashid Munir)
Date: Mon, 13 May 2019 15:10:09 -0400
Subject: [R] Plot prediction for Heckman 2-step (sampleSelection) models
 after predict.selection
Message-ID: <ca72e7ed-a3ce-916d-e623-0a82cd36802d@binghamton.edu>

Hi everyone,

I'm fairly new to R and I've run into a problem that I can't find a 
solution for.

I'm trying to plot the predicted values of a dependent variable based on 
specific values of one independent variable after running a Heckit 
(Heckman 2-step) model from the sampleSelection package.

The general model looks like this:

|Mod1 <- Heckit(selection = binaryindicator ~ x1 + z, outcome = y ~ x1, 
df, method) |

where, the data structure is as follows:

|z x1 y binaryindicator 0.5 4 300 1 0.1 8 400 1 0.2 10 500 1 0.2 18 NA 0 
0.4 20 50 1 0.3 30 1000 1 |

After pouring over documentation for sampleSelection, I can only extract 
predictions from the predict.selection function and get the 
corresponding residuals. However, this method doesn't let me specify 
predictions for one variable (say x1) and resulting predictions are from 
the entire model.

Therefore, I am looking for a way (canned function or manual) to plot 
the prediction on y from x1 with confidence intervals. All attempts to 
do so (using plot, cplot, ggpredict, effects_plot) have failed. All of 
the above functions work with linear models, but not a selection model.

I'd ideally want to also specify which equation (outcome or selection) I 
want the prediction from, but that has also eluded me, as I can't get 
any predictions to begin with. Is there a way to achieve this? In Stata, 
this can be done with the margins command, but the equivalent margins 
package in R doesn't work for sampleSelection models (for me so far). 
Any and all help would be much appreciated!


Regards,
Rashid
Poli Sci Student, SUNY Binghamton

	[[alternative HTML version deleted]]


From ju@ngomezdu@@o @end|ng |rom gm@||@com  Mon May 13 22:16:43 2019
From: ju@ngomezdu@@o @end|ng |rom gm@||@com (Juan Gomez)
Date: Mon, 13 May 2019 22:16:43 +0200
Subject: [R] Fwd:  error in duplicated() man page
In-Reply-To: <CAN+qQ08m_eD5-CFiNdu+eouZK1SKo2Tros-SDp3ibU4ApE+Q0w@mail.gmail.com>
References: <CAN+qQ0_98V=Myb6UKnJDpBh8sU2-a7BrmF8tbc_SAUeOd-wRZA@mail.gmail.com>
 <CAGxFJbQJ6qOeRCZFoDASFB0HqUBud=codcZ4YNZvwHQp2s=JVA@mail.gmail.com>
 <23764.19726.176465.757858@stat.math.ethz.ch>
 <CAGxFJbRb+JdS03u1U9HcZu2-8XLRMxPn7K_taw+iSoZFrqsx0g@mail.gmail.com>
 <CAN+qQ08m_eD5-CFiNdu+eouZK1SKo2Tros-SDp3ibU4ApE+Q0w@mail.gmail.com>
Message-ID: <CAN+qQ09fjnbU1BAHenmjvtr_pkL2a5HWFCC74gm5NmdUngjzSA@mail.gmail.com>

---------- Forwarded message ---------
De: Juan Gomez <juangomezduaso at gmail.com>
Date: vie., 10 may. 2019 a las 11:12
Subject: Re: [R] error in duplicated() man page
To: Bert Gunter <bgunter.4567 at gmail.com>


Coming back to my example, which I think illustrates my point, we have
3x2 groups (as specified in MARGIN dimensions) with one cell in each
group. The function looks for an earlier group having the same value.
It does so in column major order (IMHO) and so, it gives a FALSE for
the  2 at coordinates (2,1) and gives TRUE for the one at coordinates
(1,2).
In row major order it would find  the "2" in the first row first, and
the "2" in the second row will be considered as duplicated

If the documentation were right, I think the call would be:
duplicatedRowMajor(array(c(1,2,3,2,5,6),c(3,2)), MARGIN=1:2)
      [,1]  [,2]
[1,] FALSE  FALSE
[2,] TRUE    FALSE
[3,] FALSE  FALSE

El jue., 9 may. 2019 a las 19:15, Bert Gunter
(<bgunter.4567 at gmail.com>) escribi?:
>
> Thanks, Martin. I missed the duplication. My apology -- old age is asserting it's presence.
>
> Then my response is: I think the documentation is correct as written:
>
> > a <- matrix(rep(1:3,2), nr=3)
> > a
>      [,1] [,2]
> [1,]    1    1
> [2,]    2    2
> [3,]    3    3
>
> > duplicated(a)
> [1] FALSE FALSE FALSE
> > ## Note: Row major by default !
>
> > duplicated(a, MAR = 2)
> [1] FALSE  TRUE
>
> Again, apologies for my silly error.
>
> Cheers,
> Bert
>
>
>
> On Thu, May 9, 2019 at 8:56 AM Martin Maechler <maechler at stat.math.ethz.ch> wrote:
>>
>> >>>>> Bert Gunter
>> >>>>>     on Thu, 9 May 2019 08:46:15 -0700 writes:
>>
>>     > Juan:
>>     > No, I think there may be a bug:
>>
>>     >> duplicated(array(c(1,2,3,2,5,6),c(3,2)), MARGIN=1:2)
>>     > [,1]  [,2]
>>     > [1,] FALSE  TRUE
>>     > [2,] FALSE FALSE
>>     > [3,] FALSE FALSE
>>     > ## This is wrong
>>
>>     > ## But if we first define the array...
>>     >> a <- array(c(1,2,3,4,5,6), c(3,2))
>>     >> duplicated(a, MARGIN = 1:2)
>>     > [,1]  [,2]
>>     > [1,] FALSE FALSE
>>     > [2,] FALSE FALSE
>>     > [3,] FALSE FALSE
>>     > ## This is right
>>
>> Well, the two arrays are different:
>> The first has a '2' instead of a '4'
>> ((and this would not happen if you used 1:6  instead  ..))
>>
>>     > I'll wait a bit before filing a bug report so that any error I may be
>>     > making can be pointed out (note that my R version is NOT current, so I need
>>     > to update).
>>
>>     >> sessionInfo()
>>     > R version 3.5.2 (2018-12-20)
>>     > Platform: x86_64-apple-darwin15.6.0 (64-bit)
>>     > Running under: macOS Mojave 10.14.4
>>
>>     > Matrix products: default
>>     > BLAS:
>>     > /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
>>     > LAPACK:
>>     > /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
>>
>>     > locale:
>>     > [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>>
>>     > attached base packages:
>>     > [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>>     > loaded via a namespace (and not attached):
>>     > [1] compiler_3.5.2 tools_3.5.2
>>
>>     > Bert Gunter
>>
>>     > "The trouble with having an open mind is that people keep coming along and
>>     > sticking things into it."
>>     > -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>>     > On Thu, May 9, 2019 at 8:10 AM Juan Gomez <juangomezduaso at gmail.com> wrote:
>>
>>     >> I think there is an error in duplicated() help page when it states that:
>>     >> "The array method calculates for each element of the sub-array
>>     >> specified by MARGIN if the remaining dimensions are identical to those
>>     >> for an earlier (or later, when fromLast = TRUE) element (in row-major
>>     >> order). "
>>     >> Instead of:
>>     >> "... (in column-major order)"
>>     >>
>>     >> For instance:
>>     >> duplicated(array(c(1,2,3,2,5,6),c(3,2)), MARGIN=1:2)
>>     >> [,1]  [,2]
>>     >> [1,] FALSE  TRUE
>>     >> [2,] FALSE FALSE
>>     >> [3,] FALSE FALSE
>>     >> >
>>     >>
>>     >> ______________________________________________
>>     >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>     >> https://stat.ethz.ch/mailman/listinfo/r-help
>>     >> PLEASE do read the posting guide
>>     >> http://www.R-project.org/posting-guide.html
>>     >> and provide commented, minimal, self-contained, reproducible code.
>>     >>
>>
>>     > [[alternative HTML version deleted]]
>>
>>     > ______________________________________________
>>     > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>     > https://stat.ethz.ch/mailman/listinfo/r-help
>>     > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>     > and provide commented, minimal, self-contained, reproducible code.


From mchowe||2 @end|ng |rom gm@||@com  Tue May 14 18:51:20 2019
From: mchowe||2 @end|ng |rom gm@||@com (Michael Howell)
Date: Tue, 14 May 2019 11:51:20 -0500
Subject: [R] Different predictions with forecast::auto.arima()
Message-ID: <CAFH+Q7w+O--=Thz_4OUy0pbUF+qxp1JodwE1TDSo_=Ukwo4XCQ@mail.gmail.com>

Good morning,
I was asking a more statistics oriented question on another board and
someone demonstrated auto.arima() from the forecast package on my data. The
function returned a (2,1,0) model with drift. However when I used the same
function it returns a (1,1,0) model. There were no obvious differences in
the code. The only thing passed to it was the data. How might this happen?
The (2,1,0) model works better so I would like to be able to reproduce the
results.

Regards,
Michael Howell

	[[alternative HTML version deleted]]


From M@ri@@@BRATU m@iii@g oii ext@ec@europ@@eu  Tue May 14 16:17:32 2019
From: M@ri@@@BRATU m@iii@g oii ext@ec@europ@@eu (M@ri@@@BRATU m@iii@g oii ext@ec@europ@@eu)
Date: Tue, 14 May 2019 14:17:32 +0000
Subject: [R] How to publish a new package on CRAN
Message-ID: <65fa0f6d5de4498d93a7aa74fc05dbb8@ext.ec.europa.eu>

Hello to all. We are a group of developpers and want to know how we can publish a new package to CRAN website.


Thanks in advance for suggestions.


Regards,

  Marian

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Tue May 14 20:42:40 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Tue, 14 May 2019 19:42:40 +0100
Subject: [R] How to publish a new package on CRAN
In-Reply-To: <65fa0f6d5de4498d93a7aa74fc05dbb8@ext.ec.europa.eu>
References: <65fa0f6d5de4498d93a7aa74fc05dbb8@ext.ec.europa.eu>
Message-ID: <667aec8e-d6bc-7292-9218-286341d6a0b6@sapo.pt>

Hello,

There is a mailing list dedicated to package development. See

https://stat.ethz.ch/mailman/listinfo/r-package-devel


Hope this helps,

Rui Barradas

?s 15:17 de 14/05/19, Marian.BRATU at ext.ec.europa.eu escreveu:
> Hello to all. We are a group of developpers and want to know how we can publish a new package to CRAN website.
> 
> 
> Thanks in advance for suggestions.
> 
> 
> Regards,
> 
>    Marian
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch@dunc@n @end|ng |rom gm@||@com  Tue May 14 20:56:46 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Tue, 14 May 2019 14:56:46 -0400
Subject: [R] How to publish a new package on CRAN
In-Reply-To: <667aec8e-d6bc-7292-9218-286341d6a0b6@sapo.pt>
References: <65fa0f6d5de4498d93a7aa74fc05dbb8@ext.ec.europa.eu>
 <667aec8e-d6bc-7292-9218-286341d6a0b6@sapo.pt>
Message-ID: <7f36219b-c22b-e520-dd2a-f20e89711248@gmail.com>

On 14/05/2019 2:42 p.m., Rui Barradas wrote:
> Hello,
> 
> There is a mailing list dedicated to package development. See
> 
> https://stat.ethz.ch/mailman/listinfo/r-package-devel

There are also instructions to follow at

https://cran.r-project.org/web/packages/policies.html

Duncan Murdoch

> 
> 
> Hope this helps,
> 
> Rui Barradas
> 
> ?s 15:17 de 14/05/19, Marian.BRATU at ext.ec.europa.eu escreveu:
>> Hello to all. We are a group of developpers and want to know how we can publish a new package to CRAN website.
>>
>>
>> Thanks in advance for suggestions.
>>
>>
>> Regards,
>>
>>     Marian
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue May 14 20:57:21 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 14 May 2019 12:57:21 -0600
Subject: [R] Different predictions with forecast::auto.arima()
In-Reply-To: <CAFH+Q7w+O--=Thz_4OUy0pbUF+qxp1JodwE1TDSo_=Ukwo4XCQ@mail.gmail.com>
References: <CAFH+Q7w+O--=Thz_4OUy0pbUF+qxp1JodwE1TDSo_=Ukwo4XCQ@mail.gmail.com>
Message-ID: <6074A4D9-E7EF-47BB-9706-366789557282@dcn.davis.ca.us>

This is definitely a statistics question still so not on topic here... as changing the data is exactly the kind of thing that can have this effect.

On May 14, 2019 10:51:20 AM MDT, Michael Howell <mchowell2 at gmail.com> wrote:
>Good morning,
>I was asking a more statistics oriented question on another board and
>someone demonstrated auto.arima() from the forecast package on my data.
>The
>function returned a (2,1,0) model with drift. However when I used the
>same
>function it returns a (1,1,0) model. There were no obvious differences
>in
>the code. The only thing passed to it was the data. How might this
>happen?
>The (2,1,0) model works better so I would like to be able to reproduce
>the
>results.
>
>Regards,
>Michael Howell
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From mchowe||2 @end|ng |rom gm@||@com  Tue May 14 20:59:59 2019
From: mchowe||2 @end|ng |rom gm@||@com (Michael Howell)
Date: Tue, 14 May 2019 13:59:59 -0500
Subject: [R] Different predictions with forecast::auto.arima()
In-Reply-To: <6074A4D9-E7EF-47BB-9706-366789557282@dcn.davis.ca.us>
References: <CAFH+Q7w+O--=Thz_4OUy0pbUF+qxp1JodwE1TDSo_=Ukwo4XCQ@mail.gmail.com>
 <6074A4D9-E7EF-47BB-9706-366789557282@dcn.davis.ca.us>
Message-ID: <CAFH+Q7xgdBynyW2uuxcgRr1DDS5oyOoaJqMKscu_3omQza6ptw@mail.gmail.com>

Oh I see. Nevermind then.

On Tue, May 14, 2019 at 1:57 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> This is definitely a statistics question still so not on topic here... as
> changing the data is exactly the kind of thing that can have this effect.
>
> On May 14, 2019 10:51:20 AM MDT, Michael Howell <mchowell2 at gmail.com>
> wrote:
> >Good morning,
> >I was asking a more statistics oriented question on another board and
> >someone demonstrated auto.arima() from the forecast package on my data.
> >The
> >function returned a (2,1,0) model with drift. However when I used the
> >same
> >function it returns a (1,1,0) model. There were no obvious differences
> >in
> >the code. The only thing passed to it was the data. How might this
> >happen?
> >The (2,1,0) model works better so I would like to be able to reproduce
> >the
> >results.
> >
> >Regards,
> >Michael Howell
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From @purd|e@@ @end|ng |rom gm@||@com  Wed May 15 04:31:58 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Wed, 15 May 2019 14:31:58 +1200
Subject: [R] How to publish a new package on CRAN
In-Reply-To: <7f36219b-c22b-e520-dd2a-f20e89711248@gmail.com>
References: <65fa0f6d5de4498d93a7aa74fc05dbb8@ext.ec.europa.eu>
 <667aec8e-d6bc-7292-9218-286341d6a0b6@sapo.pt>
 <7f36219b-c22b-e520-dd2a-f20e89711248@gmail.com>
Message-ID: <CAB8pepwx1r_02eapCKzxO4NoT7i3Pv=CE6CfopeDqF57Gf-0Ag@mail.gmail.com>

And there's the R Manuals:
https://cran.r-project.org/manuals.html

The most relevant one (for this purpose) is "Writing R Extensions".

	[[alternative HTML version deleted]]


From @purd|e@@ @end|ng |rom gm@||@com  Wed May 15 06:00:49 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Wed, 15 May 2019 16:00:49 +1200
Subject: [R] Different predictions with forecast::auto.arima()
In-Reply-To: <6074A4D9-E7EF-47BB-9706-366789557282@dcn.davis.ca.us>
References: <CAFH+Q7w+O--=Thz_4OUy0pbUF+qxp1JodwE1TDSo_=Ukwo4XCQ@mail.gmail.com>
 <6074A4D9-E7EF-47BB-9706-366789557282@dcn.davis.ca.us>
Message-ID: <CAB8pepwqjytOC20AxRn6W9pSeAvQjyTcfqe=Y7V6B4PjiYxDnA@mail.gmail.com>

> This is definitely a statistics question still so not on topic here... as
changing the data is exactly the kind of thing that can have this effect.

I'm sorry.
I disagree.
This is a question about reproducible code.
So, I don't see why it should be considered off topic.

> >The function returned a (2,1,0) model with drift. However when I used the
> >same function it returns a (1,1,0) model. There were no obvious
differences
> >in the code. The only thing passed to it was the data. How might this
> >happen?

Either a different version of R, a difference in the forecast package, a
difference in one of it's imports (which there are many), or different
input.
If you're sure the input is the same, then it must be one of the other
reasons.

I suggest reading the documentation for the relevant forecast package
functions.

The auto.arima() function alone has many arguments.
Changing their values may produce a more desirable model.

	[[alternative HTML version deleted]]


From @ndrew@h@||ord @end|ng |rom gm@||@com  Wed May 15 06:56:37 2019
From: @ndrew@h@||ord @end|ng |rom gm@||@com (Andrew Halford)
Date: Wed, 15 May 2019 15:56:37 +1100
Subject: [R] multiple graphs on one plot
In-Reply-To: <CA+8X3fX=Zt9NK3a6j=ECxRRBZc1FXn=c3jacae8y9F-di7RnRQ@mail.gmail.com>
References: <CAJrFtqJM+1K61AHMSXy__RQq7c1UNaqGFqcJwSpw98krUfX0_w@mail.gmail.com>
 <CA+8X3fX=Zt9NK3a6j=ECxRRBZc1FXn=c3jacae8y9F-di7RnRQ@mail.gmail.com>
Message-ID: <CAJrFtqJXPs12zP3BLTrzt-Q9d_+ESSwGz+DpjVi0kFnMF2BGiA@mail.gmail.com>

Hi Jim,

Many thanks for your help and yes CW is carapace width. Here is the final
coding I used...I set the peak of the curves at max frequency bin for each
sex. I also added the means and SD's from my data. According to my visual
diagnostics (qqplots, density plots) the frequency distributions do appear
normally distributed in this case. To be honest Ive struggled to find
whether CW is normally distributed in mud crab populations or not.


f<- lf_crabs$cw[lf_crabs$sex=='female']
m<- lf_crabs$cw[lf_crabs$sex=='male']

# find mean and sd to determine normal curve dimensions
m_m <-mean(m)
sd_m <-sqrt(var(m))
m_f <-mean(f)
sd_f <-sqrt(varf(f))

mf <- list(f,m)
multhist(mf, xlab="CW", ylab="Frequency", ylim=c(0,100),main="All Measured
Crabs", col=c("dark gray", "light gray"),
         breaks=seq(90,210, by=10),beside=TRUE,space=c(0,0.5))
legend("topright", c("Females", "Males"), fill=c("dark gray", "light gray"))
lines(seq(0,32,length.out=121),rescale(dnorm(90:210,145.4867,20.99906),c(0,50)),col="dark
gray",lwd=2)
lines(seq(0,32,length.out=121),rescale(dnorm(90:210,151.0783,21.88299),c(0,80)),col="light
gray",lwd=2)
abline(v=145.4867,lwd=2,col="red")

On Mon, 13 May 2019 at 17:30, Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Andrew,
> First, a little mind reading. My crystal ball says that "cw" can be
> interpreted as "carapace width". It didn't tell me the parameters of
> the distribution, so:
>
> set.seed(1234)
> mf<-list(rnorm(400,145,15),rnorm(400,160,15))
> library(plotrix)
> multhist(mf, xlab="CW", ylab="Frequency", ylim=c(0,100),main="All Measured
> Crabs", col=c("dark gray", "light gray"),
>          breaks=seq(90,210, by=10),beside=TRUE,space=c(0,0.5))
> legend("topright", c("Females", "Males"), fill=c("dark gray", "light
> gray"))
> lines(seq(0,32,length.out=121),rescale(dnorm(90:210,145,15),c(0,100)))
>
> This produces what I think you are after. Note that it may be
> misleading as the distribution of carapace width in real mud crabs
> doesn't look normal to me.
>
> Jim
>
> On Mon, May 13, 2019 at 3:00 PM Andrew Halford <andrew.halford at gmail.com>
> wrote:
> >
> > Hi Listers
> >
> > I've been trying to make a single graphic that has frequency histograms
> for
> > male and female mud crabs displayed side by side (such as when using the
> > beside=TRUE command for barplots). I then want to display a normal
> > distribution on top of the male and female histograms.
> >
> > I have been using the multhist command in Plotrix to generate the
> > histograms without too much problem, but I cannot get the normal
> > distributions to plot up on the same graph.
> >
> > Histograms plot
> >
> > mf <-
> >
> list(lf_crabs$cw[lf_crabs$sex=='female'],lf_crabs$cw[lf_crabs$sex=='male'])
> > multhist(mf, xlab="CW", ylab="Frequency", ylim=c(0,100),main="All
> Measured
> > Crabs", col=c("dark gray", "light gray"),
> >          breaks=seq(90,210, by=10),beside=TRUE,space=c(0,0.5))
> > legend("topright", c("Females", "Males"), fill=c("dark gray", "light
> gray"))
> >
> > Then I try to add a normal distribution curve just to the female data
> but I
> > cant get the output to plot
> >
> > points(seq(min(mf[[1]]), max(mf[[1]]), length.out=300),
> >        dnorm(seq(min(mf[[1]]), max(mf[[1]]), length.out=300),
> >              mean(mf[[1]]), sd(mf[[1]])),type="l", col="dark gray")
> >
> > Even trying to add an abline to the plot doesn't work.
> >
> > What am I missing?
> >
> > cheers
> >
> > Andy
> >
> > --
> > Andrew Halford Ph.D
> > Senior Coastal Fisheries Scientist
> > Pacific Community | Communaut? du Pacifique CPS ? B.P. D5 | 98848 Noumea,
> > New Caledonia | Noum?a, Nouvelle-Cal?donie
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>


-- 
Andrew Halford Ph.D
Senior Coastal Fisheries Scientist
Pacific Community | Communaut? du Pacifique CPS ? B.P. D5 | 98848 Noumea,
New Caledonia | Noum?a, Nouvelle-Cal?donie

	[[alternative HTML version deleted]]


From r@@t@t@@me@@@ge@ @end|ng |rom gm@||@com  Tue May 14 21:51:49 2019
From: r@@t@t@@me@@@ge@ @end|ng |rom gm@||@com (r messages)
Date: Tue, 14 May 2019 20:51:49 +0100
Subject: [R] dealing with random.walk in intervention analysis using
 TSA::arimax
Message-ID: <CADyFY1hZmicDNgo+0WGZqyJ_Q1Xfo_cw_Yu2-A-GpjcE1TYJTQ@mail.gmail.com>

Hi Listers,

    I am trying to do intervention analysis using arimax function,
typically on financial time series (random walk).

I found that the function can't handle raw random walk series unless we
supply an already differenced version of it.

example data:

random.walk <- ts(c(64.6100,64.8200,64.7600,64.6700,64.5300,64.6300,64.6400,64.8100,64.8500,64.8600,65.0400,65.0000,64.7200,65.0600,64.3100,64.4700,64.5400,64.4100,64.4100
,64.5200,64.5350,64.4900,64.2100,64.3600,64.5450,64.6100,64.7800,64.7050,64.6600,64.4600,64.5000,64.4700,64.4700,64.6400,64.6400,64.2900,64.4800,64.3100
,64.2500,64.2500,64.0300,64.1450,64.4500,64.0700,64.2500,64.2000,64.1150,64.3000,64.3600,64.4600,64.2300,64.1700,64.1800,63.9900,63.9000,64.0399,64.2600
,64.5050,64.6300,64.4100,64.4950,64.5100,64.5601,63.7300,64.1100,64.0500,64.1800,64.2700,64.3600,64.4500,64.9800,64.8400,65.0000,65.1500,65.3750,65.3650
,65.3500,65.9700,65.9600,65.6200,65.5600,65.6300,65.5900,65.6750,65.6650,65.4600,65.6000,65.5500,65.5250,65.4500,65.3200,65.2000,65.2500,65.5400,65.5400
,65.6850,65.9000,65.8950,65.5100,65.0900,65.3500,65.2300,65.2500,65.4400,65.6950,65.5899,65.6750,65.4800,65.3900,65.1400,65.3600,65.5200,65.5100,64.9050
,64.9300,64.9550,64.9000,64.9100,64.8800,65.1200,65.7100,65.6400,65.7100,65.7500,65.6350,65.6400,65.9100,66.2550,66.4750,66.3800,66.2900,66.3100,66.2100
,66.2300,66.3500,66.3800,66.1800,66.2800,66.2200,66.2800,66.7400,66.7450,66.6900,66.4900,66.5850,66.5600,66.5000,66.4200,66.6000,66.6300,66.6300,66.7400
,66.7100,66.5800,67.1900,67.9300,67.6100,67.5400,67.6900,67.4150,67.4600,67.6400,67.9600,68.2000,68.1600,67.8600,67.8800,68.0100,68.7300,68.4900,68.4100
,68.5300,68.5799,68.4300,68.1300,68.5600,68.2844,68.4393,68.5700,68.6200,68.4750,68.5900,68.2700,68.7600,68.7950,68.7093,68.6800,68.7000,68.6300,68.4800
,68.4150,68.4800,68.5300,68.3600,68.1750,68.4300,68.4200,68.1600,68.1900,68.3000,68.4300,68.2500,68.3850,68.5200,68.0800,67.8900,68.0664,67.8200,67.5100
,67.7200), frequency = 7)


step.function <- c(rep(0, 154), rep(1, 56))


e.g.

Below throws an exception saying the fixed length was wrong for
stats::arima.
Reason for this is because it calls stats::arima. it has an argument called
"fixed" which we use to fixed some parameters in the arima equation.
random walk typically has nothing to fix (if comes with no drift term)
however TSA::arimax force-feeds some "fixed" values, causing the exception
here.

arimax(random.walk, order=c(0,1,0), xtransf=step.function, transfer=c(1,0))

Following works, but we had to fit an extra arima(0,0,1) to our differenced
random walk. (note if we do auto.arima or we peruse acf and pacf, an extra
arima(0,0,1) isn't necessary or justified.

random.walk.diff <- diff(random.walk)
arimax(random.walk.diff, order=c(0,0,1), xtransf=step.function,
transfer=c(1,0))

It is either a bug or by design. I don't follow; I tried to debug the code
but it is hardly human readable for me, there are many acronyms which is
hard to make sense and easy to lose track of.

What do you guys think?

Thanks very much for your input.

Regards,

Qiuxiao

	[[alternative HTML version deleted]]


From gr@eme@r@d@v|d@on @end|ng |rom gm@||@com  Wed May 15 10:12:38 2019
From: gr@eme@r@d@v|d@on @end|ng |rom gm@||@com (Graeme Davidson)
Date: Wed, 15 May 2019 09:12:38 +0100
Subject: [R] Iterating dates over multiple ids
Message-ID: <B61EC444-EA11-43A6-8D45-E14B9322EE01@gmail.com>

Hi all, 

I want to create a df that repeats the dates over ids. Previously I tried expand.grid but this created every possible iteration of dates and ids, when all I want is the start and end dates repeated for each month. In other words 01/03/19 with an end date of 28/02/19 would be unsuitable. I have also been told that rbind may be unsuitable when the data is scaled up to tens of thousands of rows. I would appreciate any suggestions. 

library(lubridate)
#get list of month dates for last 6 months
end_months <- ymd("2018-11-01")+ months(0:6)-days(1)
start_months <- ymd("2018-10-01")+ months(0:6)

#list of view ids required to make api call (pseudonymised) viewId <- c(26494958, 477448251, 47843527, 96382507, 537821552, 67482819)
All the best

Graeme R Davidson PhD

Data and Insight Analyst 
	[[alternative HTML version deleted]]


From @tyen @end|ng |rom ntu@edu@tw  Wed May 15 11:33:46 2019
From: @tyen @end|ng |rom ntu@edu@tw (Steven Yen)
Date: Wed, 15 May 2019 17:33:46 +0800
Subject: [R] Printing matrix/table in a procedure
Message-ID: <bc452ca5-d076-e2b0-987b-6b2762c82bef@ntu.edu.tw>

Dear All,

I would like to get a matrix (table) printed in a procedure, as 
attempted below. Please help. Thanks.

test<-function(x){
 ? table<-matrix(x,nrow=4)
 ? cat("\nTable:\n",table)
invisible(list(table=table))
}

x<-1:20
test(x)

-- 
styen at ntu.edu.tw (S.T. Yen)


	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed May 15 13:00:35 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 15 May 2019 12:00:35 +0100
Subject: [R] Printing matrix/table in a procedure
In-Reply-To: <bc452ca5-d076-e2b0-987b-6b2762c82bef@ntu.edu.tw>
References: <bc452ca5-d076-e2b0-987b-6b2762c82bef@ntu.edu.tw>
Message-ID: <e58858de-29dd-e57d-a22b-3399c63bbb49@sapo.pt>

Hello,

What's wrong with

test <- function(x){
   table <- matrix(x, nrow = 4)
   cat("\nTable:\n")
   print(table)
   invisible(list(table = table))
}


Hope this helps,

Rui Barradas

?s 10:33 de 15/05/19, Steven Yen escreveu:
> Dear All,
> 
> I would like to get a matrix (table) printed in a procedure, as
> attempted below. Please help. Thanks.
> 
> test<-function(x){
>   ? table<-matrix(x,nrow=4)
>   ? cat("\nTable:\n",table)
> invisible(list(table=table))
> }
> 
> x<-1:20
> test(x)
>


From pro|jcn@@h @end|ng |rom gm@||@com  Wed May 15 15:08:30 2019
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Wed, 15 May 2019 09:08:30 -0400
Subject: [R] How to publish a new package on CRAN
In-Reply-To: <CAB8pepwx1r_02eapCKzxO4NoT7i3Pv=CE6CfopeDqF57Gf-0Ag@mail.gmail.com>
References: <65fa0f6d5de4498d93a7aa74fc05dbb8@ext.ec.europa.eu>
 <667aec8e-d6bc-7292-9218-286341d6a0b6@sapo.pt>
 <7f36219b-c22b-e520-dd2a-f20e89711248@gmail.com>
 <CAB8pepwx1r_02eapCKzxO4NoT7i3Pv=CE6CfopeDqF57Gf-0Ag@mail.gmail.com>
Message-ID: <965b19d0-fd45-c433-e2ca-17ce9526e3d3@gmail.com>

In reading the original post, I could not help but get a feeling that the writers were
going through an exercise in learning how to put a package on CRAN. Having organized "Navigating
the R Package Universe" at UseR!2017, where Spencer Graves, Julia Silge and I pointed out the
difficulties for users in finding appropriate tools among the thousands of packages, perhaps
an effort to organize or modify EXISTING packages would be more useful.

It is NOT that new packages are unwelcome per se, but that we continue to need organization and
amalgamation of these new packages into collections or categories so that similar functionality
can be accessed more efficiently. And as a retired academic, I know how much "new" is valued over
"review and assessment" of existing material. I've found several "new" publications of codes I
published nearly half a century ago in the literature from time to time. Maybe the posters could
write RPlagiarizedCheck.

JN


On 2019-05-14 10:31 p.m., Abby Spurdle wrote:
> And there's the R Manuals:
> https://cran.r-project.org/manuals.html
> 
> The most relevant one (for this purpose) is "Writing R Extensions".
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bgunter@4567 @end|ng |rom gm@||@com  Wed May 15 16:48:43 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 15 May 2019 07:48:43 -0700
Subject: [R] How to publish a new package on CRAN
In-Reply-To: <965b19d0-fd45-c433-e2ca-17ce9526e3d3@gmail.com>
References: <65fa0f6d5de4498d93a7aa74fc05dbb8@ext.ec.europa.eu>
 <667aec8e-d6bc-7292-9218-286341d6a0b6@sapo.pt>
 <7f36219b-c22b-e520-dd2a-f20e89711248@gmail.com>
 <CAB8pepwx1r_02eapCKzxO4NoT7i3Pv=CE6CfopeDqF57Gf-0Ag@mail.gmail.com>
 <965b19d0-fd45-c433-e2ca-17ce9526e3d3@gmail.com>
Message-ID: <CAGxFJbQF3Pk9m+PZs84CmBRj=spOSiChN=TvZOm5Bw3n8A1EJg@mail.gmail.com>

This appears to be off topic here, but, when expanded, might make a nice
post on the R-bloggers site.

Cheers,
Bert

On Wed, May 15, 2019 at 6:10 AM J C Nash <profjcnash at gmail.com> wrote:

> In reading the original post, I could not help but get a feeling that the
> writers were
> going through an exercise in learning how to put a package on CRAN. Having
> organized "Navigating
> the R Package Universe" at UseR!2017, where Spencer Graves, Julia Silge
> and I pointed out the
> difficulties for users in finding appropriate tools among the thousands of
> packages, perhaps
> an effort to organize or modify EXISTING packages would be more useful.
>
> It is NOT that new packages are unwelcome per se, but that we continue to
> need organization and
> amalgamation of these new packages into collections or categories so that
> similar functionality
> can be accessed more efficiently. And as a retired academic, I know how
> much "new" is valued over
> "review and assessment" of existing material. I've found several "new"
> publications of codes I
> published nearly half a century ago in the literature from time to time.
> Maybe the posters could
> write RPlagiarizedCheck.
>
> JN
>
>
> On 2019-05-14 10:31 p.m., Abby Spurdle wrote:
> > And there's the R Manuals:
> > https://cran.r-project.org/manuals.html
> >
> > The most relevant one (for this purpose) is "Writing R Extensions".
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From mchowe||2 @end|ng |rom gm@||@com  Wed May 15 20:39:36 2019
From: mchowe||2 @end|ng |rom gm@||@com (Michael Howell)
Date: Wed, 15 May 2019 13:39:36 -0500
Subject: [R] Different predictions with forecast::auto.arima()
In-Reply-To: <CAB8pepwqjytOC20AxRn6W9pSeAvQjyTcfqe=Y7V6B4PjiYxDnA@mail.gmail.com>
References: <CAFH+Q7w+O--=Thz_4OUy0pbUF+qxp1JodwE1TDSo_=Ukwo4XCQ@mail.gmail.com>
 <6074A4D9-E7EF-47BB-9706-366789557282@dcn.davis.ca.us>
 <CAB8pepwqjytOC20AxRn6W9pSeAvQjyTcfqe=Y7V6B4PjiYxDnA@mail.gmail.com>
Message-ID: <CAFH+Q7zfu6K7w8GKoFzfy+NcDY6JDkFgcKV288qBPwPkk3XHHQ@mail.gmail.com>

Thank you very much for your advice.

On Tue, May 14, 2019 at 11:01 PM Abby Spurdle <spurdle.a at gmail.com> wrote:

> > This is definitely a statistics question still so not on topic here...
> as changing the data is exactly the kind of thing that can have this
> effect.
>
> I'm sorry.
> I disagree.
> This is a question about reproducible code.
> So, I don't see why it should be considered off topic.
>
> > >The function returned a (2,1,0) model with drift. However when I used
> the
> > >same function it returns a (1,1,0) model. There were no obvious
> differences
> > >in the code. The only thing passed to it was the data. How might this
> > >happen?
>
> Either a different version of R, a difference in the forecast package, a
> difference in one of it's imports (which there are many), or different
> input.
> If you're sure the input is the same, then it must be one of the other
> reasons.
>
> I suggest reading the documentation for the relevant forecast package
> functions.
>
> The auto.arima() function alone has many arguments.
> Changing their values may produce a more desirable model.
>
>

	[[alternative HTML version deleted]]


From n|co|@@rugg|ero@unt @end|ng |rom gm@||@com  Wed May 15 22:29:51 2019
From: n|co|@@rugg|ero@unt @end|ng |rom gm@||@com (Nicola Ruggiero)
Date: Wed, 15 May 2019 13:29:51 -0700
Subject: [R] converting zipcodes to latitude/longitude
In-Reply-To: <CA+8X3fUzS+csW_e2yFSTt0133zKPke5+UPC6p9-GTtxtCk3osg@mail.gmail.com>
References: <CAEneZ8kJjfb4zRu44Xnj677hKkX1GBSyOZfxZRhYM-MxdJ-SnA@mail.gmail.com>
 <CA+8X3fUzS+csW_e2yFSTt0133zKPke5+UPC6p9-GTtxtCk3osg@mail.gmail.com>
Message-ID: <CAEneZ8kZ5z5vyGGffn_T=WXf8XNgbTsiv3rdZuurBWGJfAWMmQ@mail.gmail.com>

Hi Jim,

I ended up collaborating with someone, and, on the basis of looking at
your code (we did take it into consideration and talk about it), we
came up with this:

library(stringr)
numextract <- function(string){
     str_extract(string, "\\-*\\d+\\,*\\d*")
}
myDataSet$zip<-numextract(myDataSet$state)
combineddata<-merge(zipcode, myDataSet, by.x="zip", by.y="zip")

So, as I understand it, we build a function the purpose of which was
to extract the numerical value from a string value, imputed that into
a column, then merged the two data frames together. It worked!

Now I just need to figure out this thing called shape data...basically
I need to figure out how to interpose a shape of the United States
underneath my data points so that I can see them over the location to
which they correspond.

Nicola

On Mon, May 13, 2019 at 9:09 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi Nicola,
> Getting the blank rows will be a bit more difficult and I don't see
> why they should be in the final data frame, so:
>
> townzip<-read.table(text="waltham, Massachusetts 02451
> Columbia, SC 29209
>
> Wheat Ridge , Colorado 80033
> Charlottesville, Virginia 22902
> Fairbanks, AK 99709
> Montpelier, VT 05602
> Dobbs Ferry, New York 10522
>
> Henderson , Kentucky 42420",
> sep="\t",stringsAsFactors=FALSE)
> zip_split<-function(x) {
>  commasplit<-unlist(strsplit(x,","))
>  state<-trimws(gsub("[[:digit:]]","",commasplit[2]))
>  zip<-trimws(gsub("[[:alpha:]]","",commasplit[2]))
>  return(c(commasplit[1],state,zip))
> }
> townzipsplit<-as.data.frame(t(sapply(townzip$V1,zip_split)))
> rownames(townzipsplit)<-NULL
> names(townzipsplit)<-c("town","state","zip")
> townzipsplit$latlon<-NA
> # I don't know the name of the zipcode column in the "zipcode" data frame
> newzipdf<-merge(townzipsplit,zipcodedf,by.x="zip",by.y="zip")
>
> Jim
>
> On Tue, May 14, 2019 at 5:57 AM Nicola Ruggiero
> <nicola.ruggiero.unt at gmail.com> wrote:
> >
> > Hello everyone,
> >
> > I've downloaded Jeffrey Breen's R package "zipcode," which has the
> > latitude and longitude for all of the US zip codes. So, this is a
> > data.frame with 43,191 observations. That's one data frame in my
> > environment.
> >
> > Then, I have another data.frame with over 100,000 observations that
> > look like this:
> >
> > waltham, Massachusetts 02451
> > Columbia, SC 29209
> >
> > Wheat Ridge , Colorado 80033
> > Charlottesville, Virginia 22902
> > Fairbanks, AK 99709
> > Montpelier, VT 05602
> > Dobbs Ferry, New York 10522
> >
> > Henderson , Kentucky 42420
> >
> > The spaces represent absences in the column. Regardless,
> > I need to figure out how to write a code that would, presumably, match
> > the zipcodes and produce another column to the data frame with the
> > latitude and longitude. So, for example, the code would recognize
> > 02451 above, and, in the the column next to it, the code would write
> > 42.3765? N, 71.2356? W in the column next to it, since that's the
> > latitude and longitude for Waltham, Massachusetts.
> >
> > Any idea of how to begin a code that would perform such an operation?
> >
> > Again, I have a data.frame with the zipcodes linked to the the
> > latitudes and longitudes, on the one hand, and another data.frame with
> > only zipcodes (and some holes). I need to produce the corresponding
> > latitude/longitudes in the latter data.frame.
> >
> > Nicola
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From m|ch@e|@p@bou||ne@u @end|ng |rom gm@||@com  Thu May 16 01:07:47 2019
From: m|ch@e|@p@bou||ne@u @end|ng |rom gm@||@com (Michael Boulineau)
Date: Wed, 15 May 2019 16:07:47 -0700
Subject: [R] how to separate string from numbers in a large txt file
Message-ID: <CAH+cTGM9sFCxegaMHzu-_9Prhvvh9MbVDCWJ1_x7wkO9jC47mQ@mail.gmail.com>

I have a wild and crazy text file, the head of which looks like this:

2016-07-01 02:50:35 <john> hey
2016-07-01 02:51:26 <jane> waiting for plane to Edinburgh
2016-07-01 02:51:45 <john> thinking about my boo
2016-07-01 02:52:07 <jane> nothing crappy has happened, not really
2016-07-01 02:52:20 <john> plane went by pretty fast, didn't sleep
2016-07-01 02:54:08 <jane> no idea what time it is or where I am really
2016-07-01 02:54:17 <john> just know it's london
2016-07-01 02:56:44 <jane> you are probably asleep
2016-07-01 02:58:45 <jane> I hope fish was fishy in a good eay
2016-07-01 02:58:56 <jone> ?
2016-07-01 02:59:34 <jane> ???
2016-07-01 03:02:48 <john> British security is a little more rigorous...

It goes on for a while. It's a big file. But I feel like it's going to
be difficult to annotate with the coreNLP library or package. I'm
doing natural language processing. In other words, I'm curious as to
how I would shave off the dates, that is, to make it look like:

<john> hey
<jane> waiting for plane to Edinburgh
 <john> thinking about my boo
<jane> nothing crappy has happened, not really
<john> plane went by pretty fast, didn't sleep
<jane> no idea what time it is or where I am really
<john> just know it's london
<jane> you are probably asleep
<jane> I hope fish was fishy in a good eay
 <jone> ?
<jane> ???
<john> British security is a little more rigorous...

To be clear, then, I'm trying to clean a large text file by writing a
regular expression? such that I create a new object with no numbers or
dates.

Michael


From dw|n@em|u@ @end|ng |rom comc@@t@net  Thu May 16 05:47:47 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Wed, 15 May 2019 20:47:47 -0700
Subject: [R] how to separate string from numbers in a large txt file
In-Reply-To: <CAH+cTGM9sFCxegaMHzu-_9Prhvvh9MbVDCWJ1_x7wkO9jC47mQ@mail.gmail.com>
References: <CAH+cTGM9sFCxegaMHzu-_9Prhvvh9MbVDCWJ1_x7wkO9jC47mQ@mail.gmail.com>
Message-ID: <c6e69644-66a1-7031-4d7c-1000e0abbea4@comcast.net>


On 5/15/19 4:07 PM, Michael Boulineau wrote:
> I have a wild and crazy text file, the head of which looks like this:
>
> 2016-07-01 02:50:35 <john> hey
> 2016-07-01 02:51:26 <jane> waiting for plane to Edinburgh
> 2016-07-01 02:51:45 <john> thinking about my boo
> 2016-07-01 02:52:07 <jane> nothing crappy has happened, not really
> 2016-07-01 02:52:20 <john> plane went by pretty fast, didn't sleep
> 2016-07-01 02:54:08 <jane> no idea what time it is or where I am really
> 2016-07-01 02:54:17 <john> just know it's london
> 2016-07-01 02:56:44 <jane> you are probably asleep
> 2016-07-01 02:58:45 <jane> I hope fish was fishy in a good eay
> 2016-07-01 02:58:56 <jone> ?
> 2016-07-01 02:59:34 <jane> ???
> 2016-07-01 03:02:48 <john> British security is a little more rigorous...

Looks entirely not-"crazy". Typical log file format.

Two possibilities: 1) Use `read.fwf` from pkg foreign; 2) Use regex 
(i.e. the sub-function) to strip everything up to the "<". Read 
`?regex`. Since that's not a metacharacters you could use a pattern 
".+<" and replace with "".

And do read the Posting Guide. Cross-posting to StackOverflow and Rhelp, 
at least within hours of each, is considered poor manners.


-- 

David.

>
> It goes on for a while. It's a big file. But I feel like it's going to
> be difficult to annotate with the coreNLP library or package. I'm
> doing natural language processing. In other words, I'm curious as to
> how I would shave off the dates, that is, to make it look like:
>
> <john> hey
> <jane> waiting for plane to Edinburgh
>   <john> thinking about my boo
> <jane> nothing crappy has happened, not really
> <john> plane went by pretty fast, didn't sleep
> <jane> no idea what time it is or where I am really
> <john> just know it's london
> <jane> you are probably asleep
> <jane> I hope fish was fishy in a good eay
>   <jone> ?
> <jane> ???
> <john> British security is a little more rigorous...
>
> To be clear, then, I'm trying to clean a large text file by writing a
> regular expression? such that I create a new object with no numbers or
> dates.
>
> Michael
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drj|m|emon @end|ng |rom gm@||@com  Thu May 16 06:36:31 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Thu, 16 May 2019 14:36:31 +1000
Subject: [R] converting zipcodes to latitude/longitude
In-Reply-To: <CAEneZ8kZ5z5vyGGffn_T=WXf8XNgbTsiv3rdZuurBWGJfAWMmQ@mail.gmail.com>
References: <CAEneZ8kJjfb4zRu44Xnj677hKkX1GBSyOZfxZRhYM-MxdJ-SnA@mail.gmail.com>
 <CA+8X3fUzS+csW_e2yFSTt0133zKPke5+UPC6p9-GTtxtCk3osg@mail.gmail.com>
 <CAEneZ8kZ5z5vyGGffn_T=WXf8XNgbTsiv3rdZuurBWGJfAWMmQ@mail.gmail.com>
Message-ID: <CA+8X3fUOwfFMOPoDyo-Ds6Qxncam3w8sCSbWyPa+35oi_R6wHg@mail.gmail.com>

Hi Nicola,
Good to learn that you solved the problem. Shape files are usually a
set of polygons for the named areas.  I did some work with the "rgdal"
package a while ago and it wasn't very difficult. There might be
better methods now, so posting to R-SIG-geo is a good idea.

Jim

On Thu, May 16, 2019 at 6:30 AM Nicola Ruggiero
<nicola.ruggiero.unt at gmail.com> wrote:
>
> Hi Jim,
>
> I ended up collaborating with someone, and, on the basis of looking at
> your code (we did take it into consideration and talk about it), we
> came up with this:
>
> library(stringr)
> numextract <- function(string){
>      str_extract(string, "\\-*\\d+\\,*\\d*")
> }
> myDataSet$zip<-numextract(myDataSet$state)
> combineddata<-merge(zipcode, myDataSet, by.x="zip", by.y="zip")
>
> So, as I understand it, we build a function the purpose of which was
> to extract the numerical value from a string value, imputed that into
> a column, then merged the two data frames together. It worked!
>
> Now I just need to figure out this thing called shape data...basically
> I need to figure out how to interpose a shape of the United States
> underneath my data points so that I can see them over the location to
> which they correspond.
>
> Nicola
>
> On Mon, May 13, 2019 at 9:09 PM Jim Lemon <drjimlemon at gmail.com> wrote:
> >
> > Hi Nicola,
> > Getting the blank rows will be a bit more difficult and I don't see
> > why they should be in the final data frame, so:
> >
> > townzip<-read.table(text="waltham, Massachusetts 02451
> > Columbia, SC 29209
> >
> > Wheat Ridge , Colorado 80033
> > Charlottesville, Virginia 22902
> > Fairbanks, AK 99709
> > Montpelier, VT 05602
> > Dobbs Ferry, New York 10522
> >
> > Henderson , Kentucky 42420",
> > sep="\t",stringsAsFactors=FALSE)
> > zip_split<-function(x) {
> >  commasplit<-unlist(strsplit(x,","))
> >  state<-trimws(gsub("[[:digit:]]","",commasplit[2]))
> >  zip<-trimws(gsub("[[:alpha:]]","",commasplit[2]))
> >  return(c(commasplit[1],state,zip))
> > }
> > townzipsplit<-as.data.frame(t(sapply(townzip$V1,zip_split)))
> > rownames(townzipsplit)<-NULL
> > names(townzipsplit)<-c("town","state","zip")
> > townzipsplit$latlon<-NA
> > # I don't know the name of the zipcode column in the "zipcode" data frame
> > newzipdf<-merge(townzipsplit,zipcodedf,by.x="zip",by.y="zip")
> >
> > Jim
> >
> > On Tue, May 14, 2019 at 5:57 AM Nicola Ruggiero
> > <nicola.ruggiero.unt at gmail.com> wrote:
> > >
> > > Hello everyone,
> > >
> > > I've downloaded Jeffrey Breen's R package "zipcode," which has the
> > > latitude and longitude for all of the US zip codes. So, this is a
> > > data.frame with 43,191 observations. That's one data frame in my
> > > environment.
> > >
> > > Then, I have another data.frame with over 100,000 observations that
> > > look like this:
> > >
> > > waltham, Massachusetts 02451
> > > Columbia, SC 29209
> > >
> > > Wheat Ridge , Colorado 80033
> > > Charlottesville, Virginia 22902
> > > Fairbanks, AK 99709
> > > Montpelier, VT 05602
> > > Dobbs Ferry, New York 10522
> > >
> > > Henderson , Kentucky 42420
> > >
> > > The spaces represent absences in the column. Regardless,
> > > I need to figure out how to write a code that would, presumably, match
> > > the zipcodes and produce another column to the data frame with the
> > > latitude and longitude. So, for example, the code would recognize
> > > 02451 above, and, in the the column next to it, the code would write
> > > 42.3765? N, 71.2356? W in the column next to it, since that's the
> > > latitude and longitude for Waltham, Massachusetts.
> > >
> > > Any idea of how to begin a code that would perform such an operation?
> > >
> > > Again, I have a data.frame with the zipcodes linked to the the
> > > latitudes and longitudes, on the one hand, and another data.frame with
> > > only zipcodes (and some holes). I need to produce the corresponding
> > > latitude/longitudes in the latter data.frame.
> > >
> > > Nicola
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @tyen @end|ng |rom ntu@edu@tw  Thu May 16 07:35:26 2019
From: @tyen @end|ng |rom ntu@edu@tw (Steven Yen)
Date: Thu, 16 May 2019 13:35:26 +0800
Subject: [R] Printing matrix/table in a procedure
In-Reply-To: <e58858de-29dd-e57d-a22b-3399c63bbb49@sapo.pt>
References: <bc452ca5-d076-e2b0-987b-6b2762c82bef@ntu.edu.tw>
 <e58858de-29dd-e57d-a22b-3399c63bbb49@sapo.pt>
Message-ID: <da177b4a-215d-3744-2d00-6251a770a843@ntu.edu.tw>

Great! Thanks.

On 5/15/2019 7:00 PM, Rui Barradas wrote:
> Hello,
>
> What's wrong with
>
> test <- function(x){
> ? table <- matrix(x, nrow = 4)
> ? cat("\nTable:\n")
> ? print(table)
> ? invisible(list(table = table))
> }
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 10:33 de 15/05/19, Steven Yen escreveu:
>> Dear All,
>>
>> I would like to get a matrix (table) printed in a procedure, as
>> attempted below. Please help. Thanks.
>>
>> test<-function(x){
>> ? ? table<-matrix(x,nrow=4)
>> ? ? cat("\nTable:\n",table)
>> invisible(list(table=table))
>> }
>>
>> x<-1:20
>> test(x)
>>
>

-- 
styen at ntu.edu.tw (S.T. Yen)


	[[alternative HTML version deleted]]


From m|ch@e|@p@bou||ne@u @end|ng |rom gm@||@com  Thu May 16 21:30:13 2019
From: m|ch@e|@p@bou||ne@u @end|ng |rom gm@||@com (Michael Boulineau)
Date: Thu, 16 May 2019 12:30:13 -0700
Subject: [R] how to separate string from numbers in a large txt file
In-Reply-To: <c6e69644-66a1-7031-4d7c-1000e0abbea4@comcast.net>
References: <CAH+cTGM9sFCxegaMHzu-_9Prhvvh9MbVDCWJ1_x7wkO9jC47mQ@mail.gmail.com>
 <c6e69644-66a1-7031-4d7c-1000e0abbea4@comcast.net>
Message-ID: <CAH+cTGM5YvWV_G0pRgMqjCzuRrFTzd0nOc7yoThfGbGMUyMjsQ@mail.gmail.com>

Thanks for this tip on etiquette, David. I will be sure and not do that again.

I tried the read.fwf from the foreign package, with a code like this:

 d <- read.fwf("hangouts-conversation.txt",
                widths= c(10,10,20,40),
                col.names=c("date","time","person","comment"),
                strip.white=TRUE)

But it threw this error:

Error in scan(file = file, what = what, sep = sep, quote = quote, dec = dec,  :
  line 6347 did not have 4 elements

Interestingly, though, the error only happened when I increased the
width size. But I had to increase the size, or else I couldn't "see"
anything.  The comment was so small that nothing was being captured by
the size of the column. so to speak.

It seems like what's throwing me is that there's no comma that
demarcates the end of the text proper. For example:

2016-07-01 15:34:30 <John Doe> Lame. We were in a starbucks2016-07-01
15:35:02 <Jane Doe> Hmm that's interesting2016-07-01 15:35:09 <Jane
Doe> You must want coffees2016-07-01 15:35:25 <John Doe> There was
lots of Starbucks in my day2016-07-01 15:35:47

It was interesting, too, when I pasted the text into the email, it
self-formatted into the way I wanted it to look. I had to manually
make it look like it does above, since that's the way that it looks in
the txt file. I wonder if it's being organized by XML or something.

Anyways, There's always a space between the two sideways carrots, just
like there is right now: <John Doe> See. Space. And there's always a
space between the data and time. Like this. 2016-07-01 15:34:30 See.
Space. But there's never a space between the end of the comment and
the next date. Like this: We were in a starbucks2016-07-01 15:35:02
See. starbucks and 2016 are smooshed together.

This code is also on the table right now too.

a <- read.table("E:/working
directory/-189/hangouts-conversation2.txt", quote="\"",
comment.char="", fill=TRUE)

h<-cbind(hangouts.conversation2[,1:2],hangouts.conversation2[,3:5],hangouts.conversation2[,6:9])

aa<-gsub("[^[:digit:]]","",h)
my.data.num <- as.numeric(str_extract(h, "[0-9]+"))

Those last lines are a work in progress. I wish I could import a
picture of what it looks like when it's translated into a data frame.
The fill=TRUE helped to get the data in table that kind of sort of
works, but the comments keep bleeding into the data and time column.
It's like

2016-07-01 15:59:17 <Jane Doe> Seriously I've never been
over               there
2016-07-01 15:59:27 <Jane Doe> It confuses me :(

And then, maybe, the "seriously" will be in a column all to itself, as
will be the "I've'"and the "never" etc.

I will use a regular expression if I have to, but it would be nice to
keep the dates and times on there. Originally, I thought they were
meaningless, but I've since changed my mind on that count. The time of
day isn't so important. But, especially since, say, Gmail itself knows
how to quickly recognize what it is, I know it can be done. I know
this data has structure to it.

Michael



On Wed, May 15, 2019 at 8:47 PM David Winsemius <dwinsemius at comcast.net> wrote:
>
>
> On 5/15/19 4:07 PM, Michael Boulineau wrote:
> > I have a wild and crazy text file, the head of which looks like this:
> >
> > 2016-07-01 02:50:35 <john> hey
> > 2016-07-01 02:51:26 <jane> waiting for plane to Edinburgh
> > 2016-07-01 02:51:45 <john> thinking about my boo
> > 2016-07-01 02:52:07 <jane> nothing crappy has happened, not really
> > 2016-07-01 02:52:20 <john> plane went by pretty fast, didn't sleep
> > 2016-07-01 02:54:08 <jane> no idea what time it is or where I am really
> > 2016-07-01 02:54:17 <john> just know it's london
> > 2016-07-01 02:56:44 <jane> you are probably asleep
> > 2016-07-01 02:58:45 <jane> I hope fish was fishy in a good eay
> > 2016-07-01 02:58:56 <jone>
> > 2016-07-01 02:59:34 <jane>
> > 2016-07-01 03:02:48 <john> British security is a little more rigorous...
>
> Looks entirely not-"crazy". Typical log file format.
>
> Two possibilities: 1) Use `read.fwf` from pkg foreign; 2) Use regex
> (i.e. the sub-function) to strip everything up to the "<". Read
> `?regex`. Since that's not a metacharacters you could use a pattern
> ".+<" and replace with "".
>
> And do read the Posting Guide. Cross-posting to StackOverflow and Rhelp,
> at least within hours of each, is considered poor manners.
>
>
> --
>
> David.
>
> >
> > It goes on for a while. It's a big file. But I feel like it's going to
> > be difficult to annotate with the coreNLP library or package. I'm
> > doing natural language processing. In other words, I'm curious as to
> > how I would shave off the dates, that is, to make it look like:
> >
> > <john> hey
> > <jane> waiting for plane to Edinburgh
> >   <john> thinking about my boo
> > <jane> nothing crappy has happened, not really
> > <john> plane went by pretty fast, didn't sleep
> > <jane> no idea what time it is or where I am really
> > <john> just know it's london
> > <jane> you are probably asleep
> > <jane> I hope fish was fishy in a good eay
> >   <jone>
> > <jane>
> > <john> British security is a little more rigorous...
> >
> > To be clear, then, I'm trying to clean a large text file by writing a
> > regular expression? such that I create a new object with no numbers or
> > dates.
> >
> > Michael
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From dw|n@em|u@ @end|ng |rom comc@@t@net  Thu May 16 22:05:10 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Thu, 16 May 2019 13:05:10 -0700
Subject: [R] how to separate string from numbers in a large txt file
In-Reply-To: <CAH+cTGM5YvWV_G0pRgMqjCzuRrFTzd0nOc7yoThfGbGMUyMjsQ@mail.gmail.com>
References: <CAH+cTGM9sFCxegaMHzu-_9Prhvvh9MbVDCWJ1_x7wkO9jC47mQ@mail.gmail.com>
 <c6e69644-66a1-7031-4d7c-1000e0abbea4@comcast.net>
 <CAH+cTGM5YvWV_G0pRgMqjCzuRrFTzd0nOc7yoThfGbGMUyMjsQ@mail.gmail.com>
Message-ID: <9c9f1c81-309f-91cb-7e26-f02fc4bd45d8@comcast.net>


On 5/16/19 12:30 PM, Michael Boulineau wrote:
> Thanks for this tip on etiquette, David. I will be sure and not do that again.
>
> I tried the read.fwf from the foreign package, with a code like this:
>
>   d <- read.fwf("hangouts-conversation.txt",
>                  widths= c(10,10,20,40),
>                  col.names=c("date","time","person","comment"),
>                  strip.white=TRUE)
>
> But it threw this error:
>
> Error in scan(file = file, what = what, sep = sep, quote = quote, dec = dec,  :
>    line 6347 did not have 4 elements


So what does line 6347 look like? (Use `readLines` and print it out.)

>
> Interestingly, though, the error only happened when I increased the
> width size. But I had to increase the size, or else I couldn't "see"
> anything.  The comment was so small that nothing was being captured by
> the size of the column. so to speak.
>
> It seems like what's throwing me is that there's no comma that
> demarcates the end of the text proper. For example:

Not sure why you thought there should be a comma. Lines usually end 
with? <cr> and or a <lf>.


Once you have the raw text in a character vector from `readLines` named, 
say, 'chrvec', then you could selectively substitute commas for spaces 
with regex. (Now that you no longer desire to remove the dates and times.)

sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", chrvec)

This will not do any replacements when the pattern is not matched. See 
this test:


 > newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4", chrvec)
 > newvec
 ?[1] "2016-07-01,02:50:35,<john>,hey"
 ?[2] "2016-07-01,02:51:26,<jane>,waiting for plane to Edinburgh"
 ?[3] "2016-07-01,02:51:45,<john>,thinking about my boo"
 ?[4] "2016-07-01,02:52:07,<jane>,nothing crappy has happened, not really"
 ?[5] "2016-07-01,02:52:20,<john>,plane went by pretty fast, didn't sleep"
 ?[6] "2016-07-01,02:54:08,<jane>,no idea what time it is or where I am 
really"
 ?[7] "2016-07-01,02:54:17,<john>,just know it's london"
 ?[8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
 ?[9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a good eay"
[10] "2016-07-01 02:58:56 <jone>"
[11] "2016-07-01 02:59:34 <jane>"
[12] "2016-07-01,03:02:48,<john>,British security is a little more 
rigorous..."


You should probably remove the "empty comment" lines.


-- 

David.

>
> 2016-07-01 15:34:30 <John Doe> Lame. We were in a starbucks2016-07-01
> 15:35:02 <Jane Doe> Hmm that's interesting2016-07-01 15:35:09 <Jane
> Doe> You must want coffees2016-07-01 15:35:25 <John Doe> There was
> lots of Starbucks in my day2016-07-01 15:35:47
>
> It was interesting, too, when I pasted the text into the email, it
> self-formatted into the way I wanted it to look. I had to manually
> make it look like it does above, since that's the way that it looks in
> the txt file. I wonder if it's being organized by XML or something.
>
> Anyways, There's always a space between the two sideways carrots, just
> like there is right now: <John Doe> See. Space. And there's always a
> space between the data and time. Like this. 2016-07-01 15:34:30 See.
> Space. But there's never a space between the end of the comment and
> the next date. Like this: We were in a starbucks2016-07-01 15:35:02
> See. starbucks and 2016 are smooshed together.
>
> This code is also on the table right now too.
>
> a <- read.table("E:/working
> directory/-189/hangouts-conversation2.txt", quote="\"",
> comment.char="", fill=TRUE)
>
> h<-cbind(hangouts.conversation2[,1:2],hangouts.conversation2[,3:5],hangouts.conversation2[,6:9])
>
> aa<-gsub("[^[:digit:]]","",h)
> my.data.num <- as.numeric(str_extract(h, "[0-9]+"))
>
> Those last lines are a work in progress. I wish I could import a
> picture of what it looks like when it's translated into a data frame.
> The fill=TRUE helped to get the data in table that kind of sort of
> works, but the comments keep bleeding into the data and time column.
> It's like
>
> 2016-07-01 15:59:17 <Jane Doe> Seriously I've never been
> over               there
> 2016-07-01 15:59:27 <Jane Doe> It confuses me :(
>
> And then, maybe, the "seriously" will be in a column all to itself, as
> will be the "I've'"and the "never" etc.
>
> I will use a regular expression if I have to, but it would be nice to
> keep the dates and times on there. Originally, I thought they were
> meaningless, but I've since changed my mind on that count. The time of
> day isn't so important. But, especially since, say, Gmail itself knows
> how to quickly recognize what it is, I know it can be done. I know
> this data has structure to it.
>
> Michael
>
>
>
> On Wed, May 15, 2019 at 8:47 PM David Winsemius <dwinsemius at comcast.net> wrote:
>>
>> On 5/15/19 4:07 PM, Michael Boulineau wrote:
>>> I have a wild and crazy text file, the head of which looks like this:
>>>
>>> 2016-07-01 02:50:35 <john> hey
>>> 2016-07-01 02:51:26 <jane> waiting for plane to Edinburgh
>>> 2016-07-01 02:51:45 <john> thinking about my boo
>>> 2016-07-01 02:52:07 <jane> nothing crappy has happened, not really
>>> 2016-07-01 02:52:20 <john> plane went by pretty fast, didn't sleep
>>> 2016-07-01 02:54:08 <jane> no idea what time it is or where I am really
>>> 2016-07-01 02:54:17 <john> just know it's london
>>> 2016-07-01 02:56:44 <jane> you are probably asleep
>>> 2016-07-01 02:58:45 <jane> I hope fish was fishy in a good eay
>>> 2016-07-01 02:58:56 <jone>
>>> 2016-07-01 02:59:34 <jane>
>>> 2016-07-01 03:02:48 <john> British security is a little more rigorous...
>> Looks entirely not-"crazy". Typical log file format.
>>
>> Two possibilities: 1) Use `read.fwf` from pkg foreign; 2) Use regex
>> (i.e. the sub-function) to strip everything up to the "<". Read
>> `?regex`. Since that's not a metacharacters you could use a pattern
>> ".+<" and replace with "".
>>
>> And do read the Posting Guide. Cross-posting to StackOverflow and Rhelp,
>> at least within hours of each, is considered poor manners.
>>
>>
>> --
>>
>> David.
>>
>>> It goes on for a while. It's a big file. But I feel like it's going to
>>> be difficult to annotate with the coreNLP library or package. I'm
>>> doing natural language processing. In other words, I'm curious as to
>>> how I would shave off the dates, that is, to make it look like:
>>>
>>> <john> hey
>>> <jane> waiting for plane to Edinburgh
>>>    <john> thinking about my boo
>>> <jane> nothing crappy has happened, not really
>>> <john> plane went by pretty fast, didn't sleep
>>> <jane> no idea what time it is or where I am really
>>> <john> just know it's london
>>> <jane> you are probably asleep
>>> <jane> I hope fish was fishy in a good eay
>>>    <jone>
>>> <jane>
>>> <john> British security is a little more rigorous...
>>>
>>> To be clear, then, I'm trying to clean a large text file by writing a
>>> regular expression? such that I create a new object with no numbers or
>>> dates.
>>>
>>> Michael
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m|ch@e|@p@bou||ne@u @end|ng |rom gm@||@com  Fri May 17 00:53:04 2019
From: m|ch@e|@p@bou||ne@u @end|ng |rom gm@||@com (Michael Boulineau)
Date: Thu, 16 May 2019 15:53:04 -0700
Subject: [R] how to separate string from numbers in a large txt file
In-Reply-To: <9c9f1c81-309f-91cb-7e26-f02fc4bd45d8@comcast.net>
References: <CAH+cTGM9sFCxegaMHzu-_9Prhvvh9MbVDCWJ1_x7wkO9jC47mQ@mail.gmail.com>
 <c6e69644-66a1-7031-4d7c-1000e0abbea4@comcast.net>
 <CAH+cTGM5YvWV_G0pRgMqjCzuRrFTzd0nOc7yoThfGbGMUyMjsQ@mail.gmail.com>
 <9c9f1c81-309f-91cb-7e26-f02fc4bd45d8@comcast.net>
Message-ID: <CAH+cTGMx4_Lz2gGDf2AuUaaZF2Xaq4McuPRoHu4JJNc-ORd=Ew@mail.gmail.com>

OK. So, I named the object test and then checked the 6347th item

> test <- readLines ("hangouts-conversation.txt)
> test [6347]
[1] "2016-10-21 10:56:37 <John Doe> Admit#8242"

Perhaps where it was getting screwed up is, since the end of this is a
number (8242), then, given that there's no space between the number
and what ought to be the next row, R didn't know where to draw the
line. Sure enough, it looks like this when I go to the original file
and control f "#8242"

2016-10-21 10:35:36 <Jane Doe> What's your login
2016-10-21 10:56:29 <John Doe> John_Doe
2016-10-21 10:56:37 <John Doe> Admit#8242
2016-10-21 11:00:13 <Jane Doe> Okay so you have a discussion

Again, it doesn't look like that in the file. Gmail automatically
formats it like that when I paste it in. More to the point, it looks
like

2016-10-21 10:35:36 <Jane Doe> What's your login2016-10-21 10:56:29
<John Doe> John_Doe2016-10-21 10:56:37 <John Doe> Admit#82422016-10-21
11:00:13 <Jane Doe> Okay so you have a discussion

Notice Admit#82422016. So there's that.

Then I built object test2.

test2 <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", test)

This worked for 84 lines, then this happened.

> test2 [84]
[1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
> test2 [85]
[1] "//1,//2,//3,//4"
> test [85]
[1] "2016-07-01 02:50:35 <John Doe> hey"

Notice how I toggled back and forth between test and test2 there. So,
whatever happened with the regex, it happened in the switch from 84 to
85, I guess. It went on like

[990] "//1,//2,//3,//4"
 [991] "//1,//2,//3,//4"
 [992] "//1,//2,//3,//4"
 [993] "//1,//2,//3,//4"
 [994] "//1,//2,//3,//4"
 [995] "//1,//2,//3,//4"
 [996] "//1,//2,//3,//4"
 [997] "//1,//2,//3,//4"
 [998] "//1,//2,//3,//4"
 [999] "//1,//2,//3,//4"
[1000] "//1,//2,//3,//4"

up until line 1000, then I reached max.print.

Michael

On Thu, May 16, 2019 at 1:05 PM David Winsemius <dwinsemius at comcast.net> wrote:
>
>
> On 5/16/19 12:30 PM, Michael Boulineau wrote:
> > Thanks for this tip on etiquette, David. I will be sure and not do that again.
> >
> > I tried the read.fwf from the foreign package, with a code like this:
> >
> >   d <- read.fwf("hangouts-conversation.txt",
> >                  widths= c(10,10,20,40),
> >                  col.names=c("date","time","person","comment"),
> >                  strip.white=TRUE)
> >
> > But it threw this error:
> >
> > Error in scan(file = file, what = what, sep = sep, quote = quote, dec = dec,  :
> >    line 6347 did not have 4 elements
>
>
> So what does line 6347 look like? (Use `readLines` and print it out.)
>
> >
> > Interestingly, though, the error only happened when I increased the
> > width size. But I had to increase the size, or else I couldn't "see"
> > anything.  The comment was so small that nothing was being captured by
> > the size of the column. so to speak.
> >
> > It seems like what's throwing me is that there's no comma that
> > demarcates the end of the text proper. For example:
>
> Not sure why you thought there should be a comma. Lines usually end
> with  <cr> and or a <lf>.
>
>
> Once you have the raw text in a character vector from `readLines` named,
> say, 'chrvec', then you could selectively substitute commas for spaces
> with regex. (Now that you no longer desire to remove the dates and times.)
>
> sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", chrvec)
>
> This will not do any replacements when the pattern is not matched. See
> this test:
>
>
>  > newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4", chrvec)
>  > newvec
>   [1] "2016-07-01,02:50:35,<john>,hey"
>   [2] "2016-07-01,02:51:26,<jane>,waiting for plane to Edinburgh"
>   [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
>   [4] "2016-07-01,02:52:07,<jane>,nothing crappy has happened, not really"
>   [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast, didn't sleep"
>   [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or where I am
> really"
>   [7] "2016-07-01,02:54:17,<john>,just know it's london"
>   [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
>   [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a good eay"
> [10] "2016-07-01 02:58:56 <jone>"
> [11] "2016-07-01 02:59:34 <jane>"
> [12] "2016-07-01,03:02:48,<john>,British security is a little more
> rigorous..."
>
>
> You should probably remove the "empty comment" lines.
>
>
> --
>
> David.
>
> >
> > 2016-07-01 15:34:30 <John Doe> Lame. We were in a starbucks2016-07-01
> > 15:35:02 <Jane Doe> Hmm that's interesting2016-07-01 15:35:09 <Jane
> > Doe> You must want coffees2016-07-01 15:35:25 <John Doe> There was
> > lots of Starbucks in my day2016-07-01 15:35:47
> >
> > It was interesting, too, when I pasted the text into the email, it
> > self-formatted into the way I wanted it to look. I had to manually
> > make it look like it does above, since that's the way that it looks in
> > the txt file. I wonder if it's being organized by XML or something.
> >
> > Anyways, There's always a space between the two sideways carrots, just
> > like there is right now: <John Doe> See. Space. And there's always a
> > space between the data and time. Like this. 2016-07-01 15:34:30 See.
> > Space. But there's never a space between the end of the comment and
> > the next date. Like this: We were in a starbucks2016-07-01 15:35:02
> > See. starbucks and 2016 are smooshed together.
> >
> > This code is also on the table right now too.
> >
> > a <- read.table("E:/working
> > directory/-189/hangouts-conversation2.txt", quote="\"",
> > comment.char="", fill=TRUE)
> >
> > h<-cbind(hangouts.conversation2[,1:2],hangouts.conversation2[,3:5],hangouts.conversation2[,6:9])
> >
> > aa<-gsub("[^[:digit:]]","",h)
> > my.data.num <- as.numeric(str_extract(h, "[0-9]+"))
> >
> > Those last lines are a work in progress. I wish I could import a
> > picture of what it looks like when it's translated into a data frame.
> > The fill=TRUE helped to get the data in table that kind of sort of
> > works, but the comments keep bleeding into the data and time column.
> > It's like
> >
> > 2016-07-01 15:59:17 <Jane Doe> Seriously I've never been
> > over               there
> > 2016-07-01 15:59:27 <Jane Doe> It confuses me :(
> >
> > And then, maybe, the "seriously" will be in a column all to itself, as
> > will be the "I've'"and the "never" etc.
> >
> > I will use a regular expression if I have to, but it would be nice to
> > keep the dates and times on there. Originally, I thought they were
> > meaningless, but I've since changed my mind on that count. The time of
> > day isn't so important. But, especially since, say, Gmail itself knows
> > how to quickly recognize what it is, I know it can be done. I know
> > this data has structure to it.
> >
> > Michael
> >
> >
> >
> > On Wed, May 15, 2019 at 8:47 PM David Winsemius <dwinsemius at comcast.net> wrote:
> >>
> >> On 5/15/19 4:07 PM, Michael Boulineau wrote:
> >>> I have a wild and crazy text file, the head of which looks like this:
> >>>
> >>> 2016-07-01 02:50:35 <john> hey
> >>> 2016-07-01 02:51:26 <jane> waiting for plane to Edinburgh
> >>> 2016-07-01 02:51:45 <john> thinking about my boo
> >>> 2016-07-01 02:52:07 <jane> nothing crappy has happened, not really
> >>> 2016-07-01 02:52:20 <john> plane went by pretty fast, didn't sleep
> >>> 2016-07-01 02:54:08 <jane> no idea what time it is or where I am really
> >>> 2016-07-01 02:54:17 <john> just know it's london
> >>> 2016-07-01 02:56:44 <jane> you are probably asleep
> >>> 2016-07-01 02:58:45 <jane> I hope fish was fishy in a good eay
> >>> 2016-07-01 02:58:56 <jone>
> >>> 2016-07-01 02:59:34 <jane>
> >>> 2016-07-01 03:02:48 <john> British security is a little more rigorous...
> >> Looks entirely not-"crazy". Typical log file format.
> >>
> >> Two possibilities: 1) Use `read.fwf` from pkg foreign; 2) Use regex
> >> (i.e. the sub-function) to strip everything up to the "<". Read
> >> `?regex`. Since that's not a metacharacters you could use a pattern
> >> ".+<" and replace with "".
> >>
> >> And do read the Posting Guide. Cross-posting to StackOverflow and Rhelp,
> >> at least within hours of each, is considered poor manners.
> >>
> >>
> >> --
> >>
> >> David.
> >>
> >>> It goes on for a while. It's a big file. But I feel like it's going to
> >>> be difficult to annotate with the coreNLP library or package. I'm
> >>> doing natural language processing. In other words, I'm curious as to
> >>> how I would shave off the dates, that is, to make it look like:
> >>>
> >>> <john> hey
> >>> <jane> waiting for plane to Edinburgh
> >>>    <john> thinking about my boo
> >>> <jane> nothing crappy has happened, not really
> >>> <john> plane went by pretty fast, didn't sleep
> >>> <jane> no idea what time it is or where I am really
> >>> <john> just know it's london
> >>> <jane> you are probably asleep
> >>> <jane> I hope fish was fishy in a good eay
> >>>    <jone>
> >>> <jane>
> >>> <john> British security is a little more rigorous...
> >>>
> >>> To be clear, then, I'm trying to clean a large text file by writing a
> >>> regular expression? such that I create a new object with no numbers or
> >>> dates.
> >>>
> >>> Michael
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From p_conno||y @end|ng |rom @||ng@hot@co@nz  Fri May 17 01:04:20 2019
From: p_conno||y @end|ng |rom @||ng@hot@co@nz (Patrick Connolly)
Date: Fri, 17 May 2019 11:04:20 +1200
Subject: [R] Bounding box on plotting files inserted into a LaTeX document
Message-ID: <20190516230420.GA4898@slingshot.co.nz>

I'm trying to write basic latex code to insert a pdf graphic into a
document.  I can use Rstudio to knit an Rmd file successfully
inserting the plot into the document.  I can get the latex code if the
"save tex" box is ticked, so I get the correct syntax used.  

I don't need all the fancy things that .Rmd files can handle.  I just
want a simple .tex file.  However, if I try to use a pdf file normally
produced by using the pdf device, I get a message about a missing
bounding box.  So I thought it might work to use R to make an eps
file, which pdflatex automatically converts to a pdf.  It does import
the graphic, but it takes up a whole page, which indicates that it
ignores the bounding box.

What does work is to use Rstudio interactively to export the graphic
to a pdf file which pdflatex handles correctly.  But that's a clunky
procedure which I'd like to avoid.  It must be possible since graphs
were included in LaTeX documents for decades before there was Rstudio.

TIA

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}                   Great minds discuss ideas    
 _( Y )_  	         Average minds discuss events 
(:_~*~_:)                  Small minds discuss people  
 (_)-(_)  	                      ..... Eleanor Roosevelt
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From m@rc_@chw@rtz @end|ng |rom me@com  Fri May 17 01:52:43 2019
From: m@rc_@chw@rtz @end|ng |rom me@com (Marc Schwartz)
Date: Thu, 16 May 2019 19:52:43 -0400
Subject: [R] 
 Bounding box on plotting files inserted into a LaTeX document
In-Reply-To: <20190516230420.GA4898@slingshot.co.nz>
References: <20190516230420.GA4898@slingshot.co.nz>
Message-ID: <DA2A8675-683F-45D4-85BC-73C5DA5FF88C@me.com>



> On May 16, 2019, at 7:04 PM, Patrick Connolly <p_connolly at slingshot.co.nz> wrote:
> 
> I'm trying to write basic latex code to insert a pdf graphic into a
> document.  I can use Rstudio to knit an Rmd file successfully
> inserting the plot into the document.  I can get the latex code if the
> "save tex" box is ticked, so I get the correct syntax used.  
> 
> I don't need all the fancy things that .Rmd files can handle.  I just
> want a simple .tex file.  However, if I try to use a pdf file normally
> produced by using the pdf device, I get a message about a missing
> bounding box.  So I thought it might work to use R to make an eps
> file, which pdflatex automatically converts to a pdf.  It does import
> the graphic, but it takes up a whole page, which indicates that it
> ignores the bounding box.
> 
> What does work is to use Rstudio interactively to export the graphic
> to a pdf file which pdflatex handles correctly.  But that's a clunky
> procedure which I'd like to avoid.  It must be possible since graphs
> were included in LaTeX documents for decades before there was Rstudio.
> 
> TIA


Patrick,

Are you explicitly calling postscript() in an R session to create the figure?

If so, see the Details section of ?postscript, which notes the following:

The postscript produced for a single R plot is EPS (Encapsulated PostScript) compatible, and can be included into other documents, e.g., into LaTeX, using \includegraphics{<filename>}. For use in this way you will probably want to use setEPS() to set the defaults as horizontal = FALSE, onefile = FALSE, paper = "special". Note that the bounding box is for the device region: if you find the white space around the plot region excessive, reduce the margins of the figure region via par(mar = ).

So essentially:

postscript("YourPlot.eps", width = 6.0, height = 6.0,
           horizontal = FALSE, onefile = FALSE, paper = "special")

Plot code here...

dev.off()



Note that instead of pdflatex, you can also use latex, followed by dvips and then ps2pdf, if you want to go that way with the CLI workflow. Might depend upon whether or not you use other PS specific markup in the .tex file, like pstricks. For example:

latex YourTeXFile.tex
dvips YourTeXFile -o YourTexFile.ps
ps2pdf YourTeXFile.ps YourTeXFile.pdf


Regards,

Marc Schwartz


From dw|n@em|u@ @end|ng |rom comc@@t@net  Fri May 17 05:29:51 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Thu, 16 May 2019 20:29:51 -0700
Subject: [R] how to separate string from numbers in a large txt file
In-Reply-To: <CAH+cTGMx4_Lz2gGDf2AuUaaZF2Xaq4McuPRoHu4JJNc-ORd=Ew@mail.gmail.com>
References: <CAH+cTGM9sFCxegaMHzu-_9Prhvvh9MbVDCWJ1_x7wkO9jC47mQ@mail.gmail.com>
 <c6e69644-66a1-7031-4d7c-1000e0abbea4@comcast.net>
 <CAH+cTGM5YvWV_G0pRgMqjCzuRrFTzd0nOc7yoThfGbGMUyMjsQ@mail.gmail.com>
 <9c9f1c81-309f-91cb-7e26-f02fc4bd45d8@comcast.net>
 <CAH+cTGMx4_Lz2gGDf2AuUaaZF2Xaq4McuPRoHu4JJNc-ORd=Ew@mail.gmail.com>
Message-ID: <6322e735-5659-848e-3a50-9f066f1299ad@comcast.net>


On 5/16/19 3:53 PM, Michael Boulineau wrote:
> OK. So, I named the object test and then checked the 6347th item
>
>> test <- readLines ("hangouts-conversation.txt)
>> test [6347]
> [1] "2016-10-21 10:56:37 <John Doe> Admit#8242"
>
> Perhaps where it was getting screwed up is, since the end of this is a
> number (8242), then, given that there's no space between the number
> and what ought to be the next row, R didn't know where to draw the
> line. Sure enough, it looks like this when I go to the original file
> and control f "#8242"
>
> 2016-10-21 10:35:36 <Jane Doe> What's your login
> 2016-10-21 10:56:29 <John Doe> John_Doe
> 2016-10-21 10:56:37 <John Doe> Admit#8242


An octothorpe is an end of line signifier and is interpreted as allowing 
comments. You can prevent that interpretation with suitable choice of 
parameters to `read.table` or `read.csv`. I don't understand why that 
should cause anu error or a failure to match that pattern.

> 2016-10-21 11:00:13 <Jane Doe> Okay so you have a discussion
>
> Again, it doesn't look like that in the file. Gmail automatically
> formats it like that when I paste it in. More to the point, it looks
> like
>
> 2016-10-21 10:35:36 <Jane Doe> What's your login2016-10-21 10:56:29
> <John Doe> John_Doe2016-10-21 10:56:37 <John Doe> Admit#82422016-10-21
> 11:00:13 <Jane Doe> Okay so you have a discussion
>
> Notice Admit#82422016. So there's that.
>
> Then I built object test2.
>
> test2 <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", test)
>
> This worked for 84 lines, then this happened.

It may have done something but as you later discovered my first code for 
the pattern was incorrect. I had tested it (and pasted in the results of 
the test) . The way to refer to a capture class is with back-slashes 
before the numbers, not forward-slashes. Try this:


 > newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4", chrvec)
 > newvec
 ?[1] "2016-07-01,02:50:35,<john>,hey"
 ?[2] "2016-07-01,02:51:26,<jane>,waiting for plane to Edinburgh"
 ?[3] "2016-07-01,02:51:45,<john>,thinking about my boo"
 ?[4] "2016-07-01,02:52:07,<jane>,nothing crappy has happened, not really"
 ?[5] "2016-07-01,02:52:20,<john>,plane went by pretty fast, didn't sleep"
 ?[6] "2016-07-01,02:54:08,<jane>,no idea what time it is or where I am 
really"
 ?[7] "2016-07-01,02:54:17,<john>,just know it's london"
 ?[8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
 ?[9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a good eay"
[10] "2016-07-01 02:58:56 <jone>"
[11] "2016-07-01 02:59:34 <jane>"
[12] "2016-07-01,03:02:48,<john>,British security is a little more 
rigorous..."


I made note of the fact that the 10th and 11th lines had no commas.

>
>> test2 [84]
> [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"

That line didn't have any "<" so wasn't matched.


You could remove all none matching lines for pattern of

dates<space>times<space>"<"<name>">"<space><anything>


with:


chrvec <- chrvec[ grepl("^.{10} .{8} <.+> .+$)", chrvec)]


Do read:

?read.csv

?regex


-- 

David


>> test2 [85]
> [1] "//1,//2,//3,//4"
>> test [85]
> [1] "2016-07-01 02:50:35 <John Doe> hey"
>
> Notice how I toggled back and forth between test and test2 there. So,
> whatever happened with the regex, it happened in the switch from 84 to
> 85, I guess. It went on like
>
> [990] "//1,//2,//3,//4"
>   [991] "//1,//2,//3,//4"
>   [992] "//1,//2,//3,//4"
>   [993] "//1,//2,//3,//4"
>   [994] "//1,//2,//3,//4"
>   [995] "//1,//2,//3,//4"
>   [996] "//1,//2,//3,//4"
>   [997] "//1,//2,//3,//4"
>   [998] "//1,//2,//3,//4"
>   [999] "//1,//2,//3,//4"
> [1000] "//1,//2,//3,//4"
>
> up until line 1000, then I reached max.print.

> Michael
>
> On Thu, May 16, 2019 at 1:05 PM David Winsemius <dwinsemius at comcast.net> wrote:
>>
>> On 5/16/19 12:30 PM, Michael Boulineau wrote:
>>> Thanks for this tip on etiquette, David. I will be sure and not do that again.
>>>
>>> I tried the read.fwf from the foreign package, with a code like this:
>>>
>>>    d <- read.fwf("hangouts-conversation.txt",
>>>                   widths= c(10,10,20,40),
>>>                   col.names=c("date","time","person","comment"),
>>>                   strip.white=TRUE)
>>>
>>> But it threw this error:
>>>
>>> Error in scan(file = file, what = what, sep = sep, quote = quote, dec = dec,  :
>>>     line 6347 did not have 4 elements
>>
>> So what does line 6347 look like? (Use `readLines` and print it out.)
>>
>>> Interestingly, though, the error only happened when I increased the
>>> width size. But I had to increase the size, or else I couldn't "see"
>>> anything.  The comment was so small that nothing was being captured by
>>> the size of the column. so to speak.
>>>
>>> It seems like what's throwing me is that there's no comma that
>>> demarcates the end of the text proper. For example:
>> Not sure why you thought there should be a comma. Lines usually end
>> with  <cr> and or a <lf>.
>>
>>
>> Once you have the raw text in a character vector from `readLines` named,
>> say, 'chrvec', then you could selectively substitute commas for spaces
>> with regex. (Now that you no longer desire to remove the dates and times.)
>>
>> sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", chrvec)
>>
>> This will not do any replacements when the pattern is not matched. See
>> this test:
>>
>>
>>   > newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4", chrvec)
>>   > newvec
>>    [1] "2016-07-01,02:50:35,<john>,hey"
>>    [2] "2016-07-01,02:51:26,<jane>,waiting for plane to Edinburgh"
>>    [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
>>    [4] "2016-07-01,02:52:07,<jane>,nothing crappy has happened, not really"
>>    [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast, didn't sleep"
>>    [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or where I am
>> really"
>>    [7] "2016-07-01,02:54:17,<john>,just know it's london"
>>    [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
>>    [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a good eay"
>> [10] "2016-07-01 02:58:56 <jone>"
>> [11] "2016-07-01 02:59:34 <jane>"
>> [12] "2016-07-01,03:02:48,<john>,British security is a little more
>> rigorous..."
>>
>>
>> You should probably remove the "empty comment" lines.
>>
>>
>> --
>>
>> David.
>>
>>> 2016-07-01 15:34:30 <John Doe> Lame. We were in a starbucks2016-07-01
>>> 15:35:02 <Jane Doe> Hmm that's interesting2016-07-01 15:35:09 <Jane
>>> Doe> You must want coffees2016-07-01 15:35:25 <John Doe> There was
>>> lots of Starbucks in my day2016-07-01 15:35:47
>>>
>>> It was interesting, too, when I pasted the text into the email, it
>>> self-formatted into the way I wanted it to look. I had to manually
>>> make it look like it does above, since that's the way that it looks in
>>> the txt file. I wonder if it's being organized by XML or something.
>>>
>>> Anyways, There's always a space between the two sideways carrots, just
>>> like there is right now: <John Doe> See. Space. And there's always a
>>> space between the data and time. Like this. 2016-07-01 15:34:30 See.
>>> Space. But there's never a space between the end of the comment and
>>> the next date. Like this: We were in a starbucks2016-07-01 15:35:02
>>> See. starbucks and 2016 are smooshed together.
>>>
>>> This code is also on the table right now too.
>>>
>>> a <- read.table("E:/working
>>> directory/-189/hangouts-conversation2.txt", quote="\"",
>>> comment.char="", fill=TRUE)
>>>
>>> h<-cbind(hangouts.conversation2[,1:2],hangouts.conversation2[,3:5],hangouts.conversation2[,6:9])
>>>
>>> aa<-gsub("[^[:digit:]]","",h)
>>> my.data.num <- as.numeric(str_extract(h, "[0-9]+"))
>>>
>>> Those last lines are a work in progress. I wish I could import a
>>> picture of what it looks like when it's translated into a data frame.
>>> The fill=TRUE helped to get the data in table that kind of sort of
>>> works, but the comments keep bleeding into the data and time column.
>>> It's like
>>>
>>> 2016-07-01 15:59:17 <Jane Doe> Seriously I've never been
>>> over               there
>>> 2016-07-01 15:59:27 <Jane Doe> It confuses me :(
>>>
>>> And then, maybe, the "seriously" will be in a column all to itself, as
>>> will be the "I've'"and the "never" etc.
>>>
>>> I will use a regular expression if I have to, but it would be nice to
>>> keep the dates and times on there. Originally, I thought they were
>>> meaningless, but I've since changed my mind on that count. The time of
>>> day isn't so important. But, especially since, say, Gmail itself knows
>>> how to quickly recognize what it is, I know it can be done. I know
>>> this data has structure to it.
>>>
>>> Michael
>>>
>>>
>>>
>>> On Wed, May 15, 2019 at 8:47 PM David Winsemius <dwinsemius at comcast.net> wrote:
>>>> On 5/15/19 4:07 PM, Michael Boulineau wrote:
>>>>> I have a wild and crazy text file, the head of which looks like this:
>>>>>
>>>>> 2016-07-01 02:50:35 <john> hey
>>>>> 2016-07-01 02:51:26 <jane> waiting for plane to Edinburgh
>>>>> 2016-07-01 02:51:45 <john> thinking about my boo
>>>>> 2016-07-01 02:52:07 <jane> nothing crappy has happened, not really
>>>>> 2016-07-01 02:52:20 <john> plane went by pretty fast, didn't sleep
>>>>> 2016-07-01 02:54:08 <jane> no idea what time it is or where I am really
>>>>> 2016-07-01 02:54:17 <john> just know it's london
>>>>> 2016-07-01 02:56:44 <jane> you are probably asleep
>>>>> 2016-07-01 02:58:45 <jane> I hope fish was fishy in a good eay
>>>>> 2016-07-01 02:58:56 <jone>
>>>>> 2016-07-01 02:59:34 <jane>
>>>>> 2016-07-01 03:02:48 <john> British security is a little more rigorous...
>>>> Looks entirely not-"crazy". Typical log file format.
>>>>
>>>> Two possibilities: 1) Use `read.fwf` from pkg foreign; 2) Use regex
>>>> (i.e. the sub-function) to strip everything up to the "<". Read
>>>> `?regex`. Since that's not a metacharacters you could use a pattern
>>>> ".+<" and replace with "".
>>>>
>>>> And do read the Posting Guide. Cross-posting to StackOverflow and Rhelp,
>>>> at least within hours of each, is considered poor manners.
>>>>
>>>>
>>>> --
>>>>
>>>> David.
>>>>
>>>>> It goes on for a while. It's a big file. But I feel like it's going to
>>>>> be difficult to annotate with the coreNLP library or package. I'm
>>>>> doing natural language processing. In other words, I'm curious as to
>>>>> how I would shave off the dates, that is, to make it look like:
>>>>>
>>>>> <john> hey
>>>>> <jane> waiting for plane to Edinburgh
>>>>>     <john> thinking about my boo
>>>>> <jane> nothing crappy has happened, not really
>>>>> <john> plane went by pretty fast, didn't sleep
>>>>> <jane> no idea what time it is or where I am really
>>>>> <john> just know it's london
>>>>> <jane> you are probably asleep
>>>>> <jane> I hope fish was fishy in a good eay
>>>>>     <jone>
>>>>> <jane>
>>>>> <john> British security is a little more rigorous...
>>>>>
>>>>> To be clear, then, I'm trying to clean a large text file by writing a
>>>>> regular expression? such that I create a new object with no numbers or
>>>>> dates.
>>>>>
>>>>> Michael
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From p_conno||y @end|ng |rom @||ng@hot@co@nz  Fri May 17 09:57:17 2019
From: p_conno||y @end|ng |rom @||ng@hot@co@nz (Patrick Connolly)
Date: Fri, 17 May 2019 19:57:17 +1200
Subject: [R] 
 Bounding box on plotting files inserted into a LaTeX document
In-Reply-To: <DA2A8675-683F-45D4-85BC-73C5DA5FF88C@me.com>
References: <20190516230420.GA4898@slingshot.co.nz>
 <DA2A8675-683F-45D4-85BC-73C5DA5FF88C@me.com>
Message-ID: <20190517075717.GB4898@slingshot.co.nz>

On Thu, 16-May-2019 at 07:52PM -0400, Marc Schwartz wrote:

|> 
|> 
|> > On May 16, 2019, at 7:04 PM, Patrick Connolly <p_connolly at slingshot.co.nz> wrote:
|> > 

[...]

|> 
|> 
|> Patrick,
|> 
|> Are you explicitly calling postscript() in an R session to create the figure?
|> 
|> If so, see the Details section of ?postscript, which notes the following:
|> 

|> The postscript produced for a single R plot is EPS (Encapsulated
|> PostScript) compatible, and can be included into other documents,
|> e.g., into LaTeX, using \includegraphics{<filename>}. For use in
|> this way you will probably want to use setEPS() to set the defaults
|> as horizontal = FALSE, onefile = FALSE, paper = "special". Note
|> that the bounding box is for the device region: if you find the
|> white space around the plot region excessive, reduce the margins of
|> the figure region via par(mar = ).

|> So essentially:
|> 
|> postscript("YourPlot.eps", width = 6.0, height = 6.0,
|>            horizontal = FALSE, onefile = FALSE, paper = "special")

I failed to understand what the 'onefile' argument was about.  I
thought I needed it to be TRUE because I was making a single plot in a
single file. :-( I knew there was something simple I was missing.  I
would have noticed it if I'd read that Details section properly.

|> 
|> Plot code here...
|> 
|> dev.off()
|> 
|> 
|> 

|> Note that instead of pdflatex, you can also use latex, followed by
|> dvips and then ps2pdf, if you want to go that way with the CLI
|> workflow. Might depend upon whether or not you use other PS
|> specific markup in the .tex file, like pstricks. For example:

I used to have a little shell script that did the dvips and ps2pdf but
I recently found that pdflatex seems to do that anyway, so I thought
that was easier and more transportable.

Thanks, Marc.

|> latex YourTeXFile.tex
|> dvips YourTeXFile -o YourTexFile.ps
|> ps2pdf YourTeXFile.ps YourTeXFile.pdf
|> 
|> 
|> Regards,
|> 
|> Marc Schwartz
|> 

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}                   Great minds discuss ideas    
 _( Y )_  	         Average minds discuss events 
(:_~*~_:)                  Small minds discuss people  
 (_)-(_)  	                      ..... Eleanor Roosevelt
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From jo@hu@@c|ement @end|ng |rom ym@||@com  Fri May 17 03:15:40 2019
From: jo@hu@@c|ement @end|ng |rom ym@||@com (Joshua Clement)
Date: Fri, 17 May 2019 01:15:40 +0000 (UTC)
Subject: [R] Need help with xgboost
References: <312216165.1549287.1558055741189.ref@mail.yahoo.com>
Message-ID: <312216165.1549287.1558055741189@mail.yahoo.com>

Hi all,
I'm trying to learn how to use xgboost. I'm working with the 2016 GSS dataset (attached) and trying to determine what variables influence number of children. I've successfully used this to teach myself GLMs, decision trees, and random forests.
My problem is, I can't get the xgboost program (attached) to predict number of children in the testing set. Predictions range from 0.504 to 0.518, and number of children is supposed to be anywhere from 0 to 8, integers only. If anyone can show me what I'm doing wrong, I will be very grateful.
Thanks,Josh
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: xgb trial.R
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190517/addb7c53/attachment.ksh>

From d|ne@hch@ndr@@ek@r@dk @end|ng |rom gm@||@com  Fri May 17 08:19:19 2019
From: d|ne@hch@ndr@@ek@r@dk @end|ng |rom gm@||@com (DINESHKUMAR)
Date: Fri, 17 May 2019 11:49:19 +0530
Subject: [R] Kriging
Message-ID: <CA+tCXtPwf625MEfO6j69B_Ywwau8PGhGmGPio2umEC8kPgDHDA@mail.gmail.com>

how to create prediction grid like meuse grid
*Thanks & Regards*

*C Dineshkumar*
*Bsc Agriculture*
*M.Tech Remote Sensing and GIS*

	[[alternative HTML version deleted]]


From wdun|@p @end|ng |rom t|bco@com  Fri May 17 17:20:13 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Fri, 17 May 2019 08:20:13 -0700
Subject: [R] how to separate string from numbers in a large txt file
In-Reply-To: <6322e735-5659-848e-3a50-9f066f1299ad@comcast.net>
References: <CAH+cTGM9sFCxegaMHzu-_9Prhvvh9MbVDCWJ1_x7wkO9jC47mQ@mail.gmail.com>
 <c6e69644-66a1-7031-4d7c-1000e0abbea4@comcast.net>
 <CAH+cTGM5YvWV_G0pRgMqjCzuRrFTzd0nOc7yoThfGbGMUyMjsQ@mail.gmail.com>
 <9c9f1c81-309f-91cb-7e26-f02fc4bd45d8@comcast.net>
 <CAH+cTGMx4_Lz2gGDf2AuUaaZF2Xaq4McuPRoHu4JJNc-ORd=Ew@mail.gmail.com>
 <6322e735-5659-848e-3a50-9f066f1299ad@comcast.net>
Message-ID: <CAF8bMcbgrVcN9J7Aygq2D5RWaU4MS-GH+_+zMHU=AO+DU5DVWw@mail.gmail.com>

Consider using readLines() and strcapture() for reading such a file.  E.g.,
suppose readLines(files) produced a character vector like

x <- c("2016-10-21 10:35:36 <Jane Doe> What's your login",
          "2016-10-21 10:56:29 <John Doe> John_Doe",
          "2016-10-21 10:56:37 <John Doe> Admit#8242",
          "October 23, 1819 12:34 <Jane Eyre> I am not an angel")

Then you can make a data.frame with columns When, Who, and What by
supplying a pattern containing three parenthesized capture expressions:
> z <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
[[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
             x, proto=data.frame(stringsAsFactors=FALSE, When="", Who="",
What=""))
> str(z)
'data.frame':   4 obs. of  3 variables:
 $ When: chr  "2016-10-21 10:35:36" "2016-10-21 10:56:29" "2016-10-21
10:56:37" NA
 $ Who : chr  "<Jane Doe>" "<John Doe>" "<John Doe>" NA
 $ What: chr  "What's your login" "John_Doe" "Admit#8242" NA

Lines that don't match the pattern result in NA's - you might make a second
pass over the corresponding elements of x with a new pattern.

You can convert the When column from character to time with as.POSIXct().

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Thu, May 16, 2019 at 8:30 PM David Winsemius <dwinsemius at comcast.net>
wrote:

>
> On 5/16/19 3:53 PM, Michael Boulineau wrote:
> > OK. So, I named the object test and then checked the 6347th item
> >
> >> test <- readLines ("hangouts-conversation.txt)
> >> test [6347]
> > [1] "2016-10-21 10:56:37 <John Doe> Admit#8242"
> >
> > Perhaps where it was getting screwed up is, since the end of this is a
> > number (8242), then, given that there's no space between the number
> > and what ought to be the next row, R didn't know where to draw the
> > line. Sure enough, it looks like this when I go to the original file
> > and control f "#8242"
> >
> > 2016-10-21 10:35:36 <Jane Doe> What's your login
> > 2016-10-21 10:56:29 <John Doe> John_Doe
> > 2016-10-21 10:56:37 <John Doe> Admit#8242
>
>
> An octothorpe is an end of line signifier and is interpreted as allowing
> comments. You can prevent that interpretation with suitable choice of
> parameters to `read.table` or `read.csv`. I don't understand why that
> should cause anu error or a failure to match that pattern.
>
> > 2016-10-21 11:00:13 <Jane Doe> Okay so you have a discussion
> >
> > Again, it doesn't look like that in the file. Gmail automatically
> > formats it like that when I paste it in. More to the point, it looks
> > like
> >
> > 2016-10-21 10:35:36 <Jane Doe> What's your login2016-10-21 10:56:29
> > <John Doe> John_Doe2016-10-21 10:56:37 <John Doe> Admit#82422016-10-21
> > 11:00:13 <Jane Doe> Okay so you have a discussion
> >
> > Notice Admit#82422016. So there's that.
> >
> > Then I built object test2.
> >
> > test2 <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", test)
> >
> > This worked for 84 lines, then this happened.
>
> It may have done something but as you later discovered my first code for
> the pattern was incorrect. I had tested it (and pasted in the results of
> the test) . The way to refer to a capture class is with back-slashes
> before the numbers, not forward-slashes. Try this:
>
>
>  > newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4", chrvec)
>  > newvec
>   [1] "2016-07-01,02:50:35,<john>,hey"
>   [2] "2016-07-01,02:51:26,<jane>,waiting for plane to Edinburgh"
>   [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
>   [4] "2016-07-01,02:52:07,<jane>,nothing crappy has happened, not really"
>   [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast, didn't sleep"
>   [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or where I am
> really"
>   [7] "2016-07-01,02:54:17,<john>,just know it's london"
>   [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
>   [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a good eay"
> [10] "2016-07-01 02:58:56 <jone>"
> [11] "2016-07-01 02:59:34 <jane>"
> [12] "2016-07-01,03:02:48,<john>,British security is a little more
> rigorous..."
>
>
> I made note of the fact that the 10th and 11th lines had no commas.
>
> >
> >> test2 [84]
> > [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
>
> That line didn't have any "<" so wasn't matched.
>
>
> You could remove all none matching lines for pattern of
>
> dates<space>times<space>"<"<name>">"<space><anything>
>
>
> with:
>
>
> chrvec <- chrvec[ grepl("^.{10} .{8} <.+> .+$)", chrvec)]
>
>
> Do read:
>
> ?read.csv
>
> ?regex
>
>
> --
>
> David
>
>
> >> test2 [85]
> > [1] "//1,//2,//3,//4"
> >> test [85]
> > [1] "2016-07-01 02:50:35 <John Doe> hey"
> >
> > Notice how I toggled back and forth between test and test2 there. So,
> > whatever happened with the regex, it happened in the switch from 84 to
> > 85, I guess. It went on like
> >
> > [990] "//1,//2,//3,//4"
> >   [991] "//1,//2,//3,//4"
> >   [992] "//1,//2,//3,//4"
> >   [993] "//1,//2,//3,//4"
> >   [994] "//1,//2,//3,//4"
> >   [995] "//1,//2,//3,//4"
> >   [996] "//1,//2,//3,//4"
> >   [997] "//1,//2,//3,//4"
> >   [998] "//1,//2,//3,//4"
> >   [999] "//1,//2,//3,//4"
> > [1000] "//1,//2,//3,//4"
> >
> > up until line 1000, then I reached max.print.
>
> > Michael
> >
> > On Thu, May 16, 2019 at 1:05 PM David Winsemius <dwinsemius at comcast.net>
> wrote:
> >>
> >> On 5/16/19 12:30 PM, Michael Boulineau wrote:
> >>> Thanks for this tip on etiquette, David. I will be sure and not do
> that again.
> >>>
> >>> I tried the read.fwf from the foreign package, with a code like this:
> >>>
> >>>    d <- read.fwf("hangouts-conversation.txt",
> >>>                   widths= c(10,10,20,40),
> >>>                   col.names=c("date","time","person","comment"),
> >>>                   strip.white=TRUE)
> >>>
> >>> But it threw this error:
> >>>
> >>> Error in scan(file = file, what = what, sep = sep, quote = quote, dec
> = dec,  :
> >>>     line 6347 did not have 4 elements
> >>
> >> So what does line 6347 look like? (Use `readLines` and print it out.)
> >>
> >>> Interestingly, though, the error only happened when I increased the
> >>> width size. But I had to increase the size, or else I couldn't "see"
> >>> anything.  The comment was so small that nothing was being captured by
> >>> the size of the column. so to speak.
> >>>
> >>> It seems like what's throwing me is that there's no comma that
> >>> demarcates the end of the text proper. For example:
> >> Not sure why you thought there should be a comma. Lines usually end
> >> with  <cr> and or a <lf>.
> >>
> >>
> >> Once you have the raw text in a character vector from `readLines` named,
> >> say, 'chrvec', then you could selectively substitute commas for spaces
> >> with regex. (Now that you no longer desire to remove the dates and
> times.)
> >>
> >> sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", chrvec)
> >>
> >> This will not do any replacements when the pattern is not matched. See
> >> this test:
> >>
> >>
> >>   > newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4",
> chrvec)
> >>   > newvec
> >>    [1] "2016-07-01,02:50:35,<john>,hey"
> >>    [2] "2016-07-01,02:51:26,<jane>,waiting for plane to Edinburgh"
> >>    [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
> >>    [4] "2016-07-01,02:52:07,<jane>,nothing crappy has happened, not
> really"
> >>    [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast, didn't
> sleep"
> >>    [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or where I am
> >> really"
> >>    [7] "2016-07-01,02:54:17,<john>,just know it's london"
> >>    [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
> >>    [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a good eay"
> >> [10] "2016-07-01 02:58:56 <jone>"
> >> [11] "2016-07-01 02:59:34 <jane>"
> >> [12] "2016-07-01,03:02:48,<john>,British security is a little more
> >> rigorous..."
> >>
> >>
> >> You should probably remove the "empty comment" lines.
> >>
> >>
> >> --
> >>
> >> David.
> >>
> >>> 2016-07-01 15:34:30 <John Doe> Lame. We were in a starbucks2016-07-01
> >>> 15:35:02 <Jane Doe> Hmm that's interesting2016-07-01 15:35:09 <Jane
> >>> Doe> You must want coffees2016-07-01 15:35:25 <John Doe> There was
> >>> lots of Starbucks in my day2016-07-01 15:35:47
> >>>
> >>> It was interesting, too, when I pasted the text into the email, it
> >>> self-formatted into the way I wanted it to look. I had to manually
> >>> make it look like it does above, since that's the way that it looks in
> >>> the txt file. I wonder if it's being organized by XML or something.
> >>>
> >>> Anyways, There's always a space between the two sideways carrots, just
> >>> like there is right now: <John Doe> See. Space. And there's always a
> >>> space between the data and time. Like this. 2016-07-01 15:34:30 See.
> >>> Space. But there's never a space between the end of the comment and
> >>> the next date. Like this: We were in a starbucks2016-07-01 15:35:02
> >>> See. starbucks and 2016 are smooshed together.
> >>>
> >>> This code is also on the table right now too.
> >>>
> >>> a <- read.table("E:/working
> >>> directory/-189/hangouts-conversation2.txt", quote="\"",
> >>> comment.char="", fill=TRUE)
> >>>
> >>>
> h<-cbind(hangouts.conversation2[,1:2],hangouts.conversation2[,3:5],hangouts.conversation2[,6:9])
> >>>
> >>> aa<-gsub("[^[:digit:]]","",h)
> >>> my.data.num <- as.numeric(str_extract(h, "[0-9]+"))
> >>>
> >>> Those last lines are a work in progress. I wish I could import a
> >>> picture of what it looks like when it's translated into a data frame.
> >>> The fill=TRUE helped to get the data in table that kind of sort of
> >>> works, but the comments keep bleeding into the data and time column.
> >>> It's like
> >>>
> >>> 2016-07-01 15:59:17 <Jane Doe> Seriously I've never been
> >>> over               there
> >>> 2016-07-01 15:59:27 <Jane Doe> It confuses me :(
> >>>
> >>> And then, maybe, the "seriously" will be in a column all to itself, as
> >>> will be the "I've'"and the "never" etc.
> >>>
> >>> I will use a regular expression if I have to, but it would be nice to
> >>> keep the dates and times on there. Originally, I thought they were
> >>> meaningless, but I've since changed my mind on that count. The time of
> >>> day isn't so important. But, especially since, say, Gmail itself knows
> >>> how to quickly recognize what it is, I know it can be done. I know
> >>> this data has structure to it.
> >>>
> >>> Michael
> >>>
> >>>
> >>>
> >>> On Wed, May 15, 2019 at 8:47 PM David Winsemius <
> dwinsemius at comcast.net> wrote:
> >>>> On 5/15/19 4:07 PM, Michael Boulineau wrote:
> >>>>> I have a wild and crazy text file, the head of which looks like this:
> >>>>>
> >>>>> 2016-07-01 02:50:35 <john> hey
> >>>>> 2016-07-01 02:51:26 <jane> waiting for plane to Edinburgh
> >>>>> 2016-07-01 02:51:45 <john> thinking about my boo
> >>>>> 2016-07-01 02:52:07 <jane> nothing crappy has happened, not really
> >>>>> 2016-07-01 02:52:20 <john> plane went by pretty fast, didn't sleep
> >>>>> 2016-07-01 02:54:08 <jane> no idea what time it is or where I am
> really
> >>>>> 2016-07-01 02:54:17 <john> just know it's london
> >>>>> 2016-07-01 02:56:44 <jane> you are probably asleep
> >>>>> 2016-07-01 02:58:45 <jane> I hope fish was fishy in a good eay
> >>>>> 2016-07-01 02:58:56 <jone>
> >>>>> 2016-07-01 02:59:34 <jane>
> >>>>> 2016-07-01 03:02:48 <john> British security is a little more
> rigorous...
> >>>> Looks entirely not-"crazy". Typical log file format.
> >>>>
> >>>> Two possibilities: 1) Use `read.fwf` from pkg foreign; 2) Use regex
> >>>> (i.e. the sub-function) to strip everything up to the "<". Read
> >>>> `?regex`. Since that's not a metacharacters you could use a pattern
> >>>> ".+<" and replace with "".
> >>>>
> >>>> And do read the Posting Guide. Cross-posting to StackOverflow and
> Rhelp,
> >>>> at least within hours of each, is considered poor manners.
> >>>>
> >>>>
> >>>> --
> >>>>
> >>>> David.
> >>>>
> >>>>> It goes on for a while. It's a big file. But I feel like it's going
> to
> >>>>> be difficult to annotate with the coreNLP library or package. I'm
> >>>>> doing natural language processing. In other words, I'm curious as to
> >>>>> how I would shave off the dates, that is, to make it look like:
> >>>>>
> >>>>> <john> hey
> >>>>> <jane> waiting for plane to Edinburgh
> >>>>>     <john> thinking about my boo
> >>>>> <jane> nothing crappy has happened, not really
> >>>>> <john> plane went by pretty fast, didn't sleep
> >>>>> <jane> no idea what time it is or where I am really
> >>>>> <john> just know it's london
> >>>>> <jane> you are probably asleep
> >>>>> <jane> I hope fish was fishy in a good eay
> >>>>>     <jone>
> >>>>> <jane>
> >>>>> <john> British security is a little more rigorous...
> >>>>>
> >>>>> To be clear, then, I'm trying to clean a large text file by writing a
> >>>>> regular expression? such that I create a new object with no numbers
> or
> >>>>> dates.
> >>>>>
> >>>>> Michael
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From B|||@Po||ng @end|ng |rom ze||@@com  Fri May 17 20:02:05 2019
From: B|||@Po||ng @end|ng |rom ze||@@com (Bill Poling)
Date: Fri, 17 May 2019 18:02:05 +0000
Subject: [R] Help understanding the relationship between R-3.6.0 and RStudio
Message-ID: <BN7PR02MB507382FA2FDB673BFB754A9FEA0B0@BN7PR02MB5073.namprd02.prod.outlook.com>

Hello.

I do not think I have had this problem (assuming it is a problem) in the past.

I downloaded and installed R3.6.0 which is indicted in the console when I open R itself.

R version 3.6.0 RC (2019-04-24 r76423) -- "Planting of a Tree"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-w64-mingw32/x64 (64-bit)

However, in RStudio the sessionInfo() remains

R version 3.5.3 (2019-03-11)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 17134)

I also installed the latest version of RStudio 1.2.1335 as well "after" installing R 3.6.0.

I also rebooted my computer.

I am not sure why this time the two do not seem to be (for lack of a better word) in sink?

Thank you for any insight

WHP

Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri May 17 20:12:11 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 17 May 2019 11:12:11 -0700
Subject: [R] 
 Help understanding the relationship between R-3.6.0 and RStudio
In-Reply-To: <BN7PR02MB507382FA2FDB673BFB754A9FEA0B0@BN7PR02MB5073.namprd02.prod.outlook.com>
References: <BN7PR02MB507382FA2FDB673BFB754A9FEA0B0@BN7PR02MB5073.namprd02.prod.outlook.com>
Message-ID: <449BC48E-7468-44A6-80A2-11B129B95512@dcn.davis.ca.us>

This is actually a question about RStudio, not R, so is technically off topic.  However, I think the answer to your question is in the Global Options (General) configuration screen.

On May 17, 2019 11:02:05 AM PDT, Bill Poling <Bill.Poling at zelis.com> wrote:
>Hello.
>
>I do not think I have had this problem (assuming it is a problem) in
>the past.
>
>I downloaded and installed R3.6.0 which is indicted in the console when
>I open R itself.
>
>R version 3.6.0 RC (2019-04-24 r76423) -- "Planting of a Tree"
>Copyright (C) 2019 The R Foundation for Statistical Computing
>Platform: x86_64-w64-mingw32/x64 (64-bit)
>
>However, in RStudio the sessionInfo() remains
>
>R version 3.5.3 (2019-03-11)
>Platform: x86_64-w64-mingw32/x64 (64-bit)
>Running under: Windows 10 x64 (build 17134)
>
>I also installed the latest version of RStudio 1.2.1335 as well "after"
>installing R 3.6.0.
>
>I also rebooted my computer.
>
>I am not sure why this time the two do not seem to be (for lack of a
>better word) in sink?
>
>Thank you for any insight
>
>WHP
>
>Confidentiality Notice This message is sent from Zelis.
>...{{dropped:13}}
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From m@rc_@chw@rtz @end|ng |rom me@com  Fri May 17 20:14:19 2019
From: m@rc_@chw@rtz @end|ng |rom me@com (Marc Schwartz)
Date: Fri, 17 May 2019 14:14:19 -0400
Subject: [R] 
 Help understanding the relationship between R-3.6.0 and RStudio
In-Reply-To: <BN7PR02MB507382FA2FDB673BFB754A9FEA0B0@BN7PR02MB5073.namprd02.prod.outlook.com>
References: <BN7PR02MB507382FA2FDB673BFB754A9FEA0B0@BN7PR02MB5073.namprd02.prod.outlook.com>
Message-ID: <9548F469-06ED-4599-B210-94CC05663D3C@me.com>



> On May 17, 2019, at 2:02 PM, Bill Poling <Bill.Poling at zelis.com> wrote:
> 
> Hello.
> 
> I do not think I have had this problem (assuming it is a problem) in the past.
> 
> I downloaded and installed R3.6.0 which is indicted in the console when I open R itself.
> 
> R version 3.6.0 RC (2019-04-24 r76423) -- "Planting of a Tree"
> Copyright (C) 2019 The R Foundation for Statistical Computing
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> 
> However, in RStudio the sessionInfo() remains
> 
> R version 3.5.3 (2019-03-11)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows 10 x64 (build 17134)
> 
> I also installed the latest version of RStudio 1.2.1335 as well "after" installing R 3.6.0.
> 
> I also rebooted my computer.
> 
> I am not sure why this time the two do not seem to be (for lack of a better word) in sink?
> 
> Thank you for any insight
> 
> WHP


Hi,

I don't use RStudio, which is a GUI/IDE on top of R, it is not R.

That being said, a quick Google search supports my intuition, which is that RStudio appears to be able to support multiple R version installations:

  https://support.rstudio.com/hc/en-us/articles/200486138-Changing-R-versions-for-RStudio-desktop

RStudio also has their own support venue:

  https://support.rstudio.com/hc/en-us

If I read correctly, it looks like you actually installed a "Release Candidate" (RC) version of 3.6.0 for Windows. So you probably want to visit a CRAN mirror and download the release version of 3.6.0:

R version 3.6.0 (2019-04-26) -- "Planting of a Tree"

If you do not want to have multiple R versions on your computer, you can use the normal Windows application uninstall process to remove the older version(s).

Regards,

Marc Schwartz


From B|||@Po||ng @end|ng |rom ze||@@com  Fri May 17 20:19:14 2019
From: B|||@Po||ng @end|ng |rom ze||@@com (Bill Poling)
Date: Fri, 17 May 2019 18:19:14 +0000
Subject: [R] 
 Help understanding the relationship between R-3.6.0 and RStudio
In-Reply-To: <449BC48E-7468-44A6-80A2-11B129B95512@dcn.davis.ca.us>
References: <BN7PR02MB507382FA2FDB673BFB754A9FEA0B0@BN7PR02MB5073.namprd02.prod.outlook.com>
 <449BC48E-7468-44A6-80A2-11B129B95512@dcn.davis.ca.us>
Message-ID: <BN7PR02MB5073B836B04099F816B723ABEA0B0@BN7PR02MB5073.namprd02.prod.outlook.com>

Thank you Jeff, I will review the Global Options.

WHP


From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
Sent: Friday, May 17, 2019 2:12 PM
To: r-help at r-project.org; Bill Poling <Bill.Poling at zelis.com>; r-help (r-help at r-project.org) <r-help at r-project.org>
Subject: Re: [R] Help understanding the relationship between R-3.6.0 and RStudio

This is actually a question about RStudio, not R, so is technically off topic. However, I think the answer to your question is in the Global Options (General) configuration screen.

On May 17, 2019 11:02:05 AM PDT, Bill Poling <mailto:Bill.Poling at zelis.com> wrote:
>Hello.
>
>I do not think I have had this problem (assuming it is a problem) in
>the past.
>
>I downloaded and installed R3.6.0 which is indicted in the console when
>I open R itself.
>
>R version 3.6.0 RC (2019-04-24 r76423) -- "Planting of a Tree"
>Copyright (C) 2019 The R Foundation for Statistical Computing
>Platform: x86_64-w64-mingw32/x64 (64-bit)
>
>However, in RStudio the sessionInfo() remains
>
>R version 3.5.3 (2019-03-11)
>Platform: x86_64-w64-mingw32/x64 (64-bit)
>Running under: Windows 10 x64 (build 17134)
>
>I also installed the latest version of RStudio 1.2.1335 as well "after"
>installing R 3.6.0.
>
>I also rebooted my computer.
>
>I am not sure why this time the two do not seem to be (for lack of a
>better word) in sink?
>
>Thank you for any insight
>
>WHP
>
>Confidentiality Notice This message is sent from Zelis.
>...{{dropped:13}}
>
>______________________________________________
>mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.

Confidentiality Notice This message is sent from Zelis. This transmission may contain information which is privileged and confidential and is intended for the personal and confidential use of the named recipient only. Such information may be protected by applicable State and Federal laws from this disclosure or unauthorized use. If the reader of this message is not the intended recipient, or the employee or agent responsible for delivering the message to the intended recipient, you are hereby notified that any disclosure, review, discussion, copying, or taking any action in reliance on the contents of this transmission is strictly prohibited. If you have received this transmission in error, please contact the sender immediately. Zelis, 2018.

From B|||@Po||ng @end|ng |rom ze||@@com  Fri May 17 20:21:16 2019
From: B|||@Po||ng @end|ng |rom ze||@@com (Bill Poling)
Date: Fri, 17 May 2019 18:21:16 +0000
Subject: [R] 
 Help understanding the relationship between R-3.6.0 and RStudio
In-Reply-To: <9548F469-06ED-4599-B210-94CC05663D3C@me.com>
References: <BN7PR02MB507382FA2FDB673BFB754A9FEA0B0@BN7PR02MB5073.namprd02.prod.outlook.com>
 <9548F469-06ED-4599-B210-94CC05663D3C@me.com>
Message-ID: <BN7PR02MB5073FDD934BDFD3D75B8F585EA0B0@BN7PR02MB5073.namprd02.prod.outlook.com>

Thank you Mark, I was unaware of the RC distinction and the multiple R version issue.

Appreciate your help.

WHP


From: Marc Schwartz <marc_schwartz at me.com>
Sent: Friday, May 17, 2019 2:14 PM
To: Bill Poling <Bill.Poling at zelis.com>
Cc: R-help <r-help at r-project.org>
Subject: Re: [R] Help understanding the relationship between R-3.6.0 and RStudio



> On May 17, 2019, at 2:02 PM, Bill Poling <mailto:Bill.Poling at zelis.com> wrote:
>
> Hello.
>
> I do not think I have had this problem (assuming it is a problem) in the past.
>
> I downloaded and installed R3.6.0 which is indicted in the console when I open R itself.
>
> R version 3.6.0 RC (2019-04-24 r76423) -- "Planting of a Tree"
> Copyright (C) 2019 The R Foundation for Statistical Computing
> Platform: x86_64-w64-mingw32/x64 (64-bit)
>
> However, in RStudio the sessionInfo() remains
>
> R version 3.5.3 (2019-03-11)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows 10 x64 (build 17134)
>
> I also installed the latest version of RStudio 1.2.1335 as well "after" installing R 3.6.0.
>
> I also rebooted my computer.
>
> I am not sure why this time the two do not seem to be (for lack of a better word) in sink?
>
> Thank you for any insight
>
> WHP


Hi,

I don't use RStudio, which is a GUI/IDE on top of R, it is not R.

That being said, a quick Google search supports my intuition, which is that RStudio appears to be able to support multiple R version installations:

https://support.rstudio.com/hc/en-us/articles/200486138-Changing-R-versions-for-RStudio-desktop

RStudio also has their own support venue:

https://support.rstudio.com/hc/en-us

If I read correctly, it looks like you actually installed a "Release Candidate" (RC) version of 3.6.0 for Windows. So you probably want to visit a CRAN mirror and download the release version of 3.6.0:

R version 3.6.0 (2019-04-26) -- "Planting of a Tree"

If you do not want to have multiple R versions on your computer, you can use the normal Windows application uninstall process to remove the older version(s).

Regards,

Marc Schwartz

Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}


From B|||@Po||ng @end|ng |rom ze||@@com  Fri May 17 20:32:32 2019
From: B|||@Po||ng @end|ng |rom ze||@@com (Bill Poling)
Date: Fri, 17 May 2019 18:32:32 +0000
Subject: [R] 
 Help understanding the relationship between R-3.6.0 and RStudio
In-Reply-To: <9548F469-06ED-4599-B210-94CC05663D3C@me.com>
References: <BN7PR02MB507382FA2FDB673BFB754A9FEA0B0@BN7PR02MB5073.namprd02.prod.outlook.com>
 <9548F469-06ED-4599-B210-94CC05663D3C@me.com>
Message-ID: <BN7PR02MB507396D097441034D439D217EA0B0@BN7PR02MB5073.namprd02.prod.outlook.com>

I fixed it by removing previous versions as suggested.

> sessionInfo()
R version 3.6.0 RC (2019-04-24 r76423)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 17134)

I will have to go out and get the non RC version now.

Thank you.

WHP

From: Marc Schwartz <marc_schwartz at me.com>
Sent: Friday, May 17, 2019 2:14 PM
To: Bill Poling <Bill.Poling at zelis.com>
Cc: R-help <r-help at r-project.org>
Subject: Re: [R] Help understanding the relationship between R-3.6.0 and RStudio



> On May 17, 2019, at 2:02 PM, Bill Poling <mailto:Bill.Poling at zelis.com> wrote:
>
> Hello.
>
> I do not think I have had this problem (assuming it is a problem) in the past.
>
> I downloaded and installed R3.6.0 which is indicted in the console when I open R itself.
>
> R version 3.6.0 RC (2019-04-24 r76423) -- "Planting of a Tree"
> Copyright (C) 2019 The R Foundation for Statistical Computing
> Platform: x86_64-w64-mingw32/x64 (64-bit)
>
> However, in RStudio the sessionInfo() remains
>
> R version 3.5.3 (2019-03-11)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows 10 x64 (build 17134)
>
> I also installed the latest version of RStudio 1.2.1335 as well "after" installing R 3.6.0.
>
> I also rebooted my computer.
>
> I am not sure why this time the two do not seem to be (for lack of a better word) in sink?
>
> Thank you for any insight
>
> WHP


Hi,

I don't use RStudio, which is a GUI/IDE on top of R, it is not R.

That being said, a quick Google search supports my intuition, which is that RStudio appears to be able to support multiple R version installations:

https://support.rstudio.com/hc/en-us/articles/200486138-Changing-R-versions-for-RStudio-desktop

RStudio also has their own support venue:

https://support.rstudio.com/hc/en-us

If I read correctly, it looks like you actually installed a "Release Candidate" (RC) version of 3.6.0 for Windows. So you probably want to visit a CRAN mirror and download the release version of 3.6.0:

R version 3.6.0 (2019-04-26) -- "Planting of a Tree"

If you do not want to have multiple R versions on your computer, you can use the normal Windows application uninstall process to remove the older version(s).

Regards,

Marc Schwartz

Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}


From m|ch@e|@p@bou||ne@u @end|ng |rom gm@||@com  Fri May 17 20:36:22 2019
From: m|ch@e|@p@bou||ne@u @end|ng |rom gm@||@com (Michael Boulineau)
Date: Fri, 17 May 2019 11:36:22 -0700
Subject: [R] how to separate string from numbers in a large txt file
In-Reply-To: <CAF8bMcbgrVcN9J7Aygq2D5RWaU4MS-GH+_+zMHU=AO+DU5DVWw@mail.gmail.com>
References: <CAH+cTGM9sFCxegaMHzu-_9Prhvvh9MbVDCWJ1_x7wkO9jC47mQ@mail.gmail.com>
 <c6e69644-66a1-7031-4d7c-1000e0abbea4@comcast.net>
 <CAH+cTGM5YvWV_G0pRgMqjCzuRrFTzd0nOc7yoThfGbGMUyMjsQ@mail.gmail.com>
 <9c9f1c81-309f-91cb-7e26-f02fc4bd45d8@comcast.net>
 <CAH+cTGMx4_Lz2gGDf2AuUaaZF2Xaq4McuPRoHu4JJNc-ORd=Ew@mail.gmail.com>
 <6322e735-5659-848e-3a50-9f066f1299ad@comcast.net>
 <CAF8bMcbgrVcN9J7Aygq2D5RWaU4MS-GH+_+zMHU=AO+DU5DVWw@mail.gmail.com>
Message-ID: <CAH+cTGNfpLSurqmsoMeci0GWhC0ncOp7zTEju3fJW5T4+tCsGA@mail.gmail.com>

This seemed to work:

> a <- readLines ("hangouts-conversation-6.csv.txt")
> b <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4", a)
> b [1:84]

And the first 85 lines looks like this:

[83] "2016-06-28 21:02:28 *** Jane Doe started a video chat"
[84] "2016-06-28 21:12:43 *** John Doe ended a video chat"

Then they transition to the commas:

> b [84:100]
 [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
 [2] "2016-07-01,02:50:35,<John Doe>,hey"
 [3] "2016-07-01,02:51:26,<John Doe>,waiting for plane to Edinburgh"
 [4] "2016-07-01,02:51:45,<John Doe>,thinking about my boo"

Even the strange bit on line 6347 was caught by this:

> b [6346:6348]
[1] "2016-10-21,10:56:29,<John Doe>,John_Doe"
[2] "2016-10-21,10:56:37,<John Doe>,Admit#8242"
[3] "2016-10-21,11:00:13,<Jane Doe>,Okay so you have a discussion"

Perhaps most awesomely, the code catches spaces that are interposed
into the comment itself:

> b [4]
[1] "2016-01-27,09:15:20,<Jane Doe>,Hey "
  > b [85]
[1] "2016-07-01,02:50:35,<John Doe>,hey"

Notice whether there is a space after the "hey" or not.

These are the first two lines:

[1] "???2016-01-27 09:14:40 *** Jane Doe started a video chat"
[2] "2016-01-27,09:15:20,<Jane
Doe>,https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf"

So, who knows what happened with the ??? at the beginning of [1]
directly above. But notice how there are no commas in [1] but there
appear in [2]. I don't see why really long ones like [2] directly
above would be a problem, were they to be translated into a csv or
data frame column.

Now, with the commas in there, couldn't we write this into a csv or a
data.frame? Some of this data will end up being garbage, I imagine.
Like in [2] directly above. Or with [83] and [84] at the top of this
discussion post/email. Embarrassingly, I've been trying to convert
this into a data.frame or csv but I can't manage to. I've been using
the write.csv function, but I don't think I've been getting the
arguments correct.

At the end of the day, I would like a data.frame and/or csv with the
following four columns: date, time, person, comment.

I tried this, too:

> c <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
+ [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
+                 a, proto=data.frame(stringsAsFactors=FALSE, When="", Who="",
+                                     What=""))

But all I got was this:

> c [1:100, ]
    When  Who What
1   <NA> <NA> <NA>
2   <NA> <NA> <NA>
3   <NA> <NA> <NA>
4   <NA> <NA> <NA>
5   <NA> <NA> <NA>
6   <NA> <NA> <NA>

It seems to have caught nothing.

> unique (c)
  When  Who What
1 <NA> <NA> <NA>

But I like that it converted into columns. That's a really great
format. With a little tweaking, it'd be a great code for this data
set.

Michael

On Fri, May 17, 2019 at 8:20 AM William Dunlap via R-help
<r-help at r-project.org> wrote:
>
> Consider using readLines() and strcapture() for reading such a file.  E.g.,
> suppose readLines(files) produced a character vector like
>
> x <- c("2016-10-21 10:35:36 <Jane Doe> What's your login",
>           "2016-10-21 10:56:29 <John Doe> John_Doe",
>           "2016-10-21 10:56:37 <John Doe> Admit#8242",
>           "October 23, 1819 12:34 <Jane Eyre> I am not an angel")
>
> Then you can make a data.frame with columns When, Who, and What by
> supplying a pattern containing three parenthesized capture expressions:
> > z <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
> [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
>              x, proto=data.frame(stringsAsFactors=FALSE, When="", Who="",
> What=""))
> > str(z)
> 'data.frame':   4 obs. of  3 variables:
>  $ When: chr  "2016-10-21 10:35:36" "2016-10-21 10:56:29" "2016-10-21
> 10:56:37" NA
>  $ Who : chr  "<Jane Doe>" "<John Doe>" "<John Doe>" NA
>  $ What: chr  "What's your login" "John_Doe" "Admit#8242" NA
>
> Lines that don't match the pattern result in NA's - you might make a second
> pass over the corresponding elements of x with a new pattern.
>
> You can convert the When column from character to time with as.POSIXct().
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>
> On Thu, May 16, 2019 at 8:30 PM David Winsemius <dwinsemius at comcast.net>
> wrote:
>
> >
> > On 5/16/19 3:53 PM, Michael Boulineau wrote:
> > > OK. So, I named the object test and then checked the 6347th item
> > >
> > >> test <- readLines ("hangouts-conversation.txt)
> > >> test [6347]
> > > [1] "2016-10-21 10:56:37 <John Doe> Admit#8242"
> > >
> > > Perhaps where it was getting screwed up is, since the end of this is a
> > > number (8242), then, given that there's no space between the number
> > > and what ought to be the next row, R didn't know where to draw the
> > > line. Sure enough, it looks like this when I go to the original file
> > > and control f "#8242"
> > >
> > > 2016-10-21 10:35:36 <Jane Doe> What's your login
> > > 2016-10-21 10:56:29 <John Doe> John_Doe
> > > 2016-10-21 10:56:37 <John Doe> Admit#8242
> >
> >
> > An octothorpe is an end of line signifier and is interpreted as allowing
> > comments. You can prevent that interpretation with suitable choice of
> > parameters to `read.table` or `read.csv`. I don't understand why that
> > should cause anu error or a failure to match that pattern.
> >
> > > 2016-10-21 11:00:13 <Jane Doe> Okay so you have a discussion
> > >
> > > Again, it doesn't look like that in the file. Gmail automatically
> > > formats it like that when I paste it in. More to the point, it looks
> > > like
> > >
> > > 2016-10-21 10:35:36 <Jane Doe> What's your login2016-10-21 10:56:29
> > > <John Doe> John_Doe2016-10-21 10:56:37 <John Doe> Admit#82422016-10-21
> > > 11:00:13 <Jane Doe> Okay so you have a discussion
> > >
> > > Notice Admit#82422016. So there's that.
> > >
> > > Then I built object test2.
> > >
> > > test2 <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", test)
> > >
> > > This worked for 84 lines, then this happened.
> >
> > It may have done something but as you later discovered my first code for
> > the pattern was incorrect. I had tested it (and pasted in the results of
> > the test) . The way to refer to a capture class is with back-slashes
> > before the numbers, not forward-slashes. Try this:
> >
> >
> >  > newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4", chrvec)
> >  > newvec
> >   [1] "2016-07-01,02:50:35,<john>,hey"
> >   [2] "2016-07-01,02:51:26,<jane>,waiting for plane to Edinburgh"
> >   [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
> >   [4] "2016-07-01,02:52:07,<jane>,nothing crappy has happened, not really"
> >   [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast, didn't sleep"
> >   [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or where I am
> > really"
> >   [7] "2016-07-01,02:54:17,<john>,just know it's london"
> >   [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
> >   [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a good eay"
> > [10] "2016-07-01 02:58:56 <jone>"
> > [11] "2016-07-01 02:59:34 <jane>"
> > [12] "2016-07-01,03:02:48,<john>,British security is a little more
> > rigorous..."
> >
> >
> > I made note of the fact that the 10th and 11th lines had no commas.
> >
> > >
> > >> test2 [84]
> > > [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
> >
> > That line didn't have any "<" so wasn't matched.
> >
> >
> > You could remove all none matching lines for pattern of
> >
> > dates<space>times<space>"<"<name>">"<space><anything>
> >
> >
> > with:
> >
> >
> > chrvec <- chrvec[ grepl("^.{10} .{8} <.+> .+$)", chrvec)]
> >
> >
> > Do read:
> >
> > ?read.csv
> >
> > ?regex
> >
> >
> > --
> >
> > David
> >
> >
> > >> test2 [85]
> > > [1] "//1,//2,//3,//4"
> > >> test [85]
> > > [1] "2016-07-01 02:50:35 <John Doe> hey"
> > >
> > > Notice how I toggled back and forth between test and test2 there. So,
> > > whatever happened with the regex, it happened in the switch from 84 to
> > > 85, I guess. It went on like
> > >
> > > [990] "//1,//2,//3,//4"
> > >   [991] "//1,//2,//3,//4"
> > >   [992] "//1,//2,//3,//4"
> > >   [993] "//1,//2,//3,//4"
> > >   [994] "//1,//2,//3,//4"
> > >   [995] "//1,//2,//3,//4"
> > >   [996] "//1,//2,//3,//4"
> > >   [997] "//1,//2,//3,//4"
> > >   [998] "//1,//2,//3,//4"
> > >   [999] "//1,//2,//3,//4"
> > > [1000] "//1,//2,//3,//4"
> > >
> > > up until line 1000, then I reached max.print.
> >
> > > Michael
> > >
> > > On Thu, May 16, 2019 at 1:05 PM David Winsemius <dwinsemius at comcast.net>
> > wrote:
> > >>
> > >> On 5/16/19 12:30 PM, Michael Boulineau wrote:
> > >>> Thanks for this tip on etiquette, David. I will be sure and not do
> > that again.
> > >>>
> > >>> I tried the read.fwf from the foreign package, with a code like this:
> > >>>
> > >>>    d <- read.fwf("hangouts-conversation.txt",
> > >>>                   widths= c(10,10,20,40),
> > >>>                   col.names=c("date","time","person","comment"),
> > >>>                   strip.white=TRUE)
> > >>>
> > >>> But it threw this error:
> > >>>
> > >>> Error in scan(file = file, what = what, sep = sep, quote = quote, dec
> > = dec,  :
> > >>>     line 6347 did not have 4 elements
> > >>
> > >> So what does line 6347 look like? (Use `readLines` and print it out.)
> > >>
> > >>> Interestingly, though, the error only happened when I increased the
> > >>> width size. But I had to increase the size, or else I couldn't "see"
> > >>> anything.  The comment was so small that nothing was being captured by
> > >>> the size of the column. so to speak.
> > >>>
> > >>> It seems like what's throwing me is that there's no comma that
> > >>> demarcates the end of the text proper. For example:
> > >> Not sure why you thought there should be a comma. Lines usually end
> > >> with  <cr> and or a <lf>.
> > >>
> > >>
> > >> Once you have the raw text in a character vector from `readLines` named,
> > >> say, 'chrvec', then you could selectively substitute commas for spaces
> > >> with regex. (Now that you no longer desire to remove the dates and
> > times.)
> > >>
> > >> sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", chrvec)
> > >>
> > >> This will not do any replacements when the pattern is not matched. See
> > >> this test:
> > >>
> > >>
> > >>   > newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4",
> > chrvec)
> > >>   > newvec
> > >>    [1] "2016-07-01,02:50:35,<john>,hey"
> > >>    [2] "2016-07-01,02:51:26,<jane>,waiting for plane to Edinburgh"
> > >>    [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
> > >>    [4] "2016-07-01,02:52:07,<jane>,nothing crappy has happened, not
> > really"
> > >>    [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast, didn't
> > sleep"
> > >>    [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or where I am
> > >> really"
> > >>    [7] "2016-07-01,02:54:17,<john>,just know it's london"
> > >>    [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
> > >>    [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a good eay"
> > >> [10] "2016-07-01 02:58:56 <jone>"
> > >> [11] "2016-07-01 02:59:34 <jane>"
> > >> [12] "2016-07-01,03:02:48,<john>,British security is a little more
> > >> rigorous..."
> > >>
> > >>
> > >> You should probably remove the "empty comment" lines.
> > >>
> > >>
> > >> --
> > >>
> > >> David.
> > >>
> > >>> 2016-07-01 15:34:30 <John Doe> Lame. We were in a starbucks2016-07-01
> > >>> 15:35:02 <Jane Doe> Hmm that's interesting2016-07-01 15:35:09 <Jane
> > >>> Doe> You must want coffees2016-07-01 15:35:25 <John Doe> There was
> > >>> lots of Starbucks in my day2016-07-01 15:35:47
> > >>>
> > >>> It was interesting, too, when I pasted the text into the email, it
> > >>> self-formatted into the way I wanted it to look. I had to manually
> > >>> make it look like it does above, since that's the way that it looks in
> > >>> the txt file. I wonder if it's being organized by XML or something.
> > >>>
> > >>> Anyways, There's always a space between the two sideways carrots, just
> > >>> like there is right now: <John Doe> See. Space. And there's always a
> > >>> space between the data and time. Like this. 2016-07-01 15:34:30 See.
> > >>> Space. But there's never a space between the end of the comment and
> > >>> the next date. Like this: We were in a starbucks2016-07-01 15:35:02
> > >>> See. starbucks and 2016 are smooshed together.
> > >>>
> > >>> This code is also on the table right now too.
> > >>>
> > >>> a <- read.table("E:/working
> > >>> directory/-189/hangouts-conversation2.txt", quote="\"",
> > >>> comment.char="", fill=TRUE)
> > >>>
> > >>>
> > h<-cbind(hangouts.conversation2[,1:2],hangouts.conversation2[,3:5],hangouts.conversation2[,6:9])
> > >>>
> > >>> aa<-gsub("[^[:digit:]]","",h)
> > >>> my.data.num <- as.numeric(str_extract(h, "[0-9]+"))
> > >>>
> > >>> Those last lines are a work in progress. I wish I could import a
> > >>> picture of what it looks like when it's translated into a data frame.
> > >>> The fill=TRUE helped to get the data in table that kind of sort of
> > >>> works, but the comments keep bleeding into the data and time column.
> > >>> It's like
> > >>>
> > >>> 2016-07-01 15:59:17 <Jane Doe> Seriously I've never been
> > >>> over               there
> > >>> 2016-07-01 15:59:27 <Jane Doe> It confuses me :(
> > >>>
> > >>> And then, maybe, the "seriously" will be in a column all to itself, as
> > >>> will be the "I've'"and the "never" etc.
> > >>>
> > >>> I will use a regular expression if I have to, but it would be nice to
> > >>> keep the dates and times on there. Originally, I thought they were
> > >>> meaningless, but I've since changed my mind on that count. The time of
> > >>> day isn't so important. But, especially since, say, Gmail itself knows
> > >>> how to quickly recognize what it is, I know it can be done. I know
> > >>> this data has structure to it.
> > >>>
> > >>> Michael
> > >>>
> > >>>
> > >>>
> > >>> On Wed, May 15, 2019 at 8:47 PM David Winsemius <
> > dwinsemius at comcast.net> wrote:
> > >>>> On 5/15/19 4:07 PM, Michael Boulineau wrote:
> > >>>>> I have a wild and crazy text file, the head of which looks like this:
> > >>>>>
> > >>>>> 2016-07-01 02:50:35 <john> hey
> > >>>>> 2016-07-01 02:51:26 <jane> waiting for plane to Edinburgh
> > >>>>> 2016-07-01 02:51:45 <john> thinking about my boo
> > >>>>> 2016-07-01 02:52:07 <jane> nothing crappy has happened, not really
> > >>>>> 2016-07-01 02:52:20 <john> plane went by pretty fast, didn't sleep
> > >>>>> 2016-07-01 02:54:08 <jane> no idea what time it is or where I am
> > really
> > >>>>> 2016-07-01 02:54:17 <john> just know it's london
> > >>>>> 2016-07-01 02:56:44 <jane> you are probably asleep
> > >>>>> 2016-07-01 02:58:45 <jane> I hope fish was fishy in a good eay
> > >>>>> 2016-07-01 02:58:56 <jone>
> > >>>>> 2016-07-01 02:59:34 <jane>
> > >>>>> 2016-07-01 03:02:48 <john> British security is a little more
> > rigorous...
> > >>>> Looks entirely not-"crazy". Typical log file format.
> > >>>>
> > >>>> Two possibilities: 1) Use `read.fwf` from pkg foreign; 2) Use regex
> > >>>> (i.e. the sub-function) to strip everything up to the "<". Read
> > >>>> `?regex`. Since that's not a metacharacters you could use a pattern
> > >>>> ".+<" and replace with "".
> > >>>>
> > >>>> And do read the Posting Guide. Cross-posting to StackOverflow and
> > Rhelp,
> > >>>> at least within hours of each, is considered poor manners.
> > >>>>
> > >>>>
> > >>>> --
> > >>>>
> > >>>> David.
> > >>>>
> > >>>>> It goes on for a while. It's a big file. But I feel like it's going
> > to
> > >>>>> be difficult to annotate with the coreNLP library or package. I'm
> > >>>>> doing natural language processing. In other words, I'm curious as to
> > >>>>> how I would shave off the dates, that is, to make it look like:
> > >>>>>
> > >>>>> <john> hey
> > >>>>> <jane> waiting for plane to Edinburgh
> > >>>>>     <john> thinking about my boo
> > >>>>> <jane> nothing crappy has happened, not really
> > >>>>> <john> plane went by pretty fast, didn't sleep
> > >>>>> <jane> no idea what time it is or where I am really
> > >>>>> <john> just know it's london
> > >>>>> <jane> you are probably asleep
> > >>>>> <jane> I hope fish was fishy in a good eay
> > >>>>>     <jone>
> > >>>>> <jane>
> > >>>>> <john> British security is a little more rigorous...
> > >>>>>
> > >>>>> To be clear, then, I'm trying to clean a large text file by writing a
> > >>>>> regular expression? such that I create a new object with no numbers
> > or
> > >>>>> dates.
> > >>>>>
> > >>>>> Michael
> > >>>>>
> > >>>>> ______________________________________________
> > >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>>>> PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > >>>>> and provide commented, minimal, self-contained, reproducible code.
> > >>> ______________________________________________
> > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>> PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > >>> and provide commented, minimal, self-contained, reproducible code.
> > >> ______________________________________________
> > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > >> and provide commented, minimal, self-contained, reproducible code.
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wdun|@p @end|ng |rom t|bco@com  Fri May 17 21:12:17 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Fri, 17 May 2019 12:12:17 -0700
Subject: [R] how to separate string from numbers in a large txt file
In-Reply-To: <CAH+cTGNfpLSurqmsoMeci0GWhC0ncOp7zTEju3fJW5T4+tCsGA@mail.gmail.com>
References: <CAH+cTGM9sFCxegaMHzu-_9Prhvvh9MbVDCWJ1_x7wkO9jC47mQ@mail.gmail.com>
 <c6e69644-66a1-7031-4d7c-1000e0abbea4@comcast.net>
 <CAH+cTGM5YvWV_G0pRgMqjCzuRrFTzd0nOc7yoThfGbGMUyMjsQ@mail.gmail.com>
 <9c9f1c81-309f-91cb-7e26-f02fc4bd45d8@comcast.net>
 <CAH+cTGMx4_Lz2gGDf2AuUaaZF2Xaq4McuPRoHu4JJNc-ORd=Ew@mail.gmail.com>
 <6322e735-5659-848e-3a50-9f066f1299ad@comcast.net>
 <CAF8bMcbgrVcN9J7Aygq2D5RWaU4MS-GH+_+zMHU=AO+DU5DVWw@mail.gmail.com>
 <CAH+cTGNfpLSurqmsoMeci0GWhC0ncOp7zTEju3fJW5T4+tCsGA@mail.gmail.com>
Message-ID: <CAF8bMcaSJRaUXK4h2ssaDoJwXaKCUZT2Vq1cutNiNDjQ7ga2Xg@mail.gmail.com>

The pattern I gave worked for the lines that you originally showed from the
data file ('a'), before you put commas into them.  If the name is either of
the form "<name>" or "***" then the "(<[^>]*>)" needs to be changed so
something like "(<[^>]*>|[*]{3})".

The " ???" at the start of the imported data may come from the byte order
mark that Windows apps like to put at the front of a text file in UTF-8 or
UTF-16 format.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Fri, May 17, 2019 at 11:53 AM Michael Boulineau <
michael.p.boulineau at gmail.com> wrote:

> This seemed to work:
>
> > a <- readLines ("hangouts-conversation-6.csv.txt")
> > b <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4", a)
> > b [1:84]
>
> And the first 85 lines looks like this:
>
> [83] "2016-06-28 21:02:28 *** Jane Doe started a video chat"
> [84] "2016-06-28 21:12:43 *** John Doe ended a video chat"
>
> Then they transition to the commas:
>
> > b [84:100]
>  [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
>  [2] "2016-07-01,02:50:35,<John Doe>,hey"
>  [3] "2016-07-01,02:51:26,<John Doe>,waiting for plane to Edinburgh"
>  [4] "2016-07-01,02:51:45,<John Doe>,thinking about my boo"
>
> Even the strange bit on line 6347 was caught by this:
>
> > b [6346:6348]
> [1] "2016-10-21,10:56:29,<John Doe>,John_Doe"
> [2] "2016-10-21,10:56:37,<John Doe>,Admit#8242"
> [3] "2016-10-21,11:00:13,<Jane Doe>,Okay so you have a discussion"
>
> Perhaps most awesomely, the code catches spaces that are interposed
> into the comment itself:
>
> > b [4]
> [1] "2016-01-27,09:15:20,<Jane Doe>,Hey "
>   > b [85]
> [1] "2016-07-01,02:50:35,<John Doe>,hey"
>
> Notice whether there is a space after the "hey" or not.
>
> These are the first two lines:
>
> [1] "???2016-01-27 09:14:40 *** Jane Doe started a video chat"
> [2] "2016-01-27,09:15:20,<Jane
> Doe>,
> https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf
> "
>
> So, who knows what happened with the ??? at the beginning of [1]
> directly above. But notice how there are no commas in [1] but there
> appear in [2]. I don't see why really long ones like [2] directly
> above would be a problem, were they to be translated into a csv or
> data frame column.
>
> Now, with the commas in there, couldn't we write this into a csv or a
> data.frame? Some of this data will end up being garbage, I imagine.
> Like in [2] directly above. Or with [83] and [84] at the top of this
> discussion post/email. Embarrassingly, I've been trying to convert
> this into a data.frame or csv but I can't manage to. I've been using
> the write.csv function, but I don't think I've been getting the
> arguments correct.
>
> At the end of the day, I would like a data.frame and/or csv with the
> following four columns: date, time, person, comment.
>
> I tried this, too:
>
> > c <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
> + [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
> +                 a, proto=data.frame(stringsAsFactors=FALSE, When="",
> Who="",
> +                                     What=""))
>
> But all I got was this:
>
> > c [1:100, ]
>     When  Who What
> 1   <NA> <NA> <NA>
> 2   <NA> <NA> <NA>
> 3   <NA> <NA> <NA>
> 4   <NA> <NA> <NA>
> 5   <NA> <NA> <NA>
> 6   <NA> <NA> <NA>
>
> It seems to have caught nothing.
>
> > unique (c)
>   When  Who What
> 1 <NA> <NA> <NA>
>
> But I like that it converted into columns. That's a really great
> format. With a little tweaking, it'd be a great code for this data
> set.
>
> Michael
>
> On Fri, May 17, 2019 at 8:20 AM William Dunlap via R-help
> <r-help at r-project.org> wrote:
> >
> > Consider using readLines() and strcapture() for reading such a file.
> E.g.,
> > suppose readLines(files) produced a character vector like
> >
> > x <- c("2016-10-21 10:35:36 <Jane Doe> What's your login",
> >           "2016-10-21 10:56:29 <John Doe> John_Doe",
> >           "2016-10-21 10:56:37 <John Doe> Admit#8242",
> >           "October 23, 1819 12:34 <Jane Eyre> I am not an angel")
> >
> > Then you can make a data.frame with columns When, Who, and What by
> > supplying a pattern containing three parenthesized capture expressions:
> > > z <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
> > [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
> >              x, proto=data.frame(stringsAsFactors=FALSE, When="", Who="",
> > What=""))
> > > str(z)
> > 'data.frame':   4 obs. of  3 variables:
> >  $ When: chr  "2016-10-21 10:35:36" "2016-10-21 10:56:29" "2016-10-21
> > 10:56:37" NA
> >  $ Who : chr  "<Jane Doe>" "<John Doe>" "<John Doe>" NA
> >  $ What: chr  "What's your login" "John_Doe" "Admit#8242" NA
> >
> > Lines that don't match the pattern result in NA's - you might make a
> second
> > pass over the corresponding elements of x with a new pattern.
> >
> > You can convert the When column from character to time with as.POSIXct().
> >
> > Bill Dunlap
> > TIBCO Software
> > wdunlap tibco.com
> >
> >
> > On Thu, May 16, 2019 at 8:30 PM David Winsemius <dwinsemius at comcast.net>
> > wrote:
> >
> > >
> > > On 5/16/19 3:53 PM, Michael Boulineau wrote:
> > > > OK. So, I named the object test and then checked the 6347th item
> > > >
> > > >> test <- readLines ("hangouts-conversation.txt)
> > > >> test [6347]
> > > > [1] "2016-10-21 10:56:37 <John Doe> Admit#8242"
> > > >
> > > > Perhaps where it was getting screwed up is, since the end of this is
> a
> > > > number (8242), then, given that there's no space between the number
> > > > and what ought to be the next row, R didn't know where to draw the
> > > > line. Sure enough, it looks like this when I go to the original file
> > > > and control f "#8242"
> > > >
> > > > 2016-10-21 10:35:36 <Jane Doe> What's your login
> > > > 2016-10-21 10:56:29 <John Doe> John_Doe
> > > > 2016-10-21 10:56:37 <John Doe> Admit#8242
> > >
> > >
> > > An octothorpe is an end of line signifier and is interpreted as
> allowing
> > > comments. You can prevent that interpretation with suitable choice of
> > > parameters to `read.table` or `read.csv`. I don't understand why that
> > > should cause anu error or a failure to match that pattern.
> > >
> > > > 2016-10-21 11:00:13 <Jane Doe> Okay so you have a discussion
> > > >
> > > > Again, it doesn't look like that in the file. Gmail automatically
> > > > formats it like that when I paste it in. More to the point, it looks
> > > > like
> > > >
> > > > 2016-10-21 10:35:36 <Jane Doe> What's your login2016-10-21 10:56:29
> > > > <John Doe> John_Doe2016-10-21 10:56:37 <John Doe>
> Admit#82422016-10-21
> > > > 11:00:13 <Jane Doe> Okay so you have a discussion
> > > >
> > > > Notice Admit#82422016. So there's that.
> > > >
> > > > Then I built object test2.
> > > >
> > > > test2 <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", test)
> > > >
> > > > This worked for 84 lines, then this happened.
> > >
> > > It may have done something but as you later discovered my first code
> for
> > > the pattern was incorrect. I had tested it (and pasted in the results
> of
> > > the test) . The way to refer to a capture class is with back-slashes
> > > before the numbers, not forward-slashes. Try this:
> > >
> > >
> > >  > newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4",
> chrvec)
> > >  > newvec
> > >   [1] "2016-07-01,02:50:35,<john>,hey"
> > >   [2] "2016-07-01,02:51:26,<jane>,waiting for plane to Edinburgh"
> > >   [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
> > >   [4] "2016-07-01,02:52:07,<jane>,nothing crappy has happened, not
> really"
> > >   [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast, didn't
> sleep"
> > >   [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or where I am
> > > really"
> > >   [7] "2016-07-01,02:54:17,<john>,just know it's london"
> > >   [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
> > >   [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a good eay"
> > > [10] "2016-07-01 02:58:56 <jone>"
> > > [11] "2016-07-01 02:59:34 <jane>"
> > > [12] "2016-07-01,03:02:48,<john>,British security is a little more
> > > rigorous..."
> > >
> > >
> > > I made note of the fact that the 10th and 11th lines had no commas.
> > >
> > > >
> > > >> test2 [84]
> > > > [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
> > >
> > > That line didn't have any "<" so wasn't matched.
> > >
> > >
> > > You could remove all none matching lines for pattern of
> > >
> > > dates<space>times<space>"<"<name>">"<space><anything>
> > >
> > >
> > > with:
> > >
> > >
> > > chrvec <- chrvec[ grepl("^.{10} .{8} <.+> .+$)", chrvec)]
> > >
> > >
> > > Do read:
> > >
> > > ?read.csv
> > >
> > > ?regex
> > >
> > >
> > > --
> > >
> > > David
> > >
> > >
> > > >> test2 [85]
> > > > [1] "//1,//2,//3,//4"
> > > >> test [85]
> > > > [1] "2016-07-01 02:50:35 <John Doe> hey"
> > > >
> > > > Notice how I toggled back and forth between test and test2 there. So,
> > > > whatever happened with the regex, it happened in the switch from 84
> to
> > > > 85, I guess. It went on like
> > > >
> > > > [990] "//1,//2,//3,//4"
> > > >   [991] "//1,//2,//3,//4"
> > > >   [992] "//1,//2,//3,//4"
> > > >   [993] "//1,//2,//3,//4"
> > > >   [994] "//1,//2,//3,//4"
> > > >   [995] "//1,//2,//3,//4"
> > > >   [996] "//1,//2,//3,//4"
> > > >   [997] "//1,//2,//3,//4"
> > > >   [998] "//1,//2,//3,//4"
> > > >   [999] "//1,//2,//3,//4"
> > > > [1000] "//1,//2,//3,//4"
> > > >
> > > > up until line 1000, then I reached max.print.
> > >
> > > > Michael
> > > >
> > > > On Thu, May 16, 2019 at 1:05 PM David Winsemius <
> dwinsemius at comcast.net>
> > > wrote:
> > > >>
> > > >> On 5/16/19 12:30 PM, Michael Boulineau wrote:
> > > >>> Thanks for this tip on etiquette, David. I will be sure and not do
> > > that again.
> > > >>>
> > > >>> I tried the read.fwf from the foreign package, with a code like
> this:
> > > >>>
> > > >>>    d <- read.fwf("hangouts-conversation.txt",
> > > >>>                   widths= c(10,10,20,40),
> > > >>>                   col.names=c("date","time","person","comment"),
> > > >>>                   strip.white=TRUE)
> > > >>>
> > > >>> But it threw this error:
> > > >>>
> > > >>> Error in scan(file = file, what = what, sep = sep, quote = quote,
> dec
> > > = dec,  :
> > > >>>     line 6347 did not have 4 elements
> > > >>
> > > >> So what does line 6347 look like? (Use `readLines` and print it
> out.)
> > > >>
> > > >>> Interestingly, though, the error only happened when I increased the
> > > >>> width size. But I had to increase the size, or else I couldn't
> "see"
> > > >>> anything.  The comment was so small that nothing was being
> captured by
> > > >>> the size of the column. so to speak.
> > > >>>
> > > >>> It seems like what's throwing me is that there's no comma that
> > > >>> demarcates the end of the text proper. For example:
> > > >> Not sure why you thought there should be a comma. Lines usually end
> > > >> with  <cr> and or a <lf>.
> > > >>
> > > >>
> > > >> Once you have the raw text in a character vector from `readLines`
> named,
> > > >> say, 'chrvec', then you could selectively substitute commas for
> spaces
> > > >> with regex. (Now that you no longer desire to remove the dates and
> > > times.)
> > > >>
> > > >> sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", chrvec)
> > > >>
> > > >> This will not do any replacements when the pattern is not matched.
> See
> > > >> this test:
> > > >>
> > > >>
> > > >>   > newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4",
> > > chrvec)
> > > >>   > newvec
> > > >>    [1] "2016-07-01,02:50:35,<john>,hey"
> > > >>    [2] "2016-07-01,02:51:26,<jane>,waiting for plane to Edinburgh"
> > > >>    [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
> > > >>    [4] "2016-07-01,02:52:07,<jane>,nothing crappy has happened, not
> > > really"
> > > >>    [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast, didn't
> > > sleep"
> > > >>    [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or where
> I am
> > > >> really"
> > > >>    [7] "2016-07-01,02:54:17,<john>,just know it's london"
> > > >>    [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
> > > >>    [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a good
> eay"
> > > >> [10] "2016-07-01 02:58:56 <jone>"
> > > >> [11] "2016-07-01 02:59:34 <jane>"
> > > >> [12] "2016-07-01,03:02:48,<john>,British security is a little more
> > > >> rigorous..."
> > > >>
> > > >>
> > > >> You should probably remove the "empty comment" lines.
> > > >>
> > > >>
> > > >> --
> > > >>
> > > >> David.
> > > >>
> > > >>> 2016-07-01 15:34:30 <John Doe> Lame. We were in a
> starbucks2016-07-01
> > > >>> 15:35:02 <Jane Doe> Hmm that's interesting2016-07-01 15:35:09 <Jane
> > > >>> Doe> You must want coffees2016-07-01 15:35:25 <John Doe> There was
> > > >>> lots of Starbucks in my day2016-07-01 15:35:47
> > > >>>
> > > >>> It was interesting, too, when I pasted the text into the email, it
> > > >>> self-formatted into the way I wanted it to look. I had to manually
> > > >>> make it look like it does above, since that's the way that it
> looks in
> > > >>> the txt file. I wonder if it's being organized by XML or something.
> > > >>>
> > > >>> Anyways, There's always a space between the two sideways carrots,
> just
> > > >>> like there is right now: <John Doe> See. Space. And there's always
> a
> > > >>> space between the data and time. Like this. 2016-07-01 15:34:30
> See.
> > > >>> Space. But there's never a space between the end of the comment and
> > > >>> the next date. Like this: We were in a starbucks2016-07-01 15:35:02
> > > >>> See. starbucks and 2016 are smooshed together.
> > > >>>
> > > >>> This code is also on the table right now too.
> > > >>>
> > > >>> a <- read.table("E:/working
> > > >>> directory/-189/hangouts-conversation2.txt", quote="\"",
> > > >>> comment.char="", fill=TRUE)
> > > >>>
> > > >>>
> > >
> h<-cbind(hangouts.conversation2[,1:2],hangouts.conversation2[,3:5],hangouts.conversation2[,6:9])
> > > >>>
> > > >>> aa<-gsub("[^[:digit:]]","",h)
> > > >>> my.data.num <- as.numeric(str_extract(h, "[0-9]+"))
> > > >>>
> > > >>> Those last lines are a work in progress. I wish I could import a
> > > >>> picture of what it looks like when it's translated into a data
> frame.
> > > >>> The fill=TRUE helped to get the data in table that kind of sort of
> > > >>> works, but the comments keep bleeding into the data and time
> column.
> > > >>> It's like
> > > >>>
> > > >>> 2016-07-01 15:59:17 <Jane Doe> Seriously I've never been
> > > >>> over               there
> > > >>> 2016-07-01 15:59:27 <Jane Doe> It confuses me :(
> > > >>>
> > > >>> And then, maybe, the "seriously" will be in a column all to
> itself, as
> > > >>> will be the "I've'"and the "never" etc.
> > > >>>
> > > >>> I will use a regular expression if I have to, but it would be nice
> to
> > > >>> keep the dates and times on there. Originally, I thought they were
> > > >>> meaningless, but I've since changed my mind on that count. The
> time of
> > > >>> day isn't so important. But, especially since, say, Gmail itself
> knows
> > > >>> how to quickly recognize what it is, I know it can be done. I know
> > > >>> this data has structure to it.
> > > >>>
> > > >>> Michael
> > > >>>
> > > >>>
> > > >>>
> > > >>> On Wed, May 15, 2019 at 8:47 PM David Winsemius <
> > > dwinsemius at comcast.net> wrote:
> > > >>>> On 5/15/19 4:07 PM, Michael Boulineau wrote:
> > > >>>>> I have a wild and crazy text file, the head of which looks like
> this:
> > > >>>>>
> > > >>>>> 2016-07-01 02:50:35 <john> hey
> > > >>>>> 2016-07-01 02:51:26 <jane> waiting for plane to Edinburgh
> > > >>>>> 2016-07-01 02:51:45 <john> thinking about my boo
> > > >>>>> 2016-07-01 02:52:07 <jane> nothing crappy has happened, not
> really
> > > >>>>> 2016-07-01 02:52:20 <john> plane went by pretty fast, didn't
> sleep
> > > >>>>> 2016-07-01 02:54:08 <jane> no idea what time it is or where I am
> > > really
> > > >>>>> 2016-07-01 02:54:17 <john> just know it's london
> > > >>>>> 2016-07-01 02:56:44 <jane> you are probably asleep
> > > >>>>> 2016-07-01 02:58:45 <jane> I hope fish was fishy in a good eay
> > > >>>>> 2016-07-01 02:58:56 <jone>
> > > >>>>> 2016-07-01 02:59:34 <jane>
> > > >>>>> 2016-07-01 03:02:48 <john> British security is a little more
> > > rigorous...
> > > >>>> Looks entirely not-"crazy". Typical log file format.
> > > >>>>
> > > >>>> Two possibilities: 1) Use `read.fwf` from pkg foreign; 2) Use
> regex
> > > >>>> (i.e. the sub-function) to strip everything up to the "<". Read
> > > >>>> `?regex`. Since that's not a metacharacters you could use a
> pattern
> > > >>>> ".+<" and replace with "".
> > > >>>>
> > > >>>> And do read the Posting Guide. Cross-posting to StackOverflow and
> > > Rhelp,
> > > >>>> at least within hours of each, is considered poor manners.
> > > >>>>
> > > >>>>
> > > >>>> --
> > > >>>>
> > > >>>> David.
> > > >>>>
> > > >>>>> It goes on for a while. It's a big file. But I feel like it's
> going
> > > to
> > > >>>>> be difficult to annotate with the coreNLP library or package. I'm
> > > >>>>> doing natural language processing. In other words, I'm curious
> as to
> > > >>>>> how I would shave off the dates, that is, to make it look like:
> > > >>>>>
> > > >>>>> <john> hey
> > > >>>>> <jane> waiting for plane to Edinburgh
> > > >>>>>     <john> thinking about my boo
> > > >>>>> <jane> nothing crappy has happened, not really
> > > >>>>> <john> plane went by pretty fast, didn't sleep
> > > >>>>> <jane> no idea what time it is or where I am really
> > > >>>>> <john> just know it's london
> > > >>>>> <jane> you are probably asleep
> > > >>>>> <jane> I hope fish was fishy in a good eay
> > > >>>>>     <jone>
> > > >>>>> <jane>
> > > >>>>> <john> British security is a little more rigorous...
> > > >>>>>
> > > >>>>> To be clear, then, I'm trying to clean a large text file by
> writing a
> > > >>>>> regular expression? such that I create a new object with no
> numbers
> > > or
> > > >>>>> dates.
> > > >>>>>
> > > >>>>> Michael
> > > >>>>>
> > > >>>>> ______________________________________________
> > > >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> see
> > > >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> > > >>>>> PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > >>>>> and provide commented, minimal, self-contained, reproducible
> code.
> > > >>> ______________________________________________
> > > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > > >>> PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > >>> and provide commented, minimal, self-contained, reproducible code.
> > > >> ______________________________________________
> > > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > > >> PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > >> and provide commented, minimal, self-contained, reproducible code.
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From kry|ov@r00t @end|ng |rom gm@||@com  Fri May 17 21:43:46 2019
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Fri, 17 May 2019 22:43:46 +0300
Subject: [R] how to separate string from numbers in a large txt file
In-Reply-To: <CAH+cTGNfpLSurqmsoMeci0GWhC0ncOp7zTEju3fJW5T4+tCsGA@mail.gmail.com>
References: <CAH+cTGM9sFCxegaMHzu-_9Prhvvh9MbVDCWJ1_x7wkO9jC47mQ@mail.gmail.com>
 <c6e69644-66a1-7031-4d7c-1000e0abbea4@comcast.net>
 <CAH+cTGM5YvWV_G0pRgMqjCzuRrFTzd0nOc7yoThfGbGMUyMjsQ@mail.gmail.com>
 <9c9f1c81-309f-91cb-7e26-f02fc4bd45d8@comcast.net>
 <CAH+cTGMx4_Lz2gGDf2AuUaaZF2Xaq4McuPRoHu4JJNc-ORd=Ew@mail.gmail.com>
 <6322e735-5659-848e-3a50-9f066f1299ad@comcast.net>
 <CAF8bMcbgrVcN9J7Aygq2D5RWaU4MS-GH+_+zMHU=AO+DU5DVWw@mail.gmail.com>
 <CAH+cTGNfpLSurqmsoMeci0GWhC0ncOp7zTEju3fJW5T4+tCsGA@mail.gmail.com>
Message-ID: <20190517224346.2b8822de@Tarkus>

On Fri, 17 May 2019 11:36:22 -0700
Michael Boulineau <michael.p.boulineau at gmail.com> wrote:

> So, who knows what happened with the ??? at the beginning of [1]
> directly above.

 perl -Mutf8 -MEncode=encode,decode -Mcharnames=:full \
 -E'say charnames::viacode ord decode utf8 => encode latin1 => "???"'
# ZERO WIDTH NO-BREAK SPACE

So the text seems to have been encoded in UTF-8, then decoded as
Latin-1. If you have multiple such artefacts and want to get rid of
them, try:

a <- readLines(con <- file("hangouts-conversation-6.csv.txt", encoding
= "UTF-8")); close(con); rm(con)

-- 
Best regards,
Ivan


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri May 17 22:03:19 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 17 May 2019 13:03:19 -0700
Subject: [R] how to separate string from numbers in a large txt file
In-Reply-To: <CAF8bMcaSJRaUXK4h2ssaDoJwXaKCUZT2Vq1cutNiNDjQ7ga2Xg@mail.gmail.com>
References: <CAH+cTGM9sFCxegaMHzu-_9Prhvvh9MbVDCWJ1_x7wkO9jC47mQ@mail.gmail.com>
 <c6e69644-66a1-7031-4d7c-1000e0abbea4@comcast.net>
 <CAH+cTGM5YvWV_G0pRgMqjCzuRrFTzd0nOc7yoThfGbGMUyMjsQ@mail.gmail.com>
 <9c9f1c81-309f-91cb-7e26-f02fc4bd45d8@comcast.net>
 <CAH+cTGMx4_Lz2gGDf2AuUaaZF2Xaq4McuPRoHu4JJNc-ORd=Ew@mail.gmail.com>
 <6322e735-5659-848e-3a50-9f066f1299ad@comcast.net>
 <CAF8bMcbgrVcN9J7Aygq2D5RWaU4MS-GH+_+zMHU=AO+DU5DVWw@mail.gmail.com>
 <CAH+cTGNfpLSurqmsoMeci0GWhC0ncOp7zTEju3fJW5T4+tCsGA@mail.gmail.com>
 <CAF8bMcaSJRaUXK4h2ssaDoJwXaKCUZT2Vq1cutNiNDjQ7ga2Xg@mail.gmail.com>
Message-ID: <EEA9FD46-A67C-4FB7-8245-350819C4EA62@dcn.davis.ca.us>

If byte order mark is the issue then you can specify the file encoding as "UTF-8-BOM" and it won't show up in your data any more.

On May 17, 2019 12:12:17 PM PDT, William Dunlap via R-help <r-help at r-project.org> wrote:
>The pattern I gave worked for the lines that you originally showed from
>the
>data file ('a'), before you put commas into them.  If the name is
>either of
>the form "<name>" or "***" then the "(<[^>]*>)" needs to be changed so
>something like "(<[^>]*>|[*]{3})".
>
>The " ???" at the start of the imported data may come from the byte
>order
>mark that Windows apps like to put at the front of a text file in UTF-8
>or
>UTF-16 format.
>
>Bill Dunlap
>TIBCO Software
>wdunlap tibco.com
>
>
>On Fri, May 17, 2019 at 11:53 AM Michael Boulineau <
>michael.p.boulineau at gmail.com> wrote:
>
>> This seemed to work:
>>
>> > a <- readLines ("hangouts-conversation-6.csv.txt")
>> > b <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4", a)
>> > b [1:84]
>>
>> And the first 85 lines looks like this:
>>
>> [83] "2016-06-28 21:02:28 *** Jane Doe started a video chat"
>> [84] "2016-06-28 21:12:43 *** John Doe ended a video chat"
>>
>> Then they transition to the commas:
>>
>> > b [84:100]
>>  [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
>>  [2] "2016-07-01,02:50:35,<John Doe>,hey"
>>  [3] "2016-07-01,02:51:26,<John Doe>,waiting for plane to Edinburgh"
>>  [4] "2016-07-01,02:51:45,<John Doe>,thinking about my boo"
>>
>> Even the strange bit on line 6347 was caught by this:
>>
>> > b [6346:6348]
>> [1] "2016-10-21,10:56:29,<John Doe>,John_Doe"
>> [2] "2016-10-21,10:56:37,<John Doe>,Admit#8242"
>> [3] "2016-10-21,11:00:13,<Jane Doe>,Okay so you have a discussion"
>>
>> Perhaps most awesomely, the code catches spaces that are interposed
>> into the comment itself:
>>
>> > b [4]
>> [1] "2016-01-27,09:15:20,<Jane Doe>,Hey "
>>   > b [85]
>> [1] "2016-07-01,02:50:35,<John Doe>,hey"
>>
>> Notice whether there is a space after the "hey" or not.
>>
>> These are the first two lines:
>>
>> [1] "???2016-01-27 09:14:40 *** Jane Doe started a video chat"
>> [2] "2016-01-27,09:15:20,<Jane
>> Doe>,
>>
>https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf
>> "
>>
>> So, who knows what happened with the ??? at the beginning of [1]
>> directly above. But notice how there are no commas in [1] but there
>> appear in [2]. I don't see why really long ones like [2] directly
>> above would be a problem, were they to be translated into a csv or
>> data frame column.
>>
>> Now, with the commas in there, couldn't we write this into a csv or a
>> data.frame? Some of this data will end up being garbage, I imagine.
>> Like in [2] directly above. Or with [83] and [84] at the top of this
>> discussion post/email. Embarrassingly, I've been trying to convert
>> this into a data.frame or csv but I can't manage to. I've been using
>> the write.csv function, but I don't think I've been getting the
>> arguments correct.
>>
>> At the end of the day, I would like a data.frame and/or csv with the
>> following four columns: date, time, person, comment.
>>
>> I tried this, too:
>>
>> > c <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
>> + [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
>> +                 a, proto=data.frame(stringsAsFactors=FALSE,
>When="",
>> Who="",
>> +                                     What=""))
>>
>> But all I got was this:
>>
>> > c [1:100, ]
>>     When  Who What
>> 1   <NA> <NA> <NA>
>> 2   <NA> <NA> <NA>
>> 3   <NA> <NA> <NA>
>> 4   <NA> <NA> <NA>
>> 5   <NA> <NA> <NA>
>> 6   <NA> <NA> <NA>
>>
>> It seems to have caught nothing.
>>
>> > unique (c)
>>   When  Who What
>> 1 <NA> <NA> <NA>
>>
>> But I like that it converted into columns. That's a really great
>> format. With a little tweaking, it'd be a great code for this data
>> set.
>>
>> Michael
>>
>> On Fri, May 17, 2019 at 8:20 AM William Dunlap via R-help
>> <r-help at r-project.org> wrote:
>> >
>> > Consider using readLines() and strcapture() for reading such a
>file.
>> E.g.,
>> > suppose readLines(files) produced a character vector like
>> >
>> > x <- c("2016-10-21 10:35:36 <Jane Doe> What's your login",
>> >           "2016-10-21 10:56:29 <John Doe> John_Doe",
>> >           "2016-10-21 10:56:37 <John Doe> Admit#8242",
>> >           "October 23, 1819 12:34 <Jane Eyre> I am not an angel")
>> >
>> > Then you can make a data.frame with columns When, Who, and What by
>> > supplying a pattern containing three parenthesized capture
>expressions:
>> > > z <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
>> > [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
>> >              x, proto=data.frame(stringsAsFactors=FALSE, When="",
>Who="",
>> > What=""))
>> > > str(z)
>> > 'data.frame':   4 obs. of  3 variables:
>> >  $ When: chr  "2016-10-21 10:35:36" "2016-10-21 10:56:29"
>"2016-10-21
>> > 10:56:37" NA
>> >  $ Who : chr  "<Jane Doe>" "<John Doe>" "<John Doe>" NA
>> >  $ What: chr  "What's your login" "John_Doe" "Admit#8242" NA
>> >
>> > Lines that don't match the pattern result in NA's - you might make
>a
>> second
>> > pass over the corresponding elements of x with a new pattern.
>> >
>> > You can convert the When column from character to time with
>as.POSIXct().
>> >
>> > Bill Dunlap
>> > TIBCO Software
>> > wdunlap tibco.com
>> >
>> >
>> > On Thu, May 16, 2019 at 8:30 PM David Winsemius
><dwinsemius at comcast.net>
>> > wrote:
>> >
>> > >
>> > > On 5/16/19 3:53 PM, Michael Boulineau wrote:
>> > > > OK. So, I named the object test and then checked the 6347th
>item
>> > > >
>> > > >> test <- readLines ("hangouts-conversation.txt)
>> > > >> test [6347]
>> > > > [1] "2016-10-21 10:56:37 <John Doe> Admit#8242"
>> > > >
>> > > > Perhaps where it was getting screwed up is, since the end of
>this is
>> a
>> > > > number (8242), then, given that there's no space between the
>number
>> > > > and what ought to be the next row, R didn't know where to draw
>the
>> > > > line. Sure enough, it looks like this when I go to the original
>file
>> > > > and control f "#8242"
>> > > >
>> > > > 2016-10-21 10:35:36 <Jane Doe> What's your login
>> > > > 2016-10-21 10:56:29 <John Doe> John_Doe
>> > > > 2016-10-21 10:56:37 <John Doe> Admit#8242
>> > >
>> > >
>> > > An octothorpe is an end of line signifier and is interpreted as
>> allowing
>> > > comments. You can prevent that interpretation with suitable
>choice of
>> > > parameters to `read.table` or `read.csv`. I don't understand why
>that
>> > > should cause anu error or a failure to match that pattern.
>> > >
>> > > > 2016-10-21 11:00:13 <Jane Doe> Okay so you have a discussion
>> > > >
>> > > > Again, it doesn't look like that in the file. Gmail
>automatically
>> > > > formats it like that when I paste it in. More to the point, it
>looks
>> > > > like
>> > > >
>> > > > 2016-10-21 10:35:36 <Jane Doe> What's your login2016-10-21
>10:56:29
>> > > > <John Doe> John_Doe2016-10-21 10:56:37 <John Doe>
>> Admit#82422016-10-21
>> > > > 11:00:13 <Jane Doe> Okay so you have a discussion
>> > > >
>> > > > Notice Admit#82422016. So there's that.
>> > > >
>> > > > Then I built object test2.
>> > > >
>> > > > test2 <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4",
>test)
>> > > >
>> > > > This worked for 84 lines, then this happened.
>> > >
>> > > It may have done something but as you later discovered my first
>code
>> for
>> > > the pattern was incorrect. I had tested it (and pasted in the
>results
>> of
>> > > the test) . The way to refer to a capture class is with
>back-slashes
>> > > before the numbers, not forward-slashes. Try this:
>> > >
>> > >
>> > >  > newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)",
>"\\1,\\2,\\3,\\4",
>> chrvec)
>> > >  > newvec
>> > >   [1] "2016-07-01,02:50:35,<john>,hey"
>> > >   [2] "2016-07-01,02:51:26,<jane>,waiting for plane to Edinburgh"
>> > >   [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
>> > >   [4] "2016-07-01,02:52:07,<jane>,nothing crappy has happened,
>not
>> really"
>> > >   [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast,
>didn't
>> sleep"
>> > >   [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or
>where I am
>> > > really"
>> > >   [7] "2016-07-01,02:54:17,<john>,just know it's london"
>> > >   [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
>> > >   [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a good
>eay"
>> > > [10] "2016-07-01 02:58:56 <jone>"
>> > > [11] "2016-07-01 02:59:34 <jane>"
>> > > [12] "2016-07-01,03:02:48,<john>,British security is a little
>more
>> > > rigorous..."
>> > >
>> > >
>> > > I made note of the fact that the 10th and 11th lines had no
>commas.
>> > >
>> > > >
>> > > >> test2 [84]
>> > > > [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
>> > >
>> > > That line didn't have any "<" so wasn't matched.
>> > >
>> > >
>> > > You could remove all none matching lines for pattern of
>> > >
>> > > dates<space>times<space>"<"<name>">"<space><anything>
>> > >
>> > >
>> > > with:
>> > >
>> > >
>> > > chrvec <- chrvec[ grepl("^.{10} .{8} <.+> .+$)", chrvec)]
>> > >
>> > >
>> > > Do read:
>> > >
>> > > ?read.csv
>> > >
>> > > ?regex
>> > >
>> > >
>> > > --
>> > >
>> > > David
>> > >
>> > >
>> > > >> test2 [85]
>> > > > [1] "//1,//2,//3,//4"
>> > > >> test [85]
>> > > > [1] "2016-07-01 02:50:35 <John Doe> hey"
>> > > >
>> > > > Notice how I toggled back and forth between test and test2
>there. So,
>> > > > whatever happened with the regex, it happened in the switch
>from 84
>> to
>> > > > 85, I guess. It went on like
>> > > >
>> > > > [990] "//1,//2,//3,//4"
>> > > >   [991] "//1,//2,//3,//4"
>> > > >   [992] "//1,//2,//3,//4"
>> > > >   [993] "//1,//2,//3,//4"
>> > > >   [994] "//1,//2,//3,//4"
>> > > >   [995] "//1,//2,//3,//4"
>> > > >   [996] "//1,//2,//3,//4"
>> > > >   [997] "//1,//2,//3,//4"
>> > > >   [998] "//1,//2,//3,//4"
>> > > >   [999] "//1,//2,//3,//4"
>> > > > [1000] "//1,//2,//3,//4"
>> > > >
>> > > > up until line 1000, then I reached max.print.
>> > >
>> > > > Michael
>> > > >
>> > > > On Thu, May 16, 2019 at 1:05 PM David Winsemius <
>> dwinsemius at comcast.net>
>> > > wrote:
>> > > >>
>> > > >> On 5/16/19 12:30 PM, Michael Boulineau wrote:
>> > > >>> Thanks for this tip on etiquette, David. I will be sure and
>not do
>> > > that again.
>> > > >>>
>> > > >>> I tried the read.fwf from the foreign package, with a code
>like
>> this:
>> > > >>>
>> > > >>>    d <- read.fwf("hangouts-conversation.txt",
>> > > >>>                   widths= c(10,10,20,40),
>> > > >>>                  
>col.names=c("date","time","person","comment"),
>> > > >>>                   strip.white=TRUE)
>> > > >>>
>> > > >>> But it threw this error:
>> > > >>>
>> > > >>> Error in scan(file = file, what = what, sep = sep, quote =
>quote,
>> dec
>> > > = dec,  :
>> > > >>>     line 6347 did not have 4 elements
>> > > >>
>> > > >> So what does line 6347 look like? (Use `readLines` and print
>it
>> out.)
>> > > >>
>> > > >>> Interestingly, though, the error only happened when I
>increased the
>> > > >>> width size. But I had to increase the size, or else I
>couldn't
>> "see"
>> > > >>> anything.  The comment was so small that nothing was being
>> captured by
>> > > >>> the size of the column. so to speak.
>> > > >>>
>> > > >>> It seems like what's throwing me is that there's no comma
>that
>> > > >>> demarcates the end of the text proper. For example:
>> > > >> Not sure why you thought there should be a comma. Lines
>usually end
>> > > >> with  <cr> and or a <lf>.
>> > > >>
>> > > >>
>> > > >> Once you have the raw text in a character vector from
>`readLines`
>> named,
>> > > >> say, 'chrvec', then you could selectively substitute commas
>for
>> spaces
>> > > >> with regex. (Now that you no longer desire to remove the dates
>and
>> > > times.)
>> > > >>
>> > > >> sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", chrvec)
>> > > >>
>> > > >> This will not do any replacements when the pattern is not
>matched.
>> See
>> > > >> this test:
>> > > >>
>> > > >>
>> > > >>   > newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)",
>"\\1,\\2,\\3,\\4",
>> > > chrvec)
>> > > >>   > newvec
>> > > >>    [1] "2016-07-01,02:50:35,<john>,hey"
>> > > >>    [2] "2016-07-01,02:51:26,<jane>,waiting for plane to
>Edinburgh"
>> > > >>    [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
>> > > >>    [4] "2016-07-01,02:52:07,<jane>,nothing crappy has
>happened, not
>> > > really"
>> > > >>    [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast,
>didn't
>> > > sleep"
>> > > >>    [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or
>where
>> I am
>> > > >> really"
>> > > >>    [7] "2016-07-01,02:54:17,<john>,just know it's london"
>> > > >>    [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
>> > > >>    [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a
>good
>> eay"
>> > > >> [10] "2016-07-01 02:58:56 <jone>"
>> > > >> [11] "2016-07-01 02:59:34 <jane>"
>> > > >> [12] "2016-07-01,03:02:48,<john>,British security is a little
>more
>> > > >> rigorous..."
>> > > >>
>> > > >>
>> > > >> You should probably remove the "empty comment" lines.
>> > > >>
>> > > >>
>> > > >> --
>> > > >>
>> > > >> David.
>> > > >>
>> > > >>> 2016-07-01 15:34:30 <John Doe> Lame. We were in a
>> starbucks2016-07-01
>> > > >>> 15:35:02 <Jane Doe> Hmm that's interesting2016-07-01 15:35:09
><Jane
>> > > >>> Doe> You must want coffees2016-07-01 15:35:25 <John Doe>
>There was
>> > > >>> lots of Starbucks in my day2016-07-01 15:35:47
>> > > >>>
>> > > >>> It was interesting, too, when I pasted the text into the
>email, it
>> > > >>> self-formatted into the way I wanted it to look. I had to
>manually
>> > > >>> make it look like it does above, since that's the way that it
>> looks in
>> > > >>> the txt file. I wonder if it's being organized by XML or
>something.
>> > > >>>
>> > > >>> Anyways, There's always a space between the two sideways
>carrots,
>> just
>> > > >>> like there is right now: <John Doe> See. Space. And there's
>always
>> a
>> > > >>> space between the data and time. Like this. 2016-07-01
>15:34:30
>> See.
>> > > >>> Space. But there's never a space between the end of the
>comment and
>> > > >>> the next date. Like this: We were in a starbucks2016-07-01
>15:35:02
>> > > >>> See. starbucks and 2016 are smooshed together.
>> > > >>>
>> > > >>> This code is also on the table right now too.
>> > > >>>
>> > > >>> a <- read.table("E:/working
>> > > >>> directory/-189/hangouts-conversation2.txt", quote="\"",
>> > > >>> comment.char="", fill=TRUE)
>> > > >>>
>> > > >>>
>> > >
>>
>h<-cbind(hangouts.conversation2[,1:2],hangouts.conversation2[,3:5],hangouts.conversation2[,6:9])
>> > > >>>
>> > > >>> aa<-gsub("[^[:digit:]]","",h)
>> > > >>> my.data.num <- as.numeric(str_extract(h, "[0-9]+"))
>> > > >>>
>> > > >>> Those last lines are a work in progress. I wish I could
>import a
>> > > >>> picture of what it looks like when it's translated into a
>data
>> frame.
>> > > >>> The fill=TRUE helped to get the data in table that kind of
>sort of
>> > > >>> works, but the comments keep bleeding into the data and time
>> column.
>> > > >>> It's like
>> > > >>>
>> > > >>> 2016-07-01 15:59:17 <Jane Doe> Seriously I've never been
>> > > >>> over               there
>> > > >>> 2016-07-01 15:59:27 <Jane Doe> It confuses me :(
>> > > >>>
>> > > >>> And then, maybe, the "seriously" will be in a column all to
>> itself, as
>> > > >>> will be the "I've'"and the "never" etc.
>> > > >>>
>> > > >>> I will use a regular expression if I have to, but it would be
>nice
>> to
>> > > >>> keep the dates and times on there. Originally, I thought they
>were
>> > > >>> meaningless, but I've since changed my mind on that count.
>The
>> time of
>> > > >>> day isn't so important. But, especially since, say, Gmail
>itself
>> knows
>> > > >>> how to quickly recognize what it is, I know it can be done. I
>know
>> > > >>> this data has structure to it.
>> > > >>>
>> > > >>> Michael
>> > > >>>
>> > > >>>
>> > > >>>
>> > > >>> On Wed, May 15, 2019 at 8:47 PM David Winsemius <
>> > > dwinsemius at comcast.net> wrote:
>> > > >>>> On 5/15/19 4:07 PM, Michael Boulineau wrote:
>> > > >>>>> I have a wild and crazy text file, the head of which looks
>like
>> this:
>> > > >>>>>
>> > > >>>>> 2016-07-01 02:50:35 <john> hey
>> > > >>>>> 2016-07-01 02:51:26 <jane> waiting for plane to Edinburgh
>> > > >>>>> 2016-07-01 02:51:45 <john> thinking about my boo
>> > > >>>>> 2016-07-01 02:52:07 <jane> nothing crappy has happened, not
>> really
>> > > >>>>> 2016-07-01 02:52:20 <john> plane went by pretty fast,
>didn't
>> sleep
>> > > >>>>> 2016-07-01 02:54:08 <jane> no idea what time it is or where
>I am
>> > > really
>> > > >>>>> 2016-07-01 02:54:17 <john> just know it's london
>> > > >>>>> 2016-07-01 02:56:44 <jane> you are probably asleep
>> > > >>>>> 2016-07-01 02:58:45 <jane> I hope fish was fishy in a good
>eay
>> > > >>>>> 2016-07-01 02:58:56 <jone>
>> > > >>>>> 2016-07-01 02:59:34 <jane>
>> > > >>>>> 2016-07-01 03:02:48 <john> British security is a little
>more
>> > > rigorous...
>> > > >>>> Looks entirely not-"crazy". Typical log file format.
>> > > >>>>
>> > > >>>> Two possibilities: 1) Use `read.fwf` from pkg foreign; 2)
>Use
>> regex
>> > > >>>> (i.e. the sub-function) to strip everything up to the "<".
>Read
>> > > >>>> `?regex`. Since that's not a metacharacters you could use a
>> pattern
>> > > >>>> ".+<" and replace with "".
>> > > >>>>
>> > > >>>> And do read the Posting Guide. Cross-posting to
>StackOverflow and
>> > > Rhelp,
>> > > >>>> at least within hours of each, is considered poor manners.
>> > > >>>>
>> > > >>>>
>> > > >>>> --
>> > > >>>>
>> > > >>>> David.
>> > > >>>>
>> > > >>>>> It goes on for a while. It's a big file. But I feel like
>it's
>> going
>> > > to
>> > > >>>>> be difficult to annotate with the coreNLP library or
>package. I'm
>> > > >>>>> doing natural language processing. In other words, I'm
>curious
>> as to
>> > > >>>>> how I would shave off the dates, that is, to make it look
>like:
>> > > >>>>>
>> > > >>>>> <john> hey
>> > > >>>>> <jane> waiting for plane to Edinburgh
>> > > >>>>>     <john> thinking about my boo
>> > > >>>>> <jane> nothing crappy has happened, not really
>> > > >>>>> <john> plane went by pretty fast, didn't sleep
>> > > >>>>> <jane> no idea what time it is or where I am really
>> > > >>>>> <john> just know it's london
>> > > >>>>> <jane> you are probably asleep
>> > > >>>>> <jane> I hope fish was fishy in a good eay
>> > > >>>>>     <jone>
>> > > >>>>> <jane>
>> > > >>>>> <john> British security is a little more rigorous...
>> > > >>>>>
>> > > >>>>> To be clear, then, I'm trying to clean a large text file by
>> writing a
>> > > >>>>> regular expression? such that I create a new object with no
>> numbers
>> > > or
>> > > >>>>> dates.
>> > > >>>>>
>> > > >>>>> Michael
>> > > >>>>>
>> > > >>>>> ______________________________________________
>> > > >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
>more,
>> see
>> > > >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> > > >>>>> PLEASE do read the posting guide
>> > > http://www.R-project.org/posting-guide.html
>> > > >>>>> and provide commented, minimal, self-contained,
>reproducible
>> code.
>> > > >>> ______________________________________________
>> > > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>see
>> > > >>> https://stat.ethz.ch/mailman/listinfo/r-help
>> > > >>> PLEASE do read the posting guide
>> > > http://www.R-project.org/posting-guide.html
>> > > >>> and provide commented, minimal, self-contained, reproducible
>code.
>> > > >> ______________________________________________
>> > > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>see
>> > > >> https://stat.ethz.ch/mailman/listinfo/r-help
>> > > >> PLEASE do read the posting guide
>> > > http://www.R-project.org/posting-guide.html
>> > > >> and provide commented, minimal, self-contained, reproducible
>code.
>> > > > ______________________________________________
>> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>see
>> > > > https://stat.ethz.ch/mailman/listinfo/r-help
>> > > > PLEASE do read the posting guide
>> > > http://www.R-project.org/posting-guide.html
>> > > > and provide commented, minimal, self-contained, reproducible
>code.
>> > >
>> > > ______________________________________________
>> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > > https://stat.ethz.ch/mailman/listinfo/r-help
>> > > PLEASE do read the posting guide
>> > > http://www.R-project.org/posting-guide.html
>> > > and provide commented, minimal, self-contained, reproducible
>code.
>> > >
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From m|ch@e|@p@bou||ne@u @end|ng |rom gm@||@com  Fri May 17 22:18:18 2019
From: m|ch@e|@p@bou||ne@u @end|ng |rom gm@||@com (Michael Boulineau)
Date: Fri, 17 May 2019 13:18:18 -0700
Subject: [R] how to separate string from numbers in a large txt file
In-Reply-To: <EEA9FD46-A67C-4FB7-8245-350819C4EA62@dcn.davis.ca.us>
References: <CAH+cTGM9sFCxegaMHzu-_9Prhvvh9MbVDCWJ1_x7wkO9jC47mQ@mail.gmail.com>
 <c6e69644-66a1-7031-4d7c-1000e0abbea4@comcast.net>
 <CAH+cTGM5YvWV_G0pRgMqjCzuRrFTzd0nOc7yoThfGbGMUyMjsQ@mail.gmail.com>
 <9c9f1c81-309f-91cb-7e26-f02fc4bd45d8@comcast.net>
 <CAH+cTGMx4_Lz2gGDf2AuUaaZF2Xaq4McuPRoHu4JJNc-ORd=Ew@mail.gmail.com>
 <6322e735-5659-848e-3a50-9f066f1299ad@comcast.net>
 <CAF8bMcbgrVcN9J7Aygq2D5RWaU4MS-GH+_+zMHU=AO+DU5DVWw@mail.gmail.com>
 <CAH+cTGNfpLSurqmsoMeci0GWhC0ncOp7zTEju3fJW5T4+tCsGA@mail.gmail.com>
 <CAF8bMcaSJRaUXK4h2ssaDoJwXaKCUZT2Vq1cutNiNDjQ7ga2Xg@mail.gmail.com>
 <EEA9FD46-A67C-4FB7-8245-350819C4EA62@dcn.davis.ca.us>
Message-ID: <CAH+cTGNOkAnPnn-aru-BJUbhrgeYMQBTqjSYT764nKzcdXvTvA@mail.gmail.com>

Very interesting. I'm sure I'll be trying to get rid of the byte order
mark eventually. But right now, I'm more worried about getting the
character vector into either a csv file or data.frame; that way, I can
be able to work with the data neatly tabulated into four columns:
date, time, person, comment. I assume it's a write.csv function, but I
don't know what arguments to put in it. header=FALSE? fill=T?

Micheal

On Fri, May 17, 2019 at 1:03 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>
> If byte order mark is the issue then you can specify the file encoding as "UTF-8-BOM" and it won't show up in your data any more.
>
> On May 17, 2019 12:12:17 PM PDT, William Dunlap via R-help <r-help at r-project.org> wrote:
> >The pattern I gave worked for the lines that you originally showed from
> >the
> >data file ('a'), before you put commas into them.  If the name is
> >either of
> >the form "<name>" or "***" then the "(<[^>]*>)" needs to be changed so
> >something like "(<[^>]*>|[*]{3})".
> >
> >The " ???" at the start of the imported data may come from the byte
> >order
> >mark that Windows apps like to put at the front of a text file in UTF-8
> >or
> >UTF-16 format.
> >
> >Bill Dunlap
> >TIBCO Software
> >wdunlap tibco.com
> >
> >
> >On Fri, May 17, 2019 at 11:53 AM Michael Boulineau <
> >michael.p.boulineau at gmail.com> wrote:
> >
> >> This seemed to work:
> >>
> >> > a <- readLines ("hangouts-conversation-6.csv.txt")
> >> > b <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4", a)
> >> > b [1:84]
> >>
> >> And the first 85 lines looks like this:
> >>
> >> [83] "2016-06-28 21:02:28 *** Jane Doe started a video chat"
> >> [84] "2016-06-28 21:12:43 *** John Doe ended a video chat"
> >>
> >> Then they transition to the commas:
> >>
> >> > b [84:100]
> >>  [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
> >>  [2] "2016-07-01,02:50:35,<John Doe>,hey"
> >>  [3] "2016-07-01,02:51:26,<John Doe>,waiting for plane to Edinburgh"
> >>  [4] "2016-07-01,02:51:45,<John Doe>,thinking about my boo"
> >>
> >> Even the strange bit on line 6347 was caught by this:
> >>
> >> > b [6346:6348]
> >> [1] "2016-10-21,10:56:29,<John Doe>,John_Doe"
> >> [2] "2016-10-21,10:56:37,<John Doe>,Admit#8242"
> >> [3] "2016-10-21,11:00:13,<Jane Doe>,Okay so you have a discussion"
> >>
> >> Perhaps most awesomely, the code catches spaces that are interposed
> >> into the comment itself:
> >>
> >> > b [4]
> >> [1] "2016-01-27,09:15:20,<Jane Doe>,Hey "
> >>   > b [85]
> >> [1] "2016-07-01,02:50:35,<John Doe>,hey"
> >>
> >> Notice whether there is a space after the "hey" or not.
> >>
> >> These are the first two lines:
> >>
> >> [1] "???2016-01-27 09:14:40 *** Jane Doe started a video chat"
> >> [2] "2016-01-27,09:15:20,<Jane
> >> Doe>,
> >>
> >https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf
> >> "
> >>
> >> So, who knows what happened with the ??? at the beginning of [1]
> >> directly above. But notice how there are no commas in [1] but there
> >> appear in [2]. I don't see why really long ones like [2] directly
> >> above would be a problem, were they to be translated into a csv or
> >> data frame column.
> >>
> >> Now, with the commas in there, couldn't we write this into a csv or a
> >> data.frame? Some of this data will end up being garbage, I imagine.
> >> Like in [2] directly above. Or with [83] and [84] at the top of this
> >> discussion post/email. Embarrassingly, I've been trying to convert
> >> this into a data.frame or csv but I can't manage to. I've been using
> >> the write.csv function, but I don't think I've been getting the
> >> arguments correct.
> >>
> >> At the end of the day, I would like a data.frame and/or csv with the
> >> following four columns: date, time, person, comment.
> >>
> >> I tried this, too:
> >>
> >> > c <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
> >> + [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
> >> +                 a, proto=data.frame(stringsAsFactors=FALSE,
> >When="",
> >> Who="",
> >> +                                     What=""))
> >>
> >> But all I got was this:
> >>
> >> > c [1:100, ]
> >>     When  Who What
> >> 1   <NA> <NA> <NA>
> >> 2   <NA> <NA> <NA>
> >> 3   <NA> <NA> <NA>
> >> 4   <NA> <NA> <NA>
> >> 5   <NA> <NA> <NA>
> >> 6   <NA> <NA> <NA>
> >>
> >> It seems to have caught nothing.
> >>
> >> > unique (c)
> >>   When  Who What
> >> 1 <NA> <NA> <NA>
> >>
> >> But I like that it converted into columns. That's a really great
> >> format. With a little tweaking, it'd be a great code for this data
> >> set.
> >>
> >> Michael
> >>
> >> On Fri, May 17, 2019 at 8:20 AM William Dunlap via R-help
> >> <r-help at r-project.org> wrote:
> >> >
> >> > Consider using readLines() and strcapture() for reading such a
> >file.
> >> E.g.,
> >> > suppose readLines(files) produced a character vector like
> >> >
> >> > x <- c("2016-10-21 10:35:36 <Jane Doe> What's your login",
> >> >           "2016-10-21 10:56:29 <John Doe> John_Doe",
> >> >           "2016-10-21 10:56:37 <John Doe> Admit#8242",
> >> >           "October 23, 1819 12:34 <Jane Eyre> I am not an angel")
> >> >
> >> > Then you can make a data.frame with columns When, Who, and What by
> >> > supplying a pattern containing three parenthesized capture
> >expressions:
> >> > > z <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
> >> > [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
> >> >              x, proto=data.frame(stringsAsFactors=FALSE, When="",
> >Who="",
> >> > What=""))
> >> > > str(z)
> >> > 'data.frame':   4 obs. of  3 variables:
> >> >  $ When: chr  "2016-10-21 10:35:36" "2016-10-21 10:56:29"
> >"2016-10-21
> >> > 10:56:37" NA
> >> >  $ Who : chr  "<Jane Doe>" "<John Doe>" "<John Doe>" NA
> >> >  $ What: chr  "What's your login" "John_Doe" "Admit#8242" NA
> >> >
> >> > Lines that don't match the pattern result in NA's - you might make
> >a
> >> second
> >> > pass over the corresponding elements of x with a new pattern.
> >> >
> >> > You can convert the When column from character to time with
> >as.POSIXct().
> >> >
> >> > Bill Dunlap
> >> > TIBCO Software
> >> > wdunlap tibco.com
> >> >
> >> >
> >> > On Thu, May 16, 2019 at 8:30 PM David Winsemius
> ><dwinsemius at comcast.net>
> >> > wrote:
> >> >
> >> > >
> >> > > On 5/16/19 3:53 PM, Michael Boulineau wrote:
> >> > > > OK. So, I named the object test and then checked the 6347th
> >item
> >> > > >
> >> > > >> test <- readLines ("hangouts-conversation.txt)
> >> > > >> test [6347]
> >> > > > [1] "2016-10-21 10:56:37 <John Doe> Admit#8242"
> >> > > >
> >> > > > Perhaps where it was getting screwed up is, since the end of
> >this is
> >> a
> >> > > > number (8242), then, given that there's no space between the
> >number
> >> > > > and what ought to be the next row, R didn't know where to draw
> >the
> >> > > > line. Sure enough, it looks like this when I go to the original
> >file
> >> > > > and control f "#8242"
> >> > > >
> >> > > > 2016-10-21 10:35:36 <Jane Doe> What's your login
> >> > > > 2016-10-21 10:56:29 <John Doe> John_Doe
> >> > > > 2016-10-21 10:56:37 <John Doe> Admit#8242
> >> > >
> >> > >
> >> > > An octothorpe is an end of line signifier and is interpreted as
> >> allowing
> >> > > comments. You can prevent that interpretation with suitable
> >choice of
> >> > > parameters to `read.table` or `read.csv`. I don't understand why
> >that
> >> > > should cause anu error or a failure to match that pattern.
> >> > >
> >> > > > 2016-10-21 11:00:13 <Jane Doe> Okay so you have a discussion
> >> > > >
> >> > > > Again, it doesn't look like that in the file. Gmail
> >automatically
> >> > > > formats it like that when I paste it in. More to the point, it
> >looks
> >> > > > like
> >> > > >
> >> > > > 2016-10-21 10:35:36 <Jane Doe> What's your login2016-10-21
> >10:56:29
> >> > > > <John Doe> John_Doe2016-10-21 10:56:37 <John Doe>
> >> Admit#82422016-10-21
> >> > > > 11:00:13 <Jane Doe> Okay so you have a discussion
> >> > > >
> >> > > > Notice Admit#82422016. So there's that.
> >> > > >
> >> > > > Then I built object test2.
> >> > > >
> >> > > > test2 <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4",
> >test)
> >> > > >
> >> > > > This worked for 84 lines, then this happened.
> >> > >
> >> > > It may have done something but as you later discovered my first
> >code
> >> for
> >> > > the pattern was incorrect. I had tested it (and pasted in the
> >results
> >> of
> >> > > the test) . The way to refer to a capture class is with
> >back-slashes
> >> > > before the numbers, not forward-slashes. Try this:
> >> > >
> >> > >
> >> > >  > newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)",
> >"\\1,\\2,\\3,\\4",
> >> chrvec)
> >> > >  > newvec
> >> > >   [1] "2016-07-01,02:50:35,<john>,hey"
> >> > >   [2] "2016-07-01,02:51:26,<jane>,waiting for plane to Edinburgh"
> >> > >   [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
> >> > >   [4] "2016-07-01,02:52:07,<jane>,nothing crappy has happened,
> >not
> >> really"
> >> > >   [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast,
> >didn't
> >> sleep"
> >> > >   [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or
> >where I am
> >> > > really"
> >> > >   [7] "2016-07-01,02:54:17,<john>,just know it's london"
> >> > >   [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
> >> > >   [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a good
> >eay"
> >> > > [10] "2016-07-01 02:58:56 <jone>"
> >> > > [11] "2016-07-01 02:59:34 <jane>"
> >> > > [12] "2016-07-01,03:02:48,<john>,British security is a little
> >more
> >> > > rigorous..."
> >> > >
> >> > >
> >> > > I made note of the fact that the 10th and 11th lines had no
> >commas.
> >> > >
> >> > > >
> >> > > >> test2 [84]
> >> > > > [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
> >> > >
> >> > > That line didn't have any "<" so wasn't matched.
> >> > >
> >> > >
> >> > > You could remove all none matching lines for pattern of
> >> > >
> >> > > dates<space>times<space>"<"<name>">"<space><anything>
> >> > >
> >> > >
> >> > > with:
> >> > >
> >> > >
> >> > > chrvec <- chrvec[ grepl("^.{10} .{8} <.+> .+$)", chrvec)]
> >> > >
> >> > >
> >> > > Do read:
> >> > >
> >> > > ?read.csv
> >> > >
> >> > > ?regex
> >> > >
> >> > >
> >> > > --
> >> > >
> >> > > David
> >> > >
> >> > >
> >> > > >> test2 [85]
> >> > > > [1] "//1,//2,//3,//4"
> >> > > >> test [85]
> >> > > > [1] "2016-07-01 02:50:35 <John Doe> hey"
> >> > > >
> >> > > > Notice how I toggled back and forth between test and test2
> >there. So,
> >> > > > whatever happened with the regex, it happened in the switch
> >from 84
> >> to
> >> > > > 85, I guess. It went on like
> >> > > >
> >> > > > [990] "//1,//2,//3,//4"
> >> > > >   [991] "//1,//2,//3,//4"
> >> > > >   [992] "//1,//2,//3,//4"
> >> > > >   [993] "//1,//2,//3,//4"
> >> > > >   [994] "//1,//2,//3,//4"
> >> > > >   [995] "//1,//2,//3,//4"
> >> > > >   [996] "//1,//2,//3,//4"
> >> > > >   [997] "//1,//2,//3,//4"
> >> > > >   [998] "//1,//2,//3,//4"
> >> > > >   [999] "//1,//2,//3,//4"
> >> > > > [1000] "//1,//2,//3,//4"
> >> > > >
> >> > > > up until line 1000, then I reached max.print.
> >> > >
> >> > > > Michael
> >> > > >
> >> > > > On Thu, May 16, 2019 at 1:05 PM David Winsemius <
> >> dwinsemius at comcast.net>
> >> > > wrote:
> >> > > >>
> >> > > >> On 5/16/19 12:30 PM, Michael Boulineau wrote:
> >> > > >>> Thanks for this tip on etiquette, David. I will be sure and
> >not do
> >> > > that again.
> >> > > >>>
> >> > > >>> I tried the read.fwf from the foreign package, with a code
> >like
> >> this:
> >> > > >>>
> >> > > >>>    d <- read.fwf("hangouts-conversation.txt",
> >> > > >>>                   widths= c(10,10,20,40),
> >> > > >>>
> >col.names=c("date","time","person","comment"),
> >> > > >>>                   strip.white=TRUE)
> >> > > >>>
> >> > > >>> But it threw this error:
> >> > > >>>
> >> > > >>> Error in scan(file = file, what = what, sep = sep, quote =
> >quote,
> >> dec
> >> > > = dec,  :
> >> > > >>>     line 6347 did not have 4 elements
> >> > > >>
> >> > > >> So what does line 6347 look like? (Use `readLines` and print
> >it
> >> out.)
> >> > > >>
> >> > > >>> Interestingly, though, the error only happened when I
> >increased the
> >> > > >>> width size. But I had to increase the size, or else I
> >couldn't
> >> "see"
> >> > > >>> anything.  The comment was so small that nothing was being
> >> captured by
> >> > > >>> the size of the column. so to speak.
> >> > > >>>
> >> > > >>> It seems like what's throwing me is that there's no comma
> >that
> >> > > >>> demarcates the end of the text proper. For example:
> >> > > >> Not sure why you thought there should be a comma. Lines
> >usually end
> >> > > >> with  <cr> and or a <lf>.
> >> > > >>
> >> > > >>
> >> > > >> Once you have the raw text in a character vector from
> >`readLines`
> >> named,
> >> > > >> say, 'chrvec', then you could selectively substitute commas
> >for
> >> spaces
> >> > > >> with regex. (Now that you no longer desire to remove the dates
> >and
> >> > > times.)
> >> > > >>
> >> > > >> sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", chrvec)
> >> > > >>
> >> > > >> This will not do any replacements when the pattern is not
> >matched.
> >> See
> >> > > >> this test:
> >> > > >>
> >> > > >>
> >> > > >>   > newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)",
> >"\\1,\\2,\\3,\\4",
> >> > > chrvec)
> >> > > >>   > newvec
> >> > > >>    [1] "2016-07-01,02:50:35,<john>,hey"
> >> > > >>    [2] "2016-07-01,02:51:26,<jane>,waiting for plane to
> >Edinburgh"
> >> > > >>    [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
> >> > > >>    [4] "2016-07-01,02:52:07,<jane>,nothing crappy has
> >happened, not
> >> > > really"
> >> > > >>    [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast,
> >didn't
> >> > > sleep"
> >> > > >>    [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or
> >where
> >> I am
> >> > > >> really"
> >> > > >>    [7] "2016-07-01,02:54:17,<john>,just know it's london"
> >> > > >>    [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
> >> > > >>    [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a
> >good
> >> eay"
> >> > > >> [10] "2016-07-01 02:58:56 <jone>"
> >> > > >> [11] "2016-07-01 02:59:34 <jane>"
> >> > > >> [12] "2016-07-01,03:02:48,<john>,British security is a little
> >more
> >> > > >> rigorous..."
> >> > > >>
> >> > > >>
> >> > > >> You should probably remove the "empty comment" lines.
> >> > > >>
> >> > > >>
> >> > > >> --
> >> > > >>
> >> > > >> David.
> >> > > >>
> >> > > >>> 2016-07-01 15:34:30 <John Doe> Lame. We were in a
> >> starbucks2016-07-01
> >> > > >>> 15:35:02 <Jane Doe> Hmm that's interesting2016-07-01 15:35:09
> ><Jane
> >> > > >>> Doe> You must want coffees2016-07-01 15:35:25 <John Doe>
> >There was
> >> > > >>> lots of Starbucks in my day2016-07-01 15:35:47
> >> > > >>>
> >> > > >>> It was interesting, too, when I pasted the text into the
> >email, it
> >> > > >>> self-formatted into the way I wanted it to look. I had to
> >manually
> >> > > >>> make it look like it does above, since that's the way that it
> >> looks in
> >> > > >>> the txt file. I wonder if it's being organized by XML or
> >something.
> >> > > >>>
> >> > > >>> Anyways, There's always a space between the two sideways
> >carrots,
> >> just
> >> > > >>> like there is right now: <John Doe> See. Space. And there's
> >always
> >> a
> >> > > >>> space between the data and time. Like this. 2016-07-01
> >15:34:30
> >> See.
> >> > > >>> Space. But there's never a space between the end of the
> >comment and
> >> > > >>> the next date. Like this: We were in a starbucks2016-07-01
> >15:35:02
> >> > > >>> See. starbucks and 2016 are smooshed together.
> >> > > >>>
> >> > > >>> This code is also on the table right now too.
> >> > > >>>
> >> > > >>> a <- read.table("E:/working
> >> > > >>> directory/-189/hangouts-conversation2.txt", quote="\"",
> >> > > >>> comment.char="", fill=TRUE)
> >> > > >>>
> >> > > >>>
> >> > >
> >>
> >h<-cbind(hangouts.conversation2[,1:2],hangouts.conversation2[,3:5],hangouts.conversation2[,6:9])
> >> > > >>>
> >> > > >>> aa<-gsub("[^[:digit:]]","",h)
> >> > > >>> my.data.num <- as.numeric(str_extract(h, "[0-9]+"))
> >> > > >>>
> >> > > >>> Those last lines are a work in progress. I wish I could
> >import a
> >> > > >>> picture of what it looks like when it's translated into a
> >data
> >> frame.
> >> > > >>> The fill=TRUE helped to get the data in table that kind of
> >sort of
> >> > > >>> works, but the comments keep bleeding into the data and time
> >> column.
> >> > > >>> It's like
> >> > > >>>
> >> > > >>> 2016-07-01 15:59:17 <Jane Doe> Seriously I've never been
> >> > > >>> over               there
> >> > > >>> 2016-07-01 15:59:27 <Jane Doe> It confuses me :(
> >> > > >>>
> >> > > >>> And then, maybe, the "seriously" will be in a column all to
> >> itself, as
> >> > > >>> will be the "I've'"and the "never" etc.
> >> > > >>>
> >> > > >>> I will use a regular expression if I have to, but it would be
> >nice
> >> to
> >> > > >>> keep the dates and times on there. Originally, I thought they
> >were
> >> > > >>> meaningless, but I've since changed my mind on that count.
> >The
> >> time of
> >> > > >>> day isn't so important. But, especially since, say, Gmail
> >itself
> >> knows
> >> > > >>> how to quickly recognize what it is, I know it can be done. I
> >know
> >> > > >>> this data has structure to it.
> >> > > >>>
> >> > > >>> Michael
> >> > > >>>
> >> > > >>>
> >> > > >>>
> >> > > >>> On Wed, May 15, 2019 at 8:47 PM David Winsemius <
> >> > > dwinsemius at comcast.net> wrote:
> >> > > >>>> On 5/15/19 4:07 PM, Michael Boulineau wrote:
> >> > > >>>>> I have a wild and crazy text file, the head of which looks
> >like
> >> this:
> >> > > >>>>>
> >> > > >>>>> 2016-07-01 02:50:35 <john> hey
> >> > > >>>>> 2016-07-01 02:51:26 <jane> waiting for plane to Edinburgh
> >> > > >>>>> 2016-07-01 02:51:45 <john> thinking about my boo
> >> > > >>>>> 2016-07-01 02:52:07 <jane> nothing crappy has happened, not
> >> really
> >> > > >>>>> 2016-07-01 02:52:20 <john> plane went by pretty fast,
> >didn't
> >> sleep
> >> > > >>>>> 2016-07-01 02:54:08 <jane> no idea what time it is or where
> >I am
> >> > > really
> >> > > >>>>> 2016-07-01 02:54:17 <john> just know it's london
> >> > > >>>>> 2016-07-01 02:56:44 <jane> you are probably asleep
> >> > > >>>>> 2016-07-01 02:58:45 <jane> I hope fish was fishy in a good
> >eay
> >> > > >>>>> 2016-07-01 02:58:56 <jone>
> >> > > >>>>> 2016-07-01 02:59:34 <jane>
> >> > > >>>>> 2016-07-01 03:02:48 <john> British security is a little
> >more
> >> > > rigorous...
> >> > > >>>> Looks entirely not-"crazy". Typical log file format.
> >> > > >>>>
> >> > > >>>> Two possibilities: 1) Use `read.fwf` from pkg foreign; 2)
> >Use
> >> regex
> >> > > >>>> (i.e. the sub-function) to strip everything up to the "<".
> >Read
> >> > > >>>> `?regex`. Since that's not a metacharacters you could use a
> >> pattern
> >> > > >>>> ".+<" and replace with "".
> >> > > >>>>
> >> > > >>>> And do read the Posting Guide. Cross-posting to
> >StackOverflow and
> >> > > Rhelp,
> >> > > >>>> at least within hours of each, is considered poor manners.
> >> > > >>>>
> >> > > >>>>
> >> > > >>>> --
> >> > > >>>>
> >> > > >>>> David.
> >> > > >>>>
> >> > > >>>>> It goes on for a while. It's a big file. But I feel like
> >it's
> >> going
> >> > > to
> >> > > >>>>> be difficult to annotate with the coreNLP library or
> >package. I'm
> >> > > >>>>> doing natural language processing. In other words, I'm
> >curious
> >> as to
> >> > > >>>>> how I would shave off the dates, that is, to make it look
> >like:
> >> > > >>>>>
> >> > > >>>>> <john> hey
> >> > > >>>>> <jane> waiting for plane to Edinburgh
> >> > > >>>>>     <john> thinking about my boo
> >> > > >>>>> <jane> nothing crappy has happened, not really
> >> > > >>>>> <john> plane went by pretty fast, didn't sleep
> >> > > >>>>> <jane> no idea what time it is or where I am really
> >> > > >>>>> <john> just know it's london
> >> > > >>>>> <jane> you are probably asleep
> >> > > >>>>> <jane> I hope fish was fishy in a good eay
> >> > > >>>>>     <jone>
> >> > > >>>>> <jane>
> >> > > >>>>> <john> British security is a little more rigorous...
> >> > > >>>>>
> >> > > >>>>> To be clear, then, I'm trying to clean a large text file by
> >> writing a
> >> > > >>>>> regular expression? such that I create a new object with no
> >> numbers
> >> > > or
> >> > > >>>>> dates.
> >> > > >>>>>
> >> > > >>>>> Michael
> >> > > >>>>>
> >> > > >>>>> ______________________________________________
> >> > > >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
> >more,
> >> see
> >> > > >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >> > > >>>>> PLEASE do read the posting guide
> >> > > http://www.R-project.org/posting-guide.html
> >> > > >>>>> and provide commented, minimal, self-contained,
> >reproducible
> >> code.
> >> > > >>> ______________________________________________
> >> > > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >see
> >> > > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >> > > >>> PLEASE do read the posting guide
> >> > > http://www.R-project.org/posting-guide.html
> >> > > >>> and provide commented, minimal, self-contained, reproducible
> >code.
> >> > > >> ______________________________________________
> >> > > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >see
> >> > > >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> > > >> PLEASE do read the posting guide
> >> > > http://www.R-project.org/posting-guide.html
> >> > > >> and provide commented, minimal, self-contained, reproducible
> >code.
> >> > > > ______________________________________________
> >> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >see
> >> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > > > PLEASE do read the posting guide
> >> > > http://www.R-project.org/posting-guide.html
> >> > > > and provide commented, minimal, self-contained, reproducible
> >code.
> >> > >
> >> > > ______________________________________________
> >> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> > > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > > PLEASE do read the posting guide
> >> > > http://www.R-project.org/posting-guide.html
> >> > > and provide commented, minimal, self-contained, reproducible
> >code.
> >> > >
> >> >
> >> >         [[alternative HTML version deleted]]
> >> >
> >> > ______________________________________________
> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> > and provide commented, minimal, self-contained, reproducible code.
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.


From bor|@@@te|pe @end|ng |rom utoronto@c@  Sat May 18 01:28:20 2019
From: bor|@@@te|pe @end|ng |rom utoronto@c@ (Boris Steipe)
Date: Fri, 17 May 2019 23:28:20 +0000
Subject: [R] how to separate string from numbers in a large txt file
In-Reply-To: <CAH+cTGNOkAnPnn-aru-BJUbhrgeYMQBTqjSYT764nKzcdXvTvA@mail.gmail.com>
References: <CAH+cTGM9sFCxegaMHzu-_9Prhvvh9MbVDCWJ1_x7wkO9jC47mQ@mail.gmail.com>
 <c6e69644-66a1-7031-4d7c-1000e0abbea4@comcast.net>
 <CAH+cTGM5YvWV_G0pRgMqjCzuRrFTzd0nOc7yoThfGbGMUyMjsQ@mail.gmail.com>
 <9c9f1c81-309f-91cb-7e26-f02fc4bd45d8@comcast.net>
 <CAH+cTGMx4_Lz2gGDf2AuUaaZF2Xaq4McuPRoHu4JJNc-ORd=Ew@mail.gmail.com>
 <6322e735-5659-848e-3a50-9f066f1299ad@comcast.net>
 <CAF8bMcbgrVcN9J7Aygq2D5RWaU4MS-GH+_+zMHU=AO+DU5DVWw@mail.gmail.com>
 <CAH+cTGNfpLSurqmsoMeci0GWhC0ncOp7zTEju3fJW5T4+tCsGA@mail.gmail.com>
 <CAF8bMcaSJRaUXK4h2ssaDoJwXaKCUZT2Vq1cutNiNDjQ7ga2Xg@mail.gmail.com>
 <EEA9FD46-A67C-4FB7-8245-350819C4EA62@dcn.davis.ca.us>
 <CAH+cTGNOkAnPnn-aru-BJUbhrgeYMQBTqjSYT764nKzcdXvTvA@mail.gmail.com>
Message-ID: <9B170703-AAAF-4D52-9744-1FDE70559B05@utoronto.ca>

Don't start putting in extra commas and then reading this as csv. That approach is broken. The correct approach is what Bill outlined: read everything with readLines(), and then use a proper regular expression with strcapture(). 

You need to pre-process the object that readLines() gives you: replace the contents of the videochat lines, and make it conform to the format of the other lines before you process it into your data frame.

Approximately something like 

# read the raw data
tmp <- readLines("hangouts-conversation-6.csv.txt")

# process all video chat lines
patt <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+) "  # (year time )*** (word word)
tmp <- gsub(patt, "\\1<\\2> ", tmp)

# next, use strcapture()

Note that this makes the assumption that your names are always exactly two words containing only letters. If that assumption is not true, more though needs to go into the regex. But you can test that:

patt <- " <\\w+ \\w+> "   #" <word word> "
sum( ! grepl(patt, tmp)))

... will give the number of lines that remain in your file that do not have a tag that can be interpreted as "Who"

Once that is fine, use Bill's approach - or a regular expression of your own design - to create your data frame.

Hope this helps,
Boris




> On 2019-05-17, at 16:18, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
> 
> Very interesting. I'm sure I'll be trying to get rid of the byte order
> mark eventually. But right now, I'm more worried about getting the
> character vector into either a csv file or data.frame; that way, I can
> be able to work with the data neatly tabulated into four columns:
> date, time, person, comment. I assume it's a write.csv function, but I
> don't know what arguments to put in it. header=FALSE? fill=T?
> 
> Micheal
> 
> On Fri, May 17, 2019 at 1:03 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>> 
>> If byte order mark is the issue then you can specify the file encoding as "UTF-8-BOM" and it won't show up in your data any more.
>> 
>> On May 17, 2019 12:12:17 PM PDT, William Dunlap via R-help <r-help at r-project.org> wrote:
>>> The pattern I gave worked for the lines that you originally showed from
>>> the
>>> data file ('a'), before you put commas into them.  If the name is
>>> either of
>>> the form "<name>" or "***" then the "(<[^>]*>)" needs to be changed so
>>> something like "(<[^>]*>|[*]{3})".
>>> 
>>> The " ???" at the start of the imported data may come from the byte
>>> order
>>> mark that Windows apps like to put at the front of a text file in UTF-8
>>> or
>>> UTF-16 format.
>>> 
>>> Bill Dunlap
>>> TIBCO Software
>>> wdunlap tibco.com
>>> 
>>> 
>>> On Fri, May 17, 2019 at 11:53 AM Michael Boulineau <
>>> michael.p.boulineau at gmail.com> wrote:
>>> 
>>>> This seemed to work:
>>>> 
>>>>> a <- readLines ("hangouts-conversation-6.csv.txt")
>>>>> b <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4", a)
>>>>> b [1:84]
>>>> 
>>>> And the first 85 lines looks like this:
>>>> 
>>>> [83] "2016-06-28 21:02:28 *** Jane Doe started a video chat"
>>>> [84] "2016-06-28 21:12:43 *** John Doe ended a video chat"
>>>> 
>>>> Then they transition to the commas:
>>>> 
>>>>> b [84:100]
>>>> [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
>>>> [2] "2016-07-01,02:50:35,<John Doe>,hey"
>>>> [3] "2016-07-01,02:51:26,<John Doe>,waiting for plane to Edinburgh"
>>>> [4] "2016-07-01,02:51:45,<John Doe>,thinking about my boo"
>>>> 
>>>> Even the strange bit on line 6347 was caught by this:
>>>> 
>>>>> b [6346:6348]
>>>> [1] "2016-10-21,10:56:29,<John Doe>,John_Doe"
>>>> [2] "2016-10-21,10:56:37,<John Doe>,Admit#8242"
>>>> [3] "2016-10-21,11:00:13,<Jane Doe>,Okay so you have a discussion"
>>>> 
>>>> Perhaps most awesomely, the code catches spaces that are interposed
>>>> into the comment itself:
>>>> 
>>>>> b [4]
>>>> [1] "2016-01-27,09:15:20,<Jane Doe>,Hey "
>>>>> b [85]
>>>> [1] "2016-07-01,02:50:35,<John Doe>,hey"
>>>> 
>>>> Notice whether there is a space after the "hey" or not.
>>>> 
>>>> These are the first two lines:
>>>> 
>>>> [1] "???2016-01-27 09:14:40 *** Jane Doe started a video chat"
>>>> [2] "2016-01-27,09:15:20,<Jane
>>>> Doe>,
>>>> 
>>> https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf
>>>> "
>>>> 
>>>> So, who knows what happened with the ??? at the beginning of [1]
>>>> directly above. But notice how there are no commas in [1] but there
>>>> appear in [2]. I don't see why really long ones like [2] directly
>>>> above would be a problem, were they to be translated into a csv or
>>>> data frame column.
>>>> 
>>>> Now, with the commas in there, couldn't we write this into a csv or a
>>>> data.frame? Some of this data will end up being garbage, I imagine.
>>>> Like in [2] directly above. Or with [83] and [84] at the top of this
>>>> discussion post/email. Embarrassingly, I've been trying to convert
>>>> this into a data.frame or csv but I can't manage to. I've been using
>>>> the write.csv function, but I don't think I've been getting the
>>>> arguments correct.
>>>> 
>>>> At the end of the day, I would like a data.frame and/or csv with the
>>>> following four columns: date, time, person, comment.
>>>> 
>>>> I tried this, too:
>>>> 
>>>>> c <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
>>>> + [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
>>>> +                 a, proto=data.frame(stringsAsFactors=FALSE,
>>> When="",
>>>> Who="",
>>>> +                                     What=""))
>>>> 
>>>> But all I got was this:
>>>> 
>>>>> c [1:100, ]
>>>>    When  Who What
>>>> 1   <NA> <NA> <NA>
>>>> 2   <NA> <NA> <NA>
>>>> 3   <NA> <NA> <NA>
>>>> 4   <NA> <NA> <NA>
>>>> 5   <NA> <NA> <NA>
>>>> 6   <NA> <NA> <NA>
>>>> 
>>>> It seems to have caught nothing.
>>>> 
>>>>> unique (c)
>>>>  When  Who What
>>>> 1 <NA> <NA> <NA>
>>>> 
>>>> But I like that it converted into columns. That's a really great
>>>> format. With a little tweaking, it'd be a great code for this data
>>>> set.
>>>> 
>>>> Michael
>>>> 
>>>> On Fri, May 17, 2019 at 8:20 AM William Dunlap via R-help
>>>> <r-help at r-project.org> wrote:
>>>>> 
>>>>> Consider using readLines() and strcapture() for reading such a
>>> file.
>>>> E.g.,
>>>>> suppose readLines(files) produced a character vector like
>>>>> 
>>>>> x <- c("2016-10-21 10:35:36 <Jane Doe> What's your login",
>>>>>          "2016-10-21 10:56:29 <John Doe> John_Doe",
>>>>>          "2016-10-21 10:56:37 <John Doe> Admit#8242",
>>>>>          "October 23, 1819 12:34 <Jane Eyre> I am not an angel")
>>>>> 
>>>>> Then you can make a data.frame with columns When, Who, and What by
>>>>> supplying a pattern containing three parenthesized capture
>>> expressions:
>>>>>> z <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
>>>>> [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
>>>>>             x, proto=data.frame(stringsAsFactors=FALSE, When="",
>>> Who="",
>>>>> What=""))
>>>>>> str(z)
>>>>> 'data.frame':   4 obs. of  3 variables:
>>>>> $ When: chr  "2016-10-21 10:35:36" "2016-10-21 10:56:29"
>>> "2016-10-21
>>>>> 10:56:37" NA
>>>>> $ Who : chr  "<Jane Doe>" "<John Doe>" "<John Doe>" NA
>>>>> $ What: chr  "What's your login" "John_Doe" "Admit#8242" NA
>>>>> 
>>>>> Lines that don't match the pattern result in NA's - you might make
>>> a
>>>> second
>>>>> pass over the corresponding elements of x with a new pattern.
>>>>> 
>>>>> You can convert the When column from character to time with
>>> as.POSIXct().
>>>>> 
>>>>> Bill Dunlap
>>>>> TIBCO Software
>>>>> wdunlap tibco.com
>>>>> 
>>>>> 
>>>>> On Thu, May 16, 2019 at 8:30 PM David Winsemius
>>> <dwinsemius at comcast.net>
>>>>> wrote:
>>>>> 
>>>>>> 
>>>>>> On 5/16/19 3:53 PM, Michael Boulineau wrote:
>>>>>>> OK. So, I named the object test and then checked the 6347th
>>> item
>>>>>>> 
>>>>>>>> test <- readLines ("hangouts-conversation.txt)
>>>>>>>> test [6347]
>>>>>>> [1] "2016-10-21 10:56:37 <John Doe> Admit#8242"
>>>>>>> 
>>>>>>> Perhaps where it was getting screwed up is, since the end of
>>> this is
>>>> a
>>>>>>> number (8242), then, given that there's no space between the
>>> number
>>>>>>> and what ought to be the next row, R didn't know where to draw
>>> the
>>>>>>> line. Sure enough, it looks like this when I go to the original
>>> file
>>>>>>> and control f "#8242"
>>>>>>> 
>>>>>>> 2016-10-21 10:35:36 <Jane Doe> What's your login
>>>>>>> 2016-10-21 10:56:29 <John Doe> John_Doe
>>>>>>> 2016-10-21 10:56:37 <John Doe> Admit#8242
>>>>>> 
>>>>>> 
>>>>>> An octothorpe is an end of line signifier and is interpreted as
>>>> allowing
>>>>>> comments. You can prevent that interpretation with suitable
>>> choice of
>>>>>> parameters to `read.table` or `read.csv`. I don't understand why
>>> that
>>>>>> should cause anu error or a failure to match that pattern.
>>>>>> 
>>>>>>> 2016-10-21 11:00:13 <Jane Doe> Okay so you have a discussion
>>>>>>> 
>>>>>>> Again, it doesn't look like that in the file. Gmail
>>> automatically
>>>>>>> formats it like that when I paste it in. More to the point, it
>>> looks
>>>>>>> like
>>>>>>> 
>>>>>>> 2016-10-21 10:35:36 <Jane Doe> What's your login2016-10-21
>>> 10:56:29
>>>>>>> <John Doe> John_Doe2016-10-21 10:56:37 <John Doe>
>>>> Admit#82422016-10-21
>>>>>>> 11:00:13 <Jane Doe> Okay so you have a discussion
>>>>>>> 
>>>>>>> Notice Admit#82422016. So there's that.
>>>>>>> 
>>>>>>> Then I built object test2.
>>>>>>> 
>>>>>>> test2 <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4",
>>> test)
>>>>>>> 
>>>>>>> This worked for 84 lines, then this happened.
>>>>>> 
>>>>>> It may have done something but as you later discovered my first
>>> code
>>>> for
>>>>>> the pattern was incorrect. I had tested it (and pasted in the
>>> results
>>>> of
>>>>>> the test) . The way to refer to a capture class is with
>>> back-slashes
>>>>>> before the numbers, not forward-slashes. Try this:
>>>>>> 
>>>>>> 
>>>>>>> newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)",
>>> "\\1,\\2,\\3,\\4",
>>>> chrvec)
>>>>>>> newvec
>>>>>>  [1] "2016-07-01,02:50:35,<john>,hey"
>>>>>>  [2] "2016-07-01,02:51:26,<jane>,waiting for plane to Edinburgh"
>>>>>>  [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
>>>>>>  [4] "2016-07-01,02:52:07,<jane>,nothing crappy has happened,
>>> not
>>>> really"
>>>>>>  [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast,
>>> didn't
>>>> sleep"
>>>>>>  [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or
>>> where I am
>>>>>> really"
>>>>>>  [7] "2016-07-01,02:54:17,<john>,just know it's london"
>>>>>>  [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
>>>>>>  [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a good
>>> eay"
>>>>>> [10] "2016-07-01 02:58:56 <jone>"
>>>>>> [11] "2016-07-01 02:59:34 <jane>"
>>>>>> [12] "2016-07-01,03:02:48,<john>,British security is a little
>>> more
>>>>>> rigorous..."
>>>>>> 
>>>>>> 
>>>>>> I made note of the fact that the 10th and 11th lines had no
>>> commas.
>>>>>> 
>>>>>>> 
>>>>>>>> test2 [84]
>>>>>>> [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
>>>>>> 
>>>>>> That line didn't have any "<" so wasn't matched.
>>>>>> 
>>>>>> 
>>>>>> You could remove all none matching lines for pattern of
>>>>>> 
>>>>>> dates<space>times<space>"<"<name>">"<space><anything>
>>>>>> 
>>>>>> 
>>>>>> with:
>>>>>> 
>>>>>> 
>>>>>> chrvec <- chrvec[ grepl("^.{10} .{8} <.+> .+$)", chrvec)]
>>>>>> 
>>>>>> 
>>>>>> Do read:
>>>>>> 
>>>>>> ?read.csv
>>>>>> 
>>>>>> ?regex
>>>>>> 
>>>>>> 
>>>>>> --
>>>>>> 
>>>>>> David
>>>>>> 
>>>>>> 
>>>>>>>> test2 [85]
>>>>>>> [1] "//1,//2,//3,//4"
>>>>>>>> test [85]
>>>>>>> [1] "2016-07-01 02:50:35 <John Doe> hey"
>>>>>>> 
>>>>>>> Notice how I toggled back and forth between test and test2
>>> there. So,
>>>>>>> whatever happened with the regex, it happened in the switch
>>> from 84
>>>> to
>>>>>>> 85, I guess. It went on like
>>>>>>> 
>>>>>>> [990] "//1,//2,//3,//4"
>>>>>>>  [991] "//1,//2,//3,//4"
>>>>>>>  [992] "//1,//2,//3,//4"
>>>>>>>  [993] "//1,//2,//3,//4"
>>>>>>>  [994] "//1,//2,//3,//4"
>>>>>>>  [995] "//1,//2,//3,//4"
>>>>>>>  [996] "//1,//2,//3,//4"
>>>>>>>  [997] "//1,//2,//3,//4"
>>>>>>>  [998] "//1,//2,//3,//4"
>>>>>>>  [999] "//1,//2,//3,//4"
>>>>>>> [1000] "//1,//2,//3,//4"
>>>>>>> 
>>>>>>> up until line 1000, then I reached max.print.
>>>>>> 
>>>>>>> Michael
>>>>>>> 
>>>>>>> On Thu, May 16, 2019 at 1:05 PM David Winsemius <
>>>> dwinsemius at comcast.net>
>>>>>> wrote:
>>>>>>>> 
>>>>>>>> On 5/16/19 12:30 PM, Michael Boulineau wrote:
>>>>>>>>> Thanks for this tip on etiquette, David. I will be sure and
>>> not do
>>>>>> that again.
>>>>>>>>> 
>>>>>>>>> I tried the read.fwf from the foreign package, with a code
>>> like
>>>> this:
>>>>>>>>> 
>>>>>>>>>   d <- read.fwf("hangouts-conversation.txt",
>>>>>>>>>                  widths= c(10,10,20,40),
>>>>>>>>> 
>>> col.names=c("date","time","person","comment"),
>>>>>>>>>                  strip.white=TRUE)
>>>>>>>>> 
>>>>>>>>> But it threw this error:
>>>>>>>>> 
>>>>>>>>> Error in scan(file = file, what = what, sep = sep, quote =
>>> quote,
>>>> dec
>>>>>> = dec,  :
>>>>>>>>>    line 6347 did not have 4 elements
>>>>>>>> 
>>>>>>>> So what does line 6347 look like? (Use `readLines` and print
>>> it
>>>> out.)
>>>>>>>> 
>>>>>>>>> Interestingly, though, the error only happened when I
>>> increased the
>>>>>>>>> width size. But I had to increase the size, or else I
>>> couldn't
>>>> "see"
>>>>>>>>> anything.  The comment was so small that nothing was being
>>>> captured by
>>>>>>>>> the size of the column. so to speak.
>>>>>>>>> 
>>>>>>>>> It seems like what's throwing me is that there's no comma
>>> that
>>>>>>>>> demarcates the end of the text proper. For example:
>>>>>>>> Not sure why you thought there should be a comma. Lines
>>> usually end
>>>>>>>> with  <cr> and or a <lf>.
>>>>>>>> 
>>>>>>>> 
>>>>>>>> Once you have the raw text in a character vector from
>>> `readLines`
>>>> named,
>>>>>>>> say, 'chrvec', then you could selectively substitute commas
>>> for
>>>> spaces
>>>>>>>> with regex. (Now that you no longer desire to remove the dates
>>> and
>>>>>> times.)
>>>>>>>> 
>>>>>>>> sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", chrvec)
>>>>>>>> 
>>>>>>>> This will not do any replacements when the pattern is not
>>> matched.
>>>> See
>>>>>>>> this test:
>>>>>>>> 
>>>>>>>> 
>>>>>>>>> newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)",
>>> "\\1,\\2,\\3,\\4",
>>>>>> chrvec)
>>>>>>>>> newvec
>>>>>>>>   [1] "2016-07-01,02:50:35,<john>,hey"
>>>>>>>>   [2] "2016-07-01,02:51:26,<jane>,waiting for plane to
>>> Edinburgh"
>>>>>>>>   [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
>>>>>>>>   [4] "2016-07-01,02:52:07,<jane>,nothing crappy has
>>> happened, not
>>>>>> really"
>>>>>>>>   [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast,
>>> didn't
>>>>>> sleep"
>>>>>>>>   [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or
>>> where
>>>> I am
>>>>>>>> really"
>>>>>>>>   [7] "2016-07-01,02:54:17,<john>,just know it's london"
>>>>>>>>   [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
>>>>>>>>   [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a
>>> good
>>>> eay"
>>>>>>>> [10] "2016-07-01 02:58:56 <jone>"
>>>>>>>> [11] "2016-07-01 02:59:34 <jane>"
>>>>>>>> [12] "2016-07-01,03:02:48,<john>,British security is a little
>>> more
>>>>>>>> rigorous..."
>>>>>>>> 
>>>>>>>> 
>>>>>>>> You should probably remove the "empty comment" lines.
>>>>>>>> 
>>>>>>>> 
>>>>>>>> --
>>>>>>>> 
>>>>>>>> David.
>>>>>>>> 
>>>>>>>>> 2016-07-01 15:34:30 <John Doe> Lame. We were in a
>>>> starbucks2016-07-01
>>>>>>>>> 15:35:02 <Jane Doe> Hmm that's interesting2016-07-01 15:35:09
>>> <Jane
>>>>>>>>> Doe> You must want coffees2016-07-01 15:35:25 <John Doe>
>>> There was
>>>>>>>>> lots of Starbucks in my day2016-07-01 15:35:47
>>>>>>>>> 
>>>>>>>>> It was interesting, too, when I pasted the text into the
>>> email, it
>>>>>>>>> self-formatted into the way I wanted it to look. I had to
>>> manually
>>>>>>>>> make it look like it does above, since that's the way that it
>>>> looks in
>>>>>>>>> the txt file. I wonder if it's being organized by XML or
>>> something.
>>>>>>>>> 
>>>>>>>>> Anyways, There's always a space between the two sideways
>>> carrots,
>>>> just
>>>>>>>>> like there is right now: <John Doe> See. Space. And there's
>>> always
>>>> a
>>>>>>>>> space between the data and time. Like this. 2016-07-01
>>> 15:34:30
>>>> See.
>>>>>>>>> Space. But there's never a space between the end of the
>>> comment and
>>>>>>>>> the next date. Like this: We were in a starbucks2016-07-01
>>> 15:35:02
>>>>>>>>> See. starbucks and 2016 are smooshed together.
>>>>>>>>> 
>>>>>>>>> This code is also on the table right now too.
>>>>>>>>> 
>>>>>>>>> a <- read.table("E:/working
>>>>>>>>> directory/-189/hangouts-conversation2.txt", quote="\"",
>>>>>>>>> comment.char="", fill=TRUE)
>>>>>>>>> 
>>>>>>>>> 
>>>>>> 
>>>> 
>>> h<-cbind(hangouts.conversation2[,1:2],hangouts.conversation2[,3:5],hangouts.conversation2[,6:9])
>>>>>>>>> 
>>>>>>>>> aa<-gsub("[^[:digit:]]","",h)
>>>>>>>>> my.data.num <- as.numeric(str_extract(h, "[0-9]+"))
>>>>>>>>> 
>>>>>>>>> Those last lines are a work in progress. I wish I could
>>> import a
>>>>>>>>> picture of what it looks like when it's translated into a
>>> data
>>>> frame.
>>>>>>>>> The fill=TRUE helped to get the data in table that kind of
>>> sort of
>>>>>>>>> works, but the comments keep bleeding into the data and time
>>>> column.
>>>>>>>>> It's like
>>>>>>>>> 
>>>>>>>>> 2016-07-01 15:59:17 <Jane Doe> Seriously I've never been
>>>>>>>>> over               there
>>>>>>>>> 2016-07-01 15:59:27 <Jane Doe> It confuses me :(
>>>>>>>>> 
>>>>>>>>> And then, maybe, the "seriously" will be in a column all to
>>>> itself, as
>>>>>>>>> will be the "I've'"and the "never" etc.
>>>>>>>>> 
>>>>>>>>> I will use a regular expression if I have to, but it would be
>>> nice
>>>> to
>>>>>>>>> keep the dates and times on there. Originally, I thought they
>>> were
>>>>>>>>> meaningless, but I've since changed my mind on that count.
>>> The
>>>> time of
>>>>>>>>> day isn't so important. But, especially since, say, Gmail
>>> itself
>>>> knows
>>>>>>>>> how to quickly recognize what it is, I know it can be done. I
>>> know
>>>>>>>>> this data has structure to it.
>>>>>>>>> 
>>>>>>>>> Michael
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> On Wed, May 15, 2019 at 8:47 PM David Winsemius <
>>>>>> dwinsemius at comcast.net> wrote:
>>>>>>>>>> On 5/15/19 4:07 PM, Michael Boulineau wrote:
>>>>>>>>>>> I have a wild and crazy text file, the head of which looks
>>> like
>>>> this:
>>>>>>>>>>> 
>>>>>>>>>>> 2016-07-01 02:50:35 <john> hey
>>>>>>>>>>> 2016-07-01 02:51:26 <jane> waiting for plane to Edinburgh
>>>>>>>>>>> 2016-07-01 02:51:45 <john> thinking about my boo
>>>>>>>>>>> 2016-07-01 02:52:07 <jane> nothing crappy has happened, not
>>>> really
>>>>>>>>>>> 2016-07-01 02:52:20 <john> plane went by pretty fast,
>>> didn't
>>>> sleep
>>>>>>>>>>> 2016-07-01 02:54:08 <jane> no idea what time it is or where
>>> I am
>>>>>> really
>>>>>>>>>>> 2016-07-01 02:54:17 <john> just know it's london
>>>>>>>>>>> 2016-07-01 02:56:44 <jane> you are probably asleep
>>>>>>>>>>> 2016-07-01 02:58:45 <jane> I hope fish was fishy in a good
>>> eay
>>>>>>>>>>> 2016-07-01 02:58:56 <jone>
>>>>>>>>>>> 2016-07-01 02:59:34 <jane>
>>>>>>>>>>> 2016-07-01 03:02:48 <john> British security is a little
>>> more
>>>>>> rigorous...
>>>>>>>>>> Looks entirely not-"crazy". Typical log file format.
>>>>>>>>>> 
>>>>>>>>>> Two possibilities: 1) Use `read.fwf` from pkg foreign; 2)
>>> Use
>>>> regex
>>>>>>>>>> (i.e. the sub-function) to strip everything up to the "<".
>>> Read
>>>>>>>>>> `?regex`. Since that's not a metacharacters you could use a
>>>> pattern
>>>>>>>>>> ".+<" and replace with "".
>>>>>>>>>> 
>>>>>>>>>> And do read the Posting Guide. Cross-posting to
>>> StackOverflow and
>>>>>> Rhelp,
>>>>>>>>>> at least within hours of each, is considered poor manners.
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> --
>>>>>>>>>> 
>>>>>>>>>> David.
>>>>>>>>>> 
>>>>>>>>>>> It goes on for a while. It's a big file. But I feel like
>>> it's
>>>> going
>>>>>> to
>>>>>>>>>>> be difficult to annotate with the coreNLP library or
>>> package. I'm
>>>>>>>>>>> doing natural language processing. In other words, I'm
>>> curious
>>>> as to
>>>>>>>>>>> how I would shave off the dates, that is, to make it look
>>> like:
>>>>>>>>>>> 
>>>>>>>>>>> <john> hey
>>>>>>>>>>> <jane> waiting for plane to Edinburgh
>>>>>>>>>>>    <john> thinking about my boo
>>>>>>>>>>> <jane> nothing crappy has happened, not really
>>>>>>>>>>> <john> plane went by pretty fast, didn't sleep
>>>>>>>>>>> <jane> no idea what time it is or where I am really
>>>>>>>>>>> <john> just know it's london
>>>>>>>>>>> <jane> you are probably asleep
>>>>>>>>>>> <jane> I hope fish was fishy in a good eay
>>>>>>>>>>>    <jone>
>>>>>>>>>>> <jane>
>>>>>>>>>>> <john> British security is a little more rigorous...
>>>>>>>>>>> 
>>>>>>>>>>> To be clear, then, I'm trying to clean a large text file by
>>>> writing a
>>>>>>>>>>> regular expression? such that I create a new object with no
>>>> numbers
>>>>>> or
>>>>>>>>>>> dates.
>>>>>>>>>>> 
>>>>>>>>>>> Michael
>>>>>>>>>>> 
>>>>>>>>>>> ______________________________________________
>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
>>> more,
>>>> see
>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>>> and provide commented, minimal, self-contained,
>>> reproducible
>>>> code.
>>>>>>>>> ______________________________________________
>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>> see
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>> code.
>>>>>>>> ______________________________________________
>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>> see
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>> code.
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>> see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible
>>> code.
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible
>>> code.
>>>>>> 
>>>>> 
>>>>>        [[alternative HTML version deleted]]
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>> 
>>>      [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> --
>> Sent from my phone. Please excuse my brevity.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From er|nm@hodge@@ @end|ng |rom gm@||@com  Sat May 18 05:18:26 2019
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Fri, 17 May 2019 21:18:26 -0600
Subject: [R] Kriging
In-Reply-To: <CA+tCXtPwf625MEfO6j69B_Ywwau8PGhGmGPio2umEC8kPgDHDA@mail.gmail.com>
References: <CA+tCXtPwf625MEfO6j69B_Ywwau8PGhGmGPio2umEC8kPgDHDA@mail.gmail.com>
Message-ID: <CACxE24mMRyTk3H_EgoaSG6k20e7EjeeMDrGLWx25vpjyCPx7Pw@mail.gmail.com>

Hi!

If you have a Spatial Points Data Frame, you can use the following function
to create a grid:

create1 <- function(obj) {

# Function that creates a new_data object if one is missing

        convex_hull = chull(coordinates(obj)[,1],coordinates(obj)[,2])

        convex_hull = c(convex_hull, convex_hull[1]) # Close the polygon

        d = Polygon(coordinates(obj)[convex_hull, ])

        new_data = spsample(d, 20, type = "regular")

        gridded(new_data) = TRUE

        return(new_data)

}



Hope this helps!


Sincerely,

Erin




Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com


On Fri, May 17, 2019 at 8:01 AM DINESHKUMAR <dineshchandrasekar.dk at gmail.com>
wrote:

> how to create prediction grid like meuse grid
> *Thanks & Regards*
>
> *C Dineshkumar*
> *Bsc Agriculture*
> *M.Tech Remote Sensing and GIS*
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From pd@|gd @end|ng |rom gm@||@com  Sat May 18 10:44:04 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Sat, 18 May 2019 10:44:04 +0200
Subject: [R] 
 Help understanding the relationship between R-3.6.0 and RStudio
In-Reply-To: <BN7PR02MB507396D097441034D439D217EA0B0@BN7PR02MB5073.namprd02.prod.outlook.com>
References: <BN7PR02MB507382FA2FDB673BFB754A9FEA0B0@BN7PR02MB5073.namprd02.prod.outlook.com>
 <9548F469-06ED-4599-B210-94CC05663D3C@me.com>
 <BN7PR02MB507396D097441034D439D217EA0B0@BN7PR02MB5073.namprd02.prod.outlook.com>
Message-ID: <4B4B0A41-171E-425D-BDE9-490C8184D770@gmail.com>

Actually, you might go for 3.6.0-patched. There was a somewhat annoying bug affecting the package installation menu in 3.6.0.

-pd

> On 17 May 2019, at 20:32 , Bill Poling <Bill.Poling at zelis.com> wrote:
> 
> I fixed it by removing previous versions as suggested.
> 
>> sessionInfo()
> R version 3.6.0 RC (2019-04-24 r76423)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows 10 x64 (build 17134)
> 
> I will have to go out and get the non RC version now.
> 
> Thank you.
> 
> WHP
> 
> From: Marc Schwartz <marc_schwartz at me.com>
> Sent: Friday, May 17, 2019 2:14 PM
> To: Bill Poling <Bill.Poling at zelis.com>
> Cc: R-help <r-help at r-project.org>
> Subject: Re: [R] Help understanding the relationship between R-3.6.0 and RStudio
> 
> 
> 
>> On May 17, 2019, at 2:02 PM, Bill Poling <mailto:Bill.Poling at zelis.com> wrote:
>> 
>> Hello.
>> 
>> I do not think I have had this problem (assuming it is a problem) in the past.
>> 
>> I downloaded and installed R3.6.0 which is indicted in the console when I open R itself.
>> 
>> R version 3.6.0 RC (2019-04-24 r76423) -- "Planting of a Tree"
>> Copyright (C) 2019 The R Foundation for Statistical Computing
>> Platform: x86_64-w64-mingw32/x64 (64-bit)
>> 
>> However, in RStudio the sessionInfo() remains
>> 
>> R version 3.5.3 (2019-03-11)
>> Platform: x86_64-w64-mingw32/x64 (64-bit)
>> Running under: Windows 10 x64 (build 17134)
>> 
>> I also installed the latest version of RStudio 1.2.1335 as well "after" installing R 3.6.0.
>> 
>> I also rebooted my computer.
>> 
>> I am not sure why this time the two do not seem to be (for lack of a better word) in sink?
>> 
>> Thank you for any insight
>> 
>> WHP
> 
> 
> Hi,
> 
> I don't use RStudio, which is a GUI/IDE on top of R, it is not R.
> 
> That being said, a quick Google search supports my intuition, which is that RStudio appears to be able to support multiple R version installations:
> 
> https://support.rstudio.com/hc/en-us/articles/200486138-Changing-R-versions-for-RStudio-desktop
> 
> RStudio also has their own support venue:
> 
> https://support.rstudio.com/hc/en-us
> 
> If I read correctly, it looks like you actually installed a "Release Candidate" (RC) version of 3.6.0 for Windows. So you probably want to visit a CRAN mirror and download the release version of 3.6.0:
> 
> R version 3.6.0 (2019-04-26) -- "Planting of a Tree"
> 
> If you do not want to have multiple R versions on your computer, you can use the normal Windows application uninstall process to remove the older version(s).
> 
> Regards,
> 
> Marc Schwartz
> 
> Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Sat May 18 15:57:06 2019
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Sat, 18 May 2019 13:57:06 +0000 (UTC)
Subject: [R] Nested structure data simulation
References: <1342550043.2326226.1558187826295.ref@mail.yahoo.com>
Message-ID: <1342550043.2326226.1558187826295@mail.yahoo.com>

Dear R-Experts,

In a data simulation, I would like a balanced distribution with a nested structure for classroom and teacher (not for school). I mean 50 pupils belonging to C1, 50 other pupils belonging to C2, 50 other pupils belonging to C3 and so on. Then I want the 50 pupils belonging to C1 with T1, the 50 pupils belonging to C2 with T2, the 50 pupils belonging to C3 with T3 and so on. The school don?t have to be nested, I just want a balanced distribution, I mean 60 pupils in S1, 60 other pupils in S2 and so on. 
Here below the reproducible example. 
Many thanks for your help.

##############
set.seed(123) ? 
# G?n?ration al?atoire des colonnes 
pupils<-1:300 ? 
classroom<-sample(c("C1","C2","C3","C4","C5","C6"),300,replace=T) ? teacher<-sample(c("T1","T2","T3","T4","T5","T6"),300,replace=T) ? school<-sample(c("S1","S2","S3","S4","S5"),300,replace=T) ? ? 
##############


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sat May 18 16:17:20 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 18 May 2019 07:17:20 -0700
Subject: [R] Nested structure data simulation
In-Reply-To: <1342550043.2326226.1558187826295@mail.yahoo.com>
References: <1342550043.2326226.1558187826295.ref@mail.yahoo.com>
 <1342550043.2326226.1558187826295@mail.yahoo.com>
Message-ID: <DA9B4E0F-A768-4D8C-A635-F8551A49E417@dcn.davis.ca.us>

Wouldn't the students/teachers/schools be enumerated and the properties you are studying be random/correlated according to the enumerated values?

On May 18, 2019 6:57:06 AM PDT, varin sacha via R-help <r-help at r-project.org> wrote:
>Dear R-Experts,
>
>In a data simulation, I would like a balanced distribution with a
>nested structure for classroom and teacher (not for school). I mean 50
>pupils belonging to C1, 50 other pupils belonging to C2, 50 other
>pupils belonging to C3 and so on. Then I want the 50 pupils belonging
>to C1 with T1, the 50 pupils belonging to C2 with T2, the 50 pupils
>belonging to C3 with T3 and so on. The school don?t have to be nested,
>I just want a balanced distribution, I mean 60 pupils in S1, 60 other
>pupils in S2 and so on. 
>Here below the reproducible example. 
>Many thanks for your help.
>
>##############
>set.seed(123) ? 
># G?n?ration al?atoire des colonnes 
>pupils<-1:300 ? 
>classroom<-sample(c("C1","C2","C3","C4","C5","C6"),300,replace=T) ?
>teacher<-sample(c("T1","T2","T3","T4","T5","T6"),300,replace=T) ?
>school<-sample(c("S1","S2","S3","S4","S5"),300,replace=T) ? ? 
>##############
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Sat May 18 16:40:50 2019
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Sat, 18 May 2019 14:40:50 +0000 (UTC)
Subject: [R] Nested structure data simulation
In-Reply-To: <CAPm+3sBE3RX6kpogMXDXB6G+nHYWvsGTX4duztmM+OCDpRT76Q@mail.gmail.com>
References: <1342550043.2326226.1558187826295.ref@mail.yahoo.com>
 <1342550043.2326226.1558187826295@mail.yahoo.com>
 <CAPm+3sBE3RX6kpogMXDXB6G+nHYWvsGTX4duztmM+OCDpRT76Q@mail.gmail.com>
Message-ID: <795568903.3860573.1558190450476@mail.yahoo.com>

Many thanks Jeff and Linus,

Yes to Jeff,

OK with Linus but....

classroom <- rep(c("C1","C2","C3","C4","C5","C6"), 50) [sample(1:300)] 

how can I include the nested structure, I mean the teacher. Now, I would like the 50 pupils belonging to C1 with T1, the 50 pupils belonging to C2 with T2, the 50 pupils belonging to C3 with T3 and so on.






Le samedi 18 mai 2019 ? 16:20:33 UTC+2, Linus Chen <linus.l.chen at gmail.com> a ?crit : 





Dear varin sacha,

Not very sure what you want, but will the following help a little?

tmp <- rep(c("C1","C2","C3","C4","C5","C6"), 50) # make a character
vector, with 50 "C1", 50 "C2", ...
classroom <- tmp[sample(1:300)] # make a random permutation.

Certainly you may also make it into one line:
classroom <- rep(c("C1","C2","C3","C4","C5","C6"), 50) [sample(1:30)]

Best,
Lei Chen

On Sat, May 18, 2019 at 3:57 PM varin sacha via R-help
<r-help at r-project.org> wrote:
>
> Dear R-Experts,
>
> In a data simulation, I would like a balanced distribution with a nested structure for classroom and teacher (not for school). I mean 50 pupils belonging to C1, 50 other pupils belonging to C2, 50 other pupils belonging to C3 and so on. Then I want the 50 pupils belonging to C1 with T1, the 50 pupils belonging to C2 with T2, the 50 pupils belonging to C3 with T3 and so on. The school don?t have to be nested, I just want a balanced distribution, I mean 60 pupils in S1, 60 other pupils in S2 and so on.
> Here below the reproducible example.
> Many thanks for your help.
>
> ##############
> set.seed(123)
> # G?n?ration al?atoire des colonnes
> pupils<-1:300
> classroom<-sample(c("C1","C2","C3","C4","C5","C6"),300,replace=T)? teacher<-sample(c("T1","T2","T3","T4","T5","T6"),300,replace=T)? school<-sample(c("S1","S2","S3","S4","S5"),300,replace=T)

> ##############
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bor|@@@te|pe @end|ng |rom utoronto@c@  Sat May 18 16:52:46 2019
From: bor|@@@te|pe @end|ng |rom utoronto@c@ (Boris Steipe)
Date: Sat, 18 May 2019 14:52:46 +0000
Subject: [R] Nested structure data simulation
In-Reply-To: <1342550043.2326226.1558187826295@mail.yahoo.com>
References: <1342550043.2326226.1558187826295.ref@mail.yahoo.com>
 <1342550043.2326226.1558187826295@mail.yahoo.com>
Message-ID: <BBDA4312-A62F-450A-9E9F-655DAF278FF2@utoronto.ca>

Can you build your data top-down?



schools <- paste("s", 1:6, sep="")

classes <- character()
for (school in schools) {
  classes <- c(classes, paste(school, paste("c", 1:5, sep=""), sep = "."))
}

pupils <- character()
for (class in classes) {
  pupils <- c(pupils, paste(class, paste("p", 1:10, sep=""), sep = "."))
}



B.



> On 2019-05-18, at 09:57, varin sacha via R-help <r-help at r-project.org> wrote:
> 
> Dear R-Experts,
> 
> In a data simulation, I would like a balanced distribution with a nested structure for classroom and teacher (not for school). I mean 50 pupils belonging to C1, 50 other pupils belonging to C2, 50 other pupils belonging to C3 and so on. Then I want the 50 pupils belonging to C1 with T1, the 50 pupils belonging to C2 with T2, the 50 pupils belonging to C3 with T3 and so on. The school don?t have to be nested, I just want a balanced distribution, I mean 60 pupils in S1, 60 other pupils in S2 and so on. 
> Here below the reproducible example. 
> Many thanks for your help.
> 
> ##############
> set.seed(123)   
> # G?n?ration al?atoire des colonnes 
> pupils<-1:300   
> classroom<-sample(c("C1","C2","C3","C4","C5","C6"),300,replace=T)   teacher<-sample(c("T1","T2","T3","T4","T5","T6"),300,replace=T)   school<-sample(c("S1","S2","S3","S4","S5"),300,replace=T)     
> ##############
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Sat May 18 22:03:37 2019
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Sat, 18 May 2019 20:03:37 +0000 (UTC)
Subject: [R] Nested structure data simulation
In-Reply-To: <BBDA4312-A62F-450A-9E9F-655DAF278FF2@utoronto.ca>
References: <1342550043.2326226.1558187826295.ref@mail.yahoo.com>
 <1342550043.2326226.1558187826295@mail.yahoo.com>
 <BBDA4312-A62F-450A-9E9F-655DAF278FF2@utoronto.ca>
Message-ID: <570252025.3999934.1558209817043@mail.yahoo.com>

Dear Boris,

Yes, top-down, no problem. Many thanks, but in your code did you not forget "teacher" ? As a reminder teacher has to be nested with classes. I mean the 50 pupils belonging to C1 must be with (teacher 1) T1, the 50 pupils belonging to C2 with T2, the 50 pupils belonging to C3 with T3 and so on.

Best,


Le samedi 18 mai 2019 ? 16:52:48 UTC+2, Boris Steipe <boris.steipe at utoronto.ca> a ?crit : 





Can you build your data top-down?



schools <- paste("s", 1:6, sep="")

classes <- character()
for (school in schools) {
? classes <- c(classes, paste(school, paste("c", 1:5, sep=""), sep = "."))
}

pupils <- character()
for (class in classes) {
? pupils <- c(pupils, paste(class, paste("p", 1:10, sep=""), sep = "."))
}



B.



> On 2019-05-18, at 09:57, varin sacha via R-help <r-help at r-project.org> wrote:
> 
> Dear R-Experts,
> 
> In a data simulation, I would like a balanced distribution with a nested structure for classroom and teacher (not for school). I mean 50 pupils belonging to C1, 50 other pupils belonging to C2, 50 other pupils belonging to C3 and so on. Then I want the 50 pupils belonging to C1 with T1, the 50 pupils belonging to C2 with T2, the 50 pupils belonging to C3 with T3 and so on. The school don?t have to be nested, I just want a balanced distribution, I mean 60 pupils in S1, 60 other pupils in S2 and so on. 
> Here below the reproducible example. 
> Many thanks for your help.
> 
> ##############
> set.seed(123)? 
> # G?n?ration al?atoire des colonnes 
> pupils<-1:300? 
> classroom<-sample(c("C1","C2","C3","C4","C5","C6"),300,replace=T)? teacher<-sample(c("T1","T2","T3","T4","T5","T6"),300,replace=T)? school<-sample(c("S1","S2","S3","S4","S5"),300,replace=T)? ? 

> ##############
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From djnord|und @end|ng |rom gm@||@com  Sat May 18 22:26:14 2019
From: djnord|und @end|ng |rom gm@||@com (Daniel Nordlund)
Date: Sat, 18 May 2019 13:26:14 -0700
Subject: [R] Nested structure data simulation
In-Reply-To: <570252025.3999934.1558209817043@mail.yahoo.com>
References: <1342550043.2326226.1558187826295.ref@mail.yahoo.com>
 <1342550043.2326226.1558187826295@mail.yahoo.com>
 <BBDA4312-A62F-450A-9E9F-655DAF278FF2@utoronto.ca>
 <570252025.3999934.1558209817043@mail.yahoo.com>
Message-ID: <7b4b91f7-5e4f-fa1f-d066-2ac408adc003@gmail.com>


On 5/18/2019 1:03 PM, varin sacha via R-help wrote:
> Dear Boris,
>
> Yes, top-down, no problem. Many thanks, but in your code did you not forget "teacher" ? As a reminder teacher has to be nested with classes. I mean the 50 pupils belonging to C1 must be with (teacher 1) T1, the 50 pupils belonging to C2 with T2, the 50 pupils belonging to C3 with T3 and so on.
>
> Best,
>
>
> Le samedi 18 mai 2019 ? 16:52:48 UTC+2, Boris Steipe <boris.steipe at utoronto.ca> a ?crit :
>
>
>
<<<snip>>>

Given your design, you cannot distinguish between class and teacher. 
They are one and the same thing.? It doesn't make sense to include both 
in your model.

Dan

-- 
Daniel Nordlund
Port Townsend, WA  USA


From m|ch@e|@p@bou||ne@u @end|ng |rom gm@||@com  Sun May 19 00:32:53 2019
From: m|ch@e|@p@bou||ne@u @end|ng |rom gm@||@com (Michael Boulineau)
Date: Sat, 18 May 2019 15:32:53 -0700
Subject: [R] how to separate string from numbers in a large txt file
In-Reply-To: <9B170703-AAAF-4D52-9744-1FDE70559B05@utoronto.ca>
References: <CAH+cTGM9sFCxegaMHzu-_9Prhvvh9MbVDCWJ1_x7wkO9jC47mQ@mail.gmail.com>
 <c6e69644-66a1-7031-4d7c-1000e0abbea4@comcast.net>
 <CAH+cTGM5YvWV_G0pRgMqjCzuRrFTzd0nOc7yoThfGbGMUyMjsQ@mail.gmail.com>
 <9c9f1c81-309f-91cb-7e26-f02fc4bd45d8@comcast.net>
 <CAH+cTGMx4_Lz2gGDf2AuUaaZF2Xaq4McuPRoHu4JJNc-ORd=Ew@mail.gmail.com>
 <6322e735-5659-848e-3a50-9f066f1299ad@comcast.net>
 <CAF8bMcbgrVcN9J7Aygq2D5RWaU4MS-GH+_+zMHU=AO+DU5DVWw@mail.gmail.com>
 <CAH+cTGNfpLSurqmsoMeci0GWhC0ncOp7zTEju3fJW5T4+tCsGA@mail.gmail.com>
 <CAF8bMcaSJRaUXK4h2ssaDoJwXaKCUZT2Vq1cutNiNDjQ7ga2Xg@mail.gmail.com>
 <EEA9FD46-A67C-4FB7-8245-350819C4EA62@dcn.davis.ca.us>
 <CAH+cTGNOkAnPnn-aru-BJUbhrgeYMQBTqjSYT764nKzcdXvTvA@mail.gmail.com>
 <9B170703-AAAF-4D52-9744-1FDE70559B05@utoronto.ca>
Message-ID: <CAH+cTGNaw7p-it4B7te4BBsay=v6J-5OS-P_WGDJJ6Z0ymxtvw@mail.gmail.com>

Going back and thinking through what Boris and William were saying
(also Ivan), I tried this:

a <- readLines ("hangouts-conversation-6.csv.txt")
b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)"
c <- gsub(b, "\\1<\\2> ", a)
> head (c)
[1] "???2016-01-27 09:14:40 *** Jane Doe started a video chat"
[2] "2016-01-27 09:15:20 <Jane Doe>
https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf"
[3] "2016-01-27 09:15:20 <Jane Doe> Hey "
[4] "2016-01-27 09:15:22 <John Doe>  ended a video chat"
[5] "2016-01-27 21:07:11 <Jane Doe>  started a video chat"
[6] "2016-01-27 21:26:57 <John Doe>  ended a video chat"

The ??? is still there, since I forgot to do what Ivan had suggested, namely,

a <- readLines(con <- file("hangouts-conversation-6.csv.txt", encoding
= "UTF-8")); close(con); rm(con)

But then the new code is still turning out only NAs when I apply
strcapture (). This was what happened next:

> d <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
+ [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
+                 c, proto=data.frame(stringsAsFactors=FALSE, When="", Who="",
+                                     What=""))
> head (d)
  When  Who What
1 <NA> <NA> <NA>
2 <NA> <NA> <NA>
3 <NA> <NA> <NA>
4 <NA> <NA> <NA>
5 <NA> <NA> <NA>
6 <NA> <NA> <NA>

I've been reading up on regular expressions, too, so this code seems
spot on. What's going wrong?

Michael

On Fri, May 17, 2019 at 4:28 PM Boris Steipe <boris.steipe at utoronto.ca> wrote:
>
> Don't start putting in extra commas and then reading this as csv. That approach is broken. The correct approach is what Bill outlined: read everything with readLines(), and then use a proper regular expression with strcapture().
>
> You need to pre-process the object that readLines() gives you: replace the contents of the videochat lines, and make it conform to the format of the other lines before you process it into your data frame.
>
> Approximately something like
>
> # read the raw data
> tmp <- readLines("hangouts-conversation-6.csv.txt")
>
> # process all video chat lines
> patt <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+) "  # (year time )*** (word word)
> tmp <- gsub(patt, "\\1<\\2> ", tmp)
>
> # next, use strcapture()
>
> Note that this makes the assumption that your names are always exactly two words containing only letters. If that assumption is not true, more though needs to go into the regex. But you can test that:
>
> patt <- " <\\w+ \\w+> "   #" <word word> "
> sum( ! grepl(patt, tmp)))
>
> ... will give the number of lines that remain in your file that do not have a tag that can be interpreted as "Who"
>
> Once that is fine, use Bill's approach - or a regular expression of your own design - to create your data frame.
>
> Hope this helps,
> Boris
>
>
>
>
> > On 2019-05-17, at 16:18, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
> >
> > Very interesting. I'm sure I'll be trying to get rid of the byte order
> > mark eventually. But right now, I'm more worried about getting the
> > character vector into either a csv file or data.frame; that way, I can
> > be able to work with the data neatly tabulated into four columns:
> > date, time, person, comment. I assume it's a write.csv function, but I
> > don't know what arguments to put in it. header=FALSE? fill=T?
> >
> > Micheal
> >
> > On Fri, May 17, 2019 at 1:03 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> >>
> >> If byte order mark is the issue then you can specify the file encoding as "UTF-8-BOM" and it won't show up in your data any more.
> >>
> >> On May 17, 2019 12:12:17 PM PDT, William Dunlap via R-help <r-help at r-project.org> wrote:
> >>> The pattern I gave worked for the lines that you originally showed from
> >>> the
> >>> data file ('a'), before you put commas into them.  If the name is
> >>> either of
> >>> the form "<name>" or "***" then the "(<[^>]*>)" needs to be changed so
> >>> something like "(<[^>]*>|[*]{3})".
> >>>
> >>> The " ???" at the start of the imported data may come from the byte
> >>> order
> >>> mark that Windows apps like to put at the front of a text file in UTF-8
> >>> or
> >>> UTF-16 format.
> >>>
> >>> Bill Dunlap
> >>> TIBCO Software
> >>> wdunlap tibco.com
> >>>
> >>>
> >>> On Fri, May 17, 2019 at 11:53 AM Michael Boulineau <
> >>> michael.p.boulineau at gmail.com> wrote:
> >>>
> >>>> This seemed to work:
> >>>>
> >>>>> a <- readLines ("hangouts-conversation-6.csv.txt")
> >>>>> b <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4", a)
> >>>>> b [1:84]
> >>>>
> >>>> And the first 85 lines looks like this:
> >>>>
> >>>> [83] "2016-06-28 21:02:28 *** Jane Doe started a video chat"
> >>>> [84] "2016-06-28 21:12:43 *** John Doe ended a video chat"
> >>>>
> >>>> Then they transition to the commas:
> >>>>
> >>>>> b [84:100]
> >>>> [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
> >>>> [2] "2016-07-01,02:50:35,<John Doe>,hey"
> >>>> [3] "2016-07-01,02:51:26,<John Doe>,waiting for plane to Edinburgh"
> >>>> [4] "2016-07-01,02:51:45,<John Doe>,thinking about my boo"
> >>>>
> >>>> Even the strange bit on line 6347 was caught by this:
> >>>>
> >>>>> b [6346:6348]
> >>>> [1] "2016-10-21,10:56:29,<John Doe>,John_Doe"
> >>>> [2] "2016-10-21,10:56:37,<John Doe>,Admit#8242"
> >>>> [3] "2016-10-21,11:00:13,<Jane Doe>,Okay so you have a discussion"
> >>>>
> >>>> Perhaps most awesomely, the code catches spaces that are interposed
> >>>> into the comment itself:
> >>>>
> >>>>> b [4]
> >>>> [1] "2016-01-27,09:15:20,<Jane Doe>,Hey "
> >>>>> b [85]
> >>>> [1] "2016-07-01,02:50:35,<John Doe>,hey"
> >>>>
> >>>> Notice whether there is a space after the "hey" or not.
> >>>>
> >>>> These are the first two lines:
> >>>>
> >>>> [1] "???2016-01-27 09:14:40 *** Jane Doe started a video chat"
> >>>> [2] "2016-01-27,09:15:20,<Jane
> >>>> Doe>,
> >>>>
> >>> https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf
> >>>> "
> >>>>
> >>>> So, who knows what happened with the ??? at the beginning of [1]
> >>>> directly above. But notice how there are no commas in [1] but there
> >>>> appear in [2]. I don't see why really long ones like [2] directly
> >>>> above would be a problem, were they to be translated into a csv or
> >>>> data frame column.
> >>>>
> >>>> Now, with the commas in there, couldn't we write this into a csv or a
> >>>> data.frame? Some of this data will end up being garbage, I imagine.
> >>>> Like in [2] directly above. Or with [83] and [84] at the top of this
> >>>> discussion post/email. Embarrassingly, I've been trying to convert
> >>>> this into a data.frame or csv but I can't manage to. I've been using
> >>>> the write.csv function, but I don't think I've been getting the
> >>>> arguments correct.
> >>>>
> >>>> At the end of the day, I would like a data.frame and/or csv with the
> >>>> following four columns: date, time, person, comment.
> >>>>
> >>>> I tried this, too:
> >>>>
> >>>>> c <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
> >>>> + [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
> >>>> +                 a, proto=data.frame(stringsAsFactors=FALSE,
> >>> When="",
> >>>> Who="",
> >>>> +                                     What=""))
> >>>>
> >>>> But all I got was this:
> >>>>
> >>>>> c [1:100, ]
> >>>>    When  Who What
> >>>> 1   <NA> <NA> <NA>
> >>>> 2   <NA> <NA> <NA>
> >>>> 3   <NA> <NA> <NA>
> >>>> 4   <NA> <NA> <NA>
> >>>> 5   <NA> <NA> <NA>
> >>>> 6   <NA> <NA> <NA>
> >>>>
> >>>> It seems to have caught nothing.
> >>>>
> >>>>> unique (c)
> >>>>  When  Who What
> >>>> 1 <NA> <NA> <NA>
> >>>>
> >>>> But I like that it converted into columns. That's a really great
> >>>> format. With a little tweaking, it'd be a great code for this data
> >>>> set.
> >>>>
> >>>> Michael
> >>>>
> >>>> On Fri, May 17, 2019 at 8:20 AM William Dunlap via R-help
> >>>> <r-help at r-project.org> wrote:
> >>>>>
> >>>>> Consider using readLines() and strcapture() for reading such a
> >>> file.
> >>>> E.g.,
> >>>>> suppose readLines(files) produced a character vector like
> >>>>>
> >>>>> x <- c("2016-10-21 10:35:36 <Jane Doe> What's your login",
> >>>>>          "2016-10-21 10:56:29 <John Doe> John_Doe",
> >>>>>          "2016-10-21 10:56:37 <John Doe> Admit#8242",
> >>>>>          "October 23, 1819 12:34 <Jane Eyre> I am not an angel")
> >>>>>
> >>>>> Then you can make a data.frame with columns When, Who, and What by
> >>>>> supplying a pattern containing three parenthesized capture
> >>> expressions:
> >>>>>> z <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
> >>>>> [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
> >>>>>             x, proto=data.frame(stringsAsFactors=FALSE, When="",
> >>> Who="",
> >>>>> What=""))
> >>>>>> str(z)
> >>>>> 'data.frame':   4 obs. of  3 variables:
> >>>>> $ When: chr  "2016-10-21 10:35:36" "2016-10-21 10:56:29"
> >>> "2016-10-21
> >>>>> 10:56:37" NA
> >>>>> $ Who : chr  "<Jane Doe>" "<John Doe>" "<John Doe>" NA
> >>>>> $ What: chr  "What's your login" "John_Doe" "Admit#8242" NA
> >>>>>
> >>>>> Lines that don't match the pattern result in NA's - you might make
> >>> a
> >>>> second
> >>>>> pass over the corresponding elements of x with a new pattern.
> >>>>>
> >>>>> You can convert the When column from character to time with
> >>> as.POSIXct().
> >>>>>
> >>>>> Bill Dunlap
> >>>>> TIBCO Software
> >>>>> wdunlap tibco.com
> >>>>>
> >>>>>
> >>>>> On Thu, May 16, 2019 at 8:30 PM David Winsemius
> >>> <dwinsemius at comcast.net>
> >>>>> wrote:
> >>>>>
> >>>>>>
> >>>>>> On 5/16/19 3:53 PM, Michael Boulineau wrote:
> >>>>>>> OK. So, I named the object test and then checked the 6347th
> >>> item
> >>>>>>>
> >>>>>>>> test <- readLines ("hangouts-conversation.txt)
> >>>>>>>> test [6347]
> >>>>>>> [1] "2016-10-21 10:56:37 <John Doe> Admit#8242"
> >>>>>>>
> >>>>>>> Perhaps where it was getting screwed up is, since the end of
> >>> this is
> >>>> a
> >>>>>>> number (8242), then, given that there's no space between the
> >>> number
> >>>>>>> and what ought to be the next row, R didn't know where to draw
> >>> the
> >>>>>>> line. Sure enough, it looks like this when I go to the original
> >>> file
> >>>>>>> and control f "#8242"
> >>>>>>>
> >>>>>>> 2016-10-21 10:35:36 <Jane Doe> What's your login
> >>>>>>> 2016-10-21 10:56:29 <John Doe> John_Doe
> >>>>>>> 2016-10-21 10:56:37 <John Doe> Admit#8242
> >>>>>>
> >>>>>>
> >>>>>> An octothorpe is an end of line signifier and is interpreted as
> >>>> allowing
> >>>>>> comments. You can prevent that interpretation with suitable
> >>> choice of
> >>>>>> parameters to `read.table` or `read.csv`. I don't understand why
> >>> that
> >>>>>> should cause anu error or a failure to match that pattern.
> >>>>>>
> >>>>>>> 2016-10-21 11:00:13 <Jane Doe> Okay so you have a discussion
> >>>>>>>
> >>>>>>> Again, it doesn't look like that in the file. Gmail
> >>> automatically
> >>>>>>> formats it like that when I paste it in. More to the point, it
> >>> looks
> >>>>>>> like
> >>>>>>>
> >>>>>>> 2016-10-21 10:35:36 <Jane Doe> What's your login2016-10-21
> >>> 10:56:29
> >>>>>>> <John Doe> John_Doe2016-10-21 10:56:37 <John Doe>
> >>>> Admit#82422016-10-21
> >>>>>>> 11:00:13 <Jane Doe> Okay so you have a discussion
> >>>>>>>
> >>>>>>> Notice Admit#82422016. So there's that.
> >>>>>>>
> >>>>>>> Then I built object test2.
> >>>>>>>
> >>>>>>> test2 <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4",
> >>> test)
> >>>>>>>
> >>>>>>> This worked for 84 lines, then this happened.
> >>>>>>
> >>>>>> It may have done something but as you later discovered my first
> >>> code
> >>>> for
> >>>>>> the pattern was incorrect. I had tested it (and pasted in the
> >>> results
> >>>> of
> >>>>>> the test) . The way to refer to a capture class is with
> >>> back-slashes
> >>>>>> before the numbers, not forward-slashes. Try this:
> >>>>>>
> >>>>>>
> >>>>>>> newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)",
> >>> "\\1,\\2,\\3,\\4",
> >>>> chrvec)
> >>>>>>> newvec
> >>>>>>  [1] "2016-07-01,02:50:35,<john>,hey"
> >>>>>>  [2] "2016-07-01,02:51:26,<jane>,waiting for plane to Edinburgh"
> >>>>>>  [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
> >>>>>>  [4] "2016-07-01,02:52:07,<jane>,nothing crappy has happened,
> >>> not
> >>>> really"
> >>>>>>  [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast,
> >>> didn't
> >>>> sleep"
> >>>>>>  [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or
> >>> where I am
> >>>>>> really"
> >>>>>>  [7] "2016-07-01,02:54:17,<john>,just know it's london"
> >>>>>>  [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
> >>>>>>  [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a good
> >>> eay"
> >>>>>> [10] "2016-07-01 02:58:56 <jone>"
> >>>>>> [11] "2016-07-01 02:59:34 <jane>"
> >>>>>> [12] "2016-07-01,03:02:48,<john>,British security is a little
> >>> more
> >>>>>> rigorous..."
> >>>>>>
> >>>>>>
> >>>>>> I made note of the fact that the 10th and 11th lines had no
> >>> commas.
> >>>>>>
> >>>>>>>
> >>>>>>>> test2 [84]
> >>>>>>> [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
> >>>>>>
> >>>>>> That line didn't have any "<" so wasn't matched.
> >>>>>>
> >>>>>>
> >>>>>> You could remove all none matching lines for pattern of
> >>>>>>
> >>>>>> dates<space>times<space>"<"<name>">"<space><anything>
> >>>>>>
> >>>>>>
> >>>>>> with:
> >>>>>>
> >>>>>>
> >>>>>> chrvec <- chrvec[ grepl("^.{10} .{8} <.+> .+$)", chrvec)]
> >>>>>>
> >>>>>>
> >>>>>> Do read:
> >>>>>>
> >>>>>> ?read.csv
> >>>>>>
> >>>>>> ?regex
> >>>>>>
> >>>>>>
> >>>>>> --
> >>>>>>
> >>>>>> David
> >>>>>>
> >>>>>>
> >>>>>>>> test2 [85]
> >>>>>>> [1] "//1,//2,//3,//4"
> >>>>>>>> test [85]
> >>>>>>> [1] "2016-07-01 02:50:35 <John Doe> hey"
> >>>>>>>
> >>>>>>> Notice how I toggled back and forth between test and test2
> >>> there. So,
> >>>>>>> whatever happened with the regex, it happened in the switch
> >>> from 84
> >>>> to
> >>>>>>> 85, I guess. It went on like
> >>>>>>>
> >>>>>>> [990] "//1,//2,//3,//4"
> >>>>>>>  [991] "//1,//2,//3,//4"
> >>>>>>>  [992] "//1,//2,//3,//4"
> >>>>>>>  [993] "//1,//2,//3,//4"
> >>>>>>>  [994] "//1,//2,//3,//4"
> >>>>>>>  [995] "//1,//2,//3,//4"
> >>>>>>>  [996] "//1,//2,//3,//4"
> >>>>>>>  [997] "//1,//2,//3,//4"
> >>>>>>>  [998] "//1,//2,//3,//4"
> >>>>>>>  [999] "//1,//2,//3,//4"
> >>>>>>> [1000] "//1,//2,//3,//4"
> >>>>>>>
> >>>>>>> up until line 1000, then I reached max.print.
> >>>>>>
> >>>>>>> Michael
> >>>>>>>
> >>>>>>> On Thu, May 16, 2019 at 1:05 PM David Winsemius <
> >>>> dwinsemius at comcast.net>
> >>>>>> wrote:
> >>>>>>>>
> >>>>>>>> On 5/16/19 12:30 PM, Michael Boulineau wrote:
> >>>>>>>>> Thanks for this tip on etiquette, David. I will be sure and
> >>> not do
> >>>>>> that again.
> >>>>>>>>>
> >>>>>>>>> I tried the read.fwf from the foreign package, with a code
> >>> like
> >>>> this:
> >>>>>>>>>
> >>>>>>>>>   d <- read.fwf("hangouts-conversation.txt",
> >>>>>>>>>                  widths= c(10,10,20,40),
> >>>>>>>>>
> >>> col.names=c("date","time","person","comment"),
> >>>>>>>>>                  strip.white=TRUE)
> >>>>>>>>>
> >>>>>>>>> But it threw this error:
> >>>>>>>>>
> >>>>>>>>> Error in scan(file = file, what = what, sep = sep, quote =
> >>> quote,
> >>>> dec
> >>>>>> = dec,  :
> >>>>>>>>>    line 6347 did not have 4 elements
> >>>>>>>>
> >>>>>>>> So what does line 6347 look like? (Use `readLines` and print
> >>> it
> >>>> out.)
> >>>>>>>>
> >>>>>>>>> Interestingly, though, the error only happened when I
> >>> increased the
> >>>>>>>>> width size. But I had to increase the size, or else I
> >>> couldn't
> >>>> "see"
> >>>>>>>>> anything.  The comment was so small that nothing was being
> >>>> captured by
> >>>>>>>>> the size of the column. so to speak.
> >>>>>>>>>
> >>>>>>>>> It seems like what's throwing me is that there's no comma
> >>> that
> >>>>>>>>> demarcates the end of the text proper. For example:
> >>>>>>>> Not sure why you thought there should be a comma. Lines
> >>> usually end
> >>>>>>>> with  <cr> and or a <lf>.
> >>>>>>>>
> >>>>>>>>
> >>>>>>>> Once you have the raw text in a character vector from
> >>> `readLines`
> >>>> named,
> >>>>>>>> say, 'chrvec', then you could selectively substitute commas
> >>> for
> >>>> spaces
> >>>>>>>> with regex. (Now that you no longer desire to remove the dates
> >>> and
> >>>>>> times.)
> >>>>>>>>
> >>>>>>>> sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", chrvec)
> >>>>>>>>
> >>>>>>>> This will not do any replacements when the pattern is not
> >>> matched.
> >>>> See
> >>>>>>>> this test:
> >>>>>>>>
> >>>>>>>>
> >>>>>>>>> newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)",
> >>> "\\1,\\2,\\3,\\4",
> >>>>>> chrvec)
> >>>>>>>>> newvec
> >>>>>>>>   [1] "2016-07-01,02:50:35,<john>,hey"
> >>>>>>>>   [2] "2016-07-01,02:51:26,<jane>,waiting for plane to
> >>> Edinburgh"
> >>>>>>>>   [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
> >>>>>>>>   [4] "2016-07-01,02:52:07,<jane>,nothing crappy has
> >>> happened, not
> >>>>>> really"
> >>>>>>>>   [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast,
> >>> didn't
> >>>>>> sleep"
> >>>>>>>>   [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or
> >>> where
> >>>> I am
> >>>>>>>> really"
> >>>>>>>>   [7] "2016-07-01,02:54:17,<john>,just know it's london"
> >>>>>>>>   [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
> >>>>>>>>   [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a
> >>> good
> >>>> eay"
> >>>>>>>> [10] "2016-07-01 02:58:56 <jone>"
> >>>>>>>> [11] "2016-07-01 02:59:34 <jane>"
> >>>>>>>> [12] "2016-07-01,03:02:48,<john>,British security is a little
> >>> more
> >>>>>>>> rigorous..."
> >>>>>>>>
> >>>>>>>>
> >>>>>>>> You should probably remove the "empty comment" lines.
> >>>>>>>>
> >>>>>>>>
> >>>>>>>> --
> >>>>>>>>
> >>>>>>>> David.
> >>>>>>>>
> >>>>>>>>> 2016-07-01 15:34:30 <John Doe> Lame. We were in a
> >>>> starbucks2016-07-01
> >>>>>>>>> 15:35:02 <Jane Doe> Hmm that's interesting2016-07-01 15:35:09
> >>> <Jane
> >>>>>>>>> Doe> You must want coffees2016-07-01 15:35:25 <John Doe>
> >>> There was
> >>>>>>>>> lots of Starbucks in my day2016-07-01 15:35:47
> >>>>>>>>>
> >>>>>>>>> It was interesting, too, when I pasted the text into the
> >>> email, it
> >>>>>>>>> self-formatted into the way I wanted it to look. I had to
> >>> manually
> >>>>>>>>> make it look like it does above, since that's the way that it
> >>>> looks in
> >>>>>>>>> the txt file. I wonder if it's being organized by XML or
> >>> something.
> >>>>>>>>>
> >>>>>>>>> Anyways, There's always a space between the two sideways
> >>> carrots,
> >>>> just
> >>>>>>>>> like there is right now: <John Doe> See. Space. And there's
> >>> always
> >>>> a
> >>>>>>>>> space between the data and time. Like this. 2016-07-01
> >>> 15:34:30
> >>>> See.
> >>>>>>>>> Space. But there's never a space between the end of the
> >>> comment and
> >>>>>>>>> the next date. Like this: We were in a starbucks2016-07-01
> >>> 15:35:02
> >>>>>>>>> See. starbucks and 2016 are smooshed together.
> >>>>>>>>>
> >>>>>>>>> This code is also on the table right now too.
> >>>>>>>>>
> >>>>>>>>> a <- read.table("E:/working
> >>>>>>>>> directory/-189/hangouts-conversation2.txt", quote="\"",
> >>>>>>>>> comment.char="", fill=TRUE)
> >>>>>>>>>
> >>>>>>>>>
> >>>>>>
> >>>>
> >>> h<-cbind(hangouts.conversation2[,1:2],hangouts.conversation2[,3:5],hangouts.conversation2[,6:9])
> >>>>>>>>>
> >>>>>>>>> aa<-gsub("[^[:digit:]]","",h)
> >>>>>>>>> my.data.num <- as.numeric(str_extract(h, "[0-9]+"))
> >>>>>>>>>
> >>>>>>>>> Those last lines are a work in progress. I wish I could
> >>> import a
> >>>>>>>>> picture of what it looks like when it's translated into a
> >>> data
> >>>> frame.
> >>>>>>>>> The fill=TRUE helped to get the data in table that kind of
> >>> sort of
> >>>>>>>>> works, but the comments keep bleeding into the data and time
> >>>> column.
> >>>>>>>>> It's like
> >>>>>>>>>
> >>>>>>>>> 2016-07-01 15:59:17 <Jane Doe> Seriously I've never been
> >>>>>>>>> over               there
> >>>>>>>>> 2016-07-01 15:59:27 <Jane Doe> It confuses me :(
> >>>>>>>>>
> >>>>>>>>> And then, maybe, the "seriously" will be in a column all to
> >>>> itself, as
> >>>>>>>>> will be the "I've'"and the "never" etc.
> >>>>>>>>>
> >>>>>>>>> I will use a regular expression if I have to, but it would be
> >>> nice
> >>>> to
> >>>>>>>>> keep the dates and times on there. Originally, I thought they
> >>> were
> >>>>>>>>> meaningless, but I've since changed my mind on that count.
> >>> The
> >>>> time of
> >>>>>>>>> day isn't so important. But, especially since, say, Gmail
> >>> itself
> >>>> knows
> >>>>>>>>> how to quickly recognize what it is, I know it can be done. I
> >>> know
> >>>>>>>>> this data has structure to it.
> >>>>>>>>>
> >>>>>>>>> Michael
> >>>>>>>>>
> >>>>>>>>>
> >>>>>>>>>
> >>>>>>>>> On Wed, May 15, 2019 at 8:47 PM David Winsemius <
> >>>>>> dwinsemius at comcast.net> wrote:
> >>>>>>>>>> On 5/15/19 4:07 PM, Michael Boulineau wrote:
> >>>>>>>>>>> I have a wild and crazy text file, the head of which looks
> >>> like
> >>>> this:
> >>>>>>>>>>>
> >>>>>>>>>>> 2016-07-01 02:50:35 <john> hey
> >>>>>>>>>>> 2016-07-01 02:51:26 <jane> waiting for plane to Edinburgh
> >>>>>>>>>>> 2016-07-01 02:51:45 <john> thinking about my boo
> >>>>>>>>>>> 2016-07-01 02:52:07 <jane> nothing crappy has happened, not
> >>>> really
> >>>>>>>>>>> 2016-07-01 02:52:20 <john> plane went by pretty fast,
> >>> didn't
> >>>> sleep
> >>>>>>>>>>> 2016-07-01 02:54:08 <jane> no idea what time it is or where
> >>> I am
> >>>>>> really
> >>>>>>>>>>> 2016-07-01 02:54:17 <john> just know it's london
> >>>>>>>>>>> 2016-07-01 02:56:44 <jane> you are probably asleep
> >>>>>>>>>>> 2016-07-01 02:58:45 <jane> I hope fish was fishy in a good
> >>> eay
> >>>>>>>>>>> 2016-07-01 02:58:56 <jone>
> >>>>>>>>>>> 2016-07-01 02:59:34 <jane>
> >>>>>>>>>>> 2016-07-01 03:02:48 <john> British security is a little
> >>> more
> >>>>>> rigorous...
> >>>>>>>>>> Looks entirely not-"crazy". Typical log file format.
> >>>>>>>>>>
> >>>>>>>>>> Two possibilities: 1) Use `read.fwf` from pkg foreign; 2)
> >>> Use
> >>>> regex
> >>>>>>>>>> (i.e. the sub-function) to strip everything up to the "<".
> >>> Read
> >>>>>>>>>> `?regex`. Since that's not a metacharacters you could use a
> >>>> pattern
> >>>>>>>>>> ".+<" and replace with "".
> >>>>>>>>>>
> >>>>>>>>>> And do read the Posting Guide. Cross-posting to
> >>> StackOverflow and
> >>>>>> Rhelp,
> >>>>>>>>>> at least within hours of each, is considered poor manners.
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> --
> >>>>>>>>>>
> >>>>>>>>>> David.
> >>>>>>>>>>
> >>>>>>>>>>> It goes on for a while. It's a big file. But I feel like
> >>> it's
> >>>> going
> >>>>>> to
> >>>>>>>>>>> be difficult to annotate with the coreNLP library or
> >>> package. I'm
> >>>>>>>>>>> doing natural language processing. In other words, I'm
> >>> curious
> >>>> as to
> >>>>>>>>>>> how I would shave off the dates, that is, to make it look
> >>> like:
> >>>>>>>>>>>
> >>>>>>>>>>> <john> hey
> >>>>>>>>>>> <jane> waiting for plane to Edinburgh
> >>>>>>>>>>>    <john> thinking about my boo
> >>>>>>>>>>> <jane> nothing crappy has happened, not really
> >>>>>>>>>>> <john> plane went by pretty fast, didn't sleep
> >>>>>>>>>>> <jane> no idea what time it is or where I am really
> >>>>>>>>>>> <john> just know it's london
> >>>>>>>>>>> <jane> you are probably asleep
> >>>>>>>>>>> <jane> I hope fish was fishy in a good eay
> >>>>>>>>>>>    <jone>
> >>>>>>>>>>> <jane>
> >>>>>>>>>>> <john> British security is a little more rigorous...
> >>>>>>>>>>>
> >>>>>>>>>>> To be clear, then, I'm trying to clean a large text file by
> >>>> writing a
> >>>>>>>>>>> regular expression? such that I create a new object with no
> >>>> numbers
> >>>>>> or
> >>>>>>>>>>> dates.
> >>>>>>>>>>>
> >>>>>>>>>>> Michael
> >>>>>>>>>>>
> >>>>>>>>>>> ______________________________________________
> >>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
> >>> more,
> >>>> see
> >>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>>>> PLEASE do read the posting guide
> >>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>>>> and provide commented, minimal, self-contained,
> >>> reproducible
> >>>> code.
> >>>>>>>>> ______________________________________________
> >>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >>> see
> >>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>> PLEASE do read the posting guide
> >>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>> and provide commented, minimal, self-contained, reproducible
> >>> code.
> >>>>>>>> ______________________________________________
> >>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >>> see
> >>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>> PLEASE do read the posting guide
> >>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>> and provide commented, minimal, self-contained, reproducible
> >>> code.
> >>>>>>> ______________________________________________
> >>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >>> see
> >>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>> PLEASE do read the posting guide
> >>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>> and provide commented, minimal, self-contained, reproducible
> >>> code.
> >>>>>>
> >>>>>> ______________________________________________
> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>> PLEASE do read the posting guide
> >>>>>> http://www.R-project.org/posting-guide.html
> >>>>>> and provide commented, minimal, self-contained, reproducible
> >>> code.
> >>>>>>
> >>>>>
> >>>>>        [[alternative HTML version deleted]]
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide
> >>>> http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>>> ______________________________________________
> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide
> >>>> http://www.R-project.org/posting-guide.html
> >>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>>
> >>>      [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >> --
> >> Sent from my phone. Please excuse my brevity.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>


From bor|@@@te|pe @end|ng |rom utoronto@c@  Sun May 19 01:30:32 2019
From: bor|@@@te|pe @end|ng |rom utoronto@c@ (Boris Steipe)
Date: Sat, 18 May 2019 23:30:32 +0000
Subject: [R] how to separate string from numbers in a large txt file
In-Reply-To: <CAH+cTGNaw7p-it4B7te4BBsay=v6J-5OS-P_WGDJJ6Z0ymxtvw@mail.gmail.com>
References: <CAH+cTGM9sFCxegaMHzu-_9Prhvvh9MbVDCWJ1_x7wkO9jC47mQ@mail.gmail.com>
 <c6e69644-66a1-7031-4d7c-1000e0abbea4@comcast.net>
 <CAH+cTGM5YvWV_G0pRgMqjCzuRrFTzd0nOc7yoThfGbGMUyMjsQ@mail.gmail.com>
 <9c9f1c81-309f-91cb-7e26-f02fc4bd45d8@comcast.net>
 <CAH+cTGMx4_Lz2gGDf2AuUaaZF2Xaq4McuPRoHu4JJNc-ORd=Ew@mail.gmail.com>
 <6322e735-5659-848e-3a50-9f066f1299ad@comcast.net>
 <CAF8bMcbgrVcN9J7Aygq2D5RWaU4MS-GH+_+zMHU=AO+DU5DVWw@mail.gmail.com>
 <CAH+cTGNfpLSurqmsoMeci0GWhC0ncOp7zTEju3fJW5T4+tCsGA@mail.gmail.com>
 <CAF8bMcaSJRaUXK4h2ssaDoJwXaKCUZT2Vq1cutNiNDjQ7ga2Xg@mail.gmail.com>
 <EEA9FD46-A67C-4FB7-8245-350819C4EA62@dcn.davis.ca.us>
 <CAH+cTGNOkAnPnn-aru-BJUbhrgeYMQBTqjSYT764nKzcdXvTvA@mail.gmail.com>
 <9B170703-AAAF-4D52-9744-1FDE70559B05@utoronto.ca>
 <CAH+cTGNaw7p-it4B7te4BBsay=v6J-5OS-P_WGDJJ6Z0ymxtvw@mail.gmail.com>
Message-ID: <0F8CD89C-BDE2-44D0-8E18-0CA2706EB53F@utoronto.ca>

This works for me:

# sample data
c <- character()
c[1] <- "2016-01-27 09:14:40 <Jane Doe> started a video chat"
c[2] <- "2016-01-27 09:15:20 <Jane Doe> https://lh3.googleusercontent.com/"
c[3] <- "2016-01-27 09:15:20 <Jane Doe> Hey "
c[4] <- "2016-01-27 09:15:22 <John Doe>  ended a video chat"
c[5] <- "2016-01-27 21:07:11 <Jane Doe>  started a video chat"
c[6] <- "2016-01-27 21:26:57 <John Doe>  ended a video chat"


# regex  ^(year)       (time)      <(word word)>\\s*(string)$
patt <- "^([0-9-]{10}) ([0-9:]{8}) <(\\w+ \\w+)>\\s*(.+)$" 
proto <- data.frame(date = character(),
                    time = character(),
                    name = character(),
                    text = character(),
                    stringsAsFactors = TRUE)
d <- strcapture(patt, c, proto)



        date     time     name                               text
1 2016-01-27 09:14:40 Jane Doe               started a video chat
2 2016-01-27 09:15:20 Jane Doe https://lh3.googleusercontent.com/
3 2016-01-27 09:15:20 Jane Doe                               Hey 
4 2016-01-27 09:15:22 John Doe                 ended a video chat
5 2016-01-27 21:07:11 Jane Doe               started a video chat
6 2016-01-27 21:26:57 John Doe                 ended a video chat



B.


> On 2019-05-18, at 18:32, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
> 
> Going back and thinking through what Boris and William were saying
> (also Ivan), I tried this:
> 
> a <- readLines ("hangouts-conversation-6.csv.txt")
> b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)"
> c <- gsub(b, "\\1<\\2> ", a)
>> head (c)
> [1] "???2016-01-27 09:14:40 *** Jane Doe started a video chat"
> [2] "2016-01-27 09:15:20 <Jane Doe>
> https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf"
> [3] "2016-01-27 09:15:20 <Jane Doe> Hey "
> [4] "2016-01-27 09:15:22 <John Doe>  ended a video chat"
> [5] "2016-01-27 21:07:11 <Jane Doe>  started a video chat"
> [6] "2016-01-27 21:26:57 <John Doe>  ended a video chat"
> 
> The ??? is still there, since I forgot to do what Ivan had suggested, namely,
> 
> a <- readLines(con <- file("hangouts-conversation-6.csv.txt", encoding
> = "UTF-8")); close(con); rm(con)
> 
> But then the new code is still turning out only NAs when I apply
> strcapture (). This was what happened next:
> 
>> d <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
> + [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
> +                 c, proto=data.frame(stringsAsFactors=FALSE, When="", Who="",
> +                                     What=""))
>> head (d)
>  When  Who What
> 1 <NA> <NA> <NA>
> 2 <NA> <NA> <NA>
> 3 <NA> <NA> <NA>
> 4 <NA> <NA> <NA>
> 5 <NA> <NA> <NA>
> 6 <NA> <NA> <NA>
> 
> I've been reading up on regular expressions, too, so this code seems
> spot on. What's going wrong?
> 
> Michael
> 
> On Fri, May 17, 2019 at 4:28 PM Boris Steipe <boris.steipe at utoronto.ca> wrote:
>> 
>> Don't start putting in extra commas and then reading this as csv. That approach is broken. The correct approach is what Bill outlined: read everything with readLines(), and then use a proper regular expression with strcapture().
>> 
>> You need to pre-process the object that readLines() gives you: replace the contents of the videochat lines, and make it conform to the format of the other lines before you process it into your data frame.
>> 
>> Approximately something like
>> 
>> # read the raw data
>> tmp <- readLines("hangouts-conversation-6.csv.txt")
>> 
>> # process all video chat lines
>> patt <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+) "  # (year time )*** (word word)
>> tmp <- gsub(patt, "\\1<\\2> ", tmp)
>> 
>> # next, use strcapture()
>> 
>> Note that this makes the assumption that your names are always exactly two words containing only letters. If that assumption is not true, more though needs to go into the regex. But you can test that:
>> 
>> patt <- " <\\w+ \\w+> "   #" <word word> "
>> sum( ! grepl(patt, tmp)))
>> 
>> ... will give the number of lines that remain in your file that do not have a tag that can be interpreted as "Who"
>> 
>> Once that is fine, use Bill's approach - or a regular expression of your own design - to create your data frame.
>> 
>> Hope this helps,
>> Boris
>> 
>> 
>> 
>> 
>>> On 2019-05-17, at 16:18, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
>>> 
>>> Very interesting. I'm sure I'll be trying to get rid of the byte order
>>> mark eventually. But right now, I'm more worried about getting the
>>> character vector into either a csv file or data.frame; that way, I can
>>> be able to work with the data neatly tabulated into four columns:
>>> date, time, person, comment. I assume it's a write.csv function, but I
>>> don't know what arguments to put in it. header=FALSE? fill=T?
>>> 
>>> Micheal
>>> 
>>> On Fri, May 17, 2019 at 1:03 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>>> 
>>>> If byte order mark is the issue then you can specify the file encoding as "UTF-8-BOM" and it won't show up in your data any more.
>>>> 
>>>> On May 17, 2019 12:12:17 PM PDT, William Dunlap via R-help <r-help at r-project.org> wrote:
>>>>> The pattern I gave worked for the lines that you originally showed from
>>>>> the
>>>>> data file ('a'), before you put commas into them.  If the name is
>>>>> either of
>>>>> the form "<name>" or "***" then the "(<[^>]*>)" needs to be changed so
>>>>> something like "(<[^>]*>|[*]{3})".
>>>>> 
>>>>> The " ???" at the start of the imported data may come from the byte
>>>>> order
>>>>> mark that Windows apps like to put at the front of a text file in UTF-8
>>>>> or
>>>>> UTF-16 format.
>>>>> 
>>>>> Bill Dunlap
>>>>> TIBCO Software
>>>>> wdunlap tibco.com
>>>>> 
>>>>> 
>>>>> On Fri, May 17, 2019 at 11:53 AM Michael Boulineau <
>>>>> michael.p.boulineau at gmail.com> wrote:
>>>>> 
>>>>>> This seemed to work:
>>>>>> 
>>>>>>> a <- readLines ("hangouts-conversation-6.csv.txt")
>>>>>>> b <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4", a)
>>>>>>> b [1:84]
>>>>>> 
>>>>>> And the first 85 lines looks like this:
>>>>>> 
>>>>>> [83] "2016-06-28 21:02:28 *** Jane Doe started a video chat"
>>>>>> [84] "2016-06-28 21:12:43 *** John Doe ended a video chat"
>>>>>> 
>>>>>> Then they transition to the commas:
>>>>>> 
>>>>>>> b [84:100]
>>>>>> [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
>>>>>> [2] "2016-07-01,02:50:35,<John Doe>,hey"
>>>>>> [3] "2016-07-01,02:51:26,<John Doe>,waiting for plane to Edinburgh"
>>>>>> [4] "2016-07-01,02:51:45,<John Doe>,thinking about my boo"
>>>>>> 
>>>>>> Even the strange bit on line 6347 was caught by this:
>>>>>> 
>>>>>>> b [6346:6348]
>>>>>> [1] "2016-10-21,10:56:29,<John Doe>,John_Doe"
>>>>>> [2] "2016-10-21,10:56:37,<John Doe>,Admit#8242"
>>>>>> [3] "2016-10-21,11:00:13,<Jane Doe>,Okay so you have a discussion"
>>>>>> 
>>>>>> Perhaps most awesomely, the code catches spaces that are interposed
>>>>>> into the comment itself:
>>>>>> 
>>>>>>> b [4]
>>>>>> [1] "2016-01-27,09:15:20,<Jane Doe>,Hey "
>>>>>>> b [85]
>>>>>> [1] "2016-07-01,02:50:35,<John Doe>,hey"
>>>>>> 
>>>>>> Notice whether there is a space after the "hey" or not.
>>>>>> 
>>>>>> These are the first two lines:
>>>>>> 
>>>>>> [1] "???2016-01-27 09:14:40 *** Jane Doe started a video chat"
>>>>>> [2] "2016-01-27,09:15:20,<Jane
>>>>>> Doe>,
>>>>>> 
>>>>> https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf
>>>>>> "
>>>>>> 
>>>>>> So, who knows what happened with the ??? at the beginning of [1]
>>>>>> directly above. But notice how there are no commas in [1] but there
>>>>>> appear in [2]. I don't see why really long ones like [2] directly
>>>>>> above would be a problem, were they to be translated into a csv or
>>>>>> data frame column.
>>>>>> 
>>>>>> Now, with the commas in there, couldn't we write this into a csv or a
>>>>>> data.frame? Some of this data will end up being garbage, I imagine.
>>>>>> Like in [2] directly above. Or with [83] and [84] at the top of this
>>>>>> discussion post/email. Embarrassingly, I've been trying to convert
>>>>>> this into a data.frame or csv but I can't manage to. I've been using
>>>>>> the write.csv function, but I don't think I've been getting the
>>>>>> arguments correct.
>>>>>> 
>>>>>> At the end of the day, I would like a data.frame and/or csv with the
>>>>>> following four columns: date, time, person, comment.
>>>>>> 
>>>>>> I tried this, too:
>>>>>> 
>>>>>>> c <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
>>>>>> + [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
>>>>>> +                 a, proto=data.frame(stringsAsFactors=FALSE,
>>>>> When="",
>>>>>> Who="",
>>>>>> +                                     What=""))
>>>>>> 
>>>>>> But all I got was this:
>>>>>> 
>>>>>>> c [1:100, ]
>>>>>>   When  Who What
>>>>>> 1   <NA> <NA> <NA>
>>>>>> 2   <NA> <NA> <NA>
>>>>>> 3   <NA> <NA> <NA>
>>>>>> 4   <NA> <NA> <NA>
>>>>>> 5   <NA> <NA> <NA>
>>>>>> 6   <NA> <NA> <NA>
>>>>>> 
>>>>>> It seems to have caught nothing.
>>>>>> 
>>>>>>> unique (c)
>>>>>> When  Who What
>>>>>> 1 <NA> <NA> <NA>
>>>>>> 
>>>>>> But I like that it converted into columns. That's a really great
>>>>>> format. With a little tweaking, it'd be a great code for this data
>>>>>> set.
>>>>>> 
>>>>>> Michael
>>>>>> 
>>>>>> On Fri, May 17, 2019 at 8:20 AM William Dunlap via R-help
>>>>>> <r-help at r-project.org> wrote:
>>>>>>> 
>>>>>>> Consider using readLines() and strcapture() for reading such a
>>>>> file.
>>>>>> E.g.,
>>>>>>> suppose readLines(files) produced a character vector like
>>>>>>> 
>>>>>>> x <- c("2016-10-21 10:35:36 <Jane Doe> What's your login",
>>>>>>>         "2016-10-21 10:56:29 <John Doe> John_Doe",
>>>>>>>         "2016-10-21 10:56:37 <John Doe> Admit#8242",
>>>>>>>         "October 23, 1819 12:34 <Jane Eyre> I am not an angel")
>>>>>>> 
>>>>>>> Then you can make a data.frame with columns When, Who, and What by
>>>>>>> supplying a pattern containing three parenthesized capture
>>>>> expressions:
>>>>>>>> z <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
>>>>>>> [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
>>>>>>>            x, proto=data.frame(stringsAsFactors=FALSE, When="",
>>>>> Who="",
>>>>>>> What=""))
>>>>>>>> str(z)
>>>>>>> 'data.frame':   4 obs. of  3 variables:
>>>>>>> $ When: chr  "2016-10-21 10:35:36" "2016-10-21 10:56:29"
>>>>> "2016-10-21
>>>>>>> 10:56:37" NA
>>>>>>> $ Who : chr  "<Jane Doe>" "<John Doe>" "<John Doe>" NA
>>>>>>> $ What: chr  "What's your login" "John_Doe" "Admit#8242" NA
>>>>>>> 
>>>>>>> Lines that don't match the pattern result in NA's - you might make
>>>>> a
>>>>>> second
>>>>>>> pass over the corresponding elements of x with a new pattern.
>>>>>>> 
>>>>>>> You can convert the When column from character to time with
>>>>> as.POSIXct().
>>>>>>> 
>>>>>>> Bill Dunlap
>>>>>>> TIBCO Software
>>>>>>> wdunlap tibco.com
>>>>>>> 
>>>>>>> 
>>>>>>> On Thu, May 16, 2019 at 8:30 PM David Winsemius
>>>>> <dwinsemius at comcast.net>
>>>>>>> wrote:
>>>>>>> 
>>>>>>>> 
>>>>>>>> On 5/16/19 3:53 PM, Michael Boulineau wrote:
>>>>>>>>> OK. So, I named the object test and then checked the 6347th
>>>>> item
>>>>>>>>> 
>>>>>>>>>> test <- readLines ("hangouts-conversation.txt)
>>>>>>>>>> test [6347]
>>>>>>>>> [1] "2016-10-21 10:56:37 <John Doe> Admit#8242"
>>>>>>>>> 
>>>>>>>>> Perhaps where it was getting screwed up is, since the end of
>>>>> this is
>>>>>> a
>>>>>>>>> number (8242), then, given that there's no space between the
>>>>> number
>>>>>>>>> and what ought to be the next row, R didn't know where to draw
>>>>> the
>>>>>>>>> line. Sure enough, it looks like this when I go to the original
>>>>> file
>>>>>>>>> and control f "#8242"
>>>>>>>>> 
>>>>>>>>> 2016-10-21 10:35:36 <Jane Doe> What's your login
>>>>>>>>> 2016-10-21 10:56:29 <John Doe> John_Doe
>>>>>>>>> 2016-10-21 10:56:37 <John Doe> Admit#8242
>>>>>>>> 
>>>>>>>> 
>>>>>>>> An octothorpe is an end of line signifier and is interpreted as
>>>>>> allowing
>>>>>>>> comments. You can prevent that interpretation with suitable
>>>>> choice of
>>>>>>>> parameters to `read.table` or `read.csv`. I don't understand why
>>>>> that
>>>>>>>> should cause anu error or a failure to match that pattern.
>>>>>>>> 
>>>>>>>>> 2016-10-21 11:00:13 <Jane Doe> Okay so you have a discussion
>>>>>>>>> 
>>>>>>>>> Again, it doesn't look like that in the file. Gmail
>>>>> automatically
>>>>>>>>> formats it like that when I paste it in. More to the point, it
>>>>> looks
>>>>>>>>> like
>>>>>>>>> 
>>>>>>>>> 2016-10-21 10:35:36 <Jane Doe> What's your login2016-10-21
>>>>> 10:56:29
>>>>>>>>> <John Doe> John_Doe2016-10-21 10:56:37 <John Doe>
>>>>>> Admit#82422016-10-21
>>>>>>>>> 11:00:13 <Jane Doe> Okay so you have a discussion
>>>>>>>>> 
>>>>>>>>> Notice Admit#82422016. So there's that.
>>>>>>>>> 
>>>>>>>>> Then I built object test2.
>>>>>>>>> 
>>>>>>>>> test2 <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4",
>>>>> test)
>>>>>>>>> 
>>>>>>>>> This worked for 84 lines, then this happened.
>>>>>>>> 
>>>>>>>> It may have done something but as you later discovered my first
>>>>> code
>>>>>> for
>>>>>>>> the pattern was incorrect. I had tested it (and pasted in the
>>>>> results
>>>>>> of
>>>>>>>> the test) . The way to refer to a capture class is with
>>>>> back-slashes
>>>>>>>> before the numbers, not forward-slashes. Try this:
>>>>>>>> 
>>>>>>>> 
>>>>>>>>> newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)",
>>>>> "\\1,\\2,\\3,\\4",
>>>>>> chrvec)
>>>>>>>>> newvec
>>>>>>>> [1] "2016-07-01,02:50:35,<john>,hey"
>>>>>>>> [2] "2016-07-01,02:51:26,<jane>,waiting for plane to Edinburgh"
>>>>>>>> [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
>>>>>>>> [4] "2016-07-01,02:52:07,<jane>,nothing crappy has happened,
>>>>> not
>>>>>> really"
>>>>>>>> [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast,
>>>>> didn't
>>>>>> sleep"
>>>>>>>> [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or
>>>>> where I am
>>>>>>>> really"
>>>>>>>> [7] "2016-07-01,02:54:17,<john>,just know it's london"
>>>>>>>> [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
>>>>>>>> [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a good
>>>>> eay"
>>>>>>>> [10] "2016-07-01 02:58:56 <jone>"
>>>>>>>> [11] "2016-07-01 02:59:34 <jane>"
>>>>>>>> [12] "2016-07-01,03:02:48,<john>,British security is a little
>>>>> more
>>>>>>>> rigorous..."
>>>>>>>> 
>>>>>>>> 
>>>>>>>> I made note of the fact that the 10th and 11th lines had no
>>>>> commas.
>>>>>>>> 
>>>>>>>>> 
>>>>>>>>>> test2 [84]
>>>>>>>>> [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
>>>>>>>> 
>>>>>>>> That line didn't have any "<" so wasn't matched.
>>>>>>>> 
>>>>>>>> 
>>>>>>>> You could remove all none matching lines for pattern of
>>>>>>>> 
>>>>>>>> dates<space>times<space>"<"<name>">"<space><anything>
>>>>>>>> 
>>>>>>>> 
>>>>>>>> with:
>>>>>>>> 
>>>>>>>> 
>>>>>>>> chrvec <- chrvec[ grepl("^.{10} .{8} <.+> .+$)", chrvec)]
>>>>>>>> 
>>>>>>>> 
>>>>>>>> Do read:
>>>>>>>> 
>>>>>>>> ?read.csv
>>>>>>>> 
>>>>>>>> ?regex
>>>>>>>> 
>>>>>>>> 
>>>>>>>> --
>>>>>>>> 
>>>>>>>> David
>>>>>>>> 
>>>>>>>> 
>>>>>>>>>> test2 [85]
>>>>>>>>> [1] "//1,//2,//3,//4"
>>>>>>>>>> test [85]
>>>>>>>>> [1] "2016-07-01 02:50:35 <John Doe> hey"
>>>>>>>>> 
>>>>>>>>> Notice how I toggled back and forth between test and test2
>>>>> there. So,
>>>>>>>>> whatever happened with the regex, it happened in the switch
>>>>> from 84
>>>>>> to
>>>>>>>>> 85, I guess. It went on like
>>>>>>>>> 
>>>>>>>>> [990] "//1,//2,//3,//4"
>>>>>>>>> [991] "//1,//2,//3,//4"
>>>>>>>>> [992] "//1,//2,//3,//4"
>>>>>>>>> [993] "//1,//2,//3,//4"
>>>>>>>>> [994] "//1,//2,//3,//4"
>>>>>>>>> [995] "//1,//2,//3,//4"
>>>>>>>>> [996] "//1,//2,//3,//4"
>>>>>>>>> [997] "//1,//2,//3,//4"
>>>>>>>>> [998] "//1,//2,//3,//4"
>>>>>>>>> [999] "//1,//2,//3,//4"
>>>>>>>>> [1000] "//1,//2,//3,//4"
>>>>>>>>> 
>>>>>>>>> up until line 1000, then I reached max.print.
>>>>>>>> 
>>>>>>>>> Michael
>>>>>>>>> 
>>>>>>>>> On Thu, May 16, 2019 at 1:05 PM David Winsemius <
>>>>>> dwinsemius at comcast.net>
>>>>>>>> wrote:
>>>>>>>>>> 
>>>>>>>>>> On 5/16/19 12:30 PM, Michael Boulineau wrote:
>>>>>>>>>>> Thanks for this tip on etiquette, David. I will be sure and
>>>>> not do
>>>>>>>> that again.
>>>>>>>>>>> 
>>>>>>>>>>> I tried the read.fwf from the foreign package, with a code
>>>>> like
>>>>>> this:
>>>>>>>>>>> 
>>>>>>>>>>>  d <- read.fwf("hangouts-conversation.txt",
>>>>>>>>>>>                 widths= c(10,10,20,40),
>>>>>>>>>>> 
>>>>> col.names=c("date","time","person","comment"),
>>>>>>>>>>>                 strip.white=TRUE)
>>>>>>>>>>> 
>>>>>>>>>>> But it threw this error:
>>>>>>>>>>> 
>>>>>>>>>>> Error in scan(file = file, what = what, sep = sep, quote =
>>>>> quote,
>>>>>> dec
>>>>>>>> = dec,  :
>>>>>>>>>>>   line 6347 did not have 4 elements
>>>>>>>>>> 
>>>>>>>>>> So what does line 6347 look like? (Use `readLines` and print
>>>>> it
>>>>>> out.)
>>>>>>>>>> 
>>>>>>>>>>> Interestingly, though, the error only happened when I
>>>>> increased the
>>>>>>>>>>> width size. But I had to increase the size, or else I
>>>>> couldn't
>>>>>> "see"
>>>>>>>>>>> anything.  The comment was so small that nothing was being
>>>>>> captured by
>>>>>>>>>>> the size of the column. so to speak.
>>>>>>>>>>> 
>>>>>>>>>>> It seems like what's throwing me is that there's no comma
>>>>> that
>>>>>>>>>>> demarcates the end of the text proper. For example:
>>>>>>>>>> Not sure why you thought there should be a comma. Lines
>>>>> usually end
>>>>>>>>>> with  <cr> and or a <lf>.
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> Once you have the raw text in a character vector from
>>>>> `readLines`
>>>>>> named,
>>>>>>>>>> say, 'chrvec', then you could selectively substitute commas
>>>>> for
>>>>>> spaces
>>>>>>>>>> with regex. (Now that you no longer desire to remove the dates
>>>>> and
>>>>>>>> times.)
>>>>>>>>>> 
>>>>>>>>>> sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", chrvec)
>>>>>>>>>> 
>>>>>>>>>> This will not do any replacements when the pattern is not
>>>>> matched.
>>>>>> See
>>>>>>>>>> this test:
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>>> newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)",
>>>>> "\\1,\\2,\\3,\\4",
>>>>>>>> chrvec)
>>>>>>>>>>> newvec
>>>>>>>>>>  [1] "2016-07-01,02:50:35,<john>,hey"
>>>>>>>>>>  [2] "2016-07-01,02:51:26,<jane>,waiting for plane to
>>>>> Edinburgh"
>>>>>>>>>>  [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
>>>>>>>>>>  [4] "2016-07-01,02:52:07,<jane>,nothing crappy has
>>>>> happened, not
>>>>>>>> really"
>>>>>>>>>>  [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast,
>>>>> didn't
>>>>>>>> sleep"
>>>>>>>>>>  [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or
>>>>> where
>>>>>> I am
>>>>>>>>>> really"
>>>>>>>>>>  [7] "2016-07-01,02:54:17,<john>,just know it's london"
>>>>>>>>>>  [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
>>>>>>>>>>  [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a
>>>>> good
>>>>>> eay"
>>>>>>>>>> [10] "2016-07-01 02:58:56 <jone>"
>>>>>>>>>> [11] "2016-07-01 02:59:34 <jane>"
>>>>>>>>>> [12] "2016-07-01,03:02:48,<john>,British security is a little
>>>>> more
>>>>>>>>>> rigorous..."
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> You should probably remove the "empty comment" lines.
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> --
>>>>>>>>>> 
>>>>>>>>>> David.
>>>>>>>>>> 
>>>>>>>>>>> 2016-07-01 15:34:30 <John Doe> Lame. We were in a
>>>>>> starbucks2016-07-01
>>>>>>>>>>> 15:35:02 <Jane Doe> Hmm that's interesting2016-07-01 15:35:09
>>>>> <Jane
>>>>>>>>>>> Doe> You must want coffees2016-07-01 15:35:25 <John Doe>
>>>>> There was
>>>>>>>>>>> lots of Starbucks in my day2016-07-01 15:35:47
>>>>>>>>>>> 
>>>>>>>>>>> It was interesting, too, when I pasted the text into the
>>>>> email, it
>>>>>>>>>>> self-formatted into the way I wanted it to look. I had to
>>>>> manually
>>>>>>>>>>> make it look like it does above, since that's the way that it
>>>>>> looks in
>>>>>>>>>>> the txt file. I wonder if it's being organized by XML or
>>>>> something.
>>>>>>>>>>> 
>>>>>>>>>>> Anyways, There's always a space between the two sideways
>>>>> carrots,
>>>>>> just
>>>>>>>>>>> like there is right now: <John Doe> See. Space. And there's
>>>>> always
>>>>>> a
>>>>>>>>>>> space between the data and time. Like this. 2016-07-01
>>>>> 15:34:30
>>>>>> See.
>>>>>>>>>>> Space. But there's never a space between the end of the
>>>>> comment and
>>>>>>>>>>> the next date. Like this: We were in a starbucks2016-07-01
>>>>> 15:35:02
>>>>>>>>>>> See. starbucks and 2016 are smooshed together.
>>>>>>>>>>> 
>>>>>>>>>>> This code is also on the table right now too.
>>>>>>>>>>> 
>>>>>>>>>>> a <- read.table("E:/working
>>>>>>>>>>> directory/-189/hangouts-conversation2.txt", quote="\"",
>>>>>>>>>>> comment.char="", fill=TRUE)
>>>>>>>>>>> 
>>>>>>>>>>> 
>>>>>>>> 
>>>>>> 
>>>>> h<-cbind(hangouts.conversation2[,1:2],hangouts.conversation2[,3:5],hangouts.conversation2[,6:9])
>>>>>>>>>>> 
>>>>>>>>>>> aa<-gsub("[^[:digit:]]","",h)
>>>>>>>>>>> my.data.num <- as.numeric(str_extract(h, "[0-9]+"))
>>>>>>>>>>> 
>>>>>>>>>>> Those last lines are a work in progress. I wish I could
>>>>> import a
>>>>>>>>>>> picture of what it looks like when it's translated into a
>>>>> data
>>>>>> frame.
>>>>>>>>>>> The fill=TRUE helped to get the data in table that kind of
>>>>> sort of
>>>>>>>>>>> works, but the comments keep bleeding into the data and time
>>>>>> column.
>>>>>>>>>>> It's like
>>>>>>>>>>> 
>>>>>>>>>>> 2016-07-01 15:59:17 <Jane Doe> Seriously I've never been
>>>>>>>>>>> over               there
>>>>>>>>>>> 2016-07-01 15:59:27 <Jane Doe> It confuses me :(
>>>>>>>>>>> 
>>>>>>>>>>> And then, maybe, the "seriously" will be in a column all to
>>>>>> itself, as
>>>>>>>>>>> will be the "I've'"and the "never" etc.
>>>>>>>>>>> 
>>>>>>>>>>> I will use a regular expression if I have to, but it would be
>>>>> nice
>>>>>> to
>>>>>>>>>>> keep the dates and times on there. Originally, I thought they
>>>>> were
>>>>>>>>>>> meaningless, but I've since changed my mind on that count.
>>>>> The
>>>>>> time of
>>>>>>>>>>> day isn't so important. But, especially since, say, Gmail
>>>>> itself
>>>>>> knows
>>>>>>>>>>> how to quickly recognize what it is, I know it can be done. I
>>>>> know
>>>>>>>>>>> this data has structure to it.
>>>>>>>>>>> 
>>>>>>>>>>> Michael
>>>>>>>>>>> 
>>>>>>>>>>> 
>>>>>>>>>>> 
>>>>>>>>>>> On Wed, May 15, 2019 at 8:47 PM David Winsemius <
>>>>>>>> dwinsemius at comcast.net> wrote:
>>>>>>>>>>>> On 5/15/19 4:07 PM, Michael Boulineau wrote:
>>>>>>>>>>>>> I have a wild and crazy text file, the head of which looks
>>>>> like
>>>>>> this:
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 2016-07-01 02:50:35 <john> hey
>>>>>>>>>>>>> 2016-07-01 02:51:26 <jane> waiting for plane to Edinburgh
>>>>>>>>>>>>> 2016-07-01 02:51:45 <john> thinking about my boo
>>>>>>>>>>>>> 2016-07-01 02:52:07 <jane> nothing crappy has happened, not
>>>>>> really
>>>>>>>>>>>>> 2016-07-01 02:52:20 <john> plane went by pretty fast,
>>>>> didn't
>>>>>> sleep
>>>>>>>>>>>>> 2016-07-01 02:54:08 <jane> no idea what time it is or where
>>>>> I am
>>>>>>>> really
>>>>>>>>>>>>> 2016-07-01 02:54:17 <john> just know it's london
>>>>>>>>>>>>> 2016-07-01 02:56:44 <jane> you are probably asleep
>>>>>>>>>>>>> 2016-07-01 02:58:45 <jane> I hope fish was fishy in a good
>>>>> eay
>>>>>>>>>>>>> 2016-07-01 02:58:56 <jone>
>>>>>>>>>>>>> 2016-07-01 02:59:34 <jane>
>>>>>>>>>>>>> 2016-07-01 03:02:48 <john> British security is a little
>>>>> more
>>>>>>>> rigorous...
>>>>>>>>>>>> Looks entirely not-"crazy". Typical log file format.
>>>>>>>>>>>> 
>>>>>>>>>>>> Two possibilities: 1) Use `read.fwf` from pkg foreign; 2)
>>>>> Use
>>>>>> regex
>>>>>>>>>>>> (i.e. the sub-function) to strip everything up to the "<".
>>>>> Read
>>>>>>>>>>>> `?regex`. Since that's not a metacharacters you could use a
>>>>>> pattern
>>>>>>>>>>>> ".+<" and replace with "".
>>>>>>>>>>>> 
>>>>>>>>>>>> And do read the Posting Guide. Cross-posting to
>>>>> StackOverflow and
>>>>>>>> Rhelp,
>>>>>>>>>>>> at least within hours of each, is considered poor manners.
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> --
>>>>>>>>>>>> 
>>>>>>>>>>>> David.
>>>>>>>>>>>> 
>>>>>>>>>>>>> It goes on for a while. It's a big file. But I feel like
>>>>> it's
>>>>>> going
>>>>>>>> to
>>>>>>>>>>>>> be difficult to annotate with the coreNLP library or
>>>>> package. I'm
>>>>>>>>>>>>> doing natural language processing. In other words, I'm
>>>>> curious
>>>>>> as to
>>>>>>>>>>>>> how I would shave off the dates, that is, to make it look
>>>>> like:
>>>>>>>>>>>>> 
>>>>>>>>>>>>> <john> hey
>>>>>>>>>>>>> <jane> waiting for plane to Edinburgh
>>>>>>>>>>>>>   <john> thinking about my boo
>>>>>>>>>>>>> <jane> nothing crappy has happened, not really
>>>>>>>>>>>>> <john> plane went by pretty fast, didn't sleep
>>>>>>>>>>>>> <jane> no idea what time it is or where I am really
>>>>>>>>>>>>> <john> just know it's london
>>>>>>>>>>>>> <jane> you are probably asleep
>>>>>>>>>>>>> <jane> I hope fish was fishy in a good eay
>>>>>>>>>>>>>   <jone>
>>>>>>>>>>>>> <jane>
>>>>>>>>>>>>> <john> British security is a little more rigorous...
>>>>>>>>>>>>> 
>>>>>>>>>>>>> To be clear, then, I'm trying to clean a large text file by
>>>>>> writing a
>>>>>>>>>>>>> regular expression? such that I create a new object with no
>>>>>> numbers
>>>>>>>> or
>>>>>>>>>>>>> dates.
>>>>>>>>>>>>> 
>>>>>>>>>>>>> Michael
>>>>>>>>>>>>> 
>>>>>>>>>>>>> ______________________________________________
>>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
>>>>> more,
>>>>>> see
>>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>>>>> and provide commented, minimal, self-contained,
>>>>> reproducible
>>>>>> code.
>>>>>>>>>>> ______________________________________________
>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>>>> see
>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>> code.
>>>>>>>>>> ______________________________________________
>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>>>> see
>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>> code.
>>>>>>>>> ______________________________________________
>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>>>> see
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>> PLEASE do read the posting guide
>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>> code.
>>>>>>>> 
>>>>>>>> ______________________________________________
>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>> PLEASE do read the posting guide
>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>> code.
>>>>>>>> 
>>>>>>> 
>>>>>>>       [[alternative HTML version deleted]]
>>>>>>> 
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>> 
>>>>> 
>>>>>     [[alternative HTML version deleted]]
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> --
>>>> Sent from my phone. Please excuse my brevity.
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m|ch@e|@p@bou||ne@u @end|ng |rom gm@||@com  Sun May 19 02:34:49 2019
From: m|ch@e|@p@bou||ne@u @end|ng |rom gm@||@com (Michael Boulineau)
Date: Sat, 18 May 2019 17:34:49 -0700
Subject: [R] how to separate string from numbers in a large txt file
In-Reply-To: <0F8CD89C-BDE2-44D0-8E18-0CA2706EB53F@utoronto.ca>
References: <CAH+cTGM9sFCxegaMHzu-_9Prhvvh9MbVDCWJ1_x7wkO9jC47mQ@mail.gmail.com>
 <c6e69644-66a1-7031-4d7c-1000e0abbea4@comcast.net>
 <CAH+cTGM5YvWV_G0pRgMqjCzuRrFTzd0nOc7yoThfGbGMUyMjsQ@mail.gmail.com>
 <9c9f1c81-309f-91cb-7e26-f02fc4bd45d8@comcast.net>
 <CAH+cTGMx4_Lz2gGDf2AuUaaZF2Xaq4McuPRoHu4JJNc-ORd=Ew@mail.gmail.com>
 <6322e735-5659-848e-3a50-9f066f1299ad@comcast.net>
 <CAF8bMcbgrVcN9J7Aygq2D5RWaU4MS-GH+_+zMHU=AO+DU5DVWw@mail.gmail.com>
 <CAH+cTGNfpLSurqmsoMeci0GWhC0ncOp7zTEju3fJW5T4+tCsGA@mail.gmail.com>
 <CAF8bMcaSJRaUXK4h2ssaDoJwXaKCUZT2Vq1cutNiNDjQ7ga2Xg@mail.gmail.com>
 <EEA9FD46-A67C-4FB7-8245-350819C4EA62@dcn.davis.ca.us>
 <CAH+cTGNOkAnPnn-aru-BJUbhrgeYMQBTqjSYT764nKzcdXvTvA@mail.gmail.com>
 <9B170703-AAAF-4D52-9744-1FDE70559B05@utoronto.ca>
 <CAH+cTGNaw7p-it4B7te4BBsay=v6J-5OS-P_WGDJJ6Z0ymxtvw@mail.gmail.com>
 <0F8CD89C-BDE2-44D0-8E18-0CA2706EB53F@utoronto.ca>
Message-ID: <CAH+cTGMsD+PkTKoFfB8=3WeWPQAWjhmuOCKZfnB7jR03m7RtDQ@mail.gmail.com>

It appears to have worked, although there were three little quirks.
The ; close(con); rm(con) didn't work for me; the first row of the
data.frame was all NAs, when all was said and done; and then there
were still three *** on the same line where the ??? was apparently
deleted.

> a <- readLines ("hangouts-conversation-6.txt", encoding = "UTF-8")
> b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)"
> c <- gsub(b, "\\1<\\2> ", a)
> head (c)
[1] "?2016-01-27 09:14:40 *** Jane Doe started a video chat"
[2] "2016-01-27 09:15:20 <Jane Doe>
https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf"
[3] "2016-01-27 09:15:20 <Jane Doe> Hey "
[4] "2016-01-27 09:15:22 <John Doe>  ended a video chat"
[5] "2016-01-27 21:07:11 <Jane Doe>  started a video chat"
[6] "2016-01-27 21:26:57 <John Doe>  ended a video chat"

You can see those three *** there. But no ???

> d <- "^([0-9-]{10}) ([0-9:]{8}) <(\\w+ \\w+)>\\s*(.+)$"
> e <- data.frame(date = character(),
+                     time = character(),
+                     name = character(),
+                     text = character(),
+                     stringsAsFactors = TRUE)
f <- strcapture(d, c, e)
> f <- f [-c(1),]

When I look at the data.frame, it looks like [1] above, the one with
the three ***, was deleted--surely because of the second regex (the
one where I made object d above).

Thanks for your help everyone. I really learned a lot. The first thing
I'm going to do is continue to study regular expressions, although I
do have a much better sense of them than when I started.

But, before I do anything else, I'm going to study the regex in this
particular code. For example, I'm still not sure why there has to the
second \\w+ in the (\\w+ \\w+). Little things like that.

Michael


On Sat, May 18, 2019 at 4:30 PM Boris Steipe <boris.steipe at utoronto.ca> wrote:
>
> This works for me:
>
> # sample data
> c <- character()
> c[1] <- "2016-01-27 09:14:40 <Jane Doe> started a video chat"
> c[2] <- "2016-01-27 09:15:20 <Jane Doe> https://lh3.googleusercontent.com/"
> c[3] <- "2016-01-27 09:15:20 <Jane Doe> Hey "
> c[4] <- "2016-01-27 09:15:22 <John Doe>  ended a video chat"
> c[5] <- "2016-01-27 21:07:11 <Jane Doe>  started a video chat"
> c[6] <- "2016-01-27 21:26:57 <John Doe>  ended a video chat"
>
>
> # regex  ^(year)       (time)      <(word word)>\\s*(string)$
> patt <- "^([0-9-]{10}) ([0-9:]{8}) <(\\w+ \\w+)>\\s*(.+)$"
> proto <- data.frame(date = character(),
>                     time = character(),
>                     name = character(),
>                     text = character(),
>                     stringsAsFactors = TRUE)
> d <- strcapture(patt, c, proto)
>
>
>
>         date     time     name                               text
> 1 2016-01-27 09:14:40 Jane Doe               started a video chat
> 2 2016-01-27 09:15:20 Jane Doe https://lh3.googleusercontent.com/
> 3 2016-01-27 09:15:20 Jane Doe                               Hey
> 4 2016-01-27 09:15:22 John Doe                 ended a video chat
> 5 2016-01-27 21:07:11 Jane Doe               started a video chat
> 6 2016-01-27 21:26:57 John Doe                 ended a video chat
>
>
>
> B.
>
>
> > On 2019-05-18, at 18:32, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
> >
> > Going back and thinking through what Boris and William were saying
> > (also Ivan), I tried this:
> >
> > a <- readLines ("hangouts-conversation-6.csv.txt")
> > b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)"
> > c <- gsub(b, "\\1<\\2> ", a)
> >> head (c)
> > [1] "???2016-01-27 09:14:40 *** Jane Doe started a video chat"
> > [2] "2016-01-27 09:15:20 <Jane Doe>
> > https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf"
> > [3] "2016-01-27 09:15:20 <Jane Doe> Hey "
> > [4] "2016-01-27 09:15:22 <John Doe>  ended a video chat"
> > [5] "2016-01-27 21:07:11 <Jane Doe>  started a video chat"
> > [6] "2016-01-27 21:26:57 <John Doe>  ended a video chat"
> >
> > The ??? is still there, since I forgot to do what Ivan had suggested, namely,
> >
> > a <- readLines(con <- file("hangouts-conversation-6.csv.txt", encoding
> > = "UTF-8")); close(con); rm(con)
> >
> > But then the new code is still turning out only NAs when I apply
> > strcapture (). This was what happened next:
> >
> >> d <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
> > + [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
> > +                 c, proto=data.frame(stringsAsFactors=FALSE, When="", Who="",
> > +                                     What=""))
> >> head (d)
> >  When  Who What
> > 1 <NA> <NA> <NA>
> > 2 <NA> <NA> <NA>
> > 3 <NA> <NA> <NA>
> > 4 <NA> <NA> <NA>
> > 5 <NA> <NA> <NA>
> > 6 <NA> <NA> <NA>
> >
> > I've been reading up on regular expressions, too, so this code seems
> > spot on. What's going wrong?
> >
> > Michael
> >
> > On Fri, May 17, 2019 at 4:28 PM Boris Steipe <boris.steipe at utoronto.ca> wrote:
> >>
> >> Don't start putting in extra commas and then reading this as csv. That approach is broken. The correct approach is what Bill outlined: read everything with readLines(), and then use a proper regular expression with strcapture().
> >>
> >> You need to pre-process the object that readLines() gives you: replace the contents of the videochat lines, and make it conform to the format of the other lines before you process it into your data frame.
> >>
> >> Approximately something like
> >>
> >> # read the raw data
> >> tmp <- readLines("hangouts-conversation-6.csv.txt")
> >>
> >> # process all video chat lines
> >> patt <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+) "  # (year time )*** (word word)
> >> tmp <- gsub(patt, "\\1<\\2> ", tmp)
> >>
> >> # next, use strcapture()
> >>
> >> Note that this makes the assumption that your names are always exactly two words containing only letters. If that assumption is not true, more though needs to go into the regex. But you can test that:
> >>
> >> patt <- " <\\w+ \\w+> "   #" <word word> "
> >> sum( ! grepl(patt, tmp)))
> >>
> >> ... will give the number of lines that remain in your file that do not have a tag that can be interpreted as "Who"
> >>
> >> Once that is fine, use Bill's approach - or a regular expression of your own design - to create your data frame.
> >>
> >> Hope this helps,
> >> Boris
> >>
> >>
> >>
> >>
> >>> On 2019-05-17, at 16:18, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
> >>>
> >>> Very interesting. I'm sure I'll be trying to get rid of the byte order
> >>> mark eventually. But right now, I'm more worried about getting the
> >>> character vector into either a csv file or data.frame; that way, I can
> >>> be able to work with the data neatly tabulated into four columns:
> >>> date, time, person, comment. I assume it's a write.csv function, but I
> >>> don't know what arguments to put in it. header=FALSE? fill=T?
> >>>
> >>> Micheal
> >>>
> >>> On Fri, May 17, 2019 at 1:03 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> >>>>
> >>>> If byte order mark is the issue then you can specify the file encoding as "UTF-8-BOM" and it won't show up in your data any more.
> >>>>
> >>>> On May 17, 2019 12:12:17 PM PDT, William Dunlap via R-help <r-help at r-project.org> wrote:
> >>>>> The pattern I gave worked for the lines that you originally showed from
> >>>>> the
> >>>>> data file ('a'), before you put commas into them.  If the name is
> >>>>> either of
> >>>>> the form "<name>" or "***" then the "(<[^>]*>)" needs to be changed so
> >>>>> something like "(<[^>]*>|[*]{3})".
> >>>>>
> >>>>> The " ???" at the start of the imported data may come from the byte
> >>>>> order
> >>>>> mark that Windows apps like to put at the front of a text file in UTF-8
> >>>>> or
> >>>>> UTF-16 format.
> >>>>>
> >>>>> Bill Dunlap
> >>>>> TIBCO Software
> >>>>> wdunlap tibco.com
> >>>>>
> >>>>>
> >>>>> On Fri, May 17, 2019 at 11:53 AM Michael Boulineau <
> >>>>> michael.p.boulineau at gmail.com> wrote:
> >>>>>
> >>>>>> This seemed to work:
> >>>>>>
> >>>>>>> a <- readLines ("hangouts-conversation-6.csv.txt")
> >>>>>>> b <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4", a)
> >>>>>>> b [1:84]
> >>>>>>
> >>>>>> And the first 85 lines looks like this:
> >>>>>>
> >>>>>> [83] "2016-06-28 21:02:28 *** Jane Doe started a video chat"
> >>>>>> [84] "2016-06-28 21:12:43 *** John Doe ended a video chat"
> >>>>>>
> >>>>>> Then they transition to the commas:
> >>>>>>
> >>>>>>> b [84:100]
> >>>>>> [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
> >>>>>> [2] "2016-07-01,02:50:35,<John Doe>,hey"
> >>>>>> [3] "2016-07-01,02:51:26,<John Doe>,waiting for plane to Edinburgh"
> >>>>>> [4] "2016-07-01,02:51:45,<John Doe>,thinking about my boo"
> >>>>>>
> >>>>>> Even the strange bit on line 6347 was caught by this:
> >>>>>>
> >>>>>>> b [6346:6348]
> >>>>>> [1] "2016-10-21,10:56:29,<John Doe>,John_Doe"
> >>>>>> [2] "2016-10-21,10:56:37,<John Doe>,Admit#8242"
> >>>>>> [3] "2016-10-21,11:00:13,<Jane Doe>,Okay so you have a discussion"
> >>>>>>
> >>>>>> Perhaps most awesomely, the code catches spaces that are interposed
> >>>>>> into the comment itself:
> >>>>>>
> >>>>>>> b [4]
> >>>>>> [1] "2016-01-27,09:15:20,<Jane Doe>,Hey "
> >>>>>>> b [85]
> >>>>>> [1] "2016-07-01,02:50:35,<John Doe>,hey"
> >>>>>>
> >>>>>> Notice whether there is a space after the "hey" or not.
> >>>>>>
> >>>>>> These are the first two lines:
> >>>>>>
> >>>>>> [1] "???2016-01-27 09:14:40 *** Jane Doe started a video chat"
> >>>>>> [2] "2016-01-27,09:15:20,<Jane
> >>>>>> Doe>,
> >>>>>>
> >>>>> https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf
> >>>>>> "
> >>>>>>
> >>>>>> So, who knows what happened with the ??? at the beginning of [1]
> >>>>>> directly above. But notice how there are no commas in [1] but there
> >>>>>> appear in [2]. I don't see why really long ones like [2] directly
> >>>>>> above would be a problem, were they to be translated into a csv or
> >>>>>> data frame column.
> >>>>>>
> >>>>>> Now, with the commas in there, couldn't we write this into a csv or a
> >>>>>> data.frame? Some of this data will end up being garbage, I imagine.
> >>>>>> Like in [2] directly above. Or with [83] and [84] at the top of this
> >>>>>> discussion post/email. Embarrassingly, I've been trying to convert
> >>>>>> this into a data.frame or csv but I can't manage to. I've been using
> >>>>>> the write.csv function, but I don't think I've been getting the
> >>>>>> arguments correct.
> >>>>>>
> >>>>>> At the end of the day, I would like a data.frame and/or csv with the
> >>>>>> following four columns: date, time, person, comment.
> >>>>>>
> >>>>>> I tried this, too:
> >>>>>>
> >>>>>>> c <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
> >>>>>> + [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
> >>>>>> +                 a, proto=data.frame(stringsAsFactors=FALSE,
> >>>>> When="",
> >>>>>> Who="",
> >>>>>> +                                     What=""))
> >>>>>>
> >>>>>> But all I got was this:
> >>>>>>
> >>>>>>> c [1:100, ]
> >>>>>>   When  Who What
> >>>>>> 1   <NA> <NA> <NA>
> >>>>>> 2   <NA> <NA> <NA>
> >>>>>> 3   <NA> <NA> <NA>
> >>>>>> 4   <NA> <NA> <NA>
> >>>>>> 5   <NA> <NA> <NA>
> >>>>>> 6   <NA> <NA> <NA>
> >>>>>>
> >>>>>> It seems to have caught nothing.
> >>>>>>
> >>>>>>> unique (c)
> >>>>>> When  Who What
> >>>>>> 1 <NA> <NA> <NA>
> >>>>>>
> >>>>>> But I like that it converted into columns. That's a really great
> >>>>>> format. With a little tweaking, it'd be a great code for this data
> >>>>>> set.
> >>>>>>
> >>>>>> Michael
> >>>>>>
> >>>>>> On Fri, May 17, 2019 at 8:20 AM William Dunlap via R-help
> >>>>>> <r-help at r-project.org> wrote:
> >>>>>>>
> >>>>>>> Consider using readLines() and strcapture() for reading such a
> >>>>> file.
> >>>>>> E.g.,
> >>>>>>> suppose readLines(files) produced a character vector like
> >>>>>>>
> >>>>>>> x <- c("2016-10-21 10:35:36 <Jane Doe> What's your login",
> >>>>>>>         "2016-10-21 10:56:29 <John Doe> John_Doe",
> >>>>>>>         "2016-10-21 10:56:37 <John Doe> Admit#8242",
> >>>>>>>         "October 23, 1819 12:34 <Jane Eyre> I am not an angel")
> >>>>>>>
> >>>>>>> Then you can make a data.frame with columns When, Who, and What by
> >>>>>>> supplying a pattern containing three parenthesized capture
> >>>>> expressions:
> >>>>>>>> z <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
> >>>>>>> [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
> >>>>>>>            x, proto=data.frame(stringsAsFactors=FALSE, When="",
> >>>>> Who="",
> >>>>>>> What=""))
> >>>>>>>> str(z)
> >>>>>>> 'data.frame':   4 obs. of  3 variables:
> >>>>>>> $ When: chr  "2016-10-21 10:35:36" "2016-10-21 10:56:29"
> >>>>> "2016-10-21
> >>>>>>> 10:56:37" NA
> >>>>>>> $ Who : chr  "<Jane Doe>" "<John Doe>" "<John Doe>" NA
> >>>>>>> $ What: chr  "What's your login" "John_Doe" "Admit#8242" NA
> >>>>>>>
> >>>>>>> Lines that don't match the pattern result in NA's - you might make
> >>>>> a
> >>>>>> second
> >>>>>>> pass over the corresponding elements of x with a new pattern.
> >>>>>>>
> >>>>>>> You can convert the When column from character to time with
> >>>>> as.POSIXct().
> >>>>>>>
> >>>>>>> Bill Dunlap
> >>>>>>> TIBCO Software
> >>>>>>> wdunlap tibco.com
> >>>>>>>
> >>>>>>>
> >>>>>>> On Thu, May 16, 2019 at 8:30 PM David Winsemius
> >>>>> <dwinsemius at comcast.net>
> >>>>>>> wrote:
> >>>>>>>
> >>>>>>>>
> >>>>>>>> On 5/16/19 3:53 PM, Michael Boulineau wrote:
> >>>>>>>>> OK. So, I named the object test and then checked the 6347th
> >>>>> item
> >>>>>>>>>
> >>>>>>>>>> test <- readLines ("hangouts-conversation.txt)
> >>>>>>>>>> test [6347]
> >>>>>>>>> [1] "2016-10-21 10:56:37 <John Doe> Admit#8242"
> >>>>>>>>>
> >>>>>>>>> Perhaps where it was getting screwed up is, since the end of
> >>>>> this is
> >>>>>> a
> >>>>>>>>> number (8242), then, given that there's no space between the
> >>>>> number
> >>>>>>>>> and what ought to be the next row, R didn't know where to draw
> >>>>> the
> >>>>>>>>> line. Sure enough, it looks like this when I go to the original
> >>>>> file
> >>>>>>>>> and control f "#8242"
> >>>>>>>>>
> >>>>>>>>> 2016-10-21 10:35:36 <Jane Doe> What's your login
> >>>>>>>>> 2016-10-21 10:56:29 <John Doe> John_Doe
> >>>>>>>>> 2016-10-21 10:56:37 <John Doe> Admit#8242
> >>>>>>>>
> >>>>>>>>
> >>>>>>>> An octothorpe is an end of line signifier and is interpreted as
> >>>>>> allowing
> >>>>>>>> comments. You can prevent that interpretation with suitable
> >>>>> choice of
> >>>>>>>> parameters to `read.table` or `read.csv`. I don't understand why
> >>>>> that
> >>>>>>>> should cause anu error or a failure to match that pattern.
> >>>>>>>>
> >>>>>>>>> 2016-10-21 11:00:13 <Jane Doe> Okay so you have a discussion
> >>>>>>>>>
> >>>>>>>>> Again, it doesn't look like that in the file. Gmail
> >>>>> automatically
> >>>>>>>>> formats it like that when I paste it in. More to the point, it
> >>>>> looks
> >>>>>>>>> like
> >>>>>>>>>
> >>>>>>>>> 2016-10-21 10:35:36 <Jane Doe> What's your login2016-10-21
> >>>>> 10:56:29
> >>>>>>>>> <John Doe> John_Doe2016-10-21 10:56:37 <John Doe>
> >>>>>> Admit#82422016-10-21
> >>>>>>>>> 11:00:13 <Jane Doe> Okay so you have a discussion
> >>>>>>>>>
> >>>>>>>>> Notice Admit#82422016. So there's that.
> >>>>>>>>>
> >>>>>>>>> Then I built object test2.
> >>>>>>>>>
> >>>>>>>>> test2 <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4",
> >>>>> test)
> >>>>>>>>>
> >>>>>>>>> This worked for 84 lines, then this happened.
> >>>>>>>>
> >>>>>>>> It may have done something but as you later discovered my first
> >>>>> code
> >>>>>> for
> >>>>>>>> the pattern was incorrect. I had tested it (and pasted in the
> >>>>> results
> >>>>>> of
> >>>>>>>> the test) . The way to refer to a capture class is with
> >>>>> back-slashes
> >>>>>>>> before the numbers, not forward-slashes. Try this:
> >>>>>>>>
> >>>>>>>>
> >>>>>>>>> newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)",
> >>>>> "\\1,\\2,\\3,\\4",
> >>>>>> chrvec)
> >>>>>>>>> newvec
> >>>>>>>> [1] "2016-07-01,02:50:35,<john>,hey"
> >>>>>>>> [2] "2016-07-01,02:51:26,<jane>,waiting for plane to Edinburgh"
> >>>>>>>> [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
> >>>>>>>> [4] "2016-07-01,02:52:07,<jane>,nothing crappy has happened,
> >>>>> not
> >>>>>> really"
> >>>>>>>> [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast,
> >>>>> didn't
> >>>>>> sleep"
> >>>>>>>> [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or
> >>>>> where I am
> >>>>>>>> really"
> >>>>>>>> [7] "2016-07-01,02:54:17,<john>,just know it's london"
> >>>>>>>> [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
> >>>>>>>> [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a good
> >>>>> eay"
> >>>>>>>> [10] "2016-07-01 02:58:56 <jone>"
> >>>>>>>> [11] "2016-07-01 02:59:34 <jane>"
> >>>>>>>> [12] "2016-07-01,03:02:48,<john>,British security is a little
> >>>>> more
> >>>>>>>> rigorous..."
> >>>>>>>>
> >>>>>>>>
> >>>>>>>> I made note of the fact that the 10th and 11th lines had no
> >>>>> commas.
> >>>>>>>>
> >>>>>>>>>
> >>>>>>>>>> test2 [84]
> >>>>>>>>> [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
> >>>>>>>>
> >>>>>>>> That line didn't have any "<" so wasn't matched.
> >>>>>>>>
> >>>>>>>>
> >>>>>>>> You could remove all none matching lines for pattern of
> >>>>>>>>
> >>>>>>>> dates<space>times<space>"<"<name>">"<space><anything>
> >>>>>>>>
> >>>>>>>>
> >>>>>>>> with:
> >>>>>>>>
> >>>>>>>>
> >>>>>>>> chrvec <- chrvec[ grepl("^.{10} .{8} <.+> .+$)", chrvec)]
> >>>>>>>>
> >>>>>>>>
> >>>>>>>> Do read:
> >>>>>>>>
> >>>>>>>> ?read.csv
> >>>>>>>>
> >>>>>>>> ?regex
> >>>>>>>>
> >>>>>>>>
> >>>>>>>> --
> >>>>>>>>
> >>>>>>>> David
> >>>>>>>>
> >>>>>>>>
> >>>>>>>>>> test2 [85]
> >>>>>>>>> [1] "//1,//2,//3,//4"
> >>>>>>>>>> test [85]
> >>>>>>>>> [1] "2016-07-01 02:50:35 <John Doe> hey"
> >>>>>>>>>
> >>>>>>>>> Notice how I toggled back and forth between test and test2
> >>>>> there. So,
> >>>>>>>>> whatever happened with the regex, it happened in the switch
> >>>>> from 84
> >>>>>> to
> >>>>>>>>> 85, I guess. It went on like
> >>>>>>>>>
> >>>>>>>>> [990] "//1,//2,//3,//4"
> >>>>>>>>> [991] "//1,//2,//3,//4"
> >>>>>>>>> [992] "//1,//2,//3,//4"
> >>>>>>>>> [993] "//1,//2,//3,//4"
> >>>>>>>>> [994] "//1,//2,//3,//4"
> >>>>>>>>> [995] "//1,//2,//3,//4"
> >>>>>>>>> [996] "//1,//2,//3,//4"
> >>>>>>>>> [997] "//1,//2,//3,//4"
> >>>>>>>>> [998] "//1,//2,//3,//4"
> >>>>>>>>> [999] "//1,//2,//3,//4"
> >>>>>>>>> [1000] "//1,//2,//3,//4"
> >>>>>>>>>
> >>>>>>>>> up until line 1000, then I reached max.print.
> >>>>>>>>
> >>>>>>>>> Michael
> >>>>>>>>>
> >>>>>>>>> On Thu, May 16, 2019 at 1:05 PM David Winsemius <
> >>>>>> dwinsemius at comcast.net>
> >>>>>>>> wrote:
> >>>>>>>>>>
> >>>>>>>>>> On 5/16/19 12:30 PM, Michael Boulineau wrote:
> >>>>>>>>>>> Thanks for this tip on etiquette, David. I will be sure and
> >>>>> not do
> >>>>>>>> that again.
> >>>>>>>>>>>
> >>>>>>>>>>> I tried the read.fwf from the foreign package, with a code
> >>>>> like
> >>>>>> this:
> >>>>>>>>>>>
> >>>>>>>>>>>  d <- read.fwf("hangouts-conversation.txt",
> >>>>>>>>>>>                 widths= c(10,10,20,40),
> >>>>>>>>>>>
> >>>>> col.names=c("date","time","person","comment"),
> >>>>>>>>>>>                 strip.white=TRUE)
> >>>>>>>>>>>
> >>>>>>>>>>> But it threw this error:
> >>>>>>>>>>>
> >>>>>>>>>>> Error in scan(file = file, what = what, sep = sep, quote =
> >>>>> quote,
> >>>>>> dec
> >>>>>>>> = dec,  :
> >>>>>>>>>>>   line 6347 did not have 4 elements
> >>>>>>>>>>
> >>>>>>>>>> So what does line 6347 look like? (Use `readLines` and print
> >>>>> it
> >>>>>> out.)
> >>>>>>>>>>
> >>>>>>>>>>> Interestingly, though, the error only happened when I
> >>>>> increased the
> >>>>>>>>>>> width size. But I had to increase the size, or else I
> >>>>> couldn't
> >>>>>> "see"
> >>>>>>>>>>> anything.  The comment was so small that nothing was being
> >>>>>> captured by
> >>>>>>>>>>> the size of the column. so to speak.
> >>>>>>>>>>>
> >>>>>>>>>>> It seems like what's throwing me is that there's no comma
> >>>>> that
> >>>>>>>>>>> demarcates the end of the text proper. For example:
> >>>>>>>>>> Not sure why you thought there should be a comma. Lines
> >>>>> usually end
> >>>>>>>>>> with  <cr> and or a <lf>.
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> Once you have the raw text in a character vector from
> >>>>> `readLines`
> >>>>>> named,
> >>>>>>>>>> say, 'chrvec', then you could selectively substitute commas
> >>>>> for
> >>>>>> spaces
> >>>>>>>>>> with regex. (Now that you no longer desire to remove the dates
> >>>>> and
> >>>>>>>> times.)
> >>>>>>>>>>
> >>>>>>>>>> sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", chrvec)
> >>>>>>>>>>
> >>>>>>>>>> This will not do any replacements when the pattern is not
> >>>>> matched.
> >>>>>> See
> >>>>>>>>>> this test:
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>> newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)",
> >>>>> "\\1,\\2,\\3,\\4",
> >>>>>>>> chrvec)
> >>>>>>>>>>> newvec
> >>>>>>>>>>  [1] "2016-07-01,02:50:35,<john>,hey"
> >>>>>>>>>>  [2] "2016-07-01,02:51:26,<jane>,waiting for plane to
> >>>>> Edinburgh"
> >>>>>>>>>>  [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
> >>>>>>>>>>  [4] "2016-07-01,02:52:07,<jane>,nothing crappy has
> >>>>> happened, not
> >>>>>>>> really"
> >>>>>>>>>>  [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast,
> >>>>> didn't
> >>>>>>>> sleep"
> >>>>>>>>>>  [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or
> >>>>> where
> >>>>>> I am
> >>>>>>>>>> really"
> >>>>>>>>>>  [7] "2016-07-01,02:54:17,<john>,just know it's london"
> >>>>>>>>>>  [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
> >>>>>>>>>>  [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a
> >>>>> good
> >>>>>> eay"
> >>>>>>>>>> [10] "2016-07-01 02:58:56 <jone>"
> >>>>>>>>>> [11] "2016-07-01 02:59:34 <jane>"
> >>>>>>>>>> [12] "2016-07-01,03:02:48,<john>,British security is a little
> >>>>> more
> >>>>>>>>>> rigorous..."
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> You should probably remove the "empty comment" lines.
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> --
> >>>>>>>>>>
> >>>>>>>>>> David.
> >>>>>>>>>>
> >>>>>>>>>>> 2016-07-01 15:34:30 <John Doe> Lame. We were in a
> >>>>>> starbucks2016-07-01
> >>>>>>>>>>> 15:35:02 <Jane Doe> Hmm that's interesting2016-07-01 15:35:09
> >>>>> <Jane
> >>>>>>>>>>> Doe> You must want coffees2016-07-01 15:35:25 <John Doe>
> >>>>> There was
> >>>>>>>>>>> lots of Starbucks in my day2016-07-01 15:35:47
> >>>>>>>>>>>
> >>>>>>>>>>> It was interesting, too, when I pasted the text into the
> >>>>> email, it
> >>>>>>>>>>> self-formatted into the way I wanted it to look. I had to
> >>>>> manually
> >>>>>>>>>>> make it look like it does above, since that's the way that it
> >>>>>> looks in
> >>>>>>>>>>> the txt file. I wonder if it's being organized by XML or
> >>>>> something.
> >>>>>>>>>>>
> >>>>>>>>>>> Anyways, There's always a space between the two sideways
> >>>>> carrots,
> >>>>>> just
> >>>>>>>>>>> like there is right now: <John Doe> See. Space. And there's
> >>>>> always
> >>>>>> a
> >>>>>>>>>>> space between the data and time. Like this. 2016-07-01
> >>>>> 15:34:30
> >>>>>> See.
> >>>>>>>>>>> Space. But there's never a space between the end of the
> >>>>> comment and
> >>>>>>>>>>> the next date. Like this: We were in a starbucks2016-07-01
> >>>>> 15:35:02
> >>>>>>>>>>> See. starbucks and 2016 are smooshed together.
> >>>>>>>>>>>
> >>>>>>>>>>> This code is also on the table right now too.
> >>>>>>>>>>>
> >>>>>>>>>>> a <- read.table("E:/working
> >>>>>>>>>>> directory/-189/hangouts-conversation2.txt", quote="\"",
> >>>>>>>>>>> comment.char="", fill=TRUE)
> >>>>>>>>>>>
> >>>>>>>>>>>
> >>>>>>>>
> >>>>>>
> >>>>> h<-cbind(hangouts.conversation2[,1:2],hangouts.conversation2[,3:5],hangouts.conversation2[,6:9])
> >>>>>>>>>>>
> >>>>>>>>>>> aa<-gsub("[^[:digit:]]","",h)
> >>>>>>>>>>> my.data.num <- as.numeric(str_extract(h, "[0-9]+"))
> >>>>>>>>>>>
> >>>>>>>>>>> Those last lines are a work in progress. I wish I could
> >>>>> import a
> >>>>>>>>>>> picture of what it looks like when it's translated into a
> >>>>> data
> >>>>>> frame.
> >>>>>>>>>>> The fill=TRUE helped to get the data in table that kind of
> >>>>> sort of
> >>>>>>>>>>> works, but the comments keep bleeding into the data and time
> >>>>>> column.
> >>>>>>>>>>> It's like
> >>>>>>>>>>>
> >>>>>>>>>>> 2016-07-01 15:59:17 <Jane Doe> Seriously I've never been
> >>>>>>>>>>> over               there
> >>>>>>>>>>> 2016-07-01 15:59:27 <Jane Doe> It confuses me :(
> >>>>>>>>>>>
> >>>>>>>>>>> And then, maybe, the "seriously" will be in a column all to
> >>>>>> itself, as
> >>>>>>>>>>> will be the "I've'"and the "never" etc.
> >>>>>>>>>>>
> >>>>>>>>>>> I will use a regular expression if I have to, but it would be
> >>>>> nice
> >>>>>> to
> >>>>>>>>>>> keep the dates and times on there. Originally, I thought they
> >>>>> were
> >>>>>>>>>>> meaningless, but I've since changed my mind on that count.
> >>>>> The
> >>>>>> time of
> >>>>>>>>>>> day isn't so important. But, especially since, say, Gmail
> >>>>> itself
> >>>>>> knows
> >>>>>>>>>>> how to quickly recognize what it is, I know it can be done. I
> >>>>> know
> >>>>>>>>>>> this data has structure to it.
> >>>>>>>>>>>
> >>>>>>>>>>> Michael
> >>>>>>>>>>>
> >>>>>>>>>>>
> >>>>>>>>>>>
> >>>>>>>>>>> On Wed, May 15, 2019 at 8:47 PM David Winsemius <
> >>>>>>>> dwinsemius at comcast.net> wrote:
> >>>>>>>>>>>> On 5/15/19 4:07 PM, Michael Boulineau wrote:
> >>>>>>>>>>>>> I have a wild and crazy text file, the head of which looks
> >>>>> like
> >>>>>> this:
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> 2016-07-01 02:50:35 <john> hey
> >>>>>>>>>>>>> 2016-07-01 02:51:26 <jane> waiting for plane to Edinburgh
> >>>>>>>>>>>>> 2016-07-01 02:51:45 <john> thinking about my boo
> >>>>>>>>>>>>> 2016-07-01 02:52:07 <jane> nothing crappy has happened, not
> >>>>>> really
> >>>>>>>>>>>>> 2016-07-01 02:52:20 <john> plane went by pretty fast,
> >>>>> didn't
> >>>>>> sleep
> >>>>>>>>>>>>> 2016-07-01 02:54:08 <jane> no idea what time it is or where
> >>>>> I am
> >>>>>>>> really
> >>>>>>>>>>>>> 2016-07-01 02:54:17 <john> just know it's london
> >>>>>>>>>>>>> 2016-07-01 02:56:44 <jane> you are probably asleep
> >>>>>>>>>>>>> 2016-07-01 02:58:45 <jane> I hope fish was fishy in a good
> >>>>> eay
> >>>>>>>>>>>>> 2016-07-01 02:58:56 <jone>
> >>>>>>>>>>>>> 2016-07-01 02:59:34 <jane>
> >>>>>>>>>>>>> 2016-07-01 03:02:48 <john> British security is a little
> >>>>> more
> >>>>>>>> rigorous...
> >>>>>>>>>>>> Looks entirely not-"crazy". Typical log file format.
> >>>>>>>>>>>>
> >>>>>>>>>>>> Two possibilities: 1) Use `read.fwf` from pkg foreign; 2)
> >>>>> Use
> >>>>>> regex
> >>>>>>>>>>>> (i.e. the sub-function) to strip everything up to the "<".
> >>>>> Read
> >>>>>>>>>>>> `?regex`. Since that's not a metacharacters you could use a
> >>>>>> pattern
> >>>>>>>>>>>> ".+<" and replace with "".
> >>>>>>>>>>>>
> >>>>>>>>>>>> And do read the Posting Guide. Cross-posting to
> >>>>> StackOverflow and
> >>>>>>>> Rhelp,
> >>>>>>>>>>>> at least within hours of each, is considered poor manners.
> >>>>>>>>>>>>
> >>>>>>>>>>>>
> >>>>>>>>>>>> --
> >>>>>>>>>>>>
> >>>>>>>>>>>> David.
> >>>>>>>>>>>>
> >>>>>>>>>>>>> It goes on for a while. It's a big file. But I feel like
> >>>>> it's
> >>>>>> going
> >>>>>>>> to
> >>>>>>>>>>>>> be difficult to annotate with the coreNLP library or
> >>>>> package. I'm
> >>>>>>>>>>>>> doing natural language processing. In other words, I'm
> >>>>> curious
> >>>>>> as to
> >>>>>>>>>>>>> how I would shave off the dates, that is, to make it look
> >>>>> like:
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> <john> hey
> >>>>>>>>>>>>> <jane> waiting for plane to Edinburgh
> >>>>>>>>>>>>>   <john> thinking about my boo
> >>>>>>>>>>>>> <jane> nothing crappy has happened, not really
> >>>>>>>>>>>>> <john> plane went by pretty fast, didn't sleep
> >>>>>>>>>>>>> <jane> no idea what time it is or where I am really
> >>>>>>>>>>>>> <john> just know it's london
> >>>>>>>>>>>>> <jane> you are probably asleep
> >>>>>>>>>>>>> <jane> I hope fish was fishy in a good eay
> >>>>>>>>>>>>>   <jone>
> >>>>>>>>>>>>> <jane>
> >>>>>>>>>>>>> <john> British security is a little more rigorous...
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> To be clear, then, I'm trying to clean a large text file by
> >>>>>> writing a
> >>>>>>>>>>>>> regular expression? such that I create a new object with no
> >>>>>> numbers
> >>>>>>>> or
> >>>>>>>>>>>>> dates.
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> Michael
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> ______________________________________________
> >>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
> >>>>> more,
> >>>>>> see
> >>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>>>>>> PLEASE do read the posting guide
> >>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>>>>>> and provide commented, minimal, self-contained,
> >>>>> reproducible
> >>>>>> code.
> >>>>>>>>>>> ______________________________________________
> >>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >>>>> see
> >>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>>>> PLEASE do read the posting guide
> >>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
> >>>>> code.
> >>>>>>>>>> ______________________________________________
> >>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >>>>> see
> >>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>>> PLEASE do read the posting guide
> >>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>>> and provide commented, minimal, self-contained, reproducible
> >>>>> code.
> >>>>>>>>> ______________________________________________
> >>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >>>>> see
> >>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>> PLEASE do read the posting guide
> >>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>> and provide commented, minimal, self-contained, reproducible
> >>>>> code.
> >>>>>>>>
> >>>>>>>> ______________________________________________
> >>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>> PLEASE do read the posting guide
> >>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>> and provide commented, minimal, self-contained, reproducible
> >>>>> code.
> >>>>>>>>
> >>>>>>>
> >>>>>>>       [[alternative HTML version deleted]]
> >>>>>>>
> >>>>>>> ______________________________________________
> >>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>> PLEASE do read the posting guide
> >>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>
> >>>>>> ______________________________________________
> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>> PLEASE do read the posting guide
> >>>>>> http://www.R-project.org/posting-guide.html
> >>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>
> >>>>>
> >>>>>     [[alternative HTML version deleted]]
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide
> >>>>> http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>>> --
> >>>> Sent from my phone. Please excuse my brevity.
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>


From B|||@Po||ng @end|ng |rom ze||@@com  Sun May 19 12:24:25 2019
From: B|||@Po||ng @end|ng |rom ze||@@com (Bill Poling)
Date: Sun, 19 May 2019 10:24:25 +0000
Subject: [R] 
 Help understanding the relationship between R-3.6.0 and RStudio
In-Reply-To: <4B4B0A41-171E-425D-BDE9-490C8184D770@gmail.com>
References: <BN7PR02MB507382FA2FDB673BFB754A9FEA0B0@BN7PR02MB5073.namprd02.prod.outlook.com>
 <9548F469-06ED-4599-B210-94CC05663D3C@me.com>
 <BN7PR02MB507396D097441034D439D217EA0B0@BN7PR02MB5073.namprd02.prod.outlook.com>
 <4B4B0A41-171E-425D-BDE9-490C8184D770@gmail.com>
Message-ID: <BN7PR02MB5073AF873AF6CF2DB5999475EA050@BN7PR02MB5073.namprd02.prod.outlook.com>

Good morning, I will head your advice, good to know, thank you Peter.

WHP


From: peter dalgaard <pdalgd at gmail.com>
Sent: Saturday, May 18, 2019 4:44 AM
To: Bill Poling <Bill.Poling at zelis.com>
Cc: Marc Schwartz <marc_schwartz at me.com>; R-help <r-help at r-project.org>
Subject: Re: [R] Help understanding the relationship between R-3.6.0 and RStudio

Actually, you might go for 3.6.0-patched. There was a somewhat annoying bug affecting the package installation menu in 3.6.0.

-pd

> On 17 May 2019, at 20:32 , Bill Poling <mailto:Bill.Poling at zelis.com> wrote:
>
> I fixed it by removing previous versions as suggested.
>
>> sessionInfo()
> R version 3.6.0 RC (2019-04-24 r76423)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows 10 x64 (build 17134)
>
> I will have to go out and get the non RC version now.
>
> Thank you.
>
> WHP
>
> From: Marc Schwartz <mailto:marc_schwartz at me.com>
> Sent: Friday, May 17, 2019 2:14 PM
> To: Bill Poling <mailto:Bill.Poling at zelis.com>
> Cc: R-help <mailto:r-help at r-project.org>
> Subject: Re: [R] Help understanding the relationship between R-3.6.0 and RStudio
>
>
>
>> On May 17, 2019, at 2:02 PM, Bill Poling <mailto:Bill.Poling at zelis.com> wrote:
>>
>> Hello.
>>
>> I do not think I have had this problem (assuming it is a problem) in the past.
>>
>> I downloaded and installed R3.6.0 which is indicted in the console when I open R itself.
>>
>> R version 3.6.0 RC (2019-04-24 r76423) -- "Planting of a Tree"
>> Copyright (C) 2019 The R Foundation for Statistical Computing
>> Platform: x86_64-w64-mingw32/x64 (64-bit)
>>
>> However, in RStudio the sessionInfo() remains
>>
>> R version 3.5.3 (2019-03-11)
>> Platform: x86_64-w64-mingw32/x64 (64-bit)
>> Running under: Windows 10 x64 (build 17134)
>>
>> I also installed the latest version of RStudio 1.2.1335 as well "after" installing R 3.6.0.
>>
>> I also rebooted my computer.
>>
>> I am not sure why this time the two do not seem to be (for lack of a better word) in sink?
>>
>> Thank you for any insight
>>
>> WHP
>
>
> Hi,
>
> I don't use RStudio, which is a GUI/IDE on top of R, it is not R.
>
> That being said, a quick Google search supports my intuition, which is that RStudio appears to be able to support multiple R version installations:
>
> https://support.rstudio.com/hc/en-us/articles/200486138-Changing-R-versions-for-RStudio-desktop
>
> RStudio also has their own support venue:
>
> https://support.rstudio.com/hc/en-us
>
> If I read correctly, it looks like you actually installed a "Release Candidate" (RC) version of 3.6.0 for Windows. So you probably want to visit a CRAN mirror and download the release version of 3.6.0:
>
> R version 3.6.0 (2019-04-26) -- "Planting of a Tree"
>
> If you do not want to have multiple R versions on your computer, you can use the normal Windows application uninstall process to remove the older version(s).
>
> Regards,
>
> Marc Schwartz
>
> Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}
>
> ______________________________________________
> mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: mailto:pd.mes at cbs.dk Priv: mailto:PDalgd at gmail.com








Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}


From bor|@@@te|pe @end|ng |rom utoronto@c@  Sun May 19 13:37:26 2019
From: bor|@@@te|pe @end|ng |rom utoronto@c@ (Boris Steipe)
Date: Sun, 19 May 2019 11:37:26 +0000
Subject: [R] how to separate string from numbers in a large txt file
In-Reply-To: <CAH+cTGMsD+PkTKoFfB8=3WeWPQAWjhmuOCKZfnB7jR03m7RtDQ@mail.gmail.com>
References: <CAH+cTGM9sFCxegaMHzu-_9Prhvvh9MbVDCWJ1_x7wkO9jC47mQ@mail.gmail.com>
 <c6e69644-66a1-7031-4d7c-1000e0abbea4@comcast.net>
 <CAH+cTGM5YvWV_G0pRgMqjCzuRrFTzd0nOc7yoThfGbGMUyMjsQ@mail.gmail.com>
 <9c9f1c81-309f-91cb-7e26-f02fc4bd45d8@comcast.net>
 <CAH+cTGMx4_Lz2gGDf2AuUaaZF2Xaq4McuPRoHu4JJNc-ORd=Ew@mail.gmail.com>
 <6322e735-5659-848e-3a50-9f066f1299ad@comcast.net>
 <CAF8bMcbgrVcN9J7Aygq2D5RWaU4MS-GH+_+zMHU=AO+DU5DVWw@mail.gmail.com>
 <CAH+cTGNfpLSurqmsoMeci0GWhC0ncOp7zTEju3fJW5T4+tCsGA@mail.gmail.com>
 <CAF8bMcaSJRaUXK4h2ssaDoJwXaKCUZT2Vq1cutNiNDjQ7ga2Xg@mail.gmail.com>
 <EEA9FD46-A67C-4FB7-8245-350819C4EA62@dcn.davis.ca.us>
 <CAH+cTGNOkAnPnn-aru-BJUbhrgeYMQBTqjSYT764nKzcdXvTvA@mail.gmail.com>
 <9B170703-AAAF-4D52-9744-1FDE70559B05@utoronto.ca>
 <CAH+cTGNaw7p-it4B7te4BBsay=v6J-5OS-P_WGDJJ6Z0ymxtvw@mail.gmail.com>
 <0F8CD89C-BDE2-44D0-8E18-0CA2706EB53F@utoronto.ca>
 <CAH+cTGMsD+PkTKoFfB8=3WeWPQAWjhmuOCKZfnB7jR03m7RtDQ@mail.gmail.com>
Message-ID: <C5A52A04-CB49-442F-81CE-26CBC619C47A@utoronto.ca>

Inline



> On 2019-05-18, at 20:34, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
> 
> It appears to have worked, although there were three little quirks.
> The ; close(con); rm(con) didn't work for me; the first row of the
> data.frame was all NAs, when all was said and done;

You will get NAs for lines that can't be matched to the regular expression. That's a good thing, it allows you to test whether your assumptions were valid for the entire file:

# number of failed strcapture()
sum(is.na(e$date))


> and then there
> were still three *** on the same line where the ??? was apparently
> deleted.

This is a sign that something else happened with the line that prevented the regex from matching. In that case you need to investigate more. I see an invalid multibyte character at the beginning of the line you posted below.

> 
>> a <- readLines ("hangouts-conversation-6.txt", encoding = "UTF-8")
>> b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)"
>> c <- gsub(b, "\\1<\\2> ", a)
>> head (c)
> [1] "?2016-01-27 09:14:40 *** Jane Doe started a video chat"
> [2] "2016-01-27 09:15:20 <Jane Doe>
> https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf"

[...]

> But, before I do anything else, I'm going to study the regex in this
> particular code. For example, I'm still not sure why there has to the
> second \\w+ in the (\\w+ \\w+). Little things like that.

\w is the metacharacter for alphanumeric characters, \w+ designates something we could call a word. Thus \w+ \w+ are two words separated by a single blank. This corresponds to your example, but, as I wrote previously, you need to think very carefully whether this covers all possible cases (Could there be only one word? More than one blank? Could letters be separated by hyphens or periods?) In most cases we could have more robustly matched everything between "<" and ">" (taking care to test what happens if the message contains those characters). But for the video chat lines we need to make an assumption about what is name and what is not. If "started a video chat" is the only possibility in such lines, you can use this information instead. If there are other possibilities, you need a different strategy. In NLP there is no one-approach-fits-all.

To validate the structure of the names in your transcripts, you can look at

patt <- " <.+?> "   # " <any string, not greedy> "
m <- regexpr(patt, c)
unique(regmatches(c, m))



B.



> 
> Michael
> 
> 
> On Sat, May 18, 2019 at 4:30 PM Boris Steipe <boris.steipe at utoronto.ca> wrote:
>> 
>> This works for me:
>> 
>> # sample data
>> c <- character()
>> c[1] <- "2016-01-27 09:14:40 <Jane Doe> started a video chat"
>> c[2] <- "2016-01-27 09:15:20 <Jane Doe> https://lh3.googleusercontent.com/"
>> c[3] <- "2016-01-27 09:15:20 <Jane Doe> Hey "
>> c[4] <- "2016-01-27 09:15:22 <John Doe>  ended a video chat"
>> c[5] <- "2016-01-27 21:07:11 <Jane Doe>  started a video chat"
>> c[6] <- "2016-01-27 21:26:57 <John Doe>  ended a video chat"
>> 
>> 
>> # regex  ^(year)       (time)      <(word word)>\\s*(string)$
>> patt <- "^([0-9-]{10}) ([0-9:]{8}) <(\\w+ \\w+)>\\s*(.+)$"
>> proto <- data.frame(date = character(),
>>                    time = character(),
>>                    name = character(),
>>                    text = character(),
>>                    stringsAsFactors = TRUE)
>> d <- strcapture(patt, c, proto)
>> 
>> 
>> 
>>        date     time     name                               text
>> 1 2016-01-27 09:14:40 Jane Doe               started a video chat
>> 2 2016-01-27 09:15:20 Jane Doe https://lh3.googleusercontent.com/
>> 3 2016-01-27 09:15:20 Jane Doe                               Hey
>> 4 2016-01-27 09:15:22 John Doe                 ended a video chat
>> 5 2016-01-27 21:07:11 Jane Doe               started a video chat
>> 6 2016-01-27 21:26:57 John Doe                 ended a video chat
>> 
>> 
>> 
>> B.
>> 
>> 
>>> On 2019-05-18, at 18:32, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
>>> 
>>> Going back and thinking through what Boris and William were saying
>>> (also Ivan), I tried this:
>>> 
>>> a <- readLines ("hangouts-conversation-6.csv.txt")
>>> b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)"
>>> c <- gsub(b, "\\1<\\2> ", a)
>>>> head (c)
>>> [1] "???2016-01-27 09:14:40 *** Jane Doe started a video chat"
>>> [2] "2016-01-27 09:15:20 <Jane Doe>
>>> https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf"
>>> [3] "2016-01-27 09:15:20 <Jane Doe> Hey "
>>> [4] "2016-01-27 09:15:22 <John Doe>  ended a video chat"
>>> [5] "2016-01-27 21:07:11 <Jane Doe>  started a video chat"
>>> [6] "2016-01-27 21:26:57 <John Doe>  ended a video chat"
>>> 
>>> The ??? is still there, since I forgot to do what Ivan had suggested, namely,
>>> 
>>> a <- readLines(con <- file("hangouts-conversation-6.csv.txt", encoding
>>> = "UTF-8")); close(con); rm(con)
>>> 
>>> But then the new code is still turning out only NAs when I apply
>>> strcapture (). This was what happened next:
>>> 
>>>> d <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
>>> + [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
>>> +                 c, proto=data.frame(stringsAsFactors=FALSE, When="", Who="",
>>> +                                     What=""))
>>>> head (d)
>>> When  Who What
>>> 1 <NA> <NA> <NA>
>>> 2 <NA> <NA> <NA>
>>> 3 <NA> <NA> <NA>
>>> 4 <NA> <NA> <NA>
>>> 5 <NA> <NA> <NA>
>>> 6 <NA> <NA> <NA>
>>> 
>>> I've been reading up on regular expressions, too, so this code seems
>>> spot on. What's going wrong?
>>> 
>>> Michael
>>> 
>>> On Fri, May 17, 2019 at 4:28 PM Boris Steipe <boris.steipe at utoronto.ca> wrote:
>>>> 
>>>> Don't start putting in extra commas and then reading this as csv. That approach is broken. The correct approach is what Bill outlined: read everything with readLines(), and then use a proper regular expression with strcapture().
>>>> 
>>>> You need to pre-process the object that readLines() gives you: replace the contents of the videochat lines, and make it conform to the format of the other lines before you process it into your data frame.
>>>> 
>>>> Approximately something like
>>>> 
>>>> # read the raw data
>>>> tmp <- readLines("hangouts-conversation-6.csv.txt")
>>>> 
>>>> # process all video chat lines
>>>> patt <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+) "  # (year time )*** (word word)
>>>> tmp <- gsub(patt, "\\1<\\2> ", tmp)
>>>> 
>>>> # next, use strcapture()
>>>> 
>>>> Note that this makes the assumption that your names are always exactly two words containing only letters. If that assumption is not true, more though needs to go into the regex. But you can test that:
>>>> 
>>>> patt <- " <\\w+ \\w+> "   #" <word word> "
>>>> sum( ! grepl(patt, tmp)))
>>>> 
>>>> ... will give the number of lines that remain in your file that do not have a tag that can be interpreted as "Who"
>>>> 
>>>> Once that is fine, use Bill's approach - or a regular expression of your own design - to create your data frame.
>>>> 
>>>> Hope this helps,
>>>> Boris
>>>> 
>>>> 
>>>> 
>>>> 
>>>>> On 2019-05-17, at 16:18, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
>>>>> 
>>>>> Very interesting. I'm sure I'll be trying to get rid of the byte order
>>>>> mark eventually. But right now, I'm more worried about getting the
>>>>> character vector into either a csv file or data.frame; that way, I can
>>>>> be able to work with the data neatly tabulated into four columns:
>>>>> date, time, person, comment. I assume it's a write.csv function, but I
>>>>> don't know what arguments to put in it. header=FALSE? fill=T?
>>>>> 
>>>>> Micheal
>>>>> 
>>>>> On Fri, May 17, 2019 at 1:03 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>>>>> 
>>>>>> If byte order mark is the issue then you can specify the file encoding as "UTF-8-BOM" and it won't show up in your data any more.
>>>>>> 
>>>>>> On May 17, 2019 12:12:17 PM PDT, William Dunlap via R-help <r-help at r-project.org> wrote:
>>>>>>> The pattern I gave worked for the lines that you originally showed from
>>>>>>> the
>>>>>>> data file ('a'), before you put commas into them.  If the name is
>>>>>>> either of
>>>>>>> the form "<name>" or "***" then the "(<[^>]*>)" needs to be changed so
>>>>>>> something like "(<[^>]*>|[*]{3})".
>>>>>>> 
>>>>>>> The " ???" at the start of the imported data may come from the byte
>>>>>>> order
>>>>>>> mark that Windows apps like to put at the front of a text file in UTF-8
>>>>>>> or
>>>>>>> UTF-16 format.
>>>>>>> 
>>>>>>> Bill Dunlap
>>>>>>> TIBCO Software
>>>>>>> wdunlap tibco.com
>>>>>>> 
>>>>>>> 
>>>>>>> On Fri, May 17, 2019 at 11:53 AM Michael Boulineau <
>>>>>>> michael.p.boulineau at gmail.com> wrote:
>>>>>>> 
>>>>>>>> This seemed to work:
>>>>>>>> 
>>>>>>>>> a <- readLines ("hangouts-conversation-6.csv.txt")
>>>>>>>>> b <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4", a)
>>>>>>>>> b [1:84]
>>>>>>>> 
>>>>>>>> And the first 85 lines looks like this:
>>>>>>>> 
>>>>>>>> [83] "2016-06-28 21:02:28 *** Jane Doe started a video chat"
>>>>>>>> [84] "2016-06-28 21:12:43 *** John Doe ended a video chat"
>>>>>>>> 
>>>>>>>> Then they transition to the commas:
>>>>>>>> 
>>>>>>>>> b [84:100]
>>>>>>>> [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
>>>>>>>> [2] "2016-07-01,02:50:35,<John Doe>,hey"
>>>>>>>> [3] "2016-07-01,02:51:26,<John Doe>,waiting for plane to Edinburgh"
>>>>>>>> [4] "2016-07-01,02:51:45,<John Doe>,thinking about my boo"
>>>>>>>> 
>>>>>>>> Even the strange bit on line 6347 was caught by this:
>>>>>>>> 
>>>>>>>>> b [6346:6348]
>>>>>>>> [1] "2016-10-21,10:56:29,<John Doe>,John_Doe"
>>>>>>>> [2] "2016-10-21,10:56:37,<John Doe>,Admit#8242"
>>>>>>>> [3] "2016-10-21,11:00:13,<Jane Doe>,Okay so you have a discussion"
>>>>>>>> 
>>>>>>>> Perhaps most awesomely, the code catches spaces that are interposed
>>>>>>>> into the comment itself:
>>>>>>>> 
>>>>>>>>> b [4]
>>>>>>>> [1] "2016-01-27,09:15:20,<Jane Doe>,Hey "
>>>>>>>>> b [85]
>>>>>>>> [1] "2016-07-01,02:50:35,<John Doe>,hey"
>>>>>>>> 
>>>>>>>> Notice whether there is a space after the "hey" or not.
>>>>>>>> 
>>>>>>>> These are the first two lines:
>>>>>>>> 
>>>>>>>> [1] "???2016-01-27 09:14:40 *** Jane Doe started a video chat"
>>>>>>>> [2] "2016-01-27,09:15:20,<Jane
>>>>>>>> Doe>,
>>>>>>>> 
>>>>>>> https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf
>>>>>>>> "
>>>>>>>> 
>>>>>>>> So, who knows what happened with the ??? at the beginning of [1]
>>>>>>>> directly above. But notice how there are no commas in [1] but there
>>>>>>>> appear in [2]. I don't see why really long ones like [2] directly
>>>>>>>> above would be a problem, were they to be translated into a csv or
>>>>>>>> data frame column.
>>>>>>>> 
>>>>>>>> Now, with the commas in there, couldn't we write this into a csv or a
>>>>>>>> data.frame? Some of this data will end up being garbage, I imagine.
>>>>>>>> Like in [2] directly above. Or with [83] and [84] at the top of this
>>>>>>>> discussion post/email. Embarrassingly, I've been trying to convert
>>>>>>>> this into a data.frame or csv but I can't manage to. I've been using
>>>>>>>> the write.csv function, but I don't think I've been getting the
>>>>>>>> arguments correct.
>>>>>>>> 
>>>>>>>> At the end of the day, I would like a data.frame and/or csv with the
>>>>>>>> following four columns: date, time, person, comment.
>>>>>>>> 
>>>>>>>> I tried this, too:
>>>>>>>> 
>>>>>>>>> c <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
>>>>>>>> + [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
>>>>>>>> +                 a, proto=data.frame(stringsAsFactors=FALSE,
>>>>>>> When="",
>>>>>>>> Who="",
>>>>>>>> +                                     What=""))
>>>>>>>> 
>>>>>>>> But all I got was this:
>>>>>>>> 
>>>>>>>>> c [1:100, ]
>>>>>>>>  When  Who What
>>>>>>>> 1   <NA> <NA> <NA>
>>>>>>>> 2   <NA> <NA> <NA>
>>>>>>>> 3   <NA> <NA> <NA>
>>>>>>>> 4   <NA> <NA> <NA>
>>>>>>>> 5   <NA> <NA> <NA>
>>>>>>>> 6   <NA> <NA> <NA>
>>>>>>>> 
>>>>>>>> It seems to have caught nothing.
>>>>>>>> 
>>>>>>>>> unique (c)
>>>>>>>> When  Who What
>>>>>>>> 1 <NA> <NA> <NA>
>>>>>>>> 
>>>>>>>> But I like that it converted into columns. That's a really great
>>>>>>>> format. With a little tweaking, it'd be a great code for this data
>>>>>>>> set.
>>>>>>>> 
>>>>>>>> Michael
>>>>>>>> 
>>>>>>>> On Fri, May 17, 2019 at 8:20 AM William Dunlap via R-help
>>>>>>>> <r-help at r-project.org> wrote:
>>>>>>>>> 
>>>>>>>>> Consider using readLines() and strcapture() for reading such a
>>>>>>> file.
>>>>>>>> E.g.,
>>>>>>>>> suppose readLines(files) produced a character vector like
>>>>>>>>> 
>>>>>>>>> x <- c("2016-10-21 10:35:36 <Jane Doe> What's your login",
>>>>>>>>>        "2016-10-21 10:56:29 <John Doe> John_Doe",
>>>>>>>>>        "2016-10-21 10:56:37 <John Doe> Admit#8242",
>>>>>>>>>        "October 23, 1819 12:34 <Jane Eyre> I am not an angel")
>>>>>>>>> 
>>>>>>>>> Then you can make a data.frame with columns When, Who, and What by
>>>>>>>>> supplying a pattern containing three parenthesized capture
>>>>>>> expressions:
>>>>>>>>>> z <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
>>>>>>>>> [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
>>>>>>>>>           x, proto=data.frame(stringsAsFactors=FALSE, When="",
>>>>>>> Who="",
>>>>>>>>> What=""))
>>>>>>>>>> str(z)
>>>>>>>>> 'data.frame':   4 obs. of  3 variables:
>>>>>>>>> $ When: chr  "2016-10-21 10:35:36" "2016-10-21 10:56:29"
>>>>>>> "2016-10-21
>>>>>>>>> 10:56:37" NA
>>>>>>>>> $ Who : chr  "<Jane Doe>" "<John Doe>" "<John Doe>" NA
>>>>>>>>> $ What: chr  "What's your login" "John_Doe" "Admit#8242" NA
>>>>>>>>> 
>>>>>>>>> Lines that don't match the pattern result in NA's - you might make
>>>>>>> a
>>>>>>>> second
>>>>>>>>> pass over the corresponding elements of x with a new pattern.
>>>>>>>>> 
>>>>>>>>> You can convert the When column from character to time with
>>>>>>> as.POSIXct().
>>>>>>>>> 
>>>>>>>>> Bill Dunlap
>>>>>>>>> TIBCO Software
>>>>>>>>> wdunlap tibco.com
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> On Thu, May 16, 2019 at 8:30 PM David Winsemius
>>>>>>> <dwinsemius at comcast.net>
>>>>>>>>> wrote:
>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> On 5/16/19 3:53 PM, Michael Boulineau wrote:
>>>>>>>>>>> OK. So, I named the object test and then checked the 6347th
>>>>>>> item
>>>>>>>>>>> 
>>>>>>>>>>>> test <- readLines ("hangouts-conversation.txt)
>>>>>>>>>>>> test [6347]
>>>>>>>>>>> [1] "2016-10-21 10:56:37 <John Doe> Admit#8242"
>>>>>>>>>>> 
>>>>>>>>>>> Perhaps where it was getting screwed up is, since the end of
>>>>>>> this is
>>>>>>>> a
>>>>>>>>>>> number (8242), then, given that there's no space between the
>>>>>>> number
>>>>>>>>>>> and what ought to be the next row, R didn't know where to draw
>>>>>>> the
>>>>>>>>>>> line. Sure enough, it looks like this when I go to the original
>>>>>>> file
>>>>>>>>>>> and control f "#8242"
>>>>>>>>>>> 
>>>>>>>>>>> 2016-10-21 10:35:36 <Jane Doe> What's your login
>>>>>>>>>>> 2016-10-21 10:56:29 <John Doe> John_Doe
>>>>>>>>>>> 2016-10-21 10:56:37 <John Doe> Admit#8242
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> An octothorpe is an end of line signifier and is interpreted as
>>>>>>>> allowing
>>>>>>>>>> comments. You can prevent that interpretation with suitable
>>>>>>> choice of
>>>>>>>>>> parameters to `read.table` or `read.csv`. I don't understand why
>>>>>>> that
>>>>>>>>>> should cause anu error or a failure to match that pattern.
>>>>>>>>>> 
>>>>>>>>>>> 2016-10-21 11:00:13 <Jane Doe> Okay so you have a discussion
>>>>>>>>>>> 
>>>>>>>>>>> Again, it doesn't look like that in the file. Gmail
>>>>>>> automatically
>>>>>>>>>>> formats it like that when I paste it in. More to the point, it
>>>>>>> looks
>>>>>>>>>>> like
>>>>>>>>>>> 
>>>>>>>>>>> 2016-10-21 10:35:36 <Jane Doe> What's your login2016-10-21
>>>>>>> 10:56:29
>>>>>>>>>>> <John Doe> John_Doe2016-10-21 10:56:37 <John Doe>
>>>>>>>> Admit#82422016-10-21
>>>>>>>>>>> 11:00:13 <Jane Doe> Okay so you have a discussion
>>>>>>>>>>> 
>>>>>>>>>>> Notice Admit#82422016. So there's that.
>>>>>>>>>>> 
>>>>>>>>>>> Then I built object test2.
>>>>>>>>>>> 
>>>>>>>>>>> test2 <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4",
>>>>>>> test)
>>>>>>>>>>> 
>>>>>>>>>>> This worked for 84 lines, then this happened.
>>>>>>>>>> 
>>>>>>>>>> It may have done something but as you later discovered my first
>>>>>>> code
>>>>>>>> for
>>>>>>>>>> the pattern was incorrect. I had tested it (and pasted in the
>>>>>>> results
>>>>>>>> of
>>>>>>>>>> the test) . The way to refer to a capture class is with
>>>>>>> back-slashes
>>>>>>>>>> before the numbers, not forward-slashes. Try this:
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>>> newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)",
>>>>>>> "\\1,\\2,\\3,\\4",
>>>>>>>> chrvec)
>>>>>>>>>>> newvec
>>>>>>>>>> [1] "2016-07-01,02:50:35,<john>,hey"
>>>>>>>>>> [2] "2016-07-01,02:51:26,<jane>,waiting for plane to Edinburgh"
>>>>>>>>>> [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
>>>>>>>>>> [4] "2016-07-01,02:52:07,<jane>,nothing crappy has happened,
>>>>>>> not
>>>>>>>> really"
>>>>>>>>>> [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast,
>>>>>>> didn't
>>>>>>>> sleep"
>>>>>>>>>> [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or
>>>>>>> where I am
>>>>>>>>>> really"
>>>>>>>>>> [7] "2016-07-01,02:54:17,<john>,just know it's london"
>>>>>>>>>> [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
>>>>>>>>>> [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a good
>>>>>>> eay"
>>>>>>>>>> [10] "2016-07-01 02:58:56 <jone>"
>>>>>>>>>> [11] "2016-07-01 02:59:34 <jane>"
>>>>>>>>>> [12] "2016-07-01,03:02:48,<john>,British security is a little
>>>>>>> more
>>>>>>>>>> rigorous..."
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> I made note of the fact that the 10th and 11th lines had no
>>>>>>> commas.
>>>>>>>>>> 
>>>>>>>>>>> 
>>>>>>>>>>>> test2 [84]
>>>>>>>>>>> [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
>>>>>>>>>> 
>>>>>>>>>> That line didn't have any "<" so wasn't matched.
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> You could remove all none matching lines for pattern of
>>>>>>>>>> 
>>>>>>>>>> dates<space>times<space>"<"<name>">"<space><anything>
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> with:
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> chrvec <- chrvec[ grepl("^.{10} .{8} <.+> .+$)", chrvec)]
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> Do read:
>>>>>>>>>> 
>>>>>>>>>> ?read.csv
>>>>>>>>>> 
>>>>>>>>>> ?regex
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> --
>>>>>>>>>> 
>>>>>>>>>> David
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>>>> test2 [85]
>>>>>>>>>>> [1] "//1,//2,//3,//4"
>>>>>>>>>>>> test [85]
>>>>>>>>>>> [1] "2016-07-01 02:50:35 <John Doe> hey"
>>>>>>>>>>> 
>>>>>>>>>>> Notice how I toggled back and forth between test and test2
>>>>>>> there. So,
>>>>>>>>>>> whatever happened with the regex, it happened in the switch
>>>>>>> from 84
>>>>>>>> to
>>>>>>>>>>> 85, I guess. It went on like
>>>>>>>>>>> 
>>>>>>>>>>> [990] "//1,//2,//3,//4"
>>>>>>>>>>> [991] "//1,//2,//3,//4"
>>>>>>>>>>> [992] "//1,//2,//3,//4"
>>>>>>>>>>> [993] "//1,//2,//3,//4"
>>>>>>>>>>> [994] "//1,//2,//3,//4"
>>>>>>>>>>> [995] "//1,//2,//3,//4"
>>>>>>>>>>> [996] "//1,//2,//3,//4"
>>>>>>>>>>> [997] "//1,//2,//3,//4"
>>>>>>>>>>> [998] "//1,//2,//3,//4"
>>>>>>>>>>> [999] "//1,//2,//3,//4"
>>>>>>>>>>> [1000] "//1,//2,//3,//4"
>>>>>>>>>>> 
>>>>>>>>>>> up until line 1000, then I reached max.print.
>>>>>>>>>> 
>>>>>>>>>>> Michael
>>>>>>>>>>> 
>>>>>>>>>>> On Thu, May 16, 2019 at 1:05 PM David Winsemius <
>>>>>>>> dwinsemius at comcast.net>
>>>>>>>>>> wrote:
>>>>>>>>>>>> 
>>>>>>>>>>>> On 5/16/19 12:30 PM, Michael Boulineau wrote:
>>>>>>>>>>>>> Thanks for this tip on etiquette, David. I will be sure and
>>>>>>> not do
>>>>>>>>>> that again.
>>>>>>>>>>>>> 
>>>>>>>>>>>>> I tried the read.fwf from the foreign package, with a code
>>>>>>> like
>>>>>>>> this:
>>>>>>>>>>>>> 
>>>>>>>>>>>>> d <- read.fwf("hangouts-conversation.txt",
>>>>>>>>>>>>>                widths= c(10,10,20,40),
>>>>>>>>>>>>> 
>>>>>>> col.names=c("date","time","person","comment"),
>>>>>>>>>>>>>                strip.white=TRUE)
>>>>>>>>>>>>> 
>>>>>>>>>>>>> But it threw this error:
>>>>>>>>>>>>> 
>>>>>>>>>>>>> Error in scan(file = file, what = what, sep = sep, quote =
>>>>>>> quote,
>>>>>>>> dec
>>>>>>>>>> = dec,  :
>>>>>>>>>>>>>  line 6347 did not have 4 elements
>>>>>>>>>>>> 
>>>>>>>>>>>> So what does line 6347 look like? (Use `readLines` and print
>>>>>>> it
>>>>>>>> out.)
>>>>>>>>>>>> 
>>>>>>>>>>>>> Interestingly, though, the error only happened when I
>>>>>>> increased the
>>>>>>>>>>>>> width size. But I had to increase the size, or else I
>>>>>>> couldn't
>>>>>>>> "see"
>>>>>>>>>>>>> anything.  The comment was so small that nothing was being
>>>>>>>> captured by
>>>>>>>>>>>>> the size of the column. so to speak.
>>>>>>>>>>>>> 
>>>>>>>>>>>>> It seems like what's throwing me is that there's no comma
>>>>>>> that
>>>>>>>>>>>>> demarcates the end of the text proper. For example:
>>>>>>>>>>>> Not sure why you thought there should be a comma. Lines
>>>>>>> usually end
>>>>>>>>>>>> with  <cr> and or a <lf>.
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> Once you have the raw text in a character vector from
>>>>>>> `readLines`
>>>>>>>> named,
>>>>>>>>>>>> say, 'chrvec', then you could selectively substitute commas
>>>>>>> for
>>>>>>>> spaces
>>>>>>>>>>>> with regex. (Now that you no longer desire to remove the dates
>>>>>>> and
>>>>>>>>>> times.)
>>>>>>>>>>>> 
>>>>>>>>>>>> sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", chrvec)
>>>>>>>>>>>> 
>>>>>>>>>>>> This will not do any replacements when the pattern is not
>>>>>>> matched.
>>>>>>>> See
>>>>>>>>>>>> this test:
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>>> newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)",
>>>>>>> "\\1,\\2,\\3,\\4",
>>>>>>>>>> chrvec)
>>>>>>>>>>>>> newvec
>>>>>>>>>>>> [1] "2016-07-01,02:50:35,<john>,hey"
>>>>>>>>>>>> [2] "2016-07-01,02:51:26,<jane>,waiting for plane to
>>>>>>> Edinburgh"
>>>>>>>>>>>> [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
>>>>>>>>>>>> [4] "2016-07-01,02:52:07,<jane>,nothing crappy has
>>>>>>> happened, not
>>>>>>>>>> really"
>>>>>>>>>>>> [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast,
>>>>>>> didn't
>>>>>>>>>> sleep"
>>>>>>>>>>>> [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or
>>>>>>> where
>>>>>>>> I am
>>>>>>>>>>>> really"
>>>>>>>>>>>> [7] "2016-07-01,02:54:17,<john>,just know it's london"
>>>>>>>>>>>> [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
>>>>>>>>>>>> [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a
>>>>>>> good
>>>>>>>> eay"
>>>>>>>>>>>> [10] "2016-07-01 02:58:56 <jone>"
>>>>>>>>>>>> [11] "2016-07-01 02:59:34 <jane>"
>>>>>>>>>>>> [12] "2016-07-01,03:02:48,<john>,British security is a little
>>>>>>> more
>>>>>>>>>>>> rigorous..."
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> You should probably remove the "empty comment" lines.
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> --
>>>>>>>>>>>> 
>>>>>>>>>>>> David.
>>>>>>>>>>>> 
>>>>>>>>>>>>> 2016-07-01 15:34:30 <John Doe> Lame. We were in a
>>>>>>>> starbucks2016-07-01
>>>>>>>>>>>>> 15:35:02 <Jane Doe> Hmm that's interesting2016-07-01 15:35:09
>>>>>>> <Jane
>>>>>>>>>>>>> Doe> You must want coffees2016-07-01 15:35:25 <John Doe>
>>>>>>> There was
>>>>>>>>>>>>> lots of Starbucks in my day2016-07-01 15:35:47
>>>>>>>>>>>>> 
>>>>>>>>>>>>> It was interesting, too, when I pasted the text into the
>>>>>>> email, it
>>>>>>>>>>>>> self-formatted into the way I wanted it to look. I had to
>>>>>>> manually
>>>>>>>>>>>>> make it look like it does above, since that's the way that it
>>>>>>>> looks in
>>>>>>>>>>>>> the txt file. I wonder if it's being organized by XML or
>>>>>>> something.
>>>>>>>>>>>>> 
>>>>>>>>>>>>> Anyways, There's always a space between the two sideways
>>>>>>> carrots,
>>>>>>>> just
>>>>>>>>>>>>> like there is right now: <John Doe> See. Space. And there's
>>>>>>> always
>>>>>>>> a
>>>>>>>>>>>>> space between the data and time. Like this. 2016-07-01
>>>>>>> 15:34:30
>>>>>>>> See.
>>>>>>>>>>>>> Space. But there's never a space between the end of the
>>>>>>> comment and
>>>>>>>>>>>>> the next date. Like this: We were in a starbucks2016-07-01
>>>>>>> 15:35:02
>>>>>>>>>>>>> See. starbucks and 2016 are smooshed together.
>>>>>>>>>>>>> 
>>>>>>>>>>>>> This code is also on the table right now too.
>>>>>>>>>>>>> 
>>>>>>>>>>>>> a <- read.table("E:/working
>>>>>>>>>>>>> directory/-189/hangouts-conversation2.txt", quote="\"",
>>>>>>>>>>>>> comment.char="", fill=TRUE)
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>> 
>>>>>>> h<-cbind(hangouts.conversation2[,1:2],hangouts.conversation2[,3:5],hangouts.conversation2[,6:9])
>>>>>>>>>>>>> 
>>>>>>>>>>>>> aa<-gsub("[^[:digit:]]","",h)
>>>>>>>>>>>>> my.data.num <- as.numeric(str_extract(h, "[0-9]+"))
>>>>>>>>>>>>> 
>>>>>>>>>>>>> Those last lines are a work in progress. I wish I could
>>>>>>> import a
>>>>>>>>>>>>> picture of what it looks like when it's translated into a
>>>>>>> data
>>>>>>>> frame.
>>>>>>>>>>>>> The fill=TRUE helped to get the data in table that kind of
>>>>>>> sort of
>>>>>>>>>>>>> works, but the comments keep bleeding into the data and time
>>>>>>>> column.
>>>>>>>>>>>>> It's like
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 2016-07-01 15:59:17 <Jane Doe> Seriously I've never been
>>>>>>>>>>>>> over               there
>>>>>>>>>>>>> 2016-07-01 15:59:27 <Jane Doe> It confuses me :(
>>>>>>>>>>>>> 
>>>>>>>>>>>>> And then, maybe, the "seriously" will be in a column all to
>>>>>>>> itself, as
>>>>>>>>>>>>> will be the "I've'"and the "never" etc.
>>>>>>>>>>>>> 
>>>>>>>>>>>>> I will use a regular expression if I have to, but it would be
>>>>>>> nice
>>>>>>>> to
>>>>>>>>>>>>> keep the dates and times on there. Originally, I thought they
>>>>>>> were
>>>>>>>>>>>>> meaningless, but I've since changed my mind on that count.
>>>>>>> The
>>>>>>>> time of
>>>>>>>>>>>>> day isn't so important. But, especially since, say, Gmail
>>>>>>> itself
>>>>>>>> knows
>>>>>>>>>>>>> how to quickly recognize what it is, I know it can be done. I
>>>>>>> know
>>>>>>>>>>>>> this data has structure to it.
>>>>>>>>>>>>> 
>>>>>>>>>>>>> Michael
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>> On Wed, May 15, 2019 at 8:47 PM David Winsemius <
>>>>>>>>>> dwinsemius at comcast.net> wrote:
>>>>>>>>>>>>>> On 5/15/19 4:07 PM, Michael Boulineau wrote:
>>>>>>>>>>>>>>> I have a wild and crazy text file, the head of which looks
>>>>>>> like
>>>>>>>> this:
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> 2016-07-01 02:50:35 <john> hey
>>>>>>>>>>>>>>> 2016-07-01 02:51:26 <jane> waiting for plane to Edinburgh
>>>>>>>>>>>>>>> 2016-07-01 02:51:45 <john> thinking about my boo
>>>>>>>>>>>>>>> 2016-07-01 02:52:07 <jane> nothing crappy has happened, not
>>>>>>>> really
>>>>>>>>>>>>>>> 2016-07-01 02:52:20 <john> plane went by pretty fast,
>>>>>>> didn't
>>>>>>>> sleep
>>>>>>>>>>>>>>> 2016-07-01 02:54:08 <jane> no idea what time it is or where
>>>>>>> I am
>>>>>>>>>> really
>>>>>>>>>>>>>>> 2016-07-01 02:54:17 <john> just know it's london
>>>>>>>>>>>>>>> 2016-07-01 02:56:44 <jane> you are probably asleep
>>>>>>>>>>>>>>> 2016-07-01 02:58:45 <jane> I hope fish was fishy in a good
>>>>>>> eay
>>>>>>>>>>>>>>> 2016-07-01 02:58:56 <jone>
>>>>>>>>>>>>>>> 2016-07-01 02:59:34 <jane>
>>>>>>>>>>>>>>> 2016-07-01 03:02:48 <john> British security is a little
>>>>>>> more
>>>>>>>>>> rigorous...
>>>>>>>>>>>>>> Looks entirely not-"crazy". Typical log file format.
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> Two possibilities: 1) Use `read.fwf` from pkg foreign; 2)
>>>>>>> Use
>>>>>>>> regex
>>>>>>>>>>>>>> (i.e. the sub-function) to strip everything up to the "<".
>>>>>>> Read
>>>>>>>>>>>>>> `?regex`. Since that's not a metacharacters you could use a
>>>>>>>> pattern
>>>>>>>>>>>>>> ".+<" and replace with "".
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> And do read the Posting Guide. Cross-posting to
>>>>>>> StackOverflow and
>>>>>>>>>> Rhelp,
>>>>>>>>>>>>>> at least within hours of each, is considered poor manners.
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> --
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> David.
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> It goes on for a while. It's a big file. But I feel like
>>>>>>> it's
>>>>>>>> going
>>>>>>>>>> to
>>>>>>>>>>>>>>> be difficult to annotate with the coreNLP library or
>>>>>>> package. I'm
>>>>>>>>>>>>>>> doing natural language processing. In other words, I'm
>>>>>>> curious
>>>>>>>> as to
>>>>>>>>>>>>>>> how I would shave off the dates, that is, to make it look
>>>>>>> like:
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> <john> hey
>>>>>>>>>>>>>>> <jane> waiting for plane to Edinburgh
>>>>>>>>>>>>>>>  <john> thinking about my boo
>>>>>>>>>>>>>>> <jane> nothing crappy has happened, not really
>>>>>>>>>>>>>>> <john> plane went by pretty fast, didn't sleep
>>>>>>>>>>>>>>> <jane> no idea what time it is or where I am really
>>>>>>>>>>>>>>> <john> just know it's london
>>>>>>>>>>>>>>> <jane> you are probably asleep
>>>>>>>>>>>>>>> <jane> I hope fish was fishy in a good eay
>>>>>>>>>>>>>>>  <jone>
>>>>>>>>>>>>>>> <jane>
>>>>>>>>>>>>>>> <john> British security is a little more rigorous...
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> To be clear, then, I'm trying to clean a large text file by
>>>>>>>> writing a
>>>>>>>>>>>>>>> regular expression? such that I create a new object with no
>>>>>>>> numbers
>>>>>>>>>> or
>>>>>>>>>>>>>>> dates.
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> Michael
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> ______________________________________________
>>>>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
>>>>>>> more,
>>>>>>>> see
>>>>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>>>>>>> and provide commented, minimal, self-contained,
>>>>>>> reproducible
>>>>>>>> code.
>>>>>>>>>>>>> ______________________________________________
>>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>>>>>> see
>>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>>>> code.
>>>>>>>>>>>> ______________________________________________
>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>>>>>> see
>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>>>> code.
>>>>>>>>>>> ______________________________________________
>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>>>>>> see
>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>>>> code.
>>>>>>>>>> 
>>>>>>>>>> ______________________________________________
>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>>>> code.
>>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>>      [[alternative HTML version deleted]]
>>>>>>>>> 
>>>>>>>>> ______________________________________________
>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>> PLEASE do read the posting guide
>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>>> 
>>>>>>>> ______________________________________________
>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>> PLEASE do read the posting guide
>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>>> 
>>>>>>> 
>>>>>>>    [[alternative HTML version deleted]]
>>>>>>> 
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide
>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>> 
>>>>>> --
>>>>>> Sent from my phone. Please excuse my brevity.
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Sun May 19 14:05:47 2019
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Sun, 19 May 2019 12:05:47 +0000 (UTC)
Subject: [R] Nested structure data simulation
In-Reply-To: <CAPm+3sCQk=qZu9gc2LStZQNRZfoVMrhQ5Wc8Q3OrfK3BGjMmvQ@mail.gmail.com>
References: <1342550043.2326226.1558187826295.ref@mail.yahoo.com>
 <1342550043.2326226.1558187826295@mail.yahoo.com>
 <BBDA4312-A62F-450A-9E9F-655DAF278FF2@utoronto.ca>
 <570252025.3999934.1558209817043@mail.yahoo.com>
 <CAPm+3sCQk=qZu9gc2LStZQNRZfoVMrhQ5Wc8Q3OrfK3BGjMmvQ@mail.gmail.com>
Message-ID: <1560846858.4204873.1558267547377@mail.yahoo.com>

Many thanks to all of you for your responses.

So, I will try to be clearer with a larger example. Te end of my mail is the more important to understand what I am trying to do. I am trying to simulate data to fit a linear mixed model (nested not crossed). More precisely, I would love to get at the end of the process, a table (.txt) with columns and rows. Column 1 and Rows will be the 2000 pupils and the columns the different variables : Column 2 = classes ; Column 3 = teachers, Column 4 = schools ; Column 5 = gender (boy or girl) ; Column 6 = mark in Frecnh

Pupils are nested? in classes, classes are nested in schools. The teacher are part of the process.

I want to simulate a dataset with n=2000 pupils, 100 classes, 50 teachers and 10 schools.
- Pupils n?1 to pupils n?2000 (p1, p2, p3, p4, ..., p2000)
- Classes n?1 to classes n?100 (c1, c2, c3, c4,..., c100)
- Teachers n?1 to teacher n?50 ( t1, t2, t3, t4, ..., t50)
- Schools n?1 to chool n?10 (s1, s2, s3, s4, ..., s10)

The nested structure is as followed : 

-- School 1 with teacher 1 to teacher 5 (t1, t2, t3, t4 and t5) with classes 1 to classes 10 (c1, c2, c3, c4, c5, c6, c7, c8,c9,c10), pupils n?1 to pupils n?200 (p1, p2, p3, p4,..., p200).

-- School 2 with teacher 6 to teacher 10, with classes 11 to classes 20, pupils n?201 to pupils n?400

-- and so on

The table (.txt) I would love to get at the end is the following :

??????? Class????Teacher??? School??? gender??? Mark
1?????? c1??????? t1??????????????? s1??????????? boy ?????? 5
2?????? c1????????t1????????????????s1????????????boy????????5.5
3?????? c1????????t1????????????????s1????????????girl????????4.5
4?????? c1????????t1????????????????s1????????????girl????????6
5?????? c1????????t1????????????????s1????????????boy?????? 3.5
6?????? ...??????? ....??????????????? ....??????????? .....??????? ..... ????????????? 

The first 20 rows with c1, with t1, with s1, gender (randomly slected) and mark (andomly selected) from 1 to 6
The rows 21 to 40 with c2 with t1 with s1
The rows 41 to 60 with c3 with t2 with s1
The rows 61 to 80 with c4 with t2 with s1
The rows 81 to 100 with c5 with t3 with s1
The rows 101 to 120 with c6 with t3 with s1
The rows 121 to 140 with c7 with t4 with s1
The rows 141 to 160 with c8 with t4 with s1
The rows 161 to 180 with c9 with t5 with s1
The rows 181 to 200 with c10 with t5 with s1

The rows 201 to 220 with c11 with t6 with s2
The rows 221 to 240 with c12 with t6 with s2

And so on...

Is it possible to do that ? Or am I dreaming ?


Le dimanche 19 mai 2019 ? 10:45:43 UTC+2, Linus Chen <linus.l.chen at gmail.com> a ?crit : 





Dear varin sacha,

I think it will help us help you, if you give a clearer description of
what exactly you want.

I assume the situation is that you know what a data structure you
want, but do not know
how to conveniently create such structure.
And that is where others can help you.
So, please, describe the wanted data structure more thoroughly,
ideally with example.

Thanks,
Lei

On Sat, May 18, 2019 at 10:04 PM varin sacha via R-help
<r-help at r-project.org> wrote:
>
> Dear Boris,
>
> Yes, top-down, no problem. Many thanks, but in your code did you not forget "teacher" ? As a reminder teacher has to be nested with classes. I mean the 50 pupils belonging to C1 must be with (teacher 1) T1, the 50 pupils belonging to C2 with T2, the 50 pupils belonging to C3 with T3 and so on.
>
> Best,
>
>
> Le samedi 18 mai 2019 ? 16:52:48 UTC+2, Boris Steipe <boris.steipe at utoronto.ca> a ?crit :
>
>
>
>
>
> Can you build your data top-down?
>
>
>
> schools <- paste("s", 1:6, sep="")
>
> classes <- character()
> for (school in schools) {
>? classes <- c(classes, paste(school, paste("c", 1:5, sep=""), sep = "."))
> }
>
> pupils <- character()
> for (class in classes) {
>? pupils <- c(pupils, paste(class, paste("p", 1:10, sep=""), sep = "."))
> }
>
>
>
> B.
>
>
>
> > On 2019-05-18, at 09:57, varin sacha via R-help <r-help at r-project.org> wrote:
> >
> > Dear R-Experts,
> >
> > In a data simulation, I would like a balanced distribution with a nested structure for classroom and teacher (not for school). I mean 50 pupils belonging to C1, 50 other pupils belonging to C2, 50 other pupils belonging to C3 and so on. Then I want the 50 pupils belonging to C1 with T1, the 50 pupils belonging to C2 with T2, the 50 pupils belonging to C3 with T3 and so on. The school don?t have to be nested, I just want a balanced distribution, I mean 60 pupils in S1, 60 other pupils in S2 and so on.
> > Here below the reproducible example.
> > Many thanks for your help.
> >
> > ##############
> > set.seed(123)
> > # G?n?ration al?atoire des colonnes
> > pupils<-1:300
> > classroom<-sample(c("C1","C2","C3","C4","C5","C6"),300,replace=T)? teacher<-sample(c("T1","T2","T3","T4","T5","T6"),300,replace=T)? school<-sample(c("S1","S2","S3","S4","S5"),300,replace=T)
>
> > ##############
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.

>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bor|@@@te|pe @end|ng |rom utoronto@c@  Sun May 19 15:26:36 2019
From: bor|@@@te|pe @end|ng |rom utoronto@c@ (Boris Steipe)
Date: Sun, 19 May 2019 13:26:36 +0000
Subject: [R] Nested structure data simulation
In-Reply-To: <1560846858.4204873.1558267547377@mail.yahoo.com>
References: <1342550043.2326226.1558187826295.ref@mail.yahoo.com>
 <1342550043.2326226.1558187826295@mail.yahoo.com>
 <BBDA4312-A62F-450A-9E9F-655DAF278FF2@utoronto.ca>
 <570252025.3999934.1558209817043@mail.yahoo.com>
 <CAPm+3sCQk=qZu9gc2LStZQNRZfoVMrhQ5Wc8Q3OrfK3BGjMmvQ@mail.gmail.com>
 <1560846858.4204873.1558267547377@mail.yahoo.com>
Message-ID: <5F8F8017-2E90-4DE5-898F-492C6CC57E1A@utoronto.ca>

Fair enough - there are additional assumptions needed, which I make as follows:
  - each class has the same size
  - each teacher teaches the same number of classes
  - the number of boys and girls is random within a class
  - there are 60% girls   (just for illustration that it does not have to be equal)
  

To make the dependencies explicit, I define them so, and in a way that they can't be inconsistent.

nS <- 10        # Schools
nTpS <- 5       # Teachers per School
nCpT <- 2       # Classes per teacher
nPpC <- 20      # Pupils per class
nS * nTpS * nCpT * nPpC == 2000   # Validate


mySim <- data.frame(School  = paste0("s", rep(1:nS, each = nTpS*nCpT*nPpC)),
                    Teacher = paste0("t", rep(1:(nTpS*nS), each = nCpT*nPpC)),
                    Class   = paste0("c", rep(1:(nCpT*nTpS*nS), each = nPpC)),
                    Gender  = sample(c("boy", "girl"),
                                     (nS*nTpS*nCpT*nPpC),
                                     prob = c(0.4, 0.6),
                                     replace = TRUE),
                    Mark    = numeric(nS*nTpS*nCpT*nPpC),
                    stringsAsFactors = FALSE)
                    

Then you fill mySim$Mark with values from your linear mixed model ...

mySim$Mark[i] <- simMarks(mySim[i])  # ... or something equivalent.


All good?

Cheers,
Boris



> On 2019-05-19, at 08:05, varin sacha <varinsacha at yahoo.fr> wrote:
> 
> Many thanks to all of you for your responses.
> 
> So, I will try to be clearer with a larger example. Te end of my mail is the more important to understand what I am trying to do. I am trying to simulate data to fit a linear mixed model (nested not crossed). More precisely, I would love to get at the end of the process, a table (.txt) with columns and rows. Column 1 and Rows will be the 2000 pupils and the columns the different variables : Column 2 = classes ; Column 3 = teachers, Column 4 = schools ; Column 5 = gender (boy or girl) ; Column 6 = mark in Frecnh
> 
> Pupils are nested  in classes, classes are nested in schools. The teacher are part of the process.
> 
> I want to simulate a dataset with n=2000 pupils, 100 classes, 50 teachers and 10 schools.
> - Pupils n?1 to pupils n?2000 (p1, p2, p3, p4, ..., p2000)
> - Classes n?1 to classes n?100 (c1, c2, c3, c4,..., c100)
> - Teachers n?1 to teacher n?50 ( t1, t2, t3, t4, ..., t50)
> - Schools n?1 to chool n?10 (s1, s2, s3, s4, ..., s10)
> 
> The nested structure is as followed : 
> 
> -- School 1 with teacher 1 to teacher 5 (t1, t2, t3, t4 and t5) with classes 1 to classes 10 (c1, c2, c3, c4, c5, c6, c7, c8,c9,c10), pupils n?1 to pupils n?200 (p1, p2, p3, p4,..., p200).
> 
> -- School 2 with teacher 6 to teacher 10, with classes 11 to classes 20, pupils n?201 to pupils n?400
> 
> -- and so on
> 
> The table (.txt) I would love to get at the end is the following :
> 
>         Class    Teacher    School    gender    Mark
> 1       c1        t1                s1            boy        5
> 2       c1        t1                s1            boy        5.5
> 3       c1        t1                s1            girl        4.5
> 4       c1        t1                s1            girl        6
> 5       c1        t1                s1            boy       3.5
> 6       ...        ....                ....            .....        .....               
> 
> The first 20 rows with c1, with t1, with s1, gender (randomly slected) and mark (andomly selected) from 1 to 6
> The rows 21 to 40 with c2 with t1 with s1
> The rows 41 to 60 with c3 with t2 with s1
> The rows 61 to 80 with c4 with t2 with s1
> The rows 81 to 100 with c5 with t3 with s1
> The rows 101 to 120 with c6 with t3 with s1
> The rows 121 to 140 with c7 with t4 with s1
> The rows 141 to 160 with c8 with t4 with s1
> The rows 161 to 180 with c9 with t5 with s1
> The rows 181 to 200 with c10 with t5 with s1
> 
> The rows 201 to 220 with c11 with t6 with s2
> The rows 221 to 240 with c12 with t6 with s2
> 
> And so on...
> 
> Is it possible to do that ? Or am I dreaming ?
> 
> 
> Le dimanche 19 mai 2019 ? 10:45:43 UTC+2, Linus Chen <linus.l.chen at gmail.com> a ?crit : 
> 
> 
> 
> 
> 
> Dear varin sacha,
> 
> I think it will help us help you, if you give a clearer description of
> what exactly you want.
> 
> I assume the situation is that you know what a data structure you
> want, but do not know
> how to conveniently create such structure.
> And that is where others can help you.
> So, please, describe the wanted data structure more thoroughly,
> ideally with example.
> 
> Thanks,
> Lei
> 
> On Sat, May 18, 2019 at 10:04 PM varin sacha via R-help
> <r-help at r-project.org> wrote:
>> 
>> Dear Boris,
>> 
>> Yes, top-down, no problem. Many thanks, but in your code did you not forget "teacher" ? As a reminder teacher has to be nested with classes. I mean the 50 pupils belonging to C1 must be with (teacher 1) T1, the 50 pupils belonging to C2 with T2, the 50 pupils belonging to C3 with T3 and so on.
>> 
>> Best,
>> 
>> 
>> Le samedi 18 mai 2019 ? 16:52:48 UTC+2, Boris Steipe <boris.steipe at utoronto.ca> a ?crit :
>> 
>> 
>> 
>> 
>> 
>> Can you build your data top-down?
>> 
>> 
>> 
>> schools <- paste("s", 1:6, sep="")
>> 
>> classes <- character()
>> for (school in schools) {
>>   classes <- c(classes, paste(school, paste("c", 1:5, sep=""), sep = "."))
>> }
>> 
>> pupils <- character()
>> for (class in classes) {
>>   pupils <- c(pupils, paste(class, paste("p", 1:10, sep=""), sep = "."))
>> }
>> 
>> 
>> 
>> B.
>> 
>> 
>> 
>>> On 2019-05-18, at 09:57, varin sacha via R-help <r-help at r-project.org> wrote:
>>> 
>>> Dear R-Experts,
>>> 
>>> In a data simulation, I would like a balanced distribution with a nested structure for classroom and teacher (not for school). I mean 50 pupils belonging to C1, 50 other pupils belonging to C2, 50 other pupils belonging to C3 and so on. Then I want the 50 pupils belonging to C1 with T1, the 50 pupils belonging to C2 with T2, the 50 pupils belonging to C3 with T3 and so on. The school don?t have to be nested, I just want a balanced distribution, I mean 60 pupils in S1, 60 other pupils in S2 and so on.
>>> Here below the reproducible example.
>>> Many thanks for your help.
>>> 
>>> ##############
>>> set.seed(123)
>>> # G?n?ration al?atoire des colonnes
>>> pupils<-1:300
>>> classroom<-sample(c("C1","C2","C3","C4","C5","C6"),300,replace=T)  teacher<-sample(c("T1","T2","T3","T4","T5","T6"),300,replace=T)  school<-sample(c("S1","S2","S3","S4","S5"),300,replace=T)
>> 
>>> ##############
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> 
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Sun May 19 17:14:02 2019
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Sun, 19 May 2019 15:14:02 +0000 (UTC)
Subject: [R] Nested structure data simulation
In-Reply-To: <5F8F8017-2E90-4DE5-898F-492C6CC57E1A@utoronto.ca>
References: <1342550043.2326226.1558187826295.ref@mail.yahoo.com>
 <1342550043.2326226.1558187826295@mail.yahoo.com>
 <BBDA4312-A62F-450A-9E9F-655DAF278FF2@utoronto.ca>
 <570252025.3999934.1558209817043@mail.yahoo.com>
 <CAPm+3sCQk=qZu9gc2LStZQNRZfoVMrhQ5Wc8Q3OrfK3BGjMmvQ@mail.gmail.com>
 <1560846858.4204873.1558267547377@mail.yahoo.com>
 <5F8F8017-2E90-4DE5-898F-492C6CC57E1A@utoronto.ca>
Message-ID: <370367748.4299867.1558278842667@mail.yahoo.com>

Dear Boris,

Great !!!! But what about Mark in your R code ? Don't we have to precise in the R code that mark ranges between 1 to 6 (1 ; 1.5 ; 2 ; 2.5 ; 3 ; 3.5 ; 4 ; 4.5 ; 5 ; 5.5 ; 6) ?

By the way, to fit a linear mixed model, I use lme4 package and then the lmer function works with the variables like in this example here below :

library(lme4)
mm=lmer(Mark ~Gender + (1 | School / Class), data=Dataset) 

With your R code, how can I write the lmer function to make it work ?

Best,
S.







Le dimanche 19 mai 2019 ? 15:26:39 UTC+2, Boris Steipe <boris.steipe at utoronto.ca> a ?crit : 





Fair enough - there are additional assumptions needed, which I make as follows:
? - each class has the same size
? - each teacher teaches the same number of classes
? - the number of boys and girls is random within a class
? - there are 60% girls? (just for illustration that it does not have to be equal)
? 

To make the dependencies explicit, I define them so, and in a way that they can't be inconsistent.

nS <- 10? ? ? ? # Schools
nTpS <- 5? ? ? # Teachers per School
nCpT <- 2? ? ? # Classes per teacher
nPpC <- 20? ? ? # Pupils per class
nS * nTpS * nCpT * nPpC == 2000? # Validate


mySim <- data.frame(School? = paste0("s", rep(1:nS, each = nTpS*nCpT*nPpC)),
? ? ? ? ? ? ? ? ? ? Teacher = paste0("t", rep(1:(nTpS*nS), each = nCpT*nPpC)),
? ? ? ? ? ? ? ? ? ? Class? = paste0("c", rep(1:(nCpT*nTpS*nS), each = nPpC)),
? ? ? ? ? ? ? ? ? ? Gender? = sample(c("boy", "girl"),
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (nS*nTpS*nCpT*nPpC),
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? prob = c(0.4, 0.6),
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? replace = TRUE),
? ? ? ? ? ? ? ? ? ? Mark? ? = numeric(nS*nTpS*nCpT*nPpC),
? ? ? ? ? ? ? ? ? ? stringsAsFactors = FALSE)
? ? ? ? ? ? ? ? ? ? 

Then you fill mySim$Mark with values from your linear mixed model ...

mySim$Mark[i] <- simMarks(mySim[i])? # ... or something equivalent.


All good?

Cheers,
Boris



> On 2019-05-19, at 08:05, varin sacha <varinsacha at yahoo.fr> wrote:
> 
> Many thanks to all of you for your responses.
> 
> So, I will try to be clearer with a larger example. Te end of my mail is the more important to understand what I am trying to do. I am trying to simulate data to fit a linear mixed model (nested not crossed). More precisely, I would love to get at the end of the process, a table (.txt) with columns and rows. Column 1 and Rows will be the 2000 pupils and the columns the different variables : Column 2 = classes ; Column 3 = teachers, Column 4 = schools ; Column 5 = gender (boy or girl) ; Column 6 = mark in Frecnh
> 
> Pupils are nested? in classes, classes are nested in schools. The teacher are part of the process.
> 
> I want to simulate a dataset with n=2000 pupils, 100 classes, 50 teachers and 10 schools.
> - Pupils n?1 to pupils n?2000 (p1, p2, p3, p4, ..., p2000)
> - Classes n?1 to classes n?100 (c1, c2, c3, c4,..., c100)
> - Teachers n?1 to teacher n?50 ( t1, t2, t3, t4, ..., t50)
> - Schools n?1 to chool n?10 (s1, s2, s3, s4, ..., s10)
> 
> The nested structure is as followed : 
> 
> -- School 1 with teacher 1 to teacher 5 (t1, t2, t3, t4 and t5) with classes 1 to classes 10 (c1, c2, c3, c4, c5, c6, c7, c8,c9,c10), pupils n?1 to pupils n?200 (p1, p2, p3, p4,..., p200).
> 
> -- School 2 with teacher 6 to teacher 10, with classes 11 to classes 20, pupils n?201 to pupils n?400
> 
> -- and so on
> 
> The table (.txt) I would love to get at the end is the following :
> 
>? ? ? ? Class? ? Teacher? ? School? ? gender? ? Mark
> 1? ? ? c1? ? ? ? t1? ? ? ? ? ? ? ? s1? ? ? ? ? ? boy? ? ? ? 5
> 2? ? ? c1? ? ? ? t1? ? ? ? ? ? ? ? s1? ? ? ? ? ? boy? ? ? ? 5.5
> 3? ? ? c1? ? ? ? t1? ? ? ? ? ? ? ? s1? ? ? ? ? ? girl? ? ? ? 4.5
> 4? ? ? c1? ? ? ? t1? ? ? ? ? ? ? ? s1? ? ? ? ? ? girl? ? ? ? 6
> 5? ? ? c1? ? ? ? t1? ? ? ? ? ? ? ? s1? ? ? ? ? ? boy? ? ? 3.5
> 6? ? ? ...? ? ? ? ....? ? ? ? ? ? ? ? ....? ? ? ? ? ? .....? ? ? ? .....? ? ? ? ? ? ? 
> 
> The first 20 rows with c1, with t1, with s1, gender (randomly slected) and mark (andomly selected) from 1 to 6
> The rows 21 to 40 with c2 with t1 with s1
> The rows 41 to 60 with c3 with t2 with s1
> The rows 61 to 80 with c4 with t2 with s1
> The rows 81 to 100 with c5 with t3 with s1
> The rows 101 to 120 with c6 with t3 with s1
> The rows 121 to 140 with c7 with t4 with s1
> The rows 141 to 160 with c8 with t4 with s1
> The rows 161 to 180 with c9 with t5 with s1
> The rows 181 to 200 with c10 with t5 with s1
> 
> The rows 201 to 220 with c11 with t6 with s2
> The rows 221 to 240 with c12 with t6 with s2
> 
> And so on...
> 
> Is it possible to do that ? Or am I dreaming ?
> 
> 
> Le dimanche 19 mai 2019 ? 10:45:43 UTC+2, Linus Chen <linus.l.chen at gmail.com> a ?crit : 
> 
> 
> 
> 
> 
> Dear varin sacha,
> 
> I think it will help us help you, if you give a clearer description of
> what exactly you want.
> 
> I assume the situation is that you know what a data structure you
> want, but do not know
> how to conveniently create such structure.
> And that is where others can help you.
> So, please, describe the wanted data structure more thoroughly,
> ideally with example.
> 
> Thanks,
> Lei
> 
> On Sat, May 18, 2019 at 10:04 PM varin sacha via R-help
> <r-help at r-project.org> wrote:
>> 
>> Dear Boris,
>> 
>> Yes, top-down, no problem. Many thanks, but in your code did you not forget "teacher" ? As a reminder teacher has to be nested with classes. I mean the 50 pupils belonging to C1 must be with (teacher 1) T1, the 50 pupils belonging to C2 with T2, the 50 pupils belonging to C3 with T3 and so on.
>> 
>> Best,
>> 
>> 
>> Le samedi 18 mai 2019 ? 16:52:48 UTC+2, Boris Steipe <boris.steipe at utoronto.ca> a ?crit :
>> 
>> 
>> 
>> 
>> 
>> Can you build your data top-down?
>> 
>> 
>> 
>> schools <- paste("s", 1:6, sep="")
>> 
>> classes <- character()
>> for (school in schools) {
>>? classes <- c(classes, paste(school, paste("c", 1:5, sep=""), sep = "."))
>> }
>> 
>> pupils <- character()
>> for (class in classes) {
>>? pupils <- c(pupils, paste(class, paste("p", 1:10, sep=""), sep = "."))
>> }
>> 
>> 
>> 
>> B.
>> 
>> 
>> 
>>> On 2019-05-18, at 09:57, varin sacha via R-help <r-help at r-project.org> wrote:
>>> 
>>> Dear R-Experts,
>>> 
>>> In a data simulation, I would like a balanced distribution with a nested structure for classroom and teacher (not for school). I mean 50 pupils belonging to C1, 50 other pupils belonging to C2, 50 other pupils belonging to C3 and so on. Then I want the 50 pupils belonging to C1 with T1, the 50 pupils belonging to C2 with T2, the 50 pupils belonging to C3 with T3 and so on. The school don?t have to be nested, I just want a balanced distribution, I mean 60 pupils in S1, 60 other pupils in S2 and so on.
>>> Here below the reproducible example.
>>> Many thanks for your help.
>>> 
>>> ##############
>>> set.seed(123)
>>> # G?n?ration al?atoire des colonnes
>>> pupils<-1:300
>>> classroom<-sample(c("C1","C2","C3","C4","C5","C6"),300,replace=T)? teacher<-sample(c("T1","T2","T3","T4","T5","T6"),300,replace=T)? school<-sample(c("S1","S2","S3","S4","S5"),300,replace=T)
>> 
>>> ##############
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> 
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From m|ch@e|@p@bou||ne@u @end|ng |rom gm@||@com  Sun May 19 19:56:19 2019
From: m|ch@e|@p@bou||ne@u @end|ng |rom gm@||@com (Michael Boulineau)
Date: Sun, 19 May 2019 10:56:19 -0700
Subject: [R] how to separate string from numbers in a large txt file
In-Reply-To: <C5A52A04-CB49-442F-81CE-26CBC619C47A@utoronto.ca>
References: <CAH+cTGM9sFCxegaMHzu-_9Prhvvh9MbVDCWJ1_x7wkO9jC47mQ@mail.gmail.com>
 <c6e69644-66a1-7031-4d7c-1000e0abbea4@comcast.net>
 <CAH+cTGM5YvWV_G0pRgMqjCzuRrFTzd0nOc7yoThfGbGMUyMjsQ@mail.gmail.com>
 <9c9f1c81-309f-91cb-7e26-f02fc4bd45d8@comcast.net>
 <CAH+cTGMx4_Lz2gGDf2AuUaaZF2Xaq4McuPRoHu4JJNc-ORd=Ew@mail.gmail.com>
 <6322e735-5659-848e-3a50-9f066f1299ad@comcast.net>
 <CAF8bMcbgrVcN9J7Aygq2D5RWaU4MS-GH+_+zMHU=AO+DU5DVWw@mail.gmail.com>
 <CAH+cTGNfpLSurqmsoMeci0GWhC0ncOp7zTEju3fJW5T4+tCsGA@mail.gmail.com>
 <CAF8bMcaSJRaUXK4h2ssaDoJwXaKCUZT2Vq1cutNiNDjQ7ga2Xg@mail.gmail.com>
 <EEA9FD46-A67C-4FB7-8245-350819C4EA62@dcn.davis.ca.us>
 <CAH+cTGNOkAnPnn-aru-BJUbhrgeYMQBTqjSYT764nKzcdXvTvA@mail.gmail.com>
 <9B170703-AAAF-4D52-9744-1FDE70559B05@utoronto.ca>
 <CAH+cTGNaw7p-it4B7te4BBsay=v6J-5OS-P_WGDJJ6Z0ymxtvw@mail.gmail.com>
 <0F8CD89C-BDE2-44D0-8E18-0CA2706EB53F@utoronto.ca>
 <CAH+cTGMsD+PkTKoFfB8=3WeWPQAWjhmuOCKZfnB7jR03m7RtDQ@mail.gmail.com>
 <C5A52A04-CB49-442F-81CE-26CBC619C47A@utoronto.ca>
Message-ID: <CAH+cTGOijsRxzJy3w9QS11axwD+Z36-Pive6dOhm2xK6Jm5PAg@mail.gmail.com>

> b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)"

so the ^ signals that the regex BEGINS with a number (that could be
any number, 0-9) that is only 10 characters long (then there's the
dash in there, too, with the 0-9-, which I assume enabled the regex to
grab the - that's between the numbers in the date), followed by a
single space, followed by a unit that could be any number, again, but
that is only 8 characters long this time. For that one, it will
include the colon, hence the 9:, although for that one ([0-9:]{8} ), I
don't get why the space is on the inside in that one, after the {8},
whereas the space is on the outside with the other one ^([0-9-]{10} ,
directly after the {10}. Why is that?

Then three *** [*]{3}, then the (\\w+ \\w+)", which Boris explained so
well above. I guess I still don't get why this one seemed to have
deleted the *** out of the mix, plus I still don't why it didn't
remove the *** from the first one.

2016-03-20 19:29:37 *** Jane Doe started a video chat
2016-03-20 19:30:35 *** John Doe ended a video chat
2016-04-02 12:59:36 *** Jane Doe started a video chat
2016-04-02 13:00:43 *** John Doe ended a video chat
2016-04-02 13:01:08 *** Jane Doe started a video chat
2016-04-02 13:01:41 *** John Doe ended a video chat
2016-04-02 13:03:51 *** John Doe started a video chat
2016-04-02 13:06:35 *** John Doe ended a video chat

This is a random sample from the beginning of the txt file with no
edits. The ***s were deleted, all but the first one, the one that had
the ??? but that was taken out by the encoding = "UTF-8". I know that
the function was c <- gsub(b, "\\1<\\2> ", a), so it had a gsub () on
there, the point of which is to do substitution work.

Oh, I get it, I think. The \\1<\\2> in the gsub () puts the <> around
the names, so that it's consistent with the rest of the data, so that
the names in the text about that aren't enclosed in the <> are
enclosed like the rest of them. But I still don't get why or how the
gsub () replaced the *** with the <>...

This one is more straightforward.

> d <- "^([0-9-]{10}) ([0-9:]{8}) <(\\w+ \\w+)>\\s*(.+)$"

any number with - for 10 characters, followed by a space. Oh, there's
no space in this one ([0-9:]{8}), after the {8}. Hu. So, then, any
number with : for 8 characters, followed by any two words separated by
a space and enclosed in <>. And then the \\s* is followed by a single
space? Or maybe it puts space on both sides (on the side of the #s to
the left, and then the comment to the right). The (.+)$ is anything
whatsoever until the end.

Michael


On Sun, May 19, 2019 at 4:37 AM Boris Steipe <boris.steipe at utoronto.ca> wrote:
>
> Inline
>
>
>
> > On 2019-05-18, at 20:34, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
> >
> > It appears to have worked, although there were three little quirks.
> > The ; close(con); rm(con) didn't work for me; the first row of the
> > data.frame was all NAs, when all was said and done;
>
> You will get NAs for lines that can't be matched to the regular expression. That's a good thing, it allows you to test whether your assumptions were valid for the entire file:
>
> # number of failed strcapture()
> sum(is.na(e$date))
>
>
> > and then there
> > were still three *** on the same line where the ??? was apparently
> > deleted.
>
> This is a sign that something else happened with the line that prevented the regex from matching. In that case you need to investigate more. I see an invalid multibyte character at the beginning of the line you posted below.
>
> >
> >> a <- readLines ("hangouts-conversation-6.txt", encoding = "UTF-8")
> >> b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)"
> >> c <- gsub(b, "\\1<\\2> ", a)
> >> head (c)
> > [1] "?2016-01-27 09:14:40 *** Jane Doe started a video chat"
> > [2] "2016-01-27 09:15:20 <Jane Doe>
> > https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf"
>
> [...]
>
> > But, before I do anything else, I'm going to study the regex in this
> > particular code. For example, I'm still not sure why there has to the
> > second \\w+ in the (\\w+ \\w+). Little things like that.
>
> \w is the metacharacter for alphanumeric characters, \w+ designates something we could call a word. Thus \w+ \w+ are two words separated by a single blank. This corresponds to your example, but, as I wrote previously, you need to think very carefully whether this covers all possible cases (Could there be only one word? More than one blank? Could letters be separated by hyphens or periods?) In most cases we could have more robustly matched everything between "<" and ">" (taking care to test what happens if the message contains those characters). But for the video chat lines we need to make an assumption about what is name and what is not. If "started a video chat" is the only possibility in such lines, you can use this information instead. If there are other possibilities, you need a different strategy. In NLP there is no one-approach-fits-all.
>
> To validate the structure of the names in your transcripts, you can look at
>
> patt <- " <.+?> "   # " <any string, not greedy> "
> m <- regexpr(patt, c)
> unique(regmatches(c, m))
>
>
>
> B.
>
>
>
> >
> > Michael
> >
> >
> > On Sat, May 18, 2019 at 4:30 PM Boris Steipe <boris.steipe at utoronto.ca> wrote:
> >>
> >> This works for me:
> >>
> >> # sample data
> >> c <- character()
> >> c[1] <- "2016-01-27 09:14:40 <Jane Doe> started a video chat"
> >> c[2] <- "2016-01-27 09:15:20 <Jane Doe> https://lh3.googleusercontent.com/"
> >> c[3] <- "2016-01-27 09:15:20 <Jane Doe> Hey "
> >> c[4] <- "2016-01-27 09:15:22 <John Doe>  ended a video chat"
> >> c[5] <- "2016-01-27 21:07:11 <Jane Doe>  started a video chat"
> >> c[6] <- "2016-01-27 21:26:57 <John Doe>  ended a video chat"
> >>
> >>
> >> # regex  ^(year)       (time)      <(word word)>\\s*(string)$
> >> patt <- "^([0-9-]{10}) ([0-9:]{8}) <(\\w+ \\w+)>\\s*(.+)$"
> >> proto <- data.frame(date = character(),
> >>                    time = character(),
> >>                    name = character(),
> >>                    text = character(),
> >>                    stringsAsFactors = TRUE)
> >> d <- strcapture(patt, c, proto)
> >>
> >>
> >>
> >>        date     time     name                               text
> >> 1 2016-01-27 09:14:40 Jane Doe               started a video chat
> >> 2 2016-01-27 09:15:20 Jane Doe https://lh3.googleusercontent.com/
> >> 3 2016-01-27 09:15:20 Jane Doe                               Hey
> >> 4 2016-01-27 09:15:22 John Doe                 ended a video chat
> >> 5 2016-01-27 21:07:11 Jane Doe               started a video chat
> >> 6 2016-01-27 21:26:57 John Doe                 ended a video chat
> >>
> >>
> >>
> >> B.
> >>
> >>
> >>> On 2019-05-18, at 18:32, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
> >>>
> >>> Going back and thinking through what Boris and William were saying
> >>> (also Ivan), I tried this:
> >>>
> >>> a <- readLines ("hangouts-conversation-6.csv.txt")
> >>> b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)"
> >>> c <- gsub(b, "\\1<\\2> ", a)
> >>>> head (c)
> >>> [1] "???2016-01-27 09:14:40 *** Jane Doe started a video chat"
> >>> [2] "2016-01-27 09:15:20 <Jane Doe>
> >>> https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf"
> >>> [3] "2016-01-27 09:15:20 <Jane Doe> Hey "
> >>> [4] "2016-01-27 09:15:22 <John Doe>  ended a video chat"
> >>> [5] "2016-01-27 21:07:11 <Jane Doe>  started a video chat"
> >>> [6] "2016-01-27 21:26:57 <John Doe>  ended a video chat"
> >>>
> >>> The ??? is still there, since I forgot to do what Ivan had suggested, namely,
> >>>
> >>> a <- readLines(con <- file("hangouts-conversation-6.csv.txt", encoding
> >>> = "UTF-8")); close(con); rm(con)
> >>>
> >>> But then the new code is still turning out only NAs when I apply
> >>> strcapture (). This was what happened next:
> >>>
> >>>> d <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
> >>> + [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
> >>> +                 c, proto=data.frame(stringsAsFactors=FALSE, When="", Who="",
> >>> +                                     What=""))
> >>>> head (d)
> >>> When  Who What
> >>> 1 <NA> <NA> <NA>
> >>> 2 <NA> <NA> <NA>
> >>> 3 <NA> <NA> <NA>
> >>> 4 <NA> <NA> <NA>
> >>> 5 <NA> <NA> <NA>
> >>> 6 <NA> <NA> <NA>
> >>>
> >>> I've been reading up on regular expressions, too, so this code seems
> >>> spot on. What's going wrong?
> >>>
> >>> Michael
> >>>
> >>> On Fri, May 17, 2019 at 4:28 PM Boris Steipe <boris.steipe at utoronto.ca> wrote:
> >>>>
> >>>> Don't start putting in extra commas and then reading this as csv. That approach is broken. The correct approach is what Bill outlined: read everything with readLines(), and then use a proper regular expression with strcapture().
> >>>>
> >>>> You need to pre-process the object that readLines() gives you: replace the contents of the videochat lines, and make it conform to the format of the other lines before you process it into your data frame.
> >>>>
> >>>> Approximately something like
> >>>>
> >>>> # read the raw data
> >>>> tmp <- readLines("hangouts-conversation-6.csv.txt")
> >>>>
> >>>> # process all video chat lines
> >>>> patt <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+) "  # (year time )*** (word word)
> >>>> tmp <- gsub(patt, "\\1<\\2> ", tmp)
> >>>>
> >>>> # next, use strcapture()
> >>>>
> >>>> Note that this makes the assumption that your names are always exactly two words containing only letters. If that assumption is not true, more though needs to go into the regex. But you can test that:
> >>>>
> >>>> patt <- " <\\w+ \\w+> "   #" <word word> "
> >>>> sum( ! grepl(patt, tmp)))
> >>>>
> >>>> ... will give the number of lines that remain in your file that do not have a tag that can be interpreted as "Who"
> >>>>
> >>>> Once that is fine, use Bill's approach - or a regular expression of your own design - to create your data frame.
> >>>>
> >>>> Hope this helps,
> >>>> Boris
> >>>>
> >>>>
> >>>>
> >>>>
> >>>>> On 2019-05-17, at 16:18, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
> >>>>>
> >>>>> Very interesting. I'm sure I'll be trying to get rid of the byte order
> >>>>> mark eventually. But right now, I'm more worried about getting the
> >>>>> character vector into either a csv file or data.frame; that way, I can
> >>>>> be able to work with the data neatly tabulated into four columns:
> >>>>> date, time, person, comment. I assume it's a write.csv function, but I
> >>>>> don't know what arguments to put in it. header=FALSE? fill=T?
> >>>>>
> >>>>> Micheal
> >>>>>
> >>>>> On Fri, May 17, 2019 at 1:03 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> >>>>>>
> >>>>>> If byte order mark is the issue then you can specify the file encoding as "UTF-8-BOM" and it won't show up in your data any more.
> >>>>>>
> >>>>>> On May 17, 2019 12:12:17 PM PDT, William Dunlap via R-help <r-help at r-project.org> wrote:
> >>>>>>> The pattern I gave worked for the lines that you originally showed from
> >>>>>>> the
> >>>>>>> data file ('a'), before you put commas into them.  If the name is
> >>>>>>> either of
> >>>>>>> the form "<name>" or "***" then the "(<[^>]*>)" needs to be changed so
> >>>>>>> something like "(<[^>]*>|[*]{3})".
> >>>>>>>
> >>>>>>> The " ???" at the start of the imported data may come from the byte
> >>>>>>> order
> >>>>>>> mark that Windows apps like to put at the front of a text file in UTF-8
> >>>>>>> or
> >>>>>>> UTF-16 format.
> >>>>>>>
> >>>>>>> Bill Dunlap
> >>>>>>> TIBCO Software
> >>>>>>> wdunlap tibco.com
> >>>>>>>
> >>>>>>>
> >>>>>>> On Fri, May 17, 2019 at 11:53 AM Michael Boulineau <
> >>>>>>> michael.p.boulineau at gmail.com> wrote:
> >>>>>>>
> >>>>>>>> This seemed to work:
> >>>>>>>>
> >>>>>>>>> a <- readLines ("hangouts-conversation-6.csv.txt")
> >>>>>>>>> b <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4", a)
> >>>>>>>>> b [1:84]
> >>>>>>>>
> >>>>>>>> And the first 85 lines looks like this:
> >>>>>>>>
> >>>>>>>> [83] "2016-06-28 21:02:28 *** Jane Doe started a video chat"
> >>>>>>>> [84] "2016-06-28 21:12:43 *** John Doe ended a video chat"
> >>>>>>>>
> >>>>>>>> Then they transition to the commas:
> >>>>>>>>
> >>>>>>>>> b [84:100]
> >>>>>>>> [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
> >>>>>>>> [2] "2016-07-01,02:50:35,<John Doe>,hey"
> >>>>>>>> [3] "2016-07-01,02:51:26,<John Doe>,waiting for plane to Edinburgh"
> >>>>>>>> [4] "2016-07-01,02:51:45,<John Doe>,thinking about my boo"
> >>>>>>>>
> >>>>>>>> Even the strange bit on line 6347 was caught by this:
> >>>>>>>>
> >>>>>>>>> b [6346:6348]
> >>>>>>>> [1] "2016-10-21,10:56:29,<John Doe>,John_Doe"
> >>>>>>>> [2] "2016-10-21,10:56:37,<John Doe>,Admit#8242"
> >>>>>>>> [3] "2016-10-21,11:00:13,<Jane Doe>,Okay so you have a discussion"
> >>>>>>>>
> >>>>>>>> Perhaps most awesomely, the code catches spaces that are interposed
> >>>>>>>> into the comment itself:
> >>>>>>>>
> >>>>>>>>> b [4]
> >>>>>>>> [1] "2016-01-27,09:15:20,<Jane Doe>,Hey "
> >>>>>>>>> b [85]
> >>>>>>>> [1] "2016-07-01,02:50:35,<John Doe>,hey"
> >>>>>>>>
> >>>>>>>> Notice whether there is a space after the "hey" or not.
> >>>>>>>>
> >>>>>>>> These are the first two lines:
> >>>>>>>>
> >>>>>>>> [1] "???2016-01-27 09:14:40 *** Jane Doe started a video chat"
> >>>>>>>> [2] "2016-01-27,09:15:20,<Jane
> >>>>>>>> Doe>,
> >>>>>>>>
> >>>>>>> https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf
> >>>>>>>> "
> >>>>>>>>
> >>>>>>>> So, who knows what happened with the ??? at the beginning of [1]
> >>>>>>>> directly above. But notice how there are no commas in [1] but there
> >>>>>>>> appear in [2]. I don't see why really long ones like [2] directly
> >>>>>>>> above would be a problem, were they to be translated into a csv or
> >>>>>>>> data frame column.
> >>>>>>>>
> >>>>>>>> Now, with the commas in there, couldn't we write this into a csv or a
> >>>>>>>> data.frame? Some of this data will end up being garbage, I imagine.
> >>>>>>>> Like in [2] directly above. Or with [83] and [84] at the top of this
> >>>>>>>> discussion post/email. Embarrassingly, I've been trying to convert
> >>>>>>>> this into a data.frame or csv but I can't manage to. I've been using
> >>>>>>>> the write.csv function, but I don't think I've been getting the
> >>>>>>>> arguments correct.
> >>>>>>>>
> >>>>>>>> At the end of the day, I would like a data.frame and/or csv with the
> >>>>>>>> following four columns: date, time, person, comment.
> >>>>>>>>
> >>>>>>>> I tried this, too:
> >>>>>>>>
> >>>>>>>>> c <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
> >>>>>>>> + [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
> >>>>>>>> +                 a, proto=data.frame(stringsAsFactors=FALSE,
> >>>>>>> When="",
> >>>>>>>> Who="",
> >>>>>>>> +                                     What=""))
> >>>>>>>>
> >>>>>>>> But all I got was this:
> >>>>>>>>
> >>>>>>>>> c [1:100, ]
> >>>>>>>>  When  Who What
> >>>>>>>> 1   <NA> <NA> <NA>
> >>>>>>>> 2   <NA> <NA> <NA>
> >>>>>>>> 3   <NA> <NA> <NA>
> >>>>>>>> 4   <NA> <NA> <NA>
> >>>>>>>> 5   <NA> <NA> <NA>
> >>>>>>>> 6   <NA> <NA> <NA>
> >>>>>>>>
> >>>>>>>> It seems to have caught nothing.
> >>>>>>>>
> >>>>>>>>> unique (c)
> >>>>>>>> When  Who What
> >>>>>>>> 1 <NA> <NA> <NA>
> >>>>>>>>
> >>>>>>>> But I like that it converted into columns. That's a really great
> >>>>>>>> format. With a little tweaking, it'd be a great code for this data
> >>>>>>>> set.
> >>>>>>>>
> >>>>>>>> Michael
> >>>>>>>>
> >>>>>>>> On Fri, May 17, 2019 at 8:20 AM William Dunlap via R-help
> >>>>>>>> <r-help at r-project.org> wrote:
> >>>>>>>>>
> >>>>>>>>> Consider using readLines() and strcapture() for reading such a
> >>>>>>> file.
> >>>>>>>> E.g.,
> >>>>>>>>> suppose readLines(files) produced a character vector like
> >>>>>>>>>
> >>>>>>>>> x <- c("2016-10-21 10:35:36 <Jane Doe> What's your login",
> >>>>>>>>>        "2016-10-21 10:56:29 <John Doe> John_Doe",
> >>>>>>>>>        "2016-10-21 10:56:37 <John Doe> Admit#8242",
> >>>>>>>>>        "October 23, 1819 12:34 <Jane Eyre> I am not an angel")
> >>>>>>>>>
> >>>>>>>>> Then you can make a data.frame with columns When, Who, and What by
> >>>>>>>>> supplying a pattern containing three parenthesized capture
> >>>>>>> expressions:
> >>>>>>>>>> z <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
> >>>>>>>>> [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
> >>>>>>>>>           x, proto=data.frame(stringsAsFactors=FALSE, When="",
> >>>>>>> Who="",
> >>>>>>>>> What=""))
> >>>>>>>>>> str(z)
> >>>>>>>>> 'data.frame':   4 obs. of  3 variables:
> >>>>>>>>> $ When: chr  "2016-10-21 10:35:36" "2016-10-21 10:56:29"
> >>>>>>> "2016-10-21
> >>>>>>>>> 10:56:37" NA
> >>>>>>>>> $ Who : chr  "<Jane Doe>" "<John Doe>" "<John Doe>" NA
> >>>>>>>>> $ What: chr  "What's your login" "John_Doe" "Admit#8242" NA
> >>>>>>>>>
> >>>>>>>>> Lines that don't match the pattern result in NA's - you might make
> >>>>>>> a
> >>>>>>>> second
> >>>>>>>>> pass over the corresponding elements of x with a new pattern.
> >>>>>>>>>
> >>>>>>>>> You can convert the When column from character to time with
> >>>>>>> as.POSIXct().
> >>>>>>>>>
> >>>>>>>>> Bill Dunlap
> >>>>>>>>> TIBCO Software
> >>>>>>>>> wdunlap tibco.com
> >>>>>>>>>
> >>>>>>>>>
> >>>>>>>>> On Thu, May 16, 2019 at 8:30 PM David Winsemius
> >>>>>>> <dwinsemius at comcast.net>
> >>>>>>>>> wrote:
> >>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> On 5/16/19 3:53 PM, Michael Boulineau wrote:
> >>>>>>>>>>> OK. So, I named the object test and then checked the 6347th
> >>>>>>> item
> >>>>>>>>>>>
> >>>>>>>>>>>> test <- readLines ("hangouts-conversation.txt)
> >>>>>>>>>>>> test [6347]
> >>>>>>>>>>> [1] "2016-10-21 10:56:37 <John Doe> Admit#8242"
> >>>>>>>>>>>
> >>>>>>>>>>> Perhaps where it was getting screwed up is, since the end of
> >>>>>>> this is
> >>>>>>>> a
> >>>>>>>>>>> number (8242), then, given that there's no space between the
> >>>>>>> number
> >>>>>>>>>>> and what ought to be the next row, R didn't know where to draw
> >>>>>>> the
> >>>>>>>>>>> line. Sure enough, it looks like this when I go to the original
> >>>>>>> file
> >>>>>>>>>>> and control f "#8242"
> >>>>>>>>>>>
> >>>>>>>>>>> 2016-10-21 10:35:36 <Jane Doe> What's your login
> >>>>>>>>>>> 2016-10-21 10:56:29 <John Doe> John_Doe
> >>>>>>>>>>> 2016-10-21 10:56:37 <John Doe> Admit#8242
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> An octothorpe is an end of line signifier and is interpreted as
> >>>>>>>> allowing
> >>>>>>>>>> comments. You can prevent that interpretation with suitable
> >>>>>>> choice of
> >>>>>>>>>> parameters to `read.table` or `read.csv`. I don't understand why
> >>>>>>> that
> >>>>>>>>>> should cause anu error or a failure to match that pattern.
> >>>>>>>>>>
> >>>>>>>>>>> 2016-10-21 11:00:13 <Jane Doe> Okay so you have a discussion
> >>>>>>>>>>>
> >>>>>>>>>>> Again, it doesn't look like that in the file. Gmail
> >>>>>>> automatically
> >>>>>>>>>>> formats it like that when I paste it in. More to the point, it
> >>>>>>> looks
> >>>>>>>>>>> like
> >>>>>>>>>>>
> >>>>>>>>>>> 2016-10-21 10:35:36 <Jane Doe> What's your login2016-10-21
> >>>>>>> 10:56:29
> >>>>>>>>>>> <John Doe> John_Doe2016-10-21 10:56:37 <John Doe>
> >>>>>>>> Admit#82422016-10-21
> >>>>>>>>>>> 11:00:13 <Jane Doe> Okay so you have a discussion
> >>>>>>>>>>>
> >>>>>>>>>>> Notice Admit#82422016. So there's that.
> >>>>>>>>>>>
> >>>>>>>>>>> Then I built object test2.
> >>>>>>>>>>>
> >>>>>>>>>>> test2 <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4",
> >>>>>>> test)
> >>>>>>>>>>>
> >>>>>>>>>>> This worked for 84 lines, then this happened.
> >>>>>>>>>>
> >>>>>>>>>> It may have done something but as you later discovered my first
> >>>>>>> code
> >>>>>>>> for
> >>>>>>>>>> the pattern was incorrect. I had tested it (and pasted in the
> >>>>>>> results
> >>>>>>>> of
> >>>>>>>>>> the test) . The way to refer to a capture class is with
> >>>>>>> back-slashes
> >>>>>>>>>> before the numbers, not forward-slashes. Try this:
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>> newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)",
> >>>>>>> "\\1,\\2,\\3,\\4",
> >>>>>>>> chrvec)
> >>>>>>>>>>> newvec
> >>>>>>>>>> [1] "2016-07-01,02:50:35,<john>,hey"
> >>>>>>>>>> [2] "2016-07-01,02:51:26,<jane>,waiting for plane to Edinburgh"
> >>>>>>>>>> [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
> >>>>>>>>>> [4] "2016-07-01,02:52:07,<jane>,nothing crappy has happened,
> >>>>>>> not
> >>>>>>>> really"
> >>>>>>>>>> [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast,
> >>>>>>> didn't
> >>>>>>>> sleep"
> >>>>>>>>>> [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or
> >>>>>>> where I am
> >>>>>>>>>> really"
> >>>>>>>>>> [7] "2016-07-01,02:54:17,<john>,just know it's london"
> >>>>>>>>>> [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
> >>>>>>>>>> [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a good
> >>>>>>> eay"
> >>>>>>>>>> [10] "2016-07-01 02:58:56 <jone>"
> >>>>>>>>>> [11] "2016-07-01 02:59:34 <jane>"
> >>>>>>>>>> [12] "2016-07-01,03:02:48,<john>,British security is a little
> >>>>>>> more
> >>>>>>>>>> rigorous..."
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> I made note of the fact that the 10th and 11th lines had no
> >>>>>>> commas.
> >>>>>>>>>>
> >>>>>>>>>>>
> >>>>>>>>>>>> test2 [84]
> >>>>>>>>>>> [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
> >>>>>>>>>>
> >>>>>>>>>> That line didn't have any "<" so wasn't matched.
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> You could remove all none matching lines for pattern of
> >>>>>>>>>>
> >>>>>>>>>> dates<space>times<space>"<"<name>">"<space><anything>
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> with:
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> chrvec <- chrvec[ grepl("^.{10} .{8} <.+> .+$)", chrvec)]
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> Do read:
> >>>>>>>>>>
> >>>>>>>>>> ?read.csv
> >>>>>>>>>>
> >>>>>>>>>> ?regex
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> --
> >>>>>>>>>>
> >>>>>>>>>> David
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>>> test2 [85]
> >>>>>>>>>>> [1] "//1,//2,//3,//4"
> >>>>>>>>>>>> test [85]
> >>>>>>>>>>> [1] "2016-07-01 02:50:35 <John Doe> hey"
> >>>>>>>>>>>
> >>>>>>>>>>> Notice how I toggled back and forth between test and test2
> >>>>>>> there. So,
> >>>>>>>>>>> whatever happened with the regex, it happened in the switch
> >>>>>>> from 84
> >>>>>>>> to
> >>>>>>>>>>> 85, I guess. It went on like
> >>>>>>>>>>>
> >>>>>>>>>>> [990] "//1,//2,//3,//4"
> >>>>>>>>>>> [991] "//1,//2,//3,//4"
> >>>>>>>>>>> [992] "//1,//2,//3,//4"
> >>>>>>>>>>> [993] "//1,//2,//3,//4"
> >>>>>>>>>>> [994] "//1,//2,//3,//4"
> >>>>>>>>>>> [995] "//1,//2,//3,//4"
> >>>>>>>>>>> [996] "//1,//2,//3,//4"
> >>>>>>>>>>> [997] "//1,//2,//3,//4"
> >>>>>>>>>>> [998] "//1,//2,//3,//4"
> >>>>>>>>>>> [999] "//1,//2,//3,//4"
> >>>>>>>>>>> [1000] "//1,//2,//3,//4"
> >>>>>>>>>>>
> >>>>>>>>>>> up until line 1000, then I reached max.print.
> >>>>>>>>>>
> >>>>>>>>>>> Michael
> >>>>>>>>>>>
> >>>>>>>>>>> On Thu, May 16, 2019 at 1:05 PM David Winsemius <
> >>>>>>>> dwinsemius at comcast.net>
> >>>>>>>>>> wrote:
> >>>>>>>>>>>>
> >>>>>>>>>>>> On 5/16/19 12:30 PM, Michael Boulineau wrote:
> >>>>>>>>>>>>> Thanks for this tip on etiquette, David. I will be sure and
> >>>>>>> not do
> >>>>>>>>>> that again.
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> I tried the read.fwf from the foreign package, with a code
> >>>>>>> like
> >>>>>>>> this:
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> d <- read.fwf("hangouts-conversation.txt",
> >>>>>>>>>>>>>                widths= c(10,10,20,40),
> >>>>>>>>>>>>>
> >>>>>>> col.names=c("date","time","person","comment"),
> >>>>>>>>>>>>>                strip.white=TRUE)
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> But it threw this error:
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> Error in scan(file = file, what = what, sep = sep, quote =
> >>>>>>> quote,
> >>>>>>>> dec
> >>>>>>>>>> = dec,  :
> >>>>>>>>>>>>>  line 6347 did not have 4 elements
> >>>>>>>>>>>>
> >>>>>>>>>>>> So what does line 6347 look like? (Use `readLines` and print
> >>>>>>> it
> >>>>>>>> out.)
> >>>>>>>>>>>>
> >>>>>>>>>>>>> Interestingly, though, the error only happened when I
> >>>>>>> increased the
> >>>>>>>>>>>>> width size. But I had to increase the size, or else I
> >>>>>>> couldn't
> >>>>>>>> "see"
> >>>>>>>>>>>>> anything.  The comment was so small that nothing was being
> >>>>>>>> captured by
> >>>>>>>>>>>>> the size of the column. so to speak.
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> It seems like what's throwing me is that there's no comma
> >>>>>>> that
> >>>>>>>>>>>>> demarcates the end of the text proper. For example:
> >>>>>>>>>>>> Not sure why you thought there should be a comma. Lines
> >>>>>>> usually end
> >>>>>>>>>>>> with  <cr> and or a <lf>.
> >>>>>>>>>>>>
> >>>>>>>>>>>>
> >>>>>>>>>>>> Once you have the raw text in a character vector from
> >>>>>>> `readLines`
> >>>>>>>> named,
> >>>>>>>>>>>> say, 'chrvec', then you could selectively substitute commas
> >>>>>>> for
> >>>>>>>> spaces
> >>>>>>>>>>>> with regex. (Now that you no longer desire to remove the dates
> >>>>>>> and
> >>>>>>>>>> times.)
> >>>>>>>>>>>>
> >>>>>>>>>>>> sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", chrvec)
> >>>>>>>>>>>>
> >>>>>>>>>>>> This will not do any replacements when the pattern is not
> >>>>>>> matched.
> >>>>>>>> See
> >>>>>>>>>>>> this test:
> >>>>>>>>>>>>
> >>>>>>>>>>>>
> >>>>>>>>>>>>> newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)",
> >>>>>>> "\\1,\\2,\\3,\\4",
> >>>>>>>>>> chrvec)
> >>>>>>>>>>>>> newvec
> >>>>>>>>>>>> [1] "2016-07-01,02:50:35,<john>,hey"
> >>>>>>>>>>>> [2] "2016-07-01,02:51:26,<jane>,waiting for plane to
> >>>>>>> Edinburgh"
> >>>>>>>>>>>> [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
> >>>>>>>>>>>> [4] "2016-07-01,02:52:07,<jane>,nothing crappy has
> >>>>>>> happened, not
> >>>>>>>>>> really"
> >>>>>>>>>>>> [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast,
> >>>>>>> didn't
> >>>>>>>>>> sleep"
> >>>>>>>>>>>> [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or
> >>>>>>> where
> >>>>>>>> I am
> >>>>>>>>>>>> really"
> >>>>>>>>>>>> [7] "2016-07-01,02:54:17,<john>,just know it's london"
> >>>>>>>>>>>> [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
> >>>>>>>>>>>> [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a
> >>>>>>> good
> >>>>>>>> eay"
> >>>>>>>>>>>> [10] "2016-07-01 02:58:56 <jone>"
> >>>>>>>>>>>> [11] "2016-07-01 02:59:34 <jane>"
> >>>>>>>>>>>> [12] "2016-07-01,03:02:48,<john>,British security is a little
> >>>>>>> more
> >>>>>>>>>>>> rigorous..."
> >>>>>>>>>>>>
> >>>>>>>>>>>>
> >>>>>>>>>>>> You should probably remove the "empty comment" lines.
> >>>>>>>>>>>>
> >>>>>>>>>>>>
> >>>>>>>>>>>> --
> >>>>>>>>>>>>
> >>>>>>>>>>>> David.
> >>>>>>>>>>>>
> >>>>>>>>>>>>> 2016-07-01 15:34:30 <John Doe> Lame. We were in a
> >>>>>>>> starbucks2016-07-01
> >>>>>>>>>>>>> 15:35:02 <Jane Doe> Hmm that's interesting2016-07-01 15:35:09
> >>>>>>> <Jane
> >>>>>>>>>>>>> Doe> You must want coffees2016-07-01 15:35:25 <John Doe>
> >>>>>>> There was
> >>>>>>>>>>>>> lots of Starbucks in my day2016-07-01 15:35:47
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> It was interesting, too, when I pasted the text into the
> >>>>>>> email, it
> >>>>>>>>>>>>> self-formatted into the way I wanted it to look. I had to
> >>>>>>> manually
> >>>>>>>>>>>>> make it look like it does above, since that's the way that it
> >>>>>>>> looks in
> >>>>>>>>>>>>> the txt file. I wonder if it's being organized by XML or
> >>>>>>> something.
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> Anyways, There's always a space between the two sideways
> >>>>>>> carrots,
> >>>>>>>> just
> >>>>>>>>>>>>> like there is right now: <John Doe> See. Space. And there's
> >>>>>>> always
> >>>>>>>> a
> >>>>>>>>>>>>> space between the data and time. Like this. 2016-07-01
> >>>>>>> 15:34:30
> >>>>>>>> See.
> >>>>>>>>>>>>> Space. But there's never a space between the end of the
> >>>>>>> comment and
> >>>>>>>>>>>>> the next date. Like this: We were in a starbucks2016-07-01
> >>>>>>> 15:35:02
> >>>>>>>>>>>>> See. starbucks and 2016 are smooshed together.
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> This code is also on the table right now too.
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> a <- read.table("E:/working
> >>>>>>>>>>>>> directory/-189/hangouts-conversation2.txt", quote="\"",
> >>>>>>>>>>>>> comment.char="", fill=TRUE)
> >>>>>>>>>>>>>
> >>>>>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>
> >>>>>>> h<-cbind(hangouts.conversation2[,1:2],hangouts.conversation2[,3:5],hangouts.conversation2[,6:9])
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> aa<-gsub("[^[:digit:]]","",h)
> >>>>>>>>>>>>> my.data.num <- as.numeric(str_extract(h, "[0-9]+"))
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> Those last lines are a work in progress. I wish I could
> >>>>>>> import a
> >>>>>>>>>>>>> picture of what it looks like when it's translated into a
> >>>>>>> data
> >>>>>>>> frame.
> >>>>>>>>>>>>> The fill=TRUE helped to get the data in table that kind of
> >>>>>>> sort of
> >>>>>>>>>>>>> works, but the comments keep bleeding into the data and time
> >>>>>>>> column.
> >>>>>>>>>>>>> It's like
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> 2016-07-01 15:59:17 <Jane Doe> Seriously I've never been
> >>>>>>>>>>>>> over               there
> >>>>>>>>>>>>> 2016-07-01 15:59:27 <Jane Doe> It confuses me :(
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> And then, maybe, the "seriously" will be in a column all to
> >>>>>>>> itself, as
> >>>>>>>>>>>>> will be the "I've'"and the "never" etc.
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> I will use a regular expression if I have to, but it would be
> >>>>>>> nice
> >>>>>>>> to
> >>>>>>>>>>>>> keep the dates and times on there. Originally, I thought they
> >>>>>>> were
> >>>>>>>>>>>>> meaningless, but I've since changed my mind on that count.
> >>>>>>> The
> >>>>>>>> time of
> >>>>>>>>>>>>> day isn't so important. But, especially since, say, Gmail
> >>>>>>> itself
> >>>>>>>> knows
> >>>>>>>>>>>>> how to quickly recognize what it is, I know it can be done. I
> >>>>>>> know
> >>>>>>>>>>>>> this data has structure to it.
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> Michael
> >>>>>>>>>>>>>
> >>>>>>>>>>>>>
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> On Wed, May 15, 2019 at 8:47 PM David Winsemius <
> >>>>>>>>>> dwinsemius at comcast.net> wrote:
> >>>>>>>>>>>>>> On 5/15/19 4:07 PM, Michael Boulineau wrote:
> >>>>>>>>>>>>>>> I have a wild and crazy text file, the head of which looks
> >>>>>>> like
> >>>>>>>> this:
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> 2016-07-01 02:50:35 <john> hey
> >>>>>>>>>>>>>>> 2016-07-01 02:51:26 <jane> waiting for plane to Edinburgh
> >>>>>>>>>>>>>>> 2016-07-01 02:51:45 <john> thinking about my boo
> >>>>>>>>>>>>>>> 2016-07-01 02:52:07 <jane> nothing crappy has happened, not
> >>>>>>>> really
> >>>>>>>>>>>>>>> 2016-07-01 02:52:20 <john> plane went by pretty fast,
> >>>>>>> didn't
> >>>>>>>> sleep
> >>>>>>>>>>>>>>> 2016-07-01 02:54:08 <jane> no idea what time it is or where
> >>>>>>> I am
> >>>>>>>>>> really
> >>>>>>>>>>>>>>> 2016-07-01 02:54:17 <john> just know it's london
> >>>>>>>>>>>>>>> 2016-07-01 02:56:44 <jane> you are probably asleep
> >>>>>>>>>>>>>>> 2016-07-01 02:58:45 <jane> I hope fish was fishy in a good
> >>>>>>> eay
> >>>>>>>>>>>>>>> 2016-07-01 02:58:56 <jone>
> >>>>>>>>>>>>>>> 2016-07-01 02:59:34 <jane>
> >>>>>>>>>>>>>>> 2016-07-01 03:02:48 <john> British security is a little
> >>>>>>> more
> >>>>>>>>>> rigorous...
> >>>>>>>>>>>>>> Looks entirely not-"crazy". Typical log file format.
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>> Two possibilities: 1) Use `read.fwf` from pkg foreign; 2)
> >>>>>>> Use
> >>>>>>>> regex
> >>>>>>>>>>>>>> (i.e. the sub-function) to strip everything up to the "<".
> >>>>>>> Read
> >>>>>>>>>>>>>> `?regex`. Since that's not a metacharacters you could use a
> >>>>>>>> pattern
> >>>>>>>>>>>>>> ".+<" and replace with "".
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>> And do read the Posting Guide. Cross-posting to
> >>>>>>> StackOverflow and
> >>>>>>>>>> Rhelp,
> >>>>>>>>>>>>>> at least within hours of each, is considered poor manners.
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>> --
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>> David.
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> It goes on for a while. It's a big file. But I feel like
> >>>>>>> it's
> >>>>>>>> going
> >>>>>>>>>> to
> >>>>>>>>>>>>>>> be difficult to annotate with the coreNLP library or
> >>>>>>> package. I'm
> >>>>>>>>>>>>>>> doing natural language processing. In other words, I'm
> >>>>>>> curious
> >>>>>>>> as to
> >>>>>>>>>>>>>>> how I would shave off the dates, that is, to make it look
> >>>>>>> like:
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> <john> hey
> >>>>>>>>>>>>>>> <jane> waiting for plane to Edinburgh
> >>>>>>>>>>>>>>>  <john> thinking about my boo
> >>>>>>>>>>>>>>> <jane> nothing crappy has happened, not really
> >>>>>>>>>>>>>>> <john> plane went by pretty fast, didn't sleep
> >>>>>>>>>>>>>>> <jane> no idea what time it is or where I am really
> >>>>>>>>>>>>>>> <john> just know it's london
> >>>>>>>>>>>>>>> <jane> you are probably asleep
> >>>>>>>>>>>>>>> <jane> I hope fish was fishy in a good eay
> >>>>>>>>>>>>>>>  <jone>
> >>>>>>>>>>>>>>> <jane>
> >>>>>>>>>>>>>>> <john> British security is a little more rigorous...
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> To be clear, then, I'm trying to clean a large text file by
> >>>>>>>> writing a
> >>>>>>>>>>>>>>> regular expression? such that I create a new object with no
> >>>>>>>> numbers
> >>>>>>>>>> or
> >>>>>>>>>>>>>>> dates.
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> Michael
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> ______________________________________________
> >>>>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
> >>>>>>> more,
> >>>>>>>> see
> >>>>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>>>>>>>> PLEASE do read the posting guide
> >>>>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>>>>>>>> and provide commented, minimal, self-contained,
> >>>>>>> reproducible
> >>>>>>>> code.
> >>>>>>>>>>>>> ______________________________________________
> >>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >>>>>>> see
> >>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>>>>>> PLEASE do read the posting guide
> >>>>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
> >>>>>>> code.
> >>>>>>>>>>>> ______________________________________________
> >>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >>>>>>> see
> >>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>>>>> PLEASE do read the posting guide
> >>>>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
> >>>>>>> code.
> >>>>>>>>>>> ______________________________________________
> >>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >>>>>>> see
> >>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>>>> PLEASE do read the posting guide
> >>>>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
> >>>>>>> code.
> >>>>>>>>>>
> >>>>>>>>>> ______________________________________________
> >>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>>> PLEASE do read the posting guide
> >>>>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>>> and provide commented, minimal, self-contained, reproducible
> >>>>>>> code.
> >>>>>>>>>>
> >>>>>>>>>
> >>>>>>>>>      [[alternative HTML version deleted]]
> >>>>>>>>>
> >>>>>>>>> ______________________________________________
> >>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>> PLEASE do read the posting guide
> >>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>>>
> >>>>>>>> ______________________________________________
> >>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>> PLEASE do read the posting guide
> >>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>>>
> >>>>>>>
> >>>>>>>    [[alternative HTML version deleted]]
> >>>>>>>
> >>>>>>> ______________________________________________
> >>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>> PLEASE do read the posting guide
> >>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>
> >>>>>> --
> >>>>>> Sent from my phone. Please excuse my brevity.
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From bor|@@@te|pe @end|ng |rom utoronto@c@  Sun May 19 23:12:36 2019
From: bor|@@@te|pe @end|ng |rom utoronto@c@ (Boris Steipe)
Date: Sun, 19 May 2019 21:12:36 +0000
Subject: [R] Nested structure data simulation
In-Reply-To: <370367748.4299867.1558278842667@mail.yahoo.com>
References: <1342550043.2326226.1558187826295.ref@mail.yahoo.com>
 <1342550043.2326226.1558187826295@mail.yahoo.com>
 <BBDA4312-A62F-450A-9E9F-655DAF278FF2@utoronto.ca>
 <570252025.3999934.1558209817043@mail.yahoo.com>
 <CAPm+3sCQk=qZu9gc2LStZQNRZfoVMrhQ5Wc8Q3OrfK3BGjMmvQ@mail.gmail.com>
 <1560846858.4204873.1558267547377@mail.yahoo.com>
 <5F8F8017-2E90-4DE5-898F-492C6CC57E1A@utoronto.ca>
 <370367748.4299867.1558278842667@mail.yahoo.com>
Message-ID: <42303FC7-B0CB-47B2-9C01-50F238F52470@utoronto.ca>

My mental model for such a simulation is that you create data from a known distribution, then use your model to check that you can recover the known parameters from the data. Thus how the marks are created depends on what influences them. Here is a toy model to illustrate this - expanding on my code sample:


# a function to generate marks from parameters
rMarks <- function(n, m, s) {
  # a normal distribution limited to between 1 and 6, in 0.5 intervals, with
  # mean m and standard deviation s
  marks <- rnorm(n, m, s)
  marks <- round(marks * 2) / 2
  marks[marks < 1] <- 1
  marks[marks > 6] <- 6
  return(marks)
}

# Teachers in two categories: 70% of teachers (tNormal) grade everyone according to 
# a marks distribution with m = 3.5 and sd = 1 ; the others grade girls with a 
# m = 4.5 and sd = 0.7 and boys with m = 3.0 and sd = 1.2

# define who are the "normal teachers"
x <- paste0("t", 1:(nS * nTpS))
tNormal <- sample(x, round(nS * nTpS * 0.7), replace = FALSE)

# this is rather pedestrian code, but as explicit as I can make it ...
for (i in 1:nrow(mySim)) {
  if (mySim$Teacher[i] %in% tNormal) {
    m <- 3.5
    s <- 1.0
  } else {
    if (mySim$Gender[i] == "girl") {
      m <- 4.5
      s <- 0.7
    } else {
      m <- 3.0
      s <- 1.2 
    }
  }
  mySim$Mark[i] <- rMarks(1, m, s)
}

# Validate
table(mySim$Mark)
hist(mySim$Mark[mySim$Teacher %in% tNormal],
     col = "#0000BB44")
hist(mySim$Mark[ ! mySim$Teacher %in% tNormal],
     add = TRUE,
     col = "#BB000044")

Then the challenge is to recover the parameters from your analysis. 


Cheers,
Boris



> On 2019-05-19, at 11:14, varin sacha <varinsacha at yahoo.fr> wrote:
> 
> Dear Boris,
> 
> Great !!!! But what about Mark in your R code ? Don't we have to precise in the R code that mark ranges between 1 to 6 (1 ; 1.5 ; 2 ; 2.5 ; 3 ; 3.5 ; 4 ; 4.5 ; 5 ; 5.5 ; 6) ?
> 
> By the way, to fit a linear mixed model, I use lme4 package and then the lmer function works with the variables like in this example here below :
> 
> library(lme4)
> mm=lmer(Mark ~Gender + (1 | School / Class), data=Dataset) 
> 
> With your R code, how can I write the lmer function to make it work ?
> 
> Best,
> S.
> 
> 
> 
> 
> 
> 
> 
> Le dimanche 19 mai 2019 ? 15:26:39 UTC+2, Boris Steipe <boris.steipe at utoronto.ca> a ?crit : 
> 
> 
> 
> 
> 
> Fair enough - there are additional assumptions needed, which I make as follows:
>   - each class has the same size
>   - each teacher teaches the same number of classes
>   - the number of boys and girls is random within a class
>   - there are 60% girls  (just for illustration that it does not have to be equal)
>   
> 
> To make the dependencies explicit, I define them so, and in a way that they can't be inconsistent.
> 
> nS <- 10        # Schools
> nTpS <- 5      # Teachers per School
> nCpT <- 2      # Classes per teacher
> nPpC <- 20      # Pupils per class
> nS * nTpS * nCpT * nPpC == 2000  # Validate
> 
> 
> mySim <- data.frame(School  = paste0("s", rep(1:nS, each = nTpS*nCpT*nPpC)),
>                     Teacher = paste0("t", rep(1:(nTpS*nS), each = nCpT*nPpC)),
>                     Class  = paste0("c", rep(1:(nCpT*nTpS*nS), each = nPpC)),
>                     Gender  = sample(c("boy", "girl"),
>                                     (nS*nTpS*nCpT*nPpC),
>                                     prob = c(0.4, 0.6),
>                                     replace = TRUE),
>                     Mark    = numeric(nS*nTpS*nCpT*nPpC),
>                     stringsAsFactors = FALSE)
>                     
> 
> Then you fill mySim$Mark with values from your linear mixed model ...
> 
> mySim$Mark[i] <- simMarks(mySim[i])  # ... or something equivalent.
> 
> 
> All good?
> 
> Cheers,
> Boris
> 
> 
> 
>> On 2019-05-19, at 08:05, varin sacha <varinsacha at yahoo.fr> wrote:
>> 
>> Many thanks to all of you for your responses.
>> 
>> So, I will try to be clearer with a larger example. Te end of my mail is the more important to understand what I am trying to do. I am trying to simulate data to fit a linear mixed model (nested not crossed). More precisely, I would love to get at the end of the process, a table (.txt) with columns and rows. Column 1 and Rows will be the 2000 pupils and the columns the different variables : Column 2 = classes ; Column 3 = teachers, Column 4 = schools ; Column 5 = gender (boy or girl) ; Column 6 = mark in Frecnh
>> 
>> Pupils are nested  in classes, classes are nested in schools. The teacher are part of the process.
>> 
>> I want to simulate a dataset with n=2000 pupils, 100 classes, 50 teachers and 10 schools.
>> - Pupils n?1 to pupils n?2000 (p1, p2, p3, p4, ..., p2000)
>> - Classes n?1 to classes n?100 (c1, c2, c3, c4,..., c100)
>> - Teachers n?1 to teacher n?50 ( t1, t2, t3, t4, ..., t50)
>> - Schools n?1 to chool n?10 (s1, s2, s3, s4, ..., s10)
>> 
>> The nested structure is as followed : 
>> 
>> -- School 1 with teacher 1 to teacher 5 (t1, t2, t3, t4 and t5) with classes 1 to classes 10 (c1, c2, c3, c4, c5, c6, c7, c8,c9,c10), pupils n?1 to pupils n?200 (p1, p2, p3, p4,..., p200).
>> 
>> -- School 2 with teacher 6 to teacher 10, with classes 11 to classes 20, pupils n?201 to pupils n?400
>> 
>> -- and so on
>> 
>> The table (.txt) I would love to get at the end is the following :
>> 
>>         Class    Teacher    School    gender    Mark
>> 1      c1        t1                s1            boy        5
>> 2      c1        t1                s1            boy        5.5
>> 3      c1        t1                s1            girl        4.5
>> 4      c1        t1                s1            girl        6
>> 5      c1        t1                s1            boy      3.5
>> 6      ...        ....                ....            .....        .....              
>> 
>> The first 20 rows with c1, with t1, with s1, gender (randomly slected) and mark (andomly selected) from 1 to 6
>> The rows 21 to 40 with c2 with t1 with s1
>> The rows 41 to 60 with c3 with t2 with s1
>> The rows 61 to 80 with c4 with t2 with s1
>> The rows 81 to 100 with c5 with t3 with s1
>> The rows 101 to 120 with c6 with t3 with s1
>> The rows 121 to 140 with c7 with t4 with s1
>> The rows 141 to 160 with c8 with t4 with s1
>> The rows 161 to 180 with c9 with t5 with s1
>> The rows 181 to 200 with c10 with t5 with s1
>> 
>> The rows 201 to 220 with c11 with t6 with s2
>> The rows 221 to 240 with c12 with t6 with s2
>> 
>> And so on...
>> 
>> Is it possible to do that ? Or am I dreaming ?
>> 
>> 
>> Le dimanche 19 mai 2019 ? 10:45:43 UTC+2, Linus Chen <linus.l.chen at gmail.com> a ?crit : 
>> 
>> 
>> 
>> 
>> 
>> Dear varin sacha,
>> 
>> I think it will help us help you, if you give a clearer description of
>> what exactly you want.
>> 
>> I assume the situation is that you know what a data structure you
>> want, but do not know
>> how to conveniently create such structure.
>> And that is where others can help you.
>> So, please, describe the wanted data structure more thoroughly,
>> ideally with example.
>> 
>> Thanks,
>> Lei
>> 
>> On Sat, May 18, 2019 at 10:04 PM varin sacha via R-help
>> <r-help at r-project.org> wrote:
>>> 
>>> Dear Boris,
>>> 
>>> Yes, top-down, no problem. Many thanks, but in your code did you not forget "teacher" ? As a reminder teacher has to be nested with classes. I mean the 50 pupils belonging to C1 must be with (teacher 1) T1, the 50 pupils belonging to C2 with T2, the 50 pupils belonging to C3 with T3 and so on.
>>> 
>>> Best,
>>> 
>>> 
>>> Le samedi 18 mai 2019 ? 16:52:48 UTC+2, Boris Steipe <boris.steipe at utoronto.ca> a ?crit :
>>> 
>>> 
>>> 
>>> 
>>> 
>>> Can you build your data top-down?
>>> 
>>> 
>>> 
>>> schools <- paste("s", 1:6, sep="")
>>> 
>>> classes <- character()
>>> for (school in schools) {
>>>   classes <- c(classes, paste(school, paste("c", 1:5, sep=""), sep = "."))
>>> }
>>> 
>>> pupils <- character()
>>> for (class in classes) {
>>>   pupils <- c(pupils, paste(class, paste("p", 1:10, sep=""), sep = "."))
>>> }
>>> 
>>> 
>>> 
>>> B.
>>> 
>>> 
>>> 
>>>> On 2019-05-18, at 09:57, varin sacha via R-help <r-help at r-project.org> wrote:
>>>> 
>>>> Dear R-Experts,
>>>> 
>>>> In a data simulation, I would like a balanced distribution with a nested structure for classroom and teacher (not for school). I mean 50 pupils belonging to C1, 50 other pupils belonging to C2, 50 other pupils belonging to C3 and so on. Then I want the 50 pupils belonging to C1 with T1, the 50 pupils belonging to C2 with T2, the 50 pupils belonging to C3 with T3 and so on. The school don?t have to be nested, I just want a balanced distribution, I mean 60 pupils in S1, 60 other pupils in S2 and so on.
>>>> Here below the reproducible example.
>>>> Many thanks for your help.
>>>> 
>>>> ##############
>>>> set.seed(123)
>>>> # G?n?ration al?atoire des colonnes
>>>> pupils<-1:300
>>>> classroom<-sample(c("C1","C2","C3","C4","C5","C6"),300,replace=T)  teacher<-sample(c("T1","T2","T3","T4","T5","T6"),300,replace=T)  school<-sample(c("S1","S2","S3","S4","S5"),300,replace=T)
>>> 
>>>> ##############
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> 


From bor|@@@te|pe @end|ng |rom utoronto@c@  Sun May 19 23:31:25 2019
From: bor|@@@te|pe @end|ng |rom utoronto@c@ (Boris Steipe)
Date: Sun, 19 May 2019 21:31:25 +0000
Subject: [R] how to separate string from numbers in a large txt file
In-Reply-To: <CAH+cTGOijsRxzJy3w9QS11axwD+Z36-Pive6dOhm2xK6Jm5PAg@mail.gmail.com>
References: <CAH+cTGM9sFCxegaMHzu-_9Prhvvh9MbVDCWJ1_x7wkO9jC47mQ@mail.gmail.com>
 <c6e69644-66a1-7031-4d7c-1000e0abbea4@comcast.net>
 <CAH+cTGM5YvWV_G0pRgMqjCzuRrFTzd0nOc7yoThfGbGMUyMjsQ@mail.gmail.com>
 <9c9f1c81-309f-91cb-7e26-f02fc4bd45d8@comcast.net>
 <CAH+cTGMx4_Lz2gGDf2AuUaaZF2Xaq4McuPRoHu4JJNc-ORd=Ew@mail.gmail.com>
 <6322e735-5659-848e-3a50-9f066f1299ad@comcast.net>
 <CAF8bMcbgrVcN9J7Aygq2D5RWaU4MS-GH+_+zMHU=AO+DU5DVWw@mail.gmail.com>
 <CAH+cTGNfpLSurqmsoMeci0GWhC0ncOp7zTEju3fJW5T4+tCsGA@mail.gmail.com>
 <CAF8bMcaSJRaUXK4h2ssaDoJwXaKCUZT2Vq1cutNiNDjQ7ga2Xg@mail.gmail.com>
 <EEA9FD46-A67C-4FB7-8245-350819C4EA62@dcn.davis.ca.us>
 <CAH+cTGNOkAnPnn-aru-BJUbhrgeYMQBTqjSYT764nKzcdXvTvA@mail.gmail.com>
 <9B170703-AAAF-4D52-9744-1FDE70559B05@utoronto.ca>
 <CAH+cTGNaw7p-it4B7te4BBsay=v6J-5OS-P_WGDJJ6Z0ymxtvw@mail.gmail.com>
 <0F8CD89C-BDE2-44D0-8E18-0CA2706EB53F@utoronto.ca>
 <CAH+cTGMsD+PkTKoFfB8=3WeWPQAWjhmuOCKZfnB7jR03m7RtDQ@mail.gmail.com>
 <C5A52A04-CB49-442F-81CE-26CBC619C47A@utoronto.ca>
 <CAH+cTGOijsRxzJy3w9QS11axwD+Z36-Pive6dOhm2xK6Jm5PAg@mail.gmail.com>
Message-ID: <13D5D19C-0EA2-4234-9EDC-539A01DB7690@utoronto.ca>

Inline ...

> On 2019-05-19, at 13:56, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
> 
>> b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)"
> 
> so the ^ signals that the regex BEGINS with a number (that could be
> any number, 0-9) that is only 10 characters long (then there's the
> dash in there, too, with the 0-9-, which I assume enabled the regex to
> grab the - that's between the numbers in the date)

That's right. Note that within a "character class" the hyphen can have tow meanings: normally it defines a range of characters, but if it appears as the last character before "]" it is a literal hyphen.

> , followed by a
> single space, followed by a unit that could be any number, again, but
> that is only 8 characters long this time. For that one, it will
> include the colon, hence the 9:, although for that one ([0-9:]{8} ),

Right.


> I
> don't get why the space is on the inside in that one, after the {8},

The space needs to be preserved between the time and the name. I wrote
b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)" # space in the first captured expression
c <- gsub(b, "\\1<\\2> ", a)
 ... but I could have written
b <- "^([0-9-]{10} [0-9:]{8})[*]{3} (\\w+ \\w+)" 
c <- gsub(b, "\\1 <\\2> ", a)  # space in the substituted string
... same result


> whereas the space is on the outside with the other one ^([0-9-]{10} ,
> directly after the {10}. Why is that?

In the second case, I capture without a space, because I don't want the space in the results, after the time.


> 
> Then three *** [*]{3}, then the (\\w+ \\w+)", which Boris explained so
> well above. I guess I still don't get why this one seemed to have
> deleted the *** out of the mix, plus I still don't why it didn't
> remove the *** from the first one.

Because the entire first line was not matched since it had a malformed character preceding the date.

> 
> 2016-03-20 19:29:37 *** Jane Doe started a video chat
> 2016-03-20 19:30:35 *** John Doe ended a video chat
> 2016-04-02 12:59:36 *** Jane Doe started a video chat
> 2016-04-02 13:00:43 *** John Doe ended a video chat
> 2016-04-02 13:01:08 *** Jane Doe started a video chat
> 2016-04-02 13:01:41 *** John Doe ended a video chat
> 2016-04-02 13:03:51 *** John Doe started a video chat
> 2016-04-02 13:06:35 *** John Doe ended a video chat
> 
> This is a random sample from the beginning of the txt file with no
> edits. The ***s were deleted, all but the first one, the one that had
> the ??? but that was taken out by the encoding = "UTF-8". I know that
> the function was c <- gsub(b, "\\1<\\2> ", a), so it had a gsub () on
> there, the point of which is to do substitution work.
> 
> Oh, I get it, I think. The \\1<\\2> in the gsub () puts the <> around
> the names, so that it's consistent with the rest of the data, so that
> the names in the text about that aren't enclosed in the <> are
> enclosed like the rest of them. But I still don't get why or how the
> gsub () replaced the *** with the <>...

In gsub(b, "\\1<\\2> ", a) the work is done by the backreferences \\1 and \\2. The expression says:
Substitute ALL of the match with the first captured expression, then " <", then the second captured expression, then "> ". The rest of the line is not substituted and appears as-is.


> 
> This one is more straightforward.
> 
>> d <- "^([0-9-]{10}) ([0-9:]{8}) <(\\w+ \\w+)>\\s*(.+)$"
> 
> any number with - for 10 characters, followed by a space. Oh, there's
> no space in this one ([0-9:]{8}), after the {8}. Hu. So, then, any
> number with : for 8 characters, followed by any two words separated by
> a space and enclosed in <>. And then the \\s* is followed by a single
> space? Or maybe it puts space on both sides (on the side of the #s to
> the left, and then the comment to the right). The (.+)$ is anything
> whatsoever until the end.

\s is the metacharacter for "whitespace". \s* means zero or more whitespace. I'm matching that OUTSIDE of the captured expression, to removes any leading spaces from the data that goes into the data frame.


Cheers,
Boris




> 
> Michael
> 
> 
> On Sun, May 19, 2019 at 4:37 AM Boris Steipe <boris.steipe at utoronto.ca> wrote:
>> 
>> Inline
>> 
>> 
>> 
>>> On 2019-05-18, at 20:34, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
>>> 
>>> It appears to have worked, although there were three little quirks.
>>> The ; close(con); rm(con) didn't work for me; the first row of the
>>> data.frame was all NAs, when all was said and done;
>> 
>> You will get NAs for lines that can't be matched to the regular expression. That's a good thing, it allows you to test whether your assumptions were valid for the entire file:
>> 
>> # number of failed strcapture()
>> sum(is.na(e$date))
>> 
>> 
>>> and then there
>>> were still three *** on the same line where the ??? was apparently
>>> deleted.
>> 
>> This is a sign that something else happened with the line that prevented the regex from matching. In that case you need to investigate more. I see an invalid multibyte character at the beginning of the line you posted below.
>> 
>>> 
>>>> a <- readLines ("hangouts-conversation-6.txt", encoding = "UTF-8")
>>>> b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)"
>>>> c <- gsub(b, "\\1<\\2> ", a)
>>>> head (c)
>>> [1] "?2016-01-27 09:14:40 *** Jane Doe started a video chat"
>>> [2] "2016-01-27 09:15:20 <Jane Doe>
>>> https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf"
>> 
>> [...]
>> 
>>> But, before I do anything else, I'm going to study the regex in this
>>> particular code. For example, I'm still not sure why there has to the
>>> second \\w+ in the (\\w+ \\w+). Little things like that.
>> 
>> \w is the metacharacter for alphanumeric characters, \w+ designates something we could call a word. Thus \w+ \w+ are two words separated by a single blank. This corresponds to your example, but, as I wrote previously, you need to think very carefully whether this covers all possible cases (Could there be only one word? More than one blank? Could letters be separated by hyphens or periods?) In most cases we could have more robustly matched everything between "<" and ">" (taking care to test what happens if the message contains those characters). But for the video chat lines we need to make an assumption about what is name and what is not. If "started a video chat" is the only possibility in such lines, you can use this information instead. If there are other possibilities, you need a different strategy. In NLP there is no one-approach-fits-all.
>> 
>> To validate the structure of the names in your transcripts, you can look at
>> 
>> patt <- " <.+?> "   # " <any string, not greedy> "
>> m <- regexpr(patt, c)
>> unique(regmatches(c, m))
>> 
>> 
>> 
>> B.
>> 
>> 
>> 
>>> 
>>> Michael
>>> 
>>> 
>>> On Sat, May 18, 2019 at 4:30 PM Boris Steipe <boris.steipe at utoronto.ca> wrote:
>>>> 
>>>> This works for me:
>>>> 
>>>> # sample data
>>>> c <- character()
>>>> c[1] <- "2016-01-27 09:14:40 <Jane Doe> started a video chat"
>>>> c[2] <- "2016-01-27 09:15:20 <Jane Doe> https://lh3.googleusercontent.com/"
>>>> c[3] <- "2016-01-27 09:15:20 <Jane Doe> Hey "
>>>> c[4] <- "2016-01-27 09:15:22 <John Doe>  ended a video chat"
>>>> c[5] <- "2016-01-27 21:07:11 <Jane Doe>  started a video chat"
>>>> c[6] <- "2016-01-27 21:26:57 <John Doe>  ended a video chat"
>>>> 
>>>> 
>>>> # regex  ^(year)       (time)      <(word word)>\\s*(string)$
>>>> patt <- "^([0-9-]{10}) ([0-9:]{8}) <(\\w+ \\w+)>\\s*(.+)$"
>>>> proto <- data.frame(date = character(),
>>>>                   time = character(),
>>>>                   name = character(),
>>>>                   text = character(),
>>>>                   stringsAsFactors = TRUE)
>>>> d <- strcapture(patt, c, proto)
>>>> 
>>>> 
>>>> 
>>>>       date     time     name                               text
>>>> 1 2016-01-27 09:14:40 Jane Doe               started a video chat
>>>> 2 2016-01-27 09:15:20 Jane Doe https://lh3.googleusercontent.com/
>>>> 3 2016-01-27 09:15:20 Jane Doe                               Hey
>>>> 4 2016-01-27 09:15:22 John Doe                 ended a video chat
>>>> 5 2016-01-27 21:07:11 Jane Doe               started a video chat
>>>> 6 2016-01-27 21:26:57 John Doe                 ended a video chat
>>>> 
>>>> 
>>>> 
>>>> B.
>>>> 
>>>> 
>>>>> On 2019-05-18, at 18:32, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
>>>>> 
>>>>> Going back and thinking through what Boris and William were saying
>>>>> (also Ivan), I tried this:
>>>>> 
>>>>> a <- readLines ("hangouts-conversation-6.csv.txt")
>>>>> b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)"
>>>>> c <- gsub(b, "\\1<\\2> ", a)
>>>>>> head (c)
>>>>> [1] "???2016-01-27 09:14:40 *** Jane Doe started a video chat"
>>>>> [2] "2016-01-27 09:15:20 <Jane Doe>
>>>>> https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf"
>>>>> [3] "2016-01-27 09:15:20 <Jane Doe> Hey "
>>>>> [4] "2016-01-27 09:15:22 <John Doe>  ended a video chat"
>>>>> [5] "2016-01-27 21:07:11 <Jane Doe>  started a video chat"
>>>>> [6] "2016-01-27 21:26:57 <John Doe>  ended a video chat"
>>>>> 
>>>>> The ??? is still there, since I forgot to do what Ivan had suggested, namely,
>>>>> 
>>>>> a <- readLines(con <- file("hangouts-conversation-6.csv.txt", encoding
>>>>> = "UTF-8")); close(con); rm(con)
>>>>> 
>>>>> But then the new code is still turning out only NAs when I apply
>>>>> strcapture (). This was what happened next:
>>>>> 
>>>>>> d <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
>>>>> + [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
>>>>> +                 c, proto=data.frame(stringsAsFactors=FALSE, When="", Who="",
>>>>> +                                     What=""))
>>>>>> head (d)
>>>>> When  Who What
>>>>> 1 <NA> <NA> <NA>
>>>>> 2 <NA> <NA> <NA>
>>>>> 3 <NA> <NA> <NA>
>>>>> 4 <NA> <NA> <NA>
>>>>> 5 <NA> <NA> <NA>
>>>>> 6 <NA> <NA> <NA>
>>>>> 
>>>>> I've been reading up on regular expressions, too, so this code seems
>>>>> spot on. What's going wrong?
>>>>> 
>>>>> Michael
>>>>> 
>>>>> On Fri, May 17, 2019 at 4:28 PM Boris Steipe <boris.steipe at utoronto.ca> wrote:
>>>>>> 
>>>>>> Don't start putting in extra commas and then reading this as csv. That approach is broken. The correct approach is what Bill outlined: read everything with readLines(), and then use a proper regular expression with strcapture().
>>>>>> 
>>>>>> You need to pre-process the object that readLines() gives you: replace the contents of the videochat lines, and make it conform to the format of the other lines before you process it into your data frame.
>>>>>> 
>>>>>> Approximately something like
>>>>>> 
>>>>>> # read the raw data
>>>>>> tmp <- readLines("hangouts-conversation-6.csv.txt")
>>>>>> 
>>>>>> # process all video chat lines
>>>>>> patt <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+) "  # (year time )*** (word word)
>>>>>> tmp <- gsub(patt, "\\1<\\2> ", tmp)
>>>>>> 
>>>>>> # next, use strcapture()
>>>>>> 
>>>>>> Note that this makes the assumption that your names are always exactly two words containing only letters. If that assumption is not true, more though needs to go into the regex. But you can test that:
>>>>>> 
>>>>>> patt <- " <\\w+ \\w+> "   #" <word word> "
>>>>>> sum( ! grepl(patt, tmp)))
>>>>>> 
>>>>>> ... will give the number of lines that remain in your file that do not have a tag that can be interpreted as "Who"
>>>>>> 
>>>>>> Once that is fine, use Bill's approach - or a regular expression of your own design - to create your data frame.
>>>>>> 
>>>>>> Hope this helps,
>>>>>> Boris
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>>> On 2019-05-17, at 16:18, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
>>>>>>> 
>>>>>>> Very interesting. I'm sure I'll be trying to get rid of the byte order
>>>>>>> mark eventually. But right now, I'm more worried about getting the
>>>>>>> character vector into either a csv file or data.frame; that way, I can
>>>>>>> be able to work with the data neatly tabulated into four columns:
>>>>>>> date, time, person, comment. I assume it's a write.csv function, but I
>>>>>>> don't know what arguments to put in it. header=FALSE? fill=T?
>>>>>>> 
>>>>>>> Micheal
>>>>>>> 
>>>>>>> On Fri, May 17, 2019 at 1:03 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>>>>>>> 
>>>>>>>> If byte order mark is the issue then you can specify the file encoding as "UTF-8-BOM" and it won't show up in your data any more.
>>>>>>>> 
>>>>>>>> On May 17, 2019 12:12:17 PM PDT, William Dunlap via R-help <r-help at r-project.org> wrote:
>>>>>>>>> The pattern I gave worked for the lines that you originally showed from
>>>>>>>>> the
>>>>>>>>> data file ('a'), before you put commas into them.  If the name is
>>>>>>>>> either of
>>>>>>>>> the form "<name>" or "***" then the "(<[^>]*>)" needs to be changed so
>>>>>>>>> something like "(<[^>]*>|[*]{3})".
>>>>>>>>> 
>>>>>>>>> The " ???" at the start of the imported data may come from the byte
>>>>>>>>> order
>>>>>>>>> mark that Windows apps like to put at the front of a text file in UTF-8
>>>>>>>>> or
>>>>>>>>> UTF-16 format.
>>>>>>>>> 
>>>>>>>>> Bill Dunlap
>>>>>>>>> TIBCO Software
>>>>>>>>> wdunlap tibco.com
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> On Fri, May 17, 2019 at 11:53 AM Michael Boulineau <
>>>>>>>>> michael.p.boulineau at gmail.com> wrote:
>>>>>>>>> 
>>>>>>>>>> This seemed to work:
>>>>>>>>>> 
>>>>>>>>>>> a <- readLines ("hangouts-conversation-6.csv.txt")
>>>>>>>>>>> b <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4", a)
>>>>>>>>>>> b [1:84]
>>>>>>>>>> 
>>>>>>>>>> And the first 85 lines looks like this:
>>>>>>>>>> 
>>>>>>>>>> [83] "2016-06-28 21:02:28 *** Jane Doe started a video chat"
>>>>>>>>>> [84] "2016-06-28 21:12:43 *** John Doe ended a video chat"
>>>>>>>>>> 
>>>>>>>>>> Then they transition to the commas:
>>>>>>>>>> 
>>>>>>>>>>> b [84:100]
>>>>>>>>>> [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
>>>>>>>>>> [2] "2016-07-01,02:50:35,<John Doe>,hey"
>>>>>>>>>> [3] "2016-07-01,02:51:26,<John Doe>,waiting for plane to Edinburgh"
>>>>>>>>>> [4] "2016-07-01,02:51:45,<John Doe>,thinking about my boo"
>>>>>>>>>> 
>>>>>>>>>> Even the strange bit on line 6347 was caught by this:
>>>>>>>>>> 
>>>>>>>>>>> b [6346:6348]
>>>>>>>>>> [1] "2016-10-21,10:56:29,<John Doe>,John_Doe"
>>>>>>>>>> [2] "2016-10-21,10:56:37,<John Doe>,Admit#8242"
>>>>>>>>>> [3] "2016-10-21,11:00:13,<Jane Doe>,Okay so you have a discussion"
>>>>>>>>>> 
>>>>>>>>>> Perhaps most awesomely, the code catches spaces that are interposed
>>>>>>>>>> into the comment itself:
>>>>>>>>>> 
>>>>>>>>>>> b [4]
>>>>>>>>>> [1] "2016-01-27,09:15:20,<Jane Doe>,Hey "
>>>>>>>>>>> b [85]
>>>>>>>>>> [1] "2016-07-01,02:50:35,<John Doe>,hey"
>>>>>>>>>> 
>>>>>>>>>> Notice whether there is a space after the "hey" or not.
>>>>>>>>>> 
>>>>>>>>>> These are the first two lines:
>>>>>>>>>> 
>>>>>>>>>> [1] "???2016-01-27 09:14:40 *** Jane Doe started a video chat"
>>>>>>>>>> [2] "2016-01-27,09:15:20,<Jane
>>>>>>>>>> Doe>,
>>>>>>>>>> 
>>>>>>>>> https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf
>>>>>>>>>> "
>>>>>>>>>> 
>>>>>>>>>> So, who knows what happened with the ??? at the beginning of [1]
>>>>>>>>>> directly above. But notice how there are no commas in [1] but there
>>>>>>>>>> appear in [2]. I don't see why really long ones like [2] directly
>>>>>>>>>> above would be a problem, were they to be translated into a csv or
>>>>>>>>>> data frame column.
>>>>>>>>>> 
>>>>>>>>>> Now, with the commas in there, couldn't we write this into a csv or a
>>>>>>>>>> data.frame? Some of this data will end up being garbage, I imagine.
>>>>>>>>>> Like in [2] directly above. Or with [83] and [84] at the top of this
>>>>>>>>>> discussion post/email. Embarrassingly, I've been trying to convert
>>>>>>>>>> this into a data.frame or csv but I can't manage to. I've been using
>>>>>>>>>> the write.csv function, but I don't think I've been getting the
>>>>>>>>>> arguments correct.
>>>>>>>>>> 
>>>>>>>>>> At the end of the day, I would like a data.frame and/or csv with the
>>>>>>>>>> following four columns: date, time, person, comment.
>>>>>>>>>> 
>>>>>>>>>> I tried this, too:
>>>>>>>>>> 
>>>>>>>>>>> c <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
>>>>>>>>>> + [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
>>>>>>>>>> +                 a, proto=data.frame(stringsAsFactors=FALSE,
>>>>>>>>> When="",
>>>>>>>>>> Who="",
>>>>>>>>>> +                                     What=""))
>>>>>>>>>> 
>>>>>>>>>> But all I got was this:
>>>>>>>>>> 
>>>>>>>>>>> c [1:100, ]
>>>>>>>>>> When  Who What
>>>>>>>>>> 1   <NA> <NA> <NA>
>>>>>>>>>> 2   <NA> <NA> <NA>
>>>>>>>>>> 3   <NA> <NA> <NA>
>>>>>>>>>> 4   <NA> <NA> <NA>
>>>>>>>>>> 5   <NA> <NA> <NA>
>>>>>>>>>> 6   <NA> <NA> <NA>
>>>>>>>>>> 
>>>>>>>>>> It seems to have caught nothing.
>>>>>>>>>> 
>>>>>>>>>>> unique (c)
>>>>>>>>>> When  Who What
>>>>>>>>>> 1 <NA> <NA> <NA>
>>>>>>>>>> 
>>>>>>>>>> But I like that it converted into columns. That's a really great
>>>>>>>>>> format. With a little tweaking, it'd be a great code for this data
>>>>>>>>>> set.
>>>>>>>>>> 
>>>>>>>>>> Michael
>>>>>>>>>> 
>>>>>>>>>> On Fri, May 17, 2019 at 8:20 AM William Dunlap via R-help
>>>>>>>>>> <r-help at r-project.org> wrote:
>>>>>>>>>>> 
>>>>>>>>>>> Consider using readLines() and strcapture() for reading such a
>>>>>>>>> file.
>>>>>>>>>> E.g.,
>>>>>>>>>>> suppose readLines(files) produced a character vector like
>>>>>>>>>>> 
>>>>>>>>>>> x <- c("2016-10-21 10:35:36 <Jane Doe> What's your login",
>>>>>>>>>>>       "2016-10-21 10:56:29 <John Doe> John_Doe",
>>>>>>>>>>>       "2016-10-21 10:56:37 <John Doe> Admit#8242",
>>>>>>>>>>>       "October 23, 1819 12:34 <Jane Eyre> I am not an angel")
>>>>>>>>>>> 
>>>>>>>>>>> Then you can make a data.frame with columns When, Who, and What by
>>>>>>>>>>> supplying a pattern containing three parenthesized capture
>>>>>>>>> expressions:
>>>>>>>>>>>> z <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
>>>>>>>>>>> [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
>>>>>>>>>>>          x, proto=data.frame(stringsAsFactors=FALSE, When="",
>>>>>>>>> Who="",
>>>>>>>>>>> What=""))
>>>>>>>>>>>> str(z)
>>>>>>>>>>> 'data.frame':   4 obs. of  3 variables:
>>>>>>>>>>> $ When: chr  "2016-10-21 10:35:36" "2016-10-21 10:56:29"
>>>>>>>>> "2016-10-21
>>>>>>>>>>> 10:56:37" NA
>>>>>>>>>>> $ Who : chr  "<Jane Doe>" "<John Doe>" "<John Doe>" NA
>>>>>>>>>>> $ What: chr  "What's your login" "John_Doe" "Admit#8242" NA
>>>>>>>>>>> 
>>>>>>>>>>> Lines that don't match the pattern result in NA's - you might make
>>>>>>>>> a
>>>>>>>>>> second
>>>>>>>>>>> pass over the corresponding elements of x with a new pattern.
>>>>>>>>>>> 
>>>>>>>>>>> You can convert the When column from character to time with
>>>>>>>>> as.POSIXct().
>>>>>>>>>>> 
>>>>>>>>>>> Bill Dunlap
>>>>>>>>>>> TIBCO Software
>>>>>>>>>>> wdunlap tibco.com
>>>>>>>>>>> 
>>>>>>>>>>> 
>>>>>>>>>>> On Thu, May 16, 2019 at 8:30 PM David Winsemius
>>>>>>>>> <dwinsemius at comcast.net>
>>>>>>>>>>> wrote:
>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> On 5/16/19 3:53 PM, Michael Boulineau wrote:
>>>>>>>>>>>>> OK. So, I named the object test and then checked the 6347th
>>>>>>>>> item
>>>>>>>>>>>>> 
>>>>>>>>>>>>>> test <- readLines ("hangouts-conversation.txt)
>>>>>>>>>>>>>> test [6347]
>>>>>>>>>>>>> [1] "2016-10-21 10:56:37 <John Doe> Admit#8242"
>>>>>>>>>>>>> 
>>>>>>>>>>>>> Perhaps where it was getting screwed up is, since the end of
>>>>>>>>> this is
>>>>>>>>>> a
>>>>>>>>>>>>> number (8242), then, given that there's no space between the
>>>>>>>>> number
>>>>>>>>>>>>> and what ought to be the next row, R didn't know where to draw
>>>>>>>>> the
>>>>>>>>>>>>> line. Sure enough, it looks like this when I go to the original
>>>>>>>>> file
>>>>>>>>>>>>> and control f "#8242"
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 2016-10-21 10:35:36 <Jane Doe> What's your login
>>>>>>>>>>>>> 2016-10-21 10:56:29 <John Doe> John_Doe
>>>>>>>>>>>>> 2016-10-21 10:56:37 <John Doe> Admit#8242
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> An octothorpe is an end of line signifier and is interpreted as
>>>>>>>>>> allowing
>>>>>>>>>>>> comments. You can prevent that interpretation with suitable
>>>>>>>>> choice of
>>>>>>>>>>>> parameters to `read.table` or `read.csv`. I don't understand why
>>>>>>>>> that
>>>>>>>>>>>> should cause anu error or a failure to match that pattern.
>>>>>>>>>>>> 
>>>>>>>>>>>>> 2016-10-21 11:00:13 <Jane Doe> Okay so you have a discussion
>>>>>>>>>>>>> 
>>>>>>>>>>>>> Again, it doesn't look like that in the file. Gmail
>>>>>>>>> automatically
>>>>>>>>>>>>> formats it like that when I paste it in. More to the point, it
>>>>>>>>> looks
>>>>>>>>>>>>> like
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 2016-10-21 10:35:36 <Jane Doe> What's your login2016-10-21
>>>>>>>>> 10:56:29
>>>>>>>>>>>>> <John Doe> John_Doe2016-10-21 10:56:37 <John Doe>
>>>>>>>>>> Admit#82422016-10-21
>>>>>>>>>>>>> 11:00:13 <Jane Doe> Okay so you have a discussion
>>>>>>>>>>>>> 
>>>>>>>>>>>>> Notice Admit#82422016. So there's that.
>>>>>>>>>>>>> 
>>>>>>>>>>>>> Then I built object test2.
>>>>>>>>>>>>> 
>>>>>>>>>>>>> test2 <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4",
>>>>>>>>> test)
>>>>>>>>>>>>> 
>>>>>>>>>>>>> This worked for 84 lines, then this happened.
>>>>>>>>>>>> 
>>>>>>>>>>>> It may have done something but as you later discovered my first
>>>>>>>>> code
>>>>>>>>>> for
>>>>>>>>>>>> the pattern was incorrect. I had tested it (and pasted in the
>>>>>>>>> results
>>>>>>>>>> of
>>>>>>>>>>>> the test) . The way to refer to a capture class is with
>>>>>>>>> back-slashes
>>>>>>>>>>>> before the numbers, not forward-slashes. Try this:
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>>> newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)",
>>>>>>>>> "\\1,\\2,\\3,\\4",
>>>>>>>>>> chrvec)
>>>>>>>>>>>>> newvec
>>>>>>>>>>>> [1] "2016-07-01,02:50:35,<john>,hey"
>>>>>>>>>>>> [2] "2016-07-01,02:51:26,<jane>,waiting for plane to Edinburgh"
>>>>>>>>>>>> [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
>>>>>>>>>>>> [4] "2016-07-01,02:52:07,<jane>,nothing crappy has happened,
>>>>>>>>> not
>>>>>>>>>> really"
>>>>>>>>>>>> [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast,
>>>>>>>>> didn't
>>>>>>>>>> sleep"
>>>>>>>>>>>> [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or
>>>>>>>>> where I am
>>>>>>>>>>>> really"
>>>>>>>>>>>> [7] "2016-07-01,02:54:17,<john>,just know it's london"
>>>>>>>>>>>> [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
>>>>>>>>>>>> [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a good
>>>>>>>>> eay"
>>>>>>>>>>>> [10] "2016-07-01 02:58:56 <jone>"
>>>>>>>>>>>> [11] "2016-07-01 02:59:34 <jane>"
>>>>>>>>>>>> [12] "2016-07-01,03:02:48,<john>,British security is a little
>>>>>>>>> more
>>>>>>>>>>>> rigorous..."
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> I made note of the fact that the 10th and 11th lines had no
>>>>>>>>> commas.
>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>>> test2 [84]
>>>>>>>>>>>>> [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
>>>>>>>>>>>> 
>>>>>>>>>>>> That line didn't have any "<" so wasn't matched.
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> You could remove all none matching lines for pattern of
>>>>>>>>>>>> 
>>>>>>>>>>>> dates<space>times<space>"<"<name>">"<space><anything>
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> with:
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> chrvec <- chrvec[ grepl("^.{10} .{8} <.+> .+$)", chrvec)]
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> Do read:
>>>>>>>>>>>> 
>>>>>>>>>>>> ?read.csv
>>>>>>>>>>>> 
>>>>>>>>>>>> ?regex
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> --
>>>>>>>>>>>> 
>>>>>>>>>>>> David
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>>>> test2 [85]
>>>>>>>>>>>>> [1] "//1,//2,//3,//4"
>>>>>>>>>>>>>> test [85]
>>>>>>>>>>>>> [1] "2016-07-01 02:50:35 <John Doe> hey"
>>>>>>>>>>>>> 
>>>>>>>>>>>>> Notice how I toggled back and forth between test and test2
>>>>>>>>> there. So,
>>>>>>>>>>>>> whatever happened with the regex, it happened in the switch
>>>>>>>>> from 84
>>>>>>>>>> to
>>>>>>>>>>>>> 85, I guess. It went on like
>>>>>>>>>>>>> 
>>>>>>>>>>>>> [990] "//1,//2,//3,//4"
>>>>>>>>>>>>> [991] "//1,//2,//3,//4"
>>>>>>>>>>>>> [992] "//1,//2,//3,//4"
>>>>>>>>>>>>> [993] "//1,//2,//3,//4"
>>>>>>>>>>>>> [994] "//1,//2,//3,//4"
>>>>>>>>>>>>> [995] "//1,//2,//3,//4"
>>>>>>>>>>>>> [996] "//1,//2,//3,//4"
>>>>>>>>>>>>> [997] "//1,//2,//3,//4"
>>>>>>>>>>>>> [998] "//1,//2,//3,//4"
>>>>>>>>>>>>> [999] "//1,//2,//3,//4"
>>>>>>>>>>>>> [1000] "//1,//2,//3,//4"
>>>>>>>>>>>>> 
>>>>>>>>>>>>> up until line 1000, then I reached max.print.
>>>>>>>>>>>> 
>>>>>>>>>>>>> Michael
>>>>>>>>>>>>> 
>>>>>>>>>>>>> On Thu, May 16, 2019 at 1:05 PM David Winsemius <
>>>>>>>>>> dwinsemius at comcast.net>
>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> On 5/16/19 12:30 PM, Michael Boulineau wrote:
>>>>>>>>>>>>>>> Thanks for this tip on etiquette, David. I will be sure and
>>>>>>>>> not do
>>>>>>>>>>>> that again.
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> I tried the read.fwf from the foreign package, with a code
>>>>>>>>> like
>>>>>>>>>> this:
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> d <- read.fwf("hangouts-conversation.txt",
>>>>>>>>>>>>>>>               widths= c(10,10,20,40),
>>>>>>>>>>>>>>> 
>>>>>>>>> col.names=c("date","time","person","comment"),
>>>>>>>>>>>>>>>               strip.white=TRUE)
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> But it threw this error:
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> Error in scan(file = file, what = what, sep = sep, quote =
>>>>>>>>> quote,
>>>>>>>>>> dec
>>>>>>>>>>>> = dec,  :
>>>>>>>>>>>>>>> line 6347 did not have 4 elements
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> So what does line 6347 look like? (Use `readLines` and print
>>>>>>>>> it
>>>>>>>>>> out.)
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> Interestingly, though, the error only happened when I
>>>>>>>>> increased the
>>>>>>>>>>>>>>> width size. But I had to increase the size, or else I
>>>>>>>>> couldn't
>>>>>>>>>> "see"
>>>>>>>>>>>>>>> anything.  The comment was so small that nothing was being
>>>>>>>>>> captured by
>>>>>>>>>>>>>>> the size of the column. so to speak.
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> It seems like what's throwing me is that there's no comma
>>>>>>>>> that
>>>>>>>>>>>>>>> demarcates the end of the text proper. For example:
>>>>>>>>>>>>>> Not sure why you thought there should be a comma. Lines
>>>>>>>>> usually end
>>>>>>>>>>>>>> with  <cr> and or a <lf>.
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> Once you have the raw text in a character vector from
>>>>>>>>> `readLines`
>>>>>>>>>> named,
>>>>>>>>>>>>>> say, 'chrvec', then you could selectively substitute commas
>>>>>>>>> for
>>>>>>>>>> spaces
>>>>>>>>>>>>>> with regex. (Now that you no longer desire to remove the dates
>>>>>>>>> and
>>>>>>>>>>>> times.)
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", chrvec)
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> This will not do any replacements when the pattern is not
>>>>>>>>> matched.
>>>>>>>>>> See
>>>>>>>>>>>>>> this test:
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)",
>>>>>>>>> "\\1,\\2,\\3,\\4",
>>>>>>>>>>>> chrvec)
>>>>>>>>>>>>>>> newvec
>>>>>>>>>>>>>> [1] "2016-07-01,02:50:35,<john>,hey"
>>>>>>>>>>>>>> [2] "2016-07-01,02:51:26,<jane>,waiting for plane to
>>>>>>>>> Edinburgh"
>>>>>>>>>>>>>> [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
>>>>>>>>>>>>>> [4] "2016-07-01,02:52:07,<jane>,nothing crappy has
>>>>>>>>> happened, not
>>>>>>>>>>>> really"
>>>>>>>>>>>>>> [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast,
>>>>>>>>> didn't
>>>>>>>>>>>> sleep"
>>>>>>>>>>>>>> [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or
>>>>>>>>> where
>>>>>>>>>> I am
>>>>>>>>>>>>>> really"
>>>>>>>>>>>>>> [7] "2016-07-01,02:54:17,<john>,just know it's london"
>>>>>>>>>>>>>> [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
>>>>>>>>>>>>>> [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a
>>>>>>>>> good
>>>>>>>>>> eay"
>>>>>>>>>>>>>> [10] "2016-07-01 02:58:56 <jone>"
>>>>>>>>>>>>>> [11] "2016-07-01 02:59:34 <jane>"
>>>>>>>>>>>>>> [12] "2016-07-01,03:02:48,<john>,British security is a little
>>>>>>>>> more
>>>>>>>>>>>>>> rigorous..."
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> You should probably remove the "empty comment" lines.
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> --
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> David.
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> 2016-07-01 15:34:30 <John Doe> Lame. We were in a
>>>>>>>>>> starbucks2016-07-01
>>>>>>>>>>>>>>> 15:35:02 <Jane Doe> Hmm that's interesting2016-07-01 15:35:09
>>>>>>>>> <Jane
>>>>>>>>>>>>>>> Doe> You must want coffees2016-07-01 15:35:25 <John Doe>
>>>>>>>>> There was
>>>>>>>>>>>>>>> lots of Starbucks in my day2016-07-01 15:35:47
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> It was interesting, too, when I pasted the text into the
>>>>>>>>> email, it
>>>>>>>>>>>>>>> self-formatted into the way I wanted it to look. I had to
>>>>>>>>> manually
>>>>>>>>>>>>>>> make it look like it does above, since that's the way that it
>>>>>>>>>> looks in
>>>>>>>>>>>>>>> the txt file. I wonder if it's being organized by XML or
>>>>>>>>> something.
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> Anyways, There's always a space between the two sideways
>>>>>>>>> carrots,
>>>>>>>>>> just
>>>>>>>>>>>>>>> like there is right now: <John Doe> See. Space. And there's
>>>>>>>>> always
>>>>>>>>>> a
>>>>>>>>>>>>>>> space between the data and time. Like this. 2016-07-01
>>>>>>>>> 15:34:30
>>>>>>>>>> See.
>>>>>>>>>>>>>>> Space. But there's never a space between the end of the
>>>>>>>>> comment and
>>>>>>>>>>>>>>> the next date. Like this: We were in a starbucks2016-07-01
>>>>>>>>> 15:35:02
>>>>>>>>>>>>>>> See. starbucks and 2016 are smooshed together.
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> This code is also on the table right now too.
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> a <- read.table("E:/working
>>>>>>>>>>>>>>> directory/-189/hangouts-conversation2.txt", quote="\"",
>>>>>>>>>>>>>>> comment.char="", fill=TRUE)
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>> h<-cbind(hangouts.conversation2[,1:2],hangouts.conversation2[,3:5],hangouts.conversation2[,6:9])
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> aa<-gsub("[^[:digit:]]","",h)
>>>>>>>>>>>>>>> my.data.num <- as.numeric(str_extract(h, "[0-9]+"))
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> Those last lines are a work in progress. I wish I could
>>>>>>>>> import a
>>>>>>>>>>>>>>> picture of what it looks like when it's translated into a
>>>>>>>>> data
>>>>>>>>>> frame.
>>>>>>>>>>>>>>> The fill=TRUE helped to get the data in table that kind of
>>>>>>>>> sort of
>>>>>>>>>>>>>>> works, but the comments keep bleeding into the data and time
>>>>>>>>>> column.
>>>>>>>>>>>>>>> It's like
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> 2016-07-01 15:59:17 <Jane Doe> Seriously I've never been
>>>>>>>>>>>>>>> over               there
>>>>>>>>>>>>>>> 2016-07-01 15:59:27 <Jane Doe> It confuses me :(
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> And then, maybe, the "seriously" will be in a column all to
>>>>>>>>>> itself, as
>>>>>>>>>>>>>>> will be the "I've'"and the "never" etc.
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> I will use a regular expression if I have to, but it would be
>>>>>>>>> nice
>>>>>>>>>> to
>>>>>>>>>>>>>>> keep the dates and times on there. Originally, I thought they
>>>>>>>>> were
>>>>>>>>>>>>>>> meaningless, but I've since changed my mind on that count.
>>>>>>>>> The
>>>>>>>>>> time of
>>>>>>>>>>>>>>> day isn't so important. But, especially since, say, Gmail
>>>>>>>>> itself
>>>>>>>>>> knows
>>>>>>>>>>>>>>> how to quickly recognize what it is, I know it can be done. I
>>>>>>>>> know
>>>>>>>>>>>>>>> this data has structure to it.
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> Michael
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> On Wed, May 15, 2019 at 8:47 PM David Winsemius <
>>>>>>>>>>>> dwinsemius at comcast.net> wrote:
>>>>>>>>>>>>>>>> On 5/15/19 4:07 PM, Michael Boulineau wrote:
>>>>>>>>>>>>>>>>> I have a wild and crazy text file, the head of which looks
>>>>>>>>> like
>>>>>>>>>> this:
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> 2016-07-01 02:50:35 <john> hey
>>>>>>>>>>>>>>>>> 2016-07-01 02:51:26 <jane> waiting for plane to Edinburgh
>>>>>>>>>>>>>>>>> 2016-07-01 02:51:45 <john> thinking about my boo
>>>>>>>>>>>>>>>>> 2016-07-01 02:52:07 <jane> nothing crappy has happened, not
>>>>>>>>>> really
>>>>>>>>>>>>>>>>> 2016-07-01 02:52:20 <john> plane went by pretty fast,
>>>>>>>>> didn't
>>>>>>>>>> sleep
>>>>>>>>>>>>>>>>> 2016-07-01 02:54:08 <jane> no idea what time it is or where
>>>>>>>>> I am
>>>>>>>>>>>> really
>>>>>>>>>>>>>>>>> 2016-07-01 02:54:17 <john> just know it's london
>>>>>>>>>>>>>>>>> 2016-07-01 02:56:44 <jane> you are probably asleep
>>>>>>>>>>>>>>>>> 2016-07-01 02:58:45 <jane> I hope fish was fishy in a good
>>>>>>>>> eay
>>>>>>>>>>>>>>>>> 2016-07-01 02:58:56 <jone>
>>>>>>>>>>>>>>>>> 2016-07-01 02:59:34 <jane>
>>>>>>>>>>>>>>>>> 2016-07-01 03:02:48 <john> British security is a little
>>>>>>>>> more
>>>>>>>>>>>> rigorous...
>>>>>>>>>>>>>>>> Looks entirely not-"crazy". Typical log file format.
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> Two possibilities: 1) Use `read.fwf` from pkg foreign; 2)
>>>>>>>>> Use
>>>>>>>>>> regex
>>>>>>>>>>>>>>>> (i.e. the sub-function) to strip everything up to the "<".
>>>>>>>>> Read
>>>>>>>>>>>>>>>> `?regex`. Since that's not a metacharacters you could use a
>>>>>>>>>> pattern
>>>>>>>>>>>>>>>> ".+<" and replace with "".
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> And do read the Posting Guide. Cross-posting to
>>>>>>>>> StackOverflow and
>>>>>>>>>>>> Rhelp,
>>>>>>>>>>>>>>>> at least within hours of each, is considered poor manners.
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> --
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> David.
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> It goes on for a while. It's a big file. But I feel like
>>>>>>>>> it's
>>>>>>>>>> going
>>>>>>>>>>>> to
>>>>>>>>>>>>>>>>> be difficult to annotate with the coreNLP library or
>>>>>>>>> package. I'm
>>>>>>>>>>>>>>>>> doing natural language processing. In other words, I'm
>>>>>>>>> curious
>>>>>>>>>> as to
>>>>>>>>>>>>>>>>> how I would shave off the dates, that is, to make it look
>>>>>>>>> like:
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> <john> hey
>>>>>>>>>>>>>>>>> <jane> waiting for plane to Edinburgh
>>>>>>>>>>>>>>>>> <john> thinking about my boo
>>>>>>>>>>>>>>>>> <jane> nothing crappy has happened, not really
>>>>>>>>>>>>>>>>> <john> plane went by pretty fast, didn't sleep
>>>>>>>>>>>>>>>>> <jane> no idea what time it is or where I am really
>>>>>>>>>>>>>>>>> <john> just know it's london
>>>>>>>>>>>>>>>>> <jane> you are probably asleep
>>>>>>>>>>>>>>>>> <jane> I hope fish was fishy in a good eay
>>>>>>>>>>>>>>>>> <jone>
>>>>>>>>>>>>>>>>> <jane>
>>>>>>>>>>>>>>>>> <john> British security is a little more rigorous...
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> To be clear, then, I'm trying to clean a large text file by
>>>>>>>>>> writing a
>>>>>>>>>>>>>>>>> regular expression? such that I create a new object with no
>>>>>>>>>> numbers
>>>>>>>>>>>> or
>>>>>>>>>>>>>>>>> dates.
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> Michael
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> ______________________________________________
>>>>>>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
>>>>>>>>> more,
>>>>>>>>>> see
>>>>>>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>>>>>>>>> and provide commented, minimal, self-contained,
>>>>>>>>> reproducible
>>>>>>>>>> code.
>>>>>>>>>>>>>>> ______________________________________________
>>>>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>>>>>>>> see
>>>>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>>>>>> code.
>>>>>>>>>>>>>> ______________________________________________
>>>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>>>>>>>> see
>>>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>>>>>> code.
>>>>>>>>>>>>> ______________________________________________
>>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>>>>>>>> see
>>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>>>>>> code.
>>>>>>>>>>>> 
>>>>>>>>>>>> ______________________________________________
>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>>>>>> code.
>>>>>>>>>>>> 
>>>>>>>>>>> 
>>>>>>>>>>>     [[alternative HTML version deleted]]
>>>>>>>>>>> 
>>>>>>>>>>> ______________________________________________
>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>>>>> 
>>>>>>>>>> ______________________________________________
>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>>   [[alternative HTML version deleted]]
>>>>>>>>> 
>>>>>>>>> ______________________________________________
>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>>> 
>>>>>>>> --
>>>>>>>> Sent from my phone. Please excuse my brevity.
>>>>>>> 
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>> 
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From m|ch@e|@p@bou||ne@u @end|ng |rom gm@||@com  Mon May 20 00:11:03 2019
From: m|ch@e|@p@bou||ne@u @end|ng |rom gm@||@com (Michael Boulineau)
Date: Sun, 19 May 2019 15:11:03 -0700
Subject: [R] how to separate string from numbers in a large txt file
In-Reply-To: <13D5D19C-0EA2-4234-9EDC-539A01DB7690@utoronto.ca>
References: <CAH+cTGM9sFCxegaMHzu-_9Prhvvh9MbVDCWJ1_x7wkO9jC47mQ@mail.gmail.com>
 <c6e69644-66a1-7031-4d7c-1000e0abbea4@comcast.net>
 <CAH+cTGM5YvWV_G0pRgMqjCzuRrFTzd0nOc7yoThfGbGMUyMjsQ@mail.gmail.com>
 <9c9f1c81-309f-91cb-7e26-f02fc4bd45d8@comcast.net>
 <CAH+cTGMx4_Lz2gGDf2AuUaaZF2Xaq4McuPRoHu4JJNc-ORd=Ew@mail.gmail.com>
 <6322e735-5659-848e-3a50-9f066f1299ad@comcast.net>
 <CAF8bMcbgrVcN9J7Aygq2D5RWaU4MS-GH+_+zMHU=AO+DU5DVWw@mail.gmail.com>
 <CAH+cTGNfpLSurqmsoMeci0GWhC0ncOp7zTEju3fJW5T4+tCsGA@mail.gmail.com>
 <CAF8bMcaSJRaUXK4h2ssaDoJwXaKCUZT2Vq1cutNiNDjQ7ga2Xg@mail.gmail.com>
 <EEA9FD46-A67C-4FB7-8245-350819C4EA62@dcn.davis.ca.us>
 <CAH+cTGNOkAnPnn-aru-BJUbhrgeYMQBTqjSYT764nKzcdXvTvA@mail.gmail.com>
 <9B170703-AAAF-4D52-9744-1FDE70559B05@utoronto.ca>
 <CAH+cTGNaw7p-it4B7te4BBsay=v6J-5OS-P_WGDJJ6Z0ymxtvw@mail.gmail.com>
 <0F8CD89C-BDE2-44D0-8E18-0CA2706EB53F@utoronto.ca>
 <CAH+cTGMsD+PkTKoFfB8=3WeWPQAWjhmuOCKZfnB7jR03m7RtDQ@mail.gmail.com>
 <C5A52A04-CB49-442F-81CE-26CBC619C47A@utoronto.ca>
 <CAH+cTGOijsRxzJy3w9QS11axwD+Z36-Pive6dOhm2xK6Jm5PAg@mail.gmail.com>
 <13D5D19C-0EA2-4234-9EDC-539A01DB7690@utoronto.ca>
Message-ID: <CAH+cTGN+QPsi6DCRViGM_0wbhvGfe2m0K8pTm+j-x=noQFQNKg@mail.gmail.com>

For context:

> In gsub(b, "\\1<\\2> ", a) the work is done by the backreferences \\1 and \\2. The expression says:
> Substitute ALL of the match with the first captured expression, then " <", then the second captured expression, then "> ". The rest of the line is >not substituted and appears as-is.

Back to me: I guess what's giving me trouble is where to draw the line
in terms of the end or edge of the expression. Given the code, then,

> a <- readLines ("hangouts-conversation-6.txt", encoding = "UTF-8")
> b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)"
> c <- gsub(b, "\\1<\\2> ", a)

to me, it would seem as though this is the first captured expression,
that is, as though \\1 refers back to ^([0-9-]{10} [0-9:]{8} ), since
there are parenthesis around it, or since [0-9-]{10} [0-9:]{8} is
enclosed in parentheses. Then it would seem as though [*]{3} is the
second expression, and (\\w+ \\w+) is the third. According to this
(admittedly wrong) logic, it would seem as though the <> would go
around the date--like

> 2016-03-20 <19:29:37> *** Jane Doe started a video chat

The back references here recalls Davis's code earlier:

> sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", chrvec)

There, commas were put around everything, and there you can see the
edge of the expression very well. ^(.{10}) = first. (.{8}) = second.
(<.+>) = third. (.+$) = fourth. So, by the same logic, it would seem
as though in

> b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)"

that ^([0-9-]{10} [0-9:]{8} ) is first, that [*]{3} is second, and
that  (\\w+ \\w+) is third.

But, if Boris is to be right, and he is, obviously, then it would have
to be the case that this entire thing, namely, ^([0-9-]{10} [0-9:]{8}
)[*]{3}, is the first expression, since only if that were true would
the <> be able to go around the names, as in

[3] "2016-01-27 09:15:20 <Jane Doe> Hey "

Again, so 2016-01-27 09:15:20 would have to be an entire unit, an
expression. So I guess what I don't understand is how ^([0-9-]{10}
[0-9:]{8} )[*]{3} can be an entire expression, although my hunch would
be that it has something to do with the ^ or with the space after the
} and before the (, as in

> {3} (\\w+

Back to earlier:

> The rest of the line is not substituted and appears as-is.

Is that due to the space after the \\2? in

> "\\1<\\2> "

Notice space after > and before "

Michael

On Sun, May 19, 2019 at 2:31 PM Boris Steipe <boris.steipe at utoronto.ca> wrote:
>
> Inline ...
>
> > On 2019-05-19, at 13:56, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
> >
> >> b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)"
> >
> > so the ^ signals that the regex BEGINS with a number (that could be
> > any number, 0-9) that is only 10 characters long (then there's the
> > dash in there, too, with the 0-9-, which I assume enabled the regex to
> > grab the - that's between the numbers in the date)
>
> That's right. Note that within a "character class" the hyphen can have tow meanings: normally it defines a range of characters, but if it appears as the last character before "]" it is a literal hyphen.
>
> > , followed by a
> > single space, followed by a unit that could be any number, again, but
> > that is only 8 characters long this time. For that one, it will
> > include the colon, hence the 9:, although for that one ([0-9:]{8} ),
>
> Right.
>
>
> > I
> > don't get why the space is on the inside in that one, after the {8},
>
> The space needs to be preserved between the time and the name. I wrote
> b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)" # space in the first captured expression
> c <- gsub(b, "\\1<\\2> ", a)
>  ... but I could have written
> b <- "^([0-9-]{10} [0-9:]{8})[*]{3} (\\w+ \\w+)"
> c <- gsub(b, "\\1 <\\2> ", a)  # space in the substituted string
> ... same result
>
>
> > whereas the space is on the outside with the other one ^([0-9-]{10} ,
> > directly after the {10}. Why is that?
>
> In the second case, I capture without a space, because I don't want the space in the results, after the time.
>
>
> >
> > Then three *** [*]{3}, then the (\\w+ \\w+)", which Boris explained so
> > well above. I guess I still don't get why this one seemed to have
> > deleted the *** out of the mix, plus I still don't why it didn't
> > remove the *** from the first one.
>
> Because the entire first line was not matched since it had a malformed character preceding the date.
>
> >
> > 2016-03-20 19:29:37 *** Jane Doe started a video chat
> > 2016-03-20 19:30:35 *** John Doe ended a video chat
> > 2016-04-02 12:59:36 *** Jane Doe started a video chat
> > 2016-04-02 13:00:43 *** John Doe ended a video chat
> > 2016-04-02 13:01:08 *** Jane Doe started a video chat
> > 2016-04-02 13:01:41 *** John Doe ended a video chat
> > 2016-04-02 13:03:51 *** John Doe started a video chat
> > 2016-04-02 13:06:35 *** John Doe ended a video chat
> >
> > This is a random sample from the beginning of the txt file with no
> > edits. The ***s were deleted, all but the first one, the one that had
> > the ??? but that was taken out by the encoding = "UTF-8". I know that
> > the function was c <- gsub(b, "\\1<\\2> ", a), so it had a gsub () on
> > there, the point of which is to do substitution work.
> >
> > Oh, I get it, I think. The \\1<\\2> in the gsub () puts the <> around
> > the names, so that it's consistent with the rest of the data, so that
> > the names in the text about that aren't enclosed in the <> are
> > enclosed like the rest of them. But I still don't get why or how the
> > gsub () replaced the *** with the <>...
>
> In gsub(b, "\\1<\\2> ", a) the work is done by the backreferences \\1 and \\2. The expression says:
> Substitute ALL of the match with the first captured expression, then " <", then the second captured expression, then "> ". The rest of the line is not substituted and appears as-is.
>
>
> >
> > This one is more straightforward.
> >
> >> d <- "^([0-9-]{10}) ([0-9:]{8}) <(\\w+ \\w+)>\\s*(.+)$"
> >
> > any number with - for 10 characters, followed by a space. Oh, there's
> > no space in this one ([0-9:]{8}), after the {8}. Hu. So, then, any
> > number with : for 8 characters, followed by any two words separated by
> > a space and enclosed in <>. And then the \\s* is followed by a single
> > space? Or maybe it puts space on both sides (on the side of the #s to
> > the left, and then the comment to the right). The (.+)$ is anything
> > whatsoever until the end.
>
> \s is the metacharacter for "whitespace". \s* means zero or more whitespace. I'm matching that OUTSIDE of the captured expression, to removes any leading spaces from the data that goes into the data frame.
>
>
> Cheers,
> Boris
>
>
>
>
> >
> > Michael
> >
> >
> > On Sun, May 19, 2019 at 4:37 AM Boris Steipe <boris.steipe at utoronto.ca> wrote:
> >>
> >> Inline
> >>
> >>
> >>
> >>> On 2019-05-18, at 20:34, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
> >>>
> >>> It appears to have worked, although there were three little quirks.
> >>> The ; close(con); rm(con) didn't work for me; the first row of the
> >>> data.frame was all NAs, when all was said and done;
> >>
> >> You will get NAs for lines that can't be matched to the regular expression. That's a good thing, it allows you to test whether your assumptions were valid for the entire file:
> >>
> >> # number of failed strcapture()
> >> sum(is.na(e$date))
> >>
> >>
> >>> and then there
> >>> were still three *** on the same line where the ??? was apparently
> >>> deleted.
> >>
> >> This is a sign that something else happened with the line that prevented the regex from matching. In that case you need to investigate more. I see an invalid multibyte character at the beginning of the line you posted below.
> >>
> >>>
> >>>> a <- readLines ("hangouts-conversation-6.txt", encoding = "UTF-8")
> >>>> b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)"
> >>>> c <- gsub(b, "\\1<\\2> ", a)
> >>>> head (c)
> >>> [1] "?2016-01-27 09:14:40 *** Jane Doe started a video chat"
> >>> [2] "2016-01-27 09:15:20 <Jane Doe>
> >>> https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf"
> >>
> >> [...]
> >>
> >>> But, before I do anything else, I'm going to study the regex in this
> >>> particular code. For example, I'm still not sure why there has to the
> >>> second \\w+ in the (\\w+ \\w+). Little things like that.
> >>
> >> \w is the metacharacter for alphanumeric characters, \w+ designates something we could call a word. Thus \w+ \w+ are two words separated by a single blank. This corresponds to your example, but, as I wrote previously, you need to think very carefully whether this covers all possible cases (Could there be only one word? More than one blank? Could letters be separated by hyphens or periods?) In most cases we could have more robustly matched everything between "<" and ">" (taking care to test what happens if the message contains those characters). But for the video chat lines we need to make an assumption about what is name and what is not. If "started a video chat" is the only possibility in such lines, you can use this information instead. If there are other possibilities, you need a different strategy. In NLP there is no one-approach-fits-all.
> >>
> >> To validate the structure of the names in your transcripts, you can look at
> >>
> >> patt <- " <.+?> "   # " <any string, not greedy> "
> >> m <- regexpr(patt, c)
> >> unique(regmatches(c, m))
> >>
> >>
> >>
> >> B.
> >>
> >>
> >>
> >>>
> >>> Michael
> >>>
> >>>
> >>> On Sat, May 18, 2019 at 4:30 PM Boris Steipe <boris.steipe at utoronto.ca> wrote:
> >>>>
> >>>> This works for me:
> >>>>
> >>>> # sample data
> >>>> c <- character()
> >>>> c[1] <- "2016-01-27 09:14:40 <Jane Doe> started a video chat"
> >>>> c[2] <- "2016-01-27 09:15:20 <Jane Doe> https://lh3.googleusercontent.com/"
> >>>> c[3] <- "2016-01-27 09:15:20 <Jane Doe> Hey "
> >>>> c[4] <- "2016-01-27 09:15:22 <John Doe>  ended a video chat"
> >>>> c[5] <- "2016-01-27 21:07:11 <Jane Doe>  started a video chat"
> >>>> c[6] <- "2016-01-27 21:26:57 <John Doe>  ended a video chat"
> >>>>
> >>>>
> >>>> # regex  ^(year)       (time)      <(word word)>\\s*(string)$
> >>>> patt <- "^([0-9-]{10}) ([0-9:]{8}) <(\\w+ \\w+)>\\s*(.+)$"
> >>>> proto <- data.frame(date = character(),
> >>>>                   time = character(),
> >>>>                   name = character(),
> >>>>                   text = character(),
> >>>>                   stringsAsFactors = TRUE)
> >>>> d <- strcapture(patt, c, proto)
> >>>>
> >>>>
> >>>>
> >>>>       date     time     name                               text
> >>>> 1 2016-01-27 09:14:40 Jane Doe               started a video chat
> >>>> 2 2016-01-27 09:15:20 Jane Doe https://lh3.googleusercontent.com/
> >>>> 3 2016-01-27 09:15:20 Jane Doe                               Hey
> >>>> 4 2016-01-27 09:15:22 John Doe                 ended a video chat
> >>>> 5 2016-01-27 21:07:11 Jane Doe               started a video chat
> >>>> 6 2016-01-27 21:26:57 John Doe                 ended a video chat
> >>>>
> >>>>
> >>>>
> >>>> B.
> >>>>
> >>>>
> >>>>> On 2019-05-18, at 18:32, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
> >>>>>
> >>>>> Going back and thinking through what Boris and William were saying
> >>>>> (also Ivan), I tried this:
> >>>>>
> >>>>> a <- readLines ("hangouts-conversation-6.csv.txt")
> >>>>> b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)"
> >>>>> c <- gsub(b, "\\1<\\2> ", a)
> >>>>>> head (c)
> >>>>> [1] "???2016-01-27 09:14:40 *** Jane Doe started a video chat"
> >>>>> [2] "2016-01-27 09:15:20 <Jane Doe>
> >>>>> https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf"
> >>>>> [3] "2016-01-27 09:15:20 <Jane Doe> Hey "
> >>>>> [4] "2016-01-27 09:15:22 <John Doe>  ended a video chat"
> >>>>> [5] "2016-01-27 21:07:11 <Jane Doe>  started a video chat"
> >>>>> [6] "2016-01-27 21:26:57 <John Doe>  ended a video chat"
> >>>>>
> >>>>> The ??? is still there, since I forgot to do what Ivan had suggested, namely,
> >>>>>
> >>>>> a <- readLines(con <- file("hangouts-conversation-6.csv.txt", encoding
> >>>>> = "UTF-8")); close(con); rm(con)
> >>>>>
> >>>>> But then the new code is still turning out only NAs when I apply
> >>>>> strcapture (). This was what happened next:
> >>>>>
> >>>>>> d <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
> >>>>> + [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
> >>>>> +                 c, proto=data.frame(stringsAsFactors=FALSE, When="", Who="",
> >>>>> +                                     What=""))
> >>>>>> head (d)
> >>>>> When  Who What
> >>>>> 1 <NA> <NA> <NA>
> >>>>> 2 <NA> <NA> <NA>
> >>>>> 3 <NA> <NA> <NA>
> >>>>> 4 <NA> <NA> <NA>
> >>>>> 5 <NA> <NA> <NA>
> >>>>> 6 <NA> <NA> <NA>
> >>>>>
> >>>>> I've been reading up on regular expressions, too, so this code seems
> >>>>> spot on. What's going wrong?
> >>>>>
> >>>>> Michael
> >>>>>
> >>>>> On Fri, May 17, 2019 at 4:28 PM Boris Steipe <boris.steipe at utoronto.ca> wrote:
> >>>>>>
> >>>>>> Don't start putting in extra commas and then reading this as csv. That approach is broken. The correct approach is what Bill outlined: read everything with readLines(), and then use a proper regular expression with strcapture().
> >>>>>>
> >>>>>> You need to pre-process the object that readLines() gives you: replace the contents of the videochat lines, and make it conform to the format of the other lines before you process it into your data frame.
> >>>>>>
> >>>>>> Approximately something like
> >>>>>>
> >>>>>> # read the raw data
> >>>>>> tmp <- readLines("hangouts-conversation-6.csv.txt")
> >>>>>>
> >>>>>> # process all video chat lines
> >>>>>> patt <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+) "  # (year time )*** (word word)
> >>>>>> tmp <- gsub(patt, "\\1<\\2> ", tmp)
> >>>>>>
> >>>>>> # next, use strcapture()
> >>>>>>
> >>>>>> Note that this makes the assumption that your names are always exactly two words containing only letters. If that assumption is not true, more though needs to go into the regex. But you can test that:
> >>>>>>
> >>>>>> patt <- " <\\w+ \\w+> "   #" <word word> "
> >>>>>> sum( ! grepl(patt, tmp)))
> >>>>>>
> >>>>>> ... will give the number of lines that remain in your file that do not have a tag that can be interpreted as "Who"
> >>>>>>
> >>>>>> Once that is fine, use Bill's approach - or a regular expression of your own design - to create your data frame.
> >>>>>>
> >>>>>> Hope this helps,
> >>>>>> Boris
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>> On 2019-05-17, at 16:18, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
> >>>>>>>
> >>>>>>> Very interesting. I'm sure I'll be trying to get rid of the byte order
> >>>>>>> mark eventually. But right now, I'm more worried about getting the
> >>>>>>> character vector into either a csv file or data.frame; that way, I can
> >>>>>>> be able to work with the data neatly tabulated into four columns:
> >>>>>>> date, time, person, comment. I assume it's a write.csv function, but I
> >>>>>>> don't know what arguments to put in it. header=FALSE? fill=T?
> >>>>>>>
> >>>>>>> Micheal
> >>>>>>>
> >>>>>>> On Fri, May 17, 2019 at 1:03 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> >>>>>>>>
> >>>>>>>> If byte order mark is the issue then you can specify the file encoding as "UTF-8-BOM" and it won't show up in your data any more.
> >>>>>>>>
> >>>>>>>> On May 17, 2019 12:12:17 PM PDT, William Dunlap via R-help <r-help at r-project.org> wrote:
> >>>>>>>>> The pattern I gave worked for the lines that you originally showed from
> >>>>>>>>> the
> >>>>>>>>> data file ('a'), before you put commas into them.  If the name is
> >>>>>>>>> either of
> >>>>>>>>> the form "<name>" or "***" then the "(<[^>]*>)" needs to be changed so
> >>>>>>>>> something like "(<[^>]*>|[*]{3})".
> >>>>>>>>>
> >>>>>>>>> The " ???" at the start of the imported data may come from the byte
> >>>>>>>>> order
> >>>>>>>>> mark that Windows apps like to put at the front of a text file in UTF-8
> >>>>>>>>> or
> >>>>>>>>> UTF-16 format.
> >>>>>>>>>
> >>>>>>>>> Bill Dunlap
> >>>>>>>>> TIBCO Software
> >>>>>>>>> wdunlap tibco.com
> >>>>>>>>>
> >>>>>>>>>
> >>>>>>>>> On Fri, May 17, 2019 at 11:53 AM Michael Boulineau <
> >>>>>>>>> michael.p.boulineau at gmail.com> wrote:
> >>>>>>>>>
> >>>>>>>>>> This seemed to work:
> >>>>>>>>>>
> >>>>>>>>>>> a <- readLines ("hangouts-conversation-6.csv.txt")
> >>>>>>>>>>> b <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4", a)
> >>>>>>>>>>> b [1:84]
> >>>>>>>>>>
> >>>>>>>>>> And the first 85 lines looks like this:
> >>>>>>>>>>
> >>>>>>>>>> [83] "2016-06-28 21:02:28 *** Jane Doe started a video chat"
> >>>>>>>>>> [84] "2016-06-28 21:12:43 *** John Doe ended a video chat"
> >>>>>>>>>>
> >>>>>>>>>> Then they transition to the commas:
> >>>>>>>>>>
> >>>>>>>>>>> b [84:100]
> >>>>>>>>>> [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
> >>>>>>>>>> [2] "2016-07-01,02:50:35,<John Doe>,hey"
> >>>>>>>>>> [3] "2016-07-01,02:51:26,<John Doe>,waiting for plane to Edinburgh"
> >>>>>>>>>> [4] "2016-07-01,02:51:45,<John Doe>,thinking about my boo"
> >>>>>>>>>>
> >>>>>>>>>> Even the strange bit on line 6347 was caught by this:
> >>>>>>>>>>
> >>>>>>>>>>> b [6346:6348]
> >>>>>>>>>> [1] "2016-10-21,10:56:29,<John Doe>,John_Doe"
> >>>>>>>>>> [2] "2016-10-21,10:56:37,<John Doe>,Admit#8242"
> >>>>>>>>>> [3] "2016-10-21,11:00:13,<Jane Doe>,Okay so you have a discussion"
> >>>>>>>>>>
> >>>>>>>>>> Perhaps most awesomely, the code catches spaces that are interposed
> >>>>>>>>>> into the comment itself:
> >>>>>>>>>>
> >>>>>>>>>>> b [4]
> >>>>>>>>>> [1] "2016-01-27,09:15:20,<Jane Doe>,Hey "
> >>>>>>>>>>> b [85]
> >>>>>>>>>> [1] "2016-07-01,02:50:35,<John Doe>,hey"
> >>>>>>>>>>
> >>>>>>>>>> Notice whether there is a space after the "hey" or not.
> >>>>>>>>>>
> >>>>>>>>>> These are the first two lines:
> >>>>>>>>>>
> >>>>>>>>>> [1] "???2016-01-27 09:14:40 *** Jane Doe started a video chat"
> >>>>>>>>>> [2] "2016-01-27,09:15:20,<Jane
> >>>>>>>>>> Doe>,
> >>>>>>>>>>
> >>>>>>>>> https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf
> >>>>>>>>>> "
> >>>>>>>>>>
> >>>>>>>>>> So, who knows what happened with the ??? at the beginning of [1]
> >>>>>>>>>> directly above. But notice how there are no commas in [1] but there
> >>>>>>>>>> appear in [2]. I don't see why really long ones like [2] directly
> >>>>>>>>>> above would be a problem, were they to be translated into a csv or
> >>>>>>>>>> data frame column.
> >>>>>>>>>>
> >>>>>>>>>> Now, with the commas in there, couldn't we write this into a csv or a
> >>>>>>>>>> data.frame? Some of this data will end up being garbage, I imagine.
> >>>>>>>>>> Like in [2] directly above. Or with [83] and [84] at the top of this
> >>>>>>>>>> discussion post/email. Embarrassingly, I've been trying to convert
> >>>>>>>>>> this into a data.frame or csv but I can't manage to. I've been using
> >>>>>>>>>> the write.csv function, but I don't think I've been getting the
> >>>>>>>>>> arguments correct.
> >>>>>>>>>>
> >>>>>>>>>> At the end of the day, I would like a data.frame and/or csv with the
> >>>>>>>>>> following four columns: date, time, person, comment.
> >>>>>>>>>>
> >>>>>>>>>> I tried this, too:
> >>>>>>>>>>
> >>>>>>>>>>> c <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
> >>>>>>>>>> + [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
> >>>>>>>>>> +                 a, proto=data.frame(stringsAsFactors=FALSE,
> >>>>>>>>> When="",
> >>>>>>>>>> Who="",
> >>>>>>>>>> +                                     What=""))
> >>>>>>>>>>
> >>>>>>>>>> But all I got was this:
> >>>>>>>>>>
> >>>>>>>>>>> c [1:100, ]
> >>>>>>>>>> When  Who What
> >>>>>>>>>> 1   <NA> <NA> <NA>
> >>>>>>>>>> 2   <NA> <NA> <NA>
> >>>>>>>>>> 3   <NA> <NA> <NA>
> >>>>>>>>>> 4   <NA> <NA> <NA>
> >>>>>>>>>> 5   <NA> <NA> <NA>
> >>>>>>>>>> 6   <NA> <NA> <NA>
> >>>>>>>>>>
> >>>>>>>>>> It seems to have caught nothing.
> >>>>>>>>>>
> >>>>>>>>>>> unique (c)
> >>>>>>>>>> When  Who What
> >>>>>>>>>> 1 <NA> <NA> <NA>
> >>>>>>>>>>
> >>>>>>>>>> But I like that it converted into columns. That's a really great
> >>>>>>>>>> format. With a little tweaking, it'd be a great code for this data
> >>>>>>>>>> set.
> >>>>>>>>>>
> >>>>>>>>>> Michael
> >>>>>>>>>>
> >>>>>>>>>> On Fri, May 17, 2019 at 8:20 AM William Dunlap via R-help
> >>>>>>>>>> <r-help at r-project.org> wrote:
> >>>>>>>>>>>
> >>>>>>>>>>> Consider using readLines() and strcapture() for reading such a
> >>>>>>>>> file.
> >>>>>>>>>> E.g.,
> >>>>>>>>>>> suppose readLines(files) produced a character vector like
> >>>>>>>>>>>
> >>>>>>>>>>> x <- c("2016-10-21 10:35:36 <Jane Doe> What's your login",
> >>>>>>>>>>>       "2016-10-21 10:56:29 <John Doe> John_Doe",
> >>>>>>>>>>>       "2016-10-21 10:56:37 <John Doe> Admit#8242",
> >>>>>>>>>>>       "October 23, 1819 12:34 <Jane Eyre> I am not an angel")
> >>>>>>>>>>>
> >>>>>>>>>>> Then you can make a data.frame with columns When, Who, and What by
> >>>>>>>>>>> supplying a pattern containing three parenthesized capture
> >>>>>>>>> expressions:
> >>>>>>>>>>>> z <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
> >>>>>>>>>>> [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
> >>>>>>>>>>>          x, proto=data.frame(stringsAsFactors=FALSE, When="",
> >>>>>>>>> Who="",
> >>>>>>>>>>> What=""))
> >>>>>>>>>>>> str(z)
> >>>>>>>>>>> 'data.frame':   4 obs. of  3 variables:
> >>>>>>>>>>> $ When: chr  "2016-10-21 10:35:36" "2016-10-21 10:56:29"
> >>>>>>>>> "2016-10-21
> >>>>>>>>>>> 10:56:37" NA
> >>>>>>>>>>> $ Who : chr  "<Jane Doe>" "<John Doe>" "<John Doe>" NA
> >>>>>>>>>>> $ What: chr  "What's your login" "John_Doe" "Admit#8242" NA
> >>>>>>>>>>>
> >>>>>>>>>>> Lines that don't match the pattern result in NA's - you might make
> >>>>>>>>> a
> >>>>>>>>>> second
> >>>>>>>>>>> pass over the corresponding elements of x with a new pattern.
> >>>>>>>>>>>
> >>>>>>>>>>> You can convert the When column from character to time with
> >>>>>>>>> as.POSIXct().
> >>>>>>>>>>>
> >>>>>>>>>>> Bill Dunlap
> >>>>>>>>>>> TIBCO Software
> >>>>>>>>>>> wdunlap tibco.com
> >>>>>>>>>>>
> >>>>>>>>>>>
> >>>>>>>>>>> On Thu, May 16, 2019 at 8:30 PM David Winsemius
> >>>>>>>>> <dwinsemius at comcast.net>
> >>>>>>>>>>> wrote:
> >>>>>>>>>>>
> >>>>>>>>>>>>
> >>>>>>>>>>>> On 5/16/19 3:53 PM, Michael Boulineau wrote:
> >>>>>>>>>>>>> OK. So, I named the object test and then checked the 6347th
> >>>>>>>>> item
> >>>>>>>>>>>>>
> >>>>>>>>>>>>>> test <- readLines ("hangouts-conversation.txt)
> >>>>>>>>>>>>>> test [6347]
> >>>>>>>>>>>>> [1] "2016-10-21 10:56:37 <John Doe> Admit#8242"
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> Perhaps where it was getting screwed up is, since the end of
> >>>>>>>>> this is
> >>>>>>>>>> a
> >>>>>>>>>>>>> number (8242), then, given that there's no space between the
> >>>>>>>>> number
> >>>>>>>>>>>>> and what ought to be the next row, R didn't know where to draw
> >>>>>>>>> the
> >>>>>>>>>>>>> line. Sure enough, it looks like this when I go to the original
> >>>>>>>>> file
> >>>>>>>>>>>>> and control f "#8242"
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> 2016-10-21 10:35:36 <Jane Doe> What's your login
> >>>>>>>>>>>>> 2016-10-21 10:56:29 <John Doe> John_Doe
> >>>>>>>>>>>>> 2016-10-21 10:56:37 <John Doe> Admit#8242
> >>>>>>>>>>>>
> >>>>>>>>>>>>
> >>>>>>>>>>>> An octothorpe is an end of line signifier and is interpreted as
> >>>>>>>>>> allowing
> >>>>>>>>>>>> comments. You can prevent that interpretation with suitable
> >>>>>>>>> choice of
> >>>>>>>>>>>> parameters to `read.table` or `read.csv`. I don't understand why
> >>>>>>>>> that
> >>>>>>>>>>>> should cause anu error or a failure to match that pattern.
> >>>>>>>>>>>>
> >>>>>>>>>>>>> 2016-10-21 11:00:13 <Jane Doe> Okay so you have a discussion
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> Again, it doesn't look like that in the file. Gmail
> >>>>>>>>> automatically
> >>>>>>>>>>>>> formats it like that when I paste it in. More to the point, it
> >>>>>>>>> looks
> >>>>>>>>>>>>> like
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> 2016-10-21 10:35:36 <Jane Doe> What's your login2016-10-21
> >>>>>>>>> 10:56:29
> >>>>>>>>>>>>> <John Doe> John_Doe2016-10-21 10:56:37 <John Doe>
> >>>>>>>>>> Admit#82422016-10-21
> >>>>>>>>>>>>> 11:00:13 <Jane Doe> Okay so you have a discussion
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> Notice Admit#82422016. So there's that.
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> Then I built object test2.
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> test2 <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4",
> >>>>>>>>> test)
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> This worked for 84 lines, then this happened.
> >>>>>>>>>>>>
> >>>>>>>>>>>> It may have done something but as you later discovered my first
> >>>>>>>>> code
> >>>>>>>>>> for
> >>>>>>>>>>>> the pattern was incorrect. I had tested it (and pasted in the
> >>>>>>>>> results
> >>>>>>>>>> of
> >>>>>>>>>>>> the test) . The way to refer to a capture class is with
> >>>>>>>>> back-slashes
> >>>>>>>>>>>> before the numbers, not forward-slashes. Try this:
> >>>>>>>>>>>>
> >>>>>>>>>>>>
> >>>>>>>>>>>>> newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)",
> >>>>>>>>> "\\1,\\2,\\3,\\4",
> >>>>>>>>>> chrvec)
> >>>>>>>>>>>>> newvec
> >>>>>>>>>>>> [1] "2016-07-01,02:50:35,<john>,hey"
> >>>>>>>>>>>> [2] "2016-07-01,02:51:26,<jane>,waiting for plane to Edinburgh"
> >>>>>>>>>>>> [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
> >>>>>>>>>>>> [4] "2016-07-01,02:52:07,<jane>,nothing crappy has happened,
> >>>>>>>>> not
> >>>>>>>>>> really"
> >>>>>>>>>>>> [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast,
> >>>>>>>>> didn't
> >>>>>>>>>> sleep"
> >>>>>>>>>>>> [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or
> >>>>>>>>> where I am
> >>>>>>>>>>>> really"
> >>>>>>>>>>>> [7] "2016-07-01,02:54:17,<john>,just know it's london"
> >>>>>>>>>>>> [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
> >>>>>>>>>>>> [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a good
> >>>>>>>>> eay"
> >>>>>>>>>>>> [10] "2016-07-01 02:58:56 <jone>"
> >>>>>>>>>>>> [11] "2016-07-01 02:59:34 <jane>"
> >>>>>>>>>>>> [12] "2016-07-01,03:02:48,<john>,British security is a little
> >>>>>>>>> more
> >>>>>>>>>>>> rigorous..."
> >>>>>>>>>>>>
> >>>>>>>>>>>>
> >>>>>>>>>>>> I made note of the fact that the 10th and 11th lines had no
> >>>>>>>>> commas.
> >>>>>>>>>>>>
> >>>>>>>>>>>>>
> >>>>>>>>>>>>>> test2 [84]
> >>>>>>>>>>>>> [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
> >>>>>>>>>>>>
> >>>>>>>>>>>> That line didn't have any "<" so wasn't matched.
> >>>>>>>>>>>>
> >>>>>>>>>>>>
> >>>>>>>>>>>> You could remove all none matching lines for pattern of
> >>>>>>>>>>>>
> >>>>>>>>>>>> dates<space>times<space>"<"<name>">"<space><anything>
> >>>>>>>>>>>>
> >>>>>>>>>>>>
> >>>>>>>>>>>> with:
> >>>>>>>>>>>>
> >>>>>>>>>>>>
> >>>>>>>>>>>> chrvec <- chrvec[ grepl("^.{10} .{8} <.+> .+$)", chrvec)]
> >>>>>>>>>>>>
> >>>>>>>>>>>>
> >>>>>>>>>>>> Do read:
> >>>>>>>>>>>>
> >>>>>>>>>>>> ?read.csv
> >>>>>>>>>>>>
> >>>>>>>>>>>> ?regex
> >>>>>>>>>>>>
> >>>>>>>>>>>>
> >>>>>>>>>>>> --
> >>>>>>>>>>>>
> >>>>>>>>>>>> David
> >>>>>>>>>>>>
> >>>>>>>>>>>>
> >>>>>>>>>>>>>> test2 [85]
> >>>>>>>>>>>>> [1] "//1,//2,//3,//4"
> >>>>>>>>>>>>>> test [85]
> >>>>>>>>>>>>> [1] "2016-07-01 02:50:35 <John Doe> hey"
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> Notice how I toggled back and forth between test and test2
> >>>>>>>>> there. So,
> >>>>>>>>>>>>> whatever happened with the regex, it happened in the switch
> >>>>>>>>> from 84
> >>>>>>>>>> to
> >>>>>>>>>>>>> 85, I guess. It went on like
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> [990] "//1,//2,//3,//4"
> >>>>>>>>>>>>> [991] "//1,//2,//3,//4"
> >>>>>>>>>>>>> [992] "//1,//2,//3,//4"
> >>>>>>>>>>>>> [993] "//1,//2,//3,//4"
> >>>>>>>>>>>>> [994] "//1,//2,//3,//4"
> >>>>>>>>>>>>> [995] "//1,//2,//3,//4"
> >>>>>>>>>>>>> [996] "//1,//2,//3,//4"
> >>>>>>>>>>>>> [997] "//1,//2,//3,//4"
> >>>>>>>>>>>>> [998] "//1,//2,//3,//4"
> >>>>>>>>>>>>> [999] "//1,//2,//3,//4"
> >>>>>>>>>>>>> [1000] "//1,//2,//3,//4"
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> up until line 1000, then I reached max.print.
> >>>>>>>>>>>>
> >>>>>>>>>>>>> Michael
> >>>>>>>>>>>>>
> >>>>>>>>>>>>> On Thu, May 16, 2019 at 1:05 PM David Winsemius <
> >>>>>>>>>> dwinsemius at comcast.net>
> >>>>>>>>>>>> wrote:
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>> On 5/16/19 12:30 PM, Michael Boulineau wrote:
> >>>>>>>>>>>>>>> Thanks for this tip on etiquette, David. I will be sure and
> >>>>>>>>> not do
> >>>>>>>>>>>> that again.
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> I tried the read.fwf from the foreign package, with a code
> >>>>>>>>> like
> >>>>>>>>>> this:
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> d <- read.fwf("hangouts-conversation.txt",
> >>>>>>>>>>>>>>>               widths= c(10,10,20,40),
> >>>>>>>>>>>>>>>
> >>>>>>>>> col.names=c("date","time","person","comment"),
> >>>>>>>>>>>>>>>               strip.white=TRUE)
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> But it threw this error:
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> Error in scan(file = file, what = what, sep = sep, quote =
> >>>>>>>>> quote,
> >>>>>>>>>> dec
> >>>>>>>>>>>> = dec,  :
> >>>>>>>>>>>>>>> line 6347 did not have 4 elements
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>> So what does line 6347 look like? (Use `readLines` and print
> >>>>>>>>> it
> >>>>>>>>>> out.)
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> Interestingly, though, the error only happened when I
> >>>>>>>>> increased the
> >>>>>>>>>>>>>>> width size. But I had to increase the size, or else I
> >>>>>>>>> couldn't
> >>>>>>>>>> "see"
> >>>>>>>>>>>>>>> anything.  The comment was so small that nothing was being
> >>>>>>>>>> captured by
> >>>>>>>>>>>>>>> the size of the column. so to speak.
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> It seems like what's throwing me is that there's no comma
> >>>>>>>>> that
> >>>>>>>>>>>>>>> demarcates the end of the text proper. For example:
> >>>>>>>>>>>>>> Not sure why you thought there should be a comma. Lines
> >>>>>>>>> usually end
> >>>>>>>>>>>>>> with  <cr> and or a <lf>.
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>> Once you have the raw text in a character vector from
> >>>>>>>>> `readLines`
> >>>>>>>>>> named,
> >>>>>>>>>>>>>> say, 'chrvec', then you could selectively substitute commas
> >>>>>>>>> for
> >>>>>>>>>> spaces
> >>>>>>>>>>>>>> with regex. (Now that you no longer desire to remove the dates
> >>>>>>>>> and
> >>>>>>>>>>>> times.)
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>> sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", chrvec)
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>> This will not do any replacements when the pattern is not
> >>>>>>>>> matched.
> >>>>>>>>>> See
> >>>>>>>>>>>>>> this test:
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)",
> >>>>>>>>> "\\1,\\2,\\3,\\4",
> >>>>>>>>>>>> chrvec)
> >>>>>>>>>>>>>>> newvec
> >>>>>>>>>>>>>> [1] "2016-07-01,02:50:35,<john>,hey"
> >>>>>>>>>>>>>> [2] "2016-07-01,02:51:26,<jane>,waiting for plane to
> >>>>>>>>> Edinburgh"
> >>>>>>>>>>>>>> [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
> >>>>>>>>>>>>>> [4] "2016-07-01,02:52:07,<jane>,nothing crappy has
> >>>>>>>>> happened, not
> >>>>>>>>>>>> really"
> >>>>>>>>>>>>>> [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast,
> >>>>>>>>> didn't
> >>>>>>>>>>>> sleep"
> >>>>>>>>>>>>>> [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or
> >>>>>>>>> where
> >>>>>>>>>> I am
> >>>>>>>>>>>>>> really"
> >>>>>>>>>>>>>> [7] "2016-07-01,02:54:17,<john>,just know it's london"
> >>>>>>>>>>>>>> [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
> >>>>>>>>>>>>>> [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a
> >>>>>>>>> good
> >>>>>>>>>> eay"
> >>>>>>>>>>>>>> [10] "2016-07-01 02:58:56 <jone>"
> >>>>>>>>>>>>>> [11] "2016-07-01 02:59:34 <jane>"
> >>>>>>>>>>>>>> [12] "2016-07-01,03:02:48,<john>,British security is a little
> >>>>>>>>> more
> >>>>>>>>>>>>>> rigorous..."
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>> You should probably remove the "empty comment" lines.
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>> --
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>> David.
> >>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> 2016-07-01 15:34:30 <John Doe> Lame. We were in a
> >>>>>>>>>> starbucks2016-07-01
> >>>>>>>>>>>>>>> 15:35:02 <Jane Doe> Hmm that's interesting2016-07-01 15:35:09
> >>>>>>>>> <Jane
> >>>>>>>>>>>>>>> Doe> You must want coffees2016-07-01 15:35:25 <John Doe>
> >>>>>>>>> There was
> >>>>>>>>>>>>>>> lots of Starbucks in my day2016-07-01 15:35:47
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> It was interesting, too, when I pasted the text into the
> >>>>>>>>> email, it
> >>>>>>>>>>>>>>> self-formatted into the way I wanted it to look. I had to
> >>>>>>>>> manually
> >>>>>>>>>>>>>>> make it look like it does above, since that's the way that it
> >>>>>>>>>> looks in
> >>>>>>>>>>>>>>> the txt file. I wonder if it's being organized by XML or
> >>>>>>>>> something.
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> Anyways, There's always a space between the two sideways
> >>>>>>>>> carrots,
> >>>>>>>>>> just
> >>>>>>>>>>>>>>> like there is right now: <John Doe> See. Space. And there's
> >>>>>>>>> always
> >>>>>>>>>> a
> >>>>>>>>>>>>>>> space between the data and time. Like this. 2016-07-01
> >>>>>>>>> 15:34:30
> >>>>>>>>>> See.
> >>>>>>>>>>>>>>> Space. But there's never a space between the end of the
> >>>>>>>>> comment and
> >>>>>>>>>>>>>>> the next date. Like this: We were in a starbucks2016-07-01
> >>>>>>>>> 15:35:02
> >>>>>>>>>>>>>>> See. starbucks and 2016 are smooshed together.
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> This code is also on the table right now too.
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> a <- read.table("E:/working
> >>>>>>>>>>>>>>> directory/-189/hangouts-conversation2.txt", quote="\"",
> >>>>>>>>>>>>>>> comment.char="", fill=TRUE)
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>> h<-cbind(hangouts.conversation2[,1:2],hangouts.conversation2[,3:5],hangouts.conversation2[,6:9])
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> aa<-gsub("[^[:digit:]]","",h)
> >>>>>>>>>>>>>>> my.data.num <- as.numeric(str_extract(h, "[0-9]+"))
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> Those last lines are a work in progress. I wish I could
> >>>>>>>>> import a
> >>>>>>>>>>>>>>> picture of what it looks like when it's translated into a
> >>>>>>>>> data
> >>>>>>>>>> frame.
> >>>>>>>>>>>>>>> The fill=TRUE helped to get the data in table that kind of
> >>>>>>>>> sort of
> >>>>>>>>>>>>>>> works, but the comments keep bleeding into the data and time
> >>>>>>>>>> column.
> >>>>>>>>>>>>>>> It's like
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> 2016-07-01 15:59:17 <Jane Doe> Seriously I've never been
> >>>>>>>>>>>>>>> over               there
> >>>>>>>>>>>>>>> 2016-07-01 15:59:27 <Jane Doe> It confuses me :(
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> And then, maybe, the "seriously" will be in a column all to
> >>>>>>>>>> itself, as
> >>>>>>>>>>>>>>> will be the "I've'"and the "never" etc.
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> I will use a regular expression if I have to, but it would be
> >>>>>>>>> nice
> >>>>>>>>>> to
> >>>>>>>>>>>>>>> keep the dates and times on there. Originally, I thought they
> >>>>>>>>> were
> >>>>>>>>>>>>>>> meaningless, but I've since changed my mind on that count.
> >>>>>>>>> The
> >>>>>>>>>> time of
> >>>>>>>>>>>>>>> day isn't so important. But, especially since, say, Gmail
> >>>>>>>>> itself
> >>>>>>>>>> knows
> >>>>>>>>>>>>>>> how to quickly recognize what it is, I know it can be done. I
> >>>>>>>>> know
> >>>>>>>>>>>>>>> this data has structure to it.
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> Michael
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>> On Wed, May 15, 2019 at 8:47 PM David Winsemius <
> >>>>>>>>>>>> dwinsemius at comcast.net> wrote:
> >>>>>>>>>>>>>>>> On 5/15/19 4:07 PM, Michael Boulineau wrote:
> >>>>>>>>>>>>>>>>> I have a wild and crazy text file, the head of which looks
> >>>>>>>>> like
> >>>>>>>>>> this:
> >>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>> 2016-07-01 02:50:35 <john> hey
> >>>>>>>>>>>>>>>>> 2016-07-01 02:51:26 <jane> waiting for plane to Edinburgh
> >>>>>>>>>>>>>>>>> 2016-07-01 02:51:45 <john> thinking about my boo
> >>>>>>>>>>>>>>>>> 2016-07-01 02:52:07 <jane> nothing crappy has happened, not
> >>>>>>>>>> really
> >>>>>>>>>>>>>>>>> 2016-07-01 02:52:20 <john> plane went by pretty fast,
> >>>>>>>>> didn't
> >>>>>>>>>> sleep
> >>>>>>>>>>>>>>>>> 2016-07-01 02:54:08 <jane> no idea what time it is or where
> >>>>>>>>> I am
> >>>>>>>>>>>> really
> >>>>>>>>>>>>>>>>> 2016-07-01 02:54:17 <john> just know it's london
> >>>>>>>>>>>>>>>>> 2016-07-01 02:56:44 <jane> you are probably asleep
> >>>>>>>>>>>>>>>>> 2016-07-01 02:58:45 <jane> I hope fish was fishy in a good
> >>>>>>>>> eay
> >>>>>>>>>>>>>>>>> 2016-07-01 02:58:56 <jone>
> >>>>>>>>>>>>>>>>> 2016-07-01 02:59:34 <jane>
> >>>>>>>>>>>>>>>>> 2016-07-01 03:02:48 <john> British security is a little
> >>>>>>>>> more
> >>>>>>>>>>>> rigorous...
> >>>>>>>>>>>>>>>> Looks entirely not-"crazy". Typical log file format.
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> Two possibilities: 1) Use `read.fwf` from pkg foreign; 2)
> >>>>>>>>> Use
> >>>>>>>>>> regex
> >>>>>>>>>>>>>>>> (i.e. the sub-function) to strip everything up to the "<".
> >>>>>>>>> Read
> >>>>>>>>>>>>>>>> `?regex`. Since that's not a metacharacters you could use a
> >>>>>>>>>> pattern
> >>>>>>>>>>>>>>>> ".+<" and replace with "".
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> And do read the Posting Guide. Cross-posting to
> >>>>>>>>> StackOverflow and
> >>>>>>>>>>>> Rhelp,
> >>>>>>>>>>>>>>>> at least within hours of each, is considered poor manners.
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> --
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>> David.
> >>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>> It goes on for a while. It's a big file. But I feel like
> >>>>>>>>> it's
> >>>>>>>>>> going
> >>>>>>>>>>>> to
> >>>>>>>>>>>>>>>>> be difficult to annotate with the coreNLP library or
> >>>>>>>>> package. I'm
> >>>>>>>>>>>>>>>>> doing natural language processing. In other words, I'm
> >>>>>>>>> curious
> >>>>>>>>>> as to
> >>>>>>>>>>>>>>>>> how I would shave off the dates, that is, to make it look
> >>>>>>>>> like:
> >>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>> <john> hey
> >>>>>>>>>>>>>>>>> <jane> waiting for plane to Edinburgh
> >>>>>>>>>>>>>>>>> <john> thinking about my boo
> >>>>>>>>>>>>>>>>> <jane> nothing crappy has happened, not really
> >>>>>>>>>>>>>>>>> <john> plane went by pretty fast, didn't sleep
> >>>>>>>>>>>>>>>>> <jane> no idea what time it is or where I am really
> >>>>>>>>>>>>>>>>> <john> just know it's london
> >>>>>>>>>>>>>>>>> <jane> you are probably asleep
> >>>>>>>>>>>>>>>>> <jane> I hope fish was fishy in a good eay
> >>>>>>>>>>>>>>>>> <jone>
> >>>>>>>>>>>>>>>>> <jane>
> >>>>>>>>>>>>>>>>> <john> British security is a little more rigorous...
> >>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>> To be clear, then, I'm trying to clean a large text file by
> >>>>>>>>>> writing a
> >>>>>>>>>>>>>>>>> regular expression? such that I create a new object with no
> >>>>>>>>>> numbers
> >>>>>>>>>>>> or
> >>>>>>>>>>>>>>>>> dates.
> >>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>> Michael
> >>>>>>>>>>>>>>>>>
> >>>>>>>>>>>>>>>>> ______________________________________________
> >>>>>>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
> >>>>>>>>> more,
> >>>>>>>>>> see
> >>>>>>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>>>>>>>>>> PLEASE do read the posting guide
> >>>>>>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>>>>>>>>>> and provide commented, minimal, self-contained,
> >>>>>>>>> reproducible
> >>>>>>>>>> code.
> >>>>>>>>>>>>>>> ______________________________________________
> >>>>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >>>>>>>>> see
> >>>>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>>>>>>>> PLEASE do read the posting guide
> >>>>>>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
> >>>>>>>>> code.
> >>>>>>>>>>>>>> ______________________________________________
> >>>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >>>>>>>>> see
> >>>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>>>>>>> PLEASE do read the posting guide
> >>>>>>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
> >>>>>>>>> code.
> >>>>>>>>>>>>> ______________________________________________
> >>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >>>>>>>>> see
> >>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>>>>>> PLEASE do read the posting guide
> >>>>>>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
> >>>>>>>>> code.
> >>>>>>>>>>>>
> >>>>>>>>>>>> ______________________________________________
> >>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>>>>> PLEASE do read the posting guide
> >>>>>>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
> >>>>>>>>> code.
> >>>>>>>>>>>>
> >>>>>>>>>>>
> >>>>>>>>>>>     [[alternative HTML version deleted]]
> >>>>>>>>>>>
> >>>>>>>>>>> ______________________________________________
> >>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>>>> PLEASE do read the posting guide
> >>>>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>>>>>
> >>>>>>>>>> ______________________________________________
> >>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>>> PLEASE do read the posting guide
> >>>>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>>>>>
> >>>>>>>>>
> >>>>>>>>>   [[alternative HTML version deleted]]
> >>>>>>>>>
> >>>>>>>>> ______________________________________________
> >>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>> PLEASE do read the posting guide
> >>>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>>>
> >>>>>>>> --
> >>>>>>>> Sent from my phone. Please excuse my brevity.
> >>>>>>>
> >>>>>>> ______________________________________________
> >>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From bor|@@@te|pe @end|ng |rom utoronto@c@  Mon May 20 00:38:28 2019
From: bor|@@@te|pe @end|ng |rom utoronto@c@ (Boris Steipe)
Date: Sun, 19 May 2019 22:38:28 +0000
Subject: [R] how to separate string from numbers in a large txt file
In-Reply-To: <CAH+cTGN+QPsi6DCRViGM_0wbhvGfe2m0K8pTm+j-x=noQFQNKg@mail.gmail.com>
References: <CAH+cTGM9sFCxegaMHzu-_9Prhvvh9MbVDCWJ1_x7wkO9jC47mQ@mail.gmail.com>
 <c6e69644-66a1-7031-4d7c-1000e0abbea4@comcast.net>
 <CAH+cTGM5YvWV_G0pRgMqjCzuRrFTzd0nOc7yoThfGbGMUyMjsQ@mail.gmail.com>
 <9c9f1c81-309f-91cb-7e26-f02fc4bd45d8@comcast.net>
 <CAH+cTGMx4_Lz2gGDf2AuUaaZF2Xaq4McuPRoHu4JJNc-ORd=Ew@mail.gmail.com>
 <6322e735-5659-848e-3a50-9f066f1299ad@comcast.net>
 <CAF8bMcbgrVcN9J7Aygq2D5RWaU4MS-GH+_+zMHU=AO+DU5DVWw@mail.gmail.com>
 <CAH+cTGNfpLSurqmsoMeci0GWhC0ncOp7zTEju3fJW5T4+tCsGA@mail.gmail.com>
 <CAF8bMcaSJRaUXK4h2ssaDoJwXaKCUZT2Vq1cutNiNDjQ7ga2Xg@mail.gmail.com>
 <EEA9FD46-A67C-4FB7-8245-350819C4EA62@dcn.davis.ca.us>
 <CAH+cTGNOkAnPnn-aru-BJUbhrgeYMQBTqjSYT764nKzcdXvTvA@mail.gmail.com>
 <9B170703-AAAF-4D52-9744-1FDE70559B05@utoronto.ca>
 <CAH+cTGNaw7p-it4B7te4BBsay=v6J-5OS-P_WGDJJ6Z0ymxtvw@mail.gmail.com>
 <0F8CD89C-BDE2-44D0-8E18-0CA2706EB53F@utoronto.ca>
 <CAH+cTGMsD+PkTKoFfB8=3WeWPQAWjhmuOCKZfnB7jR03m7RtDQ@mail.gmail.com>
 <C5A52A04-CB49-442F-81CE-26CBC619C47A@utoronto.ca>
 <CAH+cTGOijsRxzJy3w9QS11axwD+Z36-Pive6dOhm2xK6Jm5PAg@mail.gmail.com>
 <13D5D19C-0EA2-4234-9EDC-539A01DB7690@utoronto.ca>
 <CAH+cTGN+QPsi6DCRViGM_0wbhvGfe2m0K8pTm+j-x=noQFQNKg@mail.gmail.com>
Message-ID: <E6C35BE4-F743-4E53-B772-F5A1E5A98377@utoronto.ca>

Inline



> On 2019-05-19, at 18:11, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
> 
> For context:
> 
>> In gsub(b, "\\1<\\2> ", a) the work is done by the backreferences \\1 and \\2. The expression says:
>> Substitute ALL of the match with the first captured expression, then " <", then the second captured expression, then "> ". The rest of the line is >not substituted and appears as-is.
> 
> Back to me: I guess what's giving me trouble is where to draw the line
> in terms of the end or edge of the expression. Given the code, then,
> 
>> a <- readLines ("hangouts-conversation-6.txt", encoding = "UTF-8")
>> b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)"
>> c <- gsub(b, "\\1<\\2> ", a)
> 
> to me, it would seem as though this is the first captured expression,
> that is, as though \\1 refers back to ^([0-9-]{10} [0-9:]{8} ), since
> there are parenthesis around it, or since [0-9-]{10} [0-9:]{8} is
> enclosed in parentheses.

That's correct: parentheses in regular expressions delimit captured substrings.



> Then it would seem as though [*]{3} is the
> second expression, and (\\w+ \\w+) is the third.

Note that "[*]{3}" has no parentheses, is not captured and is not accounted for in the back-references.

\\1 and \\2 refers only to the captured substrings - everything else contributes to whether the regex matches at all, but is no longer considered after the match.

> According to this
> (admittedly wrong) logic, it would seem as though the <> would go
> around the date--like

No:  it goes around \\2, which is (\\w+ \\w+)

> 
>> 2016-03-20 <19:29:37> *** Jane Doe started a video chat
> 
> The back references here recalls Davis's code earlier:
> 
>> sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", chrvec)
> 
> There, commas were put around everything, and there you can see the
> edge of the expression very well. ^(.{10}) = first. (.{8}) = second.
> (<.+>) = third. (.+$) = fourth. So, by the same logic, it would seem
> as though in
> 
>> b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)"
> 
> that ^([0-9-]{10} [0-9:]{8} ) is first, that [*]{3} is second, and
> that  (\\w+ \\w+) is third.
> 
> But, if Boris is to be right, and he is, obviously, then it would have
> to be the case that this entire thing, namely, ^([0-9-]{10} [0-9:]{8}
> )[*]{3}, is the first expression,

Actually "[*]{3}" is not part of the first expression - it is discarded because not in parentheses

> since only if that were true would
> the <> be able to go around the names, as in
> 
> [3] "2016-01-27 09:15:20 <Jane Doe> Hey "
> 
> Again, so 2016-01-27 09:15:20 would have to be an entire unit, an
> expression.

The word "expression" has a different technical meaning, but colloquially you are right.


> So I guess what I don't understand is how ^([0-9-]{10}
> [0-9:]{8} )[*]{3} can be an entire expression, although my hunch would
> be that it has something to do with the ^ or with the space after the
> } and before the (, as in
> 
>> {3} (\\w+
> 

No. Just the parentheses.


> Back to earlier:
> 
>> The rest of the line is not substituted and appears as-is.
> 
> Is that due to the space after the \\2? in
> 
>> "\\1<\\2> 

No, that is because the substitution in gsub() targets only the match of the regex - and the string to the end is not part of the regex.


Cheers,
Boris

> Notice space after > and before "
> 
> Michael
> 
> On Sun, May 19, 2019 at 2:31 PM Boris Steipe <boris.steipe at utoronto.ca> wrote:
>> 
>> Inline ...
>> 
>>> On 2019-05-19, at 13:56, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
>>> 
>>>> b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)"
>>> 
>>> so the ^ signals that the regex BEGINS with a number (that could be
>>> any number, 0-9) that is only 10 characters long (then there's the
>>> dash in there, too, with the 0-9-, which I assume enabled the regex to
>>> grab the - that's between the numbers in the date)
>> 
>> That's right. Note that within a "character class" the hyphen can have tow meanings: normally it defines a range of characters, but if it appears as the last character before "]" it is a literal hyphen.
>> 
>>> , followed by a
>>> single space, followed by a unit that could be any number, again, but
>>> that is only 8 characters long this time. For that one, it will
>>> include the colon, hence the 9:, although for that one ([0-9:]{8} ),
>> 
>> Right.
>> 
>> 
>>> I
>>> don't get why the space is on the inside in that one, after the {8},
>> 
>> The space needs to be preserved between the time and the name. I wrote
>> b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)" # space in the first captured expression
>> c <- gsub(b, "\\1<\\2> ", a)
>> ... but I could have written
>> b <- "^([0-9-]{10} [0-9:]{8})[*]{3} (\\w+ \\w+)"
>> c <- gsub(b, "\\1 <\\2> ", a)  # space in the substituted string
>> ... same result
>> 
>> 
>>> whereas the space is on the outside with the other one ^([0-9-]{10} ,
>>> directly after the {10}. Why is that?
>> 
>> In the second case, I capture without a space, because I don't want the space in the results, after the time.
>> 
>> 
>>> 
>>> Then three *** [*]{3}, then the (\\w+ \\w+)", which Boris explained so
>>> well above. I guess I still don't get why this one seemed to have
>>> deleted the *** out of the mix, plus I still don't why it didn't
>>> remove the *** from the first one.
>> 
>> Because the entire first line was not matched since it had a malformed character preceding the date.
>> 
>>> 
>>> 2016-03-20 19:29:37 *** Jane Doe started a video chat
>>> 2016-03-20 19:30:35 *** John Doe ended a video chat
>>> 2016-04-02 12:59:36 *** Jane Doe started a video chat
>>> 2016-04-02 13:00:43 *** John Doe ended a video chat
>>> 2016-04-02 13:01:08 *** Jane Doe started a video chat
>>> 2016-04-02 13:01:41 *** John Doe ended a video chat
>>> 2016-04-02 13:03:51 *** John Doe started a video chat
>>> 2016-04-02 13:06:35 *** John Doe ended a video chat
>>> 
>>> This is a random sample from the beginning of the txt file with no
>>> edits. The ***s were deleted, all but the first one, the one that had
>>> the ??? but that was taken out by the encoding = "UTF-8". I know that
>>> the function was c <- gsub(b, "\\1<\\2> ", a), so it had a gsub () on
>>> there, the point of which is to do substitution work.
>>> 
>>> Oh, I get it, I think. The \\1<\\2> in the gsub () puts the <> around
>>> the names, so that it's consistent with the rest of the data, so that
>>> the names in the text about that aren't enclosed in the <> are
>>> enclosed like the rest of them. But I still don't get why or how the
>>> gsub () replaced the *** with the <>...
>> 
>> In gsub(b, "\\1<\\2> ", a) the work is done by the backreferences \\1 and \\2. The expression says:
>> Substitute ALL of the match with the first captured expression, then " <", then the second captured expression, then "> ". The rest of the line is not substituted and appears as-is.
>> 
>> 
>>> 
>>> This one is more straightforward.
>>> 
>>>> d <- "^([0-9-]{10}) ([0-9:]{8}) <(\\w+ \\w+)>\\s*(.+)$"
>>> 
>>> any number with - for 10 characters, followed by a space. Oh, there's
>>> no space in this one ([0-9:]{8}), after the {8}. Hu. So, then, any
>>> number with : for 8 characters, followed by any two words separated by
>>> a space and enclosed in <>. And then the \\s* is followed by a single
>>> space? Or maybe it puts space on both sides (on the side of the #s to
>>> the left, and then the comment to the right). The (.+)$ is anything
>>> whatsoever until the end.
>> 
>> \s is the metacharacter for "whitespace". \s* means zero or more whitespace. I'm matching that OUTSIDE of the captured expression, to removes any leading spaces from the data that goes into the data frame.
>> 
>> 
>> Cheers,
>> Boris
>> 
>> 
>> 
>> 
>>> 
>>> Michael
>>> 
>>> 
>>> On Sun, May 19, 2019 at 4:37 AM Boris Steipe <boris.steipe at utoronto.ca> wrote:
>>>> 
>>>> Inline
>>>> 
>>>> 
>>>> 
>>>>> On 2019-05-18, at 20:34, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
>>>>> 
>>>>> It appears to have worked, although there were three little quirks.
>>>>> The ; close(con); rm(con) didn't work for me; the first row of the
>>>>> data.frame was all NAs, when all was said and done;
>>>> 
>>>> You will get NAs for lines that can't be matched to the regular expression. That's a good thing, it allows you to test whether your assumptions were valid for the entire file:
>>>> 
>>>> # number of failed strcapture()
>>>> sum(is.na(e$date))
>>>> 
>>>> 
>>>>> and then there
>>>>> were still three *** on the same line where the ??? was apparently
>>>>> deleted.
>>>> 
>>>> This is a sign that something else happened with the line that prevented the regex from matching. In that case you need to investigate more. I see an invalid multibyte character at the beginning of the line you posted below.
>>>> 
>>>>> 
>>>>>> a <- readLines ("hangouts-conversation-6.txt", encoding = "UTF-8")
>>>>>> b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)"
>>>>>> c <- gsub(b, "\\1<\\2> ", a)
>>>>>> head (c)
>>>>> [1] "?2016-01-27 09:14:40 *** Jane Doe started a video chat"
>>>>> [2] "2016-01-27 09:15:20 <Jane Doe>
>>>>> https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf"
>>>> 
>>>> [...]
>>>> 
>>>>> But, before I do anything else, I'm going to study the regex in this
>>>>> particular code. For example, I'm still not sure why there has to the
>>>>> second \\w+ in the (\\w+ \\w+). Little things like that.
>>>> 
>>>> \w is the metacharacter for alphanumeric characters, \w+ designates something we could call a word. Thus \w+ \w+ are two words separated by a single blank. This corresponds to your example, but, as I wrote previously, you need to think very carefully whether this covers all possible cases (Could there be only one word? More than one blank? Could letters be separated by hyphens or periods?) In most cases we could have more robustly matched everything between "<" and ">" (taking care to test what happens if the message contains those characters). But for the video chat lines we need to make an assumption about what is name and what is not. If "started a video chat" is the only possibility in such lines, you can use this information instead. If there are other possibilities, you need a different strategy. In NLP there is no one-approach-fits-all.
>>>> 
>>>> To validate the structure of the names in your transcripts, you can look at
>>>> 
>>>> patt <- " <.+?> "   # " <any string, not greedy> "
>>>> m <- regexpr(patt, c)
>>>> unique(regmatches(c, m))
>>>> 
>>>> 
>>>> 
>>>> B.
>>>> 
>>>> 
>>>> 
>>>>> 
>>>>> Michael
>>>>> 
>>>>> 
>>>>> On Sat, May 18, 2019 at 4:30 PM Boris Steipe <boris.steipe at utoronto.ca> wrote:
>>>>>> 
>>>>>> This works for me:
>>>>>> 
>>>>>> # sample data
>>>>>> c <- character()
>>>>>> c[1] <- "2016-01-27 09:14:40 <Jane Doe> started a video chat"
>>>>>> c[2] <- "2016-01-27 09:15:20 <Jane Doe> https://lh3.googleusercontent.com/"
>>>>>> c[3] <- "2016-01-27 09:15:20 <Jane Doe> Hey "
>>>>>> c[4] <- "2016-01-27 09:15:22 <John Doe>  ended a video chat"
>>>>>> c[5] <- "2016-01-27 21:07:11 <Jane Doe>  started a video chat"
>>>>>> c[6] <- "2016-01-27 21:26:57 <John Doe>  ended a video chat"
>>>>>> 
>>>>>> 
>>>>>> # regex  ^(year)       (time)      <(word word)>\\s*(string)$
>>>>>> patt <- "^([0-9-]{10}) ([0-9:]{8}) <(\\w+ \\w+)>\\s*(.+)$"
>>>>>> proto <- data.frame(date = character(),
>>>>>>                  time = character(),
>>>>>>                  name = character(),
>>>>>>                  text = character(),
>>>>>>                  stringsAsFactors = TRUE)
>>>>>> d <- strcapture(patt, c, proto)
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>>      date     time     name                               text
>>>>>> 1 2016-01-27 09:14:40 Jane Doe               started a video chat
>>>>>> 2 2016-01-27 09:15:20 Jane Doe https://lh3.googleusercontent.com/
>>>>>> 3 2016-01-27 09:15:20 Jane Doe                               Hey
>>>>>> 4 2016-01-27 09:15:22 John Doe                 ended a video chat
>>>>>> 5 2016-01-27 21:07:11 Jane Doe               started a video chat
>>>>>> 6 2016-01-27 21:26:57 John Doe                 ended a video chat
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> B.
>>>>>> 
>>>>>> 
>>>>>>> On 2019-05-18, at 18:32, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
>>>>>>> 
>>>>>>> Going back and thinking through what Boris and William were saying
>>>>>>> (also Ivan), I tried this:
>>>>>>> 
>>>>>>> a <- readLines ("hangouts-conversation-6.csv.txt")
>>>>>>> b <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+)"
>>>>>>> c <- gsub(b, "\\1<\\2> ", a)
>>>>>>>> head (c)
>>>>>>> [1] "???2016-01-27 09:14:40 *** Jane Doe started a video chat"
>>>>>>> [2] "2016-01-27 09:15:20 <Jane Doe>
>>>>>>> https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf"
>>>>>>> [3] "2016-01-27 09:15:20 <Jane Doe> Hey "
>>>>>>> [4] "2016-01-27 09:15:22 <John Doe>  ended a video chat"
>>>>>>> [5] "2016-01-27 21:07:11 <Jane Doe>  started a video chat"
>>>>>>> [6] "2016-01-27 21:26:57 <John Doe>  ended a video chat"
>>>>>>> 
>>>>>>> The ??? is still there, since I forgot to do what Ivan had suggested, namely,
>>>>>>> 
>>>>>>> a <- readLines(con <- file("hangouts-conversation-6.csv.txt", encoding
>>>>>>> = "UTF-8")); close(con); rm(con)
>>>>>>> 
>>>>>>> But then the new code is still turning out only NAs when I apply
>>>>>>> strcapture (). This was what happened next:
>>>>>>> 
>>>>>>>> d <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
>>>>>>> + [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
>>>>>>> +                 c, proto=data.frame(stringsAsFactors=FALSE, When="", Who="",
>>>>>>> +                                     What=""))
>>>>>>>> head (d)
>>>>>>> When  Who What
>>>>>>> 1 <NA> <NA> <NA>
>>>>>>> 2 <NA> <NA> <NA>
>>>>>>> 3 <NA> <NA> <NA>
>>>>>>> 4 <NA> <NA> <NA>
>>>>>>> 5 <NA> <NA> <NA>
>>>>>>> 6 <NA> <NA> <NA>
>>>>>>> 
>>>>>>> I've been reading up on regular expressions, too, so this code seems
>>>>>>> spot on. What's going wrong?
>>>>>>> 
>>>>>>> Michael
>>>>>>> 
>>>>>>> On Fri, May 17, 2019 at 4:28 PM Boris Steipe <boris.steipe at utoronto.ca> wrote:
>>>>>>>> 
>>>>>>>> Don't start putting in extra commas and then reading this as csv. That approach is broken. The correct approach is what Bill outlined: read everything with readLines(), and then use a proper regular expression with strcapture().
>>>>>>>> 
>>>>>>>> You need to pre-process the object that readLines() gives you: replace the contents of the videochat lines, and make it conform to the format of the other lines before you process it into your data frame.
>>>>>>>> 
>>>>>>>> Approximately something like
>>>>>>>> 
>>>>>>>> # read the raw data
>>>>>>>> tmp <- readLines("hangouts-conversation-6.csv.txt")
>>>>>>>> 
>>>>>>>> # process all video chat lines
>>>>>>>> patt <- "^([0-9-]{10} [0-9:]{8} )[*]{3} (\\w+ \\w+) "  # (year time )*** (word word)
>>>>>>>> tmp <- gsub(patt, "\\1<\\2> ", tmp)
>>>>>>>> 
>>>>>>>> # next, use strcapture()
>>>>>>>> 
>>>>>>>> Note that this makes the assumption that your names are always exactly two words containing only letters. If that assumption is not true, more though needs to go into the regex. But you can test that:
>>>>>>>> 
>>>>>>>> patt <- " <\\w+ \\w+> "   #" <word word> "
>>>>>>>> sum( ! grepl(patt, tmp)))
>>>>>>>> 
>>>>>>>> ... will give the number of lines that remain in your file that do not have a tag that can be interpreted as "Who"
>>>>>>>> 
>>>>>>>> Once that is fine, use Bill's approach - or a regular expression of your own design - to create your data frame.
>>>>>>>> 
>>>>>>>> Hope this helps,
>>>>>>>> Boris
>>>>>>>> 
>>>>>>>> 
>>>>>>>> 
>>>>>>>> 
>>>>>>>>> On 2019-05-17, at 16:18, Michael Boulineau <michael.p.boulineau at gmail.com> wrote:
>>>>>>>>> 
>>>>>>>>> Very interesting. I'm sure I'll be trying to get rid of the byte order
>>>>>>>>> mark eventually. But right now, I'm more worried about getting the
>>>>>>>>> character vector into either a csv file or data.frame; that way, I can
>>>>>>>>> be able to work with the data neatly tabulated into four columns:
>>>>>>>>> date, time, person, comment. I assume it's a write.csv function, but I
>>>>>>>>> don't know what arguments to put in it. header=FALSE? fill=T?
>>>>>>>>> 
>>>>>>>>> Micheal
>>>>>>>>> 
>>>>>>>>> On Fri, May 17, 2019 at 1:03 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>>>>>>>>> 
>>>>>>>>>> If byte order mark is the issue then you can specify the file encoding as "UTF-8-BOM" and it won't show up in your data any more.
>>>>>>>>>> 
>>>>>>>>>> On May 17, 2019 12:12:17 PM PDT, William Dunlap via R-help <r-help at r-project.org> wrote:
>>>>>>>>>>> The pattern I gave worked for the lines that you originally showed from
>>>>>>>>>>> the
>>>>>>>>>>> data file ('a'), before you put commas into them.  If the name is
>>>>>>>>>>> either of
>>>>>>>>>>> the form "<name>" or "***" then the "(<[^>]*>)" needs to be changed so
>>>>>>>>>>> something like "(<[^>]*>|[*]{3})".
>>>>>>>>>>> 
>>>>>>>>>>> The " ???" at the start of the imported data may come from the byte
>>>>>>>>>>> order
>>>>>>>>>>> mark that Windows apps like to put at the front of a text file in UTF-8
>>>>>>>>>>> or
>>>>>>>>>>> UTF-16 format.
>>>>>>>>>>> 
>>>>>>>>>>> Bill Dunlap
>>>>>>>>>>> TIBCO Software
>>>>>>>>>>> wdunlap tibco.com
>>>>>>>>>>> 
>>>>>>>>>>> 
>>>>>>>>>>> On Fri, May 17, 2019 at 11:53 AM Michael Boulineau <
>>>>>>>>>>> michael.p.boulineau at gmail.com> wrote:
>>>>>>>>>>> 
>>>>>>>>>>>> This seemed to work:
>>>>>>>>>>>> 
>>>>>>>>>>>>> a <- readLines ("hangouts-conversation-6.csv.txt")
>>>>>>>>>>>>> b <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "\\1,\\2,\\3,\\4", a)
>>>>>>>>>>>>> b [1:84]
>>>>>>>>>>>> 
>>>>>>>>>>>> And the first 85 lines looks like this:
>>>>>>>>>>>> 
>>>>>>>>>>>> [83] "2016-06-28 21:02:28 *** Jane Doe started a video chat"
>>>>>>>>>>>> [84] "2016-06-28 21:12:43 *** John Doe ended a video chat"
>>>>>>>>>>>> 
>>>>>>>>>>>> Then they transition to the commas:
>>>>>>>>>>>> 
>>>>>>>>>>>>> b [84:100]
>>>>>>>>>>>> [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
>>>>>>>>>>>> [2] "2016-07-01,02:50:35,<John Doe>,hey"
>>>>>>>>>>>> [3] "2016-07-01,02:51:26,<John Doe>,waiting for plane to Edinburgh"
>>>>>>>>>>>> [4] "2016-07-01,02:51:45,<John Doe>,thinking about my boo"
>>>>>>>>>>>> 
>>>>>>>>>>>> Even the strange bit on line 6347 was caught by this:
>>>>>>>>>>>> 
>>>>>>>>>>>>> b [6346:6348]
>>>>>>>>>>>> [1] "2016-10-21,10:56:29,<John Doe>,John_Doe"
>>>>>>>>>>>> [2] "2016-10-21,10:56:37,<John Doe>,Admit#8242"
>>>>>>>>>>>> [3] "2016-10-21,11:00:13,<Jane Doe>,Okay so you have a discussion"
>>>>>>>>>>>> 
>>>>>>>>>>>> Perhaps most awesomely, the code catches spaces that are interposed
>>>>>>>>>>>> into the comment itself:
>>>>>>>>>>>> 
>>>>>>>>>>>>> b [4]
>>>>>>>>>>>> [1] "2016-01-27,09:15:20,<Jane Doe>,Hey "
>>>>>>>>>>>>> b [85]
>>>>>>>>>>>> [1] "2016-07-01,02:50:35,<John Doe>,hey"
>>>>>>>>>>>> 
>>>>>>>>>>>> Notice whether there is a space after the "hey" or not.
>>>>>>>>>>>> 
>>>>>>>>>>>> These are the first two lines:
>>>>>>>>>>>> 
>>>>>>>>>>>> [1] "???2016-01-27 09:14:40 *** Jane Doe started a video chat"
>>>>>>>>>>>> [2] "2016-01-27,09:15:20,<Jane
>>>>>>>>>>>> Doe>,
>>>>>>>>>>>> 
>>>>>>>>>>> https://lh3.googleusercontent.com/-_WQF5kRcnpk/Vqj7J4aK1jI/AAAAAAAAAVA/GVqutPqbSuo/s0/be8ded30-87a6-4e80-bdfa-83ed51591dbf
>>>>>>>>>>>> "
>>>>>>>>>>>> 
>>>>>>>>>>>> So, who knows what happened with the ??? at the beginning of [1]
>>>>>>>>>>>> directly above. But notice how there are no commas in [1] but there
>>>>>>>>>>>> appear in [2]. I don't see why really long ones like [2] directly
>>>>>>>>>>>> above would be a problem, were they to be translated into a csv or
>>>>>>>>>>>> data frame column.
>>>>>>>>>>>> 
>>>>>>>>>>>> Now, with the commas in there, couldn't we write this into a csv or a
>>>>>>>>>>>> data.frame? Some of this data will end up being garbage, I imagine.
>>>>>>>>>>>> Like in [2] directly above. Or with [83] and [84] at the top of this
>>>>>>>>>>>> discussion post/email. Embarrassingly, I've been trying to convert
>>>>>>>>>>>> this into a data.frame or csv but I can't manage to. I've been using
>>>>>>>>>>>> the write.csv function, but I don't think I've been getting the
>>>>>>>>>>>> arguments correct.
>>>>>>>>>>>> 
>>>>>>>>>>>> At the end of the day, I would like a data.frame and/or csv with the
>>>>>>>>>>>> following four columns: date, time, person, comment.
>>>>>>>>>>>> 
>>>>>>>>>>>> I tried this, too:
>>>>>>>>>>>> 
>>>>>>>>>>>>> c <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
>>>>>>>>>>>> + [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
>>>>>>>>>>>> +                 a, proto=data.frame(stringsAsFactors=FALSE,
>>>>>>>>>>> When="",
>>>>>>>>>>>> Who="",
>>>>>>>>>>>> +                                     What=""))
>>>>>>>>>>>> 
>>>>>>>>>>>> But all I got was this:
>>>>>>>>>>>> 
>>>>>>>>>>>>> c [1:100, ]
>>>>>>>>>>>> When  Who What
>>>>>>>>>>>> 1   <NA> <NA> <NA>
>>>>>>>>>>>> 2   <NA> <NA> <NA>
>>>>>>>>>>>> 3   <NA> <NA> <NA>
>>>>>>>>>>>> 4   <NA> <NA> <NA>
>>>>>>>>>>>> 5   <NA> <NA> <NA>
>>>>>>>>>>>> 6   <NA> <NA> <NA>
>>>>>>>>>>>> 
>>>>>>>>>>>> It seems to have caught nothing.
>>>>>>>>>>>> 
>>>>>>>>>>>>> unique (c)
>>>>>>>>>>>> When  Who What
>>>>>>>>>>>> 1 <NA> <NA> <NA>
>>>>>>>>>>>> 
>>>>>>>>>>>> But I like that it converted into columns. That's a really great
>>>>>>>>>>>> format. With a little tweaking, it'd be a great code for this data
>>>>>>>>>>>> set.
>>>>>>>>>>>> 
>>>>>>>>>>>> Michael
>>>>>>>>>>>> 
>>>>>>>>>>>> On Fri, May 17, 2019 at 8:20 AM William Dunlap via R-help
>>>>>>>>>>>> <r-help at r-project.org> wrote:
>>>>>>>>>>>>> 
>>>>>>>>>>>>> Consider using readLines() and strcapture() for reading such a
>>>>>>>>>>> file.
>>>>>>>>>>>> E.g.,
>>>>>>>>>>>>> suppose readLines(files) produced a character vector like
>>>>>>>>>>>>> 
>>>>>>>>>>>>> x <- c("2016-10-21 10:35:36 <Jane Doe> What's your login",
>>>>>>>>>>>>>      "2016-10-21 10:56:29 <John Doe> John_Doe",
>>>>>>>>>>>>>      "2016-10-21 10:56:37 <John Doe> Admit#8242",
>>>>>>>>>>>>>      "October 23, 1819 12:34 <Jane Eyre> I am not an angel")
>>>>>>>>>>>>> 
>>>>>>>>>>>>> Then you can make a data.frame with columns When, Who, and What by
>>>>>>>>>>>>> supplying a pattern containing three parenthesized capture
>>>>>>>>>>> expressions:
>>>>>>>>>>>>>> z <- strcapture("^([[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}
>>>>>>>>>>>>> [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}) +(<[^>]*>) *(.*$)",
>>>>>>>>>>>>>         x, proto=data.frame(stringsAsFactors=FALSE, When="",
>>>>>>>>>>> Who="",
>>>>>>>>>>>>> What=""))
>>>>>>>>>>>>>> str(z)
>>>>>>>>>>>>> 'data.frame':   4 obs. of  3 variables:
>>>>>>>>>>>>> $ When: chr  "2016-10-21 10:35:36" "2016-10-21 10:56:29"
>>>>>>>>>>> "2016-10-21
>>>>>>>>>>>>> 10:56:37" NA
>>>>>>>>>>>>> $ Who : chr  "<Jane Doe>" "<John Doe>" "<John Doe>" NA
>>>>>>>>>>>>> $ What: chr  "What's your login" "John_Doe" "Admit#8242" NA
>>>>>>>>>>>>> 
>>>>>>>>>>>>> Lines that don't match the pattern result in NA's - you might make
>>>>>>>>>>> a
>>>>>>>>>>>> second
>>>>>>>>>>>>> pass over the corresponding elements of x with a new pattern.
>>>>>>>>>>>>> 
>>>>>>>>>>>>> You can convert the When column from character to time with
>>>>>>>>>>> as.POSIXct().
>>>>>>>>>>>>> 
>>>>>>>>>>>>> Bill Dunlap
>>>>>>>>>>>>> TIBCO Software
>>>>>>>>>>>>> wdunlap tibco.com
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>> On Thu, May 16, 2019 at 8:30 PM David Winsemius
>>>>>>>>>>> <dwinsemius at comcast.net>
>>>>>>>>>>>>> wrote:
>>>>>>>>>>>>> 
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> On 5/16/19 3:53 PM, Michael Boulineau wrote:
>>>>>>>>>>>>>>> OK. So, I named the object test and then checked the 6347th
>>>>>>>>>>> item
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> test <- readLines ("hangouts-conversation.txt)
>>>>>>>>>>>>>>>> test [6347]
>>>>>>>>>>>>>>> [1] "2016-10-21 10:56:37 <John Doe> Admit#8242"
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> Perhaps where it was getting screwed up is, since the end of
>>>>>>>>>>> this is
>>>>>>>>>>>> a
>>>>>>>>>>>>>>> number (8242), then, given that there's no space between the
>>>>>>>>>>> number
>>>>>>>>>>>>>>> and what ought to be the next row, R didn't know where to draw
>>>>>>>>>>> the
>>>>>>>>>>>>>>> line. Sure enough, it looks like this when I go to the original
>>>>>>>>>>> file
>>>>>>>>>>>>>>> and control f "#8242"
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> 2016-10-21 10:35:36 <Jane Doe> What's your login
>>>>>>>>>>>>>>> 2016-10-21 10:56:29 <John Doe> John_Doe
>>>>>>>>>>>>>>> 2016-10-21 10:56:37 <John Doe> Admit#8242
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> An octothorpe is an end of line signifier and is interpreted as
>>>>>>>>>>>> allowing
>>>>>>>>>>>>>> comments. You can prevent that interpretation with suitable
>>>>>>>>>>> choice of
>>>>>>>>>>>>>> parameters to `read.table` or `read.csv`. I don't understand why
>>>>>>>>>>> that
>>>>>>>>>>>>>> should cause anu error or a failure to match that pattern.
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> 2016-10-21 11:00:13 <Jane Doe> Okay so you have a discussion
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> Again, it doesn't look like that in the file. Gmail
>>>>>>>>>>> automatically
>>>>>>>>>>>>>>> formats it like that when I paste it in. More to the point, it
>>>>>>>>>>> looks
>>>>>>>>>>>>>>> like
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> 2016-10-21 10:35:36 <Jane Doe> What's your login2016-10-21
>>>>>>>>>>> 10:56:29
>>>>>>>>>>>>>>> <John Doe> John_Doe2016-10-21 10:56:37 <John Doe>
>>>>>>>>>>>> Admit#82422016-10-21
>>>>>>>>>>>>>>> 11:00:13 <Jane Doe> Okay so you have a discussion
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> Notice Admit#82422016. So there's that.
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> Then I built object test2.
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> test2 <- sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4",
>>>>>>>>>>> test)
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> This worked for 84 lines, then this happened.
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> It may have done something but as you later discovered my first
>>>>>>>>>>> code
>>>>>>>>>>>> for
>>>>>>>>>>>>>> the pattern was incorrect. I had tested it (and pasted in the
>>>>>>>>>>> results
>>>>>>>>>>>> of
>>>>>>>>>>>>>> the test) . The way to refer to a capture class is with
>>>>>>>>>>> back-slashes
>>>>>>>>>>>>>> before the numbers, not forward-slashes. Try this:
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)",
>>>>>>>>>>> "\\1,\\2,\\3,\\4",
>>>>>>>>>>>> chrvec)
>>>>>>>>>>>>>>> newvec
>>>>>>>>>>>>>> [1] "2016-07-01,02:50:35,<john>,hey"
>>>>>>>>>>>>>> [2] "2016-07-01,02:51:26,<jane>,waiting for plane to Edinburgh"
>>>>>>>>>>>>>> [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
>>>>>>>>>>>>>> [4] "2016-07-01,02:52:07,<jane>,nothing crappy has happened,
>>>>>>>>>>> not
>>>>>>>>>>>> really"
>>>>>>>>>>>>>> [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast,
>>>>>>>>>>> didn't
>>>>>>>>>>>> sleep"
>>>>>>>>>>>>>> [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or
>>>>>>>>>>> where I am
>>>>>>>>>>>>>> really"
>>>>>>>>>>>>>> [7] "2016-07-01,02:54:17,<john>,just know it's london"
>>>>>>>>>>>>>> [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
>>>>>>>>>>>>>> [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a good
>>>>>>>>>>> eay"
>>>>>>>>>>>>>> [10] "2016-07-01 02:58:56 <jone>"
>>>>>>>>>>>>>> [11] "2016-07-01 02:59:34 <jane>"
>>>>>>>>>>>>>> [12] "2016-07-01,03:02:48,<john>,British security is a little
>>>>>>>>>>> more
>>>>>>>>>>>>>> rigorous..."
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> I made note of the fact that the 10th and 11th lines had no
>>>>>>>>>>> commas.
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> test2 [84]
>>>>>>>>>>>>>>> [1] "2016-06-28 21:12:43 *** John Doe ended a video chat"
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> That line didn't have any "<" so wasn't matched.
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> You could remove all none matching lines for pattern of
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> dates<space>times<space>"<"<name>">"<space><anything>
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> with:
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> chrvec <- chrvec[ grepl("^.{10} .{8} <.+> .+$)", chrvec)]
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> Do read:
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> ?read.csv
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> ?regex
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> --
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> David
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> test2 [85]
>>>>>>>>>>>>>>> [1] "//1,//2,//3,//4"
>>>>>>>>>>>>>>>> test [85]
>>>>>>>>>>>>>>> [1] "2016-07-01 02:50:35 <John Doe> hey"
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> Notice how I toggled back and forth between test and test2
>>>>>>>>>>> there. So,
>>>>>>>>>>>>>>> whatever happened with the regex, it happened in the switch
>>>>>>>>>>> from 84
>>>>>>>>>>>> to
>>>>>>>>>>>>>>> 85, I guess. It went on like
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> [990] "//1,//2,//3,//4"
>>>>>>>>>>>>>>> [991] "//1,//2,//3,//4"
>>>>>>>>>>>>>>> [992] "//1,//2,//3,//4"
>>>>>>>>>>>>>>> [993] "//1,//2,//3,//4"
>>>>>>>>>>>>>>> [994] "//1,//2,//3,//4"
>>>>>>>>>>>>>>> [995] "//1,//2,//3,//4"
>>>>>>>>>>>>>>> [996] "//1,//2,//3,//4"
>>>>>>>>>>>>>>> [997] "//1,//2,//3,//4"
>>>>>>>>>>>>>>> [998] "//1,//2,//3,//4"
>>>>>>>>>>>>>>> [999] "//1,//2,//3,//4"
>>>>>>>>>>>>>>> [1000] "//1,//2,//3,//4"
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> up until line 1000, then I reached max.print.
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> Michael
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> On Thu, May 16, 2019 at 1:05 PM David Winsemius <
>>>>>>>>>>>> dwinsemius at comcast.net>
>>>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> On 5/16/19 12:30 PM, Michael Boulineau wrote:
>>>>>>>>>>>>>>>>> Thanks for this tip on etiquette, David. I will be sure and
>>>>>>>>>>> not do
>>>>>>>>>>>>>> that again.
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> I tried the read.fwf from the foreign package, with a code
>>>>>>>>>>> like
>>>>>>>>>>>> this:
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> d <- read.fwf("hangouts-conversation.txt",
>>>>>>>>>>>>>>>>>              widths= c(10,10,20,40),
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>> col.names=c("date","time","person","comment"),
>>>>>>>>>>>>>>>>>              strip.white=TRUE)
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> But it threw this error:
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> Error in scan(file = file, what = what, sep = sep, quote =
>>>>>>>>>>> quote,
>>>>>>>>>>>> dec
>>>>>>>>>>>>>> = dec,  :
>>>>>>>>>>>>>>>>> line 6347 did not have 4 elements
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> So what does line 6347 look like? (Use `readLines` and print
>>>>>>>>>>> it
>>>>>>>>>>>> out.)
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> Interestingly, though, the error only happened when I
>>>>>>>>>>> increased the
>>>>>>>>>>>>>>>>> width size. But I had to increase the size, or else I
>>>>>>>>>>> couldn't
>>>>>>>>>>>> "see"
>>>>>>>>>>>>>>>>> anything.  The comment was so small that nothing was being
>>>>>>>>>>>> captured by
>>>>>>>>>>>>>>>>> the size of the column. so to speak.
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> It seems like what's throwing me is that there's no comma
>>>>>>>>>>> that
>>>>>>>>>>>>>>>>> demarcates the end of the text proper. For example:
>>>>>>>>>>>>>>>> Not sure why you thought there should be a comma. Lines
>>>>>>>>>>> usually end
>>>>>>>>>>>>>>>> with  <cr> and or a <lf>.
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> Once you have the raw text in a character vector from
>>>>>>>>>>> `readLines`
>>>>>>>>>>>> named,
>>>>>>>>>>>>>>>> say, 'chrvec', then you could selectively substitute commas
>>>>>>>>>>> for
>>>>>>>>>>>> spaces
>>>>>>>>>>>>>>>> with regex. (Now that you no longer desire to remove the dates
>>>>>>>>>>> and
>>>>>>>>>>>>>> times.)
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> sub("^(.{10}) (.{8}) (<.+>) (.+$)", "//1,//2,//3,//4", chrvec)
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> This will not do any replacements when the pattern is not
>>>>>>>>>>> matched.
>>>>>>>>>>>> See
>>>>>>>>>>>>>>>> this test:
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> newvec <- sub("^(.{10}) (.{8}) (<.+>) (.+$)",
>>>>>>>>>>> "\\1,\\2,\\3,\\4",
>>>>>>>>>>>>>> chrvec)
>>>>>>>>>>>>>>>>> newvec
>>>>>>>>>>>>>>>> [1] "2016-07-01,02:50:35,<john>,hey"
>>>>>>>>>>>>>>>> [2] "2016-07-01,02:51:26,<jane>,waiting for plane to
>>>>>>>>>>> Edinburgh"
>>>>>>>>>>>>>>>> [3] "2016-07-01,02:51:45,<john>,thinking about my boo"
>>>>>>>>>>>>>>>> [4] "2016-07-01,02:52:07,<jane>,nothing crappy has
>>>>>>>>>>> happened, not
>>>>>>>>>>>>>> really"
>>>>>>>>>>>>>>>> [5] "2016-07-01,02:52:20,<john>,plane went by pretty fast,
>>>>>>>>>>> didn't
>>>>>>>>>>>>>> sleep"
>>>>>>>>>>>>>>>> [6] "2016-07-01,02:54:08,<jane>,no idea what time it is or
>>>>>>>>>>> where
>>>>>>>>>>>> I am
>>>>>>>>>>>>>>>> really"
>>>>>>>>>>>>>>>> [7] "2016-07-01,02:54:17,<john>,just know it's london"
>>>>>>>>>>>>>>>> [8] "2016-07-01,02:56:44,<jane>,you are probably asleep"
>>>>>>>>>>>>>>>> [9] "2016-07-01,02:58:45,<jane>,I hope fish was fishy in a
>>>>>>>>>>> good
>>>>>>>>>>>> eay"
>>>>>>>>>>>>>>>> [10] "2016-07-01 02:58:56 <jone>"
>>>>>>>>>>>>>>>> [11] "2016-07-01 02:59:34 <jane>"
>>>>>>>>>>>>>>>> [12] "2016-07-01,03:02:48,<john>,British security is a little
>>>>>>>>>>> more
>>>>>>>>>>>>>>>> rigorous..."
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> You should probably remove the "empty comment" lines.
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> --
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> David.
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> 2016-07-01 15:34:30 <John Doe> Lame. We were in a
>>>>>>>>>>>> starbucks2016-07-01
>>>>>>>>>>>>>>>>> 15:35:02 <Jane Doe> Hmm that's interesting2016-07-01 15:35:09
>>>>>>>>>>> <Jane
>>>>>>>>>>>>>>>>> Doe> You must want coffees2016-07-01 15:35:25 <John Doe>
>>>>>>>>>>> There was
>>>>>>>>>>>>>>>>> lots of Starbucks in my day2016-07-01 15:35:47
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> It was interesting, too, when I pasted the text into the
>>>>>>>>>>> email, it
>>>>>>>>>>>>>>>>> self-formatted into the way I wanted it to look. I had to
>>>>>>>>>>> manually
>>>>>>>>>>>>>>>>> make it look like it does above, since that's the way that it
>>>>>>>>>>>> looks in
>>>>>>>>>>>>>>>>> the txt file. I wonder if it's being organized by XML or
>>>>>>>>>>> something.
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> Anyways, There's always a space between the two sideways
>>>>>>>>>>> carrots,
>>>>>>>>>>>> just
>>>>>>>>>>>>>>>>> like there is right now: <John Doe> See. Space. And there's
>>>>>>>>>>> always
>>>>>>>>>>>> a
>>>>>>>>>>>>>>>>> space between the data and time. Like this. 2016-07-01
>>>>>>>>>>> 15:34:30
>>>>>>>>>>>> See.
>>>>>>>>>>>>>>>>> Space. But there's never a space between the end of the
>>>>>>>>>>> comment and
>>>>>>>>>>>>>>>>> the next date. Like this: We were in a starbucks2016-07-01
>>>>>>>>>>> 15:35:02
>>>>>>>>>>>>>>>>> See. starbucks and 2016 are smooshed together.
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> This code is also on the table right now too.
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> a <- read.table("E:/working
>>>>>>>>>>>>>>>>> directory/-189/hangouts-conversation2.txt", quote="\"",
>>>>>>>>>>>>>>>>> comment.char="", fill=TRUE)
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>> h<-cbind(hangouts.conversation2[,1:2],hangouts.conversation2[,3:5],hangouts.conversation2[,6:9])
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> aa<-gsub("[^[:digit:]]","",h)
>>>>>>>>>>>>>>>>> my.data.num <- as.numeric(str_extract(h, "[0-9]+"))
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> Those last lines are a work in progress. I wish I could
>>>>>>>>>>> import a
>>>>>>>>>>>>>>>>> picture of what it looks like when it's translated into a
>>>>>>>>>>> data
>>>>>>>>>>>> frame.
>>>>>>>>>>>>>>>>> The fill=TRUE helped to get the data in table that kind of
>>>>>>>>>>> sort of
>>>>>>>>>>>>>>>>> works, but the comments keep bleeding into the data and time
>>>>>>>>>>>> column.
>>>>>>>>>>>>>>>>> It's like
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> 2016-07-01 15:59:17 <Jane Doe> Seriously I've never been
>>>>>>>>>>>>>>>>> over               there
>>>>>>>>>>>>>>>>> 2016-07-01 15:59:27 <Jane Doe> It confuses me :(
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> And then, maybe, the "seriously" will be in a column all to
>>>>>>>>>>>> itself, as
>>>>>>>>>>>>>>>>> will be the "I've'"and the "never" etc.
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> I will use a regular expression if I have to, but it would be
>>>>>>>>>>> nice
>>>>>>>>>>>> to
>>>>>>>>>>>>>>>>> keep the dates and times on there. Originally, I thought they
>>>>>>>>>>> were
>>>>>>>>>>>>>>>>> meaningless, but I've since changed my mind on that count.
>>>>>>>>>>> The
>>>>>>>>>>>> time of
>>>>>>>>>>>>>>>>> day isn't so important. But, especially since, say, Gmail
>>>>>>>>>>> itself
>>>>>>>>>>>> knows
>>>>>>>>>>>>>>>>> how to quickly recognize what it is, I know it can be done. I
>>>>>>>>>>> know
>>>>>>>>>>>>>>>>> this data has structure to it.
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> Michael
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> On Wed, May 15, 2019 at 8:47 PM David Winsemius <
>>>>>>>>>>>>>> dwinsemius at comcast.net> wrote:
>>>>>>>>>>>>>>>>>> On 5/15/19 4:07 PM, Michael Boulineau wrote:
>>>>>>>>>>>>>>>>>>> I have a wild and crazy text file, the head of which looks
>>>>>>>>>>> like
>>>>>>>>>>>> this:
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> 2016-07-01 02:50:35 <john> hey
>>>>>>>>>>>>>>>>>>> 2016-07-01 02:51:26 <jane> waiting for plane to Edinburgh
>>>>>>>>>>>>>>>>>>> 2016-07-01 02:51:45 <john> thinking about my boo
>>>>>>>>>>>>>>>>>>> 2016-07-01 02:52:07 <jane> nothing crappy has happened, not
>>>>>>>>>>>> really
>>>>>>>>>>>>>>>>>>> 2016-07-01 02:52:20 <john> plane went by pretty fast,
>>>>>>>>>>> didn't
>>>>>>>>>>>> sleep
>>>>>>>>>>>>>>>>>>> 2016-07-01 02:54:08 <jane> no idea what time it is or where
>>>>>>>>>>> I am
>>>>>>>>>>>>>> really
>>>>>>>>>>>>>>>>>>> 2016-07-01 02:54:17 <john> just know it's london
>>>>>>>>>>>>>>>>>>> 2016-07-01 02:56:44 <jane> you are probably asleep
>>>>>>>>>>>>>>>>>>> 2016-07-01 02:58:45 <jane> I hope fish was fishy in a good
>>>>>>>>>>> eay
>>>>>>>>>>>>>>>>>>> 2016-07-01 02:58:56 <jone>
>>>>>>>>>>>>>>>>>>> 2016-07-01 02:59:34 <jane>
>>>>>>>>>>>>>>>>>>> 2016-07-01 03:02:48 <john> British security is a little
>>>>>>>>>>> more
>>>>>>>>>>>>>> rigorous...
>>>>>>>>>>>>>>>>>> Looks entirely not-"crazy". Typical log file format.
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> Two possibilities: 1) Use `read.fwf` from pkg foreign; 2)
>>>>>>>>>>> Use
>>>>>>>>>>>> regex
>>>>>>>>>>>>>>>>>> (i.e. the sub-function) to strip everything up to the "<".
>>>>>>>>>>> Read
>>>>>>>>>>>>>>>>>> `?regex`. Since that's not a metacharacters you could use a
>>>>>>>>>>>> pattern
>>>>>>>>>>>>>>>>>> ".+<" and replace with "".
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> And do read the Posting Guide. Cross-posting to
>>>>>>>>>>> StackOverflow and
>>>>>>>>>>>>>> Rhelp,
>>>>>>>>>>>>>>>>>> at least within hours of each, is considered poor manners.
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> --
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> David.
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> It goes on for a while. It's a big file. But I feel like
>>>>>>>>>>> it's
>>>>>>>>>>>> going
>>>>>>>>>>>>>> to
>>>>>>>>>>>>>>>>>>> be difficult to annotate with the coreNLP library or
>>>>>>>>>>> package. I'm
>>>>>>>>>>>>>>>>>>> doing natural language processing. In other words, I'm
>>>>>>>>>>> curious
>>>>>>>>>>>> as to
>>>>>>>>>>>>>>>>>>> how I would shave off the dates, that is, to make it look
>>>>>>>>>>> like:
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> <john> hey
>>>>>>>>>>>>>>>>>>> <jane> waiting for plane to Edinburgh
>>>>>>>>>>>>>>>>>>> <john> thinking about my boo
>>>>>>>>>>>>>>>>>>> <jane> nothing crappy has happened, not really
>>>>>>>>>>>>>>>>>>> <john> plane went by pretty fast, didn't sleep
>>>>>>>>>>>>>>>>>>> <jane> no idea what time it is or where I am really
>>>>>>>>>>>>>>>>>>> <john> just know it's london
>>>>>>>>>>>>>>>>>>> <jane> you are probably asleep
>>>>>>>>>>>>>>>>>>> <jane> I hope fish was fishy in a good eay
>>>>>>>>>>>>>>>>>>> <jone>
>>>>>>>>>>>>>>>>>>> <jane>
>>>>>>>>>>>>>>>>>>> <john> British security is a little more rigorous...
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> To be clear, then, I'm trying to clean a large text file by
>>>>>>>>>>>> writing a
>>>>>>>>>>>>>>>>>>> regular expression? such that I create a new object with no
>>>>>>>>>>>> numbers
>>>>>>>>>>>>>> or
>>>>>>>>>>>>>>>>>>> dates.
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> Michael
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> ______________________________________________
>>>>>>>>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
>>>>>>>>>>> more,
>>>>>>>>>>>> see
>>>>>>>>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>>>>>>>>>>> and provide commented, minimal, self-contained,
>>>>>>>>>>> reproducible
>>>>>>>>>>>> code.
>>>>>>>>>>>>>>>>> ______________________________________________
>>>>>>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>>>>>>>>>> see
>>>>>>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>>>>>>>> code.
>>>>>>>>>>>>>>>> ______________________________________________
>>>>>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>>>>>>>>>> see
>>>>>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>>>>>>>> code.
>>>>>>>>>>>>>>> ______________________________________________
>>>>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>>>>>>>>>> see
>>>>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>>>>>>>> code.
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> ______________________________________________
>>>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>>>>>>>> code.
>>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>>    [[alternative HTML version deleted]]
>>>>>>>>>>>>> 
>>>>>>>>>>>>> ______________________________________________
>>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>>>>>>> 
>>>>>>>>>>>> ______________________________________________
>>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>>>>>>> 
>>>>>>>>>>> 
>>>>>>>>>>>  [[alternative HTML version deleted]]
>>>>>>>>>>> 
>>>>>>>>>>> ______________________________________________
>>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>>>>> 
>>>>>>>>>> --
>>>>>>>>>> Sent from my phone. Please excuse my brevity.
>>>>>>>>> 
>>>>>>>>> ______________________________________________
>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>>> 
>>>>>>> 
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>> 
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>> 
>>>> 
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @neh@@b|@hno| @end|ng |rom gm@||@com  Mon May 20 20:19:43 2019
From: @neh@@b|@hno| @end|ng |rom gm@||@com (Sneha Bishnoi)
Date: Mon, 20 May 2019 14:19:43 -0400
Subject: [R] =?utf-8?q?Brown=E2=80=99s_One-parameter_Linear_Method_in_R?=
Message-ID: <CAOsJHwBMBet0cksAZGJvaeuwzbJAtMtf9tCH4x55D+Zw3OpfjA@mail.gmail.com>

Dear R-Help members,

Do you guys know any packages that implement Brown's exponential smoothing
in R. I have using forecast package for most of my forecasting algorithms,
but a new problem requires me to build a best fit forecast method &
parameter selection for each forecast method with Brown's exponential
smoothing as one of the forecast method.
Would appreciate any help in this regard before I start looking to write a
code from scratch.

Thanks in advance for your help!

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon May 20 20:52:07 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 20 May 2019 11:52:07 -0700
Subject: [R] 
 =?utf-8?q?Brown=E2=80=99s_One-parameter_Linear_Method_in_R?=
In-Reply-To: <CAOsJHwBMBet0cksAZGJvaeuwzbJAtMtf9tCH4x55D+Zw3OpfjA@mail.gmail.com>
References: <CAOsJHwBMBet0cksAZGJvaeuwzbJAtMtf9tCH4x55D+Zw3OpfjA@mail.gmail.com>
Message-ID: <06D434B5-1588-4129-8993-62A681354466@dcn.davis.ca.us>

Always try rseek.org and indicate what you searched with and what else you hope can be found before posting requests like this. I don't know this specific algorithm but a couple of hits for "Brown exponential smoothing" looked promising to me.

On May 20, 2019 11:19:43 AM PDT, Sneha Bishnoi <sneha.bishnoi at gmail.com> wrote:
>Dear R-Help members,
>
>Do you guys know any packages that implement Brown's exponential
>smoothing
>in R. I have using forecast package for most of my forecasting
>algorithms,
>but a new problem requires me to build a best fit forecast method &
>parameter selection for each forecast method with Brown's exponential
>smoothing as one of the forecast method.
>Would appreciate any help in this regard before I start looking to
>write a
>code from scratch.
>
>Thanks in advance for your help!
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From ||nu@@|@chen @end|ng |rom gm@||@com  Sat May 18 16:19:57 2019
From: ||nu@@|@chen @end|ng |rom gm@||@com (Linus Chen)
Date: Sat, 18 May 2019 16:19:57 +0200
Subject: [R] Nested structure data simulation
In-Reply-To: <1342550043.2326226.1558187826295@mail.yahoo.com>
References: <1342550043.2326226.1558187826295.ref@mail.yahoo.com>
 <1342550043.2326226.1558187826295@mail.yahoo.com>
Message-ID: <CAPm+3sBE3RX6kpogMXDXB6G+nHYWvsGTX4duztmM+OCDpRT76Q@mail.gmail.com>

Dear varin sacha,

Not very sure what you want, but will the following help a little?

tmp <- rep(c("C1","C2","C3","C4","C5","C6"), 50) # make a character
vector, with 50 "C1", 50 "C2", ...
classroom <- tmp[sample(1:300)] # make a random permutation.

Certainly you may also make it into one line:
classroom <- rep(c("C1","C2","C3","C4","C5","C6"), 50) [sample(1:30)]

Best,
Lei Chen

On Sat, May 18, 2019 at 3:57 PM varin sacha via R-help
<r-help at r-project.org> wrote:
>
> Dear R-Experts,
>
> In a data simulation, I would like a balanced distribution with a nested structure for classroom and teacher (not for school). I mean 50 pupils belonging to C1, 50 other pupils belonging to C2, 50 other pupils belonging to C3 and so on. Then I want the 50 pupils belonging to C1 with T1, the 50 pupils belonging to C2 with T2, the 50 pupils belonging to C3 with T3 and so on. The school don?t have to be nested, I just want a balanced distribution, I mean 60 pupils in S1, 60 other pupils in S2 and so on.
> Here below the reproducible example.
> Many thanks for your help.
>
> ##############
> set.seed(123)
> # G?n?ration al?atoire des colonnes
> pupils<-1:300
> classroom<-sample(c("C1","C2","C3","C4","C5","C6"),300,replace=T)   teacher<-sample(c("T1","T2","T3","T4","T5","T6"),300,replace=T)   school<-sample(c("S1","S2","S3","S4","S5"),300,replace=T)
> ##############
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ||nu@@|@chen @end|ng |rom gm@||@com  Sun May 19 10:45:05 2019
From: ||nu@@|@chen @end|ng |rom gm@||@com (Linus Chen)
Date: Sun, 19 May 2019 10:45:05 +0200
Subject: [R] Nested structure data simulation
In-Reply-To: <570252025.3999934.1558209817043@mail.yahoo.com>
References: <1342550043.2326226.1558187826295.ref@mail.yahoo.com>
 <1342550043.2326226.1558187826295@mail.yahoo.com>
 <BBDA4312-A62F-450A-9E9F-655DAF278FF2@utoronto.ca>
 <570252025.3999934.1558209817043@mail.yahoo.com>
Message-ID: <CAPm+3sCQk=qZu9gc2LStZQNRZfoVMrhQ5Wc8Q3OrfK3BGjMmvQ@mail.gmail.com>

Dear varin sacha,

I think it will help us help you, if you give a clearer description of
what exactly you want.

I assume the situation is that you know what a data structure you
want, but do not know
how to conveniently create such structure.
And that is where others can help you.
So, please, describe the wanted data structure more thoroughly,
ideally with example.

Thanks,
Lei

On Sat, May 18, 2019 at 10:04 PM varin sacha via R-help
<r-help at r-project.org> wrote:
>
> Dear Boris,
>
> Yes, top-down, no problem. Many thanks, but in your code did you not forget "teacher" ? As a reminder teacher has to be nested with classes. I mean the 50 pupils belonging to C1 must be with (teacher 1) T1, the 50 pupils belonging to C2 with T2, the 50 pupils belonging to C3 with T3 and so on.
>
> Best,
>
>
> Le samedi 18 mai 2019 ? 16:52:48 UTC+2, Boris Steipe <boris.steipe at utoronto.ca> a ?crit :
>
>
>
>
>
> Can you build your data top-down?
>
>
>
> schools <- paste("s", 1:6, sep="")
>
> classes <- character()
> for (school in schools) {
>   classes <- c(classes, paste(school, paste("c", 1:5, sep=""), sep = "."))
> }
>
> pupils <- character()
> for (class in classes) {
>   pupils <- c(pupils, paste(class, paste("p", 1:10, sep=""), sep = "."))
> }
>
>
>
> B.
>
>
>
> > On 2019-05-18, at 09:57, varin sacha via R-help <r-help at r-project.org> wrote:
> >
> > Dear R-Experts,
> >
> > In a data simulation, I would like a balanced distribution with a nested structure for classroom and teacher (not for school). I mean 50 pupils belonging to C1, 50 other pupils belonging to C2, 50 other pupils belonging to C3 and so on. Then I want the 50 pupils belonging to C1 with T1, the 50 pupils belonging to C2 with T2, the 50 pupils belonging to C3 with T3 and so on. The school don?t have to be nested, I just want a balanced distribution, I mean 60 pupils in S1, 60 other pupils in S2 and so on.
> > Here below the reproducible example.
> > Many thanks for your help.
> >
> > ##############
> > set.seed(123)
> > # G?n?ration al?atoire des colonnes
> > pupils<-1:300
> > classroom<-sample(c("C1","C2","C3","C4","C5","C6"),300,replace=T)  teacher<-sample(c("T1","T2","T3","T4","T5","T6"),300,replace=T)  school<-sample(c("S1","S2","S3","S4","S5"),300,replace=T)
>
> > ##############
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ||nu@@|@chen @end|ng |rom gm@||@com  Sun May 19 20:59:28 2019
From: ||nu@@|@chen @end|ng |rom gm@||@com (Linus Chen)
Date: Sun, 19 May 2019 20:59:28 +0200
Subject: [R] Nested structure data simulation
In-Reply-To: <370367748.4299867.1558278842667@mail.yahoo.com>
References: <1342550043.2326226.1558187826295.ref@mail.yahoo.com>
 <1342550043.2326226.1558187826295@mail.yahoo.com>
 <BBDA4312-A62F-450A-9E9F-655DAF278FF2@utoronto.ca>
 <570252025.3999934.1558209817043@mail.yahoo.com>
 <CAPm+3sCQk=qZu9gc2LStZQNRZfoVMrhQ5Wc8Q3OrfK3BGjMmvQ@mail.gmail.com>
 <1560846858.4204873.1558267547377@mail.yahoo.com>
 <5F8F8017-2E90-4DE5-898F-492C6CC57E1A@utoronto.ca>
 <370367748.4299867.1558278842667@mail.yahoo.com>
Message-ID: <CAPm+3sB63a4KWgaWk3gynsFtHP7AaN8SR_HrxcE=Cab8g8_xMQ@mail.gmail.com>

Dear varin sacha

On Sun, May 19, 2019 at 5:14 PM varin sacha via R-help
<r-help at r-project.org> wrote:
>
> Dear Boris,
>
> Great !!!! But what about Mark in your R code ? Don't we have to precise in the R code that mark ranges between 1 to 6 (1 ; 1.5 ; 2 ; 2.5 ; 3 ; 3.5 ; 4 ; 4.5 ; 5 ; 5.5 ; 6) ?

I think Boris is just setting up a framework for you. It is up to you
to decide the actual values. :)
You maybe want to create a MyData object, with the method Boris has
shown, but filling the Mark field with random numbers.

Cheers,
Lei

>
> By the way, to fit a linear mixed model, I use lme4 package and then the lmer function works with the variables like in this example here below :
>
> library(lme4)
> mm=lmer(Mark ~Gender + (1 | School / Class), data=Dataset)
>
> With your R code, how can I write the lmer function to make it work ?
>
> Best,
> S.
>
>
>
>
>
>
>
> Le dimanche 19 mai 2019 ? 15:26:39 UTC+2, Boris Steipe <boris.steipe at utoronto.ca> a ?crit :
>
>
>
>
>
> Fair enough - there are additional assumptions needed, which I make as follows:
>   - each class has the same size
>   - each teacher teaches the same number of classes
>   - the number of boys and girls is random within a class
>   - there are 60% girls  (just for illustration that it does not have to be equal)
>
>
> To make the dependencies explicit, I define them so, and in a way that they can't be inconsistent.
>
> nS <- 10        # Schools
> nTpS <- 5      # Teachers per School
> nCpT <- 2      # Classes per teacher
> nPpC <- 20      # Pupils per class
> nS * nTpS * nCpT * nPpC == 2000  # Validate
>
>
> mySim <- data.frame(School  = paste0("s", rep(1:nS, each = nTpS*nCpT*nPpC)),
>                     Teacher = paste0("t", rep(1:(nTpS*nS), each = nCpT*nPpC)),
>                     Class  = paste0("c", rep(1:(nCpT*nTpS*nS), each = nPpC)),
>                     Gender  = sample(c("boy", "girl"),
>                                     (nS*nTpS*nCpT*nPpC),
>                                     prob = c(0.4, 0.6),
>                                     replace = TRUE),
>                     Mark    = numeric(nS*nTpS*nCpT*nPpC),
>                     stringsAsFactors = FALSE)
>
>
> Then you fill mySim$Mark with values from your linear mixed model ...
>
> mySim$Mark[i] <- simMarks(mySim[i])  # ... or something equivalent.
>
>
> All good?
>
> Cheers,
> Boris
>
>
>
> > On 2019-05-19, at 08:05, varin sacha <varinsacha at yahoo.fr> wrote:
> >
> > Many thanks to all of you for your responses.
> >
> > So, I will try to be clearer with a larger example. Te end of my mail is the more important to understand what I am trying to do. I am trying to simulate data to fit a linear mixed model (nested not crossed). More precisely, I would love to get at the end of the process, a table (.txt) with columns and rows. Column 1 and Rows will be the 2000 pupils and the columns the different variables : Column 2 = classes ; Column 3 = teachers, Column 4 = schools ; Column 5 = gender (boy or girl) ; Column 6 = mark in Frecnh
> >
> > Pupils are nested  in classes, classes are nested in schools. The teacher are part of the process.
> >
> > I want to simulate a dataset with n=2000 pupils, 100 classes, 50 teachers and 10 schools.
> > - Pupils n?1 to pupils n?2000 (p1, p2, p3, p4, ..., p2000)
> > - Classes n?1 to classes n?100 (c1, c2, c3, c4,..., c100)
> > - Teachers n?1 to teacher n?50 ( t1, t2, t3, t4, ..., t50)
> > - Schools n?1 to chool n?10 (s1, s2, s3, s4, ..., s10)
> >
> > The nested structure is as followed :
> >
> > -- School 1 with teacher 1 to teacher 5 (t1, t2, t3, t4 and t5) with classes 1 to classes 10 (c1, c2, c3, c4, c5, c6, c7, c8,c9,c10), pupils n?1 to pupils n?200 (p1, p2, p3, p4,..., p200).
> >
> > -- School 2 with teacher 6 to teacher 10, with classes 11 to classes 20, pupils n?201 to pupils n?400
> >
> > -- and so on
> >
> > The table (.txt) I would love to get at the end is the following :
> >
> >        Class    Teacher    School    gender    Mark
> > 1      c1        t1                s1            boy        5
> > 2      c1        t1                s1            boy        5.5
> > 3      c1        t1                s1            girl        4.5
> > 4      c1        t1                s1            girl        6
> > 5      c1        t1                s1            boy      3.5
> > 6      ...        ....                ....            .....        .....
> >
> > The first 20 rows with c1, with t1, with s1, gender (randomly slected) and mark (andomly selected) from 1 to 6
> > The rows 21 to 40 with c2 with t1 with s1
> > The rows 41 to 60 with c3 with t2 with s1
> > The rows 61 to 80 with c4 with t2 with s1
> > The rows 81 to 100 with c5 with t3 with s1
> > The rows 101 to 120 with c6 with t3 with s1
> > The rows 121 to 140 with c7 with t4 with s1
> > The rows 141 to 160 with c8 with t4 with s1
> > The rows 161 to 180 with c9 with t5 with s1
> > The rows 181 to 200 with c10 with t5 with s1
> >
> > The rows 201 to 220 with c11 with t6 with s2
> > The rows 221 to 240 with c12 with t6 with s2
> >
> > And so on...
> >
> > Is it possible to do that ? Or am I dreaming ?
> >
> >
> > Le dimanche 19 mai 2019 ? 10:45:43 UTC+2, Linus Chen <linus.l.chen at gmail.com> a ?crit :
> >
> >
> >
> >
> >
> > Dear varin sacha,
> >
> > I think it will help us help you, if you give a clearer description of
> > what exactly you want.
> >
> > I assume the situation is that you know what a data structure you
> > want, but do not know
> > how to conveniently create such structure.
> > And that is where others can help you.
> > So, please, describe the wanted data structure more thoroughly,
> > ideally with example.
> >
> > Thanks,
> > Lei
> >
> > On Sat, May 18, 2019 at 10:04 PM varin sacha via R-help
> > <r-help at r-project.org> wrote:
> >>
> >> Dear Boris,
> >>
> >> Yes, top-down, no problem. Many thanks, but in your code did you not forget "teacher" ? As a reminder teacher has to be nested with classes. I mean the 50 pupils belonging to C1 must be with (teacher 1) T1, the 50 pupils belonging to C2 with T2, the 50 pupils belonging to C3 with T3 and so on.
> >>
> >> Best,
> >>
> >>
> >> Le samedi 18 mai 2019 ? 16:52:48 UTC+2, Boris Steipe <boris.steipe at utoronto.ca> a ?crit :
> >>
> >>
> >>
> >>
> >>
> >> Can you build your data top-down?
> >>
> >>
> >>
> >> schools <- paste("s", 1:6, sep="")
> >>
> >> classes <- character()
> >> for (school in schools) {
> >>  classes <- c(classes, paste(school, paste("c", 1:5, sep=""), sep = "."))
> >> }
> >>
> >> pupils <- character()
> >> for (class in classes) {
> >>  pupils <- c(pupils, paste(class, paste("p", 1:10, sep=""), sep = "."))
> >> }
> >>
> >>
> >>
> >> B.
> >>
> >>
> >>
> >>> On 2019-05-18, at 09:57, varin sacha via R-help <r-help at r-project.org> wrote:
> >>>
> >>> Dear R-Experts,
> >>>
> >>> In a data simulation, I would like a balanced distribution with a nested structure for classroom and teacher (not for school). I mean 50 pupils belonging to C1, 50 other pupils belonging to C2, 50 other pupils belonging to C3 and so on. Then I want the 50 pupils belonging to C1 with T1, the 50 pupils belonging to C2 with T2, the 50 pupils belonging to C3 with T3 and so on. The school don?t have to be nested, I just want a balanced distribution, I mean 60 pupils in S1, 60 other pupils in S2 and so on.
> >>> Here below the reproducible example.
> >>> Many thanks for your help.
> >>>
> >>> ##############
> >>> set.seed(123)
> >>> # G?n?ration al?atoire des colonnes
> >>> pupils<-1:300
> >>> classroom<-sample(c("C1","C2","C3","C4","C5","C6"),300,replace=T)  teacher<-sample(c("T1","T2","T3","T4","T5","T6"),300,replace=T)  school<-sample(c("S1","S2","S3","S4","S5"),300,replace=T)
> >>
> >>> ##############
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From eb@15242 @end|ng |rom gm@||@com  Tue May 21 16:55:21 2019
From: eb@15242 @end|ng |rom gm@||@com (Ed Siefker)
Date: Tue, 21 May 2019 09:55:21 -0500
Subject: [R] cygwin clipboard
Message-ID: <CALRb-ofC1zxvYYH5C8mMxDWXesQSsD9kwg=BNeO6ANmN86UcVw@mail.gmail.com>

I'd like to be able to access the windows clipboard from R under Cygwin.
But...

> read.table(file="clipboard")
Error in file(file, "rt") : cannot open the connection
In addition: Warning message:
In file(file, "rt") : unable to contact X11 display
>

Is this supported in any way?  Thanks
-Ed


From kry|ov@r00t @end|ng |rom gm@||@com  Tue May 21 17:26:56 2019
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Tue, 21 May 2019 18:26:56 +0300
Subject: [R] cygwin clipboard
In-Reply-To: <CALRb-ofC1zxvYYH5C8mMxDWXesQSsD9kwg=BNeO6ANmN86UcVw@mail.gmail.com>
References: <CALRb-ofC1zxvYYH5C8mMxDWXesQSsD9kwg=BNeO6ANmN86UcVw@mail.gmail.com>
Message-ID: <20190521182656.13a2e164@trisector>

On Tue, 21 May 2019 09:55:21 -0500
Ed Siefker <ebs15242 at gmail.com> wrote:

> I'd like to be able to access the windows clipboard from R under
> Cygwin.

Searching for "cygwin windows clipboard" offers reading from
and writing to /dev/clipboard as a file and getclip.exe / putclip.exe
from cygutils-extra package[*] which you could use in a pipe().

-- 
Best regards,
Ivan

[*]
https://cygwin.com/cgi-bin2/package-cat.cgi?file=x86_64%2Fcygutils-extra%2Fcygutils-extra-1.4.16-2&grep=cygutils-extra


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue May 21 17:33:07 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 21 May 2019 08:33:07 -0700
Subject: [R] cygwin clipboard
In-Reply-To: <CALRb-ofC1zxvYYH5C8mMxDWXesQSsD9kwg=BNeO6ANmN86UcVw@mail.gmail.com>
References: <CALRb-ofC1zxvYYH5C8mMxDWXesQSsD9kwg=BNeO6ANmN86UcVw@mail.gmail.com>
Message-ID: <25F71E4F-510E-4D2D-BA26-22E5A4150032@dcn.davis.ca.us>

I think the "clipboard" filename is handled in the Windows build of R. I don't use the cygwin build of R, so I can't test, but Googling "cygwin clipboard" finds references to "/dev/clipboard" implemented by cygwin independent from R.

I can't let the opportunity pass to strongly recommend avoiding work processes that route data through the clipboard... it is an anti-pattern for reproducibility... and reproducibility leads to incremental process improvement and more defensible results.

On May 21, 2019 7:55:21 AM PDT, Ed Siefker <ebs15242 at gmail.com> wrote:
>I'd like to be able to access the windows clipboard from R under
>Cygwin.
>But...
>
>> read.table(file="clipboard")
>Error in file(file, "rt") : cannot open the connection
>In addition: Warning message:
>In file(file, "rt") : unable to contact X11 display
>>
>
>Is this supported in any way?  Thanks
>-Ed
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From rm1238 @end|ng |rom njm@@rutger@@edu  Mon May 20 03:25:41 2019
From: rm1238 @end|ng |rom njm@@rutger@@edu (Ricardo Martinez Zamudio)
Date: Mon, 20 May 2019 01:25:41 +0000
Subject: [R] Question Mac OS X compiled binaries for WGCNA f ro R 3.60
Message-ID: <56E1578C-FB0F-4A6B-8B8B-A6DB94EBF8CA@njms.rutgers.edu>

Hello,

I was wondering if there are any updates regarding the new release of WGCNA for R 3.60.
I am trying to update my R studio to follow up with some analyses.

Thanks a lot for your help,

Ricardo

From murdoch@dunc@n @end|ng |rom gm@||@com  Wed May 22 00:54:00 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Tue, 21 May 2019 18:54:00 -0400
Subject: [R] cygwin clipboard
In-Reply-To: <CALRb-ofC1zxvYYH5C8mMxDWXesQSsD9kwg=BNeO6ANmN86UcVw@mail.gmail.com>
References: <CALRb-ofC1zxvYYH5C8mMxDWXesQSsD9kwg=BNeO6ANmN86UcVw@mail.gmail.com>
Message-ID: <390a973f-0ad5-c8c1-2aaf-1b1949937336@gmail.com>

On 21/05/2019 10:55 a.m., Ed Siefker wrote:
> I'd like to be able to access the windows clipboard from R under Cygwin.
> But...
> 
>> read.table(file="clipboard")
> Error in file(file, "rt") : cannot open the connection
> In addition: Warning message:
> In file(file, "rt") : unable to contact X11 display
>>
> 
> Is this supported in any way?  Thanks

Cygwin is not supported in any way by the R project, as far as I know. 
There's a native Windows build that is supported, and I'd strongly 
recommend you use it instead.

Duncan Murdoch


From o|@guer@|yndonm@rk429 @end|ng |rom gm@||@com  Wed May 22 04:44:23 2019
From: o|@guer@|yndonm@rk429 @end|ng |rom gm@||@com (Lyndz)
Date: Wed, 22 May 2019 11:44:23 +0900
Subject: [R] Significance of difference in means using Monte Carlo
 Simulation in R
Message-ID: <CA+V+8dUm8igUYhpa6_okz6WOAaQPOUg6LWNQdhSWAXN=VMhY+g@mail.gmail.com>

I want to implement a statistical test by the following paper in R:
https://journals.ametsoc.org/doi/full/10.1175/JCLI4217.1

**Details**

The above paper calculates the significance of the difference in means
between two periods :1961-1983 and 1984-2000 of tropical cyclone passage
frequency (not-normally distributed) using Monte Carlo simulation. The
following steps are provided:

>1). First, 9999 randomly sorted 40-yr time series of the typhoon passage
frequency are prepared.

>2). Averages of the former 23-yr values (1961-1983) minus those of the
latter 17-yr values are calculated.

>3). From the rank of the original difference value among 10000 samples,
the significance level is estimated.

Suppose I have the following 40-yr time series.

    > dat<-floor(runif(40, min=0, max=20))
    > dput(dat)
    c(15, 14, 1, 16, 0, 18, 5, 8, 19, 7, 11, 15, 2, 17, 12, 16, 1,
    9, 9, 19, 12, 17, 15, 10, 1, 11, 10, 12, 17, 10, 4, 2, 9, 10,
    9, 5, 13, 6, 0, 17)

**PROBLEMS**

1. I am new in R and I am not sure how to implement this test.
Any suggestion on how to do this in R?
I'll appreciate any help.

*Lyndz*

	[[alternative HTML version deleted]]


From pd@|gd @end|ng |rom gm@||@com  Wed May 22 14:31:34 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Wed, 22 May 2019 14:31:34 +0200
Subject: [R] cygwin clipboard
In-Reply-To: <390a973f-0ad5-c8c1-2aaf-1b1949937336@gmail.com>
References: <CALRb-ofC1zxvYYH5C8mMxDWXesQSsD9kwg=BNeO6ANmN86UcVw@mail.gmail.com>
 <390a973f-0ad5-c8c1-2aaf-1b1949937336@gmail.com>
Message-ID: <2ACF30A6-B4BB-4650-803F-1C82C60B7DD3@gmail.com>

What Duncan says, but in case you are feeling adventurous or have strong reasons to be in the Cygwin parallel universe, notice that the error indicates that R is loooking for an X11 display.

So, at a minimum, you need to have 

(a) an X11 display server running
(b) R setup so that it knows how to find X11 (usually via the DISPLAY env. variable)

Even so, you still risk finding that the X11 clipboard does not integrate with the Windows clipboard, so al bets are off.

- pd

> On 22 May 2019, at 00:54 , Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> 
> On 21/05/2019 10:55 a.m., Ed Siefker wrote:
>> I'd like to be able to access the windows clipboard from R under Cygwin.
>> But...
>>> read.table(file="clipboard")
>> Error in file(file, "rt") : cannot open the connection
>> In addition: Warning message:
>> In file(file, "rt") : unable to contact X11 display
>>> 
>> Is this supported in any way?  Thanks
> 
> Cygwin is not supported in any way by the R project, as far as I know. There's a native Windows build that is supported, and I'd strongly recommend you use it instead.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From B|||@Po||ng @end|ng |rom ze||@@com  Wed May 22 14:35:29 2019
From: B|||@Po||ng @end|ng |rom ze||@@com (Bill Poling)
Date: Wed, 22 May 2019 12:35:29 +0000
Subject: [R] Help with R coding
Message-ID: <BN7PR02MB50735C06A6940036A55A358DEA000@BN7PR02MB5073.namprd02.prod.outlook.com>

Good morning.

#R version 3.6.0 Patched (2019-05-19 r76539)
#Platform: x86_64-w64-mingw32/x64 (64-bit)
#Running under: Windows >= 8 x64 (build 9200)

I need a calculated field  For the Rate of Avg_AllowByLimit where the Allowed_AmtFlag = TRUE BY Each Code

I have almost got this.

#So far I have this
tmp1 <- tmp %>%
group_by(HCPCSCode) %>%
summarise(Avg_AllowByLimit = mean(Avg_AllowByLimit[which(Avg_AllowByLimit!=0)]))

# But I need Something like that + This

WHERE AllowByLimitFlag == TRUE

I cannot seem to get it in there correctly

Thank you for any help

WHP

#Here is some data
HCPCSCode Avg_AllowByLimit AllowByLimitFlag
1      J1745             4.50            FALSE
2      J9299            18.70            FALSE
3      J9306            14.33            FALSE
4      J9355             7.13            FALSE
5      J0897             8.61            FALSE
6      J9034             3.32            FALSE
7      J9034             3.32            FALSE
8      J9045            15.60            FALSE
9      J9035             2.77             TRUE
10     J1190             3.62            FALSE
11     J2250           879.10            FALSE
12     J9033             2.92            FALSE
13     J1745             4.50             TRUE
14     J2785            12.11            FALSE
15     J9045            15.60            FALSE
16     J2350             7.81            FALSE
17     J2469            10.65             TRUE
18     J2796             6.27            FALSE
19     J2796             6.27            FALSE
20     J9355             7.13            FALSE
21     J9045            15.60            FALSE
22     J2505             2.73            FALSE
23     J1786             2.81            FALSE
24     J3262             3.26            FALSE
25     J0696           168.87            FALSE
26     J0178             1.52             TRUE
27     J9271             5.55            FALSE
28     J3380            80.99            FALSE
29     J9355             7.13             TRUE
30     J2469            10.65            FALSE
31     J9045            15.60            FALSE
32     J1459             3.64            FALSE
33     J9305             8.74            FALSE
34     J9034             3.32            FALSE
35     J9034             3.32            FALSE

Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed May 22 15:45:39 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 22 May 2019 14:45:39 +0100
Subject: [R] Help with R coding
In-Reply-To: <BN7PR02MB50735C06A6940036A55A358DEA000@BN7PR02MB5073.namprd02.prod.outlook.com>
References: <BN7PR02MB50735C06A6940036A55A358DEA000@BN7PR02MB5073.namprd02.prod.outlook.com>
Message-ID: <a2a5027f-8758-5fb9-f4a0-087a5de0f229@sapo.pt>

Hello,

Maybe filter the AllowByLimitFlag values first (not tested)?


tmp1 <- tmp %>%
   group_by(HCPCSCode) %>%
   filter(AllowByLimitFlag) %>%
   summarise(Avg_AllowByLimit = 
mean(Avg_AllowByLimit[which(Avg_AllowByLimit!=0)]))


Hope this helps,

Rui Barradas


?s 13:35 de 22/05/19, Bill Poling escreveu:
> tmp1 <- tmp %>%
> group_by(HCPCSCode) %>%
> summarise(Avg_AllowByLimit = mean(Avg_AllowByLimit[which(Avg_AllowByLimit!=0)]))
> 
> # But I need Something like that + This
> 
> WHERE AllowByLimitFlag == TRUE


From B|||@Po||ng @end|ng |rom ze||@@com  Wed May 22 15:53:50 2019
From: B|||@Po||ng @end|ng |rom ze||@@com (Bill Poling)
Date: Wed, 22 May 2019 13:53:50 +0000
Subject: [R] Help with R coding
In-Reply-To: <a2a5027f-8758-5fb9-f4a0-087a5de0f229@sapo.pt>
References: <BN7PR02MB50735C06A6940036A55A358DEA000@BN7PR02MB5073.namprd02.prod.outlook.com>
 <a2a5027f-8758-5fb9-f4a0-087a5de0f229@sapo.pt>
Message-ID: <BN7PR02MB5073727C06939E929217EE29EA000@BN7PR02MB5073.namprd02.prod.outlook.com>

Thank you Rui, but that is getting me the overall mean not the mean of just the observations that are flagged as TRUE Grouped By the HCPCSCode
 (AllowByLimitFlag == TRUE).

I also tried adding it to the filter you suggested but that does not seem to work either?

tmp1 <- tmp %>%
  group_by(HCPCSCode) %>%
  filter(AllowByLimitFlag==TRUE) %>%
  summarise(Avg_AllowByLimit =
              mean(Avg_AllowByLimit[which(Avg_AllowByLimit!=0)]))

However, I really appreciate your help Sir!

WHP


William H. Poling, Ph.D., MPH | Manager, Data Science
Data Intelligence & Analytics
Zelis Healthcare


-----Original Message-----
From: Rui Barradas <ruipbarradas at sapo.pt>
Sent: Wednesday, May 22, 2019 9:46 AM
To: Bill Poling <Bill.Poling at zelis.com>; r-help (r-help at r-project.org) <r-help at r-project.org>
Subject: Re: [R] Help with R coding

Hello,

Maybe filter the AllowByLimitFlag values first (not tested)?


tmp1 <- tmp %>%
   group_by(HCPCSCode) %>%
   filter(AllowByLimitFlag) %>%
   summarise(Avg_AllowByLimit =
mean(Avg_AllowByLimit[which(Avg_AllowByLimit!=0)]))


Hope this helps,

Rui Barradas


?s 13:35 de 22/05/19, Bill Poling escreveu:
> tmp1 <- tmp %>%
> group_by(HCPCSCode) %>%
> summarise(Avg_AllowByLimit = mean(Avg_AllowByLimit[which(Avg_AllowByLimit!=0)]))
>
> # But I need Something like that + This
>
> WHERE AllowByLimitFlag == TRUE


Confidentiality Notice This message is sent from Zelis. This transmission may contain information which is privileged and confidential and is intended for the personal and confidential use of the named recipient only. Such information may be protected by applicable State and Federal laws from this disclosure or unauthorized use. If the reader of this message is not the intended recipient, or the employee or agent responsible for delivering the message to the intended recipient, you are hereby notified that any disclosure, review, discussion, copying, or taking any action in reliance on the contents of this transmission is strictly prohibited. If you have received this transmission in error, please contact the sender immediately. Zelis, 2018.

From wjm1 @end|ng |rom c@@@co|umb|@@edu  Wed May 22 15:57:51 2019
From: wjm1 @end|ng |rom c@@@co|umb|@@edu (William Michels)
Date: Wed, 22 May 2019 06:57:51 -0700
Subject: [R] Help with R coding
In-Reply-To: <BN7PR02MB50735C06A6940036A55A358DEA000@BN7PR02MB5073.namprd02.prod.outlook.com>
References: <BN7PR02MB50735C06A6940036A55A358DEA000@BN7PR02MB5073.namprd02.prod.outlook.com>
Message-ID: <CAA99HCybGtO7L2cCfQ3eihw+-d=YHAHYhDeHZyraprMQzfswtA@mail.gmail.com>

Morning Bill, I take it this is dplyr? You might try:

tmp1 <- HCPC %>%
group_by(HCPCSCode) %>%
summarise(Avg_AllowByLimit =
mean(Avg_AllowByLimit[which(Avg_AllowByLimit!=0 & AllowByLimitFlag ==
TRUE)]))

The code above gives "NaN" for cases where AllowByLimitFlag == FALSE.
Maybe this is the answer you desire, otherwise you can filter out
"NaN" rows.

Cheers,

Bill.

W. Michels, Ph.D.


On Wed, May 22, 2019 at 5:41 AM Bill Poling <Bill.Poling at zelis.com> wrote:
>
> Good morning.
>
> #R version 3.6.0 Patched (2019-05-19 r76539)
> #Platform: x86_64-w64-mingw32/x64 (64-bit)
> #Running under: Windows >= 8 x64 (build 9200)
>
> I need a calculated field  For the Rate of Avg_AllowByLimit where the Allowed_AmtFlag = TRUE BY Each Code
>
> I have almost got this.
>
> #So far I have this
> tmp1 <- tmp %>%
> group_by(HCPCSCode) %>%
> summarise(Avg_AllowByLimit = mean(Avg_AllowByLimit[which(Avg_AllowByLimit!=0)]))
>
> # But I need Something like that + This
>
> WHERE AllowByLimitFlag == TRUE
>
> I cannot seem to get it in there correctly
>
> Thank you for any help
>
> WHP
>
> #Here is some data
> HCPCSCode Avg_AllowByLimit AllowByLimitFlag
> 1      J1745             4.50            FALSE
> 2      J9299            18.70            FALSE
> 3      J9306            14.33            FALSE
> 4      J9355             7.13            FALSE
> 5      J0897             8.61            FALSE
> 6      J9034             3.32            FALSE
> 7      J9034             3.32            FALSE
> 8      J9045            15.60            FALSE
> 9      J9035             2.77             TRUE
> 10     J1190             3.62            FALSE
> 11     J2250           879.10            FALSE
> 12     J9033             2.92            FALSE
> 13     J1745             4.50             TRUE
> 14     J2785            12.11            FALSE
> 15     J9045            15.60            FALSE
> 16     J2350             7.81            FALSE
> 17     J2469            10.65             TRUE
> 18     J2796             6.27            FALSE
> 19     J2796             6.27            FALSE
> 20     J9355             7.13            FALSE
> 21     J9045            15.60            FALSE
> 22     J2505             2.73            FALSE
> 23     J1786             2.81            FALSE
> 24     J3262             3.26            FALSE
> 25     J0696           168.87            FALSE
> 26     J0178             1.52             TRUE
> 27     J9271             5.55            FALSE
> 28     J3380            80.99            FALSE
> 29     J9355             7.13             TRUE
> 30     J2469            10.65            FALSE
> 31     J9045            15.60            FALSE
> 32     J1459             3.64            FALSE
> 33     J9305             8.74            FALSE
> 34     J9034             3.32            FALSE
> 35     J9034             3.32            FALSE
>
> Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed May 22 16:06:28 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 22 May 2019 07:06:28 -0700
Subject: [R] Help with R coding
In-Reply-To: <a2a5027f-8758-5fb9-f4a0-087a5de0f229@sapo.pt>
References: <BN7PR02MB50735C06A6940036A55A358DEA000@BN7PR02MB5073.namprd02.prod.outlook.com>
 <a2a5027f-8758-5fb9-f4a0-087a5de0f229@sapo.pt>
Message-ID: <1256F013-2B05-4767-9CA7-88F8BAB00169@dcn.davis.ca.us>

Generally more efficient to filter before grouping.

Note that summarize clears out whatever isn't mentioned in it, so the subsetting currently being done in the mean call could also be done in the pre-filter step and you can avoid filtering other columns and then discarding them by limiting the operations to only those columns that will be referenced:

tmp1 <- tmp %>%
   select( HCPCSCode, Avg_AllowByLimit, AllowByLimitFlag )
   filter( AllowByLimitFlag & Avg_AllowByLimit != 0 ) %>%
   group_by( HCPCSCode ) %>%
   summarise( Avg_AllowByLimit = mean( Avg_AllowByLimit ) )


On May 22, 2019 6:45:39 AM PDT, Rui Barradas <ruipbarradas at sapo.pt> wrote:
>Hello,
>
>Maybe filter the AllowByLimitFlag values first (not tested)?
>
>
>tmp1 <- tmp %>%
>   group_by(HCPCSCode) %>%
>   filter(AllowByLimitFlag) %>%
>   summarise(Avg_AllowByLimit = 
>mean(Avg_AllowByLimit[which(Avg_AllowByLimit!=0)]))
>
>
>Hope this helps,
>
>Rui Barradas
>
>
>?s 13:35 de 22/05/19, Bill Poling escreveu:
>> tmp1 <- tmp %>%
>> group_by(HCPCSCode) %>%
>> summarise(Avg_AllowByLimit =
>mean(Avg_AllowByLimit[which(Avg_AllowByLimit!=0)]))
>> 
>> # But I need Something like that + This
>> 
>> WHERE AllowByLimitFlag == TRUE
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From B|||@Po||ng @end|ng |rom ze||@@com  Wed May 22 16:10:04 2019
From: B|||@Po||ng @end|ng |rom ze||@@com (Bill Poling)
Date: Wed, 22 May 2019 14:10:04 +0000
Subject: [R] Help with R coding
In-Reply-To: <1256F013-2B05-4767-9CA7-88F8BAB00169@dcn.davis.ca.us>
References: <BN7PR02MB50735C06A6940036A55A358DEA000@BN7PR02MB5073.namprd02.prod.outlook.com>
 <a2a5027f-8758-5fb9-f4a0-087a5de0f229@sapo.pt>
 <1256F013-2B05-4767-9CA7-88F8BAB00169@dcn.davis.ca.us>
Message-ID: <BN7PR02MB5073D774193ECB2A6A8232F6EA000@BN7PR02MB5073.namprd02.prod.outlook.com>

Thank you Jeff!

WHP

From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
Sent: Wednesday, May 22, 2019 10:06 AM
To: r-help at r-project.org; Rui Barradas <ruipbarradas at sapo.pt>; Bill Poling <Bill.Poling at zelis.com>; r-help (r-help at r-project.org) <r-help at r-project.org>
Subject: Re: [R] Help with R coding

Generally more efficient to filter before grouping.

Note that summarize clears out whatever isn't mentioned in it, so the subsetting currently being done in the mean call could also be done in the pre-filter step and you can avoid filtering other columns and then discarding them by limiting the operations to only those columns that will be referenced:

tmp1 <- tmp %>%
select( HCPCSCode, Avg_AllowByLimit, AllowByLimitFlag )
filter( AllowByLimitFlag & Avg_AllowByLimit != 0 ) %>%
group_by( HCPCSCode ) %>%
summarise( Avg_AllowByLimit = mean( Avg_AllowByLimit ) )


On May 22, 2019 6:45:39 AM PDT, Rui Barradas <mailto:ruipbarradas at sapo.pt> wrote:
>Hello,
>
>Maybe filter the AllowByLimitFlag values first (not tested)?
>
>
>tmp1 <- tmp %>%
> group_by(HCPCSCode) %>%
> filter(AllowByLimitFlag) %>%
> summarise(Avg_AllowByLimit =
>mean(Avg_AllowByLimit[which(Avg_AllowByLimit!=0)]))
>
>
>Hope this helps,
>
>Rui Barradas
>
>
>?s 13:35 de 22/05/19, Bill Poling escreveu:
>> tmp1 <- tmp %>%
>> group_by(HCPCSCode) %>%
>> summarise(Avg_AllowByLimit =
>mean(Avg_AllowByLimit[which(Avg_AllowByLimit!=0)]))
>>
>> # But I need Something like that + This
>>
>> WHERE AllowByLimitFlag == TRUE
>
>______________________________________________
>mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.

Confidentiality Notice This message is sent from Zelis. This transmission may contain information which is privileged and confidential and is intended for the personal and confidential use of the named recipient only. Such information may be protected by applicable State and Federal laws from this disclosure or unauthorized use. If the reader of this message is not the intended recipient, or the employee or agent responsible for delivering the message to the intended recipient, you are hereby notified that any disclosure, review, discussion, copying, or taking any action in reliance on the contents of this transmission is strictly prohibited. If you have received this transmission in error, please contact the sender immediately. Zelis, 2018.

From bgunter@4567 @end|ng |rom gm@||@com  Wed May 22 16:39:02 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 22 May 2019 07:39:02 -0700
Subject: [R] Significance of difference in means using Monte Carlo
 Simulation in R
In-Reply-To: <CA+V+8dUm8igUYhpa6_okz6WOAaQPOUg6LWNQdhSWAXN=VMhY+g@mail.gmail.com>
References: <CA+V+8dUm8igUYhpa6_okz6WOAaQPOUg6LWNQdhSWAXN=VMhY+g@mail.gmail.com>
Message-ID: <CAGxFJbSLbPSARM9Cuvq-zurMLHWpLRArJ8KPN3XJ3BKCKcuRrg@mail.gmail.com>

This list is for Help and usually expects you to first show us your own
efforts. You need to do your own homework and spend some time with one or
more of the many good R tutorials on the Web. Some suggestions canbe found
here, though you will find many more by simple web searches:

https://www.rstudio.com/online-learning/#r-programming

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, May 22, 2019 at 12:56 AM Lyndz <olagueralyndonmark429 at gmail.com>
wrote:

> I want to implement a statistical test by the following paper in R:
> https://journals.ametsoc.org/doi/full/10.1175/JCLI4217.1
>
> **Details**
>
> The above paper calculates the significance of the difference in means
> between two periods :1961-1983 and 1984-2000 of tropical cyclone passage
> frequency (not-normally distributed) using Monte Carlo simulation. The
> following steps are provided:
>
> >1). First, 9999 randomly sorted 40-yr time series of the typhoon passage
> frequency are prepared.
>
> >2). Averages of the former 23-yr values (1961-1983) minus those of the
> latter 17-yr values are calculated.
>
> >3). From the rank of the original difference value among 10000 samples,
> the significance level is estimated.
>
> Suppose I have the following 40-yr time series.
>
>     > dat<-floor(runif(40, min=0, max=20))
>     > dput(dat)
>     c(15, 14, 1, 16, 0, 18, 5, 8, 19, 7, 11, 15, 2, 17, 12, 16, 1,
>     9, 9, 19, 12, 17, 15, 10, 1, 11, 10, 12, 17, 10, 4, 2, 9, 10,
>     9, 5, 13, 6, 0, 17)
>
> **PROBLEMS**
>
> 1. I am new in R and I am not sure how to implement this test.
> Any suggestion on how to do this in R?
> I'll appreciate any help.
>
> *Lyndz*
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @god@v|de@@dp @end|ng |rom gm@||@com  Wed May 22 14:31:08 2019
From: @god@v|de@@dp @end|ng |rom gm@||@com (Agostino Della Porta)
Date: Wed, 22 May 2019 14:31:08 +0200
Subject: [R] Problema estrazione file in formato "large STFDF" in formato
 "ASCII" compatibile con GIS
Message-ID: <CAD11wiqnnA4tQUVoYwJBa_DA3KYHM3=+MBrADaBh_qyDCNtaiw@mail.gmail.com>

Salve, mi sto occupando nel mio tirocinio di analisi di dati
spazio-temporali con il Kriging spazio-temporale. Una volta ottenuta la
mappa di predizione con questa riga di codice:

pred <- krigeST ( PPB ~ 1 ,  data = dataSTIDF , modelList =
simpleSumMetric_Vgm, newdata = grid.ST, fullCovariance = FALSE)
Non riesco a trovare un comando che mi possa far estrarre il file "pred" in
formato large STFDF in formato ASCII da poter poi utilizzare in GIS

<http://www.avg.com/email-signature?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
Mail
priva di virus. www.avg.com
<http://www.avg.com/email-signature?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
<#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>

	[[alternative HTML version deleted]]


From B|||@Po||ng @end|ng |rom ze||@@com  Wed May 22 18:07:27 2019
From: B|||@Po||ng @end|ng |rom ze||@@com (Bill Poling)
Date: Wed, 22 May 2019 16:07:27 +0000
Subject: [R] Help with R coding
In-Reply-To: <CAA99HCybGtO7L2cCfQ3eihw+-d=YHAHYhDeHZyraprMQzfswtA@mail.gmail.com>
References: <BN7PR02MB50735C06A6940036A55A358DEA000@BN7PR02MB5073.namprd02.prod.outlook.com>
 <CAA99HCybGtO7L2cCfQ3eihw+-d=YHAHYhDeHZyraprMQzfswtA@mail.gmail.com>
Message-ID: <BN7PR02MB5073B942E8E73EED3C4C280FEA000@BN7PR02MB5073.namprd02.prod.outlook.com>

Thank you William appreciate your response Sir.

WHP

From: William Michels <wjm1 at caa.columbia.edu>
Sent: Wednesday, May 22, 2019 9:58 AM
To: Bill Poling <Bill.Poling at zelis.com>
Cc: r-help (r-help at r-project.org) <r-help at r-project.org>
Subject: Re: [R] Help with R coding

Morning Bill, I take it this is dplyr? You might try:

tmp1 <- HCPC %>%
group_by(HCPCSCode) %>%
summarise(Avg_AllowByLimit =
mean(Avg_AllowByLimit[which(Avg_AllowByLimit!=0 & AllowByLimitFlag ==
TRUE)]))

The code above gives "NaN" for cases where AllowByLimitFlag == FALSE.
Maybe this is the answer you desire, otherwise you can filter out
"NaN" rows.

Cheers,

Bill.

W. Michels, Ph.D.


On Wed, May 22, 2019 at 5:41 AM Bill Poling <mailto:Bill.Poling at zelis.com> wrote:
>
> Good morning.
>
> #R version 3.6.0 Patched (2019-05-19 r76539)
> #Platform: x86_64-w64-mingw32/x64 (64-bit)
> #Running under: Windows >= 8 x64 (build 9200)
>
> I need a calculated field For the Rate of Avg_AllowByLimit where the Allowed_AmtFlag = TRUE BY Each Code
>
> I have almost got this.
>
> #So far I have this
> tmp1 <- tmp %>%
> group_by(HCPCSCode) %>%
> summarise(Avg_AllowByLimit = mean(Avg_AllowByLimit[which(Avg_AllowByLimit!=0)]))
>
> # But I need Something like that + This
>
> WHERE AllowByLimitFlag == TRUE
>
> I cannot seem to get it in there correctly
>
> Thank you for any help
>
> WHP
>
> #Here is some data
> HCPCSCode Avg_AllowByLimit AllowByLimitFlag
> 1 J1745 4.50 FALSE
> 2 J9299 18.70 FALSE
> 3 J9306 14.33 FALSE
> 4 J9355 7.13 FALSE
> 5 J0897 8.61 FALSE
> 6 J9034 3.32 FALSE
> 7 J9034 3.32 FALSE
> 8 J9045 15.60 FALSE
> 9 J9035 2.77 TRUE
> 10 J1190 3.62 FALSE
> 11 J2250 879.10 FALSE
> 12 J9033 2.92 FALSE
> 13 J1745 4.50 TRUE
> 14 J2785 12.11 FALSE
> 15 J9045 15.60 FALSE
> 16 J2350 7.81 FALSE
> 17 J2469 10.65 TRUE
> 18 J2796 6.27 FALSE
> 19 J2796 6.27 FALSE
> 20 J9355 7.13 FALSE
> 21 J9045 15.60 FALSE
> 22 J2505 2.73 FALSE
> 23 J1786 2.81 FALSE
> 24 J3262 3.26 FALSE
> 25 J0696 168.87 FALSE
> 26 J0178 1.52 TRUE
> 27 J9271 5.55 FALSE
> 28 J3380 80.99 FALSE
> 29 J9355 7.13 TRUE
> 30 J2469 10.65 FALSE
> 31 J9045 15.60 FALSE
> 32 J1459 3.64 FALSE
> 33 J9305 8.74 FALSE
> 34 J9034 3.32 FALSE
> 35 J9034 3.32 FALSE
>
> Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}
>
> ______________________________________________
> mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Confidentiality Notice This message is sent from Zelis. This transmission may contain information which is privileged and confidential and is intended for the personal and confidential use of the named recipient only. Such information may be protected by applicable State and Federal laws from this disclosure or unauthorized use. If the reader of this message is not the intended recipient, or the employee or agent responsible for delivering the message to the intended recipient, you are hereby notified that any disclosure, review, discussion, copying, or taking any action in reliance on the contents of this transmission is strictly prohibited. If you have received this transmission in error, please contact the sender immediately. Zelis, 2018.

From ||@t@ @end|ng |rom dewey@myzen@co@uk  Wed May 22 19:34:44 2019
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Wed, 22 May 2019 18:34:44 +0100
Subject: [R] 
 Problema estrazione file in formato "large STFDF" in formato
 "ASCII" compatibile con GIS
In-Reply-To: <CAD11wiqnnA4tQUVoYwJBa_DA3KYHM3=+MBrADaBh_qyDCNtaiw@mail.gmail.com>
References: <CAD11wiqnnA4tQUVoYwJBa_DA3KYHM3=+MBrADaBh_qyDCNtaiw@mail.gmail.com>
Message-ID: <36e374f8-02b1-9144-712d-c29f72b91e75@dewey.myzen.co.uk>

Dear Agostino

I am afraid this list is an English language one but I suggest that as 
well as trying to translate your message you post on the list which 
specialises in this sort of thing

https://stat.ethz.ch/mailman/listinfo/r-sig-geo

where they may be better able to help you.

Michael

On 22/05/2019 13:31, Agostino Della Porta wrote:
> Salve, mi sto occupando nel mio tirocinio di analisi di dati
> spazio-temporali con il Kriging spazio-temporale. Una volta ottenuta la
> mappa di predizione con questa riga di codice:
> 
> pred <- krigeST ( PPB ~ 1 ,  data = dataSTIDF , modelList =
> simpleSumMetric_Vgm, newdata = grid.ST, fullCovariance = FALSE)
> Non riesco a trovare un comando che mi possa far estrarre il file "pred" in
> formato large STFDF in formato ASCII da poter poi utilizzare in GIS
> 
> <http://www.avg.com/email-signature?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
> Mail
> priva di virus. www.avg.com
> <http://www.avg.com/email-signature?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
> <#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ---
> This email has been checked for viruses by AVG.
> https://www.avg.com
> 
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From reichm@@j m@iii@g oii sbcgiob@i@@et  Wed May 22 23:43:42 2019
From: reichm@@j m@iii@g oii sbcgiob@i@@et (reichm@@j m@iii@g oii sbcgiob@i@@et)
Date: Wed, 22 May 2019 16:43:42 -0500
Subject: [R] Calculating date difference in days
Message-ID: <000801d510e7$6deac590$49c050b0$@sbcglobal.net>

R Help

I have a function to calculate a date difference in days but my results come
back in hours.  I suspect I am using the as.POSIXlt function incorrectly . 

Suggestions?

# Start time of data to be considered 
start_day <- "2016-04-30"

# Make event and sequence IDs into factors 
elapsed_days <- function(end_date, start_date){
  ed <- as.POSIXlt(end_date) 
  sd <- as.POSIXlt(start_date) 
  ed-sd 
}

trans_sequence$eventID <- elapsed_days(trans_sequence$Date, start_day)


> trans_sequence
# A tibble: 39 x 5
# Groups:   Emitter [15]
   Emitter Date        SIZE Geohash
eventID  
     <int> <date>     <int> <chr>
<time>   
 1       1 2016-05-01    12 A;B;C;D;E;F;G;H;I;J;K;L
19 hours
 2       1 2016-05-02     5 A;B;C;D;E
43 hours
 3       1 2016-05-05    11 A;B;C;D;E;F;G;H;I;J;K
115 hours
 4       2 2016-05-01     9 C;D;E;F;G;H;I;J;K
19 hours
 5       2 2016-05-02     3 F;G;H
43 hours
 6       2 2016-05-05     3 L;M;N
115 hours
 7       3 2016-05-01     3 L;M;N
19 hours
 8       3 2016-05-02     3 I;J;K
43 hours
 9       3 2016-05-04    25
A;B;C;D;E;F;G;H;I;J;K;L;M;N;O;P;Q;R;S;T;U;V;W;X;Y  91 hours
10       3 2016-05-05     7 O;P;Q;R;S;T;U
115 hours

Jeff Reichman


	[[alternative HTML version deleted]]


From bor|@@@te|pe @end|ng |rom utoronto@c@  Wed May 22 23:52:33 2019
From: bor|@@@te|pe @end|ng |rom utoronto@c@ (Boris Steipe)
Date: Wed, 22 May 2019 21:52:33 +0000
Subject: [R] Calculating date difference in days
In-Reply-To: <000801d510e7$6deac590$49c050b0$@sbcglobal.net>
References: <000801d510e7$6deac590$49c050b0$@sbcglobal.net>
Message-ID: <74A5F54B-2E25-434E-8357-342EDBB120FD@utoronto.ca>

ed <- as.POSIXlt("2018-03-10")
sd <- as.POSIXlt("2018-02-10")
as.numeric(ed-sd)
[1] 28

ed <- as.POSIXlt("2000-03-10")
sd <- as.POSIXlt("2000-02-10")
as.numeric(ed-sd)
[1] 29

Cheers,
B.


> On 2019-05-22, at 17:43, reichmanj at sbcglobal.net wrote:
> 
> R Help
> 
> I have a function to calculate a date difference in days but my results come
> back in hours.  I suspect I am using the as.POSIXlt function incorrectly . 
> 
> Suggestions?
> 
> # Start time of data to be considered 
> start_day <- "2016-04-30"
> 
> # Make event and sequence IDs into factors 
> elapsed_days <- function(end_date, start_date){
>  ed <- as.POSIXlt(end_date) 
>  sd <- as.POSIXlt(start_date) 
>  ed-sd 
> }
> 
> trans_sequence$eventID <- elapsed_days(trans_sequence$Date, start_day)
> 
> 
>> trans_sequence
> # A tibble: 39 x 5
> # Groups:   Emitter [15]
>   Emitter Date        SIZE Geohash
> eventID  
>     <int> <date>     <int> <chr>
> <time>   
> 1       1 2016-05-01    12 A;B;C;D;E;F;G;H;I;J;K;L
> 19 hours
> 2       1 2016-05-02     5 A;B;C;D;E
> 43 hours
> 3       1 2016-05-05    11 A;B;C;D;E;F;G;H;I;J;K
> 115 hours
> 4       2 2016-05-01     9 C;D;E;F;G;H;I;J;K
> 19 hours
> 5       2 2016-05-02     3 F;G;H
> 43 hours
> 6       2 2016-05-05     3 L;M;N
> 115 hours
> 7       3 2016-05-01     3 L;M;N
> 19 hours
> 8       3 2016-05-02     3 I;J;K
> 43 hours
> 9       3 2016-05-04    25
> A;B;C;D;E;F;G;H;I;J;K;L;M;N;O;P;Q;R;S;T;U;V;W;X;Y  91 hours
> 10       3 2016-05-05     7 O;P;Q;R;S;T;U
> 115 hours
> 
> Jeff Reichman
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wdun|@p @end|ng |rom t|bco@com  Thu May 23 00:35:00 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Wed, 22 May 2019 15:35:00 -0700
Subject: [R] Calculating date difference in days
In-Reply-To: <000801d510e7$6deac590$49c050b0$@sbcglobal.net>
References: <000801d510e7$6deac590$49c050b0$@sbcglobal.net>
Message-ID: <CAF8bMcbsoXqAjGFHOZaYyN=+bZu5jTm2uhgObR97UC5ua9oRvw@mail.gmail.com>

You can use units<- to change the time units of the difference.  E.g.,

> d <- as.POSIXlt("2018-03-10") -  as.POSIXlt("2018-03-09 02:00:00")
> d
Time difference of 22 hours
> units(d) <- "days"
> d
Time difference of 0.9166667 days
>
> units(d) <- "mins"
> d
Time difference of 1320 mins
> units(d) <- "secs"
> d
Time difference of 79200 secs
> units(d) <- "weeks"
> d
Time difference of 0.1309524 weeks

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Wed, May 22, 2019 at 2:44 PM <reichmanj at sbcglobal.net> wrote:

> R Help
>
> I have a function to calculate a date difference in days but my results
> come
> back in hours.  I suspect I am using the as.POSIXlt function incorrectly .
>
> Suggestions?
>
> # Start time of data to be considered
> start_day <- "2016-04-30"
>
> # Make event and sequence IDs into factors
> elapsed_days <- function(end_date, start_date){
>   ed <- as.POSIXlt(end_date)
>   sd <- as.POSIXlt(start_date)
>   ed-sd
> }
>
> trans_sequence$eventID <- elapsed_days(trans_sequence$Date, start_day)
>
>
> > trans_sequence
> # A tibble: 39 x 5
> # Groups:   Emitter [15]
>    Emitter Date        SIZE Geohash
> eventID
>      <int> <date>     <int> <chr>
> <time>
>  1       1 2016-05-01    12 A;B;C;D;E;F;G;H;I;J;K;L
> 19 hours
>  2       1 2016-05-02     5 A;B;C;D;E
> 43 hours
>  3       1 2016-05-05    11 A;B;C;D;E;F;G;H;I;J;K
> 115 hours
>  4       2 2016-05-01     9 C;D;E;F;G;H;I;J;K
> 19 hours
>  5       2 2016-05-02     3 F;G;H
> 43 hours
>  6       2 2016-05-05     3 L;M;N
> 115 hours
>  7       3 2016-05-01     3 L;M;N
> 19 hours
>  8       3 2016-05-02     3 I;J;K
> 43 hours
>  9       3 2016-05-04    25
> A;B;C;D;E;F;G;H;I;J;K;L;M;N;O;P;Q;R;S;T;U;V;W;X;Y  91 hours
> 10       3 2016-05-05     7 O;P;Q;R;S;T;U
> 115 hours
>
> Jeff Reichman
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu May 23 00:59:39 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 22 May 2019 15:59:39 -0700
Subject: [R] Calculating date difference in days
In-Reply-To: <000801d510e7$6deac590$49c050b0$@sbcglobal.net>
References: <000801d510e7$6deac590$49c050b0$@sbcglobal.net>
Message-ID: <CC93F8F8-9C95-476C-B98D-4193D86062AF@dcn.davis.ca.us>

At the point where you say "date difference in days" IMO you have departed from what `difftime` is for and are in the realm of a numeric measure. I ignore the units inside `difftime` at all times and convert to numeric with a units argument if I want to be that specific about how the measure is represented.

You may or may not recall the difference between angle ABC and the measure of angle ABC (with a bar over it) from geometry... but the idea is the same... distinguish the thing (time interval) from the numbers used to quantify it (numeric).

elapsed_days <- function(end_date, start_date){
  ed <- as.POSIXlt(end_date) 
  sd <- as.POSIXlt(start_date) 
  as.numeric( ed-sd, units="days" )
}


On May 22, 2019 2:43:42 PM PDT, reichmanj at sbcglobal.net wrote:
>R Help
>
>I have a function to calculate a date difference in days but my results
>come
>back in hours.  I suspect I am using the as.POSIXlt function
>incorrectly . 
>
>Suggestions?
>
># Start time of data to be considered 
>start_day <- "2016-04-30"
>
># Make event and sequence IDs into factors 
>elapsed_days <- function(end_date, start_date){
>  ed <- as.POSIXlt(end_date) 
>  sd <- as.POSIXlt(start_date) 
>  ed-sd 
>}
>
>trans_sequence$eventID <- elapsed_days(trans_sequence$Date, start_day)
>
>
>> trans_sequence
># A tibble: 39 x 5
># Groups:   Emitter [15]
>   Emitter Date        SIZE Geohash
>eventID  
>     <int> <date>     <int> <chr>
><time>   
> 1       1 2016-05-01    12 A;B;C;D;E;F;G;H;I;J;K;L
>19 hours
> 2       1 2016-05-02     5 A;B;C;D;E
>43 hours
> 3       1 2016-05-05    11 A;B;C;D;E;F;G;H;I;J;K
>115 hours
> 4       2 2016-05-01     9 C;D;E;F;G;H;I;J;K
>19 hours
> 5       2 2016-05-02     3 F;G;H
>43 hours
> 6       2 2016-05-05     3 L;M;N
>115 hours
> 7       3 2016-05-01     3 L;M;N
>19 hours
> 8       3 2016-05-02     3 I;J;K
>43 hours
> 9       3 2016-05-04    25
>A;B;C;D;E;F;G;H;I;J;K;L;M;N;O;P;Q;R;S;T;U;V;W;X;Y  91 hours
>10       3 2016-05-05     7 O;P;Q;R;S;T;U
>115 hours
>
>Jeff Reichman
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @purd|e@@ @end|ng |rom gm@||@com  Thu May 23 02:57:27 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Thu, 23 May 2019 12:57:27 +1200
Subject: [R] cygwin clipboard
In-Reply-To: <CALRb-ofC1zxvYYH5C8mMxDWXesQSsD9kwg=BNeO6ANmN86UcVw@mail.gmail.com>
References: <CALRb-ofC1zxvYYH5C8mMxDWXesQSsD9kwg=BNeO6ANmN86UcVw@mail.gmail.com>
Message-ID: <CAB8pepwjPUSU_BwQRC6OigMH5x3XhfD1kyeiSe9tuXH700yehw@mail.gmail.com>

> I'd like to be able to access the windows clipboard from R under Cygwin.
> Is this supported in any way?  Thanks

Hi Ed

You can access the Windows clipboard under Cygwin.
I ran R within Cygwin.
I was able to use read.table (file="clipboard") for both plain text and
Excel tables.

> #table copied from excel
> tbl = read.table (file="clipboard", TRUE)
> tbl
  A B  C  D
1 1 5  9 13
2 2 6 10 14
3 3 7 11 15
4 4 8 12 16

Note that when copying plain text you may need to include a final newline
character.

The X11() message is suspicious.
I'm wondering what type of object you're trying to copy...?


Abs

	[[alternative HTML version deleted]]


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Thu May 23 09:19:34 2019
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Thu, 23 May 2019 09:19:34 +0200
Subject: [R] r kernlab find best cost parameter automatically
Message-ID: <CAMk+s2RifHVnp2VtpB-C_DS5F8_Zk5O77ObKX6o0TE5Qk-4vEw@mail.gmail.com>

Dear all,
I am using kernlab to implement an SVM analysis. The model I am
building has the syntax:

`ksvm(<y> ~ <x>, data = <dataframe>, type = "C-svc", kernel =
"rbfdot", kpar = "automatic", C = <k>, prob.model = TRUE)`

Here, I can use different values of `k` to give different costs to the
model. Each time I give a different k, the results obviously change.
Would be possible to automate the selection of the best <k> value?
Thank you

-- 
Best regards,
Luigi


From murdoch@dunc@n @end|ng |rom gm@||@com  Thu May 23 11:08:27 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Thu, 23 May 2019 05:08:27 -0400
Subject: [R] cygwin clipboard
In-Reply-To: <CAB8pepwjPUSU_BwQRC6OigMH5x3XhfD1kyeiSe9tuXH700yehw@mail.gmail.com>
References: <CALRb-ofC1zxvYYH5C8mMxDWXesQSsD9kwg=BNeO6ANmN86UcVw@mail.gmail.com>
 <CAB8pepwjPUSU_BwQRC6OigMH5x3XhfD1kyeiSe9tuXH700yehw@mail.gmail.com>
Message-ID: <4ad0069d-0e22-d85b-0cbd-0f0b988624af@gmail.com>

On 22/05/2019 8:57 p.m., Abby Spurdle wrote:
>  > I'd like to be able to access the windows clipboard from R under Cygwin.
>  > Is this supported in any way?? Thanks
> 
> Hi Ed
> 
> You can access the Windows clipboard under Cygwin.
> I ran R within Cygwin.
> I was able to use read.table (file="clipboard") for both plain text and 
> Excel tables.
> 
>  > #table copied from excel
>  > tbl = read.table (file="clipboard", TRUE)
>  > tbl
>  ? A B ?C ?D
> 1 1 5 ?9 13
> 2 2 6 10 14
> 3 3 7 11 15
> 4 4 8 12 16
> 
> Note that when copying plain text you may need to include a final 
> newline character.
> 
> The X11() message is suspicious.
> I'm wondering what type of object you're trying to copy...?

I think Ed was running the Cygwin build of R.  Cygwin can run the 
Windows build of R, and it sounds as though that's what you did (and 
what Ed should do.)

Duncan Murdoch


From t@n@@@ @end|ng |rom gm@||@com  Thu May 23 21:38:14 2019
From: t@n@@@ @end|ng |rom gm@||@com (Bogdan Tanasa)
Date: Thu, 23 May 2019 12:38:14 -0700
Subject: [R] to run an older version of R on my machine
Message-ID: <CA+JEM03Od8i7gTLAv99U4SiQbKKuJMxzPdvW-UaoBA3JX6iUdQ@mail.gmail.com>

Dear all,

if you could help me please with a solution to a simple question :

i believe that my ubuntu machine automatically installed R 3.6.0 : when i
type : > R. it says :

R version 3.6.0 (2019-04-26) -- "Planting of a Tree"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

However, I need to use a previous version of R, namely R 3.5, that was
installed and did run on my Ubuntu machine, and I can see lots of folders
in the directory (a long list follows below) :

/home/bogdan/R/x86_64-pc-linux-gnu-library/3.5

Please would you advise how can I revert to R 3.5 instead of using R 3.6 .
Thanks a lot,

bogdan

ps : the list of folders in ~/R/x86_64-pc-linux-gnu-library/3.5

abind/
acepack/
ALL/
alphahull/
amap/
annotate/
AnnotationDbi/
AnnotationFilter/
AnnotationForge/
apcluster/
ape/
aroma.light/
askpass/
assertthat/
backports/
base64enc/
bbmle/
beachmat/
beeswarm/
BH/
bibtex/
bindr/
bindrcpp/
Biobase/
BiocFileCache/
BiocGenerics/
biocGraph/
BiocInstaller/
BiocManager/
BiocNeighbors/
BiocParallel/
BiocStyle/
BiocVersion/
biocViews/
biomaRt/
Biostrings/
biovizBase/
bit/
bit64/
bitops/
bladderbatch/
blob/
bookdown/
brew/
broom/
BSgenome/
Cairo/
callr/
car/
carData/
Category/
caTools/
CCA/
CCP/
cellranger/
cellrangerRkit/
checkmate/
circlize/
cli/
clipr/
clisymbols/
coda/
colorspace/
combinat/
ComplexHeatmap/
contfrac/
corpcor/
corrplot/
cowplot/
crayon/
crosstalk/
cubature/
curl/
cvTools/
data.table/
DBI/
dbplyr/
DDRTree/
DelayedArray/
DelayedMatrixStats/
deldir/
densityClust/
DEoptimR/
desc/
DESeq/
DESeq2/
deSolve/
destiny/
devtools/
dichromat/
digest/
diptest/
distillery/
doBy/
docopt/
doParallel/
doRNG/
doSNOW/
dotCall64/
dplyr/
DropletUtils/
dtw/
dynamicTreeCut/
e1071/
EDASeq/
edgeR/
ellipse/
ellipsis/
elliptic/
EnrichmentBrowser/
enrichR/
EnsDb.Hsapiens.v86/
EnsDb.Mmusculus.v79/
ensembldb/
evaluate/
extRemes/
fansi/
fastcluster/
fastICA/
fda/
fields/
fitdistrplus/
fit.models/
flexmix/
FNN/
forcats/
foreach/
formatR/
Formula/
fpc/
fs/
futile.logger/
futile.options/
gage/
gbRd/
gdata/
genefilter/
geneplotter/
generics/
GenomeInfoDb/
GenomeInfoDbData/
GenomicAlignments/
GenomicFeatures/
GenomicRanges/
GEOquery/
GetoptLong/
GGally/
ggbeeswarm/
ggbio/
ggdendro/
ggfortify/
ggplot2/
ggrepel/
ggridges/
ggthemes/
gh/
git2r/
githubinstall/
Glimma/
GlobalOptions/
glue/
gmodels/
GO.db/
goftest/
googleVis/
GOplot/
GOstats/
gplots/
graph/
graphite/
gridExtra/
GSA/
GSEABase/
gtable/
gtools/
hash/
haven/
HDF5Array/
hdf5r/
hexbin/
highr/
Hmisc/
hms/
HSMMSingleCell/
htmlTable/
htmltools/
htmlwidgets/
httpuv/
httr/
hwriter/
hypergeo/
ica/
igraph/
impute/
ini/
inline/
IRanges/
irlba/
iterators/
jsonlite/
kBET/
KEGGgraph/
KEGGREST/
kernlab/
knitr/
labeling/
laeken/
lambda.r/
lars/
later/
latticeExtra/
lazyeval/
limma/
Linnorm/
lle/
lme4/
Lmoments/
lmtest/
locfit/
loo/
lsei/
lubridate/
M3Drop/
magrittr/
maps/
maptools/
markdown/
MAST/
Matrix/
MatrixModels/
matrixStats/
mclust/
MCMCglmm/
memoise/
metap/
mime/
minqa/
mixtools/
mnormt/
mockery/
modelr/
modeltools/
moments/
monocle/
munsell/
Mus.musculus/
mvoutlier/
mvtnorm/
NADA/
nloptr/
npsurv/
numDeriv/
openssl/
openxlsx/
OrganismDbi/
org.Hs.eg.db/
org.Mm.eg.db/
orthopolynom/
ouija/
packrat/
pathview/
pbapply/
pbkrtest/
pcaMethods/
pcaPP/
pcaReduce/
penalized/
permute/
PFAM.db/
pheatmap/
pillar/
pkgbuild/
pkgconfig/
pkgload/
pkgmaker/
PKI/
plogr/
plotly/
pls/
plyr/
plyranges/
png/
polyclip/
polynom/
prabclus/
praise/
preprocessCore/
prettyunits/
processx/
progress/
promises/
ProtGenerics/
proxy/
pryr/
ps/
purrr/
qlcMatrix/
quantreg/
R6/
randomForest/
ranger/
RANN/
rappdirs/
RBGL/
rcmdcheck/
RColorBrewer/
Rcpp/
RcppAnnoy/
RcppArmadillo/
RcppEigen/
RcppProgress/
RCurl/
Rdpack/
readr/
readxl/
refGenome/
registry/
reldist/
rematch/
remotes/
ReportingTools/
reprex/
reshape/
reshape2/
reticulate/
Rgraphviz/
rhdf5/
Rhdf5lib/
rio/
rJava/
rjson/
RJSONIO/
rlang/
RMariaDB/
rmarkdown/
R.methodsS3/
Rmisc/
RMTstat/
rngtools/
robCompositions/
robust/
robustbase/
ROCR/
R.oo/
Rook/
rprojroot/
rrcov/
Rsamtools/
rsconnect/
RSQLite/
rstan/
rstudioapi/
rsvd/
rtracklayer/
Rtsne/
RUnit/
R.utils/
RUVSeq/
rvest/
S4Vectors/
safe/
SC3/
scales/
scater/
scatterplot3d/
scde/
scfind/
scImpute/
scmap/
SCnorm/
scran/
scRNAseq/
scRNA.seq.funcs/
SDMTools/
segmented/
selectr/
sessioninfo/
Seurat/
sgeostat/
shape/
shiny/
ShortRead/
SingleCellExperiment/
slam/
SLICER/
smoother/
snow/
snowfall/
sourcetools/
sp/
spam/
SparseM/
sparsesvd/
spatstat/
spatstat.data/
spatstat.utils/
SPIA/
splancs/
sROC/
StanHeaders/
statmod/
stringi/
stringr/
SummarizedExperiment/
sva/
sys/
tensor/
tensorA/
testthat/
tibble/
tidyr/
tidyselect/
tidyverse/
tinytex/
topGO/
trimcluster/
tripack/
truncnorm/
TSCAN/
tsne/
TTR/
TxDb.Mmusculus.UCSC.mm10.ensGene/
TxDb.Mmusculus.UCSC.mm10.knownGene/
UpSetR/
usethis/
utf8/
VariantAnnotation/
vcd/
vegan/
VennDiagram/
Vennerable/
venneuler/
VGAM/
VIM/
vipor/
viridis/
viridisLite/
WGCNA/
whisker/
withr/
WriteXLS/
xfun/
XML/
xml2/
xopen/
xtable/
xts/
XVector/
yaml/
zCompositions/
zip/
zlibbioc/
zoo/

	[[alternative HTML version deleted]]


From reichm@@j m@iii@g oii sbcgiob@i@@et  Fri May 24 00:00:38 2019
From: reichm@@j m@iii@g oii sbcgiob@i@@et (reichm@@j m@iii@g oii sbcgiob@i@@et)
Date: Thu, 23 May 2019 17:00:38 -0500
Subject: [R] cspade {arulesSequences} error
Message-ID: <000001d511b2$f56c1610$e0444230$@sbcglobal.net>

R-Help

 

When I run the cspade function from the arulesSequences package...

 

s1 <- cspade(trans_matrix, parameter = list(support = 0.3), control =
list(verbose = TRUE))

#s1 <- cspade(trans_matrix, parameter = list(support = 0.3), control =
list(verbose = TRUE), tmpdir = "C:\\Temp")

 

I receive the following error .

error in file (con, "r"): cannot open the connection. cannot open file
'D:\Temp\cspade13403a927eaa.out': no such file or directory

 

now the program works just fine at home so the issue is with my  work
computer.  I suspect its some sort of read/write issue but the functions
seems to write the files out just fine then deletes them and errors out.

 

Any suggestions

 

Jeff Reichamn

 


	[[alternative HTML version deleted]]


From jun@@hen@ut @end|ng |rom gm@||@com  Fri May 24 06:40:55 2019
From: jun@@hen@ut @end|ng |rom gm@||@com (Jun Shen)
Date: Fri, 24 May 2019 00:40:55 -0400
Subject: [R] Fwd: How to retain the NA as a value for splitting a dataframe
In-Reply-To: <CAMCXXmq8Kd+aiHbhvRLeCXkDkajfMjcYHx_oKqoLgZKjH3gV1Q@mail.gmail.com>
References: <CAMCXXmq8Kd+aiHbhvRLeCXkDkajfMjcYHx_oKqoLgZKjH3gV1Q@mail.gmail.com>
Message-ID: <CAMCXXmo6e8SmnKnWktp71cHz+pHY2e4beAPvpjUz2ikoc1RtGQ@mail.gmail.com>

Dear list,

Say I have a data frame with NA in the variable which I want to use as a
sorting variable for splitting the data frame.

df <- data.frame(A=1:10, B=c(rep(99,5), rep(100,5)), C=c(rep(NA,3),
rep(1,3), rep(2,4)))

split(df, f=df[c('C')], drop=FALSE), I got the output as follows. I was
hoping to retain the part of the df where C=NA. drop=FALSE doesn't seem to
take effect here. Appreciate any comments. Thanks.

$`1`
  A   B C
4 4  99 1
5 5  99 1
6 6 100 1

$`2`
    A   B C
7   7 100 2
8   8 100 2
9   9 100 2
10 10 100 2


Jun

	[[alternative HTML version deleted]]


From gerr|t@e|chner @end|ng |rom m@th@un|-g|e@@en@de  Fri May 24 08:13:02 2019
From: gerr|t@e|chner @end|ng |rom m@th@un|-g|e@@en@de (Gerrit Eichner)
Date: Fri, 24 May 2019 08:13:02 +0200
Subject: [R] 
 Fwd: How to retain the NA as a value for splitting a dataframe
In-Reply-To: <CAMCXXmo6e8SmnKnWktp71cHz+pHY2e4beAPvpjUz2ikoc1RtGQ@mail.gmail.com>
References: <CAMCXXmq8Kd+aiHbhvRLeCXkDkajfMjcYHx_oKqoLgZKjH3gV1Q@mail.gmail.com>
 <CAMCXXmo6e8SmnKnWktp71cHz+pHY2e4beAPvpjUz2ikoc1RtGQ@mail.gmail.com>
Message-ID: <66ff9b4e-009b-6aed-829f-ee36596da172@math.uni-giessen.de>

Hello, Jun,

try

split(df, f = factor(df$C, exclude = NULL))

For more info see ?factor, of course.

  Regards  --  Gerrit

---------------------------------------------------------------------
Dr. Gerrit Eichner                   Mathematical Institute, Room 212
gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
http://www.uni-giessen.de/eichner
---------------------------------------------------------------------

Am 24.05.2019 um 06:40 schrieb Jun Shen:
> Dear list,
> 
> Say I have a data frame with NA in the variable which I want to use as a
> sorting variable for splitting the data frame.
> 
> df <- data.frame(A=1:10, B=c(rep(99,5), rep(100,5)), C=c(rep(NA,3),
> rep(1,3), rep(2,4)))
> 
> split(df, f=df[c('C')], drop=FALSE), I got the output as follows. I was
> hoping to retain the part of the df where C=NA. drop=FALSE doesn't seem to
> take effect here. Appreciate any comments. Thanks.
> 
> $`1`
>    A   B C
> 4 4  99 1
> 5 5  99 1
> 6 6 100 1
> 
> $`2`
>      A   B C
> 7   7 100 2
> 8   8 100 2
> 9   9 100 2
> 10 10 100 2
> 
> 
> Jun
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @||redo@corte||@n|co|@u @end|ng |rom gm@||@com  Fri May 24 08:39:11 2019
From: @||redo@corte||@n|co|@u @end|ng |rom gm@||@com (Alfredo Cortell)
Date: Fri, 24 May 2019 08:39:11 +0200
Subject: [R] to run an older version of R on my machine
In-Reply-To: <CA+JEM03Od8i7gTLAv99U4SiQbKKuJMxzPdvW-UaoBA3JX6iUdQ@mail.gmail.com>
References: <CA+JEM03Od8i7gTLAv99U4SiQbKKuJMxzPdvW-UaoBA3JX6iUdQ@mail.gmail.com>
Message-ID: <CAPxhGmj-MZ+9KkaiDNph3Zfux9L45cBzX9SSc3d_5U5gFEevjg@mail.gmail.com>

Hi Bogdan,

The way I do this is the following: I have different R versions installed,
and then I downloaded and use Rswitch to change between versions. You just
open it, select the version you want, and R will open in that version
directly. It works with R and Rstudio in a MacOS HighSierra, although I
have read that it doesn't work in every platform. I don't know about
ubuntu. I would advise you to try it anyhow. Good luck with that!

All the best,

Alfredo

El jue., 23 may. 2019 a las 21:38, Bogdan Tanasa (<tanasa at gmail.com>)
escribi?:

> Dear all,
>
> if you could help me please with a solution to a simple question :
>
> i believe that my ubuntu machine automatically installed R 3.6.0 : when i
> type : > R. it says :
>
> R version 3.6.0 (2019-04-26) -- "Planting of a Tree"
> Copyright (C) 2019 The R Foundation for Statistical Computing
> Platform: x86_64-pc-linux-gnu (64-bit)
>
> However, I need to use a previous version of R, namely R 3.5, that was
> installed and did run on my Ubuntu machine, and I can see lots of folders
> in the directory (a long list follows below) :
>
> /home/bogdan/R/x86_64-pc-linux-gnu-library/3.5
>
> Please would you advise how can I revert to R 3.5 instead of using R 3.6 .
> Thanks a lot,
>
> bogdan
>
> ps : the list of folders in ~/R/x86_64-pc-linux-gnu-library/3.5
>
> abind/
> acepack/
> ALL/
> alphahull/
> amap/
> annotate/
> AnnotationDbi/
> AnnotationFilter/
> AnnotationForge/
> apcluster/
> ape/
> aroma.light/
> askpass/
> assertthat/
> backports/
> base64enc/
> bbmle/
> beachmat/
> beeswarm/
> BH/
> bibtex/
> bindr/
> bindrcpp/
> Biobase/
> BiocFileCache/
> BiocGenerics/
> biocGraph/
> BiocInstaller/
> BiocManager/
> BiocNeighbors/
> BiocParallel/
> BiocStyle/
> BiocVersion/
> biocViews/
> biomaRt/
> Biostrings/
> biovizBase/
> bit/
> bit64/
> bitops/
> bladderbatch/
> blob/
> bookdown/
> brew/
> broom/
> BSgenome/
> Cairo/
> callr/
> car/
> carData/
> Category/
> caTools/
> CCA/
> CCP/
> cellranger/
> cellrangerRkit/
> checkmate/
> circlize/
> cli/
> clipr/
> clisymbols/
> coda/
> colorspace/
> combinat/
> ComplexHeatmap/
> contfrac/
> corpcor/
> corrplot/
> cowplot/
> crayon/
> crosstalk/
> cubature/
> curl/
> cvTools/
> data.table/
> DBI/
> dbplyr/
> DDRTree/
> DelayedArray/
> DelayedMatrixStats/
> deldir/
> densityClust/
> DEoptimR/
> desc/
> DESeq/
> DESeq2/
> deSolve/
> destiny/
> devtools/
> dichromat/
> digest/
> diptest/
> distillery/
> doBy/
> docopt/
> doParallel/
> doRNG/
> doSNOW/
> dotCall64/
> dplyr/
> DropletUtils/
> dtw/
> dynamicTreeCut/
> e1071/
> EDASeq/
> edgeR/
> ellipse/
> ellipsis/
> elliptic/
> EnrichmentBrowser/
> enrichR/
> EnsDb.Hsapiens.v86/
> EnsDb.Mmusculus.v79/
> ensembldb/
> evaluate/
> extRemes/
> fansi/
> fastcluster/
> fastICA/
> fda/
> fields/
> fitdistrplus/
> fit.models/
> flexmix/
> FNN/
> forcats/
> foreach/
> formatR/
> Formula/
> fpc/
> fs/
> futile.logger/
> futile.options/
> gage/
> gbRd/
> gdata/
> genefilter/
> geneplotter/
> generics/
> GenomeInfoDb/
> GenomeInfoDbData/
> GenomicAlignments/
> GenomicFeatures/
> GenomicRanges/
> GEOquery/
> GetoptLong/
> GGally/
> ggbeeswarm/
> ggbio/
> ggdendro/
> ggfortify/
> ggplot2/
> ggrepel/
> ggridges/
> ggthemes/
> gh/
> git2r/
> githubinstall/
> Glimma/
> GlobalOptions/
> glue/
> gmodels/
> GO.db/
> goftest/
> googleVis/
> GOplot/
> GOstats/
> gplots/
> graph/
> graphite/
> gridExtra/
> GSA/
> GSEABase/
> gtable/
> gtools/
> hash/
> haven/
> HDF5Array/
> hdf5r/
> hexbin/
> highr/
> Hmisc/
> hms/
> HSMMSingleCell/
> htmlTable/
> htmltools/
> htmlwidgets/
> httpuv/
> httr/
> hwriter/
> hypergeo/
> ica/
> igraph/
> impute/
> ini/
> inline/
> IRanges/
> irlba/
> iterators/
> jsonlite/
> kBET/
> KEGGgraph/
> KEGGREST/
> kernlab/
> knitr/
> labeling/
> laeken/
> lambda.r/
> lars/
> later/
> latticeExtra/
> lazyeval/
> limma/
> Linnorm/
> lle/
> lme4/
> Lmoments/
> lmtest/
> locfit/
> loo/
> lsei/
> lubridate/
> M3Drop/
> magrittr/
> maps/
> maptools/
> markdown/
> MAST/
> Matrix/
> MatrixModels/
> matrixStats/
> mclust/
> MCMCglmm/
> memoise/
> metap/
> mime/
> minqa/
> mixtools/
> mnormt/
> mockery/
> modelr/
> modeltools/
> moments/
> monocle/
> munsell/
> Mus.musculus/
> mvoutlier/
> mvtnorm/
> NADA/
> nloptr/
> npsurv/
> numDeriv/
> openssl/
> openxlsx/
> OrganismDbi/
> org.Hs.eg.db/
> org.Mm.eg.db/
> orthopolynom/
> ouija/
> packrat/
> pathview/
> pbapply/
> pbkrtest/
> pcaMethods/
> pcaPP/
> pcaReduce/
> penalized/
> permute/
> PFAM.db/
> pheatmap/
> pillar/
> pkgbuild/
> pkgconfig/
> pkgload/
> pkgmaker/
> PKI/
> plogr/
> plotly/
> pls/
> plyr/
> plyranges/
> png/
> polyclip/
> polynom/
> prabclus/
> praise/
> preprocessCore/
> prettyunits/
> processx/
> progress/
> promises/
> ProtGenerics/
> proxy/
> pryr/
> ps/
> purrr/
> qlcMatrix/
> quantreg/
> R6/
> randomForest/
> ranger/
> RANN/
> rappdirs/
> RBGL/
> rcmdcheck/
> RColorBrewer/
> Rcpp/
> RcppAnnoy/
> RcppArmadillo/
> RcppEigen/
> RcppProgress/
> RCurl/
> Rdpack/
> readr/
> readxl/
> refGenome/
> registry/
> reldist/
> rematch/
> remotes/
> ReportingTools/
> reprex/
> reshape/
> reshape2/
> reticulate/
> Rgraphviz/
> rhdf5/
> Rhdf5lib/
> rio/
> rJava/
> rjson/
> RJSONIO/
> rlang/
> RMariaDB/
> rmarkdown/
> R.methodsS3/
> Rmisc/
> RMTstat/
> rngtools/
> robCompositions/
> robust/
> robustbase/
> ROCR/
> R.oo/
> Rook/
> rprojroot/
> rrcov/
> Rsamtools/
> rsconnect/
> RSQLite/
> rstan/
> rstudioapi/
> rsvd/
> rtracklayer/
> Rtsne/
> RUnit/
> R.utils/
> RUVSeq/
> rvest/
> S4Vectors/
> safe/
> SC3/
> scales/
> scater/
> scatterplot3d/
> scde/
> scfind/
> scImpute/
> scmap/
> SCnorm/
> scran/
> scRNAseq/
> scRNA.seq.funcs/
> SDMTools/
> segmented/
> selectr/
> sessioninfo/
> Seurat/
> sgeostat/
> shape/
> shiny/
> ShortRead/
> SingleCellExperiment/
> slam/
> SLICER/
> smoother/
> snow/
> snowfall/
> sourcetools/
> sp/
> spam/
> SparseM/
> sparsesvd/
> spatstat/
> spatstat.data/
> spatstat.utils/
> SPIA/
> splancs/
> sROC/
> StanHeaders/
> statmod/
> stringi/
> stringr/
> SummarizedExperiment/
> sva/
> sys/
> tensor/
> tensorA/
> testthat/
> tibble/
> tidyr/
> tidyselect/
> tidyverse/
> tinytex/
> topGO/
> trimcluster/
> tripack/
> truncnorm/
> TSCAN/
> tsne/
> TTR/
> TxDb.Mmusculus.UCSC.mm10.ensGene/
> TxDb.Mmusculus.UCSC.mm10.knownGene/
> UpSetR/
> usethis/
> utf8/
> VariantAnnotation/
> vcd/
> vegan/
> VennDiagram/
> Vennerable/
> venneuler/
> VGAM/
> VIM/
> vipor/
> viridis/
> viridisLite/
> WGCNA/
> whisker/
> withr/
> WriteXLS/
> xfun/
> XML/
> xml2/
> xopen/
> xtable/
> xts/
> XVector/
> yaml/
> zCompositions/
> zip/
> zlibbioc/
> zoo/
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From t@n@@@ @end|ng |rom gm@||@com  Fri May 24 12:25:29 2019
From: t@n@@@ @end|ng |rom gm@||@com (Bogdan Tanasa)
Date: Fri, 24 May 2019 03:25:29 -0700
Subject: [R] to run an older version of R on my machine
In-Reply-To: <CAPxhGmj-MZ+9KkaiDNph3Zfux9L45cBzX9SSc3d_5U5gFEevjg@mail.gmail.com>
References: <CA+JEM03Od8i7gTLAv99U4SiQbKKuJMxzPdvW-UaoBA3JX6iUdQ@mail.gmail.com>
 <CAPxhGmj-MZ+9KkaiDNph3Zfux9L45cBzX9SSc3d_5U5gFEevjg@mail.gmail.com>
Message-ID: <CA+JEM01OCqEQjM2G2Ni1imgiPHcr_4RFMYX1Ru8NSWMrYz3Org@mail.gmail.com>

thanks a lot, Alfredo ! did not know about Rswitch - very helpful !

On Thu, May 23, 2019 at 11:39 PM Alfredo Cortell <
alfredo.cortell.nicolau at gmail.com> wrote:

> Hi Bogdan,
>
> The way I do this is the following: I have different R versions installed,
> and then I downloaded and use Rswitch to change between versions. You just
> open it, select the version you want, and R will open in that version
> directly. It works with R and Rstudio in a MacOS HighSierra, although I
> have read that it doesn't work in every platform. I don't know about
> ubuntu. I would advise you to try it anyhow. Good luck with that!
>
> All the best,
>
> Alfredo
>
> El jue., 23 may. 2019 a las 21:38, Bogdan Tanasa (<tanasa at gmail.com>)
> escribi?:
>
>> Dear all,
>>
>> if you could help me please with a solution to a simple question :
>>
>> i believe that my ubuntu machine automatically installed R 3.6.0 : when i
>> type : > R. it says :
>>
>> R version 3.6.0 (2019-04-26) -- "Planting of a Tree"
>> Copyright (C) 2019 The R Foundation for Statistical Computing
>> Platform: x86_64-pc-linux-gnu (64-bit)
>>
>> However, I need to use a previous version of R, namely R 3.5, that was
>> installed and did run on my Ubuntu machine, and I can see lots of folders
>> in the directory (a long list follows below) :
>>
>> /home/bogdan/R/x86_64-pc-linux-gnu-library/3.5
>>
>> Please would you advise how can I revert to R 3.5 instead of using R 3.6 .
>> Thanks a lot,
>>
>> bogdan
>>
>> ps : the list of folders in ~/R/x86_64-pc-linux-gnu-library/3.5
>>
>> abind/
>> acepack/
>> ALL/
>> alphahull/
>> amap/
>> annotate/
>> AnnotationDbi/
>> AnnotationFilter/
>> AnnotationForge/
>> apcluster/
>> ape/
>> aroma.light/
>> askpass/
>> assertthat/
>> backports/
>> base64enc/
>> bbmle/
>> beachmat/
>> beeswarm/
>> BH/
>> bibtex/
>> bindr/
>> bindrcpp/
>> Biobase/
>> BiocFileCache/
>> BiocGenerics/
>> biocGraph/
>> BiocInstaller/
>> BiocManager/
>> BiocNeighbors/
>> BiocParallel/
>> BiocStyle/
>> BiocVersion/
>> biocViews/
>> biomaRt/
>> Biostrings/
>> biovizBase/
>> bit/
>> bit64/
>> bitops/
>> bladderbatch/
>> blob/
>> bookdown/
>> brew/
>> broom/
>> BSgenome/
>> Cairo/
>> callr/
>> car/
>> carData/
>> Category/
>> caTools/
>> CCA/
>> CCP/
>> cellranger/
>> cellrangerRkit/
>> checkmate/
>> circlize/
>> cli/
>> clipr/
>> clisymbols/
>> coda/
>> colorspace/
>> combinat/
>> ComplexHeatmap/
>> contfrac/
>> corpcor/
>> corrplot/
>> cowplot/
>> crayon/
>> crosstalk/
>> cubature/
>> curl/
>> cvTools/
>> data.table/
>> DBI/
>> dbplyr/
>> DDRTree/
>> DelayedArray/
>> DelayedMatrixStats/
>> deldir/
>> densityClust/
>> DEoptimR/
>> desc/
>> DESeq/
>> DESeq2/
>> deSolve/
>> destiny/
>> devtools/
>> dichromat/
>> digest/
>> diptest/
>> distillery/
>> doBy/
>> docopt/
>> doParallel/
>> doRNG/
>> doSNOW/
>> dotCall64/
>> dplyr/
>> DropletUtils/
>> dtw/
>> dynamicTreeCut/
>> e1071/
>> EDASeq/
>> edgeR/
>> ellipse/
>> ellipsis/
>> elliptic/
>> EnrichmentBrowser/
>> enrichR/
>> EnsDb.Hsapiens.v86/
>> EnsDb.Mmusculus.v79/
>> ensembldb/
>> evaluate/
>> extRemes/
>> fansi/
>> fastcluster/
>> fastICA/
>> fda/
>> fields/
>> fitdistrplus/
>> fit.models/
>> flexmix/
>> FNN/
>> forcats/
>> foreach/
>> formatR/
>> Formula/
>> fpc/
>> fs/
>> futile.logger/
>> futile.options/
>> gage/
>> gbRd/
>> gdata/
>> genefilter/
>> geneplotter/
>> generics/
>> GenomeInfoDb/
>> GenomeInfoDbData/
>> GenomicAlignments/
>> GenomicFeatures/
>> GenomicRanges/
>> GEOquery/
>> GetoptLong/
>> GGally/
>> ggbeeswarm/
>> ggbio/
>> ggdendro/
>> ggfortify/
>> ggplot2/
>> ggrepel/
>> ggridges/
>> ggthemes/
>> gh/
>> git2r/
>> githubinstall/
>> Glimma/
>> GlobalOptions/
>> glue/
>> gmodels/
>> GO.db/
>> goftest/
>> googleVis/
>> GOplot/
>> GOstats/
>> gplots/
>> graph/
>> graphite/
>> gridExtra/
>> GSA/
>> GSEABase/
>> gtable/
>> gtools/
>> hash/
>> haven/
>> HDF5Array/
>> hdf5r/
>> hexbin/
>> highr/
>> Hmisc/
>> hms/
>> HSMMSingleCell/
>> htmlTable/
>> htmltools/
>> htmlwidgets/
>> httpuv/
>> httr/
>> hwriter/
>> hypergeo/
>> ica/
>> igraph/
>> impute/
>> ini/
>> inline/
>> IRanges/
>> irlba/
>> iterators/
>> jsonlite/
>> kBET/
>> KEGGgraph/
>> KEGGREST/
>> kernlab/
>> knitr/
>> labeling/
>> laeken/
>> lambda.r/
>> lars/
>> later/
>> latticeExtra/
>> lazyeval/
>> limma/
>> Linnorm/
>> lle/
>> lme4/
>> Lmoments/
>> lmtest/
>> locfit/
>> loo/
>> lsei/
>> lubridate/
>> M3Drop/
>> magrittr/
>> maps/
>> maptools/
>> markdown/
>> MAST/
>> Matrix/
>> MatrixModels/
>> matrixStats/
>> mclust/
>> MCMCglmm/
>> memoise/
>> metap/
>> mime/
>> minqa/
>> mixtools/
>> mnormt/
>> mockery/
>> modelr/
>> modeltools/
>> moments/
>> monocle/
>> munsell/
>> Mus.musculus/
>> mvoutlier/
>> mvtnorm/
>> NADA/
>> nloptr/
>> npsurv/
>> numDeriv/
>> openssl/
>> openxlsx/
>> OrganismDbi/
>> org.Hs.eg.db/
>> org.Mm.eg.db/
>> orthopolynom/
>> ouija/
>> packrat/
>> pathview/
>> pbapply/
>> pbkrtest/
>> pcaMethods/
>> pcaPP/
>> pcaReduce/
>> penalized/
>> permute/
>> PFAM.db/
>> pheatmap/
>> pillar/
>> pkgbuild/
>> pkgconfig/
>> pkgload/
>> pkgmaker/
>> PKI/
>> plogr/
>> plotly/
>> pls/
>> plyr/
>> plyranges/
>> png/
>> polyclip/
>> polynom/
>> prabclus/
>> praise/
>> preprocessCore/
>> prettyunits/
>> processx/
>> progress/
>> promises/
>> ProtGenerics/
>> proxy/
>> pryr/
>> ps/
>> purrr/
>> qlcMatrix/
>> quantreg/
>> R6/
>> randomForest/
>> ranger/
>> RANN/
>> rappdirs/
>> RBGL/
>> rcmdcheck/
>> RColorBrewer/
>> Rcpp/
>> RcppAnnoy/
>> RcppArmadillo/
>> RcppEigen/
>> RcppProgress/
>> RCurl/
>> Rdpack/
>> readr/
>> readxl/
>> refGenome/
>> registry/
>> reldist/
>> rematch/
>> remotes/
>> ReportingTools/
>> reprex/
>> reshape/
>> reshape2/
>> reticulate/
>> Rgraphviz/
>> rhdf5/
>> Rhdf5lib/
>> rio/
>> rJava/
>> rjson/
>> RJSONIO/
>> rlang/
>> RMariaDB/
>> rmarkdown/
>> R.methodsS3/
>> Rmisc/
>> RMTstat/
>> rngtools/
>> robCompositions/
>> robust/
>> robustbase/
>> ROCR/
>> R.oo/
>> Rook/
>> rprojroot/
>> rrcov/
>> Rsamtools/
>> rsconnect/
>> RSQLite/
>> rstan/
>> rstudioapi/
>> rsvd/
>> rtracklayer/
>> Rtsne/
>> RUnit/
>> R.utils/
>> RUVSeq/
>> rvest/
>> S4Vectors/
>> safe/
>> SC3/
>> scales/
>> scater/
>> scatterplot3d/
>> scde/
>> scfind/
>> scImpute/
>> scmap/
>> SCnorm/
>> scran/
>> scRNAseq/
>> scRNA.seq.funcs/
>> SDMTools/
>> segmented/
>> selectr/
>> sessioninfo/
>> Seurat/
>> sgeostat/
>> shape/
>> shiny/
>> ShortRead/
>> SingleCellExperiment/
>> slam/
>> SLICER/
>> smoother/
>> snow/
>> snowfall/
>> sourcetools/
>> sp/
>> spam/
>> SparseM/
>> sparsesvd/
>> spatstat/
>> spatstat.data/
>> spatstat.utils/
>> SPIA/
>> splancs/
>> sROC/
>> StanHeaders/
>> statmod/
>> stringi/
>> stringr/
>> SummarizedExperiment/
>> sva/
>> sys/
>> tensor/
>> tensorA/
>> testthat/
>> tibble/
>> tidyr/
>> tidyselect/
>> tidyverse/
>> tinytex/
>> topGO/
>> trimcluster/
>> tripack/
>> truncnorm/
>> TSCAN/
>> tsne/
>> TTR/
>> TxDb.Mmusculus.UCSC.mm10.ensGene/
>> TxDb.Mmusculus.UCSC.mm10.knownGene/
>> UpSetR/
>> usethis/
>> utf8/
>> VariantAnnotation/
>> vcd/
>> vegan/
>> VennDiagram/
>> Vennerable/
>> venneuler/
>> VGAM/
>> VIM/
>> vipor/
>> viridis/
>> viridisLite/
>> WGCNA/
>> whisker/
>> withr/
>> WriteXLS/
>> xfun/
>> XML/
>> xml2/
>> xopen/
>> xtable/
>> xts/
>> XVector/
>> yaml/
>> zCompositions/
>> zip/
>> zlibbioc/
>> zoo/
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From t@n@@@ @end|ng |rom gm@||@com  Fri May 24 12:27:45 2019
From: t@n@@@ @end|ng |rom gm@||@com (Bogdan Tanasa)
Date: Fri, 24 May 2019 03:27:45 -0700
Subject: [R] computing standard deviation in R and in Python
Message-ID: <CA+JEM03YPfU2T6sm7RfT4+kgkY3wB=k6+dz6eNxTnPXfpx+QGw@mail.gmail.com>

Dear all, please would you advise :

do python and R have different ways to compute the standard deviation (sd) ?

for example, in python, starting with :

a = np.array([[1,2,3],  [4,5,6], [7,8,9]])
print(a.std(axis=1)) ### per row : [0.81649658 0.81649658 0.81649658]
print(a.std(axis=0)) ### per column : [2.44948974 2.44948974 2.44948974]

# and in R :



z <- matrix(c(1,2,3,4,5,6,7,8,9), nrow=3, ncol=3, byrow=T)
# z# [,1] [,2] [,3]#[1,] 1 2 3#[2,] 4 5 6#[3,] 7 8 9
# apply(z, 1, sd)
sd(z[1,]) #1
sd(z[2,]) #1
sd(z[3,]) #1
# apply(z, 2, sd)
sd(z[,1]) #3
sd(z[,2]) #3
sd(z[,3]) #3

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri May 24 13:35:52 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 24 May 2019 12:35:52 +0100
Subject: [R] computing standard deviation in R and in Python
In-Reply-To: <CA+JEM03YPfU2T6sm7RfT4+kgkY3wB=k6+dz6eNxTnPXfpx+QGw@mail.gmail.com>
References: <CA+JEM03YPfU2T6sm7RfT4+kgkY3wB=k6+dz6eNxTnPXfpx+QGw@mail.gmail.com>
Message-ID: <108a7a70-92a0-2bb4-47b4-449903b26f32@sapo.pt>

Hello,

This has to do with what kind of variance estimator is being used.
R uses the unbiased estimator and Python the MLE one.



var1 <- function(x){
   n <- length(x)
   (sum(x^2) - sum(x)^2/n)/(n - 1)
}
var2 <- function(x){
   n <- length(x)
   (sum(x^2) - sum(x)^2/n)/n
}

sd1 <- function(x) sqrt(var1(x))
sd2 <- function(x) sqrt(var2(x))

z <- matrix(c(1,2,3,4,5,6,7,8,9), nrow=3, ncol=3, byrow=T)

apply(z, 1, sd1)  # R
apply(z, 1, sd2)  # Python

apply(z, 2, sd1)  # R
apply(z, 2, sd2)  # Python


Hope this helps,

Rui Barradas

?s 11:27 de 24/05/19, Bogdan Tanasa escreveu:
> Dear all, please would you advise :
> 
> do python and R have different ways to compute the standard deviation (sd) ?
> 
> for example, in python, starting with :
> 
> a = np.array([[1,2,3],  [4,5,6], [7,8,9]])
> print(a.std(axis=1)) ### per row : [0.81649658 0.81649658 0.81649658]
> print(a.std(axis=0)) ### per column : [2.44948974 2.44948974 2.44948974]
> 
> # and in R :
> 
> 
> 
> z <- matrix(c(1,2,3,4,5,6,7,8,9), nrow=3, ncol=3, byrow=T)
> # z# [,1] [,2] [,3]#[1,] 1 2 3#[2,] 4 5 6#[3,] 7 8 9
> # apply(z, 1, sd)
> sd(z[1,]) #1
> sd(z[2,]) #1
> sd(z[3,]) #1
> # apply(z, 2, sd)
> sd(z[,1]) #3
> sd(z[,2]) #3
> sd(z[,3]) #3
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From t@n@@@ @end|ng |rom gm@||@com  Fri May 24 13:39:21 2019
From: t@n@@@ @end|ng |rom gm@||@com (Bogdan Tanasa)
Date: Fri, 24 May 2019 04:39:21 -0700
Subject: [R] computing standard deviation in R and in Python
In-Reply-To: <108a7a70-92a0-2bb4-47b4-449903b26f32@sapo.pt>
References: <CA+JEM03YPfU2T6sm7RfT4+kgkY3wB=k6+dz6eNxTnPXfpx+QGw@mail.gmail.com>
 <108a7a70-92a0-2bb4-47b4-449903b26f32@sapo.pt>
Message-ID: <CA+JEM02Eo_O_kAcbOWOuR_deyXcDQNttT1N_XPnurd+525_isg@mail.gmail.com>

Dear Rui, thank you very much !

On Fri, May 24, 2019 at 4:35 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> This has to do with what kind of variance estimator is being used.
> R uses the unbiased estimator and Python the MLE one.
>
>
>
> var1 <- function(x){
>    n <- length(x)
>    (sum(x^2) - sum(x)^2/n)/(n - 1)
> }
> var2 <- function(x){
>    n <- length(x)
>    (sum(x^2) - sum(x)^2/n)/n
> }
>
> sd1 <- function(x) sqrt(var1(x))
> sd2 <- function(x) sqrt(var2(x))
>
> z <- matrix(c(1,2,3,4,5,6,7,8,9), nrow=3, ncol=3, byrow=T)
>
> apply(z, 1, sd1)  # R
> apply(z, 1, sd2)  # Python
>
> apply(z, 2, sd1)  # R
> apply(z, 2, sd2)  # Python
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 11:27 de 24/05/19, Bogdan Tanasa escreveu:
> > Dear all, please would you advise :
> >
> > do python and R have different ways to compute the standard deviation
> (sd) ?
> >
> > for example, in python, starting with :
> >
> > a = np.array([[1,2,3],  [4,5,6], [7,8,9]])
> > print(a.std(axis=1)) ### per row : [0.81649658 0.81649658 0.81649658]
> > print(a.std(axis=0)) ### per column : [2.44948974 2.44948974 2.44948974]
> >
> > # and in R :
> >
> >
> >
> > z <- matrix(c(1,2,3,4,5,6,7,8,9), nrow=3, ncol=3, byrow=T)
> > # z# [,1] [,2] [,3]#[1,] 1 2 3#[2,] 4 5 6#[3,] 7 8 9
> > # apply(z, 1, sd)
> > sd(z[1,]) #1
> > sd(z[2,]) #1
> > sd(z[3,]) #1
> > # apply(z, 2, sd)
> > sd(z[,1]) #3
> > sd(z[,2]) #3
> > sd(z[,3]) #3
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>

	[[alternative HTML version deleted]]


From p@u|bern@|07 @end|ng |rom gm@||@com  Fri May 24 15:10:27 2019
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Fri, 24 May 2019 08:10:27 -0500
Subject: [R] Subsetting Data from a Dataframe
Message-ID: <CAMOcQfP-ZL1o8-0L17gEent7wULvF2tOZ1njU4PAKGdO+FQJkA@mail.gmail.com>

Dear friends,

Hope you are all doing well. I would like to know how to retrieve a
complete dataframe (all the columns), except for the cases when one of the
columns have either nulls or NAs.

In this case, I?d like to retrieve all the columns but only the cases
(rows) where Var5 has values different than 0, null or NAs.

I am attaching the dput() of my dataset as a reference:

> dput(datos)
structure(list(VarDate = structure(c(355L, 321L, 86L, 155L, 121L,
255L, 20L, 288L, 221L, 188L, 53L, 389L, 356L, 322L, 87L, 156L,
122L, 256L, 21L, 289L, 222L, 189L, 54L, 390L, 357L, 323L, 88L,
157L, 123L, 257L, 22L, 290L, 223L, 190L, 55L, 391L, 358L, 324L,
89L, 158L, 124L, 258L, 23L, 291L, 224L, 191L, 56L, 392L, 359L,
325L, 90L, 159L, 125L, 259L, 24L, 292L, 225L, 192L, 57L, 393L,
360L, 326L, 91L, 160L, 126L, 260L, 25L, 293L, 226L, 193L, 58L,
394L, 361L, 327L, 92L, 161L, 127L, 261L, 26L, 294L, 227L, 194L,
59L, 395L, 362L, 328L, 93L, 162L, 128L, 262L, 27L, 295L, 228L,
195L, 60L, 396L, 363L, 329L, 94L, 163L, 129L, 263L, 28L, 296L,
229L, 196L, 61L, 397L, 364L, 330L, 95L, 164L, 130L, 264L, 29L,
297L, 230L, 197L, 62L, 398L, 365L, 331L, 96L, 165L, 131L, 265L,
30L, 298L, 231L, 198L, 63L, 399L, 366L, 332L, 97L, 166L, 132L,
266L, 31L, 299L, 232L, 199L, 64L, 400L, 367L, 333L, 98L, 167L,
133L, 267L, 32L, 300L, 233L, 200L, 65L, 401L, 368L, 334L, 99L,
168L, 134L, 268L, 33L, 301L, 234L, 201L, 66L, 402L, 369L, 335L,
100L, 135L, 101L, 235L, 1L, 269L, 202L, 169L, 34L, 370L, 336L,
302L, 67L, 136L, 102L, 236L, 2L, 270L, 203L, 170L, 35L, 371L,
337L, 303L, 68L, 137L, 103L, 237L, 3L, 271L, 204L, 171L, 36L,
372L, 338L, 304L, 69L, 138L, 104L, 238L, 4L, 272L, 205L, 172L,
37L, 373L, 339L, 305L, 70L, 139L, 105L, 239L, 5L, 273L, 206L,
173L, 38L, 374L, 340L, 306L, 71L, 140L, 106L, 240L, 6L, 274L,
207L, 174L, 39L, 375L, 341L, 307L, 72L, 141L, 107L, 241L, 7L,
275L, 208L, 175L, 40L, 376L, 342L, 308L, 73L, 142L, 108L, 242L,
8L, 276L, 209L, 176L, 41L, 377L, 343L, 309L, 74L, 143L, 109L,
243L, 9L, 277L, 210L, 177L, 42L, 378L, 344L, 310L, 75L, 144L,
110L, 244L, 10L, 278L, 211L, 178L, 43L, 379L, 345L, 311L, 76L,
145L, 111L, 245L, 11L, 279L, 212L, 179L, 44L, 380L, 346L, 312L,
77L, 146L, 112L, 246L, 12L, 280L, 213L, 180L, 45L, 381L, 347L,
313L, 78L, 147L, 113L, 247L, 13L, 281L, 214L, 181L, 46L, 382L,
348L, 314L, 79L, 148L, 114L, 248L, 14L, 282L, 215L, 182L, 47L,
383L, 349L, 315L, 80L, 149L, 115L, 249L, 15L, 283L, 216L, 183L,
48L, 384L, 350L, 316L, 81L, 150L, 116L, 250L, 16L, 284L, 217L,
184L, 49L, 385L, 351L, 317L, 82L, 151L, 117L, 251L, 17L, 285L,
218L, 185L, 50L, 386L, 352L, 318L, 83L, 152L, 118L, 252L, 18L,
286L, 219L, 186L, 51L, 387L, 353L, 319L, 84L, 153L, 119L, 253L,
19L, 287L, 220L, 187L, 52L, 388L, 354L, 320L, 85L, 154L, 120L,
254L), .Label = c("1-Apr-00", "1-Apr-01", "1-Apr-02", "1-Apr-03",
"1-Apr-04", "1-Apr-05", "1-Apr-06", "1-Apr-07", "1-Apr-08", "1-Apr-09",
"1-Apr-10", "1-Apr-11", "1-Apr-12", "1-Apr-13", "1-Apr-14", "1-Apr-15",
"1-Apr-16", "1-Apr-17", "1-Apr-18", "1-Apr-86", "1-Apr-87", "1-Apr-88",
"1-Apr-89", "1-Apr-90", "1-Apr-91", "1-Apr-92", "1-Apr-93", "1-Apr-94",
"1-Apr-95", "1-Apr-96", "1-Apr-97", "1-Apr-98", "1-Apr-99", "1-Aug-00",
"1-Aug-01", "1-Aug-02", "1-Aug-03", "1-Aug-04", "1-Aug-05", "1-Aug-06",
"1-Aug-07", "1-Aug-08", "1-Aug-09", "1-Aug-10", "1-Aug-11", "1-Aug-12",
"1-Aug-13", "1-Aug-14", "1-Aug-15", "1-Aug-16", "1-Aug-17", "1-Aug-18",
"1-Aug-86", "1-Aug-87", "1-Aug-88", "1-Aug-89", "1-Aug-90", "1-Aug-91",
"1-Aug-92", "1-Aug-93", "1-Aug-94", "1-Aug-95", "1-Aug-96", "1-Aug-97",
"1-Aug-98", "1-Aug-99", "1-Dec-00", "1-Dec-01", "1-Dec-02", "1-Dec-03",
"1-Dec-04", "1-Dec-05", "1-Dec-06", "1-Dec-07", "1-Dec-08", "1-Dec-09",
"1-Dec-10", "1-Dec-11", "1-Dec-12", "1-Dec-13", "1-Dec-14", "1-Dec-15",
"1-Dec-16", "1-Dec-17", "1-Dec-18", "1-Dec-85", "1-Dec-86", "1-Dec-87",
"1-Dec-88", "1-Dec-89", "1-Dec-90", "1-Dec-91", "1-Dec-92", "1-Dec-93",
"1-Dec-94", "1-Dec-95", "1-Dec-96", "1-Dec-97", "1-Dec-98", "1-Dec-99",
"1-Feb-00", "1-Feb-01", "1-Feb-02", "1-Feb-03", "1-Feb-04", "1-Feb-05",
"1-Feb-06", "1-Feb-07", "1-Feb-08", "1-Feb-09", "1-Feb-10", "1-Feb-11",
"1-Feb-12", "1-Feb-13", "1-Feb-14", "1-Feb-15", "1-Feb-16", "1-Feb-17",
"1-Feb-18", "1-Feb-19", "1-Feb-86", "1-Feb-87", "1-Feb-88", "1-Feb-89",
"1-Feb-90", "1-Feb-91", "1-Feb-92", "1-Feb-93", "1-Feb-94", "1-Feb-95",
"1-Feb-96", "1-Feb-97", "1-Feb-98", "1-Feb-99", "1-Jan-00", "1-Jan-01",
"1-Jan-02", "1-Jan-03", "1-Jan-04", "1-Jan-05", "1-Jan-06", "1-Jan-07",
"1-Jan-08", "1-Jan-09", "1-Jan-10", "1-Jan-11", "1-Jan-12", "1-Jan-13",
"1-Jan-14", "1-Jan-15", "1-Jan-16", "1-Jan-17", "1-Jan-18", "1-Jan-19",
"1-Jan-86", "1-Jan-87", "1-Jan-88", "1-Jan-89", "1-Jan-90", "1-Jan-91",
"1-Jan-92", "1-Jan-93", "1-Jan-94", "1-Jan-95", "1-Jan-96", "1-Jan-97",
"1-Jan-98", "1-Jan-99", "1-Jul-00", "1-Jul-01", "1-Jul-02", "1-Jul-03",
"1-Jul-04", "1-Jul-05", "1-Jul-06", "1-Jul-07", "1-Jul-08", "1-Jul-09",
"1-Jul-10", "1-Jul-11", "1-Jul-12", "1-Jul-13", "1-Jul-14", "1-Jul-15",
"1-Jul-16", "1-Jul-17", "1-Jul-18", "1-Jul-86", "1-Jul-87", "1-Jul-88",
"1-Jul-89", "1-Jul-90", "1-Jul-91", "1-Jul-92", "1-Jul-93", "1-Jul-94",
"1-Jul-95", "1-Jul-96", "1-Jul-97", "1-Jul-98", "1-Jul-99", "1-Jun-00",
"1-Jun-01", "1-Jun-02", "1-Jun-03", "1-Jun-04", "1-Jun-05", "1-Jun-06",
"1-Jun-07", "1-Jun-08", "1-Jun-09", "1-Jun-10", "1-Jun-11", "1-Jun-12",
"1-Jun-13", "1-Jun-14", "1-Jun-15", "1-Jun-16", "1-Jun-17", "1-Jun-18",
"1-Jun-86", "1-Jun-87", "1-Jun-88", "1-Jun-89", "1-Jun-90", "1-Jun-91",
"1-Jun-92", "1-Jun-93", "1-Jun-94", "1-Jun-95", "1-Jun-96", "1-Jun-97",
"1-Jun-98", "1-Jun-99", "1-Mar-00", "1-Mar-01", "1-Mar-02", "1-Mar-03",
"1-Mar-04", "1-Mar-05", "1-Mar-06", "1-Mar-07", "1-Mar-08", "1-Mar-09",
"1-Mar-10", "1-Mar-11", "1-Mar-12", "1-Mar-13", "1-Mar-14", "1-Mar-15",
"1-Mar-16", "1-Mar-17", "1-Mar-18", "1-Mar-19", "1-Mar-86", "1-Mar-87",
"1-Mar-88", "1-Mar-89", "1-Mar-90", "1-Mar-91", "1-Mar-92", "1-Mar-93",
"1-Mar-94", "1-Mar-95", "1-Mar-96", "1-Mar-97", "1-Mar-98", "1-Mar-99",
"1-May-00", "1-May-01", "1-May-02", "1-May-03", "1-May-04", "1-May-05",
"1-May-06", "1-May-07", "1-May-08", "1-May-09", "1-May-10", "1-May-11",
"1-May-12", "1-May-13", "1-May-14", "1-May-15", "1-May-16", "1-May-17",
"1-May-18", "1-May-86", "1-May-87", "1-May-88", "1-May-89", "1-May-90",
"1-May-91", "1-May-92", "1-May-93", "1-May-94", "1-May-95", "1-May-96",
"1-May-97", "1-May-98", "1-May-99", "1-Nov-00", "1-Nov-01", "1-Nov-02",
"1-Nov-03", "1-Nov-04", "1-Nov-05", "1-Nov-06", "1-Nov-07", "1-Nov-08",
"1-Nov-09", "1-Nov-10", "1-Nov-11", "1-Nov-12", "1-Nov-13", "1-Nov-14",
"1-Nov-15", "1-Nov-16", "1-Nov-17", "1-Nov-18", "1-Nov-85", "1-Nov-86",
"1-Nov-87", "1-Nov-88", "1-Nov-89", "1-Nov-90", "1-Nov-91", "1-Nov-92",
"1-Nov-93", "1-Nov-94", "1-Nov-95", "1-Nov-96", "1-Nov-97", "1-Nov-98",
"1-Nov-99", "1-Oct-00", "1-Oct-01", "1-Oct-02", "1-Oct-03", "1-Oct-04",
"1-Oct-05", "1-Oct-06", "1-Oct-07", "1-Oct-08", "1-Oct-09", "1-Oct-10",
"1-Oct-11", "1-Oct-12", "1-Oct-13", "1-Oct-14", "1-Oct-15", "1-Oct-16",
"1-Oct-17", "1-Oct-18", "1-Oct-85", "1-Oct-86", "1-Oct-87", "1-Oct-88",
"1-Oct-89", "1-Oct-90", "1-Oct-91", "1-Oct-92", "1-Oct-93", "1-Oct-94",
"1-Oct-95", "1-Oct-96", "1-Oct-97", "1-Oct-98", "1-Oct-99", "1-Sep-00",
"1-Sep-01", "1-Sep-02", "1-Sep-03", "1-Sep-04", "1-Sep-05", "1-Sep-06",
"1-Sep-07", "1-Sep-08", "1-Sep-09", "1-Sep-10", "1-Sep-11", "1-Sep-12",
"1-Sep-13", "1-Sep-14", "1-Sep-15", "1-Sep-16", "1-Sep-17", "1-Sep-18",
"1-Sep-86", "1-Sep-87", "1-Sep-88", "1-Sep-89", "1-Sep-90", "1-Sep-91",
"1-Sep-92", "1-Sep-93", "1-Sep-94", "1-Sep-95", "1-Sep-96", "1-Sep-97",
"1-Sep-98", "1-Sep-99"), class = "factor"), Var1 = c(150L, 140L,
148L, 157L, 105L, 132L, 123L, 139L, 128L, 174L, 152L, 137L, 143L,
159L, 126L, 142L, 122L, 165L, 122L, 136L, 146L, 128L, 139L, 124L,
128L, 128L, 133L, 143L, 126L, 135L, 137L, 151L, 125L, 123L, 135L,
132L, 128L, 139L, 121L, 134L, 103L, 124L, 124L, 135L, 121L, 118L,
134L, 116L, 123L, 106L, 93L, 145L, 105L, 134L, 122L, 124L, 133L,
128L, 134L, 134L, 116L, 111L, 124L, 104L, 127L, 127L, 121L, 110L,
120L, 128L, 133L, 112L, 122L, 117L, 121L, 114L, 114L, 133L, 127L,
133L, 120L, 148L, 132L, 133L, 152L, 138L, 128L, 127L, 107L, 134L,
130L, 136L, 125L, 140L, 127L, 113L, 118L, 106L, 106L, 97L, 108L,
116L, 111L, 131L, 114L, 139L, 133L, 121L, 146L, 143L, 139L, 131L,
117L, 137L, 143L, 138L, 109L, 145L, 127L, 122L, 129L, 124L, 120L,
96L, 115L, 114L, 121L, 126L, 119L, 125L, 124L, 119L, 133L, 103L,
106L, 120L, 107L, 113L, 115L, 131L, 108L, 121L, 99L, 102L, 113L,
108L, 105L, 118L, 98L, 100L, 101L, 119L, 91L, 112L, 91L, 105L,
115L, 93L, 106L, 87L, 69L, 89L, 91L, 89L, 82L, 85L, 89L, 75L,
88L, 72L, 72L, 70L, 57L, 90L, 95L, 102L, 109L, 84L, 88L, 70L,
103L, 94L, 100L, 91L, 68L, 90L, 81L, 117L, 102L, 86L, 78L, 95L,
79L, 82L, 82L, 87L, 79L, 87L, 83L, 78L, 74L, 78L, 81L, 74L, 77L,
69L, 78L, 73L, 62L, 56L, 63L, 70L, 60L, 64L, 67L, 60L, 81L, 57L,
64L, 67L, 67L, 69L, 90L, 70L, 62L, 79L, 69L, 66L, 66L, 64L, 58L,
64L, 50L, 72L, 59L, 77L, 78L, 64L, 78L, 50L, 60L, 58L, 59L, 60L,
44L, 54L, 51L, 52L, 59L, 59L, 63L, 65L, 62L, 66L, 57L, 48L, 59L,
53L, 46L, 53L, 58L, 57L, 66L, 46L, 48L, 41L, 57L, 55L, 50L, 49L,
60L, 66L, 60L, 66L, 57L, 57L, 64L, 61L, 58L, 66L, 53L, 56L, 66L,
55L, 56L, 83L, 71L, 72L, 54L, 51L, 52L, 60L, 49L, 64L, 72L, 67L,
59L, 66L, 56L, 63L, 73L, 56L, 57L, 53L, 54L, 56L, 56L, 80L, 70L,
73L, 53L, 71L, 68L, 47L, 59L, 79L, 49L, 67L, 70L, 64L, 66L, 69L,
52L, 66L, 57L, 52L, 57L, 59L, 49L, 70L, 54L, 60L, 63L, 72L, 69L,
62L, 52L, 49L, 64L, 49L, 48L, 65L, 63L, 52L, 54L, 70L, 48L, 45L,
48L, 47L, 42L, 50L, 38L, 57L, 51L, 38L, 42L, 50L, 54L, 56L, 59L,
40L, 50L, 43L, 35L, 43L, 39L, 31L, 32L, 39L, 40L, 33L, 35L, 41L,
38L, 39L, 28L, 39L, 32L, 39L, 35L, 31L, 32L, 38L, 30L, 21L, 33L,
37L, 28L, 34L, 30L, 37L, 34L, 38L, 34L, 34L, 38L, 33L, 33L, 27L,
18L, 33L), Var2 = c(821273L, 625955L, 809990L, 729112L, 532151L,
725098L, 619868L, 704282L, 580952L, 975656L, 783082L, 678787L,
685767L, 784148L, 628750L, 696202L, 695546L, 907842L, 554405L,
770651L, 746794L, 738427L, 756031L, 677908L, 707298L, 617374L,
760057L, 842253L, 680889L, 731068L, 833567L, 882259L, 695913L,
741626L, 773759L, 746727L, 696405L, 714477L, 688004L, 767584L,
674190L, 730536L, 733525L, 857631L, 750634L, 678859L, 792624L,
594360L, 638948L, 695963L, 553269L, 975142L, 692757L, 902180L,
806432L, 881918L, 847507L, 820608L, 704557L, 790379L, 700958L,
702484L, 781250L, 611012L, 761236L, 815931L, 759334L, 647436L,
738675L, 755135L, 762259L, 686818L, 736825L, 779627L, 728106L,
626037L, 609227L, 710760L, 769409L, 829428L, 693645L, 860311L,
673870L, 633099L, 735438L, 618685L, 697920L, 695360L, 555367L,
655665L, 723109L, 772520L, 686184L, 791464L, 745570L, 659855L,
505423L, 628448L, 452854L, 537319L, 583909L, 638940L, 649767L,
699971L, 688656L, 794720L, 711849L, 610735L, 750121L, 856431L,
760421L, 766238L, 748331L, 751135L, 949228L, 925412L, 701986L,
836017L, 796566L, 683700L, 795191L, 708331L, 634132L, 496022L,
676314L, 668954L, 712050L, 739238L, 674401L, 668638L, 693023L,
702984L, 706872L, 595021L, 588132L, 549140L, 629381L, 623842L,
605154L, 762123L, 635163L, 596381L, 478583L, 527694L, 582280L,
576937L, 609620L, 514848L, 531708L, 559237L, 597501L, 542658L,
449916L, 541199L, 422740L, 520411L, 550272L, 470397L, 471954L,
400365L, 306711L, 445820L, 520121L, 492774L, 424645L, 384204L,
480749L, 320850L, 625051L, 459398L, 376164L, 400453L, 322230L,
503949L, 509943L, 578626L, 611236L, 459764L, 488844L, 421423L,
549084L, 505593L, 531538L, 535442L, 400745L, 524029L, 485794L,
629792L, 572611L, 542791L, 457679L, 599733L, 503070L, 529176L,
448977L, 543827L, 420837L, 534159L, 452072L, 380600L, 422340L,
456238L, 448117L, 349250L, 463418L, 451798L, 422773L, 469039L,
401945L, 371124L, 426345L, 512084L, 376136L, 338565L, 398287L,
348028L, 540228L, 370901L, 382410L, 380287L, 518778L, 434662L,
633628L, 383930L, 409503L, 449935L, 314311L, 309324L, 311242L,
390780L, 252049L, 348992L, 286589L, 413858L, 318478L, 434770L,
358908L, 337788L, 349755L, 244330L, 293098L, 378302L, 350259L,
267338L, 245683L, 337204L, 257568L, 306317L, 314226L, 349265L,
387763L, 383584L, 318554L, 402885L, 299765L, 295652L, 285204L,
350783L, 262768L, 347901L, 402663L, 289501L, 391287L, 295352L,
238815L, 330207L, 370494L, 262839L, 265180L, 323103L, 369609L,
327294L, 407061L, 352180L, 398966L, 272813L, 289250L, 249749L,
269799L, 253926L, 212019L, 299012L, 232329L, 232591L, 226118L,
359578L, 350889L, 333566L, 289996L, 314330L, 355278L, 343200L,
263987L, 360194L, 375162L, 449485L, 310118L, 385856L, 255153L,
289999L, 387187L, 293586L, 362148L, 268204L, 293395L, 312468L,
292847L, 343747L, 311347L, 387747L, 249736L, 338624L, 334526L,
228124L, 299618L, 367627L, 197001L, 273961L, 339628L, 305371L,
321103L, 307636L, 180838L, 300252L, 238870L, 296085L, 299428L,
292380L, 220003L, 333949L, 248405L, 264780L, 300803L, 336372L,
338379L, 257623L, 222979L, 226364L, 349364L, 260642L, 227835L,
237010L, 306306L, 253793L, 169667L, 333917L, 203583L, 204326L,
245043L, 200200L, 206647L, 266962L, 171125L, 227414L, 209053L,
132254L, 126972L, 159604L, 217526L, 186575L, 274024L, 155100L,
182481L, 198915L, 185028L, 165374L, 167406L, 114972L, 153905L,
131214L, 186645L, 133336L, 195207L, 168546L, 175644L, 210364L,
150943L, 142076L, 128045L, 147020L, 157380L, 110542L, 143323L,
169639L, 136196L, 94668L, 215810L, 170180L, 162091L, 139682L,
175133L, 153901L, 158420L, 127673L, 177445L, 156123L, 179980L,
142630L, 160726L, 181325L, 92592L, 191459L), Var3 = c(1023833L,
924771L, 1021634L, 1043681L, 752874L, 947427L, 859879L, 999681L,
835358L, 1252862L, 1047489L, 974229L, 1029854L, 1115937L, 868082L,
972236L, 850169L, 1143772L, 805427L, 965832L, 970293L, 884963L,
996303L, 891808L, 900303L, 835772L, 972515L, 1003546L, 852313L,
987354L, 1028777L, 1117341L, 893702L, 879153L, 1029602L, 982566L,
886107L, 997185L, 875060L, 964200L, 810422L, 862861L, 915637L,
1011660L, 870443L, 864038L, 1004598L, 792715L, 869050L, 792638L,
689028L, 1077470L, 831436L, 988032L, 923528L, 918479L, 1018139L,
990291L, 980459L, 1048576L, 863124L, 854886L, 909880L, 798497L,
938327L, 982216L, 859205L, 827323L, 841991L, 913290L, 983918L,
793665L, 895188L, 906126L, 863296L, 842577L, 832591L, 976840L,
932345L, 981143L, 801512L, 1002306L, 931473L, 902389L, 1067539L,
934557L, 871256L, 883418L, 718243L, 853101L, 892091L, 959493L,
875254L, 992752L, 891876L, 779707L, 761433L, 758197L, 702417L,
694400L, 743969L, 827773L, 788336L, 891501L, 784553L, 936542L,
892221L, 805682L, 986109L, 997737L, 952337L, 913596L, 881975L,
973655L, 1042842L, 991280L, 811610L, 969744L, 871970L, 832743L,
896535L, 898091L, 754328L, 708464L, 757153L, 777688L, 830975L,
891923L, 869821L, 803335L, 882650L, 866547L, 868705L, 747149L,
713600L, 762138L, 735163L, 800943L, 699566L, 913384L, 738171L,
782984L, 667440L, 658650L, 777300L, 722849L, 721270L, 720574L,
649446L, 751518L, 719568L, 747856L, 611773L, 753152L, 650260L,
686105L, 742319L, 611929L, 640700L, 591455L, 422915L, 551312L,
589984L, 552638L, 427190L, 450827L, 556519L, 395846L, 590983L,
512959L, 452685L, 449679L, 377117L, 592666L, 630967L, 677562L,
744774L, 559144L, 565164L, 431371L, 661244L, 562805L, 635075L,
611218L, 465263L, 600953L, 570639L, 741657L, 655390L, 624346L,
519072L, 655233L, 573364L, 555229L, 568871L, 673415L, 562549L,
632082L, 648735L, 547210L, 541646L, 591035L, 573215L, 453220L,
500569L, 483082L, 494882L, 556911L, 400361L, 408601L, 441426L,
505124L, 417482L, 421750L, 417484L, 431120L, 504564L, 333542L,
369482L, 431832L, 475688L, 437028L, 581278L, 408507L, 383338L,
502921L, 392616L, 381636L, 437582L, 402350L, 342748L, 420895L,
304549L, 452065L, 392927L, 525972L, 474072L, 439285L, 506800L,
301623L, 360435L, 431918L, 406583L, 348245L, 311187L, 397553L,
328374L, 386618L, 359652L, 410604L, 415530L, 423691L, 387479L,
427504L, 361311L, 328342L, 384716L, 375293L, 337292L, 378871L,
369858L, 368082L, 447470L, 319699L, 309456L, 324288L, 416815L,
350970L, 305021L, 340379L, 415899L, 415863L, 459164L, 424415L,
426744L, 381180L, 421914L, 432177L, 439146L, 471412L, 358047L,
424430L, 439595L, 378718L, 402841L, 558536L, 475445L, 496879L,
386381L, 371953L, 362575L, 410119L, 307376L, 464888L, 497654L,
505371L, 428729L, 484825L, 340312L, 403197L, 483014L, 425448L,
442173L, 382614L, 365801L, 377530L, 412724L, 582561L, 497182L,
534839L, 372528L, 490513L, 524295L, 353328L, 435740L, 606586L,
339952L, 446177L, 541098L, 505165L, 471455L, 483144L, 372510L,
468379L, 381000L, 388039L, 448045L, 406707L, 363432L, 492507L,
389963L, 458037L, 446202L, 520027L, 534002L, 411419L, 382377L,
374016L, 489217L, 385358L, 354344L, 480102L, 486111L, 396875L,
424706L, 549349L, 331376L, 327566L, 364968L, 365715L, 327275L,
368299L, 277930L, 416003L, 393085L, 322959L, 306650L, 401742L,
420621L, 429129L, 462222L, 335711L, 416480L, 338851L, 329130L,
338371L, 310220L, 255807L, 258337L, 319225L, 331696L, 276512L,
288625L, 329139L, 311016L, 332946L, 231066L, 315569L, 251276L,
255198L, 294448L, 251216L, 262122L, 322465L, 230834L, 164729L,
272468L, 267101L, 223642L, 273754L, 262820L, 299455L, 282714L,
290049L, 277035L, 270754L, 315434L, 267553L, 273266L, 231968L,
145726L, 270737L), Var4 = c(1842916L, 1650947L, 1826792L, 1868854L,
1349682L, 1700705L, 1553206L, 1789866L, 1481587L, 2260812L, 1875500L,
1757982L, 1844106L, 2015221L, 1559047L, 1747566L, 1547336L, 2068885L,
1434585L, 1754398L, 1751677L, 1605872L, 1791263L, 1606000L, 1622632L,
1501355L, 1751479L, 1813795L, 1534006L, 1763481L, 1862021L, 2011101L,
1579945L, 1582041L, 1845277L, 1774748L, 1604085L, 1798036L, 1581748L,
1747547L, 1474606L, 1553108L, 1662061L, 1832545L, 1555462L, 1560721L,
1803880L, 1421983L, 1705225L, 1570989L, 1370337L, 2130208L, 1648356L,
1967994L, 1840196L, 1814451L, 2018955L, 1953286L, 1943577L, 2068666L,
1709487L, 1707015L, 1816645L, 1592986L, 1852489L, 1947445L, 1705050L,
1626904L, 1661471L, 1810373L, 1935931L, 1564800L, 1770490L, 1793351L,
1704562L, 1657873L, 1624573L, 1891007L, 1842884L, 1940814L, 1598949L,
1995798L, 1844261L, 1783182L, 2300414L, 2035373L, 1897734L, 1927624L,
1576325L, 1864730L, 1945731L, 2087827L, 1887315L, 2160582L, 1940982L,
1709248L, 1641578L, 1651927L, 1523934L, 1520967L, 1615398L, 1796216L,
1709032L, 1929426L, 1715439L, 2042236L, 1922861L, 1752159L, 2120420L,
2176839L, 2056198L, 1978545L, 1917299L, 2087989L, 2287761L, 2165191L,
1765206L, 2101615L, 1886381L, 1800709L, 1932399L, 1938540L, 1624589L,
1515305L, 1636013L, 1678695L, 1805357L, 1920032L, 1869846L, 1733193L,
1911342L, 1879654L, 1888584L, 1617636L, 1553308L, 1766792L, 1708563L,
1866907L, 1619900L, 2131880L, 1723028L, 1804310L, 1561439L, 1532808L,
1799613L, 1669279L, 1695310L, 1793913L, 1640780L, 1887981L, 1824651L,
1855067L, 1546987L, 1893343L, 1615038L, 1713931L, 1841102L, 1552554L,
1609310L, 1471592L, 1057635L, 1394483L, 1478938L, 1400849L, 1092941L,
1127207L, 1410334L, 985857L, 1502482L, 1282296L, 1131903L, 1134120L,
944486L, 1492348L, 1544775L, 1704514L, 1844986L, 1394241L, 1419907L,
1102451L, 1678210L, 1416968L, 1586854L, 1531570L, 1177832L, 1522673L,
1437979L, 1854961L, 1643220L, 1585851L, 1329138L, 1660453L, 1455096L,
1418685L, 1445048L, 1695041L, 1416778L, 1604949L, 1612551L, 1384783L,
1369524L, 1502777L, 1430407L, 1143067L, 1375883L, 1332591L, 1361450L,
1509461L, 1111893L, 1122316L, 1198293L, 1371022L, 1131783L, 1216891L,
1201775L, 1251192L, 1468533L, 971716L, 1080623L, 1243478L, 1380687L,
1252032L, 1685018L, 1186716L, 1110004L, 1448460L, 1142144L, 1108305L,
1262013L, 1177100L, 975649L, 1223630L, 872010L, 1291482L, 1130856L,
1565162L, 1453895L, 1346179L, 1511057L, 933375L, 1123020L, 1316433L,
1238018L, 1094457L, 1021577L, 1236134L, 1035408L, 1242196L, 1160343L,
1344770L, 1334352L, 1322612L, 1243856L, 1369881L, 1101393L, 1068661L,
1242855L, 1181028L, 1068507L, 1198975L, 1155222L, 1282687L, 1578816L,
1099740L, 1055999L, 1110803L, 1387463L, 1211896L, 1032759L, 1147958L,
1453268L, 1598432L, 1712287L, 1558504L, 1606923L, 1393160L, 1547333L,
1573425L, 1616768L, 1759911L, 1328534L, 1568315L, 1640106L, 1485560L,
1594149L, 2158755L, 1907469L, 1950358L, 1514739L, 1504915L, 1463329L,
1612138L, 1272940L, 1820813L, 1961831L, 2001256L, 1615047L, 1881349L,
1280810L, 1535983L, 1868300L, 1597679L, 1668499L, 1636259L, 1558331L,
1620336L, 1774802L, 2464370L, 2173591L, 2324146L, 1622177L, 2095321L,
2353130L, 1678844L, 1908599L, 2661533L, 1523244L, 2089773L, 2456706L,
2291627L, 2112795L, 2146462L, 1708033L, 2082991L, 1868523L, 1870792L,
2163766L, 1965448L, 1740014L, 2384685L, 1859087L, 2227283L, 2082207L,
2435339L, 2437914L, 1938528L, 1975502L, 1891861L, 2447307L, 1998905L,
1788622L, 2416936L, 2471771L, 2023577L, 2129774L, 2756620L, 1669619L,
1679035L, 1879060L, 1893919L, 1653989L, 1877943L, 1389847L, 2102051L,
1982800L, 1609563L, 1534998L, 2031637L, 2090443L, 2146740L, 2306902L,
1704790L, 2067778L, 1676170L, 1664308L, 1649995L, 1586174L, 1320104L,
1356841L, 1590752L, 1683829L, 1385250L, 1441666L, 1653611L, 1558089L,
1669990L, 1163876L, 1572544L, 1255959L, 1259118L, 1479161L, 1278374L,
1336800L, 1606781L, 1171646L, 851189L, 1385792L, 1344448L, 1115996L,
1366224L, 1340041L, 1527343L, 1408925L, 1454184L, 1424354L, 1381617L,
1558428L, 1353706L, 1386354L, 1189991L, 763254L, 1386571L), Var5 =
structure(c(1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 167L, 167L, 167L,
167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L,
167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L,
167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L,
167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L,
167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L,
167L, 167L, 167L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 167L, 1L, 1L, 167L, 167L, 167L, 167L, 167L,
167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L,
1L, 167L, 1L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L,
167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 103L, 120L, 115L, 118L, 92L,
102L, 108L, 97L, 113L, 128L, 117L, 122L, 125L, 126L, 139L, 131L,
112L, 129L, 127L, 79L, 116L, 124L, 104L, 98L, 93L, 82L, 100L,
123L, 83L, 88L, 66L, 68L, 101L, 41L, 81L, 119L, 99L, 84L, 47L,
85L, 31L, 38L, 58L, 46L, 90L, 43L, 48L, 91L, 32L, 54L, 78L, 87L,
53L, 33L, 55L, 57L, 13L, 69L, 10L, 77L, 52L, 161L, 153L, 145L,
147L, 9L, 155L, 138L, 156L, 130L, 136L, 148L, 145L, 15L, 4L,
163L, 3L, 72L, 109L, 11L, 63L, 39L, 110L, 107L, 105L, 50L, 71L,
73L, 51L, 60L, 67L, 70L, 61L, 37L, 75L, 18L, 95L, 5L, 45L, 7L,
34L, 44L, 24L, 164L, 56L, 2L, 40L, 78L, 30L, 36L, 65L, 166L,
17L, 22L, 62L, 19L, 16L, 162L, 25L, 14L, 159L, 8L, 12L, 149L,
6L, 158L, 165L, 157L, 143L, 150L, 146L, 142L, 144L, 152L, 96L,
80L, 26L, 140L, 114L, 154L, 42L, 28L, 76L, 29L, 160L, 86L, 89L,
121L, 27L, 59L, 64L, 49L, 135L, 151L, 111L, 74L, 134L, 35L, 106L,
94L, 23L, 21L, 20L, 137L, 132L, 133L, 141L), .Label = c("0",
"1004", "1017", "1025", "1029", "1032", "1034", "1052", "1093",
"1094", "1096", "1097", "1102", "1108", "1137", "1140", "1149",
"1151", "1156", "117", "121", "1216", "122", "1223", "1229",
"123", "124", "128", "131", "1332", "1336", "1356", "1371", "1376",
"138", "1381", "1428", "1446", "1448", "1473", "1478", "148",
"1502", "1506", "1516", "1536", "1551", "1556", "156", "1569",
"1582", "1598", "1610", "1659", "1686", "1703", "1731", "1733",
"174", "1741", "1750", "1755", "1783", "179", "1794", "1799",
"1800", "1805", "1816", "1819", "1874", "1876", "1879", "188",
"1883", "189", "1899", "1908", "1928", "195", "1963", "2005",
"2006", "2013", "2041", "206", "2074", "2076", "208", "2193",
"2201", "2260", "2294", "230", "2324", "233", "2373", "2388",
"2478", "2491", "2509", "2535", "2537", "2556", "2599", "260",
"2648", "2651", "2657", "2677", "268", "2836", "2844", "286",
"2942", "2944", "2990", "3006", "3086", "3124", "314", "3177",
"3230", "3287", "3355", "3390", "3441", "3455", "3469", "352",
"3533", "369", "376", "381", "384", "394", "397", "413", "4288",
"45", "451", "459", "478", "524", "529", "549", "569", "583",
"638", "643", "65", "650", "720", "74", "757", "761", "819",
"824", "839", "86", "861", "865", "881", "898", "924", "984",
"NULL"), class = "factor"), Var6 = structure(c(2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 2L,
2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), .Label = c("0",
"NULL"), class = "factor"), Var7 = c(0L, 0L, 0L, 0L, 0L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
0L, 0L, 0L, 0L, 0L, 509986L, 457334L, 423431L, 491864L, 291347L,
348557L, 415865L, 392211L, 335553L, 298921L, 383274L, 316046L,
373400L, 346341L, 389709L, 401434L, 409913L, 374109L, 412340L,
351334L, 318191L, 371280L, 362450L, 326204L, 365452L, 359126L,
353572L, 432630L, 308945L, 298529L, 314237L, 397880L, 339472L,
294810L, 328184L, 399537L, 400889L, 442645L, 407200L, 411296L,
367262L, 407430L, 417298L, 422888L, 453429L, 344743L, 408298L,
422106L, 363156L, 387085L, 536751L, 458335L, 478253L, 371372L,
355103L, 351371L, 396376L, 295519L, 447933L, 478155L, 486481L,
413681L, 467253L, 327260L, 386971L, 467136L, 411711L, 426864L,
369899L, 355287L, 365007L, 397091L, 561952L, 477049L, 513256L,
357575L, 470663L, 505672L, 340253L, 418327L, 582188L, 326354L,
427588L, 517728L, 485881L, 453163L, 463697L, 357187L, 449418L,
365614L, 372260L, 431281L, 390126L, 347262L, 472019L, 374950L,
440093L, 428659L, 494099L, 507023L, 395678L, 366396L, 358862L,
469883L, 369744L, 341466L, 461598L, 466752L, 381354L, 407277L,
527482L, 317936L, 314390L, 351180L, 351920L, 315951L, 354029L,
267565L, 399528L, 377080L, 310084L, 294011L, 385220L, 406141L,
412094L, 444809L, 321582L, 399966L, 326061L, 315964L, 323720L,
297622L, 244843L, 247743L, 306002L, 318435L, 266651L, 277154L,
314510L, 297403L, 319653L, 222712L, 301423L, 240825L, 244846L,
282070L, 241318L, 252998L, 310381L, 221844L, 159100L, 261463L,
256824L, 214117L, 263015L, 253039L, 289615L, 272659L, 279316L,
267969L, 262087L, 304361L, 258202L, 262335L, 221760L, 139884L,
260220L), Var8 = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L,
169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L,
169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L,
169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L,
169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L,
169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 169L,
1L, 1L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L,
169L, 169L, 169L, 169L, 169L, 169L, 169L, 1L, 169L, 1L, 169L,
169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L,
169L, 169L, 169L, 169L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 162L, 143L, 130L, 158L, 30L, 68L, 125L, 104L, 61L, 37L,
98L, 49L, 93L, 66L, 102L, 113L, 119L, 94L, 123L, 70L, 51L, 90L,
81L, 57L, 84L, 80L, 73L, 135L, 41L, 36L, 44L, 108L, 62L, 32L,
60L, 110L, 112L, 137L, 115L, 120L, 87L, 117L, 126L, 129L, 142L,
65L, 118L, 128L, 82L, 101L, 166L, 144L, 155L, 91L, 75L, 71L,
106L, 33L, 139L, 154L, 157L, 124L, 149L, 59L, 100L, 148L, 121L,
131L, 89L, 76L, 83L, 107L, 167L, 153L, 163L, 78L, 151L, 160L,
63L, 127L, 168L, 58L, 132L, 164L, 156L, 141L, 146L, 77L, 140L,
85L, 92L, 134L, 103L, 67L, 152L, 95L, 136L, 133L, 159L, 161L,
105L, 86L, 79L, 150L, 88L, 64L, 145L, 147L, 97L, 116L, 165L,
50L, 45L, 69L, 72L, 47L, 74L, 23L, 109L, 96L, 42L, 31L, 99L,
114L, 122L, 138L, 54L, 111L, 56L, 48L, 55L, 35L, 10L, 12L, 40L,
52L, 22L, 26L, 46L, 34L, 53L, 7L, 38L, 8L, 11L, 28L, 9L, 13L,
43L, 6L, 3L, 18L, 15L, 4L, 21L, 14L, 29L, 25L, 27L, 24L, 19L,
39L, 16L, 20L, 5L, 2L, 17L), .Label = c("0", "139884", "159100",
"214117", "221760", "221844", "222712", "240825", "241318", "244843",
"244846", "247743", "252998", "253039", "256824", "258202", "260220",
"261463", "262087", "262335", "263015", "266651", "267565", "267969",
"272659", "277154", "279316", "282070", "289615", "291347", "294011",
"294810", "295519", "297403", "297622", "298529", "298921", "301423",
"304361", "306002", "308945", "310084", "310381", "314237", "314390",
"314510", "315951", "315964", "316046", "317936", "318191", "318435",
"319653", "321582", "323720", "326061", "326204", "326354", "327260",
"328184", "335553", "339472", "340253", "341466", "344743", "346341",
"347262", "348557", "351180", "351334", "351371", "351920", "353572",
"354029", "355103", "355287", "357187", "357575", "358862", "359126",
"362450", "363156", "365007", "365452", "365614", "366396", "367262",
"369744", "369899", "371280", "371372", "372260", "373400", "374109",
"374950", "377080", "381354", "383274", "385220", "386971", "387085",
"389709", "390126", "392211", "395678", "396376", "397091", "397880",
"399528", "399537", "399966", "400889", "401434", "406141", "407200",
"407277", "407430", "408298", "409913", "411296", "411711", "412094",
"412340", "413681", "415865", "417298", "418327", "422106", "422888",
"423431", "426864", "427588", "428659", "431281", "432630", "440093",
"442645", "444809", "447933", "449418", "453163", "453429", "457334",
"458335", "461598", "463697", "466752", "467136", "467253", "469883",
"470663", "472019", "477049", "478155", "478253", "485881", "486481",
"491864", "494099", "505672", "507023", "509986", "513256", "517728",
"527482", "536751", "561952", "582188", "NULL"), class = "factor")), class
= "data.frame", row.names = c(NA,
-402L))
>
Any help and/or guidance will be greatly appreciated,

Cheers,

Paul

	[[alternative HTML version deleted]]


From @@r@h@go@|ee @end|ng |rom gm@||@com  Fri May 24 17:18:11 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Fri, 24 May 2019 11:18:11 -0400
Subject: [R] Subsetting Data from a Dataframe
In-Reply-To: <CAMOcQfP-ZL1o8-0L17gEent7wULvF2tOZ1njU4PAKGdO+FQJkA@mail.gmail.com>
References: <CAMOcQfP-ZL1o8-0L17gEent7wULvF2tOZ1njU4PAKGdO+FQJkA@mail.gmail.com>
Message-ID: <CAM_vjum7SkNFUcksNeUeBO-T=g_QXCX85yLa40VevRTe8uD_Dg@mail.gmail.com>

Hi Paul,

Thanks for the reproducible data. You really only need to provide
enough to illustrate your question, but this works.

I suspect you have a data import problem - I doubt you really want so
many columns to be factors! Probably you need to specify that NULL
means something specific, rather than being an ordinary string.

> str(datos)
'data.frame': 402 obs. of  9 variables:
 $ VarDate: Factor w/ 402 levels "1-Apr-00","1-Apr-01",..: 355 321 86
155 121 255 20 288 221 188 ...
 $ Var1   : int  150 140 148 157 105 132 123 139 128 174 ...
 $ Var2   : int  821273 625955 809990 729112 532151 725098 619868
704282 580952 975656 ...
 $ Var3   : int  1023833 924771 1021634 1043681 752874 947427 859879
999681 835358 1252862 ...
 $ Var4   : int  1842916 1650947 1826792 1868854 1349682 1700705
1553206 1789866 1481587 2260812 ...
 $ Var5   : Factor w/ 167 levels "0","1004","1017",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Var6   : Factor w/ 2 levels "0","NULL": 2 2 2 2 2 2 2 2 2 2 ...
 $ Var7   : int  0 0 0 0 0 0 0 0 0 0 ...
 $ Var8   : Factor w/ 169 levels "0","139884","159100",..: 1 1 1 1 1 1
1 1 1 1 ...

Regardless, you want the subset() command.

Here's one option, to give you the idea. Note that the values are in
quotes because your Var5 is a factor. If it isn't supposed to be, you
may want is.null() in the same way. I included NA because you asked,
but there aren't any NA values in Var5.

datos.new <- subset(datos, !(Var5 %in% c("0", "NULL")) & !is.na(Var5))

You can read the help for subset for more options, but basically you
need to construct something that returns a logical value.

Sarah

On Fri, May 24, 2019 at 9:11 AM Paul Bernal <paulbernal07 at gmail.com> wrote:
>
> Dear friends,
>
> Hope you are all doing well. I would like to know how to retrieve a
> complete dataframe (all the columns), except for the cases when one of the
> columns have either nulls or NAs.
>
> In this case, I?d like to retrieve all the columns but only the cases
> (rows) where Var5 has values different than 0, null or NAs.
>
> I am attaching the dput() of my dataset as a reference:
>
> > dput(datos)
> structure(list(VarDate = structure(c(355L, 321L, 86L, 155L, 121L,
> 255L, 20L, 288L, 221L, 188L, 53L, 389L, 356L, 322L, 87L, 156L,
> 122L, 256L, 21L, 289L, 222L, 189L, 54L, 390L, 357L, 323L, 88L,
> 157L, 123L, 257L, 22L, 290L, 223L, 190L, 55L, 391L, 358L, 324L,
> 89L, 158L, 124L, 258L, 23L, 291L, 224L, 191L, 56L, 392L, 359L,
> 325L, 90L, 159L, 125L, 259L, 24L, 292L, 225L, 192L, 57L, 393L,
> 360L, 326L, 91L, 160L, 126L, 260L, 25L, 293L, 226L, 193L, 58L,
> 394L, 361L, 327L, 92L, 161L, 127L, 261L, 26L, 294L, 227L, 194L,
> 59L, 395L, 362L, 328L, 93L, 162L, 128L, 262L, 27L, 295L, 228L,
> 195L, 60L, 396L, 363L, 329L, 94L, 163L, 129L, 263L, 28L, 296L,
> 229L, 196L, 61L, 397L, 364L, 330L, 95L, 164L, 130L, 264L, 29L,
> 297L, 230L, 197L, 62L, 398L, 365L, 331L, 96L, 165L, 131L, 265L,
> 30L, 298L, 231L, 198L, 63L, 399L, 366L, 332L, 97L, 166L, 132L,
> 266L, 31L, 299L, 232L, 199L, 64L, 400L, 367L, 333L, 98L, 167L,
> 133L, 267L, 32L, 300L, 233L, 200L, 65L, 401L, 368L, 334L, 99L,
> 168L, 134L, 268L, 33L, 301L, 234L, 201L, 66L, 402L, 369L, 335L,
> 100L, 135L, 101L, 235L, 1L, 269L, 202L, 169L, 34L, 370L, 336L,
> 302L, 67L, 136L, 102L, 236L, 2L, 270L, 203L, 170L, 35L, 371L,
> 337L, 303L, 68L, 137L, 103L, 237L, 3L, 271L, 204L, 171L, 36L,
> 372L, 338L, 304L, 69L, 138L, 104L, 238L, 4L, 272L, 205L, 172L,
> 37L, 373L, 339L, 305L, 70L, 139L, 105L, 239L, 5L, 273L, 206L,
> 173L, 38L, 374L, 340L, 306L, 71L, 140L, 106L, 240L, 6L, 274L,
> 207L, 174L, 39L, 375L, 341L, 307L, 72L, 141L, 107L, 241L, 7L,
> 275L, 208L, 175L, 40L, 376L, 342L, 308L, 73L, 142L, 108L, 242L,
> 8L, 276L, 209L, 176L, 41L, 377L, 343L, 309L, 74L, 143L, 109L,
> 243L, 9L, 277L, 210L, 177L, 42L, 378L, 344L, 310L, 75L, 144L,
> 110L, 244L, 10L, 278L, 211L, 178L, 43L, 379L, 345L, 311L, 76L,
> 145L, 111L, 245L, 11L, 279L, 212L, 179L, 44L, 380L, 346L, 312L,
> 77L, 146L, 112L, 246L, 12L, 280L, 213L, 180L, 45L, 381L, 347L,
> 313L, 78L, 147L, 113L, 247L, 13L, 281L, 214L, 181L, 46L, 382L,
> 348L, 314L, 79L, 148L, 114L, 248L, 14L, 282L, 215L, 182L, 47L,
> 383L, 349L, 315L, 80L, 149L, 115L, 249L, 15L, 283L, 216L, 183L,
> 48L, 384L, 350L, 316L, 81L, 150L, 116L, 250L, 16L, 284L, 217L,
> 184L, 49L, 385L, 351L, 317L, 82L, 151L, 117L, 251L, 17L, 285L,
> 218L, 185L, 50L, 386L, 352L, 318L, 83L, 152L, 118L, 252L, 18L,
> 286L, 219L, 186L, 51L, 387L, 353L, 319L, 84L, 153L, 119L, 253L,
> 19L, 287L, 220L, 187L, 52L, 388L, 354L, 320L, 85L, 154L, 120L,
> 254L), .Label = c("1-Apr-00", "1-Apr-01", "1-Apr-02", "1-Apr-03",
> "1-Apr-04", "1-Apr-05", "1-Apr-06", "1-Apr-07", "1-Apr-08", "1-Apr-09",
> "1-Apr-10", "1-Apr-11", "1-Apr-12", "1-Apr-13", "1-Apr-14", "1-Apr-15",
> "1-Apr-16", "1-Apr-17", "1-Apr-18", "1-Apr-86", "1-Apr-87", "1-Apr-88",
> "1-Apr-89", "1-Apr-90", "1-Apr-91", "1-Apr-92", "1-Apr-93", "1-Apr-94",
> "1-Apr-95", "1-Apr-96", "1-Apr-97", "1-Apr-98", "1-Apr-99", "1-Aug-00",
> "1-Aug-01", "1-Aug-02", "1-Aug-03", "1-Aug-04", "1-Aug-05", "1-Aug-06",
> "1-Aug-07", "1-Aug-08", "1-Aug-09", "1-Aug-10", "1-Aug-11", "1-Aug-12",
> "1-Aug-13", "1-Aug-14", "1-Aug-15", "1-Aug-16", "1-Aug-17", "1-Aug-18",
> "1-Aug-86", "1-Aug-87", "1-Aug-88", "1-Aug-89", "1-Aug-90", "1-Aug-91",
> "1-Aug-92", "1-Aug-93", "1-Aug-94", "1-Aug-95", "1-Aug-96", "1-Aug-97",
> "1-Aug-98", "1-Aug-99", "1-Dec-00", "1-Dec-01", "1-Dec-02", "1-Dec-03",
> "1-Dec-04", "1-Dec-05", "1-Dec-06", "1-Dec-07", "1-Dec-08", "1-Dec-09",
> "1-Dec-10", "1-Dec-11", "1-Dec-12", "1-Dec-13", "1-Dec-14", "1-Dec-15",
> "1-Dec-16", "1-Dec-17", "1-Dec-18", "1-Dec-85", "1-Dec-86", "1-Dec-87",
> "1-Dec-88", "1-Dec-89", "1-Dec-90", "1-Dec-91", "1-Dec-92", "1-Dec-93",
> "1-Dec-94", "1-Dec-95", "1-Dec-96", "1-Dec-97", "1-Dec-98", "1-Dec-99",
> "1-Feb-00", "1-Feb-01", "1-Feb-02", "1-Feb-03", "1-Feb-04", "1-Feb-05",
> "1-Feb-06", "1-Feb-07", "1-Feb-08", "1-Feb-09", "1-Feb-10", "1-Feb-11",
> "1-Feb-12", "1-Feb-13", "1-Feb-14", "1-Feb-15", "1-Feb-16", "1-Feb-17",
> "1-Feb-18", "1-Feb-19", "1-Feb-86", "1-Feb-87", "1-Feb-88", "1-Feb-89",
> "1-Feb-90", "1-Feb-91", "1-Feb-92", "1-Feb-93", "1-Feb-94", "1-Feb-95",
> "1-Feb-96", "1-Feb-97", "1-Feb-98", "1-Feb-99", "1-Jan-00", "1-Jan-01",
> "1-Jan-02", "1-Jan-03", "1-Jan-04", "1-Jan-05", "1-Jan-06", "1-Jan-07",
> "1-Jan-08", "1-Jan-09", "1-Jan-10", "1-Jan-11", "1-Jan-12", "1-Jan-13",
> "1-Jan-14", "1-Jan-15", "1-Jan-16", "1-Jan-17", "1-Jan-18", "1-Jan-19",
> "1-Jan-86", "1-Jan-87", "1-Jan-88", "1-Jan-89", "1-Jan-90", "1-Jan-91",
> "1-Jan-92", "1-Jan-93", "1-Jan-94", "1-Jan-95", "1-Jan-96", "1-Jan-97",
> "1-Jan-98", "1-Jan-99", "1-Jul-00", "1-Jul-01", "1-Jul-02", "1-Jul-03",
> "1-Jul-04", "1-Jul-05", "1-Jul-06", "1-Jul-07", "1-Jul-08", "1-Jul-09",
> "1-Jul-10", "1-Jul-11", "1-Jul-12", "1-Jul-13", "1-Jul-14", "1-Jul-15",
> "1-Jul-16", "1-Jul-17", "1-Jul-18", "1-Jul-86", "1-Jul-87", "1-Jul-88",
> "1-Jul-89", "1-Jul-90", "1-Jul-91", "1-Jul-92", "1-Jul-93", "1-Jul-94",
> "1-Jul-95", "1-Jul-96", "1-Jul-97", "1-Jul-98", "1-Jul-99", "1-Jun-00",
> "1-Jun-01", "1-Jun-02", "1-Jun-03", "1-Jun-04", "1-Jun-05", "1-Jun-06",
> "1-Jun-07", "1-Jun-08", "1-Jun-09", "1-Jun-10", "1-Jun-11", "1-Jun-12",
> "1-Jun-13", "1-Jun-14", "1-Jun-15", "1-Jun-16", "1-Jun-17", "1-Jun-18",
> "1-Jun-86", "1-Jun-87", "1-Jun-88", "1-Jun-89", "1-Jun-90", "1-Jun-91",
> "1-Jun-92", "1-Jun-93", "1-Jun-94", "1-Jun-95", "1-Jun-96", "1-Jun-97",
> "1-Jun-98", "1-Jun-99", "1-Mar-00", "1-Mar-01", "1-Mar-02", "1-Mar-03",
> "1-Mar-04", "1-Mar-05", "1-Mar-06", "1-Mar-07", "1-Mar-08", "1-Mar-09",
> "1-Mar-10", "1-Mar-11", "1-Mar-12", "1-Mar-13", "1-Mar-14", "1-Mar-15",
> "1-Mar-16", "1-Mar-17", "1-Mar-18", "1-Mar-19", "1-Mar-86", "1-Mar-87",
> "1-Mar-88", "1-Mar-89", "1-Mar-90", "1-Mar-91", "1-Mar-92", "1-Mar-93",
> "1-Mar-94", "1-Mar-95", "1-Mar-96", "1-Mar-97", "1-Mar-98", "1-Mar-99",
> "1-May-00", "1-May-01", "1-May-02", "1-May-03", "1-May-04", "1-May-05",
> "1-May-06", "1-May-07", "1-May-08", "1-May-09", "1-May-10", "1-May-11",
> "1-May-12", "1-May-13", "1-May-14", "1-May-15", "1-May-16", "1-May-17",
> "1-May-18", "1-May-86", "1-May-87", "1-May-88", "1-May-89", "1-May-90",
> "1-May-91", "1-May-92", "1-May-93", "1-May-94", "1-May-95", "1-May-96",
> "1-May-97", "1-May-98", "1-May-99", "1-Nov-00", "1-Nov-01", "1-Nov-02",
> "1-Nov-03", "1-Nov-04", "1-Nov-05", "1-Nov-06", "1-Nov-07", "1-Nov-08",
> "1-Nov-09", "1-Nov-10", "1-Nov-11", "1-Nov-12", "1-Nov-13", "1-Nov-14",
> "1-Nov-15", "1-Nov-16", "1-Nov-17", "1-Nov-18", "1-Nov-85", "1-Nov-86",
> "1-Nov-87", "1-Nov-88", "1-Nov-89", "1-Nov-90", "1-Nov-91", "1-Nov-92",
> "1-Nov-93", "1-Nov-94", "1-Nov-95", "1-Nov-96", "1-Nov-97", "1-Nov-98",
> "1-Nov-99", "1-Oct-00", "1-Oct-01", "1-Oct-02", "1-Oct-03", "1-Oct-04",
> "1-Oct-05", "1-Oct-06", "1-Oct-07", "1-Oct-08", "1-Oct-09", "1-Oct-10",
> "1-Oct-11", "1-Oct-12", "1-Oct-13", "1-Oct-14", "1-Oct-15", "1-Oct-16",
> "1-Oct-17", "1-Oct-18", "1-Oct-85", "1-Oct-86", "1-Oct-87", "1-Oct-88",
> "1-Oct-89", "1-Oct-90", "1-Oct-91", "1-Oct-92", "1-Oct-93", "1-Oct-94",
> "1-Oct-95", "1-Oct-96", "1-Oct-97", "1-Oct-98", "1-Oct-99", "1-Sep-00",
> "1-Sep-01", "1-Sep-02", "1-Sep-03", "1-Sep-04", "1-Sep-05", "1-Sep-06",
> "1-Sep-07", "1-Sep-08", "1-Sep-09", "1-Sep-10", "1-Sep-11", "1-Sep-12",
> "1-Sep-13", "1-Sep-14", "1-Sep-15", "1-Sep-16", "1-Sep-17", "1-Sep-18",
> "1-Sep-86", "1-Sep-87", "1-Sep-88", "1-Sep-89", "1-Sep-90", "1-Sep-91",
> "1-Sep-92", "1-Sep-93", "1-Sep-94", "1-Sep-95", "1-Sep-96", "1-Sep-97",
> "1-Sep-98", "1-Sep-99"), class = "factor"), Var1 = c(150L, 140L,
> 148L, 157L, 105L, 132L, 123L, 139L, 128L, 174L, 152L, 137L, 143L,
> 159L, 126L, 142L, 122L, 165L, 122L, 136L, 146L, 128L, 139L, 124L,
> 128L, 128L, 133L, 143L, 126L, 135L, 137L, 151L, 125L, 123L, 135L,
> 132L, 128L, 139L, 121L, 134L, 103L, 124L, 124L, 135L, 121L, 118L,
> 134L, 116L, 123L, 106L, 93L, 145L, 105L, 134L, 122L, 124L, 133L,
> 128L, 134L, 134L, 116L, 111L, 124L, 104L, 127L, 127L, 121L, 110L,
> 120L, 128L, 133L, 112L, 122L, 117L, 121L, 114L, 114L, 133L, 127L,
> 133L, 120L, 148L, 132L, 133L, 152L, 138L, 128L, 127L, 107L, 134L,
> 130L, 136L, 125L, 140L, 127L, 113L, 118L, 106L, 106L, 97L, 108L,
> 116L, 111L, 131L, 114L, 139L, 133L, 121L, 146L, 143L, 139L, 131L,
> 117L, 137L, 143L, 138L, 109L, 145L, 127L, 122L, 129L, 124L, 120L,
> 96L, 115L, 114L, 121L, 126L, 119L, 125L, 124L, 119L, 133L, 103L,
> 106L, 120L, 107L, 113L, 115L, 131L, 108L, 121L, 99L, 102L, 113L,
> 108L, 105L, 118L, 98L, 100L, 101L, 119L, 91L, 112L, 91L, 105L,
> 115L, 93L, 106L, 87L, 69L, 89L, 91L, 89L, 82L, 85L, 89L, 75L,
> 88L, 72L, 72L, 70L, 57L, 90L, 95L, 102L, 109L, 84L, 88L, 70L,
> 103L, 94L, 100L, 91L, 68L, 90L, 81L, 117L, 102L, 86L, 78L, 95L,
> 79L, 82L, 82L, 87L, 79L, 87L, 83L, 78L, 74L, 78L, 81L, 74L, 77L,
> 69L, 78L, 73L, 62L, 56L, 63L, 70L, 60L, 64L, 67L, 60L, 81L, 57L,
> 64L, 67L, 67L, 69L, 90L, 70L, 62L, 79L, 69L, 66L, 66L, 64L, 58L,
> 64L, 50L, 72L, 59L, 77L, 78L, 64L, 78L, 50L, 60L, 58L, 59L, 60L,
> 44L, 54L, 51L, 52L, 59L, 59L, 63L, 65L, 62L, 66L, 57L, 48L, 59L,
> 53L, 46L, 53L, 58L, 57L, 66L, 46L, 48L, 41L, 57L, 55L, 50L, 49L,
> 60L, 66L, 60L, 66L, 57L, 57L, 64L, 61L, 58L, 66L, 53L, 56L, 66L,
> 55L, 56L, 83L, 71L, 72L, 54L, 51L, 52L, 60L, 49L, 64L, 72L, 67L,
> 59L, 66L, 56L, 63L, 73L, 56L, 57L, 53L, 54L, 56L, 56L, 80L, 70L,
> 73L, 53L, 71L, 68L, 47L, 59L, 79L, 49L, 67L, 70L, 64L, 66L, 69L,
> 52L, 66L, 57L, 52L, 57L, 59L, 49L, 70L, 54L, 60L, 63L, 72L, 69L,
> 62L, 52L, 49L, 64L, 49L, 48L, 65L, 63L, 52L, 54L, 70L, 48L, 45L,
> 48L, 47L, 42L, 50L, 38L, 57L, 51L, 38L, 42L, 50L, 54L, 56L, 59L,
> 40L, 50L, 43L, 35L, 43L, 39L, 31L, 32L, 39L, 40L, 33L, 35L, 41L,
> 38L, 39L, 28L, 39L, 32L, 39L, 35L, 31L, 32L, 38L, 30L, 21L, 33L,
> 37L, 28L, 34L, 30L, 37L, 34L, 38L, 34L, 34L, 38L, 33L, 33L, 27L,
> 18L, 33L), Var2 = c(821273L, 625955L, 809990L, 729112L, 532151L,
> 725098L, 619868L, 704282L, 580952L, 975656L, 783082L, 678787L,
> 685767L, 784148L, 628750L, 696202L, 695546L, 907842L, 554405L,
> 770651L, 746794L, 738427L, 756031L, 677908L, 707298L, 617374L,
> 760057L, 842253L, 680889L, 731068L, 833567L, 882259L, 695913L,
> 741626L, 773759L, 746727L, 696405L, 714477L, 688004L, 767584L,
> 674190L, 730536L, 733525L, 857631L, 750634L, 678859L, 792624L,
> 594360L, 638948L, 695963L, 553269L, 975142L, 692757L, 902180L,
> 806432L, 881918L, 847507L, 820608L, 704557L, 790379L, 700958L,
> 702484L, 781250L, 611012L, 761236L, 815931L, 759334L, 647436L,
> 738675L, 755135L, 762259L, 686818L, 736825L, 779627L, 728106L,
> 626037L, 609227L, 710760L, 769409L, 829428L, 693645L, 860311L,
> 673870L, 633099L, 735438L, 618685L, 697920L, 695360L, 555367L,
> 655665L, 723109L, 772520L, 686184L, 791464L, 745570L, 659855L,
> 505423L, 628448L, 452854L, 537319L, 583909L, 638940L, 649767L,
> 699971L, 688656L, 794720L, 711849L, 610735L, 750121L, 856431L,
> 760421L, 766238L, 748331L, 751135L, 949228L, 925412L, 701986L,
> 836017L, 796566L, 683700L, 795191L, 708331L, 634132L, 496022L,
> 676314L, 668954L, 712050L, 739238L, 674401L, 668638L, 693023L,
> 702984L, 706872L, 595021L, 588132L, 549140L, 629381L, 623842L,
> 605154L, 762123L, 635163L, 596381L, 478583L, 527694L, 582280L,
> 576937L, 609620L, 514848L, 531708L, 559237L, 597501L, 542658L,
> 449916L, 541199L, 422740L, 520411L, 550272L, 470397L, 471954L,
> 400365L, 306711L, 445820L, 520121L, 492774L, 424645L, 384204L,
> 480749L, 320850L, 625051L, 459398L, 376164L, 400453L, 322230L,
> 503949L, 509943L, 578626L, 611236L, 459764L, 488844L, 421423L,
> 549084L, 505593L, 531538L, 535442L, 400745L, 524029L, 485794L,
> 629792L, 572611L, 542791L, 457679L, 599733L, 503070L, 529176L,
> 448977L, 543827L, 420837L, 534159L, 452072L, 380600L, 422340L,
> 456238L, 448117L, 349250L, 463418L, 451798L, 422773L, 469039L,
> 401945L, 371124L, 426345L, 512084L, 376136L, 338565L, 398287L,
> 348028L, 540228L, 370901L, 382410L, 380287L, 518778L, 434662L,
> 633628L, 383930L, 409503L, 449935L, 314311L, 309324L, 311242L,
> 390780L, 252049L, 348992L, 286589L, 413858L, 318478L, 434770L,
> 358908L, 337788L, 349755L, 244330L, 293098L, 378302L, 350259L,
> 267338L, 245683L, 337204L, 257568L, 306317L, 314226L, 349265L,
> 387763L, 383584L, 318554L, 402885L, 299765L, 295652L, 285204L,
> 350783L, 262768L, 347901L, 402663L, 289501L, 391287L, 295352L,
> 238815L, 330207L, 370494L, 262839L, 265180L, 323103L, 369609L,
> 327294L, 407061L, 352180L, 398966L, 272813L, 289250L, 249749L,
> 269799L, 253926L, 212019L, 299012L, 232329L, 232591L, 226118L,
> 359578L, 350889L, 333566L, 289996L, 314330L, 355278L, 343200L,
> 263987L, 360194L, 375162L, 449485L, 310118L, 385856L, 255153L,
> 289999L, 387187L, 293586L, 362148L, 268204L, 293395L, 312468L,
> 292847L, 343747L, 311347L, 387747L, 249736L, 338624L, 334526L,
> 228124L, 299618L, 367627L, 197001L, 273961L, 339628L, 305371L,
> 321103L, 307636L, 180838L, 300252L, 238870L, 296085L, 299428L,
> 292380L, 220003L, 333949L, 248405L, 264780L, 300803L, 336372L,
> 338379L, 257623L, 222979L, 226364L, 349364L, 260642L, 227835L,
> 237010L, 306306L, 253793L, 169667L, 333917L, 203583L, 204326L,
> 245043L, 200200L, 206647L, 266962L, 171125L, 227414L, 209053L,
> 132254L, 126972L, 159604L, 217526L, 186575L, 274024L, 155100L,
> 182481L, 198915L, 185028L, 165374L, 167406L, 114972L, 153905L,
> 131214L, 186645L, 133336L, 195207L, 168546L, 175644L, 210364L,
> 150943L, 142076L, 128045L, 147020L, 157380L, 110542L, 143323L,
> 169639L, 136196L, 94668L, 215810L, 170180L, 162091L, 139682L,
> 175133L, 153901L, 158420L, 127673L, 177445L, 156123L, 179980L,
> 142630L, 160726L, 181325L, 92592L, 191459L), Var3 = c(1023833L,
> 924771L, 1021634L, 1043681L, 752874L, 947427L, 859879L, 999681L,
> 835358L, 1252862L, 1047489L, 974229L, 1029854L, 1115937L, 868082L,
> 972236L, 850169L, 1143772L, 805427L, 965832L, 970293L, 884963L,
> 996303L, 891808L, 900303L, 835772L, 972515L, 1003546L, 852313L,
> 987354L, 1028777L, 1117341L, 893702L, 879153L, 1029602L, 982566L,
> 886107L, 997185L, 875060L, 964200L, 810422L, 862861L, 915637L,
> 1011660L, 870443L, 864038L, 1004598L, 792715L, 869050L, 792638L,
> 689028L, 1077470L, 831436L, 988032L, 923528L, 918479L, 1018139L,
> 990291L, 980459L, 1048576L, 863124L, 854886L, 909880L, 798497L,
> 938327L, 982216L, 859205L, 827323L, 841991L, 913290L, 983918L,
> 793665L, 895188L, 906126L, 863296L, 842577L, 832591L, 976840L,
> 932345L, 981143L, 801512L, 1002306L, 931473L, 902389L, 1067539L,
> 934557L, 871256L, 883418L, 718243L, 853101L, 892091L, 959493L,
> 875254L, 992752L, 891876L, 779707L, 761433L, 758197L, 702417L,
> 694400L, 743969L, 827773L, 788336L, 891501L, 784553L, 936542L,
> 892221L, 805682L, 986109L, 997737L, 952337L, 913596L, 881975L,
> 973655L, 1042842L, 991280L, 811610L, 969744L, 871970L, 832743L,
> 896535L, 898091L, 754328L, 708464L, 757153L, 777688L, 830975L,
> 891923L, 869821L, 803335L, 882650L, 866547L, 868705L, 747149L,
> 713600L, 762138L, 735163L, 800943L, 699566L, 913384L, 738171L,
> 782984L, 667440L, 658650L, 777300L, 722849L, 721270L, 720574L,
> 649446L, 751518L, 719568L, 747856L, 611773L, 753152L, 650260L,
> 686105L, 742319L, 611929L, 640700L, 591455L, 422915L, 551312L,
> 589984L, 552638L, 427190L, 450827L, 556519L, 395846L, 590983L,
> 512959L, 452685L, 449679L, 377117L, 592666L, 630967L, 677562L,
> 744774L, 559144L, 565164L, 431371L, 661244L, 562805L, 635075L,
> 611218L, 465263L, 600953L, 570639L, 741657L, 655390L, 624346L,
> 519072L, 655233L, 573364L, 555229L, 568871L, 673415L, 562549L,
> 632082L, 648735L, 547210L, 541646L, 591035L, 573215L, 453220L,
> 500569L, 483082L, 494882L, 556911L, 400361L, 408601L, 441426L,
> 505124L, 417482L, 421750L, 417484L, 431120L, 504564L, 333542L,
> 369482L, 431832L, 475688L, 437028L, 581278L, 408507L, 383338L,
> 502921L, 392616L, 381636L, 437582L, 402350L, 342748L, 420895L,
> 304549L, 452065L, 392927L, 525972L, 474072L, 439285L, 506800L,
> 301623L, 360435L, 431918L, 406583L, 348245L, 311187L, 397553L,
> 328374L, 386618L, 359652L, 410604L, 415530L, 423691L, 387479L,
> 427504L, 361311L, 328342L, 384716L, 375293L, 337292L, 378871L,
> 369858L, 368082L, 447470L, 319699L, 309456L, 324288L, 416815L,
> 350970L, 305021L, 340379L, 415899L, 415863L, 459164L, 424415L,
> 426744L, 381180L, 421914L, 432177L, 439146L, 471412L, 358047L,
> 424430L, 439595L, 378718L, 402841L, 558536L, 475445L, 496879L,
> 386381L, 371953L, 362575L, 410119L, 307376L, 464888L, 497654L,
> 505371L, 428729L, 484825L, 340312L, 403197L, 483014L, 425448L,
> 442173L, 382614L, 365801L, 377530L, 412724L, 582561L, 497182L,
> 534839L, 372528L, 490513L, 524295L, 353328L, 435740L, 606586L,
> 339952L, 446177L, 541098L, 505165L, 471455L, 483144L, 372510L,
> 468379L, 381000L, 388039L, 448045L, 406707L, 363432L, 492507L,
> 389963L, 458037L, 446202L, 520027L, 534002L, 411419L, 382377L,
> 374016L, 489217L, 385358L, 354344L, 480102L, 486111L, 396875L,
> 424706L, 549349L, 331376L, 327566L, 364968L, 365715L, 327275L,
> 368299L, 277930L, 416003L, 393085L, 322959L, 306650L, 401742L,
> 420621L, 429129L, 462222L, 335711L, 416480L, 338851L, 329130L,
> 338371L, 310220L, 255807L, 258337L, 319225L, 331696L, 276512L,
> 288625L, 329139L, 311016L, 332946L, 231066L, 315569L, 251276L,
> 255198L, 294448L, 251216L, 262122L, 322465L, 230834L, 164729L,
> 272468L, 267101L, 223642L, 273754L, 262820L, 299455L, 282714L,
> 290049L, 277035L, 270754L, 315434L, 267553L, 273266L, 231968L,
> 145726L, 270737L), Var4 = c(1842916L, 1650947L, 1826792L, 1868854L,
> 1349682L, 1700705L, 1553206L, 1789866L, 1481587L, 2260812L, 1875500L,
> 1757982L, 1844106L, 2015221L, 1559047L, 1747566L, 1547336L, 2068885L,
> 1434585L, 1754398L, 1751677L, 1605872L, 1791263L, 1606000L, 1622632L,
> 1501355L, 1751479L, 1813795L, 1534006L, 1763481L, 1862021L, 2011101L,
> 1579945L, 1582041L, 1845277L, 1774748L, 1604085L, 1798036L, 1581748L,
> 1747547L, 1474606L, 1553108L, 1662061L, 1832545L, 1555462L, 1560721L,
> 1803880L, 1421983L, 1705225L, 1570989L, 1370337L, 2130208L, 1648356L,
> 1967994L, 1840196L, 1814451L, 2018955L, 1953286L, 1943577L, 2068666L,
> 1709487L, 1707015L, 1816645L, 1592986L, 1852489L, 1947445L, 1705050L,
> 1626904L, 1661471L, 1810373L, 1935931L, 1564800L, 1770490L, 1793351L,
> 1704562L, 1657873L, 1624573L, 1891007L, 1842884L, 1940814L, 1598949L,
> 1995798L, 1844261L, 1783182L, 2300414L, 2035373L, 1897734L, 1927624L,
> 1576325L, 1864730L, 1945731L, 2087827L, 1887315L, 2160582L, 1940982L,
> 1709248L, 1641578L, 1651927L, 1523934L, 1520967L, 1615398L, 1796216L,
> 1709032L, 1929426L, 1715439L, 2042236L, 1922861L, 1752159L, 2120420L,
> 2176839L, 2056198L, 1978545L, 1917299L, 2087989L, 2287761L, 2165191L,
> 1765206L, 2101615L, 1886381L, 1800709L, 1932399L, 1938540L, 1624589L,
> 1515305L, 1636013L, 1678695L, 1805357L, 1920032L, 1869846L, 1733193L,
> 1911342L, 1879654L, 1888584L, 1617636L, 1553308L, 1766792L, 1708563L,
> 1866907L, 1619900L, 2131880L, 1723028L, 1804310L, 1561439L, 1532808L,
> 1799613L, 1669279L, 1695310L, 1793913L, 1640780L, 1887981L, 1824651L,
> 1855067L, 1546987L, 1893343L, 1615038L, 1713931L, 1841102L, 1552554L,
> 1609310L, 1471592L, 1057635L, 1394483L, 1478938L, 1400849L, 1092941L,
> 1127207L, 1410334L, 985857L, 1502482L, 1282296L, 1131903L, 1134120L,
> 944486L, 1492348L, 1544775L, 1704514L, 1844986L, 1394241L, 1419907L,
> 1102451L, 1678210L, 1416968L, 1586854L, 1531570L, 1177832L, 1522673L,
> 1437979L, 1854961L, 1643220L, 1585851L, 1329138L, 1660453L, 1455096L,
> 1418685L, 1445048L, 1695041L, 1416778L, 1604949L, 1612551L, 1384783L,
> 1369524L, 1502777L, 1430407L, 1143067L, 1375883L, 1332591L, 1361450L,
> 1509461L, 1111893L, 1122316L, 1198293L, 1371022L, 1131783L, 1216891L,
> 1201775L, 1251192L, 1468533L, 971716L, 1080623L, 1243478L, 1380687L,
> 1252032L, 1685018L, 1186716L, 1110004L, 1448460L, 1142144L, 1108305L,
> 1262013L, 1177100L, 975649L, 1223630L, 872010L, 1291482L, 1130856L,
> 1565162L, 1453895L, 1346179L, 1511057L, 933375L, 1123020L, 1316433L,
> 1238018L, 1094457L, 1021577L, 1236134L, 1035408L, 1242196L, 1160343L,
> 1344770L, 1334352L, 1322612L, 1243856L, 1369881L, 1101393L, 1068661L,
> 1242855L, 1181028L, 1068507L, 1198975L, 1155222L, 1282687L, 1578816L,
> 1099740L, 1055999L, 1110803L, 1387463L, 1211896L, 1032759L, 1147958L,
> 1453268L, 1598432L, 1712287L, 1558504L, 1606923L, 1393160L, 1547333L,
> 1573425L, 1616768L, 1759911L, 1328534L, 1568315L, 1640106L, 1485560L,
> 1594149L, 2158755L, 1907469L, 1950358L, 1514739L, 1504915L, 1463329L,
> 1612138L, 1272940L, 1820813L, 1961831L, 2001256L, 1615047L, 1881349L,
> 1280810L, 1535983L, 1868300L, 1597679L, 1668499L, 1636259L, 1558331L,
> 1620336L, 1774802L, 2464370L, 2173591L, 2324146L, 1622177L, 2095321L,
> 2353130L, 1678844L, 1908599L, 2661533L, 1523244L, 2089773L, 2456706L,
> 2291627L, 2112795L, 2146462L, 1708033L, 2082991L, 1868523L, 1870792L,
> 2163766L, 1965448L, 1740014L, 2384685L, 1859087L, 2227283L, 2082207L,
> 2435339L, 2437914L, 1938528L, 1975502L, 1891861L, 2447307L, 1998905L,
> 1788622L, 2416936L, 2471771L, 2023577L, 2129774L, 2756620L, 1669619L,
> 1679035L, 1879060L, 1893919L, 1653989L, 1877943L, 1389847L, 2102051L,
> 1982800L, 1609563L, 1534998L, 2031637L, 2090443L, 2146740L, 2306902L,
> 1704790L, 2067778L, 1676170L, 1664308L, 1649995L, 1586174L, 1320104L,
> 1356841L, 1590752L, 1683829L, 1385250L, 1441666L, 1653611L, 1558089L,
> 1669990L, 1163876L, 1572544L, 1255959L, 1259118L, 1479161L, 1278374L,
> 1336800L, 1606781L, 1171646L, 851189L, 1385792L, 1344448L, 1115996L,
> 1366224L, 1340041L, 1527343L, 1408925L, 1454184L, 1424354L, 1381617L,
> 1558428L, 1353706L, 1386354L, 1189991L, 763254L, 1386571L), Var5 =
> structure(c(1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 167L, 167L, 167L,
> 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L,
> 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L,
> 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L,
> 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L,
> 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L,
> 167L, 167L, 167L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 167L, 1L, 1L, 167L, 167L, 167L, 167L, 167L,
> 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L,
> 1L, 167L, 1L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L,
> 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 103L, 120L, 115L, 118L, 92L,
> 102L, 108L, 97L, 113L, 128L, 117L, 122L, 125L, 126L, 139L, 131L,
> 112L, 129L, 127L, 79L, 116L, 124L, 104L, 98L, 93L, 82L, 100L,
> 123L, 83L, 88L, 66L, 68L, 101L, 41L, 81L, 119L, 99L, 84L, 47L,
> 85L, 31L, 38L, 58L, 46L, 90L, 43L, 48L, 91L, 32L, 54L, 78L, 87L,
> 53L, 33L, 55L, 57L, 13L, 69L, 10L, 77L, 52L, 161L, 153L, 145L,
> 147L, 9L, 155L, 138L, 156L, 130L, 136L, 148L, 145L, 15L, 4L,
> 163L, 3L, 72L, 109L, 11L, 63L, 39L, 110L, 107L, 105L, 50L, 71L,
> 73L, 51L, 60L, 67L, 70L, 61L, 37L, 75L, 18L, 95L, 5L, 45L, 7L,
> 34L, 44L, 24L, 164L, 56L, 2L, 40L, 78L, 30L, 36L, 65L, 166L,
> 17L, 22L, 62L, 19L, 16L, 162L, 25L, 14L, 159L, 8L, 12L, 149L,
> 6L, 158L, 165L, 157L, 143L, 150L, 146L, 142L, 144L, 152L, 96L,
> 80L, 26L, 140L, 114L, 154L, 42L, 28L, 76L, 29L, 160L, 86L, 89L,
> 121L, 27L, 59L, 64L, 49L, 135L, 151L, 111L, 74L, 134L, 35L, 106L,
> 94L, 23L, 21L, 20L, 137L, 132L, 133L, 141L), .Label = c("0",
> "1004", "1017", "1025", "1029", "1032", "1034", "1052", "1093",
> "1094", "1096", "1097", "1102", "1108", "1137", "1140", "1149",
> "1151", "1156", "117", "121", "1216", "122", "1223", "1229",
> "123", "124", "128", "131", "1332", "1336", "1356", "1371", "1376",
> "138", "1381", "1428", "1446", "1448", "1473", "1478", "148",
> "1502", "1506", "1516", "1536", "1551", "1556", "156", "1569",
> "1582", "1598", "1610", "1659", "1686", "1703", "1731", "1733",
> "174", "1741", "1750", "1755", "1783", "179", "1794", "1799",
> "1800", "1805", "1816", "1819", "1874", "1876", "1879", "188",
> "1883", "189", "1899", "1908", "1928", "195", "1963", "2005",
> "2006", "2013", "2041", "206", "2074", "2076", "208", "2193",
> "2201", "2260", "2294", "230", "2324", "233", "2373", "2388",
> "2478", "2491", "2509", "2535", "2537", "2556", "2599", "260",
> "2648", "2651", "2657", "2677", "268", "2836", "2844", "286",
> "2942", "2944", "2990", "3006", "3086", "3124", "314", "3177",
> "3230", "3287", "3355", "3390", "3441", "3455", "3469", "352",
> "3533", "369", "376", "381", "384", "394", "397", "413", "4288",
> "45", "451", "459", "478", "524", "529", "549", "569", "583",
> "638", "643", "65", "650", "720", "74", "757", "761", "819",
> "824", "839", "86", "861", "865", "881", "898", "924", "984",
> "NULL"), class = "factor"), Var6 = structure(c(2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 2L,
> 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), .Label = c("0",
> "NULL"), class = "factor"), Var7 = c(0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 509986L, 457334L, 423431L, 491864L, 291347L,
> 348557L, 415865L, 392211L, 335553L, 298921L, 383274L, 316046L,
> 373400L, 346341L, 389709L, 401434L, 409913L, 374109L, 412340L,
> 351334L, 318191L, 371280L, 362450L, 326204L, 365452L, 359126L,
> 353572L, 432630L, 308945L, 298529L, 314237L, 397880L, 339472L,
> 294810L, 328184L, 399537L, 400889L, 442645L, 407200L, 411296L,
> 367262L, 407430L, 417298L, 422888L, 453429L, 344743L, 408298L,
> 422106L, 363156L, 387085L, 536751L, 458335L, 478253L, 371372L,
> 355103L, 351371L, 396376L, 295519L, 447933L, 478155L, 486481L,
> 413681L, 467253L, 327260L, 386971L, 467136L, 411711L, 426864L,
> 369899L, 355287L, 365007L, 397091L, 561952L, 477049L, 513256L,
> 357575L, 470663L, 505672L, 340253L, 418327L, 582188L, 326354L,
> 427588L, 517728L, 485881L, 453163L, 463697L, 357187L, 449418L,
> 365614L, 372260L, 431281L, 390126L, 347262L, 472019L, 374950L,
> 440093L, 428659L, 494099L, 507023L, 395678L, 366396L, 358862L,
> 469883L, 369744L, 341466L, 461598L, 466752L, 381354L, 407277L,
> 527482L, 317936L, 314390L, 351180L, 351920L, 315951L, 354029L,
> 267565L, 399528L, 377080L, 310084L, 294011L, 385220L, 406141L,
> 412094L, 444809L, 321582L, 399966L, 326061L, 315964L, 323720L,
> 297622L, 244843L, 247743L, 306002L, 318435L, 266651L, 277154L,
> 314510L, 297403L, 319653L, 222712L, 301423L, 240825L, 244846L,
> 282070L, 241318L, 252998L, 310381L, 221844L, 159100L, 261463L,
> 256824L, 214117L, 263015L, 253039L, 289615L, 272659L, 279316L,
> 267969L, 262087L, 304361L, 258202L, 262335L, 221760L, 139884L,
> 260220L), Var8 = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L,
> 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L,
> 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L,
> 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L,
> 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L,
> 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 169L,
> 1L, 1L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L,
> 169L, 169L, 169L, 169L, 169L, 169L, 169L, 1L, 169L, 1L, 169L,
> 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L,
> 169L, 169L, 169L, 169L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 162L, 143L, 130L, 158L, 30L, 68L, 125L, 104L, 61L, 37L,
> 98L, 49L, 93L, 66L, 102L, 113L, 119L, 94L, 123L, 70L, 51L, 90L,
> 81L, 57L, 84L, 80L, 73L, 135L, 41L, 36L, 44L, 108L, 62L, 32L,
> 60L, 110L, 112L, 137L, 115L, 120L, 87L, 117L, 126L, 129L, 142L,
> 65L, 118L, 128L, 82L, 101L, 166L, 144L, 155L, 91L, 75L, 71L,
> 106L, 33L, 139L, 154L, 157L, 124L, 149L, 59L, 100L, 148L, 121L,
> 131L, 89L, 76L, 83L, 107L, 167L, 153L, 163L, 78L, 151L, 160L,
> 63L, 127L, 168L, 58L, 132L, 164L, 156L, 141L, 146L, 77L, 140L,
> 85L, 92L, 134L, 103L, 67L, 152L, 95L, 136L, 133L, 159L, 161L,
> 105L, 86L, 79L, 150L, 88L, 64L, 145L, 147L, 97L, 116L, 165L,
> 50L, 45L, 69L, 72L, 47L, 74L, 23L, 109L, 96L, 42L, 31L, 99L,
> 114L, 122L, 138L, 54L, 111L, 56L, 48L, 55L, 35L, 10L, 12L, 40L,
> 52L, 22L, 26L, 46L, 34L, 53L, 7L, 38L, 8L, 11L, 28L, 9L, 13L,
> 43L, 6L, 3L, 18L, 15L, 4L, 21L, 14L, 29L, 25L, 27L, 24L, 19L,
> 39L, 16L, 20L, 5L, 2L, 17L), .Label = c("0", "139884", "159100",
> "214117", "221760", "221844", "222712", "240825", "241318", "244843",
> "244846", "247743", "252998", "253039", "256824", "258202", "260220",
> "261463", "262087", "262335", "263015", "266651", "267565", "267969",
> "272659", "277154", "279316", "282070", "289615", "291347", "294011",
> "294810", "295519", "297403", "297622", "298529", "298921", "301423",
> "304361", "306002", "308945", "310084", "310381", "314237", "314390",
> "314510", "315951", "315964", "316046", "317936", "318191", "318435",
> "319653", "321582", "323720", "326061", "326204", "326354", "327260",
> "328184", "335553", "339472", "340253", "341466", "344743", "346341",
> "347262", "348557", "351180", "351334", "351371", "351920", "353572",
> "354029", "355103", "355287", "357187", "357575", "358862", "359126",
> "362450", "363156", "365007", "365452", "365614", "366396", "367262",
> "369744", "369899", "371280", "371372", "372260", "373400", "374109",
> "374950", "377080", "381354", "383274", "385220", "386971", "387085",
> "389709", "390126", "392211", "395678", "396376", "397091", "397880",
> "399528", "399537", "399966", "400889", "401434", "406141", "407200",
> "407277", "407430", "408298", "409913", "411296", "411711", "412094",
> "412340", "413681", "415865", "417298", "418327", "422106", "422888",
> "423431", "426864", "427588", "428659", "431281", "432630", "440093",
> "442645", "444809", "447933", "449418", "453163", "453429", "457334",
> "458335", "461598", "463697", "466752", "467136", "467253", "469883",
> "470663", "472019", "477049", "478155", "478253", "485881", "486481",
> "491864", "494099", "505672", "507023", "509986", "513256", "517728",
> "527482", "536751", "561952", "582188", "NULL"), class = "factor")), class
> = "data.frame", row.names = c(NA,
> -402L))
> >
> Any help and/or guidance will be greatly appreciated,
>
> Cheers,
>
> Paul
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From re@dyto|e@rn90 @end|ng |rom gm@||@com  Fri May 24 10:43:25 2019
From: re@dyto|e@rn90 @end|ng |rom gm@||@com (Ready Learner)
Date: Fri, 24 May 2019 10:43:25 +0200
Subject: [R] Generating nested models for order selection tests
Message-ID: <CAAOtLrjwztmkQpZFP1kDVgZebafEZAROeA8fhLA4+X95fJsB1Q@mail.gmail.com>

Hello everyone,

I have created a parametric additive model for the median house price (as
the response) and with the number of tax forms (x1) and the number of
healthcare facilities (x2) as my covariates. I should mention that both of
the covariates have quadratic effects in my model.

Now I want to do a hypothesis testing. I am taking the mentioned parametric
model as my null state (hypothesis) and I want to use "order selection
test" to test it against a nonparametric alternative hypothesis. Based on
what I understood from few related articles I have read, I should create a
sequence of nested models. I am thinking about using polynomial or cosine
functions as my basis function. In either case, I have to create a series
of models (i.e. the sequence of nested models via series expansion) based
on the basis function to test the hypothesis.
Is there any way to do this automatically in R?

Kind regards,
readyToLearn

	[[alternative HTML version deleted]]


From GC@rden@@2 @end|ng |rom med@m|@m|@edu  Fri May 24 16:54:46 2019
From: GC@rden@@2 @end|ng |rom med@m|@m|@edu (Cardenas, Gabriel A)
Date: Fri, 24 May 2019 14:54:46 +0000
Subject: [R] 
 mboost: Proportional odds boosting model - how to specify the
 offset?
Message-ID: <SN6PR07MB5038FFBE56E189EFDAA2506EDB020@SN6PR07MB5038.namprd07.prod.outlook.com>

I keep getting this error

in base::rowSums(x, na.rm = na.rm, dims = dims, ...) :
  'x' must be numeric
In addition: Warning message:

When I try to run confint on the opt object can you help please thnx

	[[alternative HTML version deleted]]


From |br@h|mmyr|@m7 @end|ng |rom gm@||@com  Thu May 23 03:21:21 2019
From: |br@h|mmyr|@m7 @end|ng |rom gm@||@com (Myriam Ibrahim)
Date: Wed, 22 May 2019 21:21:21 -0400
Subject: [R] [R-pkgs] Release Announcement : Riex Package
In-Reply-To: <CAKzWEHQFakehnf-WhkeO-Lrd-caXtzWvDzSkruTBiUqUhn6N_w@mail.gmail.com>
References: <CAKzWEHQFakehnf-WhkeO-Lrd-caXtzWvDzSkruTBiUqUhn6N_w@mail.gmail.com>
Message-ID: <CAKzWEHT3H+eHvTb8bY8KP3UeWkx4=TMkD3RUvefwcz=eU9wkeA@mail.gmail.com>

Hi,

Please find below the links related to 'Riex' package release announcement.
If you have any questions or need additional information,  let me know.

   - Release Announcement *-  *https://myriamibrahim.com/portfolio/riex/
   - CRAN  - https://cran.r-project.org/web/packages/Riex/index.html
   - Github - https://github.com/TheEliteAnalyst/Riex

Thank you for all your efforts and support to R Community !

Best regards,
Myriam Ibrahim


On Fri, May 17, 2019 at 12:32 PM Myriam Ibrahim <ibrahimmyriam7 at gmail.com>
wrote:

> Hi,
>
> Please find attached the .md  and .html files for 'Riex' release
> announcement.
>
> I'm hoping this is the designated email for that purpose. If you believe
> this is incorrect, please advise who should I reach out to.
>
> Thank you in advance for your help and support. If you have any questions
> or need additional details about this request, let me know.
>
> Best regards,
> Myriam
>

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From bgunter@4567 @end|ng |rom gm@||@com  Fri May 24 18:48:48 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 24 May 2019 09:48:48 -0700
Subject: [R] Generating nested models for order selection tests
In-Reply-To: <CAAOtLrjwztmkQpZFP1kDVgZebafEZAROeA8fhLA4+X95fJsB1Q@mail.gmail.com>
References: <CAAOtLrjwztmkQpZFP1kDVgZebafEZAROeA8fhLA4+X95fJsB1Q@mail.gmail.com>
Message-ID: <CAGxFJbRPFnwBOVNcDdYNR=uQ=E2phvXwH8cG02ReM=42Y5KQpg@mail.gmail.com>

Purely statistical questions are generally off topic here, and your query
may fall under that rubric. But you should try searching at rseek.org and R
task views -- https://cran.r-project.org/web/views/  -- perhaps under the
SocialScience heading or others that may use the methodology to which you
refer.

Cheers,
Bert

On Fri, May 24, 2019 at 8:48 AM Ready Learner <readytolearn90 at gmail.com>
wrote:

> Hello everyone,
>
> I have created a parametric additive model for the median house price (as
> the response) and with the number of tax forms (x1) and the number of
> healthcare facilities (x2) as my covariates. I should mention that both of
> the covariates have quadratic effects in my model.
>
> Now I want to do a hypothesis testing. I am taking the mentioned parametric
> model as my null state (hypothesis) and I want to use "order selection
> test" to test it against a nonparametric alternative hypothesis. Based on
> what I understood from few related articles I have read, I should create a
> sequence of nested models. I am thinking about using polynomial or cosine
> functions as my basis function. In either case, I have to create a series
> of models (i.e. the sequence of nested models via series expansion) based
> on the basis function to test the hypothesis.
> Is there any way to do this automatically in R?
>
> Kind regards,
> readyToLearn
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Fri May 24 18:50:49 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 24 May 2019 09:50:49 -0700
Subject: [R] 
 mboost: Proportional odds boosting model - how to specify the
 offset?
In-Reply-To: <SN6PR07MB5038FFBE56E189EFDAA2506EDB020@SN6PR07MB5038.namprd07.prod.outlook.com>
References: <SN6PR07MB5038FFBE56E189EFDAA2506EDB020@SN6PR07MB5038.namprd07.prod.outlook.com>
Message-ID: <CAGxFJbQhTiVgDxxn5DyMqfCjNJcUnC5+A7e_VcpJ4_gMM8hz1A@mail.gmail.com>

I would have thought the message was obvious. Follow the posting guide:
show us a subset of your data.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, May 24, 2019 at 9:26 AM Cardenas, Gabriel A <
GCardenas2 at med.miami.edu> wrote:

> I keep getting this error
>
> in base::rowSums(x, na.rm = na.rm, dims = dims, ...) :
>   'x' must be numeric
> In addition: Warning message:
>
> When I try to run confint on the opt object can you help please thnx
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Fri May 24 18:57:31 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 24 May 2019 09:57:31 -0700
Subject: [R] 
 mboost: Proportional odds boosting model - how to specify the
 offset?
In-Reply-To: <CAGxFJbQhTiVgDxxn5DyMqfCjNJcUnC5+A7e_VcpJ4_gMM8hz1A@mail.gmail.com>
References: <SN6PR07MB5038FFBE56E189EFDAA2506EDB020@SN6PR07MB5038.namprd07.prod.outlook.com>
 <CAGxFJbQhTiVgDxxn5DyMqfCjNJcUnC5+A7e_VcpJ4_gMM8hz1A@mail.gmail.com>
Message-ID: <CAGxFJbQFJbJB84YcesHiD_gAw+5x2kM-zxQjNmDoWTBOYZAJtg@mail.gmail.com>

... and I should have said, show us your code. We've no idea what you tried
to do (or at least I don't).

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, May 24, 2019 at 9:50 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> I would have thought the message was obvious. Follow the posting guide:
> show us a subset of your data.
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Fri, May 24, 2019 at 9:26 AM Cardenas, Gabriel A <
> GCardenas2 at med.miami.edu> wrote:
>
>> I keep getting this error
>>
>> in base::rowSums(x, na.rm = na.rm, dims = dims, ...) :
>>   'x' must be numeric
>> In addition: Warning message:
>>
>> When I try to run confint on the opt object can you help please thnx
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri May 24 19:07:06 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 24 May 2019 18:07:06 +0100
Subject: [R] Subsetting Data from a Dataframe
In-Reply-To: <CAMOcQfP-ZL1o8-0L17gEent7wULvF2tOZ1njU4PAKGdO+FQJkA@mail.gmail.com>
References: <CAMOcQfP-ZL1o8-0L17gEent7wULvF2tOZ1njU4PAKGdO+FQJkA@mail.gmail.com>
Message-ID: <0a179e6c-f86a-4cba-a2c3-44cb6f6268b0@sapo.pt>

Hello,

Maybe something like the following is what you want.
The code first creates a logical index of columns with at least one NA 
or "NULL" (character string, not NULL) values. Then extracts only those 
columns from the dataframe.

inx <- sapply(datos, function(x) any(x == "NULL" | is.na(x)))
datos2 <- datos[inx]


Hope this helps,

Rui Barradas


?s 14:10 de 24/05/19, Paul Bernal escreveu:
> Dear friends,
> 
> Hope you are all doing well. I would like to know how to retrieve a
> complete dataframe (all the columns), except for the cases when one of the
> columns have either nulls or NAs.
> 
> In this case, I?d like to retrieve all the columns but only the cases
> (rows) where Var5 has values different than 0, null or NAs.
> 
> I am attaching the dput() of my dataset as a reference:
> 
>> dput(datos)
> structure(list(VarDate = structure(c(355L, 321L, 86L, 155L, 121L,
> 255L, 20L, 288L, 221L, 188L, 53L, 389L, 356L, 322L, 87L, 156L,
> 122L, 256L, 21L, 289L, 222L, 189L, 54L, 390L, 357L, 323L, 88L,
> 157L, 123L, 257L, 22L, 290L, 223L, 190L, 55L, 391L, 358L, 324L,
> 89L, 158L, 124L, 258L, 23L, 291L, 224L, 191L, 56L, 392L, 359L,
> 325L, 90L, 159L, 125L, 259L, 24L, 292L, 225L, 192L, 57L, 393L,
> 360L, 326L, 91L, 160L, 126L, 260L, 25L, 293L, 226L, 193L, 58L,
> 394L, 361L, 327L, 92L, 161L, 127L, 261L, 26L, 294L, 227L, 194L,
> 59L, 395L, 362L, 328L, 93L, 162L, 128L, 262L, 27L, 295L, 228L,
> 195L, 60L, 396L, 363L, 329L, 94L, 163L, 129L, 263L, 28L, 296L,
> 229L, 196L, 61L, 397L, 364L, 330L, 95L, 164L, 130L, 264L, 29L,
> 297L, 230L, 197L, 62L, 398L, 365L, 331L, 96L, 165L, 131L, 265L,
> 30L, 298L, 231L, 198L, 63L, 399L, 366L, 332L, 97L, 166L, 132L,
> 266L, 31L, 299L, 232L, 199L, 64L, 400L, 367L, 333L, 98L, 167L,
> 133L, 267L, 32L, 300L, 233L, 200L, 65L, 401L, 368L, 334L, 99L,
> 168L, 134L, 268L, 33L, 301L, 234L, 201L, 66L, 402L, 369L, 335L,
> 100L, 135L, 101L, 235L, 1L, 269L, 202L, 169L, 34L, 370L, 336L,
> 302L, 67L, 136L, 102L, 236L, 2L, 270L, 203L, 170L, 35L, 371L,
> 337L, 303L, 68L, 137L, 103L, 237L, 3L, 271L, 204L, 171L, 36L,
> 372L, 338L, 304L, 69L, 138L, 104L, 238L, 4L, 272L, 205L, 172L,
> 37L, 373L, 339L, 305L, 70L, 139L, 105L, 239L, 5L, 273L, 206L,
> 173L, 38L, 374L, 340L, 306L, 71L, 140L, 106L, 240L, 6L, 274L,
> 207L, 174L, 39L, 375L, 341L, 307L, 72L, 141L, 107L, 241L, 7L,
> 275L, 208L, 175L, 40L, 376L, 342L, 308L, 73L, 142L, 108L, 242L,
> 8L, 276L, 209L, 176L, 41L, 377L, 343L, 309L, 74L, 143L, 109L,
> 243L, 9L, 277L, 210L, 177L, 42L, 378L, 344L, 310L, 75L, 144L,
> 110L, 244L, 10L, 278L, 211L, 178L, 43L, 379L, 345L, 311L, 76L,
> 145L, 111L, 245L, 11L, 279L, 212L, 179L, 44L, 380L, 346L, 312L,
> 77L, 146L, 112L, 246L, 12L, 280L, 213L, 180L, 45L, 381L, 347L,
> 313L, 78L, 147L, 113L, 247L, 13L, 281L, 214L, 181L, 46L, 382L,
> 348L, 314L, 79L, 148L, 114L, 248L, 14L, 282L, 215L, 182L, 47L,
> 383L, 349L, 315L, 80L, 149L, 115L, 249L, 15L, 283L, 216L, 183L,
> 48L, 384L, 350L, 316L, 81L, 150L, 116L, 250L, 16L, 284L, 217L,
> 184L, 49L, 385L, 351L, 317L, 82L, 151L, 117L, 251L, 17L, 285L,
> 218L, 185L, 50L, 386L, 352L, 318L, 83L, 152L, 118L, 252L, 18L,
> 286L, 219L, 186L, 51L, 387L, 353L, 319L, 84L, 153L, 119L, 253L,
> 19L, 287L, 220L, 187L, 52L, 388L, 354L, 320L, 85L, 154L, 120L,
> 254L), .Label = c("1-Apr-00", "1-Apr-01", "1-Apr-02", "1-Apr-03",
> "1-Apr-04", "1-Apr-05", "1-Apr-06", "1-Apr-07", "1-Apr-08", "1-Apr-09",
> "1-Apr-10", "1-Apr-11", "1-Apr-12", "1-Apr-13", "1-Apr-14", "1-Apr-15",
> "1-Apr-16", "1-Apr-17", "1-Apr-18", "1-Apr-86", "1-Apr-87", "1-Apr-88",
> "1-Apr-89", "1-Apr-90", "1-Apr-91", "1-Apr-92", "1-Apr-93", "1-Apr-94",
> "1-Apr-95", "1-Apr-96", "1-Apr-97", "1-Apr-98", "1-Apr-99", "1-Aug-00",
> "1-Aug-01", "1-Aug-02", "1-Aug-03", "1-Aug-04", "1-Aug-05", "1-Aug-06",
> "1-Aug-07", "1-Aug-08", "1-Aug-09", "1-Aug-10", "1-Aug-11", "1-Aug-12",
> "1-Aug-13", "1-Aug-14", "1-Aug-15", "1-Aug-16", "1-Aug-17", "1-Aug-18",
> "1-Aug-86", "1-Aug-87", "1-Aug-88", "1-Aug-89", "1-Aug-90", "1-Aug-91",
> "1-Aug-92", "1-Aug-93", "1-Aug-94", "1-Aug-95", "1-Aug-96", "1-Aug-97",
> "1-Aug-98", "1-Aug-99", "1-Dec-00", "1-Dec-01", "1-Dec-02", "1-Dec-03",
> "1-Dec-04", "1-Dec-05", "1-Dec-06", "1-Dec-07", "1-Dec-08", "1-Dec-09",
> "1-Dec-10", "1-Dec-11", "1-Dec-12", "1-Dec-13", "1-Dec-14", "1-Dec-15",
> "1-Dec-16", "1-Dec-17", "1-Dec-18", "1-Dec-85", "1-Dec-86", "1-Dec-87",
> "1-Dec-88", "1-Dec-89", "1-Dec-90", "1-Dec-91", "1-Dec-92", "1-Dec-93",
> "1-Dec-94", "1-Dec-95", "1-Dec-96", "1-Dec-97", "1-Dec-98", "1-Dec-99",
> "1-Feb-00", "1-Feb-01", "1-Feb-02", "1-Feb-03", "1-Feb-04", "1-Feb-05",
> "1-Feb-06", "1-Feb-07", "1-Feb-08", "1-Feb-09", "1-Feb-10", "1-Feb-11",
> "1-Feb-12", "1-Feb-13", "1-Feb-14", "1-Feb-15", "1-Feb-16", "1-Feb-17",
> "1-Feb-18", "1-Feb-19", "1-Feb-86", "1-Feb-87", "1-Feb-88", "1-Feb-89",
> "1-Feb-90", "1-Feb-91", "1-Feb-92", "1-Feb-93", "1-Feb-94", "1-Feb-95",
> "1-Feb-96", "1-Feb-97", "1-Feb-98", "1-Feb-99", "1-Jan-00", "1-Jan-01",
> "1-Jan-02", "1-Jan-03", "1-Jan-04", "1-Jan-05", "1-Jan-06", "1-Jan-07",
> "1-Jan-08", "1-Jan-09", "1-Jan-10", "1-Jan-11", "1-Jan-12", "1-Jan-13",
> "1-Jan-14", "1-Jan-15", "1-Jan-16", "1-Jan-17", "1-Jan-18", "1-Jan-19",
> "1-Jan-86", "1-Jan-87", "1-Jan-88", "1-Jan-89", "1-Jan-90", "1-Jan-91",
> "1-Jan-92", "1-Jan-93", "1-Jan-94", "1-Jan-95", "1-Jan-96", "1-Jan-97",
> "1-Jan-98", "1-Jan-99", "1-Jul-00", "1-Jul-01", "1-Jul-02", "1-Jul-03",
> "1-Jul-04", "1-Jul-05", "1-Jul-06", "1-Jul-07", "1-Jul-08", "1-Jul-09",
> "1-Jul-10", "1-Jul-11", "1-Jul-12", "1-Jul-13", "1-Jul-14", "1-Jul-15",
> "1-Jul-16", "1-Jul-17", "1-Jul-18", "1-Jul-86", "1-Jul-87", "1-Jul-88",
> "1-Jul-89", "1-Jul-90", "1-Jul-91", "1-Jul-92", "1-Jul-93", "1-Jul-94",
> "1-Jul-95", "1-Jul-96", "1-Jul-97", "1-Jul-98", "1-Jul-99", "1-Jun-00",
> "1-Jun-01", "1-Jun-02", "1-Jun-03", "1-Jun-04", "1-Jun-05", "1-Jun-06",
> "1-Jun-07", "1-Jun-08", "1-Jun-09", "1-Jun-10", "1-Jun-11", "1-Jun-12",
> "1-Jun-13", "1-Jun-14", "1-Jun-15", "1-Jun-16", "1-Jun-17", "1-Jun-18",
> "1-Jun-86", "1-Jun-87", "1-Jun-88", "1-Jun-89", "1-Jun-90", "1-Jun-91",
> "1-Jun-92", "1-Jun-93", "1-Jun-94", "1-Jun-95", "1-Jun-96", "1-Jun-97",
> "1-Jun-98", "1-Jun-99", "1-Mar-00", "1-Mar-01", "1-Mar-02", "1-Mar-03",
> "1-Mar-04", "1-Mar-05", "1-Mar-06", "1-Mar-07", "1-Mar-08", "1-Mar-09",
> "1-Mar-10", "1-Mar-11", "1-Mar-12", "1-Mar-13", "1-Mar-14", "1-Mar-15",
> "1-Mar-16", "1-Mar-17", "1-Mar-18", "1-Mar-19", "1-Mar-86", "1-Mar-87",
> "1-Mar-88", "1-Mar-89", "1-Mar-90", "1-Mar-91", "1-Mar-92", "1-Mar-93",
> "1-Mar-94", "1-Mar-95", "1-Mar-96", "1-Mar-97", "1-Mar-98", "1-Mar-99",
> "1-May-00", "1-May-01", "1-May-02", "1-May-03", "1-May-04", "1-May-05",
> "1-May-06", "1-May-07", "1-May-08", "1-May-09", "1-May-10", "1-May-11",
> "1-May-12", "1-May-13", "1-May-14", "1-May-15", "1-May-16", "1-May-17",
> "1-May-18", "1-May-86", "1-May-87", "1-May-88", "1-May-89", "1-May-90",
> "1-May-91", "1-May-92", "1-May-93", "1-May-94", "1-May-95", "1-May-96",
> "1-May-97", "1-May-98", "1-May-99", "1-Nov-00", "1-Nov-01", "1-Nov-02",
> "1-Nov-03", "1-Nov-04", "1-Nov-05", "1-Nov-06", "1-Nov-07", "1-Nov-08",
> "1-Nov-09", "1-Nov-10", "1-Nov-11", "1-Nov-12", "1-Nov-13", "1-Nov-14",
> "1-Nov-15", "1-Nov-16", "1-Nov-17", "1-Nov-18", "1-Nov-85", "1-Nov-86",
> "1-Nov-87", "1-Nov-88", "1-Nov-89", "1-Nov-90", "1-Nov-91", "1-Nov-92",
> "1-Nov-93", "1-Nov-94", "1-Nov-95", "1-Nov-96", "1-Nov-97", "1-Nov-98",
> "1-Nov-99", "1-Oct-00", "1-Oct-01", "1-Oct-02", "1-Oct-03", "1-Oct-04",
> "1-Oct-05", "1-Oct-06", "1-Oct-07", "1-Oct-08", "1-Oct-09", "1-Oct-10",
> "1-Oct-11", "1-Oct-12", "1-Oct-13", "1-Oct-14", "1-Oct-15", "1-Oct-16",
> "1-Oct-17", "1-Oct-18", "1-Oct-85", "1-Oct-86", "1-Oct-87", "1-Oct-88",
> "1-Oct-89", "1-Oct-90", "1-Oct-91", "1-Oct-92", "1-Oct-93", "1-Oct-94",
> "1-Oct-95", "1-Oct-96", "1-Oct-97", "1-Oct-98", "1-Oct-99", "1-Sep-00",
> "1-Sep-01", "1-Sep-02", "1-Sep-03", "1-Sep-04", "1-Sep-05", "1-Sep-06",
> "1-Sep-07", "1-Sep-08", "1-Sep-09", "1-Sep-10", "1-Sep-11", "1-Sep-12",
> "1-Sep-13", "1-Sep-14", "1-Sep-15", "1-Sep-16", "1-Sep-17", "1-Sep-18",
> "1-Sep-86", "1-Sep-87", "1-Sep-88", "1-Sep-89", "1-Sep-90", "1-Sep-91",
> "1-Sep-92", "1-Sep-93", "1-Sep-94", "1-Sep-95", "1-Sep-96", "1-Sep-97",
> "1-Sep-98", "1-Sep-99"), class = "factor"), Var1 = c(150L, 140L,
> 148L, 157L, 105L, 132L, 123L, 139L, 128L, 174L, 152L, 137L, 143L,
> 159L, 126L, 142L, 122L, 165L, 122L, 136L, 146L, 128L, 139L, 124L,
> 128L, 128L, 133L, 143L, 126L, 135L, 137L, 151L, 125L, 123L, 135L,
> 132L, 128L, 139L, 121L, 134L, 103L, 124L, 124L, 135L, 121L, 118L,
> 134L, 116L, 123L, 106L, 93L, 145L, 105L, 134L, 122L, 124L, 133L,
> 128L, 134L, 134L, 116L, 111L, 124L, 104L, 127L, 127L, 121L, 110L,
> 120L, 128L, 133L, 112L, 122L, 117L, 121L, 114L, 114L, 133L, 127L,
> 133L, 120L, 148L, 132L, 133L, 152L, 138L, 128L, 127L, 107L, 134L,
> 130L, 136L, 125L, 140L, 127L, 113L, 118L, 106L, 106L, 97L, 108L,
> 116L, 111L, 131L, 114L, 139L, 133L, 121L, 146L, 143L, 139L, 131L,
> 117L, 137L, 143L, 138L, 109L, 145L, 127L, 122L, 129L, 124L, 120L,
> 96L, 115L, 114L, 121L, 126L, 119L, 125L, 124L, 119L, 133L, 103L,
> 106L, 120L, 107L, 113L, 115L, 131L, 108L, 121L, 99L, 102L, 113L,
> 108L, 105L, 118L, 98L, 100L, 101L, 119L, 91L, 112L, 91L, 105L,
> 115L, 93L, 106L, 87L, 69L, 89L, 91L, 89L, 82L, 85L, 89L, 75L,
> 88L, 72L, 72L, 70L, 57L, 90L, 95L, 102L, 109L, 84L, 88L, 70L,
> 103L, 94L, 100L, 91L, 68L, 90L, 81L, 117L, 102L, 86L, 78L, 95L,
> 79L, 82L, 82L, 87L, 79L, 87L, 83L, 78L, 74L, 78L, 81L, 74L, 77L,
> 69L, 78L, 73L, 62L, 56L, 63L, 70L, 60L, 64L, 67L, 60L, 81L, 57L,
> 64L, 67L, 67L, 69L, 90L, 70L, 62L, 79L, 69L, 66L, 66L, 64L, 58L,
> 64L, 50L, 72L, 59L, 77L, 78L, 64L, 78L, 50L, 60L, 58L, 59L, 60L,
> 44L, 54L, 51L, 52L, 59L, 59L, 63L, 65L, 62L, 66L, 57L, 48L, 59L,
> 53L, 46L, 53L, 58L, 57L, 66L, 46L, 48L, 41L, 57L, 55L, 50L, 49L,
> 60L, 66L, 60L, 66L, 57L, 57L, 64L, 61L, 58L, 66L, 53L, 56L, 66L,
> 55L, 56L, 83L, 71L, 72L, 54L, 51L, 52L, 60L, 49L, 64L, 72L, 67L,
> 59L, 66L, 56L, 63L, 73L, 56L, 57L, 53L, 54L, 56L, 56L, 80L, 70L,
> 73L, 53L, 71L, 68L, 47L, 59L, 79L, 49L, 67L, 70L, 64L, 66L, 69L,
> 52L, 66L, 57L, 52L, 57L, 59L, 49L, 70L, 54L, 60L, 63L, 72L, 69L,
> 62L, 52L, 49L, 64L, 49L, 48L, 65L, 63L, 52L, 54L, 70L, 48L, 45L,
> 48L, 47L, 42L, 50L, 38L, 57L, 51L, 38L, 42L, 50L, 54L, 56L, 59L,
> 40L, 50L, 43L, 35L, 43L, 39L, 31L, 32L, 39L, 40L, 33L, 35L, 41L,
> 38L, 39L, 28L, 39L, 32L, 39L, 35L, 31L, 32L, 38L, 30L, 21L, 33L,
> 37L, 28L, 34L, 30L, 37L, 34L, 38L, 34L, 34L, 38L, 33L, 33L, 27L,
> 18L, 33L), Var2 = c(821273L, 625955L, 809990L, 729112L, 532151L,
> 725098L, 619868L, 704282L, 580952L, 975656L, 783082L, 678787L,
> 685767L, 784148L, 628750L, 696202L, 695546L, 907842L, 554405L,
> 770651L, 746794L, 738427L, 756031L, 677908L, 707298L, 617374L,
> 760057L, 842253L, 680889L, 731068L, 833567L, 882259L, 695913L,
> 741626L, 773759L, 746727L, 696405L, 714477L, 688004L, 767584L,
> 674190L, 730536L, 733525L, 857631L, 750634L, 678859L, 792624L,
> 594360L, 638948L, 695963L, 553269L, 975142L, 692757L, 902180L,
> 806432L, 881918L, 847507L, 820608L, 704557L, 790379L, 700958L,
> 702484L, 781250L, 611012L, 761236L, 815931L, 759334L, 647436L,
> 738675L, 755135L, 762259L, 686818L, 736825L, 779627L, 728106L,
> 626037L, 609227L, 710760L, 769409L, 829428L, 693645L, 860311L,
> 673870L, 633099L, 735438L, 618685L, 697920L, 695360L, 555367L,
> 655665L, 723109L, 772520L, 686184L, 791464L, 745570L, 659855L,
> 505423L, 628448L, 452854L, 537319L, 583909L, 638940L, 649767L,
> 699971L, 688656L, 794720L, 711849L, 610735L, 750121L, 856431L,
> 760421L, 766238L, 748331L, 751135L, 949228L, 925412L, 701986L,
> 836017L, 796566L, 683700L, 795191L, 708331L, 634132L, 496022L,
> 676314L, 668954L, 712050L, 739238L, 674401L, 668638L, 693023L,
> 702984L, 706872L, 595021L, 588132L, 549140L, 629381L, 623842L,
> 605154L, 762123L, 635163L, 596381L, 478583L, 527694L, 582280L,
> 576937L, 609620L, 514848L, 531708L, 559237L, 597501L, 542658L,
> 449916L, 541199L, 422740L, 520411L, 550272L, 470397L, 471954L,
> 400365L, 306711L, 445820L, 520121L, 492774L, 424645L, 384204L,
> 480749L, 320850L, 625051L, 459398L, 376164L, 400453L, 322230L,
> 503949L, 509943L, 578626L, 611236L, 459764L, 488844L, 421423L,
> 549084L, 505593L, 531538L, 535442L, 400745L, 524029L, 485794L,
> 629792L, 572611L, 542791L, 457679L, 599733L, 503070L, 529176L,
> 448977L, 543827L, 420837L, 534159L, 452072L, 380600L, 422340L,
> 456238L, 448117L, 349250L, 463418L, 451798L, 422773L, 469039L,
> 401945L, 371124L, 426345L, 512084L, 376136L, 338565L, 398287L,
> 348028L, 540228L, 370901L, 382410L, 380287L, 518778L, 434662L,
> 633628L, 383930L, 409503L, 449935L, 314311L, 309324L, 311242L,
> 390780L, 252049L, 348992L, 286589L, 413858L, 318478L, 434770L,
> 358908L, 337788L, 349755L, 244330L, 293098L, 378302L, 350259L,
> 267338L, 245683L, 337204L, 257568L, 306317L, 314226L, 349265L,
> 387763L, 383584L, 318554L, 402885L, 299765L, 295652L, 285204L,
> 350783L, 262768L, 347901L, 402663L, 289501L, 391287L, 295352L,
> 238815L, 330207L, 370494L, 262839L, 265180L, 323103L, 369609L,
> 327294L, 407061L, 352180L, 398966L, 272813L, 289250L, 249749L,
> 269799L, 253926L, 212019L, 299012L, 232329L, 232591L, 226118L,
> 359578L, 350889L, 333566L, 289996L, 314330L, 355278L, 343200L,
> 263987L, 360194L, 375162L, 449485L, 310118L, 385856L, 255153L,
> 289999L, 387187L, 293586L, 362148L, 268204L, 293395L, 312468L,
> 292847L, 343747L, 311347L, 387747L, 249736L, 338624L, 334526L,
> 228124L, 299618L, 367627L, 197001L, 273961L, 339628L, 305371L,
> 321103L, 307636L, 180838L, 300252L, 238870L, 296085L, 299428L,
> 292380L, 220003L, 333949L, 248405L, 264780L, 300803L, 336372L,
> 338379L, 257623L, 222979L, 226364L, 349364L, 260642L, 227835L,
> 237010L, 306306L, 253793L, 169667L, 333917L, 203583L, 204326L,
> 245043L, 200200L, 206647L, 266962L, 171125L, 227414L, 209053L,
> 132254L, 126972L, 159604L, 217526L, 186575L, 274024L, 155100L,
> 182481L, 198915L, 185028L, 165374L, 167406L, 114972L, 153905L,
> 131214L, 186645L, 133336L, 195207L, 168546L, 175644L, 210364L,
> 150943L, 142076L, 128045L, 147020L, 157380L, 110542L, 143323L,
> 169639L, 136196L, 94668L, 215810L, 170180L, 162091L, 139682L,
> 175133L, 153901L, 158420L, 127673L, 177445L, 156123L, 179980L,
> 142630L, 160726L, 181325L, 92592L, 191459L), Var3 = c(1023833L,
> 924771L, 1021634L, 1043681L, 752874L, 947427L, 859879L, 999681L,
> 835358L, 1252862L, 1047489L, 974229L, 1029854L, 1115937L, 868082L,
> 972236L, 850169L, 1143772L, 805427L, 965832L, 970293L, 884963L,
> 996303L, 891808L, 900303L, 835772L, 972515L, 1003546L, 852313L,
> 987354L, 1028777L, 1117341L, 893702L, 879153L, 1029602L, 982566L,
> 886107L, 997185L, 875060L, 964200L, 810422L, 862861L, 915637L,
> 1011660L, 870443L, 864038L, 1004598L, 792715L, 869050L, 792638L,
> 689028L, 1077470L, 831436L, 988032L, 923528L, 918479L, 1018139L,
> 990291L, 980459L, 1048576L, 863124L, 854886L, 909880L, 798497L,
> 938327L, 982216L, 859205L, 827323L, 841991L, 913290L, 983918L,
> 793665L, 895188L, 906126L, 863296L, 842577L, 832591L, 976840L,
> 932345L, 981143L, 801512L, 1002306L, 931473L, 902389L, 1067539L,
> 934557L, 871256L, 883418L, 718243L, 853101L, 892091L, 959493L,
> 875254L, 992752L, 891876L, 779707L, 761433L, 758197L, 702417L,
> 694400L, 743969L, 827773L, 788336L, 891501L, 784553L, 936542L,
> 892221L, 805682L, 986109L, 997737L, 952337L, 913596L, 881975L,
> 973655L, 1042842L, 991280L, 811610L, 969744L, 871970L, 832743L,
> 896535L, 898091L, 754328L, 708464L, 757153L, 777688L, 830975L,
> 891923L, 869821L, 803335L, 882650L, 866547L, 868705L, 747149L,
> 713600L, 762138L, 735163L, 800943L, 699566L, 913384L, 738171L,
> 782984L, 667440L, 658650L, 777300L, 722849L, 721270L, 720574L,
> 649446L, 751518L, 719568L, 747856L, 611773L, 753152L, 650260L,
> 686105L, 742319L, 611929L, 640700L, 591455L, 422915L, 551312L,
> 589984L, 552638L, 427190L, 450827L, 556519L, 395846L, 590983L,
> 512959L, 452685L, 449679L, 377117L, 592666L, 630967L, 677562L,
> 744774L, 559144L, 565164L, 431371L, 661244L, 562805L, 635075L,
> 611218L, 465263L, 600953L, 570639L, 741657L, 655390L, 624346L,
> 519072L, 655233L, 573364L, 555229L, 568871L, 673415L, 562549L,
> 632082L, 648735L, 547210L, 541646L, 591035L, 573215L, 453220L,
> 500569L, 483082L, 494882L, 556911L, 400361L, 408601L, 441426L,
> 505124L, 417482L, 421750L, 417484L, 431120L, 504564L, 333542L,
> 369482L, 431832L, 475688L, 437028L, 581278L, 408507L, 383338L,
> 502921L, 392616L, 381636L, 437582L, 402350L, 342748L, 420895L,
> 304549L, 452065L, 392927L, 525972L, 474072L, 439285L, 506800L,
> 301623L, 360435L, 431918L, 406583L, 348245L, 311187L, 397553L,
> 328374L, 386618L, 359652L, 410604L, 415530L, 423691L, 387479L,
> 427504L, 361311L, 328342L, 384716L, 375293L, 337292L, 378871L,
> 369858L, 368082L, 447470L, 319699L, 309456L, 324288L, 416815L,
> 350970L, 305021L, 340379L, 415899L, 415863L, 459164L, 424415L,
> 426744L, 381180L, 421914L, 432177L, 439146L, 471412L, 358047L,
> 424430L, 439595L, 378718L, 402841L, 558536L, 475445L, 496879L,
> 386381L, 371953L, 362575L, 410119L, 307376L, 464888L, 497654L,
> 505371L, 428729L, 484825L, 340312L, 403197L, 483014L, 425448L,
> 442173L, 382614L, 365801L, 377530L, 412724L, 582561L, 497182L,
> 534839L, 372528L, 490513L, 524295L, 353328L, 435740L, 606586L,
> 339952L, 446177L, 541098L, 505165L, 471455L, 483144L, 372510L,
> 468379L, 381000L, 388039L, 448045L, 406707L, 363432L, 492507L,
> 389963L, 458037L, 446202L, 520027L, 534002L, 411419L, 382377L,
> 374016L, 489217L, 385358L, 354344L, 480102L, 486111L, 396875L,
> 424706L, 549349L, 331376L, 327566L, 364968L, 365715L, 327275L,
> 368299L, 277930L, 416003L, 393085L, 322959L, 306650L, 401742L,
> 420621L, 429129L, 462222L, 335711L, 416480L, 338851L, 329130L,
> 338371L, 310220L, 255807L, 258337L, 319225L, 331696L, 276512L,
> 288625L, 329139L, 311016L, 332946L, 231066L, 315569L, 251276L,
> 255198L, 294448L, 251216L, 262122L, 322465L, 230834L, 164729L,
> 272468L, 267101L, 223642L, 273754L, 262820L, 299455L, 282714L,
> 290049L, 277035L, 270754L, 315434L, 267553L, 273266L, 231968L,
> 145726L, 270737L), Var4 = c(1842916L, 1650947L, 1826792L, 1868854L,
> 1349682L, 1700705L, 1553206L, 1789866L, 1481587L, 2260812L, 1875500L,
> 1757982L, 1844106L, 2015221L, 1559047L, 1747566L, 1547336L, 2068885L,
> 1434585L, 1754398L, 1751677L, 1605872L, 1791263L, 1606000L, 1622632L,
> 1501355L, 1751479L, 1813795L, 1534006L, 1763481L, 1862021L, 2011101L,
> 1579945L, 1582041L, 1845277L, 1774748L, 1604085L, 1798036L, 1581748L,
> 1747547L, 1474606L, 1553108L, 1662061L, 1832545L, 1555462L, 1560721L,
> 1803880L, 1421983L, 1705225L, 1570989L, 1370337L, 2130208L, 1648356L,
> 1967994L, 1840196L, 1814451L, 2018955L, 1953286L, 1943577L, 2068666L,
> 1709487L, 1707015L, 1816645L, 1592986L, 1852489L, 1947445L, 1705050L,
> 1626904L, 1661471L, 1810373L, 1935931L, 1564800L, 1770490L, 1793351L,
> 1704562L, 1657873L, 1624573L, 1891007L, 1842884L, 1940814L, 1598949L,
> 1995798L, 1844261L, 1783182L, 2300414L, 2035373L, 1897734L, 1927624L,
> 1576325L, 1864730L, 1945731L, 2087827L, 1887315L, 2160582L, 1940982L,
> 1709248L, 1641578L, 1651927L, 1523934L, 1520967L, 1615398L, 1796216L,
> 1709032L, 1929426L, 1715439L, 2042236L, 1922861L, 1752159L, 2120420L,
> 2176839L, 2056198L, 1978545L, 1917299L, 2087989L, 2287761L, 2165191L,
> 1765206L, 2101615L, 1886381L, 1800709L, 1932399L, 1938540L, 1624589L,
> 1515305L, 1636013L, 1678695L, 1805357L, 1920032L, 1869846L, 1733193L,
> 1911342L, 1879654L, 1888584L, 1617636L, 1553308L, 1766792L, 1708563L,
> 1866907L, 1619900L, 2131880L, 1723028L, 1804310L, 1561439L, 1532808L,
> 1799613L, 1669279L, 1695310L, 1793913L, 1640780L, 1887981L, 1824651L,
> 1855067L, 1546987L, 1893343L, 1615038L, 1713931L, 1841102L, 1552554L,
> 1609310L, 1471592L, 1057635L, 1394483L, 1478938L, 1400849L, 1092941L,
> 1127207L, 1410334L, 985857L, 1502482L, 1282296L, 1131903L, 1134120L,
> 944486L, 1492348L, 1544775L, 1704514L, 1844986L, 1394241L, 1419907L,
> 1102451L, 1678210L, 1416968L, 1586854L, 1531570L, 1177832L, 1522673L,
> 1437979L, 1854961L, 1643220L, 1585851L, 1329138L, 1660453L, 1455096L,
> 1418685L, 1445048L, 1695041L, 1416778L, 1604949L, 1612551L, 1384783L,
> 1369524L, 1502777L, 1430407L, 1143067L, 1375883L, 1332591L, 1361450L,
> 1509461L, 1111893L, 1122316L, 1198293L, 1371022L, 1131783L, 1216891L,
> 1201775L, 1251192L, 1468533L, 971716L, 1080623L, 1243478L, 1380687L,
> 1252032L, 1685018L, 1186716L, 1110004L, 1448460L, 1142144L, 1108305L,
> 1262013L, 1177100L, 975649L, 1223630L, 872010L, 1291482L, 1130856L,
> 1565162L, 1453895L, 1346179L, 1511057L, 933375L, 1123020L, 1316433L,
> 1238018L, 1094457L, 1021577L, 1236134L, 1035408L, 1242196L, 1160343L,
> 1344770L, 1334352L, 1322612L, 1243856L, 1369881L, 1101393L, 1068661L,
> 1242855L, 1181028L, 1068507L, 1198975L, 1155222L, 1282687L, 1578816L,
> 1099740L, 1055999L, 1110803L, 1387463L, 1211896L, 1032759L, 1147958L,
> 1453268L, 1598432L, 1712287L, 1558504L, 1606923L, 1393160L, 1547333L,
> 1573425L, 1616768L, 1759911L, 1328534L, 1568315L, 1640106L, 1485560L,
> 1594149L, 2158755L, 1907469L, 1950358L, 1514739L, 1504915L, 1463329L,
> 1612138L, 1272940L, 1820813L, 1961831L, 2001256L, 1615047L, 1881349L,
> 1280810L, 1535983L, 1868300L, 1597679L, 1668499L, 1636259L, 1558331L,
> 1620336L, 1774802L, 2464370L, 2173591L, 2324146L, 1622177L, 2095321L,
> 2353130L, 1678844L, 1908599L, 2661533L, 1523244L, 2089773L, 2456706L,
> 2291627L, 2112795L, 2146462L, 1708033L, 2082991L, 1868523L, 1870792L,
> 2163766L, 1965448L, 1740014L, 2384685L, 1859087L, 2227283L, 2082207L,
> 2435339L, 2437914L, 1938528L, 1975502L, 1891861L, 2447307L, 1998905L,
> 1788622L, 2416936L, 2471771L, 2023577L, 2129774L, 2756620L, 1669619L,
> 1679035L, 1879060L, 1893919L, 1653989L, 1877943L, 1389847L, 2102051L,
> 1982800L, 1609563L, 1534998L, 2031637L, 2090443L, 2146740L, 2306902L,
> 1704790L, 2067778L, 1676170L, 1664308L, 1649995L, 1586174L, 1320104L,
> 1356841L, 1590752L, 1683829L, 1385250L, 1441666L, 1653611L, 1558089L,
> 1669990L, 1163876L, 1572544L, 1255959L, 1259118L, 1479161L, 1278374L,
> 1336800L, 1606781L, 1171646L, 851189L, 1385792L, 1344448L, 1115996L,
> 1366224L, 1340041L, 1527343L, 1408925L, 1454184L, 1424354L, 1381617L,
> 1558428L, 1353706L, 1386354L, 1189991L, 763254L, 1386571L), Var5 =
> structure(c(1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 167L, 167L, 167L,
> 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L,
> 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L,
> 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L,
> 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L,
> 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L,
> 167L, 167L, 167L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 167L, 1L, 1L, 167L, 167L, 167L, 167L, 167L,
> 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L,
> 1L, 167L, 1L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L,
> 167L, 167L, 167L, 167L, 167L, 167L, 167L, 167L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 103L, 120L, 115L, 118L, 92L,
> 102L, 108L, 97L, 113L, 128L, 117L, 122L, 125L, 126L, 139L, 131L,
> 112L, 129L, 127L, 79L, 116L, 124L, 104L, 98L, 93L, 82L, 100L,
> 123L, 83L, 88L, 66L, 68L, 101L, 41L, 81L, 119L, 99L, 84L, 47L,
> 85L, 31L, 38L, 58L, 46L, 90L, 43L, 48L, 91L, 32L, 54L, 78L, 87L,
> 53L, 33L, 55L, 57L, 13L, 69L, 10L, 77L, 52L, 161L, 153L, 145L,
> 147L, 9L, 155L, 138L, 156L, 130L, 136L, 148L, 145L, 15L, 4L,
> 163L, 3L, 72L, 109L, 11L, 63L, 39L, 110L, 107L, 105L, 50L, 71L,
> 73L, 51L, 60L, 67L, 70L, 61L, 37L, 75L, 18L, 95L, 5L, 45L, 7L,
> 34L, 44L, 24L, 164L, 56L, 2L, 40L, 78L, 30L, 36L, 65L, 166L,
> 17L, 22L, 62L, 19L, 16L, 162L, 25L, 14L, 159L, 8L, 12L, 149L,
> 6L, 158L, 165L, 157L, 143L, 150L, 146L, 142L, 144L, 152L, 96L,
> 80L, 26L, 140L, 114L, 154L, 42L, 28L, 76L, 29L, 160L, 86L, 89L,
> 121L, 27L, 59L, 64L, 49L, 135L, 151L, 111L, 74L, 134L, 35L, 106L,
> 94L, 23L, 21L, 20L, 137L, 132L, 133L, 141L), .Label = c("0",
> "1004", "1017", "1025", "1029", "1032", "1034", "1052", "1093",
> "1094", "1096", "1097", "1102", "1108", "1137", "1140", "1149",
> "1151", "1156", "117", "121", "1216", "122", "1223", "1229",
> "123", "124", "128", "131", "1332", "1336", "1356", "1371", "1376",
> "138", "1381", "1428", "1446", "1448", "1473", "1478", "148",
> "1502", "1506", "1516", "1536", "1551", "1556", "156", "1569",
> "1582", "1598", "1610", "1659", "1686", "1703", "1731", "1733",
> "174", "1741", "1750", "1755", "1783", "179", "1794", "1799",
> "1800", "1805", "1816", "1819", "1874", "1876", "1879", "188",
> "1883", "189", "1899", "1908", "1928", "195", "1963", "2005",
> "2006", "2013", "2041", "206", "2074", "2076", "208", "2193",
> "2201", "2260", "2294", "230", "2324", "233", "2373", "2388",
> "2478", "2491", "2509", "2535", "2537", "2556", "2599", "260",
> "2648", "2651", "2657", "2677", "268", "2836", "2844", "286",
> "2942", "2944", "2990", "3006", "3086", "3124", "314", "3177",
> "3230", "3287", "3355", "3390", "3441", "3455", "3469", "352",
> "3533", "369", "376", "381", "384", "394", "397", "413", "4288",
> "45", "451", "459", "478", "524", "529", "549", "569", "583",
> "638", "643", "65", "650", "720", "74", "757", "761", "819",
> "824", "839", "86", "861", "865", "881", "898", "924", "984",
> "NULL"), class = "factor"), Var6 = structure(c(2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 2L,
> 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), .Label = c("0",
> "NULL"), class = "factor"), Var7 = c(0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 509986L, 457334L, 423431L, 491864L, 291347L,
> 348557L, 415865L, 392211L, 335553L, 298921L, 383274L, 316046L,
> 373400L, 346341L, 389709L, 401434L, 409913L, 374109L, 412340L,
> 351334L, 318191L, 371280L, 362450L, 326204L, 365452L, 359126L,
> 353572L, 432630L, 308945L, 298529L, 314237L, 397880L, 339472L,
> 294810L, 328184L, 399537L, 400889L, 442645L, 407200L, 411296L,
> 367262L, 407430L, 417298L, 422888L, 453429L, 344743L, 408298L,
> 422106L, 363156L, 387085L, 536751L, 458335L, 478253L, 371372L,
> 355103L, 351371L, 396376L, 295519L, 447933L, 478155L, 486481L,
> 413681L, 467253L, 327260L, 386971L, 467136L, 411711L, 426864L,
> 369899L, 355287L, 365007L, 397091L, 561952L, 477049L, 513256L,
> 357575L, 470663L, 505672L, 340253L, 418327L, 582188L, 326354L,
> 427588L, 517728L, 485881L, 453163L, 463697L, 357187L, 449418L,
> 365614L, 372260L, 431281L, 390126L, 347262L, 472019L, 374950L,
> 440093L, 428659L, 494099L, 507023L, 395678L, 366396L, 358862L,
> 469883L, 369744L, 341466L, 461598L, 466752L, 381354L, 407277L,
> 527482L, 317936L, 314390L, 351180L, 351920L, 315951L, 354029L,
> 267565L, 399528L, 377080L, 310084L, 294011L, 385220L, 406141L,
> 412094L, 444809L, 321582L, 399966L, 326061L, 315964L, 323720L,
> 297622L, 244843L, 247743L, 306002L, 318435L, 266651L, 277154L,
> 314510L, 297403L, 319653L, 222712L, 301423L, 240825L, 244846L,
> 282070L, 241318L, 252998L, 310381L, 221844L, 159100L, 261463L,
> 256824L, 214117L, 263015L, 253039L, 289615L, 272659L, 279316L,
> 267969L, 262087L, 304361L, 258202L, 262335L, 221760L, 139884L,
> 260220L), Var8 = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L,
> 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L,
> 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L,
> 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L,
> 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L,
> 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 169L,
> 1L, 1L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L,
> 169L, 169L, 169L, 169L, 169L, 169L, 169L, 1L, 169L, 1L, 169L,
> 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L, 169L,
> 169L, 169L, 169L, 169L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 162L, 143L, 130L, 158L, 30L, 68L, 125L, 104L, 61L, 37L,
> 98L, 49L, 93L, 66L, 102L, 113L, 119L, 94L, 123L, 70L, 51L, 90L,
> 81L, 57L, 84L, 80L, 73L, 135L, 41L, 36L, 44L, 108L, 62L, 32L,
> 60L, 110L, 112L, 137L, 115L, 120L, 87L, 117L, 126L, 129L, 142L,
> 65L, 118L, 128L, 82L, 101L, 166L, 144L, 155L, 91L, 75L, 71L,
> 106L, 33L, 139L, 154L, 157L, 124L, 149L, 59L, 100L, 148L, 121L,
> 131L, 89L, 76L, 83L, 107L, 167L, 153L, 163L, 78L, 151L, 160L,
> 63L, 127L, 168L, 58L, 132L, 164L, 156L, 141L, 146L, 77L, 140L,
> 85L, 92L, 134L, 103L, 67L, 152L, 95L, 136L, 133L, 159L, 161L,
> 105L, 86L, 79L, 150L, 88L, 64L, 145L, 147L, 97L, 116L, 165L,
> 50L, 45L, 69L, 72L, 47L, 74L, 23L, 109L, 96L, 42L, 31L, 99L,
> 114L, 122L, 138L, 54L, 111L, 56L, 48L, 55L, 35L, 10L, 12L, 40L,
> 52L, 22L, 26L, 46L, 34L, 53L, 7L, 38L, 8L, 11L, 28L, 9L, 13L,
> 43L, 6L, 3L, 18L, 15L, 4L, 21L, 14L, 29L, 25L, 27L, 24L, 19L,
> 39L, 16L, 20L, 5L, 2L, 17L), .Label = c("0", "139884", "159100",
> "214117", "221760", "221844", "222712", "240825", "241318", "244843",
> "244846", "247743", "252998", "253039", "256824", "258202", "260220",
> "261463", "262087", "262335", "263015", "266651", "267565", "267969",
> "272659", "277154", "279316", "282070", "289615", "291347", "294011",
> "294810", "295519", "297403", "297622", "298529", "298921", "301423",
> "304361", "306002", "308945", "310084", "310381", "314237", "314390",
> "314510", "315951", "315964", "316046", "317936", "318191", "318435",
> "319653", "321582", "323720", "326061", "326204", "326354", "327260",
> "328184", "335553", "339472", "340253", "341466", "344743", "346341",
> "347262", "348557", "351180", "351334", "351371", "351920", "353572",
> "354029", "355103", "355287", "357187", "357575", "358862", "359126",
> "362450", "363156", "365007", "365452", "365614", "366396", "367262",
> "369744", "369899", "371280", "371372", "372260", "373400", "374109",
> "374950", "377080", "381354", "383274", "385220", "386971", "387085",
> "389709", "390126", "392211", "395678", "396376", "397091", "397880",
> "399528", "399537", "399966", "400889", "401434", "406141", "407200",
> "407277", "407430", "408298", "409913", "411296", "411711", "412094",
> "412340", "413681", "415865", "417298", "418327", "422106", "422888",
> "423431", "426864", "427588", "428659", "431281", "432630", "440093",
> "442645", "444809", "447933", "449418", "453163", "453429", "457334",
> "458335", "461598", "463697", "466752", "467136", "467253", "469883",
> "470663", "472019", "477049", "478155", "478253", "485881", "486481",
> "491864", "494099", "505672", "507023", "509986", "513256", "517728",
> "527482", "536751", "561952", "582188", "NULL"), class = "factor")), class
> = "data.frame", row.names = c(NA,
> -402L))
>>
> Any help and/or guidance will be greatly appreciated,
> 
> Cheers,
> 
> Paul
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From GC@rden@@2 @end|ng |rom med@m|@m|@edu  Fri May 24 19:43:36 2019
From: GC@rden@@2 @end|ng |rom med@m|@m|@edu (Cardenas, Gabriel A)
Date: Fri, 24 May 2019 17:43:36 +0000
Subject: [R] 
 mboost: Proportional odds boosting model - how to specify the
 offset?
In-Reply-To: <CAGxFJbQFJbJB84YcesHiD_gAw+5x2kM-zxQjNmDoWTBOYZAJtg@mail.gmail.com>
References: <SN6PR07MB5038FFBE56E189EFDAA2506EDB020@SN6PR07MB5038.namprd07.prod.outlook.com>
 <CAGxFJbQhTiVgDxxn5DyMqfCjNJcUnC5+A7e_VcpJ4_gMM8hz1A@mail.gmail.com>
 <CAGxFJbQFJbJB84YcesHiD_gAw+5x2kM-zxQjNmDoWTBOYZAJtg@mail.gmail.com>
Message-ID: <SN6PR07MB5038D54BF949E2F9C75A14D9DB020@SN6PR07MB5038.namprd07.prod.outlook.com>

Thanks for your help.
Mainly, at one point I code the dependent variable PREP24 to dichotomous and run boosted logistic right. I try to use type=?glm? and I get nothing out of the analysis. So I just leave it as binomial()  and then I get this strange response in the .obj where I get a response with a different response for every row? Then I get that error out of confint?

From: Bert Gunter <bgunter.4567 at gmail.com>
Sent: Friday, May 24, 2019 12:58 PM
To: Cardenas, Gabriel A <GCardenas2 at med.miami.edu>
Cc: r-help at r-project.org
Subject: Re: [R] mboost: Proportional odds boosting model - how to specify the offset?

... and I should have said, show us your code. We've no idea what you tried to do (or at least I don't).

Bert Gunter

"The trouble with having an open mind is that people keep coming along and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, May 24, 2019 at 9:50 AM Bert Gunter <bgunter.4567 at gmail.com<mailto:bgunter.4567 at gmail.com>> wrote:
I would have thought the message was obvious. Follow the posting guide: show us a subset of your data.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, May 24, 2019 at 9:26 AM Cardenas, Gabriel A <GCardenas2 at med.miami.edu<mailto:GCardenas2 at med.miami.edu>> wrote:
I keep getting this error

in base::rowSums(x, na.rm = na.rm, dims = dims, ...) :
  'x' must be numeric
In addition: Warning message:

When I try to run confint on the opt object can you help please thnx

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help<https://nam01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=02%7C01%7CGCardenas2%40med.miami.edu%7C213a0d9d89964df3614c08d6e068f10a%7C2a144b72f23942d48c0e6f0f17c48e33%7C0%7C0%7C636943138659886506&sdata=MOYiygI2g59rHOD53aqlfpKXQbA2xKb%2FFoZMQiw658M%3D&reserved=0>
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html<https://nam01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.R-project.org%2Fposting-guide.html&data=02%7C01%7CGCardenas2%40med.miami.edu%7C213a0d9d89964df3614c08d6e068f10a%7C2a144b72f23942d48c0e6f0f17c48e33%7C0%7C0%7C636943138659886506&sdata=o4V9nNtarvaF4HHEz%2B3j4bEF59OhT69%2Fl8bDjczsFEc%3D&reserved=0>
and provide commented, minimal, self-contained, reproducible code.

From @purd|e@@ @end|ng |rom gm@||@com  Sat May 25 00:47:45 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Sat, 25 May 2019 10:47:45 +1200
Subject: [R] r kernlab find best cost parameter automatically
In-Reply-To: <CAMk+s2RifHVnp2VtpB-C_DS5F8_Zk5O77ObKX6o0TE5Qk-4vEw@mail.gmail.com>
References: <CAMk+s2RifHVnp2VtpB-C_DS5F8_Zk5O77ObKX6o0TE5Qk-4vEw@mail.gmail.com>
Message-ID: <CAB8pepyEF18KU1vfYKTGHuR3ZwcrDq3dxYqdKR5R8vhSxP6jYw@mail.gmail.com>

> Would be possible to automate the selection of the best <k> value?

Can you define "best", precisely?

	[[alternative HTML version deleted]]


From @|yeng@ @end|ng |rom y@hoo@com  Sat May 25 00:04:51 2019
From: @|yeng@ @end|ng |rom y@hoo@com (Sunanda Iyengar)
Date: Fri, 24 May 2019 22:04:51 +0000 (UTC)
Subject: [R] Calling R code from Javascript
References: <700507480.7403414.1558735491361.ref@mail.yahoo.com>
Message-ID: <700507480.7403414.1558735491361@mail.yahoo.com>

Hello,
Came across this e-mail when considering plumber to use R Gui.I am very new to this and i am not sure how to use plumber to connect a javascrpt code to run R. I have a chunk of R code which does some number crunching work which is very neat. I want this to run from a browser API call using plumber within javascript. Please let me if it is possible. If you have a javascript example I would be very thankful for the share.
If there are other ways to run R code from javascript, pl let me know.

Regards,Sunanda
   
   -    


	[[alternative HTML version deleted]]


From h@@@n@d|w@n @end|ng |rom gm@||@com  Sat May 25 04:12:37 2019
From: h@@@n@d|w@n @end|ng |rom gm@||@com (Hasan Diwan)
Date: Fri, 24 May 2019 19:12:37 -0700
Subject: [R] Calling R code from Javascript
In-Reply-To: <700507480.7403414.1558735491361@mail.yahoo.com>
References: <700507480.7403414.1558735491361.ref@mail.yahoo.com>
 <700507480.7403414.1558735491361@mail.yahoo.com>
Message-ID: <CAP+bYWCM6WkNLKBCSti5bF9qu_x1xoa3PLFieUhO03dqsMgfNQ@mail.gmail.com>

Sunanda,
I'd suggest you expose a model and methods to the web using shiny or
something. After which, you just need to use jquery to call the HTTP
endpoint. If you need further assistance, please email me off list and I'll
provide. -- H

On Fri, 24 May 2019 at 18:05, Sunanda Iyengar via R-help <
r-help at r-project.org> wrote:

> Hello,
> Came across this e-mail when considering plumber to use R Gui.I am very
> new to this and i am not sure how to use plumber to connect a javascrpt
> code to run R. I have a chunk of R code which does some number crunching
> work which is very neat. I want this to run from a browser API call using
> plumber within javascript. Please let me if it is possible. If you have a
> javascript example I would be very thankful for the share.
> If there are other ways to run R code from javascript, pl let me know.
>
> Regards,Sunanda
>
>    -
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
OpenPGP:
https://sks-keyservers.net/pks/lookup?op=get&search=0xFEBAD7FFD041BBA1
If you wish to request my time, please do so using
*bit.ly/hd1AppointmentRequest
<http://bit.ly/hd1AppointmentRequest>*.
Si vous voudrais faire connnaisance, allez a *bit.ly/hd1AppointmentRequest
<http://bit.ly/hd1AppointmentRequest>*.

<https://sks-keyservers.net/pks/lookup?op=get&search=0xFEBAD7FFD041BBA1>Sent
from my mobile device
Envoye de mon portable

	[[alternative HTML version deleted]]


From ju@ngomezdu@@o @end|ng |rom gm@||@com  Sat May 25 09:48:37 2019
From: ju@ngomezdu@@o @end|ng |rom gm@||@com (Juan Gomez)
Date: Sat, 25 May 2019 09:48:37 +0200
Subject: [R] Error in unique() man page
Message-ID: <CAN+qQ08FzhqVNFEuLYCzssYBZiBmV7PrggAcp1dmeBnQERk8CQ@mail.gmail.com>

Function unique() man page states that:
"The array method calculates for each element of the dimension
specified by MARGIN if the remaining dimensions are identical to those
for an earlier element (in row-major order). "
I think that the last precission: "(in row-major order)" is
meaningless here, as the function is dividing the array in just one
axis: the specified in the MARGIN argument. This MARGIN is rows by
default, but this has nothing to do with the concept of :
"row-major-order".
This issue is related to the one reported in:
https://stat.ethz.ch/pipermail/r-help/2019-May/462620.html


From r@||@m@|den @end|ng |rom gm@||@com  Sat May 25 14:38:07 2019
From: r@||@m@|den @end|ng |rom gm@||@com (Raffa)
Date: Sat, 25 May 2019 14:38:07 +0200
Subject: [R] Increasing number of observations worsen the regression model
Message-ID: <1824c21d-7676-c895-dc81-352f2a0c7722@gmail.com>

I have the following code:

```

rm(list=ls())
N = 30000
xvar <- runif(N, -10, 10)
e <- rnorm(N, mean=0, sd=1)
yvar <- 1 + 2*xvar + e
plot(xvar,yvar)
lmMod <- lm(yvar~xvar)
print(summary(lmMod))
domain <- seq(min(xvar), max(xvar))??? # define a vector of x values to 
feed into model
lines(domain, predict(lmMod, newdata = data.frame(xvar=domain)))??? # 
add regression line, using `predict` to generate y-values

```

I expected the coefficients to be something similar to [1,2]. Instead R 
keeps throwing at me random numbers that are not statistically 
significant and don't fit the model, and I have 20k observations. For 
example

```

Call:
lm(formula = yvar ~ xvar)

Residuals:
 ??? Min????? 1Q? Median????? 3Q???? Max
-21.384? -8.908?? 1.016? 10.972? 23.663

Coefficients:
 ???????????? Estimate Std. Error t value Pr(>|t|)
(Intercept) 0.0007145? 0.0670316?? 0.011??? 0.991
xvar??????? 0.0168271? 0.0116420?? 1.445??? 0.148

Residual standard error: 11.61 on 29998 degrees of freedom
Multiple R-squared:? 7.038e-05,??? Adjusted R-squared: 3.705e-05
F-statistic: 2.112 on 1 and 29998 DF,? p-value: 0.1462

```


The strange thing is that the code works perfectly for N=200 or N=2000. 
It's only for larger N that this thing happen U(for example, N=20000). I 
have tried to ask for example in CrossValidated 
<https://stats.stackexchange.com/questions/410050/increasing-number-of-observations-worsen-the-regression-model> 
but the code works for them. Any help?

I am runnign R 3.6.0 on Kubuntu 19.04

Best regards

Raffaele


	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Sun May 26 16:06:26 2019
From: j|ox @end|ng |rom mcm@@ter@c@ (Fox, John)
Date: Sun, 26 May 2019 14:06:26 +0000
Subject: [R] 
 Increasing number of observations worsen the regression model
In-Reply-To: <14360_1558877817_x4QDavRJ002449_1824c21d-7676-c895-dc81-352f2a0c7722@gmail.com>
References: <14360_1558877817_x4QDavRJ002449_1824c21d-7676-c895-dc81-352f2a0c7722@gmail.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC836BC1D99@FHSDB2D11-2.csu.mcmaster.ca>

Dear Raffaele,

Using your code, with one modification -- setting the seed for R's random number generator to make the result reproducible -- I get:

> set.seed(12345)

. . .

> lmMod <- lm(yvar~xvar)
> print(summary(lmMod))

Call:
lm(formula = yvar ~ xvar)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.0293 -0.6732  0.0021  0.6749  4.2883 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 1.0057713  0.0057529   174.8   <2e-16 ***
xvar        2.0000889  0.0009998  2000.4   <2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Residual standard error: 0.9964 on 29998 degrees of freedom
Multiple R-squared:  0.9926,	Adjusted R-squared:  0.9926 
F-statistic: 4.002e+06 on 1 and 29998 DF,  p-value: < 2.2e-16

which is more or less what one would expect.

My guess: you've saved your R workspace from a previous session, and it is then loaded at the start of your R session; something in the saved workspace is affecting the result, although frankly I can't think what that might be.

I hope this helps,
 John

-----------------------------------------------------------------
John Fox
Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
Web: https://socialsciences.mcmaster.ca/jfox/



> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Raffa
> Sent: Saturday, May 25, 2019 8:38 AM
> To: r-help at r-project.org
> Subject: [R] Increasing number of observations worsen the regression model
> 
> I have the following code:
> 
> ```
> 
> rm(list=ls())
> N = 30000
> xvar <- runif(N, -10, 10)
> e <- rnorm(N, mean=0, sd=1)
> yvar <- 1 + 2*xvar + e
> plot(xvar,yvar)
> lmMod <- lm(yvar~xvar)
> print(summary(lmMod))
> domain <- seq(min(xvar), max(xvar))??? # define a vector of x values to feed
> into model lines(domain, predict(lmMod, newdata =
> data.frame(xvar=domain)))??? # add regression line, using `predict` to generate
> y-values
> 
> ```
> 
> I expected the coefficients to be something similar to [1,2]. Instead R keeps
> throwing at me random numbers that are not statistically significant and don't
> fit the model, and I have 20k observations. For example
> 
> ```
> 
> Call:
> lm(formula = yvar ~ xvar)
> 
> Residuals:
>  ??? Min????? 1Q? Median????? 3Q???? Max
> -21.384? -8.908?? 1.016? 10.972? 23.663
> 
> Coefficients:
>  ???????????? Estimate Std. Error t value Pr(>|t|)
> (Intercept) 0.0007145? 0.0670316?? 0.011??? 0.991
> xvar??????? 0.0168271? 0.0116420?? 1.445??? 0.148
> 
> Residual standard error: 11.61 on 29998 degrees of freedom Multiple R-
> squared:? 7.038e-05,??? Adjusted R-squared: 3.705e-05
> F-statistic: 2.112 on 1 and 29998 DF,? p-value: 0.1462
> 
> ```
> 
> 
> The strange thing is that the code works perfectly for N=200 or N=2000.
> It's only for larger N that this thing happen U(for example, N=20000). I have
> tried to ask for example in CrossValidated
> <https://stats.stackexchange.com/questions/410050/increasing-number-of-
> observations-worsen-the-regression-model>
> but the code works for them. Any help?
> 
> I am runnign R 3.6.0 on Kubuntu 19.04
> 
> Best regards
> 
> Raffaele
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From rm@h@rp @end|ng |rom me@com  Sun May 26 17:09:14 2019
From: rm@h@rp @end|ng |rom me@com (R. Mark Sharp)
Date: Sun, 26 May 2019 10:09:14 -0500
Subject: [R] 
 Increasing number of observations worsen the regression model
In-Reply-To: <1824c21d-7676-c895-dc81-352f2a0c7722@gmail.com>
References: <1824c21d-7676-c895-dc81-352f2a0c7722@gmail.com>
Message-ID: <430BAC0E-04DB-402E-B5DD-7A326C1BA392@me.com>

Raffa, 

I ran this on a MacOS machine and got what you expected. I added a call to sessionInfo() for your information.

> rm(list=ls())
> N = 30000
> xvar <- runif(N, -10, 10)
> e <- rnorm(N, mean=0, sd=1)
> yvar <- 1 + 2*xvar + e
> plot(xvar,yvar)
> lmMod <- lm(yvar~xvar)
> print(summary(lmMod))

Call:
lm(formula = yvar ~ xvar)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.2407 -0.6738 -0.0031  0.6822  4.0619 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 1.0059022  0.0057370   175.3   <2e-16 ***
xvar        2.0005811  0.0009918  2017.2   <2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Residual standard error: 0.9937 on 29998 degrees of freedom
Multiple R-squared:  0.9927,	Adjusted R-squared:  0.9927 
F-statistic: 4.069e+06 on 1 and 29998 DF,  p-value: < 2.2e-16

> domain <- seq(min(xvar), max(xvar))    # define a vector of x values to feed into model
> lines(domain, predict(lmMod, newdata = data.frame(xvar=domain)))    # add regression line, using `predict` to generate y-values
> sessionInfo()
R version 3.6.0 (2019-04-26)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS Mojave 10.14.4

Matrix products: default
BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

loaded via a namespace (and not attached):
[1] compiler_3.6.0




R. Mark Sharp, Ph.D.
Data Scientist and Biomedical Statistical Consultant
7526 Meadow Green St.
San Antonio, TX 78251
mobile: 210-218-2868
rmsharp at me.com











> On May 25, 2019, at 7:38 AM, Raffa <raffamaiden at gmail.com> wrote:
> 
> I have the following code:
> 
> ```
> 
> rm(list=ls())
> N = 30000
> xvar <- runif(N, -10, 10)
> e <- rnorm(N, mean=0, sd=1)
> yvar <- 1 + 2*xvar + e
> plot(xvar,yvar)
> lmMod <- lm(yvar~xvar)
> print(summary(lmMod))
> domain <- seq(min(xvar), max(xvar))    # define a vector of x values to 
> feed into model
> lines(domain, predict(lmMod, newdata = data.frame(xvar=domain)))    # 
> add regression line, using `predict` to generate y-values
> 
> ```
> 
> I expected the coefficients to be something similar to [1,2]. Instead R 
> keeps throwing at me random numbers that are not statistically 
> significant and don't fit the model, and I have 20k observations. For 
> example
> 
> ```
> 
> Call:
> lm(formula = yvar ~ xvar)
> 
> Residuals:
>     Min      1Q  Median      3Q     Max
> -21.384  -8.908   1.016  10.972  23.663
> 
> Coefficients:
>              Estimate Std. Error t value Pr(>|t|)
> (Intercept) 0.0007145  0.0670316   0.011    0.991
> xvar        0.0168271  0.0116420   1.445    0.148
> 
> Residual standard error: 11.61 on 29998 degrees of freedom
> Multiple R-squared:  7.038e-05,    Adjusted R-squared: 3.705e-05
> F-statistic: 2.112 on 1 and 29998 DF,  p-value: 0.1462
> 
> ```
> 
> 
> The strange thing is that the code works perfectly for N=200 or N=2000. 
> It's only for larger N that this thing happen U(for example, N=20000). I 
> have tried to ask for example in CrossValidated 
> <https://stats.stackexchange.com/questions/410050/increasing-number-of-observations-worsen-the-regression-model> 
> but the code works for them. Any help?
> 
> I am runnign R 3.6.0 on Kubuntu 19.04
> 
> Best regards
> 
> Raffaele
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From phh@80 @end|ng |rom gm@||@com  Sun May 26 20:13:04 2019
From: phh@80 @end|ng |rom gm@||@com (Paul Smith)
Date: Sun, 26 May 2019 19:13:04 +0100
Subject: [R] kableExtra: How to change kable font family
Message-ID: <CALS=5mr5=q+UFuXODCZ-bDH0v9QZzkmgGiU5BcTrG9VC8h4Xpw@mail.gmail.com>

Dear All,

Is it possible to change the font family to typewriter of the tables
generated by kable from kableExtra package?

Thanks in advance,

Paul


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon May 27 03:22:06 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sun, 26 May 2019 18:22:06 -0700
Subject: [R] kableExtra: How to change kable font family
In-Reply-To: <CALS=5mr5=q+UFuXODCZ-bDH0v9QZzkmgGiU5BcTrG9VC8h4Xpw@mail.gmail.com>
References: <CALS=5mr5=q+UFuXODCZ-bDH0v9QZzkmgGiU5BcTrG9VC8h4Xpw@mail.gmail.com>
Message-ID: <8E87274F-FECA-4ED1-AA07-B30EF3CA6421@dcn.davis.ca.us>

Perhaps... ?cell_spec

On May 26, 2019 11:13:04 AM PDT, Paul Smith <phhs80 at gmail.com> wrote:
>Dear All,
>
>Is it possible to change the font family to typewriter of the tables
>generated by kable from kableExtra package?
>
>Thanks in advance,
>
>Paul
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Mon May 27 08:33:08 2019
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Mon, 27 May 2019 08:33:08 +0200
Subject: [R] r kernlab find best cost parameter automatically
In-Reply-To: <CAB8pepyEF18KU1vfYKTGHuR3ZwcrDq3dxYqdKR5R8vhSxP6jYw@mail.gmail.com>
References: <CAMk+s2RifHVnp2VtpB-C_DS5F8_Zk5O77ObKX6o0TE5Qk-4vEw@mail.gmail.com>
 <CAB8pepyEF18KU1vfYKTGHuR3ZwcrDq3dxYqdKR5R8vhSxP6jYw@mail.gmail.com>
Message-ID: <CAMk+s2Q_Rr=AEDeskK1Sst3dPektwVZFOPXEC=uTHFmveaVWOA@mail.gmail.com>

That giving the best trade between sensitivity and specificity.

On Sat, May 25, 2019 at 12:47 AM Abby Spurdle <spurdle.a at gmail.com> wrote:
>
> > Would be possible to automate the selection of the best <k> value?
>
> Can you define "best", precisely?
>
>


-- 
Best regards,
Luigi


From re@dyto|e@rn90 @end|ng |rom gm@||@com  Mon May 27 09:43:42 2019
From: re@dyto|e@rn90 @end|ng |rom gm@||@com (Ready Learner)
Date: Mon, 27 May 2019 09:43:42 +0200
Subject: [R] Generating nested models for order selection tests
In-Reply-To: <CAGxFJbRPFnwBOVNcDdYNR=uQ=E2phvXwH8cG02ReM=42Y5KQpg@mail.gmail.com>
References: <CAAOtLrjwztmkQpZFP1kDVgZebafEZAROeA8fhLA4+X95fJsB1Q@mail.gmail.com>
 <CAGxFJbRPFnwBOVNcDdYNR=uQ=E2phvXwH8cG02ReM=42Y5KQpg@mail.gmail.com>
Message-ID: <CAAOtLrjYyQUYdOY7Df8RmEwRO47qvpOqt-OUpRaVW5VA2OODVA@mail.gmail.com>

Dear Bert,

Thank you for your response. I apologize for getting back to you a little
late.
I do not think that my question is statistical. As a matter of fact, I do
know what I want to do in terms of statistics. The problem is that I do not
know how I can do it via R. To be more precise my question is: Is there a
way to create a sequence of nested models via series expansion (based on a
basis function) in R?

All the best,
RL

On Fri, May 24, 2019 at 6:49 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Purely statistical questions are generally off topic here, and your query
> may fall under that rubric. But you should try searching at rseek.org and
> R task views -- https://cran.r-project.org/web/views/  -- perhaps under
> the SocialScience heading or others that may use the methodology to which
> you refer.
>
> Cheers,
> Bert
>
> On Fri, May 24, 2019 at 8:48 AM Ready Learner <readytolearn90 at gmail.com>
> wrote:
>
>> Hello everyone,
>>
>> I have created a parametric additive model for the median house price (as
>> the response) and with the number of tax forms (x1) and the number of
>> healthcare facilities (x2) as my covariates. I should mention that both of
>> the covariates have quadratic effects in my model.
>>
>> Now I want to do a hypothesis testing. I am taking the mentioned
>> parametric
>> model as my null state (hypothesis) and I want to use "order selection
>> test" to test it against a nonparametric alternative hypothesis. Based on
>> what I understood from few related articles I have read, I should create a
>> sequence of nested models. I am thinking about using polynomial or cosine
>> functions as my basis function. In either case, I have to create a series
>> of models (i.e. the sequence of nested models via series expansion) based
>> on the basis function to test the hypothesis.
>> Is there any way to do this automatically in R?
>>
>> Kind regards,
>> readyToLearn
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From phh@80 @end|ng |rom gm@||@com  Mon May 27 10:08:32 2019
From: phh@80 @end|ng |rom gm@||@com (Paul Smith)
Date: Mon, 27 May 2019 09:08:32 +0100
Subject: [R] kableExtra: How to change kable font family
In-Reply-To: <8E87274F-FECA-4ED1-AA07-B30EF3CA6421@dcn.davis.ca.us>
References: <CALS=5mr5=q+UFuXODCZ-bDH0v9QZzkmgGiU5BcTrG9VC8h4Xpw@mail.gmail.com>
 <8E87274F-FECA-4ED1-AA07-B30EF3CA6421@dcn.davis.ca.us>
Message-ID: <CALS=5mr-SLRdtuvBGGSW-S5+LGNyzkrCYj3d1JVc-vZfJ7hN5w@mail.gmail.com>

Thanks, Jeff, but apparently there is no option to change font family
through cell_spec.

Paul


On Mon, May 27, 2019 at 2:22 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>
> Perhaps... ?cell_spec
>
> On May 26, 2019 11:13:04 AM PDT, Paul Smith <phhs80 at gmail.com> wrote:
> >Dear All,
> >
> >Is it possible to change the font family to typewriter of the tables
> >generated by kable from kableExtra package?
> >
> >Thanks in advance,
> >
> >Paul
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.


From kry|ov@r00t @end|ng |rom gm@||@com  Mon May 27 10:47:54 2019
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Mon, 27 May 2019 11:47:54 +0300
Subject: [R] 
 Increasing number of observations worsen the regression model
In-Reply-To: <1824c21d-7676-c895-dc81-352f2a0c7722@gmail.com>
References: <1824c21d-7676-c895-dc81-352f2a0c7722@gmail.com>
Message-ID: <20190527114754.52c675be@Tarkus>

On Sat, 25 May 2019 14:38:07 +0200
Raffa <raffamaiden at gmail.com> wrote:

> I have tried to ask for example in CrossValidated 
> <https://stats.stackexchange.com/questions/410050/increasing-number-of-observations-worsen-the-regression-model> 
> but the code works for them. Any help?

In the comments you note that the problem went away after you replaced
Intel MKL with OpenBLAS. This is important.

The code that fits linear models in R is somewhat complex[*]; if
you want to get to the bottom of the problem, you may have to take
parts of it and feed them differently-sized linear regression problems
until you narrow it down to a specific set of calls to BLAS or LAPACK
functions which Intel MKL provides.

One option would be to ask at Intel MKL forums[**].

-- 
Best regards,
Ivan

[*]
https://madrury.github.io/jekyll/update/statistics/2016/07/20/lm-in-R.html

[**] https://software.intel.com/en-us/forums/intel-math-kernel-library/


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon May 27 15:17:41 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 27 May 2019 06:17:41 -0700
Subject: [R] kableExtra: How to change kable font family
In-Reply-To: <CALS=5mr-SLRdtuvBGGSW-S5+LGNyzkrCYj3d1JVc-vZfJ7hN5w@mail.gmail.com>
References: <CALS=5mr5=q+UFuXODCZ-bDH0v9QZzkmgGiU5BcTrG9VC8h4Xpw@mail.gmail.com>
 <8E87274F-FECA-4ED1-AA07-B30EF3CA6421@dcn.davis.ca.us>
 <CALS=5mr-SLRdtuvBGGSW-S5+LGNyzkrCYj3d1JVc-vZfJ7hN5w@mail.gmail.com>
Message-ID: <975D661F-9190-47BB-9922-9BA7B51119B4@dcn.davis.ca.us>

What about the "monospace" setting?

On May 27, 2019 1:08:32 AM PDT, Paul Smith <phhs80 at gmail.com> wrote:
>Thanks, Jeff, but apparently there is no option to change font family
>through cell_spec.
>
>Paul
>
>
>On Mon, May 27, 2019 at 2:22 AM Jeff Newmiller
><jdnewmil at dcn.davis.ca.us> wrote:
>>
>> Perhaps... ?cell_spec
>>
>> On May 26, 2019 11:13:04 AM PDT, Paul Smith <phhs80 at gmail.com> wrote:
>> >Dear All,
>> >
>> >Is it possible to change the font family to typewriter of the tables
>> >generated by kable from kableExtra package?
>> >
>> >Thanks in advance,
>> >
>> >Paul
>> >
>> >______________________________________________
>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >PLEASE do read the posting guide
>> >http://www.R-project.org/posting-guide.html
>> >and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Sent from my phone. Please excuse my brevity.

-- 
Sent from my phone. Please excuse my brevity.


From phh@80 @end|ng |rom gm@||@com  Mon May 27 16:19:33 2019
From: phh@80 @end|ng |rom gm@||@com (Paul Smith)
Date: Mon, 27 May 2019 15:19:33 +0100
Subject: [R] kableExtra: How to change kable font family
In-Reply-To: <975D661F-9190-47BB-9922-9BA7B51119B4@dcn.davis.ca.us>
References: <CALS=5mr5=q+UFuXODCZ-bDH0v9QZzkmgGiU5BcTrG9VC8h4Xpw@mail.gmail.com>
 <8E87274F-FECA-4ED1-AA07-B30EF3CA6421@dcn.davis.ca.us>
 <CALS=5mr-SLRdtuvBGGSW-S5+LGNyzkrCYj3d1JVc-vZfJ7hN5w@mail.gmail.com>
 <975D661F-9190-47BB-9922-9BA7B51119B4@dcn.davis.ca.us>
Message-ID: <CALS=5mofM-S3rDojCCTXc+ACzWa4bKQJATCt04cNyaPJWn=RfA@mail.gmail.com>

Yes, Jeff, that works! Thanks!

However, if I wanted to change the font family to a family different
of typewriter, I guess it would be impossible.

Paul

On Mon, May 27, 2019 at 2:17 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>
> What about the "monospace" setting?
>
> On May 27, 2019 1:08:32 AM PDT, Paul Smith <phhs80 at gmail.com> wrote:
> >Thanks, Jeff, but apparently there is no option to change font family
> >through cell_spec.
> >
> >Paul
> >
> >
> >On Mon, May 27, 2019 at 2:22 AM Jeff Newmiller
> ><jdnewmil at dcn.davis.ca.us> wrote:
> >>
> >> Perhaps... ?cell_spec
> >>
> >> On May 26, 2019 11:13:04 AM PDT, Paul Smith <phhs80 at gmail.com> wrote:
> >> >Dear All,
> >> >
> >> >Is it possible to change the font family to typewriter of the tables
> >> >generated by kable from kableExtra package?
> >> >
> >> >Thanks in advance,
> >> >
> >> >Paul
> >> >
> >> >______________________________________________
> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >https://stat.ethz.ch/mailman/listinfo/r-help
> >> >PLEASE do read the posting guide
> >> >http://www.R-project.org/posting-guide.html
> >> >and provide commented, minimal, self-contained, reproducible code.
> >>
> >> --
> >> Sent from my phone. Please excuse my brevity.
>
> --
> Sent from my phone. Please excuse my brevity.


From r@||@m@|den @end|ng |rom gm@||@com  Sun May 26 16:09:12 2019
From: r@||@m@|den @end|ng |rom gm@||@com (Raffa)
Date: Sun, 26 May 2019 16:09:12 +0200
Subject: [R] 
 Increasing number of observations worsen the regression model
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC836BC1D99@FHSDB2D11-2.csu.mcmaster.ca>
References: <14360_1558877817_x4QDavRJ002449_1824c21d-7676-c895-dc81-352f2a0c7722@gmail.com>
 <ACD1644AA6C67E4FBD0C350625508EC836BC1D99@FHSDB2D11-2.csu.mcmaster.ca>
Message-ID: <31e5795e-0080-a336-f3d7-3c9fd709ea43@gmail.com>

I have solved the problem. It was caused by Intel MKL. Uninstalling 
Intel MKL and using OpenBLAS instead fixed the problem completely.

Notice that, using Intel MKL, I was able to reproduce the problem by 
computing the regression coefficients directly from the usual formula, 
which were returned completely wrong.

My guess is that some optimization of Intel MKL which is activated in 
"large" matrices give completely wrong result (I don't know which 
operation exactly)

Thanks,

Best,

Raffaele Mancuso

On 26/05/19 16:06, Fox, John wrote:
> Dear Raffaele,
>
> Using your code, with one modification -- setting the seed for R's random number generator to make the result reproducible -- I get:
>
>> set.seed(12345)
> . . .
>
>> lmMod <- lm(yvar~xvar)
>> print(summary(lmMod))
> Call:
> lm(formula = yvar ~ xvar)
>
> Residuals:
>      Min      1Q  Median      3Q     Max
> -4.0293 -0.6732  0.0021  0.6749  4.2883
>
> Coefficients:
>               Estimate Std. Error t value Pr(>|t|)
> (Intercept) 1.0057713  0.0057529   174.8   <2e-16 ***
> xvar        2.0000889  0.0009998  2000.4   <2e-16 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Residual standard error: 0.9964 on 29998 degrees of freedom
> Multiple R-squared:  0.9926,	Adjusted R-squared:  0.9926
> F-statistic: 4.002e+06 on 1 and 29998 DF,  p-value: < 2.2e-16
>
> which is more or less what one would expect.
>
> My guess: you've saved your R workspace from a previous session, and it is then loaded at the start of your R session; something in the saved workspace is affecting the result, although frankly I can't think what that might be.
>
> I hope this helps,
>   John
>
> -----------------------------------------------------------------
> John Fox
> Professor Emeritus
> McMaster University
> Hamilton, Ontario, Canada
> Web: https://socialsciences.mcmaster.ca/jfox/
>
>
>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Raffa
>> Sent: Saturday, May 25, 2019 8:38 AM
>> To: r-help at r-project.org
>> Subject: [R] Increasing number of observations worsen the regression model
>>
>> I have the following code:
>>
>> ```
>>
>> rm(list=ls())
>> N = 30000
>> xvar <- runif(N, -10, 10)
>> e <- rnorm(N, mean=0, sd=1)
>> yvar <- 1 + 2*xvar + e
>> plot(xvar,yvar)
>> lmMod <- lm(yvar~xvar)
>> print(summary(lmMod))
>> domain <- seq(min(xvar), max(xvar))??? # define a vector of x values to feed
>> into model lines(domain, predict(lmMod, newdata =
>> data.frame(xvar=domain)))??? # add regression line, using `predict` to generate
>> y-values
>>
>> ```
>>
>> I expected the coefficients to be something similar to [1,2]. Instead R keeps
>> throwing at me random numbers that are not statistically significant and don't
>> fit the model, and I have 20k observations. For example
>>
>> ```
>>
>> Call:
>> lm(formula = yvar ~ xvar)
>>
>> Residuals:
>>   ??? Min????? 1Q? Median????? 3Q???? Max
>> -21.384? -8.908?? 1.016? 10.972? 23.663
>>
>> Coefficients:
>>   ???????????? Estimate Std. Error t value Pr(>|t|)
>> (Intercept) 0.0007145? 0.0670316?? 0.011??? 0.991
>> xvar??????? 0.0168271? 0.0116420?? 1.445??? 0.148
>>
>> Residual standard error: 11.61 on 29998 degrees of freedom Multiple R-
>> squared:? 7.038e-05,??? Adjusted R-squared: 3.705e-05
>> F-statistic: 2.112 on 1 and 29998 DF,? p-value: 0.1462
>>
>> ```
>>
>>
>> The strange thing is that the code works perfectly for N=200 or N=2000.
>> It's only for larger N that this thing happen U(for example, N=20000). I have
>> tried to ask for example in CrossValidated
>> <https://stats.stackexchange.com/questions/410050/increasing-number-of-
>> observations-worsen-the-regression-model>
>> but the code works for them. Any help?
>>
>> I am runnign R 3.6.0 on Kubuntu 19.04
>>
>> Best regards
>>
>> Raffaele
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From @purd|e@@ @end|ng |rom gm@||@com  Tue May 28 03:27:41 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Tue, 28 May 2019 13:27:41 +1200
Subject: [R] cspade {arulesSequences} error
In-Reply-To: <000001d511b2$f56c1610$e0444230$@sbcglobal.net>
References: <000001d511b2$f56c1610$e0444230$@sbcglobal.net>
Message-ID: <CAB8pepyk0ttHbEWgexT1oRbaZeq6NV0s25Au106guaCYyQxyXw@mail.gmail.com>

> s1 <- cspade(trans_matrix, parameter = list(support = 0.3), control =
> list(verbose = TRUE))

> error in file (con, "r"): cannot open the connection. cannot open file
> 'D:\Temp\cspade13403a927eaa.out': no such file or directory

Note that you haven't provided a fully reproducible example.
(As I don't know what trans_matrix is, and I'm not sure at what point the
error message is generated, which is important).

I installed the lasted versions of arules and arulesSequences.
I tried to run one of the examples from the cspade() function's
documentation.
It ran without any problems.

Also note the location "D:\Temp" is suspicious. This is not where I would
expect temporary files to be created, and makes me suspect there's
something (somewhere on your computer) that's not typical. This could
include your working directory, startup script files (if any), environment
variables (if any) or other parts of your R code, or any OS settings.

Sorry, I know that's not the most helpful answer...

	[[alternative HTML version deleted]]


From pd@|gd @end|ng |rom gm@||@com  Tue May 28 09:33:06 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Tue, 28 May 2019 09:33:06 +0200
Subject: [R] cspade {arulesSequences} error
In-Reply-To: <000001d511b2$f56c1610$e0444230$@sbcglobal.net>
References: <000001d511b2$f56c1610$e0444230$@sbcglobal.net>
Message-ID: <DA5470BC-DFB2-4FAC-9144-C1EB71158796@gmail.com>

Antivirus checkers sometimes temporarily move new files to a "safe location" for checking. That can produce this kind of effect. 

-pd

> On 24 May 2019, at 00:00 , reichmanj at sbcglobal.net wrote:
> 
> R-Help
> 
> 
> 
> When I run the cspade function from the arulesSequences package...
> 
> 
> 
> s1 <- cspade(trans_matrix, parameter = list(support = 0.3), control =
> list(verbose = TRUE))
> 
> #s1 <- cspade(trans_matrix, parameter = list(support = 0.3), control =
> list(verbose = TRUE), tmpdir = "C:\\Temp")
> 
> 
> 
> I receive the following error .
> 
> error in file (con, "r"): cannot open the connection. cannot open file
> 'D:\Temp\cspade13403a927eaa.out': no such file or directory
> 
> 
> 
> now the program works just fine at home so the issue is with my  work
> computer.  I suspect its some sort of read/write issue but the functions
> seems to write the files out just fine then deletes them and errors out.
> 
> 
> 
> Any suggestions
> 
> 
> 
> Jeff Reichamn
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From @t@r@kykwe@| @end|ng |rom gm@||@com  Tue May 28 12:42:08 2019
From: @t@r@kykwe@| @end|ng |rom gm@||@com (Kwesi A. Quagraine)
Date: Tue, 28 May 2019 03:42:08 -0700
Subject: [R] URGENT help-Problem with panel barplot spacing
Message-ID: <CAGD2cKc1sW2aO+H1vw8zZ=Z4K72mfAq0ROAcoT4sBKz+it1r1g@mail.gmail.com>

Hello All,

I am struggling to control the spaces between barplots I have panelled. I
would be grateful for any help in reducing the spaces between the plots.
For instance, how I can reduce distance between 1 and 2, 2 and 3 etc.

I would appreciate any help on this.

Thanks

Here?s a snippet of my code and attached is the current image I generate
from the command;

##for djf
postscript("fig_paper2_fre_obs_models_djf_1980_2013_4x3_new.eps",width=10,height=8,paper="special",horizontal=T,onefile=T)

par(mfrow=c(3,4))
par(mar=c(6,11,1,2))

a =1
for (j in a) {
  djf.bar<- barplot(djf.gcms[,j],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
j,type="n",font.main = 1, cex.main = 1.5,las=1, axisnames = T, width =
0.8,cex.names=1.5,horiz = TRUE)
  abline(v=c(seq(0,50,10)),col='grey',lwd=0.2)
  par(new=TRUE)
  djf.bar<-barplot(djf.gcms[,j],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
j,font.main = 1, cex.main = 1.5,las=1, axisnames = T, width =
0.8,cex.names=1.5,horiz = TRUE)
  ## Add text at top of bars
  text(y = djf.bar, x = djf.gcms[,j], label = round(djf.gcms[,j],
digits=0), pos = 4, cex = 1.2)
}


for (i in 2:4) {
  djf.bar<-barplot(djf.gcms[,i],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
i, yaxs="i",type="n",font.main = 1, cex.main = 1.1,las=1, axisnames =
FALSE, width = 0.8,cex.names=1.0,horiz = TRUE)
  abline(v=c(seq(0,50,10)),col='grey',lwd=0.2)
  par(new=TRUE)
  djf.bar<-barplot(djf.gcms[,i],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
i, yaxs="i",font.main = 1, cex.main = 1.1,las=1, axisnames = FALSE, width =
0.8,cex.names=1.0,horiz = TRUE)
  text(y = djf.bar, x = djf.gcms[,i], label = round(djf.gcms[,i],
digits=0), pos = 4, cex = 1.0)

}

b=5
for (k in b) {
  djf.bar<-barplot(djf.gcms[,k],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
k,type="n",font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
0.8,cex.names=1.5,horiz = TRUE)
  abline(v=c(seq(0,50,10)),col='grey',lwd=0.2)
  par(new=TRUE)
  djf.bar<-barplot(djf.gcms[,k],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
k,font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
0.8,cex.names=1.5,horiz = TRUE)
  text(y = djf.bar, x = djf.gcms[,k], label = round(djf.gcms[,k],
digits=0), pos = 4, cex = 1.2)

}

for (n in 6:8) {
  barplot(djf.gcms[,n],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
n,type="n",font.main = 1, cex.main = 1.1,las=1, axisnames = FALSE, width =
0.8,cex.names=1.0,horiz = TRUE)
  abline(v=c(seq(0,50,10)),col='grey',lwd=0.2)
  par(new=TRUE)
  barplot(djf.gcms[,n],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
n,font.main = 1, cex.main = 1.1,las=1, axisnames = FALSE, width =
0.8,cex.names=1.0,horiz = TRUE)
  text(y = djf.bar, x = djf.gcms[,n], label = round(djf.gcms[,n],
digits=0), pos = 4, cex = 1.0)

}

c = 9
for (m in c) {
  djf.bar<-barplot(djf.gcms[,m],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
m,type="n",font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
0.8,cex.names=1.5,horiz = TRUE)
  abline(v=c(seq(0,50,10)),col='grey',lwd=0.2)
  par(new=TRUE)
  djf.bar<-barplot(djf.gcms[,m],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
m,font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
0.8,cex.names=1.5,horiz = TRUE)
  title("Frequency (%)", line = -17.0)
  text(y = djf.bar, x = djf.gcms[,m], label = round(djf.gcms[,m],
digits=0), pos = 4, cex = 1.2)

}

for (i in 10:12) {
  djf.bar<-barplot(djf.gcms[,i],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
i,type="n",font.main = 1, cex.main = 1.1,las=1, axisnames = F, width =
0.8,cex.names=1.0,horiz = TRUE)
  abline(v=c(seq(0,50,10)),col='grey',lwd=0.2)
  par(new=TRUE)
  djf.bar<-barplot(djf.gcms[,i],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
i,font.main = 1, cex.main = 1.1,las=1, axisnames = F, width =
0.8,cex.names=1.0,horiz = TRUE) #xlab="Frequency (%)"
  title("Frequency (%)", line = -17.0)
  text(y = djf.bar, x = djf.gcms[,i], label = round(djf.gcms[,i],
digits=0), pos = 4, cex = 1.0)

}
dev.off()



------------
Try not to become a man of success but rather a man of value- Albert
Einstein

Kwesi A. Quagraine
Department of Physics
School of Physical Sciences
College of Agriculture and Natural Sciences
University of Cape Coast
Cape Coast, Ghana

Alt. Email: kwesi at csag.uct.ac.za
Web: http://www.recycleupghana.org/
Office: +27 21 650 3164
Skype: quagraine_cwasi

From reichm@@j m@iii@g oii sbcgiob@i@@et  Tue May 28 12:56:13 2019
From: reichm@@j m@iii@g oii sbcgiob@i@@et (reichm@@j m@iii@g oii sbcgiob@i@@et)
Date: Tue, 28 May 2019 05:56:13 -0500
Subject: [R] cspade {arulesSequences} error
In-Reply-To: <CAB8pepyk0ttHbEWgexT1oRbaZeq6NV0s25Au106guaCYyQxyXw@mail.gmail.com>
References: <000001d511b2$f56c1610$e0444230$@sbcglobal.net>
 <CAB8pepyk0ttHbEWgexT1oRbaZeq6NV0s25Au106guaCYyQxyXw@mail.gmail.com>
Message-ID: <000401d51543$f85eeb60$e91cc220$@sbcglobal.net>

Abby

 

My code runs just fine at home but when I transfer it to work and rerun it error out at the line shown. Even errors out when I try the example sin the package documentation. So I?m pretty sure is isn?t my code but rather how our IT folks have configured our work systems. Peter Dalgaard suggested that it might be our antivirus so I will look into that. As far as the D:\Temp location I changed that from the default thinking it may have been a read/write issue with my system.

 

Jeff

 

From: Abby Spurdle <spurdle.a at gmail.com> 
Sent: Monday, May 27, 2019 8:28 PM
To: reichmanj at sbcglobal.net
Cc: r-help <R-help at r-project.org>
Subject: Re: [R] cspade {arulesSequences} error

 

> s1 <- cspade(trans_matrix, parameter = list(support = 0.3), control =
> list(verbose = TRUE))

> error in file (con, "r"): cannot open the connection. cannot open file
> 'D:\Temp\cspade13403a927eaa.out': no such file or directory

Note that you haven't provided a fully reproducible example.

(As I don't know what trans_matrix is, and I'm not sure at what point the error message is generated, which is important).

 

I installed the lasted versions of arules and arulesSequences.

I tried to run one of the examples from the cspade() function's documentation.

It ran without any problems.

 

Also note the location "D:\Temp" is suspicious. This is not where I would expect temporary files to be created, and makes me suspect there's something (somewhere on your computer) that's not typical. This could include your working directory, startup script files (if any), environment variables (if any) or other parts of your R code, or any OS settings.

 

Sorry, I know that's not the most helpful answer...

 

 


	[[alternative HTML version deleted]]


From r-p@ck@ge@ @end|ng |rom r-project@org  Tue May 28 14:14:14 2019
From: r-p@ck@ge@ @end|ng |rom r-project@org (Marc Schwartz via R-packages)
Date: Tue, 28 May 2019 08:14:14 -0400
Subject: [R] [R-pkgs] WriteXLS version 5.0.0 now available
Message-ID: <63DFFC50-CA0B-48EC-B09A-E11AD86715BD@me.com>

Hi all,

I am pleased to announce that WriteXLS version 5.0.0 is now available on CRAN:

  https://cran.r-project.org/web/packages/WriteXLS/index.html

This supersedes version 4.1.0 and should appear on your local CRAN mirror in due course, if not there already.

WriteXLS, first released in 2009, is a cross-platform, Perl-based R function to create Excel 2003 (XLS) and Excel 2007 (XLSX) files from one or more data frames. 

The primary change in this major new version is improved support for Unicode/UTF-8 variable width character sets, especially on Windows. This is accomplished via the use of writeLines(..., useBytes = TRUE), rather than write.table(), to create intermediate CSV files from the source R data frames. These exported CSV files are then parsed by Perl scripts to create the resultant Excel file.

The prior use of write.table() to create the CSV files could result in the re-encoding of the content of the CSV files into the locale of the computer upon which the code is running. This could result in the loss of the source character encoding used in the originating R data frames, where variable width character sets are in use. This was primarily an issue on Windows, given the limitations of Unicode/UTF-8 support on that OS.

I want to thank Hamid Moshen for first raising this issue in late March, providing test datasets in multiple Unicode/UTF-8 character sets that I could use to reproduce and correct the issues, and his testing of the changes required.

Warm regards,

Marc Schwartz

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From @hmed@t|@80 @end|ng |rom gm@||@com  Tue May 28 18:07:41 2019
From: @hmed@t|@80 @end|ng |rom gm@||@com (Ahmed Attia)
Date: Tue, 28 May 2019 18:07:41 +0200
Subject: [R] Graphic functions in the ade4TKGUI graphic user interface in R
Message-ID: <CAG6S0OmF62Vd=KkZ1+s=tj2US1jse7s0Ls7LnyEpF2pGzWP1+w@mail.gmail.com>

I have a problem with the graphic functions in the ade4TKGUI graphic
user interface in R. I want to reset the label size and boxes options,
but get this error message

 Error in (function ()  : object 'pmvar' not found

or this error message

Error in s.label(xax = 1, yax = 2, plabels.cex = 1, plabels.boxes.draw
= TRUE,  : non convenient selection for dfxy (can not be converted to
dataframe)

Any suggestions. Thanks






Ahmed Attia, Ph.D.
Agronomist & Crop Modeler


From jw@ng @end|ng |rom c@ub@edu  Tue May 28 18:51:02 2019
From: jw@ng @end|ng |rom c@ub@edu (Jianjun Wang)
Date: Tue, 28 May 2019 16:51:02 +0000
Subject: [R] Error in using tokens script
Message-ID: <BYAPR07MB421365019576755228B2D5EBD21E0@BYAPR07MB4213.namprd07.prod.outlook.com>

Hi, Colleagues.

As shown at the bottom, I was able to obtain information from a column (named "Message") of a text data (named "SMSSpamCollection" (i.e., SMSSpamCollection$Message[44]).  But when I ran
SMSSpamCollection.token <-tokens(SMSSpamCollection$Message, what="word",
+                                  remove_url = TRUE,
+                                  remove_numbers= TRUE, remove_punct= TRUE,
+                                  remove_symbols= TRUE, remove_hyphens= TRUE)
The feedback was
Error in tokens.default(SMSSpamCollection$Message, what = "word", remove_url = TRUE,  :
  tokens() only works on character, corpus, tokens objects.

I would appreciate some guidance to fix the error message!

Thank you.

JJ

> SMSSpamCollection$Message[44]
[1] Great! I hope you like your man well endowed. I am  &lt;#&gt;  inches...
3026 Levels: 'An Amazing Quote'' - Sometimes in life its difficult to decide whats wrong!! a lie that brings a smile or the truth that brings a tear.... ...

	[[alternative HTML version deleted]]


From rmh @end|ng |rom temp|e@edu  Tue May 28 19:55:54 2019
From: rmh @end|ng |rom temp|e@edu (Richard M. Heiberger)
Date: Tue, 28 May 2019 13:55:54 -0400
Subject: [R] URGENT help-Problem with panel barplot spacing
In-Reply-To: <CAGD2cKc1sW2aO+H1vw8zZ=Z4K72mfAq0ROAcoT4sBKz+it1r1g@mail.gmail.com>
References: <CAGD2cKc1sW2aO+H1vw8zZ=Z4K72mfAq0ROAcoT4sBKz+it1r1g@mail.gmail.com>
Message-ID: <CAGx1TMD4pm70=J281325QeEUbSna9ZyYJFCP3nEZwcc5CROkGA@mail.gmail.com>

I think this is what you want.  You didn't send a reproducible example
(no values for djf.gcms or for cores1).

For what I think you are doing, lattice would be much simpler.  It
handles the repetition within each panel for you.

## generate some data
djf.gcms <- matrix(sample(50, size=9*12, replace=TRUE), 9, 12)

library(lattice)
library(latticeExtra)

djfs <- cbind(stack(data.frame(djf.gcms)), letter=factor(letters[1:9]))
head(djfs)

tmp <-
barchart(letter ~ values | ind, group=letter, col=1:9, data=djfs,
horizontal=TRUE,
         stack=TRUE, type="i",
         scales=list(x=list(alternating=FALSE, axs="i", limits=c(0,59))),
         origin=0, layout=c(4, 3), between=list(x=1, y=2)) +
  layer(panel.text(x=x+5, y, label=x)) +
  layer(panel.abline(v=seq(0,50,10), col="gray"), under=TRUE)
tmp

Rich

The best place to start learning lattice is the trellis book
http://geog.uoregon.edu/GeogR/pdfs/trellis.user.pdf

The definitive reference is Deepayan Sarkar's book,
 Lattice: Multivariate Data Visualization with R
https://www.e-reading.club/bookreader.php/137342/Lattice._Multivariate_Data_Visualization_with_R.pdf

My book (HH2) is
Heiberger, Richard M. and Holland, Burt (2015).
 Statistical Analysis and Data Display: An Intermediate Course with
Examples in R.
 Springer, second edition. ISBN 978-1-4939- 2121-8.
https://www.springer.com/us/book/9781493921218

See HH2 Chapter 4 Graphs for a general discussion
and many examples throughout the book and in the accompanying CRAN package HH.

install.packages("HH")

On Tue, May 28, 2019 at 6:42 AM Kwesi A. Quagraine
<starskykwesi at gmail.com> wrote:
>
> Hello All,
>
> I am struggling to control the spaces between barplots I have panelled. I
> would be grateful for any help in reducing the spaces between the plots.
> For instance, how I can reduce distance between 1 and 2, 2 and 3 etc.
>
> I would appreciate any help on this.
>
> Thanks
>
> Here?s a snippet of my code and attached is the current image I generate
> from the command;
>
> ##for djf
> postscript("fig_paper2_fre_obs_models_djf_1980_2013_4x3_new.eps",width=10,height=8,paper="special",horizontal=T,onefile=T)
>
> par(mfrow=c(3,4))
> par(mar=c(6,11,1,2))
>
> a =1
> for (j in a) {
>   djf.bar<- barplot(djf.gcms[,j],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> j,type="n",font.main = 1, cex.main = 1.5,las=1, axisnames = T, width =
> 0.8,cex.names=1.5,horiz = TRUE)
>   abline(v=c(seq(0,50,10)),col='grey',lwd=0.2)
>   par(new=TRUE)
>   djf.bar<-barplot(djf.gcms[,j],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> j,font.main = 1, cex.main = 1.5,las=1, axisnames = T, width =
> 0.8,cex.names=1.5,horiz = TRUE)
>   ## Add text at top of bars
>   text(y = djf.bar, x = djf.gcms[,j], label = round(djf.gcms[,j],
> digits=0), pos = 4, cex = 1.2)
> }
>
>
> for (i in 2:4) {
>   djf.bar<-barplot(djf.gcms[,i],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> i, yaxs="i",type="n",font.main = 1, cex.main = 1.1,las=1, axisnames =
> FALSE, width = 0.8,cex.names=1.0,horiz = TRUE)
>   abline(v=c(seq(0,50,10)),col='grey',lwd=0.2)
>   par(new=TRUE)
>   djf.bar<-barplot(djf.gcms[,i],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> i, yaxs="i",font.main = 1, cex.main = 1.1,las=1, axisnames = FALSE, width =
> 0.8,cex.names=1.0,horiz = TRUE)
>   text(y = djf.bar, x = djf.gcms[,i], label = round(djf.gcms[,i],
> digits=0), pos = 4, cex = 1.0)
>
> }
>
> b=5
> for (k in b) {
>   djf.bar<-barplot(djf.gcms[,k],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> k,type="n",font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
> 0.8,cex.names=1.5,horiz = TRUE)
>   abline(v=c(seq(0,50,10)),col='grey',lwd=0.2)
>   par(new=TRUE)
>   djf.bar<-barplot(djf.gcms[,k],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> k,font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
> 0.8,cex.names=1.5,horiz = TRUE)
>   text(y = djf.bar, x = djf.gcms[,k], label = round(djf.gcms[,k],
> digits=0), pos = 4, cex = 1.2)
>
> }
>
> for (n in 6:8) {
>   barplot(djf.gcms[,n],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> n,type="n",font.main = 1, cex.main = 1.1,las=1, axisnames = FALSE, width =
> 0.8,cex.names=1.0,horiz = TRUE)
>   abline(v=c(seq(0,50,10)),col='grey',lwd=0.2)
>   par(new=TRUE)
>   barplot(djf.gcms[,n],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> n,font.main = 1, cex.main = 1.1,las=1, axisnames = FALSE, width =
> 0.8,cex.names=1.0,horiz = TRUE)
>   text(y = djf.bar, x = djf.gcms[,n], label = round(djf.gcms[,n],
> digits=0), pos = 4, cex = 1.0)
>
> }
>
> c = 9
> for (m in c) {
>   djf.bar<-barplot(djf.gcms[,m],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> m,type="n",font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
> 0.8,cex.names=1.5,horiz = TRUE)
>   abline(v=c(seq(0,50,10)),col='grey',lwd=0.2)
>   par(new=TRUE)
>   djf.bar<-barplot(djf.gcms[,m],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> m,font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
> 0.8,cex.names=1.5,horiz = TRUE)
>   title("Frequency (%)", line = -17.0)
>   text(y = djf.bar, x = djf.gcms[,m], label = round(djf.gcms[,m],
> digits=0), pos = 4, cex = 1.2)
>
> }
>
> for (i in 10:12) {
>   djf.bar<-barplot(djf.gcms[,i],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> i,type="n",font.main = 1, cex.main = 1.1,las=1, axisnames = F, width =
> 0.8,cex.names=1.0,horiz = TRUE)
>   abline(v=c(seq(0,50,10)),col='grey',lwd=0.2)
>   par(new=TRUE)
>   djf.bar<-barplot(djf.gcms[,i],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> i,font.main = 1, cex.main = 1.1,las=1, axisnames = F, width =
> 0.8,cex.names=1.0,horiz = TRUE) #xlab="Frequency (%)"
>   title("Frequency (%)", line = -17.0)
>   text(y = djf.bar, x = djf.gcms[,i], label = round(djf.gcms[,i],
> digits=0), pos = 4, cex = 1.0)
>
> }
> dev.off()
>
>
>
> ------------
> Try not to become a man of success but rather a man of value- Albert
> Einstein
>
> Kwesi A. Quagraine
> Department of Physics
> School of Physical Sciences
> College of Agriculture and Natural Sciences
> University of Cape Coast
> Cape Coast, Ghana
>
> Alt. Email: kwesi at csag.uct.ac.za
> Web: http://www.recycleupghana.org/
> Office: +27 21 650 3164
> Skype: quagraine_cwasi
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Tue May 28 20:41:22 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Tue, 28 May 2019 19:41:22 +0100
Subject: [R] 
 Graphic functions in the ade4TKGUI graphic user interface in R
In-Reply-To: <CAG6S0OmF62Vd=KkZ1+s=tj2US1jse7s0Ls7LnyEpF2pGzWP1+w@mail.gmail.com>
References: <CAG6S0OmF62Vd=KkZ1+s=tj2US1jse7s0Ls7LnyEpF2pGzWP1+w@mail.gmail.com>
Message-ID: <05ed7f7d-deef-48e8-a132-f3ab117442ea@sapo.pt>

Hello,

Cross-posting is not welcome, you should wait a few days for a SO [1] 
answer before posting to R-Help.

[1] 
https://stackoverflow.com/questions/56347402/graphic-functions-in-the-ade4tkgui-graphic-user-interface-in-r

Rui Barradas

?s 17:07 de 28/05/19, Ahmed Attia escreveu:
> I have a problem with the graphic functions in the ade4TKGUI graphic
> user interface in R. I want to reset the label size and boxes options,
> but get this error message
> 
>   Error in (function ()  : object 'pmvar' not found
> 
> or this error message
> 
> Error in s.label(xax = 1, yax = 2, plabels.cex = 1, plabels.boxes.draw
> = TRUE,  : non convenient selection for dfxy (can not be converted to
> dataframe)
> 
> Any suggestions. Thanks
> 
> 
> 
> 
> 
> 
> Ahmed Attia, Ph.D.
> Agronomist & Crop Modeler
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @t@r@kykwe@| @end|ng |rom gm@||@com  Tue May 28 22:47:14 2019
From: @t@r@kykwe@| @end|ng |rom gm@||@com (Kwesi A. Quagraine)
Date: Tue, 28 May 2019 13:47:14 -0700
Subject: [R] URGENT help-Problem with panel barplot spacing
In-Reply-To: <CAGx1TMD4pm70=J281325QeEUbSna9ZyYJFCP3nEZwcc5CROkGA@mail.gmail.com>
References: <CAGD2cKc1sW2aO+H1vw8zZ=Z4K72mfAq0ROAcoT4sBKz+it1r1g@mail.gmail.com>
 <CAGx1TMD4pm70=J281325QeEUbSna9ZyYJFCP3nEZwcc5CROkGA@mail.gmail.com>
Message-ID: <CAGD2cKcX6fHdUL4_scRp3ArqxAhL++z90-QoGqz+QUxZ9bgELw@mail.gmail.com>

Thanks for the code, Rich. I now get the concept of using the lattice
library.

Most grateful for the references shared.

Regards
Kwesi

On 28 May 2019 at 19:56:06, Richard M. Heiberger (rmh at temple.edu) wrote:

I think this is what you want. You didn't send a reproducible example
(no values for djf.gcms or for cores1).

For what I think you are doing, lattice would be much simpler. It
handles the repetition within each panel for you.

## generate some data
djf.gcms <- matrix(sample(50, size=9*12, replace=TRUE), 9, 12)

library(lattice)
library(latticeExtra)

djfs <- cbind(stack(data.frame(djf.gcms)), letter=factor(letters[1:9]))
head(djfs)

tmp <-
barchart(letter ~ values | ind, group=letter, col=1:9, data=djfs,
horizontal=TRUE,
stack=TRUE, type="i",
scales=list(x=list(alternating=FALSE, axs="i", limits=c(0,59))),
origin=0, layout=c(4, 3), between=list(x=1, y=2)) +
layer(panel.text(x=x+5, y, label=x)) +
layer(panel.abline(v=seq(0,50,10), col="gray"), under=TRUE)
tmp

Rich

The best place to start learning lattice is the trellis book
http://geog.uoregon.edu/GeogR/pdfs/trellis.user.pdf

The definitive reference is Deepayan Sarkar's book,
Lattice: Multivariate Data Visualization with R
https://www.e-reading.club/bookreader.php/137342/Lattice._Multivariate_Data_Visualization_with_R.pdf

My book (HH2) is
Heiberger, Richard M. and Holland, Burt (2015).
Statistical Analysis and Data Display: An Intermediate Course with
Examples in R.
Springer, second edition. ISBN 978-1-4939- 2121-8.
https://www.springer.com/us/book/9781493921218

See HH2 Chapter 4 Graphs for a general discussion
and many examples throughout the book and in the accompanying CRAN package
HH.

install.packages("HH")

On Tue, May 28, 2019 at 6:42 AM Kwesi A. Quagraine
<starskykwesi at gmail.com> wrote:
>
> Hello All,
>
> I am struggling to control the spaces between barplots I have panelled. I
> would be grateful for any help in reducing the spaces between the plots.
> For instance, how I can reduce distance between 1 and 2, 2 and 3 etc.
>
> I would appreciate any help on this.
>
> Thanks
>
> Here?s a snippet of my code and attached is the current image I generate
> from the command;
>
> ##for djf
>
postscript("fig_paper2_fre_obs_models_djf_1980_2013_4x3_new.eps",width=10,height=8,paper="special",horizontal=T,onefile=T)

>
> par(mfrow=c(3,4))
> par(mar=c(6,11,1,2))
>
> a =1
> for (j in a) {
> djf.bar<- barplot(djf.gcms[,j],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> j,type="n",font.main = 1, cex.main = 1.5,las=1, axisnames = T, width =
> 0.8,cex.names=1.5,horiz = TRUE)
> abline(v=c(seq(0,50,10)),col='grey',lwd=0.2)
> par(new=TRUE)
> djf.bar<-barplot(djf.gcms[,j],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> j,font.main = 1, cex.main = 1.5,las=1, axisnames = T, width =
> 0.8,cex.names=1.5,horiz = TRUE)
> ## Add text at top of bars
> text(y = djf.bar, x = djf.gcms[,j], label = round(djf.gcms[,j],
> digits=0), pos = 4, cex = 1.2)
> }
>
>
> for (i in 2:4) {
> djf.bar<-barplot(djf.gcms[,i],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> i, yaxs="i",type="n",font.main = 1, cex.main = 1.1,las=1, axisnames =
> FALSE, width = 0.8,cex.names=1.0,horiz = TRUE)
> abline(v=c(seq(0,50,10)),col='grey',lwd=0.2)
> par(new=TRUE)
> djf.bar<-barplot(djf.gcms[,i],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> i, yaxs="i",font.main = 1, cex.main = 1.1,las=1, axisnames = FALSE, width
=
> 0.8,cex.names=1.0,horiz = TRUE)
> text(y = djf.bar, x = djf.gcms[,i], label = round(djf.gcms[,i],
> digits=0), pos = 4, cex = 1.0)
>
> }
>
> b=5
> for (k in b) {
> djf.bar<-barplot(djf.gcms[,k],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> k,type="n",font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
> 0.8,cex.names=1.5,horiz = TRUE)
> abline(v=c(seq(0,50,10)),col='grey',lwd=0.2)
> par(new=TRUE)
> djf.bar<-barplot(djf.gcms[,k],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> k,font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
> 0.8,cex.names=1.5,horiz = TRUE)
> text(y = djf.bar, x = djf.gcms[,k], label = round(djf.gcms[,k],
> digits=0), pos = 4, cex = 1.2)
>
> }
>
> for (n in 6:8) {
> barplot(djf.gcms[,n],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> n,type="n",font.main = 1, cex.main = 1.1,las=1, axisnames = FALSE, width
=
> 0.8,cex.names=1.0,horiz = TRUE)
> abline(v=c(seq(0,50,10)),col='grey',lwd=0.2)
> par(new=TRUE)
> barplot(djf.gcms[,n],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> n,font.main = 1, cex.main = 1.1,las=1, axisnames = FALSE, width =
> 0.8,cex.names=1.0,horiz = TRUE)
> text(y = djf.bar, x = djf.gcms[,n], label = round(djf.gcms[,n],
> digits=0), pos = 4, cex = 1.0)
>
> }
>
> c = 9
> for (m in c) {
> djf.bar<-barplot(djf.gcms[,m],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> m,type="n",font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
> 0.8,cex.names=1.5,horiz = TRUE)
> abline(v=c(seq(0,50,10)),col='grey',lwd=0.2)
> par(new=TRUE)
> djf.bar<-barplot(djf.gcms[,m],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> m,font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
> 0.8,cex.names=1.5,horiz = TRUE)
> title("Frequency (%)", line = -17.0)
> text(y = djf.bar, x = djf.gcms[,m], label = round(djf.gcms[,m],
> digits=0), pos = 4, cex = 1.2)
>
> }
>
> for (i in 10:12) {
> djf.bar<-barplot(djf.gcms[,i],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> i,type="n",font.main = 1, cex.main = 1.1,las=1, axisnames = F, width =
> 0.8,cex.names=1.0,horiz = TRUE)
> abline(v=c(seq(0,50,10)),col='grey',lwd=0.2)
> par(new=TRUE)
> djf.bar<-barplot(djf.gcms[,i],ylim=c(0,9),xlim=c(0,50),col=cores1,main =
> i,font.main = 1, cex.main = 1.1,las=1, axisnames = F, width =
> 0.8,cex.names=1.0,horiz = TRUE) #xlab="Frequency (%)"
> title("Frequency (%)", line = -17.0)
> text(y = djf.bar, x = djf.gcms[,i], label = round(djf.gcms[,i],
> digits=0), pos = 4, cex = 1.0)
>
> }
> dev.off()
>
>
>
> ------------
> Try not to become a man of success but rather a man of value- Albert
> Einstein
>
> Kwesi A. Quagraine
> Department of Physics
> School of Physical Sciences
> College of Agriculture and Natural Sciences
> University of Cape Coast
> Cape Coast, Ghana
>
> Alt. Email: kwesi at csag.uct.ac.za
> Web: http://www.recycleupghana.org/
> Office: +27 21 650 3164
> Skype: quagraine_cwasi
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

------------
Try not to become a man of success but rather a man of value- Albert
Einstein

Kwesi A. Quagraine
Department of Physics
School of Physical Sciences
College of Agriculture and Natural Sciences
University of Cape Coast
Cape Coast, Ghana

Alt. Email: kwesi at csag.uct.ac.za
Web: http://www.recycleupghana.org/
Office: +27 21 650 3164
Skype: quagraine_cwasi

	[[alternative HTML version deleted]]


From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Wed May 29 10:46:26 2019
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Wed, 29 May 2019 08:46:26 +0000 (UTC)
Subject: [R] Fitdistrplus package : Error messages
References: <1133052359.12277004.1559119586658.ref@mail.yahoo.com>
Message-ID: <1133052359.12277004.1559119586658@mail.yahoo.com>

Dear R-Experts,

Here is a toy example, reproducible example, I get error messages. I have tried to fix it by myself using "google is my friend", but I did not get it. If somebody can help me to fix these errors, would be highly appreciated.

##########################
install.packages("fitdistrplus")
library(fitdistrplus)

x=c(3,3.5,4.5,5,5.5,5.5,4.5,3.5,5,6,3,4,5,4.5,4,5.5,3.5,3,3)? 

##Poisson distribution
f2p=fitdist(x, "pois")
plot(f2p)
summary(f2p)

##negative binomial distribution
f2n=fitdist(x,"nbinom")
plot(f2n)
summary(f2n)
####################################


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed May 29 11:54:57 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 29 May 2019 10:54:57 +0100
Subject: [R] Fitdistrplus package : Error messages
In-Reply-To: <1133052359.12277004.1559119586658@mail.yahoo.com>
References: <1133052359.12277004.1559119586658.ref@mail.yahoo.com>
 <1133052359.12277004.1559119586658@mail.yahoo.com>
Message-ID: <55ed789b-5c98-e05e-6fbc-854852522cd6@sapo.pt>

Hello,

This is because you have non-integer values in vector x?


This one works.

##Poisson distribution
f2p <- fitdist(floor(x), "pois")
plot(f2p)
summary(f2p)


Not so well

##negative binomial distribution
f2n <- fitdist(floor(x), "nbinom")
f2n <- fitdist(ceiling(x), "nbinom")
f2n <- fitdist(round(x), "nbinom")

plot(f2n)
summary(f2n)


Hope this helps,

Rui Barradas

?s 09:46 de 29/05/19, varin sacha via R-help escreveu:
> Dear R-Experts,
> 
> Here is a toy example, reproducible example, I get error messages. I have tried to fix it by myself using "google is my friend", but I did not get it. If somebody can help me to fix these errors, would be highly appreciated.
> 
> ##########################
> install.packages("fitdistrplus")
> library(fitdistrplus)
> 
> x=c(3,3.5,4.5,5,5.5,5.5,4.5,3.5,5,6,3,4,5,4.5,4,5.5,3.5,3,3)
> 
> ##Poisson distribution
> f2p=fitdist(x, "pois")
> plot(f2p)
> summary(f2p)
> 
> ##negative binomial distribution
> f2n=fitdist(x,"nbinom")
> plot(f2n)
> summary(f2n)
> ####################################
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From B|||@Po||ng @end|ng |rom ze||@@com  Wed May 29 17:49:33 2019
From: B|||@Po||ng @end|ng |rom ze||@@com (Bill Poling)
Date: Wed, 29 May 2019 15:49:33 +0000
Subject: [R] Help with staked bar plot values
Message-ID: <BN7PR02MB5073A62BF175DBD1BFFFADD4EA1F0@BN7PR02MB5073.namprd02.prod.outlook.com>

Hello

#RStudio Version 1.2.1335
sessionInfo()
# R version 3.6.0 Patched (2019-05-19 r76539)
# Platform: x86_64-w64-mingw32/x64 (64-bit)
# Running under: Windows 10 x64 (build 17134)


I am all over the google map with this so far but have made progress.

#https://www.r-graph-gallery.com/stacked-barplot/

#https://stackoverflow.com/questions/20349929/stacked-bar-plot-in-r

#https://stackoverflow.com/questions/41908635/r-ggplot2-adding-value-to-bar-plot

Here is my data for example use:

df6 <- df5[,c(1,2,3)]

head(df6,n=12)

Month_Yr       RPct     ASPct
1   2018-01 0.03060779 0.1288720
2   2018-02 0.02584442 0.1087038
3   2018-03 0.02879822 0.1268878
4   2018-04 0.02661778 0.1204089
5   2018-05 0.02390651 0.1158945
6   2018-06 0.02831802 0.1174230
7   2018-07 0.02545577 0.1106545
8   2018-08 0.02738861 0.1179146
9   2018-09 0.02826573 0.1286884
10  2018-10 0.02167206 0.1213102
11  2018-11 0.02203140 0.1158986
12  2018-12 0.02209121 0.1309418

These are percentages by Month_Yr

dput(df6)
structure(list(Month_Yr = c("2018-01", "2018-02", "2018-03",
"2018-04", "2018-05", "2018-06", "2018-07", "2018-08", "2018-09",
"2018-10", "2018-11", "2018-12"), RPct = c(0.0306077919610775,
0.0258444184406978, 0.0287982244057369, 0.0266177822179555, 0.0239065089349008,
0.0283180203081446, 0.0254557749242774, 0.0273886068149913, 0.028265726678494,
0.0216720600309651, 0.022031396916971, 0.0220912074431471), ASPct = c(0.128871966484723,
0.108703765196302, 0.126887785714588, 0.120408859429047, 0.11589445999452,
0.11742296270427, 0.110654529518569, 0.117914621843884, 0.128688428092512,
0.121310162536382, 0.115898591408793, 0.13094179316738)), class = "data.frame", row.names = c(NA,
-12L))

My basic stack bar plot works

tbl<-melt(df6,id.vars="Month_Yr")

ggplot(tbl,aes(x=Month_Yr,y=value,fill=variable))+geom_bar(stat='identity') +
  scale_y_continuous(labels = scales::percent)

So I would like a stacked Bar plot of same but adds the value, in percentage 25% 75% etc into the bar itself.

Here is my attempt so far

ggplot(tbl,aes(x=Month_Yr,y=value,fill=variable))+geom_bar(stat='identity') +
  scale_y_continuous(labels = scales::percent) +
  geom_text(aes(label = sprintf("%.1f", value), y= value),  vjust = 3)+
  guides(fill=FALSE)

This version of plot has 0.1 in all the bars at ~ the top of the lower bar and 0.0 at the bottom of the lower bar?

1. How can I get the correct values into the top and bottom bars please?
2. Also how can I get the Y axis to have 10 breaks from 0 to 100 rather than the 0 to 15%

Thank you for your support

WHP









Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Thu May 30 03:40:09 2019
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Thu, 30 May 2019 01:40:09 +0000
Subject: [R] Tying to underdressed the magic of lm redux
Message-ID: <BN7PR03MB373052F07EDFB128F6BE2C75E2180@BN7PR03MB3730.namprd03.prod.outlook.com>

Thanks to several kind people, I understand how to use deparse(substitute(paramter)) to get as text strings the arguments passed to an R function. What I still can't do is put the text strings recovered by deparse(substitute(parameter)) back together to get the columns of a dataframe passed to the function. What I want to do is pass a column name to a function along with the name of the dataframe and then, within the function access the column of the dataframe.

I want the function below to print the columns of the dataframe testdata, i.e. testdata[,"FSG"] and testdata[,"GCM"]. I have tried several ways to tell the function to print the columns; none of them work.

I thank everyone who has helped in the past, and those people who will help me now!

John

testdata <- structure(list(FSG = c(271L, 288L, 269L, 297L, 311L, 217L, 235L,

                                   172L, 201L, 162L), CGM = c(205L, 273L, 226L, 235L, 311L, 201L,

                                   203L, 155L, 182L, 163L)), row.names = c(NA, 10L), class = "data.frame")

cat("This is the data frame")

class(testdata)

testdata



BAPlot <- function(first,second,indata){

  # these lines of code work

    col1 <- deparse(substitute(first))

    col2 <- deparse(substitute(second))

    thedata <- deparse(substitute(third))

    print(col1)

    print(col2)

    print(thedata)

    cat("This gets the data, but not as a dataframe\n")

    zoop<-paste(indata)

    print(zoop)

    cat("End This gets the data, but not as a dataframe\n")

     # these lines do not work

    print(indata[,first])

    print(indata[,"first"])

    print(thedata[,col1])

    paste(zoop[,paste(first)])

    paste(zoop[,first])

    zap<-paste(first)

    print(zap)

}




	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Thu May 30 03:59:20 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 29 May 2019 18:59:20 -0700
Subject: [R] Tying to underdressed the magic of lm redux
In-Reply-To: <BN7PR03MB373052F07EDFB128F6BE2C75E2180@BN7PR03MB3730.namprd03.prod.outlook.com>
References: <BN7PR03MB373052F07EDFB128F6BE2C75E2180@BN7PR03MB3730.namprd03.prod.outlook.com>
Message-ID: <CAGxFJbQNcYBAF60w7QpTvcFnhdEsf6fUv1+DhxHJOVVD=QHHwg@mail.gmail.com>

Basically, huh?

> df <- data.frame(a = 1:3, b = letters[1:3])
> nm <- names(df)
> print(df[,nm[1]])
[1] 1 2 3
> print(df[,nm[2]])
[1] a b c
Levels: a b c

This can be done within a function, of course:

> demo <- function(df, colnames){
+    print(df[,colnames])
+ }
> demo(df,c("a","b"))
  a b
1 1 a
2 2 b
3 3 c

Am I missing something? (Apologies, if so).

Bert Gunter



On Wed, May 29, 2019 at 6:40 PM Sorkin, John <jsorkin at som.umaryland.edu>
wrote:

> Thanks to several kind people, I understand how to use
> deparse(substitute(paramter)) to get as text strings the arguments passed
> to an R function. What I still can't do is put the text strings recovered
> by deparse(substitute(parameter)) back together to get the columns of a
> dataframe passed to the function. What I want to do is pass a column name
> to a function along with the name of the dataframe and then, within the
> function access the column of the dataframe.
>
> I want the function below to print the columns of the dataframe testdata,
> i.e. testdata[,"FSG"] and testdata[,"GCM"]. I have tried several ways to
> tell the function to print the columns; none of them work.
>
> I thank everyone who has helped in the past, and those people who will
> help me now!
>
> John
>
> testdata <- structure(list(FSG = c(271L, 288L, 269L, 297L, 311L, 217L,
> 235L,
>
>                                    172L, 201L, 162L), CGM = c(205L, 273L,
> 226L, 235L, 311L, 201L,
>
>                                    203L, 155L, 182L, 163L)), row.names =
> c(NA, 10L), class = "data.frame")
>
> cat("This is the data frame")
>
> class(testdata)
>
> testdata
>
>
>
> BAPlot <- function(first,second,indata){
>
>   # these lines of code work
>
>     col1 <- deparse(substitute(first))
>
>     col2 <- deparse(substitute(second))
>
>     thedata <- deparse(substitute(third))
>
>     print(col1)
>
>     print(col2)
>
>     print(thedata)
>
>     cat("This gets the data, but not as a dataframe\n")
>
>     zoop<-paste(indata)
>
>     print(zoop)
>
>     cat("End This gets the data, but not as a dataframe\n")
>
>      # these lines do not work
>
>     print(indata[,first])
>
>     print(indata[,"first"])
>
>     print(thedata[,col1])
>
>     paste(zoop[,paste(first)])
>
>     paste(zoop[,first])
>
>     zap<-paste(first)
>
>     print(zap)
>
> }
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Thu May 30 04:33:01 2019
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Thu, 30 May 2019 02:33:01 +0000
Subject: [R] Tying to underdressed the magic of lm redux
In-Reply-To: <CAGxFJbQNcYBAF60w7QpTvcFnhdEsf6fUv1+DhxHJOVVD=QHHwg@mail.gmail.com>
References: <BN7PR03MB373052F07EDFB128F6BE2C75E2180@BN7PR03MB3730.namprd03.prod.outlook.com>,
 <CAGxFJbQNcYBAF60w7QpTvcFnhdEsf6fUv1+DhxHJOVVD=QHHwg@mail.gmail.com>
Message-ID: <A29061BB-2987-4D89-B939-C733A2ECC203@som.umaryland.edu>

Bert,
Thank you for your reply. You are correct that your code will print the contents of the data frame. While it works, it is not as elegant as the lm function. One does not have to pass the independent and dependent variables to lm In parentheses.

Fit1<-lm(y~x,data=mydata)

None of the parameters to lm are passed in quotation marks. Somehow, using deparse(substitute()) and other magic lm is able to get the data in the dataframe mydata. I want to be able to do the same magic in functions I write; pass a dataframe and column names, all without quotation marks and be able to write code that will provide access to the columns of the dataframe without having to pass the column names in quotation marks.
Thank you,
John

John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street<x-apple-data-detectors://12>
GRECC<x-apple-data-detectors://12> (BT/18/GR)
Baltimore, MD 21201-1524<x-apple-data-detectors://13/0>
(Phone) 410-605-711<tel:410-605-7119>9
(Fax) 410-605-7913<tel:410-605-7913> (Please call phone number above prior to faxing)

On May 29, 2019, at 9:59 PM, Bert Gunter <bgunter.4567 at gmail.com<mailto:bgunter.4567 at gmail.com>> wrote:

Basically, huh?

> df <- data.frame(a = 1:3, b = letters[1:3])
> nm <- names(df)
> print(df[,nm[1]])
[1] 1 2 3
> print(df[,nm[2]])
[1] a b c
Levels: a b c

This can be done within a function, of course:

> demo <- function(df, colnames){
+    print(df[,colnames])
+ }
> demo(df,c("a","b"))
  a b
1 1 a
2 2 b
3 3 c

Am I missing something? (Apologies, if so).

Bert Gunter



On Wed, May 29, 2019 at 6:40 PM Sorkin, John <jsorkin at som.umaryland.edu<mailto:jsorkin at som.umaryland.edu>> wrote:
Thanks to several kind people, I understand how to use deparse(substitute(paramter)) to get as text strings the arguments passed to an R function. What I still can't do is put the text strings recovered by deparse(substitute(parameter)) back together to get the columns of a dataframe passed to the function. What I want to do is pass a column name to a function along with the name of the dataframe and then, within the function access the column of the dataframe.

I want the function below to print the columns of the dataframe testdata, i.e. testdata[,"FSG"] and testdata[,"GCM"]. I have tried several ways to tell the function to print the columns; none of them work.

I thank everyone who has helped in the past, and those people who will help me now!

John

testdata <- structure(list(FSG = c(271L, 288L, 269L, 297L, 311L, 217L, 235L,

                                   172L, 201L, 162L), CGM = c(205L, 273L, 226L, 235L, 311L, 201L,

                                   203L, 155L, 182L, 163L)), row.names = c(NA, 10L), class = "data.frame")

cat("This is the data frame")

class(testdata)

testdata



BAPlot <- function(first,second,indata){

  # these lines of code work

    col1 <- deparse(substitute(first))

    col2 <- deparse(substitute(second))

    thedata <- deparse(substitute(third))

    print(col1)

    print(col2)

    print(thedata)

    cat("This gets the data, but not as a dataframe\n")

    zoop<-paste(indata)

    print(zoop)

    cat("End This gets the data, but not as a dataframe\n")

     # these lines do not work

    print(indata[,first])

    print(indata[,"first"])

    print(thedata[,col1])

    paste(zoop[,paste(first)])

    paste(zoop[,first])

    zap<-paste(first)

    print(zap)

}




        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Thu May 30 05:27:13 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 29 May 2019 20:27:13 -0700
Subject: [R] Tying to underdressed the magic of lm redux
In-Reply-To: <A29061BB-2987-4D89-B939-C733A2ECC203@som.umaryland.edu>
References: <BN7PR03MB373052F07EDFB128F6BE2C75E2180@BN7PR03MB3730.namprd03.prod.outlook.com>
 <CAGxFJbQNcYBAF60w7QpTvcFnhdEsf6fUv1+DhxHJOVVD=QHHwg@mail.gmail.com>
 <A29061BB-2987-4D89-B939-C733A2ECC203@som.umaryland.edu>
Message-ID: <CAGxFJbRj3FT5w5=Ks5wqQubBO1=L6y=Hb_wip7HbGMfxYHPw1A@mail.gmail.com>

Depends on how you want to specify variables. You are not clear (to me) on
this. But, for instance:

demo <- function(form,df)
{
   av <- all.vars(form)
   df[,av]
}
demo(~a+b, df)
demo(a~b,df)

?all.vars, ?all.names  for details

Bert Gunter


On Wed, May 29, 2019 at 7:33 PM Sorkin, John <jsorkin at som.umaryland.edu>
wrote:

> Bert,
> Thank you for your reply. You are correct that your code will print the
> contents of the data frame. While it works, it is not as elegant as the lm
> function. One does not have to pass the independent and dependent variables
> to lm In parentheses.
>
> Fit1<-lm(y~x,data=mydata)
>
> None of the parameters to lm are passed in quotation marks. Somehow, using
> deparse(substitute()) and other magic lm is able to get the data in the
> dataframe mydata. I want to be able to do the same magic in functions I
> write; pass a dataframe and column names, all without quotation marks and
> be able to write code that will provide access to the columns of the
> dataframe without having to pass the column names in quotation marks.
> Thank you,
> John
>
> John David Sorkin M.D., Ph.D.
>
> Professor of Medicine
>
> Chief, Biostatistics and Informatics
>
> University of Maryland School of Medicine Division of Gerontology and
> Geriatric Medicine
>
> Baltimore VA Medical Center
>
> 10 North Greene Street
>
> GRECC (BT/18/GR)
>
> Baltimore, MD 21201-1524
>
> (Phone) 410-605-711 <410-605-7119>9
>
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
>
> On May 29, 2019, at 9:59 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
> Basically, huh?
>
> > df <- data.frame(a = 1:3, b = letters[1:3])
> > nm <- names(df)
> > print(df[,nm[1]])
> [1] 1 2 3
> > print(df[,nm[2]])
> [1] a b c
> Levels: a b c
>
> This can be done within a function, of course:
>
> > demo <- function(df, colnames){
> +    print(df[,colnames])
> + }
> > demo(df,c("a","b"))
>   a b
> 1 1 a
> 2 2 b
> 3 3 c
>
> Am I missing something? (Apologies, if so).
>
> Bert Gunter
>
>
>
> On Wed, May 29, 2019 at 6:40 PM Sorkin, John <jsorkin at som.umaryland.edu>
> wrote:
>
>> Thanks to several kind people, I understand how to use
>> deparse(substitute(paramter)) to get as text strings the arguments passed
>> to an R function. What I still can't do is put the text strings recovered
>> by deparse(substitute(parameter)) back together to get the columns of a
>> dataframe passed to the function. What I want to do is pass a column name
>> to a function along with the name of the dataframe and then, within the
>> function access the column of the dataframe.
>>
>> I want the function below to print the columns of the dataframe testdata,
>> i.e. testdata[,"FSG"] and testdata[,"GCM"]. I have tried several ways to
>> tell the function to print the columns; none of them work.
>>
>> I thank everyone who has helped in the past, and those people who will
>> help me now!
>>
>> John
>>
>> testdata <- structure(list(FSG = c(271L, 288L, 269L, 297L, 311L, 217L,
>> 235L,
>>
>>                                    172L, 201L, 162L), CGM = c(205L, 273L,
>> 226L, 235L, 311L, 201L,
>>
>>                                    203L, 155L, 182L, 163L)), row.names =
>> c(NA, 10L), class = "data.frame")
>>
>> cat("This is the data frame")
>>
>> class(testdata)
>>
>> testdata
>>
>>
>>
>> BAPlot <- function(first,second,indata){
>>
>>   # these lines of code work
>>
>>     col1 <- deparse(substitute(first))
>>
>>     col2 <- deparse(substitute(second))
>>
>>     thedata <- deparse(substitute(third))
>>
>>     print(col1)
>>
>>     print(col2)
>>
>>     print(thedata)
>>
>>     cat("This gets the data, but not as a dataframe\n")
>>
>>     zoop<-paste(indata)
>>
>>     print(zoop)
>>
>>     cat("End This gets the data, but not as a dataframe\n")
>>
>>      # these lines do not work
>>
>>     print(indata[,first])
>>
>>     print(indata[,"first"])
>>
>>     print(thedata[,col1])
>>
>>     paste(zoop[,paste(first)])
>>
>>     paste(zoop[,first])
>>
>>     zap<-paste(first)
>>
>>     print(zap)
>>
>> }
>>
>>
>>
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From |v@n@|@m@n @end|ng |rom gm@||@com  Wed May 29 01:34:07 2019
From: |v@n@|@m@n @end|ng |rom gm@||@com (Ivan Bezerra Allaman)
Date: Tue, 28 May 2019 20:34:07 -0300
Subject: [R] Calling any method within default method!
Message-ID: <CAKZQXCz4VdB0X5ySAEe=MrgKwiuke0mc_CkH3LsX5ZnytSmWLg@mail.gmail.com>

Good night dear!
For years I have a problem that I have avoided with the use of the switch
function, but now I want to solve by following the good practices of object
orientation (OOP).
My function was created to generate experiments according to some input
parameters. Therefore, the first argument does not have a class defined as
"data.frame", "matrix", etc., so that from the generic, specific methods can
be called.
I made a simple example to show my problem.

foo <- function (x, ...) UseMethod ('foo')

foo.default <- function(x,
                        a = 10,
                        b = NULL,
                        cc = 2,
                        dd = 3,
                        type = c ('brazil', 'argentina'),
                        ...){

  ty <- match.arg(type)

  obj <- list(a = a,
                 b = b,
                 cc = cc,
                 dd = dd)
  class (obj) <- paste ('foo',
                                   ty,
                                  sep = '.')

  res <- foo(x = obj, ...)
}

foo.brazil <- function(x, ...){
  a <- x$a
  cc <- x$cc
  res <- a + cc
  return (res)
}

foo.argentina <- function(x, ...){
  cc <- x$cc
  dd <- x$dd
  res <- sqrt(cc + dd)
  return (res)
}

foo(a = 1)

If anyone has any light I thank them immensely.

-- 
\begin{signature}
<<>>=
Prof. D.Sc. Ivan Bezerra Allaman
Laborat?rio de Estat?stica Computacional
Departamento de Ci?ncias Exatas e Tecnol?gicas
Universidade Estadual de Santa Cruz

Ilh?us/BA - Brasil
Fone: +55 73 3680-5622
E-mail:ivanalaman at gmail.com
@
\end{signature}

	[[alternative HTML version deleted]]


From jw@ng @end|ng |rom c@ub@edu  Wed May 29 17:55:16 2019
From: jw@ng @end|ng |rom c@ub@edu (Jianjun Wang)
Date: Wed, 29 May 2019 15:55:16 +0000
Subject: [R] Error in using tokens script
In-Reply-To: <a3c7228f7d60450b847dd618295d7eef@SRVEXCHCM1302.precheza.cz>
References: <BYAPR07MB421365019576755228B2D5EBD21E0@BYAPR07MB4213.namprd07.prod.outlook.com>,
 <a3c7228f7d60450b847dd618295d7eef@SRVEXCHCM1302.precheza.cz>
Message-ID: <BYAPR07MB4213DBFBF7C68CA7DFDBAA08D21F0@BYAPR07MB4213.namprd07.prod.outlook.com>

Thank you, Petr.


I found your guidance very helpful!


JJ

________________________________
From: PIKAL Petr <petr.pikal at precheza.cz>
Sent: Wednesday, May 29, 2019 12:31:31 AM
To: Jianjun Wang; r-help at R-project.org
Subject: RE: Error in using tokens script

Hi.

Most probably SMSSpamCollection$Message is not character, corpus or tokens object (which error message politely suggests).

You need to change it to be correct mode

functions ?str, ?mode, ?typeof

to see your objects.

And maybe also R intro chapter about objects and its differences.

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Jianjun Wang
> Sent: Tuesday, May 28, 2019 6:51 PM
> To: r-help at R-project.org
> Subject: [R] Error in using tokens script
>
> Hi, Colleagues.
>
> As shown at the bottom, I was able to obtain information from a column
> (named "Message") of a text data (named "SMSSpamCollection" (i.e.,
> SMSSpamCollection$Message[44]).  But when I ran SMSSpamCollection.token
> <-tokens(SMSSpamCollection$Message, what="word",
> +                                  remove_url = TRUE,
> +                                  remove_numbers= TRUE, remove_punct= TRUE,
> +                                  remove_symbols= TRUE, remove_hyphens=
> + TRUE)
> The feedback was
> Error in tokens.default(SMSSpamCollection$Message, what = "word",
> remove_url = TRUE,  :
>   tokens() only works on character, corpus, tokens objects.
>
> I would appreciate some guidance to fix the error message!
>
> Thank you.
>
> JJ
>
> > SMSSpamCollection$Message[44]
> [1] Great! I hope you like your man well endowed. I am  &lt;#&gt;  inches...
> 3026 Levels: 'An Amazing Quote'' - Sometimes in life its difficult to decide
> whats wrong!! a lie that brings a smile or the truth that brings a tear.... ...
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIGaQ&c=8Ipd-S27WuaKn7LZs55QTnbDbMQSs_VN5Yh9G3ue5PM&r=6YMmmhzICZ4OUwhDfmC3QQ&m=LUpJDuot2AUCyWEmCgVRCoXYKjSYCu7ckyeK7upUq4I&s=0dIIMZFHah0wvGMbe5KAFMvjfgT_oKYeOmsL5pqGNA8&e=
> PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2D&d=DwIGaQ&c=8Ipd-S27WuaKn7LZs55QTnbDbMQSs_VN5Yh9G3ue5PM&r=6YMmmhzICZ4OUwhDfmC3QQ&m=LUpJDuot2AUCyWEmCgVRCoXYKjSYCu7ckyeK7upUq4I&s=OLGbSDf02XONRvcC1XNalDpnrdpIYaI1KBF2MmJvWag&e=
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://urldefense.proofpoint.com/v2/url?u=https-3A__www.precheza.cz_zasady-2Dochrany-2Dosobnich-2Dudaju_&d=DwIGaQ&c=8Ipd-S27WuaKn7LZs55QTnbDbMQSs_VN5Yh9G3ue5PM&r=6YMmmhzICZ4OUwhDfmC3QQ&m=LUpJDuot2AUCyWEmCgVRCoXYKjSYCu7ckyeK7upUq4I&s=8cMbsv-ZkxDvNeiZvk8D0orcV_Y4h5NB8FJxHN_CjqM&e=  | Information about processing and protection of business partner?s personal data are available on website: https://urldefense.proofpoint.com/v2/url?u=https-3A__www.precheza.cz_en_personal-2Ddata-2Dprotection-2Dprinciples_&d=DwIGaQ&c=8Ipd-S27WuaKn7LZs55QTnbDbMQSs_VN5Yh9G3ue5PM&r=6YMmmhzICZ4OUwhDfmC3QQ&m=LUpJDuot2AUCyWEmCgVRCoXYKjSYCu7ckyeK7upUq4I&s=MF2IFhsgZJkQV61u-wtP4mAr1B_XsYNf9neVXpv9WcU&e=
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl?en? o vylou?en? odpov?dnosti: https://urldefense.proofpoint.com/v2/url?u=https-3A__www.precheza.cz_01-2Ddovetek_&d=DwIGaQ&c=8Ipd-S27WuaKn7LZs55QTnbDbMQSs_VN5Yh9G3ue5PM&r=6YMmmhzICZ4OUwhDfmC3QQ&m=LUpJDuot2AUCyWEmCgVRCoXYKjSYCu7ckyeK7upUq4I&s=trxOYmiq7ocP9bxSqEtga9MtXWjQGgRK8sMacrZq2wk&e=  | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://urldefense.proofpoint.com/v2/url?u=https-3A__www.precheza.cz_en_01-2Ddisclaimer_&d=DwIGaQ&c=8Ipd-S27WuaKn7LZs55QTnbDbMQSs_VN5Yh9G3ue5PM&r=6YMmmhzICZ4OUwhDfmC3QQ&m=LUpJDuot2AUCyWEmCgVRCoXYKjSYCu7ckyeK7upUq4I&s=oLigFdPzgIUCL2MmZB7ZN4mZWgKL07CV8nA9IJ5FoHU&e=


	[[alternative HTML version deleted]]


From n|co@@chuck @end|ng |rom gm@||@com  Wed May 29 18:44:26 2019
From: n|co@@chuck @end|ng |rom gm@||@com (Nicolas Schuck)
Date: Wed, 29 May 2019 18:44:26 +0200
Subject: [R] Mac/PC differences in lmer results
Message-ID: <5CBE1637-3941-4741-B5A4-8104F0B66386@gmail.com>

Dear fellow R coders, 

I am observing differences in results obtained using glmer when using a Mac or Linux computer versus a PC. Specifically, I am talking about a relatively complex glmer model with a nested random effects structure. The model is set up in the following way: 
gcctrl = glmerControl(optimizer=c('nloptwrap'), optCtrl = list(maxfun = 500000), calc.derivs = FALSE)

glmer_pre_instr1 = glmer(
      formula = cbind(FREQ, NSAMP-FREQ) ~ FDIST_minz + poly(RFREQ,2) + ROI + (1 + FDIST_minz + RFREQ + ROI|ID/COL), 
      data = cdf_pre_instr, 
      family = binomial, 
      control = gcctrl)

Code and data of an example for which I find reproducible, non-negligible differences between Mac/Win can be found here: https://gitlab.com/nschuck/glmer_sandbox/tree/master <https://gitlab.com/nschuck/glmer_sandbox/tree/master>
The differences between the fitted models seem to be most pronounced regarding the estimated correlation structure of the random effects terms. Mac and Linux yield very similar results, but Windows deviates quite a bit in some cases. This has a large impact on p values obtained when performing model comparisons. I have tried this on Mac OS 10.14, Windows 10 and Ubuntu and Debian. All systems I have tried are using lme 1.1.21 and R 3.5+. 

Does anyone have an idea what the underlying cause might be? 

Thanks, 
Nico 
 



	[[alternative HTML version deleted]]


From roger@bo@ @end|ng |rom gm@||@com  Tue May 28 21:58:58 2019
From: roger@bo@ @end|ng |rom gm@||@com (Roger Bos)
Date: Tue, 28 May 2019 15:58:58 -0400
Subject: [R] error when installing zoo package on macos: file 'DESCRIPTION'
 does not exist
Message-ID: <CAPV07m9SwYWLn97_mKGXLPY6gD8KnJgzWU_-ZAO=_KzmEV+7cQ@mail.gmail.com>

I was able to upgrade to the latest version of the zoo package on my linux
machine, but my mac is given an error message that the DESCRIPTION does not
exist.  I am sure that is not the real problem, but I don't know how to
debug or figure out what the real problem is.  The output is below and my
sessionInfo() is as follows:

R version 3.6.0 (2019-04-26)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS Mojave 10.14.5

Matrix products: default
BLAS:
/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
LAPACK:
/Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  tools     methods
base

other attached packages:
[1] googledrive_0.1.3   alphavantager_0.1.1

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.1        rstudioapi_0.10   magrittr_1.5      usethis_1.5.0
  devtools_2.0.2    tidyselect_0.2.5  pkgload_1.0.2
 [8] R6_2.4.0          rlang_0.3.4       dplyr_0.8.1       pkgbuild_1.0.3
 sessioninfo_1.1.1 cli_1.1.0         withr_2.1.2
[15] remotes_2.0.4     fortunes_1.5-4    rprojroot_1.3-2   assertthat_0.2.1
 digest_0.6.18     tibble_2.1.1      crayon_1.3.4
[22] processx_3.3.1    purrr_0.3.2       callr_3.2.0       fs_1.3.1
 ps_1.3.0          curl_3.3          testthat_2.1.1
[29] memoise_1.1.0     glue_1.3.1        compiler_3.6.0    pillar_1.4.0
 backports_1.1.4   desc_1.2.0        prettyunits_1.0.2
[36] pkgconfig_2.0.2


install.packages("zoo")

  There is a binary version available but the source version is later:
    binary source needs_compilation
zoo  1.8-5  1.8-6              TRUE

Do you want to install from sources the package which needs compilation?
(Yes/no/cancel) yes
installing the source package ?zoo?

trying URL 'https://cran.rstudio.com/src/contrib/zoo_1.8-6.tar.gz'
Content type 'application/x-gzip' length 853504 bytes (833 KB)
==================================================
downloaded 833 KB

* installing *source* package ?zoo? ...
** package ?zoo? successfully unpacked and MD5 sums checked
** using staged installation
** libs
clang -I"/Library/Frameworks/R.framework/Resources/include" -DNDEBUG
-I../inst/include  -isysroot
/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk -I/usr/local/include
 -fPIC  -Wall -g -O2  -c coredata.c -o coredata.o
clang -I"/Library/Frameworks/R.framework/Resources/include" -DNDEBUG
-I../inst/include  -isysroot
/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk -I/usr/local/include
 -fPIC  -Wall -g -O2  -c init.c -o init.o
clang -I"/Library/Frameworks/R.framework/Resources/include" -DNDEBUG
-I../inst/include  -isysroot
/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk -I/usr/local/include
 -fPIC  -Wall -g -O2  -c lag.c -o lag.o
clang -dynamiclib -Wl,-headerpad_max_install_names -undefined
dynamic_lookup -single_module -multiply_defined suppress
-L/Library/Frameworks/R.framework/Resources/lib -L/usr/local/lib -o zoo.so
coredata.o init.o lag.o -F/Library/Frameworks/R.framework/.. -framework R
-Wl,-framework -Wl,CoreFoundation
installing to
/Library/Frameworks/R.framework/Versions/3.6/Resources/library/00LOCK-zoo/00new/zoo/libs
** R
** demo
** inst
** byte-compile and prepare package for lazy loading
Error in tools:::.read_description(file) :
  file 'DESCRIPTION' does not exist
Calls: suppressPackageStartupMessages ... withCallingHandlers ->
.getRequiredPackages -> <Anonymous> -> <Anonymous>
Execution halted
ERROR: lazy loading failed for package ?zoo?
* removing
?/Library/Frameworks/R.framework/Versions/3.6/Resources/library/zoo?
* restoring previous
?/Library/Frameworks/R.framework/Versions/3.6/Resources/library/zoo?
Warning in install.packages :
  installation of package ?zoo? had non-zero exit status

The downloaded source packages are in
?/private/var/folders/pf/8_rzvpr95ybbr5n31grmh4dc0000gp/T/RtmpzRQrtQ/downloaded_packages?

	[[alternative HTML version deleted]]


From petr@p|k@| @end|ng |rom prechez@@cz  Wed May 29 09:31:31 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Wed, 29 May 2019 07:31:31 +0000
Subject: [R] Error in using tokens script
In-Reply-To: <BYAPR07MB421365019576755228B2D5EBD21E0@BYAPR07MB4213.namprd07.prod.outlook.com>
References: <BYAPR07MB421365019576755228B2D5EBD21E0@BYAPR07MB4213.namprd07.prod.outlook.com>
Message-ID: <a3c7228f7d60450b847dd618295d7eef@SRVEXCHCM1302.precheza.cz>

Hi.

Most probably SMSSpamCollection$Message is not character, corpus or tokens object (which error message politely suggests).

You need to change it to be correct mode

functions ?str, ?mode, ?typeof

to see your objects.

And maybe also R intro chapter about objects and its differences.

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Jianjun Wang
> Sent: Tuesday, May 28, 2019 6:51 PM
> To: r-help at R-project.org
> Subject: [R] Error in using tokens script
>
> Hi, Colleagues.
>
> As shown at the bottom, I was able to obtain information from a column
> (named "Message") of a text data (named "SMSSpamCollection" (i.e.,
> SMSSpamCollection$Message[44]).  But when I ran SMSSpamCollection.token
> <-tokens(SMSSpamCollection$Message, what="word",
> +                                  remove_url = TRUE,
> +                                  remove_numbers= TRUE, remove_punct= TRUE,
> +                                  remove_symbols= TRUE, remove_hyphens=
> + TRUE)
> The feedback was
> Error in tokens.default(SMSSpamCollection$Message, what = "word",
> remove_url = TRUE,  :
>   tokens() only works on character, corpus, tokens objects.
>
> I would appreciate some guidance to fix the error message!
>
> Thank you.
>
> JJ
>
> > SMSSpamCollection$Message[44]
> [1] Great! I hope you like your man well endowed. I am  &lt;#&gt;  inches...
> 3026 Levels: 'An Amazing Quote'' - Sometimes in life its difficult to decide
> whats wrong!! a lie that brings a smile or the truth that brings a tear.... ...
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


From bgunter@4567 @end|ng |rom gm@||@com  Thu May 30 16:34:57 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 30 May 2019 07:34:57 -0700
Subject: [R] Mac/PC differences in lmer results
In-Reply-To: <5CBE1637-3941-4741-B5A4-8104F0B66386@gmail.com>
References: <5CBE1637-3941-4741-B5A4-8104F0B66386@gmail.com>
Message-ID: <CAGxFJbQ=qBhu864b3jQHgRt9o8Oz8r_hBLx-1e=XFQe7YsvMOw@mail.gmail.com>

The BLAS in use on each?

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, May 30, 2019 at 5:27 AM Nicolas Schuck <nico.schuck at gmail.com>
wrote:

> Dear fellow R coders,
>
> I am observing differences in results obtained using glmer when using a
> Mac or Linux computer versus a PC. Specifically, I am talking about a
> relatively complex glmer model with a nested random effects structure. The
> model is set up in the following way:
> gcctrl = glmerControl(optimizer=c('nloptwrap'), optCtrl = list(maxfun =
> 500000), calc.derivs = FALSE)
>
> glmer_pre_instr1 = glmer(
>       formula = cbind(FREQ, NSAMP-FREQ) ~ FDIST_minz + poly(RFREQ,2) + ROI
> + (1 + FDIST_minz + RFREQ + ROI|ID/COL),
>       data = cdf_pre_instr,
>       family = binomial,
>       control = gcctrl)
>
> Code and data of an example for which I find reproducible, non-negligible
> differences between Mac/Win can be found here:
> https://gitlab.com/nschuck/glmer_sandbox/tree/master <
> https://gitlab.com/nschuck/glmer_sandbox/tree/master>
> The differences between the fitted models seem to be most pronounced
> regarding the estimated correlation structure of the random effects terms.
> Mac and Linux yield very similar results, but Windows deviates quite a bit
> in some cases. This has a large impact on p values obtained when performing
> model comparisons. I have tried this on Mac OS 10.14, Windows 10 and Ubuntu
> and Debian. All systems I have tried are using lme 1.1.21 and R 3.5+.
>
> Does anyone have an idea what the underlying cause might be?
>
> Thanks,
> Nico
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Thu May 30 16:58:47 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 30 May 2019 07:58:47 -0700
Subject: [R] Mac/PC differences in lmer results
In-Reply-To: <DF796728-5E00-451B-8C4C-54CDDA2863E4@gmail.com>
References: <5CBE1637-3941-4741-B5A4-8104F0B66386@gmail.com>
 <CAGxFJbQ=qBhu864b3jQHgRt9o8Oz8r_hBLx-1e=XFQe7YsvMOw@mail.gmail.com>
 <DF796728-5E00-451B-8C4C-54CDDA2863E4@gmail.com>
Message-ID: <CAGxFJbRNA-uTeRBsmtq_utJ9aSEoz9T8uYY=nkE94=q2SxLOyQ@mail.gmail.com>

Unless there us good reason not to, always cc the list. I have done so here.

The R Installation manual has some info on how to use different BLASes I
believe, but someone with expertise (I have none) needs to respond to your
queries.

On Thu, May 30, 2019 at 7:50 AM Nicolas Schuck <nico.schuck at gmail.com>
wrote:

> I know that it is in use on the Mac, see sessionInfo below. I have to
> check on the Win system. Why would that make such a difference and how
> could I make the Win get the same results as the Unix Systems?
>
> R version 3.6.0 (2019-04-26)
>
> Platform: x86_64-apple-darwin15.6.0 (64-bit)
>
> Running under: macOS Mojave 10.14.5
>
> Matrix products: default
>
> BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
>
> LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
>
> Random number generation:
>
>  RNG:     Mersenne-Twister
>
>  Normal:  Inversion Sample:  Rounding
>
> locale:[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>
> attached base packages:[1] stats     graphics  grDevices utils     datasets  methods   base
>
> Thanks, Nico
>
> On 30. May 2019, at 16:34, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
> The BLAS in use on each?
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Thu, May 30, 2019 at 5:27 AM Nicolas Schuck <nico.schuck at gmail.com>
> wrote:
>
>> Dear fellow R coders,
>>
>> I am observing differences in results obtained using glmer when using a
>> Mac or Linux computer versus a PC. Specifically, I am talking about a
>> relatively complex glmer model with a nested random effects structure. The
>> model is set up in the following way:
>> gcctrl = glmerControl(optimizer=c('nloptwrap'), optCtrl = list(maxfun =
>> 500000), calc.derivs = FALSE)
>>
>> glmer_pre_instr1 = glmer(
>>       formula = cbind(FREQ, NSAMP-FREQ) ~ FDIST_minz + poly(RFREQ,2) +
>> ROI + (1 + FDIST_minz + RFREQ + ROI|ID/COL),
>>       data = cdf_pre_instr,
>>       family = binomial,
>>       control = gcctrl)
>>
>> Code and data of an example for which I find reproducible, non-negligible
>> differences between Mac/Win can be found here:
>> https://gitlab.com/nschuck/glmer_sandbox/tree/master <
>> https://gitlab.com/nschuck/glmer_sandbox/tree/master>
>> The differences between the fitted models seem to be most pronounced
>> regarding the estimated correlation structure of the random effects terms.
>> Mac and Linux yield very similar results, but Windows deviates quite a bit
>> in some cases. This has a large impact on p values obtained when performing
>> model comparisons. I have tried this on Mac OS 10.14, Windows 10 and Ubuntu
>> and Debian. All systems I have tried are using lme 1.1.21 and R 3.5+.
>>
>> Does anyone have an idea what the underlying cause might be?
>>
>> Thanks,
>> Nico
>>
>>
>>
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From B|||@Po||ng @end|ng |rom ze||@@com  Thu May 30 17:49:24 2019
From: B|||@Po||ng @end|ng |rom ze||@@com (Bill Poling)
Date: Thu, 30 May 2019 15:49:24 +0000
Subject: [R] Help with staked bar plot values Version 2
Message-ID: <BN7PR02MB5073A9B13371B638CFE68566EA180@BN7PR02MB5073.namprd02.prod.outlook.com>

Hello

#RStudio Version 1.2.1335
sessionInfo()
# R version 3.6.0 Patched (2019-05-19 r76539) # Platform: x86_64-w64-mingw32/x64 (64-bit) # Running under: Windows 10 x64 (build 17134)


I am still trying to get this stacked bar plot with values incereted in the bars to work.

str(df3)
'data.frame':12 obs. of  4 variables:
 $ Month_Yr             : chr  "2018-01" "2018-02" "2018-03" "2018-04" ...
 $ CurrentBilledAccepted: num  50612538 52140848 75341460 75733565 78990033 ...
 $ CurrentBilled        : num  166049599 152054677 211204320 193905607 215781328 ...
 $ TotalBilled          : num  281806122 258607481 345235833 331032520 379911877 ...

dput(df3)
structure(list(Month_Yr = c("2018-01", "2018-02", "2018-03",
"2018-04", "2018-05", "2018-06", "2018-07", "2018-08", "2018-09",
"2018-10", "2018-11", "2018-12"), CurrentBilledAccepted = c(50612538.37,
52140847.86, 75341460.14, 75733565.33, 78990032.7, 75709569.66,
90937537.58, 88087064.92, 77733110.21, 121863059.73, 118697983.95,
122725876.36), CurrentBilled = c(166049599.06, 152054677.36,
211204320.27, 193905606.94, 215781328.14, 219237895.09, 262944323.08,
238565431.51, 205345173.62, 290686847.52, 273146859.13, 300757011.52
), TotalBilled = c(281806122.24, 258607481.1, 345235832.56, 331032519.52,
379911877.36, 392791354.13, 425577554.8, 412169215.64, 359511992.21,
507625318.05, 463895541.75, 515639239.79)), class = "data.frame", row.names = c(NA,
-12L))

This simple version works:

df3 <- df2[,c(1,4,3,2)]#Reorder the column positions

tbl<-melt(df3,id.vars="Month_Yr")

ggplot(tbl,aes(x=Month_Yr,y=value,fill=variable))+geom_bar(stat='identity')


#When I go to add values to the plot it worked earlier this AM.

Then I reopened RStudio and ran everything up to this point without error and got to this and re-ran and it does not work

#Add values to the plot bars
#https://stackoverflow.com/questions/6644997/showing-data-values-on-stacked-bar-chart-in-ggplot2

tbl<-melt(df3,id.vars="Month_Yr")

str(tbl)

p <- qplot(Month_Yr, variable, data = tbl, geom = "bar", fill = variable, theme_set(theme_bw()))
p1 <- p + geom_text(aes(label = value), size = 3, hjust = 0.5, vjust = 3, position = "stack") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 0.5)) +
    labs(title = "Plot Example")
p1

The error is:
Don't know how to automatically pick scale for object of type function. Defaulting to continuous.
Error: All columns in a tibble must be 1d or 2d objects:
* Column `label` is function

I have no idea what that means?

Can someone help please?

Thank you.

WHP



Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}


From B|||@Po||ng @end|ng |rom ze||@@com  Thu May 30 19:00:36 2019
From: B|||@Po||ng @end|ng |rom ze||@@com (Bill Poling)
Date: Thu, 30 May 2019 17:00:36 +0000
Subject: [R] Help with staked bar plot values Version 2
In-Reply-To: <BN7PR02MB5073A9B13371B638CFE68566EA180@BN7PR02MB5073.namprd02.prod.outlook.com>
References: <BN7PR02MB5073A9B13371B638CFE68566EA180@BN7PR02MB5073.namprd02.prod.outlook.com>
Message-ID: <BN7PR02MB507384F8EC190373E304B286EA180@BN7PR02MB5073.namprd02.prod.outlook.com>

I have what I need now, thank you.

WHP

#This one working

ggplot(tbl, aes(x = Month_Yr, y = value, fill = variable, label = value)) +
  geom_bar(stat = "identity") +
  geom_text(size = 3, position = position_stack(vjust = 0.5)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 0.5)) +
  labs(title = "Plot Example")

-----Original Message-----
From: Bill Poling
Sent: Thursday, May 30, 2019 11:49 AM
To: r-help (r-help at r-project.org) <r-help at r-project.org>
Cc: Bill Poling <Bill.Poling at zelis.com>
Subject: Help with staked bar plot values Version 2

Hello

#RStudio Version 1.2.1335
sessionInfo()
# R version 3.6.0 Patched (2019-05-19 r76539) # Platform: x86_64-w64-mingw32/x64 (64-bit) # Running under: Windows 10 x64 (build 17134)


I am still trying to get this stacked bar plot with values incereted in the bars to work.

str(df3)
'data.frame':12 obs. of  4 variables:
 $ Month_Yr             : chr  "2018-01" "2018-02" "2018-03" "2018-04" ...
 $ CurrentBilledAccepted: num  50612538 52140848 75341460 75733565 78990033 ...
 $ CurrentBilled        : num  166049599 152054677 211204320 193905607 215781328 ...
 $ TotalBilled          : num  281806122 258607481 345235833 331032520 379911877 ...

dput(df3)
structure(list(Month_Yr = c("2018-01", "2018-02", "2018-03", "2018-04", "2018-05", "2018-06", "2018-07", "2018-08", "2018-09", "2018-10", "2018-11", "2018-12"), CurrentBilledAccepted = c(50612538.37, 52140847.86, 75341460.14, 75733565.33, 78990032.7, 75709569.66, 90937537.58, 88087064.92, 77733110.21, 121863059.73, 118697983.95, 122725876.36), CurrentBilled = c(166049599.06, 152054677.36, 211204320.27, 193905606.94, 215781328.14, 219237895.09, 262944323.08, 238565431.51, 205345173.62, 290686847.52, 273146859.13, 300757011.52 ), TotalBilled = c(281806122.24, 258607481.1, 345235832.56, 331032519.52, 379911877.36, 392791354.13, 425577554.8, 412169215.64, 359511992.21, 507625318.05, 463895541.75, 515639239.79)), class = "data.frame", row.names = c(NA,
-12L))

This simple version works:

df3 <- df2[,c(1,4,3,2)]#Reorder the column positions

tbl<-melt(df3,id.vars="Month_Yr")

ggplot(tbl,aes(x=Month_Yr,y=value,fill=variable))+geom_bar(stat='identity')


#When I go to add values to the plot it worked earlier this AM.

Then I reopened RStudio and ran everything up to this point without error and got to this and re-ran and it does not work

#Add values to the plot bars
#https://stackoverflow.com/questions/6644997/showing-data-values-on-stacked-bar-chart-in-ggplot2

tbl<-melt(df3,id.vars="Month_Yr")

str(tbl)

p <- qplot(Month_Yr, variable, data = tbl, geom = "bar", fill = variable, theme_set(theme_bw()))
p1 <- p + geom_text(aes(label = value), size = 3, hjust = 0.5, vjust = 3, position = "stack") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 0.5)) +
    labs(title = "Plot Example")
p1

The error is:
Don't know how to automatically pick scale for object of type function. Defaulting to continuous.
Error: All columns in a tibble must be 1d or 2d objects:
* Column `label` is function

I have no idea what that means?

Can someone help please?

Thank you.

WHP



Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}


From tomm@c @end|ng |rom nov@@edu  Thu May 30 20:42:59 2019
From: tomm@c @end|ng |rom nov@@edu (Thomas MacFarland)
Date: Thu, 30 May 2019 18:42:59 +0000
Subject: [R] Choropleth map over an outline of Florida's 67 counties
Message-ID: <BN8PR06MB61619FD91F7187FA55ADE671AF180@BN8PR06MB6161.namprd06.prod.outlook.com>

Everyone:

I am trying to build a choropleth map that uses colored shading to identify the number of incidents for a specific event during Calendar Year 2018 throughout Florida, ideally on a county-by-county basis.

I'd also like to include the county borders in the map, to emphasize urban v rural.  However, if the county borders cannot be displayed along with the choropleth colored shading that would be acceptable.

Everything is fine up to the choropleth map part of this activity and that is where I'm stuck.  Any idea, pointers, recommended references, etc. would be helpful.  Somehow, I need to have the dataset with number of incidents interface with the mapping part of this activity and I've run out of online resources for review - but still no success.

Thanks and I'm very grateful for any assistance.

Best wishes.

Tom

#############################################################
# < CY2018Incidents67FloridaCounties >
#
# There are two main goals for this activity:
#
# Produce a map of Florida that outlines all 67 counties,
# which will serve as a base map of Florida.
#
# Place over the base map of Florida a choropleth map that
# uses colored shading to give a sense of the number of
# incidents, by county, during Calendar Year 2018.
#
# Ideally, the county outlines will show along with the
# colored shading.
#
# As expected, the highly-populated urban counties will have
# more incidents than the sparsely-populated rural counties.

# Start out with a clean slate.

ls()
rm(list=ls(all=TRUE))
ls()

# Install the required packages.

install.packages(c("ggplot2", "ggthemes", "ggmap", "maps",
  "mapdata"))

library(ggplot2); library(ggthemes); library(ggmap)
library(maps); library(mapdata); library()


# Build a dataframe of U.S. states and a dataframe of
# Florida.

States.df <- map_data("state")
str(States.df)
head(States.df)
Florida.df <- subset(States.df, region == "florida")
str(Florida.df)
head(Florida.df)

# Build a dataframe of U.S. counties and a dataframe of
# Florida counties.

Counties.df <- map_data("county")
str(Counties.df)
head(Counties.df)
FloridaCounties.df <- subset(Counties.df, region == "florida")
str(FloridaCounties.df)
head(FloridaCounties.df)

# Produce a map that outlines all 67 counties in Florida.

Florida_map1 <- ggplot2::ggplot(data = FloridaCounties.df,
  mapping = aes(x = long, y = lat, group = group)) +
  coord_fixed(1.3) +
  geom_polygon(color = "red", fill = "aliceblue")

par(ask=TRUE); Florida_map1
  # Map of Florida with all 67 counties in a red outline

# Build a dataframe that details the number of Calendar Year
# 2018 incidents for each county.

CY2018Incidents.df <- read.table(textConnection("
FIPSCode   Incidents         # County
12001      89                # Alachua
12003      1                 # Baker
12005      21                # Bay
12007      4                 # Bradford
12009      165               # Brevard
12011      8218              # Broward
12013      1                 # Calhoun
12015      35                # Charlotte
12017      16                # Citrus
12019      42                # Clay
12021      115               # Collier
12023      10                # Columbia
12027      7                 # De Soto
12029      0                 # Dixie
12031      368               # Duval
12033      21                # Escambia
12035      29                # Flagler
12037      2                 # Franklin
12039      9                 # Gadsden
12041      0                 # Gilchrist
12043      2                 # Glades
12045      0                 # Gulf
12047      0                 # Hamilton
12049      3                 # Hardee
12051      15                # Hendry
12053      24                # Hernando
12055      16                # Highlands
12057      613               # Hillsborough
12059      0                 # Holmes
12061      48                # Indian River
12063      2                 # Jackson
12065      0                 # Jefferson
12067      0                 # Lafayette
12069      80                # Lake
12071      391               # Lee
12073      62                # Leon
12075      3                 # Levy
12077      0                 # Liberty
12079      2                 # Madison
12081      72                # Manatee
12083      63                # Marion
12085      70                # Martin
12086      4075              # Miami-Dade
12087      18                # Monroe
12089      12                # Nassau
12091      25                # Okaloosa
12093      5                 # Okeechobee
12095      547               # Orange
12097      93                # Osceola
12099      1602              # Palm Beach
12101      115               # Pasco
12103      232               # Pinellas
12105      154               # Polk
12107      10                # Putnam
12113      14                # Santa Rosa
12115      61                # Sarasota
12117      183               # Seminole
12109      60                # St. Johns
12111      170               # St. Lucie
12119      1                 # Sumter
12121      2                 # Suwannee
12123      1                 # Taylor
12125      0                 # Union
12127      137               # Volusia
12129      1                 # Wakulla
12131      5                 # Walton
12133      2"), header=TRUE) # Washington

attach(CY2018Incidents.df)
str(CY2018Incidents.df)
head(CY2018Incidents.df)

# And here is where I am stuck!  How do I produce what would
# otherwise be Florida_map2, where the choropleth map of
# incidents is placed over the map that outlines Florida's 67
# counties.

# May 30, 2019 02:25 PM

#############################################################

----------
Thomas W. MacFarland, Ed.D.
Senior Research Associate; Institutional Effectiveness and Associate Professor
Nova Southeastern University
Voice 954-262-5395 tommac at nova.edu<mailto:tommac at nova.edu>


	[[alternative HTML version deleted]]


From @purd|e@@ @end|ng |rom gm@||@com  Fri May 31 03:10:44 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Fri, 31 May 2019 13:10:44 +1200
Subject: [R] Calling any method within default method!
In-Reply-To: <CAKZQXCz4VdB0X5ySAEe=MrgKwiuke0mc_CkH3LsX5ZnytSmWLg@mail.gmail.com>
References: <CAKZQXCz4VdB0X5ySAEe=MrgKwiuke0mc_CkH3LsX5ZnytSmWLg@mail.gmail.com>
Message-ID: <CAB8pepxvCitwdh9ZV+HvbnmX3vREBvhj42TQnQAvurkAWiANLg@mail.gmail.com>

> For years I have a problem that I have avoided with the use of the switch
> function, but now I want to solve by following the good practices of
object
> orientation (OOP).
> My function was created to generate experiments according to some input
> parameters. Therefore, the first argument does not have a class defined as
> "data.frame", "matrix", etc., so that from the generic, specific methods
can
> be called.
> I made a simple example to show my problem.
> If anyone has any light I thank them immensely.

Welcome to the world of OOP.
Possibly my favorite topic.
:)

However, I don't understand your question.

> foo(a = 1)

This is not how method dispatch is designed to work.
Lets say you have a function to create a brazil (Brazil) object.
(Which sets or prepends "brazil" to the class attribute).
> brazil = function (...some.arguments...)
> {    x = ...some.thing...
>      class (x) = c ("brazil", class (x) )
>      ...some.other.constructor.code....
>      ...return.x...
> }

Then you would write:
> b = brazil ()
> foo (b, a=1) # not foo (a=1)

And the brazil method would be called for generic foo and object b.

Note that there are many variations of this approach.
And someone will probably correct me, if I don't also mention that R has
two main object oriented systems.
This one relates to S3, the other is S4 partly implemented in the methods
package.

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri May 31 05:10:49 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 30 May 2019 20:10:49 -0700
Subject: [R] Choropleth map over an outline of Florida's 67 counties
In-Reply-To: <BN8PR06MB61619FD91F7187FA55ADE671AF180@BN8PR06MB6161.namprd06.prod.outlook.com>
References: <BN8PR06MB61619FD91F7187FA55ADE671AF180@BN8PR06MB6161.namprd06.prod.outlook.com>
Message-ID: <BB855B7E-6B44-4F98-87B9-9B8BC698CF7D@dcn.davis.ca.us>

Since Googling "chloropleth ggplot" yields many possibilities (e.g.[1]), I can't say I am very motivated to read them and guess what deficiencies you found that made them miss your target. 

Also note that had you read the Posting Guide you would know that there is a mailing list dedicated to spatial analysis with R... r-sig-geo.

[1] https://unconj.ca/blog/choropleth-maps-with-r-and-ggplot2.html

On May 30, 2019 11:42:59 AM PDT, Thomas MacFarland <tommac at nova.edu> wrote:
>Everyone:
>
>I am trying to build a choropleth map that uses colored shading to
>identify the number of incidents for a specific event during Calendar
>Year 2018 throughout Florida, ideally on a county-by-county basis.
>
>I'd also like to include the county borders in the map, to emphasize
>urban v rural.  However, if the county borders cannot be displayed
>along with the choropleth colored shading that would be acceptable.
>
>Everything is fine up to the choropleth map part of this activity and
>that is where I'm stuck.  Any idea, pointers, recommended references,
>etc. would be helpful.  Somehow, I need to have the dataset with number
>of incidents interface with the mapping part of this activity and I've
>run out of online resources for review - but still no success.
>
>Thanks and I'm very grateful for any assistance.
>
>Best wishes.
>
>Tom
>
>#############################################################
># < CY2018Incidents67FloridaCounties >
>#
># There are two main goals for this activity:
>#
># Produce a map of Florida that outlines all 67 counties,
># which will serve as a base map of Florida.
>#
># Place over the base map of Florida a choropleth map that
># uses colored shading to give a sense of the number of
># incidents, by county, during Calendar Year 2018.
>#
># Ideally, the county outlines will show along with the
># colored shading.
>#
># As expected, the highly-populated urban counties will have
># more incidents than the sparsely-populated rural counties.
>
># Start out with a clean slate.
>
>ls()
>rm(list=ls(all=TRUE))
>ls()
>
># Install the required packages.
>
>install.packages(c("ggplot2", "ggthemes", "ggmap", "maps",
>  "mapdata"))
>
>library(ggplot2); library(ggthemes); library(ggmap)
>library(maps); library(mapdata); library()
>
>
># Build a dataframe of U.S. states and a dataframe of
># Florida.
>
>States.df <- map_data("state")
>str(States.df)
>head(States.df)
>Florida.df <- subset(States.df, region == "florida")
>str(Florida.df)
>head(Florida.df)
>
># Build a dataframe of U.S. counties and a dataframe of
># Florida counties.
>
>Counties.df <- map_data("county")
>str(Counties.df)
>head(Counties.df)
>FloridaCounties.df <- subset(Counties.df, region == "florida")
>str(FloridaCounties.df)
>head(FloridaCounties.df)
>
># Produce a map that outlines all 67 counties in Florida.
>
>Florida_map1 <- ggplot2::ggplot(data = FloridaCounties.df,
>  mapping = aes(x = long, y = lat, group = group)) +
>  coord_fixed(1.3) +
>  geom_polygon(color = "red", fill = "aliceblue")
>
>par(ask=TRUE); Florida_map1
>  # Map of Florida with all 67 counties in a red outline
>
># Build a dataframe that details the number of Calendar Year
># 2018 incidents for each county.
>
>CY2018Incidents.df <- read.table(textConnection("
>FIPSCode   Incidents         # County
>12001      89                # Alachua
>12003      1                 # Baker
>12005      21                # Bay
>12007      4                 # Bradford
>12009      165               # Brevard
>12011      8218              # Broward
>12013      1                 # Calhoun
>12015      35                # Charlotte
>12017      16                # Citrus
>12019      42                # Clay
>12021      115               # Collier
>12023      10                # Columbia
>12027      7                 # De Soto
>12029      0                 # Dixie
>12031      368               # Duval
>12033      21                # Escambia
>12035      29                # Flagler
>12037      2                 # Franklin
>12039      9                 # Gadsden
>12041      0                 # Gilchrist
>12043      2                 # Glades
>12045      0                 # Gulf
>12047      0                 # Hamilton
>12049      3                 # Hardee
>12051      15                # Hendry
>12053      24                # Hernando
>12055      16                # Highlands
>12057      613               # Hillsborough
>12059      0                 # Holmes
>12061      48                # Indian River
>12063      2                 # Jackson
>12065      0                 # Jefferson
>12067      0                 # Lafayette
>12069      80                # Lake
>12071      391               # Lee
>12073      62                # Leon
>12075      3                 # Levy
>12077      0                 # Liberty
>12079      2                 # Madison
>12081      72                # Manatee
>12083      63                # Marion
>12085      70                # Martin
>12086      4075              # Miami-Dade
>12087      18                # Monroe
>12089      12                # Nassau
>12091      25                # Okaloosa
>12093      5                 # Okeechobee
>12095      547               # Orange
>12097      93                # Osceola
>12099      1602              # Palm Beach
>12101      115               # Pasco
>12103      232               # Pinellas
>12105      154               # Polk
>12107      10                # Putnam
>12113      14                # Santa Rosa
>12115      61                # Sarasota
>12117      183               # Seminole
>12109      60                # St. Johns
>12111      170               # St. Lucie
>12119      1                 # Sumter
>12121      2                 # Suwannee
>12123      1                 # Taylor
>12125      0                 # Union
>12127      137               # Volusia
>12129      1                 # Wakulla
>12131      5                 # Walton
>12133      2"), header=TRUE) # Washington
>
>attach(CY2018Incidents.df)
>str(CY2018Incidents.df)
>head(CY2018Incidents.df)
>
># And here is where I am stuck!  How do I produce what would
># otherwise be Florida_map2, where the choropleth map of
># incidents is placed over the map that outlines Florida's 67
># counties.
>
># May 30, 2019 02:25 PM
>
>#############################################################
>
>----------
>Thomas W. MacFarland, Ed.D.
>Senior Research Associate; Institutional Effectiveness and Associate
>Professor
>Nova Southeastern University
>Voice 954-262-5395 tommac at nova.edu<mailto:tommac at nova.edu>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri May 31 11:06:27 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 31 May 2019 10:06:27 +0100
Subject: [R] Calling any method within default method!
In-Reply-To: <CAKZQXCz4VdB0X5ySAEe=MrgKwiuke0mc_CkH3LsX5ZnytSmWLg@mail.gmail.com>
References: <CAKZQXCz4VdB0X5ySAEe=MrgKwiuke0mc_CkH3LsX5ZnytSmWLg@mail.gmail.com>
Message-ID: <41812009-8d53-375d-2a23-dc4d6077ec87@sapo.pt>

Hello,

Your foo.default is wrong in:

1) The general principle. S3 methods dispatch on the class attribute, 
you cannot expect to set that attribute in foo.default

  and have it call/dispatch
  and have it call/dispatch
  and ...

2) Even if this were possible, your assignment of the class attribute is 
wrong, you assign "foo.brazil" and "foo.argentina" so the methods to be 
called would be, respectively,

foo.foo.brazil
foo.foo.argentina

Nonsense! But in fact it's just a typo.

Try assigning just "brazil" or "argentina" and error 1) above (not very 
well explained, BTW) will still occur.


A possible solution is to have a different generic, bar(), with methods 
bar.brazil and bar.argentina. Something like this:


foo <- function (x, ...) UseMethod ('foo')

foo.default <- function(x, a = 10, b = NULL, cc = 2, dd = 3,
                         type = c('brazil', 'argentina'), ...){

   ty <- match.arg(type)

   obj <- list(a = a, b = b, cc = cc, dd = dd)
   class (obj) <- ty
   res <- bar(x = obj, ...)
   res
}

bar <- function (x, ...) UseMethod ('bar')
bar.brazil <- function(x, ...){
   a <- x$a
   cc <- x$cc
   res <- a + cc
   res
}

bar.argentina <- function(x, ...){
   cc <- x$cc
   dd <- x$dd
   res <- sqrt(cc + dd)
   res
}

foo(a = 1)
#[1] 3

foo(a = 1, type = 'argentina')
#[1] 2.236068


Hope this helps,

Rui Barradas


?s 00:34 de 29/05/19, Ivan Bezerra Allaman escreveu:
> Good night dear!
> For years I have a problem that I have avoided with the use of the switch
> function, but now I want to solve by following the good practices of object
> orientation (OOP).
> My function was created to generate experiments according to some input
> parameters. Therefore, the first argument does not have a class defined as
> "data.frame", "matrix", etc., so that from the generic, specific methods can
> be called.
> I made a simple example to show my problem.
> 
> foo <- function (x, ...) UseMethod ('foo')
> 
> foo.default <- function(x,
>                          a = 10,
>                          b = NULL,
>                          cc = 2,
>                          dd = 3,
>                          type = c ('brazil', 'argentina'),
>                          ...){
> 
>    ty <- match.arg(type)
> 
>    obj <- list(a = a,
>                   b = b,
>                   cc = cc,
>                   dd = dd)
>    class (obj) <- paste ('foo',
>                                     ty,
>                                    sep = '.')
> 
>    res <- foo(x = obj, ...)
> }
> 
> foo.brazil <- function(x, ...){
>    a <- x$a
>    cc <- x$cc
>    res <- a + cc
>    return (res)
> }
> 
> foo.argentina <- function(x, ...){
>    cc <- x$cc
>    dd <- x$dd
>    res <- sqrt(cc + dd)
>    return (res)
> }
> 
> foo(a = 1)
> 
> If anyone has any light I thank them immensely.
>


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri May 31 12:40:19 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 31 May 2019 11:40:19 +0100
Subject: [R] Calling any method within default method!
In-Reply-To: <41812009-8d53-375d-2a23-dc4d6077ec87@sapo.pt>
References: <CAKZQXCz4VdB0X5ySAEe=MrgKwiuke0mc_CkH3LsX5ZnytSmWLg@mail.gmail.com>
 <41812009-8d53-375d-2a23-dc4d6077ec87@sapo.pt>
Message-ID: <c77ac72a-432a-253c-b71f-36c1b6d1958d@sapo.pt>

Hello,

Inline.

?s 10:06 de 31/05/19, Rui Barradas escreveu:
> Hello,
> 
> Your foo.default is wrong in:
> 
> 1) The general principle. S3 methods dispatch on the class attribute, 
> you cannot expect to set that attribute in foo.default
> 
>  ?and have it call/dispatch
>  ?and have it call/dispatch
>  ?and ...

Let me make this more clear.
Your foo.default, with an extra code line right after assigning the 
class to obj, print(class(obj)), outputs the following.


[1] "foo.brazil"
[1] "foo.brazil"
[1] "foo.brazil"
[1] "foo.brazil"
<--- etc --->
[1] "foo.brazil"
[1] "foo.brazil"
[1] "foo.brazil"
[1] "foo.brazil"
Error: C stack usage  7972260 is too close to the limit


It will never reach the methods you need. But if you correct the typo 
and assign like it is below, it will work as expected.


foo <- function (x, ...) UseMethod ('foo')
foo.default <- function(x, a = 10, b = NULL, cc = 2, dd = 3,
                        type = c ('brazil', 'argentina'), ...){

   ty <- match.arg(type)

   obj <- list(a = a, b = b, cc = cc, dd = dd)
   class (obj) <- ty
   res <- foo(x = obj, ...)
   res
}
foo.brazil <- function(x, ...){
   a <- x$a
   cc <- x$cc
   res <- a + cc
   res
}
foo.argentina <- function(x, ...){
   cc <- x$cc
   dd <- x$dd
   res <- sqrt(cc + dd)
   res
}

foo(a = 1)
#[1] 3

foo(a = 1, type = "argentina")
#[1] 2.236068


Hope this helps,

Rui Barradas




> 
> 2) Even if this were possible, your assignment of the class attribute is 
> wrong, you assign "foo.brazil" and "foo.argentina" so the methods to be 
> called would be, respectively,
> 
> foo.foo.brazil
> foo.foo.argentina
> 
> Nonsense! But in fact it's just a typo.
> 
> Try assigning just "brazil" or "argentina" and error 1) above (not very 
> well explained, BTW) will still occur.
> 
> 
> A possible solution is to have a different generic, bar(), with methods 
> bar.brazil and bar.argentina. Something like this:
> 
> 
> foo <- function (x, ...) UseMethod ('foo')
> 
> foo.default <- function(x, a = 10, b = NULL, cc = 2, dd = 3,
>  ??????????????????????? type = c('brazil', 'argentina'), ...){
> 
>  ? ty <- match.arg(type)
> 
>  ? obj <- list(a = a, b = b, cc = cc, dd = dd)
>  ? class (obj) <- ty
>  ? res <- bar(x = obj, ...)
>  ? res
> }
> 
> bar <- function (x, ...) UseMethod ('bar')
> bar.brazil <- function(x, ...){
>  ? a <- x$a
>  ? cc <- x$cc
>  ? res <- a + cc
>  ? res
> }
> 
> bar.argentina <- function(x, ...){
>  ? cc <- x$cc
>  ? dd <- x$dd
>  ? res <- sqrt(cc + dd)
>  ? res
> }
> 
> foo(a = 1)
> #[1] 3
> 
> foo(a = 1, type = 'argentina')
> #[1] 2.236068
> 
> 
> Hope this helps,
> 
> Rui Barradas
> 
> 
> ?s 00:34 de 29/05/19, Ivan Bezerra Allaman escreveu:
>> Good night dear!
>> For years I have a problem that I have avoided with the use of the switch
>> function, but now I want to solve by following the good practices of 
>> object
>> orientation (OOP).
>> My function was created to generate experiments according to some input
>> parameters. Therefore, the first argument does not have a class 
>> defined as
>> "data.frame", "matrix", etc., so that from the generic, specific 
>> methods can
>> be called.
>> I made a simple example to show my problem.
>>
>> foo <- function (x, ...) UseMethod ('foo')
>>
>> foo.default <- function(x,
>> ???????????????????????? a = 10,
>> ???????????????????????? b = NULL,
>> ???????????????????????? cc = 2,
>> ???????????????????????? dd = 3,
>> ???????????????????????? type = c ('brazil', 'argentina'),
>> ???????????????????????? ...){
>>
>> ?? ty <- match.arg(type)
>>
>> ?? obj <- list(a = a,
>> ????????????????? b = b,
>> ????????????????? cc = cc,
>> ????????????????? dd = dd)
>> ?? class (obj) <- paste ('foo',
>> ??????????????????????????????????? ty,
>> ?????????????????????????????????? sep = '.')
>>
>> ?? res <- foo(x = obj, ...)
>> }
>>
>> foo.brazil <- function(x, ...){
>> ?? a <- x$a
>> ?? cc <- x$cc
>> ?? res <- a + cc
>> ?? return (res)
>> }
>>
>> foo.argentina <- function(x, ...){
>> ?? cc <- x$cc
>> ?? dd <- x$dd
>> ?? res <- sqrt(cc + dd)
>> ?? return (res)
>> }
>>
>> foo(a = 1)
>>
>> If anyone has any light I thank them immensely.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From btyner @end|ng |rom gm@||@com  Fri May 31 15:25:19 2019
From: btyner @end|ng |rom gm@||@com (Benjamin Tyner)
Date: Fri, 31 May 2019 09:25:19 -0400
Subject: [R] Matrix::bdiag doesn't like being given a single named argument
Message-ID: <ba659cdc-ad67-7419-db90-8bb441ff6235@gmail.com>

Hello,

Perhaps not a bug, but interesting because the error only happens when 
there is a single named argument.

 ?? > m <- matrix(1, 1, 1)
 ?? > library(Matrix)
 ?? > bdiag(m)
 ?? 1 x 1 sparse Matrix of class "dgCMatrix"

 ?? [1,] 1
 ?? > bdiag(a = m)
 ?? Error in is.list(...) : supplied argument name 'a' does not match 'x'
 ?? > bdiag(a = m, b = m)
 ?? 2 x 2 sparse Matrix of class "dgCMatrix"

 ?? [1,] 1 .
 ?? [2,] . 1

Moreover, this works fine:

 ?? > bdiag(list(a = m))
 ?? 1 x 1 sparse Matrix of class "dgCMatrix"

 ?? [1,] 1

Thoughts?

Regards

Ben


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri May 31 16:05:02 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 31 May 2019 07:05:02 -0700
Subject: [R] 
 Matrix::bdiag doesn't like being given a single named argument
In-Reply-To: <ba659cdc-ad67-7419-db90-8bb441ff6235@gmail.com>
References: <ba659cdc-ad67-7419-db90-8bb441ff6235@gmail.com>
Message-ID: <EE929E1A-4402-4C72-A3BF-F7E629C6B70A@dcn.davis.ca.us>

My take would be that there is no reason to specify argument names at all when calling bdiag, and clearly there is a reason to not do so.

The error seems to arise from using the S3 method is.list, which allows your final example to work. is.list(a=1) clearly fails to match the x argument, but is.list(list(a=1)) works just fine.

On May 31, 2019 6:25:19 AM PDT, Benjamin Tyner <btyner at gmail.com> wrote:
>Hello,
>
>Perhaps not a bug, but interesting because the error only happens when 
>there is a single named argument.
>
> ?? > m <- matrix(1, 1, 1)
> ?? > library(Matrix)
> ?? > bdiag(m)
> ?? 1 x 1 sparse Matrix of class "dgCMatrix"
>
> ?? [1,] 1
> ?? > bdiag(a = m)
>?? Error in is.list(...) : supplied argument name 'a' does not match
>'x'
> ?? > bdiag(a = m, b = m)
> ?? 2 x 2 sparse Matrix of class "dgCMatrix"
>
> ?? [1,] 1 .
> ?? [2,] . 1
>
>Moreover, this works fine:
>
> ?? > bdiag(list(a = m))
> ?? 1 x 1 sparse Matrix of class "dgCMatrix"
>
> ?? [1,] 1
>
>Thoughts?
>
>Regards
>
>Ben
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From t@@@p@t@|ou @end|ng |rom gm@||@com  Fri May 31 03:44:24 2019
From: t@@@p@t@|ou @end|ng |rom gm@||@com (=?UTF-8?B?VGhlb2ZhbmlhLVNvdGlyaWEgUGF0c2lvdSwgzqbOsc6vzrcgzqDOrM+Ez4POuc6/z4U=?=)
Date: Fri, 31 May 2019 03:44:24 +0200
Subject: [R] MCMCglmm model set-up and interpretation
Message-ID: <CABcmRQ4GVx38AZbbQ5=_3RcJGk9rhFWO11prGV8JN3uddS9N3g@mail.gmail.com>

Dear list,



I am new to MCMCglmm and I am trying to test in my data whether there is a
significant plot effect (and of which plot) on each treatment per group of
species while accounting for phylogenetic relatedness.

My data is structured as follows:

Species: 100 species

Species type: 3 levels

Treatments: 3 levels

Plots: 14 levels

Response variable Y: continuous



What I have done is to fit the following model (after testing for various
priors I came up with the following expanded one)

prior.exp <- list(G = list(G1 = list(V = 1,  nu = 1e+06, alpha.mu=0, alpha.
V=1000)), R = list(V =1, nu = 1e+06))



fit.mod = MCMCglmm(log(Y) ~ -1 + Treatment + Plot + Group + Group*
Treatment * Plot,

            random = ~Species ,  ginverse = list(Species = ainv01), data =
input,

                family = "gaussian", nitt = 5e+06, burnin =6000, thin = 150,

                prior = prior.exp ,verbose=F)



1) Is this syntax correct to extract the effect of the plot on the
treatment per group of species or should I use Group:Treatment:Plot as they
are more like nested effects? Is it correct removing the intercept here?

2) The model summary comes up with CI per treatment for all treatment types
but for the Group and Treatment, one of the levels is kept for comparison
and is missing. This is confusing for more than 2 factors with more than 3
levels interaction, as I cannot figure out which factor's level is kept as
the reference for the given CI.

3) For the overall interaction of the 3 categorical variables, the summary
comes up with specific factor levels and not the overall effect of each
variable or their interactions.

Many thanks in advance!
Faye.

	[[alternative HTML version deleted]]


From d@v|d-wong912 @end|ng |rom hotm@||@com  Fri May 31 09:21:06 2019
From: d@v|d-wong912 @end|ng |rom hotm@||@com (Wong David)
Date: Fri, 31 May 2019 07:21:06 +0000
Subject: [R] Problem in using 'plm' package after updating R
Message-ID: <HK0PR03MB41784B34903B31A2DF4CF0EBB7190@HK0PR03MB4178.apcprd03.prod.outlook.com>

Dear Madam/ Sir,

After I updated the latest packages in R this afternoon, I found that I cannot use 'plm' package. The error messages are as follows:

"R version 3.3.3 (2017-03-06) -- "Another Canoe"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: i386-w64-mingw32/i386 (32-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

Warning: namespace ??plm?? is not available and has been replaced by .GlobalEnv when processing object ??inst_f??

[Previously saved workspace restored]

> utils:::menuInstallPkgs()
--- Please select a CRAN mirror for use in this session ---

  There is a binary version available but the source version is later:
    binary source needs_compilation
plm  1.6-6  2.0-1             FALSE

installing the source package ??plm??

trying URL 'https://mirror-hk.koddos.net/CRAN/src/contrib/plm_2.0-1.tar.gz'
Content type 'application/x-gzip' length 2343782 bytes (2.2 MB)
downloaded 2.2 MB

* installing *source* package 'plm' ...
** package 'plm' successfully unpacked and MD5 sums checked
** R
** data
** inst
** preparing package for lazy loading
Error in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]) :
  there is no package called 'stringr'
ERROR: lazy loading failed for package 'plm'
* removing 'C:/Users/David Wong/Documents/R/win-library/3.3/plm'
* restoring previous 'C:/Users/David Wong/Documents/R/win-library/3.3/plm'

The downloaded source packages are in
        ??C:\Users\David Wong\AppData\Local\Temp\RtmpEdDjdI\downloaded_packages??
Warning messages:
1: running command '"C:/PROGRA~1/R/R-33~1.3/bin/i386/R" CMD INSTALL -l "C:\Users\David Wong\Documents\R\win-library\3.3" C:\Users\DAVIDW~1\AppData\Local\Temp\RtmpEdDjdI/downloaded_packages/plm_2.0-1.tar.gz' had status 1
2: In install.packages(NULL, .libPaths()[1L], dependencies = NA, type = type) :
  installation of package ??plm?? had non-zero exit status
> library(plm)
Error in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]) :
  there is no package called ??stringr??
Error: package or namespace load failed for ??plm??
> help.start()
starting httpd help server ... done
If nothing happens, you should open ??http://127.0.0.1:19429/doc/html/index.html?? yourself

The error messages are highlighted in blue. Please assist.

Thanks and regards,
David Wong


	[[alternative HTML version deleted]]


From |v@n@|@m@n @end|ng |rom gm@||@com  Fri May 31 13:35:10 2019
From: |v@n@|@m@n @end|ng |rom gm@||@com (Ivan Bezerra Allaman)
Date: Fri, 31 May 2019 08:35:10 -0300
Subject: [R] Calling any method within default method!
Message-ID: <CAKZQXCxocoxe7eOwbUwDa3je7VtWfzb8ucTUNQixgakQ2SfBWw@mail.gmail.com>

Hi Abby and Rui!

Thank's by your responses!

Rui,

His explanations were very illuminating. I had not realized the
misconception,

paste ('foo', ...)

Thank you!

-- 
\begin{signature}
<<>>=
Prof. D.Sc. Ivan Bezerra Allaman
Laborat?rio de Estat?stica Computacional
Departamento de Ci?ncias Exatas e Tecnol?gicas
Universidade Estadual de Santa Cruz

Ilh?us/BA - Brasil
Fone: +55 73 3680-5622
E-mail:ivanalaman at gmail.com
@
\end{signature}

	[[alternative HTML version deleted]]


